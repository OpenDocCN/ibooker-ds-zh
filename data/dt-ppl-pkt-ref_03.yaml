- en: Chapter 3\. Common Data Pipeline Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章\. 常见数据管道模式
- en: Even for seasoned data engineers, designing a new data pipeline is a new journey
    each time. As discussed in [Chapter 2](ch02.xhtml#ch02), differing data sources
    and infrastructure present both challenges and opportunities. In addition, pipelines
    are built with different goals and constraints. Must the data be processed in
    near real time? Can it be updated daily? Will it be modeled for use in a dashboard
    or as input to a machine learning model?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于经验丰富的数据工程师来说，设计新的数据管道每次都是一次新的旅程。正如在[第二章](ch02.xhtml#ch02)中讨论的，不同的数据来源和基础设施既是挑战也是机遇。此外，管道的构建目标和约束也各不相同。数据是否需要几乎实时处理？可以每天更新吗？将其建模用于仪表盘还是作为机器学习模型的输入？
- en: Thankfully, there are some common patterns in data pipelines that have proven
    successful and are extensible to many use cases. In this chapter, I will define
    these patterns. Subsequent chapters implement pipelines built on them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数据管道中存在一些常见模式已被证明成功，并且可以扩展到许多用例。在本章中，我将定义这些模式。后续章节将基于这些模式实现管道。
- en: ETL and ELT
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ETL 和 ELT
- en: There is perhaps no pattern more well known than ETL and its more modern sibling,
    ELT. Both are patterns widely used in data warehousing and business intelligence.
    In more recent years, they’ve inspired pipeline patterns for data science and
    machine learning models running in production. They are so well known that many
    people use these terms synonymously with data pipelines rather than patterns that
    many pipelines follow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 或许没有比 ETL 及其现代化的姊妹 ELT 更为人熟知的模式了。这两者都是广泛用于数据仓库和商业智能的模式。近年来，它们还为运行在生产中的数据科学和机器学习模型的管道模式提供了灵感。它们如此著名，以至于许多人将这些术语与数据管道同义使用，而不是许多管道遵循的模式。
- en: Given their roots in data warehousing, it’s easiest to describe them in that
    context, which is what this section does. Later sections in this chapter describe
    how they are used for particular use cases.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到它们源自数据仓库，最容易在这个背景下描述它们，这也是本节所做的。本章后续部分将描述它们在特定用例中的使用方式。
- en: Both patterns are approaches to data processing used to feed data into a data
    warehouse and make it useful to analysts and reporting tools. The difference between
    the two is the order of their final two steps (transform and load), but the design
    implications in choosing between them are substantial, as I’ll explain throughout
    this chapter. First, let’s explore the steps of ETL and ELT.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模式都是用于将数据输入数据仓库并使其对分析师和报告工具有用的数据处理方法。它们之间的区别在于其最后两个步骤（转换和加载）的顺序，但在选择它们之间的设计影响方面存在重大差异，正如我将在本章中详细解释的那样。首先，让我们探讨
    ETL 和 ELT 的步骤。
- en: The *extract* step gathers data from various sources in preparation for loading
    and transforming. [Chapter 2](ch02.xhtml#ch02) discussed the diversity of these
    sources and methods of extraction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*提取* 步骤从各种来源收集数据，为加载和转换做准备。[第二章](ch02.xhtml#ch02)讨论了这些来源的多样性和提取方法。'
- en: The *load* step brings either the raw data (in the case of ELT) or the fully
    transformed data (in the case of ETL) into the final destination. Either way,
    the end result is loading data into the data warehouse, data lake, or other destination.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*加载* 步骤将原始数据（在 ELT 的情况下）或完全转换后的数据（在 ETL 的情况下）带入最终目的地。无论哪种方式，最终结果都是将数据加载到数据仓库、数据湖或其他目的地中。'
- en: The *transform* step is where the raw data from each source system is combined
    and formatted in a such a way that it’s useful to analysts, visualization tools,
    or whatever use case your pipeline is serving. There’s a lot to this step, regardless
    of whether you design your process as ETL or ELT, all of which is explored in
    detail in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*转换* 步骤是从每个源系统的原始数据中合并和格式化数据，使其对分析师、可视化工具或管道服务的任何用例都有用。无论您将流程设计为 ETL 还是 ELT，此步骤都有很多内容，所有这些内容都将在[第六章](ch06.xhtml#ch06)中详细探讨。'
- en: The Emergence of ELT over ETL
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELT 的兴起超越了 ETL
- en: ETL was the gold standard of data pipeline patterns for decades. Though it’s
    still used, more recently ELT has emerged as the pattern of choice. Why? Prior
    to the modern breed of data warehouses, primarily in the cloud (see [Chapter 2](ch02.xhtml#ch02)),
    data teams didn’t have access to data warehouses with the storage or compute necessary
    to handle loading vast amounts of raw data and transforming it into usable data
    models all in the same place. In addition, data warehouses at the time were row-based
    databases that worked well for transactional use cases, but not for the high-volume,
    bulk queries that are commonplace in analytics. Thus, data was first extracted
    from source systems and then transformed on a separate system before being loaded
    into a warehouse for any final data modeling and querying by analysts and visualization
    tools.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，ETL一直是数据管道模式的黄金标准。尽管现在仍在使用，但最近ELT已经成为首选模式。为什么呢？在现代云端主导的数据仓库之前（参见[第 2 章](ch02.xhtml#ch02)），数据团队没有访问具备足够存储或计算能力的数据仓库，无法处理大量原始数据加载和转换成可用数据模型的需求。此外，当时的数据仓库是面向事务使用场景的行存储数据库，但对于分析中常见的大容量批量查询并不适用。因此，数据首先从源系统中提取出来，然后在另一个系统上进行转换，最后再加载到数据仓库中供分析师和可视化工具查询和模型化使用。
- en: The majority of today’s data warehouses are built on highly scalable, columnar
    databases that can both store and run bulk transforms on large datasets in a cost-effective
    manner. Thanks to the I/O efficiency of a columnar database, data compression,
    and the ability to distribute data and queries across many nodes that can work
    together to process data, things have changed. It’s now better to focus on extracting
    data and loading it into a data warehouse where you can then perform the necessary
    transformations to complete the pipeline.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当今大多数数据仓库基于高度可扩展的列存储数据库构建，可以在成本效益高的情况下存储和运行大规模数据转换操作。得益于列存储数据库的I/O效率、数据压缩能力以及能够跨多个节点分发数据和查询的能力，情况已经发生了变化。现在更加适合专注于提取数据并加载到数据仓库，然后在那里执行必要的转换操作来完成数据管道。
- en: The impact of the difference between row-based and column-based data warehouses
    cannot be overstated. [Figure 3-1](#fig_0301) illustrates an example of how records
    are stored on disk in a row-based database, such as MySQL or Postgres. Each row
    of the database is stored together on disk, in one or more blocks depending on
    the size of each record. If a record is smaller than a single block or not cleanly
    divisible by the block size, it leaves some disk space unused.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 行存储和列存储数据仓库之间的差异影响深远。[图 3-1](#fig_0301)展示了如何在行存储数据库（如MySQL或Postgres）中将记录存储在磁盘上的示例。数据库的每一行都整体存储在磁盘上的一个或多个块中，具体取决于每条记录的大小。如果记录小于一个块或者无法整除块大小，则会导致一些磁盘空间未被使用。
- en: '![dppr 0301](Images/dppr_0301.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0301](Images/dppr_0301.png)'
- en: Figure 3-1\. A table stored in a row-based storage database. Each block contains
    a record (row) from the table.
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 存储在行存储数据库中的表格。每个块包含表中的一条记录（行）。
- en: Consider an online transaction processing (OLTP) database use case such as an
    e-commerce web application that leverages a MySQL database for storage. The web
    app requests reads and writes from and to the MySQL database, often involving
    multiple values from each record, such as the details of an order on an order
    confirmation page. It’s also likely to query or update only one order at a time.
    Therefore, row-based storage is optimal since the data the application needs is
    stored in close proximity on disk, and the amount of data queried at one time
    is small.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个在线事务处理（OLTP）数据库用例，比如利用MySQL数据库存储的电子商务网站应用程序。Web应用程序从MySQL数据库读取和写入数据，通常涉及每条记录中的多个值，比如订单确认页面上的订单详情。同时，可能仅查询或更新一个订单。因此，行存储是最优选择，因为应用程序所需的数据在磁盘上的靠近位置存储，并且每次查询的数据量较小。
- en: The inefficient use of disk space due to records leaving empty space in blocks
    is a reasonable trade-off in this case, as the speed to reading and writing single
    records frequently is what’s most important. However, in analytics the situation
    is reversed. Instead of the need to read and write small amounts of data frequently,
    we often read and write a large amount of data infrequently. In addition, it’s
    less likely that an analytical query requires many, or all, of the columns in
    a table but rather a single column of a table with many columns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于记录在块中留下空间导致的磁盘空间利用不佳，在这种情况下是可以接受的平衡，因为频繁读写单个记录的速度是最重要的。然而，在分析中情况恰恰相反。我们通常不需要频繁读写少量数据，而是偶尔需要读写大量数据。此外，分析查询很少需要表中的许多列，而是一个具有许多列的表中的单个列。
- en: For example, consider the order table in our fictional e-commerce application.
    Among other things, it contains the dollar amount of the order as well as the
    country it’s shipping to. Unlike the web application, which works with orders
    one at a time, an analyst using the data warehouse will want to analyze orders
    in bulk. In addition, the table containing order data in the data warehouse has
    additional columns that contain values from multiple tables in our MySQL database.
    For example, it might contain the information about the customer who placed the
    order. Perhaps the analyst wants to sum up all orders placed by customers with
    currently active accounts. Such a query might involve millions of records, but
    only read from two columns, OrderTotal and CustomerActive. After all, analytics
    is not about creating or changing data (like in OLTP) but rather the derivation
    of metrics and the understanding of data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们虚构的电子商务应用程序中考虑订单表。除其他事项外，它包含订单金额以及正在发货的国家。与按顺序处理订单的Web应用程序不同，使用数据仓库的分析师将希望批量分析订单。此外，数据仓库中包含订单数据的表还包含来自我们的
    MySQL 数据库中多个表的值的额外列。例如，它可能包含下订单的客户的信息。也许分析师想要总结所有由当前活跃账户的客户下的订单。这样的查询可能涉及数百万条记录，但仅需读取两列，OrderTotal
    和 CustomerActive。毕竟，分析不是关于创建或更改数据（例如在 OLTP 中）而是关于推导指标和理解数据。
- en: As illustrated in [Figure 3-2](#fig_0302), a columnar database, such as Snowflake
    or Amazon Redshift, stores data in disk blocks by column rather than row. In our
    use case, the query written by the analyst only needs to access blocks that store
    OrderTotal and CustomerActive values rather than blocks that store the row-based
    records such as the MySQL database. Thus, there’s less disk I/O as well as less
    data to load into memory to perform the filtering and summing required by the
    analyst’s query. A final benefit is reduction in storage, thanks to the fact that
    blocks can be fully utilized and optimally compressed since the same data type
    is stored in each block rather than multiple types that tend to occur in a single
    row-based record.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 3-2](#fig_0302) 所示，例如 Snowflake 或 Amazon Redshift 这样的列数据库，将数据按列而非按行存储在磁盘块中。在我们的使用案例中，分析师编写的查询只需访问存储
    OrderTotal 和 CustomerActive 值的块，而不是存储像 MySQL 数据库那样基于行的记录的块。因此，需要访问的磁盘 I/O 较少，需要加载到内存中执行分析查询所需的筛选和求和操作的数据也较少。最终的好处是存储量减少，因为块可以充分利用并进行最佳压缩，因为每个块中存储的是相同的数据类型，而不是在单个基于行的记录中常见的多种类型。
- en: All in all, the emergence of columnar databases means that storing, transforming,
    and querying large datasets is efficient within a data warehouse. Data engineers
    can use that to their advantage by building pipeline steps that specialize in
    extracting and loading data into warehouses where it can be transformed, modeled,
    and queried by analysts and data scientists who are more comfortable within the
    confines of a database. As such, ELT has taken over as the ideal pattern for data
    warehouse pipelines as well as other use cases in machine learning and data product
    development.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，列式数据库的出现意味着在数据仓库中高效存储、转换和查询大型数据集。数据工程师可以利用这一优势，通过构建专门用于从数据仓库中提取和加载数据的流水线步骤，让分析师和数据科学家可以在数据库的限制范围内进行数据转换、建模和查询。因此，ELT
    已成为数据仓库流水线以及机器学习和数据产品开发中其他用例的理想模式。
- en: '![dppr 0302](Images/dppr_0302.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0302](Images/dppr_0302.png)'
- en: Figure 3-2\. A table stored in a column-based storage database. Each disk block
    contains data from the same column. The two columns involved in our example query
    are highlighted. Only these blocks must be accessed to run the query. Each block
    contains data of the same type, making compression optimal.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 存储在基于列的存储数据库中的表。每个磁盘块包含来自同一列的数据。我们示例查询涉及的两列已经突出显示。只有这些块需要访问才能运行查询。每个块包含相同类型的数据，使压缩效果最佳。
- en: EtLT Subpattern
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EtLT子模式
- en: When ELT emerged as the dominant pattern, it became clear that doing some transformation
    after extraction, but before loading, was still beneficial. However, instead of
    transformation involving business logic or data modeling, this type of transformation
    is more limited in scope. I refer to this as *lowercase t* transformation, or
    *EtLT*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当ELT模式成为主流模式时，明确的是在加载之前执行一些转换仍然是有益的。但是，与涉及业务逻辑或数据建模的转换不同，这种类型的转换范围更有限。我将其称为*小写t*转换，或*EtLT*。
- en: 'Some examples of the type of transformation that fits into the EtLT subpattern
    include the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 符合EtLT子模式的转换类型的一些示例包括以下内容：
- en: Deduplicate records in a table
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表中去重记录
- en: Parse URL parameters into individual components
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将URL参数解析为各个组件
- en: Mask or otherwise obfuscate sensitive data
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩盖或以其他方式混淆敏感数据
- en: These types of transforms are either fully disconnected from business logic
    or, in the case of something like masking sensitive data, at times required as
    early in a pipeline as possible for legal or security reasons. In addition, there
    is value in using the right tool for the right job. As Chapters [4](ch04.xhtml#ch04)
    and [5](ch05.xhtml#ch05) illustrate in greater detail, most modern data warehouses
    load data most efficiently if it’s prepared well. In pipelines moving a high volume
    of data, or where latency is key, performing some basic transforms between the
    extract and load steps is worth the effort.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的转换要么完全与业务逻辑脱节，要么（例如，对敏感数据进行掩码）有时由于法律或安全原因需要尽早在管道中执行。此外，使用合适的工具完成合适的工作也有其价值。正如第[4](ch04.xhtml#ch04)章和第[5](ch05.xhtml#ch05)章详细说明的那样，如果准备充分，大多数现代数据仓库可以以最高效的方式加载数据。在处理大量数据或关键性能时延要求高的管道中，在提取和加载步骤之间执行一些基本的转换是值得的。
- en: You can assume that the remaining ELT-related patterns are designed to include
    the EtLT subpattern as well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以假设剩余的ELT相关模式设计的目的是包含EtLT子模式。
- en: ELT for Data Analysis
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于数据分析的ELT
- en: ELT has become the most common and, in my opinion, most optimal pattern for
    pipelines built for data analysis. As already discussed, columnar databases are
    well suited to handling high volumes of data. They are also designed to handle
    wide tables, meaning tables with many columns, thanks to the fact that only data
    in columns used in a given query are scanned on disk and loaded into memory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于为数据分析构建的管道来说，ELT已经成为最常见且我认为是最优化的模式。正如前面讨论的那样，列存储数据库非常适合处理大量数据。它们还设计用于处理宽表，即具有许多列的表，这要归功于仅扫描磁盘上用于特定查询的列数据并将其加载到内存中的事实。
- en: Beyond technical considerations, data analysts are typically fluent in SQL.
    With ELT, data engineers can focus on the extract and load steps in a pipeline
    (data ingestion), while analysts can utilize SQL to transform the data that’s
    been ingested as needed for reporting and analysis. Such a clean separation is
    not possible with an ETL pattern, as data engineers are needed across the entire
    pipeline. As shown in [Figure 3-3](#fig_0303), ELT allows data team members to
    focus on their strengths with less interdependencies and coordination.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术考虑因素外，数据分析员通常精通SQL。使用ELT时，数据工程师可以专注于管道中的提取和加载步骤（数据摄取），而分析员可以利用SQL根据需要转换已被摄入的数据以供报告和分析。这种清晰的分离在ETL模式中是不可能的，因为数据工程师在整个管道中都是必需的。如[图 3-3](#fig_0303)所示，ELT允许数据团队成员专注于各自的优势，减少了相互依赖和协调的需要。
- en: In addition, the ELT pattern reduces the need to predict exactly what analysts
    will do with the data at the time of building extract and load processes. Though
    understanding the general use case is required to extract and load the proper
    data, saving the transform step for later gives analysts more options and flexibility.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ELT模式减少了在构建提取和加载过程时需要准确预测分析员将如何使用数据的必要性。虽然了解一般用例是提取和加载适当数据的要求，但是将转换步骤推迟到后面为分析员提供了更多选择和灵活性。
- en: '![dppr 0303](Images/dppr_0303.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0303](Images/dppr_0303.png)'
- en: Figure 3-3\. The ELT pattern allows for a clean split of responsibilities between
    data engineers and data analysts (or data scientists). Each role can work autonomously
    with the tools and languages they are comfortable in.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. ELT模式允许数据工程师和数据分析师（或数据科学家）在工具和语言上独立工作。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: With the emergence of ELT, data analysts have become more autonomous and empowered
    to deliver value from data without being “blocked” by data engineers. Data engineers
    can focus on data ingestion and supporting infrastructure that enables analysts
    to write and deploy their own transform code written as SQL. With that empowerment
    have come new job titles such as the *analytics engineer*. [Chapter 6](ch06.xhtml#ch06)
    discusses how these data analysts and analytics engineers transform data to build
    data models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ELT的出现，数据分析师变得更加自主，并能够通过自己编写和部署作为SQL编写的转换代码，从数据中提取价值而不被数据工程师“阻塞”。数据工程师可以专注于数据引入和支持基础设施。随着这种赋权，出现了新的工作职称，如*分析工程师*。[第
    6 章](ch06.xhtml#ch06) 讨论了这些数据分析师和分析工程师如何转换数据以构建数据模型。
- en: ELT for Data Science
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学的ELT
- en: Data pipelines built for data science teams are similar to those built for data
    analysis in a data warehouse. Like the analysis use case, data engineers are focused
    on ingesting data into a data warehouse or data lake. However, data scientists
    have different needs from the data than data analysts do.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为数据科学团队构建的数据管道与数据仓库中为数据分析构建的管道类似。与分析用例类似，数据工程师专注于将数据引入数据仓库或数据湖。然而，数据科学家对数据的需求与数据分析师不同。
- en: Though data science is a broad field, in general, data scientists will need
    access to more granular—and at times raw—data than data analysts do. While data
    analysts build data models that produce metrics and power dashboards, data scientists
    spend their days exploring data and building predictive models. While the details
    of the role of a data scientist are out of the scope of this book, this high-level
    distinction matters to the design of pipelines serving data scientists.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据科学是一个广泛的领域，但总体来说，数据科学家需要比数据分析师更详细和有时更原始的数据访问。虽然数据分析师建立的数据模型能生成指标并支持仪表板，数据科学家则花费大部分时间探索数据和构建预测模型。虽然本书不涵盖数据科学家的具体角色细节，但这种高层次的区别对于为数据科学家设计流水线是重要的。
- en: If you’re building pipelines to support data scientists, you’ll find that the
    extract and load steps of the ELT pattern will remain pretty much the same as
    they will for supporting analytics. Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05)
    outline those steps in technical detail. Data scientists might also benefit from
    working with some of the data models built for analysts in the transform step
    of an ELT pipeline ([Chapter 6](ch06.xhtml#ch06)), but they’ll likely branch off
    and use much of the data acquired during extract-load.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建管道以支持数据科学家，你会发现ELT模式中的提取和加载步骤与支持分析的步骤基本相同。第 [4 章](ch04.xhtml#ch04) 和
    [5 章](ch05.xhtml#ch05) 详细介绍了这些技术步骤。数据科学家在ELT管道的转换步骤中也可能受益于与分析师构建的某些数据模型一起工作（[第
    6 章](ch06.xhtml#ch06)），但他们可能会分支出去，并使用在提取加载过程中获取的大部分数据。
- en: ELT for Data Products and Machine Learning
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据产品和机器学习的ELT
- en: 'Data is used for more than analysis, reporting, and predictive models. It’s
    also used for powering *data products*. Some common examples of data products
    include the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不仅用于分析、报告和预测模型，还用于支持*数据产品*。一些常见的数据产品示例包括以下内容：
- en: A content recommendation engine that powers a video streaming home screen
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动视频流首页的内容推荐引擎
- en: A personalized search engine on an e-commerce website
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子商务网站上的个性化搜索引擎
- en: An application that performs sentiment analysis on user-generated restaurant
    reviews
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个应用程序，对用户生成的餐厅评论进行情感分析
- en: Each of those data products is likely powered by one or more machine learning
    (ML) models, which are hungry for training and validation data. Such data may
    come from a variety of source systems and undergo some level of transformation
    to prepare it for use in the model. An ELT-like pattern is well suited for such
    needs, though there are a number of specific challenges in all steps of a pipeline
    that’s designed for a data product.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据产品中的每一个可能都由一个或多个机器学习（ML）模型驱动，这些模型需要训练和验证数据。这些数据可能来自各种源系统，并经过一定程度的转换以准备用于模型。类似ELT的模式非常适合这些需求，尽管面向数据产品设计的管道的所有步骤都存在一些具体的挑战。
- en: Steps in a Machine Learning Pipeline
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习流水线中的步骤
- en: Like pipelines built for analysis, which this book is primarily focused on,
    pipelines built for ML follow a pattern similar to ELT—at least in the beginning
    of the pipeline. The difference is that instead of the transform step focusing
    on transforming data into data models, once data is extracted and loaded into
    a warehouse or data lake, there a several steps involved in building and updating
    the ML model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 像本书主要专注的分析管道一样，构建用于机器学习的管道也遵循与ELT类似的模式，至少在管道的开始阶段是如此。不同之处在于，转换步骤不再专注于将数据转换为数据模型，而是一旦数据被提取并加载到数据仓库或数据湖中，就涉及到多个步骤来构建和更新机器学习模型。
- en: 'If you’re familiar with ML development, these steps may look familiar as well:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉机器学习开发，这些步骤可能看起来也很熟悉：
- en: Data ingestion
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入
- en: This step follows the same process that I outline in Chapters [4](ch04.xhtml#ch04)
    and [5](ch05.xhtml#ch05). Though the data you ingest may differ, the logic remains
    primarily the same for pipelines built for analytics as well as ML, but with one
    additional consideration for ML pipelines. That is, ensuring that the data you
    ingest is versioned in a way that ML models can later refer to as a specific dataset
    for training or validation. There are a number of tools and approaches for versioning
    datasets. I suggest referring to [“Further Reading on ML Pipelines”](#further-reading-3)
    to learn more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤遵循我在第 [4](ch04.xhtml#ch04) 章和 [5](ch05.xhtml#ch05) 章中概述的相同过程。尽管您输入的数据可能不同，但对于分析和机器学习构建的管道，逻辑基本保持一致，但对于机器学习管道还有一个额外考虑的因素。即，确保您输入的数据以可以供后续机器学习模型引用的特定数据集版本化。有多种工具和方法可以对数据集进行版本管理。建议参考
    [“ML管道进一步阅读”](#further-reading-3) 以了解更多信息。
- en: Data preprocessing
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理
- en: The data that’s ingested is unlikely to be ready to use in ML development. Preprocessing
    is where data is cleaned and otherwise prepared for models. For example, this
    is the step in a pipeline where text is tokenized, features are converted to numerical
    values, and input values are normalized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 摄入的数据不太可能直接用于机器学习开发。预处理是数据清理和为模型准备数据的过程。例如，在管道中，这是文本标记化、特征转换为数值以及输入值归一化的步骤。
- en: Model training
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练
- en: After new data is ingested and preprocessed, ML models need to be retrained.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在新数据被摄入和预处理后，机器学习模型需要重新训练。
- en: Model deployment
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署
- en: Deploying models to production can be the most challenging part of going from
    research-oriented machine learning to a true data product. Here, not only is versioning
    of datasets necessary, but versioning of trained models is also needed. Often,
    a REST API is used to allow for querying of a deployed model, and API endpoints
    for various versions of a model will be used. It’s a lot to keep track of and
    takes coordination between data scientists, machine learning engineers, and data
    engineers to get to a production state. A well-designed pipeline is key to gluing
    it together.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境可能是从以研究为导向的机器学习到真正数据产品的最具挑战性的部分。在这里，不仅需要对数据集进行版本控制，还需要对训练过的模型进行版本控制。通常情况下，会使用REST
    API来允许查询已部署模型，并将用于各个模型版本的API端点。这是一个需要跟踪和协调数据科学家、机器学习工程师和数据工程师共同努力达到生产状态的过程。一个设计良好的管道是将这一切粘合在一起的关键。
- en: Incorporate Feedback in the Pipeline
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在管道中整合反馈
- en: Any good ML pipeline will also include gathering feedback for improving the
    model. Take the example of a content recommendation model for a video streaming
    service. To measure and improve the model in the future, you’ll need to keep track
    of what it recommends to users, what recommendations they click, and what recommended
    content they enjoy after they click it. To do so, you’ll need to work with the
    development team leveraging the model on the streaming services home screen. They’ll
    need to implement some type of event collection that keeps track of each recommendation
    made to each user; the version of the model that recommended it; and when it’s
    clicked; and then carry that click-through to the data they’re likely already
    collecting related to a user’s content consumption.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 任何优秀的机器学习管道还将包括收集反馈以改进模型的步骤。以视频流服务的内容推荐模型为例。为了在未来衡量和改进模型，您需要跟踪其推荐给用户的内容、用户点击的推荐以及点击后他们享受的推荐内容。为此，您需要与负责在流服务主屏上使用模型的开发团队合作。他们需要实施一些类型的事件收集，以跟踪每个用户推荐的推荐内容，推荐时使用的模型版本，以及点击后的行为，并将点击行为与他们可能已经收集的有关用户内容消费的数据联系起来。
- en: All that information can then be ingested back into the data warehouse and incorporated
    into future versions of the model, either as training data or to be analyzed and
    considered by a human (a data scientist perhaps) for inclusion in a future model
    or experiment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些信息都可以被摄入到数据仓库中，并被纳入到未来模型的版本中，可以作为训练数据，也可以被数据科学家（也许是数据科学家）分析和考虑，以便将来纳入到模型或实验中。
- en: In addition, the data collected can be ingested, transformed, and analyzed by
    data analysts in the ELT pattern described throughout this book. Analysts will
    often be tasked with measuring the effectiveness of models and building dashboards
    to display key metrics of the model to the organization. Stakeholders can use
    such dashboards to make sense of how effective various models are to the business
    and to their customers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，收集到的数据可以被数据分析师在本书中描述的ELT模式中摄入、转换和分析。分析师通常被委托测量模型的有效性，并构建仪表板来显示组织的模型关键指标。利益相关者可以使用这些仪表板来理解各种模型对业务和客户的效果如何。
- en: Further Reading on ML Pipelines
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有关ML流水线的进一步阅读
- en: 'Building pipelines for machine learning models is a robust topic. Depending
    on your infrastructure choices and the complexity of your ML environment, there
    are several books I recommend for further learning:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为机器学习模型构建流水线是一个强大的主题。根据您的基础设施选择和ML环境的复杂性，我建议您阅读以下几本书进行进一步学习：
- en: '*Building Machine Learning Pipelines* by Hannes Hapke and Catherine Nelson
    (O’Reilly, 2020)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建机器学习流水线* by Hannes Hapke 和 Catherine Nelson (O’Reilly, 2020)'
- en: '*Machine Learning with Scikit-Learn, Keras, and TensorFlow*, 2nd edition, by
    Aurélien Géron (O’Reilly, 2019)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Scikit-Learn、Keras和TensorFlow进行机器学习*，第二版，作者Aurélien Géron (O’Reilly, 2019)'
- en: 'In addition, the following book is a highly accessible introduction to machine
    learning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下书籍是一个极其易于理解的机器学习入门：
- en: '*Introduction to Machine Learning with Python* by Andreas C. Müller and Sarah
    Guido (O’Reilly, 2016)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python机器学习入门* by Andreas C. Müller 和 Sarah Guido (O’Reilly, 2016)'
