- en: 13 Robust machine learning with ML Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 使用机器学习管道进行鲁棒的机器学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using transformers and estimators to transform data into ML features
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用转换器和估计器将数据转换为机器学习特征
- en: Assembling features into a vector through an ML pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过机器学习管道将特征组装成向量
- en: Training a simple ML model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个简单的机器学习模型
- en: Evaluating a model using relevant performance metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相关的性能指标评估模型
- en: Optimizing a model using cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证优化模型
- en: Interpreting a model’s decision-making process through feature weights
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征权重解释模型的决策过程
- en: 'In the previous chapter, we set the stage for machine learning: from a raw
    data set, we tamed the data and crafted features based on our exploration and
    analysis of the data. Looking back at the data transformation steps from chapter
    12, we performed the following work, resulting in a data frame named `food_features`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们为机器学习奠定了基础：从原始数据集开始，我们驯化了数据，并根据我们对数据的探索和分析创建了特征。回顾第12章中的数据转换步骤，我们执行了以下工作，最终得到名为`food_features`的数据框：
- en: Read a CSV file containing dish names and multiple columns as feature candidates
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取包含菜名和多个列作为特征候选的CSV文件
- en: Sanitized the column names (lowered the case and fixed the punctuation, spacing,
    and nonprintable characters)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理了列名（将大小写转换为小写，并修正了标点符号、空格和非打印字符）
- en: Removed illogical and irrelevant records
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除了不合理和不相关的记录
- en: Filled the `null` values of binary columns to `0.0`
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二进制列的`null`值填充为`0.0`
- en: Capped the amounts for `calories`, `protein`, `fat`, and `sodium` to the 99th
    percentile
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`calories`、`protein`、`fat`和`sodium`的数值限制在99百分位数
- en: Created ratio features (number of calories from a macro over number of calories
    for the dish)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了比率特征（宏量营养素的热量与菜肴热量的比率）
- en: Imputed the `mean` of continuous features
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 补充了连续特征的`mean`值
- en: Scaled continuous features between `0.0` and `1.0`
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连续特征缩放到`0.0`到`1.0`之间
- en: Tip If you want to catch up with the code from chapter 12, I included the code
    that leads to `food_features` in the book’s repository under `./code/` `Ch12/end_of_chapter.py`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您想跟上第12章的代码，我在书的仓库中包含了生成`food_features`的代码，位于`./code/` `Ch12/end_of_chapter.py`。
- en: In this chapter, we continue our journey to a robust ML training program. We
    delve deeper into transformers and estimators, this time in the context of an
    ML pipeline. With this new tool, we first train and evaluate our initial model.
    We then learn about *customizing* an ML pipeline at runtime, using cross-validation,
    a popular ML optimization technique, for optimizing our model parameters. Finally,
    we briefly discuss model interpretability by extracting the model coefficients
    (the weights attributed to each parameter) from our ML pipeline.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续我们的鲁棒机器学习训练之旅。我们更深入地探讨了转换器和估计器，这次是在机器学习管道的背景下。有了这个新工具，我们首先训练和评估我们的初始模型。然后，我们学习如何在运行时*定制*机器学习管道，使用交叉验证，这是一种流行的机器学习优化技术，来优化我们的模型参数。最后，我们简要讨论了通过从我们的机器学习管道中提取模型系数（分配给每个参数的权重）来解释模型的可解释性。
- en: ML pipelines are how PySpark implements ML capabilities. They provide better
    code organization and flexibility, at the expense of a little preparation up front.
    This chapters starts by explaining what an ML pipeline is, using the dessert prediction
    data set we created in chapter 12\. We review just enough theory about transformers,
    estimators, and ML pipelines to get us started.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道是PySpark实现机器学习功能的方式。它们提供了更好的代码组织和灵活性，但需要前期一点准备。本章首先解释了什么是机器学习管道，使用我们在第12章创建的甜点预测数据集。我们回顾了关于转换器、估计器和机器学习管道的足够理论，以便我们开始。
- en: '13.1 Transformers and estimators: The building blocks of ML in Spark'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 转换器和估计器：Spark中机器学习的基本构建块
- en: 'This section covers the two main components of ML pipelines: transformers and
    estimators. We take a second look at transformers and estimators in the context
    of reusable and parameterizable building blocks. From a 36,000 feet view, an ML
    pipeline is an ordered list of transformers and estimators. This section goes
    from that high-level overview to a more in-depth understanding. However, it is
    crucial that we understand not only how to create, but also how to modify those
    building blocks to use ML pipelines with optimal efficiency.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了机器学习管道的两个主要组件：转换器和估计器。我们再次从可重用和可参数化的构建块的角度审视转换器和估计器。从36000英尺的高度来看，机器学习管道是一个转换器和估计器的有序列表。本节从高级概述深入到更深入的理解。然而，了解如何创建这些构建块以及如何修改它们以使用机器学习管道达到最佳效率是至关重要的。
- en: 'Transformers and estimators are very useful classes for ML modeling. When we
    train an ML model, we get back a *fitted* model, which is akin to a new program
    that we did not code explicitly. This new data-driven program then has one sole
    purpose: taking a properly formatted data set and transforming it by appending
    a prediction column. In this section, we see that transformers and estimators
    not only provide a useful abstraction for ML modeling, but they also provide portability
    through serialization and deserialization. This means that you can train and save
    your ML model and deploy it in another environment.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器和估算器是ML建模中非常有用的类。当我们训练一个ML模型时，我们会得到一个*拟合*的模型，这就像是一个我们没有明确编写的程序。这个新的数据驱动程序只有一个单一的目的：接受一个格式正确的数据集，并通过添加预测列来对其进行转换。在本节中，我们看到转换器和估算器不仅为ML建模提供了一个有用的抽象，而且通过序列化和反序列化提供了可移植性。这意味着你可以训练和保存你的ML模型，并在另一个环境中部署它。
- en: 'To illustrate how a transformer and an estimator are parameterized, we will
    use a transformer and an estimator defined and used in chapter 12:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明转换器和估算器的参数化方式，我们将使用在第12章中定义和使用的转换器和估算器：
- en: '`continuous_assembler`—a `VectorAssembler` transformer that takes five columns
    and creates a Vector column to be used for model training'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`continuous_assembler`—一个`VectorAssembler`转换器，它接受五个列并创建一个用于模型训练的向量列'
- en: '`consinuous_scaler`—a `MinMaxScaler` estimator that scales values contained
    in a Vector column, returning values between `0` and `1` for each element in the
    vectors'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consinuous_scaler`—一个`MinMaxScaler`估算器，它将向量列中包含的值进行缩放，为向量中的每个元素返回介于`0`和`1`之间的值'
- en: For convenience, I include the relevant code in the following listing. We start
    with the transformer and then build on it to introduce the estimator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我在下面的列表中包含了相关的代码。我们首先介绍转换器，然后在此基础上介绍估算器。
- en: Listing 13.1 The `VectorAssembler` and `MinMaxScaler` example
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1 `VectorAssembler`和`MinMaxScaler`示例
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '13.1.1 Data comes in, data comes out: The Transformer'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 数据进来，数据出去：转换器
- en: This section formally introduces the transformer as the first building block
    of an ML pipeline. We introduce the general transformer blueprint and how to access
    and modify its parameterization. This added context on the transformer plays a
    crucial role when we want to run experiments with our ML code or optimize our
    ML models (see section 13.3.3).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本节正式介绍了转换器作为ML管道的第一个构建块。我们介绍了通用的转换器蓝图以及如何访问和修改其参数化。当我们想要运行带有ML代码的实验或优化ML模型时（参见第13.3.3节），这个关于转换器的额外上下文起着至关重要的作用。
- en: 'In our `VectorAssembler` transformer example, introduced in figure 13.1, we
    provide two arguments to the constructor: `inputCols` and `outpulCol`. These arguments
    provide the necessary functionality to create a fully functional `VectorAssembler`
    transformer. This transformer’s sole purpose—through its `transform()` method—is
    to take the values in `inputCols` (assembled values) and return a single column,
    named `outputCol`, that contains a vector of all the assembled values.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍的`VectorAssembler`转换器示例中（如图13.1所示），我们向构造函数提供了两个参数：`inputCols`和`outputCol`。这些参数提供了创建一个完全功能的`VectorAssembler`转换器所需的功能。这个转换器的唯一目的——通过其`transform()`方法——是获取`inputCols`（组装值）中的值，并返回一个名为`outputCol`的单列，其中包含所有组装值的向量。
- en: '![](../Images/13-01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-01.png)'
- en: Figure 13.1 The `continuous_assembler` transformer, along with its Params. The
    transformer uses the `transform()` method to apply a predefined transformation
    to the data frame, passed as input.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 `continuous_assembler`转换器及其Params。该转换器使用`transform()`方法对作为输入传递的数据帧应用预定义的转换。
- en: The parameterization of a transformer is called *Params* (capital P). When instantiating
    a transformers class, just like with any Python class, we pass the parameters
    we want as arguments, making sure to explicitly specify each keyword. Once the
    transformer has been instantiated, PySpark provides us with a set of methods to
    extract and modify Params. The next two sections cover retrieving and modifying
    Params after the transformer’s instantiation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器的参数化称为*Params*（大写的P）。当实例化一个转换器类时，就像使用任何Python类一样，我们传递我们想要的参数作为参数，确保明确指定每个关键字。一旦转换器被实例化，PySpark就为我们提供了一套方法来提取和修改Params。接下来的两个部分将介绍在转换器实例化之后如何检索和修改Params。
- en: 'Peeking at the signature of `VectorAssembler`: Keyword-only arguments'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 查看一下`VectorAssembler`的签名：关键字参数
- en: 'If you look at the signature for `VectorAssembler` (and pretty much every transformer
    and estimator in the `pyspark.ml` modules), you’ll see an asterisk at the beginning
    of the parameters list:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 `VectorAssembler` 的签名（以及 `pyspark.ml` 模块中的几乎所有转换器和估计器的签名），你会在参数列表的开头看到一个星号：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In Python, every parameter after the asterisk (`*`) is called a *keyword-only
    argument*, meaning that we need to mention the keyword. For instance, we couldn’t
    do `VectorAssembler("input_column",` `"output_column")`. For more information,
    refer to PEP (Python Enhancement Proposal) 3102 at [http://mng.bz/4jKV](http://mng.bz/4jKV).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，每个星号（`*`）之后的参数被称为*关键字参数*，这意味着我们需要提及该关键字。例如，我们无法这样做 `VectorAssembler("input_column",
    "output_column")`。更多信息，请参阅 PEP（Python 增强提案）3102，链接为 [http://mng.bz/4jKV](http://mng.bz/4jKV)。
- en: As a fun add-on, Python also supports *positional-only parameters* with the
    slash (`/`) character. See PEP 570 ([http://mng.bz/QWqj](http://mng.bz/QWqj)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项有趣的附加功能，Python 还支持使用斜杠（`/`）字符的*位置参数*。请参阅 PEP 570 ([http://mng.bz/QWqj](http://mng.bz/QWqj))。
- en: 'Peeking under the hood: Getting and explaining the Params'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看内部结构：获取和解释 Params
- en: 'Looking back at figure 13.1, the instantiation of `VectorAssembler` accepted
    three arguments: `inputCols`, `outputCol`, and `handleInvalid`. We also hinted
    that the configuration of a transformer (and estimator, by the same occasion)
    class instance relied on Params, which drove the behavior of the transformers.
    In this section, we explore Params, highlight their similarities and differences
    compared to regular class attributes, and address why those differences matter.
    You might think, “Well, I know how to get attributes out of a Python class, and
    transformers are Python classes.” While that is correct, transformers (and estimators)
    follow a more Java/ Scala-like design, and I recommend not skipping over this
    section. It’s short and useful and will allow you to avoid headaches when working
    with ML pipelines.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾图 13.1，`VectorAssembler` 的实例化接受三个参数：`inputCols`、`outputCol` 和 `handleInvalid`。我们也暗示了转换器（以及估计器，在此同时）类实例的配置依赖于
    Params，这驱动了转换器的行为。在本节中，我们探讨 Params，突出它们与常规类属性相似之处和不同之处，并解释为什么这些差异很重要。你可能认为，“嗯，我知道如何从
    Python 类中获取属性，转换器也是 Python 类。”虽然这是正确的，但转换器（和估计器）遵循更类似 Java/Scala 的设计，我建议不要跳过这一节。它很短，但很有用，这将帮助你避免在处理
    ML 管道时遇到麻烦。
- en: First, let’s do what any Python developer would do and access one of the attributes
    of the transformer directly. In listing 13.2, we see that accessing the `outputCol`
    attribute of `continuous_assembler` does not yield `continuous`, like when we
    passed to the constructor. Instead, we get a reference to an object called a Param
    (class `pyspark.ml.param.Param`), which wraps each of our transformer’s attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们做任何 Python 开发者都会做的事情，直接访问转换器的一个属性。在列表 13.2 中，我们看到访问 `continuous_assembler`
    的 `outputCol` 属性并不会得到 `continuous`，就像我们传递给构造函数时那样。相反，我们得到一个名为 Param 的对象的引用（类 `pyspark.ml.param.Param`），它封装了我们转换器每个属性。
- en: Listing 13.2 Accessing a transformer’s parameters to yield a `Param`
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.2 通过访问转换器的参数以获取 `Param`
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Rather than returning the continuous value passed as an argument to outputCol,
    we get an object called a Param.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们得到的不是传递给 `outputCol` 的连续值，而是一个名为 Param 的对象。
- en: To directly access the value of a specific param, we use a *getter* method,
    which is simply putting the word `get`, followed by the name of our Param in CamelCase.
    In the case of `outputCol`, shown in the next listing, the getter method is called
    `getOutputCol()` (note the capital O).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要直接访问特定参数的值，我们使用*获取器*方法，这很简单，只需在单词 `get` 后面加上我们参数的 CamelCase 名称。在下一个列表中显示的 `outputCol`
    的情况下，获取器方法被称为 `getOutputCol()`（注意大写的 O）。
- en: Listing 13.3 Accessing the value of the `outputCol` Param through `getOutputCol()`
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.3 通过 `getOutputCol()` 访问 `outputCol` Param 的值
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So far, Params seem like they add boilerplate with little benefit. `explainParam()`
    changes this. This method provides documentation about the Param as well as the
    value. This is best explained by an example, and we see the output of explaining
    the `outputCol` Param in listing 13.4.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Params 似乎只是添加了一些无用的样板代码。`explainParam()` 改变了这一点。此方法提供了关于 Param 以及其值的文档。这最好通过一个例子来说明，我们可以在列表
    13.4 中看到解释 `outputCol` Param 的输出。
- en: Tip If you want to see all the Params at once, you can also use the pluralized
    version, `explainParams()`. This method takes no argument and will return a newline-delimited
    string of all the Params.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 如果你想一次性看到所有参数，你也可以使用复数形式，`explainParams()`。此方法不接受任何参数，并将返回包含所有参数的换行分隔字符串。
- en: 'The string output contains the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串输出包含以下内容：
- en: 'The name of the Param: `outputCol`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的名称：`outputCol`
- en: 'A short description of the Param: `output` `column` `name`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的简短描述：`output` `column` `name`
- en: 'The `default` value of the Param: `VectorAssembler_e18a6589d2d5__output`, used
    if we don’t explicitly pass a value ourselves'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的 `default` 值：`VectorAssembler_e18a6589d2d5__output`，如果我们没有明确传递值时使用
- en: 'The `current` value of the Param: `continuous`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的 `current` 值：`continuous`
- en: Listing 13.4 Explaining the `outputCol` Param with `explainParam`
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.4 使用 `explainParam` 解释 `outputCol` 参数
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ This is the name and a short description of the outputCol Param.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是输出列参数的名称和简短描述。
- en: ❷ We defined a value for outputCol.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们为输出列定义了一个值。
- en: In this section, we addressed the relevant information of our transformer’s
    Params. The ideas in this section also apply to estimators (see section 13.1.2).
    In the next section, we stop looking at Params, and we start changing them. Transformers
    will have no secret from us!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了我们的转换器参数的相关信息。本节中的想法也适用于估计器（参见 13.1.2 节）。在下一节中，我们将停止查看参数，并开始更改它们。转换器将不再对我们有任何秘密！
- en: What about the plain `getParam()` method?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，普通的 `getParam()` 方法呢？
- en: Transformers (and estimators) provide the plain `getParam()`. It simply returns
    the Param, just like accessing the `outputCol` did at the beginning of the section.
    I believe this is done so that PySpark transformers can have a consistent API
    with their Java/Scala equivalent.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器（和估计器）提供了简单的 `getParam()` 方法。它简单地返回参数，就像本节开头访问 `outputCol` 一样。我相信这是为了使 PySpark
    转换器能够与其 Java/Scala 等效的 API 保持一致。
- en: Setting params of an instantiated transformer using getters and setters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用获取器和设置器设置实例化转换器的参数
- en: 'In this section, we modify the Params of a transformer. Simple as that! This
    is mainly useful in two scenarios:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们修改了转换器的参数。就这么简单！这主要在两种情况下很有用：
- en: You are building your transformer in the REPL, and you want to experiment with
    different Param-eterizations.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在 REPL 中构建你的转换器，并想尝试不同的参数化。
- en: You are optimizing your ML pipeline Params, like we do in section 13.3.3.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在优化你的机器学习管道参数，就像我们在 13.3.3 节中所做的那样。
- en: Tip Just like the previous section on getting Params, setting Params works the
    same for estimators.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 就像上一节关于获取参数的内容一样，设置参数对于估计器来说也是一样的。
- en: How do we change the Params of a transformer? For every getter, there is a *setter*,
    which is simply putting the word `set`, followed by the name of our Param in CamelCase.
    Unlike getters, setters take the new value as their sole argument. In listing
    13.5, we change the `outputCol` Param to `more_continuous` using the relevant
    setter method. This operation returns the transformed transformer but also makes
    the modification in place, which means that you do not have to assign the result
    of a setter to a variable (see the sidebar at the end of this section for more
    information and how to avoid potential pitfalls).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何更改转换器的参数？对于每个获取器，都有一个 *设置器*，它只是将单词 `set` 放在参数名称之后，使用驼峰命名法。与获取器不同，设置器将新值作为其唯一参数。在列表
    13.5 中，我们使用相关的设置器方法将 `outputCol` 参数更改为 `more_continuous`。此操作返回转换后的转换器，但也会就地进行修改，这意味着你不需要将设置器的结果赋值给变量（有关更多信息以及如何避免潜在陷阱，请参阅本节末尾的侧边栏）。
- en: Listing 13.5 Setting the `outputCol` Param to `more_continuous`
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.5 将 `outputCol` 参数设置为 `more_continuous`
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ While the setOutputCol() method returns a new transformer object, it also
    makes the modification in place, so we don’t have to assign the result to a variable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当 `setOutputCol()` 方法返回一个新的转换器对象时，它也会就地进行修改，所以我们不需要将结果赋值给变量。
- en: 'If you need to change multiple Params as once (e.g., you want to change the
    input and output columns in one fell swoop while experimenting with different
    scenarios), you can use the `setParams()` method. `setParams()` has the same signature
    as the constructor: you just pass the new values as keywords, as shown in the
    next listing.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一次性更改多个参数（例如，在实验不同场景时，你希望一次性更改输入和输出列），你可以使用 `setParams()` 方法。`setParams()`
    方法与构造函数有相同的签名：你只需按关键字传递新值，如下一列表所示。
- en: Listing 13.6 Changing multiple Params at once using `setParams()`
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.6 使用 `setParams()` 一次性更改多个参数
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Params not passed to setParams keep their previous value (set in listing 13.5).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 未传递给setParams的Params保持其先前值（在列表13.5中设置）。
- en: Finally, if you want to return a Param to its default value, you can use the
    `clear()` method. This time, you need to pass the `Param` object. For instance,
    in listing 13.7, we reset the `handleInvalid` Param by using `clear()`. We pass
    the actual Param as an argument, accessed via the attribute slot seen at the beginning
    of the section, `continuous_assembler.handleInvalid`. This will prove useful if
    you have a transformer that has both `inputCol/outputCol` and `inputCols/outputCols`
    as possible Params. PySpark only allows one set to be active at once, so if you
    want to move between one column and multiple columns, you need to `clear()` the
    ones not being used.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想将Param返回到其默认值，你可以使用`clear()`方法。这次，你需要传递`Param`对象。例如，在列表13.7中，我们通过使用`clear()`重置`handleInvalid`
    Param。我们通过属性槽传递实际的Param作为参数，该槽在节的开头可以看到，`continuous_assembler.handleInvalid`。如果你有一个具有`inputCol/outputCol`和`inputCols/outputCols`作为可能Params的转换器，这将非常有用。PySpark一次只允许一个集合处于活动状态，因此如果你想在单列和多列之间切换，你需要`clear()`那些未使用的列。
- en: Listing 13.7 Clearing the current value of the `handleInvalid` Param with `clear()`
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.7 使用`clear()`清除`handleInvalid` Param的当前值
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ handleInvalid returned to its original value, error.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ handleInvalid返回到其原始值，错误。
- en: This is it, folks! In this section, we learned in greater detail the how and
    why of a transformer, as well as how to get, set, and clear its Params. In the
    next section, we apply this useful knowledge to speed through the second building
    block of an ML pipeline, the estimator.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部了，朋友们！在本节中，我们更详细地了解了转换器的如何和为什么，以及如何获取、设置和清除其Params。在下一节中，我们将这些有用的知识应用于快速通过ML管道的第二个构建块，即估计器。
- en: 'Transformers and estimators are passed by reference: The copy() method'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器和估计器是通过引用传递的：`copy()`方法
- en: Thus far, we have been used to a fluent API (see chapter 1), where each data
    frame transformation generates a new data frame. This enables method chaining,
    which makes our data transformation code very readable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经习惯了流畅的API（见第1章），其中每个数据帧转换都会生成一个新的数据帧。这使方法链式化成为可能，这使得我们的数据转换代码非常易于阅读。
- en: 'When working with transformers (and estimators), remember that they are passed
    by reference and that setters modify the object in place. If you assign your transformer
    to a new variable name and then use a setter on either of those variables, it’ll
    modify the Param for both references:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用转换器（和估计器）时，请记住它们是通过引用传递的，并且setter会就地修改对象。如果你将转换器分配给新变量名，然后对这两个变量中的任何一个使用setter，它将修改两个引用的Param：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Both the outputCol of continuous_assembler and new_continuous_assembler were
    modified by the setter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ continuous_assembler和new_continuous_assembler的outputCol都已被setter修改。
- en: 'The solution to this is to `copy()` the transformer and then assign the copy
    to the new variable:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是`copy()`转换器，然后将副本分配给新变量：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ When making a copy, modifications to the Params of copy_continuous_assembler
    don’t impact continuous_assembler.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在复制时，对copy_continuous_assembler的Params的修改不会影响continuous_assembler。
- en: '13.1.2 Data comes in, transformer comes out: The Estimator'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 数据进入，转换器输出：估计器
- en: This section covers the estimator, the second half of the ML pipeline. Just
    like with transformers, understanding how to operate and configure an estimator
    is an invaluable step in creating an efficient ML pipeline. Where a transformer
    transforms an input data frame into an output data frame, an estimator is fitted
    on an input data frame and returns an output transformer. In this section, we
    see that this relationship between transformers and estimators means that they
    are Param-eterized the same way as explained in section 13.1.1\. We focus on estimator
    usage through the `fit()` method (versus `transform()` for the transformer), which
    is really the only notable difference for the end user.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了估计器，ML管道的第二部分。就像转换器一样，了解如何操作和配置估计器是创建高效ML管道的宝贵步骤。转换器将输入数据帧转换为输出数据帧，估计器在输入数据帧上拟合并返回输出转换器。在本节中，我们看到转换器和估计器之间的关系意味着它们与第13.1.1节中解释的相同方式进行参数化。我们专注于通过`fit()`方法（与转换器的`transform()`方法相比）使用估计器，这对于最终用户来说确实是唯一的显著区别。
- en: Where a transformer uses a `transform()` method, applied to a data frame, to
    return a transformed data frame, an estimator uses a `fit()` method, applied to
    a data frame, to return a fully parameterized transformer called a `Model`. This
    distinction enables estimators to configure transformers based on the input data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，转换器使用`transform()`方法，应用于数据帧，以返回一个转换后的数据帧，估计器使用`fit()`方法，应用于数据帧，以返回一个完全参数化的转换器，称为`Model`。这种区别使得估计器可以根据输入数据配置转换器。
- en: 'As an example, the `MinMaxScaler` estimator in figure 13.2 takes four parameters,
    two of which rely on the default value:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图13.2中的`MinMaxScaler`估计器有四个参数，其中两个依赖于默认值：
- en: '`min` and `max`, which are the minimum and maximum values our scaled column
    will take. We keep both at their default of `0.0` and `1.0`, respectively.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min`和`max`，它们是我们缩放列将采取的最小值和最大值。我们分别将它们保持在默认值`0.0`和`1.0`。'
- en: '`inputCols` and `outputCols` are the input and output column, respectively.
    They follow the same conventions as the transformer.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputCols`和`outputCols`分别是输入列和输出列。它们遵循与转换器相同的约定。'
- en: '![](../Images/13-02.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-02.png)'
- en: Figure 13.2 The `MinMaxScaler` estimator, along with its Params. The transformer
    uses the `fit()` method to create and parameterize a `Model` (a subtype of transformer)
    using the data frame passed as an argument.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 `MinMaxScaler`估计器及其参数。转换器使用`fit()`方法创建并参数化一个`Model`（转换器的子类型），使用作为参数传递的数据帧。
- en: 'In order to scale the values between `min` and `max`, we need to extract from
    the input column the minimum value (which I call `E_min`), as well as the maximum
    value (`E_max`). `E_min` is transformed to `0.0`, `E_max` is transformed to `1.0`,
    and any value in between takes a value between `min` and `max`, using the following
    formula (see the exercise at the end of the section for a corner [or edge] case
    when `E_max` and `E_min` are the same):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在`min`和`max`之间缩放值，我们需要从输入列中提取最小值（我称之为`E_min`），以及最大值（`E_max`）。`E_min`转换为`0.0`，`E_max`转换为`1.0`，介于两者之间的任何值都取`min`和`max`之间的值，使用以下公式（参见本节末尾的练习，以一个角落[或边缘]情况为例，当`E_max`和`E_min`相同时）：
- en: '![](../Images/13-01-unnumb-01.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-01-unnumb-01.png)'
- en: 'Because the transformation relies on actual values from the data, we can’t
    use a plain transformer, which expects to “know” everything (through its Param-eterization)
    before it can apply the `transform()` method. In the case of the `MinMaxScaler`,
    we can translate `E_min` and `E_max` as simple operations (`min()` and `max()`
    come from `pyspark .sql.functions`):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换依赖于数据中的实际值，我们无法使用普通的转换器，它期望在应用`transform()`方法之前“知道”一切（通过其参数化）。在`MinMaxScaler`的情况下，我们可以将`E_min`和`E_max`转换为简单的操作（`min()`和`max()`来自`pyspark.sql.functions`）：
- en: '`E_min` `=` `min(inputCol)`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`E_min` `=` `min(inputCol)`'
- en: '`E_max` `=` `max(inputCol)`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`E_max` `=` `max(inputCol)`'
- en: Once these values are computed (during the `fit()` method), PySpark creates,
    Param-eterizes, and returns a transformer/model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些值被计算（在`fit()`方法期间），PySpark就会创建、参数化并返回一个转换器/模型。
- en: 'This `fit()`/`transform()` approach applies for estimators that are far more
    complex than `MinMaxScaler`. Case in point: ML models are actually implemented
    as estimators in Spark. In the next section, we assemble our collection of transformers
    and estimators into a cohesive ML pipeline, complete with machine learning (finally!).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种`fit()`/`transform()`方法适用于比`MinMaxScaler`复杂得多的估计器。以案例为证：ML模型实际上在Spark中作为估计器实现。在下一节中，我们将我们的转换器和估计器集合组装成一个统一的ML管道，其中包括机器学习（终于！）。
- en: Exercise 13.1
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 练习13.1
- en: What happens with the MinMaxScaler when `E_min` `==` `E_max`?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当`E_min` `==` `E_max`时，MinMaxScaler会发生什么？
- en: 13.2 Building a (complete) machine learning pipeline
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 构建一个（完整的）机器学习管道
- en: Now that we are strong in our knowledge of transformers and estimators, and
    can create, modify, and operate them, we are ready to tackle the last element
    of a successful ML pipeline, the `Pipeline` itself. Pipelines build on transformers
    and estimators to make training, evaluating, and optimizing ML models much clearer
    and more explicit. In this section, we build an ML pipeline with the estimators
    we used for our dessert prediction feature preparation program and add the modeling
    step in the mix.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对转换器和估计器的知识已经很强，并且可以创建、修改和操作它们，我们准备应对成功ML管道的最后一个元素，即`Pipeline`本身。管道建立在转换器和估计器的基础上，使训练、评估和优化ML模型变得更加清晰和明确。在本节中，我们使用用于我们的甜点预测特征准备程序的估计器构建一个ML管道，并在其中添加建模步骤。
- en: ML pipelines are estimators. End of lesson.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ML管道是估计器。本课结束。
- en: '...'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: In all seriousness, ML pipelines are implemented through the `Pipeline` class,
    which is a specialized version of the estimator. The `Pipeline` estimator has
    only one Param, called `stages`, which takes a list of transformers and estimators.
    To illustrate a pipeline, how about we create one? Starting from the `food` data
    frame from `code/Ch12/end_of_chapter.py`—which is our data set before we applied
    `Imputer`, `VectorAssembler`, and `MinMaxScaler`—we create a `food_pipeline` in
    the next listing containing our previous estimators and transformers as stages.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 严肃地说，ML管道是通过`Pipeline`类实现的，这是一个估计器的特殊版本。`Pipeline`估计器只有一个参数，称为`stages`，它接受一个转换器和估计器的列表。为了说明管道，我们为什么不创建一个呢？从`code/Ch12/end_of_chapter.py`中的`food`数据帧开始——这是我们在应用`Imputer`、`VectorAssembler`和`MinMaxScaler`之前的数据集——我们在下一列表中创建一个`food_pipeline`，其中包含我们之前的估计器和转换器作为阶段。
- en: Listing 13.8 The `food_pipeline` pipeline, containing three stages
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.8 `food_pipeline`管道，包含三个阶段
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The two estimators and the transformer used in chapter 12 for our feature
    preparation are repeated here for convenience.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 本章用于特征准备的两个估计器和转换器在此处重复列出，以方便使用。
- en: ❷ The food_pipeline pipeline contains three stages, encoded in the stages Param.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `food_pipeline`管道包含三个阶段，编码在`stages`参数中。
- en: In practical terms, since the pipeline is an estimator, it has a `fit()` method
    that generates a `PipelineModel`. Under the hood, the pipeline applies each stage
    in order, calling the appropriate method depending on if the stage is a transformer
    (`transform()`) or an estimator (`fit()`). By wrapping all of our individual stages
    into a pipeline, we only have one method to call, `fit()`, knowing that PySpark
    will do the right thing to yield a `PipelineModel` (figure 13.3). Although PySpark
    calls the result of a fitted pipeline a pipeline model, we have no machine learning
    model as a stage (this is covered in section 13.2.2). With a pipeline in place,
    though, our code becomes more modular and easier to maintain. We can add, remove,
    and change stages, relying on the pipeline definition to know what work will happen.
    I often create multiple iterations of transformers and estimators, trying them
    iteratively (this will also be very useful when we talk about optimizing the model
    in section 13.3.3) until I am satisfied with the results. I can define a few dozen
    transformers and estimators and keep some old definitions just in case; as long
    as my pipeline is clear, I feel confident about how my data gets processed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，由于管道是一个估计器，它有一个`fit()`方法，该方法生成一个`PipelineModel`。在底层，管道按顺序应用每个阶段，根据阶段是转换器（`transform()`）还是估计器（`fit()`）调用适当的方法。通过将我们所有的单个阶段包装到一个管道中，我们只需要调用一个方法，即`fit()`，知道PySpark会做正确的事情以生成一个`PipelineModel`（见图13.3）。尽管PySpark将拟合管道的结果称为管道模型，但我们没有作为阶段的机器学习模型（这将在13.2.2节中介绍）。然而，有了管道，我们的代码变得更加模块化，更容易维护。我们可以添加、删除和更改阶段，依靠管道定义来了解将发生什么工作。我经常创建多个转换器和估计器的迭代版本，尝试它们，直到我对结果满意（这在我们讨论13.3.3节中的模型优化时也会非常有用）。我可以定义几十个转换器和估计器，并保留一些旧的定义以防万一；只要我的管道清晰，我对我的数据处理方式就很有信心。
- en: '![](../Images/13-03.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-03.png)'
- en: Figure 13.3 Our `food_pipeline` illustrated. When calling `fit()` using a data
    frame, that data frame gets passed as an argument to the first stage. Each estimator
    stage gets evaluated (transformers are passed verbatim), and the resulting transformers/models
    form the stages of the `PipelineModel`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 我们的`food_pipeline`展示。当使用数据帧调用`fit()`时，该数据帧作为参数传递给第一个阶段。每个估计器阶段都会被评估（转换器被原样传递），生成的转换器/模型形成`PipelineModel`的阶段。
- en: When using pipelines, remember that the data frame will travel each stage. For
    instance, the `continuous_scaler` stage will have the output of the data frame
    transformed by `continuous_scaler`. For estimator stages, the data frame stays
    identical, as `fit()` does not transform the data frame but returns a `Model`
    instead.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用管道时，请记住数据帧将遍历每个阶段。例如，`continuous_scaler`阶段将输出经过`continuous_scaler`转换的数据帧。对于估计器阶段，数据帧保持不变，因为`fit()`不转换数据帧，而是返回一个`Model`。
- en: 'This section introduced the `Pipeline` object as an estimator with a special
    purpose: running other transformers and estimators. In the next section, we finish
    assembling our data before getting ready to model.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了`Pipeline`对象作为一个具有特殊目的的估计器：运行其他转换器和估计器。在下一节中，我们在准备建模之前完成数据的组装。
- en: 13.2.1 Assembling the final data set with the vector column type
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用向量列类型组装最终数据集
- en: This section explores the vector column type and the `VectorAssembler` in the
    context of model preparation. PySpark requires all the data fed into a machine
    learning estimator, as well as some other estimators like the `MinMaxScaler`,
    to be in a single-vector column. We review the variables inputted in the model
    and how to assemble them seamlessly using the `VectorAssembler`. I finally introduce
    the ML metadata PySpark imputes when assembling columns so that we can easily
    remember what’s where.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了向量列类型以及在模型准备过程中使用的 `VectorAssembler`。PySpark 要求所有输入到机器学习估计器的数据，以及一些其他估计器（如
    `MinMaxScaler`），都应在一个单向量列中。我们回顾了模型中输入的变量以及如何使用 `VectorAssembler` 无缝地组装它们。最后，我介绍了
    PySpark 在组装列时填充的 ML 元数据，这样我们就可以轻松记住每个变量的位置。
- en: 'We already know how to assemble data into a vector: use the `VectorAssembler`.
    Nothing is new here; we can create a `VectorAssembler` stage to assemble all the
    columns we want to be provided to our ML training. In listing 13.9, we assemble
    all of our `BINARY_COLUMNS`, the `_ratio` columns we created in chapter 12, and
    the `continuous_` `scaled` vector column from our pipeline. PySpark will do the
    right thing when assembling vector columns in another vector: rather than getting
    nested vectors, the assembly step will flatten everything into a single, ready-to-use
    vector.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何将数据组装成向量：使用 `VectorAssembler`。这里没有新内容；我们可以创建一个 `VectorAssembler` 阶段来组装我们想要提供给
    ML 训练的所有列。在列表 13.9 中，我们组装了所有的 `BINARY_COLUMNS`，在第 12 章中创建的 `_ratio` 列，以及来自我们管道的
    `continuous_` `scaled` 向量列。当 PySpark 在另一个向量中组装向量列时，它会做正确的事情：而不是得到嵌套的向量，组装步骤会将所有内容展平成一个单一、可使用的向量。
- en: Listing 13.9 Creating the vector assembler and updating stages of the `food_pipeline`
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.9 创建向量组装器并更新 `food_pipeline` 的阶段
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ food_pipeline_model becomes a PipelineModel . . .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `food_pipeline_model` 变成了一个 PipelineModel . . .
- en: ❷ . . . that can then transform() a data frame.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 然后可以将其转换()为一个数据框。
- en: Our data frame is ready for machine learning! We have a number of records, each
    with
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据框已经准备好进行机器学习了！我们有许多记录，每条记录都包含
- en: A *target* (or *label* ) column, `dessert`, containing a binary input (`1.0`
    if the recipe is a dessert, 0.0 otherwise)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *目标*（或 *标签*）列，`dessert`，包含一个二进制输入（如果食谱是甜点则为 `1.0`，否则为 0.0）
- en: A vector of *features*, called `features`, containing all the information we
    want to train our machine learning model with
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为 `features` 的 *特征* 向量，包含我们想要用其训练机器学习模型的所有信息
- en: 'Let’s look at our work, shall we? When we look at the relevant columns in our
    data frame, like in listing 13.10, we see that the `features` column looks very
    different than anything we’ve seen before. We provide 513 distinct features (see
    the `513` at the beginning of the `features` column value) with a large number
    of zeroes. This is called a *sparse* features set. When storing vectors, PySpark
    has two choices for representing vectors:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的工作成果，好吗？当我们查看数据框中的相关列，如列表 13.10 所示，我们会发现 `features` 列看起来与我们之前见过的任何内容都大不相同。我们提供了
    513 个不同的特征（见 `features` 列值开头的 `513`），其中包含大量的零。这被称为 *稀疏* 特征集。当存储向量时，PySpark 有两种选择来表示向量：
- en: A *dense* representation, where a `Vector` in PySpark is simply a NumPy (a high-performance
    multidimensional array library for Python) single-dimensional array object
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种 *密集* 表示，其中 PySpark 中的 `Vector` 简单地是一个 NumPy（Python 的高性能多维数组库）单维数组对象
- en: A *sparse* representation, where a `Vector` in PySpark is an optimized sparse
    vector compatible with the SciPy (a scientific computing library in Python) `scipy.sparse`
    matrix.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种 *稀疏* 表示，其中 PySpark 中的 `Vector` 是一个与 Python 中的 SciPy（Python 中的科学计算库）`scipy.sparse`
    矩阵兼容的优化稀疏向量。
- en: Listing 13.10 Displaying the `features` column with sparse vector representation
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.10 使用稀疏向量表示显示 `features` 列
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Since we have 513 elements in our vector, with a majority of zeroes, PySpark
    uses a sparse vector representation to save some space.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于我们的向量中有 513 个元素，其中大多数是零，PySpark 使用稀疏向量表示来节省一些空间。
- en: 'In practice, you don’t decide if a `Vector` is sparse or dense: PySpark will
    convert between the two as needed. I bring the difference up since they look different
    when you `show()` them within a data frame. We already saw the dense vector representation
    (just like an array) in chapter 12 when we looked at the correlation matrix. To
    illustrate a sparse vector with less that 513 elements, I wrote the same sample
    vector twice, using the two different notations. A sparse vector is a triple containing'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你不需要决定一个 `Vector` 是稀疏的还是密集的：PySpark 将根据需要在这两者之间转换。我提出这个差异是因为当你在数据框中 `show()`
    时，它们看起来不同。我们在第 12 章查看相关矩阵时已经看到了密集向量表示（就像一个数组）。为了说明具有不到 513 个元素的稀疏向量，我使用了相同的样本向量两次，使用了两种不同的表示法。一个稀疏向量是一个包含
- en: The length of a vector
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的长度
- en: An array of positions where the elements are nonzero
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元素非零的位置数组
- en: An array of nonzero values
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非零值数组
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip In case you need them, `pyspark.sql.linalg.Vectors` has functions and methods
    for creating your vectors from scratch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你需要，`pyspark.sql.linalg.Vectors` 有用于从头创建向量的函数和方法。
- en: Now that everything is in a vector, how do we remember what’s where? In chapter
    6, I mentioned briefly that PySpark allows for a metadata dictionary to be attached
    to a column, and that this metadata is used when using PySpark’s machine learning
    capabilities. Well, now’s the time! Let’s look at that metadata.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都变成了向量，我们如何记住它们在哪里？在第 6 章中，我简要地提到 PySpark 允许将元数据字典附加到列上，并且当使用 PySpark 的机器学习功能时，会使用这个元数据。现在是时候了！让我们看看那个元数据。
- en: Listing 13.11 Getting PySpark to unfold the metadata
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.11 让 PySpark 展开元数据
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The column schema for an assembled vector will keep track of the features
    making its composition under the metadata attribute.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 组装向量的列模式将跟踪其组成的特征，这些特征在元数据属性下。
- en: ❷ For scaled variables, since they originate from a VectorAssembler, PySpark
    gives them a generic name, but you can track their name from the original vector
    column (here continuous_assembled) as needed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于缩放变量，由于它们来自 VectorAssembler，PySpark 给它们一个通用名称，但你可以根据需要从原始向量列（此处为 continuous_assembled）跟踪它们的名称。
- en: This section covered the assembly into a final feature vector, the last stage
    before sending our data for training. We revisited and explored the `Vector` data
    structure in greater detail, interpreting its dense and sparse representation.
    Finally, we used the `VectorAssembler` transformer to combine all our features,
    including those already in a `Vector`, and presented the metadata contained in
    the `features` vector. Ready to model?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了将最终特征向量组装起来，这是在将数据发送进行训练之前的最后一个阶段。我们更详细地回顾并探讨了 `Vector` 数据结构，解释了它的密集和稀疏表示。最后，我们使用了
    `VectorAssembler` 转换器来组合所有我们的特征，包括那些已经在 `Vector` 中的特征，并展示了包含在 `features` 向量中的元数据。准备好建模了吗？
- en: 13.2.2 Training an ML model using a LogisticRegression classifier
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 使用逻辑回归分类器训练 ML 模型
- en: This section covers machine learning in PySpark. OK, I might sell it a little
    too much. This section covers *the addition of an ML model stage into a pipeline*.
    In PySpark, training an ML model is nothing more than adding a stage in our ML
    pipeline. Still, since we don’t do things just because, we take the time to review
    our algorithm selection. While it’s easy to take every algorithm available in
    PySpark and try them every time, each one has properties that make it more suitable
    for certain types of problems. Often, the business problem you are trying to solve
    will provide hints about what properties your model should have. Do you want an
    easily explainable model, one that is robust to outliers (data points that are
    out of the normal expected range), or do you chase pure model accuracy? In this
    section, we take our original ask—is this recipe a dessert or not?—and select
    a first model type to integrate into our ML pipeline.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 PySpark 中的机器学习。好吧，我可能有点夸大其词了。本节涵盖了*将 ML 模型阶段添加到管道中*。在 PySpark 中，训练 ML
    模型不过是向我们的 ML 管道中添加一个阶段。尽管如此，我们不会无端地做事，所以我们花时间来审查我们的算法选择。虽然很容易在 PySpark 中使用每个可用的算法并尝试它们，但每个算法都有使其更适合某些类型问题的特性。通常，你试图解决的商业问题会提供关于你的模型应具有哪些特性的提示。你想要一个易于解释的模型，一个对异常值（超出正常预期范围的观测数据点）具有鲁棒性的模型，还是你追求纯粹的模型精度？在本节中，我们选择了我们的原始要求——这个食谱是甜点还是不是？——并选择了一个首先集成到我们的
    ML 管道中的模型类型。
- en: Because our target is binary (`0.0` or `1.0`), we restrict ourselves to a *classification
    algorithm*. Like the name suggests, classification algorithms are made for predicting
    a finite series of outcomes. If your target has a relatively small number of distinct
    values and there is no specific order between them, you are facing a classification
    problem. On the flip side, should we want to predict the number of calories in
    a recipe,[¹](#pgfId-1016932) we would use a *regression algorithm*, which can
    predict a target taking any numerical value.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标是二进制的（`0.0`或`1.0`），我们限制自己使用**分类算法**。正如其名所示，分类算法是为了预测一系列有限的结果而设计的。如果你的目标有相对较少的不同值，并且它们之间没有特定的顺序，那么你面临的是一个分类问题。另一方面，如果我们想要预测食谱中的卡路里数量，[¹](#pgfId-1016932)我们将使用**回归算法**，它可以预测任何数值的目标。
- en: The *logistic regression* algorithm, despite its name, is a classification algorithm
    that belongs to the family of generalized linear models. This family of models
    is well understood and quite powerful, yet easier to explain than other models,
    such as decision trees and neural networks. Despite its simplicity, logistic regression
    is omnipresent in a classification setting. The most famous example is the credit
    score, which to this day is powered by logistic regression models. While we didn’t
    really need explainability for our specific use case here, looking at the biggest
    drivers for our model (section 13.4) informs us about the most impactful inputs
    of our prediction.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名为逻辑回归，但该算法实际上是一种属于广义线性模型族的分类算法。这个模型族被广泛理解，并且非常强大，而且比其他模型（如决策树和神经网络）更容易解释。尽管逻辑回归很简单，但在分类设置中无处不在。最著名的例子是信用评分，至今仍由逻辑回归模型驱动。虽然我们在这里的具体用例中并不真正需要可解释性，但查看我们模型的最大驱动因素（第13.4节）可以让我们了解预测中最有影响力的输入。
- en: Note Logistic regression is not without its faults. Linear models are less flexible
    than other families of models. They also require the data to be scaled, meaning
    that the range of each feature should be consistent. In our case, all of our features
    are either binary or scaled between `0.0` and `1.0`, so this is not a problem.
    For an ML-centric approach to model selection, see the sidebar at the beginning
    of chapter 12 for some excellent references.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：逻辑回归并非没有缺陷。线性模型比其他模型族更缺乏灵活性。它们还要求数据进行缩放，这意味着每个特征的范围应该一致。在我们的案例中，所有特征要么是二进制，要么在`0.0`和`1.0`之间缩放，所以这不是问题。关于以机器学习为中心的模型选择方法，请参阅第12章开头的侧边栏，其中有一些优秀的参考文献。
- en: 'Before integrating our logistic regression into our pipeline, we need to create
    the estimator. This estimator is called `LogisticRegression` and comes from the
    `pyspark.ml` `.classification` module. The API documentation page for the `LogisticRegression`
    (available in iPython/Jupyter via `LogisticRegression?` or online at [http://mng.bz/
    XWr6](http://mng.bz/XWr6)) lists 21 Params we can set. To get things started,
    we use as many default settings as we can, focusing on making the plumbing work.
    We revisit some of those Params when we talk about hyper-parameter optimization
    in section 13.3.3\. The only three Params we set are the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的逻辑回归集成到管道之前，我们需要创建估计器。这个估计器被称为`LogisticRegression`，来自`pyspark.ml`的`.classification`模块。`LogisticRegression`的API文档页面（在iPython/Jupyter中通过`LogisticRegression?`或在线[http://mng.bz/XWr6](http://mng.bz/XWr6)）列出了我们可以设置的21个参数。为了开始，我们尽可能多地使用默认设置，专注于使管道工作。当我们讨论第13.3.3节中的超参数优化时，我们会重新访问一些这些参数。我们设置的唯一三个参数如下：
- en: '`featuresCol`: the column containing our features vector'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featuresCol`：包含我们的特征向量的列'
- en: '`labelCol`: the column containing our label (or target)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelCol`：包含我们标签（或目标）的列'
- en: '`predictionCol`: the column that will contain the predictions of our model'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictionCol`：将包含我们模型预测的列'
- en: Listing 13.12 Adding a `LogisticRegression` estimator to our pipeline
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.12 向我们的管道添加`LogisticRegression`估计器
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Up next, we `fit()` our pipeline, as shown in listing 13.13\. Before doing
    so, we need to split our data set into two portions using `randomSplit()`: one
    for *training*, which we feed to our pipeline, and one for *testing*, which is
    what we use to evaluate our model fit. This allows us to have some confidence
    about our model’s ability to generalize over data that it has never seen before.
    Think of the training set as the study material and the testing set as the exam:
    if you give the exam as part of the study material, the grades are going to be
    much higher, but that does not accurately reflect the students’ performance.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`fit()`我们的管道，如列表13.13所示。在这样做之前，我们需要使用`randomSplit()`将我们的数据集分成两部分：一部分用于*训练*，我们将它喂给我们的管道；另一部分用于*测试*，这是我们用来评估模型拟合度的。这使我们能够对我们的模型在之前未见过的数据上泛化的能力有信心。将训练集视为学习材料，将测试集视为考试：如果你将考试作为学习材料的一部分，分数将会很高，但这并不能准确反映学生的表现。
- en: Finally, before fitting our pipeline, we `cache()` the training data frame.
    As you’ll remember from chapter 11, we do this because machine learning uses the
    data frame repeatedly, so caching in memory provides an increase in speed if your
    cluster has enough memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在拟合我们的管道之前，我们`cache()`训练数据框。正如你从第11章所记得的，我们这样做是因为机器学习会重复使用数据框，所以如果您的集群有足够的内存，内存中的缓存可以提供速度上的提升。
- en: Warning Although PySpark will use the same seed, which should guarantee that
    the split will be consistent across runs, there are some cases where PySpark will
    break that consistency. If you want to be 100% certain about your splits, split
    your data frame, write each one to disk, and then read them from the disk location.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：尽管PySpark将使用相同的种子，这应该可以保证在运行之间保持一致性，但在某些情况下，PySpark可能会打破这种一致性。如果你想100%确定你的分割，请分割数据框，将每个数据框写入磁盘，然后从磁盘位置读取它们。
- en: Listing 13.13 Splitting our data frame for training and testing
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.13 分割我们的数据框以进行训练和测试
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ randomSplit() takes a list of partitions, each containing the fraction of
    the data set. The second attribute is a seed for the random number generator.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `randomSplit()`接受一个分区列表，每个分区包含数据集的一部分。第二个属性是随机数生成器的种子。
- en: ❷ This time, we fit() on train and transform() on test rather than using the
    same data frame for both operations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这次，我们在训练集上`fit()`，在测试集上`transform()`，而不是使用相同的数据框进行这两个操作。
- en: 'With a model now trained and a prediction made on a testing set, we’re ready
    for the last stage of our model: evaluating and dissecting our model.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个模型，并在测试集上做出了预测，我们准备好进入我们模型的最后阶段：评估和剖析我们的模型。
- en: The logistic regression in a nutshell
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，逻辑回归
- en: Feel free to skip this sidebar if math is not your cup of tea.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数学不是你的强项，你可以自由地跳过这个侧边栏。
- en: 'It’s easier to think about the logistic regression if we understand the linear
    regression model first. In school, you probably have learned about the simple
    one-variable regression displayed. In this case, `y` is the dependent variable/target,
    `x` is the dependent variable/feature, and m is `x`’s `coefficient,` `while` `b`
    is the *intercept* (or the value of the coefficient is zero):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们先理解线性回归模型，那么思考逻辑回归会更容易。在学校，你可能已经学过关于简单一元回归的展示。在这种情况下，`y`是因变量/目标，`x`是因变量/特征，而`m`是`x`的`系数`，而`b`是*截距*（或者系数的值是零）：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A linear regression takes this simple formula and applies it to multiple features.
    In other words, `x` and `m` become vectors of value. If we use an index-based
    notation, it would look like this (some statistics textbooks might use a different
    but equivalent notation):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归将这个简单公式应用到多个特征上。换句话说，`x`和`m`变成了值的向量。如果我们使用基于索引的表示法，它看起来会是这样（一些统计学教科书可能会使用不同但等效的表示法）：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we have *n* features and coefficients. This linear regression formula
    is called the *linear component* and is usually written `Xβ` (x for the observations,
    β [beta] for the coefficients vector). A linear regression’s prediction can span
    any number, from negative infinity to infinity—there are no boundaries to the
    formula.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有*n*个特征和系数。这个线性回归公式被称为*线性组件*，通常写作`Xβ`（x代表观测值，β[beta]代表系数向量）。线性回归的预测可以跨越任何数值，从负无穷大到正无穷大——公式没有界限。
- en: How do we get a classification model out of this? Behold, the logistic regression!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从这个模型中得到一个分类模型呢？看吧，这就是逻辑回归！
- en: 'The logistic regression takes its name from the *logit* transformation. The
    logit transformation takes our linear component `X`β and yields a function that
    is between zero and one. The formula is the expanded form of the logistic function.
    Note the location of the linear component. It looks like an arbitrary choice of
    function, but there is a lot of theory behind it, and this form is very convenient:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的名称来源于*对数*转换。对数转换将我们的线性组件`X`β转换为一个介于零到一之间的函数。该公式是对数函数的展开形式。注意线性组件的位置。它看起来像是一个任意选择的功能，但背后有许多理论，并且这种形式非常方便：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `y` of the logistic function will return, for any value of `Xβ`, a number
    from zero to one. For turning this into a binary feature, we apply a simple threshold:
    `1` `if` `y` `>/=` `0.5,` `else` `0`. If you want your model to be more or less
    sensitive, you can change this threshold. If you are curious about the raw `y`
    result of logistic regression, see the `rawPrediction` column of your prediction
    data set: you’ll get a vector containing [`Xβ`, `-Xβ`]. The `probability` column
    will contain the `y` as defined in the logit formula for both values in `rawPrediction`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数的`y`对于任何`Xβ`的值将返回一个介于零到一之间的数字。为了将其转换为二元特征，我们应用一个简单的阈值：如果`y``>=` `0.5`，则返回`1`，否则返回`0`。如果您想使模型更敏感或更不敏感，您可以更改此阈值。如果您对逻辑回归的原始`y`结果感兴趣，请查看您的预测数据集中的`rawPrediction`列：您将得到一个包含`[Xβ,
    -Xβ]`的向量。`probability`列将包含`rawPrediction`中定义的`y`，这是根据对数公式对两个值：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 13.3 Evaluating and optimizing our model
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 评估和优化我们的模型
- en: 'Modeling is done. How do we know how good of a job we did? In this section,
    we perform a crucial step of data science: reviewing our model results and tuning
    their implementation. Both steps are crucial in making sure we are producing the
    best model we can with the input we’re provided, maximizing the odds that our
    model will be useful.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建已完成。我们如何知道我们做得有多好？在本节中，我们执行数据科学的关键步骤：审查我们的模型结果并调整其实现。这两个步骤对于确保我们使用提供的输入产生最佳模型至关重要，最大限度地提高模型有用的可能性。
- en: PySpark provides a clear API that dovetails with the ML pipeline abstraction
    (how fortunate!). We start by reviewing out first naive model performance through
    a tailored evaluator object that provides relevant metrics for our binary classifier.
    We then try to improve on our model’s accuracy by turning some dials (called *hyperparameters*)
    through a process called cross-validation. By the end of this section, you’ll
    have a reproducible blueprint for evaluating and optimizing models.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark提供了一个清晰的API，与ML管道抽象（多么幸运！）相匹配。我们首先通过一个定制的评估器对象来审查我们的第一个朴素模型性能，该对象为我们提供了二元分类器的相关指标。然后，我们通过交叉验证过程调整一些旋钮（称为*超参数*）来尝试提高模型准确性。到本节结束时，您将有一个可重复使用的评估和优化模型的蓝图。
- en: '13.3.1 Assessing model accuracy: Confusion matrix and evaluator object'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 评估模型准确性：混淆矩阵和评估器对象
- en: 'This section covers two popular metrics when evaluating a binary classification
    algorithm. At the core, we want to accurately predict our label (dessert or not?)
    column. There are multiple ways to slice and dice our model’s results; for instance,
    is predicting “dessert” on a liver mousse worse than predicting “not dessert”
    on a tiramisu? Selecting and optimizing for the appropriate metric is crucial
    in yielding an impactful model. We focus on two different ways to review our model
    results:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了评估二元分类算法时常用的两个指标。核心目标是准确预测我们的标签（甜点或不是？）列。有多种方式来切割和剖析我们的模型结果；例如，预测肝慕斯为“甜点”比预测提拉米苏为“非甜点”更糟糕吗？选择和优化适当的指标对于产生有影响力的模型至关重要。我们关注两种不同的方式来审查我们的模型结果：
- en: The *confusion matrix*, which gives us a 2 × 2 matrix of predictions versus
    labels and makes it easy to get metrics like precision (how good are we at identifying
    desserts?) and recall (how good are we at identifying not desserts?)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混淆矩阵*，它提供了一个2×2的预测与标签矩阵，使我们能够轻松获取诸如精确度（我们在识别甜点方面做得有多好？）和召回率（我们在识别非甜点方面做得有多好？）等指标。'
- en: The *receiver operating characteristic curve* (or ROC curve), which shows the
    diagnostic ability of our model as we change its prediction threshold (more on
    this later)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*受试者工作特征曲线*（或ROC曲线），它显示了随着我们改变其预测阈值时模型诊断能力的变化（关于这一点稍后讨论）'
- en: 'Confusion matrix: A simple way to review classification results'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵：一种简单的方法来审查分类结果
- en: In this section, we create a confusion matrix of our results versus the true
    label to assess the ability of our model to accurately predict desserts. Confusion
    matrices are very simple formats for presenting classification results and therefore
    are very popular when communicating results. On the flip side, they do not provide
    any guidance on the actual performance of the model, which is why we usually combine
    them with a set of metrics.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了一个混淆矩阵，用于比较我们的结果与真实标签，以评估我们的模型准确预测甜点的能力。混淆矩阵是呈现分类结果的一种非常简单的格式，因此在交流结果时非常受欢迎。另一方面，它们不提供任何关于模型实际性能的指导，这就是为什么我们通常将它们与一系列指标结合使用。
- en: 'The easiest way to picture a confusion matrix for our dessert classification
    model is to make a 2 × 2 table where the rows are the label (true) results and
    columns are the predicted values, just like in figure 13.4\. This also gives us
    four measures that are useful for creating performance metrics:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们的甜点分类模型的混淆矩阵最简单的方法是制作一个 2 × 2 的表格，其中行是标签（真实）结果，列是预测值，就像图 13.4 一样。这也给我们提供了四个用于创建性能指标的有用度量：
- en: '*True negative (TN)*—Both the label and the prediction are zero. We accurately
    identified a nondessert.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阴性 (TN)*—标签和预测都是零。我们准确地识别了一个非甜点。'
- en: '*True positive (TP)*—Both the label and the prediction are one. We accurately
    identified a dessert.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阳性 (TP)*—标签和预测都是一。我们准确地识别了一个甜点。'
- en: '*False positive (FP)*—We predicted dessert (1) when the dish was not a dessert
    (0). This is also called a *type I error*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阳性 (FP)*—我们预测甜点（1）当菜肴实际上不是甜点（0）。这也被称为*第一类错误*。'
- en: '*False negative (FN)*—We predicted nondessert (0) when the dish was a dessert
    (1). This is also called a *type II error*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阴性 (FN)*—我们预测非甜点（0）当菜肴实际上是甜点（1）。这也被称为*第二类错误*。'
- en: '![](../Images/13-04.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-04.png)'
- en: Figure 13.4 A visual depiction of precision and recall, as seen in a confusion
    matrix. The precision of a model measures how many of the model’s predictions
    (prediction = 1) are legitimate (label = 1). The recall of a model measures how
    many of the true positives (label = 1) are caught by the model (prediction = 1).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 混淆矩阵中精密度和召回率的视觉表示。一个模型的精确度衡量模型预测（预测 = 1）中有多少是合法的（标签 = 1）。一个模型的召回率衡量有多少真实阳性（标签
    = 1）被模型捕获（预测 = 1）。
- en: From those four measures, we can craft a multitude of metrics to evaluate our
    model. The two most popular are *precision* and *recall*, as depicted in figure
    13.4\. Before getting into performance metrics, let’s create our confusion matrix.
    PySpark only provides a confusion matrix through the legacy `pyspark.mllib` module,
    which is now in maintenance mode. Truth be told, for such a simple operation,
    I’d rather do it by hand, like in listing 13.14\. For this, we group by the label
    (`dessert`) and then `pivot()` on the prediction column, using `count()` as the
    cells’ value. `pivot()` takes every value from the column passed as an argument
    and creates a column out of it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从这四个度量中，我们可以制定出多种指标来评估我们的模型。最流行的是*精确度*和*召回率*，如图 13.4 所示。在深入了解性能指标之前，让我们创建我们的混淆矩阵。PySpark
    只通过传统的 `pyspark.mllib` 模块提供混淆矩阵，该模块现在处于维护模式。说实话，对于这样简单的操作，我宁愿手动完成，就像列表 13.14 一样。为此，我们按标签（`dessert`）分组，然后在预测列上使用
    `pivot()`，使用 `count()` 作为单元格的值。`pivot()` 从作为参数传递的列中的每个值创建一个列。
- en: Listing 13.14 Creating a confusion matrix for our model using `pivot()`
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.14 使用 `pivot()` 为我们的模型创建混淆矩阵
- en: '[PRE21]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Prediction values are the columns . . .
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预测值是列 ...
- en: ❷ . . . and label values (dessert) are the rows—our confusion matrix, by hand.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ... 标签值（甜点）是行——我们手工制作的混淆矩阵。
- en: 'Note The confusion matrix shows that our data set has a lot more non-desserts
    than desserts. In the classification world this is called a *class imbalanced
    data set* and is often pretty undesirable for model training. In this specific
    case, the class imbalance is manageable: for a thorough review of the topic, check
    out *Learning from imbalanced data: open challenges and future directions* by
    Bartosz Krawczyk (2016, [http://mng.bz/W7Yd](http://mng.bz/W7Yd)).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：混淆矩阵显示我们的数据集中非甜点比甜点多得多。在分类世界中，这被称为*类别不平衡数据集*，通常对模型训练来说并不理想。在这种情况下，类别不平衡是可管理的：对于该主题的彻底回顾，请参阅
    Bartosz Krawczyk 的 *从不平衡数据中学习：开放挑战和未来方向*（2016年，[http://mng.bz/W7Yd](http://mng.bz/W7Yd)）。
- en: Now that the confusion matrix is under our belt, let’s tackle the precision
    and recall. Before Spark 3.1, you needed to rely (again) on the legacy RDD-based
    MLlib (`pyspark` `.mllib.evaluation.MulticlassMetrics`) to get precision and recall.
    In Spark 3.1, we now have access to a new `LogisticRegressionSummary` object that
    avoids the trip to the RDD world. Because it is a recent addition, I provide the
    code for both, focusing on the future-proof data frame approach.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经掌握了混淆矩阵，那么让我们来处理精确率和召回率。在 Spark 3.1 之前，你需要（再次）依赖传统的基于 RDD 的 MLlib (`pyspark`
    `.mllib.evaluation.MulticlassMetrics`) 来获取精确率和召回率。在 Spark 3.1 中，我们现在可以访问一个新的 `LogisticRegressionSummary`
    对象，它避免了访问 RDD 世界。由于这是一个新添加的功能，我提供了两种方法的代码，重点关注未来证明的数据帧方法。
- en: For the data frame approach (Spark 3.1+), we need to first extract our fitted
    model from the pipeline model. For this, we can use the `stages` attribute of
    `pipeline_ food_model` and access just the last item. From that model, called
    `lr_model` in listing 13.15, we call `evaluate()` on the `results` data set. `evaluate()`
    will error out any prediction columns that exist, so I simply give the relevant
    ones (`dessert`, `features`) to it. It’s a small price to pay to avoid computing
    the metrics by hand. Note that PySpark does not know which label we consider positive
    and negative. Because of this, the precision and recall are accessible through
    `precisionByLabel` and `recallByLabel`, which both return lists of precision/recall
    for each label in order.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据帧方法（Spark 3.1+），我们首先需要从管道模型中提取我们的拟合模型。为此，我们可以使用 `pipeline_ food_model` 的
    `stages` 属性并访问最后一个项目。从该模型，在列表 13.15 中称为 `lr_model`，我们在 `results` 数据集上调用 `evaluate()`。`evaluate()`
    将错误地输出任何存在的预测列，所以我只是给它提供了相关的列（`dessert`，`features`）。这是避免手动计算指标的小小代价。请注意，PySpark
    也不知道我们考虑哪个标签是正标签和负标签。因此，精确率和召回率可以通过 `precisionByLabel` 和 `recallByLabel` 访问，这两个都返回按顺序排列的每个标签的精确率/召回率列表。
- en: Listing 13.15 Computing the precision and recall (Spark 3.1+)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.15 使用 Spark 3.1+ 计算精确率和召回率
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The last stage of the pipeline model is the fitted ML model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 管道模型的最后阶段是拟合的 ML 模型。
- en: ❷ Since PySpark has no notion of which label is the positive one, it’ll compute
    precision and recall for both and put them in a list. We pick the second one (dessert
    == 1.0).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于 PySpark 没有关于哪个标签是正标签的概念，它将计算两个标签的精确率和召回率并将它们放入一个列表中。我们选择第二个（甜点 == 1.0）。
- en: '❸ Over 90% precision and recall on the first try: pop the champagne (is champagne
    a dessert?)!'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第一次尝试就达到了超过 90% 的精确率和召回率：打开香槟（香槟是甜点吗）！
- en: For those using the legacy RDD-based MLLib, the process is quite similar, but
    we first need to move our data to an RDD containing pairs of `(prediction,` `label)`.
    Then, we need to pass the RDD to the `pyspark.ml.evaluation.MulticlassMetrics`
    and extract the relevant metric. This time, `precision()` and `recall()` are *methods*,
    so we need to pass the positive label (`1.0`) as an argument.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用传统基于 RDD 的 MLLib 的用户，过程相当类似，但我们首先需要将我们的数据移动到一个包含 `(prediction,` `label)`
    对的 RDD 中。然后，我们需要将 RDD 传递给 `pyspark.ml.evaluation.MulticlassMetrics` 并提取相关指标。这次，`precision()`
    和 `recall()` 是 *方法*，因此我们需要将正标签 (`1.0`) 作为参数传递。
- en: Listing 13.16 Computing precision and recall through an RDD-based API
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.16 通过 RDD-based API 计算精确率和召回率
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In this section, we covered useful metrics for evaluating our binary classification
    model. The next section introduces another useful perspective when evaluating
    a binary classifier: the ROC curve.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了评估我们的二分类模型的有用指标。下一节将介绍评估二分类器时另一个有用的视角：ROC 曲线。
- en: '13.3.2 True positives vs. false positives: The ROC curve'
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 真阳性 vs. 假阳性：ROC 曲线
- en: 'This section covers another common metric used when evaluating binary classification
    models. The *receiver operating characteristic curve*, commonly known as a ROC
    (pronounced like “rock”) curve, provides a visual cue of the performance of the
    model. We also discuss the main metric that leverages the ROC curve: the area
    under the ROC curve. This alternate way of showcasing performance models provides
    hints on how we can optimize the model, discriminating power by tweaking the decision
    boundary. This will prove useful when we want to optimize our model for our use
    case (see section 13.3.3).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在评估二分类模型时使用的另一个常用指标。*接收者操作特征曲线*，通常称为 ROC（发音为“rock”）曲线，提供了模型性能的视觉提示。我们还讨论了利用
    ROC 曲线的主要指标：ROC 曲线下面的面积。这种展示性能模型的不同方式为我们提供了如何通过调整决策边界来优化模型、提高判别能力的线索。当我们想要针对我们的用例优化模型时，这将非常有用（参见
    13.3.3 节）。
- en: The logistic regression (this applies to most classification models as well)
    predicts a value between 0 and 1 (see the “The Logistic Regression in a Nutshell”
    sidebar for more information) where PySpark stores a column named `probability`.
    By default, any probability equal or over `0.5` will yield a prediction of `1.0`,
    while any probability under `0.5` will yield a `0.0`. It turns out that we can
    change that threshold by, you guessed it, changing the `threshold` Param of the
    `LogisticRegression` object.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归（这也适用于大多数分类模型）预测一个介于 0 和 1 之间的值（有关更多信息，请参阅“逻辑回归概述”侧边栏），PySpark 存储一个名为 `probability`
    的列。默认情况下，任何等于或超过 `0.5` 的概率将产生预测 `1.0`，而任何低于 `0.5` 的概率将产生 `0.0`。实际上，我们可以通过更改 `LogisticRegression`
    对象的 `threshold` 参数来更改该阈值。
- en: Because this threshold is such an important concept in discriminating the power
    of a classification model, PySpark stores the raw prediction (for those math-inclined
    folks, the linear component `x`β as seen in “The Logistic Regression in a Nutshell”
    sidebar) in a `rawPrediction` column. By using this raw prediction and changing
    the threshold without retraining the model, we can have a sense of the performance
    of the model according to a different sensitivity. Because ROC curves are much
    better explained visually, I skip a few steps and present the result in figure
    13.5, along with some relevant elements. The code to generate this chart follows
    later in this section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个阈值在区分分类模型的判别能力方面是一个如此重要的概念，PySpark 将原始预测（对于那些数学爱好者，请参阅“逻辑回归概述”侧边栏中看到的线性组件
    `x`β）存储在一个 `rawPrediction` 列中。通过使用这个原始预测并更改阈值，而无需重新训练模型，我们可以根据不同的灵敏度感知模型的性能。由于
    ROC 曲线在视觉上解释得更好，我跳过了一些步骤，并在图 13.5 中展示了结果，以及一些相关元素。生成此图表的代码将在本节稍后提供。
- en: In a nutshell, the ROC curve maps false positive rates (FPRs) to true positive
    rates (TPRs). A perfectly accurate model will have a TPR of 100% and an FPR of
    0%, meaning that every prediction is on point. We therefore want the curve shown
    in figure 13.5 to hit the top-left corner as much as possible. A less-precise
    model will get close to the dotted line, which echoes what we consider a *random*
    model (FPR = TPR).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，ROC 曲线将假阳性率（FPRs）映射到真阳性率（TPRs）。一个完全准确的模型将有一个 100% 的 TPR 和 0% 的 FPR，这意味着每个预测都是准确的。因此，我们希望图
    13.5 中显示的曲线尽可能多地触及右上角。一个不太精确的模型将接近虚线，这与我们认为的 *随机* 模型（FPR = TPR）相呼应。
- en: '![](../Images/13-05.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-05.png)'
- en: Figure 13.5 The ROC curve for our model. We want the solid curve to hit the
    top-left corner as much as possible.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 我们模型的 ROC 曲线。我们希望实线尽可能多地触及右上角。
- en: Okay, we saw our ROC curve. How do we create one?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们看到了我们的 ROC 曲线。我们如何创建一个？
- en: The ROC curve is obtained through the `BinaryClassificationEvaluator` object.
    In listing 13.17, we instantiate said object, asking explicitly for the `areaUnderROC`
    metric, which yields the ROC curve. Our evaluator takes the raw prediction as
    an input. The evaluator generates a single measure, which is the area under the
    ROC curve, a number between zero and one (higher is better). We’re doing great,
    but it would be nice to know what this number means.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线是通过 `BinaryClassificationEvaluator` 对象获得的。在列表 13.17 中，我们实例化该对象，明确要求 `areaUnderROC`
    指标，这将产生 ROC 曲线。我们的评估器以原始预测作为输入。评估器生成一个单一度量，即 ROC 曲线下的面积，一个介于零和一之间的数字（越高越好）。我们做得很好，但想知道这个数字的含义会更好。
- en: Tip The other option we can use for the metric is `areaUnderPR`, which will
    give us the area under the precision-recall curve. This is useful when the classes
    are very imbalanced or when dealing with rare events.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们可以用于该指标的另一个选项是 `areaUnderPR`，这将给出精确率-召回率曲线下的面积。这在类别非常不平衡或处理罕见事件时很有用。
- en: Listing 13.17 Creating and evaluating a `BinaryClassificationEvaluator` object
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.17 创建和评估一个 `BinaryClassificationEvaluator` 对象
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ We pass our label (or target) and the rawPrediction column generated by our
    model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们传递我们的标签（或目标）以及由我们的模型生成的原始预测列。
- en: 'As seen earlier in this section, a perfect model would hit the top-left corner.
    To have a sense numerically of how close we are to this, the area under the ROC
    curve is used: it’s the ratio of the chart that is under our ROC curve. An AUC
    (area under the curve) score of 0.9929 means that 99.29% of the area of the chart
    is under the ROC curve.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节前面所见，一个完美的模型将触及右上角。为了在数值上了解我们离这个目标有多近，我们使用 ROC 曲线下的面积：它是位于我们 ROC 曲线下的图表面积的比例。AUC（曲线下的面积）得分为
    0.9929 意味着图表的 99.29% 面积位于 ROC 曲线下。
- en: Listing 13.18 Using `matplotlib` to display the ROC curve
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.18 使用`matplotlib`显示ROC曲线
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For the first run, our model did amazingly. This does not mean that our job
    is over: initial model fitting is the first step in having a production-ready
    model. Our code looks a little artisanal and could use a cup or two of robustness.
    We also need to make sure that our model will stay accurate as time goes by, which
    is why having an automated metrics pipeline is important.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次运行，我们的模型表现惊人。这并不意味着我们的工作已经完成：初始模型拟合是拥有一个生产就绪模型的第一个步骤。我们的代码看起来有点手工艺，可能需要一些鲁棒性。我们还需要确保模型随着时间的推移保持准确，这就是为什么拥有一个自动化的度量管道很重要的原因。
- en: The last section looks at optimizing some aspect of our model training to yield
    better performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节探讨了优化模型训练的一些方面以提高性能。
- en: 13.3.3 Optimizing hyperparameters with cross-validation
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.3 使用交叉验证优化超参数
- en: This section covers how we can optimize some of the Params provided by the `LogisticRegression`.
    By fine-tuning some aspects of the model training (how Spark builds the fitted
    model), we can hope to yield better model accuracy. For this, we use a technique
    called *cross-validation*. Cross-validation resamples the data set into training
    and testing sets to assess the ability of the model to generalize over new data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们如何优化`LogisticRegression`提供的某些参数。通过微调模型训练的一些方面（Spark构建拟合模型的方式），我们有望提高模型的准确度。为此，我们使用了一种称为*交叉验证*的技术。交叉验证将数据集重新采样为训练集和测试集，以评估模型对新数据的泛化能力。
- en: 'In section 13.2.2, we saw that we split our data set into two parts: one for
    training and one for testing. With cross-validation, we subdivide the training
    set one more time to try to find the optimal set of Params for our `LogisticRegression`
    estimator. In ML terms, those Params are called hyperparameters since they are
    the parameters we use to train the model (which will internally contain parameters
    to make a prediction). We call hyperparameter optimization the process of selecting
    the best hyperparameters for a given situation/data set.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在13.2.2节中，我们看到了我们将数据集分成两部分：一部分用于训练，另一部分用于测试。使用交叉验证，我们进一步将训练集细分一次，以尝试找到适合我们的`LogisticRegression`估计器的最佳参数集。在机器学习的术语中，这些参数被称为超参数，因为它们是我们用来训练模型（模型内部将包含用于预测的参数）的参数。我们将超参数优化称为选择给定情况/数据集的最佳超参数的过程。
- en: 'Before digging into the details of how cross-validation works, let’s select
    a hyperparameter to optimize against. To keep the example simple and computationally
    manageable, we only build a simple grid with a single hyperparameter: `elasticNetParam`.
    This hyperparameter (called α) can take any value between `0.0` and `1.0`. To
    start, I keep myself to two values, namely `0.0` (the default) and `1.0`.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨交叉验证的工作原理之前，让我们选择一个要优化的超参数。为了使示例简单且易于计算，我们只构建了一个包含单个超参数的简单网格：`elasticNetParam`。这个超参数（称为α）可以取`0.0`到`1.0`之间的任何值。起初，我仅保留了两个值，即`0.0`（默认值）和`1.0`。
- en: Tip For more information about α, check out *Introduction to Statistical Learning*,
    by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani (Springer,
    2013). Trevor Hastie co-invented the concept of *Elastic Net*, which is what this
    α is about!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：想了解更多关于α的信息，请参阅Gareth James、Daniela Witten、Trevor Hastie和Rob Tibshirani合著的《统计学习引论》（Springer，2013年）。Trevor
    Hastie共同发明了*弹性网络*的概念，这正是α所涉及的内容！
- en: 'To build the set of hyperparameters we wish to evaluate our model against,
    we use the `ParamGridBuilder`, which assists in creating a Param Map, as shown
    in listing 13.19\. For this, we start with the builder class (just like we did
    with `SparkSession.builder` in chapter 2). This builder class can take a series
    of `addGrid()` methods taking two parameters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们希望评估模型的超参数集，我们使用`ParamGridBuilder`，它有助于创建参数映射，如列表13.19所示。为此，我们从一个构建器类开始（就像我们在第2章中使用`SparkSession.builder`一样）。这个构建器类可以接受一系列`addGrid()`方法，这些方法接受两个参数：
- en: The Param of the stage we want to modify. In this case, our `LogisticRegression`
    estimator was assigned to the variable `lr`, so `lr.elasticNetParam` is the Param
    in question.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要修改的阶段参数。在这种情况下，我们的`LogisticRegression`估计器被分配给变量`lr`，因此`lr.elasticNetParam`是我们要讨论的参数。
- en: The values we wish to assign the hyperparameter on, passed as a list.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望分配给超参数的值，以列表的形式传递。
- en: Once we are done, we call build(), and a list of *Param Maps* is returned. Each
    element of the list is a dictionary (called `Map` in Scala, hence the name) of
    the Params that will be passed to our pipeline when fitting. Often, we want to
    set the hyperparameters for the model estimator, but nothing prevents us from
    changing the Params from another stage, for instance the `preml_assembler`, should
    we want to remove features (see the exercises). Just be sure to be consistent
    if you meddle with `inputCol/outputCol` to avoid missing column errors.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们调用 build()，返回一个 *Param Maps* 列表。列表中的每个元素都是一个字典（在 Scala 中称为 `Map`，因此得名），它将传递给我们的管道以进行拟合的
    Params。通常，我们想要设置模型估计器的超参数，但没有任何东西阻止我们从另一个阶段更改 Params，例如 `preml_assembler`，如果我们想删除特征（见练习）。只是确保如果你修改
    `inputCol/outputCol`，要保持一致性，以避免丢失列错误。
- en: Listing 13.19 Using `ParamGridBuilder` to build a set of hyperparameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.19 使用 `ParamGridBuilder` 构建一组超参数
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ ParamGridBuilder() is a builder class . . .
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `ParamGridBuilder()` 是一个构建类 . . .
- en: ❷ . . . to which we can append addGrid() methods, setting an α of 0.0 and 1.0
    . . .
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 我们可以附加 addGrid() 方法，设置 α 为 0.0 和 1.0 . . .
- en: ❸ . . . until we call build() to finalize the grid!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ . . . 直到我们调用 build() 方法来最终确定网格！
- en: ❹ I’ve edited the output to keep only the relevant elements.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我已编辑输出，仅保留相关元素。
- en: Grid, done. Now onto cross-validation. PySpark provides out-of-the-box *K-fold
    cross-validation* through the `CrossValidator` class. In a nutshell, cross-validation
    tries every combination of hyperparameters by splitting the data set into a set
    of nonoverlapping, randomly partitioned sets (called *folds*), which are used
    as separate training and validation[²](#pgfId-1020556) data sets. In figure 13.6,
    I demonstrate an example with `k` `=` `3` folds. For each element of the grid,
    PySpark will perform three train-validation splits, fit the model on the train
    portion (which contains 2/3 of the data), and then evaluate the performance metric
    selected on the validation set (which contains the remaining 1/3).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 网格，完成。现在进行交叉验证。PySpark 通过 `CrossValidator` 类提供开箱即用的 *K 折交叉验证*。简而言之，交叉验证通过将数据集分割成一系列非重叠、随机划分的集合（称为
    *折*），这些集合被用作独立的训练和验证数据集。在图 13.6 中，我演示了一个 `k` `=` `3` 折的例子。对于网格中的每个元素，PySpark 将执行三次训练-验证分割，在包含
    2/3 数据的训练部分上拟合模型，然后评估在验证集（包含剩余的 1/3）上选择的性能指标。
- en: '![](../Images/13-06.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-06.png)'
- en: Figure 13.6 Three-fold cross-validation. For each Param map, we perform three
    train/validation cycles, each time using a different third as a validation set.
    The selected metric `areaUnderROC` is then combined (averaged), and this becomes
    the performance metric for the resulting model plus Param Map.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 三折交叉验证。对于每个 Param 映射，我们执行三次训练/验证周期，每次使用不同的三分之一作为验证集。然后，选定的指标 `areaUnderROC`
    被组合（平均），这成为最终模型加上 Param 映射的性能指标。
- en: 'Code-wise, the `CrossValidator` object combines everything under a single abstraction.
    To build a cross-validator, we need three elements, all of which we’ve encountered
    so far:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 代码上，`CrossValidator` 对象将所有内容组合在一个单一抽象之下。要构建交叉验证器，我们需要三个元素，所有这些我们之前都已经遇到过：
- en: 'An `estimator`, which contains the model we wish to evaluate (here: `food_
    pipeline`)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `estimator`，它包含我们希望评估的模型（在这里：`food_pipeline`）
- en: An `estimatorParamMaps` set, which is the list of Param Maps we created earlier
    in the section
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `estimatorParamMaps` 集合，这是我们之前在章节中创建的 Param 映射列表
- en: An `evaluator`, which carries the metric we wish to optimize against (here,
    we reuse the one we created in listing 13.17)
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `evaluator`，它携带我们希望优化的指标（在这里，我们重用了列表 13.17 中创建的指标）
- en: 'In listing 13.20, we also provide a few parameters: the `numFolds` `=` `3`,
    setting our number of folds `k` to `3`, a seed (`13`) for keeping the random number
    generator consistent between runs, and `collectSubModels=True` to keep a version
    of every model being trained (to compare the results). The resulting cross-validator
    follows the same conventions as an estimator, so we apply the `fit()` method to
    start the cross-validation process.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 13.20 中，我们还提供了一些参数：`numFolds` `=` `3`，将我们的折数 `k` 设置为 `3`，一个种子（`13`）以保持运行之间随机数生成器的一致性，以及
    `collectSubModels=True` 以保留每个正在训练的模型的版本（以比较结果）。生成的交叉验证器遵循与估计器相同的约定，因此我们应用 `fit()`
    方法来启动交叉验证过程。
- en: Listing 13.20 Creating and using a `CrossValidator` object
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.20 创建和使用 `CrossValidator` 对象
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ We use the fit() method on a CrossValidator, just like with any pipeline.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在 CrossValidator 上使用 fit() 方法，就像对任何管道一样。
- en: ❷ elasticNetParam == 1.0 wins by a razor-thin margin.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ elasticNetParam == 1.0 以微弱的优势获胜。
- en: 'To extract the `areaUnderROC` (which is the metric our evaluator was tracking)
    from the models trained, we use the `avgMetrics` attribute. Here, we are really
    splitting some hairs: both models are basically indistinguishable performance-wise,
    with the `elasticNetParam` `==` `1.0` model winning by a thin margin. Finally,
    we extract the `bestModel` so we can use it as a pipeline model.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要从训练好的模型中提取`areaUnderROC`（这是我们评估者跟踪的指标），我们使用`avgMetrics`属性。在这里，我们实际上是在吹毛求疵：两个模型在性能上基本上无法区分，`elasticNetParam`
    `==` `1.0`的模型以微弱的优势获胜。最后，我们提取`bestModel`以便将其用作管道模型。
- en: We have done a lot in this section. Thanks to PySpark’s friendly and consistent
    API, we were able to evaluate our model two ways (precision versus recall, as
    well as area under the ROC curve) and then optimize our pipeline’s hyperparameters
    through cross-validation. Knowing which model to use, which metric to select,
    and which process to follow could fill many books (it already has!), but I find
    doing data science with PySpark very pleasing. It’s nice to know that syntax does
    not get in the way!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们做了很多工作。多亏了PySpark友好且一致的API，我们能够以两种方式评估我们的模型（精确度与召回率，以及ROC曲线下的面积），然后通过交叉验证优化管道的超参数。知道使用哪个模型，选择哪个指标，以及遵循哪个过程可能需要很多书籍（它已经有了！），但我发现使用PySpark进行数据科学非常令人愉快。语法不会成为障碍真是太好了！
- en: In the next and final section of the chapter, we brush against model interpretability.
    We explore the coefficients of the model’s features and discuss some improvements
    based on our findings.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一节和最后一节中，我们触及了模型的可解释性。我们探讨了模型特征的系数，并基于我们的发现讨论了一些改进。
- en: '13.4 Getting the biggest drivers from our model: Extracting the coefficients'
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 从我们的模型中获取最大驱动因素：提取系数
- en: This section covers the extraction of our model features and their coefficients.
    We use those coefficients to get a sense of the most important features of the
    model and plan some improvements for a second iteration.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了提取我们的模型特征及其系数。我们使用这些系数来了解模型最重要的特征，并为第二次迭代规划一些改进。
- en: 'In section 13.2.1, I explained that the features in a vector built via the
    `VectorAssembly` object keep the feature names in the column’s metadata dictionary,
    and showed how to access them. We can access the schema through the `StructField`
    (see chapter 6 for a deep dive into the schema and the `StructField`) contained
    in the top-level `StructField`. Then we just need to match the variables to the
    coefficient in the right order. In listing 13.21, I show the metadata for the
    `features` column and extract the relevant fields. I then extract the coefficients
    from my `lrModel` through the `coefficient.values` attribute. Note that PySpark
    keeps the intercept of the model under the `intercept` slot: because I like to
    show everything in one fell swoop, I tend to add it as another “feature” in my
    table.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在13.2.1节中，我解释了通过`VectorAssembly`对象构建的向量中的特征保留了列的元数据字典中的特征名称，并展示了如何访问它们。我们可以通过顶层`StructField`（参见第6章以深入了解模式和`StructField`）来访问模式。然后我们只需按正确的顺序将变量与系数匹配。在列表13.21中，我展示了`features`列的元数据并提取了相关字段。然后我通过`lrModel`的`coefficient.values`属性提取系数。请注意，PySpark将模型的截距保留在`intercept`槽中：因为我喜欢一次性展示所有内容，所以我倾向于将截距作为表中的另一个“特征”添加。
- en: Tip If you have non-numeric features, PySpark will store the metadata for the
    features in a `binary` key instead of a `numeric` one. In our case, since we knew
    the features were binary, we did not treat them as special, and PySpark lumped
    them into the numeric metadata key.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如果您有非数值特征，PySpark将使用`binary`键而不是`numeric`键来存储特征的元数据。在我们的情况下，因为我们知道特征是二进制的，我们没有将它们视为特殊处理，PySpark将它们合并到数值元数据键中。
- en: Listing 13.21 Extracting the feature names from the `features` vector
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.21 从`features`向量中提取特征名称
- en: '[PRE28]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ I add the intercept manually from the intercept slot.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我从截距槽中手动添加了截距。
- en: ❷ I add the intercept manually from the intercept slot.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我从截距槽中手动添加了截距。
- en: ❸ Since negative and positive values are equally important in logistic regression,
    I create an absolute value column for easy ordering.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于在逻辑回归中负值和正值同等重要，我创建了一个绝对值列以便于排序。
- en: In chapter 12, I explained that having features on the same scale (here from
    zero to one) helps with the interpretability of the coefficients. Each coefficient
    gets multiplied by a value between `0` and `1`, so they are all consistent with
    one another. By ordering them by their absolute value, we can see which coefficients
    have the greatest impact on our model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章中，我解释了具有相同尺度（此处为零到一）的特征有助于系数的可解释性。每个系数都会乘以一个介于`0`和`1`之间的值，因此它们都是一致的。通过按绝对值排序，我们可以看到哪些系数对我们的模型影响最大。
- en: A coefficient close to zero, like `kirsch`, `lemon`, and `food_processor`, means
    that this feature is not very predictive of our model. On the flip side, a very
    high or low coefficient, like `cauliflower`, `horseradish`, and `quick_and_healthy`,
    means that this feature is highly predictive. When using a linear model, a positive
    coefficient means that the feature will predict toward a `1.0` (the dish is a
    dessert). Our results are not too surprising; looking at the very negative features,
    it seems that the presence of horseradish or a “quick and healthy” recipe means
    “not a dessert!”
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接近零的系数，如`kirsch`、`lemon`和`food_processor`，意味着这个特征对我们的模型不是很有预测性。另一方面，非常高的或低的系数，如`cauliflower`、`horseradish`和`quick_and_healthy`，意味着这个特征具有高度预测性。在使用线性模型时，正系数意味着该特征将预测为`1.0`（菜肴是甜点）。我们的结果并不令人惊讶；查看非常负的特征，似乎意味着存在芥末或“快速健康”的食谱意味着“不是甜点！”
- en: Unsurprisingly, data science revolves very much around one’s ability to process,
    extract information, and apply the right abstractions on the data. Statistical
    know-how becomes very useful when you need to know the why of specific models.
    PySpark provides functionality for a growing number of models, but each one will
    use a similar estimator/transformer setup. Now that you understand how the different
    components work, applying a different model will be a *piece of cake*!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 并非意外，数据科学很大程度上依赖于一个人处理、提取信息和在数据上应用正确抽象的能力。当你需要了解特定模型的原因时，统计知识变得非常有用。PySpark为越来越多的模型提供了功能，但每个模型都将使用类似的估计器/转换器设置。现在你了解了不同组件的工作原理，应用不同的模型将变得轻而易举！
- en: 'Portability of ML pipelines: Read and write'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道的可移植性：读取和写入
- en: 'Serializing and deserializing an ML pipeline is as simple as writing and reading
    a data frame. A Spark `PipelineModel` has a `write` method that works just like
    the one for the data frame (with the exception of the `format`, as the pipeline
    format is predefined). Setting the `overwrite()` option allows you to overwrite
    any existing model:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化和反序列化机器学习管道就像写入和读取数据帧一样简单。Spark的`PipelineModel`有一个`write`方法，它的工作方式与数据帧的`write`方法类似（除了`format`，因为管道格式是预定义的）。设置`overwrite()`选项允许你覆盖任何现有模型：
- en: '[PRE29]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result is a directory named `am_I_a_dessert_the_model`. To read it back,
    we need to import the `PipelineModel` class and call the `load()` method:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个名为`am_I_a_dessert_the_model`的目录。要读取它，我们需要导入`PipelineModel`类并调用`load()`方法：
- en: '[PRE30]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Transformers are objects that, through a `transform()` method, modify a data
    frame based on a set of Params that drives its behavior. We use a transformer
    stage when we want to deterministically transform a data frame.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器是对象，通过`transform()`方法，根据一组驱动其行为的Params修改数据帧。当我们想要确定性地转换数据帧时，我们会使用转换器阶段。
- en: Estimators are objects that, through a `fit()` method, take a data frame and
    return a fully parameterized transformer called a model. We use an estimator stage
    when we want to transform a data frame using a data-dependent transformer.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器是对象，通过`fit()`方法，接受一个数据帧并返回一个完全参数化的转换器，称为模型。当我们想要使用数据相关的转换器转换数据帧时，我们会使用估计器阶段。
- en: ML pipelines are like estimators, as they use the `fit()` method to yield a
    pipeline model. They have a single Param, `stages`, that carries an ordered list
    of transformers and estimators to be applied on a data frame.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习管道类似于估计器，因为它们使用`fit()`方法生成管道模型。它们有一个单一的Param，`stages`，它携带一个要应用于数据帧的转换器和估计器的有序列表。
- en: Before training a model, every feature needs to be assembled in a vector using
    the `VectorAssembler` transformer. This provides a single optimized (sparse or
    dense) column containing all the features for machine learning.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型之前，每个特征都需要使用`VectorAssembler`转换器组装成一个向量。这提供了一个包含所有特征的单一优化（稀疏或密集）列，用于机器学习。
- en: PySpark provides useful metrics for model evaluation through a set of evaluator
    objects. You select the appropriate one based on your type of prediction (binary
    classification = `BinaryClassificationEvaluator`).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark通过一系列评估器对象提供用于模型评估的有用指标。您可以根据预测类型选择合适的评估器（二分类 = `BinaryClassificationEvaluator`）。
- en: With a Param Map grid, an evaluator, and an estimator, we can perform model
    hyperparameter optimization to try different scenarios and try to improve model
    accuracy.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用参数映射网格、评估器和估计器，我们可以执行模型超参数优化，尝试不同的场景，并尝试提高模型精度。
- en: Cross-validation is a technique that resamples the data frame into different
    partitions before fitting/testing the model. We use cross-validation to test if
    the model performs consistently when it sees different data.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证是一种在拟合/测试模型之前将数据帧重新采样到不同分区的技术。我们使用交叉验证来测试模型在看到不同数据时是否表现一致。
- en: '* * *'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ This is a very simple model since the calories are directly tied to carbs,
    proteins, and fat. See chapter 12 for the formula.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 这是一个非常简单的模型，因为卡路里直接与碳水化合物、蛋白质和脂肪相关联。请参阅第12章以获取公式。
- en: ² The documentation calls them train and test, but I prefer to be unambiguous
    and reserve the “test” moniker for the split we did before running the data through
    the pipeline.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ² 文档称它们为训练集和测试集，但我更喜欢明确区分，将“测试集”这个名称保留为我们运行数据通过管道之前的分割。
