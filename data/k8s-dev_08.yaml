- en: 6 Scaling up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Scaling Pods and nodes manually
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动扩展 Pods 和节点
- en: Using CPU utilization and other metrics to scale Pod replicas dynamically
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CPU 利用率和其他指标动态扩展 Pod 副本
- en: Utilizing managed platforms to add and remove nodes based on the resources your
    Pods require
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用托管平台根据您的 Pods 所需资源添加和删除节点
- en: Using low-priority placeholder Pods to provision burst capacity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用低优先级占位符 Pods 来提供突发容量
- en: Architecting apps so that they can be scaled
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计应用程序以便它们可以扩展
- en: Now that we have the application deployed and have health checks in place to
    keep it running without intervention, it’s a good time to look at how you’re going
    to scale up. I’ve named this chapter “Scaling up,” as I think everyone cares deeply
    about whether their system architecture can handle being scaled up when your application
    becomes wildly successful and you need to serve all your new users. But, don’t
    worry, I’ll also cover scaling down so you can save money during the quiet periods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了应用程序，并设置了健康检查以保持其运行而无需干预，现在是时候考虑如何进行扩展了。我将其命名为“扩展”，因为我认为每个人都非常关心当应用程序取得巨大成功，您需要为所有新用户提供服务时，系统架构是否能够处理扩展。但别担心，我还会介绍如何缩小规模，以便您在淡季节省费用。
- en: The goal is, ultimately, to operationalize our deployment using automatic scaling.
    That way, we can be fast asleep or relaxing on a beach in Australia, and our application
    can be responding to traffic spikes dynamically. To get there, we’ll need to ensure
    that the application is capable of scaling, understand the scaling interactions
    of Pods and nodes in the Kubernetes cluster, and determine the right metrics to
    configure an autoscaler to do it all for us.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的目标是，通过自动扩展来使我们的部署投入运行。这样，我们就可以在澳大利亚的海滩上熟睡或放松，而我们的应用程序可以动态地响应流量峰值。为了达到这个目标，我们需要确保应用程序能够扩展，了解
    Kubernetes 集群中 Pods 和节点的扩展交互，并确定正确的指标来配置自动扩展器为我们完成所有这些工作。
- en: 6.1 Scaling Pods and nodes
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 扩展 Pods 和节点
- en: Getting your application containerized and deployed on Kubernetes is a great
    step toward building an application deployment that is capable of scaling and
    supporting your growth. Let’s now go over how to actually scale things up when
    that moment of success arrives and the traffic increases (and scale things down
    to save some money in the quiet periods).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的应用程序容器化并在 Kubernetes 上部署，这是构建能够扩展并支持您增长的应用程序部署的巨大一步。现在，让我们来了解一下，当成功时刻到来，流量增加时，如何实际进行扩展（并在淡季降低规模以节省一些费用）。
- en: 'In Kubernetes, there are essentially two resources that you need to scale:
    your application (Pods) and the compute resources they run on (nodes). What can
    make life a bit complicated is that the way you scale these resources is separate,
    even though the requirements (e.g., more application capacity) are somewhat correlated.
    It’s not enough to just scale Pods as they’ll run out of compute resources to
    run on, nor is it enough to scale up nodes alone as that just adds empty capacity.
    Scaling both in unison and at the correct ratio is what’s needed. Fortunately,
    there are some tools to make your life easier (and some fully automated platforms
    that take care of everything for you), which I’ll cover in the following discussion.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，您需要扩展的基本资源有两个：您的应用程序（Pods）以及它们运行的计算资源（节点）。使生活变得复杂的是，您扩展这些资源的方式是分开的，尽管需求（例如，更多的应用程序容量）在一定程度上是相关的。仅仅扩展
    Pods 是不够的，因为它们会耗尽运行所需的计算资源，同样，仅仅扩展节点也是不够的，因为这只会增加空余容量。需要同时扩展并且保持正确的比例。幸运的是，有一些工具可以使您的生活更轻松（以及一些完全自动化的平台，它们会为您处理一切），以下讨论中我会涉及这些内容。
- en: Firstly, to handle more traffic to your application, you’ll need to increase
    the number of Pod replicas. Starting with the manual approach, you can achieve
    this by updating your Deployment configuration with the desired number of replicas
    as follows.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了处理更多流向您应用程序的流量，您需要增加 Pod 副本的数量。从手动方法开始，您可以通过以下方式更新您的 Deployment 配置以实现所需的副本数。
- en: Listing 6.1 Chapter06/6.1_Replicas/deploy.yaml
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 第 06 章/6.1_Replicas/deploy.yaml
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The replicas field specifies how many copies of your Pod you want to be running.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 副本字段指定了您希望运行的 Pod 的副本数量。
- en: As usual, you can apply changes you make to config with `kubectl` `apply` `-f`
    `deploy.yaml`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，您可以使用 `kubectl` 命令 `apply` `-f` `deploy.yaml` 来应用您对配置所做的更改。
- en: '`kubectl` also offers a convenient imperative command that can achieve the
    same result:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 还提供了一个方便的命令行命令，可以达到相同的效果：'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, if you try to add too many replicas, you’ll soon run out of space in
    your cluster for those Pods to be scheduled. That’s where scaling the nodes come
    in. You’ll know when you’ve run out of room when you run `kubectl` `get` `pods`,
    and a bunch are listed as `Pending`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试添加太多的副本，你很快就会在你的集群中没有足够的空间来调度这些Pod。这就是节点扩展发挥作用的地方。当你运行`kubectl get pods`时，你会知道你已经没有空间了，因为会有很多Pod被列为`Pending`。
- en: Pods can be in `Pending` for a number of reasons, the most common of which (if
    the Pod has been in that state for a minute or so) is a lack of resources. Essentially,
    the lack of resources is an unsatisfied condition, and the Pod remains `Pending`
    until the condition can be satisfied. There can be other unsatisfied conditions
    as well if the Pod has dependencies (like requiring to be deployed on a node with
    another Pod that hasn’t been created). To disambiguate, describe the Pod with
    `kubectl` `describe` `pod` `$POD_NAME` and look at the events. If you see an event,
    such as `FailedScheduling` with a message like `Insufficient` `CPU`, you likely
    need to add more nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pods可能因为多种原因处于`Pending`状态，其中最常见的原因（如果Pod处于这种状态一分钟或更长时间）是资源不足。本质上，资源不足是一个未满足的条件，Pod将保持`Pending`状态，直到条件得到满足。如果Pod有依赖关系（例如，需要部署在另一个尚未创建的Pod所在的节点上），也可能存在其他未满足的条件。为了消除歧义，使用`kubectl
    describe pod $POD_NAME`描述Pod并查看事件。如果你看到一个事件，例如带有类似`Insufficient CPU`消息的`FailedScheduling`，你很可能需要添加更多的节点。
- en: Nodeless Kubernetes
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无节点Kubernetes
- en: I’d like to take a moment to cover nodeless Kubernetes platforms. In my opinion,
    the ideal cloud Kubernetes platform is one where you don’t really need to worry
    a whole lot about nodes. After all, if you’re using the cloud, why not have a
    platform that provisions the node resources that are needed based on the Pod’s
    requirements so you can focus more on creating great applications and services?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我想花一点时间来谈谈无节点Kubernetes平台。在我看来，理想的云Kubernetes平台是一个你不必过多担心节点的平台。毕竟，如果你使用云服务，为什么不有一个根据Pod需求提供所需节点资源的平台，这样你就可以更多地专注于创建优秀的应用和服务呢？
- en: 'In my role as a product manager at Google Cloud, this is exactly the product
    I built with my team. We named it *GKE Autopilot*. It’s a platform that frees
    developers from worrying about nodes. With GKE Autopilot, you create standard
    Kubernetes workloads like Deployments, StatefulSets, and Jobs, specifying the
    replica counts and the required CPU and memory resources. Autopilot then provisions
    the necessary compute resources to run your Pods and manages the compute capacity
    on your behalf. This has two key advantages: it enhances developer efficiency
    by eliminating the need to define computing requirements twice (in the Pod and
    the node), and it improves operational efficiency by significantly reducing the
    burden of node management.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我作为谷歌云产品经理的角色中，这正是我和我的团队一起构建的产品。我们将其命名为*GKE Autopilot*。这是一个让开发者从担心节点中解放出来的平台。使用GKE
    Autopilot，你可以创建标准的Kubernetes工作负载，如Deployments、StatefulSets和Jobs，指定副本计数和所需的CPU和内存资源。然后Autopilot会为你提供必要的计算资源来运行Pod，并代表你管理计算容量。这有两个关键优势：它通过消除在Pod和节点中定义计算需求的需要来提高开发者的效率，并通过显著减少节点管理的负担来提高运营效率。
- en: One thing that sets Autopilot apart is that the Kubernetes node concept retains
    some relevance. Much of the node-related scheduling logic (like spread topologies,
    affinity, and anti-affinity, covered in chapter 8) is relevant and can still be
    used. Autopilot is nodeless in the sense that you no longer need to worry about
    how nodes are provisioned or managed, but it doesn’t completely abstract away
    or hide nodes. After all, there *is* a machine somewhere running your code, and
    this can have physical relevance around things like failure domains or wanting
    to co-locate Pods for reduced latency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Autopilot与众不同的一个特点是Kubernetes节点概念仍然保持一定的相关性。许多与节点相关的调度逻辑（如第8章中提到的拓扑分布、亲和性和反亲和性）仍然相关，并且仍然可以使用。Autopilot在无节点意义上意味着你不再需要担心节点是如何提供或管理的，但它并没有完全抽象或隐藏节点。毕竟，某处确实有一台机器在运行你的代码，这可能在诸如故障域或希望为了降低延迟而将Pod放置在一起等方面具有物理相关性。
- en: I believe Autopilot has a best-of-both-worlds design that gives you the node-level
    controls that you need while still removing the burden of operating and administering
    those nodes. No need to care anymore about how many nodes you have, their size
    and shape, whether they are healthy, and whether they are sitting idle or underused.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为 Autopilot 具有最佳的设计，它为你提供了所需的节点级控制，同时仍然消除了操作和管理这些节点的负担。不再需要关心你有多少节点，它们的大小和形状，它们是否健康，以及它们是否处于空闲或未充分利用状态。
- en: If you are using GKE Autopilot or a platform like it, you can basically ignore
    everything in this chapter that talks about scaling *nodes* and focus purely on
    scaling *Pods*. Scaling Pods manually or automatically (like with a Horizontal
    Pod Autoscaler) is all you need to do with Autopilot, as the system will provision
    the necessary node resources for you without any additional configuration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 GKE Autopilot 或类似的平台，你可以基本上忽略本章中关于扩展 *节点* 的所有内容，而纯粹关注扩展 *Pods*。使用 Autopilot，手动或自动（例如使用水平
    Pod 自动扩展器）扩展 Pods 是你需要做的所有事情，因为系统会为你配置必要的节点资源，无需任何额外配置。
- en: 'To scale the nodes, you’ll need to consult your Kubernetes provider’s platform
    documentation, as Kubernetes itself doesn’t orchestrate nodes. In the case of
    Google Kubernetes Engine (GKE), if you use Autopilot, nodes are provisioned automatically,
    and you can skip right ahead to section 6.2\. For GKE clusters with node pools,
    the command looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要扩展节点，你需要查阅你的 Kubernetes 提供商的平台文档，因为 Kubernetes 本身并不编排节点。在 Google Kubernetes
    Engine (GKE) 的情况下，如果你使用 Autopilot，节点将自动配置，你可以直接跳到第 6.2 节。对于具有节点池的 GKE 集群，命令看起来是这样的：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Scaling down is performed with the same commands. When you scale down the nodes,
    depending on your provider, you should be able to run the same command as to scale
    up. The cluster will first cordon the nodes to prevent new Pods from being scheduled
    on them, then drain the nodes, evicting all Pods while giving them time to shutdown
    gracefully. Pods managed in a Deployment or other higher-order workload construct
    will be rescheduled on other nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小扩展使用相同的命令执行。当你缩小节点时，根据你的提供商，你应该能够运行与扩展时相同的命令。集群首先隔离节点以防止新的 Pods 被调度到它们上，然后排空节点，在给
    Pods 时间优雅关闭的同时驱逐所有 Pods。在 Deployment 或其他高级工作负载结构中管理的 Pods 将在其他节点上重新调度。
- en: Manually cordoning and draining nodes
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 手动隔离和排空节点
- en: 'If you want to observe what happens when nodes are scaled down, you can manually
    cordon, drain, and remove nodes with the following commands:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要观察节点缩小时的操作，你可以使用以下命令手动隔离、排空和移除节点：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that deleting the node via `kubectl` does not always delete the underlying
    VM, meaning you may still be charged for it! If you delete a node with `kubectl`
    `delete` `node $NODE_NAME`, follow up to make sure the VM is also deleted. On
    GKE in Autopilot mode, cordon and drain is enough to remove the node from use
    and the system will handle the deletion for you. For GKE node-based clusters,
    be sure to delete the VM yourself, for example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过 `kubectl` 删除节点并不总是删除底层的虚拟机，这意味着你仍然可能需要为此付费！如果你使用 `kubectl delete node
    $NODE_NAME` 删除节点，请跟进以确保虚拟机也被删除。在 Autopilot 模式下的 GKE 中，隔离和排空足以将节点从使用中移除，系统会为你处理删除。对于基于节点的
    GKE 集群，请确保你自己删除虚拟机，例如：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generally, the cluster will perform these actions automatically as you scale
    nodes down, so you don’t normally need to run them yourself; however, they do
    come in handy if you ever need to remove a node because it’s misbehaving.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你在缩小节点时，集群会自动执行这些操作，所以你通常不需要自己运行它们；然而，如果你需要移除一个表现不佳的节点，这些操作会很有用。
- en: So, this is how you scale Pods and nodes by hand. Read on to learn how to automate
    both these operations with horizontal Pod autoscaling to scale Pods and cluster
    autoscaling to scale nodes (for cloud providers that offer it).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是你手动扩展 Pods 和节点的方式。继续阅读，了解如何通过水平 Pod 自动扩展来自动化这两个操作以扩展 Pods，以及通过集群自动扩展来扩展节点（对于提供此功能的云提供商）。
- en: 6.2 Horizontal Pod autoscaling
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 水平 Pod 自动扩展
- en: Scaling the number of Pod replicas of your application in Kubernetes is called
    *horizontal Pod autoscaling*. It’s horizontal, as you’re increasing the number
    of replicas to serve increased traffic, rather than vertical, which instead implies
    increasing the resources available to each replica. Generally, to scale up a system,
    it’s horizontal scaling that you want.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，扩展应用程序的 Pod 副本数量被称为 *水平 Pod 自动扩展*。它是水平的，因为您正在增加副本数量以服务增加的流量，而不是垂直的，垂直意味着增加每个副本可用的资源。通常，为了扩展系统，您希望进行水平扩展。
- en: Kubernetes includes a feature called the Horizontal Pod Autoscaler (HPA), a
    system whereby you specify a Pod *metric* like CPU usage to observe and target,
    along with some scaling limits (minimum and maximum replicas). The HPA will then
    attempt to satisfy your metric by creating and removing Pods. In the case of CPU,
    if your target is, say, 20% CPU utilization, the HPA will add replicas when your
    average utilization (across all Pods) goes above 20% (of what the Pod requested
    in its resource requests) and remove them when it goes below 20%. These actions
    are subject to a minimum and maximum limit you provide, as well as cooldown periods
    to avoid too much churn. We can create an HPA for our Deployment as in the following
    listing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 包含一个名为水平 Pod 自动扩展器（HPA）的功能，这是一个系统，您指定一个 Pod *指标*（如 CPU 使用率）进行观察和目标，以及一些扩展限制（最小和最大副本数）。然后
    HPA 将尝试通过创建和删除 Pod 来满足您的指标。在 CPU 的情况下，如果您的目标是，比如说，20% 的 CPU 利用率，当您的平均利用率（跨所有 Pod）超过
    20%（Pod 在其资源请求中请求的）时，HPA 将添加副本，当它低于 20% 时将其删除。这些操作受您提供的最小和最大限制以及冷却期的影响，以避免过多的波动。我们可以在以下列表中为我们的部署创建一个
    HPA。
- en: Listing 6.2 Chapter06/6.2_HPA/hpa.yaml
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 第06章/6.2_HPA/hpa.yaml
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Minimum number of replicas
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最小副本数
- en: ❷ Maximum number of replicas
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 最大副本数
- en: ❸ The CPU utilization target. The HPA will create more replicas when the Pod
    CPU utilization is higher than this value
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ CPU 利用率目标。当 Pod 的 CPU 利用率高于此值时，HPA 将创建更多副本
- en: ❹ The Deployment that will be scaled
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将要扩展的部署
- en: 'You can also create it imperatively. As always, I prefer the config approach
    as it makes it easier to edit things later. But here is the equivalent imperative
    command for completeness:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以强制创建它。像往常一样，我更喜欢配置方法，因为它使得稍后编辑事物变得更容易。但这里提供了等效的强制命令，以保持完整性：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To test this, we’ll need to make the CPU really busy. Using the following two
    listings, let’s add a really CPU-intensive path to our timeserver application:
    calculating pi.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们需要让 CPU 真的忙碌起来。使用以下两个列表，让我们向 timeserver 应用程序添加一个非常 CPU 密集型的路径：计算 π。
- en: Listing 6.3 Chapter06/timeserver4/pi.py
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 第06章/timeserver4/pi.py
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Listing 6.3 is our method to calculate pi, and listing 6.4 shows the addition
    of a new URL path to server.py, which calls it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 是我们计算 π 的方法，列表 6.4 显示了向 server.py 添加新的 URL 路径，并调用它。
- en: Listing 6.4 Chapter06/timeserver4/server.py
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 第06章/timeserver4/server.py
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The new HTTP path
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新的 HTTP 路径
- en: The following listing provides a revised Deployment that references the new
    version of the container with the added path. To work correctly with the HPA,
    it’s important to set resource requests, which we added in chapter 5 and are present
    here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了一个修订后的部署，它引用了带有新路径的容器的新版本。为了与 HPA 正确工作，设置资源请求非常重要，我们在第 5 章中添加了这些请求，并且在这里也存在。
- en: Listing 6.5 Chapter06/6.2_HPA/deploy.yaml
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 第06章/6.2_HPA/deploy.yaml
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Replicas initially set to 1
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始副本数设置为 1
- en: ❷ The new app version
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 新的应用程序版本
- en: ❸ Resource requests are important for the HPA to work correctly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 资源请求对于 HPA 正确工作很重要。
- en: 'We can now create the Deployment, Service, and HPA:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建部署、服务和 HPA：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'While you’re waiting for the external IP to be provisioned, you can start watching
    the CPU utilization of your Pods with the following command (I suggest putting
    it in a new window):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当您等待外部 IP 分配时，您可以使用以下命令开始监视 Pod 的 CPU 利用率（我建议将其放入新窗口）：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once you have the external IP, generate some load on the endpoint. Apache Bench,
    which you can install on most systems, works well for this. The following command
    will send 50 requests simultaneously to our endpoint, until 10,000 have been sent--that
    should do it:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了外部 IP，在端点生成一些负载。Apache Bench，您可以在大多数系统上安装，对此效果很好。以下命令将同时发送 50 个请求到我们的端点，直到发送了
    10,000 个请求——这应该足够了：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can watch the scaling status of the Deployment with the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下方式查看部署的扩展状态：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The Linux `watch` command is convenient for watching all resources using a
    single command (which `kubectl` can’t do by itself):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 的 `watch` 命令方便地使用单个命令监视所有资源（`kubectl` 本身无法做到这一点）：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If all goes correctly, you should observe the CPU utilization increase as visible
    with `kubectl` `top` `pods`, and more Pod replicas being created. Once you stop
    sending load to the endpoint (e.g., by interrupting `ab` or waiting for it to
    finish), you should observe these replicas gradually being removed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该观察到 CPU 利用率随着 `kubectl top pods` 命令的可见性而增加，并且更多的 Pod 副本被创建。一旦你停止向端点发送负载（例如，通过中断
    `ab` 或等待其完成），你应该观察到这些副本逐渐被移除。
- en: 'You may observe that replicas are scheduled faster when scaling up in response
    to the high request load than when they are removed while scaling down when the
    requests stop. That’s just the system being a little cautious when removing capacity
    to avoid churn in case demand spikes. Here’s what it looked like for my sample
    run:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会观察到，在响应高请求负载进行扩容时，副本的调度速度比在请求停止时缩容时移除副本要快。这只是系统在移除容量时稍微谨慎一些，以避免需求激增时的波动。以下是我样本运行的情况：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The HPA shown here worked pretty well using the CPU metric, but there’s a catch:
    your workload may not be CPU bound. Unlike the CPU-intensive request used in the
    demo, many HTTP services spend a lot of time waiting on external services like
    databases. These deployments may need to scale using other metrics like the number
    of requests per second (RPS) hitting the service rather than the CPU utilization.
    Kubernetes offers two built-in metrics: CPU (demonstrated in the previous example)
    and memory. It doesn’t directly support metrics like RPS, but it can be configured
    by using custom and external metrics exposed by your monitoring service. The next
    section covers this situation.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的 HPA 使用 CPU 指标效果相当不错，但有一个问题：你的工作负载可能不是 CPU 密集型的。与演示中使用的 CPU 密集型请求不同，许多
    HTTP 服务花费大量时间等待外部服务，如数据库。这些部署可能需要使用其他指标进行缩放，例如每秒请求次数（RPS），而不是 CPU 利用率。Kubernetes
    提供了两个内置指标：CPU（在先前的示例中演示）和内存。它不直接支持 RPS 这样的指标，但可以通过使用由你的监控服务公开的自定义和外部指标进行配置。下一节将介绍这种情况。
- en: What about vertical Pod autoscaling?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，垂直 Pod 自动缩放呢？
- en: Vertical Pod autoscaling (VPA) is a concept whereby Pods are scaled vertically
    by adjusting their CPU and memory resources. Implementations in Kubernetes achieve
    VPA by observing the Pods resource usage and dynamically changing the Pod’s resource
    requests over time. Kubernetes doesn’t offer a VPA implementation out of the box,
    although an open source implementation is available,^a and cloud providers, including
    GKE, offer their own versions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直 Pod 自动缩放（VPA）是一种概念，通过调整 Pod 的 CPU 和内存资源来垂直缩放 Pod。在 Kubernetes 中的实现通过观察 Pod
    资源使用情况并在时间动态地改变 Pod 的资源请求来实现 VPA。Kubernetes 并不提供内置的 VPA 实现，尽管有一个开源实现^a，以及包括 GKE
    在内的云提供商提供他们自己的版本。
- en: As a VPA can determine a Pod’s resource requests automatically, it could save
    you some effort and provide some resource efficiency. It’s also the right tool
    for the job if you need the Pod’s resource requests to be adjusted dynamically
    over time (for Pods that have resource requirements that fluctuate widely).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 VPA 可以自动确定 Pod 的资源请求，它可以节省你一些精力并提供一些资源效率。如果你需要 Pod 的资源请求随时间动态调整（对于资源需求波动很大的
    Pod），这也是合适的工具。
- en: Using a VPA adds its own complexity and may not always play nicely with the
    HPA. I would focus first on setting appropriate Pod resource requests and the
    horizontal scaling of replicas.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VPA 会增加其自身的复杂性，并且不一定总是与 HPA 玩得很好。我首先会关注设置合适的 Pod 资源请求和副本的水平缩放。
- en: ^a [https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ^a [https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)
- en: 6.2.1 External metrics
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 外部指标
- en: 'One popular scaling metric is requests per second (RPS). The basis of using
    RPS metrics for scaling is that you measure how many requests an instance of your
    application can serve every second (the replica’s capacity). Then, you divide
    the current number of requests by this amount, and voila, you have the number
    of replicas needed:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的缩放指标是每秒请求次数（RPS）。使用 RPS 指标进行缩放的基础是测量你的应用程序实例每秒可以处理多少请求（副本的容量）。然后，将当前请求的数量除以这个值，
    voila，你就有了所需的副本数量：
- en: replica_count = RPS ÷ replica_capacity
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: replica_count = RPS ÷ replica_capacity
- en: The benefit of the RPS metric is that if you are confident that your application
    can handle the RPS that you tested it for, then you can be confident that it can
    scale under load as it’s the autoscaler’s job to provision enough capacity.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: RPS 指标的好处是，如果你确信你的应用程序可以处理你对其测试的 RPS，那么你可以确信它可以在负载下进行扩展，因为自动扩展器的任务是提供足够的容量。
- en: In fact, even if you’re not doing *automatic* scaling, this metric is still
    a really good way to plan your capacity. You can measure the capacity of your
    replicas, project your traffic, and increase your replicas accordingly. But with
    Kubernetes, we can also configure an HPA with the RPS metric for automatic scaling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使你不在进行 *自动* 扩展，这个指标也是一个非常好的规划容量的方法。你可以测量副本的容量，预测流量，并相应地增加副本。但是，使用 Kubernetes，我们也可以配置一个带有
    RPS 指标的 HPA 以进行自动扩展。
- en: Now, in this case, we’ll be using the *external metric* property of the HPA.
    One problem with this is that the metric, as its name suggests, is sourced from
    outside the cluster. So, if you’re using a different monitoring solution than
    the one I use in my example, you’ll need to look up what the relevant RPS metric
    is. Fortunately, RPS is a pretty common metric, and any monitoring solution worth
    its salt will offer it.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这种情况下，我们将使用 HPA 的 *外部指标* 属性。这个问题的一个问题是，正如其名称所暗示的，这个指标是从集群外部获取的。所以，如果你使用的是与我示例中使用的不同的监控解决方案，你需要查找相关的
    RPS 指标。幸运的是，RPS 是一个非常常见的指标，任何值得信赖的监控解决方案都会提供它。
- en: In prior chapters, we discussed a few different ways to get traffic into your
    cluster via a so-called layer-4 load balancer, which operates at a TCP/IP layer,
    and a so-called layer-7 Ingress, which operates at the HTTP layer. As *requests*
    are an HTTP concept, you’ll need to be using an Ingress to get this metric. Ingress
    is covered in the next chapter in depth; for now, it’s enough to know that this
    object sees and inspects your HTTP traffic and thus can expose a metric for the
    number of requests you are getting.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们讨论了几种通过所谓的第 4 层负载均衡器（在 TCP/IP 层运行）和所谓的第 7 层 Ingress（在 HTTP 层运行）将流量引入集群的不同方法。由于
    *请求* 是 HTTP 概念，你需要使用 Ingress 来获取这个指标。Ingress 将在下一章中深入讨论；现在，只需知道这个对象可以查看和检查你的 HTTP
    流量，因此可以暴露你接收到的请求数量指标。
- en: For this example, as shown in the following two listings, we will use the same
    Deployment but expose it on an Ingress, via a Service of type `NodePort` (instead
    of type `LoadBalancer` from the prior chapters).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，如下两个列表所示，我们将使用相同的部署，但通过一个类型为 `NodePort` 的服务（而不是之前章节中的 `LoadBalancer`
    类型）将其暴露在 Ingress 上。
- en: Listing 6.6 Chapter06/6.2.1_ExternalMetricGCP/service.yaml
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 第 06 章/6.2.1_ExternalMetricGCP/service.yaml
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The name of this internal service
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 此内部服务的名称
- en: ❷ Type NodePort is used for the ingress.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 NodePort 类型用于 Ingress。
- en: Listing 6.7 Chapter06/6.2.1_ExternalMetricGCP/ingress.yaml
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.7 第 06 章/6.2.1_ExternalMetricGCP/ingress.yaml
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ References the internal service from listing 6.6
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 引用列表 6.6 中的内部服务
- en: Now if you’re using Google Cloud, the following HPA definition can pick up the
    RPS metric from the Ingress once you replace the forwarding rule name with your
    own.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Google Cloud，一旦你将转发规则名称替换为你自己的，以下 HPA 定义就可以从 Ingress 中获取 RPS 指标。
- en: Listing 6.8 Chapter06/6.2.1_ExternalMetricGCP/hpa.yaml
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.8 第 06 章/6.2.1_ExternalMetricGCP/hpa.yaml
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The external metric
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 外部指标
- en: The `forwarding_rule_name` is how the metric server knows which Ingress object
    you’re talking about. You can omit the `selector` field completely, but then it
    will match on all Ingress objects—probably not what you want.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`forwarding_rule_name` 是指标服务器知道你正在谈论哪个 Ingress 对象的方式。你可以完全省略 `selector` 字段，但这样它将匹配所有
    Ingress 对象——这可能不是你想要的。'
- en: 'Complicating matters this forwarding rule name is a platform-specific resource
    name and not the Kubernetes object name (in this example, that name is set automatically
    by GKE). To discover the platform resource name, after waiting a few minutes for
    your Ingress to be configured, you can describe your Ingress object:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使问题复杂化的是，这个转发规则名称是一个平台特定的资源名称，而不是 Kubernetes 对象名称（在这个例子中，该名称由 GKE 自动设置）。为了发现平台资源名称，在等待几分钟以配置你的
    Ingress 之后，你可以描述你的 Ingress 对象：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The forwarding rule name
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 转发规则名称
- en: 'Another way to query this information, which is important if you are configuring
    automated tooling, is to understand where the data is within the object structure
    and use the JsonPath format of `kubectl`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种查询此信息的方法，这对于配置自动化工具非常重要，是了解数据在对象结构中的位置，并使用 `kubectl` 的 JsonPath 格式：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TIP I built the JsonPath expression by first querying the `-o=json` version
    of the Ingress and then figuring out the path through a combination of looking
    at the JsonPath docs, Stack Overflow, and trial and error.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：我通过首先查询 Ingress 的 `-o=json` 版本，然后通过查看 JsonPath 文档、Stack Overflow 和试错来构建
    JsonPath 表达式。
- en: Once you have the objects ready, there’s one last step, which is to ensure Cloud
    Monitoring is enabled for the workloads in your cluster, and install some glue
    that gives the HPA access to the metrics. Follow the instructions[¹](#pgfId-1036162)
    to install the Custom Metrics - Stackdriver Adapter*.*
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦对象准备就绪，还有最后一步，即确保您的集群中的工作负载已启用 Cloud Monitoring，并安装一些粘合剂，以便 HPA 可以访问指标。按照[¹](#pgfId-1036162)中的说明安装自定义指标
    - Stackdriver 适配器*.*。
- en: 'With our Deployment, Service of type `NodePort`, Ingress, HPA, and metrics
    adapter all configured, we can now try it out! Generate some requests to the Ingress
    (replacing the IP of your Ingress, obtained via `kubectl` `get` `ingress`):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的部署、服务类型为 `NodePort`、入口、HPA 和指标适配器都配置完成后，我们现在可以尝试一下！生成一些对入口的请求（替换您的入口 IP，通过
    `kubectl get ingress` 获取）：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And, in a separate window, observe the scale out:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在另一个窗口中，观察扩展：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: One thing you may notice is that it’s already easier to validate that the system
    is performing more as expected. Apache Bench allows you to specify concurrent
    requests; you can see how long they take (and therefore calculate the RPS) and
    look at the number of replicas to determine whether it’s right. This process is
    a bit harder with the CPU metric, where in order to test, you may have to try
    to make the Pod as busy as possible, as we did in the previous example. This property
    of scaling based on *user requests* is one reason why it is a popular metric to
    use.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，验证系统是否按预期运行已经更容易了。Apache Bench 允许你指定并发请求；你可以看到它们花费了多长时间（因此可以计算 RPS）并查看副本数量以确定是否正确。这个过程对于
    CPU 指标来说有点困难，为了测试，你可能需要尽可能让 Pod 变得忙碌，就像我们在上一个示例中所做的那样。基于 *用户请求* 的扩展属性是它成为流行指标的一个原因。
- en: Observing and debugging
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 观察和调试
- en: 'To see what the HPA is doing, you can run `kubectl` `describe` `hpa`. Pay particular
    attention to the `ScalingActive` condition. If it is `False`, it likely means
    that your metric is not active, which can occur for a number of reasons: (1) the
    metric adapter wasn’t installed (or isn’t authenticated), (2) your metric name
    or selector is wrong, or (3) there just aren’t any monitoring samples for the
    given metric available yet. Note that even with the correct configuration, you
    will see `False` when there is no monitoring data samples (e.g., there are no
    requests), so be sure to send some requests to the endpoint and wait a minute
    or two for the data to come through before investigating further.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 HPA 在做什么，你可以运行 `kubectl describe hpa`。特别注意 `ScalingActive` 条件。如果它是 `False`，这通常意味着你的指标未激活，这可能由多种原因造成：（1）指标适配器未安装（或未认证），（2）你的指标名称或选择器错误，或者（3）尚无针对给定指标的监控样本。请注意，即使配置正确，在没有监控数据样本的情况下（例如，没有请求），你也会看到
    `False`，所以在进一步调查之前，请确保向端点发送一些请求并等待一分钟或两分钟，以便数据通过。
- en: AverageValue vs. value
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值 vs. 值
- en: In the previous example, we used `targetAverageValue`. `targetAverageValue`
    is the target *per Pod* value of the metric. `targetValue` is an alternative,
    which is the target absolute value. As the RPS capacity is calculated at a per-Pod
    level, it’s `targetAverageValue` we want.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们使用了 `targetAverageValue`。`targetAverageValue` 是该指标的每个 Pod 的目标值。`targetValue`
    是一个替代方案，它是目标绝对值。由于 RPS 容量是在每个 Pod 级别计算的，所以我们想要的是 `targetAverageValue`。
- en: Other metrics
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其他指标
- en: 'Another popular external metric when dealing with background tasks (covered
    in chapter 10) is the Pub/Sub queue length. Pub/Sub is a queuing system that allows
    you to have a queue of work that needs to be performed, and you can set up a workload
    in Kubernetes to process that queue. For such a setup, you may wish to react to
    the queue size by adding or removing Pod replicas (workers that can process the
    queue). You can find a fully worked example on the GKE docs[²](#pgfId-1036199)
    for this; essentially, it boils down to an HPA that looks like the previous one,
    just with a different metric:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理后台任务（在第 10 章中介绍）时，另一个流行的外部度量标准是 Pub/Sub 队列长度。Pub/Sub 是一个排队系统，允许您有一个需要执行的工作队列，您可以在
    Kubernetes 中设置一个工作负载来处理该队列。对于此类设置，您可能希望通过添加或删除 Pod 副本（可以处理队列的工作者）来对队列大小做出反应。您可以在
    GKE 文档中找到一个完整的示例[²](#pgfId-1036199)；本质上，它类似于之前的 HPA，只是度量不同：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This configuration consists of the metric name and the resource identifier for
    the metric—in this case, the Google Cloud Pub/Sub subscription identifier.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置包括度量名称和度量资源标识符——在这种情况下，是 Google Cloud Pub/Sub 订阅标识符。
- en: Other monitoring solutions
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其他监控解决方案
- en: External metrics are something you should be able to configure for *any* Kubernetes
    monitoring system. While the worked example given previously uses Cloud Monitoring
    on Google Cloud, the same principles should apply if you’re using Prometheus or
    another cloud monitoring system. To get things going, you’ll need to determine
    (1) how to install the metric adapter for your monitoring solution, (2) what the
    metric name is in that system, and (3) the right way to select the metric resource.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 外部度量标准是您应该能够为 *任何* Kubernetes 监控系统配置的。虽然之前给出的示例使用了 Google Cloud 上的 Cloud Monitoring，但如果您使用
    Prometheus 或其他云监控系统，同样的原则也应该适用。要开始，您需要确定（1）如何为您的监控解决方案安装度量适配器，（2）在该系统中度量名称是什么，以及（3）正确选择度量资源的方式。
- en: 6.3 Node autoscaling and capacity planning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 节点自动缩放和容量规划
- en: Using horizonal Pod autoscaling is a great way to automatically scale your Pods
    based on demand. However, if you have to scale nodes manually to add and remove
    capacity, it still requires a person in the loop. The common Cluster Autoscaler
    cluster functionality enables you to scale nodes on demand and pairs well with
    the HPA to scale Pods.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用水平 Pod 自动缩放是一种根据需求自动缩放您的 Pod 的好方法。然而，如果您必须手动缩放节点以添加和删除容量，仍然需要人工介入。常见的集群自动缩放器集群功能使您能够根据需求缩放节点，并且与
    HPA 配合良好以缩放 Pod。
- en: 6.3.1 Cluster autoscaling
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 集群自动缩放
- en: Cluster autoscaling is not part of the base Kubernetes API but rather is an
    optional component. Fortunately, most cloud providers offer it (or something similar)
    and build it in as a property of the platform that will scale the number of nodes
    for you, allowing you to focus just on your application and how many replicas
    it has. As this feature is platform-dependent, the exact implementation will vary
    (and not all providers offer it). Search for “[product name] Cluster Autoscaler”
    to find the relevant docs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放不是 Kubernetes 基础 API 的一部分，而是一个可选组件。幸运的是，大多数云服务提供商都提供（或提供类似功能）并将其作为平台的一个属性来构建，该属性可以为您自动缩放节点数量，让您只需关注您的应用程序及其副本数量。由于此功能依赖于平台，具体的实现会有所不同（并非所有提供商都提供此功能）。搜索“[产品名称]
    集群自动缩放器”以找到相关文档。
- en: In the case of GKE, if you use the Autopilot mode of operation, clusters have
    built-in node provisioning and autoscaling; no further configuration is required.
    For GKE clusters with node pools, you can configure autoscaling when creating
    a node pool or updating an existing node pool.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GKE 的情况下，如果您使用操作模式的 Autopilot，集群将内置节点预配和自动缩放；无需进一步配置。对于具有节点池的 GKE 集群，您可以在创建节点池或更新现有节点池时配置自动缩放。
- en: When using cluster autoscaling, you can focus on scaling your own workloads,
    having the cluster respond automatically (figure 6.1). This is really convenient,
    as it can solve the `Pending` Pods problem both when scaling existing workloads
    and deploying new ones. Read the specific implementation details of your provider,
    though, to understand what cases are not covered (e.g., how Pods that are too
    big to fit on current node configurations are handled).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用集群自动缩放时，您可以专注于缩放您的工作负载，让集群自动响应（如图 6.1）。这非常方便，因为它可以在缩放现有工作负载和部署新工作负载时解决 `Pending`
    Pods 问题。不过，请阅读您提供商的具体实现细节，以了解哪些情况不受覆盖（例如，如何处理太大而无法适应当前节点配置的 Pods）。
- en: '![06-01](../../OEBPS/Images/06-01.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![06-01](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 The Cluster Autoscaler watches for pending Pods and creates new nodes
    if needed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 集群自动扩展器监视挂起的Pod，并在需要时创建新节点。
- en: Traditional cluster autoscalers may only add new nodes of an existing predefined
    configuration, requiring you to define each possible node type you wish to use,
    so be sure to read the docs. GKE can add new nodes of any type if you use Autopilot
    (no configuration needed; that’s how it works out of the box) or node-based mode
    with Node Auto Provisioning configured.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的集群自动扩展器可能只能添加现有预定义配置的新节点，需要你定义你希望使用的每个可能的节点类型，所以请务必阅读文档。如果你使用Autopilot（无需配置；这是出厂设置的工作方式）或基于节点的模式并配置了节点自动供应，GKE可以添加任何类型的新节点。
- en: Cluster autoscaling and other provider tools that can add and remove nodes automatically
    make your life easier by allowing you to mostly ignore nodes and focus purely
    on your own Pods. When paired with Pod-based scaling like HorizontalPodAutoscaler,
    you can have a fairly hands-off, automated deployment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动扩展和其他可以自动添加和删除节点的提供者工具通过允许你主要忽略节点并专注于自己的Pod来使你的生活变得更轻松。当与基于Pod的扩展如HorizontalPodAutoscaler配合使用时，你可以有一个相当不干预、自动化的部署。
- en: 6.3.2 Spare capacity with cluster autoscaling
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 集群自动扩展的备用容量
- en: One of the drawbacks of autoscaling nodes compared to manually adding nodes
    is that sometimes the autoscaler can tune things a little *too* well and result
    in no spare capacity. This can be great for keeping costs down, but it makes it
    slower to start new Pods, as capacity needs to be provisioned before the Pod can
    start up.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动添加节点相比，自动扩展节点的缺点之一是有时自动扩展器可能会调整得“太”好，导致没有备用容量。这可以很好地降低成本，但会使启动新Pod的速度变慢，因为容量需要在Pod启动之前配置。
- en: Adding new nodes and then starting the Pod is slower than adding new Pods to
    existing nodes. Nodes have to be provisioned and booted, while Pods that get scheduled
    onto existing nodes just have to pull the container and boot—and if the container
    is already in the cache, they can even start booting right away. As shown in figure
    6.2, the newly scheduled Pod must wait for capacity to be provisioned before it
    can begin booting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新节点然后启动Pod比向现有节点添加新Pod要慢。节点需要配置和启动，而那些被调度到现有节点的Pod只需拉取容器并启动——如果容器已经在缓存中，它们甚至可以立即开始启动。如图6.2所示，新调度的Pod必须等待容量配置完毕后才能开始启动。
- en: '![06-02](../../OEBPS/Images/06-02.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![06-02](../../OEBPS/Images/06-02.png)'
- en: Figure 6.2 Dynamically adding capacity with autoscaling to accommodate newly
    scheduled Pods
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 使用自动扩展动态添加容量以适应新调度的Pod
- en: One way to solve both of these problems while still keeping your autoscaler
    is to use a low-priority placeholder Pod. This Pod does nothing itself other than
    reserve capacity (keeping additional nodes up and running on standby). This Pod’s
    priority is low, so when your own workloads scale up, they can preempt this Pod
    and use the node capacity (figure 6.3).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在保持自动扩展器的同时解决这两个问题的方法是通过使用低优先级的占位符Pod。这个Pod本身不执行任何操作，只是保留容量（保持额外的节点处于待机状态）。这个Pod的优先级很低，所以当你的工作负载扩展时，它们可以抢占这个Pod并使用节点容量（图6.3）。
- en: '![06-03](../../OEBPS/Images/06-03.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![06-03](../../OEBPS/Images/06-03.png)'
- en: Figure 6.3 Autoscaling with a placeholder Pod, allowing for rapid booting of
    new Pods using spare capacity
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 使用占位符Pod进行自动扩展，允许使用备用容量快速启动新Pod
- en: To create our placeholder Pod deployment, first, we’ll need a PriorityClass.
    This priority class should have a priority lower than zero (we want every other
    priority class to preempt it), as in the following listing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的占位符Pod部署，首先，我们需要一个PriorityClass。这个优先级类应该有一个低于零的优先级（我们希望其他所有优先级类都能被它抢占），如下所示。
- en: Listing 6.9 Chapter06/6.3.2_PlaceholderPod/placeholder-priority.yaml
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 第06章/6.3.2_PlaceholderPod/placeholder-priority.yaml
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Low-priority value for placeholder Pods
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 占位符Pod的低优先级值
- en: ❷ Won’t preempt other Pods
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不会抢占其他Pod
- en: Now, we can create our “do nothing” container Deployment, as in the following
    listing.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建我们的“不执行任何操作”的容器部署，如下所示。
- en: Listing 6.10 Chapter06/6.3.2_PlaceholderPod/placeholder-deploy.yaml
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 第06章/6.3.2_PlaceholderPod/placeholder-deploy.yaml
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ How many replicas do you want? This, with the CPU and memory requests, determines
    the size of the headroom capacity provided by the placeholder Pod.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你想要多少个副本？这，加上CPU和内存请求，决定了占位符Pod提供的头空间容量的大小。
- en: ❷ Uses the priority class we just created
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用我们刚刚创建的优先级类
- en: ❸ We want this Pod to shut down immediately with no grace period.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们希望这个Pod立即关闭，没有任何宽限期。
- en: ❹ This is our “do nothing” command.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这是我们的“什么都不做”命令。
- en: ❺ The resources that will be reserved by the placeholder Pod. This should be
    equal to the largest Pod you wish to replace this Pod.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 占位符Pod将保留的资源。这应该等于你希望替换此Pod的最大Pod。
- en: When creating this yourself, consider the number of replicas you need and the
    size (memory and CPU requests) of each replica. The size should be at least the
    size of your largest regular Pod; otherwise, your workload may not fit in the
    space when the placeholder Pod is preempted. At the same time, don’t increase
    the size too much; it would be better to use more replicas than replicas that
    are much larger than your standard workloads, Pods if you wish to reserve extra
    capacity.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当你自己创建时，考虑你需要多少个副本以及每个副本的大小（内存和CPU请求）。大小应该至少与你的最大常规Pod大小相同；否则，当占位符Pod被抢占时，你的工作负载可能无法适应空间。同时，不要增加太多大小；如果你希望保留额外容量，使用比标准工作负载、Pod大得多的副本可能更好。
- en: For these placeholder Pods to be preempted by other Pods that you schedule,
    those Pods will need to have a priority class that both has a higher value and
    doesn’t have a `preemptionPolicy` of `Never`. Fortunately, the default priority
    class has a `value` of `0` and a `preemptionPolicy` of `PreemptLowerPriority`,
    so, by default, all other Pods will displace our placeholder Pod.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这些占位符Pod被你安排的其他Pod抢占，那些Pod需要有优先级类，其值更高，并且没有`preemptionPolicy`为`Never`。幸运的是，默认优先级类有一个`value`为`0`和`preemptionPolicy`为`PreemptLowerPriority`，所以默认情况下，所有其他Pod都会替换我们的占位符Pod。
- en: To represent the Kubernetes default as its own priority class, it would look
    like listing 6.11\. As you don’t actually need to change the default, I wouldn’t
    bother configuring this. But, if you’re creating your own priority classes, you
    can use this listing as the reference (just don’t set `globalDefault` to `true`
    unless that’s what you really intend). Once again, for the placeholder Pod preemption
    to work, be sure *not* to set `preemptionPolicy` to `Never`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示Kubernetes默认值作为其自己的优先级类，它看起来就像列表6.11。由于你实际上不需要更改默认值，所以我不必麻烦配置这个。但是，如果你正在创建自己的优先级类，你可以使用这个列表作为参考（只是不要将`globalDefault`设置为`true`，除非你真的打算这样做）。再次强调，为了使占位符Pod抢占工作，务必*不要*将`preemptionPolicy`设置为`Never`。
- en: Listing 6.11 Chapter06/6.3.2_PlaceholderPod/default-priority.yaml
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 Chapter06/6.3.2_PlaceholderPod/default-priority.yaml
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Priority value higher than the placeholder Pods
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 优先级值高于占位符Pod
- en: ❷ Will preempt other Pods
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将抢占其他Pod
- en: ❸ Set as the default so other Pods will preempt the placeholder Pods.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置为默认值，以便其他Pod抢占占位符Pod。
- en: Placeholder Pods encapsulated in a Deployment like this are useful for providing
    constant scaling headroom, giving you a defined amount of capacity ready for quick
    scheduling. Alternatively, you can encapsulate them in a Job for one-off capacity
    provisioning, a CronJob to have the capacity provisioned on a schedule, or run
    them as standalone Pods.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样的部署封装的占位符Pod对于提供恒定的扩展空间很有用，为你提供一定量的容量，以便快速调度。或者，你可以将它们封装在Job中，用于一次性容量配置，CronJob以按计划配置容量，或者作为独立Pod运行。
- en: 6.4 Building your app to scale
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 构建可扩展的应用程序
- en: Scaling your application up is only part of the equation. The application itself
    needs to build with scaling in mind. Even though you may not be at the point of
    your growth where you need to worry about these matters, I believe that *the time
    when you need to scale is not the time to design how you’re going to scale!*
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展你的应用程序只是方程的一部分。应用程序本身需要考虑扩展来构建。即使你可能还没有到达需要担心这些问题的增长点，我相信*你需要扩展的时候，不是设计如何扩展的时候！*
- en: When your application is one with unpredictable growth (e.g., a startup with
    potentially unlimited users), you really want to plan ahead to avoid the “success
    failure” scenario. This is where, in a breakout moment of your success, the app
    fails because it can’t handle the scale. Since you don’t know when this breakout
    moment will be, you need to have designed for this ahead of time. Not every startup
    will have a breakout moment, but if yours does, you want to be ready to capitalize
    on the opportunity; otherwise, it could all be for naught.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的应用程序是具有不可预测增长的应用（例如，具有潜在无限用户的初创公司）时，你真的需要提前规划以避免“成功失败”的场景。这就是在你成功的突破时刻，应用程序失败，因为它无法处理规模。由于你不知道这个突破时刻何时会发生，你需要提前为此做好准备。并不是每个初创公司都会有一个突破时刻，但如果你有，你希望准备好利用这个机会；否则，一切可能都徒劳无功。
- en: Fortunately, by choosing Kubernetes to orchestrate your containers, you are
    starting with a really solid foundation for a scalable app. When designing the
    application, there are some other factors to keep in mind that are largely independent
    of Kubernetes. Most scalable design principles apply to both Kubernetes and non-Kubernetes
    environments, but I’ll cover a few best practices worth keeping in mind when building
    a scalable app on Kubernetes. Attending to some scalability principles as you
    develop your application could matter in the future when your breakout moment
    arrives and you need to scale it to the moon.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过选择 Kubernetes 来编排你的容器，你已经在为可扩展的应用程序打下了坚实的基础。在设计应用程序时，还有一些其他因素需要考虑，这些因素在很大程度上与
    Kubernetes 独立。大多数可扩展设计原则都适用于 Kubernetes 和非 Kubernetes 环境，但我会介绍一些在 Kubernetes 上构建可扩展应用程序时值得记住的最佳实践。在开发应用程序时关注一些可扩展性原则，在未来你的突破时刻到来并需要将其扩展到极致时可能会很重要。
- en: 6.4.1 Avoiding state
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 避免状态
- en: One of the most important aspects of being able to scale is avoiding the local
    state in your applications. A stateless design is where each replica (instance)
    of your application that’s running can serve any incoming request without reference
    to any data stored locally on any other instance. Local ephemeral storage can
    be used for temporary data processing as long as it’s not shared between replicas
    and doesn’t need to be available for the next request that comes in.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 能够扩展的一个重要方面是避免在应用程序中存在本地状态。无状态设计是指你的应用程序运行的每个副本（实例）都可以在没有参考任何存储在另一个实例上的本地数据的情况下服务任何传入的请求。本地短暂存储可以用于临时数据处理，只要它不在副本之间共享，并且不需要为下一个请求提供可用性。
- en: NOTE The property of the application being stateless is, I believe, the most
    important factor in the popular Twelve-Factor App design methodology. ([https://12factor.net/processes](https://12factor.net/processes)).
    Stateless apps are easier to scale and maintain as each instance can independently
    serve any request.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我认为应用程序的无状态属性是流行的十二要素应用程序设计方法中最重要的因素。（[https://12factor.net/processes](https://12factor.net/processes)）。无状态应用程序更容易扩展和维护，因为每个实例都可以独立地处理任何请求。'
- en: Unlike with a classical host machine, in Kubernetes, all data written to disk
    by the container is ephemeral (deleted when the container is terminated or restarted)
    by default. It is possible to create stateful applications using persistent volumes
    and the StatefulSet construct (see chapter 9), but, by default, containers are
    treated as stateless, and you generally want to keep it that way so that you can
    scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的宿主机不同，在 Kubernetes 中，容器写入磁盘的所有数据默认都是短暂的（当容器终止或重启时会被删除）。你可以使用持久卷和 StatefulSet
    构造（见第 9 章）来创建有状态的应用程序，但默认情况下，容器被视为无状态的，你通常希望保持这种状态以便进行扩展。
- en: Rather than storing state on disks that you manage in Kubernetes, use external
    data stores to store data instead, like SQL and NoSQL databases for structured
    data, object storage for files, and memory databases like Redis for session state.
    To support your ability to scale, choose managed services (rather than self-hosting)
    and ensure the services you choose can handle your potential growth.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是在 Kubernetes 中管理磁盘上存储状态，使用外部数据存储来存储数据，例如 SQL 和 NoSQL 数据库用于结构化数据，对象存储用于文件，以及像
    Redis 这样的内存数据库用于会话状态。为了支持你的扩展能力，选择托管服务（而不是自托管）并确保你选择的服务可以处理你的潜在增长。
- en: This is not to say that all state is bad. After all, you need *somewhere* to
    store your state, and sometimes this needs to be a self-hosted application. When
    you do create such an application, be sure to choose highly scalable solutions,
    like a popular open source solution with a track record of success (e.g., Redis).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说所有状态都是坏的。毕竟，你需要某个地方来存储你的状态，有时这可能需要是一个自托管的应用程序。当你创建这样的应用程序时，请确保选择高度可扩展的解决方案，例如具有成功记录的流行开源解决方案（例如
    Redis）。
- en: Relational database gotchas
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库的陷阱
- en: If you use a relational database like MySQL or PostgreSQL to store data, there
    are more than a few potential pitfalls worth paying attention to.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用像 MySQL 或 PostgreSQL 这样的关系型数据库来存储数据，那么有相当多的潜在陷阱值得注意。
- en: '**Taming your queries**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**驯服你的查询**'
- en: It goes without saying that inefficient queries will give you inefficient scaling,
    slowing down as the amount of data increases and the number of requests increase.
    To keep things under control, I recommend logging and analyzing your queries and
    starting early in the development process (you don’t want to wait until your app
    is a hit to look at the queries!).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不言而喻，低效的查询会导致低效的扩展，随着数据量的增加和请求数量的增加而变慢。为了保持控制，我建议记录并分析你的查询，并在开发早期阶段就开始（你不想等到你的应用成为热门后才查看查询！）。
- en: You can’t improve what you don’t measure, so logging the performance of SQL
    queries that are performed during each request is the most important first step.
    Look for requests that generate a lot of queries, or slow queries, and start there.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能提高你没有衡量的东西，所以记录每个请求中执行的 SQL 查询的性能是最重要的第一步。寻找生成大量查询或慢查询的请求，并从这里开始。
- en: Both MYSQL and PostgreSQL support the `EXPLAIN` command, which can help analyze
    specific queries for performance. Common tactics to improve performance include
    adding indices for commonly searched columns and reducing the number of `JOIN`s
    you need to perform. MySQL’s documentation “Optimizing SELECT Statements”^a goes
    into great detail on many different optimization tactics.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL 和 PostgreSQL 都支持 `EXPLAIN` 命令，这可以帮助分析特定查询的性能。提高性能的常见策略包括为常用搜索列添加索引和减少需要执行的
    `JOIN` 数量。MySQL 的文档“优化 SELECT 语句”^a 详细介绍了许多不同的优化策略。
- en: '**Avoiding N+1 queries**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**避免 N+1 查询**'
- en: Even if your queries are superefficient, each individual query you make to the
    database has overhead. Ideally, each request your application processes should
    perform a constant number of queries, regardless of how much data is displayed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的查询非常高效，你对数据库的每个查询都会产生开销。理想情况下，你的应用程序处理的每个请求都应该执行固定数量的查询，无论显示多少数据。
- en: If you have a request that renders a list of objects, you ideally want to serve
    this request without generating a separate query for each of those objects. This
    is commonly referred to as the N+1 query problem (as when the problem occurs,
    there is often one query to get the list and then one for each item [N items]
    in the list).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个渲染对象列表的请求，你理想上希望不为列表中的每个对象生成单独的查询。这通常被称为 N+1 查询问题（当问题发生时，通常有一个查询用于获取列表，然后为列表中的每个项目
    [N 个项目] 有一个查询）。
- en: This antipattern is particularly common with systems that use object-rational
    mapping (ORM) and feature lazy loading between parent and child objects. Rendering
    the child objects of a one-to-many relationship with lazy loading typically results
    in N+1 queries (one query for the parent and N queries for the N child objects),
    which will show up in your logs. Fortunately, there is normally a way with such
    systems to indicate up front that you plan to access the child objects so that
    the queries can be batched.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种反模式在那些使用对象关系映射（ORM）并且具有父子对象懒加载功能的系统中尤为常见。使用懒加载渲染一对多关系中的子对象通常会导致 N+1 查询（一个查询用于父对象，N
    个查询用于 N 个子对象），这些查询将出现在你的日志中。幸运的是，在这样系统中通常有方法可以提前表明你打算访问子对象，以便可以将查询批量处理。
- en: 'Such N+1 query situations can normally be optimized into a constant number
    of queries, either with a `JOIN` to return the child objects in the list query
    or two queries: one to get the record set and a second to get the details for
    the child objects in that set. Remember, the goal is to have a small constant
    number of queries per request, and in particular, the number of queries shouldn’t
    scale linearly with the number of records being presented.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 N+1 查询情况通常可以被优化为固定数量的查询，无论是通过 `JOIN` 来在列表查询中返回子对象，还是两个查询：一个用于获取记录集，另一个用于获取该集中子对象的详细信息。记住，目标是每个请求都只有少量固定的查询，特别是查询的数量不应该与展示的记录数量成线性关系。
- en: '**Using read replicas for SELECT queries**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用只读副本进行 SELECT 查询**'
- en: One of the best ways to reduce the strain on your primary database is to create
    a read replica. In cloud environments, this is often really trivial to set up.
    Send all your read queries to your read replica (or replicas!) to keep the load
    off the primary read/write instance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻主数据库压力的最好方法之一是创建一个只读副本。在云环境中，这通常非常简单。将所有只读查询发送到你的只读副本（或副本！）以减轻主读/写实例的负载。
- en: To design your application with this pattern in mind before you actually need
    a read replica, you could have two database connections in your application to
    the same database, using the second to simulate the read replica. Set up the read-only
    connection with its own user that only has read permissions. Later, when you need
    to deploy an actual read replica, you can simply update the instance address of
    your second connection, and you’re good to go!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在你实际上需要读取副本之前，考虑到这种模式来设计你的应用程序，你可以在应用程序中设置到同一数据库的两个数据库连接，使用第二个来模拟读取副本。设置只具有读取权限的自己的只读连接的用户。稍后，当你需要部署实际的读取副本时，你只需更新第二个连接的实例地址，然后就可以继续了！
- en: '**Incrementing primary keys**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**递增主键**'
- en: If you really hit it big, you may end up regretting using incrementing primary
    keys. They’re a problem for scaling, as they assume a single writable database
    instance (inhibiting horizontal scaling) and require a lock when inserting, which
    affects performance (i.e., you can’t insert two records at once).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的取得了巨大成功，你可能会后悔使用递增主键。它们是扩展的问题，因为它们假设一个单一的可写数据库实例（阻碍了水平扩展）并且在插入时需要加锁，这会影响性能（即，你不能同时插入两个记录）。
- en: This is really only a problem at very large scale, but worth keeping in mind
    as it’s harder to rearchitect things when you suddenly need to scale up. The common
    solution to this problem is global UUIDs (e.g., `8fe05a6e-e65d-11ea-b0da-00155d51dc33`),
    a 128-bit number commonly displayed as a hexadecimal string, which can be uniquely
    generated by any client (including code running on the user’s device).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上只在大规模时才是一个问题，但值得记住，因为当你突然需要扩展时，重新架构事情会更困难。解决这个问题的常见方法是使用全局UUID（例如，`8fe05a6e-e65d-11ea-b0da-00155d51dc33`），这是一个128位的数字，通常以十六进制字符串的形式显示，可以由任何客户端（包括在用户设备上运行的代码）唯一生成。
- en: When Twitter needed to scale up, it opted, instead, to create its own global
    incrementing IDs to retain the property that they are sortable (i.e., newer tweets
    have a higher ID number), which you can read about in their post “Announcing Snowflake.”^b
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当Twitter需要扩展时，它选择创建自己的全局递增ID以保留它们可排序的特性（即，较新的推文具有更高的ID号），你可以在他们的文章“宣布Snowflake”中了解更多信息。^b
- en: On the other hand, you might prefer to keep incrementing primary keys for aesthetic
    reasons, like when the record ID is exposed to the user (as in the case of a tweet
    ID) or for simplicity. Even if you plan to keep your incrementing primary keys
    for a while, one step you can still take early on is not using auto-incrementing
    primary keys in places where they wouldn’t add any value, like, say, a user session
    object—maybe not *every* table needs an incrementing primary key.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可能出于美学原因更喜欢保留递增主键，例如当记录ID暴露给用户时（如推文ID的情况）或为了简单起见。即使你计划保留递增主键一段时间，你仍然可以在一开始就采取的一个步骤是不在它们不会增加任何价值的地方使用自动递增主键，比如用户会话对象——也许不是*每个*表都需要递增主键。
- en: ^a [https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html](https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^a [https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html](https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html)
- en: ^b [https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html](https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^b [https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html](https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html)
- en: 6.4.2 Microservice architectures
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 微服务架构
- en: One way to build up your application is by splitting services into multiple
    services, often described as using a microservice architecture. This method is
    basically just creating several internal services to perform separate tasks and
    using remote procedure calls (an HTTP request, essentially) to call those functions
    from other services. This contrasts with the monolith service design approach
    of having the complete program logic in a single container.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 构建你的应用程序的一种方法是将服务拆分为多个服务，这通常被描述为使用微服务架构。这种方法基本上就是创建几个内部服务来执行不同的任务，并通过远程过程调用（本质上是一个HTTP请求）从其他服务调用这些函数。这与将完整程序逻辑放在单个容器中的单体服务设计方法形成对比。
- en: While there are some benefits to splitting up a monolith into multiple smaller
    services, there are some drawbacks as well, so I don’t advocate using a microservice
    architecture just for the sake of it. Benefits include being able to use different
    programming languages for each service, develop them independently (e.g., by separate
    teams), and use independent scaling. Drawbacks include more complex debugging
    and integration testing, as you now have more components and need a way to trace
    requests through the system.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将单一架构拆分为多个较小的服务有一些好处，但也存在一些缺点，所以我并不主张仅仅为了使用微服务架构。好处包括可以为每个服务使用不同的编程语言，独立开发（例如，由不同的团队），以及独立扩展。缺点包括更复杂的调试和集成测试，因为你现在有更多的组件，需要一种方法来追踪系统中的请求。
- en: Microservice vs. monolith
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务与单一架构
- en: Should you build microservices or a monolith? For the sake of this debate, which
    I’m not going to litigate in this book, let me share two views on the topic and
    let you judge for yourself.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该构建微服务还是单一架构？为了这次辩论，我不会在本书中详细讨论，但让我分享两个关于这个话题的观点，由你自己来判断。
- en: David Heinemeier Hansson (DHH) writes in his post “The Majestic Monolith”^a
    that microservices are for large tech companies, and most smaller teams are better
    served by a monolith. His argument is that while microservices can have advantages
    in certain situations, it’s not always clear cut, and the overhead—particularly
    for smaller teams—is not worth it.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: David Heinemeier Hansson (DHH) 在他的文章“宏伟的单一架构”^a 中写道，微服务适合大型科技公司，而对于大多数小型团队来说，单一架构可能更合适。他的论点是，尽管微服务在某些情况下可能具有优势，但并非总是那么明确，尤其是对于小型团队来说，这种开销（尤其是额外的开销）并不值得。
- en: James Lewis and Martin Fowler, in their essay “Microservices,”^b lay out a well-thought-out
    and balanced view of microservices. One benefit highlighted is a product mentality,
    whereby internal teams focus on building and managing their own components, a
    decentralized approach that allows teams to make their own architectural decisions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: James Lewis 和 Martin Fowler 在他们的文章“微服务”^b 中阐述了对微服务的深思熟虑且平衡的观点。其中一个突出的好处是产品心态，即内部团队专注于构建和管理自己的组件，这种去中心化的方法允许团队做出自己的架构决策。
- en: ^a [https://m.signalvnoise.com/the-majestic-monolith/](https://m.signalvnoise.com/the-majestic-monolith/)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^a [https://m.signalvnoise.com/the-majestic-monolith/](https://m.signalvnoise.com/the-majestic-monolith/)
- en: ^b [http://martinfowler.com/articles/microservices.html](http://martinfowler.com/articles/microservices.html)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^b [http://martinfowler.com/articles/microservices.html](http://martinfowler.com/articles/microservices.html)
- en: Whether you go all in on microservices or not, the key point I want to focus
    on here is that if you have multiple services, you can scale them separately.
    This is true, of course, even if you have just a single internal service in addition
    to your main application—there’s no need to make *every* endpoint its own service
    to benefit from this architecture. For example, say you have a web application
    that mostly serves HTML and JSON requests, but one endpoint does some real-time
    graphics work that uses more memory than your average request. It might be worth
    creating a separate Deployment (even one using the same container) to serve the
    graphics endpoint so you can scale it separately and also isolate it a bit.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是否完全采用微服务，我想强调的关键点是，如果你有多个服务，你可以单独扩展它们。当然，即使你除了主应用程序外只有一个内部服务，这也是正确的——没有必要让每个端点都成为自己的服务以从这种架构中受益。例如，假设你有一个主要提供HTML和JSON请求的Web应用程序，但有一个端点执行一些需要比平均请求更多内存的实时图形工作。可能值得创建一个单独的部署（甚至可以使用相同的容器）来提供服务端点，这样你就可以单独扩展它，并且稍微隔离它。
- en: There are a couple of ways to do this. You can have a single frontend that calls
    the internal service, as illustrated in figure 6.4, or you can have end users
    connect to this new service directly, as shown in figure 6.5.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种实现方式。你可以有一个单独的前端来调用内部服务，如图6.4所示，或者你可以让最终用户直接连接到这个新服务，如图6.5所示。
- en: '![06-04](../../OEBPS/Images/06-04.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![06-04](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 Two HTTP paths being served by the same frontend that communicates
    with an internal service
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 由同一前端提供服务的两个HTTP路径，该前端与内部服务通信
- en: '![06-05](../../OEBPS/Images/06-05.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![06-05](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 Two paths being served by separate services
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 由不同服务提供的两个路径
- en: Whether you are going all in on microservices, splitting off a single service
    to be handled by its own individually scalable Deployment, using multiple programming
    languages, or running internally developed and open source software to provide
    your application, you will end up creating *internal services* in Kubernetes.
    Internal services are provisioned with a private cluster IP address and are called
    by other services in the cluster to deliver this architecture. The next chapter
    covers how to configure such internal services.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是全情投入微服务，还是将单个服务拆分出来由其单独可扩展的部署处理，使用多种编程语言，或者运行内部开发和开源软件来提供你的应用程序，你最终都会在Kubernetes中创建*内部服务*。内部服务使用私有集群IP地址进行配置，并由集群中的其他服务调用以提供这种架构。下一章将介绍如何配置此类内部服务。
- en: 6.4.3 Background tasks
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 背景任务
- en: Another important factor to help you scale is to avoid having any heavy inline
    processing. For example, let’s say you have an endpoint that returns a thumbnail
    of an image and will generate the thumbnail if it doesn’t exist in the cache.
    You could place this logic inline, where the user requests the thumbnail and the
    service responds by returning the thumbnail from the cache or generating one if
    the cache is empty. The problem with such a design is that serving the thumbnail
    from the cache should be very fast while creating the thumbnail is not. If a lot
    of requests come in, all needing to create a thumbnail, the server could slow
    down or crash. Plus, it’s hard to scale because some requests are lightweight
    and others are really heavy. You could scale up this service but could still be
    unlucky and have your load balancer direct all the heavy requests at a single
    instance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个帮助你扩展的重要因素是避免进行任何重量级的内联处理。例如，假设你有一个端点返回图像的缩略图，并且如果缓存中没有该缩略图，则会生成缩略图。你可以将此逻辑内联，即用户请求缩略图，服务通过返回缓存中的缩略图或在没有缓存的情况下生成一个来响应。这种设计的问题在于，从缓存中提供缩略图应该非常快，而创建缩略图则不是。如果有很多请求进来，都需要创建缩略图，服务器可能会变慢或崩溃。此外，由于某些请求很轻量，而其他请求则非常重，因此很难进行扩展。你可以扩展此服务，但仍然可能不幸地将所有重请求都导向单个实例。
- en: The solution is to use the background task pattern, covered in detail in chapter
    10\. Essentially, when heavy processing is needed, rather than doing it in line,
    you schedule a task and return a status code to the client indicating it should
    retry the request. There is a container configured to process this task queue,
    which can be scaled accurately based on the current queue length. So, the request
    comes in, resulting in a cache miss and a queued task. If things go well, when
    the client automatically retries the request after a short time, the thumbnail
    will have been processed by the background queue and be ready for serving—a similar
    end result for the user, a little extra work to build a background queue and a
    client with retry logic, but much better scalability.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用第10章详细介绍的背景任务模式。基本上，当需要重量级处理时，而不是直接进行，你安排一个任务，并向客户端返回一个状态码，指示它应该重试请求。有一个配置了处理此任务队列的容器，可以根据当前队列长度准确地进行扩展。因此，请求进来，导致缓存未命中并排队。如果一切顺利，当客户端在短时间内自动重试请求后，缩略图将已由后台队列处理并准备好提供服务——对用户来说，这是一个类似的结果，需要额外构建一个后台队列和具有重试逻辑的客户端，但具有更好的可扩展性。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Kubernetes is well-suited to help you scale; some of the largest applications
    out there run on Kubernetes.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes非常适合帮助你进行扩展；一些最大的应用程序都在Kubernetes上运行。
- en: To make the most of this architecture, design your application from the get-go
    so that it can scale horizontally.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了充分利用这种架构，从一开始就设计你的应用程序，使其能够进行横向扩展。
- en: HorizontalPodAutoscaler can be used to provision new Pods as needed, working
    together with cluster autoscaling for a complete autoscaling solution.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平Pod自动扩展器可以根据需要提供新的Pod，与集群自动扩展协同工作，形成一个完整的自动扩展解决方案。
- en: You’re not confined to CPU metrics and can scale your Pods based on any metric
    exported by your monitoring solution.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不仅限于CPU指标，可以根据你的监控解决方案导出的任何指标来扩展你的Pod。
- en: Cluster autoscaling feature (if supported by your provider) can be used to provision
    new nodes as needed.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群自动扩展功能（如果由你的提供商支持）可以根据需要提供新的节点。
- en: Placeholder Pods can be used to add capacity headroom even while autoscaling.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 占位符Pod可以在自动扩展的同时增加容量空间。
- en: Consider splitting your application into microservices or simply hosting multiple
    deployments of the same application to allow for separate scaling groups.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑将您的应用程序拆分为微服务，或者简单地托管同一应用程序的多个部署，以便允许独立的扩展组。
- en: '* * *'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) [https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics](https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics](https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics)'
- en: ^(2.) [https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub](https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub](https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub)'
