- en: Part 4\. Dimension reduction
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四部分：降维
- en: You’re now on your way to becoming a supervised machine learning virtuoso! So
    far, your toolbox of machine learning algorithms gives you the skills to tackle
    many real-world classification and regression problems. We’re now going to move
    into the realm of unsupervised learning, where we are no longer relying on labeled
    data to learn patterns from the data. Because we no longer have a ground truth
    to compare to, validating the performance of unsupervised learners can be challenging,
    but I’ll show practical ways to ensure the best performance possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在正走在成为监督机器学习大师的路上！到目前为止，您的机器学习算法工具箱已经为您提供了解决许多现实世界分类和回归问题的技能。我们现在将进入无监督学习的领域，在那里我们不再依赖于标记数据来从数据中学习模式。因为我们不再有真实标签进行比较，验证无监督学习者的性能可能会很具挑战性，但我将展示确保最佳性能的实用方法。
- en: 'Recall from [chapter 1](kindle_split_010.html#ch01) that unsupervised learning
    can be divided into two goals: dimension reduction and clustering. In [chapters
    13](kindle_split_025.html#ch13), [14](kindle_split_026.html#ch14), and [15](kindle_split_027.html#ch15),
    I’ll introduce you to several dimension-reduction algorithms you can use to turn
    a large number of variables into a smaller, more manageable number. Our motivations
    for doing this might be to simplify the process of visualizing patterns in data
    with many dimensions; or as a preprocessing step before passing our data into
    a supervised algorithm, to mitigate the curse of dimensionality.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第1章](kindle_split_010.html#ch01)，无监督学习可以分为两个目标：降维和聚类。在第13章、第14章和第15章中，我将向您介绍几种降维算法，您可以使用这些算法将大量变量转换为更少、更易于管理的数量。我们这样做的原因可能是为了简化可视化高维数据中模式的过程；或者作为将数据传递给监督算法之前的预处理步骤，以减轻维度灾难。
- en: Chapter 13\. Maximizing variance with principal component analysis
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13章：使用主成分分析最大化方差
- en: '*This chapter covers*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding dimension reduction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解降维
- en: Dealing with high dimensionality and collinearity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理高维性和多重共线性
- en: Using principal component analysis to reduce dimensionality
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主成分分析进行降维
- en: Dimension reduction comprises a number of approaches that turn a set of (potentially
    many) variables into a smaller number of variables that retain as much of the
    original, multidimensional information as possible. We sometimes want to reduce
    the number of dimensions we’re working with in a dataset, to help us visualize
    the relationships in the data or to avoid the strange phenomena that occur in
    high dimensions. So dimension reduction is a critical skill to add to your machine
    learning toolbox!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 降维包括多种方法，可以将一组（可能很多）变量转换为更少的变量，同时尽可能保留原始的多维信息。我们有时希望减少我们在数据集中处理的维度数量，以帮助我们可视化数据中的关系或避免高维中出现的奇怪现象。因此，降维是您机器学习工具箱中需要添加的一项关键技能！
- en: 'Our first stop in dimension reduction brings us to a very well-known and useful
    technique: *principal component analysis* (*PCA*). PCA, which has been around
    since the turn of the twentieth century, creates new variables that are linear
    combinations of the original variables. In this way, PCA is similar to discriminant
    analysis, which we encountered in [chapter 5](kindle_split_015.html#ch05); but
    instead of constructing new variables that separate classes, PCA constructs new
    variables that explain most of the variation/information in the data. In fact,
    there are no labels for PCA, because it is unsupervised and learns patterns in
    the data itself without a ground truth. We can then use the two or three of these
    new variables that capture most of the information as inputs to regression, classification,
    or clustering algorithms, as well as use them to better understand how the variables
    in our data are related to each other.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在降维的第一站将带我们到一个非常著名且实用的技术：*主成分分析* (*PCA*)。PCA自20世纪初以来一直存在，它创建了新的变量，这些变量是原始变量的线性组合。因此，PCA与我们在第5章中遇到的判别分析类似；但PCA不是构建新的变量来区分类别，而是构建新的变量来解释数据中的大部分变化/信息。实际上，PCA没有标签，因为它是无监督的，并且在不依赖真实标签的情况下从数据本身中学习模式。然后我们可以使用这些新变量中的两个或三个来捕获大部分信息，作为回归、分类或聚类算法的输入，同时也可以使用它们来更好地理解我们数据中的变量是如何相互关联的。
- en: '|  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The first historical example of dimension reduction was a map with two dimensions.
    Another form of dimension reduction that we encounter in our daily lives is the
    compression of audio into formats like .mp3 and .flac.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的第一个历史例子是具有两个维度的地图。我们在日常生活中遇到的另一种降维形式是将音频压缩成 .mp3 和 .flac 等格式。
- en: '|  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The mlr package doesn’t have a dimension-reduction class of tasks, and it doesn’t
    have a class of dimension-reduction learners (something like `dimred.[ALGORITHM]`,
    I suppose). PCA is the only dimension-reduction algorithm wrapped by mlr that
    we can include as a preprocessing step (like imputation or feature selection).
    In view of this, we’re going to leave the safety of the mlr package for the time
    being.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: mlr 包没有降维任务的类别，也没有降维学习者的类别（我想象中可能是 `dimred.[ALGORITHM]` 这样的）。PCA 是 mlr 包中唯一被封装的降维算法，我们可以将其作为预处理步骤（如插补或特征选择）包括在内。鉴于这一点，我们暂时将
    mlr 包的安全性放在一边。
- en: By the end of this chapter, I hope you’ll understand what dimension reduction
    is and why we sometimes need it. I will show you how the PCA algorithm works and
    how you can use it to reduce the dimensions of a dataset to help identify counterfeit
    banknotes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能理解降维是什么以及为什么我们有时需要它。我将向你展示 PCA 算法是如何工作的，以及你如何可以使用它来降低数据集的维度，以帮助识别假钞。
- en: 13.1\. Why dimension reduction?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1. 为什么需要降维？
- en: 'In this section, I’ll show you the main reasons for applying dimension reduction:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示应用降维的主要理由：
- en: Making it easier to visualize a dataset with many variables
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使可视化具有许多变量的数据集变得更加容易
- en: Mitigating the curse of dimensionality
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解维度灾难
- en: Mitigating the effects of collinearity
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解多重共线性的影响
- en: I’ll expand on what the curse of dimensionality and collinearity are and why
    they cause problems for machine learning, as well as why dimension reduction can
    reduce the impact of both when we’re searching for patterns in data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我将扩展说明维度灾难和多重共线性是什么，以及它们为什么会给机器学习带来问题，以及为什么降维可以减少我们在数据中寻找模式时这两种问题的冲击。
- en: 13.1.1\. Visualizing high-dimensional data
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.1. 可视化高维数据
- en: When starting an exploratory analysis, one of the first things you should always
    do is plot your data. It’s important that we, as data scientists, have an intuitive
    understanding of the structure of our data, the relationships between variables,
    and how the data is distributed. But what if we have a dataset containing thousands
    of variables? Where do we even start? Plotting each of these variables against
    each other isn’t really an option anymore, so how can we get a feel for the overall
    structure in our data? Well, we can reduce the dimensions down to a more manageable
    number, and plot these instead. We won’t get all the information of the original
    dataset when doing this, but it will help us identify patterns in our data, like
    clusters of cases that might suggest a grouping structure in the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始探索性分析时，你应该做的第一件事之一就是绘制你的数据。对我们这些数据科学家来说，理解我们数据的结构、变量之间的关系以及数据的分布方式是很重要的。但如果我们有一个包含数千个变量的数据集呢？我们甚至从哪里开始？将每个变量与其他变量一一绘制出来已经不再是可行的选项了，那么我们如何获得我们数据中整体结构的感受呢？嗯，我们可以将维度降低到一个更易于管理的数量，并绘制这些数据。当我们这样做时，我们不会得到原始数据集的所有信息，但这将帮助我们识别数据中的模式，比如可能表明数据中存在分组结构的案例集群。
- en: 13.1.2\. Consequences of the curse of dimensionality
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.2. 维度灾难的后果
- en: In [chapter 5](kindle_split_015.html#ch05), I discussed the curse of dimensionality.
    This slightly dramatic-sounding phenomenon describes a set of challenges we encounter
    when trying to identify patterns in a dataset with many variables. One aspect
    of the curse of dimensionality is that for a fixed number of cases, as we increase
    the number of dimensions in the dataset (increase the feature space), the cases
    get further and further apart. To reiterate this point in [figure 13.1](#ch13fig01),
    I’ve reproduced [figure 5.2](kindle_split_015.html#ch05fig02) from [chapter 5](kindle_split_015.html#ch05).
    In this situation, the data is said to become *sparse*. Many machine learning
    algorithms struggle to learn patterns from sparse data and may start to learn
    from the noise in the dataset instead.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_015.html#ch05)中，我讨论了维度的诅咒。这个听起来有些戏剧性的现象描述了我们在尝试从具有许多变量的数据集中识别模式时遇到的挑战。维度诅咒的一个方面是，对于固定数量的案例，当我们增加数据集中的维度（增加特征空间）时，案例之间的距离会越来越远。为了重申这一点，在[图13.1](#ch13fig01)中，我重新绘制了[第5章中的图5.2](kindle_split_015.html#ch05fig02)。在这种情况下，数据被称为*稀疏的*。许多机器学习算法在从稀疏数据中学习模式时都会遇到困难，并且可能开始从数据集中的噪声中学习。
- en: Figure 13.1\. Data becomes more sparse as the number of dimensions increases.
    Two classes are shown in one-, two-, and three-dimensional feature spaces. The
    dotted lines in the three-dimensional representation are to clarify the position
    of the points along the z-axis. Note the increasing empty space with increased
    dimensions.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.1\. 随着维度的增加，数据变得更加稀疏。在单维、二维和三维特征空间中显示了两个类别。三维表示中的虚线用于澄清点在z轴上的位置。注意随着维度的增加，空隙越来越大。
- en: '![](fig13-1_alt.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](fig13-1_alt.jpg)'
- en: Another aspect of the curse of dimensionality is that as the number of dimensions
    increases, the distances between the cases begin to converge to a single value.
    Put another way, for a particular case, the ratio between the distance to its
    nearest neighbor and its furthest neighbor tends toward 1 in high dimensions.
    This presents a challenge to algorithms that rely on measuring distances (particularly
    Euclidean distance), such as k-nearest neighbors, because distance starts to become
    meaningless.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒的另一个方面是，随着维度的增加，案例之间的距离开始收敛到一个单一值。换句话说，对于特定的案例，其最近邻和最远邻之间的距离比在高度维度中趋向于1。这对依赖于测量距离（尤其是欧几里得距离）的算法提出了挑战，如k最近邻算法，因为距离开始变得没有意义。
- en: Finally, it’s quite common to encounter situations in which we have many more
    variables than we have cases in the data. This is referred to as the *p >> n problem*,
    where *p* is the number of variables and *n* is the number of cases. This, again,
    results in sparse regions of the feature space, making it difficult for many algorithms
    to converge on an optimal solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们经常会遇到这样的情况：我们拥有的变量比数据中的案例多得多。这被称为*p >> n问题*，其中*p*是变量的数量，*n*是案例的数量。这再次导致特征空间的稀疏区域，使得许多算法难以收敛到最优解。
- en: 13.1.3\. Consequences of collinearity
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.3\. 共线性的后果
- en: Variables in a dataset often have varying degrees of correlation with each other.
    Sometimes we may have two variables that correlate very highly with each other,
    such that one basically contains the information of the other (say, with a Pearson
    correlation coefficient > 0.9). In such situations, these variables are said to
    be *collinear* or exhibit *collinearity*. An example of two variables that might
    be collinear are annual income and the maximum amount of money a bank is willing
    to loan someone; you could probably predict one from the other with a high degree
    of accuracy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的变量往往与其他变量有不同程度的关联。有时我们可能有两个高度相关的变量，以至于一个基本上包含了另一个的信息（例如，皮尔逊相关系数> 0.9）。在这种情况下，这些变量被称为*共线性*或表现出*共线性*。两个可能共线性的变量的例子是年收入和银行愿意贷款给某人的最大金额；你可能会以很高的准确性从其中一个预测另一个。
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: When more than two variables are collinear, we say we have *multicollinearity*
    in our dataset. When one variable can be *perfectly* predicted from another variable
    or combination of variables, we are said to have *perfect collinearity*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当超过两个变量共线性时，我们说我们的数据集中存在*多重共线性*。当一个变量可以从另一个变量或变量的组合中*完美地*预测时，我们说存在*完美共线性*。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: So what’s the problem with collinearity? Well, it depends on the goal of your
    analysis and what algorithms you are using. The most commonly encountered negative
    impact of collinearity is on the parameter estimates of linear regression models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，多重共线性有什么问题呢？嗯，这取决于你分析的目标以及你使用的算法。多重共线性最常遇到的负面影响是线性回归模型参数估计。
- en: Let’s say you’re trying to predict the value of houses based on the number of
    bedrooms, the age of the house in years, and the age of the house in months, using
    linear regression. The age variables are perfectly collinear with each other,
    because there’s no information contained in one that is not contained in the other.
    The parameter estimates (slopes) for the two predictor variables describe the
    relationship between each predictor and the outcome variable, after accounting
    for the effect of the other variable. If both predictor variables capture most
    of (or all of, in this case) the same information about the outcome variable,
    then when we account for the effect of one, there will be no information left
    for the other one to contribute. As a result, the parameter estimates for both
    predictors will be smaller than they should be (because each was estimated after
    accounting for the effect of the other).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在尝试根据卧室数量、房屋年龄（以年为单位）和房屋年龄（以月为单位）来预测房屋的价值，使用线性回归。年龄变量之间完全多重共线性，因为其中一个变量所包含的信息在另一个变量中都没有。两个预测变量的参数估计（斜率）描述了每个预测变量与结果变量之间的关系，在考虑其他变量的影响之后。如果两个预测变量捕捉到关于结果变量的大部分（或全部，在这种情况下）相同信息，那么当我们考虑一个变量的影响时，另一个变量将没有信息可以贡献。因此，两个预测变量的参数估计将比应有的要小（因为每个都是在考虑了另一个变量的影响之后估计的）。
- en: So collinearity makes the parameter estimates more variable and more sensitive
    to small changes in the data. This is mostly a problem if you’re interested in
    interpreting and making inferences about the parameter estimates. If all you care
    about is predictive accuracy, and not interpreting the model parameters, then
    collinearity may not be a problem for you at all.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，多重共线性使得参数估计更加多变，并且对数据的小幅变化更加敏感。这主要是一个问题，如果你对解释和推断参数估计感兴趣的话。如果你只关心预测准确性，而不是解释模型参数，那么多重共线性对你来说可能根本不是问题。
- en: It’s worth mentioning, however, that collinearity is particularly problematic
    when working with the naive Bayes algorithm you learned about in [chapter 6](kindle_split_016.html#ch06).
    Recall that the “naive” in naive Bayes refers to the fact that this algorithm
    assumes independence between predictors. This assumption is often invalid in the
    real world, but naive Bayes is usually resistant to small correlations between
    predictor variables. When predictors are highly correlated, however, the predictive
    performance of naive Bayes will suffer considerably, though this is usually easy
    to identify when you cross-validate your model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，当使用你在第6章中学习的朴素贝叶斯算法时，多重共线性尤其成问题。[第6章](kindle_split_016.html#ch06)中提到的“朴素”指的是该算法假设预测变量之间是独立的。这种假设在现实世界中通常是不成立的，但朴素贝叶斯通常对预测变量之间的小相关性具有抵抗力。然而，当预测变量高度相关时，朴素贝叶斯的预测性能将显著下降，尽管在交叉验证模型时通常很容易识别这一点。
- en: 13.1.4\. Mitigating the curse of dimensionality and collinearity by using -
    dimension reduction
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.4\. 通过使用 - 维度降低来缓解维度诅咒和多重共线性
- en: How can you mitigate the impacts of the curse of dimensionality and/or collinearity
    on the predictive performance of your models? Why, with dimension reduction, of
    course! If you can compress most of the information from 100 variables into just
    2 or 3, then the problems of data sparsity and near-equal distances disappear.
    If you turn two collinear variables into one new variable that captures all the
    information of both, then the problem of dependence between the variables disappears.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何减轻维度诅咒和/或多重共线性对模型预测性能的影响？当然，通过维度降低！如果你能将100个变量的大部分信息压缩到仅仅2或3个变量中，那么数据稀疏性和接近相等距离的问题就会消失。如果你将两个多重共线性变量转换为一个新变量，该变量可以捕捉到两个变量的所有信息，那么变量之间的依赖性问题就会消失。
- en: 'But we’ve already encountered another set of techniques that can mitigate the
    curse of dimensionality and collinearity: regularization. As we saw in [chapter
    11](kindle_split_022.html#ch11), regularization can be used to shrink the parameter
    estimates and even completely remove weakly contributing predictors. Regularization
    can therefore reduce sparsity resulting from the curse of dimensionality, and
    remove variables that are collinear with others.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们已经遇到了另一组可以减轻维度灾难和多重共线性问题的技术：正则化。正如我们在第 11 章（kindle_split_022.html#ch11）中看到的，正则化可以用来缩小参数估计，甚至完全移除贡献较弱的预测因子。因此，正则化可以减少维度灾难导致的稀疏性，并移除与其他变量共线的变量。
- en: '|  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: For most people, tackling the curse of dimensionality is a more important use
    of dimension reduction than reducing collinearity.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数人来说，解决维度灾难比减少多重共线性更重要。
- en: '|  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 13.2\. What is principal component analysis?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2\. 主成分分析是什么？
- en: In this section, I’ll show you what PCA is, how it works, and why it’s useful.
    Imagine that we measure two variables on seven people, and we want to compress
    this information down into a single variable using PCA. The first thing we need
    to do is center the variables by subtracting each variable’s mean from its corresponding
    value for each case.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示 PCA 是什么，它是如何工作的，以及为什么它是有用的。想象一下，我们测量了七个人的两个变量，并希望使用 PCA 将这些信息压缩成一个单一的变量。我们首先需要做的是通过从每个案例中每个变量的对应值中减去每个变量的均值来对变量进行中心化。
- en: In addition to centering our variables, we can also scale them by dividing each
    variable by its standard deviation. This is important if the variables are measured
    on different scales—otherwise, those on large scales will be weighted more heavily.
    If our variables are on similar scales, this standardization step isn’t necessary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对变量进行中心化之外，我们还可以通过将每个变量除以其标准差来对它们进行缩放。如果变量是在不同的尺度上测量的，这很重要——否则，那些在大尺度上的变量会被赋予更重的权重。如果我们的变量在相似的尺度上，这个标准化步骤就不必要了。
- en: 'With our centered and (possibly) scaled data, PCA now finds a new axis that
    satisfies two conditions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据被中心化（可能还进行了缩放）之后，PCA 现在找到一个满足两个条件的新轴：
- en: The axis passes through the origin.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个轴穿过原点。
- en: The axis maximizes the variance of the data along itself.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个轴最大化了数据沿自身的方差。
- en: The new axis that satisfies these conditions is called the first *principal
    axis*. When the data is projected onto this principal axis (moved at a right angle
    onto the nearest point on the axis), this new variable is called the first *principal
    component*, often abbreviated PC1\. This process of centering the data and finding
    PC1 is shown in [figure 13.2](#ch13fig02).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这些条件的新轴被称为第一个 *主轴*。当数据投影到这个主轴上（以直角移动到轴上的最近点）时，这个新变量被称为第一个 *主成分*，通常缩写为 PC1。数据中心化和找到
    PC1 的这个过程如图 13.2 所示。
- en: 'Figure 13.2\. The first thing we do before applying the PCA algorithm is (usually)
    to center the data by subtracting the mean of each variable for each case. This
    places the origin at the center of the data. The first principal axis is then
    found: it is the axis that passes through the origin and maximizes the variance
    of the data when projected onto it.'
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.2\. 在应用 PCA 算法之前，我们首先（通常）通过减去每个案例中每个变量的均值来对数据进行中心化。这使数据原点位于数据中心。然后找到第一个主轴：它是通过原点并且当数据投影到它上面时，最大化数据方差的轴。
- en: '![](fig13-2_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig13-2_alt.jpg)'
- en: The first principal axis is the line through the origin of the data that, once
    the data is projected onto it, has the greatest variance along it and is said
    to “maximize the variance.” This is illustrated in [figure 13.3](#ch13fig03).
    This axis is chosen because if this is the line that accounts for the majority
    of the variance in the data, then it is also the line that accounts for the majority
    of the information in the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主轴是通过数据原点的线，一旦数据投影到它上面，沿着这条线具有最大的方差，并被称为“最大化方差”。这如图 13.3 所示。这个轴被选择是因为如果这条线解释了数据中大部分的方差，那么它也解释了数据中大部分的信息。
- en: Figure 13.3\. What it means for the first principal axis to “maximize the variance.”
    The left-side plot shows a sub-optimal candidate principal axis. The right-side
    plot shows the optimal candidate principal axis. The data is shown projected onto
    each principal axis below the respective plots. The variance of the data along
    the axis is greatest on the right side.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.3. 首要主轴“最大化方差”的含义。左侧图显示了次优候选主轴。右侧图显示了最优候选主轴。数据在各自的图表下方投影到每个主轴上。数据沿轴的方差在右侧最大。
- en: '![](fig13-3_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图13-3_替代](fig13-3_alt.jpg)'
- en: This new principal axis is actually a linear combination of the predictor variables.
    Look again at [figure 13.3](#ch13fig03). The first principal axis extends through
    the two clusters of cases to form a negative slope between var 1 and var 2\. Just
    like in linear regression, we can express this line in terms of how one variable
    changes when the other variable changes (as the line passes through the origin,
    the intercept is 0). Take a look at [figure 13.4](#ch13fig04), where I’ve highlighted
    how much var 2 changes when var 1 increases by two units along the principal axis.
    For every two-unit change in var 1, var 2 decreases by 0.68 units.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的主轴实际上是预测变量的线性组合。再次看看[图13.3](#ch13fig03)。第一个主轴穿过两个案例簇，在var 1和var 2之间形成一个负斜率。就像在线性回归中一样，我们可以通过一个变量随另一个变量变化（当线通过原点时，截距为0）来表示这条线。看看[图13.4](#ch13fig04)，我突出显示了当var
    1沿主轴增加两个单位时，var 2的变化量。对于var 1的每两个单位的变化，var 2减少0.68个单位。
- en: 'It’s useful to have a standardized way of describing the slope through our
    feature space. In linear regression, we can define a slope in terms of how much
    y changes with a one-unit increase in x. But we often don’t have any notion of
    predictor variables and outcome variables when performing PCA: we just have a
    set of variables we wish to compress. Instead, we define the principal axis in
    terms of how far we need to go along each variable (the x- and y-axes in the two-dimensional
    example in [figure 13.4](#ch13fig04)) so that the distance from the origin is
    equal to 1.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个标准化的方式来描述通过我们的特征空间的斜率是有用的。在线性回归中，我们可以通过y随x增加一个单位而变化的量来定义斜率。但在执行PCA时，我们通常没有预测变量和结果变量的任何概念：我们只有一组我们希望压缩的变量。相反，我们通过每个变量（在[图13.4](#ch13fig04)中二维示例中的x轴和y轴）需要走多远来定义主轴（这样从原点的距离等于1）。
- en: 'Have another look at [figure 13.4](#ch13fig04). We’re trying to calculate the
    length of sides a and b of the triangle when length c is equal to 1\. This will
    then tell us how far along var 1 and var 2 we need to go, to be one unit away
    from the origin along the principal axis. How do we calculate the length of c?
    Why, our good friend Pythagoras’s theorem can help! By applying c² = a² + b²,
    we can work out that if we go along var 1 2.00 units and along var 2 –0.68 units,
    the length of c is equal to 2.11\. To normalize this such that the length of c
    is equal to 1, we simply divide all three sides of the triangle by 2.11\. We now
    define our principal axis as follows: for every 0.95 unit increase in var 1, we
    decrease along var 2 by 0.32.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 再看看[图13.4](#ch13fig04)。我们试图计算当长度c等于1时，三角形的边a和b的长度。这将告诉我们沿着var 1和var 2需要走多远，才能在主轴上离原点有1个单位的距离。我们如何计算c的长度？为什么，我们的好朋友毕达哥拉斯定理可以帮助我们！通过应用c²
    = a² + b²，我们可以计算出如果我们沿着var 1走2.00个单位，沿着var 2走-0.68个单位，c的长度等于2.11。为了使c的长度等于1，我们只需将三角形的三个边都除以2.11。现在我们定义我们的主轴如下：对于var
    1每增加0.95个单位，我们在var 2上减少0.32个单位。
- en: Figure 13.4\. Calculating the eigenvector for a principal component. The distances
    along each variable are scaled so that they mark a point that is one unit along
    the principal axis away from the origin. We can illustrate this graphically by
    taking a triangle defined by the change in one variable over the change in the
    other variable, and using Pythagoras’s theorem to find the distance from the origin
    to divide by.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.4. 计算主成分的特征向量。每个变量的距离都进行了缩放，以便它们标记一个点，该点距离原点沿主轴有1个单位的距离。我们可以通过取一个由一个变量的变化量除以另一个变量的变化量定义的三角形，并使用毕达哥拉斯定理来找到从原点到该点的距离来进行图形说明。
- en: '![](fig13-4_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图13-4_替代](fig13-4_alt.jpg)'
- en: Note that this transformation doesn’t change the direction of the line; all
    it does is normalize everything so that the distance from the origin is 1\. These
    normalized distances along each variable that define a principal axis are called
    an *eigenvector*. The formula for the principal component that results from the
    principal axis is therefore
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种变换不会改变线的方向；它所做的只是将所有东西标准化，使得从原点到点的距离为1。这些定义主轴的每个变量的标准化距离被称为*特征向量*。因此，从主轴得到的主成分的公式是
- en: equation 13.1\.
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式13.1。
- en: '![](eq13-1.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![方程式13.1](eq13-1.jpg)'
- en: So for any particular case, we center it (subtract the mean of each variable),
    take its value of var 1 and multiply by 0.95, and then add the result to the value
    of var 2 multiplied by –0.32, to get this case’s value of PC1\. The value of a
    principal component for a case is called its *component score*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于任何特定的情况，我们将其中心化（减去每个变量的均值），取其方差1的值并乘以0.95，然后将结果加到方差2乘以-0.32的值上，以得到该案例的PC1值。一个案例的主成分值被称为其*成分得分*。
- en: Once we’ve found the first principal axis, we need to find the next one. PCA
    will find as many principal axes as there are variables or one less than the number
    of cases in the dataset, whichever is smaller. So the first principal component
    is always the one that explains most of the variance in the data. Concretely,
    if we calculate the variance of the cases along each principal component, PC1
    will have the largest value. The variance of the data along a particular principal
    component is called its *eigenvalue*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了第一个主轴，我们需要找到下一个。PCA将找到与变量数量一样多或比数据集中的案例数量少一个的主轴，取较小者。因此，第一个主成分总是解释数据中大部分方差的那个。具体来说，如果我们计算每个主成分的案例方差，PC1将具有最大的值。沿着特定主成分的数据方差被称为其*特征值*。
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If eigenvectors define the direction of the principal axis through the original
    feature space, eigenvalues define the magnitude of spread along the principal
    axis.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征空间中的特征向量定义了主轴的方向，那么特征值定义了沿着主轴的扩散程度。
- en: '|  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Once the first principal axis is found, the next one must be orthogonal to it.
    When we have only two dimensions in our dataset, this means the second principal
    axis will form a right angle with the first. The example in [figure 13.5](#ch13fig05)
    shows a cloud of cases being projected onto their first and second principal axes.
    When converting only two variables into two principal components, plotting the
    component scores of the data amounts to rotating the data around the origin.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到第一个主轴，下一个主轴必须与它正交。当我们数据集中只有两个维度时，这意味着第二个主轴将与第一个主轴形成直角。图[13.5](#ch13fig05)中的例子显示了一组案例被投影到它们的第一和第二个主轴上。当仅将两个变量转换为两个主成分时，绘制数据的成分得分相当于围绕原点旋转数据。
- en: Figure 13.5\. In a two-dimensional feature space, the first principal axis is
    the one that maximizes the variance (as it always is), and the second principal
    axis is orthogonal (at a right angle) to the first. In this situation, plotting
    the principal components simply results in a rotation of the data.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.5。在二维特征空间中，第一个主轴是最大化方差的那一个（正如它总是那样），第二个主轴与第一个主轴正交（成直角）。在这种情况下，绘制主成分仅仅导致数据的旋转。
- en: '![](fig13-5.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图13.5](fig13-5.jpg)'
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'This imposed orthogonality is one of the reasons PCA is good at removing collinearity
    between variables: it can turn a set of correlated variables into a set of uncorrelated
    (orthogonal) variables.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种强加的正交性是PCA擅长去除变量之间共线性原因之一：它可以将一组相关变量转换为一组不相关（正交）变量。
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: After rotating the data in [figure 13.5](#ch13fig05), the majority of the variance
    in the data is explained by PC1, and PC2 is orthogonal to it. But PCA is usually
    used to *reduce dimensions*, not just rotate bivariate data, so how are the principal
    axes calculated when we have a higher-dimensional space? Take a look at [figure
    13.6](#ch13fig06). We have a cloud of data in three dimensions that is closest
    to us at the bottom right of the feature space and gets further from us at the
    top left (notice that the points get smaller). The first principal axis is still
    the one that explains most of the variance in the data, but this time it extends
    through three-dimensional space (from front right to top left). The same process
    occurs in a feature space that has more than three dimensions, but it’s difficult
    to visualize that!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在旋转了[图13.5](#ch13fig05)中的数据后，数据中的大部分方差都由PC1解释，而PC2与它正交。但主成分分析（PCA）通常用于**降低维度**，而不仅仅是旋转双变量数据，那么当我们处于更高维空间时，主轴是如何计算的？看看[图13.6](#ch13fig06)吧。我们在三维空间中有一个数据云，它在特征空间的右下角离我们最近，而在左上角离我们越来越远（注意点变得越小）。第一个主成分轴仍然是解释数据中大部分方差的那个轴，但这次它延伸到了三维空间（从右前到左上）。在具有超过三个维度的特征空间中，这个过程也会发生，但可视化起来比较困难！
- en: The second principal axis is still orthogonal to the first, but as we now have
    three dimensions to play around with, it is free to rotate around the first in
    a plane that still maintains a right angle between them. I’ve illustrated this
    rotational freedom with a circle around the origin that gets fainter, the further
    away from us it is. The second principal axis is the one that is orthogonal to
    the first but explains the majority of the remaining variance in the data. The
    third principal axis must be orthogonal to the preceding axes (at right angles
    to both of them) and therefore has no freedom to move. The first principal component
    always explains the most variance, followed by the second, the third, and so on.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主成分轴仍然与第一个正交，但现在我们有三个维度可以操作，它可以在保持它们之间直角关系的平面上自由围绕第一个轴旋转。我用一个围绕原点的圆圈来表示这种旋转自由度，离我们越远，圆圈就越淡。第二个主成分轴是那个与第一个正交但解释数据中剩余大部分方差的主成分轴。第三个主成分轴必须与前两个轴正交（与它们都成直角），因此没有移动的自由。第一个主成分总是解释最多的方差，然后是第二个、第三个，以此类推。
- en: Figure 13.6\. In a three-dimensional feature space, the second principal axis
    is still orthogonal to the first principal axis, but it has freedom to rotate
    around the first (indicated by the ellipse with arrows in the left-side plot)
    until it maximizes the remaining variance. The third principal axis is orthogonal
    to the first and second principal axes and so has no freedom to rotate; it explains
    the least amount of variance.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.6。在三维特征空间中，第二个主成分轴仍然与第一个主成分轴正交，但它有自由在第一个轴周围旋转（左侧图表中的箭头椭圆所示），直到最大化剩余的方差。第三个主成分轴与第一个和第二个主成分轴正交，因此没有旋转的自由；它解释的方差最少。
- en: '![](fig13-6_alt.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图13-6](fig13-6_alt.jpg)'
- en: At this point you might be asking, if PCA calculates principal components for
    the smaller of the number of variables or the number of cases minus one, how exactly
    does it reduce the number of dimensions? Well, simply calculating the principal
    components isn’t dimension reduction at all! Dimension reduction comes into it
    regarding *how many of the principal components we decide to keep in the remainder
    of our analysis*. In the example in [figure 13.6](#ch13fig06), we have three principal
    components, but the first two account for 79% + 12% = 91% of the variation in
    the dataset. If these two principal components capture enough of the information
    in the original dataset to make the dimension reduction worthwhile (perhaps we
    get better results from a clustering or classification algorithm), then we can
    happily discard the remaining 9% of the information. Later in the chapter, I’ll
    show you some ways to decide how many principal components to keep.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道，如果PCA计算的是变量数量较少的那个或案例数量减一，那么它究竟是如何降低维度数的？好吧，仅仅计算主成分根本就不是降维！降维涉及到**我们在分析剩余部分决定保留多少主成分**。在[图13.6](#ch13fig06)的例子中，我们有三个主成分，但前两个解释了数据集中79%
    + 12% = 91%的变异。如果这两个主成分能够捕捉到原始数据集中足够的信息，使得降维变得有价值（也许我们可以从聚类或分类算法中获得更好的结果），那么我们可以愉快地丢弃剩余的9%的信息。在章节的后面部分，我会向你展示一些决定保留多少主成分的方法。
- en: 13.3\. Building your first PCA model
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3. 构建你的第一个PCA模型
- en: In this section, we’ll turn the PCA theory we just covered into skills by reducing
    the dimensions of a dataset, using PCA. Imagine that you work for the Swiss Federal
    Department of Finance (due to your love of money, chocolate, cheese, and political
    neutrality). The department believes that a large number of counterfeit Swiss
    bankotes are in circulation, and it’s your job to find a way of identifying them.
    Nobody has looked into this before, and there is no labeled data to go on. So
    you ask 200 of your colleagues to each give you a banknote (you promise to give
    them back), and you measure the dimensions of each note. You hope that there will
    be some discrepancies between genuine notes and counterfeit ones that you may
    be able to identify using PCA.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用PCA降低数据集的维度，将我们刚刚讨论的PCA理论转化为技能。想象一下，你为瑞士联邦财政部工作（由于你对金钱、巧克力、奶酪和政治中立性的热爱）。该部门认为流通中有大量伪造的瑞士银行券，你的任务是找到一种方法来识别它们。在此之前没有人研究过这个问题，也没有标记的数据可以依据。因此，你要求200名同事每人给你一张纸币（你承诺会还给他们），并测量每张纸币的尺寸。你希望真钞和假钞之间可能存在一些差异，你可以使用PCA来识别。
- en: In this section, we’ll tackle this problem by
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过以下方法来解决这个问题：
- en: Exploring and plotting the original dataset before PCA
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PCA之前探索和绘制原始数据集
- en: Using the `prcomp()` function to learn the principal components from the data
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`prcomp()`函数从数据中学习主成分
- en: Exploring and plotting the result of the PCA model
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索和绘制PCA模型的结果
- en: 13.3.1\. Loading and exploring the banknote dataset
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.1\. 加载和探索银行券数据集
- en: We’ll start by loading the tidyverse packages, loading the data from the mclust
    package, and converting the data frame into a tibble. We have a tibble containing
    200 banknotes with 7 variables.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载tidyverse包，从mclust包中加载数据，并将数据框转换为tibble。我们有一个包含200张纸币的tibble，有7个变量。
- en: Listing 13.1\. Loading the banknote dataset
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.1\. 加载银行券数据集
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The keen-eyed among you may have noticed that this tibble is, in fact, labeled.
    We have the variable `Status` telling us whether each note is genuine or counterfeit.
    This is purely for teaching purposes; we’re going to exclude it from the PCA analysis
    but map the labels onto the final principal components later, to see whether the
    PCA model separates the classes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 眼尖的你们可能已经注意到，这个tibble实际上是标记过的。我们有变量`Status`告诉我们每张纸币是真钞还是假钞。这纯粹是为了教学目的；我们将从PCA分析中排除它，但稍后会将标签映射到最终的主成分上，以查看PCA模型是否将类别分开。
- en: In situations where I have a clear outcome variable, I often plot each of my
    predictor variables against the outcome (as we’ve done in previous chapters).
    In unsupervised learning situations, we don’t have an outcome variable, so I prefer
    to plot all variables against each other (provided I don’t have so many variables
    as to prohibit doing so). We can do this easily using the `ggpairs()` function
    from the GGally package, which you may need to install first. We pass our tibble
    as the first argument to the `ggpairs()` function, and then we supply any additional
    aesthetic mappings by passing ggplot2’s `aes()` function to the mapping argument.
    Finally, we add a `theme_bw()` layer to add the black-and-white theme.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我有明确的因变量的情况下，我经常将每个预测变量与因变量（如我们在前面的章节中所做的那样）进行绘图。在不监督学习的情况下，我们没有因变量，所以我更喜欢将所有变量相互绘制（前提是我没有那么多变量以至于无法这样做）。我们可以使用GGally包中的`ggpairs()`函数轻松完成此操作，你可能需要先安装它。我们将我们的tibble作为第一个参数传递给`ggpairs()`函数，然后通过将ggplot2的`aes()`函数传递给映射参数来提供任何额外的美学映射。最后，我们添加一个`theme_bw()`层来添加黑白主题。
- en: Listing 13.2\. Plotting the data with `ggpairs()`
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.2\. 使用`ggpairs()`绘制数据
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resulting plot is shown in [figure 13.7](#ch13fig07). The output from `ggpairs()`
    takes a little getting used to, but it draws a different kind of plot for each
    combination of variable types. For example, along the top row of facets are box
    plots showing the distribution of each continuous variable against the categorical
    variable. We get the same thing in histogram form down the left column of facets.
    The diagonal facets show the distributions of values for each variable, ignoring
    all others. Finally, dot plots shown the bivariate relationships between pairs
    of continuous variables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在 [图 13.7](#ch13fig07)。`ggpairs()` 函数的输出需要一点时间来习惯，但它为每种变量类型的组合绘制了不同类型的图表。例如，在面板顶部的一行是箱线图，显示了每个连续变量相对于分类变量的分布。在面板左侧的列中，我们以直方图的形式得到相同的结果。对角面板显示了每个变量的值分布，忽略所有其他变量。最后，点图显示了成对连续变量之间的二元关系。
- en: Figure 13.7\. The result of calling the `ggpairs()` function on our banknote
    dataset. Each variable is plotted against every other variable, with different
    plot types drawn depending on the combination of variable types.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.7\. 在我们的纸币数据集上调用 `ggpairs()` 函数的结果。每个变量都与每个其他变量进行绘图，根据变量类型的组合绘制不同的图表类型。
- en: '![](fig13-7_alt.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](fig13-7_alt.jpg)'
- en: Looking at the plots, we can see that some of the variables seem to differentiate
    between the genuine and counterfeit banknotes, such as the `Diagonal` variable.
    The `Length` variable, however, contains little information that discriminates
    the two classes of banknotes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图表，我们可以看到一些变量似乎可以区分真伪纸币，例如 `Diagonal` 变量。然而，`Length` 变量却包含很少的信息来区分这两种纸币类别。
- en: '|  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You see that if we had many more variables, visualizing them against each other
    in this way would start to become difficult!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，如果我们有更多的变量，以这种方式可视化它们之间的关系将开始变得困难！
- en: '|  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 13.3.2\. Performing PCA
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.2\. 执行主成分分析
- en: In this section, we’re going to use the PCA algorithm to learn the principal
    components of our banknote dataset. To do this, I’ll introduce you to the `prcomp()`
    function from the stats package that comes with your base R installation. Once
    we’ve done this, we’ll inspect the output of this function to interpret the component
    scores of the principal components. I’ll then show you how to extract and interpret
    *variable loadings* from the principal components, which tell us how much each
    original variable correlates with each principal component.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用主成分分析算法来学习我们的纸币数据集的主成分。为此，我将向您介绍随您的 base R 安装一起提供的 stats 包中的 `prcomp()`
    函数。一旦我们完成这个，我们将检查该函数的输出以解释主成分的成分得分。然后，我将向您展示如何从主成分中提取和解释 *变量载荷*，这告诉我们每个原始变量与每个主成分的相关程度。
- en: Listing 13.3\. Performing the PCA
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.3\. 执行主成分分析
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We first use the `select()` function to remove the `Status` variable, and pipe
    the resulting data into the `prcomp()` function. There are two additional important
    arguments to the `prcomp()` function: `center` and `scale`. The `center` argument
    controls whether the data is mean-centered before applying PCA, and its default
    value is `TRUE`. We should always center the data before applying PCA because
    this removes the intercept and forces the principal axes to pass through the origin.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 `select()` 函数删除 `Status` 变量，然后将结果数据管道输入到 `prcomp()` 函数中。`prcomp()` 函数有两个额外的重要参数：`center`
    和 `scale`。`center` 参数控制是否在应用主成分分析之前对数据进行均值中心化，其默认值是 `TRUE`。我们应该在应用主成分分析之前始终对数据进行中心化，因为这会移除截距并迫使主轴通过原点。
- en: The `scale` argument controls whether the variables are divided by their standard
    deviations to put them all on the same scale as each other, and its default value
    is `FALSE`. There isn’t a clear consensus on whether you should standardize your
    variables before running PCA. A common rule of thumb is that if your original
    variables are measured on a similar scale, standardization isn’t necessary; but
    if you have one variable measuring grams and another measuring kilograms, you
    should standardize them by setting `scale = TRUE` to put them on the same scale.
    This is important because if you have one variable measured on a much larger scale,
    this variable will dominate the eigenvectors, and the other variables will contribute
    much less information to the principal components. In this example, we’ll set
    `scale = TRUE`, but one of the exercises for this chapter is to set `scale = FALSE`
    and compare the results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale`参数控制变量是否通过它们的方差除以以使它们彼此处于相同的尺度，其默认值是`FALSE`。在运行PCA之前是否应该标准化变量并没有明确的共识。一个常见的经验法则是，如果原始变量是在相似的尺度上测量的，则不需要标准化；但如果有一个变量测量的是克，另一个变量测量的是千克，你应该通过设置`scale
    = TRUE`来标准化它们，使它们处于相同的尺度。这很重要，因为如果你有一个变量在很大的尺度上测量，这个变量将主导特征向量，而其他变量对主成分的贡献将少得多。在这个例子中，我们将设置`scale
    = TRUE`，但本章的一个练习是设置`scale = FALSE`并比较结果。'
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we’re not interested in including the `Status` variable in
    our dimension-reduction model; but even if we were, PCA cannot handle categorical
    variables. If you have categorical variables, your options are to encode them
    as numeric (which may or may not work), use a different approach for dimension
    reduction (there are some that handle categorical variables that I won’t discuss
    here), or extract the principal components from the continuous variables and then
    recombine these with the categorical variables in the final dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们并不感兴趣将`Status`变量包含在我们的降维模型中；但即使我们感兴趣，PCA也无法处理分类变量。如果你有分类变量，你的选择是将它们编码为数值（这可能或可能不起作用），使用不同的降维方法（有一些可以处理分类变量，这里不会讨论），或者从连续变量中提取主成分，然后在最终数据集中将这些主成分与分类变量重新组合。
- en: '|  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: When we print the `pca` object, we get a printout of some information from our
    model. The `Standard deviations` component is a vector of the standard deviations
    of the data along each of the principal components. Because the variance is the
    square of the standard deviation, to convert these standard deviations into the
    eigenvalues for the principal components, we can simply square them. Notice that
    the values get smaller from left to right? This is because the principal components
    explain sequentially less of the variance in the data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印`pca`对象时，我们得到了模型的一些信息输出。`标准差`组件是沿着每个主成分数据的标准差向量。因为方差是标准差的平方，要将这些标准差转换为主成分的特征值，我们只需将它们平方。注意，值是从左到右逐渐变小的吗？这是因为主成分按顺序解释了数据中越来越少的方差。
- en: The `Rotation` component contains the six eigenvectors. Remember that these
    eigenvectors describe how far along each original variable we go, so that we’re
    one unit along the principal axis away from the origin. These eigenvectors describe
    the direction of the principal axes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`旋转`组件包含六个特征向量。记住，这些特征向量描述了我们在每个原始变量上的距离，这样我们就沿着主轴离原点有一个单位。这些特征向量描述了主轴的方向。'
- en: If we pass our PCA results to the `summary()` function, we get a breakdown of
    the importance of each of the principal components. The `Standard deviation` row
    is the same as we saw a moment ago and contains the square root of the eigenvalues.
    The `Proportion of Variance` row tells us how much of the total variance is accounted
    for by each principal component. This is calculated by dividing each eigenvalue
    by the sum of the eigenvalues. The `Cumulative Proportion` row tells us how much
    variance is accounted for by the principal components so far. For example, we
    can see that PC1 and PC2 account for 49.1% and 21.3% of the total variance, respectively;
    cumulatively, they both account for 70.4%. This information is useful when we’re
    deciding how many principal components to retain for our downstream analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将PCA结果传递给`summary()`函数，我们将得到每个主成分重要性的分解。`标准差`行与我们刚才看到的相同，包含特征值的平方根。`方差比例`行告诉我们每个主成分解释了多少总方差。这是通过将每个特征值除以特征值的总和来计算的。`累积比例`行告诉我们到目前为止主成分解释了多少方差。例如，我们可以看到PC1和PC2分别解释了总方差的49.1%和21.3%；累积起来，它们共同解释了70.4%。当我们决定为下游分析保留多少主成分时，这些信息是有用的。
- en: If we’re interested in interpreting our principal components, it’s useful to
    extract the *variable loadings*. The variable loadings tell us how much each of
    the original variables correlates with each of the principal components. The formula
    for calculating the variable loadings for a particular principal component is
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对解释主成分感兴趣，提取*变量载荷*是有用的。变量载荷告诉我们原始变量中的每一个与每一个主成分的相关程度。计算特定主成分变量载荷的公式是
- en: equation 13.2\.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式 13.2。
- en: '![](eq13-2.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](eq13-2.jpg)'
- en: We can calculate all of the variable loadings simultaneously for all principal
    components and return them as a tibble using the `map_dfc()` function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`map_dfc()`函数同时计算所有主成分的变量载荷，并将它们作为tibble返回。
- en: Listing 13.4\. Calculating variable loadings
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.4\. 计算变量载荷
- en: '[PRE3]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can interpret these values as Pearson correlation coefficients, so we can
    see that the `Length` variable has very little correlation with PC1 (0.012) but
    a very strong negative correlation with PC2 (–0.922). This helps us conclude that,
    on average, cases with a small component score for PC2 have a larger `Length`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些值解释为皮尔逊相关系数，因此我们可以看到`长度`变量与PC1的相关性非常小（0.012），但与PC2有非常强的负相关性（-0.922）。这有助于我们得出结论，平均而言，PC2成分得分较小的案例，其`长度`较大。
- en: 13.3.3\. Plotting the result of our PCA
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.3\. 绘制我们的PCA结果
- en: Next, let’s plot the results of our PCA model to better understand the relationships
    in the data by seeing if the model has revealed any patterns. There are some nice
    plotting functions for PCA results in the factoextra package, so let’s install
    and load this package and play with it (see [listing 13.5](#ch13ex05)). Once you’ve
    loaded the package, use the `get_pca()` function to grab the information from
    our PCA model so we can apply factoextra functions to it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过查看模型是否揭示了任何模式来绘制我们的PCA模型结果，以更好地理解数据中的关系。factoextra包中有一些用于PCA结果的优秀绘图函数，所以让我们安装并加载这个包，并对其进行操作（见[列表
    13.5](#ch13ex05)）。一旦加载了包，使用`get_pca()`函数从我们的PCA模型中获取信息，以便我们可以对其应用factoextra函数。
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Although we manually calculated the variable loadings in [listing 13.4](#ch13ex04),
    an easier way of extracting this information is by printing the `$coord` component
    of the `pcaDat` object we create in [listing 13.5](#ch13ex05).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在[列表 13.4](#ch13ex04)中手动计算了变量载荷，但提取这些信息的一种更简单的方法是打印我们在[列表 13.5](#ch13ex05)中创建的`pcaDat`对象的`$coord`组件。
- en: '|  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The `fviz_pca_biplot()` function draws a *biplot*. A biplot is a common method
    of simultaneously plotting the component scores, and the variable loadings for
    the first two principal components. You can see the biplot in the top left of
    [figure 13.8](#ch13fig08). The dots show the component scores for each of the
    banknotes against the first two principal components, and the arrows indicate
    the variable loadings of each variable. This plot helps us identify that we seem
    to have two distinct clusters of banknotes, and the arrows help us to see which
    variables tend to correlate with each of the clusters. For example, the rightmost
    cluster in this plot tends to have higher values for the `Diagonal` variable.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`fviz_pca_biplot()` 函数绘制一个 *载荷图*。载荷图是一种常见的绘图方法，可以同时绘制前两个主成分的成分得分和变量载荷。您可以在
    [图 13.8](#ch13fig08) 的左上角看到载荷图。点表示每张纸币相对于前两个主成分的成分得分，箭头指示每个变量的载荷。这个图帮助我们确定似乎有两群不同的纸币，箭头帮助我们看到哪些变量倾向于与每个群组相关。例如，这个图中最右侧的群组在
    `Diagonal` 变量上往往有更高的值。'
- en: '|  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The `label = "var"` argument tells the function to only label the variables;
    otherwise, it labels each case with its row number, and this makes me go cross-eyed.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`label = "var"` 参数告诉函数只标记变量；否则，它将每个案例的行号作为标签，这让我感到非常不适。'
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The `fviz_pca_var()` function draws a *variable loading plot*. You can see
    the variable loading plot at top right in [figure 13.8](#ch13fig08). Notice that
    this shows the same variable loading arrows as in the biplot, but now the axes
    represent the correlation of each of the variables with each principal component.
    If you look again at the variable loadings calculated in [listing 13.4](#ch13ex04),
    you’ll see that this plot is showing the same information: how much each original
    variable correlates with the first two principal components.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`fviz_pca_var()` 函数绘制一个 *变量载荷图*。您可以在 [图 13.8](#ch13fig08) 的右上角看到变量载荷图。请注意，这显示了与载荷图相同的变量载荷箭头，但现在轴代表每个变量与每个主成分的相关性。如果您再次查看
    [列表 13.4](#ch13ex04) 中计算出的变量载荷，您会看到这个图显示了相同的信息：每个原始变量与第一个两个主成分的相关程度。'
- en: Figure 13.8\. Typical exploratory plots for PCA analysis as supplied by the
    factoextra package. The top-left plot shows a biplot, combining each case’s component
    scores with arrows to show the variable loadings. The top-right plot shows the
    variable loading plot with a correlation circle (the boundary within which the
    variable loadings must lie). The bottom scree plots show the eigenvalue (left)
    and percentage explained variance (right).
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.8\. 由 factoextra 包提供的 PCA 分析的典型探索性图。左上角的图显示了一个载荷图，结合了每个案例的成分得分和表示变量载荷的箭头。右上角的图显示了带有相关圆（变量载荷必须位于其边界内）的变量载荷图。底部的
    scree 图显示了特征值（左）和百分比解释方差（右）。
- en: '![](fig13-8_alt.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig13-8_alt.jpg)'
- en: The `fviz_screeplot()` function draws a *scree plot*. A scree plot is a common
    way of plotting the principal components against the amount of variance they explain
    in the data, as a graphical way to help identify how many principal components
    to retain. The function allows us to plot either the eigenvalue or the percentage
    variance accounted for by each principal component, using the `choice` argument.
    You can see scree plots with these two different y-axes in the bottom two plots
    in [figure 13.8](#ch13fig08).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`fviz_screeplot()` 函数绘制一个 *碎石图*。碎石图是一种常见的绘图方式，将主成分与它们在数据中解释的方差量进行对比，作为一种图形化的方式来帮助确定保留多少个主成分。该函数允许我们使用
    `choice` 参数来绘制每个主成分的特征值或百分比方差。您可以在 [图 13.8](#ch13fig08) 的底部两个图中看到这两种不同的 y 轴上的
    scree 图。'
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Scree plots are so named because they resemble a *scree slope*, the collection
    of rocks and rubble that accumulates at the foot of a cliff due to weathering
    and erosion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 碎石图之所以得名，是因为它们类似于 *碎石坡*，这是由于风化侵蚀而在悬崖脚下积累的岩石和碎屑。
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 13.5\. Plotting the PCA results
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.5\. 绘制 PCA 结果
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I’ve condensed the four plots from [listing 13.5](#ch13ex05) into a single figure
    ([figure 13.8](#ch13fig08)) to save space.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我将 [列表 13.5](#ch13ex05) 中的四个图压缩成一个单独的图 ([图 13.8](#ch13fig08)) 以节省空间。
- en: When deciding how many principal components to retain, there are a few rules
    of thumb. One is to keep the principal components that cumulatively explain at
    least 80% of the variance. Another is to retain all principal components with
    eigenvalues of at least 1; the mean of all the eigenvalues is always 1, so this
    results in retaining principal components that contain more information than the
    average. A third rule of thumb is to look for an “elbow” in the scree plot and
    exclude principal components beyond the elbow (although there is no obvious elbow
    in our example). Instead of relying too much on these rules of thumb, I look at
    my data projected onto the principal components, and consider how much information
    I can tolerate losing for my application. If I’m applying PCA to my data before
    applying a machine learning algorithm to it, I prefer to use automated feature-selection
    methods, as we did in previous chapters, to select the combination of principal
    components that results in the best performance.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定保留多少个主成分时，有一些经验法则。一个是保留累积解释至少80%方差的成分。另一个是保留所有特征值至少为1的成分；所有特征值的平均值总是1，因此这会导致保留包含比平均值更多信息的成分。第三个经验法则是寻找“拐点”在特征值分布图上，并排除拐点之后的成分（尽管在我们的例子中没有明显的拐点）。而不是过分依赖这些经验法则，我会查看我的数据在主成分上的投影，并考虑我可以为我的应用容忍丢失多少信息。如果我在应用机器学习算法之前对我的数据进行PCA，我更喜欢使用自动特征选择方法，就像我们在前面的章节中所做的那样，以选择导致最佳性能的主成分组合。
- en: Finally, let’s plot our first two principal components against each other and
    see how well they’re able to separate the genuine and counterfeit banknotes. We
    first mutate the original dataset to include a column of component scores for
    PC1and PC2 (extracted from our `pca` object using `$x`). We then plot the principal
    components against each other and add a color aesthetic for the `Status` variable.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将我们的前两个主成分相互对比，看看它们能有多好地分离真伪银行钞票。我们首先将原始数据集变异，包括一个PC1和PC2的成分得分列（使用`$x`从我们的`pca`对象中提取）。然后我们将主成分相互对比，并为`Status`变量添加颜色美学。
- en: Listing 13.6\. Mapping genuine and counterfeit labels
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.6\. 映射真伪标签
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Figure 13.9\. The PCA component scores are plotted for each case, shaded by
    whether they were genuine or counterfeit.
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.9\. 对于每个案例，PCA成分得分被绘制出来，根据它们是否为真钞或假钞进行着色。
- en: '![](fig13-9_alt.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](fig13-9_alt.jpg)'
- en: The resulting plot is shown in [figure 13.9](#ch13fig09). We started with six
    continuous variables and condensed most of that information into just two principal
    components that contain enough information to separate the two clusters of banknotes!
    If we didn’t have labels, having identified different clusters of data, we would
    now try to understand what those two clusters were, and perhaps come up with a
    way of discriminating genuine banknotes from counterfeits.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图13.9](#ch13fig09)中。我们开始时有六个连续变量，并将大部分信息压缩成仅包含两个主成分，这两个主成分包含了足够的信息来分离两种银行钞票的集群！如果我们没有标签，已经识别出不同的数据集群，我们现在会尝试理解这两个集群是什么，也许会想出一个区分真钞和假钞的方法。
- en: '|  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 1**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Add a `stat_ellipse()` layer to the plot in [figure 13.9](#ch13fig09) to add
    95% confidence ellipses to each class of banknote.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图13.9](#ch13fig09)的图表中添加一个`stat_ellipse()`层，为每种银行钞票类别添加95%置信椭圆。
- en: '|  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 13.3.4\. Computing the component scores of new data
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.4\. 计算新数据的成分得分
- en: We have our PCA model, but what do we do when we get new data? Well, because
    the eigenvectors describe exactly how much each variable contributes to the value
    of each principal component, we can simply calculate the component scores of new
    data (including centering and scaling, if we performed this as part of the model).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了PCA模型，但当我们得到新数据时我们该怎么办？嗯，因为特征向量精确地描述了每个变量对每个主成分价值的贡献程度，我们可以简单地计算新数据的成分得分（包括中心化和缩放，如果我们将其作为模型的一部分执行）。
- en: Let’s generate some new data to see how this works in practice. In [listing
    13.7](#ch13ex07), we first define a tibble consisting of two new cases, and all
    the same variables entered into our PCA model. To calculate the component scores
    of these new cases, we simply use the `predict()` function, passing the model
    as the first argument and the new data as the second argument. As we can see,
    the `predict()` function returns both cases’ component scores for each of the
    principal components.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些新的数据来查看这在实践中是如何工作的。在 [列表 13.7](#ch13ex07) 中，我们首先定义了一个包含两个新案例的 tibble，以及所有输入到我们的
    PCA 模型中的相同变量。为了计算这些新案例的成分得分，我们只需使用 `predict()` 函数，将模型作为第一个参数，将新数据作为第二个参数。正如我们所看到的，`predict()`
    函数返回每个主成分的每个案例的成分得分。
- en: Listing 13.7\. Computing the component scores of new data
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.7\. 计算新数据的成分得分
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’ve learned how to apply PCA to your data and interpret the information it
    provides. In the next chapter, I’ll introduce two *nonlinear* dimension-reduction
    techniques. I suggest that you save your .R file, because we’re going to continue
    using the same dataset in the next chapter. This is so we can compare the performance
    of these nonlinear algorithms to the representation we created here using PCA.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经学会了如何将 PCA 应用于您的数据并解释它提供的信息。在下一章中，我将介绍两种 *非线性* 维度降低技术。我建议您保存您的 .R 文件，因为我们将在下一章继续使用相同的数据集。这样我们可以比较这些非线性算法的性能与这里使用
    PCA 创建的表示。
- en: 13.4\. Strengths and weaknesses of PCA
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4\. PCA 的优势和劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    PCA will perform well for you.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对给定的任务表现良好，但以下是一些优势和劣势，这将帮助您决定 PCA 是否适合您。
- en: 'The strengths of PCA are as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的优势如下：
- en: PCA creates new axes that are directly interpretable in terms of the original
    variables.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 创建了可以直接用原始变量解释的新轴。
- en: New data can be projected onto the principal axes.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据可以投影到主轴上。
- en: PCA is really a mathematical transformation and so is computationally inexpensive.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 实际上是一种数学变换，因此计算成本较低。
- en: 'The weaknesses of PCA are these:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的劣势如下：
- en: Mapping from high dimensions to low dimensions cannot be nonlinear.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从高维到低维的映射不能是非线性的。
- en: It cannot handle categorical variables natively.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法原生处理分类变量。
- en: The final number of principal components to retain must be decided by us for
    the application at hand.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于手头的应用，必须由我们决定保留最终主成分的数量。
- en: '|  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: 'Rerun the PCA on our Swiss banknote dataset, but this time set the `scale`
    argument to `FALSE`. Compare the following to the PCA we trained on scaled data:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的瑞士纸币数据集上重新运行 PCA，但这次将 `scale` 参数设置为 `FALSE`。将以下结果与我们在缩放数据上训练的 PCA 进行比较：
- en: Eigenvalues
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征值
- en: Eigenvectors
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征向量
- en: Biplot
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双变量图
- en: Variable loading plot
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量加载图
- en: Scree plot
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切片图
- en: '|  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: Do the same as in [exercise 2](#ch13sb02) again, but this time set the arguments
    `center = FALSE` and `scale = TRUE`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 再次执行与 [练习 2](#ch13sb02) 相同的操作，但这次将参数 `center = FALSE` 和 `scale = TRUE` 设置。
- en: '|  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Dimension reduction is a class of unsupervised learning that learns a low-dimensional
    representation of a high-dimensional dataset while retaining as much information
    as possible.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低是一种无监督学习，它在尽可能保留信息的同时，学习高维数据集的低维表示。
- en: PCA is a linear dimension-reduction technique that finds new axes that maximize
    the variance in the data. The first of these principal axes maximizes the most
    variance, followed by the second, and the third, and so on, which are all orthogonal
    to the previously computed axes.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 是一种线性维度降低技术，它找到新的轴，以最大化数据中的方差。这些主轴中的第一个最大化最大方差，然后是第二个，第三个，以此类推，它们都与之前计算过的轴正交。
- en: When data is projected onto these principal axes, the new variables are called
    principal components.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据投影到这些主轴上时，新的变量被称为主成分。
- en: In PCA, eigenvalues represent the variance along a principal component, and
    the eigenvector represents the direction of the principal axis through the original
    feature space.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PCA 中，特征值表示主成分沿方差的大小，而特征向量表示通过原始特征空间的主轴方向。
- en: Solutions to exercises
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习解答
- en: 'Add 95% confidence ellipses to the plot of PCA1 versus PCA2:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 95% 置信椭圆添加到 PCA1 与 PCA2 的图中：
- en: '[PRE7]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Compare the PCA results when `scale = FALSE`:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较当 `scale = FALSE` 时的 PCA 结果：
- en: '[PRE8]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Compare the PCA results when `center = FALSE` and `scale = TRUE`:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较当`center = FALSE`和`scale = TRUE`时的PCA结果：
- en: '[PRE9]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Chapter 14\. Maximizing similarity with t-SNE and UMAP
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第14章. 使用t-SNE和UMAP最大化相似性
- en: '*This chapter covers*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding nonlinear dimension reduction
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解非线性降维
- en: Using t-distributed stochastic neighbor embedding
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用t分布随机邻域嵌入
- en: Using uniform manifold approximation and projection
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均匀流形近似和投影
- en: In the last chapter, I introduced you to PCA as our first dimension-reduction
    technique. While PCA is a linear dimension-reduction algorithm (it finds linear
    combinations of the original variables), sometimes the information in a set of
    variables can’t be extracted as a linear combination of these variables. In such
    situations, there are a number of nonlinear dimension-reduction algorithms we
    can turn to, such as *t-distributed stochastic neighbor embedding* (t-SNE), and
    *uniform manifold approximation and projection* (UMAP).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我向您介绍了PCA作为我们的第一个降维技术。虽然PCA是一个线性降维算法（它找到原始变量的线性组合），但有时一组变量中的信息不能提取为这些变量的线性组合。在这种情况下，我们可以转向许多非线性降维算法，例如*t分布随机邻域嵌入*（t-SNE）和*均匀流形近似和投影*（UMAP）。
- en: t-SNE is one of the most popular nonlinear dimension-reduction algorithms. It
    measures the distance between each observation in the dataset and every other
    observation, and then randomizes the observations across (usually) two new axes.
    The observations are then iteratively shuffled around these new axes until their
    distances to each other in this two-dimensional space are as similar to the distances
    in the original high-dimensional space as possible.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是最受欢迎的非线性降维算法之一。它测量数据集中每个观测值与其他每个观测值之间的距离，然后随机地将观测值分配到（通常是）两个新的轴上。然后，观测值在这些新轴周围迭代地重新排列，直到它们在这个二维空间中的距离尽可能接近原始高维空间中的距离。
- en: UMAP is another nonlinear dimension-reduction algorithm that overcomes some
    of the limitations of t-SNE. It works similarly to t-SNE (finds distances in a
    feature space with many variables and then tries to reproduce these distances
    in low-dimensional space), but differs in the way it measures distances.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP是另一种非线性降维算法，它克服了t-SNE的一些局限性。它的工作方式与t-SNE相似（在具有许多变量的特征空间中找到距离，然后试图在低维空间中重现这些距离），但在测量距离的方式上有所不同。
- en: By the end of this chapter, I hope you’ll understand what nonlinear dimension
    reduction is and why it can be beneficial compared to linear dimension reduction.
    I will show you how the t-SNE and UMAP algorithms work and how they’re different
    from each other, and we’ll apply each of them to our banknote dataset from [chapter
    13](kindle_split_025.html#ch13) so we can compare their performance with PCA.
    If you no longer have the `swissTib` and `newBanknotes` objects defined in your
    global environment, just rerun [listings 13.1](kindle_split_025.html#ch13ex01)
    and [13.7](kindle_split_025.html#ch13ex07).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望您能理解非线性降维是什么，以及为什么它比线性降维更有益。我将向您展示t-SNE和UMAP算法是如何工作的，以及它们之间的不同之处，并且我们将将它们应用到第13章中的钞票数据集[chapter
    13](kindle_split_025.html#ch13)上，以便我们可以比较它们的性能与PCA。如果您在全局环境中不再有`swissTib`和`newBanknotes`对象，只需重新运行[列表13.1](kindle_split_025.html#ch13ex01)和[13.7](kindle_split_025.html#ch13ex07)。
- en: 14.1\. What is t-SNE?
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1. 什么是t-SNE？
- en: In this section, I’ll show you what t-distributed stochastic neighbor embedding
    is, how it works, and why it’s useful. t-distributed stochastic neighbor embedding
    is such a mouthful—I’m glad people shorten it to t-SNE (usually pronounced “tee-snee,”
    or occasionally “tiz-nee”), not least because when you hear someone say it, you
    can say “bless you,” and everyone laughs (at least the first few times).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示什么是t分布随机邻域嵌入，它是如何工作的，以及为什么它是有用的。t分布随机邻域嵌入这个名字太长了——我很高兴人们把它缩写为t-SNE（通常发音为“tee-snee”，偶尔为“tiz-nee”），至少当你听到有人这么说时，你可以回答“愿上帝保佑你”，然后大家都会笑（至少最初几次）。
- en: Whereas PCA is a linear dimension-reduction algorithm (because it finds new
    axes that are linear combinations of the original variables), t-SNE is a nonlinear
    dimension-reduction algorithm. It is nonlinear because instead of finding new
    axes that are logical combinations of the original variables, it focuses on the
    similarities between nearby cases in a dataset and tries to reproduce these similarities
    in a lower-dimensional space. The main benefit of this approach is that t-SNE
    will almost always do a better job than PCA of highlighting patterns in the data
    (such as clusters). One of the downsides of this approach is that the axes are
    no longer interpretable, because they don’t represent logical combinations of
    the original variables.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA是一个线性降维算法（因为它找到新的轴，这些轴是原始变量的线性组合）不同，t-SNE是一个非线性降维算法。它是非线性的，因为它不是寻找新的轴，这些轴是原始变量的逻辑组合，而是关注数据集中附近案例之间的相似性，并试图在低维空间中重现这些相似性。这种方法的主要好处是，t-SNE几乎总是比PCA更好地突出数据中的模式（如集群）。这种方法的一个缺点是，轴不再可解释，因为它们不代表原始变量的逻辑组合。
- en: The first step in the t-SNE algorithm is to compute the distance between each
    case and every other case in the dataset. By default, this distance is the Euclidean
    distance, which is the straight-line distance between any two points in the feature
    space (but we can use other measures of distance instead). These distances are
    then converted into probabilities. This is illustrated in [figure 14.1](#ch14fig01).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE算法的第一步是计算数据集中每个案例与其他每个案例之间的距离。默认情况下，这个距离是欧几里得距离，即特征空间中任意两点之间的直线距离（但我们也可以使用其他距离度量）。然后，这些距离被转换成概率。这可以在[图14.1](#ch14fig01)中看到。
- en: For a particular case in the dataset, the distance between this case and all
    other cases is measured. Then a normal distribution is centered on this case,
    and the distances are converted into probabilities by mapping them onto the probability
    density of the normal distribution. The standard deviation of this normal distribution
    is inversely related to the density of cases around the case in question. Put
    another way, if there are lots of cases nearby (more dense), then the standard
    deviation of the normal distribution is smaller; but if there are few cases nearby
    (less dense), then the standard deviation is larger.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中特定的一个案例，测量这个案例与其他所有案例之间的距离。然后在这个案例上建立一个正态分布，并将距离转换成概率，通过将它们映射到正态分布的概率密度。这个正态分布的标准差与问题案例周围案例的密度成反比。换句话说，如果附近有很多案例（更密集），那么正态分布的标准差会更小；但如果附近案例较少（密度较低），那么标准差会更大。
- en: After converting the distances to probabilities, the probabilities for each
    case are scaled by dividing them by their sum. This makes the probabilities sum
    to 1 for every case in the dataset. Using different standard deviations for different
    densities, and then normalizing the probabilities to 1 for every case, means if
    there are dense clusters and sparse clusters of cases in the dataset, t-SNE will
    expand the dense clusters and compress the sparse ones so they can be visualized
    more easily together. The exact relationship between data density and the standard
    deviation of the normal distribution depends on a hyperparameter called *perplexity*,
    which we’ll discuss shortly.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将距离转换为概率后，每个案例的概率通过除以它们的总和进行缩放。这使得数据集中每个案例的概率总和为1。使用不同的标准差来处理不同的密度，并将每个案例的概率归一化到1，意味着如果数据集中有密集的案例集群和稀疏的案例集群，t-SNE将扩展密集集群并压缩稀疏集群，以便更容易一起可视化。数据密度与正态分布标准差之间的确切关系取决于一个称为*困惑度*的超参数，我们将在稍后讨论。
- en: Figure 14.1\. t-SNE measures the distance from each case to every other case,
    converted into a probability by fitting a normal distribution over the current
    case. These probabilities are scaled by dividing them by their sum, so that they
    add to 1.
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.1\. t-SNE通过在当前案例上拟合正态分布来测量每个案例到其他每个案例的距离，并将其转换为概率。这些概率通过除以它们的总和进行缩放，因此它们加起来为1。
- en: '![](fig14-1_alt.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图14-1](fig14-1_alt.jpg)'
- en: Once the scaled probabilities have been calculated for each case in the dataset,
    we have a matrix of probabilities that describes how similar each case is to each
    of the other cases. This is visualized in [figure 14.2](#ch14fig02) as a heatmap,
    which is a useful way of thinking about it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出了数据集中每个案例的缩放概率，我们就得到了一个概率矩阵，它描述了每个案例与其他每个案例的相似程度。这在[图14.2](#ch14fig02)中以热图的形式展示，这是一种思考它的有用方式。
- en: Our matrix of probabilities is now our reference, or template, for how the data
    values relate to each other in the original, high-dimensional space. The next
    step in the t-SNE algorithm is to randomize the cases along (usually) two new
    axes (this is where the “stochastic” bit of the name comes from).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的概率矩阵现在是我们如何将数据值关联到原始、高维空间中的参考，或模板。t-SNE算法的下一步是在（通常是）两个新轴上随机化案例（这就是“随机”这个名字的由来）。
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: It doesn’t need to be two axes, but it commonly is. This is because humans struggle
    to visualize data in more than two dimensions at once, and because, beyond three
    dimensions, the computational cost of t-SNE becomes more and more prohibitive.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要是两个轴，但通常是这样的。这是因为人类难以同时可视化超过两个维度中的数据，而且因为，超过三个维度，t-SNE的计算成本会越来越高，变得难以承受。
- en: '|  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: t-SNE calculates the distances between the cases in this new, randomized, low-dimensional
    space and converts them into probabilities just like before. The only difference
    is that instead of using the normal distribution, it now uses Student’s t distribution.
    The t distribution looks a bit like a normal distribution, except that it’s not
    quite as tall in the middle, and its tails are flatter and extend further out
    (see [figure 14.3](#ch14fig03)). It’s a bit like if someone sat on a normal distribution
    and squashed it. This is where the “t” in t-SNE comes from. I’ll explain why we
    use the t distribution momentarily.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE计算在这个新的、随机化的低维空间中案例之间的距离，并将它们转换为概率，就像之前一样。唯一的不同是，它现在使用Student’s t分布，而不是正态分布。t分布看起来有点像正态分布，但中间部分不那么高，尾部更平，延伸得更远（参见[图14.3](#ch14fig03)）。这就像有人在正态分布上坐下并把它压扁了。这就是t-SNE中“t”的由来。我马上会解释为什么我们使用t分布。
- en: 'Figure 14.2\. The scaled probabilities for each case are stored as a matrix
    of values. This is visualized here as a heatmap: the closer two cases are, the
    darker the box is that represents their distance in the heatmap.'
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.2\. 每个案例的缩放概率存储为一个值矩阵。这里以热图的形式展示：两个案例越接近，代表它们在热图中的距离的方框就越暗。
- en: '![](fig14-2_alt.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片14-2](fig14-2_alt.jpg)'
- en: Figure 14.3\. When converting distances in the lower-dimensional representation
    into probabilities, t-SNE fits a Student’s t distribution over the current case
    instead of a normal distribution. The Student’s t distribution has longer tails,
    meaning dissimilar cases are pushed further away to achieve the same probability
    as in the high-dimensional representation.
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.3\. 在将低维表示中的距离转换为概率时，t-SNE在当前案例上拟合Student’s t分布，而不是正态分布。Student’s t分布的尾部更长，这意味着不相似的案例被推得更远，以达到与高维表示相同概率。
- en: '![](fig14-3_alt.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片14-3](fig14-3_alt.jpg)'
- en: The job for t-SNE now is to “shuffle” the data points around these new axes,
    step by step, to make the matrix of probabilities in the lower-dimensional space
    look as close as possible to the matrix of probabilities in the original, high-dimensional
    space. The intuition here is that if the matrices are as similar as possible,
    then the data each case was close to in the original feature space will still
    be close by in the low-dimensional space. You can think of this as a game of attraction
    and repulsion.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE现在的任务是“重新排列”这些新轴上的数据点，逐步进行，使得低维空间中的概率矩阵尽可能接近原始、高维空间中的概率矩阵。这里的直觉是，如果矩阵尽可能相似，那么每个案例在原始特征空间中接近的数据点在低维空间中仍然会接近。你可以把这想成一个吸引和排斥的游戏。
- en: To make the probability matrix in low-dimensional space look like the one in
    high-dimensional space, each case needs to move closer to cases that were close
    to it in the original data, and away from cases that were far away. So cases that
    should be nearby will pull their neighbor toward them, but cases that should be
    far away will push non-neighbors away from them. The balance of these attractive
    and repulsive forces causes each case in the dataset to move in a direction that
    makes the two probability matrices a little more similar. Now, in this new position,
    the low-dimensional probability matrix is calculated again, and the cases move
    again, making the low- and high-dimensional matrices look a little more similar
    again. This process continues until we reach a predetermined number of iterations,
    or until the *divergence* (difference) between the matrices stops improving. This
    whole process is illustrated in [figure 14.4](#ch14fig04).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使低维空间中的概率矩阵看起来像高维空间中的矩阵，每个案例都需要移动到在原始数据中与其靠近的案例附近，并远离远离的案例。因此，应该靠近的案例会将其邻居拉向自己，而应该远离的案例则会将非邻居推开。这些吸引力和排斥力的平衡导致数据集中的每个案例都朝着使两个概率矩阵稍微更相似的方向移动。现在，在这个新位置，再次计算低维概率矩阵，案例再次移动，使低维和高维矩阵看起来稍微更相似。这个过程会继续进行，直到达到预定的迭代次数，或者直到矩阵之间的**散度**（差异）不再改善。整个过程在[图14.4](#ch14fig04)中得到了说明。
- en: Figure 14.4\. Cases are randomly initialized over the new axes (one axis is
    shown here). The probability matrix is computed for this axis, and the cases are
    shuffled around to make this matrix resemble the original, high-dimensional matrix
    by minimizing the Kullback-Leibler (KL) divergence. During shuffling, cases are
    attracted toward cases that are similar to them (lines with circles) and repulsed
    away from cases that are dissimilar (lines with triangles).
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.4。案例在新的轴上随机初始化（这里显示了其中一个轴）。为此轴计算概率矩阵，并通过最小化Kullback-Leibler（KL）散度来重新排列案例，使这个矩阵类似于原始的高维矩阵。在重新排列过程中，案例会被吸引向与其相似的案例（带有圆圈的线条）并排斥不相似的案例（带有三角形的线条）。
- en: '![](fig14-4_alt.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![fig14-4_alt](fig14-4_alt.jpg)'
- en: '|  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The difference between the two matrices is measured using a statistic called
    the *Kullback-Leibler divergence*, which is large when the matrices are very different
    and zero when the matrices are perfectly identical.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 两个矩阵之间的差异是通过一个称为**Kullback-Leibler散度**的统计量来衡量的，当矩阵非常不同时，这个散度很大，而当矩阵完全相同的时候，散度为零。
- en: '|  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Why do we use the t distribution to convert distances into probabilities in
    the low-dimensional space? Well, notice again from [figure 14.4](#ch14fig04) that
    the tails of the t distribution are wider than for the normal distribution. This
    means that, in order to get the same probability as from the normal distribution,
    dissimilar cases need to be pushed further away from the case the t distribution
    is centered over. This helps spread out clusters of data that might be present
    in the data, helping us to identify them more easily. A major consequence of this,
    however, is that t-SNE is often said to retain *local* structure in the low-dimensional
    representation, but it doesn’t usually retain *global* structure. Practically,
    this means we can interpret cases that are close to each other in the final representation
    as being similar to each other, but we can’t easily say which clusters of cases
    were more similar to other clusters of cases in the original data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要使用t分布将距离转换为低维空间中的概率？再次注意，从[图14.4](#ch14fig04)中可以看出，t分布的尾部比正态分布的尾部更宽。这意味着，为了得到与正态分布相同的概率，不相似的案例需要被推得更远，远离t分布中心所在的案例。这有助于分散数据中可能存在的数据簇，帮助我们更容易地识别它们。然而，这一点的重大后果是，t-SNE通常被认为在低维表示中保留了**局部**结构，但它通常不保留**全局**结构。实际上，这意味着我们可以将最终表示中彼此靠近的案例解释为彼此相似，但我们不能轻易地说原始数据中哪些案例簇比其他案例簇更相似。
- en: 'Once this iterative process has converged at a low KL divergence, we should
    have a low-dimensional representation of our original data that preserves the
    similarities between nearby cases. While t-SNE typically outperforms PCA for highlighting
    patterns in data, it does have some significant limitations:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个迭代过程在低KL散度下收敛，我们应该有一个低维度的原始数据表示，它保留了附近案例之间的相似性。虽然t-SNE通常在突出数据中的模式方面优于PCA，但它确实有一些显著的局限性：
- en: 'It is infamously computationally expensive: its computation time increases
    exponentially with the number of cases in the dataset. There is a multicore implementation
    (see [https://github.com/RGLab/Rtsne.multicore](https://github.com/RGLab/Rtsne.multicore)),
    but for extremely large datasets, t-SNE could take hours to run.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在计算上非常昂贵：其计算时间随着数据集中案例数量的增加而呈指数增长。有一个多核实现（见 [https://github.com/RGLab/Rtsne.multicore](https://github.com/RGLab/Rtsne.multicore)），但对于极其大的数据集，t-SNE
    可能需要数小时才能运行。
- en: It cannot project new data onto the embedding. By this I mean that, because
    the initial placement of the data onto the new axes is random, rerunning t-SNE
    on the same dataset repeatedly will give you slightly different results. Thus
    we can’t use the `predict()` function to map new data onto the lower-dimensional
    representation as we can with PCA. This prohibits us from using t-SNE as part
    of a machine learning pipeline and pretty much relegates its use to data exploration
    and visualization.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能将新数据投影到嵌入中。我的意思是，由于数据在新的轴上的初始放置是随机的，反复在相同的数据集上运行 t-SNE 将会给出略微不同的结果。因此，我们不能像使用
    PCA 那样使用 `predict()` 函数将新数据映射到低维表示。这阻止了我们将 t-SNE 作为机器学习流程的一部分使用，实际上将其使用限制在数据探索和可视化中。
- en: 'Distances between clusters often don’t mean anything. Say we have three clusters
    of data in our final t-SNE representation: two are close, and a third is far away
    from the other two. Because t-SNE focuses on local, not global, structure, we
    cannot say that the first two clusters are more similar to each other than they
    are to the third cluster.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类之间的距离通常没有意义。比如说，在我们的最终 t-SNE 表示中，有三个数据点聚类：两个彼此靠近，第三个离其他两个较远。因为 t-SNE 专注于局部结构而不是全局结构，所以我们不能说前两个聚类比第三个聚类更相似。
- en: t-SNE doesn’t necessarily preserve the distances or density of the data in the
    final representation, so passing the output of t-SNE into clustering algorithms
    that rely on distances or densities tends not to work as well as you might expect.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE 并不一定保留最终表示中数据的距离或密度，因此将 t-SNE 的输出传递给依赖于距离或密度的聚类算法通常不如预期有效。
- en: We need to select sensible values for a number of hyperparameters, which can
    be difficult if the t-SNE algorithm takes minutes to hours to run on a dataset.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要为许多超参数选择合理的值，如果 t-SNE 算法在数据集上运行需要数分钟到数小时，这可能会很困难。
- en: 14.2\. Building your first t-SNE embedding
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2. 构建第一个 t-SNE 嵌入
- en: In this section, I’m going to show you how to use the t-SNE algorithm to create
    a low-dimensional embedding of our Swiss banknote dataset, to see how it compares
    with the PCA model we created in the previous chapter. First, we’ll install and
    load the Rtsne package in R, and then I’ll explain the various hyperparameters
    that control how t-SNE learns. Then, we’ll create a t-SNE embedding using the
    optimal combination of hyperparameters. Finally, we’ll plot the new, lower-dimensional
    representation learned by the t-SNE algorithm, and compare it to the PCA representation
    we plotted in [chapter 13](kindle_split_025.html#ch13).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用 t-SNE 算法创建瑞士银行钞票数据集的低维嵌入，以比较我们在上一章中创建的 PCA 模型。首先，我们将安装并加载 Rtsne
    包到 R 中，然后我将解释控制 t-SNE 学习方式的各个超参数。然后，我们将使用最佳的超参数组合创建 t-SNE 嵌入。最后，我们将绘制 t-SNE 算法学习的新低维表示，并将其与我们之前在
    [第 13 章](kindle_split_025.html#ch13) 中绘制的 PCA 表示进行比较。
- en: 14.2.1\. Performing t-SNE
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1. 执行 t-SNE
- en: 'Let’s start by installing and loading the Rtsne package:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装和加载 Rtsne 包开始：
- en: '[PRE10]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 't-SNE has four important hyperparameters that can drastically change the resulting
    embedding:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 有四个重要的超参数，这些参数可以极大地改变结果嵌入：
- en: '***perplexity—*** Controls the width of the distributions used to convert distances
    into probabilities. High values place more focus on global structure, whereas
    small values place more focus on local structure. Typical values lie in the range
    5 to 50\. The default value is 30.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***perplexity（困惑度）*** — 控制用于将距离转换为概率的分布的宽度。高值更多地关注全局结构，而小值更多地关注局部结构。典型值位于 5
    到 50 的范围内。默认值为 30。'
- en: '***theta—*** Controls the trade-off between speed and accuracy. Because t-SNE
    is slow, people commonly use an implementation called *Barnes-Hut* t-SNE, which
    allows us to perform the embedding much faster but with some loss of accuracy.
    The *theta* hyperparameter controls this trade-off, with 0 being “exact” t-SNE
    and 1 being the fastest but least accurate t-SNE. The default value is 0.5.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***theta—*** 控制速度和准确度之间的权衡。因为t-SNE很慢，人们通常使用一个称为*Barnes-Hut* t-SNE的实现，这允许我们更快地执行嵌入，但会损失一些准确性。*theta*超参数控制这种权衡，0表示“精确”的t-SNE，1表示最快但最不准确的t-SNE。默认值是0.5。'
- en: '***eta—*** How far each data point moves at each iteration (also called the
    *learning rate*). Lower values need more iterations to reach convergence but may
    result in a more accurate embedding. The default value is 200, and this is usually
    fine.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***eta—*** 每次迭代中每个数据点移动的距离（也称为*学习率*）。较低的值需要更多的迭代次数以达到收敛，但可能会导致更准确的嵌入。默认值是200，这通常是可以接受的。'
- en: '***max_iter—*** The maximum iterations allowed before computation stops. This
    will depend on your computational budget, but it’s important to have enough iterations
    to reach convergence. The default value is 1,000.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***max_iter—*** 允许在计算停止之前的最大迭代次数。这取决于你的计算预算，但确保有足够的迭代次数以达到收敛是很重要的。默认值是1,000。'
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The most important hyperparameters to tune are usually *perplexity* and *max_iter*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通常需要调整的最重要超参数是*perplexity*和*max_iter*。
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Our approach to tuning hyperparameters thus far has been to allow an automated
    tuning process to choose the best combination for us, through either a grid search
    or random search. But due to its computational cost, most people will run t-SNE
    with its default hyperparameter values and change them if the embedding doesn’t
    look sensible. If this sounds very subjective, that’s because it is; but people
    are usually able to identify visually whether t-SNE is pulling apart clusters
    of observations nicely.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们调整超参数的方法是允许自动调整过程为我们选择最佳组合，通过网格搜索或随机搜索。但由于其计算成本，大多数人会使用t-SNE的默认超参数值，并在嵌入看起来不合理时更改它们。如果这听起来非常主观，那是因为它是；但人们通常能够通过视觉识别t-SNE是否很好地将观察到的簇分开。
- en: To give you a visual aid for how each of these hyperparameters affects the final
    embedding, I’ve run t-SNE on our Swiss banknote data using a grid of hyperparameter
    values. [Figure 14.5](#ch14fig05) shows the final embeddings with different combinations
    of *theta* (rows) and *perplexity* (columns) using the default values of *eta*
    and *max_iter*. Notice that the clusters become tighter with larger values of
    *perplexity* and are lost with very low values. Also notice that for reasonable
    values of *perplexity*, the clusters are best resolved when *theta* is set to
    0 (exact t-SNE).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个视觉辅助，说明每个超参数如何影响最终的嵌入，我已经使用一组超参数值在我们的瑞士银行票据数据上运行了t-SNE。[图14.5](#ch14fig05)显示了使用默认的*eta*和*max_iter*值的*theta*（行）和*perplexity*（列）的不同组合的最终嵌入。注意，随着*perplexity*值的增大，簇变得更加紧密，而在非常低的值时簇会丢失。此外，注意对于合理的*perplexity*值，当*theta*设置为0（精确的t-SNE）时，簇的分辨率最佳。
- en: Figure 14.5\. The effect on the final t-SNE embedding of the banknote dataset
    of changing *theta* (row facets) and *perplexity* (column facets) using the default
    values of *eta* and *max_iter*
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.5\. 使用默认的*eta*和*max_iter*值，改变*theta*（行面元）和*perplexity*（列面元）对银行票据数据集最终t-SNE嵌入的影响
- en: '![](fig14-5_alt.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig14-5_alt.jpg)'
- en: Figure 14.6\. The effect on the final t-SNE embedding of the banknote dataset
    of changing *max_iter* (row facets) and *eta* (column facets) using the default
    values of *theta* and *perplexity*
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.6\. 使用默认的*theta*和*perplexity*值，改变*max_iter*（行面元）和*eta*（列面元）对银行票据数据集最终t-SNE嵌入的影响
- en: '![](fig14-6_alt.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig14-6_alt.jpg)'
- en: '[Figure 14.6](#ch14fig06) shows the final embeddings with different combinations
    of *max_iter* (rows) and *eta* (columns). The effect here is a little more subtle,
    but smaller values of *eta* need a larger number of iterations in order to converge
    (because the cases move in smaller steps at each iteration). For example, for
    an *eta* of 100, 1,000 iterations is sufficient to separate the clusters; but
    with an *eta* of 1, the clusters remain poorly resolved after 1,000 iterations.
    If you would like to see the code I used to generate these figures, the code for
    this chapter is available at [www.manning.com/books/machine-learning-with-r-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-tidyverse-and-mlr).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14.6](#ch14fig06) 展示了不同组合的 *max_iter*（行）和 *eta*（列）的最终嵌入。这里的效果稍微微妙一些，但较小的
    *eta* 值需要更多的迭代次数才能收敛（因为每个迭代中案例移动的步长更小）。例如，对于 *eta* 为 100 的情况，1,000 次迭代就足以分离簇；但使用
    *eta* 为 1 的情况下，簇在 1,000 次迭代后仍然分辨率不佳。如果你想看到我用来生成这些图代码，本章的代码可在 [www.manning.com/books/machine-learning-with-r-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-tidyverse-and-mlr)
    获取。'
- en: Now that you’re a little more tuned in to how t-SNE’s hyperparameters affect
    its performance, let’s run t-SNE on our Swiss banknote dataset. Just like for
    PCA, we first select all the columns except the categorical variable (t-SNE also
    cannot handle categorical variables) and pipe this data into the `Rtsne()` function.
    We manually set the values of the *perplexity*, *theta*, and *max_iter* hyperparameters
    (honestly, I rarely alter *eta*) and set the argument `verbose = TRUE` so the
    algorithm prints a running commentary on what the KL divergence is at each iteration.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 t-SNE 的超参数如何影响其性能有了更多的了解，让我们在我们的瑞士纸币数据集上运行 t-SNE。就像 PCA 一样，我们首先选择除了分类变量之外的所有列（t-SNE
    也不能处理分类变量），并将这些数据通过 `Rtsne()` 函数传递。我们手动设置 *perplexity*、*theta* 和 *max_iter* 超参数的值（说实话，我很少改变
    *eta*），并将 `verbose = TRUE` 参数设置为真，以便算法在每次迭代时打印出 KL 散度的运行注释。
- en: Listing 14.1\. Running t-SNE
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.1\. 运行 t-SNE
- en: '[PRE11]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: By default, the `Rtsne()` function reduces the dataset to two dimensions. If
    you want to return another number, you can set this using the `dims` argument.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Rtsne()` 函数将数据集减少到二维。如果你想返回另一个数字，你可以使用 `dims` 参数来设置这个值。
- en: '|  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: That didn’t take too long, did it? For a small dataset like this, t-SNE takes
    only a few seconds. But it quickly gets slow (see what I did there?) as the dataset
    increases in size.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这没有花太多时间，对吧？对于如此小的数据集，t-SNE 只需要几秒钟。但随着数据集大小的增加，它会迅速变慢（看我在这里做了什么？）。
- en: 14.2.2\. Plotting the result of t-SNE
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2\. 绘制 t-SNE 的结果
- en: Next, let’s plot the two t-SNE dimensions against each other to see how well
    they separated the genuine and counterfeit banknotes. Because we can’t interpret
    the axes in terms of how much each variable correlates with them, it’s common
    for people to color their t-SNE plots by the values of each of their original
    variables, to help identify which clusters have higher and lower values. To do
    this, we first use the `mutate_if()` function to center the numeric variables
    in our original dataset (by setting `.funs = scale` and `.predicate = is.numeric`).
    We include `scale = FALSE` to only center the variables, not divide by their standard
    deviations. The reason we center the variables is that we’re going to shade by
    their value on the plots, and we don’t want variables with larger values dominating
    the color scales (omit this line and see the difference in the final plot for
    yourself).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将两个 t-SNE 维度相互绘制，以查看它们如何将真钞和假钞分开。因为我们不能根据每个变量与它们的关联程度来解释轴，所以人们通常根据它们原始变量的值来着色
    t-SNE 图，以帮助识别哪些簇具有更高的和更低的值。为此，我们首先使用 `mutate_if()` 函数将原始数据集中的数值变量居中（通过设置 `.funs
    = scale` 和 `.predicate = is.numeric`）。我们包括 `scale = FALSE` 以仅居中变量，而不是除以其标准差。我们居中变量的原因是我们将根据它们的值在图上着色，我们不希望具有较大值的变量主导颜色刻度（省略此行，并亲自查看最终图中的差异）。
- en: Next, we mutate two new columns that contain the t-SNE axes values for each
    case. Finally, we gather the data so that we can facet by each of the original
    variables. We plot this data, mapping the value of each original variable to the
    color aesthetic and the status of each banknote (genuine versus counterfeit) to
    the shape aesthetic, and facet by the original variables. We add a custom color
    scale gradient to make the color scale more readable in print.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们突变两个包含每个案例t-SNE轴值的新的列。最后，我们收集数据，以便我们可以根据原始变量进行分面。我们绘制这些数据，将每个原始变量的值映射到颜色美学，并将每张纸币的状态（真币与假币）映射到形状美学，并根据原始变量进行分面。我们添加一个自定义颜色渐变，使颜色刻度在打印时更易读。
- en: Listing 14.2\. Plotting the t-SNE embedding
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.2\. 绘制t-SNE嵌入图
- en: '[PRE12]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Figure 14.7\. tSNE1 and tSNE2 axes plotted against each other, faceted and shaded
    by the original variables, and shaped by whether each case was a genuine or counterfeit
    banknote
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.7\. tSNE1和tSNE2轴相互绘制，根据原始变量分面和着色，以及根据每个案例是真实纸币还是假币进行形状绘制
- en: '![](fig14-7_alt.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片14-7_alt](fig14-7_alt.jpg)'
- en: 'The resulting plot is shown in [figure 14.7](#ch14fig07). Wow! Notice how much
    better t-SNE does than PCA at representing the differences between the two clusters
    in a feature space with only two dimensions. The clusters are well resolved, although
    if you look closely, you can see a couple of cases that seem to be in the wrong
    cluster. Shading the points by the value of each variable also helps us identify
    that counterfeit notes tend to have lower values of the `Diagonal` variable and
    higher values of the `Bottom` and `Top` variables. It also seems as though there
    might be a small second cluster of counterfeit notes: this could be a set of notes
    made by a different counterfeiter, or an artifact of an imperfect combination
    of hyperparameters. More investigation would be needed to tell if these are actually
    a distinct cluster.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图14.7](#ch14fig07)。哇！注意t-SNE在仅用两个维度表示特征空间中两个簇之间的差异时比PCA做得好得多。簇被很好地解析，尽管如果你仔细看，你可以看到一些似乎在错误簇中的案例。通过每个变量的值着色点也有助于我们识别假币往往具有较低的`对角线`变量值和较高的`底部`和`顶部`变量值。似乎还可能有一个小的假币第二簇：这可能是由不同的伪造者制作的纸币集，或者是不完美的超参数组合的产物。需要更多的调查来确定这些是否实际上是不同的簇。
- en: '|  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Do your plots look a little different than mine? Of course they do! Remember
    that the initial embedding is random (stochastic), so each time you run t-SNE
    on the same data and with the same hyperparameters, you’ll get a slightly different
    embedding.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你的图表看起来和我的一样吗？当然不一样了！记住，初始嵌入是随机的（随机性的），所以每次你在相同的数据和相同的超参数上运行t-SNE时，你都会得到一个略微不同的嵌入。
- en: '|  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Recreate the plot in [figure 14.7](#ch14fig07), but this time don’t center the
    variables before running t-SNE on them (just remove the `mutate_if()` layer).
    Can you see why scaling was necessary?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制[图14.7](#ch14fig07)中的图表，但这次在运行t-SNE之前不要对变量进行中心化（只需移除`mutate_if()`层）。你能看出为什么缩放是必要的吗？
- en: '|  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 14.3\. What is UMAP?
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3\. 什么是UMAP？
- en: In this section, I’ll show you what UMAP is, how it works, and why it’s useful.
    Uniform manifold approximation and projection, fortunately shortened to UMAP,
    is a nonlinear dimension-reduction algorithm like t-SNE. UMAP is state of the
    art, having only been published in 2018, and it has a few benefits over the t-SNE
    algorithm.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示UMAP是什么，它是如何工作的，以及为什么它有用。均匀流形近似和投影，幸运的是简称为UMAP，是一种类似于t-SNE的非线性降维算法。UMAP是前沿技术，仅在2018年发表，并且它相对于t-SNE算法有一些优势。
- en: First, it’s considerably faster than t-SNE, where the length of time it takes
    to run increases less than the square of the number of cases in the dataset. To
    put this in perspective, a dataset that might take t-SNE hours to compress will
    take UMAP minutes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它比t-SNE快得多，t-SNE的运行时间长度增加小于数据集中案例数量的平方。为了更具体地说明这一点，一个可能需要t-SNE数小时来压缩的数据集将只需要UMAP几分钟。
- en: The second benefit (and the main benefit, in my eyes) is that UMAP is a deterministic
    algorithm. In other words, given the same input, it will always give the same
    output. This means that, unlike with t-SNE, we can project new data onto the lower-dimensional
    representation, allowing us to incorporate UMAP into our machine learning pipelines.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个优势（在我看来是主要优势）是UMAP是一个确定性算法。换句话说，给定相同的输入，它总是会给出相同的输出。这意味着，与t-SNE不同，我们可以将新数据投影到低维表示中，使我们能够将UMAP纳入我们的机器学习流程中。
- en: The third benefit is that UMAP preserves both local *and* global structure.
    Practically, this means that not only can we interpret two cases close to each
    other in lower dimensions as being similar to each other in high dimensions, but
    we can also interpret two *clusters* of cases close to each other as being more
    similar to each other in high dimensions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个好处是UMAP保留了局部**和**全局结构。实际上，这意味着我们不仅可以将低维空间中彼此靠近的两个案例解释为在高层空间中彼此相似，而且我们还可以将彼此靠近的两个**簇**解释为在高层空间中更相似。
- en: So how does UMAP work? Well, UMAP assumes the data is distributed along a *manifold*.
    A manifold is an *n*-dimensional smooth geometric shape where, for every point
    on this manifold, there exists a small neighborhood around that point that looks
    like a flat, two-dimensional plane. If that doesn’t make sense to you, consider
    that the world is a three-dimensional manifold, any part of which can be mapped
    into a flat representation literally called a map. UMAP searches for a surface,
    or a space with many dimensions, along which the data is distributed. The distances
    between cases *along the manifold* can then be calculated, and a lower-dimensional
    representation of the data can be optimized iteratively to reproduce these distances.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，UMAP是如何工作的呢？嗯，UMAP假设数据分布在一条**流形**上。流形是一个**n**维光滑几何形状，其中，对于流形上的每个点，都存在一个围绕该点的小邻域，该邻域看起来像是一个平坦的二维平面。如果你觉得这很难理解，可以考虑世界是一个三维流形，其任何部分都可以映射到一个平面的表示，即地图。UMAP在数据分布的表面或具有许多维度的空间中搜索。然后可以计算案例在流形上的距离，并通过迭代优化数据的低维表示来重现这些距离。
- en: Prefer a visual representation? Me too. Have a look at [figure 14.8](#ch14fig08).
    I’ve drawn a question mark as a manifold and randomly seeded 15 cases around the
    manifold across 2 variables. UMAP’s job is to learn the question mark manifold
    so that it can measure the distances between cases along the manifold instead
    of ordinary Euclidean distance, like t-SNE does. It achieves this by searching
    a region around each case, for another case. Where these regions encapsulate another
    case, the cases get connected by an edge. This is what I’ve done in the top row
    of [figure 14.8](#ch14fig08)—but can you see that the manifold is incomplete?
    There are gaps in my question mark. This is because the regions I searched around
    each case had the same radius, and the data wasn’t uniformly distributed along
    the manifold. If the cases had been spaced out along the question mark at regular
    intervals, then this approach would have worked, provided I selected an appropriate
    radius for the search regions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 倾向于视觉表示？我也是。看看[图14.8](#ch14fig08)。我在两个变量上围绕流形随机放置了15个案例，并将问号作为流形绘制出来。UMAP的工作是学习问号流形，以便它可以测量案例在流形上的距离，而不是像t-SNE那样测量普通欧几里得距离。它通过在每个案例周围搜索另一个案例来实现这一点。在这些区域包含另一个案例的地方，案例通过边连接起来。这就是我在[图14.8](#ch14fig08)的上排中所做的——但你能否看出流形是不完整的？我的问号中存在间隙。这是因为我在每个案例周围搜索的区域具有相同的半径，数据在流形上分布不均匀。如果案例在问号上以均匀间隔排列，那么这种方法将有效，前提是我选择了适当的搜索区域半径。
- en: Figure 14.8\. How UMAP learns a manifold. UMAP expands a search region around
    each case. A naive form of this is shown in the top row, where the radius of each
    search region is the same. When cases with overlapping search regions are connected
    by edges, there are gaps in the manifold. In the bottom row, the search region
    extends to the nearest neighbor and then extends outward in a fuzzy way, with
    a radius inversely related to the density of data in that region. This results
    in a complete manifold.
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.8\. UMAP如何学习一个流形。UMAP在每个案例周围扩展一个搜索区域。这种形式的直观表示在上排中，其中每个搜索区域的半径相同。当具有重叠搜索区域的案例通过边连接时，流形中会有间隙。在下排中，搜索区域扩展到最近的邻居，然后以一种模糊的方式向外扩展，其半径与该区域数据的密度成反比。这导致了一个完整的流形。
- en: '![](fig14-8_alt.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图14.8的替代图片](fig14-8_alt.jpg)'
- en: 'Real-world data is rarely evenly distributed, and UMAP solves this problem
    in two ways. First, it expands each search region for each case until it meets
    its nearest neighbor. This ensures that there are no orphan cases: while there
    can be multiple, disconnected manifolds in a dataset, every case must connect
    to at least one other case. Second, UMAP creates an additional search region that
    has a larger radius in lower-density areas and a smaller radius in high-density
    regions. These search regions are described as *fuzzy*, in that the further from
    the center another case finds itself, the lower the probability that an edge exists
    between those cases. This forces an artificial uniform distribution of the cases
    (and is where the “uniform” in UMAP comes from). This process is represented in
    the lower row of [figure 14.8](#ch14fig08); notice that we now get a more complete
    estimation of the underlying manifold.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据很少均匀分布，UMAP以两种方式解决这个问题。首先，它为每个案例扩展每个搜索区域，直到它遇到其最近邻。这确保了没有孤儿案例：虽然数据集中可以有多个不相连的流形，但每个案例必须至少连接到另一个案例。其次，UMAP在低密度区域创建一个具有更大半径的额外搜索区域，在高密度区域创建一个具有更小半径的搜索区域。这些搜索区域被描述为*模糊的*，因为另一个案例离中心越远，这些案例之间存在边的概率就越低。这强制对案例进行人工均匀分布（这也是UMAP中“均匀”一词的来源）。这个过程在[图14.8](#ch14fig08)的下方行中表示；请注意，我们现在得到了对潜在流形的更完整估计。
- en: The next step is to place the data onto a new manifold in (usually) two new
    dimensions. Then the algorithm iteratively shuffles this new manifold around until
    the distances between the cases along the manifold look like the distances between
    the cases along the original, high-dimensional manifold. This is similar to the
    optimization step of t-SNE, except that UMAP minimizes a different loss function
    called *cross-entropy* (whereas t-SNE minimizes KL divergence).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据放置在（通常是）两个新维度的新流形上。然后算法迭代地围绕这个新流形进行随机排列，直到流形上案例之间的距离看起来像原始高维流形上案例之间的距离。这与t-SNE的优化步骤类似，但UMAP最小化的是称为*交叉熵*的不同损失函数（而t-SNE最小化KL散度）。
- en: '|  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Just like for t-SNE, we can create more than two new dimensions if we want to.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 就像t-SNE一样，如果我们想的话，我们可以创建超过两个新维度。
- en: '|  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Once UMAP has learned the lower-dimensional manifold, new data can be projected
    onto this manifold to get the values on the new axes for visualization or as input
    for another machine learning algorithm.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦UMAP学会了低维流形，新的数据就可以投影到这个流形上，以获得新轴上的值，用于可视化或作为其他机器学习算法的输入。
- en: '|  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: UMAP can also be used to perform *supervised dimension reduction*, which really
    just means that given high-dimensional, labeled data, it learns a manifold that
    can be used to classify cases into groups.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP还可以用于执行*监督降维*，这实际上只是意味着给定高维、标记的数据，它学习一个流形，可以用来将案例分类到组中。
- en: '|  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 14.4\. Building your first UMAP model
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4. 构建您的第一个UMAP模型
- en: In this section, I’m going to show you how to use the UMAP algorithm to create
    a low-dimensional embedding of our Swiss banknote dataset. Remember that we’re
    trying to see if we can find a lower-dimensional representation of this dataset
    to help us identify patterns, such as different types of banknotes. We’ll start
    by installing and loading the umap package in R. Just as we did for t-SNE, we’ll
    discuss UMAP’s hyperparameters and how they affect the embedding. Then we’ll train
    a UMAP model on the banknote dataset and plot it to see how it compares with our
    PCA model and t-SNE embedding.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我将向您展示如何使用UMAP算法创建瑞士银行钞票数据集的低维嵌入。请记住，我们正在尝试找到这个数据集的低维表示，以帮助我们识别模式，例如不同类型的钞票。我们将首先在R中安装和加载umap包。就像我们对t-SNE所做的那样，我们将讨论UMAP的超参数以及它们如何影响嵌入。然后我们将在银行钞票数据集上训练一个UMAP模型，并绘制它以比较它与我们的PCA模型和t-SNE嵌入。 '
- en: 14.4.1\. Performing UMAP
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.1. 执行UMAP
- en: 'In this section, we’ll install and load the umap package and then tune and
    train our UMAP model. Let’s start by installing and loading the umap package:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将安装并加载umap包，然后调整和训练我们的UMAP模型。让我们首先安装并加载umap包：
- en: '[PRE13]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Just like t-SNE, UMAP has four important hyperparameters that control the resulting
    embedding:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 就像t-SNE一样，UMAP有四个重要的超参数，它们控制着结果的嵌入：
- en: '***n_neighbors—*** Controls the radius of the fuzzy search region. Larger values
    will include more neighboring cases, forcing the algorithm to focus on more global
    structure. Smaller values will include fewer neighbors, forcing the algorithm
    to focus on more local structure.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***n_neighbors—*** 控制模糊搜索区域的半径。较大的值将包括更多的邻近案例，迫使算法关注更全局的结构。较小的值将包括较少的邻居，迫使算法关注更局部结构。'
- en: '***min_dist—*** Defines the minimum distance apart that cases are allowed to
    be in the lower-dimensional representation. Low values result in “clumpy” embeddings,
    whereas larger values result in cases being spread further apart.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***min_dist—*** 定义了案例在低维表示中允许的最小距离。低值会导致“密集”的嵌入，而高值会导致案例分布得更远。'
- en: '***metric—*** Defines which distance metric UMAP will use to measure distances
    along the manifold. By default, UMAP uses ordinary Euclidean distance, but other
    (sometimes crazy) distance metrics can be used instead. A common alternative to
    Euclidean distance is *Manhattan distance* (also called *taxi cab distance*):
    instead of measuring the distance between two points as a single (possibly diagonal)
    distance, it measures the distance between two points one variable at a time and
    adds up these little journeys, just like a taxi cab driving around blocks in a
    city. We can also apply t-SNE with distance metrics other than Euclidean, but
    we first need to manually calculate these distances ourselves. The UMAP implementation
    just lets us specify the distance we want, and it takes care of the rest.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***metric—*** 定义了UMAP将使用哪种距离度量来测量流形上的距离。默认情况下，UMAP使用普通欧几里得距离，但也可以使用其他（有时很疯狂）的距离度量。欧几里得距离的常见替代方案是*曼哈顿距离*（也称为*出租车距离*）：它不是将两点之间的距离作为一个单一的（可能是对角线）距离来测量，而是逐个变量地测量两点之间的距离，并将这些小旅程相加，就像出租车在城市街区中行驶一样。我们还可以使用除了欧几里得距离以外的距离度量来应用t-SNE，但我们首先需要手动计算这些距离。UMAP实现只是让我们指定我们想要的距离，然后它会处理其余的事情。'
- en: '***n_epochs—*** Defines the number of iterations of the optimization step.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***n_epochs—*** 定义了优化步骤的迭代次数。'
- en: Once again, to give you a visual aid of how each of these hyperparameters affects
    the final embedding, I’ve run UMAP on our Swiss banknote data using a grid of
    hyperparameter values. [Figure 14.9](#ch14fig09) shows the final embeddings with
    different combinations of *n_neighbors* (rows) and *min_dist* (columns) using
    the default values of *metric* and *n_epochs*. Notice that the cases are more
    spread out for smaller values of *n_neighbors* and *min_dist* and that the clusters
    begin to break apart with low values for the *n_neighbors* hyperparameter.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，为了给您提供一个视觉辅助，展示每个超参数如何影响最终的嵌入，我已经使用一组超参数值在我们的瑞士纸币数据上运行了UMAP。[图14.9](#ch14fig09)显示了使用默认的*metric*和*n_epochs*值，不同组合的*n_neighbors*（行）和*min_dist*（列）的最终嵌入。请注意，*n_neighbors*和*min_dist*的值较小时，案例分布得更开，并且当*n_neighbors*超参数的值较低时，聚类开始分解。
- en: Figure 14.9\. The effect on the final UMAP embedding of the banknote dataset
    of changing *n_neighbors* (row facets) and *min_dist* (column facets) using the
    default values of *metric* and *n_epochs*
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.9\. 通过改变*n_neighbors*（行面元）和*min_dist*（列面元）的默认值，对纸币数据集的最终UMAP嵌入的影响，其中*n_neighbors*表示邻居数量，*min_dist*表示最小距离。
- en: '![](fig14-9_alt.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](fig14-9_alt.jpg)'
- en: Figure 14.10\. The effect on the final UMAP embedding of the swissTib dataset
    of changing *metric* (row facets) and *n_epochs* (column facets) using the default
    values of *n_neighbors* and *min_dist*
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.10\. 通过改变*metric*（行面元）和*n_epochs*（列面元）的默认值，对瑞士纸币数据集的最终UMAP嵌入的影响，其中*metric*表示度量，*n_epochs*表示迭代次数。
- en: '![](fig14-10_alt.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](fig14-10_alt.jpg)'
- en: '[Figure 14.10](#ch14fig10) shows the final embeddings with different combinations
    of *metric* (rows) and *n_epochs* (columns). The effect here is a little more
    subtle, but the clusters tend to be farther apart with more iterations. It also
    looks as though Manhattan distance does a slightly better job of breaking up those
    three smaller clusters (which we’ve not seen before!). If you would like to see
    the code I used to generate these figures, the code for this chapter is available
    at [www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.10](#ch14fig10)显示了具有不同组合的*度量*（行）和*n_epochs*（列）的最终嵌入。这里的效果稍微微妙一些，但集群在更多迭代中往往更远。看起来曼哈顿距离在打破那些三个较小的集群（我们之前没有见过！）方面做得稍微好一些！如果你想看到我用来生成这些图形的代码，本章的代码可在[www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr)找到。'
- en: I hope that demystifies UMAP’s hyperparameters a little. Now let’s run UMAP
    on our Swiss banknote dataset. Just like before, we first select all the columns
    except the categorical variable (UMAP cannot currently handle categorical variables,
    but this may change in the future) and pipe this data into the `as.matrix()` function
    (just to prevent an irritating warning message). This matrix is then piped into
    the `umap()` function, within which we manually set the values of all four hyperparameters
    and set the argument `verbose = TRUE` so the algorithm prints a running commentary
    on the number of epochs (iterations) that have passed.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这能稍微揭开UMAP超参数的神秘面纱。现在让我们在我们的瑞士钞票数据集上运行UMAP。就像之前一样，我们首先选择所有除了分类变量（目前UMAP无法处理分类变量，但将来可能会改变）之外的所有列，并将这些数据通过`as.matrix()`函数（只是为了防止一个令人烦恼的警告信息）。然后，这个矩阵被传递到`umap()`函数中，我们在其中手动设置所有四个超参数的值，并将`verbose
    = TRUE`参数设置为，以便算法打印出经过的每个epoch（迭代）的运行注释。
- en: Listing 14.3\. Performing UMAP
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.3. 执行UMAP
- en: '[PRE14]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 14.4.2\. Plotting the result of UMAP
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.2. 绘制UMAP的结果
- en: Next, let’s plot the two UMAP dimensions against each other to see how well
    they separated the genuine and counterfeit banknotes. We go through exactly the
    same process as we did in [listing 14.2](#ch14ex02) to reshape the data so it’s
    ready for plotting.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将两个UMAP维度相互对比，看看它们如何将真钞和假钞分开。我们通过与[列表14.2](#ch14ex02)中相同的步骤来重塑数据，使其准备好绘图。
- en: Listing 14.4\. Plotting the UMAP embedding
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.4. 绘制UMAP嵌入
- en: '[PRE15]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The resulting plot is shown in [figure 14.11](#ch14fig11). The UMAP embedding
    seems to suggest the existence of three different clusters of counterfeit banknotes!
    Perhaps there are three different counterfeiters at large.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图14.11](#ch14fig11)中。UMAP嵌入似乎暗示存在三个不同的假钞集群！也许有三个不同的造假者活跃着。
- en: Figure 14.11\. UMAP1 and UMAP2 axes plotted against each other, faceted and
    shaded by the original variables, and shaped by whether each case was a genuine
    or counterfeit banknote
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.11. UMAP1和UMAP2轴相互对比，通过原始变量进行分面和着色，并通过每个案例是真钞还是假钞进行形状塑造
- en: '![](fig14-11_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图14-11](fig14-11_alt.jpg)'
- en: 14.4.3\. Computing the UMAP embeddings of new data
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.3. 计算新数据的UMAP嵌入
- en: 'Remember I said that, unlike t-SNE, new data can be projected reproducibly
    onto a UMAP embedding? Well, let’s do this for the `newBanknotes` tibble we defined
    when predicting PCA component scores in [chapter 13](kindle_split_025.html#ch13)
    (rerun [listing 13.7](kindle_split_025.html#ch13ex07) if you no longer have this
    defined). In fact, the process is exactly the same: we use the `predict()` function
    with the model as the first argument and the new data as the second argument.
    This outputs a matrix, where the rows represent the two cases and the columns
    represent the UMAP axes:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我之前说过，与t-SNE不同，新数据可以被可重复地投影到UMAP嵌入中吗？好吧，让我们为在[第13章](kindle_split_025.html#ch13)中预测PCA成分得分时定义的`newBanknotes`
    tibble做这件事（如果你不再有这个定义，请重新运行[列表13.7](kindle_split_025.html#ch13ex07)）。实际上，这个过程完全相同：我们使用`predict()`函数，将模型作为第一个参数，新数据作为第二个参数。这将输出一个矩阵，其中行代表两个案例，列代表UMAP轴：
- en: '[PRE16]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 14.5\. Strengths and weaknesses of t-SNE and UMAP
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5. t-SNE和UMAP的优势和劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    t-SNE and UMAP will perform well for you.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对给定的任务表现良好，但以下是一些优势和劣势，这将帮助你决定t-SNE和UMAP是否适合你。
- en: 'The strengths of t-SNE and UMAP are as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE和UMAP的优势如下：
- en: They can learn nonlinear patterns in the data.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以在数据中学习非线性模式。
- en: They tend to separate clusters of cases better than PCA.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们比PCA更好地分离案例的簇。
- en: UMAP can make predictions on new data.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP可以对新数据进行预测。
- en: UMAP is computationally inexpensive.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP计算成本较低。
- en: UMAP preserves both local *and* global distances.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP保留局部和全局距离。
- en: 'The weaknesses of t-SNE and UMAP are these:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE和UMAP的弱点如下：
- en: The new axes of t-SNE and UMAP are not directly interpretable in terms of the
    original variables.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE和UMAP的新轴不能直接从原始变量中解释。
- en: t-SNE cannot make predictions on new data (different result each time).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE不能对新数据进行预测（每次结果都不同）。
- en: t-SNE is computationally expensive.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE计算成本较高。
- en: t-SNE doesn’t necessarily preserve global structure.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE不一定保留全局结构。
- en: They cannot handle categorical variables natively.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们无法原生处理分类变量。
- en: '|  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: 'Rerun UMAP on our Swiss banknote dataset, but this time include the argument
    `n_components = 3` (feel free to experiment by changing the values of the other
    hyperparameters). Pass the `$layout` component of the UMAP object to the `GGally
    ::ggpairs()` function. (Tip: You’ll need to wrap this object in `as.data.frame()`,
    or `ggpairs()` will have a hissy fit.)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的瑞士钞票数据集上重新运行UMAP，但这次包括参数`n_components = 3`（请随意通过更改其他超参数的值进行实验）。将UMAP对象的`$layout`组件传递给`GGally::ggpairs()`函数。（提示：您需要将此对象包裹在`as.data.frame()`中，否则`ggpairs()`会发怒。）
- en: '|  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: t-SNE and UMAP are nonlinear dimension-reduction algorithms.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE和UMAP是非线性降维算法。
- en: t-SNE converts the distances between all cases in the data into probabilities
    based on the normal distribution and then iteratively shuffles the cases around
    in a lower-dimensional space to reproduce these distances.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE将数据中所有案例之间的距离转换为基于正态分布的概率，然后迭代地在低维空间中重新排列案例以再现这些距离。
- en: In the lower-dimensional space, t-SNE uses Student’s t distribution to convert
    distances to probabilities to better separate clusters of data.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在低维空间中，t-SNE使用Student的t分布将距离转换为概率，以更好地分离数据簇。
- en: UMAP learns a manifold that the data are arranged along and then iteratively
    shuffles the data around in a lower-dimensional space to reproduce the distances
    between cases along the manifold.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP学习数据排列的流形，然后在低维空间中迭代地重新排列数据以再现沿流形的案例之间的距离。
- en: Solutions to exercises
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习解答
- en: 'Recreate the plot of t-SNE1 versus t-SNE2 without scaling the variables first:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不对变量进行缩放，重新创建t-SNE1与t-SNE2的绘图：
- en: '[PRE17]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Rerun UMAP, but output and plot three new axes instead of two:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行UMAP，但输出和绘制三个新轴而不是两个：
- en: '[PRE18]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Chapter 15\. Self-organizing maps and locally linear embedding
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第15章\. 自组织映射和局部线性嵌入
- en: '*This chapter covers*'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Creating self-organizing maps to reduce dimensionality
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自组织映射以降低维度
- en: Creating locally linear embeddings of high-dimensional data
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建高维数据的局部线性嵌入
- en: 'In this chapter, we’re continuing with dimension reduction: the class of machine
    learning tasks focused on representing the information contained in a large number
    of variables, in a smaller number of variables. As you learned in [chapters 13](kindle_split_025.html#ch13)
    and [14](kindle_split_026.html#ch14), there are multiple possible ways to reduce
    the dimensions of a dataset. Which dimension-reduction algorithm works best for
    you depends on the structure of your data and what you’re trying to achieve. Therefore,
    in this chapter, I’m going to add two more nonlinear dimension-reduction algorithms
    to your ever-growing machine learning toolbox: self-organizing maps (SOMs) and
    locally linear embedding (LLE).'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续讨论降维：一类机器学习任务，专注于以更少的变量表示大量变量中包含的信息。正如你在[第13章](kindle_split_025.html#ch13)和[第14章](kindle_split_026.html#ch14)中学到的，有多种方法可以降低数据集的维度。哪种降维算法最适合你取决于你的数据结构和你要实现的目标。因此，在本章中，我将向你的不断增长的机器学习工具箱中添加两个额外的非线性降维算法：自组织映射（SOMs）和局部线性嵌入（LLE）。
- en: '15.1\. Prerequisites: Grids of nodes and manifolds'
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1\. 前提：节点网格和流形
- en: Both the SOM and LLE algorithms reduce a large dataset into a smaller, more
    manageable number of variables, but they work in very different ways. The SOM
    algorithm creates a two-dimensional grid of *nodes*, like grid references on a
    map. Each case in the data is placed into a node and then shuffled around the
    nodes so that cases that are more similar to each other in the original data are
    put close together on the map.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: SOM算法和LLE算法都将大量数据集缩减为更小、更易于管理的变量数量，但它们的工作方式非常不同。SOM算法创建了一个二维的节点网格，就像地图上的网格参考。数据中的每个案例都被放置在一个节点中，然后在这些节点周围进行随机排列，使得在原始数据中彼此更相似的案例在地图上被放置得更近。
- en: This is probably difficult to picture in your head, so let’s look at an analogy.
    Imagine that you have a big jar of beads with your sewing kit. There are beads
    of different sizes and weights, and some are more elongated than others. It’s
    raining, and there’s nothing better to do, so you decide that you will organize
    your beads into sets to make it easier to find the beads you need in the future.
    You arrange a grid of bowls on the table and consider each bead in turn. You then
    place beads that are most similar to each other in the same bowl. You put beads
    that are similar, but not the same, in adjacent bowls, while beads that are very
    different go into bowls that are far away from each other. An example of what
    this might look like is shown in [figure 15.1](#ch15fig01).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很难在脑海中想象，所以让我们来看一个类比。想象一下，你有一个装满珠子的罐子，里面有你缝纫工具的珠子。珠子大小和重量不同，有些比其他的长。外面在下雨，没有更好的事情可做，所以你决定将珠子组织成几组，以便将来更容易找到所需的珠子。你在桌子上排列了一个碗的网格，并依次考虑每个珠子。然后，你将彼此最相似的珠子放在同一个碗里。相似的但不同的珠子放在相邻的碗里，而非常不同的珠子则放在彼此远离的碗里。一个可能的样子示例显示在[图15.1](#ch15fig01)中。
- en: Figure 15.1\. Placing beads into bowls based on their characteristics. Similar
    beads are placed in the same or nearby bowls, while dissimilar beads are placed
    in bowls far away from each other. One bowl didn’t have any beads placed in it,
    but that’s okay.
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.1\. 基于珠子的特征将珠子放入碗中。相似的珠子被放在同一个或附近的碗中，而不相似的珠子被放在彼此远离的碗中。有一个碗没有放入任何珠子，但这没关系。
- en: '![](fig15-1.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![图15-1](fig15-1.jpg)'
- en: Once you’ve placed all the beads into bowls, you look at your grid and notice
    that a pattern has emerged. All the large, spherical beads congregate around the
    top-right corner of the grid. As you move from right to left, the beads get smaller;
    and as you move from top to bottom, the beads become more elongated. Your process
    of placing beads into bowls based on the similarities between them has revealed
    structure in the beads.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将所有珠子都放入碗中，你看看你的网格，会发现出现了一种模式。所有的大球形珠子都聚集在网格的右上角。当你从右向左移动时，珠子变小；当你从上向下移动时，珠子变得更长。你根据珠子之间的相似性将珠子放入碗中的过程揭示了珠子的结构。
- en: This is what self-organizing maps try to do. The “map” of a self-organizing
    map is equivalent to the grid of bowls, where each bowl is called a *node*.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是自组织图试图做的事情。自组织图的“地图”相当于碗的网格，其中每个碗被称为*节点*。
- en: The LLE algorithm, on the other hand, learns a manifold on which the data lies,
    similar to the UMAP algorithm you saw in [chapter 14](kindle_split_026.html#ch14).
    Recall that a manifold is an *n*-dimensional smooth geometric shape that can be
    constructed from a series of linear “patches.” Whereas UMAP tries to learn the
    manifold in one go, LLE looks for these local, linear patches of data around each
    case, and then combines these linear patches together to form the (potentially
    nonlinear) manifold.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LLE算法学习数据所在的手性，类似于你在[第14章](kindle_split_026.html#ch14)中看到UMAP算法。回想一下，手性是一个*n*-维光滑的几何形状，可以通过一系列线性“补丁”构建。UMAP试图一次性学习手性，而LLE则寻找每个案例周围的局部线性补丁，然后将这些线性补丁组合起来形成（可能是非线性的）手性。
- en: If this is hard to picture, take a look at [figure 15.2](#ch15fig02). A sphere
    is a smooth, three-dimensional manifold. We can approximate a sphere by breaking
    it up into a series of flat surfaces that combine together (the more of these
    surfaces we use, the more closely we can approximate the sphere). This is shown
    on the left side of [figure 15.2](#ch15fig02). Imagine that someone gave you a
    flat sheet of paper and a pair of scissors, and asked you to create a sphere.
    You might cut the sheet into the kind of shape shown on the right side of [figure
    15.2](#ch15fig02). You could then fold this flat sheet of paper to approximate
    the sphere. Can you see that the flat, two-dimensional cutting is a lower-dimensional
    representation of the sphere? This is the general principle behind LLE, except
    that it tries to learn the manifold that represents the data, and represents it
    in fewer dimensions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这很难想象，请看[图15.2](#ch15fig02)。球是一个光滑的三维流形。我们可以通过将其分解成一系列组合在一起的平面来近似球体（我们使用的这些表面越多，我们就能越接近地近似球体）。这显示在[图15.2](#ch15fig02)的左侧。想象一下，有人给你一张平面的纸张和一把剪刀，并要求你制作一个球体。你可能会将纸张切割成[图15.2](#ch15fig02)右侧所示的那种形状。然后你可以折叠这张平面的纸张来近似球体。你能看出这个平面的二维切割是球体的低维表示吗？这是LLE背后的基本原理，只不过它试图学习表示数据的流形，并以更少的维度表示它。
- en: Figure 15.2\. A sphere is a three-dimensional manifold. We can reconstruct a
    sphere as a series of linear patches that connect to one another. This three-dimensional
    manifold of a sphere can be represented in two dimensions by cutting a sheet of
    paper in a certain way.
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.2. 球是一个三维流形。我们可以通过一系列相互连接的线性补丁来重建球体。这个球体的三维流形可以通过以某种方式切割纸张来在二维中表示。
- en: '![](fig15-2_alt.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![fig15-2_alt](fig15-2_alt.jpg)'
- en: In this chapter, I’ll show you in more detail how the SOM and LLE algorithms
    work and how we can use them to reduce the dimensions of data collected on various
    flea beetles. I’ll also show you a particularly fun example of how LLE can “unroll”
    some complex and unusually shaped data.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将更详细地展示SOM和LLE算法的工作原理以及我们如何使用它们来降低各种跳甲虫收集的数据的维度。我还会展示一个特别有趣的例子，说明LLE如何“展开”一些复杂和形状异常的数据。
- en: 15.2\. What are self-organizing maps?
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2. 什么是自组织映射？
- en: In this section, I’ll explain what SOMs are, how they work, and why they’re
    useful for dimension reduction. Consider the purpose of a map. Maps conveniently
    represent the layout of a part of the globe (which is not flat) in two dimensions,
    such that areas of the planet that are close to each other are drawn close to
    each other on the map. This is a convoluted way of saying that you’ll find India
    drawn closer to Sri Lanka than to Madagascar, because they are closer to each
    other in space.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释SOM是什么，它们是如何工作的，以及为什么它们对降维很有用。考虑地图的目的。地图方便地将地球的一部分（不是平面的）以二维的形式表示出来，使得在地图上彼此靠近的地球区域实际上是彼此靠近的。这是以一种复杂的方式来说明，你会发现印度比马达加斯加更靠近斯里兰卡，因为它们在空间上彼此更近。
- en: The goal of a SOM is very similar; but instead of countries, towns, and cities,
    the SOM tries to represent a dataset in two dimensions, such that cases in the
    data that are more similar to each other are drawn close to each other. The first
    step of the algorithm is to create a grid of nodes in a two-dimensional lattice
    (like the grid of bowls in [figure 15.1](#ch15fig01)).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的目标非常相似；但与国家、城镇和城市不同，SOM试图在二维中表示数据集，使得彼此更相似的数据案例在地图上彼此靠近。算法的第一步是在二维晶格中创建一个节点网格（就像[图15.1](#ch15fig01)中的碗的网格一样）。
- en: 15.2.1\. Creating the grid of nodes
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.1. 创建节点网格
- en: In this section, I’ll fully explain what I mean when I say the SOM algorithm
    creates a grid of nodes. Much like the grid of bowls we were sorting beads into
    in [figure 15.1](#ch15fig01), the SOM algorithm starts by creating a grid of nodes.
    For now, you can just think of a node as a bowl into which we will eventually
    put cases from the dataset. I’ve used the word *grid* to help you picture the
    lattice structure of the nodes, but the word *map* is more commonly used, so we’ll
    use this from now on.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将全面解释当我提到SOM算法创建节点网格时我的意思。这与我们在[图15.1](#ch15fig01)中排序珠子的碗的网格非常相似，SOM算法首先创建一个节点网格。目前，你可以将节点想象成一个碗，我们将最终将数据集中的案例放入其中。我使用“网格”这个词来帮助你想象节点的晶格结构，但“地图”这个词更常用，所以从现在起我们将使用这个词。
- en: The map can be made up of square/rectangular nodes, much like square grid references
    on a map; or hexagonal nodes, which fit together snugly like a honeycomb. When
    the map is made of square nodes, each node is connected to four of its neighbors
    (you could say they’re its north, south, east, and west neighbors). When the map
    is made of hexagonal nodes, each node is connected to six of its neighbors (northeast,
    east, southeast, southwest, west, and northwest). [Figure 15.3](#ch15fig03) shows
    two different ways that square and hexagonal SOMs are commonly represented. The
    left-side representation shows each node as a circle, connected to its neighbors
    with lines or *edges*. The right-side representation shows each node as a square
    or hexagon, connected to its neighbors across its flat sides. The dimensions of
    the map (how many rows and columns there are) need to be decided upon by us; I’ll
    show you how to choose an appropriate map size later in the chapter. Remember,
    we’re still thinking of these nodes as bowls.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 地图可以由正方形/矩形节点组成，就像地图上的正方形网格参考一样；或者由六边形节点组成，它们紧密地像蜂巢一样排列。当地图由正方形节点组成时，每个节点与其四个邻居相连（可以说它们是它的北、南、东和西邻居）。当地图由六边形节点组成时，每个节点与其六个邻居相连（东北、东、东南、西南、西和西北）。[图15.3](#ch15fig03)展示了正方形和六边形SOMs的两种常见表示方式。左侧表示将每个节点表示为一个圆圈，通过线条或**边**与邻居相连。右侧表示将每个节点表示为一个正方形或六边形，通过其平坦的侧面与邻居相连。地图的尺寸（有多少行和列）需要我们决定；我将在本章后面向你展示如何选择合适的地图大小。记住，我们仍然将这些节点视为碗。
- en: Figure 15.3\. Common graphical representations of square and hexagonal self-organizing
    maps. The top two maps show a grid of rectangular nodes that are each connected
    to four neighbors. The bottom two maps show a grid of hexagonal nodes that are
    each connected to six neighbors.
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.3\. 常见的正方形和六边形自组织地图的图形表示。上面两张地图显示了一个由矩形节点组成的网格，每个节点都与四个邻居相连。下面两张地图显示了一个由六边形节点组成的网格，每个节点都与六个邻居相连。
- en: '![](fig15-3_alt.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图15-3替代](fig15-3_alt.jpg)'
- en: '|  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-392
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: SOMs were created by a Finnish computer scientist named Teuvo Kohonen, so you
    will sometimes see them called *Kohonen maps*. The SOM algorithm has been so popular
    that Professor Kohonen is the most frequently cited Finnish computer scientist
    of all time.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织地图（SOMs）是由一位名叫Teuvo Kohonen的芬兰计算机科学家创建的，因此有时你会看到它们被称为**Kohonen地图**。SOM算法如此受欢迎，以至于Kohonen教授是有史以来被引用次数最多的芬兰计算机科学家。
- en: '|  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Once the map has been created, the next step is to randomly assign each node
    a set of *weights*.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了地图，下一步就是随机为每个节点分配一组**权重**。
- en: 15.2.2\. Randomly assigning weights, and placing cases in nodes
  id: totrans-396
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.2\. 随机分配权重，并将案例放置在节点中
- en: In this section, I’ll explain what I mean by the term *weights*, and what they
    relate to. I’ll show you how these weights are randomly initialized for every
    node in the map.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释“权重”这个术语的含义以及它们的相关性。我会向你展示这些权重是如何为地图中的每个节点随机初始化的。
- en: Imagine that we have a dataset with three variables, and we want to distribute
    the cases of this dataset across the nodes of our map. Eventually, we hope the
    algorithm will place the cases in the nodes such that similar cases are in the
    same node or a nearby node, and dissimilar cases are placed in nodes far away
    from each other.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含三个变量的数据集，我们希望将这个数据集的案例分布到地图的节点上。最终，我们希望算法将案例放置在节点上，使得相似的案例位于同一个节点或附近的节点，而不同的案例则放置在彼此远离的节点上。
- en: 'After the creation of the map, the next thing the algorithm does is randomly
    assign each node a set of weights: one weight for each variable in the dataset.
    So for our example, each node has three weights, because we have three variables.
    These weights are just random numbers, and you can think of them as guesses for
    the value of each of the variables. If this is hard to visualize, take a look
    at [figure 15.4](#ch15fig04). We have a dataset containing three variables, and
    we are looking at three nodes from a map. Each node has three numbers written
    under it: one corresponding to each variable in the dataset. For example, the
    weights for node 1 are 3 (for var 1), 9 (for var 2), and 1 (for var 3). Remember,
    at this point these are just random guesses for the value of each variable.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建地图之后，算法接下来要做的事情是随机为每个节点分配一组权重：数据集中每个变量的一个权重。所以，在我们的例子中，每个节点有三个权重，因为我们有三个变量。这些权重只是随机数，你可以把它们看作是对每个变量值的猜测。如果这很难想象，请看[图15.4](#ch15fig04)。我们有一个包含三个变量的数据集，我们正在查看地图上的三个节点。每个节点下面都写着三个数字：一个对应于数据集中的每个变量。例如，节点1的权重是3（变量1）、9（变量2）和1（变量3）。记住，在这个阶段，这些只是对每个变量值的随机猜测。
- en: Next, the algorithm chooses a case at random from the dataset and calculates
    which node’s weights are the closest match to this case’s values for each of the
    variables. For example, if there were a case in the dataset whose values for var
    1, var 2, and var 3 were 3, 9, and 1, respectively, this case would perfectly
    match the weights of node 1\. To find which node’s weights are most similar to
    the case in question, the distance is calculated between each case and the weights
    of each node in the map. This distance is usually the squared Euclidean distance.
    Remember that Euclidean distance is just the straight-line distance between two
    points, so the squared Euclidean distance just omits the square root step to make
    the computation faster.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，算法从数据集中随机选择一个案例，并计算每个变量的值与该案例的值最接近的节点权重。例如，如果数据集中有一个案例，其变量1、变量2和变量3的值分别为3、9和1，那么这个案例将完美匹配节点1的权重。为了找到与所讨论案例最相似的节点权重，算法计算每个案例与地图中每个节点权重的距离。这个距离通常是平方欧几里得距离。记住，欧几里得距离只是两点之间的直线距离，所以平方欧几里得距离只是省略了开方步骤，以加快计算速度。
- en: In [figure 15.4](#ch15fig04), you can see the distances calculated between the
    first case and each of the node’s weights. This case is most similar to the weights
    of node 1, because it has the smallest squared Euclidean distance to them (93.09).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15.4](#ch15fig04)中，你可以看到计算出的第一个案例与每个节点权重之间的距离。这个案例与节点1的权重最相似，因为它与它们的平方欧几里得距离最小（93.09）。
- en: '|  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The illustration in [figure 15.4](#ch15fig04) shows only three nodes, for brevity,
    but the distance is calculated for every single node on the map.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.4](#ch15fig04)中的插图只显示了三个节点，为了简洁起见，但地图上的每个节点都会计算距离。'
- en: '|  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Once the distances between a particular case and all of the nodes have been
    calculated, the node with the smallest distance (most similar to the case) is
    selected as that case’s *best matching unit* (BMU). This is illustrated in [figure
    15.5](#ch15fig05). Just like when we put beads into bowls, the algorithm takes
    that case and places it inside its BMU.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出特定案例与所有节点的距离，就选择距离最小的节点（与案例最相似）作为该案例的*最佳匹配单元*（BMU）。这如图15.5所示。就像我们往碗里放珠子一样，算法将那个案例放在它的BMU里。
- en: Figure 15.4\. How distances between each case to each node are calculated. The
    arrows pointing from each variable to each node represent the weight for that
    variable on that particular node (for example, the weights of node 1 are 3, 9,
    and 1). Distance is calculated by finding the difference between a node’s weights
    and a case’s value for each variable, squaring these differences, and summing
    them.
  id: totrans-407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.4。计算每个案例与每个节点之间距离的方法。从每个变量指向每个节点的箭头代表该变量在该特定节点上的权重（例如，节点1的权重是3、9和1）。距离是通过找到节点权重与案例每个变量的值的差异，平方这些差异，并将它们相加来计算的。
- en: '![](fig15-4_alt.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![图15-4_alt](fig15-4_alt.jpg)'
- en: Figure 15.5\. At each stage of the algorithm, the node whose weights have the
    smallest distance to a particular case is selected as the best matching unit (BMU)
    for that case.
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.5。在算法的每个阶段，选择与特定案例距离最小的节点作为该案例的最佳匹配单元（BMU）。
- en: '![](fig15-5.jpg)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![图15-5](fig15-5.jpg)'
- en: 15.2.3\. Updating node weights to better match the cases inside them
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.3。更新节点权重以更好地匹配其内部的案例
- en: 'In this section, I’ll show you how the weights of a case’s BMU and the weights
    of the surrounding nodes are updated to more closely match the data. First, though,
    let’s summarize our knowledge of the SOM algorithm so far:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何更新案例的BMU及其周围节点的权重，以更接近数据。首先，让我们总结一下到目前为止我们对SOM算法的了解：
- en: Create the map of nodes.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建节点图。
- en: Randomly assign weights to each node (one for each variable in the dataset).
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机为每个节点分配权重（每个变量一个）。
- en: Select a case at random, and calculate its distance to the weights of every
    node in the map.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个案例，并计算其与地图中每个节点权重的距离。
- en: Put the case into the node whose weights have the smallest distance to the case
    (the case’s BMU).
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将案例放入权重与案例距离最小的节点（案例的BMU）中。
- en: 'Now that the BMU has been selected, its weights are updated to be more similar
    to the case we placed inside it. However, it’s not only the BMU’s weights that
    are updated. Nodes in the *neighborhood* of the BMU also have their weights updated
    (nodes that are near to the BMU). We can define the neighborhood in a few different
    ways: a common way is to use the *bubble function*. With the bubble function,
    we simply define a radius (or bubble) around the BMU, and all nodes inside that
    radius have their weights updated to the same degree. Any nodes outside the radius
    are not updated at all. For the bubble function, a radius of 3 would include any
    node within three direct connections of the BMU.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在BMU已经被选中，其权重被更新，以使其与我们放入其中的案例更相似。然而，不仅仅是BMU的权重被更新。BMU的**邻域**中的节点也更新了它们的权重（靠近BMU的节点）。我们可以以几种不同的方式定义邻域：一种常见的方式是使用**气泡函数**。使用气泡函数，我们只需在BMU周围定义一个半径（或气泡），半径内的所有节点都会以相同的程度更新权重。半径外的节点则完全不更新。对于气泡函数，半径为3将包括BMU三个直接连接内的任何节点。
- en: Another popular choice is to update the node weights of the map based on how
    far they are from the BMU (the farther from the BMU, the less the node’s weights
    are updated). This is most commonly done using the *Gaussian* function. You can
    picture this as though we fit a Gaussian distribution centered over the BMU, and
    the node weights around the BMU are updated proportionally to the density of the
    Gaussian over them. We still define a radius around the BMU that defines how broad
    or skinny the Gaussian is, but this time it’s a soft radius that has no hard cutoff.
    The Gaussian function is popular, but it’s a little more computationally expensive
    than the simple bubble function.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的选择是根据节点与BMU的距离来更新地图中节点的权重（距离BMU越远，节点的权重更新越少）。这通常使用**高斯**函数来完成。你可以想象，我们为BMU拟合一个中心的高斯分布，BMU周围的节点权重将按照高斯密度成比例更新。我们仍然定义一个围绕BMU的半径，以定义高斯分布的宽度或瘦度，但这次它是一个没有硬截止的软半径。高斯函数很受欢迎，但它的计算成本比简单的气泡函数要高一些。
- en: '|  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-420
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The bubble and Gaussian functions used to update the weights of the nodes in
    the neighborhood around the BMU are called *neighborhood functions*.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 用于更新BMU周围节点权重的气泡函数和高斯函数被称为**邻域函数**。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Our choice of neighborhood function is a hyperparameter, as it will affect the
    way our map updates its nodes but cannot be estimated from the data itself.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的邻域函数是一个超参数，因为它会影响地图更新节点的方式，但不能从数据本身估计出来。
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You will sometimes see the set of weights for a node referred to as its *codebook
    vector*.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时会看到节点的权重集被称为其**代码向量**。
- en: '|  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Whichever neighborhood function we use, the benefit of updating node weights
    in a neighborhood around the BMU is that, over time, doing so creates neighborhoods
    of nodes that are similar to each other but still capture some variation in the
    data. Another trick the algorithm uses is that, as time goes on, both the radius
    of this neighborhood and the amount by which the weights are updated get smaller.
    This means the map is updated very rapidly initially and then makes smaller and
    smaller updates as the learning process continues. This helps the map converge
    to a solution that, hopefully, places similar cases in the same or nearby nodes.
    This process of updating node weights in the neighborhood of the BMU is illustrated
    in [figure 15.6](#ch15fig06).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用哪种邻域函数，在BMU周围更新节点权重的优点是，随着时间的推移，这样做会创建出彼此相似但仍然捕捉到数据中一些变化的节点邻域。算法使用的另一个技巧是，随着时间的推移，这个邻域的半径以及更新权重的量都会变小。这意味着地图最初更新得非常快，然后在学习过程中继续进行越来越小的更新。这有助于地图收敛到一个解决方案，希望将相似的案例放置在相同或附近的节点中。这个过程在[图15.6](#ch15fig06)中说明了在BMU邻域更新节点权重。
- en: Figure 15.6\. Between the first and last iteration of the algorithm, both the
    radius of the neighborhood around a BMU (the darkest node) and the amount by which
    neighboring node weights are updated get smaller. The radius of a Gaussian neighborhood
    function is shown as a translucent circle centered over the BMU, and the amount
    each neighboring node is updated is represented by how dark its shading is. If
    the bubble neighborhood function was shown, all nodes would be shaded the same
    (as they’re updated by the same amount).
  id: totrans-429
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.6. 在算法的第一次迭代和最后一次迭代之间，BMU周围邻域的半径（最暗的节点）以及更新相邻节点权重的量都变小了。高斯邻域函数的半径以半透明的圆圈形式显示在BMU上方，每个相邻节点更新的量通过其阴影的深浅来表示。如果显示气泡邻域函数，所有节点都会被同样着色（因为它们以相同的量更新）。
- en: '![](fig15-6.jpg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-6.jpg)'
- en: Now that we’ve determined the BMU for a particular case and updated its weights
    and the weights of its neighbors, we simply repeat the procedure for the next
    iteration, selecting another random case from the data. As this process continues,
    cases will likely be selected more than once and will move around the map as their
    BMU changes over time. To put it another way, cases will change nodes if the one
    they are currently in is no longer their BMU. Eventually, similar cases will converge
    to a particular region of the map.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了一个特定案例的BMU并更新了其权重及其邻居的权重，我们只需重复该过程进行下一次迭代，从数据中随机选择另一个案例。随着这个过程继续，案例可能会被多次选择，并且随着其BMU随时间变化，它们将在图上移动。换句话说，如果它们当前所在的节点不再是它们的BMU，案例将改变节点。最终，相似的案例将收敛到图上的特定区域。
- en: The result is that over time, the nodes on the map start to fit the dataset
    better. And eventually, cases that are similar to each other in the original feature
    space will be placed either in the same node or in nearby nodes on the map.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，随着时间的推移，地图上的节点开始更好地拟合数据集。最终，在原始特征空间中相似的案例将被放置在同一个节点或地图上附近的节点。
- en: '|  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that the *feature space* refers to all possible combinations of predictor
    variable values.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，*特征空间*指的是预测变量值的所有可能组合。
- en: '|  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Before we get our hands dirty by building our own SOM, let’s recap the whole
    algorithm to make sure it sticks in your mind:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们动手构建自己的SOM之前，让我们回顾一下整个算法，以确保它留在你的脑海中：
- en: Create the map of nodes.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建节点图。
- en: Randomly assign weights to each node (one for each variable in the dataset).
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机为每个节点分配权重（每个变量一个）。
- en: Select a case at random, and calculate its distance to the weights of every
    node in the map.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个案例，并计算其与图中每个节点权重的距离。
- en: Put the case into the node whose weights have the smallest distance to the case
    (the case’s BMU).
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将案例放入权重与案例距离最小的节点（案例的BMU）。
- en: Update the weights of the BMU and the nodes in its neighborhood (depending on
    the neighborhood function) to more closely match the cases inside it.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新BMU及其邻域（根据邻域函数）的权重，以更接近其内部的案例。
- en: Repeat steps 3-5 for the specified number of iterations.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3-5，直到指定的迭代次数。
- en: 15.3\. Building your first SOM
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3. 构建你的第一个SOM
- en: In this section, I’ll show you how to use the SOM algorithm to reduce the dimensions
    of a dataset into a two-dimensional map. By doing so, we hope to reveal some structure
    in the data by placing similar cases in the same or nearby nodes. For example,
    if a grouping structure is hidden in the data, we hope that different groups will
    separate to different regions of the map. I’ll also show you the algorithm’s hyperparameters
    and what they do.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用SOM算法将数据集的维度减少到二维地图。通过这样做，我们希望通过将相似的案例放置在相同的或附近的节点上，揭示数据中的某些结构。例如，如果数据中隐藏着分组结构，我们希望不同的组会分开到地图的不同区域。我还会向您展示算法的超参数以及它们的作用。
- en: '|  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that a hyperparameter is a variable that controls the performance/function
    of an algorithm but cannot be directly estimated from the data itself.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，超参数是一个控制算法性能/功能的变量，但不能直接从数据本身估计出来。
- en: '|  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Imagine that you’re the ringleader of a flea circus. You decide to take measurements
    for all of your fleas to see if different groups of fleas perform better at certain
    circus tasks. Let’s start by loading the tidyverse and GGally packages:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是跳蚤马戏团的头目。你决定测量所有跳蚤，看看不同的跳蚤群体在特定的马戏团任务中表现是否更好。让我们先加载tidyverse和GGally包：
- en: '[PRE19]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 15.3.1\. Loading and exploring the flea dataset
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.1\. 加载和探索跳蚤数据集
- en: Now let’s load the data, which is built into the GGally package; convert it
    into a tibble (with `as_tibble()`); and plot it using the `ggpairs()` function
    we discovered in [chapter 14](kindle_split_026.html#ch14).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载数据，这些数据是GGally包内置的；将其转换为tibble（使用`as_tibble()`）；并使用我们在第14章中发现的`ggpairs()`函数绘制它。
- en: Listing 15.1\. Loading and exploring the flea dataset
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.1\. 加载和探索跳蚤数据集
- en: '[PRE20]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have a tibble containing 7 variables, measured on 74 different fleas. The
    `species` variable is a factor telling us the species each flea belongs to, while
    the others are continuous measurements made on various parts of the fleas’ bodies.
    We’re going to omit the `species` variable from our dimension reduction, but we’ll
    use it later to see whether our SOM clusters together fleas from the same species.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含7个变量的tibble数据集，这些变量是在74只跳蚤的不同部位测量的。`species`变量是一个因子，告诉我们每只跳蚤属于哪个物种，而其他变量是对跳蚤身体各个部位的连续测量。在降维过程中，我们将省略`species`变量，但稍后我们会使用它来查看SOM是否将同一物种的跳蚤聚在一起。
- en: The resulting plot is shown in [figure 15.7](#ch15fig07). We can see that the
    three species of fleas can be discriminated between using different combinations
    of the continuous variables. Let’s train a SOM to reduce these six continuous
    variables into a representation with only two dimensions, and see how well it
    separates the three species of fleas.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图15.7](#ch15fig07)。我们可以看到，通过使用连续变量的不同组合，可以区分三种跳蚤的物种。让我们训练一个SOM，将这六个连续变量减少到只有两个维度的表示，并看看它如何将三种跳蚤的物种分开。
- en: Figure 15.7\. A matrix of plots created using the `ggpairs()` function, plotting
    all variables against each other from the flea dataset. Because the individual
    plots are quite small, I’ve manually zoomed in on one plot with a virtual magnifying
    glass (much like one you might need to use to see the fleas).
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.7\. 使用`ggpairs()`函数创建的矩阵图，将跳蚤数据集中的所有变量相互绘制。由于单个图相当小，我已手动使用虚拟放大镜放大了一个图（就像你可能需要用来看跳蚤的放大镜一样）。
- en: '![](fig15-7_alt.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-7_alt.jpg)'
- en: 15.3.2\. Training the SOM
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.2\. 训练SOM
- en: 'Let’s train our SOM to place fleas in nodes such that (hopefully) fleas of
    the same species are placed near each other and fleas of different species are
    separated. We start by installing and loading the kohonen package (named after
    Teuvo Kohonen, of course). The next thing we need to do is create a grid of nodes
    that will become our map. We do this using the `somgrid()` function (as shown
    in [listing 15.2](#ch15ex02)), and we have a few choices to make:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练我们的SOM，将跳蚤放置在节点上，以便（希望）同一物种的跳蚤彼此靠近，而不同物种的跳蚤则分开。我们首先安装并加载kohonen包（当然是以Teuvo
    Kohonen的名字命名的）。接下来，我们需要做的是创建一个将成为我们地图的节点网格。我们使用`somgrid()`函数来完成此操作（如[列表15.2](#ch15ex02)所示），并且我们有几个选择：
- en: The dimensions of the map
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地图的维度
- en: Whether our map will be made of rectangular or hexagonal nodes
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的地图是由矩形节点还是六边形节点组成
- en: Which neighborhood function to use
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用哪种邻域函数
- en: How the edges of the map will behave
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地图的边缘将如何表现
- en: I’ve used the arguments of the `somgrid()` function to make these choices, but
    let’s explore what they each mean and how they each affect the resulting map.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用了`somgrid()`函数的参数来做出这些选择，但让我们来探讨一下它们各自的意义以及它们如何影响生成的地图。
- en: Listing 15.2\. Loading the kohonen package and creating a SOM grid
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.2\. 加载kohonen包并创建SOM网格
- en: '[PRE21]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Choosing the dimensions of the map
  id: totrans-469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择地图的维度
- en: First, we need to choose the number of nodes in the x and y dimensions, using
    the `xdim` and `ydim` arguments, respectively. This is very important because
    it determines the size of the map and the granularity with which it will partition
    our cases. How do we choose the dimensions of our map? This, as it turns out,
    isn’t an easy question to answer. Too few nodes, and all of our data will be piled
    up so that clusters of cases merge with each other. Too many nodes, and we could
    end up with nodes containing a single case, or even no cases at all, diluting
    any clusters and preventing interpretation.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用`xdim`和`ydim`参数分别选择x和y维度的节点数量。这一点非常重要，因为它决定了地图的大小以及它将如何划分我们的案例。我们如何选择地图的维度？实际上，这并不是一个容易回答的问题。节点太少，所有数据都会堆积在一起，以至于案例的簇会相互合并。节点太多，我们可能会得到包含单个案例或根本没有案例的节点，这会稀释任何簇并阻止解释。
- en: The optimal dimensions of a SOM depend largely on the number of cases in the
    data. We want to aim to have cases in most of the nodes for a start, but really
    the optimal number of nodes in the SOM is whichever best reveals patterns in the
    data. We can also plot the *quality* of each node, which is a measure of the average
    difference between each case in a particular node and that node’s final weights.
    We can then consider choosing a map size that gives us the best-quality nodes.
    In this example, we’ll start by creating a 5 × 5 grid, but this subjectivity in
    selecting the dimensions of the map is arguably a weakness of SOMs.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的最佳维度在很大程度上取决于数据中的案例数量。我们首先希望大多数节点中都有案例，但真正最佳的SOM节点数量是能最好揭示数据中模式的数量。我们还可以绘制每个节点的*质量*，这是衡量特定节点中每个案例与该节点最终权重之间平均差异的度量。然后我们可以考虑选择一个地图大小，以给我们提供最佳质量的节点。在这个例子中，我们将从一个5×5的网格开始，但选择地图维度的主观性可以说是SOM的一个弱点。
- en: '|  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'The x and y dimensions of the grid don’t need to be of equal length. If I find
    a grid dimensionality that reveals patterns reasonably well in a dataset, I may
    extend the map in one dimension to see if this further helps to separate clusters
    of cases. There is an implementation of the SOM algorithm called *growing SOM*,
    where the algorithm grows the size of the grid based on the data. After you finish
    this chapter, I suggest you have a look at the GrowingSOM package in R: [https://github.com/alexhunziker/GrowingSOM](https://github.com/alexhunziker/GrowingSOM).'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 网格的x和y维度不需要长度相等。如果我发现某个网格维度在数据集中合理地揭示了模式，我可能会在某一维度上扩展地图，看看这能否进一步帮助区分案例的簇。有一种SOM算法的实现称为*增长SOM*，该算法根据数据增长网格的大小。完成这一章后，我建议您查看R中的GrowingSOM包：[https://github.com/alexhunziker/GrowingSOM](https://github.com/alexhunziker/GrowingSOM)。
- en: '|  |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Choosing whether the map has rectangular or hexagonal nodes
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择地图是否具有矩形或六边形节点
- en: The next choice is to decide whether our grid is formed of rectangular or hexagonal
    nodes. Rectangular nodes are connected to four adjacent nodes, whereas hexagonal
    nodes are connected to six adjacent nodes. Thus when a node’s weights are updated,
    a hexagonal node will update its six immediate neighbors the most, whereas a rectangular
    node will update its four immediate neighbors the most. While hexagonal nodes
    can potentially result in “smoother” maps in which clusters of data appear more
    rounded (whereas clusters of data in a grid of rectangular nodes may appear “blocky”),
    it depends on your data. In this example, we’ll specify that we want a hexagonal
    topology by setting the `topo = "hexagonal"` argument.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个选择是决定我们的网格是由矩形节点还是六边形节点组成。矩形节点与四个相邻节点相连，而六边形节点与六个相邻节点相连。因此，当一个节点的权重更新时，六边形节点将最多更新其六个直接邻居，而矩形节点将最多更新其四个直接邻居。虽然六边形节点可能产生“更平滑”的地图，其中数据簇看起来更圆滑（而矩形节点的网格中的数据簇可能看起来“块状”），但这取决于你的数据。在这个例子中，我们将通过设置`topo
    = "hexagonal"`参数来指定我们想要一个六边形拓扑。
- en: '|  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: I usually prefer the results I get from hexagonal nodes, both in terms of the
    patterns they reveal in my data and aesthetically.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常更喜欢六边形节点给出的结果，无论是从它们在我数据中揭示的模式，还是从美学角度来看。
- en: '|  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Choosing a neighborhood function
  id: totrans-482
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择邻域函数
- en: Next, we need to choose which neighborhood function we’re going to use, supplying
    our choice to the `neighbourhood.fct` argument (note the British spelling). The
    two options are `"bubble"` and `"gaussian"`, corresponding to the two neighborhood
    functions we discussed earlier. Our choice of neighborhood function is a hyperparameter,
    and we could tune it; but for this example we’re just going to use the bubble
    neighborhood function, which is the default.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择我们将使用哪个邻域函数，将我们的选择提供给 `neighbourhood.fct` 参数（注意英国拼写）。两个选项是 `"bubble"`
    和 `"gaussian"`，对应于我们之前讨论的两个邻域函数。我们选择的邻域函数是一个超参数，我们可以调整它；但在这个例子中，我们只是将使用气泡邻域函数，这是默认设置。
- en: Choosing how the map edges behave
  id: totrans-484
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择映射边的行为
- en: The final choice we need to make is whether we want our grid to be *toroidal*
    (another word to impress your friends with). If the grid is toroidal, nodes on
    the left edge of the map are connected to the nodes on the right edge (and the
    equivalent for nodes on the top and bottom edges). If you were to walk off the
    left edge of a toroidal map, you would reappear on the right! Because nodes on
    the edges have fewer connections to other nodes, their weights tend to be updated
    less than those of nodes in the middle of the map. Therefore, it may be beneficial
    to use a toroidal map to help prevent cases from “piling up” on the map edges,
    though toroidal maps tend to be harder to interpret. In this example, we will
    set the toroidal argument to `FALSE` to make the final map more interpretable.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的最后一个选择是是否希望我们的网格是 *toroidal*（另一个可以用来让你的朋友印象深刻的话）。如果网格是 toroidal，则地图左边的节点连接到右边的节点（以及顶部和底部边界的等效节点）。如果你从一个
    toroidal 地图的左边边缘走开，你会在右边重新出现！因为边缘上的节点与其他节点的连接较少，它们的权重更新往往少于地图中间的节点。因此，使用 toroidal
    地图可能有助于防止案例在地图边缘“堆积”，尽管 toroidal 地图往往更难解释。在这个例子中，我们将 toroidal 参数设置为 `FALSE`，以便使最终的地图更容易解释。
- en: Training the SOM with the som() function
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 `som()` 函数训练 SOM
- en: Now that we’ve initialized our grid, we can pass our tibble into the `som()`
    function to train our map.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初始化了我们的网格，我们可以将我们的 tibble 传递给 `som()` 函数来训练我们的映射。
- en: Listing 15.3\. Training the SOM
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 15.3\. 训练 SOM
- en: '[PRE22]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We start by piping the tibble into the `select()` function to remove the `species`
    factor. Cases are assigned to the node with the most similar weights, so it’s
    important to scale our variables so that variables on large scales aren’t given
    more importance. To this end, we pipe the output of the `select()` function call
    into the `scale()` function to center and scale each variable.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过将 tibble 管道到 `select()` 函数来移除 `species` 因子。案例被分配到具有最相似权重的节点，因此我们需要对变量进行缩放，以确保大尺度上的变量不会得到更多的重视。为此，我们将
    `select()` 函数调用的输出管道到 `scale()` 函数，以对每个变量进行居中和缩放。
- en: 'To build the SOM, we use the `som()` function from the kohonen package, supplying
    the following:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 SOM，我们使用 kohonen 包中的 `som()` 函数，提供以下内容：
- en: The data as the first argument
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据作为第一个参数
- en: The grid object created in [listing 15.2](#ch15ex02) as the second argument
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [清单 15.2](#ch15ex02) 中创建的网格对象作为第二个参数
- en: The two hyperparameter arguments *rlen* and *alpha*
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个超参数参数 *rlen* 和 *alpha*
- en: The *rlen* hyperparameter is simply the number of times the dataset is presented
    to the algorithm for sampling (the number of iterations); the default is 100\.
    Just like in other algorithms we’ve seen, more iterations are usually better until
    we get diminishing returns. I’ll show you soon how to assess whether you’ve included
    enough iterations.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '*rlen* 超参数简单来说就是数据集被算法用于采样的次数（即迭代次数）；默认值为 100。就像我们在其他算法中看到的那样，更多的迭代通常更好，直到我们得到递减的回报。我很快就会向你展示如何评估你是否已经包含了足够的迭代次数。'
- en: The *alpha* hyperparameter is the learning rate and is a vector of two values.
    Remember that as the number of iterations increases, the amount by which the weights
    of each node is updated decreases. This is controlled by the two values of *alpha*.
    Iteration 1 uses the first value of *alpha*, which linearly declines to the second
    value of *alpha* at the last iteration.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '*alpha* 超参数是学习率，它是一个包含两个值的向量。记住，随着迭代次数的增加，每个节点权重更新的量会减少。这是由 *alpha* 的两个值控制的。第
    1 次迭代使用 *alpha* 的第一个值，它以线性方式递减到第 1 次迭代的第二个 *alpha* 值。'
- en: The vector `c(0.05, 0.01)` is the default; but for larger SOMs, if you’re concerned
    the SOM is doing a poor job of separating classes with subtle differences between
    them, you can experiment with reducing these values to make the learning rate
    even slower.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 `c(0.05, 0.01)` 是默认值；但对于较大的 SOM，如果你担心 SOM 在区分具有微妙差异的类别时做得不好，你可以尝试将这些值降低以使学习率变得更慢。
- en: '|  |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: If you make the learning rate of an algorithm slower, you typically need to
    increase the number of iterations to help it converge to a stable result.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将算法的学习率设置得更慢，通常需要增加迭代次数以帮助它收敛到稳定的结果。
- en: '|  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 15.3.3\. Plotting the SOM result
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.3\. 绘制 SOM 结果图
- en: Now that we’ve trained our SOM, let’s plot some diagnostic information about
    it. The kohonen package comes with plotting functions to draw SOMs, but it uses
    base R graphics rather than ggplot2\. The syntax to plot a SOM object is `plot(x,
    type, shape)`, where `x` is our SOM object, `type` is the type of plot we want
    to draw, and `shape` lets us specify whether we want the nodes to be drawn as
    circles or with straight edges (squares if the grid is rectangular, hexagons if
    the grid is hexagonal).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的 SOM，让我们绘制一些关于它的诊断信息。kohonen 包附带用于绘制 SOM 的绘图函数，但它使用的是基础 R 图形而不是
    ggplot2。绘制 SOM 对象的语法是 `plot(x, type, shape)`，其中 `x` 是我们的 SOM 对象，`type` 是我们想要绘制的图表类型，而
    `shape` 允许我们指定是否要将节点绘制为圆形或带有直线边（如果网格是矩形的则为正方形，如果网格是六边形的则为六边形）。
- en: Listing 15.4\. Plotting SOM diagnostics
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.4\. 绘制 SOM 诊断图
- en: '[PRE23]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: I prefer to draw straight-edged plots, but the choice is aesthetic only. Experiment
    with setting the `shape` argument to `"round"` and `"straight"`.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 我更喜欢绘制边缘直线的图表，但这个选择只是美学上的。尝试将 `shape` 参数设置为 `"round"` 和 `"straight"` 进行实验。
- en: '|  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: There are six different diagnostic plots we can draw for our SOM, but rather
    than writing out the `plot()` function six times, we define a vector with the
    names of the plot types and use `walk()` to plot them all at once. We first split
    the plotting device into six regions by running `par(mfrow = c(2, 3))`.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为我们的 SOM 绘制六种不同的诊断图表，但与其将 `plot()` 函数写六次，我们定义一个包含图表类型名称的向量，并使用 `walk()`
    一次性绘制它们。我们首先通过运行 `par(mfrow = c(2, 3))` 将绘图设备分为六个区域。
- en: We could achieve the same thing with `purrr::map()`, but `purrr::walk()` calls
    a function for its side effects (such as drawing a plot) and silently returns
    its input (which is useful if you want to plot an intermediate dataset in a series
    of operations that pipe into each other). The convenience here is that `purrr:::walk()`
    doesn’t print any output to the console.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 `purrr::map()` 实现相同的效果，但 `purrr::walk()` 调用一个函数以产生副作用（如绘制图表）并静默返回其输入（如果你想在一系列操作中绘制中间数据集，这很有用）。这里的便利之处在于
    `purrr:::walk()` 不会向控制台打印任何输出。
- en: '|  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The kohonen package also contains a function called `map()`. If you have the
    kohonen package *and* the purrr package loaded, it’s a good idea to include the
    package prefix in the function call (`kohonen::map()` and `purrr::map()`).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: kohonen 包还包含一个名为 `map()` 的函数。如果你已经加载了 kohonen 包和 purrr 包，那么在函数调用中包含包前缀是一个好主意（`kohonen::map()`
    和 `purrr::map()`）。
- en: '|  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The resulting plots are shown in [figure 15.8](#ch15fig08). The Codes plot is
    a fan plot representation of the weights for each node. Each segment of the fan
    represents the weight for a particular variable (as designated in the legend),
    and the distance the fan extends from the center represents the magnitude of its
    weight. For example, nodes in the top-left corner of my plot have the highest
    weights for the `tars2` variable. This plot can help us to identify regions of
    the map that are associated with higher or lower values of particular variables.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在[图 15.8](#ch15fig08)中。代码图是每个节点权重的扇形图表示。扇形的每一部分代表特定变量（如图例中所示）的权重，扇形从中心延伸的距离代表其权重的幅度。例如，我图表的左上角的节点对于
    `tars2` 变量的权重最高。这个图表可以帮助我们识别与特定变量值较高或较低相关的地图区域。
- en: Figure 15.8\. Diagnostic plots for our SOM. The Codes fan plot for each node
    indicates the weight for each variable. The Training Progress plot shows the mean
    distance between each case and its BMU for each iteration. The Counts plot shows
    the number of cases per node. The Quality plot shows the mean distance between
    each case and the weights of its BMU. The Neighbor Distance plot shows the sum
    of differences between cases in one node and cases in neighboring nodes. The Mapping
    plot draws the cases inside their assigned nodes.
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.8\. 我们SOM的诊断图。每个节点的代码扇形图表示每个变量的权重。训练进度图显示了每个迭代中每个案例与其BMU之间的平均距离。案例数量图显示了每个节点的案例数量。质量图显示了每个案例与其BMU权重之间的平均距离。邻域距离图显示了同一节点中案例与相邻节点中案例之间差异的总和。映射图绘制了分配给每个节点的案例。
- en: '![](fig15-8_alt.jpg)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-8_alt.jpg)'
- en: '|  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Do your plots look a little different than mine? That’s because the node weights
    are randomly initialized each time we run the algorithm. Arguably, this is a disadvantage
    of the SOM algorithm, as it may produce different results on the same data when
    run repeatedly. This disadvantage is mitigated by the fact that—unlike t-SNE,
    for example—we can map new data onto an existing SOM.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 你的图看起来和我的一样吗？这是因为每次运行算法时节点权重都是随机初始化的。可以说，这是SOM算法的一个缺点，因为它可能在重复运行时对相同的数据产生不同的结果。这个缺点通过以下事实得到了缓解——与t-SNE不同，我们可以将新数据映射到现有的SOM上。
- en: '|  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The Training Progress plot helps us to assess if we have included enough iterations
    while training the SOM. The x-axis shows the number of iterations (specified by
    the *rlen* argument), and the y-axis shows the mean distance between each case
    and its BMU at each iteration. We hope to see the profile of this plot flatten
    out before we reach our maximum number of iterations, which it seems to in this
    case. If we felt that the plot hadn’t leveled out yet, we would increase the number
    of iterations.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 训练进度图帮助我们评估在训练SOM时是否包含了足够的迭代。x轴显示了迭代次数（由*rlen*参数指定），y轴显示了每个迭代中每个案例与其BMU之间的平均距离。我们希望看到这个图的轮廓在我们达到最大迭代次数之前变平，在这个例子中似乎就是这样。如果我们觉得这个图还没有平缓下来，我们会增加迭代次数。
- en: The Counts plot is a heatmap showing the number of cases assigned to each node.
    In this plot, we’re looking to be sure we don’t have lots of empty nodes (suggesting
    the map is too big) and that we have a reasonably even distribution of cases across
    the map. If we had lots of cases piled up at the edges, we might consider increasing
    the map dimensions or training a toroidal map instead.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 案例数量图是一个热力图，显示了分配给每个节点的案例数量。在这个图中，我们希望确保我们没有很多空节点（表明地图太大），并且案例在地图上的分布相当均匀。如果我们有很多案例堆积在边缘，我们可能会考虑增加地图维度或训练一个环状地图。
- en: The Quality plot shows the mean distance between each case and the weights of
    its BMU. The lower this value is, the better.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 质量图显示了每个案例与其BMU权重之间的平均距离。这个值越低，越好。
- en: The Neighbor Distance plot shows the sum of distances between cases in one node
    and cases in the neighboring nodes. You’ll sometimes see this referred to as a
    *U matrix plot*, and it can be useful in identifying clusters of cases on the
    map. Because cases on the edge of a cluster of nodes have a greater distance to
    cases in an adjacent cluster of nodes, high-distance nodes tend to separate clusters.
    This often looks like dark regions of the map (potential clusters) separated by
    light regions. It’s difficult to interpret a map as small as this, but it appears
    as though we may have clusters on the left and right edges, and possibly a cluster
    at the top center.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域距离图显示了同一节点中案例与相邻节点中案例之间的距离总和。有时你会看到这被称为*U矩阵图*，它有助于在地图上识别案例的簇。由于簇边缘的案例与相邻簇的案例距离更远，因此高距离节点往往将簇分开。这通常看起来像是地图上的暗区（潜在簇）被亮区（潜在簇）分开。解释如此小的地图是困难的，但看起来我们可能在左右边缘和可能在上中心有一个簇。
- en: Finally, the Mapping plot shows the distribution of cases among the nodes. Note
    that the position of a case within a node doesn’t mean anything—they are just
    *dodged* (moved a small, random distance) so that they don’t all sit on top of
    each other.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，映射图显示了案例在节点中的分布。请注意，案例在节点内的位置没有任何意义——它们只是*避开*（移动一小段随机距离），这样它们就不会全部堆叠在一起。
- en: 'The Codes plot is a useful way to visualize the weights of each node, but it
    becomes difficult to read when you have many variables, and it doesn’t give an
    interpretable indication of magnitude. Instead, I prefer to create heatmaps: one
    for each variable. We use the `getCodes()` function to extract the weights for
    each node, where each row is a node and each column is a variable, and convert
    this into a tibble. The following listing shows how to then create a separate
    heatmap for each variable, this time using `iwalk()` to iterate over each of the
    columns.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 代码图是可视化每个节点权重的有用方式，但当变量很多时，它变得难以阅读，并且不提供可解释的量级指示。相反，我更喜欢创建热图：每个变量一个。我们使用`getCodes()`函数提取每个节点的权重，其中每一行是一个节点，每一列是一个变量，并将其转换为tibble。以下列表显示了如何为每个变量创建单独的热图，这次使用`iwalk()`遍历每一列。
- en: Listing 15.5\. Plotting heatmaps for each variable
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.5。为每个变量绘制热图
- en: '[PRE24]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|  |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 2](kindle_split_011.html#ch02) that each of the `map()`
    functions has an `i` equivalent (`imap()`, `imap_dbl()`, `iwalk()`, and so on)
    that allows us to pass the name/position of each element to the function. The
    `iwalk()` function is shorthand for `walk2(.x, .y = names(.x), .f)`, allowing
    us to access the name of each element by using `.y` inside our function.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第2章](kindle_split_011.html#ch02)，每个`map()`函数都有一个`i`等价物（`imap()`、`imap_dbl()`、`iwalk()`等），它允许我们将每个元素的名称/位置传递给函数。`iwalk()`函数是`walk2(.x,
    .y = names(.x), .f)`的简写，允许我们通过在函数内部使用`.y`来访问每个元素的名称。
- en: '|  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We set the `type` argument equal to `"property"`, which allows us to color each
    node by some numerical property. We then use the `property` argument to tell the
    function exactly what property we want to plot. To set the title of each plot
    equal to the name of the variable it displays, we set the `main` argument equal
    to `.y` (this is why I chose to use `iwalk()` instead of `walk()`).
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`type`参数设置为`"property"`，这允许我们根据某些数值属性对每个节点进行着色。然后我们使用`property`参数告诉函数我们想要绘制哪个属性。为了将每个图的标题设置为显示的变量的名称，我们将`main`参数设置为`.y`（这就是为什么我选择使用`iwalk()`而不是`walk()`）。
- en: The resulting plot is shown in [figure 15.9](#ch15fig09). The heatmaps show
    very different patterns of weights for each of the variables. Nodes on the right
    side of the map have higher weights for the `tars1` and `aede2` variables and
    lower weights for the `aede3` variable (which is lowest in the bottom-right corner
    of the map). Nodes in the upper-left corner of the map have higher weights for
    the `tars2`, `head`, and `aede1` variables. Because the variables were scaled
    before training the SOM, the heatmap scales are in standard deviation units for
    each variable.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图15.9](#ch15fig09)中。热图显示了每个变量非常不同的权重模式。地图右侧的节点对于`tars1`和`aede2`变量具有更高的权重，而对于`aede3`变量（在地图右下角最低）具有较低的权重。地图左上角的节点对于`tars2`、`head`和`aede1`变量具有更高的权重。由于变量在训练SOM之前进行了缩放，因此热图的刻度是以每个变量的标准差单位。
- en: Because we have some class information about our fleas, let’s plot our SOM,
    coloring each case by its species.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对跳蚤有一些类别信息，让我们根据物种对SOM进行着色。
- en: Listing 15.6\. Plotting flea species onto the SOM
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.6。将跳蚤物种绘制到SOM上
- en: '[PRE25]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Figure 15.9\. Separate heatmaps showing node weights for each original variable.
    The scales are in standard deviation units.
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.9。分别显示每个原始变量节点权重的热图。刻度是以标准差单位。
- en: '![](fig15-9_alt.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![图15-9的替代图片](fig15-9_alt.jpg)'
- en: First, we define a vector of colors to use to distinguish the classes from each
    other. Then, we create a mapping plot using the `plot()` function, and using the
    `type = "mapping"` argument. We set the `pch = 21` argument to use a filled circle
    to indicate each case (so we can set a background color for each species). The
    `bg` argument sets the background color of the points. By converting the `species`
    variable into a numeric vector and using it to subset the color vector, each point
    will have a background color corresponding to its species. Finally, we use the
    `shape` argument to draw hexagons instead of circles, and set the background color
    (`bgcol`) equal to `"lightgrey"`.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个颜色向量，用于区分不同的类别。然后，我们使用`plot()`函数创建一个映射图，并使用`type = "mapping"`参数。我们将`pch
    = 21`参数设置为使用实心圆来表示每个案例（因此我们可以为每个物种设置背景颜色）。`bg`参数设置点的背景颜色。通过将`species`变量转换为数值向量并使用它来子集颜色向量，每个点都将有一个与其物种对应的背景颜色。最后，我们使用`shape`参数绘制六边形而不是圆形，并将背景颜色（`bgcol`）设置为`"lightgrey"`。
- en: The resulting plot is shown in [figure 15.10](#ch15fig10). Can you see that
    the SOM has arranged itself such that fleas from the same species (that are more
    similar to each other than fleas from other species) are assigned to nodes near
    cases of the same species? I’ve created a plot on the right side of [figure 15.10](#ch15fig10)
    that used a clustering algorithm to find clusters of nodes. I’ve colored the nodes
    by the cluster each node was assigned to, and added thick borders that separate
    the clusters. Because we haven’t covered clustering yet, I don’t want to explain
    how I did this (the code is available at [www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr)),
    but I wanted to show you that the SOM managed to separate the different classes
    and that clustering can be performed on a SOM! We’ll start covering clustering
    in the next chapter.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '结果图显示在[图 15.10](#ch15fig10)中。你能看到 SOM 已经自行排列，使得来自同一物种的跳蚤（彼此之间比来自其他物种的跳蚤更相似）被分配到与同一物种案例相近的节点中吗？我在[图
    15.10](#ch15fig10)的右侧创建了一个图，该图使用聚类算法找到节点簇。我根据每个节点所属的簇给节点着色，并添加了分隔簇的粗边框。因为我们还没有介绍聚类，所以我不想解释我是如何做到这一点的（代码可在[www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr)找到），但我想向你展示
    SOM 成功地将不同的类别分开，并且可以在 SOM 上执行聚类！我们将在下一章开始介绍聚类。 '
- en: Figure 15.10\. Showing class membership on the SOM. The left-side mapping plot
    shows cases drawn inside their assigned nodes, shaded by which flea species they
    belong to. The right-side plot shows the same information, but nodes are shaded
    by cluster membership after applying a clustering algorithm to the nodes. The
    solid black lines separate nodes assigned to different clusters.
  id: totrans-544
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.10\. 在 SOM 上显示类别成员。左侧映射图显示了绘制在其分配节点内的案例，根据它们所属的跳蚤物种进行着色。右侧图显示了相同的信息，但节点在应用聚类算法后根据簇成员进行着色。实线黑色线条分隔了分配给不同簇的节点。
- en: '![](fig15-10_alt.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-10_alt.jpg)'
- en: '|  |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: SOMs are a little different than other dimension-reduction techniques, in that
    they don’t really create new variables for which each case is given a value (for
    example, the principal components in PCA). SOMs reduce dimensionality by placing
    cases into nodes on a two-dimensional map, rather than creating new variables.
    So if we want to perform cluster analysis on the result of a SOM, we can use the
    weights to cluster the nodes. This essentially treats each node as a case in a
    new dataset. If our cluster analysis returns clusters of nodes, we can assign
    cases from the original dataset to the cluster that their node belongs to.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs 与其他降维技术略有不同，因为它们并不真正创建新的变量，每个案例都赋予一个值（例如，PCA 中的主成分）。SOMs 通过将案例放置在二维地图上的节点中而不是创建新变量来降低维度。因此，如果我们想在
    SOM 的结果上执行聚类分析，我们可以使用权重来聚类节点。这本质上是将每个节点视为新数据集中的一个案例。如果我们的聚类分析返回节点簇，我们可以将原始数据集中的案例分配给其节点所属的簇。
- en: '|  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: 'Create another map using the `somgrid()` function, but this time set the arguments
    as follows:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `somgrid()` 函数创建另一个地图，但这次设置参数如下：
- en: '`topo = rectangular`'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topo = rectangular`'
- en: '`toroidal = TRUE`'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`toroidal = TRUE`'
- en: Train a SOM using this map, and create its mapping plot, as in [figure 15.10](#ch15fig10).
    Notice how each node is now connected with four of its neighbors. Can you see
    how the `toroidal` argument affects the final map? If not, set this argument to
    `FALSE`, but keep everything else the same, and see the difference.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此地图训练一个 SOM，并创建其映射图，如[图 15.10](#ch15fig10)所示。注意每个节点现在与四个邻居节点相连。你能看到 `toroidal`
    参数如何影响最终地图吗？如果不能，将此参数设置为 `FALSE`，但保持其他一切不变，看看有什么区别。
- en: '|  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 15.3.4\. Mapping new data onto the SOM
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.4\. 将新数据映射到 SOM 上
- en: In this section, I’ll show you how we can take new data and map it onto our
    trained SOM. Let’s create two new cases with all of the continuous variables in
    the data we used to train the SOM.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何将新数据映射到我们的训练好的 SOM 上。让我们创建两个新案例，这些案例包含我们用于训练 SOM 的数据中的所有连续变量。
- en: Listing 15.7\. Plotting flea species onto the SOM
  id: totrans-559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.7\. 在 SOM 上绘制跳蚤物种
- en: '[PRE26]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once we define the tibble, we pipe it into the `scale()` function, because
    we trained the SOM on scaled data. But here’s the really important part: a common
    mistake is to scale the new data by subtracting its own mean and dividing by its
    own standard deviation. This will likely lead to an incorrect mapping, because
    we need to subtract the mean and divide by the standard deviation *of the training
    set*. Fortunately, these values are stored as attributes of the scaled dataset,
    and we can access them using the `attr()` function.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了tibble，我们就将其管道输入到`scale()`函数中，因为我们是在缩放数据上训练SOM的。但这里有一个非常重要的部分：一个常见的错误是通过对新数据减去其自身的平均值并除以其自身的标准差来缩放新数据。这可能会导致错误的映射，因为我们需要减去训练集的平均值并除以标准差。幸运的是，这些值存储为缩放数据集的属性，我们可以使用`attr()`函数来访问它们。
- en: '|  |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you’re not quite sure what the `attr()` function is retrieving, run `attributes(fleaScaled)`
    to see the full list of attributes of the `fleaScaled` object.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定`attr()`函数正在检索什么，请运行`attributes(fleaScaled)`以查看`fleaScaled`对象的完整属性列表。
- en: '|  |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We use the `predict()` function with the SOM object as the first argument and
    the new, scaled data as the second argument, to map the new data onto our SOM.
    We can then plot the position of the new data on the map using the `plot()` function,
    supplying the `type = "mapping"` argument. The `classif` argument allows us to
    specify an object returned by the `predict()` function, to draw only the new data.
    This time, we use the argument `shape = "round"` to show what the circular nodes
    look like.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`predict()`函数，将SOM对象作为第一个参数，将新的、缩放后的数据作为第二个参数，将新数据映射到我们的SOM上。然后，我们可以使用`plot()`函数，提供`type
    = "mapping"`参数，在地图上绘制新数据的位置。`classif`参数允许我们指定由`predict()`函数返回的对象，以仅绘制新数据。这次，我们使用`shape
    = "round"`参数来显示圆形节点的外观。
- en: The resulting plot is shown in [figure 15.11](#ch15fig11). Each case is placed
    in a separate node whose weights best represent the case’s variable values. Look
    back at [figures 15.9](#ch15fig09) and [15.10](#ch15fig10) and see what you can
    infer about these two cases based on their position on the map.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图15.11](#ch15fig11)。每个案例都被放置在一个单独的节点中，其权重最能代表案例的变量值。回顾[图15.9](#ch15fig09)和[15.10](#ch15fig10)，看看您可以根据它们在地图上的位置对这些两个案例做出什么推断。
- en: Figure 15.11\. New data can be mapped onto an existing SOM. This mapping plot
    shows a graphical representation of the nodes to which the two new cases are assigned.
  id: totrans-568
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.11.新数据可以映射到现有的SOM上。此映射图显示了分配给两个新案例的节点图形表示。
- en: '![](fig15-11.jpg)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-11.jpg)'
- en: '|  |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Using SOMs for supervised learning**'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用SOMs进行监督学习**'
- en: We’re concentrating on SOMs for their use as unsupervised learners for dimension
    reduction. This is probably the most common use for SOMs, but they can also be
    used for both regression and classification, making SOMs very unusual among machine
    learning algorithms.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于SOMs，因为它们作为无监督学习器用于降维。这可能是SOMs最常见的使用方式，但它们也可以用于回归和分类，这使得SOMs在机器学习算法中非常独特。
- en: 'In a supervised setting, SOMs actually create two maps: let’s call them the
    x and y maps. The x map is the same as what you’ve learned so far; the weights
    of its nodes are iteratively updated such that similar cases are placed in nearby
    nodes and dissimilar cases are placed in distant nodes, using only the predictor
    variables in the dataset. Once the cases are placed into their respective nodes
    on the x map, they don’t move. The weights of the y map’s nodes represent values
    of the outcome variable. The algorithm now randomly selects cases again and iteratively
    updates the weights of each y map node to better match the values of the outcome
    variable of the cases in that node. The weights could represent a continuous outcome
    variable (in the case of regression) or a set of class probabilities (in the case
    of classification).'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习环境中，SOMs实际上创建了两个映射：让我们称它们为x映射和y映射。x映射与您迄今为止所学的相同；其节点的权重会迭代更新，以便将相似案例放置在附近的节点中，将不相似案例放置在较远的节点中，仅使用数据集中的预测变量。一旦案例被放置在x映射上的相应节点中，它们就不会移动。y映射的节点权重代表结果变量的值。现在，算法再次随机选择案例，并迭代更新每个y映射节点的权重，以更好地匹配该节点中案例的结果变量值。这些权重可以代表连续的结果变量（在回归的情况下）或一组类别概率（在分类的情况下）。
- en: We can train a supervised SOM using the `xyf()` function from the kohonen package.
    Use `?xyf()` to learn more.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用kohonen包中的`xyf()`函数来训练一个监督SOM。使用`?xyf()`了解更多信息。
- en: '|  |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 15.4\. What is locally linear embedding?
  id: totrans-576
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.什么是局部线性嵌入？
- en: 'In this section, I’ll explain what LLE is, how it works, why it’s useful, and
    how it differs from SOMs. Just like UMAP, the LLE algorithm tries to identify
    an underlying manifold that the data lies on. But LLE does this in a slightly
    different way: instead of trying to learn the manifold all at once, it learns
    local, linear patches of data around each case and then combines these linear
    patches to form the (potentially nonlinear) manifold.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释LLE是什么，它是如何工作的，为什么它有用，以及它与SOMs的不同之处。就像UMAP一样，LLE算法试图识别数据所在的基础流形。但LLE以略不同的方式做到这一点：它不是试图一次性学习流形，而是在每个案例周围学习局部、线性的数据片段，然后将这些线性片段组合起来形成（可能是非线性的）流形。
- en: '|  |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'An oft-quoted mantra of the LLE algorithm is to “think globally, fit locally”:
    the algorithm looks at small, local patches around each case and uses these patches
    to construct the wider manifold.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: LLE算法经常引用的格言是“全局思考，局部拟合”：算法查看每个案例周围的局部小片，并使用这些小片来构建更广泛的流形。
- en: '|  |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The LLE algorithm is particularly good at “unrolling” or “unfurling” data that
    is rolled or twisted into unusual shapes. For example, imagine a three-dimensional
    dataset where the cases are rolled up into a Swiss roll. The LLE algorithm is
    capable of unrolling the data and representing it as a two-dimensional rectangle
    of data points.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: LLE算法特别擅长“展开”或“展开”卷曲或扭曲成不寻常形状的数据。例如，想象一个三维数据集，其中案例被卷成瑞士卷。LLE算法能够展开数据，并将其表示为数据点的二维矩形。
- en: Figure 15.12\. The distance between each case and every other case is calculated,
    and their k-nearest neighbors are assigned (distance along the z-axis in the top-left
    plot is indicated by the size of the circle). For each case, the algorithm learns
    a set of weights, one for each nearest neighbor, that sum to 1\. Each neighbor’s
    variable values are multiplied by its weight (so row 1 becomes x = 3.1 × 0.1,
    y = 2.0 × 0.1, z = 0.1 × 0.1). The weighted values of each neighbor are summed
    (the columns are summed) to approximate the original values of the selected case.
  id: totrans-583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.12. 计算每个案例与其他每个案例之间的距离，并分配它们的k个最近邻（左上角图中沿z轴的距离通过圆圈的大小表示）。对于每个案例，算法学习一组权重，每个最近邻一个，这些权重之和为1。每个邻居的变量值乘以其权重（因此第1行变为x
    = 3.1 × 0.1，y = 2.0 × 0.1，z = 0.1 × 0.1）。每个邻居的加权值相加（列相加）以近似所选案例的原始值。
- en: '![](fig15-12_alt.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![图15.12的替代图片](fig15-12_alt.jpg)'
- en: 'So how does the LLE algorithm work? Take a look at [figure 15.12](#ch15fig12).
    It starts by selecting a case from the dataset and calculating its k-nearest neighbors
    (this is just like in the kNN algorithm from [chapter 3](kindle_split_013.html#ch03),
    so *k* is a hyperparameter of the LLE algorithm). LLE then represents this case
    as a linear, weighted sum of its *k* neighbors. I can already hear you asking:
    what does that mean? Well, each of the *k* neighbors is assigned a weight: a value
    between 0 and 1, such that the weights for all the k-nearest neighbors sum to
    1\. The variable values of a particular neighbor are multiplied by its weight
    (so the weighted values are a fraction of the original values).'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，LLE算法是如何工作的呢？看看[图15.12](#ch15fig12)。它首先从数据集中选择一个案例并计算其k个最近邻（这就像第3章中的kNN算法一样，所以*k*是LLE算法的超参数）。LLE随后将此案例表示为其*k*个邻居的线性、加权总和。我都能听到你在问：那是什么意思？好吧，每个*k*个邻居都被分配了一个权重：一个介于0和1之间的值，使得所有k个最近邻的权重之和为1。特定邻居的变量值乘以其权重（因此加权值是原始值的一部分）。
- en: '|  |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-587
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because the LLE algorithm relies on measuring the distance between cases to
    calculate the nearest neighbors, it is sensitive to differences between the scales
    of the variables. It’s often a good idea to scale the data before embedding it.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LLE算法依赖于测量案例之间的距离来计算最近邻，所以它对变量尺度之间的差异很敏感。在嵌入之前对数据进行缩放通常是一个好主意。
- en: '|  |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When the weighted values for each variable are added up across the k-nearest
    neighbors, this new weighted sum should approximate the variable values of the
    case for which we calculated the k-nearest neighbors in the first place. Therefore,
    the LLE algorithm learns a weight for each nearest neighbor such that, when we
    multiply each neighbor by its weight and add these values together, we get the
    original case (or an approximation). This is what I mean when I say LLE represents
    each case as a linear, weighted sum of its neighbors.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个变量的加权值在k个最近邻中累加时，这个新的加权总和应该近似于我们最初计算k个最近邻的案例的变量值。因此，LLE算法为每个最近邻学习一个权重，使得当我们乘以每个邻居的权重并将这些值相加时，我们得到原始案例（或近似值）。这就是我说LLE将每个案例表示为其邻居的线性加权总和时的意思。
- en: 'This process is repeated for each case in the dataset: its k-nearest neighbors
    are calculated, and then weights are learned that can be used to reconstruct it.
    Because the weights are combined linearly (summed), the algorithm is essentially
    learning a linear “patch” around each case. But how does it combine these patches
    to learn the manifold? Well, the data is placed into a low-dimensional space,
    typically two or three dimensions, such that the coordinates in this new space
    preserve the weights learned in the previous step. Put another way, the data is
    placed in this new feature space such that each case can *still* be calculated
    from the weighted sum of its neighbors.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程对数据集中的每个案例重复进行：计算其k个最近邻，然后学习可以用来重建它的权重。因为权重是线性组合的（相加），所以该算法本质上是在每个案例周围学习一个线性“补丁”。但是它是如何组合这些补丁来学习流形的呢？嗯，数据被放置在一个低维空间中，通常是两到三个维度，这样在这个新空间中的坐标保留了之前步骤中学习的权重。换句话说，数据被放置在这个新特征空间中，这样每个案例仍然可以通过其邻居的加权总和来计算。
- en: 15.5\. Building your first LLE
  id: totrans-592
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5\. 构建您的第一个LLE
- en: In this section, I’ll show you how to use the LLE algorithm to reduce the dimensions
    of a dataset into a two-dimensional map. We’ll start with an unusual example that
    really shows off the power of LLE as a nonlinear dimension-reduction algorithm.
    This example is unusual because it represents data shaped in a three-dimensional
    *S* that is unlike something we’re likely to encounter in the real world. Then
    we’ll use LLE to create a two-dimensional embedding of our flea circus data to
    see how it compares to the SOM we created earlier.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用LLE算法将数据集的维度降低到二维地图。我们将从一个非常规的例子开始，这个例子真正展示了LLE作为非线性降维算法的强大功能。这个例子之所以不寻常，是因为它表示的是三维形状为*S*的数据，这与我们在现实世界中可能遇到的情况不同。然后我们将使用LLE创建我们跳蚤马戏团数据的二维嵌入，以查看它与我们之前创建的SOM相比如何。
- en: 15.5.1\. Loading and exploring the S-curve dataset
  id: totrans-594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.5.1\. 加载和探索S曲线数据集
- en: 'Let’s start by installing and loading the lle package:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装并加载lle包：
- en: '[PRE27]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next, let’s load the lle_scurve_data dataset from the lle package, give names
    to its variables, and convert it into a tibble. We have a tibble containing 800
    cases and 3 variables.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从lle包中加载lle_scurve_data数据集，为其变量命名，并将其转换为tibble。我们有一个包含800个案例和3个变量的tibble。
- en: Listing 15.8\. Loading the S-curve dataset
  id: totrans-598
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.8\. 加载S曲线数据集
- en: '[PRE28]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This dataset consists of cases that are folded into the shape of the letter
    *S* in three dimensions. Let’s create a three-dimensional plot to visualize this,
    using the plot3D and plot3Drgl packages (starting with their installation).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含折叠成三维中字母*S*形状的案例。让我们创建一个三维图来可视化这一点，使用plot3D和plot3Drgl包（从它们的安装开始）。
- en: Listing 15.9\. Plotting the S-curve dataset in three dimensions
  id: totrans-601
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.9\. 在三维中绘制S曲线数据集
- en: '[PRE29]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `scatter3D()` function allows us to create a three-dimensional plot, and
    the `plotrgl()` function lets us rotate it interactively. Here is a summary of
    the arguments to `scatter3D()`:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '`scatter3D()`函数允许我们创建一个三维图，而`plotrgl()`函数允许我们交互式地旋转它。以下是`scatter3D()`函数参数的摘要：'
- en: '`x`, `y`, and `z`—Which variables to plot on their respective axes.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`、`y`和`z`—要在各自的轴上绘制哪些变量。'
- en: '`pch`—The shape of the points we wish to draw (`19` draws filled circles).'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pch`—我们希望绘制的点的形状（`19`绘制实心圆圈）。'
- en: '`bty`—The box type that’s drawn around the data (`"b2"` draws a white box with
    gridlines; use `?scatter3D` to see the alternatives).'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bty`—绘制在数据周围的框类型（`"b2"`绘制带有网格线的白色框；使用`?scatter3D`查看其他选项）。'
- en: '`colkey`—Whether we want a legend for the coloring of each point.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colkey`—我们是否想要为每个点的着色添加图例。'
- en: '`theta` and `phi`—The viewing angle of the plot.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`theta`和`phi`—该图的视角角度。'
- en: '`col`—The color palette we want to use to indicate the value of the `z` variable.
    Here, we use the `ramp.col()` function to specify the start and end colors of
    a color gradient.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col`—我们想要使用的调色板，以指示 `z` 变量的值。在这里，我们使用 `ramp.col()` 函数来指定颜色渐变的起始和结束颜色。'
- en: Once we’ve created our static plot, we can turn it into an interactive plot
    that we can rotate by clicking and rotating it with our mouse, by simply calling
    the `plotrgl()` function with no arguments.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了我们的静态图表，我们可以通过简单地调用 `plotrgl()` 函数（不带任何参数）将其转换为交互式图表，我们可以通过点击和旋转鼠标来旋转它。
- en: '|  |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-612
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can use your mouse scroll wheel to zoom in and out of this interactive plot.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用鼠标滚轮来放大和缩小这个交互式图表。
- en: '|  |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The resulting plot is shown in [figure 15.13](#ch15fig13). Can you see that
    the data forms a three-dimensional *S*? This is an unusual dataset for sure, but
    one which I hope demonstrates the power of LLE for learning the manifold that
    underlies a dataset.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在 [图 15.13](#ch15fig13) 中。你能看到数据形成了一个三维的 *S* 形吗？这确实是一个不寻常的数据集，但我希望它能展示
    LLE 在学习数据集下隐含的流形时的强大能力。
- en: Figure 15.13\. The S-curve dataset plotted in three dimensions using the `scatter3D()`
    function. The shading of the points is mapped to the `z` variable.
  id: totrans-616
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.13\. 使用 `scatter3D()` 函数在三维中绘制的 S 曲线数据集。点的阴影映射到 `z` 变量。
- en: '![](fig15-13.jpg)'
  id: totrans-617
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig15-13.jpg)'
- en: 15.5.2\. Training the LLE
  id: totrans-618
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.5.2\. 训练 LLE
- en: Aside from the number of dimensions to which we want to reduce our dataset (usually
    two or three), *k* is the only hyperparameter we need to select. We can choose
    the best-performing value of *k* by using the `calc_k()` function. This function
    applies the LLE algorithm to our data, using different values of *k* in a range
    we specify. For each embedding that uses a different *k*, `calc_k()` calculates
    the distances between cases in the original data and in the low-dimensional representation.
    The correlation coefficient between these distances is calculated (ρ, or “rho”)
    and used to calculate a metric (1 – ρ ˆ 2) that can be used to select *k*. The
    value of *k* with the smallest value for this metric is the one that best preserves
    the distances between cases in the high- and low-dimensional representations.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们想要将数据集减少到的维度数量（通常是两到三个），*k* 是我们唯一需要选择的超参数。我们可以通过使用 `calc_k()` 函数来选择 *k*
    的最佳性能值。这个函数将 LLE 算法应用于我们的数据，使用我们在指定范围内指定的不同 *k* 值。对于使用不同 *k* 的每个嵌入，`calc_k()`
    计算原始数据中以及在低维表示中的案例之间的距离。这些距离之间的相关系数被计算出来（ρ，或“rho”），并用于计算一个指标（1 – ρ²），该指标可用于选择
    *k*。这个指标值最小的 *k* 是在高低维表示之间最好地保留了案例之间距离的值。
- en: 'Here is a summary of the arguments of `calc_k()`:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `calc_k()` 函数参数的摘要：
- en: The first argument is the dataset.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数是数据集。
- en: The `m` argument is the number of dimensions we want to reduce our dataset into.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m` 参数是我们想要将数据集减少到的维数。'
- en: The `kmin` and `kmax` arguments specify the minimum and maximum values of the
    range of *k* values the function will use.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmin` 和 `kmax` 参数指定函数将使用的 *k* 值范围的最低值和最高值。'
- en: The `cpus` argument lets us specify the number of cores we want to use for parallelization
    (I used `parallel::detectCores()` to use all of them).
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpus` 参数让我们指定我们想要用于并行化的核心数（我使用了 `parallel::detectCores()` 来使用所有核心）。'
- en: '|  |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because we’re calculating an embedding for each value of *k*, if our range of
    values is large and/or our dataset contains many cases, I recommend parallelizing
    this function by setting the `parallel` argument to `TRUE`.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为每个 *k* 值计算一个嵌入，如果我们的值范围很大，并且/或者我们的数据集中包含许多案例，我建议通过将 `parallel` 参数设置为 `TRUE`
    来并行化此函数。
- en: '|  |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: When this function has finished, it will draw a plot showing the 1 – ρ² metric
    for each value of *k* (see [figure 15.14](#ch15fig14)).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个函数完成后，它将绘制一个图表，显示每个 *k* 值的 1 – ρ² 指标（见 [图 15.14](#ch15fig14)）。
- en: Figure 15.14\. Plotting 1 – ρ² against *k* to find the optimal value of *k*.
    The solid horizontal line indicates the value of *k* with the lowest 1 – ρ².
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.14\. 将 1 – ρ² 对 *k* 进行绘图以找到 *k* 的最佳值。实线水平线表示 1 – ρ² 最低的 *k* 值。
- en: '![](fig15-14_alt.jpg)'
  id: totrans-631
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig15-14_alt.jpg)'
- en: The `calc_k()` function also returns a `data.frame` containing the 1 – ρ² metric
    for each value of *k*. We use the `filter()` function to select the row containing
    the lowest value of the `rho` column. We will use the value of *k* that corresponds
    to this smallest value, to train our final LLE. In this example, the optimal value
    of *k* is 17 neighbors.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc_k()` 函数还返回一个包含每个 *k* 值的 1 – ρ² 度量的 `data.frame`。我们使用 `filter()` 函数选择包含
    `rho` 列最低值的行。我们将使用与这个最小值相对应的 *k* 值来训练我们的最终 LLE。在这个例子中，*k* 的最佳值是 17 个邻居。'
- en: '|  |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-634
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: This is a little confusing because, actually, we want the highest value of rho
    (ρ), which gives us the smallest value of 1 – ρ². Despite this column being called
    `rho`, it contains the values of 1 – ρ², and so we want the smallest of these
    values.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点令人困惑，因为我们实际上想要最高的 rho (ρ) 值，这给我们最小的 1 – ρ² 值。尽管这个列被称为 `rho`，但它包含 1 – ρ² 的值，因此我们想要这些值中最小的一个。
- en: '|  |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Finally, we run the LLE algorithm using the `lle()` function, supplying the
    following:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `lle()` 函数运行 LLE 算法，提供以下内容：
- en: The data as the first argument
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据作为第一个参数
- en: The number of dimensions we want to embed into as the `m` argument
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要嵌入的维数的数量作为 `m` 参数
- en: The value of the *k* hyperparameter
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 超参数的值'
- en: Listing 15.10\. Calculating *k* and performing the LLE
  id: totrans-641
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.10\. 计算 *k* 并执行 LLE
- en: '[PRE30]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 15.5.3\. Plotting the LLE result
  id: totrans-643
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.5.3\. 绘制 LLE 结果
- en: Now that we’ve performed our embedding, let’s extract the two new LLE axes and
    plot the data onto them. This will allow us to visualize our data in this new,
    two-dimensional space to see if the algorithm has revealed a grouping structure.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了嵌入，让我们提取两个新的 LLE 轴并将数据绘制到它们上面。这将使我们能够在这个新的二维空间中可视化我们的数据，以查看算法是否揭示了分组结构。
- en: Listing 15.11\. Plotting the LLE
  id: totrans-645
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.11\. 绘制 LLE
- en: '[PRE31]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We start by mutating two new columns onto our original tibble, each containing
    the values of one of the new LLE axes. We then use the `ggplot()` function to
    plot the two LLE axes against each other, mapping the *z* variable to the color
    aesthetic. We add a `geom_point()` layer and a `scale_color_gradient()` layer
    that specifies the extreme colors of a color scale that will be mapped to the
    *z* variable. This will allow us to directly compare the position of each case
    in our new, two-dimensional representation to its position in the three-dimensional
    plot in [figure 15.13](#ch15fig13).
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在我们的原始 tibble 上突变两个新列，每个列包含一个新 LLE 轴的值。然后我们使用 `ggplot()` 函数将两个 LLE 轴相互绘制，将
    *z* 变量映射到颜色美学。我们添加一个 `geom_point()` 层和一个 `scale_color_gradient()` 层，该层指定将映射到 *z*
    变量的颜色尺度的极端颜色。这将使我们能够直接比较每个案例在我们新的二维表示中的位置与其在 [图 15.13](#ch15fig13) 中的三维图中的位置。
- en: The resulting plot is shown in [figure 15.15](#ch15fig15). Can you see that
    LLE has flattened out the *S* shape into a flat, two-dimensional rectangle of
    points? If not, take a look back at [figure 15.13](#ch15fig13) and try to relate
    the two figures. It’s almost as if the data had been drawn onto a folded piece
    of paper, and LLE straightened it out! This is the power of manifold-learning
    algorithms for dimension reduction.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 15.15](#ch15fig15) 中。你能看到 LLE 将 *S* 形状扁平化为一个平坦的二维点矩形吗？如果不能，请回顾 [图
    15.13](#ch15fig13) 并尝试将两个图联系起来。这几乎就像数据被画在折叠的纸张上，而 LLE 将其拉直了！这是降维的流形学习算法的力量。
- en: Figure 15.15\. Plotting the two-dimensional embedding of the S-curve data. The
    shading of the points is mapped to the `z` variable, the same as in [figure 15.11](#ch15fig11).
  id: totrans-649
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.15\. 绘制 S 曲线数据的二维嵌入图。点的阴影映射到 `z` 变量，与 [图 15.11](#ch15fig11) 中的相同。
- en: '![](fig15-15_alt.jpg)'
  id: totrans-650
  prefs: []
  type: TYPE_IMG
  zh: '![](fig15-15_alt.jpg)'
- en: 15.6\. Building an LLE of our flea data
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.6\. 构建我们跳蚤数据的 LLE
- en: One criticism that is sometimes leveled at LLE is that it is designed to handle
    “toy data”—in other words, data that is constructed to form interesting and unusual
    shapes, but which rarely (if ever) manifests in real-world datasets. The S-curve
    data we worked on in the previous section is an example of toy data that was generated
    to test algorithms that learn a manifold that the data lies on. So in this section,
    we’re going to see how well LLE performs on our flea circus dataset, and whether
    it can identify the clusters of fleas like our SOM could.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 有时对LLE的批评是，它被设计来处理“玩具数据”——换句话说，是构建成有趣和不同形状的数据，但在现实世界的数据集中很少（如果有的话）体现出来。我们在上一节中工作的S曲线数据是这种玩具数据的例子，它是为了测试学习数据所在流形的算法而生成的。因此，在本节中，我们将看到LLE在我们的跳蚤马戏团数据集上的表现如何，以及它是否能够像我们的SOM一样识别跳蚤簇。
- en: 'We’re going to follow the same procedure as for the S-curve dataset:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循与S曲线数据集相同的程序：
- en: Use the `calc_k()` function to calculate the best-performing value of *k*.
  id: totrans-654
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`calc_k()`函数计算最佳性能的*k*值。
- en: Perform the embedding in two dimensions.
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在二维空间中进行嵌入。
- en: Plot the two new LLE axes against each other.
  id: totrans-656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个新的LLE轴相互绘制出来。
- en: This time, let’s map the `species` variable to the color aesthetic, to see how
    well our LLE embedding separates the clusters.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，让我们将`物种`变量映射到颜色美学上，看看我们的LLE嵌入如何将簇分离得有多好。
- en: Listing 15.12\. Performing and plotting LLE on the flea dataset
  id: totrans-658
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.12。在跳蚤数据集上执行和绘制LLE
- en: '[PRE32]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The resulting plots are shown in [figure 15.16](#ch15fig16) (I combined the
    plots into a single figure to save room). LLE seems to do a decent job of separating
    the different species of fleas, though the result isn’t quite as impressive as
    the way LLE was able to unravel the S-curve dataset.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图15.16](#ch15fig16)中（我将图表合并到一个图中以节省空间）。LLE似乎在分离跳蚤的不同物种方面做得相当不错，尽管结果并不像LLE能够解开S曲线数据集那样令人印象深刻。
- en: Figure 15.16\. Plotting the output of [listing 15.12](#ch15ex12). The top plot
    shows 1 – ρ² for different values of *k*. The lower plot shows the two-dimensional
    embedding of the flea data, shaded by species.
  id: totrans-661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.16。绘制[列表15.12](#ch15ex12)的输出。顶部图显示了不同*k*值的1 – ρ²。底部图显示了跳蚤数据的二维嵌入，按物种着色。
- en: '![](fig15-16_alt.jpg)'
  id: totrans-662
  prefs: []
  type: TYPE_IMG
  zh: '![图15-16](fig15-16_alt.jpg)'
- en: '|  |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-664
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Sadly, because each case is reconstructed as a weighted sum of its neighbors,
    new data cannot be projected onto an LLE map. For this reason, LLE cannot be easily
    used as a preprocessing step for other machine learning algorithms, as new data
    can’t be passed through it.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，因为每个案例都被重建为其邻居的加权求和，新数据不能投影到LLE映射上。因此，LLE不能轻易用作其他机器学习算法的预处理步骤，因为新数据不能通过它。
- en: '|  |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: Add 95% confidence ellipses for each flea species to the lower plot shown in
    [figure 15.16](#ch15fig16).
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个跳蚤物种的95%置信椭圆添加到[图15.16](#ch15fig16)中显示的底部图中。
- en: '|  |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 15.7\. Strengths and weaknesses of SOMs and LLE
  id: totrans-671
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.7。SOMs和LLE的优势与劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    the SOM or LLE will perform well for you.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对特定任务表现良好，但以下是一些优势和劣势，这将帮助您决定SOM或LLE是否适合您。
- en: 'The strengths of SOMs and LLE are as follows:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs和LLE的优势如下：
- en: They are both nonlinear dimension-reduction algorithms, and so can reveal patterns
    in the data where linear algorithms (like PCA) may fail.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都是非线性降维算法，因此可以揭示数据中的模式，而线性算法（如PCA）可能无法揭示这些模式。
- en: New data can be mapped onto an existing SOM.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据可以映射到现有的SOM上。
- en: They are reasonably inexpensive to train.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的训练成本相对较低。
- en: Rerunning the LLE algorithm on the same dataset with the same value of *k* will
    always produce the same embedding.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同的*k*值下，重新运行LLE算法在相同的数据集上会产生相同的嵌入。
- en: 'The weaknesses of SOMs and LLE are these:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs和LLE的劣势如下：
- en: They cannot natively handle categorical variables.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们无法原生地处理分类变量。
- en: The lower-dimensional representations are not directly interpretable in terms
    of the original variables.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低维表示在原始变量的意义上不可直接解释。
- en: They are sensitive to data on different scales.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对不同尺度的数据敏感。
- en: New data cannot be mapped onto an existing LLE.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据不能映射到现有的LLE上。
- en: They don’t necessarily preserve the global structure of the data.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不一定保留数据的全局结构。
- en: Rerunning the SOM algorithm on the same dataset will produce a different map
    each time.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同的数据集上重新运行SOM算法每次都会产生不同的映射。
- en: Small SOMs can be difficult to interpret, so the algorithm works best with large
    datasets (greater than hundreds of cases).
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型 SOM 可能难以解释，因此该算法在大型数据集（超过数百个案例）上表现最佳。
- en: '|  |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 3**'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: Using the original `somGrid` we created, create another SOM, but increase the
    number of iterations to 10,000, and set the *alpha* argument to `c(0.1, 0.001)`
    to slow the learning rate. Create the mapping plot just like in [exercise 1](#ch15sb01).
    Retrain and plot the SOM multiple times. Is the mapping less variable than before?
    Can you think why?
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们创建的原始 `somGrid`，创建另一个 SOM，但将迭代次数增加到 10,000，并将 *alpha* 参数设置为 `c(0.1, 0.001)`
    以减慢学习速率。创建与练习 1 中相同的映射图。多次重新训练并绘制 SOM。映射是否比之前更稳定？你能想到为什么吗？
- en: '|  |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 4**'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: Repeat our LLE embedding, but embed in three dimensions instead of two. Plot
    this new embedding using the `scatter3()` function, coloring the points by species.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 重复我们的 LLE 嵌入，但将嵌入在三维而不是二维空间中。使用 `scatter3()` 函数绘制这个新的嵌入，并按物种着色点。
- en: '|  |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 5**'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 5**'
- en: Repeat our LLE embedding (in two dimensions), but this time use the unscaled
    variables. Plot the two LLE axes against each other, and map the `species` variable
    to the color aesthetic. Compare this embedding to the result using scaled variables.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 重复我们的 LLE 嵌入（在二维中），但这次使用未缩放的变量。将两个 LLE 轴相互绘制，并将 `species` 变量映射到颜色美学。将此嵌入与使用缩放变量的结果进行比较。
- en: '|  |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Summary
  id: totrans-698
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: SOMs create a grid/map of nodes to which cases in the dataset are assigned.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SOMs 创建一个网格/地图，将数据集中的案例分配到节点上。
- en: SOMs learn patterns in the data by updating the weights of each node until the
    map converges to a set of weights that preserves similarities among the cases.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SOMs 通过更新每个节点的权重来学习数据中的模式，直到地图收敛到一组权重，该权重保留案例之间的相似性。
- en: New data can be mapped onto an existing SOM, and SOM nodes can be clustered
    based on their weights.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据可以映射到现有的 SOM 上，并且可以根据它们的权重对 SOM 节点进行聚类。
- en: LLE reconstructs each case as a linear weighted sum of its neighbors.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLE 将每个案例重建为其邻居的线性加权总和。
- en: LLE then embeds the data in a lower-dimensional feature space that preserves
    the weights.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLE 然后将数据嵌入到一个低维特征空间中，该空间保留了权重。
- en: LLE is excellent at learning complex manifolds that underlie a set of data,
    but new data cannot be mapped onto an existing embedding.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLE 极佳于学习一组数据下复杂的流形，但新数据不能映射到现有的嵌入中。
- en: Solutions to exercises
  id: totrans-705
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习解答
- en: 'Train a rectangular, toroidal SOM:'
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个矩形、环形的 SOM：
- en: '[PRE33]'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Add 95% confidence ellipses for each flea species to the plot of LLE1 versus
    LLE2:'
  id: totrans-708
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个跳蚤物种的 95% 置信椭圆添加到 LLE1 与 LLE2 的图中：
- en: '[PRE34]'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Train a SOM with more iterations, but a slower learning rate:'
  id: totrans-710
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更多迭代但较慢的学习速率训练 SOM：
- en: '[PRE35]'
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Train an LLE in three dimensions:'
  id: totrans-712
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三维空间中训练 LLE：
- en: '[PRE36]'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Train an LLE on the unscaled flea data:'
  id: totrans-714
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在未缩放的跳蚤数据上训练 LLE：
- en: '[PRE37]'
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
