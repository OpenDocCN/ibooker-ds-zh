- en: 'Chapter 4\. Replication and other controllers: deploying managed pods'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章\. 复制和其他控制器：部署托管Pod
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Keeping pods healthy
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持Pod健康
- en: Running multiple instances of the same pod
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行相同Pod的多个实例
- en: Automatically rescheduling pods after a node fails
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点失败后自动重新调度Pod
- en: Scaling pods horizontally
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 横向扩展Pod
- en: Running system-level pods on each cluster node
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个集群节点上运行系统级Pod
- en: Running batch jobs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行批处理作业
- en: Scheduling jobs to run periodically or once in the future
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期或将来运行作业的调度
- en: As you’ve learned so far, pods represent the basic deployable unit in Kubernetes.
    You know how to create, supervise, and manage them manually. But in real-world
    use cases, you want your deployments to stay up and running automatically and
    remain healthy without any manual intervention. To do this, you almost never create
    pods directly. Instead, you create other types of resources, such as ReplicationControllers
    or Deployments, which then create and manage the actual pods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，Pod是Kubernetes中的基本可部署单元。你知道如何手动创建、监督和管理它们。但在实际应用场景中，你希望你的部署能够自动保持运行状态，无需任何手动干预，并保持健康。为此，你几乎从不直接创建Pod。相反，你创建其他类型的资源，例如ReplicationControllers或Deployments，然后由它们创建和管理实际的Pod。
- en: When you create unmanaged pods (such as the ones you created in the previous
    chapter), a cluster node is selected to run the pod and then its containers are
    run on that node. In this chapter, you’ll learn that Kubernetes then monitors
    those containers and automatically restarts them if they fail. But if the whole
    node fails, the pods on the node are lost and will not be replaced with new ones,
    unless those pods are managed by the previously mentioned ReplicationControllers
    or similar. In this chapter, you’ll learn how Kubernetes checks if a container
    is still alive and restarts it if it isn’t. You’ll also learn how to run managed
    pods—both those that run indefinitely and those that perform a single task and
    then stop.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建未管理的Pod（例如你在上一章中创建的Pod）时，集群节点被选中来运行Pod，然后在该节点上运行其容器。在本章中，你将了解到Kubernetes随后会监控这些容器，并在它们失败时自动重新启动它们。但如果整个节点失败，该节点上的Pod将丢失，并且不会用新的Pod替换，除非这些Pod由之前提到的ReplicationControllers或类似资源管理。在本章中，你将了解Kubernetes如何检查容器是否仍然存活，并在它不存活时重新启动它。你还将了解如何运行托管Pod——包括那些无限期运行和那些执行单个任务后停止的Pod。
- en: 4.1\. Keeping pods healthy
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1\. 保持Pod健康
- en: One of the main benefits of using Kubernetes is the ability to give it a list
    of containers and let it keep those containers running somewhere in the cluster.
    You do this by creating a Pod resource and letting Kubernetes pick a worker node
    for it and run the pod’s containers on that node. But what if one of those containers
    dies? What if all containers of a pod die?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes的主要好处之一是能够给它一个容器列表，并让它保持这些容器在集群中的某个位置运行。你通过创建Pod资源并让Kubernetes为它选择一个工作节点，并在该节点上运行Pod的容器来实现这一点。但如果其中一个容器死亡怎么办？如果一个Pod的所有容器都死亡了怎么办？
- en: As soon as a pod is scheduled to a node, the Kubelet on that node will run its
    containers and, from then on, keep them running as long as the pod exists. If
    the container’s main process crashes, the Kubelet will restart the container.
    If your application has a bug that causes it to crash every once in a while, Kubernetes
    will restart it automatically, so even without doing anything special in the app
    itself, running the app in Kubernetes automatically gives it the ability to heal
    itself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一个Pod被调度到某个节点，该节点上的Kubelet将运行其容器，并且从那时起，只要Pod存在，就会保持它们运行。如果容器的主进程崩溃，Kubelet将重新启动容器。如果你的应用程序有一个导致它偶尔崩溃的bug，Kubernetes会自动重新启动它，因此即使应用程序本身没有做任何特殊处理，在Kubernetes中运行应用程序也会自动赋予它自我修复的能力。
- en: But sometimes apps stop working without their process crashing. For example,
    a Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM
    process will keep running. It would be great to have a way for an app to signal
    to Kubernetes that it’s no longer functioning properly and have Kubernetes restart
    it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时应用程序停止工作，而其进程并未崩溃。例如，一个存在内存泄漏的Java应用程序将开始抛出OutOfMemoryErrors，但JVM进程将继续运行。如果能有一种方法让应用程序向Kubernetes发出信号，表明它不再正常工作，并且让Kubernetes重新启动它，那就太好了。
- en: We’ve said that a container that crashes is restarted automatically, so maybe
    you’re thinking you could catch these types of errors in the app and exit the
    process when they occur. You can certainly do that, but it still doesn’t solve
    all your problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说过，如果一个容器崩溃了，它会自动重启，所以你可能认为你可以在应用程序中捕获这些类型的错误，并在它们发生时退出进程。你当然可以这样做，但这仍然不能解决你所有的问题。
- en: For example, what about those situations when your app stops responding because
    it falls into an infinite loop or a deadlock? To make sure applications are restarted
    in such cases, you must check an application’s health from the outside and not
    depend on the app doing it internally.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你的应用程序停止响应，因为它陷入无限循环或死锁时，这种情况怎么办？为了确保在这种情况下应用程序被重启，你必须从外部检查应用程序的健康状况，而不是依赖应用程序内部进行。
- en: 4.1.1\. Introducing liveness probes
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1.1\. 介绍存活探针
- en: Kubernetes can check if a container is still alive through liveness probes.
    You can specify a liveness probe for each container in the pod’s specification.
    Kubernetes will periodically execute the probe and restart the container if the
    probe fails.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以通过存活探针检查容器是否仍然存活。你可以在Pod规范中为每个容器指定一个存活探针。Kubernetes将定期执行探针，如果探针失败，则重启容器。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Kubernetes also supports readiness probes, which we’ll learn about in the next
    chapter. Be sure not to confuse the two. They’re used for two different things.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还支持就绪性探针，我们将在下一章中学习。务必不要混淆这两个概念。它们用于不同的事情。
- en: '|  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Kubernetes can probe a container using one of the three mechanisms:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以使用三种机制之一来探测容器：
- en: An HTTP GET probe performs an HTTP GET request on the container’s IP address,
    a port and path you specify. If the probe receives a response, and the response
    code doesn’t represent an error (in other words, if the HTTP response code is
    2xx or 3xx), the probe is considered successful. If the server returns an error
    response code or if it doesn’t respond at all, the probe is considered a failure
    and the container will be restarted as a result.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP GET探针对容器IP地址、你指定的端口和路径执行HTTP GET请求。如果探针收到响应，并且响应代码不表示错误（换句话说，如果HTTP响应代码是2xx或3xx），则探针被视为成功。如果服务器返回错误响应代码或根本不响应，则探针被视为失败，容器将被重启。
- en: A TCP Socket probe tries to open a TCP connection to the specified port of the
    container. If the connection is established successfully, the probe is successful.
    Otherwise, the container is restarted.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCP Socket探针尝试与容器的指定端口建立TCP连接。如果连接成功建立，则探针成功。否则，容器将被重启。
- en: An Exec probe executes an arbitrary command inside the container and checks
    the command’s exit status code. If the status code is 0, the probe is successful.
    All other codes are considered failures.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Exec探针在容器内部执行任意命令并检查命令的退出状态码。如果状态码为0，则探针成功。所有其他代码都被视为失败。
- en: 4.1.2\. Creating an HTTP-based liveness probe
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1.2\. 创建基于HTTP的存活探针
- en: Let’s see how to add a liveness probe to your Node.js app. Because it’s a web
    app, it makes sense to add a liveness probe that will check whether its web server
    is serving requests. But because this particular Node.js app is too simple to
    ever fail, you’ll need to make the app fail artificially.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将存活探针添加到你的Node.js应用程序中。因为它是一个Web应用程序，添加一个检查其Web服务器是否正在处理请求的存活探针是有意义的。但是，因为这个特定的Node.js应用程序过于简单，永远不会失败，所以你需要人为地使应用程序失败。
- en: To properly demo liveness probes, you’ll modify the app slightly and make it
    return a 500 Internal Server Error HTTP status code for each request after the
    fifth one—your app will handle the first five client requests properly and then
    return an error on every subsequent request. Thanks to the liveness probe, it
    should be restarted when that happens, allowing it to properly handle client requests
    again.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确演示存活探针，你将稍微修改应用程序，使其在第五个请求之后对每个请求返回500内部服务器错误HTTP状态码——你的应用程序将正确处理前五个客户端请求，然后在随后的每个请求上返回错误。多亏了存活探针，它应该在发生这种情况时重启，以便它可以再次正确处理客户端请求。
- en: You can find the code of the new app in the book’s code archive (in the folder
    Chapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so
    you don’t need to build it yourself.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码存档中找到新应用程序的代码（在Chapter04/kubia-unhealthy文件夹中）。我已经将容器镜像推送到Docker Hub，所以你不需要自己构建它。
- en: You’ll create a new pod that includes an HTTP GET liveness probe. The following
    listing shows the YAML for the pod.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你将创建一个新的Pod，该Pod包含一个基于HTTP GET的存活探针。以下列表显示了Pod的YAML。
- en: 'Listing 4.1\. Adding a liveness probe to a pod: kubia-liveness-probe.yaml'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1\. 向 pod 添加存活探针：kubia-liveness-probe.yaml
- en: '`apiVersion: v1 kind: pod metadata:   name: kubia-liveness spec:   containers:
      - image: luksa/kubia-unhealthy` `1` `name: kubia     livenessProbe:` `2` `httpGet:`
    `2` `path: /` `3` `port: 8080` `4`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: pod metadata:   name: kubia-liveness spec:   containers:
      - image: luksa/kubia-unhealthy` `1` `name: kubia     livenessProbe:` `2` `httpGet:`
    `2` `path: /` `3` `port: 8080` `4`'
- en: 1 This is the image containing the (somewhat) broken app.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这是包含（有些）损坏的应用程序的镜像。
- en: 2 A liveness probe that will perform an HTTP GET
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 将执行 HTTP GET 的存活探针
- en: 3 The path to request in the HTTP request
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 HTTP 请求中请求的路径
- en: 4 The network port the probe should connect to
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 探针应连接到的网络端口
- en: The pod descriptor defines an `httpGet` liveness probe, which tells Kubernetes
    to periodically perform HTTP GET requests on path `/` on port `8080` to determine
    if the container is still healthy. These requests start as soon as the container
    is run.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: pod 描述符定义了一个 `httpGet` 存活探针，它告诉 Kubernetes 定期在端口 `8080` 上的路径 `/` 上执行 HTTP GET
    请求，以确定容器是否仍然健康。这些请求在容器启动时立即开始。
- en: After five such requests (or actual client requests), your app starts returning
    HTTP status code 500, which Kubernetes will treat as a probe failure, and will
    thus restart the container.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行五次此类请求（或实际客户端请求）后，您的应用程序开始返回 HTTP 状态码 500，Kubernetes 将将其视为探针失败，因此会重启容器。
- en: 4.1.3\. Seeing a liveness probe in action
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1.3\. 观察存活探针的实际操作
- en: 'To see what the liveness probe does, try creating the pod now. After about
    a minute and a half, the container will be restarted. You can see that by running
    `kubectl get`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看存活探针的作用，现在尝试创建 pod。大约一分半钟后，容器将被重启。您可以通过运行 `kubectl get` 来查看：
- en: '`$ kubectl get po kubia-liveness` `NAME             READY     STATUS    RESTARTS  
    AGE kubia-liveness   1/1       Running   1          2m`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po kubia-liveness` `NAME             READY     STATUS    RESTARTS  
    AGE kubia-liveness   1/1       运行中   1          2m`'
- en: The `RESTARTS` column shows that the pod’s container has been restarted once
    (if you wait another minute and a half, it gets restarted again, and then the
    cycle continues indefinitely).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`RESTARTS` 列显示 pod 的容器已重启一次（如果您再等一分半钟，它将再次重启，然后无限期地继续循环）。'
- en: '|  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Obtaining the application log of a crashed container
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 获取崩溃容器的应用程序日志
- en: In the previous chapter, you learned how to print the application’s log with
    `kubectl logs`. If your container is restarted, the `kubectl logs` command will
    show the log of the current container.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何使用 `kubectl logs` 打印应用程序的日志。如果您的容器被重启，`kubectl logs` 命令将显示当前容器的日志。
- en: 'When you want to figure out why the previous container terminated, you’ll want
    to see those logs instead of the current container’s logs. This can be done by
    using the `--previous` option:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想找出前一个容器终止的原因时，您会想查看那些日志而不是当前容器的日志。这可以通过使用 `--previous` 选项来完成：
- en: '`$ kubectl logs mypod --previous`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl logs mypod --previous`'
- en: '|  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: You can see why the container had to be restarted by looking at what `kubectl
    describe` prints out, as shown in the following listing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看 `kubectl describe` 输出的内容来了解为什么容器必须重启，如下所示。
- en: Listing 4.2\. A pod’s description after its container is restarted
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2\. 容器重启后的 pod 描述
- en: '`$ kubectl describe po kubia-liveness` `Name:           kubia-liveness ...
    Containers:   kubia:     Container ID:       docker://480986f8     Image:             
    luksa/kubia-unhealthy     Image ID:           docker://sha256:2b208508     Port:
        State:              Running` `1` `Started:          Sun, 14 May 2017 11:41:40
    +0200` `1` `Last State:         Terminated` `2` `Reason:           Error` `2`
    `Exit Code:        137` `2` `Started:          Mon, 01 Jan 0001 00:00:00 +0000`
    `2` `Finished:         Sun, 14 May 2017 11:41:38 +0200` `2` `Ready:             
    True     Restart Count:      1` `3` `Liveness:           http-get http://:8080/
    delay=0s timeout=1s                         period=10s #success=1 #failure=3    
    ... Events: ... Killing container with id docker://95246981:pod "kubia-liveness
    ..."     container "kubia" is unhealthy, it will be killed and re-created.`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po kubia-liveness` `Name:           kubia-liveness ...
    Containers:    kubia:    Container ID:       docker://480986f8    Image:             
    luksa/kubia-unhealthy    Image ID:           docker://sha256:2b208508    Port:           
    State:              Running    1    Started:          Sun, 14 May 2017 11:41:40
    +0200    1    Last State:         Terminated    2    Reason:           Error   
    2    Exit Code:        137    2    Started:          Mon, 01 Jan 0001 00:00:00
    +0000    2    Finished:         Sun, 14 May 2017 11:41:38 +0200    2    Ready:             
    True    3    Restart Count:      1    3    Liveness:           http-get http://:8080/
    delay=0s timeout=1s    period=10s #success=1 #failure=3    ... Events:    ...
    Killing container with id docker://95246981:pod "kubia-liveness ..."    container
    "kubia" is unhealthy, it will be killed and re-created.`'
- en: 1 The container is currently running.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 当前容器正在运行。
- en: 2 The previous container terminated with an error and exited with code 137.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 前一个容器因错误终止并退出，退出代码为 137。
- en: 3 The container has been restarted once.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 容器已重启一次。
- en: 'You can see that the container is currently running, but it previously terminated
    because of an error. The exit code was `137`, which has a special meaning—it denotes
    that the process was terminated by an external signal. The number `137` is a sum
    of two numbers: `128+x`, where `x` is the signal number sent to the process that
    caused it to terminate. In the example, `x` equals `9`, which is the number of
    the `SIGKILL` signal, meaning the process was killed forcibly.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到容器当前正在运行，但它之前因为错误而终止。退出代码是 `137`，它有特殊含义——表示进程被外部信号终止。数字 `137` 是两个数字的和：`128+x`，其中
    `x` 是发送给导致进程终止的进程的信号号。在示例中，`x` 等于 `9`，这是 `SIGKILL` 信号的数量，意味着进程被强制终止。
- en: The events listed at the bottom show why the container was killed—Kubernetes
    detected the container was unhealthy, so it killed and re-created it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列在底部的事件显示了容器被终止的原因——Kubernetes 检测到容器不健康，因此将其终止并重新创建。
- en: '|  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When a container is killed, a completely new container is created—it’s not the
    same container being restarted again.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器被终止时，会创建一个全新的容器——它不是再次重启的同一个容器。
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 4.1.4\. Configuring additional properties of the liveness probe
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1.4\. 配置存活探测的附加属性
- en: 'You may have noticed that `kubectl describe` also displays additional information
    about the liveness probe:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到 `kubectl describe` 还显示了关于存活探测的附加信息：
- en: '`Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1`
    ![](images/00006.jpg) `#failure=3`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1`
    ![图片](images/00006.jpg) `#failure=3`'
- en: Beside the liveness probe options you specified explicitly, you can also see
    additional properties, such as `delay`, `timeout`, `period`, and so on. The `delay=0s`
    part shows that the probing begins immediately after the container is started.
    The `timeout` is set to only 1 second, so the container must return a response
    in 1 second or the probe is counted as failed. The container is probed every 10
    seconds (`period=10s`) and the container is restarted after the probe fails three
    consecutive times (`#failure=3`).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你明确指定的存活探测选项之外，你还可以看到其他附加属性，例如 `delay`、`timeout`、`period` 等。`delay=0s` 部分表示探测在容器启动后立即开始。`timeout`
    设置为仅 1 秒，因此容器必须在 1 秒内返回响应，否则探测被视为失败。容器每 10 秒探测一次（`period=10s`），容器在探测连续三次失败后重启（`#failure=3`）。
- en: These additional parameters can be customized when defining the probe. For example,
    to set the initial delay, add the `initialDelaySeconds` property to the liveness
    probe as shown in the following listing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些附加参数可以在定义探测时进行自定义。例如，要设置初始延迟，将 `initialDelaySeconds` 属性添加到存活探测中，如下面的列表所示。
- en: 'Listing 4.3\. A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3\. 带有初始延迟的存活探测：kubia-liveness-probe-initial-delay.yaml
- en: '`livenessProbe:      httpGet:        path: /        port: 8080      initialDelaySeconds:
    15` `1`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`livenessProbe:    httpGet:        path: /        port: 8080    initialDelaySeconds:
    15` `1`'
- en: 1 Kubernetes will wait 15 seconds before executing the first probe.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Kubernetes 将在执行第一个探测前等待 15 秒。
- en: If you don’t set the initial delay, the prober will start probing the container
    as soon as it starts, which usually leads to the probe failing, because the app
    isn’t ready to start receiving requests. If the number of failures exceeds the
    failure threshold, the container is restarted before it’s even able to start responding
    to requests properly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有设置初始延迟，探测器将在容器启动时立即开始探测，这通常会导致探测失败，因为应用程序还没有准备好开始接收请求。如果失败次数超过失败阈值，容器在能够正确响应请求之前就会被重启。
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Always remember to set an initial delay to account for your app’s startup time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要记得设置一个初始延迟来考虑你的应用程序的启动时间。
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: I’ve seen this on many occasions and users were confused why their container
    was being restarted. But if they’d used `kubectl describe`, they’d have seen that
    the container terminated with exit code 137 or 143, telling them that the pod
    was terminated externally. Additionally, the listing of the pod’s events would
    show that the container was killed because of a failed liveness probe. If you
    see this happening at pod startup, it’s because you failed to set `initialDelaySeconds`
    appropriately.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我在很多情况下都见过这种情况，用户们困惑为什么他们的容器会被重启。但如果他们使用了 `kubectl describe`，他们就会看到容器以退出代码 137
    或 143 终止，这告诉他们 pod 是被外部终止的。此外，pod 事件的列表将显示容器被杀死是因为存活探测失败。如果你在 pod 启动时看到这种情况发生，那是因为你没有适当地设置
    `initialDelaySeconds`。
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Exit code 137 signals that the process was killed by an external signal (exit
    code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 + 15 (SIGTERM).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 退出代码 137 表示进程被外部信号终止（退出代码是 128 + 9（SIGKILL））。同样，退出代码 143 对应于 128 + 15（SIGTERM）。
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.1.5\. Creating effective liveness probes
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1.5\. 创建有效的存活探测
- en: For pods running in production, you should always define a liveness probe. Without
    one, Kubernetes has no way of knowing whether your app is still alive or not.
    As long as the process is still running, Kubernetes will consider the container
    to be healthy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在生产中运行的 pod，你应该始终定义一个存活探测。如果没有，Kubernetes 没有办法知道你的应用程序是否仍然存活。只要进程仍在运行，Kubernetes
    就会认为容器是健康的。
- en: What a liveness probe should check
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 存活探测应该检查的内容
- en: Your simplistic liveness probe simply checks if the server is responding. While
    this may seem overly simple, even a liveness probe like this does wonders, because
    it causes the container to be restarted if the web server running within the container
    stops responding to HTTP requests. Compared to having no liveness probe, this
    is a major improvement, and may be sufficient in most cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你的简单存活探测只是检查服务器是否响应。虽然这可能看起来过于简单，但即使是这样的存活探测也能产生奇迹，因为它会导致容器在运行在容器内的 Web 服务器停止响应
    HTTP 请求时重启。与没有存活探测相比，这是一个重大的改进，在大多数情况下可能已经足够。
- en: But for a better liveness check, you’d configure the probe to perform requests
    on a specific URL path (`/health`, for example) and have the app perform an internal
    status check of all the vital components running inside the app to ensure none
    of them has died or is unresponsive.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了更好的存活检查，你应配置探测在特定的 URL 路径上执行请求（例如 `/health`），并让应用程序对应用程序内部运行的所有关键组件进行内部状态检查，以确保它们都没有死亡或无响应。
- en: '|  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Make sure the `/health` HTTP endpoint doesn’t require authentication; otherwise
    the probe will always fail, causing your container to be restarted indefinitely.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 `/health` HTTP 端点不需要身份验证；否则，探测将始终失败，导致你的容器无限重启。
- en: '|  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Be sure to check only the internals of the app and nothing influenced by an
    external factor. For example, a frontend web server’s liveness probe shouldn’t
    return a failure when the server can’t connect to the backend database. If the
    underlying cause is in the database itself, restarting the web server container
    will not fix the problem. Because the liveness probe will fail again, you’ll end
    up with the container restarting repeatedly until the database becomes accessible
    again.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要检查应用程序的内部，而不是受外部因素影响的任何内容。例如，如果前端 Web 服务器无法连接到后端数据库，其存活探测不应该返回失败。如果根本原因在数据库本身，重启
    Web 服务器容器并不能解决问题。因为存活探测将再次失败，你最终会看到容器反复重启，直到数据库再次可访问。
- en: Keeping probes light
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 保持探测轻量
- en: Liveness probes shouldn’t use too many computational resources and shouldn’t
    take too long to complete. By default, the probes are executed relatively often
    and are only allowed one second to complete. Having a probe that does heavy lifting
    can slow down your container considerably. Later in the book, you’ll also learn
    about how to limit CPU time available to a container. The probe’s CPU time is
    counted in the container’s CPU time quota, so having a heavyweight liveness probe
    will reduce the CPU time available to the main application processes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 生存探针不应使用过多的计算资源，也不应花费太长时间完成。默认情况下，探针执行相对频繁，并且只允许一秒钟完成。拥有一个执行大量工作的探针可能会显著减慢容器的速度。在本书的后面部分，您还将了解到如何限制容器可用的CPU时间。探针的CPU时间计入容器的CPU时间配额，因此拥有一个重量级的生存探针会减少主应用程序进程可用的CPU时间。
- en: '|  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you’re running a Java app in your container, be sure to use an HTTP GET liveness
    probe instead of an Exec probe, where you spin up a whole new JVM to get the liveness
    information. The same goes for any JVM-based or similar applications, whose start-up
    procedure requires considerable computational resources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在容器中运行Java应用程序，请务必使用HTTP GET生存探针而不是Exec探针，后者会启动一个新的JVM来获取生存信息。对于任何基于JVM或类似的应用程序，其启动过程需要大量的计算资源也是如此。
- en: '|  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Don’t bother implementing retry loops in your probes
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在您的探针中实现重试循环
- en: You’ve already seen that the failure threshold for the probe is configurable
    and usually the probe must fail multiple times before the container is killed.
    But even if you set the failure threshold to 1, Kubernetes will retry the probe
    several times before considering it a single failed attempt. Therefore, implementing
    your own retry loop into the probe is wasted effort.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到探针的失败阈值是可配置的，并且通常探针必须失败多次，容器才会被杀死。但即使您将失败阈值设置为1，Kubernetes也会在将其视为单个失败尝试之前重试探针几次。因此，在探针中实现自己的重试循环是徒劳的。
- en: Liveness probe wrap-up
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 生存探针总结
- en: You now understand that Kubernetes keeps your containers running by restarting
    them if they crash or if their liveness probes fail. This job is performed by
    the Kubelet on the node hosting the pod—the Kubernetes Control Plane components
    running on the master(s) have no part in this process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在明白，Kubernetes通过在容器崩溃或生存探针失败时重启它们来保持容器运行。这项工作由运行在托管Pod的节点上的Kubelet执行——运行在主节点上的Kubernetes控制平面组件在这个过程中没有发挥作用。
- en: But if the node itself crashes, it’s the Control Plane that must create replacements
    for all the pods that went down with the node. It doesn’t do that for pods that
    you create directly. Those pods aren’t managed by anything except by the Kubelet,
    but because the Kubelet runs on the node itself, it can’t do anything if the node
    fails.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果节点本身崩溃，那么控制平面必须为与节点一起宕机的所有Pod创建替代品。它不会为直接创建的Pod做这件事。这些Pod除了由Kubelet管理之外，没有其他管理方式，但由于Kubelet运行在节点本身上，如果节点失败，它什么也无法做。
- en: To make sure your app is restarted on another node, you need to have the pod
    managed by a ReplicationController or similar mechanism, which we’ll discuss in
    the rest of this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您的应用程序在另一个节点上重启，您需要让Pod由ReplicationController或类似机制管理，我们将在本章的其余部分讨论这一点。
- en: 4.2\. Introducing ReplicationControllers
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2. 介绍ReplicationController
- en: A ReplicationController is a Kubernetes resource that ensures its pods are always
    kept running. If the pod disappears for any reason, such as in the event of a
    node disappearing from the cluster or because the pod was evicted from the node,
    the ReplicationController notices the missing pod and creates a replacement pod.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController是Kubernetes资源，确保其Pod始终运行。如果Pod因任何原因消失，例如节点从集群中消失或Pod被从节点驱逐，ReplicationController会注意到缺失的Pod并创建一个替代Pod。
- en: '[Figure 4.1](#filepos382961) shows what happens when a node goes down and takes
    two pods with it. Pod A was created directly and is therefore an unmanaged pod,
    while pod B is managed by a ReplicationController. After the node fails, the ReplicationController
    creates a new pod (pod B2) to replace the missing pod B, whereas pod A is lost
    completely—nothing will ever recreate it.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.1](#filepos382961) 展示了当一个节点宕机并带走两个Pod时会发生什么。Pod A是直接创建的，因此是一个未管理的Pod，而Pod
    B由ReplicationController管理。节点失败后，ReplicationController创建一个新的Pod（Pod B2）来替换缺失的Pod
    B，而Pod A则完全丢失——它永远不会被重新创建。'
- en: Figure 4.1\. When a node fails, only pods backed by a ReplicationController
    are recreated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1。当一个节点失败时，只有由ReplicationController支持的Pod会被重新创建。
- en: '![](images/00173.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00173.jpg)'
- en: The ReplicationController in the figure manages only a single pod, but Replication-Controllers,
    in general, are meant to create and manage multiple copies (replicas) of a pod.
    That’s where ReplicationControllers got their name from.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的ReplicationController只管理一个Pod，但通常情况下，ReplicationController的目的是创建和管理Pod的多个副本（replicas）。这就是ReplicationController名字的由来。
- en: 4.2.1\. The operation of a ReplicationController
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.1\. ReplicationController的操作
- en: A ReplicationController constantly monitors the list of running pods and makes
    sure the actual number of pods of a “type” always matches the desired number.
    If too few such pods are running, it creates new replicas from a pod template.
    If too many such pods are running, it removes the excess replicas.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController持续监控正在运行的Pod列表，并确保“类型”的Pod实际数量始终与目标数量匹配。如果运行的此类Pod太少，它会从Pod模板创建新的副本。如果运行的此类Pod太多，它会删除多余的副本。
- en: 'You might be wondering how there can be more than the desired number of replicas.
    This can happen for a few reasons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么副本数量会超过目标数量。这可能是由于以下几个原因：
- en: Someone creates a pod of the same type manually.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工创建了一个相同类型的Pod。
- en: Someone changes an existing pod’s “type.”
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工更改现有Pod的“类型”。
- en: Someone decreases the desired number of pods, and so on.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工减少所需的Pod数量，等等。
- en: I’ve used the term pod “type” a few times. But no such thing exists. Replication-Controllers
    don’t operate on pod types, but on sets of pods that match a certain label selector
    (you learned about them in the previous chapter).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我多次使用了“Pod类型”这个术语。但事实上并不存在这样的类型。ReplicationController不是在Pod类型上操作，而是在匹配特定标签选择器的Pod集合上操作（您在上一章中已经了解过它们）。
- en: Introducing the controller’s reconciliation loop
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍控制器的协调循环
- en: A ReplicationController’s job is to make sure that an exact number of pods always
    matches its label selector. If it doesn’t, the ReplicationController takes the
    appropriate action to reconcile the actual with the desired number. The operation
    of a Replication-Controller is shown in [figure 4.2](#filepos385134).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController的职责是确保Pod的确切数量始终与其标签选择器匹配。如果不匹配，ReplicationController将采取适当的行动来协调实际数量与目标数量。ReplicationController的操作在[图4.2](#filepos385134)中显示。
- en: Figure 4.2\. A ReplicationController’s reconciliation loop
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2\. ReplicationController的协调循环
- en: '![](images/00191.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00191.jpg)'
- en: Understanding the three parts of a ReplicationController
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 理解ReplicationController的三个部分
- en: 'A ReplicationController has three essential parts (also shown in [figure 4.3](#filepos386048)):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController有三个基本部分（也显示在[图4.3](#filepos386048)中）：
- en: A label selector, which determines what pods are in the ReplicationController’s
    scope
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标签选择器，它决定了哪些Pod位于ReplicationController的作用域内
- en: A replica count, which specifies the desired number of pods that should be running
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个副本数量，它指定了应该运行的目标Pod数量
- en: A pod template, which is used when creating new pod replicas
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Pod模板，用于创建新的Pod副本
- en: Figure 4.3\. The three key parts of a ReplicationController (pod selector, replica
    count, and pod template)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3\. ReplicationController的三个关键部分（pod选择器、副本数量和Pod模板）
- en: '![](images/00013.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00013.jpg)'
- en: A ReplicationController’s replica count, the label selector, and even the pod
    template can all be modified at any time, but only changes to the replica count
    affect existing pods.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController的副本数量、标签选择器，甚至Pod模板都可以随时修改，但只有副本数量的更改会影响现有的Pod。
- en: Understanding the effect of changing the controller’s label selec- ctor or pod
    template
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理解更改控制器标签选择器或Pod模板的影响
- en: Changes to the label selector and the pod template have no effect on existing
    pods. Changing the label selector makes the existing pods fall out of the scope
    of the Replication-Controller, so the controller stops caring about them. ReplicationControllers
    also don’t care about the actual “contents” of its pods (the container images,
    environment variables, and other things) after they create the pod. The template
    therefore only affects new pods created by this ReplicationController. You can
    think of it as a cookie cutter for cutting out new pods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 标签选择器和Pod模板的更改对现有Pod没有影响。更改标签选择器会使现有Pod脱离ReplicationController的作用域，因此控制器不再关心它们。ReplicationController在创建Pod后，也不再关心其Pod的实际“内容”（容器镜像、环境变量和其他事物）。因此，模板只影响由该ReplicationController创建的新Pod。您可以将它视为切割新Pod的模具。
- en: Understanding the benefits of using a ReplicationController
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 理解使用ReplicationController的好处
- en: 'Like many things in Kubernetes, a ReplicationController, although an incredibly
    simple concept, provides or enables the following powerful features:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Kubernetes 中的许多事物一样，尽管 ReplicationController 是一个极其简单的概念，但它提供了或启用了以下强大的功能：
- en: It makes sure a pod (or multiple pod replicas) is always running by starting
    a new pod when an existing one goes missing.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过在现有的 Pod 失踪时启动一个新的 Pod，确保一个 Pod（或多个 Pod 副本）始终在运行。
- en: When a cluster node fails, it creates replacement replicas for all the pods
    that were running on the failed node (those that were under the Replication-Controller’s
    control).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当集群节点失败时，它会为在失败的节点上运行的所有 Pod 创建替换副本（那些在 Replication-Controller 控制下的 Pod）。
- en: It enables easy horizontal scaling of pods—both manual and automatic (see horizontal
    pod auto-scaling in [chapter 15](index_split_113.html#filepos1423922)).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使得 Pod 的水平扩展变得简单——无论是手动还是自动（参见第 15 章中的水平 Pod 自动扩展 [index_split_113.html#filepos1423922])。
- en: '|  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A pod instance is never relocated to another node. Instead, the Replication-Controller
    creates a completely new pod instance that has no relation to the instance it’s
    replacing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 实例永远不会被转移到另一个节点。相反，Replication-Controller 会创建一个全新的 Pod 实例，它与它所替换的实例没有任何关系。
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 4.2.2\. Creating a ReplicationController
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.2\. 创建一个 ReplicationController
- en: Let’s look at how to create a ReplicationController and then see how it keeps
    your pods running. Like pods and other Kubernetes resources, you create a ReplicationController
    by posting a JSON or YAML descriptor to the Kubernetes API server.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建一个 ReplicationController，然后看看它是如何保持 Pod 运行的。像 Pod 和其他 Kubernetes 资源一样，您通过向
    Kubernetes API 服务器发送 JSON 或 YAML 描述符来创建 ReplicationController。
- en: You’re going to create a YAML file called kubia-rc.yaml for your Replication-Controller,
    as shown in the following listing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您将创建一个名为 kubia-rc.yaml 的 YAML 文件，用于您的 ReplicationController，如下所示。
- en: 'Listing 4.4\. A YAML definition of a ReplicationController: kubia-rc.yaml'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4\. 一个 ReplicationController 的 YAML 定义：kubia-rc.yaml
- en: '`apiVersion: v1 kind: ReplicationController` `1` `metadata:   name: kubia`
    `2` `spec:   replicas: 3` `3` `selector:` `4` `app: kubia` `4` `template:` `5`
    `metadata:` `5` `labels:` `5` `app: kubia` `5` `spec:` `5` `containers:` `5` `-
    name: kubia` `5` `image: luksa/kubia` `5` `ports:` `5` `- containerPort: 8080`
    `5`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ReplicationController` `1` `metadata:   name: kubia`
    `2` `spec:   replicas: 3` `3` `selector:` `4` `app: kubia` `4` `template:` `5`
    `metadata:` `5` `labels:` `5` `app: kubia` `5` `spec:` `5` `containers:` `5` `-
    name: kubia` `5` `image: luksa/kubia` `5` `ports:` `5` `- containerPort: 8080`
    `5`'
- en: 1 This manifest defines a ReplicationController (RC)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此清单定义了一个 ReplicationController（RC）
- en: 2 The name of this ReplicationController
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这个 ReplicationController 的名称
- en: 3 The desired number of pod instances
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 所需的 Pod 实例数量
- en: 4 The pod selector determining what pods the RC is operating on
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 确定 RC 正在操作的 Pod 的 Pod 选择器
- en: 5 The pod template for creating new pods
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 创建新 Pod 的 Pod 模板
- en: When you post the file to the API server, Kubernetes creates a new Replication-Controller
    named `kubia`, which makes sure three pod instances always match the label selector
    `app=kubia`. When there aren’t enough pods, new pods will be created from the
    provided pod template. The contents of the template are almost identical to the
    pod definition you created in the previous chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将文件发送到 API 服务器时，Kubernetes 创建一个名为 `kubia` 的新 Replication-Controller，确保三个 Pod
    实例始终匹配标签选择器 `app=kubia`。当 Pod 数量不足时，将根据提供的 Pod 模板创建新的 Pod。模板的内容几乎与您在上一章中创建的 Pod
    定义相同。
- en: The pod labels in the template must obviously match the label selector of the
    ReplicationController; otherwise the controller would create new pods indefinitely,
    because spinning up a new pod wouldn’t bring the actual replica count any closer
    to the desired number of replicas. To prevent such scenarios, the API server verifies
    the ReplicationController definition and will not accept it if it’s misconfigured.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模板中的 Pod 标签必须显然与 ReplicationController 的标签选择器匹配；否则，控制器将无限期地创建新的 Pod，因为启动一个新的
    Pod 并不会使实际副本数更接近所需的副本数。为了防止此类情况，API 服务器会验证 ReplicationController 的定义，如果配置错误则不会接受它。
- en: Not specifying the selector at all is also an option. In that case, it will
    be configured automatically from the labels in the pod template.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 完全不指定选择器也是一种选择。在这种情况下，它将自动从 Pod 模板中的标签配置。
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Don’t specify a pod selector when defining a ReplicationController. Let Kubernetes
    extract it from the pod template. This will keep your YAML shorter and simpler.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义 ReplicationController 时不要指定 Pod 选择器。让 Kubernetes 从 Pod 模板中提取它。这将使您的 YAML
    更短、更简单。
- en: '|  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'To create the ReplicationController, use the `kubectl create` command, which
    you already know:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建ReplicationController，使用你已知的`kubectl create`命令：
- en: '`$ kubectl create -f kubia-rc.yaml` `replicationcontroller "kubia" created`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f kubia-rc.yaml` `replicationcontroller "kubia" created`'
- en: As soon as the ReplicationController is created, it goes to work. Let’s see
    what it does.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建ReplicationController，它就会开始工作。让我们看看它做了什么。
- en: 4.2.3\. Seeing the ReplicationController in action
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.3\. 查看ReplicationController的实际操作
- en: 'Because no pods exist with the `app=kubia` label, the ReplicationController
    should spin up three new pods from the pod template. List the pods to see if the
    ReplicationController has done what it’s supposed to:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因为没有Pod具有`app=kubia`标签，ReplicationController应该从Pod模板启动三个新的Pod。列出Pods以查看ReplicationController是否完成了它应该做的事情：
- en: '`$ kubectl get pods` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-53thy   0/1       ContainerCreating   0          2s kubia-k0xz6   0/1      
    ContainerCreating   0          2s kubia-q3vkg   0/1       ContainerCreating  
    0          2s`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-53thy   0/1       ContainerCreating   0          2s kubia-k0xz6   0/1      
    ContainerCreating   0          2s kubia-q3vkg   0/1       ContainerCreating  
    0          2s`'
- en: Indeed, it has! You wanted three pods, and it created three pods. It’s now managing
    those three pods. Next you’ll mess with them a little to see how the Replication-Controller
    responds.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 的确如此！你想要三个Pod，它创建了三个Pod。现在它正在管理这三个Pod。接下来，你会稍微干扰它们，以查看Replication-Controller如何响应。
- en: Seeing the ReplicationController respond to a deleted pod
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 查看ReplicationController对已删除Pod的响应
- en: 'First, you’ll delete one of the pods manually to see how the ReplicationController
    spins up a new one immediately, bringing the number of matching pods back to three:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将手动删除一个Pod，以查看ReplicationController如何立即启动一个新的Pod，将匹配的Pod数量恢复到三个：
- en: '`$ kubectl delete pod kubia-53thy` `pod "kubia-53thy" deleted`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete pod kubia-53thy` `pod "kubia-53thy" deleted`'
- en: 'Listing the pods again shows four of them, because the one you deleted is terminating,
    and a new pod has already been created:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 再次列出Pods时，显示四个Pod，因为你要删除的那个Pod正在终止，并且已经创建了一个新的Pod：
- en: '`$ kubectl get pods` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-53thy   1/1       Terminating         0          3m kubia-oini2   0/1      
    ContainerCreating   0          2s kubia-k0xz6   1/1       Running            
    0          3m kubia-q3vkg   1/1       Running             0          3m`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-53thy   1/1       Terminating         0          3m kubia-oini2   0/1      
    ContainerCreating   0          2s kubia-k0xz6   1/1       Running            
    0          3m kubia-q3vkg   1/1       Running             0          3m`'
- en: The ReplicationController has done its job again. It’s a nice little helper,
    isn’t it?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController再次完成了它的任务。它是一个很好的小助手，不是吗？
- en: Getting information about a ReplicationController
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 获取ReplicationController的信息
- en: 'Now, let’s see what information the `kubectl get` command shows for ReplicationControllers:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`kubectl get`命令对于ReplicationControllers显示的信息：
- en: '`$ kubectl get rc` `NAME      DESIRED   CURRENT   READY     AGE kubia     3        
    3         2         3m`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get rc` `NAME      DESIRED   CURRENT   READY     AGE kubia     3        
    3         2         3m`'
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We’re using `rc` as a shorthand for `replicationcontroller`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`rc`作为`replicationcontroller`的简称。
- en: '|  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: You see three columns showing the desired number of pods, the actual number
    of pods, and how many of them are ready (you’ll learn what that means in the next
    chapter, when we talk about readiness probes).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到有三列显示所需的Pod数量、实际Pod数量以及有多少个是就绪的（你将在下一章讨论就绪性探针时了解这意味着什么）。
- en: You can see additional information about your ReplicationController with the
    `kubectl describe` command, as shown in the following listing.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl describe`命令查看关于你的ReplicationController的更多信息，如下所示。
- en: Listing 4.5\. Displaying details of a ReplicationController with `kubectl describe`
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5\. 使用`kubectl describe`显示ReplicationController的详细信息
- en: '`$ kubectl describe rc kubia` `Name:           kubia Namespace:      default
    Selector:       app=kubia Labels:         app=kubia Annotations:    <none> Replicas:      
    3 current / 3 desired` `1` `Pods Status:    4 Running / 0 Waiting / 0 Succeeded
    / 0 Failed` `2` `Pod Template:   Labels:       app=kubia   Containers:   ...  
    Volumes:      <none> Events:` `3` `From                    Type      Reason          
    Message ----                    -------  ------            ------- replication-controller 
    Normal   SuccessfulCreate  Created pod: kubia-53thy replication-controller  Normal  
    SuccessfulCreate  Created pod: kubia-k0xz6 replication-controller  Normal   SuccessfulCreate 
    Created pod: kubia-q3vkg replication-controller  Normal   SuccessfulCreate  Created
    pod: kubia-oini2`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe rc kubia` `名称：           kubia 命名空间：      default 选择器：      
    app=kubia 标签：         app=kubia 注解：    无` `副本：       3 当前 / 3 期望` `1` `Pods状态：   
    4 运行 / 0 等待 / 0 成功 / 0 失败` `2` `Pod模板：    标签：       app=kubia 容器：   ... 卷：     
    无 事件：` `3` `来源                    类型      原因           消息 ----                   
    -------  ------            ------- replication-controller  正常   成功创建 Created pod:
    kubia-53thy replication-controller  正常   成功创建 Created pod: kubia-k0xz6 replication-controller 
    正常   成功创建 Created pod: kubia-q3vkg replication-controller  正常   成功创建 Created pod:
    kubia-oini2`'
- en: 1 The actual vs. the desired number of pod instances
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 实际与期望的Pod实例数量
- en: 2 Number of pod instances per pod status
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 每个Pod状态的Pod实例数量
- en: 3 The events related to this ReplicationController
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 与此ReplicationController相关的事件
- en: The current number of replicas matches the desired number, because the controller
    has already created a new pod. It shows four running pods because a pod that’s
    terminating is still considered running, although it isn’t counted in the current
    replica count.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当前副本数量与期望数量匹配，因为控制器已经创建了一个新的Pod。它显示有四个正在运行的Pod，因为一个正在终止的Pod仍然被视为正在运行，尽管它不计入当前的副本计数。
- en: The list of events at the bottom shows the actions taken by the Replication-Controller—it
    has created four pods so far.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的活动列表显示了Replication-Controller采取的行动——它已经创建了四个Pod。
- en: Understanding exactly what caused the controller to create a new pod
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 确切理解导致控制器创建新Pod的原因
- en: The controller is responding to the deletion of a pod by creating a new replacement
    pod (see [figure 4.4](#filepos399666)). Well, technically, it isn’t responding
    to the deletion itself, but the resulting state—the inadequate number of pods.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器通过创建一个新的替换Pod来响应Pod的删除（见[图4.4](#filepos399666)）。好吧，技术上讲，它并不是在响应删除本身，而是响应的结果——Pod数量不足。
- en: Figure 4.4\. If a pod disappears, the ReplicationController sees too few pods
    and creates a new replacement pod.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4。如果一个Pod消失了，ReplicationController会看到Pod数量过少，并创建一个新的替换Pod。
- en: '![](images/00033.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00033.jpg)'
- en: While a ReplicationController is immediately notified about a pod being deleted
    (the API server allows clients to watch for changes to resources and resource
    lists), that’s not what causes it to create a replacement pod. The notification
    triggers the controller to check the actual number of pods and take appropriate
    action.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ReplicationController会立即通知关于Pod被删除的情况（API服务器允许客户端监视资源及其列表的变化），但这并不是导致它创建替换Pod的原因。通知触发控制器检查实际的Pod数量并采取适当的行动。
- en: Responding to a node failure
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 响应节点故障
- en: Seeing the ReplicationController respond to the manual deletion of a pod isn’t
    too interesting, so let’s look at a better example. If you’re using Google Kubernetes
    Engine to run these examples, you have a three-node Kubernetes cluster. You’re
    going to disconnect one of the nodes from the network to simulate a node failure.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 观察ReplicationController对Pod手动删除的响应并不太有趣，所以让我们看看更好的例子。如果你使用Google Kubernetes
    Engine来运行这些示例，你有一个三节点Kubernetes集群。你将断开其中一个节点与网络的连接来模拟节点故障。
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using Minikube, you can’t do this exercise, because you only have
    one node that acts both as a master and a worker node.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Minikube，你不能做这个练习，因为你只有一个节点同时作为主节点和工作节点。
- en: '|  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If a node fails in the non-Kubernetes world, the ops team would need to migrate
    the applications running on that node to other machines manually. Kubernetes,
    on the other hand, does that automatically. Soon after the ReplicationController
    detects that its pods are down, it will spin up new pods to replace them.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在非Kubernetes世界中，如果一个节点失败，运维团队需要手动将运行在该节点上的应用程序迁移到其他机器上。另一方面，Kubernetes会自动完成这项工作。ReplicationController检测到其Pod挂起后不久，就会启动新的Pod来替换它们。
- en: Let’s see this in action. You need to `ssh` into one of the nodes with the `gcloud
    compute ssh` command and then shut down its network interface with `sudo ifconfig
    eth0 down`, as shown in the following listing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际操作。你需要使用`gcloud compute ssh`命令登录到节点之一，然后使用`sudo ifconfig eth0 down`关闭其网络接口，如下所示。
- en: '|  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Choose a node that runs at least one of your pods by listing pods with the `-o
    wide` option.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`-o wide`选项列出Pod来选择至少运行了一个Pod的节点。
- en: '|  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 4.6\. Simulating a node failure by shutting down its network interface
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6\. 通过关闭网络接口来模拟节点故障
- en: '`$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko` `Enter passphrase
    for key ''/home/luksa/.ssh/google_compute_engine'':  Welcome to Kubernetes v1.6.4!
    ...` `luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko` `输入密钥对`/home/luksa/.ssh/google_compute_engine`的密码：`欢迎使用Kubernetes
    v1.6.4! ...` `luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0
    down`'
- en: 'When you shut down the network interface, the `ssh` session will stop responding,
    so you need to open up another terminal or hard-exit from the `ssh` session. In
    the new terminal you can list the nodes to see if Kubernetes has detected that
    the node is down. This takes a minute or so. Then, the node’s status is shown
    as `NotReady`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当你关闭网络接口时，`ssh`会话将停止响应，因此你需要打开另一个终端或从`ssh`会话中硬退出。在新的终端中，你可以列出节点以查看Kubernetes是否检测到节点已关闭。这需要大约一分钟的时间。然后，节点的状态将显示为`NotReady`：
- en: '`$ kubectl get node` `NAME                                   STATUS     AGE
    gke-kubia-default-pool-b46381f1-opc5   Ready      5h gke-kubia-default-pool-b46381f1-s8gj  
    Ready      5h gke-kubia-default-pool-b46381f1-zwko   NotReady   5h` `1`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get node` `NAME                                   STATUS     AGE
    gke-kubia-default-pool-b46381f1-opc5   Ready      5h gke-kubia-default-pool-b46381f1-s8gj  
    Ready      5h gke-kubia-default-pool-b46381f1-zwko   NotReady   5h` `1`'
- en: 1 Node isn’t ready, because it’s disconnected from the network
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 节点未就绪，因为它已断开网络连接
- en: 'If you list the pods now, you’ll still see the same three pods as before, because
    Kubernetes waits a while before rescheduling pods (in case the node is unreachable
    because of a temporary network glitch or because the Kubelet is restarting). If
    the node stays unreachable for several minutes, the status of the pods that were
    scheduled to that node changes to `Unknown`. At that point, the ReplicationController
    will immediately spin up a new pod. You can see this by listing the pods again:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在列出Pod，你仍然会看到之前相同的三个Pod，因为Kubernetes在重新调度Pod之前会等待一段时间（以防节点因临时网络故障或Kubelet重启而不可达）。如果节点连续几分钟都不可达，分配给该节点的Pod的状态将变为`Unknown`。到那时，ReplicationController将立即启动一个新的Pod。你可以通过再次列出Pod来看到这一点：
- en: '`$ kubectl get pods` `NAME          READY   STATUS    RESTARTS   AGE kubia-oini2  
    1/1     Running   0          10m kubia-k0xz6   1/1     Running   0          10m
    kubia-q3vkg   1/1     Unknown   0          10m` `1` `kubia-dmdck   1/1     Running  
    0          5s` `2`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods` `NAME          READY   STATUS    RESTARTS   AGE kubia-oini2  
    1/1     Running   0          10m kubia-k0xz6   1/1     Running   0          10m
    kubia-q3vkg   1/1     Unknown   0          10m` `1` `kubia-dmdck   1/1     Running  
    0          5s` `2`'
- en: 1 This pod’s status is unknown, because its node is unreachable.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这个Pod的状态是未知，因为它的节点不可达。
- en: 2 This pod was created five seconds ago.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这个Pod是五秒前创建的。
- en: Looking at the age of the pods, you see that the `kubia-dmdck` pod is new. You
    again have three pod instances running, which means the ReplicationController
    has again done its job of bringing the actual state of the system to the desired
    state.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察Pod的年龄，你可以看到`kubia-dmdck` Pod是新的。你现在又有三个Pod实例正在运行，这意味着ReplicationController再次完成了其工作，将系统的实际状态带到期望状态。
- en: The same thing happens if a node fails (either breaks down or becomes unreachable).
    No immediate human intervention is necessary. The system heals itself automatically.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点失败（无论是崩溃还是变得不可达），也不需要立即的人工干预。系统会自动修复。
- en: 'To bring the node back, you need to reset it with the following command:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要将节点恢复，你需要使用以下命令：
- en: '`$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko`'
- en: When the node boots up again, its status should return to `Ready`, and the pod
    whose status was `Unknown` will be deleted.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点再次启动时，其状态应该返回到`Ready`，状态为`Unknown`的Pod将被删除。
- en: 4.2.4\. Moving pods in and out of the scope of a ReplicationController
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.4\. 在ReplicationController的作用范围内移动Pod
- en: Pods created by a ReplicationController aren’t tied to the ReplicationController
    in any way. At any moment, a ReplicationController manages pods that match its
    label selector. By changing a pod’s labels, it can be removed from or added to
    the scope of a ReplicationController. It can even be moved from one ReplicationController
    to another.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由ReplicationController创建的pod与ReplicationController没有任何绑定关系。在任何时刻，ReplicationController管理着匹配其标签选择器的pod。通过更改pod的标签，它可以从ReplicationController的作用域中移除或添加。它甚至可以被移动到另一个ReplicationController。
- en: '|  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Although a pod isn’t tied to a ReplicationController, the pod does reference
    it in the `metadata.ownerReferences` field, which you can use to easily find which
    ReplicationController a pod belongs to.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管pod没有绑定到ReplicationController，但pod确实在`metadata.ownerReferences`字段中引用了它，你可以使用这个字段轻松地找到pod所属的ReplicationController。
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If you change a pod’s labels so they no longer match a ReplicationController’s
    label selector, the pod becomes like any other manually created pod. It’s no longer
    managed by anything. If the node running the pod fails, the pod is obviously not
    rescheduled. But keep in mind that when you changed the pod’s labels, the replication
    controller noticed one pod was missing and spun up a new pod to replace it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改pod的标签，使其不再匹配ReplicationController的标签选择器，那么pod就像任何其他手动创建的pod一样。它不再由任何东西管理。如果运行pod的节点失败，pod显然不会被重新调度。但请注意，当你更改pod的标签时，复制控制器注意到丢失了一个pod，并启动了一个新的pod来替换它。
- en: Let’s try this with your pods. Because your ReplicationController manages pods
    that have the `app=kubia` label, you need to either remove this label or change
    its value to move the pod out of the ReplicationController’s scope. Adding another
    label will have no effect, because the ReplicationController doesn’t care if the
    pod has any additional labels. It only cares whether the pod has all the labels
    referenced in the label selector.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用你的pod试一试。因为你的ReplicationController管理着带有 `app=kubia` 标签的pod，所以你需要移除这个标签或更改其值，将pod移出ReplicationController的作用域。添加另一个标签将没有任何效果，因为ReplicationController不关心pod是否有任何额外的标签。它只关心pod是否具有标签选择器中引用的所有标签。
- en: Adding labels to pods managed by a ReplicationController
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 向由ReplicationController管理的pod添加标签
- en: 'Let’s confirm that a ReplicationController doesn’t care if you add additional
    labels to its managed pods:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认 ReplicationController 不关心你是否为其管理的 pod 添加额外的标签：
- en: '`$ kubectl label pod kubia-dmdck type=special` `pod "kubia-dmdck" labeled`
    `$ kubectl get pods --show-labels` `NAME          READY   STATUS    RESTARTS  
    AGE   LABELS kubia-oini2   1/1     Running   0          11m   app=kubia kubia-k0xz6  
    1/1     Running   0          11m   app=kubia kubia-dmdck   1/1     Running   0         
    1m    app=kubia,type=special`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl label pod kubia-dmdck type=special` `pod "kubia-dmdck" labeled`
    `$ kubectl get pods --show-labels` `NAME          READY   STATUS    RESTARTS  
    AGE   LABELS kubia-oini2   1/1     Running   0          11m   app=kubia kubia-k0xz6  
    1/1     Running   0          11m   app=kubia kubia-dmdck   1/1     Running   0         
    1m    app=kubia,type=special`'
- en: You’ve added the `type=special` label to one of the pods. Listing all pods again
    shows the same three pods as before, because no change occurred as far as the
    ReplicationController is concerned.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经将 `type=special` 标签添加到了一个pod上。再次列出所有pod，应该显示与之前相同的三个pod，因为就ReplicationController而言没有发生变化。
- en: Changing the labels of a managed pod
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 更改管理pod的标签
- en: 'Now, you’ll change the `app=kubia` label to something else. This will make
    the pod no longer match the ReplicationController’s label selector, leaving it
    to only match two pods. The ReplicationController should therefore start a new
    pod to bring the number back to three:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将更改 `app=kubia` 标签为其他内容。这将使 pod 无法匹配 ReplicationController 的标签选择器，使其只能匹配两个
    pod。因此，ReplicationController 应该启动一个新的 pod，将数量恢复到三个：
- en: '`$ kubectl label pod kubia-dmdck app=foo --overwrite` `pod "kubia-dmdck" labeled`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl label pod kubia-dmdck app=foo --overwrite` `pod "kubia-dmdck" labeled`'
- en: The `--overwrite` argument is necessary; otherwise `kubectl` will only print
    out a warning and won’t change the label, to prevent you from inadvertently changing
    an existing label’s value when your intent is to add a new one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`--overwrite` 参数是必要的；否则 `kubectl` 将只会打印出一个警告，而不会更改标签，以防止你在意图添加新标签时意外更改现有标签的值。'
- en: 'Listing all the pods again should now show four pods:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 再次列出所有pod现在应该显示四个pod：
- en: '`$ kubectl get pods -L app` `NAME         READY  STATUS             RESTARTS 
    AGE  APP kubia-2qneh  0/1    ContainerCreating  0         2s   kubia` `1` `kubia-oini2 
    1/1    Running            0         20m  kubia kubia-k0xz6  1/1    Running           
    0         20m  kubia kubia-dmdck  1/1    Running            0         10m  foo`
    `2`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods -L app` `名称         READY  状态             RESTARTS  AGE 
    APP kubia-2qneh  0/1    容器创建中  0         2s   kubia` `1` `kubia-oini2  1/1   
    运行中            0         20m  kubia kubia-k0xz6  1/1    运行中            0        
    20m  kubia kubia-dmdck  1/1    运行中            0         10m  foo` `2`'
- en: 1 Newly created pod that replaces the pod you removed from the scope of the
    ReplicationController
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 新创建的Pod，用于替换你从ReplicationController作用范围中移除的Pod
- en: 2 Pod no longer managed by the ReplicationController
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 Pod不再由ReplicationController管理
- en: '|  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You’re using the `-L app` option to display the `app` label in a column.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在使用`-L app`选项在列中显示`app`标签。
- en: '|  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'There, you now have four pods altogether: one that isn’t managed by your Replication-Controller
    and three that are. Among them is the newly created pod.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你总共有四个Pod：一个不受你的Replication-Controller管理，另外三个是。其中就包括新创建的Pod。
- en: '[Figure 4.5](#filepos411922) illustrates what happened when you changed the
    pod’s labels so they no longer matched the ReplicationController’s pod selector.
    You can see your three pods and your ReplicationController. After you change the
    pod’s label from `app=kubia` to `app=foo`, the ReplicationController no longer
    cares about the pod. Because the controller’s replica count is set to 3 and only
    two pods match the label selector, the ReplicationController spins up pod `kubia-2qneh`
    to bring the number back up to three. Pod `kubia-dmdck` is now completely independent
    and will keep running until you delete it manually (you can do that now, because
    you don’t need it anymore).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.5](#filepos411922)展示了当你更改Pod的标签，使其不再匹配ReplicationController的Pod选择器时发生了什么。你可以看到你的三个Pod和你的ReplicationController。在你将Pod的标签从`app=kubia`更改为`app=foo`后，ReplicationController就不再关心这个Pod了。因为控制器的副本计数设置为3，只有两个Pod匹配标签选择器，所以ReplicationController启动了`kubia-2qneh`
    Pod，将数量恢复到3。`kubia-dmdck` Pod现在完全独立，将一直运行，直到你手动删除它（你现在可以这样做，因为你不再需要它了）。'
- en: Figure 4.5\. Removing a pod from the scope of a ReplicationController by changing
    its labels
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5\. 通过更改标签从ReplicationController的作用范围中移除Pod
- en: '![](images/00053.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00053.jpg)'
- en: Removing pods from controllers in practice
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中从控制器中移除Pod
- en: Removing a pod from the scope of the ReplicationController comes in handy when
    you want to perform actions on a specific pod. For example, you might have a bug
    that causes your pod to start behaving badly after a specific amount of time or
    a specific event. If you know a pod is malfunctioning, you can take it out of
    the ReplicationController’s scope, let the controller replace it with a new one,
    and then debug or play with the pod in any way you want. Once you’re done, you
    delete the pod.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想对特定的Pod执行操作时，从ReplicationController的作用范围中移除Pod会很有用。例如，你可能有一个bug，导致Pod在经过一段时间或特定事件后开始表现不佳。如果你知道某个Pod出现故障，你可以将其从ReplicationController的作用范围中移除，让控制器用一个新的Pod替换它，然后以任何你想要的方式调试或操作这个Pod。一旦完成，你就可以删除这个Pod。
- en: Changing the ReplicationController’s label selector
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 修改ReplicationController的标签选择器
- en: As an exercise to see if you fully understand ReplicationControllers, what do
    you think would happen if instead of changing the labels of a pod, you modified
    the Replication-Controller’s label selector?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 作为检验你是否完全理解ReplicationController的练习，如果你不是更改Pod的标签，而是修改Replication-Controller的标签选择器，你认为会发生什么？
- en: If your answer is that it would make all the pods fall out of the scope of the
    ReplicationController, which would result in it creating three new pods, you’re
    absolutely right. And it shows that you understand how ReplicationControllers
    work.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的答案是这会使所有Pod脱离ReplicationController的作用范围，从而导致它创建三个新的Pod，你完全正确。这表明你理解了ReplicationController是如何工作的。
- en: Kubernetes does allow you to change a ReplicationController’s label selector,
    but that’s not the case for the other resources that are covered in the second
    half of this chapter and which are also used for managing pods. You’ll never change
    a controller’s label selector, but you’ll regularly change its pod template. Let’s
    take a look at that.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许你更改ReplicationController的标签选择器，但本章后半部分介绍的其他资源（这些资源也用于管理Pod）则不是这样。你永远不会更改控制器的标签选择器，但你会经常更改其Pod模板。让我们看看这一点。
- en: 4.2.5\. Changing the pod template
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.5\. 修改Pod模板
- en: A ReplicationController’s pod template can be modified at any time. Changing
    the pod template is like replacing a cookie cutter with another one. It will only
    affect the cookies you cut out afterward and will have no effect on the ones you’ve
    already cut (see [figure 4.6](#filepos414391)). To modify the old pods, you’d
    need to delete them and let the Replication-Controller replace them with new ones
    based on the new template.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController 的 pod 模板可以在任何时候进行修改。更改 pod 模板就像更换一个模具。它只会影响之后切出的饼干，而不会影响你已经切出的饼干（参见[图
    4.6](#filepos414391)）。要修改旧 pod，你需要删除它们，并让 Replication-Controller 用基于新模板的新 pod
    替换它们。
- en: Figure 4.6\. Changing a ReplicationController’s pod template only affects pods
    created afterward and has no effect on existing pods.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6\. 更改 ReplicationController 的 pod 模板只会影响之后创建的 pod，而不会影响现有的 pod。
- en: '![](images/00071.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00071.jpg)'
- en: 'As an exercise, you can try editing the ReplicationController and adding a
    label to the pod template. You can edit the ReplicationController with the following
    command:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，你可以尝试编辑 ReplicationController 并为 pod 模板添加一个标签。你可以使用以下命令编辑 ReplicationController：
- en: '`$ kubectl edit rc kubia`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl edit rc kubia`'
- en: 'This will open the ReplicationController’s YAML definition in your default
    text editor. Find the pod template section and add an additional label to the
    metadata. After you save your changes and exit the editor, `kubectl` will update
    the ReplicationController and print the following message:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的默认文本编辑器中打开 ReplicationController 的 YAML 定义。找到 pod 模板部分并添加一个额外的标签到元数据。在你保存更改并退出编辑器后，`kubectl`
    将更新 ReplicationController 并打印以下消息：
- en: '`replicationcontroller "kubia" edited`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`replicationcontroller "kubia" edited`'
- en: You can now list pods and their labels again and confirm that they haven’t changed.
    But if you delete the pods and wait for their replacements to be created, you’ll
    see the new label.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以再次列出 pod 和它们的标签，并确认它们没有改变。但如果你删除 pod 并等待它们的替代品被创建，你会看到新的标签。
- en: Editing a ReplicationController like this to change the container image in the
    pod template, deleting the existing pods, and letting them be replaced with new
    ones from the new template could be used for upgrading pods, but you’ll learn
    a better way of doing that in [chapter 9](index_split_074.html#filepos865425).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式编辑 ReplicationController 以更改 pod 模板中的容器镜像，删除现有的 pod，并让它们由新模板中的新 pod 替换，这可以用于升级
    pod，但你将在第 9 章（index_split_074.html#filepos865425）中学习更好的方法。
- en: '|  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Configuring kubectl edit to use a different text editor
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 kubectl edit 以使用不同的文本编辑器
- en: 'You can tell `kubectl` to use a text editor of your choice by setting the `KUBE_EDITOR`
    environment variable. For example, if you’d like to use `nano` for editing Kubernetes
    resources, execute the following command (or put it into your `~/.bashrc` or an
    equivalent file):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过设置 `KUBE_EDITOR` 环境变量来告诉 `kubectl` 使用你选择的文本编辑器。例如，如果你想使用 `nano` 编辑 Kubernetes
    资源，执行以下命令（或将它放入你的 `~/.bashrc` 或等效文件中）：
- en: '`export KUBE_EDITOR="/usr/bin/nano"`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`export KUBE_EDITOR="/usr/bin/nano"`'
- en: If the `KUBE_EDITOR` environment variable isn’t set, `kubectl edit` falls back
    to using the default editor, usually configured through the `EDITOR` environment
    variable.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有设置 `KUBE_EDITOR` 环境变量，`kubectl edit` 将回退到使用默认编辑器，通常通过 `EDITOR` 环境变量配置。
- en: '|  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 4.2.6\. Horizontally scaling pods
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.6\. 水平扩展 pod
- en: You’ve seen how ReplicationControllers make sure a specific number of pod instances
    is always running. Because it’s incredibly simple to change the desired number
    of replicas, this also means scaling pods horizontally is trivial.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了 ReplicationControllers 如何确保始终运行特定数量的 pod 实例。由于更改期望副本数量的操作极其简单，这也意味着水平扩展
    pod 是微不足道的。
- en: Scaling the number of pods up or down is as easy as changing the value of the
    replicas field in the ReplicationController resource. After the change, the Replication-Controller
    will either see too many pods exist (when scaling down) and delete part of them,
    or see too few of them (when scaling up) and create additional pods.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 上调或下调 pod 的数量就像更改 ReplicationController 资源中 replicas 字段的值一样简单。更改后，Replication-Controller
    将会看到 pod 数量过多（在缩小时）并删除其中的一部分，或者看到 pod 数量过少（在扩小时）并创建额外的 pod。
- en: Scaling up a ReplicationController
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 ReplicationController
- en: 'Your ReplicationController has been keeping three instances of your pod running.
    You’re going to scale that number up to 10 now. As you may remember, you’ve already
    scaled a ReplicationController in [chapter 2](index_split_022.html#filepos185841).
    You could use the same command as before:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你的ReplicationController一直在运行你的Pod的三个实例。现在你将把这个数字增加到10。你可能还记得，你已经在[第2章](index_split_022.html#filepos185841)中扩展了一个ReplicationController。你可以使用之前的相同命令：
- en: '`$ kubectl scale rc kubia --replicas=10`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl scale rc kubia --replicas=10`'
- en: But you’ll do it differently this time.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次你会用不同的方式来做。
- en: Scaling a ReplicationController by editing its definition
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编辑定义扩展ReplicationController
- en: 'Instead of using the `kubectl scale` command, you’re going to scale it in a
    declarative way by editing the ReplicationController’s definition:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用`kubectl scale`命令，你将通过编辑ReplicationController的定义以声明式的方式对其进行扩展：
- en: '`$ kubectl edit rc kubia`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl edit rc kubia`'
- en: When the text editor opens, find the `spec.replicas` field and change its value
    to `10`, as shown in the following listing.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当文本编辑器打开时，找到`spec.replicas`字段，并将其值更改为`10`，如下所示。
- en: Listing 4.7\. Editing the RC in a text editor by running `kubectl edit`
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.7. 通过运行`kubectl edit`在文本编辑器中编辑RC
- en: '`# Please edit the object below. Lines beginning with a ''#'' will be ignored,
    # and an empty file will abort the edit. If an error occurs while saving # this
    file will be reopened with the relevant failures. apiVersion: v1 kind: ReplicationController
    metadata:   ... spec:   replicas: 3` `1` `selector:     app: kubia   ...`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`# 请编辑以下对象。以''#''开头的行将被忽略，# 一个空文件将终止编辑。如果在保存此文件时发生错误，则将重新打开此文件并显示相关失败。apiVersion:
    v1 kind: ReplicationController metadata:   ... spec:   replicas: 3` `1` `selector:
        app: kubia   ...`'
- en: 1 Change the number 3 to number 10 in this line.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 将此行中的数字3更改为数字10。
- en: 'When you save the file and close the editor, the ReplicationController is updated
    and it immediately scales the number of pods to 10:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当你保存文件并关闭编辑器时，ReplicationController将被更新，并且它立即将Pod的数量扩展到10：
- en: '`$ kubectl get rc` `NAME      DESIRED   CURRENT   READY     AGE kubia     10       
    10        4         21m`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get rc` `NAME      DESIRED   CURRENT   READY     AGE kubia     10       
    10        4         21m`'
- en: There you go. If the `kubectl scale` command makes it look as though you’re
    telling Kubernetes exactly what to do, it’s now much clearer that you’re making
    a declarative change to the desired state of the ReplicationController and not
    telling Kubernetes to do something.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。如果`kubectl scale`命令让你看起来像是在告诉Kubernetes确切要做什么，那么现在就更加清楚，你是在对ReplicationController的期望状态进行声明式更改，而不是告诉Kubernetes做什么。
- en: Scaling down with the kubectl scale command
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kubectl scale命令缩小规模
- en: 'Now scale back down to 3\. You can use the `kubectl scale` command:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将其缩放到3。你可以使用`kubectl scale`命令：
- en: '`$ kubectl scale rc kubia --replicas=3`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl scale rc kubia --replicas=3`'
- en: All this command does is modify the `spec.replicas` field of the ReplicationController’s
    definition—like when you changed it through `kubectl edit`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令所做的只是修改ReplicationController定义中的`spec.replicas`字段——就像你通过`kubectl edit`修改它一样。
- en: Understanding the declarative approach to scaling
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 理解声明式扩展方法
- en: 'Horizontally scaling pods in Kubernetes is a matter of stating your desire:
    “I want to have x number of instances running.” You’re not telling Kubernetes
    what or how to do it. You’re just specifying the desired state.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中水平扩展Pod是一个表达你愿望的问题：“我想运行x个实例。”你并没有告诉Kubernetes做什么或如何做。你只是指定了期望的状态。
- en: This declarative approach makes interacting with a Kubernetes cluster easy.
    Imagine if you had to manually determine the current number of running instances
    and then explicitly tell Kubernetes how many additional instances to run. That’s
    more work and is much more error-prone. Changing a simple number is much easier,
    and in [chapter 15](index_split_113.html#filepos1423922), you’ll learn that even
    that can be done by Kubernetes itself if you enable horizontal pod auto-scaling.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这种声明式方法使得与Kubernetes集群交互变得简单。想象一下，如果你必须手动确定当前运行的实例数量，然后明确告诉Kubernetes要运行多少额外的实例。那将是一项更多的工作，并且更容易出错。更改一个简单的数字要容易得多，在[第15章](index_split_113.html#filepos1423922)中，你将了解到，如果你启用了水平Pod自动扩展，甚至那也可以由Kubernetes本身完成。
- en: 4.2.7\. Deleting a ReplicationController
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.7. 删除一个ReplicationController
- en: When you delete a ReplicationController through `kubectl delete`, the pods are
    also deleted. But because pods created by a ReplicationController aren’t an integral
    part of the ReplicationController, and are only managed by it, you can delete
    only the ReplicationController and leave the pods running, as shown in [figure
    4.7](#filepos422047).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当你通过`kubectl delete`删除ReplicationController时，Pod也会被删除。但由于由ReplicationController创建的Pod不是ReplicationController的组成部分，并且仅由它管理，因此你可以仅删除ReplicationController并保持Pod运行，如图4.7所示。
- en: Figure 4.7\. Deleting a replication controller with `--cascade=false` leaves
    pods unmanaged.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7\. 使用`--cascade=false`删除replication controller会留下未管理的Pod。
- en: '![](images/00090.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00090.jpg)'
- en: This may be useful when you initially have a set of pods managed by a Replication-Controller,
    and then decide to replace the ReplicationController with a ReplicaSet, for example
    (you’ll learn about them next.). You can do this without affecting the pods and
    keep them running without interruption while you replace the Replication-Controller
    that manages them.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能在你最初有一组由ReplicationController管理的Pod时很有用，然后决定用ReplicaSet替换ReplicationController，例如（你将在下一章中了解它们）。你可以这样做而不影响Pod，并在替换管理它们的ReplicationController时保持它们的无间断运行。
- en: 'When deleting a ReplicationController with `kubectl delete`, you can keep its
    pods running by passing the `--cascade=false` option to the command. Try that
    now:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`kubectl delete`删除ReplicationController时，可以通过向命令传递`--cascade=false`选项来保持其Pod运行。现在尝试一下：
- en: '`$ kubectl delete rc kubia --cascade=false` `replicationcontroller "kubia"
    deleted`'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete rc kubia --cascade=false` `replicationcontroller "kubia"
    deleted`'
- en: You’ve deleted the ReplicationController so the pods are on their own. They
    are no longer managed. But you can always create a new ReplicationController with
    the proper label selector and make them managed again.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经删除了ReplicationController，所以Pod们现在独立运行。它们不再受管理。但你可以始终创建一个新的ReplicationController，并使用适当的标签选择器来再次管理它们。
- en: 4.3\. Using ReplicaSets instead of ReplicationControllers
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3\. 使用ReplicaSet代替ReplicationController
- en: Initially, ReplicationControllers were the only Kubernetes component for replicating
    pods and rescheduling them when nodes failed. Later, a similar resource called
    a ReplicaSet was introduced. It’s a new generation of ReplicationController and
    replaces it completely (ReplicationControllers will eventually be deprecated).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，ReplicationController是Kubernetes中用于复制Pod并在节点失败时重新调度Pod的唯一组件。后来，引入了一个类似资源，称为ReplicaSet。它是ReplicationController的新一代，并完全取代了它（ReplicationController最终将被弃用）。
- en: You could have started this chapter by creating a ReplicaSet instead of a Replication-Controller,
    but I felt it would be a good idea to start with what was initially available
    in Kubernetes. Plus, you’ll still see ReplicationControllers used in the wild,
    so it’s good for you to know about them. That said, you should always create ReplicaSets
    instead of ReplicationControllers from now on. They’re almost identical, so you
    shouldn’t have any trouble using them instead.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 您本可以以创建ReplicaSet而不是ReplicationController开始本章，但我认为从Kubernetes最初可用的内容开始是一个好主意。此外，你仍然会在野外看到ReplicationController的使用，所以了解它们对你来说是有好处的。话虽如此，从现在开始，你应该始终创建ReplicaSet而不是ReplicationController。它们几乎相同，所以你应该不会在使用它们时遇到任何麻烦。
- en: You usually won’t create them directly, but instead have them created automatically
    when you create the higher-level Deployment resource, which you’ll learn about
    in [chapter 9](index_split_074.html#filepos865425). In any case, you should understand
    ReplicaSets, so let’s see how they differ from ReplicationControllers.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常不会直接创建它们，而是在创建更高层次的Deployment资源时自动创建它们，你将在第9章（index_split_074.html#filepos865425）中了解它。无论如何，你应该了解ReplicaSet，让我们看看它们与ReplicationController有何不同。
- en: 4.3.1\. Comparing a ReplicaSet to a ReplicationController
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.1\. 将ReplicaSet与ReplicationController进行比较
- en: A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive
    pod selectors. Whereas a ReplicationController’s label selector only allows matching
    pods that include a certain label, a ReplicaSet’s selector also allows matching
    pods that lack a certain label or pods that include a certain label key, regardless
    of its value.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet的行为与ReplicationController完全相同，但它具有更丰富的Pod选择器。而ReplicationController的标签选择器仅允许匹配包含特定标签的Pod，ReplicaSet的选择器还允许匹配缺少特定标签或包含特定标签键的Pod，无论其值如何。
- en: Also, for example, a single ReplicationController can’t match pods with the
    label `env=production` and those with the label `env=devel` at the same time.
    It can only match either pods with the `env=production` label or pods with the
    `env=devel` label. But a single ReplicaSet can match both sets of pods and treat
    them as a single group.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，例如，单个ReplicationController不能同时匹配带有标签`env=production`和带有标签`env=devel`的Pods。它只能匹配带有`env=production`标签的Pods或带有`env=devel`标签的Pods。但单个ReplicaSet可以匹配这两组Pods并将它们视为一个单一组。
- en: Similarly, a ReplicationController can’t match pods based merely on the presence
    of a label key, regardless of its value, whereas a ReplicaSet can. For example,
    a ReplicaSet can match all pods that include a label with the key `env`, whatever
    its actual value is (you can think of it as `env=*`).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，ReplicationController不能仅仅根据标签键的存在来匹配Pods，无论其值如何，而ReplicaSet可以。例如，ReplicaSet可以匹配所有包含键为`env`的标签的Pods，无论其实际值是什么（你可以将其视为`env=*`）。
- en: 4.3.2\. Defining a ReplicaSet
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.2\. 定义ReplicaSet
- en: You’re going to create a ReplicaSet now to see how the orphaned pods that were
    created by your ReplicationController and then abandoned earlier can now be adopted
    by a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet
    by creating a new file called kubia-replicaset.yaml with the contents in the following
    listing.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将创建一个ReplicaSet来查看之前由你的ReplicationController创建然后被遗弃的孤儿Pods现在如何可以被ReplicaSet所采用。首先，你需要将你的ReplicationController重写为一个ReplicaSet，通过创建一个名为kubia-replicaset.yaml的新文件，并包含以下列表中的内容。
- en: 'Listing 4.8\. A YAML definition of a ReplicaSet: kubia-replicaset.yaml'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.8\. ReplicaSet的YAML定义：kubia-replicaset.yaml
- en: '`apiVersion: apps/v1beta2` `1` `kind: ReplicaSet` `1` `metadata:   name: kubia
    spec:   replicas: 3   selector:     matchLabels:` `2` `app: kubia` `2` `template:`
    `3` `metadata:` `3` `labels:` `3` `app: kubia` `3` `spec:` `3` `containers:` `3`
    `- name: kubia` `3` `image: luksa/kubia` `3`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: apps/v1beta2` `1` `kind: ReplicaSet` `1` `metadata:` `1` `name:
    kubia` `1` `spec:` `1` `replicas: 3` `1` `selector:` `1` `matchLabels:` `2` `app:
    kubia` `2` `template:` `3` `metadata:` `3` `labels:` `3` `app: kubia` `3` `spec:`
    `3` `containers:` `3` `- name: kubia` `3` `image: luksa/kubia` `3`'
- en: 1 ReplicaSets aren’t part of the v1 API, but belong to the apps API group and
    version v1beta2\.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ReplicaSets不是v1 API的一部分，但属于`apps` API组，版本为v1beta2。
- en: 2 You’re using the simpler matchLabels selector here, which is much like a ReplicationController’s
    selector.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你在这里使用的是更简单的matchLabels选择器，这与ReplicationController的选择器非常相似。
- en: 3 The template is the same as in the ReplicationController.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 模板与ReplicationController中的相同。
- en: The first thing to note is that ReplicaSets aren’t part of the v1 API, so you
    need to ensure you specify the proper `apiVersion` when creating the resource.
    You’re creating a resource of type ReplicaSet which has much the same contents
    as the Replication-Controller you created earlier.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，ReplicaSets不是v1 API的一部分，所以在创建资源时你需要确保指定正确的`apiVersion`。你正在创建一个类型为ReplicaSet的资源，其内容与之前创建的Replication-Controller非常相似。
- en: The only difference is in the selector. Instead of listing labels the pods need
    to have directly under the `selector` property, you’re specifying them under `selector.matchLabels`.
    This is the simpler (and less expressive) way of defining label selectors in a
    ReplicaSet. Later, you’ll look at the more expressive option, as well.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于选择器。你不需要在`selector`属性下直接列出Pods需要拥有的标签，而是在`selector.matchLabels`下指定它们。这是在ReplicaSet中定义标签选择器的一种更简单（且不那么表达性）的方式。稍后，你还将看到更表达性的选项。
- en: '|  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: About the API version attribute
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 关于API版本属性
- en: 'This is your first opportunity to see that the `apiVersion` property specifies
    two things:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你第一次看到`apiVersion`属性指定了两件事：
- en: The API group (which is `apps` in this case)
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API组（在这个例子中是`apps`）
- en: The actual API version (`v1beta2`)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际的API版本（`v1beta2`）
- en: 'You’ll see throughout the book that certain Kubernetes resources are in what’s
    called the core API group, which doesn’t need to be specified in the `apiVersion`
    field (you just specify the version—for example, you’ve been using `apiVersion:
    v1` when defining Pod resources). Other resources, which were introduced in later
    Kubernetes versions, are categorized into several API groups. Look at the inside
    of the book’s covers to see all resources and their respective API groups.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '你会在整本书中看到，某些Kubernetes资源位于所谓的核心API组中，在`apiVersion`字段中不需要指定（你只需指定版本——例如，在定义Pod资源时，你一直使用`apiVersion:
    v1`）。其他资源，在后来的Kubernetes版本中引入，被分类到几个API组中。查看书的封面内部以查看所有资源和它们各自的API组。'
- en: '|  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Because you still have three pods matching the `app=kubia` selector running
    from earlier, creating this ReplicaSet will not cause any new pods to be created.
    The ReplicaSet will take those existing three pods under its wing.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你仍然有三个匹配 `app=kubia` 选择器的 pod 在之前运行，创建这个 ReplicaSet 不会导致创建任何新的 pod。ReplicaSet
    将将现有的三个 pod 纳入其翼下。
- en: 4.3.3\. Creating and examining a ReplicaSet
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.3\. 创建和检查 ReplicaSet
- en: 'Create the ReplicaSet from the YAML file with the `kubectl create` command.
    After that, you can examine the ReplicaSet with `kubectl get` and `kubectl describe`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl create` 命令从 YAML 文件创建 ReplicaSet。之后，你可以使用 `kubectl get` 和 `kubectl
    describe` 检查 ReplicaSet：
- en: '`$ kubectl get rs` `NAME      DESIRED   CURRENT   READY     AGE kubia     3        
    3         3         3s`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get rs` `NAME      DESIRED   CURRENT   READY     AGE kubia     3        
    3         3         3s`'
- en: '|  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Use `rs` shorthand, which stands for `replicaset`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `rs` 简写，代表 `replicaset`。
- en: '|  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '`$ kubectl describe rs` `Name:           kubia Namespace:      default Selector:      
    app=kubia Labels:         app=kubia Annotations:    <none> Replicas:       3 current
    / 3 desired Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod
    Template:   Labels:       app=kubia   Containers:   ...   Volumes:      <none>
    Events:         <none>`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe rs` `Name:           kubia Namespace:      default Selector:      
    app=kubia Labels:         app=kubia Annotations:    <none> Replicas:       3 current
    / 3 desired Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod
    Template:    Labels:       app=kubia    Containers:   ...    Volumes:      <none>   
    Events:         <none>`'
- en: As you can see, the ReplicaSet isn’t any different from a ReplicationController.
    It’s showing it has three replicas matching the selector. If you list all the
    pods, you’ll see they’re still the same three pods you had before. The ReplicaSet
    didn’t create any new ones.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，ReplicaSet 与 ReplicationController 没有任何不同。它显示有三个副本匹配选择器。如果你列出所有 pod，你会看到它们仍然是之前的那三个
    pod。ReplicaSet 没有创建任何新的 pod。
- en: 4.3.4\. Using the ReplicaSet’s more expressive label selectors
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.4\. 使用 ReplicaSet 的更表达式标签选择器
- en: The main improvements of ReplicaSets over ReplicationControllers are their more
    expressive label selectors. You intentionally used the simpler `matchLabels` selector
    in the first ReplicaSet example to see that ReplicaSets are no different from
    Replication-Controllers. Now, you’ll rewrite the selector to use the more powerful
    `matchExpressions` property, as shown in the following listing.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 相比 ReplicationController 的主要改进是它们更表达式的标签选择器。你故意在第一个 ReplicaSet 示例中使用了更简单的
    `matchLabels` 选择器，以查看 ReplicaSet 与 Replication-Controllers 没有任何不同。现在，你将重写选择器以使用更强大的
    `matchExpressions` 属性，如下所示。
- en: 'Listing 4.9\. A `matchExpressions` selector: kubia-replicaset-matchexpressions.yaml'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.9\. 一个 `matchExpressions` 选择器：kubia-replicaset-matchexpressions.yaml
- en: '`selector:    matchExpressions:      - key: app` `1` `operator: In` `2` `values:`
    `2` `- kubia` `2`'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`selector:   matchExpressions:    - key: app    operator: In    values:   
    - kubia`'
- en: 1 This selector requires the pod to contain a label with the “app” key.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此选择器要求 pod 包含一个具有“app”键的标签。
- en: 2 The label’s value must be “kubia”.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 标签的值必须是“kubia”。
- en: '|  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Only the selector is shown. You’ll find the whole ReplicaSet definition in the
    book’s code archive.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 只显示了选择器。你可以在书籍的代码存档中找到整个 ReplicaSet 定义。
- en: '|  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'You can add additional expressions to the selector. As in the example, each
    expression must contain a `key`, an `operator`, and possibly (depending on the
    operator) a list of `values`. You’ll see four valid operators:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以向选择器添加额外的表达式。例如，每个表达式必须包含一个 `key`、一个 `operator` 以及可能（根据操作符）的 `values` 列表。你将看到四个有效的操作符：
- en: '`In—`Label’s value must match one of the specified `values`.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`In—`标签的值必须匹配指定的 `values`。'
- en: '`NotIn—`Label’s value must not match any of the specified `values`.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NotIn—`标签的值必须不匹配指定的任何 `values`。'
- en: '`Exists—`Pod must include a label with the specified key (the value isn’t important).
    When using this operator, you shouldn’t specify the `values` field.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exists—`Pod 必须包含具有指定键的标签（值不重要）。当使用此操作符时，你不应该指定 `values` 字段。'
- en: '`DoesNotExist—`Pod must not include a label with the specified key. The `values`
    property must not be specified.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoesNotExist—`Pod 必须不包含具有指定键的标签。`values` 属性不得指定。'
- en: If you specify multiple expressions, all those expressions must evaluate to
    true for the selector to match a pod. If you specify both `matchLabels` and `matchExpressions`,
    all the labels must match and all the expressions must evaluate to true for the
    pod to match the selector.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你指定了多个表达式，所有这些表达式都必须评估为真，选择器才能匹配 pod。如果你指定了 `matchLabels` 和 `matchExpressions`，所有标签都必须匹配，所有表达式都必须评估为真，pod
    才能匹配选择器。
- en: 4.3.5\. Wrapping up ReplicaSets
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.5. 总结 ReplicaSets
- en: This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.
    Remember, always use them instead of ReplicationControllers, but you may still
    find ReplicationControllers in other people’s deployments.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对 ReplicaSets 作为 ReplicationControllers 的替代方法的快速介绍。记住，始终使用它们而不是 ReplicationControllers，但你可能会在其他人的部署中找到
    ReplicationControllers。
- en: 'Now, delete the ReplicaSet to clean up your cluster a little. You can delete
    the ReplicaSet the same way you’d delete a ReplicationController:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，删除 ReplicaSet 以清理你的集群。你可以用删除 ReplicationController 的相同方式删除 ReplicaSet：
- en: '`$ kubectl delete rs kubia` `replicaset "kubia" deleted`'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete rs kubia` `replicaset "kubia" deleted`'
- en: Deleting the ReplicaSet should delete all the pods. List the pods to confirm
    that’s the case.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 删除 ReplicaSet 应该会删除所有 pod。列出 pod 以确认这一点。
- en: 4.4\. Running exactly one pod on each node with DaemonSets
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 4.4. 在每个节点上使用 DaemonSet 运行一个 pod
- en: Both ReplicationControllers and ReplicaSets are used for running a specific
    number of pods deployed anywhere in the Kubernetes cluster. But certain cases
    exist when you want a pod to run on each and every node in the cluster (and each
    node needs to run exactly one instance of the pod, as shown in [figure 4.8](#filepos436735)).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationControllers 和 ReplicaSets 都用于在 Kubernetes 集群中的任何位置运行特定数量的 pod。但在某些情况下，你可能希望
    pod 在集群的每个节点上运行（并且每个节点需要运行 pod 的确切一个实例，如图 4.8 所示 [图 4.8](#filepos436735)）。
- en: Figure 4.8\. DaemonSets run only a single pod replica on each node, whereas
    ReplicaSets scatter them around the whole cluster randomly.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8. DaemonSet 在每个节点上只运行一个 pod 副本，而 ReplicaSets 则将它们随机散布在整个集群中。
- en: '![](images/00108.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00108.jpg)'
- en: Those cases include infrastructure-related pods that perform system-level operations.
    For example, you’ll want to run a log collector and a resource monitor on every
    node. Another good example is Kubernetes’ own kube-proxy process, which needs
    to run on all nodes to make services work.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情况包括与基础设施相关的 pod，它们执行系统级操作。例如，你可能在每个节点上运行日志收集器和资源监控器。另一个很好的例子是 Kubernetes
    自身的 kube-proxy 进程，它需要在所有节点上运行以使服务工作。
- en: Outside of Kubernetes, such processes would usually be started through system
    init scripts or the systemd daemon during node boot up. On Kubernetes nodes, you
    can still use systemd to run your system processes, but then you can’t take advantage
    of all the features Kubernetes provides.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 之外，此类进程通常会在节点启动时通过系统初始化脚本或 systemd 守护进程启动。在 Kubernetes 节点上，你仍然可以使用
    systemd 运行你的系统进程，但那时你就无法利用 Kubernetes 提供的所有功能。
- en: 4.4.1\. Using a DaemonSet to run a pod on every node
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 4.4.1. 使用 DaemonSet 在每个节点上运行 pod
- en: To run a pod on all cluster nodes, you create a DaemonSet object, which is much
    like a ReplicationController or a ReplicaSet, except that pods created by a DaemonSet
    already have a target node specified and skip the Kubernetes Scheduler. They aren’t
    scattered around the cluster randomly.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 要在所有集群节点上运行 pod，你需要创建一个 DaemonSet 对象，这与 ReplicationController 或 ReplicaSet 非常相似，只不过由
    DaemonSet 创建的 pod 已经指定了目标节点，并跳过了 Kubernetes 调度器。它们不会在集群中随机分布。
- en: A DaemonSet makes sure it creates as many pods as there are nodes and deploys
    each one on its own node, as shown in [figure 4.8](#filepos436735).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 确保创建与节点数量相等的 pod，并将每个 pod 部署在其自己的节点上，如图 4.8 所示 [图 4.8](#filepos436735)。
- en: Whereas a ReplicaSet (or ReplicationController) makes sure that a desired number
    of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a
    desired replica count. It doesn’t need it because its job is to ensure that a
    pod matching its pod selector is running on each node.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ReplicaSet（或 ReplicationController）确保集群中存在所需数量的 pod 副本，但 DaemonSet 没有任何关于所需副本数量的概念。它不需要它，因为它的任务是确保匹配其
    pod 选择器的 pod 在每个节点上运行。
- en: If a node goes down, the DaemonSet doesn’t cause the pod to be created elsewhere.
    But when a new node is added to the cluster, the DaemonSet immediately deploys
    a new pod instance to it. It also does the same if someone inadvertently deletes
    one of the pods, leaving the node without the DaemonSet’s pod. Like a ReplicaSet,
    a DaemonSet creates the pod from the pod template configured in it.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点宕机，DaemonSet 不会在其他地方创建 pod。但当集群中添加新节点时，DaemonSet 会立即在该节点上部署一个新的 pod 实例。如果有人意外删除了其中一个
    pod，导致节点没有 DaemonSet 的 pod，它也会这样做。像 ReplicaSet 一样，DaemonSet 会从配置在其内的 pod 模板中创建
    pod。
- en: 4.4.2\. Using a DaemonSet to run pods only on certain nodes
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 4.4.2. 使用 DaemonSet 仅在特定节点上运行 pod
- en: A DaemonSet deploys pods to all nodes in the cluster, unless you specify that
    the pods should only run on a subset of all the nodes. This is done by specifying
    the `node-Selector` property in the pod template, which is part of the DaemonSet
    definition (similar to the pod template in a ReplicaSet or ReplicationController).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 将 Pod 部署到集群中的所有节点，除非您指定 Pod 应仅在所有节点的一个子集上运行。这是通过在 Pod 模板中指定 `node-Selector`
    属性来完成的，该属性是 DaemonSet 定义的一部分（类似于 ReplicaSet 或 ReplicationController 中的 Pod 模板）。
- en: You’ve already used node selectors to deploy a pod onto specific nodes in [chapter
    3](index_split_028.html#filepos271328). A node selector in a DaemonSet is similar—it
    defines the nodes the DaemonSet must deploy its pods to.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在 [第 3 章](index_split_028.html#filepos271328) 中使用节点选择器将 Pod 部署到特定的节点。在 DaemonSet
    中的节点选择器类似——它定义了 DaemonSet 必须部署其 Pod 的节点。
- en: '|  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Later in the book, you’ll learn that nodes can be made unschedulable, preventing
    pods from being deployed to them. A DaemonSet will deploy pods even to such nodes,
    because the unschedulable attribute is only used by the Scheduler, whereas pods
    managed by a DaemonSet bypass the Scheduler completely. This is usually desirable,
    because DaemonSets are meant to run system services, which usually need to run
    even on unschedulable nodes.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分，您将了解到节点可以被设置为不可调度，防止 Pod 部署到它们。DaemonSet 仍会将 Pod 部署到这样的节点，因为不可调度属性仅由调度器使用，而由
    DaemonSet 管理的 Pod 完全绕过调度器。这通常是期望的，因为 DaemonSets 的目的是运行系统服务，这些服务通常需要在不可调度的节点上运行。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Explaining DaemonSets with an example
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 以示例解释 DaemonSet
- en: Let’s imagine having a daemon called `ssd-monitor` that needs to run on all
    nodes that contain a solid-state drive (SSD). You’ll create a DaemonSet that runs
    this daemon on all nodes that are marked as having an SSD. The cluster administrators
    have added the `disk=ssd` label to all such nodes, so you’ll create the DaemonSet
    with a node selector that only selects nodes with that label, as shown in [figure
    4.9](#filepos441058).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象有一个名为 `ssd-monitor` 的守护进程需要在包含固态硬盘（SSD）的所有节点上运行。您将创建一个 DaemonSet，在标记为具有
    SSD 的所有节点上运行此守护进程。集群管理员已将这些节点的 `disk=ssd` 标签添加到所有此类节点，因此您将创建一个带有节点选择器的 DaemonSet，仅选择具有该标签的节点，如图
    4.9 所示。
- en: Figure 4.9\. Using a DaemonSet with a node selector to deploy system pods only
    on certain nodes
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9\. 使用带有节点选择器的 DaemonSet 仅在特定节点上部署系统 Pod
- en: '![](images/00128.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00128.jpg)'
- en: Creating a DaemonSet YAML definition
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 DaemonSet YAML 定义
- en: You’ll create a DaemonSet that runs a mock `ssd-monitor` process, which prints
    “SSD OK” to the standard output every five seconds. I’ve already prepared the
    mock container image and pushed it to Docker Hub, so you can use it instead of
    building your own. Create the YAML for the DaemonSet, as shown in the following
    listing.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您将创建一个运行模拟 `ssd-monitor` 进程的 DaemonSet，该进程每五秒向标准输出打印一次“SSD OK”。我已经准备好了模拟容器镜像并将其推送到
    Docker Hub，因此您可以使用它而不是自己构建。创建 DaemonSet 的 YAML，如下所示。
- en: 'Listing 4.10\. A YAML for a DaemonSet: ssd-monitor-daemonset.yaml'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.10\. DaemonSet 的 YAML：ssd-monitor-daemonset.yaml
- en: '`apiVersion: apps/v1beta2` `1` `kind: DaemonSet` `1` `metadata:   name: ssd-monitor
    spec:   selector:     matchLabels:       app: ssd-monitor   template:     metadata:
          labels:         app: ssd-monitor     spec:       nodeSelector:` `2` `disk:
    ssd` `2` `containers:       - name: main         image: luksa/ssd-monitor`'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: apps/v1beta2` `1` `kind: DaemonSet` `1` `metadata:` `1` `name:
    ssd-monitor` `1` `spec:` `1` `selector:` `1` `matchLabels:` `1` `app: ssd-monitor`
    `1` `template:` `1` `metadata:` `1` `labels:` `1` `app: ssd-monitor` `1` `spec:`
    `1` `nodeSelector:` `2` `disk: ssd` `2` `containers:` `1` `- name: main` `1` `image:
    luksa/ssd-monitor`'
- en: 1 DaemonSets are in the apps API group, version v1beta2\.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 DaemonSet 位于 apps API 组，版本 v1beta2。
- en: 2 The pod template includes a node selector, which selects nodes with the disk=ssd
    label.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 Pod 模板包括一个节点选择器，该选择器选择具有 disk=ssd 标签的节点。
- en: You’re defining a DaemonSet that will run a pod with a single container based
    on the `luksa/ssd-monitor` container image. An instance of this pod will be created
    for each node that has the `disk=ssd` label.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在定义一个 DaemonSet，该 DaemonSet 将基于 `luksa/ssd-monitor` 容器镜像运行一个包含单个容器的 Pod。对于每个具有
    `disk=ssd` 标签的节点，都将创建一个此 Pod 的实例。
- en: Creating the DaemonSet
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 DaemonSet
- en: 'You’ll create the DaemonSet like you always create resources from a YAML file:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 您将像创建 YAML 文件中的资源一样创建 DaemonSet：
- en: '`$ kubectl create -f ssd-monitor-daemonset.yaml` `daemonset "ssd-monitor" created`'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f ssd-monitor-daemonset.yaml` `daemonset "ssd-monitor" created`'
- en: 'Let’s see the created DaemonSet:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看创建的 DaemonSet：
- en: '`$ kubectl get ds` `NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE 
    NODE-SELECTOR ssd-monitor   0        0        0      0           0          disk=ssd`'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get ds` `NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE 
    NODE-SELECTOR ssd-monitor   0        0        0      0           0          disk=ssd`'
- en: 'Those zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 那些零看起来很奇怪。DaemonSet 没有部署任何 pod 吗？列出 pod：
- en: '`$ kubectl get po` `No resources found.`'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `No resources found.`'
- en: Where are the pods? Do you know what’s going on? Yes, you forgot to label your
    nodes with the `disk=ssd` label. No problem—you can do that now. The DaemonSet
    should detect that the nodes’ labels have changed and deploy the pod to all nodes
    with a matching label. Let’s see if that’s true.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 那些pod在哪里？你知道发生了什么吗？是的，你忘记给你的节点添加 `disk=ssd` 标签了。没问题——你现在可以这么做。DaemonSet 应该会检测到节点的标签已更改，并将
    pod 部署到所有匹配标签的节点上。让我们看看这是否是真的。
- en: Adding the required label to your node(s)
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 给你的节点（们）添加所需的标签
- en: 'Regardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll
    need to list the nodes first, because you’ll need to know the node’s name when
    labeling it:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在使用 Minikube、GKE 还是其他多节点集群，你首先需要列出节点，因为当你标记节点时，你需要知道节点的名称：
- en: '`$ kubectl get node` `NAME       STATUS    AGE       VERSION minikube   Ready    
    4d        v1.6.0`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get node` `NAME       STATUS    AGE       VERSION minikube   Ready    
    4d        v1.6.0`'
- en: 'Now, add the `disk=ssd` label to one of your nodes like this:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，像这样给你的一个节点添加 `disk=ssd` 标签：
- en: '`$ kubectl label node minikube disk=ssd` `node "minikube" labeled`'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl label node minikube disk=ssd` `node "minikube" labeled`'
- en: '|  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Replace `minikube` with the name of one of your nodes if you’re not using Minikube.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是使用 Minikube，将 `minikube` 替换为你节点的一个名称。
- en: '|  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The DaemonSet should have created one pod now. Let’s see:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 现在应该创建了一个 pod。让我们看看：
- en: '`$ kubectl get po` `NAME                READY     STATUS    RESTARTS   AGE
    ssd-monitor-hgxwq   1/1       Running   0          35s`'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                READY     STATUS    RESTARTS   AGE
    ssd-monitor-hgxwq   1/1       Running   0          35s`'
- en: Okay; so far so good. If you have multiple nodes and you add the same label
    to further nodes, you’ll see the DaemonSet spin up pods for each of them.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧；到目前为止一切顺利。如果你有多个节点，并且你给更多的节点添加相同的标签，你会看到 DaemonSet 会为每个节点启动 pod。
- en: Removing the required label from the node
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 从节点移除所需的标签
- en: Now, imagine you’ve made a mistake and have mislabeled one of the nodes. It
    has a spinning disk drive, not an SSD. What happens if you change the node’s label?
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你犯了一个错误，错误地标记了一个节点。它有一个旋转的磁盘驱动器，而不是 SSD。如果你更改节点的标签会发生什么？
- en: '`$ kubectl label node minikube disk=hdd --overwrite` `node "minikube" labeled`'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl label node minikube disk=hdd --overwrite` `node "minikube" labeled`'
- en: 'Let’s see if the change has any effect on the pod that was running on that
    node:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个变化是否对那个节点上运行的 pod 有任何影响：
- en: '`$ kubectl get po` `NAME                READY     STATUS        RESTARTS  
    AGE ssd-monitor-hgxwq   1/1       Terminating   0          4m`'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                READY     STATUS        RESTARTS  
    AGE ssd-monitor-hgxwq   1/1       Terminating   0          4m`'
- en: The pod is being terminated. But you knew that was going to happen, right? This
    wraps up your exploration of DaemonSets, so you may want to delete your `ssd-monitor`
    DaemonSet. If you still have any other daemon pods running, you’ll see that deleting
    the DaemonSet deletes those pods as well.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 正在终止。但你知道这是要发生的，对吧？这标志着你对 DaemonSet 的探索结束，所以你可能想要删除你的 `ssd-monitor` DaemonSet。如果你还有其他正在运行的
    daemon pod，你会看到删除 DaemonSet 也会删除那些 pod。
- en: 4.5\. Running pods that perform a single completable task
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5\. 运行执行单个可完成任务的 pod
- en: Up to now, we’ve only talked about pods than need to run continuously. You’ll
    have cases where you only want to run a task that terminates after completing
    its work. ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks
    that are never considered completed. Processes in such pods are restarted when
    they exit. But in a completable task, after its process terminates, it should
    not be restarted again.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了需要持续运行的 pod。你会有一些只想运行在完成工作后终止的任务的情况。ReplicationControllers、ReplicaSets
    和 DaemonSets 运行的是持续任务，这些任务永远不会被认为已完成。此类 pod 中的进程在退出时会被重启。但在可完成的任务中，在其进程终止后，它不应该再次重启。
- en: 4.5.1\. Introducing the Job resource
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5.1\. 介绍 Job 资源
- en: Kubernetes includes support for this through the Job resource, which is similar
    to the other resources we’ve discussed in this chapter, but it allows you to run
    a pod whose container isn’t restarted when the process running inside finishes
    successfully. Once it does, the pod is considered complete.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通过作业资源支持这一点，这与我们在本章中讨论的其他资源类似，但它允许你运行一个容器，当运行在其中的进程成功完成后，该容器不会被重启。一旦完成，该
    Pod 就被认为是完成了。
- en: In the event of a node failure, the pods on that node that are managed by a
    Job will be rescheduled to other nodes the way ReplicaSet pods are. In the event
    of a failure of the process itself (when the process returns an error exit code),
    the Job can be configured to either restart the container or not.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点故障的情况下，由作业管理的该节点上的 Pod 将像 ReplicaSet Pod 一样重新调度到其他节点。如果进程本身发生故障（当进程返回错误退出代码时），可以配置作业是重启容器还是不重启。
- en: '[Figure 4.10](#filepos448956) shows how a pod created by a Job is rescheduled
    to a new node if the node it was initially scheduled to fails. The figure also
    shows both a managed pod, which isn’t rescheduled, and a pod backed by a ReplicaSet,
    which is.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.10](#filepos448956) 展示了如果最初调度的节点失败，由作业创建的 Pod 将如何重新调度到新的节点。该图还显示了两种类型的
    Pod：一种是由作业管理的 Pod，它不会被重新调度；另一种是由 ReplicaSet 支持的 Pod，它会被重新调度。'
- en: Figure 4.10\. Pods managed by Jobs are rescheduled until they finish successfully.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10\. 由作业管理的 Pod 将在成功完成之前重新调度。
- en: '![](images/00146.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00146.jpg)'
- en: For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task
    finishes properly. You could run the task in an unmanaged pod and wait for it
    to finish, but in the event of a node failing or the pod being evicted from the
    node while it is performing its task, you’d need to manually recreate it. Doing
    this manually doesn’t make sense—especially if the job takes hours to complete.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作业对于临时任务很有用，在这些任务中，任务正确完成至关重要。你可以在未管理的 Pod 中运行任务并等待其完成，但在节点故障或 Pod 在执行任务时被从节点驱逐的情况下，你需要手动重新创建它。手动这样做没有意义——特别是如果作业需要数小时才能完成。
- en: An example of such a job would be if you had data stored somewhere and you needed
    to transform and export it somewhere. You’re going to emulate this by running
    a container image built on top of the `busybox` image, which invokes the `sleep`
    command for two minutes. I’ve already built the image and pushed it to Docker
    Hub, but you can peek into its Dockerfile in the book’s code archive.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的作业的一个例子是，如果你在某处存储了数据，你需要将其转换并导出到某处。你将通过运行基于 `busybox` 图像构建的容器镜像来模拟这个过程，该镜像将执行两分钟的
    `sleep` 命令。我已经构建了镜像并将其推送到 Docker Hub，但你可以在本书的代码存档中查看其 Dockerfile。
- en: 4.5.2\. Defining a Job resource
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5.2\. 定义作业资源
- en: Create the Job manifest as in the following listing.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下列表创建作业清单。
- en: 'Listing 4.11\. A YAML definition of a Job: exporter.yaml'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.11\. 作业的 YAML 定义：exporter.yaml
- en: '`apiVersion: batch/v1` `1` `kind: Job` `1` `metadata:   name: batch-job spec:`
    `2` `template:     metadata:       labels:` `2` `app: batch-job` `2` `spec:      
    restartPolicy: OnFailure` `3` `containers:       - name: main         image: luksa/batch-job`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: batch/v1` `1` `kind: Job` `1` `metadata:   name: batch-job spec:`
    `2` `template:     metadata:       labels:` `2` `app: batch-job` `2` `spec:      
    restartPolicy: OnFailure` `3` `containers:       - name: main         image: luksa/batch-job`'
- en: 1 Jobs are in the batch API group, version v1.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 作业位于 batch API 组，版本 v1。
- en: 2 You’re not specifying a pod selector (it will be created based on the labels
    in the pod template).
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你没有指定 Pod 选择器（它将基于 Pod 模板中的标签创建）。
- en: 3 Jobs can’t use the default restart policy, which is Always.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 作业不能使用默认的重启策略，即 Always。
- en: Jobs are part of the `batch` API group and `v1` API version. The YAML defines
    a resource of type Job that will run the `luksa/batch-job` image, which invokes
    a process that runs for exactly 120 seconds and then exits.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 作业是 `batch` API 组和 `v1` API 版本的一部分。YAML 定义了一个类型为 Job 的资源，该资源将运行 `luksa/batch-job`
    镜像，该镜像调用一个运行恰好 120 秒然后退出的进程。
- en: In a pod’s specification, you can specify what Kubernetes should do when the
    processes running in the container finish. This is done through the `restartPolicy`
    pod spec property, which defaults to `Always`. Job pods can’t use the default
    policy, because they’re not meant to run indefinitely. Therefore, you need to
    explicitly set the restart policy to either `OnFailure` or `Never`. This setting
    is what prevents the container from being restarted when it finishes (not the
    fact that the pod is being managed by a Job resource).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pod的规范中，你可以指定当容器中运行的进程结束时Kubernetes应该做什么。这是通过`restartPolicy` Pod规范属性完成的，默认为`Always`。作业Pod不能使用默认策略，因为它们不是无限期运行的。因此，你需要明确地将重启策略设置为`OnFailure`或`Never`。这个设置是防止容器在完成时重启的原因（而不是Pod由作业资源管理）。
- en: 4.5.3\. Seeing a Job run a pod
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5.3\. 观察作业运行Pod
- en: 'After you create this Job with the `kubectl create` command, you should see
    it start up a pod immediately:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create`命令创建此作业后，你应该立即看到它启动一个Pod：
- en: '`$ kubectl get jobs` `NAME        DESIRED   SUCCESSFUL   AGE batch-job   1        
    0            2s` `$ kubectl get po` `NAME              READY     STATUS    RESTARTS  
    AGE batch-job-28qf4   1/1       Running   0          4s`'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get jobs` `NAME        DESIRED   SUCCESSFUL   AGE batch-job   1        
    0            2s` `$ kubectl get po` `NAME              READY     STATUS    RESTARTS  
    AGE batch-job-28qf4   1/1       Running   0          4s`'
- en: 'After the two minutes have passed, the pod will no longer show up in the pod
    list and the Job will be marked as completed. By default, completed pods aren’t
    shown when you list pods, unless you use the `--show-all` (or `-a)` switch:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 经过两分钟后，Pod将不再出现在Pod列表中，作业将被标记为完成。默认情况下，完成后的Pod在列出Pod时不会显示，除非你使用`--show-all`（或`-a`）开关：
- en: '`$ kubectl get po -a` `NAME              READY     STATUS      RESTARTS   AGE
    batch-job-28qf4   0/1       Completed   0          2m`'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -a` `NAME              READY     STATUS      RESTARTS   AGE
    batch-job-28qf4   0/1       Completed   0          2m`'
- en: 'The reason the pod isn’t deleted when it completes is to allow you to examine
    its logs; for example:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Pod在完成时不会被删除的原因是允许你检查其日志；例如：
- en: '`$ kubectl logs batch-job-28qf4` `Fri Apr 29 09:58:22 UTC 2016 Batch job starting
    Fri Apr 29 10:00:22 UTC 2016 Finished succesfully`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl logs batch-job-28qf4` `Fri Apr 29 09:58:22 UTC 2016 Batch job starting
    Fri Apr 29 10:00:22 UTC 2016 Finished successfully`'
- en: 'The pod will be deleted when you delete it or the Job that created it. Before
    you do that, let’s look at the Job resource again:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当你删除它或创建它的作业时，Pod将被删除。在你这样做之前，让我们再次看看作业资源：
- en: '`$ kubectl get job` `NAME        DESIRED   SUCCESSFUL   AGE batch-job   1        
    1            9m`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get job` `NAME        DESIRED   SUCCESSFUL   AGE batch-job   1        
    1            9m`'
- en: The Job is shown as having completed successfully. But why is that piece of
    information shown as a number instead of as `yes` or `true`? And what does the
    `DESIRED` column indicate?
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 作业显示为成功完成。但为什么这条信息以数字的形式显示，而不是`yes`或`true`？`DESIRED`列表示什么？
- en: 4.5.4\. Running multiple pod instances in a Job
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5.4\. 在作业中运行多个Pod实例
- en: Jobs may be configured to create more than one pod instance and run them in
    parallel or sequentially. This is done by setting the `completions` and the `parallelism`
    properties in the Job spec.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 作业可以被配置为创建多个Pod实例，并可以并行或顺序地运行它们。这是通过在作业规范中设置`completions`和`parallelism`属性来完成的。
- en: Running job pods sequentially
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序运行作业Pod
- en: If you need a Job to run more than once, you set `completions` to how many times
    you want the Job’s pod to run. The following listing shows an example.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要作业运行多次，你可以将`completions`设置为作业Pod需要运行的次数。以下列表显示了一个示例。
- en: 'Listing 4.12\. A Job requiring multiple completions: multi-completion-batch-job.yaml'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.12\. 需要多次完成的作业：multi-completion-batch-job.yaml
- en: '`apiVersion: batch/v1 kind: Job metadata:   name: multi-completion-batch-job
    spec:   completions: 5` `1` `template:     <template is the same as in listing
    4.11>`'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: batch/v1 kind: Job metadata:   name: multi-completion-batch-job
    spec:   completions: 5` `1` `template:     <template is the same as in listing
    4.11>`'
- en: 1 Setting completions to 5 makes this Job run five pods sequentially.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 将完成数设置为5使得此作业顺序运行五个Pod。
- en: This Job will run five pods one after the other. It initially creates one pod,
    and when the pod’s container finishes, it creates the second pod, and so on, until
    five pods complete successfully. If one of the pods fails, the Job creates a new
    pod, so the Job may create more than five pods overall.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 此作业将依次运行五个Pod。它最初创建一个Pod，当Pod的容器完成时，它创建第二个Pod，依此类推，直到五个Pod成功完成。如果一个Pod失败，作业将创建一个新的Pod，因此作业总共可能创建超过五个Pod。
- en: Running job pods in parallel
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 并行运行作业Pod
- en: Instead of running single Job pods one after the other, you can also make the
    Job run multiple pods in parallel. You specify how many pods are allowed to run
    in parallel with the `parallelism` Job spec property, as shown in the following
    listing.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 除了依次运行单个作业Pod之外，你还可以让作业并行运行多个Pod。你可以通过`parallelism`作业规范属性指定允许并行运行的Pod数量，如下所示。
- en: 'Listing 4.13\. Running Job pods in parallel: multi-completion-parallel-batch-job.yaml'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.13\. 并行运行作业Pod：multi-completion-parallel-batch-job.yaml
- en: '`apiVersion: batch/v1 kind: Job metadata:   name: multi-completion-batch-job
    spec:   completions: 5` `1` `parallelism: 2` `2` `template:     <same as in listing
    4.11>`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: batch/v1 kind: Job metadata:   name: multi-completion-batch-job
    spec:   completions: 5` `1` `parallelism: 2` `2` `template:     <same as in listing
    4.11>`'
- en: 1 This job must ensure five pods complete successfully.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这个作业必须确保五个Pod成功完成。
- en: 2 Up to two pods can run in parallel.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 最多可以并行运行两个Pod。
- en: 'By setting `parallelism` to 2, the Job creates two pods and runs them in parallel:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`parallelism`设置为2，作业会创建两个Pod并在并行运行：
- en: '`$ kubectl get po` `NAME                               READY   STATUS     RESTARTS  
    AGE multi-completion-batch-job-lmmnk   1/1     Running    0          21s multi-completion-batch-job-qx4nq  
    1/1     Running    0          21s`'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                               READY   STATUS     RESTARTS  
    AGE multi-completion-batch-job-lmmnk   1/1     Running    0          21s multi-completion-batch-job-qx4nq  
    1/1     Running    0          21s`'
- en: As soon as one of them finishes, the Job will run the next pod, until five pods
    finish successfully.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦其中一个完成，作业将运行下一个Pod，直到五个Pod成功完成。
- en: Scaling a Job
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展作业
- en: 'You can even change a Job’s `parallelism` property while the Job is running.
    This is similar to scaling a ReplicaSet or ReplicationController, and can be done
    with the `kubectl scale` command:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以在作业运行时更改作业的`parallelism`属性。这类似于扩展ReplicaSet或ReplicationController，可以使用`kubectl
    scale`命令来完成：
- en: '`$ kubectl scale job multi-completion-batch-job --replicas 3` `job "multi-completion-batch-job"
    scaled`'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl scale job multi-completion-batch-job --replicas 3` `job "multi-completion-batch-job"
    scaled`'
- en: Because you’ve increased `parallelism` from 2 to 3, another pod is immediately
    spun up, so three pods are now running.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你已经将`parallelism`从2增加到3，因此立即启动了另一个Pod，现在有三个Pod正在运行。
- en: 4.5.5\. Limiting the time allowed for a Job pod to complete
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5.5\. 限制作业Pod完成所需的时间
- en: We need to discuss one final thing about Jobs. How long should the Job wait
    for a pod to finish? What if the pod gets stuck and can’t finish at all (or it
    can’t finish fast enough)?
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要讨论关于作业的最后一件事。作业应该等待Pod完成多长时间？如果Pod卡住并且根本无法完成（或者无法足够快地完成）怎么办？
- en: A pod’s time can be limited by setting the `activeDeadlineSeconds` property
    in the pod spec. If the pod runs longer than that, the system will try to terminate
    it and will mark the Job as failed.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在Pod规范中设置`activeDeadlineSeconds`属性来限制Pod的运行时间。如果Pod运行时间超过这个时间，系统将尝试终止它，并将作业标记为失败。
- en: '|  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can configure how many times a Job can be retried before it is marked as
    failed by specifying the `spec.backoffLimit` field in the Job manifest. If you
    don’t explicitly specify it, it defaults to 6\.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在作业清单中指定`spec.backoffLimit`字段来配置作业在标记为失败之前可以重试的次数。如果你没有明确指定它，则默认为6。
- en: '|  |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 4.6\. Scheduling Jobs to run periodically or once in the future
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6\. 定期或未来一次性运行作业的调度
- en: Job resources run their pods immediately when you create the Job resource. But
    many batch jobs need to be run at a specific time in the future or repeatedly
    in the specified interval. In Linux- and UNIX-like operating systems, these jobs
    are better known as cron jobs. Kubernetes supports them, too.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建作业资源时，作业资源会立即运行其Pod。但许多批处理作业需要在未来的某个特定时间运行，或者在指定的间隔内重复运行。在Linux和UNIX-like操作系统上，这些作业通常被称为cron作业。Kubernetes也支持它们。
- en: A cron job in Kubernetes is configured by creating a CronJob resource. The schedule
    for running the job is specified in the well-known cron format, so if you’re familiar
    with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter of
    seconds.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的cron作业通过创建CronJob资源进行配置。作业运行的计划是在众所周知的cron格式中指定的，所以如果你熟悉常规的cron作业，你将在几秒钟内理解Kubernetes的CronJobs。
- en: At the configured time, Kubernetes will create a Job resource according to the
    Job template configured in the CronJob object. When the Job resource is created,
    one or more pod replicas will be created and started according to the Job’s pod
    template, as you learned in the previous section. There’s nothing more to it.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置的时间，Kubernetes将根据CronJob对象中配置的工作模板创建一个工作资源。当工作资源被创建时，根据工作Pod模板，将创建一个或多个Pod副本并启动，正如你在上一节中学到的。除此之外没有其他的事情。
- en: Let’s look at how to create CronJobs.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建CronJob。
- en: 4.6.1\. Creating a CronJob
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6.1\. 创建CronJob
- en: Imagine you need to run the batch job from your previous example every 15 minutes.
    To do that, create a CronJob resource with the following specification.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要每15分钟运行一次你之前示例中的批处理作业。为此，创建一个具有以下规范的CronJob资源。
- en: 'Listing 4.14\. YAML for a CronJob resource: cronjob.yaml'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.14\. CronJob资源YAML：cronjob.yaml
- en: '`apiVersion: batch/v1beta1` `1` `kind: CronJob metadata:   name: batch-job-every-fifteen-minutes
    spec:   schedule: "0,15,30,45 * * * *"` `2` `jobTemplate:     spec:       template:`
    `3` `metadata:` `3` `labels:` `3` `app: periodic-batch-job` `3` `spec:` `3` `restartPolicy:
    OnFailure` `3` `containers:` `3` `- name: main` `3` `image: luksa/batch-job` `3`'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: batch/v1beta1` `1` `kind: CronJob metadata: ` `name: batch-job-every-fifteen-minutes
    spec: ` `schedule: "0,15,30,45 * * * *"` `2` `jobTemplate: `   `spec: `   `template:`
    `3` `metadata:` `3` `labels:` `3` `app: periodic-batch-job` `3` `spec:` `3` `restartPolicy:
    OnFailure` `3` `containers:` `3` `- name: main` `3` `image: luksa/batch-job` `3`'
- en: 1 API group is batch, version is v1beta1
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 API组是批处理，版本是v1beta1
- en: 2 This job should run at the 0, 15, 30 and 45 minutes of every hour, every day.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这个工作应该在每小时的第0分钟、第15分钟、第30分钟和第45分钟运行。
- en: 3 The template for the Job resources that will be created by this CronJob
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 由这个CronJob创建的工作资源模板
- en: As you can see, it’s not too complicated. You’ve specified a schedule and a
    template from which the Job objects will be created.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这并不复杂。你已经指定了一个日程安排和一个模板，从这个模板中将会创建工作对象。
- en: Configuring the schedule
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 配置日程
- en: 'If you’re unfamiliar with the cron schedule format, you’ll find great tutorials
    and explanations online, but as a quick introduction, from left to right, the
    schedule contains the following five entries:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉cron日程格式，你可以在网上找到很好的教程和解释，但作为一个快速介绍，从左到右，日程包含以下五个条目：
- en: Minute
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分钟
- en: Hour
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时
- en: Day of month
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份中的某一天
- en: Month
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份
- en: Day of week.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期中的某一天。
- en: In the example, you want to run the job every 15 minutes, so the schedule needs
    to be `"0,15,30,45 * * * *"`, which means at the 0, 15, 30 and 45 minutes mark
    of every hour (first asterisk), of every day of the month (second asterisk), of
    every month (third asterisk) and on every day of the week (fourth asterisk).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，你希望每15分钟运行一次工作，所以日程需要设置为`"0,15,30,45 * * * *"`，这意味着在每个小时的第0分钟、第15分钟、第30分钟和第45分钟（第一个星号），每月的每一天（第二个星号），每月的每一天（第三个星号）以及星期的每一天（第四个星号）。
- en: If, instead, you wanted it to run every 30 minutes, but only on the first day
    of the month, you’d set the schedule to `"0,30 * 1 * *"`, and if you want it to
    run at 3AM every Sunday, you’d set it to `"0 3 * * 0"` (the last zero stands for
    Sunday).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望它每30分钟运行一次，但只在每月的第一天运行，你可以将日程设置为`"0,30 * 1 *"`，如果你想它在每周日早上3点运行，你可以设置为`"0
    3 * * 0"`（最后的零代表周日）。
- en: Configuring the Job template
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 配置工作模板
- en: A CronJob creates Job resources from the `jobTemplate` property configured in
    the CronJob spec, so refer to [section 4.5](index_split_043.html#filepos447333)
    for more information on how to configure it.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob从CronJob规范中配置的`jobTemplate`属性创建工作资源，因此请参阅[第4.5节](index_split_043.html#filepos447333)以获取有关如何配置的更多信息。
- en: 4.6.2\. Understanding how scheduled jobs are run
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6.2\. 理解预定作业的运行方式
- en: Job resources will be created from the CronJob resource at approximately the
    scheduled time. The Job then creates the pods.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 工作资源将在大约预定时间从CronJob资源创建。然后工作会创建Pod。
- en: It may happen that the Job or pod is created and run relatively late. You may
    have a hard requirement for the job to not be started too far over the scheduled
    time. In that case, you can specify a deadline by specifying the `startingDeadlineSeconds`
    field in the CronJob specification as shown in the following listing.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现工作或Pod创建和运行相对较晚的情况。你可能对工作开始时间有严格的要求，不能超过预定时间太远。在这种情况下，你可以通过在CronJob规范中指定`startingDeadlineSeconds`字段来设置一个截止日期，如下面的列表所示。
- en: Listing 4.15\. Specifying a `startingDeadlineSeconds` for a CronJob
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.15\. 为CronJob指定`startingDeadlineSeconds`
- en: '`apiVersion: batch/v1beta1 kind: CronJob spec:   schedule: "0,15,30,45 * *
    * *"   startingDeadlineSeconds: 15` `1` `...`'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: batch/v1beta1 kind: CronJob spec: schedule: "0,15,30,45 * * *
    *" startingDeadlineSeconds: 15` `1` `...`'
- en: 1 At the latest, the pod must start running at 15 seconds past the scheduled
    time.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1. 最晚，Pod 必须在预定时间后的 15 秒开始运行。
- en: In the example in [listing 4.15](#filepos465619), one of the times the job is
    supposed to run is 10:30:00\. If it doesn’t start by 10:30:15 for whatever reason,
    the job will not run and will be shown as Failed.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 4.15](#filepos465619)的示例中，作业应该运行的时间之一是 10:30:00。如果由于任何原因在 10:30:15 之前没有启动，作业将不会运行，并显示为失败。
- en: In normal circumstances, a CronJob always creates only a single Job for each
    execution configured in the schedule, but it may happen that two Jobs are created
    at the same time, or none at all. To combat the first problem, your jobs should
    be idempotent (running them multiple times instead of once shouldn’t lead to unwanted
    results). For the second problem, make sure that the next job run performs any
    work that should have been done by the previous (missed) run.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，CronJob 总是为调度中配置的每次执行创建单个作业，但有时可能会同时创建两个作业，或者一个都不创建。为了解决第一个问题，你的作业应该是幂等的（多次运行而不是单次运行不应导致不希望的结果）。对于第二个问题，确保下一个作业运行执行之前（错过）运行应该完成的所有工作。
- en: 4.7\. Summary
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 4.7. 摘要
- en: You’ve now learned how to keep pods running and have them rescheduled in the
    event of node failures. You should now know that
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经学会了如何在节点故障的情况下保持 Pod 运行并重新调度。你应该现在知道
- en: You can specify a liveness probe to have Kubernetes restart your container as
    soon as it’s no longer healthy (where the app defines what’s considered healthy).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以指定一个存活探针，以便 Kubernetes 在容器不再健康时立即重启它（其中应用程序定义了什么被认为是健康的）。
- en: Pods shouldn’t be created directly, because they will not be re-created if they’re
    deleted by mistake, if the node they’re running on fails, or if they’re evicted
    from the node.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 不应该直接创建，因为如果它们被错误删除、运行它们的节点失败或从节点中驱逐，它们将不会被重新创建。
- en: ReplicationControllers always keep the desired number of pod replicas running.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationControllers 总是保持所需数量的 Pod 副本运行。
- en: Scaling pods horizontally is as easy as changing the desired replica count on
    a ReplicationController.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平扩展 Pod 与在 ReplicationController 上更改所需副本计数一样简单。
- en: Pods aren’t owned by the ReplicationControllers and can be moved between them
    if necessary.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 不属于 ReplicationControllers，如果需要，可以在它们之间移动。
- en: A ReplicationController creates new pods from a pod template. Changing the template
    has no effect on existing pods.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationController 从 Pod 模板创建新的 Pod。更改模板对现有 Pod 没有影响。
- en: ReplicationControllers should be replaced with ReplicaSets and Deployments,
    which provide the same functionality, but with additional powerful features.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该用 ReplicaSets 和 Deployments 替换 ReplicationControllers，它们提供相同的功能，但具有额外的强大功能。
- en: ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,
    whereas DaemonSets make sure every node runs a single instance of a pod defined
    in the DaemonSet.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationControllers 和 ReplicaSets 将 Pod 调度到集群的随机节点，而 DaemonSets 确保每个节点运行一个在
    DaemonSet 中定义的 Pod 的单个实例。
- en: Pods that perform a batch task should be created through a Kubernetes Job resource,
    not directly or through a ReplicationController or similar object.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行批处理任务的 Pod 应该通过 Kubernetes 作业资源创建，而不是直接创建或通过 ReplicationController 或类似对象创建。
- en: Jobs that need to run sometime in the future can be created through CronJob
    resources.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在未来某个时间运行的作业可以通过 CronJob 资源创建。
