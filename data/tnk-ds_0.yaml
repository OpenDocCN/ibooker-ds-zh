- en: Part 1\. Preparing and gathering data and knowledge
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分\. 准备和收集数据和知识
- en: The process of data science begins with preparation. You need to establish what
    you know, what you have, what you can get, where you are, and where you would
    like to be. This last one is of utmost importance; a project in data science needs
    to have a purpose and corresponding goals. Only when you have well-defined goals
    can you begin to survey the available resources and all the possibilities for
    moving toward those goals.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的过程始于准备阶段。你需要确定你所知道的，你所拥有的，你可以得到的，你目前的位置，以及你希望达到的位置。最后这一点至关重要；数据科学项目需要有一个目的和相应的目标。只有当你有明确的目标时，你才能开始调查可用的资源以及实现这些目标的所有可能性。
- en: '[Part 1](#part01) of this book begins with a chapter discussing my process-oriented
    perspective of data science projects. After that, we move along to the deliberate
    and important step of setting good goals for the project. The subsequent three
    chapters cover the three most important data-centric steps of the process: exploration,
    wrangling, and assessment. At the end of this part, you’ll be intimately familiar
    with the data you have and relevant data you can get. More important, you’ll know
    if and how it can help you achieve the goals of the project.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[本书的第一部分](#part01)从讨论我的以过程为导向的数据科学项目视角的章节开始。之后，我们继续进行一个重要且故意的步骤，即为项目设定良好的目标。接下来的三个章节涵盖了过程中最重要三个以数据为中心的步骤：探索、整理和评估。在这一部分的最后，你将熟悉你拥有的数据和你可以获取的相关数据。更重要的是，你会知道它是否以及如何帮助你实现项目的目标。'
- en: Chapter 1\. Philosophies of data science
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章\. 数据科学的哲学
- en: '*This chapter covers*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: The role of a data scientist and how it’s different from that of a software
    developer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家的角色以及它与软件开发者的角色有何不同
- en: The greatest asset of a data scientist, awareness, particularly in the presence
    of significant uncertainties
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家最大的资产是意识，尤其是在存在重大不确定性的情况下
- en: 'Prerequisites for reading this book: basic knowledge of software development
    and statistics'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读此书的前提：软件开发和统计学的基本知识
- en: Setting priorities for a project while keeping the big picture in mind
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在考虑大局的同时为项目设定优先级
- en: 'Best practices: tips that can make life easier during a project'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳实践：在项目期间可以使生活更轻松的技巧
- en: 'In the following pages, I introduce data science as a set of processes and
    concepts that act as a guide for making progress and decisions within a data-centric
    project. This contrasts with the view of data science as a set of statistical
    and software tools and the knowledge to use them, which in my experience is the
    far more popular perspective taken in conversations and texts on data science
    (see [figure 1.1](kindle_split_010_split_000.xhtml#ch01fig01) for a humorous take
    on perspectives of data science). I don’t mean to say that these two perspectives
    contradict each other; they’re complementary. But to neglect one in favor of the
    other would be foolish, and so in this book I address the less-discussed side:
    process, both in practice and in thought.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几页中，我将数据科学介绍为一套过程和概念，这些过程和概念作为在以数据为中心的项目中进行进步和决策的指南。这与将数据科学视为一套统计和软件工具以及使用它们的知识的观点形成对比，在我的经验中，这种观点在关于数据科学的对话和文本中更为普遍（参见[图1.1](kindle_split_010_split_000.xhtml#ch01fig01)以幽默的方式看待数据科学的观点）。我并不是说这两种观点相互矛盾；它们是互补的。但是忽视其中之一而偏爱另一个将是愚蠢的，因此在这本书中，我讨论了较少讨论的方面：实践和思想中的过程。
- en: Figure 1.1\. Some stereotypical perspectives on data science
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1\. 对数据科学的某些典型观点
- en: '![](Images/01fig01_alt.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig01_alt.jpg)'
- en: 'To compare with carpentry, knowing how to use hammers, drills, and saws isn’t
    the same as knowing how to build a chair. Likewise, if you know the process of
    building a chair, that doesn’t mean you’re any good with the hammers, drills,
    and saws that might be used in the process. To build a good chair, you have to
    know how to use the tools as well as what, specifically, to do with them, step
    by step. Throughout this book, I try to discuss tools enough to establish an understanding
    of how they work, but I focus far more on when they should be used and how and
    why. I perpetually ask and answer the question: what should be done next?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与木工相比，知道如何使用锤子、钻头和锯子并不等同于知道如何制作一把椅子。同样，如果你知道制作椅子的过程，这并不意味着你在使用过程中可能会用到的锤子、钻头和锯子方面有任何优势。要制作一把好椅子，你必须知道如何使用这些工具，以及具体如何一步步地使用它们。在这本书中，我试图充分讨论工具，以便建立对它们如何工作的理解，但我更关注它们何时应该使用，以及如何和为什么使用。我不断地提出并回答一个问题：接下来应该做什么？
- en: In this chapter, using relatively high-level descriptions and examples, I discuss
    how the thought processes of a data scientist can be more important than the specific
    tools used and how certain concepts pervade nearly all aspects of work in data
    science.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，使用相对高级的描述和示例，我讨论了数据科学家的思维过程为什么可能比使用的具体工具更重要，以及某些概念几乎渗透到数据科学工作的所有方面。
- en: 1.1\. Data science and this book
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 数据科学与本书
- en: The origins of data science as a field of study or vocational pursuit lie somewhere
    between statistics and software development. Statistics can be thought of as the
    schematic drawing and software as the machine. Data flows through both, either
    conceptually or actually, and perhaps it was only in recent years that practitioners
    began to give data top billing, though data science owes much to any number of
    older fields that combine statistics and software, such as operations research,
    analytics, and decision science.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学作为一个研究领域或职业追求的起源，介于统计学和软件开发之间。统计学可以被视为蓝图，而软件则被视为机器。数据通过这两者流动，无论是概念上还是实际上，也许只是在最近几年，从业者才开始将数据置于首位，尽管数据科学在很大程度上受益于任何将统计学和软件结合在一起的老牌领域，如运筹学、分析和决策科学。
- en: In addition to statistics and software, many folks say that data science has
    a third major component, which is something along the lines of subject matter
    expertise or domain knowledge. Although it certainly is important to understand
    a problem before you try to solve it, a good data scientist can switch domains
    and begin contributing relatively soon. Just as a good accountant can quickly
    learn the financial nuances of a new industry, and a good engineer can pick up
    the specifics of designing various types of products, a good data scientist can
    switch to a completely new domain and begin to contribute within a short time.
    That is not to say that domain knowledge has little value, but compared to software
    development and statistics, domain-specific knowledge usually takes the least
    time to learn well enough to help solve problems involving data. It’s also the
    one interchangeable component of the three. If you can do data science, you can
    walk into a planning meeting for a brand-new data-centric project, and almost
    everyone else in the room will have the domain knowledge you need, whereas almost
    no one else will have the skills to write good analytic software that works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统计学和软件之外，许多人说数据科学还有一个第三大组成部分，那就是类似专业知识或领域知识的东西。虽然在你尝试解决问题之前理解问题确实很重要，但一个好的数据科学家可以转换领域，并相对很快地开始做出贡献。就像一个好的会计可以快速学习新行业的财务细微差别，一个好的工程师可以掌握设计各种类型产品的具体细节一样，一个好的数据科学家可以转换到一个全新的领域，并在短时间内开始做出贡献。这并不是说领域知识没有价值，但与软件开发和统计学相比，特定领域的知识通常需要最短的时间来学习，以便足够好地帮助解决涉及数据的问题。它也是三个组成部分中可以互换的组成部分。如果你能做数据科学，你可以走进一个全新的以数据为中心的项目规划会议，房间里几乎每个人都会有你需要的领域知识，而几乎没有人会拥有编写良好分析软件的技能，这种软件能够工作。
- en: Throughout this book—perhaps you’ve noticed already—I choose to use the term
    *data-centric* instead of the more popular *data-driven* when describing software,
    projects, and problems, because I find the idea of data *driving* any of these
    to be a misleading concept. Data should drive software only when that software
    is being built expressly for moving, storing, or otherwise handing the data. Software
    that’s intended to address project or business goals should not be driven by data.
    That would be putting the cart before the horse. Problems and goals exist independently
    of any data, software, or other resources, but those resources may serve to solve
    the problems and to achieve the goals. The term *data-centric* reflects that data
    is an integral part of the solution, and I believe that using it instead of *data-driven*
    admits that we need to view the problems not from the perspective of the data
    but from the perspective of the goals and problems that data can help us address.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中——也许你已经注意到了——我选择使用“以数据为中心”这个术语，而不是更流行的“以数据驱动”，来描述软件、项目和问题，因为我认为数据“驱动”任何这些概念是一个误导性的观点。只有当软件明确用于移动、存储或以其他方式处理数据时，数据才应该驱动软件。旨在解决项目或商业目标的软件不应该由数据驱动。那将是本末倒置。问题和目标独立于任何数据、软件或其他资源存在，但那些资源可以用来解决问题和实现目标。“以数据为中心”这个术语反映了数据是解决方案的一个组成部分，我相信使用它而不是“以数据驱动”承认了我们需要从目标和数据可以帮助我们解决的问题的角度来看待问题，而不是从数据的角度来看待问题。
- en: Such statements about proper perspective are common in this book. In every chapter
    I try to maintain the reader’s focus on the most important things, and in times
    of uncertainty about project outcomes, I try to give guidelines that help you
    decide which are the most important things. In some ways, I think that locating
    and maintaining focus on the most important aspects of a project is one of the
    most valuable skills that I attempt to instruct within these pages. Data scientists
    must have many hard skills—knowledge of software development and statistics among
    them—but I’ve found this soft skill of maintaining appropriate perspective and
    awareness of the many moving parts in any data-centric problem to be very difficult
    yet very rewarding for most data scientists I know.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，关于正确视角的陈述很常见。在每一章中，我都试图让读者关注最重要的事情，在项目结果不确定的时候，我会尝试提供一些指导方针，帮助你决定哪些是最重要的事情。从某种意义上说，我认为定位并保持对项目最重要方面的关注是我在这几页中试图教授的最有价值的技能之一。数据科学家必须具备许多硬技能——其中就包括软件开发和统计学知识——但我发现，对于大多数我所知道的数据科学家来说，保持适当的视角和对任何以数据为中心的问题中众多变动部分的意识是一种非常困难但非常有价值的软技能。
- en: Sometimes data quality becomes an important issue; sometimes the major issue
    is data volume, processing speed, parameters of an algorithm, interpretability
    of results, or any of the many other aspects of the problem. Ignoring any of these
    at the moment it becomes important can compromise or entirely invalidate subsequent
    results. As a data scientist, I have as my goal to make sure that no important
    aspect of a project goes awry unnoticed. When something goes wrong—and something
    will—I want to notice it so that I can fix it. Throughout this chapter and the
    entire book, I will continue to stress the importance of maintaining awareness
    of all aspects of a project, particularly those in which there is uncertainty
    about potential outcomes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据质量成为一个重要问题；有时主要问题是数据量、处理速度、算法参数、结果的可解释性，或者问题的许多其他方面。在这些问题变得重要时忽视任何一点都可能损害或完全无效化后续结果。作为一名数据科学家，我的目标是确保项目的任何重要方面都不会被忽视而出现问题。当出现问题——这是不可避免的——我希望能够注意到它，以便我可以修复它。在整个章节和整本书中，我将继续强调保持对项目所有方面的意识的重要性，尤其是那些关于潜在结果不确定性的方面。
- en: 'The lifecycle of a data science project can be divided into three phases, as
    illustrated in [figure 1.2](kindle_split_010_split_001.xhtml#ch01fig02). This
    book is organized around these phases. The first part covers preparation, emphasizing
    that a bit of time and effort spent gathering information at the beginning of
    the project can spare you from big headaches later. The second part covers building
    a product for the customer, from planning to execution, using what you’ve learned
    from the first section as well as all of the tools that statistics and software
    can provide. The third and final part covers finishing a project: delivering the
    product, getting feedback, making revisions, supporting the product, and wrapping
    up a project neatly. While discussing each phase, this book includes some self-reflection,
    in that it regularly asks you, the reader, to reconsider what you’ve done in previous
    steps, with the possibility of redoing them in some other way if it seems like
    a good idea. By the end of the book, you’ll hopefully have a firm grasp of these
    thought processes and considerations when making decisions as a data scientist
    who wants to use data to get valuable results.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目生命周期可以分为三个阶段，如图1.2[图1.2](kindle_split_010_split_001.xhtml#ch01fig02)所示。本书围绕这些阶段组织。第一部分涵盖准备阶段，强调在项目开始时花点时间和精力收集信息可以让你在以后避免大麻烦。第二部分涵盖为客户构建产品，从规划到执行，使用第一部分学到的知识以及统计学和软件可以提供的所有工具。第三部分和最后一部分涵盖完成项目：交付产品、获取反馈、进行修订、支持产品以及整洁地结束项目。在讨论每个阶段时，本书包括一些自我反思，即它经常要求你，作为读者，重新考虑你在之前步骤中做过的事情，如果看起来是个好主意，就有可能以某种其他方式重新做。到本书结束时，你可能会希望对这些思考过程和作为希望使用数据获得有价值结果的科学家在做出决策时的考虑有一个牢固的把握。
- en: Figure 1.2\. The data science process
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2. 数据科学过程
- en: '![](Images/01fig02_alt.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2](Images/01fig02_alt.jpg)'
- en: 1.2\. Awareness is valuable
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 意识到这一点很有价值
- en: If I had a dollar for every time a software developer told me that an analytic
    software tool “doesn’t work,” I’d be a wealthy man. That’s not to say that I think
    all analytic software tools work well or at all—that most certainly is not the
    case—but I think it motivates a discussion of one of the most pervasive discrepancies
    between the perspective of a data scientist and that of what I would call a “pure”
    software developer—one who doesn’t normally interact with raw or “unwrangled”
    data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每次有软件开发者告诉我一个分析软件工具“不起作用”，我都会有一笔财富。这并不是说我认为所有的分析软件工具都工作得很好或根本不起作用——这绝对不是事实——但我认为这促使我们讨论数据科学家和所谓的“纯”软件开发者（通常不与原始或“未整理”的数据互动）之间最普遍的差异之一。
- en: 'A good example of this discrepancy occurred when a budding startup founder
    approached me with a problem he was having. The task was to extract names, places,
    dates, and other key information from emails related to upcoming travel so that
    this data could be used in a mobile application that would keep track of the user’s
    travel plans. The problem the founder was having is a common one: emails and other
    documents come in all shapes and sizes, and parsing them for useful information
    is a challenge. It’s difficult to extract this specific travel-related data when
    emails from different airlines, hotels, booking websites, and so on have different
    formats, not to mention that these formats change quite frequently. Google and
    others seem to have good tools for extracting such data within their own apps,
    but these tools generally aren’t made available to external developers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异的一个很好的例子发生在一个初出茅庐的创业公司创始人向我提出一个问题时。任务是提取与即将到来的旅行相关的电子邮件中的姓名、地点、日期和其他关键信息，以便这些数据可以用于一个跟踪用户旅行计划的移动应用程序。创始人遇到的问题很常见：电子邮件和其他文档形式多样，大小不一，解析它们以获取有用信息是一个挑战。当来自不同航空公司、酒店、预订网站等的电子邮件格式不同时，提取这些特定的与旅行相关的数据就更加困难，更不用说这些格式经常发生变化。谷歌和其他公司似乎在自己的应用程序中提供了很好的提取此类数据的工具，但这些工具通常不会对外部开发者开放。
- en: 'Both the founder and I were aware that there are, as usual, two main strategies
    for addressing this challenge: manual brute force and scripting. We could also
    use some mixture of the two. Given that brute force would entail creating a template
    for each email format as well as a new template every time the format changed,
    neither of us wanted to follow that path. A script that could parse any email
    and extract the relevant information sounded great, but it also sounded extremely
    complex and almost impossible to write. A compromise between the two extreme approaches
    seemed best, as it usually does.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 创始人和我都知道，通常有两大策略可以应对这个挑战：手动暴力破解和脚本编写。我们也可以将这两种方法结合起来使用。鉴于暴力破解需要为每种电子邮件格式创建一个模板，以及每次格式更改时都需要一个新的模板，我们都不想走这条路。一个能够解析任何电子邮件并提取相关信息脚本听起来很棒，但它听起来也非常复杂，几乎不可能编写。两种极端方法之间的折衷方案似乎是最好的，正如它通常所做的那样。
- en: 'While speaking with both the founder and the lead software developer, I suggested
    that they forge a compromise between brute force and pure scripting: develop some
    simple templates for the most common formats, check for similarities and common
    structural patterns, and then write a simple script that could match chunks of
    familiar template HTML or text within new emails and extract data from known positions
    within those chunks. I called this *algorithmic templating* at the time, for better
    or for worse. This suggestion obviously wouldn’t solve the problem entirely, but
    it would make some progress in the right direction, and, more importantly, it
    would give some insight into the common structural patterns within the most common
    formats and highlight specific challenges that were yet unknown but possibly easy
    to solve.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在与创始人和主要软件开发者交谈时，我建议他们在暴力破解和纯脚本编写之间找到一个折衷方案：为最常见的格式开发一些简单的模板，检查相似性和常见的结构模式，然后编写一个简单的脚本，该脚本可以匹配新电子邮件中熟悉的模板HTML或文本的块，并从这些块中提取已知位置的数据。我当时称之为*算法模板化*，不管好坏。这个建议显然不能完全解决问题，但它会在正确的方向上取得一些进展，更重要的是，它将揭示最常见的格式中的常见结构模式，并突出一些尚未知晓但可能容易解决的特定挑战。
- en: The software developer mentioned that he had begun building a solution using
    a popular tool for natural language processing (NLP) that could recognize and
    extract dates, names, and places. He then said that he still thought the NLP tool
    would solve the problem and that he would let me know after he had implemented
    it fully. I told him that natural language is notoriously tricky to parse and
    analyze and that I had less confidence in NLP tools than he did but I hoped he
    was right.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 软件开发人员提到，他已经开始使用一种流行的自然语言处理（NLP）工具构建解决方案，该工具能够识别和提取日期、姓名和地点。然后他说，他仍然认为NLP工具能够解决这个问题，并在完全实施后告诉我。我告诉他，自然语言解析和分析非常复杂，我对NLP工具的信心不如他那么高，但我希望他是正确的。
- en: A couple of weeks later, I spoke again with the founder and the software developer,
    was told that the NLP tool didn’t work, and was asked again for help. The NLP
    tool could recognize most dates and locations, but, to paraphrase one issue, “Most
    of the time, in emails concerning flight reservations, the booking date appears
    first in the email, then the departure date, the arrival date, and then possibly
    the dates for the return flight. But in some HTML email formats, the booking date
    appears between the departure and arrival dates. What should we do then?”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 几周后，我又与创始人以及软件开发者进行了交谈，被告知自然语言处理（NLP）工具无法正常工作，并再次请求他们的帮助。该NLP工具能够识别大多数日期和地点，但，为了转述一个问题，"大多数情况下，在涉及航班预订的电子邮件中，预订日期通常出现在邮件的第一位，然后是出发日期，到达日期，然后可能是返程日期。但在某些HTML电子邮件格式中，预订日期出现在出发日期和到达日期之间。那么我们该怎么办呢？"
- en: That the NLP tool doesn’t work to solve 100% of the problem is clear. But it
    did solve some intermediate problems, such as recognizing names and dates, even
    if it couldn’t place them precisely within the travel plan itself. I don’t want
    to stretch the developer’s words or take them out of context; this is a tough
    problem for data scientists and a *very* tough problem for others. Failing to
    solve the problem on the first try is hardly a total failure. But this part of
    the project was stalled for a few weeks while the three of us tried to find an
    experienced data scientist with enough time to try to help overcome this specific
    problem. Such a delay is costly to a startup—or any company for that matter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: NLP工具无法解决100%的问题这一点是显而易见的。但它确实解决了某些中间问题，例如识别姓名和日期，即使它无法在旅行计划本身中精确地放置它们。我不想曲解开发者的原话或脱离上下文；这对数据科学家来说是一个难题，对其他人来说则是一个**非常**难题。第一次尝试未能解决问题几乎不能算作彻底失败。但在这个项目中，我们三个人尝试了几个星期，试图找到一个有足够时间来尝试帮助解决这个具体问题的经验丰富的数据科学家，这一部分工作因此停滞了几个星期。这样的延误对初创公司——或者任何公司来说——都是代价高昂的。
- en: The lesson I’ve learned through experiences like these is that awareness is
    incredibly valuable when working on problems involving data. A good developer
    using good tools to address what seems like a very tractable problem can run into
    trouble if they haven’t considered the many possibilities that can happen when
    code begins to process data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样的经历，我学到的教训是，在处理涉及数据的问题时，意识极其宝贵。一个优秀的开发者使用优秀的工具来解决看似非常容易解决的问题，如果他们没有考虑到代码开始处理数据时可能发生的许多可能性，他们可能会遇到麻烦。
- en: Uncertainty is an adversary of coldly logical algorithms, and being aware of
    how those algorithms might break down in unusual circumstances expedites the process
    of fixing problems when they occur—and they will occur. A data scientist’s main
    responsibility is to try to imagine all of the possibilities, address the ones
    that matter, and reevaluate them all as successes and failures happen. That is
    why—no matter how much code I write—awareness and familiarity with uncertainty
    are the most valuable things I can offer as a data scientist. Some people might
    tell you not to daydream at work, but an imagination can be a data scientist’s
    best friend if you can use it to prepare yourself for the certainty that something
    will go wrong.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性是冷静逻辑算法的对手，意识到这些算法在异常情况下可能会崩溃，可以加快在问题发生时解决问题的过程——它们会发生。数据科学家的主要责任是尝试想象所有可能性，解决那些重要的可能性，并在成功和失败发生时重新评估它们。这就是为什么——无论我写了多少代码——意识和对不确定性的熟悉是我作为数据科学家能提供的最有价值的东西。有些人可能会告诉你不要在工作时做白日梦，但如果你能利用它来为可能出错的事实做好准备，想象力可以成为数据科学家的最佳朋友。
- en: 1.3\. Developer vs. data scientist
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3. 开发者与数据科学家
- en: A good software developer (or engineer) and a good data scientist have several
    traits in common. Both are good at designing and building complex systems with
    many interconnected parts; both are familiar with many different tools and frameworks
    for building these systems; both are adept at foreseeing potential problems in
    those systems before they’re actualized. But in general, software developers design
    systems consisting of many well-defined components, whereas data scientists work
    with systems wherein at least one of the components isn’t well defined prior to
    being built, and that component is usually closely involved with data processing
    or analysis.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的软件开发者（或工程师）和优秀的数据科学家有几个共同点。两者都擅长设计和构建由许多相互连接的部分组成的复杂系统；两者都熟悉构建这些系统所需的各种工具和框架；两者都擅长在系统实现之前预见潜在的问题。但一般来说，软件开发者设计由许多定义良好的组件组成的系统，而数据科学家则与至少一个组件在构建之前没有明确定义的系统打交道，而这个组件通常与数据处理或分析密切相关。
- en: The systems of software developers and those of data scientists can be compared
    with the mathematical concepts of logic and probability, respectively. The logical
    statement “if A, then B” can be coded easily in any programming language, and
    in some sense every computer program consists of a very large number of such statements
    within various contexts. The probabilistic statement “if A, then *probably* B”
    isn’t nearly as straightforward. Any good data-centric application contains many
    such statements—consider the Google search engine (“These are *probably* the most
    relevant pages”), product recommendations on Amazon.com (“We *think* you’ll *probably*
    like these things”), website analytics (“Your site visitors are *probably* from
    North America and each views *about* three pages”).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 软件开发者和数据科学家的系统可以分别与数学概念中的逻辑和概率相对应。逻辑陈述“如果A，则B”可以很容易地用任何编程语言编码，从某种意义上说，每个计算机程序都包含大量这样的陈述，这些陈述在不同的上下文中。概率性陈述“如果A，则*可能*B”则远非那么直接。任何好的以数据为中心的应用都包含许多这样的陈述——考虑一下谷歌搜索引擎（“这些*可能*是最相关的页面”），亚马逊网站上的产品推荐（“我们认为你*可能*会喜欢这些商品”），网站分析（“你的网站访客*可能*来自北美，每个访客大约查看*三个页面”）。
- en: Data scientists specialize in creating systems that rely on probabilistic statements
    about data and results. In the previous case of a system that finds travel information
    within an email, we can make a statement such as “If we know the email contains
    a departure date, the NLP tool can *probably* extract it.” For a good NLP tool,
    with a little fiddling, this statement is likely true. But if we become overconfident
    and reformulate the statement without the word *probably*, this new statement
    is much less likely to be true. It might be true some of the time, but it certainly
    won’t be true all of the time. This confusion of probability for certainty is
    precisely the challenge that most software developers must overcome when they
    begin a project in data science.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家专门从事创建依赖于关于数据和结果概率性陈述的系统。在之前提到的在电子邮件中查找旅行信息的系统案例中，我们可以做出如下陈述：“如果我们知道电子邮件包含出发日期，NLP工具*可能*能够提取它。”对于一个优秀的NLP工具，经过一点调整，这个陈述很可能是正确的。但如果我们过于自信，在陈述中省略了*可能*这个词，那么这个新的陈述很可能是不正确的。它可能在某些时候是正确的，但肯定不会在所有时候都是正确的。这种将概率与确定性混淆的困惑正是大多数软件开发者在开始数据科学项目时必须克服的挑战。
- en: 'When, as a software developer, you come from a world of software specifications,
    well-documented or open-source code libraries, and product features that either
    work or they don’t (“Report a bug!”), the concept of uncertainty in software may
    seem foreign. Software can be compared to a car: loosely speaking, if you have
    all of the right pieces, and you put them together in the right way, the car works,
    and it will take you where you want it to go if you operate it according to the
    manual. If the car isn’t working correctly, then quite literally something is
    broken and can be fixed. This, to me, is directly analogous to pure software development.
    Building a self-driving car to race autonomously across a desert, on the other
    hand, is more like data science. I don’t mean to say that data science is as outrageously
    cool as an autonomous desert-racing vehicle but that you’re never sure your car
    is even going to make it to the finish line or if the task is even possible. So
    many unknown and random variables are in play that there’s absolutely no guarantee
    where the car will end up, and there’s not even a guarantee that *any* car will
    ever finish a race—until a car does it.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当作为一个软件开发者，你从一个软件规范、文档齐全或开源代码库、产品特性要么工作要么不工作（“报告一个错误！”）的世界中来时，软件中的不确定性概念可能看起来很陌生。软件可以比作一辆汽车：粗略地说，如果你有所有正确的部件，并且以正确的方式将它们组合在一起，汽车就会工作，如果你按照手册操作，它就会带你到你想要去的地方。如果汽车不能正常工作，那么字面上来说，确实有什么东西坏了，可以修复。对我来说，这直接类似于纯软件开发。另一方面，建造一辆自动驾驶汽车在沙漠中自主赛车，则更像数据科学。我并不是说数据科学像自动驾驶沙漠赛车那样酷得离谱，而是你永远不能确定你的汽车能否到达终点线，或者任务是否可行。有如此多的未知和随机变量在起作用，以至于绝对无法保证汽车会停在何处，甚至无法保证*任何*汽车能够完成比赛——直到有一辆车做到了。
- en: If a self-driving car makes it 90% of the way to the finish line but is washed
    into a ditch by a rainstorm, it would hardly be appropriate to say that the autonomous
    car doesn’t work. Likewise if the car didn’t technically cross the finish line
    but veered around it and continued for another 100 miles. Furthermore, it wouldn’t
    be appropriate to enter a self-driving sedan, built for roads, into a desert race
    and to subsequently proclaim that the car doesn’t work when it gets stuck on a
    sand dune. That’s precisely how I feel when someone applies a purpose-built data-centric
    tool to a different purpose; they get bad results, and they proclaim that it doesn’t
    work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一辆自动驾驶汽车在完成90%的路程时被暴雨冲进沟里，那么说这辆自动驾驶汽车不工作显然是不恰当的。同样，如果汽车没有真正越过终点线，而是绕过它继续行驶了100英里，这也是不恰当的。此外，将专为道路设计的自动驾驶轿车带入沙漠比赛，并在它卡在沙丘上时宣称汽车不工作，这也是不恰当的。这正是当有人将专为特定目的设计的以数据为中心的工具应用于不同目的时我的感受；他们得到的是不良结果，并宣称它不工作。
- en: For a more concrete example, suppose you’ve been told by a website owner, “The
    typical user visits four pages of our site before leaving.” Suppose you do an
    analysis of a new data set of site usage and find that the average user is visiting
    eight pages before leaving. Does that mean there’s an error? Are you using the
    mean user when you should be using the median user? Does this new data include
    a different type of user or usage? These are questions that a data scientist,
    not a software developer, typically answers, because they involve data exploration
    and uncertainty. Implementing a software solution based on these questions and
    their answers can certainly benefit from the expertise of a software developer,
    but the exploration itself—necessarily involving statistics—falls squarely within
    the realm of a data scientist. In [chapter 5](kindle_split_014_split_000.xhtml#ch05),
    we’ll look at data assessment and evaluation as a useful tool for preventing and
    diagnosing problems and for helping avoid the case where a seemingly finished
    software product fails in some way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体的例子，假设一个网站所有者告诉你，“典型的用户在离开之前会访问我们网站的四个页面。” 假设你对网站使用的新数据集进行了分析，发现平均用户在离开之前会访问八个页面。这意味着有错误吗？当你应该使用中位数用户时，你却在使用平均值用户吗？这些新数据是否包括不同类型的用户或使用方式？这些问题通常是由数据科学家而不是软件开发者回答的，因为它们涉及数据探索和不确定性。基于这些问题及其答案实施软件解决方案当然可以受益于软件开发者的专业知识，但探索本身——必然涉及统计学——完全属于数据科学家的领域。在[第5章](kindle_split_014_split_000.xhtml#ch05)中，我们将探讨数据评估和评估作为预防、诊断问题以及帮助避免看似完成的软件产品在某些方面失败的实用工具。
- en: It’s worth noting that, though I’ve seemed to pit data scientists and software
    developers against each other, this conflict (if I can call it that) can also
    be internal to a single person. While working on data science projects, I often
    find myself trading my data scientist hat for that of a software developer, particularly
    when writing production code. The reason I conceive of them as two different hats
    is that there can be conflicts of interest at times, because priorities can differ
    between the two. Openly discussing these conflicts, as I do in this book, can
    be helpful in illustrating the resolution of these differences, whether they occur
    between two or more people or within an individual who may wear either hat.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管我似乎把数据科学家和软件开发者对立起来，但这种冲突（如果我可以称之为冲突的话）也可以存在于一个人身上。在从事数据科学项目时，我经常发现自己从数据科学家的帽子换成了软件开发者的帽子，尤其是在编写生产代码时。我把它们看作是两顶不同的帽子，是因为有时可能会有利益冲突，因为两个领域的优先级可能不同。像我在这本书中这样做，公开讨论这些冲突，可以帮助说明这些差异的解决，无论它们是发生在两个人或更多人之间，还是发生在可能戴这两顶帽子之一的个人身上。
- en: 1.4\. Do I need to be a software developer?
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 我需要成为一名软件开发者吗？
- en: Earlier, I discussed the difference between data scientists and software developers,
    often as if those are the only two options. Certainly, they are not. And you don’t
    need to be either in order to gain something from this book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我讨论了数据科学家和软件开发者之间的区别，常常好像只有这两个选项。当然，并非如此。而且，你不需要成为其中任何一个才能从这本书中获得收益。
- en: Knowledge of a statistical software tool is a prerequisite for doing practical
    data science, but this can be as simple as a common spreadsheet program (for example,
    the divisive but near-ubiquitous Microsoft Excel). In theory, someone could be
    a data scientist without ever touching a computer or other device. Understanding
    the problem, the data, and relevant statistical methods could be enough, as long
    as someone else can follow your intentions and write the code. In practice, this
    doesn’t happen often.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握统计软件工具的知识是进行实际数据科学工作的先决条件，但这可以简单到只是一个常见的电子表格程序（例如，虽然存在争议但几乎无处不在的微软Excel）。从理论上讲，一个人可以不接触电脑或其他设备就成为一名数据科学家。理解问题、数据和相关的统计方法可能就足够了，只要有人能理解你的意图并编写代码。在实践中，这种情况并不常见。
- en: 'Alternatively, you may be someone who often works with data scientists, and
    you’d like to understand the process without necessarily understanding the technology.
    In this case, there’s also something in this book for you. One of my primary goals
    is to enumerate the many considerations that must be taken into account when solving
    a data-centric problem. In many cases, I’ll be directing explanations in this
    book toward some semi-fictionalized colleagues from my past and present: biologists,
    finance executives, product owners, managers, or others who may have given me
    data and asked me a single question: “Can you analyze this for me?” For that last
    case, perhaps if I write it down here, in detail and with plenty of examples,
    I won’t have to repeat (yet again) that it’s never that simple. An analysis demands
    a question, and I’ll discuss both of those in depth on these pages.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可能是一个经常与数据科学家合作的人，你希望了解这个过程，而无需 necessarily 理解技术。在这种情况下，本书中也有适合你的内容。我的主要目标之一是列举在解决以数据为中心的问题时必须考虑的许多因素。在许多情况下，我将在这本书中针对我过去和现在的一些半虚构同事进行解释：生物学家、金融高管、产品所有者、经理或其他可能给我数据并问我一个简单问题的人：“你能为我分析这个吗？”对于最后一个案例，也许如果我在这里详细地写下来，并附上大量的例子，我就不必（再次）重复说这并不那么简单。分析需要一个问题，我将在这些页面上深入讨论这两个问题。
- en: This book is about the process of thinking about and doing data science, but
    clearly software can’t be ignored. Software—as an industry and its products—is
    the data scientist’s toolbox. The tools of the craft are the enablers of work
    that’s beyond the capabilities of the human mind and body alone. But in this book,
    I’ll cover software only as much as is necessary to explore existing strengths
    and limitations of the software tools and to provide concrete examples of their
    use for clarification. Otherwise, I’ll try to write abstractly about software—without
    being impractical—so that the explanations are accessible by as many people as
    possible, technical or not, and years from now, the explanations may still be
    valuable, even after we’ve moved on to newer (and better?) software languages
    and products.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是关于思考和实践数据科学的过程，但显然软件不能被忽视。软件——作为一个行业及其产品——是数据科学家的工具箱。工艺的工具是超越人类心智和身体能力的工作的推动者。但在本书中，我将只涵盖必要的软件，以便探索现有软件工具的优势和局限性，并提供它们使用的具体例子以供说明。否则，我将尝试以抽象的方式撰写关于软件的内容——而不失实用性——以便尽可能多的人能够理解，无论是技术人员还是非技术人员，而且多年以后，这些解释可能仍然有价值，即使我们已经开始转向更新（和更好？）的软件语言和产品。
- en: 1.5\. Do I need to know statistics?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 我需要了解统计学吗？
- en: As with software, expert knowledge of statistics certainly helps but isn’t necessary.
    At my core, I’m a mathematician and statistician, and so I’m most likely to veer
    into an overly technical tangent in these fields. But I despise jargon and presumed
    knowledge more than most, and so I’ll try hard to include accessible conceptual
    explanations of statistical concepts; hopefully, these are sufficient to any reader
    with a little imagination and perseverance. Where I fall short, I’ll try to direct
    you to some resources with more thorough explanations. As always, I’m an advocate
    of using web searches to find more information on topics that interest you, but
    at least in some cases, it may be better to bear with me for a few pages before
    heading down a rabbit hole of web pages about statistics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与软件一样，对统计学的专业知识当然有帮助，但并非必要。本质上，我是一个数学家和统计学家，因此我可能会在这些领域过于技术性地偏离主题。但我比大多数人更讨厌行话和假设知识，所以我将尽力包括对统计概念的易于理解的概念解释；希望这些解释对任何有想象力且坚持不懈的读者来说都是足够的。在我不足之处，我会尽力引导你到一些提供更详尽解释的资源。一如既往，我倡导使用网络搜索来查找你感兴趣主题的更多信息，但在某些情况下，在深入统计学网页的兔子洞之前，至少先跟我读几页可能更好。
- en: In the meantime, to get you started conceptually, consider the field of statistics
    as the theoretical embodiment of the processes that generate the data you encounter
    on a daily basis. An anonymous website user is a random variable who might click
    any number of things depending on what’s going on in their head. Social media
    data reflects the thoughts and concerns of the populace. Purchases of consumer
    goods depend on both the needs of the consumers as well as marketing campaigns
    for the goods. In each of these cases, you must theorize about how intangible
    thoughts, needs, and reactions are eventually translated into measurable actions
    that create data. Statistics provides a framework for this theorizing. This book
    will spend less time on complex theoretical justifications for statistical models
    and more on formulating mental models of data-generating processes and translating
    those mental models into statistical terminology, equations, and, ultimately,
    code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，为了在概念上帮助你入门，请考虑统计学领域作为你每天遇到的数据生成过程的理论体现。一个匿名的网站用户是一个随机变量，他们可能会根据他们头脑中的情况点击任何数量的事物。社交媒体数据反映了公众的思想和担忧。消费品购买既取决于消费者的需求，也取决于商品的市场营销活动。在这些情况下，你必须理论化无形的思想、需求和反应最终如何转化为可测量的行动，从而创建数据。统计学为此理论化提供了一个框架。本书将花较少的时间在统计模型的复杂理论依据上，而更多地关注制定数据生成过程的心理模型，并将这些心理模型转化为统计术语、方程式，最终是代码。
- en: '1.6\. Priorities: knowledge first, technology second, opinions third'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6. 优先级：知识优先，技术其次，意见第三
- en: This section title is an adage of mine. I use it to help settle disputes in
    the never-ending battle between the various concerns of every data science project—for
    example, software versus statistics, changing business need versus project timeline,
    data quality versus accuracy of results. Each individual concern pushes and pulls
    on the others as a project progresses, and we’re forced to make choices whenever
    two of them disagree on a course of action. I’ve developed a simple framework
    to help with that.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个章节标题是我的一个格言。我使用它来帮助解决数据科学项目中各种关注点之间永无止境的斗争——例如，软件与统计学、不断变化的企业需求与项目时间表、数据质量与结果准确性。随着项目的进展，每个关注点都会对其他关注点施加推拉，每当两个关注点在行动方案上意见不一致时，我们就被迫做出选择。我为此开发了一个简单的框架来帮助处理这些问题。
- en: Knowledge, technology, and opinions are typically what you have at the beginning
    of any project; they are the three things that turn data into answers. *Knowledge*
    is what you know for a fact. *Technology* is the set of tools you have at your
    disposal. *Opinions* are those little almost facts you want to consider true but
    shouldn’t quite yet. It’s important to establish a hierarchy for your thought
    processes so that less-important things don’t steamroll more-important ones because
    they’re easier or more popular or because someone has a hunch.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 知识、技术和意见通常是任何项目开始时你所拥有的东西；它们是将数据转化为答案的三个要素。“知识”是你确信无疑知道的东西。“技术”是你可用的工具集。“意见”是你想要考虑为真的小事实，但还不完全确定。为你的思维过程建立层次结构很重要，这样不那么重要的事情就不会因为它们更容易或更受欢迎，或者因为有人有预感而压倒更重要的事情。
- en: 'In practice, the hierarchy looks like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，层次结构看起来是这样的：
- en: '***Knowledge first—*** Get to know your problem, your data, your approach,
    and your goal before you do anything else, and keep those at the forefront of
    your mind.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***知识第一——*** 在做任何事情之前，了解你的问题、你的数据、你的方法和你的目标，并将这些放在首要位置。'
- en: '***Technology second—*** Software is a tool that serves you. It both enables
    and constrains you. It shouldn’t dictate your approach to the problem except in
    extenuating circumstances.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***技术第二——*** 软件是为你服务的工具。它既使你能够，也限制了你的能力。除非在特殊情况之下，它不应该规定你解决问题的方法。'
- en: '***Opinions third—*** Opinions, intuition, and wishful thinking are to be used
    only as guides toward theories that can be proven correct and not as the focus
    of any project.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***观点第三——*** 意见、直觉和愿望都是作为指向可以证明正确的理论的指南来使用的，而不应该成为任何项目的焦点。'
- en: 'I’m not advocating that knowledge should always take precedence over technology
    in every decision—and likewise for technology over opinion—but if the hierarchy
    is to be turned upside down, you should be doing it deliberately and for a very
    good reason. For instance, suppose you have a large amount of data and a statistical
    model for which you would like to estimate parameters. Furthermore, you already
    have a tool for loading that data into a system that performs a type of statistical
    parameter optimization called *maximum likelihood estimation* (MLE). You know
    that your data and statistical model are complex enough, possibly, to generate
    many reasonable parameter values, and so using MLE to find a single most likely
    parameter value might give unpredictable results. There exist more robust alternatives,
    but you don’t currently have one implemented. You have two options:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不是在提倡在每一个决策中知识都应该优先于技术——同样，技术也应该优先于观点——但如果要颠倒这个顺序，你应该是有意为之，并且有非常好的理由。例如，假设你拥有大量数据和一个你希望估计参数的统计模型。此外，你已经有一个工具可以将这些数据加载到执行一种称为*最大似然估计*（MLE）的统计参数优化的系统中。你知道你的数据和统计模型可能足够复杂，可以生成许多合理的参数值，因此使用MLE来找到一个最可能的参数值可能会产生不可预测的结果。存在更稳健的替代方案，但你目前还没有实现。你有两个选择：
- en: Build a new tool that can do robust parameter estimation.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个能够进行稳健参数估计的新工具。
- en: Use the tool you have to do MLE.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你已有的工具来进行最大似然估计（MLE）。
- en: Your knowledge says you should do A, if you had unlimited time and resources,
    but the technology indicates that you should do B because A requires a tremendous
    outlay of resources. The pragmatic decision is probably B, but that inverts the
    hierarchy. As mentioned earlier, you can do this but only deliberately and for
    a very good reason. The good reason might be the difference in time and money
    that you’ll need to spend on A and B. By *deliberately*, I mean that you should
    not make the decision lightly and you should not forget it. If you choose B, you
    should pass along with any results the knowledge that you made a sacrifice in
    quality in the interest of a cost savings, and you should make note of this in
    documentation and technical reports. You should appropriately intensify quality
    control and perform tests that check specifically for the types of optimization
    errors/biases to which MLE is prone. Making the decision and then forgetting the
    reasoning is a path to underwhelming or misleading results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你的知识告诉你应该做A，如果你有无限的时间和资源，但技术表明你应该做B，因为A需要巨大的资源投入。务实的选择可能是B，但这颠倒了顺序。如前所述，你可以这样做，但只能是有意为之，并且有非常好的理由。这个好理由可能是你在A和B上需要花费的时间和金钱的差异。通过*有意为之*，我的意思是说你不应该轻率地做出决定，也不应该忘记它。如果你选择B，你应该将任何结果与你在质量上做出的牺牲（为了节省成本）的知识一起传递，并在文档和技术报告中注明这一点。你应该适当地加强质量控制，并执行专门检查MLE容易出现的优化错误/偏差的测试。做出决定然后忘记推理是导致令人失望或误导性结果的道路。
- en: Opinion presents an even fuzzier challenge. Sometimes people are blinded by
    the potential of finding truly amazing results and forget to consider what might
    happen if those results aren’t evident in the data. In the heyday of big data,
    any number of software startups attempted to exploit social media—particularly
    Twitter and its “firehose”—to determine trends within various business markets,
    and they often ran into obstacles much larger than they expected. Scale of computation
    and data, parsing of natural language in only 140 characters, and inferring random
    variables on messy data are often involved. Only the very best of these firms
    were able to extract meaningful, significant knowledge from that data and earn
    a profit with it. The rest were forced to give it up or change their focus. Each
    of these startups, at some point, had to decide whether they wanted to spend even
    more time and money chasing a goal that was based mainly on hope and not on evidence.
    I’m sure many of them regretted how far they’d gone and how much money they’d
    spent when they did decide to pack it in.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 观点带来的挑战更加模糊。有时人们会被找到真正惊人结果的潜力所蒙蔽，忘记考虑如果这些结果在数据中不明显会发生什么。在大数据繁荣时期，无数软件初创公司试图利用社交媒体——尤其是Twitter及其“消防水龙”——来确定各个商业市场的趋势，他们常常遇到比预期更大的障碍。计算和数据的规模、在仅140个字符内解析自然语言，以及在混乱数据上推断随机变量都常常涉及其中。只有这些公司中的佼佼者才能从数据中提取有意义的、重要的知识并从中获利。其余的被迫放弃或改变他们的重点。每个这些初创公司，在某个时候，都必须决定他们是否愿意花费更多的时间和金钱去追逐一个主要基于希望而不是基于证据的目标。我相信他们中的许多人当他们决定放弃时，一定会后悔他们走了多远以及他们花了多少钱。
- en: Often people are blinded by what they think is possible, and they forget to
    consider that it might not be possible or that it might be much more expensive
    than estimated. These are opinions—guesses—not knowledge, and they shouldn’t play
    a primary role in data analysis and product development. Goals are not certain
    to be achieved but are required for any project, so it’s imperative not to take
    the goal and its attainability for granted. You should always consider current
    knowledge first and seek to expand that knowledge incrementally until you either
    achieve the goal or are forced to abandon it. I’ve mentioned this uncertainty
    of attainability as a particularly stark difference between the philosophies of
    software development and data science. In data science, a goal is much less likely
    to be achievable in exactly its original form. In a room full of software developers
    or inexperienced data scientists, be particularly wary of anyone presupposing
    without evidence that a goal is 100% achievable.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常常被他们认为可能的事情所蒙蔽，忘记考虑它可能不可能实现，或者它可能比预期的要昂贵得多。这些都是观点——猜测——而不是知识，它们不应当在数据分析产品开发中扮演主要角色。目标并不一定能够实现，但任何项目都需要目标，因此，不要理所当然地认为目标及其可实现性是必要的。你应该始终首先考虑当前的知识，并寻求逐步扩展这一知识，直到你实现目标或被迫放弃它。我曾提到这种可实现性的不确定性是软件开发和数据科学哲学之间一个特别鲜明的差异。在数据科学中，目标不太可能以原始形式实现。在一个充满软件开发人员或缺乏经验的数据科学家的房间里，特别要警惕那些没有证据就预设目标100%可实现的人。
- en: 'Remember: knowledge first, then technology, and then opinion. It’s not a perfect
    framework, but I’ve found it helpful.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：先知识，再技术，最后是观点。这不是一个完美的框架，但我发现它很有帮助。
- en: 1.7\. Best practices
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7. 最佳实践
- en: In my years of analyzing data and writing code as an applied mathematician,
    PhD student researcher, bioinformatics software engineer, data scientist, or any
    of the other titles I’ve had, I’ve run into a few problems involving poor project
    management on my part. When I worked for years on my own research projects, which
    no one else touched or looked at, I frequently managed to set poorly documented
    code aside for long enough that I forgot how it worked. I’d also forgotten which
    version of the results was the most recent. I’d managed to make it nearly impossible
    for anyone else to take up my projects after I left that position. None of this
    was intentional; it was largely negligence but also ignorance of the usual ways
    in which people ensure that their project’s materials and code can survive a long
    while on the shelf or in another’s hands.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我作为应用数学家、博士研究生、生物信息学软件工程师、数据科学家或我所拥有的其他任何头衔的这些年里，我遇到了一些涉及我自身项目管理不善的问题。当我独自工作多年，我的研究项目无人触及或查看时，我经常设法将文档不充分的代码放置很长时间，以至于忘记了它是如何工作的。我也忘记了哪个版本的结果是最新的。我在离开那个职位后，几乎让任何人都无法接手我的项目。这一切都不是故意的；这主要是疏忽，也是对人们确保项目材料和代码能够在书架上或他人手中长期存活的通常方式的忽视。
- en: When working on a team—in particular, a team of experienced software developers—someone
    has usually established a set of best practices for documentation and preservation
    of project materials and code. It’s usually important that everyone on the team
    abides by the team’s strategies for these things, but in the absence of a team
    strategy, or if you’re working by yourself, you can do a few things to make your
    life as a data scientist easier in the long run. The following subsections cover
    a few of my favorite ways to stay organized.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当在一个团队中工作时——特别是经验丰富的软件开发团队——通常有人已经建立了一套关于文档和项目材料及代码保存的最佳实践。通常，确保团队中的每个人都遵守这些策略是很重要的，但在没有团队策略的情况下，或者如果你是独立工作，你可以做一些事情来使你的数据科学家生涯在长远来看更加轻松。以下小节涵盖了保持组织的一些建议。
- en: 1.7.1\. Documentation
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.1\. 文档
- en: 'Can you imagine what one of your peers might have to go through to take over
    your project if you left suddenly? Would taking over your project be a horrible
    experience? If you answer yes, please do these future peers—and yourself—a favor
    by staying current on your documentation. Here are some tips:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想象如果你的同伴在你突然离开时接管你的项目，他们可能需要经历什么吗？接管你的项目会是一个糟糕的经历吗？如果你回答是，请为这些未来的同伴——以及你自己——行个方便，保持你的文档更新。以下是一些建议：
- en: Comment your code so that a peer unfamiliar with your work can understand what
    the code does.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对你的代码进行注释，以便不熟悉你工作的人可以理解代码的功能。
- en: For a finished piece of software—even a simple script—write a short description
    of how to use it and put this in the same place (for example, the same file folder)
    as the code.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个完成的软件作品——即使是简单的脚本——写一个简短的说明，说明如何使用它，并将其放在与代码相同的地方（例如，相同的文件文件夹）。
- en: Make sure everything—files, folders, documents, and so on—has a meaningful name.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保所有东西——文件、文件夹、文档等——都有一个有意义的名称。
- en: 1.7.2\. Code repositories and versioning
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.2\. 代码仓库和版本控制
- en: Some software products have been built specifically to contain and manage source
    code for software. These are called *source code repositories* (or *repos*), and
    they can help you immensely, for a number of reasons.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一些软件产品是专门为了包含和管理软件源代码而构建的。这些被称为*源代码仓库*（或*repos*），它们可以从多个方面极大地帮助你。
- en: First, most modern repos are based on versioning systems, which are also great.
    A versioning system tracks the changes you make in your code, allows you to create
    and compare different versions of your code, and generally makes the process of
    writing and modifying code much more pleasant once you get used to it. The drawback
    of repos and versioning is that they take time to learn and incorporate into your
    normal workflow. They’re both worth the time, however. At the time of this writing,
    both [Bitbucket.org](http://Bitbucket.org) and [GitHub.com](http://GitHub.com)
    provide free web hosting of code repos, although both websites host both public
    and private repos, so make sure you don’t accidentally make all of your source
    code public. Git is currently the most popular versioning system, and it incorporates
    nicely into both repo hosts mentioned. You can find tutorials on how to get started
    on the hosts’ web pages.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，大多数现代仓库都是基于版本控制系统的，这也是很好的。版本控制系统跟踪你在代码中做出的更改，允许你创建和比较代码的不同版本，一旦你习惯了，通常会使编写和修改代码的过程变得更加愉快。仓库和版本控制的缺点是它们需要时间来学习和融入你的正常工作流程。然而，两者都值得花费时间。在撰写本文时，[Bitbucket.org](http://Bitbucket.org)
    和 [GitHub.com](http://GitHub.com) 都提供免费的代码仓库网络托管服务，尽管这两个网站都托管了公共和私有仓库，所以请确保你不会不小心将所有源代码公开。Git是目前最受欢迎的版本控制系统，并且很好地融入了上述提到的仓库托管服务。你可以在托管者的网页上找到如何开始的教程。
- en: Another reason why I find a remote repo-hosting service nearly indispensable
    is that it functions as a backup. My code will be safe even if my computer is
    accidentally crushed by an autonomous desert-race vehicle (though that hasn’t
    happened to me yet). I make it a habit to *push* (send or upload) my latest code
    changes to the remote host almost every day, or about as often as I might schedule
    an automatic backup on a standard data-backup service.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现远程仓库托管服务几乎不可或缺的另一个原因是它充当备份的作用。即使我的电脑不小心被一辆自主沙漠赛车压碎（尽管这还没有发生在我身上），我的代码也会安全。我养成了几乎每天都将最新的代码更改*推送*（发送或上传）到远程主机的习惯，或者大约像在标准数据备份服务上安排自动备份那样频繁。
- en: Some code-hosting services have great web interfaces for looking through code
    history, various versions, development status, and the like, which has become
    standard practice for collaborating on teams and large projects. It’s helpful
    for individuals and small projects as well, particularly when returning to an
    old project or trying to figure out which changes you made since a particular
    time in the past.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些代码托管服务提供了查看代码历史、各种版本、开发状态等功能出色的网页界面，这已成为团队合作和大型项目的标准实践。对于个人和小型项目来说，这也很有帮助，尤其是在回到一个旧项目或试图弄清楚自上次特定时间以来你做了哪些更改时。
- en: Finally, remote repos let you access your code from any place with web access.
    You don’t need a computer with the appropriate editor and environment to browse
    through code. No, you normally can’t do much except browse code (and maybe do
    simple editing) from these web interfaces, but the best ones have good language-specific
    code highlighting and a few other features that make browsing easy and useful.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，远程仓库允许你从任何有网络访问的地方访问你的代码。你不需要一台装有适当编辑器和环境的电脑来浏览代码。不，你通常不能做很多除了浏览代码（也许还能进行简单的编辑）之外的事情，但最好的那些有良好的语言特定代码高亮和其他一些使浏览变得容易和有用的功能。
- en: 'Here are some tips on repos and versioning:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于仓库和版本控制的提示：
- en: Using a remote source code repo is now standard practice for most groups that
    write code; use one!
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用远程源代码仓库现在是大多数编写代码的团队的常规做法；使用一个！
- en: It’s absolutely worth the effort to learn Git or another versioning system;
    learn it!
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Git或其他版本控制系统绝对值得付出努力；学习它！
- en: Commit your code changes to the repo often, perhaps daily or whenever you finish
    a specific task. Push those changes to a remote repo, so they’re backed up and
    shared with others on your team.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常将代码更改提交到仓库，可能是每天或每当完成一个特定任务时。将这些更改推送到远程仓库，以便它们得到备份并与团队中的其他人共享。
- en: If your next code changes will break something, do the work in a location that
    won’t affect the production version or the development by other team members.
    For example, you could create a new branch in Git.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的下一次代码更改会破坏某些东西，请在不会影响生产版本或其他团队成员开发的地方进行工作。例如，你可以在Git中创建一个新的分支。
- en: Use versioning, branching, and forking (tutorials abound on the internet) instead
    of copying and pasting code from one place to another and therefore having to
    maintain/modify the same code in multiple places.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用版本控制、分支和分叉（互联网上有很多教程）而不是从一处复制粘贴代码到另一处，因此需要在多个地方维护/修改相同的代码。
- en: Most software teams have a Git guru. Ask them whenever you have questions about
    best practices; the time spent learning now will pay off in the end.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数软件团队都有一个Git大师。每当你有关于最佳实践的问题时，就向他们请教；现在花在学习上的时间最终会得到回报。
- en: 1.7.3\. Code organization
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.3\. 代码组织
- en: Many books detail good coding practices, and I don’t intend to replicate or
    replace them here. But a few guidelines can be very helpful, particularly when
    you try to share or reuse code. Most people who have been programming for some
    time will be familiar with these, but I’ve found that many people—particularly
    in academia and other work environments without much coding collaboration—don’t
    always know about them or adhere to them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 许多书籍详细介绍了良好的编码实践，我并不打算在这里复制或取代它们。但一些指南可能非常有帮助，尤其是在你尝试共享或重用代码时。大多数有一定编程经验的人都会熟悉这些，但我发现许多人——尤其是在学术和其他没有太多编码协作的工作环境中——并不总是了解它们或遵守它们。
- en: 'Here are the guidelines:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些指南：
- en: Try to use coding patterns that are common for the particular programming language.
    Python, R, and Java, for instance, all have significant differences in the way
    developers typically organize their code. Any popular resource for the language
    contains examples and guidelines for such coding patterns.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量使用特定编程语言的常见编码模式。例如，Python、R和Java在开发者通常组织代码的方式上都有显著差异。任何流行的语言资源都包含此类编码模式的示例和指南。
- en: Use meaningful names for variables and other objects. This makes your code much
    more understandable for new collaborators and your future self.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为变量和其他对象使用有意义的名称。这使得你的代码对新合作者和未来的你来说更容易理解。
- en: Use plenty of informative comments, for the reasons just mentioned.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大量的信息性注释，正如前面提到的理由。
- en: Don’t copy and paste code. Having the same code active in two places means twice
    as much work for you when you want to change it. Encapsulating that code in a
    function, method, object, or library makes sure that later modifications happen
    in only one place.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要复制粘贴代码。在两个地方都有相同的代码意味着当你想要更改它时，你需要做的工作量加倍。将这段代码封装在函数、方法、对象或库中，确保后续的修改只在一个地方发生。
- en: Try to code in chunks with specific functionalities. For scripts, this means
    having well-commented snippets (optionally in separate files or libraries) that
    each accomplish a specific task. For applications, this means having relatively
    simple functions, objects, and methods so that each does specific things. A good
    general rule is if you can’t name your snippet, function, or method in such a
    way that the name generally describes everything the chunk of code accomplishes,
    you should probably break the chunk into smaller, simpler chunks.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试以具有特定功能的方式编写代码块。对于脚本，这意味着拥有注释良好的代码片段（可选地放在单独的文件或库中），每个片段都完成特定的任务。对于应用程序，这意味着拥有相对简单的函数、对象和方法，以便每个都能完成特定的事情。一个很好的通用规则是，如果你不能以某种方式命名你的代码片段、函数或方法，使得名称通常描述了代码块所完成的一切，那么你可能需要将代码块拆分成更小、更简单的代码块。
- en: Don’t optimize prematurely. This is a common mantra for programmers everywhere.
    Make sure you code your algorithm in a logical, coherent fashion, and only if
    you find out later that your implementation is inefficient should you try to shave
    off compute cycles and bytes of memory.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要过早优化。这是程序员普遍遵循的一个常见格言。确保你的算法代码逻辑清晰、连贯，只有在你发现你的实现效率低下时，才尝试减少计算周期和内存字节。
- en: Pretend that someone else will, at some point, join your project. Ask yourself,
    “Could they read my code and figure out what it does?” If not, spend a little
    time organizing and commenting sooner rather than later.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设将来会有人加入你的项目。问问自己，“他们能读懂我的代码并弄清楚它在做什么吗？”如果不能，那么尽早花点时间组织和注释，而不是拖到后来。
- en: 1.7.4\. Ask questions
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.4\. 提问
- en: This may sound obvious or trivial, but it’s so important that I include it here.
    I’ve already discussed how awareness is one of the greatest strengths of a good
    data scientist; likewise, the unwillingness to gain awareness via any and all
    means can be a great weakness. The stereotype of an introverted academic being
    too shy to ask for help may be familiar to you, but have you heard of the know-it-all
    PhD data scientist who was too proud to admit he didn’t know something? I certainly
    have. Pride is perhaps a bigger pitfall to data scientists these days than shyness,
    but you should be wary of both.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来很显然或微不足道，但它如此重要，以至于我把它包括在这里。我已经讨论过，意识是优秀数据科学家最大的优势之一；同样，不愿意通过任何和所有方式获得意识可能是一个巨大的弱点。内向的学术人士因为害羞而不愿意寻求帮助的刻板印象你可能很熟悉，但你听说过那种自以为是的博士数据科学家，因为太骄傲而不承认自己不知道某些事情吗？我确实听说过。如今，骄傲可能比害羞是更大的陷阱，但你应该对两者都保持警惕。
- en: Software engineers, business strategists, sales executives, marketers, researchers,
    and other data scientists all know more than you do about their particular domains
    or projects, and it would be a shame to ignore the wealth of knowledge they possess,
    due to shyness, pride, or any other reason. A business setting wherein nearly
    everyone else has a role different from yours provides ample opportunity to learn
    about the company and the industry. This is the subject matter expertise, or domain
    knowledge, that I’ve mentioned already. Nontechnical business folks, in my experience,
    have a tendency to treat you, the data scientist, as the smart one in the conversation,
    but don’t forget that they tend to know a lot more than you about project goals
    and business problems, two extremely important concepts. Never hesitate to have
    a lengthy discussion with someone who knows the business side of the project and
    problems you’re working on. I often find that such conversations illuminate projects
    in new ways, sometimes causing strategy changes but always contributing to the
    domain knowledge that’s necessary for me to finish a project successfully.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程师、商业策略师、销售主管、市场营销人员、研究人员以及其他数据科学家，他们对各自领域或项目了解的都比你要多，因为害羞、骄傲或其他任何原因而忽视他们所拥有的丰富知识，这将是件遗憾的事情。在一个几乎每个人都扮演着与你不同角色的商业环境中，你有很多机会了解公司和行业。这就是我之前提到的主题专业知识或领域知识。根据我的经验，非技术性的商业人士往往倾向于把你，数据科学家，看作是谈话中的智者，但不要忘记他们往往比你更了解项目目标和商业问题，这两个概念极其重要。不要犹豫与了解项目商业面和你在处理的问题的人进行长时间的讨论。我经常发现这样的对话以新的方式照亮了项目，有时导致策略的改变，但总是有助于我成功完成项目所需的领域知识。
- en: 1.7.5\. Stay close to the data
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.5\. 紧密关注数据
- en: Being “close to the data” means making sure that the methods and algorithms
    that you’re applying to the data aren’t so dense as to obscure it. Another way
    to phrase this would be “don’t use methods that are more complex than needed,
    and always be conscious of the possibility of mistakes.”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: “紧密关注数据”意味着确保你应用到的数据方法和算法不会过于复杂以至于掩盖了数据本身。另一种表述方式可能是“不要使用比必需更复杂的方法，并且始终意识到出错的可能性。”
- en: 'Many people will argue with this piece of advice, and I agree with these detractors
    that many complex methods have proven their worth. The field of machine learning
    is a perfect example of a source of such methods. In the cases where complex methods
    (*black box* methods, in some cases) give considerable advantage, the concept
    of being close to the data can be adapted: make sure that some results from complex
    methods can be verified, justified, or supported by simpler methods that are close
    to the data. This could include a glorified form of spot-checking, whereby you
    can pick some results at random, extract the raw data that’s relevant to these
    results, and use logic and/or simple statistics to make sure that the results
    make sense intuitively. Straying too far from the data without a safety line can
    get you in trouble, because these problems are the hardest to diagnose. Throughout
    the book, and in each example, I’ll return to this concept and give more specifics.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人会反对这条建议，我同意这些批评者，许多复杂的方法已经证明了自己的价值。机器学习领域就是这样一个方法来源的完美例子。在复杂方法（在某些情况下称为“黑盒”方法）能带来相当大的优势的情况下，接近数据的概念可以适应：确保复杂方法的一些结果可以通过接近数据的一些简单方法来验证、证明或支持。这可能包括一种夸张的抽查形式，你可以随机选择一些结果，提取与这些结果相关的原始数据，并使用逻辑和/或简单的统计方法来确保结果在直观上是有意义的。在没有安全线的情况下远离数据可能会让你陷入麻烦，因为这些问题是诊断起来最困难的。在整个书中，以及在每个例子中，我都会回到这个概念，并给出更具体的说明。
- en: '1.8\. Reading this book: how I discuss concepts'
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8\. 阅读这本书：我是如何讨论概念的
- en: It has been my experience—over and over—that complex concepts appear vastly
    more complex at the point when you begin to learn about them, as compared to later,
    after you’ve begun to understand them. This is not only because all concepts seem
    less complex once you begin to understand them, but also because people, in general,
    who might be explaining a concept to you or who are writing about it revel in
    the complexities that they understand and often have little patience or sympathy
    for those who don’t understand. For example, it’s difficult for most statistics
    professors to explain a simple statistical test to a layperson. This inability
    to explain in simple terms extends across all fields and industries, I’ve found.
    Part of this problem is that most people love to use jargon, and they love to
    prove their knowledge of it. This is, perhaps, a fault of human nature. But from
    this I’ve learned to ignore my frustrations at the beginning of learning a new,
    complex concept and not to get hung up on any specific point until I’ve gone through
    the concept as a whole.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我的经验——一次又一次地——是，复杂的概念在你开始了解它们的时候看起来比后来理解它们的时候要复杂得多。这不仅是因为一旦你开始理解它们，所有概念看起来都变得不那么复杂，而且也因为人们，总的来说，他们可能正在向你解释一个概念或正在写关于它的内容，他们沉醉于他们所理解的复杂性，而对于那些不理解的人来说，他们往往缺乏耐心或同情。例如，大多数统计学教授很难向一个外行人解释一个简单的统计测试。我发现，这种无法用简单术语解释的能力在所有领域和行业中都存在。这个问题的部分原因在于，大多数人喜欢使用行话，并且喜欢证明他们对这些行话的了解。这可能也许是人类本性的一个缺陷。但从此我学会了在学习一个新、复杂的概念的初期忽略我的挫败感，并且不要在了解整个概念之前纠结于任何具体点。
- en: Throughout this book, I try to explain new concepts in simple terms before getting
    into specifics. This is the way I prefer to learn and the way I prefer to explain.
    Despite this, you’ll still have moments when you get stuck. I implore you suspend
    your frustration, stick with it to the end of the chapter, and then review the
    concept as a whole. At that point, if something still doesn’t make sense, perhaps
    rereading the paragraphs in question can help, and if not, feel free to consult
    other resources. I employ conceptual thought processes, and I intend to focus
    on the whole before the parts. Knowing this in advance may help you while reading.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我试图用简单的术语解释新概念，然后再深入具体细节。这是我偏好的学习方式和解释方式。尽管如此，你仍然会遇到一些难以理解的时刻。我恳请你不要气馁，坚持看到章节的结尾，然后整体回顾这个概念。在那个时刻，如果仍有不明白的地方，也许重新阅读有疑问的段落会有所帮助，如果还不行，请随时查阅其他资源。我使用概念性思维过程，并打算在分析部分之前先关注整体。提前了解这一点可能会在阅读时对你有所帮助。
- en: Before diving into the practical steps of the data science process in the next
    chapter and beyond, I’d like to note that this book is intended to be accessible
    for non-experts in data science, software, and statistics. If you’re not a beginner,
    you may occasionally find some sections that cover material you already know.
    I’d like to think that every section of this book provides a useful or even fresh
    perspective on its topic, even for experts, but if you’re pressed for time, feel
    free to skip ahead to something of more use to you. On the other hand, I’d like
    to discourage skipping whole chapters of this book when reading it for the first
    time. Each chapter describes a step in the data science process, and skipping
    one of them could break the continuity of the process and how it’s presented.
    Rather than skipping, it would be better to read at least the beginning and the
    end of the chapter and to skim section by section in order to gain important context
    for subsequent chapters.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章以及更深入地探讨数据科学过程的具体步骤之前，我想指出，这本书旨在让数据科学、软件和统计学非专业人士也能轻松阅读。如果你不是初学者，你可能会偶尔发现一些涵盖你已经了解的内容的部分。我希望这本书的每一部分都能为读者提供有用的甚至新颖的视角，即使是对专家来说也是如此；但如果你时间紧迫，请随意跳到对你更有用的部分。另一方面，我建议在首次阅读这本书时不要跳过整个章节。每一章都描述了数据科学过程中的一个步骤，跳过任何一个都可能会打断过程的连续性和展示方式。与其跳过，不如至少阅读每一章的开头和结尾，并逐节浏览，以便为后续章节获得重要背景信息。
- en: As a final disclaimer, throughout the book I pull many practical examples from
    my own experience. Several of my early, formative data science projects were in
    the field of bioinformatics, and so sometimes I delve into discussions of genetics,
    RNA, and other biological concepts. This may seem heavy to some people, and that’s
    OK. It’s not necessary to understand the biology as long as you’re able to understand
    the data-oriented aspects such as project goals, data sources, and statistical
    analysis. This is also true for examples from other fields, but I rely particularly
    heavily on bioinformatics because I can still remember well what it was like to
    encounter various challenges of data science at that early phase of my career.
    Furthermore, I won’t shy away from using highly specified examples because it’s
    always good practice for a data scientist to learn the basics of a new field and
    attempt to apply experience and knowledge to problems therein. I do, however,
    try to present all examples in a way that anyone can understand.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的声明，我在整本书中引用了许多来自我自己的经验实例。我的早期、形成性的数据科学项目主要集中在生物信息学领域，因此有时我会深入讨论遗传学、RNA和其他生物学概念。这可能对一些人来说显得有些复杂，但没关系。只要你能理解数据导向的方面，如项目目标、数据来源和统计分析，就不需要了解生物学。这也适用于其他领域的例子，但我特别依赖生物信息学，因为我还能清楚地记得在我职业生涯早期遇到各种数据科学挑战时的感受。此外，我不会回避使用高度具体的例子，因为对于数据科学家来说，学习新领域的基础知识并尝试将经验和知识应用于其中是一个很好的实践。然而，我会尽量以任何人都能够理解的方式呈现所有例子。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Awareness is perhaps the biggest strength of a good data scientist, particularly
    in the face of uncertainty.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意识力可能是优秀数据科学家的最大优势，尤其是在面对不确定性时。
- en: Dealing with uncertainty is often what separates the role of a data scientist
    from that of a software developer.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应对不确定性通常是区分数据科学家角色和软件开发者角色的关键。
- en: Setting priorities and balancing them with limitations and requirements can
    be done using a helpful framework that I’ve outlined.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个有助于确定优先级并平衡限制和要求的有用框架。
- en: Using some of the best practices that I suggest can spare you from headaches
    later.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我建议的一些最佳实践可以让你以后避免头痛。
- en: I’m a conceptual learner and teacher who loves talking about things abstractly
    before delving into practical examples, so please keep that in mind throughout
    the book.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是一个喜欢在深入实际例子之前先抽象地谈论事物的概念学习者兼教师，所以请记住这一点，贯穿整本书。
- en: Chapter 2\. Setting goals by asking good questions
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章：通过提出好问题来设定目标
- en: '*This chapter covers*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Putting yourself in the customer’s shoes
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 站在客户的立场上思考
- en: Asking specific, useful questions of the data
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向数据提出具体、有用的问题
- en: Understanding the strengths and limitations of the data in answering those questions
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据在回答这些问题时的优势和局限性
- en: Connecting those questions and answers to project goals
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些问题和答案与项目目标联系起来
- en: Planning backward from the desired goal, not forward from data and software
    tools
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从期望的目标逆向规划，而不是从数据和软件工具开始向前规划
- en: '[Figure 2.1](kindle_split_011_split_000.xhtml#ch02fig01) shows where we are
    in the data science process: setting goals, which is the first step of the preparation
    phase. In a data science project, as in many other fields, the main goals should
    be set at the beginning of the project. All the work you do after setting goals
    is making use of data, statistics, and programming to move toward and achieve
    those goals. This chapter emphasizes how important this initial phase is and gives
    some guidance on how to develop and state goals in a useful way.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.1](kindle_split_011_split_000.xhtml#ch02fig01) 展示了我们在数据科学过程中的位置：设定目标，这是准备阶段的第一步。在数据科学项目中，就像在许多其他领域一样，主要目标应该在项目开始时设定。在设定目标之后，你所做的一切工作都是利用数据、统计学和编程来朝着并实现这些目标。本章强调了这一初始阶段的重要性，并提供了一些关于如何以有用方式制定和陈述目标的指导。'
- en: 'Figure 2.1\. The first step of the preparation phase of the data science process:
    setting goals'
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1. 数据科学过程准备阶段的第一步：设定目标
- en: '![](Images/02fig01_alt.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig01_alt.jpg)'
- en: 2.1\. Listening to the customer
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 倾听客户
- en: 'Every project in data science has a customer. Sometimes the customer is someone
    who pays you or your business to do the project—for example, a client or contracting
    agency. In academia, the customer might be a laboratory scientist who has asked
    you to analyze their data. Sometimes the customer is you, your boss, or another
    colleague. No matter who the customer might be, they have some expectations about
    what they might receive from you, the data scientist who has been given the project.
    Often, these expectations relate to the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的每个项目都有一个客户。有时客户是支付你或你的公司进行项目的人——例如，客户或承包机构。在学术界，客户可能是一位要求你分析他们数据的实验室科学家。有时客户是你、你的老板或另一位同事。无论客户是谁，他们对你，即被分配项目的数据科学家，都有一些期望。通常，这些期望与以下内容相关：
- en: Questions that need to be answered or problems that need to be solved
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要回答的问题或需要解决的问题
- en: A tangible final product, such as a report or software application
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有形的最终产品，例如报告或软件应用
- en: Summaries of prior research or related projects and products
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前期研究或相关项目及产品的摘要
- en: 'Expectations can come from almost anywhere. Some are hopes and dreams, and
    others are drawn from experience or knowledge of similar projects. But a typical
    discussion of expectations boils down to two sides: what the customer wants versus
    what the data scientist thinks is possible. This could be described as wishes
    versus pragmatism, with the customer describing their desires and the data scientist
    approving, rejecting, or qualifying each one based on apparent feasibility. On
    the other hand, if you’d like to think of yourself, the data scientist, as a genie,
    a granter of wishes, you wouldn’t be the only one to do so!'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 期望可以来自几乎任何地方。有些是希望和梦想，而另一些则是从经验或对类似项目的知识中汲取而来。但典型的期望讨论归结为两个方面：客户想要什么与数据科学家认为可能什么。这可以描述为愿望与实用主义，客户描述他们的愿望，而数据科学家根据明显的可行性予以批准、拒绝或限定每一个。另一方面，如果你愿意将自己视为数据科学家，一个愿望的授予者，那么你不会是唯一这样做的人！
- en: 2.1.1\. Resolving wishes and pragmatism
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 解决愿望与实用主义
- en: With regard to the customer’s wishes, they can range from completely reasonable
    to utterly outlandish, and this is OK. Much of business development and hard science
    is driven by intuition. CEOs, biologists, marketers, and physicists alike use
    their experience and knowledge to develop theories about how the world works.
    Some of these theories are backed by solid data and analysis, but others come
    more from intuition, which is a conceptual framework that the person has developed
    while working extensively in their field. A notable difference between many fields
    and data science is that in data science, if a customer has a wish, even an experienced
    data scientist may not know whether it’s possible. Whereas a software engineer
    usually knows what tasks software tools are capable of performing, and a biologist
    knows more or less what the laboratory can do, a data scientist who has not yet
    seen or worked with the relevant data is faced with a large amount of uncertainty,
    principally about what specific data is available and about how much evidence
    it can provide to answer any given question. Uncertainty is, again, a major factor
    in the data scientific process and should be kept at the forefront of your mind
    when talking with customers about their wishes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关于客户的愿望，它们可以从完全合理到完全荒谬，这是可以接受的。商业发展和硬科学的大部分内容都是由直觉驱动的。CEO、生物学家、营销人员和物理学家都使用他们的经验和知识来发展关于世界如何运作的理论。其中一些理论有可靠的数据和分析支持，但其他理论则更多地来自直觉，这是个人在广泛从事其领域工作时发展起来的概念框架。许多领域与数据科学的一个显著区别是，在数据科学中，如果客户有一个愿望，即使是经验丰富的数据科学家也可能不知道这是否可行。而软件工程师通常知道软件工具能够执行的任务，生物学家也大致知道实验室能够做什么，但尚未见过或处理过相关数据的数据科学家面临着大量的不确定性，主要关于具体可用的数据以及它能够提供多少证据来回答任何给定的问题。不确定性再次是数据科学过程中的一个主要因素，在谈论客户愿望时应该始终将其放在首位。
- en: For example, during the few years that I worked with biologists and gene expression
    data, I began to develop my own conceptual ideas about how RNA is translated from
    DNA and how strands of RNA float around in a cell and interact with other molecules.
    I’m a visual person, so I often found myself picturing a strand of RNA comprising
    hundreds or maybe thousands of nucleotides, each one appearing like one of four
    letters representing a base compound (A, C, G, or T; I’ll use *T* in place of
    *U* for convenience) and the whole strand looking like a long, flexible chain,
    a sentence that makes sense only to the machinery within the cell. Because of
    the chemistry of RNA and its nucleotides, complementary sequences like to bind
    to one another; A likes to bind to T, and C likes to bind to G. When two strands
    of RNA contain near-complementary sequences, they may very well stick to each
    other. A single strand of RNA might also fold in upon and stick to itself if it’s
    flexible enough and contains mutually complementary sequences. I’ve used this
    conceptual framework on many occasions to make guesses about the types of things
    that can happen when a bunch of RNA is floating around in a cell.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我与生物学家和基因表达数据共事的几年里，我开始形成自己关于RNA如何从DNA翻译以及RNA链如何在细胞中漂浮并与其他分子相互作用的观念。我是一个视觉型的人，所以我经常想象一条由数百或甚至数千个核苷酸组成的RNA链，每个核苷酸看起来像代表一种碱基化合物的四个字母之一（A、C、G或T；为了方便，我将用*T*代替*U*）并且整个链看起来像一条长而灵活的链，只有细胞内部的机器才能理解这个句子。由于RNA及其核苷酸的化学性质，互补序列倾向于相互结合；A喜欢与T结合，C喜欢与G结合。当两条RNA链含有近互补序列时，它们很可能粘在一起。如果一条RNA链足够灵活并且包含相互互补的序列，它也可能折叠并粘附在自己身上。我已经多次使用这个概念框架来猜测当许多RNA在细胞中漂浮时可能发生的事情。
- en: When I began to work with microRNA data, it made sense to me that microRNA—short
    sequences of about 20 nucleotides—might bind to a section of a genetic mRNA sequence
    (RNA translated directly from a strand of DNA corresponding to a specific gene,
    which is typically much longer) and inhibit other molecules from interacting with
    the gene’s mRNA, effectively rendering that gene sequence useless. It makes conceptual
    sense to me that one bit of RNA can stick to a section of genetic RNA and end
    up blocking another molecule from sticking to the same section. This concept is
    supported by scientific journal articles and hard data showing that microRNA can
    inhibit expression or function of genetic mRNA if they have complementary sequences.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始处理microRNA数据时，对我来说，microRNA——大约由20个核苷酸组成的短序列——可能绑定到基因mRNA序列的一部分（这是直接从对应特定基因的DNA链（通常要长得多）翻译过来的RNA），从而阻止其他分子与基因的mRNA相互作用，实际上使该基因序列变得无用。对我来说，一个RNA片段可以粘附到遗传RNA的一部分，并最终阻止另一个分子粘附到同一部分，这个概念在概念上是合理的。这一概念得到了科学期刊文章和硬数据的支持，这些数据表明，如果它们有互补序列，microRNA可以抑制遗传mRNA的表达或功能。
- en: A professor of biology I was working with had a much more nuanced conceptual
    framework describing how he saw this system of genes, microRNA, and mRNA. In particular,
    he had been working with the biology of *Mus musculus*—a common mouse—for decades
    and could list any number of notable genes, their functions, related genes, and
    physical systems, and characteristics that are measurably affected if one begins
    to do experiments that knock out those genes. Because the professor knew more
    than I will ever know about the genetics of mice, and because it would be impossible
    for him to share all of his knowledge with me, it was incredibly important for
    us to talk through the goals and expectations of a project prior to spending too
    much time working on any aspect of the project. Without his input, I would be
    guessing at what the biologically relevant goals were. If I was wrong, which was
    likely, that work would be wasted. For example, certain specific microRNAs have
    been well studied and are known to accomplish basic functions within a cell and
    little more. If one of the goals of the project was to discover new functions
    of little-studied microRNAs, we would probably want to exclude certain families
    of microRNAs from the analysis. If we didn’t exclude them, they would most likely
    add to the noise of an already very noisy genetic conversation within a cell.
    This is merely one of a large number of important things that the professor knew
    that I didn’t, making a lengthy discussion of goals, expectations, and caveats
    necessary before starting the project in earnest.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾与一位生物学家教授合作，他有一个更加细腻的概念框架，描述了他如何看待基因、microRNA和mRNA这个系统。特别是，他几十年来一直在研究小鼠的生物学——一种常见的小鼠——并能列出任何数量的显著基因、它们的功能、相关基因和物理系统，以及如果开始进行敲除这些基因的实验，会受到可测量的影响的特征。因为这位教授对小鼠遗传学的了解比我多得多，而且他不可能与我分享他所有的知识，所以在我们投入太多时间在项目的任何方面之前，讨论项目的目标和期望非常重要。没有他的意见，我就会猜测生物相关的目标是什么。如果我是错的，这很可能是，那么这项工作就会白费。例如，某些特定的microRNA已经被充分研究，并且已知它们在细胞内完成基本功能，而几乎没有其他功能。如果项目的目标之一是发现研究较少的microRNA的新功能，我们可能会希望排除某些microRNA家族的分析。如果我们不排除它们，它们很可能会增加细胞内已经非常嘈杂的遗传对话的噪音。这只是教授知道而我不知道的大量重要事情之一，因此在认真开始项目之前，进行关于目标、期望和警告的长时间讨论是必要的。
- en: In some sense, and it is a very common one, a project can be deemed successful
    if and only if the customer is satisfied with the results. There are exceptions
    to this guideline, but nevertheless it’s important always to have the expectations
    and goals in mind during every step of a data science project. Unfortunately,
    in my own experience, expectations aren’t usually clear or obvious at the very
    beginning of a project, or they’re not easy to formulate concisely. I’ve settled
    on a few practices that help me figure out reasonable goals that can guide me
    through each step of a project involving data science.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，这是一种非常普遍的看法，一个项目只有在客户对结果满意的情况下才能被认为是成功的。虽然有一些例外，但无论如何，在数据科学项目的每个步骤中始终牢记期望和目标是很重要的。不幸的是，根据我的经验，期望通常在项目一开始时并不清晰或明显，或者它们不容易简洁地表达。我已经形成了一些习惯，这些习惯帮助我确定合理的目标，这些目标可以指导我完成涉及数据科学项目的每个步骤。
- en: 2.1.2\. The customer is probably not a data scientist
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 客户可能不是数据科学家
- en: A funny thing about customer expectations is that they may not be appropriate.
    It’s not always—or even usually—the customer’s fault, because the problems that
    data science addresses are inherently complex, and if the customer understood
    their own problem fully, they likely wouldn’t need a data scientist to help them.
    That’s why I always cut customers some slack when they’re unclear in their language
    or understanding, and I view the process of setting expectations and goals as
    a joint exercise that could be said to resemble conflict resolution or relationship
    therapy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 关于客户期望的一个有趣之处在于，它们可能并不合适。这并不总是——甚至通常不是——客户的错，因为数据科学所解决的问题本质上是复杂的，如果客户完全理解他们自己的问题，他们可能不需要数据科学家来帮助他们。这就是为什么当客户在语言或理解上不清楚时，我总是给他们一些宽容，并将设定期望和目标的过程视为一种类似冲突解决或关系治疗的联合练习。
- en: You—the data scientist—and the customer share a mutual interest in completing
    the project successfully, but the two of you likely have different specific motivations,
    different skills, and, most important, different perspectives. Even if you are
    the customer, you can think of yourself as having two halves, one (the data scientist)
    who is focused on getting results and another (the customer) who is focused on
    using those results to do something real, or external to the project itself. In
    this way, a project in data science begins by finding agreement between two personalities,
    two perspectives, that if they aren’t conflicting are at the very least disparate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你——数据科学家——和客户都对项目成功完成有着共同的兴趣，但你们两个可能有着不同的具体动机、不同的技能，最重要的是，不同的视角。即使你是客户，你也可以将自己视为有两个部分，一个（数据科学家）专注于获得结果，另一个（客户）专注于利用这些结果做些实际的事情，或者超出项目本身的事情。这样，数据科学项目就从寻找两个性格、两个视角之间的共识开始，即使它们并不冲突，至少也是不同的。
- en: Although there is not, strictly speaking, a conflict between you and the customer,
    sometimes it can seem that way as you both muddle your way toward some semblance
    of a set of goals that are both achievable (for the data scientist) and helpful
    (for the customer). And, as in conflict resolution and relationship therapy, feelings
    are involved. These feelings can be ideological and driven by personal experience,
    preference, or opinion and may not make sense to the other party. A little patience
    and understanding, without too much judgment, can be extremely beneficial to both
    of you and, more importantly, to the project.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然严格来说，你和客户之间没有冲突，但有时在你们都朝着一些看似可实现（对数据科学家来说）且有益（对客户来说）的目标摸索的过程中，可能会感觉像是那样。而且，正如在冲突解决和关系治疗中一样，情感是存在的。这些情感可能是意识形态的，由个人经验、偏好或意见驱动，可能对另一方来说没有意义。一点耐心和理解，不要过多评判，对你们两个人，更重要的是对项目，都极为有益。
- en: 2.1.3\. Asking specific questions to uncover fact, not opinions
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 提出具体问题以揭示事实，而非意见
- en: When a customer is describing a theory or hypothesis about the system that you’re
    about to investigate, they are almost certainly expressing a mixture of fact and
    opinion, and it can be important to distinguish between the two. For example,
    in a study of cancer development in mice, the biology professor told me, “It is
    well known which genes are cancer related, and this study is concerned with only
    those genes and the microRNAs that inhibit them.” One might be tempted to take
    this statement at face value and analyze data from only the cancer-related genes,
    but this could be a mistake, because there is some ambiguity in the statement.
    Principally, it’s not clear whether other supposedly non-cancer-related genes
    can be involved in auxiliary roles within the complex reactions incited by the
    experiments or whether it is well known and proven that the expression of cancer-related
    genes is entirely independent of other genes. In the case of the former, it wouldn’t
    be a good idea to ignore the data corresponding to non-cancer-related genes, whereas
    in the case of the latter, it might be a good idea. Without resolving this issue,
    it’s not clear which is the appropriate choice. Therefore, it’s important to ask.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户在描述你即将调查的系统中的理论或假设时，他们几乎肯定是在表达事实和观点的混合体，区分这两者可能很重要。例如，在关于小鼠癌症发展的研究中，生物教授告诉我，“众所周知哪些基因与癌症相关，这项研究只关注那些基因以及抑制它们的microRNAs。”有人可能会倾向于直接接受这个陈述并仅分析与癌症相关基因的数据，但这可能是一个错误，因为陈述中存在一些模糊性。原则上，不清楚其他所谓的非癌症相关基因是否可以参与由实验引发的复杂反应的辅助角色，或者是否众所周知且已证明癌症相关基因的表达完全独立于其他基因。在前一种情况下，忽略与非癌症相关基因对应的数据可能不是一个好主意，而在后一种情况下，可能是一个好主意。如果不解决这个问题，就不知道哪种选择是合适的。因此，询问是很重要的。
- en: It’s also important that the question itself be formulated in a way that the
    customer understands. It wouldn’t be wise to ask, for example, “Should I ignore
    the data from the non-cancer-related genes?” This is a question about the practice
    of data science in this specific case, and it falls under your domain, not a biologist’s.
    You should ask, rather, something similar to, “Do you have any evidence that the
    expression of cancer-related genes is independent, in general, of other genes?”
    This is a question about biology, and hopefully the biology professor would understand
    it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，问题本身要以客户理解的方式提出。例如，问“我应该忽略非癌症相关基因的数据吗？”是不明智的。这是一个关于特定案例中数据科学实践的问题，属于你的领域，而不是生物学家。你更应该问类似的问题，比如，“你是否有任何证据表明癌症相关基因的表达通常独立于其他基因？”这是一个关于生物学的问题，希望生物教授能够理解。
- en: In his answer, it is important to distinguish between what he thinks and what
    he knows. If the professor merely thinks that the expression of these genes is
    independent of others, then it’s certainly something to keep in mind throughout
    the project, but you shouldn’t make any important decisions—such as ignoring certain
    data—based on it. If, on the other hand, the professor can cite scientific research
    supporting his claim, then it’s advisable to use this fact to make decisions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的回答中，区分他认为的和已知的是很重要的。如果教授只是认为这些基因的表达与其他基因无关，那么在整个项目中保持这一点当然是有意义的，但你不应该基于此做出任何重要决定——例如忽略某些数据。另一方面，如果教授可以引用支持他主张的科学研究，那么建议使用这个事实来做出决定。
- en: In any project, you, the data scientist, are an expert in statistics and in
    software tools, but the principal subject-matter expert is very often someone
    else, as in the case involving the professor of biology. In learning from this
    subject matter expert, you should ask questions that give you some intuitive sense
    of how the system under investigation works and also questions that attempt to
    separate fact from opinion and intuition. Basing practical decisions on fact is
    always a good idea, but basing them on opinion can be dangerous. The maxim “Trust
    but confirm” is appropriate here. If I had ignored any of the genes in the data
    set, I may very well have missed a crucial aspect of the complex interaction taking
    place among various types of RNA in the cancer experiments. Cancer, it turns out,
    is a very complex disease on the genetic level as well as on the medical one.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何项目中，您作为数据科学家，是统计学和软件工具的专家，但主要领域的专家通常不是您，就像涉及生物教授的案例一样。在向这个领域的专家学习时，您应该提出一些问题，这些问题能给您一些关于正在研究的系统如何工作的直观感觉，以及一些试图区分事实、观点和直觉的问题。基于事实做出实际决策总是好主意，但基于观点则可能很危险。这里适用的格言是“信任但核实”。如果我忽略了数据集中的任何基因，我可能真的会错过癌症实验中各种RNA之间复杂相互作用的关键方面。结果证明，癌症在遗传水平上以及医学水平上都是一个非常复杂的疾病。
- en: '2.1.4\. Suggesting deliverables: guess and check'
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 提出交付成果：猜测并检查
- en: Your customer probably doesn’t understand data science and what it can do. Asking
    them “What would you like to appear in the final report?” or “What should this
    analytic application do?” can easily result in “I don’t know” or, even worse,
    a suggestion that doesn’t make sense. Data science is not their area of expertise,
    and they’re probably not fully aware of the possibilities and limitations of software
    and data. It’s usually best to approach the question of final product with a series
    of suggestions and then to note the customer’s reaction.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您的客户可能并不了解数据科学及其所能带来的功能。询问他们“您希望在最终报告中看到什么？”或者“这个分析应用应该做什么？”很可能会得到“我不知道”或者更糟糕的，一个不合理的建议。数据科学不是他们的专业领域，他们可能并不完全清楚软件和数据的可能性和局限性。通常最好通过一系列建议来提出关于最终产品的问题，然后记录客户的反应。
- en: One of my favorite questions to ask a customer is “Can you give me an example
    of a sentence that you might like to see in a final report?” I might get responses
    such as “I’d like to see something like, ‘MicroRNA-X seems to inhibit Gene Y significantly,’”
    or “Gene Y and Gene Z seem to be expressed at the same levels in all samples tested.”
    Answers like these give a great starting point for conceiving the format of the
    final product. If the customer can give you seed ideas like these, you can expand
    on them to make suggestions of final products. You might then ask, “What if I
    gave you a table of the strongest interactions between specific microRNAs and
    genetic mRNAs?” Maybe the customer would say that this would be valuable—or maybe
    not.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢问客户的问题之一是：“您能给我一个例子，说明您希望在最终报告中看到的句子吗？”我可能会得到这样的回答：“我想看到类似这样的句子，比如‘MicroRNA-X似乎显著抑制基因Y’”，或者‘基因Y和基因Z似乎在所有测试样本中表达水平相同’。”这样的回答为构思最终产品的格式提供了一个很好的起点。如果客户能提供这样的种子想法，您就可以在此基础上提出最终产品的建议。然后您可能会问：“如果我给您一个表格，显示特定miRNA和遗传mRNA之间最强的相互作用，会怎样？”也许客户会认为这很有价值——也许不会。
- en: It’s most likely, however, that a customer makes less-clear statements, such
    as “I’d like to know which microRNAs are important in cancer development.” For
    this you’ll need clarification if you hope to complete the project successfully.
    What does *important* mean in a biological sense? How might this importance manifest
    itself in the available data? It’s vital to get answers to these questions before
    proceeding; if you don’t know how microRNA importance might manifest itself in
    the data, how will you know when you’ve found it?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，客户可能做出更不明确的陈述，例如：“我想知道哪些miRNA在癌症发展中很重要。”为了成功完成项目，您需要对此进行澄清。在生物学意义上，“重要”是什么意思？这种重要性如何在现有数据中体现？在继续之前，得到这些问题的答案至关重要；如果您不知道miRNA的重要性如何在数据中体现，您如何知道何时找到了它？
- en: 'One mistake that many others and I have made on occasion is to conflate correlation
    with significance. Some people talk about the confusion of correlation and causation;
    here’s an example: a higher percentage of helmet-wearing cyclists are involved
    in accidents than non-helmet-wearing cyclists. It might be tempting to conclude
    that helmets cause accidents, but this is probably fallacious. The correlation
    between helmets and accidents doesn’t imply that helmets cause accidents; nor
    does it imply that accidents cause the wearing of helmets (directly). In reality,
    cyclists who ride on busier and more dangerous roads are more likely to wear helmets
    and also are more likely to get into accidents. The act of riding on more dangerous
    roads causes both. In the question of helmets and accidents, there’s no direct
    causation despite the existence of correlation. Causation, in turn, is merely
    one example of a way that correlation might be significant. If you’re conducting
    a study on the use of helmets and the rates of accidents, then this correlation
    might be significant even if it doesn’t imply causation. I should stress that
    *significance*, as I use the term, is determined by the project’s goals. This
    knowledge of a helmet–accident correlation could lead to considering (and modeling)
    the level of traffic and danger on each road as part of the project. Significance,
    also, is not guaranteed by correlation. I’m fairly certain that more cycling accidents
    happen on sunny days, but this is because more cyclists are on the road on sunny
    days and not because of any other significant relationship (barring rain). It’s
    not immediately clear to me how I might use this information to further my goals,
    and so I wouldn’t spend much time exploring it. The correlation doesn’t seem to
    have any significance in this particular case.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人和我偶尔犯的一个错误是将相关性等同于显著性。有些人谈论相关性混淆因果关系；以下是一个例子：戴头盔的自行车事故发生率高于不戴头盔的自行车。可能会诱使人们得出结论说头盔导致事故，但这可能是错误的。头盔和事故之间的相关性并不意味着头盔导致事故；也不意味着事故导致人们戴头盔（直接）。实际上，在更繁忙、更危险的路上骑行的自行车手更有可能戴头盔，也更有可能发生事故。在更危险的路上骑行这一行为同时导致了这两者。在头盔和事故的问题上，尽管存在相关性，但没有直接的因果关系。因果关系反过来又是一个相关性可能重要的例子。如果你正在进行关于头盔使用和事故率的调查，那么即使没有因果关系，这种相关性也可能很重要。我必须强调，我所说的*显著性*是由项目目标决定的。关于头盔-事故相关性的知识可能导致考虑（并模拟）每个道路的交通和危险水平作为项目的一部分。显著性也不一定由相关性保证。我相当确信更多的自行车事故发生在晴朗的日子里，但这是因为在晴朗的日子里路上有更多的自行车手，而不是因为任何其他显著的关系（除了雨天）。我并不清楚如何利用这些信息进一步实现我的目标，因此我不会花太多时间探索它。在这种情况下，相关性似乎没有任何意义。
- en: In gene/RNA expression experiments, thousands of RNA sequences are measured
    within only 10–20 biological samples. Such an analysis with far more variables
    (expression levels for each RNA sequence or gene) than data points (samples) is
    called *high-dimensional* or often *under-determined* because there are so many
    variables that some of them are correlated by random chance, and it would be fallacious
    to say that they’re related in a real biological sense. If you present a list
    of strong correlations to the biology professor, he’ll spot immediately that some
    of your reported correlations are unimportant or, worse, contrary to established
    research, and you’ll have to go back and do more analyses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在基因/RNA表达实验中，仅在10-20个生物样本中就测量了成千上万的RNA序列。这种分析变量（每个RNA序列或基因的表达水平）远多于数据点（样本）的情况被称为*高维*或通常称为*欠确定*，因为变量太多，其中一些可能只是随机机会相关联，因此说它们在真实生物学意义上相关是错误的。如果你向生物学教授展示一系列与生物学相关的强相关性，他立刻会指出你报告的一些相关性并不重要，或者更糟糕的是，与既定研究相悖，你将不得不回去做更多分析。
- en: 2.1.5\. Iterate your ideas based on knowledge, not wishes
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. 基于知识而非愿望迭代你的想法
- en: As it’s important, within your acquired domain knowledge, to separate fact from
    opinion, it’s also important to avoid letting excessive optimism make you blind
    to obstacles and difficulties. I’ve long thought that an invaluable skill of good
    data scientists is the ability to foresee potential difficulties and to leave
    open a path around them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在你获得的专业知识领域内，区分事实和观点很重要，同样重要的是避免过度乐观使你忽视障碍和困难。我长期以来认为，优秀数据科学家的一项宝贵技能是预见潜在困难并留出绕过它们的路径。
- en: It’s popular, in the software industry today, to make claims about analytic
    capabilities while they’re still under development. This, I’ve learned, is a tactic
    of salesmanship that often seems necessary, in particular for young startups,
    to get ahead in a competitive industry. When I work with a startup, it always
    makes me nervous when a colleague is actively selling a piece of analytic software
    that I said I *think* I can build but that I’m not 100% sure will work exactly
    as planned, given some limitation of the data I have available. When I make bold
    statements about a hypothetical product, I try to keep them, as much as possible,
    in the realm of things that I’m almost certain I can do. In the case that I can’t,
    I try to have a backup plan that doesn’t involve the trickiest parts of the original
    plan.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今软件行业，在分析功能仍在开发中时宣称其能力是很常见的。据我所知，这是一种销售策略，对于年轻的初创公司来说，在竞争激烈的市场中取得领先似乎是一种必要手段。当我与一家初创公司合作时，如果我的同事正在积极推销一款我声称可以构建但并不完全确定能否按计划工作的分析软件，这总是让我感到紧张，因为考虑到我所拥有的数据的某些限制。当我关于一个假设产品做出大胆声明时，我会尽可能地将其保持在几乎可以确定我能做到的事情的范围内。如果我不能做到，我会尝试制定一个不涉及原始计划中最复杂部分的备用计划。
- en: Imagine you want to develop an application that summarizes news articles. You’d
    need to create an algorithm that can parse the sentences and paragraphs in the
    article and extract the main ideas. It’s possible to write an algorithm that does
    this, but it’s not clear how well it will perform. Summaries may be successful
    in some sense for a majority of articles, but there’s a big difference between
    51% successful and 99% successful, and you won’t know where your particular algorithm
    falls within that range until you’ve built a first version at least. Blindly selling
    and feverishly developing this algorithm might seem like the best idea; hard work
    will pay off, right? Maybe. This task is hard. It’s entirely possible that, try
    as you might, you never get better than 75% success, and maybe that’s not good
    enough from a business perspective. What do you do then? Do you give up and close
    up shop? Do you, only after this failure, begin looking for alternatives?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想开发一个总结新闻文章的应用程序。你需要创建一个算法，该算法可以解析文章中的句子和段落并提取主要观点。编写这样的算法是可能的，但尚不清楚其性能如何。总结可能在某些意义上对大多数文章都有效，但51%的成功率和99%的成功率之间有很大的差距，而且你只有在至少构建了一个版本之后才能知道你的特定算法在这个范围内处于什么位置。盲目地推销和狂热地开发这个算法可能看起来是最好的主意；辛勤的工作会得到回报，对吧？也许吧。这项任务很难。完全有可能，尽管你尽力了，但你永远无法超过75%的成功率，而且从商业角度来看，这可能还不够好。那么你该怎么办？你是放弃并关闭店铺吗？你是在这次失败之后才开始寻找替代方案的吗？
- en: 'Good data scientists know when a task is hard even before they begin. Sentences
    and paragraphs are complicated, random variables that often seem designed specifically
    to thwart any algorithm you might throw at them. In case of failure, I always
    go back to first principles, in a sense. I ask myself: what problem am I trying
    to solve? What is the end goal, beyond summarization?'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的数据科学家在开始之前就知道一项任务有多难。句子和段落是复杂的、随机的变量，它们往往似乎专门设计来挫败你可能会向它们投掷的任何算法。在失败的情况下，我总是回到第一原理，从某种意义上说。我会问自己：我试图解决什么问题？除了总结之外，最终目标是什么？
- en: If the goal is to build a product that makes reading news more efficient, maybe
    there’s another way to address the problem of inefficient news readers. Perhaps
    it’s easier to aggregate similar articles and present them together to the reader.
    Maybe it’s possible to design a better news reader through friendlier design or
    by incorporating social media.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是构建一个使阅读新闻更高效的产品，也许有另一种方法来解决效率低下的新闻读者的难题。也许更容易的做法是将类似的文章聚合在一起并一起呈现给读者。也许可以通过更友好的设计或通过整合社交媒体来设计一个更好的新闻阅读器。
- en: No one ever wants to declare failure, but data science is a risky business,
    and to pretend that failure never happens is a failure in itself. There are always
    multiple ways to address a problem, and formulating a plan that acknowledges a
    likelihood of obstacles and failure can allow you to gain value from minor successes
    along the way, even if the main goal isn’t achieved.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人愿意宣布失败，但数据科学是一个充满风险的业务，假装失败从未发生本身就是一种失败。总有多种方法可以解决问题，制定一个承认可能遇到障碍和失败的计划，可以让你在实现主要目标之前，从微小的成功中获得价值。
- en: A far greater mistake would be to ignore the possibility of failure and also
    the need to test and evaluate the performance of the application. If you assume
    that the product works nearly perfectly, but it doesn’t, delivering the product
    to your customer could be a huge mistake. Can you imagine if you began selling
    an untested application that supposedly summarized news articles, but soon thereafter
    your users began to complain that the summaries were completely wrong? Not only
    would the application be a failure, but you and your company might gain a reputation
    for software that doesn’t work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更大的错误是忽视失败的可能性，以及测试和评估应用程序性能的需要。如果你假设产品几乎完美无缺，但实际上并非如此，将产品交付给客户可能是一个巨大的错误。你能想象如果你开始销售一个据说总结新闻文章但很快用户开始抱怨总结完全错误的应用程序吗？不仅应用程序会失败，你和你的公司可能会因为软件不工作而获得坏名声。
- en: 2.2\. Ask good questions—of the data
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 提出好的问题——针对数据
- en: 'It may seem at first glance that this section could be included with the previous
    one, and I’ve even mentioned a few ways in which good questions may be asked of
    the customer. But in this section I discuss the question as an inquiry not only
    into the knowledge of the customer but also into the capabilities of the data.
    A data set will tell us no more than what we ask of it, and even then, the data
    may not be capable of answering the question. These are the two most dangerous
    pitfalls:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，这个部分似乎可以与前一部分合并，我甚至提到了几种向客户提出好问题的方法。但在这个部分，我讨论的问题不仅是对客户知识的探究，也是对数据能力的探究。数据集所能告诉我们的不会超过我们向它提出的问题，而且即使如此，数据也可能无法回答问题。这是两个最危险的陷阱：
- en: Expecting the data to be able to answer a question it can’t
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望数据能够回答它无法回答的问题
- en: Asking questions of the data that don’t solve the original problem
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据提出的问题不能解决原始问题
- en: Asking questions that lead to informative answers and subsequently improved
    results is an important and nuanced challenge that deserves much more discussion
    than it typically receives. The examples of good, or at least helpful, questions
    I’ve mentioned in previous sections were somewhat specific in their phrasing and
    scope, even if they can apply to many types of projects. In the following subsections,
    I attempt to define and describe a good question with the intent of delivering
    a sort of framework or thought process for generating good questions for an arbitrary
    project. Hopefully you’ll see how it might be possible to ask yourself some questions
    in order to arrive at some useful, good questions you might ask of the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 提出能够引导出有信息量答案并随后改善结果的问题是一个重要且复杂的问题，它值得比通常所得到的更多讨论。我在前几节中提到的良好或至少有帮助的问题在措辞和范围上相当具体，即使它们可以应用于许多类型的项目。在接下来的小节中，我试图定义并描述一个好问题，目的是提供一个框架或思维过程，用于为任意项目生成好问题。希望你能看到，通过提出一些问题，你可能会得到一些有用的、好的问题，你可以向数据提出这些问题。
- en: 2.2.1\. Good questions are concrete in their assumptions
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1. 好问题的假设是具体的
- en: No question is quite as tricky to answer as one that’s based on faulty assumptions.
    But a question based on unclear assumptions is a close second. Every question
    has assumptions, and if those assumptions don’t hold, it could spell disaster
    for your project. It’s important to think about the assumptions that your questions
    require and decide whether these assumptions are safe. And in order for you to
    figure out if the assumptions are safe, they need to be *concrete*, meaning well
    defined and able to be tested.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何问题像基于错误假设的问题那样难以回答。但基于不明确假设的问题紧随其后。每个问题都有假设，如果这些假设不成立，可能会对你的项目造成灾难。考虑你问题的假设并决定这些假设是否安全是很重要的。而且为了你能判断假设是否安全，它们需要是*具体的*，这意味着定义明确且能够被测试。
- en: 'For a brief while I worked at a hedge fund. I was in the quantitative research
    department, and our principal goal was, as with any hedge fund, to find patterns
    in financial markets that might be exploited for monetary benefit. A key aspect
    of the trading algorithms that I worked with was a method for model selection.
    Model selection is to mathematical modeling what trying on pants is to shopping:
    we try many of them, judge them, and then select one or a few that seem to work
    well for us, hoping that they serve us well in the future.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 简短地来说，我曾经在一家对冲基金工作过。我在量化研究部门，我们的主要目标是，就像任何对冲基金一样，寻找金融市场中可能被利用以获得货币收益的模式。我工作的交易算法的一个关键方面是模型选择的方法。模型选择对于数学建模来说，就像试穿裤子购物一样：我们尝试了很多，评判它们，然后选择一个或几个看起来对我们很有用的，希望它们在未来能为我们服务得很好。
- en: Several months after I began working at this hedge fund, another mathematician
    was hired, fresh out of graduate school. She began working directly with the model
    selection aspect of the algorithms. One day, while walking to lunch, she began
    to describe to me how a number of the mathematical models of the commodities markets
    had begun to diverge widely from their long-term average success rates. For example,
    let’s assume that Model A has correctly predicted whether the daily price of crude
    oil has gone up or down 55% of the time over the last three years. But in the
    past four weeks, Model A had been correct only 32% of the time. My colleague informed
    me that because the success rate of Model A had fallen below its long-term average,
    it was bound to pick back up over the next several weeks, and we should bet on
    the predictions of Model A.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我开始在这家对冲基金工作几个月后，又聘请了一位刚从研究生院毕业的数学家。她开始直接参与算法中的模型选择方面。有一天，在去吃午饭的路上，她开始向我描述，许多商品市场的数学模型已经开始与它们长期平均成功率大相径庭。例如，假设模型A在过去三年中有55%的时间正确预测了原油日价的涨跌。但在过去的四周里，模型A只有32%的时间是正确的。我的同事告诉我，由于模型A的成功率已经低于其长期平均水平，它必然会在接下来的几周内恢复，我们应该赌模型A的预测。
- en: Frankly, I was disappointed with my colleague, but hers was an easy mistake
    to make. When a certain quantity—in this case the success rate of Model A—typically
    returns to its long-term mean, it’s known as *mean reversion*, and it’s a famously
    contested assumption of many real-life systems, not the least of which are the
    world’s financial markets.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 实话实说，我对我的同事感到失望，但她的错误很容易犯。当一个特定的数量——在这个例子中是模型A的成功率——通常返回其长期均值时，这被称为*均值回归*，并且它是许多现实生活系统的著名争议假设之一，其中最不多的就是世界金融市场。
- en: Innumerable systems in this world don’t subscribe to mean reversion. Flipping
    a standard coin is one of them. If you flip a coin 100 times and you see heads
    only 32 times, do you think you’re going to see more than 50 heads in the next
    100 tosses? I certainly don’t, at least to the point that I would bet on it. The
    history of a (fair) coin being tossed doesn’t affect the future of the coin, and
    commodities markets are in general the same way. Granted, many funds find exploitable
    patterns in financial markets, but these are the exceptions rather than the rule.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个世界上无数的系统都不遵循均值回归。抛一枚标准硬币就是其中之一。如果你抛硬币100次，只看到32次正面，你认为在接下来的100次投掷中你会看到超过50次正面吗？我肯定不会，至少不会到愿意赌这个程度。一枚（公平的）硬币被抛掷的历史不会影响硬币的未来，商品市场通常也是这样。诚然，许多基金在金融市场中发现了可利用的模式，但这些只是例外而不是规则。
- en: 'The assumption of mean reversion is a great example of a fallacious assumption
    in a question that you might ask the data. In this case, my colleague was asking
    “Will Model A’s success rate return to its long-term average?” and, based on the
    assumption of mean reversion, the answer would be yes: mean reversion implies
    that Model A will be correct more often when it has recently been on a streak
    of incorrectness. But if you don’t assume mean reversion in this case, the answer
    would be “I have no idea.”'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 均值回归的假设是你在问数据时可能遇到的错误假设的一个很好的例子。在这种情况下，我的同事问的是“模型A的成功率会回到其长期平均水平吗？”基于均值回归的假设，答案将是肯定的：均值回归意味着当模型A最近连续出现错误时，它将更频繁地正确。但如果你在这个情况下不假设均值回归，答案将是“我不知道。”
- en: 'It’s extremely important to acknowledge your assumptions—there are always assumptions—and
    to make sure that they are true, or at least to make sure that your results won’t
    be ruined if the assumptions turn out not to be true. But this is easier said
    than done. One way to accomplish this is to break down all of the reasoning between
    your analysis and your conclusion into specific logical steps and to make sure
    that all of the gaps are filled in. In the case of my former colleague, the original
    steps of reasoning were these:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 承认你的假设——总是有假设——并且确保它们是真实的，或者至少确保如果假设被证明是错误的，你的结果不会被破坏，这一点非常重要。但这说起来容易做起来难。实现这一目标的一种方法是将你分析到结论之间的所有推理分解成具体的逻辑步骤，并确保所有差距都被填补。在我以前同事的例子中，原始的推理步骤如下：
- en: The success rate of Model A has recently been relatively low.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型 A 的成功率最近相对较低。
- en: Therefore, the success rate of Model A will be relatively high in the near future.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，模型 A 在近期内的成功率将相对较高。
- en: 'The data tells you 1, and then 2 is the conclusion you draw. If it isn’t obvious
    that a logical step is missing here, it might be easier to see it when you replace
    the success rate of Model A with an arbitrary quantity X that might go up or down
    over time:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 数据告诉你 1，然后 2 是你得出的结论。如果这里缺失的逻辑步骤不明显，那么当你用可能随时间上升或下降的任意数量 X 替换模型 A 的成功率时，可能更容易看到它：
- en: X has gone down recently.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X 最近已经下降。
- en: Therefore, X will go up soon.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，X 将很快上升。
- en: 'Think of all of the things X could be: stock price, rainfall, grades in school,
    bank account balance. For how many of these does the previous logic make sense?
    Is there a missing step? I would argue that there is indeed a missing step. The
    logic should be like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 想想 X 可能是什么：股价、降雨量、学校的成绩、银行账户余额。对于这些中的哪一个，前面的逻辑是有意义的？是否有缺失的步骤？我会争辩说确实有一个缺失的步骤。逻辑应该是这样的：
- en: X has gone down recently.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X 最近已经下降。
- en: Because X always corrects itself towards a certain value, V,
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为 X 总是会纠正自己，朝着某个值 V，
- en: X will go up soon, toward V.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X 将很快上升，朝着 V 方向。
- en: 'Note that the data has told you 1, as before, and you’d like to be able to
    draw the conclusion in 3, but 3 is dependent on 2 being true. Is 2 true? Again,
    think of all of the things X could be. Certainly, 2 is not true for a bank account
    balance or rainfall, so it can’t always be true. You must ask yourself if it’s
    true for the particular quantity you’re examining: do you have any reason to believe
    that, for an arbitrary period, Model A should be correct in its prediction 55%
    of the time? In this case, the only evidence you have that Model A is correct
    55% of the time is that Model A historically has been correct 55% of the time.
    This is something like circular reasoning, which isn’t enough real evidence to
    justify the assumption. Mean reversion shouldn’t be taken as truth, and the conclusion
    that Model A should be correct 55% of the time (or more) in the near future isn’t
    justified.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据已经告诉了你 1，就像之前一样，你希望能够在 3 中得出结论，但 3 依赖于 2 是真实的。2 是真实的吗？再次，想想 X 可能是什么。显然，对于银行账户余额或降雨量来说，2
    不是真实的，所以它不可能总是真实的。你必须问自己，对于你正在检查的特定数量来说，它是否是真实的：你有什么理由相信，在任意时间段内，模型 A 的预测应该有 55%
    的时间是正确的？在这种情况下，你唯一能证明模型 A 有 55% 的时间是正确的证据是，模型 A 历史上有 55% 的时间是正确的。这就像循环推理一样，这不是足够的真实证据来证明假设。均值回归不应该被视为真理，而且模型
    A 在近期内应该有 55% 或更多的时间是正确的这一结论是没有根据的。
- en: As a mathematician, I’ve been trained to separate all analysis, argument, and
    conclusion into logical steps, and this experience has proven itself invaluable
    in making and justifying real-life conclusions and predictions through data science.
    Formal reasoning is probably the skill I value the most among those I learned
    through my mathematics course work in college. One important fact about reasoning
    is—to again emphasize the point I’m trying to make in this section—a false or
    unclear assumption starts you out in a questionable place, and you should make
    every effort to avoid relying on such false assumptions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数学家，我受过训练，将所有分析、论证和结论都分成逻辑步骤，这种经验在通过数据科学做出和证明现实生活中的结论和预测方面证明了自己是无价的。在大学数学课程工作中学到的技能中，形式推理可能是我最重视的。关于推理的一个重要事实是——再次强调我在本节中试图说明的观点——一个错误或不清晰的假设会让你从一个可疑的地方开始，你应该尽一切努力避免依赖这样的错误假设。
- en: '2.2.2\. Good answers: measurable success without too much cost'
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2. 好的答案：可衡量的成功，成本不高
- en: Perhaps shifting focus to the answers to good questions can shed more light
    on what the good question comprises as well as help you decide when your answers
    are sufficient. The answer to a good question should measurably improve the project’s
    situation in some way. The point is that you should be asking questions that,
    whatever the answer, make your job a little easier by moving you closer to a practical
    result.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也许将重点转移到对好问题的答案上，可以更清楚地了解好问题包含的内容，以及帮助你决定何时你的答案足够。好问题的答案应该在某种程度上可衡量地改善项目的状况。关键是，你应该提出无论答案如何都能使你的工作变得更容易，并使你更接近实际结果的问题。
- en: How do you know if answering a question will move you closer to a useful, practical
    result? Let’s return to the idea that one of a data scientist’s most valuable
    traits is their awareness of what might occur and their ability to prepare for
    that. If you can imagine all (or at least most) possible outcomes, then you can
    follow the logical conclusions from them. If you know the logical conclusions—the
    additional knowledge that you can deduce from your new outcome—then you can figure
    out whether they will help you with the goals of your project.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何知道回答一个问题是否会让你更接近有用的、实际的结果？让我们回到这样的想法：数据科学家最宝贵的特质之一是他们对可能发生的事情的意识以及他们为这种情况做准备的能力。如果你能想象所有（或至少大多数）可能的结果，那么你可以从它们中得出逻辑结论。如果你知道逻辑结论——你可以从新的结果中推断出的额外知识——那么你可以弄清楚它们是否会帮助你实现项目的目标。
- en: There can be a wide range of possible outcomes, many of which can be helpful.
    Though this is not an exhaustive list, you can move closer to the goals of your
    project if you ask and answer questions that lead to positive or negative results,
    elimination of possible paths or conclusions, or increasing situational awareness.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有广泛的结果，其中许多是有帮助的。尽管这不是一个详尽的列表，但如果你提出并回答导致正面或负面结果、消除可能的路径或结论或提高情境意识的问题，你就可以更接近你项目的目标。
- en: Both positive and negative results can be helpful. What I call *positive* results
    are those that confirm what you suspected and/or hoped for when you initially
    asked the question. These are helpful, obviously, because they fit into your thought
    processes about the project and also move you directly toward your goals. After
    all, goals are yet-unrealized positive results that, if confirmed, give some tangible
    benefit to your customer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正面和负面结果都可以是有帮助的。我所说的*正面*结果是指那些在你最初提问时你怀疑和/或希望得到的结果。这些结果显然是有帮助的，因为它们符合你对项目的思考过程，并且直接推动你朝着目标前进。毕竟，目标还是未实现的正面结果，如果得到证实，会给你的客户提供一些实际的好处。
- en: '*Negative* results are helpful because they inform you that something that
    you thought is probably true is indeed false. These results usually feel like
    setbacks but, practically speaking, they’re the most informative of all possible
    results. What if you found out that the sun was not going to rise tomorrow, despite
    all of the historical evidence to the contrary? This is an extreme example, but
    can you imagine how informative that would be, if it were confirmed true? It would
    change everything, and you would be very likely one of very few people who knew
    it, given that it was so counterintuitive. In that way, negative results can be
    the most helpful, though often they require you to readjust your goals based on
    the new information. At the very least, negative results force you to rethink
    your project to account for those results, a process that leads to more informed
    choices and a more realistic path for your project.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*负面*结果是有帮助的，因为它们告诉你，你原本认为可能正确的事情实际上是不正确的。这些结果通常感觉像是挫折，但从实际的角度来看，它们是所有可能结果中最具信息量的。假设你发现尽管有所有相反的历史证据，太阳明天不会升起，这会是什么情况？这是一个极端的例子，但你能想象如果它被证实为真，会有多具信息量吗？这会改变一切，而且你很可能会是少数几个知道这一点的人之一，因为它是如此反直觉的。从这种意义上说，负面结果可能是最有帮助的，尽管它们通常需要你根据新的信息重新调整目标。至少，负面结果迫使你重新思考你的项目，以考虑这些结果，这个过程会导致更明智的选择和更现实的项目路径。'
- en: 'As I mentioned in [chapter 1](kindle_split_010_split_000.xhtml#ch01), data
    science is fraught with uncertainty. There are always many possible paths to a
    solution, many possible paths to failure, and even more paths to the gray area
    between success and failure. Evidence of improbability or outright elimination
    of any of these possible paths or conclusions can be helpful to inform and focus
    the next steps of the project. A path can be eliminated or deemed improbable in
    many ways, which might include the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在[第1章](kindle_split_010_split_000.xhtml#ch01)中提到的，数据科学充满了不确定性。通往解决方案的路径总是很多，通往失败的道路也很多，甚至通往成功与失败之间的灰色地带的路径更多。对于任何这些可能路径或结论的不可能性证据或直接排除，都可能有助于通知和聚焦项目的下一步。一条路径可以通过多种方式被排除或被认为是不可能的，这可能包括以下几种：
- en: New information making a path far less likely
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新信息使一条路径的可能性大大降低
- en: New information making other paths far more likely
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新信息使其他路径的可能性大大增加
- en: Technical challenges that make exploring certain paths very difficult or impossible
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术挑战使得探索某些路径非常困难或不可能
- en: If eliminating a path doesn’t seem like it’s helping—maybe it was one of the
    only paths that might have succeeded—keep in mind that your situation has become
    simpler regardless, which can be good. Or take the chance to rethink your set
    of paths and your knowledge of the project. Maybe there’s more data, more resources,
    or something else that you haven’t thought of yet that might help you gain a new
    perspective on the challenges.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果排除一条路径似乎没有帮助——也许它是唯一可能成功的路径之一——请记住，无论如何你的情况已经变得简单了，这可能是好事。或者，利用这个机会重新思考你的路径集合和项目知识。也许有更多数据、更多资源，或者你还没有想到的其他东西，这些都可能帮助你从新的角度看待挑战。
- en: In data science, increasing situational awareness is always good. What you don’t
    know can hurt you, because an unknown quantity will sneak into some aspect of
    your project and ruin the results. A question can be good if it helps you gain
    insight into how a system works or what peripheral events are occurring that affect
    the data set. If you find yourself saying “I wonder if...” at some point, or if
    a colleague does the same, ask yourself if that thought relates to a question
    that can help you gain some context for the project—if not answer some larger,
    more direct question. Being introspective in this way brings some formality and
    procedure to the often fuzzy task of looking for good results.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，提高情境意识总是好的。你不知道的东西可能会伤害你，因为一个未知量会悄悄地进入你项目的某个方面并破坏结果。如果一个问题有助于你深入了解一个系统是如何工作的，或者哪些外围事件正在发生，这些事件会影响数据集，那么这个问题就是好的。如果你在某些时候发现自己说“我想知道如果...”，或者如果同事也有同样的想法，问问自己这个想法是否与一个问题相关，这个问题可以帮助你为项目获得一些背景信息——如果不是回答一个更大、更直接的问题。以这种方式进行内省，为通常模糊的任务寻找好的结果带来了一些正式性和程序性。
- en: 2.3\. Answering the question using data
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 使用数据回答问题
- en: You have good questions, and now you want answers. Answers that provide solutions
    to problems are, after all, the goal of your entire project. Getting an answer
    from a project in data science usually looks something like the formula, or recipe,
    in [figure 2.2](kindle_split_011_split_003.xhtml#ch02fig02). Although sometimes
    one of the ingredients—good question, relevant data, or insightful analysis—is
    simpler to obtain than the others, all three are crucial to getting a useful answer.
    Also, the four adjectives I chose, one in each ingredient (*good*, *relevant*,
    *insightful*) and one in the result (*useful*), should not be ignored, because
    without them the formula doesn’t always work. The product of any old question,
    data, and analysis isn’t always an answer, much less a useful one. It’s worth
    repeating that you always need to be deliberate and thoughtful in every step of
    a project, and the elements of this formula are not exceptions. For example, if
    you have a good question but irrelevant data, an answer will be difficult to find.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你有很好的问题，现在你想要答案。毕竟，提供解决问题的答案是整个项目的目标。从数据科学项目中获得答案通常看起来像[图2.2](kindle_split_011_split_003.xhtml#ch02fig02)中的公式或食谱。尽管有时其中一个成分——好的问题、相关数据或深入的分析——比其他成分更容易获得，但三者对于获得有用的答案都是至关重要的。此外，我选择的四个形容词（每个成分中有一个：*好的*、*相关的*、*有洞察力的*）和结果中的一个（*有用的*）不应被忽视，因为没有它们，公式并不总是有效。任何旧问题、数据和分析的产物并不总是答案，更不用说有用的答案了。值得重申的是，你需要在项目的每个步骤中都深思熟虑，而这个公式的元素也不例外。例如，如果你有一个好的问题但数据不相关，找到答案将很困难。
- en: Figure 2.2\. The recipe for a useful answer in a data science project
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2. 数据科学项目中有用答案的食谱
- en: '![](Images/02fig02.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/02fig02.jpg)'
- en: 2.3.1\. Is the data relevant and sufficient?
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1. 数据是否相关且足够？
- en: 'It’s not always easy to tell whether data is relevant. For example, let’s say
    that you’re building a beer recommendation algorithm. A user will choose a few
    beers that they like, and the algorithm will recommend other beers that they might
    like. You might hypothesize that a beer drinker typically likes certain types
    of beer but not others, and so their favorites tend to cluster into their favorite
    beer types. This is the good question: do beer drinkers like beers of certain
    types significantly more than others? You have access to a data set from a popular
    beer-rating website, composed of one- to five-star ratings by thousands of site
    users. You’d like to test your hypothesis using this data; is this data relevant?'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总是容易判断数据是否相关。例如，假设你正在构建一个啤酒推荐算法。用户会选择一些他们喜欢的啤酒，算法将推荐他们可能喜欢的其他啤酒。你可能假设啤酒爱好者通常喜欢某些类型的啤酒而不喜欢其他类型的啤酒，因此他们的喜好倾向于聚集在他们最喜欢的啤酒类型中。这是一个好问题：啤酒爱好者是否对某些类型的啤酒比对其他类型的啤酒有显著偏好？你有一个来自流行的啤酒评分网站的数据集，由数千名网站用户的一到五星级评分组成。你希望使用这些数据来测试你的假设；这些数据是否相关？
- en: 'Fairly soon, you realize that the data set is a CSV file containing only three
    columns: `USER_NAME`, `BEER_NAME`, and `RATING`. (Drat! No beer types.) A data
    set that seemed immensely relevant before now seems less so for this particular
    question. Certainly, for a question about beer types, you need to know, for each
    beer, what type it is. Therefore, to answer the question you must either find
    a data set matching beers to beer types or try to infer the beer types from the
    data you already have, perhaps based on the name of the beer.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 很快你就会意识到数据集是一个只包含三个列的CSV文件：`USER_NAME`、`BEER_NAME`和`RATING`。（糟糕！没有啤酒类型。）之前看似极其相关的数据集现在对这个问题来说似乎不那么相关了。当然，对于一个关于啤酒类型的问题，你需要知道每款啤酒的类型。因此，为了回答这个问题，你必须找到匹配啤酒和啤酒类型的数据集，或者尝试从你已有的数据中推断啤酒类型，可能基于啤酒的名称。
- en: 'Either way, it should be apparent that the data set that at first glance seemed
    perfectly capable of answering the question needs some additional resources in
    order to do so. A data scientist with some foresight and awareness can anticipate
    these sorts of problems before they cost you or your colleagues time and/or money.
    The first step is to outline specifically how data will help you answer your question.
    In the case of the investigation of affinity for beer types, such a statement
    might suffice:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，都应该很明显，最初看起来能够完美回答问题的数据集需要一些额外的资源才能做到这一点。一个有远见和意识的数据科学家可以在这些问题耗费你或你的同事时间或金钱之前预见这些问题。第一步是具体说明数据将如何帮助你回答你的问题。在调查对啤酒类型的亲和力的情况下，这样的声明可能就足够了：
- en: In order to find out whether beer drinkers like certain types of beers significantly
    more than others, we need data containing beer name, beer type, user names, and
    their individual ratings for the beers. With this data, we can perform a statistical
    test, such as an analysis of variance (ANOVA), with each beer type as a variable,
    and examine whether beer type is a significant influencer of rating for individual
    users.
  id: totrans-201
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了找出啤酒爱好者是否对某些类型的啤酒比对其他类型的啤酒有显著偏好，我们需要包含啤酒名称、啤酒类型、用户名以及他们对啤酒的个别评分的数据。有了这些数据，我们可以进行统计分析，例如方差分析（ANOVA），将每种啤酒类型作为一个变量，并检查啤酒类型是否对个别用户的评分有显著影响。
- en: Disregarding the lack of specific detail in the description of the statistical
    test (it’s not important here, but we’ll return to it in a later chapter), you
    have here a basic outline of what might be done to answer the question using data
    you believe to be available. There may be other such outlines that serve the purpose
    equally well or better, but a good outline states what data you would need and
    how you would use that data to answer the question. By stating what specific properties
    the data set should have, you—or anyone you’re working with—can check quickly
    to see if a data set (or more than one) fulfills the requirements.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略统计测试描述中缺乏具体细节的问题（在这里并不重要，但我们在后面的章节中会回到这个问题），你这里有一个基本的大纲，说明如何使用你认为可用的数据来回答问题。可能有其他类似的方案也能同样好或更好地达到目的，但一个好的大纲会说明你需要哪些数据以及你将如何使用这些数据来回答问题。通过说明数据集应具有的具体属性，你或你合作的人可以快速检查数据集（或多个数据集）是否满足要求。
- en: I (like many people I know) have on occasion begun creating algorithms based
    on data I *thought* I had instead of the data I *actually* had. When I realized
    my mistake, I had wasted some time (thankfully not much) on code that was worthless.
    By making a short outline of how you’re going to answer a question using data,
    it would be very easy to check to make sure that a data set you’re considering
    using contains all the information you’ve listed as a requirement—before you start
    coding. If the data set is lacking a vital piece of information, you can then
    adjust your plan either to find the missing piece elsewhere or to devise another
    plan/outline that doesn’t require it. Planning during this stage of a data science
    project can save you quite a lot of time and effort later. Having to modify code
    heavily at a late stage of a project or scrap it altogether is not usually an
    efficient use of time.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我（就像我认识的大多数人一样）有时会基于我认为拥有的数据而不是实际拥有的数据来创建算法。当我意识到我的错误时，我已经在毫无价值的代码上浪费了一些时间（幸运的是，不多）。通过简要概述你打算如何使用数据来回答问题，你就可以很容易地检查确保你打算使用的数据集包含了你列出的所有必要信息——在你开始编码之前。如果数据集缺少关键信息，你可以调整你的计划，要么在其他地方找到缺失的部分，要么制定另一个不需要它的计划/大纲。在数据科学项目的这个阶段进行规划可以节省你以后大量的时间和精力。在项目后期大量修改代码或完全废弃它通常不是有效利用时间的方式。
- en: In the following subsections, I outline some steps you can use to develop a
    solid but detailed plan to find and use data to answer a specific question. In
    later chapters, I’ll discuss gathering and using data in detail, but here I’d
    like to cover the basic ideas of what to note and keep in mind throughout the
    project. You usually have many choices to make, and if those choices later turn
    out to be inefficient—or plain wrong—then it helps to have a record of other choices
    you could have made instead.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我概述了一些你可以使用的步骤，以制定一个坚实而详细的计划来寻找和使用数据来回答特定的问题。在后面的章节中，我将详细讨论收集和使用数据，但在这里我想介绍在整个项目中需要注意和牢记的基本思想。你通常有很多选择，如果这些选择后来证明效率低下——或者完全错误——那么记录下你可以做出的其他选择会很有帮助。
- en: 2.3.2\. Has someone done this before?
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 以前有人做过吗？
- en: 'This should always be the first step in developing a plan: check the internet
    for blog posts, scientific articles, open-source projects, research descriptions
    from universities, or anything else you can find related to the project you’re
    starting. If someone else has done something similar, you may gain a lot of insight
    into the challenges and capabilities that you haven’t yet encountered. Again,
    awareness is very helpful.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是制定计划的第一个步骤：在网上搜索博客文章、科学文章、开源项目、大学的研究描述或任何你可以找到的与你要开始的项目相关的其他内容。如果其他人已经做了类似的事情，你可能会对尚未遇到的挑战和能力获得很多洞察。再次强调，意识是非常有帮助的。
- en: Regarding similar projects that others have done, you’re likely to encounter
    similar problems and similar solutions in your project, so it’s best to learn
    as much as you can about what you should watch out for and how to handle it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 关于其他人已经完成的项目，你可能会在你的项目中遇到类似的问题和类似的解决方案，因此最好尽可能多地了解你应该注意什么以及如何处理这些问题。
- en: Sometimes, a little searching will lead you to useful data sets you hadn’t seen
    yet, analytic methods you may not have considered, or, best of all, results or
    conclusions that you can use in your own project. Presuming that the analyses
    were rigorous—but it’s always best to verify—someone else may have done a lot
    of your work for you.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一点搜索可能会带你找到你之前未见过的有用数据集，你可能没有考虑过的分析方法，或者最好的是，你可以用于你自己的项目的结果或结论。假设分析是严谨的——但总是最好进行验证——其他人可能已经为你做了很多工作。
- en: 2.3.3\. Figuring out what data and software you could use
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 确定可以使用哪些数据和软件
- en: Now that you’ve searched around, both for prior similar projects and for relevant
    data sets, you should take stock of what you have and what you still need.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经搜索了相关信息，包括先前的类似项目和相关的数据集，你应该盘点一下你有什么，你还需要什么。
- en: Data
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据
- en: If you can imagine a data set that would help you tremendously, but that data
    set doesn’t exist anywhere, it often helps to make a note about it in case you
    realize something can be done later. For instance, if you’re missing the beer
    type labels in the data set of beer ratings, you may find that many breweries
    have the types listed on their websites or that other web pages do. This provides
    the opportunity to collect them yourself, potentially at a cost of time and effort.
    If you make a note of this need and this potential solution, then at any point
    in the future you can reevaluate the costs and benefits and make a decision about
    how to proceed. Because of the many uncertainties associated with data-centric
    projects, costs and benefits of possible choices may change in scope or scale
    at virtually any time.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想象出一个可以极大地帮助你数据集，但这个数据集在任何地方都不存在，那么在将来你意识到可以做什么的时候，记下这一点通常很有帮助。例如，如果你在啤酒评分数据集中缺少啤酒类型标签，你可能会发现许多酿酒厂在他们的网站上列出了这些类型，或者在其他网页上。这为你提供了自己收集它们的机会，可能需要时间和精力。如果你记下这个需求和这个潜在解决方案，那么在未来的任何时候，你都可以重新评估成本和收益，并决定如何继续前进。由于与以数据为中心的项目相关的许多不确定性，可能选择的成本和收益几乎在任何时候都可能发生变化。
- en: Software
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软件
- en: Most data scientists have a favorite tool for data analysis, but that tool may
    not always be appropriate for what you intend to do. It’s usually a good idea
    to think about the analysis you want to do conceptually before you try to match
    that concept to a piece of software that can turn the concept into a reality.
    You may decide that a statistical test can provide a good answer if the data supports
    it, that machine learning can figure out the classifications you need, or that
    a simple database query can answer the question. In any of these cases, you may
    know of a good way to use your favorite tool to perform the analysis, but first
    you might want to consider the format of your data, any data transformations that
    might have to take place, the amount of data you have, the method of loading the
    data into analytic tools, and finally the manner of the analysis in the tools.
    Thinking through all of these steps before you perform them and considering how
    they might work in practice can certainly lead to better choice of software tools.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家都有一个用于数据分析的 favorite 工具，但这个工具可能并不总是适合你打算做的事情。在尝试将这个概念与可以将概念转化为现实的软件相匹配之前，考虑你想要进行的分析的概念通常是一个好主意。你可能决定，如果数据支持，统计测试可以提供好的答案，机器学习可以找出你需要分类，或者简单的数据库查询可以回答问题。在任何这些情况下，你可能知道如何使用你喜欢的工具来执行分析，但首先你可能想要考虑你数据的格式，可能需要进行的任何数据转换，你拥有的数据量，将数据加载到分析工具中的方法，以及最后在工具中的分析方法。在执行这些步骤之前思考所有这些步骤，并考虑它们在实际中可能如何工作，当然可以导致软件工具选择的更好选择。
- en: If you aren’t familiar with many software tools and techniques, you can skip
    this step for now and continue reading, because I cover these in later chapters.
    But for now I merely want to emphasize the importance of thinking through—deliberately—various
    options before choosing any. The decision can have large implications and should
    not be taken lightly.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉许多软件工具和技术，你现在可以跳过这一步，继续阅读，因为我在后面的章节中会涵盖这些内容。但就目前而言，我只想强调在做出任何选择之前，有意识地思考各种选项的重要性。这个决定可能具有重大影响，不应轻率对待。
- en: 2.3.4\. Anticipate obstacles to getting everything you want
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4. 预期实现所有愿望的障碍
- en: 'Here are some questions you might want to ask yourself at this planning stage
    of the project:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的这个规划阶段，你可能想要问自己以下问题：
- en: Is the data easily accessed and extracted?
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否容易访问和提取？
- en: Is there anything you might not know about the data that could be important?
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否知道关于数据的一些可能重要的信息，但你可能不知道？
- en: Do you have enough data?
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否有足够的数据？
- en: Do you have too much data? Will it take too long to process?
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否有太多的数据？处理这些数据是否会花费太多时间？
- en: Is there missing data?
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有缺失的数据？
- en: If you’re combining multiple data sets, are you sure everything will integrate
    correctly? Are names, IDs, and codes the same in both data sets?
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在组合多个数据集，你是否确定一切都会正确集成？两个数据集中名称、ID和代码是否相同？
- en: What happens if your statistical test or other algorithm doesn’t give the result
    you expect?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的统计测试或其他算法没有给出你预期的结果，会发生什么？
- en: Do you have a way to spot-check your results? What if the checks show errors
    somewhere?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有方法来检查你的结果吗？如果检查显示某处有错误怎么办？
- en: Some of these may seem obvious, but I’ve seen enough people—data scientists,
    software engineers, and others—forget to consider these things, and they paid
    for their negligence later. It’s better to remind you to be skeptical at the very
    beginning so that the uncertainty of the task doesn’t cost you nearly as much
    later.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些可能看起来很明显，但我已经看到足够多的人——数据科学家、软件工程师以及其他一些人——忘记考虑这些事情，并且他们在后来的忽视中付出了代价。最好是在一开始就提醒你要持怀疑态度，这样任务的不可确定性就不会给你带来太多的损失。
- en: 2.4\. Setting goals
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 设定目标
- en: I’ve mentioned goals several times in this chapter, but I haven’t yet addressed
    them directly. Though you probably began the project with some goals in mind,
    now is a good time to evaluate them in the context of the questions, data, and
    answers that you expect to be working with.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中多次提到了目标，但尚未直接讨论它们。尽管你可能已经带着一些目标开始了这个项目，但现在是一个很好的时机，在预期的疑问、数据和答案的背景下评估它们。
- en: 'Typically, initial goals are set with some business purpose in mind. If you’re
    not in business—you’re in research, for example—then the purpose is usually some
    external use of the results, such as furthering scientific knowledge in a particular
    field or providing an analytic tool for someone else to use. Though goals originate
    outside the context of the project itself, each goal should be put through a pragmatic
    filter based on data science. This filter includes asking these questions:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，初始目标是在考虑一些商业目的的情况下设定的。如果你不在商业领域——比如你在研究——那么目的通常是外部使用结果，比如在特定领域推进科学知识或为其他人提供分析工具。尽管目标起源于项目本身之外的环境，但每个目标都应该根据数据科学进行实用筛选。这个筛选包括提出以下问题：
- en: What is possible?
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是可能的？
- en: What is valuable?
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是有价值的？
- en: What is efficient?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是高效的？
- en: Applying this filter to all putative goals within the context of the good questions,
    possible answers, available data, and foreseen obstacles can help you arrive at
    a solid set of project goals that are, well, possible, valuable, and efficient
    to achieve.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个筛选应用于所有潜在目标，考虑到好问题、可能的答案、可用数据和预见的障碍，可以帮助你确定一个坚实的目标集，这些目标既可能实现，又具有价值，并且是高效的。
- en: 2.4.1\. What is possible?
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 什么是可能的？
- en: Sometimes it’s obvious what is possible, but sometimes it’s not. In the following
    chapters, I’ll describe how some tasks that might seem easy are not. For instance,
    finding appropriate data, wrangling the data, exploiting the data to get answers,
    designing software to perform the analysis, and confronting any other obstacle
    such as those I referenced in the previous section may affect your ability to
    achieve a certain goal. The more complex a task becomes and the less you know
    about the tasks beforehand, the less likely it is that the task is possible. For
    example, if you think a certain data set exists but you haven’t confirmed it yet,
    then achieving any goal requiring that data set might not be possible. For any
    goal with uncertainties, consider the possibility that achieving the goal might
    be impossible.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有时很明显什么是可能的，但有时并不明显。在接下来的章节中，我将描述一些看似简单的任务实际上并不简单。例如，找到合适的数据，处理数据，利用数据来获取答案，设计软件进行分析，以及面对任何其他障碍，如我在上一节中提到的那些，都可能影响你实现某个目标的能力。任务越复杂，你对任务事先了解得越少，任务实现的可能性就越小。例如，如果你认为某个数据集存在，但你还没有确认，那么实现任何需要该数据集的目标可能是不可能的。对于任何具有不确定性的目标，考虑实现目标可能是不可能的。
- en: 2.4.2\. What is valuable?
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 什么是有价值的？
- en: Some goals give more benefit than others. If resources are scarce, everything
    else being equal, it’s better to pursue the more beneficial goal. In business,
    this might mean pursuing the goal(s) that are expected to give the highest profit
    increase. In academia, this might mean aiming for the most impactful scientific
    publication. Formally considering the expected value of achieving a goal, as I
    suggest here, creates a deliberate context and framework for project planning.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一些目标比其他目标带来更多的利益。如果资源稀缺，在其他条件相同的情况下，追求更有益的目标会更好。在商业领域，这可能意味着追求预期将带来最高利润增长的目标。在学术界，这可能意味着追求最具影响力的科学出版物。正式考虑实现目标预期的价值，正如我这里所建议的，为项目规划创造了一个有意识的背景和框架。
- en: 2.4.3\. What is efficient?
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 什么是高效的？
- en: 'After considering what is possible and what is valuable, you can consider the
    effort and resources it might take to achieve each goal. Then you can approximate
    efficiency via this equation:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了可能性和价值之后，你可以考虑实现每个目标可能需要的努力和资源。然后你可以通过以下方程式来近似效率：
- en: efficiency = value / effort * possibility
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 效率 = 价值 / 努力 * 可能性
- en: The overall efficiency of achieving a goal is the value of achieving it divided
    by the effort it took to achieve it (the value gained per unit of effort) multiplied
    by the possibility (the probability that it will be achieved at all). Efficiency
    goes up with the value of the goal, down if more effort is required, and also
    down if the goal seems less possible of being achieved. This is only a rough calculation,
    and it means more to me conceptually than it does practically, but I do find it
    helpful.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 实现目标的整体效率是实现它的价值除以实现它所需的努力（每单位努力获得的价值）乘以可能性（实现它的概率）。效率随着目标价值的增加而提高，如果需要更多的努力，则效率会下降，如果目标看起来不太可能实现，则效率也会下降。这只是一个粗略的计算，它在概念上对我更有意义，但在实际应用中帮助不大，但我确实觉得它很有帮助。
- en: '2.5\. Planning: be flexible'
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5. 计划：保持灵活
- en: Given all your knowledge of the project, all the research you’ve done so far,
    and all the hypothetical questions you’ve asked yourself about the data and the
    software tools you might use, it’s time to formulate a plan. This should not be
    a plan containing sequential steps with outcomes that are presumed beforehand.
    The uncertainty of data and data science virtually guarantees that something won’t
    turn out the way you expect. It’s a good strategy to think of a few different
    ways that you might achieve your goals. Even the goals themselves can be flexible.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到你对项目的所有了解，你迄今为止所做的研究，以及你对自己可能使用的关于数据和软件工具的假设性问题，现在是制定计划的时候了。这不应该是一个包含事先假设结果的连续步骤的计划。数据和数据科学的不可确定性几乎保证了事情不会按照你预期的方向发展。考虑几种可能实现目标的不同方法是一个好的策略。甚至目标本身也可以是灵活的。
- en: These alternative paths might represent different overarching strategies, but
    in most cases two paths in the plan will diverge wherever there’s an anticipated
    uncertainty, where the two most likely scenarios indicate two different strategies
    for addressing the outcome of the uncertainty. It’s definitely advisable to make
    a plan from the beginning to the first major uncertainty. If you want to stop
    there, it might save you some planning time now, but it’s even better to map out
    all of the most likely paths, in particular if you’re working with multiple people.
    That way, everyone can see where the project is headed and knows from the very
    beginning that there will be problems and detours, but you don’t yet know which
    ones they will be. Such is the life of a data scientist!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些替代路径可能代表不同的总体策略，但在大多数情况下，计划中的两条路径会在预期存在不确定性的地方分叉，即两个最可能的情况表明两种不同的策略来应对不确定性的结果。从一开始就制定一个计划到第一个主要不确定性是非常明智的。如果你想在那里停下来，这可能会节省你一些计划时间，但最好还是规划出所有最可能的路径，特别是如果你与多个人合作。这样，每个人都可以看到项目将走向何方，并且从一开始就知道会有问题和绕路，但你还没有知道它们会是哪些。这就是数据科学家的生活！
- en: Last, the plans you formulate here will be revisited periodically throughout
    the project (and this book), so the early stages of the plans are the most important
    ones. A good goal is to plan the next steps to put you in the best position to
    be well informed the next time you revisit plans and goals. Increasing knowledge
    and reducing uncertainty are always good things.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你在这里制定的计划将在整个项目（以及本书）中定期回顾，因此计划的早期阶段是最重要的。一个好的目标是为下一步制定计划，以便你在下次回顾计划和目标时能获得充分的信息。增加知识和减少不确定性总是好事。
- en: Exercises
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Consider the following scenario:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：
- en: You’re working at a firm that consolidates personal finance data for its customers,
    which are primarily individual consumers. Let’s call this firm Filthy Money, Inc,
    or FMI. Through FMI’s primary product, a web application, customers can view all
    their data in one place instead of needing to log into the websites of each of
    their financial accounts separately. The typical customer has connected several
    of their accounts, such as bank accounts and credit cards, to the FMI app.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你在一家为顾客整合个人财务数据的公司工作，这些顾客主要是个人消费者。让我们称这家公司为 Filthy Money, Inc，简称 FMI。通过 FMI
    的主要产品，一个网络应用程序，客户可以在一个地方查看他们所有的数据，而不是需要分别登录他们每个财务账户的网站。典型的客户已经将他们的几个账户，如银行账户和信用卡，连接到了
    FMI 应用程序。
- en: 'One of FMI’s lead product designers has asked you to help build a new product
    component called Filthy Money Forecasting, which was conceived to provide users
    of the FMI app with near-term forecasts of their accounts and overall financial
    status based on their spending and earning habits. The product designer wants
    to collaborate with you to figure out what is possible and what some good product
    features might be:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: FMI的主要产品设计师要求你帮助构建一个名为“脏钱预测”的新产品组件，该组件旨在根据用户的消费和收入习惯，为FMI应用的用户提供其账户和整体财务状况的短期预测。产品设计师希望与你合作，找出可能性和一些可能的好产品特性：
- en: '**1\.**'
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1.**'
- en: ''
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are three questions you would ask the product designer?
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你会向产品设计师提出哪三个问题？
- en: '**2\.**'
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2.**'
- en: ''
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are three good questions you might ask of the data?
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能会对数据提出哪三个好问题？
- en: '**3\.**'
  id: totrans-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3.**'
- en: ''
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are three possible goals for the project?
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 项目可能的三项目标是什么？
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Stay aware: experience, domain experts, and knowledge of other related projects
    help you plan and anticipate problems before they arise.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持警觉：经验、领域专家和其他相关项目的知识有助于你在问题出现之前进行规划和预测。
- en: Be aware of the customer’s perspective and potential lack of data science knowledge.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意客户的角度和可能缺乏数据科学知识。
- en: Make sure the project focuses on answering questions that are good.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保项目专注于回答好问题。
- en: Take time to think through all possible paths to answering those good questions.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花时间思考所有可能的路径来回答那些好问题。
- en: Set goals using a pragmatic perspective of what the customer wants, the questions
    you’ve developed, and the possible paths to getting answers.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用客户想要、你已开发的问题以及获取答案的可能路径的实用视角来设定目标。
- en: 'Chapter 3\. Data all around us: the virtual wilderness'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章. 我们周围的数据：虚拟荒野
- en: '*This chapter covers*'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Discovering data you may need
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现你可能需要的数据
- en: Interacting with data in various environments
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各种环境中与数据交互
- en: Combining disparate data sets
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合不同的数据集
- en: 'This chapter discusses the principal species of study of the data scientist:
    data. Having possession of data—namely, useful data—is often taken as a foregone
    conclusion, but it’s not usually a good idea to assume anything of the sort. As
    with any topic worthy of scientific examination, data can be hard to find and
    capture and is rarely completely understood. Any mistaken notion about a data
    set that you possess or would like to possess can lead to costly problems, so
    in this chapter, I discuss the treatment of data as an object of scientific study.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论数据科学家研究的主要物种：数据。拥有数据——即有用的数据——通常被视为理所当然，但通常不主张做这样的假设。与任何值得科学研究的主题一样，数据可能难以找到和捕捉，并且很少完全被理解。对你拥有的或希望拥有的数据集的任何错误观念都可能导致代价高昂的问题，因此在本章中，我讨论了将数据作为科学研究对象的处理方式。
- en: 3.1\. Data as the object of study
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 数据作为研究对象
- en: 'In recent years, there has been a seemingly never-ending discussion about whether
    the field of data science is merely a reincarnation or an offshoot—in the Big
    Data Age—of any of a number of older fields that combine software engineering
    and data analysis: operations research, decision sciences, analytics, data mining,
    mathematical modeling, or applied statistics, for example. As with any trendy
    term or topic, the discussion over its definition and concept will cease only
    when the popularity of the term dies down. I don’t think I can define *data science*
    any better than many of those who have done so before me, so let a definition
    from Wikipedia ([https://en.wikipedia.org/wiki/Data_science](https://en.wikipedia.org/wiki/Data_science)),
    paraphrased, suffice:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，关于数据科学领域是否仅仅是大数据时代一些较老领域（如运筹学、决策科学、分析、数据挖掘、数学建模或应用统计学等）的转世或分支的讨论似乎永无止境。与任何流行术语或话题一样，关于其定义和概念的讨论只有在术语的流行度下降后才会停止。我认为我无法比之前那些定义数据科学的人做得更好，所以这里引用维基百科（[https://en.wikipedia.org/wiki/Data_science](https://en.wikipedia.org/wiki/Data_science)）的定义，稍作改写，就足够了：
- en: Data science is the extraction of knowledge from data.
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据科学是从数据中提取知识。
- en: Simple enough, but that description doesn’t distinguish data *science* from
    the many other similar terms, except perhaps to claim that *data science* is an
    umbrella term for the whole lot. On the other hand, this era of data science has
    a property that no previous era had, and it is, to me, a fairly compelling reason
    to apply a new term to the types of things that data scientists do that previous
    applied statisticians and data-oriented software engineers did not. This reason
    helps me underscore an often-overlooked but very important aspect of data science,
    as shown in [figure 3.1](kindle_split_012_split_001.xhtml#ch03fig01).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然足够简单，但这个描述并没有区分数据*科学*与其他许多类似术语，除了可能声称*数据科学*是一个涵盖所有这些术语的总称。另一方面，这个数据科学时代有一个以前时代所不具备的特性，对我来说，这是一个相当有说服力的理由来为新术语命名，这些术语用于描述数据科学家所做的事情，而以前的应⽤统计学家和数据导向的软件工程师并没有做这些事情。这个理由帮助我强调了数据科学中经常被忽视但非常重要的一个方面，如图3.1[figure
    3.1](kindle_split_012_split_001.xhtml#ch03fig01)所示。
- en: 'Figure 3.1\. The second step of the preparation phase of the data science process:
    exploring available data'
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1. 数据科学过程准备阶段第二步：探索可用数据
- en: '![](Images/03fig01_alt.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig01_alt.jpg)'
- en: 3.1.1\. The users of computers and the internet became data generators
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 计算机和互联网的用户成为数据生成者
- en: Throughout recent history, computers have made incredible advances in computational
    power, storage, and general capacity to accomplish previously unheard-of tasks.
    Every generation since the invention of the modern computer nearly a century ago
    has seen ever-shrinking machines that are orders of magnitude more powerful than
    the most powerful supercomputers of the previous generation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的历史中，计算机在计算能力、存储能力和完成以前闻所未闻的任务的通用能力方面取得了惊人的进步。自近一个世纪前现代计算机发明以来，每一代计算机都见证了机器的体积不断缩小，其性能比上一代最强大的超级计算机高几个数量级。
- en: The time period including the second half of the twentieth century through the
    beginning of the twenty-first, and including the present day, is often referred
    to as the Information Age. The Information Age, characterized by the rise to ubiquity
    of computers and then the internet, can be divided into several smaller shifts
    that relate to analysis of data.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 包括二十世纪后半叶至二十一世纪初以及现在的这个时期，通常被称为信息时代。信息时代，以计算机和互联网的普及为特征，可以细分为几个更小的转变，这些转变与数据分析有关。
- en: First, early computers were used mainly to make calculations that previously
    took an unreasonable amount of time. Cracking military codes, navigating ships,
    and performing simulations in applied physics were among the computationally intensive
    tasks that were performed by early computers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，早期的计算机主要用于进行以前需要不合理时间的计算。破解军事密码、船舶导航以及在应用物理学中进行模拟是早期计算机执行的计算密集型任务。
- en: Second, people began using computers to communicate, and the internet developed
    in size and capacity. It became possible for data and results to be sent easily
    across a large distance. This enabled a data analyst to amass larger and more
    varied data sets in one place for study. Internet access for the average person
    in a developed country increased dramatically in the 1990s, giving hundreds of
    millions of people access to published information and data.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，人们开始使用计算机进行沟通，互联网在规模和容量上得到了发展。数据和信息可以轻松地发送到很远的距离。这使得数据分析师能够在一个地方收集更大、更多样化的数据集进行研究。在20世纪90年代，发达国家的普通人的互联网接入量大幅增加，使数十亿人能够访问发布的信息和数据。
- en: Third, whereas early use of the internet by the populace consisted mainly of
    consuming published content and communication with other people, soon the owners
    of many websites and applications realized that the aggregation of actions of
    their users provided valuable insight into the success of their own product and
    sometimes human behavior in general. These sites began to collect user data in
    the form of clicks, typed text, site visits, and any other actions a user might
    take. Users began to produce more data than they consumed.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，虽然早期互联网的使用主要由公众消费发布的内容和与他人沟通组成，但很快许多网站和应用的所有者意识到，他们的用户行为的聚合提供了对其自身产品成功以及有时是普遍人类行为的宝贵见解。这些网站开始收集用户数据，形式包括点击、输入文本、网站访问以及其他用户可能采取的任何行动。用户开始产生的数据比他们消费的数据更多。
- en: Fourth, the advent of mobile devices and smartphones that are connected to the
    internet made possible an enormous advance in the amount and specificity of user
    data being collected. At any given moment, your mobile device is capable of recording
    and transmitting every bit of information that its sensors can collect (location,
    movement, camera image and video, sound, and so on) as well as every action that
    you take deliberately while using the device. This can potentially be a huge amount
    of information, if you enable or allow its collection.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，移动互联网和智能手机的出现，使得收集用户数据的数量和具体性有了巨大的进步。在任何时刻，您的移动设备都能够记录和传输其传感器所能收集的每一比特信息（位置、移动、摄像头图像和视频、声音等），以及您在使用设备时故意采取的每一个动作。如果您启用或允许其收集，这可能会是一大批信息。
- en: Fifth—though this isn’t necessarily subsequent to the advent of personal mobile
    devices—is the inclusion of data collection and internet connectivity in almost
    everything electronic. Often referred to as the Internet of Things (IoT), these
    can include everything from your car to your wristwatch to the weather sensor
    on top of your office building. Certainly, collecting and transmitting information
    from devices began well before the twenty-first century, but its ubiquity is relatively
    new, as is the availability of the resultant data on the internet in various forms,
    processed or raw, for free or for sale.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第五——尽管这并不一定是个人移动设备出现之后的直接结果——是几乎在所有电子产品中包含数据收集和互联网连接。通常被称为物联网（IoT），这可以包括从您的汽车到您的手表，再到您办公楼顶部的气象传感器。当然，从设备收集和传输信息开始的时间远早于21世纪，但其普遍性相对较新，互联网上以各种形式（处理过的或原始的，免费或出售）可用的结果数据也是如此。
- en: Through these stages of growth of computing devices and the internet, the online
    world became not merely a place for consuming information but a data-collection
    tool in and of itself. A friend of mine in high school in the late 1990s set up
    a website offering electronic greeting cards as a front for collecting email addresses.
    He sold the resulting list of millions of email addresses for a few hundred thousand
    dollars. This is a primitive example of the value of user data for purposes completely
    unrelated to the website itself and a perfect example of something I’m sorry to
    have missed out on in my youth. By the early 2000s, similar-sized collections
    of email addresses were no longer worth nearly this much money, but other sorts
    of user data became highly desirable and could likewise fetch high prices.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算设备和互联网增长的这些阶段，网络世界不仅仅是一个消费信息的地方，本身也成为了一个数据收集工具。我在1990年代末的高中时期有一个朋友，他建立了一个网站，提供电子贺卡作为收集电子邮件地址的幌子。他以几十万美元的价格出售了由此产生的数百万个电子邮件地址列表。这是一个用户数据价值的原始例子，其用途与网站本身完全无关，也是我年轻时很遗憾错过的一个完美例子。到2000年代初，类似规模的电子邮件地址列表不再值这么多钱，但其他类型的用户数据变得非常受欢迎，同样可以卖出高价。
- en: 3.1.2\. Data for its own sake
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 数据本身的价值
- en: As people and businesses realized that user data could be sold for considerable
    sums of money, they began to collect it indiscriminately. Very large quantities
    of data began to pile up in data stores everywhere. Online retailers began to
    store not only everything you bought but also every item you viewed and every
    link you clicked. Video games stored every step your avatar ever took and which
    opponents it vanquished. Various social networks stored everything you and your
    friends ever did.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们和企业意识到用户数据可以以相当大的金额出售时，他们开始不加区分地收集它。大量数据开始堆积在各个数据存储库中。在线零售商开始存储您购买的所有商品，以及您查看的每一项商品和点击的每一个链接。视频游戏存储了您的角色形象所走过的每一步以及它击败的每一个对手。各种社交网络存储了您和您的朋友所做的一切。
- en: The purpose of collecting all of this data wasn’t always to sell it, though
    that happens frequently. Because virtually every major website and application
    uses its own data to optimize the experience and effectiveness of users, site
    and app publishers are typically torn between the value of the data as something
    that can be sold and the value of the data when held and used internally. Many
    publishers are afraid to sell their data because that opens up the possibility
    that someone else will figure out something lucrative to do with it. Many of them
    keep their data to themselves, hoarding it for the future, when they supposedly
    will have enough time to wring all value out of it.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 收集所有这些数据的初衷并不总是为了出售，尽管这种情况经常发生。因为几乎每个主要网站和应用都使用自己的数据来优化用户体验和效果，因此网站和应用发布者通常在数据的销售价值和使用内部价值之间犹豫不决。许多出版商害怕出售他们的数据，因为这可能会让其他人发现用它赚钱的方法。他们中的许多人将数据保留在自己手中，为未来储备，届时他们将有足够的时间榨取数据的全部价值。
- en: Internet juggernauts Facebook and Amazon collect vast amounts of data every
    minute of every day, but in my estimation, the data they possess is largely unexploited.
    Facebook is focused on marketing and advertising revenue, when they have one of
    the largest data sets comprising human behavior all around the world. Product
    designers, marketers, social engineers, and sociologists alike could probably
    make great advances in their fields, both academic and industrial, if they had
    access to Facebook’s data. Amazon, in turn, has data that could probably upend
    many beloved economic principles and create several new ones if it were turned
    over to academic institutions. Or it might be able to change the way retail, manufacturing,
    and logistics work throughout the entire industry.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网巨头Facebook和Amazon每天每分钟都在收集大量的数据，但据我估计，他们所拥有的数据大部分尚未得到充分利用。Facebook专注于营销和广告收入，而他们拥有全球人类行为数据中最大的数据集之一。产品设计师、市场营销人员、社会工程师和社会学家如果能够访问Facebook的数据，很可能会在他们的学术和工业领域取得重大进展。反过来，如果Amazon的数据被学术机构利用，可能会颠覆许多受喜爱的经济原则，并创造几个新的原则。或者它可能能够改变整个行业中零售、制造和物流的工作方式。
- en: These internet behemoths know that their data is valuable, and they’re confident
    that no one else possesses similar data sets of anywhere near the same size or
    quality. Innumerable companies would gladly pay top dollar for access to the data,
    but Facebook and Amazon have—I surmise—aspirations of their own to use their data
    to its fullest extent and therefore don’t want anyone else to grab the resulting
    profits. If these companies had unlimited resources, surely they would try to
    wring every dollar out of every byte of data. But no matter how large and powerful
    they are, they’re still limited in resources, and they’re forced to focus on the
    uses of the data that affect their bottom lines most directly, to the exclusion
    of some otherwise valuable efforts.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些互联网巨头知道他们的数据很有价值，并且他们自信没有其他人拥有如此规模或质量相似的数据集。无数的公司愿意为访问数据支付高价，但Facebook和Amazon——我推测——有自己使用数据的最大潜力的抱负，因此不希望其他人抢走由此产生的利润。如果这些公司拥有无限资源，他们肯定会试图从每个字节的数据中榨取每一美元。但无论他们多么庞大和强大，他们仍然在资源上有限，并且被迫专注于那些最直接影响他们底线的数据用途，而排除了一些其他有价值的工作。
- en: On the other hand, some companies have elected to provide access to their data.
    Twitter is a notable example. For a fee, you can access the full stream of data
    on the Twitter platform and use it in your own project. An entire industry has
    developed around brokering the sale of data, for profit. A prominent example of
    this is the market of data from various major stock exchanges, which has long
    been available for purchase.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一些公司已经选择提供对其数据的访问权限。Twitter是一个显著的例子。通过付费，你可以访问Twitter平台上的完整数据流，并将其用于自己的项目。围绕数据销售的中介行业已经发展起来，以盈利为目的。这一领域的突出例子是来自各个主要证券交易所的数据市场，这些数据长期以来一直可供购买。
- en: Academic and nonprofit organizations often make data sets available publicly
    and for free, but there may be limitations on how you can use them. Because of
    the disparity of data sets even within a single scientific field, there has been
    a trend toward consolidation of both location and format of data sets. Several
    major fields have created organizations whose sole purpose is to maintain databases
    containing as many data sets as possible from that field. It’s often a requirement
    that authors of scientific articles submit their data to one of these canonical
    data repositories prior to publication of their work.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 学术和非营利组织通常会将数据集免费公开提供，但可能对如何使用它们有所限制。由于即使在单个科学领域内数据集的差异，数据集的地理位置和格式的合并趋势已经出现。几个主要领域已经创建了组织，其唯一目的是维护包含尽可能多该领域数据集的数据库。通常，科学文章的作者在发表作品之前必须将这些数据提交给这些经典数据存储库之一。
- en: In whichever form, data is now ubiquitous, and rather than being merely a tool
    that analysts might use to draw conclusions, it has become a purpose of its own.
    Companies now seem to collect data as an end, not a means, though many of them
    claim to be planning to use the data in the future. Independent of other defining
    characteristics of the Information Age, data has gained its own role, its own
    organizations, and its own value.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 不论以何种形式，数据现在无处不在，它已经不再仅仅是分析师用来得出结论的工具，而成为了一个目的本身。公司现在似乎收集数据作为一种目的，而不是手段，尽管他们中的许多人声称计划在未来使用这些数据。独立于信息时代的其他定义特征，数据已经获得了自己的角色、自己的组织和自己的价值。
- en: 3.1.3\. Data scientist as explorer
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 数据科学家作为探险家
- en: In the twenty-first century, data is being collected at unprecedented rates,
    and in many cases it’s not being collected for a specific purpose. Whether private,
    public, for free, for sale, structured, unstructured, big, normal size, social,
    scientific, passive, active, or any other type, data sets are accumulating everywhere.
    Whereas for centuries data analysts collected their own data or were given a data
    set to work on, for the first time in history many people across many industries
    are collecting data first and then asking, “What can I do with this?” Still others
    are asking, “Does the data already exist that can solve my problem?”
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在21世纪，数据以前所未有的速度被收集，在许多情况下，收集数据并不是为了特定的目的。无论是私人、公共、免费、出售、结构化、非结构化、大数据、常规大小、社交、科学、被动、主动，还是任何其他类型，数据集无处不在地积累。而几个世纪以来，数据分析师收集自己的数据或被分配一个数据集来工作，历史上第一次，许多来自多个行业的人首先收集数据，然后问，“我能用这个做什么？”还有其他人问，“有没有现成的数据可以解决我的问题？”
- en: In this way data—all data everywhere, as a hypothetical aggregation—has become
    an entity worthy of study and exploration. In years past, data sets were usually
    collected deliberately, so that they represented some intentional measurement
    of the real world. But more recently the internet, ubiquitous electronic devices,
    and a latent fear of missing out on hidden value in data have led us to collect
    as much data as possible, often on the loose premise that we *might* use it later.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，数据——无处不在的数据，作为一个假设的集合——已经成为一个值得研究和探索的实体。在过去，数据集通常是有意收集的，因此它们代表了现实世界的一些有意测量。但最近，互联网、无处不在的电子设备和对于错过数据中潜在价值的潜在恐惧，导致我们尽可能多地收集数据，通常基于一个松散的前提，即我们*可能*会在将来使用它。
- en: '[Figure 3.2](kindle_split_012_split_001.xhtml#ch03fig02) shows an interpretation
    of four major innovation types in computing history: computing power itself, networking
    and communication between computers, collection and use of big data, and rigorous
    statistical analysis of that big data. By *big data*, I mean merely the recent
    movement to capture, organize, and use any and all data possible. Each of these
    computing innovations begins with a problem that begs to be addressed and then
    goes through four phases of development, in a process that’s similar to the technological
    surge cycle of Carlota Perez (*Technological Revolutions and Financial Capital*,
    Edward Elgar Publishing, 2002) but with a focus on computing innovation and its
    effect on computer users and the general public.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.2](kindle_split_012_split_001.xhtml#ch03fig02) 展示了计算历史中四种主要创新类型的解释：计算能力本身、计算机之间的网络和通信、大数据的收集和使用，以及大数据的严格统计分析。我所说的“大数据”仅仅是指最近捕捉、组织和使用任何可能的数据的运动。每一种计算创新都是从需要解决的一个问题开始的，然后经历四个发展阶段，这个过程类似于卡洛塔·佩雷斯（Carlota
    Perez）的技术革命与金融资本（Technological Revolutions and Financial Capital，Edward Elgar
    Publishing，2002）的技术激增周期，但重点在于计算创新及其对计算机用户和公众的影响。'
- en: Figure 3.2\. We’re currently in the refinement phase of big data collection
    and use and in the widespread adoption phase of statistical analysis of big data.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2。我们目前处于大数据收集和使用的完善阶段，以及大数据统计分析的广泛应用阶段。
- en: '![](Images/03fig02_alt.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig02_alt.jpg)'
- en: 'For each innovation included in the figure, there are five stages:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图中包含的每一项创新都包含五个阶段：
- en: '***Problem—*** There is a problem that computers can address in some way.'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***问题——*** 存在着计算机可以以某种方式解决的问题。'
- en: '***Invention—*** The computing technology that can address that problem is
    created.'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***发明——*** 可以解决该问题的计算技术被创造出来。'
- en: '***Proof/recognition—*** Someone uses the computing technology in a meaningful
    way, and its value is proven or at least recognized by some experts.'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***证明/认可——*** 有人在有意义的方面使用计算技术，其价值得到证明或至少被一些专家认可。'
- en: '***Adoption—*** The newly proven technology is widely put to use in industry.'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***采用——*** 新近证明的技术在工业中得到广泛应用。'
- en: '***Refinement—*** People develop new versions, more capabilities, higher efficiency,
    integrations with other tools, and so on.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***完善——*** 人们开发新的版本，更多功能，更高效率，与其他工具的集成等等。'
- en: Because we’re currently in the refinement phase of big data collection and the
    widespread adoption phase of statistical analysis of that data, we’ve created
    an entire data ecosystem wherein the knowledge that has been extracted is only
    a very small portion of the total knowledge contained. Not only has much of the
    knowledge not been extracted yet, but in many cases the full extent and properties
    of the data set are not understood by anyone except maybe a few software engineers
    who set up the system; the only people who might understand what’s contained in
    the data are people who are probably too busy or specialized to make use of it.
    The aggregation of all of this underutilized or poorly understood data to me is
    like an entirely new continent with many undiscovered species of plants and animals,
    some entirely unfamiliar organisms, and possibly a few legacy structures left
    by civilizations long departed.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们目前处于大数据收集的完善阶段和该数据统计分析的广泛应用阶段，我们创造了一个完整的数据生态系统，其中提取的知识只是包含的总知识的一小部分。不仅许多知识尚未被提取，而且在许多情况下，数据集的全面范围和属性甚至没有人理解，除非是设置系统的少数软件工程师；可能只有那些可能太忙或过于专业而无法利用它的人才能理解数据中包含的内容。所有这些未充分利用或理解不佳的数据的聚合对我来说就像一个全新的大陆，上面有许多未发现的植物和动物物种，一些完全陌生的生物，以及可能是一些由久远文明留下的遗产结构。
- en: 'There are exceptions to this characterization. Google, Amazon, Facebook, and
    Twitter are good examples of companies that are ahead of the curve. They are,
    in some cases, engaging in behavior that matches a later stage of innovation.
    For example, by allowing access to its entire data set (often for a fee), Twitter
    seems to be operating within the *refinement* stage of big data collection and
    use. People everywhere are trying to squeeze every last bit of knowledge out of
    users’ tweets. Likewise, Google seems to be doing a good job of analyzing its
    data in a rigorous statistical manner. Its work on search-by-image, Google Analytics,
    and even its basic text search are good examples of solid statistics on a large
    scale. One can easily argue that Google has a long way to go, however. If today’s
    ecosystem of data is like a largely unexplored continent, then the data scientist
    is its explorer. Much like famous early European explorers of the Americas or
    Pacific islands, a good explorer is good at several things:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这种描述有例外。谷歌、亚马逊、Facebook和Twitter是走在曲线前面的公司的良好例子。在某些情况下，它们正在进行与创新后期阶段相匹配的行为。例如，通过允许访问其整个数据集（通常需要付费），Twitter似乎正在大数据收集和使用的*完善*阶段运营。世界各地的人们都在试图从用户的推文中榨取每一丝知识。同样，谷歌似乎在以严谨的统计方式分析其数据方面做得很好。它在图像搜索、谷歌分析甚至其基本的文本搜索方面的工作都是大规模坚实统计的良例。然而，可以轻松地争论说，谷歌还有很长的路要走。如果今天的数据生态系统就像一个很大程度上未被探索的大陆，那么数据科学家就是其探险家。就像著名的早期欧洲美洲或太平洋岛屿的探险家一样，一个好的探险家擅长几件事情：
- en: Accessing interesting areas
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入有趣领域
- en: Recognizing new and interesting things
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到新和有趣的事物
- en: Recognizing the signs that something interesting might be close
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到可能有趣的事物即将出现的迹象
- en: Handling things that are new, unfamiliar, or sensitive
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理新、不熟悉或敏感的事物
- en: Evaluating new and unfamiliar things
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估新和不熟悉的事物
- en: Drawing connections between familiar things and unfamiliar things
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在熟悉的事物和陌生的事物之间建立联系
- en: Avoiding pitfalls
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免陷阱
- en: An explorer of a jungle in South America may have used a machete to chop through
    the jungle brush, stumbled across a few loose-cut stones, deduced that a millennium-old
    temple was nearby, found the temple, and then learned from the ruins about the
    religious rituals of the ancient tribe. A data scientist might hack together a
    script that pulls some social networking data from a public API, realize that
    a few people compose major hubs of social activity, discover that those people
    often mention a new photo-sharing app in their posts on the social network, pull
    more data from the photo-sharing app’s public API, and in combining the two data
    sets with some statistical analysis learn about the behavior of network influencers
    in online communities. Both cases derive previously unknown information about
    how a society operates. Like an explorer, a modern data scientist typically must
    survey the landscape, take careful note of surroundings, wander around a bit,
    and dive into some unfamiliar territory to see what happens. When they find something
    interesting, they must examine it, figure out what it can do, learn from it, and
    be able to apply that knowledge in the future. Although analyzing data isn’t a
    new field, the existence of data everywhere—often regardless of whether anyone
    is making use of it—enables us to apply the scientific method to discovery and
    analysis of a preexisting world of data. This, to me, is the differentiator between
    data science and all of its predecessors. There’s so much data that no one can
    possibly understand it all, so we treat it as a world unto itself, worthy of exploration.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在南美洲丛林中的探险家可能会用一把大刀砍开丛林灌木，偶然发现几块松散的石头，推断出一个千年古庙就在附近，找到古庙后，从废墟中了解到古代部落的宗教仪式。数据科学家可能会编写一个脚本，从公共API中提取一些社交网络数据，意识到少数人构成了主要的社会活动中心，发现这些人经常在他们社交网络上的帖子中提到一个新的照片分享应用，从照片分享应用的公共API中获取更多数据，然后通过一些统计分析将两个数据集结合起来，了解在线社区中网络影响者的行为。这两种情况都揭示了关于社会运作的先前未知信息。像探险家一样，现代数据科学家通常必须调查地形，仔细注意周围环境，四处走走，然后深入一些不熟悉的领域看看会发生什么。当他们发现有趣的东西时，他们必须检查它，弄清楚它能做什么，从中学习，并且能够将这种知识应用于未来。尽管数据分析不是一个新领域，但数据无处不在的存在——通常无论是否有人在使用它——使我们能够将科学方法应用于对现有数据世界的发现和分析。对我来说，这就是数据科学与所有其前辈之间的区别。数据如此之多，没有人可能全部理解，所以我们将其视为一个独立的世界，值得探索。
- en: 'This idea of data as a wilderness is one of the most compelling reasons for
    using the term *data science* instead of any of its counterparts. To get real
    truth and useful answers from data, we must use the scientific method, or in our
    case, the *data scientific method*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据视为一片荒野的想法，是使用“数据科学”这一术语而不是其任何对应术语的最有说服力的理由之一。要从数据中获得真实的事实和有用的答案，我们必须使用科学方法，或者在我们这个案例中，是“数据科学方法”：
- en: Ask a question.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提出一个问题。
- en: State a hypothesis about the answer to the question.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对问题的答案提出一个假设。
- en: Make a testable prediction that would provide evidence in favor of the hypothesis
    if correct.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提出一个可测试的预测，如果正确，将提供支持假设的证据。
- en: Test the prediction via an experiment involving data.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过涉及数据的实验来测试预测。
- en: Draw the appropriate conclusions through analyses of experimental results.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对实验结果的分析得出适当的结论。
- en: In this way, data scientists are merely doing what scientists have been doing
    for centuries, albeit in a digital world. Today, some of our greatest explorers
    spend their time in virtual worlds, and we can gain powerful knowledge without
    ever leaving our computers.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，数据科学家只是在做科学家几个世纪以来一直在做的事情，尽管是在数字世界中。今天，我们的一些最伟大的探险家把时间花在虚拟世界中，我们可以在不离开电脑的情况下获得强大的知识。
- en: 3.2\. Where data might live, and how to interact with it
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据可能存在的地方以及如何与之互动
- en: Before we dive in to the unexplored wilderness that is the state of data today,
    I’d like to discuss the forms that data might take, what those forms mean, and
    how we might treat them initially. Flat files, XML, and JSON are a few data formats,
    and each has its own properties and idiosyncrasies. Some are simpler than others
    or more suited to certain purposes. In this section, I discuss several types of
    formats and storage methods, some of their strengths and weaknesses, and how you
    might take advantage of them.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨当今数据状态的未知领域之前，我想讨论数据可能采取的形式，这些形式的意义，以及我们最初如何处理它们。平面文件、XML和JSON是一些数据格式，每种格式都有其自身的特性和独特之处。有些比其他更简单，或者更适合某些用途。在本节中，我将讨论几种格式和存储方法，它们的一些优点和缺点，以及你如何利用它们。
- en: Although plenty of people will object to this, I decided to include in this
    section a discussion of databases and APIs as well. Commingling a discussion of
    file formats with software tools for data storage makes sense to me because at
    the beginning of a data science project, any of these formats or data sources
    is a valid answer to the question “Where is the data now?” File, database, or
    API, what the data scientist needs to know is “How do I access and extract the
    data I need?” and so that’s my purpose here.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管很多人可能会反对这一点，但我决定在本节中包含对数据库和API的讨论。将文件格式的讨论与数据存储的软件工具结合起来对我来说是有意义的，因为在数据科学项目的初期，这些格式或数据源中的任何一个都是对“数据现在在哪里？”这一问题的有效回答。文件、数据库或API，数据科学家需要知道的是“我如何访问和提取我需要的数据？”这就是我在这里的目的。
- en: '[Figure 3.3](kindle_split_012_split_002.xhtml#ch03fig03) shows three basic
    ways a data scientist might access data. It could be a file on a file system,
    and the data scientist could read the file into their favorite analysis tool.
    Or the data could be in a database, which is also on a file system, but in order
    to access the data, the data scientist has to use the database’s interface, which
    is a software layer that helps store and extract data. Finally, the data could
    be behind an application programming interface (API), which is a software layer
    between the data scientist and some system that might be completely unknown or
    foreign. In all three cases, the data can be stored and/or delivered to the data
    scientist in any format I discuss in this section or any other. Storage and delivery
    of data are so closely intertwined in some systems that I choose to treat them
    as a single concept: getting data into your analysis tools.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.3](kindle_split_012_split_002.xhtml#ch03fig03) 展示了数据科学家可能访问数据的三种基本方式。这可能是一个文件系统上的文件，数据科学家可以将文件读入他们最喜欢的分析工具中。或者数据可能存储在数据库中，数据库也位于文件系统中，但为了访问数据，数据科学家必须使用数据库的接口，这是一个帮助存储和提取数据的软件层。最后，数据可能位于应用程序编程接口（API）之后，这是数据科学家和可能完全未知或陌生的系统之间的软件层。在这三种情况下，数据可以以本节中讨论的任何格式或任何其他格式存储和/或交付给数据科学家。在某些系统中，存储和交付数据紧密交织在一起，因此我选择将它们视为一个单一的概念：将数据放入你的分析工具中。'
- en: 'Figure 3.3\. Three ways a data scientist might access data: from a file system,
    database, or API'
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3\. 数据科学家可能访问数据的三种方式：从文件系统、数据库或API
- en: '![](Images/03fig03_alt.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图片 3.3](Images/03fig03_alt.jpg)'
- en: 'In no way do I purport to cover all possible data formats or systems, nor will
    I list all technical details. My goal here is principally to give descriptions
    that would make a reader feel comfortable talking about and approaching each one.
    I can still remember when extracting data from a conventional database was daunting
    for me, and with this section I’d like to put even beginners at ease. Only if
    you’re fairly comfortable with these basic forms of data storage and access can
    you move along to the most important part of data science: what the data can tell
    you.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不声称涵盖所有可能的数据格式或系统，也不会列出所有技术细节。我的主要目标是提供描述，让读者感到舒适地讨论和接近每一个。我仍然记得，从传统数据库中提取数据对我来说曾经是令人畏惧的，通过本节，我希望让即使是初学者也能感到轻松。只有在你对这些基本的数据存储和访问形式相当熟悉的情况下，你才能继续前进到数据科学最重要的部分：数据能告诉你什么。
- en: 3.2.1\. Flat files
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 平面文件
- en: '*Flat files* are plain-vanilla data sets, the default data format in most cases
    if no one has gone through the effort to implement something else. Flat files
    are self-contained, and you don’t need any special programs to view the data contained
    inside. You can open a flat file for viewing in a program typically called a text
    editor, and many text editors are available for every major operating system.
    Flat files contain ASCII (or UTF-8) text, each character of text using (most likely)
    8 bits (1 byte) of memory/storage. A file containing only the word `DATA` will
    be of size 32 bits. If there is an end-of-line (EOL) character after the word
    `DATA`, the file will be 40 bits, because an EOL character is needed to signify
    that a line has ended. My explanation of this might seem simplistic to many people,
    but even some of these basic concepts will become important later on as we discuss
    other formats, so I feel it’s best to outline some basic properties of the flat
    file so that we might compare other data formats later.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*平面文件* 是纯文本数据集，在没有人为努力去实现其他格式的情况下，大多数情况下的默认数据格式。平面文件是自包含的，你不需要任何特殊的程序来查看其中包含的数据。你可以使用通常称为文本编辑器的程序打开平面文件进行查看，并且每个主要操作系统都有许多文本编辑器可用。平面文件包含
    ASCII（或 UTF-8）文本，每个文本字符使用（很可能）8位（1字节）的内存/存储。只包含单词 `DATA` 的文件大小将是 32 位。如果在 `DATA`
    单词之后有一个行结束（EOL）字符，文件大小将是 40 位，因为需要一个 EOL 字符来表示一行已经结束。我的解释可能对许多人来说看起来很简单，但即使是一些这些基本概念，在我们讨论其他格式时也会变得很重要，所以我感觉最好概述一下平面文件的一些基本属性，这样我们就可以在以后比较其他数据格式。'
- en: 'There are two main subtypes of the flat file: plain text and delimited. *Plain
    text* is words, as you might type them on your keyboard. It could look like this:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 平面文件主要有两种子类型：纯文本和分隔符。*纯文本* 是键盘上输入的单词。它可能看起来像这样：
- en: '[PRE0]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This file would contain seven lines, or technically eight if there’s an end-of-line
    character at the end of the final line of text. A plain text flat file is a bunch
    of characters stored in one of two (or so) possible very common formats. This
    is not the same as a text document stored in a word processor format, such as
    Microsoft Word or Open-Office Writer. (See the subsection “Common bad formats.”)
    Word processor file formats potentially contain much more information, including
    overhead such as style formats and metadata about the file format itself, as well
    as objects like images and tables that may have been inserted into a document.
    Plain text is the minimal file format for containing words and only words—no style,
    no fancy images. Numbers and some special characters are OK too.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件将包含七行，或者技术上说是八行，如果文本最后一行的末尾有一个行结束字符。纯文本平面文件是一系列字符存储在两种（或更多）非常常见的格式之一中。这不同于存储在文字处理程序格式中的文本文档，例如
    Microsoft Word 或 Open-Office Writer。（参见“常见错误格式”小节。）文字处理程序文件格式可能包含更多信息，包括如样式格式和有关文件格式本身的元数据等开销，以及可能被插入到文档中的图像和表格等对象。纯文本是包含单词和仅单词的最小文件格式——没有样式，没有花哨的图像。数字和一些特殊字符也可以。
- en: 'But if your data contains numerous entries, a delimited file might be a better
    idea. A *delimited file* is plain text but with the stipulation that every so
    often in the file a delimiter will appear, and if you line up the delimiters properly,
    you can make something that looks like a table, with rows, columns, and headers.
    A delimited file might look like this:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你的数据包含大量条目，分隔符文件可能是一个更好的选择。*分隔符文件* 是纯文本，但有一个规定，即文件中每隔一段时间会出现一个分隔符，如果你正确地对齐分隔符，你可以制作出类似表格的东西，有行、列和标题。分隔符文件可能看起来像这样：
- en: '[PRE2]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s call this table `JOBS_2015`, because it represents a fictional set of
    house-painting jobs that started in 2015, with the customer name, ID, paint color,
    and completion status.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这个表格为 `JOBS_2015`，因为它代表了一组虚构的2015年开始的房屋粉刷工作，包括客户姓名、ID、油漆颜色和完成状态。
- en: This table happens to be tab-delimited—or tab-separated value (TSV)—meaning
    that columns are separated by the tab character. If opened in a text editor, such
    a file would usually appear as it does here, but it might optionally display the
    text `\t` where a tab would otherwise appear. This is because a tab, like an end-of-line
    character, can be represented with a single ASCII character, and that character
    is typically represented with `\t`, if not rendered as variable-length whitespace
    that aligns characters into a tabular format.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格恰好是制表符分隔的——或者说制表符分隔值（TSV）——这意味着列由制表符字符分隔。如果在一个文本编辑器中打开，这样的文件通常会像这里一样显示，但可能会选择性地显示文本`\t`，而制表符本应出现的位置。这是因为制表符，就像行尾字符一样，可以用单个ASCII字符表示，而这个字符通常用`\t`表示，除非以可变长度的空白字符渲染，以将字符对齐到表格格式。
- en: 'If `JOBS_2015` were stored as a comma-separated value (CSV) format, it would
    appear like this in a standard text editor:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`JOBS_2015`以逗号分隔值（CSV）格式存储，在标准文本编辑器中它将看起来像这样：
- en: '[PRE3]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The commas have taken the place of the tab characters, but the data is still
    the same. In either case, you can see that the data in the file can be interpreted
    as a set of rows and columns. The rows represent one job each for `Alison`, `Brian`,
    and `Clara`, and the column names on the header (first) line are `NAME`, `ID`,
    `COLOR`, and `DONE`, giving the types of details of the job contained within the
    table.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 逗号取代了制表符字符，但数据仍然是相同的。在任何情况下，您都可以看到文件中的数据可以解释为一组行和列。行代表`Alison`、`Brian`和`Clara`的每个工作，而标题（第一）行上的列名是`NAME`、`ID`、`COLOR`和`DONE`，给出了表中包含的工作的详细信息类型。
- en: Most programs, including spreadsheets and some programming languages, require
    the same number of delimiters on each line (except possibly the header line) so
    that when they try to read the file, the number of columns is consistent, and
    each line contributes exactly one entry to each column. Some software tools don’t
    require this, and they each have specific ways of dealing with varying numbers
    of entries on each line.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数程序，包括电子表格和一些编程语言，要求每行（除可能的首行外）具有相同数量的定界符，以便在它们尝试读取文件时，列数保持一致，并且每行恰好向每列贡献一个条目。一些软件工具不需要这样做，并且它们各自有处理每行条目数量变化的具体方式。
- en: I should note here that delimited files are typically interpreted as tables,
    like spreadsheets. Furthermore, as plain text files can be read and stored using
    a word processing program, delimited files can typically be loaded into a spreadsheet
    program like Microsoft Excel or OpenOffice Calc.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该在这里指出，定界符分隔的文件通常被解释为表格，就像电子表格一样。此外，由于纯文本文件可以使用文字处理程序进行读取和存储，因此定界符分隔的文件通常可以加载到电子表格程序，如Microsoft
    Excel或OpenOffice Calc中。
- en: Any common program for manipulating text or tables can read flat files. Popular
    programming languages all include functions and methods that can read such files.
    My two most familiar languages, Python (the `csv` package) and R (the `read.table`
    function and its variants), contain methods that can easily load a CSV or TSV
    file into the most relevant data types in those languages. For plain text also,
    Python (`readlines`) and R (`readLines`) have methods that read a file line by
    line and allow for the parsing of the text via whatever methods you see fit. Packages
    in both languages—and many others—provide even more functionality for loading
    files of related types, and I suggest looking at recent language and package documentation
    to find out whether another file-loading method better suits your needs.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 任何用于操作文本或表格的常见程序都可以读取平面文件。所有流行的编程语言都包括可以读取此类文件的功能和方法。我最熟悉的两种语言，Python（`csv`包）和R（`read.table`函数及其变体），都包含可以将CSV或TSV文件轻松加载到这些语言中最相关数据类型的方法。对于纯文本，Python（`readlines`）和R（`readLines`）也有按行读取文件的方法，允许通过您认为合适的方法解析文本。这两种语言以及许多其他语言的包提供了加载相关类型文件的功能，我建议查看最近的语言和包文档，以了解是否有其他文件加载方法更适合您的需求。
- en: Without compressing files, flat files are more or less the smallest and simplest
    common file formats for text or tables. Other file formats contain other information
    about the specifics of the file format or the data structure, as appropriate.
    Because they’re the simplest file formats, they’re usually the easiest to read.
    But because they’re so lean, they provide no additional functionality other than
    showing the data, so for larger data sets, flat files become inefficient. It can
    take minutes or hours for a language like Python to scan a flat file containing
    millions of lines of text. In cases where reading flat files is too slow, there
    are alternative data storage systems designed to parse through large amounts of
    data quickly. These are called *databases* and are covered in a later section.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 不压缩文件时，平面文件几乎是文本或表格中最小和最简单的常见文件格式。其他文件格式包含有关文件格式或数据结构的特定信息。由于它们是最简单的文件格式，因此通常最容易阅读。但是，由于它们非常精简，除了显示数据外，它们不提供任何其他功能，因此对于较大的数据集，平面文件变得效率低下。使用Python这样的语言扫描包含数百万行文本的平面文件可能需要几分钟或几小时。在读取平面文件太慢的情况下，有设计用来快速解析大量数据的替代数据存储系统。这些被称为*数据库*，将在后面的章节中介绍。
- en: 3.2.2\. HTML
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. HTML
- en: 'A *markup language* is plain text marked up with tags or specially denoted
    instructions for how the text should be interpreted. The very popular Hypertext
    Markup Language (HTML) is used widely on the internet, and a snippet might look
    like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '*标记语言*是用标签或特别指定的指令标记的纯文本，这些指令说明了文本应该如何被解释。非常流行的超文本标记语言（HTML）在互联网上被广泛使用，一个片段可能看起来像这样：'
- en: '[PRE4]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: An HTML interpreter knows that everything between the `<html>` and `</html>`
    tags should be considered and read like HTML. Similarly, everything between the
    `<body>` and `</body>` tags will be considered the body of the document, which
    has special meaning in HTML rendering. Most HTML tags are of the format `<TAGNAME>`
    to begin the annotation and `</TAGNAME>` to end it, for an arbitrary TAGNAME.
    Everything between the two tags is now treated as being annotated by TAGNAME,
    which an interpreter can use to render the document. The two `<div>` tags in the
    example show how two blocks of text and other content can be denoted, and a class
    called `column` is applied to the `div`, allowing the interpreter to treat a `column`
    instance in a special way.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: HTML解释器知道，在`<html>`和`</html>`标签之间的所有内容都应该被视为HTML并读取。同样，在`<body>`和`</body>`标签之间的所有内容将被视为文档的主体，这在HTML渲染中具有特殊含义。大多数HTML标签的格式为`<TAGNAME>`以开始注释，`</TAGNAME>`以结束注释，其中TAGNAME是任意名称。现在，两个标签之间的所有内容都被视为由TAGNAME注释，解释器可以使用它来渲染文档。示例中的两个`<div>`标签展示了如何标记两块文本和其他内容，并且将一个名为`column`的类应用于`div`，允许解释器以特殊方式处理`column`实例。
- en: HTML is used primarily to create web pages, and so it usually looks more like
    a document than a data set, with a header, body, and some style and formatting
    information. HTML is not typically used to store raw data, but it’s certainly
    capable of doing so. In fact, the concept of *web scraping* usually entails writing
    code that can fetch and read web pages, interpreting the HTML, and scraping out
    the specific pieces of the HTML page that are of interest to the scraper.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: HTML主要用于创建网页，因此它通常看起来更像文档而不是数据集，具有标题、主体以及一些样式和格式化信息。HTML通常不用于存储原始数据，但它当然能够做到这一点。实际上，*网络爬虫*的概念通常涉及编写能够抓取和读取网页、解释HTML并抓取HTML页面中感兴趣的具体部分的代码。
- en: For instance, suppose we’re interested in collecting as many blog posts as possible
    and that a particular blogging platform uses the `<div class="column">` tag to
    denote columns in blog posts. We could write a script that systematically visits
    a blog, interprets the HTML, looks for the `<div class="column">` tag, captures
    all text between it and the corresponding `</div>` tag, and discards everything
    else, before proceeding to another blog to do the same. This is web scraping,
    and it might come in handy if the data you need isn’t contained in one of the
    other more friendly formats. Web scraping is sometimes prohibited by website owners,
    so it’s best to be careful and check the copyright and terms of service of the
    site before scraping.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们感兴趣的是收集尽可能多的博客文章，并且一个特定的博客平台使用 `<div class="column">` 标签来表示博客文章中的列。我们可以编写一个脚本，系统地访问一个博客，解析
    HTML，寻找 `<div class="column">` 标签，捕获它和相应的 `</div>` 标签之间的所有文本，然后丢弃其他所有内容，然后再去访问另一个博客做同样的事情。这是网络爬虫，如果所需数据不包含在其他更友好的格式中，这可能会很有用。网络爬虫有时被网站所有者禁止，因此最好小心行事，在爬虫之前检查网站的版权和服务条款。
- en: 3.2.3\. XML
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. XML
- en: 'Extensible Markup Language (XML) can look a lot like HTML but is generally
    more suitable for storing and transmitting documents and data other than web pages.
    The previous snippet of HTML can be valid XML, though most XML documents begin
    with a tag that declares a particular XML version, such as the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展标记语言（XML）看起来很像 HTML，但通常更适合存储和传输文档和数据，而不仅仅是网页。前面的 HTML 片段可以是有效的 XML，尽管大多数
    XML 文档都是以一个声明特定 XML 版本的标签开始的，如下所示：
- en: '[PRE5]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This declaration helps ensure that an XML interpreter reads tags in the appropriate
    way. Otherwise, XML works similarly to HTML but without most of the overhead associated
    with web pages. XML is now used as a standard format for offline documents such
    as the OpenOffice and Microsoft Office formats. Because the XML specification
    is designed to be machine-readable, it also can be used for data transmission,
    such as through APIs. For example, many official financial documents are available
    publicly in the Extensible Business Reporting Language (XBRL), which is XML-based.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这个声明有助于确保 XML 解释器以适当的方式读取标签。否则，XML 的工作方式与 HTML 类似，但没有与网页相关的许多开销。XML 现在用作离线文档的标准格式，例如
    OpenOffice 和 Microsoft Office 格式。由于 XML 规范旨在机器可读，它也可以用于数据传输，例如通过 API。例如，许多官方财务文件以可扩展商业报告语言（XBRL）的形式公开，而
    XBRL 是基于 XML 的。
- en: 'This is a representation of the first two rows of the table `JOBS_2015` in
    XML:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对表格 `JOBS_2015` 的前两行的 XML 表示：
- en: '[PRE6]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can see that each row of the table is denoted by a `<JOB>` tag, and within
    each `JOB`, the table’s column names have been used as tags to denote the various
    fields of information. Clearly, storing data in this format takes up more disk
    space than a standard table because XML tags take up disk space, but XML is much
    more flexible, because it’s not limited to a row-and-column format. For this reason,
    it has become popular in applications and documents using non-tabular data and
    other formats requiring such flexibility.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，表格的每一行都由一个 `<JOB>` 标签表示，并且在每个 `JOB` 中，表格的列名已被用作标签来表示各种信息字段。显然，以这种格式存储数据比标准表格占用更多的磁盘空间，因为
    XML 标签占用磁盘空间，但 XML 的灵活性要大得多，因为它不受行和列格式的限制。因此，它在需要这种灵活性的应用程序和文档中变得流行。
- en: 3.2.4\. JSON
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. JSON
- en: 'Though not a markup language, JavaScript Object Notation (JSON) is functionally
    similar, at least when storing or transmitting data. Instead of describing a document,
    JSON typically describes something more like a data structure, such as a list,
    map, or dictionary in many popular programming languages. Here’s the data from
    the first two rows of the table `JOBS_2015` in JSON:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是标记语言，但 JavaScript 对象表示法（JSON）在功能上相似，至少在存储或传输数据时是这样。JSON 通常描述的更像是一种数据结构，例如在许多流行的编程语言中的列表、映射或字典。以下是表格
    `JOBS_2015` 的前两行的 JSON 数据：
- en: '[PRE7]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In terms of structure, this JSON representation looks a lot like the XML representation
    you’ve already seen. But the JSON representation is leaner in terms of the number
    of characters needed to express the data, because JSON was designed to represent
    data objects and not as a document markup language. Therefore, for transmitting
    data, JSON has become very popular. One huge benefit of JSON is that it can be
    read directly as JavaScript code, and many popular programming languages including
    Python and Java have natural representations of JSON as native data objects. For
    interoperability between programming languages, JSON is almost unparalleled in
    its ease of use.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构方面，这种 JSON 表示形式看起来与您已经看到的 XML 表示形式非常相似。但 JSON 表示形式在表达数据所需的字符数量上更为精简，因为 JSON
    被设计用来表示数据对象，而不是作为文档标记语言。因此，在传输数据方面，JSON 已经变得非常流行。JSON 的一个巨大好处是它可以直接作为 JavaScript
    代码读取，许多流行的编程语言，包括 Python 和 Java，都有对 JSON 作为原生数据对象的自然表示。对于编程语言之间的互操作性，JSON 几乎无与伦比，其易用性极高。
- en: 3.2.5\. Relational databases
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5. 关系数据库
- en: '*Databases* are data storage systems that have been optimized to store and
    retrieve data as efficiently as possible within various contexts. Theoretically,
    a relational database (the most common type of database) contains little more
    than a set of tables that could likewise be represented by a delimited file, as
    already discussed: row and column names and one data point per row-column pair.
    But databases are designed to search—or *query*, in the common jargon—for specific
    values or ranges of values within the entries of the table.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据库* 是一种数据存储系统，它被优化以在各种环境中尽可能高效地存储和检索数据。从理论上讲，关系数据库（最常见的数据库类型）包含的不仅仅是表格的集合，这些表格同样可以用已讨论的分隔文件来表示：行和列的名称以及每一行-列对的单一数据点。但数据库被设计为搜索——或者用常见的术语来说，*查询*——表格条目中的特定值或值范围。'
- en: 'For example, let’s revisit the `JOBS_2015` table:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们回顾一下 `JOBS_2015` 表：
- en: '[PRE8]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'But this time assume that this table is one of many stored in a database. A
    database query could be stated in plain English as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次假设这个表格是存储在数据库中的许多表格之一。一个数据库查询可以用简单的英语表述如下：
- en: '[PRE9]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This query should return the following:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询应该返回以下内容：
- en: '[PRE10]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s a basic query, and every database has its own language for expressing
    queries like this, though many databases share the same basis query language,
    the most common being Structured Query Language (SQL).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的查询，尽管许多数据库都有自己的语言来表述这样的查询，尽管许多数据库共享相同的查询语言基础，最常见的是结构化查询语言（SQL）。
- en: Now imagine that the table contains millions of rows and you’d like to do a
    query similar to the one just shown. Through some tricks of software engineering,
    which I won’t discuss here, a well-designed database is able to retrieve a set
    of table rows matching certain criteria (a query) much faster than a scan of a
    flat file would. This means that if you’re writing an application that needs to
    search for specific data very often, you may improve retrieval speed by orders
    of magnitude if you use a database instead of a flat file.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，如果表格包含数百万行，并且你想执行与刚才显示的类似的查询。通过一些软件工程的技巧，这里不会详细讨论，一个设计良好的数据库能够比扫描平面文件更快地检索出符合某些标准（查询）的表格行集。这意味着如果你正在编写一个需要经常搜索特定数据的应用程序，如果你使用数据库而不是平面文件，你可能会将检索速度提高几个数量级。
- en: The main reason why databases are good at retrieving specific data quickly is
    the database index. A *database index* is itself a data structure that helps that
    database software find relevant data quickly. It’s like a structural map of the
    database content, which has been sorted and stored in a clever way and might need
    to be updated every time data in the database changes. Database indexes are not
    universal, however, meaning that the administrator of the database needs to choose
    which columns of the tables are to be indexed, if the default settings aren’t
    appropriate. The columns that are chosen to be indexed are the ones upon which
    querying will be most efficient, and so the choice of index is an important one
    for the efficiency of your applications that use that database.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库擅长快速检索特定数据的主要原因在于数据库索引。一个 *数据库索引* 本身就是一个数据结构，它帮助数据库软件快速找到相关数据。它就像数据库内容的结构图，已经以巧妙的方式排序和存储，并且可能需要在数据库中的数据发生变化时更新。然而，数据库索引并非通用，这意味着数据库管理员需要选择哪些表格列应该被索引，如果默认设置不合适的话。被选为索引的列是查询将最有效的列，因此索引的选择对于使用该数据库的应用程序的效率来说是一个重要的选择。
- en: Besides querying, another operation that databases are typically good at is
    joining tables. Querying and joining aren’t the only two things that databases
    are good at, but they’re by far the most commonly utilized reasons to use a database
    over another data storage system. *Joining*, in database jargon, means taking
    two tables of data and combining them to make another table that contains some
    of the information of both of the original tables.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询之外，数据库通常擅长的另一个操作是连接表。查询和连接并不是数据库擅长的唯一两个操作，但它们是使用数据库而不是其他数据存储系统的最常见原因。在数据库术语中，“连接”意味着将两个数据表合并，以创建一个新的表，该表包含两个原始表的一些信息。
- en: 'For example, assume you have the following table, named `CUST_ZIP_CODES`:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个名为 `CUST_ZIP_CODES` 的以下表格：
- en: '[PRE11]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You’d like to investigate which paint colors have been used in which ZIP codes
    in 2015\. Because the colors used on the various jobs are in `JOBS_2015` and the
    customers’ ZIP codes are in `CUST_ZIP_CODES`, you need to join the tables in order
    to match colors with ZIP codes. An inner join matching `ID` from table `JOBS_2015`
    and `CUST_ID` from table `CUST_ZIP_CODES` could be stated in plain English:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 你想调查2015年中哪些油漆颜色被用于哪些ZIP代码。因为各种工作中的颜色在 `JOBS_2015` 表中，而客户的ZIP代码在 `CUST_ZIP_CODES`
    表中，你需要连接这两个表以匹配颜色和ZIP代码。可以使用以下简单的英语描述内连接，匹配 `JOBS_2015` 表的 `ID` 和 `CUST_ZIP_CODES`
    表的 `CUST_ID`：
- en: '[PRE12]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You’re telling the database to first match up the customer ID numbers from the
    two tables and then show you only the two columns you care about. Note that there
    are no duplicate column names between the two tables, so there’s no ambiguity
    in naming. But in practice you’d normally have to use a notation like `CUST_ZIP_CODES.CUST_ID`
    to denote the `CUST_ID` column of `CUST_ZIP_CODES`. I use the shorter version
    here for brevity.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你是在告诉数据库首先匹配两个表中的客户ID号码，然后只显示你关心的两列。请注意，两个表之间没有重复的列名，所以没有命名上的歧义。但在实践中，你通常需要使用类似
    `CUST_ZIP_CODES.CUST_ID` 的表示法来表示 `CUST_ZIP_CODES` 表的 `CUST_ID` 列。这里我使用简短版本以节省篇幅。
- en: 'The result of the join would look like this:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 连接的结果看起来可能如下所示：
- en: '[PRE13]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Joining can be a very big operation if the original tables are big. If each
    table had millions of different IDs, it could take a long time to sort them all
    and match them up. Therefore, if you’re joining tables, you should minimize the
    size of those tables (primarily by number of rows) because the database software
    will have to shuffle all rows of both tables based on the join criteria until
    all appropriate combinations of rows have been created in the new table. Joins
    should be done sparingly and with care.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果原始表很大，连接可能是一个非常庞大的操作。如果每个表都有数百万个不同的ID，那么对所有这些ID进行排序和匹配可能需要很长时间。因此，如果你要连接表，你应该尽量减小这些表的大小（主要是行数），因为数据库软件将不得不根据连接标准对两个表的所有行进行重新排序，直到在新表中创建所有适当的行组合。连接应该谨慎且小心地进行。
- en: It’s a good general rule, if you’re going to query *and* join, to query the
    data *first* before joining. For example, if you care only about the `COLOR` in
    `ZIP_CODE 21230`, it’s usually better to query `CUST_ZIP_CODES` for `ZIP_CODE=21230`
    first and join the result with `JOBS_2015` instead of joining first and then querying.
    This way, there might be far less matching to do, and the execution of the operation
    will be much faster overall. For more information and guidance on optimizing database
    operations, you’ll find plenty of practical database books in circulation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要进行查询和连接操作，一个好的通用规则是先查询数据，然后再进行连接。例如，如果你只关心 `ZIP_CODE 21230` 中的 `COLOR`，通常最好是先查询
    `CUST_ZIP_CODES` 表中的 `ZIP_CODE=21230`，然后将结果与 `JOBS_2015` 表连接，而不是先连接然后查询。这样，可能需要匹配的记录会少得多，整体操作执行速度也会更快。关于优化数据库操作的信息和指导，你可以在市面上找到许多实用的数据库书籍。
- en: You can think of databases in general as well-organized libraries, and their
    indexes are good librarians. A librarian can find the book you need in a matter
    of seconds, when it might have taken you quite a long time to find it yourself.
    If you have a relatively large data set and find that your code or software tool
    is spending a lot of time searching for the data it needs at any given moment,
    setting up a database is certainly worth considering.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将数据库一般地看作是有良好组织的图书馆，它们的索引就像是优秀的图书管理员。图书管理员可以在几秒钟内找到你需要的那本书，而你可能需要花很长时间才能找到它。如果你有一个相对较大的数据集，并且发现你的代码或软件工具在任意时刻都在花费大量时间搜索所需数据，那么设置数据库绝对值得考虑。
- en: 3.2.6\. Non-relational databases
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6. 非关系型数据库
- en: Even if you don’t have tabular data, you might still be able to make use of
    the efficiency of database indexing. An entire genre of databases called *NoSQL*
    (often interpreted as “Not only SQL”) allows for database schemas outside the
    more traditional SQL-style relational databases. Graph databases and document
    databases are typically classified as NoSQL databases.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有表格数据，你也可能仍然能够利用数据库索引的效率。一类名为 *NoSQL*（通常解释为“不仅限于 SQL”）的数据库允许在更传统的 SQL 风格的关系数据库之外使用数据库模式。图数据库和文档数据库通常被归类为
    NoSQL 数据库。
- en: Many NoSQL databases return query results in familiar formats. Elasticsearch
    and MongoDB, for instance, return results in JSON format (discussed in [section
    3.2.4](kindle_split_012_split_002.xhtml#ch03lev2sec7)). Elasticsearch in particular
    is a document-oriented database that’s very good at indexing the contents of text.
    If you’re working with numerous blog posts or books, for example, and you’re performing
    operations such as counting the occurrences of words within each blog post or
    book, then Elasticsearch is typically a good choice, if indexed properly.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NoSQL 数据库以熟悉的格式返回查询结果。例如，Elasticsearch 和 MongoDB 返回的结果以 JSON 格式（在第 3.2.4
    节中讨论）。Elasticsearch 尤其是一个面向文档的数据库，在索引文本内容方面非常出色。例如，如果你正在处理大量的博客文章或书籍，并且执行诸如计算每篇博客文章或书籍中单词出现次数的操作，那么如果正确索引，Elasticsearch
    通常是一个不错的选择。
- en: Another possible advantage of some NoSQL databases is that, because of the flexibility
    of the schema, you can put almost anything into a NoSQL database without much
    hassle. Strings? Maps? Lists? Sure! Why not? MongoDB, for instance, is extremely
    easy to set up and use, but then you do lose some performance that you might have
    gained by setting up a more rigorous index and schema that apply to your data.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 NoSQL 数据库的另一个可能优势是，由于模式的高度灵活性，你可以几乎将任何东西放入 NoSQL 数据库而不会遇到太多麻烦。字符串？映射？列表？当然！为什么不呢？例如，MongoDB
    非常容易设置和使用，但这样你可能就会失去通过设置更严格的索引和适用于你数据的模式所获得的一些性能。
- en: All in all, if you’re working with large amounts of non-tabular data, there’s
    a good chance that someone has developed a database that’s good at indexing, querying,
    and retrieving your type of data. It’s certainly worth a quick look around the
    internet to see what others are using in similar cases.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，如果你正在处理大量非表格数据，那么很可能有人已经开发了一个擅长索引、查询和检索你这类数据的数据库。在网上快速查找一下，看看别人在类似情况下使用什么，这绝对值得。
- en: 3.2.7\. APIs
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.7\. API
- en: An *application programming interface* (API) in its most common forms is a set
    of rules for communicating with a piece of software. With respect to data, think
    of an API as a gateway through which you can make requests and then receive the
    data, using a well-defined set of terms. Databases have APIs; they define the
    language that you must use in your query, for instance, in order to receive the
    data you want.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 一种 *应用程序编程接口* (API) 在其最常见的形式中是一套与软件进行通信的规则。在数据方面，可以将 API 视为一个网关，通过它你可以发出请求并接收数据，使用一组定义良好的术语。数据库有
    API；它们定义了你在查询中必须使用的语言，例如，为了接收你想要的数据。
- en: Many websites also have APIs. Tumblr, for instance, has a public API that allows
    you to ask for and receive information about Tumblr content of certain types,
    in JSON format. Tumblr has huge databases containing all the billions of posts
    hosted on its blogging service. But it has decided what you, as a member of the
    public, can and can’t access within the databases. The methods of access and the
    limitations are defined by the API.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站也有 API。例如，Tumblr 就有一个公开的 API，允许你请求并接收有关 Tumblr 某类内容的详细信息，以 JSON 格式。Tumblr
    拥有包含其博客服务上托管的所有数十亿条帖子的巨大数据库。但它已经决定了作为公众成员的你可以在数据库中访问什么以及不能访问什么。访问方法和限制由 API 定义。
- en: 'Tumblr’s API is a REST API accessible via HTTP. I’ve never found the technical
    definition of *REST API* to be helpful, but it’s a term that people use when discussing
    APIs that are accessible via HTTP—meaning you can usually access them from a web
    browser—and that respond with information in a familiar format. For instance,
    if you register with Tumblr as a developer (it’s free), you can get an API key.
    This API key is a string that’s unique to you, and it tells Tumblr that you’re
    the one using the API whenever you make a request. Then, in your web browser,
    you can paste the URL [http://api.tumblr.com/v2/blog/good.tumblr.com/info?api_key=API_KEY](http://api.tumblr.com/v2/blog/good.tumblr.com/info?api_key=API_KEY),
    which will request information about a particular blog on Tumblr (replacing API_KEY
    with the API key that you were given). After you press Enter, the response should
    appear in your browser window and look something like this (after some reformatting):'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Tumblr 的 API 是一个可通过 HTTP 访问的 REST API。我从未发现 *REST API* 的技术定义很有帮助，但这是一个当人们讨论可通过
    HTTP 访问的 API 时使用的术语——这意味着您通常可以从网页浏览器中访问它们，并且以熟悉的格式返回信息。例如，如果您在 Tumblr 上注册为开发者（免费），您可以得到一个
    API 密钥。这个 API 密钥是一个只属于您的字符串，它告诉 Tumblr 每当您发起请求时，您正在使用 API。然后，在您的网页浏览器中，您可以粘贴 URL
    [http://api.tumblr.com/v2/blog/good.tumblr.com/info?api_key=API_KEY](http://api.tumblr.com/v2/blog/good.tumblr.com/info?api_key=API_KEY)，这将请求有关
    Tumblr 上特定博客的信息（用您获得的 API 密钥替换 API_KEY）。按下 Enter 后，响应应该出现在您的浏览器窗口中，看起来像这样（经过一些重新格式化）：
- en: '[PRE14]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is JSON with some HTML in the `description` field. It contains some metadata
    about the status of the request and then a `response` field containing the data
    that was requested. Assuming you know how to parse JSON strings (and likewise
    HTML), you can use this in a programmatic way. If you were curious about the number
    of likes of Tumblr blogs, you could use this API to request information about
    any number of blogs and compare the numbers of likes that they have received.
    You wouldn’t want to do that, though, from your browser window, because it would
    take a very long time.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在 `description` 字段中包含一些 HTML 的 JSON。它包含有关请求状态的元数据，然后是一个 `response` 字段，包含请求的数据。假设您知道如何解析
    JSON 字符串（以及类似地 HTML），您可以使用它以编程方式。如果您对 Tumblr 博客的点赞数感兴趣，您可以使用此 API 请求有关任何数量博客的信息，并比较它们收到的点赞数。但是，您不应该从您的浏览器窗口中这样做，因为这会花费很长时间。
- en: 'In order to capture the Tumblr API response programmatically, you need to use
    an HTTP or URL package in your favorite programming language. In Python there
    is `urllib`, in Java `HttpUrlConnection`, and R has `url`, but there are many
    other packages for each of these languages that perform similar tasks. In any
    case, you’ll have to assemble the request URL (as a string object/variable) and
    then pass that request to the appropriate URL retrieval method, which should return
    a response similar to the previous one that can be captured as another object/variable.
    Here’s an example in Python:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以编程方式捕获 Tumblr API 的响应，您需要在您喜欢的编程语言中使用 HTTP 或 URL 包。在 Python 中有 `urllib`，在
    Java 中有 `HttpUrlConnection`，而 R 有 `url`，但每种语言都有许多执行类似任务的包。无论如何，您都需要组装请求 URL（作为一个字符串对象/变量），然后将该请求传递给适当的
    URL 获取方法，该方法应返回一个类似于上一个响应的响应，可以捕获为另一个对象/变量。以下是一个 Python 中的示例：
- en: '[PRE15]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After running these lines, the variable `response` should contain a JSON string
    that looks similar to the response shown.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些代码后，变量 `response` 应该包含一个类似于之前显示的 JSON 字符串。
- en: I remember learning how to use an API like this one from Python, and I was a
    bit confused and overwhelmed at first. Getting the request URL exactly right can
    be tricky if you’re assembling it programmatically from various parts (for example,
    base URL, parameters, API key, and so on). But being adept at using APIs like
    this one can be one of the most powerful tools in data collection, because so
    much data is available through these gateways.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得我是从 Python 中学习如何使用这样的 API 的，一开始我有点困惑和不知所措。如果从各个部分（例如，基本 URL、参数、API 密钥等）以编程方式组装请求
    URL，那么得到完全正确的请求 URL 可能有点棘手。但熟练使用这样的 API 可以成为数据收集中最强大的工具之一，因为通过这些网关可以获取到大量数据。
- en: 3.2.8\. Common bad formats
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.8. 常见错误格式
- en: 'It’s no secret that I’m not a fan of the typical suites of office software:
    word processing programs, spreadsheets, mail clients. Thankfully, I’m not often
    required to use them. I avoid them whenever possible and never more so than when
    doing data science. That doesn’t mean that I won’t deal with those files; on the
    contrary, I wouldn’t throw away free data. But I make sure to get away from any
    inconvenient formats as quickly as possible. There usually isn’t a good way to
    interact with them unless I’m using the highly specialized programs that were
    built for them, and these programs typically aren’t capable of the analysis that
    a data scientist usually needs. I can’t remember the last time I did (or saw)
    a solid bit of data science in Microsoft Excel; to me, Excel’s methods for analysis
    are limited, and the interface is unwieldy for anything but looking at tables.
    But I know I’m biased, so don’t mind me if you’re convinced you can do rigorous
    analysis within a spreadsheet. OpenOffice Calc and Microsoft Excel both allow
    you to export individual sheets from a spreadsheet into CSV formats. If a Microsoft
    Word document contains text I’d like to use, I export it either into plain text
    or maybe HTML or XML.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是办公室软件套件的粉丝，这并不是什么秘密：文字处理程序、电子表格、邮件客户端。幸运的是，我很少需要使用它们。只要可能，我都会避免使用它们，尤其是在做数据科学时。这并不意味着我不会处理这些文件；相反，我不会放弃免费的数据。但我确保尽快摆脱任何不便的格式。除非我使用为它们专门构建的高度专业化的程序，否则通常没有很好的方法与它们交互。这些程序通常无法进行数据科学家通常需要的分析。我不记得上一次我是如何（或看到）在Microsoft
    Excel中做（或看到）一段扎实的科学数据；对我来说，Excel的分析方法有限，界面对于查看表格以外的任何东西来说都不方便。但我知道我有偏见，所以如果你确信你可以在电子表格内进行严谨的分析，请不要介意我。OpenOffice
    Calc和Microsoft Excel都允许您将电子表格中的单个工作表导出为CSV格式。如果Microsoft Word文档包含我想使用的文本，我会将其导出为纯文本、HTML或XML格式。
- en: A PDF can be a tricky thing as well. I’ve exported lots of text (or copied and
    pasted) from PDFs into plain text files that I then read into a Python program.
    This is one of my favorite examples of *data wrangling*, a topic I devote an entire
    chapter to, and so for now it will suffice to say that exporting or scraping text
    from a PDF (where possible) is usually a good idea whenever you want to analyze
    that text.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: PDF也可能是一个棘手的东西。我已经从PDF中导出了很多文本（或复制粘贴）到纯文本文件中，然后我将这些文件读入Python程序。这是我最喜欢的数据整理例子之一，我将在整个章节中讨论这个话题，所以现在只需说，在可能的情况下，从PDF（如果可能）导出或抓取文本通常是一个好主意，无论你想要分析该文本。
- en: 3.2.9\. Unusual formats
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.9. 不寻常的格式
- en: This is the umbrella category for all data formats and storage systems with
    which I’m unfamiliar. All sorts of formats are available, and I’m sure someone
    had a good reason to develop them, but for whatever reason they’re not familiar
    to me. Sometimes they’re archaic; maybe they were superseded by another format,
    but some legacy data sets haven’t yet been updated.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有我不熟悉的数据格式和存储系统的总类别。各种格式都可用，我相信有人出于很好的理由开发了它们，但出于某种原因，它们对我来说并不熟悉。有时它们是过时的；也许它们被另一种格式取代了，但一些遗留数据集尚未更新。
- en: Sometimes the formats are highly specialized. I once participated in a project
    exploring the chemical structure of a compound and its connection to the way the
    compound smelled (its aroma). The `RDKit` package ([www.rdkit.org](http://www.rdkit.org))
    provided a ton of helpful functionality for parsing through chemical structures
    and substructures. But much of this functionality was highly specific to chemical
    structure and its notation. Plus the package made heavy use of a fairly sophisticated
    binary representation of certain aspects of chemical structure that greatly improved
    the computational efficiency of the algorithms but also made them extremely difficult
    to understand.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 有时格式非常专业。我曾经参与过一个项目，探索一个化合物的化学结构和它与化合物气味（其香气）的联系。`RDKit`包（[www.rdkit.org](http://www.rdkit.org)）提供了大量有用的功能，用于解析化学结构和子结构。但其中许多功能高度特定于化学结构和其符号。此外，该包对化学结构的某些方面的相当复杂的二进制表示进行了大量使用，这大大提高了算法的计算效率，但也使得它们极其难以理解。
- en: 'Here’s what I do when I encounter a data storage system unlike anything I’ve
    seen before:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 当我遇到一个与我之前所见完全不同的数据存储系统时，我会这样做：
- en: Search and search (and search) online for a few examples of people doing something
    similar to what I want to do. How difficult might it be to adapt these examples
    to my needs?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线搜索并搜索（再搜索）几个与我想要做的事情类似的人的例子。将这些例子适应我的需求可能会有多难？
- en: Decide how badly I want the data. Is it worth the trouble? What are the alternatives?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定我有多想得到这些数据。这是否值得麻烦？有哪些替代方案？
- en: If it’s worth it, I try to generalize from the similar examples I found. Sometimes
    I can gradually expand from examples by fiddling with parameters and methods.
    I try a few things and see what happens.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果值得，我会尝试从找到的类似例子中归纳总结。有时，通过调整参数和方法，我可以逐渐从例子中扩展。我会尝试几件事情，看看会发生什么。
- en: Dealing with completely unfamiliar data formats or storage systems can be its
    own type of exploration, but rest assured that someone somewhere has accessed
    the data before. If no one has ever accessed the data, then someone was completely
    mistaken in creating the data format in the first place. When in doubt, send a
    few emails and try to find someone who can help you.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完全陌生的数据格式或存储系统可能是其自身的一种探索类型，但请放心，某处有人之前已经访问过这些数据。如果没有人访问过这些数据，那么在最初创建数据格式时，有人肯定犯了完全的错误。如有疑问，发送几封电子邮件，尝试找到能帮助你的人。
- en: 3.2.10\. Deciding which format to use
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.10. 决定使用哪种格式
- en: Sometimes you don’t have a choice. The data comes in a certain format, and you
    have to deal with it. But if you find that format inefficient, unwieldy, or unpopular,
    you’re usually free to set up a secondary data store that might make things easier,
    but at the additional cost of the time and effort it takes you to set up the secondary
    data store. For applications where access efficiency is critical, the cost can
    be worth it. For smaller projects, maybe not. You’ll have to cross that bridge
    when you get there.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你并没有选择。数据以某种格式传入，你必须处理它。但如果你发现这种格式效率低下、难以操作或不受欢迎，你通常可以自由地设置一个二级数据存储，这可能使事情变得更容易，但代价是设置二级数据存储所需的时间和精力。对于访问效率至关重要的应用，这种成本可能是值得的。对于较小的项目，可能就不一定了。当你到达那里时，你必须跨过那座桥。
- en: I’ll conclude this section with a few general rules about what data formats
    to use, when you have a choice, and in particular when you’re going to be accessing
    the data from a programming language. [Table 3.1](kindle_split_012_split_002.xhtml#ch03table01)
    gives the most common good format for interacting with data of particular types.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我将以一些关于在可以选择的情况下以及特别是当你打算从编程语言访问数据时使用哪些数据格式的通用规则来结束本节。[表3.1](kindle_split_012_split_002.xhtml#ch03table01)给出了与特定类型数据交互的最常见优秀格式。
- en: Table 3.1\. Some common types of data and formats that are good for storing
    them
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1. 一些常见的数据类型和适合存储它们的格式
- en: '| Type of data | Good, common format |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 优秀的，常见的格式 |'
- en: '| --- | --- |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Tabular data, small amount | Delimited flat file |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 表格数据，数量较少 | 分隔的平面文件 |'
- en: '| Tabular data, large amount with lots of searching/querying | Relational database
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 表格数据，大量且需要大量搜索/查询 | 关系型数据库 |'
- en: '| Plain text, small amount | Flat file |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 纯文本，数量较少 | 平面文件 |'
- en: '| Plain text, large amount | Non-relational database with text search capabilities
    |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 纯文本，大量 | 具有文本搜索功能的关系型数据库 |'
- en: '| Transmitting data between components | JSON |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 在组件之间传输数据 | JSON |'
- en: '| Transmitting documents | XML |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 传输文档 | XML |'
- en: 'And here are a few guidelines for choosing or converting data formats:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些关于选择或转换数据格式的指南：
- en: For spreadsheets and other office documents, export!
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于电子表格和其他办公文档，导出！
- en: More common formats are usually better for your data type and application.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更常见的格式通常更适合你的数据类型和应用。
- en: Don’t spend too much time converting from a certain format to your favorite;
    weigh the costs and benefits first.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要花太多时间从某种格式转换到你的首选格式；首先权衡成本和收益。
- en: Now that I’ve covered many of the forms in which data might be presented to
    you, hopefully you’ll feel somewhat comfortable in a high-level conversation about
    data formats, stores, and APIs. As always, never hesitate to ask someone for details
    about a term or system you haven’t heard of before. New systems are being developed
    constantly, and in my experience, anyone who recently learned about a system is
    usually eager to help others learn about it.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经涵盖了数据可能呈现给你的许多形式，希望你在关于数据格式、存储和API的高级对话中会感到有些舒适。一如既往，不要犹豫，向别人询问你之前没听过的术语或系统的详细信息。新的系统正在不断开发中，根据我的经验，最近了解过某个系统的人通常很乐意帮助他人了解它。
- en: 3.3\. Scouting for data
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 数据侦察
- en: The previous section discussed many of the common forms that data takes, from
    file formats to databases to APIs. I intended to make these data forms more approachable,
    as well as to increase awareness about the ways you might look for data. It’s
    not hard to find data, much like it’s not hard to find a tree or a river (in certain
    climates). But finding the data that can help you solve your problem is a different
    story. Or maybe you already have data from an internal system. It may seem like
    that data can answer the major questions of your project, but you shouldn’t take
    it for granted. Maybe a data set out there will perfectly complement the data
    you already have and greatly improve results. There’s so much data on the internet
    and elsewhere; some part of it should be able to help you. Even if not, a quick
    search is certainly worth it, even for a long-shot possibility.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节讨论了许多数据常见的格式，从文件格式到数据库再到API。我的目的是使这些数据形式更容易接近，并提高您对寻找数据方式的意识。找到数据并不难，就像在特定气候中找到一棵树或一条河一样简单。但找到能帮助您解决问题数据则是另一回事。或者，也许您已经从内部系统中获得了数据。这些数据似乎可以回答您项目的重大问题，但您不应该想当然。也许某个数据集可以完美地补充您已经拥有的数据，并极大地提高结果。互联网和其他地方有如此多的数据；其中一部分应该能够帮助您。即使不能，快速搜索也绝对值得，即使是对可能性很小的搜索也是如此。
- en: In this section, I discuss the act of looking for data that might help you with
    your project. This is the exploration I talked about at the beginning of this
    chapter. Now that you have some exposure to common forms of data from the previous
    section, you can focus less on the format and more on the content and whether
    it can help you.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我讨论了寻找可能帮助您项目的数据的行为。这正是我在本章开头所提到的探索。现在，您已经对上一节中常见的数据形式有所了解，您可以减少对格式的关注，更多地关注内容和它是否能够帮助您。
- en: '3.3.1\. First step: Google search'
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 第一步：谷歌搜索
- en: 'This may seem obvious, but I still feel like mentioning it: Google searches
    are not perfect. To make them work as well as possible, you have to know what
    to search for and what you’re looking for in the search results. Given the last
    section’s introduction to data formats, you now have a little more ammunition
    for Google searches than before.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来很明显，但我仍然觉得有必要提一下：谷歌搜索并不完美。为了使它们尽可能有效，您必须知道要搜索什么以及您在搜索结果中寻找什么。鉴于上一节对数据格式的介绍，您现在在谷歌搜索中比以前有更多的“弹药”。
- en: A Google search for “Tumblr data” gives different results from a search for
    “Tumblr API.” I’m not sure which I prefer, given that I don’t have a specific
    project involving Tumblr at the moment. The former returns results involving the
    term *data* as used on Tumblr posts as well as third parties selling historical
    Tumblr data. The latter returns results that deal almost exclusively with the
    official Tumblr API, which contains considerable up-to-the-minute information
    about Tumblr posts. Depending on your project, one might be better than the other.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌搜索“Tumblr数据”和搜索“Tumblr API”的结果不同。鉴于我目前没有涉及Tumblr的具体项目，我不确定我更喜欢哪一个。前者返回涉及Tumblr帖子中使用的术语*数据*以及第三方销售历史Tumblr数据的结果。后者返回几乎完全与官方Tumblr
    API相关的结果，其中包含有关Tumblr帖子的最新信息。根据您的项目，一个可能比另一个更好。
- en: But it’s definitely worth keeping in mind that terms such as *data* and *API*
    do make a difference in web searches. Try the searches “social networking” and
    “social networking API.” There’s a dramatic difference in results.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 但确实值得记住的是，诸如*数据*和*API*之类的术语在网页搜索中确实会产生影响。尝试搜索“社交网络”和“社交网络API”。结果有明显的差异。
- en: Therefore, when searching for data related to your project, be sure to include
    modifying terms like *historical*, *API*, *real time*, and so on, because they
    do make a difference. Likewise, watch out for them in the search results. This
    may be obvious, but it makes a considerable difference in your ability to find
    what you’re looking for, and so it’s worth repeating.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在搜索与您的项目相关的数据时，务必包括如*历史*、*API*、*实时*等修饰词，因为它们确实会产生影响。同样，在搜索结果中也要注意它们。这可能看起来很明显，但它对您找到所需内容的能力有相当大的影响，因此值得重复。
- en: 3.3.2\. Copyright and licensing
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 版权和许可
- en: 'I’ve talked about searching for, accessing, and using data, but there’s another
    very important concern: are you *allowed* to use it?'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经讨论了搜索、访问和使用数据，但还有一个非常重要的关注点：您*是否允许*使用它？
- en: As with software licenses, data may have licensing, copyright, or other restrictions
    that can make it illegal to use the data for certain purposes. If the data comes
    from academic sources, for example (universities, research institutions, and the
    like), then there’s often a restriction that the data can’t be used for profit.
    Proprietary data, such as that of Tumblr or Twitter, often comes with the restriction
    that you can’t use the data to replicate functionality that the platform itself
    provides. You may not be able to make a Tumblr client that does the same things
    as the standard Tumblr platform, but perhaps if you offer other functionality
    not included in the platform, there would be no restriction. Restrictions like
    these are tricky, and it’s best to read any legal documentation that the data
    provider offers. In addition, it’s usually good to search for other examples of
    people and companies using the data in a similar way and see if there are any
    references to legal concerns. Precedent is no guarantee that a particular use
    of the data is legally sound, but it may provide guidance in your decision to
    use the data or not.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 就像软件许可证一样，数据可能存在许可、版权或其他限制，这可能会使得在特定目的下使用数据成为非法行为。例如，如果数据来自学术来源（如大学、研究机构等），那么通常会有一个限制，即数据不能用于盈利。像Tumblr或Twitter这样的专有数据，通常带有限制，即你不能使用数据来复制平台本身提供的功能。你可能无法制作一个与标准Tumblr平台做同样事情的Tumblr客户端，但如果你提供平台不包括的其他功能，可能就不会有限制。这样的限制很棘手，最好阅读数据提供者提供的任何法律文件。此外，通常好的做法是搜索其他人或公司以类似方式使用数据的例子，看看是否有任何关于法律问题的参考。先例并不能保证特定数据的使用在法律上是合理的，但它可能在你是否使用数据的决定上提供指导。
- en: All in all, you should remain keenly aware that most data sets not owned by
    you or your organization come with restrictions on use. Without confirming that
    your use case is legal, you remain at risk of losing access to the data or, even
    worse, a lawsuit.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，你应该敏锐地意识到，大多数不属于你或你所在组织的数据集都带有使用限制。如果不确认你的使用案例是合法的，你仍然面临失去数据访问权或更糟糕的是，面临诉讼的风险。
- en: '3.3.3\. The data you have: is it enough?'
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3. 你拥有的数据：是否足够？
- en: Let’s say you’ve found data and confirmed that you’re allowed to use it for
    your project. Should you keep looking for more data, or should you attack the
    data you have immediately? The answer to this question is—like pretty much everything
    in data science—tricky. In this case, the answer is tricky because data sets aren’t
    always what they seem to be or what you want them to be. Take the example of Uber,
    the taxi service app publisher. I recently read that Uber was compelled (upon
    losing an appeal) to turn over trip data to New York City’s Taxi and Limousine
    Commission (TLC). Suppose you’re an employee of the TLC, and you’d like to compare
    Uber with traditional taxi services in regard to the number of trips taken by
    riders over many specific routes. Given that you have data from both Uber and
    traditional taxis, it may seem straightforward to compare the number of trips
    for similar routes between the two types of car services. But once you begin your
    analysis, you realize that Uber had provided pick-up and drop-off locations in
    terms of ZIP codes, which happen to be the minimum specificity required by the
    TLC. ZIP codes can cover large areas, though admittedly less so in New York City
    than anywhere else. Addresses, or at least city blocks, would have been considerably
    better from a data analysis perspective, but requiring such specificity presents
    legal troubles regarding the privacy of personal data of the users of taxi services,
    so it’s understandable.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经找到了数据，并确认你可以为你的项目使用它。你应该继续寻找更多数据，还是应该立即处理你拥有的数据？这个问题的答案——就像数据科学中的几乎所有事情一样——很棘手。在这种情况下，答案之所以棘手，是因为数据集并不总是像它们看起来或你希望它们看起来的那样，或者你希望它们成为的那样。以Uber为例，这是一家出租车服务应用发布商。我最近读到，Uber（在败诉后）被迫（在上诉失败后）向纽约市的出租车和豪华轿车委员会（TLC）提交行程数据。假设你是TLC的员工，你希望比较Uber与传统出租车服务在许多特定路线上的乘客行程数量。鉴于你拥有Uber和传统出租车服务的数据，似乎很容易比较两种汽车服务之间相似路线的行程数量。但一旦你开始分析，你就会意识到Uber提供的是以ZIP代码为单位的接送地点，而这恰好是TLC所要求的最低具体性。ZIP代码可以覆盖很大的区域，尽管在纽约市可能不如其他地方那么大。从数据分析的角度来看，地址或至少城市街区会更好，但要求这样的具体性会带来关于出租车服务用户个人隐私的法律问题，因此这是可以理解的。
- en: 'So what should you do? After the first waves of disappointment wear off, you
    should probably check to see whether your data will suffice after all or if you
    need to supplement this data and/or amend your project plans. There’s often a
    simple way to accomplish this: can you run through a few specific examples of
    your intended analyses and see if it makes a significant difference?'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该怎么做呢？在最初的失望情绪消退之后，你可能需要检查你的数据是否足够，或者你是否需要补充这些数据以及/或者修改你的项目计划。通常，有一种简单的方法可以完成这个任务：你能通过几个你打算分析的特定例子来运行一下，看看是否会产生显著的影响吗？
- en: 'In this taxi-versus-Uber example, you’d like to find out whether the relative
    non-specificity of ZIP code can still provide a useful approximation for the many
    routes you’d like to evaluate. Pick a specific route, say Times Square (ZIP code:
    10036) to the Brooklyn Academy of Music (ZIP code: 11217). If a car travels between
    10036 and 11217, what other specific routes might the rider have taken? In this
    case, those same ZIP codes could also describe a trip from the Intrepid Sea, Air
    & Space Museum to Grand Army Plaza, or likewise a trip from a restaurant in Hell’s
    Kitchen to an apartment in Park Slope. These probably don’t mean much to people
    outside the New York City area, but for our purposes it suffices to say that these
    other locations are up to a kilometer from the origin and destination of the chosen
    route, a distance that’s about a ten-minute walk and, by NYC standards, not very
    short. It’s up to you, the data scientist, to make a decision about whether these
    other locations in the same ZIP codes are close enough or too far from their intended
    targets. And this decision, in turn, should be made based on the project’s goals
    and the precise questions (from [chapter 2](kindle_split_011_split_000.xhtml#ch02))
    that you’re hoping to answer.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个出租车与优步的例子中，你想要找出ZIP代码的相对非特异性是否仍然可以为你想要评估的许多路线提供一个有用的近似值。选择一个特定的路线，比如说从时代广场（ZIP代码：10036）到布鲁克林音乐学院（ZIP代码：11217）。如果一辆车在10036和11217之间行驶，那么乘客可能还乘坐了哪些其他特定的路线？在这种情况下，那些相同的ZIP代码也可能描述从无畏号海、空与太空博物馆到格兰德军营广场的行程，或者同样是从地狱厨房的一家餐厅到帕克斜坡的一套公寓的行程。这些可能对纽约市以外的人来说意义不大，但就我们的目的而言，可以说这些其他地点距离所选路线的起点和终点最多一公里，这个距离大约是十分钟的步行路程，按照纽约的标准，并不是很短。决定这些在同一ZIP代码中的其他地点是否足够接近或太远于它们的目标，这取决于你，作为数据科学家。这个决定反过来应该基于项目的目标和（来自[第2章](kindle_split_011_split_000.xhtml#ch02)）你希望回答的精确问题。
- en: 3.3.4\. Combining data sources
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4. 合并数据源
- en: If you find that your data set is insufficient to answer your questions, and
    you can’t find a data set that is sufficient, it might still be possible to combine
    data sets to find answers. This is yet another point that seems obvious at times
    but is worth mentioning because of its importance and because of a few tricky
    points that might pop up.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现你的数据集不足以回答你的问题，而且你找不到足够的数据集，那么仍然有可能通过合并数据集来找到答案。这看似是一个显而易见但值得提及的观点，因为它的重要性，以及可能出现的几个棘手问题。
- en: Combining two (or more) data sets can be like fitting puzzle pieces together.
    If the puzzle is, metaphorically, the complete data set you wish you had, then
    each piece of the puzzle—a data set—needs to cover precisely what the other pieces
    don’t. Sure, unlike puzzle pieces, data sets can overlap in some sense, but any
    gap left after all the present pieces have been assembled is an obstacle that
    needs to be overcome or circumvented, either by changing the plan or some other
    reevaluation of how you’re going to answer your questions.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个（或更多）数据集合并可能就像拼拼图一样。如果这个拼图在比喻意义上是你希望拥有的完整数据集，那么拼图的每一块——即数据集——都需要精确地覆盖其他拼图块所没有的部分。当然，与拼图块不同，数据集在某种程度上可以重叠，但所有现有拼图块组合后留下的任何间隙都是一个需要克服或绕过的障碍，无论是通过改变计划还是通过重新评估你将如何回答你的问题。
- en: Your multiple data sets might be coming in multiple formats. If you’re adept
    at manipulating each of these formats, this doesn’t usually present a problem,
    but it can be tough to conceptualize how the data sets relate to one another if
    they’re in vastly different forms. A database table and a CSV file are similar
    to me—they both have rows and columns—and so I can typically imagine how they
    might fit together, as in the database example earlier in this chapter, with one
    data set (one of the tables) providing the customer’s color choice and another
    data set (the other table) providing the customer’s ZIP code. These two can be
    combined easily because both data sets are based on the same set of customer IDs.
    If you can imagine how you might match up the customer IDs between the two data
    sets and then combine the accompanying information—a *join*, in database parlance—then
    you can imagine how to combine these two data sets meaningfully.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 你的多个数据集可能以多种格式到来。如果你擅长操作这些格式中的每一个，这通常不会造成问题，但如果它们以截然不同的形式存在，那么理解数据集之间如何相互关联可能会很困难。对我来说，数据库表和CSV文件很相似——它们都有行和列——所以我通常可以想象它们可能如何结合在一起，就像本章前面提到的数据库示例中，一个数据集（一个表）提供客户的颜色选择，另一个数据集（另一个表）提供客户的ZIP代码。这两个可以很容易地结合起来，因为这两个数据集都是基于相同的客户ID集。如果你能想象如何将两个数据集中的客户ID匹配起来，然后结合相关信息——在数据库术语中称为“连接”——那么你可以想象如何有用地结合这两个数据集。
- en: On the other hand, combining data sets might not be so simple. During my time
    as the lead data scientist at a Baltimore analytic software firm, I took part
    in a project in which our team was analyzing email data sets as part of a legal
    investigation. The collection of emails was delivered to us in the form of a few
    dozen files in PST format, which is Microsoft Outlook’s archival format. I’d seen
    this format before, because I’d worked previously with the now-public and commonly
    studied Enron email data set. Each archive file comprised the email from one person’s
    computer, and because the people under investigation often emailed each other,
    the data sets overlapped. Each email, excepting deleted emails, was present in
    each of the senders’ and recipients’ archives. It’s tempting to think that it
    would be easy to combine all of the email archives in to a single file—I chose
    a simple, large CSV file as the goal format—and then analyze this file. But it
    wasn’t so easy.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，合并数据集可能并不那么简单。在我担任巴尔的摩一家分析软件公司首席数据科学家期间，我参与了一个项目，我们的团队在法律调查中分析电子邮件数据集。这些电子邮件以几十个PST格式的文件形式交付给我们，这是微软Outlook的存档格式。我之前见过这种格式，因为我之前曾与现在公开且广泛研究的安然电子邮件数据集合作过。每个存档文件包含一个人的电脑上的电子邮件，由于被调查的人经常互相发送电子邮件，数据集存在重叠。除了已删除的电子邮件外，每封电子邮件都存在于发送者和接收者的存档中。人们可能会觉得将所有电子邮件存档合并成一个文件很容易——我选择了一个简单的大型CSV文件作为目标格式，然后分析这个文件。但事实并非如此。
- en: 'Extracting individual emails from each archive and turning each of them into
    a row of a CSV file was, comparatively, the easy part. The hard part, I quickly
    realized, was making sure I could keep all of the senders and recipients straight.
    As it turns out, the names listed in the sender and recipient fields of emails
    are not standardized—when you send an email, what appears in the `SENDER` field
    isn’t always the same as what appears in the `RECIPIENT` field when someone writes
    an email to you. In fact, even within each of these two fields, names are not
    consistent. If Nikola Tesla sent an email to Thomas Edison (names have been changed
    to protect the innocent), the `SENDER` and `RECIPIENT` fields might be any of
    the following:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个存档中提取单个电子邮件并将它们转换为CSV文件的一行相对容易。我很快意识到，困难的部分是确保我能清楚地记录所有发送者和接收者。实际上，电子邮件发送者和接收者字段中列出的名称并不标准化——当你发送电子邮件时，`SENDER`字段中显示的内容并不总是与某人给你发送电子邮件时`RECIPIENT`字段中显示的内容相同。事实上，即使在每个这些字段中，名称也不一致。如果尼古拉·特斯拉给托马斯·爱迪生（为保护无辜者，姓名已更改）发送电子邮件，`SENDER`和`RECIPIENT`字段可能是以下任何一种：
- en: '[PRE16]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Some of these would be recognizable as Tesla or Edison, even out of context,
    but others would not. To be sure each email is attributed to the right person,
    you’d also need a list of email addresses matched to the correct names. I didn’t
    have that list, so I did the best I could, made some assumptions, and used some
    fuzzy string matching with spot-checking (discussed more in the next chapter on
    data wrangling) to match as many emails as possible with the appropriate names.
    I thought the multiple email data sets would merge nicely together, but I soon
    found out that this was not the case.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些可能被识别为特斯拉或爱迪生，即使在没有上下文的情况下也是如此，但其他一些则不然。为了确保每封电子邮件都归因于正确的人，你还需要一个与正确姓名匹配的电子邮件地址列表。我没有那个列表，所以我尽我所能，做出了一些假设，并使用了一些模糊字符串匹配和抽查（在下一章关于数据整理中讨论）来尽可能多地匹配电子邮件与适当的姓名。我认为多个电子邮件数据集可以很好地合并在一起，但很快我就发现情况并非如此。
- en: Data sets can differ in any number of ways; format, nomenclature, and scope
    (geographic, temporal, and so on) are a few. As in [section 3.3.3](kindle_split_012_split_003.xhtml#ch03lev2sec16)
    on finding out whether your data is enough, before you spend too much time manipulating
    your multiple data sets or diving into analyses, it’s usually extremely helpful
    and informative to spot-check a few data points and attempt a quick analysis on
    a small scale. A quick look into a few PST files in the email example made me
    aware of the disparate naming schemes across files and fields and allowed me to
    plan within the project for the extra time and inevitable matching errors that
    arose.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以以任何数量的方式不同；格式、命名法、范围（地理、时间等）只是其中的一些。正如[第3.3.3节](kindle_split_012_split_003.xhtml#ch03lev2sec16)中提到的，在花费太多时间处理多个数据集或进行深入分析之前，了解你的数据是否足够，通常是非常有帮助和有信息的，可以在小规模上检查几个数据点并尝试快速分析。在电子邮件示例中快速查看几个PST文件让我意识到文件和字段之间不同的命名方案，并使我能够在项目中计划额外的时间和不可避免的匹配错误。
- en: Now imagine combining this email data set with internal chat messages in a JSON
    format—potentially containing a different set of user names—with a set of events/appointments
    in a proprietary calendar format. Assembling them into a single timeline with
    unambiguous user names is no simple task, but it might be possible with care and
    awareness of the potential pitfalls.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，将这个电子邮件数据集与内部聊天消息（可能包含不同的用户名）以及专有日历格式的活动/预约结合起来——将它们组装成一个具有明确用户名的单一时间线并不是一个简单的任务，但通过小心谨慎和意识到潜在的风险，这可能是有可能的。
- en: 3.3.5\. Web scraping
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5. 网络爬虫
- en: Sometimes you can find the information you need on the internet, but it’s not
    what you might call a data set in the traditional sense. Social media profiles,
    like those on Facebook or LinkedIn, are great examples of data that’s viewable
    on the internet but not readily available in a standard data format. Therefore,
    some people choose to scrape it from the web.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可以在互联网上找到你需要的信息，但这并不是传统意义上的数据集。社交媒体资料，如Facebook或LinkedIn上的资料，是互联网上可查看但不易以标准数据格式获取的数据的好例子。因此，有些人选择从网络上爬取这些数据。
- en: I should definitely mention that web scraping is against the terms of service
    for many websites. And some sites have guards in place that will shut down your
    access if they detect a scraper. Sometimes they detect you because you’re visiting
    web pages much more quickly than a human being can, such as several thousand pages
    in few minutes or even a few hours. Regardless, people have used scraping techniques
    to gather useful data they wouldn’t have otherwise, in some cases circumventing
    any guards by adapting the scraper to act more human.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须明确指出，网络爬虫在很多网站的条款服务中都是被禁止的。有些网站设置了防护措施，一旦检测到爬虫，就会关闭你的访问权限。有时他们能检测到你，因为你访问网页的速度比人类快得多，比如几分钟内访问几千页，甚至几小时。无论如何，人们已经使用爬虫技术收集了他们原本无法获得的有用数据，在某些情况下，通过调整爬虫的行为使其更像人类来绕过任何防护措施。
- en: Two important things that a web scraper must do well are visit lots of URLs
    programmatically and capture the right information from the pages. If you wanted
    to know about your friend network on Facebook, you could theoretically write a
    script that visits the Facebook profiles of all of your friends, saves the profile
    pages, and then parses the pages to get lists of their friends, visits their friends’
    profiles, and so on. This works only for people who have allowed you to view their
    profiles and friend lists, and would not work for private profiles.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网络爬虫必须做好的两件重要事情是程序化地访问大量URL并从页面中捕获正确的信息。如果你想知道你在Facebook上的朋友网络，理论上你可以编写一个脚本，访问你所有朋友的Facebook个人资料，保存个人资料页面，然后解析页面以获取他们朋友的列表，访问他们朋友的个人资料，等等。这仅适用于允许你查看他们个人资料和好友列表的人，而对于私人资料则不起作用。
- en: An example of web scraping that became popular in early 2014 is that of mathematician
    Chris McKinlay, who used a web scraper to capture data from thousands of profiles
    on the popular dating website OKCupid. He used the information he gathered—mostly
    women’s answers to multiple-choice questions on the site—to cluster the women
    into a few types and subsequently optimize a separate profile for himself for
    each of the types he found generally attractive. Because he optimized each profile
    for a certain cluster/type of women, women in that cluster had a high matching
    score (according to OKCupid’s own algorithms) for the respective profile and were
    therefore more likely to engage him in conversation and ultimately to go out on
    a date with him. It seems to have worked out well for him, earning him dozens
    of dates before he met the woman with whom he wanted to start a monogamous relationship.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年初流行的一个网络爬虫的例子是数学家克里斯·麦金莱（Chris McKinlay）的例子，他使用网络爬虫从流行的约会网站OKCupid上的数千个个人资料中捕获数据。他使用收集到的信息——主要是女性对该网站上多项选择题的回答——将女性分为几种类型，并随后为每种他认为有吸引力的类型优化了自己的个人资料。因为他为每种女性集群/类型优化了个人资料，所以该集群中的女性与相应个人资料有很高的匹配分数（根据OKCupid自己的算法），因此更有可能与他交谈，并最终与他出去约会。这似乎对他很有帮助，在他遇到想要开始一夫一妻关系的那位女性之前，他已经有数十次约会了。
- en: For more on the practicalities of building a web scraper, see the documentation
    for the HTTP- and HTML-related utilities of your favorite programming language
    and any number of online guides, as well as [section 3.2](kindle_split_012_split_002.xhtml#ch03lev1sec2)
    on data formats, particularly the discussion of HTML.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建网络爬虫的实用性，请参阅您喜欢的编程语言的HTTP和HTML相关实用程序的文档，以及任何数量的在线指南，特别是关于数据格式[第3.2节](kindle_split_012_split_002.xhtml#ch03lev1sec2)的讨论，尤其是关于HTML的讨论。
- en: 3.3.6\. Measuring or collecting things yourself
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.6\. 自己测量或收集事物
- en: Contrary to the principal way I’ve presented data in this chapter—a product
    of a culture that wants data for its own sake, existing regardless of whether
    someone intends to use it—you sometimes have the opportunity to collect data the
    old-fashioned way. Methods could be as simple as personally counting the number
    of people crossing a street at a particular crosswalk or perhaps emailing a survey
    to a group of interest. When starting a new project, if you ever ask yourself,
    “Does the data I need exist?” and find that the answer is “No” or “Yes, but I
    can’t get access to it,” then maybe it would be helpful to ask, “*Can* the data
    exist?”
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 与我在本章中展示数据的主要方式相反——这是一种想要数据本身的文化产物，无论某人是否打算使用它——你有时有机会以传统方式收集数据。方法可能简单到亲自数一数在特定人行横道处穿越的人数，或者也许给一群感兴趣的人发送调查问卷。当你开始一个新项目时，如果你曾问自己，“我需要的数据存在吗？”并且发现答案是“不存在”或“存在，但我无法获取它”，那么也许问一问“*数据*可以存在吗？”会有所帮助。
- en: 'The question “Can the data exist?” is intended to draw attention to the potential
    for simple measures you can take that can create the data set you want. These
    include the following:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: “数据可以存在吗？”这个问题旨在引起人们对你可以采取的简单措施的关注，这些措施可以创建你想要的数据集。以下是一些包括在内的措施：
- en: '***Measuring things in real life—*** Using tape measures, counting, asking
    questions personally, and so on may seem outmoded, but it’s often underrated.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现实生活中测量事物——**使用卷尺、计数、亲自提问等方法可能看起来过时，但它们通常被低估。'
- en: '***Measuring things online—*** Clicking around the internet and counting relevant
    web pages, numbers of relevant Google search results, and number of occurrences
    of certain terms on certain Wikipedia pages, among others, can benefit your project.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在线测量事物—*** 在互联网上四处点击，计算相关网页、相关谷歌搜索结果的数量以及某些维基百科页面上某些术语的出现次数等，都可以对你的项目有所帮助。'
- en: '***Scripting and web scraping—*** Repeated API calls or web scraping of certain
    pages over a period of time can be useful when certain elements in the API or
    web page are constantly changing but you don’t have access to the history.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***脚本和网页抓取—*** 在一定时间内重复API调用或抓取某些页面可能是有用的，当API或网页中的某些元素不断变化，但你无法访问历史记录时。'
- en: '***Data-collection devices—*** Today’s concept of the Internet of Things gets
    considerable media buzz partially for its value in creating data from physical
    devices, some of which are capable of recording the physical world—for example,
    cameras, thermometers, and gyroscopes. Do you have a device (your mobile phone?)
    that can help you? Can you buy one?'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据收集设备—*** 今天的物联网概念在很大程度上得益于其从物理设备中创建数据的价值，其中一些设备能够记录物理世界——例如，摄像头、温度计和陀螺仪。你有没有一个设备（比如你的手机）能帮助你？你能买一个吗？'
- en: '***Log files or archives—*** Sometimes jargonized into *digital trail* or *exhaust*,
    log files are (or can be) left behind by many software applications. Largely untouched,
    they’re usually called to action only in exceptional circumstances (crashes! bugs!).
    Can you put them to good use in your project?'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***日志文件或存档—*** 日志文件有时被俗称为“数字足迹”或“残骸”，它们是由许多软件应用程序留下的（或可以留下的）。它们通常在非常特殊的情况下才会被投入使用（崩溃！错误！）！你能在你的项目中将它们用于良好的用途吗？'
- en: 'For that last bullet, much like web scraping, the primary tasks are to identify
    manually whether and where the log files contain data that can help you and to
    learn how to extract this useful data programmatically from a set of log files
    that contain, in most cases, a bunch of other data that you’ll never need. This,
    perhaps, is the frontier of the data wilderness: creating conceptually new data
    sets using other data that exists for an entirely different purpose. I believe
    *data alchemy* has been put forth as a possible name for this phenomenon, but
    I’ll leave you to judge whether your own data extractions and transformations
    merit such a mystical title.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一个要点，与网络爬虫类似，主要任务是手动识别日志文件中是否以及在哪里包含有助于你的数据，并学习如何从包含大量你永远不会需要的数据的日志文件集中程序化地提取这些有用的数据。这可能就是数据荒野的前沿：使用为完全不同的目的而存在的其他数据创建概念上全新的数据集。我相信有人提出了“数据炼金术”作为这种现象的可能名称，但我会让你自己判断你自己的数据提取和转换是否值得这样一个神秘的称号。
- en: '3.4\. Example: microRNA and gene expression'
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 示例：miRNA和基因表达
- en: When I was a PhD student, most of my research was related to quantitative modeling
    of gene expression. I mentioned working in genetics previously, but I haven’t
    delved deeply until now. I find it to be an incredibly interesting field.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 当我还是一名博士生时，我的大部分研究都与基因表达的定量建模相关。我之前提到过我在遗传学方面的工作，但直到现在我才深入探讨。我发现这是一个极其有趣的领域。
- en: '*Genetics* is the study of the code from which all living things are built.
    This code is present in every organism’s genome, which is composed of DNA or RNA,
    and copies of it are present in every cell. If an organism’s genome has been sequenced,
    then its genome has been parsed into genes and other types of non-gene sequences.
    Here I focus only on the genes and their expression. Biologists’ concept of *gene
    expression* involves the activity of known genes within a biological sample, and
    we measure gene expression using any of several tools that can measure the *copy
    number*, or concentration of specific RNA sequence fragments that are related
    directly to these genes. If an RNA fragment contains a sequence that’s known to
    match a certain gene but not other genes, then that sequence can be used as an
    indicator of the expression of the gene. If that RNA sequence occurs very often
    (high copy number or concentration) in the biological sample, then the expression
    of the corresponding gene is said to be high, and a sequence that occurs rarely
    indicates that its associated gene is expressed at a low level.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '*遗传学*是研究构成所有生物的代码的科学。这个代码存在于每个生物体的基因组中，由DNA或RNA组成，并且在每个细胞中都存在其副本。如果一个生物体的基因组已经被测序，那么它的基因组已经被解析成基因和其他类型的非基因序列。在这里，我只关注基因及其表达。生物学家对*基因表达*的概念涉及生物样本中已知基因的活动，我们使用任何可以测量与这些基因直接相关的特定RNA序列片段的*拷贝数*或浓度的工具来测量基因表达。如果一个RNA片段包含一个已知与某个基因匹配但与其他基因不匹配的序列，那么这个序列可以用作该基因表达的指标。如果这个RNA序列在生物样本中非常频繁地出现（高拷贝数或浓度），那么相应基因的表达就被认为是高的，而出现频率低的序列则表明其相关的基因表达水平低。'
- en: Two technologies, known as *microarrays* and *sequencing*, are common methods
    for measuring gene expression via the concentration or copy number of RNA sequences
    found in biological samples. Sequencing tends to be favored now, but at the time
    of my PhD research, I was analyzing data from microarrays. The data had been given
    to me by a collaborator at the University of Maryland School of Medicine, who
    had been studying the stem cells of *Mus musculus*—a common mouse—through various
    stages of development. In the earliest stages, stem cells are known to be of a
    general type that can subsequently develop into any of a number of *differentiated*,
    or specialized, cell types. The progression of cells through these stages of undifferentiated
    and then specific differentiated stem cell types isn’t fully understood, but it
    had been hypothesized by my collaborators and others that a special class of RNA
    sequences called microRNA might be involved.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 两种称为*微阵列*和*测序*的技术是测量生物样本中RNA序列浓度或拷贝数的常用方法，以测量基因表达。现在测序通常更受欢迎，但在我的博士研究时期，我正在分析微阵列的数据。这些数据是由马里兰大学医学院的一位合作者提供的，他一直在研究*Mus
    musculus*（一种常见的老鼠）在各个发育阶段的干细胞。在最早阶段，干细胞被知为一种可以随后发展成为多种*分化*或特殊细胞类型的通用类型。细胞通过这些未分化阶段和随后特定分化干细胞类型的进展并不完全理解，但我的合作者和其他人已经假设，一类称为微RNA的特殊RNA序列可能参与其中。
- en: '*MicroRNAs* (or miRs) are short RNA sequences (about 20 base pairs, vastly
    shorter than most genes) that are known to be present in virtually all organisms.
    To help determine whether miRs help regulate the development of stem cells and
    differentiation, my collaborators used microarrays to measure the expression of
    both genes and miRs throughout the early stages of development of stem cells.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '*微RNA*（或miRs）是一类短的RNA序列（大约20个碱基对，远短于大多数基因），已知几乎存在于所有生物体中。为了确定miRs是否有助于调节干细胞的发育和分化，我的合作者使用微阵列测量了基因和miRs在干细胞发育早期阶段的表达。'
- en: The data set consisted of microarray data for both genes and miRs for each of
    the seven stem cell types. A single microarray measures several thousand genes
    or, alternatively, a few hundred miRs. And for each stem cell type, there were
    two to three replicates, meaning that each biological sample was analyzed using
    two to three gene-oriented microarrays and two to three miR-oriented microarrays.
    Replicates are helpful for analyzing variance between samples as well as identifying
    outliers. Given 7 stem cell types and 2 to 3 replicates each for genes and miRs,
    I had 33 microarrays in total.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括七个干细胞类型中每个基因和miR的微阵列数据。单个微阵列可以测量几千个基因，或者，也可以测量几百个miR。对于每种干细胞类型，有两个到三个重复，这意味着每个生物样本都使用两个到三个基因导向的微阵列和两个到三个miR导向的微阵列进行分析。重复有助于分析样本之间的差异以及识别异常值。考虑到有7种干细胞类型，每种类型基因和miR各有2到3个重复，我总共有了33个微阵列。
- en: Because miRs are thought mainly to inhibit expression of genes—they apparently
    bind to complementary sections of genetic RNA and block that RNA from being copied—the
    main question I asked of the data set was “Can I find any evidence of specific
    miRs inhibiting the expression of specific genes?” Is the expression of any certain
    gene routinely low whenever the expression of a specific miR is high? In addition,
    I wanted to know whether the expression and inhibiting activity of any miRs could
    be highly correlated with particular stages of stem cell development and differentiation.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 因为miR被认为主要是抑制基因的表达——它们显然与遗传RNA的互补部分结合并阻止该RNA被复制——我对数据集的主要问题是“我能否找到任何特定miR抑制特定基因表达的证据？”当特定miR的表达量高时，任何特定基因的表达是否通常都很低？此外，我还想知道任何miR的表达和抑制活性是否可以与干细胞发育和分化的特定阶段高度相关。
- en: Though no one had previously studied this specific topic—the effect of miRs
    in mouse stem cell development—a fair amount of work had been done on related
    topics. Of particular note was the class of statistical algorithms that attempted
    to characterize whether a particular miR would target (inhibit) a specific section
    of genetic RNA, based solely on the sequence information alone. If a miR’s base
    sequence looks like this
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有人之前研究过这个具体主题——miR在小鼠干细胞发育中的影响——但已经在相关主题上做了相当多的工作。特别值得注意的是，一类统计算法试图仅根据序列信息来描述一个特定的miR是否会靶向（抑制）遗传RNA的特定部分。如果一个miR的碱基序列看起来像这样
- en: ACATGTAACCTGTAGATAGAT
  id: totrans-478
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ACATGTAACCTGTAGATAGAT
- en: (again, I use *T* in place of *U* for convenience), then a perfectly complementary
    genetic RNA sequence would be
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: (再次，为了方便，我用 *T* 代替 *U*），那么一个完美的互补遗传RNA序列会是
- en: TGTACATTGGACATCTATCTA
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TGTACATTGGACATCTATCTA
- en: 'because, within an RNA sequence, the nucleotide A is complementary to T, and
    C is complementary to G. Because these miRs are floating around in a cell’s cytoplasm,
    as are genetic RNA sequences, there’s no guarantee that even a perfect match will
    bind and inhibit gene expression. Under perfect conditions, such complementary
    sequences will bind, but nothing in biology is perfect. It’s also likely that
    a miR and its perfect match will float past each other like two ships passing
    in the night, as they say. Also, it’s a funny quirk of all RNA sequences that
    sometimes they bend a little too much and get stuck to themselves—for miRs the
    result is known as a *hairpin* because of the shape that’s easy to imagine. In
    any case, it’s not a foregone conclusion that perfectly complementary sequences
    will bind; nor is it true that imperfect matches won’t bind. Many researchers
    have explored this and developed algorithms that assign miR-gene pairs matching
    scores based on complementarity and other features of the sequences. These are
    generally referred to as *target prediction* algorithms, and I made use of two
    such algorithms in my work: one called TargetScan ([www.targetscan.org](http://www.targetscan.org))
    and another called miRanda ([www.microrna.org](http://www.microrna.org)).'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 因为，在RNA序列中，核苷酸A与T互补，C与G互补。由于这些miR在细胞的细胞质中漂浮，就像遗传RNA序列一样，因此即使是一个完美的匹配也不能保证会结合并抑制基因表达。在完美的条件下，这些互补序列会结合，但生物学中没有什么是完美的。据说是这样，一个miR和它的完美匹配就像两艘在夜幕下错过的船一样漂浮过去。此外，RNA序列的一个有趣的特性是，有时它们会弯曲得太多而粘附在自己身上——对于miR来说，结果是被称为*发夹*，因为这种形状很容易想象。无论如何，完美互补序列的结合并不是必然的；同样，不完美的匹配也不会结合。许多研究人员已经探索了这个问题，并开发了基于序列互补性和其他特征的算法，为miR-基因对分配匹配分数。这些通常被称为*目标预测*算法，我在我的工作中使用了两个这样的算法：一个叫做TargetScan
    ([www.targetscan.org](http://www.targetscan.org))，另一个叫做miRanda ([www.microrna.org](http://www.microrna.org))).
- en: 'Both TargetScan and miRanda are widely viewed as the products of solid scientific
    research, and both of these algorithms and their predictions are freely available
    on the internet. For any miR-gene pair in my microarray data sets, I had at least
    two target prediction scores indicating whether the miR is likely to inhibit expression
    of the gene. The files I obtained from TargetScan look like this (with some columns
    removed for clarity):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: TargetScan和miRanda都被广泛视为坚实科学研究的产物，这两个算法及其预测结果都可以在互联网上免费获取。对于我的微阵列数据集中的任何miR-基因对，我至少有两个目标预测分数，表明miR是否可能抑制基因的表达。我从TargetScan获得的数据文件看起来像这样（为了清晰起见，删除了一些列）：
- en: '[PRE17]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, for each gene and miR/miRNA, TargetScan has given a score representing
    the likelihood that the miR will target the genetic RNA. miRanda provides similar
    files. These scores are known to be imperfect, but they are informative, so I
    decided to include them as evidence but not certainty of inhibition of the gene’s
    expression by the miR.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，对于每个基因和miR/miRNA，TargetScan都给出一个分数，表示miR靶向遗传RNA的可能性。miRanda提供了类似的文件。这些分数被认为是并不完美，但它们是有信息的，所以我决定将它们包括作为miR抑制基因表达的证据，但不是确凿的证据。
- en: My main data set was still the set of microarrays I had from my collaborators’
    lab, and from these I would be able to analyze all expression values of genes
    and miRs and determine positive and negative correlations between them. Also,
    I could use the target predictions as further evidence in favor of certain miR-gene
    target pairs. In the framework of Bayesian statistics—discussed more in [chapter
    8](kindle_split_018_split_000.xhtml#ch08)—the target predictions can be considered
    a priori knowledge, and I could adjust that knowledge based on the new data I
    collected—the new microarray data I received from my collaborators. In this way,
    neither the prediction nor the noisy data set was taken as truth, but both informed
    the final estimates of which miR-gene pairs are most likely true targeting interactions.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我的主要数据集仍然是我从合作者的实验室获得的微阵列数据集，从这些数据中我可以分析所有基因和miR的表达值，并确定它们之间的正负相关性。此外，我还可以将目标预测作为支持某些miR-基因靶对进一步的证据。在贝叶斯统计框架内——在第8章中讨论得更多——目标预测可以被视为先验知识，我可以根据我收集的新数据调整这种知识——即我从合作者那里收到的新的微阵列数据。这样，预测和噪声数据集都不被视为真理，但它们都为最终估计哪些miR-基因对最可能是真正的靶向相互作用提供了信息。
- en: 'So far in this section, I’ve talked about combining gene expression data with
    microRNA data to search for targeting interactions between them and to analyze
    the effects of miRs on stem cell development. In addition, I included two target
    prediction data sets as further evidence that certain miRs target certain genes.
    As I completed analysis based on these data sets, I needed to be able to show
    that the miRs and genes that my models indicated as being related to stem cell
    development made sense in some way. There were two ways I might typically do this:
    ask my biologist collaborators to test some of my results in the lab to confirm
    that they were correct, or find more data sets online somewhere that were already
    validated and that supported my results in some way.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，我谈到了将基因表达数据与microRNA数据相结合，以搜索它们之间的靶向相互作用，并分析miRs对干细胞发育的影响。此外，我还包括了两个靶标预测数据集作为进一步证据，表明某些miRs靶向某些基因。在我基于这些数据集完成分析后，我需要能够表明我的模型指示与干细胞发育相关的miRs和基因在某种程度上是有意义的。我可能会以两种方式来做这件事：要求我的生物学家合作者在实验室中测试我的部分结果以确认它们是正确的，或者在网上找到一些已经验证并支持我的结果的数据集。
- en: If I’d had no experience working with this sort of data, I might have Googled
    “validated microRNA targeting” or “stem cell development gene annotations,” but
    because I knew from past projects that a large public set of annotations of genes
    known as Gene Ontology (GO) terms was available, as well as a database of validated
    miR-gene targeting interactions already reported in scientific publications, I
    didn’t have to search much. GO term annotation can be accessed via a few web-based
    tools ([geneontology.org](http://geneontology.org)) as well as a package for the
    R language, among others. I had previously used these annotations for analyzing
    groups of genes to see whether they have some things in common. In the case of
    this project, it would help to confirm my results if any group of genes found
    significant within my model with respect to stem cell development also had a significant
    number of GO annotations related to stem cells and stem cell development.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我没有与这类数据工作的经验，我可能会在谷歌上搜索“已验证的microRNA靶向”或“干细胞发育基因注释”，但由于我知道从过去的项目中，一个名为基因本体（GO）术语的大规模公共基因注释集是可用的，以及一个已报道在科学出版物中的已验证的miR-基因靶向相互作用数据库，所以我无需搜索太多。GO术语注释可以通过几个基于网络的工具（[geneontology.org](http://geneontology.org)）以及R语言的包等访问。我之前曾使用这些注释来分析基因组，看看它们是否有共同之处。在这个项目中，如果任何在干细胞发育方面具有显著性的基因组在我的模型中也有与干细胞和干细胞发育相关的显著数量的GO注释，这将有助于证实我的结果。
- en: Also, I obviously preferred that any miR-gene target pairs that my model found
    significant would have already been validated in some other reliable way. This
    is where the data set of *validated* targeting interactions on [www.microrna.org](http://www.microrna.org)
    comes in. It’s certainly a useful data set, but one important aspect of it is
    that, although some miR-gene target pairs have been confirmed, just because a
    pair *hasn’t* been confirmed doesn’t mean that it *isn’t* a true target pair.
    If my model found a particular target pair significant, but it hadn’t been validated
    yet, that didn’t indicate at all that the model was wrong. On the other hand,
    if a validated target pair did not appear significant according to my model, then
    there was some reason for concern. Overall, in the validation step of my project,
    I hoped that all or most of the validated target pairs appeared significant according
    to the model, but I didn’t necessarily need to see validations for my most significant
    results.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我显然更倾向于认为，我模型发现的任何具有显著性的miR-基因靶标对，已经通过其他可靠的方式进行过验证。这就是[www.microrna.org](http://www.microrna.org)上*已验证*的靶向相互作用数据集的作用所在。这当然是一个有用的数据集，但其中一点重要之处在于，尽管一些miR-基因靶标对已经得到确认，但这并不意味着那些尚未得到确认的对就不是真正的靶标对。如果我的模型发现某个特定的靶标对具有显著性，但尚未经过验证，这并不表明模型是错误的。另一方面，如果一个已验证的靶标对根据我的模型没有显示出显著性，那么这就有一些值得关注的理由。总的来说，在我的项目验证步骤中，我希望所有或大多数已验证的靶标对根据模型显示出显著性，但我不一定需要看到我最重要的结果都有验证。
- en: Lastly, my collaborators had some interest in which families of microRNAs (groups
    of miRs with partially matching sequences) contributed to which stages of stem
    cell development. It turned out that TargetScan provided a nicely formatted file
    matching miRs with their families. In addition to the gene expression microarrays,
    the microRNA expression microarrays, two target prediction algorithm results,
    a set of gene annotations, and some validated miR-gene target pairs, I added a
    miR family data set.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我的合作者对哪些miRNA家族（具有部分匹配序列的miRs组）对干细胞发育的哪些阶段做出了贡献很感兴趣。结果发现，TargetScan提供了一个很好地匹配miRs及其家族的格式化文件。除了基因表达微阵列、miRNA表达微阵列、两个目标预测算法的结果、一组基因注释和一些已验证的miR-gene目标对之外，我还添加了一个miR家族数据集。
- en: Needless to say, there were many moving parts in this project. Also, as happens
    quite often in academia, the resulting scientific papers couldn’t include every
    piece of analysis. There’s one paper describing the model and application to a
    public data set (“Inferring MicroRNA Regulation of mRNA with Partially Ordered
    Samples of Paired Expression Data and Exogenous Prediction Algorithms,” PloS ONE,
    2012) and another paper describing the application to the mouse data set (“Correlated
    miR-mRNA Expression Signatures of Mouse Hematopoietic Stem and Progenitor Cell
    Subsets Predict ‘Stemness’ and ‘Myeloid’ Interaction Networks,” PLoS ONE, 2014).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 不言而喻，这个项目中有很多环节。此外，在学术界，这种情况相当常见，最终的科学论文无法包含每一项分析。有一篇论文描述了模型及其在公共数据集上的应用（“通过部分有序样本的配对表达数据和外源性预测算法推断mRNA的miRNA调控”，PloS
    ONE，2012年）和另一篇论文描述了其在小鼠数据集上的应用（“小鼠造血干细胞和祖细胞亚群的miR-mRNA表达相关特征预测‘干性’和‘髓性’相互作用网络”，PLoS
    ONE，2014年）。
- en: I won’t describe all results here in detail, but I was quite satisfied with
    the project. After matching miRs and genes from their respective expression data
    sets with their predicted target pairs from TargetScan and miRanda, I analyzed
    them via a Bayesian model incorporating all of this data and validated it using
    GO annotations and known target pair validations, with some miR family analysis
    tacked on. The results weren’t perfect; bioinformatics is notoriously complex,
    not to mention imperfect in its data quality. But most validated target pairs
    were significant, and some relevant GO annotations were overrepresented in significant
    groups of genes. In later chapters, I’ll delve more deeply into statistical models,
    their significance, and drawing conclusions from results, but for now I’d like
    to leave this example as one in which various data sets have been combined in
    ways that make new analyses possible.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里详细描述所有结果，但我对这个项目非常满意。在将miRs及其各自的表达数据集中的预测目标对与TargetScan和miRanda的预测目标对匹配后，我通过一个包含所有这些数据的贝叶斯模型分析了它们，并使用GO注释和已知的目标对验证来验证它，还附加了一些miR家族分析。结果并不完美；生物信息学非常复杂，数据质量也不完美。但大多数已验证的目标对都是显著的，一些相关的GO注释在显著的基因组中过度表示。在后面的章节中，我将更深入地探讨统计模型、其重要性以及从结果中得出结论，但现在我想把这个例子作为一个例子，其中各种数据集以使新的分析成为可能的方式被结合在一起。
- en: Exercises
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Continuing with the Filthy Money Forecasting personal finance app scenario
    from the last chapter’s exercises, try these exercises:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上一章练习中的Filthy Money Forecasting个人财务应用场景，尝试以下练习：
- en: '**1\.**'
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1\.**'
- en: ''
  id: totrans-495
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: List three potential data sources that you expect would be good to examine for
    this project. For each, also list how you would expect to access the data (for
    example, database, API, and so on).
  id: totrans-496
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 列出三个你预期对这个项目有益的潜在数据来源。对于每个来源，也列出你预期如何访问数据（例如，数据库、API等）。
- en: '**2\.**'
  id: totrans-497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2\.**'
- en: ''
  id: totrans-498
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consider the three project goals you listed in exercise 3 in the last chapter.
    What data would you need in order to achieve them?
  id: totrans-499
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑到上一章练习3中列出的三个项目目标。为了实现这些目标，你需要哪些数据？
- en: Summary
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Data science treats *data* as something that might exist independently of any
    specific purpose. Much like nature itself, and all of the organisms and species
    that exist within it, discovered or undiscovered, the realm of data is worth exploration
    and study.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学将*数据*视为可能独立于任何特定目的存在的东西。就像自然本身，以及其中存在的所有已发现或未发现的生物和物种一样，数据领域值得探索和研究。
- en: Data can be found in many places with varying accessibility, complexity, and
    reliability.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以存在于许多地方，其可访问性、复杂性和可靠性各不相同。
- en: It’s best to become familiar with some of the forms that data might take, as
    well as how to view and manipulate these forms.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好先熟悉数据可能采取的一些形式，以及如何查看和操作这些形式。
- en: Before assuming that a data set contains what you want, it’s best to evaluate
    data extent and quality.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在假设数据集包含你想要的内容之前，最好评估数据的范围和质量。
- en: Finding and recognizing data sets that are useful for your project aren’t always
    straightforward, but some preliminary investigation can help.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到和识别对项目有用的数据集并不总是直接的，但一些初步调查可能会有所帮助。
- en: 'Chapter 4\. Data wrangling: from capture to domestication'
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章\. 数据整理：从捕获到驯化
- en: '*This chapter covers*'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: “本章涵盖”
- en: Ways to wrangle data
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整理数据的方法
- en: Helpful tools and techniques
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有用的工具和技术
- en: Some common pitfalls
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些常见的陷阱
- en: One definition of *wrangling* is “having a long and complicated dispute.” That
    sounds about right.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 对“整理”的一个定义是“长期且复杂的争论。”听起来很贴切。
- en: '*Data wrangling* is the process of taking data and information in difficult,
    unstructured, or otherwise arbitrary formats and converting it into something
    that conventional software can use. Like many aspects of data science, it’s not
    so much a process as it is a collection of strategies and techniques that can
    be applied within the context of an overall project strategy. Wrangling isn’t
    a task with steps that can be prescribed exactly beforehand. Every case is different
    and takes some problem solving to get good results. Before I discuss specific
    techniques and strategies of data wrangling, as shown in [figure 4.1](kindle_split_013_split_000.xhtml#ch04fig01),
    I’ll introduce a case study that I’ll use to illustrate those techniques and strategies
    throughout the chapter.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: “数据整理”是将数据和信息从困难、非结构化或任意格式中提取出来，并转换为传统软件可以使用的过程。像数据科学的其他许多方面一样，它与其说是一个过程，不如说是一系列策略和技术，这些策略和技术可以在整体项目策略的背景下应用。整理不是一个可以事先精确规定的步骤任务。每个案例都不同，需要一些问题解决技巧才能取得好结果。在我讨论数据整理的具体技术和策略之前，如图4.1所示，我将介绍一个案例研究，我将使用这个案例研究来展示本章中提到的那些技术和策略。
- en: 'Figure 4.1\. The third step of the preparation phase of the data science process:
    data wrangling'
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1\. 数据科学过程准备阶段第三步：数据整理
- en: '![](Images/04fig01_alt.jpg)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig01_alt.jpg)'
- en: '4.1\. Case study: best all-time performances in track and field'
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 案例研究：田径运动史上最佳表现
- en: While I was in graduate school, the Jamaican sprinter Usain Bolt began to astonish
    the world with amazing performances in both the 100 m and 200 m dashes. In the
    2008 Olympic Games in Beijing, Bolt ran a world record 9.69 seconds in the 100
    m dash despite celebrating his victory with outstretched arms well before he crossed
    the finish line. A few days later, Bolt’s time of 19.30 sec in the 200 m final
    broke Michael Johnson’s world record, which had been widely considered one of
    the best marks in the entire sport of track and field. Usain Bolt had claimed
    his position as the greatest sprinter in history, but he continued to improve.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在研究生院学习时，牙买加短跑运动员尤塞恩·博尔特开始以100米和200米赛跑中惊人的表现震惊世界。在北京2008年奥运会上，博尔特在100米赛跑中跑出了9.69秒的世界纪录，尽管他在冲过终点线之前就伸展开双臂庆祝胜利。几天后，博尔特在200米决赛中的19.30秒打破了迈克尔·约翰逊的世界纪录，这个纪录被广泛认为是整个田径运动中最好的成绩之一。尤塞恩·博尔特已经确立了自己作为历史上最伟大的短跑运动员的地位，但他仍在不断进步。
- en: At the 2009 World Championships in Berlin, Usain Bolt broke both of his own
    world records. He ran 9.58 sec in the 100 m dash and 19.19 sec in the 200 m dash,
    making all of his competitors, the fastest people in the world, appear average.
    Specifically, in the 100 m dash, American Tyson Gay set a national record—and
    the USA is traditionally good at sprinting—but was meters behind Bolt at the finish
    line. I was so impressed by Bolt’s performances, and so ubiquitous were discussions
    of “How good are Usain Bolt’s marks compared to other world records?” and “No
    other athlete has dominated like Bolt has,” that I decided to quantify the improbability
    of what Bolt had done. I wanted to settle the score for all of the armchair track
    and field enthusiasts out there who mostly made guesses and used anecdotes and
    heuristic methods to show how rare a given performance is.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在2009年柏林世界锦标赛上，尤塞恩·博尔特打破了自身的两项世界纪录。他在100米比赛中跑出了9.58秒，在200米比赛中跑出了19.19秒，使得所有竞争对手，世界上最快的人，看起来都变得平庸。具体来说，在100米比赛中，美国运动员泰森·盖伊打破了国家纪录——而美国传统上擅长短跑——但在终点线落后博尔特数米。我对博尔特的表现印象深刻，关于“尤塞恩·博尔特的成绩与其他世界纪录相比如何？”和“没有其他运动员像博尔特那样统治过”的讨论无处不在，因此我决定量化博尔特所做的事情的不可能性。我想为所有那些坐在家里对田径充满热情的人解决这个账目，他们大多数只是猜测，使用轶事和启发式方法来展示某个特定表现是多么罕见。
- en: 4.1.1\. Common heuristic comparisons
  id: totrans-518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 常见启发式比较
- en: Armchair track and field enthusiasts tend to compare world records in different
    events by how old they are or how close anyone else has come to breaking them.
    Michael Johnson’s 200 m dash world record was 12 years old when Usain Bolt broke
    it, whereas the 100 m world record was less than a year old when Usain Bolt broke
    it the first time in early 2008, before breaking it again at the 2008 Olympics
    and the 2009 World Championships. The age of a world record probably does indicate
    some amount of strength of the record, but it’s certainly not a perfect measure.
    Was Bolt’s 19.19 sec mark for the 200 m worse than his 19.30 sec mark because
    the previous record was younger? Certainly not.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 坐在家里的田径爱好者倾向于通过世界纪录的年龄或其他人接近打破它们的程度来比较不同项目的世界纪录。迈克尔·约翰逊的200米世界纪录在尤塞恩·博尔特打破它时已经12年了，而100米世界纪录在博尔特在2008年初第一次打破它时不到一年，之后又在2008年奥运会和2009年世界锦标赛上再次打破。世界纪录的年龄可能确实表明了记录的一些强度，但这绝对不是完美的衡量标准。博尔特的200米19.19秒的成绩比他的19.30秒成绩差，仅仅是因为之前的记录更年轻吗？当然不是。
- en: Sometimes people will cite the percentage improvement of a mark over the second-best
    mark as evidence that it’s good. The world record javelin throw by Jan Zelezný
    of 98.48 m is 2.9% farther than his second-best throw and 5.8% farther than anyone
    else’s ever. Usain Bolt’s still-current world record of 9.58 sec was 1.1% faster
    than his second-best time and 1.3% faster than the fastest mark by another person,
    which happened to be the second-place finisher in the same race, Tyson Gay. Again,
    this is a reasonable indicator of a good performance but is nowhere near perfect
    because of the high variance of those second-best performances. If, for some reason,
    the second-best performance had never happened, the percentage could change dramatically.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候人们会引用一个分数相对于第二高分分数的百分比提升作为它是好的证据。捷克运动员扬·泽利兹尼的世界纪录标枪投掷成绩为98.48米，比他的第二高分投掷远了2.9%，比任何其他人的最高成绩远了5.8%。牙买加运动员尤塞恩·博尔特目前保持的世界纪录9.58秒，比他的第二快成绩快了1.1%，比另一位运动员的最好成绩快了1.3%，而这位运动员恰好是同一比赛中的第二名，他就是泰森·盖伊。再次强调，这确实是良好表现的合理指标，但由于这些第二高分表现的高变异性，它远非完美。如果，出于某种原因，第二高分表现从未发生过，百分比可能会发生剧烈变化。
- en: 4.1.2\. IAAF Scoring Tables
  id: totrans-521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 国际田联评分表
- en: More sophisticated methods existed at the time I began this project in 2009,
    but they also had their shortcomings. The most widely accepted method for comparing
    performances between events in track and field among coaches, fans, and many others
    is a set of points tables called the IAAF Scoring Tables of Athletics. Typically
    every few years, an updated set of points tables is published by the International
    Association of Athletics Federations (IAAF). The IAAF also publishes the *combined
    events* points tables that are used in the multidiscipline events such as the
    men’s decathlon and women’s heptathlon, in which competitors are given points
    for their performances in each of the disciplines, and the winner is the athlete
    with the most total points. In combined events, the points tables are the basis
    for the competition itself. The Scoring Tables for individual events have little
    effect on competition, with the exception of certain track and field meetings
    that award prizes based on the tables.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在我2009年开始这个项目的时候，存在更复杂的方法，但它们也有其不足之处。教练、粉丝以及许多人普遍接受的方法是，在田径项目中比较不同项目表现的是一套称为国际田联田径评分表的点数表。通常每几年，国际田联就会发布一套更新的点数表。国际田联还发布了用于多学科赛事（如男子十项全能和女子七项全能）的*综合项目*点数表，在这些赛事中，选手在每个项目中的表现都会得到分数，而获胜者是总分最高的运动员。在综合项目中，点数表是比赛本身的基础。个人项目的评分表对比赛的影响很小，除了某些基于表格颁发奖品的田径会议。
- en: The 2008 Scoring Tables, the most recent tables at the time Usain Bolt ran 9.58
    sec in the 100 m dash, gave the world-record performance a score of 1374 (the
    tables’ highest score listed is 1400 points). In the next update to the Scoring
    Tables, in 2011, Usain Bolt’s 2009 performance received a score of 1356\. This
    was quite a dramatic change in score, and such a large change did not occur in
    most events. For reference, according to the 2008 tables, 9.58 sec 100 m was equivalent
    to a 42.09 sec 400 m dash, whereas according to the 2011 tables, it was equivalent
    to a 42.37 sec 400 m. In an Olympic 400 m final, 0.28 sec is easily the difference
    between a gold medal and no medal. According to the Scoring Tables, Usain Bolt’s
    world record was getting worse.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年的评分表，在尤塞恩·博尔特以9.58秒完成100米赛跑时，当时最新的表格，给世界纪录表现评分为1374分（表格中列出的最高分数是1400分）。在评分表的下一轮更新中，2011年，尤塞恩·博尔特2009年的表现得到了1356分的评分。这种评分的巨大变化在大多数项目中都没有发生。为了参考，根据2008年的表格，9.58秒的100米成绩相当于42.09秒的400米赛跑，而根据2011年的表格，它相当于42.37秒的400米。在奥运会的400米决赛中，0.28秒的差距很容易就是金牌和没有奖牌的区别。根据评分表，尤塞恩·博尔特的世界纪录正在变差。
- en: There is a reason for this. The IAAF’s Combined Scoring Tables are known to
    be based on a relatively small set of the best performances in each event. The
    small set might be the top 10 or top 25, but I’m merely guessing, because the
    methods aren’t fully disclosed. If the tables are based heavily on a small set
    of the top performances, then a dramatic change in this set will dramatically
    affect the scores in the next update to the tables. The 2008 and 2009 track and
    field seasons produced some incredible 100 m performances, even without Usain
    Bolt. These performances affected the next set of scores, released in 2011. In
    some sense, through the Scoring Tables, Usain Bolt’s world-record performances
    and the strong performances of his rivals eventually made themselves less impressive.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有原因的。国际田联的综合评分表众所周知是基于每个项目相对较小的一组最佳成绩。这组小数据可能包括前10名或前25名，但我只是猜测，因为方法并没有完全公开。如果表格高度依赖于一小组顶尖表现，那么这一组的变化将对下一次更新表格的分数产生显著影响。2008年和2009年的田径赛季产生了一些令人难以置信的100米成绩，即使没有尤塞恩·博尔特。这些成绩影响了2011年发布的下一组分数。从某种意义上说，通过评分表，尤塞恩·博尔特的世界纪录表现以及他对手的强劲表现最终变得不那么令人印象深刻。
- en: By recounting all of this, I mean only to show how basing models on too little
    data can significantly distort results and conclusions. In the Scoring Tables,
    a few great performances within two or three years of great results drastically
    changed our impression of how good such performances are. I made it my goal to
    use all data I could find to generate a scoring method that would not only be
    less sensitive to changes in the best performances but would also be a good predictor
    of future level of performance, which represents a good set of out-of-sample data.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回顾这一切，我的意思是只想展示基于太少数据构建模型如何会显著扭曲结果和结论。在评分表中，过去两三年内的一些出色表现极大地改变了我们对这些表现有多好的印象。我的目标是使用我能找到的所有数据来生成一种评分方法，这种方法不仅对最佳表现的改变不太敏感，而且还能很好地预测未来表现水平，这代表了一组良好的外样本数据。
- en: 4.1.3\. Comparing performances using all data available
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 使用所有可用数据比较表现
- en: 'My biggest question when starting out: how much data can I find? All manner
    of searching the internet and asking friends led me to conclude that [alltime-athletics.com](http://alltime-athletics.com)
    provides the most complete set of elite track and field performances available.
    The site provides lists of the top performances ever in all Olympic track and
    field events; for some events, the lists contain thousands of performances. I
    felt that if I used all of this data and a little statistical knowledge, I could
    improve on the robustness and predictive power of the IAAF’s Scoring Tables.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我开始时的最大疑问：我能找到多少数据？各种互联网搜索和询问朋友让我得出结论，[alltime-athletics.com](http://alltime-athletics.com)
    提供了最完整的精英田径表现数据集。该网站提供了所有奥运田径项目的最佳表现列表；对于某些项目，列表中包含数千次表现。我觉得，如果我能使用所有这些数据和一点统计知识，我就能提高IAAF评分表的鲁棒性和预测能力。
- en: The first step was to wrangle the data. The lists of the top performances were
    on a website but weren’t available in a convenient format such as CSV, so I’d
    have to resort to web scraping in some way. In addition to that, I’d need to compare
    my scores and results with those from my leading competitor, the IAAF Scoring
    Tables, which were available only in PDF. Neither web pages nor PDFs are ideal
    for programmatic parsing, and both can get quite messy when considering HTML structure
    in web pages and page headers, footers, and numbers in PDFs. Simply put, it would
    take some wrangling.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是整理数据。最佳表现列表在网站上，但不是以CSV等方便的格式提供，所以我必须通过某种方式进行网络抓取。此外，我还需要将我的分数和结果与我的主要竞争对手IAAF评分表进行比较，这些评分表仅以PDF格式提供。网页和PDF都不是程序解析的理想选择，考虑到网页中的HTML结构和PDF中的页眉、页脚和数字，两者都可能变得相当杂乱。简单来说，这将需要一些整理。
- en: 'Throughout the following sections, I’ll continue to refer to the two wrangling
    tasks in this track and field project:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我将继续提到这个田径项目中的两个整理任务：
- en: Wrangling the lists of top performances from the website [alltime-athletics.com](http://alltime-athletics.com)
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网站 [alltime-athletics.com](http://alltime-athletics.com) 整理最佳表现列表
- en: Wrangling the IAAF Scoring Tables from the PDF that contains them
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含它们的PDF中整理IAAF评分表
- en: 4.2\. Getting ready to wrangle
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 准备开始整理数据
- en: Some people like to dive in immediately and start throwing data around, but
    that’s not my style. I’m a bit more deliberate. I like to look around a bit before
    I write any code or commit to trying any strategies. I do a few things to gather
    some good information about the data that can help me wrangle more effectively.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人喜欢立即跳进去，开始随意处理数据，但那不是我的风格。我更倾向于谨慎行事。我喜欢在编写任何代码或尝试任何策略之前先四处看看。我会做一些事情来收集一些关于数据的良好信息，这有助于我更有效地整理数据。
- en: In this section, first I show you what it might look like to have messy data
    that needs to be wrangled. Then I list a few steps that you can take before you
    begin to wrangle that are helpful for figuring out what you have, what you should
    do, and how much trouble you might be in.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先我会向你展示整理前可能遇到的杂乱数据的样子。然后我会列出一些在你开始整理之前可以采取的步骤，这些步骤有助于你弄清楚你有什么，你应该做什么，以及你可能遇到多少麻烦。
- en: 4.2.1\. Some types of messy data
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 一些杂乱数据类型
- en: The thing about messy data sets is that each set is messy in its own way. If
    all messy data looked the same, we would find a way to parse it and use it quickly
    and efficiently. Although I can’t possibly enumerate every way that data is messy
    and in need of wrangling, I can describe a few ways and hope that it helps you
    get an idea of the ways things can go wrong as you dive into a new data set and
    how you might be able to prepare for it.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 关于杂乱的数据集，每个数据集都有其独特的方式。如果所有杂乱的数据看起来都一样，我们就会找到一种方法来解析它并快速高效地使用它。尽管我无法一一列举数据杂乱和需要整理的每一种方式，但我可以描述几种方式，并希望这能帮助你了解在深入研究新数据集时可能会出现的问题，以及你如何为此做好准备。
- en: If you’ve been working in data science for a few years, I’m sure you’ve seen
    cases like these—or worse. As of 2016, we still haven’t entered the Era of Clean
    Data, and I’m beginning to wonder if we’ll ever get there.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在数据科学领域工作了几年，我敢肯定你一定见过这样的案例——或者更糟。截至2016年，我们还没有进入“干净数据时代”，我开始怀疑我们是否能够实现。
- en: Data to be scraped
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 需要爬取的数据
- en: I mentioned web scraping in [chapter 3](kindle_split_012_split_000.xhtml#ch03),
    but you might also scrape data from PDFs and other unconventional sources. *Scraping*
    is the process of programmatically pulling selected elements from sources that
    weren’t designed to be accessed programmatically. The data to be scraped is usually
    poorly structured. But if you write a sophisticated scraper, the data may be neat
    and tidy at the end.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[第3章](kindle_split_012_split_000.xhtml#ch03)中提到了网络爬虫，但你也可以从PDF和其他非传统来源中爬取数据。“爬取”是指从未设计为程序访问的来源中程序性地提取选定元素的过程。要爬取的数据通常结构不佳。但如果你编写一个复杂的爬虫，数据最终可能会变得整洁有序。
- en: Corrupted data
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 损坏的资料
- en: 'Sometimes you may find data that’s not only poorly formatted but downright
    corrupted. This usually means that some aspect of the file is either missing or
    has been obfuscated because of a disk error or other low-level problem. It can
    be like losing the instructions for building a model plane or accidentally mixing
    up a stack of index cards that are supposed to be kept in order. A commonly corrupted
    file format is PST, an email archive. But thankfully we have many tools for recovering
    most of the data from such a corrupted file. (That’s one of the few benefits of
    having a file format that’s often corrupted: someone developed an anti-corruption
    tool!)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你会发现数据不仅格式不佳，而且直接损坏。这通常意味着文件的一些方面要么缺失，要么由于磁盘错误或其他低级问题而被隐藏。这可以像丢失制作模型飞机的说明或意外混淆应该按顺序排列的一叠索引卡一样。常见的损坏文件格式是PST，一种电子邮件存档。但幸运的是，我们有许多工具可以从这样的损坏文件中恢复大部分数据。（这是文件格式经常损坏的少数好处之一：有人开发了一个反损坏工具！）
- en: Poorly designed databases
  id: totrans-542
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设计不良的数据库
- en: Databases have growing pains too, and sometimes that means that two databases
    that weren’t intended to be used together are now your best sources of information.
    Sources of error include database values or keys that don’t match each other and
    incongruences in scope, depth, APIs, or schemas.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库也有成长的烦恼，有时这意味着原本不打算一起使用的两个数据库现在成为了你获取信息的最佳来源。错误来源包括数据库值或键不匹配以及范围、深度、API或模式的不一致。
- en: 4.2.2\. Pretend you’re an algorithm
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2. 假装你是一个算法
- en: I’ve already discussed web scraping in general, and given that the data I wanted
    for my track and field project was available on a website, scraping seemed like
    a good choice. In the Google Chrome browser I used the option View Page Source
    to see the raw HTML that composes the page that lists the top men’s 100 m dash
    results of all time. This is what a programmatic scraper will be reading.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经讨论了一般性的网络爬虫，鉴于我为我田径项目所需的数据在网站上可用，爬虫似乎是一个不错的选择。在Google Chrome浏览器中，我使用了“查看页面源代码”选项来查看构成列出史上所有男子100米短跑最佳成绩页面原始HTML。这就是程序化爬虫将要读取的内容。
- en: Sometimes it’s easy to parse through HTML to extract the elements you want,
    but sometimes it takes a bit of imagination. I tend toward role playing; pretending
    to be a wrangling script—a bit of code that programmatically turns messy data
    into nice data—helps me figure out how I might write a script later, and it gives
    me a good idea of how difficult that task might be and what problems I might encounter.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候解析HTML以提取所需元素很容易，但有时候需要一点想象力。我倾向于角色扮演；假装我是一个整理脚本——一段将杂乱数据程序性地转换为整洁数据的代码——这有助于我思考如何编写脚本，并给我一个很好的想法，了解这项任务可能有多难以及我可能会遇到什么问题。
- en: 'The first step in wrangling, and in pretending to be a script, is to look at
    the raw data (for example, using View Page Source on a web page or looking at
    HTML in a text editor). This is what any code that you write will be seeing, and
    so it’s what you should look at when figuring out how to deal with it. Some header
    lines and other material at the top of the page, a section of the page containing
    the men’s 100 m dash data, look like this:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 整理的第一步，以及假装成脚本，是查看原始数据（例如，使用网页上的“查看页面源代码”或使用文本编辑器查看HTML）。你编写的任何代码都将看到这些，所以在考虑如何处理它时，你应该查看这些内容。页面顶部的某些标题行和其他材料，以及包含男子100米冲刺数据的页面部分，看起来是这样的：
- en: '[PRE18]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is the section of the page where the list of the best men’s 100 m performances
    starts. Because I’m writing this in 2016, there are performances on this list
    now that were not there when I first analyzed this data in 2011\. But the format
    is the same.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 这是页面中开始列出最佳男子100米成绩的部分。因为我是在2016年写这篇文档的，所以现在列表中有一些在2011年我首次分析这些数据时并不存在的成绩。但格式是一样的。
- en: Stepping into my role as a wrangling script, I imagine what I would do if I
    encountered this chunk of HTML. Nothing before this section is useful to me, so
    the main goal is to start capturing the top marks at this point. I recognize the
    line containing Usain Bolt’s 9.58 sec performance as the world record and beginning
    of the data I want to capture, but how will a script recognize it?
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 当我扮演一个整理脚本的角色时，我会想象如果我遇到这一块HTML会怎么做。这个部分之前的内容对我没有用，所以主要目标是从此处开始捕捉最高分。我认出包含尤塞恩·博尔特9.58秒世界纪录的那一行，这是我要捕捉的数据的开始，但脚本将如何识别它呢？
- en: 'One way to recognize an athlete’s performance is to test each line of the file
    to see if it looks like this:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 识别运动员成绩的一种方法是对文件的每一行进行测试，看它是否看起来像这样：
- en: '[PRE19]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Any method that tries to match each line of this HTML would have to be able
    to recognize integers, numbers, names, countries, and the like. This task is often
    more complex than it initially may seem. How, for example, would you test for
    a name in the third column? Would you test for capitalized words? Two words separated
    by a space? Would `Joshua J. Johnson` meet your criteria? What about `Leonard
    Miles-Mills` or the city `Kuala Lumpur`? All of these appear in the list. This
    process is less simple than it looks, and so I usually try a different strategy
    before resorting to pattern recognition.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 任何试图匹配此HTML每一行的方法都必须能够识别整数、数字、名称、国家等等。这项任务可能比最初看起来要复杂得多。例如，你将如何测试第三列中的名称？你会测试首字母大写的单词吗？由空格分隔的两个单词？`Joshua
    J. Johnson`是否符合你的标准？又比如`Leonard Miles-Mills`或者城市`Kuala Lumpur`？所有这些都在列表中。这个过程看起来并不简单，所以我通常在求助于模式识别之前尝试不同的策略。
- en: Document structure, particularly in HTML or XML, can provide good clues to wrangling
    scripts about where the valuable data starts. What comes right before the data?
    In this case, the data is preceded by a `<PRE>` tag. Regardless of what this tag
    means, it’s worth checking to see if it appears often on the page or only right
    before the data set starts. Investigating this, I see that in the page, the first
    time the `<PRE>` tag appears is right before the data. And, thankfully, this is
    true of the pages for all track and field events, so I could safely use the `<PRE>`
    tag to denote the beginning of the data set for each of the events.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 文档结构，尤其是在HTML或XML中，可以为整理脚本提供关于有价值数据开始位置的良好线索。数据之前是什么？在这种情况下，数据是由一个`<PRE>`标签
    precedes。不管这个标签代表什么，都值得检查它是否经常出现在页面上，或者是否只在数据集开始之前出现。调查这个，我发现在这个页面上，`<PRE>`标签第一次出现是在数据之前。而且，幸运的是，这一点对所有田径项目的页面都是适用的，所以我可以安全地使用`<PRE>`标签来表示每个事件数据集的开始。
- en: Imagining myself as a wrangling script, I would begin reading the HTML line
    by line, and I’d look for a `<PRE>` tag. When I find a `<PRE>` tag, I know that
    the next line will be a row of data in the particular table format, where the
    second column is the measured performance, the third column is the name, the fourth
    is the country, and so on. The text parser in your favorite scripting language
    can read that line, separate the columns into fields using either tab characters
    or multiple consecutive spaces, and store each text field in a variable or data
    structure.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 将自己想象成一个整理脚本，我会逐行读取HTML，并寻找`<PRE>`标签。当我找到一个`<PRE>`标签时，我知道下一行将是特定表格格式中的数据行，其中第二列是测量的性能，第三列是姓名，第四列是国家，等等。你喜欢的脚本语言中的文本解析器可以读取这一行，使用制表符或多个连续空格将列分离成字段，并将每个文本字段存储在变量或数据结构中。
- en: '4.2.3\. Keep imagining: what are the possible obstacles and uncertainties?'
  id: totrans-556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3. 持续想象：可能遇到的障碍和不确定性有哪些？
- en: Now that you know where the valuable data starts within each HTML page and how
    to begin capturing lines of data, you still have to continue playing the role,
    in your mind, of the wrangling script, as it continues line by line through the
    file.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道每个HTML页面中宝贵数据开始的位置以及如何开始捕获数据行，你仍然需要在心里继续扮演整理脚本的角色，因为它会逐行通过文件。
- en: 'As I scroll downward through the raw HTML of men’s 100 m performances, most
    lines look like they should: performance, name, country, and so on, all in their
    appropriate places. But every now and then there’s a funny character sequence.
    Sometimes in the city column I see the text `&uuml;` or `&eacute;` between some
    other letters. These entries seem weird, and I worry that a script (wait, that’s
    me!) might not know what to do. I look for the corresponding places in the rendered
    HTML (the web page itself) and realize that these character sequences are the
    HTML representations of ü and é, respectively. I also could perform an internet
    search to figure out what those character sequences mean.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 当我向下滚动查看男子100米表现的原始HTML时，大多数行看起来都应该是这样的：性能、姓名、国家等，都在它们适当的位置。但时不时地会出现一些奇怪的字符序列。有时在城市列中，我看到一些字母之间有文本`&uuml;`或`&eacute;`。这些条目看起来很奇怪，我担心脚本（等等，那是我！）可能不知道该怎么办。我查找渲染HTML（网页本身）中的相应位置，并意识到这些字符序列分别是ü和é的HTML表示。我也可以进行网络搜索来弄清楚这些字符序列的含义。
- en: Zürich and San José occur fairly often in the list of locations for some of
    the top performances, so it’s obvious I need to worry about characters that are
    represented by special character sequences in HTML. Are there others I haven’t
    seen yet? Should I be wary whenever an ampersand occurs in any field? The answers
    to each of these questions is likely yes, but I’d like to draw attention to the
    fact that *I don’t know yet*.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世和圣何塞在顶级表现地点列表中出现的频率相当高，所以很明显我需要担心HTML中表示特殊字符序列的字符。还有我没有见过的其他字符吗？每当任何字段中出现&符号时，我应该小心吗？对这些问题的答案可能是肯定的，但我想要强调的是*我还没有弄清楚*。
- en: 'This is an important point in data wrangling: lots of things can happen that
    you might not expect. Therefore, if there is one tenet of wrangling, it is this:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在数据整理中的一个重要观点：可能会发生很多你预料不到的事情。因此，如果有一个整理原则，那就是：
- en: '*Double-check everything.*'
  id: totrans-561
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*双重检查一切*。'
- en: A manual second check is preferred, but a programmatic check can work, too,
    if you’re careful. If I don’t have an absurd amount of data, as a second check
    I like to scroll through the post-wrangle, supposedly clean data files. Sometimes
    a quick scroll-through can reveal some obvious mistakes that an hour of software
    debugging would reveal. What if a column of data shifted somewhere? One extra
    tab character can mess up a lot of parsing algorithms, including some standard
    R packages and the occasional Excel import, among others.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 建议进行第二次手动检查，但如果小心操作，程序性检查也可以工作。如果我没有大量的数据，作为第二次检查，我喜欢浏览经过整理后的、理论上干净的数据文件。有时候快速浏览就能发现一些明显的错误，这些错误可能需要一小时的软件调试才能发现。如果数据列移动了位置怎么办？一个额外的制表符字符可能会搞乱很多解析算法，包括一些标准的R包和偶尔的Excel导入等。
- en: What else might go wrong while trying to wrangle some data programmatically?
    What in the file(s) looks a little weird? What checks can you do afterward that
    might uncover some important mistakes in your wrangling? Every case is different,
    but every case deserves careful and deliberate thought about any of the possibilities
    for parsing error. Yet again, awareness is the most important thing a data scientist
    can have at this point.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试以编程方式整理数据时，还可能发生什么问题？文件（们）中有什么看起来有点奇怪的地方？在整理之后，你可以进行哪些检查，可能会发现整理过程中的一些重要错误？每个案例都是不同的，但每个案例都值得仔细和深思熟虑地考虑任何可能的解析错误。再次强调，意识是数据科学家在这个阶段最重要的东西。
- en: 4.2.4\. Look at the end of the data and the file
  id: totrans-564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 查看数据末尾和文件
- en: If I’m pretending to be a wrangling script, I’m going to start at the beginning
    of the file and finish at the end. Many unexpected things can happen in the middle,
    so there are no guarantees that I’ll arrive at the end of the file in the expected
    state. I may have wrangled people’s names in the place of cities and dates in
    the place of countries of origin. Who knows, unless I’ve had the amazing luck
    to write a bugless script for a data set without irregularities? Does such a script
    or such a data set exist? Assuming that’s not the case—and assuming I’ve made
    at least one mistake in my current conception of a wrangling script—I probably
    should examine the wrangled data file(s) at the beginning, at the end, and at
    least a few places in the middle.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我假装自己是一个整理脚本，我会从文件的开始处开始，到文件的末尾结束。在中间可能会发生许多意外的事情，所以我无法保证我会以预期的状态到达文件的末尾。我可能在城市的位置整理了人们的名字，在原产国的位置整理了日期。谁知道呢，除非我有写出一个无错误的脚本的惊人运气，而这个数据集没有不规则性？这样的脚本或这样的数据集存在吗？假设情况并非如此——并且假设我在当前对整理脚本的构想中至少犯了一个错误——我可能应该检查整理后的数据文件（们）的开始、末尾以及至少中间的一些地方。
- en: 'It takes a lot of scrolling through the list of men’s 100 m performances to
    find the first line of HTML that doesn’t belong in the data set. It took me an
    embarrassingly long time before I realized that there were additional, nonstandard
    lists of best performances at the bottom of the pages. For the men’s 100 m page,
    there are lists for running start and manual timing after the main list. These
    lists look identical to the main list but are separated from the main list by
    a few tags of HTML. The transition from the main list to the first auxiliary list
    looks like this:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到不属于数据集的第一行HTML，需要浏览男子100米成绩列表的末尾。在我意识到页面的底部有额外的、非标准的最佳成绩列表之前，我花了令人尴尬的长的时间。对于男子100米页面，除了主列表之后，还有起跑和手动计时的列表。这些列表看起来与主列表相同，但通过几个HTML标签与主列表分开。从主列表到第一个辅助列表的过渡看起来是这样的：
- en: '[PRE20]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The HTML tag that denoted the beginning of the desired data, `<PRE>`, is closed
    at the end of the main list with the `</pre>` tag closure. (Note that HTML tags
    are generally not case sensitive.) This would be a good way to end the parsing
    of the data set. As a wrangling script, that’s what I’d do, unless I want to capture
    the auxiliary lists as well. Do I? In this case, no I don’t. I want only completely
    legal marks that abide by all world-record-eligible rules, because I want to compare
    world records and every performance that came close. It’s probably not helpful
    to consider performances under conditions that are nonstandard and inherently
    different from the core set of legal performances.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 表示所需数据开始的HTML标签`<PRE>`在主列表的末尾用`</pre>`标签关闭。（注意，HTML标签通常不区分大小写。）这将是一个结束数据集解析的好方法。作为一个整理脚本，我会这样做，除非我想同时捕获辅助列表。我这样做吗？在这种情况下，不，我不这样做。我只想要完全合法的标记，这些标记遵守所有世界纪录资格规则，因为我想要比较世界纪录以及所有接近世界纪录的表现。考虑在非标准且本质上与合法表现核心集不同的条件下进行的表现可能并不有用。
- en: If my wrangling script ignored the end of the useful data set, in this case
    the `</pre>` tag closure, and assumed that the data continued until the end of
    the file, the script would probably collect the nonstandard results at the bottom
    of the page. Or the script wouldn’t know what to do when the data stopped fitting
    the appropriate column format that was established. In this case and many others,
    looking at the end of the wrangled data file is crucial to determining whether
    the data wrangling was successful. It’s up to you, the data scientist, to decide
    which aspects of wrangling are most important and to make sure those aspects are
    completed properly. But it’s almost always important to scan the file to the end,
    if for no other reason than to make sure that nothing weird happened right before
    the script finished.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我的整理脚本忽略了有用数据集的末尾，在这个例子中是`</pre>`标签的闭合，并假设数据一直持续到文件末尾，那么脚本可能会收集页面底部的非标准结果。或者，当数据不再符合已建立的适当列格式时，脚本可能不知道该怎么做。在这种情况下和许多其他情况下，查看整理后的数据文件末尾对于确定数据整理是否成功至关重要。这取决于你，作为数据科学家，决定整理的哪些方面最重要，并确保这些方面得到适当完成。但几乎总是很重要，要扫描文件到末尾，至少要确保在脚本完成之前没有发生任何奇怪的事情。
- en: 4.2.5\. Make a plan
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 制定计划
- en: Now that I’ve discussed the process of imagining yourself as a wrangling script,
    parsing through raw data and extracting the parts that are needed, the next step
    is to consider all the information that you’ve gathered so far and to make a plan
    for wrangling.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经讨论了想象自己是一个整理脚本的流程，解析原始数据并提取所需的部分，下一步是考虑你迄今为止收集到的所有信息，并制定整理计划。
- en: Based on what you’ve seen in the track and field data, a good option would be
    to download all the web pages containing all the Olympic track and field events
    and to parse them as we discussed, using HTML structure as appropriate, double-checking
    everything. One complication of this that I haven’t yet mentioned—if you’re going
    to use a pure web-scraping strategy—is that you need a list of all of the web
    addresses of the pages for individual events in order to download all of them
    programmatically. Sometimes such a list of site addresses to scrape is easy to
    generate, but each of the 48 individual pages (24 Olympic events each, for men
    and women) has a unique address that needs to be copied or typed manually. Therefore,
    you might need to manually create a list of 48 web addresses that needed to be
    scraped, plus you’d need to write the wrangling script that parses the HTML, as
    already discussed. That’s one potential plan that could wrangle the data you need.
    But because each page address needs to be copied or typed manually, I don’t think
    you’d save much time by programmatically visiting each of the manually typed page
    addresses when compared to downloading each of the pages manually. Those were
    two good options, but they weren’t the only ways to wrangle the data I needed.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你在田径数据中看到的内容，一个不错的选择是下载包含所有奥运田径赛事的所有网页，并像我们讨论的那样解析它们，适当使用HTML结构，并仔细检查每一项。我还没有提到的这个问题的复杂性之一是——如果你打算使用纯网页抓取策略——你需要一个包含所有单个赛事网页的网址列表，以便程序化地下载所有这些网页。有时生成这样的网址列表很容易，但每个48个单独的页面（每个奥运项目男女各24项）都有一个唯一的地址需要手动复制或输入。因此，你可能需要手动创建一个包含48个需要抓取的网页地址的列表，此外，你还需要编写解析HTML的脚本，正如已经讨论的那样。这是可能整理所需数据的计划之一。但是，由于每个页面地址都需要手动复制或输入，与手动下载每个页面相比，我认为通过程序化访问手动输入的每个页面地址并不会节省多少时间。那两个都是不错的选择，但它们并不是整理所需数据的唯一方法。
- en: In the end, I decided not to go with web scraping. Parsing through HTML structure
    wouldn’t have been incredibly hard, as you’ve seen, but I realized there was another
    way. The pages of [alltime-athletics.com](http://alltime-athletics.com) aren’t
    heavy with HTML. The HTML and the text that appears on the web page as rendered
    HTML aren’t too different. There’s not much styling on the page, particularly
    in the part of the page containing the data I needed. In both cases, the top performances
    were given as a whitespace-separated table, with one performance per line. In
    that way, there was little difference between raw HTML and rendered HTML. As I’ve
    already mentioned, however, some individual characters appeared differently in
    raw HTML than they did in rendered HTML. For example, in raw HTML, a city might
    appear as `Z&uuml;rich` but in rendered HTML it would appear as `Zürich`.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我决定不采用网络爬虫的方式。正如你所见，解析HTML结构并不会特别困难，但我意识到还有另一种方法。在[alltime-athletics.com](http://alltime-athletics.com)上的页面并没有大量使用HTML。网页上作为渲染HTML显示的HTML和文本并没有太大的区别。页面上没有太多样式，尤其是在包含所需数据的页面部分。在两种情况下，最佳表现都以空格分隔的表格形式给出，每行一个表现。因此，原始HTML和渲染HTML之间几乎没有区别。然而，正如我已经提到的，一些个别字符在原始HTML中的显示方式与在渲染HTML中的显示方式不同。例如，在原始HTML中，一个城市可能显示为`Z&uuml;rich`，但在渲染HTML中则显示为`Zürich`。
- en: I decided to take the post-HTML, already rendered web page version so I didn’t
    have to worry about character rendering or any other HTML-parsing issues. I needed
    data from 48 pages, and instead of writing (or pretending to be) a script that
    would download each of these pages and parse the HTML for each Olympic event,
    I decided to copy the text from each of the web pages myself.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定采用已经渲染好的网页版本，这样我就不必担心字符渲染或其他HTML解析问题。我需要从48个页面中获取数据，而不是编写（或假装是）一个脚本，该脚本将下载每个页面并解析每个奥运项目的HTML，我决定亲自从每个网页上复制文本。
- en: 'I would visit each of the 48 web pages with Chrome, my web browser, press Ctrl-A
    to select all text on each page, press Ctrl-C to copy the text, and then press
    Ctrl-V to paste the text from each event’s page into a separate flat file. It
    was some manual work to copy the pages, but I wouldn’t have to worry about translating
    HTML or scripting the downloading of the pages. That was the plan I settled on:
    skip the HTML parsing, copy all the pages manually, and then write a simple script
    that would pull the track and field performance data into a usable format. It
    might not be the best plan for every project, but it’s certainly good in this
    scenario.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 我会用Chrome浏览器访问这48个网页，按下Ctrl-A选择每个页面的所有文本，按下Ctrl-C复制文本，然后按下Ctrl-V将每个事件页面的文本粘贴到一个单独的平面文件中。复制页面是一项手动工作，但我不必担心翻译HTML或编写下载页面的脚本。这就是我最终确定计划：跳过HTML解析，手动复制所有页面，然后编写一个简单的脚本，将田径表现数据拉入可用的格式。这可能不是每个项目的最佳计划，但在这个场景中确实很好。
- en: In general, the choice of data wrangling plan should depend heavily on all of
    the information you discover while first investigating the data. If you can imagine
    parsing the data or accessing it in some hypothetical way—I try to play the role
    of a wrangling script—then you can write a script that does the same thing. Pretend
    you’re a wrangling script, imagine what might happen with your data, and then
    write the script later. Data wrangling is such an uncertain process that it’s
    always best to explore a bit and to make a wrangling plan based on what you’ve
    seen.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，数据清洗计划的选取应主要依赖于你在初步调查数据时发现的所有信息。如果你能想象以某种假设的方式解析数据或访问它——我试图扮演一个清洗脚本的角色——那么你可以编写一个执行相同操作的脚本。假装你是一个清洗脚本，想象一下你的数据可能会发生什么，然后稍后编写脚本。数据清洗是一个如此不确定的过程，因此最好先探索一番，并根据你所看到的情况制定一个清洗计划。
- en: 4.3\. Techniques and tools
  id: totrans-577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 技术和工具
- en: Data wrangling is an incredibly abstract process with uncertain outcomes at
    nearly every step. There’s no one way or one tool to accomplish the goal of making
    messy data clean. If someone tells you they have a tool that can wrangle any data,
    then either that tool is a programming language or they’re lying. Many tools are
    good for doing many things, but no one tool can wrangle arbitrary data. Honestly,
    I don’t think that will ever be possible, though we can certainly make progressively
    better tools. Data exists in so many forms and for so many purposes that it’s
    likely that no one application can ever exist that’s able to read arbitrary data
    with an arbitrary purpose. Simply put, data wrangling is an uncertain thing that
    requires specific tools in specific circumstances to get the job done.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理是一个极其抽象的过程，几乎在每一步都有不确定的结果。没有一种方法或一种工具可以完成使杂乱数据变得干净的目标。如果有人告诉你他们有一个可以整理任何数据的工具，那么这个工具要么是编程语言，要么他们在撒谎。许多工具适合做很多事情，但没有一种工具可以整理任意数据。坦白说，我认为这永远也不可能实现，尽管我们可以不断制造出更好的工具。数据以如此多种形式和如此多目的存在，以至于可能没有一种应用程序能够读取具有任意目的的任意数据。简单来说，数据整理是一项不确定的事情，需要根据特定情况使用特定的工具来完成工作。
- en: 4.3.1\. File format converters
  id: totrans-579
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 文件格式转换器
- en: Among HTML, CSV, PDF, TXT, and any other common formats a file might take, it’s
    helpful to know when it’s possible to convert from one file format directly to
    another.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML、CSV、PDF、TXT以及其他可能使用的任何常见文件格式中，了解是否可以直接从一种文件格式转换为另一种文件格式是有帮助的。
- en: The IAAF Scoring Tables are published as a PDF, a format that’s not conducive
    to data analysis. But there are file format converters that can take PDFs and
    produce other file formats, such as text files and HTML. The Unix application
    `pdf2txt` extracts text from PDFs and saves it to a text file. `pdf2html` is another
    useful format converter and something that might be useful to most data scientists.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 国际田联（IAAF）的评分表以PDF格式发布，这种格式不利于数据分析。但有一些文件格式转换器可以将PDF转换为其他文件格式，例如文本文件和HTML。Unix应用程序`pdf2txt`可以从PDF中提取文本并将其保存到文本文件中。`pdf2html`是另一个有用的格式转换器，可能对大多数数据科学家都有用。
- en: Many file format converters are available—many of them are free or open source—and
    it’s certainly worth a Google search to figure out whether the format you have
    is easily convertible to a format you want.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 许多文件格式转换器可供使用——其中许多是免费或开源的——当然值得进行一次谷歌搜索，以确定您拥有的格式是否可以轻松转换为所需的格式。
- en: 4.3.2\. Proprietary data wranglers
  id: totrans-583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 专有数据整理器
- en: In 2016, there’s no shortage of companies that want to wrangle your data for
    you—for a price. In some ways, it’s unsettling how many software products claim
    to be able to do this, when I know from personal experience that many of them
    are severely limited. On the other hand, some of these products work well.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 到2016年，愿意为您整理数据的公司并不短缺——而且往往是要收费的。从某种意义上说，许多软件产品声称能够做到这一点，而我知道从个人经验来看，其中许多产品功能严重受限，这让人感到不安。另一方面，其中一些产品效果很好。
- en: If you can find any proprietary products that can convert the data you have
    into the data you want, then they’re well worth the consideration. Spending a
    little money on these proprietary tools may be worth it if your project gets done
    much earlier, but the industry is young and changing too fast for me to give any
    sort of meaningful survey here.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能找到可以将您拥有的数据转换为所需数据的专有产品，那么它们值得考虑。如果您的项目可以提前完成，那么在这些专有工具上花费一点钱可能是值得的，但这个行业很年轻，变化太快，我无法在这里提供任何有意义的调查。
- en: '4.3.3\. Scripting: use the plan, but then guess and check'
  id: totrans-586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 脚本：使用计划，然后猜测并检查
- en: Earlier in this chapter, I talked about pretending that I was a wrangling script
    and described what I would do in that role. This is exactly the basis for writing
    a real script for wrangling data.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我谈到了假装我是一个整理脚本，并描述了在这个角色中我会做什么。这正是编写真实数据整理脚本的依据。
- en: 'As you imagine being a script and reading through files, recognizing where
    useful data begins and ends and how to parse that data, you should get an idea
    of how complex the task is and what the main features might be. For simpler tasks,
    simple tools are often useful for accomplishing them. The Unix command line, for
    instance, is helpful for tasks like these:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想象自己是一个脚本，阅读文件，识别有用数据的开始和结束，以及如何解析这些数据时，您应该对任务的复杂性和主要功能有一个概念。对于更简单的任务，简单的工具通常很有用。例如，Unix命令行对于这些任务很有帮助：
- en: Extracting all lines from a file that contain a certain word (`grep` is one
    such tool)
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含特定单词的文件中提取所有行（`grep`就是这样的工具之一）
- en: Converting occurrences of a certain word or character into another (`tr`, `sed`,
    `awk`)
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将某个单词或字符的出现转换为另一个（`tr`、`sed`、`awk`）
- en: Cutting files into smaller chunks (`split`)
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件切割成更小的块（`split`）
- en: Those are a few, but if your wrangling plan entails a few such simple steps,
    the command line might be the way to go.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些，但如果你的清洗计划包括几个这样的简单步骤，命令行可能就是最佳选择。
- en: But if you have a more complex set of operations to perform, and none of the
    aforementioned file format converters or proprietary wranglers fits the bill,
    then a scripting language is usually your best bet. Both Python and R are common
    languages for writing wrangling scripts, and it’s usually best to choose whichever
    language you’re most comfortable with, because you’ll probably be trying several
    different techniques in a short amount of time.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你有一系列更复杂的操作要执行，而且上述文件格式转换器或专有清洗工具都不符合要求，那么脚本语言通常是你的最佳选择。Python和R都是编写清洗脚本的常见语言，通常最好选择你最熟悉的语言，因为你可能会在短时间内尝试几种不同的技术。
- en: Writing a wrangling script, at least the first version, is usually not a well-orchestrated
    affair. In fact, it fits nicely with a current usage of the word *hacking* to
    mean trying a bunch of things until you find a few that get the job done. Being
    able to load, manipulate, write, and transform data quickly is the most important
    capability you should strive for when choosing your scripting languages or tools.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 编写数据清洗脚本，至少是第一版，通常不是一个精心安排的过程。实际上，它与当前对“黑客”一词的使用非常契合，即尝试许多事情直到找到几个能完成任务的方法。能够快速加载、操作、编写和转换数据是你选择脚本语言或工具时应追求的最重要能力。
- en: 'Because you’ve already imagined being a script and parsing the data, you should
    have a good general idea what the final script will do, but I’m sure that some
    of the details are still unclear. You can make some informed decisions about how
    best to wrangle, or you can guess and check if that seems more time efficient.
    Now is also a good time to revisit the manual-versus-automate question: can you
    wrangle manually in a shorter time than you can write a script? And will you reuse
    the script? Go ahead and take whichever path seems like the biggest payoff for
    the lowest cost in time and effort.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你已经想象自己是一个脚本并解析数据，你应该对最终脚本将做什么有一个很好的总体概念，但我肯定一些细节仍然不清楚。你可以就如何最好地进行清洗做出一些明智的决定，或者你可以猜测并检查，如果这样做似乎更节省时间。现在也是重新审视手动与自动问题的好时机：你能否在比编写脚本更短的时间内手动清洗？你还会重用脚本吗？继续选择似乎能带来最大回报、成本最低的时间和精力的路径。
- en: Awareness is important while writing a wrangling script. This includes staying
    aware of the status of the data, the script, the results, what the goals are,
    and what each of the potentially many wrangling steps and tools is gaining you.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写数据清洗脚本时，保持警觉性非常重要。这包括关注数据、脚本、结果的状态，明确目标，以及了解每个可能的大量清洗步骤和工具能为你带来什么。
- en: 4.4\. Common pitfalls
  id: totrans-597
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 常见陷阱
- en: With messy data, any wrangling script conceived based on looking at only part
    of that data is bound to have omitted something important. Even if you were observant
    and thorough in considering all possibilities, there’s still a risk that something
    might go wrong. In this section, I outline a few of these pitfalls and give some
    noticeable symptoms that your wrangling script may have fallen into one.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理杂乱的数据时，任何仅基于查看数据的一部分而构思的数据清洗脚本肯定会有遗漏重要内容的情况。即使你在考虑所有可能性时非常警觉和彻底，仍然存在出错的风险。在本节中，我概述了一些这些陷阱，并给出了一些明显的症状，表明你的数据清洗脚本可能已经陷入其中。
- en: 4.4.1\. Watch out for Windows/Mac/Linux problems
  id: totrans-599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1. 注意Windows/Mac/Linux问题
- en: When I first began wrangling data, in 2005 or so, major data incompatibilities
    between the most popular operating systems were a problem that I didn’t think
    would still exist in 2016\. But the three major OSs still haven’t agreed on a
    single way to denote line endings in text files. Any day now, all specs for text
    files might magically be unified across OSs, but until then there will be issues
    with conversion from files on one operating system to another.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在2005年左右开始处理数据时，最流行的操作系统之间的大数据不兼容性问题是我认为在2016年仍然存在的一个问题。但三大操作系统仍然没有就文本文件中行结束的表示达成一致。任何一天，所有文本文件的规范可能都会神奇地跨操作系统统一，但在此之前，从一种操作系统的文件转换到另一种操作系统将会有问题。
- en: Since time immemorial (since the 1970s), Unix—and later, by inheritance, Linux—has
    used the line feed (LF) denotation of a new line, whereas versions of Mac OS before
    version 9.0 used the carriage return (CR) character for a new line. Since 1999,
    Mac OS has joined the other Unix derivatives in using the line feed to denote
    a newline, but Microsoft Windows continues to use a hybrid CR+LF line ending.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 从古至今（自20世纪70年代以来），Unix以及后来的Linux通过继承，都使用换行符（LF）来表示新行，而版本在9.0之前的Mac OS使用回车符（CR）来表示新行。自1999年以来，Mac
    OS已经与其他Unix衍生版本一起使用换行符来表示换行，但Microsoft Windows仍然继续使用混合的CR+LF行结束符。
- en: I won’t attempt to address any specific problems here; they can be varied and
    nuanced. But I should stress that they can sneak up on you. Improperly parsing
    line endings can lead to various problems, so it pays to be wary of the output
    of the parsing. In addition to this and other OS complexities, each programming
    language has its own faculties for reading files of various types. Depending on
    which package or native method you choose to read files, make sure you pay attention
    to how it parses text, line endings, and other special characters. This information
    should be present in the language documentation.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会尝试在这里解决任何具体问题；它们可能各不相同，且复杂多变。但我应该强调的是，它们可能会突然出现在你面前。不正确地解析行结束符可能会导致各种问题，因此要小心处理解析的输出。除了这一点和其他操作系统复杂性之外，每种编程语言都有自己读取不同类型文件的能力。根据你选择的包或本地方法来读取文件，确保你注意它如何解析文本、行结束符和其他特殊字符。这些信息应该包含在语言文档中。
- en: 'When looking at your newly wrangled data, some signs that you may have encountered
    an issue with OS files formats include the following:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看你整理好的新数据时，以下是一些可能表明你遇到了操作系统文件格式问题的迹象：
- en: You seem to have far more lines of text than you think you should have.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你似乎有比你认为的更多的文本行。
- en: You seem to have far too few lines of text.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你似乎有太多的文本行。
- en: Weird-looking characters are interspersed through the file.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件中穿插着一些看起来奇怪的字符。
- en: 4.4.2\. Escape characters
  id: totrans-607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 转义字符
- en: When dealing with text, some characters have special meaning. For example, on
    the most common Unix (or Linux) shells (command-line interpreters), the character
    `*` is a wildcard that represents all files in the current directory. But if you
    precede it with the common escape character `\` as in `\*`, then the special meaning
    is removed, representing only the simple asterisk character. The line
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文本时，一些字符具有特殊含义。例如，在最常见的Unix（或Linux）shell（命令行解释器）中，字符`*`是一个通配符，代表当前目录中的所有文件。但如果你在它前面加上常见的转义字符`\`，如`\*`，那么特殊含义就会消失，只代表简单的星号字符。该行
- en: '[PRE21]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: removes all files in the current directory, whereas
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 删除当前目录中的所有文件，而
- en: '[PRE22]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: removes the file named simply `*`, if it exists. In this case, the backslash
    character is said to *escape* the wildcard character.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在，则删除名为`*`的文件。在这种情况下，反斜杠字符被说成是*转义*通配符。
- en: 'Such escape characters can occur in text files and text/string variables in
    programming languages. For instance, let’s say you want to read a text file that
    looks like this in a text editor:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的转义字符可能出现在编程语言中的文本文件和文本/字符串变量中。例如，假设你想要在一个文本编辑器中读取一个看起来像这样的文本文件：
- en: '[PRE23]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is three lines of text, followed by a line containing no characters. The
    third line contains tab characters in between the letters.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 这是三行文本，后面跟着一个没有任何字符的行。第三行包含字母之间的制表符。
- en: 'Many common programming languages, including Python and R, upon reading this
    file with the simplest file reader (or file stream), would see this:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 许多常见的编程语言，包括Python和R，在用最简单的文件读取器（或文件流）读取此文件时，会看到以下内容：
- en: '[PRE24]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice that the line breaks have been replaced by `\n` and the tab characters
    by `\t`. This contains the same information as before, but it’s encoded in a single
    string, simply by using ASCII characters (`n` and `t`) with escape characters
    (backslash) to represent whitespace elements like the newline and the tab.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，行断点已被`\n`替换，制表符已被`\t`替换。这包含与之前相同的信息，但它被编码在一个单独的字符串中，只需使用ASCII字符（`n`和`t`）和转义字符（反斜杠）来表示空白元素，如换行符和制表符。
- en: 'Another complication: creating string variables in languages such as Python
    or R includes the use of quotation marks, and quotation marks can affect escaping
    in various ways. Let’s put the previous string in double quotes, assign it to
    a variable in Python (the `>>>` represents the Python interpreter prompt), and
    then check the contents of the variable, first by entering only the variable name
    itself and then by using the `print` command:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个复杂的问题是：在Python或R等语言中创建字符串变量时，需要使用引号，而引号可能会以各种方式影响转义。让我们将之前的字符串放在双引号中，将其分配给Python中的一个变量（`>>>`
    代表Python解释器提示符），然后检查变量的内容，首先只输入变量名本身，然后使用 `print` 命令：
- en: '[PRE25]'
  id: totrans-620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that the two checks of the variable contents confirm that it’s a string
    (first shown in single quotation marks) and that this string renders as in the
    original file when making use of the `print` command, which translates the escaped
    characters as the appropriate newline and tab characters. In this way, a single
    string variable can represent the data from an entire file, with each line separated
    by the escaped newline character.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对变量内容的两次检查确认了它是一个字符串（最初以单引号显示）并且当使用 `print` 命令时，这个字符串会以原始文件中的方式渲染，其中转义字符被转换为适当的换行符和制表符字符。这样，一个单独的字符串变量可以表示整个文件的数据，每行由转义换行符分隔。
- en: 'The fun begins when using escape characters within quotations or quotations
    within quotations. Let’s say you have a second file that looks like this in a
    text editor:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 当在引号内使用转义字符或在引号内使用引号时，乐趣就开始了。假设你有一个第二个文件，在文本编辑器中看起来像这样：
- en: '[PRE26]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Storing this file in a single Python variable and then checking its contents
    looks like this:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件存储在单个Python变量中，然后检查其内容看起来像这样：
- en: '[PRE27]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That’s clearly a bit more complicated than the previous example. Because the
    text itself contains quotation marks, these need to be escaped within the string
    variable.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然比之前的例子复杂一些。因为文本本身包含引号，所以这些引号需要在字符串变量中进行转义。
- en: 'As a final example of escape characters, let’s assume you have a data set containing
    some emails. People could have written anything they want in the emails, including
    quotation marks, tabs, or anything else. For an unspecified reason, you want to
    store the email data in a text file, with one email per line. One email reads:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 作为转义字符的最终示例，让我们假设你有一个包含一些电子邮件的数据集。人们可以在电子邮件中写下任何他们想写的内容，包括引号、制表符或其他任何内容。由于某种未指定原因，你想要将电子邮件数据存储在文本文件中，每行一个电子邮件。一个电子邮件的内容如下：
- en: '[PRE28]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This this email could be encoded as the following string:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 这封电子邮件可以编码为以下字符串：
- en: '[PRE29]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And it could be stored in Python as a string variable like this, escaping the
    internal quotations:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以像这样存储在Python中的字符串变量中，转义内部引号：
- en: '[PRE30]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now you want to write this email to one line of a file, so you check the contents
    of the variable and then print/render them (the `print` command replicates in
    the terminal what would be written to a file):'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你想要将这封电子邮件写入文件的一行，因此你检查变量的内容，然后打印/渲染它们（`print` 命令在终端中复制的将是要写入文件的内容）：
- en: '[PRE31]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This isn’t what you intended. The `print` process is rendering the newlines,
    and this data is no longer on a single row. It looks like you have to escape the
    newline characters again, as shown here:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是你想要的结果。`print` 过程正在渲染换行符，这些数据不再位于单行上。看起来你不得不再次转义换行符，如下所示：
- en: '[PRE32]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now it’s printing on a single line, as intended, because you used double backslashes.
    Keep in mind that in the variable `s`, the sequence `\\n` represents a literal
    backslash (only one—the first of the pair is the escaping one) and the character
    `n`. Sometimes this can be confusing, so it’s best to be careful when dealing
    with complex escapes.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它按预期打印在单行上，因为使用了双反斜杠。记住，在变量 `s` 中，序列 `\\n` 代表一个字面上的反斜杠（只有一个——成对中的第一个是转义的那个）和字符
    `n`。有时这可能会令人困惑，因此处理复杂的转义时最好小心。
- en: It doesn’t have to stop at two backslashes. The backslash is a common escape
    character, and you may not have to worry much about it if you’re using smart file
    readers and writers, but if you’re using many nested quotation marks and newlines,
    escaping can be problematic. One time in R, I was using five or six backslashes
    in a row because of some nuanced escaping. It was quite astonishing for me to
    realize that I had to use `\\\\\\n` to accomplish my task, because none of the
    file reader’s parameters seemed to get everything right.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 不必只限于两个反斜杠。反斜杠是一个常见的转义字符，如果你使用的是智能文件读取器和写入器，你可能不必太担心它，但如果你使用了很多嵌套引号和新行，转义可能会成为问题。有一次在R中，我因为一些细微的转义而连续使用了五个或六个反斜杠。我意识到我必须使用`\\\\\\n`来完成我的任务，这对我来说相当令人惊讶，因为文件读取器的所有参数似乎都没有正确处理一切。
- en: 'Symptoms that you might have a problem with escaping may include the following:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 你在转义时可能遇到的问题的迹象可能包括以下内容：
- en: Some lines or strings are too long or too short.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些行或字符串过长或过短。
- en: You try to read a file line by line but end up with one long line.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你试图逐行读取文件，但最终得到的是一行很长的文本。
- en: You find text inside quotation marks that doesn’t belong there.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在引号内发现了一些不属于那里的文本。
- en: You get errors while reading or writing files.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在读取或写入文件时遇到错误。
- en: 4.4.3\. The outliers
  id: totrans-644
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 异常值
- en: Sometimes a data point contains a perfectly valid value, from a logical perspective,
    but that value isn’t realistic from the perspective of a subject matter expert.
    In our track and field example, at some point on the list of top 100 m dash times,
    the number 8.9 may have appeared. Now 8.9 is a perfectly logical number from a
    statistical perspective, but anyone who knows anything about track knows that
    no one has ever run 8.9 seconds in the 100 m dash. It might be that the 8.9 resulted
    in a typo somewhere along the line, or perhaps one row wasn’t formatted right,
    or any other imaginable reason for a data point to be in the wrong place. But
    the point is that sometimes incorrect data can sneak its way into your project
    without causing an error or otherwise making itself obvious. This is where summary
    statistics and exploratory graphs can be helpful.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，一个数据点从逻辑角度来看可能包含一个完全有效的值，但从主题专家的角度来看，这个值并不现实。在我们的田径例子中，在100米短跑最佳时间列表中，某个时刻可能出现了8.9这个数字。现在，从统计角度来看，8.9是一个完全合理的数字，但任何了解田径的人都知道，没有人曾在100米短跑中跑出8.9秒。可能是因为8.9在某个地方出现了打字错误，或者可能是因为某一行格式不正确，或者任何其他可能导致数据点出现在错误位置的可想象原因。但关键是，有时错误的数据可能会悄无声息地进入你的项目，而不会引起错误或使其明显。这就是总结统计和探索性图表可以发挥作用的地方。
- en: In this case, merely checking the range of values—minimum to maximum—could have
    caught the error. During this project I plotted histograms of all of the data,
    not only to check for errors but also to gain some awareness about what the data
    sets looked like. A histogram probably would have highlighted a data error like
    this, though. It’s usually a good idea to spend a little time generating some
    summaries—statistical or visual—even if they don’t seem necessary. They can prevent
    errors and promote awareness. I cover data assessment, including basic descriptive
    statistics, summaries, and diagnostics, in [chapter 5](kindle_split_014_split_000.xhtml#ch05),
    and some of those techniques can be used here as well, to make sure the wrangling
    has been successful.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，仅仅检查值的范围——从最小值到最大值——就可能捕捉到错误。在这个项目中，我绘制了所有数据的直方图，不仅是为了检查错误，也是为了对数据集的外观有一些了解。虽然直方图可能会突出这种数据错误，但通常花点时间生成一些总结——无论是统计的还是视觉的——即使它们看起来并不必要。它们可以防止错误并提高意识。我在[第5章](kindle_split_014_split_000.xhtml#ch05)中涵盖了数据评估，包括基本描述性统计、总结和诊断，其中一些技术也可以在这里使用，以确保数据整理成功。
- en: 4.4.4\. Horror stories around the wranglers’ campfire
  id: totrans-647
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4\. 在整理者营火旁的恐怖故事
- en: 'Any seasoned data scientist has a story or two to tell: data ghost stories
    that haunt a data scientist for years, The One That Got Away, the near-miss when
    disaster was averted by a last-minute detection of The Bug, the time “the data
    was SO bad... (how bad was it?).” I have nothing more to add here; you’ll get
    most of my stories throughout this book, but don’t be afraid to ask your local
    veteran data scientists about theirs. The stories are probably pretty nerdy, but
    hey, aren’t we all?'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 任何经验丰富的数据科学家都有几个故事可以讲述：困扰数据科学家多年的数据幽灵故事，那个“溜走的故事”，灾难即将发生时，通过最后一刻发现“虫子”而避免的险情，以及“数据太糟糕了...（糟糕到什么程度？）”的那个时刻。在这里我没有什么更多要说的；你将在本书中了解到我的大部分故事，但不要害怕向你的当地资深数据科学家询问他们的故事。这些故事可能很“书呆子”，但嘿，我们难道不都是吗？
- en: Exercises
  id: totrans-649
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Continuing with the Filthy Money Forecasting personal finance app scenario
    first described in [chapter 2](kindle_split_011_split_000.xhtml#ch02), and relating
    to previous chapters’ exercises, imagine these scenarios:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在先于[第 2 章](kindle_split_011_split_000.xhtml#ch02)中首次描述的 Filthy Money Forecasting
    个人理财应用场景的基础上继续，并与前几章的练习相关，想象以下场景：
- en: '**1\.**'
  id: totrans-651
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.'
- en: ''
  id: totrans-652
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You’re about to begin pulling data from FMI’s internal database, which is a
    relational database. List three potential problems that you would be wary of while
    accessing and using the data.
  id: totrans-653
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你即将开始从 FMI 的内部数据库中提取数据，这是一个关系型数据库。列出你在访问和使用数据时应该警惕的三个潜在问题。
- en: '**2\.**'
  id: totrans-654
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.'
- en: ''
  id: totrans-655
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the internal database, for each financial transaction you find that a field
    called `description` containing a string (plain text) seems to provide some useful
    information. But the string doesn’t seem to have a consistent format from entry
    to entry, and no one you’ve talked to can tell you exactly how that field is generated
    or any more information about it. What would be your strategy for trying to extract
    information from this field?
  id: totrans-656
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在内部数据库中，对于每个你找到的金融交易，一个名为 `description` 的字段似乎包含一些有用的信息，该字段包含一个字符串（纯文本）。但这个字符串似乎没有从一条记录到另一条记录的固定格式，而且你交谈过的任何人也无法告诉你该字段是如何生成的或更多关于它的信息。你将如何制定从该字段中提取信息的策略？
- en: Summary
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Data wrangling is the process of capturing the useful parts of raw and unkempt
    data under some bad conditions.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理是在一些不良条件下捕获原始和杂乱无章数据的有用部分的过程。
- en: Think like a computer program in order to create a good data-wrangling script.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象自己是一个计算机程序，以便创建一个良好的数据处理脚本。
- en: It helps to familiarize yourself with some typical tools and strategies and
    some common wrangling pitfalls.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉一些典型工具和策略以及一些常见的处理陷阱会有所帮助。
- en: Good wrangling comes down to solid planning before wrangling and then some guessing
    and checking to see what works.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的处理取决于在处理之前进行扎实的规划和一些猜测与检查，以查看哪些方法有效。
- en: Spending a little extra time on data wrangling can save you a lot of pain later.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据处理上花点额外的时间可以让你以后少受很多痛苦。
- en: 'Chapter 5\. Data assessment: poking and prodding'
  id: totrans-663
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章. 数据评估：探索和检查
- en: '*This chapter covers*'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Descriptive statistics and other techniques for learning about your data
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性统计和其他了解数据的技术
- en: Checking assumptions you have about your data and its contents
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查你对数据和其内容的假设
- en: Sifting through your data for examples of things you want to find
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的数据中筛选出你想要找到的事物
- en: Performing quick, rough analyses to gain insight before spending a lot of time
    on software or product development
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在花费大量时间进行软件或产品开发之前进行快速、粗略的分析以获得洞察
- en: '[Figure 5.1](kindle_split_014_split_000.xhtml#ch05fig01) shows where we are
    in the data science process: assessing the data available and the progress we’ve
    made so far. In previous chapters we’ve searched for, captured, and wrangled data.
    Most likely, you’ve learned a lot along the way, but you’re still not ready to
    throw the data at the problem and hope that questions get answered. First, you
    have to learn as much as you can about what you have: its contents, scope, and
    limitations, among other features.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.1](kindle_split_014_split_000.xhtml#ch05fig01) 展示了我们在数据科学流程中的位置：评估可用的数据和到目前为止的进展。在前几章中，我们搜索、捕获和处理了数据。你很可能在这个过程中学到了很多，但你仍然没有准备好将数据扔给问题并希望问题得到解答。首先，你必须尽可能多地了解你所拥有的：其内容、范围和限制，以及其他特征。'
- en: 'Figure 5.1\. The fourth and final step of the preparation phase of the data
    science process: assessing available data and progress so far'
  id: totrans-670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1\. 数据科学流程准备阶段的第四步和最后一步：评估现有数据和到目前为止的进展
- en: '![](Images/05fig01_alt.jpg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![图片 5.1](Images/05fig01_alt.jpg)'
- en: It can be tempting to start developing a data-centric product or sophisticated
    statistical methods as soon as possible, but the benefits of getting to know your
    data are well worth the sacrifice of a little time and effort. If you know more
    about your data—and if you maintain awareness about it and how you might analyze
    it—you’ll make more informed decisions at every step throughout your data science
    project and will reap the benefits later.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易一上来就尽快开发以数据为中心的产品或复杂的统计方法，但了解数据的益处完全值得牺牲一点时间和精力。如果你对你的数据了解得更多——如果你保持对其以及你如何分析它的意识——你将在数据科学项目的每个步骤中做出更明智的决定，并在以后收获好处。
- en: '5.1\. Example: the Enron email data set'
  id: totrans-673
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 示例：恩隆电子邮件数据集
- en: In my first job at a software company—after years of research-oriented, academic
    data science—I was helping to build software that would analyze communication
    patterns of employees of large, heavily regulated organizations in order to detect
    anomalous or problematic behavior. My employer, a Baltimore startup, developed
    software that helps make sense of massive amounts of employee data, which in many
    cases must be archived for a number of years according to current law and often
    contains evidence of wrongdoing that can be useful in investigations of known
    infractions as well as in the detection of yet-unknown ones. Two good examples
    of potential customers are compliance officers at large financial institutions
    and security departments at government agencies. Both of these have the express
    responsibility to prevent anyone from divulging or mishandling privileged or secret
    information. The monitoring of employees’ use of internal networks is often mandated
    or highly recommended by regulating agencies. Needless to say, we needed to do
    a thorough statistical analysis of employee communications and other activity
    while still being extremely careful regarding points of ethics and privacy.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的第一份软件公司工作——在多年的以研究为导向的学术数据科学之后——我正在帮助构建分析大型、高度监管的组织员工沟通模式的软件，以检测异常或问题行为。我的雇主，一家巴尔的摩初创公司，开发了帮助理解大量员工数据的软件，在许多情况下，根据现行法律，这些数据必须存档多年，并且常常包含可以用于调查已知违规行为以及检测未知违规行为的证据。两个潜在的顾客例子是大金融机构的合规官员和政府机构的安保部门。这两者都有明确的责任防止任何人泄露或不当处理机密或秘密信息。监管机构通常要求或强烈建议监控员工对内部网络的的使用。不用说，我们需要对员工沟通和其他活动进行彻底的统计分析，同时还要在伦理和隐私方面极为小心。
- en: But privacy was not much of a concern for one of the first data sets we used
    for demonstration purposes. The set of emails that were collected by investigators
    after the Enron scandal in the early 2000s is now a matter of public record and
    is well documented and studied by researchers ([www.cs.cmu.edu/~./enron](http://www.cs.cmu.edu/~./enron)).
    Because it’s one of the most comprehensive and relevant public data sets available,
    we wanted to use the Enron emails to test and demonstrate the capabilities of
    our software.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于我们最初用于演示目的的第一个数据集来说，隐私并不是一个很大的问题。2000年代初恩隆丑闻后，调查人员收集的电子邮件集合现在是公开记录的一部分，并且已经被研究人员广泛记录和研究([www.cs.cmu.edu/~./enron](http://www.cs.cmu.edu/~./enron))。因为它是最全面和相关的公共数据集之一，我们想使用恩隆电子邮件来测试和展示我们软件的功能。
- en: Several versions of the Enron data set are available, including text versions
    in CSV format as well as the original proprietary format, PST, which can be generated
    by Microsoft Outlook. [Chapter 4](kindle_split_013_split_000.xhtml#ch04) covered
    the basics of data wrangling, and all of the problems and warnings described there
    certainly applied here. Depending on which version of the data set we started
    with, various preprocessing and wrangling steps may have already been done for
    us. Mostly, this was a good thing, but we always had to be wary of mistakes or
    non-standard choices that may have been made before the data got to us.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的恩隆数据集版本有几个，包括CSV格式的文本版本以及原始的专有格式PST，后者可以通过Microsoft Outlook生成。[第4章](kindle_split_013_split_000.xhtml#ch04)介绍了数据整理的基础知识，其中描述的所有问题和警告在这里当然也适用。根据我们从哪个版本的数据集开始，可能已经为我们完成了各种预处理和整理步骤。大多数情况下，这是一件好事，但我们始终必须警惕在数据到达我们之前可能已经做出的错误或非标准选择。
- en: For this reason, and innumerable others, we needed to treat our data set like
    an unfamiliar beast. As with a newly discovered animal species, what we thought
    we had might not be what we had. Our initial assumptions might not have been true,
    and even if they were true, within a (figurative) species, there could have been
    tremendous differences from one individual to another. Likewise, even if you’re
    confident that your data set contains what you think it contains, the data itself
    surely varies from one data point to another. Without a preliminary assessment,
    you may run into problems with outliers, biases, precision, specificity, or any
    number of other inherent aspects of the data. In order to uncover these and get
    to know the data better, the first step of post-wrangling data analysis is to
    calculate some descriptive statistics.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以及无数其他原因，我们需要将我们的数据集视为一个不熟悉的生物。就像一个新发现的动物物种一样，我们以为的可能是我们并没有的。我们的初始假设可能并不正确，即使它们是正确的，在（比喻的）物种内部，个体之间可能存在巨大的差异。同样，即使你自信你的数据集包含你所认为的内容，数据本身肯定是从一个数据点到另一个数据点的变化。没有初步评估，你可能会遇到异常值、偏差、精度、特异性或其他数据固有的众多方面的问题。为了揭示这些并更好地了解数据，数据整理后的数据分析的第一步是计算一些描述性统计。
- en: 5.2\. Descriptive statistics
  id: totrans-678
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 描述性统计
- en: 'Descriptive statistics are what you might think:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性统计可能正如你所想：
- en: Descriptions of a data set
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的描述
- en: Summaries of a data set
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的摘要
- en: Maximum values
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大值
- en: Minimum values
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值
- en: Average values
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均值
- en: A list of possible values
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能值的列表
- en: A range of time covered by the data set
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集覆盖的时间范围
- en: And much more
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多
- en: 'Those are examples; for a definition, let’s look at one from Wikipedia:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是例子；对于定义，让我们看看维基百科上的一个例子：
- en: '*Descriptive statistics is the discipline of quantitatively describing the
    main features of a collection of information, or the quantitative description
    itself.*'
  id: totrans-689
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*描述性统计是定量描述信息集合的主要特征或定量描述本身的学科。*'
- en: Descriptive statistics is both a set of techniques and the description of the
    data sets that are produced using those techniques.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性统计既是一套技术，也是使用这些技术生成的数据集的描述。
- en: It’s often hard to discuss descriptive statistics without mentioning inferential
    statistics. *Inferential statistics* is the practice of using the data you have
    to deduce—or infer—knowledge or quantities of which you don’t have direct measurements
    or data. For example, surveying 1000 voters in a political election and then attempting
    to predict the results of the general population (presumably far larger than 1000
    individuals) uses inferential statistics. Descriptive statistics concerns itself
    with only the data you have, namely the 1000 survey responses. In this example,
    the generalization step from sample to population separates the two concepts.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论描述性统计时，往往难以不提及推断性统计。*推断性统计*是使用你拥有的数据来推断——或者推断——你没有直接测量或数据的知识或数量。例如，在政治选举中对1000名选民进行调查，然后试图预测整个选民群体的结果（可能远大于1000人）就使用了推断性统计。描述性统计只关注你拥有的数据，即1000份调查回应。在这个例子中，从样本到总体的推广步骤将这两个概念分开。
- en: 'With respect to a data set, you can say the following:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集，你可以这样说：
- en: Descriptive statistics asks, “What do I have?”
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性统计问的是，“我有什么？”
- en: Inferential statistics asks, “What can I conclude?”
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断性统计问的是，“我能得出什么结论？”
- en: Although descriptive and inferential statistics can be spoken of as two different
    techniques, the border between them is often blurry. In the case of election surveys,
    as in many others, you would have to perform descriptive statistics on the 1000
    data points in order to infer anything about the rest of the voting populace that
    wasn’t surveyed, and it isn’t always clear where the description stops and where
    the inference starts.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然描述性统计和推断性统计可以被视为两种不同的技术，但它们之间的界限通常是模糊的。在选举调查的情况下，就像在许多其他情况下一样，你必须对1000个数据点进行描述性统计，以便推断出关于未调查的其余投票人群的任何信息，而且并不总是清楚描述在哪里结束，推断在哪里开始。
- en: 'I think most statisticians and businesspeople alike would agree that it takes
    inferential statistics to draw most of the cool conclusions: when the world’s
    population will peak and then start to decline, how fast a viral epidemic will
    spread, when the stock market will go up, whether people on Twitter have generally
    positive or negative sentiment about a topic, and so on. But descriptive statistics
    plays an incredibly important role in making these conclusions possible. It pays
    to know the data you have and what it can do for you.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为大多数统计学家和商人都同意，要得出大多数有趣的结论需要推断统计：世界人口何时达到顶峰然后开始下降，病毒流行病将如何快速传播，股市何时会上涨，Twitter上的人们对某个话题是否普遍持有积极或消极的观点，等等。但描述性统计在使这些结论成为可能方面发挥着极其重要的作用。了解你拥有的数据和它能为你做什么是有益的。
- en: 5.2.1\. Stay close to the data
  id: totrans-697
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 保持接近数据
- en: I mentioned staying *close to the data* in [chapter 1](kindle_split_010_split_000.xhtml#ch01)
    as well, but it’s certainly worth repeating and is perhaps more important to mention
    here. The purpose of calculating descriptive statistics at this stage in a data
    science project is to learn about your data set so that you understand its capabilities
    and limitations; trying to do anything but learn about your data at this point
    would be a mistake. Complex statistical techniques such as those in machine learning,
    predictive analytics, and probabilistic modeling, for example, are completely
    out of the question for the moment.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[第一章](kindle_split_010_split_000.xhtml#ch01)中也提到了保持*接近数据*，但这当然值得重复，也许在这里提到更为重要。在数据科学项目的这个阶段计算描述性统计的目的，是为了了解你的数据集，以便你了解其能力和局限性；在这个阶段尝试做任何除了了解数据之外的事情都是错误的。像机器学习、预测分析和概率建模等复杂的统计技术，目前完全不适合。
- en: Some people would argue with me, saying that it’s OK to dive right in and apply
    some machine learning (for example) to your data, because you’ll get to know the
    data as you go along, and if you’re astute, you’ll recognize any problems as they
    come and then remedy them. I wholeheartedly disagree. Complex methods like most
    of those used in machine learning today are not easily dissected or even understood.
    Random forests, neural networks, and support vector machines, among others, may
    be understood in theory, but each of these has so many moving parts that one person
    (or a team) can’t possibly comprehend all of the specific pieces and values involved
    in obtaining a single result. Therefore, when you notice an incorrect result,
    even one that’s grossly incorrect, it’s not straightforward to extract from a
    complex model exactly which pieces contributed to this egregious error. More importantly,
    complex models that involve some randomness (again, most machine learning techniques)
    may not reproduce a specific error if you rerun the algorithm. Such unpredictability
    in sophisticated statistical methods also suggests that you should get to know
    your own data before you allow any random processes or black boxes to draw conclusions
    for you.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会和我争论，说直接跳入并应用一些机器学习（例如）到你的数据中是可以的，因为你在进行的过程中会逐渐了解数据，如果你足够敏锐，你会在问题出现时识别并纠正它们。我完全不同意。像今天大多数机器学习中使用的复杂方法一样，这些方法并不容易分解，甚至难以理解。随机森林、神经网络和支持向量机等，虽然在理论上可以理解，但每个这些方法都有许多动态部分，一个人（或一个团队）不可能完全理解获得单个结果所涉及的特定部分和值。因此，当你注意到一个错误的结果，即使是一个极其错误的结果时，从复杂的模型中直接提取出导致这种严重错误的特定部分并不简单。更重要的是，涉及一些随机性（再次强调，大多数机器学习技术）的复杂模型可能不会在重新运行算法时重现特定的错误。这种在复杂统计方法中的不可预测性也表明，在允许任何随机过程或黑盒为你得出结论之前，你应该先了解自己的数据。
- en: 'The definition I use for *close to the data* is this:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 我对“接近数据”的定义是这样的：
- en: '*You are close to the data if you are computing statistics that you are able
    to verify manually or that you can replicate exactly using another statistical
    tool.*'
  id: totrans-701
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你正在计算可以手动验证或可以使用另一个统计工具完全复制的统计量，那么你就接近数据了.*'
- en: In this phase of the project, you should calculate descriptive statistics that
    you can verify easily by some other means, and in some cases you should do that
    verification to be sure. Therefore, because you’re doing simple calculations and
    double-checking them, you can be nearly 100% certain that the results are correct,
    and the set of close-to-the-data descriptive statistics that you accumulate becomes
    a sort of inviolable canon of knowledge about your data set that will be of great
    use later. If you ever run across results that contradict these or seem unlikely
    in relation to these, then you can be nearly certain that you’ve made a significant
    error at some point in producing those results. In addition, which results within
    the canon are contradicted can be hugely informative in diagnosing the error.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目阶段，你应该计算可以通过其他方式轻松验证的描述性统计，在某些情况下，你应该进行验证以确保正确性。因此，因为你正在进行简单的计算并双重检查，你可以几乎100%地确信结果是正确的，而你积累的接近数据的描述性统计数据集成为了一种关于你的数据集的知识不可侵犯的规范，这在以后将非常有用。如果你遇到的结果与这些结果相矛盾或似乎与这些结果不太可能相关，那么你可以几乎肯定你在产生这些结果的过程中某个时候犯了一个重大错误。此外，哪些规范内的结果被矛盾可以极大地有助于诊断错误。
- en: Staying close to the data ensures that you can be incredibly certain about these
    preliminary results, and keeping a set of good descriptive statistics with you
    throughout your project provides you an easy reference to compare with subsequent
    more relevant but more abstruse results that are the real focus of your project.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 保持接近数据确保你可以非常确信这些初步结果，并在整个项目中保持一组良好的描述性统计数据，这为你提供了一个容易参考的参考，可以与随后的更相关但更抽象的结果进行比较，这些结果是你的项目真正的焦点。
- en: 5.2.2\. Common descriptive statistics
  id: totrans-704
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2. 常见的描述性统计
- en: Examples of helpful and informative descriptive statistics methods include but
    are not limited to mean, variance, median, sum, histogram, scatter plot, tabular
    summaries, quantiles, maximum, minimum, and cumulative distributions. Any or all
    of these might be helpful in your next project, and it’s largely a matter of both
    preference and relevance when deciding which ones you might calculate in order
    to serve your goals.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的和有信息量的描述性统计方法包括但不限于平均值、方差、中位数、总和、直方图、散点图、表格摘要、分位数、最大值、最小值和累积分布。这些中的任何一个或所有都可能在你下一个项目中有所帮助，而决定你可能会计算哪些以实现你的目标，在很大程度上是偏好和相关性的问题。
- en: 'In the Enron email data set, here’s the first line of statistical questioning
    that occurs to me:'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 在安然电子邮件数据集中，以下是我想到的第一个统计问题：
- en: How many people are there?
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少人？
- en: How many messages are there?
  id: totrans-708
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少条消息？
- en: How many messages did individual people write?
  id: totrans-709
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 个人写了多少条消息？
- en: A short paper called “Introducing the Enron Corpus” (2004) by Brian Klimt and
    Yiming Yang gives a good summary that answers these questions.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇简短的论文，名为“介绍安然语料库”（2004），由Brian Klimt和Yiming Yang撰写，对这些问题给出了很好的总结。
- en: In the Enron email corpus, there are 619,446 messages from the accounts of 158
    employees. But by removing mass emails and duplicate emails appearing in multiple
    accounts, Klimt and Yang reduced the data to 200,399 emails in a clean version
    of the data set. In the clean version, the average user sent 757 emails. These
    are useful facts to know. Without them, it may not be obvious that there is a
    problem if, later, a statistical model suggests that most people send dozens of
    emails per day. Because this data set spans two years, roughly speaking (another
    descriptive statistic!), we know that two to three emails per day is much more
    typical.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 在安然电子邮件语料库中，有来自158名员工账户的619,446条消息。但通过移除大量电子邮件和在多个账户中出现的重复电子邮件，Klimt和Yang将数据集减少到200,399条干净的电子邮件。在干净的版本中，平均用户发送了757封电子邮件。这些是有用的信息。如果没有这些信息，后来如果某个统计模型表明大多数人每天发送几十封电子邮件，可能不会很明显地发现问题。因为这个数据集跨越了两年，粗略地说（另一个描述性统计！），我们知道每天两到三封电子邮件要典型得多。
- en: Speaking of ranges of time, in the Enron data set and others, I’ve seen dates
    reported incorrectly. Because of the way dates are formatted, a corrupted file
    can easily cause dates to be reported as 1900 or 1970 or another year, when that’s
    obviously erroneous. Enron didn’t exist until much later, and for that matter
    neither did email as we know it. If you want to use time as an important variable
    in your later analyses, having a few emails a century before the rest may be a
    big problem. It would have been helpful to recognize these issues while wrangling
    the data, as described in [chapter 4](kindle_split_013_split_000.xhtml#ch04),
    but it may have slipped past unnoticed, and some descriptive statistics can help
    you catch it now.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 说到时间范围，在Enron数据集和其他数据集中，我见过日期报告错误的情况。由于日期的格式方式，一个损坏的文件可以轻易地将日期报告为1900年、1970年或其他年份，这显然是错误的。Enron直到很久以后才存在，同样，我们所知道的电子邮件也是。如果你想在后续分析中将时间作为一个重要变量，那么在其余电子邮件一个世纪之前的几封电子邮件可能是一个大问题。在整理数据时，如[第4章](kindle_split_013_split_000.xhtml#ch04)所述，识别这些问题会有所帮助，但这些问题可能被忽视，一些描述性统计可以帮助你现在捕捉到它。
- en: For example, let’s say you’re interested in analyzing how many emails are sent
    from year to year, but you skipped descriptive statistics, and you jumped straight
    into writing a statistical application that begins in the year of the earliest
    email (circa 1900) and ends at the latest (circa 2003). Your results would be
    heavily biased by the many years in the middle of that range that contained no
    messages. You might catch this error early and not lose much time in the process,
    but for larger, more complex analyses, you might not be so lucky. Comparing the
    real date range with your presumed one beforehand could have uncovered the erroneous
    dates more quickly. In today’s big data world, it wouldn’t be uncommon for someone
    to write an application that does this—analyzes quantities of emails over time—for
    billions of emails, which would probably require using a computing cluster and
    would cost hundreds or thousands of dollars per run. Not doing your homework—descriptive
    statistics—in that case could be costly.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你对分析每年发送的电子邮件数量感兴趣，但你跳过了描述性统计，直接编写了一个从最早电子邮件年份（约1900年）开始，到最新年份（约2003年）结束的统计应用程序。你的结果会受到该范围内中间多年没有消息的严重影响。你可能会在早期发现这个错误，并且在这个过程中不会浪费太多时间，但对于更大、更复杂的分析，你可能不会这么幸运。在事先比较实际日期范围和假设的日期范围之前，可能会更快地发现错误的日期。在今天的海量数据世界中，有人编写一个分析数十亿封电子邮件随时间发送量的应用程序并不罕见——这可能需要使用计算集群，并且每次运行可能需要花费数百或数千美元。在这种情况下，不做作业——描述性统计——可能会代价高昂。
- en: 5.2.3\. Choosing specific statistics to calculate
  id: totrans-714
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3. 选择要计算的特定统计数据
- en: In the paper describing the Enron corpus, Klimt and Yang make it clear that
    they’re primarily focused on classifying the emails into topics or other groups.
    In their case, dates and times are less important than subjects, term frequencies,
    and email threads. Their choice of descriptive statistics reflects that.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述Enron语料库的论文中，Klimt和Yang明确指出，他们主要关注将电子邮件分类到主题或其他组别。在他们的情况下，日期和时间不如主题、词频和电子邮件线程重要。他们选择的描述性统计反映了这一点。
- en: 'We were concerned mainly with users’ behavior over time, and so we calculated
    descriptive statistics such as these:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要关注用户随时间的行为，因此我们计算了如下描述性统计：
- en: Total number of emails sent per month
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月发送的电子邮件总数
- en: Most prolific email senders and the number they sent
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最活跃的电子邮件发送者及其发送的电子邮件数量
- en: Number of emails sent each month by the most prolific senders
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最活跃发送者每月发送的电子邮件数量
- en: Most prolific email recipients and the number they received
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最活跃的电子邮件接收者及其接收的电子邮件数量
- en: Most prolific sender-recipient pairs and the number of emails they exchanged
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最活跃的发件人和收件人配对以及他们交换的电子邮件数量
- en: 'It’s not always obvious which statistics would be the best choice for your
    particular project, but you can ask yourself a few questions that will help lead
    you to useful choices:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总是明显知道哪些统计数据最适合你的特定项目，但你可以问自己几个问题，这些问题将帮助你做出有用的选择：
- en: How much data is there, and how much of it is relevant?
  id: totrans-723
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少数据，其中有多少是相关的？
- en: What are the one or two most relevant aspects of the data with respect to the
    project?
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与项目相关的数据中，最相关的一两个方面是什么？
- en: Considering the most relevant aspects, what do typical data points look like?
  id: totrans-725
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑最相关的方面，典型的数据点是什么样的？
- en: Considering the most relevant aspects, what do the most extreme data points
    look like?
  id: totrans-726
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到最相关的方面，最极端的数据点看起来是什么样的？
- en: Question 1 is usually fairly straightforward to answer. For the Enron data set,
    you find the total number of emails, or the total number of email accounts, both
    of which I’ve mentioned already. Or if the project concerns only a subset of the
    data—for example, emails involving Ken Lay, the CEO who was later convicted of
    multiple counts of fraud, or maybe only emails sent in 2001—then you should find
    the totals for that subset as well. Is there enough relevant data for you to accomplish
    the goals of the project? Always be wary that prior data wrangling may not have
    been perfect, and so obtaining the precise subset may not be as easy as it seems.
    Errors in name or date formatting, among other things, could cause problems.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 问题1通常很容易回答。对于安然数据集，你将找到电子邮件的总数或电子邮件账户的总数，这两点我之前已经提到。或者，如果项目只关注数据的一个子集——例如，涉及肯·莱（Ken
    Lay）的电子邮件，他是后来因多项欺诈指控而被定罪的CEO，或者可能只是2001年发送的电子邮件——那么你也应该找到该子集的总数。你是否有足够的相关数据来完成项目目标？始终要警惕，之前的数据整理可能并不完美，因此获取精确的子集可能不像看起来那么容易。姓名或日期格式错误等问题可能会引起问题。
- en: Question 2 concerns the focus of the project. If you’re studying the rise and
    fall of Enron as an organization, then time is a relevant aspect of the data.
    If you’re looking mainly at email classification, as Klimt and Yang were, then
    email folders are important, as are the subject and body text from the emails.
    Word counts or other language features may be informative at this point. Think
    about your project, look at a few individual data points, and ask yourself, “Which
    part do I care about the most?”
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 问题2关注项目的重点。如果你在研究安然作为一个组织的兴衰，那么时间就是数据的一个相关方面。如果你主要关注电子邮件分类，就像Klimt和Yang所做的那样，那么电子邮件文件夹、电子邮件的主题和正文文本都很重要。在这个阶段，单词计数或其他语言特征可能是有信息的。想想你的项目，看看几个个别数据点，然后问自己，“我最关心的是哪个部分？”
- en: For question 3, take the answer to question 2 and calculate some summary statistics
    on the values corresponding to those aspects. If a time variable is important
    to you, then calculate a mean, median, and maybe some quantiles of all of the
    email timestamps in the data set (don’t forget to convert timestamps to a numerical
    value—for example, Unix time—and then back again for a sanity check). You might
    also calculate the number of emails sent each week, month, or year. If email classification
    is important to you, then add up the emails that appear in each of the folders
    and find the folders that contain the most emails across all accounts. Or look
    at how different people have different numbers and percentages of their emails
    in different folders. Do any of these results surprise you? Given your project
    goals, can you foresee any problems occurring in your analysis?
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题3，将问题2的答案用于计算与那些方面相对应的值的某些汇总统计。如果你认为时间变量很重要，那么计算数据集中所有电子邮件时间戳的平均值、中位数以及一些分位数（别忘了将时间戳转换为数值——例如，Unix时间——然后再转换回来进行合理性检查）。你也可以计算每周、每月或每年发送的电子邮件数量。如果你认为电子邮件分类很重要，那么将每个文件夹中出现的电子邮件数量加起来，并找出包含最多电子邮件的文件夹。或者看看不同的人在不同文件夹中电子邮件的数量和百分比有何不同。这些结果中有任何让你感到惊讶的吗？根据你的项目目标，你能预见分析过程中可能出现的任何问题吗？
- en: Question 4 is similar to question 3, but instead of looking at typical values,
    looks at extreme values such as maximum and minimum. The earliest and latest timestamps,
    as well as some extreme quantiles such as 0.01 and 0.99, can be useful. For email
    classifications, you should look at the folders containing the most emails as
    well as folders that may contain few—it’s likely many folders contain one or a
    few emails and are possibly mostly useless for analysis. Perhaps for later stages
    of the project you would consider excluding these. When looking at extreme values,
    are there any values so high or so low that they don’t make sense? How many values
    are outside a reasonable range? For categorical or other non-numeric data, what
    are the most common and least common categories? Are all of these meaningful and
    useful to subsequent analysis?
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 问题4与问题3类似，但不是查看典型值，而是查看极端值，如最大值和最小值。最早和最晚的时间戳，以及一些极端分位数，如0.01和0.99，可能很有用。对于电子邮件分类，你应该查看包含最多电子邮件的文件夹以及可能包含少量电子邮件的文件夹——很可能许多文件夹只包含一个或几个电子邮件，并且可能对分析大部分无用。也许在项目的后期阶段，你会考虑排除这些文件夹。在查看极端值时，是否存在某些过高或过低的值没有意义？有多少值超出了合理范围？对于分类或其他非数值数据，最常见的和最不常见的类别是什么？所有这些对后续分析都有意义且有用吗？
- en: 5.2.4\. Make tables or graphs where appropriate
  id: totrans-731
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4. 在适当的情况下制作表格或图表
- en: Beyond calculating the raw values of these statistics, you might find value
    in formatting some of the data as tables, such as the quantities of emails for
    the various categories of most prolific, or as graphs, such as the timelines of
    monthly quantities of email sent and received.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算这些统计量的原始值之外，你可能还会发现将一些数据格式化为表格很有价值，例如，对于最多产的各类别的电子邮件数量，或者作为图表，例如发送和接收的电子邮件每月数量的时间线。
- en: Tables and graphs can convey information more thoroughly and more quickly at
    times than pure text. Producing tables and graphs and keeping them for reference
    throughout your project is a good idea.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 表格和图表有时比纯文本更全面、更快地传达信息。在整个项目中制作表格和图表并保留它们以供参考是个好主意。
- en: '[Figure 5.2](kindle_split_014_split_002.xhtml#ch05fig02) shows two plots excerpted
    from Klimt and Yang’s paper. They’re graphical representations of descriptive
    statistics. The first shows a cumulative distribution of users versus the number
    of messages they sent within the data set. The second plots the number of messages
    in Enron employees’ inboxes versus the number of folders that were present in
    those email accounts. If you’re interested in the number of emails sent by the
    various employees or in how the employees used folders, it might be good to keep
    these graphs handy and compare them with all subsequent results. It’s possible
    that they’ll either help verify that your results are reasonable or indicate that
    your results are not reasonable and then help you diagnose the problem.'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.2](kindle_split_014_split_002.xhtml#ch05fig02) 展示了从Klimt和Yang的论文中摘录的两个图表。它们是描述性统计的图形表示。第一个显示了用户在数据集中发送的消息数量与累积分布的关系。第二个则显示了Enron员工收件箱中的消息数量与这些电子邮件账户中存在的文件夹数量之间的关系。如果你对各种员工发送的电子邮件数量或员工如何使用文件夹感兴趣，保留这些图表并与所有后续结果进行比较可能是个好主意。它们可能会帮助你验证你的结果是合理的，或者表明你的结果不合理，并帮助你诊断问题。'
- en: 'Figure 5.2\. Two graphs redrawn from Klimt and Yang’s “The Enron Corpus: A
    New Dataset for Email Classification Research” (published by Springer in *Machine
    Learning: ECML 2004*).'
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '图5.2. 从Klimt和Yang的《The Enron Corpus：用于电子邮件分类研究的新数据集》（由Springer在*Machine Learning:
    ECML 2004*出版）中重新绘制的两个图表。'
- en: '![](Images/05fig02_alt.jpg)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig02_alt.jpg)'
- en: The types of descriptive graphs or tables that are appropriate for your project
    might be different from these, but they similarly should address the aspects of
    the data that are relevant to your goals and the questions you hope to answer.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 适合你项目的描述性图表或表格的类型可能与这些不同，但它们同样应该解决与你的目标和希望回答的问题相关的数据方面。
- en: 5.3\. Check assumptions about the data
  id: totrans-738
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 检查关于数据的基本假设
- en: Whether we like to admit it or not, we all make assumptions about data sets.
    As implied in the previous section, we might assume that our data is contained
    within a particular time period. Or we might assume that the names of the folders
    that contain emails are appropriate descriptors of the topics or classifications
    of those emails. These assumptions about the data can be expectations or hopes,
    conscious or subconscious.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是否愿意承认，我们都会对数据集做出假设。正如前文所暗示的，我们可能会假设我们的数据包含在特定的某个时间段内。或者我们可能会假设包含电子邮件的文件夹名称是这些电子邮件主题或分类的适当描述符。这些关于数据的假设可能是期望或希望，有意识的或无意识的。
- en: 5.3.1\. Assumptions about the contents of the data
  id: totrans-740
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 关于数据内容的假设
- en: Let’s consider the element of time in the Enron data. I certainly assumed, when
    I began looking at the data, that the emails would span the few years between
    the advent of email in the late 1990s and the demise of the firm in the early
    2000s. I would have been mistaken, because of the potential errors or corruption
    in the date formatting that I’ve already mentioned. In practice, I saw dates far
    outside the range that I assumed as well as some other dates that were questionable.
    My assumption about the date range certainly needed to be checked.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑安然数据中的时间元素。当我开始查看数据时，我确实假设电子邮件会跨越20世纪90年代末电子邮件出现和21世纪初公司倒闭之间的几年。我会犯错误，因为已经提到的日期格式潜在的错误或损坏。实际上，我看到了远超出我假设的范围的日期，以及一些可疑的日期。我对日期范围的假设当然需要检查。
- en: If you want to use the folder names in the email accounts to inform you about
    the contents of emails within, there’s an implied assumption that these folder
    names are indeed informative. You definitely would want to check this, which would
    likely involve a fair amount of manual work, such as reading a bunch of emails
    and using your best judgment about whether the folder name describes what’s in
    the email.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用电子邮件账户中的文件夹名称来告知你电子邮件的内容，那么就隐含了一个假设，即这些文件夹名称确实是有信息的。你肯定希望检查这一点，这可能会涉及到相当多的手动工作，比如阅读大量的电子邮件，并使用你最好的判断来决定文件夹名称是否描述了电子邮件的内容。
- en: One specific thing to watch out for is missing data or placeholder values. Most
    people tend to assume—or at least hope—that all fields in the data contain a usable
    value. But often emails have no subject, or there is no name in the From field,
    or in CSV data there might be `NA`, `NaN`, or a blank space where a number should
    be. It’s always a good idea to check whether such placeholder values occur often
    enough to cause problems.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 需要特别注意的一个具体问题是缺失数据或占位符值。大多数人往往会假设——至少是希望——数据中的所有字段都包含可用的值。但通常电子邮件没有主题，或者发件人字段中没有姓名，或者在CSV数据中可能有`NA`、`NaN`或空白空间，本应填写数字的地方。始终检查这类占位符值是否频繁出现到足以引起问题的程度是个好主意。
- en: 5.3.2\. Assumptions about the distribution of the data
  id: totrans-744
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 关于数据分布的假设
- en: Beyond the contents and range of the data, you may have further assumptions
    about its distribution. In all honesty, I know many statisticians who will get
    excited about the heading of this section but then disappointed with its contents.
    Statisticians *love* to check the appropriateness of distribution assumptions.
    Try Googling “normality test” or go straight to the Wikipedia page and you’ll
    see what I mean. It seems there are about a million ways to test whether your
    data is normally distributed, and that’s one statistical distribution.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据和范围的内容之外，你可能会对其分布有进一步的假设。坦白说，我知道许多统计学家会对本节标题感到兴奋，但对其内容感到失望。统计学家*喜欢*检查分布假设的适当性。尝试在谷歌上搜索“正态性检验”或直接访问维基百科页面，你就会明白我的意思。似乎有数百万种方法来测试你的数据是否呈正态分布，而这只是统计分布中的一种。
- en: I’ll probably be banned from all future statistics conferences for writing this,
    but I’m not usually that rigorous. Generally, plotting the data using a histogram
    or scatter plot can tell you whether the assumption you want to make is at all
    reasonable. For example, [figure 5.3](kindle_split_014_split_003.xhtml#ch05fig03)
    is a graphic from one of my research papers in which I analyzed performances in
    track and field. Pictured is a histogram of the best men’s 400 m performances
    of all time (after taking their logarithms), and overlaid on it is the curve of
    a normal distribution. That the top performances fit the tail of a normal distribution
    was one of the key assumptions of my research, so I needed to justify that assumption.
    I didn’t use any of the statistical tests for normality, partially because I was
    dealing with a tail of the distribution—only the best performances, not all performances
    in history—but also because I intended to use the normal distribution unless it
    was obviously inappropriate for the data. To me, visually comparing the histogram
    with a plot of the normal distribution sufficed as verification of the assumption.
    The histogram was similar enough to the bell curve for my purposes.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能因为写这篇文章而被所有未来的统计学会议禁止参加，但我的通常标准并不那么严格。通常，使用直方图或散点图绘制数据可以告诉你你想要做出的假设是否合理。例如，[图5.3](kindle_split_014_split_003.xhtml#ch05fig03)
    是我从我的研究论文中摘取的一张图表，其中我分析了田径运动的表现。图中展示的是所有时间以来最佳男子400米成绩的直方图（在取对数后），并且在其上叠加的是正态分布曲线。顶级表现符合正态分布的尾部是我研究的关键假设之一，因此我需要证明这个假设。我没有使用任何用于检验正态性的统计测试，部分原因是我处理的是分布的尾部——只有最佳表现，而不是历史上的所有表现——但同时也因为我打算使用正态分布，除非显然不适用于数据。对我来说，将直方图与正态分布图进行视觉比较就足以作为假设的验证。对于我的目的来说，直方图与钟形曲线足够相似。
- en: Figure 5.3\. The logarithms of the best men’s 400 m performances of all time
    seem to fit the tail of a normal distribution.
  id: totrans-747
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3\. 所有时间以来最佳男子400米成绩的对数似乎符合正态分布的尾部。
- en: '![](Images/05fig03_alt.jpg)'
  id: totrans-748
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig03_alt.jpg)'
- en: Although I may have been less than statistically rigorous with the distribution
    of the track and field data, I don’t want to be dismissive of the value of checking
    the distributions of data. Bad things can happen if you assume you have normally
    distributed data and you don’t. Statistical models that assume normal distributions
    don’t handle outliers well, and the vast majority of popular statistical models
    make some sort of assumption of normality. This includes the most common kinds
    of linear regression, as well as the t-test. Assuming normality when your data
    isn’t even close can make your results appear significant when in fact they’re
    insignificant or plain wrong.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在处理田径数据分布时可能不够统计严谨，但我并不想轻视检查数据分布的价值。如果你假设数据是正态分布的，而实际上不是，那么会发生不好的事情。假设正态分布的统计模型处理异常值不佳，而绝大多数流行的统计模型都做出了某种正态性的假设。这包括最常见的线性回归类型，以及t检验。当你的数据根本不符合正态分布时，假设正态性可能会让你的结果看起来具有显著性，而实际上它们可能是无关紧要的或完全错误的。
- en: This last statement is valid for any statistical distribution, not only the
    normal. You may have categorical data that you think is uniformly distributed,
    when in fact some categories appear far more often than others. Social networking
    statistics, such as the kind I’ve calculated from the Enron data set—number of
    emails sent, number of people contacted in a day, and so on—are notoriously non-normal.
    They’re typically something like exponentially or geometrically distributed, both
    of which you should also check against the data before assuming them.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一点适用于任何统计分布，而不仅仅是正态分布。你可能有一些你认为均匀分布的类别数据，而实际上某些类别出现的频率远高于其他类别。例如，从安然数据集中计算出的社交网络统计数据——一天发送的电子邮件数量、一天联系的人数等等——都是非正态的。它们通常是指数分布或几何分布，这两种分布你都应该在假设它们之前对数据进行检查。
- en: All in all, although it might be OK to skip a statistical test for checking
    that your data fits a particular distribution, do be careful and make sure that
    your data matches any assumed distribution at least roughly. Skipping this step
    can be catastrophic for results.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，虽然可能可以跳过检查数据是否符合特定分布的统计测试，但一定要小心，并确保你的数据至少大致符合任何假设的分布。跳过这一步可能会对结果造成灾难性影响。
- en: 5.3.3\. A handy trick for uncovering your assumptions
  id: totrans-752
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 一个揭示你假设的实用技巧
- en: 'If you feel like you don’t have assumptions, or you’re not sure what your assumptions
    are, or even if you think you know all of your assumptions, try this: describe
    your data and project to a friend—what’s in the data set and what you’re going
    to do with it—and write down your description. Then, dissect your description,
    looking for assumptions.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得自己没有假设，或者你不确定你的假设是什么，甚至如果你认为你知道所有的假设，尝试这样做：向朋友描述你的数据和项目——数据集中有什么，你打算用它做什么——并写下你的描述。然后，剖析你的描述，寻找假设。
- en: 'For example, I might describe my original project involving the Enron data
    like this: “My data set is a bunch of emails, and I’m going to establish organization-wide
    patterns of behavior over the network of people using techniques from social network
    analysis. I’d like to draw conclusions about things like employee responsiveness
    as well as communication up the hierarchy, with a boss.”'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我可能这样描述我的原始项目，涉及安然数据：“我的数据集是一堆电子邮件，我打算使用社会网络分析技术来建立整个网络的人员行为模式。我想得出关于员工响应性以及向上层级的沟通，包括老板在内的结论。”
- en: 'In dissecting this description, you should first identify phrases and then
    think about what assumptions might underlie them, as in the following:'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 在剖析这个描述时，你应该首先识别短语，然后思考可能隐藏在它们背后的假设，如下所示：
- en: '***My data set is a bunch of emails—*** That’s probably true, but it might
    be worth checking to see whether there might be other non-email data types in
    there, such as chat messages or call logs.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***我的数据集是一堆电子邮件—*** 这可能是真的，但检查一下是否可能存在其他非电子邮件数据类型，比如聊天消息或通话记录。'
- en: '***Organization-wide—*** What is the organization? Are you assuming it’s clearly
    defined, or are there fuzzy boundaries? It might help to run some descriptive
    statistics regarding the boundaries of the organization, possibly people with
    a certain email address domain or people who wrote more than a certain number
    of messages.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***组织范围—*** 组织是什么？你认为它是明确界定的，还是存在模糊的边界？运行一些关于组织边界的描述性统计可能有所帮助，比如拥有特定电子邮件地址域的人或写了超过一定数量信息的人。'
- en: '***Patterns of behavior—*** What assumptions do you have about what constitutes
    a pattern of behavior? Does everyone need to engage in the same behavior in order
    for it to be declared a pattern, or do you have a set of patterns and you’re looking
    for individual examples that match those patterns?'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***行为模式—*** 你对构成行为模式有哪些假设？是否每个人都需要进行相同的行为才能被宣布为模式，还是你有一套模式，你正在寻找符合这些模式的个别例子？'
- en: '***Network of people—*** Does everyone in the network need to be connected?
    Can there be unconnected people? Are you planning to assume a certain statistical
    model from social network analysis literature? Does it require certain assumptions?'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***人员网络—*** 网络中的每个人都需要连接吗？是否可以有不连接的人？你计划从社会网络分析文献中假设某种统计模型吗？这需要某些假设吗？'
- en: '***Responsiveness—*** What do you assume this term means? Can you define it
    statistically and verify that the data supports such a definition by using the
    basic definition along with some descriptive statistics?'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***响应性—*** 你认为这个术语意味着什么？你能用统计数据来定义它，并通过使用基本定义和一些描述性统计来验证数据支持这样的定义吗？'
- en: '***Hierarchy—*** Are you assuming you have complete knowledge of the organization’s
    hierarchy? Do you assume that it’s rigid, or does it change?'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***层级结构—*** 你是否假设你对组织的层级结构有完整的了解？你认为它是僵化的，还是它会变化？'
- en: Realizing when you’re making assumptions—by dissecting your project description
    and then asking such questions—can help you avoid many problems later. You wouldn’t
    want to find out that a critical assumption was false only after you had completed
    your analysis, found odd results, and then gone back to investigate. Even more,
    you wouldn’t want a critical assumption to be false and never notice it.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 通过剖析你的项目描述并提出这样的问题，意识到你在做出假设，可以帮助你避免许多后续问题。你不想在完成分析、发现奇怪的结果后，再回去调查时发现一个关键假设是错误的。更重要的是，你不想一个关键假设是错误的，却从未注意到。
- en: 5.4\. Looking for something specific
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 寻找特定内容
- en: Data science projects have all sorts of goals. One common goal is to be able
    to find entities within your data set that match a certain conceptual description.
    Here, I’m using the term *entity* to represent any unique individual represented
    in your data set. An entity could be a specific person, place, date, IP address,
    genetic sequence, or other distinct item.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目有多种目标。一个常见的目标是能够在你的数据集中找到与某种概念描述相匹配的实体。在这里，我使用术语*实体*来表示你数据集中表示的任何独特个体。实体可能是一个特定的人、地点、日期、IP地址、遗传序列或其他独特项目。
- en: If you’re working in online retailing, you might consider customers as your
    main entities, and you might want to identify those who are likely to purchase
    a new video game system or a new book by a particular author. If you’re working
    in advertising, you might be looking for people who are most likely to respond
    to a particular advertisement. If you’re working in finance, you might be looking
    for equities on the stock market that are about to increase in price. If it were
    possible to perform a simple search for these characterizations, the job would
    be easy and you wouldn’t need data science or statistics. But although these characterizations
    aren’t inherent in the data (can you imagine a stock that tells you when it’s
    about to go up?), you often can recognize them when you see them, at least in
    retrospect. The main challenge in such data science projects is to create a method
    of finding these interesting entities in a timely manner.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从事在线零售，你可能将客户视为主要实体，并可能希望识别那些可能购买新视频游戏系统或特定作者的新书的人。如果你从事广告，你可能正在寻找最有可能对特定广告做出反应的人。如果你从事金融，你可能正在寻找即将价格上涨的股票市场股票。如果能够对这些描述进行简单的搜索，工作就会变得容易，你就不需要数据科学或统计学。但尽管这些描述不是数据固有的（你能想象一个告诉你即将上涨的股票吗？），当你看到它们时，你通常可以识别出来，至少在事后。这类数据科学项目的主要挑战是及时找到这些有趣的实体。
- en: 'In the Enron email data set, we were looking for suspicious behavior that might
    somehow be connected to the illegal activity that we now know was taking place
    at the company. Although suspicious behavior in an email data set can take many
    forms, we can name a few: in general, employees discussing illegal activity, trying
    to cover something up, talking to suspicious people, or otherwise communicating
    in an abnormal fashion.'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 在安然电子邮件数据集中，我们正在寻找可能与我们现在知道在公司发生的非法活动有关的可疑行为。尽管电子邮件数据集中的可疑行为可以采取多种形式，我们可以列举一些：一般来说，员工讨论非法活动，试图掩盖某些事情，与可疑人物交谈，或以其他异常方式沟通。
- en: We already had statistical models of communication across social/organizational
    networks that we wanted to apply to the Enron data set, but there were any number
    of ways in which we could configure the model and its parameters in order to find
    suspicious behavior in its various forms. There was no guarantee that we would
    find the kind we were looking for, and there was also no guarantee that we would
    find any at all. One reason we might not find any was that there might not have
    been any.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了跨社会/组织网络的通信统计模型，我们希望将其应用于安然数据集，但我们有无数种方式可以配置模型及其参数，以找到各种形式的可疑行为。我们无法保证我们会找到我们正在寻找的那种，也无法保证我们会找到任何可疑行为。我们可能找不到任何可疑行为的一个原因可能是实际上根本不存在可疑行为。
- en: 5.4.1\. Find a few examples
  id: totrans-768
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 寻找一些示例
- en: If you’re looking for something fairly specific, something interesting in your
    data, try to find something. Go through the data manually or use some simple searches
    or basic statistics to locate some examples of these interesting things. You should
    stay close to the data and be able to verify that these examples are indeed interesting.
    If you have a lot of data—if it’s hard even to browse through it—it’s OK to take
    a subset of the data and look for some good examples there.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找相当具体的东西，你数据中的某些有趣的东西，试着找到一些。手动浏览数据或使用一些简单的搜索或基本统计来定位这些有趣事物的示例。你应该靠近数据，并能够验证这些示例确实有趣。如果你有大量数据——即使浏览都很困难——可以取数据的一个子集，并在那里寻找一些好的示例。
- en: If you can’t find any interesting examples, you might be in trouble. Sometimes,
    interesting things are rare or don’t exist in the form you think they do. In fact,
    the Enron data, as published, doesn’t contain a trail of clues or a smoking gun
    of any kind. It often helps to dig deeper, to change the way you’re searching,
    to think differently about the data and what you’re looking for, or otherwise
    to exhaust all possible ways to find the good, interesting examples in the data.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你找不到任何有趣的示例，你可能遇到麻烦了。有时，有趣的事情很少见或者根本不存在于你认为它们存在的那种形式。事实上，根据公布的情况，安然数据不包含任何线索或任何类型的烟幕弹。深入挖掘、改变你的搜索方式、以不同的方式思考数据和你要找的东西，或者以其他方式穷尽所有可能找到数据中好例子、有趣例子的方法，通常是有帮助的。
- en: Sometimes brute force works. A team of a few people could theoretically read
    all of the Enron emails in a few days. It wouldn’t be fun—and I’m sure more than
    a few lawyers have done it—but it is possible and would be the most thorough way
    of finding what you’re looking for. I feel fairly confident in saying that over
    the course of the several months that I worked with the Enron data, I read most
    of the emails in that data set. This would have been a hallmark of a data science
    project gone wrong had the goal of the project not extended far beyond the Enron
    data set. We were developing software whose entire purpose was to make reading
    all emails unnecessary. We wanted to use the Enron data to characterize what suspicious
    communications look like so that we could use those characterizations to find
    such communications in other data sets. That’s why brute force made sense for
    us at the time. Depending on your data, looking through all your data manually
    might make sense for you, too.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候暴力搜索是有效的。理论上，一个由几个人组成的小组可以在几天内阅读完所有的安然邮件。这不会很有趣——我相信不止几个律师做过这件事——但它是有可能的，并且将是找到你想要的东西的最彻底的方法。我相当有信心地说，在我与安然数据一起工作的几个月里，我阅读了该数据集中大部分的邮件。如果项目的目标没有远远超出安然数据集，这将是一个数据科学项目出错的特征。我们正在开发软件，其整个目的就是使阅读所有邮件变得不必要。我们想使用安然数据来描述可疑通信的样子，以便我们可以使用这些描述在其他数据集中找到这样的通信。这就是为什么在当时对我们来说暴力搜索是有意义的。根据你的数据，手动查看所有数据可能对你来说也是有意义的。
- en: 'You might also use brute force on a subset of your data. Think of it this way:
    if 1 in 1000 entities—data points, people, days, messages, whatever—is supposed
    to be interesting, then you should find one if you manually look at more than
    0.1% of a data set consisting of a million entities.'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可能在你的数据子集上使用暴力搜索。可以这样想：如果1000个实体（数据点、人、天、消息，等等）中有1个是有趣的，那么如果你手动查看由一百万个实体组成的数据集的超过0.1%，你应该能找到它。
- en: You should probably look at more data than that to be sure you haven’t had bad
    luck, but if you’ve covered 1% of the data and you still haven’t found any, you
    know the interesting entities are rarer than you thought or nonexistent. You can
    adjust these percentages for rarer or more common entities.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要查看更多的数据以确保你没有运气不好，但如果你已经覆盖了1%的数据而你还没有找到任何，你就知道有趣的实体比你想的更稀少或者根本不存在。你可以调整这些百分比以适应更稀少或更常见的实体。
- en: If it turns out that you can’t find any interesting entities via any means,
    the only options are to look for another type of interesting thing, following
    another of your project’s goals, or to go back to all your data sources and find
    another data set that contains some trace of something you find interesting. It’s
    not fun to have to do that, but it’s a real possibility and not that rare in practice.
    Let’s be optimistic, though, and assume you were successful in finding some interesting
    entities within your data set.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最终你无法通过任何方式找到任何有趣的实体，唯一的选择是寻找另一种有趣的事物，遵循你项目中的另一个目标，或者回到所有数据源并找到另一个包含你感兴趣的事物痕迹的数据集。这样做并不有趣，但在实践中这是一个真实可能性，并不罕见。不过，让我们保持乐观，假设你在数据集中成功找到了一些有趣的实体。
- en: '5.4.2\. Characterize the examples: what makes them different?'
  id: totrans-775
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2. 描述示例：它们有什么不同？
- en: Once you’ve found at least a few examples of interesting things, take a close
    look and see how they’re represented within your data. The goal in this step is
    to figure out which features and attributes of the data could help you accomplish
    your goal of finding even more examples of these interesting things. Often you
    may be able to recognize by simple inspection some pattern or value that the interesting
    examples share, some aspect of their data points that could identify them and
    differentiate them from the rest of the data set.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到了一些有趣事物的例子，仔细观察它们在你的数据中的表示方式。在这个步骤中的目标是找出哪些数据特征和属性可以帮助你实现找到更多这类有趣事物的目标。通常，你可能会通过简单的检查识别出有趣例子共享的一些模式或值，这些数据点的某些方面可以用来识别它们并将它们与其他数据集区分开来。
- en: 'For email data, is it the email text that has interesting content and terms,
    or is it the time at which the email was sent? Or is it possibly that the sender
    and recipient(s) themselves are interesting? For other data types, have a look
    at the various fields and values that are present and make note of the ones that
    seem to be most important in differentiating the interesting things from everything
    else. After all, that’s the foundation of most statistics (in particular, machine
    learning) projects: differentiating two (or more) groups of things from one another.
    If you can get a rough idea of how you can do this manually, then it’s far easier
    to create a statistical model and implement it in code that will help you find
    many more of these examples, which I cover in a later chapter.'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电子邮件数据，是有趣内容的电子邮件文本，还是发送电子邮件的时间？或者，可能是发送者和接收者本身很有趣？对于其他数据类型，查看各种字段和值，并注意那些似乎在不同程度上区分有趣事物与其他事物的值。毕竟，这是大多数统计（特别是机器学习）项目的基础：区分两个（或更多）事物组。如果你能大致了解如何手动完成这项工作，那么创建统计模型并在代码中实现它来帮助你找到更多这样的例子就简单多了，这些内容我在后面的章节中会介绍。
- en: Often there’s nothing quantitative about a data point that’s remarkable or easily
    differentiable from typical data points, but it’s interesting nonetheless. Take,
    for instance, a single email from Andrew Fastow, the CFO of Enron in its final
    years and one of the main perpetrators of the fraudulent activity that later landed
    him and others in jail. In the data set, none of the emails from Fastow contains
    any sign of fraud or secrecy, but what’s interesting is that there are only nine
    emails from him in the entire corpus. As CFO, one would think his role would include
    communicating with others more often than once every couple of months. Therefore,
    either he avoided email or he did a good job of deleting his emails from all servers
    and others’ personal inboxes and archives. In any case, an email from Fastow might
    be remarkable not because of any inherent information but only within the context
    of its rarity.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个显著或容易与典型数据点区分开的数据点并没有什么量化特征，但仍然很有趣。以安德鲁·费斯托（Andrew Fastow）为例，他是安然公司在其最后几年中的首席财务官，也是后来使他和其他人入狱的欺诈活动的主要肇事者之一。在数据集中，费斯托的邮件中没有任何欺诈或秘密的迹象，但有趣的是，在整个语料库中只有他九封邮件。作为首席财务官，人们可能会认为他的角色应该包括与其他人更频繁的沟通，而不仅仅是每隔几个月一次。因此，他要么避免使用电子邮件，要么他很好地删除了所有服务器和其他人的个人收件箱和存档中的邮件。无论如何，费斯托的邮件可能之所以引人注目，并不是因为任何固有的信息，而仅仅是因为它的罕见性。
- en: 'In a similar way, interesting things in your data might be characterizable
    by their context or by something I might call their neighborhood. *Neighborhood*
    is a term I’m borrowing from topology, a branch of mathematics:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，你数据中的有趣事物可能可以通过它们的上下文或我可能称之为它们的邻域来描述。"邻域"这个术语是从拓扑学中借用的，拓扑学是数学的一个分支：
- en: '*The* neighborhood *of a [data] point is, loosely speaking, a set of other
    points that are similar to, or located near, the point in question.*'
  id: totrans-780
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一个数据点的* 邻域，从广义上讲，是一组与所讨论的点相似或位于该点附近的其他点。'
- en: 'Similarity and location can take on many meanings. For the Enron data, we could
    define one type of neighborhood of a particular email as “the set of emails sent
    by this email’s sender.” Or we could define a neighborhood as “the set of emails
    sent during the same week.” Both of these definitions contain a notion of similarity
    or nearness. By the first definition, an email sent by Andrew Fastow has a small
    neighborhood indeed: only eight other emails. By the second definition of neighborhood,
    the neighborhoods are much larger, often with several hundred emails in a given
    week.'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性和位置可以有多种含义。对于安然数据，我们可以定义一个特定邮件的一个邻域类型为“由这个邮件的发送者发送的邮件集合。”或者我们可以定义一个邻域为“在同一周内发送的邮件集合。”这两种定义都包含相似性或邻近性的概念。根据第一种定义，安德鲁·费斯托夫发送的邮件确实有一个很小的邻域：只有八封其他邮件。根据邻域的第二种定义，邻域要大得多，通常在给定的一周内有几百封邮件。
- en: 'In addition to aspects of a data point itself, you can use its neighborhood
    to help characterize it. If emails from Andrew Fastow are interesting to you,
    maybe all emails sent by people who seldom write emails are interesting. In that
    case, one quantitative characterization of *interesting* uses the same-sender
    definition of neighborhood and can be stated like this:'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据点的本身属性外，你还可以使用其邻域来帮助描述它。如果你对安德鲁·费斯托夫的邮件感兴趣，也许所有很少写邮件的人发送的邮件都很有趣。在这种情况下，对“有趣”的一个定量描述使用相同的发送者定义的邻域，可以表述如下：
- en: '*An email might be interesting if it’s sent by someone who rarely sends emails.*'
  id: totrans-783
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果邮件是由很少发邮件的人发送的，那么这封邮件可能很有趣。*'
- en: You can incorporate this statement into a statistical model. It is quantitative
    (rarity can be quantified) and it can be determined by information contained within
    the data set that you have.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个陈述纳入统计模型中。它是定量的（稀少性可以量化）并且可以通过你拥有的数据集中的信息来确定。
- en: Likewise, you could use a time-based neighborhood to create another characterization
    of *interesting*. Maybe, hypothetically, you found during your search an email
    that was sent in the middle of the night, from a work account to a private email
    address, asking to meet at an all-night diner. No such email exists in the Enron
    data set, but I like to pretend that the case was much more dramatic than it was—data
    science’s version of cloak and dagger, perhaps.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以使用基于时间的邻域来创建对“有趣”的另一种描述。也许，假设你在搜索过程中发现了一封在半夜发送的邮件，从工作账户发送到私人电子邮件地址，要求在一个通宵餐厅见面。在安然数据集中不存在这样的邮件，但我喜欢想象这个情况比实际情况要戏剧化得多——可能是数据科学版的神秘和间谍活动。
- en: 'This concept of middle of the night, or odd hours, can be quantified in a few
    ways. One way is to choose hours of the day that represent middle of the night.
    Another way is to characterize odd hours as those hours in which almost no emails
    were written. You could use a time neighborhood of a few hours and characterize
    some interesting emails like this:'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关于半夜或奇怪时间点的概念可以通过几种方式来量化。一种方式是选择代表半夜的时段。另一种方式是将奇怪时间点定义为几乎没有人发邮件的时段。你可以使用几个小时的时段来描述一些有趣的邮件，例如：
- en: '*An email might be interesting if there are few other emails sent from within
    two hours both before and after the email in question.*'
  id: totrans-787
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果问题邮件前后两小时内发送的其他邮件很少，那么这封邮件可能很有趣。*'
- en: This characterization, like the previous one, is both quantitative (*few* can
    be quantified empirically) and answerable within the data set you have.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 这种描述，就像之前的一个，既是定量的（“少”可以通过经验来量化）又是在你拥有的数据集中可回答的。
- en: A good characterization of an interesting entity or data point is one that is
    quantitative, that is present in or calculable from the data you have, and that
    in some way helps differentiate it from normal data, if only a little. We’ll use
    these characterizations, and I’ll talk about them more, in later sections on preliminary
    analyses and choosing statistical models.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的实体或数据点的良好描述是它既定量，又存在于或可从你的数据中计算出来，并且在某种程度上有助于将其与正常数据区分开来，即使只是略微不同。我们将在后续的初步分析和选择统计模型部分中使用这些描述，并进一步讨论。
- en: 5.4.3\. Data snooping (or not)
  id: totrans-790
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3. 数据挖掘（或不是）
- en: Some might call it *data snooping* to poke around in the data, find examples
    of something you find interesting, and then tailor subsequent analyses to fit
    the examples. Some might say that this will unfairly bias the results and make
    them appear better than they are. For example, if you’re looking to estimate the
    number of blue pickup trucks in your neighborhood, and you happen to know that
    there’s usually a blue pickup truck parked a few blocks away, you’ll probably
    walk in that direction, counting trucks along the way, and that one truck that
    you already know about could skew your results upward, if only slightly. Or, at
    best, you’ll walk around randomly, but being close to your house and on one of
    your typical routes, you’re more likely to walk past it.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会把在数据中四处窥探、找到你感兴趣的东西的例子，然后调整后续分析以适应这些例子称为*数据调查*。有些人可能会说，这会不公平地偏差结果，使它们看起来比实际情况更好。例如，如果你正在寻找你所在社区中蓝色皮卡车的数量，你碰巧知道通常有几辆蓝色皮卡车停在你家附近几条街外，你可能会朝那个方向走去，沿途数着卡车，而你已经知道的那辆卡车可能会略微偏向上调你的结果。或者，在最坏的情况下，你会随机走动，但如果你家附近并且在你典型的路线上，你更有可能经过它。
- en: You want to avoid significant bias in your results, so you want to be careful
    not to let the preliminary characterizations I’m suggesting affect that. Data
    snooping can be a problem, and astute critics are right to say you should avoid
    it sometimes. But snooping is a problem only when assessing the accuracy or quality
    of your results. In particular, if you already know about some of the things your
    methods are attempting to discover again, you’re likely to be successful in those
    cases, and your results will be unfairly good.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望避免结果中的重大偏差，所以你需要注意不要让我建议的初步描述影响这一点。数据调查可能是一个问题，而且敏锐的批评家正确地指出，有时你应该避免它。但是，只有在你评估结果的准确性或质量时，调查才是一个问题。特别是，如果你已经知道你的方法试图再次发现的一些事情，你可能会在这些情况下取得成功，你的结果将是不公平地好。
- en: But you’re not at the assessment phase yet. Right now, while trying to find
    and characterize data points, entities, and other things within the data set that
    are interesting and rare, you should do everything you can to be successful, because
    it’s a hard task. Later, though, all this useful snooping can complicate assessment
    of results, so I bring it up now to make you take note of a potential complication
    and to address potential critics who might say that you shouldn’t snoop around
    in your data.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 但你还没有到达评估阶段。现在，当你试图在数据集中找到和描述有趣且罕见的数据点、实体和其他事物时，你应该尽一切努力取得成功，因为这是一项艰巨的任务。然而，稍后，所有这些有用的调查可能会使结果评估复杂化，所以我现在提出这一点，让你注意潜在的复杂性和解决可能批评你不应该在数据中四处窥探的人。
- en: 5.5\. Rough statistical analysis
  id: totrans-794
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 粗略的统计分析
- en: Already in this chapter I’ve discussed basic descriptive statistics, validating
    assumptions, and characterizing some types of interesting things you’re looking
    for. Now, in terms of statistical sophistication, it’s time to take the analysis
    up one notch, but not two. I cover full-fledged statistical modeling and analysis
    in [chapter 7](kindle_split_017_split_000.xhtml#ch07), but before you get that
    far, it’s better to take only a single step in that direction and see how it works
    out.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我已经讨论了基本描述性统计、验证假设以及描述你正在寻找的一些有趣事物的类型。现在，从统计复杂性来看，是时候将分析提升一个层次，但不是两个。我在第7章（[第7章](kindle_split_017_split_000.xhtml#ch07)）中涵盖了完整的统计建模和分析，但在你达到那个阶段之前，最好只朝那个方向迈出一小步，看看结果如何。
- en: Most sophisticated statistical algorithms take a while to implement; sometimes
    they take a while to run or compute on all your data. And as I’ve mentioned, a
    lot of them are quite fragile or difficult when it comes to understanding how
    and why they give a specific result and if it’s correct, in some sense. That’s
    why I prefer to approach such sophisticated analyses slowly and with care, particularly
    with a new or unfamiliar data set.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数复杂的统计算法需要一段时间来实现；有时它们运行或计算所有数据需要花费一些时间。正如我之前提到的，其中许多在理解它们如何以及为什么给出特定结果，以及是否正确时相当脆弱或困难。这就是为什么我更喜欢缓慢且谨慎地处理这种复杂的分析，尤其是在处理新的或不熟悉的数据集时。
- en: If some of the statistical concepts in this section are unfamiliar to you, feel
    free to skip ahead for now and come back to this section after you finish the
    rest of the book—or at least [chapter 7](kindle_split_017_split_000.xhtml#ch07).
    If you’re already familiar with most sophisticated statistical methods, this section
    can help you decide whether your planned statistical method is a good choice.
    Or if you don’t know yet what method you might use, this section can help you
    figure it out.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 如果本节中的一些统计概念对你来说不熟悉，现在可以随意跳过，并在你完成本书的其余部分——或者至少[第7章](kindle_split_017_split_000.xhtml#ch07)之后回到这一节。如果你已经熟悉大多数复杂的统计方法，本节可以帮助你决定你计划的统计方法是否是一个好选择。或者如果你还不知道你可能会使用什么方法，本节可以帮助你弄清楚。
- en: 5.5.1\. Dumb it down
  id: totrans-798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1. 简化
- en: Most statistical methods can be translated into rough versions that can be implemented
    and tested in a fraction of the time when compared to the full method. Trying
    one or a few of these now, before you begin the full implementation and analysis,
    can provide tremendous insight into what statistical methods will be useful to
    you and how.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数统计方法都可以翻译成简化的版本，与完整方法相比，可以在更短的时间内实现和测试。在你开始全面实施和分析之前，尝试其中的一些方法，可以为你提供关于哪些统计方法对你有用的巨大洞察，以及如何使用它们。
- en: If, for your final analysis, all you intend to do is a linear regression or
    a t-test, by all means charge right in. This section concerns primarily those
    projects that will likely include some sort of classification, clustering, inference,
    modeling, or any other statistical method that has more than a few parameters,
    fixed or variable.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你最终的分析只是进行线性回归或t检验，那就直接进行吧。本节主要关注那些可能包括某种形式的分类、聚类、推断、建模或其他具有多个参数（固定或变量）的统计方法的那些项目。
- en: Classification
  id: totrans-801
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分类
- en: If you plan to do some classification as part of your analysis, there are numerous
    statistical models designed for the task, from random forests to support vector
    machines to gradient boosting. But one of the simplest methods of classification
    is logistic regression.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划在分析中做一些分类，有许多统计模型是为这项任务设计的，从随机森林到支持向量机再到梯度提升。但分类中最简单的方法之一是逻辑回归。
- en: The task of classification is, in its simplest form, assigning one of two class
    labels to entities based on a set of entity features that you’ve chosen and whose
    values you’ve calculated from the data. Typically, the labels are 0 and 1, where
    1 represents interesting in the same sense I’ve used previously, and 0 is normal.
    You can have more classes and more complicated classification, but I’ll save that
    for later.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务在其最简单形式上，是根据你选择并从数据中计算出的实体特征集，将两个类别标签之一分配给实体。通常，标签是0和1，其中1代表有趣，与之前我使用的意义相同，而0是正常。你可以有更多类别和更复杂的分类，但我将留到以后再讲。
- en: The most sophisticated methods in classification have many moving parts and
    therefore have the potential to perform much better than logistic regression.
    But as I’ve mentioned, they’re much harder to understand and debug. Logistic regression
    is a relatively simple method that works like linear regression, except the output
    values (the predictions, for new data) are between 0 and 1.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 分类中最复杂的方法有很多可变部分，因此它们有可能比逻辑回归表现得更好。但正如我之前提到的，它们理解和调试起来要困难得多。逻辑回归是一种相对简单的方法，它的工作方式类似于线性回归，只不过输出值（对新数据的预测）介于0和1之间。
- en: Compared to classification methods from machine learning, logistic regression
    is much faster to calculate and has virtually no parameters that you need to fiddle
    with. On the other hand, it carries a few assumptions—such as a certain type of
    normality—so if you have profoundly skewed or otherwise weird data values, it
    might not be the best choice.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习的分类方法相比，逻辑回归的计算速度要快得多，几乎不需要你调整的参数。另一方面，它包含一些假设——例如某种类型的正态性——所以如果你有严重偏斜或其他奇怪的数据值，它可能不是最佳选择。
- en: If you have a favorite entity feature that you think will help classify yet-unknown
    entities, try it as the only feature/parameter in your logistic regression model.
    Your software tool can tell you whether the feature does indeed help, and then
    you can proceed to try another feature, either by itself or in addition to the
    first. Starting simple is generally the best choice, then increasing complexity,
    and checking to see if it helps.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个你认为可以帮助分类未知实体的实体特征，尝试将其作为逻辑回归模型中唯一的特征/参数。你的软件工具可以告诉你该特征是否确实有帮助，然后你可以尝试另一个特征，无论是单独尝试还是与第一个特征一起尝试。从简单开始通常是最佳选择，然后增加复杂性，并检查它是否有所帮助。
- en: One good candidate for an informative feature for finding suspicious emails
    in the Enron data set is the time of day at which the email was sent. Late-night
    emails might prove suspicious. Another feature might be the number of recipients
    of the email.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 在Enron数据集中寻找可疑电子邮件的有用特征的一个良好候选是发送电子邮件的时间。深夜发送的电子邮件可能证明是可疑的。另一个特征可能是电子邮件的收件人数。
- en: Another, more general method for investigating the usefulness of features for
    classification is to look at the distributions of feature values for each of the
    two classes (0 or 1). Using a couple of plots, you can see whether there seems
    to be a significant difference between the feature values of the two classes.
    [Figure 5.4](kindle_split_014_split_005.xhtml#ch05fig04) is a two-dimensional
    plot of data points from three classes, designated by shape. The x- and y-axes
    represent two feature values for the data points. If your goal is to make statistical
    software that can find data points that are square—without knowing the true shape—you’d
    probably be in good shape; square data points have high x- and y-values. It would
    be easy to find a statistical model that correctly identifies square data points.
    The tough part might be finding the features that, when plotted, give neatly grouped
    classes such as these. Creating plots can help you find those good, useful features
    by giving you a sense of where the classes fall in the space of all data points
    and can help you figure out how to develop or tweak features to make them better.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更通用的方法来调查特征对分类的有用性是查看两个类别（0或1）中每个类别的特征值分布。通过几个图表，你可以看到两个类别的特征值之间是否似乎存在显著差异。[图5.4](kindle_split_014_split_005.xhtml#ch05fig04)
    是三个类别（由形状指定）的数据点的二维图。x轴和y轴代表数据点的两个特征值。如果你的目标是开发出一种可以找到正方形数据点的统计软件——而不需要知道其真实形状——你可能做得很好；正方形数据点具有高的x和y值。找到正确识别正方形数据点的统计模型可能很容易。困难的部分可能在于找到当绘制时能给出这样整齐分组类别的特征。创建图表可以帮助你通过给你一个关于类别在所有数据点空间中的位置的感觉来找到那些好的、有用的特征，并帮助你弄清楚如何开发或调整特征以使其更好。
- en: Figure 5.4\. A plot of three classes, given by shape, in two dimensions^([[1](kindle_split_014_split_005.xhtml#ch05fnn01)])
  id: totrans-809
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4。由形状给出的三个类别的二维图^([[1](kindle_split_014_split_005.xhtml#ch05fnn01)])
- en: ¹
  id: totrans-810
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-811
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From [https://en.wikipedia.org/wiki/Cluster_analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
    (public domain)
  id: totrans-812
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来自 [https://en.wikipedia.org/wiki/Cluster_analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
    (公有领域)
- en: '![](Images/05fig04.jpg)'
  id: totrans-813
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4](Images/05fig04.jpg)'
- en: Lastly, if you have a favorite statistical method for classification that you
    understand, and you know how to adjust its parameters in your statistical software
    to make it simple and easy to understand, then that’s also a good way to perform
    a rough and fast classification. For example, you might use a random forest with
    10 trees and a maximum depth of 2\. Or a support vector machine with linear kernel
    functions can be reasonably easy to understand if you know the theory behind it.
    Both of these might be good choices if you’re familiar with how those techniques
    work. If you do choose this route, it’s important that you understand how to evaluate
    the results of your method, check the contributions of the various features, and
    make sure that the method is working the way you expect it to.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你熟悉并理解你喜欢的统计分类方法，并且知道如何在统计软件中调整其参数以使其简单易懂，那么这也是进行快速粗略分类的好方法。例如，你可能使用一个有10棵树的随机森林，最大深度为2。或者，如果你了解其背后的理论，具有线性核函数的支持向量机可能也相对容易理解。如果你熟悉这些技术的工作方式，这两个都可能是不错的选择。如果你选择这条路线，重要的是你要理解如何评估你方法的结果，检查各种特征的贡献，并确保该方法按预期工作。
- en: Clustering
  id: totrans-815
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 聚类
- en: '*Clustering* is conceptually a lot like classification—there are entities with
    feature values that are intended to fall into groups—except there are no explicit,
    known labels. The process of clustering is often called *unsupervised learning*
    because the results are groups of similar entities—but because there are no labels,
    it may not immediately be clear what each group, or cluster, represents. It often
    takes manual inspection or descriptive statistics to figure out what kinds of
    entities are in each cluster.'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类*在概念上与分类有很多相似之处——存在具有特征值的实体，这些特征值旨在落入不同的组中——但是没有明确的、已知的标签。聚类过程通常被称为*无监督学习*，因为结果是相似实体的组——但是由于没有标签，可能一开始并不清楚每个组或聚类代表什么。通常需要手动检查或描述性统计来弄清楚每个聚类中包含哪些类型的实体。'
- en: As a rough version of clustering, I like to plot various values pertaining to
    entities and use plain visual inspection to determine whether the entities tend
    to form clusters. For data or entities with many aspects and values, it may take
    a while to visually inspect plots of one or two dimensions/variables at a time.
    But if you believe that a few key features should differentiate groups of entities
    from one another, you should be able to see that in a two-dimensional plot. If
    you can’t, you may want to revisit some assumptions that you’ve made. Blindly
    assuming that two entities are similar merely because they fall into the same
    cluster can lead to problems later. For example, even without labels/colors, the
    data points in [figure 5.4](kindle_split_014_split_005.xhtml#ch05fig04) seem to
    cluster well into three groups. A clustering algorithm should be able to find
    them, if you know there are three clusters. On the other hand, if your data doesn’t
    group together so nicely, clustering might give poor results.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 作为聚类的粗略版本，我喜欢绘制与实体相关的各种值，并使用直观的视觉检查来确定实体是否倾向于形成聚类。对于具有许多方面和值的数据和实体，可能需要花费一些时间来直观检查一个或两个维度/变量的图。但是，如果你认为一些关键特征应该区分不同实体组，你应该能在二维图中看到这一点。如果你看不到，你可能需要重新审视你做出的某些假设。仅仅因为两个实体落入同一个聚类就盲目地假设它们相似，可能会在以后导致问题。例如，即使没有标签/颜色，[图5.4](kindle_split_014_split_005.xhtml#ch05fig04)中的数据点似乎很好地聚成了三个组。如果你知道有三个聚类，聚类算法应该能够找到它们。另一方面，如果你的数据没有很好地分组，聚类可能会给出较差的结果。
- en: Beyond visual inspection, the simplest versions of clustering contain few variables
    and few clusters (most of the time, you have to set the number of clusters beforehand).
    If you can choose, say, three or four of your favorite entity features, and they
    cluster nicely using one of the simplest clustering algorithms, perhaps k-means,
    then you’re off to a good start and can proceed to more sophisticated clustering
    methods or configurations. It may also help to plot the results of your clustering
    algorithm within your software tool to make sure everything looks like it makes
    sense.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直观检查之外，最简单的聚类版本包含的变量和聚类很少（大多数情况下，你必须事先设置聚类数量）。如果你可以选择，比如选择三个或四个你最喜欢的实体特征，并且它们使用最简单的聚类算法（如k-means）很好地聚类，那么你就已经迈出了良好的第一步，可以继续使用更复杂的聚类方法或配置。在你的软件工具中绘制聚类算法的结果也可能有所帮助，以确保一切看起来都有意义。
- en: Inference
  id: totrans-819
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推断
- en: '*Statistical inference* is the estimation of a quantitative value that you
    haven’t observed directly. For instance, in the case of the Enron project I’ve
    mentioned, at one point we wanted to estimate the probability that each employee
    would send an email to their boss as opposed to any other potential recipient.
    We intended to include this probability as a latent variable in a statistical
    model of communication, a complex model whose optimal parameter values could be
    found only via a complex optimization technique. For a lot of data, it could be
    slow.'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计推断*是对你没有直接观察到的定量值的估计。例如，在我提到的安然项目中，我们曾经想要估计每个员工向他们的老板发送电子邮件的概率，而不是向任何其他潜在收件人的概率。我们打算将这个概率作为一个潜在变量包含在通信的统计模型中，这是一个复杂的模型，其最优参数值只能通过复杂的优化技术找到。对于大量数据，这可能很慢。'
- en: But we could approximate the inference of this particular latent parameter value
    by counting the number of times each employee wrote an email to their boss and
    how many times they didn’t. It’s a rough approximation, but later, if the full
    model’s optimal parameter is found to be something quite different, we’d know
    that something may have gone wrong. If the two values differ, it doesn’t mean
    that something definitely did go wrong, but if we don’t understand and can’t figure
    out why they differ, it definitely shows that we don’t know how the model works
    on our data.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以通过计算每个员工向老板发送电子邮件的次数以及他们没有发送的次数来近似这个特定潜在参数值的推断。这是一个粗略的近似，但后来，如果完整模型的最佳参数被发现与相当不同，我们会知道可能出了问题。如果这两个值不同，并不意味着肯定出了问题，但如果我们不能理解和无法找出它们不同的原因，这确实表明我们不知道模型是如何在我们数据上工作的。
- en: 'We could approach other latent variables in our statistical model in the same
    way: find a way to get a rough approximation and make note of it for later comparison
    with the estimated value within the full model. Not only is this a good check
    for possible errors, but it also tells us things about our data that we may not
    have learned while calculating descriptive statistics, as discussed earlier in
    this chapter—and, even better, these new pieces of information are specific to
    our project’s goals.'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用同样的方式处理统计模型中的其他潜在变量：找到一种方法来获得粗略的近似，并为此做笔记，以便稍后与完整模型中的估计值进行比较。这不仅是一个检查可能错误的好方法，而且它还告诉我们关于我们的数据的事情，这些事情在我们之前讨论的计算描述性统计时可能没有学到——而且，更好的是，这些新的信息是针对我们项目目标的。
- en: Other statistical methods
  id: totrans-823
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他统计方法
- en: I certainly haven’t covered how to do a rough approximation of every statistical
    method here, but hopefully the previous examples give you the idea. As with almost
    everything in data science, there’s no one solution; nor are there 10 or 100\.
    There are an infinite number of ways to approach each step of the process.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 我当然没有涵盖如何对每种统计方法进行粗略近似的细节，但希望前面的例子给你提供了思路。与数据科学中的几乎所有事情一样，没有一种解决方案；也没有10种或100种。有无数种方法可以处理过程的每一步。
- en: You have to be creative in how you devise and apply quick-and-dirty methods
    because every project is different and has different goals. The main point is
    that you shouldn’t apply sophisticated statistical methods without first making
    sure they’re reasonably appropriate for your project’s goals and your data and
    that you’re using them properly. Applying a simple version of the statistical
    analysis first gives you a feeling for how the method interacts with your data
    and whether it’s appropriate. [Chapter 7](kindle_split_017_split_000.xhtml#ch07)
    discusses several types of statistical methods in far more detail, and from it
    you should be able to gather more ideas for your own analyses.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 你在设计和应用快速简便的方法时必须富有创意，因为每个项目都是不同的，并且有不同的目标。主要观点是，在确保它们合理适用于你的项目目标和数据，并且你正确使用它们之前，你不应该应用复杂的统计方法。首先应用统计分析的简单版本，这会让你对方法如何与你的数据互动以及它是否合适有一个感觉。[第7章](kindle_split_017_split_000.xhtml#ch07)详细讨论了几种统计方法，从中你应该能够为你的分析收集更多想法。
- en: 5.5.2\. Take a subset of the data
  id: totrans-826
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2. 取数据的一个子集
- en: Often you have too much data to run even a simple analysis in a timely fashion.
    At this stage of doing many rough preliminary analyses, it might be OK to use
    subsets of data for testing the applicability of simple statistical methods.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你拥有太多的数据，以至于无法及时进行简单的分析。在这个阶段进行许多粗略的初步分析时，使用数据子集来测试简单统计方法的适用性可能是可以的。
- en: 'If you do apply the rough statistical methods to subsets of the full data set,
    watch out for a few pitfalls:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实将粗略的统计方法应用于整个数据集的子集，请注意以下一些陷阱：
- en: Make sure you have enough data and entities for the statistical methods to give
    significant results. The more complicated the method is, the more data you need.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你有足够的数据和实体，以便统计方法可以给出有意义的成果。方法越复杂，你需要的数据就越多。
- en: If the subset is not representative of the full data set, your results could
    be way off. Calculate descriptive statistics on this subset and compare them to
    the relevant descriptive statistics on the full data set. If they’re similar in
    ways that matter—keep your projects goals in mind—then you’re in good shape.
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果子集不能代表整个数据集，你的结果可能会大相径庭。对这个子集计算描述性统计，并将它们与整个数据集的相关描述性统计进行比较。如果它们在重要的方面相似——记住你的项目目标——那么你处于良好的状态。
- en: If you try only one subset, you may unknowingly have chosen a highly specialized
    or biased subset, even if you run some descriptive statistics. But if you take
    three distinct subsets, do a quick analysis on them all, and get similar results,
    you can be reasonably certain the results will generalize to the full data set.
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只尝试一个子集，你可能会无意中选择了高度专业或具有偏见的子集，即使你运行了一些描述性统计。但如果你取三个不同的子集，对它们进行快速分析，并得到相似的结果，你可以合理地确信这些结果可以推广到整个数据集。
- en: If you try different subsets and get different results, it might not be a bad
    thing. Try to figure out why. Data inherently has variance, and different data
    might give different results, slightly or greatly. Use descriptive statistics
    and diagnosis of these simple statistical methods to make sure you understand
    what’s happening.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你尝试不同的子集并得到不同的结果，这不一定是个坏事情。试着找出原因。数据本质上具有变异性，不同的数据可能会给出不同的结果，无论是轻微还是显著。使用描述性统计和这些简单统计方法的诊断来确保你理解正在发生的事情。
- en: '5.5.3\. Increasing sophistication: does it improve results?'
  id: totrans-833
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. 提高复杂性：它是否改善了结果？
- en: If you can’t get at least moderately good or promising results from a simple
    statistical method, proceeding with a more sophisticated method is dangerous.
    Increasing the sophistication of your method should improve results, but only
    if you’re on the right track. If the simple version of the method isn’t appropriate
    for your data or project, or if the algorithm isn’t configured properly, chances
    are that stepping up the sophistication isn’t going to help. Also, it’s harder
    to fix the configuration of a more sophisticated method, so if you begin with
    an improper configuration of a simple method, your configuration of the more sophisticated
    version will probably be as improper—or even more so—and harder to fix.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法从一种简单的统计方法中获得至少中等或令人期待的结果，那么继续使用更复杂的方法是危险的。提高你方法的专业性应该能改善结果，但前提是你必须走在正确的道路上。如果该方法简单版本不适合你的数据或项目，或者算法配置不当，那么提高复杂性可能不会有所帮助。此外，更复杂方法的配置更难修复，所以如果你从一个简单方法的错误配置开始，那么更复杂版本的配置可能同样不正确，甚至更难修复。
- en: I like to make sure I have a solid, simple method that I understand and that
    clearly gives some helpful if not ideal results, and then I check the results
    as I step up the sophistication of the method. If the results improve with each
    step, I know I’m doing something right. If the results don’t improve, I know I’m
    either doing something wrong or I’ve reached the limit of complexity that the
    data or my project’s goals can handle.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢确保我有一个稳固、简单的方法，我理解它，并且它清楚地给出了一些有用的结果（即使不是理想的结果），然后我在提高方法复杂性的过程中检查结果。如果每一步的结果都在改善，我知道我在做正确的事情。如果结果没有改善，我知道我可能做错了什么，或者我已经达到了数据或项目目标可以处理的复杂性的极限。
- en: Applying methods that are too complex for the data or setting goals that can’t
    handle it is generally called *over-fitting*. Specifically, this implies that
    the methods have too many moving parts, and all these moving parts work perfectly
    on your data, but then when you give the method new data, the accuracy of the
    results isn’t nearly as good. I cover over-fitting in more detail in [chapter
    7](kindle_split_017_split_000.xhtml#ch07), but for now let it suffice to say that
    sophistication should lead to better results—to a point—and if you’re not experiencing
    that, there’s likely a problem somewhere.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 应用过于复杂的数据或设定无法处理的目标通常被称为*过度拟合*。具体来说，这意味着方法有太多的可变部分，并且所有这些可变部分在你的数据上工作得完美无缺，但当你给方法提供新数据时，结果的准确性却远不如之前。我在[第7章](kindle_split_017_split_000.xhtml#ch07)中更详细地讨论了过度拟合，但就目前而言，可以说，专业性应该导致更好的结果——直到某个程度——如果你没有体验到这一点，那么可能某个地方存在问题。
- en: Exercises
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Continuing with the Filthy Money Forecasting personal finance app scenario
    first described in [chapter 2](kindle_split_011_split_000.xhtml#ch02), and relating
    to previous chapters’ exercises, try these exercises:'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用在[第2章](kindle_split_011_split_000.xhtml#ch02)中首先描述的Filthy Money Forecasting个人理财应用场景，并关联到之前章节的练习，尝试以下练习：
- en: '**1\.**'
  id: totrans-839
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1\.**'
- en: ''
  id: totrans-840
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given that a main goal of the app is to provide accurate forecasts, describe
    three types of descriptive statistics you would want to perform on the data in
    order to understand it better.
  id: totrans-841
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑到该应用的主要目标是提供准确的预测，描述三种你希望在数据上执行以更好地理解数据的描述性统计类型。
- en: '**2\.**'
  id: totrans-842
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2\.**'
- en: ''
  id: totrans-843
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Assume that you’re strongly considering trying to use a statistical model to
    classify repeating and one-time financial transactions. What are three assumptions
    you might have about the transactions in one of these two categories?
  id: totrans-844
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设你强烈考虑尝试使用统计模型来分类重复和一次性金融交易。关于这两种类别中的交易，你可能会有哪些假设？
- en: Summary
  id: totrans-845
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Instead of jumping straight into a sophisticated statistical analysis, be intentionally
    deliberate during the exploratory phase of data science, because most problems
    can be avoided—or fixed quickly—through knowledge and awareness.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据科学的探索阶段，不要直接跳入复杂的统计分析，要故意谨慎，因为大多数问题可以通过知识和意识避免或快速解决。
- en: Before analyzing data, state your prior assumptions about the data and check
    that they’re appropriate.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分析数据之前，明确你对数据的先验假设，并检查它们是否合适。
- en: Before assuming there are needles in your haystack, sift through the data, manually
    if necessary, and find some good examples of the types of things you want to find
    more of during the project.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在假设你的麦秆堆里有针之前，手动筛选数据，找到一些好的例子，这些例子是你希望在项目期间找到更多类似事物的类型。
- en: Perform rough statistical analyses on subsets of the data to guarantee you’re
    on the right track before you devote too much time to a full software implementation.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据的子集进行粗略的统计分析，以确保在投入大量时间进行完整软件实现之前，你处于正确的轨道上。
- en: Record any exploratory results somewhere handy; they might be of use later when
    making follow-up decisions.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任何探索性结果记录在方便的地方；它们可能在后续决策时有用。
