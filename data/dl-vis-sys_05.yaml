- en: 4 Structuring DL projects and hyperparameter tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 结构化DL项目和超参数调整
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining performance metrics
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义性能指标
- en: Designing baseline models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计基线模型
- en: Preparing training data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练数据
- en: Evaluating a model and improving its performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型并提高其性能
- en: 'This chapter concludes the first part of this book, providing a foundation
    for deep learning (DL). In chapter 2, you learned how to build a multilayer perceptron
    (MLP). In chapter 3, you learned about a neural network architecture topology
    that is very commonly used in computer vision (CV) problems: convolutional neural
    networks (CNNs). In this chapter, we will wrap up this foundation by discussing
    how to structure your machine learning (ML) project from start to finish. You
    will learn strategies to quickly and efficiently get your DL systems working,
    analyze the results, and improve network performance.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书的第一部分，为深度学习（DL）提供了基础。在第2章中，你学习了如何构建多层感知器（MLP）。在第3章中，你了解了在计算机视觉（CV）问题中非常常用的神经网络架构拓扑：卷积神经网络（CNN）。在本章中，我们将通过讨论如何从头到尾构建你的机器学习（ML）项目来结束这个基础。你将学习策略，以快速有效地让你的DL系统运行，分析结果，并提高网络性能。
- en: 'As you might have already noticed from the previous projects, DL is a very
    empirical process. It relies on running experiments and observing model performance
    more than having one go-to formula for success that fits all problems. We often
    have an initial idea for a solution, code it up, run the experiment to see how
    it did, and then use the outcome of this experiment to refine our ideas. When
    building and tuning a neural network, you will find yourself making many seemingly
    arbitrary decisions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经从前面的项目中注意到的，深度学习（DL）是一个非常经验的过程。它依赖于运行实验和观察模型性能，而不是有一个适用于所有问题的成功公式。我们通常有一个初始的解决方案想法，将其编码，运行实验以查看其表现，然后使用这个实验的结果来完善我们的想法。在构建和调整神经网络时，你会发现自己在做出许多看似随机的决策：
- en: What is a good architecture to start with?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从何开始选择一个好的架构？
- en: How many hidden layers should you stack?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该堆叠多少隐藏层？
- en: How many hidden units or filters should go in each layer?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层应该有多少隐藏单元或过滤器？
- en: What is the learning rate?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率是多少？
- en: Which activation function should you use?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该使用哪种激活函数？
- en: Which yields better results, getting more data or tuning hyperparameters?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个更有利于获得更好的结果，是获取更多数据还是调整超参数？
- en: 'In this chapter, you will learn the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Defining the performance metrics for your system --In addition to model accuracy,
    you will use other metrics like precision, recall, and F-score to evaluate your
    network.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义您系统的性能指标 -- 除了模型准确度之外，你还将使用其他指标，如精确度、召回率和F分数来评估你的网络。
- en: Designing a baseline model --You will choose an appropriate neural network architecture
    to run your first experiment.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计基线模型 -- 你将选择一个合适的神经网络架构来运行你的第一个实验。
- en: Getting your data ready for training --In real-world problems, data comes in
    messy, not ready to be fed to a neural network. In this section, you will massage
    your data to get it ready for learning.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练数据 -- 在现实世界的问题中，数据通常是杂乱的，不适合直接输入到神经网络中。在本节中，你将对数据进行处理，使其准备好进行学习。
- en: Evaluating your model and interpreting its performance --When training is complete,
    you analyze your model’s performance to identify bottlenecks and narrow down improvement
    options. This means diagnosing which of the network components are performing
    worse than expected and identifying whether poor performance is due to overfitting,
    underfitting, or a defect in the data.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估你的模型并解释其性能 -- 训练完成后，你分析模型性能以识别瓶颈并缩小改进选项。这意味着诊断哪些网络组件的表现不如预期，并确定性能不佳是由于过拟合、欠拟合还是数据缺陷。
- en: Improving the network and tuning hyperparameters --Finally, we will dive deep
    into the most important hyperparameters to help develop your intuition about which
    hyperparameters you need to tune. You will use tuning strategies to make incremental
    changes based on your diagnosis from the previous step.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进网络和调整超参数 -- 最后，我们将深入探讨最重要的超参数，以帮助你发展对需要调整哪些超参数的直觉。你将使用调整策略，根据上一步的诊断进行增量更改。
- en: TIP With more practice and experimentation, DL engineers and researchers build
    their intuition over time as to the most effective ways to make improvements.
    My advice is to get your hands dirty and try different architectures and approaches
    to develop your hyperparameter-tuning skills.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 通过更多的实践和实验，深度学习工程师和研究人员会随着时间的推移逐渐建立起对最有效改进方法的直觉。我的建议是亲自动手尝试不同的架构和方法，以发展你的超参数调整技能。
- en: Ready? Let’s get started!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？让我们开始吧！
- en: 4.1 Defining performance metrics
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 定义性能指标
- en: Performance metrics allow us to evaluate our system. When we develop a model,
    we want to find out how well it is working. The simplest way to measure the “goodness”
    of our model is by measuring its accuracy. The accuracy metric measures how many
    times our model made the correct prediction. So, if we test the model with 100
    input samples, and it made the correct prediction 90 times, this means the model
    is 90% accurate.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标允许我们评估我们的系统。当我们开发模型时，我们想知道它工作得如何。衡量我们模型“好坏”的最简单方法就是衡量其准确率。准确率指标衡量我们的模型正确预测的次数。因此，如果我们用
    100 个输入样本测试模型，并且它正确预测了 90 次，这意味着该模型是 90% 准确的。
- en: 'Here is the equation used to calculate model accuracy:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是计算模型准确率的公式：
- en: '![](../Images/4-F01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-F01.png)'
- en: 4.1.1 Is accuracy the best metric for evaluating a model?
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 准确率是评估模型的最佳指标吗？
- en: 'We have been using accuracy as a metric for evaluating our model in earlier
    projects, and it works fine in many cases. But let’s consider the following problem:
    you are designing a medical diagnosis test for a rare disease. Suppose that only
    one in every million people has this disease. Without any training or even building
    a system at all, if you hardcode the output to be always negative (no disease
    found), your system will always achieve 99.999% accuracy. Is that good? The system
    is 99.999% accurate, which might sound fantastic, but it will never capture the
    patients with the disease. This means the accuracy metric is not suitable to measure
    the “goodness” of this model. We need other evaluation metrics that measure different
    aspects of the model’s prediction ability.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在早期项目中一直使用准确率作为评估模型的指标，在很多情况下效果良好。但让我们考虑以下问题：你正在设计一种用于罕见疾病的医学诊断测试。假设每百万分之一的人患有这种疾病。如果没有任何训练甚至根本不构建系统，如果你将输出硬编码为始终为负（未发现疾病），则你的系统将始终达到
    99.999% 的准确率。这是好事吗？系统达到了 99.999% 的准确率，这听起来可能很神奇，但它永远不会捕捉到患有疾病的患者。这意味着准确率指标不适合衡量该模型的“好坏”。我们需要其他评估指标来衡量模型预测能力的不同方面。
- en: 4.1.2 Confusion matrix
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 混淆矩阵
- en: 'To set the stage for other metrics, we will use a confusion matrix : a table
    that describes the performance of a classification model. The confusion matrix
    itself is relatively simple to understand, but the related terminology can be
    a little confusing at first. Once you understand it, you’ll find that the concept
    is really intuitive and makes a lot of sense. Let’s go through it step by step.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为其他指标做准备，我们将使用混淆矩阵：一个描述分类模型性能的表格。混淆矩阵本身相对容易理解，但相关的术语一开始可能会有些令人困惑。一旦你理解了它，你会发现这个概念非常直观，并且非常有意义。让我们一步一步地来探讨它。
- en: The goal is to describe model performance from different angles other than prediction
    accuracy. For example, suppose we are building a classifier to predict whether
    a patient is sick or healthy. The expected classifications are either positive
    (the patient is sick) or negative (the patient is healthy). We run our model on
    1,000 patients and enter the model predictions in table 4.1.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是从除预测准确率之外的不同角度描述模型性能。例如，假设我们正在构建一个分类器来预测患者是否患病。预期的分类是阳性（患者患病）或阴性（患者健康）。我们在
    1,000 名患者上运行我们的模型，并将模型预测输入到表 4.1 中。
- en: Table 4.1 Running our model to predict healthy vs. sick patients
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 运行我们的模型以预测健康与患病患者
- en: '|  | Predicted sick (positive) | Predicted healthy (negative) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | 预测患病（阳性） | 预测健康（阴性） |'
- en: '| Sick patients (positive) | 100 | 30 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 患病患者（阳性） | 100 | 30 |'
- en: '| True positives (TP) | False negative (FN) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 真阳性（TP） | 假阴性（FN） |'
- en: '| Healthy patients (negative) | 70 | 800 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 健康患者（阴性） | 70 | 800 |'
- en: '| False positives (FP) | True negatives (TN) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 假阳性（FP） | 真阴性（TN） |'
- en: 'Let’s now define the most basic terms, which are whole numbers (not rates):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义最基本的概念，它们是整数（不是比率）：
- en: True positives (TP) --The model correctly predicted yes (the patient has the
    disease).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性（TP）--模型正确预测了是（患者患有疾病）。
- en: True negatives (TN) --The model correctly predicted no (the patient does not
    have the disease).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阴性（TN）--模型正确地预测为无（患者没有疾病）。
- en: False positives (FP) --The model falsely predicted yes, but the patient actually
    does not have the disease (in some literature known as a Type I error or error
    of the first kind).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性（FP）--模型错误地预测为有，但实际上患者并没有疾病（在某些文献中称为第一类错误或第一类错误）。
- en: False negatives (FN) --The model falsely predicted no, but the patient actually
    does have the disease (in some literature known as a Type II error or error of
    the second kind).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性（FN）--模型错误地预测为无，但实际上患者确实患有疾病（在某些文献中称为第二类错误或第二类错误）。
- en: The patients that the model predicts are negative (no disease) are the ones
    that the model believes are healthy, and we can send them home without further
    care. The patients that the model predicts are positive (have disease) are the
    ones that we will send for further investigation. Which mistake would we rather
    make? Mistakenly diagnosing someone as positive (has disease) and sending them
    for more investigation is not as bad as mistakenly diagnosing someone as negative
    (healthy) and sending them home at risk to their life. The obvious choice of evaluation
    metric here is that we care more about the number of false negatives (FN). We
    want to find all the sick people, even if the model accidentally classifies some
    healthy people as sick. This metric is called recall.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测为阴性（无疾病）的患者是模型认为健康的人，我们可以让他们回家，无需进一步治疗。模型预测为阳性（有疾病）的患者是我们将送他们进行进一步检查的人。我们更愿意犯哪种错误？错误地将某人诊断为阳性（有疾病）并让他们接受更多检查，不如错误地将某人诊断为阴性（健康）并让他们回家，冒着生命危险。在这里，评价指标显然是，我们更关心假阴性（FN）的数量。我们希望找到所有患病的人，即使模型错误地将一些健康人分类为患病。这个指标被称为召回率。
- en: 4.1.3 Precision and recall
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 精确率和召回率
- en: 'Recall (also known as sensitivity) tells us how many of the sick patients our
    model incorrectly diagnosed as well. In other words, how many times did the model
    incorrectly diagnose a sick patient as negative (false negative, FN)? Recall is
    calculated by the following equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率（也称为灵敏度）告诉我们模型错误地将多少名患病患者诊断为正常。换句话说，模型错误地将多少名患病患者诊断为阴性（假阴性，FN）？召回率通过以下公式计算：
- en: '![](../Images/4-F02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-F02.png)'
- en: 'Precision (also known as specificity) is the opposite of recall. It tells us
    how many of the well patients our model incorrectly diagnosed as sick. In other
    words, how many times did the model incorrectly diagnose a well patient as positive
    (false positive, FP)? Precision is calculated by the following equation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率（也称为特异性）是召回率的对立面。它告诉我们模型错误地将多少名健康患者诊断为患病。换句话说，模型错误地将多少名健康患者诊断为阳性（假阳性，FP）？精确率通过以下公式计算：
- en: '![](../Images/4-F03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-F03.png)'
- en: Identifying an appropriate metric
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 确定合适的指标
- en: 'It is important to note that although in the example of health diagnostics
    we decided that recall is a better metric, other use cases require different metrics,
    like precision. To identify the most appropriate metric for your problem, ask
    yourself which of the two possible false predictions is more consequential: false
    positive or false negative. If your answer is FP, then you are looking for precision.
    If FN is more significant, then recall is your answer.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是，尽管在健康诊断的例子中我们决定召回率是一个更好的指标，但其他用例可能需要不同的指标，如精确率。为了确定您问题的最合适的指标，请问自己两种可能的错误预测哪一种更严重：假阳性还是假阴性。如果您的答案是FP，那么您正在寻找精确率。如果FN更严重，那么召回率就是您的答案。
- en: 'Consider a spam email classifier, for example. Which of the two false predictions
    would you care about more: falsely classifying a non-spam email as spam, in which
    case it gets lost, or falsely classifying a spam email as non-spam, after which
    it makes its way to the inbox folder? I believe you would care more about the
    former. You don’t want the receiver to lose an email because your model misclassified
    it as spam. We want to catch all spam, but it is very bad to lose a non-spam email.
    In this example, precision is a suitable metric to use.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以垃圾邮件分类器为例。您更关心哪种错误预测：错误地将非垃圾邮件分类为垃圾邮件，导致其丢失，还是错误地将垃圾邮件分类为非垃圾邮件，之后它进入收件箱文件夹？我相信您会更关心前者。您不希望收件人因为模型将其错误分类为垃圾邮件而丢失邮件。我们希望捕捉到所有垃圾邮件，但丢失非垃圾邮件是非常糟糕的。在这个例子中，精确率是一个合适的指标。
- en: In some applications, you might care about both precision and recall at the
    same time. That’s called an F-score, as explained next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，你可能会同时关心精确率和召回率。这被称为F分数，如以下所述。
- en: 4.1.4 F-score
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 F分数
- en: 'In many cases, we want to summarize the performance of a classifier with a
    single metric that represents both recall and precision. To do so, we can convert
    precision (*p*) and recall (r ) into a single F-score metric. In mathematics,
    this is called the harmonic mean of p and r:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们希望用一个代表召回率和精确率的单一指标来总结分类器的性能。为此，我们可以将精确率（*p*）和召回率（r）转换为单一的F分数指标。在数学上，这被称为p和r的调和平均数：
- en: '![](../Images/4-F04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-F04.png)'
- en: 'The F-score gives a good overall representation of how your model is performing.
    Let’s take a look at the health-diagnostics example again. We agreed that this
    is a high-recall model. But what if the model is doing really well on the FN and
    giving us a high recall score, but it’s performing poorly on the FP and giving
    us a low precision score? Doing poorly on FP means, in order to not miss any sick
    patients, it is mistakenly diagnosing a lot of patients as sick, to be on the
    safe side. So, while recall might be more important for this problem, it is good
    to look at the model from both scores--precision and recall--together:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: F分数可以很好地整体反映你的模型表现如何。让我们再次看看健康诊断的例子。我们一致认为这是一个高召回率的模型。但如果模型在FN上表现很好，给出了高召回率分数，但在FP上表现不佳，给出了低精确率分数怎么办？FP表现不佳意味着，为了不漏掉任何生病患者，它错误地将许多患者诊断为生病，以确保安全。因此，虽然召回率对于这个问题可能更重要，但查看模型时从精确率和召回率两个分数一起考虑是好的：
- en: '|  | Precision | Recall | F-score |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确率 | 召回率 | F分数 |'
- en: '| Classifier A | 95% | 90% | 92.4% |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 分类器A | 95% | 90% | 92.4% |'
- en: '| Classifier B | 98% | 85% | 91% |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 分类器B | 98% | 85% | 91% |'
- en: NOTE Defining the model evaluation metric is a necessary step because it will
    guide your approach to improving the system. Without clearly defined metrics,
    it can be difficult to tell whether changes to a ML system result in progress
    or not.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：定义模型评估指标是一个必要的步骤，因为它将指导你改进系统的方法。如果没有明确定义的指标，很难判断对机器学习系统的更改是否导致进步。
- en: 4.2 Designing a baseline model
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 设计基线模型
- en: 'Now that you have selected the metrics you will use to evaluate your system,
    it is time to establish a reasonable end-to-end system for training your model.
    Depending on the problem you are solving, you need to design the baseline to suit
    your network type and architecture. In this step, you will want to answer questions
    like these:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经选择了你将用于评估系统的指标，是时候建立一个合理的端到端系统来训练你的模型了。根据你要解决的问题，你需要设计基线以适应你的网络类型和架构。在这一步，你将想要回答以下问题：
- en: Should I use an MLP or CNN network (or RNN, explained later in the book)?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否应该使用MLP或CNN网络（或RNN，本书后面将解释）？
- en: Should I use other object detection techniques like YOLO or SSD (explained in
    later chapters)?
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否应该使用其他目标检测技术，如YOLO或SSD（在后续章节中解释）？
- en: How deep should my network be?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的网络应该有多深？
- en: Which activation type will I use?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将使用哪种激活类型？
- en: What kind of optimizer do I use?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将使用哪种优化器？
- en: Do I need to add any other regularization layers like dropout or batch normalization
    to avoid overfitting?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否需要添加其他正则化层，如dropout或批量归一化，以避免过拟合？
- en: If your problem is similar to another problem that has been studied extensively,
    you will do well to first copy the model and algorithm already known to perform
    the best for that task. You can even use a model that was trained on a different
    dataset for your own problem without having to train it from scratch. This is
    called transfer learning and will be discussed in detail in chapter 6.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的问题与已经被广泛研究的问题相似，你最好首先复制已知在该任务上表现最佳的模型和算法。你甚至可以使用在另一个数据集上训练的模型来解决你的问题，而无需从头开始训练。这被称为迁移学习，将在第6章中详细讨论。
- en: 'For example, in the last chapter’s project, we used the architecture of the
    popular AlexNet as a baseline model. Figure 4.1 shows the architecture of an AlexNet
    deep CNN, with the dimensions of each layer. The input layer is followed by five
    convolutional layers (CONV1 through CONV5), the output of the fifth convolutional
    layer is fed into two fully connected layers (FC6 through FC7), and the output
    layer is a fully connected layer (FC8) with a softmax function:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在上一个章节的项目中，我们使用了流行的 AlexNet 架构作为基线模型。图 4.1 显示了一个 AlexNet 深度 CNN 的架构，以及每层的尺寸。输入层后面是五个卷积层（CONV1
    到 CONV5），第五个卷积层的输出被送入两个全连接层（FC6 到 FC7），输出层是一个具有 softmax 函数的全连接层（FC8）：
- en: INPUT ⇒ CONV1 ⇒ POOL1 ⇒ CONV2 ⇒ POOL2 ⇒ CONV3 ⇒ CONV4 ⇒ CONV5 ⇒ POOL3 ⇒ FC6
    ⇒ FC7 ⇒ SOFTMAX_8
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 ⇒ CONV1 ⇒ POOL1 ⇒ CONV2 ⇒ POOL2 ⇒ CONV3 ⇒ CONV4 ⇒ CONV5 ⇒ POOL3 ⇒ FC6 ⇒ FC7
    ⇒ SOFTMAX_8
- en: '![](../Images/4-1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-1.png)'
- en: Figure 4.1 The AlexNet architecture consists of five convolutional layers and
    three FC layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 AlexNet 架构由五个卷积层和三个 FC 层组成。
- en: 'Looking at the AlexNet architecture, you will find all the network hyperparameters
    that you need to get started with your own model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 AlexNet 架构，您将找到您开始自己的模型所需的所有网络超参数：
- en: 'Network depth (number of layers): 5 convolutional layers plus 3 fully connected
    layers'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络深度（层数）：5 个卷积层加上 3 个全连接层
- en: 'Layers’ depth (number of filters): CONV1 = 96, CONV2 = 256, CONV3 = 384, CONV4
    = 385, CONV5 = 256'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数的深度（过滤器数量）：CONV1 = 96，CONV2 = 256，CONV3 = 384，CONV4 = 385，CONV5 = 256
- en: 'Filter size: 11 × 11, 5 × 5, 3 × 3, 3 × 3, 3 × 3'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器大小：11 × 11、5 × 5、3 × 3、3 × 3、3 × 3
- en: ReLU as the activation function in the hidden layers (CONV1 all the way to FC7)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 作为隐藏层（从 CONV1 到 FC7）的激活函数
- en: Max pooling layers after CONV1, CONV2, and CONV5
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CONV1、CONV2 和 CONV5 后的池化层
- en: FC6 and FC7 with 4,096 neurons each
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个有 4,096 个神经元的 FC6 和 FC7
- en: FC8 with 1000 neurons, using a softmax activation function
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC8 有 1000 个神经元，使用 softmax 激活函数
- en: NOTE In the next chapter, we will discuss some of the most popular CNN architectures
    along with their code implementations in Keras. We will look at networks like
    LeNet, AlexNet, VGG, ResNet, and Inception that will build your understanding
    of what architecture works best for different problems and perhaps inspire you
    to invent your own CNN architecture.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在下一章中，我们将讨论一些最流行的 CNN 架构及其在 Keras 中的代码实现。我们将查看 LeNet、AlexNet、VGG、ResNet 和
    Inception 等网络，这将帮助您了解针对不同问题的最佳架构，并可能激发您发明自己的 CNN 架构。
- en: 4.3 Getting your data ready for training
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 准备您的数据以进行训练
- en: We have defined the performance metrics that we will use to evaluate our model
    and have built the architecture of our baseline model. Let’s get our data ready
    for training. It is important to note that this process varies a lot based on
    the problem and data you have. Here, I’ll explain the basic data-massaging techniques
    that you need to perform before training your model. I’ll also help you develop
    an instinct for what “ready data” looks like so you can determine which preprocessing
    techniques you need.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了我们将用于评估我们的模型的性能指标，并构建了我们的基线模型架构。让我们准备好我们的数据以进行训练。需要注意的是，这个过程在很大程度上取决于您的问题和数据。在这里，我将解释您在训练模型之前需要执行的基本数据预处理技术。我还会帮助您培养对“准备好的数据”外观的直觉，以便您确定需要哪些预处理技术。
- en: 4.3.1 Splitting your data for train/validation/test
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 将您的数据拆分为训练/验证/测试
- en: 'When we train a ML model, we split the data into train and test datasets (figure
    4.2). We use the training dataset to train the model and update the weights, and
    then we evaluate the model against the test dataset that it hasn’t seen before.
    The golden rule here is this: never use the test data for training. The reason
    we should never show the test samples to the model while training is to make sure
    the model is not cheating. We show the model the training samples to learn their
    features, and then we test how it generalizes on a dataset that it has never seen,
    to get an unbiased evaluation of its performance.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个机器学习模型时，我们将数据拆分为训练和测试数据集（图 4.2）。我们使用训练数据集来训练模型并更新权重，然后我们使用模型之前未见过的测试数据集来评估模型。这里的黄金法则如下：永远不要使用测试数据来训练。我们不应该在训练过程中向模型展示测试样本的原因是确保模型没有作弊。我们向模型展示训练样本以学习它们的特征，然后我们测试它在从未见过的数据集上的泛化能力，以获得其性能的无偏评估。
- en: '![](../Images/4-2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-2.png)'
- en: Figure 4.2 Splitting the data into training and testing datasets
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 将数据拆分为训练和测试数据集
- en: What is the validation dataset?
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是验证数据集？
- en: After each epoch during the training process, we need to evaluate the model’s
    accuracy and error to see how it is performing and tune its parameters. If we
    use the test dataset to evaluate the model during training, we will break our
    golden rule of never using the testing data during training. The test data is
    only used to evaluate the final performance of the model after training is complete.
    So we make an additional split called a validation dataset to evaluate and tune
    parameters during training (figure 4.3). Once the model has completed training,
    we test its final performance over the test dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的每个epoch之后，我们需要评估模型的准确性和误差，以查看其表现并调整其参数。如果我们使用测试数据集在训练过程中评估模型，我们将违反我们永不使用测试数据训练的黄金法则。测试数据仅在训练完成后评估模型的最终性能。因此，我们进行了一个额外的分割，称为验证数据集，以在训练过程中评估和调整参数（图4.3）。一旦模型完成训练，我们将在测试数据集上测试其最终性能。
- en: '![](../Images/4-3.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-3.png)'
- en: Figure 4.3 An additional split called a validation dataset to evaluate the model
    during training while keeping the test subset for the final test after training
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 在训练过程中评估模型的一个额外分割，称为验证数据集，以保持测试子集在训练完成后的最终测试
- en: 'Take a look at this pseudo code for model training:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这个用于模型训练的伪代码：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we saw in the project in chapter 3, when we train the model, we get `train_loss`,
    `train_acc`, `val_loss`, and `val_acc` after each epoch (figure 4.4). We use this
    data to analyze the network’s performance and diagnose overfitting and underfitting,
    as you will see in section 4.4.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3章的项目中看到的，当我们训练模型时，在每个epoch后，我们得到`train_loss`、`train_acc`、`val_loss`和`val_acc`（图4.4）。我们使用这些数据来分析网络的性能并诊断过拟合和欠拟合，正如你将在4.4节中看到的。
- en: '![](../Images/4-4.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-4.png)'
- en: Figure 4.4 Training results after each epoch
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 每个epoch后的训练结果
- en: What is a good train/validation/test data split?
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是好的训练/验证/测试数据分割？
- en: Traditionally, an 80/20 or 70/30 split between train and test datasets is used
    in ML projects. When we add the validation dataset, we went with 60/20/20 or 70/15/15\.
    But that was back when an entire dataset was just tens of thousands of samples.
    With the huge amount of data we have now, sometimes 1% for both the validation
    and the test set is enough. For example, if our dataset contains 1 million samples,
    10,000 samples is very reasonable for each of the test and validation sets, because
    it doesn’t make sense to hold back several hundred thousand samples of your dataset.
    It is better to use this data for model training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在机器学习项目中，训练和测试数据集之间使用80/20或70/30的分割。当我们添加验证数据集时，我们采用了60/20/20或70/15/15的分割。但那是在整个数据集只有几万个样本的时候。现在我们拥有的数据量巨大，有时验证和测试集各1%就足够了。例如，如果我们的数据集包含100万个样本，每个测试和验证集有10,000个样本是非常合理的，因为保留几十万个样本的数据集是没有意义的。更好的做法是使用这些数据来训练模型。
- en: So, to recap, if you have a relatively small dataset, the traditional ratios
    might be okay. But if you are dealing with a large dataset, then it is fine to
    set your train and validation sets to much smaller values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，如果你有一个相对较小的数据集，传统的比例可能就足够了。但是，如果你处理的是大型数据集，那么将训练和验证集设置为较小的值是完全可以的。
- en: Be sure datasets are from the same distribution
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据集来自相同的分布
- en: An important thing to be aware of when splitting your data is to make sure your
    train/validation/test datasets come from the same distribution. Suppose you are
    building a car classifier that will be deployed on cell phones to detect car models.
    Keep in mind that DL networks are data-hungry, and the common rule of thumb is
    that the more data you have, the better your model will perform. So, to source
    your data, you decide to crawl the internet for car images that are all high-quality,
    professionally-framed images. You train your model and tune it, you achieve satisfying
    results on your test dataset, and you are ready to release the model to the world--only
    to discover that it is performing poorly on real-life images taken by phone cameras.
    This happens because your model has been trained and tuned to achieve good results
    on high-quality images, so it fails to generalize on real-life images that may
    be blurry or lower resolution or have different characteristics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割你的数据时，需要注意的一个重要事项是确保你的训练/验证/测试数据集来自相同的分布。假设你正在构建一个将在手机上部署的汽车分类器，用于检测汽车型号。记住，深度学习网络对数据有很强的需求，一个常见的经验法则是：你拥有的数据越多，你的模型表现越好。因此，为了获取数据，你决定从互联网上爬取所有高质量的、专业拍摄的汽车图像。你训练你的模型并调整它，你在测试数据集上取得了令人满意的结果，你准备将模型发布到世界——却发现它在手机摄像头拍摄的现实生活中的图像上表现不佳。这是因为你的模型已经被训练和调整以在高质量图像上取得好结果，因此它无法泛化到可能模糊、分辨率较低或具有不同特征的现实中图像。
- en: In more technical words, your training and validation datasets are composed
    of high-quality images, whereas the production images (real life) are lower-quality
    images. Thus it is very important that you add lower-quality images to your train
    and validate datasets. Hence, the train/validate/test datasets should come from
    the same distribution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 用更技术性的语言来说，你的训练集和验证集由高质量图像组成，而生产图像（现实生活中的图像）则是低质量图像。因此，将低质量图像添加到你的训练和验证数据集中非常重要。因此，训练/验证/测试数据集应来自相同的分布。
- en: 4.3.2 Data preprocessing
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 数据预处理
- en: Before you feed your data to the neural network, you will need to do some data
    cleanup and processing to get it ready for your learning model. There are several
    preprocessing techniques to choose from, based on the state of your dataset and
    the problem you are solving. The good news about neural networks is that they
    require minimal data preprocessing. When given a large amount of training data,
    they are able to extract and learn features from raw data, unlike the other traditional
    ML techniques.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在你将数据输入到神经网络之前，你需要进行一些数据清理和预处理，以便为你的学习模型做好准备。根据你的数据集状态和你要解决的问题，你可以选择几种预处理技术。关于神经网络的好消息是，它们需要最少的数据预处理。当给定大量训练数据时，它们能够从原始数据中提取和学习特征，这与其他传统机器学习技术不同。
- en: With that said, preprocessing still might be required to improve performance
    or work within specific limitations on the neural network, such as converting
    images to grayscale, image resizing, normalization, and data augmentation. In
    this section, we’ll go through these preprocessing concepts; we’ll see their code
    implementations in the project at the end of the chapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，预处理仍然可能需要，以提高性能或适应神经网络的具体限制，例如将图像转换为灰度、图像缩放、归一化和数据增强。在本节中，我们将介绍这些预处理概念；我们将在本章末尾的项目中看到它们的代码实现。
- en: Image grayscaling
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像灰度化
- en: 'We talked in chapter 3 about how color images are represented in three matrices
    versus only one matrix for grayscale images; color images add computational complexity
    with their many parameters. You can make a judgment call about converting all
    your images to grayscale, if your problem doesn’t require color, to save on the
    computational complexity. A good rule of thumb here is to use the human-level
    performance rule: if you are able to identify the object with your eyes in grayscale
    images, then a neural network will probably be able to do the same.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第三章中讨论了彩色图像是如何用三个矩阵表示的，而灰度图像只用一个矩阵表示；彩色图像通过其许多参数增加了计算复杂性。如果你的问题不需要颜色，你可以判断是否将所有图像转换为灰度，以节省计算复杂性。这里的一个好经验法则是使用人类水平性能规则：如果你能在灰度图像中用眼睛识别出物体，那么神经网络很可能也能做到同样的。
- en: Image resizing
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像缩放
- en: 'One limitation for neural networks is that they require all images to be the
    same shape. If you are using MLPs, for example, the number of nodes in the input
    layer must be equal to the number of pixels in the image (remember how, in chapter
    3, we flattened the image to feed it to the MLP). The same is true for CNNs. You
    need to set the input shape of the first convolutional layer. To demonstrate this,
    let’s look at the Keras code to add the first CNN layer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个限制是它们需要所有图像具有相同的形状。例如，如果您使用MLPs，输入层的节点数必须等于图像中的像素数（记住在第三章中我们如何将图像展平以供MLP使用）。对于CNNs也是如此。您需要设置第一卷积层的输入形状。为了演示这一点，让我们看看添加第一个CNN层的Keras代码：
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we have to define the shape of the image at the first convolutional
    layer. For example, if we have three images with dimensions of 32 × 32, 28 × 28,
    and 64 × 64, we have to resize all the images to one size before feeding them
    to the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们必须在第一卷积层中定义图像的形状。例如，如果我们有三个尺寸为32 × 32、28 × 28和64 × 64的图像，我们必须在将它们输入模型之前将所有图像调整到同一尺寸。
- en: Data normalization
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据归一化
- en: Data normalization is the process of rescaling your data to ensure that each
    input feature (pixel, in the image case) has a similar data distribution. Often,
    raw images are composed of pixels with varying scales (ranges of values). For
    example, one image may have a pixel value range from 0 to 255, and another may
    have a range of 20 to 200\. Although not required, it is preferred to normalize
    the pixel values to the range of 0 to 1 to boost learning performance and make
    the network converge faster.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化是将您的数据缩放的过程，以确保每个输入特征（在图像的情况下为像素）具有相似的数据分布。通常，原始图像由具有不同尺度（值范围）的像素组成。例如，一张图像的像素值范围可能从0到255，而另一张图像的范围可能为20到200。虽然这不是必需的，但将像素值归一化到0到1的范围以提高学习性能并使网络更快收敛是首选的。
- en: 'To make learning faster for your neural network, your data should have the
    following characteristics:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您的神经网络学习更快，您的数据应具有以下特征：
- en: Small values --Typically, most values should be in the [0, 1] range.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小值 --通常，大多数值应在[0, 1]范围内。
- en: Homogenous --All pixels should have values in the same range.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均匀性 --所有像素的值应在相同的范围内。
- en: Data normalization is done by subtracting the mean from each pixel and then
    dividing the result by the standard deviation. The distribution of such data resembles
    a Gaussian curve centered at zero. To demonstrate the normalization process, figure
    4.5 illustrates the operation in a scatterplot.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化是通过从每个像素中减去均值，然后将结果除以标准差来完成的。此类数据的分布类似于以零为中心的高斯曲线。为了演示归一化过程，图4.5展示了在散点图中的操作。
- en: '![](../Images/4-5.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5](../Images/4-5.png)'
- en: Figure 4.5 To normalize data, we subtract the mean from each pixel and divide
    the result by the standard deviation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 为了归一化数据，我们从每个像素中减去均值，然后将结果除以标准差。
- en: TIP Make sure you normalize your training and test data by using the same mean
    and standard deviation, because you want your data to go through the same transformation
    and rescale exactly the same way. You will see how this is implemented in the
    project at the end of this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 确保您使用相同的均值和标准差对训练数据和测试数据进行归一化，因为您希望数据通过相同的转换并精确地以相同的方式进行缩放。您将在本章末尾的项目中看到这是如何实现的。
- en: In non-normalized data, the cost function will likely look like a squished,
    elongated bowl. After you normalize your features, your cost function will look
    more symmetric. Figure 4.6 shows the cost function of two features, F1 and F2.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在未归一化的数据中，损失函数可能看起来像一个挤压、拉长的碗。在归一化特征后，您的损失函数将看起来更加对称。图4.6显示了两个特征F1和F2的损失函数。
- en: '![](../Images/4-6.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6](../Images/4-6.png)'
- en: Figure 4.6 Normalized features help the GD algorithm go straight forward toward
    the minimum error, thereby reaching it quickly (left). With non-normalized features,
    the GD oscillates toward the direction of the minimum error and reaches the minimum
    more slowly (right).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 归一化特征有助于GD算法直接向最小误差前进，从而快速达到（左）。使用未归一化的特征时，GD会在最小误差的方向上振荡，并且达到最小值较慢（右）。
- en: As you can see, for normalized features, the GD algorithm goes straight forward
    toward the minimum error, thereby reaching it quickly. But for non-normalized
    features, it oscillates toward the direction of the minimum error and ends with
    a long march down the error mountain. It will eventually reach the minimum, but
    it will take longer to converge.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于归一化的特征，GD算法直接向最小误差方向前进，因此快速达到它。但对于非归一化的特征，它会在最小误差方向上振荡，最终以长时间下降误差山结束。它最终会达到最小值，但收敛需要更长的时间。
- en: TIP Why does GD oscillate for non-normalized features? If we don’t normalize
    our data, the range of distribution of feature values will likely be different
    for each feature, and thus the learning rate will cause corrections in each dimension
    that differ proportionally from one another. This forces GD to oscillate to the
    direction of the minimum error and ends up with a longer path down the error.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 为什么GD在非归一化特征上会振荡？如果我们不对数据进行归一化，特征值的分布范围可能会因每个特征而异，因此学习率将导致每个维度上的校正成比例地不同。这迫使GD振荡到最小误差方向，并最终以更长的路径下降误差。
- en: Image augmentation
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像增强
- en: Data augmentation will be discussed in more detail later in this chapter, when
    we cover regularization techniques. But it is important for you to know that this
    is another preprocessing technique that you have in your toolbelt to use when
    needed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强将在本章后面更详细地讨论，当我们介绍正则化技术时。但重要的是你要知道，这是你工具箱中另一个预处理技术，在需要时可以使用。
- en: 4.4 Evaluating the model and interpreting its performance
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 评估模型和解释其性能
- en: After the baseline model is established and the data is preprocessed, it is
    time to train the model and measure its performance. After training is complete,
    you need to determine if there are bottlenecks, diagnose which components are
    performing poorly, and determine whether the poor performance is due to overfitting,
    underfitting, or a defect in the training data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立基线模型并对数据进行预处理之后，是时候训练模型并测量其性能了。训练完成后，你需要确定是否存在瓶颈，诊断哪些组件表现不佳，并确定性能不佳是由于过拟合、欠拟合还是训练数据中的缺陷。
- en: One of the main criticisms of neural networks is that they are “black boxes.”
    Even when they work very well, it is hard to understand why they work so well.
    Many efforts are being made to improve the interpretability of neural networks,
    and this field is likely to evolve rapidly in the next few years. In this section,
    I’ll show you how to diagnose neural networks and analyze their behavior.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的主要批评之一是它们是“黑盒子”。即使它们工作得非常好，也很难理解为什么它们工作得这么好。许多努力正在被做出以提高神经网络的解释性，这个领域可能在接下来的几年里快速发展。在本节中，我将向你展示如何诊断神经网络并分析其行为。
- en: 4.4.1 Diagnosing overfitting and underfitting
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 诊断过拟合和欠拟合
- en: 'After running your experiment, you want to observe its performance, determine
    if bottlenecks are impacting its performance, and look for indicators of areas
    you need to improve. The main cause of poor performance in ML is either overfitting
    or underfitting the training dataset. We talked about overfitting and underfitting
    in chapter 3, but now we will dive a little deeper to understand how to detect
    when the system is fitting the training data too much (overfitting) and when it
    is too simple to fit the data (underfitting):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行你的实验后，你希望观察其性能，确定瓶颈是否影响了其性能，并寻找需要改进的区域。机器学习性能不佳的主要原因是训练数据集的过拟合或欠拟合。我们在第三章中讨论了过拟合和欠拟合，但现在我们将更深入地探讨如何检测系统过度拟合训练数据（过拟合）以及它过于简单以拟合数据（欠拟合）的情况：
- en: 'Underfitting means the model is too simple: it fails to learn the training
    data, so it performs poorly on the training data. One example of underfitting
    is using a single perceptron to classify the'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合意味着模型过于简单：它未能学习训练数据，因此在训练数据上表现不佳。欠拟合的一个例子是使用单个感知器对数据进行分类
- en: '![](../Images/Star.png) and ![](../Images/Circle.png) shapes in figure 4.7\.
    As you can see, a straight line does not split the data accurately.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![星形](../Images/Star.png) 和 ![圆形](../Images/Circle.png) 图形在图4.7中。如你所见，一条直线并不能准确分割数据。'
- en: '![](../Images/4-7.png)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/4-7.png)'
- en: Figure 4.7 An example of underfitting
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.7 欠拟合的例子
- en: 'Overfitting is when the model is too complex for the problem at hand. Instead
    of learning features that fit the training data, it actually memorizes the training
    data. So it performs very well on the training data, but it fails to generalize
    when tested with new data that it hasn’t seen before. In figure 4.8, you see that
    the model fits the data too well: it splits the training data, but this kind of
    fitting will fail to generalize.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合是指模型对于当前问题过于复杂。它不是学习适合训练数据的特征，而是实际上记住了训练数据。因此，它在训练数据上表现非常好，但在测试之前未见过的新数据上无法泛化。在图4.8中，你可以看到模型对数据拟合得太好：它分割了训练数据，但这种拟合将无法泛化。
- en: '![](../Images/4-8.png)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/4-8.png)'
- en: Figure 4.8 An example of overfitting
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.8 过拟合的一个例子
- en: 'We want to build a model that is just right for the data: not too complex,
    causing overfit, or too simple, causing underfit. In figure 4.9, you see that
    the model missed on a data sample of the shape O, but it looks much more likely
    to generalize on new data.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望构建一个适合数据的模型：既不过于复杂导致过拟合，也不过于简单导致欠拟合。在图4.9中，你可以看到模型错过了一个形状为O的数据样本，但它看起来更有可能在新的数据上泛化。
- en: '![](../Images/4-9.png)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/4-9.png)'
- en: Figure 4.9 A model that is just right for the data and will generalize
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.9 一个适合数据的模型，并且能够泛化
- en: TIP The analogy I like to use to explain overfitting and underfitting is a student
    studying for an exam. Underfitting is when the student doesn’t study very well
    and so fails the exam. Overfitting is when the student memorizes the book and
    can answer correctly when asked questions from the book, but answers poorly when
    asked questions from outside the book. The student failed to generalize. What
    we want is a student to learn from the book (training data) well enough to be
    able to generalize when asked questions related to the book material.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 我喜欢用来解释过拟合和欠拟合的类比是学生为考试做准备。欠拟合是当学生没有好好复习，所以考试不及格。过拟合是当学生记住了书本内容，在书本问题上的回答正确，但面对书本外的提问回答得不好。学生没有泛化。我们希望的是一个学生能够从书本（训练数据）中学到足够多的知识，以便在问到与书本材料相关的问题时能够泛化。
- en: 'To diagnose underfitting and overfitting, the two values to focus on while
    training are the training error and the validation error:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要诊断欠拟合和过拟合，在训练过程中需要关注的两个值是训练误差和验证误差：
- en: If the model is doing very well on the training set but relatively poorly on
    the validation set, then it is overfitting. For example, if `train_error` is 1%
    and `val_error` is 10%, it looks like the model has memorized the training dataset
    but is failing to generalize on the validation set. In this case, you might consider
    tuning your hyperparameters to avoid overfitting and iteratively train, test,
    and evaluate until you achieve an acceptable performance.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型在训练集上表现很好，但在验证集上相对较差，那么它就是过拟合。例如，如果`train_error`是1%，而`val_error`是10%，看起来模型已经记住了训练数据集，但在验证集上无法泛化。在这种情况下，你可能需要调整超参数以避免过拟合，并通过迭代训练、测试和评估，直到达到可接受的性能。
- en: If the model is performing poorly on the training set, then it is underfitting.
    For example, if the `train_error` is 14% and `val_error` is 15%, the model might
    be too simple and is failing to learn the training set. You might want to consider
    adding more hidden layers or training longer (more epochs), or try different neural
    network architectures.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型在训练集上表现不佳，那么它就是欠拟合。例如，如果`train_error`是14%，而`val_error`是15%，模型可能过于简单，无法学习训练集。你可能想要考虑添加更多的隐藏层或延长训练时间（更多的epoch），或者尝试不同的神经网络架构。
- en: In the next section, we will discuss several hyperparameter-tuning techniques
    to avoid overfitting and underfitting.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论几种避免过拟合和欠拟合的超参数调整技术。
- en: Using human-level performance to identify a Bayes error rate
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人类水平的表现来识别贝叶斯误差率
- en: We talked about achieving a satisfying performance, but how can we know whether
    performance is good or not? We need a realistic baseline to compare the training
    and validation errors to, in order to know whether we are improving. Ideally,
    a 0% error rate is great, but it is not a realistic target for all problems and
    may even be impossible. That is why we need to define a Bayes error rate.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了实现令人满意的性能，但如何知道性能是好是坏？我们需要一个现实的基础线来比较训练和验证误差，以便知道我们是否在改进。理想情况下，0%的误差率是很好的，但并不是所有问题都现实，甚至可能是不可能的。这就是为什么我们需要定义贝叶斯误差率。
- en: 'A Bayes error rate represents the best possible error our model can achieve
    (theoretically). Since humans are usually very good with visual tasks, we can
    use human-level performance as a proxy to measure Bayes error. For example, if
    you are working on a relatively simple task like classifying dogs and cats, humans
    are very accurate. The human error rate will be very low: say, 0.5%. Then we want
    to compare the `train_error` of our model with this value. If our model accuracy
    is 95%, that’s not satisfying performance, and the model might be underfitting.
    On the other hand, suppose we are working on a more complex task for humans, like
    building a medical image classification model for radiologists. The human error
    rate could be a little higher here: say, 5%. Then a model that is 95% accurate
    is actually doing a good job.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯错误率表示我们的模型理论上可以达到的最佳错误率。由于人类通常在视觉任务上非常出色，我们可以用人类水平的表现作为贝叶斯错误的代理来衡量。例如，如果你正在处理一个相对简单的任务，比如分类狗和猫，人类非常准确。人类的错误率将非常低：比如说，0.5%。然后我们想比较我们模型的`train_error`与这个值。如果我们的模型准确率是95%，那么这不是令人满意的表现，模型可能欠拟合。另一方面，假设我们正在处理一个对人类来说更复杂的任务，比如为放射科医生构建医学图像分类模型。这里的错误率可能会稍高一些：比如说，5%。那么一个准确率为95%的模型实际上做得很好。
- en: 'Of course, this is not to say that DL models can never surpass human performance:
    on the contrary. But it is a good way to draw a baseline to gauge whether a model
    is doing well. (Note that the example error percentages are just arbitrary numbers
    for the sake of the example.)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不是说深度学习模型永远无法超越人类性能：相反。但这是一种很好的方法来设定基线，以衡量模型是否表现良好。（注意，示例中的错误百分比只是为了示例而任意选择的数字。）
- en: 4.4.2 Plotting the learning curves
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 绘制学习曲线
- en: Instead of looking at the training verbose output and comparing the error numbers,
    one way to diagnose overfitting and underfitting is to plot your training and
    validation errors throughout the training, as you see in figure 4.10.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查看训练详细输出并比较错误数字外，诊断过拟合和欠拟合的一种方法是在训练过程中绘制你的训练和验证错误，如图4.10所示。
- en: Figure 4.10A shows that the network improves the loss value (aka learns) on
    the training data but fails to generalize on the validation data. Learning on
    the validation data progresses in the first couple of epochs and then flattens
    out and maybe decreases. This is a form of overfitting. Note that this graph shows
    that the network is actually learning on the training data, a good sign that training
    is happening. So you don’t need to add more hidden units, nor do you need to build
    a more complex model. If anything, your network is too complex for your data,
    because it is learning so much that it is actually memorizing the data and failing
    to generalize to new data. In this case, your next step might be to collect more
    data or apply techniques to avoid overfitting.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10A显示，网络在训练数据上提高了损失值（即学习），但在验证数据上未能泛化。在验证数据上的学习在最初的几个epoch中进展，然后趋于平稳，甚至可能下降。这是一种过拟合的形式。请注意，这个图表显示网络实际上在训练数据上学习，这是训练正在进行的良好迹象。所以你不需要添加更多的隐藏单元，也不需要构建一个更复杂的模型。如果有什么的话，你的网络对你的数据来说太复杂了，因为它学习得太多，实际上是在记忆数据，未能泛化到新数据。在这种情况下，你的下一步可能是收集更多数据或应用避免过拟合的技术。
- en: Figure 4.10B shows that the network performs poorly on both training and validation
    data. In this case, your network is not learning. You don’t need more data, because
    the network is too simple to learn from the data you already have. Your next step
    is to build a more complex model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10B显示，网络在训练和验证数据上都表现不佳。在这种情况下，你的网络没有在学习。你不需要更多的数据，因为网络太简单，无法从你已有的数据中学习。你的下一步是构建一个更复杂的模型。
- en: '![](../Images/4-10.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-10.png)'
- en: Figure 4.10 (*a*) The network improves the loss value on the training data but
    fails to generalize on the validation data. (*b*) The network performs poorly
    on both the training and validation data. (C) The network learns the training
    data and generalizes to the validation data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 (*a*) 网络在训练数据上提高了损失值（即学习），但在验证数据上未能泛化。(*b*) 网络在训练和验证数据上都表现不佳。(*c*) 网络学习了训练数据并泛化到验证数据。
- en: Figure 4.10C shows that the network is doing a good job of learning the training
    data and generalizing to the validation data. This means there is a good chance
    that the network will have good performance out in the wild on test data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10C显示，网络在学习和泛化验证数据方面做得很好。这意味着网络在野外测试数据上可能会有很好的性能。
- en: '4.4.3 Exercise: Building, training, and evaluating a network'
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 练习：构建、训练和评估网络
- en: Before we move on to hyperparameter tuning, let’s run a quick experiment to
    see how we split the data and build, train, and visualize the model results. You
    can see an exercise notebook for this at [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www.computervisionbook.com](http://www.computervisionbook.com).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行超参数调整之前，让我们快速运行一个实验，看看我们如何分割数据，构建、训练和可视化模型结果。您可以在 [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    或 [www.computervisionbook.com](http://www.computervisionbook.com) 找到这个练习的笔记本。
- en: 'In this exercise, we will do the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将执行以下操作：
- en: Create toy data for our experiment
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我们的实验创建玩具数据
- en: Split the data into 80% training and 20% testing datasets
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为 80% 的训练数据和 20% 的测试数据集
- en: Build the MLP neural network
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多层感知器（MLP）神经网络
- en: Train the model
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Evaluate the model
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Visualize the results
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化结果
- en: 'Here are the steps:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是步骤：
- en: 'Import the dependencies:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入依赖项：
- en: '[PRE2]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The scikit-learn library to generate sample data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 scikit-learn 库生成样本数据
- en: ❷ Keras method that converts a class vector to a binary class matrix (one-hot
    encoding)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将类别向量转换为二进制类别矩阵（独热编码）的 Keras 方法
- en: ❸ Neural networks and layers library
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 神经网络和层库
- en: ❹ Visualization library
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 可视化库
- en: 'Use `make_blobs` from scikit-learn to generate a toy dataset with only two
    features and three label classes:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `make_blobs` 生成一个只有两个特征和三个标签类别的玩具数据集：
- en: '[PRE3]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Use `to_categorical` from Keras to one-hot-encode the label:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 的 `to_categorical` 对标签进行独热编码：
- en: '[PRE4]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Split the dataset into 80% training data and 20% test data. Note that we did
    not create a validation dataset in this example, for simplicity:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为 80% 的训练数据和 20% 的测试数据。注意，在这个例子中，为了简化，我们没有创建验证数据集：
- en: '[PRE5]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Develop the model architecture--here, a very simple, two-layer MLP network
    (figure 4.11 shows the model summary):'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发模型架构--这里，一个非常简单的、两层 MLP 网络模型（图 4.11 显示了模型摘要）：
- en: '[PRE6]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Two input dimensions because we have two features. ReLU activation function
    for hidden layers.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 由于我们有两个特征，因此有两个输入维度。隐藏层使用 ReLU 激活函数。
- en: ❷ Softmax activation for the output layer with three nodes because we have three
    classes
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 输出层使用 softmax 激活函数，因为有三类，有三个节点
- en: ❸ Cross-entropy loss function (explained in chapter 2) and adam optimizer (explained
    in the next section)
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 交叉熵损失函数（在第 2 章中解释）和 Adam 优化器（将在下一节中解释）
- en: '![](../Images/4-11.png)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4-11](../Images/4-11.png)'
- en: Figure 4.11 Model summary
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.11 模型摘要
- en: 'Train the model for 1,000 epochs:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型 1,000 个周期：
- en: '[PRE7]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Evaluate the model:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型：
- en: '[PRE8]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Plot the learning curves of model accuracy (figure 4.12):'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制模型准确率的学习曲线（图 4.12）：
- en: '[PRE9]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/4-12.png)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4-12](../Images/4-12.png)'
- en: 'Figure 4.12 The learning curves: both train and test curves fit the data with
    similar behavior.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.12 学习曲线：训练和测试曲线都表现出类似的行为，与数据拟合。
- en: 'Let’s evaluate the network. Looking at the learning curve in figure 4.12, you
    can see that both train and test curves fit the data with a similar behavior.
    This means the network is not overfitting, which would be indicated if the train
    curve was doing well but the test curve was not. But could the network be underfitting?
    Maybe: 82% on a very simple dataset like this is considered poor performance.
    To improve the performance of this neural network, I would try to build a more
    complex network and experiment with other underfitting techniques.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估网络。查看图 4.12 中的学习曲线，你可以看到训练和测试曲线都表现出类似的行为。这意味着网络没有过拟合，如果训练曲线表现良好而测试曲线不佳，则表明过拟合。但是，网络可能欠拟合吗？也许：在如此简单的数据集上
    82% 的性能被认为是较差的。为了提高这个神经网络的性能，我会尝试构建一个更复杂的网络，并尝试其他欠拟合技术。
- en: 4.5 Improving the network and tuning hyperparameters
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 改进网络和调整超参数
- en: 'After you run your training experiment and diagnose for overfitting and underfitting,
    you need to decide whether it is more effective to spend your time tuning the
    network, cleaning up and processing your data, or collecting more data. The last
    thing you want to do is to spend a few months working in one direction only to
    find out that it barely improves network performance. So, before discussing the
    different hyperparameters to tune, let’s answer this question first: should you
    collect more data?'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在你运行训练实验并诊断过拟合和欠拟合之后，你需要决定是否值得花时间调整网络、清理和加工你的数据，或者收集更多数据。你最不想做的事情就是花几个月时间只在一个方向上工作，最后发现几乎没提高网络性能。所以，在讨论不同的超参数调整之前，让我们先回答这个问题：你应该收集更多数据吗？
- en: 4.5.1 Collecting more data vs. tuning hyperparameters
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 收集更多数据与调整超参数
- en: We know that deep neural networks thrive on lots of data. With that in mind,
    ML novices often throw more data to the learning algorithm as their first attempt
    to improve its performance. But collecting and labeling more data is not always
    a feasible option and, depending on your problem, could be very costly. Plus,
    it might not even be that effective.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道深度神经网络在大量数据上表现良好。考虑到这一点，机器学习新手通常会尝试将更多数据投入到学习算法中，作为提高其性能的第一步。但是，收集和标注更多数据并不总是可行的选择，而且根据你的问题，可能会非常昂贵。此外，它可能甚至并不那么有效。
- en: 'NOTE While efforts are being made to automate some of the data-labeling process,
    at the time of writing, most labeling is done manually, especially in CV problems.
    By manually, I mean that actual human beings look at each image and label them
    one by one (this is called human in the loop). Here is another layer of complexity:
    if you are labeling lung X-ray images to detect a certain tumor, for example,
    you need qualified physicians to diagnose the images. This will cost a lot more
    than hiring people to classify dogs and cats. So collecting more data might be
    a good solution for some accuracy issues and increase the model’s robustness,
    but it is not always a feasible option.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：尽管正在努力自动化数据标注的一些过程，但在撰写本文时，大多数标注仍然是手动完成的，尤其是在计算机视觉问题中。这里指的是实际的人类查看每一张图片并逐个进行标注（这被称为人机交互）。这里还有另一层复杂性：如果你正在标注肺部X光片以检测某种肿瘤，例如，你需要有资格的医生来诊断这些图像。这比雇佣人来分类狗和猫的成本要高得多。因此，对于某些准确性问题，收集更多数据可能是一个好的解决方案，并提高模型的鲁棒性，但这并不总是可行的选择。
- en: In other scenarios, it is much better to collect more data than to improve the
    learning algorithm. So it would be nice if you had quick and effective ways to
    figure out whether it is better to collect more data or tune the model hyperparameters.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，收集更多数据比改进学习算法要好得多。所以，如果你有快速有效的方法来确定是收集更多数据还是调整模型超参数会更好，那就太好了。
- en: 'The process I use to make this decision is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来做出这个决定的流程如下：
- en: Determine whether the performance on the training set is acceptable as-is.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定训练集上的性能是否可以接受。
- en: 'Visualize and observe the performance of these two metrics: training accuracy
    (`train_acc`) and validation accuracy (`val_acc`).'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化并观察这两个指标的性能：训练准确率（`train_acc`）和验证准确率（`val_acc`）。
- en: If the network yields poor performance on the training dataset, this is a sign
    of underfitting. There is no reason to gather more data, because the learning
    algorithm is not using the training data that is already available. Instead, try
    tuning the hyperparameters or cleaning up the training data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果网络在训练数据集上表现不佳，这是欠拟合的迹象。没有理由收集更多数据，因为学习算法没有使用已经可用的训练数据。相反，尝试调整超参数或清理训练数据。
- en: If performance on the training set is acceptable but is much worse on the test
    dataset, then the network is overfitting your training data and failing to generalize
    to the validation set. In this case, collecting more data could be effective.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果训练集上的性能可以接受，但在测试数据集上却差得多，那么网络就是过拟合了训练数据，并且未能推广到验证集。在这种情况下，收集更多数据可能是有效的。
- en: TIP When evaluating model performance, the goal is to categorize the high-level
    problem. If it’s a data problem, spend more time on data preprocessing or collecting
    more data. If it’s a learning algorithm problem, try to tune the network.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在评估模型性能时，目标是分类高级问题。如果是数据问题，则花更多时间在数据预处理或收集更多数据上。如果是学习算法问题，则尝试调整网络。
- en: 4.5.2 Parameters vs. hyperparameters
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 参数与超参数
- en: Let’s not get parameters confused with hyperparameters. Hyperparameters are
    the variables that we set and tune. Parameters are the variables that the network
    updates with no direct manipulation from us. Parameters are variables that are
    learned and updated by the network during training, and we do not adjust them.
    In neural networks, parameters are the weights and biases that are optimized automatically
    during the backpropagation process to produce the minimum error. In contrast,
    hyperparameters are variables that are not learned by the network. They are set
    by the ML engineer before training the model and then tuned. These are variables
    that define the network structure and determine how the network is trained. Hyperparameter
    examples include learning rate, batch size, number of epochs, number of hidden
    layers, and others discussed in the next section.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不要将参数与超参数混淆。超参数是我们设置和调整的变量。参数是网络在没有任何直接操作的情况下更新的变量。参数是网络在训练期间学习并更新的变量，我们不会调整它们。在神经网络中，参数是在反向传播过程中自动优化以产生最小误差的权重和偏差。相比之下，超参数是网络没有学习的变量。它们在训练模型之前由机器学习工程师设置，然后进行调整。这些变量定义了网络结构并决定了网络的训练方式。超参数的例子包括学习率、批量大小、训练轮数、隐藏层数量以及其他在下一节中讨论的内容。
- en: Turning the knobs
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 调整旋钮
- en: 'Think of hyperparameters as knobs on a closed box (the neural network). Our
    job is to set and tune the knobs to yield the best performance:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将超参数想象成封闭盒子（神经网络）上的旋钮。我们的任务是设置和调整旋钮以获得最佳性能：
- en: '![](../Images/4-unnumb-1K.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-1K.png)'
- en: 4.5.3 Neural network hyperparameters
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 神经网络超参数
- en: DL algorithms come with several hyperparameters that control many aspects of
    the model’s behavior. Some hyperparameters affect the time and memory cost of
    running the algorithm, and others affect the model’s prediction ability.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法带有多个控制模型行为许多方面的超参数。一些超参数影响算法的运行时间和内存成本，而其他超参数影响模型的预测能力。
- en: The challenge with hyperparameter tuning is that there are no magic numbers
    that work for every problem. This is related to the no free lunch theorem that
    we referred to in chapter 1\. Good hyperparameter values depend on the dataset
    and the task at hand. Choosing the best hyperparameters and knowing how to tune
    them require an understanding of what each hyperparameter does. In this section,
    you will build your intuition about why you would want to nudge a hyperparameter
    one way or another, and I’ll propose good starting values for some of the most
    effective hyperparameters.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整的挑战在于没有适用于每个问题的魔法数字。这与我们在第一章中提到的无免费午餐定理相关。好的超参数值取决于数据集和任务。选择最佳超参数并了解如何调整它们需要理解每个超参数的作用。在本节中，你将建立对为什么想要调整超参数一个方向或另一个方向的理解，我将提出一些最有效超参数的良好起始值。
- en: 'Generally speaking, we can categorize neural network hyperparameters into three
    main categories:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，我们可以将神经网络超参数分为三个主要类别：
- en: Network architecture
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构
- en: Number of hidden layers (network depth)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层数量（网络深度）
- en: Number of neurons in each layer (layer width)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层的神经元数量（层宽度）
- en: Activation type
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活类型
- en: Learning and optimization
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习和优化
- en: Learning rate and decay schedule
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率和衰减计划
- en: Mini-batch size
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量大小
- en: Optimization algorithms
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法
- en: Number of training iterations or epochs (and early stopping criteria)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练迭代次数或轮数（以及提前停止标准）
- en: Regularization techniques to avoid overfitting
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免过拟合的正则化技术
- en: L2 regularization
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化
- en: Dropout layers
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 层
- en: Data augmentation
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: We discussed all of these hyperparameters in chapters 2 and 3 except the regularization
    techniques. Next, we will cover them quickly with a focus on understanding what
    happens when we tune each knob up or down and how to know which hyperparameter
    to tune.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二章和第三章中讨论了所有这些超参数，除了正则化技术。接下来，我们将快速介绍它们，重点关注当我们调整每个旋钮上下时会发生什么，以及如何知道应该调整哪个超参数。
- en: 4.5.4 Network architecture
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.4 网络架构
- en: 'First, let’s talk about the hyperparameters that define the neural network
    architecture:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈定义神经网络架构的超参数：
- en: Number of hidden layers (representing the network depth)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层数量（代表网络深度）
- en: Number of neurons in each layer, also known as hidden units (representing the
    network width)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的神经元数量，也称为隐藏单元（代表网络宽度）
- en: Activation functions
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Depth and width of the neural network
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络的深度和宽度
- en: Whether you are designing an MLP, CNN, or other neural network, you need to
    decide on the number of hidden layers in your network (depth) and the number of
    neurons in each layer (width). The number of hidden layers and units describes
    the learning capacity of the network. The goal is to set the number large enough
    for the network to learn the data features. A smaller network might underfit,
    and a larger network might overfit. To know what is a “large enough” network,
    you pick a starting point, observe the performance, and then tune up or down.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在设计一个MLP、CNN还是其他神经网络，你都需要决定你网络中的隐藏层数量（深度）以及每层的神经元数量（宽度）。隐藏层数量和单元数量描述了网络的学习能力。目标是设置足够大的数量，以便网络能够学习数据特征。一个较小的网络可能会欠拟合，而一个较大的网络可能会过拟合。要知道什么是“足够大”的网络，你需要选择一个起点，观察性能，然后调整上下。
- en: The more complex the dataset, the more learning capacity the model will need
    to learn its features. Take a look at the three datasets in figure 4.13.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集越复杂，模型学习其特征所需的学习能力就越大。看看图4.13中的三个数据集。
- en: If you provide the model with too much learning capacity (too many hidden units),
    it might tend to overfit the data and memorize the training set. If your model
    is overfitting, you might want to decrease the number of hidden units.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你给模型提供过多的学习容量（过多的隐藏单元），它可能会倾向于过拟合数据并记住训练集。如果你的模型正在过拟合，你可能想要减少隐藏单元的数量。
- en: '![](../Images/4-13.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-13.png)'
- en: Figure 4.13 The more complex the dataset, the more learning capacity the model
    will need to learn its features.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 数据集越复杂，模型学习其特征所需的学习能力就越大。
- en: Generally, it is good to add hidden neurons until the validation error no longer
    improves. The trade-off is that it is computationally expensive to train deeper
    networks. Having a small number of units may lead to underfitting, while having
    more units is usually not harmful, with appropriate regularization (like dropout
    and others discussed later in this chapter).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，添加隐藏神经元直到验证错误不再提高是好的。权衡的是，训练更深层的网络在计算上很昂贵。拥有较少的单元可能会导致欠拟合，而拥有更多的单元通常不会有害，只要适当的正则化（如dropout和本章后面讨论的其他内容）。
- en: Try playing around with the Tensorflow playground ([https://playground.tensorflow
    .org](https://playground.tensorflow.org)) to develop more intuition. Experiment
    with different architectures, and gradually add more layers and more units in
    hidden layers while observing the network’s learning behavior.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在Tensorflow playground（[https://playground.tensorflow.org](https://playground.tensorflow.org)）上玩一玩，以培养更多的直觉。尝试不同的架构，并在观察网络的学习行为的同时，逐渐添加更多层和隐藏层中的更多单元。
- en: Activation type
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活类型
- en: 'Activation functions (discussed extensively in chapter 2) introduce nonlinearity
    to our neurons. Without activations, our neurons would pass linear combinations
    (weighted sums) to each other and not solve any nonlinear problems. This is a
    very active area of research: every few weeks, we are introduced to new types
    of activations, and there are many available. But at the time of writing, ReLU
    and its variations (like Leaky ReLU) perform the best in hidden layers. And in
    the output layer, it is very common to use the softmax function for classification
    problems, with the number of neurons equal to the number of classes in your problem.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数（在第2章中广泛讨论）为我们的神经元引入了非线性。没有激活，我们的神经元只会将线性组合（加权求和）传递给彼此，而无法解决任何非线性问题。这是一个非常活跃的研究领域：每隔几周，我们就会接触到新的激活类型，而且有很多可供选择。但在撰写本文时，ReLU及其变体（如Leaky
    ReLU）在隐藏层中表现最佳。而在输出层，对于分类问题，非常常见的是使用softmax函数，其神经元数量等于你问题中的类别数量。
- en: Layers and parameters
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 层和参数
- en: When considering the number of hidden layers and units in your neural network
    architecture, it is useful to think in terms of the number of parameters in the
    network and their effect on computational complexity. The more neurons in your
    network, the more parameters the network has to optimize. (In chapter 3, we learned
    how to print the model summary to see the total number of parameters that will
    be trained.)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑你的神经网络架构中的隐藏层数量和单元数量时，考虑网络中的参数数量及其对计算复杂性的影响是有用的。你网络中的神经元越多，网络需要优化的参数就越多。（在第3章中，我们学习了如何打印模型摘要以查看将要训练的总参数数量。）
- en: 'Based on your hardware setup for the training process (computational power
    and memory), you can determine whether you need to reduce the number of parameters.
    To reduce the number of training parameters, you can do one of the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的硬件配置（计算能力和内存）来决定您是否需要减少参数数量。为了减少训练参数数量，您可以执行以下操作之一：
- en: Reduce the depth and width of the network (hidden layers and units). This will
    reduce the number of training parameters and, hence, reduce the neural network
    complexity.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少网络的深度和宽度（隐藏层和单元）。这将减少训练参数数量，从而降低神经网络复杂性。
- en: Add pooling layers, or tweak the strides and padding of the convolutional layers
    to reduce the feature map dimensions. This will lower the number of parameters.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加池化层，或者调整卷积层的步长和填充，以减少特征图维度。这将降低参数数量。
- en: These are just examples to help you see how you will look at the number of training
    parameters in real projects and the trade-offs you will need to make. Complex
    networks lead to a large number of training params, which in turn lead to high
    needs for computational power and memory.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是帮助您了解如何在真实项目中看待训练参数数量以及您需要做出的权衡的例子。复杂的网络导致大量的训练参数，这反过来又导致对计算能力和内存的高需求。
- en: The best way to build your baseline architecture is to look at the popular architectures
    available to solve specific problems and start from there; evaluate its performance,
    tune its hyperparameters, and repeat. Remember how we were inspired by AlexNet
    to design our CNN in the image classification project in chapter 3\. In the next
    chapter, we will explore some of the most popular CNN architectures like LeNet,
    AlexNet, VGG, ResNet, and Inception.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 构建您的基线架构的最佳方式是查看可用于解决特定问题的流行架构，并从这里开始；评估其性能，调整其超参数，并重复。记得我们在第3章的图像分类项目中如何受到AlexNet的启发来设计我们的CNN。在第5章中，我们将探讨一些最流行的CNN架构，如LeNet、AlexNet、VGG、ResNet和Inception。
- en: 4.6 Learning and optimization
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 学习与优化
- en: Now that we have built our network architecture, it is time to discuss the hyperparameters
    that determine how the network learns and optimize its parameter to achieve the
    minimum error.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了我们的网络架构，是时候讨论那些决定网络如何学习和优化其参数以实现最小误差的超参数了。
- en: 4.6.1 Learning rate and decay schedule
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 学习率和衰减计划
- en: The learning rate is the single most important hyperparameter, and one should
    always make sure that it has been tuned. If there is only time to optimize one
    hyperparameter, then this is the hyperparameter that is worth tuning.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是单个最重要的超参数，应该始终确保它已经被调整。如果只有时间优化一个超参数，那么这个值得调整的超参数就是它。
- en: --Yoshua Bengio
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: --约书亚·本吉奥
- en: The learning rate (lr value) was covered extensively in chapter 2\. As a refresher,
    let’s think about how gradient descent (GD) works. The GD optimizer searches for
    the optimal values of weights that yield the lowest error possible. When setting
    up our optimizer, we need to define the step size that it takes when it descends
    the error mountain. This step size is the learning rate. It represents how fast
    or slow the optimizer descends the error curve. When we plot the cost function
    with only one weight, we get the oversimplified U-curve in figure 4.14, where
    the weight is randomly initialized at a point on the curve.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（lr值）在第2章中已经进行了广泛讨论。作为复习，让我们思考一下梯度下降（GD）是如何工作的。GD优化器寻找权重最优值，以产生尽可能低的误差。在设置我们的优化器时，我们需要定义它下降误差山时采取的步长。这个步长就是学习率。它代表了优化器下降误差曲线的速度快慢。当我们仅用一个权重绘制成本函数时，如图4.14所示，我们得到一个简化的U形曲线，其中权重随机初始化在曲线上的一个点上。
- en: '![](../Images/4-14.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-14.png)'
- en: Figure 4.14 When we plot the cost function with only one weight, we get an oversimplified
    U-curve.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 当我们仅用一个权重绘制成本函数时，得到的是一个过于简化的U形曲线。
- en: The GD calculates the gradient to find the direction that reduces the error
    (derivative). In figure 4.14, the descending direction is to the right. The GD
    starts taking steps down after each iteration (epoch). Now, as you can see in
    figure 4.15, if we make a miraculously correct choice of the learning rate value,
    we land on the best weight value that minimizes the error in only one step. This
    is an impossible case that I’m using for elaboration purposes. Let’s call this
    the ideal lr value.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法计算梯度以找到减少错误的方向（导数）。在图4.14中，下降方向是向右的。梯度下降法在每个迭代（epoch）之后开始向下移动。现在，正如你在图4.15中可以看到的，如果我们奇迹般地正确选择了学习率值，我们就能在一步之内找到最小化错误的最佳权重值。这是一个不可能的情况，我正在用它来阐述。让我们称这个值为理想的学习率。
- en: '![](../Images/4-15.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-15.png)'
- en: Figure 4.15 if we make a miraculously correct choice of the learning rate value,
    we land on the best weight value that minimizes the error in only one step.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 如果我们奇迹般地正确选择了学习率值，我们就能在一步之内找到最小化错误的最佳权重值。
- en: If the learning rate is smaller than the ideal lr value, then the model can
    continue to learn by taking smaller steps down the error curve until it finds
    the most optimal value for the weight (figure 4.16). Much smaller means it will
    eventually converge but will take longer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率小于理想学习率值，那么模型可以通过沿着错误曲线采取更小的步骤继续学习，直到找到权重最优化值（图4.16）。太小意味着它最终会收敛，但需要更长的时间。
- en: '![](../Images/4-16.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-16.png)'
- en: 'Figure 4.16 A learning rate smaller than the ideal lr value: the model takes
    smaller steps down the error curve.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 理想学习率值更小的学习率：模型沿着错误曲线采取更小的步骤。
- en: If the learning rate is larger than the ideal lr value, the optimizer will overshoot
    the optimal weight value in the first step, and then overshoot again on the other
    side in the next step (figure 4.17). This could possibly yield a lower error than
    what we started with and converge to a reasonable value, but not the lowest error
    that we are trying to reach.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率大于理想的学习率值，优化器会在第一步中超过最优权重值，然后在下一步中再次在另一侧超过（图4.17）。这可能会产生比我们开始时更低的错误，并收敛到一个合理的值，但不是我们试图达到的最低错误。
- en: '![](../Images/4-17.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-17.png)'
- en: 'Figure 4.17 A learning rate larger than the ideal lr value: the optimizer overshoot
    the optimal weight value.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 学习率大于理想学习率值：优化器超过最优权重值。
- en: If the learning rate is much larger than the ideal lr value (more than twice
    as much), the optimizer will not only overshoot the ideal weight, but get farther
    and farther from the min error (figure 4.18). This phenomenon is called divergence.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率远大于理想的学习率值（多出两倍以上），优化器不仅会超过理想的权重值，而且会越来越远离最小错误（图4.18）。这种现象称为发散。
- en: '![](../Images/4-18.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-18.png)'
- en: 'Figure 4.18 A learning rate much larger than the ideal lr value: the optimizer
    gets farther from the min error.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 学习率远大于理想学习率值：优化器越来越远离最小错误。
- en: Too-high vs. too-low learning rate
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 过高与过低的学习率
- en: 'Setting the learning rate high or low is a trade-off between the optimizer
    speed versus performance. Too-low lr requires many epochs to converge, often too
    many. Theoretically, if the learning rate is too small, the algorithm is guaranteed
    to eventually converge if kept running for infinity time. On the other hand, too-high
    lr might get us to a lower error value faster because we take bigger steps down
    the error curve, but there is a better chance that the algorithm will oscillate
    and diverge away from the minimum value. So, ideally, we want to pick the lr that
    is just right (optimal): it swiftly reaches the minimum point without being so
    big that it might diverge.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将学习率设置得过高或过低是在优化器速度与性能之间进行权衡。太低的学习率需要很多个epoch才能收敛，通常太多。理论上，如果学习率太小，算法在无限时间内运行的情况下是保证最终收敛的。另一方面，过高的学习率可能会更快地将我们带到更低的错误值，因为我们沿着错误曲线采取了更大的步骤，但算法振荡并偏离最小值的可能性更大。因此，理想情况下，我们希望选择一个恰到好处的学习率（最优）：它迅速达到最小点，而不会太大以至于可能发散。
- en: 'When plotting the loss value against the number of training iterations (epochs),
    you will notice the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当将损失值与训练迭代次数（epochs）进行绘图时，你会注意到以下情况：
- en: Much smaller lr--The loss keeps decreasing but needs a lot more time to converge.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 太小的学习率--损失值持续下降，但需要更多的时间来收敛。
- en: Larger lr--The loss achieves a better value than what we started with, but is
    still far from optimal.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的学习率--损失值达到了比我们开始时更好的值，但仍然远未达到最优。
- en: Much larger lr--The loss might initially decrease, but it starts to increase
    as the weight values get farther and farther away from the optimal values.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较大的学习率--损失可能最初会下降，但随着权重值越来越远离最佳值，它开始上升。
- en: Good lr--The loss decreases consistently until it reaches the minimum possible
    value.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的学习率--损失持续下降，直到达到可能的最小值。
- en: '![](../Images/4-unnumb-2KEY.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-2KEY.png)'
- en: The difference between very high, high, good, and low learning rates
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 非常高、高、良好和低学习率之间的区别
- en: 4.6.2 A systematic approach to find the optimal learning rate
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.2 寻找最佳学习率的系统方法
- en: The optimal learning rate will be dependent on the topology of your loss landscape,
    which in turn is dependent on both your model architecture and your dataset. Whether
    you are using Keras, Tensorflow, PyTorch, or any other DL library, using the default
    learning rate value of the optimizer is a good start leading to decent results.
    Each optimizer type has its own default value. Read the documentation of the DL
    library that you are using to find out the default value of your optimizer. If
    your model doesn’t train well, you can play around with the lr variable using
    the usual suspects--0.1, 0.01, 0.001, 0.0001, 0.00001, and 0.000001--to improve
    performance or speed up training by searching for an optimal learning rate.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳学习率将取决于你的损失景观的拓扑结构，这反过来又取决于你的模型架构和你的数据集。无论你使用Keras、Tensorflow、PyTorch还是任何其他深度学习库，使用优化器的默认学习率值是一个好的开始，可以导致不错的结果。每种优化器类型都有自己的默认值。阅读你使用的深度学习库的文档，以了解你优化器的默认值。如果你的模型训练效果不佳，你可以通过使用常见的值--0.1、0.01、0.001、0.0001、0.00001和0.000001--来调整学习率变量，以改善性能或加快训练速度，以寻找最佳学习率。
- en: 'The way to debug this is to look at the validation loss values in the training
    verbose:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 调试此问题的方法是查看训练过程中的验证损失值：
- en: If `val_loss` decreases after each step, that’s good. Keep training until it
    stops improving.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果每次步骤后`val_loss`都下降，那很好。继续训练，直到它停止改进。
- en: 'If training is complete and `val_loss` is still decreasing, then maybe the
    learning rate was so small that it didn’t converge yet. In this case, you can
    do one of two things:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练完成且`val_loss`仍在下降，那么可能是因为学习率太小，还没有收敛。在这种情况下，你可以做以下两件事之一：
- en: Train again with the same learning rate but with more training iterations (epochs)
    to give the optimizer more time to converge.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相同的学习率，但增加更多的训练迭代（周期）来给优化器更多的时间收敛。
- en: Increase the lr value a little and train again.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微增加学习率（lr）的值，然后再次进行训练。
- en: If `val_loss` starts to increase or oscillate up and down, then the learning
    rate is too high and you need to decrease its value.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`val_loss`开始增加或上下波动，那么学习率太高，你需要降低其值。
- en: 4.6.3 Learning rate decay and adaptive learning
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.3 学习率衰减和自适应学习
- en: 'Finding the learning rate value that is just right for your problem is an iterative
    process. You start with a static lr value, wait until training is complete, evaluate,
    and then tune. Another way to go about tuning your learning rate is to set a learning
    rate decay: a method by which the learning rate changes during training. It often
    performs better than a static value, and drastically reduces the time required
    to get optimal results.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适合你问题的最佳学习率是一个迭代的过程。你从一个静态的学习率值开始，等待训练完成，评估，然后调整。调整学习率的另一种方法是设置学习率衰减：一种在训练过程中改变学习率的方法。它通常比静态值表现更好，并且可以大幅减少获得最佳结果所需的时间。
- en: By now, it’s clear that when we try lower learning values, we have a better
    chance to get to a lower error point. But training it will take longer. In some
    cases, training takes so long it becomes infeasible. A good trick is to implement
    a decay rate in our learning rate. The decay rate tells our network to automatically
    decrease the lr throughout the training process. For example, we can decrease
    the lr by a constant value of (*x*) for each (*n*) number of steps. This way,
    we can start with the higher value to take bigger steps toward the minimum, and
    then gradually decrease the learning rate every (*n*) epochs to avoid overshooting
    the ideal lr.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，很明显，当我们尝试降低学习率时，我们更有可能达到更低的错误点。但训练它将需要更长的时间。在某些情况下，训练时间过长，变得不可行。一个很好的技巧是在我们的学习率中实现一个衰减率。衰减率告诉我们的网络在整个训练过程中自动降低学习率。例如，我们可以通过每个(*n*)步数减少一个常数(*x*)来降低学习率。这样，我们可以从较高的值开始，以更大的步幅向最小值迈进，然后在每个(*n*)个周期后逐渐降低学习率，以避免超过理想的学习率。
- en: One way to accomplish this is by reducing the learning rate linearly (linear
    decay). For example, you can decrease it by half every five epochs, as shown in
    figure 4.19.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一种方法是通过线性减少学习率（线性衰减）。例如，你可以每五个epoch将学习率减半，如图4.19所示。
- en: '![](../Images/4-19.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-19.png)'
- en: Figure 4.19 Decreasing the lr by half every five epochs
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 每5个epoch将学习率lr减半
- en: Another way is to decrease the lr exponentially (exponential decay). For example,
    you can multiply it by 0.1 every eight epochs (figure 4.20). Clearly, the network
    will converge a lot slower than with linear decay, but it will eventually converge.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是指数减少学习率（指数衰减）。例如，你可以每8个epoch将学习率乘以0.1（见图4.20）。显然，与线性衰减相比，网络收敛速度会慢得多，但最终会收敛。
- en: '![](../Images/4-20.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-20.png)'
- en: Figure 4.20 Multiplying the lr by 0.1 every eight epochs
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 每8个epoch将学习率lr乘以0.1
- en: 'Other clever learning algorithms have an adaptive learning rate (adaptive learning).
    These algorithms use a heuristic approach that automatically updates the lr when
    the training stops. This means not only decreasing the lr when needed, but also
    increasing it when improvements are too slow (too-small lr). Adaptive learning
    usually works better than other learning rate-setting strategies. Adam and Adagrad
    are examples of adaptive learning optimizers: more on adaptive optimizers later
    in this chapter.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 其他聪明的学习算法具有自适应学习率（自适应学习）。这些算法使用启发式方法，在训练停止时自动更新学习率。这意味着不仅需要在必要时减少学习率，还需要在改进太慢（学习率太小）时增加它。自适应学习通常比其他学习率设置策略更有效。Adam和Adagrad是自适应学习优化器的例子：本章后面将详细介绍自适应优化器。
- en: 4.6.4 Mini-batch size
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.4 小批量大小
- en: Mini-batch size is another hyperparameter that you need to set and tune in the
    optimizer algorithm. The `batch_size` hyperparameter has a big effect on resource
    requirements of the training process and speed.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量大小是优化器算法中你需要设置和调整的另一个超参数。`batch_size`超参数对训练过程的资源需求和速度有重大影响。
- en: 'In order to understand the mini-batch, let’s back up to the three GD types
    that we explained in chapter 2--batch, stochastic, and mini-batch:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解小批量，让我们回顾一下我们在第二章中解释的三个GD类型--批量、随机和批量：
- en: Batch gradient descent (BGD) --We feed the entire dataset to the network all
    at once, apply the feedforward process, calculate the error, calculate the gradient,
    and backpropagate to update the weights. The optimizer calculates the gradient
    by looking at the error generated after it sees all the training data, and the
    weights are updated only once after each epoch. So, in this case, the mini-batch
    size equals the entire training dataset. The main advantage of BGD is that it
    has relatively low noise and bigger steps toward the minimum (see figure 4.21).
    The main disadvantage is that it can take too long to process the entire training
    dataset at each step, especially when training on big data. BGD also requires
    a huge amount of memory for training large datasets, which might not be available.
    BGD might be a good option if you are training on a small dataset.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量梯度下降（BGD）--我们将整个数据集一次性喂给网络，应用前向过程，计算误差，计算梯度，并通过反向传播来更新权重。优化器通过查看所有训练数据生成的误差来计算梯度，并且每个epoch后只更新一次权重。因此，在这种情况下，小批量大小等于整个训练数据集。BGD的主要优点是它具有相对较低的噪声和更大的向最小值迈进的一步（见图4.21）。主要缺点是处理整个训练数据集可能需要太长时间，尤其是在大数据集上训练时。BGD还需要大量的内存来训练大型数据集，这可能不可用。如果你在训练小型数据集，BGD可能是一个不错的选择。
- en: '![](../Images/4-21.png)'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/4-21.png)'
- en: Figure 4.21 Batch GD with low noise on its path to the minimum error
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.21 批量GD在向最小误差迈进的过程中具有低噪声
- en: Stochastic gradient descent (SGD) --Also called online learning. We feed the
    network a single instance of the training data at a time and use this one instance
    to do the forward pass, calculate error, calculate the gradient, and backpropagate
    to update the weights (figure 4.22). In SGD, the weights are updated after it
    sees each single instance (as opposed to processing the entire dataset before
    each step for BGD). SGD can be extremely noisy as it oscillates on its way to
    the global minimum because it takes a step down after each single instance, which
    could sometimes be in the wrong direction. This noise can be reduced by using
    a smaller learning rate, so, on average, it takes you in a good direction and
    almost always performs better than BGD. With SGD you get to make progress quickly
    and usually reach very close to the global minimum. The main disadvantage is that
    by calculating the GD for one instance at a time, you lose the speed gain that
    comes with matrix multiplication in the training calculations.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）--也称为在线学习。我们每次向网络提供单个训练数据实例，并使用这个实例进行前向传播、计算误差、计算梯度，并反向传播以更新权重（图4.22）。在SGD中，权重在看到每个单个实例后更新（与在每一步之前处理整个数据集的批量梯度下降（BGD）相反）。SGD可能会非常嘈杂，因为它在向全局最小值振荡的过程中会向下迈出一步，这有时可能是错误的方向。通过使用较小的学习率可以减少这种噪声，因此，平均而言，它将你引向正确的方向，并且几乎总是比BGD表现更好。使用SGD，你可以快速取得进展，通常非常接近全局最小值。主要缺点是，通过逐个实例计算GD，你失去了训练计算中矩阵乘法带来的速度提升。
- en: '![](../Images/4-22.png)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图4.22 随机GD在高噪声下振荡其路径至最小误差](../Images/4-22.png)'
- en: Figure 4.22 Stochastic GD with high noise that oscillates on its path to the
    minimum error
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.22 随机GD在高噪声下振荡其路径至最小误差
- en: To recap BGD and SGD, on one extreme, if you set your mini-batch size to 1 (stochastic
    training), the optimizer will take a step down the error curve after computing
    the gradient for every single instance of the training data. This is good, but
    you lose the increased speed of using matrix multiplication. On the other extreme,
    if your mini-batch size is your entire training dataset, then you are using BGD.
    It takes too long to make a step toward the minimum error when processing large
    datasets. Between the two extremes, there is mini-batch GD.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾BGD和SGD，在一种极端情况下，如果你将你的小批量大小设置为1（随机训练），优化器将在计算每个单个训练数据实例的梯度后沿着误差曲线向下迈出一步。这是好的，但你失去了使用矩阵乘法带来的速度提升。在另一种极端情况下，如果你的小批量大小是整个训练数据集，那么你正在使用BGD。处理大型数据集时，向最小误差迈出一步需要太长时间。在这两个极端之间，存在小批量梯度下降（MB-GD）。
- en: Mini-batch gradient descent (MB-GD) --A compromise between batch and stochastic
    GD. Instead of computing the gradient from one sample (SGD) or all training samples
    (BGD), we divide the training sample into mini-batches to compute the gradient
    from. This way, we can take advantage of matrix multiplication for faster training
    and start making progress instead of having to wait to train the entire training
    set.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量梯度下降（MB-GD）--在批量梯度下降和随机梯度下降之间的折衷方案。我们不是从单个样本（SGD）或所有训练样本（BGD）计算梯度，而是将训练样本分成小批量来计算梯度。这样，我们可以利用矩阵乘法加快训练速度，并开始取得进展，而不是等待训练整个训练集。
- en: Guidelines for choosing mini-batch size
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 选择小批量大小的指南
- en: First, if you have a small dataset (around less than 2,000), you might be better
    off using BGD. You can train the entire dataset quite fast.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你有一个小数据集（大约少于2,000个），你可能更适合使用BGD。你可以相当快地训练整个数据集。
- en: 'For large datasets, you can use a scale of mini-batch size values. A typical
    starting value for the mini-batch is 64 or 128\. You can then tune it up and down
    on this scale: 32, 64, 128, 256, 512, 1024, and keep doubling it as needed to
    speed up training. But make sure that your mini-batch size fits in your CPU/GPU
    memory. Mini-batch sizes of 1024 and larger are possible but quite rare. A larger
    mini-batch size allows a computational boost that uses matrix multiplication in
    the training calculations. But that comes at the expense of needing more memory
    for the training process and generally more computational resources. The following
    figure shows the relationship between batch size, computational resources, and
    number of epochs needed for neural network training:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，你可以使用一系列迷你批量大小的值。迷你批量大小的典型起始值是64或128。然后你可以在这个范围内调整它：32，64，128，256，512，1024，并在需要时将其翻倍以加快训练速度。但请确保你的迷你批量大大小于你的CPU/GPU内存。1024和更大的迷你批量大大小于可能，但相当罕见。更大的迷你批量大大小于允许在训练计算中使用矩阵乘法来提高计算效率。但这需要更多的训练过程内存和通常更多的计算资源。以下图显示了批量大小、计算资源和神经网络训练所需的轮数之间的关系：
- en: '![](../Images/4-unnumb-3K.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-3K.png)'
- en: The relationship between batch size, computational resources, and number of
    epochs
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小、计算资源和训练轮数之间的关系
- en: 4.7 Optimization algorithms
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 优化算法
- en: In the history of DL, many researchers proposed optimization algorithms and
    showed that they work well with some problems. But most of them subsequently proved
    to not generalize well to the wide range of neural networks that we might want
    to train. In time, the DL community came to feel that the GD algorithm and some
    of its variants work well. So far, we have discussed batch, stochastic, and mini-batch
    GD.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的历史中，许多研究人员提出了优化算法，并表明它们在某些问题上的表现良好。但其中大多数随后被证明不能很好地推广到我们可能想要训练的广泛神经网络。随着时间的推移，深度学习社区逐渐认识到GD算法及其一些变体表现良好。到目前为止，我们已经讨论了批量、随机和迷你批量的GD。
- en: We learned that choosing a proper learning rate can be challenging because a
    too-small learning rate leads to painfully slow convergence, while a too-large
    learning rate can hinder convergence and cause the loss function to fluctuate
    around the minimum or even diverge. We need more creative solutions to further
    optimize GD.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，选择合适的学习率可能具有挑战性，因为学习率太小会导致收敛速度痛苦地缓慢，而学习率太大可能会阻碍收敛，并导致损失函数在最小值周围波动，甚至发散。我们需要更多创造性的解决方案来进一步优化GD。
- en: NOTE Optimizer types are well explained in the documentation of most DL frameworks.
    In this section, I’ll explain the concepts of two of the most popular gradient-descent-based
    optimizers--Momentum and Adam--that really stand out and have been shown to work
    well across a wide range of DL architectures. This will help you build a good
    foundation to dive deeper into other optimization algorithms. For more about optimization
    algorithms, read “An overview of gradient descent optimization algorithms” by
    Sebastian Ruder ([https://arxiv.org/pdf/1609.04747.pdf](https://arxiv.org/pdf/1609.04747.pdf)).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：大多数深度学习框架的文档中都有很好地解释了优化器类型。在本节中，我将解释两种最受欢迎的基于梯度下降的优化器——动量和Adam——的概念，它们确实非常突出，并且在广泛的深度学习架构中已被证明效果良好。这将帮助你建立一个良好的基础，以便更深入地了解其他优化算法。有关优化算法的更多信息，请阅读Sebastian
    Ruder的“梯度下降优化算法概述”（[https://arxiv.org/pdf/1609.04747.pdf](https://arxiv.org/pdf/1609.04747.pdf))。
- en: 4.7.1 Gradient descent with momentum
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.1 带动量的梯度下降
- en: Recall that SGD ends up with some oscillations in the vertical direction toward
    the minimum error (figure 4.23). These oscillations slow down the convergence
    process and make it harder to use larger learning rates, which could result in
    your algorithm overshooting and diverging.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，SGD最终会在垂直方向上向最小误差振荡（图4.23）。这些振荡会减慢收敛过程，并使得使用更大的学习率更加困难，这可能导致你的算法超出并发散。
- en: '![](../Images/4-23.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-23.png)'
- en: Figure 4.23 SGD oscillates in the vertical direction toward the minimum error.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23 SGD在垂直方向上向最小误差振荡。
- en: To reduce these oscillations, a technique called momentum was invented that
    lets the GD navigate along relevant directions and softens the oscillation in
    irrelevant directions. In other words, it makes learning slower in the vertical-direction
    oscillations and faster in the horizontal-direction progress, which will help
    the optimizer reach the target minimum much faster.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少这些振荡，发明了一种称为动量的技术，它让GD沿着相关方向导航，并减轻了无关方向中的振荡。换句话说，它使得在垂直方向上的振荡学习速度变慢，而在水平方向上的进步速度变快，这将帮助优化器更快地达到目标最小值。
- en: 'This is similar to the idea of momentum from classical physics: when a snowball
    rolls down a hill, it accumulates momentum, going faster and faster. In the same
    way, our momentum term increases for dimensions whose gradients point in the same
    direction and reduces updates for dimensions whose gradients change direction.
    This leads to faster convergence and reduces oscillations.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这与经典物理学中的动量概念类似：当一个雪球从山上滚下来时，它会积累动量，越滚越快。同样，我们的动量项会增加梯度指向同一方向的维度，并减少梯度改变方向的维度的更新。这有助于加快收敛速度并减少振荡。
- en: How the math works in momentum
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 动量中的数学是如何工作的
- en: 'The math here is really simple and straightforward. The momentum is built by
    adding a velocity term to the equation that updates the weight:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数学非常简单直接。动量是通过向更新权重的方程中添加速度项来构建的：
- en: '*w[new]* = *w[old]* - *α* *dE*/*dw**[i]* ❶'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*w[new]* = *w[old]* - *α* *dE*/*dw**[i]* ❶'
- en: '*w[new]* = *w[old]* - learning rate × gradient + velocity term ❷'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*w[new]* = *w[old]* - 学习率 × 梯度 + 速度项 ❷'
- en: ❶ Original update rule
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 原始更新规则
- en: ❷ New rule after adding velocity
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加速度后的新规则
- en: The velocity term equals the weighted average of the past gradients.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 速度项等于过去梯度的加权平均值。
- en: 4.7.2 Adam
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.2 Adam
- en: Adam stands for adaptive moment estimation. Adam keeps an exponentially decaying
    average of past gradients, similar to momentum. Whereas momentum can be seen as
    a ball rolling down a slope, Adam behaves like a heavy ball with friction to slow
    down the momentum and control it. Adam usually outperforms other optimizers because
    it helps train a neural network model much more quickly than the techniques we
    have seen earlier.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Adam代表自适应动量估计。Adam保持过去梯度的指数衰减平均值，类似于动量。而动量可以看作是一个滚下斜坡的球，Adam则像是一个带有摩擦的重球，它会减慢动量并控制它。Adam通常比其他优化器表现更好，因为它可以帮助我们更快地训练神经网络模型。
- en: 'Again, we have new hyperparameters to tune. But the good news is that the default
    values of major DL frameworks often work well, so you may not need to tune at
    all--except for the learning rate, which is not an Adam-specific hyperparameter:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们有新的超参数需要调整。但好消息是，主要深度学习框架的默认值通常效果很好，所以你可能不需要调整——除了学习率，它不是Adam特有的超参数：
- en: '[PRE10]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The authors of Adam propose these default values:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Adam的作者提出了这些默认值：
- en: The learning rate needs to be tuned.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整学习率。
- en: For the momentum term β1, a common choice is 0.9.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于动量项 β1，一个常见的选择是 0.9。
- en: For the RMSprop term β 2, a common choice is 0.999.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 RMSprop 项 β2，一个常见的选择是 0.999。
- en: ε is set to 10-8.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ε 被设置为 10^-8。
- en: 4.7.3 Number of epochs and early stopping criteria
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.3 epochs数量和提前停止标准
- en: A training iteration, or epoch, is when the model goes a full cycle and sees
    the entire training dataset at once. The epoch hyperparameter is set to define
    how many iterations our network continues training. The more training iterations,
    the more our model learns the features of our training data. To diagnose whether
    your network needs more or fewer training epochs, keep your eyes on the training
    and validation error values.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 训练迭代，或称为一个epoch，是指模型完成一个完整周期并一次性看到整个训练数据集。epoch超参数被设置为定义我们的网络继续训练的迭代次数。训练迭代越多，我们的模型就越能学习到训练数据的特征。为了诊断你的网络是否需要更多或更少的训练epochs，关注训练和验证错误值。
- en: The intuitive way to think about this is that we want to continue training as
    long as the error value is decreasing. Correct? Let’s take a look at the sample
    verbose output from a network training in figure 4.24.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这个直观的方式是，我们希望只要错误值在下降，就继续训练。对吗？让我们看看图 4.24 中网络训练的样本详细输出。
- en: '![](../Images/4-24.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-24.png)'
- en: Figure 4.24 Sample verbose output of the first five epochs. Both training and
    validation errors are improving.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24 第一五个epochs的样本详细输出。训练和验证错误都在改善。
- en: You can see that both training and validation errors are decreasing. This means
    the network is still learning. It doesn’t make sense to stop the training at this
    point. The network is clearly still making progress toward the minimum error.
    Let’s let it train for six more epochs and observe the results (figure 4.25).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到训练和验证错误都在下降。这意味着网络仍在学习。在这个时候停止训练是没有意义的。网络显然仍在朝着最小错误的方向进步。让我们再让它训练六个epoch，并观察结果（图4.25）。
- en: '![](../Images/4-25.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-25.png)'
- en: Figure 4.25 The training error is still improving, but the validation error
    started oscillating from epoch 8 onward.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.25 训练错误仍在改善，但验证错误从第8个epoch开始开始波动。
- en: It looks like the training error is doing well and still improving. That’s good.
    This means the network is improving on the training set. However, if you look
    at epochs 8 and 9, you will see that `val_error` started to oscillate and increase.
    Improving `train_error` while not improving `val_error` means the network is starting
    to overfit the training data and failing to generalize to the validation data.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来训练错误做得很好，仍在改善。这是好的。这意味着网络在训练集上正在改善。然而，如果你看第8个和第9个epoch，你会看到`val_error`开始波动并增加。在`train_error`改善的同时`val_error`没有改善意味着网络开始对训练数据进行过拟合，并且无法推广到验证数据。
- en: Let’s plot the training and validation errors (figure 4.26). You can see that
    both the training and validation errors were improving at first, but then the
    validation error started to increase, leading to overfitting. We need to find
    a way to stop the training just before it starts to overfit. This technique is
    called early stopping.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制训练和验证错误（图4.26）。你可以看到一开始训练和验证错误都在改善，但随后验证错误开始增加，导致过拟合。我们需要找到一种方法在训练开始过拟合之前停止训练。这种技术被称为提前停止。
- en: 4.7.4 Early stopping
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.4 提前停止
- en: 'Early stopping is an algorithm widely used to determine the right time to stop
    the training process before overfitting happens. It simply monitors the validation
    error value and stops the training when the value starts to increase. Here is
    the early stopping function in Keras:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止是一种广泛使用的算法，用于在过拟合发生之前确定停止训练过程的时间。它简单地监控验证错误值，并在值开始增加时停止训练。以下是Keras中的提前停止函数：
- en: '[PRE11]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `EarlyStopping` function takes the following arguments:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStopping`函数接受以下参数：'
- en: '`monitor`--The metric you monitor during training. Usually we want to keep
    an eye on `val_loss` because it represents our internal testing of model performance.
    If the network is doing well on the validation data, it will probably do well
    on test data and production.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor`--训练期间监控的指标。通常我们希望关注`val_loss`，因为它代表我们对模型性能的内部测试。如果网络在验证数据上表现良好，它可能在测试数据和生产数据上表现也好。'
- en: '`min_delta`--The minimum change that qualifies as an improvement. There is
    no standard value for this variable. To decide the `min_delta` value, run a few
    epochs and see the change in error and validation accuracy. Define `min_delta`
    according to the rate of change. The default value of 0 works pretty well in many
    cases.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_delta`--作为改善的最低变化量。这个变量没有标准值。为了决定`min_delta`的值，运行几个epoch并观察错误和验证精度的变化。根据变化率定义`min_delta`。默认值0在很多情况下都相当有效。'
- en: '`patience`--This variable tells the algorithm how many epochs it should wait
    before stopping the training if the error does not improve. For example, if we
    set `patience` equal to 1, the training will stop at the epoch where the error
    increases. We must be a little flexible, though, because it is very common for
    the error to oscillate a little and continue improving. We can stop the training
    if it hasn’t improved in the last 10 or 20 epochs.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patience`--这个变量告诉算法在错误没有改善的情况下应该等待多少个epoch后停止训练。例如，如果我们把`patience`设置为1，训练将在错误增加的epoch时停止。不过，我们必须要有一点灵活性，因为错误通常会稍微波动一下然后继续改善。如果我们发现训练在最后10个或20个epoch内没有改善，我们可以停止训练。'
- en: TIP The good thing about early stopping is that it allows you to worry less
    about the epochs hyperparameter. You can set a high number of epochs and let the
    stopping algorithm take care of stopping the training when error stops improving.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 提前停止的好处是它让你不必太担心epoch超参数。你可以设置一个较高的epoch数量，并让停止算法在错误停止改善时负责停止训练。
- en: 4.8 Regularization techniques to avoid overfitting
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 避免过拟合的正则化技术
- en: 'If you observe that your neural network is overfitting the training data, your
    network might be too complex and need to be simplified. One of the first techniques
    you should try is regularization. In this section, we will discuss three of the
    most common regularization techniques: L2, dropout, and data augmentation.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察到你的神经网络正在过拟合训练数据，那么你的网络可能过于复杂，需要简化。你应该尝试的第一个技术之一就是正则化。在本节中，我们将讨论三种最常见的正则化技术：L2、dropout和数据增强。
- en: '![](../Images/4-26.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-26.png)'
- en: Figure 4.26 Improving `train_error` while not improving `val_error` means the
    network is starting to overfit.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.26 在不提高`val_error`的情况下提高`train_error`意味着网络开始过拟合。
- en: 4.8.1 L2 regularization
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1 L2正则化
- en: The basic idea of L2 regularization is that it penalizes the error function
    by adding a regularization term to it. This, in turn, reduces the weight values
    of the hidden units and makes them too small, very close to zero, to help simplify
    the model.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化的基本思想是在误差函数中添加一个正则化项，从而惩罚误差函数。这反过来又降低了隐藏单元的权重值，使它们变得非常小，接近于零，以帮助简化模型。
- en: 'Let’s see how regularization works. First, we update the error function by
    adding the regularization term:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看正则化是如何工作的。首先，我们通过添加正则化项来更新误差函数：
- en: error function*[new]* = error function*[old]* + regularization term
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数*[new]* = 误差函数*[old]* + 正则化项
- en: Note that you can use any of the error functions explained in chapter 2, like
    MSE or cross entropy. Now, let’s take a look at the regularization term
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可以使用第二章中解释的任何误差函数，如MSE或交叉熵。现在，让我们看看正则化项
- en: '*L2 regularization term = λ/2m * Σ* || *w* ||*²*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*L2正则化项 = λ/2m * Σ* || *w* ||*²*'
- en: 'where lambda (λ) is the regularization parameter, *m* is the number of instances,
    and w is the weight. The updated error function looks like this:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中lambda (λ)是正则化参数，*m*是实例数量，w是权重。更新的误差函数看起来像这样：
- en: '*error function* *[new]* *= error function* *[old]**+ λ/2m * Σ* || *w* ||*²*'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*误差函数* *[new]* *= 误差函数* *[old]* + λ/2m * Σ* || *w* ||*²*'
- en: 'Why does L2 regularization reduce overfitting? Well, let’s look at how the
    weights are updated during the backpropagation process. We learned from chapter
    2 that the optimizer calculates the derivative of the error, multiplies it by
    the learning rate, and subtracts this value from the old weight. Here is the backpropagation
    equation that updates the weights:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么L2正则化可以减少过拟合？好吧，让我们看看在反向传播过程中权重是如何更新的。我们从第二章中了解到，优化器计算误差的导数，将其乘以学习率，然后从旧权重中减去这个值。以下是更新权重的反向传播方程：
- en: '![](../Images/4-unnumb-4.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-unnumb-4.png)'
- en: Since we add the regularization term to the error function, the new error becomes
    larger than the old error. This means its derivative (∂Error/∂Wx) is also bigger,
    leading to a smaller Wnew . L2 regularization is also known as weight decay, as
    it forces the weights to decay toward zero (but not exactly zero).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们向误差函数中添加了正则化项，新的误差比旧的误差大。这意味着它的导数（∂Error/∂Wx）也更大，导致Wnew更小。L2正则化也称为权重衰减，因为它迫使权重向零衰减（但不是正好为零）。
- en: Reducing weights leads to a simpler neural network
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 减少权重导致更简单的神经网络
- en: 'To see how this works, consider: if the regularization term is so large that,
    when multiplied with the learning rate, it will be equal to Wold, then this will
    make the new weight equal to zero. This cancels the effect of this neuron, leading
    to a simpler neural network with fewer neurons.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这是如何工作的，考虑以下情况：如果正则化项非常大，以至于当乘以学习率时，它将等于Wold，那么这将使新权重等于零。这取消了该神经元的效应，导致具有更少神经元的更简单的神经网络。
- en: In practice, L2 regularization does not make the weights equal to zero. It just
    makes them smaller to reduce their effect. A large regularization parameter (`ƛ`)
    lead to negligible weights. When the weights are negligible, the model will not
    learn much from these units. This will make the network simpler and thus reduce
    overfitting
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，L2正则化不会使权重等于零。它只是使它们变得更小以减少它们的影响。大的正则化参数（`ƛ`）会导致权重可忽略。当权重可忽略时，模型将不会从这些单元中学到很多。这将使网络更简单，从而减少过拟合
- en: '![](../Images/4-unnumb-5K.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-unnumb-5K.png)'
- en: L2 regularization reduces the weights and simplifies the network to reduce overfitting.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化减少了权重并简化了网络以减少过拟合。
- en: 'This is what L2 regularization looks like in Keras:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是L2正则化在Keras中的样子：
- en: '[PRE12]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ When adding a hidden layer to your network, add the kernel_regularization
    argument with the L2 regularizer
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当向你的网络添加隐藏层时，添加带有L2正则化的kernel_regularization参数
- en: The lambda value is a hyperparameter that you can tune. The default value of
    your DL library usually works well. If you still see signs of overfitting, increase
    the lambda hyperparameter to reduce the model complexity.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda值是一个你可以调整的超参数。你的深度学习库的默认值通常效果很好。如果你仍然看到过拟合的迹象，增加lambda超参数以降低模型复杂性。
- en: 4.8.2 Dropout layers
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2 Dropout层
- en: 'Dropout is another regularization technique that is very effective for simplifying
    a neural network and avoiding overfitting. We discussed dropout extensively in
    chapter 3\. The dropout algorithm is fairly simple: at every training iteration,
    every neuron has a probability p of being temporarily ignored (dropped out) during
    this training iteration. This means it may be active during subsequent iterations.
    While it is counterintuitive to intentionally pause the learning on some of the
    network neurons, it is quite surprising how well this technique works. The probability
    p is a hyperparameter that is called dropout rate and is typically set in the
    range of 0.3 to 0.5\. Start with 0.3, and if you see signs of overfitting, increase
    the rate.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种非常有效的正则化技术，可以简化神经网络并避免过拟合。我们在第3章中详细讨论了dropout。dropout算法相当简单：在每次训练迭代中，每个神经元都有概率p在这次训练迭代中被临时忽略（dropout）。这意味着它可能在后续迭代中是活跃的。虽然故意暂停网络中某些神经元的训练看起来有些反直觉，但这个技术效果相当惊人。概率p是一个超参数，被称为dropout率，通常设置在0.3到0.5之间。从0.3开始，如果你看到过拟合的迹象，就增加这个比率。
- en: TIP I like to think of dropout as tossing a coin every morning with your team
    to decide who will do a specific critical task. After a few iterations, all your
    team members will learn how to do this task and not rely on a single member to
    get it done. The team would become much more resilient to change.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 我喜欢把dropout想象成每天早上和你的团队扔硬币来决定谁将执行一个特定的关键任务。经过几次迭代后，所有团队成员都会学会如何完成这项任务，而不会依赖于单个成员来完成。这样，团队对变化的抵抗力会大大增强。
- en: Both L2 regularization and dropout aim to reduce network complexity by reducing
    its neurons’ effectiveness. The difference is that dropout completely cancels
    the effect of some neurons with every iteration, while L2 regularization just
    reduces the weight values to reduce the neurons’ effectiveness. Both lead to a
    more robust, resilient neural network and reduce overfitting. It is recommended
    that you use both types of regularization techniques in your network.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化和dropout都旨在通过减少神经元的效率来降低网络复杂性。区别在于dropout在每次迭代中完全取消一些神经元的效应，而L2正则化只是减少权重值来降低神经元的效率。两者都导致一个更鲁棒、更有弹性的神经网络并减少过拟合。建议你在网络中使用这两种正则化技术。
- en: 4.8.3 Data augmentation
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.3 数据增强
- en: One way to avoid overfitting is to obtain more data. Since this is not always
    a feasible option, we can augment our training data by generating new instances
    of the same images with some transformations. Data augmentation can be an inexpensive
    way to give your learning algorithm more training data and therefore reduce overfitting.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合的一种方法是通过获取更多数据。由于这并不总是可行的选择，我们可以通过生成一些变换后的相同图像的新实例来增强我们的训练数据。数据增强可以是一种经济实惠的方法，为你的学习算法提供更多训练数据，从而减少过拟合。
- en: The many image-augmentation techniques include flipping, rotation, scaling,
    zooming, lighting conditions, and many other transformations that you can apply
    to your dataset to provide a variety of images to train on. In figure 4.27, you
    can see some of the transformation techniques applied to an image of the digit
    6.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 许多图像增强技术包括翻转、旋转、缩放、缩放、光照条件以及你可以应用于数据集的许多其他变换，以提供各种图像进行训练。在图4.27中，你可以看到应用于数字6图像的一些变换技术。
- en: '![](../Images/4-27.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图4-27](../Images/4-27.png)'
- en: Figure 4.27 Various image augmentation techniques applied to an image of the
    digit 6
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.27应用于数字6图像的各种图像增强技术
- en: In figure 4.27, we created 20 new images that the network can learn from. The
    main advantage of synthesizing images like this is that now you have more data
    (20×) that tells your algorithm that if an image is the digit 6, then even if
    you flip it vertically or horizontally or rotate it, it’s still the digit 6\.
    This makes the model more robust to detect the number 6 in any form and shape.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.27中，我们创建了20张新的图像，网络可以从这些图像中学习。这种合成图像的主要优势是现在你有更多的数据（20倍）来告诉你的算法，如果一个图像是数字6，那么即使你垂直或水平翻转它或旋转它，它仍然是数字6。这使得模型对检测任何形式和形状的数字6更加鲁棒。
- en: Data augmentation is considered a regularization technique because allowing
    the network to see many variants of the object reduces its dependence on the original
    form of the object during feature learning. This makes the network more resilient
    when tested on new data.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强被认为是一种正则化技术，因为允许网络看到对象的许多变体可以减少它在特征学习过程中对对象原始形式的依赖。这使得网络在测试新数据时更加鲁棒。
- en: 'Data augmentation in Keras looks like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的数据增强看起来是这样的：
- en: '[PRE13]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Imports ImageDataGenerator from Keras
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从Keras导入ImageDataGenerator
- en: ❷ Generates batches of new image data. ImageDataGenerator takes transformation
    types as arguments. Here, we set horizontal and vertical flip to True. See the
    Keras documentation (or your DL library) for more transformation arguments.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成新的图像数据批次。ImageDataGenerator接受变换类型作为参数。在这里，我们将水平和垂直翻转设置为True。有关更多变换参数，请参阅Keras文档（或您的深度学习库）。
- en: ❸ Computes the data augmentation on the training set
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练集上计算数据增强
- en: 4.9 Batch normalization
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.9 批归一化
- en: Earlier in this chapter, we talked about data normalization to speed up learning.
    The normalization techniques we discussed were focused on preprocessing the training
    set before feeding it to the input layer. If the input layer benefits from normalization,
    why not do the same thing for the extracted features in the hidden units, which
    are changing all the time and get much more improvement in training speed and
    network resilience (figure 4.28)? This process is called batch normalization (BN).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们讨论了数据归一化以加快学习速度。我们讨论的归一化技术集中在在输入层之前对训练集进行预处理。如果输入层从归一化中受益，为什么不对隐藏单元中提取的特征做同样的事情呢？这些特征一直在变化，并且可以在训练速度和网络鲁棒性（图4.28）上获得更多的改进。这个过程被称为批归一化（BN）。
- en: '![](../Images/4-28.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-28.png)'
- en: Figure 4.28 Batch normalization is normalizing the extracted features in hidden
    units.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.28 批归一化是对隐藏单元中提取的特征进行归一化。
- en: 4.9.1 The covariate shift problem
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.1 协变量偏移问题
- en: Before we define covariate shift, let’s take a look at an example to illustrate
    the problem that batch normalization (BN) confronts. Suppose you are building
    a cat classifier, and you train your algorithm on images of white cats only. When
    you test this classifier on images with cats that are different colors, it will
    not perform well. Why? Because the model has been trained on a training set with
    a specific distribution (white cats). When the distribution changes in the test
    set, it confuses the model (figure 4.29).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义协变量偏移之前，让我们通过一个例子来说明批归一化（BN）面临的问题。假设你正在构建一个猫分类器，并且只使用白色猫的图像来训练你的算法。当你用不同颜色的猫的图像测试这个分类器时，它将不会表现良好。为什么？因为模型是在具有特定分布（白色猫）的训练集上训练的。当测试集中的分布发生变化时，它会混淆模型（图4.29）。
- en: '![](../Images/4-29.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-29.png)'
- en: Figure 4.29 Graph A is the training set of only white cats, and graph B is the
    testing set with cats of various colors. The circles represent the cat images,
    and the stars represent the non-cat images.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.29 图A是仅包含白色猫的训练集，图B是包含各种颜色猫的测试集。圆圈代表猫的图像，星星代表非猫图像。
- en: We should not expect that the model trained on the data in graph A will do very
    well with the new distribution in graph B. The idea of the change in data distribution
    goes by the fancy name covariate shift.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该期望在图A中的数据上训练的模型会在图B中的新分布上表现良好。数据分布变化的概念被称为协变量偏移。
- en: DEFINITION If a model is learning to map dataset *x* to label y, then if the
    distribution of *x* changes, it’s known as covariate shift. When that happens,
    you might need to retrain your learning algorithm.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：如果一个模型正在学习将数据集 *x* 映射到标签 y，那么如果 *x* 的分布发生变化，就称为协变量偏移。当这种情况发生时，你可能需要重新训练你的学习算法。
- en: 4.9.2 Covariate shift in neural networks
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.2 神经网络中的协变量偏移
- en: 'To understand how covariate shift happens in neural networks, consider the
    simple four-layer MLP in figure 4.30\. Let’s look at the network from the third-layer
    (L3) perspective. Its input are the activation values in L2 (a 12, a 22, a 32,
    and a 42), which are the features extracted from the previous layers. L3 is trying
    to map these inputs to *ŷ* to make it as close as possible to the label y. While
    the third layer is doing that, the network is adapting the values of the parameters
    from previous layers. As the parameters (w, b) are changing in layer 1, the activation
    values in the second layer are changing, too. So from the perspective of the third
    hidden layer, the values of the second hidden layer are changing all the time:
    the MLP is suffering from the problem of covariate shift. Batch norm reduces the
    degree of change in the distribution of the hidden unit values, causing these
    values to become more stable so that the later layers of the neural network have
    firmer ground to stand on.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解神经网络中协变量偏移是如何发生的，可以考虑图 4.30 中的简单四层 MLP。让我们从第三层（L3）的角度来看这个网络。它的输入是 L2 层的激活值（a12、a22、a32
    和 a42），这些是从前一层提取的特征。L3 正在尝试将这些输入映射到 *ŷ*，使其尽可能接近标签 y。当第三层这样做的时候，网络正在调整前一层参数的值。随着参数（w、b）在第一层中变化，第二层的激活值也在变化。因此，从第三隐藏层的角度来看，第二隐藏层的值一直在变化：MLP
    正在遭受协变量偏移的问题。批归一化减少了隐藏单元值分布的变化程度，使得这些值变得更加稳定，这样神经网络的后续层就有更坚实的基础。
- en: '![](../Images/4-30.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-30.png)'
- en: Figure 4.30 A simple four-layer MLP. L1 features are input to the L2 layer.
    The same is true for layers 2, 3, and 4.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.30 一个简单的四层 MLP。L1 特征输入到 L2 层。对于 2、3 和 4 层也是如此。
- en: 'NOTE It is important to realize that batch normalization does not cancel or
    reduce the change in the hidden unit values. What it does is ensure that the distribution
    of that change remains the same: even if the exact values of the units change,
    the mean and variance do not change.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：重要的是要认识到批归一化不会取消或减少隐藏单元值的变化。它所做的是确保这种变化分布保持不变：即使单元的确切值发生变化，均值和方差不会变化。
- en: 4.9.3 How does batch normalization work?
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.3 批归一化是如何工作的？
- en: 'In their 2015 paper “Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift” ([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)),
    Sergey Ioffe and Christian Szegedy proposed the BN technique to reduce covariate
    shift. Batch normalization adds an operation in the neural network just before
    the activation function of each layer to do the following:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '在他们 2015 年的论文“Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift”([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167))中，Sergey
    Ioffe 和 Christian Szegedy 提出了 BN 技术，以减少协变量偏移。批归一化在神经网络中每个层的激活函数之前添加了一个操作，以执行以下操作：'
- en: Zero-center the inputs
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零中心化输入
- en: Normalize the zero-centered inputs
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化零中心化输入
- en: Scale and shift the results
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放和移位结果
- en: This operation lets the model learn the optimal scale and mean of the inputs
    for each layer.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作让模型学习每个层的输入的最佳尺度和均值。
- en: How the math works in batch normalization
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化中的数学原理
- en: 'To zero-center the inputs, the algorithm needs to calculate the input mean
    and standard deviation (the input here means the current mini-batch: hence the
    term batch normalization):'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将输入零中心化，算法需要计算输入均值和标准差（这里的输入指的是当前的 mini-batch：因此有批归一化的说法）：
- en: '![](../Images/4-30_E1.png)  ❶'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](../Images/4-30_E1.png)  ❶'
- en: '![](../Images/4-30_E2.png)  ❷'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](../Images/4-30_E2.png)  ❷'
- en: ❶ Mini-batch mean
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 小批量均值
- en: ❷ Mini-batch variance
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 小批量方差
- en: where *m* is the number of instances in the mini-batch, *μ*[B] is the mean,
    and *σ*[B] is the standard deviation over the current mini-batch.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *m* 是 mini-batch 中的实例数量，*μ*[B] 是当前 mini-batch 的均值，*σ*[B] 是标准差。
- en: 'Normalize the input:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化输入：
- en: '![](../Images/4-30_E3.png)'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/4-30_E3.png)'
- en: where *x̂* is the zero-centered and normalized input. Note that there is a variable
    here that we added (ε). This is a tiny number (typically 10-5) to avoid division
    by zero if σ is zero in some estimates.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *x̂* 是零均值和归一化的输入。注意这里我们添加了一个变量（ε）。这是一个很小的数（通常是 10^-5），以避免在 σ 在某些估计中为零时发生除以零的情况。
- en: Scale and shift the results. We multiply the normalized output by a variable
    *γ* to scale it and add (β ) to shift it
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放和移位结果。我们将归一化输出乘以变量 *γ* 以缩放它，并加上 (β) 以移位它
- en: '*y[i] ← γ X[i] + β*'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*y[i] ← γ X[i] + β*'
- en: where *y[i]* is the output of the BN operation, scaled and shifted.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*y[i]* 是 BN 操作的输出，经过缩放和移位。'
- en: 'Notice that BN introduces two new learnable parameters to the network: *γ*
    and β . So our optimization algorithm will update the parameters of *γ* and β
    just like it updates weights and biases. In practice, this means you may find
    that training is rather slow at first, while GD is searching for the optimal scales
    and offsets for each layer, but it accelerates once it’s found reasonably good
    values.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，BN向网络引入了两个新的可学习参数：*γ*和β。因此，我们的优化算法将像更新权重和偏差一样更新*γ*和β的参数。在实践中，这意味着你可能会发现训练最初相当缓慢，因为GD正在寻找每个层的最佳缩放和偏移量，但一旦找到了合理的值，它就会加速。
- en: '[i]'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[i]'
- en: 4.9.4 Batch normalization implementation in Keras
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.4 Keras中的批归一化实现
- en: It is important to know how batch normalization works so you can get a better
    understanding of what your code is doing. But when using BN in your network, you
    don’t have to implement all these details yourself. Implementing BN is often done
    by adding one line of code, using any DL framework. In Keras, the way you add
    batch normalization to your neural network is by adding a BN layer after the hidden
    layer, to normalize its results before they are fed to the next layer.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 了解批归一化的工作原理非常重要，这样你可以更好地理解你的代码正在做什么。但是，当你在网络中使用BN时，你不必自己实现所有这些细节。实现BN通常只需添加一行代码，使用任何深度学习框架。在Keras中，你通过在隐藏层之后添加一个BN层来将批归一化添加到你的神经网络中，以便在将结果馈送到下一层之前对其进行归一化。
- en: 'The following code snippet shows you how to add a BN layer when building your
    neural network:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在构建神经网络时添加BN层：
- en: '[PRE14]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Imports the BatchNormalization layer from the Keras library
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从Keras库中导入BatchNormalization层
- en: ❷ Initiates the model
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化模型
- en: ❸ Adds the first hidden layer
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加第一个隐藏层
- en: ❹ Adds the batch norm layer to normalize the results of layer 1
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将批归一化层添加到第1层的输出结果中进行归一化
- en: ❺ If you are adding dropout to your network, it is preferable to add it after
    the batch norm layer because you don’t want the nodes that are randomly turned
    off to miss the normalization step.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果你正在向你的网络添加dropout，最好在批归一化层之后添加它，因为你不希望随机关闭的节点错过归一化步骤。
- en: ❻ Adds the second hidden layer
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 添加第二个隐藏层
- en: ❼ Adds the batch norm layer to normalize the results of layer 2
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将批归一化层添加到第2层的输出结果中进行归一化
- en: ❽ Output layer
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 输出层
- en: 4.9.5 Batch normalization recap
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.5 批归一化回顾
- en: The intuition that I hope you’ll take away from this discussion is that BN applies
    the normalization process not just to the input layer, but also to the values
    in the hidden layers in a neural network. This weakens the coupling of the learning
    process between earlier and later layers, allowing each layer of the network to
    learn more independently.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能从这次讨论中得到的直觉是，BN不仅应用于输入层，还应用于神经网络中隐藏层的值。这减弱了早期和后期层之间学习过程的耦合，允许网络的每一层更独立地学习。
- en: From the perspective of the later layers in the network, the earlier layers
    don’t get to shift around as much because they are constrained to have the same
    mean and variance. This makes the job of learning easier in the later layers.
    The way this happens is by ensuring that the hidden units have a standardized
    distribution (mean and variance) controlled by two explicit parameters, *γ* and
    β , which the learning algorithm sets during training.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络中较后层的角度来看，较前层的节点不能移动太多，因为它们被限制为具有相同的均值和方差。这使得在较后层的学习工作更容易。这种情况的发生是通过确保隐藏单元具有由两个显式参数*γ*和β控制的标准化分布（均值和方差），学习算法在训练期间设置这些参数。
- en: '4.10 Project: Achieve high accuracy on image classification'
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.10 项目：在图像分类中实现高准确率
- en: In this project, we will revisit the CIFAR-10 classification project from chapter
    3 and apply some of the improvement techniques from this chapter to increase the
    accuracy from ~65% to ~90%. You can follow along with this example by visiting
    the book’s website, [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www .computervisionbook.com](http://www.computervisionbook.com), to see the
    code notebook.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将回顾第3章中的CIFAR-10分类项目，并将本章的一些改进技术应用于提高准确率，从约65%提高到约90%。你可以通过访问书籍的网站[www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)或[www.computervisionbook.com](http://www.computervisionbook.com)来跟随这个示例，查看代码笔记本。
- en: 'We will accomplish the project by following these steps:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤完成项目：
- en: Import the dependencies.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入依赖项。
- en: 'Get the data ready for training:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练数据：
- en: Download the data from the Keras library.
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Keras库中下载数据
- en: Split it into train, validate, and test datasets.
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其分割为训练、验证和测试数据集。
- en: Normalize the data.
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化数据。
- en: One-hot encode the labels.
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对标签进行独热编码。
- en: 'Build the model architecture. In addition to regular convolutional and pooling
    layers, as in chapter 3, we add the following layers to our architecture:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型架构。除了第3章中提到的常规卷积和池化层外，我们还将以下层添加到我们的架构中：
- en: Deeper neural network to increase learning capacity
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更深的神经网络以增加学习容量
- en: Dropout layers
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout层
- en: L2 regularization to our convolutional layers
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对卷积层应用L2正则化
- en: Batch normalization layers
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化层
- en: Train the model.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate the model.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。
- en: Plot the learning curve.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制学习曲线。
- en: Let’s see how this is implemented.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何实现的。
- en: 'Step 1: Import dependencies'
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：导入依赖项
- en: 'Here’s the Keras code to import the needed dependencies:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这是导入所需依赖项的Keras代码：
- en: '[PRE15]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Keras library to download the datasets, preprocess images, and network components
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用Keras库下载数据集，预处理图像和网络组件
- en: ❷ Imports numpy for math operations
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入numpy进行数学运算
- en: ❸ Imports the matplotlib library to visualize results
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 导入matplotlib库以可视化结果
- en: 'Step 2: Get the data ready for training'
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：准备训练数据
- en: Keras has some datasets available for us to download and experiment with. These
    datasets are usually preprocessed and almost ready to be fed to the neural network.
    In this project, we use the CIFAR-10 dataset, which consists of 50,000 32 × 32
    color training images, labeled over 10 categories, and 10,000 test images. Check
    the Keras documentation for more datasets like CIFAR-100, MNIST, Fashion-MNIST,
    and more.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Keras为我们提供了可下载和实验的一些数据集。这些数据集通常是预处理的，几乎可以直接输入到神经网络中。在本项目中，我们使用的是CIFAR-10数据集，它包含50,000张32
    × 32彩色训练图像，分为10个类别，以及10,000张测试图像。有关CIFAR-100、MNIST、Fashion-MNIST等更多数据集，请查看Keras文档。
- en: 'Keras provides the CIFAR-10 dataset already split into training and testing
    sets. We will load them and then split the training dataset into 45,000 images
    for training and 5,000 images for validation, as explained in this chapter:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: Keras已经将CIFAR-10数据集分割成训练集和测试集。我们将加载它们，然后将训练数据集分割成45,000张图像用于训练和5,000张图像用于验证，如本章所述：
- en: '[PRE16]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Downloads and splits the data
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载并分割数据
- en: ❷ Breaks the training set into training and validation sets
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将训练集分割为训练集和验证集
- en: 'Let’s print the shape of `x_train`, `x_valid`, and `x_test`:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印`x_train`、`x_valid`和`x_test`的形状：
- en: '[PRE17]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The format of the shape tuple is as follows: (number of instances, width, height,
    channels).'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 形状元组的格式如下：（实例数量，宽度，高度，通道）。
- en: Normalize the data
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 归一化数据
- en: 'Normalizing the pixel values of our images is done by subtracting the mean
    from each pixel and then dividing the result by the standard deviation:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从每个像素中减去平均值然后除以标准差来对图像的像素值进行归一化：
- en: '[PRE18]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: One-hot encode the labels
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对标签进行独热编码
- en: 'To one-hot encode the labels in the train, valid, and test datasets, we use
    the `to_` `categorical` function in Keras:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练、验证和测试数据集中对标签进行独热编码，我们使用Keras中的`to_categorical`函数：
- en: '[PRE19]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Data augmentation
  id: totrans-491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'For augmentation techniques, we will arbitrarily go with the following transformations:
    rotation, width and height shift, and horizontal flip. When you are working on
    problems, view the images that the network missed or provided poor detections
    for and try to understand why it is not performing well on them. Then create your
    hypothesis and experiment with it. For example, if the missed images were of shapes
    that are rotated, you might want to try the rotation augmentation. You would apply
    that, experiment, evaluate, and repeat. You will come to your decisions purely
    from analyzing your data and understanding the network performance:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 对于增强技术，我们将任意选择以下转换：旋转、宽度和高度偏移以及水平翻转。当你处理问题时，查看网络未能识别或提供较差检测的图像，并尝试理解为什么它在这些图像上表现不佳。然后创建你的假设并对其进行实验。例如，如果遗漏的图像是旋转的形状，你可能想尝试旋转增强。你会应用它，实验，评估，并重复。你将完全通过分析你的数据和了解网络性能来做出决定：
- en: '[PRE20]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Data augmentation
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据增强
- en: ❷ Computes the data augmentation on the training set
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练集上计算数据增强
- en: 'Step 3: Build the model architecture'
  id: totrans-496
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：构建模型架构
- en: In chapter 3, we built an architecture inspired by AlexNet (3 CONV + 2 FC).
    In this project, we will build a deeper network for increased learning capacity
    (6 CONV + 1 FC).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们构建了一个受AlexNet启发的架构（3个卷积层 + 2个全连接层）。在本项目中，我们将构建一个更深层的网络以增加学习容量（6个卷积层
    + 1个全连接层）。
- en: 'The network has the following configuration:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 网络具有以下配置：
- en: Instead of adding a pooling layer after each convolutional layer, we will add
    one after every two convolutional layers. This idea was inspired by VGGNet, a
    popular neural network architecture developed by the Visual Geometry Group (University
    of Oxford). VGGNet will be explained in chapter 5.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不会在每个卷积层之后添加池化层，而是每两个卷积层之后添加一个。这个想法是受VGGNet的启发，VGGNet是由牛津大学视觉几何组开发的一个流行的神经网络架构。VGGNet将在第5章中解释。
- en: Inspired by VGGNet, we will set the `kernel_size` of our convolutional layers
    to 3 × 3 and the `pool_size` of the pooling layer to 2 × 2.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受VGGNet的启发，我们将卷积层的`kernel_size`设置为3 × 3，并将池化层的`pool_size`设置为2 × 2。
- en: We will add dropout layers every other convolutional layer, with (*p*) ranges
    from 0.2 and 0.4.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在每隔一个卷积层后添加Dropout层，(*p*)的范围从0.2到0.4。
- en: A batch normalization layer will be added after each convolutional layer to
    normalize the input for the following layer.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个卷积层之后将添加一个批量归一化层，以归一化下一层的输入。
- en: In Keras, L2 regularization is added to the convolutional layer code.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中，L2正则化被添加到卷积层代码中。
- en: 'Here’s the code:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE21]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Number of hidden units variable. We declare this variable here and use it
    in our convolutional layers to make it easier to update from one place.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 隐藏单元数量变量。我们在这里声明这个变量，并在我们的卷积层中使用它，以便更容易从一处更新。
- en: ❷ L2 regularization hyperparameter (`ƛ`)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ L2正则化超参数（`ƛ`）
- en: ❸ Creates a sequential model (a linear stack of layers)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个序列模型（层线性堆叠）
- en: ❹ Notice that we define the input_shape here because this is the first convolutional
    layer. We don’t need to do that for the remaining layers.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 注意，我们在这里定义了input_shape，因为这是第一个卷积层。对于剩余的层，我们不需要这样做。
- en: ❺ Adds L2 regularization to the convolutional layer
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在卷积层中添加L2正则化
- en: ❻ Uses a ReLU activation function for all hidden layers
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 所有隐藏层使用ReLU激活函数
- en: ❼ Adds a batch normalization layer
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 添加一个批量归一化层
- en: ❽ Dropout layer with 20% probability
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 20%概率的Dropout层
- en: ❾ Number of hidden units = 64
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 隐藏单元数量 = 64
- en: ❿ Flattens the feature map into a 1D features vector (explained in chapter 3)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 将特征图展平为1D特征向量（在第3章中解释）
- en: ⓫ 10 hidden units because the dataset has 10 class labels. Softmax activation
    function is used for the output layer (explained in chapter 2)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 10个隐藏单元，因为数据集有10个类别标签。输出层使用Softmax激活函数（在第2章中解释）
- en: ⓬ Prints the model summary
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 打印模型摘要
- en: The model summary is shown in figure 4.31.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如图4.31所示。
- en: 'Step 4: Train the model'
  id: totrans-519
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：训练模型
- en: 'Before we jump into the training code, let’s discuss the strategy behind some
    of the hyperparameter settings:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入训练代码之前，让我们讨论一下一些超参数设置背后的策略：
- en: '`batch_size`--This is the mini-batch hyperparameter that we covered in this
    chapter. The higher the `batch_size`, the faster your algorithm learns. You can
    start with a mini-batch of 64 and double this value to speed up training. I tried
    256 on my machine and got the following error, which means my machine was running
    out of memory. I then lowered it back to 128:'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`--这是本章中我们讨论的迷你批大小超参数。`batch_size`越高，你的算法学习得越快。你可以从64个迷你批开始，并将此值加倍以加快训练速度。我在我的机器上尝试了256，得到了以下错误，这意味着我的机器内存不足。然后我将它降低到128：'
- en: '[PRE22]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`epochs`--I started with 50 training iterations and found that the network
    was still improving. So I kept adding more epochs and observing the training results.
    In this project, I was able to achieve >90% accuracy after 125 epochs. As you
    will see soon, there is still room for improvement if you let it train longer.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`--我开始使用50次训练迭代，发现网络仍在改进。所以我继续添加更多的epochs并观察训练结果。在这个项目中，我在125个epochs后达到了>90%的准确率。正如你很快就会看到的，如果你让它训练更长的时间，仍然有改进的空间。'
- en: '![](../Images/4-31.png)'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/4-31.png)'
- en: Figure 4.31 Model summary
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.31 模型摘要
- en: Optimizer --I used the Adam optimizer. See section 4.7 to learn more about optimization
    algorithms.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器--我使用了Adam优化器。参见第4.7节了解更多关于优化算法的信息。
- en: NOTE It is important to note that I’m using a GPU for this experiment. The training
    took around 3 hours. It is recommended that you use your own GPU or any cloud
    computing service to get the best results. If you don’t have access to a GPU,
    I recommend that you try a smaller number of epochs or plan to leave your machine
    training overnight or even for a couple of days, depending on your CPU specifications.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请注意，我在这个实验中使用了GPU。训练大约花费了3个小时。建议您使用自己的GPU或任何云计算服务以获得最佳结果。如果您无法访问GPU，我建议您尝试更少的epoch数量，或者计划让您的机器在夜间或甚至几天内进行训练，具体取决于您的CPU规格。
- en: 'Let’s see the training code:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练代码：
- en: '[PRE23]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Mini-batch size
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 小批量大小
- en: ❷ Number of training iterations
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练迭代次数
- en: ❸ Path f the file where the best weights will be saved, and a Boolean True to
    save the weights only when there is an improvement
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定保存最佳权重的文件路径，以及一个布尔值True，表示只有当权重有所改进时才保存权重。
- en: ❹ Adam optimizer with a learning rate = 0.0001
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 学习率=0.0001的Adam优化器
- en: ❺ Cross-entropy loss function (explained in chapter 2)
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 交叉熵损失函数（在第2章中解释）
- en: ❻ Allows you to do real-time data augmentation on images on CPU in parallel
    to training your model on GPU. The callback to the checkpointer saves the model
    weights; you can add other callbacks like an early stopping function.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 允许您在CPU上并行对图像进行实时数据增强，同时训练您的模型在GPU上。检查点回调保存模型权重；您可以添加其他回调，如早停功能。
- en: When you run this code, you will see the verbose output of the network training
    for each epoch. Keep your eyes on the `loss` and `val_loss` values to analyze
    the network and diagnose bottlenecks. Figure 4.32 shows the verbose output of
    epochs 121 to 125.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，您将看到每个epoch的网络训练的详细输出。关注`loss`和`val_loss`值以分析网络并诊断瓶颈。图4.32显示了第121至125个epoch的详细输出。
- en: '![](../Images/4-32.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-32.png)'
- en: Figure 4.32 Verbose output of epochs 121 to 125
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.32 第121至125个epoch的详细输出
- en: 'Step 5: Evaluate the model'
  id: totrans-539
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：评估模型
- en: 'To evaluate the model, we use a Keras function called `evaluate` and print
    the results:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型，我们使用Keras函数`evaluate`并打印结果：
- en: '[PRE24]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Plot learning curves
  id: totrans-542
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 绘制学习曲线
- en: 'Plot the learning curves to analyze the training performance and diagnose overfitting
    and underfitting (figure 4.33):'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制学习曲线以分析训练性能并诊断过拟合和欠拟合（图4.33）：
- en: '[PRE25]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/4-33.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-33.png)'
- en: Figure 4.33 Learning curves
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.33 学习曲线
- en: Further improvements
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 进一步改进
- en: 'Accuracy of 90% is pretty good, but you can still improve further. Here are
    some ideas you can experiment with:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 90%的准确率相当不错，但您仍然可以进一步提高。以下是一些您可以尝试的想法：
- en: More training epochs --Notice that the network was improving until epoch 123\.
    You can increase the number of epochs to 150 or 200 and let the network train
    longer.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多训练epoch--请注意，网络直到第123个epoch都在改进。您可以增加epoch数量到150或200，让网络训练更长的时间。
- en: Deeper network --Try adding more layers to increase the model complexity, which
    increases the learning capacity.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度网络--尝试添加更多层以增加模型复杂度，这会增加学习容量。
- en: Lower learning rate --Decrease the lr (you should train longer if you do so).
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率--降低lr（如果您这样做，应该训练更长的时间）。
- en: Different CNN architecture --Try something like Inception or ResNet (explained
    in detail in the next chapter). You can get up to 95% accuracy with the ResNet
    neural network after 200 epochs of training.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的CNN架构--尝试使用Inception或ResNet（在下一章中详细解释）。经过200个epoch的训练后，ResNet神经网络可以达到高达95%的准确率。
- en: Transfer learning --In chapter 6, we will explore the technique of using a pretrained
    network on your dataset to get higher results with a fraction of the learning
    time.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习--在第6章中，我们将探讨使用预训练网络在您的数据集上以更少的学习时间获得更高结果的技术。
- en: Summary
  id: totrans-554
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The general rule of thumb is that the deeper your network is, the better it
    learns.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般规则是，您的网络越深，它学得越好。
- en: At the time of writing, ReLU performs best in hidden layers, and softmax performs
    best in the output layer.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在撰写本文时，ReLU在隐藏层中表现最佳，softmax在输出层中表现最佳。
- en: Stochastic gradient descent usually succeeds in finding a minimum. But if you
    need fast convergence and are training a complex neural network, it’s safe to
    go with Adam.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降通常能成功找到最小值。但如果你需要快速收敛并且正在训练一个复杂的神经网络，使用Adam是安全的。
- en: Usually, the more you train, the better.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，您训练得越多，效果越好。
- en: L2 regularization and dropout work well together to reduce network complexity
    and overfitting.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化和dropout结合使用可以很好地减少网络复杂性和过拟合。
