- en: '6 Sentiment classification: Large movie-review dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 情感分类：大型电影评论数据集
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using text and word frequency (Bag of Words) to represent sentiment
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文本和词频（词袋模型）来表示情感
- en: Building sentiment classifier using logistic regression and with softmax
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归和softmax构建情感分类器
- en: Measuring classification accuracy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量分类精度
- en: Computing ROC curve and measure classifier effectiveness
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算ROC曲线并衡量分类器的有效性
- en: Submitting your results to the Kaggle challenge for movie reviews
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的结果提交到Kaggle的电影评论挑战赛
- en: One of the magic uses of machine learning that impresses everyone nowadays is
    teaching the computer to learn from text. With social media, SMS text, Facebook
    Messenger, What’s App, Twitter, and other sources generating hundreds of billions
    of text messages a day, there is no shortage of text to learn from.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，机器学习的神奇用途之一，让每个人都印象深刻的是教会计算机从文本中学习。随着社交媒体、短信、Facebook Messenger、WhatsApp、Twitter和其他来源每天产生数百亿条文本消息，可供学习的文本资源是充足的。
- en: 'TIP Check out this famous infographic demonstrating the abundance of textual
    data arriving each day from various media platforms: [http://mng.bz/ yrXq](https://shortener.manning.com/yrXq).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 查看这个著名的信息图表，展示了每天从各种媒体平台到达的大量文本数据：[http://mng.bz/yrXq](https://shortener.manning.com/yrXq)。
- en: Social media companies, phone providers, and app makers are all trying to use
    the messages you send to make decisions and classify you. Have you ever sent your
    significant other an SMS text message about the Thai food you ate for lunch and
    then later saw ads on your social media pop up, recommending new Thai restaurants
    to visit? Scary as it seems that Big Brother is trying to identify and understand
    your food habits, online streaming service companies are also using practical
    applications to try to determine whether you enjoyed their products.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体公司、电话提供商和应用程序制造商都在尝试使用你发送的消息来做出决策和分类你。你是否曾给你的另一半发送了一条关于你午餐吃的泰国菜的短信，然后后来在你的社交媒体上看到了弹出广告，推荐你去新的泰国餐厅？虽然听起来很可怕，好像大哥在试图识别和理解你的饮食习惯，但在线流媒体服务公司也在使用实际应用来尝试确定你是否喜欢他们的产品。
- en: After watching a film, have you ever taken the time to issue a simple review,
    such as “Wow, that was a great movie! Loved Bill’s performance!” or “That movie
    was grossly inappropriate, was well over three hours long, and after first being
    disgusted by the gore, I fell asleep because there was no plot!” (Okay, admittedly,
    I may have written that last comment on some online platform.) YouTube is famous
    for other users coming not only to watch the videos and viral content, but also
    to engage in the act of reading the comments—looking at the written reviews of
    content for movies, videos, and other digital media. These reviews are simple
    in the sense that you can fire and forget a quick sentence or two, get your feelings
    out there, and move on with your life. Sometimes, the comments are hilarious,
    or angry, or extremely positive; ultimately, they run the gamut of emotions that
    online participants could experience while viewing the content.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在观看一部电影后，你是否曾花时间发表一个简单的评论，比如“哇，那是一部很棒的电影！我喜欢比尔的表现！”或者“那部电影非常不合适，超过了三个小时，一开始对血腥场面感到厌恶，后来因为没有任何情节而睡着了！”（好吧，诚实地讲，我可能在上一个在线平台上写了最后那条评论。）YouTube以其用户不仅来观看视频和病毒性内容，还参与阅读评论——查看电影、视频和其他数字媒体的内容的书面评论而闻名。这些评论很简单，因为你可以说一句话或两句话，表达你的感受，然后继续你的生活。有时，评论非常有趣，或愤怒，或极端积极；最终，它们涵盖了在线参与者观看内容时可能体验到的所有情绪范围。
- en: Those emotions are quite useful to online media service companies. Given an
    easy way to classify sentiment, the companies could determine whether a particular
    video of a celebrity generated extreme sadness or extremely positive responses.
    In turn, if the companies could first classify and then associate those emotions
    with what you did next—if, after watching a movie, you provided a few sentences
    of positive commentary and clicked a link to buy more movies starring the lead
    actor—they’d have the whole cause-and-effect pipeline. The media company could
    generate more of that content or show you more of the types of content that you
    are interested in. Doing so may generate increased revenue, such as if your positive
    reaction led you to purchase something about that celebrity afterward.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情感对在线媒体服务公司非常有用。如果有一个简单的分类情感的方法，公司可以确定某个名人视频是否引发了极端的悲伤或极端积极的反应。反过来，如果公司能够首先分类，然后将这些情感与你接下来所做的事情相关联——如果你在看完电影后提供了一些积极的评论并点击了一个链接购买主演的电影，那么他们就有了一个完整的因果关系流程。媒体公司可以生成更多这样的内容或向你展示更多你感兴趣的内容类型。这样做可能会增加收入，比如如果你的积极反应导致你在之后购买了与该名人相关的东西。
- en: 'You’re learning about a methodology for using machine learning to perform classification
    on input data and, by classifying it, generating some label for that input. Sentiment
    can be thought of in two ways: first as binary sentiment (such as positive/ negative
    reaction) and then as multiclass sentiment (such as hate, sad, neutral, like,
    or love). You learned the two following techniques to handle those cases, which
    you’ll try out in this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在学习一种使用机器学习对输入数据进行分类的方法，并通过分类为该输入生成一些标签。情感可以有两种思考方式：首先作为二元情感（如正面/负面反应），然后作为多类情感（如仇恨、悲伤、中性、喜欢或爱）。你学习了以下两种处理这些情况的技术，你将在本章中尝试这些技术：
- en: Logistic regression for the binary sentiment
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于二元情感的逻辑回归
- en: Softmax regression for multiclass classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于多类分类的Softmax回归
- en: The challenge with the input in this case is that it’s text, not some nice input
    vector of numbers like the randomly generated data points that our trusty NumPy
    library generated for us in chapter 5\. Luckily for you, the text and information
    retrieval community has developed a technique to handle mapping text to a numerical
    feature vector— perfect for machine learning. This technique is called the Bag
    of Words model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入的挑战在于它是文本，而不是像我们在第5章中由我们信任的NumPy库为我们生成的那些随机数据点那样的美好输入向量。幸运的是，文本和信息检索社区已经开发了一种将文本映射到数值特征向量的技术——非常适合机器学习。这种技术被称为词袋模型。
- en: 6.1 Using the Bag of Words model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 使用词袋模型
- en: 'The *Bag of Words* model is a method from natural language processing (NLP)
    that takes as input text in the form of a sentence and turns it into a feature
    vector by considering the extracted vocabulary words and the frequency of their
    occurrences. It’s named as such because each word frequency count is like a “bag,”
    with each occurrence of a word being an item in that bag. Bag of Words is a state-of-the-art
    model that allows you to take a review of a movie and convert it to a feature
    vector, which you will need to classify its sentiment. Consider the following
    review snippet text, written about a Michael Jackson movie:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型**是自然语言处理（NLP）中的一个方法，它将句子形式的文本作为输入，通过考虑提取的词汇和它们出现的频率，将其转换为一个特征向量。之所以命名为“词袋”，是因为每个单词的频率计数就像一个“袋子”，其中每个单词的出现都是一个袋子中的项目。词袋模型是一个最先进的模型，它允许你将电影评论转换为特征向量，这将用于对其情感进行分类。考虑以下关于迈克尔·杰克逊电影的评论片段文本：'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first step in applying the Bag of Words model to process this review is
    preprocessing the text and extract only the words with actual meaning. Usually,
    this process involves removing any nonletter characters—such as numbers, annotations
    such as HTML tags, and punctuation—and strips the text down to its bare words.
    After that, the approach reduces the remaining words in the subset to those that
    are nouns, verbs, or adjectives, and takes out articles, conjunctions, and other
    stop words—words that are not distinguishing features of the text itself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将词袋模型应用于处理此评论的第一步是预处理文本，并仅提取具有实际意义的单词。通常，这个过程涉及删除任何非字母字符——如数字、HTML标签等注释和标点符号——并将文本简化为其基本单词。之后，该方法将剩余的单词子集减少到名词、动词或形容词，并去除冠词、连词和其他停用词——这些词不是文本本身的区分性特征。
- en: NOTE Many canned stop-word lists are available. Those used by Python’s Natural
    Language Toolkit (NLTK) are a good starting point; you can find them at [https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280).
    Stop words are usually language-specific, so you want to make sure whichever list
    you use suits the language you are processing. Luckily for you, NLTK currently
    handles stop words from 21 languages. You can read more about it at [http://mng.bz/MoPn](http://mng.bz/MoPn).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：许多现成的停用词列表可供使用。Python 自然语言工具包（NLTK）中使用的列表是一个很好的起点；您可以在[https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280)找到它们。停用词通常是语言特定的，因此您需要确保您使用的列表适合您正在处理的语言。幸运的是，NLTK
    目前可以处理 21 种语言的停用词。您可以在[http://mng.bz/MoPn](http://mng.bz/MoPn)了解更多信息。
- en: When that step is complete, the Bag of Words model generates a count histogram
    of the remaining vocabulary words, and that histogram becomes the fingerprint
    for the input text. Often, the fingerprint is normalized by dividing the counts
    by the max count, resulting in a feature vector of values between 0 and 1\. The
    whole process is shown in figure 6.1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当这一步完成时，词袋模型生成剩余词汇词的计数直方图，该直方图成为输入文本的指纹。通常，通过将计数除以最大计数来归一化指纹，从而得到介于 0 和 1 之间的值特征向量。整个过程如图
    6.1 所示。
- en: '![CH06_F01_Mattmann2](../Images/CH06_F01_Mattmann2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Mattmann2](../Images/CH06_F01_Mattmann2.png)'
- en: Figure 6.1 A visual depiction of the Bag of Words model. Text is analyzed and
    cleaned, and words are counted to form a histogram, which is then normalized to
    obtain a feature vector representation of the input text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 词袋模型的视觉表示。文本被分析和清理，然后计数形成直方图，然后归一化以获得输入文本的特征向量表示。
- en: 6.1.1 Applying the Bag of Words model to movie reviews
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 将词袋模型应用于电影评论
- en: To get started with the Bag of Words model, you’ll need some review text. The
    Kaggle Bag of Words Meets Bags of Popcorn challenge is an excellent, already-completed
    competition that looked at 50,000 movie reviews from Internet Movie Database ([IMDb.com](https://www.imdb.com/))
    to generate a sentiment classification from those movie reviews. You can read
    more about the challenge at [http://mng.bz/aw0B](http://mng.bz/aw0B). You will
    use those reviews in this chapter to build sentiment classifiers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用词袋模型，您需要一些评论文本。Kaggle 词袋模型与爆米花袋挑战是一个优秀的、已经完成的比赛，该比赛分析了来自互联网电影数据库（[IMDb.com](https://www.imdb.com/)）的
    50,000 部电影评论，以生成这些电影评论的情感分类。您可以在[http://mng.bz/aw0B](http://mng.bz/aw0B)了解更多关于这个挑战的信息。您将在本章中使用这些评论来构建情感分类器。
- en: To get started, grab the labeledTrainData.tsv file from [http://mng.bz/ggEE](http://mng.bz/ggEE),
    and save it to your local drive. You’ll also want to download the testData.tsv
    file from [http:// mng.bz/emWv](http://mng.bz/emWv); you’ll use that file later.
    The files are formatted as tab-separated values (TSV) files, with the columns
    corresponding to a unique identifier (`id`), the sentiment (`1` for positive or
    `0` for negative), and the review itself in HTML format, per row.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，从[http://mng.bz/ggEE](http://mng.bz/ggEE)获取 labeledTrainData.tsv 文件，并将其保存到您的本地驱动器上。您还想要下载
    testData.tsv 文件，从[http:// mng.bz/emWv](http://mng.bz/emWv)；您稍后会使用该文件。这些文件格式为制表符分隔值（TSV）文件，列对应于唯一标识符（`id`）、情感（`1`
    表示正面或 `0` 表示负面）以及每行的 HTML 格式评论。
- en: Let’s try our Bag of Words model and create a function to handle creating machine-learning-ready
    input features from the input labeledTrainData.tsv file. Open a new notebook called
    sentiment_classifier.ipynb, and create a `review_to_words` function. The first
    thing the function does is convert an HTML review from IMDb into review text by
    calling the Tika Python library. Tika Python is a content analysis library whose
    main functionalities include file type identification, text and metadata extraction
    from more than 1,400 formats, and language identification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试我们的词袋模型，并创建一个函数来处理从输入 labeledTrainData.tsv 文件创建机器学习准备好的输入特征。打开一个名为 sentiment_classifier.ipynb
    的新笔记本，并创建一个 `review_to_words` 函数。该函数首先通过调用 Tika Python 库将 IMDb 的 HTML 评论转换为评论文本。Tika
    Python 是一个内容分析库，其主要功能包括文件类型识别、从 1400 多种格式中提取文本和元数据以及语言识别。
- en: TIP A full explanation of Tika is the subject of another Manning book written
    by me. Seriously, check out Tika in Action ([https://www.manning.com/books/ tika-in-action](https://www.manning.com/books/tika-in-action)).
    Here, you use it to strip all the HTML tags in text, using the `parser` interface
    and its `from_buffer` method, which takes as input a string buffer and outputs
    the associated extracted text from the HTML parser.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TIP Tika的完整解释是我写的另一本Manning书籍的主题。认真地说，查看Tika in Action ([https://www.manning.com/books/tika-in-action](https://www.manning.com/books/tika-in-action))。在这里，你使用它来移除文本中的所有HTML标签，使用`parser`接口和其`from_buffer`方法，该方法接受一个字符串缓冲区作为输入，并输出HTML解析器关联的提取文本。
- en: With the extracted review text in hand, use Python’s `re` (for *regular expression*)
    module to use a common pattern `[^a-zA-z]`, which means start from the beginning
    of the string (the ^ symbol), scan and identify only uppercase and lowercase letters
    *a* through *z*, and replace everything else with a whitespace character.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有提取的评论文本后，使用Python的`re`（用于*正则表达式*）模块，使用一个常见的模式`[^a-zA-z]`，这意味着从字符串的开始（`^`符号）扫描并识别仅大写和小写字母`a`到`z`，并将其他所有内容替换为空白字符。
- en: The next step is converting the text all to lowercase. Word-case has meaning
    for interpreting a sentence or language but little meaning for counting word occurrences
    independent of the structure. Stop words, including conjunctions and articles,
    are removed next via Python’s NLTK library. The library has support for stop words
    from 21 languages, so you’ll use the ones for English, because you’re working
    with IMDb’s English reviews. The final step is joining the remaining words as
    a string. The output of listing 6.1 is a thinned-down version of the original
    listing with only the meaningful words and no HTML—in other words, clean text.
    That clean text will be the actual input to the Bag of Words model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将文本全部转换为小写。单词的大小写对于解释句子或语言有含义，但对于独立于结构的单词出现次数计数则意义不大。接下来，通过Python的NLTK库移除停用词，包括连词和冠词。该库支持21种语言的停用词，因此你会使用英语的停用词，因为你正在处理IMDb的英语评论。最后一步是将剩余的单词作为一个字符串连接起来。列表6.1的输出是原始列表的简化版本，只包含有意义的单词和没有HTML——换句话说，是干净的文本。这个干净的文本将是Bag
    of Words模型的实际输入。
- en: Listing 6.1 Creating features from the input text of the reviews
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 从评论的输入文本创建特征
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Function converts a raw review to a string of words by using Apache Tika.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 函数通过使用Apache Tika将原始评论转换为单词字符串
- en: ❷ Removes nonletters
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 移除非字母字符
- en: ❸ Converts to lowercase, split into individual words
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 转换为小写，分割成单个单词
- en: ❹ Converts stop words to a set, which is much faster than searching list
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将停用词转换为集合，这比在列表中搜索要快得多
- en: ❺ Removes stop words
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 移除停用词
- en: ❻ Joins the words back into one string separated by space
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将单词重新组合成一个由空格分隔的字符串
- en: Armed with our function to generate clean review text, you can start running
    the function over the 25,000 reviews in labeledTrainData.tsv. But first, you need
    to load those reviews into Python.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们生成干净评论文本的函数，你就可以开始在该标签化训练数据.tsv中的25,000条评论上运行该函数。但首先，你需要将这些评论加载到Python中。
- en: 6.1.2 Cleaning all the movie reviews
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 清理所有电影评论
- en: A handy library that loads a TSV into Python efficiently is the Pandas library
    for creating, manipulating, and saving dataframes. You can think of a dataframe
    as being a table that is machine-learning-ready. Each column in the table is a
    feature you can use in machine learning, and the rows are input for training or
    testing. Pandas provides functions for adding and dropping feature columns, and
    for augmenting and replacing row values in sophisticated ways. Pandas is the subject
    of many books (I didn’t write them!), and Google provides tens of thousands of
    results on the subject, but for your purposes here, you can use Pandas to create
    a machine-learning-ready dataframe from the input TSV file. Then Pandas can help
    you inspect the number of features, rows, and columns in your input.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方便的库是Pandas库，它可以高效地将TSV加载到Python中，用于创建、操作和保存数据框。你可以将数据框想象成一个机器学习就绪的表格。表格中的每一列都是一个可以用于机器学习的特征，而每一行是用于训练或测试的输入。Pandas提供了添加和删除特征列的功能，以及以复杂方式增强和替换行值的功能。Pandas是许多书籍的主题（我没有写它们！），Google提供了关于此主题的数万个结果，但就你的目的而言，你可以使用Pandas从输入TSV文件创建一个机器学习就绪的数据框。然后Pandas可以帮助你检查输入中的特征数、行数和列数。
- en: With that dataframe, you run your review-text-cleaning code to generate clean
    reviews to which you can apply the Bag of Words model. First, call the Pandas
    `read_csv` function, and tell it that you are reading a TSV file with no header
    row, with the tab character (\t) as the delimiter, and that you do not want it
    to quote the feature values. When the train data is loaded, print its shape and
    column values, demonstrating the ease of using Pandas to inspect your dataframe.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用那个数据框，你运行你的文本清理代码以生成干净的评论，然后你可以应用词袋模型。首先，调用 Pandas 的 `read_csv` 函数，并告诉它你正在读取一个没有标题行的
    TSV 文件，使用制表符（\t）作为分隔符，并且你不想引用特征值。当训练数据被加载时，打印其形状和列值，展示了使用 Pandas 检查数据框的便捷性。
- en: Because cleaning 25,000 movie reviews can take a while, you will use Python’s
    TQDM helper library to keep track of your progress. TQDM is an extensible progress-bar
    library that print status to the command line or to a Jupyter notebook. You wrap
    your iteration step—the `range` function in listing 6.2—as a `tqdm` object. Then
    every iteration step causes a progress-bar increment to be visible to the user,
    either via the command line or in a notebook. TQDM is a great way to fire and
    forget a long-running machine-learning operation and still know that something
    is going on when you come back to check on it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于清理 25,000 部电影评论可能需要一段时间，你将使用 Python 的 TQDM 辅助库来跟踪进度。TQDM 是一个可扩展的进度条库，它将状态打印到命令行或
    Jupyter 笔记本。你将迭代步骤——列表 6.2 中的 `range` 函数——包装成一个 `tqdm` 对象。然后每个迭代步骤都会使进度条的增加对用户可见，无论是通过命令行还是在笔记本中。TQDM
    是一种很好的方式，可以在长时间运行的机器学习操作中“发射并忘记”，同时当你回来检查时仍然知道有事情在进行。
- en: Listing 6.2 prints the training shape `(25000, 3)` corresponding to 25,000 reviews
    and 3 columns (id, sentiment, and review), and the output `array(['id', 'sentiment',
    'review'], dtype=object)` corresponding to those column values. Add the code in
    listing 6.2 to your sentiment_classifier.ipynb notebook to generate 25,000 clean-text
    reviews and keep track of the progress.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 打印了训练数据的形状 `(25000, 3)`，对应于 25,000 条评论和 3 列（id、sentiment 和 review），以及输出
    `array(['id', 'sentiment', 'review'], dtype=object)`，对应于那些列值。将列表 6.2 中的代码添加到你的
    sentiment_classifier.ipynb 笔记本中，以生成 25,000 条干净的文本评论并跟踪进度。
- en: Listing 6.2 Using Pandas to read the movie reviews and apply your cleaning function
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 使用 Pandas 读取电影评论并应用你的清理函数
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Reads the 25,000 reviews from the input TSV file
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从输入 TSV 文件中读取 25,000 条评论
- en: ❷ Prints the shape of the training data and number of values
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印训练数据的形状和值数量
- en: ❸ Gets the number of reviews based on the dataframe column size
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据数据框列的大小获取评论数量
- en: ❹ Initializes an empty list to hold the clean reviews
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化一个空列表来保存干净的评论
- en: ❺ Loops over each review and cleans it by using your function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历每个评论并使用你的函数清理它
- en: Now that you have the clean reviews, it’s time to apply the Bag of Words model.
    Python’s SK-learn library ([https://scikit-learn.org](https://scikit-learn.org/))
    is an extensible machine-learning library that provides a lot of features complementary
    to TensorFlow. Even though some of the features overlap, I use SK-learn’s data
    cleaning functions quite a bit throughout the book. You don’t have to be a purist.
    SK-learn comes with a fantastic implementation of Bag of Words called `CountVectorizer``,`
    for example; you’ll use it in listing 6.3 to apply the Bag of Words model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了干净的评论，是时候应用词袋模型了。Python 的 SK-learn 库 ([https://scikit-learn.org](https://scikit-learn.org/))
    是一个可扩展的机器学习库，它提供了许多与 TensorFlow 相互补的功能。尽管一些功能有重叠，但我在这本书中相当多地使用了 SK-learn 的数据清理函数。你不必是纯粹主义者。SK-learn
    随带了一个名为 `CountVectorizer` 的出色实现，例如；你将在列表 6.3 中使用它来应用词袋模型。
- en: First, create the `CountVectorizer` with some initial hyperparameters. These
    hyperparameters tell SK-learn whether you want it to do any text analysis, such
    as tokenization, preprocessing, or removal of stop words. I omit it here because
    you’ve already written your own text-cleaning function in listing 6.1 and applied
    it in 6.2 to the input text.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用一些初始超参数创建 `CountVectorizer`。这些超参数告诉 SK-learn 是否要执行任何文本分析，例如分词、预处理或移除停用词。我在这里省略了它，因为你已经在列表
    6.1 中编写了自己的文本清理函数，并在 6.2 中将其应用于输入文本。
- en: One parameter of note is `max_features`, which controls the size of the learned
    vocabulary from the text. Choosing a size of `5000` ensures that the TensorFlow
    model you build has sufficient richness and that the resulting Bag of Words fingerprints
    for each review can be learned without exploding the amount of RAM on your machine.
    Obviously, you can play around with this example of parameter tuning later, given
    a larger machine and more time. A general rule of thumb is that a vocabulary on
    the order of thousands should provide sufficient learnability for English movies.
    For news, scientific literature, and other domains, however, you may need to experiment
    to find an optimal value.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的参数是`max_features`，它控制从文本中学习到的词汇的大小。选择`5000`的大小可以确保你构建的TensorFlow模型具有足够的丰富性，并且每个评论的Bag
    of Words指纹可以在不耗尽你机器上的RAM的情况下学习。显然，你可以根据更大的机器和更多的时间在这个参数调整示例上玩耍。一个一般性的规则是，一个大约有数千个词汇的词汇表应该为英语电影的足够学习性提供支持。然而，对于新闻、科学文献和其他领域，你可能需要实验以找到最佳值。
- en: Call `fit_transform` to provide the clean reviews you generated in listing 6.2
    and to get back the vectorized Bag of Words, one row per review, with the row
    contents being the count per vocabulary word per review. Then convert the vector
    to a NumPy array; print its shape; and ensure that you see `(25000,5000)`, corresponding
    to 25,000 input rows with 5,000 features per row. Add the code from listing 6.3
    to your notebook.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`fit_transform`以提供你在列表6.2中生成的干净评论，并获取向量化的Bag of Words，每行一个评论，行内容为每个评论中每个词汇的计数。然后将向量转换为NumPy数组；打印其形状；并确保你看到`(25000,5000)`，对应于25,000个输入行，每行有5,000个特征。将列表6.3中的代码添加到你的笔记本中。
- en: Listing 6.3 Applying the Bag of Words model to obtain your training data
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 将Bag of Words模型应用于获取训练数据
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Imports the CountVectorizer and instantiates the Bag of Words model
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入CountVectorizer并实例化Bag of Words模型
- en: ❷ Fits the model, learns the vocabulary, and transforms training data into vectors
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调整模型，学习词汇，并将训练数据转换为向量
- en: ❸ Converts the results to a NumPy array
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将结果转换为NumPy数组
- en: ❹ Prints the resultant input feature shape (25000,5000)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印结果输入特征形状（25000,5000）
- en: 6.1.3 Exploratory data analysis on your Bag of Words
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 对你的Bag of Words进行探索性数据分析
- en: Doing some exploratory data analysis is always a good thing, and you may want
    to inspect the values of the vocabulary returned from `CountVectorizer` to get
    a feel for what words are present across all the reviews. You’ll want to convince
    yourself that there is something to learn here. What you are looking for is some
    statistical distribution across the words and associated patterns that the classifier
    will learn to identify from that distribution. If all the counts are the same
    in every review, and you can’t eyeball a difference between them, the machine-learning
    algorithm will have the same difficulty.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 进行一些探索性数据分析始终是一件好事，你可能想检查`CountVectorizer`返回的词汇值，以了解所有评论中存在的单词。你将想要确信这里确实有东西可以学习。你想要寻找的是单词之间的某种统计分布和相关的模式，分类器将从这个分布中学习以识别。如果每个评论中的计数都相同，并且你无法通过肉眼区分它们，那么机器学习算法将面临相同的困难。
- en: The great things about SK-learn and `CountVectorizer` is that they not only
    provide a one- or two-line API call to create the Bag of Words output, but also
    allow easy inspection of the result. You can get the vocabulary words learned
    and print them, count their size by using a quick NumPy sum method to bin by word,
    and then take a look at the first 100 words and their sums across all reviews.
    The code to perform these tasks is in listing 6.4.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: SK-learn和`CountVectorizer`的伟大之处在于，它们不仅提供了一个简单的一行或两行API调用以创建Bag of Words输出，而且允许轻松检查结果。你可以获取已学习的词汇并打印它们，通过使用快速的NumPy求和方法按单词进行分箱，然后查看前100个单词及其在所有评论中的总和。执行这些任务的代码在列表6.4中。
- en: Listing 6.4 Exploratory data analysis on the returned Bag of Words
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 对返回的Bag of Words进行探索性数据分析
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Gets the learned vocabulary and prints its size and the learned words
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取已学习的词汇并打印其大小和已学习的单词
- en: ❷ Sums the counts of each vocabulary word
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 求每个词汇的词频总和
- en: ❸ Prints the vocabulary word and the number of times it appears in the training
    set
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印词汇词及其在训练集中出现的次数
- en: ❹ Plots the word count for the first 100 words
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制前100个单词的词频图
- en: Figure 6.2 shows the output set of words printed for the first 100 words in
    all 25,000 reviews. I could have picked any random set of 100 words from the vocabulary,
    but to keep the example simple, I picked the first 100\. Even in the first 100
    words, there appears to be statistical significance in the count of those words
    across reviews; the counts are not all the same, and there is no uniformity. Some
    words are used more often than others, and there are some obvious outliers, so
    it looks as though there is a signal for a classifier to learn. You get started
    building a logistic regression classifier in section 6.2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2显示了所有25000条评论中前100个单词的输出词集。我本可以随机选择词汇表中的任意100个单词，但为了使示例简单，我选择了前100个。即使在第一个100个单词中，这些单词在评论中的计数似乎也具有统计学意义；计数并不相同，也没有一致性。有些单词比其他单词使用得更频繁，还有一些明显的异常值，所以看起来似乎有一个信号供分类器学习。你可以在第6.2节开始构建逻辑回归分类器。
- en: '![CH06_F02_Mattmann2](../Images/CH06_F02_Mattmann2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Mattmann2](../Images/CH06_F02_Mattmann2.png)'
- en: Figure 6.2 Vocabulary counts summed across all 25,000 reviews of the first 100
    words in the extracted 5,000-word vocabulary
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 显示了所有25000条评论中提取的5000词词汇表中前100个单词的词汇计数总和
- en: 6.2 Building a sentiment classifier using logistic regression
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 使用逻辑回归构建情感分类器
- en: 'In chapter 5, when dealing with logistic regression, you identified the dependent
    and independent variables. In sentiment analysis, your dependent variable is your
    5,000D feature vector Bag of Words per review, and you have 25,000 to train on.
    Your independent variable is the sentiment value: a `1` corresponding to a positive
    review from IMDb or a `0` corresponding to a user’s negative sentiment about the
    movie.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，当处理逻辑回归时，你确定了因变量和自变量。在情感分析中，你的因变量是每条评论的5000D特征向量词袋，你有25000条数据用于训练。你的自变量是情感值：一个`1`对应IMDb中的正面评论，一个`0`对应用户对电影的负面情感。
- en: What about the movie titles?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那电影标题呢？
- en: Have you noticed that the IMDb data you are using is the review and the sentiment,
    but no title? Where are the title words? Those words could factor into the sentiment
    if they contain trigger words that map to the words that moviegoers used in their
    reviews. But overall, you don’t need the titles—only a sentiment (something to
    learn) and a review.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到你使用的IMDb数据是评论和情感，但没有标题？标题词在哪里？如果这些词包含映射到电影观众在评论中使用的单词的触发词，它们可能会影响情感。但总体来说，你不需要标题——只需要情感（要学习的东西）和评论。
- en: Try to picture the space of solutions that your classifier will be exploring,
    given the training data and feature space. You can imagine a vector plane—call
    it the ground—and call the vertical axis the elevation distance from the ground
    as though you were standing on it and looking up to the sky—the sentiment. On
    the ground plane, you have a vector beginning at the origin from where you are
    standing and proceeding in every direction corresponding to a particular word
    from your vocabulary—5,000 axes, if you will—shooting out a distance that corresponds
    to the count of that particular word that the vector describes. Data points in
    this plane are the specific counts on each of the word axes, and the y value is
    whether the collection of counts on each plane for a particular point implies
    a sentiment of `1` or `0`. Your imagining should look similar to figure 6.3.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试想象你的分类器将在给定的训练数据和特征空间中探索的解决方案空间。你可以想象一个矢量平面——称之为地面——将垂直轴称为从地面上方站立并向上看天空的垂直距离——情感。在地面上，有一个从你站立的原点开始的矢量，向每个方向延伸，对应于你的词汇表中的特定单词——如果你愿意，有5000个轴——射出与该矢量描述的特定单词的计数相对应的距离。这个平面上的数据点是每个单词轴上的特定计数，y值是特定点的平面上的计数集合是否意味着情感为`1`或`0`。你的想象应该类似于图6.3。
- en: '![CH06_F03_Mattmann2](../Images/CH06_F03_Mattmann2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F03_Mattmann2](../Images/CH06_F03_Mattmann2.png)'
- en: Figure 6.3 Picturing the construction of the classifier by using logistic regression.
    Your feature space is the count of the words arranged as the plane three-dimensionally,
    where the value is the occurrence count. The y-axis corresponds to the sentiment
    result (`0` or `1`).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 通过使用逻辑回归构建分类器的想象。你的特征空间是按三维排列的单词计数，其中值是发生次数。y轴对应于情感结果（`0`或`1`）。
- en: 'Given this construction, we can represent the logistic regression equation
    that corresponds to this classifier by using the following equations. The goal
    is to have a linear function with all the dependent variables and their associated
    weights (`1` through `5000`) as the parameter to the sigmoid (`sig`) function,
    which results in a smooth curve that fluctuates between `0` and `1`, corresponding
    to the sentiment independent variable:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这种结构，我们可以使用以下方程来表示与这个分类器相对应的逻辑回归方程。目标是拥有一个线性函数，其中包含所有依赖变量及其相关权重（`1` 到 `5000`）作为
    sigmoid (`sig`) 函数的参数，这将产生一个在 `0` 和 `1` 之间波动的平滑曲线，对应于情感独立变量：
- en: '*M*(*x, v*) = *sig*(*wx* + *w*)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*M*(*x, v*) = *sig*(*wx* + *w*)'
- en: sentiment = *sig*(*w*[1]*x*[1] + *w*[2]*x*[2] + ⋅⋅⋅ + *w*[5000]*x*[5000] + *w*[0])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 情感 = *sig*(*w*[1]*x*[1] + *w*[2]*x*[2] + ⋅⋅⋅ + *w*[5000]*x*[5000] + *w*[0])
- en: 6.2.1 Setting up the training for your model
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 为你的模型设置训练
- en: You’re ready to set up your TensorFlow logistic regression classifier. Begin
    with an arbitrary learning rate of 0.1 to start, and train for 2,000 epochs (which
    worked well on my laptop), especially because you will perform early stopping.
    *Early stopping* is a technique that measures the difference in loss (or error
    rate) between the previous epoch and the current epoch. If the error rate shifts
    between epochs by some minor threshold epsilon, the model is said to be stable,
    and you can break early in your training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好设置你的 TensorFlow 逻辑回归分类器了。开始时，可以设置一个任意的学习率 0.1，并训练 2,000 个周期（在我的笔记本电脑上效果很好），尤其是因为你将执行早期停止。*早期停止*
    是一种技术，它测量前一个周期和当前周期之间损失（或错误率）的差异。如果错误率在周期之间通过某个小的阈值 epsilon 发生变化，则认为模型是稳定的，你可以在训练中提前终止。
- en: You’ll set up your sigmoid function, which is needed for the model as well.
    As discussed in chapter 5, this function ensures that the backpropagation process
    used to learn the appropriate model weights during each training step after applying
    the cost function has a smooth gradient step that fluctuates between `0` and `1`.
    The sigmoid function has precisely those properties.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你将设置 sigmoid 函数，这对于模型也是必需的。如第 5 章所述，这个函数确保在应用成本函数后，在每次训练步骤中学习适当的模型权重时，反向传播过程有一个在
    `0` 和 `1` 之间波动的平滑梯度步骤。sigmoid 函数恰好具有这些属性。
- en: 'Create the placeholders in TensorFlow for the Y values that you will learn,
    the sentiment labels, and your placeholder for the X input 5,000 × 25,000-dimensional
    feature vector: one Bag of Words vector per movie review and 25,000 movie reviews.
    In listing 6.5, you use a Python dictionary to store each Bag of Words vector,
    indexed `X0-X4999`. The `w` variable (the weights) is one for each dependent variable
    `X` and one constant `w` added at the end of the linear equation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中创建用于你将学习的 Y 值的占位符，情感标签，以及你的 X 输入占位符 5,000 × 25,000 维度特征向量：每个电影评论一个词袋向量，共有
    25,000 个电影评论。在列表 6.5 中，你使用一个 Python 字典来存储每个词袋向量，索引为 `X0-X4999`。`w` 变量（权重）对应于每个依赖变量
    `X`，并在线性方程的末尾添加一个常数 `w`。
- en: The `cost` function is the same convex cross-entropy loss function used in chapter
    5, and you will use gradient descent as your optimizer. The complete code to set
    up the model is shown in listing 6.5.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`cost` 函数与第 5 章中使用的相同凸交叉熵损失函数，你将使用梯度下降作为你的优化器。设置模型的完整代码在列表 6.5 中展示。'
- en: Listing 6.5 Setting up the training for the logistic regression sentiment classifier
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 设置逻辑回归情感分类器的训练
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Sets up the initial model hyperparameters for learning rate and number of
    epochs
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置学习率和周期数的初始模型超参数
- en: ❷ Sets up the logistic regression model
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置逻辑回归模型
- en: ❸ Defines TensorFlow placeholders to inject the actual input and label values
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义 TensorFlow 占位符以注入实际的输入和标签值
- en: ❹ Extracts the labels to learn from the Pandas dataframe
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从 Pandas 数据框中提取学习用的标签
- en: ❺ Constructs the logistic regression model to learn
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 构建要学习的逻辑回归模型
- en: ❻ Defines the cross-entropy cost function and train operation for each learning
    step
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义每个学习步骤的交叉熵成本函数和训练操作
- en: After setting up your model, you can perform the training by using TensorFlow.
    As I mentioned earlier, you’ll perform early stopping to save yourself useless
    epochs when the `loss` function and model response cost settle down.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好你的模型后，你可以使用 TensorFlow 进行训练。如我之前提到的，你将执行早期停止以节省在 `loss` 函数和模型响应成本稳定时的无用周期。
- en: 6.2.2 Performing the training for your model
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 对你的模型进行训练
- en: 'Create a `tf.train.Saver` to save the model graph and the trained weights so
    that you can reload them later to make classification predictions with your trained
    model. The training steps are similar to what you’ve seen before: you initialize
    TensorFlow and this time use TQDM to keep track of and incrementally print the
    progress in training so that you have some indicator. Training will possibly take
    30 to 45 minutes to train and will consume gigabytes of memory—at least, it did
    on my fairly beefy Mac laptop— so TQDM is a must-have, letting you know how the
    training process is going.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`tf.train.Saver`来保存模型图和训练权重，以便你可以在以后重新加载它们，并使用训练好的模型进行分类预测。训练步骤与之前看到的大致相同：你初始化TensorFlow，这次使用TQDM来跟踪和逐步打印训练进度，以便你有一些指标。训练可能需要30到45分钟，并且将消耗数GB的内存——至少，在我的相当强大的Mac笔记本电脑上是这样——所以TQDM是必不可少的，让你知道训练过程正在进行。
- en: The training step injects the 5,000-dimensional feature vector into the X placeholder
    dictionary that you created and the associated sentiment labels into the Y placeholder
    variables from TensorFlow. You’ll use your convex `loss` function as the model
    cost and compare the value of the cost in the previous epoch with the current
    one to determine whether your code should perform early stopping in the training
    to save precious cycles. The threshold value of `0.0001` was chosen arbitrarily
    but could be a hyperparameter to explore, given additional cycles and time. Listing
    6.6 shows full training process for the logistic regression sentiment classifier.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤将5000维的特征向量注入你创建的X占位符字典中，并将相关的情感标签注入到TensorFlow的Y占位符变量中。你将使用你的凸`loss`函数作为模型成本，并将前一个epoch的成本值与当前epoch的成本值进行比较，以确定你的代码是否应该在训练中执行早期停止以节省宝贵的周期。`0.0001`的阈值是任意选择的，但考虑到额外的周期和时间，它可能是一个可以探索的超参数。列表6.6显示了逻辑回归情感分类器的完整训练过程。
- en: Listing 6.6 Performing the training step for the logistic regression sentiment
    classifier
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 执行逻辑回归情感分类器的训练步骤
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Creates the saver to capture your model graph and associated trained weights
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建保存器以捕获你的模型图和相关训练权重
- en: ❷ Captures the previous loss function value to test for early stopping
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 捕获先前的损失函数值以测试早期停止
- en: ❸ Provides the 25,000-review, 5,000-dimensional feature vectors and the sentiment
    labels
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提供了25,000条评论，5,000维的特征向量和情感标签
- en: ❹ Tests whether the previous loss value varies from the current loss value by
    some small threshold and breaks if so
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 测试先前的损失值是否与当前损失值有微小差异，并在有差异时中断
- en: ❺ Obtains the trained weights while the model graph is still loaded
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在模型图加载时获取训练权重
- en: ❻ Saves the model graph and associated trained weights
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存模型图和相关的训练权重
- en: You’ve trained your first text sentiment classifier by using logistic regression!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经通过使用逻辑回归训练了你的第一个文本情感分类器！
- en: Next, I’ll show you how to use this sentiment classifier to make predictions
    on new unseen data. You’ll also learn how to evaluate the classifier’s accuracy
    and precision, and get a feel for how well it is performing by running it against
    the test data from the Kaggle competition and then submitting your results to
    Kaggle.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向你展示如何使用这个情感分类器对新未见的数据进行预测。你还将学习如何评估分类器的准确性和精确度，并通过运行Kaggle竞赛的测试数据来了解其性能，然后将你的结果提交到Kaggle。
- en: 6.3 Making predictions using your sentiment classifier
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 使用你的情感分类器进行预测
- en: 'Now that you’ve built your classifier, how do you use it to make predictions?
    Two key pieces of information are stored when you make that trusty call to `tf.train.Saver`
    to save your checkpoint file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了你的分类器，你该如何使用它来进行预测呢？当你调用`tf.train.Saver`来保存你的检查点文件时，会存储两个关键信息：
- en: The checkpoint contains the model weights you arrived at—in this case, the weights
    of the `sigmoid` linear portion corresponding to each vocabulary word in your
    Bag of Words model.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点包含你到达的模型权重——在这种情况下，对应于你的词袋模型中每个词汇的`sigmoid`线性部分的权重。
- en: The checkpoint contains the model graph and its current state, in case you want
    to pick up where you left off and continue training its next epochs.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点包含模型图及其当前状态，以防你想从上次停止的地方继续训练并继续下一个epoch。
- en: Making predictions is as simple as loading the checkpoint file and applying
    those weights to the model. As I showed you in chapter 5, there is no need to
    reuse your TensorFlow version of the `y_model` function from listing 6.5—the `tf.sigmoid`
    function—because doing so loads the model graph and requires additional resources
    to prepare TensorFlow to continue training. Instead, you can apply the learned
    weights to a NumPy version of the model—the inline `sigmoid` function from listing
    6.5—because you won’t be doing any further training on it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 进行预测就像加载检查点文件并将这些权重应用到模型上一样简单。正如我在第5章中向您展示的那样，您不需要重新使用列表6.5中的`y_model`函数的TensorFlow版本——`tf.sigmoid`函数——因为这样做会加载模型图，并需要额外的资源来准备TensorFlow以继续训练。相反，您可以将学习到的权重应用到模型的NumPy版本——列表6.5中的内联`sigmoid`函数——因为您不会对它进行任何进一步的训练。
- en: 'Seems pretty simple, right? But I left out one major thing that I need to cover
    first. This will generalize to the rest of the book, in which you perform machine
    learning, train a model, and use it to make predictions that aid your automated
    decisions. Consider the steps you performed for your model training:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很简单，对吧？但我省略了一个需要首先解决的问题。这将在本书的其余部分得到推广，其中您执行机器学习、训练模型并使用它来做出辅助自动化决策的预测。考虑您为模型训练执行的步骤：
- en: Perform data cleaning of 25,000 movie reviews.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对25,000条电影评论进行数据清洗。
- en: Strip HTML.
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去除HTML标签。
- en: Remove punctuation and consider only `a-zA-Z.`
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除标点符号，并仅考虑`a-zA-Z.`。
- en: Remove stop words.
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除停用词。
- en: Apply the Bag of Words model, limiting vocabulary to a 5,000-word feature vector.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用词袋模型，将词汇限制在5,000个单词的特征向量中。
- en: Use 25,000 vectors of size 5,000 and associated 25,000 labels for sentiment
    1, 0 and logistic regression to make a classification model.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用25,000个大小为5,000的向量和相关的25,000个标签（情感1、0和逻辑回归）来构建分类模型。
- en: 'Now suppose that you want to use your model to perform predictions on some
    new text, such as the following two sentences. The first sentence is clearly a
    negative review, and the second is a positive review:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设您想使用您的模型对新文本进行预测，例如以下两个句子。第一个句子显然是负面评论，第二个是正面评论：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How do you provide these sentences to your model to make sentiment predictions?
    You need to apply to the prediction process the data-preprocessing steps that
    you used during training so that you are considering the text the same way that
    you trained on it. You were training on 5,000-dimensional feature vectors, so
    you’ll need to do the same thing to prepare the input text to make predictions.
    Additionally, you need to take heed of one more step. The weights that you generated
    during training were under the auspices of a common shared vocabulary of 5,000
    words generated by the `CountVectorizer`. The unseen input text that you are making
    predictions on may have a different vocabulary from your trained text vocabulary.
    In other words, that unseen text may use other words, perhaps more or less, than
    you trained on. Yet you spent nearly 45 minutes training your logistic-regression
    sentiment classifier and perhaps even longer preparing the input and labels for
    that training. Is that work invalidated? Do you have to perform training all over
    again?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您如何将这些句子提供给模型以进行情感预测？您需要在预测过程中应用您在训练期间使用的相同数据预处理步骤，这样您考虑文本的方式与您训练的方式相同。您是在5,000维特征向量上进行训练的，因此您需要做同样的事情来准备输入文本以进行预测。此外，您还需要注意一个额外的步骤。您在训练期间生成的权重是在由`CountVectorizer`生成的5,000个单词的共同共享词汇下产生的。您正在对其进行的预测的未见过输入文本可能具有与您的训练文本词汇不同的词汇。换句话说，那个未见过文本可能使用了您训练时没有使用的其他单词，可能更多或更少。然而，您花费了近45分钟来训练您的逻辑回归情感分类器，也许甚至更长的时间来准备训练的输入和标签。这项工作被无效化了吗？您是否必须从头开始进行训练？
- en: Remember I mentioned earlier that choosing a value of `5000` for the vocabulary
    size in `CountVectorizer` allows for sufficient richness, but you may have to
    tune or explore to get the best fit. So vocabulary size does matter, and what
    you are predicting with your trained model matters as well. Having 5,000 words
    in a vocabulary left after preprocessing steps and data cleaning can achieve high
    accuracy during training and on unseen data—as high as 87% in my training, which
    you’ll reproduce later in the chapter when you use receiver operating characteristic
    (ROC) curves. But who is to say that 10,000 words wouldn’t have achieved even
    higher accuracy? Not me!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我之前提到过，在`CountVectorizer`中将词汇大小设置为`5000`可以提供足够的丰富性，但你可能需要调整或探索以获得最佳匹配。所以词汇大小确实很重要，你用训练模型预测的内容也同样重要。在预处理步骤和数据清理后，词汇表中剩下5,000个单词可以在训练和未见数据上实现高准确率——在我的训练中高达87%，你将在本章后面使用接收者操作特征（ROC）曲线时重现这一点。但谁能说10,000个单词不会达到更高的准确率呢？不是我！
- en: Indeed, using more words in your vocabulary may achieve higher accuracy. The
    result depends on the data you intend to make predictions on and the generality
    of that data. It also has a big influence on your overall memory and CPU and GPU
    requirements for training, because using more features for each input vector will
    no doubt take more resources. Note that if the vocabulary of your unseen data
    overlaps sufficiently with the representative vocabulary from your training data,
    there is no need to increase its size.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，使用更多词汇可能达到更高的准确率。结果取决于你打算进行预测的数据以及该数据的普遍性。它还会对你的整体内存和CPU以及GPU训练需求产生重大影响，因为为每个输入向量使用更多特征无疑会消耗更多资源。请注意，如果你的未见数据词汇与训练数据中的代表性词汇有足够的重叠，就没有必要增加其大小。
- en: 'TIP Figuring optimal vocabulary size is an exercise best left for a semester-long
    statistics or NLP graduate course, but suffice it to say that you may want to
    explore this hyperparameter further. This posting has some nice pointers on vocabulary
    size: [http://mng.bz/pzA8](http://mng.bz/pzA8).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：确定最佳词汇大小是最好留给一个学期的统计学或NLP研究生课程的任务，但简单来说，你可能需要进一步探索这个超参数。这篇帖子有一些关于词汇大小的良好建议：[http://mng.bz/pzA8](http://mng.bz/pzA8)。
- en: To move forward and implement the sentiment-prediction function that this chapter
    focuses on, you need to figure out the overlap of the vocabulary vector from your
    new text and its vector with the existing vocabulary words from training. Then
    you will consider only the counts in your Bag of Words model for prediction for
    those overlapping terms in the fingerprint. These counts will be compared with
    those of the new text that you want to predict sentiment on based on your trained
    model. The entire prediction pipeline and its relationship with the training pipeline
    are illustrated in figure 6.4.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向前推进并实现本章重点关注的情感预测功能，你需要确定你新文本的词汇向量与训练中现有词汇的交集。然后，你将只考虑指纹中重叠术语在Bag of Words模型中的计数，并将这些计数与基于训练模型预测情感的新文本的计数进行比较。整个预测流程及其与训练流程的关系在图6.4中展示。
- en: '![CH06_F04_Mattmann2](../Images/CH06_F04_Mattmann2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04_Mattmann2](../Images/CH06_F04_Mattmann2.png)'
- en: Figure 6.4 Making predictions with machine learning. During training (top),
    you preprocess the input data by cleaning the text, convert it to a 5,000-dimensional
    feature vector, and use it to learn sentiment labels (`1` or `0`) for 25,000 movie
    reviews. To make predictions with the learned model (right), you need to perform
    the same data cleaning steps and in addition figure out the overlap of next text
    and its vocabulary with your trained one.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 使用机器学习进行预测。在训练过程中（顶部），你通过清理文本、将其转换为5,000维特征向量，并使用它来学习25,000篇电影评论的情感标签（`1`或`0`）。要使用学习到的模型进行预测（右侧），你需要执行相同的数据清理步骤，并且还需要确定下一个文本及其词汇与你的训练集的交集。
- en: 'Let’s start writing the `predict` function, which should take the unmodified
    review text as input, along with the training vocabulary and learned weights from
    the training process. You need to apply the same data cleaning process, so you
    clean the text by doing the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编写`predict`函数，该函数应接受未修改的评论文本作为输入，以及训练过程中的训练词汇和学到的权重。你需要应用相同的数据清理过程，因此你通过以下方式清理文本：
- en: Tokenizing it
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化
- en: Removing punctuation and noncharacters
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号和非字符
- en: Removing stop words
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Rejoining the tokens
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新组合标记
- en: Afterward, apply the Bag of Words model again, and generate a function with
    which you can make sentiment predictions on unseen input text. The function should
    focus on figuring out the overlap of the new learned vocabulary words from the
    input with those of the training vocabulary. For each overlapping word, consider
    the word count; and other elements in the feature vector will be zero. The resultant
    feature vector should be fed to the sigmoid function for your logistic regression
    model, using the optimal learned weights. The result—a probability between `0`
    and `1` of its sentiment—is compared with a threshold value of `0.5` to determine
    whether the sentiment is `1` or `0`. Listing 6.7 shows the `predict` function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，再次应用词袋模型，并生成一个函数，该函数可以用于对未见过的输入文本进行情感预测。该函数应专注于确定从输入中学习的新词汇与训练词汇的重叠部分。对于每个重叠的单词，考虑单词计数；特征向量中的其他元素将为零。结果特征向量应输入到sigmoid函数中，用于你的逻辑回归模型，使用最优的学习权重。结果——情感的概率介于`0`和`1`之间——与阈值值`0.5`进行比较，以确定情感是`1`还是`0`。列表6.7显示了`predict`函数。
- en: Listing 6.7 Making predictions with the logistic regression sentiment classifier
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 使用逻辑回归情感分类器进行预测
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Takes review text to test, with the training vocabulary, learned weights,
    and threshold cutoff for a positive or negative prediction as parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以参数形式接受用于测试的评论文本、训练词汇、学习权重和正负预测的阈值截止值
- en: ❷ Cleans the review, using the same function you used for training
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用与训练相同的函数清理评论
- en: ❸ Creates the test vocabulary and counts
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建测试词汇和计数
- en: ❹ Converts to a NumPy array of vocabulary and counts
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将词汇和计数转换为NumPy数组
- en: ❺ Figures out the intersection of the test vocabulary from the review with the
    actual full vocabulary
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 确定评论词汇与实际完整词汇的交集
- en: ❻ All zeros for the 5,000-feature vector except for the overlap indices that
    we have counts for
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 除了我们计数重叠索引的5,000特征向量外，其余都是零
- en: ❼ Applies your logistic regression model with the learned weights
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用学习到的权重应用你的逻辑回归模型
- en: ❽ If the predicted probability is greater than 0.5, sentiment is 1; otherwise,
    it is 0.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 如果预测概率大于0.5，则情感为1；否则，为0。
- en: Try the function on the following test reviews, `new_neg_review` and `new_pos_review`.
    The function properly predicts the negative review as a `0` and the positive review
    as a `1`. Cool, right?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下测试评论`new_neg_review`和`new_pos_review`上尝试该函数。该函数正确地将负面评论预测为`0`，将正面评论预测为`1`。酷，对吧？
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that you have a `predict` function, you can use it to compute a confusion
    matrix (chapter 3). Creating a confusion matrix of true positives, false positives,
    true negatives, and false negatives allows you to measure the classifier’s ability
    to predict each class and compute precision and recall. Additionally, you can
    generate an ROC curve and test how much better your classifier is than the baseline.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了`predict`函数，你可以使用它来计算混淆矩阵（第3章）。创建真实阳性、假阳性、真实阴性和假阴性的混淆矩阵，这允许你衡量分类器预测每个类别的能力，并计算精确率和召回率。此外，你可以生成ROC曲线，并测试你的分类器比基线好多少。
- en: 6.4 Measuring the effectiveness of your classifier
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 测量你的分类器的有效性
- en: Now that you can predict the sentiment of unseen text with your logistic-regression
    classifier, a good way to measure its general effectiveness is to test it at scale
    on lots of unseen text. You used 25,000 IMDb movie reviews to train the classifier,
    so you’ll use the other 25,000 that you held out for testing. When you trained
    your classifier by using Kaggle’s TSV file, you were using a joined version of
    the raw IMDb review data. You won’t always have that benefit; sometimes, you need
    to preprocess the raw data. To make sure that you can handle both approaches to
    data preparation and cleaning, use the original raw aclImdb_v1.tar.gz file ([http://mng.bz/Ov5R](http://mng.bz/Ov5R))
    and prepare it for testing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用你的逻辑回归分类器预测未见过的文本的情感了，一个衡量其总体有效性的好方法是将其大规模应用于大量未见过的文本。你使用了25,000条IMDb电影评论来训练分类器，所以你会使用另外25,000条你保留用于测试的。当你使用Kaggle的TSV文件训练分类器时，你使用的是原始IMDb评论数据的合并版本。你并不总是有这个好处；有时，你需要预处理原始数据。为了确保你可以处理这两种数据准备和清理方法，使用原始的aclImdb_v1.tar.gz文件([http://mng.bz/Ov5R](http://mng.bz/Ov5R))并为其准备测试。
- en: 'Unzip the aclImdb_v1.tar.gz file. You see have a folder structure that looks
    like this where each entry below is either a file (like `README`) or a directory
    (like `test` and `train`):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 解压aclImdb_v1.tar.gz文件。你现在有一个看起来像这样的文件夹结构，其中每个条目下面要么是文件（如`README`），要么是目录（如`test`和`train`）：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Open the test directory. Inside are more files (*.txt and *.feat) and folders
    (neg and pos):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 打开测试目录。里面包含更多文件（*.txt和*.feat）和文件夹（neg和pos）：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The folders pos (for *positive*) and neg (for *negative*) hold 12,500 text files
    containing the movie reviews, each corresponding to unseen positive and negative
    reviews, so you’ll create two variables—`only_pos_file_contents` and `only_neg_file_contents`—to
    correspond to them. Read the reviews into those two variables by using the two
    loops. Python’s built-in `os.isfile` function makes sure that as the code iterates
    through and assesses directory listing objects, a test is performed to identify
    whether the object is a file (not a directory). The `os.listdir` method lists
    files in a directory. The code in listing 6.8 loads the test IMDb reviews.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹pos（表示*正面*）和neg（表示*负面*）包含12,500个文本文件，其中包含电影评论，每个文件对应于未见过的正面和负面评论，因此你将创建两个变量——`only_pos_file_contents`和`only_neg_file_contents`——来对应它们。通过使用两个循环将评论读取到这两个变量中。Python的内置`os.isfile`函数确保在代码遍历和评估目录列表对象时，执行一个测试以确定该对象是否为文件（而不是目录）。`os.listdir`方法列出目录中的文件。列表6.8中的代码加载了测试IMDb评论。
- en: Listing 6.8 Loading the test IMDb reviews
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8：加载测试IMDb评论
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Iterates and identifies paths for text files of positive and negative reviews
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历并识别正面和负面评论文本文件的路径
- en: ❷ Reads the positive reviews into a list of 12,500 text objects
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将正面评论读取到包含12,500个文本对象的列表中
- en: ❸ Reads the negative reviews into a list of 12,500 text objects
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将负面评论读取到包含12,500个文本对象的列表中
- en: ❹ Creates a placeholder for the 25,000 sentiment values
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为25,000个情感值创建占位符
- en: With the test reviews loaded into memory, in `only_pos_file_contents` and `only_
    neg_file_contents` and with the labels placeholder `predictions_test` variable,
    you can use the `predict` function to count true and false positives and true
    and false negatives, and then compute precision and recall for your classifier.
    Precision is defined as
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在将测试评论加载到内存中的`only_pos_file_contents`和`only_neg_file_contents`以及标签占位符`predictions_test`变量中后，你可以使用`predict`函数来计数真实和虚假正例以及真实和虚假负例，然后计算分类器的精确度和召回率。精确度定义为
- en: '![CH06_F04EQ02_Mattmann2](../Images/CH06_F04EQ02_Mattmann2.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04EQ02_Mattmann2](../Images/CH06_F04EQ02_Mattmann2.png)'
- en: and recall is defined as
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率定义为
- en: '![CH06_F04EQ03_Mattmann2](../Images/CH06_F04EQ03_Mattmann2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04EQ03_Mattmann2](../Images/CH06_F04EQ03_Mattmann2.png)'
- en: The code in listing 6.9 iterates through the positive-sentiment files, invokes
    your `predict` function, stores the result in the `predictions_test` variable.
    It follows up by calling the `predict` function on the negative file contents.
    Because calling the `predict` function can take a few seconds per call, depending
    on the horsepower on your laptop, you will again use the `tqdm` library to track
    progress during each iteration loop. The final parts of the listing print the
    precision and recall of your classifier, and then the sum of true and false positives
    and true and false negatives. True and false positives and negatives are measured
    from applying your classifier to the unseen test reviews. Running listing 6.9
    outputs `precision 0.859793 recall 0.875200,` which is an exceptional result for
    your first classifier!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9中的代码遍历正面情感文件，调用你的`predict`函数，将结果存储在`predictions_test`变量中。随后，它通过在负面文件内容上调用`predict`函数来继续操作。由于调用`predict`函数可能每次需要几秒钟，这取决于你笔记本电脑的处理能力，你将再次使用`tqdm`库来跟踪每次迭代循环的进度。列表的最后部分打印出你的分类器的精确度和召回率，然后是真实和虚假正例以及真实和虚假负例的总数。真实和虚假正例以及负例是通过将你的分类器应用于未见过的测试评论来衡量的。运行列表6.9输出`精确度
    0.859793 召回率 0.875200`，这对于你的第一个分类器来说是一个出色的结果！
- en: Listing 6.9 Computing the confusion matrix, precision, and recall
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9：计算混淆矩阵、精确度和召回率
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Initializes count of true and false positives and true and false negatives
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化真实和虚假正例以及真实和虚假负例的计数
- en: ❷ Iterates through positive sentiment text files and call predict function,
    and computes TP and FN
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历正面情感文本文件并调用预测函数，并计算TP和FN
- en: ❸ Iterates through negative sentiment text files and call predict function,
    and computes TN and FP
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历负面情感文本文件并调用预测函数，并计算TN和FP
- en: ❹ Computes and prints precision and recall
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算并打印精确度和召回率
- en: 'Given the generated predictions, you can take the next step: examining the
    area under the curve (AUC) and creating an ROC curve to determine how much better
    your classifier is than the baseline. Rather than implementing this process yourself,
    you can use the `roc_curve` function from SK-learn and then some Matplotlib to
    plot the results.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 给定生成的预测结果，您可以进行下一步：检查曲线下面积 (AUC) 并创建 ROC 曲线，以确定您的分类器比基线好多少。您不必自己实现这个过程，可以使用
    SK-learn 的 `roc_curve` 函数，然后使用一些 Matplotlib 来绘制结果。
- en: To use the `roc_curve` function, you need the `predictions_test` variable from
    listing 6.9, which is the result of running the `predict` function on all the
    true-positive examples and then on the true-negative examples. Then you need a
    variable that you will call `outcomes_test,` which is the ground truth. Because
    the ground truth includes 12,500 positive followed by 12,500 negative sentiment
    examples, you can create the `outcomes_test` variable by initializing a call to
    `np.ones`—a NumPy function that creates an array of the size-specified (12,500
    `1`s)—and then appending a call to `np.zeros`—a NumPy function that creates an
    array of the size specified (12,500 `0`s).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `roc_curve` 函数，您需要列表 6.9 中的 `predictions_test` 变量，这是在所有真正例上运行 `predict`
    函数的结果，然后是在真正例上运行的结果。然后您需要一个您将称为 `outcomes_test` 的变量，这是真实情况。因为真实情况包括 12,500 个正面情感示例后面跟着
    12,500 个负面情感示例，您可以通过初始化一个调用 `np.ones`（NumPy 函数，创建指定大小的数组，包含 12,500 个 `1`）的调用来创建
    `outcomes_test` 变量，然后附加一个调用 `np.zeros`（NumPy 函数，创建指定大小的数组，包含 12,500 个 `0`）。
- en: When those variables are generated, you call the `roc_curve` function; obtain
    the true-positive (`tpr`) and false-positive (`fpr`) rates; and pass them to the
    `auc` function, which gives the AUC stored in the `roc_auc` variable. The remaining
    portions of listing 6.10 set up the plot with the baseline classifier in dashed
    line at 0, 1 on the x-axis to 0, 1 on the y-axis and then your actual results
    from the classifier, using the `tpr` and `fpr` values in a solid line above the
    dashed line (figure 6.5).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些变量生成后，您调用 `roc_curve` 函数；获取真正率 (`tpr`) 和假正率 (`fpr`)；然后将它们传递给 `auc` 函数，该函数将
    AUC 存储在 `roc_auc` 变量中。列表 6.10 的剩余部分设置了带有基线分类器在 x 轴从 0 到 1、y 轴从 0 到 1 的虚线，然后使用
    `tpr` 和 `fpr` 值在虚线上方的实线表示您的分类器实际结果（图 6.5）。
- en: Listing 6.10 Measuring your classifier’s performance against the baseline with
    ROC
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.10 使用 ROC 测量您的分类器性能与基线
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Creates an array of labels of 1s (for positive) and 0s (for negative) sized
    proportional to the number of positive and negative files
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个大小与正面和负面文件数量成比例的标签数组（1 表示正面，0 表示负面）
- en: ❷ Computes false positive rate (fpr), true positive rate (tpr), and area under
    the curve (roc_auc)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算假正率 (fpr)、真正率 (tpr) 和曲线下面积 (roc_auc)
- en: ❸ Initializes Matplotlib and sets up the line style and labels for the baseline
    ROC and for the classifier results
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化 Matplotlib 并设置基线 ROC 和分类器结果的线型和标签
- en: ❹ Creates the legend and plot title
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建图例和图表标题
- en: ❺ Shows the plot
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示图表
- en: '![CH06_F05_Mattmann2](../Images/CH06_F05_Mattmann2.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Mattmann2](../Images/CH06_F05_Mattmann2.png)'
- en: Figure 6.5 The ROC curve for your logistic regression sentiment classifier.
    The performance is much better than the baseline, with an ROC curve/AUC of 0.87,
    or 87%.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 逻辑回归情感分类器的 ROC 曲线。性能比基线好得多，ROC 曲线/AUC 为 0.87，或 87%。
- en: You’ve assessed the accuracy of your logistic regression classifier, computing
    its precision, recalling and generating an ROC and AUC curve, and comparing its
    performance with the baseline, which is quite good (nearly 90%). As I mentioned
    in chapter 5, any classifier with this type of performance likely will perform
    well in the wild because it was trained on a balanced dataset and evaluated on
    unseen data with equal balance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经评估了您的逻辑回归分类器的准确性，计算了其精确度、召回率和生成 ROC 和 AUC 曲线，并将其性能与基线进行了比较，基线表现相当好（几乎 90%）。正如我在第
    5 章中提到的，任何具有这种性能的分类器很可能在实际应用中表现良好，因为它是在平衡数据集上训练并在具有相等平衡的未见数据上评估的。
- en: Another technique discussed in chapter 5 is softmax regression, which has the
    natural benefit of extending beyond two-class prediction (or binary classification)
    to N-class prediction. Even though we don’t have N>2 classes to train on here,
    it’s worth exploring how you would create a softmax version of your sentiment
    classifier so that you can get some real experience building it. You can reuse
    most of the work you’ve performed in this chapter, so let’s get to it!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章中讨论的另一种技术是 softmax 回归，它具有自然地将预测扩展到两个类别以上的好处（或二分类）。尽管我们在这里没有 N>2 个类别进行训练，但探索如何创建情感分类器的
    softmax 版本仍然值得，这样你可以获得一些实际构建它的经验。你可以重用本章中完成的大部分工作，让我们开始吧！
- en: 6.5 Creating the softmax-regression sentiment classifier
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 创建 softmax-regression 情感分类器
- en: 'The benefit of the softmax-regression approach is that it extends classification
    to prediction of more than two classes. The construction of the classification
    approach is similar to logistic regression. You have the following equations,
    in which you take a linear set of dependent variables (the weights, `w`, that
    you learn) and run them through the `sigmoid` function, which resolves the value
    in a smooth curve to `0` or `1` quickly. Recall the logistic regression construction:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: softmax-regression 方法的好处是它将分类扩展到预测超过两个类别的预测。分类方法的结构类似于逻辑回归。你有以下方程，其中你取一个线性相关变量集（你学习的权重
    `w`）并通过 `sigmoid` 函数运行，该函数将值迅速解析为平滑曲线的 `0` 或 `1`。回想一下逻辑回归的结构：
- en: '*Y* = *sig*(*linear*)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *sig*(*linear*)'
- en: '*Y* = *sig*(*w*[1]*x*[1] + *w*[2]*x*[2] + ⋅⋅⋅ + *w*[0])'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *sig*(*w*[1]*x*[1] + *w*[2]*x*[2] + ⋅⋅⋅ + *w*[0])'
- en: With softmax regression, you have a similar construction, wherein you learn
    weights of size `num_features` × `num_labels`. In this case, the weights correspond
    to the weights to be multiplied by the dependent variables, similar to logistic
    regression. You also learn a bias matrix of size `num_labels`, again similar to
    standard logistic regression. The key difference with softmax regression is that
    you apply the `softmax` function instead of `sigmoid` to learn the probability
    distribution over the *N* classes you are trying to predict. The equation for
    `softmax` is
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 softmax 回归，你有一个类似的结构，其中你学习 `num_features` × `num_labels` 大小的权重。在这种情况下，权重对应于乘以相关变量的权重，类似于逻辑回归。你还学习一个大小为
    `num_labels` 的偏差矩阵，这同样类似于标准逻辑回归。与 softmax 回归的关键区别在于，你应用 `softmax` 函数而不是 `sigmoid`
    函数来学习你试图预测的 *N* 个类别的概率分布。`softmax` 的方程式是
- en: '*Y* = *WX* + *B*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *WX* + *B*'
- en: Now consider the `softmax` classifier in the context of your sentiment classification
    problem. You are trying to learn the probability, *Y*, that a set of text implies
    one of two labels, positive or negative, for each of the 25,000 reviews that you
    will test on. The set of text for each review is converted to a feature vector
    of 5,000 dimensions, one for each of the 25,000 reviews (*X* ). The weights are
    associated weights for each of the two classes—positive or negative—to multiply
    by the 5,000 dependent variables, the associated matrix of which we refer to as
    *W*. Finally, *B* is the bias for each of the two classes—positive and negative—to
    add, forming the linear equation used for regression. Figure 6.6 shows this construction.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑在情感分类问题的背景下 `softmax` 分类器。你试图学习一组文本表示一个标签的概率，*Y*，这个标签可以是正标签或负标签，对于你将要测试的每一条
    25,000 条评论。每条评论的文本集被转换为一个 5,000 维的特征向量，每个特征向量对应于 25,000 条评论中的一个 (*X* )。权重是与两个类别（正类别或负类别）相关联的权重，用于乘以
    5,000 个相关变量，我们将其相关矩阵称为 *W*。最后，*B* 是两个类别（正类别和负类别）的偏差，用于添加，形成用于回归的线性方程。图 6.6 展示了这种结构。
- en: '![CH06_F06_Mattmann2](../Images/CH06_F06_Mattmann2.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Mattmann2](../Images/CH06_F06_Mattmann2.png)'
- en: Figure 6.6 The softmax-regression approach. The input X field is 25,000 reviews
    and a 5,000-dimensional Bag of Words feature vector. The model learns W, which
    is a matrix of size num features, 5000, by num labels 2, for positive and negative
    classes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 softmax-regression 方法。输入 X 字段是 25,000 条评论和一个 5,000 维的词袋特征向量。模型学习 W，它是一个大小为
    num features, 5000, by num labels 2 的矩阵，用于正负类别。
- en: Given this construction, you can start with some code to put together these
    matrices and prepare them for your classifier. You’ll begin by creating the label
    matrix ground truth. Each entry in the ground truth needs to be encoded with one-hot
    encoding (chapter 5). One-hot encoding is the process of generating categorical
    labels such as `[0, 1]` and `[1,0]` to represent categories A and B—in this case,
    positive sentiment and negative sentiment. Because softmax regression predicts
    the particular class according to an ordered matrix, if you would like `1` to
    correspond to positive and `0` to negative, the column order of that matrix should
    be that the 0th index represents negative and the 1th index is positive, so your
    label encoding should be `[1,0]` for negative sentiment and `[0,1]` for positive.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这种结构，您可以从一些代码开始，将这些矩阵组合起来，为您的分类器做准备。您将从创建标签矩阵的地面真实值开始。地面真实值中的每个条目都需要使用 one-hot
    编码（第 5 章）。one-hot 编码是将类别标签如 `[0, 1]` 和 `[1,0]` 生成的过程，以表示类别 A 和 B——在这种情况下，表示积极情感和消极情感。因为
    softmax 回归根据有序矩阵预测特定类别，如果您希望 `1` 对应于积极，`0` 对应于消极，那么该矩阵的列顺序应该是 0th 索引表示消极，1th 索引表示积极，因此您的标签编码应该是消极情感的
    `[1,0]` 和积极情感的 `[0,1]`。
- en: You’ll reuse the Pandas dataframe from listing 6.2 and use some of the powerful
    features of the Pandas libraries. Dataframes are like in-memory Python relational
    tables, so you can query them with SQL-like constructs. To select all the rows
    in the dataframe in which sentiment is positive (or `1`), for example, you can
    iterate the dataframe by its length and then individually select rows whose value
    for the sentiment column is `1.0`. You’ll use this capability to generate your
    one-hot encoding and then create a NumPy matrix of size 25,000 × 2 out of the
    result.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您将重用列表 6.2 中的 Pandas 数据框，并使用 Pandas 库的一些强大功能。数据框就像内存中的 Python 关系表，因此您可以使用类似
    SQL 的结构查询它们。例如，要选择数据框中情感为积极（或 `1`）的所有行，您可以遍历数据框的长度，然后单独选择情感列值为 `1.0` 的行。您将使用此功能生成
    one-hot 编码，然后从结果中创建一个大小为 25,000 × 2 的 NumPy 矩阵。
- en: Your training input is the output from the `CountVectorizer` in listing 6.11
    converted to a NumPy float matrix of size (25000 × 5000) features. In listing
    6.11, you create the input for your softmax-regression sentiment classifier. The
    output of the listing is the shape of the *X* input `xs.shape`, or `(25000,5000)`,
    and the label matrix shape `labels.shape`, or `(25000,2)`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您的训练输入是来自列表 6.11 的 `CountVectorizer` 的输出，转换为一个大小为 (25000 × 5000) 的特征 NumPy 浮点矩阵。在列表
    6.11 中，您创建了用于您的 softmax-regression 情感分类器的输入。列表的输出是 *X* 输入的形状 `xs.shape`，或 `(25000,5000)`，以及标签矩阵的形状
    `labels.shape`，或 `(25000,2)`。
- en: Listing 6.11 Creating the input for your softmax-regression classifier
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.11 创建 softmax-regression 分类器的输入
- en: '[PRE15]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ One-hot encodes the positive sentiment examples
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ One-hot encodes the positive sentiment examples
- en: ❷ One-hot encodes the negative sentiment examples
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ One-hot encodes the negative sentiment examples
- en: ❸ Converts the label matrix to a NumPy matrix for training
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Converts the label matrix to a NumPy matrix for training
- en: ❹ Extracts the NumPy array of 25000 × 5000 Bag of Words vectors for the training
    reviews
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Extracts the NumPy array of 25000 × 5000 Bag of Words vectors for the training
    reviews
- en: ❺ Prints the shape of the X input matrix (25000,5000) and of the label (25000,2)
    matrix
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ Prints the shape of the X input matrix (25000,5000) and of the label (25000,2)
    matrix
- en: With the training inputs encoded, the next step before creating your TensorFlow
    softmax-regression classifier is shuffling the input data. One reason to do this
    is to prevent the classifier from remembering the order of the inputs and labels,
    instead learning that the mapping of input Bag of Words vectors to sentiment labels
    is what matters. You can use NumPy’s `arange` method, which generates a range
    of number indices with the given shape, to take care of this task.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 TensorFlow softmax-regression 分类器之前，下一步是对输入数据进行洗牌。这样做的一个原因是为了防止分类器记住输入和标签的顺序，而是学习输入
    Bag of Words 向量到情感标签的映射才是重要的。您可以使用 NumPy 的 `arange` 方法，该方法根据给定的形状生成一系列数字索引，来处理这个任务。
- en: You can call `arange` with the size of the number of training samples (`xs.shape[0]`,
    or `25000`) and then use NumPy’s `random` module and its `shuffle` method `np.random
    .shuffle` to randomize those indices. The randomized index array `arr` can be
    used to index into the `xs` and `labels` array to shuffle them randomly. You can
    apply code to prevent the classifier from remembering the order of the data from
    listing 6.12 and use it to set up your training process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用训练样本的数量(`xs.shape[0]`，或`25000`)调用`arange`，然后使用NumPy的`random`模块及其`shuffle`方法`np.random.shuffle`来随机化这些索引。随机化的索引数组`arr`可以用来索引`xs`和`labels`数组，以随机打乱它们。你可以应用列表6.12中防止分类器记住数据顺序的代码，并使用它来设置你的训练过程。
- en: Listing 6.12 Preventing the classifier from remembering the order of the input
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 防止分类器记住输入的顺序
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Generates an array of indices of size 25,000
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成一个大小为25000的索引数组
- en: ❷ Shuffles the indices and stores the result in arr
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打乱索引并将结果存储在arr中
- en: ❸ Uses the shuffled index array arr to shuffle X and the labels
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用打乱的索引数组arr来打乱X和标签
- en: You’re almost ready to code your softmax classifier. The last step before model
    construction and training is getting your test data ready for accuracy evaluation,
    which you want to do after training and which you need for your softmax-predict
    function, which looks a teeny bit different. So that you don’t have to run through
    the whole cleaning process again on your reviews (you took care of that job in
    listing 6.2), you’ll need to create a function that assumes that you already have
    clean reviews but runs the Bag of Words model against those clean reviews separately
    to generate the test feature vectors of size 25000 × 5000.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你几乎准备好编写你的softmax分类器了。在模型构建和训练之前，最后一步是准备你的测试数据以进行准确度评估，你希望在训练后进行这项操作，并且你需要它来为你的softmax-predict函数提供支持，该函数看起来略有不同。因此，你不需要再次对你的评论进行整个清理过程（你在列表6.2中已经处理了这个任务），你需要创建一个函数，该函数假设你已经有了干净的评论，但会针对这些干净的评论单独运行Bag
    of Words模型以生成25000 × 5000大小的测试特征向量。
- en: In addition, you need to use the training vocabulary generated for the 25,000
    training reviews when testing your softmax classifier on unseen data, so you will
    prepare the test reviews and labels for evaluation now that you have a trained
    sentiment classifier. If you employ the `predict` function from listing 6.7, you
    can separate the call to `review_to_words` at the beginning and then perform the
    same steps. Listing 6.13 performs this task and generates the test feature vectors
    for the 25,000 test reviews. You’ll use those vectors to measure accuracy after
    training and to measure the performance of your new softmax classifier shortly
    thereafter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在测试softmax分类器对未见数据时，你需要使用为25000条训练评论生成的训练词汇，因此，现在你有了训练好的情感分类器，你可以准备测试评论和标签以进行评估。如果你使用列表6.7中的`predict`函数，你可以将`review_to_words`的调用分开，然后执行相同的步骤。列表6.13执行此任务并为25000条测试评论生成测试特征向量。你将使用这些向量在训练后测量准确度，并在不久之后测量你新softmax分类器的性能。
- en: Listing 6.13 Preparing test reviews and labels for evaluation after training
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.13 在训练后准备测试评论和标签以进行评估
- en: '[PRE17]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Assumption is that the review is clean, so create the test vocabulary and
    counts.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 假设评论是干净的，因此创建测试词汇表和计数。
- en: ❷ Runs the CountVectorizer and generates the 25000 × 5000 feature matrix
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行CountVectorizer并生成25000 × 5000的特征矩阵
- en: ❸ Gets the vocabulary of the provided test review set for evaluation
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取提供的测试评论集的词汇表以进行评估
- en: ❹ Figures out the intersection of the test vocabulary from the review with the
    actual full vocabulary
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 找出评论的测试词汇与实际完整词汇的交集
- en: ❺ All zeros for the 5000 feature vector except for the overlap indices that
    we have counts for
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对于5000个特征向量，除了我们计数的重叠索引外，所有值都是零
- en: ❻ Returns the feature vector for the provided individual review and training
    vocabulary
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回提供的单个评论和训练词汇的特征向量
- en: ❼ Creates a new array containing the set of 12,500 positive reviews and then
    appends text from the 12,500 negative reviews
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 创建一个包含12500条正面评论的新数组，然后追加12500条负面评论的文本
- en: ❽ Cleans the text from the test reviews
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 清理测试评论中的文本
- en: ❾ Creates the (25000,5000) feature vector to evaluate your classifier against
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 创建一个(25000,5000)的特征向量来评估你的分类器
- en: Armed with data to evaluate your classifier, you’re ready to begin the training
    process. As with your earlier TensorFlow training, you’ll define hyperparameters
    first. Arbitrarily train for 1,000 epochs, and use a learning rate of 0.01 with
    batch size 100\. Again, with some experimentation and hyperparameter tuning, you
    may find better starting values, but the parameters in listing 6.14 are good enough
    to experiment with. The training process can take up to 30 minutes on your laptop,
    so you’ll employ your friend TQDM again to make sure you are tracking the progress
    if you can step away from your computer while it learns. After your model is trained,
    you’ll use a `tf.train.Saver` to save the file, and you’ll print the test accuracy,
    which ends up being 0.81064, or 82%. Not bad!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借用于评估你的分类器的数据，你准备好开始训练过程了。与之前的 TensorFlow 训练一样，你首先定义超参数。任意训练 1,000 个周期，使用学习率为
    0.01，批量大小为 100。同样，通过一些实验和超参数调整，你可能找到更好的起始值，但列表 6.14 中的参数已经足够用于实验。训练过程在你的笔记本电脑上可能需要长达
    30 分钟，所以你将再次使用你的朋友 TQDM 来确保你在离开电脑时可以跟踪进度。在模型训练完成后，你将使用 `tf.train.Saver` 来保存文件，并打印测试准确率，最终结果是
    0.81064，或 82%。还不错！
- en: Listing 6.14 Training the softmax-regression classifier using batch training
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.14 使用批量训练训练 softmax 回归分类器
- en: '[PRE18]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Defines the hyperparameters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义超参数
- en: ❷ Defines the TensorFlow placeholders for input vectors, sentiment labels, weights,
    and biases
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义 TensorFlow 占位符，用于输入向量、情感标签、权重和偏差
- en: ❸ Creates the model using TensorFlow softmax and matrix multiplication Y = WX+b
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 TensorFlow softmax 和矩阵乘法 Y = WX+b 创建模型
- en: ❹ Defines the log loss cost and train operation, and accounts for when loss
    is zero preventing NaN issues
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义对数损失成本和训练操作，并考虑当损失为零时防止 NaN 问题
- en: ❺ Creates a saver to save the model graph and weights
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建一个保存模型图和权重的保存器
- en: ❻ Trains on a batch of 100 input Bag of Words vectors and sentiment labels
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在 100 个输入 Bag of Words 向量和情感标签的批次上训练
- en: ❼ Computes and prints the loss for each batch step of 250,000
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算并打印每个 250,000 步的批次损失
- en: ❽ Prints learned weights and biases
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 打印学习到的权重和偏差
- en: ❾ Prints the accuracy by evaluating against unseen 25,000 reviews and sentiment
    labels
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 通过评估未见过的 25,000 条评论和情感标签来打印准确率
- en: ❿ Saves the softmax-regression model
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 保存 softmax 回归模型
- en: Besides the accuracy, which is quite decent, you can see that the softmax classifier
    performed slightly worse than the logistic regression binary classifier. But don’t
    worry; you didn’t do any parameter tuning. Still, convince yourself of your classifier’s
    power by generating its ROC curve and computing the AUC to evaluate it. I’ll take
    you through that process next. Before you start on the ROC, though, you’ll need
    one new function to perform predictions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了相当不错的准确率之外，你还可以看到 softmax 分类器在二进制逻辑回归分类器上表现略差。但别担心；你没有进行任何参数调整。不过，通过生成 ROC
    曲线和计算 AUC 来评估它，你可以确信你的分类器的强大。我将在下一部分带你完成这个过程。不过，在开始 ROC 之前，你需要一个新函数来进行预测。
- en: This function differs only slightly from that shown in listing 6.7, and that
    subtle difference is one of the key takeaways that you’ll use to choose between
    using logistic regression or softmax regression in your future machine-learning
    tasks. In listing 6.7, the last step in the `predict` function used a threshold
    to determine whether your sigmoid output should be mapped to a `0` or `1`. As
    you’ll recall, sigmoid oscillates between `0` and `1` toward its edges. You need
    to define a threshold—usually, the median, `0.5`—to determine whether the points
    in the middle should fall on the `0` or `1` side. So the output of binary logistic
    regression is `0` or `1`, and what comes with it is a corresponding distance between
    the actual value and the defined threshold. You could think of that distance as
    showing confidence in the algorithm’s decision to classify the input as `1` or
    `0`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数与列表 6.7 中显示的函数只有细微差别，而这个细微差别是你在未来机器学习任务中选择使用逻辑回归或 softmax 回归时需要掌握的关键要点之一。在列表
    6.7 中，`predict` 函数的最后一步使用阈值来确定你的 sigmoid 输出是否应该映射到 `0` 或 `1`。正如你所回忆的，sigmoid 在其边缘在
    `0` 和 `1` 之间振荡。你需要定义一个阈值——通常是中位数，`0.5`——来确定中间的点应该落在 `0` 或 `1` 的哪一侧。因此，二元逻辑回归的输出是
    `0` 或 `1`，与之相伴的是实际值与定义的阈值之间的对应距离。你可以将这个距离视为显示算法对输入分类为 `1` 或 `0` 的决策的信心。
- en: Softmax logistic regression is a bit different. The output is a matrix of size`(num_
    samples, num_classes)` or size (rows, columns). When you give the algorithm one
    review or row and try to classify the input into two classes or columns, you get
    a matrix like `[[0.02 98.4]]`. This matrix indicates the algorithm is 0.02% confident
    that the input is a negative sentiment (the 0th column) and 98.4% confident that
    it is positive (the 1th column). For 25,000 reviews, you would get a matrix with
    25,000 rows of those two-column confidence values for each class. The softmax
    output isn’t a `0` or a `1`, as it is in binary logistic regression. The `predict_softmax`
    function needs to take that fact into account and figure out what the maximum
    value is in the column dimension.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 逻辑回归略有不同。输出是一个大小为 `(num_samples, num_classes)` 或 `(行, 列)` 的矩阵。当你给算法一个评论或行并尝试将输入分类到两个类别或列时，你会得到一个如
    `[[0.02 98.4]]` 的矩阵。这个矩阵表明算法有 0.02% 的信心认为输入是负面情绪（第 0 列）和 98.4% 的信心认为它是正面（第 1 列）。对于
    25,000 条评论，你会得到一个包含每个类别的两个列置信值的 25,000 行矩阵。Softmax 输出不是 `0` 或 `1`，就像在二进制逻辑回归中那样。`predict_softmax`
    函数需要考虑这个事实，并找出列维度上的最大值。
- en: NumPy provides the `np.argmax` function to do precisely that. You provide a
    NumPy array as the first parameter; the second parameter identifies which dimensional
    axis to test. The function returns the axis index with the maximum value. For
    `np.argmax([[0.02 98.4]],1),` the function would yield `1.` Listing 6.15 is similar
    to listing 6.7; the only difference is the way you interpret the output with `np.argmax`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 提供了 `np.argmax` 函数来精确地做到这一点。你提供 NumPy 数组作为第一个参数；第二个参数标识要测试的维度轴。该函数返回具有最大值的轴索引。对于
    `np.argmax([[0.02 98.4]],1)`，该函数将返回 `1.` 列表 6.15 与列表 6.7 类似；唯一的区别是你如何使用 `np.argmax`
    解释输出。
- en: Listing 6.15 Creating the `predict` function for your softmax-regression classifier
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.15 为你的 softmax 回归分类器创建 `predict` 函数
- en: '[PRE19]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Cleans the review
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清理评论
- en: ❷ Creates the test vocabulary and counts
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建测试词汇表并计数
- en: ❸ Applies the Bag of Words model and generates the vocabulary for the review
    to test
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用词袋模型并为测试评论生成词汇表
- en: ❹ Figures out the intersection of the test vocabulary from the review with the
    actual full vocabulary
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 确定评论中的测试词汇与实际完整词汇的交集
- en: ❺ All zeros for the 5,000-feature vector except for the overlap indices that
    we have counts for
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对于 5,000 个特征的向量，除了我们计数重叠索引之外，所有值都是零
- en: ❻ Makes the prediction and gets the softmax matrix back
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 进行预测并获取 softmax 矩阵
- en: ❼ The predicted class is 0-axis for negative or 1-axis for positive with np.argmax.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 `np.argmax` 获取测试情感级别
- en: Armed with your new `predict_softmax` function, you can generate the ROC curve
    to evaluate your classifier, which is similar to listing 6.10\. Instead of calling
    `predict` on each review, you load the saved softmax-regression model; apply its
    predictions to the entire test review dataset, with the learned weights and biases;
    and then use `np.argmax` to obtain the set of predictions for all 25,000 reviews
    at the same time. The output ROC curve follows listing 6.16 and demonstrates 81%
    accuracy against the test data when your softmax classifier is used. If you’ve
    got some extra time and cycles, play around with those hyperparameters; see whether
    you can tune them to achieve better results than your logistic regression classifier.
    Isn’t baking off your machine-learning algorithms fun?
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有了你新的 `predict_softmax` 函数，你可以生成 ROC 曲线来评估你的分类器，这与列表 6.10 类似。不是对每个评论调用 `predict`，而是加载保存的
    softmax 回归模型；将其预测应用于整个测试评论数据集，使用学习到的权重和偏差；然后使用 `np.argmax` 同时获取所有 25,000 条评论的预测集。输出的
    ROC 曲线如列表 6.16 所示，当使用 softmax 分类器时，对测试数据的准确率为 81%。如果你有一些额外的时间和循环，可以尝试调整这些超参数；看看你是否可以将它们调整到比你的逻辑回归分类器更好的结果。不是很有趣吗？
- en: Listing 6.16 Generating the ROC curve and evaluating your softmax classifier
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.16 生成 ROC 曲线和评估你的 softmax 分类器
- en: '[PRE20]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Loads and restores the softmax-regression model
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载并恢复 softmax 回归模型
- en: ❷ Predicts the sentiment of all 25,000 reviews at the same time and use np.argmax
    to generate all sentiments
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 同时预测所有 25,000 条评论的情感，并使用 `np.argmax` 生成所有情感
- en: ❸ Uses np.argmax to obtain the testing sentiment levels
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 `np.argmax` 获取测试情感级别
- en: ❹ Sets up the predict sentiments to test
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置预测情感以进行测试
- en: ❺ Creates the ROC curve and AUC, using the resultant false positive rate and
    true positive rate
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建 ROC 曲线和 AUC，使用结果中的假阳性率和真阳性率
- en: ❻ Generates the plot for the baseline classifier and your softmax classifier
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 生成基线分类器和你的softmax分类器的图表
- en: You can see the result of running listing 6.16 in figure 6.7, which depicts
    the ROC curve evaluating your classifier. In the spirit of competition, why not
    submit your results to the original Bag of Words Meets Bags of Popcorn challenge
    on the machine-learning competition platform, Kaggle? The process is amazingly
    simple, as I’ll show you in section 6.6.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图6.7中看到运行列表6.16的结果，它描绘了评估你的分类器的ROC曲线。本着竞争的精神，为什么不将你的结果提交到机器学习竞赛平台Kaggle上的原始“单词袋遇见爆米花袋”挑战赛呢？这个过程非常简单，就像我在第6.6节中要展示的那样。
- en: '![CH06_F07_Mattmann2](../Images/CH06_F07_Mattmann2.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F07_Mattmann2](../Images/CH06_F07_Mattmann2.png)'
- en: Figure 6.7 The ROC curve and AUC curve for the softmax-regression sentiment
    classifier, which performs slightly worse than your logistic regression classifier,
    with 81% accuracy
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 softmax回归情感分类器的ROC曲线和AUC曲线，其表现略逊于你的逻辑回归分类器，准确率为81%
- en: 6.6 Submitting your results to Kaggle
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 将你的结果提交到Kaggle
- en: The Bag of Words Meets Bags of Popcorn challenge ended some years ago, but Kaggle
    will still let you upload your machine-learning algorithm to see where it places
    on the leaderboard. This simple process shows you how well your machine-learning
    measures up. The Python code with all the work you’ve already done is fairly trivial.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: “单词袋遇见爆米花袋”挑战赛几年前就结束了，但Kaggle仍然允许你上传你的机器学习算法，看看它在排行榜上的位置。这个简单的流程展示了你的机器学习表现如何。你之前已经完成的所有工作的Python代码相当简单。
- en: Call your `predict` function from the original binary logistic-regression sentiment
    classifier. Because it performed better than the softmax one—87% compared with
    81%—you’ll want to submit your best work. If you use the original Kaggle test
    CSV that you loaded in listing 6.2, you can generating the resulting Pandas dataframe
    test and run your `predict` function over it. Then you can use another great feature
    of Pandas to add an entire column, mapping it to the associated rows. As long
    as the new column has the same number of columns, you can use a one-line function
    to create a new dataframe with the additional column. Given that new dataframe,
    you can use Pandas’s built-in function to output a dataframe to a CSV and obtain
    your `Bag_of_Words_model.csv` output to upload to Kaggle. The code in listing
    6.17 generates that CSV file.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始的二进制逻辑回归情感分类器中调用你的`predict`函数。因为它比softmax分类器表现更好——87%对81%——你将想要提交你的最佳作品。如果你使用原始的Kaggle测试CSV文件，你可以生成结果Pandas数据框test，并在其上运行你的`predict`函数。然后你可以使用Pandas的另一个出色功能来添加一个完整的列，将其映射到相关的行。只要新列有相同数量的列，你就可以使用一个单行函数创建一个新的数据框，并添加额外的列。有了这个新的数据框，你可以使用Pandas的内置函数将数据框输出到CSV，并获取你的`Bag_of_Words_model.csv`输出以上传到Kaggle。列表6.17中的代码生成了那个CSV文件。
- en: Listing 6.17 Generating a Kaggle submission of your sentiment classification
    results
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.17 生成Kaggle提交的情感分类结果
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Generates Kaggle submission empty list and appends reviews one by one
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成Kaggle提交的空列表并逐个添加评论
- en: ❷ Calls the predict function to get a sentiment of 1 or 0
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调用predict函数以获取1或0的情感
- en: ❸ Copies the results to a Pandas dataframe with an id column and a sentiment
    column
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将结果复制到包含id列和情感列的Pandas数据框中
- en: ❹ Uses Pandas to write the comma-separated output file
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用Pandas写入逗号分隔的输出文件
- en: 'Now that you’ve generated the Kaggle submission, you can upload the results
    CSV file to Kaggle. Assuming that you have already created your account, you can
    visit [https:// www.kaggle.com/c/word2vec-nlp-tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial)
    to make your submission. Use the following instructions, which you can repeat
    for future Kaggle competitions:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经生成了Kaggle提交，你可以上传结果CSV文件到Kaggle。假设你已经创建了你的账户，你可以访问[https://www.kaggle.com/c/word2vec-nlp-tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial)来提交你的作品。使用以下说明，你可以为未来的Kaggle比赛重复使用：
- en: Click the blue Join the Competition function.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击蓝色的“加入比赛”功能。
- en: Click the blue Make a Late Submission button.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击蓝色的“提交迟交作业”按钮。
- en: Click the window area, do a file select after you have run listing 6.17 and
    generated the Bag_of_Words_model.csv file, and then select that file.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击窗口区域，在运行列表6.17并生成Bag_of_Words_model.csv文件后进行文件选择，然后选择该文件。
- en: Click the blue Make Submission button to submit your predictions and see where
    you stand on the leaderboard.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击蓝色的“提交预测”按钮以提交你的预测并查看你在排行榜上的位置。
- en: I told you it was easy, right? You can see the results of making your Kaggle
    submission in figure 6.8.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是告诉你很容易吗？你可以在图6.8中看到你提交Kaggle的结果。
- en: '![CH06_F08_Mattmann2](../Images/CH06_F08_Mattmann2.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Mattmann2](../Images/CH06_F08_Mattmann2.png)'
- en: Figure 6.8 Making your Kaggle submission
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 提交你的Kaggle结果
- en: That’s it! You’ve submitted your results to Kaggle and entered your results
    for the world of machine-learning experts to see. Those reasonable results should
    place you squarely in the top middle of the competition. (Too bad it’s over.)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经将你的结果提交到了Kaggle，并且让你的结果进入了机器学习专家的世界，供他们查看。这些合理的成果应该让你在竞赛中稳居中上水平。（遗憾的是，比赛已经结束了。）
- en: You’ve done excellent work applying logistic regression and softmax regression
    to text analysis. You’ll spend the next few chapters finding out how you can learn
    from data without labels, using unsupervised approaches.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你在将逻辑回归和softmax回归应用于文本分析方面做得非常出色。在接下来的几章中，你将了解到如何使用无监督方法从无标签的数据中学习。
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can transform text into n-dimensional features by creating a vocabulary
    and counting the occurrence of those words.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过创建词汇表并计算这些词的出现次数，将文本转换为n维特征。
- en: Using text and word frequency, you can apply the famous Bag of Words model from
    NLP to represent sentiment over a corpus of text reviews of movies from IMDb.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过文本和词频，你可以将自然语言处理中的著名“词袋”模型应用于代表IMDb电影评论文本语料库中的情感。
- en: Using Pandas dataframes, you can use a Python machine-learning library for representing
    matrices and vectors as in-memory tables to store classifier output and associated
    text.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Pandas数据框，你可以使用Python机器学习库将矩阵和向量作为内存中的表来表示，以存储分类器输出和相关文本。
- en: You built a TensorFlow-based sentiment classifier for text-based movie reviews
    using logistic regression and the associated process. You also built a TensorFlow-based
    sentiment classifier using logistic regression with softmax. It varies in both
    the data preparation steps but also in how you interpret the model response.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你使用逻辑回归和相关的流程构建了一个基于TensorFlow的文本电影评论情感分类器。你还使用逻辑回归和softmax构建了一个基于TensorFlow的情感分类器。它在数据准备步骤以及如何解释模型响应方面都有所不同。
- en: Measuring classification accuracy is typically done by identifying and counting
    true positives, false positives, true negatives, false negatives.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量分类准确度通常是通过识别和计数真正的阳性、假阳性、真正的阴性和假阴性来完成的。
- en: Computing an ROC curve allows you to measure the effectiveness of both of your
    trained classifiers and their effectiveness.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算ROC曲线可以让你衡量你训练的两个分类器的有效性及其有效性。
- en: You submitted your results to the Kaggle challenge for Movie Reviews to see
    how well you scored against other machine-learning researchers trying to automatically
    predict sentiment from text.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将你的结果提交到了Kaggle的“电影评论”挑战，以查看你的得分与其他试图从文本中自动预测情感的机器学习研究人员相比如何。
