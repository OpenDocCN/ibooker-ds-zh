- en: Chapter 9\. Classic Data Design Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 经典数据设计模式
- en: 'This chapter discusses some of the most fundamental and classic data design
    patterns used in the vast majority of big data solutions. Even though these are
    simple design patterns, they are useful in solving many common data problems,
    and I’ve used many of them in examples in this book. In this chapter, I will present
    PySpark implementations of the following design patterns:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在绝大多数大数据解决方案中使用的一些最基本和经典的数据设计模式。尽管这些是简单的设计模式，但它们在解决许多常见的数据问题中很有用，我在本书的示例中使用了许多这些模式。在本章中，我将介绍以下设计模式的PySpark实现：
- en: Input-Map-Output
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-映射-输出
- en: Input-Filter-Output
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-过滤-输出
- en: Input-Map-Reduce-Output
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-映射-减少-输出
- en: Input-Multiple-Maps-Reduce-Output
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Input-Multiple-Maps-Reduce-Output
- en: Input-Map-Combiner-Reduce-Output
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-映射-合并器-减少-输出
- en: Input-MapPartitions-Reduce-Output
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-映射分区-减少-输出
- en: Input-Inverted-Index-Pattern-Output
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入-反向索引-模式-输出
- en: Before we get started, however, I’d like to address the question of what I mean
    by “design patterns.” In computer science and software engineering, given a commonly
    occurring problem, a design pattern is a reusable solution to that problem. It’s
    a template or best practice for how to solve a problem, not a finished design
    that can be transformed directly into code. The patterns presented in this chapter
    will equip you to handle a wide range of data analysis tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在我们开始之前，我想解释一下“设计模式”的含义。在计算机科学和软件工程中，针对常见问题，设计模式是对该问题的可重用解决方案。它是解决问题的模板或最佳实践，而不是可以直接转换为代码的成品设计。本章介绍的模式将使您能够处理各种数据分析任务。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The data design patterns discussed in this chapter are basic patterns. You
    can create your own, depending on your requirements. For additional examples,
    see [“MapReduce: Simplified Data Processing on Large Clusters”](https://oreil.ly/jS7MV)
    by Jeffrey Dean and Sanjay Ghemawat.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '本章讨论的数据设计模式是基本模式。您可以根据自己的需求创建自己的模式。有关更多示例，请参见Jeffrey Dean和Sanjay Ghemawat的《MapReduce:
    简化大规模集群上的数据处理》（https://oreil.ly/jS7MV）。'
- en: Input-Map-Output
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入-映射-输出
- en: 'Input-Map-Output is the simplest design pattern for data analysis: as illustrated
    in [Figure 9-1](#input_map_putput_design_pattern), you read the input from a set
    of files, then apply a series of functions to each record, and finally produce
    the desired output. There is no restriction on what a mapper can create from its
    input: it can create a set of new records or (key, value) pairs.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-映射-输出是数据分析的最简单设计模式：如[图 9-1](#input_map_putput_design_pattern)所示，您从一组文件中读取输入，然后对每条记录应用一系列函数，最后生成所需的输出。映射器可以根据其输入创建任何内容，没有限制：它可以创建一组新记录或（键，值）对。
- en: '![daws 0901](Images/daws_0901.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0901](Images/daws_0901.png)'
- en: Figure 9-1\. The Input-Map-Output design pattern
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1. 输入-映射-输出设计模式
- en: No reduction is involved, but sometimes the map phase is used to clean and reformat
    data. This a very common design pattern used to change the format of input data
    and generate output data, which can be used by other mappers and reducers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 没有减少，但有时会使用映射阶段来清理和重新格式化数据。这是一种非常常见的设计模式，用于改变输入数据的格式并生成输出数据，其他映射器和减少器可以使用。
- en: RDD Solution
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD解决方案
- en: Sometimes the map phase is used to clean and reformat data before generating
    (key, value) pairs to be consumed by reducers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有时映射阶段用于在生成供减少器使用的(键，值)对之前清理和重新格式化数据。
- en: 'Consider a scenario where the input records have a gender field that can contain
    values such as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样的情况，输入记录具有可以包含诸如以下值的性别字段：
- en: 'Female representation: `"0", "f", "F", "Female", "female"`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 女性表示：`"0", "f", "F", "Female", "female"`
- en: 'Male representation: `"1", "m", "M", "Male", "male"`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 男性表示：`"1", "m", "M", "Male", "male"`
- en: 'and you want to normalize the gender field as `{"female", "male", "unknown"}`.
    Let’s assume that each record has the following format:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要将性别字段标准化为`{"female", "male", "unknown"}`，假设每条记录的格式如下：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following function can facilitate the `map()` transformation and will create
    a triplet of `(user_id, normalized_gender, address)` per input record:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的函数可以促进`map()`转换，并将每条输入记录创建为`(user_id, normalized_gender, address)`的三元组：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Given a source `rdd` as an `RDD[String]`, then your mapper transformation will
    be as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定源`rdd`作为`RDD[String]`，则您的映射器转换将如下所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Another scenario might be analyzing movie rating records of the form `<user_id><,><movie_id><,><rating>`,
    where your goal is to create a (key, value) pair of `(<movie_id>, (<user_id>,
    <rating>)` per record. Further assume that all ratings will be converted to integer
    numbers. You can use the following mapper function for this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个场景可能是分析形式为`<user_id><,><movie_id><,><rating>`的电影评分记录，你的目标是为每条记录创建一个`(<movie_id>,
    (<user_id>, <rating>)`的键值对。 进一步假设所有评分将转换为整数。 你可以使用以下的映射器函数：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'What if you want to map a single input record/element into multiple target
    elements, dropping (filtering out) records/elements where appropriate? To map
    a single record into multiple target elements, Spark offers the `flatMap()` transformation
    for this; it works on a single element (like `map()`) and produces multiple target
    elements. So, if your input is `RDD[V]` and you want to map each `V` into a set
    of elements of type `T`, you can do this with `flatMap()` as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将单个输入记录/元素映射到多个目标元素，并适当地丢弃（过滤掉）记录/元素，该怎么办？ Spark提供了`flatMap()`转换来实现这一点；它在单个元素上工作（类似于`map()`）并生成多个目标元素。
    因此，如果你的输入是`RDD[V]`，你想将每个`V`映射为一组类型为`T`的元素，你可以像下面这样使用`flatMap()`：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For example, if for input record `v` you create `t = [t1, t2, t3]`, then `v`
    will be mapped to three elements of the `target_rdd` as `t1`, `t2`, and `t3`.
    If `t=[]`—an empty list—then no element will be created in the `target_rdd`: `v`
    is filtered out.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果对于输入记录`v`，你创建了`t = [t1, t2, t3]`，那么`v`将映射到`target_rdd`的三个元素`t1`、`t2`和`t3`。
    如果`t=[]`——一个空列表——那么`target_rdd`将不会创建任何元素：`v`会被过滤掉。
- en: 'As this example suggests, if you want to map and filter at the same time, mapping
    some records and filtering others, you can implement this with `flatMap()` as
    well. For example, suppose you have records in the following format:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，如果你想同时映射和过滤，即映射一些记录并过滤其他记录，你也可以使用单个`flatMap()`转换来实现这一点。 例如，假设你有以下格式的记录：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Your goal is to keep only the records consisting of two words, separated by
    a comma (that is, bigrams); you want to drop (filter out) all the other records.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是只保留由逗号分隔的两个单词组成的记录（即二元组），你想要丢弃（过滤掉）所有其他记录。
- en: 'Consider this source RDD:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个源RDD：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, `rdd` has three elements. You would like to keep `''w1,w2''`, `''w3,w4''`,
    `''w5,w6''`, `''w7,w8''`, and `''w10,w11''` but drop `''w9''` (since this is not
    a bigram). The following PySpark snippet shows how to achieve this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`rdd`有三个元素。 你想保留`'w1,w2'`、`'w3,w4'`、`'w5,w6'`、`'w7,w8'`和`'w10,w11'`，但是丢弃`'w9'`（因为这不是一个二元组）。
    以下的PySpark片段展示了如何实现这一点：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As this example shows, you can map the records you want to keep into multiple
    target elements and filter out the ones you don’t want to keep at the same time
    using a single `flatMap()` transformation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，你可以将要保留的记录映射为多个目标元素，并同时使用单个`flatMap()`转换过滤掉你不想保留的记录。
- en: DataFrame Solution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame解决方案
- en: 'Spark has an `RDD.map()` function, but it does not have this `map()` function
    for DataFrames. Spark’s DataFrame does not have an explicit `map()` function,
    but we can achieve the `map()` equivalency in many ways: we can add new columns
    by applying `DataFrame.withColumn()` and drop existing columns by `DataFrame.drop()`.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有一个`RDD.map()`函数，但是DataFrame没有这个`map()`函数。 Spark的DataFrame没有显式的`map()`函数，但我们可以通过多种方法实现等价的`map()`：我们可以通过`DataFrame.withColumn()`添加新列，并通过`DataFrame.drop()`删除现有列。
- en: 'Consider a DataFrame as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个DataFrame：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Suppose we want to calculate total weekly pay by adding `overtime_hours` to
    `weekly_pay`. Therefore, we want to create a new column `total weekly pay` based
    on the values of two columns: `overtime_hours` and `weekly_pay`. Assume that over
    time rate is $20 per hour.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想通过将`overtime_hours`加到`weekly_pay`来计算总周薪。 因此，我们希望基于`overtime_hours`和`weekly_pay`的值创建一个新列`total
    weekly pay`。 假设加班费率为每小时$20。
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To keep all the columns, do the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要保留所有列，请执行以下操作：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Essentially you have to map the row to a tuple containing all of the existing
    columns and add in the new column(s).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，你必须将行映射到一个包含所有现有列并添加新列的元组。
- en: If your columns are too many to enumerate, you could also just add a tuple to
    the existing row.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的列太多而无法枚举，你也可以只向现有行添加一个元组。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can also add a `total_pay` column using `DataFrame.withColumn()`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`DataFrame.withColumn()`添加一个`total_pay`列：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Flat Mapper functionality
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flat Mapper功能
- en: Spark’s DataFrame does not have a `flatMap()` transformation (to flatten one
    element into many target elements), but instead it offers the `explode()` function,
    which returns a new row for each element in the given `column` (expressed as a
    list or dictionary) and uses the default column name `col` for elements in the
    array and key and value for elements in the dictionary unless specified otherwise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 DataFrame 没有`flatMap()`转换（将一个元素展平为多个目标元素），而是提供了`explode()`函数，该函数返回给定列（表示为列表或字典）中每个元素的新行，并对数组中的元素使用默认列名`col`，对字典中的元素使用键和值，除非另有指定。
- en: Below is a complete example, which shows how to use `explode()` function as
    an equivalent to `RDD.flatMap()` transformation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个完整的示例，展示如何使用`explode()`函数作为等效于`RDD.flatMap()`转换的方法。
- en: Let’s first create a DataFrame, where two column are lists.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个包含两列列表的 DataFrame。
- en: 'Next, we look at exploding multiple columns for a given DataFrame. Note that
    only one generator is allowed per `select` clause: this means that you can not
    explode two columns at the same time (but you can explode them iteratively one-by-one).
    The following example shows how to explode two columns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看如何针对给定的 DataFrame 展开多列。请注意，每个`select`子句只允许一个生成器：这意味着您不能同时展开两列（但可以逐个迭代地展开它们）。以下示例展示了如何展开两列：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next we explode on the `languages` column, which is an array:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看一下`languages`列，这是一个数组：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, when exploding a column, if a column is a empty list, then that
    is dropped from exploding result (`tex` and `max` are dropped since they have
    an associated empty lists). Note that `ted` and `dan` were dropped since the exploded
    column value was an empty list.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当展开一列时，如果某列是空列表，则该列会从展开结果中删除（`tex` 和 `max` 因具有关联的空列表而被删除）。请注意，`ted` 和 `dan`
    被删除，因为展开的列值为空列表。
- en: 'Next, we explode on the `education` column:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下`education`列：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that name `max` is dropped since the exploded column value was an empty
    list.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于展开的列值为空列表，名称为`max`的列被删除。
- en: Input-Filter-Output
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入-过滤-输出
- en: The Input-Filter-Output data design pattern, illustrated in [Figure 9-2](#input_filter_output_desing_pattern),
    is a simple pattern that lets you keep records that satisfy your data requirements
    while removing the unwanted records. You read the input from a set of files, then
    apply one or more filter functions to each record, keeping the records that satisfy
    the Boolean predicate and dropping the others.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](#input_filter_output_desing_pattern)中所示的输入-过滤-输出数据设计模式是一种简单的模式，它允许您保留满足数据要求的记录，同时移除不需要的记录。您可以从一组文件中读取输入，然后对每条记录应用一个或多个过滤函数，保留满足布尔谓词的记录并丢弃其他记录。'
- en: '![daws 0902](Images/daws_0902.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0902](Images/daws_0902.png)'
- en: Figure 9-2\. The Input-Filter-Output design pattern
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 输入-过滤-输出设计模式
- en: This is a useful design pattern for situations in which your dataset is large
    and you want to take a subset of this data to focus in on and maybe perform a
    follow-on analysis.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在数据集大且您希望选择其中一部分数据进行关注和可能进行后续分析的情况下非常有用的设计模式。
- en: A simple scenario is reading input records consisting of URLs, keeping the valid
    ones while discarding the nonvalid URLs. This design pattern can be implemented
    with RDDs and DataFrames.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的场景是读取由 URL 组成的输入记录，保留有效的 URL 并丢弃无效的 URL。这种设计模式可以通过 RDD 和 DataFrame 实现。
- en: 'Here are a few sample records:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例记录：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO1-1)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO1-1)'
- en: Valid URL
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有效 URL
- en: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO1-2)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO1-2)'
- en: Invalid URL
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 无效 URL
- en: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO1-3)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO1-3)'
- en: Valid URL
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有效 URL
- en: '[![4](Images/4.png)](#co_classic_data_design_patterns_CO1-4)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_classic_data_design_patterns_CO1-4)'
- en: Invalid URL
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 无效 URL
- en: RDD Solution
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 解决方案
- en: 'This design pattern can be easily implemented using the `RDD.filter()` function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计模式可以通过`RDD.filter()`函数轻松实现：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: DataFrame Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 解决方案
- en: 'Alternatively, you can use the `DataFrame.filter()` function to keep the desired
    records and drop the undesired records:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`DataFrame.filter()`函数来保留所需的记录并丢弃不需要的记录：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: DataFrame Filter
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 过滤
- en: Spark’s `filter()` function is used to filter the elements/rows from RDD/DataFrame
    based on the given condition. For DataFrames, you may also use a `where()` clause
    instead of the `filter()` function if you are coming from a SQL background. Both
    of these functions (`filter()` and `where()`) operate exactly the same. The goal
    of `filter()` and `where()` are to keep the desired elements/rows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 `filter()` 函数用于根据给定条件过滤 RDD/DataFrame 中的元素/行。对于 DataFrame，如果您来自 SQL
    背景，也可以使用 `where()` 子句代替 `filter()` 函数。这两个函数（`filter()` 和 `where()`）的操作完全相同。`filter()`
    和 `where()` 的目标是保留所需的元素/行。
- en: 'Consider a DataFrame as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 DataFrame 如下：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Suppose we want to keep rows where `weekly_pay` is greater than 490.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想保留 `weekly_pay` 大于 490 的行：
- en: 'Let’s first use `filter()`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用 `filter()`：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can achieve the same functionality by the `where` clause:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `where` 子句实现相同的功能：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `filter()` can be used on single and multiple conditions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()` 可以用于单个和多个条件：'
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Input-Map-Reduce-Output
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入-映射-Reduce-输出
- en: The Input-Map-Reduce-Output design pattern, illustrated in [Figure 9-3](#input_map_reduce_output_design_pattern),
    is the most common design pattern for aggregation operations, such as finding
    the sum or average of values by key.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-映射-Reduce-输出设计模式，如图 [9-3](#input_map_reduce_output_design_pattern) 所示，是聚合操作（例如按键找到值的总和或平均值）中最常见的设计模式。
- en: RDD Solution
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 解决方案
- en: 'Spark offers the following powerful solutions for implementing this design
    pattern, many different combinations of which can be used to solve data problems:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了以下强大的解决方案来实现这种设计模式，可以使用多种不同的组合来解决数据问题：
- en: 'Map phase: `map()`, `flatMap()`, `mapPartitions()`, `filter()`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射阶段：`map()`、`flatMap()`、`mapPartitions()`、`filter()`
- en: 'Reduce phase: `reduceByKey()`, `groupByKey()`, `aggregateByKey()`, `combineByKey()`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reduce 阶段：`reduceByKey()`、`groupByKey()`、`aggregateByKey()`、`combineByKey()`
- en: '![daws 0903](Images/daws_0903.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0903](Images/daws_0903.png)'
- en: Figure 9-3\. The Input-Map-Reduce-Output design pattern
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 输入-映射-Reduce-输出设计模式
- en: 'This is the simplest MapReduce design pattern: read data, perform a map transformation—usually
    creating (key, value) pairs—aggregate (sum, average, etc.) all of the values for
    the same key, then save the output.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的 MapReduce 设计模式：读取数据，执行映射转换（通常创建（键，值）对），对相同键的所有值进行聚合（求和、平均值等），然后保存输出。
- en: 'Suppose you have records with the format `<name><,><age><,><salary>` and you
    want to compute the average salary per age group, where the age groups are defined
    as `0-15`, `16-20`, `21-25`, …, `96-100`. First, you need to read the input and
    create an RDD/DataFrame. The mapper will then process one record at a time and
    create (key, value) pairs, where the key is an age group and the value is a salary.
    For example, if our record is `alex,22,45000`, then the mapper will create the
    pair `(''21-25'', 45000)` since the age `22` falls into the age group `''21-25''`.
    The mapper function can be expressed as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有格式为 `<name><,><age><,><salary>` 的记录，并且希望计算每个年龄组的平均工资，其中年龄组定义为 `0-15`、`16-20`、`21-25`、…、`96-100`。首先，需要读取输入并创建一个
    RDD/DataFrame。然后，mapper 将逐个处理记录并创建（键，值）对，其中键是一个年龄组，值是一个工资。例如，如果我们的记录是 `alex,22,45000`，那么
    mapper 将创建配对 `('21-25', 45000)`，因为年龄 `22` 属于年龄组 `'21-25'`。Mapper 函数可以表达为：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Then, the reducer will group the keys by age groups (`0-15`, `16-20`, etc.),
    aggregate the values in each group, and find the average salary per group.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，reducer 将按年龄组（`0-15`、`16-20` 等）分组键，聚合每组中的值，并找到每组的平均工资。
- en: 'Say you have the following input:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有以下输入：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The mapper will generate the following (key, value) pairs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper 将生成以下（键，值）对：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then the reducer will group the values for each key:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 reducer 将为每个键分组值：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Grouping by key can be easily implemented with Spark’s `groupByKey()` transformation.
    Using `groupByKey()`, we cay write the reducer as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 的 `groupByKey()` 转换可以轻松实现按键分组。使用 `groupByKey()`，我们可以将 reducer 写成：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can calculate the average per age group:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算每个年龄组的平均值：
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This can be accomplished by another simple mapper:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过另一个简单的 mapper 实现：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you want to use combiners (Spark uses combiners automatically in `reduceByKey()`),
    the mapper will instead generate the following (key, value) pairs, where the value
    is `(sum, count)`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用组合器（Spark 在 `reduceByKey()` 中自动使用组合器），mapper 将生成以下（键，值）对，其中值为 `(sum, count)`：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The reason for creating `(sum, count)` as a value is to guarantee that the reducer
    function is associative and commutative. If your reducer function does not follow
    these two algebraic rules, then Spark’s `reduceByKey()` will not produce the correct
    semantics when the input data is spread across multiple partitions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`(sum, count)`作为值的原因是为了保证Reducer函数是可结合的和可交换的。如果您的Reducer函数不遵循这两个代数规则，那么Spark的`reduceByKey()`在输入数据分布在多个分区时将无法产生正确的语义。
- en: 'Given an `RDD[(key, (sum, count))]`, using Spark’s `reduceByKey()`—note that
    this reducer works on a partition-by-partition basis and uses combiners as well—we
    may write the reducer as:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`RDD[(key, (sum, count))]`，使用Spark的`reduceByKey()` — 注意，这个Reducer基于分区的基础并且也使用组合器，我们可以将Reducer编写为：
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The reducer will group the values by their associated keys:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer将根据它们关联的键对值进行分组：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, the average per age group can be caluculated by another simple mapper:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以通过另一个简单的映射器计算每个年龄组的平均值：
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It’s also possible to implement this design pattern using a combination of
    Spark’s `map()` and `combineByKey()` transformations. The map phase is exactly
    as presented previously. Using the `create_key_value_pair()` function, it will
    create the following (key, value) pairs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用Spark的`map()`和`combineByKey()`转换来实现这种设计模式的组合。映射阶段与之前介绍的完全相同。使用`create_key_value_pair()`函数，它将创建以下(key,
    value)对：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s assume that these (key, value) pairs are denoted by `age_group_rdd`.
    Then we can perform the reduction using a pair of `combineByKey()` and `mapValues()`
    transformations:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这些(key, value)对由`age_group_rdd`表示。然后我们可以使用一对`combineByKey()`和`mapValues()`转换来进行减少：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO2-1)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO2-1)'
- en: Create `C` as `(sum-of-salaries, count-of-salaries)`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`C`为`(工资总和, 工资数量)`。
- en: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO2-2)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO2-2)'
- en: Merge a salary into `C`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将工资合并到`C`中。
- en: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO2-3)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO2-3)'
- en: Combine two `C`s (from different partitions) into a single `C`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个`C`（来自不同分区）合并为一个单一的`C`。
- en: Tip
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Notice that `reduceByKey()` is a special case of `combineByKey()`. For `reduceByKey()`,
    the source and target RDDs must be of the form `RDD[(K, V])`, while for `combineByKey()`
    the source RDD can be `RDD[(K, V)]` and the target RDD can be `RDD[(K, C)]`, where
    `V` and `C` may be different data types. For example, `V` can be `Integer`, while
    `C` can be `(Integer, Integer)`. In Spark, the `combineByKey()` transformation
    is the most general and powerful reducer for (key, value) datasets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`reduceByKey()`是`combineByKey()`的一个特例。对于`reduceByKey()`，源RDD和目标RDD必须是`RDD[(K,
    V])`的形式，而对于`combineByKey()`，源RDD可以是`RDD[(K, V)]`，目标RDD可以是`RDD[(K, C)]`，其中`V`和`C`可以是不同的数据类型。例如，`V`可以是`Integer`，而`C`可以是`(Integer,
    Integer)`。在Spark中，`combineByKey()`转换是对(key, value)数据集最通用和强大的Reducer。
- en: DataFrame Solution
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame解决方案
- en: PySpark’s Dataframe offers comprehensive functionality for reduction transformations.
    You may use `Dataframe.groupby(*cols)`, which groups the DataFrame using the specified
    columns so we can run aggregation on them. The other option is to register your
    Dataframe as a table (of rows and named columns) and then use the power of SQL
    to `GROUP` `BY` and aggregate the dsired columns.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的DataFrame为减少转换提供了全面的功能。您可以使用`Dataframe.groupby(*cols)`，它使用指定的列对DataFrame进行分组，以便对它们进行聚合。另一个选项是将您的DataFrame注册为表（行和命名列）然后利用SQL的功能`GROUP
    BY`和聚合所需的列。
- en: The following example shows how to use the `groupBy()` function.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了如何使用`groupBy()`函数。
- en: 'First, let’s create a DataFrame:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个DataFrame：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we apply grouping and aggregation functions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应用分组和聚合函数：
- en: 'Describe your DataFrame:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述您的DataFrame：
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Use `groupBy()` on a DataFrame:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DataFrame上使用`groupBy()`：
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Input-Multiple-Maps-Reduce-Output
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入-多个映射-减少-输出
- en: 'The Input-Multiple-Maps-Reduce-Output design pattern involves multiple maps,
    joins, and reductions. This design pattern is also known as the *reduce-side join*,
    because the reducer is responsible for performing the join operation. To help
    you understand this design pattern, let me provide an example. Suppose we have
    the following two inputs, a Movies table and a Ratings table:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-多个映射-减少-输出设计模式涉及多个映射、连接和减少。这种设计模式也被称为*减少端连接*，因为Reducer负责执行连接操作。为了帮助您理解这种设计模式，让我举个例子。假设我们有以下两个输入，一个是电影表，一个是评分表：
- en: '| Movie-ID | Movie-Name |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 电影ID | 电影名称 |'
- en: '| --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 100 | Lion King |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 狮子王 |'
- en: '| 200 | Star Wars |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 星球大战 |'
- en: '| 300 | Fiddler on the Roof |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 300 | 风中的小提琴手 |'
- en: '| … | … |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| … | … |'
- en: '| Movie-ID | Rating | User-ID |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 电影-ID | 评分 | 用户-ID |'
- en: '| --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 100 | 4 | USER-1234 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 4 | 用户-1234 |'
- en: '| 100 | 5 | USER-3467 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 5 | 用户-3467 |'
- en: '| 200 | 4 | USER-1234 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 4 | 用户-1234 |'
- en: '| 200 | 2 | USER-1234 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 2 | 用户-1234 |'
- en: '| … | … | … |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: 'The final goal is to produce the following output, the AVG Rating table. This
    is a join of the Movies and Ratings tables, but after the join operation is completed,
    we still need to perform another reduction to find the average rating per Movie-ID:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是生成以下输出，即平均评分表。这是电影和评分表的连接，但在完成连接操作后，我们仍然需要执行另一次减少以找到每个电影-ID 的评分平均值：
- en: '| Movie-ID | Movie-Name | AVG Rating |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 电影-ID | 电影名称 | 平均评分 |'
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 100 | Lion King | 4.5 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 狮子王 | 4.5 |'
- en: '| 200 | Star Wars | 3.0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 星球大战 | 3.0 |'
- en: '| … | … | … |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: This data design pattern is illustrated by [Figure 9-4](#input_map_reduce_output_design_pattern_Reduce_side_join).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据设计模式由 [图 9-4](#input_map_reduce_output_design_pattern_Reduce_side_join) 描述。
- en: '![daws 0904](Images/daws_0904.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0904](Images/daws_0904.png)'
- en: Figure 9-4\. The Input-Map-Reduce-Output (reduce-side join) design pattern
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 输入-映射-减少-输出（减少侧连接）设计模式
- en: 'Let’s walk through it step by step:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这一过程：
- en: The mapper reads the input data that is to be combined based on a common column
    or join key. We read `Input-1`, then apply `map1()` as a mapper and create `(<Common-Key>,
    <Rest-of-Attributes>)` pairs. Applying this to the Movies table will create `(Movie-ID,
    Movie-Name)` pairs, where `Movie-ID` is a key and `Movie-Name` is a value.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mapper 读取输入数据，根据共同列或连接键进行合并。我们读取 `Input-1`，然后将 `map1()` 应用为映射器，并创建 `(<Common-Key>,
    <Rest-of-Attributes>)` 对。将此应用于电影表将创建 `(电影-ID, 电影名称)` 对，其中 `电影-ID` 是键，`电影名称` 是值。
- en: Next, we read `Input-2`, then apply `map2()` as a mapper and create `(<Common-Key>,
    <Rest-of-Attributes>)` pairs. Applying this to the Ratings table will create `(Movie-ID,
    (Movie-Name, Rating))` pairs, where `Movie-ID` is a key and `(Movie-Name, Rating)`
    is a value.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们读取 `Input-2`，然后将 `map2()` 应用为映射器，并创建 `(<Common-Key>, <Rest-of-Attributes>)`
    对。将此应用于评分表将创建 `(电影-ID, (电影名称, 评分))` 对，其中 `电影-ID` 是键，`(电影名称, 评分)` 是值。
- en: We now perform a `join()` operation between the outputs of `map1()` and `map2()`.
    Therefore, the goal is to join `(Movie-ID, Movie-Name)` pairs with `(Movie-ID,
    (Movie-Name, Rating))` pairs on the common key, `Movie-ID`. The result of this
    join is `(Movie-ID, (Rating, Movie-Name))` pairs.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们在 `map1()` 和 `map2()` 的输出之间执行 `join()` 操作。因此，目标是在共同键 `电影-ID` 上将 `(电影-ID,
    电影名称)` 对与 `(电影-ID, (电影名称, 评分))` 对进行连接。此连接的结果是 `(电影-ID, (评分, 电影名称))` 对。
- en: 'The next step is to reduce and aggregate the output of the `join()` operation
    by using `Movie-ID` as a key: we need all ratings per `Movie-ID` to find the average
    of the ratings.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用 `电影-ID` 作为键减少和聚合 `join()` 操作的输出：我们需要每个 `电影-ID` 的所有评分以找到评分的平均值。
- en: Finally, we have a simple mapper (`map3()`) calculate the average of the ratings
    and produce the final output.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有一个简单的映射器 (`map3()`) 计算评分的平均值并生成最终输出。
- en: 'For this design pattern, I will provide two PySpark solutions: one using RDDs
    and another using DataFrames.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种设计模式，我将提供两种 PySpark 解决方案：一种使用 RDD，另一种使用数据框架。
- en: RDD Solution
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 解决方案
- en: 'First, I’ll present a simple PySpark solution using RDDs. The first step is
    to prepare the inputs. We’ll create two RDDs to represent our two inputs. For
    this, I’ll define two simple tokenization functions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将展示使用 RDD 的简单 PySpark 解决方案。第一步是准备输入。我们将创建两个 RDD 来表示我们的两个输入。为此，我将定义两个简单的分词函数：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we use these functions in the mapper transformations:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在映射器转换中使用这些函数：
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So far we have created two RDDs:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了两个 RDD：
- en: '`movies_rdd`: `RDD[(Movie-ID, Movie-Name)]`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movies_rdd`: `RDD[(电影-ID, 电影名称)]`'
- en: '`ratings_rdd`: `RDD[(Movie-ID, Rating)]`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ratings_rdd`: `RDD[(电影-ID, 评分)]`'
- en: 'Now, we’ll use these two RDDs to perform the join operation on the common key,
    `Movie-ID`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这两个 RDD 来在共同键 `电影-ID` 上执行连接操作：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The last step is to use a simple mapper to prepare the final output, which
    includes the average rating per `Movie-ID`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用简单的映射器准备最终输出，其中包括每个 `电影-ID` 的平均评分：
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: DataFrame Solution
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据框架解决方案
- en: 'The solution using DataFrames is quite straightforward: we create a DataFrame
    per input and then join them on the common key, `Movie-ID`.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据框架的解决方案非常简单：我们为每个输入创建一个数据框架，然后在共同键 `电影-ID` 上进行连接。
- en: 'First let’s create the DataFrames:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们创建数据框架：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then all we have to do is perform the join operation. This is easy with DataFrames:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需要执行连接操作。这在 DataFrames 中很容易实现：
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Input-Map-Combiner-Reduce-Output
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Input-Map-Combiner-Reduce-Output
- en: The Input-Map-Combiner-Reduce-Output design pattern is very similar to Input-Map-Reduce-Output.
    The main difference is that combiners are used as well, to speed up the transformation.
    In the MapReduce paradigm (implemented in Apache Hadoop), a combiner—also known
    as a semi-reducer—is an optional function that works by accepting the outputs
    from the mapper function for each partition on a worker node, aggregating the
    results per key, and finally passing the output (key, value) pairs to the reducer
    function. In Spark, combiners are automatically executed on each worker node and
    partition, and you do not have to write any special combiner functions. An example
    of such a transformation is the `reduceByKey()` transformation, which merges the
    values for each key using an associative and commutative reduce function.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-映射-组合器-减少-输出设计模式与输入-映射-减少-输出非常相似。主要区别在于还使用了组合器，以加速转换过程。在 MapReduce 范式中（在
    Apache Hadoop 中实现），组合器——也称为半减少器——是一个可选函数，通过接受每个工作节点上每个分区的映射器函数的输出，按键聚合结果，最后将输出的
    (键, 值) 对传递给减少函数。在 Spark 中，组合器会自动在每个工作节点和分区上执行，你不必编写任何特殊的组合器函数。这样的转换的一个例子是 `reduceByKey()`
    转换，它使用一个可结合和交换的减少函数合并每个键的值。
- en: 'The main function of a combiner is to summarize and aggregate the mapper’s
    output records as (key, value) pairs, with the same key per partition. The purpose
    of this design pattern is to make sure that combiners can be used and that your
    data algorithm will not deliver incorrect results. For example, the goal is to
    sum up values per key and we have the following (key, value) pairs:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 组合器的主要功能是总结和聚合映射器的输出记录，形成 (键, 值) 对，每个分区具有相同的键。这个设计模式的目的是确保组合器可以被使用，并且你的数据算法不会产生错误的结果。例如，目标是按键汇总值，我们有以下
    (键, 值) 对：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: in the same partition, then, the job of a combiner is to summarize these as
    `(K1, 70), (K2, 18)`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个分区中，组合器的任务是将这些汇总为 `(K1, 70), (K2, 18)`。
- en: This data design pattern is illustrated by [Figure 9-5](#input_map_combine_reduce_output_design_pattern).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据设计模式由 [图 9-5](#input_map_combine_reduce_output_design_pattern) 描述。
- en: '![daws 0905](Images/daws_0905.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0905](Images/daws_0905.png)'
- en: Figure 9-5\. The Input-Map-Combine-Reduce-Output design pattern
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 输入-映射-组合器-减少-输出设计模式
- en: 'Suppose we have input representing cities and their associated temperatures,
    and our goal is to find the average temperature for each city. `source_rdd` has
    the format `RDD[(String, Double)]` for our data, where the key is the city name
    and the value is the associated temperature. To find the average temperature per
    city, you might attempt to write:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有代表城市及其相关温度的输入，并且我们的目标是找到每个城市的平均温度。`source_rdd` 的格式为 `RDD[(String, Double)]`，其中键是城市名称，值是关联的温度。为了找到每个城市的平均温度，您可能会尝试编写：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'But this is not the right transformation, so it will not compute the average
    per city. The problem is that, as we know, the average function is not associative:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不是正确的转换，因此它不会计算每个城市的平均值。问题在于，正如我们所知，平均函数不是可结合的：
- en: '[PRE47]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'That is, the average of averages is not an average. Why not? The following
    example illustrates. Suppose we have this data, on two partitions:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，平均数的平均数不是一个平均数。为什么？下面的例子可以说明。假设我们有这样的数据，在两个分区上：
- en: '[PRE48]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Our transformation will create:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的转换将创建：
- en: '[PRE49]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, combining the results of the two partitions will produce:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将两个分区的结果合并将产生：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Is 38.75 the average of (20, 30, 40, 50, 60)? Of course not! The correct average
    is (20 + 30 + 40 + 50 + 60) / 5 = 200 / 5 = 40.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 38.75 是 (20, 30, 40, 50, 60) 的平均数吗？当然不是！正确的平均数是 (20 + 30 + 40 + 50 + 60) / 5
    = 200 / 5 = 40。
- en: Because the average function is not associative, our reducer function is not
    correct—but with a minor modification, we can make the output of the mappers commutative
    and associative. This will give us the correct averages per unique city.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因为平均函数不是可结合的，我们的减少函数不正确——但稍作修改，我们可以使映射器的输出具有交换性和结合性。这将给出每个唯一城市的正确平均值。
- en: 'Say we have the following data in an RDD:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在一个 RDD 中有以下数据：
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we’ll create a new RDD from `cities_rdd` to make sure that its values
    comply with the laws of commutativity and associativity:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从 `cities_rdd` 创建一个新的 RDD，以确保其值符合交换性和结合性的法则：
- en: '[PRE52]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`cities_sum_count` is an `RDD[(city, (sum-of-temp, count-of-temp))`. Since
    we know that addition is a commutative and associative operation over a `(sum,
    count)` tuple, we can write our reduction as:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`cities_sum_count`是一个`RDD[(city, (sum-of-temp, count-of-temp))`。由于我们知道加法在`(sum,
    count)`元组上是可交换和可结合的操作，我们可以将我们的减少写成：'
- en: '[PRE53]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We then need one final mapper to find the temperature average per city:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要一个最终的映射器来找到每个城市的平均温度：
- en: '[PRE54]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Another solution for this design pattern is to use Spark’s `combineByKey()`
    transformation. If `cities_rdd` is our source RDD, then we can find the average
    temperature per city as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此设计模式的另一种解决方案是使用Spark的`combineByKey()`转换。如果`cities_rdd`是我们的源RDD，则可以按如下方式找到每个城市的平均温度：
- en: '[PRE55]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO3-1)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_classic_data_design_patterns_CO3-1)'
- en: Create `C` as `(sum, count)`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`C`为`(sum, count)`。
- en: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO3-2)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_classic_data_design_patterns_CO3-2)'
- en: Merge values per partition.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 合并每个分区的值。
- en: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO3-3)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_classic_data_design_patterns_CO3-3)'
- en: Combine two partitions (combine two `C`s into one).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个分区合并（将两个`C`合并成一个）。
- en: Tip
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For your combiners to work properly and be semantically correct, the intermediate
    values output by your mappers must be monoids and follow the algebraic laws of
    commutativity and associativity. To learn more about this design pattern, see
    [Chapter 4](ch04.xhtml#unique_chapter_id_04) and the paper [“Monoidify! Monoids
    as a Design Principle for Efficient MapReduce Algorithms”](https://oreil.ly/kWNDP)
    by Jimmy Lin.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您的组合器正常工作并在语义上正确，您的映射器输出的中间值必须是单子，并遵循交换性和结合性的代数法则。要了解更多关于这种设计模式的信息，请参见第[4](ch04.xhtml#unique_chapter_id_04)章和Jimmy
    Lin的论文[“Monoidify! Monoids as a Design Principle for Efficient MapReduce Algorithms”](https://oreil.ly/kWNDP)。
- en: Input-MapPartitions-Reduce-Output
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入-MapPartitions-Reduce-Output
- en: Input-MapPartitions-Reduce-Output is a very important data design pattern in
    which you apply a function to each partition—each of which may have thousands
    or millions of elements—as opposed to each element. We discussed this design pattern
    in Chapters [2](ch02.xhtml#Chapter-02) and [3](ch03.xhtml#Chapter-03), but because
    of its importance I wanted to cover it here in more detail. Imagine that you have
    billions of records and you want to summarize all of these records into a compact
    data structure such as a list, array, tuple, or dictionary. You can do that with
    the Input-MapPartitions-Reduce-Output design pattern, illustrated in [Figure 9-6](#input_mappartitions_output_design_pattern).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-MapPartitions-Reduce-Output是一个非常重要的数据设计模式，其中您将一个函数应用于每个分区——每个分区可能有数千或数百万个元素——而不是每个元素。我们在第[2](ch02.xhtml#Chapter-02)章和第[3](ch03.xhtml#Chapter-03)章讨论了这种设计模式，但由于其重要性，我想在这里更详细地介绍它。想象一下，您有数十亿条记录，您希望将所有这些记录汇总为如列表、数组、元组或字典之类的紧凑数据结构。您可以使用输入-MapPartitions-Reduce-Output设计模式，如[图 9-6](#input_mappartitions_output_design_pattern)所示。
- en: '![daws 0906](Images/daws_0906.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0906](Images/daws_0906.png)'
- en: Figure 9-6\. The Input-MapPartitions-Reduce-Output design pattern
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 输入-MapPartitions-Reduce-Output设计模式
- en: 'The general scenario can be summarized as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况可以总结如下：
- en: Input
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: Billions of records.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 数十亿条记录。
- en: Processing
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 处理
- en: Use `mapPartitions()` as a summarization design pattern.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mapPartitions()`作为总结设计模式。
- en: Split the input into *N* partitions, then analyze/process each partition using
    a custom function, independently and concurrently, and produce a compact data
    structure (CDS) such as an array, list, or dictionary. We can label these outputs
    as CDS-1, CDS-2, …, CDS-*N*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入分割为*N*个分区，然后使用自定义函数独立和并发地分析/处理每个分区，并产生一个紧凑的数据结构（CDS），如数组、列表或字典。我们可以将这些输出标记为CDS-1、CDS-2、…、CDS-*N*。
- en: Reducer
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer
- en: The final reducer works on the generated values CDS-1, CDS-2, …, CDS-*N*. The
    output of this step is a single compact data structure, such as an array, list,
    or dictionary.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的减少器在生成的值CDS-1、CDS-2、…、CDS-*N*上工作。这一步的输出是一个单一的紧凑数据结构，如数组、列表或字典。
- en: Spark’s `mapPartitions()` is a specialized `map()` that is called only once
    for each partition. The entire content of the partition is available as a sequential
    stream of values via the input argument (`Iterator[V]`, where `V` is the data
    type of the source RDD elements). The custom function must return an `Iterator[T]`,
    where `T` is the data type of the target RDD elements.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`mapPartitions()`是一个专门的`map()`，每个分区只调用一次。整个分区的内容作为顺序值流通过输入参数(`Iterator[V]`，其中`V`是源RDD元素的数据类型)可用。自定义函数必须返回一个`Iterator[T]`，其中`T`是目标RDD元素的数据类型。
- en: To understand this design pattern, you must understand the difference between
    `map()` and `mapPartitions()`. The method `map()` converts each element of the
    source RDD into a single element of the target RDD by applying a function. On
    the other hand, `mapPartitions()` converts each partition—comprised of thousands
    or millions of elements—of the source RDD into multiple elements of the result
    (possibly none).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这种设计模式，必须理解`map()`和`mapPartitions()`之间的区别。`map()`方法通过应用函数将源RDD的每个元素转换为目标RDD的单个元素。另一方面，`mapPartitions()`方法将源RDD的每个分区（包含成千上万个元素）转换为结果RDD的多个元素（可能为零）。
- en: 'Suppose we have billions of records of the form:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有数十亿条记录如下形式：
- en: '[PRE56]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'and our goal is to sum up the salaries of employees based on their gender.
    We want to find the following three (key, value) tuples from all the input records:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是根据员工的性别汇总薪水。我们希望从所有输入记录中找到以下三个(key, value)元组：
- en: '[PRE57]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As we can observe from the expected output, there are only three keys: `"male"`,
    `"female"`, and `"unknown"`.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从预期输出中可以观察到的，只有三个键："male"，"female"和"unknown"。
- en: 'Here are a few examples of the input records, which I’ll use to illustrate
    how this design pattern behaves:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个输入记录的示例，我将用它们来说明这种设计模式的行为：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'A naive solution would be to generate (key, value) pairs, where the key is
    a gender and the value is a salary and then aggregate the results using the `groupByKey()`
    transformation. However, this solution is not efficient and has the following
    potential problems, which we can avoid by using Spark’s `mapPartitions()` transformation:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的解决方案是生成(key, value)对，其中key是性别，value是薪水，然后使用`groupByKey()`转换来聚合结果。然而，这种解决方案效率不高，并且可能存在以下潜在问题，我们可以通过使用Spark的`mapPartitions()`转换来避免：
- en: It will create billions of (key, value) pairs, which will clutter the cluster
    network.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将创建数十亿个(key, value)对，这将使集群网络变得混乱。
- en: Since there are only three keys, if you use the `groupByKey()` transformation
    each key will have billions of values, which might cause OOM errors.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于只有三个键，如果使用`groupByKey()`转换，每个键将有数十亿个值，这可能导致OOM错误。
- en: Since there are only three keys, the cluster might not be utilized efficiently.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于只有三个键，集群可能无法有效利用。
- en: 'The Input-MapPartitions-Reduce-Output design pattern comes to our rescue and
    offers an efficient solution. First, we partition the input into *`N`* partitions,
    each containing thousands or millions of records. The value *`N`* can be determined
    based on the input size `(*N*=200, 400, 1000, 20000, …)`. The next step is to
    apply the `mapPartitions()` transformation: we map each partition and create a
    very small dictionary per partition with three keys: `"male"`, `"female"`, and
    `"unknown"`. The final reduction will be to aggregate these *`N`* dictionaries.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输入-MapPartitions-Reduce-Output设计模式挺身而出，提供了一个高效的解决方案。首先，我们将输入分区为*`N`*个分区，每个分区包含数千或数百万条记录。根据输入大小决定*`N`*的值，例如`(*N*=200,
    400, 1000, 20000, …)`。接下来的步骤是应用`mapPartitions()`转换：我们映射每个分区，并创建一个非常小的字典，其中包含三个键："male"，"female"和"unknown"。最终的规约将聚合这些*`N`*个字典。
- en: 'Let’s partition our sample input into two partitions:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将示例输入分区为两个分区：
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The main idea behind this design pattern is to partition the input and then
    process the partitions independently and concurrently. For example, if `*N*=1000`
    and you have *`N`* mappers, then all of them can be executed concurrently. Applying
    the basic mapping using `mapPartitions()`, we will generate the following dictionaries
    per partition:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计模式的主要思想是对输入进行分区，然后独立并发地处理这些分区。例如，如果`*N*=1000`并且您有*`N`*个映射器，那么所有映射器都可以并发执行。通过应用基本的映射使用`mapPartitions()`，我们将为每个分区生成以下字典：
- en: '[PRE60]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, we will apply the final reduction to aggregate the output of all the
    partitions into a single dictionary:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用最终的规约，将所有分区的输出聚合到一个单一的字典中：
- en: '[PRE61]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: When the Input-MapPartitions-Reduce-Output design pattern is used to summarize
    data, there is no scalability issue since we are creating one simple, small data
    structure (such as a dictionary) per partition. If even we set `*N*=100,000`,
    the solution is efficient, since processing 100,000 small dictionaries will not
    cause any OOM problems.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Input-MapPartitions-Reduce-Output设计模式来总结数据时，由于我们每个分区创建一个简单的小数据结构（例如字典），所以没有可扩展性问题。即使我们设置`*N*=100,000`，这个解决方案也是高效的，因为处理100,000个小字典不会导致任何OOM问题。
- en: Tip
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The most important reason to use the `mapPartitions()` transformation is performance.
    By having all the data (as a single partition) that we need to perform calculations
    on a single server node, we reduce the overhead of the shuffle (the need for serialization
    and network traffic).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mapPartitions()`转换的最重要原因是性能。通过在单个服务器节点上具有所有需要进行计算的数据（作为单个分区），我们减少了洗牌的开销（即需要序列化和网络流量）。
- en: 'An additional advantage of using Spark’s `mapPartitions()` transformation to
    implement the Input-MapPartitions-Reduce-Output design pattern is that it allows
    you to perform heavyweight initialization per partition (rather than per element).
    The following example illustrates this. `mapPartitions()` provides for the initialization
    to be done once per worker task/thread/partition instead of once per RDD data
    element:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 的`mapPartitions()`转换来实现输入-MapPartitions-Reduce-Output设计模式的另一个优点是，它允许您每个分区执行重量级初始化（而不是每个元素一次）。以下示例说明了这一点。`mapPartitions()`提供了可以一次性对工作任务/线程/分区进行初始化，而不是对每个RDD数据元素进行一次性初始化：
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Inverted Index
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向索引
- en: 'In computer science, an inverted index is a database index that stores a mapping
    from content—such as words or numbers—to its location(s) in a table, a document,
    or a set of documents. For example, consider the following input:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，反向索引是一个数据库索引，它存储了从内容（例如单词或数字）到其在表格、文档或一组文档中的位置的映射。例如，考虑以下输入：
- en: '[PRE63]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The goal of an inverted index is to create this index:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 反向索引的目标是创建此索引：
- en: '[PRE64]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now, if you want to search for “dog,” you know that it is in `[doc1, doc3]`.
    The Inverted Index design pattern generates an index from a dataset to allow for
    faster searches. This type of index is the most popular data structure used in
    document retrieval systems, and is used on a large scale in search engines.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您要搜索“dog”，您会知道它在`[doc1, doc3]`中。反向索引设计模式从数据集生成索引，以实现更快的搜索。这种类型的索引是文档检索系统中最流行的数据结构，并且在搜索引擎中大规模使用。
- en: The inverted index design pattern has advantages and disadvantages. Advantages
    include that it enables us to perform fast full-text searches (at the cost of
    increased processing when a document is added to the database), and that it is
    easy to develop. Using PySpark, we can implement this design pattern with a series
    of `map()`, `flatMap()`, and reduction transformations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 反向索引设计模式有优点和缺点。优点包括它使我们能够执行快速的全文搜索（虽然增加了将文档添加到数据库时的处理成本），并且易于开发。使用 PySpark，我们可以通过一系列`map()`、`flatMap()`和减少转换来实现这种设计模式。
- en: However, there is also a large storage overhead and high maintenance costs associated
    with update, delete, and insert operations.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更新、删除和插入操作也会带来大量的存储开销和高维护成本。
- en: Problem Statement
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Suppose we have a dataset that consists of the works of Shakespeare, split among
    many files. We want to produce an index that contains a list of all the words,
    the file(s) in which each one occurs, and the number of times it occurs.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，其中包含许多文件的莎士比亚作品。我们希望生成一个包含所有单词、每个单词出现的文件以及出现次数的索引。
- en: Input
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入
- en: 'Sample input documents for creating the inverted index can be downloaded from
    [GitHub](https://oreil.ly/5nD9H). The documents consist of a series of 35 text
    files:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建反向索引的示例输入文档可以从[GitHub](https://oreil.ly/5nD9H)下载。这些文档由一系列 35 个文本文件组成：
- en: '[PRE65]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Output
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出
- en: 'The output will be an inverted index created from all the documents read in
    the input phase. This output will have the following format:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是从输入阶段读取的所有文档创建的反向索引。此输出将具有以下格式：
- en: '[PRE66]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: which indicates that *`word`* is in *`filename1`* (with a frequency of *`frequency1`*),
    in *`filename2`* (with a frequency of *`frequency2`*), and so on.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表示*`word`*在*`filename1`*（频率为*`frequency1`*）、*`filename2`*（频率为*`frequency2`*）等中。
- en: PySpark Solution
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark 解决方案
- en: 'Our PySpark implementation of this design pattern consists of the following
    steps:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这种设计模式的 PySpark 实现包括以下步骤：
- en: Read the input files, filtering all stopwords (`a`, `of`, `the`, etc.) and apply
    stemming algorithms if desired (for example, converting `reading` to `read` and
    so on). This step creates pairs of `(path, text)`.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取输入文件，过滤掉所有停用词（`a`、`of`、`the`等），如果需要还可以应用词干提取算法（例如，将`reading`转换为`read`等）。此步骤创建了`(path,
    text)`对。
- en: Create tuples with count 1. That is, the expected output would be `((word, document),
    1)`.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建具有计数为 1 的元组。也就是说，预期的输出将是`((word, document), 1)`。
- en: Group all `(word, document)` pairs and sum the counts (reduction is required).
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分组所有`(word, document)`对并求和计数（需要归约）。
- en: Transform each tuple of `((word, document), frequency)` into `(word, (document,
    count))` so that we can count `word`(s) per `document`.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个`((word, document), frequency)`的元组转换为`(word, (document, count))`，这样我们就可以按`document`计数`word`。
- en: Output the sequence of `(document, count)` pairs into a comma-separated string.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出`(document, count)`对的序列到逗号分隔的字符串中。
- en: Save the inverted index.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存反向索引。
- en: 'Suppose we have the following three documents as input:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下三个文档作为输入：
- en: '[PRE67]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Step 1 is to read the input files and create pairs of `(path, text)`, where
    `path` is the full name of the input file and `text` is the content of the file.
    For example, if `path` denotes the file */tmp/documents/file1.txt*, then `text`
    is the content of the file *file1.txt*. Spark’s `wholeTextFiles(*path*)` function
    reads a directory of text files from a filesystem URI. Each file is read as a
    single record and returned in a (key, value) pair, where the key is the path to
    the file and the value is the content of the file:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步是读取输入文件并创建`(path, text)`对，其中`path`是输入文件的完整名称，`text`是文件的内容。例如，如果`path`表示文件*/tmp/documents/file1.txt*，那么`text`就是文件*file1.txt*的内容。Spark的`wholeTextFiles(*path*)`函数从文件系统URI中读取文本文件目录。每个文件作为单个记录读取，并返回成(key,
    value)对，其中key是文件路径，value是文件内容：
- en: '[PRE68]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Step 2 is to map each `text` into a set of `((word, document), 1)` pairs. We
    start by splitting the texts on newlines:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步是将每个`text`映射到一组`((word, document), 1)`对。我们从换行符开始分割文本：
- en: '[PRE69]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we create `(word,` `document)` pairs and map them into tuples of `((word,`
    `document),` `1)`, which indicates that the `word` belongs to the `document` with
    a frequency of 1:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建`(word, document)`对，并将它们映射到`((word, document), 1)`的元组中，这表示`word`属于`document`，频率为1：
- en: '[PRE70]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Step 3 is to perform a simple reduction to group all the `((word, document),
    1)` pairs and sum the counts:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步是执行简单的归约操作，将所有`((word, document), 1)`对分组并求和计数：
- en: '[PRE71]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'In step 4, we perform a very simple `map()` transformation that moves the `path`
    into the value part of the tuple:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们执行一个非常简单的`map()`转换，将`path`移动到元组的值部分：
- en: '[PRE72]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We do this as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做如下：
- en: '[PRE73]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, in step 5 we output the sequence of `(document, count)` pairs into a
    comma-separated string:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在第5步中，我们将`(document, count)`对的序列输出到逗号分隔的字符串中：
- en: '[PRE74]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'To implement this step, I used the `groupByKey()` transformation. You may use
    other reduction transformations, such as `reduceByKey()` or `combineByKey()`,
    to accomplish the same task. For example, you could implement this step with the
    `combineByKey()` transformation as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现此步骤，我使用了`groupByKey()`转换。您也可以使用其他归约转换，如`reduceByKey()`或`combineByKey()`来完成相同的任务。例如，您可以使用`combineByKey()`转换来实现此步骤如下：
- en: '[PRE75]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, step 6 is to save your created inverted index:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第6步中，保存您创建的反向索引：
- en: '[PRE76]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: This chapter presented some of the most common and fundamental data analysis
    design patterns with simple examples, demonstrating implementations using PySpark.
    Before you invent a new custom data transformation, you should study the available
    PySpark APIs and use them if possible (since these APIs are tested rigorously,
    you can use them with confidence). Using combinations of Spark transformations
    will enable you to solve just about any data problem.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些最常见和基础的数据分析设计模式，并通过简单的示例演示了使用PySpark的实现。在发明新的自定义数据转换之前，您应该研究现有的PySpark
    API，并尽可能使用它们（因为这些API经过严格测试，您可以放心使用它们）。使用Spark转换的组合将使您能够解决几乎任何数据问题。
- en: The next chapter introduces some practical data design patterns for production
    environments.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章介绍了一些适用于生产环境的实用数据设计模式。
