- en: Chapter 4\. Reductions in Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. Spark 中的 Reductions
- en: This chapter focuses on reduction transformations on RDDs in Spark. In particular,
    we’ll work with RDDs of (key, value) pairs, which are a common data abstraction
    required for many operations in Spark. Some initial ETL operations may be required
    to get your data into a (key, value) form, but with pair RDDs you may perform
    any desired aggregation over a set of values.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了 Spark 中 RDD 的 Reduction 转换。特别是，我们将使用（键，值）对的 RDD，这是 Spark 中许多操作所需的常见数据抽象。可能需要进行一些初始的
    ETL 操作来将数据转换为（键，值）形式，但是使用 Pair RDDs，你可以对一组值执行任何所需的聚合。
- en: 'Spark supports several powerful reduction transformations and actions. The
    most important reduction transformations are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持几种强大的 Reduction 转换和操作。最重要的 Reduction 转换包括：
- en: '`reduceByKey()`'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduceByKey()`'
- en: '`combineByKey()`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combineByKey()`'
- en: '`groupByKey()`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupByKey()`'
- en: '`aggregateByKey()`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregateByKey()`'
- en: 'All of the `*ByKey()` transformations accept a source `RDD[(K, V)]` and create
    a target `RDD[(K, C)]` (for some transformations, such as `reduceByKey()`, `V`
    and `C` are the same). The function of these transformations is to reduce all
    the values of a given key (for all unique keys), by finding, for example:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的 `*ByKey()` 转换接受一个源 `RDD[(K, V)]` 并创建一个目标 `RDD[(K, C)]`（对于某些转换，如 `reduceByKey()`，`V`
    和 `C` 是相同的）。这些转换的功能是通过查找给定键的所有值（所有唯一键）来减少所有值，例如：
- en: The average of all values
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值的平均数
- en: The sum and count of all values
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值的总和和计数
- en: The mode and median of all values
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值的模式和中位数
- en: The standard deviation of all values
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值的标准偏差
- en: Reduction Transformation Selection
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择 Reduction 转换
- en: 'As with mapper transformations, it’s important to select the right tool for
    the job. For some reduction operations (such as finding the median), the reducer
    needs access to all the values at the same time. For others, such as finding the
    sum or count of all values, it doesn’t. If you want to find the median of values
    per key, then `groupByKey()` will be a good choice, but this transformation does
    not do well if a key has lots of values (which might cause an OOM problem). On
    the other hand, if you want to find the sum or count of all values, then `reduceByKey()`
    might be a good choice: it merges the values for each key using an associative
    and commutative reduce function.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Mapper 转换类似，选择适合任务的正确工具非常重要。对于某些减少操作（如找到中位数），减少器需要同时访问所有值。对于其他操作，如找到所有值的总和或计数，这并不需要。如果你想要找到每个键的值的中位数，那么
    `groupByKey()` 将是一个不错的选择，但是如果一个键有大量的值（可能会导致内存溢出问题），这种转换就不太适合。另一方面，如果你想要找到所有值的总和或计数，那么
    `reduceByKey()` 可能是一个不错的选择：它使用可结合和交换的减少函数合并每个键的值。
- en: This chapter will show you how to use the most important Spark reduction transformations,
    through simple working PySpark examples. We will focus on the transformations
    most commonly used in Spark applications. I’ll also discuss the general concept
    of reduction, and monoids as a design principle for efficient reduction algorithms.
    We’ll start by looking at how to create pair RDDs, which are required by Spark’s
    reduction transformations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过简单的 PySpark 示例向你展示如何使用最重要的 Spark Reduction 转换。我们将重点讨论在 Spark 应用中最常用的转换。我还将讨论减少的一般概念，以及作为高效减少算法设计原则的幺半群。我们将从学习如何创建
    Pair RDDs 开始，这是 Spark Reduction 转换所必需的。
- en: Creating Pair RDDs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Pair RDDs
- en: 'Given a set of keys and their associated values, a reduction transformation
    reduces the values of each key using an algorithm (sum of value, median of values,
    etc.). The reduction transformations presented in this chapter thus work on (key,
    value) pairs, which means that the RDD elements must conform to this format. There
    are several ways to create pair RDDs in Spark. For example, you can also use `parallelize()`
    on collections (such as lists of tuples and dictionaries), as shown here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组键及其关联的值，减少转换使用算法（值的总和、值的中位数等）减少每个键的值。因此，本章介绍的减少转换适用于（键，值）对，这意味着 RDD 元素必须符合此格式。在
    Spark 中有几种创建 Pair RDDs 的方法。例如，你还可以在集合（如元组列表和字典）上使用 `parallelize()`，如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO1-1)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO1-1)'
- en: '`pair_rdd` has two keys, `{''A'', ''B''}`.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`pair_rdd` 有两个键，`{''A'', ''B''}`。'
- en: 'Next, suppose you have weather-related data and you want to create pairs of
    `(city_id, temperature)`. You can do this using the `map()` transformation. Assume
    that your input has the following format:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设你有与天气相关的数据，你想要创建 `(city_id, temperature)` 的对。你可以使用 `map()` 转换来完成这个任务。假设你的输入具有以下格式：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'First, define a function to create the desired (key, value) pairs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义一个函数来创建所需的（键，值）对：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO2-1)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO2-1)'
- en: The key is `city_id` and the value is `temperature`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 键是 `city_id`，值是 `temperature`。
- en: 'Then use `map()` to create your pair RDD:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 `map()` 创建你的键值对 RDD：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The are many other ways to create (key, value) pair RDDs: `reduceByKey()`,
    for example, accepts a source `RDD[(K, V)]` and produces a target `RDD[(K, V)]`,
    and `combineByKey()` accepts a source `RDD[(K, V)]` and produces a target `RDD[(K,
    C)]`.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他创建（键，值）对 RDD 的方法：`reduceByKey()` 例如，接受一个源 `RDD[(K, V)]` 并生成一个目标 `RDD[(K,
    V)]`，`combineByKey()` 则接受一个源 `RDD[(K, V)]` 并生成一个目标 `RDD[(K, C)]`。
- en: Reduction Transformations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩减转换
- en: 'Typically, a reduction transformation reduces the data size from a large batch
    of values (such as list of numbers) to a smaller one. Examples of reductions include:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，缩减转换会将数据大小从大批量值（如数字列表）减小到较小值。缩减的示例包括：
- en: Finding the sum and average of all values
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有值的和与平均值
- en: Finding the mean, mode, and median of all values
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有值的平均值、众数和中位数
- en: Calculating the mean and standard deviation of all values
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有值的平均值和标准差
- en: Finding the `(min, max, count)` of all values
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有值的 `(最小值、最大值、计数)`
- en: Finding the top 10 of all values
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有值的前 10 个
- en: In a nutshell, a reduction transformation roughly corresponds to the fold operation
    (also called reduce, accumulate, or aggregate) in functional programming. The
    transformation is either applied to all data elements (such as when finding the
    sum of all elements) or to all elements per key (such as when finding the sum
    of all elements per key).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，缩减转换大致对应于函数式编程中的 fold 操作（也称为 reduce、accumulate 或 aggregate）。该转换可以应用于所有数据元素（例如找到所有元素的总和）或每个键的所有元素（例如找到每个键的所有元素的总和）。
- en: A simple addition reduction over a set of numbers `{47, 11, 42, 13}` for a single
    partition is illustrated in [Figure 4-1](#Figure_4.1).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个分区的一组数字 `{47, 11, 42, 13}` 进行的简单加法缩减在 [图 4-1](#Figure_4.1) 中有所说明。
- en: '![daws 0401](Images/daws_0401.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0401](Images/daws_0401.png)'
- en: Figure 4-1\. An addition reduction in a single partition
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 单个分区中的加法缩减
- en: '[Figure 4-2](#reduction_concept) shows a reduction that sums the elements of
    two partitions. The final reduced values for Partition-1 and Partition-2 are `21`
    and `18`. Each partition performs local reductions and finally, the results from
    the two partitions are reduced.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-2](#reduction_concept) 展示了对两个分区元素求和的缩减操作。Partition-1 和 Partition-2 的最终缩减值分别为
    `21` 和 `18`。每个分区执行本地缩减，最终来自两个分区的结果被缩减。'
- en: '![daws 0402](Images/daws_0402.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0402](Images/daws_0402.png)'
- en: Figure 4-2\. An addition reduction over two partitions
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 两个分区上的加法缩减
- en: 'The reducer is a core concept in functional programming, used to transform
    a set of objects (such as numbers, strings, or lists) into a single value (such
    as the sum of numbers or concatenation of string objects). Spark and the MapReduce
    paradigm use this concept to aggregate a set of values into a single value per
    key. Consider the following (key, value) pairs, where the key is a `String` and
    the value is a list of `Integer`s:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 是函数式编程中的核心概念，用于将一组对象（如数字、字符串或列表）转换为单个值（如数字的总和或字符串对象的连接）。Spark 和 MapReduce
    范式使用该概念将一组值聚合为每个键的单个值。考虑以下（键，值）对，其中键是一个 `String`，值是一个 `Integer` 列表：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The simplest reducer will be an addition function over a set of values per
    key. After we apply this function, the result will be:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的 reducer 将是一个针对每个键的数值集合的加法函数。应用该函数后，结果将是：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or you may reduce each (key, value) to (key, pair) where the pair is `(sum-of-values,
    count-of-values)`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以将每个（键，值）缩减为（键，对），其中对是 `(值的总和，值的数量)`：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Reducers are designed to operate concurrently and independently, meaning that
    there is no synchronization between reducers. The more resources a Spark cluster
    has, the faster reductions can be done. In the worst possible case, if we have
    only one reducer, then reduction will work as a queue operation. In general, a
    cluster will offer many reducers (depending on resource availability) for the
    reduction transformation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 被设计为并行和独立操作，这意味着没有 reducer 之间的同步。Spark 集群的资源越多，缩减操作就可以越快。在最坏的情况下，如果只有一个
    reducer，那么缩减将作为队列操作进行。一般来说，集群将提供许多 reducer（取决于资源可用性）用于缩减转换。
- en: 'In MapReduce and distributed algorithms, reduction is a required operation
    in solving a problem. In the MapReduce programming paradigm, the programmer defines
    a mapper and a reducer with the following `map()` and `reduce()` signatures (note
    that [] denotes an iterable):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce和分布式算法中，减少是解决问题所需的操作。在MapReduce编程范式中，程序员定义了一个mapper和一个reducer，具有以下`map()`和`reduce()`签名（注意[]表示可迭代）：
- en: '`map()`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`'
- en: '`(K[1], V[1]) → [(K[2], V[2])]`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`(K[1], V[1]) → [(K[2], V[2])]`'
- en: '`reduce()`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`'
- en: '`(K[2], [V[2]]) → [(K[3], V[3])]`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`(K[2], [V[2]]) → [(K[3], V[3])]`'
- en: The `map()` function maps a (key[1], value[1]) pair into a set of (key[2], value[2])
    pairs. After all the map operations are completed, the sort and shuffle is done
    automatically (this functionality is provided by the MapReduce paradigm, not implemented
    by the programmer). The MapReduce sort and shuffle phase is very similar to Spark’s
    `groupByKey()` transformation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`函数将一个（键[1]，值[1]）对映射到一组（键[2]，值[2]）对。完成所有映射操作后，自动执行排序和洗牌（此功能由MapReduce范式提供，不由程序员实现）。MapReduce的排序和洗牌阶段与Spark的`groupByKey()`转换非常相似。'
- en: The `reduce()` function reduces a (key[2], [value[2]]) pair into a set of (key[3],
    value[3]) pairs. The convention is used to denote a list of objects (or an iterable
    list of objects). Therefore, we can say that a reduction transformation takes
    a list of values and reduces it to a tangible result (such as the sum of values,
    average of values, or your desired data structure).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`函数将一个（键[2]，[值[2]]）对减少为一组（键[3]，值[3]）对。约定用于表示对象的列表（或可迭代对象的列表）。因此，我们可以说，减少转换将值列表减少为具体结果（例如值的总和、值的平均值或所需的数据结构）。'
- en: Spark’s Reductions
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的减少操作
- en: 'Spark provides a rich set of easy-to-use reduction transformations. As stated
    at the beginning of this chapter, our focus will be on reductions of pair RDDs.
    Therefore, we will assume that each RDD has a set of keys and for each key (such
    as `K`) we have a set of values:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一组丰富且易于使用的减少转换。正如本章开头所述，我们的重点将放在对成对RDD的减少上。因此，我们将假设每个RDD都有一组键，并且对于每个键（如
    `K`），我们有一组值：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Table 4-1](#sparks_reduction_transformations) lists the reduction transformations
    available in Spark.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 4-1](#sparks_reduction_transformations) 列出了Spark中可用的减少转换。'
- en: Table 4-1\. Spark’s reduction transformations
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-1\. Spark的减少转换
- en: '| Transformation | Description |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 变换 | 描述 |'
- en: '| --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `aggregateByKey()` | Aggregates the values of each key using the given combine
    functions and a neutral “zero value” |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `aggregateByKey()` | 使用给定的组合函数和中性“零值”聚合每个键的值 |'
- en: '| `combineByKey()` | Generic function to combine the elements for each key
    using a custom set of aggregation functions |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `combineByKey()` | 使用自定义的聚合函数集合组合每个键的元素的通用函数 |'
- en: '| `countByKey()` | Counts the number of elements for each key, and returns
    the result to the master as a dictionary |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `countByKey()` | 计算每个键的元素数量，并将结果作为字典返回给主节点 |'
- en: '| `foldByKey()` | Merges the values for each key using an associative function
    and a neutral “zero value” |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `foldByKey()` | 使用关联函数和中性“零值”合并每个键的值 |'
- en: '| `groupByKey()` | Groups the values for each key in the RDD into a single
    sequence |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `groupByKey()` | 将RDD中每个键的值分组为单个序列 |'
- en: '| `reduceByKey()` | Merges the values for each key using an associative and
    commutative reduce function |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey()` | 使用关联和交换的减少函数合并每个键的值 |'
- en: '| `sampleByKey()` | Returns a subset of this RDD sampled by key, using variable
    sampling rates for different keys as specified by fractions |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `sampleByKey()` | 返回通过键变量采样率指定的不同键的RDD子集 |'
- en: '| `sortByKey()` | Sorts the RDD by key, so that each partition contains a sorted
    range of the elements in ascending order |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `sortByKey()` | 按键对RDD进行排序，使得每个分区包含按升序排列的元素范围 |'
- en: 'These transformation functions all act on (key, value) pairs represented by
    RDDs. In this chapter, we will look only at reductions of data over a set of given
    unique keys. For example, given the following (key, value) pairs for the key `K`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换函数都作用于由RDD表示的（键，值）对。在本章中，我们将仅关注在给定的唯一键集上的数据减少。例如，给定键 `K` 的以下（键，值）对：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'we are assuming that `K` has a list of `n (> 0)` values:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设 `K` 有一个长度为 `n (> 0)` 的值列表：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To keep it simple, the goal of reduction is to generate the following pair
    (or set of pairs):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，减少的目标是生成以下的配对（或一组配对）：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'where:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The function `f()` is called a *reducer* or *reduction* function. Spark’s reduction
    transformations apply this function over a list of values to find the reduced
    value, `R`. Note that Spark does not impose any ordering among the values (`[V[1],
    V[2], ..., V[n]]`) to be reduced.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f()`被称为*减少器*或*减少*函数。Spark的减少转换将此函数应用于值列表以找到减少的值`R`。请注意，Spark不对要减少的值（`[V[1],
    V[2], ..., V[n]]`）施加任何排序。
- en: 'This chapter will include practical examples of solutions demonstrating the
    use of the most common of Spark’s reduction transformations: `reduceByKey()`,
    `groupByKey()`, `aggregateByKey()`, and `combineByKey()`. To get you started,
    let’s look at a very simple example of the `groupByKey()` transformation. As the
    example in [Figure 4-3](#the_groupbykey_transfornation_example) shows, it works
    similarly to the SQL `GROUP BY` statement. In this example, we have four keys,
    `{A, B, C, P}`, and their associated values are grouped as a list of integers.
    The source RDD is an `RDD[(String, Integer)]`, where each element is a pair of
    `(String, Integer)`. The target RDD is an `RDD[(String, [Integer])]`, where each
    element is a pair of `(String, [Integer])`; the value is an iterable list of integers.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包括解决方案的实际示例，演示了Spark最常见的减少转换的使用：`reduceByKey()`、`groupByKey()`、`aggregateByKey()`和`combineByKey()`。为了帮助你入门，让我们看一个非常简单的`groupByKey()`转换的例子。如[图 4-3](#the_groupbykey_transfornation_example)中的例子所示，它的工作方式类似于SQL的`GROUP
    BY`语句。在这个例子中，我们有四个键， `{A, B, C, P}`，它们的相关值被分组为整数列表。源RDD是一个`RDD[(String, Integer)]`，其中每个元素是一个`(String,
    Integer)`对。目标RDD是一个`RDD[(String, [Integer])]`，其中每个元素是一个`(String, [Integer])`对；值是一个整数可迭代列表。
- en: '![daws 0403](Images/daws_0403.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0403](Images/daws_0403.png)'
- en: Figure 4-3\. The `groupByKey()` transformation
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. `groupByKey()`转换
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, Spark reductions do not sort the reduced values. For example, in
    [Figure 4-3](#the_groupbykey_transfornation_example), the reduced value for key
    `B` could be `[4, 8]` or `[8, 4]`. If desired, you may sort the values before
    the final reduction. If your reduction algorithm requires sorting, you must sort
    the values explicitly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spark的减少操作不会对减少的值进行排序。例如，在[图 4-3](#the_groupbykey_transfornation_example)中，键`B`的减少值可以是`[4,
    8]`或`[8, 4]`。如果需要，可以在最终减少之前对值进行排序。如果您的减少算法需要排序，必须显式排序值。
- en: Now that you have a general understanding of how reducers work, let’s move on
    to a practical example that demonstrates how different Spark reduction transformations
    can be used to solve a data problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对减少器的工作原理有了一般的了解，让我们继续看一个实际的例子，展示如何使用不同的Spark减少转换来解决一个数据问题。
- en: Simple Warmup Example
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的热身示例
- en: 'Suppose we have a list of pairs `(K, V)`, where `K` (the key) is a `String`
    and `V` (the value) is an `Integer`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一对列表`(K, V)`，其中`K`（键）是一个`String`，`V`（值）是一个`Integer`：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, we have four unique keys:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有四个唯一的键：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Suppose we want to combine (sum) the values per key. The result of this reduction
    will be:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要按键（sum）合并值。这种减少的结果将是：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'where:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There are many ways to add these numbers to get the desired result. How did
    we arrive at these reduced (key, value) pairs? For this example, we could use
    any of the common Spark transformations. Aggregating or combining the values per
    key is a type of reduction—in the classic MapReduce paradigm, this is called a
    *reduce by key* (or simply *reduce*) function. The MapReduce framework calls the
    application’s (user-defined) reduce function once for each unique key. The function
    iterates through the values that are associated with that key and produces zero
    or more outputs as (key, value) pairs, solving the problem of combining the elements
    of each unique key into a single value. (Note that in some applications, the result
    might be more than a single value.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以添加这些数字以获得所需的结果。我们如何得到这些减少的（键，值）对？在这个例子中，我们可以使用任何常见的Spark转换。按键聚合或组合值是减少的一种类型——在经典的MapReduce范式中，这被称为*按键减少*（或简称为*减少*）函数。MapReduce框架对每个唯一键调用应用程序（用户定义的）减少函数一次。该函数迭代与该键关联的值，并产生零个或多个输出作为（键，值）对，解决了将每个唯一键的元素组合为单个值的问题。（请注意，在某些应用程序中，结果可能不止一个值。）
- en: 'Here I present four different solutions using Spark’s transformations. For
    all solutions, we will use the following Python `data` and `key_value_pairs` RDD:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我介绍了使用Spark转换的四种不同解决方案。对于所有解决方案，我们将使用以下Python `data`和`key_value_pairs` RDD：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO3-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO3-1)'
- en: '`data` is a Python collection—a list of (key, value) pairs.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 是一个 Python 集合 —— 一个 (key, value) 对的列表。'
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO3-2)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO3-2)'
- en: '`key_value_pairs` is an `RDD[(String, Integer)]`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`key_value_pairs` 是一个 `RDD[(String, Integer)]`。'
- en: Solving with reduceByKey()
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `reduceByKey()` 解决问题
- en: 'Summing the values for a given key is pretty straightforward: add the first
    two values, then the next one, and keep going. Spark’s `reduceByKey()` transformation
    merges the values for each key using an associative and commutative reduce function.
    Combiners (optimized mini-reducers) are used in all cluster nodes before merging
    the values per partition.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定键的值求和非常直接：添加前两个值，然后添加下一个，依此类推。Spark 的 `reduceByKey()` 转换使用可结合且可交换的 reduce
    函数合并每个键的值。在所有集群节点上在合并每个分区的值之前使用组合器（优化的小型 reducer）。
- en: For the `reduceByKey()` transformation, the source RDD is an `RDD[(K, V)]` and
    the target RDD is an `RDD[(K, V)]`. Note that source and target data types of
    the RDD values (`V`) are the same. This is a limitation of `reduceByKey()`, which
    can be avoided by using `combineByKey()` or `aggregateByKey()`).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `reduceByKey()` 转换，源 RDD 是一个 `RDD[(K, V)]`，目标 RDD 是一个 `RDD[(K, V)]`。请注意，RDD
    值 (`V`) 的源和目标数据类型相同。这是 `reduceByKey()` 的一个限制，可以通过使用 `combineByKey()` 或 `aggregateByKey()`
    避免。
- en: 'We can apply the `reduceByKey()` transformation using a lambda expression (anonymous
    function):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 lambda 表达式（匿名函数）应用 `reduceByKey()` 转换：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Alternatively, we can use a defined function, such as `add`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用定义的函数，例如 `add`：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Adding values per key by `reduceByKey()` is an optimized solution, since aggregation
    happens at the partition level before the final aggregation of all the partitions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `reduceByKey()` 按键添加值是一种优化的解决方案，因为聚合发生在最终聚合所有分区之前的分区级别。
- en: Solving with groupByKey()
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `groupByKey()` 解决问题
- en: We can also solve this problem by using the `groupByKey()` transformation, but
    this solution will not perform as well because it involves moving lots of data
    to the reducer nodes (you’ll learn more about why this is the case when we discuss
    the shuffle step later in this chapter).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过使用 `groupByKey()` 转换来解决这个问题，但这种解决方案性能不佳，因为涉及将大量数据移动到 reducer 节点（在本章后面讨论
    shuffle 步骤时，您将了解更多原因）。
- en: 'With the `reduceByKey()` transformation, the source RDD is an `RDD[(K, V)]`
    and the target RDD is an `RDD[(K, [V])]`. Note that the source and target data
    types are not the same: the value data type for the source RDD is `V`, while for
    the target RDD it is `[V]` (an iterable/list of `V`s).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `reduceByKey()` 转换，源 RDD 是一个 `RDD[(K, V)]`，目标 RDD 是一个 `RDD[(K, [V])]`。请注意，源和目标数据类型不同：源
    RDD 的值数据类型是 `V`，而目标 RDD 的值数据类型是 `[V]`（一个 `V` 的可迭代列表）。
- en: 'The following example demonstrates the use of `groupByKey()` with a lambda
    expression to sum the values per key:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子演示了如何使用带有 lambda 表达式的 `groupByKey()` 来按键汇总值：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO4-1)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO4-1)'
- en: Group values per key (similar to SQL’s `GROUP BY`). Now each key will have a
    set of `Integer` values; for example, the three pairs `{('alex', 2), ('alex',
    4), ('alex', 8)}` will be reduced to a single pair, `('alex', [2, 4, 8])`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 按键分组值（类似于 SQL 的 `GROUP BY`）。现在每个键将有一组 `Integer` 值；例如，三对 `{('alex', 2), ('alex',
    4), ('alex', 8)}` 将被减少为单个对 `('alex', [2, 4, 8])`。
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO4-2)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO4-2)'
- en: Add values per key using Python’s `sum()` function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 的 `sum()` 函数添加每个键的值。
- en: Solving with aggregateByKey()
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `aggregateByKey()` 解决问题
- en: 'In simplest form, the `aggregateByKey()` transformation is defined as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，`aggregateByKey()` 转换被定义为：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It aggregates the values of each key from the source RDD into a target RDD,
    using the given combine functions and a neutral “zero value” (the initial value
    used for each partition). This function can return a different result type (`C`)
    than the type of the values in the source RDD (`V`), though in this example both
    are `Integer` data types. Thus, we need one operation for merging values within
    a single partition (merging values of type `V` into a value of type `C`) and one
    operation for merging values between partitions (merging values of type `C` from
    multiple partitions). To avoid unnecessary memory allocation, both of these functions
    are allowed to modify and return their first argument instead of creating a new
    `C`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 它将源 RDD 中每个键的值聚合到目标 RDD 中，使用给定的合并函数和中立的“零值”（用于每个分区的初始值）。这个函数可以返回一个不同的结果类型 (`C`)，而不是源
    RDD 中值的类型 (`V`)，尽管在此示例中两者都是 `Integer` 数据类型。因此，我们需要一个操作来在单个分区内合并值（将类型为 `V` 的值合并为类型为
    `C` 的值），以及一个操作来在分区之间合并值（从多个分区中合并类型为 `C` 的值）。为了避免不必要的内存分配，这两个函数都允许修改并返回它们的第一个参数，而不是创建新的
    `C`。
- en: 'The following example demonstrates the use of the `aggregateByKey()` transformation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了 `aggregateByKey()` 转换的使用：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO5-1)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO5-1)'
- en: The `zero_value` applied on each partition is `0`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于每个分区的 `zero_value` 是 `0`。
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO5-2)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO5-2)'
- en: '`seq_func` is used on a single partition.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`seq_func` 在单个分区上使用。'
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO5-3)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_reductions_in_spark_CO5-3)'
- en: '`comb_func` is used to combine the values of partitions.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`comb_func` 用于合并分区的值。'
- en: Solving with combineByKey()
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `combineByKey()` 解决问题
- en: 'The `combineByKey()` transformation is the most general and powerful of Spark’s
    reduction transformations. In its simplest form, it is defined as:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey()` 转换是 Spark 中最通用和强大的减少转换。在其最简单的形式下，它定义为：'
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Like `aggregateByKey()`, the `combineByKey()` transformation turns a source
    `RDD[(K, V)]` into a target `RDD[(K, C)]`. Again, `V` and `C` can be different
    data types (this is part of the power of `combineByKey()`—for example, `V` can
    be a `String` or `Integer`, while `C` can be a list, tuple, or dictionary), but
    for this example both are `Integer` data types.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `aggregateByKey()`，`combineByKey()` 转换将源 `RDD[(K, V)]` 转换为目标 `RDD[(K, C)]`。再次强调，`V`
    和 `C` 可以是不同的数据类型（这是 `combineByKey()` 的强大之处之一——例如，`V` 可以是 `String` 或 `Integer`，而
    `C` 可以是列表、元组或字典），但在此示例中，两者都是 `Integer` 数据类型。
- en: 'The `combineByKey()` interface allows us to customize the reduction and combining
    behavior as well as the data type. Thus, to use this transformation we have to
    provide three functions:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey()` 接口允许我们自定义减少和合并行为以及数据类型。因此，为了使用此转换，我们必须提供三个函数：'
- en: '`create_combiner`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_combiner`'
- en: This function turns a single `V` into a `C` (e.g., creating a one-element list).
    It is used within a single partition to initialize a `C`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将单个 `V` 转换为 `C`（例如，创建一个单元素列表）。它在单个分区内用于初始化 `C`。
- en: '`merge_value`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`merge_value`'
- en: This function merges a `V` into a `C` (e.g., adding it to the end of a list).
    This is used within a single partition to aggregate values into a `C`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将 `V` 合并到 `C` 中（例如，将其添加到列表的末尾）。这在单个分区内用于将值聚合到 `C` 中。
- en: '`merge_combiners`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`merge_combiners`'
- en: This function combines two `C`s into a single `C` (e.g., merging the lists).
    This is used in merging values from two partitions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将两个 `C` 合并为一个 `C`（例如，合并列表）。这在合并来自两个分区的值时使用。
- en: 'Our solution with `combineByKey()` looks like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `combineByKey()` 的解决方案如下：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO6-1)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO6-1)'
- en: '`create_combiner` creates the initial values in each partition.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_combiner` 在每个分区中创建初始值。'
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO6-2)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO6-2)'
- en: '`merge_value` merges the values in a partition.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`merge_value` 合并分区中的值。'
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO6-3)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_reductions_in_spark_CO6-3)'
- en: '`merge_combiners` merges the values from the different partitions into the
    final result.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`merge_combiners` 将来自不同分区的值合并到最终结果中。'
- en: 'To give you a better idea of the power of the `combineByKey()` transformation,
    let’s look at another example. Suppose we want to find the mean of values per
    key. To solve this, we can create a combined data type (`C`) as `(sum, count)`,
    which will hold the sums of values and their associated counts:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 `combineByKey()` 转换的功能，让我们看另一个例子。假设我们想找到每个键的值的平均值。为了解决这个问题，我们可以创建一个组合数据类型
    (`C`)，如 `(sum, count)`，它将保存值的总和及其相关的计数：
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Given three partitions named `{P1, P2, P3}`, [Figure 4-4](#figure-4-4) shows
    how to create a combiner (data type `C`), how to merge a value into a combiner,
    and finally how to merge two combiners.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 给定名为 `{P1, P2, P3}` 的三个分区，[图 4-4](#figure-4-4) 显示如何创建一个组合器（数据类型 `C`），如何将一个值合并到组合器中，最后如何合并两个组合器。
- en: '![daws 0404](Images/daws_0404.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0404](Images/daws_0404.png)'
- en: Figure 4-4\. `combineByKey()` transformation example
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. `combineByKey()` 转换示例
- en: Next, I will discuss the concept of monoids, which will help you to understand
    how combiners function in reduction transformations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将讨论单子群的概念，这将帮助您理解在减少转换中组合器的功能。
- en: What Is a Monoid?
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是单子群？
- en: Monoids are a useful design principle for writing efficient MapReduce algorithms.^([1](ch04.xhtml#idm45713562249536))
    If you don’t understand monoids, you might write reducer algorithms that do not
    produce semantically correct results. If your reducer is a monoid, then you can
    be sure that it will produce correct output in a distributed environment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 单子群是编写高效的 MapReduce 算法的有用设计原则。^([1](ch04.xhtml#idm45713562249536)) 如果您不理解单子群，您可能会编写不产生语义正确结果的减少器算法。如果您的减少器是单子群，那么您可以确信它在分布式环境中会产生正确的输出。
- en: Since Spark’s reductions execute on a partition-by-partition basis (i.e., your
    reducer function is distributed rather than being a sequential function), to get
    the proper output you need to make sure that your reducer function is semantically
    correct. We’ll look at some examples of using monoids shortly, but first let’s
    examine the underlying mathematical concept.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark 的减少是基于分区的执行（即，您的减少函数是分布式的而不是顺序函数），为了获得正确的输出，您需要确保您的减少函数在语义上是正确的。我们稍后将看一些使用单子群的示例，但首先让我们检查底层的数学概念。
- en: In algebra, a monoid is an algebraic structure with a single associative binary
    operation and an identity element (also called a zero element).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在代数中，单子群是一种具有单一关联二元操作和单位元素（也称为零元素）的代数结构。
- en: 'For our purposes, we can informally define a monoid as `M = (T, f, Zero)`,
    where:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们可以非正式地定义单子群为 `M = (T, f, Zero)`，其中：
- en: '`T` is a data type.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T` 是一个数据类型。'
- en: '`f()` is a binary operation: `f: (T, T) -> T`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f()` 是一个二元操作：`f: (T, T) -> T`。'
- en: '`Zero` is an instance of `T`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Zero` 是 `T` 类型的一个实例。'
- en: Note
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`Zero` is an identity (neutral) element of type `T`; this is not necessarily
    the number zero.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`Zero` 是类型 `T` 的身份（中性）元素；这不一定是数字零。'
- en: 'If `a`, `b`, `c`, and `Zero` are of type `T`, for the triple `(T, f, Zero)`
    to be a monoid the following properties must hold:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `a`、`b`、`c` 和 `Zero` 是类型 `T` 的，对于三元组 `(T, f, Zero)` 来说，必须满足以下属性：
- en: Binary operation
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元操作
- en: '[PRE25]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Neutral element
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中性元素
- en: '[PRE26]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Associativity
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合性
- en: '[PRE27]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Not every binary operation is a monoid. For example, the `mean()` function
    over a set of integers is not an associative function and therefore is not a monoid,
    as the following proof shows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个二元操作都是单子群。例如，整数集合上的 `mean()` 函数不是一个关联函数，因此不是一个单子群，如下面的证明所示：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'What does this mean? Given an `RDD[(String, Integer)]`, we might be tempted
    to write the following transformation to find an average per key:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思？给定一个 `RDD[(String, Integer)]`，我们可能会试图编写以下转换以找到每个键的平均值：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'But this will not produce the correct results, because the average of averages
    is not an average—in other words, the mean/average function used here is not a
    monoid. Suppose that this `rdd` has three elements: `{("A", 1), ("A", 2), ("A",
    3)}`; `{("A", 1), ("A", 2)}` are in partition 1 and `{("A", 3)}` is in partition
    2\. Using the preceding solution will result in aggregated values of `("A", 1.5)`
    for partition 1 and `("A", 3.0)` for partition 2\. Combining the results for the
    two partitions will then give us a final average of (1.5 + 3.0) / 2 = 2.25, which
    is not the correct result (the average of the three values is 2.0). If your reducer
    is a monoid, it is guaranteed to behave properly and produce correct results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不会产生正确的结果，因为平均值的平均值不是平均值—换句话说，这里使用的平均/平均函数不是幺半群。假设这个`rdd`有三个元素：`{("A", 1),
    ("A", 2), ("A", 3)}`；`{("A", 1), ("A", 2)}`在分区1中，`{("A", 3)}`在分区2中。使用上述解决方案将导致分区1的聚合值为`("A",
    1.5)`，分区2的聚合值为`("A", 3.0)`。然后，将这两个分区的结果结合起来，得到最终平均值为(1.5 + 3.0) / 2 = 2.25，这不是正确的结果（三个值的平均值为2.0）。如果您的缩小器是幺半群，它保证能够正确运行并产生正确的结果。
- en: Monoid and Non-Monoid Examples
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幺半群和非幺半群示例
- en: 'To help you understand and recognize monoids, let’s look at some monoid and
    non-monoid examples. The following are examples of monoids:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您理解和识别幺半群，让我们看一些幺半群和非幺半群的例子。以下是幺半群的示例：
- en: 'Integers with addition:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有加法的整数：
- en: '[PRE30]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Integers with multiplication:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有乘法的整数：
- en: '[PRE31]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Strings with concatenation:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有连接的字符串：
- en: '[PRE32]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Lists with concatenation:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有连接的列表：
- en: '[PRE33]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Sets with their union:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有它们的联合集：
- en: '[PRE34]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And here are some non-monoid examples:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些非幺半群示例：
- en: 'Integers with mean function:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有均值函数的整数：
- en: '[PRE35]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Integers with subtraction:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有减法的整数：
- en: '[PRE36]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Integers with division:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有除法的整数：
- en: '[PRE37]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Integers with mode function:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有模函数的整数：
- en: '[PRE38]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Integers with median function:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有中位数函数的整数：
- en: '[PRE39]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In some cases, it is possible to convert a non-monoid into a monoid. For example,
    with a simple change to our data structures we can find the correct mean of a
    set of numbers. However, there is no algorithm to convert a non-monoid structure
    to a monoid automatically.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，可以将非幺半群转换为幺半群。例如，通过对我们的数据结构进行简单更改，我们可以找到一组数字的正确平均值。但是，没有算法可以自动将非幺半群结构转换为幺半群。
- en: Writing distributed algorithms in Spark is much different from writing sequential
    algorithms on a single server, because the algorithms operate in parallel on partitioned
    data. Therefore, when writing a reducer, you need to make sure that your reduction
    function is a monoid. Now that you understand this important concept, let’s move
    on to some practical examples.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中编写分布式算法与在单个服务器上编写顺序算法大不相同，因为算法在分区数据上并行运行。因此，在编写缩小器时，您需要确保您的缩小函数是一个幺半群。现在您理解了这个重要概念，让我们继续看一些实际的例子。
- en: The Movie Problem
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电影问题
- en: The goal of this first example is to present a basic problem and then provide
    solutions using different Spark reduction transformations by means of PySpark.
    For all reduction transformations, I have carefully selected the data types such
    that they form a monoid.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子的目标是提出一个基本问题，然后通过PySpark的不同Spark减少转换提供解决方案。对于所有减少转换，我都精心选择了数据类型，使它们形成一个幺半群。
- en: 'The movie problem can be stated as follows: given a set of users, movies, and
    ratings, (in the range 1 to 5), we want to find the average rating of all movies
    by a user. So, if the user with `userID=100`) has rated four movies:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 电影问题可以陈述如下：给定一组用户、电影和评分（在1到5的范围内），我们想找出用户对所有电影的平均评分。因此，如果用户`userID=100`评价了四部电影：
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'we want to generate the following output:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想生成以下输出：
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'where:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'For this example, note that the `reduceByKey()` transformation over a set of
    ratings will not always produce the correct output, since the average (or mean)
    is not an algebraic monoid over a set of float/integer numbers. In other words,
    as discussed in the previous section, the mean of means is not equal to the mean
    of all input numbers. Here is a simple proof. Suppose we want to find the mean
    of six values (the numbers 1–6), stored in a single partition. We can do this
    with the `mean()` function as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，请注意，对一组评分进行`reduceByKey()`转换不会总是产生正确的输出，因为平均（或均值）不是一组浮点数/整数的代数幺半群。换句话说，如前面的部分所讨论的，平均值的平均值不等于所有输入数字的平均值。这里有一个简单的证明。假设我们想找出六个值（数字1-6）的平均值，存储在单个分区中。我们可以使用`mean()`函数来完成：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, let’s make `mean()` function as a distributed function. Suppose the values
    are stored on three partitions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把`mean()`函数作为一个分布式函数。假设值存储在三个分区中：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'First, we compute the mean of each partition:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算每个分区的平均值：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then we find the mean of these values. Once all partitions are processed, therefore,
    we get:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们找到这些值的平均值。一旦所有分区都被处理，因此，我们得到：
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To avoid this problem, we can use a monoid data structure (which supports associativity
    and commutativity) such as a pair of `(sum, count)`, where `sum` is the total
    sum of all numbers we have seen so far (per partition) and `count` is the number
    of ratings we have seen so far. If we define our `mean()` function as:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，我们可以使用一个支持结合性和交换性的幺半群数据结构，例如一对`(sum, count)`，其中`sum`是到目前为止（每个分区）所有数字的总和，`count`是到目前为止我们看到的评分数量。如果我们定义我们的`mean()`函数如下：
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'we get:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '[PRE48]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As this example shows, by using a monoid we can achieve associativity. Therefore,
    you may apply the `reduceByKey()` transformation when your function `f()` is commutative
    and associative:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所显示的，通过使用幺半群，我们可以实现结合性。因此，当您的函数`f()`是可交换和可结合的时候，您可以应用`reduceByKey()`转换：
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: For example, the addition (`+`) operation is commutative and associative, but
    the mean/average function does not satisfy these properties.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，加法（`+`）操作是可交换和可结合的，但均值/平均函数不满足这些属性。
- en: Note
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'As we saw in [Chapter 1](ch01.xhtml#Chapter-01), a commutative function ensures
    that the result is independent of the order of elements in the RDD being aggregated:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#Chapter-01)中看到的，一个可交换的函数确保聚合的RDD元素顺序无关紧要：
- en: '[PRE50]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'An associative function ensures that the order in which elements are grouped
    during the aggregation does not affect the final result:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一个结合函数确保在聚合过程中分组元素的顺序不影响最终结果：
- en: '[PRE51]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Input Dataset to Analyze
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入要分析的数据集
- en: The sample data we’ll use for this problem is a dataset from [MovieLens](https://oreil.ly/KOyq4).
    For simplicity, I will assume that you have downloaded and unzipped the files
    into a */tmp/movielens/* directory. Note that there is no requirement to put the
    files at the suggested location; you may place your files in your preferred directory
    and update your input paths accordingly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个问题使用的样本数据集是来自[MovieLens](https://oreil.ly/KOyq4)的数据集。为简单起见，我假设您已经下载并解压缩了文件到*/tmp/movielens/*目录中。请注意，不需要将文件放置在建议的位置；您可以将文件放置在您喜欢的目录中，并相应地更新输入路径。
- en: Tip
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The full MovieLens dataset (*ml-latest.zip*) is 265 MB. If you want to use a
    smaller dataset to run, test, and debug the programs listed here, you can instead
    download the [small MovieLens dataset](https://oreil.ly/hAfIQ), a 1 MB file consisting
    of 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的MovieLens数据集（*ml-latest.zip*）为265 MB。如果您想要使用一个更小的数据集来运行、测试和调试这里列出的程序，您可以下载[小型MovieLens数据集](https://oreil.ly/hAfIQ)，这是一个1
    MB的文件，包含了由600名用户对9,000部电影进行的100,000次评分和3,600次标签应用。
- en: 'All ratings are contained in the file *ratings.csv*. Each line of this file
    after the header row represents one rating of one movie by one user, and has the
    following format:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的评分都包含在文件*ratings.csv*中。该文件中的每一行在标题行之后表示一个用户对一部电影的一次评分，格式如下：
- en: '[PRE52]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In this file:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中：
- en: The lines are ordered first by `userId`, then, for each user, by `movieId`.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些行首先按`userId`排序，然后对于每个用户，按`movieId`排序。
- en: Ratings are made on a 5-star scale, with half-star increments (0.5 stars to
    5.0 stars).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分是在5星级的基础上进行的，增量为半星（从0.5星到5.0星）。
- en: Timestamps represent seconds since midnight Coordinated Universal Time (UTC)
    of January 1, 1970 (this field is ignored in our analysis).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳表示自1970年1月1日协调世界时（UTC）午夜以来的秒数（此字段在我们的分析中被忽略）。
- en: 'After unzipping the downloaded file, you should have the following files:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 解压下载的文件后，您应该有以下文件：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'First, check the number of records (the number of records you see might be
    different based on when you downloaded the file):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查记录数（根据您下载文件的时间，您看到的记录数可能会有所不同）：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, take a look at the first few records:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看一下前几条记录：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Since we are using RDDs, we do not need the metadata associated with the data.
    Therefore, we can remove the first line (the header line) from the *ratings.csv*
    file:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用RDDs，我们不需要与数据相关联的元数据。因此，我们可以从*ratings.csv*文件中删除第一行（标题行）：
- en: '[PRE56]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now that we’ve acquired our sample data, we can work through a few solutions
    to this problem. The first solution will use `aggregateByKey()`, but before we
    get to that I’ll present the logic behind this transformation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获取了样本数据，我们可以解决这个问题的几个方案。第一个解决方案将使用`aggregateByKey()`，但在此之前，我将介绍此转换背后的逻辑。
- en: The aggregateByKey() Transformation
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`aggregateByKey()`转换'
- en: Spark’s `aggregateByKey()` transformation initializes each key on each partition
    with the zero value, which is an initial combined data type (`C`); this is a neutral
    value, typically `(0, 0)` if the combined data type is `(sum, count)`. This zero
    value is merged with the first value in the partition to create a new `C`, which
    is then merged with the second value. This process continues until we’ve merged
    all the values for that key. Finally, if the same key exists in multiple partitions,
    these values are combined together to produce the final `C`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`aggregateByKey()`转换会初始化每个键在每个分区上的零值，这是一个初始组合数据类型（`C`）；这是一个中性值，通常为`(0,
    0)`，如果组合数据类型是`(sum, count)`。这个零值与分区中的第一个值合并以创建一个新的`C`，然后与第二个值合并。这个过程继续，直到我们合并了该键的所有值。最后，如果同一个键存在于多个分区中，则这些值将组合在一起以生成最终的`C`。
- en: Figures [4-5](#aggregatedbykey_with_zero_value_equals_0_0) and [4-6](#aggregatebykey_with_zero_value_equal_10_20)
    show how `aggregateByKey()` works with different zero values. The zero value is
    applied per key, per partition. This means that if a key *`X`* is in *`N`* partitions,
    the zero value is applied *`N`* times (each of these *`N`* partitions will be
    initialized to the zero value for key *`X`*). Therefore, it’s important to select
    this value carefully.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4-5](#aggregatedbykey_with_zero_value_equals_0_0) 和 [4-6](#aggregatebykey_with_zero_value_equal_10_20)
    展示了`aggregateByKey()`如何使用不同的零值工作。零值是按键、每个分区应用的。这意味着如果一个键 *`X`* 在 *`N`* 个分区中，零值将应用
    *`N`* 次（这 *`N`* 个分区的每个都将为键 *`X`* 初始化为零值）。因此，选择此值非常重要。
- en: '[Figure 4-5](#aggregatedbykey_with_zero_value_equals_0_0) demonstrates how
    `aggregateByKey()` works with `zero-value=(0, 0)`.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](#aggregatedbykey_with_zero_value_equals_0_0) 演示了`aggregateByKey()`如何与`zero-value=(0,
    0)`一起工作。'
- en: '![daws 0405](Images/daws_0405.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0405](Images/daws_0405.png)'
- en: Figure 4-5\. `aggregateByKey()` with `zero-value=(0, 0)`
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. `aggregateByKey()`与`zero-value=(0, 0)`
- en: Typically, you would use (0, 0) but [Figure 4-6](#aggregatebykey_with_zero_value_equal_10_20)
    demonstrates how the same transformation works with a zero value of (10, 20).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您会使用`(0, 0)`，但[图 4-6](#aggregatebykey_with_zero_value_equal_10_20)展示了如何使用零值为`(10,
    20)`的相同转换工作。
- en: '![daws 0406](Images/daws_0406.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0406](Images/daws_0406.png)'
- en: Figure 4-6\. `aggregateByKey()` with `zero-value=(10, 20)`
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. `aggregateByKey()`与`zero-value=(10, 20)`
- en: First Solution Using aggregateByKey()
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个解决方案使用`aggregateByKey()`：
- en: 'To find the average rating for each user, the first step is to map each record
    into (key, value) pairs of the form:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到每个用户的平均评分，第一步是将每条记录映射为形式为（键，值）对：
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The simplest way to add up values per key is to use the `reduceByKey()` transformation,
    but we can’t use `reduceByKey()` to find the average rating per user because,
    as we’ve seen, the mean/average function is not a monoid over a set of ratings
    (as float numbers). To make this a monoid operation, we use a pair data structure
    (a tuple of two elements) to hold a pair of values, `(sum, count)`, where `sum`
    is the aggregated sum of ratings and `count` is the number of ratings we have
    added (summed) so far, and we use the `aggregateByKey()` transformation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`reduceByKey()`转换来按键累加值的最简单方法，但是我们不能用`reduceByKey()`来计算每个用户的平均评分，因为如我们所见，平均函数对评分集（作为浮点数）不是一个幺半群。为了使其成为幺半群操作，我们使用一对数据结构（一个包含两个元素的元组），来保存一对值，`(sum,
    count)`，其中`sum`是评分的累计总和，`count`是我们迄今已添加（累加）的评分数，我们使用`aggregateByKey()`转换。
- en: Let’s prove that the pair structure `(sum, count)` with an addition operator
    over a set of numbers is a monoid.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们证明，对一组数字使用加法运算符的配对结构`(sum, count)`是一个幺半群。
- en: 'If we use `(0.0, 0)` as our zero element, it is neutral:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`(0.0, 0)`作为我们的零元素，它是中性的：
- en: '[PRE58]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The operation is commutative (that is, the result is independent of the order
    of the elements in the RDD being aggregated):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作是可交换的（即，聚合的RDD元素顺序不影响结果）：
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'It is also associative (the order in which elements are aggregated does not
    affect the final result):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 它还是可结合的（聚合元素的顺序不影响最终结果）。
- en: '[PRE60]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To make things simple, we’ll define a very basic Python function, `create_pair()`,
    which accepts a record of movie rating data and returns a pair of `(userID, rating)`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事务，我们将定义一个非常基本的Python函数，`create_pair()`，它接受电影评分数据的记录并返回`(userID, rating)`对：
- en: '[PRE61]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, we test the function:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们测试该函数：
- en: '[PRE62]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here is a PySpark solution using `aggregateByKey()` and our `create_pair()`
    function. The combined type (`C`) to denote values for the `aggregateByKey()`
    operation is a pair of `(sum-of-ratings, count-of-ratings)`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用`aggregateByKey()`和我们的`create_pair()`函数的PySpark解决方案。组合类型(`C`)用于表示`aggregateByKey()`操作的值是一对`(sum-of-ratings,
    count-of-ratings)`。
- en: '[PRE63]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO7-1)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO7-1)'
- en: The source RDD, `ratings`, is an `RDD[(String, Float)]` where the key is a `userID`
    and the value is a `rating`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 源RDD `ratings`是一个`RDD[(String, Float)]`，其中键是`userID`，值是`rating`。
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO7-2)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO7-2)'
- en: The target RDD, `sum_count`, is an `RDD[(String, (Float, Integer))]` where the
    key is a `userID` and the value is a pair `(sum-of-ratings, count-of-ratings)`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 目标RDD `sum_count` 是一个`RDD[(String, (Float, Integer))]`，其中键是`userID`，值是一对`(sum-of-ratings,
    count-of-ratings)`。
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO7-3)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_reductions_in_spark_CO7-3)'
- en: '`C` is initialized to this value in each partition.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`C`在每个分区中被初始化为此值。'
- en: '[![4](Images/4.png)](#co_reductions_in_spark_CO7-4)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_reductions_in_spark_CO7-4)'
- en: This is used to combine values within a single partition.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于在单个分区内合并值。
- en: '[![5](Images/5.png)](#co_reductions_in_spark_CO7-5)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_reductions_in_spark_CO7-5)'
- en: This is used to combine the results from different partitions.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于合并来自不同分区的结果。
- en: Let’s break down what’s happening here. First, we the `aggregateByKey()` function
    and create a result set “template” with the initial values. We’re starting the
    data out as `(0.0, 0)`, so the initial sum of ratings is `0.0` and the initial
    count of records is `0`. For each row of data, we’re going to do some adding.
    `C` is the new template, so `C[0]` is referring to our “sum” element (`sum-of-ratings`),
    while `C[1]` is the “count” element (`count-of-ratings`). Finally, we combine
    the values from the different partitions. To do this, we simply add the `C1` values
    to the `C2` values based on the template we made.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下这里发生了什么。首先，我们使用`aggregateByKey()`函数并创建一个结果集“模板”，其中包含初始值。我们将数据起始为`(0.0,
    0)`，因此评分总和初始值为`0.0`，记录计数初始值为`0`。对于每一行数据，我们将执行一些加法操作。`C`是新的模板，因此`C[0]`是指我们的“总和”元素（`sum-of-ratings`），而`C[1]`是“计数”元素（`count-of-ratings`）。最后，我们根据我们制作的模板，将不同分区的`C1`值添加到`C2`值中。
- en: 'The data in the `sum_count` RDD will end up looking like the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum_count` RDD中的数据将看起来像下面这样：'
- en: '[PRE64]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This tells us that user `100` has rated 10 movies and the sum of all their ratings
    was 40.0; user `200` has rated 13 movies and the sum of their ratings was 51.0
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们用户`100`评价了10部电影，他们所有评分的总和为40.0；用户`200`评价了13部电影，他们所有评分的总和为51.0。
- en: 'Now, to get the actual average rating per user, we need to use the `mapValues()`
    transformation and divide the first entry (`sum-of-ratings`) by the second entry
    (`count-of-ratings`):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得每个用户的实际平均评分，我们需要使用`mapValues()`转换，并将第一个条目（`sum-of-ratings`）除以第二个条目（`count-of-ratings`）：
- en: '[PRE65]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO8-1)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO8-1)'
- en: '`average_rating` is an `RDD[(String, Float)]` where the key is a `userID` and
    the value is an `average-rating`.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`average_rating`是一个`RDD[(String, Float)]`，其中键是`userID`，值是`average-rating`。'
- en: 'The contents of this RDD are as follows, giving us the result we’re looking
    for:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 此RDD的内容如下，给出了我们正在寻找的结果：
- en: '[PRE66]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Second Solution Using aggregateByKey()
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二种使用`aggregateByKey()`解决方案
- en: Here, I’ll present another solution using the `aggregateByKey()` transformation.
    Note that to save space, I have trimmed the output generated by the PySpark shell.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我将展示另一种使用`aggregateByKey()`转换的解决方案。请注意，为了节省空间，我已经修剪了PySpark shell生成的输出。
- en: 'The first step is to read the data and create (key, value) pairs, where the
    key is a `userID` and the value is a `rating`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是读取数据并创建（键，值）对，其中键是`userID`，值是`rating`：
- en: '[PRE67]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Once we’ve created the (key, value) pairs, we can apply the `aggregateByKey()`
    transformation to sum up the ratings. The initial value of `(0.0, 0)` is used
    for each partition, where `0.0` is the sum of the ratings and `0` is the number
    of ratings:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了（键，值）对，我们可以对评分进行汇总应用`aggregateByKey()`转换。每个分区的初始值`(0.0, 0)`用于此操作，其中`0.0`是评分的总和，`0`是评分的数量：
- en: '[PRE68]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO9-1)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO9-1)'
- en: The target RDD is an `RDD[(String, (Float, Integer))]`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 目标 RDD 是一个 `RDD[(String, (Float, Integer))]`。
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO9-2)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO9-2)'
- en: '`C` is initialized to `(0.0, 0)` in each partition.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`C` 在每个分区中初始化为 `(0.0, 0)`。'
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO9-3)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_reductions_in_spark_CO9-3)'
- en: This lambda expression adds a single value of `V` to `C` (used in a single partition).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Lambda 表达式将单个 `V` 的值添加到 `C` 中（在单个分区中使用）。
- en: '[![4](Images/4.png)](#co_reductions_in_spark_CO9-4)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_reductions_in_spark_CO9-4)'
- en: This lambda expression combines the values across partitions (adds two `C`s
    to create a single `C`).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Lambda 表达式将跨分区的值组合起来（将两个 `C` 相加以创建一个单独的 `C`）。
- en: 'We could use Python functions instead of lambda expressions. To do this, we
    would need to write the following functions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Python 函数替代 Lambda 表达式。为此，我们需要编写以下函数：
- en: '[PRE69]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, we can compute `sum_count` using the defined functions:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用定义的函数计算 `sum_count`：
- en: '[PRE70]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The previous step created RDD elements of the following type:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步创建了以下类型的 RDD 元素：
- en: '[PRE71]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, we do the final calculation to find the average rating per user:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行最终计算，找到每个用户的平均评分：
- en: '[PRE72]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Next, I’ll present a solution to the movies problem using `groupByKey()`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将使用 `groupByKey()` 解决电影问题。
- en: Complete PySpark Solution Using groupByKey()
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `groupByKey()` 的完整 PySpark 解决方案
- en: 'For a given set of `(K, V)` pairs, `groupByKey()` has the following signature:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的 `(K, V)` 对集合，`groupByKey()` 具有以下签名：
- en: '[PRE73]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: If the source RDD is an `RDD[(K, V)]`, the `groupByKey()` transformation groups
    the values for each key (`K`) in the RDD into a single sequence as a list/iterable
    of `V`s. It then hash-partitions the resulting RDD with the existing partitioner/parallelism
    level. The ordering of elements within each group is not guaranteed, and may even
    differ each time the resulting RDD is evaluated.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源 RDD 是 `RDD[(K, V)]`，`groupByKey()` 转换会将 RDD 中每个键（`K`）的值分组为一个 `V` 的列表/可迭代对象。然后，它会使用现有的分区器/并行级别对生成的
    RDD 进行哈希分区。每个组内元素的顺序不保证，甚至每次评估结果的 RDD 时顺序可能都不同。
- en: Tip
  id: totrans-324
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can customize both the number of partitions (`numPartitions`) and partitioning
    function (`partitionFunc`).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自定义分区数 (`numPartitions`) 和分区函数 (`partitionFunc`)。
- en: Here, I present a complete solution using the `groupByKey()` transformation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用 `groupByKey()` 转换提供了一个完整的解决方案。
- en: 'The first step is to read the data and create (key, value) pairs, where the
    key is a `userID` and the value is a `rating`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是读取数据并创建 `(key, value)` 对，其中键是 `userID`，值是 `rating`：
- en: '[PRE74]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO10-1)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO10-1)'
- en: '`ratings` is an `RDD[(String, Float)]`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratings` 是一个 `RDD[(String, Float)]`'
- en: Once we’ve created the (key, value) pairs, we can apply the `groupByKey()` transformation
    to group all ratings for a user. This step creates `(userID, [R[1], ..., R[n]])`
    pairs, where `R[1]`, …, `R[n]` are all of the ratings for a unique `userID`.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了 `(key, value)` 对，我们就可以应用 `groupByKey()` 转换来将所有用户的评分分组起来。此步骤创建了 `(userID,
    [R[1], ..., R[n]])` 对，其中 `R[1]`，…，`R[n]` 是唯一 `userID` 的所有评分。
- en: 'As you will notice, the `groupByKey()` transformation works exactly like SQL’s
    `GROUP BY`. It groups values of the same key as an iterable of values:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所注意到的，`groupByKey()` 转换的工作方式与 SQL 的 `GROUP BY` 完全相同。它将相同键的值分组为一个值的可迭代对象：
- en: '[PRE75]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO11-1)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO11-1)'
- en: '`ratings_grouped` is an `RDD[(String, [Float])]` where the key is a `userID`
    and the value is a list of `rating`s.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratings_grouped` 是一个 `RDD[(String, [Float])]`，其中键是 `userID`，值是一个 `rating`
    列表。'
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO11-2)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO11-2)'
- en: The full name of `ResultIterable` is `pyspark.resultiterable.ResultIterable`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultIterable` 的完整名称是 `pyspark.resultiterable.ResultIterable`。'
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO11-3)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_reductions_in_spark_CO11-3)'
- en: For debugging, convert the `ResultIterable` object to a list of `Integer`s.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调试，将 `ResultIterable` 对象转换为 `Integer` 列表。
- en: 'To find the average rating per user, we sum up all the ratings for each `userID`
    and then calculate the averages:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到每个用户的平均评分，我们需要将每个 `userID` 的所有评分求和，然后计算平均值：
- en: '[PRE76]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO12-1)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO12-1)'
- en: '`average_rating` is an `RDD[(String, Float)]` where the key is `userID` and
    the value is `average-rating`.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`average_rating` 是一个 `RDD[(String, Float)]`，其中键是 `userID`，值是 `average-rating`。'
- en: Complete PySpark Solution Using reduceByKey()
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `reduceByKey()` 的完整 PySpark 解决方案
- en: 'In its simplest form, `reduceByKey()` has the following signature (the source
    and target data types, `V`, must be the same):'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，`reduceByKey()`具有以下签名（源和目标数据类型`V`必须相同）：
- en: '[PRE77]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '`reduceByKey()` transformation merges the values for each key using an associative
    and commutative reduce function. This will also perform the merging locally on
    each mapper before sending the results to a reducer, similarly to a combiner in
    MapReduce. The output will be partitioned with `numPartitions` partitions, or
    the default parallelism level if `numPartitions` is not specified. The default
    partitioner is `HashPartitioner`.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey()`转换使用关联和交换的减少函数合并每个键的值。这也会在每个mapper上本地执行合并，然后将结果发送给reducer，类似于MapReduce中的combiner。输出将使用`numPartitions`分区进行分区，或者如果未指定`numPartitions`，则使用默认的并行级别。默认的分区器是`HashPartitioner`。'
- en: Since we want to find the average rating for all movies rated by a user, and
    we know that the mean of means is not a mean (the mean function is not a monoid),
    we need to add up all the ratings for each user and keep track of the number of
    movies they’ve rated. Then, `(sum_of_ratings`, `number_of_ratings)` is a monoid
    over an addition function, but at the end we need to perform one more `mapValues()`
    transformation to find the actual average rating by dividing `sum_of_ratings`
    by `number_of_ratings`. The complete solution using `reduceByKey()` is given here.
    Note that `reduceByKey()` is more efficient and scalable than a `groupByKey()`
    transformation, since merging and combining are done locally before sending data
    for the final reduction.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要找到每个用户评分的平均值，并且我们知道均值不是均值（均值函数不是幺半群），我们需要添加每个用户的所有评分并跟踪他们评分的电影数量。然后，`(总评分,
    评分数量)`是加法函数上的幺半群，但最后我们需要执行一次`mapValues()`转换来通过`评分数量`除以`总评分`找到实际的平均评分。这里给出使用`reduceByKey()`的完整解决方案。请注意，`reduceByKey()`比`groupByKey()`转换更高效和可扩展，因为合并和组合在发送数据进行最终缩减之前在本地完成。
- en: 'Step 1: Read data and create pairs'
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步：读取数据并创建对
- en: 'The first step is to read the data and create (key, value) pairs, where the
    key is a `userID` and the value is a pair of `(rating, 1)`. To use `reduceByKey()`
    for finding averages, we need to find the `(sum_of_ratings, number_of_ratings)`.
    We start by reading the input data and creating an `RDD[String]`:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是读取数据并创建(key, value)对，其中key是`userID`，value是`(rating, 1)`对。为了使用`reduceByKey()`找到平均值，我们需要找到`(总评分,
    评分数量)`。我们首先读取输入数据并创建一个`RDD[String]`：
- en: '[PRE78]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Then we transform the `RDD[String]` into an `RDD[(String, (Float, Integer))]`:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将`RDD[String]`转换为`RDD[(String, (Float, Integer))]`：
- en: '[PRE79]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO13-1)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO13-1)'
- en: Create the pair RDD.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 创建成对的RDD。
- en: 'Step 2: Use reduceByKey() to sum up ratings'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步：使用reduceByKey()汇总评分
- en: 'Once we’ve created the `(userID, (rating, 1))` pairs we can apply the `reduceByKey()`
    transformation to sum up all the ratings and the number of ratings for a given
    user. The output of this step will be tuples of `(userID,` `(sum_of_ratings,`
    `number_of_ratings))`:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了`(userID, (rating, 1))`对，我们可以应用`reduceByKey()`转换来总结给定用户的所有评分和评分数量。这一步的输出将是`(userID,`
    `(总评分,` `评分数量))`的元组：
- en: '[PRE80]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO14-1)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO14-1)'
- en: The source RDD (`ratings`) is an `RDD[(String, (Float, Integer))]`.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 源RDD(`ratings`)是`RDD[(String, (Float, Integer))]`。
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO14-2)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_reductions_in_spark_CO14-2)'
- en: The target RDD (`sum_and_count`) is an `RDD[(String, (Float, Integer))]`. Notice
    that the data types for the source and target are the same.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 目标RDD(`sum_and_count`)是`RDD[(String, (Float, Integer))]`。注意源和目标的数据类型相同。
- en: 'Step 3: Find average rating'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：找到平均评分
- en: 'Divide `sum_of_ratings` by `number_of_ratings` to find the average rating per
    user:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`评分数量`除以`总评分`找到每个用户的平均评分：
- en: '[PRE81]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Complete PySpark Solution Using combineByKey()
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`combineByKey()`的完整PySpark解决方案
- en: '`combineByKey()` is a more general and extended version of `reduceByKey()`
    where the result type can be different than the type of the values being aggregated.
    This is a limitation of `reduceByKey()`; it means that, given the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey()`是`reduceByKey()`的更一般和扩展版本，其中结果类型可以与聚合的值的类型不同。这是`reduceByKey()`的一个限制；这意味着，鉴于以下情况：'
- en: '[PRE82]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '`func(x,y)` must create a value of type `T`.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`func(x,y)`必须创建类型为`T`的值。'
- en: The `combineByKey()` transformation is an optimization that aggregates values
    for a given key before sending aggregated partition values to the designated reducer.
    This aggregation is performed in each partition, and then the values from all
    the partitions are merged into a single value. Thus, like with `reduceByKey()`,
    each partition outputs at most one value for each key to send over the network,
    which speeds up the shuffle step. However, unlike with `reduceByKey()`, the type
    of the combined (result) value does not have to match the type of the original
    value.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey()` 转换是一种优化，它在将聚合分区值发送到指定的 reducer 之前，为给定的键聚合值。此聚合在每个分区中执行，然后将所有分区的值合并为一个单一的值。因此，就像
    `reduceByKey()` 一样，每个分区最多为每个键输出一个值以发送到网络，这加速了 shuffle 步骤。然而，与 `reduceByKey()`
    不同的是，组合（结果）值的类型不必与原始值的类型匹配。'
- en: 'For a given set of `(K, V)` pairs, `combineByKey()` has the following signature
    (this transformation has many different versions; this is the simplest form):'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '-   对于给定的 `(K, V)` 对集合，`combineByKey()` 具有以下签名（此转换有许多不同版本；这是最简单的形式）：'
- en: '[PRE83]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This is a generic function to combine the elements for each key using a custom
    set of aggregation functions. It converts an `RDD[(K, V)]` into a result of type
    `RDD[(K, C)]`, where `C` is a combined type. It can be a simple data type such
    as `Integer` or `String`, or it can be a composite data structure such as a (key,
    value) pair, a triplet `(x, y, z)`, or whatever else you desire. This flexibility,
    makes `combineByKey()` a very powerful reducer.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '-   这是一个通用函数，使用自定义的聚合函数组合每个键的元素。它将 `RDD[(K, V)]` 转换为类型为 `RDD[(K, C)]` 的结果，其中
    `C` 是一个组合类型。它可以是简单的数据类型，如 `Integer` 或 `String`，也可以是复合数据结构，如 (key, value) 对，三元组
    `(x, y, z)` 或其他任何你想要的。这种灵活性使得 `combineByKey()` 成为一个非常强大的 reducer。'
- en: 'As discussed earlier in this chapter, given a source RDD `RDD[(K, V)]`, we
    have to provide three basic functions:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '-   正如本章前面讨论的，给定一个源 RDD `RDD[(K, V)]`，我们必须提供三个基本函数：'
- en: '[PRE84]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: To avoid memory allocation, both `merge_value` and `merge_combiners` are allowed
    to modify and return their first argument instead of creating a new `C` (this
    avoids creating new objects, which can be costly if you have a lot of data).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '-   为避免内存分配，`merge_value` 和 `merge_combiners` 都允许修改并返回它们的第一个参数，而不是创建新的 `C`（这避免了创建新对象，如果数据量很大，这可能是昂贵的）。'
- en: In addition, users can control (by providing additional parameters) the partitioning
    of the output RDD, the serializer that is used for the shuffle, and whether to
    perform map-side aggregation (i.e., if a mapper can produce multiple items with
    the same key). The `combineByKey()` transformation thus provides quite a bit of
    flexibility, but it is a little more complex to use than some of the other reduction
    transformations.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '-   此外，用户可以通过提供额外的参数来控制输出 RDD 的分区、用于 shuffle 的序列化程序，以及是否执行 map-side 聚合（即如果
    mapper 可以生成具有相同键的多个项目）。因此，`combineByKey()` 转换提供了相当多的灵活性，但比一些其他缩减转换更复杂。'
- en: Let’s see how we can use `combineByKey()` to solve the movie problem.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '-   让我们看看如何使用 `combineByKey()` 来解决电影问题。'
- en: 'Step 1: Read data and create pairs'
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   步骤 1：读取数据并创建对'
- en: 'As in the previous solutions, the first step is to read the data and create
    (key, value) pairs where the key is a `userID` and the value is a `rating`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '-   就像以前的解决方案一样，第一步是读取数据并创建 (key, value) 对，其中 key 是 `userID`，value 是 `rating`：'
- en: '[PRE85]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO15-1)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [![1](Images/1.png)](#co_reductions_in_spark_CO15-1)'
- en: '`rdd` is an `RDD[String]`.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '-   `rdd` 是一个 `RDD[String]`。'
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO15-2)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [![2](Images/2.png)](#co_reductions_in_spark_CO15-2)'
- en: '`ratings` is an `RDD[(String, Float)]`.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '-   `ratings` 是一个 `RDD[(String, Float)]`。'
- en: 'Step 2: Use combineByKey() to sum up ratings'
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   使用 `combineByKey()` 对评级进行求和'
- en: 'Once we’ve created the `(userID, rating)` pairs , we can apply the `combineByKey()`
    transformation to sum up all the ratings and the number of ratings for each user.
    The output of this step will be `(userID, (sum_of_ratings, number_of_ratings))`
    pairs:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '-   一旦我们创建了 `(userID, rating)` 对，我们可以应用 `combineByKey()` 转换来总结每个用户的所有评级和评级数。这一步的输出将是
    `(userID, (sum_of_ratings, number_of_ratings))` 对：'
- en: '[PRE86]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO16-1)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [![1](Images/1.png)](#co_reductions_in_spark_CO16-1)'
- en: The source RDD is an `RDD[(String, Float)]`.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '-   源 RDD 是一个 `RDD[(String, Float)]`。'
- en: '[![2](Images/2.png)](#co_reductions_in_spark_CO16-2)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [![2](Images/2.png)](#co_reductions_in_spark_CO16-2)'
- en: The target RDD is an `RDD[(String, (Float, Integer))]`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '-   目标 RDD 是一个 `RDD[(String, (Float, Integer))]`。'
- en: '[![3](Images/3.png)](#co_reductions_in_spark_CO16-3)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [![3](Images/3.png)](#co_reductions_in_spark_CO16-3)'
- en: This turns a `V` (a single value) into a `C` as `(V, 1)`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '-   这将一个 `V`（单个值）转换为 `(V, 1)`。'
- en: '[![4](Images/4.png)](#co_reductions_in_spark_CO16-4)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_reductions_in_spark_CO16-4)'
- en: This merges a `V` (rating) into a `C` as `(sum, count)`.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 将`V`（评分）合并为`C`（总和，计数）。
- en: '[![5](Images/5.png)](#co_reductions_in_spark_CO16-5)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_reductions_in_spark_CO16-5)'
- en: This combines two `C`s into a single `C`.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这将两个`C`合并为一个`C`。
- en: 'Step 3: Find average rating'
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤3：找到平均评分
- en: 'Divide `sum_of_ratings` by `number_of_ratings` to find the average rating per
    user:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sum_of_ratings`除以`number_of_ratings`以找到每个用户的平均评分：
- en: '[PRE87]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Next, we’ll examine the shuffle step in Spark’s reduction transformations.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细讨论Spark缩减转换中的洗牌步骤。
- en: The Shuffle Step in Reductions
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩减操作中的洗牌步骤
- en: 'Once all the mappers have finished emitting (key, value) pairs, MapReduce’s
    magic happens: the sort and shuffle step. This step groups (sorts) the output
    of the map phase by keys and sends the results to the reducer(s). From an efficiency
    and scalability point of view, it’s different for different transformations.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有的映射器完成了（键，值）对的发射，MapReduce的魔法发生了：排序和洗牌步骤。该步骤通过键分组（排序）映射阶段的输出，并将结果发送到减少器。从效率和可伸缩性的角度来看，不同的转换操作有所不同。
- en: The idea of sorting by keys should be familiar by now, so here I’ll focus on
    the shuffle. In a nutshell, shuffling is the process of redistributing data across
    partitions. It may or may not cause data to be moved across JVM processes, or
    even over the wire (between executors on separate servers).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很熟悉按键排序的概念了，所以我将专注于洗牌过程。简言之，洗牌是重新分配数据到分区的过程。它可能会导致数据移动到不同的JVM进程，甚至通过网络（在分布式服务器的执行器之间）。
- en: I’ll explain the concept of shuffling with an example. Imagine that you have
    a 100-node Spark cluster. Each node has records containing data on the frequency
    of URL visits, and you want to calculate the total frequency per URL. As you know
    by now, you can achieve this by reading the data and creating (key, value) pairs,
    where the key is a `URL` and the value is a `frequency`, then summing up the frequencies
    for each URL. But if the data is spread across the cluster, how can you sum up
    the values for the same key stored on different servers? The only way to do this
    is to get all the values for the same key onto the same server; then you can sum
    them up easily. This process is called shuffling.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过一个例子解释洗牌的概念。假设您有一个100节点的Spark集群。每个节点都包含有关URL访问频率的记录，并且您想计算每个URL的总频率。正如您现在所知，您可以通过读取数据并创建（键，值）对来实现，其中键是`URL`，值是`频率`，然后对每个URL的频率求和。但是如果数据分布在集群中，如何将存储在不同服务器上的相同键的值求和？唯一的方法是将同一键的所有值获取到同一台服务器上，然后可以轻松地对其进行求和。这个过程称为洗牌。
- en: There are many transformations (such as `reduceByKey()` and `join()`) that require
    shuffling of data across the cluster, but it can be an expensive operation. Shuffling
    data for `groupByKey()` is different from shuffling `reduceByKey()` data, and
    this difference affects the performance of each transformation. Therefore, it
    is very important to properly select and use reduction transformations.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多转换操作（例如`reduceByKey()`和`join()`）需要在集群中对数据进行洗牌，但这可能是一项昂贵的操作。对于`groupByKey()`的数据洗牌与`reduceByKey()`的数据洗牌不同，这种差异影响每种转换的性能。因此，正确选择和使用缩减转换非常重要。
- en: 'Consider the following PySpark solution to a simple word count problem:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下PySpark解决简单词频统计问题的方案：
- en: '[PRE88]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[![1](Images/1.png)](#co_reductions_in_spark_CO17-1)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_reductions_in_spark_CO17-1)'
- en: '`3` is the number of partitions.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`3`是分区的数量。'
- en: Since we directed the `reduceByKey()` transformation to create three partitions,
    the resulting RDD will be partitioned into three chunks, as depicted in [Figure 4-7](#sparks_shuffle_concept).
    The RDD operations are compiled into a directed acyclic graph of RDD objects,
    where each RDD maintains a pointer to the parent(s) it depends on. As this figure
    shows, at shuffle boundaries the DAG is partitioned into *stages* (Stage 1, Stage
    2, etc.) that are executed in order.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们指定了`reduceByKey()`转换创建三个分区，因此生成的RDD将被分成三个块，如图[Figure 4-7](#sparks_shuffle_concept)所示。RDD操作被编译成RDD对象的有向无环图（DAG），每个RDD都维护指向其依赖父级的指针。正如本图所示，在洗牌边界处，DAG被分割成*阶段*（Stage
    1，Stage 2等），按顺序执行。
- en: '![daws 0407](Images/daws_0407.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0407](Images/daws_0407.png)'
- en: Figure 4-7\. Spark’s shuffle concept
  id: totrans-414
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. Spark的洗牌概念
- en: Since shuffling involves copying data across executors and servers, this is
    a complex and costly operation. Let’s take a closer look at how it works for two
    Spark reduction transformations, `groupByKey()` and `reduceByKey()`. This will
    help illustrate the importance of choosing the appropriate reduction.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 由于洗牌涉及在执行器和服务器之间复制数据，这是一个复杂且昂贵的操作。让我们更详细地看看它如何适用于两个Spark的归约转换，`groupByKey()`和`reduceByKey()`。这将帮助说明选择适当的归约的重要性。
- en: Shuffle Step for groupByKey()
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`groupByKey()`的洗牌步骤'
- en: The `groupByKey()` shuffle step is pretty straightforward. It does not merge
    the values for each key; instead, the shuffle happens directly. This means a large
    volume of data gets sent to each partition, because there’s no reduction in the
    initial data values. The merging of values for each key happens after the shuffle
    step. With `groupByKey()`, a lot of data needs to be stored on final worker nodes
    (reducers), which means you may run into OOM errors if there’s lots of data per
    key. [Figure 4-8](#shuffle_step_for_groupbeykey) illustrates the process. Note
    that after `groupByKey()`, you need to call `mapValues()` to generate your final
    desired output.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey()`的洗牌步骤非常直接。它不会合并每个键的值；相反，洗牌直接发生。这意味着大量数据会发送到每个分区，因为初始数据值没有减少。每个键的值合并发生在洗牌步骤之后。使用`groupByKey()`时，大量数据需要存储在最终的工作节点（减少器）上，这意味着如果每个键有大量数据，则可能会遇到OOM错误。[图 4-8](#shuffle_step_for_groupbeykey)说明了这个过程。请注意，在`groupByKey()`之后，您需要调用`mapValues()`来生成最终期望的输出。'
- en: '![daws 0408](Images/daws_0408.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0408](Images/daws_0408.png)'
- en: Figure 4-8\. Shuffle step for `groupByKey()`
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. `groupByKey()`的洗牌步骤
- en: Because `groupByKey()` does not merge or combine values, it’s an expensive operation
    that requires moving large amounts of data over the network.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`groupByKey()`不会合并或组合值，所以它是一种昂贵的操作，需要在网络上移动大量数据。
- en: Shuffle Step for reduceByKey()
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`reduceByKey()`的洗牌步骤'
- en: With `reduceByKey()`, the data in each partition is combined so that there is
    at most one value for each key in each partition. Then the shuffle happens, and
    this data is sent over the network to the reducers, as illustrated in [Figure 4-9](#shuffle_steo_for_reducebykey).
    Note that with `reduceByKey()`, you do not need need to call `mapValues()` to
    generate your final desired output. In general, it’s equivalent to using `groupByKey()`
    and `mapValues()`, but because of the reduction in the amount of data sent over
    the network it is a much more efficient and performant solution.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`reduceByKey()`时，每个分区中的数据被组合，以便每个分区中每个键最多只有一个值。然后发生洗牌，将这些数据发送到减少器，如[图 4-9](#shuffle_steo_for_reducebykey)所示。请注意，使用`reduceByKey()`时，您不需要调用`mapValues()`来生成最终期望的输出。一般来说，它相当于使用`groupByKey()`和`mapValues()`，但由于减少了通过网络发送的数据量，这是一种更高效和更有效的解决方案。
- en: '![daws 0409](Images/daws_0409.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0409](Images/daws_0409.png)'
- en: Figure 4-9\. Shuffle step for `reduceByKey()`
  id: totrans-424
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. `reduceByKey()`的洗牌步骤
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter introduced Spark’s reduction transformations and presented multiple
    solutions to a real-world data problem with the most commonly used of these transformations:
    `reduceByKey()`, `aggregateByKey()`, `combineByKey()`, and `groupByKey()`. As
    you’ve seen, there are many ways to solve the same data problem, but they do not
    all have the same performance.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Spark的归约转换，并提供了多个解决实际数据问题的解决方案，其中最常用的是`reduceByKey()`、`aggregateByKey()`、`combineByKey()`和`groupByKey()`。正如您所见，解决相同数据问题的方法有很多种，但它们的性能并不相同。
- en: '[Table 4-2](#table_4-2) summarizes the types of transformations performed by
    these four reduction transformations (note that `V` and `C` can be different data
    types).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-2](#table_4-2)总结了这四个归约转换执行的转换类型（请注意，`V`和`C`可以是不同的数据类型）。'
- en: Table 4-2\. Comparison of Spark reductions
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. Spark归约的比较
- en: '| Reduction | Source RDD | Target RDD |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 归约 | 源RDD | 目标RDD |'
- en: '| --- | --- | --- |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `reduceByKey()` | `RDD[(K, V)]` | `RDD[(K, V)]` |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey()` | `RDD[(K, V)]` | `RDD[(K, V)]` |'
- en: '| `groupByKey()` | `RDD[(K, V)]` | `RDD[(K, [V])]` |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| `groupByKey()` | `RDD[(K, V)]` | `RDD[(K, [V])]` |'
- en: '| `aggregateByKey()` | `RDD[(K, V)]` | `RDD[(K, C)]` |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| `aggregateByKey()` | `RDD[(K, V)]` | `RDD[(K, C)]` |'
- en: '| `combineByKey()` | `RDD[(K, V)]` | `RDD[(K, C)]` |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| `combineByKey()` | `RDD[(K, V)]` | `RDD[(K, C)]` |'
- en: We learned that some of the reduction transformations (such as `reduceByKey()`
    and `combineByKey()`) are preferable over `groupByKey()`, due to the shuffle step
    for `groupByKey()` being more expensive. When possible, you should `reduceByKey()`
    instead of `groupByKey()`, or use `combineByKey()` when you are combining elements
    but your return type differs from your input value type. Overall, for large volumes
    of data, `reduceByKey()` and `combineByKey()` will perform and scale out better
    than `groupByKey()`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到一些减少转换（例如 `reduceByKey()` 和 `combineByKey()`）比 `groupByKey()` 更可取，因为 `groupByKey()`
    的洗牌步骤更昂贵。在可能的情况下，您应该使用 `reduceByKey()` 而不是 `groupByKey()`，或者在您需要组合元素但返回类型与输入值类型不同时，使用
    `combineByKey()`。总体而言，对于大量数据，`reduceByKey()` 和 `combineByKey()` 的性能和扩展性会更好。
- en: The `aggregateByKey()` transformation is more suitable for aggregations by key
    that involve computations, such as finding the sum, average, variance, etc. The
    important consideration here is that the extra computation spent for map-side
    combining can reduce the amount of data sent out to other worker nodes and the
    driver.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey()` 转换更适合需要涉及计算的按键聚合，例如求和、平均值、方差等。这里的重要考虑是，为了减少发送到其他工作节点和驱动程序的数据量，地图端合并所花费的额外计算可以起到作用。'
- en: In the next chapter we’ll move on to cover partitioning data.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论数据分区。
- en: ^([1](ch04.xhtml#idm45713562249536-marker)) For further details, see [“Monoidify!
    Monoids as a Design Principle for Efficient MapReduce Algorithms”](https://oreil.ly/X0Yg8)
    by Jimmy Lin.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45713562249536-marker)) 更多细节，请参阅由Jimmy Lin撰写的[“Monoidify!
    Monoids as a Design Principle for Efficient MapReduce Algorithms”](https://oreil.ly/X0Yg8)。
