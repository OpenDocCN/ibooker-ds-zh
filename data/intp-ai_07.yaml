- en: 5 Saliency mapping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 显著性映射
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Characteristics that make convolutional neural networks inherently black-box
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使卷积神经网络本质上成为黑盒的特征
- en: How to implement convolutional neural networks for image classification tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现用于图像分类任务的卷积神经网络
- en: How to interpret convolutional neural networks using saliency mapping techniques,
    such as vanilla backpropagation, guided backpropagation, guided Grad-CAM, and
    SmoothGrad
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用显著性映射技术（如标准反向传播、引导反向传播、引导Grad-CAM和SmoothGrad）来解释卷积神经网络
- en: Strengths and weaknesses of these saliency mapping techniques and how to perform
    sanity checks on them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些显著性映射技术的优缺点以及如何对它们进行合理性检查
- en: 'In the previous chapter, we looked at deep neural networks and learned how
    to interpret them using model-agnostic methods that are local in scope. We specifically
    learned three techniques: LIME, SHAP, and anchors. In this chapter, we will focus
    on convolutional neural networks (CNNs), a more complex neural network architecture
    used mostly for visual tasks such as image classification, image segmentation,
    object detection, and facial recognition. We will learn how to apply techniques
    learned in the previous chapter to CNNs. In addition, we will also focus on saliency
    mapping, which is a local, model-dependent, and post hoc interpretability technique.
    Saliency mapping is a great tool for interpreting CNNs because it helps us visualize
    the salient or important features for the model. We will specifically cover techniques
    such as vanilla backpropagation, guided backpropagation, integrated gradients,
    SmoothGrad, Grad-CAM, and guided Grad-CAM.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们探讨了深度神经网络，并学习了如何使用局部范围内的模型无关方法来解释它们。我们具体学习了三种技术：LIME、SHAP和锚点。在本章中，我们将专注于卷积神经网络（CNNs），这是一种更复杂的神经网络架构，主要用于视觉任务，如图像分类、图像分割、目标检测和面部识别。我们将学习如何将前一章中学到的技术应用到CNNs上。此外，我们还将关注显著性映射，这是一种局部、模型相关和事后解释的技术。显著性映射是解释CNNs的一个很好的工具，因为它帮助我们可视化模型的重要或显著特征。我们将具体介绍标准反向传播、引导反向传播、集成梯度、SmoothGrad、Grad-CAM和引导Grad-CAM等技术。
- en: This chapter follows a similar structure to the previous chapters’. We will
    start off with a concrete example where we will extend the breast cancer diagnosis
    example from chapter 4\. We will explore this new dataset containing images and
    learn how to train and evaluate CNNs in PyTorch and how to interpret them. It
    is worth reiterating that although the main focus of this chapter is interpreting
    CNNs using saliency mapping, we will also cover model training and testing. We
    will also glean some key insights in the earlier sections that will be useful
    during model interpretation. Readers who are already familiar with training and
    testing CNNs are free to skip the earlier sections and jump straight to section
    5.4, which covers model interpretability.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构与前面章节类似。我们将从一个具体的例子开始，这个例子将扩展第4章中的乳腺癌诊断示例。我们将探索这个包含图像的新数据集，并学习如何在PyTorch中训练和评估卷积神经网络（CNNs），以及如何解释它们。值得重申的是，尽管本章的主要重点是使用显著性映射来解释CNNs，但我们也会涵盖模型训练和测试。我们还会在前面的部分中提炼出一些关键见解，这些见解在模型解释过程中将非常有用。对于已经熟悉CNNs训练和测试的读者，可以自由跳过前面的部分，直接跳到第5.4节，该节涵盖了模型可解释性。
- en: '5.1 Diagnostics+ AI: Invasive ductal carcinoma detection'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 诊断+ AI：浸润性导管癌检测
- en: Invasive ductal carcinoma (IDC) is the most common form of breast cancer. In
    this chapter, we will extend the breast cancer diagnosis example from the previous
    chapter to detecting IDC. Pathologists at Diagnostics+ currently perform biopsies
    on patients where they remove small tissue samples and analyze them under the
    microscope to determine whether the patient has IDC. The pathologist splits the
    whole mount sample of the tissue into patches and determines whether each patch
    is IDC positive or negative. By delineating the exact regions of IDC in the tissue,
    the pathologist determines how aggressive or advanced the cancer is and which
    grade to assign to the patient.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 浸润性导管癌（IDC）是乳腺癌最常见的类型。在本章中，我们将扩展前一章中的乳腺癌诊断示例，以检测IDC。Diagnostics+的病理学家目前对病人进行活检，他们移除小块组织样本并在显微镜下分析，以确定病人是否有IDC。病理学家将整个组织样本分割成小块，并确定每个小块是否为IDC阳性或阴性。通过在组织中界定IDC的确切区域，病理学家确定癌症的侵略性或进展程度，以及为病人分配哪个等级。
- en: '![](../Images/CH05_F01_Thampi.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F01_Thampi.png)'
- en: Figure 5.1 Diagnostics+ AI for invasive ductal carcinoma (IDC) detection
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 Diagnostics+ AI用于检测浸润性导管癌（IDC）
- en: Diagnostics+ would like to expand the capabilities of the their AI system that
    we built in chapter 4 to automatically assess images of tissue samples. The goal
    is for the AI system to determine whether each patch in the tissue mount sample
    is IDC positive or negative and to assign a confidence measure to it. This is
    shown in figure 5.1\. By using this AI system, Diagnostics+ can automate the preprocessing
    step of delineating the regions of IDC in the tissue so that the pathologist can
    easily assign a grade to it to determine how aggressive the cancer is. Given this
    information, how would you formulate this as a machine learning problem? Because
    the target of the model is to predict whether a given image or patch is IDC positive
    or negative, we can formulate this problem as a *binary classification* problem.
    The formulation is similar to chapter 4, but the inputs to the classifier are
    images, not structured tabular data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Diagnostics+ 希望扩展我们在第4章中构建的AI系统的功能，以自动评估组织样本图像。目标是让AI系统确定组织样本中的每个切片是IDC阳性还是阴性，并为其分配一个置信度度量。这如图5.1所示。通过使用这个AI系统，Diagnostics+
    可以自动化预处理步骤，即描绘组织中的IDC区域，以便病理学家可以轻松地为它分配一个等级，以确定癌症的侵袭性。考虑到这些信息，你将如何将这个问题表述为一个机器学习问题？因为模型的目的是预测给定的图像或切片是IDC阳性还是阴性，我们可以将这个问题表述为一个
    *二元分类* 问题。这个表述与第4章类似，但分类器的输入是图像，而不是结构化的表格数据。
- en: 5.2 Exploratory data analysis
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 探索性数据分析
- en: Let’s now try to understand this new image dataset better. A lot of the insights
    gleaned in this section will help us with model training, evaluation, and interpretation.
    In this dataset, we have tissue samples from 279 patients and 277,524 images of
    tissue patches. The raw dataset is obtained from Kaggle ([http://mng.bz/0wBl](http://mng.bz/0wBl))
    and is preprocessed to extract the metadata associated with these images. The
    preprocessing notebook and the preprocessed dataset can be found in the GitHub
    repository ([http://mng.bz/KBdZ](http://mng.bz/KBdZ)) associated with this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来更好地理解这个新的图像数据集。本节中获得的许多见解将有助于我们进行模型训练、评估和解释。在这个数据集中，我们有来自279名患者的组织样本和277,524个组织切片图像。原始数据集从Kaggle([http://mng.bz/0wBl](http://mng.bz/0wBl))获得，并经过预处理以提取与这些图像相关的元数据。预处理笔记本和预处理后的数据集可以在与本书相关的GitHub仓库([http://mng.bz/KBdZ](http://mng.bz/KBdZ))中找到。
- en: 'In figure 5.2, we can see the distribution of IDC-positive and -negative patches.
    Out of the 277,524 patches, roughly 70% are IDC negative and 30% are IDC positive.
    The dataset is, therefore, highly imbalanced. To recapitulate, we need to note
    the following two things when dealing with imbalanced datasets:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.2中，我们可以看到IDC阳性与阴性切片的分布。在277,524个切片中，大约70%是IDC阴性，30%是IDC阳性。因此，数据集高度不平衡。为了回顾，在处理不平衡数据集时，我们需要注意以下两点：
- en: Use the right performance metrics (like precision, recall, and F1) when testing
    and evaluating the models.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试和评估模型时，使用正确的性能指标（如精确度、召回率和F1）。
- en: Resample the training data such that the majority class is either undersampled
    or the minority class is oversampled.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新采样训练数据，使得多数类要么被欠采样，要么少数类被过采样。
- en: '![](../Images/CH05_F02_Thampi.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F02_Thampi.png)'
- en: Figure 5.2 Distribution of IDC-positive and -negative patches
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 IDC阳性与阴性切片的分布
- en: Let’s look at a few random samples of patches. By visualizing these images,
    we can see if there are some distinct characteristics for IDC-positive and -negative
    patches. This will help us later when we have to interpret the model. Figure 5.3
    shows a random sample of four IDC-positive patches, and figure 5.4 shows a random
    sample of four IDC-negative patches. The dimension of each patch image is 50 ×
    50 pixels. We can observe that the IDC-positive patches have more dark-stained
    cells. The density of the dark stains is also higher. The darker color is typically
    used to stain nuclei. For IDC-negative samples, on the other hand, the density
    of lighter stains is higher. The lighter color is typically used to highlight
    the cytoplasm and extracellular connective tissue. We can, therefore, visually
    say that a given patch is more likely to be IDC positive if it has a high density
    of dark stains or cell nuclei. On other hand, a given patch is more likely to
    be IDC negative if it has a high density of lighter stains and a very low density
    of cell nuclei.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个随机图像块的样本。通过可视化这些图像，我们可以看到IDC阳性图像块和阴性图像块是否有某些明显的特征。这将在我们后来解释模型时有所帮助。图5.3显示了四个IDC阳性图像块的随机样本，图5.4显示了四个IDC阴性图像块的随机样本。每个图像块的尺寸是50
    × 50像素。我们可以观察到IDC阳性图像块有更多的深染细胞。深染的密度也更高。通常使用较深的颜色来染色细胞核。对于IDC阴性样本，另一方面，浅染的密度更高。浅色通常用于突出细胞质和细胞外结缔组织。因此，我们可以直观地说，如果一个图像块有高密度的深染或细胞核，那么它更有可能是IDC阳性。另一方面，如果一个图像块有高密度的浅染和非常低的细胞核密度，那么它更有可能是IDC阴性。
- en: '![](../Images/CH05_F03_Thampi.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Thampi.png)'
- en: Figure 5.3 A visualization of random IDC-positive patches
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 随机IDC阳性图像块的可视化
- en: '![](../Images/CH05_F04_Thampi.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F04_Thampi.png)'
- en: Figure 5.4 A visualization of random IDC-negative patches
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 随机IDC阴性图像块的可视化
- en: Now let’s visualize all the patches for one patient or tissue sample and the
    regions that are IDC positive. Figure 5.5 visualizes this for one patient. The
    plot on the left shows all the patches stitched together for the tissue sample.
    The plot on the right shows the same image but highlights the IDC-positive patches
    in a darker shade. This confirms our observation earlier that patches are much
    more likely to be IDC positive if they have a very high density of darker stains.
    We will come back to this visualization when we have to interpret the CNN that
    we will be training for IDC detection.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化一个患者或组织样本的所有图像块以及IDC阳性区域。图5.5为一位患者展示了这一点。左边的图显示了组织样本的所有图像块拼接在一起。右边的图显示了相同的图像，但以较深的色调突出显示IDC阳性图像块。这证实了我们之前的观察，即如果一个图像块有非常高的深染密度，那么它更有可能是IDC阳性。当我们需要解释我们将为IDC检测训练的CNN时，我们将回到这个可视化。
- en: '![](../Images/CH05_F05_Thampi.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F05_Thampi.png)'
- en: Figure 5.5 A visualization of tissue sample and IDC-positive patches
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 组织样本和IDC阳性图像块的可视化
- en: In the next section, we will prepare the data and train a CNN. The CNN will
    be used to classify each image or patch as either IDC positive or negative. Because
    the dataset is quite imbalanced, we need to evaluate the CNN using metrics such
    as precision, recall, and F1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将准备数据并训练一个卷积神经网络（CNN）。这个CNN将被用来将每张图像或图像块分类为IDC阳性或阴性。由于数据集相当不平衡，我们需要使用诸如精确度、召回率和F1等指标来评估CNN。
- en: 5.3 Convolutional neural networks
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 卷积神经网络
- en: A convolutional neural network (CNN) is a neural network architecture commonly
    used for visual tasks such as image classification, object detection, and image
    segmentation. Why are CNNs used for visual tasks and not fully connected deep
    neural networks (DNNs)? Fully connected DNNs do not capture pixel dependencies
    in an image well because images need to be flattened into a 1-D structure before
    being fed into the neural network. CNNs, on the other hand, take advantage of
    the multidimensional structure of images and capture pixel dependencies or spatial
    dependencies in an image well. CNNs are also translation invariant, meaning they
    are great at detecting shapes in an image, irrespective of where the shapes occur
    in the image. In addition, the CNN architecture can also be trained more efficiently
    to fit the input dataset because weights in the network are reused. Figure 5.6
    shows an illustration of a CNN architecture used for binary image classification.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是一种常用于视觉任务（如图像分类、目标检测和图像分割）的神经网络架构。为什么CNN用于视觉任务而不是全连接深度神经网络（DNN）？全连接DNN无法很好地捕捉图像中的像素依赖性，因为图像在输入神经网络之前需要被展平成1维结构。另一方面，CNN利用图像的多维结构，很好地捕捉图像中的像素依赖性或空间依赖性。CNN还具有平移不变性，这意味着它们擅长检测图像中的形状，无论这些形状出现在图像的哪个位置。此外，CNN架构还可以更有效地训练以适应输入数据集，因为网络中的权重被重复使用。图5.6展示了用于二值图像分类的CNN架构示意图。
- en: '![](../Images/CH05_F06_Thampi.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F06_Thampi.png)'
- en: Figure 5.6 An illustration of CNN for image classification
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 CNN用于图像分类的示意图
- en: The architecture in figure 5.6 consists of a sequence of layers called *convolution
    and pooling layers*. The combination of these two types of layers is called the
    *feature learning layers*. The objective of the feature learning layers is to
    extract hierarchical features from the input image. The first few layers will
    be extracting low-level features such as edges, colors, and gradients. By adding
    more convolution and pooling layers, the architecture learns high-level features,
    giving us a much better understanding of the characteristics of images in the
    dataset. We will cover convolutional and pooling layers in more depth later in
    this section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6中的架构由一系列称为“卷积和池化层”的层组成。这两种类型层的组合称为“特征学习层”。特征学习层的目的是从输入图像中提取层次化特征。前几层将提取低级特征，如边缘、颜色和梯度。通过添加更多的卷积和池化层，架构学习高级特征，使我们能够更好地理解数据集中图像的特征。我们将在本节稍后更深入地介绍卷积和池化层。
- en: Following the feature learning layers are layers of neurons or units that are
    fully connected, just like the DNN architecture we saw in chapter 4\. The purpose
    of these fully connected layers is to perform classification. The inputs to the
    fully connected layer are the high-level features learned by the convolution and
    pooling layers, and the output is a probability measure for the classification
    task. Because we covered how DNNs work in chapter 4, we will focus most of our
    attention now on the convolution and pooling layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征学习层之后是神经元或单元的层，这些层是完全连接的，就像我们在第4章中看到的DNN架构一样。这些全连接层的目的是执行分类。全连接层的输入是卷积和池化层学习的高级特征，输出是对分类任务的概率度量。由于我们在第4章中介绍了DNN的工作原理，我们现在将主要关注卷积和池化层。
- en: 'In chapter 1, we saw how to represent an image so that a CNN can easily process
    it, as summarized in figure 5.7\. In this example, the image of a tissue patch
    is a colored image of size 50 × 50 pixels consisting of three primary channels:
    red (R), green (G) and blue (B). This RGB image can be represented in mathematical
    form as three matrices of pixel values, one for each channel and each of size
    50 × 50.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们看到了如何表示图像以便CNN可以轻松处理，如图5.7所示。在这个例子中，组织片的图像是一个50 × 50像素大小的彩色图像，由三个主要通道组成：红色（R）、绿色（G）和蓝色（B）。这个RGB图像可以用数学形式表示为三个像素值矩阵，每个通道一个，大小为50
    × 50。
- en: '![](../Images/CH05_F07_Thampi.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F07_Thampi.png)'
- en: Figure 5.7 An illustration of how to represent a 50 × 50 image of a tissue patch
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 如何表示50 × 50像素的组织片图像
- en: Let’s now see how the convolution layer processes an image represented as a
    matrix of pixel values. This layer consists of a kernel or filter and is convolved
    with the input image to obtain a representation of the image called the *feature
    image*. Let’s break this down and look at it step by step. Figure 5.8 shows a
    simplified illustration of the operation performed in the convolution layer. In
    the figure, the image is represented as a matrix of dimension 3 × 3, and the kernel
    or filter is represented as a matrix of dimension 2 × 2\. The kernel starts off
    at the top-left corner of the image and moves from left to right until it processes
    the complete width of the image. The kernel then moves down and starts again from
    the left of the image, repeating this movement until the whole image is processed.
    Each movement of the kernel is called a *stride*. An important hyperparameter
    for the kernel is the stride length. If the stride length is 1, the kernel moves
    one step during each stride. Figure 5.8 illustrates a kernel with a stride length
    of 1\. As you can see, the kernel starts at the top-left corner of the image and
    needs to perform three strides to process the whole image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下卷积层是如何处理以像素值矩阵表示的图像的。这一层由一个核或滤波器组成，并与输入图像进行卷积，以获得称为*特征图像*的图像表示。让我们一步一步地分析它。图5.8展示了卷积层中执行操作的简化示意图。在图中，图像被表示为一个3×3维度的矩阵，而核或滤波器被表示为一个2×2维度的矩阵。核从图像的左上角开始，向右移动直到处理完整个图像的宽度。然后核向下移动，并从图像的左侧重新开始，重复这一过程直到处理完整个图像。每次核的移动被称为*步长*。对于核来说，步长长度是一个重要的超参数。如果步长长度为1，核在每次步长中移动一步。图5.8展示了步长长度为1的核。如图所示，核从图像的左上角开始，需要执行三个步长来处理整个图像。
- en: '![](../Images/CH05_F08_Thampi.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F08_Thampi.png)'
- en: Figure 5.8 An illustration of how a convolution layer creates a feature map
    from the input image
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 展示了卷积层如何从输入图像创建特征图的示例
- en: During each stride, the part of the image that is processed is convolved with
    the kernel. As we saw in chapter 2 in the context of GAMs, the convolution operation
    is essentially a dot product. An element-wise product is taken of the part of
    the image that is processed with the kernel, followed by a sum. In figure 5.8,
    we can see this illustrated for all the strides. For stride 0, for instance, the
    part of the image that is processed by the kernel is highlighted by the dashed
    box. The value obtained by taking the dot product of this image with the kernel
    is 3, and this value is placed on the top-left corner of the feature map matrix.
    In stride 1, we move one step to the right and perform the convolution operation
    again. The value that is obtained after convolution is 7, and this is placed on
    the top-right corner of the feature map matrix. This process is repeated until
    the whole image is processed. At the end of the convolution operation, we obtain
    a feature map matrix of size 2 × 2 that is meant to capture a high-level feature
    representation of the input image. The numbers within the kernel or filter are
    called *weights*. Note that in figure 5.8, the same weights are used for the convolution
    layer. This weight sharing allows the CNN to be trained a lot more efficiently
    than DNNs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次步长中，被处理的图像部分与核进行卷积。正如我们在第2章中讨论GAMs时所见，卷积操作本质上是一个点积。对与核处理的图像部分进行逐元素乘积，然后求和。在图5.8中，我们可以看到所有步长的这种表示。例如，对于步长0，通过核处理的图像部分由虚线框突出显示。这个图像与核进行点积得到的结果是3，这个值被放置在特征图矩阵的左上角。在步长1中，我们向右移动一步并再次执行卷积操作。卷积后得到的结果是7，这个值被放置在特征图矩阵的右上角。这个过程一直重复，直到处理完整个图像。在卷积操作结束时，我们得到一个2×2大小的特征图矩阵，它旨在捕获输入图像的高级特征表示。核或滤波器内的数字被称为*权重*。请注意，在图5.8中，卷积层使用了相同的权重。这种权重共享使得CNN的训练比DNNs更加高效。
- en: The objective of the learning algorithm is to determine the weights within the
    kernel or filter in the convolution layer. This is done during backpropagation.
    The size of the feature map matrix is determined by a few hyperparameters—the
    size of the input image, the size of the kernel, the stride length, and another
    hyperparameter called the padding. *Padding* refers to the number of pixels added
    to the image before performing the convolution operation. In figure 5.8, a padding
    of 0 is used where no additional pixels are added to the image. If the padding
    is set to 1, a border of pixels is added around the image where all the pixel
    values in the border are set to 0, as illustrated in figure 5.9\. Adding padding
    increases the size of the feature map and allows for a more accurate representation
    of the image. In practice, the convolution layer consists of multiple filters
    or kernels. The number of filters is another hyperparameter that we must specify
    before training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法的目标是确定卷积层内核或过滤器中的权重。这是在反向传播过程中完成的。特征图矩阵的大小由几个超参数决定——输入图像的大小、内核的大小、步长长度以及另一个称为填充的超参数。*填充*是指在执行卷积操作之前添加到图像中的像素数。在图5.8中，使用了0填充，即不向图像添加额外的像素。如果将填充设置为1，则在图像周围添加一个像素边界，其中边界中的所有像素值都设置为0，如图5.9所示。添加填充会增加特征图的大小，并允许更准确地表示图像。在实践中，卷积层由多个过滤器或内核组成。过滤器的数量是我们必须在训练之前指定的另一个超参数。
- en: '![](../Images/CH05_F09_Thampi.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F09_Thampi.png)'
- en: Figure 5.9 An illustration of padding
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 填充的示意图
- en: The convolution layer in a CNN is usually followed by a pooling layer. The purpose
    of the pooling layer is to reduce the dimensionality of the feature map further
    to reduce the computational power required during model training. A common pooling
    layer is max pooling. Like in the convolution layer, the pooling layer also consists
    of a filter. A max pooling filter returns the maximum of all the values covered
    by that filter, as illustrated in figure 5.10.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的卷积层通常后面跟着一个池化层。池化层的作用是进一步降低特征图的维度，以减少模型训练过程中所需的计算能力。一个常见的池化层是最大池化。就像在卷积层中一样，池化层也由一个过滤器组成。最大池化过滤器返回该过滤器覆盖的所有值的最大值，如图5.10所示。
- en: '![](../Images/CH05_F10_Thampi.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F10_Thampi.png)'
- en: Figure 5.10 An illustration of max pooling
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 最大池化的示意图
- en: Rapid advances in CNNs have occurred in the last decade in various tasks, such
    as image recognition, object detection, and image segmentation. This is thanks
    to massive amounts of annotated data (ImageNet [[http://www.image-net.org/](http://www.image-net.org/)]
    and CIFAR-10 and CIFAR-100 [[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)]
    being some of them) and advances in computation where deep learning models are
    leveraging the strengths of graphics processing units (GPUs). Figure 5.11 shows
    advances in CNN research over the last decade, especially in the image classification
    task using the ImageNet dataset. The ImageNet dataset is a large database of annotated
    images typically used for image classification and object detection tasks. It
    consists of more than a million images organized in a hierarchical structure consisting
    of more than 20,000 labeled categories. Figure 5.11 was obtained from Papers with
    Code ([http://mng.bz/9K8o](https://shortener.manning.com/9K8o)), a useful repository
    of state-of-the-art (SoTA) machine learning techniques. One of the major breakthroughs
    in terms of performance occurred in 2013 using the AlexNet architecture. The current
    best CNNs are based on an architecture called residual network (ResNet). Some
    of these SoTA architectures have been implemented in deep learning frameworks
    like PyTorch and Keras. We will see how to use them in the next section where
    we will train a CNN for the IDC detection task. We will specifically focus on
    the ResNet architecture because it is one of the most widely used architectures.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，CNN在各种任务中取得了快速进展，例如图像识别、目标检测和图像分割。这得益于大量标注数据（ImageNet [[http://www.image-net.org/](http://www.image-net.org/)]、CIFAR-10和CIFAR-100
    [[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)]
    等为其一）和计算能力的提升，其中深度学习模型正在利用图形处理单元（GPU）的优势。图5.11展示了过去十年CNN研究进展，特别是在使用ImageNet数据集进行图像分类任务中。ImageNet数据集是一个大型标注图像数据库，通常用于图像分类和目标检测任务。它包含超过一百万张图像，组织在一个包含超过20,000个标注类别的分层结构中。图5.11是从Papers
    with Code ([http://mng.bz/9K8o](http://mng.bz/9K8o)) 获得的，这是一个包含顶级（SoTA）机器学习技术的有用仓库。在性能方面的一个主要突破发生在2013年，使用了AlexNet架构。当前的顶级CNN基于名为残差网络（ResNet）的架构。其中一些SoTA架构已经在PyTorch和Keras等深度学习框架中实现。我们将在下一节中看到如何使用它们，在那里我们将训练一个用于IDC检测任务的CNN。我们将特别关注ResNet架构，因为它是最广泛使用的架构之一。
- en: '![](../Images/CH05_F11_Thampi.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F11_Thampi.png)'
- en: 'Figure 5.11 State-of-the-art CNN architectures for image classification on
    the ImageNet dataset (Source: [http://mng.bz/9K8o](https://shortener.manning.com/9K8o))'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 在ImageNet数据集上进行图像分类的顶级CNN架构（来源：[http://mng.bz/9K8o](http://mng.bz/9K8o)）
- en: 5.3.1 Data preparation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 数据准备
- en: 'In this section, we will prepare the data for model training. Data preparation
    is slightly different than in the previous chapters because we are dealing with
    images and not structured tabular data. Please note that the preprocessed dataset
    is used here. The code used for preprocessing and the preprocessed dataset can
    be found in the GitHub repository ([http://mng.bz/KBdZ](http://mng.bz/KBdZ)) associated
    with this book. First, let’s prepare the training, validation, and test sets.
    It is important that we do not split the data by patches but rather using the
    patient ID. This prevents data leakage across the training, validation, and test
    sets. If we randomly split the dataset by patches, patches for one patient may
    be in all three sets and, therefore, leak some information about the patient.
    The following code snippet shows how to split the dataset by patient ID:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为模型训练准备数据。数据准备与之前章节略有不同，因为我们处理的是图像而不是结构化的表格数据。请注意，这里使用的是预处理后的数据集。用于预处理的代码和预处理后的数据集可以在与本书相关的GitHub仓库（[http://mng.bz/KBdZ](http://mng.bz/KBdZ)）中找到。首先，让我们准备训练集、验证集和测试集。重要的是我们不要通过补丁来分割数据，而是使用患者ID。这可以防止训练集、验证集和测试集之间的数据泄露。如果我们随机通过补丁分割数据集，一个患者的补丁可能会出现在所有三个集中，因此可能会泄露一些关于患者的信息。以下代码片段展示了如何通过患者ID来分割数据集：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads the data into a Pandas DataFrame
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将数据加载到Pandas DataFrame中
- en: ② Extracts all unique patient IDs from the data
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从数据中提取所有唯一的患者ID
- en: ③ Splits the data into train and validation/test sets
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将数据分割为训练集和验证/测试集
- en: ④ Splits the validation/test set into separate validation and test sets
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将验证/测试集分割为单独的验证集和测试集
- en: ⑤ Extracts all patches for patient IDs in the train set
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从训练集中的患者ID提取所有补丁
- en: ⑥ Extracts all patches for patient IDs in the validation set
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 提取验证集中患者 ID 的所有补丁
- en: ⑦ Extracts all patches for patient IDs in the test set
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 提取测试集中患者 ID 的所有补丁
- en: Note that 60% of the patients are in the training set, 20% in the validation
    set, and the remaining 20% in the test set. Let’s now check to see whether the
    distribution of the target variable is similar across the three sets, as shown
    in figure 5.12\. We can see that roughly 25–30% of the patches are IDC positive
    and 70–75% are IDC negative in all three sets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，60% 的患者位于训练集中，20% 位于验证集中，剩余的 20% 位于测试集中。现在让我们检查目标变量的分布是否在三个集中相似，如图 5.12 所示。我们可以看到，在所有三个集中，大约
    25-30% 的补丁是 IDC 阳性，而 70-75% 是 IDC 阴性。
- en: '![](../Images/CH05_F12_Thampi.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F12_Thampi.png)'
- en: Figure 5.12 Target variable distribution across the training, validation, and
    test sets
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 训练集、验证集和测试集中的目标变量分布
- en: 'Let’s now create a custom class to easily load the images of patches and their
    corresponding labels. PyTorch provides a class called `Dataset` for this purpose.
    We will extend this class for the IDC dataset in this chapter. For more details
    on the `Dataset` class and also on PyTorch, please see appendix A. See the following
    code sample:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个自定义类，以便轻松加载补丁的图像及其相应的标签。PyTorch提供了一个名为 `Dataset` 的类用于此目的。在本章中，我们将扩展此类以用于
    IDC 数据集。有关 `Dataset` 类以及 PyTorch 的更多详细信息，请参阅附录 A。以下代码示例展示了如何进行：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Loads the dataset class provided by PyTorch
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载 PyTorch 提供的数据集类
- en: ② Creates a new dataset class for the image patches that extends the PyTorch
    class
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为图像补丁创建一个新的数据集类，该类扩展了 PyTorch 类
- en: ③ A constructor that initializes the list of patches, and a directory that contains
    the images and any image transformer
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个构造函数，用于初始化补丁列表，以及包含图像和任何图像转换器的目录
- en: ④ Overrides the __len__ method to return the number of image patches in the
    dataset
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 覆盖了 `__len__` 方法以返回数据集中图像补丁的数量
- en: ⑤ Overrides the __getitem__ method to return the image and label from the dataset
    at the position index
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 覆盖了 `__getitem__` 方法以从数据集的位置索引返回图像和标签
- en: ⑥ Extracts the image ID and label from the dataset
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从数据集中提取图像 ID 和标签
- en: ⑦ Opens the image and converts it to RGB
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打开图像并将其转换为 RGB
- en: ⑧ Applies the transformation on the image if defined
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 如果定义了转换，则应用于图像
- en: ⑨ Returns the image and label
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 返回图像和标签
- en: 'Let’s now define a function to transform the image of the patch. Common image
    transformations such as crops, flips, rotations, and resizing are implemented
    in the `torchvision` package. The full list of transformations can be found at
    [http://mng.bz/jy6p](http://mng.bz/jy6p). The following code snippet shows five
    transformations being performed on an image in the training set. As a data augmentation
    step, the second and third transforms flip the image randomly about the horizontal
    and vertical axes. As an exercise, create transforms for both the validation and
    test sets. Note that on the validation and test sets, you do not need to augment
    the data by flipping the image horizontally or vertically. You can name the transforms
    `trans_val` and `trans_test`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一个函数来转换补丁的图像。常见的图像转换，如裁剪、翻转、旋转和调整大小，在 `torchvision` 包中得到实现。完整的转换列表可以在
    [http://mng.bz/jy6p](http://mng.bz/jy6p) 找到。以下代码片段展示了在训练集中的图像上执行了五个转换。作为一个数据增强步骤，第二个和第三个转换随机地围绕水平和垂直轴翻转图像。作为一个练习，为验证集和测试集创建转换。请注意，在验证集和测试集上，您不需要通过水平或垂直翻转图像来增强数据。您可以命名转换
    `trans_val` 和 `trans_test`：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Imports the transforms module provided by PyTorch
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 PyTorch 提供的转换模块
- en: ② Uses the Compose class to compose several transforms together
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 `Compose` 类将多个转换组合在一起
- en: ③ The first transform resizes the image to 50 × 50 pixels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 第一个转换将图像调整大小到 50 × 50 像素。
- en: ④ The second transform flips the image about the horizontal axis.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 第二个转换围绕水平轴翻转图像。
- en: ⑤ The third transform flips the image about the vertical axis.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 第三个转换围绕垂直轴翻转图像。
- en: ⑥ The fourth transform converts the image to a NumPy array.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 第四个转换将图像转换为 NumPy 数组。
- en: ⑦ The fifth transform normalizes the image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 第五个转换对图像进行归一化。
- en: 'With the dataset class and transforms in place, we can now initialize the datasets
    and loaders. The following code snippet shows you how to initialize it for the
    training set. The `DataLoader` class provided by PyTorch allows you to batch data,
    shuffle it, and load it in parallel using multiprocessing workers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集类和转换就绪后，我们现在可以初始化数据集和加载器。以下代码片段展示了如何为训练集初始化它。PyTorch 提供的 `DataLoader` 类允许您批量数据、打乱数据，并使用多进程工作器并行加载：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Uses the DataLoader class in PyTorch to iterate through the data in batches
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 PyTorch 中的 DataLoader 类以批量迭代数据
- en: ② Creates the dataset for the patches in the training set
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建训练集中切片的数据集
- en: ③ Loads images and labels of the patches in batches of 64
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 以 64 批次的形式加载切片的图像和标签
- en: ④ Creates the data loader for the training set
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建训练集的数据加载器
- en: As an exercise, I encourage you to create similar datasets and loaders for the
    validation and test sets and name the objects `dataset_val` and `dataset_test`,
    respectively. The solutions to these exercises can be found in the GitHub repository
    associated with this book ([http://mng.bz/KBdZ](http://mng.bz/KBdZ)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，我鼓励您为验证集和测试集创建类似的数据集和加载器，分别命名为 `dataset_val` 和 `dataset_test`。这些练习的解决方案可以在与本书相关的
    GitHub 仓库中找到 ([http://mng.bz/KBdZ](http://mng.bz/KBdZ))。
- en: 5.3.2 Training and evaluating CNNs
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 训练和评估 CNN
- en: With the datasets and loaders in place, we are now ready to create the CNN model.
    We will use the ResNet architecture that is implemented in the `torchvision` package
    in PyTorch. Using `torchvision` ([http://mng.bz/jy6p](http://mng.bz/jy6p)), you
    can initialize other state-of-the-art architectures as well, such as AlexNet,
    VGG, Inception, and ResNeXt. You can also load these model architectures with
    pretrained weights by setting the pretrained flag to true. If set to true, the
    package returns a model pretrained on the ImageNet dataset. For the IDC detection
    example in this chapter, we will not use the pretrained model because it initializes
    the model weights randomly, and the model would be trained from scratch using
    the new dataset containing images of tissue patches. As an exercise, I encourage
    you to set the `pretrained` parameter to `True` to initialize the weights obtained
    by training on the ImageNet dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集和加载器就绪后，我们现在可以创建 CNN 模型。我们将使用 PyTorch 中的 `torchvision` 包实现的 ResNet 架构。使用
    `torchvision` ([http://mng.bz/jy6p](http://mng.bz/jy6p))，您还可以初始化其他最先进的架构，例如 AlexNet、VGG、Inception
    和 ResNeXt。您还可以通过将预训练标志设置为 true 来加载带有预训练权重的这些模型架构。如果设置为 true，该包将返回在 ImageNet 数据集上预训练的模型。对于本章中的
    IDC 检测示例，我们将不使用预训练模型，因为它会随机初始化模型权重，并且模型将使用包含组织切片图像的新数据集从头开始训练。作为一个练习，我鼓励您将 `pretrained`
    参数设置为 `True` 以初始化通过在 ImageNet 数据集上训练获得的权重。
- en: 'We also need to concatenate fully connected layers to the CNN to perform the
    binary classification task. We can use the next code snippet to initialize the
    CNN:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将全连接层连接到 CNN 以执行二进制分类任务。我们可以使用以下代码片段来初始化 CNN：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Sets the number of classes in the dataset, which is binary in this case
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置数据集中的类别数量，在本例中为二进制
- en: ② Uses the GPU device if CUDA is available; otherwise, sets the device as the
    CPU
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果可用 CUDA，则使用 GPU 设备；否则，将设备设置为 CPU
- en: ③ Initializes the ResNet model and extracts the number of features from the
    model
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化 ResNet 模型并从模型中提取特征数量
- en: ④ Concatenates the fully connected layers to the ResNet model for classification
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将全连接层连接到 ResNet 模型以进行分类
- en: ⑤ Transfers the model to the device
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将模型转移到设备上
- en: Note that the model is loaded on the CPU by default. For faster processing,
    you can load the model on a GPU. All the popular deep learning frameworks, including
    PyTorch, use CUDA, which stands for compute unified device architecture, to perform
    general-purpose computing on GPUs. CUDA is a platform built by NVIDIA that provides
    APIs to directly access the GPU.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认情况下模型是在 CPU 上加载的。为了加快处理速度，您可以将模型加载到 GPU 上。所有流行的深度学习框架，包括 PyTorch，都使用 CUDA（代表计算统一设备架构）在
    GPU 上执行通用计算。CUDA 是由 NVIDIA 构建的平台，它提供了直接访问 GPU 的 API。
- en: 'We can train the model using the following code snippet. Note that in this
    example, the model is trained for five epochs. The training time of this complex
    model using the IDC dataset is about 17 hours on a CPU. The training time would
    be a lot shorter if it were done on a GPU instance. You could also achieve better
    performance by increasing the number of epochs and training the model for longer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段来训练模型。注意，在这个例子中，模型训练了五个周期。使用 IDC 数据集训练这个复杂模型在 CPU 上的训练时间大约是 17 小时。如果在一个
    GPU 实例上执行，训练时间会更短。你还可以通过增加周期数或延长训练时间来提高模型性能：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s now see how well this model performs on the test set. As we did in the
    previous chapters, we compare the model performance with a reasonable baseline.
    We saw in section 5.2 that the target class is highly imbalanced (see figure 5.2),
    where IDC-negative is the majority class. One option for the baseline model is
    to predict the majority class always, that is, always predict that a tissue patch
    is IDC negative. This baseline is not reasonable, however, because the cost of
    a false negative is a lot larger than a false positive when it comes to healthcare,
    especially a cancer diagnosis. A much more reasonable strategy would be to err
    on the side of false positives—always predict that a given tissue patch is IDC
    positive. Although this strategy is not ideal, it at least gets all the positive
    cases right. In a real-life situation, the baseline model is typically predictions
    made by a human or expert (in this case, the assessments made by an expert pathologist)
    or an existing model that the business is using. For this example, unfortunately,
    we do not have access to that information and so use a baseline model that always
    predicts IDC positive.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看这个模型在测试集上的表现如何。正如我们在前面的章节中所做的那样，我们将模型性能与一个合理的基线进行比较。我们在 5.2 节中看到，目标类别高度不平衡（见图
    5.2），其中 IDC 阴性是多数类别。基线模型的一个选择是始终预测多数类别，即始终预测组织片是 IDC 阴性。然而，这种基线并不合理，因为在医疗保健领域，特别是癌症诊断中，假阴性的成本远大于假阳性。一个更合理的策略是偏向于假阳性——始终预测给定的组织片是
    IDC 阳性。尽管这种策略并不理想，但它至少确保了所有阳性病例都被正确识别。在现实情况下，基线模型通常是人为或专家（在这种情况下，由专家病理学家进行的评估）或企业正在使用的现有模型的预测。不幸的是，对于这个例子，我们无法访问这些信息，因此使用始终预测
    IDC 阳性的基线模型。
- en: 'Table 5.1 shows the three key performance metrics used to benchmark the models:
    precision, recall, and F1\. Precision measures the proportion of predicted classes
    that are accurate. Recall measures the proportion of actual classes that the model
    predicted accurately. The F1 score is the harmonic mean of precision and recall.
    Please see chapter 3 for a more detailed explanation of these metrics.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 展示了用于基准测试模型的三个关键性能指标：精确率、召回率和 F1 分数。精确率衡量预测类别中准确的比率。召回率衡量模型准确预测的实际类别的比率。F1
    分数是精确率和召回率的调和平均值。请参阅第 3 章以获取这些指标的更详细解释。
- en: If we look at the recall metric in table 5.1, the baseline model does better
    than the CNN. This is expected because the baseline model is predicting IDC positive
    all the time. Overall, though, the CNN model does much better than the baseline,
    achieving a precision of 74.4% (+45.8% better than the baseline) and an F1 score
    of 74.2% (+29.7% better than the baseline). As an exercise, I encourage you to
    tune the model and achieve higher performance either by training for longer by
    increasing the number of epochs or by changing the CNN architecture.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看表 5.1 中的召回率指标，基线模型比 CNN 模型表现更好。这是预期的，因为基线模型始终预测 IDC 阳性。然而，总的来说，CNN 模型比基线模型表现要好得多，实现了
    74.4% 的精确率（比基线高 45.8%）和 74.2% 的 F1 分数（比基线高 29.7%）。作为一个练习，我鼓励你调整模型，通过增加周期数或改变 CNN
    架构来延长训练时间，以实现更高的性能：
- en: Table 5.1 Performance comparison of baseline model with the CNN model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 基线模型与 CNN 模型的性能比较
- en: '|  | Precision (%) | Recall (%) | F1 score (%) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确率 (%) | 召回率 (%) | F1 分数 (%) |'
- en: '| Baseline model | 28.6 | 100 | 44.5 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 基线模型 | 28.6 | 100 | 44.5 |'
- en: '| CNN model (ResNet) | 74.4 (+45.8) | 74.1 (–25.9) | 74.2 (+29.7) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CNN 模型（ResNet） | 74.4 (+45.8) | 74.1 (–25.9) | 74.2 (+29.7) |'
- en: With the CNN model performing better than the baseline, let’s now interpret
    it and understand how the black-box model arrived at the final prediction.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CNN 模型比基线模型表现更好，现在让我们来解释它，并了解这个黑盒模型是如何得出最终预测的。
- en: 5.4 Interpreting CNNs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 解释 CNN
- en: As we saw in the previous section, to make a prediction using a CNN, an image
    goes through multiple convolutional and pooling layers for feature learning, followed
    by multiple layers of a fully connected deep neural network for classification.
    For the ResNet model used for IDC detection, the total number of parameters learned
    during training was *11,572,546*. Millions of complex operations are being performed
    in the network, and it becomes incredibly difficult to understand how the model
    arrived at the final prediction. This is what makes CNNs black boxes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，使用卷积神经网络（CNN）进行预测时，图像会经过多个卷积和池化层进行特征学习，随后通过多个全连接的深度神经网络层进行分类。对于用于IDC检测的ResNet模型，训练过程中学习的总参数数量为*11,572,546*。网络中正在执行数百万个复杂操作，因此理解模型如何得出最终预测变得极其困难。这正是CNN成为黑盒的原因。
- en: 5.4.1 Probability landscape
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 概率景观
- en: In the previous chapter, we saw that one way of interpreting DNNs is by visualizing
    the strengths of the edge weights. Through this technique, we could see at a high
    level what influence input features had on the final model prediction. This technique
    cannot be applied to CNNs because it is not trivial to visualize the kernels (or
    filters) in the convolution layers and the influence they have on the intermediate
    features that are learned and the final model output. We could, however, visualize
    the probability landscape of the CNN. What is the probability landscape? Using
    the CNN in the context of a binary classifier, we are essentially getting a probability
    measure for the target class. In the case of IDC detection, we are getting from
    the CNN the probability that a given input patch is IDC positive. For all the
    patches in the tissue, we can then plot the output probability of the classifier
    and visualize it as a heat map. Overlaying this heat map over the image can give
    us an indication of hot spots where the CNN detects highly likely IDC-positive
    regions. This is the probability landscape.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们了解到一种解释深度神经网络（DNN）的方法是通过可视化边缘权重的强度。通过这种技术，我们可以从高层次上看到输入特征对最终模型预测的影响。然而，这种方法不能应用于CNN，因为可视化卷积层中的核（或过滤器）及其对学习到的中间特征和最终模型输出的影响并非易事。不过，我们可以可视化CNN的概率景观。什么是概率景观？在二元分类的上下文中使用CNN，我们实际上得到了目标类的概率度量。在IDC检测的情况下，我们从CNN中得到给定输入块是IDC阳性的概率。对于组织中的所有块，我们可以绘制分类器的输出概率并将其可视化为一个热图。将此热图叠加到图像上可以给我们指示热点区域，其中CNN检测到高度可能的IDC阳性区域。这就是概率景观。
- en: '![](../Images/CH05_F13_Thampi.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH05_F13_Thampi.png)'
- en: Figure 5.13 Probabilistic landscape of a ResNet model for a whole-tissue sample
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 整个组织样本的ResNet模型的概率景观
- en: Figure 5.13 shows three plots. The left-most plot is a visualization of all
    the patches for patient 12930\. The middle plot highlights the patches that are
    IDC positive based on the ground truth labels collected from the expert pathologists.
    These first two plots are similar to the one we saw in section 5.2 (see figure
    5.5). The right-most plot shows the probability landscape of the ResNet model
    trained to detect IDC. The brighter the color, the greater the probability that
    a given patch is IDC positive. By comparing with the ground truth, we can see
    a good overlap between the model predictions and the ground truth. There are,
    however, some false positives where the model highlights regions that are not
    necessarily IDC positive. The implementation of figure 5.13 can be found in the
    GitHub repository ([http://mng.bz/KBdZ](http://mng.bz/KBdZ)) associated with this
    book. You can load the model trained in section 5.3.2 to jump straight into model
    interpretability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13显示了三个图表。最左侧的图表是患者12930的所有块的可视化。中间的图表突出显示了基于从专家病理学家收集的地面真实标签的IDC阳性块。这两个图表与我们在5.2节中看到的图表类似（见图5.5）。最右侧的图表显示了用于检测IDC的ResNet模型的概率景观。颜色越亮，给定块是IDC阳性的概率就越大。通过与地面真实情况进行比较，我们可以看到模型预测和地面真实之间存在良好的重叠。然而，也有一些假阳性，模型突出显示的区域并不一定是IDC阳性。图5.13的实现可以在与本书相关的GitHub仓库（[http://mng.bz/KBdZ](http://mng.bz/KBdZ)）中找到。您可以加载5.3.2节中训练的模型，直接进入模型可解释性。
- en: Visualizing the probability landscape is a great way of validating the output
    of the model. By comparing with the ground truth labels, we can see which cases
    the model gets wrong and tune the model accordingly. It is also a great way to
    visualize and monitor the output of the model after deploying it in production.
    The probability landscape, however, does not give us any information on how the
    model arrived at the prediction.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化概率景观是验证模型输出的好方法。通过与真实标签进行比较，我们可以看到模型在哪些情况下出错，并相应地调整模型。这也是在将模型部署到生产环境中后可视化并监控模型输出的好方法。然而，概率景观并没有告诉我们模型是如何到达预测的。
- en: 5.4.2 LIME
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 LIME
- en: 'One way of interpreting a CNN is by using any one of the model-agnostic interpretability
    techniques that we learned in the previous chapter. Let’s specifically look at
    how to apply the *LIME* interpretability technique to images and CNNs. To recapitulate,
    the LIME technique is a model-agnostic technique that is local in scope. On a
    tabular dataset, the technique works as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 解释卷积神经网络（CNN）的一种方法是通过使用我们在上一章中学到的任何一种模型无关的可解释性技术。让我们具体看看如何将*LIME*可解释性技术应用于图像和CNN。为了回顾，LIME技术是一种局部范围内的模型无关技术。在一个表格数据集上，该技术的工作方式如下：
- en: Pick one example to interpret.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个例子进行解释。
- en: Create a perturbed dataset by sampling from a Gaussian distribution given the
    mean and standard deviation of the features in the tabular dataset.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从表格数据集中特征的均值和标准差进行高斯分布采样来创建扰动数据集。
- en: Run the perturbed dataset through the black-box model, and obtain the predictions.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将扰动数据集通过黑盒模型运行，并获取预测结果。
- en: Weight the samples based on their proximity with the picked example, where samples
    that are closer to the picked example are given a higher weight. As we saw in
    chapter 4, a hyperparameter called the *kernel width* is used for weighting the
    samples. If the kernel width is small, only samples that are close to the picked
    instance will influence the interpretation.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据样本与所选例子的接近程度对样本进行加权，其中接近所选例子的样本被赋予更高的权重。正如我们在第4章中看到的，一个称为*核宽度*的超参数用于对样本进行加权。如果核宽度小，只有接近所选实例的样本才会影响解释。
- en: Finally, fit a white-box model that is easily interpretable on the weighted
    samples. For LIME, linear regression is used.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在加权样本上拟合一个易于解释的白盒模型。对于LIME，使用线性回归。
- en: The weights of the linear regression model can be used to determine the importance
    of features for that picked example. The interpretation is obtained using a surrogate
    model that is locally faithful to the example that we wish to interpret. Now,
    how do we apply LIME to images? As with tabular data, we first need to pick an
    image that we wish to interpret. Next, we have to create a perturbed dataset.
    We cannot perturb the dataset the same way as we do for tabular data, by sampling
    from a Gaussian distribution. Instead, we randomly turn pixels off and on in the
    image. This is computationally intensive because to come up with an interpretation
    that is locally faithful, we have to generate a lot of samples to run through
    the model. Moreover, pixels could be spatially correlated, and multiple pixels
    could contribute to one target class. We, therefore, segment the image into multiple
    segments, also called *superpixels*, and turn random superpixels on and off, as
    illustrated in figure 5.14.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的权重可以用来确定所选例子的特征重要性。通过使用一个局部忠实于我们希望解释的例子的代理模型，我们可以获得解释。现在，我们如何将LIME应用于图像？与表格数据类似，我们首先需要选择一个我们希望解释的图像。接下来，我们必须创建一个扰动数据集。我们不能像处理表格数据那样扰动数据集，通过从高斯分布中进行采样。相反，我们在图像中随机打开和关闭像素。这计算量很大，因为为了生成一个局部忠实的解释，我们必须生成大量的样本来运行模型。此外，像素可能在空间上相关，多个像素可能对同一个目标类别做出贡献。因此，我们将图像分割成多个部分，也称为*超像素*，并随机打开和关闭超像素，如图5.14所示。
- en: '![](../Images/CH05_F14_Thampi.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 LIME如何创建扰动图像的说明](../Images/CH05_F14_Thampi.png)'
- en: Figure 5.14 An illustration of how to create a perturbed image for LIME
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 如何为LIME创建扰动图像的说明
- en: We can read figure 5.14 from bottom to top. The idea is to segment the original
    image by grouping multiple pixels into a superpixel. In this illustration, we
    are using a simple segmentation algorithm where the original image is segmented
    into four nonoverlapping rectangular segments. Once you have formed the segmented
    image with superpixels, you can create the perturbed image by turning random superpixels
    on and off. By default, the LIME implementation uses the quickshift ([http://mng.bz/W7mw](http://mng.bz/W7mw))
    segmentation algorithm. Once you have created the perturbed dataset, the rest
    of the technique is the same as with tabular data. The weights of the linear surrogate
    model will give us an idea of the influence of features or superpixels in the
    final model prediction for the picked input image. We segment the image into superpixels
    because we are attempting to group correlated pixels together and to look at the
    effects on the final prediction.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从底部到顶部阅读图 5.14。其思想是通过将多个像素分组到超像素中来分割原始图像。在这个示例中，我们使用了一个简单的分割算法，将原始图像分割成四个不重叠的矩形段。一旦你用超像素形成了分割图像，你可以通过打开和关闭随机超像素来创建扰动图像。默认情况下，LIME
    实现使用 quickshift ([http://mng.bz/W7mw](http://mng.bz/W7mw)) 分割算法。一旦创建了扰动数据集，其余的技术与表格数据相同。线性代理模型的权重将给我们一个关于特征或超像素对所选输入图像最终模型预测影响的直观认识。我们通过将图像分割成超像素，因为我们试图将相关的像素分组在一起，并观察对最终预测的影响。
- en: 'Let’s now see how to implement LIME for the ResNet model trained earlier. We
    will first need to split the one PyTorch transform that we used earlier into two.
    The first one will transform the input Python Imaging Library (PIL) image into
    a 50 × 50 tensor, and the second one will normalize it. The first transformation,
    shown next, is required for the image segmentation algorithm in LIME:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何实现之前训练的 ResNet 模型的 LIME。我们首先需要将之前使用的单个 PyTorch 转换器分成两个。第一个将输入的 Python
    Imaging Library (PIL) 图像转换为 50 × 50 的张量，第二个将对其进行归一化。下面的第一个转换是 LIME 图像分割算法所必需的：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① The first transformation to resize the input image to 50 × 50 pixels required
    for the image segmentation algorithm
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将输入图像调整大小为 50 × 50 像素，这是图像分割算法所需的第一个转换
- en: ② The second transformation to normalize the transformed 50 × 50 input image
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ② 第二个转换，用于归一化转换后的 50 × 50 输入图像
- en: 'Next, we will need two helper functions—one to load the image file as a PIL
    image and the other to perform predictions on the perturbed dataset using the
    model. The next code snippet shows these functions, where `get_image` is the function
    to load the PIL image and `batch_predict` is the function that runs the perturbed
    `images` through the `model`. We also create a partial function that presets the
    model parameter with the ResNet model that we trained in the previous section:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要两个辅助函数——一个用于将图像文件加载为 PIL 图像，另一个用于使用模型对扰动数据集进行预测。下面的代码片段显示了这些函数，其中 `get_image`
    是加载 PIL 图像的函数，`batch_predict` 是将扰动 `images` 通过 `model` 运行的函数。我们还创建了一个部分函数，它预先设置了
    ResNet 模型参数，该参数是我们之前章节中训练的：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① A helper function to read the input RGB image into memory
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个辅助函数，用于将输入 RGB 图像读取到内存中
- en: ② Opens the image and converts it to RGB
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ② 打开图像并将其转换为 RGB
- en: ③ Returns the image
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 返回图像
- en: ④ A helper function to perform predictions on images in the perturbed dataset
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个辅助函数，用于在扰动数据集中的图像上进行预测
- en: ⑤ A function to compute the sigmoid of the input parameter
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 一个用于计算输入参数的 sigmoid 函数
- en: ⑥ Stacks all the transformed tensors for the input images
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将输入图像的所有转换张量堆叠在一起
- en: ⑦ Runs through the model to obtain the output for all images
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 通过模型运行以获得所有图像的输出
- en: ⑧ Detaches the output tensor and converts it to a NumPy array
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 断开输出张量并将其转换为 NumPy 数组
- en: ⑨ Returns the predictions as probabilities by passing them through the sigmoid
    function
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 通过通过 sigmoid 函数传递，将预测作为概率返回
- en: ⑩ A partial function to perform batch prediction using a pretrained ResNet model
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 一个部分函数，用于使用预训练的 ResNet 模型执行批量预测
- en: Note that in this code, we define a partial function called `batch_predict_with_model`.
    Partial functions in Python allow us to set a certain number of arguments in a
    function and generate a new function. We are using the `batch_predict` function
    and setting the `model` parameter with the ResNet model trained earlier. You can
    replace this with any other model that you wish to interpret using LIME.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这段代码中，我们定义了一个名为`batch_predict_with_model`的部分函数。Python中的部分函数允许我们在函数中设置一定数量的参数并生成一个新的函数。我们正在使用`batch_predict`函数，并使用之前训练的ResNet模型设置`model`参数。你可以用任何其他你希望使用LIME进行解释的模型替换它。
- en: 'Because LIME is a local interpretability technique, we need to pick an example
    to interpret. For the ResNet model, we will pick two patches to interpret—one
    that is IDC negative and the other that is IDC positive—from the test set, as
    shown here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LIME是一种局部可解释技术，我们需要挑选一个示例进行解释。对于ResNet模型，我们将挑选两个补丁进行解释——一个是IDC阴性，另一个是IDC阳性——来自测试集，如图所示：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① An IDC-negative example with ID 142
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个ID为142的IDC阴性示例
- en: ② An IDC-positive example with ID 41291
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一个ID为41291的IDC阳性示例
- en: ③ Loads the PIL image for the IDC-negative example
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载IDC阴性示例的PIL图像
- en: ④ Loads the PIL image for the IDC-positive example
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 加载IDC阳性示例的PIL图像
- en: 'Now we will initialize the LIME explainer and use that to interpret the two
    examples that we picked. The following code snippet shows how to obtain the LIME
    explanation for the IDC-negative example. As an exercise, obtain the LIME explanation
    for the IDC-positive example and set it to a variable named `idc_exp`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将初始化LIME解释器，并使用它来解释我们挑选的两个示例。以下代码片段展示了如何获取IDC阴性示例的LIME解释。作为练习，获取IDC阳性示例的LIME解释并将其赋值给名为`idc_exp`的变量：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Imports the lime_image module from the LIME library
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从LIME库中导入lime_image模块
- en: ② Initializes the LIME image explainer
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化LIME图像解释器
- en: ③ First transforms the IDC-negative image for segmentation
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 首先将IDC阴性图像进行分割转换
- en: ④ Passes the partial function to predict on the perturbed dataset using the
    ResNet model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将部分函数传递给ResNet模型，在扰动数据集上进行预测
- en: ⑤ Perturbs the segmented image to create 1,000 samples
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 对分割图像进行扰动以创建1,000个样本
- en: 'Using the LIME explanation variable shown in the previous code, obtain the
    RGB image and the 2-D mask that contains the explanation. As an exercise, obtain
    the masked LIME image for the IDC-positive example, and name the masked image
    `i_img_boundary`. To do this, you need to complete the previous exercise and obtain
    the LIME explanation for the IDC-positive example first. The solutions to these
    exercises can be found in the GitHub repository ([http://mng.bz/KBdZ](http://mng.bz/KBdZ))
    associated with this book:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一段代码中显示的LIME解释变量，获取包含解释的RGB图像和2D掩码。作为练习，获取IDC阳性示例的掩码LIME图像，并将其命名为`i_img_boundary`。为此，你需要完成之前的练习，首先获取IDC阳性示例的LIME解释。这些练习的解决方案可以在与本书相关的GitHub仓库([http://mng.bz/KBdZ](http://mng.bz/KBdZ))中找到：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Imports the mark_boundaries function from the skimage library to plot the
    segmented image
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从skimage库中导入mark_boundaries函数以绘制分割图像
- en: ② Obtains the masked LIME image for the IDC-negative example
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取IDC阴性示例的掩码LIME图像
- en: ③ Plots the masked image using the mark_boundaries function
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用mark_boundaries函数绘制掩码图像
- en: 'We can now visualize the LIME explanations for both the IDC-positive and -negative
    patches using the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码可视化IDC阳性和阴性补丁的LIME解释：
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Obtains the model confidence for the IDC-negative patch
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取IDC阴性补丁的模型置信度
- en: ② Obtains the model confidence for the IDC-positive patch
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取IDC阳性补丁的模型置信度
- en: ③ Obtains the image of the IDC-negative patch
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取IDC阴性补丁的图像
- en: ④ Obtains the image of the IDC-positive patch
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取IDC阳性补丁的图像
- en: ⑤ Obtains the patient ID for the IDC-negative patch
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 获取IDC阴性补丁的患者ID
- en: ⑥ Obtains the patient ID for the IDC-positive patch
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取IDC阳性补丁的患者ID
- en: ⑦ Creates a 2 × 2 figure to plot the original images and LIME explanations
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 创建一个2×2的图形来绘制原始图像和LIME解释
- en: ⑧ Plots the original image of the IDC-negative patch on the top-left cell
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 在左上角单元格中绘制IDC阴性补丁的原始图像
- en: ⑨ Plots the LIME explanation for the IDC-negative patch on the top-right cell
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 在右上角单元格中绘制IDC阴性补丁的LIME解释
- en: ⑩ Plots the original image of the IDC-positive patch on the bottom-left cell
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 在左下角单元格中绘制IDC阳性补丁的原始图像
- en: ⑪ Plots the LIME explanation for the IDC-positive patch on the bottom-right
    cell
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 在右下角单元格中绘制IDC阳性补丁的LIME解释
- en: Figure 5.15 shows the resulting visualization. The figure has been annotated
    where the top-left image is the original image of the IDC-negative patch. The
    top-right image is the LIME explanation of the IDC-negative patch. We can see
    that the model predicts that the patch is IDC negative with 82% confidence. From
    the original image, we can see that the density of the lighter stain is higher,
    which matches the pattern that we saw in section 5.2 (figure 5.4). The lighter
    stain is typically used to highlight the cytoplasm and extracellular connective
    tissue. If we look at the LIME explanation, we can see that the segmentation algorithm
    has highlighted two superpixels where the segmentation boundary separates the
    highly dense lighter stains from the rest of the image. The segment or superpixel
    that positively influences the prediction is shown in red (darker shade). This
    is annotated as the left half of the segmented image. The segment or superpixel
    that negatively influences the prediction is shown in green (lighter shade). This
    is annotated as the right half of the segmented image. The LIME explanation, therefore,
    seems to have correctly highlighted the dense lighter stains as contributing positively
    to predicting IDC negative with high confidence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15显示了生成的可视化结果。图中已标注，左上角图像是IDC阴性区域的原始图像。右上角图像是IDC阴性区域的LIME解释。我们可以看到模型预测该区域为IDC阴性，置信度为82%。从原始图像中，我们可以看到较浅染色质的密度较高，这与我们在第5.2节（图5.4）中看到的模式相匹配。较浅染色质通常用于突出细胞质和细胞外结缔组织。如果我们查看LIME解释，我们可以看到分割算法突出了两个超像素，分割边界将高密度较浅染色质与其他图像部分分开。对预测产生正面影响的区域或超像素用红色（较深的色调）表示，这标注为分割图像的左侧。对预测产生负面影响的区域或超像素用绿色（较浅的色调）表示，这标注为分割图像的右侧。因此，LIME解释似乎正确地突出了密集的较浅染色质，并作为预测IDC阴性具有高置信度的贡献因素。
- en: '![](../Images/CH05_F15_Thampi.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F15_Thampi.png)'
- en: Figure 5.15 The LIME explanation for IDC-negative and IDC-positive patches
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15展示了LIME对IDC阴性区域和IDC阳性区域的解释。
- en: The bottom-left image in figure 5.15 is the original image of the IDC-positive
    patch. The LIME explanation for this patch is shown on the bottom right of the
    figure. We can see from the original image that the density of darker stains is
    a lot higher, which matches the pattern that we saw in section 5.2 (figure 5.3).
    If we look at the LIME explanation now, we can see that the segmentation algorithm
    treats the entire image as the superpixel, and the whole superpixel contributes
    positively to predict IDC positive with high confidence. Although this explanation
    makes sense at a high level because the whole image consists of a high density
    of darker stains, it does not give us any additional information about what specific
    pixels are influencing the model prediction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15左下角的图像是IDC阳性区域的原始图像。该区域的LIME解释显示在图的右下角。我们可以从原始图像中看到较深染色质的密度要高得多，这与我们在第5.2节（图5.3）中看到的模式相匹配。如果我们现在查看LIME解释，我们可以看到分割算法将整个图像视为超像素，整个超像素对预测IDC阳性具有高置信度产生正面影响。尽管这种解释在高级别上是合理的，因为整个图像由高密度的较深染色质组成，但它并没有给我们提供任何关于哪些特定像素影响模型预测的额外信息。
- en: This brings us to some of the drawbacks of LIME. As we saw in chapter 4 and
    in this section, LIME is a great interpretability technique as it is model-agnostic
    and can be applied to any complex model. It has a few disadvantages, however.
    The quality of the LIME explanation depends heavily on the choice of the kernel
    width. As we saw in chapter 4, this is an important hyperparameter, and the same
    kernel width may not be applicable for all examples that we wish to interpret.
    LIME explanations can also be unstable because they depend on how the perturbed
    dataset is sampled. The interpretation is also dependent on the specific segmentation
    algorithm we use. As we saw in figure 5.15, the segmentation algorithm treats
    the entire image as a superpixel. The computation complexity of LIME is also high,
    depending on the number of pixels or superpixels that need to be turned on or
    off.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们想到了LIME的一些缺点。正如我们在第4章和本节中看到的，LIME是一种非常出色的可解释性技术，因为它对模型没有依赖性，可以应用于任何复杂的模型。然而，它也有一些缺点。LIME解释的质量很大程度上取决于核宽度的选择。正如我们在第4章中看到的，这是一个重要的超参数，相同的核宽度可能不适用于我们希望解释的所有示例。LIME解释也可能不稳定，因为它们依赖于扰动数据集的采样方式。解释还依赖于我们使用的特定分割算法。正如我们在图5.15中看到的，分割算法将整个图像视为一个超级像素。LIME的计算复杂度也较高，这取决于需要打开或关闭的像素或超级像素的数量。
- en: 5.4.3 Visual attribution methods
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 可视化归因方法
- en: 'Now let’s take a step back and look at LIME from the context of a broader class
    of interpretability methods called visual attribution methods. Visual attribution
    methods are used to attribute importance to parts of an image that influence the
    prediction made by the CNN. Three broad categories of visual attribution methods
    follow and are shown in figure 5.16:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们退一步，从更广泛的可解释性方法类别的角度来审视LIME，这类方法被称为可视化归因方法。可视化归因方法用于将重要性归因于影响CNN做出的预测的图像部分。以下列出了三种广泛的可视化归因方法类别，并在图5.16中展示：
- en: Perturbations
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干扰
- en: Gradients
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度
- en: Activations
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活
- en: '![](../Images/CH05_F16_Thampi.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F16_Thampi.png)'
- en: Figure 5.16 Types of visual attribution methods
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 可视化归因方法的类型
- en: Interpretability techniques like LIME and SHAP are *perturbation-based methods*.
    As we saw in chapter 4 and in the previous section, the idea is to perturb the
    input and probe its effects on the predictions made by the CNN. These techniques
    are *model-agnostic*, *post hoc**,* and *local* interpretability techniques. Perturbation-based
    methods are, however, computationally inefficient because each perturbation requires
    us to perform a forward pass on the complex CNN model. These techniques can also
    underestimate the importance of features, where the features are parts of the
    image that are based on the segmentation done on the original image.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LIME和SHAP的可解释性技术是*基于扰动的方 法*。正如我们在第4章和前一节中看到的，其想法是扰动输入并探测其对CNN做出的预测的影响。这些技术是*模型无关的*、*事后分析*和*局部可解释性技术*。然而，基于扰动的方
    法在计算上效率不高，因为每次扰动都需要我们在复杂的CNN模型上执行前向传递。这些技术也可能低估特征的重要性，其中特征是基于对原始图像进行的分割的部分。
- en: '*Gradient-based methods* are used to visualize the gradient of the target class
    with respect to the input image. The idea is to pick an example or image to interpret.
    We then run this image through the CNN in the forward direction to obtain the
    output prediction. We apply the backpropagation algorithm to compute the gradient
    of the output class with respect to the input image. The gradient is a good importance
    measure because it tells us which pixels need to be changed to affect the model
    output. If the magnitude of the gradient is large, then a small change to the
    pixel value will result in a large change in the input. Therefore, pixels with
    large gradient measures are considered most important, or salient, for the model.
    Gradient-based methods are sometimes also called *backpropagation methods* because
    the backpropagation algorithm is used to determine feature importance. They are
    also called *saliency maps* because a map of salient, or important, features is
    obtained. Popular gradient-based methods are *vanilla backpropagation*, *guided
    backpropagation*, *integrated gradients*, and *SmoothGrad*, which will be covered
    in the sections 5.5–5.7\. These techniques are local in scope and also post hoc.
    They are not entirely model-agnostic, however, and are weakly model-dependent.
    They are much more computationally efficient when compared to perturbation-based
    methods because only one forward and backward pass is required for one image.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于梯度的方法* 用于可视化目标类别相对于输入图像的梯度。想法是选择一个示例或图像进行解释。然后，我们将此图像通过 CNN 的正向方向运行以获得输出预测。我们应用反向传播算法来计算输出类别相对于输入图像的梯度。梯度是一个很好的重要性度量，因为它告诉我们哪些像素需要改变才能影响模型输出。如果梯度的幅度很大，那么对像素值的微小改变将导致输入的大幅变化。因此，具有大梯度度量的像素被认为对模型来说是最重要的，或是最显著的。基于梯度的方法有时也被称为
    *反向传播方法*，因为反向传播算法用于确定特征重要性。它们也被称为 *显著性图*，因为可以获得一个显著或重要的特征图。流行的基于梯度的方法包括 *vanilla
    backpropagation*、*引导反向传播*、*积分梯度* 和 *SmoothGrad*，这些将在 5.5–5.7 节中介绍。这些技术在范围上是局部的，也是事后的。然而，它们并不是完全模型无关的，并且是弱模型依赖的。与基于扰动的方'
- en: '*Activation-based methods* look at the feature maps or activations in the final
    convolutional layer and weight them based on the gradient of the target class
    with respect to those feature maps. The weights of the feature maps act as a proxy
    for the importance of the input features. This technique is called gradient-weighted
    class activation mapping (*Grad-CAM*). Because we are looking at the importance
    of the feature map in the final convolutional layer, Grad-CAM provides a *coarse-grained*
    activation map. To obtain more *fine-grained* activation maps, we can combine
    Grad-CAM and guided backpropagation—this technique is called *guided Grad-CAM*.
    We will see how Grad-CAM and guided Grad-CAM work in more detail in section 5.8\.
    Activation-based methods are also weakly model-dependent, post hoc, and local
    interpretability techniques.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于激活的方法* 会查看最终卷积层的特征图或激活，并根据目标类别对这些特征图的梯度进行加权。特征图的权重作为输入特征重要性的代理。这种技术被称为梯度加权类激活映射
    (*Grad-CAM*)。因为我们关注的是最终卷积层中特征图的重要性，所以 Grad-CAM 提供了一个 *粗粒度* 的激活图。为了获得更 *细粒度* 的激活图，我们可以结合
    Grad-CAM 和引导反向传播——这种技术被称为 *引导 Grad-CAM*。我们将在 5.8 节中更详细地了解 Grad-CAM 和引导 Grad-CAM
    的工作原理。基于激活的方法也是弱模型依赖、事后和局部可解释的技术。'
- en: 5.5 Vanilla backpropagation
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 梯度下降法
- en: In this section, we will learn about a gradient-based attribution method called
    *vanilla backpropagation*. Vanilla backpropagation was proposed in 2014 by Karen
    Simonyan et al, and this technique is illustrated in figure 5.17.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习一种基于梯度的归因方法，称为 *vanilla backpropagation*。vanilla backpropagation 由
    Karen Simonyan 等人在 2014 年提出，该技术如图 5.17 所示。
- en: '![](../Images/CH05_F17_Thampi.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F17_Thampi.png)'
- en: Figure 5.17 An illustration of vanilla backpropagation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 梯度下降法的示意图
- en: The first step is to pick an image or example to interpret. Because we are looking
    at interpreting a single instance, the scope of this interpretability technique
    is local. The second step is to perform a forward pass on the CNN to obtain the
    output class prediction. Once you have obtained the output class, the next step
    is to obtain the gradient of the output with respect to the penultimate layer
    and perform a backward pass—which we learned about in chapter 4—to ultimately
    obtain the gradient of the output class with respect to the pixels in the input
    image. The gradients for the input pixels or features are used as important measures.
    The larger the gradient for the pixel, the more important that pixel is for the
    model to predict the output class. The intuition behind it is that if the magnitude
    of the gradient is large for a given pixel, then a small change in the pixel value
    will have a larger impact on the model prediction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择一个图像或示例进行解释。因为我们正在查看解释单个实例，所以这种可解释性技术的范围是局部的。第二步是在CNN上执行前向传播以获得输出类别预测。一旦获得了输出类别，下一步就是获得输出相对于倒数第二层的梯度并执行反向传播——我们在第4章中学习了这一点——以最终获得输出类别相对于输入图像中像素的梯度。输入像素或特征的梯度被用作重要指标。像素的梯度越大，该像素对模型预测输出类的重要性就越大。其背后的直觉是，如果给定像素的梯度幅度很大，那么像素值的微小变化将对模型预测产生更大的影响。
- en: 'Vanilla backpropagation and other gradient-based methods have been implemented
    in PyTorch by Utku Ozbulak and open sourced in this GitHub repository: [http://mng.bz/8l8B](http://mng.bz/8l8B).
    These implementations cannot be directly applied to the ResNet architecture or
    ResNet-based architectures, however, and I have adapted them in this book so that
    they can be applied to these more advanced architectures. The next code snippet
    implements the vanilla backpropagation technique as a Python class:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 纯反向传播和其他基于梯度的方法已在PyTorch中由Utku Ozbulak实现，并在本GitHub仓库中开源：[http://mng.bz/8l8B](http://mng.bz/8l8B)。然而，这些实现不能直接应用于ResNet架构或基于ResNet的架构，因此我在本书中对其进行了修改，以便它们可以应用于这些更高级的架构。下面的代码片段实现了纯反向传播技术作为一个Python类：
- en: '[PRE12]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① A constructor for vanilla backpropagation that takes in the model and the
    start of the feature layers
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个用于纯反向传播的构造函数，它接受模型和特征层的起始部分
- en: ② Initializes the model object
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化模型对象
- en: ③ Initializes the gradients object as None
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将梯度对象初始化为None
- en: ④ Sets the model in evaluation mode
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将模型设置为评估模式
- en: ⑤ Sets the features object that points to the start of the feature layers in
    the model
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置指向模型中特征层起始部分的features对象
- en: ⑥ Hooks the layers so that you can compute the gradient of the output with respect
    to the input pixels
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 钩住层以便可以计算输出相对于输入像素的梯度
- en: ⑦ A function to hook the first layer to get the gradient
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 一个用于钩住第一层以获取梯度的函数
- en: ⑧ A helper function used to process the input and output gradients during backpropagation
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 用于在反向传播过程中处理输入和输出梯度的辅助函数
- en: ⑨ Sets the grad_in object to the gradient obtained from the previous layer
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 将grad_in对象设置为从上一层获得的梯度
- en: ⑩ Sets the grad_out object to the gradient obtained from the current layer
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 将grad_out对象设置为从当前层获得的梯度
- en: ⑪ Obtains the gradients with respect to the pixels in the feature map for the
    current layer
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 获得当前层相对于特征图像素的梯度
- en: ⑫ Obtains the first feature layer
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 获得第一个特征层
- en: ⑬ Registers the backward hook function to obtain the gradient of the output
    class with respect to the input pixels
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 注册反向钩子函数以获得输出类别相对于输入像素的梯度
- en: ⑭ A function to perform backpropagation to obtain the gradients
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 一个执行反向传播以获得梯度的函数
- en: ⑮ Obtains the model output by propagating the image through the model in the
    forward direction
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 通过在模型中前向传播图像来获得模型输出
- en: ⑯ Resets the gradient to 0 before backpropagating
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 在反向传播之前将梯度重置为0
- en: ⑰ Creates a one-hot-encoded tensor where the target class is set to 1
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 创建一个将目标类别设置为1的一热编码张量
- en: ⑱ Performs backward propagation
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⑱ 执行反向传播
- en: ⑲ Returns the gradients object obtained through the hook function
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ⑲ 通过钩子函数返回获得的梯度对象
- en: 'Note that the features layers for the ResNet model and other architectures
    like Inception v3 and ResNeXt can be found in the parent model and are not stored
    in a hierarchical structure as in VGG16 and AlexNet architectures, where the features
    layers are stored within the `features` key in the model. You can test this by
    initializing the VGG16 model, as follows, and printing it to see its structure:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ResNet模型和其他架构（如Inception v3和ResNeXt）的特征层可以在父模型中找到，并且它们不是以VGG16和AlexNet架构中的层次结构存储，在VGG16和AlexNet架构中，特征层存储在模型的`features`键中。你可以通过以下方式初始化VGG16模型来测试这一点，并打印其结构：
- en: '[PRE13]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of this print statement is shown next. The output is clipped and
    is meant to show how the feature layers are stored within the features key. You
    will get similar output if you replace `vgg16` with `alexnet` in the previous
    code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个打印语句的输出如下所示。输出被截断，目的是展示特征层是如何存储在`features`键中的。如果你将上一段代码中的`vgg16`替换为`alexnet`，你将得到类似的输出：
- en: '[PRE14]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The implementation by Utku Ozbulak expects the architecture to have the same
    hierarchical structure as VGG16 and AlexNet. In the vanilla backpropagation implementation
    earlier, on the other hand, the feature layers are explicitly passed to the constructor
    so that it can be used for more complex architectures. You can now instantiate
    this class for the ResNet model as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Utku Ozbulak的实现期望架构具有与VGG16和AlexNet相同的层次结构。另一方面，在先前的标准反向传播实现中，特征层被显式地传递给构造函数，以便它可以用于更复杂的架构。现在，你可以如下实例化这个类来为ResNet模型使用：
- en: '[PRE15]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will now create a helper function to obtain the gradients of the output
    with respect to the input, as shown next:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个辅助函数来获取输出相对于输入的梯度，如下所示：
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① The get_grads function takes in the gradient-based method, the dataset, and
    the index of the example to be interpreted.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ① `get_grads`函数接收基于梯度的方法、数据集和要解释的示例索引。
- en: ② Obtains the image and label at index idx
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在索引idx处获取图像和标签
- en: ③ Reshapes the image to be able to run through the model
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将图像重塑为可以通过模型运行
- en: ④ Creates the PyTorch variable where requires_grad is True to obtain the gradients
    through backpropagation
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个PyTorch变量，其中requires_grad为True，以通过反向传播获取梯度
- en: ⑤ Obtains the gradients using the generate_gradients function
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用`generate_gradients`函数获取梯度
- en: ⑥ Returns the gradients with respect to the input pixels
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 返回相对于输入像素的梯度
- en: 'We will use the same two examples that we used for the LIME technique in section
    5.4.2—one IDC-negative patch and one IDC-positive patch. We can now obtain the
    gradients using the vanilla backpropagation technique as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与第5.4.2节中LIME技术相同的两个示例——一个IDC负片和一个IDC正片。现在，我们可以使用标准的反向传播技术来获取梯度，如下所示：
- en: '[PRE17]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that the test dataset is the `PatchDataset` for the test set that we initialized
    in section 5.3.1\. The resulting gradients array shown here will have the same
    dimension as the input image. The input image has dimension 3 × 50 × 50 where
    there are three channels (red, green, blue) and the height and width of the image
    are 50 pixels each. The resulting gradients will also have the same dimension
    and can be visualized as a color image. For ease of visualization, though, we
    will convert the gradients image to grayscale. We can use the following helper
    function to convert from color to grayscale:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，测试数据集是我们第5.3.1节中初始化的`PatchDataset`。这里显示的梯度数组将与输入图像具有相同的维度。输入图像的维度为3 × 50
    × 50，其中包含三个通道（红色、绿色、蓝色），图像的高度和宽度均为50像素。生成的梯度也将具有相同的维度，可以将其可视化为彩色图像。然而，为了便于可视化，我们将梯度图像转换为灰度图。我们可以使用以下辅助函数将彩色图像转换为灰度图：
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we have the gradients obtained through vanilla backpropagation, we
    can visualize them the same way as we visualized the LIME explanation. As an exercise,
    I encourage you to extend the visualization code in section 5.4.2 to replace the
    LIME explanation with the grayscale representation of the gradients. The resulting
    figure is shown in figure 5.18.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过标准反向传播获得了梯度，我们可以像可视化LIME解释一样可视化它们。作为一个练习，我鼓励你将第5.4.2节中的可视化代码扩展，用梯度的灰度表示替换LIME解释。结果图如图5.18所示。
- en: '![](../Images/CH05_F18_Thampi.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F18_Thampi.png)'
- en: Figure 5.18 A saliency map using vanilla backpropagation
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 使用标准反向传播的显著性图
- en: Let’s focus on the IDC-negative patch first. The original image of the patch
    is shown on the top left. The grayscale representation of the gradients obtained
    through vanilla backpropagation is shown on the top right. We can see pixels with
    various shades of gray in the image. Larger gradients have a higher intensity
    of gray or appear white. This is a great way to visualize what pixels the CNN
    is focusing on to predict that the image is IDC negative with a confidence of
    82%. The salient, or important, pixels correspond to the dense lighter stains
    in the original image. Because gradients are shown at the pixel level, this is
    a much more fine-grained interpretation than LIME, where LIME focuses only on
    superpixels. Saliency maps are a great way to debug the CNN as a data scientist
    or engineer and also help the expert pathologist understand what parts of the
    image the CNN is paying attention to.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先关注 IDC-阴性区域。该区域的原始图像显示在左上角。通过传统反向传播获得的梯度灰度表示显示在右上角。我们可以看到图像中有各种灰度的像素。较大的梯度具有更高的灰度强度或呈现为白色。这是一种很好的可视化
    CNN 关注哪些像素来预测图像是 IDC 阴性且置信度为 82% 的方法。显著的或重要的像素对应于原始图像中密集的较亮污点。因为梯度在像素级别显示，所以这比
    LIME 的解释要细粒度得多，LIME 只关注超级像素。显著性图是作为数据科学家或工程师调试 CNN 的好方法，也有助于专家病理学家了解 CNN 关注图像的哪些部分。
- en: Let’s now look at the IDC-positive patch. The original image is shown on the
    bottom left, and the interpretation obtained through vanilla backpropagation is
    shown on the bottom right. We can see that a lot more pixels are lit up, which
    corresponds to the darker stains or the nuclei in the input image. This interpretation
    is a lot better than the LIME interpretation, where in LIME, the whole image was
    treated as a superpixel.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看 IDC-阳性区域。原始图像显示在左下角，通过传统反向传播获得的解释显示在右下角。我们可以看到很多像素被点亮，这对应于输入图像中的较暗的污点或细胞核。这种解释比
    LIME 的解释要好得多，在 LIME 中，整个图像被视为一个超级像素。
- en: 5.6 Guided backpropagation
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 引导反向传播
- en: '*Guided backpropagation* is another gradient-based attribution method proposed
    in 2015 by J. T. Springenberg et al. It is similar to vanilla backpropagation,
    with the only difference being in the way it handles the gradient when it passes
    through a rectified linear unit (ReLU). As we saw in chapter 4, ReLU is a nonlinear
    activation function that clips negative input values to zero. The guided backpropagation
    technique zeroes out the gradient into a ReLU if the gradient is negative or if
    input to the ReLU during the forward pass is negative. The idea behind guided
    backpropagation is to focus only on input features that positively influence the
    model prediction.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*引导反向传播* 是由 J. T. Springenberg 等人在 2015 年提出的一种基于梯度的属性方法。它与传统的反向传播类似，唯一的区别在于它处理梯度通过修正线性单元（ReLU）的方式。正如我们在第
    4 章中看到的，ReLU 是一个非线性激活函数，它将负输入值裁剪为零。引导反向传播技术会将梯度置零到 ReLU 中，如果梯度是负的，或者在正向传播过程中 ReLU
    的输入是负的。引导反向传播背后的思想是只关注对模型预测产生积极影响的输入特征。'
- en: 'The guided backpropagation technique has also been implemented in PyTorch in
    the repository at [http://mng.bz/8l8B](http://mng.bz/8l8B), but it has been adapted
    in this book so that it can be applied to more complex architectures like ResNet,
    where there are nested layers with ReLUs. The following code snippet shows the
    improved implementation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 引导反向传播技术也已实现在 PyTorch 的 [http://mng.bz/8l8B](http://mng.bz/8l8B) 仓库中，但在这本书中已经进行了适配，以便它可以应用于更复杂的架构，如
    ResNet，其中包含具有 ReLU 的嵌套层。以下代码片段显示了改进的实现：
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Imports the ReLU activation function and the Sequential container
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 ReLU 激活函数和 Sequential 容器
- en: ② The contructor for guided backpropagation is similar to vanilla backpropagation
    with one additional function call to update the ReLUs during backpropagation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ② 引导反向传播的构造函数类似于传统反向传播，只是在反向传播期间增加了一个更新 ReLU 的函数调用。
- en: ③ A function to hook the first layer to get the gradient, similar to vanilla
    backpropagation
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个用于钩子第一层以获取梯度的函数，类似于传统反向传播
- en: ④ A function to update the ReLUs
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个用于更新 ReLU 的函数
- en: ⑤ A function to impute zero for gradient values that are less than 0
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 一个用于对梯度值小于 0 的值进行零值插补的函数
- en: ⑥ Obtains the output of the ReLU in the forward pass
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取正向传播中 ReLU 的输出
- en: ⑦ Sets the identifier variable where positive values are set to 1
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 设置标识变量，其中正值设置为 1
- en: ⑧ Imputes negative gradient values with 0
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 用 0 插补负梯度值
- en: ⑨ Removes the last forward output
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 移除最后的正向输出
- en: ⑩ Returns the modified gradients
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 返回修改后的梯度
- en: ⑪ A helper function to store the outputs of the ReLUs during the forward pass
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 一个辅助函数，用于在正向传播期间存储ReLU的输出
- en: ⑫ Iterates through all the feature layers
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 遍历所有特征层
- en: ⑬ If the module is ReLU, registers the hook functions to obtain the values during
    forward pass and clip the gradients during backprop
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 如果模块是ReLU，则注册钩子函数以在正向传播期间获取值并在反向传播期间裁剪梯度
- en: ⑭ If the module is a Sequential container, iterates through its submodules
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 如果模块是Sequential容器，则遍历其子模块
- en: ⑮ If the submodule is ReLU, registers the hook functions as before
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 如果子模块是ReLU，则像之前一样注册钩子函数
- en: ⑯ If the submodule is a BasicBlock, iterates through its submodules
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 如果子模块是BasicBlock，则遍历其子模块
- en: ⑰ If the sub-submodule is a ReLU, registers the hook functions as before
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 如果子子模块是ReLU，则像之前一样注册钩子函数
- en: ⑱ A function to perform backpropagation to obtain the gradients, similar to
    vanilla backpropagation
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ⑱ 一个执行反向传播以获取梯度的函数，类似于普通反向传播
- en: 'We can use this adapted implementation for model architectures with up to three
    nested layers with ReLUs. Let’s now instantiate the guided backpropagation class
    for the ResNet model as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这种修改后的实现来处理最多具有三个嵌套层和ReLU的模型架构。现在让我们如下实例化ResNet模型的引导反向传播类：
- en: '[PRE20]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can now obtain the gradients using the same `get_gradients` helper function
    defined in section 5.5\. As an exercise, obtain the gradients for the two examples,
    convert them to grayscale, and then visualize them. The resulting figure is shown
    in figure 5.19.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用在5.5节中定义的相同`get_gradients`辅助函数来获取梯度。作为一个练习，获取两个示例的梯度，将它们转换为灰度，然后可视化。结果图如图5.19所示。
- en: '![](../Images/CH05_F19_Thampi.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F19_Thampi.png)'
- en: Figure 5.19 A saliency map using guided backpropagation
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 使用引导反向传播的显著性图
- en: The interpretation using guided backpropagation seems to show the model *f*ocusing
    on a lot more pixels for both the IDC-negative and IDC-positive patches. The pixels
    seem to correspond to regions of high-density lighter stains for the IDC-negative
    patch and high-density darker stains for the IDC-positive patch. The interpretations
    using vanilla backpropagation and guided backpropagation both seem to be legitimate,
    but which one should we use? We will discuss this in section 5.9.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用引导反向传播的解释似乎表明模型*关注*了IDC阴性片和IDC阳性片中的更多像素。对于IDC阴性片，像素似乎对应于高密度浅色斑点的区域，而对于IDC阳性片，像素似乎对应于高密度深色斑点的区域。使用普通反向传播和引导反向传播的解释似乎都是合理的，但我们应该使用哪一个？我们将在5.9节中讨论这个问题。
- en: 5.7 Other gradient-based methods
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 其他基于梯度的方法
- en: 'The vanilla and guided backpropagation methods both underestimate the importance
    of features in a model that exhibits saturation. What does this mean? Let’s take
    a look at a simple example as highlighted in a 2017 paper by Avanti Shrikumar
    et al., available at [https://arxiv.org/pdf/1704.02685;Learning](https://arxiv.org/pdf/1704.02685;Learning).
    Figure 5.20 illustrates a simple network that exhibits saturation in the output
    signal. The network takes in two inputs, *x1* and *x2.* The numbers on the arrows
    or edges are the weights that are used to multiply with the input unit that it
    connects. The final output of the network (or output signal) *y* can be evaluated
    as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 普通和引导反向传播方法都低估了在表现出饱和度的模型中特征的重要性。这意味着什么？让我们看看2017年Avanti Shrikumar等人发表的一篇论文中的简单例子，该论文可在[https://arxiv.org/pdf/1704.02685;Learning](https://arxiv.org/pdf/1704.02685;Learning)找到。图5.20展示了一个表现出输出信号饱和的简单网络。网络接受两个输入，*x1*和*x2*。箭头或边上的数字是用于与它连接的输入单元相乘的权重。网络的最终输出（或输出信号）*y*可以评估如下：
- en: '[PRE21]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If *x1* + *x2* is greater than 1, then the output signal `y` is saturated at
    1\. We can see that the gradient of the output with respect to the inputs is zero
    when the sum of the inputs is greater than 1\. At this point, both vanilla backpropagation
    and guided backpropagating underestimate the importance of the two input features
    because the gradients are 0.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*x1* + *x2*大于1，则输出信号`y`在1处饱和。我们可以看到，当输入之和大于1时，输出相对于输入的梯度为零。在这种情况下，普通反向传播和引导反向传播都低估了两个输入特征的重要性，因为梯度为0。
- en: '![](../Images/CH05_F20_Thampi.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F20_Thampi.png)'
- en: Figure 5.20 An illustration of a simple network that exhibits output signal
    saturation
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 展示了一个表现出输出信号饱和的简单网络的示意图
- en: To overcome the saturation problem, two gradient-based methods were proposed
    recently called integrated gradients ([https://arxiv.org/pdf/1703.01365.pdf](https://arxiv.org/pdf/1703.01365.pdf))
    and SmoothGrad ([https://arxiv.org/pdf/1706.03825.pdf](https://arxiv.org/pdf/1706.03825.pdf)).
    Integrated gradients were proposed in 2017 by Mukund Sundararajan et al. For a
    given input image, integrated gradients integrate the gradients as the input pixels
    are scaled from a starting value (e.g., all zeros) to their actual values. SmoothGrad
    was also proposed in 2017 by Daniel Smilkov et al. SmoothGrad adds pixelwise Gaussian
    noise to copies of the input image and then averages the resulting gradients obtained
    through vanilla backpropagation. Both of these techniques require integrating/averaging
    over multiple samples, similar to perturbation-based methods, thereby increasing
    the computational complexity. The resulting interpretations are also not guaranteed
    to be reliable, which is why we don’t explicitly cover them in this book. We will
    discuss them further in section 5.9\. For those interested, you can play around
    with these techniques using the implementation in PyTorch in the repository at
    [http://mng.bz/8l8B](http://mng.bz/8l8B).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服饱和问题，最近提出了两种基于梯度的方法，称为集成梯度 ([https://arxiv.org/pdf/1703.01365.pdf](https://arxiv.org/pdf/1703.01365.pdf))
    和 SmoothGrad ([https://arxiv.org/pdf/1706.03825.pdf](https://arxiv.org/pdf/1706.03825.pdf))。集成梯度是由
    Mukund Sundararajan 等人在 2017 年提出的。对于给定的输入图像，集成梯度将梯度积分，随着输入像素从起始值（例如，所有为零）缩放到实际值。SmoothGrad
    也是由 Daniel Smilkov 等人在 2017 年提出的。SmoothGrad 在输入图像的副本上添加像素级的高斯噪声，然后通过传统的反向传播计算得到的梯度进行平均。这两种技术都需要在多个样本上积分/平均，类似于基于扰动的技术，从而增加了计算复杂度。因此得到的解释也不一定可靠，这就是为什么我们不明确地在本书中涵盖它们。我们将在
    5.9 节中进一步讨论它们。对于感兴趣的人，您可以使用存储库中的 PyTorch 实现这些技术进行实验，存储库地址为 [http://mng.bz/8l8B](http://mng.bz/8l8B)。
- en: 5.8 Grad-CAM and guided Grad-CAM
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 Grad-CAM 和 guided Grad-CAM
- en: 'We will now focus our attention on activation-based methods. Grad-CAM was proposed
    in 2017 by R. R. Selvaraju et al. and is an activation-based attribution method
    that exploits the features learned through the convolutional layers. Grad-CAM
    looks at the feature map learned by the final convolutional layer in the CNN,
    and we obtain the importance of that feature map by calculating the gradient of
    the output with respect to the pixels in the feature map. Because we are looking
    at the feature map from the final convolutional layer, the activation map produced
    by Grad-CAM is coarse. The Grad-CAM technique is also implemented in the repository
    at [http://mng.bz/8l8B](http://mng.bz/8l8B) and is adapted as follows so that
    it can be applied to any CNN architecture. First, we will define a class called
    `CamExtractor` to obtain the output or feature map of the final convolutional
    layer and also the output of the classifier or fully connected layers:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注基于激活的方法。Grad-CAM 是由 R. R. Selvaraju 等人在 2017 年提出的，是一种基于激活的归因方法，它利用了通过卷积层学习到的特征。Grad-CAM
    查看 CNN 中最终卷积层学习的特征图，并通过计算输出相对于特征图中像素的梯度来获得该特征图的重要性。因为我们查看的是最终卷积层的特征图，所以 Grad-CAM
    生成的激活图是粗略的。Grad-CAM 技术也在 [http://mng.bz/8l8B](http://mng.bz/8l8B) 存储库中实现，并进行了以下调整，以便它可以应用于任何
    CNN 架构。首先，我们将定义一个名为 `CamExtractor` 的类，以获取最终卷积层的输出或特征图以及分类器或全连接层的输出：
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① A constructor for CamExtractor that takes in five input arguments
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ① CamExtractor 构造函数，接受五个输入参数
- en: ② The first argument sets the CNN model object.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ② 第一个参数设置 CNN 模型对象。
- en: ③ The second argument sets the start of the features layers in the CNN.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 第二个参数设置 CNN 中特征层的起始位置。
- en: ④ The third argument sets the start of the fully connected layers in the CNN.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 第三个参数设置 CNN 中全连接层的起始位置。
- en: ⑤ The fourth argument is the name of the fully connected layer.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 第四个参数是全连接层的名称。
- en: ⑥ The fifth argument is the name of the target or the final convolutional layer.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 第五个参数是目标层或最终卷积层的名称。
- en: ⑦ Initializes the gradients object as None
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 初始化梯度对象为 None
- en: ⑧ A method to save the gradients
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 保存梯度的方法
- en: ⑨ A method to do a forward pass and obtain the output of the final convolutional
    layer and to register a hook function to obtain the gradient of the output with
    respect to that layer
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 一种进行前向传播并获取最终卷积层的输出以及注册钩子函数以获取该层输出相对于该层的梯度的方法
- en: ⑩ Initializes the output of the final convolutional layer as None
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 将最终卷积层的输出初始化为 None
- en: ⑪ Iterates through all the modules in the features layers of the CNN
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 遍历CNN特征层的所有模块
- en: ⑫ Breaks once the name of the module matches the name of the fully connected
    layer
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 一旦模块名称与全连接层名称匹配，则中断
- en: ⑬ Obtains the output of the module using the input from the previous layer
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 使用来自前一层的输入获取模块的输出
- en: ⑭ If the module name matches the name of the final convolutional layer, registers
    a hook to obtain the gradient of the output with respect to this layer during
    backpropagation
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 如果模块名称与最终卷积层名称匹配，则在反向传播期间注册钩子以获取相对于此层的输出梯度
- en: ⑮ Returns the feature map for the final convolutional layer and the input to
    the fully connected layers
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 返回最终卷积层的特征图和全连接层的输入
- en: ⑯ A method to perform a forward pass on the model
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 在模型上执行前向传递的方法
- en: ⑰ Obtains the feature map for the final convolutional layer and the input to
    the fully connected layer
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 获取最终卷积层的特征图和全连接层的输入
- en: ⑱ Flattens the input to the fully connected layer
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ⑱ 将输入展平到全连接层
- en: ⑲ Passes through the fully connected layer to obtain the classifier output
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ⑲ 通过全连接层传递以获取分类器输出
- en: ⑳ Returns the feature map for the final convolutional layer and the output of
    the classifier
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ⑳ 返回最终卷积层的特征图和分类器的输出
- en: 'The `CamExtractor` class shown in this code snipper takes in the following
    five input arguments:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段中显示的`CamExtractor`类接受以下五个输入参数：
- en: '`model`—The CNN model used for image classification'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`—用于图像分类的CNN模型'
- en: '`features`—The layer that indicates the start of the feature layers in the
    CNN'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features`—在CNN中表示特征层开始的层'
- en: '`fc`—The layer that indicates the start of the fully connected layer in the
    CNN use for classification'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc`—在用于分类的CNN中表示全连接层开始的层'
- en: '`fc_layer`—The name of the fully connected layer in the model object'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc_layer`—模型对象中全连接层的名称'
- en: '`target_layer`—The name of the final convolutional layer in the model object'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_layer`—模型对象中最终卷积层的名称'
- en: 'As we saw for vanilla backpropagation and guided backpropagation, the model
    object is called `model` and the layer that indicates the start of the feature
    layers is the same object. The layer that indicates the start of the fully connected
    layer in the `model` object is `model.fc`. The name of the fully connected layer
    in the model object is `fc`, and the name of the final convolutional layer in
    the model is `layer4`. We now define the `GradCam` class to produce the class
    activation map, as shown here:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在vanilla反向传播和引导反向传播中看到的，模型对象被命名为`model`，表示特征层开始的层是同一个对象。`model`对象中表示全连接层开始的层是`model.fc`。模型对象中全连接层的名称是`fc`，模型中最终卷积层的名称是`layer4`。我们现在定义`GradCam`类来生成类激活图，如下所示：
- en: '[PRE23]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① A constructor for GradCam takes the same five arguments as CamExtractor.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ① GradCam的构造函数接受与CamExtractor相同的五个参数
- en: ② Sets the appropriate objects
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置适当的对象
- en: ③ Sets the model in evaluation mode
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将模型设置为评估模式
- en: ④ Initializes the CamExtractor object
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化CamExtractor对象
- en: ⑤ A function to generate the CAM given an input image and target class
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 给定输入图像和目标类别生成CAM的函数
- en: ⑥ Uses the extractor to get the feature map from the final convolution layer
    and the output of the classifier
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用提取器从最终卷积层和分类器的输出中获取特征图
- en: ⑦ If the target class is not specified, obtains the output class based on the
    model prediction
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 如果未指定目标类别，则根据模型预测获取输出类别
- en: ⑧ Converts the target class into a one-hot-encoded tensor
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 将目标类别转换为one-hot编码的张量
- en: ⑨ Resets the gradients before backpropagation
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 在反向传播之前重置梯度
- en: ⑩ Performs backpropagation
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 执行反向传播
- en: ⑪ Obtains the gradients of the output class with respect to the feature map
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 获取输出类别相对于特征图的梯度
- en: ⑫ Obtains the CAM by weighting the feature map by the gradients
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 通过对特征图加权梯度来获取CAM
- en: ⑬ Clips the CAM and removes negative values
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 剪裁CAM并移除负值
- en: ⑭ Normalizes the CAM between 0 and 1
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 在0和1之间归一化CAM
- en: ⑮ Scales the CAM to 0–255 to visualize as a grayscale image
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 将CAM缩放到0-255以可视化为灰度图像
- en: ⑯ Zooms the CAM and interpolates to the same dimension as the input image
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 放大CAM并插值到与输入图像相同的维度
- en: ⑰ Returns the CAM
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 返回CAM
- en: 'You can initialize the Grad-CAM object as follows. As an exercise, I encourage
    you to create activation maps for the two examples we used earlier. The solution
    to this exercise can be found in the GitHub repository associated with this book
    ([http://mng.bz/KBdZ](http://mng.bz/KBdZ)):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下方式初始化 Grad-CAM 对象。作为一个练习，我鼓励您为之前使用的两个示例创建激活图。这个练习的解决方案可以在与本书相关的 GitHub
    仓库中找到 ([http://mng.bz/KBdZ](http://mng.bz/KBdZ))：
- en: '[PRE24]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Figure 5.21 contains the resulting Grad-CAM activation maps. We can see from
    the figure that the activation map shows the importance of the feature map from
    the final convolutional layer and is quite coarse-grained. The regions in gray
    or white show highly important regions for the model prediction.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 包含了生成的 Grad-CAM 激活图。从图中我们可以看到，激活图显示了最终卷积层特征图的重要性，并且相当粗糙。灰色或白色的区域显示了模型预测中高度重要的区域。
- en: '![](../Images/CH05_F21_Thampi.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F21_Thampi.png)'
- en: Figure 5.21 An activation map using Grad-CAM
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 使用 Grad-CAM 的激活图
- en: 'To get a more fine-grained activation map, we can use the guided Grad-CAM technique.
    The guided Grad-CAM technique, proposed in 2017 by the same authors as Grad-CAM,
    essentially combines the Grad-CAM and guided backpropagation techniques. The final
    activation map produced by guided Grad-CAM is an element-wise dot product of the
    activation map produced by Grad-CAM and the saliency map produced by guided backpropagation.
    This is implemented in the following function:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更精细的激活图，我们可以使用引导 Grad-CAM 技术。引导 Grad-CAM 技术是由与 Grad-CAM 同样的作者于 2017 年提出的，本质上结合了
    Grad-CAM 和引导反向传播技术。引导 Grad-CAM 生成的最终激活图是 Grad-CAM 生成的激活图和引导反向传播生成的显著性图的逐元素点积。这在上面的函数中实现：
- en: '[PRE25]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This function takes the mask in grayscale obtained from Grad-CAM and guided
    backpropagation and returns an element-wise product of them. Figure 5.22 shows
    the activation maps produced by guided Grad-CAM for the two examples of interest.
    We can see that visualization is a lot cleaner than guided backpropagation and
    highlights areas that are consistent with IDC-negative and -positive patches.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受从 Grad-CAM 和引导反向传播获得的灰度掩码，并返回它们的逐元素乘积。图 5.22 展示了引导 Grad-CAM 为两个感兴趣的示例生成的激活图。我们可以看到，可视化比引导反向传播更干净，并突出了与
    IDC 负面和正面补丁一致的区域。
- en: '![](../Images/CH05_F22_Thampi.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F22_Thampi.png)'
- en: Figure 5.22 An activation map using guided Grad-CAM
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 使用引导 Grad-CAM 的激活图
- en: 5.9 Which attribution method should I use?
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 我应该使用哪种归因方法？
- en: Now that we have all these techniques in our arsenal, which techniques should
    we apply? In other words, which techniques produce reliable interpretations? From
    visually inspecting the interpretations using a couple of examples, we found all
    the saliency techniques to provide a measure of importance for pixels. By visually
    assessing them, we found those importance measures to be reasonable. Relying solely
    on visual or qualitative assessment, however, can be misleading.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了这些技术，我们应该应用哪些技术呢？换句话说，哪些技术能产生可靠的解释？通过使用几个示例对解释进行视觉检查，我们发现所有的显著性技术都为像素提供了一种重要性度量。通过视觉评估，我们发现这些重要性度量是合理的。然而，仅依靠视觉或定性评估可能会产生误导。
- en: 'A paper that was published in 2018 by Julius Adebayo, et al., available at
    [http://mng.bz/Exjj](http://mng.bz/Exjj), did a thorough quantitative assessment
    of the saliency methods discussed in this chapter. The following two broad classes
    of tests were done:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇由 Julius Adebayo 等人于 2018 年发表的论文，可在 [http://mng.bz/Exjj](http://mng.bz/Exjj)
    获取，对本章讨论的显著性方法进行了全面的定量评估。以下进行了两种广泛的测试：
- en: '*Model parameter randomization test*—Checks whether any effect on the saliency
    map occurs by randomizing the weights of the model, from which we would expect
    the model to make random or garbage predictions. If the output of the saliency
    method is the same for the trained model and the random model, then we can say
    that the saliency map is insensitive to the model parameters. The saliency map
    would, therefore, not be reliable for debugging the model.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型参数随机化测试*—检查通过随机化模型的权重是否会对显著性图产生影响，我们预期模型会做出随机或垃圾预测。如果显著性方法在训练模型和随机模型上的输出相同，那么我们可以说显著性图对模型参数不敏感。因此，显著性图对调试模型来说可能不可靠。'
- en: '*Data randomization test*—Checks whether any effect on the saliency map occurs
    by randomizing the labels in the training data. When we train the same model architecture
    on a copy of the training dataset where the target labels are randomized, we would
    expect the output of the saliency method to also be sensitive to it. If by randomizing
    the labels, there is no effect on the saliency map, then the method is not dependent
    on the input images and labels that exist in the original training set. The saliency
    map is, therefore, not reliable for understanding input-output relationships.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据随机化测试*—检查通过在训练数据中随机化标签是否会对显著性图产生影响。当我们在一个目标标签被随机化的训练数据集副本上训练相同的模型架构时，我们预期显著性方法的结果也会对其敏感。如果通过随机化标签，显著性图没有受到影响，那么该方法不依赖于原始训练集中存在的输入图像和标签。因此，显著性图对于理解输入输出关系不可靠。'
- en: The paper provides a couple of sanity checks that can be used in practice to
    determine how reliable the output of the saliency method is. The results of the
    sanity checks are summarized in table 5.2.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提供了一些可以在实践中使用的合理性检查，以确定显著性方法输出的可靠性。合理性检查的结果总结在表5.2中。
- en: Table 5.2 Results of the sanity checks done on visual attribution methods
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2对视觉归因方法进行的合理性检查结果
- en: '| Attribution method | Model parameter randomization test | Data randomization
    test |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 归因方法 | 模型参数随机化测试 | 数据随机化测试 |'
- en: '| Vanilla backpropagation | PASS | PASS |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 纯量反向传播 | 通过 | 通过 |'
- en: '| Guided backpropagation | FAIL | FAIL |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 指导反向传播 | 失败 | 失败 |'
- en: '| Integrated gradients | FAIL | FAIL |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 集成梯度 | 失败 | 失败 |'
- en: '| SmoothGrad | FAIL | PASS |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| SmoothGrad | 失败 | 通过 |'
- en: '| Grad-CAM | PASS | PASS |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Grad-CAM | 通过 | 通过 |'
- en: '| Guided Grad-CAM | FAIL | FAIL |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 指导Grad-CAM | 失败 | 失败 |'
- en: We can see that the methods that pass both tests are vanilla backpropagation
    and Grad-CAM. The saliency and activation maps produced by them are sensitive
    to the model and the data-generating process. They, therefore, can be used to
    reliably debug the model and to understand the relationship between the input
    images and the target label. The other techniques provide compelling images that
    explain the model prediction and seem acceptable from qualitative assessment.
    They are, however, invariant to model and label randomization and are, therefore,
    not adequate for model debugging and for understanding input-output relationships.
    The important message from these sanity checks is to be aware of confirmation
    bias. It is not enough for the interpretation to make sense qualitatively; it
    must also pass the sanity checks to be able to understand the model and input-output
    relationships better. The two tests proposed by the paper can be applied in practice
    to other interpretability techniques as well.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过两个测试的方法是纯量反向传播和Grad-CAM。它们产生的显著性和激活图对模型和数据生成过程敏感。因此，它们可以用来可靠地调试模型，并理解输入图像和目标标签之间的关系。其他技术提供了令人信服的图像来解释模型预测，并且从定性评估来看似乎是可接受的。然而，它们对模型和标签随机化是不变的，因此不足以用于模型调试和理解输入输出关系。这些合理性检查的重要信息是要意识到确认偏差。仅仅从定性上使解释有意义是不够的；它还必须通过合理性检查，才能更好地理解模型和输入输出关系。论文中提出的两个测试可以应用于实践中其他可解释技术的检查。
- en: In the next chapter, we will learn how to dissect the network further and understand
    what high-level concepts are learned by the neural network. Rather than look at
    pixel-level importance, we will learn about techniques that give us concept-level
    importance. These techniques have been shown to be sensitive to the model and
    the data-generating process and, therefore, pass the sanity checks discussed in
    this section.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何进一步剖析网络，并理解神经网络学习的高级概念。我们将不会关注像素级的重要性，而是学习那些提供概念级重要性的技术。这些技术已被证明对模型和数据生成过程敏感，因此通过本节讨论的合理性检查。
- en: Summary
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: A convolutional neural network (CNN) is a neural network architecture commonly
    used for visual tasks such as image classification, object detection, and image
    segmentation.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是一种常用于视觉任务（如图像分类、目标检测和图像分割）的神经网络架构。
- en: Fully connected DNNs do not capture pixel dependencies in an image very well
    and, therefore, cannot be trained to understand features in an image such as edges,
    colors, and gradients. CNNs, on the other hand, capture pixel dependencies or
    spatial dependencies in an image very well. We can also train the CNN architecture
    more efficiently to fit the input dataset as we reuse weights in the network.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接的深度神经网络（DNN）在图像中捕捉像素依赖性方面并不很好，因此不能训练以理解图像中的边缘、颜色和梯度等特征。另一方面，CNN在图像中很好地捕捉像素依赖性或空间依赖性。我们还可以更有效地训练CNN架构以适应输入数据集，因为我们可以在网络中重用权重。
- en: A CNN architecture typically consists of a sequence of convolution and pooling
    layers, called the feature learning layers. The objective of these layers is to
    extract hierarchical features from the input image. Following the feature learning
    convolutional layers are layers of neurons or units that are fully connected,
    and the purpose of these fully connected layers is to perform classification.
    The inputs to the fully connected layer are the high-level features learned by
    the convolution and pooling layers, and the output is a probability measure for
    the classification task.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN架构通常由一系列卷积和池化层组成，称为特征学习层。这些层的目的是从输入图像中提取层次特征。在特征学习卷积层之后是神经元或单元层，它们是完全连接的，这些完全连接层的目的是执行分类。完全连接层的输入是由卷积和池化层学习的高级特征，输出是对分类任务的概率度量。
- en: Various state-of-the-art CNN architectures, such as AlexNet, VGG, ResNet, Inception,
    and ResNeXT are implemented in popular deep learning libraries such as PyTorch
    and Keras. In PyTorch, you can initialize these architectures using the torchvision
    package.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种最先进的卷积神经网络（CNN）架构，如AlexNet、VGG、ResNet、Inception和ResNeXT，已在流行的深度学习库中实现，例如PyTorch和Keras。在PyTorch中，你可以使用torchvision包来初始化这些架构。
- en: Within a CNN, as an image goes through millions of complex operations, it becomes
    incredibly difficult to understand how the model arrived at the final prediction.
    This is what makes CNNs black boxes.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CNN中，随着图像经过数百万个复杂的操作，理解模型如何到达最终预测变得极其困难。这就是CNN成为黑盒的原因。
- en: We can use visual attribution methods to interpret CNNs. These methods are used
    to attribute importance to parts of an image that influence the prediction made
    by the CNN.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用视觉归因方法来解释CNN。这些方法用于将重要性归因于影响CNN预测的图像部分。
- en: 'Three broad categories of visual attribution methods are available: perturbations,
    gradients, and activations.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的视觉归因方法分为三大类：扰动、梯度和激活。
- en: The idea behind perturbation-based methods is to perturb the input and probe
    its effects on the predictions made by the CNN. Techniques such as LIME and SHAP
    are perturbation-based methods. These techniques, however, are computationally
    inefficient because each perturbation requires us to perform a forward pass on
    the complex CNN model. These techniques can also underestimate the importance
    of features.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于扰动的方法的理念是扰动输入并探测其对CNN预测的影响。LIME和SHAP等技术是扰动方法。然而，这些技术计算效率低下，因为每次扰动都需要我们对复杂的CNN模型执行前向传递。这些技术还可能低估特征的重要性。
- en: We can use gradient-based methods to visualize the gradient of the input image
    with respect to the target class. Pixels with large gradient measures are considered
    most important, or salient, for the model. Gradient-based methods are sometimes
    also called backpropagation methods—the backpropagation algorithm is used to determine
    feature importance and saliency maps because a map of salient or important features
    is obtained. Popular gradient-based methods are vanilla backpropagation, guided
    backpropagation, integrated gradients, and SmoothGrad.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用基于梯度的方法来可视化输入图像相对于目标类的梯度。具有大梯度度量的像素被认为对模型来说是最重要的，或称为显著的。基于梯度的方法有时也称为反向传播方法——反向传播算法用于确定特征重要性和显著性图，因为可以获得显著或重要特征的映射。流行的基于梯度的方法包括标准反向传播、引导反向传播、集成梯度和SmoothGrad。
- en: Activation-based methods look at the feature maps or activations in the final
    convolutional layer and weight them based on the gradient of the target class
    with respect to those feature maps. The weights of the feature maps act as a proxy
    for the importance of the input features. This technique is called gradient-weighted
    Class Activation Mapping (Grad-CAM).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于激活的方法查看最终卷积层中的特征图或激活，并根据目标类别对这些特征图的梯度进行加权。特征图的权重作为输入特征重要性的代理。这种技术被称为梯度加权类激活映射（Grad-CAM）。
- en: Grad-CAM provides a coarse-grained activation map. To obtain more fine-grained
    activation maps, we can combine Grad-CAM and guided backpropagation—this technique
    is called guided Grad-CAM.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grad-CAM 提供了一个粗粒度的激活图。为了获得更细粒度的激活图，我们可以结合 Grad-CAM 和 guided backpropagation——这种技术被称为
    guided Grad-CAM。
- en: The visual attribution methods that pass the model parameter randomization and
    data randomization tests are vanilla backpropagation and Grad-CAM. The saliency
    and activation maps produced by them are, therefore, more reliable for debugging
    the model and understanding the input-output relationships better.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过模型参数随机化和数据随机化测试的视觉归因方法包括 vanilla backpropagation 和 Grad-CAM。因此，它们产生的显著性和激活图在调试模型和更好地理解输入输出关系方面更加可靠。
