- en: 7 Making useful models with ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用机器学习创建有用模型
- en: 'This chapter covers:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖：
- en: Transforming data for processing
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行转换以进行处理
- en: Injecting information with feature engineering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征工程注入信息
- en: Designing the model’s structure
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计模型的架构
- en: Running the model development process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行模型开发过程
- en: Deciding which models to retain and which to reject
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定保留哪些模型和拒绝哪些模型
- en: Sprint 2 is the rollercoaster ride that we’ve been working toward; finally,
    we’re going to do some ML! The success or failure of this phase of the project
    is the pivot point for everything else. Although we created the conditions for
    success with the work in presales, sprint 0, and sprint 1, all this work will
    be for nothing if we can’t implement useful models. Creating a model is easy if
    you’ve done the hard part of getting and preparing the data. A simple model can
    involve writing a single line of code or pressing a button on a user interface.
    However, creating a useful model is much harder.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第二轮冲刺是我们一直努力的方向；终于，我们将要进行一些机器学习！这个阶段项目的成功或失败是其他一切的关键点。尽管我们在预售、冲刺0和冲刺1的工作中创造了成功的条件，但如果我们不能实施有用的模型，所有这些工作都将付诸东流。如果你已经完成了获取和准备数据的艰难部分，创建一个模型是很容易的。一个简单的模型可能只需要写一行代码或在用户界面上按一个按钮。然而，创建一个有用的模型要困难得多。
- en: 'What makes a model created with ML useful or useless? The traditions of ML
    say that a useful model is one that generalizes well: the model can effectively
    deal with data that it was not trained on and that it copes with unseen circumstances.
    But, in fact, there are a bunch of other traits that make models useful, or if
    they don’t have them, useless.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得使用机器学习创建的模型有用或无用？机器学习的传统观点认为，一个有用的模型是能够很好地泛化的模型：该模型可以有效地处理它未训练过的数据，并应对未见过的情况。但实际上，还有许多其他特性使得模型有用，或者如果没有这些特性，模型就无用。
- en: Table 7.1 lists the qualities of a useful model contrasted with a useless model.
    To deliver a useful model, the team needs to generate many features from the data,
    create a large number of candidate models, properly evaluate them, select the
    one that’s going to be the most effective, and explain all this to the people
    who are impacted by the model or to the regulators and the auditors. This process
    must be professionally executed and managed. Furthermore, without a rigorous evaluation
    process, your model may prove to be brittle in production, or it may be that you
    face difficulty in convincing your stakeholders that you have made responsible
    and appropriate choices about the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1列出了有用模型与无用模型的特性对比。为了交付一个有用的模型，团队需要从数据中生成许多特征，创建大量候选模型，正确评估它们，选择最有效的模型，并向受模型影响的人、监管者或审计员解释所有这些。这个过程必须得到专业执行和管理。此外，如果没有严格的评估过程，你的模型可能在生产中证明是脆弱的，或者你可能面临困难，无法说服你的利益相关者你对模型的选择是负责任和适当的。
- en: The three drivers of the entries in table 7.1 are the need to do modelling work
    that is aligned with the needs of all the stakeholders in the system, to build
    on solid foundations, and to ensure that the result is solid. A solid result is
    not just a great model, but it’s a model that others can verify as being great.
    Now, the team needs to generate the assets that allow that verification to take
    place.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1条目背后的三个驱动因素是：进行与系统内所有利益相关者需求一致的建模工作，建立在坚实的基础之上，并确保结果是稳固的。一个稳固的结果不仅仅是一个优秀的模型，而且是一个其他人可以验证为优秀的模型。现在，团队需要生成允许这种验证发生的资产。
- en: Table 7.1 What makes a model useful or useless?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 什么使得模型有用或无用？
- en: '| Useful | Useless | Commentary |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 有用 | 无用 | 评论 |'
- en: '| Created from a well-understood and well-prepared data infrastructure. | Developed
    from ad hoc data resources prepared with little control or thought about quality.
    | If you don’t have a process that ensures quality, you won’t know if the data
    you are working with is good. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 从一个理解透彻且准备充分的数据库基础设施中创建。 | 从缺乏控制或对质量考虑不周的临时数据资源中开发。 | 如果你没有确保质量的过程，你将不知道你正在处理的数据是否良好。
    |'
- en: '| Created in response to a well-understood requirement and meets that requirement.
    | Created because it can be; unclear if it meets any requirement. | If you don’t
    have established sponsorship for the case that the model supports, then the work
    required to get sponsorship may be prohibitive. You probably don’t have the level
    of insight required to second-guess a stakeholder’s view on business value and
    then prove that you are right. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 响应已充分理解的需求而创建并满足该需求。 | 因为可以创建而创建；不清楚是否满足任何需求。 | 如果你没有为模型支持的情况建立明确的赞助，那么获得赞助所需的工作可能是难以承受的。你可能没有足够的洞察力来质疑利益相关者对商业价值的看法，并证明你是正确的。|'
- en: '| Created with regards to ethical considerations. | Created without thought
    about ethical impact or regard for the people impacted. | Models with ethical
    issues are eventually killed by the business. The further down the road to production,
    the more damage they do. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 考虑到伦理因素而创建。 | 没有考虑伦理影响或对受影响的人的尊重而创建。 | 存在伦理问题的模型最终会被业务所淘汰。越接近生产阶段，它们造成的损害就越大。|'
- en: '| High-quality feature engineering creates strong performance. | Low-quality
    or no feature engineering. | Poor feature engineering means that model performance
    in tests is likely to be disconnected with the user or real-world expectations
    and requirements. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 高质量的特征工程创造强大的性能。 | 低质量或没有特征工程。 | 低质量或没有特征工程意味着模型在测试中的性能可能与用户或现实世界的期望和要求脱节。|'
- en: '| Purposefully designed. | Arbitrary design. | If the design is arbitrary,
    it’s likely the model will be brittle. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 有意设计。 | 任意设计。 | 如果设计是任意的，那么模型很可能是脆弱的。|'
- en: '| Thoroughly evaluated. | Not well evaluated. | When the model is poorly evaluated,
    no one knows if it works or not. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 严格评估。 | 评估不佳。 | 当模型评估不佳时，没有人知道它是否有效。|'
- en: '| Model selection process is purposeful and transparent. | Model selection
    is arbitrary. | With an arbitrary selection, who knows if you got the right one
    or why you picked it? |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 模型选择过程是有目的和透明的。 | 模型选择是任意的。 | 随意的选择，谁知道你是否得到了正确的结果，以及你为什么选择它呢？|'
- en: '| Defined modelling process used. | Modelling process is arbitrary. | A defined
    process for modeling allows for accountability with respect to model choices.
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 定义了使用的建模过程。 | 建模过程是任意的。 | 一个明确的建模过程允许对模型选择进行问责。|'
- en: '| Well documented. | No documentation. | Because documentation promotes transparency,
    it also enables problems to be identified and fixed. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 文档齐全。 | 没有文档。 | 因为文档促进了透明度，它还使得问题能够被识别和修复。|'
- en: 7.1 Sprint 2 backlog
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 Sprint 2 待办事项
- en: In sprint 2, the team implements systematic and professional modelling and evaluation
    processes. By using an organized and documented approach, the team avoids some
    of the pitfalls and common problems that produce poor quality models. At least,
    that’s what is to be hoped! Let’s look at those tasks for sprint 2 before we dive
    into the details.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sprint 2 中，团队实施系统和专业的建模和评估流程。通过使用有组织和记录的方法，团队避免了产生低质量模型的一些陷阱和常见问题。至少，这是我们所希望的！在我们深入了解细节之前，让我们先看看
    Sprint 2 的那些任务。
- en: 'Table 7.2 Backlog for Sprint 2: modelling and evaluation'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2 Sprint 2 待办事项：建模和评估
- en: '| Task # | Item |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 任务编号 | 项目 |'
- en: '| --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| S2.1 | Create a feature engineering plan, then share and review it with team.Implement
    a feature-engineering pipeline.Design and add data augmentation. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| S2.1 | 创建特征工程计划，然后与团队分享并审查它。实现特征工程管道。设计和添加数据增强。|'
- en: '| S2.2 | Create the model’s design and document it.Consider the forces acting
    on the model’s design.Decide on the decomposition of tasks to create your overall
    design.Choose component parts based on the required inductive biases given your
    data.Develop composition schemes that fuse your model’s output. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| S2.2 | 创建模型的设计并对其进行文档记录。考虑作用于模型设计的力量。决定任务的分解以创建整体设计。根据数据中给出的所需归纳偏差选择组件部分。开发融合模型输出的组合方案。|'
- en: '| S2.3 | Agree on the modeling process and set it up:  ▪ Commission and use
    an experiment tracker.  ▪ Commission and use a model repository.  ▪ Identify and
    reject obviously poor models. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| S2.3 | 就建模过程达成一致并建立：  ▪ 委派并使用实验跟踪器。  ▪ 委派并使用模型存储库。  ▪ 识别并拒绝明显较差的模型。|'
- en: '| S2.4 | Implement and commission the test environment. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| S2.4 | 实施并委派测试环境。|'
- en: '| S2.5 | Develop a set of tests to determine the model’s functional performance.Determine
    the measurements to use over the test scenarios. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| S2.5 | 开发一套测试来确定模型的性能功能。确定在测试场景中要使用的度量标准。|'
- en: '| S2.6 | Test nonfunctional features. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| S2.6 | 测试非功能性特征。|'
- en: '| S2.7 | Use the evaluation data to determine the model to use.Employ an explicit
    mechanism to use the model test/evaluation data to choose the model that used
    in production.Account for how component models will be combined in your design.Account
    for nonfunctional requirements.Account for the qualitative aspects of the model.
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| S2.7 | 使用评估数据来确定要使用的模型。采用显式机制使用模型测试/评估数据来选择生产中使用的模型。考虑如何在设计中组合组件模型。考虑非功能性需求。考虑模型的定性方面。|'
- en: '| S2.8 | Write a model delivery report. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| S2.8 | 编写模型交付报告。|'
- en: '| S2.9 | Determine and document model selection. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| S2.9 | 确定并记录模型选择。|'
- en: '| S2.10 | Review and obtain customer sign-off on model selection. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| S2.10 | 审查并获得客户对模型选择的批准。|'
- en: 7.2 Feature engineering and data augmentation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 特征工程和数据增强
- en: 'Feature engineering ticket: S2.1'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程工单：S2.1
- en: Create a feature engineering plan, then share and review it with team.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定一个特征工程计划，然后与团队分享并审查。
- en: Implement a feature engineering pipeline.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施一个特征工程流程。
- en: Design and add data augmentation.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计并添加数据增强。
- en: Feature engineering and selection is a core part of the ML pipeline. The raw
    data that’s assembled might make little sense to an ML algorithm without some
    preprocessing to create consistent, useful, and informative features. Before modelling
    starts, the data set needs to be enriched and transformed to provide the appropriate
    features for the algorithms to consume.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程和选择是机器学习流程的核心部分。未经某些预处理以创建一致、有用和有信息量的特征，原始数据可能对机器学习算法来说意义不大。在建模开始之前，数据集需要得到丰富和转换，以便为算法提供适当的特征。
- en: 'Systems of feature selection and engineering are available for use. For example,
    in their book *Feature Engineering and Selection: A Practical Approach for Predictive
    Models*, Kuhn and Johnson [7] provide an analytical perspective for identifying,
    creating, and selecting features. In this framework, they use mathematical and
    technical considerations to restructure data, making it more amenable for consumption
    by ML algorithms. A systematic approach to feature engineering is helpful in identifying
    and solving technical issues with data. In particular:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可用系统化的特征选择和工程方法。例如，在他们的书籍《特征工程与选择：预测模型的实用方法》中，Kuhn 和 Johnson [7] 提供了识别、创建和选择特征的解析视角。在这个框架中，他们使用数学和技术考虑因素来重构数据，使其更适合机器学习算法的消耗。系统化的特征工程方法有助于识别和解决数据的技术问题。特别是：
- en: Identifying and removing biases created by the same information across multiple
    fields in the data set.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和消除数据集中多个字段相同信息产生的偏差。
- en: Resolving biases created by skews of distribution and differences of scale.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决由分布偏差和尺度差异产生的偏差。
- en: Handling hierarchical data and the information distributed within the hierarchy.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理层次数据及其在层次结构中的信息分布。
- en: As well as resolving technical problems with the way the data is presented to
    the algorithms, we can use feature engineering to encode human insight and common
    sense for the machine. For example, it’s common sense (for folks raised to understand
    the Gregorian calendar) that December is followed by January, and it seems reasonable
    to assume that December is represented by the number 12 and January by the number
    1\. But given the information that there are three days, 12/30, 12/31, and 1/1,
    how can a machine be expected to deduce that the first day of January follows
    the last day of December?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决数据呈现给算法的方式中的技术问题外，我们还可以使用特征工程来编码机器的洞察力和常识。例如，对于在格里高利历中成长的人来说，常识是12月之后是1月，似乎有理由假设12月由数字12表示，1月由数字1表示。但考虑到有三天，12/30、12/31和1/1的信息，机器如何能够推断出1月的第一天紧随12月的最后一天？
- en: Another example deals with data that includes an orientation or a direction,
    often represented as a scalar value between 0’ and 360’ or sometimes as a compass
    direction or relationship between different points. If we treat this data as a
    linear quantity, then items with a bearing of 359’ and 1’ appear at the opposite
    ends of the scale. In reality, they are close, and both are almost exactly due
    north (0’± 1’). We’ll get better results if we reframe the data to account for
    this circularity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子处理包含方向或方向的数据，通常表示为0°到360°之间的标量值，有时表示为罗盘方向或不同点之间的关系。如果我们将这些数据视为线性量，那么方位为359°和1°的项目将出现在刻度尺的两端。实际上，它们很接近，两者几乎正好在正北（0°±1°）。如果我们重新构架数据以考虑这种循环性，我们会得到更好的结果。
- en: Feature engineering is the process of transforming the data in such a way that
    it makes sense to an ML algorithm (for example, the information that 1/1 is preceded
    by 12/31 and that 359’ is next to 0’ is included as knowledge for the machine).
    Instead of embedding knowledge as some sort of extra reasoning rule, a feature
    engineer rewrites the data so that it’s part of the information that the ML system
    works with.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是将数据转换成对机器学习算法（例如，1/1在12/31之前，359°在0°旁边的信息作为机器的知识）有意义的过程。特征工程师不是将知识作为某种额外的推理规则嵌入，而是重新编写数据，使其成为机器学习系统处理的信息的一部分。
- en: Given the three days, 12/30, 12/31, and 1/1, we can solve that irregularity
    by representing the date as a distance from a point. If we take Midsummer’s Day
    and then New Year’s Day (1/1), New Year’s Day represents the value 182 (often),
    the last day of the year (12/31) is 181, and the second of January (1/2) is 181
    as well. The point here is that all these examples are similar in value, so this
    transform makes more sense for the things that are similar.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定三天，12/30，12/31和1/1，我们可以通过将日期表示为从某一点的距离来解决这个问题的不规则性。如果我们以仲夏日和新年（1/1）为基准，新年（1/1）代表值为182（通常是），年底的最后一天（12/31）是181，1月2日（1/2）也是181。这里的要点是，所有这些例子在数值上都很相似，因此这种转换对于相似的事物更有意义。
- en: We can use the smart building example to show how rewriting orientations can
    be useful. As we saw in chapter 4, the building sensors record the temperature
    on days 1–366, where day 366 accounts for a leap year. Figure 7.1 shows the mean
    temperature per day for each day of the year.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用智能建筑示例来展示如何通过重新编写方向来提高其有用性。正如我们在第4章中看到的，建筑传感器记录了1-366天的温度，其中第366天考虑了闰年。图7.1显示了每年每一天的平均温度。
- en: '![](../Images/07-01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-01.png)'
- en: Figure 7.1 Temperatures plotted by day of the year (data for Suffolk Wattlesham
    weather station, 2019, from UK Government).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1展示了按一年中的某一天绘制的温度（数据来源于2019年萨福克郡沃特什姆气象站，英国政府）。
- en: 'Figure 7.2 shows two new features: one giving the distance in days of a candidate
    day from Midsummer’s Day and another with the mean temperature for that day divided
    by the distance from Midsummer’s Day and normalized from 0.0 (Midsummer’s Day)
    to 1.0 (Midwinter’s Day) if there is a leap year. That’s 183 days, so the day
    after midsummer has a value of 0.0054.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2展示了两个新特性：一个提供候选日与仲夏日的天数距离，另一个提供该日的平均温度，并将其除以从仲夏日的距离，如果存在闰年，则从0.0（仲夏日）归一化到1.0（冬至日）。这是183天，所以仲夏后的那天值为0.0054。
- en: '![](../Images/07-02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-02.png)'
- en: Figure 7.2 Temperatures plotted for the distance from midsummer (scaled 0.0
    – 1.0)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2按仲夏距离绘制的温度（缩放0.0 – 1.0）
- en: We can also create features for some unstructured data sources by using pretrained
    foundation models. In chapter 4, we used a foundation model to explore the unstructured
    data in the text of Shakespeare’s plays. We can use the same approach to create
    features from unstructured data, which we can then feed into ML. This is a different
    tack from doing ML directly over some unstructured signals, where the regularities
    and patterns are created in the foundational model’s training. We then use these
    results to represent the data in a more consumable form for the modeling process.
    With Shakespeare’s plays, we used the all-MiniLM-L12-v2 sentence transformer to
    create a concept of novelty and difference. We then extracted a concept graph
    to relate similar themes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用预训练的基础模型来为一些非结构化数据源创建特征。在第 4 章中，我们使用基础模型来探索莎士比亚剧本中的非结构化数据。我们可以使用相同的方法从非结构化数据中创建特征，然后我们可以将这些特征输入到机器学习中。这与直接在某个非结构化信号上执行机器学习的方法不同，在基础模型的训练中，基础模型中创建了规律和模式。然后我们使用这些结果以更易于消费的形式表示数据，以便于建模过程。在莎士比亚的剧本中，我们使用了
    all-MiniLM-L12-v2 句子转换器来创建新颖性和差异的概念。然后我们提取了一个概念图来关联相似的主题。
- en: We can use similar approaches (and other more direct uses of foundation models)
    to create useful features and to fuse information from unstructured and structured
    data sources. For example, the Shakespeare example showed how to find similarity
    in a space of unstructured documents (the plays). Let’s take a new problem that
    involves textual data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类似的方法（以及其他更直接的基础模型应用）来创建有用的特征，并融合来自非结构化和结构化数据源的信息。例如，莎士比亚的例子展示了如何在非结构化文档的空间（即剧本）中找到相似性。让我们考虑一个新的涉及文本数据的问题。
- en: In this scenario, an email to accept a new customer is passed into an approval
    process. It’s discovered at a later stage that the email was sent to the wrong
    person and should have been handled differently, wasting a great deal of time
    and effort. Your team informs you that they think it’s likely the text classifier
    misbehaves on emails that are new or strangely framed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一封接受新客户的电子邮件被传递到审批流程中。在后来的阶段发现，这封电子邮件被发送给了错误的人，应该以不同的方式处理，浪费了大量的时间和精力。你的团队告诉你，他们认为文本分类器在处理新或奇怪框架的电子邮件时可能表现不佳。
- en: 'Corner cases of this kind can end up consuming a disproportionate amount of
    cost in the final process, and it might be so bad as to destroy the user confidence
    in the triage system. After the EDA process, the team thinks that a feature that
    determines outliers in the training set would be useful in terms of cleaning it
    and wants to build a high-quality classifier that determines appropriate topics.
    They construct a simple feature to provide a new-weird/not-new-normal signal for
    the system. As before, the data is indexed using embeddings from a foundational
    model. The email’s novelty is rated by getting the second closest email in the
    index (ignoring the identity match) and using the similarity distance to rate
    how close this email is to it as this pseudocode shows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的边缘情况最终可能会消耗最终过程中不成比例的成本，并且可能会糟糕到破坏用户对分级系统的信心。在 EDA 过程之后，团队认为确定训练集中异常值的功能在清理方面很有用，并希望构建一个高质量的分类器，以确定适当的话题。他们构建了一个简单的特征，为系统提供一个新的奇怪/不新的正常信号。与之前一样，数据使用基础模型的嵌入进行索引。通过获取索引中第二接近的电子邮件（忽略身份匹配）并使用相似度距离来评估这封电子邮件与它的接近程度，来对电子邮件的新颖性进行评级，如下面的伪代码所示：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ email.id is the index of this email.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ email.id 是这封电子邮件的索引。
- en: ❷ match.D is the distance that the index has returned.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ match.D 是索引返回的距离。
- en: The results array contains the distance to the closest match in the index for
    each email. If you find the standard deviation of this array and set a threshold
    of two times the standard deviation plus the mean, then you can filter out the
    strangest 5% of the emails for training. If you use this feature for the topic
    classifier itself, then you can create an “Other” label and the classifications
    for the remaining topics will be stronger.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数组包含每个电子邮件在索引中最近匹配的距离。如果你找到这个数组的标准差，并将阈值设置为两倍标准差加上平均值，那么你可以过滤掉训练中最奇怪的 5% 的电子邮件。如果你将这个特征用于主题分类器本身，那么你可以创建一个“其他”标签，剩余主题的分类将更加强烈。
- en: Developing high-quality features for a domain can be time-consuming and difficult.
    A lot of experience and domain insight is required to get good results, and this
    is the reason why feature stores are valuable assets. If a previous project develops
    a way of expressing the position of a sensor in a building that provides insight
    and information, then that’s a huge win. In addition, creating a consistent way
    of using data in an organization is helpful in terms of ensuring that the behavior
    of ML algorithms is consistent and well understood. If your project is a trailblazer,
    then hopefully the feature store that you develop and leave behind will be a lasting
    asset for the client.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为一个领域开发高质量的特征可能既耗时又困难。要获得良好的结果，需要大量的经验和领域洞察，这也是为什么特征存储库是有价值的资产。如果之前的某个项目开发了一种表达建筑物中传感器位置的方法，这提供了洞察和信息，那么这将是一个巨大的成功。此外，在组织中创建一致的数据使用方式，有助于确保机器学习算法的行为一致且易于理解。如果你的项目是开拓性的，那么你开发并留下的特征存储库可能将成为客户的一个持久资产。
- en: 7.2.1 Data augmentation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 数据增强
- en: 'There is an apocryphal story often told about an early ML system that was purportedly
    developed to identify tanks in photographs (the exact application it was developed
    for is always vague). The story’s punchline is that the ML system learned not
    to recognize tanks but, instead, to recognize snowy ground because the only shots
    with a tank in were taken in the snow. The story is almost certainly fiction,
    but there is a point to be made: if an ML algorithm only has a narrow data set
    to learn from, then what it can learn is restricted. Although it might be able
    to learn a robust classifier from the information it receives, it’s also likely
    that there are traps that it can fall into, where it learns to use coincidences
    in the data to make a classification. If there are different examples, however,
    it could be that the misleading coincidences become much rarer.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个关于早期机器学习系统的传说经常被讲述，据说这个系统是为了在照片中识别坦克而开发的（它被开发的确切应用总是模糊不清）。故事的笑点是，机器学习系统不是学会识别坦克，而是学会识别雪地，因为坦克的照片都是在雪中拍摄的。这个故事几乎肯定是一个虚构的故事，但有一个要点：如果一个机器学习算法只有有限的数据集来学习，那么它能学到的内容是有限的。尽管它可能能够从接收到的信息中学习到一个鲁棒的分类器，但它也很可能陷入陷阱，学会利用数据中的巧合来做出分类。然而，如果有不同的示例，那么误导性的巧合可能会变得非常罕见。
- en: To fix this issue, Shorten and Khoshgoftaar [13] developed the technique of
    *data augmentation*. Data augmentation is a process of using transformations and
    alterations to create extra examples for training. We can use this technique to
    make ML models more robust when only a narrow set of examples is available in
    the original training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Shorten 和 Khoshgoftaar [13] 开发了**数据增强**技术。数据增强是一个通过变换和修改来创建额外训练示例的过程。我们可以使用这种技术来使机器学习模型在原始训练集中只有少量示例可用时更加鲁棒。
- en: Figure 7.3 shows a set of image augmentations. The original image example (a)
    is rotated as per (b), (c), and (d), although we can generate more rotations.
    Also, extra objects can be added into the image as per (e) and (f), other sorts
    of noise (g), inversion (h), scaling (i), and repositioning within the frame or
    scene (j). In (k), the image is mirrored, providing another example of a cat for
    the ML algorithm.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 展示了一系列图像增强。原始图像示例（a）按照（b）、（c）和（d）进行了旋转，尽管我们可以生成更多的旋转。此外，还可以根据（e）和（f）向图像中添加额外的对象，以及其他类型的噪声（g）、反转（h）、缩放（i）和框架或场景内的重新定位（j）。在（k）中，图像被镜像，为机器学习算法提供了另一个猫的示例。
- en: '![](../Images/07-03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-03.png)'
- en: Figure 7.3 Augmenting an image of a cat with simple manipulations (Wikipedia
    commons, [https://commons.wikimedia.org/wiki/File:Black_and_White_Cat_Sketch.svg](https://commons.wikimedia.org/wiki/File:Black_and_White_Cat_Sketch.svg)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 通过简单操作增强猫的图像（维基百科公共领域，[https://commons.wikimedia.org/wiki/File:Black_and_White_Cat_Sketch.svg](https://commons.wikimedia.org/wiki/File:Black_and_White_Cat_Sketch.svg))。
- en: We use augmentation processes such as the one shown in figure 7.3 to make models
    that are more robust and general. The idea is that a model that focuses on a coincidental
    feature will be less likely to be chosen by the ML process if there is a wider
    variety of signals in the training data. We can also apply similar augmentation
    processes to other forms of unstructured data. For example, we may want to add
    spelling errors to text samples, replace non-stop words (words that aren’t regarded
    as stop-words/discardable by NLP parsers) with synonyms, or pass sentences through
    an auto-translate system to change its phrasing. For image recognition systems,
    we may want to use alternations for contrast and brightness, as well as many schemes
    that introduce noise and distortions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用如图7.3所示的增加过程来构建更稳健和通用的模型。其理念是，如果一个模型专注于偶然的特征，那么在训练数据中存在更广泛的信号时，它被机器学习过程选中的可能性会更小。我们还可以将类似的增加过程应用于其他形式的无结构数据。例如，我们可能想要向文本样本中添加拼写错误，用同义词替换非停用词（不被视为停用词/可由NLP解析器丢弃的词），或者通过自动翻译系统传递句子以改变其措辞。对于图像识别系统，我们可能想要使用对比度和亮度的交替，以及许多引入噪声和失真的方案。
- en: Whatever the need, we can apply new features and data augmentation approaches
    iteratively during the modelling process as the team’s investigations proceed
    and new information about the behavior of the algorithms comes to light. How that
    process of iterative modelling is managed is discussed in section 7.4\. Once the
    first set of features is implemented, the next step for the team is to create
    the model’s design.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 无论需要什么，我们都可以在建模过程中迭代地应用新特性和数据增强方法，随着团队的调查进行和新信息关于算法行为的出现。关于迭代建模过程如何管理将在第7.4节中讨论。一旦实现了第一组特征，团队的下一步就是创建模型的设计。
- en: 7.3 Model design
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 模型设计
- en: 'Model design ticket: S2.2'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型设计票据：S2.2
- en: Create the model’s design and document it
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型的设计并对其进行文档化
- en: Consider the forces acting on the model’s design.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑作用于模型设计的力量。
- en: Decide on the decomposition of tasks to create your overall design.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定任务的分解以创建你的整体设计。
- en: Choose component parts based on the required inductive biases given your data.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据你的数据所要求的归纳偏差选择组件部分。
- en: Develop composition schemes that fuse your model’s output.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发融合模型输出的组合方案。
- en: Before the models can be built by ML algorithms, they must be designed by the
    data scientists and ML engineers in the team. As an analogy, it’s worth remembering
    that rocket engines are designed to take advantage of the technology and fuel
    available to power them to achieve the results required. If your rocket engineering
    team can’t access exotic metals and high-quality fuel, their approach will have
    to be much more pragmatic and constrained than if they had lots of titanium and
    synthetic chemicals. Similarly, ML models are designed to deliver functionally
    and nonfunctionally, given the data that’s available and the production environment
    they will operate in.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习算法构建模型之前，它们必须由团队中的数据科学家和机器学习工程师设计。作为一个类比，值得记住的是，火箭发动机的设计是为了利用可用的技术和燃料来产生所需的结果。如果你的火箭工程团队能够访问到稀有金属和高品质燃料，他们的方法将比拥有大量钛和合成化学品的方法更加务实和受限。同样，机器学习模型的设计是为了在给定可用的数据和它们将运行的生成环境中，实现功能性和非功能性。
- en: 7.3.1 Design forces
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 设计力量
- en: 'In sprint 0 and sprint 1, you exposed and clarified the requirements and constraints
    on ML modelling for the business application you are developing. A range of forces
    should have been articulated in your user stories to act on the model’s design
    that’s going to underpin your team’s approach to providing a solution. Some examples
    of these forces are listed here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在冲刺0和冲刺1中，你明确并阐明了你正在开发的商业应用中机器学习建模的需求和约束。在你的用户故事中应该已经明确了一系列力量，这些力量将作用于支撑你团队提供解决方案的方法的模型设计。以下是一些这些力量的例子：
- en: '*Quantitative performance:* A few examples of qualitative performance measures
    include a good F1 score, precision and recall, or sensitivity and specificity.
    (Chapter 8 has a detailed discussion of these different performance measurements
    if this is the first time you’ve come across them.) It’s worth being careful with
    specific measures, but essentially, you can measure the quantitative performance
    of the classifier by how effectively it does its job.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定量性能:* 一些定性性能指标包括良好的F1分数、精确率和召回率，或者敏感性和特异性。（如果这是你第一次遇到这些指标，第8章对这些不同的性能测量有详细的讨论。）在特定指标上需要谨慎，但本质上，你可以通过分类器完成其工作的有效性来衡量其定量性能。'
- en: '*Explanation/transparency:* The classifier that gives the best numerical evaluation
    might not provide a sufficient explanation for its decisions or be transparent
    enough in its workings to allow its use in a particular context or application.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解释/透明度:* 给出最佳数值评估的分类器可能无法提供足够的解释来支持其决策，或者在其工作方式上不够透明，无法在特定上下文或应用中使用。'
- en: '*Latency:* The latency of the classifier should be appropriate to the application.
    For example, you may need to execute the classifier within a fraction of a second
    of the data becoming available for an interactive application. On the other hand,
    you can transcribe a movie’s speech to create subtitles in a batch job overnight.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟:* 分类器的延迟应适合应用。例如，对于交互式应用，你可能需要在数据可用后的一瞬间执行分类器。另一方面，你可以在夜间批量作业中转录电影的对白以创建字幕。'
- en: '*Cost:* If you need to execute a classifier millions of times, the cost of
    the infrastructure that you use to run it can become prohibitive.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本:* 如果你需要执行数百万次分类器，你用来运行它的基础设施的成本可能会变得难以承受。'
- en: '*Data privacy/security:* Inferences made from the classifier’s behavior might
    disclose secrets or confidential data.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私/安全性:* 从分类器行为中得出的推论可能会泄露秘密或机密数据。'
- en: '*Reuse and data sparsity:* There might not be enough data to train some types
    of classifiers, so there may be prebuilt classifiers that you can download and
    use without training to execute some part of the project where there’s insufficient
    data.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重用和数据稀疏性:* 可能没有足够的数据来训练某些类型的分类器，因此可能存在可以下载并使用而不需要训练的预建分类器，以执行项目中数据不足的部分。'
- en: '*Project risk/time to develop:* Training classifiers from scratch can be difficult
    and risky, and unexpected behaviors may mean they can’t be used. Downloading pretrained
    components or using simple models can reduce project risk at the cost of overall
    performance.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*项目风险/开发时间:* 从头开始训练分类器可能很困难且风险很高，意外的行为可能意味着它们无法使用。下载预训练组件或使用简单模型可以在一定程度上降低项目风险，但会牺牲整体性能。'
- en: '*Robustness in production:* A highly tuned solution might be brittle and fragile
    in the real world.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产中的鲁棒性:* 高度优化的解决方案在现实世界中可能很脆弱。'
- en: Today’s ML practitioners have a range of algorithms available to deal with the
    complex functional and nonfunctional demands of modern applications. Given this
    wealth of resources, can you just pick a solution out of a toolbox that resolves
    the particular forces in your project? Sadly, the world just doesn’t work that
    way, and things are a bit more complex.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的机器学习从业者有一系列算法可供处理现代应用的复杂功能和非功能需求。鉴于这些丰富的资源，你能否从工具箱中挑选出一个解决方案来解决项目中特定的力量？遗憾的是，世界并不总是这样运作，事情要复杂得多。
- en: 7.3.2 Overall design
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 总体设计
- en: 'It is important to choose the right algorithm for the data and the problem
    at hand, and there is a huge amount of literature available to support your choice.
    For example, Kevin Murphy provides detailed, in-depth explanations of a wealth
    of algorithms in his book, *Probabilistic Machine Learning: An Introduction* [10].
    Often, though, it isn’t a simple case of picking the best option from all the
    available candidate algorithms because:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合数据和当前问题的正确算法非常重要，并且有大量的文献可供参考以支持你的选择。例如，凯文·墨菲在他的著作《概率机器学习：导论》*概率机器学习：导论*
    [10] 中提供了大量算法的详细深入解释。然而，通常并不是简单地从所有候选算法中挑选最佳选项，因为：
- en: There is no best option. Instead, you are faced with trade-offs.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有最佳选项。相反，你面临的是权衡。
- en: Nothing quite fits the application, and you must choose, at least, a bad option.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有任何东西完全适合应用，你至少必须选择一个糟糕的选项。
- en: The best option isn’t going to work for your model’s context. Your team doesn’t
    understand it, you don’t have the time to implement it, the hardware platform
    won’t support it, or (commonly) it requires too much attention and intervention
    in production, and it won’t get that from your users.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好的选择可能不适合你的模型环境。你的团队不理解它，你没有时间去实施它，硬件平台不支持它，或者（常见的是）在生产中需要太多的关注和干预，而你的用户不会提供这种关注。
- en: What can we do to overcome these everyday challenges? One pragmatic response
    to these challenges is to implement a composite model that brings together several
    techniques to address different parts of the data in a way that a single model
    is unable to do (quickly and reliably in a time-bound project). For example, you
    may require an automatic translation bot to interpret a sentence uttered in one
    of several potential languages into English. It is technically possible to implement
    this model as a single, deep network, and the performance of the single network
    may be superior to specialized single-language networks. It might be less risky,
    easier, and quicker to break the problem into parts and to use a more well-worn
    and tested solution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做些什么来克服这些日常挑战呢？对这些挑战的一个实用回应是实施一个复合模型，将几种技术结合起来，以处理数据的不同部分，这是单个模型无法做到的（在时间限制的项目中快速且可靠地）。例如，你可能需要一个自动翻译机器人将几种潜在语言中的一种语言说出的句子翻译成英语。在技术上，可以将这个模型实现为一个单一的深度网络，单个网络的性能可能优于专门的单语言网络。将问题分解成部分并使用更成熟和经过测试的解决方案可能风险更低、更容易、更快。
- en: A different issue is created by a challenge that requires several disjoint models
    to be developed, which work independently in different parts of a business process.
    For example, in a customer service flow, it may be that your system needs to recognize
    the language a customer speaks and then offer the right support information to
    them, based on the challenge they face. With these challenges, there are architectural
    choices that you can make to break the problem into bits and algorithms that can
    be made to solve each part. How are those choices made, though?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要开发几个不相连的模型，这些模型在不同的业务流程部分独立工作时，就会产生不同的问题。例如，在客户服务流程中，可能你的系统需要识别客户所说的语言，然后根据他们面临的挑战提供正确的支持信息。面对这些挑战，你可以做出一些架构选择，将问题分解成各个部分，并使用算法来解决每一部分。然而，这些选择是如何做出的呢？
- en: 7.3.3 Choosing component models
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 选择组件模型
- en: Data scientists use their expertise and experience to select the right ML algorithms
    to produce the right model for the problem at hand. Nothing substitutes for experience
    and insight, but there are some useful heuristics, which are good to have in mind
    during this process. Although these rules of thumb and general principals may
    not correspond to the best technical approach, they can be useful in terms of
    managing the project’s risk during its modelling.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家利用他们的专业知识和经验来选择合适的机器学习算法，以产生针对当前问题的正确模型。经验和对洞察力的理解是无法替代的，但在这一过程中有一些有用的启发式方法，值得考虑。尽管这些经验法则和一般原则可能不对应于最佳技术方法，但在管理建模过程中的项目风险方面可能是有用的。
- en: Two of the most basic determinants of the model is the type of data that the
    algorithm consumes and uses and the type of output that the model produces. If
    the model needs to produce a graduated control signal from 0.0 to 1.0, and it
    can only produce a binary signal of on or off, then the model is useless. If the
    ML algorithm can’t “see” in color, then the model it produces will be in black
    and white. Fundamentally, the question is, can the algorithm model the output
    distribution effectively? This is a fancy way of saying is there any output from
    the model that it is technically unable to produce? Is it able in principle to
    produce the correct proportion of output? If not, then you need to stop the team
    and get them to use approaches that can.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的两个最基本的决定因素是算法消耗和使用的类型数据以及模型产生的输出类型。如果模型需要从0.0到1.0产生一个渐变的控制信号，而它只能产生开或关的二进制信号，那么这个模型就是无用的。如果机器学习算法不能“看到”颜色，那么它产生的模型将是黑白色的。从根本上讲，问题是，算法能否有效地模拟输出分布？这是一种花哨的说法，即模型是否有什么输出它在技术上无法产生？它原则上能否产生正确的输出比例？如果不能，那么你需要停止团队的工作，并让他们使用能够做到这一点的方案。
- en: Given that the algorithms you choose are capable (in principle) of creating
    a model of the distribution that you are after, then well-known algorithms should
    be preferred to novel algorithms where possible. A new algorithm may be state-of-the-art
    and may perform a little better than the old stager that everyone has used forever
    for problems like this. Moreover, a new algorithm may also be undertested and
    not well-understood. If nothing else, a well-known algorithm should be benchmarked
    against the newest and shiniest approach advocated by the modeling team. If their
    new “gee-whiz” ML magic outperforms older and better-known architectures and approaches,
    then great! If it doesn’t, then the older approach will be a source of great comfort
    to you.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你选择的算法（原则上）能够创建出你想要的分布模型，那么在可能的情况下，应优先选择已知的算法，而不是新颖的算法。一个新算法可能是最先进的，可能比大家一直用来解决这类问题的旧方法表现得更好。此外，一个新算法可能还没有经过充分的测试，也不被充分理解。至少，一个已知的算法应该与建模团队倡导的最新和最闪亮的方案进行基准测试。如果他们的新“哇塞”机器学习魔法比老的和更知名的架构和方案表现更好，那么太好了！如果它没有，那么老的方法将给你带来极大的安慰。
- en: In general, it’s best to use a simpler algorithm if you can effectively address
    the modelling problem with it. Sophisticated, deep-network architectures, however,
    can do things that other algorithms can’t. Despite this injunction, you may find
    that the team has no option but to dive into new waters, especially when dealing
    with domains like images, sound, and natural languages.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果你可以用它有效地解决建模问题，最好使用一个简单的算法。然而，复杂的深度网络架构可以做其他算法做不到的事情。尽管有这个禁令，你可能会发现团队别无选择，只能跳入新的水域，尤其是在处理图像、声音和自然语言等领域时。
- en: Deep networks can create models for complex unstructured data, but these are
    usually considered not to be transparent or explainable. There are mechanisms
    that use deep networks because their functional performance is great. This can
    be explained to humans effectively, but these mechanisms are always inferior to
    more straightforward, transparent models, such as association rules or decision
    trees. If transparency is a design force, you may have to balance it carefully
    against the functional performance of the system.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络可以为复杂的不规则数据创建模型，但这些通常被认为是不透明或不可解释的。有一些机制使用深度网络，因为它们的性能功能很好。这些机制可以有效地向人类解释，但这些机制总是比更直接、透明的模型（如关联规则或决策树）低效。如果透明性是设计力量，你可能必须仔细权衡它与系统的功能性能。
- en: Deep networks can be expensive to train and expensive in terms of latency and
    cost when used in the production system. Remember that the team burns a lot of
    C02 when they run a data center’s worth of GPUs for a week to create a model that
    can be used to improve a building’s energy efficiency. This might not be a good
    thing for you to have to tell your sponsors about.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络在训练和在生产系统中使用时可能会很昂贵。记住，当团队运行一个数据中心价值的一周GPU来创建一个可以用来提高建筑能源效率的模型时，他们会燃烧大量的二氧化碳。这可能不是你告诉赞助商的好事情。
- en: These general principals are just good common-sense design heuristics. We choose
    our models because they will produce the required output from the input. This
    likelihood goes by the luminous title of *inductive bias*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些一般原则只是良好的常识性设计启发式方法。我们选择模型是因为它们可以从输入产生所需输出。这种可能性被称为*归纳偏差*。
- en: 7.3.4 Inductive bias
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 归纳偏差
- en: 'We can create ML models using different algorithms and approaches, but each
    of these introduce some bias into the process. Consider the sensors from the smart
    building example. The sensors have the following attributes that describe each
    of them: age, manufacturer, and installation. Installation has two values, internal/external;
    age is a scalar from 0 (new) to the number of days that the sensor has been installed.
    There are five different manufacturers, and half the sensors failed in our data.
    Because of this, we would like to make a decision tree that predicts which sensors
    are likely to fail.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的算法和方法来创建机器学习模型，但每种方法都会在过程中引入一些偏差。以智能建筑示例中的传感器为例。传感器具有以下属性来描述每个传感器：年龄、制造商和安装。安装有两个值，内部/外部；年龄是从0（新）到传感器安装的天数的标量。有五种不同的制造商，我们数据中有一半的传感器失败了。因此，我们希望创建一个决策树，预测哪些传感器可能会失败。
- en: We have some choices to make about how we’ll construct the tree. Which attribute
    should we test first? What tests should we apply to which attribute? In this case,
    a sensible test to start with might be the installation attribute. Is this sensor
    internal or external? In some situations, we might see that 80% of the external
    sensors fail and that less than 20% of the internal sensors also fail. Figure
    7.4 illustrates the effect of a split on some data with the installation attribute.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在如何构建树方面做出一些选择。我们应该首先测试哪个属性？我们应该对哪个属性应用哪些测试？在这种情况下，一个合理的起点可能是安装属性。这个传感器是内部还是外部的？在某些情况下，我们可能会看到80%的外部传感器失败，而内部传感器的失败率不到20%。图7.4说明了分割对某些具有安装属性的数据的影响。
- en: '![](../Images/07-04.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-04.png)'
- en: Figure 7.4 Splitting the training data using the installation attribute. Outside
    sensors (right) fail much more often than inside ones.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 使用安装属性分割训练数据。外部传感器（右侧）比内部传感器更频繁地失败。
- en: Even though our choice is commonsensical, in some ways it’s arbitrary. Figure
    7.5 shows an alternative split using the age attribute, which creates a clean
    division of data. Now we see that all sensors that have been installed for longer
    than 100 days always fail. A split using *test age* > 100 creates a left-hand
    node that only contains failed sensors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的选择符合常识，但在某些方面是任意的。图7.5显示了使用年龄属性进行的替代分割，它创建了数据的一个干净分割。现在我们看到，所有安装时间超过100天的传感器总是失败的。使用*测试年龄*
    > 100的分割创建了一个只包含失败传感器的左节点。
- en: '![](../Images/07-05.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-05.png)'
- en: Figure 7.5 Splitting the data using the age attribute to yield a pure node on
    the left branch. No further tests are required on this tree.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 使用年龄属性分割数据，在左分支上产生一个纯节点。在这个树上不需要进行进一步测试。
- en: More sophisticated choices have been developed, which are backed by well thought-out
    reasoning as to why they are appropriate or not. These arguments almost always
    come down to a variation of Occam’s razor, which tells us to choose the set of
    splits that tend to make for the simplest decision tree in preference to the infinity
    of other ways of determining class membership. An algorithm that uses one criterion
    for splitting a target set at a node in a decision tree, such as an information
    theoretic measure like the J-measure [9], might be more likely to discover a particular
    regularity in the data rather than an algorithm that utilizes another criterion,
    such as statistical significance or simple counting [2].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出更复杂的选择，这些选择背后有经过深思熟虑的理由，说明为什么它们是合适的或不合适的。这些论点几乎总是归结为奥卡姆剃刀的变体，它告诉我们应该选择那些倾向于产生最简单决策树的分割集，而不是其他无限多的确定类别成员的方式。一个在决策树的节点处使用一个标准来分割目标集的算法，例如使用信息论度量如J度量[9]，可能更有可能发现数据中的特定规律，而不是使用另一个标准，如统计显著性或简单计数[2]。
- en: Often, we use XGBoost [1], other boosting algorithms, and random forests to
    creating effective and robust models over tabular data. Unfortunately, they create
    complex classifiers that are hard to understand. This can make it difficult to
    use them in applications where decisions based on models should be transparent
    and readily explained.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们使用XGBoost [1]、其他提升算法和随机森林在表格数据上创建有效且鲁棒的模式。不幸的是，它们创建了复杂且难以理解的分类器。这可能会使得在基于模型的应用中使用它们变得困难，在这些应用中，基于模型的决策应该是透明的并且可以轻松解释。
- en: If an XGBoost model is too detailed and complex to be used directly, it can
    be used as a denoiser, screening out the noise that confuses simpler models. To
    do this, we need to train an XGBoost model and then use the output as the target
    variable for modeling with a simpler decision tree or an association rule discovery
    algorithm. Alternatively, you can use XGBoost as a baseline to gauge how much
    the choice of a simple explainable model is costing you in terms of model effectiveness.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个XGBoost模型过于详细和复杂，无法直接使用，它可以作为一个降噪器，筛选出混淆简单模型的噪声。为此，我们需要训练一个XGBoost模型，然后将输出作为使用简单决策树或关联规则发现算法建模的目标变量。或者，您可以将XGBoost作为基线来衡量选择简单可解释模型在模型有效性方面所付出的代价。
- en: For unstructured data such as images and text, XGBoost is less successful and
    less applicable. The development of a variety of hierarchical architectures for
    deep networks means a range of biases can be created when selecting a particular
    architecture. Choosing the correct bias means we can sometimes obtain a much lower
    loss from less data in return for a much lower burden of training and inference
    computation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如图像和文本这样的非结构化数据，XGBoost不太成功且不太适用。为深度网络开发各种层次架构意味着在选择特定架构时可以创建一系列偏差。选择正确的偏差意味着我们有时可以从更少的数据中获得更低的损失，同时以更低的训练和推理计算负担为代价。
- en: Figure 7.6 shows five different types of deep networks. These are hierarchical
    arrangements such as multi-layer perceptron’s, grid networks designed for computer
    vision tasks, recurrent networks [5] designed for speech and text-based tasks,
    graph networks [4], and attention-based networks such as transformers [11].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6显示了五种不同的深度网络。这些是如多层感知器这样的层次排列，为计算机视觉任务设计的网格网络，为语音和基于文本的任务设计的循环网络[5]，图网络[4]，以及基于注意力的网络，如变换器[11]。
- en: '![](../Images/07-06.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-06.png)'
- en: Figure 7.6 Inductive bias in ML algorithms (adapted from Jumper, Evans, et.
    al. [6]. The sequence (a) through (e) is determined by the order in which these
    biases gained popularity in the ML community. The arrows indicate the propagation
    of signal across a network or in a graph.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 机器学习算法中的归纳偏差（改编自Jumper, Evans等[6]。序列（a）至（e）是根据这些偏差在机器学习社区中获得流行程度的顺序确定的。箭头表示信号在网络或图中的传播。
- en: The network structures in figure 7.6 were developed in response to the need
    to process different types of data. For example, figure 7.7 shows a perceptron-type
    network using a hierarchical structure. This consumes the data from an image one
    pixel at a time, but because the context of the pixels is as important as its
    content, the perceptron doesn’t do well. The information in the image is largely
    stored in the relationship of the pixels to each other, and the perceptron can’t
    capture that.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6中的网络结构是为了响应处理不同类型数据的需求而开发的。例如，图7.7显示了一个使用层次结构的感知器型网络。它一次处理一个图像像素的数据，但由于像素的上下文与其内容一样重要，感知器表现不佳。图像中的信息主要存储在像素之间的关系中，而感知器无法捕捉到这一点。
- en: '![](../Images/07-07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-07.png)'
- en: Figure 7.7 What images (pixelated on the lhs) look like to a hierarchically
    arranged network (rhs). The order of the image is lost and effectively randomized
    with pixel *n* fed to the right most input of the network, distant from the pixel
    above it or diagonally adjacent to it, this makes the task of object recognition
    hard for the system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 对于一个层次排列的网络（右侧），左侧像素化的图像看起来是什么样子。图像的顺序丢失，并且通过将像素*n*输入到网络的右侧最输入端，远离上面的像素或与其对角相邻，这使系统识别对象的任务变得困难。
- en: On the other hand, we can construct convolutional networks [8]. (shown as (b)
    in figure 7.6) to consider localities. *Convolution* is the process of propagating
    information locally in the network, providing normalization and filtering of activations
    at a local level. A learned filter is passed in a sliding window over the network,
    determining how signals should be propagated from pixel to pixel, and information
    is pooled between layers. Convolutional networks became the favored architecture
    for image recognition problems after large scale training sets and compute engines
    sufficiently powerful to harness images on these types of training challenges
    became available.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以构建卷积网络[8]。如图7.6中的（b）所示）来考虑局部性。*卷积*是在网络中局部传播信息的过程，在局部级别提供激活的正常化和过滤。一个学习到的过滤器在网络的滑动窗口中传递，确定信号应如何从像素传播到像素，并在层之间汇总信息。在有了足够强大的计算引擎来处理这些训练挑战上的图像，并且有大规模的训练集之后，卷积网络成为了图像识别问题的首选架构。
- en: 7.3.5 Multiple disjoint models
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.5 多个不连续模型
- en: Many projects require that several models be constructed to support different
    parts of the AI application. The models’ results do not feed into one another
    but are consumed by a human or as separate input to a decision-making system.
    For example, a credit risk model might be made up of several disjoint models that
    represent different types or drivers of risk. One might model fraud, and another,
    dependencies that make the applicants risky, whereas a third one might model economic
    trends. These models’ output can be composed together to create a single score,
    or they can be displayed separately to a credit controller.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 许多项目需要构建多个模型来支持人工智能应用的各个部分。这些模型的结果不会相互输入，而是由人类或作为决策系统的单独输入来消费。例如，一个信用风险模型可能由几个不连续的模型组成，代表不同类型或风险驱动因素。一个模型可能模拟欺诈，另一个模型模拟使申请人有风险的依赖关系，而第三个模型可能模拟经济趋势。这些模型的输出可以组合在一起创建一个单一分数，或者可以单独显示给信用控制器。
- en: If your team is working on one integrated system, the divergent requirements
    of different models can have a significant impact both on the complexity of the
    system required (in terms of the inference and the data layers for production)
    and on the performance of the system (in terms of quantitative and nonfunctional
    performance). We need to balance the allocation of resources to each of the models
    to ensure that the overall value of the system is maximized.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的团队正在开发一个集成系统，不同模型的分歧需求将对所需系统的复杂性（在推理和数据层方面）和系统的性能（在定量和非功能性性能方面）产生重大影响。我们需要平衡对每个模型的资源分配，以确保系统的整体价值最大化。
- en: Remember, though, resources include both the time and the processor power allocated
    to the production models, as well as the time and the team’s effort during development.
    An easy trap for a team is to spend weeks polishing one model at the expense of
    not focusing on other models, which could yield far more benefit with a bit of
    work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，资源包括分配给生产模型的时问和处理器功率，以及开发期间团队的努力。一个团队容易陷入的陷阱是花费数周时间打磨一个模型，而忽略了其他模型，这些模型只需稍作努力就能带来更大的收益。
- en: 7.3.6 Model composition
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.6 模型组合
- en: An alternative setup is to use models in sequence to perform an overall reasoning
    process. Sometimes, we chain models together because we need to capture further
    intervention or outcome. For example, a decision must be made regarding the control
    of flow for some reagent in a process, and the outcome of mixing must be observed
    before making another decision about heating the solution. This situation is known
    as *model composition*, and its design process is more complex than for two isolated,
    independent models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设置是使用模型按顺序执行整体推理过程。有时，我们将模型串联起来，因为我们需要捕捉进一步的干预或结果。例如，必须就某个过程中某些试剂的流量控制做出决定，在做出关于加热溶液的另一个决定之前，必须观察混合的结果。这种情况被称为*模型组合*，其设计过程比两个独立的模型更复杂。
- en: Alternatively, it may be that we can sensibly break a single task into a set
    of dependent models, although there are cases where a single model is the best
    option. Often, problems can be quickly and reliably solved with the pragmatic
    approach of breaking the problem into a number of steps and learning each of these
    individually to create a chain of inference for the model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可能可以将单个任务合理地分解成一系列依赖模型，尽管在某些情况下，单个模型可能是最佳选择。通常，通过将问题分解成多个步骤并分别学习每个步骤来创建模型的推理链，可以快速且可靠地解决问题。
- en: 'As an example, think about the challenge of translating a sentence into English.
    The first step is to recognize the source language. The next step is to determine
    the meaning or intent of the sentence, then the final step is to render that meaning
    or intent into English in the most elegant way possible. It is possible to imagine
    a single network that encompasses all three tasks in a meta task called translate
    (and, indeed, these do exist and have definite advantages). It may, however, be
    easier to build three different networks that do each of these steps and chain
    them together with the appropriate glue and management logic. This approach has
    the following advantages:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑将句子翻译成英语的挑战。第一步是识别源语言。下一步是确定句子的含义或意图，然后最后一步是以最优雅的方式将那种含义或意图表达成英语。可以想象一个包含所有三个任务的单个网络，这些任务在元任务中被称为翻译（实际上，这些确实存在，并且具有明显的优势）。然而，可能更容易构建三个不同的网络，每个网络执行这些步骤之一，并用适当的粘合剂和管理逻辑将它们连接起来。这种方法有以下优点：
- en: We can more closely manage the technical risks of each subproject as it builds
    a component model rather than the risks of a large and challenging E2E (End 2
    End) network.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以更密切地管理每个子项目在构建组件模型时的技术风险，而不是管理大型且具有挑战性的端到端（End 2 End）网络的风险。
- en: Reusable elements like pretrained, off-the-shelf networks or data sets may be
    available for the subcomponents as these may represent more general tasks than
    the E2E solution.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复使用的元素，如预训练的现成网络或数据集，可能适用于子组件，因为这些可能代表比端到端解决方案更通用的任务。
- en: We can test individual components in isolation, enabling easier troubleshooting
    and debugging.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在隔离状态下测试单个组件，这有助于更容易地进行故障排除和调试。
- en: We can build in parallel individual components, potentially shortening development
    time (although this assumes that the cost in the engineering time for building
    a better single network is larger than the cost of integration).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以并行构建单个组件，这可能会缩短开发时间（尽管这假设构建更好的单个网络的成本高于集成成本）。
- en: We can engineer partial solutions into an overall system that provides business
    value.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将部分解决方案工程化到提供商业价值的整体系统中。
- en: 'If there is some success in developing some of these components, and if these
    components can be integrated into the business processes and used to support its
    decision makers, we can deliver a successful project. The trade-off is that there
    are also downsides:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在开发这些组件中取得了一些成功，并且如果这些组件可以集成到业务流程中并用于支持其决策者，我们可以交付一个成功的项目。然而，权衡的是也存在一些不利因素：
- en: The composite model might not perform as well as a single bespoke model.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合模型可能不如单个定制模型表现得好。
- en: Managing the productionization of a plethora of models can be hard and expensive
    in terms of documentation and process.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理大量模型的量产可能很困难且成本高昂，尤其是在文档和流程方面。
- en: Chains of models can introduce latency and throughput issues and bottlenecks
    (the fleet sails at the pace of the slowest ship).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型链可能会引入延迟和吞吐量问题以及瓶颈（舰队以最慢船只的速度航行）。
- en: It’s difficult to understand and maintain complex designs.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和维护复杂设计是困难的。
- en: When you’ve developed a model design and the integration strategy is agreed
    on and communicated to the team, the next step is for the team to realize it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你已经开发了一个模型设计，并且集成策略已达成一致并传达给团队后，下一步就是团队实现它。
- en: 7.4 Making models with ML
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用机器学习构建模型
- en: 'Model process support ticket: S2.3'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 模型流程支持工单：S2.3
- en: Agree on the modelling process and set it up.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就建模过程达成一致，并设置它。
- en: Commission and use an experiment tracker.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用并使用实验跟踪器。
- en: Commission and use a model repository.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用并使用模型存储库。
- en: Identify and reject obviously poor models.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别并拒绝明显较差的模型。
- en: The actual creation of ML models can be as complicated as architecting a complex
    deep learning network with specialist layers and feedback circuits, or it can
    be as simple as pressing the Go button on a user interface. When model creation
    is complex and involved, the expertise required to create the model is obvious,
    and the need to use an expert team to do the work is driven by the fact that non-experts
    will fail to create a model that supports the application. If you use a low-code
    or tool-driven approach to generating models, it can sometimes seem that expertise
    is not required because a valuable result looks to be on the table even though
    no experts are involved.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 实际创建机器学习模型可能像设计一个复杂的深度学习网络（具有专业层和反馈回路）一样复杂，或者像在用户界面上按下“Go”按钮一样简单。当模型创建复杂且涉及多个方面时，创建模型所需的专长是显而易见的，而需要使用专家团队来完成工作的需求是由非专家无法创建支持应用的模型这一事实驱动的。如果你使用低代码或工具驱动的方法来生成模型，有时可能会觉得不需要专业知识，因为即使没有专家参与，似乎也能得到有价值的结果。
- en: The reality is that even if the model is created with one click, insight and
    discipline are required. Everything that gets the model maker to the point of
    pushing the button and everything that happens to the model after the button is
    pushed creates value, or it creates damage. Because of this, it’s important to
    define and manage the process that you use for modeling.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使模型可以通过一键创建，也需要洞察力和纪律。模型制作者从按下按钮的那一刻起，以及按钮按下后模型所发生的一切，都会创造价值，或者造成损害。正因为如此，定义和管理你用于模型化的过程非常重要。
- en: 7.4.1 Modeling process
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 模型化过程
- en: The actual process of developing a model is curiosity driven and experimental;
    it’s both an art and a science. Building understanding about the data and the
    domain and creating deliberate designs for the model is necessary but insufficient
    for success. To get good models, the team needs to experiment, introspect, and
    investigate their way to something that works.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 开发模型的实际过程是好奇心驱动的和实验性的；它既是艺术也是科学。对数据和领域有深入理解，并为模型创造有意的结构是必要的，但不足以成功。为了得到好的模型，团队需要通过实验、内省和调查找到可行的方法。
- en: This sounds like an ad hoc and unstructured activity, and in a way, it is. Although
    a scientist may have flashes of inspiration and try something crazy, it’s all
    done in a managed framework. For a scientist, experimental details are written
    out, upfront in the lab book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是一种临时和非结构化的活动，从某种意义上说，确实如此。尽管科学家可能会有灵光一闪，尝试一些疯狂的事情，但所有这些都是在管理框架内完成的。对于科学家来说，实验细节会提前在实验记录簿中写出来。
- en: Data scientists need to use the same management framework to achieve similar
    objectives of rigor and reproducibility. It’s easy to mess up and lose some detail
    of model design, which means the model can’t be rebuilt or has elements that are
    undocumented. Additionally, the investigation needs to be organized and records
    kept so that effort isn’t replicated, redoing work that should have been put to
    bed the first time round. Not only does repeated reworking of poorly done or poorly
    documented work waste time and money, it also wastes valuable resources. The actual
    process to use prevents wasted time, and it’s also designed to prevent the team
    from fooling themselves about the quality of the models they are building.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要使用相同的管理框架来实现类似的目标，即严谨性和可重复性。很容易出错，丢失模型设计的一些细节，这意味着模型无法重建或存在未记录的元素。此外，调查需要组织，并保留记录，以便不重复工作，不重复进行本应第一次就完成的工作。不仅反复重做做得不好或记录不充分的工
    作会浪费时间和金钱，还会浪费宝贵的资源。实际使用的过程可以防止浪费时间，并且它还旨在防止团队对自己所构建的模型质量产生错误的认识。
- en: 'A big issue in modelling activity is often called *data leakage*, which is
    the corruption of the model evaluation process by accidentally allowing test information
    to escape into the training process. Essentially, data leakage means that the
    algorithm takes a peek at the test data before it’s supposed to. A typical data
    leakage scenario plays out like this: because some model just happens to do well
    on the validation data, the modelling team tends to focus on similar models and
    figures out more performance optimizations for the validation problem. Sadly,
    this fails to translate into real-world performance, and the model is a big disappointment.
    This can be because the team’s actions caused an unconscious optimization over
    the quirks and artefacts in the validation data.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模活动中，一个常见的问题通常被称为*数据泄露*，这是指在模型评估过程中，由于意外地将测试信息泄露到训练过程中，导致评估过程的腐败。本质上，数据泄露意味着算法在应该之前偷看测试数据。一个典型的数据泄露场景是这样的：由于某个模型恰好对验证数据表现良好，建模团队往往会专注于类似的模型，并为验证问题找出更多的性能优化。遗憾的是，这并没有转化为现实世界的性能，模型令人大失所望。这可能是因为团队的行为导致了对验证数据中的怪癖和伪影的无意识优化。
- en: As an aside, another common mistake is choice of the training and validation
    data. This happens especially in time series prediction problems, where cases
    from the future provide a signal that the model latches onto and then replicates
    that in the test data. For example, we use the data from a hot summer in the validation
    set for our smart building. Because the test data set is also drawn from the same
    hot summer, the model cheats on its test performance in a way that will not be
    replicated in production.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为旁注，另一个常见的错误是训练和验证数据的选取。这在时间序列预测问题中尤为常见，其中来自未来的案例为模型提供了信号，模型随后将其复制到测试数据中。例如，我们使用验证集中的炎热夏季数据来为我们的智能建筑建模。因为测试数据集也是从同一个炎热夏季抽取的，模型在测试性能上作弊，而这种作弊方式在生产环境中是不会复制的。
- en: These issues prevent the models from being properly explored. Further, the time
    that could be spent checking other promising design options gets spent chasing
    the ghost of a promising one-off performance. What to do?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题阻碍了模型得到适当的探索。此外，本可以用来检查其他有希望的设计选项的时间，都花在了追逐一个有希望的一次性性能的幽灵上。怎么办？
- en: Plan how the available time is to be spent implementing and exploring the performance
    of the model design.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 规划如何分配可用时间来实现和探索模型设计性能。
- en: For each part of the design, identify a set of experiments to create and verify
    the behavior of that component. Each experiment is an episode of modelling and
    testing. Write down what’s expected from this process.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于设计的每个部分，确定一组实验来创建和验证该组件的行为。每个实验都是建模和测试的一个阶段。写下从这个过程中期望得到的内容。
- en: Do the modelling and testing as planned and document the results.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按计划进行建模和测试，并记录结果。
- en: To understand the results, inspect the results with the appropriate tools.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要理解结果，请使用适当的工具检查结果。
- en: After each episode, review and decide on what changes to make to the plan and
    whether new features or changes to the data pipelines are required.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个阶段结束后，回顾并决定对计划进行哪些更改，以及是否需要新的功能或数据管道的更改。
- en: Select another episode from the backlog and do to same for that one.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从待办事项中选取另一个阶段，并对其执行相同的操作。
- en: The point of doing this is that by putting discipline around the process and
    documenting and reviewing what is done, you can identify and stop any process
    of over optimization (with a potential information leak). If things go wrong,
    then at least you can identify that you need to get more validation data and to
    check that the models are working good enough to warrant further development.
    Failing to do this leads to the data scientists stumbling around in the dark trying
    out things to see if they really work. By interrogating the results, and understanding
    them, and then reviewing what was learned on each iteration, the development process
    gains direction and momentum. Of course, this is in theory; just as lab experiments
    are a hard road to tread, so data science experiments are difficult to do as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是通过规范流程、记录和审查所做的工作，可以识别并停止任何过度优化的过程（可能存在信息泄露）。如果事情出错，至少可以确定需要获取更多的验证数据，并检查模型是否足够好，足以支持进一步的开发。不这样做会导致数据科学家在黑暗中摸索，试图看看他们是否真的有效。通过质询结果、理解它们，并在每次迭代中回顾所学到的知识，开发过程获得了方向和动力。当然，这只是理论上的；正如实验室实验是一条艰难的道路，数据科学实验也同样难以进行。
- en: There is another set of drivers that you and the team should consider when deciding
    to work in a planned and systematic way during model development. In traditional
    science, carefully maintained lab books allow experiments to be reproduced and
    practices checked for safety and professionalism. In the same way, keeping careful
    records of ML processes allow promising models to be reproduced and audited. This
    is going to be increasingly important as ML systems become more commonly used
    and more frequently questioned in our society. Working professionally with good
    record-keeping makes your models auditable and inspectable, and this makes them
    more useful and valuable. Next, we’ll discuss how to document modelling activity
    and how to manage and track the results.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定在模型开发过程中以计划化和系统化的方式进行工作时，你们和团队应该考虑另一组驱动因素。在传统科学中，精心维护的实验记录本允许实验可以被复制，并且实践可以检查其安全性和专业性。同样，仔细记录ML过程允许有潜力的模型可以被复制和审计。随着ML系统在我们的社会中越来越普遍地被使用和频繁地受到质疑，这一点将变得越来越重要。以专业的方式并保持良好的记录使得你的模型可审计和可检查，这使它们更有用和更有价值。接下来，我们将讨论如何记录建模活动以及如何管理和跟踪结果。
- en: 7.4.2 Experiment tracking and model repositories
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 实验跟踪和模型存储库
- en: The model repository that you selected and implemented as part of the project
    infrastructure stores instances of models built by the team. Some metadata about
    the models should be stored there as well. For example, it should contain the
    version of the algorithm used to create the model, the hyperparameters used for
    the algorithm, and the links to the training set or training pipeline versions
    that fed the data to the algorithm.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你所选定的并作为项目基础设施一部分实施的模式存储库存储了团队构建的模型实例。关于这些模型的某些元数据也应该存储在那里。例如，它应该包含创建模型所使用的算法版本，用于算法的超参数，以及链接到为算法提供数据的训练集或训练管道版本。
- en: Table 7.3 shows a small sample of some statistics recorded over several runs.
    In this figure, we used a simple linear model to make predictions. The variation
    in performance is due to the different sizes of the test set, and performance
    is estimated for the experiment using the AUC (area under the curve metric). Notice
    that the `testSize` parameter was added at 10:38am when the data scientist noticed
    that there were some differences in performance and that the size of the test
    set was not being recorded to explain them. Also notice the change in the penalty
    function selected by the ML package (Elastic_net) when a large test is tried.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3展示了在多次运行中记录的一些统计数据的样本。在这个图中，我们使用了一个简单的线性模型来进行预测。性能的变化是由于测试集大小的不同，实验的性能是通过AUC（曲线下面积指标）来估计的。请注意，当数据科学家在上午10:38注意到性能存在一些差异，并且测试集的大小没有被记录下来以解释这些差异时，添加了`testSize`参数。同时请注意，当尝试大测试时，ML包（Elastic_net）选择的惩罚函数发生了变化。
- en: Table 7.3 Example of statistics that might be recorded by an experiment tracking
    system
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3 实验跟踪系统可能记录的统计数据示例
- en: '| Start time | Duration | l1_ratio | Penalty | testSize | auc_test |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 开始时间 | 持续时间 | l1_ratio | 惩罚 | testSize | auc_test |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 15/02/2022 10:43 | 1.9 s | 0.5 | Elastic net | 0.5 | 0.93916314 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 10:43 | 1.9秒 | 0.5 | Elastic net | 0.5 | 0.93916314 |'
- en: '| 15/02/2022 10:42 | 2.1 s | 0.5 | l2 | 0.1 | 0.94286043 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 10:42 | 2.1秒 | 0.5 | l2 | 0.1 | 0.94286043 |'
- en: '| 15/02/2022 10:38 | 2.1 s | 0.5 | l2 | 0.1 | 0.94286043 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 10:38 | 2.1秒 | 0.5 | l2 | 0.1 | 0.94286043 |'
- en: '| 15/02/2022 10:34 | 2.4 s | 0.5 | l2 |  | 0.94286043 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 10:34 | 2.4秒 | 0.5 | l2 |  | 0.94286043 |'
- en: '| 15/02/2022 10:00 | 2.0 s | 0.1 | l2 |  | 0.94286043 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 10:00 | 2.0秒 | 0.1 | l2 |  | 0.94286043 |'
- en: '| 15/02/2022 09:59 | 1.9 s | 0.5 | Elastic net |  | 0.93916314 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 09:59 | 1.9秒 | 0.5 | Elastic net |  | 0.93916314 |'
- en: '| 15/02/2022 09:58 | 2.3 s | 0.1 | l2 |  | 0.94505654 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2022年2月15日 09:58 | 2.3秒 | 0.1 | l2 |  | 0.94505654 |'
- en: Recording this information is tedious and error-prone, but despite that, we
    recorded the data in the table using a package (tool) called MLflow (but there
    are many other similar tools that you can adopt). We ran this tool from the same
    Python runtime as the ML algorithm (in this case, a simple regression). MLflow
    has an API that we used to record the details of the experiment. We entered the
    statistics and parameters into the MLflow database in calls from the modelling
    code. Each time we ran the experiment, the data about the parameters and performance
    was pushed, and we can retrieve it either from a GUI or from a simple command-line
    call.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 记录这些信息既繁琐又容易出错，尽管如此，我们还是使用一个名为MLflow（但还有许多其他类似工具可供选择）的包（工具）在表中记录了数据。我们从与ML算法（在这种情况下，是一个简单的回归）相同的Python运行时运行了这个工具。MLflow有一个API，我们用它来记录实验的详细信息。我们从建模代码的调用中输入统计数据和参数到MLflow数据库中。每次我们运行实验时，都会推送有关参数和性能的数据，我们可以从GUI或简单的命令行调用中检索它。
- en: Not shown in Table 7.2 are the unique identifiers of the models that tie the
    binaries, assets (such as transfer models, test, training, and validation data
    sets), and formulas used to construct them to the outcomes and results that we
    saw during their development. This tie in is where the model repository enters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2中没有显示的是将二进制文件、资产（如迁移模型、测试、训练和验证数据集）以及构建它们所使用的公式与我们在开发过程中看到的成果和结果联系起来的唯一标识符。这种联系就是模型仓库介入的地方。
- en: Models discovered by ML algorithms are stored as binary files, encoding all
    the discovered parameter settings and weights that specify the particular model
    that was extracted from the data. These are saved in the filesystem or in a database.
    In the case of MLflow, we can store the files in the filesystem using the API
    that was invoked to store the experiment details.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由ML算法发现的模型被存储为二进制文件，编码了所有发现的参数设置和权重，这些参数和权重指定了从数据中提取的特定模型。这些文件保存在文件系统或数据库中。在MLflow的情况下，我们可以使用存储实验详细信息的API在文件系统中存储文件。
- en: Figure 7.8 shows the model stored for the run that created the entries in Table
    7.2\. You can see that the artifacts include the Conda code for all the Python
    packages we used to create the model and the pickle (.pkl) file that encodes the
    actual model. These artifacts should be enough to reconstitute and run the model
    on demand because all the dependencies and requirements to make it work are stored
    in the repository. The other directories contain the metadata that defines a model
    and how it performed in the experiment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8显示了存储在创建表7.2条目中的运行模型的文件。你可以看到，这些工件包括创建模型所使用的所有Python包的Conda代码以及编码实际模型的pickle
    (.pkl)文件。这些工件足以按需重新构建和运行模型，因为所有使它工作的依赖项和需求都存储在仓库中。其他目录包含定义模型及其在实验中表现如何的元数据。
- en: '![](../Images/07-08a.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8](../Images/07-08a.png)'
- en: Figure 7.8 The files stored in a model repository that we can use to understand
    which model is stored, how it was made, how it relates to the experiment tracking
    information, and to get it up and running again.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 存储在模型仓库中的文件，我们可以用它来了解哪个模型被存储，它是如何制作的，它与实验跟踪信息有何关联，以及如何再次启动并运行它。
- en: The data pipelines developed in sprint 1 and the features engineered at the
    start of this sprint are supportive of the data scientists’ work as well. That’s
    why the pipelines and features were developed! Additionally, this work is framed
    in the context of the team’s understanding of the data, the business problem,
    and the project’s ethical challenges. We can go one step further in making the
    ML part of the project even easier. You and the team can save yourself a lot of
    heartache by outsourcing the design of your models and the process of creating
    them to a machine.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在冲刺1中开发的数据管道和在本冲刺开始时构建的特征也支持数据科学家的工作。这就是为什么开发这些管道和特征的原因！此外，这项工作是在团队对数据的理解、业务问题和项目的伦理挑战的背景下进行的。我们可以更进一步，使项目的ML部分变得更加容易。你和团队可以通过将模型的设计和创建过程外包给机器来节省大量的麻烦。
- en: 7.4.3 AutoML and model search
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 自动机器学习和模型搜索
- en: A popular modern practice is to use an automated search system to iteratively
    create models. The search works by choosing variations of algorithm parameters
    and creating and testing models with different structures and biases. This practice
    evolved because the space of potential models that a deep-learning system can
    express is vast, and it’s not possible to check all the structures that can be
    constructed by hand. Although, arguably this kind of process arose due to the
    development of deep learning systems, but since then, the practice is commonly
    applied to tuning the performance of statistically inspired ML algorithms as well.
    Because of its heritage, this technique is also known as neural architecture search
    (NAS) and sometimes as meta-learning.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的现代做法是使用自动搜索系统迭代创建模型。搜索通过选择算法参数的变体，并创建和测试具有不同结构和偏差的模型来实现。这种做法的出现是因为深度学习系统可以表达的可能模型空间非常庞大，无法检查所有可以手工构建的结构。尽管，可以说这种过程是由于深度学习系统的发展而产生的，但自那时起，这种做法通常也应用于调整受统计启发的机器学习算法的性能。由于其遗产，这种技术也被称为神经架构搜索（NAS）和有时称为元学习。
- en: The great thing about AutoML is that you can replicate the work of a diligent
    data scientist with a single line of code or the touch of a button. It would be
    implausible and foolish to deny that there are gains to be made in terms of rapidly
    improving model performance by using AutoML. Some robust state-of-the-art results
    have been obtained by using these processes [3], and this approach automates work
    that can be tedious and error prone. However, there are problems to be aware of.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的伟大之处在于，你可以通过一行代码或轻触按钮来复制勤奋数据科学家的工作。否认通过使用AutoML快速提高模型性能所带来的收益是不切实际且愚蠢的。通过这些过程已经获得了稳健的先进结果[3]，这种方法自动化了可能繁琐且易出错的劳动。然而，也有一些问题需要注意。
- en: Model search systems are optimization systems and, therefore, can produce brittle
    models that work extremely well on hold-out data but fail on test data (or in
    production). It’s just a matter of time until a model search finds some model
    that happens to perform well on a particular test, just by luck something will
    fit. We discuss the issue of maintaining the integrity of the testing system that’s
    used to evaluate models in chapter 8, but at this point, it’s sufficient to flag
    that this can be a problem.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型搜索系统是优化系统，因此可以产生在保留数据上表现极好但在测试数据（或生产中）失败的脆弱模型。模型搜索最终会发现一些模型在特定测试上表现良好，这纯粹是运气，某种东西恰好适合。我们将在第8章讨论维护用于评估模型测试系统完整性的问题，但在此阶段，只需指出这可能会成为一个问题。
- en: It’s also hard to justify or explain the architecture or parameter settings
    discovered by a model search system. This is especially true if the optimal (in
    validation terms) architecture is discarded (possibly because of brittleness)
    and an alternative is chosen for some other reason. The process to get to this
    is now compromised with a gap. Where did the second best come from? Why was this
    route or tree chosen? The team decided to use an automated search and then ignored
    it; it’s reasonable, but it can look odd to someone outside the process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同样难以证明或解释模型搜索系统发现的架构或参数设置。如果最优（在验证意义上）架构被丢弃（可能是因为脆弱性）而选择替代方案，则尤其如此。现在，到达这个目标的过程已经受到一个差距的损害。第二好的结果是从哪里来的？为什么选择了这条路线或树？团队决定使用自动搜索，然后又忽略了它；这是合理的，但对外部的人来说可能看起来很奇怪。
- en: Another significant problem is that AutoML can be expensive in terms of compute
    resources. Although it can be a worthwhile investment, it is important to balance
    the cost of the learning process with the gains that are made in terms of model
    refinement, especially if the model is quickly discovered using a deliberate human-driven
    process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著问题是，AutoML在计算资源方面可能成本高昂。尽管这可能是一项值得的投资，但平衡学习过程成本与通过模型优化获得的收益非常重要，尤其是如果模型是通过人为驱动的故意过程快速发现的。
- en: What is a fraction of a percentage improvement for this application worth in
    terms of all the money and resources that producing it requires? A more hard-nosed
    calculation is to balance the costs between the hours spent by expensive data
    scientists driving the process and the costs in money and energy consumption of
    an AutoML process.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个应用来说，百分比改善的微小部分在所有所需金钱和资源中值多少钱？更严谨的计算是在花费昂贵的数据科学家推动过程的时间和AutoML过程在金钱和能源消耗上的成本之间进行平衡。
- en: AutoML is useful in some contexts though. Using an AutoML or model search process
    at the beginning of modelling to determine what kind of performance may be wrung
    out of a data set can be a useful way of generating a benchmark for a more systematic
    and purposeful modeling process. If a systematically derived model gets close
    to the final performance of the AutoML model, then you can know to stop; it’s
    as good as it’s going to get.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，AutoML在某些情况下是有用的。在建模初期使用AutoML或模型搜索过程来确定数据集可能达到的性能，可以是一种为更系统化和有目的的建模过程生成基准的有用方法。如果一个系统导出的模型接近AutoML模型的最终性能，那么你可以知道停止；它已经足够好了。
- en: Conversely, if AutoML fails, then this is a good indicator that no good model
    can be derived from the data. More data or better feature engineering might be
    required. In this case, you have a useful discussion point with the customer.
    It’s not necessarily that the team lacks the skills to do the ML required for
    the project or that you are being unlucky. It may be that the failure of AutoML
    indicates that the project simply isn’t going to be amenable to ML technology
    as it stands.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果AutoML失败了，那么这通常是一个很好的迹象，表明从数据中无法导出好的模型。可能需要更多数据或更好的特征工程。在这种情况下，你与客户有一个有用的讨论点。这并不一定意味着团队缺乏完成项目所需的ML技能，或者你运气不佳。可能是因为AutoML的失败表明，项目本身在当前状态下并不适合ML技术。
- en: Another use of AutoML is to try an optimization of a model that has been developed
    to gauge how much further opportunity there may be to improve it. Also, observe
    where the optimized model is performing better than the developed model that is
    at hand.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的另一个用途是尝试优化一个已经开发的模型，以评估还有多少机会可以进一步改进它。同时，观察优化后的模型在哪里比手头的开发模型表现更好。
- en: 7.5 Stinky, dirty, no good, smelly models
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 有味道、脏、不好、有味道的模型
- en: Model smell—this is like the “bad smells” that software engineers talk about
    with source code. In chapter 6, we discussed the other half of the modelling process
    in detail. Before we get to the hard-nosed stats and straight-laced explanations
    of why you should select one model over another, it’s good to talk about model
    smell.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型“味道”——这就像软件工程师在源代码中谈论的“坏味道”。在第6章中，我们详细讨论了建模过程的另一半。在我们深入探讨为什么你应该选择一个模型而不是另一个模型的硬核统计数据和直截了当的解释之前，讨论模型“味道”是很有益的。
- en: Models that perform suspiciously well or suspiciously badly become an unexpectedly
    large or implausibly small smell. They stink and are dirty, and the right thing
    to do is dig into it and find out why. Then, having figured it out, you’ll need
    to drag the model outside and beat it with a stick until it stops twitching. Then
    get rid of it (often throwing it in a local pond is a good move, but you didn’t
    hear that here).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表现可疑好或可疑差的模型会变得意外地大或小，它们有味道，很脏。正确的方法是深入调查原因。然后，弄清楚原因后，你需要把模型拖出来，用棍子打它，直到它停止抽搐。然后将其丢弃（通常把模型扔进当地的池塘是个好主意，但这里没有提到这一点）。
- en: The problem is that a stinky model can have nice performance stats associated
    with it, which is why being on the alert for such things is important. What is
    it for a model to smell?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于一个糟糕的模型可能伴随着一些令人满意的表现统计数据，这就是为什么保持警惕如此重要的原因。一个模型为什么会“有味道”？
- en: During evaluation on the validation data, models that smell perform erratically
    and inconsistently. Small changes to their parameters create large changes in
    their performance. This is just not the nice behavior that we would like to see,
    and it flags that there is something badly wrong.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证数据上的评估过程中，有“味道”的模型表现不稳定和不可靠。对它们参数的微小改变会导致它们性能的巨大变化。这并不是我们希望看到的好行为，这表明有问题。
- en: Models smell because they work better than you expected or because they work
    even though you know that there is something wrong with them.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型有“味道”，可能是因为它们比预期的表现更好，或者尽管你知道它们有问题，但它们仍然可以工作。
- en: Models smell when they stand out from all the other models that are like them,
    when a small tweak in some hyperparameter makes a huge difference in performance.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型在表现上与其他类似模型明显不同，或者对某些超参数的微小调整在性能上产生巨大差异时，模型就有“味道”。
- en: Sometimes, generating a suspect model with some of the properties in the previous
    list points to some unexpected regularity in the domain. This can be the spur
    for a new line of systematic modelling investigation. That’s fine; it sticks with
    the principle of documenting the experiments up front and then recording the results,
    and if it turns out that the model was aberrant after all, it can be safely discarded.
    It is more likely that the model smell is caused by a bug in a pipeline that trashes
    the data, because the implementation in the library or toolkit has an issue, or
    because you have made a configuration mistake.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '有时，生成具有前述列表中一些特性的可疑模型可能指向领域中的某些意外规律。这可能是推动新的系统建模调查的刺激。这是可以的；它遵循了事先记录实验并记录结果的原则，如果最终发现模型确实异常，它可以安全地被丢弃。更有可能的是，模型气味是由管道中的错误引起的，因为库或工具包的实现有问题，或者因为你犯了配置错误。 '
- en: Most commonly, all the evaluation that you are doing with the validation data
    from the training set is just not what you think it is. There’s a data leak, or
    time travels in the training or validation set. Alternatively, the model has over-fitted
    and is simply memorized data. As soon as the model is used on an unseen set, it’ll
    fail because there is no generalization.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的情况是，你使用训练集的验证数据进行的所有评估根本不是你所想的。存在数据泄露，或者在训练集或验证集中时间旅行了。或者，模型过度拟合，只是简单地记忆了数据。一旦模型被用于未见过的数据集，它就会失败，因为没有泛化。
- en: This is why it’s so important to be on the alert for model smell. Because this
    kind of model appears to work well, its appearance can seem to rescue you and
    the team from the black hole of an ML project that simply fails. It’s incredibly
    easy to grab hold of the fabulous results that the stinky model produces and move
    on to productionization. This completely understandable behavior is disastrous.
    If you catch a model spell early, then you can use it to find the bugs in your
    pipeline that need fixing. Because of the care and effort that you and the team
    have put into the project’s infrastructure and process, once those issues are
    fixed, rerunning your experiments and investigations should be straightforward
    and fast. There is a good chance that after a few iterations, your models will
    start to be robust. The stink will go away!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么保持对模型气味的警觉如此重要的原因。因为这种模型看起来工作得很好，它的外观似乎可以让你和团队从失败的机器学习项目的黑洞中解脱出来。抓住这个臭气熏天的模型产生的美妙结果并转向生产化是非常容易的。这种完全可以理解的行为是灾难性的。如果你能及早发现模型的问题，那么你可以用它来找到需要修复的管道中的错误。由于你和团队在项目的基础设施和流程上投入了精力和努力，一旦这些问题得到解决，重新运行你的实验和调查应该简单快捷。有很大可能性，经过几次迭代后，你的模型将开始变得稳健。臭味将消失！
- en: 'In this chapter, we explained the setup for the modelling activity and outlined
    the forces and processes of model design. Then we introduced the actual process
    for the modelling activity and the infrastructure needed to do that. In chapter
    8, the other half of the modelling story is covered: the evaluation and selection
    of the model that we think will do the job for us. You’ll also find out how the
    team in The Bike Shop got on with their modelling challenges.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了建模活动的设置，概述了模型设计的力量和过程。然后我们介绍了建模活动的实际过程以及完成这些所需的架构。在第8章中，我们将覆盖建模故事的另一半：评估和选择我们认为将为我们工作的模型。你还将了解到The
    Bike Shop团队在建模挑战中的表现。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Create informative features for consumption by the ML algorithms in the modelling
    phase.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在建模阶段为机器学习算法创建信息性特征。
- en: Create additional training data with data augmentation to support more robust
    models.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据增强创建额外的训练数据以支持更稳健的模型。
- en: Develop an understanding of the design forces on a model.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型设计中的设计力。
- en: Make purposeful and effective choices about the components of models that you
    want to develop.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对你想要开发的模型组件做出有目的和有效的选择。
- en: Understand and use inductive bias to inform your approach to modelling. Consider
    using hierarchical, grid-based, recurrent, graph-based, or attention-based structures.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并使用归纳偏差来指导你的建模方法。考虑使用分层、基于网格、循环、基于图或基于注意力的结构。
- en: Determine when to use composite models and how to structure them effectively
    to solve the problem at hand.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定何时使用复合模型以及如何有效地构建它们以解决当前问题。
- en: Structure and manage a controlled and purposeful modelling process.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化和管理一个受控且目的明确的建模过程。
- en: Track and manage the evolution of models using automated tools.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动化工具跟踪和管理模型的演变。
- en: Detect and determine models that should be immediately rejected based on their
    behavior or structure.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据其行为或结构检测和确定应立即拒绝的模型。
