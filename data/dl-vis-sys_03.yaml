- en: 2 Deep learning and neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 深度学习和神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding perceptrons and multilayer perceptrons
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解感知器和多层感知器
- en: Working with the different types of activation functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同类型的激活函数
- en: Training networks with feedforward, error functions, and error optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前向传播、误差函数和误差优化来训练网络
- en: Performing backpropagation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行反向传播
- en: 'In the last chapter, we discussed the computer vision (CV) pipeline components:
    the input image, preprocessing, extracting features, and the learning algorithm
    (classifier). We also discussed that in traditional ML algorithms, we manually
    extract features that produce a vector of features to be classified by the learning
    algorithm, whereas in deep learning (DL), neural networks act as both the feature
    extractor and the classifier. A neural network automatically recognizes patterns
    and extracts features from the image and classifies them into labels (figure 2.1).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了计算机视觉（CV）管道组件：输入图像、预处理、提取特征和学习算法（分类器）。我们还讨论了在传统的机器学习（ML）算法中，我们手动提取特征，生成一个特征向量，由学习算法进行分类，而在深度学习（DL）中，神经网络既是特征提取器又是分类器。神经网络自动识别模式并从图像中提取特征，并将它们分类为标签（图2.1）。
- en: '![](../Images/2-1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-1.png)'
- en: Figure 2.1 Traditional ML algorithms require manual feature extraction. A deep
    neural network automatically extracts features by passing the input image through
    its layers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 传统的机器学习算法需要手动提取特征。深度神经网络通过其层传递输入图像来自动提取特征。
- en: 'In this chapter, we will take a short pause from the CV context to open the
    DL algorithm box from figure 2.1\. We will dive deeper into how neural networks
    learn features and make predictions. Then, in the next chapter, we will come back
    to CV applications with one of the most popular DL architectures: convolutional
    neural networks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从CV背景中短暂休息，打开图2.1中的DL算法框。我们将深入探讨神经网络如何学习特征和做出预测。然后，在下一章中，我们将回到CV应用，介绍最流行的DL架构之一：卷积神经网络。
- en: 'The high-level layout of this chapter is as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的高级布局如下：
- en: 'We will begin with the most basic component of the neural network: the perceptron,
    a neural network that contains only one neuron.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从神经网络最基本的部分开始：感知器，这是一个只包含一个神经元的神经网络。
- en: 'Then we will move on to a more complex neural network architecture that contains
    hundreds of neurons to solve more complex problems. This network is called a multilayer
    perceptron (MLP), where neurons are stacked in hidden layers. Here, you will learn
    the main components of the neural network architecture: the input layer, hidden
    layers, weight connections, and output layer.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将转向一个包含数百个神经元的更复杂的神经网络架构，以解决更复杂的问题。这个网络被称为多层感知器（MLP），其中神经元堆叠在隐藏层中。在这里，你将学习神经网络架构的主要组成部分：输入层、隐藏层、权重连接和输出层。
- en: 'You will learn that the network training process consists of three main steps:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解到网络训练过程包括三个主要步骤：
- en: Feedforward operation
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向操作
- en: Calculating the error
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差
- en: 'Error optimization: using backpropagation and gradient descent to select the
    most optimum parameters that minimize the error function'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 误差优化：使用反向传播和梯度下降来选择最优化参数，以最小化误差函数
- en: 'We will dive deep into each of these steps. You will see that building a neural
    network requires making necessary design decisions: choosing an optimizer, cost
    function, and activation functions, as well as designing the architecture of the
    network, including how many layers should be connected to each other and how many
    neurons should be in each layer. Ready? Let’s get started!'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨这些步骤的每一个。你会发现构建神经网络需要做出必要的设计决策：选择优化器、损失函数和激活函数，以及设计网络架构，包括应该有多少层相互连接以及每层应该有多少个神经元。准备好了吗？让我们开始吧！
- en: '![](../Images/2-2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-2.png)'
- en: Figure 2.2 An artificial neural network consists of layers of nodes, or neurons
    connected with edges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 人工神经网络由节点层组成，节点或神经元通过边连接。
- en: 2.1 Understanding perceptrons
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解感知器
- en: Let’s take a look at the artificial neural network (ANN) diagram from chapter
    1 (figure 2.2). You can see that ANNs consist of many neurons that are structured
    in layers to perform some kind of calculations and predict an output. This architecture
    can be also called a multilayer perceptron, which is more intuitive because it
    implies that the network consists of perceptrons structured in multiple layers.
    Both terms, MLP and ANN, are used interchangeably to describe this neural network
    architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第1章（图2.2）中的人工神经网络（ANN）图。你可以看到，ANN由许多按层结构排列的神经元组成，以执行某种计算并预测输出。这种架构也可以称为多层感知器，因为它更直观，因为它暗示网络由多层感知器组成。MLP和ANN这两个术语可以互换使用，以描述这种神经网络架构。
- en: 'In the MLP diagram in figure 2.2, each node is called a neuron. We will discuss
    how MLP networks work soon, but first let’s zoom in on the most basic component
    of the neural network: the perceptron. Once you understand how a single perceptron
    works, it will become more intuitive to understand how multiple perceptrons work
    together to learn data features.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.2中的MLP图中，每个节点被称为神经元。我们很快将讨论MLP网络是如何工作的，但首先让我们聚焦于神经网络最基本的部分：感知器。一旦你理解了单个感知器的工作原理，理解多个感知器如何协同工作以学习数据特征就会变得更加直观。
- en: 2.1.1 What is a perceptron?
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 什么是感知器？
- en: The most simple neural network is the perceptron, which consists of a single
    neuron. Conceptually, the perceptron functions in a manner similar to a biological
    neuron (figure 2.3). A biological neuron receives electrical signals from its
    dendrites, modulates the electrical signals in various amounts, and then fires
    an output signal through its synapses only when the total strength of the input
    signals exceeds a certain threshold. The output is then fed to another neuron,
    and so forth.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经网络是感知器，它由单个神经元组成。从概念上讲，感知器的工作方式类似于生物神经元（图2.3）。生物神经元从其树突接收电信号，以各种程度调节电信号，然后仅在输入信号的总体强度超过某个阈值时通过其突触发出输出信号。然后输出被馈送到另一个神经元，依此类推。
- en: 'To model the biological neuron phenomenon, the artificial neuron performs two
    consecutive functions: it calculates the weighted sum of the inputs to represent
    the total strength of the input signals, and it applies a step function to the
    result to determine whether to fire the output 1 if the signal exceeds a certain
    threshold or 0 if the signal doesn’t exceed the threshold.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟生物神经元的特性，人工神经元执行两个连续的功能：它计算输入的加权总和来表示输入信号的总体强度，然后对结果应用一个阶跃函数，以确定是否在信号超过某个阈值时输出1，如果没有超过阈值则输出0。
- en: '![](../Images/2-3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-3.png)'
- en: Figure 2.3 Artificial neurons were inspired by biological neurons. Different
    neurons are connected to each other by synapses that carry information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 人工神经元是受生物神经元启发的。不同的神经元通过携带信息的突触相互连接。
- en: As we discussed in chapter 1, not all input features are equally useful or important.
    To represent that, each input node is assigned a weight value, called its connection
    weight, to reflect its importance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章中讨论的，并非所有输入特征都同等有用或重要。为了表示这一点，每个输入节点都被分配一个权重值，称为其连接权重，以反映其重要性。
- en: Connection weights
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 连接权重
- en: Not all input features are equally important (or useful) features. Each input
    feature (*x*[1]) is assigned its own weight (*w*[1]) that reflects its importance
    in the decision-making process. Inputs assigned greater weight have a greater
    effect on the output. If the weight is high, it amplifies the input signal; and
    if the weight is low, it diminishes the input signal. In common representations
    of neural networks, the weights are represented by lines or edges from the input
    node to the perceptron.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有输入特征都是同等重要（或有用）的特征。每个输入特征（*x*[1]）都被分配其自己的权重（*w*[1]），以反映其在决策过程中的重要性。分配了更高权重的输入对输出的影响更大。如果权重高，它会放大输入信号；如果权重低，它会减弱输入信号。在神经网络的一般表示中，权重由从输入节点到感知器的线条或边表示。
- en: For example, if you are predicting a house price based on a set of features
    like size, neighborhood, and number of rooms, there are three input features (*x*[1],
    *x*[2], and *x*[3]). Each of these inputs will have a different weight value that
    represents its effect on the final decision. For example, if the size of the house
    has double the effect on the price compared with the neighborhood, and the neighborhood
    has double the effect compared with the number of rooms, you will see weights
    something like 8, 4, and 2, respectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你是根据一组特征（如大小、邻里和房间数量）预测房价，那么有三个输入特征（*x*[1]、*x*[2]和*x*[3]）。这些输入中的每一个都将有不同的权重值，代表其对最终决策的影响。例如，如果房屋的大小对价格的影响是邻里的两倍，而邻里对房间数量的影响是两倍，那么你将看到权重值分别为8、4和2。
- en: How the connection values are assigned and how the learning happens is the core
    of the neural network training process. This is what we will discuss for the rest
    of this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 连接值是如何分配的，以及学习是如何发生的，这是神经网络训练过程的核心。这是我们将在本章剩余部分讨论的内容。
- en: 'In the perceptron diagram in figure 2.4, you can see the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.4的感知器图中，你可以看到以下内容：
- en: Input vector --The feature vector that is fed to the neuron. It is usually denoted
    with an uppercase *x* to represent a vector of inputs (*x*[1], *x*[2], . . .,
    *x[n]*).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入向量 --输入到神经元的特征向量。通常用大写*x*表示输入向量的输入（*x*[1]、*x*[2]，……，*x*[n]*）。
- en: Weights vector --Each *x*[1] is assigned a weight value *w*[1] that represents
    its importance to distinguish between different input datapoints.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重向量 --每个*x*[1]被分配一个权重值*w*[1]，它表示其在区分不同输入数据点中的重要性。
- en: 'Neuron functions --The calculations performed within the neuron to modulate
    the input signals: the weighted sum and step activation function.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元函数 --在神经元内进行的计算，以调节输入信号：加权总和和阶跃激活函数。
- en: Output --Controlled by the type of activation function you choose for your network.
    There are different activation functions, as we will discuss in detail in this
    chapter. For a step function, the output is either 0 or 1\. Other activation functions
    produce probability output or float numbers. The output node represents the perceptron
    prediction.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出 --由你为网络选择的激活函数类型控制。有不同类型的激活函数，我们将在本章详细讨论。对于阶跃函数，输出为0或1。其他激活函数产生概率输出或浮点数。输出节点代表感知器的预测。
- en: '![](../Images/2-4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-4.png)'
- en: Figure 2.4 Input vectors are fed to the neuron, with weights assigned to represent
    importance. Calculations performed within the neuron are weighted sum and activation
    functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 输入向量被输入到神经元中，并分配权重以表示其重要性。神经元内部进行的计算是加权总和和激活函数。
- en: Let’s take a deeper look at the weighted sum and step function calculations
    that happen inside the neuron.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解神经元内部发生的加权总和和阶跃函数计算。
- en: Weighted sum function
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权总和函数
- en: 'Also known as a linear combination, the weighted sum function is the sum of
    all inputs multiplied by their weights, and then added to a bias term. This function
    produces a straight line represented in the following equation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为线性组合，加权总和函数是所有输入乘以其权重之和，然后加上一个偏置项。此函数产生以下方程表示的直线：
- en: '*z* = Σ*x[i]* · *w[i]* + *b* (bias)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = Σ*x[i]* · *w[i]* + *b*（偏置）'
- en: '*z* = *x*[1] · *w*[1] + *x*[2] · *w*[2] + *x*[3] · *w*[3] + … + xn · *w[n]*
    + *b*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = *x*[1] · *w*[1] + *x*[2] · *w*[2] + *x*[3] · *w*[3] + … + *x*[n] · *w*[n]
    + *b*'
- en: 'Here is how we implement the weighted sum in Python:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何在Python中实现加权总和的：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ *x* is the input vector (uppercase X), w is the weights vector, and b is the
    y-intercept.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ *x* 是输入向量（大写X），w是权重向量，b是y轴截距。
- en: What is a bias in the perceptron, and why do we add it?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器中的偏置是什么，为什么我们要添加它？
- en: 'Let’s brush up our memory on some linear algebra concepts to help understand
    what’s happening under the hood. Here is the function of the straight line:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们复习一下线性代数的一些概念，以帮助理解底层发生了什么。以下是直线的函数：
- en: '![](../Images/2-unnumb-1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-1.png)'
- en: The equation of a straight line
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 直线的方程
- en: 'The function of a straight line is represented by the equation (*y = mx + b*),
    where *b* is the y-intercept. To be able to define a line, you need two things:
    the slope of the line and a point on the line. The bias is that point on the y-axis.
    Bias allows you to move the line up and down on the y-axis to better fit the prediction
    with the data. Without the bias (*b*), the line always has to go through the origin
    point (0,0), and you will get a poorer fit. To visualize the importance of bias,
    look at the graph in the above figure and try to separate the circles from the
    star using a line that passes through the origin (0,0). It is not possible.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 直线的函数由方程 (*y = mx + b*) 表示，其中 *b* 是y轴截距。要能够定义一条线，你需要两样东西：线的斜率和线上的一个点。偏差就是y轴上的那个点。偏差允许你将线在y轴上上下移动，以更好地拟合预测与数据。没有偏差（*b*），线总是必须通过原点（0,0），这将导致拟合较差。为了可视化偏差的重要性，请看上面的图表，并尝试使用通过原点（0,0）的线将圆圈与星号分开。这是不可能的。
- en: The input layer can be given biases by introducing an extra input node that
    always has a value of 1, as you can see in the next figure. In neural networks,
    the value of the bias (*b*) is treated as an extra weight and is learned and adjusted
    by the neuron to minimize the cost function, as we will learn in the following
    sections of this chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层可以通过引入一个始终具有值为1的额外输入节点来给予偏差，正如你可以在下一个图中看到的那样。在神经网络中，偏差（*b*）的值被视为一个额外的权重，并由神经元学习并调整以最小化成本函数，正如我们将在本章的后续部分中学习的那样。
- en: '![](../Images/2-unnumb-2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-2.png)'
- en: The input layer can be given biases by introducing an extra input that always
    has a value of 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层可以通过引入一个始终具有值为1的额外输入来给予偏差。
- en: Step activation function
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步激活函数
- en: In both artificial and biological neural networks, a neuron does not just output
    the bare input it receives. Instead, there is one more step, called an activation
    function; this is the decision-making unit of the brain. In ANNs, the activation
    function takes the same weighted sum input from before (*z* = Σ*x[i]* · *w[i]*
    + *b*) and activates (fires) the neuron if the weighted sum is higher than a certain
    threshold. This activation happens based on the activation function calculations.
    Later in this chapter, we’ll review the different types of activation functions
    and their general purpose in the broader context of neural networks. The simplest
    activation function used by the perceptron algorithm is the step function that
    produces a binary output (0 or 1). It basically says that if the summed input
    ≥ 0, it “fires” (output = 1); else (summed input < 0), it doesn’t fire (output
    = 0) (figure 2.5).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工和生物神经网络中，一个神经元不仅仅输出它接收到的原始输入。相反，还有一个额外的步骤，称为激活函数；这是大脑的决策单元。在ANNs中，激活函数接受之前相同的加权求和输入（*z*
    = Σ*x[i]* · *w[i]* + *b*）并在加权求和高于某个阈值时激活（触发）神经元。这种激活基于激活函数的计算。在本章的后面部分，我们将回顾不同类型的激活函数及其在神经网络更广泛背景中的通用目的。感知器算法使用的最简单的激活函数是步函数，它产生二进制输出（0或1）。它基本上说，如果求和输入
    ≥ 0，它“触发”（输出 = 1）；否则（求和输入 < 0），它不触发（输出 = 0）（图2.5）。
- en: '![](../Images/2-5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-5.png)'
- en: Figure 2.5 The step function produces a binary output (0 or 1). If the summed
    input ≥ 0, it “fires” (output = 1); else (summed input < 0) it doesn't fire (output
    = 0).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 步函数产生二进制输出（0或1）。如果求和输入 ≥ 0，它“触发”（输出 = 1）；否则（求和输入 < 0）它不触发（输出 = 0）](../Images/2-5.png)'
- en: 'This is how the step function looks in Python:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Python中步函数看起来像什么：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ *z* is the weighted sum = σ *x[i]* · *w[i]* + b
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ *z* 是加权求和 = σ *x[i]* · *w[i]* + b
- en: 2.1.2 How does the perceptron learn?
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 感知器是如何学习的？
- en: The perceptron uses trial and error to learn from its mistakes. It uses the
    weights as knobs by tuning their values up and down until the network is trained
    (figure 2.6).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器通过试错法从错误中学习。它通过调整权重的值上下移动，直到网络被训练（图2.6）。
- en: '![](../Images/2-6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-6.png)'
- en: Figure 2.6 Weights are tuned up and down during the learning process to optimize
    the value of the loss function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 在学习过程中调整权重上下移动以优化损失函数的值。
- en: 'The perceptron’s learning logic goes like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的学习逻辑是这样的：
- en: 'The neuron calculates the weighted sum and applies the activation function
    to make a prediction *ŷ*. This is called the feedforward process:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元计算加权求和并应用激活函数来做出预测 *ŷ*。这被称为前馈过程：
- en: '*ŷ* = activation(Σ*x[i]* · *w[i]* + *b*)'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ* = activation(Σ*x[i]* · *w[i]* + *b*)'
- en: 'It compares the output prediction with the correct label to calculate the error:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将输出预测与正确标签进行比较来计算错误：
- en: error = *y - ŷ*
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误 = *y - ŷ*
- en: It then updates the weight. If the prediction is too high, it adjusts the weight
    to make a lower prediction the next time, and vice versa.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它会更新权重。如果预测过高，它会调整权重以在下一次做出更低的预测，反之亦然。
- en: Repeat!
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复！
- en: This process is repeated many times, and the neuron continues to update the
    weights to improve its predictions until step 2 produces a very small error (close
    to zero), which means the neuron’s prediction is very close to the correct value.
    At this point, we can stop the training and save the weight values that yielded
    the best results to apply to future cases where the outcome is unknown.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复多次，神经元会持续更新权重以改善其预测，直到步骤2产生一个非常小的错误（接近零），这意味着神经元的预测非常接近正确值。此时，我们可以停止训练，并将产生最佳结果的权重值保存下来，以应用于未来结果未知的情况。
- en: 2.1.3 Is one neuron enough to solve complex problems?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 一个神经元足够解决复杂问题吗？
- en: The short answer is no, but let’s see why. The perceptron is a linear function.
    This means the trained neuron will produce a straight line that separates our
    data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是不，但让我们看看原因。感知器是一个线性函数。这意味着训练后的神经元将产生一条直线来分离我们的数据。
- en: 'Suppose we want to train a perceptron to predict whether a player will be accepted
    into the college squad. We collect all the data from previous years and train
    the perceptron to predict whether players will be accepted based on only two features
    (height and weight). The trained perceptron will find the best weights and bias
    values to produce the straight line that best separates the accepted from non-accepted
    (best fit). The line has this equation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要训练一个感知器来预测一个玩家是否会被大学队接受。我们收集了前几年的所有数据，并训练感知器根据仅有的两个特征（身高和体重）来预测玩家是否会被接受。训练后的感知器将找到最佳权重和偏差值，以产生最佳拟合的直线，将接受者与非接受者分开（最佳拟合）。这条线的方程如下：
- en: '*z* = height · *w*[1] + age · *w*[2] + *b*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = 身高 · *w*[1] + 年龄 · *w*[2] + *b*'
- en: 'After the training is complete on the training data, we can start using the
    perceptron to predict with new players. When we get a player who is 150 cm in
    height and 12 years old, we compute the previous equation with the values (150,
    12). When plotted in a graph (figure 2.7), you can see that it falls below the
    line: the neuron is predicting that this player will not be accepted. If it falls
    above the line, then the player will be accepted.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上完成训练后，我们可以开始使用感知器来预测新玩家的数据。当我们得到一个身高150厘米、12岁的玩家时，我们用值（150，12）计算前面的方程。当在图表（图2.7）中绘制时，你可以看到它低于这条线：神经元预测这个玩家不会被接受。如果它高于这条线，那么这个玩家将被接受。
- en: '![](../Images/2-7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-7.png)'
- en: Figure 2.7 Linearly separable data can be separated by a straight line.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 线性可分的数据可以通过一条直线分离。
- en: In figure 2.7, the single perceptron works fine because our data was linearly
    separable. This means the training data can be separated by a straight line. But
    life isn’t always that simple. What happens when we have a more complex dataset
    that cannot be separated by a straight line (nonlinear dataset)?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.7中，单个感知器工作得很好，因为我们的数据是线性可分的。这意味着训练数据可以通过一条直线分离。但生活并不总是那么简单。当我们有一个更复杂的、不能通过直线分离的数据集（非线性数据集）时会发生什么？
- en: As you can see in figure 2.8, a single straight line will not separate our training
    data. We say that it does not fit our data. We need a more complex network for
    more complex data like this. What if we built a network with two perceptrons?
    This would produce two lines. Would that help us separate the data better?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图2.8中看到的，一条单独的直线无法分离我们的训练数据。我们说它不适合我们的数据。我们需要一个更复杂的网络来处理这种更复杂的数据。如果我们构建一个包含两个感知器的网络会怎样？这将产生两条线。这会帮助我们更好地分离数据吗？
- en: Okay, this is definitely better than the straight line. But, I still see some
    color mispredictions. Can we add more neurons to make the function fit better?
    Now you are getting it. Conceptually, the more neurons we add, the better the
    network will fit our training data. In fact, if we add too many neurons, this
    will make the network overfit the training data (not good). But we will talk about
    this later. The general rule here is that the more complex our network is, the
    better it learns the features of our data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这确实比直线要好。但我仍然看到一些颜色预测错误。我们能否添加更多的神经元来使函数更好地拟合？现在你明白了。从概念上讲，我们添加的神经元越多，网络就越能拟合我们的训练数据。事实上，如果我们添加太多的神经元，这会使网络过度拟合训练数据（不好）。但我们会稍后讨论这个问题。这里的普遍规则是，我们的网络越复杂，它就越能学习我们数据的特点。
- en: '![](../Images/2-8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-8.png)'
- en: Figure 2.8 In a nonlinear dataset, a single straight line cannot separate the
    training data. A network with two perceptrons can produce two lines and help separate
    the data further in this example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 在非线性数据集中，一条直线无法分离训练数据。在这个例子中，一个具有两个感知器的网络可以产生两条直线，并帮助进一步分离数据。
- en: 2.2 Multilayer perceptrons
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 多层感知器
- en: We saw that a single perceptron works great with simple datasets that can be
    separated by a line. But, as you can imagine, the real world is much more complex
    than that. This is where neural networks can show their full potential.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，单个感知器在可以由直线分割的简单数据集上表现良好。但是，正如你可以想象的那样，现实世界比这复杂得多。这就是神经网络可以展示其全部潜力的地方。
- en: Linear vs. nonlinear problems
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 线性与非线性问题
- en: Linear datasets--The data can be split with a single straight line.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性数据集--数据可以用一条直线分割。
- en: Nonlinear datasets--The data cannot be split with a single straight line. We
    need more than one line to form a shape that splits the data.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性数据集--数据不能仅用一条直线分割。我们需要多条直线来形成一个分割数据的形状。
- en: Look at this 2D data. In the linear problem, the stars and dots can be easily
    classified by drawing a single straight line. In nonlinear data, a single line
    will not separate both shapes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个二维数据。在线性问题中，星号和点可以通过画一条直线轻松分类。在非线性数据中，一条直线无法将两种形状分开。
- en: '![](../Images/2-unnumb-3key.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-3key.png)'
- en: Examples of linear data and nonlinear data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 线性数据和非线性数据的示例
- en: To split a nonlinear dataset, we need more than one line. This means we need
    to come up with an architecture to use tens and hundreds of neurons in our neural
    network. Let’s look at the example in figure 2.9\. Remember that a perceptron
    is a linear function that produces a straight line. So in order to fit this data,
    we try to create a triangle-like shape that splits the dark dots. It looks like
    three lines would do the job.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要分割非线性数据集，我们需要多条直线。这意味着我们需要提出一个架构，在神经网络中使用成百上千的神经元。让我们看看图2.9中的例子。记住，感知器是一个产生直线的线性函数。因此，为了拟合这些数据，我们试图创建一个类似三角形的形状，以分割暗点。看起来三条线就能完成这项工作。
- en: '![](../Images/2-9.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-9.png)'
- en: Figure 2.9 A perceptron is a linear function that produces a straight line.
    So to fit this data, we need three perceptrons to create a triangle-like shape
    that splits the dark dots.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 感知器是一个产生直线的线性函数。因此，为了拟合这些数据，我们需要三个感知器来创建一个类似三角形的形状，以分割暗点。
- en: Figure 2.9 is an example of a small neural network that is used to model nonlinear
    data. In this network, we used three neurons stacked together in one layer called
    a hidden layer, so called because we don’t see the output of these layers during
    the training process.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 是一个用于模拟非线性数据的小型神经网络示例。在这个网络中，我们使用了一个隐藏层，该层由三个堆叠在一起的神经元组成，之所以称为隐藏层，是因为在训练过程中我们看不到这些层的输出。
- en: 2.2.1 Multilayer perceptron architecture
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 多层感知器架构
- en: We’ve seen how a neural network can be designed to have more than one neuron.
    Let’s expand on this idea with a more complex dataset. The diagram in figure 2.10
    is from the Tensorflow playground website ([https://playground.tensorflow.org](https://playground.tensorflow.org/)).
    We try to model a spiral dataset to distinguish between two classes. In order
    to fit this dataset, we need to build a neural network that contains tens of neurons.
    A very common neural network architecture is to stack the neurons in layers on
    top of each other, called hidden layers. Each layer has n number of neurons. Layers
    are connected to each other by weight connections. This leads to the multilayer
    perceptron (MLP) architecture in the figure.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何设计一个包含多个神经元的神经网络。让我们通过一个更复杂的数据集来扩展这个想法。图2.10中的图来自Tensorflow playground网站([https://playground.tensorflow.org](https://playground.tensorflow.org/))。我们试图模拟一个螺旋数据集以区分两个类别。为了拟合这个数据集，我们需要构建一个包含数十个神经元的神经网络。一个非常常见的神经网络架构是将神经元堆叠在彼此之上，称为隐藏层。每一层有n个神经元。层通过权重连接相互连接。这导致了图中的多层感知器（MLP）架构。
- en: 'The main components of the neural network architecture are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构的主要组件如下：
- en: '*Input layer* --Contains the feature vector.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入层* --包含特征向量。'
- en: '*Hidden layers* --The neurons are stacked on top of each other in hidden layers.
    They are called “hidden” layers because we don’t see or control the input going
    into these layers or the output. All we do is feed the feature vector to the input
    layer and see the output coming out of the output layer.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐藏层* --神经元堆叠在隐藏层之上。它们被称为“隐藏”层，因为我们看不到或控制进入这些层的输入或输出。我们唯一做的就是将特征向量输入到输入层，并查看输出层输出的结果。'
- en: '*Weight connections (edges)* --Weights are assigned to each connection between
    the nodes to reflect the importance of their influence on the final output prediction.
    In graph network terms, these are called edges connecting the nodes.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*权重连接（边）* --权重被分配给节点之间的每个连接，以反映它们对最终输出预测的影响的重要性。在图网络术语中，这些被称为连接节点的边。'
- en: '![](../Images/2-10.png)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/2-10.png)'
- en: Figure 2.10 Tensorflow playground example representation of the feature learning
    in a deep neural network
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.10 Tensorflow playground示例表示深度神经网络中的特征学习
- en: '*Output laye*r --We get the answer or prediction from our model from the output
    layer. Depending on the setup of the neural network, the final output may be a
    real-valued output (regression problem) or a set of probabilities (classification
    problem). This is determined by the type of activation function we use in the
    neurons in the output layer. We’ll discuss the different types of activation functions
    in the next section.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层* --我们从输出层得到答案或预测。根据神经网络的结构设置，最终输出可能是一个实值输出（回归问题）或一组概率（分类问题）。这取决于我们在输出层神经元中使用的激活函数的类型。我们将在下一节讨论不同类型的激活函数。'
- en: We discussed the input layer, weights, and output layer. The next area of this
    architecture is the hidden layers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了输入层、权重和输出层。这个架构的下一个区域是隐藏层。
- en: 2.2.2 What are hidden layers?
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 什么是隐藏层？
- en: This is where the core of the feature-learning process takes place. When you
    look at the hidden layer nodes in figure 2.10, you see that the early layers detect
    simple patterns to learn low-level features (straight lines). Later layers detect
    patterns within patterns to learn more complex features and shapes, then patterns
    within patterns within patterns, and so on. This concept will come in handy when
    we discuss convolutional networks in later chapters. For now, know that, in neural
    networks, we stack hidden layers to learn complex features from each other until
    we fit our data. So when you are designing your neural network, if your network
    is not fitting the data, the solution could be adding more hidden layers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这是特征学习过程的核心所在。当你查看图2.10中的隐藏层节点时，你会看到早期层检测简单的模式以学习低级特征（直线）。后面的层检测模式中的模式以学习更复杂特征和形状，然后是模式中的模式，以此类推。当我们讨论卷积网络时，这个概念将很有用。现在，要知道，在神经网络中，我们堆叠隐藏层，从彼此学习复杂特征，直到我们的数据拟合。所以当你设计你的神经网络时，如果你的网络没有拟合数据，解决方案可能是添加更多的隐藏层。
- en: 2.2.3 How many layers, and how many nodes in each layer?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 每层有多少层，每层有多少节点？
- en: As a machine learning engineer, you will mostly be designing your network and
    tuning its hyperparameters. While there is no single prescribed recipe that fits
    all models, we will try throughout this book to build your hyperparameter tuning
    intuition, as well as recommend some starting points. The number of layers and
    the number of neurons in each layer are among the important hyperparameters you
    will be designing when working with neural networks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习工程师，你将主要设计你的网络并调整其超参数。虽然没有一种单一的推荐配方适用于所有模型，但在这本书的整个过程中，我们将尝试建立你的超参数调整直觉，并推荐一些起点。当你与神经网络一起工作时，层数和每层的神经元数量是你要设计的重要超参数之一。
- en: 'A network can have one or more hidden layers (technically, as many as you want).
    Each layer has one or more neurons (again, as many as you want). Your main job,
    as a machine learning engineer, is to design these layers. Usually, when we have
    two or more hidden layers, we call this a deep neural network. The general rule
    is this: the deeper your network is, the more it will fit the training data. But
    too much depth is not a good thing, because the network can fit the training data
    so much that it fails to generalize when you show it new data (overfitting); also,
    it becomes more computationally expensive. So your job is to build a network that
    is not too simple (one neuron) and not too complex for your data. It is recommended
    that you read about different neural network architectures that are successfully
    implemented by others to build an intuition about what is too simple for your
    problem. Start from that point, maybe three to five layers (if you are training
    on a CPU), and observe the network performance. If it is performing poorly (underfitting),
    add more layers. If you see signs of overfitting (discussed later), then decrease
    the number of layers. To build a sense of how neural networks perform when you
    add more layers, play around with the Tensorflow playground ([https://playground.tensorflow.org](https://playground.tensorflow.org)).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网络可以有一个或多个隐藏层（技术上，可以是你想要的任意多个）。每个层有一个或多个神经元（同样，可以是你想要的任意多个）。作为机器学习工程师，你的主要任务是设计这些层。通常，当我们有两个或更多隐藏层时，我们称之为深度神经网络。一般规则是这样的：你的网络越深，它就越能拟合训练数据。但是，过多的深度并不是好事，因为网络可以拟合训练数据到一定程度，以至于当展示新数据时无法泛化（过拟合）；同时，它也变得更加计算密集。所以你的任务是构建一个既不太简单（一个神经元）也不太复杂的数据网络。建议你阅读其他人成功实施的不同神经网络架构，以建立对问题过于简单的直觉。从这一点开始，也许三到五层（如果你在CPU上训练），观察网络性能。如果表现不佳（欠拟合），则添加更多层。如果你看到过拟合的迹象（稍后讨论），则减少层数。为了了解添加更多层时神经网络的表现，可以在Tensorflow
    playground([https://playground.tensorflow.org](https://playground.tensorflow.org))上尝试操作。
- en: Fully connected layers
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层
- en: It is important to call out that the layers in classical MLP network architectures
    are fully connected to the next hidden layer. In the following figure, notice
    that each node in a layer is connected to all nodes in the previous layer. This
    is called a fully connected network. These edges are the weights that represent
    the importance of this node to the output value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，在经典的MLP网络架构中，层与下一隐藏层是完全连接的。在下面的图中，注意一个层中的每个节点都与前一层的所有节点相连。这被称为全连接网络。这些边是表示该节点对输出值重要性的权重。
- en: '![](../Images/2-unnumb-4K.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-4K.png)'
- en: A fully connected network
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全连接网络
- en: 'In later chapters, we will discuss other variations of neural network architecture
    (like convolutional and recurrent networks). For now, know that this is the most
    basic neural network architecture, and it can be referred to by any of these names:
    ANN, MLP, fully connected network, or feedforward network.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将讨论神经网络架构的其他变体（如卷积网络和循环网络）。现在，知道这是最基本的神经网络架构，并且可以用以下任何一种名称来引用：ANN、MLP、全连接网络或前向网络。
- en: 'Let’s do a quick exercise to find out how many edges we have in our example.
    Suppose that we designed an MLP network with two hidden layers, and each has five
    neurons:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个快速练习，找出我们示例中有多少条边。假设我们设计了一个具有两个隐藏层的MLP网络，每个隐藏层有五个神经元：
- en: '`Weights_0_1`: (4 nodes in the input layer) × (5 nodes in layer 1) + 5 biases
    [1 bias per neuron] = 25 edges'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Weights_0_1`: (输入层有4个节点) × (层1有5个节点) + 5个偏置[每个神经元一个偏置] = 25条边'
- en: '`Weights_1_2`: 5 × 5 nodes + 5 biases = 30 edges'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Weights_1_2`: 5 × 5个节点 + 5个偏置 = 30条边'
- en: '`Weights_2_output`: 5 × 3 nodes + 3 bias = 18 edges'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Weights_2_output`: 5 × 3个节点 + 3个偏置 = 18条边'
- en: Total edges (weights) in this network = 73
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本网络中的总边（权重）数 = 73
- en: We have a total of 73 weights in this very simple network. The values of these
    weights are randomly initialized, and then the network performs feedforward and
    backpropagation to learn the best values of weights that most fit our model to
    the training data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个非常简单的网络中，我们总共有73个权重。这些权重的值是随机初始化的，然后网络通过前向传播和反向传播来学习最适合训练数据的权重最佳值。
- en: 'To see the number of weights in this network, try to build this simple network
    in Keras as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这个网络中的权重数量，尝试以下方式在Keras中构建这个简单的网络：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And print the model summary:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 打印模型摘要：
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2.2.4 Some takeaways from this section
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 本节的一些要点
- en: 'Let’s recap what we’ve discussed so far:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们迄今为止讨论的内容：
- en: 'We talked about the analogy between biological and artificial neurons: both
    have inputs and a neuron that does some calculations to modulate the input signals
    and create output.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了生物神经元和人工神经元之间的类比：两者都有输入和一个执行某些计算以调节输入信号并创建输出的神经元。
- en: 'We zoomed in on the artificial neuron’s calculations to explore its two main
    functions: weighted sum and the activation function.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们聚焦于人工神经元的计算，以探索其两个主要功能：加权总和和激活函数。
- en: We saw that the network assigns random weights to all the edges. These weight
    parameters reflect the usefulness (or importance) of these features on the output
    prediction.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看到网络为所有边分配随机权重。这些权重参数反映了这些特征在输出预测中的有用性（或重要性）。
- en: Finally, we saw that perceptrons contain a single neuron. They are linear functions
    that produce a straight line to split linear data. In order to split more complex
    data (nonlinear), we need to apply more than one neuron in our network to form
    a multilayer perceptron.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们看到了感知器包含单个神经元。它们是线性函数，产生一条直线来分割线性数据。为了分割更复杂的数据（非线性），我们需要在我们的网络中应用多个神经元来形成一个多层感知器。
- en: The MLP architecture contains input features, connection weights, hidden layers,
    and an output layer.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP架构包含输入特征、连接权重、隐藏层和输出层。
- en: 'We discussed the high-level process of how the perceptron learns. The learning
    process is a repetition of three main steps: feedforward calculations to produce
    a prediction (weighted sum and activation), calculating the error, and backpropagating
    the error and updating the weights to minimize the error.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了感知器学习的高级过程。学习过程是三个主要步骤的重复：前向计算以产生预测（加权总和和激活）、计算误差以及反向传播误差并更新权重以最小化误差。
- en: 'We should also keep in mind some of the important points about neural network
    hyperparameters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该记住一些关于神经网络超参数的重要观点：
- en: 'Number of hidden layers --You can have as many layers as you want, each with
    as many neurons as you want. The general idea is that the more neurons you have,
    the better your network will learn the training data. But if you have too many
    neurons, this might lead to a phenomenon called overfitting: the network learned
    the training set so much that it memorized it instead of learning its features.
    Thus, it will fail to generalize. To get the appropriate number of layers, start
    with a small network, and observe the network performance. Then start adding layers
    until you get satisfying results.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层数量 -- 你可以拥有你想要的任何数量的层，每层可以有任意数量的神经元。一般想法是，你拥有的神经元越多，你的网络将越好地学习训练数据。但是，如果你有太多的神经元，这可能会导致一种称为过拟合的现象：网络学习训练集如此之多，以至于它记住了它而不是学习其特征。因此，它将无法泛化。为了获得适当的层数，从一个小的网络开始，并观察网络性能。然后开始添加层，直到你得到满意的结果。
- en: Activation function --There are many types of activation functions, the most
    popular being ReLU and softmax. It is recommended that you use ReLU activation
    in the hidden layers and Softmax for the output layer (you will see how this is
    implemented in most projects in this book).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数 -- 有许多类型的激活函数，最流行的是ReLU和softmax。建议你在隐藏层中使用ReLU激活函数，在输出层中使用Softmax（你将在本书中的大多数项目中看到这是如何实现的）。
- en: Error function --Measures how far the network’s prediction is from the true
    label. Mean square error is common for regression problems, and cross-entropy
    is common for classification problems.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差函数 -- 衡量网络的预测与真实标签之间的距离。均方误差是回归问题的常见度量，交叉熵是分类问题的常见度量。
- en: Optimizer --Optimization algorithms are used to find the optimum weight values
    that minimize the error. There are several optimizer types to choose from. In
    this chapter, we discuss batch gradient descent, stochastic gradient descent,
    and mini-batch gradient descent. Adam and RMSprop are two other popular optimizers
    that we don’t discuss.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器 -- 优化算法用于找到最小化误差的最优权重值。有几种优化器类型可供选择。在本章中，我们讨论了批量梯度下降、随机梯度下降和小批量梯度下降。Adam和RMSprop是两种其他流行的优化器，我们在此不讨论。
- en: Batch size --Mini-batch size is the number of sub-samples given to the network,
    after which parameter update happens. Bigger batch sizes learn faster but require
    more memory space. A good default for batch size might be 32\. Also try 64, 128,
    256, and so on.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小 -- 小批量大小是指网络接收到的子样本数量，之后参数更新发生。较大的批处理大小学习速度更快，但需要更多的内存空间。批处理大小的良好默认值可能是32。也可以尝试64、128、256等。
- en: Number of epochs --The number of times the entire training dataset is shown
    to the network while training. Increase the number of epochs until the validation
    accuracy starts decreasing even when training accuracy is increasing (overfitting).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮数 --在训练过程中，整个训练数据集被展示给网络的次数。增加训练轮数，直到验证准确率开始下降，即使训练准确率在增加（过拟合）。
- en: Learning rate --One of the optimizer’s input parameters that we tune. Theoretically,
    a learning rate that is too small is guaranteed to reach the minimum error (if
    you train for infinity time). A learning rate that is too big speeds up the learning
    but is not guaranteed to find the minimum error. The default `lr` value of the
    optimizer in most DL libraries is a reasonable start to get decent results. From
    there, go down or up by one order of magnitude. We will discuss the learning rate
    in detail in chapter 4.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 --优化器的一个输入参数，我们需要调整。从理论上讲，过小的学习率保证能够达到最小误差（如果你无限期地训练）。过大的学习率会加快学习速度，但并不保证找到最小误差。大多数深度学习库中优化器的默认`lr`值是一个合理的起点，以获得不错的结果。从那里开始，可以按一个数量级上下调整。我们将在第4章中详细讨论学习率。
- en: More on hyperparameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于超参数的内容
- en: Other hyperparameters that we have not discussed yet include dropout and regularization.
    We will discuss hyperparameter tuning in detail in chapter 4, after we cover convolutional
    neural networks in chapter 3.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未讨论的其他超参数包括dropout和正则化。在第3章介绍卷积神经网络之后，我们将在第4章中详细讨论超参数调整。
- en: In general, the best way to tune hyperparameters is by trial and error. By getting
    your hands dirty with your own projects as well as learning from other existing
    neural network architectures, you will start to develop intuition about good starting
    points for your hyperparameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，调整超参数的最佳方式是通过试错。通过自己动手做项目以及从其他现有的神经网络架构中学习，你将开始培养出对超参数良好起点的直觉。
- en: Learn to analyze your network’s performance and understand which hyperparameter
    you need to tune for each symptom. And this is what we are going to do in this
    book. By understanding the reasoning behind these hyperparameters and observing
    the network performance in the projects at the end of the chapters, you will develop
    a feel for which hyperparameter to tune for a particular effect. For example,
    if you see that your error value is not decreasing and keeps oscillating, then
    you might fix that by reducing the learning rate. Or, if you see that the network
    is performing poorly in learning the training data, this might mean that the network
    is underfitting and you need to build a more complex model by adding more neurons
    and hidden layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 学习分析你网络的性能，并了解你需要调整哪个超参数来解决每个症状。这正是本书将要讨论的内容。通过理解这些超参数背后的推理，并观察章节末尾的项目中的网络性能，你将培养出对特定效果调整哪个超参数的直觉。例如，如果你看到你的错误值没有下降并且持续振荡，那么你可能通过降低学习率来解决这个问题。或者，如果你看到网络在训练数据的学习中表现不佳，这可能意味着网络欠拟合，你需要通过添加更多神经元和隐藏层来构建一个更复杂的模型。
- en: 2.3 Activation functions
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 激活函数
- en: When you are building your neural network, one of the design decisions that
    you will need to make is what activation function to use for your neurons’ calculations.
    Activation functions are also referred to as transfer functions or nonlinearities
    because they transform the linear combination of a weighted sum into a nonlinear
    model. An activation function is placed at the end of each perceptron to decide
    whether to activate this neuron.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在构建你的神经网络时，你需要做出的设计决策之一是为你神经元的计算选择哪种激活函数。激活函数也被称为转移函数或非线性，因为它们将加权求和的线性组合转换成非线性模型。激活函数被放置在每个感知器的末尾，以决定是否激活这个神经元。
- en: Why use activation functions at all? Why not just calculate the weighted sum
    of our network and propagate that through the hidden layers to produce an output?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一定要使用激活函数呢？为什么不能只是计算我们网络的加权求和，并通过隐藏层传播以产生输出呢？
- en: The purpose of the activation function is to introduce nonlinearity into the
    network. Without it, a multilayer perceptron will perform similarly to a single
    perceptron no matter how many layers we add. Activation functions are needed to
    restrict the output value to a certain finite value. Let’s revisit the example
    of predicting whether a player gets accepted (figure 2.11).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的目的是将非线性引入网络。没有它，多层感知器将表现得与单个感知器相似，无论我们添加多少层。激活函数需要将输出值限制在某个有限值内。让我们回顾一下预测玩家是否被接受（图2.11）的例子。
- en: '![](../Images/2-11.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-11.png)'
- en: Figure 2.11 This example revisits the prediction of whether a player gets accepted
    from section 2.1.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 本例重新审视了从2.1节预测玩家是否被接受的情况。
- en: 'First, the model calculates the weighted sum and produces the linear function
    (*z*):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，模型计算加权总和并产生线性函数(*z*)：
- en: '*z* = height · *w*[1] + age · *w*[2] + *b*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = 身高 · *w*[1] + 年龄 · *w*[2] + *b*'
- en: The output of this function has no bound. *z* could literally be any number.
    We use an activation function to wrap the prediction values to a finite value.
    In this example, we use a step function where if *z* > 0, then above the line
    (accepted) and if *z* < 0, then below the line (rejected). So without the activation
    function, we just have a linear function that produces a number, but no decision
    is made in this perceptron. The activation function is what decides whether to
    fire this perceptron.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的输出没有界限。*z*可以是任何数字。我们使用激活函数将预测值包裹在一个有限值内。在这个例子中，我们使用一个阶跃函数，如果*z* > 0，则在线上（接受）以上，如果*z*
    < 0，则在线下（拒绝）。因此，如果没有激活函数，我们只有一个产生数字的线性函数，但在这种感知器中并没有做出任何决定。激活函数决定了是否触发这个感知器。
- en: There are infinite activation functions. In fact, the last few years have seen
    a lot of progress in the creation of state-of-the-art activations. However, there
    are still relatively few activations that account for the vast majority of activation
    needs. Let’s dive deeper into some of the most common types of activation functions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有无限种。实际上，在过去的几年里，在创建最先进的激活函数方面取得了大量进展。然而，仍然相对较少的激活函数能够满足大部分激活需求。让我们更深入地探讨一些最常见的激活函数类型。
- en: 2.3.1 Linear transfer function
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 线性传递函数
- en: A linear transfer function, also called an identity function, indicates that
    the function passes a signal through unchanged. In practical terms, the output
    will be equal to the input, which means we don’t actually have an activation function.
    So no matter how many layers our neural network has, all it is doing is computing
    a linear activation function or, at most, scaling the weighted average coming
    in. But it doesn’t transform input into a nonlinear function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 线性传递函数，也称为恒等函数，表示函数将信号通过而不改变。在实践中，输出将等于输入，这意味着我们实际上没有激活函数。所以无论我们的神经网络有多少层，它所做的只是计算一个线性激活函数，或者最多对输入的加权平均值进行缩放。但它不会将输入转换为非线性函数。
- en: activation(*z*) = *z = wx + b*
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 激活(*z*) = *z = wx + b*
- en: The composition of two linear functions is a linear function, so unless you
    throw a nonlinear activation function in your neural network, you are not computing
    any interesting functions no matter how deep you make your network. No learning
    here!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 两个线性函数的合成是一个线性函数，所以除非你在神经网络中添加一个非线性激活函数，否则无论你使网络有多深，你都不会计算任何有趣的函数。这里没有学习！
- en: To understand why, let’s calculate the derivative of the activation *z*(*x*)
    = *w* · *x* + *b*, where *w* = 4 and *b* = 0\. When we plot this function, it
    looks like figure 2.12\. Then the derivative of *z*(*x*) = 4*x* is *z*'(*x*) =
    4 (figure 2.13).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么，让我们计算激活*z*(*x*) = *w* · *x* + *b*的导数，其中*w* = 4和*b* = 0。当我们绘制这个函数时，它看起来像图2.12。然后*z*(*x*)
    = 4*x*的导数是*z*'(*x*) = 4（图2.13）。
- en: '![](../Images/2-12.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-12.png)'
- en: Figure 2.12 The plot for the activation function *f*(*x*) = 4*x*
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 激活函数*f*(*x*) = 4*x*的图像
- en: 'The derivative of a linear function is constant: it does not depend on the
    input value x. This means that every time we do a backpropagation, the gradient
    will be the same. And this is a big problem: we are not really improving the error,
    since the gradient is pretty much the same. This will be clearer when we discuss
    backpropagation later in this chapter.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数的导数是常数：它不依赖于输入值x。这意味着每次我们进行反向传播时，梯度都将相同。这是一个大问题：因为我们实际上并没有真正改进误差，因为梯度几乎相同。这一点在我们稍后讨论反向传播时会更加清晰。
- en: '![](../Images/2-13.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-13.png)'
- en: Figure 2.13 The plot for the derivative of z(*x*) = 4*x* is *z*'(*x*) = 4.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 z(*x*) = 4*x* 的导数图像为 *z*'(*x*) = 4.
- en: 2.3.2 Heaviside step function (binary classifier)
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 海维塞德步进函数（二元分类器）
- en: The step function produces a binary output. It basically says that if the input
    *x* > 0, it fires (output *y* = 1); else (input < 0), it doesn’t fire (output
    *y* = 0). It is mainly used in binary classification problems like true or false,
    spam or not spam, and pass or fail (figure 2.14).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数产生二元输出。它基本上说，如果输入 *x* > 0，则触发（输出 *y* = 1）；否则（输入 < 0），则不触发（输出 *y* = 0）。它主要用于二元分类问题，如真或假、垃圾邮件或非垃圾邮件、通过或失败（图
    2.14）。
- en: '![](../Images/2-14.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-14.png)'
- en: Figure 2.14 Step functions are commonly used in binary classification problems
    because they transform the input into zero or one.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 步进函数在二元分类问题中常用，因为它将输入转换为零或一。
- en: 2.3.3 Sigmoid/logistic function
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 Sigmoid/logistic 函数
- en: 'This is one of the most common activation functions. It is often used in binary
    classifiers to predict the probability of a class when you have two classes. The
    sigmoid squishes all the values to a probability between 0 and 1, which reduces
    extreme values or outliers in the data without removing them. Sigmoid or logistic
    functions convert infinite continuous variables (range between -∞ to +∞) into
    simple probabilities between 0 and 1\. It is also called the S-shape curve because
    when plotted in a graph, it produces an S-shaped curve. While the step function
    is used to produce a discrete answer (pass or fail), sigmoid is used to produce
    the probability of passing and probability of failing (figure 2.15):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的激活函数之一。它通常用于二元分类器中，当你有两个类别时，预测一个类别的概率。sigmoid 函数将所有值压缩到 0 到 1 之间的概率，这减少了数据中的极端值或异常值，而没有移除它们。sigmoid
    或 logistic 函数将无限连续变量（范围在 -∞ 到 +∞ 之间）转换为 0 到 1 之间的简单概率。它也被称为 S 形曲线，因为当在图表中绘制时，它会产生
    S 形曲线。当步进函数用于产生离散答案（通过或失败）时，sigmoid 函数用于产生通过和失败的概率（图 2.15）：
- en: σ(*z*) = 1/1 + *e*^(−1)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: σ(*z*) = 1/(1 + *e*^(-1))
- en: '![](../Images/2-15.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-15.png)'
- en: Figure 2.15 While the step function is used to produce a discrete answer (pass
    or fail), sigmoid is used to produce the probability of passing or failing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 当步进函数用于产生离散答案（通过或失败）时，sigmoid 函数用于产生通过或失败的概率。
- en: 'Here is how sigmoid is implemented in Python:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在 Python 中实现 sigmoid 函数的示例：
- en: '[PRE5]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Imports numpy
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 numpy
- en: ❷ Sigmoid activation function
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Sigmoid 激活函数
- en: Just-in-time linear algebra (optional)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 实时线性代数（可选）
- en: 'Let’s take a deeper dive into the math side of the sigmoid function to understand
    the problem it helps solve and how the sigmoid function equation is driven. Suppose
    that we are trying to predict whether patients have diabetes based on only one
    feature: their age. When we plot the data we have about our patients, we get the
    linear model shown in the figure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨 sigmoid 函数的数学方面，以了解它帮助解决的问题以及 sigmoid 函数方程是如何驱动的。假设我们正在尝试根据只有一个特征：年龄，来预测患者是否患有糖尿病。当我们绘制我们关于患者的数据时，我们得到图中的线性模型：
- en: z = β0 + β1 age
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: z = β0 + β1 年龄
- en: '![](../Images/2-unnumb-5K.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-5K.png)'
- en: The linear model we get when we plot our data about our patients
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制关于我们患者的数据时，我们得到的线性模型
- en: In this plot, you can observe the balance of probabilities that should go from
    0 to 1\. Note that when patients are below the age of 25, the predicted probabilities
    are negative; meanwhile, they are higher than 1 (100%) when patients are older
    than 43 years old. This is a clear example of why linear functions do not work
    in most cases. Now, how do we fix this to give us probabilities within the range
    of 0 < probability < 1?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，你可以观察到应该从 0 到 1 变化的概率平衡。注意，当患者年龄低于 25 岁时，预测的概率是负数；而当患者年龄超过 43 岁时，它们高于
    1（100%）。这是一个明显的例子，说明了为什么线性函数在大多数情况下不起作用。现在，我们如何修复这个问题，以给出 0 < 概率 < 1 范围内的概率？
- en: 'First, we need to do something to eliminate all the negative probability values.
    The exponential function is a great solution for this problem because the exponent
    of anything (and I mean anything) is always going to be positive. So let’s apply
    that to our linear equation to calculate the probability (*p*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要做一些事情来消除所有负概率值。指数函数是解决这个问题的绝佳方案，因为任何东西的指数（我的意思是任何东西）总是正的。所以让我们将这个应用到我们的线性方程中，以计算概率
    (*p*)：
- en: '*p* = exp(*z*) = exp(*β*[0] + *β*[1] age)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = exp(*z*) = exp(*β*[0] + *β*[1] 年龄)'
- en: 'This equation ensures that we always get probabilities greater than 0\. Now,
    what about the values that are higher than 1? We need to do something about them.
    With proportions, any given number divided by a number that is greater than it
    will give us a number smaller than 1\. Let’s do exactly that to the previous equation.
    We divide the equation by its value plus a small value: either 1 or a (in some
    cases very small) value--let’s call it epsilon (ε):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程确保我们总是得到大于0的概率。那么，对于高于1的值怎么办？我们需要对它们做些什么。通过比例，任何给定的数除以一个大于它的数，都会得到一个小于1的数。让我们对前面的方程做同样的事情。我们将方程除以其值加上一个小值：要么是1，要么是a（在某些情况下非常小）的值——让我们称它为epsilon（ε）：
- en: '*p* = exp(*z*) / (exp(*z*) + *ε*)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = exp(*z*) / (exp(*z*) + *ε*)'
- en: If you divide the equation by exp(*z*), you get
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将方程除以exp(*z*)，你得到
- en: '*p* = 1/(1 + exp(-*z*))'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 1/(1 + exp(-*z*))'
- en: When we plot the probability of this equation, we get the S shape of the sigmoid
    function, where probability is no longer below 0 or above 1\. In fact, as patients’
    ages grow, the probability asymptotically gets closer to 1; and as the weights
    move down, the function asymptotically gets closer to 0 but is never outside the
    0 < p < 1 range. This is the plot of the sigmoid function and logistic regression.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制这个方程的概率时，我们得到sigmoid函数的S形，其中概率不再低于0或高于1。事实上，随着患者年龄的增长，概率渐近地接近1；而当权重向下移动时，函数渐近地接近0，但永远不会超出0
    < p < 1的范围。这是sigmoid函数和逻辑回归的图像。
- en: '![](../Images/2-unnumb-6K.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-6K.png)'
- en: As patients get older, the probability asymptotically gets closer to 1\. This
    is the plot of the sigmoid function and logistic regression.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随着患者年龄的增长，概率渐近地接近1。这是sigmoid函数和逻辑回归的图像。
- en: Softmax function
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax函数
- en: The softmax function is a generalization of the sigmoid function. It is used
    to obtain classification probabilities when we have more than two classes. It
    forces the outputs of a neural network to sum to 1 (for example, 0 < output <
    1). A very common use case in deep learning problems is to predict a single class
    out of many options (more than two).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数是sigmoid函数的推广。当我们有超过两个类别时，它用于获得分类概率。它迫使神经网络的输出之和为1（例如，0 < 输出 < 1）。在深度学习问题中，一个非常常见的用例是从许多选项（超过两个）中预测一个单一类别。
- en: 'The softmax equation is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: softmax方程如下：
- en: '![](../Images/02_15a.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_15a.png)'
- en: Figure 2.16 shows an example of the softmax function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16展示了softmax函数的一个示例。
- en: '![](../Images/2-16.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-16.png)'
- en: Figure 2.16  The softmax function transforms the input values to probability
    values between 0 and 1.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16  softmax函数将输入值转换为0到1之间的概率值。
- en: TIP Softmax is the go-to function that you will often use at the output layer
    of a classifier when you are working on a problem where you need to predict a
    class between more than two classes. Softmax works fine if you are classifying
    two classes, as well. It will basically work like a sigmoid function. By the end
    of this section, I’ll tell you my recommendations about when to use each activation
    function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 当你在处理需要预测两个以上类别的问题时，softmax函数是你在分类器的输出层经常使用的首选函数。如果你正在对两个类别进行分类，softmax函数也能很好地工作。它基本上会像sigmoid函数一样工作。在本节结束时，我会告诉你关于何时使用每个激活函数的建议。
- en: 2.3.5 Hyperbolic tangent function (tanh)
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 双曲正切函数（tanh）
- en: 'The hyperbolic tangent function is a shifted version of the sigmoid version.
    Instead of squeezing the signal values between 0 and 1, tanh squishes all values
    to the range -1 to 1\. Tanh almost always works better than the sigmoid function
    in hidden layers because it has the effect of centering your data so that the
    mean of the data is close to zero rather than 0.5, which makes learning for the
    next layer a little bit easier:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数是sigmoid函数的平移版本。tanh不是将信号值挤压在0到1之间，而是将所有值挤压到-1到1的范围内。tanh几乎总是比sigmoid函数在隐藏层中工作得更好，因为它有使你的数据中心化的效果，使得数据的平均值接近零而不是0.5，这使得下一层的学习变得容易一些：
- en: '![](../Images/02_15d_F2.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_15d_F2.png)'
- en: One of the downsides of both sigmoid and tanh functions is that if (*z*) is
    very large or very small, then the gradient (or derivative or slope) of this function
    becomes very small (close to zero), which will slow down gradient descent (figure
    2.17). This is when the ReLU activation function (explained next) provides a solution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid和tanh函数的一个缺点是，如果(*z*)非常大或非常小，那么这个函数的梯度（或导数或斜率）会变得非常小（接近零），这将减慢梯度下降（图2.17）。这就是ReLU激活函数（下文将解释）提供解决方案的时候。
- en: '![](../Images/2-17.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-17.png)'
- en: Figure 2.17 If (*z*) is very large or very small, then the gradient (or derivative
    or slope) of this function becomes very small (close to zero).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 如果(*z*)非常大或非常小，那么这个函数的梯度（或导数或斜率）将非常小（接近零）。
- en: 2.3.6 Rectified linear unit
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.6 矩形线性单元
- en: 'The rectified linear unit (ReLU) activation function activates a node only
    if the input is above zero. If the input is below zero, the output is always zero.
    But when the input is higher than zero, it has a linear relationship with the
    output variable. The ReLU function is represented as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性单元（ReLU）激活函数仅在输入大于零时激活节点。如果输入小于零，输出始终为零。但是当输入高于零时，它与输出变量之间存在线性关系。ReLU函数表示如下：
- en: '*f*(*x*) = max (0, *x*)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) = max (0, *x*)'
- en: At the time of writing, ReLU is considered the state-of-the-art activation function
    because it works well in many different situations, and it tends to train better
    than sigmoid and tanh in hidden layers (figure 2.18).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，ReLU被认为是最先进的激活函数，因为它在许多不同的情况下都表现良好，并且它在隐藏层中训练通常比sigmoid和tanh更好（图2.18）。
- en: '![](../Images/2-18.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-18.png)'
- en: Figure 2.18 The ReLU function eliminates all negative values of the input by
    transforming them into zeros.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 ReLU函数通过将所有负值输入转换为零来消除输入的所有负值。
- en: 'Here is how ReLU is implemented in Python:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是ReLU在Python中的实现方式：
- en: '[PRE6]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ ReLU activation function
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ ReLU激活函数
- en: 2.3.7 Leaky ReLU
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.7 Leaky ReLU
- en: One disadvantage of ReLU activation is that the derivative is equal to zero
    when (*x*) is negative. Leaky ReLU is a ReLU variation that tries to mitigate
    this issue. Instead of having the function be zero when *x* < 0, leaky ReLU introduces
    a small negative slope (around 0.01) when (*x*) is negative. It usually works
    better than the ReLU function, although it’s not used as much in practice. Take
    a look at the leaky ReLU graph in figure 2.19; can you see the leak?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活的一个缺点是当(*x*)为负时，导数等于零。Leaky ReLU是ReLU的一种变体，试图减轻这个问题。Leaky ReLU在(*x*)为负时引入了一个小的负斜率（大约0.01），而不是当*x*
    < 0时函数为零。它通常比ReLU函数表现更好，尽管在实践中并不常用。看看图2.19中的Leaky ReLU图；你能看到泄漏吗？
- en: '*f* (*x*) = max(0.01*x*, *x*)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* (*x*) = max(0.01*x*, *x*)'
- en: Why 0.01? Some people like to use this as another hyperparameter to tune, but
    that would be overkill, since you already have other, bigger problems to worry
    about. Feel free to try different values (0.1, 0.01, 0.002) in your model and
    see how they work.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是0.01？有些人喜欢将其用作另一个可调的超参数，但这将是过度杀戮，因为你已经有其他更大的问题需要担心。请随意尝试在你的模型中使用不同的值（0.1，0.01，0.002）并看看它们的效果如何。
- en: '![](../Images/2-19.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-19.png)'
- en: Figure 2.19 Instead of having the function be zero when *x* < 0, leaky ReLU
    introduces a small negative slope (around 0.01) when (*x*) is negative.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 Leaky ReLU在*x* < 0时引入了一个小的负斜率（大约0.01），而不是当*x* < 0时函数为零。
- en: 'Here is how Leaky ReLU is implemented in Python:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Leaky ReLU在Python中的实现方式：
- en: '[PRE7]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Leaky ReLU activation function with a 0.01 leak
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 带有0.01泄漏的Leaky ReLU激活函数
- en: Table 2.1 summarizes the various activation functions we’ve discussed in this
    section.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1总结了本节中讨论的各种激活函数。
- en: Table 2.1 A cheat sheet of the most common activation functions
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 最常见激活函数速查表
- en: '| Activation function | Description | Plot | Equation |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 描述 | 图表 | 方程式 |'
- en: '| Linear transfer function (identity function) | The signal passes through
    it unchanged. It remains a linear function. Almost never used. | ![](../Images/2-unnumb-7.png)
    | *f*(*x*) = *x* |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 线性传递函数（恒等函数） | 信号通过它时保持不变。它仍然是一个线性函数。几乎从不使用。 | ![](../Images/2-unnumb-7.png)
    | *f*(*x*) = *x* |'
- en: '| Heaviside step function (binary classifier) | Produces a binary output of
    0 or 1\. Mainly used in binary classification to give a discrete value. | ![](../Images/2-unnumb-8.png)
    | ![](../Images/02_18_T2b.png) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 海维塞德阶跃函数（二元分类器） | 产生0或1的二进制输出。主要用于二元分类，以给出离散值。 | ![](../Images/2-unnumb-8.png)
    | ![](../Images/02_18_T2b.png) |'
- en: '| Sigmoid/ logistic function | Squishes all the values to a probability between
    0 and 1, which reduces extreme values or outliers in the data. Usually used to
    classify two classes. | ![](../Images/2-unnumb-9.png) | ![](../Images/02_18_T3b.png)
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid/逻辑函数 | 将所有值压缩到0和1之间的概率，这减少了数据中的极端值或异常值。通常用于分类两个类别。 | ![](../Images/2-unnumb-9.png)
    | ![](../Images/02_18_T3b.png) |'
- en: '| Softmax function | A generalization of the sigmoid function. Used to obtain
    classification probabilities when we have more than two classes. | ![](../Images/2-unnumb-10.png)
    | ![](../Images/02_18_T4b.png) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Softmax函数 | sigmoid函数的推广。当我们有超过两个类别时，用于获得分类概率。| ![](../Images/2-unnumb-10.png)
    | ![](../Images/02_18_T4b.png) |'
- en: '| Hyperbolic tangent function (tanh) | Squishes all values to the range of
    -1 to 1\. Tanh almost always works better than the sigmoid function in hidden
    layers. | ![](../Images/2-unnumb-11.png) | ![](../Images/02_18_T5b.png) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 双曲正切函数 (tanh) | 将所有值压缩到-1到1的范围内。Tanh几乎总是比sigmoid函数在隐藏层中表现得更好。| ![](../Images/2-unnumb-11.png)
    | ![](../Images/02_18_T5b.png) |'
- en: '| Rectified linear unit (ReLU) | Activates a node only if the input is above
    zero. Always recommended for hidden layers. Better than tanh. | ![](../Images/2-unnumb-12.png)2-unnumb-8.png
    -13 | *f*(*x*) = max (0, *x*) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 矩形线性单元 (ReLU) | 只有当输入大于零时才激活节点。总是推荐用于隐藏层。比tanh更好。| ![](../Images/2-unnumb-12.png)
    | ![](../Images/2-unnumb-8.png) -13 | *f*(*x*) = max (0, *x*) |'
- en: '| Leaky ReLU | Instead of having the function be zero when *x* < 0, leaky ReLU
    introduces a small negative slope (around 0.01) when (*x*) is negative. | ![](../Images/2-unnumb-13.png)
    | *f*(*x*) = max(0.01*x*, *x*) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReLU | 当 *x* < 0 时，函数为零，而Leaky ReLU在 *x* 为负时引入了一个小的负斜率（大约0.01）。| ![](../Images/2-unnumb-13.png)
    | *f*(*x*) = max(0.01*x*, *x*) |'
- en: Hyperparameter alert
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数警告
- en: 'Due to the number of activation functions, it may appear to be an overwhelming
    task to select the appropriate activation function for your network. While it
    is important to select a good activation function, I promise this is not going
    to be a challenging task when you design your network. There are some rules of
    thumb that you can start with, and then you can tune the model as needed. If you
    are not sure what to use, here are my two cents about choosing an activation function:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于激活函数的种类繁多，选择适合你网络的激活函数可能看起来是一项艰巨的任务。虽然选择一个好的激活函数很重要，但我保证在你设计网络时，这不会是一项具有挑战性的任务。你可以从一些经验法则开始，然后根据需要调整模型。如果你不确定该使用什么，以下是我关于选择激活函数的两分钱建议：
- en: For hidden layers--In most cases, you can use the ReLU activation function (or
    leaky ReLU) in hidden layers, as you will see in the projects that we will build
    throughout this book. It is increasingly becoming the default choice because it
    is a bit faster to compute than other activation functions. More importantly,
    it reduces the likelihood of the gradient vanishing because it does not saturate
    for large input values--as opposed to the sigmoid and tanh activation functions,
    which saturate at ~ 1\. Remember, the gradient is the slope. When the function
    plateaus, this will lead to no slope; hence, the gradient starts to vanish. This
    makes it harder to descend to the minimum error (we will talk more about this
    phenomenon, called vanishing/exploding gradients, in later chapters).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于隐藏层——在大多数情况下，你可以在隐藏层中使用ReLU激活函数（或Leaky ReLU），正如你将在本书中构建的项目中看到的那样。它正变得越来越成为默认选择，因为它比其他激活函数计算得更快。更重要的是，它减少了梯度消失的可能性，因为它不会对大输入值饱和——与sigmoid和tanh激活函数相反，它们在~
    1处饱和。记住，梯度是斜率。当函数达到平台期时，这将导致没有斜率；因此，梯度开始消失。这使得下降到最小误差变得更加困难（我们将在后面的章节中更多地讨论这种现象，称为梯度消失/爆炸）。 '
- en: 'For the output layer--The softmax activation function is generally a good choice
    for most classification problems when the classes are mutually exclusive. The
    sigmoid function serves the same purpose when you are doing binary classification.
    For regression problems, you can simply use no activation function at all, since
    the weighted sum node produces the continuous output that you need: for example,
    if you want to predict house pricing based on the prices of other houses in the
    same neighborhood.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于输出层——当类别互斥时，softmax激活函数通常是大多数分类问题的良好选择。当进行二分类时，sigmoid函数也起到相同的作用。对于回归问题，你可以简单地不使用任何激活函数，因为加权求和节点会产生你需要的连续输出：例如，如果你想根据同一地区的其他房屋价格预测房价。
- en: 2.4 The feedforward process
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 前馈过程
- en: Now that you understand how to stack perceptrons in layers, connect them with
    weights/edges, perform a weighted sum function, and apply activation functions,
    let’s implement the complete forward-pass calculations to produce a prediction
    output. The process of computing the linear combination and applying the activation
    function is called feedforward. We briefly discussed feedforward several times
    in the previous sections; let’s take a deeper look at what happens in this process.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何将感知器堆叠成层，通过权重/边连接它们，执行加权求和函数，并应用激活函数，让我们实现完整的前向传递计算以生成预测输出。计算线性组合并应用激活函数的过程称为前馈。在前面的几个部分中，我们简要讨论了前馈几次；让我们更深入地看看在这个过程中发生了什么。
- en: 'The term feedforward is used to imply the forward direction in which the information
    flows from the input layer through the hidden layers, all the way to the output
    layer. This process happens through the implementation of two consecutive functions:
    the weighted sum and the activation function. In short, the forward pass is the
    calculations through the layers to make a prediction.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 术语前馈用来表示信息从输入层通过隐藏层流向输出层的正向方向。这个过程通过实现两个连续的函数：加权求和和激活函数来完成。简而言之，前向传递是通过层进行计算以做出预测的过程。
- en: 'Let’s take a look at the simple three-layer neural network in figure 2.20 and
    explore each of its components:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图 2.20 中的简单三层神经网络，并探索其每个组成部分：
- en: Layers --This network consists of an input layer with three input features,
    and three hidden layers with 3, 4, 1 neurons in each layer.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 -- 这个网络由一个具有三个输入特征输入层和三个具有每层 3、4、1 个神经元的隐藏层组成。
- en: Weights and biases (*w*, *b*) --The edges between nodes are assigned random
    weights denoted as *W*[ab](*n*), where (*n*) indicates the layer number and (ab)
    indicates the weighted edge connecting the a th neuron in layer (*n*) to the b
    th neuron in the previous layer (*n* - 1). For example, *W*[23]^((2)) is the weight
    that connects the second node in layer 2 to the third node in layer 1 (*a*[2]²
    to *a*[1]³). (Note that you can see different denotations of *W[ab]^((n))* in
    other DL literature, which is fine as long as you follow one convention for your
    entire network.)
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和偏差 (*w*, *b*) -- 节点之间的边被分配随机权重，表示为 *W*[ab](*n*)，其中 (*n*) 表示层号，(ab) 表示连接第
    (*n*) 层中第 a 个神经元和前一层 (*n* - 1) 中第 b 个神经元的加权边。例如，*W*[23]^((2)) 是连接第 2 层第二个节点和第
    1 层第三个节点的权重（*a*[2]² 到 *a*[1]³）。（请注意，你可以在其他深度学习文献中看到 *W[ab]^((n)* 的不同表示，只要你在整个网络中遵循一个约定即可。）
- en: The biases are treated similarly to weights because they are randomly initialized,
    and their values are learned during the training process. So, for convenience,
    from this point forward we are going to represent the basis with the same notation
    that we gave for the weights (*w*). In DL literature, you will mostly find all
    weights and biases represented as (*w*) for simplicity.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏差被处理得与权重相似，因为它们是随机初始化的，其值在训练过程中学习。因此，为了方便起见，从现在开始我们将使用与权重相同的符号来表示基（*w*）。在深度学习文献中，你通常会看到所有权重和偏差都表示为(*w*)，以简化表示。
- en: Activation functions σ(*x*) --In this example, we are using the sigmoid function
    σ(*x*) as an activation function.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数 σ(*x*) -- 在这个例子中，我们使用sigmoid函数 σ(*x*) 作为激活函数。
- en: Node values (*a*) --We will calculate the weighted sum, apply the activation
    function, and assign this value to the node amn, where n is the layer number and
    *m* is the node index in the layer. For example, a 23 means node number 2 in layer
    3.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点值 (*a*) -- 我们将计算加权求和，应用激活函数，并将此值分配给节点 amn，其中 n 是层号，*m* 是层中的节点索引。例如，a 23 表示第
    3 层中的第 2 个节点。
- en: '![](../Images/2-20.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-20.png)'
- en: Figure 2.20 A simple three-layer neural network
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 一个简单的三层神经网络
- en: 2.4.1 Feedforward calculations
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 前馈计算
- en: 'We have all we need to start the feedforward calculations:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了开始前馈计算所需的一切：
- en: '![](../Images/02_19_T1.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_19_T1.png)'
- en: Then we do the same calculations for layer 2
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对第 2 层进行相同的计算
- en: '![](../Images/02_19_T2.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_19_T2.png)'
- en: 'all the way to the output prediction in layer 3:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 直至第3层的输出预测：
- en: '![](../Images/02_19_T3.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_19_T3.png)'
- en: And there you have it! You just calculated the feedforward of a two-layer neural
    network. Let’s take a moment to reflect on what we just did. Take a look at how
    many equations we need to solve for such a small network. What happens when we
    have a more complex problem with hundreds of nodes in the input layer and hundreds
    more in the hidden layers? It is more efficient to use matrices to pass through
    multiple inputs at once. Doing this allows for big computational speedups, especially
    when using tools like NumPy, where we can implement this with one line of code.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你刚刚计算了一个两层神经网络的正向传播。让我们花点时间来反思一下我们刚刚做了什么。看看我们为这样一个小的网络需要解多少个方程。当我们有一个更复杂的问题，输入层有数百个节点，隐藏层有数百个节点时会发生什么？使用矩阵一次传递多个输入会更有效率。这样做可以大幅提高计算速度，尤其是在使用NumPy这样的工具时，我们可以用一行代码来实现这一点。
- en: 'Let’s see how the matrices computation looks (figure 2.21). All we did here
    is simply stack the inputs and weights in matrices and multiply them together.
    The intuitive way to read this equation is from the right to the left. Start at
    the far right and follow with me:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看矩阵计算看起来是什么样子（图2.21）。我们在这里所做的只是简单地将输入和权重堆叠成矩阵并相乘。直观地阅读这个方程的方法是从右到左。从最右边开始，跟我一起来：
- en: We stack all the inputs together in one vector (row, column), in this case (3,
    1).
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将所有输入堆叠成一个向量（行，列），在这种情况下（3，1）。
- en: We multiply the input vector by the weights matrix from layer 1 (*W*^((1)))
    and then apply the sigmoid function.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将输入向量与第1层的权重矩阵（*W*^((1)))相乘，然后应用sigmoid函数。
- en: We multiply the result for layer 2 ⇒ σ · *W*^((2)) and layer 3 ⇒ σ · *W*^((3)).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将第2层的输出结果（σ · *W*^((2)))和第3层的输出结果（σ · *W*^((3)))相乘。
- en: If we have a fourth layer, you multiply the result from step 3 by σ · *W*^((4)),
    and so on, until we get the final prediction output *ŷ*!
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有第4层，我们将步骤3的结果乘以σ · *W*^((4))，依此类推，直到我们得到最终的预测输出 *ŷ*！
- en: 'Here is a simplified representation of this matrices formula:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这个矩阵公式的简化表示：
- en: '*ŷ* = σ · *W*^((3)) · σ · *W*^((2)) · σ · *W*^((1)) · (*x*)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = σ · *W*^((3)) · σ · *W*^((2)) · σ · *W*^((1)) · (*x*)'
- en: '![](../Images/2-21.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-21.png)'
- en: Figure 2.21 Reading from left to right, we stack the inputs together in one
    vector, multiply the input vector by the weights matrix from layer 1, apply the
    sigmoid function, and multiply the result.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 从左到右阅读，我们将输入堆叠成一个向量，将输入向量与第1层的权重矩阵相乘，应用sigmoid函数，并乘以结果。
- en: 2.4.2 Feature learning
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 特征学习
- en: 'The nodes in the hidden layers (ai) are the new features that are learned after
    each layer. For example, if you look at figure 2.20, you see that we have three
    feature inputs (x1, x2, and x3). After computing the forward pass in the first
    layer, the network learns patterns, and these features are transformed to three
    new features with different values  ![](../Images/02_20_F1.png). Then, in the
    next layer, the network learns patterns within the patterns and produces new features ![](../Images/02_20_F1b.png),
    and so forth). The produced features after each layer are not totally understood,
    and we don’t see them, nor do we have much control over them. It is part of the
    neural network magic. That’s why they are called hidden layers. What we do is
    this: we look at the final output prediction and keep tuning some parameters until
    we are satisfied by the network’s performance.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的节点（ai）是在每一层学习后的新特征。例如，如果你看图2.20，你会看到我们有三个特征输入（x1，x2和x3）。在第一层进行正向传播计算后，网络学习到模式，这些特征被转换成具有不同值的三个新特征！[](../Images/02_20_F1.png)。然后，在下一层，网络学习模式中的模式并产生新的特征！[](../Images/02_20_F1b.png)，依此类推）。每一层产生的特征并不完全理解，我们看不到它们，也没有太多控制它们。这是神经网络魔法的一部分。这就是为什么它们被称为隐藏层。我们所做的是：我们查看最终的输出预测，并调整一些参数，直到我们对网络的性能满意为止。
- en: 'To reiterate, let’s see this in a small example. In figure 2.22, you see a
    small neural network to estimate the price of a house based on three features:
    how many bedrooms it has, how big it is, and which neighborhood it is in. You
    can see that the original input feature values 3, 2000, and 1 were transformed
    into new feature values after performing the feedforward process in the first
    layer ![](../Images/02_20_F2.png). Then they were transformed again to a prediction
    output value (*ŷ*). When training a neural network, we see the prediction output
    and compare it with the true price to calculate the error and repeat the process
    until we get the minimum error.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，让我们通过一个小例子来看一下。在图2.22中，你可以看到一个小的神经网络，它根据三个特征来估算房价：卧室数量、房屋大小以及所在的社区。你可以看到，在第一层的正向传播过程中，原始输入特征值3、2000和1被转换成了新的特征值！[](../Images/02_20_F2.png)。然后它们再次被转换成预测输出值（*ŷ*）。在训练神经网络时，我们查看预测输出并与真实价格进行比较，以计算误差并重复此过程，直到我们得到最小误差。
- en: '![](../Images/2-22.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-22.png)'
- en: 'Figure 2.22 A small neural network to estimate the price of a house based on
    three features: how many bedrooms it has, how big it is, and which neighborhood
    it is in'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 一个小的神经网络，基于三个特征估算房价：卧室数量、房屋大小以及所在的社区
- en: To help visualize the feature-learning process, let’s take another look at figure
    2.9 (repeated here in figure 2.23) from the Tensorflow playground. You can see
    that the first layer learns basic features like lines and edges. The second layer
    begins to learn more complex features like corners. The process continues until
    the last layers of the network learn even more complex feature shapes like circles
    and spirals that fit the dataset.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助可视化特征学习过程，让我们再次看看Tensorflow playground中的图2.9（在此处重复为图2.23）。你可以看到，第一层学习基本特征，如线条和边缘。第二层开始学习更复杂的特征，如角落。这个过程一直持续到网络的最后几层，学习到更复杂的特征形状，如适合数据集的圆形和螺旋形。
- en: '![](../Images/2-23.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-23.png)'
- en: Figure 2.23 Learning features in multiple hidden layers
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 多个隐藏层中的特征学习
- en: 'That is how a neural network learns new features: via the network’s hidden
    layers. First, they recognize patterns in the data. Then, they recognize patterns
    within patterns; then patterns within patterns within patterns, and so on. The
    deeper the network is, the more it learns about the training data.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是神经网络学习新特征的方式：通过网络的隐藏层。首先，它们识别数据中的模式。然后，它们识别模式中的模式；然后模式中的模式中的模式，以此类推。网络越深，它对训练数据了解得就越多。
- en: Vectors and matrices refresher
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 向量和矩阵复习
- en: 'If you understood the matrix calculations we just did in the feedforward discussion,
    feel free to skip this sidebar. If you are still not convinced, hang tight: this
    sidebar is for you.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你理解了我们刚才在正向传播讨论中做的矩阵计算，你可以自由跳过这个边栏。如果你仍然不确信，请耐心等待：这个边栏是为你准备的。
- en: The feedforward calculations are a set of matrix multiplications. While you
    will not do these calculations by hand, because there are a lot of great DL libraries
    that do them for you with just one line of code, it is valuable to understand
    the mathematics that happens under the hood so you can debug your network. Especially
    because this is very trivial and interesting, let’s quickly review matrix calculations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播的计算是一组矩阵乘法。虽然你不会手动进行这些计算，因为有很多优秀的深度学习库可以只用一行代码为你完成这些计算，但了解底层发生的数学原理是有价值的，这样你可以调试你的网络。特别是因为这个过程非常简单且有趣，让我们快速回顾一下矩阵计算。
- en: 'Let’s start with some basic definitions of matrix dimensions:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从矩阵的一些基本定义开始：
- en: A scalar is a single number.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量是一个单一的数字。
- en: A vector is an array of numbers.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量是一组数字。
- en: A matrix is a 2D array.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵是一个二维数组。
- en: A tensor is an n-dimensional array with n > 2.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量是一个n维数组，其中n > 2。
- en: '![](../Images/2-unnumb-14K.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-14K.png)'
- en: 'Matrix dimensions: a scalar is a single number, a vector is an array of numbers,
    a matrix is a 2D array, and a tensor is an n-dimensional array.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵维度：标量是一个单一的数字，向量是一组数字的数组，矩阵是一个二维数组，张量是一个n维数组。
- en: 'We will follow the conventions used in most mathematical literature:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循大多数数学文献中使用的惯例：
- en: 'Scalars are written in lowercase and italics: for instance, *n*.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量用小写和斜体表示：例如，*n*。
- en: 'Vectors are written in lowercase, italics, and bold type: for instance, ***x***.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量用小写、斜体和粗体表示：例如，***x***。
- en: 'Matrices are written in uppercase, italics, and bold: for instance, ***X***.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵用大写、斜体和粗体表示：例如，***X***。
- en: 'Matrix dimensions are written as follows: (row × column).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵维度表示如下：（行 × 列）。
- en: 'Multiplication:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法：
- en: 'Scalar multiplication--Simply multiply the scalar number by all the numbers
    in the matrix. Note that scalar multiplications don’t change the matrix dimensions:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的标量乘法——简单地将标量数乘以矩阵中的所有数。注意，标量乘法不会改变矩阵的维度：
- en: '![](../Images/2-unnumb-14bK.png)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-14bK.png)'
- en: Matrix multiplication--When multiplying two matrices, such as in the case of
    (row[1] × column[1]) × (row[2] × column[2]), column[1] and row[2] must be equal
    to each other, and the product will have the dimensions (row[1] × column[2]).
    For example,
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法——当乘以两个矩阵时，例如在 (行[1] × 列[1]) × (行[2] × 列[2]) 的情况下，列[1] 和行[2] 必须相等，其乘积将具有
    (行[1] × 列[2]) 的维度。例如，
- en: '![](../Images/2-unnumb-15K.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-15K.png)'
- en: where *x* = 3 · 13 + 4 · 8 + 2 · 6 = 83, and the same for *y* = 63 and z = 37.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* = 3 · 13 + 4 · 8 + 2 · 6 = 83，*y* = 63 和 *z* = 37 同理。
- en: Now that you know the matrices multiplication rules, pull out a piece of paper
    and work through the dimensions of matrices in the earlier neural network example.
    The following figure shows the matrix equation again for your convenience.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了矩阵乘法规则，拿出一张纸，处理一下之前神经网络示例中的矩阵维度。以下图再次显示了矩阵方程，以供你方便参考。
- en: '![](../Images/2-unnumb-16K.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-16K.png)'
- en: The matrix equation from the main text. Use it to work through matrix dimensions.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 主文中的矩阵方程。使用它来处理矩阵维度。
- en: 'The last thing I want you to understand about matrices is transposition. With
    transposition, you can convert a row vector to a column vector and vice versa,
    where the shape (*m × n*) is inverted and becomes (*n × m*). The superscript (*A^T*
    ) is used for transposed matrices:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你们对矩阵了解的最后一件事是转置。通过转置，你可以将行向量转换为列向量，反之亦然，其中形状 (*m × n*) 被反转并变为 (*n × m*)。转置矩阵使用上标
    (*A^T*) 表示：
- en: '![](../Images/2-unnumb-16Kb.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-16Kb.png)'
- en: 2.5 Error functions
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 误差函数
- en: 'So far, you have learned how to implement the forward pass in neural networks
    to produce a prediction that consists of the weighted sum plus activation operations.
    Now, how do we evaluate the prediction that the network just produced? More importantly,
    how do we know how far this prediction is from the correct answer (the label)?
    The answer is this: measure the error. The selection of an error function is another
    important aspect of the design of a neural network. Error functions can also be
    referred to as cost functions or loss functions, and these terms are used interchangeably
    in DL literature.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了如何在神经网络中实现前向传递以生成由加权求和加激活操作组成的预测。现在，我们如何评估网络刚刚生成的预测？更重要的是，我们如何知道这个预测离正确答案（标签）有多远？答案是：测量误差。误差函数的选择是神经网络设计的重要方面之一。误差函数也可以称为代价函数或损失函数，这些术语在深度学习文献中可以互换使用。
- en: 2.5.1 What is the error function?
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 什么是误差函数？
- en: The error function is a measure of how “wrong” the neural network prediction
    is with respect to the expected output (the label). It quantifies how far we are
    from the correct solution. For example, if we have a high loss, then our model
    is not doing a good job. The smaller the loss, the better the job the model is
    doing. The larger the loss, the more our model needs to be trained to increase
    its accuracy.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数是衡量神经网络预测相对于预期输出（标签）的“错误程度”的度量。它量化了我们离正确解有多远。例如，如果我们有一个高的损失，那么我们的模型做得不好。损失越小，模型做得越好。损失越大，我们的模型就需要更多的训练来提高其准确性。
- en: 2.5.2 Why do we need an error function?
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 为什么我们需要误差函数？
- en: Calculating error is an optimization problem, something all machine learning
    engineers love (mathematicians, too). Optimization problems focus on defining
    an error function and trying to optimize its parameters to get the minimum error
    (more on optimization in the next section). But for now, know that, in general,
    when we are working on an optimization problem, if we are able to define the error
    function for the problem, we have a very good shot at solving it by running optimization
    algorithms to minimize the error function.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 计算误差是一个优化问题，这是所有机器学习工程师都喜欢的（数学家也是如此）。优化问题侧重于定义一个误差函数并尝试优化其参数以获得最小误差（关于优化的更多内容将在下一节中介绍）。但就现在而言，要知道，在一般情况下，当我们处理优化问题时，如果我们能够为问题定义误差函数，我们就很有机会通过运行优化算法来最小化误差函数来解决它。
- en: In optimization problems, our ultimate goal is to find the optimum variables
    (weights) that would minimize the error function as much as we can. If we don’t
    know how far from the target we are, how will we know what to change in the next
    iteration? The process of minimizing this error is called error function optimization.
    We will review several optimization methods in the next section. But for now,
    all we need to know from the error function is how far we are from the correct
    prediction, or how much we missed the desired degree of performance.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化问题中，我们的最终目标是找到最优变量（权重），以尽可能多地最小化误差函数。如果我们不知道我们离目标有多远，我们怎么知道在下一轮迭代中要改变什么？最小化这个错误的过程称为误差函数优化。在下一节中，我们将回顾几种优化方法。但就目前而言，我们需要从误差函数中了解的是我们离正确预测有多远，或者我们偏离了期望的性能程度有多远。
- en: 2.5.3 Error is always positive
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 错误始终为正
- en: 'Consider this scenario: suppose we have two data points that we are trying
    to get our network to predict correctly. If the first gives an error of 10 and
    the second gives an error of -10, then our average error is zero! This is misleading
    because “error = 0” means our network is producing perfect predictions, when,
    in fact, it missed by 10 twice. We don’t want that. We want the error of each
    prediction to be positive, so the errors don’t cancel each other when we take
    the average error. Think of an archer aiming at a target and missing by 1 inch.
    We are not really concerned about which direction they missed; all we need to
    know is how far each shot is from the target.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：假设我们有两个数据点，我们试图让我们的网络正确预测。如果第一个数据点产生10的错误，而第二个数据点产生-10的错误，那么我们的平均错误为零！这具有误导性，因为“错误=0”意味着我们的网络正在产生完美的预测，而实际上它两次都偏离了10。我们不想这样。我们希望每个预测的错误都是正数，这样当我们计算平均错误时，错误就不会相互抵消。想象一个弓箭手瞄准目标并偏离了1英寸。我们并不真正关心他们偏离的方向；我们只需要知道每一箭与目标的距离。
- en: 'A visualization of loss functions of two separate models plotted over time
    is shown in figure 2.24\. You can see that model #1 is doing a better job of minimizing
    error, whereas model #2 starts off better until epoch 6 and then plateaus.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24显示了两个独立模型随时间变化的损失函数的可视化。你可以看到模型#1在最小化错误方面做得更好，而模型#2在6个epoch之前表现更好，然后趋于平稳。
- en: 'Different loss functions will give different errors for the same prediction,
    and thus have a considerable effect on the performance of the model. A thorough
    discussion of loss functions is outside the scope of this book. Instead, we will
    focus on the two most commonly used loss functions: mean squared error (and its
    variations), usually used for regression problems, and cross-entropy, used for
    classification problems.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的损失函数会对相同的预测给出不同的错误，从而对模型的性能产生相当大的影响。对损失函数的详细讨论超出了本书的范围。相反，我们将专注于两种最常用的损失函数：均方误差（及其变体），通常用于回归问题，以及交叉熵，用于分类问题。
- en: '![](../Images/2-24.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图2-24](../Images/2-24.png)'
- en: Figure 2.24 A visualization of the loss functions of two separate models plotted
    over time
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 两个独立模型损失函数随时间变化的可视化
- en: 2.5.4 Mean square error
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 均方误差
- en: 'Mean squared error (MSE) is commonly used in regression problems that require
    the output to be a real value (like house pricing). Instead of just comparing
    the prediction output with the label (*ŷ*[i] - *y*[i]), the error is squared and
    averaged over the number of data points, as you see in this equation:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）在需要输出为实值（如房价）的回归问题中常用。与仅比较预测输出与标签（*ŷ*[i] - *y*[i]）不同，误差被平方并平均到数据点的数量，正如你在这个方程中看到的那样：
- en: '![](../Images/02_28_F1b.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24](../Images/02_28_F1b.png)'
- en: MSE is a good choice for a few reasons. The square ensures the error is always
    positive, and larger errors are penalized more than smaller errors. Also, it makes
    the math nice, which is always a plus. The notations in the formula are listed
    in table 2.2.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: MSE有几个优点。平方确保了错误始终为正数，并且较大的错误比较小的错误受到更多的惩罚。此外，它使数学变得简单，这始终是一个加分项。公式中的符号列在表2.2中。
- en: 'MSE is quite sensitive to outliers, since it squares the error value. This
    might not be an issue for the specific problem that you are solving. In fact,
    this sensitivity to outliers might be beneficial in some cases. For example, if
    you are predicting a stock price, you would want to take outliers into account,
    and sensitivity to outliers would be a good thing. In other scenarios, you wouldn’t
    want to build a model that is skewed by outliers, such as predicting a house price
    in a city. In that case, you are more interested in the median and less in the
    mean. A variation error function of MSE called mean absolute error (MAE) was developed
    just for this purpose. It averages the absolute error over the entire dataset
    without taking the square of the error:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）对异常值非常敏感，因为它平方了误差值。这可能不是你正在解决的问题的具体问题。事实上，对异常值的敏感性在某些情况下可能是有益的。例如，如果你正在预测股票价格，你会希望考虑异常值，对异常值的敏感性会是一件好事。在其他情况下，你可能不希望构建一个受异常值偏斜的模型，例如预测一个城市的房价。在这种情况下，你更感兴趣的是中位数，而不是平均值。为了这个目的，开发了一种均方误差（MSE）的变体，称为平均绝对误差（MAE），它在整个数据集上平均绝对误差，而不对误差值进行平方：
- en: Table 2.2 Meanings of notation used in regression problems
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2 回归问题中使用的符号含义
- en: '| Notation | Meaning |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 含义 |'
- en: '| E(W, b) | The loss function. Is also annotated as J(W, b) in other literature.
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| E(W, b) | 损失函数。在其他文献中也标注为 J(W, b)。|'
- en: '| W | Weights matrix. In some literature, the weights are denoted by the theta
    sign (θ). |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| W | 权重矩阵。在某些文献中，权重用希腊字母θ表示。|'
- en: '| b | Biases vector. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| b | 偏置向量。|'
- en: '| N | Number of training examples. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| N | 训练样本数量。|'
- en: '| *ŷ*i | Prediction output. Also notated as hw, b(*x*) in some DL literature.
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| *ŷ*i | 预测输出。在某些深度学习文献中也表示为hw, b(*x*)。|'
- en: '| *y*[i] | The correct output (the label). |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| *y*[i] | 正确的输出（标签）。|'
- en: '| (*ŷ*i - yi) | Usually called the residual. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| (*ŷ*i - yi) | 通常称为残差。|'
- en: '![](../Images/02_28_F2b.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_28_F2b.png)'
- en: 2.5.5 Cross-entropy
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.5 交叉熵
- en: 'Cross-entropy is commonly used in classification problems because it quantifies
    the difference between two probability distributions. For example, suppose that
    for a specific training instance, we are trying to classify a dog image out of
    three possible classes (dogs, cats, fish). The true distribution for this training
    instance is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵在分类问题中常用，因为它量化了两个概率分布之间的差异。例如，假设对于特定的训练实例，我们正在尝试从三个可能的类别（狗、猫、鱼）中分类一张狗的图片。这个训练实例的真实分布如下：
- en: '[PRE8]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can interpret this “true” distribution to mean that the training instance
    has 0% probability of being class A, 100% probability of being class B, and 0%
    probability of being class C. Now, suppose our machine learning algorithm predicts
    the following probability distribution:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个“真实”分布理解为训练实例有0%的概率属于类别A，100%的概率属于类别B，0%的概率属于类别C。现在，假设我们的机器学习算法预测以下概率分布：
- en: '[PRE9]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'How close is the predicted distribution to the true distribution? That is what
    the cross-entropy loss function determines. We can use this formula:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分布与真实分布有多接近？这就是交叉熵损失函数所确定的。我们可以使用以下公式：
- en: '![](../Images/02_28_F3b.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_28_F3b.png)'
- en: 'where (*y*) is the target probability, (*p*) is the predicted probability,
    and (*m*) is the number of classes. The sum is over the three classes: cat, dog,
    and fish. In this case, the loss is 1.2:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 (*y*) 是目标概率，(*p*) 是预测概率，(*m*) 是类别数量。求和是针对三个类别：猫、狗和鱼。在这种情况下，损失是1.2：
- en: '[PRE10]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So that is how “wrong” or “far away” our prediction is from the true distribution.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们的预测与真实分布之间的“错误”或“远离”程度。
- en: 'Let’s do this one more time, just to show how the loss changes when the network
    makes better predictions. In the previous example, we showed the network an image
    of a dog, and it predicted that the image was 30% likely to be a dog, which was
    very far from the target prediction. In later iterations, the network learns some
    patterns and gets the predictions a little better, up to 50%:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再来一次，只是为了展示当网络做出更好的预测时损失是如何变化的。在之前的例子中，我们向网络展示了一只狗的图片，它预测这张图片有30%的可能性是狗，这离目标预测非常远。在后续的迭代中，网络学习了一些模式，并将预测结果稍微改进，达到了50%：
- en: '[PRE11]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we calculate the loss again:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们再次计算损失：
- en: '[PRE12]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You see that when the network makes a better prediction (dog is up to 50% from
    30%), the loss decreases from 1.2 to 0.69\. In the ideal case, when the network
    predicts that the image is 100% likely to be a dog, the cross-entropy loss will
    be 0 (feel free to try the math).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当网络做出更好的预测（狗的概率从30%上升到50%）时，损失从1.2下降到0.69。在理想情况下，当网络预测图像有100%的可能性是狗时，交叉熵损失将是0（不妨尝试一下数学计算）。
- en: 'To calculate the cross-entropy error across all the training examples (*n*),
    we use this general formula:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算所有训练示例（*n*）的交叉熵误差，我们使用这个通用公式：
- en: '![](../Images/02_28_F4b.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/02_28_F4b.png)'
- en: NOTE It is important to note that you will not be doing these calculations by
    hand. Understanding how things work under the hood gives you better intuition
    when you are designing your neural network. In DL projects, we usually use libraries
    like Tensorflow, PyTorch, and Keras where the error function is generally a parameter
    choice.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：重要的是要注意，你不会手动进行这些计算。理解底层的工作原理，当你设计神经网络时，会给你更好的直觉。在深度学习项目中，我们通常使用Tensorflow、PyTorch和Keras等库，其中误差函数通常是一个参数选择。
- en: 2.5.6 A final note on errors and weights
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.6 关于误差和权重的一个最后说明
- en: As mentioned before, in order for the neural network to learn, it needs to minimize
    the error function as much as possible (0 is ideal). The lower the errors, the
    higher the accuracy of the model in predicting values. How do we minimize the
    error?
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了使神经网络学习，它需要尽可能最小化误差函数（0是理想的）。误差越低，模型在预测值方面的准确性就越高。我们如何最小化误差？
- en: 'Let’s look at the following perceptron example with a single input to understand
    the relationship between the weight and the error:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下具有单个输入的感知器示例来了解权重和误差之间的关系：
- en: '![](../Images/2-unnumb-17.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/2-unnumb-17.png)'
- en: 'Suppose the input *x* = 0.3, and its label (goal prediction) *y* = 0.8\. The
    prediction output (*ŷ*) of this perception is calculated as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入*x* = 0.3，其标签（目标预测）*y* = 0.8。这个感知器的预测输出(*ŷ*)计算如下：
- en: '*ŷ*[i] = *w* · *x* = *w* · 0.3'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ*[i] = *w* · *x* = *w* · 0.3'
- en: 'And the error, in its simplest form, is calculated by comparing the prediction
    *ŷ* and the label *y*:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 误差，在其最简单形式中，是通过比较预测*ŷ*和标签*y*来计算的：
- en: error = |*ŷ* - *y* |
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: error = |*ŷ* - *y* |
- en: = |(*w* · *x*) - *y* |
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: = |(*w* · *x*) - *y* |
- en: = |*w* · 0.3 - 0.8|
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: = |*w* · 0.3 - 0.8|
- en: 'If you look at this error function, you will notice that the input (*x*) and
    the goal prediction (*y*) are fixed values. They will never change for these specific
    data points. The only two variables that we can change in this equation are the
    error and the weight. Now, if we want to get to the minimum error, which variable
    can we play with? Correct: the weight! The weight acts as a knob that the network
    needs to adjust up and down until it gets the minimum error. This is how the network
    learns: by adjusting weight. When we plot the error function with respect to the
    weight, we get the graph shown in figure 2.25.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察这个误差函数，你会注意到输入(*x*)和目标预测(*y*)是固定值。对于这些特定的数据点，它们永远不会改变。在这个方程中，我们唯一可以改变的两个变量是误差和权重。现在，如果我们想达到最小误差，我们可以调整哪个变量？正确：权重！权重充当网络需要上下调整以获得最小误差的旋钮。这就是网络学习的方式：通过调整权重。当我们绘制误差函数相对于权重的图像时，我们得到图2.25中所示的图表。
- en: '![](../Images/2-25.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/2-25.png)'
- en: Figure 2.25 The network learns by adjusting weight. When we plot the error function
    with respect to weight, we get this type of graph.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 网络通过调整权重来学习。当我们绘制误差函数相对于权重的图像时，我们得到这种类型的图表。
- en: As mentioned before, we initialize the network with random weights. The weight
    lies somewhere on this curve, and our mission is to make it descend this curve
    to its optimal value with the minimum error. The process of finding the goal weights
    of the neural network happens by adjusting the weight values in an iterative process
    using an optimization algorithm.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们用随机权重初始化网络。权重位于这条曲线上，我们的任务是让它沿着曲线下降到最小误差的最优值。寻找神经网络目标权重的过程是通过使用优化算法在迭代过程中调整权重值来实现的。
- en: 2.6 Optimization algorithms
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 优化算法
- en: Training a neural network involves showing the network many examples (a training
    dataset); the network makes predictions through feedforward calculations and compares
    them with the correct labels to calculate the error. Finally, the neural network
    needs to adjust the weights (on all edges) until it gets the minimum error value,
    which means maximum accuracy. Now, all we need to do is build algorithms that
    can find the optimum weights for us.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及向网络展示许多示例（训练数据集）；网络通过前向计算进行预测，并将预测结果与正确标签进行比较以计算误差。最后，神经网络需要调整权重（所有边上的权重）直到它得到最小误差值，这意味着最大准确度。现在，我们只需要构建能够为我们找到最优权重的算法。
- en: 2.6.1 What is optimization?
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 优化是什么？
- en: Ahh, optimization! A topic that is dear to my heart, and dear to every machine
    learning engineer (mathematicians too). Optimization is a way of framing a problem
    to maximize or minimize some value. The best thing about computing an error function
    is that we turn the neural network into an optimization problem where our goal
    is to minimize the error.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 啊哈，优化！这是一个我非常喜爱，也是每个机器学习工程师（数学家也是如此）都喜爱的主题。优化是一种将问题框架化的方式，以最大化或最小化某个值。计算误差函数的最好之处在于，我们将神经网络转化为一个优化问题，我们的目标是最小化误差。
- en: Suppose you want to optimize your commute from home to work. First, you need
    to define the metric that you are optimizing (the error function). Maybe you want
    to optimize the cost of the commute, or the time, or the distance. Then, based
    on that specific loss function, you work on minimizing its value by changing some
    parameters. Changing the parameters to minimize (or maximize) a value is called
    optimization. If you choose the loss function to be the cost, maybe you will choose
    a longer commute that will take two hours, or (hypothetically) you might walk
    for five hours to minimize the cost. On the other hand, if you want to optimize
    the time spent commuting, maybe you will spend $50 to take a cab that will decrease
    the commute time to 20 minutes. Based on the loss function you defined, you can
    start changing your parameters to get the results you want.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想优化从家到工作的通勤。首先，你需要定义你正在优化的指标（误差函数）。也许你想优化通勤的成本、时间或距离。然后，基于这个特定的损失函数，你通过改变一些参数来工作以最小化其值。改变参数以最小化（或最大化）一个值被称为优化。如果你选择损失函数为成本，你可能选择一个需要两小时的更长通勤，或者（假设性地）你可能步行五小时以最小化成本。另一方面，如果你想优化通勤时间，你可能愿意花50美元乘坐出租车，将通勤时间缩短到20分钟。基于你定义的损失函数，你可以开始改变你的参数以获得你想要的结果。
- en: TIP In neural networks, optimizing the error function means updating the weights
    and biases until we find the optimal weights, or the best values for the weights
    to produce the minimum error.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 在神经网络中，优化误差函数意味着更新权重和偏差，直到我们找到最优权重，或者产生最小误差的最佳权重值。
- en: 'Let’s look at the space that we are trying to optimize:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们正在尝试优化的空间：
- en: '![](../Images/2-unnumb-18.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-18.png)'
- en: In a neural network of the simplest form, a perceptron with one input, we have
    only one weight. We can easily plot the error (that we are trying to minimize)
    with respect to this weight, represented by the 2D curve in figure 2.26 (repeated
    from earlier).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的神经网络形式中，一个只有一个输入的感知器，我们只有一个权重。我们可以很容易地绘制出相对于这个权重的误差（我们试图最小化的误差），如图2.26中的2D曲线（之前已展示）。
- en: '![](../Images/2-26.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-26.png)'
- en: Figure 2.26 The error function with respect to its weight for a single perceptron
    is a 2D curve.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.26 对于单个感知器，误差函数相对于其权重的2D曲线。
- en: But what if we have two weights? If we graph all the possible values of the
    two weights, we get a 3D plane of the error (figure 2.27).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们有两个权重呢？如果我们绘制这两个权重的所有可能值，我们得到一个包含误差的3D平面（图2.27）。
- en: '![](../Images/2-27.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-27.png)'
- en: Figure 2.27 Graphing all possible values of two weights gives a 3D error plane.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.27 绘制两个权重的所有可能值得到一个3D误差平面。
- en: What about more than two weights? Your network will probably have hundreds or
    thousands of weights (because each edge in your network has its own weight value).
    Since we humans are only equipped to understand a maximum of 3 dimensions, it
    is impossible for us to visualize error graphs when we have 10 weights, not to
    mention hundreds or thousands of weight parameters. So, from this point on, we
    will study the error function using the 2D or 3D plane of the error. In order
    to optimize the model, our goal is to search this space to find the best weights
    that will achieve the lowest possible error.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于超过两个权重的情况呢？您的网络可能拥有数百或数千个权重（因为您的网络中的每条边都有自己的权重值）。由于我们人类只能理解最多3个维度，当我们有10个权重时，我们无法可视化错误图，更不用说数百或数千个权重参数了。因此，从现在开始，我们将使用错误函数的2D或3D平面来研究错误。为了优化模型，我们的目标是搜索这个空间，找到能够实现最低可能错误的最佳权重。
- en: Why do we need an optimization algorithm? Can’t we just brute-force through
    a lot of weight values until we get the minimum error?
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么需要一个优化算法？难道我们不能只是通过尝试大量的权重值（比如1,000个值）直到我们得到最小误差吗？
- en: Suppose we used a brute-force approach where we just tried a lot of different
    possible weights (say 1,000 values) and found the weight that produced the minimum
    error. Could that work? Well, theoretically, yes. This approach might work when
    we have very few inputs and only one or two neurons in our network. Let me try
    to convince you that this approach wouldn’t scale. Let’s take a look at a scenario
    where we have a very simple neural network. Suppose we want to predict house prices
    based on only four features (inputs) and one hidden layer of five neurons (see
    figure 2.28).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用了一种暴力方法，只是尝试了大量的不同可能的权重（比如说1,000个值），并找到了产生最小误差的权重。这能行得通吗？好吧，从理论上讲，是的。这种方法在我们只有很少的输入并且网络中只有一个或两个神经元时可能有效。让我尝试说服您这种方法不会扩展。让我们看看一个非常简单的神经网络场景。假设我们只想根据四个特征（输入）和一个包含五个神经元的隐藏层来预测房价（见图2.28）。
- en: '![](../Images/2-28.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-28.png)'
- en: Figure 2.28 If we want to predict house prices based on only four features (inputs)
    and one hidden layer of five neurons, we’ll have 20 edges (weights) from the input
    to the hidden layer, plus 5 weights from the hidden layer to the output prediction.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.28 如果我们只想根据四个特征（输入）和一个包含五个神经元的隐藏层来预测房价，我们将有从输入层到隐藏层的20个边缘（权重），再加上从隐藏层到输出预测的5个权重。
- en: 'As you can see, we have 20 edges (weights) from the input to the hidden layer,
    plus 5 weights from the hidden layer to the output prediction, totaling 25 weight
    variables that need to be adjusted for optimum values. To brute-force our way
    through a simple neural network of this size, if we are trying 1,000 different
    values for each weight, then we will have a total of 1075 combinations:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们从输入层到隐藏层的边缘（权重）有20个，再加上从隐藏层到输出预测的5个权重，总共需要调整25个权重变量以获得最佳值。如果我们为每个权重尝试1,000个不同的值，那么我们将有总共1,075种组合：
- en: 1,000 × 1,000 × . . . × 1,000 = 1,00025 = 1075 combinations
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000 × 1,000 × ... × 1,000 = 1,000^25 = 1,075种组合
- en: 'Let’s say we were able to get our hands on the fastest supercomputer in the
    world: Sunway TaihuLight, which operates at a speed of 93 petaflops ⇒ 93 × 1015
    floating-point operations per second (FLOPs). In the best-case scenario, this
    supercomputer would need'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们能够得到世界上最快的超级计算机：神威·太湖之光，其运行速度为93 petaflops ⇒ 93 × 10^15 每秒浮点运算（FLOPs）。在最佳情况下，这台超级计算机将需要
- en: '![](../Images/02_34_F1.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_34_F1.png)'
- en: 'That is a huge number: it’s longer than the universe has existed. Who has that
    kind of time to wait for the network to train? Remember that this is a very simple
    neural network that usually takes a few minutes to train using smart optimization
    algorithms. In the real world, you will be building more complex networks that
    have thousands of inputs and tens of hidden layers, and you will be required to
    train them in a matter of hours (or days, or sometimes weeks). So we have to come
    up with a different approach to find the optimal weights.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的数字：它比宇宙存在的时间还要长。谁有那么多时间等待网络训练？记住，这是一个非常简单的神经网络，通常使用智能优化算法只需要几分钟就能训练。在现实世界中，您将构建更复杂的网络，这些网络有数千个输入和数十个隐藏层，并且您需要在几小时（或几天，有时是几周）内训练它们。因此，我们必须想出一种不同的方法来找到最佳权重。
- en: 'Hopefully I have convinced you that brute-forcing through the optimization
    process is not the answer. Now, let’s study the most popular optimization algorithm
    for neural networks: gradient descent. Gradient descent has several variations:
    batch gradient descent (BGD), stochastic gradient descent (SGD), and mini-batch
    GD (MB-GD).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我已经说服你，通过暴力优化过程不是答案。现在，让我们研究神经网络中最受欢迎的优化算法：梯度下降。梯度下降有几个变体：批量梯度下降（BGD）、随机梯度下降（SGD）和迷你批量GD（MB-GD）。
- en: 2.6.2 Batch gradient descent
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 批量梯度下降
- en: The general definition of a gradient (also known as a derivative) is that it
    is the function that tells you the slope or rate of change of the line that is
    tangent to the curve at any given point. It is just a fancy term for the slope
    or steepness of the curve (figure 2.29).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的通用定义（也称为导数）是，它是告诉你曲线在任意给定点的切线斜率或变化率的函数。这只是曲线斜率或陡度的花哨说法（图2.29）。
- en: '![](../Images/2-29.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-29.png)'
- en: Figure 2.29 A gradient is the function that describes the rate of change of
    the line that is tangent to a curve at any given point.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.29 梯度是描述曲线在任意给定点的切线斜率变化率的函数。
- en: Gradient descent simply means updating the weights iteratively to descend the
    slope of the error curve until we get to the point with minimum error. Let’s take
    a look at the error function that we introduced earlier with respect to the weights.
    At the initial weight point, we calculate the derivative of the error function
    to get the slope (direction) of the next step. We keep repeating this process
    to take steps down the curve until we reach the minimum error (figure 2.30).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降简单来说就是迭代更新权重，以下降误差曲线的斜率，直到我们到达最小误差的点。让我们看看我们之前引入的关于权重的误差函数。在初始权重点，我们计算误差函数的导数以得到下一步的斜率（方向）。我们重复这个过程，沿着曲线向下走，直到我们达到最小误差（图2.30）。
- en: '![](../Images/2-30.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-30.png)'
- en: Figure 2.30 Gradient descent takes incremental steps to descend the error function.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.30 梯度下降通过增量步骤下降误差函数。
- en: How does gradient descent work?
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降是如何工作的？
- en: 'To visualize how gradient descent works, let’s plot the error function in a
    3D graph (figure 2.31) and go through the process step by step. The random initial
    weight (starting weight) is at point A, and our goal is to descend this error
    mountain to the goal *w*[1] and *w*[2] weight values, which produce the minimum
    error value. The way we do that is by taking a series of steps down the curve
    until we get the minimum error. In order to descend the error mountain, we need
    to determine two things for each step:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化梯度下降的工作原理，让我们在3D图中绘制误差函数（图2.31），并逐步分析这个过程。随机初始权重（起始权重）位于点A，我们的目标是下降到误差山的目标权重值
    *w*[1] 和 *w*[2]，这些权重值产生最小的误差值。我们这样做的方式是通过一系列沿着曲线的步骤，直到我们得到最小误差。为了下降误差山，我们需要确定每一步的两个东西：
- en: The step direction (gradient)
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长方向（梯度）
- en: The step size (learning rate)
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长大小（学习率）
- en: '![](../Images/2-31.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-31.png)'
- en: Figure 2.31 The random initial weight (starting weight) is at point A. We descend
    the error mountain to the *w*[1] and *w*[2] weight values that produce the minimum
    error value.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.31 随机初始权重（起始权重）位于点A。我们下降到产生最小误差值的权重值 *w*[1] 和 *w*[2]，以下降误差山。
- en: The direction (gradient)
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方向（梯度）
- en: Suppose you are standing on the top of the error mountain at point A. To get
    to the bottom, you need to determine the step direction that results in the deepest
    descent (has the steepest slope). And what is the slope, again? It is the derivative
    of the curve. So if you are standing on top of that mountain, you need to look
    at all the directions around you and find out which direction will result in the
    deepest descent (1, 2, 3, or 4, for example). Let’s say it is direction 3; we
    choose that way. This brings us to point B, and we restart the process (calculate
    feedforward and error) and find the direction of deepest descent, and so forth,
    until we get to the bottom of the mountain.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你站在误差山的顶部点A。为了到达底部，你需要确定导致最深下降（具有最陡斜率）的步长方向。那么斜率是什么呢？它是曲线的导数。所以如果你站在那座山的顶部，你需要看看你周围的各个方向，找出哪个方向会导致最深下降（例如1、2、3或4）。假设是方向3；我们选择那条路。这把我们带到了点B，然后我们重新开始这个过程（计算前向传播和误差）并找到最深下降的方向，以此类推，直到我们到达山的底部。
- en: This process is called gradient descent. By taking the derivative of the error
    with respect to the weight (*dE / Dw*), we get the direction that we should take.
    Now there’s one thing left. The gradient only determines the direction. How large
    should the size of the step be? It could be a 1-foot step or a 100-foot jump.
    This is what we need to determine next.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为梯度下降。通过对权重相对于误差的导数（*dE / Dw*）进行计算，我们得到我们应该采取的方向。现在还有一件事。梯度只决定了方向。步长的大小应该是多少？它可能是一英尺的步长，也可能是一百英尺的跳跃。这是我们接下来需要确定的事情。
- en: The step size (learning rate α )
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步长（学习率α）
- en: The learning rate is the size of each step the network takes when it descends
    the error mountain, and it is usually denoted by the Greek letter alpha (α ).
    It is one of the most important hyperparameters that you tune when you train your
    neural network (more on that later). A larger learning rate means the network
    will learn faster (since it is descending the mountain with larger steps), and
    smaller steps mean slower learning. Well, this sounds simple enough. Let’s use
    large learning rates and complete the neural network training in minutes instead
    of waiting for hours. Right? Not quite. Let’s take a look at what could happen
    if we set a very large learning rate value.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是网络在下降误差山时每一步的大小，通常用希腊字母alpha（α）表示。它是你在训练神经网络时调整的最重要超参数之一（关于这一点稍后还会讨论）。较大的学习率意味着网络将学习得更快（因为它以更大的步长下降山），而较小的步长意味着学习较慢。听起来很简单。让我们使用大的学习率，在几分钟内完成神经网络训练，而不是等待几个小时。对吧？并不完全是这样。让我们看看如果我们设置一个非常大的学习率值可能会发生什么。
- en: 'In figure 2.32, you are starting at point A. When you take a large step in
    the direction of the arrow, instead of descending the error mountain, you end
    up at point B, on the other side. Then another large step takes you to C, and
    so forth. The error will keep oscillating and will never descend. We will talk
    more later about tuning the learning rate and how to determine if the error is
    oscillating. But for now, you need to know this: if you use a very small learning
    rate, the network will eventually descend the mountain and will get to the minimum
    error. But this training will take longer (maybe weeks or months). On the other
    hand, if you use a very large learning rate, the network might keep oscillating
    and never train. So we usually initialize the learning rate value to 0.1 or 0.01
    and see how the network performs, and then tune it further.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.32中，你从点A开始。当你沿着箭头方向迈出大步时，你不会下降误差山，而是最终到达点B，在另一边。然后另一个大步带你到C，以此类推。误差将持续振荡，永远不会下降。我们稍后会更多地讨论调整学习率以及如何确定误差是否在振荡。但就目前而言，你需要知道这一点：如果你使用一个非常小的学习率，网络最终会下降到山脚下，并达到最小误差。但这种训练会花费更长的时间（可能是几周或几个月）。另一方面，如果你使用一个非常大的学习率，网络可能会持续振荡，永远不会训练。所以我们通常将学习率初始化为0.1或0.01，并观察网络的性能，然后进一步调整。
- en: '![](../Images/2-32.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-32.png)'
- en: Figure 2.32 Setting a very large learning rate causes the error to oscillate
    and never descend.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.32 设置一个非常大的学习率会导致误差振荡而永远不会下降。
- en: Putting direction and step together
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将方向和步长结合起来
- en: 'By multiplying the direction (derivative) by the step size (learning rate),
    we get the change of the weight for each step:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将方向（导数）乘以步长（学习率），我们得到每一步的权重变化：
- en: '![](../Images/02_38_F1.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_38_F1.png)'
- en: 'We add the minus sign because the derivative always calculates the slope in
    the upward direction. Since we need to descend the mountain, we go in the opposite
    direction of the slope:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加负号是因为导数总是计算向上的斜率。由于我们需要下降山，我们就沿着斜率的反方向前进：
- en: '*w[next-step] = w[current] + Δw*'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '*w[next-step] = w[current] + Δw*'
- en: 'Calculus refresher: Calculating the partial derivative'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分复习：计算偏导数
- en: The derivative is the study of change. It measures the steepness of a curve
    at a particular point on a graph.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是变化的研究。它测量了图表上特定点的曲线的陡峭程度。
- en: '![](../Images/2-unnumb-19K.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-unnumb-19K.png)'
- en: We want to find the steepness of the curve at the exact weight point.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找到曲线在确切权重点的陡峭程度。
- en: It looks like mathematics has given us just what we are looking for. On the
    error graph, we want to find the steepness of the curve at the exact weight point.
    Thank you, math!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来数学已经给了我们我们正在寻找的东西。在误差图表上，我们想要找到曲线在确切权重点的陡峭程度。谢谢，数学！
- en: Other terms for derivative are slope and rate of change. If the error function
    is denoted as E(*x*), then the derivative of the error function with respect to
    the weight is denoted as
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的其他术语是斜率和变化率。如果误差函数表示为E(*x*)，那么误差函数关于权重的导数表示为
- en: '*d/dw•E(x)* or just *dE(x)/dw*.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '*d/dw•E(x)*或简称为*dE(x)/dw*。'
- en: This formula shows how much the total error will change when we change the weight.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式显示了当我们改变权重时，总误差将如何变化。
- en: 'Luckily, mathematicians created some rules for us to calculate the derivative.
    Since this is not a mathematics book, we will not discuss the proof of the rules.
    Instead, we will start applying these rules at this point to calculate our gradient.
    Here are the basic derivative rules:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数学家为我们制定了一些规则来计算导数。由于这不是一本数学书，我们不会讨论这些规则的证明。相反，我们将从这一点开始应用这些规则来计算我们的梯度。以下是一些基本的导数规则：
- en: '![](../Images/2-unnumb-19b.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![图2.33](../Images/2-unnumb-19b.png)'
- en: 'Let’s take a look at a simple function to apply the derivative rules:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的函数来应用导数规则：
- en: '*f*(*x*) = 10*x*⁵ + 4*x*⁷ + 12*x*'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) = 10*x*⁵ + 4*x*⁷ + 12*x*'
- en: 'We can apply the power, constant, and sum rules to get *df/df* also denoted
    as *f*'' (*x*):'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用幂、常数和求和规则来得到*df/df*，也称为*f*' (*x*)：
- en: then, *f*' (*x*) = 50*x*⁴ + 28*x*⁶ + 12
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*f*' (*x*) = 50*x*⁴ + 28*x*⁶ + 12
- en: 'To get an intuition of what this means, let’s plot *f*(*x*):'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这意味着什么，让我们绘制*f*(*x*)的图像：
- en: '![](../Images/2-unnumb-20K.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图2.33](../Images/2-unnumb-20K.png)'
- en: Using a simple function to apply derivative rules. To get the slope at any point,
    we can compute f' (*x*) at that point.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单函数应用导数规则。要得到任何点的斜率，我们可以在该点计算f' (*x*)。
- en: If we want to get the slope at any point, we can compute f ' (*x*) at that point.
    So f ' (2) gives us the slope of the line on the left, and f ' (6) gives the slope
    of the second line. Get it?
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在任何一点得到斜率，我们可以在该点计算f ' (*x*)。所以f ' (2)给出了左侧线的斜率，而f ' (6)给出了第二条线的斜率。明白了吗？
- en: 'For a last example of derivatives, let’s apply the power rule to calculate
    the derivative of the sigmoid function:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对于导数的最后一个例子，让我们应用幂规则来计算sigmoid函数的导数：
- en: '![](../Images/2-unnumb-20b.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![图2.33](../Images/2-unnumb-20b.png)'
- en: Note that you don’t need to memorize the derivative rules, nor do you need to
    calculate the derivatives of the functions yourself. Thanks to the awesome DL
    community, we have great libraries that will compute these functions for you in
    just one line of code. But it is valuable to understand how things are happening
    under the hood.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不需要记住导数规则，也不需要自己计算函数的导数。多亏了出色的深度学习社区，我们有了伟大的库，只需一行代码就能为你计算这些函数。但了解底层发生的事情是有价值的。
- en: Pitfalls of batch gradient descent
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量梯度下降的陷阱
- en: Gradient descent is a very powerful algorithm to get to the minimum error. But
    it has two major pitfalls.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一个非常强大的算法，用于达到最小误差。但它有两个主要陷阱。
- en: First, not all cost functions look like the simple bowls we saw earlier. There
    may be holes, ridges, and all sorts of irregular terrain that make reaching the
    minimum error very difficult. Consider figure 2.33, where the error function is
    a little more complex and has ups and downs.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，并非所有成本函数都像我们之前看到的简单碗一样。可能会有洞、脊和各种不规则地形，这使得达到最小误差非常困难。考虑图2.33，其中误差函数稍微复杂一些，有起伏。
- en: '![](../Images/2-33.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![图2.33](../Images/2-33.png)'
- en: Figure 2.33 Complex error functions are represented by more complex curves with
    many local minima values. Our goal is to reach the global minimum value.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.33 复杂的误差函数由更复杂的曲线表示，具有许多局部最小值。我们的目标是达到全局最小值。
- en: Remember that during weight initialization, the starting point is randomly selected.
    What if the starting point of the gradient descent algorithm is as shown in this
    figure? The error will start descending the small mountain on the right and will
    indeed reach a minimum value. But this minimum value, called the local minima,
    is not the lowest possible error value for this error function. It is the minimum
    value for the local mountain where the algorithm randomly started. Instead, we
    want to get to the lowest possible error value, the global minima.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在权重初始化期间，起点是随机选择的。如果梯度下降算法的起点如图所示，误差将开始下降右侧的小山，并确实达到一个最小值。但这个最小值，称为局部最小值，并不是这个误差函数可能的最小误差值。它是算法随机开始的地方的局部山的最小值。相反，我们想要达到最低可能的误差值，即全局最小值。
- en: Second, batch gradient descent uses the entire training set to compute the gradients
    at every step. Remember this loss function?
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，批量梯度下降在每一步都使用整个训练集来计算梯度。还记得这个损失函数吗？
- en: '![](../Images/02_42_F1.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_42_F1.png)'
- en: This means that if your training set (*n*) has 100,000,000 (100 million) records,
    the algorithm needs to sum over 100 million records just to take one step. That
    is computationally very expensive and slow. And this is why this algorithm is
    also called batch gradient descent --because it uses the entire training data
    in one batch.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果您的训练集（*n*）有1亿（1亿）条记录，算法需要计算1亿条记录的总和才能迈出一步。这在计算上非常昂贵且缓慢。这就是为什么这个算法也被称为批量梯度下降——因为它在一次批次中使用了整个训练数据。
- en: One possible approach to solving these two problems is stochastic gradient descent.
    We’ll take a look at SGD in the next section.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这两个问题的一个可能方法是随机梯度下降。我们将在下一节中探讨SGD。
- en: 2.6.3 Stochastic gradient descent
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.3 随机梯度下降
- en: In stochastic gradient descent, the algorithm randomly selects data points and
    goes through the gradient descent one data point at a time (figure 2.34). This
    provides many different weight starting points and descends all the mountains
    to calculate their local minimas. Then the minimum value of all these local minimas
    is the global minima. This sounds very intuitive; that is the concept behind the
    SGD algorithm.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机梯度下降中，算法随机选择数据点，并逐个数据点进行梯度下降（图2.34）。这提供了许多不同的权重起始点，并下降到所有山峰以计算它们的局部最小值。然后，所有这些局部最小值中的最小值就是全局最小值。这听起来非常直观；这就是SGD算法背后的概念。
- en: '![](../Images/2-34.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-34.png)'
- en: Figure 2.34 The stochastic gradient descent algorithm randomly selects data
    points across the curve and descends all of them to find the local minima.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.34 随机梯度下降算法随机选择曲线上的数据点，并将它们全部下降以找到局部最小值。
- en: 'Stochastic is just a fancy word for random. Stochastic gradient descent is
    probably the most-used optimization algorithm for machine learning in general
    and for deep learning in particular. While gradient descent measures the loss
    and gradient over the full training set to take one step toward the minimum, SGD
    randomly picks one instance in the training set for each one step and calculates
    the gradient based only on that single instance. Let’s take a look at the pseudocode
    of both GD and SGD to get a better understanding of the differences between these
    algorithms:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 随机只是一个花哨的词，指的是随机。随机梯度下降可能是机器学习中最常用的优化算法，尤其是在深度学习中。虽然梯度下降测量整个训练集的损失和梯度，以向最小值迈出一步，但SGD在每一步随机选择训练集中的单个实例，并仅基于该单个实例计算梯度。让我们看一下GD和SGD的伪代码，以更好地理解这些算法之间的差异：
- en: '| GD | Stochastic GD |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| GD | 随机GD |'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Take all the data.
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取用所有数据。
- en: Compute the gradient.
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度。
- en: Update the weights and take a step down.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重并向下移动一步。
- en: Repeat for n number of epochs (iterations).
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行n个epoch（迭代）。
- en: '![](../Images/2-unnumb-21.png)A smooth path for the GD down the error curve
    |'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/2-unnumb-21.png)GD沿着误差曲线的平滑路径下降 |'
- en: Randomly shuffle samples in the training set.
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机打乱训练集中的样本。
- en: Pick one data instance.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个数据实例。
- en: Compute the gradient.
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度。
- en: Update the weights and take a step down.
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重并向下移动一步。
- en: Pick another one data instance.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择另一个数据实例。
- en: Repeat for n number of epochs (training iterations).
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行n个epoch（训练迭代）。
- en: '![](../Images/2-unnumb-22.png)An oscillated path for SGD down the error curve
    |'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/2-unnumb-22.png)SGD沿着误差曲线的振荡路径下降 |'
- en: Because we take a step after we compute the gradient for the entire training
    data in batch GD, you can see that the path down the error is smooth and almost
    a straight line. In contrast, due to the stochastic (random) nature of SGD, you
    see the path toward the global cost minimum is not direct but may zigzag if we
    visualize the cost surface in a 2D space. That is because in SGD, every iteration
    tries to better fit just a single training example, which makes it a lot faster
    but does not guarantee that every step takes us a step down the curve. It will
    arrive close to the global minimum and, once it gets there, it will continue to
    bounce around, never settling down. In practice, this isn’t a problem because
    ending up very close to the global minimum is good enough for most practical purposes.
    SGD almost always performs better and faster than batch GD.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在批量梯度下降中在整个训练数据上计算梯度后才会进行一步，因此你可以看到误差下降的路径是平滑的，几乎是一条直线。相比之下，由于随机梯度下降（SGD）的随机（随机）性质，你看到指向全局成本最小值的路径不是直接的，如果在二维空间中可视化成本表面，它可能会出现曲折。这是因为SGD中，每次迭代都试图更好地拟合单个训练示例，这使得它变得非常快，但并不保证每一步都会使曲线下降。它将接近全局最小值，一旦到达那里，它将继续弹跳，永远不会稳定下来。在实践中，这并不是一个问题，因为接近全局最小值对于大多数实际应用来说已经足够好了。SGD几乎总是比批量梯度下降表现得更好、更快。
- en: 2.6.4 Mini-batch gradient descent
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.4 小批量梯度下降
- en: Mini-batch gradient descent (MB-GD) is a compromise between BGD and SGD. Instead
    of computing the gradient from one sample (SGD) or all samples (BGD), we divide
    the training sample into mini-batches from which to compute the gradient (a common
    mini-batch size is k = 256). MB-GD converges in fewer iterations than BGD because
    we update the weights more frequently; however, MB-GD lets us use vectorized operations,
    which typically result in a computational performance gain over SGD.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降（MB-GD）是批量梯度下降（BGD）和随机梯度下降（SGD）之间的折中方案。我们不是从单个样本（SGD）或所有样本（BGD）计算梯度，而是将训练样本分成小批量，从这些小批量中计算梯度（常见的批量大小是k
    = 256）。由于我们更频繁地更新权重，MB-GD比BGD收敛得更快；然而，MB-GD允许我们使用向量运算，这通常会导致比SGD更好的计算性能提升。
- en: 2.6.5 Gradient descent takeaways
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.5 梯度下降要点
- en: 'There is a lot going on here, so let’s sum it up, shall we? Here is how gradient
    descent is summarized in my head:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容，让我们总结一下，好吗？以下是我脑海中总结的梯度下降方法：
- en: 'Three types: batch, stochastic, and mini-batch.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种类型：批量、随机和小批量。
- en: 'All follow the same concept:'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些方法都遵循相同的概念：
- en: 'Find the direction of the steepest slope: the derivative of the error with
    respect to the weight *dE/Dw[i]*'
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到最陡斜率的方向：误差相对于权重的一阶导数 *dE/Dw[i]*
- en: Set the learning rate (or step size). The algorithm will compute the slope,
    but you will set the learning rate as a hyperparameter that you will tune by trial
    and error.
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置学习率（或步长）。算法将计算斜率，但你会将学习率作为一个超参数来设置，并通过试错法进行调整。
- en: Start the learning rate at 0.01, and then go down to 0.001, 0.0001, 0.00001\.
    The lower you set your learning rate, the more guaranteed you are to descend to
    the minimum error (if you train for an infinite time). Since we don’t have infinite
    time, 0.01 is a reasonable start, and then we go down from there.
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将学习率从0.01开始，然后降至0.001、0.0001、0.00001。你设置的学习率越低，你越有保证能够下降到最小误差（如果你无限期地训练）。由于我们没有无限的时间，0.01是一个合理的起点，然后我们从这个值开始逐渐降低。
- en: Batch GD updates the weights after computing the gradient of all the training
    data. This can be computationally very expensive when the data is huge. It doesn’t
    scale well.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量梯度下降（Batch GD）在计算所有训练数据的梯度后更新权重。当数据量很大时，这可能在计算上非常昂贵。它扩展性不好。
- en: Stochastic GD updates the weights after computing the gradient of a single instance
    of the training data. SGD is faster than BGD and usually reaches very close to
    the global minimum.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降（Stochastic GD）在计算训练数据单个实例的梯度后更新权重。SGD比批量梯度下降（BGD）更快，并且通常非常接近全局最小值。
- en: Mini-batch GD is a compromise between batch and stochastic, using neither all
    the data nor a single instance. Instead, it takes a group of training instances
    (called a mini-batch), computes the gradient on them and updates the weights,
    and then repeats until it processes all the training data. In most cases, MB-GD
    is a good starting point.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量梯度下降（Mini-batch GD）是批量梯度下降和随机梯度下降之间的折中方案，既不使用所有数据，也不使用单个实例。相反，它选取一组训练实例（称为小批量），在这些实例上计算梯度并更新权重，然后重复此过程，直到处理完所有训练数据。在大多数情况下，MB-GD是一个很好的起点。
- en: '`batch_size` is a hyperparameter that you will tune. This will come up again
    in the hyperparameter-tuning section in chapter 4\. But typically, you can start
    experimenting with batch_size = 32, 64, 128, 256.'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` 是一个需要调整的超参数。这一点将在第4章的超参数调整部分再次提到。但通常，你可以从 batch_size = 32, 64,
    128, 256 开始实验。'
- en: Don’t get batch_size confused with epochs. An epoch is the full cycle over all
    the training data. The batch is the number of training samples in the group for
    which we are computing the gradient. For example, if we have 1,000 samples in
    our training data and set batch_size = 256, then epoch 1 = batch 1 of 256 samples
    plus batch 2 (256 samples) plus batch 3 (256 samples) plus batch 4 (232 samples).
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要将batch_size与epochs混淆。一个epoch是在所有训练数据上的完整循环。批次数是在我们计算梯度的组中训练样本的数量。例如，如果我们有1,000个样本在训练数据中，并且将batch_size设置为256，那么epoch
    1 = 256个样本的batch 1加上batch 2（256个样本）加上batch 3（256个样本）加上batch 4（232个样本）。
- en: Finally, you need to know that a lot of variations to gradient descent have
    been used over the years, and this is a very active area of research. Some of
    the most popular enhancements are
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要知道，多年来已经使用了大量的梯度下降变体，这是一个非常活跃的研究领域。其中一些最受欢迎的改进包括
- en: Nesterov accelerated gradient
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov加速梯度
- en: RMSprop
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSprop
- en: Adam
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam
- en: Adagrad
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: Don’t worry about these optimizers now. In chapter 4, we will discuss tuning
    techniques to improve your optimizers in more detail.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心这些优化器。在第4章中，我们将更详细地讨论调整技术来改进你的优化器。
- en: 'I know that was a lot, but stay with me. These are the main things I want to
    you remember from this section:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这听起来很多，但请继续听。这些都是我想让你从本节记住的主要事情：
- en: How gradient descent works (slope plus step size)
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降是如何工作的（斜率加步长）
- en: The difference between batch, stochastic, and mini-batch GD
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量、随机和迷你批量的梯度下降之间的区别
- en: 'The GD hyperparameters that you will tune: learning rate and batch_size'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将调整的GD超参数：学习率和batch_size
- en: If you’ve got this covered, you are good to move to the next section. And don’t
    worry a lot about hyperparameter tuning. I’ll cover network tuning in more detail
    in coming chapters and in almost all the projects in this book.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经掌握了这些，你就可以进入下一节了。而且不要过于担心超参数调整。我将在接下来的章节中更详细地介绍网络调整，并在本书中的几乎所有项目中。
- en: 2.7 Backpropagation
  id: totrans-495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 反向传播
- en: 'Backpropagation is the core of how neural networks learn. Up until this point,
    you learned that training a neural network typically happens by the repetition
    of the following three steps:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是神经网络学习的基础。到目前为止，你已经了解到训练神经网络通常通过重复以下三个步骤来完成：
- en: 'Feedforward: get the linear combination (weighted sum), and apply the activation
    function to get the output prediction (*ŷ*):'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈：获取线性组合（加权求和），并应用激活函数以获得输出预测（*ŷ*）：
- en: '*ŷ* = σ · *W*^((3)) · σ · *W*^((2)) · σ · *W*^((1)) · (*x*)'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ŷ* = σ · *W*^((3)) · σ · *W*^((2)) · σ · *W*^((1)) · (*x*)'
- en: 'Compare the prediction with the label to calculate the error or loss function:'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预测与标签进行比较，以计算误差或损失函数：
- en: '![](../Images/02_42_F1.png)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02_42_F1.png)'
- en: 'Use a gradient descent optimization algorithm to compute the Δ*w* that optimizes
    the error function:'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降优化算法来计算Δ*w*，以优化误差函数：
- en: '![](../Images/02_38_F1.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_38_F1.png)'
- en: 'Backpropagate the Δw through the network to update the weights:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络反向传播Δw以更新权重：
- en: '![](../Images/2-unnumb-23.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-23.png)'
- en: 'In this section, we will dive deeper into the final step: backpropagation.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨最后一步：反向传播。
- en: 2.7.1 What is backpropagation?
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.1 什么是反向传播？
- en: Backpropagation, or backward pass, means propagating derivatives of the error
    with respect to each specific weight
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播，或反向传递，意味着传播误差相对于每个特定权重的导数
- en: '*dE/dw**[i]*'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '*dE/dw**[i]* '
- en: 'from the last layer (output) back to the first layer (inputs) to adjust weights.
    By propagating the Δw backward from the prediction node (*ŷ*) all the way through
    the hidden layers and back to the input layer, the weights get updated:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一层（输出）回到第一层（输入）以调整权重。通过从预测节点（*ŷ*）向后传播Δw，穿过隐藏层并回到输入层，权重得到更新：
- en: (*w**[next-step]* = *w**[current]* + Δ*w*)
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: (*w**[next-step]* = *w**[current]* + Δ*w*)
- en: This will take the error one step down the error mountain. Then the cycle starts
    again (steps 1 to 3) to update the weights and take the error another step down,
    until we get to the minimum error.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使误差沿着误差山下降一步。然后循环再次开始（步骤1到3）以更新权重并将误差再下降一步，直到我们达到最小误差。
- en: Backpropagation might sound clearer when we have only one weight. We simply
    adjust the weight by adding the Δw to the old weight *w**[new]* *= w- α•dE/dw**[i]*
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只有一个权重时，反向传播可能听起来更清晰。我们只需通过添加 Δw 到旧权重 *w**[new]* *= w- α•dE/dw**[i]* 来调整权重。
- en: But it gets complicated when we have a multilayer perceptron (MLP) network with
    many weight variables. To make this clearer, consider the scenario in figure 2.35.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们有一个具有许多权重变量的多层感知器（MLP）网络时，事情就会变得复杂。为了使这一点更清晰，请考虑图2.35中的场景。
- en: '![](../Images/2-35.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-35.png)'
- en: Figure 2.35 Backpropagation becomes complicated when we have a multilayer perceptron
    (MLP) network with many weight variables.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.35 当我们有一个具有许多权重变量的多层感知器（MLP）网络时，反向传播变得复杂。
- en: How do we compute the change of the total error with respect to *w[13]* *dE/dw**[13]*?
    Remember that *dE/dw**[13]* basically says, “How much will the total error change
    when we change the parameter *w[13]*?”
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算总误差相对于 *w[13]* 的变化率 *dE/dw**[13]*？记住，*dE/dw**[13]*基本上是说，“当我们改变参数 *w[13]*
    时，总误差会有多少变化？”
- en: We learned how to compute *dE/dw**[21]* by applying the derivative rules on
    the error function. That is straightforward because *w[21]* is directly connected
    to the error function. But to compute the derivatives of the total error with
    respect to the weights all the way back to the input, we need a calculus rule
    called the chain rule.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在误差函数上应用导数规则学习了如何计算 *dE/dw**[21]*。这很简单，因为 *w[21]* 是直接连接到误差函数的。但为了计算总误差相对于权重直到输入的导数，我们需要一个微积分规则，称为链式法则。
- en: 'Calculus refresher: Chain rule in derivatives'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分复习：导数的链式法则
- en: 'Back again to calculus. Remember the derivative rules that we listed earlier?
    One of the most important rules is the chain rule. Let’s dive deep into it to
    see how it is implemented in backpropagation:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回到微积分。还记得我们之前列出的导数规则吗？其中最重要的规则之一就是链式法则。让我们深入探讨它，看看它在反向传播中的实现方式：
- en: '![](../Images/02_45_F1.png)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_45_F1.png)'
- en: 'The chain rule is a formula for calculating the derivatives of functions that
    are composed of functions inside other functions. It is also called the outside-inside
    rule. Look at this:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则是计算由其他函数内部函数组成的函数的导数的公式。它也被称为外-内规则。看看这个：
- en: '![](../Images/02_45_F2.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_45_F2.png)'
- en: The chain rule says, “When composing functions, the derivatives just multiply.”
    That is going to be very useful for us when implementing backpropagation, because
    feedforwarding is just composing a bunch of functions, and backpropagation is
    taking the derivative at each piece of this function.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则指出，“在函数组合中，导数只是相乘。”这在实现反向传播时对我们非常有用，因为前向传播只是组合了一组函数，而反向传播是在这个函数的每一部分上求导。
- en: 'To implement the chain rule in backpropagation, all we are going to do is multiply
    a bunch of partial derivatives to get the effect of errors all the way back to
    the input. Here is how it works--but first, remember that our goal is to propagate
    the error backward all the way to the input layer. So in the following example,
    we want to calculate *dE/dx* ,which is the effect of total error on input (*x*):'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在反向传播中实现链式法则，我们只需要将多个偏导数相乘，以得到误差效应一直回到输入层。下面是如何工作的——但首先，记住我们的目标是反向传播误差直到输入层。所以在这个例子中，我们想要计算
    *dE/dx*，这是总误差对输入 (*x*) 的影响：
- en: '![](../Images/2-unnumb-24K.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb-24K.png)'
- en: All we do here is multiply the upstream gradient by the local gradient all the
    way until we get to the target value.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的只是将上游梯度乘以局部梯度，直到我们得到目标值。
- en: 'Figure 2.36 shows how backpropagation uses the chain rule to flow the gradients
    in the backward direction through the network. Let’s apply the chain rule to calculate
    the derivative of the error with respect to the third weight on the first input
    w 1,3(1), where the (1) means layer 1, and w1,3 means node number 1 and weight
    number 3:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.36显示了反向传播如何使用链式法则将梯度反向流动通过网络。让我们应用链式法则来计算误差相对于第一个输入的第三个权重 w1,3(1) 的导数，其中
    (1) 表示第1层，w1,3 表示节点编号1和权重编号3：
- en: '![](../Images/02_46_F1.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_46_F1.png)'
- en: '![](../Images/2-36.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-36.png)'
- en: Figure 2.36 Backpropagation uses the chain rule to flow gradients back through
    the network.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.36 反向传播使用链式法则将梯度反向流动通过网络。
- en: 'The equation might look complex at the beginning, but all we are doing really
    is multiplying the partial derivative of the edges starting from the output node
    all the way backward to the input node. All the notations are what makes this
    look complex, but once you understand how to read *w**[1,3]**^((1))* , the backward-pass
    equation looks like this:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式一开始可能看起来很复杂，但我们实际上只是在从输出节点开始，将边的偏导数乘到输入节点。所有这些符号都使得这个方程看起来很复杂，但一旦你理解了如何读取
    *w**[1,3]**^((1))* ，反向传播方程看起来就像这样：
- en: The error backpropagated to the edge *w**[1,3]**^((1))* = effect of error on
    edge 4 · effect on edge 3 · effect on edge 2 · effect on target edge
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播到边 *w**[1,3]**^((1))* 的错误 = 边4上的错误效应 × 边3上的错误效应 × 边2上的错误效应 × 目标边的错误效应
- en: There you have it. That is the backpropagation technique used by neural networks
    to update the weights to best fit our problem.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络用来更新权重以最佳拟合我们问题的反向传播技术。
- en: '![](../Images/2-37.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图2.37](../Images/2-37.png)'
- en: Figure 2.37 The forward pass calculates the output prediction (left). The backward
    pass passes the derivative of the error backward to update its weights (right).
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.37 前向传播计算输出预测（左）。反向传播将误差的导数反向传播以更新其权重（右）。
- en: 2.7.2 Backpropagation takeaways
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.2 反向传播要点
- en: Backpropagation is a learning procedure for neurons.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播是神经元的学习过程。
- en: Backpropagation repeatedly adjusts weights of the connections (weights) in the
    network to minimize the cost function (the difference between the actual output
    vector and the desired output vector).
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播反复调整网络中连接（权重）的权重，以最小化成本函数（实际输出向量与期望输出向量之间的差异）。
- en: As a result of the weight adjustments, hidden layers come to represent important
    features other than the features represented in the input layer.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于权重调整的结果，隐藏层开始代表除了输入层中代表的特征之外的重要特征。
- en: For each layer, the goal is to find a set of weights that ensures that for each
    input vector, the output vector produced is the same as (or close to) the desired
    output vector. The difference in values between the produced and desired outputs
    is called the error function.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一层，目标是找到一组权重，确保对于每个输入向量，产生的输出向量与（或接近）期望的输出向量相同。产生的输出和期望输出之间的差异称为误差函数。
- en: The backward pass (backpropagation; figure 2.37) starts at the end of the network,
    backpropagates or feeds the errors back, recursively applies the chain rule to
    compute gradients all the way to the inputs of the network, and then updates the
    weights.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播（图2.37）从网络的末端开始，递归地应用链式法则来计算梯度，将错误反向传播或反馈，一直计算到网络的输入端，然后更新权重。
- en: To reiterate, the goal of a typical neural network problem is to discover a
    model that best fits our data Ultimately, we want to minimize the cost or loss
    function by choosing the best set of weight parameters.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次强调，典型神经网络问题的目标是发现一个最佳拟合我们数据的模型。最终，我们希望通过选择最佳的一组权重参数来最小化成本或损失函数。
- en: Summary
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Perceptrons work fine for datasets that can be separated by one straight line
    (linear operation).
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器对于可以用一条直线（线性操作）分离的数据集效果良好。
- en: Nonlinear datasets that cannot be modeled by a straight line need a more complex
    neural network that contains many neurons. Stacking neurons in layers creates
    a multilayer perceptron.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法用直线建模的非线性数据集需要一个包含许多神经元的更复杂的神经网络。通过层叠神经元创建多层感知器。
- en: 'The network learns by the repetition of three main steps: feedforward, calculate
    error, and optimize weights.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络通过重复三个主要步骤来学习：前向传播、计算误差和优化权重。
- en: Parameters are variables that are updated by the network during the training
    process, like weights and biases. These are tuned automatically by the model during
    training.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数是网络在训练过程中更新的变量，如权重和偏差。这些在训练过程中由模型自动调整。
- en: Hyperparameters are variables that you tune, such as number of layers, activation
    functions, loss functions, optimizers, early stopping, and learning rate. We tune
    these before training the model.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数是你调整的变量，例如层数、激活函数、损失函数、优化器、早停和学习率。我们在训练模型之前调整这些参数。
