- en: 9 More stable value-based methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 更稳定的基于价值的方法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will improve on the methods you learned in the previous chapter by making
    them more stable and therefore less prone to divergence.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将通过使它们更稳定来改进你在上一章中学到的方法，从而减少它们发散的可能性。
- en: You will explore advanced value-based deep reinforcement learning methods, and
    the many components that make value-based methods better.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将探索高级基于价值的深度强化学习方法，以及使基于价值的方法变得更好的许多组成部分。
- en: You will solve the cart-pole environment in a fewer number of samples, and with
    more reliable and consistent results.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将以更少的样本数量解决cart-pole环境，并得到更可靠和一致的结果。
- en: Let thy step be slow and steady, that thou stumble not.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让你的步伐缓慢而稳健，以免跌倒。
- en: — Tokugawa Ieyasu Founder and first shōgun of the Tokugawa shogunate of Japan
    and one of the three unifiers of Japan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ——德川家康 日本德川幕府的创始人、第一任将军，也是日本三位统一者之一
- en: 'In the last chapter, you learned about value-based deep reinforcement learning.
    NFQ, the algorithm we developed, is a simple solution to the two most common issues
    value-based methods face: first, the issue that data in RL isn’t independent and
    identically distributed. It’s probably the exact opposite. The experiences are
    dependent on the policy that generates them. And, they aren’t identically distributed
    since the policy changes throughout the training process. Second, the targets
    we use aren’t stationary, either. Optimization methods require fixed targets for
    robust performance. In supervised learning, this is easy to see. We have a dataset
    with premade labels as constants, and our optimization method uses these fixed
    targets for stochastically approximating the underlying data-generating function.
    In RL, on the other hand, targets such as the *TD* target use the reward and the
    discounted predicted return from the landing state as a target. But this predicted
    return comes from the network we’re optimizing, which changes every time we execute
    the optimization steps. This issue creates a moving target that creates instabilities
    in the training process.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了基于价值的深度强化学习。NFQ，我们开发的算法，是解决基于价值的方法面临的最常见问题的简单解决方案：首先，RL中的数据不是独立同分布的问题。这可能是完全相反的。经验依赖于生成它们的策略。而且，由于策略在整个训练过程中发生变化，它们不是同分布的。其次，我们使用的目标也不是平稳的。优化方法需要固定的目标才能有稳健的性能。在监督学习中，这一点很容易看出。我们有一个带有预制标签的常量数据集，我们的优化方法使用这些固定目标来随机近似潜在的数据生成函数。另一方面，在RL中，目标如*TD*目标使用奖励和从着陆状态的折现预测回报作为目标。但是，这个预测回报来自我们正在优化的网络，每次执行优化步骤时都会发生变化。这个问题创建了一个移动的目标，在训练过程中产生了不稳定性。
- en: The way NFQ addresses these issues is through the use of batching. By growing
    a batch, we have the opportunity of optimizing several samples at the same time.
    The larger the batch, the more the opportunity for collecting a diverse set of
    experience samples. This somewhat addresses the IID assumption. NFQ addresses
    the stationarity of target requirements by using the same mini-batch in multiple
    sequential optimization steps. Remember that in NFQ, every E episode, we “fit”
    the neural network to the same mini-batch *K* times. That *K* allows the optimization
    method to move toward the target more stably. Gathering a batch and fitting the
    model for multiple iterations is similar to the way we train supervised learning
    methods, in which we gather a dataset and train for multiple epochs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NFQ解决这些问题的方法是通过批处理。通过增长一个批次，我们有同时优化多个样本的机会。批次越大，收集到的经验样本集的多样性就越多。这在一定程度上解决了独立同分布的假设问题。NFQ通过在多个连续优化步骤中使用相同的迷你批次来解决目标要求的平稳性。记住，在NFQ中，每个E个片段，我们“拟合”神经网络到相同的迷你批次*K*次。这个*K*允许优化方法更稳定地向目标移动。收集一个批次并对模型进行多次迭代拟合，类似于我们训练监督学习方法的步骤，其中我们收集数据集并进行多次epoch的训练。
- en: NFQ does an okay job, but we can do better. Now that we know the issues, we
    can address them using better techniques. In this chapter, we explore algorithms
    that address not only these issues, but other issues that you learn about making
    value-based methods more stable.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: NFQ做得还不错，但我们能做得更好。既然我们知道问题所在，我们就可以使用更好的技术来解决它们。在本章中，我们探讨了不仅解决这些问题，还解决其他问题的算法，这些问题是你在学习如何使基于价值的方法更稳定时遇到的。
- en: 'DQN: Making reinforcement learning more like supervised learning'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN：使强化学习更像监督学习
- en: The first algorithm that we discuss in this chapter is called *deep Q-network*
    (DQN). DQN is one of the most popular DRL algorithms because it started a series
    of research innovations that mark the history of RL. DQN claimed for the first
    time superhuman level performance on an Atari benchmark in which agents learned
    from raw pixel data from mere images.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的第一个算法被称为**深度Q网络**（DQN）。DQN是最受欢迎的DRL算法之一，因为它开启了一系列研究创新，标志着强化学习的历史。DQN首次在Atari基准测试中实现了超人类水平的表现，其中智能体从仅图像的原始像素数据中学习。
- en: Throughout the years, there have been many improvements proposed to DQN. And
    while these days DQN in its original form is not a go-to algorithm, with the improvements,
    many of which you learn about in this book, the algorithm still has a spot among
    the best-performing DRL agents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，针对DQN提出了许多改进。尽管如今原始形式的DQN不再是首选算法，但随着这些改进（其中许多你将在本书中学到），该算法仍然在表现最佳的DRL智能体中占有一席之地。
- en: Common problems in value-based deep reinforcement learning
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于价值的深度强化学习中的常见问题
- en: 'We must be clear and understand the two most common problems that consistently
    show up in value-based deep reinforcement learning: the violations of the IID
    assumption, and the stationarity of targets.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须明确并理解在基于价值的深度强化学习中持续出现的前两个最常见问题：违反独立同分布假设和目标的不变性。
- en: In supervised learning, we obtain a full dataset in advance. We preprocess it,
    shuffle it, and then split it into sets for training. One crucial step in this
    process is the shuffling of the dataset. By doing so, we allow our optimization
    method to avoid developing overfitting biases; reduce the variance of the training
    process; speed up convergence; and overall learn a more general representation
    of the underlying data-generating process. In reinforcement learning, unfortunately,
    data is often gathered online; as a result, the experience sample generated at
    time step *t+1* correlates with the experience sample generated at time step *t*.
    Moreover, as the policy is to improve, it changes the underlying data-generating
    process changes, too, which means that new data is locally correlated and not
    evenly distributed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们事先获得完整的数据集。我们对其进行预处理，打乱顺序，然后将其分成训练集。这个过程中的一个关键步骤是数据集的打乱。通过这样做，我们允许我们的优化方法避免发展过度拟合偏差；减少训练过程的方差；加快收敛速度；并且总体上学习一个更通用的底层数据生成过程的表示。不幸的是，在强化学习中，数据通常是在线收集的；因此，在时间步
    *t+1* 生成的经验样本与在时间步 *t* 生成的经验样本相关联。此外，由于策略是为了改进，它也会改变底层的数据生成过程，这意味着新数据是局部相关的，并且分布不均匀。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownData isn’t independently and
    identically distributed (IID) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ![简化](../Images/icons_Boil.png) | 简化数据不是独立同分布（IID）的'
- en: '|  | assumption of the data. Optimization methods have been developed with
    the assumption that samples in the dataset we train with are independent and identically
    distributed.We know, however, our samples aren’t independent, but instead, they
    come from a sequence, a time series, a trajectory. The sample at time step *t*+1
    is dependent on the sample at time step *t*. Samples are correlated, and we can’t
    prevent that from happening; it’s a natural consequence of online learning.But
    samples are also not identically distributed because they depend on the policy
    that generates the actions. We know the policy is changing through time, and for
    us that’s a good thing. We want policies to improve. But that also means the distribution
    of samples (state-action pairs visited) will change as we keep improving. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据的假设。优化方法是在假设我们训练数据集中的样本是独立同分布的情况下开发的。然而，我们知道我们的样本并不是独立的，而是来自一个序列，一个时间序列，一个轨迹。时间步
    *t*+1 的样本依赖于时间步 *t* 的样本。样本是相关的，我们无法阻止这种情况发生；这是在线学习的自然后果。但样本也不是同分布的，因为它们依赖于生成动作的策略。我们知道策略是随时间变化的，对我们来说这是好事。我们希望策略得到改进。但这也意味着随着我们不断改进，样本的分布（访问过的状态-动作对）也会发生变化。
    |'
- en: Also, in supervised learning, the targets used for training are fixed values
    on your dataset; they’re fixed throughout the training process. In reinforcement
    learning in general, and even more so in the extreme case of online learning,
    targets move with every training step of the network. At every training update
    step, we optimize the approximate value function and therefore change the shape
    of the function, possibly the entire value function. Changing the value function
    means that the target values change as well, which, in turn, means the targets
    used are no longer valid. Because the targets come from the network, even before
    we use them, we can assume targets are invalid or biased at a minimum.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在监督学习中，用于训练的目标是在你的数据集上的固定值；在整个训练过程中都是固定的。在强化学习一般情况中，尤其是在在线学习的极端情况下，目标随着网络的每次训练步骤而移动。在每次训练更新步骤中，我们优化近似值函数，因此改变函数的形状，可能是整个值函数的形状。改变值函数意味着目标值也会改变，这反过来又意味着所使用的目标不再有效。因为目标来自网络，甚至在我们在使用它们之前，我们可以假设目标至少是无效或有偏的。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownNon-stationarity of targets |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Boil.png) | 简化目标非平稳性 |'
- en: '|  | The problem of the non-stationarity of targets is depicted. These are
    the targets we use to train our network, but these targets are calculated using
    the network itself. As a result, the function changes with every update, in turn
    changing the targets.![](../Images/09_00_Sidebar02.png)Non-stationarity of targets
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标非平稳性问题描述。这些是我们用来训练网络的目标，但这些目标是使用网络本身计算得出的。因此，函数随着每次更新而改变，进而改变目标。'
- en: In NFQ, we lessen this problem by using a batch and fitting the network to a
    small fixed dataset for multiple iterations. In NFQ, we collect a small dataset,
    calculating targets, and optimize the network several times before going out to
    collect more samples. By doing this on a large batch of samples, the updates to
    the neural network are composed of many points across the function, additionally
    making changes even more stable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NFQ 中，我们通过使用批量处理并将网络拟合到一个小型固定数据集进行多次迭代来减轻这个问题。在 NFQ 中，我们收集一个小数据集，计算目标，并在收集更多样本之前优化网络几次。通过在大量样本上这样做，神经网络更新的变化由函数的许多点组成，从而使得变化更加稳定。
- en: DQN is an algorithm that addresses the question, how do we make reinforcement
    learning look more like supervised learning? Consider this question for a second,
    and think about the tweaks you would make to make the data look IID and the targets
    fixed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 是一个解决如何使强化学习看起来更像监督学习问题的算法？考虑这个问题，并思考你会对数据进行哪些调整以使其看起来是独立同分布的，并且目标值是固定的。
- en: Using target networks
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用目标网络
- en: A straightforward way to make target values more stationary is to have a separate
    network that we can fix for multiple steps and reserve it for calculating more
    stationary targets. The network with this purpose in DQN is called the *target
    network*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使目标值更加平稳的一个简单方法是有一个新的网络，我们可以固定它多次，并保留它来计算更平稳的目标。在 DQN 中，具有这种目的的网络被称为**目标网络**。
- en: '![](../Images/09_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_01.png)'
- en: Q-function optimization without a target network
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无目标网络 Q 函数优化
- en: '![](../Images/09_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_02.png)'
- en: Q-function approximation with a target network
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标网络的 Q 函数逼近
- en: By using a target network to fix targets, we mitigate the issue of “chasing
    your own tail” by artificially creating several small supervised learning problems
    presented sequentially to the agent. Our targets are fixed for as many steps as
    we fix our target network. This improves our chances of convergence, but not to
    the optimal values because such things don’t exist with non-linear function approximation,
    but convergence in general. But, more importantly, it substantially reduces the
    chance of divergence, which isn’t uncommon in value-based deep reinforcement learning
    methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用目标网络来固定目标，我们通过人为地创建几个依次呈现给代理的小型监督学习问题来缓解“追逐自己的尾巴”的问题。我们的目标固定与我们的目标网络固定的时间一样长。这提高了我们收敛的机会，但不是到最优值，因为非线性函数逼近中不存在这样的东西，但通常收敛。但更重要的是，它大大减少了发散的机会，这在基于价值的深度强化学习方法中并不罕见。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathTarget network gradient update
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Math.png) | 展示数学目标网络梯度更新 |'
- en: '|  | ![](../Images/09_02_Sidebar03.png) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/09_02_Sidebar03.png) |'
- en: It’s important to note that in practice we don’t have two “networks,” but instead,
    we have two instances of the neural network weights. We use the same model architecture
    and frequently update the weights of the target network to match the weights of
    the online network, which is the network we optimize on every step. “Frequently”
    here means something different depending on the problem, unfortunately. It’s common
    to freeze these target network weights for 10 to 10,000 steps at a time, again
    depending on the problem. (That’s time steps, not episodes. Be careful there.)
    If you’re using a convolutional neural network, such as what you’d use for learning
    in Atari games, then a 10,000-step frequency is the norm. But for more straightforward
    problems such as the cart-pole environment, 10–20 steps is more appropriate.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在实践中，我们并没有两个“网络”，而是有两个神经网络的权重实例。我们使用相同的模型架构，并频繁更新目标网络的权重以匹配在线网络的权重，这是我们在每一步上优化的网络。“频繁”在这里的含义取决于问题，不幸的是。通常，我们会冻结这些目标网络权重10到10,000步，这再次取决于问题。（那是时间步，不是回合。请小心。）如果你使用卷积神经网络，例如用于学习Atari游戏的网络，那么10,000步的频率是正常的。但对于更直接的问题，如小车-杆环境，10-20步更为合适。
- en: By using target networks, we prevent the training process from spiraling around
    because we’re fixing the targets for multiple time steps, thus allowing the online
    network weights to move consistently toward the targets before an update changes
    the optimization problem, and a new one is set. By using target networks, we stabilize
    training, but we also slow down learning because you’re no longer training on
    up-to-date values; the frozen weights of the target network can be lagging for
    up to 10,000 steps at a time. It’s essential to balance stability and speed and
    tune this hyperparameter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用目标网络，我们防止训练过程陷入螺旋，因为我们正在固定多个时间步的目标，从而允许在线网络权重在更新改变优化问题之前，一致地向目标移动，然后设置一个新的目标。通过使用目标网络，我们稳定了训练，但同时也减慢了学习速度，因为你不再基于最新的值进行训练；目标网络的冻结权重可能一次滞后多达10,000步。平衡稳定性和速度并调整这个超参数是至关重要的。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonUse of the target and online
    networks in DQN |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python使用目标网络和在线网络在DQN中的应用 |'
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Notice how we now query a target network to get the estimate of the next state.②
    We grab the maximum of those values, and make sure to treat terminal states appropriately.③
    Finally, we create the *TD* targets.④ Query the current “online” estimate.⑤ Use
    those values to create the errors.⑥ Calculate the loss, and optimize the online
    network.⑦ Notice how we use the online model for selecting actions.⑧ This is how
    the target network (lagging network) gets updated with the online network (up-to-date
    network). |
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ① 注意我们现在是如何查询目标网络来获取下一个状态的估计。② 我们获取这些值的最大值，并确保适当地处理终端状态。③ 最后，我们创建*TD*目标。④ 查询当前的“在线”估计。⑤
    使用这些值来创建误差。⑥ 计算损失，并优化在线网络。⑦ 注意我们是如何使用在线模型来选择动作的。⑧ 这就是目标网络（滞后网络）如何通过在线网络（最新网络）进行更新的。
    |
- en: Using larger networks
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用更大的网络
- en: Another way you can lessen the non-stationarity issue, to some degree, is to
    use larger networks. With more powerful networks, subtle differences between states
    are more likely to be detected. Larger networks reduce the aliasing of state-action
    pairs; the more powerful the network, the lower the aliasing; the lower the aliasing,
    the less apparent correlation between consecutive samples. And all of this can
    make target values and current estimates look more independent of each other.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在一定程度上减轻非平稳性问题的方法，是使用更大的网络。具有更强大网络的微妙状态差异更有可能被检测到。更大的网络减少了状态-动作对的混叠；网络越强大，混叠越低；混叠越低，连续样本之间的相关性就越不明显。所有这些都可以使目标值和当前估计看起来更独立于彼此。
- en: By “aliasing” here I refer to the fact that two states can look like the same
    (or quite similar) state to the neural network, but still possibly require different
    actions. State aliasing can occur when networks lack representational power. After
    all, neural networks are trying to find similarities to generalize; their job
    is to find these similarities. But, too small of a network can cause the generalization
    to go wrong. The network could get fixated with simple, easy to find patterns.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“别名”一词指的是两个状态可能看起来对神经网络来说是相同的（或者相当相似），但仍然可能需要不同的动作。当网络缺乏表示能力时，可能会发生状态别名。毕竟，神经网络试图找到相似之处以进行泛化；它们的任务是找到这些相似之处。但是，网络太小可能会导致泛化出错。网络可能会对简单、容易找到的模式产生固定。
- en: One of the motivations for using a target network is that they allow you to
    differentiate between correlated states more easily. Using a more capable network
    helps your network learn subtle differences, too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标网络的一个动机是它们允许你更容易地区分相关状态。使用一个更强大的网络也有助于你的网络学习细微的差异。
- en: But, a more powerful neural network takes longer to train. It needs not only
    more data (interaction time) but also more compute (processing time). Using a
    target network is a more robust approach to mitigating the non-stationary problem,
    but I want you to know all the tricks. It’s favorable for you to know how these
    two properties of your agent (the size of your networks, and the use of target
    networks, along with the update frequency), interact and affect final performance
    in similar ways.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个更强大的神经网络需要更长的时间来训练。它不仅需要更多的数据（交互时间），还需要更多的计算（处理时间）。使用目标网络是减轻非平稳问题的更稳健的方法，但我希望你知道所有的技巧。了解你的代理（网络的规模、使用目标网络以及更新频率）的这两个属性如何相互作用并影响最终性能是很有利的。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownWays to mitigate the fact that
    targets in reinforcement learning are non-stationary |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化目标Ways to mitigate the fact that targets
    in reinforcement learning are non-stationary |'
- en: '|  | Allow me to restate that to mitigate the non-stationarity issue we can'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 允许我重申，为了减轻非平稳性问题，我们可以'
- en: Create a target network that provides us with a temporarily stationary target
    value.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个目标网络，为我们提供一个暂时平稳的目标值。
- en: Create large-enough networks so that they can “see” the small differences between
    similar states (like those temporally correlated).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建足够大的网络，以便它们可以“看到”相似状态之间的微小差异（如那些时间上相关联的）。
- en: Target networks work and work well, and have been proven to work multiple times.
    The technique of “larger networks” is more of a hand-wavy solution than something
    scientifically proven to work every time. Feel free to experiment with this chapter’s
    Notebook. You’ll find it easy to change values and test hypotheses. |
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络效果很好，并且已经被证明多次有效。技术“更大的网络”更像是一种模糊的解决方案，而不是每次都经过科学验证的解决方案。请随意实验本章的笔记本。你会发现改变值和测试假设很容易。
    |
- en: Using experience replay
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用经验回放
- en: In our NFQ experiments, we use a mini-batch of 1,024 samples, and train with
    it for 40 iterations, alternating between calculating new targets and optimizing
    the network. These 1,024 samples are temporally correlated because most of them
    belong to the same trajectory, and the maximum number of steps in a cart-pole
    episode is 500\. One way to improve on this is to use a technique called *experience
    replay*. Experience replay consists of a data structure, often referred to as
    a replay buffer or a replay memory, that holds experience samples for several
    steps (much more than 1,024 steps), allowing the sampling of mini-batches from
    a broad set of past experiences. Having a replay buffer allows the agent two critical
    things. First, the training process can use a more diverse mini-batch for performing
    updates. Second, the agent no longer has to fit the model to the same small mini-batch
    for multiple iterations. Adequately sampling a sufficiently large replay buffer
    yields a slow-moving target, so the agent can now sample and train on every time
    step with a lower risk of divergence.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的NFQ实验中，我们使用一个包含1,024个样本的小批量，并以此进行40次迭代训练，交替进行计算新目标和优化网络。这1,024个样本在时间上是相关的，因为它们大多数属于同一条轨迹，而购物车-杆子实验中的最大步数是500步。改进这一点的办法之一是使用一种称为*经验回放*的技术。经验回放包括一种数据结构，通常被称为回放缓冲区或回放内存，它可以存储多个步骤的经验样本（远多于1,024步），从而允许从广泛的过去经验中采样小批量。拥有回放缓冲区让智能体获得两个关键的好处。首先，训练过程可以使用更多样化的小批量进行更新。其次，智能体不再需要在多次迭代中使模型适应相同的小批量。适当地采样足够大的回放缓冲区会产生一个缓慢移动的目标，因此智能体现在可以以较低的风险在每个时间步进行采样和训练。
- en: '| 0001 | A Bit Of HistoryIntroduction of experience replay |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍经验回放 |'
- en: '|  | Experience replay was introduced by Long-Ji Lin in a paper titled “Self-Improving
    Reactive Agents Based On Reinforcement Learning, Planning and Teaching,” believe
    it or not, published in *1992*! That’s right, 1992! Again, that’s when neural
    networks were referred to as “connectionism” ... Sad times!After getting his PhD
    from CMU, Dr. Lin moved through several technical roles in many different companies.
    Currently, he’s a Chief Scientist at Signifyd, leading a team that works on a
    system to predict and prevent online fraud. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | 经验回放是由Long-Ji Lin在一份题为“基于强化学习、规划和教学的自我改进反应性智能体，”的论文中引入的，无论你信不信，这篇论文发表于*1992年*！没错，1992年！再次强调，那正是神经网络被称为“连接主义”的时候...悲伤的时代！在从CMU获得博士学位后，林博士在多家公司担任了多个技术角色。目前，他是Signifyd的首席科学家，领导着一个致力于预测和防止在线欺诈的系统团队。|'
- en: There are multiple benefits to using experience replay. By sampling at random,
    we increase the probability that our updates to the neural network have low variance.
    When we used the batch in NFQ, most of the samples in that batch were correlated
    and similar. Updating with similar samples concentrates the changes on a limited
    area of the function, and that potentially overemphasizes the magnitude of the
    updates. If we sample uniformly at random from a substantial buffer, on the other
    hand, chances are that our updates to the network are better distributed all across,
    and therefore more representative of the true value function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用经验回放有多个好处。通过随机采样，我们增加了我们的神经网络更新具有低方差的概率。当我们使用NFQ中的批量时，该批量中的大多数样本都是相关的和相似的。使用相似样本进行更新会将变化集中在函数的有限区域内，这可能会过度强调更新的幅度。另一方面，如果我们从大量的缓冲区中均匀随机采样，那么我们的网络更新更有可能分布得更均匀，因此更能代表真实的价值函数。
- en: Using a replay buffer also gives the impression our data is IID so that the
    optimization method is stable. Samples appear independent and identically distributed
    because of the sampling from multiple trajectories and even policies at once.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回放缓冲区也给人一种我们的数据是独立同分布的印象，这样优化方法就稳定了。由于从多个轨迹甚至策略中同时采样，样本看起来是独立且同分布的。
- en: By storing experiences and later sampling them uniformly, we make the data entering
    the optimization method look independent and identically distributed. In practice,
    the replay buffer needs to have considerable capacity to perform optimally, from
    10,000 to 1,000,000 experiences depending on the problem. Once you hit the maximum
    size, you evict the oldest experience before inserting the new one.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过存储经验和稍后均匀采样，我们使进入优化方法的数据看起来是独立且同分布的。在实践中，回放缓冲区需要相当大的容量才能发挥最佳效果，从10,000到1,000,000个经验样本，具体取决于问题。一旦达到最大容量，在插入新经验之前，你需要移除最老的经验。
- en: '![](../Images/09_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_03.png)'
- en: DQN with a replay buffer
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 带有重放缓冲区的DQN
- en: Unfortunately, the implementation becomes a little bit of a challenge when working
    with high-dimensional observations, because poorly implemented replay buffers
    hit a hardware memory limit quickly in high-dimensional environments. In image-based
    environments, for instance, where each state representation is a stack of the
    four latest image frames, as is common for Atari games, you probably don’t have
    enough memory on your personal computer to naively store 1,000,000 experience
    samples. For the cart-pole environment, this isn’t so much of a problem. First,
    we don’t need 1,000,000 samples, and we use a buffer of size 50,000 instead. But
    also, states are represented by four-element vectors, so there isn’t so much of
    an implementation performance challenge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当与高维观察结果一起工作时，实现变得有点具有挑战性，因为实现不佳的重放缓冲区在高维环境中很快就会达到硬件内存限制。例如，在基于图像的环境中，例如，每个状态表示是四个最新图像帧的堆栈，这在Atari游戏中很常见，你可能在个人电脑上没有足够的内存来天真地存储1,000,000个经验样本。对于小车-杆环境，这并不是一个大问题。首先，我们不需要1,000,000个样本，我们使用一个大小为50,000的缓冲区。但更重要的是，状态由四个元素的向量表示，因此并没有太多的实现性能挑战。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathReplay buffer gradient update
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学重放缓冲区梯度更新'
- en: '|  | ![](../Images/09_03_Sidebar07.png) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏07.png](../Images/09_03_Sidebar07.png)'
- en: Nevertheless, by using a replay buffer, your data looks more IID and your targets
    more stationary than in reality. By training from uniformly sampled mini-batches,
    you make the RL experiences gathered online look more like a traditional supervised
    learning dataset with IID data and fixed targets. Sure, data is still changing
    as you add new and discard old samples, but these changes are happening slowly,
    and so they go somewhat unnoticed by the neural network and optimizer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，通过使用重放缓冲区，你的数据看起来比现实中更符合独立同分布（IID），而目标更稳定。通过从均匀采样的迷你批次中进行训练，你使得在线收集的强化学习经验看起来更像是一个具有独立同分布数据和固定目标的传统监督学习数据集。当然，随着你添加新的样本和丢弃旧的样本，数据仍在变化，但这些变化发生得较慢，因此神经网络和优化器几乎注意不到。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownExperience replay makes the data
    look IID, and targets somewhat stationary |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ![简化图标](../Images/icons_Boil.png) | 简化经验重放使数据看起来符合独立同分布，目标相对稳定'
- en: '|  | The best solution to the problem of data not being IID is called *experience
    replay.*The technique is simple, and it’s been around for decades: As your agent
    collects experiences tuples *e*[t]*=(S*[*t*]*,A*[*t*]*,R*[*t*+1]*,S*[*t*+1]) online,
    we insert them into a data structure, commonly referred to as the *replay buffer*
    *d*, such that *d={e*[1]*, e*[2] *, ... , e*[*M*]*}*. *m*, the size of the replay
    buffer, is a value often between 10,000 to 1,000,000, depending on the problem.We
    then train the agent on mini-batches sampled, usually uniformly at random, from
    the buffer, so that each sample has equal probability of being selected. Though,
    as you learn in the next chapter, you could possibly sample with another distribution.
    Just beware because it isn’t that straightforward. We’ll discuss details in the
    next chapter. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 解决数据不是独立同分布问题的最佳方案被称为*经验重放*。这项技术很简单，并且已经存在了几十年：当你的智能体在线收集经验元组 *e*[t]*=(S*[*t*]*,A*[*t*]*,R*[*t*+1]*,S*[*t*+1])
    时，我们将它们插入到一个数据结构中，通常称为*重放缓冲区* *d*，使得 *d={e*[1]*, e*[2] *, ... , e*[*M*]*}*. *m*，重放缓冲区的大小，通常在10,000到1,000,000之间，这取决于问题。然后我们通过从缓冲区中采样的迷你批次来训练智能体，通常随机均匀地采样，使得每个样本被选中的概率相等。尽管如此，正如你将在下一章中学到的，你可能会以另一种分布进行采样。但请注意，这并不那么简单。我们将在下一章中讨论细节。'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonA simple replay buffer |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python一个简单的重放缓冲区'
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① This is a simple replay buffer with a default maximum size of 50,000, and
    a default batch size of 64 samples.② We initialize five arrays to hold states,
    actions, reward, next states, and done flags. Shortened for brevity.③ We initialize
    several variables to do storage and sampling.④ When we store a new sample, we
    begin by unwrapping the sample variable, and then setting each array’s element
    to its corresponding value.⑤ Again removed for brevity⑥ _idx points to the next
    index to modify, so we increase it, and also make sure it loops back after reaching
    the maximum size (the end of the buffer).⑦ Size also increases with every new
    sample stored, but it doesn’t loop back to 0; it stops growing instead.⑧ In the
    sample function, we begin by determining the batch size. We use the default of
    64 if nothing else was passed.⑨ Sample batch_size ids from 0 to size.⑩ Then, extract
    the experiences from the buffer using the sampled ids.⑪ And return those experiences.⑫
    This is a handy function to return the correct size of the buffer when len(buffer)
    is called. |
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是一个简单的重放缓冲区，默认最大大小为50,000，默认批处理大小为64个样本。② 我们初始化五个数组来存储状态、动作、奖励、下一个状态和完成标志。为了简洁起见，省略了部分内容。③
    我们初始化几个变量来进行存储和采样。④ 当我们存储一个新的样本时，我们首先展开样本变量，然后设置每个数组的元素为其相应的值。⑤ 再次省略了部分内容⑥ _idx指向下一个要修改的索引，因此我们增加它，并确保在达到最大大小时（缓冲区的末尾）循环回0。⑦
    每存储一个新的样本，大小也会增加，但不会循环回0；它停止增长。⑧ 在sample函数中，我们首先确定批处理大小。如果没有传递其他内容，我们使用默认的64。⑨
    从0到大小中采样batch_size个id。⑩ 然后，使用采样id从缓冲区中提取经验。⑪ 并返回这些经验。⑫ 这是一个方便的函数，当调用len(buffer)时，返回缓冲区的正确大小。
    |
- en: Using other exploration strategies
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用其他探索策略
- en: Exploration is a vital component of reinforcement learning. In the NFQ algorithm,
    we use an epsilon-greedy exploration strategy, which consists of acting randomly
    with epsilon probability. We sample a number from a uniform distribution (0, 1).
    If the number is less than the hyperparameter constant, called epsilon, your agent
    selects an action uniformly at random (that’s including the greedy action); otherwise,
    it acts greedily.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 探索是强化学习的一个关键组成部分。在NFQ算法中，我们使用epsilon-greedy探索策略，该策略包括以epsilon概率随机行动。我们从(0, 1)的均匀分布中抽取一个数字。如果这个数字小于超参数常量，称为epsilon，你的智能体将以均匀随机的方式选择一个动作（包括贪婪动作）；否则，它将贪婪地行动。
- en: For the DQN experiments, I added to chapter 9’s Notebook some of the other exploration
    strategies introduced in chapter 4\. I adapted them to use them with neural networks,
    and they are reintroduced next. Make sure to check out all Notebooks and play
    around.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DQN实验，我在第9章的笔记本中添加了一些在第4章中介绍的其他探索策略。我将它们调整为与神经网络一起使用，并在下面重新介绍。请确保查看所有笔记本并进行实验。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonLinearly decaying epsilon-greedy
    exploration strategy |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python线性衰减的epsilon-greedy探索策略 |'
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① In a linearly decaying epsilon-greedy strategy, we start with a high epsilon
    value and decay its value in a linear fashion.② We clip epsilon to be between
    the initial and the minimum value.③ This is a variable holding the number of times
    epsilon has been updated.④ In the select_action method, we use a model and a state.⑤
    For logging purposes, I always extract the q_values.⑥ We draw the random number
    from a uniform distribution and compare it to epsilon.⑦ If higher, we use the
    argmax of the q_values; otherwise, a random action.⑧ Finally, we update epsilon,
    set a variable for logging purposes, and return the action selected. |
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在线性衰减的epsilon-greedy策略中，我们从一个较高的epsilon值开始，并以线性方式衰减其值。② 我们将epsilon裁剪到初始值和最小值之间。③
    这是一个变量，用于存储epsilon更新的次数。④ 在select_action方法中，我们使用一个模型和一个状态。⑤ 为了记录日志，我总是提取q_values。⑥
    我们从均匀分布中抽取一个随机数，并将其与epsilon进行比较。⑦ 如果更高，我们使用q_values的argmax；否则，执行随机动作。⑧ 最后，我们更新epsilon，设置一个用于记录的变量，并返回所选动作。
    |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonExponentially decaying epsilon-greedy
    exploration strategy |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python指数衰减的epsilon-greedy探索策略 |'
- en: '|  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① In the exponentially decaying strategy, the only difference is that now epsilon
    is decaying in an exponential curve.② This is yet another way to exponentially
    decay epsilon, this one uses the exponential function. The epsilon values will
    be much the same, but the decay rate will have to be a different scale.③ This
    select_action function is identical to the previous strategy. One thing I want
    to highlight is that I’m querying the q_values every time only because I’m collecting
    information to show to you. But if you care about performance, this is a bad idea.
    A faster implementation would only query the network after determining that a
    greedy action is being called for.④ exploratory_action here is a variable used
    to calculate the percentage of exploratory actions taken per episode. Only used
    for logging information. |
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在指数衰减策略中，唯一的不同是现在 ε 正在以指数曲线衰减。② 这又是另一种指数衰减 ε 的方法，这种方法使用指数函数。ε 值将非常相似，但衰减率将需要不同的尺度。③
    这个 select_action 函数与之前的策略完全相同。我想强调的是，我之所以每次都查询 q_values，只是因为我正在收集信息向您展示。但如果你关心性能，这并不是一个好主意。一个更快的实现只会在网络确定需要贪婪动作时才查询网络。④
    exploratory_action 是一个变量，用于计算每集探索动作的百分比。仅用于记录信息。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSoftmax exploration strategy
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说 PythonSoftmax 探索策略 |'
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '① In the softmax strategy, we use a temperature parameter, which, the closer
    the value is to 0, the more pronounced the differences in the values will become,
    making action selection more greedy. The temperature is decayed linearly.② Here,
    after decaying the temperature linearly, we clip its value to make sure it’s in
    an acceptable range.③ Notice that in the softmax strategy we really have no chance
    of avoiding extracting the q_values from the model. After all, actions depend
    directly on the values.④ After extracting the values, we want to accentuate their
    differences (unless temp equals 1).⑤ We normalize them to avoid an overflow in
    the exp operation.⑥ Calculate the exponential.⑦ Convert to probabilities.⑧ Finally,
    we use the probabilities to select an action. Notice how we pass the probs variable
    to the p function argument.⑨ And as before: Was the action the greedy or exploratory?
    |'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在 softmax 策略中，我们使用一个温度参数，越接近 0，值的差异将越明显，从而使动作选择更加贪婪。温度线性衰减。② 在这里，在温度线性衰减后，我们将其值剪辑到可接受的范围内。③
    注意，在 softmax 策略中，我们实际上没有避免从模型中提取 q_values 的机会。毕竟，动作直接取决于值。④ 提取值后，我们希望强调它们的差异（除非
    temp 等于 1）。⑤ 我们将它们归一化以避免 exp 操作溢出。⑥ 计算指数。⑦ 转换为概率。⑧ 最后，我们使用概率来选择动作。注意我们如何将 probs
    变量传递给 p 函数参数。⑨ 如前所述：动作是贪婪的还是探索性的？ |
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsExploration strategies
    have an impactful effect on performance |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ![细节图标](../Images/icons_Details.png) | 它在于细节探索策略对性能有显著影响 |'
- en: '|  | ![](../Images/09_03_Sidebar13.png) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | ![09_03_Sidebar13.png](../Images/09_03_Sidebar13.png) |'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsThe full deep Q-network
    (DQN) algorithm |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![细节图标](../Images/icons_Details.png) | 它在于细节完整的深度 Q 网络（DQN）算法 |'
- en: '|  | Our DQN implementation has components and settings similar to our NFQ:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 我们的 DQN 实现具有与我们的 NFQ 相似的功能和设置：'
- en: Approximate the action-value function *Q*(*s,a; θ*).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似动作值函数 *Q*(*s,a; θ*)。
- en: 'Use a state-in-values-out architecture (nodes: 4, 512,128, 2).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用状态-值输出架构（节点：4, 512, 128, 2）。
- en: Optimize the action-value function to approximate the optimal action- value
    function ***(*s,a*).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化动作值函数以逼近最优动作值函数 ***(*s,a*)。
- en: Use off-policy *TD* targets (*r + gamma*max_a’Q*(*s’,a’; θ*)) to evaluate policies.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用离策略 *TD* 目标 (*r + gamma*max_a’Q*(*s’，a’; θ*)) 来评估策略。
- en: Use mean squared error (MSE) for our loss function.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）作为我们的损失函数。
- en: Use RMSprop as our optimizer with a learning rate of 0.0005.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用学习率为 0.0005 的 RMSprop 作为我们的优化器。
- en: Some of the differences are that in the DQN implementation we now
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些不同之处在于，在 DQN 实现中我们现在
- en: Use an exponentially decaying epsilon-greedy strategy to improve policies, decaying
    from 1.0 to 0.3 in roughly 20,000 steps.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用指数衰减的 ε-greedy 策略来改进策略，大约在 20,000 步内从 1.0 衰减到 0.3。
- en: Use a replay buffer with 320 samples min, 50,000 max, and mini-batches of 64.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有 320 个最小样本、50,000 个最大样本和 64 个小批次的回放缓冲区。
- en: Use a target network that updates every 15 steps.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每 15 步更新一次的目标网络。
- en: 'DQN has three main steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 有三个主要步骤：
- en: 'Collect experience: (*S*[t] *, A*[t] *, R*[*t*+1]*, S*[*t*+1]*, D*[*t*+1]),
    and insert it into the replay buffer.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集经验：(*S*[t] *, A*[t] *, R*[*t*+1]*, S*[*t*+1]*, D*[*t*+1]), 并将其插入回放缓冲区。
- en: 'Randomly sample a mini-batch from the buffer, and calculate the off-policy
    *TD* targets for the whole batch: *r + gamma*max_a’Q*(*s’,a’; θ*).'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从缓冲区中随机抽取一个迷你批次，并计算整个批次的离线TD目标：*r + gamma*max_a’Q*(*s’，a’; θ*)。
- en: Fit the action-value function *Q*(*s,a; θ*) using MSE and RMSprop.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）和RMSprop对动作值函数 *Q*(*s,a; θ*) 进行拟合。
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 0001 | A Bit Of HistoryIntroduction of the DQN algorithm |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍DQN算法'
- en: '|  | DQN was introduced in 2013 by Volodymyr “Vlad” Mnih in a paper called
    *“Playing Atari with Deep Reinforcement Learning.”* This paper introduced DQN
    with experience replay. In 2015, another paper came out, “Human-level control
    through deep reinforcement learning.” This second paper introduced DQN with the
    addition of target networks; it’s the full DQN version you just learned about.Vlad
    got his PhD under Geoffrey Hinton (one of the fathers of deep learning), and works
    as a research scientist at Google DeepMind. He’s been recognized for his DQN contributions,
    and has been included in the 2017 MIT Technology Review 35 Innovators under 35
    list. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | DQN是由Volodymyr “Vlad” Mnih在2013年发表的一篇名为“Playing Atari with Deep Reinforcement
    Learning.”的论文中引入的。这篇论文介绍了带有经验回放的DQN。2015年，又有一篇论文发表，名为“Human-level control through
    deep reinforcement learning.”这篇第二篇论文介绍了带有目标网络的DQN；这就是您刚刚学到的完整DQN版本。Vlad在Geoffrey
    Hinton（深度学习之父之一）的指导下获得了博士学位，并在Google DeepMind担任研究科学家。他因DQN的贡献而受到认可，并被列入2017年麻省理工学院技术评论35岁以下35位创新者名单。
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpDQN passes the cart-pole environment
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 总结DQN通过了小车-杆环境'
- en: '|  | The most remarkable part of the results is that NFQ needs far more samples
    than DQN to solve the environment; DQN is more sample efficient. However, they
    take about the same time, both training (compute) and wall-clock time.![](../Images/09_03_Sidebar16.png)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 结果中最引人注目的是，NFQ需要比DQN更多的样本来解决环境；DQN更有效率。然而，它们花费的时间大约相同，包括训练（计算）和实际时间！[](../Images/09_03_Sidebar16.png)
    |'
- en: 'Double DQN: Mitigating the overestimation of action-value functions'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双重DQN：缓解动作值函数的过度估计
- en: In this section, we introduce one of the main improvements to DQN that have
    been proposed throughout the years, called *double deep Q-networks* (double DQN,
    or DDQN). This improvement consists of adding double learning to our DQN agent.
    It’s straightforward to implement, and it yields agents with consistently better
    performance than DQN. The changes required are similar to the changes applied
    to Q-learning to develop double Q-learning; however, there are several differences
    that we need to discuss.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍DQN在多年间提出的主要改进之一，称为**双深度Q网络**（double DQN，或DDQN）。这种改进包括向我们的DQN代理添加双重学习。它易于实现，并且产生的代理性能比DQN更稳定。所需的变化与用于开发双重Q学习的Q学习应用的变化类似；然而，有几个差异我们需要讨论。
- en: The problem of overestimation, take two
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过度估计的问题，有两个
- en: 'As you can probably remember from chapter 6, Q-learning tends to overestimate
    action-value functions. Our DQN agent is no different; we’re using the same off-policy
    *TD* target, after all, with that max operator. The crux of the problem is simple:
    We’re taking the max of estimated values. Estimated values are often off-center,
    some higher than the true values, some lower, but the bottom line is that they’re
    off. The problem is that we’re always taking the max of these values, so we have
    a preference for higher values, even if they aren’t correct. Our algorithms show
    a positive bias, and performance suffers.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能从第6章中记得，Q学习往往会高估动作值函数。我们的DQN代理也不例外；毕竟，我们使用的是相同的离线策略TD目标，带有那个最大操作符。问题的核心很简单：我们在估计值中取最大值。估计值通常偏离中心，有些高于真实值，有些低于真实值，但关键是它们是偏离的。问题是我们总是取这些值的最大值，所以我们偏好更高的值，即使它们并不正确。我们的算法显示出正偏差，性能受到影响。
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyThe issue with overoptimistic
    agents, and people |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Miguel.png) | 米格尔的类比关于过度乐观的代理和人们的问题'
- en: '|  | I used to like super-positive people until I learned about double DQN.
    No, seriously, imagine you meet a very optimistic person; let’s call her DQN.
    DQN is extremely optimistic. She’s experienced many things in life, from the toughest
    defeat to the highest success. The problem with DQN, though, is she expects the
    sweetest possible outcome from every single thing she does, regardless of what
    she actually does. Is that a problem?One day, DQN went to a local casino. It was
    the first time, but lucky DQN got the jackpot at the slot machines. Optimistic
    as she is, DQN immediately adjusted her value function. She thought, “Going to
    the casino is quite rewarding (the value of *Q*(*s, a*) should be high) because
    at the casino you can go to the slot machines (next state *s’*) and by playing
    the slot machines, you get the jackpot [*Q*(*s’, a’*)]”.But, there are multiple
    issues with this thinking. To begin with, DQN doesn''t play the slot machines
    every time she goes to the casino. She likes to try new things too (she explores),
    and sometimes she tries the roulette, poker, or blackjack (tries a different action).
    Sometimes the slot machine area is under maintenance and not accessible (the environment
    transitions her somewhere else). Additionally, most of the time when DQN plays
    the slot machines, she doesn’t get the jackpot (the environment is stochastic).
    After all, slot machines are called bandits for a reason, not those bandits, the
    other—never mind. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 我曾经喜欢超级积极的人，直到我了解到双DQN。不，说真的，想象一下你遇到一个非常乐观的人；让我们称她为DQN。DQN非常乐观。她经历了生活中许多事情，从最艰难的失败到最高的成功。然而，DQN的问题是她期望她所做的一切都能得到最甜美的结果，无论她实际上做了什么。这是问题吗？有一天，DQN去了当地的一家赌场。这是第一次，但幸运的DQN在老虎机上赢得了大奖。尽管她如此乐观，DQN立即调整了她的价值函数。她心想，“去赌场是非常有回报的（*Q*(*s,
    a*)的值应该很高），因为在赌场你可以去老虎机（下一个状态 *s’*），通过玩老虎机，你可以得到大奖 [*Q*(*s’， a’*)]”。但是，这种想法存在多个问题。首先，DQN并不每次去赌场都玩老虎机。她也喜欢尝试新事物（她进行探索），有时她尝试轮盘赌、扑克或21点（尝试不同的行动）。有时老虎机区域正在维护，无法访问（环境将她转移到其他地方）。此外，大多数时候当DQN玩老虎机时，她并没有赢得大奖（环境是随机的）。毕竟，老虎机被称为强盗，不是那些强盗，而是其他——不必在意。|'
- en: Separating action selection from action evaluation
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分离动作选择和动作评估
- en: One way to better understand positive bias and how we can address it when using
    function approximation is by unwrapping the *max* operator in the target calculations.
    The *max* of a Q-function is the same as the Q-function of the *argmax* action.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解正偏差以及在使用函数近似时如何解决它，一种方法是在目标计算中展开 *max* 操作符。Q函数的 *max* 与 *argmax* 行动的Q函数相同。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryWhat’s an argmax, again?
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Memory.png) | 刷新我的记忆再次什么是argmax？'
- en: '|  | The argmax function is defined as the arguments of the maxima. The argmax
    action-value function, argmax Q-function, *argmax*[*a*]*Q*(*s, a*) is the index
    of the action with the maximum value at the given state s.For example, if you
    have a *Q*(*s*) with values [–1, 0 , –4, –9] for actions 0*–*3, the *max*[*a*]*Q*(*s,
    a*) is 1 which is the index of the maximum value. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | argmax函数定义为最大值的参数。argmax动作值函数，argmax Q函数，*argmax*[*a*]*Q*(*s, a*)是在给定状态s下具有最大值的动作的索引。例如，如果你有一个*Q*(*s*)，对于动作0*–*3，其值分别为[–1,
    0, –4, –9]，则*max*[*a*]*Q*(*s, a*)是1，这是最大值的索引。|'
- en: Let’s unpack the previous sentence with the max and argmax. Notice that we made
    pretty much the same changes when we went from Q-learning to double Q-learning,
    but given that we’re using function approximation, we need to be cautious. At
    first, this unwrapping might seem like a silly step, but it helps me understand
    how to mitigate this problem.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用max和argmax来解开前面的句子。注意，当我们从Q学习到双Q学习时，我们几乎做了同样的改变，但鉴于我们使用函数近似，我们需要谨慎。起初，这种展开可能看起来像是一个愚蠢的步骤，但它帮助我理解如何减轻这个问题。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathUnwrapping the argmax |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式展开argmax'
- en: '|  | ![](../Images/09_03_Sidebar19.png) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/09_03_Sidebar19.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonUnwrapping the max in DQN
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python在DQN中展开max'
- en: '|  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① This is the original DQN way of calculating targets.② It’s important that
    we detach the target so that we do not backpropagate through it.③ We pull the
    Q-values of the next state and get their max.④ Set the value of terminal states
    to 0, and calculate the targets.⑤ This is an equivalent way of calculating targets,
    “unwrapping the max.”⑥ First, get the argmax action of the next state.⑦ Then,
    get the Q-values of the next state as before.⑧ Now, we use the indices to get
    the max values of the next states.⑨ And proceed as before. |
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这就是原始的 DQN 计算目标值的方法。② 重要的是我们要断开目标，这样我们就不通过它进行反向传播。③ 我们获取下一个状态的 Q 值并取其最大值。④
    将终端状态的值设为 0，并计算目标值。⑤ 这是一种等效的计算目标值的方法，“解开最大值。”⑥ 首先，获取下一个状态的 argmax 动作。⑦ 然后，像之前一样获取下一个状态的
    Q 值。⑧ 现在，我们使用索引来获取下一个状态的最大值。⑨ 然后继续之前的步骤。 |
- en: All we’re saying here is that taking the *max* is like asking the network, *“What’s
    the value of the highest-valued action in state?”*
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所说的就是取 *max* 就像问网络，“在状态中价值最高的动作的值是多少？”
- en: But, we are really asking two questions with a single question. First, we do
    an *argmax*, which is equivalent to asking, “Which action is the highest-valued
    action in state *s*?”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但，我们实际上用一个问题问了两个问题。首先，我们进行了一个 *argmax*，这相当于问，“在状态 *s* 中哪个动作是价值最高的动作？”
- en: And then, we use that action to get its value, equivalent to asking, “What’s
    the value of this action (which happens to be the highest-valued action) in state
    *s*?”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用该动作来获取其值，相当于问，“在状态 *s* 中这个动作（碰巧是价值最高的动作）的值是多少？”
- en: One of the problems is that we are asking both questions to the same Q-function,
    which shows bias in the same direction in both answers. In other words, the function
    approximator will answer, “I think this one is the highest-valued action in state
    *s*, and this is its value.”
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我们向同一个 Q 函数问了两个问题，这导致两个答案都显示出相同方向的偏差。换句话说，函数近似器会回答，“我认为在状态 *s* 中这个动作是价值最高的，这是它的值。”
- en: A solution
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个解决方案
- en: A way to reduce the chance of positive bias is to have two instances of the
    action-value function, the way we did in chapter 6.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 减少正偏差的一种方法是在第 6 章中我们做的那样，有两个动作值函数的实例。
- en: If you had another source of the estimates, you could ask one of the questions
    to one and the other question to the other. It’s somewhat like taking votes, or
    like an “I cut, you choose first” procedure, or like getting a second doctor’s
    opinion on health matters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还有其他来源的估计值，你可以向一个提问一个问题，向另一个提问另一个问题。这有点像投票，或者像“我切，你先选”的程序，或者像在健康问题上寻求第二位医生的意见。
- en: In double learning, one estimator selects the index of what it believes to be
    the highest-valued action, and the other estimator gives the value of this action.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在双重学习中，一个估计器选择它认为价值最高的动作的索引，而另一个估计器给出这个动作的值。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryDouble learning procedure
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ![刷新我的记忆](../Images/icons_Memory.png) | 双重学习过程 |'
- en: '|  | We did this procedure with tabular reinforcement learning in chapter 6
    under the double Q-learning agent. It goes like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 我们在第 6 章中介绍了使用表格强化学习进行此过程，在双重 Q-learning 代理下进行。过程是这样的：'
- en: You create two action-value functions, *Q*[A] and *Q*[B].
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你创建两个动作值函数，*Q*[A] 和 *Q*[B]。
- en: You flip a coin to decide which action-value function to update. For example,
    *Q*[A] on heads, *Q*[B] on tails.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你掷硬币来决定更新哪个动作值函数。例如，正面更新 *Q*[A]，反面更新 *Q*[B]。
- en: 'If you got a heads and thus get to update *Q*[A]: You select the action index
    to evaluate from *Q*[B], and evaluate it using the estimate *Q*[A] predicts. Then,
    you proceed to update *Q*[A] as usual, and leave *Q*[B] alone.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你掷出正面并因此更新 *Q*[A]：你从 *Q*[B] 中选择要评估的动作索引，并使用 *Q*[A] 预测的估计值来评估它。然后，你像往常一样更新
    *Q*[A]，而 *Q*[B] 保持不变。
- en: 'If you got a tails and thus get to update *Q*[B], you do it the other way around:
    get the index from *Q*[A], and get the value estimate from *Q*[B]. *Q*[B] gets
    updated, and *Q*[A] is left alone.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你掷出反面并因此更新 *Q*[B]，你反过来操作：从 *Q*[A] 获取索引，从 *Q*[B] 获取值估计。*Q*[B] 被更新，而 *Q*[A]
    保持不变。
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'However, implementing this double-learning procedure exactly as described when
    using function approximation (for DQN) creates unnecessary overhead. If we did
    so, we’d end up with four networks: two networks for training (*Q*[A], *Q*[B])
    and two target networks, one for each online network.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当使用函数近似（对于 DQN）时，如果完全按照描述实现这种双重学习过程，将会产生不必要的开销。如果我们这样做，最终会有四个网络：两个用于训练的网络（*Q*[A]，*Q*[B]）和两个目标网络，每个在线网络一个。
- en: Additionally, it creates a slowdown in the training process, since we’d be training
    only one of these networks at a time. Therefore, only one network would improve
    per step. This is certainly a waste.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还会减慢训练过程，因为我们一次只会训练这些网络中的一个。因此，每一步只会提高一个网络。这无疑是浪费。
- en: Doing this double-learning procedure with function approximators may still be
    better than not doing it at all, despite the extra overhead. Fortunately for us,
    there’s a simple modification to the original double-learning procedure that adapts
    it to DQN and gives us substantial improvements without the extra overhead.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有额外的开销，但使用函数逼近器进行双重学习程序可能仍然比完全不进行更好。幸运的是，我们对原始的双重学习程序进行了简单的修改，使其适应DQN，并在没有额外开销的情况下提供了实质性的改进。
- en: A more practical solution
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个更实际的解决方案
- en: Instead of adding this overhead that’s a detriment to training speed, we can
    perform double learning with the other network we already have, which is the target
    network. However, instead of training both the online and target networks, we
    continue training only the online network, but use the target network to help
    us, in a sense, cross-validate the estimates.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是增加这种损害训练速度的开销，我们可以使用我们已有的另一个网络进行双重学习，即目标网络。然而，我们不是同时训练在线和目标网络，而是继续只训练在线网络，但使用目标网络帮助我们，从某种意义上说，交叉验证估计。
- en: We want to be cautious as to which network to use for action selection and which
    network to use for action evaluation. Initially, we added the target network to
    stabilize training by avoiding chasing a moving target. To continue on this path,
    we want to make sure we use the network we’re training, the online network, for
    answering the first question. In other words, we use the online network to find
    the index of the best action. Then, we use the target network to ask the second
    question, that is, to evaluate the previously selected action.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要谨慎选择用于动作选择和动作评估的网络。最初，我们添加目标网络以通过避免追逐移动目标来稳定训练。为了继续这一路径，我们想要确保我们使用正在训练的网络，即在线网络，来回答第一个问题。换句话说，我们使用在线网络来找到最佳动作的索引。然后，我们使用目标网络来回答第二个问题，即评估之前选定的动作。
- en: This is the ordering that works best in practice, and it makes sense why it
    works. By using the target network for value estimates, we make sure the target
    values are frozen as needed for stability. If we were to implement it the other
    way around, the values would come from the online network, which is getting updated
    at every time step, and therefore changing continuously.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在实践中效果最好的排序方式，它之所以有效是有道理的。通过使用目标网络进行价值估计，我们确保目标值在需要时被冻结以保证稳定性。如果我们反过来实施，值将来自在线网络，该网络在每一步都会更新，因此会持续变化。
- en: '![](../Images/09_04.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_04.png)'
- en: Selecting action, evaluating action
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作，评估动作
- en: '| 0001 | A Bit Of HistoryIntroduction of the double DQN algorithm |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍双重DQN算法 |'
- en: '|  | Double DQN was introduced in 2015 by Hado van Hasselt, shortly after the
    release of the 2015 version of DQN. (The 2015 version of DQN is sometimes referred
    to as Nature DQN—because it was published in the Nature scientific journal, and
    sometimes as Vanilla DQN—because it is the first of many other improvements over
    the years.)In 2010, Hado also authored the double Q-learning algorithm (double
    learning for the tabular case), as an improvement to the Q-learning algorithm.
    This is the algorithm you learned about and implemented in chapter 6.Double DQN,
    also referred to as DDQN, was the first of many improvements proposed over the
    years for DQN. Back in 2015 when it was first introduced, DDQN obtained state-of-the-art
    (best at the moment) results in the Atari domain.Hado obtained his PhD from the
    University of Utrecht in the Netherlands in artificial intelligence (reinforcement
    learning). After a couple of years as a postdoctoral researcher, he got a job
    at Google DeepMind as a research scientist. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | 双重DQN是在2015年由Hado van Hasselt提出的，紧随2015版DQN的发布之后。（2015版的DQN有时被称为Nature
    DQN——因为它发表在Nature科学期刊上，有时也称为Vanilla DQN——因为它是在多年来的许多改进中的第一个。）在2010年，Hado还撰写了双重Q学习算法（表格情况下的双重学习），作为Q学习算法的改进。这是你在第6章中学到并实现的内容。双重DQN，也称为DDQN，是多年来为DQN提出的许多改进中的第一个。在2015年首次提出时，DDQN在Atari领域获得了当时最先进的（最好的）结果。Hado在荷兰乌得勒支大学获得了人工智能（强化学习）博士学位。在作为博士后研究员的几年后，他在谷歌DeepMind担任研究科学家。'
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDDQN gradient update |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 给我看数学DDQN梯度更新 |'
- en: '|  | ![](../Images/09_04_Sidebar23.png) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏图片](../Images/09_04_Sidebar23.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonDouble DQN |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonDouble DQN |'
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① In Double DQN, we use the online network to get the index of the highest-valued
    action of the next state, the argmax. Note we don’t detach the argmax because
    they’re not differentiable. The max(1)[1] returned the index of the max, which
    is already “detached.”② Then, extract the Q-values of the next state according
    to the target network.③ We then index the Q-values provided by the target network
    with the action indices provided by the online network.④ Then set up the targets
    as usual.⑤ Get the current estimates. Note this is where the gradients are flowing
    through.⑥ Calculate the loss, and step the optimizer.⑦ Here we keep using the
    online network for action selection.⑧ Updating the target network is still the
    same as before. |
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在Double DQN中，我们使用在线网络获取下一个状态中最高价值动作的索引，即argmax。注意我们并没有分离argmax，因为它们不可微分。max(1)[1]返回了最大值的索引，这已经是“分离”的。②
    然后，根据目标网络提取下一个状态的Q值。③ 我们然后使用在线网络提供的动作索引来索引目标网络提供的Q值。④ 然后像往常一样设置目标。⑤ 获取当前估计。注意这是梯度流动的地方。⑥
    计算损失，并更新优化器。⑦ 这里我们继续使用在线网络进行动作选择。⑧ 更新目标网络的方式仍然和以前一样。|
- en: A more forgiving loss function
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更宽容的损失函数
- en: In the previous chapter, we selected the L2 loss, also known as *mean square
    error* (MSE), as our loss function, mostly for its widespread use and simplicity.
    And, in reality, in a problem such as the cart-pole environment, there might not
    be a good reason to look any further. However, because I’m teaching you the ins
    and outs of the algorithms and not only “how to hammer the nail,” I’d also like
    to make you aware of the different knobs available so you can play around when
    tackling more challenging problems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们选择了L2损失，也称为*均方误差*（MSE），作为我们的损失函数，主要是因为其广泛的应用和简单性。实际上，在像购物车-杆环境这样的问题中，可能没有很好的理由再进一步探索。然而，因为我不仅教你“如何锤钉”，还教你算法的细节，所以我希望让你了解可用的不同旋钮，这样你就可以在解决更具挑战性的问题时进行尝试。
- en: '![](../Images/09_05.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_05.png)'
- en: MSE is a ubiquitous loss function because it’s simple, it makes sense, and it
    works well. But, one of the issues with using MSE for reinforcement learning is
    that it penalizes large errors more than small errors. This makes sense when doing
    supervised learning because our targets are the true value from the get-go, and
    are fixed throughout the training process. That means we’re confident that, if
    the model is very wrong, then it should be penalized more heavily than if it’s
    just wrong.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: MSE是一个无处不在的损失函数，因为它简单、合理且效果良好。但是，使用MSE进行强化学习的一个问题是，它对大错误的惩罚比对小错误的惩罚更重。在执行监督学习时这是有意义的，因为我们的目标是从一开始就是真实值，并且在整个训练过程中保持固定。这意味着我们确信，如果模型非常错误，那么它应该比只是错误时受到更重的惩罚。
- en: But as stated now several times, in reinforcement learning, we don’t have these
    true values, and the values we use to train our network are dependent on the agent
    itself. That’s a mind shift. Besides, targets are constantly changing; even when
    using target networks, they still change often. In reinforcement learning, being
    very wrong is something we expect and welcome. At the end of the day, if you think
    about it, we aren’t “training” agents; our agents learn on their own. Think about
    that for a second.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如已经多次提到的，在强化学习中，我们没有这些真实值，我们用来训练网络的值依赖于智能体本身。这是一个思维转变。此外，目标不断变化；即使在使用目标网络时，它们也经常变化。在强化学习中，我们期望并欢迎犯错误。最终，如果你仔细想想，我们并不是“训练”智能体；我们的智能体是自行学习的。想想看。
- en: A loss function not as unforgiving, and also more robust to outliers, is the
    *mean absolute error*, also known as MAE or L1 loss. MAE is defined as the average
    absolute difference between the predicted and true values, that is, the predicted
    action-value function and the *TD* target. Given that MAE is a linear function
    as opposed to quadratic such as MSE, we can expect MAE to be more successful at
    treating large errors the same way as small errors. This can come in handy in
    our case because we expect our action-value function to give wrong values at some
    point during training, particularly at the beginning. Being more resilient to
    outliers often implies errors have less effect, as compared to MSE, in terms of
    changes to our network, which means more stable learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不那么严厉的损失函数，并且对异常值更鲁棒的函数是**平均绝对误差**，也称为MAE或L1损失。MAE被定义为预测值和真实值之间平均绝对差，即预测的动作值函数和**TD**目标。鉴于MAE是一个线性函数，而不是像MSE那样的二次函数，我们可以预期MAE在处理大误差和小误差方面会更为成功。这在我们的情况下很有用，因为我们预计动作值函数在训练过程中某些时刻会给出错误值，尤其是在开始时。对异常值有更强的抵抗力通常意味着误差对我们的网络变化的影响较小，与MSE相比，这意味着更稳定的学习。
- en: '![](../Images/09_06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_06.png)'
- en: 'Now, on the flip side, one of the helpful things of MSE that MAE doesn’t have
    is the fact that its gradients decrease as the loss goes to zero. This feature
    is helpful for optimization methods because it makes it easier to reach the optima:
    lower gradients mean small changes to the network. But luckily for us, there’s
    a loss function that’s somewhat a mix of MSE and MAE, called the Huber loss.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从另一方面来看，MSE的一个有助于MAE的特性是，随着损失趋近于零，其梯度会减小。这个特性对优化方法很有帮助，因为它使得达到最优值变得更容易：较小的梯度意味着网络的小幅变化。但幸运的是，有一个损失函数是MSE和MAE的某种混合体，称为Huber损失。
- en: The *Huber loss* has the same useful property as MSE of quadratically penalizing
    the errors near zero, but it isn’t quadratic all the way out for huge errors.
    Instead, the Huber loss is quadratic (curved) near-zero error, and it becomes
    linear (straight) for errors larger than a preset threshold. Having the best of
    both worlds makes the Huber loss robust to outliers, just like MAE, and differentiable
    at 0, just like MSE.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Huber损失**具有与MSE相同的有用特性，即对接近零的误差进行二次惩罚，但它并不是对所有巨大误差都进行二次惩罚。相反，Huber损失在接近零的误差处是二次（曲线）的，而对于大于预设阈值的误差，它变为线性（直线）。两者兼而有之使得Huber损失对异常值具有鲁棒性，就像MAE一样，并且在0处可微分，就像MSE一样。'
- en: '![](../Images/09_07.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_07.png)'
- en: The Huber loss uses a hyperparameter, *δ,* to set this threshold in which the
    loss goes from quadratic to linear, basically, from MSE to MAE. If *δ* is zero,
    you’re left precisely with MAE, and if *δ* is infinite, then you’re left precisely
    with MSE. A typical value for *δ* is 1, but be aware that your loss function,
    optimization, and learning rate interact in complex ways. If you change one, you
    may need to tune several of the others. Check out the Notebook for this chapter
    so you can play around.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Huber损失使用一个超参数**δ**来设置损失从二次变为线性的阈值，基本上是从MSE变为MAE。如果**δ**为零，你将得到精确的MAE，如果**δ**是无穷大，那么你将得到精确的MSE。**δ**的一个典型值是1，但请注意，你的损失函数、优化和学习率以复杂的方式相互作用。如果你改变其中一个，你可能需要调整其他几个。查看本章的笔记本，这样你可以进行一些实验。
- en: '![](../Images/09_08.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_08.png)'
- en: Interestingly, there are at least two different ways of implementing the Huber
    loss function. You could either compute the Huber loss as defined, or compute
    the MSE loss instead, and then set all gradients larger than a threshold to a
    fixed magnitude value. You clip the magnitude of the gradients. The former depends
    on the deep learning framework you use, but the problem is that some frameworks
    don’t give you access to the *δ* hyperparameter, so you’re stuck with *δ* set
    to 1, which doesn’t always work, and isn’t always the best. The latter, often
    referred to as *loss clipping,* or better yet *gradient clipping,* is more flexible
    and, therefore, what I implement in the Notebook.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，至少有两种不同的实现Huber损失函数的方法。你可以按照定义计算Huber损失，或者计算MSE损失，然后将所有大于阈值的梯度设置为固定幅度值。你剪裁了梯度的幅度。前者取决于你使用的深度学习框架，但问题是某些框架不提供对**δ**超参数的访问，所以你只能将**δ**设置为1，这并不总是有效，也不是总是最好的。后者通常被称为**损失剪裁**，或者更好的是**梯度剪裁**，它更灵活，因此我在笔记本中实现了这种方法。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonDouble DQN with Huber loss
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python双DQN与Huber损失'
- en: '|  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE7]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① First, you calculate the targets and get the current values as before using
    double learning.② Then, calculate the loss function as Mean Squared Error, as
    before.③ Zero the optimizer and calculate the gradients in a backward step.④ Now,
    clip the gradients to the max_gradient_norm. This value can be virtually any value,
    but know that this interacts with other hyperparameters, such as learning rate.⑤
    Finally, step the optimizer. |
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ① 首先，你计算目标值并像之前一样使用双重学习得到当前值。② 然后，像之前一样计算损失函数，即均方误差。③ 将优化器归零并在反向步骤中计算梯度。④ 现在，将梯度裁剪到max_gradient_norm。这个值可以是任何值，但要知道这个值与其他超参数，如学习率相互作用。⑤
    最后，移动优化器。|
- en: Know that there’s such a thing as *reward clipping*, which is different than
    *gradient clipping*. These are two very different things, so beware. One works
    on the rewards and the other on the errors (the loss). Now, above all don’t confuse
    either of these with *Q-value clipping,* which is undoubtedly a mistake.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 知道有一种叫做*奖励裁剪*的东西，这与*梯度裁剪*不同。这两者是非常不同的，所以要注意。一个作用于奖励，另一个作用于误差（损失）。现在，最重要的是不要将这两者与*Q值裁剪*混淆，这无疑是一个错误。
- en: Remember, the goal in our case is to prevent gradients from becoming too large.
    For this, we either make the loss linear outside a given absolute *TD* error threshold
    or make the gradient constant outside a max gradient magnitude threshold.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在我们的案例中，目标是防止梯度变得过大。为此，我们可以在给定的绝对*TD*误差阈值之外使损失线性化，或者使梯度在最大梯度幅度阈值之外保持恒定。
- en: In the cart-pole environment experiments that you find in the Notebook, I implement
    the Huber loss function by using the gradient clipping technique. That is, I calculate
    MSE and then clip the gradients. However, as I mentioned before, I set the hyperparameter
    setting for the maximum gradient values to infinity. Therefore, it’s effectively
    using good-old MSE. But, please, experiment, play around, explore! The Notebooks
    I created should help you learn almost as much as the book. Set yourself free
    over there.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中你找到的cart-pole环境实验中，我通过梯度裁剪技术实现了Huber损失函数。也就是说，我首先计算均方误差（MSE），然后裁剪梯度。然而，正如我之前提到的，我将最大梯度值的超参数设置为了无穷大。因此，实际上是在使用传统的MSE。但是，请务必进行实验，玩耍，探索！我创建的笔记本应该能帮助你学到与书籍几乎一样多的知识。在那里，让自己自由发挥吧。
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsThe full double deep
    Q-network (DDQN) algorithm |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 这就是细节全双深度Q网络（DDQN）算法'
- en: '|  | DDQN is almost identical to DQN, but there are still several differences:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | DDQN几乎与DQN相同，但仍有几个区别：'
- en: Approximate the action-value function *Q*(*s, a; θ*).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似动作值函数 *Q*(*s, a; θ*)。
- en: 'Use a state-in-values-out architecture (nodes: 4, 512,128, 2).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用状态-值输出架构（节点：4, 512, 128, 2）。
- en: Optimize the action-value function to approximate the optimal action- value
    function *q**(*s,a*).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化动作值函数以近似最优动作值函数 *q**(*s,a*)。
- en: Use off-policy TD targets (*r + gamma*max_a’Q*(*s’,a’; θ*)) to evaluate policies.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用离线TD目标（*r + gamma*max_a’Q*(*s’，a’; θ*)）来评估策略。
- en: Notice that we now
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们现在
- en: Use an adjustable Huber loss, which, since we set the max_gradient_norm variable
    to “float(‘inf’),” we’re effectively using mean squared error (MSE) for our loss
    function.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可调整的Huber损失，由于我们将max_gradient_norm变量设置为“float(‘inf’),”因此我们实际上在损失函数中使用的是均方误差（MSE）。
- en: Use RMSprop as our optimizer with a learning rate of 0.0007\. Note that before
    we used 0.0005 because without double learning (vanilla DQN), several seeds fail
    if we train with a learning rate of 0.0007\. Perhaps stability? In DDQN, on the
    other hand, training with a higher learning rate works best.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用学习率为0.0007的RMSprop作为我们的优化器。请注意，在我们之前使用的是0.0005，因为没有双重学习（vanilla DQN），如果使用学习率为0.0007进行训练，则几个种子会失败。也许是因为稳定性？另一方面，在DDQN中，使用更高的学习率效果最好。
- en: In DDQN we’re still using
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDQN中我们仍然使用
- en: An exponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly
    20,000 steps) to improve policies.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种指数衰减的epsilon-greedy策略（从1.0到0.3大约需要20,000步）来改进策略。
- en: A replay buffer with 320 samples min, 50,000 max, and a batch of 64.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有320个最小样本、50,000个最大样本和64个批次的重放缓冲区。
- en: A target network that freezes for 15 steps and then updates fully.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标网络冻结15步后完全更新。
- en: 'DDQN, similar to DQN, has the same three main steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN与DQN类似，具有相同的三个主要步骤：
- en: 'Collect experience: (*S*[*t*]*, A*[*t*]*, R*[*t*+1]*, S*[*t*+1]*, D*[*t*+1]),
    and insert it into the replay buffer.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集经验：(*S*[*t*]*, A*[*t*]*, R*[*t*+1]*, S*[*t*+1]*, D*[*t*+1])，并将其插入到重放缓冲区中。
- en: 'Randomly sample a mini-batch from the buffer and calculate the off-policy *TD*
    targets for the whole batch: *r + gamma*max_a’Q*(*s’,a’; θ*).'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从缓冲区中随机抽取一个迷你批次，并计算整个批次的离策略*TD*目标：*r + gamma*max_a’Q*(*s’，a’; θ*)。
- en: Fit the action-value function *Q(s, a; θ) using MSE and RMSprop.*
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）和RMSprop来拟合动作值函数*Q(s, a; θ)*。
- en: The bottom line is that the DDQN implementation and hyperparameters are identical
    to those of DQN, except that we now use double learning and therefore train with
    a slightly higher learning rate. The addition of the Huber loss doesn’t change
    anything because we’re “clipping” gradients to a max value of infinite, which
    is equivalent to using MSE. However, for many other environments you’ll find it
    useful, so tune this hyperparameter. |
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，DDQN的实现和超参数与DQN相同，只是我们现在使用双重学习，因此以略高的学习率进行训练。添加Huber损失并没有改变什么，因为我们“剪辑”梯度到无限大的最大值，这相当于使用MSE。然而，对于许多其他环境，你会发现它很有用，所以调整这个超参数。|
- en: '| ![](../Images/icons_Tally.png) | Tally it UpDDQN is more stable than NFQ
    or DQN |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| ![计数图标](../Images/icons_Tally.png) | 累计它DDQN比NFQ或DQN更稳定'
- en: '|  | DQN and DDQN have similar performance in the cart-pole environment. However,
    this is a simple environment with a smooth reward function. In reality, DDQN should
    always give better performance.![](../Images/09_08_Sidebar27.png) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 在小车-杆环境中，DQN和DDQN有相似的性能。然而，这是一个具有平滑奖励函数的简单环境。在现实中，DDQN应该总是给出更好的性能！![侧边栏27](../Images/09_08_Sidebar27.png)
    |'
- en: Things we can still improve on
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们还可以改进的地方
- en: Surely our current value-based deep reinforcement learning method isn’t perfect,
    but it’s pretty solid. DDQN can reach superhuman performance in many of the Atari
    games. To replicate those results, you’d have to change the network to take images
    as input (a stack of four images to be able to infer things such as direction
    and velocity from the images), and, of course, tune the hyperparameters.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的当前基于价值的深度强化学习方法并不完美，但它相当稳固。DDQN在许多Atari游戏中可以达到超人类的表现。为了复制这些结果，你必须将网络改为接受图像作为输入（四个图像的堆叠，以便能够从图像中推断出方向和速度等），当然，还需要调整超参数。
- en: Yet, we can still go a little further. There are at least a couple of other
    improvements to consider that are easy to implement and impact performance in
    a positive way.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还可以更进一步。至少还有其他几个易于实现且对性能有积极影响的改进可以考虑。
- en: The first improvement requires us to reconsider the current network architecture.
    As of right now, we have a naive representation of the Q-function on our neural
    network architecture.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个改进要求我们重新考虑当前的神经网络架构。到目前为止，我们在神经网络架构中对Q函数有一个天真表示。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryCurrent neural network
    architecture |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ![刷新记忆图标](../Images/icons_Memory.png) | 刷新我的记忆当前神经网络架构'
- en: '|  | We’re literately “making reinforcement learning look like supervised learning.”
    But, we can, and should, break free from this constraint, and think out of the
    box.![](../Images/09_08_Sidebar28.png)State-in-values-out architectureIs there
    any better way of representing the Q-function? Think about this for a second while
    you look at the images on the next page. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们实际上是在“让强化学习看起来像监督学习。”但，我们可以，也应该打破这种限制，跳出思维定式！![侧边栏28](../Images/09_08_Sidebar28.png)值-状态输出架构有没有更好的方法来表示Q函数？当你看到下一页的图像时，请思考一下这个问题。|'
- en: The images on the right are bar plots representing the estimated action-value
    function Q, state-value function V, and action-advantage function A for the cart-pole
    environment with a state in which the pole is near vertical.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的图像是条形图，表示小车-杆环境中状态接近垂直时的估计动作值函数Q、状态值函数V和动作优势函数A。
- en: '![](../Images/09_09a.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图9_09a](../Images/09_09a.png)'
- en: Notice the different functions and values and start thinking about how to better
    architect the neural network so that data is used more efficiently. As a hint,
    let me remind you that the Q-values of a state are related through the V-function.
    That is, the action-value function *Q* has an essential relationship with the
    state-value function V, because of both actions in *Q*(*s*) are indexed by the
    same state *s* (in the example to the right *s*=[0.02, –0.01, –0.02, –0.04]).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意不同的函数和值，并开始思考如何更好地设计神经网络架构，以便更有效地使用数据。作为一个提示，让我提醒你，状态Q值通过V函数相关联。也就是说，动作值函数*Q*与状态值函数V有本质的联系，因为*Q*(*s*)中的动作都由相同的状态*s*索引（在右边的例子中*s*=[0.02,
    –0.01, –0.02, –0.04]）。
- en: '![](../Images/09_09b.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图9_09b](../Images/09_09b.png)'
- en: The question is, could you learn anything about *Q*(*s*, 0) if you’re using
    a *Q*(*s*, 1) sample? Look at the plot showing the action-advantage function *A*(*s*)
    and notice how much easier it is for you to eyeball the greedy action with respect
    to these estimates than when using the plot with the action-value function *Q*(*s*).
    What can you do about this? In the next chapter, we look at a network architecture
    called the *dueling network* that helps us exploit these relationships.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，如果你使用的是 *Q*(*s*, 1) 样本，你能否了解关于 *Q*(*s*, 0) 的任何信息？看看展示动作-优势函数 *A*(*s*) 的图表，注意相对于使用动作值函数
    *Q*(*s*) 的图表，你如何更容易地通过直观判断来选择贪婪动作。你能做些什么呢？在下一章中，我们将探讨一种名为 *dueling network* 的网络架构，它有助于我们利用这些关系。
- en: '![](../Images/09_09c.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09_09c.png)'
- en: The other thing to consider improving is the way we sample experiences from
    the replay buffer. As of now, we pull samples from the buffer uniformly at random,
    and I’m sure your intuition questions this approach and suggests we can do better,
    and we can.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑改进的是我们从重放缓冲区中采样经验的方式。到目前为止，我们是从缓冲区中随机均匀地抽取样本，我相信你的直觉会质疑这种方法，并建议我们可以做得更好，我们确实可以。
- en: Humans don’t go around the world remembering random things to learn from at
    random times. There’s a more systematic way in which intelligent agents “replay
    memories.” I’m pretty sure my dog chases rabbits in her sleep. Certain experiences
    are more important than others to our goals. Humans often replay experiences that
    caused them unexpected joy or pain. And it makes sense, and you need to learn
    from these experiences to generate more or less of them. In the next chapter,
    we look at ways of prioritizing the sampling of experiences to get the most out
    of each sample, when we learn about the prioritized experience replay (PER) method.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不会在世界各地随意记住随机的事情以随机的时间学习。智能代理“重放记忆”的方式更为系统。我相当确信我的狗在梦中追逐兔子。某些经历对我们实现目标来说比其他经历更重要。人类经常重放那些给他们带来意外喜悦或痛苦的经历。这是有道理的，你需要从这些经历中学习，以产生更多或更少的这些经历。在下一章中，我们将探讨优先采样经验的方法，以从每个样本中获得最大收益，当我们了解优先经验重放（PER）方法时。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about the widespread issues with value-based deep
    reinforcement learning methods. The fact that online data isn’t stationary, and
    it also isn’t independent and identically distributed as most optimization methods
    expect, creates an enormous amount of problems value-based methods are susceptible
    to.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了基于价值的深度强化学习方法中普遍存在的问题。在线数据非平稳性，以及它也不是大多数优化方法所期望的独立同分布，为基于价值的方法带来了大量问题。
- en: You learned to stabilize value-based deep reinforcement learning methods by
    using a variety of techniques that have empirical results in several benchmarks,
    and you dug deep on these components that make value-based methods more stable.
    Namely, you learned about the advantages of using target networks and replay buffers
    in an algorithm known as DQN (nature DQN, or vanilla DQN). You learned that by
    using target networks, we make the targets appear stationary to the optimizer,
    which is good for stability, albeit by sacrificing convergence speed. You also
    learned that by using replay buffers, the online data looks more IID, which, you
    also learned, is a source of significant issues in value-based bootstrapping methods.
    These two techniques combined make the algorithm sufficiently stable for performing
    well in several deep reinforcement learning tasks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过使用在多个基准测试中具有实证结果的多种技术来稳定基于价值的深度强化学习方法，并深入研究了使基于价值方法更稳定的这些组件。具体来说，你了解了在名为
    DQN 的算法中使用目标网络和重放缓冲区的优势（自然 DQN 或纯 DQN）。你了解到，通过使用目标网络，我们使目标对优化器看起来是平稳的，这对稳定性有利，尽管这牺牲了收敛速度。你还了解到，通过使用重放缓冲区，在线数据看起来更像是独立同分布的，正如你所了解的，这是基于价值的自举方法中重大问题的来源。这两种技术的结合使算法足够稳定，能够在多个深度强化学习任务中表现良好。
- en: However, there are many more potential improvements to value-based methods.
    You implemented a straightforward change that has a significant impact on performance,
    in general. You added a double-learning strategy to the baseline DQN agent that,
    when using function approximation, is known as the DDQN agent, and it mitigates
    the issues of overestimation in off-policy value-based methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他潜在的方法可以改进基于价值的方法。你实施了一个简单改变，这对性能产生了重大影响。你向基线 DQN 代理添加了双重学习策略，当使用函数逼近时，它被称为
    DDQN 代理，这有助于缓解离策略基于价值方法中的高估问题。
- en: In addition to these new algorithms, you learned about different exploration
    strategies to use with value-based methods. You learned about linearly and exponentially
    decaying epsilon-greedy and softmax exploration strategies, this time, in the
    context of function approximation. Also, you learned about different loss functions
    and which ones make more sense for reinforcement learning and why. You learned
    that the Huber loss function allows you to tune between MSE and MAE with a single
    hyperparameter, and it’s one of the preferred loss functions used in value-based
    deep reinforcement learning methods.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些新算法之外，你还学习了与基于价值的策略一起使用的不同探索策略。你学习了线性衰减和指数衰减的ε-greedy以及softmax探索策略，这次是在函数逼近的背景下。你还学习了不同的损失函数，以及哪些损失函数对强化学习更有意义以及为什么。你了解到Huber损失函数允许你通过单个超参数在均方误差（MSE）和绝对误差（MAE）之间进行调整，并且它是基于价值的深度强化学习方法中首选的损失函数之一。
- en: By now, you
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand why using online data for training neural network with optimizers
    that expect stationary and IID data is a problem in value-based DRL methods
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么使用在线数据训练期望平稳和独立同分布数据的神经网络的优化器在基于价值的深度强化学习方法中是一个问题
- en: Can solve reinforcement learning problems with continuous state-spaces with
    algorithms that are more stable and therefore give more consistent results
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用更稳定且因此能给出更一致结果的算法来解决具有连续状态空间的强化学习问题
- en: Have an understanding of state-of-the-art, value-based, deep reinforcement learning
    methods and can solve complex problems
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经理解了最先进的基于价值的深度强化学习方法，并能解决复杂问题
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 独立工作并分享你的发现 |'
- en: '|  | Here are several ideas on how to take what you have learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，可以帮助你将所学知识提升到更高层次。如果你愿意，可以将你的成果分享给全世界，并确保查看其他人所做的工作。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch09_tf01:** In this and the next chapter, we test the algorithms only
    in the cart-pole environment. Find a couple other environments and test the agents
    in those, for instance, the lunar lander environment here: [https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d),
    and the mountain car environment here: [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control).
    Did you have to make any changes to the agents, excluding hyperparameters, to
    make the agents work in these environments? Make sure to find a single set of
    hyperparameters that solve all environments. To clarify, I mean you use a single
    set of hyperparameters, and train an agent from scratch in each environment, not
    a single trained agent that does well in all environments.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch09_tf01:** 在本章和下一章中，我们仅在购物车-杆环境中测试算法。找到其他几个环境并在其中测试智能体，例如，这里提供的月球着陆器环境：[https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d)，以及这里提供的山车环境：[https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control)。你除了超参数外，是否需要对智能体进行任何更改才能使其在这些环境中工作？确保找到一组可以解决所有环境的超参数。为了澄清，我的意思是使用一组超参数，并在每个环境中从头开始训练一个智能体，而不是一个在所有环境中都表现良好的训练好的智能体。'
- en: '**#gdrl_ch09_tf02:** In this and the next chapter, we test the algorithms in
    environments that are continuous, but low dimensional. You know what a high-dimensional
    environment is? Atari environments. Look them up here (the non “ram”): [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).
    Now, modify the networks, replay buffer, and agent code in this chapter so that
    the agent can solve image-based environments. Beware that this isn’t a trivial
    task, and training will take a while, from many hours to several days.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch09_tf02:** 在本章和下一章中，我们在连续但低维度的环境中测试算法。你知道什么是高维环境吗？Atari环境。在这里查找它们（非“ram”）：[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)。现在，修改本章中的网络、重放缓冲区和智能体代码，以便智能体可以解决基于图像的环境。请注意，这不是一个简单任务，训练将需要一段时间，从数小时到数天不等。'
- en: '**#gdrl_ch09_tf03:** I mentioned that value-based methods are sensitive to
    hyperparameters. In reality, there’s something called the “deadly triad,” that
    basically tells us using neural networks with bootstrapping and off-policy is
    bad. Investigate!'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch09_tf03:** 我提到基于价值的算法对超参数很敏感。实际上，存在一个被称为“致命三角”的东西，它基本上告诉我们使用具有引导和离线策略的神经网络是糟糕的。去调查一下吧！'
- en: '**#gdrl_ch09_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch09_tf04:** 在每一章中，我都在使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他你工作过的事情。没有什么比为自己创造作业更令人兴奋的了。确保分享你打算调查的内容以及你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '用你的发现写一条推文，@mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待着你！以下是一条推文示例：“嘿，@mimoralea。我创建了一个包含学习深度强化学习资源的博客文章。查看它吧，链接：<link>
    #gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。 |'
