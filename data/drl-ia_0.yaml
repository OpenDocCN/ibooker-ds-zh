- en: Part 1\. Foundations
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分。基础
- en: '[Part 1](#part01) consists of five chapters that teach the most fundamental
    aspects of deep reinforcement learning. After reading [part 1](#part01), you’ll
    be able to understand the chapters in [part 2](kindle_split_015.html#part02) in
    any order.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一部分](#part01) 由五章组成，教授深度强化学习最基本的内容。阅读完[第一部分](#part01)后，你将能够以任何顺序理解[第二部分](kindle_split_015.html#part02)中的章节。'
- en: '[Chapter 1](kindle_split_010.html#ch01) begins with a high-level introduction
    to deep reinforcement learning, explaining its main concepts and its utility.
    In [chapter 2](kindle_split_011.html#ch02) we’ll start building practical projects
    that illustrate the basic ideas of reinforcement learning. In [chapter 3](kindle_split_012.html#ch03)
    we’ll implement a deep Q-network—the same kind of algorithm that DeepMind famously
    used to play Atari games at superhuman levels.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](kindle_split_010.html#ch01) 从对深度强化学习的高级介绍开始，解释其主要概念和用途。在第2章 [chapter
    2](kindle_split_011.html#ch02) 中，我们将开始构建展示强化学习基本思想的实际项目。在第3章 [chapter 3](kindle_split_012.html#ch03)
    中，我们将实现一个深度Q网络——与DeepMind著名地用于在超级人类水平上玩Atari游戏的算法相同。'
- en: '[Chapters 4](kindle_split_013.html#ch04) and [5](kindle_split_014.html#ch05)
    round out the most common reinforcement learning algorithms, namely policy gradient
    methods and actor-critic methods. We’ll look at the pros and cons of these approaches
    compared to deep Q-networks.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](kindle_split_013.html#ch04) 和 [第5章](kindle_split_014.html#ch05) 总结了最常见的强化学习算法，即策略梯度方法和演员-评论家方法。我们将比较这些方法与深度Q网络的优缺点。'
- en: Chapter 1\. What is reinforcement learning?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章。什么是强化学习？
- en: '*This chapter covers*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: A brief review of machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简述
- en: Introducing reinforcement learning as a subfield
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将强化学习引入作为一个子领域
- en: The basic framework of reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的基本框架
- en: '*Computer languages of the future will be more concerned with goals and less
    with procedures specified by the programmer.*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*未来的计算机语言将更多地关注目标，而不是程序员指定的程序。*'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Marvin Minksy, 1970 ACM Turing Lecture*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*马文·明斯基，1970年ACM图灵奖演讲*'
- en: If you’re reading this book, you are probably familiar with how deep neural
    networks are used for things like image classification or prediction (and if not,
    just keep reading; we also have a crash course in deep learning in the appendix).
    *Deep reinforcement learning* (DRL) is a subfield of machine learning that utilizes
    deep learning models (i.e., neural networks) in reinforcement learning (RL) tasks
    (to be defined in [section 1.2](#ch01lev1sec2)). In image classification we have
    a bunch of images that correspond to a set of discrete categories, such as images
    of different kinds of animals, and we want a machine learning model to interpret
    an image and classify the kind of animal in the image, as in [figure 1.1](#ch01fig01).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本书，你可能熟悉深度神经网络如何用于图像分类或预测（如果不熟悉，请继续阅读；附录中我们也有一个深度学习速成课程）。*深度强化学习*（DRL）是机器学习的一个子领域，它利用深度学习模型（即神经网络）在强化学习（RL）任务中（将在[1.2节](#ch01lev1sec2)中定义）。在图像分类中，我们有一系列与一组离散类别相对应的图像，例如不同种类的动物图像，我们希望机器学习模型解释图像并分类图像中的动物种类，如[图1.1](#ch01fig01)所示。
- en: Figure 1.1\. An image classifier is a function or learning algorithm that takes
    in an image and returns a class label, classifying the image into one of a finite
    number of possible categories or classes.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1。图像分类器是一个函数或学习算法，它接受一个图像并返回一个类别标签，将图像分类为有限数量的可能类别或类别之一。
- en: '![](01fig01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig01.jpg)'
- en: 1.1\. The “deep” in deep reinforcement learning
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 深度强化学习中的“深度”
- en: Deep learning models are just one of many kinds of machine learning models we
    can use to classify images. In general, we just need some sort of function that
    takes in an image and returns a class label (in this case, the label identifying
    which kind of animal is depicted in the image), and usually this function has
    a fixed set of adjustable *parameters*—we call these kinds of models *parametric*
    models. We start with a parametric model whose parameters are initialized to random
    values—this will produce random class labels for the input images. Then we use
    a *training* procedure to adjust the parameters so the function iteratively gets
    better and better at correctly classifying the images. At some point, the parameters
    will be at an optimal set of values, meaning that the model cannot get any better
    at the classification task. Parametric models can also be used for *regression,*
    where we try to fit a model to a set of data so we can make predictions for unseen
    data ([figure 1.2](#ch01fig02)). A more sophisticated approach might perform even
    better if it had more parameters or a better internal architecture.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型只是我们可以用来对图像进行分类的许多种机器学习模型之一。一般来说，我们只需要某种函数，该函数接受一个图像并返回一个类别标签（在这种情况下，标签用于识别图像中描绘的是哪种动物），通常这个函数有一个固定的可调整的*参数*集——我们称这类模型为*参数化*模型。我们从参数初始化为随机值的参数化模型开始——这将为输入图像生成随机的类别标签。然后我们使用*训练*过程来调整参数，使函数在正确分类图像方面不断变得更好。在某个时刻，参数将达到最优的值集，这意味着模型在分类任务上不能再变得更好。参数化模型也可以用于*回归*，其中我们尝试将模型拟合到一组数据，以便对未见数据做出预测（[图1.2](#ch01fig02)）。如果它有更多的参数或更好的内部架构，更复杂的方法可能会表现得更好。
- en: Figure 1.2\. Perhaps the simplest machine learning model is a simple linear
    function of the form *f(x) = mx + b*, with parameters *m* (the slope) and *b*
    (the intercept). Since it has adjustable parameters, we call it a parametric function
    or model. If we have some 2-dimensional data, we can start with a randomly initialized
    set of parameters, such as [m = 3.4, b = 0.3], and then use a training algorithm
    to optimize the parameters to fit the training data, in which case the optimal
    set of parameters is close to [m = 2, b = 1].
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2。也许最简单的机器学习模型是形式为 *f(x) = mx + b* 的简单线性函数，其中参数 *m*（斜率）和 *b*（截距）。由于它有可调整的参数，我们称之为参数化函数或模型。如果我们有一些二维数据，我们可以从一个随机初始化的参数集开始，例如
    [m = 3.4, b = 0.3]，然后使用训练算法来优化参数以拟合训练数据，在这种情况下，最优的参数集接近 [m = 2, b = 1]。
- en: '![](01fig02_alt.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2](01fig02_alt.jpg)'
- en: Deep neural networks are popular because they are in many cases the most accurate
    parametric machine learning models for a given task, like image classification.
    This is largely due to the way they represent data. Deep neural networks have
    many layers (hence the “deep”), which induces the model to learn layered representations
    of input data. This layered representation is a form of *compositionality*, meaning
    that a complex piece of data is represented as the combination of more elementary
    components, and those components can be further broken down into even simpler
    components, and so on, until you get to atomic units.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络之所以受欢迎，是因为在许多情况下，它们是针对特定任务（如图像分类）最准确的参数化机器学习模型。这很大程度上归因于它们表示数据的方式。深度神经网络有许多层（因此称为“深度”），这促使模型学习输入数据的分层表示。这种分层表示是一种*组合性*，意味着复杂的数据块被表示为更基本组件的组合，而这些组件可以进一步分解成更简单的组件，依此类推，直到达到原子单位。
- en: Human language is compositional ([figure 1.3](#ch01fig03)). For example, a book
    is composed of chapters, chapters are composed of paragraphs, paragraphs are composed
    of sentences, and so on, until you get to individual words, which are the smallest
    units of meaning. Yet each individual level conveys meaning—an entire book is
    meant to convey meaning, and its individual paragraphs are meant to convey smaller
    points. Deep neural networks can likewise learn a compositional representation
    of data—for example, they can represent an image as the composition of primitive
    contours and textures, which are composed into elementary shapes, and so on, until
    you get the complete, complex image. This ability to handle complexity with compositional
    representations is largely what makes deep learning so powerful.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语言是组合性的（[图1.3](#ch01fig03)）。例如，一本书由章节组成，章节由段落组成，段落由句子组成，以此类推，直到你得到单个单词，它们是意义的最小单位。然而，每个单独的层次都传达意义——整本书旨在传达意义，其各个段落旨在传达更小的观点。深度神经网络同样可以学习数据的组合表示——例如，它们可以将图像表示为原始轮廓和纹理的组合，这些轮廓和纹理组合成基本形状，等等，直到你得到完整的、复杂的图像。这种用组合表示处理复杂性的能力在很大程度上使得深度学习变得如此强大。
- en: Figure 1.3\. A sentence like “John hit the ball” can be decomposed into simpler
    and simpler parts until we get the individual words. In this case, we can decompose
    the sentence (denoted S) into a subject noun (N) and a verb phrase (VP). The VP
    can be further decomposed into a verb, “hit,” and a noun phrase (NP). The NP can
    then be decomposed into the individual words “the” and “ball.”
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3. 一个像“John hit the ball”这样的句子可以被分解成越来越简单的部分，直到我们得到单个单词。在这种情况下，我们可以将句子（表示为S）分解为主语名词（N）和动词短语（VP）。VP可以进一步分解为动词“hit”和名词短语（NP）。然后，NP可以分解为单个单词“the”和“ball”。
- en: '![](01fig03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3](01fig03.jpg)'
- en: 1.2\. Reinforcement learning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 强化学习
- en: It is important to distinguish between problems and their solutions, or in other
    words, between the tasks we wish to solve and the algorithms we design to solve
    them. Deep learning algorithms can be applied to many problem types and tasks.
    Image classification and prediction tasks are common applications of deep learning
    because automated image processing before deep learning was very limited, given
    the complexity of images. But there are many other kinds of tasks we might wish
    to automate, such as driving a car or balancing a portfolio of stocks and other
    assets. Driving a car includes some amount of image processing, but more importantly
    the algorithm needs to learn how to *act*, not merely to classify or predict.
    These kinds of problems, where decisions must be made or some behavior must be
    enacted, are collectively called *control tasks*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 区分问题和它们的解决方案，或者说，区分我们希望解决的问题和设计来解决这些问题的算法，是很重要的。深度学习算法可以应用于许多问题类型和任务。图像分类和预测任务是深度学习的常见应用，因为深度学习之前的自动化图像处理非常有限，考虑到图像的复杂性。但还有许多其他类型的任务我们可能希望自动化，例如驾驶汽车或平衡股票和其他资产的投资组合。驾驶汽车包括一定量的图像处理，但更重要的是算法需要学习如何*行动*，而不仅仅是分类或预测。这类问题，其中必须做出决策或必须执行某些行为，统称为*控制任务*。
- en: '*Reinforcement learning* is a generic framework for representing and solving
    control tasks, but within this framework we are free to choose which algorithms
    we want to apply to a particular control task ([figure 1.4](#ch01fig04)). Deep
    learning algorithms are a natural choice as they are able to process complex data
    efficiently, and this is why we’ll focus on *deep* reinforcement learning, but
    much of what you’ll learn in this book is the general reinforcement framework
    for control tasks (see [figure 1.5](#ch01fig05)). Then we’ll look at how you can
    design an appropriate deep learning model to fit the framework and solve a task.
    This means you will learn a lot about reinforcement learning, and you’ll probably
    will learn some things about deep learning that you didn’t know as well.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是一个表示和解决控制任务的通用框架，但在这个框架内，我们可以自由选择我们想要应用于特定控制任务的算法（[图1.4](#ch01fig04)）。深度学习算法是自然的选择，因为它们能够有效地处理复杂数据，这就是为什么我们将关注*深度*强化学习，但本书中你将学习到的大部分内容是控制任务的通用强化框架（参见[图1.5](#ch01fig05)）。然后我们将探讨如何设计一个合适的深度学习模型来适应框架并解决问题。这意味着你将学习很多关于强化学习的内容，你可能会学到一些关于深度学习的东西，你之前可能不太了解。'
- en: Figure 1.4\. As opposed to an image classifier, a reinforcement learning algorithm
    dynamically interacts with data. It continually consumes data and decides what
    actions to take—actions that will change the subsequent data presented to it.
    A video game screen might be input data for an RL algorithm, which then decides
    which action to take using the game controller, and this causes the game to update
    (e.g. the player moves or fires a weapon).
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4\. 与图像分类器不同，强化学习算法与数据的交互是动态的。它持续消耗数据并决定采取什么行动——这些行动将改变随后呈现给它的数据。视频游戏屏幕可能是一个强化学习算法的输入数据，然后它使用游戏控制器决定采取什么行动，这会导致游戏更新（例如，玩家移动或开火）。
- en: '![](01fig04.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig04.jpg)'
- en: Figure 1.5\. Deep learning is a subfield of machine learning. Deep learning
    algorithms can be used to power RL approaches to solving control tasks.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5\. 深度学习是机器学习的一个子领域。深度学习算法可以用于增强RL方法来解决控制任务。
- en: '![](01fig05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig05.jpg)'
- en: One added complexity of moving from image processing to the domain of control
    tasks is the additional element of time. With image processing, we usually train
    a deep learning algorithm on a fixed data set of images. After a sufficient amount
    of training, we typically get a high-performance algorithm that we can deploy
    to some new, unseen images. We can think of the data set as a “space” of data,
    where similar images are closer together in this abstract space and distinct images
    are farther apart ([figure 1.6](#ch01fig06)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像处理转向控制任务领域的一个额外复杂性是时间的增加。在图像处理中，我们通常在固定的图像数据集上训练深度学习算法。经过足够的训练后，我们通常得到一个高性能的算法，可以部署到一些新的、未见过的图像上。我们可以将数据集视为数据的“空间”，在这个抽象空间中，相似的图像彼此更近，不同的图像则更远（[图1.6](#ch01fig06)）。
- en: Figure 1.6\. This graphical depiction of words in a 2D space shows each word
    as a colored point. Similar words cluster together, and dissimilar words are farther
    apart. Data naturally lives in some kind of “space” with similar data living closer
    together. The labels A, B, C, and D point to particular clusters of words that
    share some semantics.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6\. 这个在二维空间中描绘单词的图形表示将每个单词表示为一个彩色点。相似的单词聚集在一起，不相似的单词则彼此更远。数据自然存在于某种“空间”中，相似的数据彼此更近。标签A、B、C和D指向一些共享某些语义的单词簇。
- en: '![](01fig06_alt.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig06_alt.jpg)'
- en: In control tasks, we similarly have a space of data to process, but each piece
    of data also has a time dimension—the data exists in both time and space. This
    means that what the algorithm decides at one time is influenced by what happened
    at a previous time. This isn’t the case for ordinary image classification and
    similar problems. Time makes the training task dynamic—the data set upon which
    the algorithm is training is not necessarily fixed but changes based on the decisions
    the algorithm makes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制任务中，我们同样有一个数据处理的空间，但每一条数据也具有时间维度——数据存在于时间和空间中。这意味着算法在某一时刻的决定会受到之前发生事件的影响。这并不是普通图像分类和类似问题的情况。时间使得训练任务动态化——算法训练的数据集不一定固定，而是根据算法做出的决策而变化。
- en: Ordinary image classification-like tasks fall under the category of *supervised
    learning*, because the algorithm is trained on how to properly classify images
    by giving it the right answers. The algorithm at first takes random guesses, and
    it is iteratively corrected until it learns the features in the image that correspond
    to the appropriate label. This requires us to already know what the right answers
    are, which can be cumbersome. If you want to train a deep learning algorithm to
    correctly classify images of various species of plants, you would have to painstakingly
    acquire thousands of such images and manually associate class labels with each
    one and prepare the data in a format that a machine learning algorithm can operate
    on, generally some type of matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 普通图像分类等任务属于**监督学习**的范畴，因为算法是通过给出正确答案来训练如何正确分类图像的。起初，算法会进行随机猜测，然后通过迭代修正直到它学会图像中与适当标签相对应的特征。这要求我们事先知道正确答案是什么，这可能很繁琐。如果你想训练一个深度学习算法来正确分类各种植物种类的图像，你必须费力地获取数千张这样的图像，并手动将类别标签与每一张图像关联起来，并准备成机器学习算法可以操作的数据格式，通常是一种矩阵类型。
- en: In contrast, in RL we don’t know exactly what the right thing to do is at every
    step. We just need to know what the ultimate goal is and what things to avoid
    doing. How do you teach a dog a trick? You have to give it tasty treats. Similarly,
    as the name suggests, we train an RL algorithm by incentivizing it to accomplish
    some high-level goal and possibly disincentivize it from doing things we don’t
    want it to do. In the case of a self-driving car, the high-level goal might be
    “get to point B from starting point A without crashing.” If it accomplishes the
    task, we reward it, and if it crashes, we penalize it. We would do this all in
    a simulator, rather than out on the real roads, so we could let it repeatedly
    try and fail at the task until it learns and gets rewarded.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在强化学习中，我们不知道在每一步确切应该做什么。我们只需要知道最终目标是什么以及要避免做什么。你是如何教狗一个把戏的？你必须给它美味的奖励。同样，正如其名所示，我们通过激励强化学习算法去完成某些高级目标，并可能从做我们不希望它做的事情中减少激励。在自动驾驶汽车的情况下，高级目标可能是“从起点A到达点B而不发生碰撞。”如果它完成了任务，我们就奖励它，如果它发生碰撞，我们就惩罚它。我们会在模拟器中这样做，而不是在真正的道路上，这样我们就可以让它反复尝试并失败，直到它学会并获得奖励。
- en: '|  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: In natural language, “reward” always means something positive, whereas in reinforcement
    learning jargon, it is a numeric quantity to be optimized. Thus, a reward can
    be positive or negative. When it is positive, it maps onto the natural language
    usage of the term, but when it is a negative value, it maps onto the natural language
    word “penalty.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言中，“奖励”总是指积极的事物，而在强化学习术语中，它是一个需要优化的数值量。因此，奖励可以是正的也可以是负的。当它是正数时，它映射到自然语言中该术语的使用，但当它是负值时，它映射到自然语言中的“惩罚”一词。
- en: '|  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The algorithm has a single objective—maximizing its reward—and in order to
    do this it must learn more elementary skills to achieve the main objective. We
    can also supply negative rewards when the algorithm chooses to do things we do
    not like, and since it is trying to maximize its reward, it will learn to avoid
    actions that lead to negative rewards. This is why it is called *reinforcement
    learning*: we either positively or negatively reinforce certain behaviors using
    reward signals (see [figure 1.7](#ch01fig07)). This is quite similar to how animals
    learn: they learn to do things that make them feel good or satisfied and to avoid
    things that cause pain.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法有一个单一目标——最大化其奖励——为了实现这一点，它必须学习更多基本的技能来实现主要目标。我们还可以在算法选择做我们不希望做的事情时提供负奖励，并且由于它试图最大化其奖励，它会学会避免导致负奖励的行动。这就是为什么它被称为*强化学习*：我们通过奖励信号（见[图1.7](#ch01fig07)）要么正要么负地强化某些行为。这与动物的学习方式非常相似：它们学会做让自己感觉良好或满意的事情，并避免导致痛苦的事情。
- en: Figure 1.7\. In the RL framework, some kind of learning algorithm decides which
    actions to take for a control task (e.g., driving a robot vacuum), and the action
    results in a positive or negative reward, which will positively or negatively
    reinforce that action and hence train the learning algorithm.
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7. 在强化学习框架中，某种学习算法决定对控制任务（例如，驾驶机器人吸尘器）采取哪些行动，而行动的结果将产生正或负的奖励，这将正或负地强化该行动，从而训练学习算法。
- en: '![](01fig07.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig07.jpg)'
- en: 1.3\. Dynamic programming versus Monte Carlo
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3. 动态规划与蒙特卡洛方法
- en: 'You now know that you can train an algorithm to accomplish some high-level
    task by assigning the completion of the task a high reward (i.e., positive reinforcement)
    and negatively reinforce things we don’t want it to do. Let’s make this concrete.
    Say the high-level goal is to train a robot vacuum to move from one room in a
    house to its dock, which is in the kitchen. It has four actions: go left, go right,
    go forward, and go reverse. At each point in time, the robot needs to decide which
    of these four actions to take. If it reaches the dock, it gets a reward of +100,
    and if it hits anything along the way, it gets a negative reward of –10\. Let’s
    say the robot has a complete 3D map of the house and has the precise location
    of the dock, but it still doesn’t know exactly what sequence of primitive actions
    to take to get to the dock.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道，你可以通过将任务的完成赋予高奖励（即正强化）以及负强化我们不希望它做的事情来训练一个算法完成某些高级任务。让我们来具体说明一下。假设高级目标是训练一个机器人吸尘器从房子的一个房间移动到厨房的码头，它有四个动作：向左、向右、向前和倒退。在每一个时间点，机器人都需要决定采取这四个动作中的哪一个。如果它到达了码头，它会获得+100的奖励，如果在路上撞到任何东西，它会获得-10的负面奖励。假设机器人拥有房子的完整3D地图，并且知道码头的精确位置，但它仍然不知道确切的动作序列来到达码头。
- en: One approach to solving this is called *dynamic programming* (DP), first articulated
    by Richard Bellman in 1957\. Dynamic programming might better be called *goal
    decomposition* as it solves complex high-level problems by decomposing them into
    smaller and smaller subproblems until it gets to a simple subproblem that can
    be solved without further information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一被称为*动态规划*（DP），最早由理查德·贝尔曼在1957年提出。动态规划可能更准确地被称为*目标分解*，因为它通过将复杂的高级问题分解成越来越小的子问题，直到找到一个可以不依赖更多信息就能解决的简单子问题。
- en: Rather than the robot trying to come up with a long sequence of primitive actions
    that will get it to the dock, it can first break the problem down into “stay in
    this room” versus “exit this room.” Since it has a complete map of the house,
    it knows it needs to exit the room, because the dock is in the kitchen. Yet it
    still doesn’t know what sequence of actions will allow it to exit the room, so
    it breaks the problem down further to “move toward the door” or “move away from
    the door.” Since the door is closer to the dock, and there is a path from the
    door to the dock, the robot knows it needs to move toward the door, but again
    it doesn’t know what sequence of primitive actions will get it toward the door.
    Lastly, it needs to decide whether to move left, right, forward, or reverse. It
    can see the door is in front of it, so it moves forward. It keeps this process
    up until it exits the room, when it must do some more goal decomposition until
    it gets to the dock.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是让机器人尝试想出一连串原始动作来达到码头，它首先可以将问题分解为“待在这个房间”与“离开这个房间”。由于它拥有整个房子的完整地图，它知道它需要离开房间，因为码头在厨房。然而，它仍然不知道什么动作序列能让它离开房间，所以它进一步将问题分解为“朝向门口移动”或“远离门口移动”。由于门口离码头更近，并且从门口到码头有一条路径，机器人知道它需要朝向门口移动，但再次它不知道什么原始动作序列能将它引向门口。最后，它需要决定是向左、向右、向前还是倒退。它可以看到门就在它面前，所以它向前移动。它继续这个过程，直到它离开房间，这时它必须进行更多的目标分解，直到到达码头。
- en: This is the essence of dynamic programming. It is a generic approach for solving
    certain kinds of problems that can be broken down into subproblems and sub-subproblems,
    and it has applications across many fields including bioinformatics, economics,
    and computer science.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是动态规划的本质。它是一种通用的方法，用于解决可以分解为子问题和子子问题的某些类型的问题，并且它在许多领域都有应用，包括生物信息学、经济学和计算机科学。
- en: In order to apply Bellman’s dynamic programming, we have to be able to break
    our problem into subproblems that we know how to solve. But even this seemingly
    innocuous assumption is difficult to realize in the real world. How do you break
    the high-level goal for a self-driving car of “get to point B from point A without
    crashing” into small non-crashing subproblems? Does a child learn to walk by first
    solving easier sub-walking problems? In RL, where we often have nuanced situations
    that may include some element of randomness, we can’t apply dynamic programming
    exactly as Bellman laid it out. In fact, DP can be considered one extreme of a
    continuum of problem-solving techniques, where the other end would be random trial
    and error.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用贝尔曼的动态规划，我们必须能够将问题分解成我们已知如何解决的子问题。但即使在现实世界中，这个看似无害的假设也很难实现。你如何将自动驾驶汽车的高层次目标“从A点安全到达B点”分解成小的非碰撞子目标？一个孩子是通过首先解决简单的子行走问题来学习走路的吗？在强化学习（RL）中，我们经常遇到可能包含一些随机元素的情况，我们无法像贝尔曼所描述的那样精确地应用动态规划。实际上，动态规划可以被视为一系列问题解决技术的一个极端，另一端则是随机尝试和错误。
- en: Another way to view this learning continuum is that in some situations we have
    maximal knowledge of the environment and in others we have minimal knowledge of
    the environment, and we need to employ different strategies in each case. If you
    need to use the bathroom in your own house, you know exactly (well, unconsciously
    at least) what sequence of muscle movements will get you to the bathroom from
    any starting position (i.e., dynamic programming-ish). This is because you know
    your house extremely well—you have a more or less perfect *model* of your house
    in your mind. If you go to a party at a house that you’ve never been to before,
    you might have to look around until you find the bathroom on your own (i.e., trial
    and error), because you don’t have a good model of that person’s house.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待这个学习连续体的方式是，在某些情况下，我们对环境的了解最多，而在其他情况下，我们对环境的了解最少，因此我们需要在每个情况下采用不同的策略。如果你需要在你自己的家里使用洗手间，你确切地知道（至少是无意识地）一系列肌肉运动将如何从任何起始位置带你到洗手间（即动态规划式的）。这是因为你非常了解你的房子——你在心中有一个或多或少完美的*模型*。如果你去一个你从未去过的人家参加派对，你可能需要四处寻找直到自己找到洗手间（即尝试和错误），因为你没有那个人的房子的良好模型。
- en: The trial and error strategy generally falls under the umbrella of *Monte Carlo
    methods*. A Monte Carlo method is essentially a random sampling from the environment.
    In many real-world problems, we have at least some knowledge of how the environment
    works, so we end up employing a mixed strategy of some amount of trial and error
    and some amount of exploiting what we already know about the environment to directly
    solve the easy sub-objectives.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试和错误策略通常属于**蒙特卡洛方法**的范畴。蒙特卡洛方法本质上是从环境中进行随机采样。在许多现实世界的问题中，我们至少对环境的工作方式有一些了解，因此我们最终采用了一种混合策略，即一部分尝试和错误，另一部分利用我们对环境的现有知识直接解决容易的子目标。
- en: 'A silly example of a mixed strategy would be if you were blindfolded, placed
    in an unknown location in your house, and told to find the bathroom by throwing
    pebbles and listening for the noise. You might start by decomposing the high-level
    goal (find the bathroom) into a more accessible sub-goal: figure out which room
    you’re currently in. To solve this sub-goal, you might throw a few pebbles in
    random directions and assess the size of the room, which might give you enough
    information to infer which room you’re in—say the bedroom. Then you’d need to
    pivot to another sub-goal: navigating to the door so you can enter the hallway.
    You’d then start throwing pebbles again, but since you remember the results of
    your last random pebble throwing, you could target your throwing to areas of less
    certainty. Iterating over this process, you might eventually find your bathroom.
    In this case, you would be applying both the goal decomposition of dynamic programming
    and the random sampling of Monte Carlo methods.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 混合策略的一个愚蠢的例子是，如果你被蒙上眼睛，放在你家里的一个未知位置，并被告诉通过扔小石子并听声音来找到洗手间。你可能首先将高层次目标（找到洗手间）分解成一个更易达到的子目标：弄清楚你现在在哪个房间里。为了解决这个子目标，你可能随机向各个方向扔几块小石子，并评估房间的大小，这可能会给你足够的信息来推断你所在的房间——比如说卧室。然后你需要转向另一个子目标：到达门口以便进入走廊。然后你又开始扔小石子，但由于你记得上次随机扔石子的结果，你可以将投掷目标对准更不确定的区域。通过迭代这个过程，你最终可能会找到洗手间。在这种情况下，你将应用动态规划的子目标分解和蒙特卡洛方法的随机采样。
- en: 1.4\. The reinforcement learning framework
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4. 强化学习框架
- en: Richard Bellman introduced dynamic programming as a general method of solving
    certain kinds of control or decision problems, but it occupies an extreme end
    of the RL continuum. Arguably, Bellman’s more important contribution was helping
    develop the standard framework for RL problems. The RL framework is essentially
    the core set of terms and concepts that every RL problem can be phrased in. This
    not only provides a standardized language for communicating with other engineers
    and researchers, it also forces us to formulate our problems in a way that is
    amenable to dynamic programming-like problem decomposition, such that we can iteratively
    optimize over local sub-problems and make progress toward achieving the global
    high-level objective. Fortunately, it’s pretty simple too.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 理查德·贝尔曼将动态规划引入作为一种解决某些类型控制或决策问题的通用方法，但它占据了强化学习连续体的极端末端。可以说，贝尔曼更重要的贡献是帮助发展强化学习问题的标准框架。强化学习框架本质上是一组核心术语和概念，每个强化学习问题都可以用这些术语和概念来表述。这不仅为与其他工程师和研究人员沟通提供了一个标准化的语言，还迫使我们以适合动态规划类似的问题分解的方式来表述我们的问题，这样我们就可以迭代优化局部子问题，并朝着实现全局高级目标迈进。幸运的是，这也很简单。
- en: To concretely illustrate the framework, let’s consider the task of building
    an RL algorithm that can learn to minimize the energy usage at a big data center.
    Computers need to be kept cool to function well, so large data centers can incur
    significant costs from cooling systems. The naive approach to keeping a data center
    cool would be to keep the air conditioning on all the time at a level that results
    in no servers ever running too hot; this would not require any fancy machine learning.
    But this is inefficient, and you could do better, since it’s unlikely that all
    servers in the center are running hot at the same times and that the data center
    usage is always at the same level. If you targeted the cooling to where and when
    it mattered most, you could achieve the same result for less money.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明框架，让我们考虑构建一个强化学习算法的任务，该算法可以学习最小化大型数据中心的能源消耗。为了良好运行，计算机需要保持冷却，因此大型数据中心可能会因冷却系统而产生显著成本。保持数据中心冷却的简单方法可能是始终开启空调，使其保持在不会让任何服务器过热的水平；这不需要任何复杂的机器学习。但这是低效的，你可以做得更好，因为不太可能所有服务器在中心同时运行过热，而且数据中心的使用水平总是相同的。如果你将冷却集中在最需要的地方和时间，你可以在更少的成本下实现相同的结果。
- en: Step one in the framework is to define your overall objective. In this case,
    our overall objective is to minimize money spent on cooling, with the constraint
    that no server in our center can surpass some threshold temperature. Although
    this appears to be two objectives, we can bundle them together into a new composite
    *objective function*. This function returns an error value that indicates how
    off-target we are at meeting the two objectives, given the current costs and the
    temperature data for the servers. The actual number that our objective function
    returns is not important; we just want to make it as low as possible. Hence, we
    need our RL algorithm to minimize this objective (error) function’s return value
    with respect to some input data, which will definitely include the running costs
    and temperature data, but may also include other useful contextual information
    that can help the algorithm predict the data center usage.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的第一步是定义你的总体目标。在这种情况下，我们的总体目标是尽量减少用于冷却的费用，同时约束条件是中心内的任何服务器温度都不能超过某个阈值。尽管这似乎是两个目标，但我们可以将它们捆绑成一个新的复合**目标函数**。这个函数返回一个错误值，表示在当前成本和服务器温度数据的情况下，我们偏离两个目标有多远。我们的目标函数返回的实际数字并不重要；我们只想让它尽可能低。因此，我们需要我们的强化学习算法最小化这个目标（错误）函数的返回值，相对于某些输入数据，这肯定包括运行成本和温度数据，但也可能包括其他有助于算法预测数据中心使用的有用上下文信息。
- en: The input data is generated by the *environment.* In general, the environment
    of a RL (or control) task is any dynamic process that produces data that is relevant
    to achieving our objective. Although we use “environment” as a technical term,
    it’s not too far abstracted from its everyday usage. As an instance of a very
    advanced RL algorithm yourself, you are always in some environment, and your eyes
    and ears are constantly consuming information produced by your environment so
    you can achieve your daily objectives. Since the environment is a *dynamic process*
    (a function of time), it may be producing a continuous stream of data of varied
    size and type. To make things algorithm-friendly, we need to take this environment
    data and bundle it into discrete packets that we call the *state* (of the environment)
    and then deliver it to our algorithm at each of its discrete time steps. The state
    reflects our knowledge of the environment at some particular time, just as a digital
    camera captures a discrete snapshot of a scene at some time (and produces a consistently
    formatted image).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是由*环境*生成的。一般来说，RL（或控制）任务的环境是任何产生与实现我们目标相关的数据的动态过程。尽管我们使用“环境”作为一个技术术语，但它并没有脱离日常用法太远。作为一个非常先进的RL算法的实例，你总是处于某种环境中，你的眼睛和耳朵不断地消耗环境产生的信息，以便你实现日常目标。由于环境是一个*动态过程*（时间的函数），它可能会产生大小和类型各异的连续数据流。为了使事情对算法友好，我们需要将这个环境数据打包成离散的数据包，我们称之为*状态*（环境的状态），然后将其在算法的每个离散时间步中交付。状态反映了我们在某个特定时间对环境的了解，就像数字相机在某个时间捕捉场景的离散快照（并产生格式一致的图像）一样。
- en: To summarize so far, we defined an objective function (minimize costs by optimizing
    temperature) that is a function of the state (current costs, current temperature
    data) of the environment (the data center and any related processes). The last
    part of our model is the RL algorithm itself. This could be *any* parametric algorithm
    that can learn from data to minimize or maximize some objective function by modifying
    its parameters. It does *not* need to be a deep learning algorithm; RL is a field
    of its own, separate from the concerns of any particular learning algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总结到目前为止，我们定义了一个目标函数（通过优化温度来最小化成本），它是环境状态（当前成本，当前温度数据）的函数（数据中心及其相关过程）。我们模型的最后一部分是RL算法本身。这可以是*任何*可以学习数据以通过修改其参数来最小化或最大化某些目标函数的参数化算法。它不需要是深度学习算法；RL是一个独立的领域，与任何特定学习算法的关注点无关。
- en: As we noted before, one of the key differences between RL (or control tasks
    generally) and ordinary supervised learning is that in a control task the algorithm
    needs to make decisions and take actions. These actions will have a causal effect
    on what happens in the future. Taking an action is a keyword in the framework,
    and it means more or less what you’d expect it to mean. However, every action
    taken is the result of analyzing the current state of the environment and attempting
    to make the best decision based on that information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，RL（或控制任务通常）与普通监督学习之间的一个关键区别是，在控制任务中，算法需要做出决策并采取行动。这些行动将对未来的事件产生因果影响。采取行动是框架中的一个关键词，它的意思大致是你所期望的。然而，每个采取的行动都是分析当前环境状态并基于该信息做出最佳决策的结果。
- en: The last concept in the RL framework is that after each action is taken, the
    algorithm is given a *reward*. The reward is a (local) signal of how well the
    learning algorithm is performing at achieving the global objective. The reward
    can be a positive signal (i.e., doing well, keep it up) or a negative signal (i.e.,
    don’t do that) even though we call both situations a “reward.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RL框架中的最后一个概念是，在采取每个动作之后，算法会得到一个*奖励*。奖励是（局部）信号，表明学习算法在实现全局目标方面的表现如何。奖励可以是积极的信号（即，做得好，继续努力）或消极的信号（即，不要这样做），尽管我们称这两种情况为“奖励”。
- en: The reward signal is the only cue the learning algorithm has to go by as it
    updates itself in hopes of performing better in the next state of the environment.
    In our data center example, we might grant the algorithm a reward of +10 (an arbitrary
    value) whenever its action reduces the error value. Or more reasonably, we might
    grant a reward proportional to how much it decreases the error. If it increases
    the error, we would give it a negative reward.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励信号是学习算法在更新自身以期望在环境下一个状态中表现更好的唯一线索。在我们的数据中心示例中，每当它的行动减少错误值时，我们可能会给算法一个+10（一个任意值）的奖励。或者更合理的是，我们可能会给予一个与错误减少程度成比例的奖励。如果它增加了错误，我们会给它一个负奖励。
- en: Lastly, let’s give our learning algorithm a fancier name, calling it the *agent*.
    The agent is the action-taking or decision-making learning algorithm in any RL
    problem. We can put this all together as shown in [figure 1.8](#ch01fig08).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们给我们的学习算法起一个更炫酷的名字，称它为**智能体**。智能体是任何强化学习问题中的行动或决策学习算法。我们可以像[图1.8](#ch01fig08)中所示的那样将这些内容全部组合起来。
- en: Figure 1.8\. The standard framework for RL algorithms. The agent takes an action
    in the environment, such as moving a chess piece, which then updates the state
    of the environment. For every action it takes, it receives a reward (e.g., +1
    for winning the game, –1 for losing the game, 0 otherwise). The RL algorithm repeats
    this process with the objective of maximizing rewards in the long term, and it
    eventually learns how the environment works.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8\. 强化学习算法的标准框架。智能体在环境中采取行动，例如移动棋子，这随后会更新环境的状态。对于它采取的每一个行动，它会收到一个奖励（例如，赢得游戏时+1，输掉游戏时-1，否则为0）。强化学习算法重复这个过程，目的是在长期内最大化奖励，并最终学会环境是如何运作的。
- en: '![](01fig08_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![01fig08_alt.jpg](01fig08_alt.jpg)'
- en: In our data center example, we hope that our agent will learn how to decrease
    our cooling costs. Unless we’re able to supply it with complete knowledge of the
    environment, it will have to employ some degree of trial and error. If we’re lucky,
    the agent might learn so well that it can be used in different environments than
    the one it was originally trained in. Since the agent is the learner, it is implemented
    as some sort of learning algorithm. And since this is a book about *deep* reinforcement
    learning, our agents will be implemented using *deep learning* algorithms (also
    known as *deep neural network*s, see [figure 1.9](#ch01fig09)). But remember,
    RL is more about the type of problem and solution than about any particular learning
    algorithm, and you could certainly use alternatives to deep neural networks. In
    fact, in [chapter 3](kindle_split_012.html#ch03) we’ll begin by using a very simple
    non-neural network algorithm, and we’ll replace it with a neural network by the
    end of the chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据中心示例中，我们希望我们的智能体学会如何降低我们的冷却成本。除非我们能提供给它对环境的完整知识，否则它将不得不采用一定程度的试错法。如果我们幸运的话，智能体可能会学得很好，以至于它可以在与最初训练的环境不同的环境中使用。由于智能体是学习者，它被实现为某种学习算法。而且，由于这是一本关于**深度**强化学习的书，我们的智能体将使用**深度学习**算法（也称为**深度神经网络**，见[图1.9](#ch01fig09)）来实现。但请记住，强化学习更多地关乎问题的类型和解决方案，而不是任何特定的学习算法，你当然可以使用深度神经网络的替代方案。实际上，在[第3章](kindle_split_012.html#ch03)中，我们将首先使用一个非常简单的非神经网络算法，并在本章结束时用神经网络替换它。
- en: Figure 1.9\. The input data (which is the state of the environment at some point
    in time) is fed into the agent (implemented as a deep neural network in this book),
    which then evaluates that data in order to take an action. The process is a little
    more involved than shown here, but this captures the essence.
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9\. 输入数据（在某个时间点的环境状态）被输入到智能体（在本书中实现为深度神经网络）中，然后智能体评估这些数据以采取行动。这个过程比这里展示的要复杂一些，但这里捕捉了其本质。
- en: '![](01fig09_alt.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![01fig09_alt.jpg](01fig09_alt.jpg)'
- en: 'The agent’s only objective is to maximize its expected rewards in the long
    term. It just repeats this cycle: process the state information, decide what action
    to take, see if it gets a reward, observe the new state, take another action,
    and so on. If we set all this up correctly, the agent will eventually learn to
    understand its environment and make reliably good decisions at every step. This
    general mechanism can be applied to autonomous vehicles, chatbots, robotics, automated
    stock trading, healthcare, and much more. We’ll explore some of these applications
    in the next section and throughout this book.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的唯一目标是最大化其长期预期奖励。它只是重复这个循环：处理状态信息，决定采取什么行动，看看是否获得奖励，观察新的状态，采取另一个行动，等等。如果我们正确设置所有这些，代理最终将学会理解其环境并在每一步做出可靠的正确决策。这种通用机制可以应用于自动驾驶汽车、聊天机器人、机器人技术、自动化股票交易、医疗保健等领域。我们将在下一节及本书的其余部分探讨一些这些应用。
- en: Most of your time in this book will be spent learning how to structure problems
    in our standard model and how to implement sufficiently powerful learning algorithms
    (agents) to solve difficult problems. For these examples, you won’t need to construct
    environments—you’ll be plugging into existing environments (such as game engines
    or other APIs). For example, OpenAI has released a Python Gym library that provides
    us with a number of environments and a straightforward interface for our learning
    algorithm to interact with. The code on the left of [figure 1.10](#ch01fig10)
    shows how simple it is to set up and use one of these environments—a car racing
    game requires only five lines of code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你大部分的时间将花在学习如何在我们标准模型中构建问题以及如何实现足够强大的学习算法（代理）来解决困难问题。对于这些示例，你不需要构建环境——你将连接到现有的环境（例如游戏引擎或其他API）。例如，OpenAI发布了一个Python
    Gym库，为我们提供了一系列环境和用于我们的学习算法与之交互的直观界面。[图1.10](#ch01fig10)左侧的代码显示了设置和使用这些环境是多么简单——一个赛车游戏只需要五行代码。
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Figure 1.10\. The OpenAI Python library comes with many environments and an
    easy-to-use interface for a learning algorithm to interact with. With just a few
    lines of code, we’ve loaded up a car racing game.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10\. OpenAI的Python库附带了许多环境和用于学习算法与交互的易于使用的界面。只需几行代码，我们就能加载一个赛车游戏。
- en: '![](01fig10.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig10.jpg)'
- en: 1.5\. What can I do with reinforcement learning?
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 我可以用强化学习做什么？
- en: We began this chapter by reviewing the basics of ordinary supervised machine
    learning algorithms, such as image classifiers, and although recent successes
    in supervised learning are important and useful, supervised learning is not going
    to get us to artificial general intelligence (AGI). We ultimately seek general-purpose
    learning machines that can be applied to multiple problems with minimal to no
    supervision and whose repertoire of skills can be transferred across domains.
    Large data-rich companies can gainfully benefit from supervised approaches, but
    smaller companies and organizations may not have the resources to exploit the
    power of machine learning. General-purpose learning algorithms would level the
    playing field for everyone, and reinforcement learning is currently the most promising
    approach toward such algorithms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章开始时回顾了普通监督机器学习算法的基本知识，例如图像分类器，尽管监督学习最近的成功很重要且有用，但监督学习并不能带我们达到人工通用智能（AGI）。我们最终寻求的是可以应用于多个问题且需要最小到没有监督的通用学习机器，其技能库可以在不同领域之间转移。大型数据丰富的公司可以从监督方法中受益，但较小的公司和组织可能没有资源来利用机器学习的力量。通用学习算法将为每个人提供一个公平的竞争环境，而强化学习是目前实现此类算法最有希望的方法。
- en: RL research and applications are still maturing, but there have been many exciting
    developments in recent years. Google’s DeepMind research group has showcased some
    impressive results and garnered international attention. The first was in 2013
    with an algorithm that could play a spectrum of Atari games at superhuman levels.
    Previous attempts at creating agents to solve these games involved fine-tuning
    the underlying algorithms to understand the specific rules of the game, often
    called *feature engineering*. These feature engineering approaches can work well
    for a particular game, but they are unable to transfer any knowledge or skills
    to a new game or domain. DeepMind’s deep Q-network (DQN) algorithm was robust
    enough to work on seven games without any game-specific tweaks (see [figure 1.11](#ch01fig11)).
    It had nothing more than the raw pixels from the screen as input and was merely
    told to maximize the score, yet the algorithm learned how to play beyond an expert
    human level.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的研究和应用仍在成熟，但近年来已经取得了许多令人兴奋的进展。谷歌的DeepMind研究小组展示了令人印象深刻的结果，并引起了国际关注。第一个是在2013年，一个能够以超人类水平玩一系列Atari游戏的算法。之前尝试创建解决这些游戏的代理涉及调整底层算法以理解游戏的特定规则，通常称为*特征工程*。这些特征工程方法对于特定的游戏可能效果很好，但它们无法将任何知识或技能转移到新的游戏或领域。DeepMind的深度Q网络（DQN）算法足够稳健，可以在七款游戏中工作，而不需要任何针对特定游戏的调整（见[图1.11](#ch01fig11)）。它只有屏幕上的原始像素作为输入，并且只是被告知要最大化得分，然而该算法学会了如何超越专家人类水平玩游戏。
- en: Figure 1.11\. DeepMind’s DQN algorithm successfully learned how to play seven
    Atari games with only the raw pixels as input and the player’s score as the objective
    to maximize. Previous algorithms, such as IBM’s Deep Blue, needed to be fine-tuned
    to play a specific game.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.11\. DeepMind的DQN算法成功地学会了如何仅使用原始像素作为输入，并以玩家的得分作为最大化的目标来玩七款Atari游戏。之前的算法，如IBM的Deep
    Blue，需要调整以玩特定的游戏。
- en: '![](01fig11_alt.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11](01fig11_alt.jpg)'
- en: 'More recently, DeepMind’s AlphaGo and AlphaZero algorithms beat the world’s
    best players at the ancient Chinese game Go. Experts believed artificial intelligence
    would not be able to play Go competitively for at least another decade because
    the game has characteristics that algorithms typically don’t handle well. Players
    do not know the best move to make at any given turn and only receive feedback
    for their actions at the end of the game. Many high-level players saw themselves
    as artists rather than calculating strategists and described winning moves as
    being beautiful or elegant. With over 10^(170) legal board positions, brute force
    algorithms (which IBM’s Deep Blue used to win at chess) were not feasible. AlphaGo
    managed this feat largely by playing simulated games of Go millions of times and
    learning which actions maximized the rewards of playing the game well. Similar
    to the Atari case, AlphaGo only had access to the same information a human player
    would: where the pieces were on the board.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，DeepMind的AlphaGo和AlphaZero算法在古老的围棋游戏中击败了世界最佳选手。专家们认为，至少还需要十年时间，人工智能才能在围棋中具有竞争力，因为这款游戏具有算法通常处理不好的特点。玩家在任何时刻都不知道最佳走法，只有在游戏结束时才能收到对动作的反馈。许多高级玩家将自己视为艺术家而不是计算策略家，并将获胜的走法描述为美丽或优雅。由于有超过10^(170)种合法的棋盘位置，蛮力算法（IBM的Deep
    Blue曾用来赢得象棋比赛）是不可行的。AlphaGo通过玩数百万次模拟的围棋游戏，并学习哪些动作最大化了游戏的奖励来实现这一壮举。与Atari案例类似，AlphaGo只能访问与人类玩家相同的信息：棋盘上棋子的位置。
- en: While algorithms that can play games better than humans are remarkable, the
    promise and potential of RL goes far beyond making better game bots. DeepMind
    was able to create a model to decrease Google’s data center cooling costs by 40%,
    something we explored earlier in this chapter as an example. Autonomous vehicles
    use RL to learn which series of actions (accelerating, turning, breaking, signaling)
    leads to passengers reaching their destinations on time and to learn how to avoid
    accidents. And researchers are training robots to complete tasks, such as learning
    to run, without explicitly programming complex motor skills.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然能够比人类玩得更好的游戏算法令人瞩目，但强化学习的承诺和潜力远不止于制作更好的游戏机器人。DeepMind能够创建一个模型，将谷歌数据中心冷却成本降低了40%，这是我们在此章前面作为例子探讨的内容。自动驾驶汽车使用强化学习来学习哪些动作序列（加速、转弯、刹车、打信号）能使乘客准时到达目的地，并学习如何避免事故。研究人员正在训练机器人完成各种任务，例如学习跑步，而不需要明确编程复杂的运动技能。
- en: Many of these examples are high stakes, like driving a car. You cannot just
    let a learning machine learn how to drive a car by trial and error. Fortunately,
    there are an increasing number of successful examples of letting learning machines
    loose in harmless simulators, and once they have mastered the simulator, letting
    them try real hardware in the real world. One instance that we will explore in
    this book is algorithmic trading. A substantial fraction of all stock trading
    is executed by computers with little to no input from human operators. Most of
    these algorithmic traders are wielded by huge hedge funds managing billions of
    dollars. In the last few years, however, we’ve seen more and more interest by
    individual traders in building trading algorithms. Indeed, Quantopian provides
    a platform where individual users can write trading algorithms in Python and test
    them in a safe, simulated environment. If the algorithms perform well, they can
    be used to trade real money. Many traders have achieved relative success with
    simple heuristics and rule-based algorithms. However, equity markets are dynamic
    and unpredictable, so a continuously learning RL algorithm has the advantage of
    being able to adapt to changing market conditions in real time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些例子都是高风险的，比如开车。你不能仅仅让学习机器通过试错来学习如何开车。幸运的是，越来越多的例子表明，让学习机器在无害的模拟器中自由探索，一旦它们掌握了模拟器，就可以让它们在现实世界中尝试真实的硬件。在这本书中，我们将探讨的一个例子是算法交易。所有股票交易中，相当大的一部分是由几乎没有人类操作员输入的计算机执行的。这些算法交易员大多由管理数十亿美元的大对冲基金操控。然而，在过去的几年里，我们看到了越来越多的个人交易者对构建交易算法的兴趣。事实上，Quantopian
    提供了一个平台，个人用户可以在其中用 Python 编写交易算法并在一个安全、模拟的环境中测试它们。如果算法表现良好，它们可以用来进行真实交易。许多交易者通过简单的启发式和基于规则的算法取得了相对的成功。然而，股票市场是动态且不可预测的，因此，一个持续学习的强化学习算法具有能够实时适应市场变化条件的优势。
- en: One practical problem we’ll tackle early in this book is advertisement placement.
    Many web businesses derive significant revenue from advertisements, and the revenue
    from ads is often tied to the number of clicks those ads can garner. There is
    a big incentive to place advertisements where they can maximize clicks. The only
    way to do this, however, is to use knowledge about the users to display the most
    appropriate ads. We generally don’t know what characteristics of the user are
    related to the right ad choices, but we can employ RL techniques to make some
    headway. If we give an RL algorithm some potentially useful information about
    the user (what we would call the environment, or state of the environment) and
    tell it to maximize ad clicks, it will learn how to associate its input data to
    its objective, and it will eventually learn which ads will produce the most clicks
    from a particular user.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期，我们将解决的一个实际问题是如何放置广告。许多网络企业从广告中获得了显著的收入，而广告收入通常与广告能够获得的点击次数相关。将广告放置在能够最大化点击的地方有很大的激励。然而，唯一的方法是利用对用户的知识来显示最合适的广告。我们通常不知道哪些用户特征与正确的广告选择相关，但我们可以采用强化学习技术取得一些进展。如果我们给强化学习算法一些关于用户可能有用的信息（我们称之为环境或环境状态），并告诉它最大化广告点击次数，它将学会如何将其输入数据与其目标相关联，并最终学会哪些广告会从特定用户那里获得最多的点击。
- en: 1.6\. Why deep reinforcement learning?
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6. 为什么是深度强化学习？
- en: We’ve made a case for reinforcement learning, but why *deep* reinforcement learning?
    RL existed long before the popular rise of deep learning. In fact, some of the
    earliest methods (which we will look at for learning purposes) involved nothing
    more than storing experiences in a lookup table (e.g., a Python dictionary) and
    updating that table on each iteration of the algorithm. The idea was to let the
    agent play around in the environment and see what happened, and to store its experiences
    of what happened in some sort of database. After a while, you could look back
    on this database of knowledge and observe what worked and what didn’t. No neural
    networks or other fancy algorithms.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为强化学习做出了辩护，但为什么是深度强化学习？强化学习在深度学习流行之前就已经存在了。事实上，一些最早的方法（我们将为了学习目的来探讨它们）仅仅涉及将经验存储在查找表中（例如，Python
    字典），并在算法的每次迭代中更新该表。想法是让智能体在环境中自由探索，看看会发生什么，并将它发生的情况存储在某种数据库中。过了一段时间，你可以回顾这个知识库，观察什么有效，什么无效。没有神经网络或其他复杂的算法。
- en: For very simple environments this actually works fairly well. For example, in
    Tic-Tac-Toe there are 255,168 valid board positions. The *lookup table* (also
    called a *memory table*) would have that many entries, which mapped from each
    state to a specific action (as shown in [figure 1.12](#ch01fig12)) and the reward
    observed (not depicted). During training, the algorithm could learn which move
    led toward more favorable positions and update that entry in the memory table.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常简单的环境，这实际上工作得相当好。例如，在井字棋中，有255,168个有效的棋盘位置。*查找表*（也称为*记忆表*）将包含那么多条目，它们将每个状态映射到特定的动作（如图1.12所示）和观察到的奖励（未展示）。在训练期间，算法可以学习哪种移动会导致更有利的位置，并更新记忆表中的条目。
- en: Figure 1.12\. An action lookup table for Tic-Tac-Toe with only three entries,
    where the “player” (an algorithm) plays X. When the player is given a board position,
    the lookup table dictates the move that they should make next. There will be an
    entry for every possible state in the game.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.12. 井字棋的动作查找表，只有三个条目，其中“玩家”（一个算法）玩X。当玩家得到一个棋盘位置时，查找表将指示他们下一步应该采取的行动。对于游戏中的每个可能状态，都会有一个条目。
- en: '![](01fig12.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图1.12](01fig12.jpg)'
- en: Once the environment gets more complicated, using a memory table becomes intractable.
    For example, every screen configuration of a video game could be considered a
    different state ([figure 1.13](#ch01fig13)). Imagine trying to store every possible
    combination of valid pixel values shown on screen in a video game! DeepMind’s
    DQN algorithm, which played Atari, was fed four 84 × 84 grayscale images at each
    step, which would lead to 256^(28228) unique game states (256 different shades
    of grey per pixel, and 4*84*84=28228 pixels). This number is much larger than
    the number of atoms in the observable universe and would definitely not fit in
    computer memory. And this was after the images were scaled down to reduce their
    size from the original 210 × 160 pixel color images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦环境变得更加复杂，使用记忆表就变得难以处理。例如，视频游戏的每个屏幕配置都可以被视为不同的状态（[图1.13](#ch01fig13)）。想象一下试图在一个视频游戏中存储屏幕上显示的每个可能的有效像素值的组合！DeepMind的DQN算法在玩Atari时，每一步都提供了四个84
    × 84的灰度图像，这将导致256^(28228)个独特的游戏状态（每个像素256种不同的灰色，4*84*84=28228像素）。这个数字比可观测宇宙中的原子数量还要大，肯定不会适合计算机内存。而且这是在将图像缩小到减少其大小从原始的210
    × 160像素彩色图像之后。
- en: Figure 1.13\. A series of three frames of Breakout. The placement of the ball
    is slightly different in each frame. If you were using a lookup table, this would
    equate to storing three unique entries in the table. A lookup table would be impractical
    as there are far too many game states to store.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.13. 《Breakout》的三帧画面。每个帧中球的位置略有不同。如果你使用查找表，这相当于在表中存储三个独特的条目。由于要存储的游戏状态太多，查找表将不切实际。
- en: '![](01fig13_alt.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图1.13](01fig13_alt.jpg)'
- en: Storing every possible state isn’t possible, but we could try to limit the possibilities.
    In the game Breakout, you control a paddle at the bottom of the screen that can
    move right or left; the objective of the game is to deflect the ball and break
    as many blocks at the top of the screen. In that case, we could define constraints—only
    look at the states when the ball is returning to the paddle, since our actions
    are not important while we are waiting for the ball at the top of the screen.
    Or we could provide our own features—instead of providing the raw image, just
    provide the position of the ball, paddle, and the remaining blocks. However, these
    methods require the programmer to understand the underlying strategies of the
    game, and they would not generalize to other environments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 存储所有可能的状态是不可能的，但我们可以尝试限制可能性。在《Breakout》游戏中，你控制屏幕底部的球拍，可以左右移动；游戏的目标是反弹球并打破屏幕顶部的方块。在这种情况下，我们可以定义约束条件——只有在球返回球拍时才查看状态，因为我们等待球在屏幕顶部时，我们的动作并不重要。或者我们可以提供自己的特征——不是提供原始图像，而是只提供球、球拍和剩余方块的位置。然而，这些方法需要程序员理解游戏背后的策略，并且它们不会推广到其他环境。
- en: That’s where deep learning comes in. A deep learning algorithm can learn to
    abstract away the details of specific arrangements of pixels and can learn the
    important features of a state. Since a deep learning algorithm has a finite number
    of parameters, we can use it to compress any possible state into something we
    can efficiently process, and then use that new representation to make our decisions.
    As a result of using neural networks, the Atari DQN only had 1792 parameters (convolutional
    neural network with 16 8 × 8 filters, 32 4 × 4 filters, and a 256-node fully connected
    hidden layer) as opposed to the 256^(28228) key/value pairs that would be needed
    to store the entire state space.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习的作用。深度学习算法可以学会抽象掉特定像素排列的细节，并学会状态的重要特征。由于深度学习算法具有有限数量的参数，我们可以用它将任何可能的状态压缩成我们可以高效处理的东西，然后使用这种新的表示来做出决策。由于使用了神经网络，Atari
    DQN只有1792个参数（16个8×8的卷积滤波器，32个4×4的卷积滤波器，以及一个256节点的全连接隐藏层），而不是存储整个状态空间所需的256^(28228)个键/值对。
- en: In the case of the Breakout game, a deep neural network might learn on its own
    to recognize the same high-level features a programmer would have to hand-engineer
    in a lookup table approach. That is, it might learn how to “see” the ball, the
    paddle, the blocks, and to recognize the direction of the ball. That’s pretty
    amazing given that it’s only being given raw pixel data. And even more interesting
    is that the learned high-level features may be transferable to other games or
    environments.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在Breakout游戏的情况下，一个深度神经网络可能能够自己学习识别程序员在查找表方法中必须手动工程化的相同高级特征。也就是说，它可能学会如何“看到”球、挡板、方块，并识别球的方向。考虑到它只被提供了原始像素数据，这已经很令人惊讶了。更有趣的是，学习到的高级特征可能可以转移到其他游戏或环境中。
- en: Deep learning is the secret sauce that makes all the recent successes in RL
    possible. No other class of algorithms has demonstrated the representational power,
    efficiency, and flexibility of deep neural networks. Moreover, neural networks
    are actually fairly simple!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是使RL最近的成功成为可能的关键因素。没有其他算法类别能够展现出深度神经网络的表征能力、效率和灵活性。此外，神经网络实际上相当简单！
- en: '1.7\. Our didactic tool: String diagrams'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7\. 我们的教具：弦图
- en: The fundamental concepts of RL have been well-established for decades, but the
    field is moving very quickly, so any particular new result could soon be out of
    date. That’s why this book focuses on teaching skills, not details with short
    half-lives. We do cover some recent advances in the field that will surely be
    supplanted in the not too distant future, but we do so only to build new skills,
    not because the particular topic we’re covering is necessarily a time-tested technique.
    We’re confident that even if some of our examples become dated, the skills you
    learn will not, and you’ll be prepared to tackle RL problems for a long time to
    come.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的基本概念已经确立了几十年，但这个领域发展非常迅速，所以任何特定的新结果都可能很快过时。这就是为什么这本书侧重于教授技能，而不是那些寿命短的细节。我们确实涵盖了一些领域中的最新进展，这些进展在不久的将来肯定会被取代，但我们这样做只是为了构建新的技能，而不是因为我们所涵盖的特定主题必然是经过时间考验的技术。我们相信，即使我们的一些例子变得过时，你学到的技能不会过时，你将准备好长期应对RL问题。
- en: Moreover, RL is a huge field with a lot to learn. We can’t possibly hope to
    cover all of it in this book. Rather than be an exhaustive RL reference or comprehensive
    course, our goal is to teach you the foundations of RL and to sample a few of
    the most exciting recent developments in the field. We expect that you will be
    able to take what you’ve learned here and easily get up to speed in the many other
    areas of RL. Plus, we have a section in [chapter 11](kindle_split_021.html#ch11)
    that gives you a roadmap of areas you might want to check out after finishing
    this book.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RL是一个巨大的领域，有很多东西要学习。我们不可能希望在这本书中涵盖所有内容。我们不是要成为RL的详尽参考或全面课程，我们的目标是教授RL的基础知识，并采样该领域一些最激动人心的最新发展。我们期望你能够将在这里学到的知识应用到RL的许多其他领域，并迅速掌握。此外，我们在第11章（kindle_split_021.html#ch11）中有一个部分，为你提供了一个在完成这本书后你可能想要检查的领域的路线图。
- en: This book is focused on teaching well, but also rigorously. Reinforcement learning
    and deep learning are both fundamentally mathematical. If you read any primary
    research articles in these fields, you will encounter potentially unfamiliar mathematical
    notations and equations. Mathematics allows us to make precise statements about
    what’s true and how things are related, and it offers rigorous explanations for
    how and why things work. We could teach RL without any math and just use Python,
    but that approach would handicap you in understanding future advances.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不仅注重教学的质量，而且注重严谨性。强化学习和深度学习在本质上都是数学性的。如果你阅读了这些领域的任何原始研究文章，你将遇到可能不熟悉的数学符号和方程。数学使我们能够对什么是真实的以及事物之间如何相关做出精确的陈述，并为事物如何以及为什么工作提供严谨的解释。我们可以不使用任何数学知识来教授强化学习，而只用Python，但这种方法会阻碍你对未来进步的理解。
- en: 'So we think the math is important, but as our editor noted, there’s a common
    saying in the publishing world: “for every equation in the book, the readership
    is halved,” which probably has some truth to it. There’s an unavoidable cognitive
    overhead in deciphering complex math equations, unless you’re a professional mathematician
    who reads and writes math all day. Faced with wanting to present a rigorous exposition
    of DRL to give readers a top-rate understanding, and yet wanting to reach as many
    people as possible, we came up with what we think is a very distinguishing feature
    of this book. As it turns out, even professional mathematicians are becoming tired
    of traditional math notation with its huge array of symbols, and within a particular
    branch of advanced mathematics called *category theory*, mathematicians have developed
    a purely graphical language called *string diagrams*. String diagrams look very
    similar to flowcharts and circuit diagrams, and they have a fairly intuitive meaning,
    but they are just as rigorous and precise as traditional mathematical notations
    largely based on Greek and Latin symbols.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们认为数学很重要，但正如我们的编辑所指出的，出版界有一句俗语：“书中每增加一个公式，读者人数就减半”，这句话可能有一定的道理。除非你是整天阅读和撰写数学公式的专业数学家，否则在解读复杂的数学方程时不可避免地会有认知上的负担。面对既要对强化学习（DRL）进行严谨的阐述以给读者提供高质量的理解，又要尽可能触及更多人，我们提出了我们认为这本书的一个非常独特的特点。结果证明，即使是专业数学家也开始厌倦了传统的数学符号体系，在被称为*范畴论*的先进数学的一个特定分支中，数学家们已经发展出一种纯图形语言，称为*弦图*。弦图看起来非常类似于流程图和电路图，它们具有相当直观的意义，但它们与基于希腊和拉丁符号的传统数学符号一样严谨和精确。
- en: '[Figure 1.14](#ch01fig14) shows a simple example of one type of string diagram
    that depicts, at a high level, a neural network with two layers. Machine learning
    (especially deep learning) involves a lot of matrix and vector operations, and
    string diagrams are particularly well-suited to describing these kinds of operations
    graphically. String diagrams are also great for describing complex processes because
    we can describe the process at varying levels of abstraction. The top panel of
    [figure 1.14](#ch01fig14) shows two rectangles representing the two layers of
    the neural network, but then we can “zoom in” (look inside the box) on layer 1
    to see what it does in more detail, which is shown in the bottom panel of [figure
    1.14](#ch01fig14).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.14](#ch01fig14) 展示了一种弦图的简单示例，它从高层次上描述了一个具有两层神经网络的图。机器学习（尤其是深度学习）涉及大量的矩阵和向量运算，弦图特别适合用图形方式描述这类运算。弦图也非常适合描述复杂的过程，因为我们可以在不同的抽象级别上描述这个过程。图1.14的顶部面板显示了代表神经网络两层的两个矩形，但我们可以“放大”（查看框内）第一层以更详细地了解它的作用，这在图1.14的底部面板中显示。'
- en: Figure 1.14\. A string diagram for a two-layer neural network. Reading from
    left to right, the top string diagram represents a neural network that accepts
    an input vector of dimension *n*, multiplies it by a matrix of dimensions *n x
    m*, returning a vector of dimension *m*. Then the nonlinear sigmoid activation
    function is applied to each element in the m-dimensional vector. This new vector
    is then fed through the same sequence of steps in layer 2, which produces the
    final output of the neural network, which is a *k*-dimensional vector.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.14\. 一个两层神经网络的弦图。从左到右阅读，顶部的弦图表示一个接受维度为 *n* 的输入向量的神经网络，将其乘以一个维度为 *n x m* 的矩阵，返回一个维度为
    *m* 的向量。然后对 m 维向量中的每个元素应用非线性sigmoid激活函数。这个新的向量随后通过层2中的相同步骤序列，产生神经网络的最终输出，这是一个
    *k*-维向量。
- en: '![](01fig14.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig14.jpg)'
- en: We will frequently use string diagrams throughout the book to communicate everything
    from complex mathematical equations to the architectures of deep neural networks.
    We will describe this graphical syntax in the next chapter, and we’ll continue
    to refine and build it up throughout the rest of the book. In some cases, this
    graphical notation is overkill for what we’re trying to explain, so we’ll use
    a combination of clear prose and Python or pseudocode. We will also include traditional
    math notation in most cases, so you will be able to learn the underlying mathematical
    concepts one way or another, whether diagrams, code, or normal mathematical notation
    most connect with you.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中频繁使用字符串图来传达从复杂的数学方程到深度神经网络架构的各种内容。我们将在下一章中描述这种图形语法，并在整本书的其余部分继续对其进行精炼和构建。在某些情况下，这种图形符号对我们试图解释的内容来说可能过于复杂，因此我们将结合使用清晰的散文和Python或伪代码。我们还将大多数情况下包含传统的数学符号，这样你就可以以某种方式学习底层数学概念，无论是图表、代码还是常规的数学符号，哪种方式最能与你产生共鸣。
- en: 1.8\. What’s next?
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8. 接下来是什么？
- en: In the next chapter, we will dive right into the real meat of RL, covering many
    of the core concepts, such as the tradeoff between exploration and exploitation,
    Markov decision processes, value functions, and policies (these terms will make
    sense soon). But first, at the beginning of the next chapter we’ll introduce some
    of the teaching methods we’ll employ throughout the book.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将直接深入到RL的真正核心内容，涵盖许多核心概念，例如探索与利用之间的权衡、马尔可夫决策过程、价值函数和政策（这些术语很快就会变得有意义）。但在下一章的开头，我们将介绍我们将在整本书中采用的一些教学方法。
- en: The rest of the book will cover core DRL algorithms that much of the latest
    research is built upon, starting with deep Q-networks, followed by policy gradient
    approaches, and then model-based algorithms. We will primarily be utilizing OpenAI’s
    Gym (mentioned earlier) to train our algorithms to understand nonlinear dynamics,
    control robots and play games ([figure 1.15](#ch01fig15)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 书的其余部分将涵盖核心的深度强化学习算法，这些算法是许多最新研究的基础，从深度Q网络开始，接着是策略梯度方法，然后是基于模型的算法。我们将主要利用OpenAI的Gym（前面提到过）来训练我们的算法，使其理解非线性动力学，控制机器人和玩游戏（[图1.15](#ch01fig15)）。
- en: 'Figure 1.15\. A depiction of a Go board, an ancient Chinese game that Google
    DeepMind used as a testbed for its AlphaGo reinforcement learning algorithm. Professional
    Go player Lee Sedol only won one game out of five, marking a turning point for
    reinforcement learning, as Go was long thought to be impervious to the kind of
    algorithmic reasoning that chess is subject to. Source: [http://mng.bz/DNX0](http://mng.bz/DNX0).'
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.15。围棋棋盘的描绘，这是一种古老的中国游戏，谷歌DeepMind将其用作其AlphaGo强化学习算法的测试平台。职业围棋选手李世石在五场比赛中只赢了一场，这标志着强化学习的一个转折点，因为长期以来人们认为围棋对算法推理的抵抗力与棋类游戏不同。来源：[http://mng.bz/DNX0](http://mng.bz/DNX0)。
- en: '![](01fig15_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15的替代文本](01fig15_alt.jpg)'
- en: In each chapter, we will open with a major problem or project that we will use
    to illustrate the important concepts and skills for that chapter. As each chapter
    progresses, we may add complexity or nuances to the starting problem to go deeper
    into some of the principles. For example, in [chapter 2](kindle_split_011.html#ch02)
    we will start with the problem of maximizing rewards at a casino slot machine,
    and by solving that problem we’ll cover most of the foundations of RL. Later we’ll
    add some complexity to that problem and change the setting from a casino to a
    business that needs to maximize advertising clicks, which will allow us to round
    out a few more core concepts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一章中，我们将以一个主要问题或项目开始，我们将用它来展示该章节的重要概念和技能。随着每一章的进展，我们可能会增加问题的复杂性和细微差别，以深入探讨某些原则。例如，在[第2章](kindle_split_011.html#ch02)中，我们将从最大化赌场老虎机奖励的问题开始，通过解决这个问题，我们将涵盖RL的大部分基础。后来，我们将增加该问题的复杂性，并将环境从赌场改为需要最大化广告点击量的企业，这将使我们能够完善一些更多的核心概念。
- en: Although this book is for those who already have experience with the basics
    of deep learning, we expect to not only teach you fun and useful RL techniques
    but also to hone your deep learning skills. In order to solve some of the more
    challenging projects, we’ll need to employ some of the latest advances in deep
    learning, such as generative adversarial networks, evolutionary methods, meta-learning,
    and transfer learning. Again, this is all in line with our skills-focused mode
    of teaching, so the particulars of these advances is not what’s important.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书是为那些已经具备深度学习基础知识的人准备的，但我们期望不仅教你有趣和有用的RL技术，而且还要磨练你的深度学习技能。为了解决一些更具挑战性的项目，我们需要采用一些深度学习的最新进展，例如生成对抗网络、进化方法、元学习和迁移学习。再次强调，所有这些都符合我们以技能为中心的教学模式，因此这些进展的细节并不重要。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Reinforcement learning is a subclass of machine learning. RL algorithms learn
    by maximizing rewards in some environment, and they’re useful when a problem involves
    making decisions or taking actions. RL algorithms can, in principle, employ any
    statistical learning model, but it has become increasingly popular and effective
    to use deep neural networks.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个子类。RL算法通过在某些环境中最大化奖励来学习，当问题涉及做出决策或采取行动时，它们是有用的。原则上，RL算法可以使用任何统计学习模型，但使用深度神经网络变得越来越流行和有效。
- en: The agent is the focus of any RL problem. It is the part of the RL algorithm
    that processes input to determine which action to take. In this book we are primarily
    focused on agents implemented as deep neural networks.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体是任何强化学习问题的焦点。它是强化学习算法中处理输入以确定采取哪个行动的部分。在这本书中，我们主要关注作为深度神经网络实现的智能体。
- en: The environment is the potentially dynamic conditions in which the agent operates.
    More generally, the environment is whatever process generates the input data for
    the agent. For example, we might have an agent flying a plane in a flight simulator,
    so the simulator would be the environment.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是智能体操作的潜在动态条件。更普遍地说，环境是生成智能体输入数据的任何过程。例如，我们可能有一个在飞行模拟器中驾驶飞机的智能体，因此模拟器就是环境。
- en: The state is a snapshot of the environment that the agent has access to and
    uses to make decisions. The environment is often a set of constantly changing
    conditions, but we can sample from the environment, and these samples at particular
    times are the state information of the environment we give to the agent.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态是智能体可以访问并用于做出决策的环境快照。环境通常是不断变化的条件集合，但我们可以从环境中采样，这些特定时间点的样本就是我们提供给智能体的环境状态信息。
- en: An action is a decision made by an agent that produces a change in its environment.
    Moving a particular chess piece is an action, and so is pressing the gas pedal
    in a car.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动是智能体在其环境中做出的决策，导致环境发生变化。移动特定的棋子是一个行动，在汽车中踩油门也是如此。
- en: A reward is a positive or negative signal given to an agent by the environment
    after it takes an action. The rewards are the only learning signals the agent
    is given. The objective of an RL algorithm (i.e., the agent) is to maximize rewards.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是环境在智能体采取行动后给出的正面或负面信号。奖励是智能体收到的唯一学习信号。强化学习算法（即智能体）的目标是最大化奖励。
- en: The general pipeline for an RL algorithm is a loop in which the agent receives
    input data (the state of the environment), the agent evaluates that data and takes
    an action from a set of possible actions given its current state, the action changes
    the environment, and the environment then sends a reward signal and new state
    information to the agent. Then the cycle repeats. When the agent is implemented
    as a deep neural network, each iteration evaluates a loss function based on the
    reward signal and backpropagates to improve the performance of the agent.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习算法的一般流程是一个循环，其中智能体接收输入数据（环境的当前状态），智能体评估这些数据并从给定其当前状态的可能行动集中选择一个行动，该行动改变环境，然后环境向智能体发送奖励信号和新状态信息。然后循环重复。当智能体作为深度神经网络实现时，每个迭代根据奖励信号评估损失函数，并通过反向传播来提高智能体的性能。
- en: 'Chapter 2\. Modeling reinforcement learning problems: Markov decision processes'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章\. 建模强化学习问题：马尔可夫决策过程
- en: '*This chapter covers*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: String diagrams and our teaching methods
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串图和我们的教学方法
- en: The PyTorch deep learning framework
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch深度学习框架
- en: Solving *n*-armed bandit problems
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决*n*-armed bandit问题
- en: Balancing exploration versus exploitation
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡探索与利用
- en: Modeling a problem as a Markov decision process (MDP)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将问题建模为马尔可夫决策过程（MDP）
- en: Implementing a neural network to solve an advertisement selection problem
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现神经网络以解决广告选择问题
- en: This chapter covers some of the most fundamental concepts in all of reinforcement
    learning, and it will be the basis for the rest of the book. But before we get
    into that, we want to first go over some of the recurring teaching methods we’ll
    employ in this book—most notably, the string diagrams we mentioned last chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了强化学习中一些最基本的概念，它将成为本书其余部分的基础。但在我们深入探讨之前，我们首先想回顾一下本书中将采用的一些常见教学方法——最值得注意的是，我们上章提到的字符串图。
- en: 2.1\. String diagrams and our teaching methods
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 字符串图和我们的教学方法
- en: In our experience, when most people try to teach something complicated, they
    tend to teach it in the reverse order in which the topic itself was developed.
    They’ll give you a bunch of definitions, terms, descriptions, and perhaps theorems,
    and then they’ll say, “great, now that we’ve covered all the theory, let’s go
    over some practice problems.” In our opinion, that’s exactly the opposite order
    in which things should be presented. Most good ideas arise as solutions to real-world
    problems, or at least imagined problems. The problem-solver stumbles across a
    potential solution, tests it, improves it, and then eventually formalizes and
    possibly mathematizes it. The terms and definitions come *after* the solution
    to the problem was developed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，当大多数人试图教授一些复杂的内容时，他们往往会按照该主题本身发展的相反顺序来教授。他们会给你一大堆定义、术语、描述，也许还有定理，然后他们会说，“太好了，现在我们已经涵盖了所有理论，让我们来回顾一些实践问题。”在我们看来，这正是应该呈现内容的相反顺序。大多数好想法都是作为解决现实世界问题或至少想象中的问题的解决方案出现的。问题解决者在偶然发现一个潜在解决方案后，对其进行测试、改进，然后最终将其形式化和可能数学化。术语和定义是在问题解决方案开发之后出现的。
- en: We think learning is most motivating and effective when you take the place of
    that original idea-maker, who was thinking of how to solve a particular problem.
    Only once the solution crystalizes does it warrant formalization, which is indeed
    necessary to establish its correctness and to faithfully communicate it to others
    in the field.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，当你扮演那个最初的想法创造者，思考如何解决特定问题时，学习是最有动力和最有效的。只有在解决方案明确之后，才值得进行形式化，这确实是必要的，以便确立其正确性，并忠实地将其传达给该领域的其他人。
- en: 'There is a powerful urge to engage in this reverse chronological mode of teaching,
    but we will do our best to resist it and develop the topic as we go. In that spirit,
    we will introduce new terms, definitions, and mathematical notations as we need
    them. For example, we will use “callouts” like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种强烈的冲动想要采用这种倒序的时间顺序教学模式，但我们将尽力抵制它，并随着讨论的进行逐步展开。本着这种精神，我们将根据需要引入新术语、定义和数学符号。例如，我们将使用如下“提示”：
- en: '|  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Definition
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: A *neural network* is a kind of machine learning model composed of multiple
    “layers” that perform a matrix-vector multiplication followed by the application
    of a nonlinear “activation” function. The matrices of the neural network are the
    model’s learnable parameters and are often called the “weights” of the neural
    network.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*是一种由多个“层”组成的机器学习模型，这些层执行矩阵-向量乘法，然后应用一个非线性“激活”函数。神经网络的矩阵是模型的可学习参数，通常被称为神经网络的“权重”。'
- en: '|  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: You will only see these callouts once per term, but we will often repeat the
    definition in different ways in the text to make sure you really understand and
    remember it. This is a course on reinforcement learning, not a textbook or reference,
    so we won’t shy away from repeating ourselves when we think something is important
    to remember.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你将只在每个术语中看到一次这些提示，但我们将经常以不同的方式在文本中重复定义，以确保你真正理解和记住它。这是一门强化学习课程，而不是教科书或参考书，所以当我们认为某些内容值得记住时，我们不会回避重复。
- en: 'Whenever we need to introduce some math, we will typically use a box showing
    the math and a pseudo-Python version of the same underlying concept. Sometimes
    it’s easier to think in terms of code or of math, and we think it’s good to get
    familiar with both. As a super simple example, if we were introducing the equation
    of a line, we would do it like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们需要介绍一些数学内容时，我们通常会使用一个显示数学公式和相同基本概念的伪Python版本的框。有时从代码或数学的角度思考更容易，我们认为熟悉两者都是有益的。作为一个非常简单的例子，如果我们正在介绍直线的方程，我们会这样做：
- en: Table 2.1\. Example of the side-by-side mathematics and pseudocode we use in
    this book
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.1.本书中使用的并列数学和伪代码示例
- en: '| Math | Pseudocode |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 伪代码 |'
- en: '| --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *y* = *mx* + *b* |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| *y* = *mx* + *b* |'
- en: '[PRE1]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: We will also include plenty of inline code (short snippets) and code listings
    (longer code examples) as well as the code for complete projects. All of the code
    in the book is provided in Jupyter Notebooks categorized by chapter on the book’s
    GitHub repository ([http://mng.bz/JzKp](http://mng.bz/JzKp)). If you’re actively
    following the text and building the projects in this book, we strongly recommend
    following the code in this associated GitHub repository rather than copying the
    code in the text—we will keep the GitHub code updated and bug-free, whereas the
    code in the book may get a bit out of date as the Python libraries we use are
    updated. The GitHub code is also more complete (e.g., showing you how to generate
    the visualizations that we include), whereas the code in the text has been kept
    as minimal as possible to focus on the underlying concepts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将包括大量的内联代码（简短片段）、代码列表（较长的代码示例）以及完整项目的代码。书中所有的代码都提供在GitHub仓库的Jupyter Notebooks中，按章节分类（[http://mng.bz/JzKp](http://mng.bz/JzKp)）。如果您正在积极跟随本书的文本并构建项目，我们强烈建议您遵循这个相关GitHub仓库中的代码，而不是复制文本中的代码——我们将保持GitHub代码的更新和错误修正，而书中的代码可能会因为Python库的更新而稍微过时。GitHub代码也更加完整（例如，展示如何生成我们包含的视觉化效果），而文本中的代码则尽可能保持最小化，以专注于基本概念。
- en: Since reinforcement learning involves a lot of interconnecting concepts that
    can become confusing when just using words, we will include a lot of diagrams
    and figures of varying sorts. The most important kind of figure we’ll use is the
    *string diagram*. It’s perhaps an odd name, but it’s a really simple idea and
    is adapted from category theory, a branch of math we mentioned in the first chapter
    where they tend to use a lot of diagrams to supplement or replace traditional
    symbolic notation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习涉及许多相互连接的概念，当仅使用文字时可能会变得令人困惑，因此我们将包含大量不同类型的图表和图形。我们将使用最重要的一种图形是*字符串图*。这个名字可能有些奇怪，但它是一个非常简单的想法，并源自我们第一章节中提到的数学的一个分支——范畴论，在那个分支中他们倾向于使用很多图表来补充或替代传统的符号表示法。
- en: You already saw the string diagram in [figure 2.1](#ch02fig01) when we introduced
    the general framework for reinforcement learning in [chapter 1](kindle_split_010.html#ch01).
    The idea is that the boxes contain nouns or noun phrases, whereas the arrows are
    labeled with verbs or verb phrases. It’s slightly different from typical flow
    diagrams, but this makes it easy to translate the string diagram into English
    prose and vice versa. It’s also very clear what the arrows are *doing* functionally.
    This particular kind of string diagram is also called an *ontological log*, or
    *olog* (“oh-log”). You can look them up if you’re curious about learning more.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您在[图2.1](#ch02fig01)中已经看到了字符串图，当时我们在第一章中介绍了强化学习的一般框架。[figure 2.1](#ch02fig01)的思路是，方框包含名词或名词短语，而箭头则标注着动词或动词短语。它与典型的流程图略有不同，但这使得将字符串图翻译成英文散文以及反过来都非常容易。箭头在功能上*做什么*也非常清晰。这种特定的字符串图也被称为*本体论日志*或*olog*（“哦-log”）。如果您想了解更多，可以查阅它们。
- en: Figure 2.1\. The standard reinforcement learning model in which an agent takes
    actions in an evolving environment that produces rewards to reinforce the actions
    of the agent.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1.标准的强化学习模型，其中智能体在一个不断变化的环境中采取行动，该环境产生奖励以强化智能体的行动。
- en: '![](02fig01_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig01_alt.jpg)'
- en: More generally, string diagrams (sometimes referred to as *wiring diagrams*
    in other sources) are flow-like diagrams that represent the flow of typed data
    along strings (i.e., directed or undirected arrows) into processes (computations,
    functions, transformations, processes, etc.), which are represented as boxes.
    The important difference between string diagrams and other similar-looking flow
    diagrams you may have seen is that all the data on the strings is explicitly typed
    (e.g., a numpy array with shape `[10, 10]`, or maybe a floating-point number),
    and the diagrams are fully compositional. By compositional, we mean that we can
    zoom in or out on the diagram to see the bigger more abstract picture or to drill
    down to the computational details.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地，字符串图解（有时在其他来源中被称为 *布线图解*）是类似于流线的图解，表示类型化的数据沿着字符串（即有向或无向箭头）流向过程（计算、函数、转换、过程等），这些过程以框的形式表示。与您可能看到的类似外观的其他流图相比，字符串图解的一个重要区别是，所有字符串上的数据都是显式类型化的（例如，形状为
    `[10, 10]` 的 numpy 数组，或者可能是一个浮点数），并且图解是完全组合的。通过组合，我们指的是我们可以放大或缩小图解，以看到更大的更抽象的图景，或者深入到计算细节。
- en: 'If we’re showing a higher-level depiction, the process boxes may just be labeled
    with a word or short phrase indicating the kind of process that happens, but we
    could also show a zoomed-in view of that process box that reveals all its internal
    details, composed of its own set of substrings and subprocesses. The compositional
    nature of these diagrams also means that we can plug parts of one diagram into
    another diagram, forming more complex diagrams, as long as all the strings’ types
    are compatible. For example, here’s a single layer of a neural network as a string
    diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们展示的是更高层次的描述，处理框可能只是用单词或短语标记，表明发生的处理类型，但我们也可以展示该处理框的放大视图，揭示其所有内部细节，这些细节由其自己的子字符串和子过程组成。这些图解的组合性质也意味着我们可以将一个图解的某些部分插入到另一个图解中，形成更复杂的图解，只要所有字符串的类型都是兼容的。例如，这里是一个神经网络单层的字符串图解：
- en: '![](pg026-1.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg026-1.jpg)'
- en: Reading from left to right, we see that some data of type `n` flows into a process
    box called Neural Network Layer and produces output of type `m`. Since neural
    networks typically take vectors as inputs and produce vectors as outputs, these
    types refer to the dimensions of the input and output vectors respectively. That
    is, this neural network layer accepts a vector of length or dimension *n* and
    produces a vector of dimension *m*. It’s possible that *n* = *m* for some neural
    network layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右阅读，我们看到类型为 `n` 的某些数据流入一个称为神经网络层的处理框，并产生类型为 `m` 的输出。由于神经网络通常以向量作为输入并产生向量作为输出，这些类型分别指输入和输出向量的维度。也就是说，这个神经网络层接受长度或维度为
    *n* 的向量，并产生维度为 *m* 的向量。对于某些神经网络层，*n* 可能等于 *m*。
- en: 'This manner of *typing* the strings is simplified, and we do it only when it’s
    clear what the types mean from the context. In other cases, we may employ mathematical
    notation such as ![](icon-r.jpg) for the set of all real numbers, which in programming
    languages basically translates to floating-point numbers. So for a vector of floating-point
    numbers with dimension *n*, we could type the strings like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对字符串的 *类型化* 方法被简化了，我们只在从上下文中可以清楚地知道类型含义时才这样做。在其他情况下，我们可能使用数学符号，如 ![图片](icon-r.jpg)
    表示所有实数的集合，这在编程语言中基本上等同于浮点数。因此，对于维度为 *n* 的浮点数向量，我们可以这样表示字符串：
- en: '![](pg026-2.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg026-2.jpg)'
- en: 'Now that the typing is richer, we not only know the dimensions of the input
    and output vectors, we know that they’re real/floating-point numbers. While this
    is almost always the case, sometimes we may be dealing with integers or binary
    numbers. In any case, our Neural Network Layer process box is left as a black
    box; we don’t know exactly what’s going on in there other than the fact that it
    transforms a vector into another vector of possibly different dimensions. We can
    decide to zoom in on this process to see what specifically is happening:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打字更加丰富，我们不仅知道输入和输出向量的维度，我们还知道它们是实数/浮点数。虽然这几乎总是这种情况，但有时我们可能正在处理整数或二进制数。无论如何，我们的神经网络层处理框仍然是一个黑盒；我们不知道里面具体发生了什么，除了它将一个向量转换成另一个可能不同维度的向量。我们可以决定深入这个过程，看看具体发生了什么：
- en: '![](pg026-3.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg026-3.jpg)'
- en: Now we can see the inside the original process box, and it is composed of its
    own set of subprocesses. We can see that our *n*-dimensional vector gets multiplied
    by a matrix of dimensions *n* × *m*, which produces an *m-*dimensional vector
    product. This vector then passes through some process called “ReLU,” which you
    may recognize as a standard neural network activation function, the rectified
    linear unit. We could continue to zoom in on the ReLU sub-subprocess if we wanted.
    Anything that deserves the name *string diagram* must be able to be scrutinized
    at any level of abstraction and remain *well typed* at any level (meaning the
    types of the data entering and exiting the processes must be compatible and make
    sense—a process that is supposed to produce sorted lists should not be hooked
    up to another process that expects integers).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到原始进程框的内部，它由其自己的子进程集组成。我们可以看到，我们的*n*维向量被一个*n* × *m*维度的矩阵相乘，从而产生一个*m*维度的向量乘积。这个向量随后通过一个称为“ReLU”的进程，你可能认识它作为标准神经网络激活函数，即线性整流单元。如果我们想的话，可以继续放大ReLU子子进程。任何值得称为*弦图*的东西都必须能够在任何抽象级别上被仔细检查，并且在任何级别上都能保持*良好类型化*（意味着进入和离开进程的数据类型必须兼容且有意义——一个本应生成排序列表的进程不应该连接到另一个期望整数的进程）。
- en: 'As long as the strings are well typed, we can string together a bunch of processes
    into a complex system. This allows us to build components once and re-use them
    wherever they’re type-matched. At a somewhat high level, we might depict a simple
    two-layer recurrent neural network (RNN) like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 只要字符串类型正确，我们就可以将多个进程组合成一个复杂的系统。这使得我们能够一次性构建组件，并在任何类型匹配的地方重复使用它们。在某种程度上，我们可以这样描述一个简单的双层循环神经网络（RNN）：
- en: '![](pg027.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg027.jpg)'
- en: This RNN takes in a *q* vector and produces an *s* vector. However, we can see
    the inside processes. There are two layers, and each one looks identical in its
    function. They each take in a vector and produce a vector, except that the output
    vector is copied and fed back into the layer process as part of the input, hence
    the recurrence.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RNN接受一个*q*向量并产生一个*s*向量。然而，我们可以看到内部进程。有两个层次，每个层次在功能上看起来完全相同。它们各自接受一个向量并产生一个向量，除了输出向量被复制并作为输入的一部分反馈到层进程中，因此产生了递归。
- en: String diagrams are a very general type of diagram; in addition to diagramming
    neural networks, we could use them to diagram how to bake a cake. A *computational
    graph* is a special kind of string diagram where all the processes represent concrete
    computations that a computer can perform, or that can be described in some programming
    language like Python. If you’ve ever visualized a computational graph in TensorFlow’s
    TensorBoard, you’ll know what we mean. The goal of a good string diagram is that
    we can view an algorithm or machine learning model at a high level to get the
    big picture, and then gradually zoom in until our string diagram is detailed enough
    for us to actually implement the algorithm based almost solely on our knowledge
    of the diagram.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串图是一种非常通用的图；除了绘制神经网络外，我们还可以用它们来绘制如何烘焙蛋糕。*计算图*是一种特殊的字符串图，其中所有进程都代表计算机可以执行的具体计算，或者可以用某种编程语言（如Python）描述。如果你曾经在TensorFlow的TensorBoard中可视化过计算图，你就会知道我们的意思。一个好的字符串图的目标是，我们可以从高层次查看算法或机器学习模型，以获得整体图景，然后逐渐放大，直到我们的字符串图足够详细，以至于我们可以几乎仅基于我们对图的知识来实现算法。
- en: Between the mathematics, simple Python code, and string diagrams that we’ll
    present in this book, you should have no problem understanding how to implement
    some pretty advanced machine learning models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将展示数学、简单的Python代码和字符串图，你应该没有问题理解如何实现一些相当高级的机器学习模型。
- en: 2.2\. Solving the multi-arm bandit
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 解决多臂老虎机问题
- en: We’re now ready to get started with a real reinforcement learning problem and
    look at the relevant concepts and skills needed to solve this problem as we go.
    But before we get too fancy, building something like AlphaGo, let’s first consider
    a simple problem. Let’s say you’re at a casino, and in front of you are 10 slot
    machines with a flashy sign that says “Play for free! Max payout is $10!” Wow,
    not bad! Intrigued, you ask one of the employees what’s going on, because it seems
    too good to be true, and she says, “It’s really true, play as much as you want,
    it’s free. Each slot machine is guaranteed to give you a reward between $0 and
    $10\. Oh, by the way, keep this to yourself, but those 10 slot machines each have
    a different average payout, so try to figure out which one gives the most rewards
    on average, and you’ll be making tons of cash!”
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始解决一个真正的强化学习问题，并在解决问题的过程中查看相关的概念和技能。但在我们变得过于复杂，构建类似 AlphaGo 的东西之前，让我们先考虑一个简单的问题。假设你在一个赌场，面前有
    10 台老虎机，一个闪亮的标志写着“免费玩！最高支付 $10！”哇，不错！好奇之下，你问一位员工发生了什么事，因为这听起来太好了，她回答说：“这真的是真的，你想玩多久就玩多久，免费的。每台老虎机都保证给你
    $0 到 $10 之间的奖励。哦，顺便说一下，保密，但这 10 台老虎机每台的平均支付都不同，所以试着找出哪一台的平均奖励最高，你将赚很多钱！”
- en: 'What kind of casino is this? Who cares, let’s just figure out how to make the
    most money! Oh by the way, here’s a joke: What’s another name for a slot machine?
    A one-armed bandit! Get it? It has one arm (a lever) and it generally steals your
    money. We could call our situation a 10-armed bandit problem, or an *n*-armed
    bandit problem more generally, where *n* is the number of slot machines. While
    this problem sounds pretty fanciful so far, you’ll see later that these *n*-armed
    bandit (or multi-armed bandit) problems do have some very practical applications.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么样的赌场？谁在乎呢，我们只要找出如何赚最多的钱！顺便说一句，这里有一个笑话：老虎机的另一个名字是什么？一个独臂强盗！明白了吗？它有一个手臂（杠杆）并且通常偷你的钱。我们可以把我们的情况称为
    10 臂强盗问题，或者更一般地称为 *n* 臂强盗问题，其中 *n* 是老虎机的数量。虽然这个问题听起来到目前为止相当离奇，但你会在后面看到这些 *n* 臂强盗（或多臂强盗）问题确实有一些非常实际的应用。
- en: 'Let’s restate our problem more formally. We have *n* possible actions (here
    *n* = 10) where an action means pulling the arm, or lever, of a particular slot
    machine, and at each play (*k*) of this game we can choose a single lever to pull.
    After taking an action (*a*) we will receive a reward, *R[k]* (reward at play
    *k*). Each lever has a unique probability distribution of payouts (rewards). For
    example, if we have 10 slot machines and play many games, slot machine #3 may
    give out an average reward of $9 whereas slot machine #1 only gives out an average
    reward of $4\. Of course, since the reward at each play is probabilistic, it is
    possible that lever #1 will by chance give us a reward of $9 on a single play.
    But if we play many games, we expect on average that slot machine #1 will be associated
    with a lower reward than #3.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更正式地重述我们的问题。我们有 *n* 种可能的行为（这里 *n* = 10），其中一种行为意味着拉动特定老虎机的臂或杠杆，并且每次游戏（*k*）中我们都可以选择拉动一个杠杆。在采取行动（*a*）后，我们将获得奖励，*R[k]*（第
    *k* 次游戏的奖励）。每个杠杆都有独特的支付（奖励）概率分布。例如，如果我们有 10 台老虎机并玩很多次游戏，老虎机 #3 可能的平均奖励为 $9，而老虎机
    #1 的平均奖励仅为 $4。当然，由于每次游戏的奖励是概率性的，有可能杠杆 #1 在单次游戏中偶然给我们 $9 的奖励。但如果我们玩很多次游戏，我们预计老虎机
    #1 的平均奖励将低于 #3。'
- en: 'Our strategy should be to play a few times, choosing different levers and observing
    our rewards for each action. Then we want to only choose the lever with the largest
    observed average reward. Thus, we need a concept of expected reward for taking
    an action (*a*) based on our previous plays. We’ll call this expected reward *Q[k]*(*a*)
    mathematically: you give the function an action (given we’re at play *k*), and
    it returns the expected reward for taking that action. This is shown formally
    here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的战略应该是先玩几次，选择不同的杠杆，并观察每次行动的奖励。然后我们只想选择观察到的平均奖励最大的杠杆。因此，我们需要一个基于我们之前游戏的概念，即采取行动（*a*）的预期奖励。我们将这个预期奖励称为
    *Q[k]*(*a*)，数学上可以这样表示：你给这个函数一个行动（假设我们在第 *k* 次游戏中），它返回采取该行动的预期奖励。这在这里被正式展示：
- en: Table 2.2\. The expected reward calculation in math and pseudocode
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.2\. 数学与伪代码中的预期奖励计算
- en: '| Math | Pseudocode |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 伪代码 |'
- en: '| --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg028.jpg) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ![](pg028.jpg) |'
- en: '[PRE2]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: That is, the expected reward at play *k* for action *a* is the arithmetic mean
    of all the previous rewards we’ve received for taking action *a*. Thus, our previous
    actions and observations influence our future actions. We might even say some
    of our previous actions *reinforce* our current and future actions, but we’ll
    come back to this later. The function *Q[k]*(*a*) is called a *value function*
    because it tells us the value of something. In particular, it is an *action*-*value
    function* because it tells us the value of taking a particular action. Since we
    typically denote this function with the symbol *Q*, it’s also often called a *Q
    function*. We’ll come back to value functions later and give a more sophisticated
    definition, but this will suffice for now.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，动作 *a* 在游戏 *k* 时的预期奖励是所有之前我们为采取动作 *a* 收到的奖励的算术平均值。因此，我们的先前动作和观察结果会影响我们的未来动作。我们甚至可以说，我们的一些先前动作*强化*了我们的当前和未来动作，但我们会稍后回到这一点。函数
    *Q[k]*(*a*) 被称为*价值函数*，因为它告诉我们某物的价值。特别是，它是一个*动作-价值函数*，因为它告诉我们采取特定动作的价值。由于我们通常用符号
    *Q* 表示这个函数，它也经常被称为*Q 函数*。我们稍后会回到价值函数，并给出一个更复杂的定义，但这对现在来说已经足够了。
- en: 2.2.1\. Exploration and exploitation
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1. 探索和利用
- en: When we first start playing, we need to play the game and observe the rewards
    we get for the various machines. We can call this strategy *exploration*, since
    we’re essentially randomly exploring the results of our actions. This is in contrast
    to a different strategy we could employ called *exploitation*, which means that
    we use our current knowledge about which machine seems to produce the most rewards,
    and keep playing that machine. Our overall strategy needs to include some amount
    of exploitation (choosing the best lever based on what we know so far) and some
    amount of exploration (choosing random levers so we can learn more). The proper
    balance of exploitation and exploration will be important to maximizing our rewards.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们刚开始玩游戏时，我们需要玩游戏并观察我们为各种机器获得的奖励。我们可以称这种策略为*探索*，因为我们实际上是在随机探索我们动作的结果。这与我们可以采用的不同策略形成对比，这种策略被称为*利用*，这意味着我们使用我们对哪个机器似乎产生最多奖励的当前知识，并继续玩那个机器。我们的整体策略需要包括一些利用（根据我们目前所知选择最佳杠杆）和一些探索（选择随机杠杆以便我们了解更多）。利用和探索的正确平衡对于最大化我们的奖励非常重要。
- en: 'How can we come up with an algorithm to figure out which slot machine has the
    largest average payout? Well, the simplest algorithm would be to just select the
    action associated with the highest Q value:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何想出一个算法来确定哪个老虎机的平均回报率最高？嗯，最简单的算法就是选择与最高 Q 值相关的动作：
- en: Table 2.3\. Computing the best action, given the expected rewards
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.3. 根据预期奖励计算最佳动作
- en: '| Math | Pseudocode |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 伪代码 |'
- en: '| --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ∀*a**[i]* ∈ *A[k]* |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ∀*a**[i]* ∈ *A[k]* |'
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| *a** = arg*max**[a]**Q[k]*(*a**[i]*) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| *a** = arg*max**[a]**Q[k]*(*a**[i]*) |'
- en: '[PRE4]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The following listing shows it as legitimate Python 3 code.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了它作为合法的 Python 3 代码。
- en: Listing 2.1\. Finding the best actions given the expected rewards in Python
    3
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1. 根据 Python 3 中的预期奖励找到最佳动作
- en: '[PRE5]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1*** Loops through all possible actions'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 遍历所有可能的动作'
- en: '***2*** Gets the value of the current action'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取当前动作的值'
- en: 'We use our above function *Q[k]*(*a*) on all the possible actions, and select
    the action that returns the maximum average reward. Since *Q[k]*(*a*) depends
    on a record of our previous actions and their associated rewards, this method
    will not evaluate actions that we haven’t already explored. Thus, we might have
    previously tried levers #1 and #3 and noticed that lever #3 gives us a higher
    reward, but with this method we’ll never think to try another lever, say #6, which,
    unbeknownst to us, actually gives out the highest average reward. This method
    of simply choosing the best lever that we know of so far is called a *greedy*
    (or exploitation) method.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在我们的上述函数 *Q[k]*(*a*) 上对所有可能的动作进行操作，并选择返回最大平均奖励的动作。由于 *Q[k]*(*a*) 依赖于我们先前动作及其相关奖励的记录，这种方法将不会评估我们尚未探索的动作。因此，我们可能之前尝试过杠杆
    #1 和 #3，并注意到杠杆 #3 给我们更高的奖励，但使用这种方法，我们永远不会想到尝试另一个杠杆，比如说 #6，而我们不知道的是，它实际上给出了最高的平均奖励。这种方法简单地选择我们目前所知的最佳杠杆被称为*贪婪*（或利用）方法。'
- en: 2.2.2\. Epsilon-greedy strategy
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2. Epsilon-greedy 策略
- en: We need some exploration of other levers (other slot machines) to discover the
    true best action. One simple modification to our previous algorithm is to change
    it to an ε(epsilon)-greedy algorithm, such that with a probability, ε, we will
    choose an action, *a*, at random, and the rest of the time (probability 1 – ε)
    we will choose the best lever based on what we currently know from past plays.
    Most of the time we will play greedy, but sometimes we will take a risk and choose
    a random lever to see what happens. The result will, of course, influence our
    future greedy actions. Let’s see if we can solve this in code with Python.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要探索其他杠杆（其他老虎机）以发现最佳动作。对我们之前的算法的一个简单修改是将它改为 ε(epsilon)-贪婪算法，这样我们就会以概率 ε 随机选择一个动作
    *a*，其余时间（概率 1 – ε）我们将根据我们从过去的游戏中了解的情况选择最佳杠杆。大多数时候我们会玩贪婪，但有时我们会冒险选择一个随机杠杆看看会发生什么。结果当然会影响我们未来的贪婪动作。让我们看看我们能否用
    Python 代码解决这个问题。
- en: Listing 2.2\. Epsilon-greedy strategy for action selection
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.2\. ε-贪婪策略的动作选择
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1*** Number of arms (number of slot machines)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 臂的数量（老虎机的数量）'
- en: '***2*** Hidden probabilities associated with each arm'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 每个臂相关的隐藏概率'
- en: '***3*** Epsilon for epsilon-greedy action selection'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** ε-贪婪动作选择的 ε 值'
- en: In this casino example, we will be solving a 10-armed bandit problem, so *n*
    = 10\. We’ve also defined a numpy array of length *n* filled with random floats
    that can be understood as probabilities. Each position in the `probs` array corresponds
    to an arm, which is a possible action. For example, the first element has index
    position 0, so action 0 is arm 0\. Each arm has an associated probability that
    weights how much reward it pays out.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个赌场示例中，我们将解决一个 10-臂老虎机问题，所以 *n* = 10。我们还定义了一个长度为 *n* 的 numpy 数组，其中填充了随机浮点数，这些数可以理解为概率。`probs`
    数组中的每个位置对应一个臂，即一个可能的动作。例如，第一个元素的位置索引为 0，因此动作 0 是臂 0。每个臂都有一个与之相关的概率，该概率决定了它支付的奖励量。
- en: 'The way we’ve chosen to implement our reward probability distributions for
    each arm is this: Each arm will have a probability, e.g., 0.7, and the maximum
    reward is $10\. We will set up a `for` loop going to `10`, and at each step it
    will add `1` to the reward if a random float is less than the arm’s probability.
    Thus, on the first loop it makes up a random float (e.g., 0.4). 0.4 is less than
    0.7, so `reward += 1`. On the next iteration, it makes up another random float
    (e.g., 0.6) which is also less than 0.7, so `reward += 1`. This continues until
    we complete 10 iterations, and then we return the final total reward, which could
    be anything between 0 and 10\. With an arm probability of 0.7, the *average* reward
    of doing this to infinity would be 7, but on any single play it could be more
    or less.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择实现每个臂的奖励概率分布的方式是这样的：每个臂将有一个概率，例如，0.7，最大奖励是 $10。我们将设置一个 `for` 循环，直到 `10`，在每次迭代中，如果随机浮点数小于臂的概率，它将奖励增加
    `1`。因此，在第一次循环中，它生成一个随机浮点数（例如，0.4）。0.4 小于 0.7，所以 `reward += 1`。在下一次迭代中，它生成另一个随机浮点数（例如，0.6），这也小于
    0.7，所以 `reward += 1`。这会一直持续到我们完成 10 次迭代，然后我们返回最终的累计奖励，这可能是 0 到 10 之间的任何值。在臂概率为
    0.7 的情况下，对无限次进行此操作的 *平均* 奖励将是 7，但在任何单次游戏中它可能会更多或更少。
- en: Listing 2.3\. Defining the reward function
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.3\. 定义奖励函数
- en: '[PRE7]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can check this by running it:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行它来检查：
- en: '[PRE8]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This output shows that running this code 2,000 times with a probability of 0.7
    indeed gives us a mean reward of close to 7 (see the histogram in [figure 2.2](#ch02fig02)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出显示，以 0.7 的概率运行此代码 2,000 次确实给出了接近 7 的平均奖励（参见 [图 2.2](#ch02fig02) 的直方图）。
- en: Figure 2.2\. The distribution of rewards for a simulated n-armed bandit with
    a 0.7 probability of payout.
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.2\. 模拟 n-臂老虎机的奖励分布。
- en: '![](02fig02_alt.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片 2.2](02fig02_alt.jpg)'
- en: The next function we’ll define is our greedy strategy of choosing the best arm
    so far. We need a way to keep track of which arms were pulled and what the resulting
    reward was. Naively, we could just have a list and append observations such as
    (arm, reward), e.g., (2, 9), indicating we chose arm 2 and received reward 9\.
    This list would grow longer as we played the game.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要定义的函数是我们选择最佳臂的贪婪策略。我们需要一种方法来跟踪哪些臂被拉动以及相应的奖励是什么。天真地，我们只需有一个列表，并将观察结果（臂，奖励）如（2，9）附加到列表中，例如，表示我们选择了臂
    2 并获得了奖励 9。随着我们玩游戏，这个列表会变得越来越长。
- en: There’s a much simpler approach, however, since we really only need to keep
    track of the average reward for each arm—we don’t need to store each observation.
    Recall that to calculate the mean of a list of numbers, *x[i]* (indexed by *i*),
    we simply need sum up all the *x[i]* values and then divide by the number of *x[i]*,
    which we will denote *k*. The mean is often denoted with the Greek letter μ (mu).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个更简单的方法，因为我们实际上只需要跟踪每个臂的平均奖励——我们不需要存储每个观察结果。回想一下，为了计算数字列表的平均值，*x[i]*（由*i*索引），我们只需要将所有的*x[i]*值加起来，然后除以*x[i]*的数量，我们将这个数量表示为*k*。平均值通常用希腊字母μ（mu）表示。
- en: '![](pg032-1.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](pg032-1.jpg)'
- en: 'The Greek uppercase symbol Σ (sigma) is used to denote a summation operation.
    The *i* notation underneath means we sum each element, *x[i]*. It’s basically
    the math equivalent of a `for` loop like:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 希腊大写符号Σ（sigma）用于表示求和操作。下方的*i*表示我们求和每个元素，*x[i]*。它基本上是数学上与`for`循环等价的：
- en: '[PRE9]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If we already have an average reward μ for a particular arm, we can update this
    average when we get a new reward by recomputing the average. We basically need
    to undo the average and then recompute it. To undo it, we multiply μ by the total
    number of values, *k*. Of course, this just gives us the sum, not the original
    set of values—you can’t undo a sum. But the total number is what we need to recompute
    the average with a new value. We just add this sum to the new value and divide
    by *k* + 1, the new total number of values.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经有了一个特定臂的平均奖励μ，当我们得到一个新的奖励时，我们可以通过重新计算平均值来更新这个平均值。我们基本上需要撤销平均并重新计算。为了撤销它，我们将μ乘以总值的数量，*k*。当然，这只会给我们总和，而不是原始的值集——你不能撤销一个总和。但总数是我们需要用新值重新计算平均值的。我们只需将这个总和加到新值上，然后除以*k*
    + 1，即新的总值数量。
- en: '![](pg032-2.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](pg032-2.jpg)'
- en: 'We can use this equation to continually update the average reward observed
    for each arm as we collect new data, and this way we only need to keep track of
    two numbers for each arm: *k*, the number of values observed, and μ, the current
    running average. We can easily store this in a 10 × 2 numpy array (assuming we
    have 10 arms). We’ll call this array the `record`.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个方程式在收集新数据时不断更新每个臂观察到的平均奖励，这样我们只需要为每个臂跟踪两个数字：*k*，观察到的值数，和μ，当前运行的平均值。我们可以很容易地将这些存储在一个10
    × 2的numpy数组中（假设我们有10个臂）。我们将这个数组称为`record`。
- en: '[PRE10]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first column of this array will store the number of times each arm has been
    pulled, and the second column will store the running average reward. Let’s write
    a function for updating the record, given a new action and reward.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数组的第1列将存储每个臂被拉动的次数，第2列将存储运行平均奖励。让我们编写一个函数来更新记录，给定一个新的动作和奖励。
- en: Listing 2.4\. Updating the reward record
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4\. 更新奖励记录
- en: '[PRE11]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This function takes the `record` array, an action (which is the index value
    of the arm), and a new reward observation. To update the average reward, it simply
    implements the mathematical function we described previously, and then increments
    the counter recording how many times that arm has been pulled.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受`record`数组、一个动作（这是臂的索引值）和一个新的奖励观察结果。为了更新平均奖励，它只是简单地实现了我们之前描述的数学函数，然后增加记录该臂被拉动次数的计数器。
- en: Next we need a function that will select which arm to pull. We want it to choose
    the arm associated with the highest average reward, so all we need to do is find
    the row in the `record` array with biggest value in column 1\. We can easily do
    this using numpy’s built-in `argmax` function, which takes in an array, finds
    the largest value in the array, and returns its index position.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要一个函数来选择拉动哪个臂。我们希望它选择与最高平均奖励相关的臂，所以我们只需要找到`record`数组中第1列值最大的行。我们可以很容易地使用numpy的内置`argmax`函数来完成这个任务，该函数接受一个数组，找到数组中的最大值，并返回其索引位置。
- en: Listing 2.5\. Computing the best action
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5\. 计算最佳动作
- en: '[PRE12]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1*** Uses numpy argmax on column 1 of the record array'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 在记录数组的第1列使用numpy的argmax'
- en: Now we can get into the main loop for playing the *n*-armed bandit game. If
    a random number is greater than the epsilon parameter, we just calculate the best
    action using the `get_best_arm` function and take that action. Otherwise we take
    a random action to ensure some amount of exploration. After choosing the arm,
    we use the `get_reward` function and observe the reward value. We then update
    the `record` array with this new observation. We repeat this process a bunch of
    times, and it will continually update the *record* array. The arm with the highest
    reward probability should eventually get chosen most often, since it will give
    out the highest average reward.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进入玩**n**臂老虎机游戏的主循环。如果随机数大于epsilon参数，我们就使用`get_best_arm`函数计算最佳动作并采取该动作。否则，我们采取随机动作以确保一定程度的探索。选择臂之后，我们使用`get_reward`函数并观察奖励值。然后我们使用这个新的观察结果更新`record`数组。我们重复这个过程多次，它将不断更新`record`数组。具有最高奖励概率的臂最终应该被选择得最频繁，因为它会给出最高的平均奖励。
- en: We’ve set it to play 500 times in the following listing, and to display a `matplotlib`
    scatter plot of the mean reward against plays. Hopefully we’ll see that the mean
    reward increases as we play more times.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的列表中将其设置为玩500次，并显示平均奖励与游戏次数的`matplotlib`散点图。希望我们会看到随着游戏次数的增加，平均奖励会增加。
- en: Listing 2.6\. Solving the *n*-armed bandit
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6\. 解决**n**臂老虎机问题
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1*** Initializes the record array to all zeros'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将记录数组初始化为零'
- en: '***2*** Randomly initializes the probabilities of rewards for each arm'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 随机初始化每个臂的奖励概率'
- en: '***3*** Chooses the best action with 0.8 probability, or randomly otherwise'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 以0.8的概率选择最佳动作，否则随机选择'
- en: '***4*** Computes the reward for choosing the arm'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 计算选择臂的奖励'
- en: '***5*** Updates the record array with the new count and reward observation
    for this arm'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用这个臂的新计数和奖励观察结果更新记录数组'
- en: '***6*** Keeps track of the running average of rewards to assess overall performance'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 跟踪奖励的运行平均值以评估整体性能'
- en: As you can see in [figure 2.3](#ch02fig03), the average reward does indeed improve
    after many plays. Our algorithm is *learning*; it is getting reinforced by previous
    good plays! And yet it is such a simple algorithm.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[图2.3](#ch02fig03)中看到的，平均奖励确实在多次游戏后有所提高。我们的算法正在**学习**；它通过之前的良好游戏得到加强！然而，它是一个非常简单的算法。
- en: Figure 2.3\. This plot shows that the average reward for each slot machine play
    increases over time, indicating we are successfully learning how to solve the
    n-armed bandit problem.
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3\. 这个图显示，每次老虎机游戏的平均奖励随时间增加，表明我们成功地学会了如何解决n臂老虎机问题。
- en: '![](02fig03_alt.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig03_alt.jpg)'
- en: The problem we’ve considered here is a *stationary* problem because the underlying
    reward probability distributions for the arms does not change over time. We certainly
    could consider a variant of this problem where this is not true—a nonstationary
    problem. In this case, a simple modification would be to allow new reward observations
    to update the average reward value stored in the record in a skewed way, so that
    it would be a weighted average, weighted toward the newest observation. This way,
    if things change over time, we would be able to track them to some degree. We
    won’t implement this slightly more complex variant here, but we will encounter
    nonstationary problems later in the book.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里考虑的问题是**静态**问题，因为臂的潜在奖励概率分布不会随时间改变。我们当然可以考虑这种情况下不成立的问题的变体——非静态问题。在这种情况下，一个简单的修改就是允许新的奖励观察结果以偏斜的方式更新记录中存储的平均奖励值，使其成为一个加权平均，偏向于最新的观察结果。这样，如果随着时间的推移事情发生变化，我们就能在一定程度上跟踪它们。我们在这里不会实现这个稍微复杂一点的变体，但在本书的后面我们会遇到非静态问题。
- en: 2.2.3\. Softmax selection policy
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. Softmax选择策略
- en: 'Imagine another type of bandit problem: A newly minted doctor specializes in
    treating patients with heart attacks. She has 10 treatment options, of which she
    can choose only 1 to treat each patient she sees. For some reason, all she knows
    is that these 10 treatments have different efficacies and risk profiles for treating
    heart attacks—she doesn’t know which one is the best yet. We could use the *n*-armed
    bandit algorithm from the previous solution, but we might want to reconsider our
    ε-greedy policy of randomly choosing a treatment once in a while. In this new
    problem, randomly choosing a treatment could result in patient death, not just
    losing some money. We really want to make sure we don’t choose the worst treatment,
    but we still want some ability to explore our options to find the best one.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 想象另一种类型的投币机问题：一位新晋医生专门治疗患有心脏病发作的患者。她有10种治疗方案可供选择，每次只能选择其中一种来治疗她看到的每位患者。出于某种原因，她只知道这10种治疗方案在治疗心脏病发作方面有不同的有效性和风险特征——她不知道哪一个是最好的。我们可以使用前一个解决方案中的*n*-armed
    bandit算法，但我们可能想要重新考虑偶尔随机选择治疗方案的ε-greedy策略。在这个新问题中，随机选择治疗方案可能会导致患者死亡，而不仅仅是损失一些钱。我们确实想确保我们不会选择最差的治疗方案，但我们仍然想有一些能力来探索我们的选项以找到最好的一个。
- en: 'This is where a *softmax* selection might be most appropriate. Instead of just
    choosing an action at random during exploration, softmax gives us a probability
    distribution across our options. The option with the largest probability would
    be equivalent to the best arm action in the previous solution, but it will also
    give us some idea about which are the second and third best actions, for example.
    This way we can randomly choose to explore other options while avoiding the very
    worst options, since they will be assigned tiny probabilities or even 0\. Here’s
    the softmax equation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是softmax选择可能最合适的地方。在探索过程中，我们不是随机选择一个动作，而是softmax给我们提供了一个关于我们选项的概率分布。概率最大的选项将等同于前一个解决方案中的最佳臂动作，但它也会给我们一些关于哪些是第二和第三好动作的想法，例如。这样我们就可以随机选择探索其他选项，同时避免选择最差的选项，因为它们将被分配极小的概率或甚至0。以下是softmax方程：
- en: Table 2.4\. The softmax equation
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表格2.4\. softmax方程
- en: '| Math | Pseudocode |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 伪代码 |'
- en: '| --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg035.jpg) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ![](pg035.jpg) |'
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Pr(*A*) is a function that accepts an action-value vector (array) and returns
    a probability distribution over the actions, such that higher value actions have
    higher probabilities. For example, if your action-value array has four possible
    actions and they all currently have the same value, say `A = [10, 10, 10, 10]`,
    then `Pr(A) = [0.25, 0.25, 0.25, 0.25]`. In other words, all the probabilities
    are the same and must sum to 1.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Pr(*A*)是一个接受动作值向量（数组）的函数，并返回一个关于动作的概率分布，其中值较高的动作具有更高的概率。例如，如果你的动作值数组有四个可能的动作，并且它们目前都具有相同的值，比如说`A
    = [10, 10, 10, 10]`，那么`Pr(A) = [0.25, 0.25, 0.25, 0.25]`。换句话说，所有概率都是相同的，并且必须总和为1。
- en: The numerator of the fraction exponentiates the action-value array divided by
    a parameter, τ, yielding a vector of the same size (i.e., length) as the input.
    The denominator sums over the exponentiation of each individual action value divided
    by τ, yielding a single number.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 分数的分子将动作值数组除以参数τ进行指数化，得到与输入相同大小（即长度）的向量。分母对每个单独的动作值除以τ的指数化求和，得到一个单一的数字。
- en: τ is a parameter called *temperature* that scales the probability distribution
    of actions. A high temperature will cause the probabilities to be very similar,
    whereas a low temperature will exaggerate differences in probabilities between
    actions. Selecting a value for this parameter requires an educated guess and some
    trial and error. The mathematical exponential *e^x* is a function call to `np.exp(...)`
    in numpy. It will apply the function element-wise across the input vector. Here’s
    how we actually write the softmax function in Python.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: τ是一个称为*温度*的参数，它缩放动作的概率分布。高温会导致概率非常相似，而低温会夸大动作之间概率的差异。为这个参数选择一个值需要有一定的猜测和一些试错。数学指数*e^x*是numpy中`np.exp(...)`的函数调用。它将在输入向量上逐元素应用该函数。以下是我们如何在Python中实际编写softmax函数。
- en: Listing 2.7\. The softmax function
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7\. softmax函数
- en: '[PRE15]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we implement the previous 10-armed bandit problem using `softmax`, we don’t
    need the `get_best_arm` function anymore. Since `softmax` produces a weighted
    probability distribution across our possible actions, we can randomly select actions
    according to their relative probabilities. That is, our best action will get chosen
    more often because it will have the highest `softmax` probability, but other actions
    will be chosen at lower frequencies.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`softmax`实现前10臂老虎机问题时，不再需要`get_best_arm`函数。因为`softmax`会在我们的可能动作上产生一个加权的概率分布，我们可以根据它们的相对概率随机选择动作。也就是说，最佳动作将被更频繁地选择，因为它将具有最高的`softmax`概率，但其他动作的选择频率将较低。
- en: 'To implement this, all we need to do is apply the `softmax` function over the
    second column (column index 1) of the `record` array, since that’s the column
    that stores the current mean reward (the action values) for each action. It will
    transform these action values into probabilities. Then we use the `np.random.choice`
    function, which accepts an arbitrary input array, *x*, and a parameter, *p*, that
    is an array of probabilities that correspond to each element in *x*. Since our
    record is initialized to all zeros, `softmax` at first will return a uniform distribution
    over all the arms, but this distribution will quickly skew toward whatever action
    is associated with the highest reward. Here’s an example of using `softmax` and
    the random choice function:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，我们只需要在`record`数组的第二列（列索引1）上应用`softmax`函数，因为那是存储每个动作当前平均奖励（动作值）的列。它将把这些动作值转换成概率。然后我们使用`np.random.choice`函数，它接受一个任意输入数组*x*和一个参数*p*，它是一个与*x*中的每个元素相对应的概率数组。由于我们的记录初始化为全零，`softmax`最初将在所有臂上返回一个均匀分布，但这个分布将迅速偏向与最高奖励相关的动作。以下是一个使用`softmax`和随机选择函数的示例：
- en: '[PRE16]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We used the numpy `arange` function to create an array from 0 to 9, corresponding
    to the indices of each arm, so the random choice function will return an arm index
    according to the supplied probability vector. We can use the same training loop
    as we did previously; we just need to change the arm selection part so it uses
    `softmax` instead of `get_best_arm`, and we need to get rid of the random action
    selection that’s part of the epsilon-greedy strategy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用numpy的`arange`函数创建一个从0到9的数组，对应于每个臂的索引，因此随机选择函数将根据提供的概率向量返回一个臂索引。我们可以使用之前相同的训练循环；我们只需要更改臂选择部分，使其使用`softmax`而不是`get_best_arm`，并且我们需要去除epsilon-greedy策略中的一部分随机动作选择。
- en: Listing 2.8\. Softmax action-selection for the *n*-armed bandit
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8\. 对于*n*-臂老虎机的softmax动作选择
- en: '[PRE17]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1*** Computes softmax probabilities for each arm with respect to their current
    action values'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 计算每个臂相对于其当前动作值的softmax概率'
- en: '***2*** Chooses an arm randomly but weighted by the softmax probabilities'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 随机选择一个臂，但根据softmax概率进行加权'
- en: Softmax action selection seems to do better than the epsilon-greedy method for
    this problem as you can tell from [figure 2.4](#ch02fig04); it looks like it converges
    on an optimal policy faster. The downside to softmax is having to manually select
    the *τ* parameter. Softmax here was pretty sensitive to *τ*, and it takes some
    time playing with it to find a good value. Obviously with epsilon-greedy we had
    to set the epsilon parameter, but choosing that parameter was much more intuitive.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图2.4](#ch02fig04)中可以看出，softmax动作选择似乎比epsilon-greedy方法对此问题表现更好；它看起来像它更快地收敛到最优策略。softmax的缺点是需要手动选择*τ*参数。这里的softmax对*τ*非常敏感，需要花一些时间调整以找到合适的值。显然，在epsilon-greedy中我们必须设置epsilon参数，但选择该参数要直观得多。
- en: Figure 2.4\. With the softmax policy, the n-armed bandit algorithm tends to
    converge faster on the maximal mean reward.
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4\. 在softmax策略下，n-臂老虎机算法倾向于更快地收敛到最大平均奖励。
- en: '![](02fig04_alt.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig04_alt.jpg)'
- en: 2.3\. Applying bandits to optimize ad placements
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 将老虎机应用于优化广告投放
- en: The slot machine example may not seem to be a particularly real-world problem,
    but if we add one element, it does become a practical business problem, with one
    big example being advertisement placement. Whenever you visit a website with ads,
    the company placing the ads wants to maximize the probability that you will click
    them.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 槽机示例可能不是一个特别真实世界的问题，但如果我们添加一个元素，它就变成了一个实际的商业问题，一个很大的例子就是广告投放。每次你访问带有广告的网站时，放置广告的公司都希望最大化你点击它们的概率。
- en: Let’s say we manage 10 e-commerce websites, each focusing on selling a different
    broad category of retail items such as computers, shoes, jewelry, etc. We want
    to increase sales by referring customers who shop on one of our sites to another
    site that they might be interested in. When a customer checks out on a particular
    site in our network, we will display an advertisement to one of our other sites
    in hopes they’ll go there and buy something else. Alternatively, we could place
    an ad for another product on the same site. Our problem is that we don’t know
    which sites we should refer users to. We could try placing random ads, but we
    suspect a more targeted approach is possible.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们管理着10个电子商务网站，每个网站专注于销售不同的大类零售商品，如电脑、鞋子、珠宝等。我们希望通过将购物于我们网站之一的客户推荐到他们可能感兴趣的另一个网站来增加销售额。当客户在我们的网络中的某个特定网站上结账时，我们将展示我们其他网站中的一个广告，希望他们能去那里购买其他商品。或者，我们可以在同一网站上放置另一个产品的广告。我们的问题是不知道我们应该将用户推荐到哪个网站。我们可以尝试随机放置广告，但我们怀疑可能有一种更精准的方法。
- en: 2.3.1\. Contextual bandits
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 上下文老虎机
- en: Perhaps you can see how this just adds a new layer of complexity to the *n*-armed
    bandit problem we considered at the beginning of the chapter. At each play of
    the game (each time a customer checks out on a particular website) we have *n*
    = 10 possible actions we can take, corresponding to the 10 different types of
    advertisements we could place. The twist is that the best ad to place may depend
    on which site in the network the current customer is on. For example, a customer
    checking out on our jewelry site may be more in the mood to buy a new pair of
    shoes to go with their new diamond necklace than they would be to buy a new laptop.
    Thus our problem is to figure out how a particular site relates to a particular
    advertisement.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你能看出这仅仅给我们在本章开头讨论的**n**臂老虎机问题增加了一层复杂性。在游戏的每一次玩（每次客户在特定网站上结账）时，我们有**n** = 10种可能采取的行动，对应于我们可以放置的10种不同类型的广告。转折在于，放置的最佳广告可能取决于当前客户所在的网络中的哪个网站。例如，在我们的珠宝网站上结账的客户可能更愿意购买一双新鞋子来搭配他们新买的钻石项链，而不是购买一台新笔记本电脑。因此，我们的问题是如何确定特定网站与特定广告之间的关系。
- en: This leads us to *state spaces*. The *n*-armed bandit problem we started with
    had an *n*-element *action space* (the space or set of all possible actions),
    but there was no concept of state. That is, there was no information in the environment
    that would help us choose a good arm. The only way we could figure out which arms
    were good is by trial and error. In the ad problem, we know the user is buying
    something on a particular site, which may give us some information about that
    user’s preferences and could help guide our decision about which ad to place.
    We call this contextual information a *state* and this new class of problems *contextual*
    bandits (see [figure 2.5](#ch02fig05)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这引导我们进入**状态空间**。我们最初讨论的**n**臂老虎机问题有一个**n**元素的**动作空间**（所有可能动作的空间或集合），但没有状态的概念。也就是说，环境中没有信息能帮助我们选择一个好的臂。我们能确定哪些臂是好的唯一方法是通过试错。在广告问题中，我们知道用户在特定网站上购买东西，这可能给我们一些关于用户偏好的信息，并可能帮助我们决定放置哪个广告。我们称这种上下文信息为**状态**，并将这类新问题称为**上下文老虎机**（见[图2.5](#ch02fig05)）。
- en: Figure 2.5\. Overview of a contextual bandit for advertisement placement. The
    agent (which is a neural network algorithm) receives state information (in this
    case, the current website the user is on), which it uses to choose which of several
    advertisements it should place at the checkout step. Users will click on the advertisement
    or not, resulting in reward signals that get relayed back to the agent for learning.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5\. 广告放置的上下文老虎机概述。代理（这是一个神经网络算法）接收状态信息（在这种情况下，用户当前所在的网站），它使用这些信息来选择在结账步骤放置几个广告中的哪一个。用户会点击广告或不点击，从而产生奖励信号，这些信号被传回代理以供学习。
- en: '![](02fig05.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig05.jpg)'
- en: '|  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: A *state* in a game (or in a reinforcement learning problem more generally)
    is the set of information available in the environment that can be used to make
    decisions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏中（或更普遍地说，在强化学习问题中）的**状态**是环境中可用于做出决策的信息集合。
- en: '|  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 2.3.2\. States, actions, rewards
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 状态、动作、奖励
- en: Before we move on, let’s consolidate some of the terms and concepts we’ve introduced
    so far. Reinforcement learning algorithms attempt to model the world in a way
    that computers can understand and calculate. In particular, RL algorithms model
    the world as if it merely involved a set of *states, S* (state space), which are
    a set of features about the environment, a set of *actions, A* (action space),
    that can be taken in a given state, and *rewards, r*, that are given for taking
    an action in a specific state. When we speak of taking a particular action in
    a particular state, we often call it a *state-action pair* (*s*, *a*).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们巩固一下我们迄今为止介绍的一些术语和概念。强化学习算法试图以计算机可以理解和计算的方式模拟世界。特别是，RL算法将世界模拟为仅涉及一组*状态，S*（状态空间），这是关于环境的特征集合，一组*动作，A*（动作空间），在给定状态下可以采取的动作，以及*奖励，r*，在特定状态下采取动作时给出的奖励。当我们提到在特定状态下采取特定动作时，我们通常称之为*状态-动作对*（*s*，*a*）。
- en: '|  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The objective of any RL algorithm is to maximize the rewards over the course
    of an entire episode.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 任何RL算法的目标是在整个剧集过程中最大化奖励。
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Since our original *n*-armed bandit problem did not have a state space, only
    an action space, we just needed to learn the relationship between actions and
    rewards. We learned the relationship by using a lookup table to store our experience
    of receiving rewards for particular actions. We stored action-reward pairs (*a[k]*,
    *r[k]*) where the reward at play *k* was an average over all past plays associated
    with taking action *a[k]*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的原始*n*-armed bandit问题没有状态空间，只有动作空间，我们只需要学习动作和奖励之间的关系。我们通过使用查找表来存储我们对特定动作获得奖励的经验来学习这种关系。我们存储动作-奖励对（*a[k]*，*r[k]*），其中在游戏*k*中获得的奖励是所有与采取动作*a[k]*相关的过去游戏的平均值。
- en: In our *n*-armed bandit problem, we only had 10 actions, so a lookup table of
    10 rows was very reasonable. But when we introduce a state space with contextual
    bandits, we start to get a combinatorial explosion of possible state-action-reward
    tuples. For example, if we have a state space of 100 states, and each state is
    associated with 10 actions, we have 1,000 different pieces of data we need to
    store and recompute. In most of the problems we’ll consider in this book, the
    state space is intractably large, so a simple lookup table is not feasible.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的*n*-armed bandit问题中，我们只有10个动作，所以一个10行的查找表是非常合理的。但是当我们引入具有上下文的bandit状态空间时，我们开始得到可能的州-动作-奖励元组的组合爆炸。例如，如果我们有一个包含100个状态的州空间，并且每个状态都与10个动作相关联，我们就需要存储和重新计算1,000种不同的数据。在本书中我们将考虑的大多数问题中，状态空间是难以处理的，所以简单的查找表是不可行的。
- en: That’s where deep learning comes in. When they’re properly trained, neural networks
    are great at learning abstractions that get rid of the details of little value.
    They can learn composable patterns and regularities in data such that they can
    effectively compress a large amount of data while retaining the important information.
    Hence, neural networks can be used to learn complex relationships between state-action
    pairs and rewards without us having to store all such experiences as raw memories.
    We often call the part of an RL algorithm that makes the decisions based on some
    information the *agent*. In order to solve the contextual bandit we’ve been discussing,
    we’ll employ a neural network as our agent.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习发挥作用的地方。当它们得到适当的训练时，神经网络擅长学习抽象，去除无价值细节。它们可以学习数据中的可组合模式和规律性，从而有效地压缩大量数据同时保留重要信息。因此，神经网络可以用来学习状态-动作对和奖励之间的复杂关系，而无需我们将所有这些经验作为原始记忆存储。我们通常称RL算法中基于某些信息做出决策的部分为*代理*。为了解决我们一直在讨论的上下文bandit问题，我们将使用神经网络作为我们的代理。
- en: First, though, we will take a moment to introduce PyTorch, the deep learning
    framework we will be using throughout this book to build neural networks.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将花一点时间介绍PyTorch，这是我们将在整本书中使用来构建神经网络的深度学习框架。
- en: 2.4\. Building networks with PyTorch
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 使用PyTorch构建网络
- en: There are many deep learning frameworks available today, with TensorFlow, MXNet,
    and PyTorch probably being the most popular. We chose to use PyTorch for this
    book because of its simplicity. It allows you to write native-looking Python code
    and still get all the goodies of a good framework like automatic differentiation
    and built-in optimization. We’ll give you a quick introduction to PyTorch here,
    but we’ll explain more as we go along. If you need to brush up on basic deep learning,
    see the appendix where we have a fairly detailed review of deep learning and more
    thorough coverage of PyTorch.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多深度学习框架可供选择，TensorFlow、MXNet 和 PyTorch 可能是最受欢迎的。我们选择使用 PyTorch 编写本书，因为它简单易用。它允许你编写看起来像原生
    Python 代码，同时还能获得良好框架的所有优点，如自动微分和内置优化。在这里，我们将快速介绍 PyTorch，但随着我们的深入，我们将提供更多解释。如果你需要复习基本深度学习知识，请参阅附录，其中我们对深度学习进行了相当详细的回顾，并对
    PyTorch 进行了更全面的介绍。
- en: 'If you’re comfortable with the numpy multidimensional array, you can replace
    almost everything you do with numpy with PyTorch. For example, here we instantiate
    a 2 × 3 matrix in numpy:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 numpy 多维数组，你可以用 PyTorch 替换几乎所有用 numpy 做的事情。例如，这里我们在 numpy 中实例化一个 2 × 3
    的矩阵：
- en: '[PRE18]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And here is how you instantiate the same matrix with PyTorch:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 PyTorch 实例化相同矩阵的方法：
- en: '[PRE19]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The PyTorch code is basically the same as the numpy version, except in PyTorch
    we call multidimensional arrays *tensors*. Unsurprisingly, this is also the term
    used in TensorFlow and other frameworks, so get used to seeing multidimensional
    arrays referred to as tensors. We can and do refer to the *tensor order*, which
    is basically how many indexed dimensions the tensor has. This gets a little confusing
    because sometimes we speak of the dimension of a vector, in which case we’re referring
    to the length of the vector. But when we speak of the order of a tensor, we mean
    how many indices it has. A vector has one index, meaning every element can be
    “addressed” by a single index value, so it’s an order 1 tensor or 1-tensor for
    short. A matrix has two indices, one for each dimension, so it’s a 2-tensor. Higher
    order tensors can be referred to as a *k-*tensor, where *k* is the order, a non-negative
    integer. On the other end, a single number is a 0-tensor, also called a *scalar*,
    since it has no indices.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 代码基本上与 numpy 版本相同，只是在 PyTorch 中我们称多维数组为 *张量*。不出所料，这也是 TensorFlow 和其他框架中使用的术语，所以请习惯看到多维数组被称为张量。我们可以并且确实会提到
    *张量阶数*，这基本上是指张量有多少个索引维度。这有点令人困惑，因为有时我们谈论向量的维度，在这种情况下，我们指的是向量的长度。但当我们谈论张量的阶数时，我们指的是它有多少个索引。一个向量有一个索引，这意味着每个元素都可以通过单个索引值“寻址”，因此它是一个阶数为
    1 的张量或简称为 1-tensor。一个矩阵有两个索引，每个维度一个，因此它是一个 2-tensor。更高阶的张量可以称为 *k-张量*，其中 *k* 是阶数，一个非负整数。另一方面，一个单独的数字是一个
    0-tensor，也称为 *标量*，因为它没有索引。
- en: 2.4.1\. Automatic differentiation
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 自动微分
- en: 'The most important features of PyTorch that we need and that numpy doesn’t
    offer are automatic differentiation and optimization. Let’s say we want to set
    up a simple linear model to predict some data of interest. We can easily define
    the model using ordinary numpy-like syntax:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中我们需要且 numpy 不提供的重要特性是自动微分和优化。假设我们想要设置一个简单的线性模型来预测一些感兴趣的数据。我们可以很容易地使用普通的
    numpy-like 语法定义模型：
- en: '[PRE20]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You simply supply the `requires_grad=True` argument to PyTorch tensors that
    you want to compute gradients for, and then call the `backward()` method on the
    last node in your computational graph, which will backpropagate gradients through
    all the nodes with `requires_grad=True`. You can then do gradient descent with
    the automatically computed gradients.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需将 `requires_grad=True` 参数提供给 PyTorch 张量，你想要计算其梯度，然后在对计算图中的最后一个节点调用 `backward()`
    方法，这将通过所有带有 `requires_grad=True` 的节点反向传播梯度。然后你可以使用自动计算的梯度进行梯度下降。
- en: 2.4.2\. Building Models
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 构建模型
- en: 'For most of this book, we won’t bother dealing directly with automatically
    computed gradients. Instead we’ll use PyTorch’s `nn` module to easily set up a
    feedforward neural network model and then use the built-in optimization algorithms
    to automatically train the neural network without having to manually specify the
    mechanics of backpropagation and gradient descent. Here’s a simple two-layer neural
    network with an optimizer set up:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们不会直接处理自动计算的梯度。相反，我们将使用 PyTorch 的 `nn` 模块轻松设置前馈神经网络模型，然后使用内置的优化算法自动训练神经网络，而无需手动指定反向传播和梯度下降的机制。以下是一个简单两层神经网络，其中设置了优化器：
- en: '[PRE21]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’ve set up a two-layer model with `ReLU` (rectified linear units) activation
    functions, defined a mean-squared error loss function, and set up an optimizer.
    All we have to do to train this model, given that we have some labeled training
    data, is start a training loop:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置了一个两层模型，使用`ReLU`（修正线性单元）激活函数，定义了一个均方误差损失函数，并设置了一个优化器。在给定一些标记的训练数据的情况下，我们只需要启动一个训练循环来训练这个模型：
- en: '[PRE22]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `x` variable is the input data to the model. The `y_correct` variable is
    a tensor representing the labeled, correct output. We make the prediction using
    the model, calculate the loss, and then compute the gradients using the `backward()`
    method on the last node in the computational graph (which is almost always the
    `loss` function). Then we just run the `step()` method on the optimizer, and it
    will run a single step of gradient descent. If we need to build more complex neural
    network architectures than the sequential model, we can write our own Python class,
    inhereit from PyTorch’s module class, and use that instead:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`变量是模型的输入数据。`y_correct`变量是一个表示标记的正确输出的张量。我们使用模型进行预测，计算损失，然后使用计算图中的最后一个节点（几乎总是损失函数）的`backward()`方法计算梯度。然后我们只需在优化器上运行`step()`方法，它将执行一次梯度下降的单步。如果我们需要构建比顺序模型更复杂的神经网络架构，我们可以编写自己的Python类，从PyTorch的模块类继承，并使用它代替：'
- en: '[PRE23]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That’s all you need to know about PyTorch for now to be productive with it.
    We will discuss a few other bells and whistles as we progress through the book.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你需要了解的PyTorch知识仅此而已，以便能够高效地使用它。随着我们通过本书的进展，我们将讨论一些其他的功能和特性。
- en: 2.5\. Solving contextual bandits
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5. 解决上下文无关赌博机问题
- en: We’ve built a simulated environment for a contextual bandit. The simulator includes
    the state (a number from 0 to 9 representing 1 of the 10 websites in the network),
    reward generation (ad clicks), and a method that chooses an action (which of 10
    ads to serve). The following listing shows the code for the contextual bandit
    environment, but don’t spend much time thinking about it as we want to demonstrate
    how to use it, not how to code it.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为上下文无关赌博机构建了一个模拟环境。模拟器包括状态（一个从0到9的数字，代表网络中的10个网站之一）、奖励生成（广告点击）以及选择动作的方法（要展示哪个广告）。以下列表显示了上下文无关赌博机环境的代码，但请不要花太多时间思考它，因为我们想展示如何使用它，而不是如何编写它。
- en: Listing 2.9\. Contextual bandit environment
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.9. 上下文无关赌博机环境
- en: '[PRE24]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1*** Number of states = number of arms, to keep things simple. Each row
    represents a state and each column an arm.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 状态数量 = 臂的数量，以保持简单。每一行代表一个状态，每一列代表一个臂。'
- en: '***2*** Choosing an arm returns a reward and updates the state.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 选择一个臂会返回一个奖励并更新状态。'
- en: The following code snippet demonstrates how to use the environment. The only
    part we need to build is the agent, which is generally the crux of any RL problem,
    since building an environment usually just involves setting up input/output with
    some data source or plugging into an existing API.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了如何使用环境。我们唯一需要构建的部分是智能体，这通常是任何强化学习问题的核心，因为构建环境通常只是设置一些数据源或连接到现有的API的输入/输出。
- en: '[PRE25]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The simulator consists of a simple Python class called `ContextBandit` that
    can be initialized to a specific number of arms. For simplicity, the number of
    states equals the number of arms, but in general the state space is often much
    larger than the action space. The class has two methods: One is `get_state(),`
    which is called with no arguments and will return a state sampled randomly from
    a uniform distribution. In most problems your state will come from a much more
    complex distribution. Calling the other method, `choose_arm(...)`, will simulate
    placing an advertisement, and it returns a reward (e.g., proportional to the number
    of ad clicks). We need to always call `get_state` and then `choose_arm`, in that
    order, to continually get new data to learn from.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器由一个简单的Python类`ContextBandit`组成，可以初始化为特定的臂的数量。为了简单起见，状态的数量等于臂的数量，但在一般情况下，状态空间通常比动作空间大得多。该类有两个方法：一个是`get_state()`，它不需要任何参数，将返回从均匀分布中随机采样的状态。在大多数问题中，你的状态将来自一个更复杂的分布。调用另一个方法`choose_arm(...)`将模拟放置广告，并返回一个奖励（例如，与广告点击数成比例）。我们需要始终按顺序调用`get_state`和`choose_arm`，以不断获取新的数据来学习。
- en: The `ContextBandit` module also includes a few helper functions, such as the
    `softmax` function and a *one-hot encoder*. A one-hot encoded vector is a vector
    where all but 1 element is set to 0\. The only nonzero element is set to 1 and
    indicates a particular state in the state space.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`ContextBandit` 模块还包括一些辅助函数，例如 `softmax` 函数和一个 *one-hot encoder*。一个one-hot编码向量是一个除了1个元素外其余都设置为0的向量。唯一的非零元素被设置为1，并指示状态空间中的一个特定状态。'
- en: Rather than using a single static reward probability distribution over *n* actions,
    like our original bandit problem, the contextual bandit simulator sets up a different
    reward distribution over the actions for each state. That is, we will have *n*
    different softmax reward distributions over actions for each of *n* states. Hence,
    we need to learn the relationship between the states and their respective reward
    distributions, and then learn which action has the highest probability for a given
    state.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们最初的单臂老虎机问题不同，上下文老虎机模拟器为每个状态设置了不同的奖励分布，而不是使用单一的静态奖励概率分布来覆盖 *n* 个动作。也就是说，我们将为每个
    *n* 个状态拥有 *n* 个不同的softmax奖励分布。因此，我们需要学习状态与其相应奖励分布之间的关系，然后学习在给定状态下哪个动作的概率最高。
- en: As with all of our projects in this book, we’ll be using PyTorch to build the
    neural network. In this case, we’re going to build a two-layer feedforward neural
    network that uses rectified linear units (ReLU) as the activation function. The
    first layer accepts a 10-element one-hot (also known as 1-of-K, where all elements
    but one are 0) encoded vector of the state, and the final layer returns a 10-element
    vector representing the predicted reward for each action given the state.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中我们所有的项目一样，我们将使用 PyTorch 来构建神经网络。在这种情况下，我们将构建一个使用ReLU（修正线性单元）作为激活函数的两层前馈神经网络。第一层接受一个10个元素的one-hot（也称为1-of-K，其中除了一个元素外所有元素都是0）编码的状态向量，而最终层返回一个10个元素的向量，表示给定状态下每个动作的预测奖励。
- en: '[Figure 2.6](#ch02fig06) shows the forward pass of the algorithm we’ve described.
    Unlike the lookup table approach, our neural network agent will learn to predict
    the rewards that each action will result in for a given state. Then we use the
    softmax function to give us a probability distribution over the actions and sample
    from this distribution to choose an arm (advertisement). Choosing an arm will
    give us a reward, which we will use to train our neural network.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.6](#ch02fig06) 展示了我们描述的算法的前向传递。与查找表方法不同，我们的神经网络代理将学会预测给定状态下每个动作将产生的奖励。然后我们使用softmax函数来给出动作的概率分布，并从这个分布中采样以选择一个臂（广告）。选择一个臂将给我们一个奖励，我们将使用这个奖励来训练我们的神经网络。'
- en: Figure 2.6\. A computational graph for a simple 10-armed contextual bandit.
    The `get_state()` function returns a state value, which is transformed into a
    one-hot vector that becomes the input data for a two-layer neural network. The
    output of the neural network is the predicted reward for each possible action,
    which is a dense vector that is run through a softmax to sample an action from
    the resulting probability distribution over the actions. The chosen action will
    return a reward and updates the state of the environment. θ[1] and θ[2] represent
    the weight parameters for each layer. The ![](01fig0n.jpg), ![](01fig0r.jpg),
    and ![](01fig0p.jpg) symbols denote the natural numbers (0, 1, 2, 3, ...), the
    real numbers (a floating-point number, for our purposes), and a probability, respectively.
    The superscript indicates the length of the vector, so ![](01fig0p.jpg)^(10) represents
    a 10-element vector where each element is a probability (such that all the elements
    sum to 1).
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6。一个简单的10臂上下文老虎机的计算图。`get_state()` 函数返回一个状态值，该值被转换成一个one-hot向量，成为两层神经网络的输入数据。神经网络的输出是每个可能动作的预测奖励，这是一个密集向量，通过softmax来从动作的概率分布中采样一个动作。所选动作将返回一个奖励并更新环境的状态。θ[1]
    和 θ[2] 代表每层的权重参数。符号 ![](01fig0n.jpg)、![](01fig0r.jpg) 和 ![](01fig0p.jpg) 分别表示自然数（0,
    1, 2, 3, ...）、实数（一个浮点数，对我们来说）和一个概率。上标表示向量的长度，因此 ![](01fig0p.jpg)^(10) 表示一个10个元素的向量，其中每个元素是一个概率（这样所有元素的总和为1）。
- en: '![](02fig06_alt.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig06_alt.jpg)'
- en: Initially our neural network will produce a random vector such as `[1.4, 50,
    4.3, 0.31, 0.43, 11, 121, 90, 8.9, 1.1]` when in state 0\. We will run softmax
    over this vector and sample an action, most likely action 6 (from actions 0 through
    9), since that is the biggest number in the example vector. Choosing action 6
    will generate a reward of say 8\. Then we train our neural network to produce
    the vector `[1.4, 50, 4.3, 0.31, 0.43, 11, 8, 90, 8.9, 1.1],` since that is the
    true reward we received for action 6, leaving the rest of the values unchanged.
    The next time when the neural network sees state 0, it will produce a reward prediction
    for action 6 closer to 8\. As we continually do this over many states and actions,
    the neural network will eventually learn to predict accurate rewards for each
    action given a state. Thus, our algorithm will be able to choose the best action
    each time, maximizing our rewards.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，我们的神经网络将在状态0时产生一个随机向量，例如`[1.4, 50, 4.3, 0.31, 0.43, 11, 121, 90, 8.9, 1.1]`。我们将对这个向量运行softmax并采样一个动作，最可能的选择是动作6（从动作0到9），因为这是示例向量中的最大数字。选择动作6将产生一个奖励，比如8。然后我们训练我们的神经网络产生向量`[1.4,
    50, 4.3, 0.31, 0.43, 11, 8, 90, 8.9, 1.1]`，因为这是我们为动作6收到的真实奖励，其余的值保持不变。下一次当神经网络看到状态0时，它将为动作6产生一个更接近8的奖励预测。随着我们在许多状态和动作上不断这样做，神经网络最终将学会根据状态预测每个动作的准确奖励。因此，我们的算法将能够每次选择最佳动作，最大化我们的奖励。
- en: 'The following code imports the necessary libraries and sets up some *hyperparameters*
    (parameters to specify model structure):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码导入必要的库并设置了一些*超参数*（用于指定模型结构的参数）：
- en: '[PRE26]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding code, `N` is the batch size, `D_in` is the input dimension,
    `H` is the hidden dimension, and `D_out` is the output dimension.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`N`是批量大小，`D_in`是输入维度，`H`是隐藏维度，`D_out`是输出维度。
- en: Now we need to set up our neural network model. It is a simple sequential (feedforward)
    neural network with two layers as we described earlier.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要设置我们的神经网络模型。它是一个简单的顺序（前馈）神经网络，与我们之前描述的一样，包含两层。
- en: '[PRE27]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’ll use the mean squared error loss function here, but others could work too.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用均方误差损失函数，但其他函数也可能适用。
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now we set up a new environment by instantiating the `ContextBandit` class,
    supplying the number of arms to its constructor. Remember, we’ve set up the environment
    such that the number of arms will be equal to the number of states.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过实例化`ContextBandit`类并为其构造函数提供臂的数量来设置一个新的环境。记住，我们已经设置了环境，使得臂的数量将与状态的数量相等。
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The main `for` loop of the algorithm will be very similar to our original *n*-armed
    bandit algorithm, but we have added the step of running a neural network and using
    the output to select an action. We’ll define a function called `train` (shown
    in [listing 2.10](#ch02ex10)) that accepts the environment instance we created
    previously, the number of epochs we want to train for, and the learning rate.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的`main`循环将与我们的原始*n*-armed bandit算法非常相似，但我们增加了一步，即运行一个神经网络并使用其输出选择一个动作。我们将定义一个名为`train`的函数（如[代码清单2.10](#ch02ex10)所示），该函数接受我们之前创建的环境实例、我们想要训练的epoch数和学习率。
- en: 'In the function, we’ll set a PyTorch variable for the current state, which
    we’ll need to one-hot encode using the `one_hot(...)` encoding function:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数中，我们将为当前状态设置一个PyTorch变量，我们需要使用`one_hot(...)`编码函数对其进行one-hot编码：
- en: '[PRE30]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Once we enter the main training `for` loop, we’ll run our neural network model
    with the randomly initialized current state vector. It will return a vector that
    represents its guess for the values of each of the possible actions. At first,
    the model will output a bunch of random values since it is not trained.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进入主训练`for`循环，我们将使用随机初始化的当前状态向量运行我们的神经网络模型。它将返回一个表示其对每个可能动作值猜测的向量。最初，由于没有经过训练，模型将输出一些随机值。
- en: We’ll run the softmax function over the model’s output to generate a probability
    distribution over the actions. We’ll then select an action using the environment’s
    `ch``oose_arm(...)` function, which will return the reward generated for taking
    that action; it will also update the environment’s current state. We’ll turn the
    reward (which is a non-negative integer) into a one-hot vector that we can use
    as our training data. We’ll then run one step of backpropagation with this reward
    vector, given the state we gave the model. Since we’re using a neural network
    model as our action-value function, we no longer have any sort of action-value
    array storing “memories;” everything is being encoded in the neural network’s
    weight parameters. The whole `train` function is shown in the following listing.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在模型的输出上运行 softmax 函数，以生成一个关于动作的概率分布。然后我们将使用环境的 `choose_arm(...)` 函数选择一个动作，该函数将返回执行该动作产生的奖励；它还将更新环境的当前状态。我们将奖励（它是一个非负整数）转换为一个
    one-hot 向量，我们可以将其用作我们的训练数据。然后我们将使用我们给模型的当前状态运行一个反向传播步骤，给定这个奖励向量。由于我们使用神经网络模型作为我们的动作值函数，我们不再有任何存储“记忆”的动作值数组；一切都被编码在神经网络的权重参数中。整个
    `train` 函数如下所示。
- en: Listing 2.10\. The main training loop
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.10\. 主要训练循环
- en: '[PRE31]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '***1*** Gets current state of the environment; converts to PyTorch variable'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 获取环境当前状态；将其转换为 PyTorch 变量'
- en: '***2*** Runs neural net forward to get reward predictions'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 运行神经网络正向以获取奖励预测'
- en: '***3*** Converts reward predictions to probability distribution with softmax'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将奖励预测转换为 softmax 概率分布'
- en: '***4*** Normalizes distribution to make sure it sums to 1'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 归一化分布以确保其总和为 1'
- en: '***5*** Chooses new action probabilistically'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 以概率选择新的动作'
- en: '***6*** Takes action, receives reward'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 执行动作，接收奖励'
- en: '***7*** Converts PyTorch tensor data to numpy array'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将 PyTorch 张量数据转换为 numpy 数组'
- en: '***8*** Updates one_hot_reward array to use as labeled training data'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 更新 one_hot_reward 数组以用作标记的训练数据'
- en: '***9*** Updates current environment state'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 更新当前环境状态'
- en: Go ahead and run this function. When we train this network for 5,000 epochs,
    we can plot the moving average of rewards earned over the training time (see [figure
    2.7](#ch02fig07); we omitted the code to produce such a graph). Our neural network
    indeed learns a fairly good understanding of the relationship between states,
    actions and rewards for this contextual bandit. The maximum reward payout for
    any play is 10, and our average is topping off around 8.5, which is close to the
    mathematical optimum for this particular bandit. Our first deep reinforcement
    learning algorithm works! Okay, it’s not a very deep network, but still!
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个函数。当我们对这个网络训练 5,000 个周期时，我们可以绘制训练时间内获得的奖励的移动平均值（见[图 2.7](#ch02fig07)；我们省略了生成此类图表的代码）。我们的神经网络确实学会了关于状态、动作和奖励之间关系的一个相当好的理解，对于这个上下文赌博。任何一次游戏的最高奖励支付为
    10，我们的平均值大约为 8.5，这接近于这个特定赌博的数学最优值。我们的第一个深度强化学习算法起作用了！好吧，这不是一个非常深的网络，但仍然！
- en: Figure 2.7\. A training graph showing the average rewards for playing the contextual
    bandit simulator using a two-layer neural network as the action-value function.
    We can see the average reward rapidly increases during training time, demonstrating
    our neural network is successfully learning.
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.7\. 使用两层神经网络作为动作值函数，展示使用上下文赌博模拟器玩游戏的平均奖励的训练图。我们可以看到，在训练时间内平均奖励迅速增加，这表明我们的神经网络正在成功学习。
- en: '![](02fig07_alt.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig07_alt.jpg)'
- en: 2.6\. The Markov property
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6\. 马尔可夫性质
- en: In our contextual bandit problem, our neural network led us to choose the best
    action given a state without reference to any other prior states. We just gave
    it the current state, and it produced the expected rewards for each possible action.
    This is an important property in reinforcement learning called the *Markov property*.
    A game (or any other control task) that exhibits the Markov property is said to
    be a *Markov decision process* (MDP). With an MDP, the current state alone contains
    enough information to choose optimal actions to maximize future rewards. Modeling
    a control task as an MDP is a key concept in reinforcement learning.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的上下文赌博问题中，我们的神经网络引导我们根据当前状态选择最佳动作，而不参考任何其他先前状态。我们只给它当前状态，它就为每个可能的动作产生了预期的奖励。这是强化学习中一个重要的性质，称为*马尔可夫性质*。如果一个游戏（或任何其他控制任务）表现出马尔可夫性质，那么它被称为*马尔可夫决策过程*（MDP）。在MDP中，当前状态本身就包含了足够的信息来选择最优动作以最大化未来的奖励。将控制任务建模为MDP是强化学习中的一个关键概念。
- en: The MDP model simplifies an RL problem dramatically, as we do not need to take
    into account all previous states or actions—we don’t need to have memory, we just
    need to analyze the present situation. Hence, we always attempt to model a problem
    as (at least approximately) a Markov decision processes. The card game Blackjack
    (also known as 21) is an MDP because we can play the game successfully just by
    knowing our current state (what cards we have, and the dealer’s one face-up card).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: MDP模型极大地简化了强化学习问题，因为我们不需要考虑所有先前的状态或动作——我们不需要记忆，我们只需要分析当前的情况。因此，我们总是试图将问题建模为（至少是近似地）马尔可夫决策过程。纸牌游戏21点（也称为21点）是一个MDP，因为我们只需要知道我们的当前状态（我们有什么牌，以及庄家的明牌）就能成功玩游戏。
- en: 'To test your understanding of the Markov property, consider each control problem
    or decision task in the following list and see if it has the Markov property or
    not:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试你对马尔可夫性质的理解，考虑以下列表中的每个控制问题或决策任务，看看它是否具有马尔可夫性质：
- en: Driving a car
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶汽车
- en: Deciding whether to invest in a stock or not
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定是否投资股票
- en: Choosing a medical treatment for a patient
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为患者选择治疗方案
- en: Diagnosing a patient’s illness
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断患者的疾病
- en: Predicting which team will win in a football game
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测哪支足球队会在足球比赛中获胜
- en: Choosing the shortest route (by distance) to some destination
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择到某个目的地的最短路线（按距离）
- en: Aiming a gun to shoot a distant target
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用枪瞄准射击远距离目标
- en: 'Okay, let’s see how you did. Here are our answers and brief explanations:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们看看你的表现。以下是我们的答案和简要解释：
- en: Driving a car can generally be considered to have the Markov property because
    you don’t need to know what happened 10 minutes ago to be able to optimally drive
    your car. You just need to know where everything is right now and where you want
    to go.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶汽车通常可以被认为是具有马尔可夫性质的，因为你不需要知道10分钟前发生了什么，就能最优地驾驶你的车。你只需要知道现在一切的位置以及你想去的地方。
- en: Deciding whether to invest in a stock or not does not meet the criteria of the
    Markov property since you would want to know the past performance of the stock
    in order to make a decision.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定是否投资股票并不符合马尔可夫性质的准则，因为你需要了解股票的过去表现来做出决定。
- en: Choosing a medical treatment seems to have the Markov property because you don’t
    need to know the biography of a person to choose a good treatment for what ails
    them right now.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为患者选择治疗方案似乎具有马尔可夫性质，因为你不需要知道一个人的生平就能为他们现在的疾病选择一个好的治疗方案。
- en: In contrast, *diagnosing* (rather than treating) would definitely require knowledge
    of past states. It is often very important to know the historical course of a
    patient’s symptoms in order to make a diagnosis.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比之下，*诊断*（而不是治疗）肯定需要了解过去的状态。了解患者症状的历史过程对于做出诊断通常非常重要。
- en: Predicting which football team will win does not have the Markov property, since,
    like the stock example, you need to know the past performance of the football
    teams to make a good prediction.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测哪支足球队会赢并不具有马尔可夫性质，因为，就像股票的例子一样，你需要了解足球队过去的性能来做出良好的预测。
- en: Choosing the shortest route to a destination has the Markov property because
    you just need to know the distance to the destination for various routes, which
    doesn’t depend on what happened yesterday.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择到目的地的最短路线具有马尔可夫性质，因为你只需要知道不同路线到目的地的距离，这并不取决于昨天发生了什么。
- en: Aiming a gun to shoot a distant target also has the Markov property since all
    you need to know is where the target is, and perhaps current conditions like wind
    velocity and the particulars of your gun. You don’t need to know the wind velocity
    of yesterday.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用枪瞄准射击远距离目标也具有马尔可夫性质，因为你只需要知道目标在哪里，以及可能的风速和你的枪的细节。你不需要知道昨天的风速。
- en: 'We hope you can appreciate that for some of those examples you could make arguments
    for or against it having the Markov property. For example, in diagnosing a patient,
    you may need to know the recent history of their symptoms, but if that is documented
    in their medical record and we consider the full medical record as our current
    state, then we’ve effectively induced the Markov property. This is an important
    thing to keep in mind: many problems may not *naturally* have the Markov property,
    but often we can induce it by jamming more information into the state.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你能理解，对于一些例子，你可以为它是否具有马尔可夫属性提出论点。例如，在诊断患者时，你可能需要知道他们症状的近期历史，但如果这些信息记录在他们的病历中，并且我们将完整的病历视为我们的当前状态，那么我们就有效地诱导了马尔可夫属性。这一点很重要：许多问题可能并不
    *自然地* 具有马尔可夫属性，但通常我们可以通过将更多信息纳入状态来诱导它。
- en: DeepMind’s deep Q-learning (or deep Q-network) algorithm learned to play Atari
    games from just raw pixel data and the current score. Do Atari games have the
    Markov property? Not exactly. In the game Pacman, if our state is the raw pixel
    data from our current frame, we have no idea if the enemy a few tiles away is
    approaching us or moving away from us, and that would strongly influence our choice
    of actions to take. This is why DeepMind’s implementation actually feeds in the
    last four frames of gameplay, effectively changing a non-MDP into an MDP. With
    the last four frames, the agent has access to the direction and speed of all players.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind 的深度 Q 学习（或深度 Q 网络）算法通过仅使用原始像素数据和当前得分来学习玩 Atari 游戏。Atari 游戏具有马尔可夫属性吗？并不完全如此。在
    Pacman 游戏中，如果我们当前的状态是当前帧的原始像素数据，我们就无法知道几格之外的敌人是向我们靠近还是远离我们，而这会强烈影响我们选择采取的行动。这就是为什么
    DeepMind 的实现实际上输入了最后四帧的游戏画面，有效地将一个非 MDP 转换成了一个 MDP。有了最后四帧，智能体可以访问所有玩家的方向和速度。
- en: '[Figure 2.8](#ch02fig08) gives a lighthearted example of a Markov decision
    process using all the concepts we’ve discussed so far. You can see there is a
    three-element state space `S = {crying baby, sleeping baby, smiling baby}`, and
    a two-element action space `A = {feed, don’t feed}`. In addition, we have transition
    probabilities noted, which are maps from an action to the probability of an outcome
    state (we’ll go over this again in the next section). Of course, in real life,
    you as the *agent* have no idea what the transition probabilities are. If you
    did, you would have a *model* of the environment. As you’ll learn later, sometimes
    an agent does have access to a model of the environment, and sometimes not. In
    the cases where the agent does not have access to the model, we may want our agent
    to learn a model of the environment (which may just approximate the true, underlying
    model).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.8](#ch02fig08) 给出了一个轻松的例子，展示了如何使用我们迄今为止讨论的所有概念来构建马尔可夫决策过程。你可以看到有三个元素的状态空间
    `S = {哭泣的婴儿，睡着的婴儿，微笑的婴儿}` 和一个两个元素的动作空间 `A = {喂食，不喂食}`。此外，我们还标注了转移概率，这些是从动作到结果状态概率的映射（我们将在下一节中再次讨论这一点）。当然，在现实生活中，作为
    *智能体* 的你并不知道这些转移概率。如果你知道了，你将拥有环境的 *模型*。正如你将学到的，有时智能体确实可以访问环境的模型，有时则不行。在智能体无法访问模型的情况下，我们可能希望我们的智能体学习环境的模型（这可能只是对真实、潜在模型的近似）。'
- en: Figure 2.8\. A simplified MDP diagram with three states and two actions. Here
    we model the parenting decision process for taking care of an infant. If the baby
    is crying, we can either administer food or not, and with some probability the
    baby will transition into a new state, and we’ll receive a reward of -1, +1, or
    +2 (based on the baby’s satisfaction).
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.8. 一个具有三个状态和两个动作的简化 MDP 图。在这里，我们模拟了照顾婴儿的育儿决策过程。如果婴儿在哭泣，我们可以选择喂食或不喂食，并且有一定概率婴儿会过渡到新的状态，我们将会收到
    -1、+1 或 +2（根据婴儿的满意度）的奖励。
- en: '![](02fig08.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8](02fig08.jpg)'
- en: '2.7\. Predicting future rewards: Value and policy functions'
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7. 预测未来奖励：值函数和策略函数
- en: Believe it or not, we actually smuggled a lot of knowledge into the previous
    sections. The way we set up our solutions to the *n*-armed bandit and contextual
    bandit are standard reinforcement learning methods, and as such, there is a whole
    bunch of established terminology and mathematics behind what we did. We introduced
    a few terms already, such as state and action spaces, but we mostly just described
    things in natural language. In order for you to understand the latest RL research
    papers and for us to make future chapters less verbose, it’s important to become
    acquainted with the jargon and the mathematics.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，我们实际上已经在前面的章节中巧妙地融入了很多知识。我们对n臂老虎机和上下文老虎机问题的解决方案是标准的强化学习方法，因此，我们所做的一切背后有一整套已建立的术语和数学。我们已经介绍了一些术语，例如状态空间和动作空间，但我们主要只是用自然语言描述了这些内容。为了让你理解最新的强化学习研究论文，以及让我们将未来的章节写得更加简洁，了解这些术语和数学是非常重要的。
- en: Let’s review and formalize what you’ve learned so far (summarized in [figure
    2.9](#ch02fig09)). A reinforcement learning algorithm essentially constructs an
    *agent,* which acts in some *environment*. The environment is often a game, but
    is more generally whatever process produces states, actions, and rewards. The
    agent has access to the current state of the environment, which is all the data
    about the environment at a particular time point, *s[t]* ε *S*. Using this state
    information, the agent takes an action, *a[t]* ε *A*, which may deterministically
    or probabilistically change the environment to be in a new state, *s[t]* [+1].
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾和形式化你到目前为止所学的知识（总结在[图2.9](#ch02fig09)中）。强化学习算法本质上构建了一个**代理**，它在某个**环境**中行动。环境通常是游戏，但更一般地说，是任何产生状态、动作和奖励的过程。代理可以访问环境的当前状态，这是在特定时间点关于环境的所有数据，即s[t]
    ε *S*。使用这些状态信息，代理采取一个动作，a[t] ε *A*，这个动作可能确定性地或以概率改变环境，使其处于新状态，s[t] [+1]。
- en: Figure 2.9\. The general process of a reinforcement learning algorithm. The
    environment produces states and rewards. The agent takes an action, *a[t]*, given
    a state, *s[t]*, at time *t* and receives a reward, *r[t]*. The agent’s goal is
    to maximize rewards by learning to take the best actions in a given state.
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9. 强化学习算法的一般过程。环境产生状态和奖励。代理在时间t给定状态s[t]时采取一个动作a[t]，并接收一个奖励r[t]。代理的目标是通过学习在给定状态下采取最佳动作来最大化奖励。
- en: '![](02fig09_alt.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9的替代图片](02fig09_alt.jpg)'
- en: The probability associated with mapping a state to a new state by taking an
    action is called the *transition probability*. The agent receives a reward, *r[t]*,
    for having taken action *a[t]* in state *s[t]* leading to a new state, *s[t]*
    [+1]. And we know that the ultimate goal of the agent (our reinforcement learning
    algorithm) is to maximize its rewards. It’s really the state transition, *s[t]*
    → *s[t]* [+1], that produces the reward, not the action per se, since the action
    may probabilistically lead to a bad state. If you’re in an action movie (no pun
    intended) and you jump off a roof onto another roof, you may land gracefully on
    the other roof or miss it completely and fall—your peril is what’s important (the
    two possible resulting states), not the fact that you jumped (the action).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采取一个动作将一个状态映射到新状态的概率称为**转移概率**。代理在状态s[t]中采取动作a[t]，导致新状态s[t] [+1]，从而获得奖励r[t]。我们知道代理（我们的强化学习算法）的最终目标是最大化其奖励。实际上产生奖励的是状态转移，即s[t]
    → s[t] [+1]，而不是动作本身，因为动作可能以概率导致一个坏状态。如果你在一场动作电影中（无意中用了双关语）从屋顶跳到另一座屋顶，你可能优雅地落在另一座屋顶上，或者完全错过并摔下来——你的危险（两种可能的结果状态）比你跳了（动作）更重要。
- en: 2.7.1\. Policy functions
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.1. 策略函数
- en: How exactly do we use our current state information to decide what action to
    take? This is where the key concepts of *value functions* and *policy functions*
    come into play, which we already have a bit of experience with. Let’s first tackle
    policies.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们究竟如何使用当前状态信息来决定采取什么动作？这正是**值函数**和**策略函数**的关键概念发挥作用的地方，我们对这些概念已经有一些经验了。让我们首先解决策略问题。
- en: In words, a policy, π, is the strategy of an agent in some environment. For
    example, the strategy of the dealer in Blackjack is to always hit until they reach
    a card value of 17 or greater. It’s a simple fixed strategy. In our *n*-armed
    bandit problem, our policy was an epsilon-greedy strategy. In general, a policy
    is a function that maps a state to a probability distribution over the set of
    possible actions in that state.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，策略，π，是智能体在某个环境中的策略。例如，黑杰克游戏中庄家的策略是始终抽牌直到达到17点或更高。这是一个简单的固定策略。在我们的 *n*-armed
    bandit 问题中，我们的策略是ε-贪婪策略。一般来说，策略是一个将状态映射到该状态下可能动作的概率分布的函数。
- en: Table 2.5\. The policy function
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.5\. 策略函数
- en: '| Math | English |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 英语 |'
- en: '| --- | --- |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| π; *s* → *Pr*(*A*∣*s*), where *s* ∈ *S* | A policy, π, is a mapping from
    states to the (probabilistically) best actions for those states. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| π; *s* → *Pr*(*A*|*s*), 其中 *s* ∈ *S* | 策略，π，是从状态到该状态下（概率上）最佳动作的映射。 |'
- en: In the mathematical notation, *s* is a state and *Pr*(*A* | *s*) is a probability
    distribution over the set of actions *A*, given state *s*. The probability of
    each action in the distribution is the probability that the action will produce
    the greatest reward.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，*s* 是一个状态，*Pr*(*A* | *s*) 是给定状态 *s* 的动作集合 *A* 的概率分布。分布中每个动作的概率是该动作产生最大奖励的概率。
- en: 2.7.2\. Optimal policy
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.2\. 最优策略
- en: The policy is the part of our reinforcement learning algorithm that chooses
    actions given its current state. We can then formulate the *optimal policy*—it’s
    the strategy that maximizes rewards.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是我们强化学习算法的一部分，它根据当前状态选择动作。然后我们可以制定*最优策略*——它是最大化奖励的策略。
- en: Table 2.6\. The optimal policy
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.6\. 最优策略
- en: '| Math | English |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 英语 |'
- en: '| --- | --- |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *π** = arg*max E*(*R*&#124;*π*), | If we know the expected rewards for following
    any possible policy, *π*, the optimal policy, *π**, is a policy that, when followed,
    produces the maximum possible rewards. |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| *π** = arg*max E*(*R*|*π*), | 如果我们知道遵循任何可能策略，*π*，的期望奖励，那么最优策略，*π**，是一个策略，当遵循时，会产生最大可能的奖励。
    |'
- en: Remember, a particular policy is a map or function, so we have some sort of
    set of possible policies; the optimal policy is just an `argmax` (which selects
    the maximum) over this set of possible policies as a function of their expected
    rewards.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，特定的策略是一个映射或函数，因此我们有一组可能的政策；最优策略只是这个可能政策集上的一个 `argmax`（它选择最大值），作为它们期望奖励的函数。
- en: 'Again, the whole goal of a reinforcement learning algorithm (our agent) is
    to choose the actions that lead to the maximal expected rewards. But there are
    two ways we can train our agent to do this:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，强化学习算法（我们的智能体）的整体目标是为了选择能够导致最大期望奖励的动作。但我们可以通过两种方式来训练我们的智能体实现这一目标：
- en: '***Directly—*** We can teach the agent to learn what actions are best, given
    what state it is in.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***直接方式——*** 我们可以教会智能体学习在给定状态下哪些动作是最好的。'
- en: '***Indirectly—*** We can teach the agent to learn which states are most valuable,
    and then to take actions that lead to the most valuable states.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***间接方式——*** 我们可以教会智能体学习哪些状态最有价值，然后采取导致最有价值状态的行动。'
- en: This indirect method leads us to the idea of value functions.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这种间接方法引导我们产生了价值函数的概念。
- en: 2.7.3\. Value functions
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.3\. 价值函数
- en: '*Value functions* are functions that map a state or a state-action pair to
    the *expected value* (the expected reward) of being in some state or taking some
    action in some state. You may recall from statistics that the expected reward
    is just the long-term average of rewards received after being in some state or
    taking some action. When we speak of *the* value function, we usually mean a state-value
    function.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '*价值函数*是将状态或状态-动作对映射到*期望值*（期望奖励）的函数。你可能还记得从统计学中，期望奖励只是处于某个状态或采取某个动作后收到的奖励的长期平均值。当我们提到*价值函数*时，我们通常指的是状态价值函数。'
- en: Table 2.7\. The state-value function
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.7\. 状态价值函数
- en: '| Math | English |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 英语 |'
- en: '| --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *V**[π]*: *s* → *E*(*R*&#124;*s*,*π*), | A value function, *V**[π]*, is a
    function that maps a state, *s*, to the expected rewards, given that we start
    in state *s* and follow some policy, *π*. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| *V**[π]*: *s* → *E*(*R*|*s*,*π*), | 价值函数，*V**[π]*，是一个将状态，*s*，映射到期望奖励的函数，前提是我们从状态
    *s* 开始，并遵循某种策略，*π*。 |'
- en: This is a function that accepts a state, *s*, and returns the expected reward
    of starting in that state and taking actions according to our policy, *π*. It
    may not be immediately obvious why the value function depends on the policy. Consider
    that in our contextual bandit problem, if our policy was to choose entirely random
    actions (i.e., sample actions from a uniform distribution), the value (expected
    reward) of a state would probably be pretty low, since we’re definitely not choosing
    the best possible actions. Instead, we want to use a policy that is not a uniform
    distribution over the actions, but is the probability distribution that would
    produce the maximum rewards when sampled. That is, the policy is what determines
    observed rewards, and the value function is a reflection of observed rewards.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个接受状态*s*并返回从该状态开始并按照我们的策略*π*采取行动的预期奖励的函数。可能不会立即明显为什么值函数依赖于策略。考虑在我们的情境式老虎机问题中，如果我们的策略是完全随机选择行动（即从均匀分布中采样行动），那么状态的价值（预期奖励）可能相当低，因为我们肯定不是选择最佳可能的行动。相反，我们希望使用一个不是行动上的均匀分布的策略，而是当采样时会产生最大奖励的概率分布。也就是说，策略决定了观察到的奖励，而值函数是观察到的奖励的反映。
- en: In our first *n*-armed bandit problem, you were introduced to state-action-value
    functions. These functions often go by the name *Q function* or *Q value*, which
    is where deep Q-learning comes from, since, as you’ll see in the next chapter,
    deep learning algorithms can be used as Q functions.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们第一个*n*-armed老虎机问题中，你被介绍到了状态-行动值函数。这些函数通常被称为*Q函数*或*Q值*，这就是深度Q学习的来源，因为正如你将在下一章中看到的，深度学习算法可以用作Q函数。
- en: Table 2.8\. The action-value (Q) function
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.8. 行动-值（Q）函数
- en: '| Math | English |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 英语 |'
- en: '| --- | --- |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *Q**[π]*: (*s*&#124;*a*) → *E*(*R*&#124;*a*,*s*,*π*), | *Q**[π]* is a function
    that maps a pair, (*s*, *a*), of a state, *s*, and an action, *a*, to the expected
    reward of taking action *a* in state *s*, given that we’re using the policy (or
    “strategy”) *π*. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| *Q**[π]*: (*s*|*a*) → *E*(*R*|*a*,*s*,*π*)， | *Q**[π]*是一个函数，它将状态*s*和行动*a*的配对映射到在状态*s*采取行动*a*的预期奖励，假设我们使用策略（或“策略”）*π*。
    |'
- en: In fact, we sort of implemented a deep Q-network to solve our contextual bandit
    problem (although it was a pretty shallow neural network), since it was essentially
    acting as a Q function. We trained it to produce accurate estimates of the expected
    reward for taking an action given a state. Our policy function was the softmax
    function over the output of the neural network.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们实现了一个深度Q网络来解决我们的情境式老虎机问题（尽管它是一个非常浅层的神经网络），因为它本质上充当了一个Q函数。我们训练它产生给定状态采取行动的预期奖励的准确估计。我们的策略函数是神经网络输出的softmax函数。
- en: We’ve covered many of the foundational concepts in reinforcement learning just
    by using *n*-armed and contextual bandits as examples. We also got our feet wet
    with deep reinforcement learning in this chapter. In the next chapter we’ll implement
    a full-blown deep Q-network similar to the algorithm that DeepMind used to play
    Atari games at superhuman levels. It will be a natural extension of what we’ve
    covered here.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅通过使用*n*-armed和情境式老虎机作为例子，就已经涵盖了强化学习中的许多基础概念。在本章中，我们也尝试了深度强化学习。在下一章中，我们将实现一个类似于DeepMind用于在超级人类水平上玩Atari游戏的算法的全功能深度Q网络。这将是我们在这里所涵盖内容的自然扩展。
- en: Summary
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: State spaces are the set of all possible states a system can be in. In Chess,
    this would be the set of all valid board configurations. An action is a function
    that maps a state, *s*, to a new state, *s*′. An action may be stochastic, such
    that it maps a state, *s*, probabilistically to a new state, *s*′. There may be
    some probability distribution over the set of possible new states from which one
    is selected. The action-space is the set of all possible actions for a particular
    state.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态空间是系统可能处于的所有可能状态的集合。在象棋中，这将是一组所有有效的棋盘配置。一个行动是将状态*s*映射到新状态*s*′的函数。一个行动可能是随机的，这样它就将以概率将状态*s*映射到新状态*s*′。可能存在一个概率分布，它覆盖了可能的新状态的集合，从中选择一个。行动空间是特定状态的所有可能行动的集合。
- en: The environment is the source of states, actions, and rewards. If we’re building
    an RL algorithm to play a game, then the game is the environment. A model of an
    environment is an approximation of the state space, action space, and transition
    probabilities.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是状态、行动和奖励的来源。如果我们正在构建一个用于玩游戏的游戏RL算法，那么游戏就是环境。环境模型是对状态空间、行动空间和转移概率的近似。
- en: Rewards are signals produced by the environment that indicate the relative success
    of taking an action in a given state. An expected reward is a statistical concept
    that informally refers to the long-term average value of some random variable
    *X* (in our case, the reward), denoted *E*[*X*]. For example, in the *n*-armed
    bandit case, *E*[*R*|*a*] (the expected reward given action *a*) is the long-term
    average reward of taking each of the *n*-actions. If we knew the probability distribution
    over the actions, *a*, then we could calculate the precise value of the expected
    reward for a game of *N* plays as ![](pg53.jpg), where *N* is the number of plays
    of the game, *p[i]* refers to the probability of action *a[i]*, and *r* refers
    to the maximum possible reward.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是环境产生的信号，表示在给定状态下采取行动的相对成功。预期奖励是一个统计概念，非正式地指代随机变量 *X*（在我们的案例中，是奖励）的长期平均价值，表示为
    *E*[*X*]。例如，在 *n*-臂老虎机案例中，*E*[*R*|*a*]（给定行动 *a* 的预期奖励）是采取每个 *n*-个行动的长期平均奖励。如果我们知道行动
    *a* 的概率分布，那么我们可以计算出 *N* 次游戏预期奖励的精确值，如 ![](pg53.jpg) 所示，其中 *N* 是游戏的次数，*p[i]* 指的是行动
    *a[i]* 的概率，而 *r* 指的是可能的最大奖励。
- en: An agent is an RL algorithm that learns to behave optimally in a given environment.
    Agents are often implemented as a deep neural network. The goal of the agent is
    to maximize expected rewards, or equivalently, to navigate to the highest value
    state.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理是一种RL算法，它学会在给定环境中以最佳方式行为。代理通常实现为深度神经网络。代理的目标是最大化预期奖励，或者等价地，导航到价值最高的状态。
- en: A policy is a particular strategy. Formally, it’s a function that either accepts
    a state and produces an action to take or produces a probability distribution
    over the action space, given the state. A common policy is the epsilon-greedy
    strategy, where with probability ε we take a random action in the action space,
    and with probability ε – 1 we choose the best action we know of so far.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策是一种特定的策略。正式地说，它是一个函数，要么接受一个状态并产生一个要采取的行动，要么在给定状态的情况下，在动作空间上产生一个概率分布。一个常见的策略是ε-贪婪策略，其中以概率ε我们在动作空间中采取一个随机行动，以概率ε
    – 1我们选择我们迄今为止所知的最佳行动。
- en: In general, a value function is any function that returns expected rewards given
    some relevant data. Without additional context, it typically refers to a state-value
    function, which is a function that accepts a state and returns the expected reward
    of starting in that state and acting according to some policy. The Q value is
    the expected reward given a state-action pair, and the Q function is a function
    that produces Q values when given a state-action pair.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，价值函数是任何返回给定相关数据的预期奖励的函数。在没有额外上下文的情况下，它通常指的是状态价值函数，这是一个接受一个状态并返回从该状态开始并按照某些策略采取行动的预期奖励的函数。Q值是在给定状态-动作对的情况下给出的预期奖励，Q函数是在给定状态-动作对的情况下产生Q值的函数。
- en: The Markov decision process is a decision-making process by which it is possible
    to make the best decisions without reference to a history of prior states.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是一种决策过程，通过它可以在不参考先前状态历史的情况下做出最佳决策。
- en: 'Chapter 3\. Predicting the best states and actions: Deep Q-networks'
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章\. 预测最佳状态和行动：深度Q网络
- en: '*This chapter covers*'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Implementing the Q function as a neural network
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Q函数实现为神经网络
- en: Building a deep Q-network using PyTorch to play Gridworld
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch构建深度Q网络来玩Gridworld
- en: Counteracting catastrophic forgetting with experience replay
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经验回放对抗灾难性遗忘
- en: Improving learning stability with target networks
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目标网络提高学习稳定性
- en: 'In this chapter we’ll start off where the deep reinforcement learning revolution
    began: DeepMind’s deep Q-networks, which learned to play Atari games. We won’t
    be using Atari games as our testbed quite yet, but we will be building virtually
    the same system DeepMind did. We’ll use a simple console-based game called Gridworld
    as our game environment.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从深度强化学习革命开始的地方开始：DeepMind的深度Q网络，它学会了玩Atari游戏。我们目前不会使用Atari游戏作为我们的测试平台，但我们将构建DeepMind所构建的几乎相同的系统。我们将使用一个简单的基于控制台的游戏Gridworld作为我们的游戏环境。
- en: Gridworld is actually a family of similar games, but they all generally involve
    a grid board with a player (or agent), an objective tile (the “goal”), and possibly
    one or more special tiles that may be barriers or may grant negative or positive
    rewards. The player can move up, down, left, or right, and the point of the game
    is to get the player to the goal tile where the player will receive a positive
    reward. The player must not only reach the goal tile but must do so following
    the shortest path, and they may need to navigate through various obstacles.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界实际上是一系列类似的游戏，但它们通常都涉及一个网格板、一个玩家（或智能体）、一个目标方块（“目标”），以及可能的一个或多个特殊方块，这些方块可能是障碍物或可能提供负面或正面的奖励。玩家可以向上、向下、向左或向右移动，游戏的目标是将玩家移动到目标方块，在那里玩家将获得正面的奖励。玩家不仅必须到达目标方块，而且必须遵循最短路径，他们可能需要通过各种障碍。
- en: 3.1\. The Q function
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. Q 函数
- en: We will use a very simple Gridworld engine that’s included in the GitHub repository
    for this book. You can download it at [http://mng.bz/JzKp](http://mng.bz/JzKp)
    in the [Chapter 3](#ch03) folder.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个非常简单的网格世界引擎，该引擎包含在本书的 GitHub 仓库中。您可以在[第 3 章](#ch03)文件夹中下载它：[http://mng.bz/JzKp](http://mng.bz/JzKp)。
- en: The Gridworld game depicted in [figure 3.1](#ch03fig01) shows the simple version
    of Gridworld we’ll start with; we’ll progressively tackle more difficult variants
    of the game. Our initial goal is to train a DRL agent to navigate the Gridworld
    board to the goal, following the most efficient route every time. But before we
    get too far into that, let’s review the key terms and concepts from the previous
    chapter, which we will continue to use here.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.1](#ch03fig01) 中所示的网格世界游戏显示了我们将从其开始的简单版本的网格世界；我们将逐步解决游戏的更复杂变体。我们的初始目标是训练一个深度强化学习智能体，使其能够每次都遵循最有效的路线导航网格世界板到目标。但在我们深入之前，让我们回顾一下前一章中的关键术语和概念，我们将在本章继续使用它们。'
- en: Figure 3.1\. This is a simple Gridworld game setup. The agent (A) must navigate
    along the shortest path to the goal tile (+) and avoid falling into the pit (–).
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.1\. 这是一个简单的网格世界游戏设置。智能体（A）必须沿着最短路径导航到目标方块（+），并避免掉入陷阱（–）。
- en: '![](03fig01.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01.jpg)'
- en: The *state* is the information that our agent receives and uses to make a decision
    about what action to take. It could be the raw pixels of a video game, sensor
    data from an autonomous vehicle, or, in the case of Gridworld, a tensor representing
    the positions of all the objects on the grid.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态* 是我们的智能体接收并用于决定采取什么行动的信息。它可能是视频游戏的原始像素，自动驾驶汽车的传感器数据，或者在网格世界的情况下，表示网格上所有对象位置的张量。'
- en: The *policy*, denoted *π*, is the strategy our agent follows when provided a
    state. For example, a policy in Blackjack might be to look at our hand (the state)
    and hit or stay randomly. Although this would be a terrible policy, the important
    point to stress is that the policy confers which actions we take. A better policy
    would be to always hit until we have 19.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略*，表示为 *π*，是智能体在给定状态时遵循的策略。例如，在二十一点中，策略可能是查看我们的手牌（状态）并随机出牌或停留。虽然这会是一个糟糕的策略，但需要强调的重要一点是，策略决定了我们采取哪些行动。一个更好的策略可能是始终出牌直到我们拥有
    19 点。'
- en: The *reward* is the feedback our agent gets after taking an action, leading
    us to a new state. For a game of chess, we could reward our agent +1 when it performs
    an action that leads to a checkmate of the other player and –1 for an action that
    leads our agent to be checkmated. Every other state could be rewarded 0, since
    we do not know if the agent is winning or not.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '*奖励* 是智能体采取行动后获得的反馈，导致我们进入新的状态。对于棋类游戏，当智能体采取导致对方被将死的行动时，我们可以奖励智能体 +1，而对于导致智能体被将死的行动，则奖励
    -1。其他所有状态都可以奖励 0，因为我们不知道智能体是否获胜。'
- en: Our agent makes a series of actions based upon its policy *π*, and repeats this
    process until the episode ends, thereby we get a succession of states, actions
    and the resulting rewards.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体根据其策略 *π* 采取一系列行动，并重复此过程直到剧集结束，从而我们得到一系列状态、行动和相应的奖励。
- en: '![](pg055.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](pg055.jpg)'
- en: We call the weighted sum of the rewards while following a policy from the starting
    state *S*[1] the *value* of that state, or a state value. We can denote this by
    the *value function V**[π]*(*s*), which accepts an initial state and returns the
    expected total reward.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循从起始状态 *S* 的策略得到的奖励加权和称为该状态的价值，或状态价值。我们可以用 *值函数 V**[π]*(*s*) 来表示，它接受一个初始状态并返回期望的总奖励。
- en: '![](pg056.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](pg056.jpg)'
- en: The coefficients *w*[1], *w*[2], etc., are the weights we apply to the rewards
    before summing them. For example, we often want to weight more recent rewards
    greater than distant future rewards. This weighted sum is an expected value, a
    common statistic in many quantitative fields, and it’s often concisely denoted
    *E*[*R* |*π*,*s*], read as “the expected rewards given a policy *π* and a starting
    state *s*.” Similarly, there is an *action-value function, Q**[π]*(*s*,*a*), that
    accepts a state *S* and an action *A* and returns the value of taking that action
    given that state; in other words, *E*[*R* |*π*,*s,a*]. Some RL algorithms or implementations
    will use one or the other.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 系数 *w*[1]，*w*[2]，等等，是我们对奖励进行求和之前应用的权重。例如，我们通常希望更近期的奖励比远期奖励更重要。这个加权总和是一个期望值，在许多定量领域中是一个常见的统计量，通常简记为
    *E*[*R* |*π*,*s*]，读作“给定策略 *π* 和起始状态 *s* 的期望奖励。”同样，还有一个 *动作值函数，Q**[π]*(*s*,*a*)，它接受一个状态
    *S* 和一个动作 *A*，并返回在给定该状态下采取该动作的价值；换句话说，*E*[*R* |*π*,*s,a*]。一些RL算法或实现将使用其中一个。
- en: Importantly, if we base our algorithm on learning the state values (as opposed
    to action values), we must keep in mind that the value of a state depends completely
    on our policy, *π*. Using Blackjack as an example, if we’re in the state of having
    a card total of 20, and we have two possible actions, hit or stay, the value of
    this state is only high if our policy says to stay when we have 20\. If our policy
    said to hit when we have 20, we would probably bust and lose the game, so the
    value of that state would be low. In other words, the value of a state is equivalent
    to the value of the highest action taken in that state.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，如果我们基于学习状态值（而不是动作值）的算法，我们必须记住状态的价值完全取决于我们的策略，*π*。以21点为例，如果我们处于拥有20点牌的总数的状态，并且有两个可能的行为，击牌或停牌，这个状态的价值只有在我们的策略说在拥有20点时停牌时才高。如果我们的策略说在拥有20点时击牌，我们可能会爆牌并输掉游戏，因此这个状态的价值就会低。换句话说，状态的价值等同于在该状态下采取的最高动作的价值。
- en: 3.2\. Navigating with Q-learning
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 使用Q学习导航
- en: In 2013, DeepMind published a paper entitled “Playing Atari with Deep Reinforcement
    Learning” that outlined their new approach to an old algorithm, which gave them
    enough performance to play six of seven Atari 2600 games at record levels. Crucially,
    the algorithm they used only relied on analyzing the raw pixel data from the games,
    just like a human would. This paper really set off the field of deep reinforcement
    learning.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，DeepMind发表了一篇题为“使用深度强化学习玩Atari”的论文，概述了他们对旧算法的新方法，这使得他们能够以创纪录的水平玩六款中的七款Atari
    2600游戏。关键的是，他们使用的算法仅依赖于分析游戏的原始像素数据，就像人类一样。这篇论文真正开启了深度强化学习的领域。
- en: The old algorithm they modified is called *Q-learning,* and it has been around
    for decades. Why did it take so long to make such significant progress? A large
    part is due to the general boost that artificial neural networks (deep learning)
    got a few years prior with the use of GPUs that allowed the training of much larger
    networks. But a significant amount is due to the specific novel features DeepMind
    implemented to address some of the other issues that reinforcement learning algorithms
    struggled with. We’ll be covering it all in this chapter.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 他们修改的旧算法被称为 *Q学习*，它已经存在了几十年。为什么需要这么长时间才取得如此显著的进步？很大一部分原因是由于几年前，随着GPU的使用，人工神经网络（深度学习）得到了普遍的推动，这使得训练更大的网络成为可能。但很大一部分原因是由于DeepMind实施的一些特定新颖功能，以解决强化学习算法所面临的某些其他问题。我们将在本章中涵盖所有内容。
- en: 3.2.1\. What is Q-learning?
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1. 什么是Q学习？
- en: What is Q-learning, you ask? If you guessed it has something to do with the
    action-value function *Q**[π]*(*s*,*a*) that we previously described, you are
    right, but that’s only a small part of the story. Q-learning is a particular method
    of learning optimal action values, but there are other methods. That is to say,
    value functions and action-value functions are general concepts in RL that appear
    in many places; Q-learning is a particular algorithm that uses those concepts.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 你问什么是Q学习？如果你猜到它与之前描述的动作值函数 *Q**[π]*(*s*,*a*) 有关，你就对了，但这只是故事的一小部分。Q学习是学习最优动作值的一种特定方法，但还有其他方法。也就是说，价值函数和动作值函数是强化学习中的通用概念，出现在许多地方；Q学习是使用这些概念的特定算法。
- en: 'Believe it or not, we sort of implemented a Q-learning algorithm in the last
    chapter when we built a neural network to optimize the ad placement problem. The
    main idea of Q-learning is that your algorithm predicts the value of a state-action
    pair, and then you compare this prediction to the observed accumulated rewards
    at some later time and update the parameters of your algorithm, so that next time
    it will make better predictions. That’s essentially what we did in the last chapter
    when our neural network predicted the expected reward (value) of each action given
    a state, observed the actual reward, and updated the network accordingly. That
    was a particular and simple implementation of a broader class of Q-learning algorithms
    that is described by the following update rule:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，我们在上一章构建神经网络以优化广告放置问题时，实际上已经实现了一种 Q-learning 算法。Q-learning 的主要思想是，你的算法预测状态-动作对的价值，然后你将这个预测与在某个较晚时间观察到的累积奖励进行比较，并更新你的算法参数，以便下次它能做出更好的预测。这正是我们在上一章所做的事情，当时我们的神经网络预测了给定状态下每个动作的预期奖励（价值），观察到了实际奖励，并相应地更新了网络。这是一个特定且简单的
    Q-learning 算法实现，它属于以下更新规则描述的更广泛类别：
- en: Table 3.1\. Q-learning update rule
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1\. Q-learning 更新规则
- en: '| **Math** |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| **数学** |'
- en: '| ![](pg057_alt.jpg) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '![图片](pg057_alt.jpg)'
- en: '| **Pseudocode** |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| **伪代码** |'
- en: '|'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE32]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '|'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **English** |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| **英文** |'
- en: '| The Q value at time *t* is updated to be the current predicted Q value plus
    the amount of value we expect in the future, given that we play optimally from
    our current state. |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 在时间 *t* 的 Q 值更新为当前的预测 Q 值加上我们期望在未来的价值量，前提是我们从当前状态进行最优操作。|'
- en: 3.2.2\. Tackling Gridworld
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 解决 Gridworld
- en: You’ve now seen the formula for Q-learning. Let’s take a step back and apply
    this formula to our Gridworld problem. Our goal in this chapter is to train a
    neural network to play a simple Gridworld game from scratch. All the agent will
    have access to is what the board looks like, just as a human player would; the
    algorithm has no informational advantage. Moreover, we’re starting with an untrained
    algorithm, so it literally knows nothing at all about the world. It has no prior
    information about how games work. The only thing we’ll provide is the reward for
    reaching the goal. The fact that we will be able to teach the algorithm to learn
    to play, starting from nothing, is actually quite impressive.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经看到了 Q-learning 的公式。让我们退一步，将这个公式应用于我们的 Gridworld 问题。本章的目标是训练一个神经网络从零开始玩一个简单的
    Gridworld 游戏。代理将能够访问的信息与人类玩家一样，即游戏板的外观；算法没有信息优势。此外，我们从一个未训练的算法开始，所以它实际上对世界一无所知。它对游戏如何运作没有任何先验信息。我们将提供的是达到目标时的奖励。能够从零开始教会算法学习玩游戏的事实实际上相当令人印象深刻。
- en: Unlike us humans who live in what appears to be a continuous flow of time, the
    algorithm lives in a discrete world, so something needs to happen at each discrete
    time step. At time step 1 the algorithm will “look” at the game board and make
    a decision about what action to take. Then the game board will be updated, and
    so on.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们人类生活在看似连续的时间流中不同，算法生活在离散的世界中，因此在每个离散时间步都需要发生某些事情。在时间步 1，算法将“查看”游戏板并决定采取什么行动。然后游戏板将被更新，依此类推。
- en: Let’s sketch out the details of this process now. Here’s the sequence of events
    for a game of Gridworld.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来详细描述这个过程的细节。以下是 Gridworld 游戏的事件序列。
- en: We start the game in some state that we’ll call *S[t]*. The state includes all
    the information about the game that we have. For our Gridworld example, the game
    state is represented as a 4 × 4 × 4 tensor. We will go into more detail about
    the specifics of the board when we implement the algorithm.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从某个称为 *S[t]* 的状态开始游戏。状态包括我们拥有的关于游戏的所有信息。对于我们的 Gridworld 示例，游戏状态表示为一个 4 × 4
    × 4 张量。当我们实现算法时，我们将详细介绍关于游戏板的详细信息。
- en: We feed the *S[t]* data and a candidate action into a deep neural network (or
    some other fancy machine-learning algorithm) and it produces a prediction of how
    valuable taking that action in that state is (see [figure 3.2](#ch03fig02)).
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 *S[t]* 数据和一个候选动作输入到一个深度神经网络（或某些其他复杂的机器学习算法）中，它会产生一个预测，即在该状态下采取该动作的价值（参见[图
    3.2](#ch03fig02)）。
- en: Figure 3.2\. The Q function could be any function that accepts a state and action
    and returns the value (expected rewards) of taking that action given that state.
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2\. Q 函数可以是接受状态和动作并返回该动作在给定状态下价值的任何函数。
- en: '![](03fig02.jpg)'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](03fig02.jpg)'
- en: Remember, the algorithm is not predicting the reward we will get after taking
    a particular action; it’s predicting the expected value (the expected rewards),
    which is the long-term average reward we will get from taking an action in a state
    and then continuing to behave according to our policy *π*. We do this for several
    (perhaps all) possible actions we could take in this state.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住，算法并不是预测采取特定动作后我们将获得的奖励；它预测的是期望值（期望奖励），这是我们采取一个状态中的动作并继续根据我们的策略 *π* 行为时将获得的长期平均奖励。我们为此状态中我们可以采取的几个（也许所有）可能动作做这件事。
- en: We take an action, perhaps because our neural network predicted it is the highest
    value action or perhaps we take a random action. We’ll label the action *A[t]*.
    We are now in a new state of the game, which we’ll call *S[t]*[+1], and we receive
    or observe a reward, labelled *R[t]*[+1]. We want to update our learning algorithm
    to reflect the actual reward we received, after taking the action it predicted
    was the best. Perhaps we got a negative reward or a really big reward, and we
    want to improve the accuracy of the algorithm’s predictions (see [figure 3.3](#ch03fig03)).
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们采取一个动作，可能是因为我们的神经网络预测它是最高价值的动作，或者我们随机采取一个动作。我们将这个动作标记为 *A[t]*。我们现在处于游戏的新状态，我们将称之为
    *S[t]*[+1]，并且我们收到或观察到奖励，标记为 *R[t]*[+1]。我们希望更新我们的学习算法，以反映我们采取它预测为最佳动作后实际收到的奖励。也许我们得到了一个负奖励或一个非常大的奖励，我们希望提高算法预测的准确性（参见[图
    3.3](#ch03fig03)）。
- en: Figure 3.3\. Schematic of Q-learning with Gridworld. The Q function accepts
    a state and an action, and returns the predicted reward (value) of that state-action
    pair. After taking the action, we observe the reward, and using the update formula,
    we use this observation to update the Q function so it makes better predictions.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3\. 使用 Gridworld 的 Q-learning 概念图。Q 函数接受一个状态和一个动作，并返回该状态-动作对的预测奖励（价值）。采取动作后，我们观察到奖励，并使用更新公式，我们使用这个观察结果来更新
    Q 函数，使其做出更好的预测。
- en: '![](03fig03_alt.jpg)'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](03fig03_alt.jpg)'
- en: Now we run the algorithm using *S[t]*[+1] as input and figure out which action
    our algorithm predicts has the highest value. We’ll call this value *Q*(*S[t]*[+1],*a*).
    To be clear, this is a single value that reflects the highest predicted *Q* value,
    given our new state and all possible actions.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用 *S[t]*[+1] 作为输入运行算法，并找出我们的算法预测具有最高价值的动作。我们将这个值称为 *Q*(*S[t]*[+1],*a*)。为了清楚起见，这是一个单一的价值，它反映了给定我们的新状态和所有可能动作的最高预测
    *Q* 值。
- en: Now we have all the pieces we need to update the algorithm’s parameters. We’ll
    perform one iteration of training using some loss function, such as mean-squared
    error, to minimize the difference between the predicted value from our algorithm
    and the target prediction of *Q*(*S[t]*,*A[t]*) + *α**[*R[t]*[+1] + *γ**max*Q*(*S[t]*[+1],*A*)
    – *Q*(*S[t]*,*A[t]*)].
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了更新算法参数所需的所有部件。我们将使用一些损失函数，例如均方误差，进行一次训练迭代，以最小化我们算法预测值与目标预测值 *Q*(*S[t]*,*A[t]*)
    + *α**[*R[t]*[+1] + *γ**max*Q*(*S[t]*[+1],*A*) – *Q*(*S[t]*,*A[t]*) 之间的差异。
- en: 3.2.3\. Hyperparameters
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 超参数
- en: 'The parameters *γ* and *α* are called *hyperparameters* because they’re parameters
    that influence how the algorithm learns but they’re not involved in the actual
    learning. The parameter α is the *learning rate* and it’s the same hyperparameter
    used to train many machine-learning algorithms. It controls how quickly we want
    the algorithm to learn from each move: a small value means it will only make small
    updates at each step, whereas a large value means the algorithm will potentially
    make large updates.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *γ* 和 *α* 被称为 *超参数*，因为它们是影响算法学习但不在实际学习过程中涉及的参数。参数 α 是 *学习率*，它与训练许多机器学习算法时使用的相同超参数相同。它控制我们希望算法从每次移动中学习的速度：小值意味着它将在每一步只进行小的更新，而大值意味着算法可能会进行大的更新。
- en: 3.2.4\. Discount factor
  id: totrans-472
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 折现因子
- en: The parameter γ, the *discount factor*, is a variable between 0 and 1 that controls
    how much our agent discounts future rewards when making a decision. Let’s take
    a simple example. Our agent has a decision between picking an action that leads
    to 0 reward then +1 reward, or an action that leads to +1 and then 0 reward (see
    [figure 3.4](#ch03fig04)).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 γ，即 *折现因子*，是一个介于 0 和 1 之间的变量，它控制我们的代理在做出决策时对未来奖励的折现程度。让我们举一个简单的例子。我们的代理面临一个选择：选择一个导致
    0 奖励然后 +1 奖励的动作，或者选择一个导致 +1 然后是 0 奖励的动作（参见[图 3.4](#ch03fig04)）。
- en: Figure 3.4\. An illustration of action trajectories leading to the same total
    reward but that may be valued differently since more recent rewards are generally
    valued more than distant rewards.
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4. 一个动作轨迹的插图，展示了导致相同总奖励但可能被不同地评估的情况，因为最近的奖励通常比遥远的奖励更有价值。
- en: '![](03fig04.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig04.jpg)'
- en: Previously, we defined the value of a trajectory as the expected reward. Both
    trajectories in [figure 3.4](#ch03fig04) provide +1 overall reward though, so
    which sequence of actions should the algorithm prefer? How can we break the tie?
    Well, if the discount factor, γ, is less than 1, we will discount future rewards
    more than immediate rewards. In this simple case, even though both paths lead
    to a total of +1 rewards, action *b* gets the +1 reward later than action *a*,
    and since we’re discounting the action further in the future, we prefer action
    *a*. We multiply the +1 reward in action *b* by a weighting factor less than 1,
    so we lower the reward from +1 to say 0.8, so the choice of action is clear.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，我们将轨迹的价值定义为期望奖励。图3.4中的两个轨迹虽然都提供了+1的总奖励，但哪个动作序列算法应该优先考虑？我们如何打破这种平局？嗯，如果折现因子γ小于1，我们将比即时奖励更多地折现未来奖励。在这个简单的情况下，尽管两条路径都导致总奖励为+1，但动作*b*获得+1奖励的时间晚于动作*a*，而且由于我们会对未来的动作进一步折现，所以我们更喜欢动作*a*。我们将动作*b*中的+1奖励乘以一个小于1的权重因子，这样就将奖励从+1降低到0.8左右，因此动作的选择就变得清晰了。
- en: The discount factor comes up in real life as well as RL. Suppose someone offers
    you $100 now or $110 one month from now. Most people would prefer to receive the
    money now, because we discount the future to some degree, which makes sense because
    the future is uncertain (what if the person offering you the money dies in two
    weeks?). Your discount factor in real life would depend on how much money someone
    would have to offer you in one month for you to be indifferent to choosing that
    versus getting $100 right now. If you would only accept $200 in a month versus
    $100 right now, your discount factor would be $100/$200 = 0.5 (per month). This
    would mean that someone would have to offer you $400 in two months for you to
    choose that option over getting $100 now, since we’d discount 0.5 for 1 month,
    and 0.5 again for the next month, which is 0.5 × 0.5 = 0.25, and 100 = 0.25*x*,
    so *x* = 400\. Perhaps you might see the pattern that discounting is exponential
    in time. The value of something at time *t* with a discount factor of *γ*:[0,1)
    is *γ**^t*.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 折现因子不仅在现实生活而且在强化学习（RL）中都会出现。假设有人现在给你100美元，或者一个月后给你110美元。大多数人更愿意现在就拿到钱，因为我们多少会折现未来，这是有道理的，因为未来是不确定的（如果给你钱的人两周后去世怎么办？）。你现实生活中的折现因子将取决于一个人必须给你多少一个月后的钱，才能让你对选择那个选项和现在拿到100美元无动于衷。如果你只愿意在一个月后接受200美元，而现在是100美元，那么你的折现因子将是100/200
    = 0.5（每月）。这意味着，如果有人要在两个月后给你400美元，你才会选择那个选项而不是现在拿到100美元，因为我们会对1个月折现0.5，然后对下一个月再折现0.5，即0.5
    × 0.5 = 0.25，而100 = 0.25*x*，所以*x* = 400。也许你可能会发现折现随时间呈指数级变化的模式。具有折现因子*γ*（[0,1)）的某物在时间*t*的价值是*γ**^t*。
- en: The discount factor needs to be between 0 and 1, and we shouldn’t set it exactly
    equal to 1, because if we don’t discount at all, we would have to consider the
    future rewards infinitely far into the future, which is impossible in practice.
    Even if we discount at 0.99999, there will eventually come a time beyond which
    we no longer consider any data, since it will be discounted to 0.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 折现因子需要介于0和1之间，我们不应该将其恰好设置为1，因为如果我们完全不进行折现，我们就必须考虑未来奖励无限期地延伸到未来，这在实践中是不可能的。即使我们以0.99999进行折现，也终将到来一个时间点，在此之后我们不再考虑任何数据，因为它们将被折现到0。
- en: 'In Q-learning, we face the same decision: how much do we consider future observed
    rewards when learning to predict Q values? Unfortunately, there’s no definitive
    answer to this, or to setting pretty much any of the hyperparameters we have control
    over. We just have to play around with these knobs and see what works best empirically.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习（Q-learning）中，我们面临相同的决策：在预测Q值时，我们考虑未来观察到的奖励有多少？遗憾的是，对此没有明确的答案，或者对设置我们能够控制的几乎所有超参数都没有明确的答案。我们只能对这些旋钮进行实验，看看什么在经验上效果最好。
- en: It’s worth pointing out that most games are *episodic*, meaning that there are
    multiple chances to take actions before the game is over, and many games like
    chess don’t naturally assign points to anything other than winning or losing the
    game. Hence, the reward signal in these games is sparse, making it difficult for
    trial-and-error based learning to reliably learn anything, as it requires seeing
    a reward fairly frequently.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的是，大多数游戏都是**分集式**的，这意味着在游戏结束之前有多次采取行动的机会，而像棋类游戏这样的游戏通常不会对除了赢得或输掉游戏之外的其他任何事情分配分数。因此，这些游戏中的奖励信号是稀疏的，这使得基于试错的学习难以可靠地学习任何东西，因为它需要频繁地看到奖励。
- en: In Gridworld, we’ve designed the game so that any move that doesn’t win the
    game receives a reward of –1, the winning move gets a reward of +10, and a losing
    move rewards –10\. It’s really only the final move of the game where the algorithm
    can say “Aha! Now I get it!” Since each episode of a Gridworld game can be won
    in a fairly small number of moves, the sparse reward problem isn’t too bad, but
    in other games it is such a significant problem that even the most advanced reinforcement
    learning algorithms have yet to reach human-level performance. One proposed method
    of dealing with this is to stop relying on the objective of maximizing expected
    rewards and instead instruct the algorithm to seek novelty, through which it will
    learn about its environment, which is something we’ll cover in [chapter 8](kindle_split_018.html#ch08).
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gridworld中，我们设计游戏的方式是，任何没有赢得游戏的移动都会得到-1的奖励，赢得游戏的移动会得到+10的奖励，而输掉游戏的移动会得到-10的奖励。实际上，只有在游戏的最后一步，算法才能说“啊哈！我现在明白了！”由于Gridworld游戏的每一局可以在相当少的移动次数内赢得，稀疏奖励问题并不是太严重，但在其他游戏中，这是一个如此严重的问题，以至于即使是最先进的强化学习算法也还没有达到人类水平的性能。一种处理这个问题的方法是停止依赖最大化预期奖励的目标，而是指导算法寻求新颖性，通过这种方式，它将了解其环境，这是我们将在[第8章](kindle_split_018.html#ch08)中讨论的内容。
- en: 3.2.5\. Building the network
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5. 构建网络
- en: Let’s dig into how we will build our deep learning algorithm for this game.
    Recall that a neural network has a particular kind of architecture or network
    topology. When you build a neural network, you have to decide how many layers
    it should have, how many parameters each layer has (the “width” of the layer),
    and how the layers are connected. Gridworld is simple enough that we don’t need
    to build anything fancy. We can get away with a fairly straightforward feedforward
    neural network with only a few layers, using the typical rectified linear activation
    unit (ReLU). The only parts that require some more careful thought are how we
    will represent our input data, and how we will represent the output layer.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解我们将如何构建这个游戏的深度学习算法。回想一下，神经网络具有特定的架构或网络拓扑。当你构建一个神经网络时，你必须决定它应该有多少层，每层有多少参数（层的“宽度”），以及层是如何连接的。Gridworld足够简单，我们不需要构建任何复杂的东西。我们可以使用只有几层的相当直接的馈前神经网络，使用典型的ReLU激活单元。唯一需要更多思考的部分是我们将如何表示我们的输入数据，以及我们将如何表示输出层。
- en: We’ll cover the output layer first. In our discussion of Q-learning, we said
    that the Q function is a function that takes some state and some action and computes
    the value of that state-action pair, *Q*(*s*,*a*). This is how the Q function
    was originally defined ([figure 3.5](#ch03fig05)). As we noted in the previous
    chapter, there is also a state-value function, usually denoted *V**[π]*(*s*),
    that computes the value of some state, given that you’re following a particular
    policy, *π*.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论输出层。在我们对Q学习的讨论中，我们说Q函数是一个函数，它接受一些状态和一些动作，并计算该状态-动作对的价值，即*Q*(*s*,*a*)。这就是Q函数最初是如何定义的（[图3.5](#ch03fig05)）。正如我们在上一章中指出的，还有一个状态值函数，通常表示为*V**[π]*(*s*)，它计算在遵循特定策略*π*的情况下某个状态的价值。
- en: Figure 3.5\. The original Q function accepts a state-action pair and returns
    the value of that state-action pair—a single number. DeepMind used a modified
    vector-valued Q function that accepts a state and returns a vector of state-action
    values, one for each possible action given the input state. The vector-valued
    Q function is more efficient, since you only need to compute the function once
    for all the actions.
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5。原始的Q函数接受一个状态-动作对，并返回该状态-动作对的价值——一个单一的数字。DeepMind使用了一个修改过的向量值Q函数，它接受一个状态并返回一个状态-动作值向量，每个可能的动作对应输入状态的一个值。向量值Q函数更高效，因为你只需要为所有动作计算一次函数。
- en: '![](03fig05_alt.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig05_alt.jpg)'
- en: Generally, we want to use the Q function because it can tell us the value of
    taking an action in some state, so we can take the action that has the highest
    predicted value. But it would be rather wasteful to separately compute the Q values
    for every possible action given the state, even though the Q function was originally
    defined that way. A much more efficient procedure, and the one that DeepMind employed
    in its implementation of deep Q-learning, is to instead recast the Q function
    as a vector-valued function, meaning that instead of computing and returning a
    single Q value for a single state-action pair, it will compute the Q values for
    all actions, given some state, and return the vector of all those Q values. So
    we might represent this new version of the Q function as *Q[A]*(*s*), where the
    subscript *A* denotes the set of all possible actions ([figure 3.5](#ch03fig05)).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望使用 Q 函数，因为它可以告诉我们处于某个状态时采取某个动作的价值，因此我们可以选择具有最高预测价值的动作。但是，考虑到 Q 函数最初是这样定义的，即使对于每个可能的动作单独计算
    Q 值会相当浪费。一个更有效的方法，也是 DeepMind 在其深度 Q 学习实现中采用的方法，是将 Q 函数重新定义为向量值函数，这意味着它将计算给定某个状态的所有动作的
    Q 值，并返回所有这些 Q 值的向量。因此，我们可以将这个新版本的 Q 函数表示为 *Q[A]*(*s*)，其中下标 *A* 表示所有可能动作的集合（[图
    3.5](#ch03fig05)）。
- en: Now it’s easy to employ a neural network as our *Q[A]*(*s*) version of the Q
    function; the last layer will simply produce an output vector of Q values—one
    for each possible action. In the case of Gridworld, there are only four possible
    actions (up, down, left, right) so the output layer will produce 4-dimensional
    vectors. We can then directly use the output of the neural network to decide what
    action to take using some action selection procedure, such as a simple epsilon-greedy
    approach or a softmax selection policy. In this chapter we’ll use the epsilon-greedy
    approach ([figure 3.6](#ch03fig06)) as DeepMind did, and instead of using a static
    ε value like we did in the last chapter, we will initialize it to a large value
    (i.e., 1, so we’ll start with a completely random selection of actions) and we
    will slowly decrement it so that after a certain number of iterations, the ε value
    will rest at some small value. In this way, we will allow the algorithm to explore
    and learn a lot in the beginning, but then it will settle into maximizing rewards
    by exploiting what it has learned. Hopefully we will set the decrementing process
    so that it will not underexplore or overexplore, but that will have to be tested
    empirically.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易将神经网络作为我们的 *Q[A]*(*s*) 版本的 Q 函数；最后一层将简单地输出一个 Q 值向量——每个可能的动作对应一个。在 Gridworld
    的情况下，只有四种可能的动作（上、下、左、右），所以输出层将产生 4 维向量。然后我们可以直接使用神经网络的输出，通过某种动作选择程序来决定采取什么动作，例如简单的
    ε-greedy 方法或 softmax 选择策略。在本章中，我们将使用 ε-greedy 方法（[图 3.6](#ch03fig06)），并且与上一章使用静态的
    ε 值不同，我们将它初始化为一个较大的值（即 1，因此我们将从完全随机的动作选择开始），然后我们将逐渐减少它，使得经过一定次数的迭代后，ε 值将停留在某个较小的值。这样，我们将在开始时允许算法探索和学习很多，但随后它将利用所学知识最大化奖励。希望我们能够设置递减过程，使其不会过度探索或不足探索，但这需要通过实验来验证。
- en: Figure 3.6\. In an epsilon-greedy action selection method, we set the epsilon
    parameter to some value, e.g., 0.1, and with that probability we will randomly
    select an action (completely ignoring the predicted Q values) or with probability
    1 – epsilon = 0.9, we will select the action associated with the highest predicted
    Q value. An additional helpful technique is to start with a high epsilon value,
    such as 1, and then slowly decrement it over the training iterations.
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.6。在 ε-greedy 动作选择方法中，我们将 ε 参数设置为某个值，例如 0.1，并且以这个概率我们将随机选择一个动作（完全忽略预测的 Q
    值）或者以 1 – ε = 0.9 的概率，我们将选择与最高预测 Q 值相关的动作。一个额外的有用技术是开始时使用一个高的 ε 值，例如 1，然后在训练迭代中逐渐减少它。
- en: '![](03fig06_alt.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig06_alt.jpg)'
- en: We have the output layer figured out—now to tackle the rest. In this chapter,
    we will construct a network of just three layers with widths of 164 (input layer),
    150 (hidden layer), 4 (the output layer you already saw). You are welcome and
    encouraged to add more hidden layers or to play with the size of the hidden layer—you
    will likely be able to achieve better results with a deeper network. We chose
    to implement a fairly shallow network here so that you can train the model with
    your own CPU (it takes our MacBook Air 1.7 GHz Intel Core i7, with 8 GB of RAM,
    only a few minutes to train).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经搞定了输出层——现在要解决剩下的部分。在本章中，我们将构建一个只有三层，宽度分别为164（输入层）、150（隐藏层）、4（你已经看到的输出层）的网络。欢迎并鼓励你添加更多的隐藏层或尝试调整隐藏层的大小——你可能会通过更深层次的网络获得更好的结果。我们选择在这里实现一个相对较浅的网络，这样你就可以使用自己的CPU来训练模型（在我们的MacBook
    Air 1.7 GHz Intel Core i7，8 GB RAM的电脑上，训练只需要几分钟）。
- en: We already discussed why the output layer is of width 4, but we haven’t talked
    about the input layer yet. Before we do that, though, we need to introduce the
    Gridworld game engine we will be using. We developed a Gridworld game for this
    book, and it is included in the GitHub repository for this chapter.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了为什么输出层的宽度是4，但还没有谈到输入层。在我们这样做之前，我们需要介绍我们将要使用的Gridworld游戏引擎。我们为这本书开发了一个Gridworld游戏，它包含在本章的GitHub仓库中。
- en: 3.2.6\. Introducing the Gridworld game engine
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6. 介绍Gridworld游戏引擎
- en: In the GitHub repository for this chapter, you’ll find a file called Gridworld.py.
    Copy and paste this file into whatever folder you’ll be working out of. You can
    include it in your Python session by running `from Gridworld import *`. The Gridworld
    module contains some classes and helper functions to run a Gridworld game instance.
    To create a Gridworld game instance, run the code in the following listing.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的GitHub仓库中，你可以找到一个名为Gridworld.py的文件。将此文件复制并粘贴到你将要工作的文件夹中。你可以通过运行`from Gridworld
    import *`将其包含在你的Python会话中。Gridworld模块包含一些类和辅助函数，用于运行Gridworld游戏实例。要创建一个Gridworld游戏实例，请运行以下列表中的代码。
- en: Listing 3.1\. Creating a Gridworld game
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1. 创建Gridworld游戏
- en: '[PRE33]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The Gridworld board is always square, so the size refers to one side’s dimension—in
    this case a 4 × 4 grid will be created. There are three ways to initialize the
    board. The first is to initialize it statically, as in [listing 3.1](#ch03ex01),
    so that the objects on the board are initialized at the same predetermined locations.
    Second, you can set `mode='player'` so that just the player is initialized at
    a random position on the board. Last, you can initialize it so that all the objects
    are placed randomly (which is harder for the algorithm to learn) using `mode='random'`.
    We’ll use all three options eventually.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: Gridworld棋盘始终是正方形，所以大小指的是一边的维度——在这种情况下，将创建一个4×4的网格。有三种初始化棋盘的方法。第一种是静态初始化，如[列表3.1](#ch03ex01)所示，这样棋盘上的对象就会初始化在相同的预定位置。第二种，你可以设置`mode='player'`，这样玩家就会在棋盘上的随机位置初始化。最后，你可以使用`mode='random'`来初始化，这样所有对象都会随机放置（这对算法的学习来说更困难）。我们最终会使用这三种选项。
- en: 'Now that we’ve created the game, let’s play it. Call the `display` method to
    display the board and the `makeMove` method to make a move. Moves are encoded
    with a single letter: *u* for up, *l* for left, and so on. After each move, you
    should display the board to see the effect. Additionally, after each move you’ll
    want to observe the reward/outcome of the move by calling the `reward` method.
    In Gridworld, every nonwinning move receives a –1 reward. The winning move (reaching
    the goal) receives a +10 reward, and there’s a –10 reward for the losing move
    (landing on the pit).'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了游戏，让我们来玩一玩。调用`display`方法来显示棋盘，调用`makeMove`方法来进行移动。移动用单个字母编码：*u*代表向上，*l*代表向左，等等。每次移动后，你应该显示棋盘以查看效果。此外，在每次移动后，你还将想要通过调用`reward`方法来观察移动的奖励/结果。在Gridworld中，每个非获胜移动都会得到-1的奖励。获胜移动（达到目标）得到+10的奖励，而失败移动（掉入陷阱）得到-10的奖励。
- en: '[PRE34]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let’s look at how the game state is actually represented, since we will
    need to feed this into our neural network. Run the following command:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看游戏状态是如何实际表示的，因为我们需要将其输入到我们的神经网络中。运行以下命令：
- en: '[PRE35]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The state is represented as a 4 × 4 × 4 tensor where the first dimension indexes
    a set of four matrices of size 4 × 4\. You can interpret this as having the dimensions
    *frames* by *height* by *width*. Each matrix is a 4 × 4 grid of zeros and a single
    1, where a 1 indicates the position of a particular object. Each matrix encodes
    the position of one of the four objects: the player, the goal, the pit, and the
    wall. If you compare the result from `display` with the game state, you can see
    that the first matrix encodes the position of the player, the second matrix encodes
    the position of the goal, the third matrix encodes the position of the pit, and
    the last matrix encodes the position of the wall.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 状态被表示为一个4 × 4 × 4的张量，其中第一个维度索引一组四个4 × 4大小的矩阵。你可以将其解释为具有*帧*、*高度*和*宽度*的维度。每个矩阵是一个4
    × 4的全零网格和一个单独的1，其中1表示特定对象的位臵。每个矩阵编码四个对象中的一个的位置：玩家、目标、坑和墙壁。如果你将`display`的结果与游戏状态进行比较，你可以看到第一个矩阵编码了玩家的位臵，第二个矩阵编码了目标的位臵，第三个矩阵编码了坑的位臵，最后一个矩阵编码了墙壁的位臵。
- en: In other words, the first dimension of this 3-tensor is divided into four separate
    grid planes, where each plane represents the position of each element. [Figure
    3.7](#ch03fig07) shows an example where the player is at grid position (2,2),
    the goal is at (0,0), the pit is at (0,1), and the wall is at (1,1), where the
    planes are (row, column). All other elements are 0s.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这个3张量的第一个维度被划分为四个独立的网格平面，其中每个平面代表每个元素的位臵。[图3.7](#ch03fig07) 展示了一个示例，其中玩家位于网格位臵(2,2)，目标位于(0,0)，坑位于(0,1)，墙壁位于(1,1)，其中平面是(行，列)。所有其他元素都是0。
- en: Figure 3.7\. This is how the Gridworld board is represented as a numpy array.
    It is a 4 x 4 x 4 tensor, composed of 4 “slices” of a 4 x 4 grid. Each grid slice
    represents the position of an individual object on the board and contains a single
    1, with all other elements being 0s. The position of the 1 indicates the position
    of that slice’s object.
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7\. 这是如何将Gridworld棋盘表示为一个numpy数组的。它是一个4 x 4 x 4的张量，由4个4 x 4网格的“切片”组成。每个网格切片代表棋盘上单个对象的位臵，并包含一个单独的1，其余元素为0。1的位置表示该切片对象的位臵。
- en: '![](03fig07.jpg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig07.jpg)'
- en: While we could, in principle, build a neural network that can operate on a 4
    × 4 × 4 tensor, it is easier to just flatten it into a 1-tensor (a vector). A
    4 × 4 × 4 tensor has 4³ = 64 total elements, so the input layer of our neural
    network must be accordingly shaped. The neural network will have to learn what
    this data means and how it relates to maximizing rewards. Remember, the algorithm
    will know absolutely nothing to begin with.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原则上我们可以构建一个可以在4 × 4 × 4张量上操作的神经网络，但将其展平成一个1张量（一个向量）更容易。一个4 × 4 × 4张量有4³ =
    64个总元素，因此我们神经网络的输入层必须相应地形状。神经网络将不得不学习这些数据的意义以及它与最大化奖励的关系。记住，算法一开始将一无所知。
- en: 3.2.7\. A neural network as the Q function
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.7\. 将神经网络作为Q函数
- en: Let’s build the neural network that will serve as our Q function. As you know,
    in this book we’re using PyTorch for all our deep learning models, but if you’re
    more comfortable with another framework such as TensorFlow or MXNet, it should
    be fairly straightforward to port the models.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个将作为我们的Q函数的神经网络。正如你所知，在这本书中，我们使用PyTorch来构建所有的深度学习模型，但如果你更熟悉TensorFlow或MXNet等其他框架，将模型移植应该相当直接。
- en: '[Figure 3.8](#ch03fig08) shows the general architecture for the model we will
    build. [Figure 3.9](#ch03fig09) shows it in string diagram form with typed strings.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.8](#ch03fig08) 展示了我们将要构建的模型的一般架构。[图3.9](#ch03fig09) 以字符串图的形式展示了它，其中包含类型化的字符串。'
- en: Figure 3.8\. The neural network model we will use to play Gridworld. The model
    has an input layer that can accept a 64-length game state vector, some hidden
    layers (we use one, but two are depicted for generality), and an output layer
    that produces a 4-length vector of Q values for each action, given the state.
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8\. 我们将用于玩Gridworld的神经网络模型。该模型有一个可以接受64位游戏状态向量的输入层，一些隐藏层（我们使用一个，但为了普遍性，展示了两个），以及一个输出层，根据状态为每个动作生成一个4位的Q值向量。
- en: '![](03fig08_alt.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig08_alt.jpg)'
- en: Figure 3.9\. String diagram for our DQN. The input is a 64-length Boolean vector,
    and the output is a 4-length real vector of Q values.
  id: totrans-512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9\. 我们的DQN的字符串图。输入是一个64位的布尔向量，输出是一个4位的Q值实向量。
- en: '![](03fig09_alt.jpg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig09_alt.jpg)'
- en: To implement this with PyTorch, we’ll use the `nn` module, which is the higher-level
    interface for PyTorch, similar to Keras for TensorFlow.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用PyTorch实现这一点，我们将使用`nn`模块，它是PyTorch的高级接口，类似于TensorFlow的Keras。
- en: Listing 3.2\. Neural network Q function
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2. 神经网络Q函数
- en: '[PRE36]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So far, all we’ve done is set up the neural network model, define a loss function
    and learning rate, set up an optimizer, and define a couple of parameters. If
    this were a simple classification neural network, we’d almost be done. We’d just
    need to set up a `for loop` to iteratively run the optimizer to minimize the model
    error with respect to the data. It’s more complicated with reinforcement learning,
    which is probably why you’re reading this book. We covered the main steps well
    earlier, but let’s zoom in a little.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的一切只是设置了神经网络模型，定义了一个损失函数和学习率，设置了一个优化器，并定义了一些参数。如果这是一个简单的分类神经网络，我们几乎就完成了。我们只需要设置一个`for`循环来迭代运行优化器，以最小化模型误差。在强化学习中，这要复杂得多，这可能是你阅读这本书的原因。我们很早就详细介绍了主要步骤，但让我们再深入一点。
- en: '[Listing 3.3](#ch03ex03) implements the main loop of the algorithm. In broad
    strokes, this is what it does:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.3](#ch03ex03)实现了算法的主要循环。大致来说，这是它所做的事情：'
- en: We set up a `for` loop for the number of epochs.
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为epoch的数量设置了一个`for`循环。
- en: In the loop, we set up a `while` loop (while the game is in progress).
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环中，我们设置了一个`while`循环（当游戏进行时）。
- en: We run the Q-network forward.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行Q网络的前向传播。
- en: We’re using an epsilon-greedy implementation, so at time *t* with probability
    ε we will choose a random action. With probability 1 – ε, we will choose the action
    associated with the highest Q value from our neural network.
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用epsilon-greedy实现，所以在时间 *t*，以概率ε我们将选择一个随机动作。以概率1 – ε，我们将选择与我们的神经网络中最高Q值相关的动作。
- en: Take action *a* as determined in the preceding step, and observe the new state
    s′ and reward *r[t]*[+1].
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行上一步确定的动作 *a*，并观察新的状态 s′ 和奖励 *r[t]*[+1]。
- en: Run the network forward using s′. Store the highest Q value, which we’ll call
    max Q.
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用s′运行网络的前向传播。存储最高的Q值，我们将其称为max Q。
- en: Our target value for training the network is *r[t]*[+1] + *γ**max*Q[A]*(*S[t]*[+1]),
    where γ (gamma) is a parameter between 0 and 1\. If after taking action *a[t]*
    the game is over, there is no legitimate *s[t]*[+1], so *γ**max*Q[A]*(*S[t]*[+1])
    is not valid and we can set it to 0\. The target becomes just *r[t]*[+1].
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练网络的目的是 *r[t]*[+1] + *γ**max*Q[A]*(*S[t]*[+1])，其中γ（伽马）是介于0和1之间的参数。如果在采取动作
    *a[t]* 后游戏结束，就没有合法的 *s[t]*[+1]，因此 *γ**max*Q[A]*(*S[t]*[+1]) 是无效的，我们可以将其设置为0。目标就变成了
    *r[t]*[+1]。
- en: Given that we have four outputs and we only want to update (i.e., train) the
    output associated with the action we just took, our target output vector is the
    same as the output vector from the first run, except we change the one output
    associated with our action to the result we computed using the Q-learning formula.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只有四个输出，而我们只想更新（即训练）我们刚刚采取的动作相关的输出，因此我们的目标输出向量与第一次运行时的输出向量相同，除了我们将与我们的动作相关的那个输出更改为我们使用Q学习公式计算出的结果。
- en: Train the model on this one sample. Then repeat steps 2–9.
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个样本上训练模型。然后重复步骤2–9。
- en: To be clear, when we first run our neural network and get an output of action
    values like this,
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，当我们第一次运行我们的神经网络并得到这样的动作值输出时，
- en: '[PRE37]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'our target vector for one iteration may look like this:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一次迭代中的目标向量可能看起来像这样：
- en: '[PRE38]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here we just changed a single entry to the value we wanted to update.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只更改了一个条目到我们想要更新的值。
- en: 'There’s one other detail we need to include in the code before we move on.
    The Gridworld game engine’s `makeMove` method expects a character such as *u*
    to make a move, but our Q-learning algorithm only knows how to generate numbers,
    so we need a simple map from numeric keys to action characters:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们还需要在代码中包含另一个细节。Gridworld游戏引擎的`makeMove`方法期望一个字符，如*u*来执行动作，但我们的Q学习算法只知道如何生成数字，因此我们需要一个简单的从数字键到动作字符的映射：
- en: '[PRE39]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Okay, let’s get to coding the main training loop.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始编写主要训练循环的代码。
- en: 'Listing 3.3\. Q-learning: Main training loop'
  id: totrans-536
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3. Q学习：主要训练循环
- en: '[PRE40]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '***1*** Creates a list to store loss values so we can plot the trend later'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建一个列表来存储损失值，以便我们可以在以后绘制趋势图'
- en: '***2*** The main training loop'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 主要训练循环'
- en: '***3*** For each epoch, we start a new game.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 对于每个epoch，我们开始一个新的游戏。'
- en: '***4*** After we create the game, we extract the state information and add
    a small amount of noise.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在我们创建游戏后，我们提取状态信息并添加少量噪声。'
- en: '***5*** Converts the numpy array into a PyTorch tensor and then into a PyTorch
    variable'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将NumPy数组转换为PyTorch张量，然后转换为PyTorch变量'
- en: '***6*** Uses the status variable to keep track of whether or not the game is
    still in progress'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用状态变量来跟踪游戏是否仍在进行中'
- en: '***7*** While this game is still in progress, plays to completion and then
    starts a new epoch'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 在这个游戏仍在进行时，玩到完成并然后开始一个新的epoch'
- en: '***8*** Runs the Q-network forward to get its predicted Q values for all the
    actions'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 运行Q网络以获取其对所有动作的预测Q值'
- en: '***9*** Selects an action using the epsilon-greedy method'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 使用epsilon-greedy方法选择动作'
- en: '***10*** Translates the numerical action into one of the action characters
    that our Gridworld game expects'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 将数值动作转换为我们的Gridworld游戏期望的动作字符之一'
- en: '***11*** After selecting an action using the epsilon-greedy method, takes the
    action'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 在使用epsilon-greedy方法选择动作后，采取该动作'
- en: '***12*** After making a move, gets the new state of the game'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 在做出移动后，获取游戏的新状态'
- en: '***13*** Finds the maximum Q value predicted from the new state'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13*** 从新状态中找到预测的最大Q值'
- en: '***14*** Calculates the target Q value'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14*** 计算目标Q值'
- en: '***15*** Creates a copy of the qval array and then updates the one element
    corresponding to the action taken'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***15*** 创建qval数组的副本，然后更新与所采取的动作相对应的一个元素'
- en: '***16*** If reward is –1, the game hasn’t been won or lost and is still in
    progress'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***16*** 如果奖励是-1，则游戏尚未结束或失败，仍在进行中'
- en: '***17*** Decrements the epsilon value each epoch'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***17*** 每个epoch递减epsilon值'
- en: '|  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-556
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Why did we add noise to the game state? It helps prevent “dead neurons,” which
    can happen with the use of rectified linear units (ReLU) as our activation function.
    Basically, because most of the elements in our game state array are 0s, they won’t
    play nice with ReLU, which is technically nondifferentiable at 0\. Hence, we add
    a tiny bit of noise so that none of the values in the state array are exactly
    0\. This might also help with *overfitting*, which is when a model learns by memorizing
    spurious details in the data without learning the abstract features of the data,
    ultimately preventing it from generalizing to new data.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在游戏状态中添加噪声？这有助于防止“死神经元”，这在使用ReLU作为我们的激活函数时可能会发生。基本上，因为我们的游戏状态数组中的大多数元素都是0，它们不会很好地与ReLU配合，ReLU在0处技术上不可微分。因此，我们添加了一点点噪声，以确保状态数组中的任何值都不是正好为0。这也有助于防止*过拟合*，即模型通过记住数据中的虚假细节来学习，而没有学习数据的抽象特征，最终阻止它泛化到新数据。
- en: '|  |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: There are a couple of things to point out that you may not have seen before.
    The first new thing is the use of the context `torch.no_grad()` when computing
    the next state Q value. Whenever we run a PyTorch model with some input, it will
    implicitly create a computational graph. Each PyTorch tensor is not only a store
    of tensor data, it also keeps track of which computations were performed to produce
    it. By using the `torch.no_grad()` context, we tell PyTorch to *not* create a
    computational graph for the code within the context; this will save memory when
    we don’t need the computational graph. When we compute the Q values for `state2`,
    we’re just using them as a target for training. We’re not going to backpropagate
    through the computational graph that would have been created if we hadn’t used
    `torch.no_grad`. We only want to backpropagate through the computational graph
    that is created when we call `model(state1),` because we want to train the parameters
    with respect to `state1`, not `state2`.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要指出，你可能之前没有见过。首先，是新引入的`torch.no_grad()`上下文在计算下一个状态Q值时的使用。每次我们用一些输入运行PyTorch模型时，它都会隐式地创建一个计算图。每个PyTorch张量不仅存储了张量数据，还跟踪了产生它的计算过程。通过使用`torch.no_grad()`上下文，我们告诉PyTorch不要为上下文内的代码创建计算图；这将在我们不需要计算图时节省内存。当我们计算`state2`的Q值时，我们只是将其用作训练的目标。我们不会通过如果没有使用`torch.no_grad`而本应创建的计算图进行反向传播。我们只想通过当我们调用`model(state1)`时创建的计算图进行反向传播，因为我们想根据`state1`而不是`state2`来训练参数。
- en: 'Here’s a simple example with a linear model:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的线性模型示例：
- en: '[PRE41]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We create two trainable parameters, `m` and `b`, by setting their `requires_grad`
    attribute to `True`, which means PyTorch will consider these parameters as nodes
    in a computational graph and will store their history of computations. Any new
    tensors that are created using `m` and `b`, such as `y` in this case, will also
    have `requires_grad` set to `True` and thus will also keep a memory of their computation
    history. You can see that the first time we call the linear model and print `y`,
    it gives us a tensor with the numeric result and also shows an attribute, `grad_fn=<AddBackward0>`.
    We can also see this attribute directly by printing `y.grad_fn`. This shows that
    this tensor was created by the addition operation. It is called `AddBackward`
    because it actually stores the derivative of the addition function.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将 `requires_grad` 属性设置为 `True` 来创建两个可训练参数 `m` 和 `b`，这意味着 PyTorch 将这些参数视为计算图中的节点，并将存储它们的计算历史。使用
    `m` 和 `b` 创建的任何新张量，例如本例中的 `y`，也将 `requires_grad` 设置为 `True`，因此也会保留它们的计算历史。你可以看到，当我们第一次调用线性模型并打印
    `y` 时，它给我们一个包含数值结果的张量，并显示一个属性，`grad_fn=<AddBackward0>`。我们也可以通过打印 `y.grad_fn` 直接看到这个属性。这表明这个张量是由加法操作创建的。它被称为
    `AddBackward`，因为它实际上存储了加法函数的导数。
- en: If you call this function given one input, it returns two outputs, like the
    opposite of addition, which takes two inputs and returns one output. Since our
    addition function is a function of two variables, there is a partial derivative
    with respect to the first input and a partial derivative with respect to the second
    input. The partial derivative of *y* = *a* + *b* with respect to *m* is ![](pg070-1.jpg)
    and ![](pg070-2.jpg). Or if *y* = *a* × *b* then ![](pg070-3.jpg) and ![](pg070-4.jpg).
    These are just the basic rules of taking derivatives. When we backpropagate from
    a given node, we need it to return all the partial derivatives, so that is why
    the `AddBackward0` gradient function returns two outputs.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你给这个函数提供一个输入，它将返回两个输出，就像加法的相反操作一样，它接受两个输入并返回一个输出。由于我们的加法函数是两个变量的函数，因此存在关于第一个输入的偏导数和关于第二个输入的偏导数。关于
    *y* = *a* + *b* 的偏导数相对于 *m* 是 ![](pg070-1.jpg) 和 ![](pg070-2.jpg)。或者如果 *y* = *a*
    × *b*，那么 ![](pg070-3.jpg) 和 ![](pg070-4.jpg)。这些只是求导的基本规则。当我们从给定的节点进行反向传播时，我们需要它返回所有偏导数，这就是为什么
    `AddBackward0` 梯度函数返回两个输出的原因。
- en: 'We can verify that PyTorch is indeed computing gradients as expected by calling
    the backward method on `y`:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在 `y` 上调用反向方法来验证 PyTorch 是否确实按照预期计算梯度：
- en: '[PRE42]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This is exactly what we would get from computing these simple partial derivatives
    in our head or on paper. In order to backpropagate efficiently, PyTorch keeps
    track of all forward computations and stores their derivatives so that eventually
    when we call the `backward()` method on the output node of our computational graph,
    it will backpropagate through these gradient functions node by node until the
    input node. That’s how we get the gradients for all the parameters in the model.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们从头脑中或在纸上计算这些简单的偏导数所能得到的结果。为了有效地进行反向传播，PyTorch 会跟踪所有正向计算并存储它们的导数，这样当我们对计算图的输出节点调用
    `backward()` 方法时，它将通过这些梯度函数逐节点反向传播直到输入节点。这就是我们如何得到模型中所有参数的梯度。
- en: Notice that we also called the `detach()` method on the `Y` tensor. This was
    actually unnecessary, since we used `torch.no_grad()` when we computed `newQ`,
    but we included it because detaching nodes from the computational graph will become
    ubiquitous throughout the rest of the book, and not properly detaching nodes is
    a common source of bugs when training a model. If we call `loss.backward(X,Y)`,
    and `Y` was associated with its own computational graph with trainable parameters,
    we would backpropagate into `Y` *and* `X,` and the training procedure would learn
    to minimize the loss by updating the trainable parameters in the `X` graph and
    the `Y` graph, whereas we only want to update the `X` graph. We *detach* the `Y`
    node from the graph so that it is just used as data and not as a computational
    graph node. You don’t need to think too hard about the details, but you do need
    to pay attention to which parts of the graph you’re actually backpropagating into
    and make sure you’re not backpropagating into the wrong nodes.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们还在`Y`张量上调用了`detach()`方法。这实际上是不必要的，因为我们计算`newQ`时使用了`torch.no_grad()`，但我们还是包含了它，因为将节点从计算图中分离出来将在本书的其余部分变得普遍，而且不正确地分离节点是训练模型时常见的错误来源。如果我们调用`loss.backward(X,Y)`，并且`Y`与其自己的计算图有关联的可训练参数，我们就会反向传播到`Y`和`X`，训练过程会学习通过更新`X`图和`Y`图中的可训练参数来最小化损失，而我们的目标只是更新`X`图。我们通过从图中分离`Y`节点，使其仅作为数据使用，而不是作为计算图节点。你不需要太深入地思考细节，但你确实需要关注你实际上反向传播到图中的哪些部分，并确保你没有反向传播到错误的节点。
- en: You can go ahead and run the training loop—1,000 epochs will be more than enough.
    Once it’s done, you can plot the losses to see if the training is successful and
    the model converges. The loss should more or less decrease and plateau over the
    training time. Our plot is shown in [figure 3.10](#ch03fig10).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续运行训练循环——1000个epoch将绰绰有余。一旦完成，你可以绘制损失图来查看训练是否成功以及模型是否收敛。损失应该在训练时间内或多或少地下降并趋于平稳。我们的图显示在[图3.10](#ch03fig10)中。
- en: Figure 3.10\. The loss plot for our first Q-learning algorithm, which is clearly
    down-trending over the training epochs.
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10\. 我们第一个Q学习算法的损失图，在训练epoch中明显呈下降趋势。
- en: '![](03fig10_alt.jpg)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig10_alt.jpg)'
- en: The loss plot is pretty noisy, but the moving average of the plot is significantly
    trending toward zero. This gives us some confidence the training worked, but we’ll
    never know until we test it. We’ve written up a simple function in [listing 3.4](#ch03ex04)
    that allows us to test the model on a single game.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 损失图相当嘈杂，但移动平均线明显趋向于零。这让我们有信心训练是成功的，但我们永远不知道直到我们测试它。我们编写了一个简单的函数，在[列表3.4](#ch03ex04)中，允许我们在单场比赛上测试模型。
- en: Listing 3.4\. Testing the Q-network
  id: totrans-572
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4\. 测试Q网络
- en: '[PRE43]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1*** While the game is still in progress'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 游戏仍在进行时'
- en: '***2*** Takes the action with the highest Q value'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 执行具有最高Q值的动作'
- en: The test function is essentially the same as the code in our training loop,
    except we don’t do any loss calculation or backpropagation. We just run the network
    forward to get the predictions. Let’s see if it learned how to play Gridworld!
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 测试函数基本上与训练循环中的代码相同，只是我们不进行任何损失计算或反向传播。我们只是运行网络前向以获取预测。让我们看看它是否学会了如何玩Gridworld！
- en: '[PRE44]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Can we get a round of applause for our Gridworld player here? Clearly it knows
    what it’s doing; it went straight for the goal!
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能为我们这里的Gridworld玩家鼓掌吗？显然它知道自己在做什么；它直接冲向了目标！
- en: 'But let’s not get too excited; that was the static version of the game, which
    is really easy. If you use our test function with `mode=''random''`, you’ll find
    some disappointment:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不要太过兴奋；那只是游戏的静态版本，实际上非常简单。如果你使用我们的测试函数并设置`mode='random'`，你可能会感到失望：
- en: '[PRE45]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is really interesting. Look carefully at the moves the network is making.
    The player starts off the game only two tiles to the right of the goal. If it
    *really* knew how to play the game, it would take the shortest path to the goal.
    Instead, it starts moving down and to the left, just like it would in the static
    game mode. It seems like the model just memorized the particular board it was
    trained on and didn’t generalize at all.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很有趣。仔细观察网络所做的动作。玩家一开始只在目标右侧两格的位置。如果它**真的**知道如何玩游戏，它就会选择最短路径到达目标。相反，它开始向下和向左移动，就像在静态游戏模式中一样。看起来这个模型只是记住了它训练过的特定棋盘，而没有进行任何泛化。
- en: Maybe we just need to train it with the game mode set to random, and then it
    would really learn? Try it. Retrain it with random mode. Maybe you’ll be luckier
    than us, but [figure 3.11](#ch03fig11) shows our loss plot with random mode and
    1,000 epochs. That doesn’t look pretty. There’s no sign that any significant learning
    is happening with random mode. (We won’t show these results, but the model *did*
    seem to learn how to play with “player” mode, where only the player is randomly
    placed on the grid.)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们只需要用随机模式训练它，然后它就会真正学会？试试看。用随机模式重新训练它。也许你比我们幸运，但[图3.11](#ch03fig11)显示了我们的随机模式损失图和1,000个epoch。这看起来并不好。没有任何迹象表明随机模式下有任何显著的学习发生。（我们不会展示这些结果，但模型*似乎*学会了如何用“玩家”模式玩游戏，在这种模式下，只有玩家被随机放置在网格上。）
- en: Figure 3.11\. The loss plot for Q-learning in random mode, which doesn’t show
    any signs of convergence.
  id: totrans-583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11。随机模式下的Q学习损失图，没有显示出任何收敛的迹象。
- en: '![](03fig11_alt.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig11_alt.jpg)'
- en: This is a big problem. Reinforcement learning won’t be worth anything if all
    it can do is learn how to memorize or weakly learn. But this is a problem that
    the DeepMind team faced, and one they solved.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大问题。如果强化学习只能学会如何记忆或弱学习，那么它将毫无价值。但这是DeepMind团队面临的问题，也是他们解决的问题。
- en: '3.3\. Preventing catastrophic forgetting: Experience replay'
  id: totrans-586
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 防止灾难性遗忘：经验重放
- en: We’re slowly building up our skills, and we want our algorithm to train on the
    harder variant of the game where all the board pieces are randomly placed on the
    grid for each new game. The algorithm can’t just memorize a sequence of steps
    to take, as before. It needs to be able to take the shortest path to the goal
    (without stepping into the pit) regardless of what the initial board configuration
    is. It needs to develop a more sophisticated representation of its environment.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在逐步提升我们的技能，并希望我们的算法能够在游戏难度更高的变体上进行训练，在这个变体中，每局新游戏时所有棋盘上的棋子都会随机放置在网格上。算法不能像以前那样仅仅记住一系列步骤。它需要能够找到通向目标的最短路径（而不踏入陷阱），无论初始的棋盘配置是什么。它需要发展出对其环境的更复杂表示。
- en: 3.3.1\. Catastrophic forgetting
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1. 灾难性遗忘
- en: 'The main problem we encountered in the previous section when we tried to train
    our model on random mode has a name: *catastrophic forgetting*. It’s actually
    a very important issue associated with gradient descent-based training methods
    in *online* training. Online training is what we’ve been doing: we backpropagate
    after each move as we play the game.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，当我们尝试在随机模式下训练我们的模型时遇到的主要问题有一个名字：*灾难性遗忘*。这实际上是与基于梯度下降的训练方法相关的非常重要的问题，尤其是在*在线*训练中。在线训练就是我们一直在做的事情：我们在玩游戏时每走一步都进行反向传播。
- en: Imagine that our algorithm is training on (learning Q values for) game 1 of
    [figure 3.12](#ch03fig12). The player is placed between the pit and the goal such
    that the goal is on the right and the pit is on the left. Using an epsilon-greedy
    strategy, the player takes a random move and by chance steps to the right and
    hits the goal. Great! The algorithm will try to learn that this state-action pair
    is associated with a high value by updating its weights in such a way that the
    output will more closely match the target value (i.e. via backpropagation).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们的算法正在训练（学习图3.12中游戏1的Q值）。玩家被放置在陷阱和目标之间，使得目标在右边，陷阱在左边。使用ε-贪婪策略，玩家随机移动，并偶然向右移动并击中目标。太好了！算法将尝试通过更新其权重，使输出更接近目标值（即通过反向传播）来学习这个状态-动作对与高值相关联。
- en: Figure 3.12\. The idea of catastrophic forgetting is that when two game states
    are very similar and yet lead to very different outcomes, the Q function will
    get “confused” and won’t be able to learn what to do. In this example, the catastrophic
    forgetting happens because the Q function learns from game 1 that moving right
    leads to a +1 reward, but in game 2, which looks very similar, it gets a reward
    of –1 after moving right. As a result, the algorithm forgets what it previously
    learned about game 1, resulting in essentially no significant learning at all.
  id: totrans-591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12。灾难性遗忘的想法是，当两个游戏状态非常相似但导致非常不同的结果时，Q函数会“困惑”，无法学习应该做什么。在这个例子中，灾难性遗忘发生是因为Q函数从游戏1中学习到向右移动会导致+1的奖励，但在看起来非常相似的游戏2中，向右移动后得到的是-1的奖励。结果，算法忘记了它之前关于游戏1学到的内容，导致几乎没有显著的学习。
- en: '![](03fig12_alt.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig12_alt.jpg)'
- en: Now game 2 is initialized and the player is again between the goal and pit,
    but this time the goal is on the *left* and the pit is on the right. Perhaps to
    our naive algorithm, the state *seems* very similar to the last game. Since last
    time moving right gave a nice positive reward, the player chooses to make one
    step to the right again, but this time it ends up in the pit and gets –1 reward.
    The player is thinking, “What is going on? I thought going to the right was the
    best decision based on my previous experience.” It may do backpropagation again
    to update its state-action value, but because this state-action is very similar
    to the last learned state-action, it may override its previously learned weights.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，游戏2已初始化，玩家再次位于目标和坑之间，但这次目标是*左边*，坑在右边。也许对我们这个简单的算法来说，状态 *看起来*与上一局游戏非常相似。由于上次向右移动给出了一个很好的正奖励，玩家选择再次向右迈出一步，但这次它最终掉进了坑里，得到了-1的奖励。玩家在想，“发生了什么事？我以为根据我的先前经验，向右走是最好的决定。”它可能再次进行反向传播来更新其状态-行动值，但由于这个状态-行动与上次学习的状态-行动非常相似，它可能会覆盖其先前学习的权重。
- en: This is the essence of catastrophic forgetting. There’s a push-pull between
    very similar state-actions (but with divergent targets) that results in this inability
    to properly learn anything. We generally don’t have this problem in the supervised
    learning realm, because we do randomized batch learning where we don’t update
    our weights until we’ve iterated through some random subset of training data and
    computed the sum or average gradient for the batch. This averages over the targets
    and stabilizes the learning.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是灾难性遗忘的本质。在非常相似的状态-行动（但具有不同的目标）之间存在推拉，这导致无法正确学习任何东西。我们通常在监督学习领域没有这个问题，因为我们进行随机批量学习，在我们迭代通过一些随机子集的训练数据并计算批次的梯度总和或平均值之前，我们不更新我们的权重。这平均了目标并稳定了学习。
- en: 3.3.2\. Experience replay
  id: totrans-595
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 经验回放
- en: Catastrophic forgetting is probably not something we have to worry about with
    the first variant of our game because the targets are always stationary, and indeed
    the model successfully learned how to play it. But with the random mode, it’s
    something we need to consider, and that is why we need to implement something
    called *experience replay*. Experience replay basically gives us batch updating
    in an online learning scheme. It’s not a big deal to implement
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们游戏的第一个变体，可能不需要担心灾难性遗忘的问题，因为目标总是静止的，而且模型确实成功地学会了如何玩。但是，在随机模式下，这是一个我们需要考虑的问题，这就是为什么我们需要实现一种称为*经验回放*的功能。经验回放基本上在在线学习方案中为我们提供了批量更新。实现它并不是什么大问题
- en: 'Here’s how experience replay works ([figure 3.13](#ch03fig13)):'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是经验回放的工作原理 ([图 3.13](#ch03fig13))：
- en: In state *s*, take action *a*, and observe the new state *s[t]*[+1] and reward
    *r[t]*[+1].
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态 *s* 中，采取行动 *a*，并观察新的状态 *s[t]*[+1] 和奖励 *r[t]*[+1]。
- en: Store this as a tuple (*s*, *a*, *s[t]*[+1], *r[t]*[+1]) in a list.
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 (*s*, *a*, *s[t]*[+1], *r[t]*[+1]) 作为元组存储在列表中。
- en: Continue to store each experience in this list until you have filled the list
    to a specific length (this is up to you to define).
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续将每个经验存储在这个列表中，直到你将列表填充到特定的长度（这由你定义）。
- en: Once the experience replay memory is filled, randomly select a subset (again,
    you need to define the subset size).
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦经验回放内存已满，随机选择一个子集（同样，你需要定义子集大小）。
- en: Iterate through this subset and calculate value updates for each subset; store
    these in a target array (such as *Y*) and store the state, *s*, of each memory
    in *X*.
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代这个子集并计算每个子集的价值更新；将这些存储在目标数组（如 *Y*）中，并将每个记忆的状态 *s* 存储在 *X* 中。
- en: Use *X* and *Y* as a mini-batch for batch training. For subsequent epochs where
    the array is full, just overwrite old values in your experience replay memory
    array.
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *X* 和 *Y* 作为批量训练的迷你批次。对于后续的周期，当数组已满时，只需覆盖经验回放内存数组中的旧值。
- en: 'Figure 3.13\. This is the general overview of experience replay, a method for
    mitigating a major problem with online training algorithms: catastrophic forgetting.
    The idea is to employ mini-batching by storing past experiences and then using
    a random subset of these experiences to update the Q-network, rather than using
    just the single most recent experience.'
  id: totrans-604
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.13\. 这是经验回放的一般概述，这是一种减轻在线训练算法主要问题的方法：灾难性遗忘。其想法是通过存储过去经验并使用这些经验的随机子集来更新Q网络，而不是仅使用最近的单一经验。
- en: '![](03fig13_alt.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig13_alt.jpg)'
- en: Thus, in addition to learning the action value for the action you just took,
    you’re also going to use a random sample of past experiences to train on, to prevent
    catastrophic forgetting.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了学习你刚刚采取的动作的动作值之外，你还将使用过去经验的一个随机样本进行训练，以防止灾难性遗忘。
- en: '[Listing 3.5](#ch03ex05) shows the same training algorithm from [listing 3.4](#ch03ex04),
    except with experience replay added. Remember, this time we’re training it on
    the harder variant of the game, where all the board pieces are randomly placed
    on the grid.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.5](#ch03ex05)显示了与[列表3.4](#ch03ex04)相同的训练算法，除了添加了经验重放。记住，这次我们在游戏的更难变体上进行训练，其中所有棋盘上的棋子都是随机放置在网格上的。'
- en: Listing 3.5\. DQN with experience replay
  id: totrans-608
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5\. 带有经验重放的DQN
- en: '[PRE46]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1*** Sets the total size of the experience replay memory'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置经验重放内存的总大小'
- en: '***2*** Sets the mini-batch size'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置小批量大小'
- en: '***3*** Creates the memory replay as a deque list'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将记忆重放作为一个deque列表创建'
- en: '***4*** Sets the maximum number of moves before game is over'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 设置游戏结束前的最大移动次数'
- en: '***5*** Selects an action using the epsilon-greedy strategy'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用ε-贪婪策略选择一个动作'
- en: '***6*** Computes Q values from the input state in order to select an action'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 从输入状态计算Q值以选择一个动作'
- en: '***7*** Creates an experience of state, reward, action, and the next state
    as a tuple'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将状态、奖励、动作和下一个状态作为一个元组创建经验'
- en: '***8*** Adds the experience to the experience replay list'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 将经验添加到经验重放列表中'
- en: '***9*** If the replay list is at least as long as the mini-batch size, begins
    the mini-batch training'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 如果重放列表的长度至少与小批量大小相同，则开始小批量训练'
- en: '***10*** Randomly samples a subset of the replay list'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 随机从重放列表中抽取一个子集'
- en: '***11*** Separates out the components of each experience into separate mini-batch
    tensors'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 将每个经验的不同组成部分分离成单独的小批量张量'
- en: '***12*** Recomputes Q values for the mini-batch of states to get gradients'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 重新计算状态小批量的Q值以获取梯度'
- en: '***13*** Computes Q values for the mini-batch of next states, but doesn’t compute
    gradients'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13*** 计算下一个状态小批量的Q值，但不计算梯度'
- en: '***14*** Computes the target Q values we want the DQN to learn'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14*** 计算DQN想要学习的目标Q值'
- en: '***15*** If the game is over, resets status and mov number'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***15*** 如果游戏结束，重置状态和移动次数'
- en: In order to store the agent’s experiences, we used a data structure called a
    *deque* in Python’s built-in collections library. It’s basically a list that you
    can set a maximum size on, so that if you try to append to the list and it is
    already full, it will remove the first item in the list and add the new item to
    the end of the list. This means new experiences replace the oldest experiences.
    The experiences themselves are tuples of `(state1, reward, action, state2, done)`
    that we append to the `replay` deque.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储代理的经验，我们在Python内置的collections库中使用了名为*deque*的数据结构。它基本上是一个可以设置最大大小的列表，所以如果你尝试向列表中添加内容而它已经满了，它将移除列表中的第一个项目，并将新项目添加到列表的末尾。这意味着新的经验会替换最老的经验。这些经验本身是`(state1,
    reward, action, state2, done)`的元组，我们将它们追加到`replay` deque中。
- en: The major difference with experience replay training is that we train with mini-batches
    of data when our replay list is full. We randomly select a subset of experiences
    from the replay, and we separate out the individual experience components into
    `state1_batch`, `reward_batch`, `action_batch`, and `state2_batch`. For example,
    `state1 _batch` is of dimensions `batch_size` × 64, or 100 × 64 in this case.
    And `reward_batch` is just a 100-length vector of integers. We follow the same
    training formula as we did earlier with fully online training, but now we’re dealing
    with mini-batches. We use the tensor `gather` method to subset the `Q1` tensor
    (a 100 × 4 tensor) by the action indices so that we only select the Q values associated
    with actions that were actually chosen, resulting in a 100-length vector.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 与经验重放训练的主要区别在于，当我们的重放列表已满时，我们使用数据的小批量进行训练。我们从重放中随机选择一个经验子集，并将单个经验组件分离成`state1_batch`、`reward_batch`、`action_batch`和`state2_batch`。例如，`state1_batch`的维度是`batch_size`
    × 64，或者在这个例子中是100 × 64。而`reward_batch`只是一个100个长度的整数向量。我们遵循与之前完全在线训练相同的训练公式，但现在我们处理的是小批量。我们使用张量`gather`方法通过动作索引对`Q1`张量（一个100
    × 4的张量）进行子集选择，以便我们只选择与实际选择的动作相关的Q值，从而得到一个100长度的向量。
- en: Notice that the target Q value, `Y = reward_batch + gamma * ((1 - done_batch)
    * torch.max(Q2,dim=1)[0])`, uses `done_batch` to set the right side to 0 if the
    game is done. Remember, if the game is over after taking an action, which we call
    a *terminal state*, there is no next state to take the maximum Q value on, so
    the target just becomes the reward, *r[t]*[+1]. The `done` variable is a Boolean,
    but we can do arithmetic on it as if it were a 0 or 1 integer, so we just take
    `1 - done` so that if `done = True`, `1 - done = 0`, and it sets the right-side
    term to 0.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，目标 Q 值 `Y = reward_batch + gamma * ((1 - done_batch) * torch.max(Q2,dim=1)[0])`
    使用 `done_batch` 来设置如果游戏结束，右侧为 0。记住，如果采取行动后游戏结束，我们称之为*终止状态*，就没有下一个状态来取最大 Q 值，所以目标就变成了奖励，*r[t]*[+1]。`done`
    变量是一个布尔值，但我们可以像对 0 或 1 整数进行算术运算一样对其进行算术运算，所以我们只需取 `1 - done`，这样如果 `done = True`，则
    `1 - done = 0`，并将右侧项设置为 0。
- en: We trained for 5,000 epochs this time, since it’s a more difficult game, but
    otherwise the Q-network model is the same as before. When we test the algorithm,
    it seems to play most of the games correctly. We wrote an additional testing script
    to see what percentage of games it wins out of 1,000 plays.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们训练了 5,000 个周期，因为这是一个更难的游戏，但除此之外，Q 网络模型与之前相同。当我们测试算法时，它似乎大多数游戏都玩得正确。我们编写了一个额外的测试脚本，以查看在
    1,000 次游戏中它赢得了多少百分比。
- en: Listing 3.6\. Testing the performance with experience replay
  id: totrans-629
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6\. 使用经验回放测试性能
- en: '[PRE47]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: When we run [listing 3.6](#ch03ex06) on our trained model (trained for 5,000
    epochs), we get about 90% accuracy. Your accuracy may be slightly better or worse.
    This certainly suggests it has learned *something* about how to play the game,
    but it’s not exactly what we would expect if the algorithm really knew what it
    was doing (although you could probably improve the accuracy with a much longer
    training time). Once you actually know how to play, you should be able to win
    every single game.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在训练了 5,000 个周期的模型上运行[列表 3.6](#ch03ex06)时，我们得到了大约 90% 的准确率。你的准确率可能略好或略差。这确实表明它已经学会了*一些*关于如何玩游戏的知识，但如果我们认为算法真的知道它在做什么，这并不是我们预期的（尽管你可能会通过更长的训练时间来提高准确率）。一旦你实际上知道如何玩游戏，你应该能够赢得每一场比赛。
- en: There’s a small caveat that some of the initialized games may actually be impossible
    to win, so the win percentage may never reach 100%; there is no logic preventing
    the goal from being in the corner, stuck behind a wall and pit, making the game
    unwinnable. The Gridworld game engine does prevent most of the impossible board
    configurations, but a small number can still get through. Not only does this mean
    we can’t win every game, but it also means the learning will be mildly corrupted,
    since it will attempt to follow a strategy that normally would work but fails
    for an unwinnable game. We wanted to keep the game logic simple to focus on illustrating
    the concepts so we did not program in the sophisticated logic needed to ensure
    100% winnable games.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个小的警告，一些初始化的游戏实际上可能根本无法获胜，所以胜率可能永远达不到 100%；没有逻辑可以防止目标在角落，卡在墙壁和坑里，使游戏无法获胜。Gridworld
    游戏引擎确实阻止了大多数不可能的棋盘配置，但仍然有一小部分可以穿过。这不仅意味着我们无法赢得每一场比赛，而且这也意味着学习将受到轻微的破坏，因为它将尝试遵循通常可以工作但无法在无法获胜的游戏中失败的策略。我们想要保持游戏逻辑简单，以便专注于说明概念，所以我们没有编写确保
    100% 可获胜游戏的复杂逻辑。
- en: There’s also another reason we’re being held back from getting into the 95%
    + accuracy territory. Let’s look at our loss plot, shown in [figure 3.14](#ch03fig14)
    showing our running average loss (yours may vary significantly).
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个原因让我们无法达到 95% + 的准确率。让我们看看我们的损失图，如图 [图 3.14](#ch03fig14) 所示，显示我们的运行平均损失（你的可能差异很大）。
- en: Figure 3.14\. The DQN loss plot after implementing experience replay, which
    shows a clearly down-trending loss, but it’s still very noisy.
  id: totrans-634
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.14\. 实施经验回放后的 DQN 损失图，显示了明显下降的损失，但仍然非常嘈杂。
- en: '![](03fig14_alt.jpg)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 的替代图片](03fig14_alt.jpg)'
- en: In the loss in [figure 3.14](#ch03fig14), you can see it’s definitely trending
    downward, but it looks pretty unstable. This is the type of plot you’d be a bit
    surprised to see in a supervised learning problem, but it’s quite common in bare
    DRL. The experience replay mechanism helps with training stabilization by reducing
    catastrophic forgetting, but there are other related sources of instability.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3.14](#ch03fig14)中的损失中，你可以看到它确实呈下降趋势，但看起来相当不稳定。这种图表在监督学习问题中可能会让你有些惊讶，但在裸
    DRL 中却很常见。经验回放机制通过减少灾难性遗忘来帮助训练稳定，但还有其他相关的不稳定来源。
- en: 3.4\. Improving stability with a target network
  id: totrans-637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 改进稳定性使用目标网络
- en: So far, we’ve been able to successfully train a deep reinforcement learning
    algorithm to learn and play Gridworld with both a deterministic static initialization
    and a slightly harder version where the player is placed randomly on the board
    each game. Unfortunately, even though the algorithm appears to learn how to play,
    it is quite possible it is just memorizing all the possible board configurations,
    since there aren’t that many on a 4 × 4 board. The hardest variant of the game
    is where the player, goal, pit, and wall are all initialized randomly each game,
    making it much more difficult for the algorithm to memorize. This ought to enforce
    some amount of actual learning, but as you saw, we’re still experiencing difficulty
    with learning this variant; we’re getting very noisy loss plots. To help address
    this, we’ll add another dimension to the updating rule that will smooth out the
    value updates.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成功训练了一个深度强化学习算法，使其能够学习和玩Gridworld，无论是使用确定性静态初始化，还是在玩家每次游戏时随机放置在板上的稍微困难版本。不幸的是，尽管算法看起来学会了如何玩游戏，但它很可能只是在记忆所有可能的板面配置，因为在一个4×4的板上并没有那么多。游戏中难度最大的变体是玩家、目标、坑和墙在每次游戏中都随机初始化，这使得算法记忆起来更加困难。这应该会强制执行一定程度的实际学习，但正如你所看到的，我们在学习这个变体时仍然遇到了困难；我们得到了非常嘈杂的损失图。为了解决这个问题，我们将在更新规则中添加另一个维度，以平滑价值更新。
- en: 3.4.1\. Learning instability
  id: totrans-639
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1. 学习不稳定性
- en: One potential problem that DeepMind identified when they published their deep
    Q-network paper was that if you keep updating the Q-network’s parameters after
    each move, you might cause instabilities to arise. The idea is that since the
    rewards may be sparse (we only give a significant reward upon winning or losing
    the game), updating on every single step, where most steps don’t get any significant
    reward, may cause the algorithm to start behaving erratically.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind在发布他们的深度Q网络论文时确定的一个潜在问题是，如果你在每次移动后都更新Q网络的参数，你可能会引起不稳定性。想法是，由于奖励可能很稀疏（我们只在赢得或输掉游戏时给予显著的奖励），在大多数步骤中不会得到任何显著奖励的情况下，每一步都进行更新可能会使算法开始表现出异常行为。
- en: For example, the Q-network might predict a high value for the “up” action in
    some state; if it moves up and by chance lands on the goal and wins, we update
    the Q-network to reflect the fact that it was rewarded +10\. The next game, however,
    it thinks “up” is a really fantastic move and predicts a high Q value, but then
    it moves up and gets a –10 reward, so we update and now it thinks “up” is not
    so great after all. Then, a few games later moving up leads to winning again.
    You can see how this might lead to a kind of oscillatory behavior, where the predicted
    Q value never settles on a reasonable value but just keeps getting jerked around.
    This is very similar to the catastrophic forgetting problem.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Q网络可能在某些状态下预测“向上”动作具有高价值；如果它向上移动并且偶然落在目标上并获胜，我们将更新Q网络以反映它获得了+10的奖励。然而，在下一场比赛中，它认为“向上”是一个非常好的动作，并预测了一个高的Q值，但随后它向上移动并得到了-10的奖励，所以我们更新后，它认为“向上”并不那么好。然后，几场比赛后向上移动又导致获胜。你可以看到这可能导致一种振荡行为，其中预测的Q值永远不会稳定在一个合理的值上，而只是不断地被拉扯。这与灾难性遗忘问题非常相似。
- en: 'This is not just a theoretical issue—it’s something that DeepMind observed
    in their own training. The solution they devised is to duplicate the Q-network
    into two copies, each with its own model parameters: the “regular” Q-network and
    a copy called the *target network* (symbolically denoted ![](qcirc.jpg)-network,
    read “Q hat”). The target network is identical to the Q-network at the beginning,
    before any training, but its own parameters lag behind the regular Q-network in
    terms of how they’re updated.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一个理论问题——这是DeepMind在他们的训练中观察到的问题。他们设计的解决方案是将Q网络复制成两个副本，每个副本都有自己的模型参数：一个是“常规”Q网络，另一个是称为*目标网络*的副本（符号表示为![](qcirc.jpg)-network，读作“Q
    hat”）。目标网络在开始训练之前与Q网络相同，但其参数在更新方面落后于常规Q网络。
- en: 'Let’s run through the sequence of events again, with the target network in
    play (we’ll leave out the details of experience replay):'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下事件序列，这次有目标网络参与（我们将省略经验重放的细节）：
- en: Initialize the Q-network with parameters (weights) *θ**[Q]* (read “theta Q”).
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用参数（权重）*θ**[Q]*（读作“theta Q”）初始化Q网络。
- en: Initialize the target network as a copy of the Q-network, but with separate
    parameters *θ**[T]* (read “theta T”), and set *θ**[T]* = *θ**[Q]*.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标网络初始化为Q网络的副本，但具有单独的参数*θ**[T]*（读作“theta T”），并将*θ**[T]* = *θ**[Q]*。
- en: Use the epsilon-greedy strategy with the Q-network’s Q values to select action
    *a*.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Q网络的Q值以epsilon-greedy策略选择动作*a*。
- en: Observe the reward and new state *r[t]*[+1],*s[t]*[+1].
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察奖励和新状态*r[t]*[+1]，*s[t]*[+1]。
- en: The target network’s Q value will be set to *r[t]*[+1] if the episode has just
    been terminated (i.e., the game was won or lost) or to *r[t]*[+1] + *γ*max*Q**[θ]**[r]*(*S[t]*[+1])
    otherwise (notice the use of the target network here).
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果剧集刚刚结束（即游戏赢了或输了），则将目标网络的Q值设置为*r[t]*[+1]，否则设置为*r[t]*[+1] + *γ*max*Q**[θ]**[r]*(*S[t]*[+1])（注意这里使用了目标网络）。
- en: Backpropagate the target network’s Q value through the Q-network (not the target
    network).
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Q网络（而不是目标网络）反向传播目标网络的Q值。
- en: Every *C* number of iterations, set *θ**[T]* = *θ**[Q]* (i.e., set the target
    network’s parameters equal to the Q-network’s parameters).
  id: totrans-650
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每*C*次迭代，将*θ**[T]* = *θ**[Q]*（即，将目标网络的参数设置为Q网络的参数）。
- en: Notice from [figure 3.15](#ch03fig15) that the only time we use the target network,
    ![](qcirc.jpg), is to calculate the target Q value for backpropagation through
    the Q-network. The idea is that we update the main Q-network’s parameters on each
    training iteration, but we decrease the effect that recent updates have on the
    action selection, hopefully improving stability.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图3.15](#ch03fig15)中可以看出，我们唯一使用目标网络的时间是计算反向传播通过Q网络的目标Q值。想法是我们更新主Q网络的参数在每个训练迭代中，但我们减少最近更新对动作选择的影响，希望提高稳定性。
- en: Figure 3.15\. This is the general overview for Q-learning with a target network.
    It’s a fairly straightforward extension of the normal Q-learning algorithm, except
    that you have a second Q-network called the target network whose predicted Q values
    are used to backpropagate through and train the main Q-network. The target network’s
    parameters are not trained, but they are periodically synchronized with the Q-network’s
    parameters. The idea is that using the target network’s Q values to train the
    Q-network will improve the stability of the training.
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15。这是具有目标网络的Q学习的总体概述。它是对正常Q学习算法的一个相当直接的扩展，除了你还有一个名为目标网络的第二个Q网络，其预测的Q值用于反向传播并通过训练主Q网络。目标网络的参数不进行训练，但它们会定期与Q网络的参数同步。这个想法是使用目标网络的Q值来训练Q网络将提高训练的稳定性。
- en: '![](03fig15_alt.jpg)'
  id: totrans-653
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig15_alt.jpg)'
- en: The code is getting a bit long now, with both experience replay and a target
    network, so we’ll just look at a portion of the full code here in the book. We’ll
    leave it to you to check out the book’s GitHub repository where you’ll find all
    the code for this chapter.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 代码现在有点长，既有经验回放又有目标网络，所以我们在这里只看完整代码的一部分。我们将留给你去检查GitHub仓库中的书籍，在那里你可以找到本章的所有代码。
- en: The following code is identical to [listing 3.5](#ch03ex05) except for a few
    lines that add in the target network capability.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码与[列表3.5](#ch03ex05)完全相同，除了添加了目标网络功能的一些行。
- en: Listing 3.7\. Target network
  id: totrans-656
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7。目标网络
- en: '[PRE48]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '***1*** Creates a second model by making an identical copy of the original
    Q-network model'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 通过创建原始Q网络模型的相同副本来创建第二个模型'
- en: '***2*** Copies the parameters of the original model'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 复制原始模型的参数'
- en: '***3*** Synchronizes the frequency parameter; every 50 steps we will copy the
    parameters of model into model2'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 同步频率参数；每50步我们将模型参数复制到model2'
- en: '***4*** (Code omitted) Uses the same other settings as in [listing 3.5](#ch03ex05)'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4***（代码省略）使用与[列表3.5](#ch03ex05)相同的其他设置'
- en: The target network is simply a lagged copy of the main DQN. Each PyTorch model
    has a `state_dict()` method that returns all of its parameters organized in a
    dictionary. We use Python’s built-in copy module to duplicate the PyTorch model
    data structure, and then we use the `load_state_dict` method on `model2` to ensure
    that it has copied the parameters of the main DQN.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络只是主DQN的滞后副本。每个PyTorch模型都有一个`state_dict()`方法，它返回所有参数组织在字典中。我们使用Python内置的copy模块来复制PyTorch模型数据结构，然后我们使用`load_state_dict`方法在`model2`上确保它已复制主DQN的参数。
- en: Next we include the full training loop, which is mostly the same as [listing
    3.5](#ch03ex05) except that we use `model2` when computing the maximum Q value
    for the next state. We also include a couple of lines of code to copy the parameters
    from the main model to `model2` every 50 iterations.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们包括完整的训练循环，它基本上与[列表3.5](#ch03ex05)相同，只是在计算下一个状态的最大Q值时使用`model2`。我们还包括了几行代码，每50次迭代就将主模型的参数复制到`model2`。
- en: Listing 3.8\. DQN with experience replay and target network
  id: totrans-664
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8\. 带有经验回放和目标网络的DQN
- en: '[PRE49]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***1*** Sets the update frequency for synchronizing the target model parameters
    to the main DQN'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置同步目标模型参数到主要DQN的更新频率'
- en: '***2*** Uses the target network to get the maximum Q value for the next state'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用目标网络获取下一个状态的最大Q值'
- en: '***3*** Copies the main model parameters to the target network'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将主要模型参数复制到目标网络'
- en: When we plot the loss for a target network approach with experience replay ([figure
    3.16](#ch03fig16)), we still get a noisy loss plot, but it is significantly less
    noisy and clearly down-trending. You should try to experiment with the hyperparameters,
    such as the experience replay buffer size, the batch size, the target network
    update frequency, and the learning rate. The performance can be quite sensitive
    to these hyperparameters.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制带有经验回放的目标网络方法的损失图（[图3.16](#ch03fig16)），我们仍然得到一个有噪声的损失图，但它明显噪声更少，并且明显呈下降趋势。你应该尝试调整超参数，例如经验回放缓冲区大小、批量大小、目标网络更新频率和学习率。性能对这些超参数可能非常敏感。
- en: Figure 3.16\. The DQN loss plot after including a target network to stabilize
    training. This shows a much faster training convergence than without the target
    network, but it has noticeable spikes of error when the target network synchronizes
    with the main DQN.
  id: totrans-670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16\. 包含目标网络以稳定训练后的DQN损失图。这显示了比没有目标网络时更快的训练收敛速度，但目标网络与主要DQN同步时会出现明显的错误峰值。
- en: '![](03fig16_alt.jpg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig16_alt.jpg)'
- en: 'When we test the trained model on 1,000 games, we get about a 3% improvement
    in win percentage compared to training without a target network. We’re getting
    a top accuracy of around 95%, which we think is probably the maximal accuracy
    given the limitations of this environment (i.e., the possibility of unwinnable
    states). We’re only training up to 5,000 epochs, where each epoch is a single
    game. The number of possible game configurations (the size of the state-space)
    is approximately 16 × 15 × 14 ×13 = 43,680 (since there are 16 possible positions
    the agent can be in on a 4 × 4 grid, and then 15 possible configurations for the
    wall, since the agent and wall can’t be overlapping in space, etc.), so we’re
    only sampling about ![](pg086.jpg) of the total number of possible starting game
    states. If the model can successfully play games it has never seen before, then
    we have some confidence it has generalized. If you’re getting good results with
    the 4 × 4 board, you should try training an agent to play on a 5 × 5 board or
    larger by changing the size parameter when creating the Gridworld game instance:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在1,000场比赛中测试训练好的模型时，与没有目标网络的训练相比，胜率提高了大约3%。我们达到了大约95%的顶级准确率，我们认为这可能是考虑到这个环境（即无法获胜的状态的可能性）的限制下的最大准确率。我们只训练到5,000个epoch，其中每个epoch代表一场游戏。可能的比赛配置数量（状态空间的大小）大约是16
    × 15 × 14 × 13 = 43,680（因为代理在4 × 4网格中可能有16个可能的位置，然后是15个可能的墙壁配置，因为代理和墙壁在空间上不能重叠等），所以我们只采样了![](pg086.jpg)的总可能起始游戏状态数。如果模型可以成功玩从未见过的游戏，那么我们有信心它已经泛化。如果你在4
    × 4的棋盘上得到好的结果，你应该尝试训练一个代理在5 × 5的棋盘或更大的棋盘上玩游戏，通过在创建Gridworld游戏实例时更改大小参数来实现：
- en: '[PRE50]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|  |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**DeepMind’s deep Q-network**'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepMind的深度Q网络**'
- en: Believe it or not, but in this chapter we basically built the deep Q-network
    (DQN) that DeepMind introduced in 2015 and that learned to play old Atari games
    at superhuman performance levels. DeepMind’s DQN used an epsilon-greedy action-selection
    strategy, experience replay, and a target network. Of course, the details of our
    implementation are different, since we are playing a custom Gridworld game and
    DeepMind was training on raw pixels from real video games. For example, one difference
    worth noting is that they actually input the last 4 frames of a game into their
    Q-network. That’s because a single frame in a video game is not enough information
    to determine the speed and direction of the objects in the game, which is important
    when deciding what action to take.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，但在这个章节中，我们基本上构建了DeepMind在2015年引入的深度Q网络（DQN），并且它学会了以超人类的表现水平玩古老的Atari游戏。DeepMind的DQN使用了ε-贪婪动作选择策略、经验回放和目标网络。当然，我们实现的细节是不同的，因为我们正在玩一个定制的网格世界游戏，而DeepMind是在真实视频游戏的原始像素上训练的。例如，一个值得注意的差异是他们实际上将游戏的最后4帧输入到他们的Q网络中。这是因为视频游戏中的单帧信息不足以确定游戏对象的速度和方向，这在决定采取什么动作时很重要。
- en: You can read more about the specifics of DeepMind’s DQN by searching for their
    paper “Human-level control through deep reinforcement learning.” One thing to
    note is that they used a neural network architecture consisting of two convolutional
    layers followed by two fully connected layers. In our case we used three fully
    connected layers. It would be a worthwhile experiment to build a model with a
    convolutional layer, and try training it with Gridworld. One huge advantage of
    convolutional layers is that they are independent of the size of the input tensor.
    When we used a fully connected layer, for example, we had to make the first dimension
    64—we used a 64 × 164 parameter matrix for the first layer. A convolutional layer,
    however, can be applied to input data of any length. This would allow you to train
    a model on a 4 × 4 grid and see if it generalizes enough to be able to play on
    a 5 × 5 or bigger grid. Go ahead, try it!
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过搜索他们的论文“通过深度强化学习实现人类水平控制”来了解更多关于DeepMind的DQN的具体信息。有一点需要注意，他们使用了一个由两个卷积层和两个全连接层组成的神经网络架构。在我们的情况下，我们使用了三个全连接层。构建一个包含卷积层并尝试在网格世界中训练它的模型将是一个有价值的实验。卷积层的一个巨大优势是它们与输入张量的大小无关。例如，当我们使用全连接层时，我们必须将第一维设置为64——我们使用了64
    × 164参数矩阵的第一层。然而，卷积层可以应用于任何长度的输入数据。这将允许你在4 × 4的网格上训练一个模型，并看看它是否足够泛化，能够在5 × 5或更大的网格上玩游戏。试试看吧！
- en: '|  |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.5\. Review
  id: totrans-679
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 复习
- en: We’ve covered a lot in this chapter, and once again we’ve smuggled in a lot
    of fundamental reinforcement learning concepts. We could have pushed a bunch of
    academic definitions in your face to start, but we resisted the temptation and
    decided to get to coding as quickly as possible. Let’s review what we’ve accomplished
    and fill in a few terminological gaps.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了大量的内容，并且再次引入了许多基本的强化学习概念。我们本可以一开始就向你们展示一大堆学术定义，但我们抵制了诱惑，决定尽可能快地开始编码。让我们回顾一下我们已经取得的成果，并填补一些术语上的空白。
- en: In this chapter we covered a particular RL algorithm called Q-learning. Q-learning
    has nothing to do with deep learning or neural networks on its own; it is an abstract
    mathematical construct. Q-learning refers to solving a control task by learning
    a function called a Q function. You give the Q function a state (e.g., a game
    state) and it predicts how valuable all the possible actions are that you could
    take given the input state, and we call these value predictions Q values. You
    decide what to do with these Q values. You might decide to take the action that
    corresponds to the highest Q value (a greedy approach), or you might opt for a
    more sophisticated selection process. As you learned in [chapter 2](kindle_split_011.html#ch02),
    you have to balance exploration (trying new things) versus exploitation (taking
    the best action you know of). In this chapter we used the standard epsilon-greedy
    approach to select actions, where we initially take random actions to explore,
    and then progressively switch our strategy to taking the highest value actions.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种名为Q学习的特定强化学习算法。Q学习本身与深度学习或神经网络无关；它是一个抽象的数学结构。Q学习指的是通过学习一个称为Q函数的函数来解决控制任务。你给Q函数一个状态（例如，游戏状态），它预测在给定输入状态下你可以采取的所有可能动作的价值，我们称这些价值预测为Q值。你决定如何处理这些Q值。你可能决定采取与最高Q值相对应的动作（贪婪方法），或者你可能选择一个更复杂的决策过程。正如你在[第二章](kindle_split_011.html#ch02)中学到的，你必须平衡探索（尝试新事物）与利用（采取已知最佳动作）。在本章中，我们使用了标准的ε-贪婪方法来选择动作，其中我们最初采取随机动作以进行探索，然后逐步将我们的策略转向采取价值最高的动作。
- en: The Q function must be learned from data. The Q function has to learn how to
    make accurate Q value predictions of states. The Q function could be anything
    really—anything from an unintelligent database to a complex deep learning algorithm.
    Since deep learning is the best class of learning algorithms we have at the moment,
    we employed neural networks as our Q functions. This means that “learning the
    Q function” is the same as training a neural network with backpropagation.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数必须从数据中学习。Q函数必须学习如何准确预测状态的价值。Q函数可以是任何东西——从无智能数据库到复杂的深度学习算法。由于深度学习是我们目前拥有的最佳学习算法类别，我们采用了神经网络作为我们的Q函数。这意味着“学习Q函数”等同于使用反向传播训练神经网络。
- en: 'One important concept about Q-learning that we held back until now is that
    it is an *off-policy* algorithm, in contrast to an *on-policy* algorithm. You
    already know what a policy is from the last chapter: it’s the strategy an algorithm
    uses to maximize rewards over time. If a human is learning to play Gridworld,
    they might employ a policy that first scouts all possible paths toward the goal
    and then selects the one that is shortest. Another policy might be to randomly
    take actions until you land on the goal.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Q学习的一个重要概念，我们一直推迟到今天才介绍，那就是它是一个**离线策略**算法，与**在线策略**算法相对。你从上一章已经知道什么是策略：它是算法用来最大化随时间奖励的策略。如果人类在学习玩网格世界，他们可能会采用一种策略，首先侦察所有通向目标的可能路径，然后选择最短的那条。另一种策略可能是随机采取动作，直到你到达目标。
- en: An off-policy reinforcement learning algorithm like Q-learning means that the
    choice of policy does not affect the ability to learn accurate Q values. Indeed,
    our Q-network could learn accurate Q values if we selected actions at random;
    eventually it would experience a number of winning and losing games and infer
    the values of states and actions. Of course, this is terribly inefficient, but
    the policy matters only insofar as it helps us learn with the least amount of
    data. In contrast, an on-policy algorithm will explicitly depend on the choice
    of policy or will directly aim at learning a policy from the data. In other words,
    in order to train our DQN, we need to collect data (experiences) from the environment,
    and we could do this using any policy, so DQN is off-policy. In contrast, an on-policy
    algorithm learns a policy while simultaneously using the same policy to collect
    experiences for training itself.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Q学习的离线强化学习算法意味着策略的选择不会影响学习准确Q值的能力。确实，如果我们在随机选择动作，我们的Q网络可以学习到准确的Q值；最终它会经历许多输赢游戏，并推断出状态和动作的价值。当然，这非常低效，但策略只有在它帮助我们用最少的数据进行学习时才有意义。相比之下，在线算法将明确依赖于策略的选择，或者将直接从数据中学习策略。换句话说，为了训练我们的DQN，我们需要从环境中收集数据（经验），我们可以使用任何策略来做这件事，所以DQN是离线的。相比之下，在线算法在学习策略的同时，同时使用相同的策略来收集用于训练自己的经验。
- en: Another key concept we’ve saved until now is the notion of *model-based* versus
    *model-free* algorithms. To make sense of this, we first need to understand what
    a model is. We use this term informally to refer to a neural network, and it’s
    often used to refer to any kind of statistical model, others being a linear model
    or a Bayesian graphical model. In another context, we might say a model is a mental
    or mathematical representation of how something works in “the real world.” If
    we understand exactly how something works (i.e., what it’s composed of and how
    those components interact) then we can not only explain data we’ve already seen,
    but we can predict data we haven’t yet seen.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直保留到现在的另一个关键概念是“基于模型”与“无模型”算法的概念。为了理解这一点，我们首先需要了解什么是模型。我们非正式地使用这个术语来指代神经网络，它通常用来指代任何类型的统计模型，其他包括线性模型或贝叶斯图模型。在另一个上下文中，我们可能会说模型是“现实世界”中某物如何工作的心理或数学表示。如果我们确切地了解某物是如何工作的（即它由什么组成以及这些组件如何相互作用），我们不仅能够解释我们已经看到的数据，而且能够预测我们尚未看到的数据。
- en: For example, weather forecasters build very sophisticated models of the climate
    that take into account many relevant variables, and they’re constantly measuring
    real-world data. They can use their models to predict the weather to some degree
    of accuracy. There’s an almost cliché statistics mantra that “all models are wrong,
    but some are useful,” meaning that it is impossible to build a model that 100%
    corresponds to reality; there will always be data or relationships that we’re
    missing. Nonetheless, many models capture enough truth about a system we’re interested
    in that they’re useful for explanation and prediction.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，天气预报员构建了非常复杂的气候模型，这些模型考虑了许多相关变量，并且他们不断地测量现实世界的数据。他们可以使用他们的模型在一定程度上预测天气。有一个几乎成了陈词滥调的统计学格言：“所有模型都是错误的，但有些是有用的”，这意味着不可能构建一个100%符合现实的模型；我们总会遗漏一些数据或关系。尽管如此，许多模型捕捉了我们感兴趣的系统中的足够真相，以至于它们对于解释和预测是有用的。
- en: If we could build an algorithm that could figure out how Gridworld works, it
    would have inferred a model of Gridworld, and it would be able to play it perfectly.
    In Q-learning, all we gave the Q-network was a numpy tensor. It had no *a priori*
    model of Gridworld, but it still learned to play by trial and error. We did not
    task the Q-network with figuring out how Gridworld works; its only job was to
    predict expected rewards. Hence, Q-learning is a model-free algorithm.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够构建一个能够弄清楚Gridworld如何工作的算法，它就会推断出一个Gridworld的模型，并且能够完美地玩它。在Q-learning中，我们给Q网络提供的只是一个numpy张量。它没有Gridworld的先验模型，但它仍然通过试错学会了如何玩。我们没有要求Q网络弄清楚Gridworld是如何工作的；它的唯一任务是预测预期的奖励。因此，Q-learning是一个无模型算法。
- en: As the human architects of algorithms, we may be able to engineer in some of
    our own domain knowledge about a problem as a model to optimize our problem. We
    could then supply this model to a learning algorithm and let it figure out the
    details. This would be a model-based algorithm. For example, most chess-playing
    algorithms are model-based; they know the rules of how chess works and what the
    result of taking certain moves will be. The only part that isn’t known (and that
    we’d want the algorithm to figure out) is what sequence of moves will win the
    game. With a model in hand, the algorithm can make long-term plans in order to
    achieve its aim.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 作为算法的人类建筑师，我们可能能够将一些关于问题的领域知识作为模型嵌入到我们的问题中，以优化我们的问题。然后我们可以将这个模型提供给学习算法，让它弄清楚细节。这将是一个基于模型的算法。例如，大多数下棋算法都是基于模型的；它们知道棋的规则以及采取某些移动的结果将会是什么。唯一未知的部分（我们希望算法能够弄清楚）是赢得游戏的移动序列。有了模型在手，算法可以制定长期计划以实现其目标。
- en: In many cases, we want to employ algorithms that can progress from being model-free
    to planning with a model. For example, a robot learning how to walk may start
    to learn by trial and error (model-free), but once it has figured out the basics
    of walking, it can start to infer a model of its environment and then plan a sequence
    of steps to get from point A to B (model-based). We’ll continue to explore on-policy,
    off-policy, model-based, and model-free algorithms in the rest of the book. In
    the next chapter we’ll look at an algorithm that will help us build a network
    that can approximate the policy function.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们希望使用能够从无模型到基于模型进行规划的算法。例如，一个学习如何行走的机器人可能一开始是通过试错（无模型）来学习的，但一旦它掌握了行走的基础，它就可以开始推断其环境的模型，然后规划一系列步骤从A点到B点（基于模型）。我们将在本书的其余部分继续探讨按策略、离策略、基于模型和无模型算法。在下一章中，我们将探讨一个可以帮助我们构建可以近似策略函数的网络算法。
- en: Summary
  id: totrans-690
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: A *state-space* is the set of all possible states that the environment can be
    in. Usually the states are encoded as tensors, so the state space may be a vector
    of type ![](icon-r.jpg)*^n* or a matrix in ![](icon-r.jpg)*^n*^×*^m*.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *状态空间* 是环境可能处于的所有可能状态的集合。通常，状态被编码为张量，因此状态空间可能是一个类型为 ![](icon-r.jpg)*^n* 的向量或一个类型为
    ![](icon-r.jpg)*^n*^×*^m* 的矩阵。
- en: An *action-space* is the set of all possible actions given a state; for example,
    the action space for the game chess would be the set of all legal moves given
    some state of the game.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动作空间* 是给定一个状态时所有可能动作的集合；例如，棋盘游戏的动作空间将是给定某些游戏状态的所有合法移动的集合。'
- en: A *state-value* is the expected sum of discounted rewards for a state given
    we follow some policy. If a state has a high state-value, that means that starting
    from this state will likely lead to high rewards.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *状态值* 是给定一个状态并遵循某些策略时，期望的折现奖励的总和。如果一个状态具有高的状态值，这意味着从这个状态开始很可能会带来高的奖励。
- en: An *action-value* is the expected rewards for taking an action in a particular
    state. It is the value of a state-action pair. If you know the action-values for
    all possible actions for a state, you can decide to take the action with the highest
    action-value, and you would expect to receive the highest reward as a result.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *动作值* 是在特定状态下采取一个动作的期望奖励。它是状态-动作对的值。如果你知道一个状态下所有可能动作的动作值，你可以决定采取动作值最高的动作，并且你预期会获得最高的奖励。
- en: A *policy function* is a function that maps states to actions. It is the function
    that “decides” which actions to take given some input state.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *策略函数* 是一个将状态映射到动作的函数。它是“决定”给定输入状态采取哪些动作的函数。
- en: '*Q function* is a function that takes a state-action pair and returns the action-value.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q函数* 是一个函数，它接受一个状态-动作对并返回动作值。'
- en: '*Q-learning* is a form of reinforcement learning where we attempt to model
    the Q function; in other words, we attempt to learn how to predict the expected
    rewards for each action given a state.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q学习* 是一种强化学习方法，我们试图建模Q函数；换句话说，我们试图学习如何预测给定状态下每个动作的期望奖励。'
- en: A *deep Q-network (DQN)* is simply where we use a deep learning algorithm as
    the model in Q-learning.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *深度Q网络 (DQN)* 简单来说就是我们在Q学习中使用深度学习算法作为模型。
- en: '*Off-policy learning* is when we learn a policy while collecting data using
    a different policy.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离策略学习* 是当我们使用不同的策略收集数据的同时学习一个策略。'
- en: '*On-policy learning* is when we learn a policy while also simultaneously using
    it to collect data for learning.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*按策略学习* 是当我们学习一个策略的同时，也同时使用它来收集学习数据。'
- en: '*Catastrophic forgetting* is a big problem that machine learning algorithms
    face when training with small batches of data at a time, where the new data being
    learned erases or corrupts the old information already learned.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灾难性遗忘* 是机器学习算法在每次使用小批量数据训练时面临的一个大问题，其中新学习的数据会擦除或损坏已学习的老信息。'
- en: '*Experience replay* is a mechanism to allow batch training of reinforcement
    learning algorithms in order to mitigate catastrophic forgetting and allow stable
    training.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*经验回放* 是一种机制，允许批量训练强化学习算法，以减轻灾难性遗忘并允许稳定训练。'
- en: A *target network* is a copy of the main DQN that we use to stabilize the update
    rule for training the main DQN.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *目标网络* 是主DQN的一个副本，我们使用它来稳定主DQN的训练更新规则。
- en: 'Chapter 4\. Learning to pick the best policy: Policy gradient methods'
  id: totrans-704
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章\. 学习选择最佳策略：策略梯度方法
- en: '*This chapter covers*'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Implementing the policy function as a neural network
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将策略函数实现为神经网络
- en: Introducing the OpenAI Gym API
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 OpenAI Gym API
- en: Applying the REINFORCE algorithm on the OpenAI CartPole problem
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenAI CartPole问题中应用REINFORCE算法
- en: In the previous chapter we discussed deep Q-networks, an off-policy algorithm
    that approximates the Q function with a neural network. The output of the Q-network
    was Q values corresponding to each action for a given state ([figure 4.1](#ch04fig01));
    recall that the Q value is the expected (weighted average) of rewards.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了深度Q网络，这是一种离策略算法，它使用神经网络来近似Q函数。Q网络的输出是对应于给定状态的每个动作的Q值（[图4.1](#ch04fig01)）；回想一下，Q值是奖励的期望（加权平均值）。
- en: Figure 4.1\. A Q-network takes a state and returns Q values (action values)
    for each action. We can use those action values to decide which actions to take.
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1\. Q网络接受一个状态并返回每个动作的Q值（动作值）。我们可以使用这些动作值来决定采取哪些动作。
- en: '![](04fig01_alt.jpg)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01_alt.jpg)'
- en: Given these predicted Q values from the Q-network, we can use some strategy
    to select actions to perform. The strategy we employed in the last chapter was
    the epsilon-greedy approach, where we selected an action at random with probability
    ε, and with probability 1 – *ε* we selected the action associated with the highest
    Q value (the action the Q-network predicts is the best, given its experience so
    far). There are any number of other policies we could have followed, such as using
    a softmax layer on the Q values.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 给定Q网络预测的这些Q值，我们可以使用某种策略来选择执行的动作。我们在上一章中采用的战略是ε-贪婪方法，其中我们以概率ε随机选择一个动作，以概率1 –
    *ε*选择与最高Q值相关的动作（Q网络根据其经验预测的最佳动作）。我们可以遵循无数其他策略，例如在Q值上使用softmax层。
- en: What if we skip selecting a policy on top of the DQN and instead train a neural
    network to output an action directly? If we do that, our neural network ends up
    being a *policy function*, or a *policy network*. Remember from [chapter 3](kindle_split_012.html#ch03)
    that a policy function, π*State* → *P*(*Action*|*State*), accepts a state and
    returns the best action. More precisely, it will return a probability distribution
    over the actions, and we can sample from this distribution to select actions.
    If a probability distribution is an unfamiliar concept to you, don’t worry. We’ll
    discuss it more in this chapter and throughout the book.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们跳过在DQN之上选择策略，而是训练一个神经网络直接输出动作，会怎样呢？如果我们这样做，我们的神经网络最终会成为一个**策略函数**，或者一个**策略网络**。记得从[第3章](kindle_split_012.html#ch03)中，策略函数π*State*
    → *P*(*Action*|*State*)接受一个状态并返回最佳动作。更精确地说，它将返回一个动作的概率分布，我们可以从这个分布中采样以选择动作。如果你对概率分布这个概念不熟悉，不要担心。我们将在本章和整本书中进一步讨论它。
- en: 4.1\. Policy function using neural networks
  id: totrans-714
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 使用神经网络的策略函数
- en: In this chapter we’ll introduce a class of algorithms that allow us to approximate
    the policy function, *π*(*s*), instead of the value function, *V**[π]* or *Q*.
    That is, instead of training a network that outputs action values, we will train
    a network to output (the probability of) actions.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一类算法，这些算法允许我们近似策略函数 *π*(*s*)，而不是值函数 *V**[π]* 或 *Q*。也就是说，我们不是训练一个输出动作值的网络，而是训练一个输出（动作的概率）的网络。
- en: 4.1.1\. Neural network as the policy function
  id: totrans-716
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 神经网络作为策略函数
- en: In contrast to a Q-network, a policy network tells us exactly what to do given
    the state we’re in. No further decisions are necessary. All we need to do is randomly
    sample from the probability distribution *P*(*A*|*S*), and we get an action to
    take ([figure 4.2](#ch04fig02)). The actions that are most likely to be beneficial
    will have the highest chance of being selected from random sampling, since they
    are assigned the highest probability.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 与Q网络相比，策略网络告诉我们给定我们当前的状态应该做什么。不需要进一步的决策。我们只需要从概率分布 *P*(*A*|*S*) 中随机采样，我们就得到一个要采取的动作（[图4.2](#ch04fig02)）。最有可能带来益处的动作将有最高的被随机采样的机会，因为它们被分配了最高的概率。
- en: Figure 4.2\. A policy network is a function that takes a state and returns a
    probability distribution over the possible actions.
  id: totrans-718
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 策略网络是一个函数，它接受一个状态并返回可能动作的概率分布。
- en: '![](04fig02_alt.jpg)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig02_alt.jpg)'
- en: Imagine the probability distribution *P*(*A*|*S*) as a jar filled with little
    notes with an action written on each. In a game with four possible actions, there
    will be notes with labels 1–4 (or 0–3 if they’re indices in Python). If our policy
    network predicts that action 2 is the most likely to result in the highest reward,
    it will fill this jar with a lot of little notes labeled 2, and fewer notes labeled
    1, 3, and 4\. In order to select an action then, all we do is close our eyes and
    grab a random note from the jar. We’re most likely to choose action 2, but sometimes
    we’ll grab another action, and that gives us the opportunity to explore. Using
    this analogy, every time the state of the environment changes, we give the state
    to our policy network, and it uses that to fill the jar with a new set of labeled
    notes representing the actions in different proportions. Then we randomly pick
    from the jar.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 将概率分布 *P*(*A*|*S*) 想象成一个装满了写有动作的小纸条的罐子。在一个有四种可能动作的游戏中，会有标有 1–4（或者在 Python 中是索引的话是
    0–3）的纸条。如果我们的策略网络预测动作 2 最有可能带来最高的奖励，它就会在这个罐子里放入很多标有 2 的小纸条，而标有 1、3 和 4 的纸条则较少。为了选择一个动作，我们只需闭上眼睛从罐子里随机抽取一张纸条。我们最有可能选择动作
    2，但有时我们会抽到另一个动作，这给了我们探索的机会。使用这个类比，每当环境状态发生变化时，我们就将状态提供给我们的策略网络，它会使用这些信息来填充一个包含不同比例标签纸条的新罐子，代表不同的动作。然后我们从罐子里随机抽取。
- en: This class of algorithms is called *policy gradient methods,* and it has a few
    important differences from the DQN algorithm; we’ll explore these differences
    in this chapter. Policy gradient methods offer a few advantages over value prediction
    methods like DQN. One is that, as we already discussed, we no longer have to worry
    about devising an action-selection strategy like epsilon-greedy; instead, we directly
    sample actions from the policy. Remember, we spent a lot of time cooking up methods
    to improve the stability of training our DQN—we had to use experience replay and
    target networks, and there are a number of other methods in the academic literature
    that we could have used. A policy network tends to simplify some of that complexity.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 这类算法被称为 *策略梯度方法*，它与 DQN 算法有几个重要的不同点；我们将在本章中探讨这些不同点。策略梯度方法相对于像 DQN 这样的值预测方法有一些优势。其中之一是，正如我们之前讨论的，我们不再需要担心设计一个动作选择策略，如
    epsilon-greedy；相反，我们直接从策略中采样动作。记住，我们花费了很多时间来想出提高我们训练 DQN 稳定性的方法——我们不得不使用经验回放和目标网络，学术文献中还有许多其他方法我们可以使用。策略网络往往简化了这些复杂性。
- en: 4.1.2\. Stochastic policy gradient
  id: totrans-722
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 随机策略梯度
- en: There are many different flavors of policy gradient methods. We will start with
    the *stochastic policy gradient* method ([figure 4.3](#ch04fig03)), which is what
    we just described. With a stochastic policy gradient, the output of our neural
    network is an action vector that represents a probability distribution.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略方法有很多不同的变种。我们将从 *随机策略梯度* 方法（[图 4.3](#ch04fig03)）开始，这正是我们刚刚描述的。在随机策略梯度中，我们神经网络的输出是一个动作向量，它代表一个概率分布。
- en: Figure 4.3\. A stochastic policy function. A policy function accepts a state
    and returns a probability distribution over actions. It is stochastic because
    it returns a probability distribution over actions rather than returning a deterministic,
    single action.
  id: totrans-724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. 随机策略函数。策略函数接受一个状态并返回一个动作的概率分布。它是随机的，因为它返回一个动作的概率分布，而不是返回一个确定性的单一动作。
- en: '![](04fig03_alt.jpg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig03_alt.jpg)'
- en: The policy we’ll follow is selecting an action from this probability distribution.
    This means that if our agent ends up in the same state twice, we may not end up
    taking the same action every time. In [figure 4.3](#ch04fig03) we feed our function
    the state, which is (1,2), and the output is a vector of probabilities corresponding
    to each action. If this was a Gridworld agent, for example, the agent would have
    a 0.50 probability of going up, no chance of going down, a 0.25 probability of
    going left, and a 0.25 probability of going right.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循的策略是从这个概率分布中选择一个动作。这意味着如果我们的智能体两次处于相同的状态，我们可能不会每次都采取相同的动作。在 [图 4.3](#ch04fig03)
    中，我们向函数输入状态，它是 (1,2)，输出是一个对应于每个动作的概率向量。例如，如果这是一个网格世界智能体，智能体向上移动的概率是 0.50，向下移动的概率是
    0，向左移动的概率是 0.25，向右移动的概率也是 0.25。
- en: If the environment is stationary, which is when the distribution of states and
    rewards is constant, and we use a deterministic strategy, we’d expect the probability
    distribution to eventually converge to a *degenerate probability distribution*,
    as shown in [figure 4.4](#ch04fig04). A degenerate probability distribution is
    a distribution in which all the probability mass is assigned to a single potential
    outcome. When dealing with discrete probability distributions as we do in this
    book, all the probabilities must sum to 1, so a degenerate distribution is one
    where all outcomes are assigned 0 probability except for one, which is assigned
    1.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境是平稳的，即状态和奖励的分布是恒定的，并且我们使用确定性策略，我们预计概率分布最终会收敛到一个*退化的概率分布*，如图4.4所示。退化的概率分布是一个将所有概率质量分配给单个潜在结果的分布。当我们处理这本书中的离散概率分布时，所有概率的总和必须为1，所以退化的分布是所有结果都被分配了0概率，除了一个被分配了1。
- en: Figure 4.4\. A deterministic policy function, often represented by the Greek
    character pi, takes a state and returns a specific action to take, unlike a stochastic
    policy, which returns a probability distribution over actions.
  id: totrans-728
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4. 一个确定性策略函数，通常用希腊字母π表示，它接受一个状态并返回一个特定的行为，这与随机策略不同，随机策略返回一个关于行为的概率分布。
- en: '![](04fig04_alt.jpg)'
  id: totrans-729
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig04_alt.jpg)'
- en: Early in training we want the distribution to be fairly uniform so that we can
    maximize exploration, but over the course of training we want the distribution
    to converge on the optimal actions, given a state. If there is only one optimal
    action for a state, we’d expect to converge toward a degenerate distribution,
    but if there are two equally good actions, then we would expect the distribution
    to have two *modes*. A mode of a probability distribution is just another word
    for a “peak.”
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的早期，我们希望分布相对均匀，以便我们可以最大化探索，但在训练过程中，我们希望分布收敛到给定状态下的最优行为。如果一个状态只有一个最优行为，我们预计会收敛到一个退化的分布，但如果有两个同样好的行为，那么我们预计分布将有两个*模式*。概率分布的模式就是“峰值”的另一种说法。
- en: '|  |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**I forget ... What’s a probability distribution?**'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '**我忘记了...什么是概率分布？**'
- en: 'In Gridworld, we had four possible actions: up, down, left, and right. We call
    this our action set or actions-space, since we can describe it mathematically
    as a set, e.g., *A* = {*up,down,left,right*} where the curly braces indicate a
    set. (A set in mathematics is just an abstract unordered collection of things
    with certain operations defined.) So what does it mean to apply a probability
    distribution over this set of actions?'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格世界中，我们有四种可能的行为：向上、向下、向左和向右。我们称这为我们的动作集或动作空间，因为我们可以用数学方式描述它，例如，*A* = {*向上,向下,向左,向右*}，其中花括号表示一个集合。（在数学中，集合只是一个具有某些操作定义的抽象无序事物集合。）那么，在这个动作集合上应用概率分布意味着什么呢？
- en: Probability is actually a very rich and even controversial topic in its own
    right. There are varying philosophical opinions on exactly what *probability*
    means. To some people, the probability means that, if you were to flip a coin
    a very large number of times (ideally an infinite number of times, mathematically
    speaking) the probability of a fair coin turning up heads is equal to the proportion
    of heads in that infinitely long sequence of flips. That is, if we flip a fair
    coin 1,000,000 times, we would expect about half of the flips to be heads and
    the other half tails, so the probability is equal to that proportion. This is
    a frequentist interpretation of probability, since probability is interpreted
    as the long-term frequency of some event repeated many times.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 概率实际上是一个非常丰富甚至有争议的话题。关于概率的确切含义，存在不同的哲学观点。对一些人来说，概率意味着，如果你要反复抛掷一枚硬币很多次（理想情况下是无限次，从数学上讲）公平硬币出现正面的概率等于无限长序列中正面的比例。也就是说，如果我们抛掷一枚公平硬币1000000次，我们预计大约一半的抛掷会是正面，另一半是反面，所以概率等于这个比例。这是概率的频率主义解释，因为概率被解释为某些事件重复多次的长期频率。
- en: Another school of thought interprets probability only as a degree of belief,
    a subjective assessment of how much someone can predict an event given the knowledge
    they currently possess. This degree of belief is often called a *credence*. The
    probability of a fair coin turning up heads is 0.5 or 50% because, given what
    we know about the coin, we don’t have any reason to predict heads more than tails,
    or tails more than heads, so we split our belief evenly across the two possible
    outcomes. Hence, anything that we can’t predict deterministically (i.e., with
    probability 0 or 1, and nothing in between) results from a lack of knowledge.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思想流派只将概率解释为信念程度，即根据他们目前拥有的知识对事件进行主观评估的程度。这种信念程度通常被称为 *信念度*。公平硬币出现正面的概率是0.5或50%，因为，根据我们对硬币的了解，我们没有理由预测正面比反面多，或者反面比正面多，所以我们平均分配我们的信念在两种可能的结果上。因此，任何我们不能确定预测（即，概率为0或1，之间没有其他东西）的东西，都是由于知识不足。
- en: You’re free to interpret probabilities however you want, since it won’t affect
    our calculations, but in this book we tend to implicitly use the credence interpretation
    of probability. For our purposes, applying a probability distribution over the
    set of actions in Gridworld, *A* = {*up,down,left,right*} means we’re assigning
    a degree of belief (a real number between 0 and 1) to each action in the set such
    that all the probabilities sum to 1\. We interpret these probabilities as the
    probability that an action is the best action to maximize the expected rewards,
    given that we’re in a certain state.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以自由地按照自己的意愿解释概率，因为这不会影响我们的计算，但在这本书中，我们倾向于隐含地使用概率的信念解释。就我们的目的而言，在网格世界动作集 *A*
    = {*上，下，左，右*} 上应用概率分布意味着我们正在为集合中的每个动作分配一个信念程度（一个介于0和1之间的实数），使得所有概率之和等于1。我们将这些概率解释为在特定状态下，一个动作是最佳动作以最大化预期奖励的概率。 '
- en: 'Concretely, a probability distribution over our action set *A* is denoted *P*(*A*):
    *A[i]* → [0,1], meaning that *P*(*A*) is a map from a set *A* to a set of real
    numbers between 0 and 1\. In particular, each element *a[i]* ∈ *A* is mapped to
    a single number between 0 and 1 such that the sum of all these numbers for each
    action is equal to 1\. We might represent this map for our Gridworld action set
    as just a vector, where we identify each position in the vector with an element
    in the action set, e.g., [up, down, left, right] → [0.25, 0.25, 0.10, 0.4]. This
    map is called a *probability mass function* (PMF).'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们动作集 *A* 上的概率分布用 *P*(*A*) 表示：*A[i]* → [0,1]，这意味着 *P*(*A*) 是一个从集合 *A* 到0到1之间实数集合的映射。特别是，每个元素
    *a[i]* ∈ *A* 都映射到一个0到1之间的单个数字，使得每个动作的所有这些数字之和等于1。我们可以将这个映射表示为我们的网格世界动作集的向量，其中我们将向量中的每个位置与动作集中的元素相对应，例如
    [上，下，左，右] → [0.25, 0.25, 0.10, 0.4]。这个映射被称为 *概率质量函数* (PMF)。
- en: What we just described is actually a *discrete* probability distribution, since
    our action set was discrete (a finite number of elements). If our action set was
    infinite, i.e., a continuous variable like velocity, we would call this a *continuous*
    probability distribution and instead we would need to define a *probability density
    function* (PDF).
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才描述的实际上是一个 *离散* 概率分布，因为我们的动作集是离散的（有限数量的元素）。如果我们的动作集是无限的，即像速度这样的连续变量，我们就会称这为
    *连续* 概率分布，并且我们需要定义一个 *概率密度函数* (PDF)。
- en: The most common example of a PDF is the normal (also known as Gaussian, or just
    bell-curve) distribution. If we have a probability with a continuous action, say
    a car game where we need to control the velocity of the car from 0 to some maximum
    value, which is a continuous variable, how might we do this with a policy network?
    Well, we could drop the idea of probability distribution and just train the network
    to produce the single value of velocity that it predicts is best, but then we’d
    risk not exploring enough (and it is difficult to train such a network). A lot
    of power comes from a little bit of randomness. The kind of neural networks we
    employ in this book only produce vectors (or tensors more generally) as output,
    so they can’t produce a continuous probability distribution—we have to be more
    clever. A PDF like a normal distribution is defined by two parameters, the mean
    and variance. Once we have those, we have a normal distribution that we can sample
    from. So we can just train a neural network to produce mean and standard deviation
    values that we can then plug into the normal distribution equation and sample
    from that.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: PDF最常见的例子是正态分布（也称为高斯分布，或简称钟形曲线）分布。如果我们有一个具有连续动作的概率，比如说一个需要控制汽车速度从0到某个最大值的游戏，这是一个连续变量，我们如何用策略网络来实现呢？好吧，我们可以放弃概率分布的想法，只训练网络产生它预测的最佳速度值，但这样我们就有可能探索不足（并且训练这样的网络是困难的）。一点随机性就能带来很多力量。我们在本书中使用的神经网络只产生向量（或更一般地说，张量）作为输出，因此它们不能产生连续的概率分布——我们必须更聪明一些。像正态分布这样的PDF由两个参数定义，即均值和方差。一旦我们有了这些，我们就有了一个可以从中采样的正态分布。所以我们可以训练一个神经网络来产生均值和标准差值，然后我们可以将这些值插入到正态分布方程中并从中采样。
- en: Don’t worry if this isn’t all making sense now. We will continue to go over
    it again and again because these concepts are ubiquitous in reinforcement learning
    and machine learning more broadly.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在这一切还没有完全明白，请不要担心。我们会一遍又一遍地继续讲解，因为这些概念在强化学习和更广泛的机器学习中无处不在。
- en: '|  |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.1.3\. Exploration
  id: totrans-742
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 探索
- en: Recall from the previous chapter that we needed our policy to include some randomness,
    which would allow us to visit new states during training. For DQNs we followed
    the epsilon-greedy policy, where there was a chance we would not follow the action
    that led to the greatest predicted reward. If we always selected the action that
    led to the maximum predicted reward, we’d never discover the even better actions
    and states available to us. For the stochastic policy gradient method, because
    our output is a probability distribution, there should be a small chance that
    we explore all spaces; only after sufficient exploration will the action distribution
    converge to producing the single best action, a degenerate distribution. Or if
    the environment itself has some randomness, the probability distribution will
    retain some probability mass to each action. When we initialize our model in the
    beginning, the probability of our agent picking each action should be approximately
    equal or uniform, since the model has zero information about which action is better.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下上一章，我们需要的策略包括一些随机性，这样我们才能在训练过程中访问新的状态。对于DQNs，我们遵循了ε-贪婪策略，这意味着我们有可能不会选择导致最大预测奖励的动作。如果我们总是选择导致最大预测奖励的动作，我们就永远不会发现我们可用的甚至更好的动作和状态。对于随机策略梯度方法，因为我们的输出是一个概率分布，应该有很小一部分机会去探索所有空间；只有经过足够的探索，动作分布才会收敛到产生单个最佳动作，一个退化的分布。或者如果环境本身有一些随机性，概率分布将保留一些概率质量到每个动作上。当我们最初初始化我们的模型时，我们的智能体选择每个动作的概率应该是大致相等或均匀的，因为模型对哪个动作更好没有任何信息。
- en: There is a variant of policy gradient called *deterministic policy gradient*
    (DPG) where there is a single output that the agent will always follow (as illustrated
    in [figure 4.4](#ch04fig04)). In the case of Gridworld, for example, it would
    produce a 4-dimensional binary vector with a 1 for the action to be taken and
    0s for the other actions. The agent won’t explore properly if it always follows
    the output because there’s no randomness in the action selection. Since the output
    of a deterministic policy function for a discrete action set would be discrete
    values, it is also difficult to get this working in the fully differentiable manner
    that we are accustomed to with deep learning, so we’ll focus on stochastic policy
    gradients. Building a notion of uncertainty into the models (e.g., using probability
    distributions) is generally a good thing.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 政策梯度算法中有一个变体称为 *确定性策略梯度*（DPG），其中有一个单一输出，智能体将始终遵循（如图 4.4 所示）。例如，在 Gridworld 的情况下，它将产生一个
    4 维的二进制向量，对于要采取的动作有一个 1，其他动作有 0。如果智能体总是遵循输出，那么它将无法正确地探索，因为动作选择中没有随机性。由于离散动作集的确定性策略函数的输出将是离散值，因此很难以我们习惯的深度学习方式完全可微分地实现这一点，所以我们将专注于随机策略梯度。将不确定性的概念（例如，使用概率分布）构建到模型中通常是件好事。
- en: '4.2\. Reinforcing good actions: The policy gradient algorithm'
  id: totrans-745
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 强化良好动作：策略梯度算法
- en: From the previous section, you understand that there is a class of algorithms
    that attempts to create a function that outputs a probability distribution over
    actions, and that this policy function *π*(*s*) can be implemented with a neural
    network. In this section we’ll delve into how to actually implement these algorithms
    and train (i.e., optimize) them.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中，你了解到有一类算法试图创建一个输出动作概率分布的函数，并且这个策略函数 *π*(*s*) 可以用神经网络实现。在本节中，我们将深入探讨如何实际实现这些算法并训练（即优化）它们。
- en: 4.2.1\. Defining an objective
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1. 定义目标函数
- en: Recall that neural networks need an objective function that is differentiable
    with respect to the network weights (parameters). In the last chapter we trained
    the deep Q-network with a minimizing mean squared error (MSE) loss function with
    respect to its predicted Q values and the target Q value. We had a nice formula
    for calculating the target Q value based on the observed reward, since Q values
    are just averaged rewards (i.e., expectations), so this was not much different
    from how we would normally train a supervised deep learning algorithm.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，神经网络需要一个关于网络权重（参数）可微分的目标函数。在上一章中，我们使用最小化均方误差（MSE）损失函数来训练深度 Q 网络，该损失函数与预测的
    Q 值和目标 Q 值相关。由于 Q 值只是平均奖励（即期望值），所以我们有一个基于观察到的奖励计算目标 Q 值的公式，因此这并不比我们通常训练监督深度学习算法的方式有太大不同。
- en: How do we train a policy network that gives us a probability distribution over
    actions given a state, *P*(*A*|*S*)? There’s no obvious way to map our observed
    rewards after taking an action to updating *P*(*A*|*S*). Training the DQN was
    not much different from solving a supervised learning problem because our Q-network
    generated a vector of predicted Q values, and by using a formula we were able
    to generate the target Q value vector. Then we just minimized the error between
    the Q-network’s output vector and our target vector.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练一个策略网络，使其在给定状态的情况下给出动作的概率分布，即 *P*(*A*|*S*)？将观察到的奖励映射到更新 *P*(*A*|*S*) 上并没有明显的途径。训练
    DQN 与解决监督学习问题并没有太大的不同，因为我们的 Q 网络生成了一组预测的 Q 值向量，然后我们通过一个公式生成了目标 Q 值向量。然后我们只需最小化
    Q 网络输出向量与我们的目标向量之间的误差。
- en: With a policy network, we’re predicting actions directly, and there is no way
    to come up with a target vector of actions we should have taken instead, given
    the rewards. All we know is whether the action led to positive or negative rewards.
    In fact, what the best action is secretly depends on a value function, but with
    a policy network we’re trying to avoid computing these action values directly.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 使用策略网络，我们直接预测动作，没有方法来生成一个目标动作向量，即我们应该采取的动作，给定奖励。我们所知道的是动作是否导致了正面的或负面的奖励。实际上，最佳动作的秘密取决于一个价值函数，但使用策略网络我们试图避免直接计算这些动作值。
- en: Let’s go through an example to see how we might optimize our policy network.
    We’ll start with some notation. Our policy network is denoted *π* and is parameterized
    by a vector *θ*, which represents all of the parameters (weights) of the neural
    network. As you know, neural networks have parameters in the form of multiple
    weight matrices, but for the purposes of easy notation and discussion, it is standard
    to consider all the network parameters together as a single long vector that we
    denote *θ* (theta).
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看一下我们如何优化我们的策略网络。我们首先从一些符号开始。我们的策略网络用*π*表示，并由一个向量*θ*参数化，它代表神经网络的所有参数（权重）。正如你所知，神经网络有多个权重矩阵形式的参数，但为了便于符号和讨论，通常将所有网络参数视为一个单一的、较长的向量，我们用*θ*（theta）表示。
- en: Whenever we run the policy network forward, the parameter vector *θ* is fixed;
    the variable is the data that gets fed into the policy network (i.e., the state).
    Hence, we denote the parameterized policy as *π[θ]*. Whenever we want to indicate
    that some input to a function is fixed, we will include it as a subscript rather
    than as an explicit input like *π*(*x*,*θ*) where *x* is some input data (i.e.,
    the state of the game). Notations like *π*(*x*,*θ*) suggest *θ* is a variable
    that changes along with *x*, whereas *π[θ]* indicates that *θ* is a fixed parameter
    of the function.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们运行策略网络时，参数向量*θ*是固定的；变量是输入到策略网络中的数据（即状态）。因此，我们将参数化的策略表示为*π[θ]*。每当我们要表示函数的某个输入是固定的，我们将它包括为下标，而不是像*π*(*x*,*θ*)这样的显式输入，其中*x*是某些输入数据（即游戏的状态）。像*π*(*x*,*θ*)这样的符号表明*θ*是一个随着*x*变化的变量，而*π[θ]*则表明*θ*是函数的固定参数。
- en: Let’s say we give our initially untrained policy network *π[θ]* some initial
    game state for Gridworld, denoted *s*, and run it forward by computing *π[θ]*(*s*).
    It returns a probability distribution over the four possible actions, such as
    [0.25, 0.25, 0.25, 0.25]. We sample from this distribution, and since it’s a uniform
    distribution, we end up taking a random action ([figure 4.5](#ch04fig05)). We
    continue to take actions by sampling from the produced action distribution until
    we reach the end of the episode.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们给最初未训练的策略网络*π[θ]*一些Gridworld的初始游戏状态，表示为*s*，并通过计算*π[θ]*(*s*)来运行它。它返回一个关于四个可能动作的概率分布，例如[0.25,
    0.25, 0.25, 0.25]。我们从这个分布中采样，由于这是一个均匀分布，我们最终采取了一个随机动作（[图4.5](#ch04fig05)）。我们继续通过从产生的动作分布中采样来采取动作，直到我们达到分段的结束。
- en: Figure 4.5\. The general overview of policy gradients for an environment with
    four possible discrete actions. First we input the state to the policy network,
    which produces a probability distribution over the actions, and then we sample
    from this distribution to take an action, which produces a new state.
  id: totrans-754
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5。具有四个可能离散动作的环境的策略梯度的概述。首先，我们将状态输入到策略网络中，它产生一个关于动作的概率分布，然后我们从这个分布中采样以采取一个动作，这会产生一个新的状态。
- en: '![](04fig05_alt.jpg)'
  id: totrans-755
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig05_alt.jpg)'
- en: Remember, some games like Gridworld are episodic, meaning that there is a well-defined
    start and end point to an episode of the game. In Gridworld, we start the game
    in some initial state and play until we either hit the pit, land on the goal,
    or take too many moves. So an episode is a sequence of states, actions, and rewards
    from an initial state to the terminal state where we win or lose the game. We
    denote this episode as
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，一些游戏如Gridworld是分段的，这意味着游戏的一个分段有一个明确的开始和结束点。在Gridworld中，我们从某个初始状态开始游戏，直到我们触碰到坑洞、达到目标或移动次数过多。因此，一个分段是从初始状态到终端状态（我们赢得或输掉游戏的状态）的一系列状态、动作和奖励。我们将这个分段表示为
- en: '| ε = (*S*[0],*A*[0],*R*[1]),(*S*[1],*A*[1],*R*[2]) ... (*S[t]*[–1],*A[t]*[–1],*R[t]*)
    |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| ε = (*S*[0],*A*[0],*R*[1]),(*S*[1],*A*[1],*R*[2]) ... (*S[t]*[–1],*A[t]*[–1],*R[t]*)
    |'
- en: 'Each tuple is one time-step of the Gridworld game (or a Markov decision process,
    more generally). After we’ve reached the end of the episode at time *t*, we’ve
    collected a bunch of historical data on what just happened. Let’s say that by
    chance we hit the goal after just three moves determined by our policy network.
    Here’s what our episode looks like:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元组是Gridworld游戏（或更一般地说，马尔可夫决策过程）的一个时间步。在我们达到时间*t*的分段结束时，我们已经收集了大量关于刚刚发生的事情的历史数据。假设我们偶然在经过策略网络决定的三个移动后达到了目标。我们的分段看起来是这样的：
- en: '| ε = (*S*[0],3,–1),(*S*[1],1,–1),(*S*[2],3,+10) |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| ε = (*S*[0],3,–1),(*S*[1],1,–1),(*S*[2],3,+10) |'
- en: We’ve encoded the actions as integers from 0 to 3 (referring to array indices
    of the action vector) and we’ve left the states denoted symbolically since they’re
    actually 64-length vectors. What is there to learn from in this episode? Well,
    we won the game, indicated by the +10 reward in the last tuple, so our actions
    must have been “good” to some degree. Given the states we were in, we should encourage
    our policy network to make those actions more likely next time. We want to reinforce
    those actions that led to a nice positive reward. We will address what happens
    when our agent loses (receives a terminal reward of –10) later in this section,
    but in the meantime we will focus on positive reinforcement.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将动作编码为从0到3的整数（指动作向量的数组索引），并且我们用符号表示状态，因为它们实际上是64长度的向量。在这个场景中我们能学到什么？嗯，我们赢得了游戏，这由最后一个元组中的+10奖励表示，所以我们的动作在某种程度上必须是“好的”。鉴于我们所处的状态，我们应该鼓励我们的策略网络在下一次更可能地采取这些动作。我们希望强化那些导致良好正面奖励的动作。我们将在本节稍后讨论我们的智能体输掉游戏（收到终端奖励-10）的情况，但在此同时，我们将专注于正强化。
- en: 4.2.2\. Action reinforcement
  id: totrans-761
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2. 动作强化
- en: We want to make small, smooth updates to our gradients to encourage the network
    to assign more probability to these winning actions in the future. Let’s focus
    on the last experience in the episode, with state *S*[2]. Remember, we’re assuming
    our policy network produced the action probability distribution [0.25, 0.25, 0.25,
    0.25], since it was untrained, and in the last time step we took action 3 (corresponding
    to element 4 in the action probability array), which resulted in us winning the
    game with a +10 reward. We want to positively reinforce this action, given state
    *S*[2], such that whenever the policy network encounters *S*[2] or a very similar
    state, it will be more confident in predicting action 3 as the highest probability
    action to take.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望对我们的梯度进行小而平滑的更新，以鼓励网络在将来为这些获胜动作分配更多的概率。让我们专注于场景中的最后一个经验，状态 *S*[2]。记住，我们假设我们的策略网络产生了动作概率分布
    [0.25, 0.25, 0.25, 0.25]，因为它尚未训练，并且在最后一个时间步中我们采取了动作3（对应于动作概率数组中的元素4），这导致我们以+10的奖励赢得了游戏。我们希望对状态
    *S*[2] 下的这个动作进行正强化，这样每当策略网络遇到 *S*[2] 或非常相似的状态时，它将更有信心预测动作3为最高概率的动作采取。
- en: A naive approach might be to make a target action distribution, [0, 0, 0, 1],
    so that our gradient descent will move the probabilities from [0.25, 0.25, 0.25,
    0.25] close to [0, 0, 0, 1], maybe ending up as [0.167, 0.167, 0.167, 0.5] (see
    [figure 4.6](#ch04fig06)). This is something we often do in the supervised learning
    realm, when we are training a softmax-based image classifier. But in that case,
    there is a single correct classification for an image, and there is no temporal
    association between each prediction. In our RL case, we want more control over
    how we make these updates. First, we want to make small, smooth updates because
    we want to maintain some stochasticity in our action sampling to adequately explore
    the environment. Second, we want to be able to weight how much we assign credit
    to each action for earlier actions. Let’s review some more notation before diving
    into these two problems.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法可能是创建一个目标动作分布，[0, 0, 0, 1]，这样我们的梯度下降就会将概率从 [0.25, 0.25, 0.25, 0.25] 移动到
    [0, 0, 0, 1] 附近，也许最终会变成 [0.167, 0.167, 0.167, 0.5]（见[图4.6](#ch04fig06)）。这在监督学习领域是经常做的事情，当我们训练一个基于softmax的图像分类器时。但在那种情况下，对于一张图片来说，有一个正确的分类，并且每个预测之间没有时间关联。在我们的强化学习案例中，我们希望对这些更新有更多的控制。首先，我们希望进行小而平滑的更新，因为我们希望在我们的动作采样中保持一些随机性，以便充分探索环境。其次，我们希望能够权衡我们对每个动作赋予的信用度，对于早期的动作。在深入探讨这两个问题之前，让我们回顾一些更多的符号。
- en: Figure 4.6\. Once an action is sampled from the policy network’s probability
    distribution, it produces a new state and reward. The reward signal is used to
    reinforce the action that was taken, that is, it increases the probability of
    that action given the state if the reward is positive, or it decreases the probability
    if the reward is negative. Notice that we only received information about action
    3 (element 4), but since the probabilities must sum to 1, we have to lower the
    probabilities of the other actions.
  id: totrans-764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6。一旦从策略网络的概率分布中采样到一个动作，它会产生一个新的状态和奖励。奖励信号用于强化所采取的动作，也就是说，如果奖励是正的，它会增加该动作在给定状态下的概率，或者如果奖励是负的，它会减少该概率。请注意，我们只收到了关于动作3（元素4）的信息，但由于概率必须总和为1，我们必须降低其他动作的概率。
- en: '![](04fig06_alt.jpg)'
  id: totrans-765
  prefs: []
  type: TYPE_IMG
  zh: '![04fig06_alt.jpg](04fig06_alt.jpg)'
- en: Recall that our policy network is typically denoted *π[θ]* when we are running
    it forward (i.e., using it to produce action probabilities), because we think
    of the network parameters, *θ*, as being fixed and the input state is what varies.
    Hence, calling *π[θ]*(*s*) for some state *s* will return a probability distribution
    over the possible actions, given a fixed set of parameters. When we are training
    the policy network, we need to vary the parameters with respect to a fixed input
    to find a set of parameters that optimizes our objective (i.e., minimizes a loss
    or maximizes a utility function), which is the function *π**[s]*(*θ*).
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当我们正向运行策略网络时，通常用 *π[θ]* 表示，因为我们认为网络参数 *θ* 是固定的，而输入状态是变化的。因此，对于某个状态 *s*，调用
    *π[θ]*(*s*) 将返回一个概率分布，表示在给定一组固定参数的情况下可能采取的动作。当我们训练策略网络时，我们需要相对于一个固定的输入来改变参数，以找到一组参数，这组参数可以优化我们的目标（即最小化损失或最大化效用函数），这个函数是
    *π[s]*(*θ*)。
- en: '|  |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-768
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: The probability of an action, given the parameters of the policy network, is
    denoted *π**[s]*(*a*|*θ*). This makes it clear that the probability of an action,
    *a*, explicitly depends on the parameterization of the policy network. In general,
    we denote a *conditional probability* as *P*(*x* | *y*), read “the probability
    distribution over *x* given *y*.” This means we have some function that takes
    a parameter *y* and returns a probability distribution over some other parameter
    *x*.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定策略网络的参数的情况下，动作的概率表示为 *π[s]*(*a*|*θ*)。这清楚地表明，动作的概率 *a* 明确依赖于策略网络的参数化。一般来说，我们用一个
    *条件概率* 表示 *P*(*x* | *y*)，读作“给定 *y* 的 *x* 的概率分布。”这意味着我们有一个函数，它接受一个参数 *y* 并返回另一个参数
    *x* 的概率分布。
- en: '|  |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In order to reinforce action 3, we want to modify our policy network parameters
    *θ* such that we increase *π**[s]*(*a*[3]|*θ*). Our objective function merely
    needs to maximize *π**[s]*(*a*[3]|*θ*) where *a*[3] is action 3 in our example.
    Before training, *π**[s]*(*a*[3]|*θ*) = 0.25, but we want to modify *θ* such that
    *π**[s]*(*a*[3]|*θ*) > 0.25\. Because all of our probabilities must sum to 1,
    maximizing *π**[s]*(*a*[3]|*θ*) will minimize the other action probabilities.
    And remember, we prefer to set things up so that we’re minimizing an objective
    function instead of maximizing, since it plays nicely with PyTorch’s built-in
    optimizers—we should instead tell PyTorch to minimize 1 – *π**[s]*(*a*|*θ*). This
    loss function approaches 0 as *π**[s]*(*a*|*θ*) nears 1, so we are encouraging
    the gradients to maximize *π**[s]*(*a*|*θ*) for the action we took. We will subsequently
    drop the subscript *a*[3], as it should be clear from the context which action
    we’re referring to.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强化动作3，我们希望修改策略网络参数 *θ*，使得我们增加 *π[s]*(*a*[3]|*θ*)。我们的目标函数只需要最大化 *π[s]*(*a*[3]|*θ*)，其中
    *a*[3] 是我们例子中的动作3。在训练之前，*π[s]*(*a*[3]|*θ*) = 0.25，但我们需要修改 *θ*，使得 *π[s]*(*a*[3]|*θ*)
    > 0.25。因为所有的概率必须加起来等于1，最大化 *π[s]*(*a*[3]|*θ*) 将会最小化其他动作的概率。记住，我们更喜欢设置目标函数为最小化而不是最大化，因为这样与
    PyTorch 内置的优化器配合得更好——我们应该告诉 PyTorch 最小化 1 – *π[s]*(*a*|*θ*)。这个损失函数当 *π[s]*(*a*|*θ*)
    接近1时趋近于0，因此我们鼓励梯度最大化我们采取的动作的 *π[s]*(*a*|*θ*)。我们将随后省略下标 *a*[3]，因为从上下文中应该很清楚我们指的是哪个动作。
- en: 4.2.3\. Log probability
  id: totrans-772
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3. 对数概率
- en: Mathematically, what we’ve described is correct. But due to computation imprecisions
    we need to make adjustments to this formula to stabilize the training. One problem
    is that probabilities are bounded by 0 and 1 by definition, so the range of values
    that the optimizer can operate over is limited and small. Sometimes probabilities
    may be extremely tiny or very close to 1, and this runs into numerical issues
    when optimizing on a computer with limited numerical precision. If we instead
    use a surrogate objective, namely –log*π**[s]*(*a*|*θ*) (where log is the natural
    logarithm), we have an objective that has a larger “dynamic range” than raw probability
    space, since the log of probability space ranges from (–∞,0), and this makes the
    log probability easier to compute. Moreover, logarithms have the nice property
    that log(*a* × *b*) = log(*a*) + log(*b*), which means when we multiply log probabilities,
    we can turn this multiplication into a sum, which is also more numerically stable
    than multiplication. If we set our objective as –log*π**[s]*(*a*|*θ*) instead
    of 1 – *π**[s]*(*a*|*θ*), our loss still abides by the intuition that the loss
    function approaches 0 as *π**[s]*(*a*|*θ*) approaches 1\. Our gradients will be
    tuned to try to increase *π**[s]*(*a*|*θ*) to 1, where *a* = action 3, for our
    running example.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上来说，我们所描述的是正确的。但由于计算精度问题，我们需要对此公式进行调整以稳定训练。一个问题是由于定义，概率被限制在0和1之间，因此优化器可以操作的范围有限且较小。有时概率可能非常小或非常接近1，这会在有限数值精度的计算机上进行优化时遇到数值问题。如果我们改用一个替代目标函数，即
    –log*π**[s]*(*a*|*θ*)（其中log是自然对数），我们有一个比原始概率空间具有更大“动态范围”的目标，因为概率空间的对数范围从（–∞,0），这使得对数概率更容易计算。此外，对数有一个很好的性质，即
    log(*a* × *b*) = log(*a*) + log(*b*)，这意味着当我们乘以对数概率时，我们可以将这个乘法转换成加法，这比乘法更数值稳定。如果我们将我们的目标设置为
    –log*π**[s]*(*a*|*θ*)而不是 1 – *π**[s]*(*a*|*θ*)，我们的损失仍然遵循损失函数随着 *π**[s]*(*a*|*θ*)
    接近1而趋近于0的直观。我们的梯度将被调整以尝试将 *π**[s]*(*a*|*θ*) 增加到1，在我们的运行示例中，*a* = 动作3。
- en: 4.2.4\. Credit assignment
  id: totrans-774
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4. 信用分配
- en: Our objective function is –log*π**[s]*(*a*|*θ*), but this assigns equal weight
    to every action in our episode. The weights in the network that produced the last
    action will be updated to the same degree as the first action. Why shouldn’t that
    be the case? Well, it makes sense that the last action right before the reward
    deserves more credit for winning the game than does the first action in the episode.
    For all we know, the first action was actually sub-optimal, but then we later
    made a comeback and hit the goal. In other words, our confidence in how “good”
    each action is diminishes the further we are from the point of reward. In a game
    of chess, we attribute more credit to the last move made than the first one. We’re
    very confident that the move that directly led to us to winning was a good move,
    but we become less confident the further back we go. How much did the move five
    time steps ago contribute to winning? We’re not so sure. This is the problem of
    *credit assignment*.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标函数是 –log*π**[s]*(*a*|*θ*)，但这会给我们的情节中的每个动作分配相同的权重。产生最后一个动作的网络中的权重将按照与第一个动作相同的程度进行更新。为什么不应该这样呢？好吧，在奖励之前的最后一个动作比情节中的第一个动作赢得游戏的信用更多是有道理的。就我们所知，第一个动作实际上是次优的，但后来我们进行了反击并达到了目标。换句话说，我们对每个动作“好”的信心随着我们离奖励点的距离越来越远而减弱。在棋类游戏中，我们给最后一个动作的信用比第一个动作多。我们非常确信直接导致我们获胜的动作是一个好动作，但随着我们回溯得越来越远，我们的信心就会减弱。五步之前的动作对获胜贡献了多少？我们不太确定。这就是信用分配的问题。
- en: We express this uncertainty by multiplying the magnitude of the update by the
    discount factor, which you learned in [chapter 3](kindle_split_012.html#ch03)
    ranges from 0 to 1\. The action right before the episode ends will have a discount
    factor of 1, meaning it will receive the full gradient update, while earlier moves
    will be discounted by a fraction such as 0.5 so the gradient steps will be smaller.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将更新幅度的量乘以折扣因子来表示这种不确定性，这个折扣因子你在[第3章](kindle_split_012.html#ch03)中学过，其范围在0到1之间。在情节结束前的动作将具有折扣因子1，这意味着它将接收完整的梯度更新，而较早的动作将被按比例折扣，例如0.5，因此梯度步长会更小。
- en: Let’s add those into our objective (loss) function. The final objective function
    that we will tell PyTorch to minimize is –γ*[t]* * *G[t]* * log*π**[s]*(*a*|*θ*).
    Remember, γ*[t]* is the discount factor, and the subscript *t* tells us its value
    will depend on the time step *t*, since we want to discount more distant actions
    more than more recent ones. The parameter *G[t]* is called the *total return*,
    or *future return*, at time step *t*. It is the return we expect to collect from
    time step *t* until the end of the episode, and it can be approximated by adding
    the rewards from some state in the episode until the end of the episode.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些添加到我们的目标（损失）函数中。我们将告诉PyTorch最小化的最终目标函数是 –γ*[t]* * *G[t]* * log*π**[s]*(*a*|*θ*）。记住，γ*[t]*是折现因子，下标*t*告诉我们它的值将取决于时间步*t*，因为我们希望对较远动作的折现要多于对较近动作的折现。参数*G[t]*被称为*t*时刻的*总回报*或*未来回报*。它是我们从时间步*t*到游戏结束期望收集的回报，可以通过将游戏期间某个状态到游戏结束的奖励相加来近似。
- en: '| *G[t]* = *r[t]* + *r[t]* [+1] ... + *r[T]* [–1] + *r[T]* |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| *G[t]* = *r[t]* + *r[t]* [+1] ... + *r[T]* [–1] + *r[T]* |'
- en: Actions temporally more distant from the received reward should be weighted
    less than actions closer. If we win a game of Gridworld, the sequence of discounted
    rewards from the start to the terminal state might look something like [0.970,
    0.980, 0.99, 1.0]. The last action led to the winning state of +1, and it is not
    discounted at all. The previous action is assigned a scaled reward by multiplying
    the terminal reward with the γ*[t]*[–1] discount factor, which we’ve set to 0.99.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 与接收奖励时间较远的动作相比，应该对接近奖励的动作赋予更低的权重。如果我们赢得了Gridworld游戏，从开始到终端状态之间的折现奖励序列可能看起来像[0.970,
    0.980, 0.99, 1.0]。最后一个动作导致了+1的获胜状态，并且没有进行任何折现。前一个动作通过将终端奖励乘以γ*[t]*[–1]折现因子来分配一个缩放后的奖励，我们将该因子设置为0.99。
- en: The discount is exponentially decayed from 1, ![](pg101.jpg), meaning that the
    discount at time *t* is calculated as the starting discount (here 0.99) exponentiated
    to the integer time distance from the reward. The length of the episode (the total
    number of time steps) is denoted *T*, and the local time step for a particular
    action is *t*. For *T* – *t* = 0, *γ**[T]*[–][0] = 0.99⁰ = 1\. For *T* – *t* =
    2, the γ*[T]*[–2] = 0.99² = 0.9801, and so on. Each time step back, the discount
    factor is exponentiated to the distance from the terminal step, which results
    in an exponential decay of the discount factor the more distant (and irrelevant)
    the action was to the reward outcome.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 折现以指数方式衰减到1，![图片](pg101.jpg)，这意味着时间*t*的折现是起始折现（此处为0.99）的整数时间距离的指数。游戏长度（总时间步数）表示为*T*，特定动作的局部时间步为*t*。对于*T*
    – *t* = 0，*γ**[T]*[–][0] = 0.99⁰ = 1。对于*T* – *t* = 2，γ*[T]*[–2] = 0.99² = 0.9801，依此类推。每向后一步，折现因子都会指数化到终端步骤的距离，这导致折现因子随着动作与奖励结果的距离增加而指数衰减。
- en: For example, if the agent is in state *S*[0] (i.e., time step *t* = 0) and it
    takes action *a*[1] and receives reward *t[t]*[+1] = –1, the target update will
    be –*γ*⁰(–1)log*π*(*a*[1]|*θ*,*S*[0]) = log*π*(*a*[1]|*θ*,*S*[0]), which is the
    log-probability output from the policy network (see [figure 4.7](#ch04fig07)).
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果智能体处于状态*S*[0]（即时间步*t* = 0）并采取动作*a*[1]，并获得奖励*r[t]*[+1] = –1，目标更新将是 –*γ*⁰(–1)log*π*(*a*[1]|*θ*,*S*[0])
    = log*π*(*a*[1]|*θ*,*S*[0])，这是策略网络（参见[图4.7](#ch04fig07)）的输出对数概率。
- en: Figure 4.7\. A string diagram for training a policy network for Gridworld. The
    policy network is a neural network parameterized by θ (the weights) that accepts
    a 64-dimensional vector for an input state. It produces a discrete 4-dimensional
    probability distribution over the actions. The sample action box samples an action
    from the distribution and produces an integer as the action, which is given to
    the environment (to produce a new state and reward) and to the loss function so
    we can reinforce that action. The reward signal is also fed into the loss function,
    which we attempt to minimize with respect to the policy network parameters.
  id: totrans-782
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7。用于训练Gridworld策略网络的弦图。策略网络是一个由θ（权重）参数化的神经网络，它接受一个64维向量作为输入状态。它产生一个关于动作的离散4维概率分布。样本动作框从分布中采样一个动作，并产生一个整数作为动作，该动作被提供给环境（以产生新的状态和奖励）以及损失函数，这样我们就可以强化该动作。奖励信号也输入到损失函数中，我们试图通过策略网络参数来最小化它。
- en: '![](04fig07_alt.jpg)'
  id: totrans-783
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig07_alt.jpg)'
- en: 4.3\. Working with OpenAI Gym
  id: totrans-784
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 使用OpenAI Gym
- en: To illustrate how policy gradients work, we’ve been using Gridworld as an example,
    since it is already familiar to you from last chapter. However, we should use
    a different problem to actually implement the policy gradient algorithm, both
    for variety and also to introduce the OpenAI Gym.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明策略梯度的工作原理，我们一直使用Gridworld作为例子，因为它在上一个章节中已经对你很熟悉了。然而，我们应该使用不同的问题来实际实现策略梯度算法，这不仅是为了多样性，也是为了介绍OpenAI
    Gym。
- en: The OpenAI Gym is an open source suite of environments with a common API that
    is perfect for testing reinforcement learning algorithms. If you come up with
    some new DRL algorithm, testing it on a few of the environments in the Gym is
    a great way to get some idea of how well it performs. The Gym contains a variety
    of environments from easy ones can be “solved” by simple linear regression all
    the way through to ones that all but require a sophisticated DRL approach (see
    [figure 4.8](#ch04fig08)). There are games, robotic control, and other types of
    environments. There’s probably something in there you’ll be interested in.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个开源的环境套件，具有通用的API，非常适合测试强化学习算法。如果你想出了一些新的DRL算法，在Gym中的几个环境中测试它是一个很好的方法，可以了解它的性能如何。Gym包含各种环境，从可以用简单线性回归“解决”的简单环境，到几乎需要复杂DRL方法的环境（参见[图4.8](#ch04fig08)）。这里有游戏、机器人控制和其它类型的环境。可能有一些你会感兴趣的环境。
- en: Figure 4.8\. Two example environments provided by OpenAI’s Gym environment.
    The OpenAI Gym provides hundreds of environments to test your reinforcement learning
    algorithms on.
  id: totrans-787
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.8。OpenAI Gym提供的环境的两个示例。OpenAI Gym提供了数百个环境，可以测试你的强化学习算法。
- en: '![](04fig08.jpg)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![04fig08.jpg](04fig08.jpg)'
- en: 'OpenAI lists all of its currently supported environments on its website: [https://gym.openai.com/envs/](https://gym.openai.com/envs/).
    At the time of writing, they are broken down into seven categories:'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI在其网站上列出了其目前支持的所有环境：[https://gym.openai.com/envs/](https://gym.openai.com/envs/)。在撰写本文时，它们被分为七个类别：
- en: Algorithms
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法
- en: Atari
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atari
- en: Box2D
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Box2D
- en: Classic control
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典控制
- en: MuJoCo
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo
- en: Robotics
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人技术
- en: Toy text
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩具文本
- en: You can also view the entire list of environments from the OpenAI registry in
    your Python shell with the following code.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用以下代码在Python shell中查看OpenAI注册表中环境的完整列表。
- en: Listing 4.1\. Listing the OpenAI Gym environments
  id: totrans-798
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1。列出OpenAI Gym环境
- en: '[PRE51]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: There are hundreds of environments to choose from (797 in v0.9.6). Unfortunately,
    some of these environments require licenses (MuJoCo) or external dependencies
    (Box2D, Atari) and will therefore require a bit of setup time. We will be starting
    with a simple example, CartPole ([figure 4.9](#ch04fig09)), to avoid any unnecessary
    complications and to get us coding right away.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 可供选择的环境有数百种（v0.9.6版本中有797种）。不幸的是，其中一些环境需要许可证（MuJoCo）或外部依赖项（Box2D、Atari），因此将需要一些设置时间。我们将从一个简单的例子开始，CartPole（[图4.9](#ch04fig09)），以避免任何不必要的复杂性，并立即开始编码。
- en: Figure 4.9\. A screenshot from the CartPole game environment in OpenAI Gym.
    There is a cart that can roll left or right, and on top of it is a pole on a pivot.
    The goal is to balance the pole upright on the cart by carefully moving the cart
    left or right.
  id: totrans-801
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9。OpenAI Gym中CartPole游戏环境的截图。有一个可以左右滚动的车，车顶上有一个可以旋转的杆。目标是小心地移动车，使杆保持垂直。
- en: '![](04fig09.jpg)'
  id: totrans-802
  prefs: []
  type: TYPE_IMG
  zh: '![04fig09.jpg](04fig09.jpg)'
- en: 4.3.1\. CartPole
  id: totrans-803
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1. CartPole
- en: The CartPole environment falls under OpenAI’s Classic Control section, and it
    has a very simple objective—don’t let the pole fall over. It’s the game equivalent
    of trying to balance a pencil on the tip of your finger. In order to balance the
    pole successfully, you have to apply just the right amount of small left and right
    movements to the cart. In this environment, there are only two actions that correspond
    to making a small push left or right.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole环境属于OpenAI的经典控制部分，它有一个非常简单的目标——不要让杆倒下。这是试图用手指尖平衡铅笔的游戏等效物。为了成功平衡杆，你必须对车施加恰到好处的微小左右移动。在这个环境中，只有两个动作，对应于向左或向右轻微推动。
- en: In the OpenAI Gym API, environments with discrete action spaces all have actions
    represented as integers from 0 to the total number of actions for the particular
    environment, so in CartPole the possible actions are 0 and 1, which denote a push
    to the left or to the right. The state is represented as a vector of length 4
    that indicates the cart position, cart velocity, pole angle, and pole velocity.
    We receive a reward of +1 for every step the pole has not fallen over, which happens
    when the pole angle is more than 12° from the center or when the cart position
    is outside the window. Hence, the goal of CartPole is to maximize the length of
    the episode, since each step returns a positive +1 reward. More information can
    be found on the OpenAI Gym GitHub page ([https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)).
    Note that not every subsequent problem has a nice specification page like CartPole
    does, but we will define the scope of the problem beforehand in all subsequent
    chapters.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenAI Gym API 中，具有离散动作空间的环境都将动作表示为从 0 到特定环境动作总数之间的整数，因此 CartPole 中的可能动作是
    0 和 1，分别表示向左或向右推。状态由长度为 4 的向量表示，指示小车位置、小车速度、杆子角度和杆子速度。每当杆子没有倒下时，我们都会收到 +1 的奖励，这发生在杆子角度超过中心
    12° 或小车位置在窗口外时。因此，CartPole 的目标是最大化连续动作的长度，因为每一步都会返回一个正的 +1 奖励。更多信息可以在 OpenAI Gym
    GitHub 页面找到([https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0))。请注意，并非每个后续问题都有像
    CartPole 那样好的规范页面，但我们在所有后续章节中都会事先定义问题的范围。
- en: 4.3.2\. The OpenAI Gym API
  id: totrans-806
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. OpenAI Gym API
- en: The OpenAI Gym has been built to be incredibly easy to use, and there are less
    than half a dozen methods that you’ll routinely use. You already saw one in [listing
    4.1](#ch04ex01) where we listed all the available environments. Another important
    method is creating an environment.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 的构建旨在极其易于使用，你将经常使用的不过寥寥数种方法。你已经在[列表 4.1](#ch04ex01)中看到了其中之一，我们列出了所有可用的环境。另一个重要的方法是创建一个环境。
- en: Listing 4.2\. Creating an environment in OpenAI Gym
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2\. 在 OpenAI Gym 中创建环境
- en: '[PRE52]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: From now on, we will be interacting solely with this `env` variable. We need
    a way to observe the current state of the environment and then to interact with
    it. There are only two methods you need to do this.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将仅与这个 `env` 变量进行交互。我们需要一种方法来观察环境的当前状态，然后与之交互。你只需要两种方法来做这件事。
- en: Listing 4.3\. Taking an action in CartPole
  id: totrans-811
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. 在 CartPole 中采取行动
- en: '[PRE53]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The `reset` method initializes the environment and returns the first state.
    For this example, we used the `sample` method of the `env.action_space` object
    to sample a random action. Soon enough, we’ll sample actions from a trained policy
    network that will act as our reinforcement learning agent.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset` 方法初始化环境并返回第一个状态。在这个例子中，我们使用了 `env.action_space` 对象的 `sample` 方法来采样一个随机动作。不久之后，我们将从训练好的策略网络中采样动作，该网络将充当我们的强化学习代理。'
- en: Once we initialize the environment, we are free to interact with it via the
    `step` method. The `step` method returns four important variables that our training
    loop needs access to in order to run. The first parameter, `state`, represents
    the next state after we take the action. The second parameter, `reward`, is the
    reward at that time step, which for our CartPole problem is 1 unless the pole
    has fallen down. The third parameter, `done`, is a Boolean that indicates whether
    or not a terminal state has been reached. For our CartPole problem, this would
    always return `false` until the pole has fallen or the cart has moved outside
    the window. The last parameter, `info`, is a dictionary with diagnostic information
    that may be useful for debugging, but we will not use it. That’s all you need
    to know to get most environments up and running in OpenAI Gym.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化环境后，我们可以通过 `step` 方法自由地与之交互。`step` 方法返回四个重要的变量，我们的训练循环需要访问这些变量才能运行。第一个参数，`state`，代表采取行动后的下一个状态。第二个参数，`reward`，是当时步骤的奖励，对于我们的
    CartPole 问题，除非杆子倒下，否则奖励为 1。第三个参数，`done`，是一个布尔值，表示是否达到了终端状态。对于我们的 CartPole 问题，这始终会返回
    `false`，直到杆子倒下或小车移出窗口。最后一个参数，`info`，是一个包含可能对调试有用的诊断信息的字典，但我们将不会使用它。这就是你需要的所有知识，以便在
    OpenAI Gym 中将大多数环境启动并运行。
- en: 4.4\. The REINFORCE algorithm
  id: totrans-815
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. REINFORCE 算法
- en: Now that you how to create an OpenAI Gym environment and have hopefully developed
    an intuition for the policy gradient algorithm, let’s dive in to getting a working
    implementation. Our discussion of policy gradients in the previous section focused
    on a particular algorithm that has been around for decades (like most of deep
    learning and reinforcement learning) called REINFORCE (yes, it’s always fully
    capitalized). We’re going to consolidate what we discussed previously, formalize
    it, and then turn it into Python code. Let’s implement the REINFORCE algorithm
    for the CartPole example.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何创建OpenAI Gym环境，并且可能已经对策略梯度算法有了直觉，让我们深入探讨实现一个工作版本。上一节中我们对策略梯度的讨论集中在一种存在了几十年的特定算法上（就像大多数深度学习和强化学习一样），称为REINFORCE（是的，它总是全部大写）。我们将整合之前讨论的内容，将其形式化，然后将其转换为Python代码。让我们为CartPole示例实现REINFORCE算法。
- en: 4.4.1\. Creating the policy network
  id: totrans-817
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 创建策略网络
- en: We will build and initialize a neural network that serves as a policy network.
    The policy network will accept state vectors as inputs, and it will produce a
    (discrete) probability distribution over the possible actions. You can think of
    the agent as a thin wrapper around the policy network that samples from the probability
    distribution to take an action. Remember, an agent in reinforcement learning is
    whatever function or algorithm takes a state and returns a concrete action that
    will be executed in the environment.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建并初始化一个作为策略网络的神经网络。策略网络将接受状态向量作为输入，并生成关于可能动作的（离散）概率分布。你可以将智能体视为围绕策略网络的一个薄包装，它从概率分布中采样以采取一个动作。记住，强化学习中的智能体是任何接收状态并返回将在环境中执行的具体动作的函数或算法。
- en: Let’s express this in code.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码来表示这一点。
- en: Listing 4.4\. Setting up the policy network
  id: totrans-820
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4\. 设置策略网络
- en: '[PRE54]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '***1*** The input data is length 4 (“l1” is short for layer 1)'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 输入数据长度为4（“l1”是层1的简称）'
- en: '***2*** The middle layer produces a vector of length 150'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 中间层生成一个长度为150的向量'
- en: '***3*** The output is a 2-length vector for the left and right actions'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 输出是一个表示左右动作的2长度向量'
- en: '***4*** The output is a softmax probability distribution over actions'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 输出是动作的softmax概率分布'
- en: 'That should all look fairly familiar to you at this point. The model is only
    two layers: a leaky ReLU activation function for the first layer, and the `Softmax`
    function for the last layer. We chose the leaky ReLU because it performed better
    empirically. You saw the `Softmax` function back in [chapter 2](kindle_split_011.html#ch02);
    it just takes an array of numbers and squishes them into the range of 0 to 1 and
    makes sure they all sum to 1, basically creating a discrete probability distribution
    out of any list of numbers that are not probabilities to start with. For example,
    `softmax([-1,2,3]) = [0.0132, 0.2654, 0.7214]`. Unsurprisingly, the `Softmax`
    function will turn the bigger numbers into larger probabilities.'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这一切对你来说应该都很熟悉。这个模型只有两层：第一层使用带泄漏的ReLU激活函数，最后一层使用`Softmax`函数。我们选择带泄漏的ReLU是因为它在经验上表现更好。你之前在[第2章](kindle_split_011.html#ch02)中见过`Softmax`函数；它只是将一个数字数组压缩到0到1的范围内，并确保它们的总和为1，基本上将任何不是概率的数字列表转换为一个离散概率分布。例如，`softmax([-1,2,3])
    = [0.0132, 0.2654, 0.7214]`。不出所料，`Softmax`函数会将较大的数字转换为较大的概率。
- en: 4.4.2\. Having the agent interact with the environment
  id: totrans-827
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 智能体与环境交互
- en: The agent consumes the state and takes an action, *a*, probabilistically. More
    specifically, the state is input to the policy network, which then produces the
    probability distribution over the actions *P*(*A*|*θ*,*S[t]*) given its current
    parameters and the state. Note, the capital *A* refers to the set of all possible
    actions given the state, whereas the lowercase *a* generally refers to a particular
    action.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体消耗状态并概率地采取动作*a*。更具体地说，状态被输入到策略网络中，然后根据其当前参数和状态生成动作的概率分布*P*(*A*|*θ*,*S[t]*)。注意，大写*A*指的是给定状态的所有可能动作的集合，而小写*a*通常指的是特定的一个动作。
- en: The policy network might return a discrete probability distribution in the form
    of a vector, such as [0.25, 0.75] for our two possible actions in CartPole. This
    means the policy network predicts that action 0 is the best with 25% probability,
    and action 1 is the best with 75% probability (or confidence). We call this array
    `pred`.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 政策网络可能会以向量的形式返回一个离散概率分布，例如CartPole中我们两个可能动作的[0.25, 0.75]。这意味着政策网络预测动作0有25%的概率是最佳选择，动作1有75%的概率（或信心）是最佳选择。我们称这个数组为`pred`。
- en: Listing 4.5\. Using the policy network to sample an action
  id: totrans-830
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5\. 使用政策网络采样动作
- en: '[PRE55]'
  id: totrans-831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '***1*** Calls the policy network model to produce predicted action probabilities'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 调用政策网络模型以产生预测动作概率'
- en: '***2*** Samples an action from the probability distribution produced by the
    policy network'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从政策网络产生的概率分布中采样一个动作'
- en: '***3*** Takes the action and receives the new state and reward. The info variable
    is produced by the environment but is irrelevant.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 执行动作并接收新的状态和奖励。信息变量由环境产生，但无关紧要。'
- en: The environment responds to the action by producing a new state, *s*[2], and
    a reward, *r*[2]. We store those into two arrays (a `states` array and an `actions`
    array) for when we need to update our model after the episode ends. We then plug
    the new state into our model, get a new state and reward, store those, and repeat
    until the episode ends (the pole falls over and the game is finished).
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 环境对动作做出响应，产生新的状态 *s*[2] 和奖励 *r*[2]。我们将这些存储到两个数组（`states` 数组和 `actions` 数组）中，以便在剧集结束后更新我们的模型。然后我们将新状态插入到我们的模型中，得到新的状态和奖励，存储这些，并重复直到剧集结束（杆子倒下，游戏结束）。
- en: 4.4.3\. Training the model
  id: totrans-836
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 训练模型
- en: 'We train the policy network by updating the parameters to minimize the objective
    (i.e., loss) function. This involves three steps:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过更新参数以最小化目标（即损失）函数来训练政策网络。这涉及三个步骤：
- en: Calculate the probability of the action actually taken at each time step.
  id: totrans-838
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在每个时间步实际采取的动作的概率。
- en: Multiply the probability by the discounted return (the sum of rewards).
  id: totrans-839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将概率乘以折现回报（奖励的总和）。
- en: Use this probability-weighted return to backpropagate and minimize the loss.
  id: totrans-840
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个概率加权的回报来进行反向传播并最小化损失。
- en: We’ll look at these in turn.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次查看这些内容。
- en: Calculating the probability of the action
  id: totrans-842
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算采取动作的概率
- en: Calculating the probability of the action taken is easy enough. We can use the
    stored past transitions to recompute the probability distributions using the policy
    network, but this time we extract just the predicted probability for the action
    that was actually taken. We’ll denote this quantity *P*(*a[t]*|*θ*,*s[t]*); this
    is a single probability value, like 0.75.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 计算采取动作的概率是足够的简单。我们可以使用存储的过去转换来使用政策网络重新计算概率分布，但这次我们只提取实际采取的动作的预测概率。我们将这个量表示为
    *P*(*a[t]*|*θ*,*s[t]*)；这是一个单一的概率值，例如0.75。
- en: To be concrete, let’s say the current state is *S*[5] (the state at time step
    5). We input that into the policy network and it returns *P**[θ]*(*A*|*s*[5])
    = [0.25,0.75]. We sample from this distribution and take action *a* = 1 (the second
    element in the action array), and after this the pole falls over and the episode
    has ended. The total duration of the episode was *T* = 5\. For each of these 5
    time steps, we took an action according to *P**[θ]*(*A*|*s[t]*) and we stored
    the specific probabilities of the actions that were actually taken, *P**[θ]*(*a*|*s[t]*),
    in an array, which might look like `[0.5, 0.3, 0.25, 0.5, 0.75]`. We simply multiply
    these probabilities by the discounted rewards (explained in the next section),
    take the sum, multiply it by –1, and call that our overall loss for this episode.
    Unlike Gridworld, in CartPole the last action is the one that loses the episode;
    we discount it the most since we want to penalize the worst move the most. In
    Gridworld we would do the opposite and discount the first action in the episode
    most, since it would be the least responsible for winning or losing.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明，假设当前状态是 *S*[5]（第 5 个时间步的状态）。我们将它输入到策略网络中，它返回 *P**[θ]*(*A*|*s*[5]) = [0.25,0.75]。我们从这个分布中进行采样，并采取动作
    *a* = 1（动作数组中的第二个元素），然后杆子倒下，场景结束。场景的总持续时间为 *T* = 5。对于这 5 个时间步中的每一个，我们都根据 *P**[θ]*(*A*|*s[t]*)
    采取动作，并将实际采取的动作的具体概率 *P**[θ]*(*a*|*s[t]*）存储在一个数组中，这可能看起来像 `[0.5, 0.3, 0.25, 0.5,
    0.75]`。我们只需将这些概率乘以折扣奖励（下一节中解释），然后将总和乘以 -1，这就是该场景的整体损失。与 Gridworld 不同，在 CartPole
    中，最后一个动作是导致场景失败的动作；我们对其折扣最大，因为我们想对最差的动作进行最重的惩罚。在 Gridworld 中，我们会做相反的事情，对场景中的第一个动作进行最大的折扣，因为它对赢得或输掉比赛的责任最小。
- en: Minimizing this objective function will tend to increase those probabilities
    *P**[θ]*(*a*|*s[t]*) weighted by the discounted rewards. So every episode we’re
    tending to increase *P**[θ]*(*a*|*s[t]*), but for a particularly long episode
    (if we’re doing well in the game and get a large end-of-episode return) we will
    increase the *P**[θ]*(*a*|*s[t]*) to a greater degree. Hence, on average over
    many episodes we will reinforce the actions that are good, and the bad actions
    will get left behind. Since probabilities must sum to 1, if we increase the probability
    of a good action, that will automatically steal probability mass from the other
    presumably less good actions. Without this redistributive nature of probabilities,
    this scheme wouldn’t work (i.e., everything both good and bad would tend to increase).
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化这个目标函数往往会增加那些加权折扣奖励的 *P**[θ]*(*a*|*s[t]*) 概率。因此，在每次场景中，我们倾向于增加 *P**[θ]*(*a*|*s[t]*)，但对于一个特别长的场景（如果我们玩游戏表现良好并且获得大的场景结束回报），我们将以更大的程度增加
    *P**[θ]*(*a*|*s[t]*)。因此，在许多场景的平均值中，我们将加强好的动作，而坏的动作将被留下。由于概率必须加起来为 1，如果我们增加一个好动作的概率，那么这会自动从其他可能不太好的动作中窃取概率质量。如果没有这种概率的再分配性质，这个方案就不会起作用（即，好和坏的动作都会倾向于增加）。
- en: Calculating future rewards
  id: totrans-846
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算未来奖励
- en: We multiply *P*(*a[t]*|*θ*,*s[t]*) by the total reward (a.k.a. return) we received
    after this state. As mentioned earlier in the section, we can get the total reward
    by just summing the rewards (which is equal to the number of time steps the episode
    lasted in CartPole) and create a return array that starts with the episode duration
    and decrements by 1 until 1\. If the episode lasted 5 time steps, the return array
    would be `[5,4,3,2,1]`. This makes sense because our first action should be rewarded
    the most, since it is the least responsible for the pole falling and losing the
    episode. In contrast, the action right before the pole fell is the worst action,
    and it should have the smallest reward. But this is a linear decrement—we want
    to discount the rewards exponentially.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *P*(*a[t]*|*θ*,*s[t]*) 乘以在此状态后收到的总奖励（也称为回报）。如前文所述，我们可以通过简单地累加奖励（在 CartPole
    中等于该场景持续的时间步数）来获得总奖励，并创建一个以场景持续时间为起始值，递减至 1 的回报数组。如果场景持续了 5 个时间步，则回报数组将是 `[5,4,3,2,1]`。这很有道理，因为我们的第一个动作应该得到最多的奖励，因为它对杆子倒下和场景失败的责任最小。相反，杆子倒下前的动作是最差的动作，它应该得到最小的奖励。但这是一个线性递减——我们希望指数衰减奖励。
- en: To compute the discounted rewards, we make an array of γ*[t]* by taking our
    γ parameter, which may be set to 0.99 for example, and exponentiating it according
    to the distance from the end of the episode. For example, we start with `gamma_t
    = [0.99, 0.99, 0.99, 0.99, 0.99]`, then create another array of exponents `exp
    = [1,2,3,4,5]` and compute `torch.power(gamma_t, exp)`, which will give us `[1.0,
    0.99, 0.98, 0.97, 0.96]`.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算折扣奖励，我们通过将γ参数（例如可能设置为0.99）进行指数化，并根据剧集结束的距离来计算它，来创建一个γ*[t]*的数组。例如，我们开始时是`gamma_t
    = [0.99, 0.99, 0.99, 0.99, 0.99]`，然后创建另一个指数数组`exp = [1,2,3,4,5]`，并计算`torch.power(gamma_t,
    exp)`，这将给我们`[1.0, 0.99, 0.98, 0.97, 0.96]`。
- en: The loss function
  id: totrans-849
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 损失函数
- en: Now that we have discounted returns, we can use these to compute the loss function
    to train the policy network. As we discussed previously, we make our loss function
    the negative log-probability of the action given the state, scaled by the reward
    returns. In PyTorch, this is defined as `-1 * torch.sum(r * torch.log(preds))`.
    We compute the loss with the data we’ve collected for the episode, and run the
    `torch` optimizer to minimize the loss. Let’s run through some actual code.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了折扣回报，我们可以使用这些来计算损失函数以训练策略网络。正如我们之前讨论的，我们将损失函数定义为给定状态的行动的对数似然，并按奖励回报进行缩放。在PyTorch中，这被定义为`-1
    * torch.sum(r * torch.log(preds))`。我们使用我们收集的剧集数据来计算损失，并运行`torch`优化器以最小化损失。让我们通过一些实际的代码来运行。
- en: Listing 4.6\. Computing the discounted rewards
  id: totrans-851
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.6. 计算折扣奖励
- en: '[PRE56]'
  id: totrans-852
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '***1*** Computes exponentially decaying rewards'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 计算指数衰减奖励'
- en: '***2*** Normalizes the rewards to be within the [0,1] interval to improve numerical
    stability'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将奖励归一化到[0,1]区间以提高数值稳定性'
- en: Here we define a special function to compute the discounted rewards given an
    array of rewards that would look like `[50,49,48,47,...]` if the episode lasted
    50 time steps. It essentially turns this linear sequence of rewards into an exponentially
    decaying sequence of rewards (e.g., `[50.0000, 48.5100, 47.0448, 45.6041, ...]`),
    and then it divides by the maximum value to bound the values in the interval `[0,1]`.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义一个特殊函数来计算给定奖励数组的折扣奖励，如果剧集持续了50个时间步，则这些奖励将看起来像`[50,49,48,47,...]`。它基本上将这个奖励的线性序列转换成指数衰减的奖励序列（例如，`[50.0000,
    48.5100, 47.0448, 45.6041, ...]`），然后除以最大值以将值限制在区间[0,1]内。
- en: The reason for this normalization step is to improve the learning efficiency
    and stability, since it keeps the return values within the same range no matter
    how big the raw return is. If the raw return is 50 in the beginning of training
    but then reaches 200 by the end of training, the gradients are going to change
    by almost an order of magnitude, which hampers stability. It will still work without
    normalization, but not as reliably.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 进行归一化步骤的原因是为了提高学习效率和稳定性，因为它无论原始回报有多大，都保持回报值在相同的范围内。如果训练开始时的原始回报是50，但到训练结束时达到200，梯度将几乎改变一个数量级，这会阻碍稳定性。即使不进行归一化也可以工作，但可靠性不高。
- en: Backpropagating
  id: totrans-857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向传播
- en: Now that we have all the variables in our objective function, we can calculate
    the loss and backpropagate to adjust the parameters. The following listing shows
    the loss function, which is just a Python translation of the math we described
    earlier.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有变量放入我们的目标函数中，我们可以计算损失并进行反向传播以调整参数。以下列表显示了损失函数，它只是我们之前描述的数学的Python翻译。
- en: Listing 4.7\. Defining the loss function
  id: totrans-859
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7. 定义损失函数
- en: '[PRE57]'
  id: totrans-860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '***1*** The loss function expects an array of action probabilities for the
    actions that were taken and the discounted rewards.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 损失函数期望一个动作概率数组，用于已采取的动作和折扣奖励。'
- en: '***2*** It computes the log of the probabilities, multiplies by the discounted
    rewards, sums them all, and flips the sign.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 计算概率的对数，乘以折扣奖励，将它们全部相加，并翻转符号。'
- en: 4.4.4\. The full training loop
  id: totrans-863
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4. 完整训练循环
- en: Initialize, collect experiences, calculate the loss from those experiences,
    backpropagate, and repeat. The following listing defines the full training loop
    of our REINFORCE agent.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化、收集经验、从这些经验中计算损失、反向传播，并重复。以下列表定义了我们的REINFORCE代理的完整训练循环。
- en: Listing 4.8\. The REINFORCE training loop
  id: totrans-865
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.8. REINFORCE训练循环
- en: '[PRE58]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '***1*** A list to keep track of the episode length over training time'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 一个用于在训练时间内跟踪剧集长度的列表'
- en: '***2*** A list of state, action, rewards (but we ignore the reward)'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 状态、动作、奖励的列表（但我们忽略了奖励）'
- en: '***3*** While in the episode'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 在剧集进行时'
- en: '***4*** Gets the action probabilities'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 获取动作概率'
- en: '***5*** Selects an action stochastically'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 随机选择一个动作'
- en: '***6*** Takes the action in the environment'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 在环境中采取动作'
- en: '***7*** Stores this transition'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 存储这个转换'
- en: '***8*** If the game is lost, breaks out of the loop'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 如果游戏失败，则退出循环'
- en: '***9*** Stores the episode length'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 存储回合长度'
- en: '***10*** Collects all the rewards in the episode in a single tensor'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 将回合中的所有奖励收集到一个单独的张量中'
- en: '***11*** Computes the discounted version of the rewards'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 计算奖励的折现版本'
- en: '***12*** Collects the states in the episode in a single tensor'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 将回合中的状态收集到一个单独的张量中'
- en: '***13*** Collects the actions in the episode in a single tensor'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13*** 将回合中的动作收集到一个单独的张量中'
- en: '***14*** Recomputes the action probabilities for all the states in the episode'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14*** 重新计算回合中所有状态的动作概率'
- en: '***15*** Subsets the action-probabilities associated with the actions that
    were actually taken'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***15*** 对实际采取的动作相关的动作概率进行子集'
- en: We start an episode, use the policy network to take actions, and record the
    states and actions we observe. Then, once we break out of an episode, we have
    to recompute the predicted probabilities to use in our loss function. Since we
    record all the transitions in each episode as a list of tuples, once we’re out
    of the episode we can separate each component of each transition (the state, action,
    and reward) into separate tensors to train on a batch of data at a time. If you
    run this code, you should be able to plot the episode duration against the episode
    number, and you will hopefully see a nicely increasing trend, as in [figure 4.10](#ch04fig10).
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始一个回合，使用策略网络来采取行动，并记录我们观察到的状态和动作。然后，一旦我们退出一个回合，我们必须重新计算用于损失函数的预测概率。由于我们在每个回合中记录所有转换为一个元组的列表，一旦我们退出回合，我们可以将每个转换的每个组件（状态、动作和奖励）分别分离成单独的张量，以便一次训练一批数据。如果你运行这段代码，你应该能够绘制出回合持续时间与回合编号的关系图，你可能会看到一条稳步上升的趋势，就像[图
    4.10](#ch04fig10) 一样。
- en: Figure 4.10\. After training the policy network to 500 epochs, we get a plot
    that demonstrates the agent really is learning how to play CartPole. Note that
    this is a moving average plot with a window of 50 to smooth the plot.
  id: totrans-883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10\. 在将策略网络训练到 500 个回合后，我们得到了一个展示代理真正学会如何玩 CartPole 的图表。请注意，这是一个窗口为 50 的移动平均图，用于平滑图表。
- en: '![](04fig10_alt.jpg)'
  id: totrans-884
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig10_alt.jpg)'
- en: The agent learns how to play CartPole! The nice thing about this example is
    that it should be able to train in under a minute on your laptop with just the
    CPU. The state of CartPole is just a 4-dimensional vector, and our policy network
    is only two small layers, so it’s much faster to train than the DQN we created
    to play Gridworld. OpenAI’s documentation says that the game is considered “solved”
    if the agent can play an episode beyond 200 time steps. Although the plot looks
    like it tops off at around 190, that’s because it’s a moving average plot. There
    are many episodes that reach 200 but a few times where it randomly fails early
    on, bringing the average down a bit. Also, we capped the episode duration at 200,
    so if you increase the cap it will be able to play even longer.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 代理学会了如何玩 CartPole！这个例子很棒的地方在于，它应该能在你的笔记本电脑上仅用 CPU 在一分钟内完成训练。CartPole 的状态只是一个
    4 维向量，我们的策略网络只有两层很小，所以它比我们为玩 Gridworld 创建的 DQN 训练得快得多。OpenAI 的文档表示，如果代理能够玩一个超过
    200 个时间步的回合，则认为游戏已被“解决”。尽管图表看起来在约 190 步时达到顶峰，但这是因为它是一个移动平均图。有许多回合达到了 200 步，但有时它会在早期随机失败，使平均数略有下降。此外，我们将回合持续时间限制在
    200 步，所以如果你增加限制，它将能够玩得更久。
- en: 4.4.5\. Chapter conclusion
  id: totrans-886
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.5\. 章节结论
- en: REINFORCE is an effective and very simple way of training a policy function,
    but it’s a little too simple. For CartPole it works very well, since the state
    space is very small and there are only two actions. If we’re dealing with an environment
    with many more possible actions, reinforcing all of them each episode and hoping
    that on average it will only reinforce the good actions becomes less and less
    reliable. In the next two chapters we’ll explore more sophisticated ways of training
    the agent.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 是一种有效且非常简单的训练策略函数的方法，但它有点过于简单。对于 CartPole 来说，由于状态空间非常小，只有两种动作，所以它工作得非常好。如果我们处理的是一个有许多可能动作的环境，那么在每个回合中强化所有这些动作，并希望平均来说只会强化好的动作，这种方法变得越来越不可靠。在接下来的两个章节中，我们将探讨更复杂的训练代理的方法。
- en: Summary
  id: totrans-888
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Probability* is a way of assigning degrees of belief about different possible
    outcomes in an unpredictable process. Each possible outcome is assigned a probability
    in the interval [0,1] such that all probabilities for all outcomes sum to 1\.
    If we believe a particular outcome is more likely than another, we assign it a
    higher probability. If we receive new information, we can change our assignments
    of probabilities.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率* 是在不可预测过程中对不同可能结果分配信念程度的一种方式。每个可能的结果都被分配一个在区间 [0,1] 内的概率，使得所有结果的概率总和为1。如果我们认为某个结果比另一个结果更有可能发生，我们就给它分配更高的概率。如果我们收到新的信息，我们可以改变我们对概率的分配。'
- en: '*Probability distribution* is the full characterization of assigned probabilities
    to possible outcomes. A probability distribution can be thought of as a function
    *P*:*O* → [0,1] that maps all possible outcomes to a real number in the interval
    [0,1] such that the sum of this function over all outcomes is 1.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率分布*是对可能结果分配概率的完整描述。概率分布可以被视为一个函数 *P*:*O* → [0,1]，它将所有可能的结果映射到区间 [0,1] 内的实数，使得该函数在所有结果上的总和为1。'
- en: A *degenerate probability distribution* is a probability distribution in which
    only 1 outcome is possible (i.e., it has probability of 1, and all other outcomes
    have a probability of 0).
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*退化的概率分布* 是一种只有1个结果可能发生的概率分布（即，它有1的概率，其他所有结果都有0的概率）。'
- en: '*Conditional probability* is the probability assigned to an outcome, assuming
    you have some additional information (the information that is conditioned).'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*条件概率* 是在假设你有一些额外信息（被条件化的信息）的情况下，对一个结果分配的概率。'
- en: A *policy* is a function, π*S* → *A*, that maps states to actions and is usually
    implemented as a probabilistic function, π*P*(*A*|*S*), that creates a probability
    distribution over actions given a state.
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略* 是一个函数，π*S* → *A*，它将状态映射到动作，通常实现为一个概率函数，π*P*(*A*|*S*)，它根据状态创建一个动作的概率分布。'
- en: The *return* is the sum of discounted rewards in an episode of the environment.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回报*是环境中一个场景中折现奖励的总和。'
- en: A *policy gradient method* is a reinforcement learning approach that tries to
    directly learn a policy by generally using a parameterized function as a policy
    function (e.g., a neural network) and training it to increase the probability
    of actions based on the observed rewards.
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略梯度方法* 是一种强化学习方法，它试图通过通常使用参数化函数作为策略函数（例如，神经网络）并对其进行训练来直接学习策略，以增加基于观察到的奖励的动作概率。'
- en: '*REINFORCE* is the simplest implementation of a policy gradient method; it
    essentially maximizes the probability of an action times the observed reward after
    taking that action, such that each action’s probability (given a state) is adjusted
    according to the size of the observed reward.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*REINFORCE* 是策略梯度方法的最简单实现；它本质上最大化了采取该动作后观察到的奖励乘以该动作的概率，使得每个动作的概率（给定一个状态）根据观察到的奖励的大小进行调整。'
- en: Chapter 5\. Tackling more complex problems with actor-critic methods
  id: totrans-897
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 使用actor-critic方法解决更复杂的问题
- en: '*This chapter covers*'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: The limitations of the REINFORCE algorithm
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REINFORCE算法的局限性
- en: Introducing a *critic* to improve sample efficiency and decrease variance
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一个 *评价者* 来提高样本效率和降低方差
- en: Using the advantage function to speed up convergence
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优势函数来加速收敛
- en: Speeding up the model by parallelizing training
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过并行化训练来加速模型
- en: In the previous chapter we introduced a vanilla version of a policy gradient
    method called REINFORCE. This algorithm worked fine for the simple CartPole example,
    but we want to be able to apply reinforcement learning to more complex environments.
    You already saw that deep Q-networks can be quite effective when the action space
    is discrete, but it has the drawback of needing a separate policy function such
    as epsilon-greedy. In this chapter you’ll learn how to combine the advantages
    of REINFORCE and those of DQN to create a class of algorithms called actor-critic
    models. These have proven to yield state-of-the-art results in many domains.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了一种名为REINFORCE的vanilla版本的策略梯度方法。这个算法对于简单的CartPole示例来说效果不错，但我们希望能够将强化学习应用于更复杂的环境。你已经看到，当动作空间是离散的时候，深度Q网络可以非常有效，但它有一个缺点，就是需要一个单独的策略函数，比如epsilon-greedy。在本章中，你将学习如何结合REINFORCE和DQN的优点，创建一类称为actor-critic模型的算法。这些算法在许多领域已经证明了其最先进的结果。
- en: The REINFORCE algorithm is generally implemented as an *episodic algorithm*,
    meaning that we only apply it to update our model parameters after the agent has
    completed an entire episode (and collected rewards along the way). Recall that
    the policy is a function, *πS* → *P*(*a*). That is, it’s a function that takes
    a state and returns a probability distribution over actions ([figure 5.1](#ch05fig01)).
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE算法通常被实现为一个*事件算法*，这意味着我们只在智能体完成整个事件（并在过程中收集奖励）之后更新我们的模型参数。回想一下，策略是一个函数，*πS*
    → *P*(*a*)。也就是说，它是一个接受状态并返回动作概率分布的函数（[图5.1](#ch05fig01)）。
- en: Figure 5.1\. A policy function takes a state and returns a probability distribution
    over actions, where a higher probability indicates an action that is more likely
    to result in the highest reward.
  id: totrans-905
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1\. 策略函数接受一个状态并返回一个动作的概率分布，其中更高的概率表示更有可能产生最高奖励的动作。
- en: '![](05fig01_alt.jpg)'
  id: totrans-906
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig01_alt.jpg)'
- en: Then we sample from this distribution to get an action, such that the most probable
    action (the “best” action) is most likely to be sampled. At the end of the episode,
    we compute the *return* of the episode, which is essentially the sum of the discounted
    rewards in the episode. The return is calculated as
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从这个分布中采样以获得一个动作，使得最可能的动作（“最佳”动作）最有可能被采样。在事件结束时，我们计算事件的*回报*，这实际上是事件中折现奖励的总和。回报的计算如下
- en: '![](pg112-1.jpg)'
  id: totrans-908
  prefs: []
  type: TYPE_IMG
  zh: '![](pg112-1.jpg)'
- en: 'After the game is over, the return for that episode is the sum of all the rewards
    acquired, multiplied by their respective discount rate, where *γ**[t]* exponentially
    decays as a function of time. For example, if action 1 was taken in state *A*
    and resulted in a return of +10, the probability of action 1 given state *A* will
    be increased a little, whereas if action 2 was taken in state *A* and resulted
    in a return of –20, the probability of action 2 given state *A* will decrease.
    Essentially, we minimize this loss function:'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏结束后，该事件的回报是所有获得奖励的总和，乘以各自的折现率，其中*γ**[t]*作为时间的函数指数衰减。例如，如果动作1在状态*A*中被采取并产生了+10的回报，那么给定状态*A*的动作1的概率将略有增加，而如果动作2在状态*A*中被采取并产生了–20的回报，那么给定状态*A*的动作2的概率将减少。本质上，我们最小化这个损失函数：
- en: '| Loss = –*log*(*P*(*a*&#124;*S*)) × *R* |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
  zh: '| 损失 = –*log*(*P*(*a*|*S*)) × *R* |'
- en: This says “minimize the logarithm of the probability of the action *a* given
    state *S* times the return *R*.” If the reward is a big positive number, and *P*(*a*[1]|*S[A]*)
    = 0.5 for example, minimizing this loss will involve increasing this probability.
    So with REINFORCE we just keep sampling episodes (or trajectories more generally)
    from the agent and environment, and periodically update the policy parameters
    by minimizing this loss.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着“最小化给定状态*S*的动作*a*的概率的对数乘以回报*R*。”如果奖励是一个大的正数，例如，*P*(*a*[1]|*S[A]*) = 0.5，那么最小化这个损失将涉及增加这个概率。所以，在REINFORCE中，我们只是持续地从智能体和环境采样事件（或更一般地说，轨迹），并定期通过最小化这个损失来更新策略参数。
- en: '|  |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-913
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember, we only apply the logarithm to the probability because a probability
    is bounded by 0 and 1, whereas the log probability is bounded by –∞ (negative
    infinity) and 0\. Given that numbers are represented by a finite number of bits,
    we can represent very small (close to 0) or very large (close to 1) probabilities
    without underflowing or overflowing the computer’s numerical precision. Logarithms
    also have nicer mathematical properties that we won’t cover, but that is why you’ll
    almost always see log probabilities used in algorithms and machine learning papers,
    even though we are conceptually interested in the raw probabilities themselves.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们只对概率应用对数，因为概率被限制在0和1之间，而对数概率被限制在–∞（负无穷大）和0之间。鉴于数字是由有限数量的位表示的，我们可以表示非常小（接近0）或非常大（接近1）的概率，而不会导致计算机数值精度的下溢或溢出。对数还具有更吸引人的数学性质，我们不会在这里介绍，但这就是为什么你几乎总是会在算法和机器学习论文中看到使用对数概率，尽管我们在概念上对原始概率本身感兴趣。
- en: '|  |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: By sampling a full episode, we get a pretty good idea of the true value of an
    action because we can see its downstream effects rather than just its immediate
    effect (which may be misleading due to randomness in the environment); this full
    episode sampling is under the umbrella of Monte Carlo approaches. But not all
    environments are episodic, and sometimes we want to be able to make updates in
    an incremental or *online* fashion, i.e., make updates at regular intervals irrespective
    of what is going on in the environment. Our deep Q-network did well in the non-episodic
    setting and it could be considered an online-learning algorithm, but it required
    an experience replay buffer in order to effectively learn.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采样一个完整剧集，我们可以很好地了解一个动作的真实价值，因为我们可以看到其下游效果，而不仅仅是其即时效果（这可能会由于环境中的随机性而具有误导性）；这种完整的剧集采样属于蒙特卡洛方法。但并非所有环境都是剧集式的，有时我们希望能够以增量或*在线*的方式更新，即在环境发生什么情况无关的情况下定期进行更新。我们的深度Q网络在非剧集设置中表现良好，它可以被认为是一个在线学习算法，但它需要一个经验重放缓冲区才能有效地学习。
- en: The replay buffer was necessary because true online learning where parameter
    updates are made after each action is unstable due to the inherent variance in
    the environment. An action taken once may by chance result in a big negative reward,
    but in expectation (the average long-term rewards) it may be a good action—updating
    after a single action may result in erroneous parameter updates that will ultimately
    prevent adequate learning.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区是必要的，因为在每次动作之后进行参数更新的真正在线学习由于环境固有的方差而不稳定。一次采取的动作可能偶然导致很大的负奖励，但在期望（平均长期奖励）中，它可能是一个好的动作——在单个动作之后更新可能会导致错误的参数更新，这最终会阻止适当的学习。
- en: In this chapter, we will introduce a new kind of policy gradient method called
    *distributed advantage actor-critic* (DA2C) that will have the online-learning
    advantages of DQN without a replay buffer. It will also have the advantages of
    policy methods where we can directly sample actions from the probability distribution
    over actions, thereby eliminating the need for choosing a policy (such as the
    epsilon-greedy policy) that we needed with DQN.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种名为*分布式优势演员-评论家*(DA2C)的新政策梯度方法，它将具有DQN的在线学习优势，而不需要重放缓冲区。它还将具有策略方法的优点，其中我们可以直接从动作的概率分布中采样动作，从而消除选择策略（如epsilon-greedy策略）的需求，这在DQN中是必需的。
- en: 5.1\. Combining the value and policy function
  id: totrans-919
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 结合值函数和策略函数
- en: The great thing about Q-learning is that it learns directly from the available
    information in the environment, which are the rewards. It basically learns to
    predict rewards, which we call values. If we use a DQN to play pinball, it will
    learn to predict the values for the two main actions—operating the left and right
    paddles. We’re then free to use these values to decide which action to take, generally
    opting for the action associated with the highest value.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的优点是它直接从环境中的可用信息中学习，这些信息是奖励。它基本上是学习预测奖励，我们称之为值。如果我们使用DQN来玩弹球，它将学习预测两个主要动作的值——操作左挡板和右挡板。然后我们可以自由地使用这些值来决定采取哪个动作，通常选择与最高值相关的动作。
- en: A policy gradient function is more directly connected to the concept of *reinforcement*,
    since we positively reinforce actions that result in a positive reward and negatively
    reinforce actions that lead to a negative reward. Hence, the policy function learns
    which actions are best in a more hidden way. In pinball, if we hit the left paddle
    and score a bunch of points, that action will get positively reinforced and will
    be more likely to be selected the next time the game is in a similar state.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度函数与*强化*的概念更直接相关，因为我们对导致正奖励的动作进行正强化，对导致负奖励的动作进行负强化。因此，策略函数以更隐蔽的方式学习哪些动作是最好的。在弹球游戏中，如果我们击打左挡板并获得大量分数，那么这个动作将得到正强化，在下一次游戏处于类似状态时更有可能被选中。
- en: In other words, Q-learning (such as DQN) uses a trainable function to directly
    model the value (the expected reward) of an action, given a state. This is a very
    intuitive way of solving a Markov decision process (MDP) since we only observe
    states and rewards—it makes sense to predict the rewards and then just take actions
    that have high predicted rewards. On the other hand, we saw the advantage of direct
    policy learning (such as policy gradients). Namely, we get a true conditional
    probability distribution over actions, *P*(*a*|*S*), that we can directly sample
    from to take an action. Naturally, someone decided it might be a good idea to
    combine these two approaches to get the advantages of both.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，Q-learning（如DQN）使用一个可训练的函数来直接模拟给定状态的动作价值（期望奖励）。这是一种解决马尔可夫决策过程（MDP）的非常直观的方法，因为我们只观察到状态和奖励——预测奖励然后只采取具有高预测奖励的动作是有意义的。另一方面，我们看到了直接策略学习（如策略梯度）的优势。也就是说，我们得到了一个关于动作的真正条件概率分布*P*(*a*|*S*)，我们可以直接从中采样来采取一个动作。自然地，有人决定将这两种方法结合起来可能是一个好主意，以获得两者的优势。
- en: 'In building such a combined value-policy learning algorithm, we will start
    with the policy learner as the foundation. There are two challenges we want to
    overcome to increase the robustness of the policy learner:'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这样的联合值策略学习算法时，我们将从策略学习器作为基础开始。我们想要克服两个挑战以提高策略学习器的鲁棒性：
- en: We want to improve the sample efficiency by updating more frequently.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望通过更频繁地更新来提高样本效率。
- en: We want to decrease the variance of the reward we used to update our model.
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望减少我们用于更新我们模型的奖励的方差。
- en: 'These problems are related, since the reward variance depends on how many samples
    we collect (more samples yields less variance). The idea behind a combined value-policy
    algorithm is to use the value learner to reduce the variance in the rewards that
    are used to train the policy. That is, instead of minimizing the REINFORCE loss
    that included direct reference to the observed return, *R*, from an episode, we
    instead add a baseline value such that the loss is now:'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题相关，因为奖励方差取决于我们收集了多少样本（更多的样本导致方差更小）。联合值策略算法背后的想法是使用值学习器来减少用于训练策略的奖励中的方差。也就是说，我们不是最小化包含对一幕中观察到的回报*R*的直接引用的REINFORCE损失，而是添加一个基线值，使得损失现在变为：
- en: '![](pg114.jpg)'
  id: totrans-927
  prefs: []
  type: TYPE_IMG
  zh: '![](pg114.jpg)'
- en: Here, *V*(*S*) is the value of state *S*, which is the state-value function
    (a function of the state) rather than an action-value function (a function of
    both state and action), although an action-value function could be used as well.
    This quantity, *V*(*S*) – *R*, is termed the *advantage*. Intuitively, the advantage
    quantity tells you how much better an action is, relative to what you expected.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V*(*S*)是状态*S*的价值，它是状态价值函数（状态的函数）而不是动作价值函数（状态和动作的函数），尽管也可以使用动作价值函数。这个量，*V*(*S*)
    – *R*，被称为*优势*。直观地说，优势量告诉你一个动作相对于你预期的有多好。
- en: '|  |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-930
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that the value function (state value or action value) implicitly depends
    on the choice of policy, so we ought to write *V**[π]*(*S*) to make it explicit;
    however, we drop the *π* subscript for notational simplicity. The policy’s influence
    on the value is crucial, since a policy of taking random actions all the time
    would result in all states being of more or less equally low value.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，价值函数（状态价值或动作价值）隐式地依赖于策略的选择，因此我们应该写*V**[π]*(*S*)来使其明确；然而，为了符号简单，我们省略了*π*下标。策略对价值的影响至关重要，因为始终采取随机动作的策略会导致所有状态的价值大致相等且较低。
- en: '|  |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Imagine that we’re training a policy on a Gridworld game with discrete actions
    and a small discrete state-space, such that we can use a vector where each position
    in the vector represents a distinct state, and the element is the average rewards
    observed after that state is visited. This look-up table would be the *V*(*S*).
    We might sample action 1 from the policy and observe reward +10, but then we’d
    use our value look-up table and see that on average we get +4 after visiting this
    state, so the advantage of action 1 given this state is 10 – 4 = +6\. This means
    that when we took action 1, we got a reward that was significantly better than
    what we expected based on past rewards from that state, which suggests that it
    was a good action. Compare this to the case where we take action 1 and receive
    reward +10 but our value look-up table says we expected to see +15, so the advantage
    is 10 – 15 = –5\. That suggests this was a relatively bad action despite the fact
    that we received a reasonably large positive reward.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们正在训练一个策略，用于具有离散动作和较小离散状态空间的Gridworld游戏，这样我们就可以使用一个向量，其中向量的每个位置代表一个独特的状态，元素是访问该状态后观察到的平均奖励。这个查找表将是*V*(*S*)。我们可能从策略中采样动作1并观察到奖励+10，但然后我们会使用我们的值查找表并看到在访问这个状态后平均得到+4，所以给定这个状态的行动1的优势是10
    – 4 = +6。这意味着当我们采取动作1时，我们得到的奖励比基于过去从该状态获得的奖励预期的要好得多，这表明这是一个好动作。将此与这种情况进行比较，我们采取动作1并收到奖励+10，但我们的值查找表表示我们预计会看到+15，所以优势是10
    – 15 = –5。这表明尽管我们得到了相当大的正奖励，但这个动作相对较差。
- en: Rather than using a look-up table, we will use some sort of parameterized model,
    such as a neural network that can be trained to predict expected rewards for a
    given state. So we want to simultaneously train a policy neural network and a
    state-value or action-value neural network.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不会使用查找表，而将使用某种参数化模型，例如可以训练来预测给定状态的预期奖励的神经网络。因此，我们希望同时训练策略神经网络和状态值或动作值神经网络。
- en: Algorithms of this sort are called *actor-critic* methods, where “actor” refers
    to the policy, because that’s where the actions are generated, and “critic” refers
    to the value function, because that’s what (in part) tells the actor how good
    its actions are. Since we’re using *R* – *V*(*S*) to train the policy rather than
    just *V*(*S*), this is called *advantage actor-critic* ([figure 5.2](#ch05fig02)).
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的算法被称为*演员-评论家*方法，其中“演员”指的是策略，因为动作是在那里生成的，而“评论家”指的是值函数，因为它（部分）告诉演员其动作有多好。由于我们使用*R*
    – *V*(*S*)来训练策略而不是仅仅*V*(*S*)，这被称为*优势演员-评论家*([图5.2](#ch05fig02))。
- en: Figure 5.2\. Q-learning falls under the category of value methods, since we
    attempt to learn action values, whereas policy gradient methods like REINFORCE
    directly attempt to learn the best actions to take. We can combine these two techniques
    into what’s called an actor-critic architecture.
  id: totrans-936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2\. Q学习属于值方法类别，因为我们试图学习动作值，而像REINFORCE这样的策略梯度方法则直接试图学习采取的最佳动作。我们可以将这两种技术结合成所谓的演员-评论家架构。
- en: '![](05fig02.jpg)'
  id: totrans-937
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig02.jpg)'
- en: '|  |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-939
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: What we have described so far would not be considered a true actor-critic method
    by some, because we’re only using the value function as a baseline and not using
    it to “bootstrap” by making a prediction about a future state based on the current
    state. You will see how bootstrapping comes into play soon.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所描述的不会被一些人视为真正的演员-评论家方法，因为我们只使用值函数作为基线，并没有使用它根据当前状态对未来的状态做出预测来“自举”。你很快就会看到自举是如何发挥作用的。
- en: '|  |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The policy network has a sensitive loss function that depends on the rewards
    collected at the end of the episode. If we naively tried to make online updates
    with the wrong type of environment, we might never learn anything because the
    rewards might be too sparse.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络有一个敏感的损失函数，它取决于在游戏结束时收集到的奖励。如果我们天真地尝试用错误类型的环境进行在线更新，我们可能永远学不到任何东西，因为奖励可能太稀疏。
- en: In Gridworld, which we introduced in [chapter 3](kindle_split_012.html#ch03),
    the reward is –1 on every move except for the end of the episode. The vanilla
    policy gradient method wouldn’t know what action to reinforce, since most actions
    result in the same reward of –1\. In contrast, a Q-network can learn decent Q
    values even when the rewards are sparse, because it *bootstraps*. When we say
    an algorithm bootstraps, we mean it can make a prediction from a prediction.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们第3章中介绍的Gridworld中，除了游戏结束时的奖励是1之外，每次移动的奖励都是-1。原始策略梯度方法不知道应该强化哪个动作，因为大多数动作的结果都是相同的奖励-1。相比之下，一个Q网络即使在奖励稀疏的情况下也能学习到合理的Q值，因为它*自举*。当我们说一个算法自举时，我们的意思是它可以从预测中做出预测。
- en: 'If we ask you what the temperature will be like two days from now, you might
    first predict what the temperature will be tomorrow, and then base your 2-day
    prediction on that ([figure 5.3](#ch05fig03)). You’re bootstrapping. If your first
    prediction is bad, your second may be even worse, so bootstrapping introduces
    a source of *bias*. Bias is a systematic deviation from the true value of something,
    in this case from the true Q values. On the other hand, making predictions from
    predictions introduces a kind of self-consistency that results in lower *variance*.
    Variance is exactly what it sounds like: a lack of precision in the predictions,
    which means predictions that vary a lot. In the temperature example, if we make
    our day 2 temperature prediction based on our day 1 prediction, it will likely
    not be too far from our day 1 prediction.'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们问你两天后的气温会怎样，你可能首先预测明天的气温，然后基于这个预测来做出两天的预测（[图5.3](#ch05fig03)）。你正在进行自举。如果你的第一个预测不准确，第二个预测可能会更糟，因此自举引入了*偏差*。偏差是指对某物真实值的系统性偏离，在这种情况下是从真实Q值中偏离。另一方面，从预测中做出预测引入了一种自我一致性，这导致*方差*降低。方差就是其字面意思：预测的不精确性，这意味着预测变化很大。在气温的例子中，如果我们根据第一天的预测来做出第二天的气温预测，它可能不会太偏离第一天的预测。
- en: Figure 5.3\. Read from left to right, raw data is fed into a predict temperature
    model that predicts the next day’s temperature. That prediction is then used in
    another prediction model that predicts day 2’s temperature. We can keep doing
    this, but initial errors will compound and our predictions will become inaccurate
    for distant predictions.
  id: totrans-945
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3。从左到右阅读，原始数据被输入到一个预测气温的模型中，该模型预测第二天的气温。然后，这个预测被用于另一个预测模型，预测第二天的气温。我们可以一直这样做，但初始错误会累积，我们的预测对于远期预测将变得不准确。
- en: '![](05fig03_alt.jpg)'
  id: totrans-946
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3](05fig03_alt.jpg)'
- en: 'Bias and variance are key concepts relevant to all of machine learning, not
    just deep learning or deep reinforcement learning ([figure 5.4](#ch05fig04)).
    Generally, if you reduce bias you will increase variance, and vice versa ([figure
    5.5](#ch05fig05)). For example, if we ask you to predict the temperature for tomorrow
    and the next day, you could give us a specific temperature: “The 2-day temperature
    forecast is 20.1 C and 20.5 C.” This is a high-precision prediction—you’ve given
    us a temperature prediction to a tenth of a degree! But you don’t have a crystal
    ball, so your prediction is almost surely going to be systematically off, biased
    toward whatever your prediction procedure involved. Or you could have told us,
    “The 2-day temperature forecast is 15–25 C and 18–27 C.” In this case, your prediction
    has a lot of spread, or variance, since you’re giving fairly wide ranges, but
    it has low bias, meaning that you have a good chance of the real temperatures
    falling in your intervals. This spread might be because your prediction algorithm
    didn’t give undue weight to any of the variables used for prediction, so it’s
    not particularly biased in any direction. Indeed, machine learning models are
    often *regularized* by imposing a penalty on the magnitude of the parameters during
    training; i.e., parameters that are significantly bigger or smaller than 0 are
    penalized. Regularization essentially means modifying your machine learning procedure
    in a way to mitigate overfitting.'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差是所有机器学习（而不仅仅是深度学习或深度强化学习）相关的关键概念（[图5.4](#ch05fig04)）。一般来说，如果你减少偏差，你会增加方差，反之亦然（[图5.5](#ch05fig05)）。例如，如果我们要求你预测明天的温度和第二天的温度，你可以给我们一个具体的温度：“2天的温度预报是20.1摄氏度和20.5摄氏度。”这是一个高精度的预测——你已经给我们提供了一个精确到十分之一的温度预测！但你没有水晶球，所以你的预测几乎肯定会有系统性偏差，偏向于你的预测程序所涉及的任何因素。或者你也可以告诉我们，“2天的温度预报是15-25摄氏度和18-27摄氏度。”在这种情况下，你的预测有很大的波动，或者说方差，因为你给出了相当宽的范围，但偏差较低，这意味着你有一个很好的机会，实际温度会落在你的区间内。这种波动可能是因为你的预测算法没有对用于预测的任何变量给予过分的权重，所以它不是特别偏向任何方向。实际上，机器学习模型通常在训练过程中通过惩罚参数的大小来进行正则化；即，惩罚那些显著大于或小于0的参数。正则化本质上意味着以减轻过拟合的方式修改你的机器学习过程。
- en: Figure 5.4\. The bias-variance tradeoff is a fundamental machine learning concept
    that says any machine learning model will have some degree of systematic deviation
    from the true data distribution and some degree of variance. You can try to reduce
    the variance of your model, but it will always come at the cost of increased bias.
  id: totrans-948
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4\. 偏差-方差权衡是机器学习的一个基本概念，它表明任何机器学习模型都将对真实数据分布有一定的系统性偏差和一定的方差。你可以尝试减少你模型的方差，但这也将不可避免地导致偏差的增加。
- en: '![](05fig04_alt.jpg)'
  id: totrans-949
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4的替代文本](05fig04_alt.jpg)'
- en: Figure 5.5\. The bias-variance tradeoff. Increasing model complexity can reduce
    bias, but it will increase variance. Reducing variance will increase bias.
  id: totrans-950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 偏差-方差权衡。增加模型复杂度可以减少偏差，但会增加方差。减少方差会增加偏差。
- en: '![](05fig05.jpg)'
  id: totrans-951
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5](05fig05.jpg)'
- en: We want to combine the potentially high-bias, low-variance value prediction
    with the potentially low-bias, high-variance policy prediction to get something
    with moderate bias and variance—something that will work well in the online setting.
    The role of the critic is hopefully starting to become clear. The actor (the policy
    network) will take a move, but the critic (the state-value network) will tell
    the actor how good or bad the action was, rather than only using the potentially
    sparse raw reward signals from the environment. Thus the critic will be a term
    in the actor’s loss function. The critic, just like with Q-learning, will learn
    directly from the reward signals coming from the environment, but the sequence
    of rewards will depend on the actions taken by the actor, so the actor affects
    the critic too, albeit more indirectly ([figure 5.6](#ch05fig06)).
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将可能具有高偏差、低方差的价值预测与可能具有低偏差、高方差的政策预测相结合，以获得具有适度偏差和方差的结果——这种结果在在线设置中会表现得很好。评论员（critic）的作用可能开始变得清晰。演员（actor，即策略网络）将采取行动，但评论员（critic，即状态-价值网络）将告诉演员这个动作是好是坏，而不仅仅是使用来自环境的可能稀疏的原始奖励信号。因此，评论员将成为演员损失函数中的一个项。评论员，就像Q-learning一样，将直接从环境中的奖励信号中学习，但奖励的序列将取决于演员采取的行动，因此演员也会影响评论员，尽管这种影响更为间接（[图5.6](#ch05fig06)）。
- en: Figure 5.6\. The general overview of actor-critic models. First, the actor predicts
    the best action and chooses the action to take, which generates a new state. The
    critic network computes the value of the old state and the new state. The relative
    value of *S[t+1]* is called its advantage, and this is the signal used to reinforce
    the action that was taken by the actor.
  id: totrans-953
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 动作-评论家模型的一般概述。首先，动作预测最佳动作并选择要采取的动作，这会生成一个新的状态。评论家网络计算旧状态和新状态的价值。*S[t+1]*的相对价值称为其优势，这是用来加强动作的信号。
- en: '![](05fig06_alt.jpg)'
  id: totrans-954
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6](05fig06_alt.jpg)'
- en: The actor is trained in part by using signals coming from the critic, but how
    exactly do we train a state-value function as opposed to the action value (Q)
    functions we’re more accustomed to? With action values, we computed the expected
    return (the sum of future discounted rewards) for a given state-action pair. Hence,
    we could predict whether a state-action pair would result in a nice positive reward,
    a bad negative reward, or something in between. But recall that with our DQN,
    our Q-network returned separate action values for each possible discrete action,
    so if we employ a reasonable policy like epsilon-greedy, the state value will
    essentially be the highest action value. Thus, the state-value function just computes
    this highest action value rather than separately computing action values for each
    action.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 动作部分是通过使用来自评论家的信号来训练的，但我们究竟如何训练状态-价值函数，而不是我们更习惯的动作-价值（Q）函数呢？对于动作价值，我们计算了给定状态-动作对的期望回报（未来折扣奖励的总和）。因此，我们可以预测一个状态-动作对是否会得到一个很好的正奖励，一个糟糕的负奖励，或者介于两者之间。但回想一下，在我们的DQN中，我们的Q网络为每个可能的不连续动作返回了单独的动作价值，所以如果我们采用像epsilon-greedy这样的合理策略，状态价值将基本上是最高动作价值。因此，状态-价值函数只是计算这个最高动作价值，而不是为每个动作分别计算动作价值。
- en: 5.2\. Distributed training
  id: totrans-956
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 分布式训练
- en: As we mentioned in the introduction, our goal in this chapter is to implement
    a model called distributed advantage actor-critic (DA2C), and we’ve discussed
    the “advantage actor-critic” part of the name at a conceptual level. Let’s do
    the same for the “distributed” part now.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中提到的，我们在这章的目标是实现一个名为分布式优势动作-评论家（DA2C）的模型，我们已经从概念上讨论了名称中的“优势动作-评论家”部分。现在让我们也来讨论“分布式”部分。
- en: For virtually all deep learning models we do *batch training*, where a random
    subset of our training data is batched together and we compute the loss for this
    entire batch before we backpropagate and do gradient descent. This is necessary
    because the gradients, if we trained with single pieces of data at a time, would
    have too much variance, and the parameters would never converge on their optimal
    values. We need to average out the noise in a batch of data to get the real signal
    before updating the model parameters.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 对于几乎所有深度学习模型，我们都进行*批量训练*，即将我们的训练数据的一个随机子集组合在一起，我们在进行反向传播和梯度下降之前计算整个批次的损失。这是必要的，因为如果一次只训练单个数据，梯度会有很大的方差，参数永远不会收敛到它们的最佳值。我们需要在批量数据中平均噪声，以在更新模型参数之前获得真实的信号。
- en: For example, if you’re training an image classifier to recognize hand-drawn
    digits, and you train it with one image at a time, the algorithm would think that
    the background pixels are just as important as the digits in the foreground; it
    can only see the signal when averaged together with other images. The same concept
    applies in reinforcement learning, which is why we had to use an experience replay
    buffer with DQN.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在训练一个图像分类器来识别手绘数字，并且你一次只用一个图像来训练它，那么算法就会认为背景像素和前景中的数字一样重要；它只能在与其他图像平均后才能看到信号。同样的概念也适用于强化学习，这就是为什么我们不得不使用带有DQN的经验回放缓冲区的原因。
- en: Having a sufficiently large replay buffer requires a lot of memory, and in some
    cases a replay buffer is impractical. A replay buffer is possible when your reinforcement
    learning environment and agent algorithm follow the strict criteria of a Markov
    decision process, and in particular, the Markov property. Recall the Markov property
    says that the optimal action for a state *S[t]* can be computed without reference
    to any prior states *S[t]*[–1]; there is no need to keep a history of previously
    visited states. For simple games, this is the case, but for more complex environments,
    it may be necessary to remember the past in order to select the best option now.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个足够大的重放缓冲区需要大量的内存，在某些情况下，重放缓冲区是不切实际的。当你的强化学习环境和代理算法遵循马尔可夫决策过程的严格标准，特别是马尔可夫属性时，重放缓冲区是可能的。回想一下，马尔可夫属性说，对于状态
    *S[t]* 的最佳动作可以在不参考任何先前状态 *S[t]*[–1] 的情况下计算；没有必要保留先前访问过的状态的历史。对于简单的游戏，这是这种情况，但对于更复杂的环境，可能需要记住过去以便现在选择最佳选项。
- en: Indeed, in many complex games it is common to use recurrent neural networks
    (RNNs) like a long short-term memory (LSTM) network or a gated recurrent unit
    (GRU). These RNNs can keep an internal state that can store traces of the past
    ([figure 5.7](#ch05fig07)). They are particularly useful for natural language
    processing (NLP) tasks where keeping track of preceding words or characters is
    critical to being able to encode or decode a sentence. Experience replay doesn’t
    work with an RNN unless the replay buffer stores entire trajectories or full episodes,
    because the RNN is designed to process sequential data.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，在许多复杂的游戏中，通常使用循环神经网络（RNNs），如长短期记忆（LSTM）网络或门控循环单元（GRU）。这些RNNs可以保持一个内部状态，可以存储过去的痕迹（[图5.7](#ch05fig07)）。它们在自然语言处理（NLP）任务中特别有用，在这些任务中，跟踪前面的单词或字符对于能够编码或解码句子至关重要。除非重放缓冲区存储整个轨迹或完整的事件，否则经验重放与RNN不兼容，因为RNN被设计来处理序列数据。
- en: Figure 5.7\. A generic recurrent neural network (RNN) layer processes a sequence
    of data by incorporating its previous output with the new input. The input on
    the left, along with a previous output is fed into an RNN module, which then produces
    an output. The output is fed back into the RNN on the next time step, and a copy
    may be fed into another layer. An RNN will not work properly with single experiences
    in an experience replay buffer since it needs to work on sequences of experiences.
  id: totrans-962
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7\. 一个通用的循环神经网络（RNN）层通过结合其先前的输出和新输入来处理一系列数据。左侧的输入以及先前的输出被送入一个RNN模块，然后产生一个输出。该输出在下一个时间步被送回RNN，并且可能被送入另一个层。由于RNN需要处理一系列经验，因此在经验重放缓冲区中使用单个经验将无法使RNN正常工作。
- en: '![](05fig07.jpg)'
  id: totrans-963
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7](05fig07.jpg)'
- en: One way to use RNNs without an experience replay is to run multiple copies of
    the agent in parallel, each with separate instantiations of the environment. By
    distributing multiple independent agents across different CPU processes ([figure
    5.8](#ch05fig08)), we can collect a varied set of experiences and therefore get
    a sample of gradients that we can average together to get a lower variance mean
    gradient. This eliminates the need for experience replay and allows us to train
    an algorithm in a completely online fashion, visiting each state only once as
    it appears in the environment.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN而不进行经验重放的一种方法是在并行运行多个代理的副本，每个代理都有独立的环境实例化。通过在不同CPU进程中分配多个独立的代理（[图5.8](#ch05fig08)），我们可以收集一系列不同的经验，因此可以得到一个可以平均的梯度样本，从而得到一个较低的方差均值梯度。这消除了对经验重放的需求，并允许我们以完全在线的方式训练算法，只访问环境中出现的状态一次。
- en: Figure 5.8\. The most common form of training a deep learning model is to feed
    a batch of data together into the model to return a batch of predictions. Then
    we compute the loss for each prediction and average or sum all the losses before
    backpropagating and updating the model parameters. This averages out the variability
    present across all the experiences. Alternatively, we can run multiple models
    with each taking a single experience and making a single prediction, backpropagate
    through each model to get the gradients, and then sum or average the gradients
    before making any parameter updates.
  id: totrans-965
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8\. 训练深度学习模型最常见的形式是将一批数据一起输入模型以返回一批预测。然后我们计算每个预测的损失，并在反向传播和更新模型参数之前平均或求和所有损失。这平均了所有经验中存在的可变性。或者，我们可以运行多个模型，每个模型只取一个经验并做出一个预测，通过每个模型进行反向传播以获取梯度，然后在做出任何参数更新之前求和或平均梯度。
- en: '![](05fig08_alt.jpg)'
  id: totrans-966
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8](05fig08_alt.jpg)'
- en: '|  |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Multiprocessing versus multithreading**'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '**多处理与多线程**'
- en: Modern desktop and laptop computers have central processing units (CPUs) with
    multiple cores, which are independent processing units capable of running computations
    simultaneously. Therefore, if you can split a computation into pieces that can
    be computed separately and combined later, you can get dramatic speed increases.
    The operating system software abstracts the physical CPU processors into virtual
    processes and threads. A process contains its own memory space, and threads run
    within a single process. There are two forms of parallel computations, *multithreading*
    and *multiprocessing*, and only in the latter form are computations performed
    truly simultaneously. In multiprocessing, computations are performed simultaneously
    on multiple, physically distinct processing units such as CPU or GPU cores.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 现代台式机和笔记本电脑具有多核心的中央处理单元（CPU），这些核心是能够同时运行计算的独立处理单元。因此，如果你可以将一个计算分解成可以单独计算并在之后组合的片段，你就可以获得显著的速度提升。操作系统软件将物理CPU处理器抽象为虚拟进程和线程。一个进程包含自己的内存空间，线程在单个进程中运行。存在两种并行计算形式，*多线程*和*多处理*，只有在后一种形式中，计算才是真正同时进行的。在多处理中，计算是在多个物理上不同的处理单元上同时进行的，例如CPU或GPU核心。
- en: '![](pg120_alt.jpg)'
  id: totrans-970
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg120_alt.jpg)'
- en: Processes are an abstraction of the underlying CPU hardware created by the operating
    system. If you have two CPUs, you can run two simultaneous processes. However,
    the operating system will let you spawn more than two virtual processes, and it
    will figure out how to multitask between them. Each process has its own memory
    address space and can have multiple threads (tasks). While one thread is waiting
    for an external process to finish (such as an input/output operation), the OS
    can let another thread run. This maximizes the use of whatever CPUs you have.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 进程是操作系统创建的对底层CPU硬件的抽象。如果你有两个CPU，你可以运行两个同时的进程。然而，操作系统会允许你生成超过两个虚拟进程，并且它会找出如何在它们之间进行多任务处理。每个进程都有自己的内存地址空间，并且可以拥有多个线程（任务）。当一个线程正在等待外部进程完成（例如输入/输出操作）时，操作系统可以让另一个线程运行。这最大化了你对任何CPU的使用。
- en: 'Multithreading is like when people multitask: they can work on only one thing
    at a time but they switch between different tasks while another task is idle.
    Therefore, tasks are not truly performed simultaneously with multithreading; it
    is a software-level mechanism to improve efficiency in running multiple computations.
    Multithreading is really effective when your task requires a lot of input/output
    operations, such as reading and writing data to the hard disk. When data is being
    read into RAM from the hard disk, computation on the CPU is idle as it waits for
    the required data, and the operating system can use that idle CPU time to work
    on a different task and then switch back when the I/O operation is done.'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程就像人们同时处理多项任务一样：他们一次只能专注于一件事情，但在其他任务空闲时会在不同任务之间切换。因此，在多线程中，任务并不是真正地同时执行；它是一种软件级别的机制，用于提高运行多个计算时的效率。当你的任务需要大量的输入/输出操作时，如读取和写入硬盘上的数据，多线程特别有效。当数据从硬盘读入RAM时，CPU的计算处于空闲状态，因为它在等待所需的数据，操作系统可以利用这段空闲的CPU时间来处理不同的任务，并在I/O操作完成后切换回来。
- en: Machine learning models generally do not require I/O operations; machine learning
    is limited by computation speed, so it benefits from true simultaneous computation
    with multiprocessing.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常不需要I/O操作；机器学习受限于计算速度，因此从多处理中真正同时计算中受益。
- en: '|  |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Large machine learning models all but require graphics processing units (GPUs)
    to perform efficiently, but distributed models on multiple CPUs can be competitive
    in some cases. Python provides a library called “multiprocessing” that makes multiprocessing
    very easy. Additionally, PyTorch wraps this library and has a method for allowing
    a model’s parameters to be shared across multiple processes. Let’s look at a simple
    example of multiprocessing.
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 大型机器学习模型几乎都需要图形处理单元（GPU）来高效运行，但在某些情况下，多个CPU上的分布式模型可以具有竞争力。Python提供了一个名为“multiprocessing”的库，使得多处理变得非常简单。此外，PyTorch封装了这个库，并提供了一种方法，允许模型的参数在多个进程之间共享。让我们看看多处理的一个简单示例。
- en: As a contrived simple example, suppose we have an array with the numbers 0,
    1, 2, 3 ... 64 and we want to square each number. Since squaring a number does
    not depend on any other numbers in the array, we can easily parallelize this across
    multiple processors.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种人为的简单示例，假设我们有一个包含数字0, 1, 2, 3 ... 64的数组，并且我们想要对每个数字进行平方。由于平方一个数字不依赖于数组中的任何其他数字，我们可以轻松地将其并行化到多个处理器上。
- en: Listing 5.1\. Introduction to multiprocessing
  id: totrans-977
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表5.1\. 多进程简介
- en: '[PRE59]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '***1*** This function takes an array and squares each element.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 此函数接受一个数组并对每个元素进行平方。'
- en: '***2*** Sets up an array with a sequence of numbers'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置一个包含数字序列的数组'
- en: '***3*** Sets up a multiprocessing processor pool with 8 processes'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置一个包含8个进程的多进程处理器池'
- en: '***4*** Uses the pool’s map function to apply the square function to each array
    in the list and returns the results in a list'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 使用池的映射函数将平方函数应用于列表中的每个数组，并将结果以列表形式返回'
- en: Here we define a function, `square`, that takes an array and squares it. This
    is the function that will get distributed across multiple processes. We create
    some sample data that is simply the list of numbers from 0 to 63, and rather than
    sequentially squaring them in a single process, we chop up the array into 8 pieces
    and compute the squares for each piece independently on a different processor
    ([figure 5.9](#ch05fig09)).
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个函数`square`，它接受一个数组并将其平方。这是将被分配到多个进程的函数。我们创建了一些示例数据，简单地是0到63的数字列表，而不是在单个进程中顺序平方它们，我们将数组分成8个部分，并在不同的处理器上独立计算每个部分的平方（[图5.9](#ch05fig09)）。
- en: Figure 5.9\. A simple multiprocessing example. We want to more efficiently square
    all the numbers in an array. Rather than squaring each element one by one, we
    can split the array into two pieces and send each piece to a different processor
    that will square them simultaneously. Then we can recombine the pieces back into
    a single array.
  id: totrans-984
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9\. 一个简单的多进程示例。我们希望更有效地对数组中的所有数字进行平方。而不是逐个平方每个元素，我们可以将数组分成两部分，并将每一部分发送到不同的处理器，这些处理器将同时平方它们。然后我们可以将这些部分重新组合成一个单一的数组。
- en: '![](05fig09_alt.jpg)'
  id: totrans-985
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig09_alt.jpg)'
- en: You can see how many hardware processors your computer has by using the `mp.cpu_count()`
    function. You can see in [listing 5.1](#ch05ex01) that we have 8\. Many modern
    computers may have 4 independent hardware processors, but they will have twice
    as many “virtual” processors via something called *hyperthreading*. Hyperthreading
    is a performance trick some processors use that can allow two processes to run
    essentially simultaneously on one physical processor. It is important not to create
    more processes than there are CPUs on your machine, as the additional processes
    will essentially function as threads, and the CPU will have to rapidly switch
    between processes.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`mp.cpu_count()`函数来查看你的计算机有多少个硬件处理器。在[代码列表5.1](#ch05ex01)中，你可以看到我们有8个。许多现代计算机可能有4个独立的硬件处理器，但它们将通过称为*超线程*的技术拥有两倍数量的“虚拟”处理器。超线程是一些处理器使用的一种性能技巧，可以使两个进程在单个物理处理器上几乎同时运行。重要的是不要创建比你机器上的CPU更多的进程，因为额外的进程将基本上作为线程运行，CPU将不得不在进程之间快速切换。
- en: In [listing 5.1](#ch05ex01) we set up a processor pool of 8 processes with `mp.Pool(8)`,
    and then we used `pool.map` to distribute the square function across the 8 pieces
    of data. You can see we get a list of 8 arrays with all their elements squared,
    just as we wanted. Processes will return as soon as they’re complete, so the order
    of the elements in the returned list may not always be in the order they were
    mapped.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 在[代码列表5.1](#ch05ex01)中，我们使用`mp.Pool(8)`设置了8个进程的处理器池，然后我们使用`pool.map`将平方函数分配到8个数据片段。你可以看到我们得到了一个包含所有元素平方的8个数组的列表，正如我们想要的。进程将在完成时立即返回，因此返回列表中元素的顺序可能不一定与映射的顺序相同。
- en: We’re going to need a bit more control over our processes than a processor pool
    allows, so we will create and start a bunch of processes manually.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要比处理器池允许的更多控制我们的进程，因此我们将手动创建和启动大量进程。
- en: Listing 5.2\. Manually starting individual processes
  id: totrans-989
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表5.2\. 手动启动单个进程
- en: '[PRE60]'
  id: totrans-990
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '***1*** Sets up a list to store a reference to each process'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置一个列表以存储每个进程的引用'
- en: '***2*** Sets up a multiprocessing queue, a data structure that can be shared
    across processes'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置一个多进程队列，这是一个可以在进程间共享的数据结构'
- en: '***3*** Sets up some sample data, a sequence of numbers'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置一些示例数据，一个数字序列'
- en: '***4*** Starts 8 processes with the square function as the target and an individual
    piece of data to process'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 以平方函数为目标，启动8个进程，每个进程处理一个单独的数据块'
- en: '***5*** Waits for each process to finish before returning to the main thread'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 在返回主线程之前等待每个进程完成'
- en: '***6*** Terminates each process'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 终止每个进程'
- en: '***7*** Converts the multiprocessing queue into a list'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将多进程队列转换为列表'
- en: This is more code, but functionally it’s the same as what we did before with
    the `Pool`. Now, though, it’s easy to share data between processes using special
    shareable data structures in the multiprocessing library, and we have more control
    over the processes.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码更多，但从功能上讲，与之前我们用`Pool`做的相同。现在，我们可以使用多进程库中的特殊可共享数据结构轻松地在进程之间共享数据，并且我们对进程有更多的控制。
- en: We modified our `square` function a little to accept an integer representing
    the process ID, the array to square, and a shared global data structure called
    a `queue` that we can put data into and extract data from using the `.get()` method.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微修改了`square`函数，使其接受一个表示进程ID的整数、一个要平方的数组以及一个名为`queue`的共享全局数据结构，我们可以使用`.get()`方法将数据放入该结构并从中提取数据。
- en: To run through the code, we first set up a list to hold the instances of our
    processes, we created the shared queue object, and we created our sample data
    as before. We then define a loop to create (in our case) 8 processes and start
    them using the `.start()` method. We add them to our processes list so we can
    access them later. Next we run through the processes list and call each process’s
    `.join()` method; this lets us wait to return anything until all the processes
    have finished. Then we call each process’s `.terminate()` method to ensure it
    is killed. Lastly, we collect all the elements of the queue into a list and print
    it out.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行代码，我们首先设置一个列表来保存进程实例，创建了共享队列对象，并像之前一样创建了样本数据。然后我们定义一个循环来创建（在我们的例子中）8个进程，并使用`.start()`方法启动它们。我们将它们添加到进程列表中，以便稍后访问。接下来，我们遍历进程列表，调用每个进程的`.join()`方法；这让我们等待所有进程完成后再返回任何内容。然后我们调用每个进程的`.terminate()`方法以确保它们被终止。最后，我们将队列的所有元素收集到一个列表中并打印出来。
- en: The results look the same as with the process pool, except they were in a random
    order. That’s really all there is to distributing a function across multiple CPU
    processors.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来与进程池相同，只是它们的顺序是随机的。这就是在多个CPU处理器上分配函数的全部内容。
- en: 5.3\. Advantage actor-critic
  id: totrans-1002
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 优势演员-评论家
- en: 'Now that we know how to distribute computation across processes, we can get
    back to the real reinforcement learning. In this section we’ll put together the
    pieces of the full distributed advantage actor-critic model. To allow fast training
    and to compare the results to the previous chapter, we will again use the CartPole
    game as our test environment. If you choose, though, you can easily adapt the
    algorithm to a more difficult game such as Pong in OpenAI Gym; you can find such
    an implementation on this chapter’s GitHub page: [http://mng.bz/JzKp](http://mng.bz/JzKp).'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何在进程之间分配计算，我们可以回到真正的强化学习。在本节中，我们将组装完整的分布式优势演员-评论家模型的各个部分。为了允许快速训练并将结果与上一章进行比较，我们再次使用CartPole游戏作为测试环境。如果你选择的话，你可以轻松地将算法适应到更困难的游戏，如OpenAI
    Gym中的Pong；你可以在本章的GitHub页面上找到这样的实现：[http://mng.bz/JzKp](http://mng.bz/JzKp)。
- en: 'So far we’ve presented the actor and critic as two separate functions, but
    we can combine them into a single neural network with two output “heads.” That’s
    what we’ll do in the following code. Instead of a normal neural network that returns
    a single vector, it can return two different vectors: one for the policy and one
    for the value. This allows for some parameter sharing between the policy and value
    that can make things more efficient, since some of the information needed to compute
    values is also useful for predicting the best action for the policy. But if a
    two-headed neural network seems too exotic right now, you can go ahead and write
    two separate neural networks—it will work just fine. Let’s look at some pseudocode
    for the algorithm. Then we’ll translate it to Python.'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将演员和评论家作为两个单独的函数来介绍，但我们可以将它们合并成一个具有两个输出“头部”的单个神经网络。这就是我们在以下代码中要做的。与返回单个向量的普通神经网络不同，它可以返回两个不同的向量：一个用于策略，一个用于价值。这允许策略和价值之间进行一些参数共享，从而提高效率，因为计算价值所需的一些信息对预测策略的最佳行动也很有用。但如果双头神经网络现在看起来太奇特，你可以继续编写两个单独的神经网络——它仍然可以正常工作。让我们看看算法的伪代码。然后我们将将其翻译成Python。
- en: Listing 5.3\. Pseudocode for online advantage actor-critic
  id: totrans-1005
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.3。在线优势演员-评论家伪代码
- en: '[PRE61]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '***1*** Iterates over epochs'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 遍历时代'
- en: '***2*** Gets the current state of the environment'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取环境的当前状态'
- en: '***3*** Predicts the value of the state'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 预测状态的价值'
- en: '***4*** Predicts the probability distribution over actions given the state'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在给定状态下预测动作的概率分布'
- en: '***5*** Samples an action from the policy’s action distribution'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 从策略的动作分布中采样一个动作'
- en: '***6*** Predicts the value of the next state'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 预测下一个状态的价值'
- en: '***7*** Calculates the advantage as the reward plus the difference between
    the next state value and the current state value'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 计算优势为奖励加上下一个状态价值与当前状态价值之间的差异'
- en: '***8*** Reinforces the action that was just taken based on the advantage'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 根据优势强化刚刚采取的动作'
- en: This is very simplified pseudocode, but it gets the main idea across. The important
    part to point out is the advantage calculation. Consider the case where we take
    an action, we receive reward +10, the value prediction is +5, and the value prediction
    for the next state is +7\. Since future predictions are always less valuable than
    the currently observed reward, we discount the value of the next state by the
    gamma discount factor. Our `advantage = 10 + 0.9*7 – 5 = 10 + (6.3 – 5) = 10 +
    1.3 = +11.3`. Since the difference between the next state value and the current
    state value is positive, it increases the overall value of the action we just
    took, so we will reinforce it more. Notice that the advantage function *bootstraps*
    because it computes a value for the current state and action based on predictions
    for a future state.
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常简化的伪代码，但它传达了主要思想。需要指出的重要部分是优势计算。考虑这种情况，我们采取一个动作，获得奖励+10，价值预测为+5，下一个状态的价值预测为+7。由于未来的预测总是比当前观察到的奖励价值低，我们通过伽马折扣因子对下一个状态的价值进行折扣。我们的`advantage
    = 10 + 0.9*7 – 5 = 10 + (6.3 – 5) = 10 + 1.3 = +11.3`。由于下一个状态的价值和当前状态的价值之间的差异是正的，它增加了我们刚刚采取的动作的整体价值，因此我们将更多地强化它。注意，优势函数*bootstraps*，因为它基于对未来状态的预测来计算当前状态和动作的价值。
- en: In this chapter we’re going to use our DA2C model on CartPole again, which is
    episodic, so if we do a full Monte Carlo update where we update after the full
    episode is complete, `value_next` will always be 0 for the last move since there
    is no next state when the episode is over. In this case, the advantage term actually
    reduces to `advantage = reward – value`, which is the value baseline we discussed
    at the beginning of the chapter. The full advantage expression, *A* = *r[t]*[+1]
    + *γ***v*(*s[t]*[+1]) – *v*(*s[t]*), is used when we do online or *N-step learning*.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将再次使用我们的DA2C模型在CartPole上，它属于周期性任务，因此如果我们进行完整的蒙特卡洛更新，在完整周期结束后更新，`value_next`对于最后一步将始终为0，因为没有下一个状态。在这种情况下，优势项实际上简化为`advantage
    = reward – value`，这是我们在本章开头讨论的价值基线。当我们进行在线或*N*步学习时，使用完整的优势表达式*A* = *r[t]*[+1]
    + *γ***v*(*s[t]*[+1]) – *v*(*s[t]*)。
- en: '![](pg125.jpg)'
  id: totrans-1017
  prefs: []
  type: TYPE_IMG
  zh: '![](pg125.jpg)'
- en: '*N*-step learning is what’s in between fully online learning and waiting for
    a full episode before updating (i.e., Monte Carlo). As the name suggests, we accumulate
    rewards over *N* steps and then compute our loss and backpropagate. The number
    of steps can be anywhere from 1, which reduces to fully online learning, to the
    maximum number of steps in the episode, which is Monte Carlo. Usually we pick
    something in between to get the advantages of both. We will first show the episodic
    actor-critic algorithm, and then we will adapt it to *N*-step with *N* set to
    10.'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*步学习介于完全在线学习和在更新前等待完整周期之间（即蒙特卡洛）。正如其名所示，我们在*N*步内累积奖励，然后计算我们的损失并进行反向传播。步数可以从1开始，这相当于完全在线学习，到周期中的最大步数，即蒙特卡洛。通常我们选择两者之间的某个值以获得两者的优势。我们首先展示周期性演员-评论家算法，然后我们将它调整为*N*步，其中*N*设置为10。'
- en: '[Figure 5.10](#ch05fig10) shows the broad overview of an actor-critic algorithm.
    An actor-critic model needs to produce both a state value and action probabilities.
    We use the action probabilities to select an action and receive a reward, which
    we compare with the state value to compute an advantage. The advantage is ultimately
    what we use to reinforce the action and train the model.'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.10](#ch05fig10)展示了演员-评论家算法的总体概述。演员-评论家模型需要生成状态价值和动作概率。我们使用动作概率来选择一个动作并接收一个奖励，我们将它与状态价值进行比较以计算优势。优势是我们最终用来强化动作和训练模型的。'
- en: Figure 5.10\. An actor-critic model produces a state value and action probabilities,
    which are used to compute an advantage value and this is the quantity that is
    used to train the model rather than raw rewards as with just *Q*-learning.
  id: totrans-1020
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10\. 一个演员-评论家模型产生状态值和动作概率，这些用于计算优势值，这是训练模型所使用的量，而不是像仅使用*Q*-学习时的原始奖励。
- en: '![](05fig10_alt.jpg)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig10_alt.jpg)'
- en: With that in mind, let’s get to coding an actor-critic model to play CartPole.
    Here’s the sequence of steps.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们编写一个演员-评论家模型来玩CartPole。以下是步骤序列。
- en: Set up our actor-critic model, a two-headed model (or you can set up two independent
    actor and critic networks). The model accepts a CartPole state as input, which
    is a vector of 4 real numbers. The actor head is just like the policy network
    (actor) from the previous chapter, so it outputs a 2-dimensional vector representing
    a discrete probability distribution over the 2 possible actions. The critic outputs
    a single number representing the state value. The critic is denoted *v*(*s*) and
    the actor is denoted *π*(*s*). Remember that *π*(*s*) returns the log probabilities
    for each possible action, which in our case is 2 actions.
  id: totrans-1023
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置我们的演员-评论家模型，一个双头模型（或者你也可以设置两个独立的演员和评论家网络）。该模型接受CartPole状态作为输入，这是一个由4个实数组成的向量。演员头就像上一章中的策略网络（演员），因此它输出一个二维向量，表示对两个可能动作的离散概率分布。评论家输出一个表示状态值的单个数字。评论家表示为*v*(*s*)，演员表示为*π*(*s*)。记住*π*(*s*)返回每个可能动作的对数概率，在我们的情况下是2个动作。
- en: While we’re in the current episode
  id: totrans-1024
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们在当前回合中时
- en: 'Define the hyperparameter: *γ* (gamma, discount factor).'
  id: totrans-1025
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数：*γ*（gamma，折扣因子）。
- en: Start a new episode, in initial state *s[t]*.
  id: totrans-1026
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始状态*s[t]*开始一个新的回合。
- en: Compute the value *v*(*s[t]*) and store it in the list.
  id: totrans-1027
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值*v*(*s[t]*)并将其存储在列表中。
- en: Compute *π*(*s[t]*), store it in the list, sample, and take action *a[t]*. Receive
    the new state *s[t]*[+1] and the reward *r[t]*[+1]. Store the reward in the list.
  id: totrans-1028
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*π*(*s[t]*)，将其存储在列表中，进行采样，并采取动作*a[t]*。接收新的状态*s[t]*[+1]和奖励*r[t]*[+1]。将奖励存储在列表中。
- en: Train
  id: totrans-1029
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练
- en: 'Initialize *R* = 0\. Loop through the rewards in reverse order to generate
    returns: *R* = *r[i]* + *γ***R*.'
  id: totrans-1030
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化*R* = 0。以相反的顺序遍历奖励以生成回报：*R* = *r[i]* + *γ***R*。
- en: 'Minimize the actor loss: –1 * *γ**[t]* * (*R – v*(*s[t]*)) * *π*(*a*|*s*).'
  id: totrans-1031
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化演员损失：-1 * *γ**[t]* * (*R – v*(*s[t]*)) * *π*(*a*|*s*).
- en: 'Minimize the critic loss: (*R* – *v*)².'
  id: totrans-1032
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化评论家损失：(*R* – *v*)²。
- en: Repeat for a new episode.
  id: totrans-1033
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为新的回合重复。
- en: The following listing implements these steps in Python.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表使用Python实现了这些步骤。
- en: Listing 5.4\. CartPole actor-critic model
  id: totrans-1035
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4\. CartPole演员-评论家模型
- en: '[PRE62]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '***1*** PyTorch wraps Python’s built-in multiprocessing library, and the API
    is the same.'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** PyTorch包装Python的内置多进程库，API是相同的。'
- en: '***2*** Defines a single combined model for the actor and critic'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 定义了一个演员和评论家的单一组合模型'
- en: '***3*** The actor head returns the log probabilities over the 2 actions.'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 演员头返回2个动作的对数概率。'
- en: '***4*** The critic returns a single number bounded by –1 and +1.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 评论家返回一个介于-1和+1之间的单个数字。'
- en: '***5*** Returns the actor and critic results as a tuple'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 返回演员和评论家结果作为元组'
- en: For CartPole, we have a fairly simple neural network, apart from having two
    output heads. In [listing 5.4](#ch05ex04) we first normalize the input so that
    the state values are all within the same range; then the normalized input is fed
    through the first two layers, which are ordinary linear layers with the ReLU activation
    functions. Then we fork the model into two paths.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CartPole，我们有一个相当简单的神经网络，除了有两个输出头之外。在[列表5.4](#ch05ex04)中，我们首先规范化输入，使状态值都在相同的范围内；然后规范化后的输入通过前两层，这两层是具有ReLU激活函数的普通线性层。然后我们将模型分成两条路径。
- en: The first path is the actor head that takes the output of layer 2 and applies
    another linear layer and then the `log_softmax` function. The `log_softmax` is
    logically equivalent to doing `log(softmax(...)))`, but the combined function
    is more numerically stable because if you compute the functions separately you
    might end up with overflowed or underflowed probabilities after the `softmax`.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条路径是演员头，它接受第2层的输出并应用另一个线性层，然后是`log_softmax`函数。`log_softmax`在逻辑上等同于执行`log(softmax(...)))`，但组合函数在数值上更稳定，因为如果你单独计算函数，你可能会在`softmax`之后得到溢出或下溢的概率。
- en: The second path is the critic head, which applies a linear layer and ReLU to
    the output of layer 2, but notice that we call `y.detach()`, which detaches the
    `y` node from the graph so the critic’s loss won’t backpropagate and modify the
    weights in layers 1 and 2 ([figure 5.11](#ch05fig11)). Only the actor will cause
    these weights to be modified. This prevents conflict between what the actor and
    critic want when the actor and critic are trying to make opposing updates to the
    earlier layers. With two-headed models, it often makes sense to make one head
    dominant and allow it to control most of the parameters by detaching the other
    head from the first several layers. Lastly, the critic applies another linear
    layer with the tanh activation function that bounds the output to the interval
    (–1,1), which is perfect for CartPole since the rewards are +1 and –1.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 第二条路径是评论家头，它对第 2 层的输出应用线性层和 ReLU，但请注意，我们调用了 `y.detach()`，这将 `y` 节点从图中断开，这样评论家的损失就不会反向传播并修改第
    1 和第 2 层的权重（[图 5.11](#ch05fig11)）。只有演员会导致这些权重被修改。这防止了当演员和评论家试图对早期层进行相反的更新时，演员和评论家之间的冲突。在双头模型中，通常有道理使一个头占主导地位，并允许它通过从第一几个层断开另一个头来控制大多数参数。最后，评论家应用另一个带有
    tanh 激活函数的线性层，该函数将输出限制在区间（–1,1）内，这对于 CartPole 来说是完美的，因为奖励是 +1 和 –1。
- en: Figure 5.11\. This is an overview of the architecture for our two-headed actor-critic
    model. It has two shared linear layers and a branching point where the output
    of the first two layers is sent to a log-softmax layer of the actor head and also
    to a ReLU layer of the critic head before finally passing through a tanh layer,
    which is an activation function that restricts output between –1 and 1\. This
    model returns a 2-tuple of tensors rather than a single tensor. Notice that the
    critic head is detached (indicated by the dotted line), which means we do not
    backpropagate from the critic head into the actor head or the beginning of the
    model. Only the actor backpropagates through the beginning of the model.
  id: totrans-1045
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11\. 这是我们双头演员-评论家模型架构的概述。它有两个共享的线性层和一个分支点，其中前两个层的输出被发送到演员头的 log-softmax
    层，同时也发送到评论家头的 ReLU 层，最后通过一个 tanh 层，这是一个限制输出在 –1 和 1 之间的激活函数。此模型返回一个包含两个张量的 2-元组，而不是单个张量。注意，评论家头是断开的（由虚线表示），这意味着我们不会从评论家头反向传播到演员头或模型的开始部分。只有演员会通过模型的开始部分进行反向传播。
- en: '![](05fig11_alt.jpg)'
  id: totrans-1046
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig11_alt.jpg)'
- en: In the following listing we develop the code necessary for distributing multiple
    instances of the actor-critic model across different processes.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们开发了将多个演员-评论家模型的实例分布到不同进程所需的代码。
- en: Listing 5.5\. Distributing the training
  id: totrans-1048
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5\. 分布式训练
- en: '[PRE63]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '***1*** Creates a global, shared actor-critic model'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建一个全局、共享的演员-评论家模型'
- en: '***2*** The shared_memory() method will allow the parameters of the model to
    be shared across processes rather than being copied.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** `shared_memory()` 方法将允许模型的参数在进程间共享，而不是复制。'
- en: '***3*** Sets up a list to store the instantiated processes'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置一个列表来存储实例化的进程'
- en: '***4*** A shared global counter using multiprocessing’s built-in shared object.
    The ‘i’ parameter indicates the type is integer.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 使用 multiprocessing 内置的共享对象共享全局计数器。参数 `i` 表示类型为整数。'
- en: '***5*** Starts a new process that runs the worker function'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 启动一个新的进程来运行工作函数'
- en: '***6*** “Joins” each process to wait for it to finish before returning to the
    main process'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** “连接”每个进程，等待其完成后再返回主进程'
- en: '***7*** Makes sure each process is terminated'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 确保每个进程都已终止'
- en: '***8*** Prints the global counter value and the first process’s exit code (which
    should be 0)'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 打印全局计数器的值和第一个进程的退出代码（应该是 0）'
- en: This is exactly the same setup we had when we demonstrated how to split up an
    array across multiple processes, except this time we’re going to be running a
    function called `worker` that will run our CartPole reinforcement learning algorithm.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在演示如何将数组拆分到多个进程时所使用的相同设置，但这次我们将运行一个名为 `worker` 的函数，该函数将运行我们的 CartPole 强化学习算法。
- en: Next we’ll define the worker function, which will run a single agent in an instance
    of the CartPole environment.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义工作函数，该函数将在 CartPole 环境的一个实例中运行单个智能体。
- en: Listing 5.6\. The main training loop
  id: totrans-1060
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.6\. 主要训练循环
- en: '[PRE64]'
  id: totrans-1061
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '***1*** Each process runs its own isolated environment and optimizer but shares
    the model.'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 每个进程运行自己的隔离环境和优化器，但共享模型。'
- en: '***2*** The run_episode function plays an episode of the game, collecting data
    along the way.'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** `run_episode` 函数播放游戏的一个场景，并在过程中收集数据。'
- en: '***3*** We use the collected data from run_episode to run one parameter update
    step.'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 我们使用 `run_episode` 收集的数据运行一个参数更新步骤。'
- en: '***4*** counter is a globally shared counter between all the running processes.'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 计数器是所有运行进程之间全局共享的计数器。'
- en: The `worker` function is the function that each individual process will run
    separately. Each worker (i.e., process) will create its own CartPole environment
    and its own optimizer but will share the actor-critic model, which is passed in
    as an argument to the function. Since the model is shared, whenever a worker updates
    the model parameters, they are updated for all the workers. This is shown at a
    high-level in [figure 5.12](#ch05fig12).
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '`worker` 函数是每个个体进程将单独运行的函数。每个工作进程（即进程）将创建自己的 CartPole 环境，以及自己的优化器，但会共享演员-评论家模型，该模型作为函数的参数传入。由于模型是共享的，因此每当工作进程更新模型参数时，所有工作进程的参数都会更新。这在[图
    5.12](#ch05fig12) 中以高级形式展示。'
- en: Figure 5.12\. Within each process, an episode of the game is run using the shared
    model. The loss is computed within each process, but the optimizer acts to update
    the shared actor-critic model that is used by each process.
  id: totrans-1067
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.12\. 在每个进程中，使用共享模型运行游戏的场景。损失在每个进程中计算，但优化器作用于更新每个进程使用的共享演员-评论家模型。
- en: '![](05fig12_alt.jpg)'
  id: totrans-1068
  prefs: []
  type: TYPE_IMG
  zh: '![05fig12_alt](05fig12_alt.jpg)'
- en: Since each worker is spawned in a new process that has its own memory, all the
    data the worker needs should be passed in as an argument to the function explicitly.
    This also prevents bugs.
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: '由于每个工作进程是在具有自己内存的新进程中生成的，因此工作进程需要的所有数据都应该明确作为函数的参数传入。这也防止了错误。 '
- en: In [listing 5.7](#ch05ex07) we define a function to run a single instance of
    the actor-critic model through one episode in the CartPole environment.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.7](#ch05ex07) 中，我们定义了一个函数，用于在 CartPole 环境中运行演员-评论家模型的单个实例，通过一个场景。
- en: Listing 5.7\. Running an episode
  id: totrans-1071
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7\. 运行一个场景
- en: '[PRE65]'
  id: totrans-1072
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '***1*** Converts the environment state from a numpy array to a PyTorch tensor'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将环境状态从 numpy 数组转换为 PyTorch 张量'
- en: '***2*** Creates lists to store the computed state values (critic), log probabilities
    (actor), and rewards'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建列表来存储计算的状态值（评论家）、动作的日志概率（演员）和奖励'
- en: '***3*** Keeps playing the game until the episode ends'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 继续玩游戏，直到场景结束'
- en: '***4*** Computes the state value and log probabilities over actions'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 计算状态值和动作的日志概率'
- en: '***5*** Using the actor’s log probabilities over actions, creates and samples
    from a categorical distribution to get an action'
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用演员的动作日志概率，创建并从分类分布中进行采样以获取一个动作'
- en: '***6*** If the last action caused the episode to end, sets the reward to –10
    and resets the environment'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 如果最后一个动作导致场景结束，则将奖励设置为 -10 并重置环境'
- en: The `run_episode` function just runs through a single episode of CartPole and
    collects the computed state values from the critic, log probabilities over actions
    from the actor, and rewards from the environment. We store these in lists and
    use them to compute our loss function later. Since this is an actor-critic method
    and not Q-learning, we take actions by directly sampling from the policy rather
    than arbitrarily choosing a policy like epsilon-greedy in Q-learning. There’s
    nothing too out of the ordinary in this function, so let’s move on to the updating
    function.
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_episode` 函数只是运行 CartPole 的单个场景，并收集评论家计算的状态值、演员的动作日志概率和环境奖励。我们将这些存储在列表中，并使用它们来计算我们的损失函数。由于这是一个演员-评论家方法，而不是
    Q-learning，我们通过直接从策略中采样来采取行动，而不是像 Q-learning 中的 epsilon-greedy 那样任意选择策略。这个函数中没有太多不寻常的地方，所以让我们继续更新函数。'
- en: Listing 5.8\. Computing and minimizing the loss
  id: totrans-1080
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8\. 计算和最小化损失
- en: '[PRE66]'
  id: totrans-1081
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '***1*** We reverse the order of the rewards, logprobs, and values_ arrays and
    call .view(-1) to make sure they’re flat.'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们反转奖励、logprobs 和 values_ 数组的顺序，并调用 .view(-1) 以确保它们是平的。'
- en: '***2*** For each reward (in reverse order), we compute the return value and
    append it to a returns array.'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 对于每个奖励（按反向顺序），我们计算回报值并将其追加到回报数组中。'
- en: '***3*** We need to detach the values tensor from the graph to prevent backpropagating
    through the critic head.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 我们需要从图中分离值张量，以防止通过评论家头部反向传播。'
- en: '***4*** The critic attempts to learn to predict the return.'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 评论家试图学习预测回报。'
- en: '***5*** We sum the actor and critic losses to get an overall loss. We scale
    down the critic loss by the clc factor.'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 我们将演员和评论家损失相加以获得总体损失。我们通过 clc 因子缩小评论家损失。'
- en: The `update_params` function is where all the action is, and it’s what sets
    distributed advantage actor-critic apart from the other algorithms we’ve learned
    so far. First we take the lists of rewards, log probabilities, and state values
    and convert them to PyTorch tensors. We then reverse their order because we want
    to consider the most recent action first, and we make sure they are flattened
    1D arrays by calling the `.view(-1)` method.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_params`函数是所有动作发生的地方，这也是它将分布式优势演员-评论家算法与其他我们之前学过的算法区分开来的地方。首先，我们将奖励、对数概率和状态值的列表转换为PyTorch张量。然后，我们反转它们的顺序，因为我们想首先考虑最近的行为，并确保通过调用`.view(-1)`方法将它们转换为展平的1D数组。'
- en: The `actor_loss` is computed as we described earlier in this section with math,
    using the advantage (technically the baseline, since there’s no bootstrapping)
    rather than the raw reward. Crucially, we must detach the values tensor from the
    graph when we use the `actor_loss`, or we will backpropagate through the actor
    and critic heads, and we only want to update the actor head. The critic loss is
    a simple squared error between the state values and the returns, and we make sure
    *not* to detach here since we want to update the critic head. Then we sum the
    actor and critic losses to get the overall loss. We scale down the critic loss
    by multiplying by 0.1 because we want the actor to learn faster than the critic.
    We return the individual losses and the length of the rewards tensor (which indicates
    how long the episode lasted) to monitor their progress during training.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: '`actor_loss`的计算方式如本节前面所述，使用优势（技术上讲是基线，因为没有回溯）而不是原始奖励。关键的是，当我们使用`actor_loss`时，我们必须从图中断开值张量，否则我们会通过演员和评论家头部进行反向传播，而我们只想更新演员头部。评论家损失是状态值和回报之间的简单平方误差，我们确保在这里不进行断开，因为我们想更新评论家头部。然后我们将演员和评论家损失相加得到整体损失。我们将评论家损失乘以0.1以缩小其规模，因为我们想让演员比评论家学得更快。我们返回个别损失和奖励张量的长度（这表示剧集的持续时间），以监控它们在训练过程中的进展。'
- en: The way we’ve set it up here, each worker will update the shared model parameters
    *asynchronously*, whenever it is done running an episode. We could have designed
    it such that we wait for all workers to finish running one episode and then sum
    their gradients together and update the shared parameters synchronously, but this
    is more complicated, and the asynchronous approach works well in practice.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们设置的方式中，每个工作进程将在完成一个剧集的运行后异步地更新共享模型参数。我们本来可以设计成等待所有工作进程完成一个剧集的运行，然后将它们的梯度相加并同步更新共享参数，但这会更复杂，而且异步方法在实践中效果很好。
- en: Put it all together and run it, and you’ll get a trained CartPole agent within
    one minute on a modern computer running on just a few CPU cores. If you plot the
    loss over time for this, it probably won’t be a nice down-trending line like you’d
    hope because the actor and critic are in competition with one another ([figure
    5.13](#ch05fig13)). The critic is incentivized to model the returns as best as
    it can (and the returns depend on what the actor does), but the actor is incentivized
    to beat the expectations of the critic. If the actor improves faster than the
    critic, the critic’s loss will be high, and vice versa, so there is a somewhat
    adversarial relationship between the two.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起运行，你将在现代计算机上仅用几个CPU核心在一分钟内得到一个训练好的CartPole代理。如果你为这个过程绘制损失随时间的变化图，它可能不会像你希望的那样是一个漂亮的下降趋势线，因为演员和评论家是相互竞争的（[图5.13](#ch05fig13)）。评论家被激励去尽可能好地模拟回报（回报取决于演员的行为），但演员被激励去超越评论家的期望。如果演员的改进速度快于评论家，评论家的损失会很高，反之亦然，因此两者之间存在着某种对抗性关系。
- en: Figure 5.13\. The actor and critic have a bit of an adversarial relationship
    since the actions that the agent take affect the loss of the critic, and the critic
    makes predictions of state values that get incorporated into the return that affects
    the training loss of the actor. Hence, the overall loss plot may look chaotic
    despite the fact that the agent is indeed increasing in performance.
  id: totrans-1091
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13。演员和评论家之间有一定的对抗性关系，因为代理采取的行动会影响评论家的损失，而评论家对状态值的预测会被纳入回报，从而影响演员的训练损失。因此，尽管代理的确在提高性能，但整体损失图可能看起来很混乱。
- en: '![](05fig13_alt.jpg)'
  id: totrans-1092
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig13_alt.jpg)'
- en: Adversarial training like this is a very powerful technique in many areas of
    machine learning, not just reinforcement learning. For example, generative adversarial
    networks (GANs) are an unsupervised method for generating realistic-appearing
    synthetic samples of data from a training data set using a pair of models that
    function similarly to an actor and critic. In fact, we will build an even more
    sophisticated adversarial model in [chapter 8](kindle_split_018.html#ch08).
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对抗性训练在机器学习的许多领域都是一个非常强大的技术，而不仅仅是强化学习。例如，生成对抗网络（GANs）是一对类似演员和评论家的模型的无监督方法，用于从训练数据集中生成看起来真实的合成数据样本。实际上，我们将在第8章构建一个更复杂的对抗性模型。[第8章](kindle_split_018.html#ch08)。
- en: The take-home here is that if you’re using an adversarial model, the loss will
    be largely uninformative (unless it goes to 0 or explodes toward infinity, in
    which case something is probably wrong). You have to rely on actually evaluating
    the objective you care about, which in our case is how well the agent is performing
    in the game. [Figure 5.14](#ch05fig14) shows the plot of average episode length
    during the first 120 epochs (about 45 seconds of) training.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要记住的是，如果你使用的是对抗性模型，损失将大部分是无信息的（除非它降到0或爆炸到无穷大，在这种情况下可能出了问题）。你必须依赖于实际评估你关心的目标，在我们的案例中，是代理在游戏中的表现如何。[图
    5.14](#ch05fig14) 展示了前120个epoch（大约45秒）训练期间平均回合长度的曲线图。
- en: Figure 5.14\. The mean episode length over training time for our Monte Carlo
    distributed advantage actor-critic model. This model is not considered a true
    critic, since the critic is not bootstrapping during training. As a result, the
    training performance has high variance.
  id: totrans-1095
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.14\. 训练时间内的平均回合长度，展示的是我们的蒙特卡洛分布式优势演员-评论家模型。这个模型不被视为真正的评论家，因为评论家在训练期间没有进行自举。因此，训练性能的方差较高。
- en: '![](05fig14_alt.jpg)'
  id: totrans-1096
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 的替代文本](05fig14_alt.jpg)'
- en: 5.4\. N-step actor-critic
  id: totrans-1097
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. N步演员-评论家
- en: In the last section, we implemented distributed advantage actor-critic, except
    that we trained in Monte Carlo mode—we ran a full episode before updating the
    model parameters. While that makes sense for a simple game like CartPole, usually
    we want to be able to make more frequent updates. We briefly touched on *N*-step
    learning before, but to reiterate, it means we simply calculate our loss and update
    the parameters after *N* steps, where *N* is whatever we choose it to be. If *N*
    is 1, this is fully online learning; if *N* is very large, it will be Monte Carlo
    again. The sweet spot is somewhere in between.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们实现了分布式优势演员-评论家，但我们在蒙特卡洛模式下进行训练——在更新模型参数之前运行一个完整的回合。虽然这对像CartPole这样的简单游戏来说是有意义的，但通常我们希望能够更频繁地更新。我们之前简要提到了*N*步学习，但为了重申，这意味着我们在*N*步之后简单地计算损失并更新参数，其中*N*是我们选择的任何值。如果*N*是1，这是完全在线学习；如果*N*非常大，它将再次是蒙特卡洛。最佳点位于两者之间。
- en: With Monte Carlo full-episode learning, we don’t take advantage of bootstrapping,
    since there’s nothing to bootstrap. We do bootstrap in online learning, as we
    did with DQN, but with 1-step learning the bootstrap may introduce a lot of bias.
    This bias may be harmless if it pushes our parameters in the right direction,
    but in some cases the bias can be so off that we never move in the right direction.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 在蒙特卡洛全回合学习中，我们没有利用自举，因为没有东西可以自举。我们在在线学习中进行了自举，就像我们在DQN中做的那样，但使用1步学习，自举可能会引入很多偏差。这种偏差如果推动我们的参数向正确方向移动可能无害，但在某些情况下，偏差可能偏离得如此之远，以至于我们永远不会向正确方向移动。
- en: This is why *N*-step learning is usually better than 1-step (online) learning—the
    target value for the critic is more accurate, so the critic’s training will be
    more stable and will be able to produce less biased state values. With bootstrapping,
    we’re making a prediction from a prediction, so the predictions will be better
    if you’re able to collect more data before making them. And we like bootstrapping
    because it improves sample efficiency; you don’t need to see as much data (e.g.,
    frames in a game) before updating the parameters in the right direction.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么*N*步学习通常比1步（在线）学习要好——评论家的目标值更准确，因此评论家的训练将更稳定，并且能够产生更少偏差的状态值。通过自举，我们从一个预测中进行预测，所以如果你在做出预测之前能够收集到更多数据，预测将更好。我们喜欢自举，因为它提高了样本效率；在参数正确方向更新之前，你不需要看到那么多数据（例如，游戏中的帧）。
- en: Let’s modify our code to do *N*-step learning. The only function we need to
    modify is `run_episode`. We need to change it to run for only *N* steps rather
    than wait for the episode to finish. If the episode finishes before *N* steps,
    the last return value will be set to 0 (since there is no next state when the
    game is over) as it was in the Monte Carlo case. However, if the episode hasn’t
    finished by *N* steps, we’ll use the last state value as our prediction for what
    the return would have been had we kept playing—that’s where the bootstrapping
    happens. Without bootstrapping, the critic is just trying to predict the future
    returns from a state, and it gets the actual returns as training data. With bootstrapping,
    it is still trying to predict future returns, but it is doing so in part by using
    its own prediction about future returns (since the training data will include
    its own prediction).
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改我们的代码以进行*N*-步学习。我们需要修改的唯一函数是`run_episode`。我们需要将其修改为只运行*N*步，而不是等待情节结束。如果情节在*N*步之前结束，最后一个返回值将被设置为0（因为游戏结束时没有下一个状态），就像在蒙特卡洛案例中一样。然而，如果在*N*步之前情节尚未结束，我们将使用最后一个状态值作为我们预测继续玩游戏将得到的返回值的预测——这就是自举发生的地方。没有自举，评论家只是在尝试从一个状态预测未来的回报，并且它将实际回报作为训练数据。有了自举，它仍然在尝试预测未来的回报，但它部分是通过使用自己对未来回报的预测来做到的（因为训练数据将包括自己的预测）。
- en: Listing 5.9\. *N*-step training with CartPole
  id: totrans-1102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.9\. 使用CartPole的*N*-步训练
- en: '[PRE67]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '***1*** The variable G refers to the return. We initialize to 0.'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 变量G指的是回报。我们将其初始化为0。'
- en: '***2*** Plays game until N steps or when episode is over'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 游戏直到*N*步或情节结束'
- en: '***3*** If episode is not done, sets return to the last state value'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 如果情节未结束，将回报设置为最后一个状态值'
- en: The only things we’ve changed are the conditions for the `while` loop (exit
    by *N* steps), and we’ve set the return to be the state value of the last step
    if the episode is not over, thereby enabling bootstrapping. This new `run_episode`
    function explicitly returns `G`, the return, so to get this to work we need to
    make a couple minor updates to the `update_params` function and the `worker` function.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一改变的是`while`循环的条件（通过*N*步退出），并且我们将返回值设置为最后一步的状态值，如果情节未结束，从而实现自举。这个新的`run_episode`函数明确返回`G`，即返回值，因此为了让它工作，我们需要对`update_params`函数和`worker`函数进行一些小的更新。
- en: 'First, add the `G` parameter to the definition of the `update_params` function,
    and change `ret_ = G`:'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将`G`参数添加到`update_params`函数的定义中，并更改`ret_ = G`：
- en: '[PRE68]'
  id: totrans-1109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The rest of the function is exactly the same and is omitted here.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的其余部分与之前完全相同，此处省略。
- en: 'All we need to change in the `worker` function is to capture the newly returned
    `G` array and pass it to `update_params`:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 在`worker`函数中，我们只需要捕获新返回的`G`数组并将其传递给`update_params`：
- en: '[PRE69]'
  id: totrans-1112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: You can run the training algorithm again as before, and everything should work
    the same except with better performance. You might be surprised at how much more
    efficient *N*-step learning is. [Figure 5.15](#ch05fig15) shows the plot of episode
    length over the first 45 seconds of training for this model.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像以前一样再次运行训练算法，除了性能更好之外，一切应该都一样。你可能会对*N*-步学习有多高效感到惊讶。[图5.15](#ch05fig15)显示了该模型训练前45秒的情节长度的图表。
- en: Figure 5.15\. Performance plot for distributed advantage actor-critic with true
    *N*-step bootstrapping. Compared to our previous Monte Carlo algorithm, the performance
    is much smoother due to the more stable critic.
  id: totrans-1114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15\. 具有真实*N*-步自举的分布式优势演员-评论家性能图。与我们的先前的蒙特卡洛算法相比，由于评论家更稳定，性能更加平滑。
- en: '![](05fig15_alt.jpg)'
  id: totrans-1115
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15](05fig15_alt.jpg)'
- en: Notice in [figure 5.15](#ch05fig15) that the *N*-step model starts getting better
    right away and reaches an episode length of 300 (after just 45 seconds), compared
    to only about 140 for the Monte Carlo version. Also notice that this plot is much
    smoother than the Monte Carlo one. Bootstrapping reduces the variance in the critic
    and allows it to learn much more rapidly than Monte Carlo.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在[图5.15](#ch05fig15)中，*N*-步模型立即开始变得更好，并在45秒后达到300个情节长度（相比之下，蒙特卡洛版本只有大约140个），也请注意这个图表比蒙特卡洛图表要平滑得多。自举减少了评论家的方差，并允许它比蒙特卡洛更快地学习。
- en: As a concrete example, imagine the case where you get 3-step rewards of [1,1,–1]
    for episode 1 and then [1,1,1] for episode 2\. The overall return for episode
    1 is 0.01 (with γ = 0.99) and 1.99 for episode 2; that’s two orders of magnitude
    difference in return just based on the random outcome of the episode early in
    training. That’s a lot of variance. Compare that to the same case except with
    (simulated) bootstrapping, so that the return for each of those episodes also
    includes the bootstrapped predicted return. With a bootstrapped return prediction
    of 1.0 for both (assuming the states are similar), the calculated returns are
    0.99 and 2.97, which are much closer than without bootstrapping. You can reproduce
    this example with the following code.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体例子，想象一下这种情况：在第一幕中，你获得了[1,1,-1]的3步奖励，然后在第二幕中获得了[1,1,1]的奖励。第一幕的整体回报是0.01（γ
    = 0.99），第二幕是1.99；仅基于训练早期幕的随机结果，回报就有两个数量级的差异。这有很多方差。将此与（模拟的）自助法相同的案例进行比较，这样每个幕的回报也包括自助预测回报。假设两个状态相似，自助回报预测为1.0，计算出的回报是0.99和2.97，比没有自助法时要接近得多。你可以使用以下代码重现这个例子。
- en: Listing 5.10\. Returns with and without bootstrapping
  id: totrans-1118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10. 有无自助法的回报
- en: '[PRE70]'
  id: totrans-1119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: To recap, in the plain policy gradient method of the previous chapter, we only
    trained a policy function that would output a probability distribution over all
    the actions, such that the predicted best action would be assigned the highest
    probability. Unlike Q-learning where a target value is learned, the policy function
    is directly reinforced to increase or decrease the probability of the action taken
    depending on the reward. Often the same action may produce opposite results in
    terms of reward, causing high variance in the training.
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在前一章的简单策略梯度方法中，我们只训练了一个策略函数，该函数会在所有动作上输出一个概率分布，使得预测的最佳动作被分配最高的概率。与学习目标值的Q-learning不同，策略函数直接根据奖励来增强或减少采取动作的概率。通常，同一个动作可能会在奖励方面产生相反的结果，导致训练中的高方差。
- en: To mitigate this, we introduced a critic model (or in this chapter we used a
    single, two-headed model) that reduces the variance of the policy function updates
    by directly modeling the state value. This way, if the actor (policy) takes an
    action and gets an unusually big or small reward, the critic can moderate this
    big swing and prevent an unusually large (and possibly destructive) parameter
    update to the policy. This also leads to the notion of advantage, where instead
    of training the policy based on raw return (average accumulated rewards), we train
    based on how much better (or worse) the action was compared to what the critic
    predicted it would be. This is helpful, because if two actions both lead to the
    same positive reward, we will naively assume their equivalent actions, but if
    we compare to what we expected would happen, and one reward performed much better
    than anticipated, that action should be reinforced more.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这种情况，我们引入了一个评论家模型（或在本章中我们使用了一个单一的双头模型），通过直接建模状态值来减少策略函数更新的方差。这样，如果演员（策略）采取一个动作并获得一个异常大或小的奖励，评论家可以调节这个大幅波动，防止策略参数更新异常大（可能是破坏性的）。这也导致了优势的概念，而不是基于原始回报（平均累积奖励）来训练策略，而是基于动作相对于评论家预测的好坏来训练。这很有帮助，因为如果两个动作都导致相同的正回报，我们可能会天真地认为它们是等效的，但如果我们将它们与我们预期的结果进行比较，并且一个回报的表现远好于预期，那么这个动作应该得到更多的强化。
- en: As with the rest of the deep learning methods, we generally must use batches
    of data in order to effectively train. Training with a single example a time introduces
    too much noise, and the training will likely never converge. To introduce batch
    training with Q-learning we used an experience replay buffer that could randomly
    select batches of previous experiences. We could have used experience replay with
    actor-critic, but it is more common to use distributed training with actor-critic
    (and, to be clear, Q-learning can also be distributed). Distributed training in
    actor-critic models is more common because we often want to use a recurrent neural
    network (RNN) layer as part of our reinforcement learning model in cases where
    keeping track of prior states is necessary or helpful in achieving the goal. But
    RNNs need a sequence of temporally related examples, and experience replay relies
    on a batch of independent experiences. We could store entire trajectories (sequences
    of experiences) in a replay buffer, but that just adds complexity. Instead, with
    distributed training and each process running online with its own environment,
    the models can easily incorporate RNNs.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他深度学习方法一样，我们通常必须使用数据批次来有效地训练。一次只使用一个示例进行训练会引入太多的噪声，并且训练可能永远不会收敛。为了引入Q学习的批次训练，我们使用了一个可以随机选择先前经验批次的经验回放缓冲区。我们本来可以使用经验回放与actor-critic结合，但更常见的是使用actor-critic的分布式训练（并且，为了明确起见，Q学习也可以分布式）。在actor-critic模型中，分布式训练更为常见，因为我们经常希望在需要或有助于实现目标的情况下，将循环神经网络（RNN）层作为我们强化学习模型的一部分。但RNN需要一个时间相关的示例序列，而经验回放依赖于一个独立的经验批次。我们可以在回放缓冲区中存储整个轨迹（经验序列），但这只会增加复杂性。相反，通过分布式训练和每个进程运行自己的环境，模型可以轻松地结合RNN。
- en: 'We didn’t cover it here, but there’s another way to train an online actor-critic
    algorithm besides distributed training: simply utilize multiple copies of your
    environment, and then batch together the states from each independent environment,
    feeding it into a single actor-critic model that will then produce independent
    predictions for each environment. This is a viable alternative to distributed
    training when the environments are not expensive to run. If your environment is
    a complicated, high-memory- and computer-intensive simulator, it’s probably going
    to be very slow to run multiple copies of it in a single process, so in that case
    a distributed approach is better.'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里没有涵盖，但除了分布式训练之外，还有另一种训练在线actor-critic算法的方法：简单地利用多个环境副本，然后将每个独立环境的州批量组合在一起，将其输入到一个actor-critic模型中，该模型将为每个环境产生独立的预测。当环境运行成本不高时，这是一种可行的分布式训练替代方案。如果你的环境是一个复杂、高内存和计算密集型的模拟器，那么在单个进程中运行多个副本可能会非常慢，所以在这种情况下，分布式方法更好。
- en: We have now covered what we consider to be the most foundational parts of reinforcement
    learning today. You should now be comfortable with the basic mathematical framework
    of reinforcement learning as a Markov decision process (MDP), and you should be
    able to able to implement Q-learning, plain policy gradient, and actor-critic
    models. If you’ve followed along so far, you should have a good foundation for
    tackling many other reinforcement learning domains.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了我们认为今天强化学习最基础的部分。你现在应该对强化学习作为马尔可夫决策过程（MDP）的基本数学框架感到舒适，你应该能够实现Q学习、普通策略梯度以及actor-critic模型。如果你一直跟随着，你应该有很好的基础去应对许多其他的强化学习领域。
- en: In the rest of the book, we’ll cover more advanced reinforcement learning methods
    with the aim of teaching you some of the most advanced RL algorithms of recent
    times in an intuitive way.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将介绍更多高级的强化学习方法，目的是以直观的方式教你一些最近时期最先进的RL算法。
- en: Summary
  id: totrans-1126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Q-learning learns to predict the discounted rewards given a state and action.
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning学习在给定状态和动作的情况下预测折扣奖励。
- en: Policy methods learn a probability distribution over actions given a state.
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略方法学习在给定状态的情况下动作的概率分布。
- en: Actor-critic models combine a Q-learner with a policy learner.
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-critic models combine a Q-learner with a policy learner.
- en: Advantage actor-critic learns to compute advantages by comparing the expected
    value of an action to the reward that was actually observed, so if an action is
    expected to result in a –1 reward but actually results in a +10 reward, its advantage
    will be higher than an action that is expected to result in +9 and actually results
    in +10.
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态优势演员-评论家通过比较动作的期望值与实际观察到的奖励来学习计算优势，因此如果一个动作预期会产生-1的奖励但实际上产生了+10的奖励，其优势将高于预期产生+9并实际产生+10的动作。
- en: Multiprocessing is running code on multiple different processors that can operate
    simultaneously and independently.
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多进程是在多个不同的处理器上同时独立运行代码，这些处理器可以同时操作。
- en: Multithreading is like multitasking; it allows you to run multiple tasks faster
    by letting the operating system quickly switch between them. When one task is
    idle (perhaps waiting for a file to download), the operating system can continue
    working on another task.
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程类似于多任务处理；它通过让操作系统快速在这些任务之间切换，允许你更快地运行多个任务。当一个任务空闲（可能是在等待文件下载）时，操作系统可以继续处理另一个任务。
- en: Distributed training works by simultaneously running multiple instances of the
    environment and a single shared instance of the DRL model; after each time step
    we compute losses for each individual model, collect the gradients for each copy
    of the model, and then sum or average them together to update the shared parameters.
    This lets us do mini-batch training without an experience replay buffer.
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式训练通过同时运行多个环境实例和一个共享的DRL模型实例来实现；在每一步之后，我们计算每个单独模型的损失，收集每个模型副本的梯度，然后将它们相加或平均以更新共享参数。这使得我们可以在没有经验重放缓冲区的情况下进行小批量训练。
- en: '*N*-step learning is in between fully online learning, which trains 1 step
    at a time, and fully Monte Carlo learning, which only trains at the end of an
    episode. *N*-step learning thus has the advantages of both: the efficiency of
    1-step learning and the accuracy of Monte Carlo.'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*-步学习介于完全在线学习和完全蒙特卡洛学习之间，完全在线学习一次训练一步，而完全蒙特卡洛学习只在情节结束时训练。因此，*N*-步学习具有两者的优点：1步学习的效率和蒙特卡洛的准确性。'
