- en: Part 3\. Higher-level abstractions and orchestration
  id: totrans-0
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第三部分：高级抽象和编排
- en: This part focuses on managing systems of components by using containers. Most
    valuable systems have of two or more components. Managing systems with a small
    number of components is manageable without much automation. But achieving consistency
    and repeatability is difficult without automation as the system grows.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这一部分侧重于通过使用容器来管理组件系统。最有价值的系统通常包含两个或更多组件。管理少量组件的系统不需要太多自动化就可以管理。但随着系统规模的扩大，没有自动化实现一致性和可重复性是困难的。
- en: Managing modern service architectures is complex and requires automated tooling.
    This part dives into higher-level abstractions such as service, environment, and
    configuration. You will learn how to use Docker to provide that automated tooling.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 管理现代服务架构是复杂的，需要自动化工具。这部分深入探讨了高级抽象，如服务、环境和配置。你将学习如何使用 Docker 提供这种自动化工具。
- en: Chapter 11\. Services with Docker and Compose
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第 11 章：使用 Docker 和 Compose 的服务
- en: This chapter covers
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding services and how they relate to containers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解服务以及它们与容器的关系
- en: Basic service administration with Docker Swarm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 进行基本服务管理
- en: Building declarative environments with Docker Compose and YAML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Compose 和 YAML 构建声明性环境
- en: Iterating projects with Compose and the `deploy` command
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Compose 和 `deploy` 命令迭代项目
- en: Scaling services and cleaning up
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展服务和清理
- en: Today most of the software we run is designed to interact with other programs,
    not human users. The resulting webs of interdependent processes serve a collective
    purpose such as processing payments, running games, facilitating global communications,
    or delivering content. When you look closely at that web, you’ll find individual
    running processes that might be running in containers. Those processes are allocated
    memory and given time on the CPU. They are bound to a network and listen for requests
    from other programs on specific ports. Their network interface and port are registered
    with a naming system so they are discoverable on that network. But as you expand
    your view and examine more processes, you’ll notice that most of them share common
    characteristics and goals.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 今天，我们运行的大多数软件都是设计用来与其他程序交互，而不是与人类用户交互。由此产生的相互依赖的过程网服务于集体目的，如处理支付、运行游戏、促进全球通信或提供内容。当你仔细观察这个网络时，你会发现可能运行在容器中的单个运行进程。这些进程被分配内存并在
    CPU 上获得时间。它们绑定到网络上，并在特定端口上监听来自其他程序的请求。它们的网络接口和端口在命名系统中注册，以便在该网络上被发现。但随着你扩大视野并检查更多进程，你会注意到它们大多数具有共同的特征和目标。
- en: Any processes, functionality, or data that must be discoverable and available
    over a network is called a service. That name, service, is an abstraction. By
    encoding those goals into an abstract term, we simplify how we talk about the
    things that use this pattern. When we talk about a specific service, we do not
    need to expressly state that the name should be discoverable via DNS or the environment-appropriate
    service-discovery mechanism. We do not need to state that the service should be
    running when a client needs to use it. Those expectations are already communicated
    by common understanding of the service abstraction. The abstraction lets us focus
    on the things that make any specific service special.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任何必须在网络上被发现和可用的进程、功能或数据都称为服务。这个名字，服务，是一个抽象。通过将这些目标编码到抽象术语中，我们简化了我们谈论使用此模式的事物的方式。当我们谈论一个特定的服务时，我们不需要明确说明该名称应该通过
    DNS 或环境适当的发现机制来发现。我们也不需要说明当客户端需要使用它时，该服务应该正在运行。这些期望已经通过服务抽象的普遍理解而传达。这种抽象让我们专注于使任何特定服务特殊的事物。
- en: We can reflect those same benefits in our tooling. Docker already does this
    for containers. Containers were described in [chapter 6](index_split_053.html#filepos544857)
    as processes that were started using specific Linux namespaces, with specific
    filesystem views, and resource allotments. We don’t have to describe those specifics
    each time we talk about a container, nor do we have to do the work of creating
    those namespaces ourselves. Docker does this for us. Docker provides tooling for
    other abstractions as well, including service.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以在我们的工具中反映出相同的优势。Docker 已经为容器做了这件事。容器在第 [6 章](index_split_053.html#filepos544857)
    中被描述为使用特定 Linux 命名空间、特定文件系统视图和资源分配启动的进程。我们不必每次谈论容器时都描述这些具体细节，也不必亲自创建这些命名空间。Docker
    会为我们完成这些。Docker 还为其他抽象提供了工具，包括服务。
- en: This chapter introduces the basic tooling that Docker provides for working with
    services in swarm mode. It covers the service life cycle, the role of an orchestrator,
    and how to interact with that orchestrator to deploy and manage services on your
    machine. You will use the tools described in this chapter throughout the remainder
    of the book. The same concepts, problems, and fundamental tooling are provided
    by all of the container orchestration systems including Kubernetes. The material
    that follows will be helpful in understanding whichever orchestrators you use
    in your daily job.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章介绍了Docker为在集群模式下处理服务提供的工具。它涵盖了服务生命周期、编排者的角色以及如何与编排者交互以在您的机器上部署和管理服务。您将在本书的剩余部分使用本章中描述的工具。所有容器编排系统，包括Kubernetes，都提供了相同的概念、问题和基本工具。以下内容将有助于理解您在日常工作中使用的任何编排器。
- en: 11.1\. A SERVICE “HELLO WORLD!”
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.1. 一个“HELLO WORLD!”服务
- en: 'Getting started with services is as easy as getting started with containers.
    In this case, you can start a “Hello World!” web server locally by running these
    two commands:'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与容器入门一样简单，开始使用服务。在这种情况下，您可以通过运行以下两个命令在本地启动一个“Hello World!”网络服务器：
- en: '`docker swarm init` `1` `docker service create \` `2` `--publish 8080:80 \
        --name hello-world \    dockerinaction/ch11_service_hw:v1`'
  id: totrans-16
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker swarm init` `1` `docker service create \` `2` `--publish 8080:80 \
        --name hello-world \    dockerinaction/ch11_service_hw:v1`'
- en: 1 Enables the service abstraction
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 启用服务抽象
- en: 2 Starts the server on localhost:8080
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 在localhost:8080上启动服务器
- en: Unlike containers, Docker services are available only when Docker is running
    in swarm mode. Initializing swarm mode starts an internal database as well as
    a long-running loop in the Docker Engine that performs service orchestration;
    see [figure 11.1](#filepos1153889). Swarm provides several other features that
    will be covered in the rest of the book. Running the `init` command in the preceding
    code enables the service management subcommands.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与容器不同，Docker服务仅在Docker以集群模式运行时才可用。初始化集群模式将启动一个内部数据库以及在Docker Engine中执行服务编排的长时间运行的循环；参见[图11.1](#filepos1153889)。蜂群还提供了本书其余部分将要介绍的其他一些功能。在前面代码中运行`init`命令启用服务管理子命令。
- en: Figure 11.1\. Initializing a swarm node
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图11.1. 初始化蜂群节点
- en: '![](images/00009.jpg)'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00009.jpg)'
- en: The `service create` subcommand defines a service named `hello-world` that should
    be available on port 8080, and uses the image, `dockerinaction/ch11_service _hw:v1`
    as shown in [figure 11.2](#filepos1154927).
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`service create`子命令定义了一个名为`hello-world`的服务，该服务应在8080端口上可用，并使用如图11.2所示的镜像`dockerinaction/ch11_service_hw:v1`。[图11.2](#filepos1154927)。'
- en: Figure 11.2\. Creating your first service
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图11.2. 创建您的第一个服务
- en: '![](images/00087.jpg)'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00087.jpg)'
- en: After you run these two commands, you should see a progress bar describing the
    state of the service. Once the progress is complete, it will be labeled `Running`,
    and the command will exit. At this point, you should be able to open http://localhost:8080
    and see a nice little message, `Hello, World! --ServiceV1`. It will also display
    the task ID (container ID) that served the request.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 运行这两个命令后，您应该会看到一个进度条，描述服务的状态。一旦进度完成，它将被标记为`Running`，并且命令将退出。此时，您应该能够打开http://localhost:8080并看到一个友好的小消息，“Hello,
    World! --ServiceV1”。它还会显示处理请求的任务ID（容器ID）。
- en: A task is a swarm concept that represents a unit of work. Each task has one
    associated container. Although there could be other types of tasks that don’t
    use containers, those are not the subject of this chapter. Swarm works only with
    tasks. The underlying components transform task definitions into containers. This
    book is not concerned with Docker internals. For our purposes, you can consider
    the terms task and container roughly interchangeable.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任务是蜂群中的一个概念，代表一个工作单元。每个任务都关联一个容器。尽管可能存在其他类型的任务不使用容器，但这些不是本章的主题。蜂群只与任务一起工作。底层组件将任务定义转换为容器。本书不涉及Docker内部结构。就我们的目的而言，可以将任务和容器这两个术语大致互换。
- en: Figure 11.3\. The swarm node automatically creates a container to run the service
    software.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图11.3. 蜂群节点自动创建一个容器来运行服务软件。
- en: '![](images/00002.jpg)'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: This should feel just like running a similar container example. Rather than
    focusing on how services and containers are similar, it is more enlightening to
    focus on how they differ. First, recognize that service workloads are implemented
    with containers. With the service running, run `docker container ps` to discover
    that there is a container running with a name, such as `hello-world.1.pqamgg6bl5eh6p8j4fj503kur`.
    There is nothing special about this container. Inspecting the container does not
    yield any particularly interesting results. You might notice a few Swarm-specific
    labels, but that is it. However, if you remove the container, something interesting
    happens. The remainder of this section describes the higher-level properties,
    the service life cycle, and how Swarm uses those to perform small automated miracles
    such as service resurrection.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这应该感觉就像运行一个类似的容器示例一样。与其关注服务和容器之间的相似之处，不如关注它们之间的不同之处。首先，认识到服务工作负载是用容器实现的。当服务运行时，运行
    `docker container ps` 来发现有一个名为 `hello-world.1.pqamgg6bl5eh6p8j4fj503kur` 的容器正在运行。这个容器没有什么特别之处。检查容器不会产生任何特别有趣的结果。你可能注意到了一些
    Swarm 特定的标签，但仅此而已。然而，如果你移除容器，会发生一些有趣的事情。本节的剩余部分将描述更高层次的属性，服务生命周期，以及 Swarm 如何使用这些属性来执行诸如服务恢复等小型自动化奇迹。
- en: 11.1.1\. Automated resurrection and replication
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.1.1\. 自动恢复和复制
- en: Bringing a service back to life is something most developers and operators are
    a bit too familiar with. For those people, manually killing the sole process running
    a service might feel like tempting fate. Like kicking an early model robot with
    artificial intelligence—it just feels like a bigger risk than we’d prefer to take.
    But with the right tools (and validation of those tools), we can be comfortable
    knowing that taking the risk probably won’t result in any Armageddon-like business
    scenario.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将服务恢复到生命状态是大多数开发人员和运维人员都比较熟悉的事情。对于这些人来说，手动终止运行服务的唯一进程可能感觉像是在诱惑命运。就像踢一个早期的人工智能机器人——这感觉比我们愿意承担的风险要大。但是，有了正确的工具（以及这些工具的验证），我们可以安心地知道承担这种风险可能不会导致任何类似世界末日般的业务场景。
- en: 'If you remove the sole container that is powering the `hello-world` service
    (from the previous section), the container will be stopped and removed, but after
    a few moments it will be back. Or at least another similarly configured container
    will be there in its place. Try this yourself: find the ID of the container running
    the server (using `docker ps`); use `docker container rm -f` to remove it; then
    issue a few `docker container ps` commands to verify that it has been removed,
    and watch a replacement appear. Next, dig into this resurrection by using the
    `service` subcommands shown in [figure 11.4](#filepos1160485).'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你移除驱动 `hello-world` 服务的唯一容器（来自上一节），容器将被停止并移除，但几分钟后它将重新启动。或者至少会有另一个配置相似的容器出现在它的位置。你可以亲自尝试一下：找到运行服务器的容器
    ID（使用 `docker ps`）；使用 `docker container rm -f` 来移除它；然后执行几个 `docker container ps`
    命令来验证它已被移除，并观察一个替代品出现。接下来，通过使用 [图 11.4](#filepos1160485) 中显示的 `service` 子命令来深入了解这种恢复。
- en: Figure 11.4\. Timeline of Swarm reactions to changes in service specification
    and state
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 11.4\. Swarm 对服务规范和状态变化的响应时间线
- en: '![](images/00068.jpg)'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00068.jpg)'
- en: 'First, run `docker service ls` to list the running services. The list will
    include the `hello-world` service and show that one replica is running, indicated
    by `1/1` in the `REPLICAS` column of the command output. This is evidenced by
    the container that came back to life. Next, run `docker service ps hello-world`
    to list the containers associated with a specific service (`hello-world` in this
    case). The list includes two entries. The list will show the first entry with
    a Desired State of “`Running`,” and a Current State of “`Running x minutes ago`.”
    The second entry will be listed as `Shutdown` and `Failed`, respectively. These
    two columns hint at critical ideas, so let’s unpack them now. Consider this excerpt
    from `docker service ps` output:'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，运行 `docker service ls` 命令来列出正在运行的服务。列表将包括 `hello-world` 服务，并显示有一个副本正在运行，这可以通过命令输出中
    `REPLICAS` 列的 `1/1` 来表示。这可以通过容器重新启动的事实得到证明。接下来，运行 `docker service ps hello-world`
    命令来列出与特定服务（在本例中为 `hello-world`）关联的容器。列表包括两个条目。列表将显示第一个条目具有“`Running`”的期望状态和“`Running
    x minutes ago`”的当前状态。第二个条目将分别列出为 `Shutdown` 和 `Failed`。这两个列提示了一些关键思想，所以现在让我们来详细解释一下。考虑以下来自
    `docker service ps` 输出的摘录：
- en: '`NAME                DESIRED STATE       CURRENT STATE hello-world.1      
    Running             Running less than a second ago \_ hello-world.1   Shutdown           
    Failed 16 seconds ago`'
  id: totrans-36
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`名称                目标状态           当前状态 hello-world.1       运行中            
    运行中，少于1秒前 \_ hello-world.1   关闭            失败，16秒前`'
- en: 'Autonomous orchestrators—such as the Swarm components in Docker—track two things:
    desired state and the current state. The desired state is what the user wants
    the system to be doing, or what it is supposed to be doing. The current state
    describes what the system is actually doing. Orchestrators track these two descriptions
    of state and reconcile the two by changing the system.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自主编排器——例如Docker中的Swarm组件——跟踪两个东西：期望状态和当前状态。期望状态是用户希望系统执行的操作，或者它应该执行的操作。当前状态描述了系统实际执行的操作。编排器跟踪这两个状态描述，并通过改变系统来协调这两个描述。
- en: 'In this example, the swarm orchestrator notices that the container for the
    `hello-world` service has failed. It doesn’t matter that you killed the process
    in this case. All the orchestrator knows is that the process has failed and that
    the desired state of the service is `Running`. Swarm knows how to make a process
    run: start a container for that service. And that is exactly what it does.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个例子中，swarm编排器注意到`hello-world`服务的容器已失败。在这种情况下，你杀死了进程并不重要。编排器所知道的是进程已失败，服务的期望状态是`运行中`。Swarm知道如何使一个进程运行：为该服务启动一个容器。这正是它所做的事情。
- en: Using higher-level abstractions with autonomous orchestrators is more like a
    partnership than using a tool. Orchestrators remember how a system should be operating
    and manipulate it without being asked to do so by a user. So, in order to use
    orchestrators effectively, you need to understand how to describe systems and
    their operation. You can learn quite a bit about managing services by inspecting
    the `hello-world` service.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用自主编排器的高级抽象更像是伙伴关系，而不是使用工具。编排器会记住系统应该如何运行，并在不经过用户请求的情况下对其进行操作。因此，为了有效地使用编排器，您需要了解如何描述系统和它们的操作。通过检查`hello-world`服务，您可以了解很多关于管理服务的内容。
- en: 'When you run `docker service inspect hello-world`, Docker will output the current
    desired state definition for the service. The resulting JSON document includes
    the following:'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您运行`docker service inspect hello-world`时，Docker将输出服务的当前期望状态定义。生成的JSON文档包括以下内容：
- en: Name of the service
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的名称
- en: Service ID
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务ID
- en: Versioning and timestamps
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制和时间戳
- en: A template for the container workloads
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器工作负载的模板
- en: A replication mode
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制模式
- en: Rollout parameters
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署参数
- en: Similar rollback parameters
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似的回滚参数
- en: A description of the service endpoint
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务端点的描述
- en: 'The first few items identify the service and its change history. It starts
    to get interesting with replication mode, and rollout and rollback parameters.
    Recall our definition of a service: any processes, functionality, or data that
    must be discoverable and available over a network. The difficulty of running a
    service is, by definition, more about managing availability of something on a
    network. So it shouldn’t be too surprising that a service definition is predominantly
    about how to run replicas, manage changes to the software, and route requests
    to the service endpoint to that software. These are the higher-level properties
    uniquely associated with the service abstraction. Let’s examine these in more
    depth.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前几个条目标识了服务和其变更历史。当涉及到复制模式和部署和回滚参数时，事情开始变得有趣。回想一下我们对服务的定义：任何必须在网络上可发现和可用的进程、功能或数据。运行服务的难度，按定义，更多地关于管理网络上某物的可用性。因此，服务定义主要关于如何运行副本、管理软件的变更以及将请求路由到服务端点以访问该软件，这并不令人惊讶。这些都是与服务抽象独特关联的高级属性。让我们更深入地研究这些。
- en: 'A replication mode tells Swarm how to run replicas of the workload. Today there
    are two modes: replicated and global. A service in replicated mode will create
    and maintain a fixed number of replicas. This is the default mode, and you can
    experiment with that now by using the `docker service scale` command. Tell your
    swarm to run three replicas of the `hello-world` service by running the following:'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 复制模式告诉Swarm如何运行工作负载的副本。今天有两种模式：复制和全局。复制模式的服务将创建并维护固定数量的副本。这是默认模式，您现在可以通过使用`docker
    service scale`命令来实验。通过运行以下命令告诉您的swarm运行三个`hello-world`服务的副本：
- en: '`docker service scale hello-world=3`'
  id: totrans-51
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service scale hello-world=3`'
- en: After the containers start up, you can verify the work by using either `docker
    container ps` or `docker service ps hello-world` to list the individual containers
    (there should be three now). You should notice that the container-naming convention
    encodes the service replica number. For example, you should see a container with
    a name such as `hello-world.3.pqamgg6bl5eh6p8j4fj503kur`. If you scale the service
    back down, you’ll also notice that higher-numbered containers are removed first.
    So if you run `docker service scale hello-world=2`, the container named `hello-world
    .3.pqamgg6bl5eh6p8j4fj503kur` will be removed. But `hello-world.1` and `hello-world.2`
    will remain untouched.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 容器启动后，你可以使用 `docker container ps` 或 `docker service ps hello-world` 来验证工作，列出单个容器（现在应该是三个）。你应该注意到容器命名约定编码了服务副本号。例如，你应该看到一个名为
    `hello-world.3.pqamgg6bl5eh6p8j4fj503kur` 的容器。如果你将服务规模缩小，你也会注意到编号较高的容器首先被删除。所以如果你运行
    `docker service scale hello-world=2`，名为 `hello-world .3.pqamgg6bl5eh6p8j4fj503kur`
    的容器将被删除。但 `hello-world.1` 和 `hello-world.2` 将保持不变。
- en: The second mode, global, tells Docker to run one replica on each node in the
    swarm cluster. This mode is more difficult to experiment with at this point because
    you’re running only a single-node cluster (unless you’re skipping ahead). Services
    in global mode are useful for maintaining a common set of infrastructure services
    that must be available locally on each node in a cluster.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第二种模式，全局模式，告诉 Docker 在集群中的每个节点上运行一个副本。这种模式目前更难进行实验，因为你正在运行一个单节点集群（除非你正在跳过）。全局模式下的服务对于维护必须在集群中每个节点上本地可用的公共基础设施服务很有用。
- en: It isn’t important that you have a deep understanding of how replication works
    in a docker swarm at this point. But it is critical to understand that maintaining
    high service availability requires running replicas of that service software.
    Using replicas allows you to replace or change portions of the replica set, or
    survive failures without impacting the service availability. When you have replicas
    of software, certain operational stories become more complicated. For example,
    upgrading software is not as simple as stopping the old version and starting the
    new one. A few properties impact change management and deployment processes.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这一点上，你不需要深入理解 Docker 集群中复制的机制。但理解维护高服务可用性需要运行该服务软件的副本是至关重要的。使用副本允许你替换或更改副本集中的部分，或者在不受服务可用性影响的情况下生存失败。当你有软件副本时，某些操作故事会变得更加复杂。例如，升级软件并不像停止旧版本并启动新版本那样简单。一些属性会影响变更管理和部署过程。
- en: 11.1.2\. Automated rollout
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.1.2\. 自动部署
- en: Rolling out a new version of replicated service software is not particularly
    complicated, but you should consider a few important parameters when automating
    the process. You have to describe the characteristics of the deployment. Those
    include order, batch sizes, and delays. This is done by specifying constraints
    and parameters to the swarm orchestrator that it will respect during the deployment
    process. An illustration of a Docker swarm’s actions when deploying an update
    is shown in [figure 11.5](#filepos1172519).
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部署复制的服务软件的新版本并不特别复杂，但在自动化此过程时，你应该考虑一些重要的参数。你必须描述部署的特性。这包括顺序、批量大小和延迟。这是通过指定约束和参数给集群编排器，使其在部署过程中遵守来完成的。Docker
    集群在部署更新时的操作示意图显示在[图 11.5](#filepos1172519)中。
- en: Figure 11.5\. Timeline of automated deployment of an updated service definition
    to a Docker swarm
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 11.5\. 将更新的服务定义自动部署到 Docker 集群的时序图
- en: '![](images/00048.jpg)'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00048.jpg)'
- en: 'Consider this command to update the `hello-world` service created earlier in
    this chapter:'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑以下命令来更新本章早期创建的 `hello-world` 服务：
- en: '`docker service update \     --image dockerinaction/ch11_service_hw:v2 \` `1`
    `--update-order stop-first \     --update-parallelism 1 \     --update-delay 30s
    \     hello-world` `2`'
  id: totrans-60
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \    --image dockerinaction/ch11_service_hw:v2 \    --update-order
    stop-first \    --update-parallelism 1 \    --update-delay 30s \    hello-world`'
- en: 1 The new image
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 新的镜像
- en: 2 The name of the service to update
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 需要更新的服务的名称
- en: 'This command tells Docker to change the `hello-world` service to use the image
    tagged `v2`. It further qualifies the deployment characteristics: only one replica
    should be updated at a time; wait for 30 seconds between updating each batch of
    replicas; and each replica should be stopped before its replacement is started.
    When you execute the command, Docker will report the deployment progress for each
    replica and the change overall. If everything goes well, it will close by stating
    that the `Service converged`. That is a particularly robotic way of telling the
    user that the command was successful. Converged is a technical way to say that
    current state of the service is the same as the desired state described by the
    command.'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此命令告诉Docker将`hello-world`服务更改为使用标记为`v2`的镜像。它进一步限定了部署特性：一次只更新一个副本；在更新每个副本批次之间等待30秒；在启动替换之前停止每个副本。当你执行此命令时，Docker将报告每个副本的部署进度以及整体变化。如果一切顺利，它将以声明`Service
    converged`结束。这是一种特别机械的方式告诉用户命令已成功执行。收敛是一个技术术语，表示服务的当前状态与命令中描述的期望状态相同。
- en: After you’ve updated the service, you should be able to reload the application
    in your browser and see that it has been signed `--ServiceV2`. This is not a particularly
    interesting example because everything just works. Things aren’t so simple in
    the real world. We use parallelism to balance the time it takes to update our
    service with protection for our users from failed transitions. We introduce delay
    between update batches to allow new service instances to become stable before
    starting (and to make sure that the underlying platform remains stable). In reality,
    30 seconds might not be enough time. It depends on the application.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 更新服务后，你应该能够在浏览器中重新加载应用程序，并看到它已被签名`--ServiceV2`。这不是一个特别有趣的例子，因为一切都很顺利。在现实世界中事情并不这么简单。我们使用并行性来平衡更新服务所需的时间，同时保护用户免受失败转换的影响。我们在更新批次之间引入延迟，以便在启动之前让新的服务实例变得稳定（并确保底层平台保持稳定）。实际上，30秒可能并不足够。这取决于应用程序。
- en: Docker and its Swarm components are application agnostic. They could never anticipate
    the behavior of all the applications that might be deployed or how they might
    fail at runtime. Instead, the Docker command line and API provide ways for the
    user to specify the methods for discovering problems and validating success, and
    the behavior for managing failed deployments.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker及其Swarm组件对应用程序是中立的。它们永远无法预测可能部署的所有应用程序的行为以及它们在运行时可能如何失败。相反，Docker命令行和API为用户提供指定发现问题和验证成功的方法，以及管理失败部署的行为。
- en: 11.1.3\. Service health and rollback
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.1.3. 服务健康和回滚
- en: A successful partnership with an orchestrator means clearly communicating the
    expected requirements and behavior of the workload you’re asking it to orchestrate.
    Although Docker can be sure that a stopped container is unhealthy, there is no
    universal and accurate definition of service health. This limits the safe assumptions
    that any orchestrator can make about service health, and deployment success or
    failure. Determining workload health or startup behavior expectations might be
    more nuanced than you’d expect unless you’re an experienced service owner.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与编排器的成功合作意味着明确沟通你要求其编排的工作负载的预期需求和行为。尽管Docker可以确信停止的容器是不健康的，但并没有一个普遍和准确的关于服务健康状态的定义。这限制了任何编排器对服务健康状态和部署成功或失败所做的安全假设。除非你是经验丰富的服务所有者，否则确定工作负载的健康状态或启动行为预期可能比你想象的要复杂。
- en: 'Before getting too far into the details, start with a simple example. In the
    most obvious case of service health problems, the new containers might fail to
    start:'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在深入细节之前，从一个简单的例子开始。在最明显的服务健康问题案例中，新的容器可能无法启动：
- en: '`docker service update \     --image dockerinaction/ch11_service_hw:start-failure
    \     hello-world`'
  id: totrans-69
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \ --image dockerinaction/ch11_service_hw:start-failure
    \ hello-world`'
- en: When you execute this command, Docker will start a deployment on the `hello-world`
    service. Unlike the others, this change will fail. By default, Docker will pause
    the deployment after the first replica fails to start. The command will exit,
    but it will continue to attempt to start that container. If you run `docker service
    ps hello-world`, you will see that two of the replicas remain on the old version
    of the service, and the other replica keeps cycling through starting and failed
    states.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你执行此命令时，Docker将在`hello-world`服务上启动部署。与其他不同，这次更改将失败。默认情况下，Docker将在第一个副本启动失败后暂停部署。命令将退出，但它将继续尝试启动该容器。如果你运行`docker
    service ps hello-world`，你会看到有两个副本仍然在旧版本的服务上，而另一个副本则持续在启动和失败状态之间循环。
- en: 'In this scenario, the deployment cannot proceed. The new version will never
    start. And as a result, the service is stuck at reduced capacity and will require
    human intervention to repair. Fix the immediate issue by using the `--rollback`
    flag on the `update` command:'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这种情况下，部署无法进行。新版本将永远不会启动。因此，服务将处于减容状态，需要人工干预才能修复。通过在`update`命令中使用`--rollback`标志来修复立即问题：
- en: '`docker service update \     --rollback \     hello-world`'
  id: totrans-72
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \    --rollback \    hello-world`'
- en: This command will ask Docker to reconcile the current state with the previous
    desired state. Docker will determine that it needs to change only one of the three
    replicas (the one that failed to start). It knows that the service is currently
    in a paused deployment and that only one of the replicas was transitioned to the
    current desired state. The other replicas will continue operating.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此命令将要求Docker将当前状态与之前期望的状态进行协调。Docker将确定它只需要更改三个副本中的一个（未能启动的那个）。它知道服务目前处于暂停部署状态，并且只有一个副本已过渡到当前期望状态。其他副本将继续运行。
- en: Knowing that rollback is appropriate for this service (no risk of incompatible
    application state changes), you can automate rollback when the deployment fails.
    Use the `--update-failure-action` flag to tell Swarm that failed deployments should
    roll back. But you should also explicitly tell Swarm which conditions should be
    considered a failure.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 知道回滚对于此服务是合适的（没有不兼容的应用状态变化的风险），你可以在部署失败时自动化回滚。使用`--update-failure-action`标志告诉Swarm失败的部署应该回滚。但你也应该明确告诉Swarm哪些条件应被视为失败。
- en: 'Suppose you’re running 100 replicas of a service and those are to be run on
    a large cluster of machines. Chances are that a certain set of conditions might
    prevent a replica from starting correctly. In that case, it might be appropriate
    to continue a deployment as long as a critical threshold of replicas are operational.
    In this next deployment, tell Swarm to tolerate start failures as long as one-third
    of the fleet is operational for illustrative purposes. You’ll use the `--update-max-failure-ratio`
    flag and specify 0.6:'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设你正在运行100个副本的服务，并且这些副本将在一个大型机器集群上运行。有可能某些条件会阻止副本正确启动。在这种情况下，只要关键阈值内的副本处于运行状态，就继续部署可能是合适的。在接下来的部署中，告诉Swarm在舰队中三分之一处于运行状态的情况下容忍启动失败，以示说明。你将使用`--update-max-failure-ratio`标志并指定0.6：
- en: '`docker service update \     --update-failure-action rollback \     --update-max-failure-ratio
    0.6 \     --image dockerinaction/ch11_service_hw:start-failure \     hello-world`'
  id: totrans-76
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \    --update-failure-action rollback \    --update-max-failure-ratio
    0.6 \    --image dockerinaction/ch11_service_hw:start-failure \    hello-world`'
- en: 'When you run this example, you’ll watch Docker attempt to deploy updated replicas
    one at a time. The first one will retry a few times before the delay expires and
    the next replica deployment starts. Immediately after the second replica fails,
    the whole deployment will be marked as failed and a rollback will be initiated.
    The output will look something like this:'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你运行此示例时，你会看到Docker尝试逐个部署更新的副本。第一个副本会在延迟过期之前重试几次，然后下一个副本部署开始。第二个副本失败后，整个部署将被标记为失败，并启动回滚。输出将类似于以下内容：
- en: '`hello-world overall progress: rolling back update: 2 out of 3 tasks 1/3: running 
    [>                  ] 2/3: starting [=====>             ] 3/3: running  [>                 
    ] rollback: update rolled back due to failure or early termination of` ![](images/00055.jpg)
    `task tdpv6fud16e4nbg3tx2jpikah service rolled back: rollback completed`'
  id: totrans-78
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`hello-world整体进度：回滚更新：3个任务中的2个 1/3：运行 [>                  ] 2/3：启动 [=====>            
    ] 3/3：运行 [>                  ] 回滚：由于失败或任务tdpv6fud16e4nbg3tx2jpikah的提前终止，更新已回滚：服务回滚：回滚完成`'
- en: After the command finishes, the service will be in the same state as it was
    prior to the update. You can verify this by running `docker service ps hello-world`.
    Note that one replica was untouched, while the other two were started much more
    recently and at nearly the same time. At this point, all the replicas will be
    running from the `dockerinaction/ch11_service_hw:v2` image.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命令执行完毕后，服务将处于更新前的相同状态。你可以通过运行`docker service ps hello-world`来验证这一点。请注意，有一个副本未受影响，而其他两个副本启动得较晚，并且几乎同时。此时，所有副本都将从`dockerinaction/ch11_service_hw:v2`镜像运行。
- en: As we mentioned earlier, running is not the same thing as service health. There
    are plenty of ways that a program might be running, but not correctly. Like other
    orchestrators, Docker models health separately from process status and provides
    a few configuration points for specifying how to determine service health.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，运行并不等同于服务健康。程序可能以多种方式运行，但并不总是正确。像其他编排器一样，Docker将健康状态与进程状态分开建模，并提供了一些配置点来指定如何确定服务健康状态。
- en: Docker is application agnostic. Rather than making assumptions about how to
    determine whether a specific task is healthy, it lets you specify a health check
    command. That command will be executed from within the container for each service
    replica. Docker will execute that command periodically on the specified schedule
    from within the task containers themselves. This behaves just like issuing a `docker
    exec` command. Health checks and related parameters can be specified at service
    creation time, changed or set on service updates, or even specified as image metadata
    by using the `HEALTHCHECK` Dockerfile directive.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker对应用程序是中立的。它不假设如何确定特定任务是否健康，而是允许你指定一个健康检查命令。该命令将在每个服务副本的容器内执行。Docker将在任务容器内按照指定的计划定期执行该命令。这就像发出一个`docker
    exec`命令一样。健康检查和相关参数可以在创建服务时指定，也可以在服务更新时更改或设置，甚至可以通过使用`HEALTHCHECK` Dockerfile指令将其指定为镜像元数据。
- en: 'Each service replica container (task) will inherit the definition of health
    and health check configuration from the service. When you want to manually inspect
    the health of a specific task in Docker, you need to inspect the container, not
    the service itself. Both the v1 and v2 versions of the service you’ve been using
    have health checks specified in the image. The images include a small custom program
    called `httpping`. It verifies that the service is responsive on localhost and
    that requests to / result in an HTTP 200 response code. The Dockerfiles include
    the following directive:'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个服务副本容器（任务）将继承服务的健康和健康检查配置定义。当你想手动检查Docker中特定任务的健康状态时，你需要检查容器，而不是服务本身。你一直使用的服务的v1和v2版本都在镜像中指定了健康检查。这些镜像包含一个名为`httpping`的小型自定义程序。它验证服务在本地主机上是否响应，并且对/的请求导致HTTP
    200响应代码。Dockerfile包含以下指令：
- en: '`HEALTHCHECK --interval=10s CMD ["/bin/httpping"]`'
  id: totrans-83
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`HEALTHCHECK --interval=10s CMD ["/bin/httpping"]`'
- en: Run `docker container ps` to see that each `hello-world` replica is marked as
    healthy in the status column. You can further inspect the configuration by inspecting
    either the image or container.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 运行`docker container ps`可以看到每个`hello-world`副本在状态列中被标记为健康。你可以通过检查镜像或容器来进一步检查配置。
- en: 'Including some default health check configuration is a good idea for images
    containing service software, but it is not always available. Consider `dockerinaction/ch11
    _service_hw:no-health`. This image is essentially the same as the v1 and v2 images,
    but it does not include any health check metadata. Update the `hello-world` service
    to use this version now:'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在包含服务软件的镜像中包含一些默认的健康检查配置是一个好主意，但并非总是可用。考虑`dockerinaction/ch11 _service_hw:no-health`这个镜像。这个镜像本质上与v1和v2镜像相同，但它不包含任何健康检查元数据。现在更新`hello-world`服务以使用这个版本：
- en: '`docker service update \     --update-failure-action rollback \     --image
    dockerinaction/ch11_service_hw:no-health \     hello-world`'
  id: totrans-86
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \ --update-failure-action rollback \ --image dockerinaction/ch11_service_hw:no-health
    \ hello-world`'
- en: 'After deploying this version, you should be able to run `docker container ps`
    again and see that the containers are no longer marked `healthy`. Docker cannot
    determine whether the service is healthy without health check metadata. It knows
    only whether the software is running. Next, update the service to add the health
    check metadata from the command line:'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部署这个版本后，你应该能够再次运行`docker container ps`并看到容器不再被标记为`healthy`。没有健康检查元数据，Docker无法确定服务是否健康。它只知道软件是否正在运行。接下来，更新服务以从命令行添加健康检查元数据：
- en: '`docker service update \     --health-cmd /bin/httpping \     --health-interval
    10s \     hello-world`'
  id: totrans-88
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \   '
- en: Health monitoring requires continuous evaluation. The interval specified here
    tells Docker how often to check the health of each service instance. When you
    run this command, you can verify that the service replicas are marked as healthy
    once again.
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 健康监控需要持续评估。这里指定的间隔告诉Docker多久检查一次每个服务实例的健康状况。当你运行这个命令时，你可以验证服务副本再次被标记为健康。
- en: Today you can also tell Docker how many times to retry a health check before
    reporting an unhealthy status, a startup delay, and a time-out for running a health
    check command. These parameters help tune the behavior to accommodate most cases.
    Sometimes a default or current health check for a service is inappropriate for
    the way you are using it. In those cases, you can create or update a service with
    health checks disabled by using the `--no-healthcheck` flag.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 今天你还可以告诉Docker在报告不健康状态之前，健康检查应该重试多少次，启动延迟以及运行健康检查命令的超时时间。这些参数有助于调整行为以适应大多数情况。有时，服务默认或当前的健康检查不适合你的使用方式。在这些情况下，你可以使用`--no-healthcheck`标志创建或更新一个禁用健康检查的服务。
- en: 'During a deployment, a new container might not start. Or it might start but
    not quite work correctly (be unhealthy). But how do you define service health?
    Timing issues might blur those definitions. How long should you wait for an instance
    to become healthy? Some but not all of the service replicas might fail or be otherwise
    unhealthy. How many or what portion of replica deployment failure can your service
    tolerate? Once you can answer these questions, you can tell Docker about those
    thresholds and tune the signal of health from your application to the orchestrator.
    In the meantime, you’re free to delete the `hello-world` service:'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在部署过程中，一个新的容器可能无法启动。或者它可能启动了，但并不完全正确（不健康）。但你是如何定义服务健康的呢？时间问题可能会模糊这些定义。你应该等待多长时间才能让实例变得健康？一些但不是所有的服务副本可能会失败或处于不健康状态。你的服务可以容忍多少或多少比例的副本部署失败？一旦你能回答这些问题，你就可以告诉Docker那些阈值，并调整从你的应用程序到编排器的健康信号。在此期间，你可以自由地删除`hello-world`服务：
- en: '`docker service rm hello-world`'
  id: totrans-92
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service rm hello-world`'
- en: Setting all of these parameters when you’re managing a service from the command
    line is a mess. When you’re managing several services, it can get even worse.
    In the next section, you’ll learn how to use the declarative tooling provided
    by Docker and make things more manageable.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你在命令行管理服务时，设置所有这些参数会变得一团糟。当你管理多个服务时，情况可能会变得更糟。在下一节中，你将学习如何使用Docker提供的声明性工具，使事情变得更加易于管理。
- en: 11.2\. DECLARATIVE SERVICE ENVIRONMENTS WITH COMPOSE V3
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.2. 使用COMPOSE V3的声明式服务环境
- en: Until this point in the book, you’ve used the Docker command line to individually
    create, change, remove, or interact with containers, images, networks, and volumes.
    We say that systems like this follow an imperative pattern. Imperative-style tools
    carry out commands issued by a user. The commands might retrieve specific information
    or describe a specific change. Programming languages and command-line tools follow
    an imperative pattern.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 到目前为止，你一直在使用Docker命令行单独创建、更改、删除或与容器、镜像、网络和卷交互。我们说这样的系统遵循命令式模式。命令式风格的工具执行用户发出的命令。这些命令可能检索特定信息或描述特定更改。编程语言和命令行工具遵循命令式模式。
- en: The benefit of imperative tools is that they give a user the ability to use
    primitive commands to describe much more complex flows and systems. But the commands
    must be followed exactly, in precise order, so that they have exclusive control
    of the working state. If another user or process is changing the state of the
    system at the same time, the two users might make undetectable conflicting changes.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命令式工具的好处是它们使用户能够使用原始命令来描述更复杂的工作流程和系统。但命令必须严格按照精确的顺序执行，以确保它们对工作状态有独占控制。如果另一个用户或进程同时更改系统的状态，这两个用户可能会做出无法检测到的冲突更改。
- en: Imperative systems have a few problems. It is burdensome for users to carefully
    plan and sequence all of the commands required to achieve a goal. Those plans
    are often challenging to audit or test. Timing or shared state issues are incredibly
    difficult to discover and test before deployment. And as most programmers will
    tell you, small or innocuous mistakes might radically change the outcome.
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命令式系统有几个问题。用户必须仔细规划和排序所有必要的命令以实现目标，这对用户来说是一项负担。这些计划通常很难审计或测试。在部署之前，时间或共享状态问题难以发现和测试。而且，正如大多数程序员会告诉你的，即使是微小的或无害的错误也可能彻底改变结果。
- en: Imagine you’re responsible for building and maintaining a system that comprises
    10, 100, or 1000 logical services, each with its own state, network attachment,
    and resource requirements. Now imagine that you are using raw containers to manage
    replicas of those services. It would be much more difficult to manage the raw
    containers than Docker services.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想象一下，你负责构建和维护一个包含10、100或1000个逻辑服务的系统，每个服务都有自己的状态、网络连接和资源需求。现在想象一下，你正在使用原始容器来管理这些服务的副本。与Docker服务相比，管理原始容器将困难得多。
- en: Docker services are declarative abstractions, as illustrated in [figure 11.6](#filepos1194613).
    When we create a service, we declare that we want a certain number of replicas
    of that service, and Docker takes care of the individual commands required to
    maintain them. Declarative tools enable users to describe the new state of a system,
    rather than the steps required to change from the current state to the new state.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker服务是声明式抽象，如图11.6所示。[图11.6](#filepos1194613)。当我们创建一个服务时，我们声明我们想要该服务的特定数量的副本，Docker负责维护它们的单个命令。声明式工具使用户能够描述系统的新的状态，而不是描述从当前状态到新状态的步骤。
- en: Figure 11.6\. Declarative processing loop
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图11.6. 声明式处理循环
- en: '![](images/00034.jpg)'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00034.jpg)'
- en: The Swarm orchestration system is a state reconciliation loop that continuously
    compares the declared state of the system that the user desires with the current
    state of the system. When it detects a difference, it uses a simple set of rules
    to change the system so that it matches the desired state.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm编排系统是一个状态协调循环，它持续比较用户期望的系统声明状态与系统的当前状态。当它检测到差异时，它使用一组简单的规则来改变系统，使其与期望状态相匹配。
- en: Declarative tools close the issues with the imperative pattern. Declarative
    interfaces simplify the system by imposing constraints on how the system operates.
    This enables the declared policy to be implemented by one or a few well-tested
    engines that can converge the system to the declared state. They are easier to
    write, audit, and comprehend. Declarative statements or documents are wonderfully
    paired with version-control systems because they allow us to effectively version-control
    the state of the systems that they describe.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 声明式工具解决了命令式模式的问题。声明式接口通过限制系统操作的方式简化了系统。这使得声明性策略可以通过一个或几个经过良好测试的引擎实现，这些引擎可以将系统收敛到声明状态。它们更容易编写、审计和理解。声明性语句或文档与版本控制系统完美搭配，因为它们允许我们有效地版本控制所描述的系统状态。
- en: Imperative and declarative tools are not in competition. You will almost never
    use one without the other. For example, when we create or update a Docker service,
    we are using an imperative tool to describe a change to the system. Issuing the
    `docker service create` command is imperative, but it creates a declarative abstraction.
    It rolls up a whole world of lower-level management commands for creating and
    removing containers, evaluating health, pulling images, managing service discovery,
    and network routing.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命令式和声明式工具并不相互竞争。你几乎永远不会单独使用其中之一。例如，当我们创建或更新Docker服务时，我们正在使用命令式工具来描述对系统的更改。发出`docker
    service create`命令是命令式的，但它创建了一个声明式抽象。它汇总了创建和删除容器、评估健康状态、拉取镜像、管理服务发现和网络路由等一系列低级管理命令。
- en: As you build more complex systems of services, volumes, networks, and configuration,
    the number of imperative commands required to achieve your goals will become a
    new burden. When that happens, it is time to adopt a higher-level declarative
    abstraction. In this case, that abstraction is a stack or complete environment
    as described with Docker Compose.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着你构建更复杂的系统，包括服务、卷、网络和配置，你需要达到目标所需的命令行数量将变成一个新的负担。当这种情况发生时，是时候采用更高层次的声明性抽象了。在这种情况下，这个抽象是一个栈或完整的环境，正如Docker
    Compose所描述的那样。
- en: 'When you need to model a whole environment of services, you should use a Docker
    stack. A stack describes collections of services, volumes, networks, and other
    configuration abstractions. The `docker` command line provides imperative commands
    for deploying, removing, and inspecting stacks. Stacks are created from a declarative
    description of an entire environment. These environments are described using the
    Docker Compose V3 file format. A Compose file that describes an environment with
    the `hello-world` service from earlier might look like the following:'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你需要模拟整个服务环境时，你应该使用Docker栈。栈描述了服务、卷、网络和其他配置抽象的集合。`docker`命令行提供了部署、删除和检查栈的命令行指令。栈是从整个环境的声明性描述中创建的。这些环境使用Docker
    Compose V3文件格式进行描述。一个描述了之前提到的`hello-world`服务的环境的Compose文件可能看起来像以下这样：
- en: '`version: "3.7" services:     hello-world:         image: dockerinaction/ch11_service_hw:v1
            ports:             - 8080:80         deploy:             replicas: 3`'
  id: totrans-107
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" services:   hello-world:     image: dockerinaction/ch11_service_hw:v1     ports:         -
    8080:80     deploy:         replicas: 3`'
- en: Compose files use Yet Another Markup Language (YAML). Not everyone is familiar
    with YAML, and that can be a hurdle to adopting several tools in this generation
    of infrastructure and workload management tooling. The good news is that people
    rarely use exotic YAML features. Most people stick to the basics.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Compose文件使用另一种标记语言（YAML）。并非每个人都熟悉YAML，这可能会成为采用这一代基础设施和工作负载管理工具中的一些工具的障碍。好消息是，人们很少使用YAML的特异功能。大多数人坚持使用基础功能。
- en: This chapter is not an exhaustive survey of Compose or the properties you will
    use to manage services. The official Docker documentation should serve that purpose.
    Expect to find every command-line feature mirrored in Compose. The next section
    is a brief primer on YAML and Compose files.
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章不是对Compose或你将用于管理服务的属性的全面调查。官方Docker文档应该起到这个作用。你可以在Compose中找到每个命令行功能的镜像。下一节是YAML和Compose文件的简要入门。
- en: 11.2.1\. A YAML primer
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.2.1. YAML入门
- en: YAML is used to describe structured documents, which are made up of structures,
    lists, maps, and scalar values. Those features are defined as continuous blocks,
    where substructures are defined with nested block definitions in a style that
    will feel familiar to most high-level language programmers.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: YAML用于描述结构化文档，这些文档由结构、列表、映射和标量值组成。这些功能定义为连续的块，其中子结构通过嵌套的块定义以大多数高级语言程序员熟悉的方式定义。
- en: The default scope of a YAML document is a single file or stream. YAML provides
    a mechanism to specify multiple documents in the same file, but Docker will use
    only the first document it encounters in a Compose file. The standard filename
    for Compose files is docker-compose.yml.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: YAML文档的默认作用域是一个单独的文件或流。YAML提供了一种机制来指定同一文件中的多个文档，但Docker将只使用它在Compose文件中遇到的第一个文档。Compose文件的标准文件名是docker-compose.yml。
- en: Comment support is one of the most popular reasons to adopt YAML instead of
    JSON today. A YAML document can include a comment at the end of any line. Comments
    are marked by a space followed by a hash sign ( `#`). Any characters that follow
    until the end of the line are ignored by the parser. Empty lines between elements
    have no impact on the document structure.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注释支持是今天选择YAML而不是JSON的最受欢迎的原因之一。YAML文档可以在任何行的末尾包含注释。注释由一个空格后跟一个井号（`#`）标记。解析器会忽略直到行尾的任何字符。元素之间的空行对文档结构没有影响。
- en: 'YAML uses three types of data and two styles of describing that data, block
    and flow. Flow collections are specified similarly to collection literals in JavaScript
    and other languages. For example, the following is a list of strings in the flow
    style:'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: YAML使用三种类型的数据和两种描述数据的方式，即块和流。流集合的指定方式与JavaScript和其他语言中的集合字面量类似。例如，以下是一个流风格中的字符串列表：
- en: '`["PersonA","PersonB"]`'
  id: totrans-115
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`["PersonA","PersonB"]`'
- en: The block style is more common and will be used in this primer except where
    noted. The three types of data are maps, lists, and scalar values.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 块样式更为常见，除了特别说明外，本指南将使用块样式。三种数据类型是映射、列表和标量值。
- en: 'Maps are defined by a set of unique properties in the form of key/value pairs
    that are delimited by a colon and space (`:` ). Whereas property names must be
    string values, property values can be any of the YAML data types except documents.
    A single structure cannot have multiple definitions for the same property. Consider
    this block style example:'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 映射由一组唯一的属性定义，这些属性以冒号和空格（`:` ）分隔的键/值对形式存在。而属性名必须是字符串值，属性值可以是 YAML 数据类型中的任何一种，但不能是文档。单个结构不能为同一属性有多个定义。考虑以下块样式示例：
- en: '`image: "alpine" command: echo hello world`'
  id: totrans-118
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`image: "alpine" command: echo hello world`'
- en: 'This document contains a single map with two properties: `image` and `command`.
    The `image` property has a scalar string value, `"alpine"`. The `command` property
    has a scalar string value, `echo hello world`. A scalar is a single value. The
    preceding example demonstrates two of the three flow scalar styles.'
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此文档包含一个具有两个属性的单个映射：`image` 和 `command`。`image` 属性有一个标量字符串值，`"alpine"`。`command`
    属性有一个标量字符串值，`echo hello world`。标量是一个单一值。前面的示例演示了三种流标量样式中的两种。
- en: The value for `image` in the preceding example is specified in double-quote
    style, which is capable of expressing arbitrary strings, by using `\` escape sequences.
    Most programmers are familiar with this string style.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前面的示例中 `image` 的值以双引号样式指定，这种样式能够通过使用 `\` 转义序列表达任意字符串。大多数程序员都熟悉这种字符串样式。
- en: The value for the command is written in plain style. The plain (unquoted) style
    has no identifying indicators and provides no form of escaping. It is therefore
    the most readable, most limited, and most context-sensitive style. A bunch of
    rules are used for plain style scalars. Plain scalars
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命令的值以纯样式编写。纯样式（未引用）没有标识符，也不提供任何形式的转义。因此，它是最易读的、最有限的和最上下文相关的样式。对于纯样式标量使用了一组规则。纯样式标量
- en: Must not be empty
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须不为空
- en: Must not contain leading or trailing whitespace characters
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须不包含前导或尾随空白字符
- en: Must not begin with an indicator character (for example, `-` or `:`) in places
    where doing so would cause an ambiguity
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应在会造成歧义的地方以指示字符（例如，`-` 或 `:`）开头
- en: Must never contain character combinations using a colon (`:`) and hash sign
    (`#`)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须不得包含使用冒号（`:`）和井号（`#`）的字符组合
- en: 'Lists (or block sequences) are series of nodes in which each element is denoted
    by a leading hyphen (`-`) indicator. For example:'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 列表（或块序列）是一系列节点，每个元素由一个前导连字符（`-`）指示。例如：
- en: '`- item 1 - item 2 - item 3 - # an empty item - item 4`'
  id: totrans-127
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`- item 1 - item 2 - item 3 - # 一个空项 - item 4`'
- en: 'Finally, YAML uses indentation to indicate content scope. Scope determines
    which block each element belongs to. There are a few rules:'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最后，YAML 使用缩进来表示内容作用域。作用域决定了每个元素属于哪个块。有一些规则：
- en: Only spaces can be used for indentation.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只能使用空格进行缩进。
- en: The amount of indentation does not matter as long as
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩进量不重要，只要
- en: All peer elements (in the same scope) have the same amount of indentation.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有同级元素（在同一作用域内）必须有相同的缩进量。
- en: Any child elements are further indented.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何子元素都进一步缩进。
- en: 'These documents are equivalent:'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些文档是等效的：
- en: '`top-level:    second-level:          # three spaces      third-level:        
    # two more spaces       - "list item"       # single additional indent on items
    in this list      another-third-level: # a third-level peer with the same two
    spaces            fourth-level: "string scalar" # 6 more spaces    another-second-level: 
    # a 2nd level peer with three spaces                - a list item # list items
    in this scope have                              # 15 total leading spaces               
    - a peer item # A peer list item with a gap in the list --- # every scope level
    adds exactly 1 space top-level: second-level:   third-level:    - "list item"
      another-third-level:    fourth-level: "string scalar" another-second-level:
      - a list item   - a peer item`'
  id: totrans-134
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`top-level:   second-level:          # 三空格   third-level:         # 再多两个空格  
    - "list item"       # 此列表项额外缩进一个空格   another-third-level: # 与相同两个空格的第三级同级   fourth-level:
    "string scalar" # 再多六个空格   another-second-level:   # 有三个空格的二级同级   - a list item  
    - a peer item # 此范围内的列表项有   # 15 个总前导空格   - a peer item # 列表中有空格的同级列表项 --- # 每个作用域级别增加正好一个空格  
    top-level: second-level:   third-level:   - "list item"   another-third-level:  
    fourth-level: "string scalar"   another-second-level:   - a list item   - a peer
    item`'
- en: The full YAML 1.2 specification is available at [http://yaml.org/spec/1.2/2009-07-21/spec.html](http://yaml.org/spec/1.2/2009-07-21/spec.html)
    and is quite readable. Armed with a basic understanding of YAML, you’re ready
    to jump into basic environment modeling with Compose.
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: YAML 1.2 的完整规范可在 [http://yaml.org/spec/1.2/2009-07-21/spec.html](http://yaml.org/spec/1.2/2009-07-21/spec.html)
    找到，并且相当易于阅读。掌握了 YAML 的基本理解，你就可以开始使用 Compose 进行基本的环境建模了。
- en: 11.2.2\. Collections of services with Compose V3
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.2.2\. 使用 Compose V3 的服务集合
- en: 'Compose files describe every first-class Docker resource type: services, volumes,
    networks, secrets, and configs. Consider a collection of three services: a PostgreSQL
    database, a MariaDB database, and a web administrative interface for managing
    those databases. You might represent those services with this Compose file:'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Compose 文件描述了每个 Docker 一级资源类型：服务、卷、网络、机密和配置。考虑一个包含三个服务的集合：一个 PostgreSQL 数据库、一个
    MariaDB 数据库以及用于管理这些数据库的 Web 管理界面。你可能可以用以下 Compose 文件来表示这些服务：
- en: '`version: "3.7" services:     postgres:         image: dockerinaction/postgres:11-alpine
            environment:             POSTGRES_PASSWORD: example      mariadb:        
    image: dockerinaction/mariadb:10-bionic         environment:             MYSQL_ROOT_PASSWORD:
    example      adminer:         image: dockerinaction/adminer:4         ports:            
    - 8080:8080`'
  id: totrans-138
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" services:   postgres:     image: dockerinaction/postgres:11-alpine     environment:       POSTGRES_PASSWORD:
    example   mariadb:     image: dockerinaction/mariadb:10-bionic     environment:       MYSQL_ROOT_PASSWORD:
    example   adminer:     image: dockerinaction/adminer:4     ports:       - 8080:8080`'
- en: 'Keep in mind that this document is in YAML, and all of the properties with
    the same indentation belong to the same map. This Compose file has two top-level
    properties: `version` and `services`. The `version` property tells the Compose
    interpreter which fields and structure to anticipate. The `services` property
    is a map of service names to service definitions. Each service definition is a
    map of properties.'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请记住，这份文档是 YAML 格式的，所有具有相同缩进的属性都属于同一个映射。这个 Compose 文件有两个顶级属性：`version` 和 `services`。`version`
    属性告诉 Compose 解释器预期哪些字段和结构。`services` 属性是一个服务名称到服务定义的映射。每个服务定义都是一个属性映射。
- en: 'In this case, the `services` map has three entries with keys: `postgres`, `mariadb`,
    and `adminer`. Each of those entries defines a service by using a small set of
    service properties such as `image`, `environment`, or `ports`. Declarative documents
    make it simple to concretely specify a full service definition. Doing so will
    reduce implicit dependencies on default values and reduce the educational overhead
    for your fellow team members. Omitting a property will use the default values
    (just as when using the command-line interface). These services each define the
    container `image`. The `postgres` and `mariadb` services specify `environment`
    variables. The `adminer` service uses `ports` to route requests to port 8080 on
    the host to port 8080 in the service container.'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这种情况下，`services` 映射有三个条目，键为：`postgres`、`mariadb` 和 `adminer`。每个条目都通过使用一组小的服务属性（如
    `image`、`environment` 或 `ports`）定义了一个服务。声明性文档使得具体指定完整的服务定义变得简单。这样做将减少对默认值的隐式依赖，并减少对团队成员的教育负担。省略属性将使用默认值（就像使用命令行界面一样）。这些服务各自定义了容器的
    `image`。`postgres` 和 `mariadb` 服务指定了 `environment` 变量。`adminer` 服务使用 `ports` 将请求路由到主机上的端口
    8080，到服务容器中的端口 8080。
- en: Creating and updating a stack
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建和更新堆栈
- en: 'Now use this Compose file to create a stack. Remember, a Docker stack is a
    named collection of services, volumes, networks, secrets, and configs. The `docker
    stack` subcommands manage stacks. Create a new file called databases.yml in an
    empty directory. Edit the file and add the preceding Compose file contents. Create
    a new stack and deploy the services it describes by using the following command:'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在请使用这个 Compose 文件创建一个堆栈。记住，Docker 堆栈是一组命名服务、卷、网络、机密和配置的集合。`docker stack` 子命令管理堆栈。在一个空目录中创建一个名为
    databases.yml 的新文件。编辑该文件并添加前面的 Compose 文件内容。通过以下命令创建一个新的堆栈并部署其描述的服务：
- en: '`docker stack deploy -c databases.yml my-databases`'
  id: totrans-143
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy -c databases.yml my-databases`'
- en: 'When you run this command, Docker will display output like this:'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你运行这个命令时，Docker将显示如下输出：
- en: '`Creating network my-databases_default Creating service my-databases_postgres
    Creating service my-databases_mariadb Creating service my-databases_adminer`'
  id: totrans-145
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Creating network my-databases_default Creating service my-databases_postgres
    Creating service my-databases_mariadb Creating service my-databases_adminer`'
- en: At this point, you can test the services by using your browser to navigate to
    http://localhost:8080\. Services are discoverable by their names within a Docker
    network. You’ll need to keep that in mind when you use the `adminer` interface
    to connect to your `postgres` and `mariadb` services. The `docker stack deploy`
    subcommand is used to create and update stacks. It always takes the Compose file
    that represents the desired state of the stack. Whenever the Compose file that
    you use differs from the definitions used in the current stack, Docker will determine
    how the two differ and make the appropriate changes.
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 到此为止，您可以通过使用浏览器导航到 http://localhost:8080 来测试服务。服务在其 Docker 网络中的名称是可发现的。当您使用
    `adminer` 接口连接到您的 `postgres` 和 `mariadb` 服务时，请记住这一点。`docker stack deploy` 子命令用于创建和更新栈。它始终需要表示栈所需状态的
    Compose 文件。每当您使用的 Compose 文件与当前栈中使用的定义不同时，Docker 将确定这两个定义之间的差异，并做出相应的更改。
- en: 'You can try this yourself. Tell Docker to create three replicas of the `adminer`
    service. Specify the `replicas` property under the `deploy` property of the `adminer`
    service. Your databases.yml file should look like this:'
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以亲自尝试。告诉 Docker 创建 `adminer` 服务的三个副本。在 `adminer` 服务的 `deploy` 属性下指定 `replicas`
    属性。您的 databases.yml 文件应该看起来像这样：
- en: '`version: "3.7" services:     postgres:         image: dockerinaction/postgres:11-alpine
            environment:             POSTGRES_PASSWORD: example      mariadb:        
    image: dockerinaction/mariadb:10-bionic         environment:             MYSQL_ROOT_PASSWORD:
    example      adminer:         image: dockerinaction/adminer:4         ports:            
    - 8080:8080         deploy:             replicas: 3`'
  id: totrans-148
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" services:   postgres:     image: dockerinaction/postgres:11-alpine     environment:         POSTGRES_PASSWORD:
    example   mariadb:     image: dockerinaction/mariadb:10-bionic     environment:         MYSQL_ROOT_PASSWORD:
    example   adminer:     image: dockerinaction/adminer:4     ports:         - 8080:8080     deploy:         replicas:
    3`'
- en: 'After you’ve updated your Compose file, repeat the earlier `docker stack deploy`
    command:'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在您更新了 Compose 文件之后，请重复之前的 `docker stack deploy` 命令：
- en: '`docker stack deploy -c databases.yml my-databases`'
  id: totrans-150
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy -c databases.yml my-databases`'
- en: 'This time, the command will display a message noting that services are being
    updated, not created:'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这次，命令将显示一条消息，指出服务正在更新，而不是创建：
- en: '`Updating service my-databases_mariadb (id: lpvun5ncnleb6mhqj8bbphsf6) Updating
    service my-databases_adminer (id: i2gatqudz9pdsaoux7auaiicm) Updating service
    my-databases_postgres (id: eejvkaqgbbl35glatt977m65a)`'
  id: totrans-152
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Updating service my-databases_mariadb (id: lpvun5ncnleb6mhqj8bbphsf6) Updating
    service my-databases_adminer (id: i2gatqudz9pdsaoux7auaiicm) Updating service
    my-databases_postgres (id: eejvkaqgbbl35glatt977m65a)`'
- en: 'The message appears to indicate that all of the services are being changed.
    But that is not the case. You can use `docker stack ps` to list all of the tasks
    and their ages:'
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 消息似乎表明所有服务都在被更改。但实际上并非如此。您可以使用 `docker stack ps` 列出所有任务及其年龄：
- en: '`docker stack ps \   --format ''{{.Name}}\t{{.CurrentState}}'' \   my-databases`
    `1`'
  id: totrans-154
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack ps   --format ''{{.Name}}\t{{.CurrentState}}''   my-databases`
    `1`'
- en: 1 Specifies which stack to list
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 指定要列出的栈
- en: 'This command should filter for the interesting columns and report something
    like the following:'
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此命令应筛选出有趣的列并报告如下：
- en: '`my-databases_mariadb.1  Running 3 minutes ago my-databases_postgres.1 Running
    3 minutes ago my-databases_adminer.1  Running 3 minutes ago  my-databases_adminer.2 
    Running about a minute ago my-databases_adminer.3  Running about a minute ago`'
  id: totrans-157
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`my-databases_mariadb.1  Running 3 minutes ago my-databases_postgres.1  Running
    3 minutes ago my-databases_adminer.1  Running 3 minutes ago  my-databases_adminer.2 
    Running about a minute ago my-databases_adminer.3  Running about a minute ago`'
- en: This view hints that none of the original service containers were touched during
    the new deployment. The only new tasks are those additional replicas of the `adminer`
    service that were required to bring the system into the state described by the
    current version of the databases.yml file. You should note that the `adminer`
    service really doesn’t work well when multiple replicas are running. We’re using
    it here for illustrative purposes only.
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此视图暗示在新部署过程中没有触及到任何原始服务容器。唯一的新任务是那些根据当前版本的 databases.yml 文件描述的系统状态所需的 `adminer`
    服务的额外副本。您应该注意，当多个副本运行时，`adminer` 服务实际上并不工作得很好。我们在这里仅使用它进行说明。
- en: Scaling down and removing services
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 缩小规模和删除服务
- en: 'When you’re using Docker and Compose, you never need to tear down or otherwise
    remove a whole stack when you’re redeploying or making changes. Let Docker figure
    it out and handle the change for you. Only one case is tricky to deal with when
    you’re working with declarative representations such as `Compose: deletion`.'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '当你使用 Docker 和 Compose 时，在重新部署或进行更改时，你永远不需要拆解或以其他方式移除整个堆栈。让 Docker 来处理并为你处理更改。当你处理如
    `Compose: deletion` 这样的声明性表示时，只有一个情况比较难以处理。'
- en: Docker will delete service replicas automatically when you scale down. It always
    chooses the highest numbered replicas for removal. For example, if you have three
    replicas of the `adminer` service running, they will be named `my-databases_adminer.1`,
    `my-databases_adminer.2`, and `my-databases_adminer.3`. If you scale down to two
    replicas, Docker will delete the replica named `my-databases_adminer.3`. Things
    get weird when you try to delete a whole service.
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你缩小服务规模时，Docker 会自动删除服务副本。它总是选择编号最高的副本进行删除。例如，如果你有 `adminer` 服务的三个副本正在运行，它们将被命名为
    `my-databases_adminer.1`、`my-databases_adminer.2` 和 `my-databases_adminer.3`。如果你将规模缩小到两个副本，Docker
    将删除名为 `my-databases_adminer.3` 的副本。当你尝试删除整个服务时，事情会变得奇怪。
- en: 'Edit the databases.yml file to delete the `mariadb` service definition and
    set the `adminer` service to two replicas. The file should look like this:'
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 编辑 databases.yml 文件以删除 `mariadb` 服务定义并将 `adminer` 服务设置为两个副本。文件应如下所示：
- en: '`version: "3.7" services:     postgres:         image: dockerinaction/postgres:11-alpine
            environment:             POSTGRES_PASSWORD: example      adminer:        
    image: dockerinaction/adminer:4         ports:             - 8080:8080        
    deploy:             replicas: 2`'
  id: totrans-163
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" services:   postgres:     image: dockerinaction/postgres:11-alpine     environment:       POSTGRES_PASSWORD:
    example   adminer:     image: dockerinaction/adminer:4     ports:       - 8080:8080     deploy:       replicas:
    2`'
- en: 'Now when you run `docker stack deploy -c databases.yml my-databases`, the command
    will generate output like this:'
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在当你运行 `docker stack deploy -c databases.yml my-databases` 命令时，该命令将生成如下输出：
- en: '`Updating service my-databases_postgres (id: lpvun5ncnleb6mhqj8bbphsf6) Updating
    service my-databases_adminer  (id: i2gatqudz9pdsaoux7auaiicm)`'
  id: totrans-165
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Updating service my-databases_postgres (id: lpvun5ncnleb6mhqj8bbphsf6) Updating
    service my-databases_adminer  (id: i2gatqudz9pdsaoux7auaiicm)`'
- en: 'The Compose file you provided to the `stack deploy` command did not have any
    reference to the `mariadb` service, so Docker did not make any changes to that
    service. When you list the tasks in your stack again, you’ll notice that the `mariadb`
    service is still running:'
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你提供给 `stack deploy` 命令的 Compose 文件没有对 `mariadb` 服务进行任何引用，因此 Docker 没有对该服务进行任何更改。当你再次列出堆栈中的任务时，你会注意到
    `mariadb` 服务仍在运行：
- en: '`docker stack ps \   --format ''{{.Name}}\t{{.CurrentState}}'' \   my-databases`
    `1`'
  id: totrans-167
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack ps \   --format ''{{.Name}}\t{{.CurrentState}}'' \   my-databases`
    `1`'
- en: 1 Specifies which stack to list
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 指定要列出哪个堆栈
- en: 'Running this command results in this output:'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行此命令将产生以下输出：
- en: '`my-databases_mariadb.1  Running 7 minutes ago my-databases_postgres.1 Running
    7 minutes ago my-databases_adminer.1  Running 7 minutes ago my-databases_adminer.2 
    Running 5 minutes ago`'
  id: totrans-170
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`my-databases_mariadb.1  Running 7 minutes ago my-databases_postgres.1 Running
    7 minutes ago my-databases_adminer.1  Running 7 minutes ago my-databases_adminer.2 
    Running 5 minutes ago`'
- en: You can see that the third replica of the `adminer` service has been removed,
    but the `mariadb` service is still running. This works as intended. Docker stacks
    can be created and managed with several Compose files. But doing so creates several
    error opportunities and is not recommended. There are two ways to delete services
    or other objects.
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以看到 `adminer` 服务的第三个副本已被移除，但 `mariadb` 服务仍在运行。这正如预期的那样。可以使用多个 Compose 文件创建和管理
    Docker 堆栈。但这样做会创建多个错误机会，并且不推荐这样做。删除服务或其他对象有两种方式。
- en: 'You can manually remove a service by using `docker service remove`. This works,
    but does not get any of the benefits of working with declarative representations.
    If this change is made manually and not reflected in your Compose files, the next
    `docker stack deploy` operation will create the service again. The cleanest way
    to remove services in a stack is by removing the service definition from your
    Compose file and then executing `docker stack deploy` with the `--prune` flag.
    Without further altering your databases.yml file, run the following:'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以使用 `docker service remove` 手动删除一个服务。这可以工作，但不会获得与声明性表示一起工作的任何好处。如果此更改是手动进行的，并且没有反映在你的
    Compose 文件中，则下一次 `docker stack deploy` 操作将再次创建该服务。在堆栈中删除服务的最干净的方法是从你的 Compose
    文件中删除服务定义，然后使用 `--prune` 标志执行 `docker stack deploy`。在不进一步更改你的 databases.yml 文件的情况下，运行以下命令：
- en: '`docker stack deploy \   -c databases.yml \   --prune \   my-databases`'
  id: totrans-173
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy \   -c databases.yml \   --prune \   my-databases`'
- en: 'This command will report that the services described by databases.yml have
    been updated, but will also report that `my-databases_mariadb` has been removed.
    When you list the tasks again, you will see that this is the case:'
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此命令将报告由 databases.yml 描述的服务已更新，但也会报告 `my-databases_mariadb` 已被删除。当您再次列出任务时，您将看到这种情况：
- en: '`my-databases_postgres.1 Running 8 minutes ago my-databases_adminer.1  Running
    8 minutes ago my-databases_adminer.2  Running 6 minutes ago`'
  id: totrans-175
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`my-databases_postgres.1 运行8分钟前 my-databases_adminer.1  运行8分钟前 my-databases_adminer.2 
    运行6分钟前`'
- en: The `--prune` flag will clean up any resource in the stack that isn’t explicitly
    referenced in the Compose file used for the deploy operation. For that reason,
    it is important to keep a Compose file that represents the entire environment.
    Otherwise, you might accidentally delete absent services or volumes, networks,
    secrets, and configs.
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`--prune` 标志将清理堆栈中任何在用于部署操作的 Compose 文件中未明确引用的资源。因此，保留一个代表整个环境的 Compose 文件非常重要。否则，您可能会意外删除不存在的服务或卷、网络、机密和配置。'
- en: 11.3\. STATEFUL SERVICES AND PRESERVING DATA
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.3. 状态化服务与数据保留
- en: The Docker stack that you’ve been working with includes a database service.
    In [chapter 4](index_split_037.html#filepos379268), you learned how to use volumes
    to separate a container life cycle from the life cycle for the data it uses. This
    is especially important for databases. As specified earlier, the stack will create
    a new volume for the `postgres` service each time the container is replaced (for
    whatever reason), and a new volume will be created for each replica. This would
    cause problems in a real-world system for which stored data is an important part
    of service identity.
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您一直在使用的 Docker 网络包括一个数据库服务。在 [第4章](index_split_037.html#filepos379268) 中，您学习了如何使用卷将容器的生命周期与其使用的数据的生命周期分开。这对于数据库来说尤为重要。如前所述，每次容器被替换（无论出于何种原因）时，堆栈将为
    `postgres` 服务创建一个新的卷，并为每个副本创建一个新的卷。这将在实际系统中引起问题，因为存储的数据是服务身份的重要组成部分。
- en: The best way to begin addressing this problem is by modeling volumes in Compose.
    Compose files use another top-level property named `volumes`. Like `services`,
    `volumes` is a map of volume definitions; the key is the name of the volume, and
    the value is a structure defining the volume properties. You do not need to specify
    values for every volume property. Docker will use defaults for omitted property
    values. The top-level property defines the volumes that can be used by services
    within the file. Using a volume in a service requires that you specify the dependency
    from the service that needs it.
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解决这个问题的最佳方式是通过在 Compose 中建模卷。Compose 文件使用另一个顶级属性名为 `volumes`。与 `services` 类似，`volumes`
    是一个卷定义的映射；键是卷的名称，值是定义卷属性的结构的值。您不需要为每个卷属性指定值。Docker 将使用默认值来处理省略的属性值。顶级属性定义了文件内服务可以使用的卷。在服务中使用卷需要您指定需要它的服务的依赖关系。
- en: 'A Compose service definition can include a `volumes` property. That property
    is a list of short or long volume specifications. Those correspond to the volumes
    and mount syntax supported by the Docker command line, respectively. We’ll use
    the long form to enhance databases.yml and add a volume to store the `postgres`
    data:'
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Compose 服务定义可以包括一个 `volumes` 属性。该属性是短或长卷规范的列表。它们分别对应于 Docker 命令行支持的卷和挂载语法。我们将使用长格式来增强
    databases.yml 并添加一个用于存储 `postgres` 数据的卷：
- en: '`version: "3.7" volumes:     pgdata: # empty definition uses volume defaults
    services:     postgres:         image: dockerinaction/postgres:11-alpine        
    volumes:             - type: volume               source: pgdata # The named volume
    above               target: /var/lib/postgresql/data         environment:            
    POSTGRES_PASSWORD: example     adminer:         image: dockerinaction/adminer:4
            ports:             - 8080:8080         deploy:             replicas: 1
    # Scale down to 1 replica so you can test`'
  id: totrans-181
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" volumes:     pgdata: # 空定义使用卷默认值 services:     postgres:        
    image: dockerinaction/postgres:11-alpine         volumes:             - type:
    volume               source: pgdata # 上面的命名卷               target: /var/lib/postgresql/data
            environment:             POSTGRES_PASSWORD: example     adminer:        
    image: dockerinaction/adminer:4         ports:             - 8080:8080        
    deploy:             replicas: 1 # 缩小到1个副本以便测试`'
- en: 'In this example, the file defines a volume named `pgdata`, and the `postgres`
    service mounts that volume at `/var/lib/postgresql/data`. That location is where
    the PostgreSQL software will store any database schema or data. Deploy the stack
    and inspect the results:'
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个例子中，该文件定义了一个名为`pgdata`的卷，`postgres`服务将此卷挂载到`/var/lib/postgresql/data`。该位置是PostgreSQL软件将存储任何数据库模式或数据的地方。部署栈并检查结果：
- en: '`docker stack deploy \   -c databases.yml \   --prune \   my-databases`'
  id: totrans-183
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy \ -c databases.yml \ --prune \ my-databases`'
- en: 'Run `docker volume ls` after the changes have been applied to verify that the
    operation was successful:'
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在应用更改后运行`docker volume ls`以验证操作是否成功：
- en: '`DRIVER        VOLUME NAME local         my-databases_pgdata`'
  id: totrans-185
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`DRIVER        VOLUME NAME local         my-databases_pgdata`'
- en: The name of the stack prefixes any resources created for it such as services
    or volumes. In this case, Docker created the volume you named `pgdata` with the
    prefix `my-databases`. You could spend time inspecting the service configuration
    or container further, but it is more interesting to perform a functional test.
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 栈的名称作为任何为其创建的资源的前缀，例如服务或卷。在这种情况下，Docker使用前缀`my-databases`创建了名为`pgdata`的卷。你可以花时间检查服务配置或容器，但进行功能测试更有趣。
- en: 'Open http://localhost:8080 and use the `adminer` interface to manage the `postgres`
    database. Select the `postgresql` driver, use `postgres` as the hostname, `postgres`
    as the username, and `example` as the password. After you’ve logged in, create
    a few tables or insert some data. When you’re done, remove the `postgres` service:'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 打开http://localhost:8080，并使用`adminer`界面管理`postgres`数据库。选择`postgresql`驱动程序，使用`postgres`作为主机名，`postgres`作为用户名，`example`作为密码。登录后，创建一些表或插入一些数据。完成后，删除`postgres`服务：
- en: '`docker service remove my-databases_postgres`'
  id: totrans-188
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service remove my-databases_postgres`'
- en: 'Then restore the service by using the Compose file:'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后使用Compose文件恢复服务：
- en: '`docker stack deploy \   -c databases.yml \   --prune \   my-databases`'
  id: totrans-190
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy \ -c databases.yml \ --prune \ my-databases`'
- en: Because the data is stored in a volume, Docker is able to attach the new database
    replica to the original `pg-data` volume. If the data wasn’t stored in the volume
    and existed in only the original replica, it would have been lost when the service
    was removed. Log into the database by using the `adminer` interface again (remember
    that the username is `postgres`, and password is `example` as specified in the
    Compose file). Examine the database and look for the changes you made. If you’ve
    followed these steps correctly, those changes and the data will be available.
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因为数据存储在卷中，Docker能够将新的数据库副本附加到原始的`pg-data`卷上。如果数据没有存储在卷中，而只存在于原始副本中，那么在服务被移除时，数据就会丢失。再次通过`adminer`界面登录数据库（记住用户名是`postgres`，密码是`example`，如Compose文件中指定）。检查数据库并查找你做出的更改。如果你正确地遵循了这些步骤，那些更改和数据将会可用。
- en: This example uses two levels of naming indirection that make it a bit complicated
    to follow. Your browser is pointed at localhost, which loads the `adminer` service,
    but you’re telling `adminer` to access the database by using the `postgres` hostname.
    The next section explains that indirection, and describes the built-in Docker
    network enhancements and how to use them with services.
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个例子使用了两个级别的命名间接，使得它有点难以理解。你的浏览器指向localhost，加载`adminer`服务，但你告诉`adminer`通过`postgres`主机名访问数据库。下一节将解释这种间接性，并描述内置的Docker网络增强功能以及如何与服务一起使用它们。
- en: 11.4\. LOAD BALANCING, SERVICE DISCOVERY, AND NETWORKS WITH COMPOSE
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11.4. 使用COMPOSE进行负载均衡、服务发现和网络
- en: In accessing the `adminer` interface from a web browser, you’re accessing the
    published port on the `adminer` service. Port publishing for a service is different
    from publishing a port on a container. Whereas containers directly map the port
    on the host interface to an interface for a specific container, services might
    be made up of many replica containers.
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在通过网页浏览器访问`adminer`界面时，你正在访问`adminer`服务上发布的端口。服务的端口发布与在容器上发布端口不同。容器直接将主机接口上的端口映射到特定容器的接口，而服务可能由许多副本容器组成。
- en: Container network DNS faces a similar challenge. When you’re resolving the network
    address of a named container, you’ll get the address of that container. But services
    might have replicas.
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 容器网络DNS面临类似的挑战。当你解析具有名称的容器的网络地址时，你会得到该容器的地址。但服务可能有副本。
- en: Docker accommodates services by creating virtual IP (VIP) addresses and balancing
    requests for a specific service between all of the associated replicas. When a
    program attached to a Docker network looks up the name of another service attached
    to that network, Docker’s built-in DNS resolver will respond with the virtual
    IP for that service’s location on the network. [Figure 11.7](#filepos1242103)
    illustrates the logical flow of service name and IP resolution.
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker通过创建虚拟IP（VIP）地址并在所有相关副本之间平衡特定服务的请求来适应服务。当连接到Docker网络的应用程序查找连接到该网络的另一个服务的名称时，Docker的内置DNS解析器将响应该服务在网络上的虚拟IP。![图11.7](#filepos1242103)说明了服务名称和IP解析的逻辑流程。
- en: Figure 11.7\. Docker network topology, service virtual IP addresses, and load
    balancing
  id: totrans-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图11.7\. Docker网络拓扑，服务虚拟IP地址和负载均衡
- en: '![](images/00053.jpg)'
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00053.jpg)'
- en: Likewise, when a request comes into the host interface for a published port
    or from an internal service, it will be routed to the target service’s virtual
    IP address. From there, it is forwarded to one of the service replicas. This means
    that there are a few more things to understand about Docker networks. When you’re
    using services, you’re using at least two Docker networks.
  id: totrans-199
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同样，当请求进入主机接口以发布端口或来自内部服务时，它将被路由到目标服务的虚拟IP地址。从那里，它被转发到服务副本之一。这意味着还有更多关于Docker网络的知识需要理解。当你使用服务时，你至少在使用两个Docker网络。
- en: 'The first network is named `ingress` and handles all port forwarding from the
    host interface to services. It is created when you initialize Docker in swarm
    mode. In this stack, there is only one service with forwarded ports, `adminer`.
    You can plainly see the associated interface in the `ingress` network upon inspection:'
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一个网络命名为`ingress`，处理从主机接口到服务的所有端口转发。当你以集群模式初始化Docker时，它会创建这个网络。在这个堆栈中，只有一个服务具有转发端口，即`adminer`。检查`ingress`网络时，你可以清楚地看到相关的接口：
- en: '`"Containers": {   "6f64f8aec8c2...": {     "Name": "my-databases_adminer.1.leijm5mpoz8o3lf4yxd7khnqn",
        "EndpointID": "9401eca40941...",     "MacAddress": "02:42:0a:ff:00:22",    
    "IPv4Address": "10.255.0.34/16",     "IPv6Address": ""   },   "ingress-sbox":
    {     "Name": "ingress-endpoint",     "EndpointID": "36c9b1b2d807...",     "MacAddress":
    "02:42:0a:ff:00:02",     "IPv4Address": "10.255.0.2/16",     "IPv6Address": ""
      } }`'
  id: totrans-201
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"容器": { "6f64f8aec8c2...": { "名称": "my-databases_adminer.1.leijm5mpoz8o3lf4yxd7khnqn",
    "端点ID": "9401eca40941...", "MAC地址": "02:42:0a:ff:00:22", "IPv4地址": "10.255.0.34/16",
    "IPv6地址": "" }, "ingress-sbox": { "名称": "ingress-endpoint", "端点ID": "36c9b1b2d807...",
    "MAC地址": "02:42:0a:ff:00:02", "IPv4地址": "10.255.0.2/16", "IPv6地址": "" } }`'
- en: Every service that uses port forwarding will have an interface in this network;
    `ingress` is critical to the Docker service functionality.
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个使用端口转发的服务都将在这个网络中有一个接口；`ingress`对于Docker服务功能至关重要。
- en: 'The second network is shared between all of the services in your stack. The
    Compose file you used to create the `my-databases` stack does not define any networks,
    but if you watch closely during initial deployment, you’ll see that Docker will
    create a network for your stack named `default`. All services in your stack will
    be attached to this network by default, and all service-to-service communication
    will go through this network. When you inspect that network, you will see three
    entries like the following:'
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第二个网络在堆栈中的所有服务之间共享。你用来创建`my-databases`堆栈的Compose文件没有定义任何网络，但如果你在初始部署期间仔细观察，你会看到Docker会为你的堆栈创建一个名为`default`的网络。默认情况下，堆栈中的所有服务都将连接到这个网络，并且所有服务之间的通信将通过这个网络进行。当你检查这个网络时，你会看到如下三个条目：
- en: '`"Containers": {   "3e57151c76bd...": {     "Name": "my-databases_postgres.1.44phu5vee7lbu3r3ao38ffqfu",
        "EndpointID": "375fe1bf3bc8...",     "MacAddress": "02:42:0a:00:05:0e",    
    "IPv4Address": "10.0.5.14/24",     "IPv6Address": ""   },   "6f64f8aec8c2...":
    {     "Name": "my-databases_adminer.1.leijm5mpoz8o3lf4yxd7khnqn",     "EndpointID":
    "7009ae008702...",     "MacAddress": "02:42:0a:00:05:0f",     "IPv4Address": "10.0.5.15/24",
        "IPv6Address": ""   },   "lb-my-databases_default": {     "Name": "my-databases_default-endpoint",
        "EndpointID": "8b94baa16c94...",     "MacAddress": "02:42:0a:00:05:04",    
    "IPv4Address": "10.0.5.4/24",     "IPv6Address": ""   } }`'
  id: totrans-204
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"Containers": {   '
- en: 'The top two interfaces are used by the individual containers running the `postgres`
    and `adminer` services. If you were to resolve the `postgres` service name to
    an IP address, you might expect the address 10.0.5.14 in response. But what you
    would get is something else not listed here. The address listed here is the container
    address, or the address that the internal load balancer would forward requests
    onto. The address you would get is listed under endpoints in the `postgres` service
    spec. When you run `docker service inspect my-databases_postgres`, part of the
    result will look like this:'
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 顶部两个接口由运行 `postgres` 和 `adminer` 服务的单个容器使用。如果您将 `postgres` 服务名称解析为 IP 地址，您可能会期望得到
    10.0.5.14 的地址。但您会得到这里未列出的其他地址。这里列出的地址是容器地址，或内部负载均衡器将请求转发到的地址。您会得到的地址在 `postgres`
    服务规范中的端点下列出。当您运行 `docker service inspect my-databases_postgres` 时，部分结果将类似于以下内容：
- en: '`"Endpoint": {   "Spec": {     "Mode": "vip"   },   "VirtualIPs": [     {      
    "NetworkID": "2wvn2x73bx55lrr0w08xk5am9",       "Addr": "10.0.5.11/24"     }  
    ] }`'
  id: totrans-206
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"Endpoint": {   '
- en: That virtual IP address is handled by the Docker internal load balancer. Connections
    to that address will be forwarded to one of the `postgres` service replicas.
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那个虚拟 IP 地址由 Docker 内部负载均衡器处理。连接到该地址的连接将被转发到 `postgres` 服务的一个副本。
- en: 'You can change service network attachment or the networks Docker creates for
    a stack with Compose. Compose can create networks with specific names, types,
    driver options, or other properties. Working with networks is similar to volumes.
    There are two parts: a top-level `networks` property that includes network definitions,
    and a `networks` property of services, where you describe attachments. Consider
    this final example:'
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以使用 Compose 更改服务网络连接或 Docker 为堆栈创建的网络。Compose 可以创建具有特定名称、类型、驱动程序选项或其他属性的网络。与网络一起工作类似于卷。它分为两部分：一个包含网络定义的顶级
    `networks` 属性，以及服务的 `networks` 属性，其中您描述连接。考虑以下最终示例：
- en: '`version: "3.7" networks:     foo:         driver: overlay volumes:     pgdata:
    # empty definition uses volume defaults services:     postgres:         image:
    dockerinaction/postgres:11-alpine         volumes:             - type: volume
                  source: pgdata # The named volume above               target: /var/lib/postgresql/data
            networks:             - foo         environment:             POSTGRES_PASSWORD:
    example     adminer:         image: dockerinaction/adminer:4         networks:
                - foo         ports:             - 8080:8080         deploy:            
    replicas: 1`'
  id: totrans-209
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: "3.7" networks:   '
- en: This example replaces the `my-databases_default` network with a network called
    `foo`. The two configurations are functionally equivalent.
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此示例将 `my-databases_default` 网络替换为名为 `foo` 的网络。这两个配置在功能上是等效的。
- en: 'There are several occasions to model networks with Compose. For example, if
    you manage multiple stacks and want to communicate on a shared network, you would
    declare that network as shown previously, but instead of specifying the driver,
    you would use the `external: true` property and the network name. Or suppose you
    have multiple groups of related services, but those services should operate in
    isolation. You would define the networks at the top level and use different network
    attachments to isolate the groups.'
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '有几种场合可以使用Compose来建模网络。例如，如果你管理多个堆栈并且想在共享网络上进行通信，你将声明该网络，如前所述，但不是指定驱动程序，而是使用`external:
    true`属性和网络名称。或者假设你有多个相关的服务组，但这些服务应该独立运行。你将在顶级定义网络，并使用不同的网络附加来隔离这些组。'
- en: SUMMARY
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter introduces higher-level Docker abstractions and working with the
    Docker declarative tooling, and using Compose to model the desired state for multi-service
    applications. Compose and declarative tooling relieve much of the tedium associated
    with command-line management of containers. The chapter covers the following:'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章介绍了更高层次的Docker抽象以及使用Docker声明性工具，以及使用Compose来建模多服务应用程序的期望状态。Compose和声明性工具减轻了与命令行管理容器相关的许多繁琐工作。本章涵盖了以下内容：
- en: A service is any process, functionality, or data that must be discoverable and
    available over a network.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何必须通过网络可发现和可用的过程、功能或数据都是一项服务。
- en: Orchestrators such as Swarm track and automatically reconcile user-provided
    desired state and the current state of Docker objects including services, volumes,
    and networks.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理器如Swarm跟踪并自动协调用户提供的期望状态和Docker对象（包括服务、卷和网络）的当前状态。
- en: Orchestrators automate service replication, resurrection, deployments, health
    checking, and rollback.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理器自动化服务的复制、恢复、部署、健康检查和回滚。
- en: The desired state is what the user wants the system to be doing, or what it
    is supposed to be doing. People can describe desired state in a declarative style
    by using Compose files.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望状态是用户希望系统执行的操作，或者它应该执行的操作。人们可以通过使用Compose文件以声明性方式描述期望状态。
- en: Compose files are structured documents represented in YAML.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Compose文件是有结构的文档，以YAML表示。
- en: Declarative environment descriptions with Compose enable environment versioning,
    sharing, iteration, and consistency.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Compose的声明性环境描述可以启用环境版本控制、共享、迭代和一致性。
- en: Compose can model services, volumes, and networks. Consult the official Compose
    file reference material for a full description of its capabilities.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Compose可以建模服务、卷和网络。有关其功能的完整描述，请参阅官方的Compose文件参考材料。
- en: Chapter 12\. First-class configuration abstractions
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第12章\. 首要级的配置抽象
- en: This chapter covers
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: The problems configuration and secrets solve and the forms those solutions take
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和秘密解决的问题及其形式
- en: Modeling and solving configuration problems for Docker services
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模和解决Docker服务的配置问题
- en: The challenge of delivering secrets to applications
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将秘密传递给应用程序的挑战
- en: Modeling and delivering secrets to Docker services
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模和将秘密传递给Docker服务
- en: Approaches for using configurations and secrets in Docker services
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Docker服务中使用配置和秘密的方法
- en: Applications often run in multiple environments and must adapt to different
    conditions in those environments. You may run an application locally, in a test
    environment integrated with collaborating applications and data sources, and finally
    in production. Perhaps you deploy an application instance for each customer in
    order to isolate or specialize each customer’s experience from that of others.
    The adaptations and specializations for each deployment are usually expressed
    via configuration. Configuration is data interpreted by an application to adapt
    its behavior to support a use case.
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用程序通常在多个环境中运行，必须适应这些环境中的不同条件。你可以在本地运行应用程序，在集成有协作应用程序和数据源的开发环境中进行测试，最后在生产环境中运行。也许你会为每个客户部署一个应用程序实例，以便隔离或专门化每个客户的体验。每个部署的适应性和专业化通常通过配置来表示。配置是应用程序解释的数据，以适应其行为以支持用例。
- en: 'Common examples of configuration data include the following:'
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置数据的常见示例包括以下内容：
- en: Features to enable or disable
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可启用或禁用的功能
- en: Locations of services the application depends on
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序所依赖的服务位置
- en: Limits for internal application resources and activities such as database connection
    pool sizes and connection time-outs
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部应用资源及活动的限制，例如数据库连接池大小和连接超时时间
- en: This chapter will show you how to use Docker’s configuration and secret resources
    to adapt Docker service deployments according to various deployment needs. Docker’s
    first-class resources for modeling configuration and secrets will be used to deploy
    a service with different behavior and features, depending on which environment
    it is deployed to. You will see how the naive approach for naming configuration
    resources is troublesome and learn about a pattern for solving that problem. Finally,
    you will learn how to safely manage and use secrets with Docker services. With
    this knowledge, you will deploy the example web application with an HTTPS listener
    that uses a managed TLS certificate.
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章将向您展示如何使用Docker的配置和秘密资源来根据不同的部署需求调整Docker服务部署。Docker将使用一等资源来建模配置和秘密，以根据部署到的环境部署具有不同行为和功能的服务。您将看到命名配置资源的简单方法是有问题的，并了解解决该问题的模式。最后，您将学习如何使用Docker服务安全地管理和使用秘密。有了这些知识，您将部署一个示例Web应用程序，该应用程序使用HTTPS监听器，并使用管理的TLS证书。
- en: 12.1\. CONFIGURATION DISTRIBUTION AND MANAGEMENT
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.1\. 配置分布与管理
- en: Most application authors do not want to change the program’s source code and
    rebuild the application each time they need to vary the behavior of an application.
    Instead, they program the application to read configuration data on startup and
    adjust its behavior accordingly at runtime.
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大多数应用程序作者不希望每次需要更改应用程序的行为时都修改程序源代码并重新构建应用程序。相反，他们编写应用程序以在启动时读取配置数据，并在运行时相应地调整其行为。
- en: In the beginning, it may be feasible to express this variation via command-line
    flags or environment variables. As the program’s configuration needs grow, many
    implementations move on to a file-based solution. These configuration files help
    you express both a greater number of configurations and more complex structures.
    The application may read its configuration from a file formatted in the ini, properties,
    JSON, YAML, TOML, or other format. Applications may also read configurations from
    a configuration server available somewhere on the network. Many applications use
    multiple strategies for reading configuration. For example, an application may
    read a file first and then merge the values of certain environment variables into
    a combined configuration.
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在开始时，可能可以通过命令行标志或环境变量来表示这种变化。随着程序配置需求的增长，许多实现转向基于文件解决方案。这些配置文件可以帮助你表达更多的配置和更复杂的结构。应用程序可能从ini、属性、JSON、YAML、TOML或其他格式的文件中读取其配置。应用程序也可能从网络上的某个配置服务器读取配置。许多应用程序使用多种策略来读取配置。例如，一个应用程序可能首先读取一个文件，然后将某些环境变量的值合并到一个组合配置中。
- en: Configuring applications is a long-standing problem that has many solutions.
    Docker directly supports several of these application configuration patterns.
    Before we discuss that, we will explore how the configuration change life cycle
    fits within the application’s change life cycle. Configurations control a wide
    set of application behaviors and so may change for many reasons, as illustrated
    in [figure 12.1](#filepos1259501).
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置应用程序是一个长期存在的问题，有许多解决方案。Docker直接支持这些应用程序配置模式中的几个。在我们讨论这一点之前，我们将探讨配置更改生命周期如何适应应用程序的更改生命周期。配置控制着广泛的程序行为，因此可能因许多原因而更改，如图12.1所示。
- en: Figure 12.1\. Timeline of application changes
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图12.1\. 应用程序更改的时间线
- en: '![](images/00016.jpg)'
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00016.jpg)'
- en: Let’s examine a few events that drive configuration changes. Configurations
    may change in conjunction with an application enhancement. For example, when developers
    add a feature, they may control access to that feature with a feature flag. An
    application deployment may need to update in response to a change outside the
    application’s scope. For example, the hostname configured for a service dependency
    may need to update from `cluster-blue` to `cluster-green`. And of course, applications
    may change code without changing configuration. These changes may occur for reasons
    that are internal or external to an application. The application’s delivery process
    must merge and deploy these changes safely regardless of the reason for the change.
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们考察一些驱动配置变更的事件。配置可能会随着应用程序的增强而改变。例如，当开发者添加一个功能时，他们可能会使用功能标志来控制对该功能的访问。应用程序部署可能需要更新以响应应用程序范围之外的变化。例如，为服务依赖项配置的主机名可能需要从
    `cluster-blue` 更新到 `cluster-green`。当然，应用程序可能在不更改配置的情况下更改代码。这些变更可能由应用程序内部或外部的原因引起。无论变更的原因是什么，应用程序的交付流程都必须安全地合并和部署这些变更。
- en: A Docker service depends on configuration resources much in the same way it
    depends on the Docker image containing the application, as illustrated in [figure
    12.2](#filepos1262083). If a configuration or secret is missing, the application
    will probably not start or behave properly. Also, once an application expresses
    a dependency on a configuration resource, the existence of that dependency must
    be stable. Applications will likely break or behave in unexpected ways if the
    configuration they depend on disappears when the application restarts. The application
    might also break if the values inside the configuration change unexpectedly. For
    example, renaming or removing entries inside a configuration file will break an
    application that does not know how to read that format. So the configuration lifecycle
    must adopt a scheme that preserves backward compatibility for existing deployments.
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker服务在很大程度上依赖于配置资源，就像它依赖于包含应用程序的Docker镜像一样，如图12.2所示。如果缺少配置或密钥，应用程序可能无法启动或正常工作。此外，一旦应用程序表达了对配置资源的依赖，该依赖的存在必须是稳定的。如果应用程序依赖的配置在应用程序重启时消失，应用程序可能会崩溃或以意外的方式表现。如果配置内部的值意外更改，应用程序也可能崩溃。例如，更改配置文件内的条目名称或删除条目将破坏不知道如何读取该格式的应用程序。因此，配置的生命周期必须采用一种方案，以保留现有部署的向后兼容性。
- en: Figure 12.2\. Applications depend on configurations.
  id: totrans-242
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图12.2. 应用程序依赖于配置。
- en: '![](images/00001.jpg)'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00001.jpg)'
- en: If you separate change for software and configuration into separate pipelines,
    tension will exist between those pipelines. Application delivery pipelines are
    usually modeled in an application-centric way, with an assumption that all changes
    will run through the application’s source repository. Because configurations can
    change for reasons that are external to the application, and applications are
    generally not built to handle changes that break backward compatibility in the
    configuration model, we need to model, integrate, and sequence configuration changes
    to avoid breaking applications. In the next section, we will separate configuration
    from an application to solve deployment variation problems. Then we will join
    the correct configuration with the service at deployment time.
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你将软件和配置的变更分开到不同的管道中，这些管道之间将存在紧张关系。应用程序交付管道通常以应用程序为中心进行建模，假设所有变更都将通过应用程序的源代码库运行。由于配置可能会因与应用程序无关的原因而改变，并且应用程序通常不是为处理配置模型中破坏向后兼容性的变更而构建的，因此我们需要对配置变更进行建模、集成和排序，以避免破坏应用程序。在下一节中，我们将从应用程序中分离配置以解决部署变化问题。然后，在部署时我们将正确配置与服务关联。
- en: 12.2\. SEPARATING APPLICATION AND CONFIGURATION
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.2. 分离应用程序和配置
- en: 'Let’s work through a problem of needing to vary configuration of a Docker service
    deployed to multiple environments. Our example application is a `greetings` service
    that says “Hello World!” in different languages. The developers of this service
    want the service to return a greeting when a user requests one. When a native
    speaker verifies that a greeting translation is accurate, it is added to the list
    of greetings in the config.common.yml file:'
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们解决一个需要调整部署到多个环境中的Docker服务配置的问题。我们的示例应用程序是一个 `greetings` 服务，它用不同的语言说“Hello
    World！”。这个服务的开发者希望当用户请求时，服务能够返回一个问候语。当母语者验证问候语的翻译准确无误时，它会被添加到 config.common.yml
    文件中的问候语列表中：
- en: '`greetings:   - ''Hello World!''   - ''Hola Mundo!''   - ''Hallo Welt!''`'
  id: totrans-247
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`greetings:   - ''Hello World!''   - ''Hola Mundo!''   - ''Hallo Welt!''`'
- en: The application image build uses the Dockerfile `COPY` instruction to populate
    the `greetings` application image with this common configuration resource. This
    is appropriate because that file has no deployment-time variation or sensitive
    data.
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用程序镜像构建使用Dockerfile中的`COPY`指令，将此通用配置资源填充到`greetings`应用程序镜像中。这是合适的，因为该文件在部署时没有变化或敏感数据。
- en: 'The service also supports loading environment-specific greetings in addition
    to the standard ones. This allows the development team to vary and test the greetings
    shown in each of three environments: dev, stage, and prod. The environment-specific
    greetings will be configured in a file named after the environment; for example,
    config.dev.yml:'
  id: totrans-249
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '服务还支持加载环境特定的问候语，除了标准问候语。这允许开发团队在每个三个环境中（dev、stage和prod）更改和测试显示的问候语。环境特定的问候语将配置在以环境命名的文件中；例如，config.dev.yml:'
- en: '`# config.dev.yml greetings:   - ''Orbis Terrarum salve!''   - ''Bonjour le
    monde!''`'
  id: totrans-250
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`# config.dev.yml greetings:   - ''Orbis Terrarum salve!''   - ''Bonjour le
    monde!''`'
- en: Both the common and environment-specific configuration files must be available
    as files on the `greetings` service container’s filesystem, as shown in [figure
    12.3](#filepos1266305).
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通用和环境特定的配置文件都必须作为文件存在于`greetings`服务容器文件系统中，如图12.3所示。
- en: Figure 12.3\. The `greetings` service supports common and environment-specific
    configuration via files.
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图12.3. `greetings`服务通过文件支持通用和环境特定的配置。
- en: '![](images/00084.jpg)'
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00084.jpg)'
- en: So the immediate problem we need to solve is how to get the environment-specific
    configuration file into the container by using only the deployment descriptor.
    Follow along with this example by cloning and reading the files in the [https://github.com/dockerinaction/ch12_greetings.git](https://github.com/dockerinaction/ch12_greetings.git)
    source repository.
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，我们需要立即解决的问题是如何仅使用部署描述符将环境特定的配置文件放入容器中。请跟随这个示例，通过克隆并阅读[https://github.com/dockerinaction/ch12_greetings.git](https://github.com/dockerinaction/ch12_greetings.git)源代码库中的文件。
- en: The `greetings` application deployment is defined using the Docker Compose application
    format and deployed to Docker Swarm as a stack. These concepts were introduced
    in [chapter 11](index_split_092.html#filepos1146874). In this application, there
    are three Compose files. The deployment configuration that is common across all
    environments is contained in docker-compose.yml. There are also environment-specific
    Compose files for both of the stage and prod environments (for example, docker-compose.prod.yml).
    The environment-specific Compose files define additional configuration and secret
    resources that the service uses in those deployments.
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`greetings`应用程序的部署使用Docker Compose应用程序格式定义，并作为堆栈部署到Docker Swarm。这些概念在[第11章](index_split_092.html#filepos1146874)中介绍。在这个应用程序中，有三个Compose文件。所有环境通用的部署配置包含在docker-compose.yml中。还有针对开发和生产环境的环境特定Compose文件（例如，docker-compose.prod.yml）。环境特定的Compose文件定义了服务在这些部署中使用的额外配置和秘密资源。'
- en: 'Here is the shared deployment descriptor, docker-compose.yml:'
  id: totrans-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '这里是共享的部署描述符，docker-compose.yml:'
- en: '`version: ''3.7''  configs:   env_specific_config:     file: ./api/config/config.${DEPLOY_ENV:-prod}.yml`
    `1` `services:    api:       image: ${IMAGE_REPOSITORY:-dockerinaction/ch12_greetings}:api
          ports:         - ''8080:8080''         - ''8443:8443''       user: ''1000''
          configs:         - source: env_specific_config           target: /config/config.${DEPLOY_ENV:-prod}.yml`
    `2` `uid: ''1000''           gid: ''1000''           mode: 0400 #default is 0444
    - readonly for all users       secrets: []       environment:         DEPLOY_ENV:
    ${DEPLOY_ENV:-prod}`'
  id: totrans-257
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: ''3.7''  configs:   env_specific_config:     file: ./api/config/config.${DEPLOY_ENV:-prod}.yml`
    `1` `services:    api:       image: ${IMAGE_REPOSITORY:-dockerinaction/ch12_greetings}:api
          ports:         - ''8080:8080''         - ''8443:8443''       user: ''1000''
          configs:         - source: env_specific_config           target: /config/config.${DEPLOY_ENV:-prod}.yml`
    `2` `uid: ''1000''           gid: ''1000''           mode: 0400 #默认是0444 - 对所有用户只读
          secrets: []       environment:         DEPLOY_ENV: ${DEPLOY_ENV:-prod}`'
- en: 1 Defines a config resource using the contents of the env-specific configuration
    file
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 使用环境特定配置文件的内容定义一个配置资源
- en: 2 Maps the env_specific_config resource to a file in the container
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 将env_specific_config资源映射到容器中的文件
- en: This Compose file loads the `greetings` application’s environment-specific configuration
    files into the `api` service’s containers. This supplements the common configuration
    file built into the application image. The `DEPLOY_ENV` environment variable parameterizes
    this deployment definition. This environment variable is used in two ways.
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此 Compose 文件将 `greetings` 应用程序的环境特定配置文件加载到 `api` 服务的容器中。这补充了内置到应用程序镜像中的通用配置文件。`DEPLOY_ENV`
    环境变量参数化此部署定义。此环境变量以两种方式使用。
- en: First, the deployment descriptor will produce different deployment definitions
    when Docker interpolates `DEPLOY_ENV`. For example, when `DEPLOY_ENV` is set to
    `dev`, Docker will reference and load config.dev.yml.
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，当 Docker 插值 `DEPLOY_ENV` 时，部署描述符将产生不同的部署定义。例如，当 `DEPLOY_ENV` 设置为 `dev` 时，Docker
    将引用并加载 config.dev.yml。
- en: Second, the deployment descriptor’s value of the `DEPLOY_ENV` variable is passed
    to the `greetings` service via an environment variable definition. This environment
    variable signals to the service which environment it is running in, enabling it
    to do things such as load configuration files named after the environment. Now
    let’s examine Docker config resources and how the environment-specific configuration
    files are managed.
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其次，部署描述符中 `DEPLOY_ENV` 变量的值将通过环境变量定义传递给 `greetings` 服务。此环境变量向服务指示其运行的环境，使其能够执行诸如加载以环境命名的配置文件等操作。现在让我们来检查
    Docker 配置资源以及如何管理环境特定配置文件。
- en: 12.2.1\. Working with the config resource
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.2.1\. 使用配置资源
- en: A Docker config resource is a Swarm cluster object that deployment authors can
    use to store runtime data needed by applications. Each config resource has a cluster-unique
    name and a value of up to 500 KB. When a Docker service uses a config resource,
    Swarm mounts a file inside the service’s container filesystems populated with
    the config resource’s contents.
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 配置资源是 Swarm 集群对象，部署作者可以使用它来存储应用程序所需的运行时数据。每个配置资源都有一个集群唯一的名称和最多 500 KB
    的值。当 Docker 服务使用配置资源时，Swarm 将在服务容器的文件系统中挂载一个包含配置资源内容的文件。
- en: 'The top-level `configs` key defines the Docker config resources that are specific
    to this application deployment. This `configs` key defines a map with one config
    resource, `env_specific_config`:'
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 顶层 `configs` 键定义了特定于此应用程序部署的 Docker 配置资源。此 `configs` 键定义了一个包含一个配置资源 `env_specific_config`
    的映射：
- en: '`configs:   env_specific_config:     file: ./api/config/config.${DEPLOY_ENV:-prod}.yml`'
  id: totrans-266
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`configs:    env_specific_config:        file: ./api/config/config.${DEPLOY_ENV:-prod}.yml`'
- en: When this stack is deployed, Docker will interpolate the filename with the value
    of the `DEPLOY_ENV` variable, read that file, and store it in a config resource
    named `env_ specific_config` inside the Swarm cluster.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当此堆栈部署时，Docker 将将 `DEPLOY_ENV` 变量的值插入到文件名中，读取该文件，并将其存储在 Swarm 集群内名为 `env_specific_config`
    的配置资源中。
- en: 'Defining a config in a deployment does not automatically give services access
    to it. To give a service access to a config, the deployment definition must map
    it under the service’s own `configs` key. The config mapping may customize the
    location, ownership, and permissions of the resulting file on the service container’s
    filesystem:'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在部署中定义配置不会自动使服务访问它。要使服务访问配置，部署定义必须将其映射到服务自己的 `configs` 键下。配置映射可以自定义结果文件在服务容器文件系统中的位置、所有权和权限：
- en: '`# ...snip... services:   api:       # ...snip...       user: ''1000''      
    configs:         - source: env_specific_config           target: /config/config.${DEPLOY_ENV:-prod}.yml`
    `1` `uid: ''1000''` `2` `gid: ''1000''           mode: 0400` `3`'
  id: totrans-269
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`# ...省略... services:    api:        # ...省略...        user: ''1000''        configs:            -
    source: env_specific_config            target: /config/config.${DEPLOY_ENV:-prod}.yml`
    `1` `uid: ''1000''` `2` `gid: ''1000''        mode: 0400` `3`'
- en: 1 Overrides default target file path is /env_specific_config
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 覆盖默认目标文件路径为 /env_specific_config
- en: 2 Overrides default uid and gid of 0, to match service user with uid 1000
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 覆盖默认的 uid 和 gid 为 0，以匹配具有 uid 1000 的服务用户
- en: 3 Overrides default file mode of 0444, read-only for all users
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 覆盖默认文件模式 0444，对所有用户只读
- en: In this example, the `env_specific_config` resource is mapped into the `greetings`
    service container with several adjustments. By default, config resources are mounted
    into the container filesystem at `/<config_name>`; for example, `/env_specific_config`.
    This example maps `env_specific_config` to the target location `/config/config.$
    {DEPLOY_ENV:-prod}.yml`. Thus, for a deployment in the dev environment, the environment-specific
    config file will appear at `/config/config.dev.yml`. The ownership of this configuration
    file is set to `userid=1000` and `groupid=1000`. By default, files for config
    resources are owned by the user ID and group ID 0\. The file permissions are also
    narrowed to a mode of 0400\. This means the file is readable by only the file
    owner, whereas the default is readable by the owner, group, and others (0444).
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个例子中，`env_specific_config`资源被映射到`greetings`服务容器中，并进行了几个调整。默认情况下，配置资源被挂载到容器文件系统中的`/<config_name>`；例如，`/env_specific_config`。此示例将`env_specific_config`映射到目标位置`/config/config.$
    {DEPLOY_ENV:-prod}.yml`。因此，对于开发环境的部署，环境特定的配置文件将出现在`/config/config.dev.yml`。此配置文件的所有权设置为`userid=1000`和`groupid=1000`。默认情况下，配置资源的文件属于用户ID和组ID
    0。文件权限也被限制为模式0400。这意味着文件只能由文件所有者读取，而默认情况下文件所有者、组和其他用户都可以读取（0444）。
- en: These changes are not strictly necessary for this application because it is
    under our control. The application could be implemented to work with Docker’s
    defaults instead. However, other applications are not as flexible and may have
    startup scripts that work in a specific way that you cannot change. In particular,
    you may need to control the configuration filename and ownership in order to accommodate
    a program that wants to run as a particular user and read configuration files
    from predetermined locations. Docker’s service config resource mapping allows
    you to accommodate these demands. You can even map a single config resource into
    multiple service definitions differently, if needed.
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些更改对于这个应用程序来说并非绝对必要，因为它是受我们控制的。该应用程序可以被实现为使用Docker的默认设置来工作。然而，其他应用程序可能没有这么灵活，并且可能有启动脚本，这些脚本以特定的方式工作，你不能更改。特别是，你可能需要控制配置文件名和所有权，以便适应想要以特定用户身份运行并从预定位置读取配置文件的程序。Docker的服务配置资源映射允许你满足这些需求。如果需要，你甚至可以将单个配置资源映射到多个不同的服务定义中。
- en: With the config resources set along with the service definition, let’s deploy
    the application.
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在配置资源和服务定义一起设置之后，让我们部署应用程序。
- en: 12.2.2\. Deploying the application
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.2.2\. 部署应用程序
- en: 'Deploy the `greetings` application with the dev environment configuration by
    running the following command:'
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过运行以下命令以开发环境配置部署`greetings`应用程序：
- en: '`DEPLOY_ENV=dev docker stack deploy \     --compose-file docker-compose.yml
    greetings_dev`'
  id: totrans-278
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`DEPLOY_ENV=dev docker stack deploy \    --compose-file docker-compose.yml
    greetings_dev`'
- en: 'After the stack deploys, you can point a web browser to the service at http://localhost:
    8080/, and should see a welcome message like this:'
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '部署栈之后，你可以通过网页浏览器访问服务，地址为 http://localhost: 8080/，应该会看到一个欢迎信息，如下所示：'
- en: '`Welcome to the Greetings API Server! Container with id 642abc384de5 responded
    at 2019-04-16 00:24:37.0958326 +0000 UTC DEPLOY_ENV: dev` `1`'
  id: totrans-280
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`欢迎使用Greetings API服务器！容器ID为642abc384de5，于2019-04-16 00:24:37.0958326 +0000
    UTC响应。DEPLOY_ENV: dev` `1`'
- en: 1 The value “dev” is read from environment variable.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 从环境变量中读取值“dev”。
- en: When you run `docker stack deploy`, Docker reads the application’s environment-specific
    configuration file and stores it as a config resource inside the Swarm cluster.
    Then when the `api` service starts, Swarm creates a copy of those files on a temporary,
    read-only filesystem. Even if you set the file mode as writable (for example,
    `rw-rw-rw-`), it will be ignored. Docker mounts these files at the target location
    specified in the config. The config file’s target location can be pretty much
    anywhere, even inside a directory that contains regular files from the application
    image. For example, the `greetings` service’s common config files (`COPY`’d into
    app image) and environment-specific config file (a Docker config resource) are
    both available in the /config directory. The application container can read these
    files when it starts up, and those files are available for the life of the container.
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您运行`docker stack deploy`时，Docker会读取应用程序的特定环境配置文件，并将其作为配置资源存储在Swarm集群中。然后当`api`服务启动时，Swarm在临时只读文件系统上创建这些文件的副本。即使您将文件模式设置为可写（例如，`rw-rw-rw-`），它也会被忽略。Docker将这些文件挂载到配置中指定的目标位置。配置文件的目标位置几乎可以是任何地方，甚至可以是在包含应用程序镜像常规文件的目录内部。例如，`greetings`服务的通用配置文件（`COPY`到应用程序镜像中）和特定环境的配置文件（一个Docker配置资源）都可在/config目录中找到。应用程序容器在启动时可以读取这些文件，并且这些文件在整个容器生命周期内都可用。
- en: 'On startup, the `greetings` application uses the `DEPLOY_ENV` environment variable
    to calculate the name of the environment-specific config file; for example, /config/
    config.dev.yml. The application then reads both of its config files and merges
    the list of greetings. You can see how the `greetings` service does this by reading
    the api/main.go file in the source repository. Now, navigate to the http://localhost:8080/greeting
    endpoint and make several requests. You should see a mix of greetings from the
    common and environment-specific config. For example:'
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在启动时，`greetings`应用程序使用`DEPLOY_ENV`环境变量来计算特定环境的配置文件名称；例如，/config/config.dev.yml。然后应用程序读取其两个配置文件并合并问候语列表。您可以通过阅读源存储库中的api/main.go文件来了解`greetings`服务是如何做到这一点的。现在，导航到http://localhost:8080/greeting端点并发出几个请求。您应该会看到来自通用和特定环境的问候语的混合。例如：
- en: '`Hello World! Orbis Terrarum salve! Hello World! Hallo Welt! Hola Mundo!`'
  id: totrans-284
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Hello World! Orbis Terrarum salve! Hello World! Hallo Welt! Hola Mundo!`'
- en: '|    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Config resources vs. config images
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置资源与配置镜像
- en: You may recall the Configuration Image per Deployment Stage pattern described
    in [chapter 10](index_split_084.html#filepos1058777). In that pattern, environment-specific
    configurations are built into an image that runs as a container, and that filesystem
    is mounted into the “real” service container's filesystem at runtime. The Docker
    config resource automates most of this pattern. Using config resources results
    in a file being mounted into the service task container and does so without needing
    to create and track additional images. The Docker config resource also allows
    you to easily mount a single config file into an arbitrary location on the filesystem.
    With the container image pattern, it’s best to mount an entire directory in order
    to avoid confusion about what file came from which image.
  id: totrans-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可能还记得在第10章中描述的“每个部署阶段配置镜像”模式。在该模式中，特定环境的配置被构建到一个作为容器运行的镜像中，并且该文件系统在运行时挂载到“真实”服务容器的文件系统上。Docker配置资源自动化了大多数这种模式。使用配置资源会导致一个文件被挂载到服务任务容器中，并且无需创建和跟踪额外的镜像。Docker配置资源还允许您轻松地将单个配置文件挂载到文件系统的任意位置。在使用容器镜像模式时，最好挂载整个目录，以避免混淆来自哪个镜像的文件。
- en: In both approaches, you’ll want to use uniquely identified config or image names.
    However, it is convenient that image names can use variable substitutions in Docker
    Compose application deployment descriptors, avoiding the resource-naming problems
    that will be discussed in the next section.
  id: totrans-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这两种方法中，您都希望使用唯一标识的配置或镜像名称。然而，在Docker Compose应用程序部署描述符中，可以使用变量替换来指定镜像名称，这避免了下一节将要讨论的资源命名问题。
- en: '|    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: So far, we have managed config resources through the convenience of a Docker
    Compose deployment definition. In the next section, we will step down an abstraction
    level and use the `docker` command-line tool to directly inspect and manage config
    resources.
  id: totrans-290
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过Docker Compose部署定义的便利性来管理配置资源。在下一节中，我们将降低抽象级别，并使用`docker`命令行工具直接检查和管理配置资源。
- en: 12.2.3\. Managing config resources directly
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.2.3\. 直接管理配置资源
- en: 'The `docker config` command provides another way to manage config resources.
    The `config` command has several subcommands for creating, inspecting, listing,
    and removing config resources: `create`, `inspect`, `ls`, and `rm`, respectively.
    You can use these commands to directly manage a Docker Swarm cluster’s config
    resources. Let’s do that now.'
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker config` 命令提供了另一种管理配置资源的方式。`config` 命令有多个子命令用于创建、检查、列出和删除配置资源：分别是 `create`、`inspect`、`ls`
    和 `rm`。您可以使用这些命令直接管理 Docker Swarm 集群的配置资源。现在让我们来做这件事。'
- en: 'Inspect the `greetings` service’s `env_specific_config` resource:'
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 检查 `greetings` 服务的 `env_specific_config` 资源：
- en: '`docker config inspect greetings_dev_env_specific_config` `1`'
  id: totrans-294
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker config inspect greetings_dev_env_specific_config` `1`'
- en: 1 Docker automatically prefixes the config resource with the greetings_dev stack
    name.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Docker 会自动将配置资源前缀为 greetings_dev 栈名称。
- en: 'This should produce output similar to the following:'
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这应该会产生类似于以下输出的结果：
- en: '`[     {         "ID": "bconc1huvlzoix3z5xj0j16u1",         "Version": {            
    "Index": 2066         },         "CreatedAt": "2019-04-12T23:39:30.6328889Z",
            "UpdatedAt": "2019-04-12T23:39:30.6328889Z",         "Spec": {            
    "Name": "greetings_dev_env_specific_config",             "Labels": {                
    "com.docker.stack.namespace": "greetings"             },             "Data":     
    "Z3JlZXRpbmdzOgogIC0gJ09yYmlzIFRlcnJhcnVtIHNhbHZlIScKICAtICdCb25qb3VyIGx     
    lIG1vbmRlIScK"         }     } ]`'
  id: totrans-297
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[   {   "ID": "bconc1huvlzoix3z5xj0j16u1",   "Version": {   "Index": 2066  
    },   "CreatedAt": "2019-04-12T23:39:30.6328889Z",   "UpdatedAt": "2019-04-12T23:39:30.6328889Z",  
    "Spec": {   "Name": "greetings_dev_env_specific_config",   "Labels": {   "com.docker.stack.namespace":
    "greetings"   },   "Data":   "Z3JlZXRpbmdzOgogIC0gJ09yYmlzIFRlcnJhcnVtIHNhbHZlIScKICAtICdCb25qb3VyIGx  
    lIG1vbmRlIScK"   }   } ]`'
- en: The `inspect` command reports metadata associated with the config resource and
    the config’s value. The config’s value is returned in the `Data` field as a Base64-encoded
    string. This data is not encrypted, so no confidentiality is provided here. The
    Base64 encoding only facilitates transportation and storage within the Docker
    Swarm cluster. When a service references a config resource, Swarm will retrieve
    this data from the cluster’s central store and place it in a file on the service
    task’s filesystem.
  id: totrans-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`inspect` 命令报告与配置资源及其值相关的元数据。配置的值以 Base64 编码的字符串形式返回在 `Data` 字段中。这些数据未加密，因此在此不提供机密性。Base64
    编码仅便于在 Docker Swarm 集群内部传输和存储。当服务引用配置资源时，Swarm 将从集群的中央存储中检索这些数据并将其放置在服务任务文件系统上的一个文件中。'
- en: 'Docker config resources are immutable and cannot be updated after they are
    created. The `docker config` command supports only `create` and `rm` operations
    to manage the cluster’s config resources. If you try to `create` a config resource
    multiple times by using the same name, Docker will return an error saying the
    resource already exists:'
  id: totrans-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 配置资源是不可变的，一旦创建后不能更新。`docker config` 命令仅支持 `create` 和 `rm` 操作来管理集群的配置资源。如果您尝试使用相同的名称多次创建配置资源，Docker
    将返回一个错误，表示资源已存在：
- en: '`$ docker config create greetings_dev_env_specific_config \     api/config/config.dev.yml
    Error response from daemon: rpc error: code = AlreadyExists` ![](images/00055.jpg)
    `desc = config greetings_dev_env_specific_config already exists`'
  id: totrans-300
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker config create greetings_dev_env_specific_config \   api/config/config.dev.yml
    Error response from daemon: rpc error: code = AlreadyExists` ![图片](images/00055.jpg)
    `desc = config greetings_dev_env_specific_config already exists`'
- en: 'Similarly, if you change the source configuration file and try to redeploy
    the stack by using the same config resource name, Docker will also respond with
    an error:'
  id: totrans-301
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同样，如果您更改源配置文件并尝试使用相同的配置资源名称重新部署堆栈，Docker 也会响应错误：
- en: '`$ DEPLOY_ENV=dev docker stack deploy \     --compose-file docker-compose.yml
    greetings_dev failed to update config greetings_dev_env_specific_config:` ![](images/00055.jpg)
    `Error response from daemon: rpc error: code = InvalidArgument` ![](images/00055.jpg)
    `desc = only updates to Labels are allowed`'
  id: totrans-302
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ DEPLOY_ENV=dev docker stack deploy \   --compose-file docker-compose.yml
    greetings_dev failed to update config greetings_dev_env_specific_config:` ![图片](images/00055.jpg)
    `Error response from daemon: rpc error: code = InvalidArgument` ![图片](images/00055.jpg)
    `desc = only updates to Labels are allowed`'
- en: 'You can visualize the relationships between a Docker service and its dependencies
    as a directed graph, like the simple one shown in [figure 12.4](#filepos1296376):'
  id: totrans-303
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以将 Docker 服务及其依赖关系可视化为一个有向图，就像图 12.4 中所示的一个简单图：
- en: Figure 12.4\. Docker services depend on config resources.
  id: totrans-304
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 12.4\. Docker 服务依赖于配置资源。
- en: '![](images/00025.jpg)'
  id: totrans-305
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00025.jpg)'
- en: Docker is trying to maintain a stable set of dependencies between Docker services
    and the config resources they depend on. If the `greetings_dev_env_specific_config`
    resource were to change or be removed, new tasks for the `greetings_dev` service
    may not start. Let’s see how Docker tracks these relationships.
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 正在尝试维护 Docker 服务与其依赖的配置资源之间的一组稳定依赖关系。如果 `greetings_dev_env_specific_config`
    资源发生变化或被删除，新的 `greetings_dev` 服务任务可能无法启动。让我们看看 Docker 如何跟踪这些关系。
- en: Config resources are each identified by a unique `ConfigID`. In this example,
    the `greetings_dev_env_specific_config` is identified by `bconc1huvlzoix3z5xj0j16u1`,
    which is visible in the `docker config inspect` command’s `greetings_dev_env_
    specific_config` output. This same config resource is referenced by its `ConfigID`
    in the `greetings` service definition.
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置资源每个都通过一个唯一的 `ConfigID` 来标识。在这个例子中，`greetings_dev_env_specific_config` 通过
    `bconc1huvlzoix3z5xj0j16u1` 来标识，这在 `docker config inspect` 命令的 `greetings_dev_env_specific_config`
    输出中是可见的。这个相同的配置资源在 `greetings` 服务定义中通过其 `ConfigID` 来引用。
- en: 'Let’s verify that now by using the `docker service inspect` command. This inspection
    command prints only the `greetings` service references to config resources:'
  id: totrans-308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们通过使用 `docker service inspect` 命令来验证这一点。这个检查命令只打印 `greetings` 服务的配置资源引用：
- en: '`docker service inspect \     --format ''{{ json .Spec.TaskTemplate.ContainerSpec.Configs
    }}'' \     greetings_dev_api`'
  id: totrans-309
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service inspect \    --format ''{{ json .Spec.TaskTemplate.ContainerSpec.Configs
    }}'' \    greetings_dev_api`'
- en: 'For this service instantiation, the command produces the following:'
  id: totrans-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于这个服务实例化，命令生成了以下内容：
- en: '`[   {     "File": {        "Name": "/config/config.dev.yml",       "UID":
    "1000",       "GID": "1000",       "Mode": 256     },     "ConfigID": "bconc1huvlzoix3z5xj0j16u1",
        "ConfigName": "greetings_dev_env_specific_config"   } ]`'
  id: totrans-311
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[    {    "File": {        "Name": "/config/config.dev.yml",        "UID":
    "1000",        "GID": "1000",        "Mode": 256    },    "ConfigID": "bconc1huvlzoix3z5xj0j16u1",    "ConfigName":
    "greetings_dev_env_specific_config"    } ]`'
- en: There are a few important things to call out. First, the `ConfigID` references
    the `greeetings_dev_env_specific_config` config resource’s unique config ID, `bconc1huvlzoix3z5xj0j16u1`.
    Second, the service-specific target file configurations have been included in
    the service’s definition. Also note that you cannot create a config if the name
    already exists or remove a config while it is in use. Recall that the `docker
    config` command does not offer an update subcommand. This may leave you wondering
    how you update configurations. [Figure 12.5](#filepos1301299) illustrates a solution
    to the problem.
  id: totrans-312
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有几点重要的事情需要指出。首先，`ConfigID` 引用了 `greeetings_dev_env_specific_config` 配置资源的唯一配置
    ID，`bconc1huvlzoix3z5xj0j16u1`。其次，服务特定的目标文件配置已被包含在服务的定义中。请注意，如果名称已存在，则无法创建配置；如果在使用中，则无法删除配置。回想一下，`docker
    config` 命令没有提供更新子命令。这可能会让您想知道如何更新配置。[图 12.5](#filepos1301299) 展示了解决这个问题的方案。
- en: Figure 12.5\. Copy on deploy
  id: totrans-313
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 12.5\. 部署时复制
- en: '![](images/00062.jpg)'
  id: totrans-314
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00062.jpg)'
- en: The answer is that you don’t update Docker config resources. Instead, when a
    configuration file changes, the deployment process should create a new resource
    with a different name and then reference that name in service deployments. The
    common convention is to append a version number to the configuration resource’s
    name. The `greetings` application’s deployment definition could define an `env_specific_config_v1`
    resource. When the configuration changes, that configuration could be stored in
    a new config resource named `env_specific_config_v2`. Services can adopt this
    new config by updating configuration references to this new config resource name.
  id: totrans-315
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案是您不需要更新 Docker 配置资源。相反，当配置文件发生变化时，部署过程应该创建一个具有不同名称的新资源，然后在服务部署中引用该名称。常见的做法是在配置资源名称后附加版本号。`greetings`
    应用程序的部署定义可以定义一个 `env_specific_config_v1` 资源。当配置发生变化时，该配置可以存储在一个名为 `env_specific_config_v2`
    的新配置资源中。服务可以通过更新配置引用到这个新的配置资源名称来采用这个新配置。
- en: This implementation of immutable config resources creates challenges for automated
    deployment pipelines. The issue is discussed in detail on GitHub issue moby/moby
    35048\. The main challenge is that the name of a config resource cannot be parameterized
    directly in the YAML Docker Compose deployment definition format. Automated deployment
    processes can work around this by using a custom script that substitutes a unique
    version into the Compose file prior to running the deployment.
  id: totrans-316
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种不可变配置资源的实现给自动化部署管道带来了挑战。这个问题在GitHub issue moby/moby 35048中进行了详细讨论。主要挑战是配置资源的名称不能直接在YAML
    Docker Compose部署定义格式中参数化。自动化部署过程可以通过在运行部署之前使用自定义脚本来替换唯一版本来解决这个问题。
- en: For example, say the deployment descriptor defines a config `env_specific_config
    _vNNN`. An automated build process could search for the `_vNNN` character sequence
    and replace it with a unique deployment identifier. The identifier could be the
    deployment job’s ID or the application’s version from a version-control system.
    A deploy job with ID 3 could rewrite all instances of `env_specific_config_vNNN`
    to `env_specific_config_v3`.
  id: totrans-317
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，假设部署描述符定义了一个配置`env_specific_config_vNNN`。自动化构建过程可以搜索`_vNNN`字符序列，并将其替换为唯一的部署标识符。该标识符可以是部署作业的ID或版本控制系统中应用程序的版本。具有ID
    3的部署作业可以将所有`env_specific_config_vNNN`实例重写为`env_specific_config_v3`。
- en: Try this config resource versioning scheme. Start by adding some greetings to
    the config.dev.yml file. Then rename the `env_specific_config` resource in docker-compose.yml
    to `env_specific_config_v2.` Be sure to update the key names in both the top-level
    config map as well as the `api` service’s list of configs. Now update the application
    by deploying the stack again. Docker should print a message saying that it is
    creating `env_specific_config_v2` and updating the service. Now when you make
    requests to the greeting endpoint, you should see the greetings you added mixed
    into the responses.
  id: totrans-318
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尝试这种配置资源版本控制方案。首先，向config.dev.yml文件中添加一些问候语。然后将docker-compose.yml中的`env_specific_config`资源重命名为`env_specific_config_v2`。确保更新顶级配置映射以及`api`服务配置列表中的键名。现在通过再次部署栈来更新应用程序。Docker应该会打印一条消息，说明它正在创建`env_specific_config_v2`并更新服务。现在当你向问候语端点发出请求时，你应该看到你添加的问候语与响应混合在一起。
- en: This approach may be acceptable to some but has a few drawbacks. First, deploying
    resources from files that don’t match version control may be a nonstarter for
    some people. This issue can be mitigated by archiving a copy of the files used
    for deployment. A second issue is that this approach will create a set of config
    resources for each deployment, and the old resources will need to be cleaned up
    by another process. That process could periodically inspect each config resource
    to determine whether it is in use and remove it if it is not.
  id: totrans-319
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种方法可能对某些人来说是可接受的，但有几个缺点。首先，从与版本控制不匹配的文件中部署资源可能对某些人来说不是起点。这个问题可以通过存档用于部署的文件副本来缓解。第二个问题是，这种方法将为每次部署创建一组配置资源，并且旧资源需要通过另一个过程清理。这个过程可以定期检查每个配置资源，以确定它是否在使用中，如果不使用则将其删除。
- en: 'We are done with the dev deployment of the `greetings` application. Clean up
    those resources and avoid conflicts with later examples by removing the stack:'
  id: totrans-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们已经完成了`greetings`应用的开发部署。清理这些资源，并通过移除栈来避免与后续示例冲突：
- en: '`docker stack rm greetings_dev`'
  id: totrans-321
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack rm greetings_dev`'
- en: 'That completes the introduction of Docker config and its integration into delivery
    pipelines. Next, we’ll examine Docker’s support for a special kind of configuration:
    secrets.'
  id: totrans-322
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这就完成了对Docker配置及其集成到交付管道的介绍。接下来，我们将检查Docker对特殊类型配置的支持：机密信息。
- en: 12.3\. SECRETS—A SPECIAL KIND OF CONFIGURATION
  id: totrans-323
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.3. 机密信息——一种特殊的配置
- en: Secrets look a lot like configuration with one important difference. The value
    of a secret is important and often highly valuable because it authenticates an
    identity or protects data. A secret may take the form of a password, API key,
    or private encryption key. If these secrets leak, someone may be able to perform
    actions or access data they are not authorized for.
  id: totrans-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机密信息与配置非常相似，但有一个重要区别。机密信息的值很重要，通常非常有价值，因为它可以验证身份或保护数据。机密信息可能以密码、API密钥或私有加密密钥的形式存在。如果这些机密信息泄露，有人可能能够执行未经授权的操作或访问数据。
- en: A further complication exists. Distributing secrets in artifacts such as Docker
    images or configuration files makes controlling access to those secrets a wide
    and difficult problem. Every point in the distribution chain needs to have robust
    and effective access controls to prevent leakage.
  id: totrans-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 存在另一个复杂问题。在 Docker 镜像或配置文件等工件中分发秘密，使得控制对这些秘密的访问成为一个广泛且困难的问题。分布链中的每一个点都需要有强大而有效的访问控制，以防止泄露。
- en: Most organizations give up on trying to deliver secrets without exposing them
    through normal application delivery channels. This is because delivery pipelines
    often have many points of access, and those points may not have been designed
    or configured to ensure confidentiality of data. Organizations avoid those problems
    by storing secrets in a secure vault and injecting them right at the final moment
    of application delivery using specialized tooling. These tools enable applications
    to access their secrets only in the runtime environment.
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大多数组织在尝试交付秘密而不通过正常的应用程序交付渠道暴露它们时都会放弃。这是因为交付管道通常有很多访问点，而这些点可能没有被设计或配置来确保数据的机密性。组织通过在安全的保险库中存储秘密并在应用程序交付的最后时刻使用专用工具注入它们来避免这些问题。这些工具使应用程序只能在运行时环境中访问其秘密。
- en: '[Figure 12.6](#filepos1309242) illustrates the flow of application configuration
    and secret data through application artifacts.'
  id: totrans-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[图 12.6](#filepos1309242) 展示了应用程序配置和秘密数据通过应用程序工件流动的过程。'
- en: Figure 12.6\. The first secret problem
  id: totrans-328
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 12.6\. 第一秘密问题
- en: '![](images/00018.jpg)'
  id: totrans-329
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00018.jpg)'
- en: If an application is started without secret information such as a password credential
    to authenticate to the secret vault, how can the vault authorize access to the
    application’s secrets? It can’t. This is the First Secret Problem. The application
    needs help bootstrapping the chain of trust that will allow it to retrieve its
    secrets. Fortunately, Docker’s design for clustering, services, and secret management
    solves this problem, as illustrated in [figure 12.7](#filepos1310342).
  id: totrans-330
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果一个应用程序在没有秘密信息（如密码凭证）的情况下启动，用于认证秘密保险库，那么保险库如何授权访问应用程序的秘密？它不能。这就是第一秘密问题。应用程序需要帮助启动信任链，以便它能够检索其秘密。幸运的是，Docker
    的集群、服务和秘密管理设计解决了这个问题，如图 12.7 所示。
- en: Figure 12.7\. The Docker swarm cluster’s chain of trust
  id: totrans-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 12.7\. Docker Swarm 集群的信任链
- en: '![](images/00052.jpg)'
  id: totrans-332
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00052.jpg)'
- en: The core of the First Secret Problem is one of identity. In order for a secret
    vault to authorize access to a given secret, it must first authenticate the identity
    of the requestor. Fortunately, Docker Swarm includes a secure secret vault and
    solves the trust bootstrapping problem for you. The Swarm secret vault is tightly
    integrated with the cluster’s identity and service management functions that are
    using secure communication channels. The Docker service ID serves as the application’s
    identity. Docker Swarm uses the service ID to determine which secrets the service’s
    tasks should have access to. When you manage application secrets with Swarm’s
    vault, you can be confident that only a person or process with administrative
    control of the Swarm cluster can provision access to a secret.
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一秘密问题的核心是身份认证。为了一个秘密保险库授权访问特定的秘密，它必须首先验证请求者的身份。幸运的是，Docker Swarm 包含一个安全的秘密保险库，并为你解决了信任初始化问题。Swarm
    秘密保险库与集群的身份和安全管理功能紧密集成，这些功能使用安全的通信渠道。Docker 服务 ID 作为应用程序的身份。Docker Swarm 使用服务
    ID 来确定服务任务应该访问哪些秘密。当你使用 Swarm 的保险库管理应用程序的秘密时，你可以确信只有拥有 Swarm 集群管理控制权的人或进程才能提供秘密的访问权限。
- en: Docker’s solution for deploying and operating services is built on a strong
    and cryptographically sound foundation. Each Docker service has an identity, and
    so does each task container running in support of that service. All of these tasks
    run on top of Swarm nodes that also have unique identities. The Docker secret
    management functionality is built on top of this foundation of identity. Every
    task has an identity that is associated with a service. The service definition
    references the secrets that the service and task needs. Since service definitions
    can be modified by only an authorized user of Docker on the manager node, Swarm
    knows which secrets a service is authorized to use. Swarm can then deliver those
    secrets to nodes that will run the service’s tasks.
  id: totrans-334
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 部署和运营服务的解决方案建立在强大且加密学上可靠的基础上。每个 Docker 服务都有一个身份，支持该服务的每个任务容器也是如此。所有这些任务都在具有独特身份的
    Swarm 节点上运行。Docker 密钥管理功能建立在这样一个身份基础之上。每个任务都有一个与服务关联的身份。服务定义引用了服务和任务所需的密钥。由于服务定义只能由管理节点上的授权
    Docker 用户修改，Swarm 知道哪些密钥服务被授权使用。然后 Swarm 可以将这些密钥传递给将运行服务任务的节点。
- en: '|    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Note
  id: totrans-336
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: The Docker swarm clustering technology implements an advanced, secure design
    to maintain a secure, highly available, and optimally performing control plane.
    You can learn more about this design and how Docker implements it at [https://www.docker.com/blog/least-privilege-container-orchestration/](https://www.docker.com/blog/least-privilege-container-orchestration/).
  id: totrans-337
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 集群技术实现了一个高级、安全的设计，以维护一个安全、高可用性和性能最优的控制平面。您可以在[https://www.docker.com/blog/least-privilege-container-orchestration/](https://www.docker.com/blog/least-privilege-container-orchestration/)了解更多关于这种设计以及
    Docker 如何实现它的信息。
- en: '|    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Docker services solve the First Secret Problem by using Swarm’s built-in identity
    management features to establish trust rather than relying on a secret passed
    via another channel to authenticate the application’s access to its secrets.
  id: totrans-339
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 服务通过使用 Swarm 内置的身份管理功能来建立信任，而不是依赖于通过另一个通道传递的密钥来验证应用程序对其密钥的访问，从而解决了第一个密钥问题。
- en: 12.3.1\. Using Docker secrets
  id: totrans-340
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12.3.1\. 使用 Docker 密钥
- en: Using Docker secret resources is similar to using Docker config resources, with
    a few adjustments.
  id: totrans-341
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Docker 密钥资源类似于使用 Docker 配置资源，只需进行一些调整。
- en: Again, Docker provides secrets to applications as files mounted to a container-specific,
    in-memory, read-only `tmpfs` filesystem. By default, secrets will be placed into
    the container’s filesystem in the /run/secrets directory. This method of delivering
    secrets to applications avoids several leakage problems inherent with providing
    secrets to applications as environment variables.
  id: totrans-342
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 再次强调，Docker 将密钥以文件形式提供给应用程序，这些文件挂载到容器特定的、内存中的只读 `tmpfs` 文件系统。默认情况下，密钥将被放置在容器的文件系统中的
    `/run/secrets` 目录下。将密钥传递给应用程序的这种方法避免了将密钥作为环境变量提供时固有的多个泄露问题。
- en: '|    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Problems with secrets as environment variables
  id: totrans-344
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为环境变量的密钥问题
- en: 'The most important and common problems with using environment variables as
    secret transfer mechanisms are as follows:'
  id: totrans-345
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用环境变量作为密钥传输机制的最重要和常见问题如下：
- en: You can’t assign access-control mechanisms to an environment variable.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无法将访问控制机制分配给环境变量。
- en: This means any process executed by the application will likely have access to
    those env vars. To illustrate this, think about what it might mean for an application
    that does image resizing via ImageMagick to execute resizing operations with untrusted
    input in the environment containing the parent application’s secrets. If the environment
    contains API keys in well-known locations, as is common with cloud providers,
    those secrets could be stolen easily. Some languages and libraries will help you
    prepare a safe process execution environment, but your mileage will vary.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着任何由应用程序执行的过程都可能访问这些环境变量。为了说明这一点，考虑一下，如果一个应用程序通过 ImageMagick 进行图像缩放，并且使用包含父应用程序密钥的环境中的不受信任输入执行缩放操作，这可能意味着什么。如果环境包含在知名位置（如云提供商常见的那样）的
    API 密钥，这些密钥可能很容易被盗。一些语言和库可以帮助您准备一个安全的过程执行环境，但效果可能会有所不同。
- en: Many applications will print all of their environment variables to standard
    out when issued a debugging command or when they crash. This means you may expose
    secrets in your logs on a regular basis.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多应用程序在接收到调试命令或崩溃时，会将其所有环境变量打印到标准输出。这意味着您可能会定期在日志中暴露密钥。
- en: '|    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Now let’s examine how to tell an application where a secret or configuration
    file has been placed into a container; see [figure 12.8](#filepos1321364).
  id: totrans-350
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在让我们看看如何告诉应用程序秘密或配置文件在容器中的放置位置；参见[图12.8](#filepos1321364)。
- en: Figure 12.8\. Provide location of secret file to read as environment variable
  id: totrans-351
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图12.8\. 提供作为环境变量读取的秘密文件的位置
- en: '![](images/00026.jpg)'
  id: totrans-352
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00026.jpg)'
- en: When applications read secrets from files, we often need to specify the location
    of that file at startup. A common pattern for solving this problem is to pass
    the location of the file containing a secret, such as a password to the application,
    as an environment variable. The location of the secret in the container is not
    sensitive information, and only processes running inside the container will have
    access to that file, file permissions permitting. The application can read the
    file specified by the environment variable to load the secret. This pattern is
    also useful for communicating the location of configuration files.
  id: totrans-353
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当应用程序从文件中读取秘密时，我们通常需要在启动时指定该文件的位置。解决此问题的常见模式是将包含秘密（如密码）的文件位置作为环境变量传递给应用程序。容器中秘密的位置不是敏感信息，并且只有运行在容器内的进程才能访问该文件，前提是文件权限允许。应用程序可以通过读取由环境变量指定的文件来加载秘密。这种模式也适用于传达配置文件的位置。
- en: Let’s work through an example; we’ll provide the `greetings` service with a
    TLS certificate and private key so that it can start a secure HTTPS listener.
    We will store the certificate’s private key as a Docker secret and the public
    key as a config. Then we will provide those resources to the `greeting` service’s
    production service configuration. Finally, we will specify the location of the
    files to the `greetings` service via an environment variable so that it knows
    where to load those files from.
  id: totrans-354
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '让我们通过一个例子来分析；我们将为`greetings`服务提供一个TLS证书和私钥，以便它可以启动一个安全的HTTPS监听器。我们将证书的私钥存储为Docker秘密，并将公钥作为配置。然后我们将这些资源提供给`greeting`服务的生产服务配置。最后，我们将通过环境变量指定文件的位置给`greetings`服务，以便它知道从哪里加载这些文件。 '
- en: Now we will deploy a new instance of the `greetings` service with the stack
    configured for production. The deployment command is similar to the one you ran
    previously. However, the production deployment includes an additional `--compose-file`
    option to incorporate the deployment configuration in docker-compose.prod.yml.
    The second change is to deploy the stack by using the name `greetings_prod` instead
    of `greetings_dev`.
  id: totrans-355
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在我们将部署一个配置为生产的`greetings`服务的新实例。部署命令与您之前运行的命令类似。然而，生产部署包括一个额外的`--compose-file`选项，用于将部署配置包含在`docker-compose.prod.yml`中。第二个更改是使用名称`greetings_prod`而不是`greetings_dev`来部署堆栈。
- en: 'Run this `docker stack deploy` command now:'
  id: totrans-356
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在运行这个`docker stack deploy`命令：
- en: '`DEPLOY_ENV=prod docker stack deploy --compose-file docker-compose.yml \  
    --compose-file docker-compose.prod.yml \   greetings_prod`'
  id: totrans-357
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`DEPLOY_ENV=prod docker stack deploy --compose-file docker-compose.yml \ --compose-file
    docker-compose.prod.yml \ greetings_prod`'
- en: 'You should see some output:'
  id: totrans-358
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您应该会看到一些输出：
- en: '`Creating network greetings_prod_default Creating config greetings_prod_env_specific_config
    service api: secret not found: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`'
  id: totrans-359
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Creating network greetings_prod_default Creating config greetings_prod_env_specific_config
    service api: secret not found: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`'
- en: 'The deployment fails because the `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`
    secret resource is not found. Let’s examine docker-compose.prod.yml and determine
    why this is happening. Here are the contents of that file:'
  id: totrans-360
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部署失败是因为找不到`ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`秘密资源。让我们检查`docker-compose.prod.yml`文件并确定这是为什么。以下是该文件的内容：
- en: '`version: ''3.7'' configs:   ch12_greetings_svc-prod-TLS_CERT_V1:     external:
    true  secrets:   ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1:     external: true  services:   
    api:       environment:          CERT_PRIVATE_KEY_FILE: ''/run/secrets/cert_private_key.pem''
            CERT_FILE: ''/config/svc.crt''       configs:         - source: ch12_greetings_svc-prod-TLS_CERT_V1
              target: /config/svc.crt           uid: ''1000''           gid: ''1000''
              mode: 0400       secrets:         - source: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1
              target: cert_private_key.pem           uid: ''1000''           gid:
    ''1000''           mode: 0400`'
  id: totrans-361
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: ''3.7'' configs:    ch12_greetings_svc-prod-TLS_CERT_V1:      external:
    true  secrets:    ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1:      external: true  services:     api:       environment:         CERT_PRIVATE_KEY_FILE:
    ''/run/secrets/cert_private_key.pem''         CERT_FILE: ''/config/svc.crt''         configs:         -
    source: ch12_greetings_svc-prod-TLS_CERT_V1           target: /config/svc.crt          
    uid: ''1000''           gid: ''1000''           mode: 0400         secrets:         -
    source: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1           target: cert_private_key.pem          
    uid: ''1000''           gid: ''1000''           mode: 0400`'
- en: 'There is a top-level `secrets` key that defines a secret named `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`,
    the same as what was reported in the error. The secret definition has a key we
    have not seen before, `external: true`. This means that the value of the secret
    is not defined by this deployment definition, which is prone to leakage. Instead,
    the `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1` secret must be created by a Swarm
    cluster administrator using the `docker` CLI. Once the secret is defined in the
    cluster, this application deployment can reference it.'
  id: totrans-362
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '存在一个顶层 `secrets` 键，定义了一个名为 `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1` 的秘密，这与错误报告中报告的内容相同。秘密定义中有一个我们之前未见过的键，`external:
    true`。这意味着秘密的值不是由这个部署定义定义的，这容易导致泄露。相反，必须由 Swarm 集群管理员使用 `docker` CLI 创建 `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`
    秘密。一旦在集群中定义了秘密，这个应用程序部署就可以引用它。'
- en: 'Let’s define the secret now by running the following command:'
  id: totrans-363
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在让我们通过运行以下命令来定义秘密：
- en: '`$ cat api/config/insecure.key | \     docker secret create ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1
    - vnyy0gr1a09be0vcfvvqogeoj`'
  id: totrans-364
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ cat api/config/insecure.key | \    docker secret create ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1
    - vnyy0gr1a09be0vcfvvqogeoj`'
- en: 'The `docker secret create` command requires two arguments: the name of the
    secret and the value of that secret. The value may be specified either by providing
    the location of a file, or by using the `–` (hyphen) character to indicate that
    the value will be provided via standard input. This shell command demonstrates
    the latter form by printing the contents of the example TLS certificate’s private
    key, `insecure.key`, into the `docker secret create` command. The command completes
    successfully and prints the ID of the secret: `vnyy0gr1a09be0vcfvvqogeoj`.'
  id: totrans-365
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker secret create` 命令需要两个参数：秘密的名称和该秘密的值。值可以通过提供文件的路径来指定，或者使用 `–`（连字符）字符来指示值将通过标准输入提供。这个
    shell 命令通过将示例 TLS 证书的私钥 `insecure.key` 的内容打印到 `docker secret create` 命令中，展示了后者的形式。命令成功完成并打印了秘密的
    ID：`vnyy0gr1a09be0vcfvvqogeoj`。'
- en: '|    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Warning
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告
- en: Do not use this certificate and private key for anything but working through
    these examples. The private key has not been kept confidential and thus cannot
    protect your data effectively.
  id: totrans-368
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不要将此证书和私钥用于除了处理这些示例之外的其他任何用途。私钥没有被保密，因此不能有效地保护你的数据。
- en: '|    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: 'Use the `docker secret inspect` command to view details about the secret resource
    that Docker created:'
  id: totrans-370
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `docker secret inspect` 命令来查看 Docker 创建的秘密资源的详细信息：
- en: '`$ docker secret inspect ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1 [     {
            "ID": "vnyy0gr1a09be0vcfvvqogeoj",         "Version": {             "Index":
    2172         },          "CreatedAt": "2019-04-17T22:04:19.3078685Z",        
    "UpdatedAt": "2019-04-17T22:04:19.3078685Z",         "Spec": {             "Name":
    "ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1",             "Labels": {}        
    }     } ]`'
  id: totrans-371
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker secret inspect ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1 [     {       "ID":
    "vnyy0gr1a09be0vcfvvqogeoj",       "Version": {           "Index": 2172       },          
    "CreatedAt": "2019-04-17T22:04:19.3078685Z",           "UpdatedAt": "2019-04-17T22:04:19.3078685Z",          
    "Spec": {             "Name": "ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1",            
    "Labels": {}         }       } ]`'
- en: Notice that there is no `Data` field as there was with a config resource. The
    secret’s value is not available via the Docker tools or Docker Engine API. The
    secret’s value is guarded closely by the Docker Swarm control plane. Once you
    load a secret into Swarm, you cannot retrieve it by using the `docker` CLI. The
    secret is available only to services that use it. You may also notice that the
    secret’s spec does not contain any labels, as it is managed outside the scope
    of a stack.
  id: totrans-372
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，与配置资源不同，这里没有`Data`字段。密钥的值无法通过Docker工具或Docker Engine API获取。密钥的值被Docker Swarm控制平面严格保护。一旦将密钥加载到Swarm中，就无法使用`docker`
    CLI检索它。密钥仅对使用它的服务可用。您可能还会注意到，密钥的规范不包含任何标签，因为它是在堆栈范围之外管理的。
- en: 'When Docker creates containers for the `greetings` service, the secret will
    be mapped into the container in a way that is almost identical to the process
    we already described for config resources. Here is the relevant section from the
    docker-compose.prod.yml file:'
  id: totrans-373
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当Docker为`greetings`服务创建容器时，密钥将按照与我们之前描述的配置资源的过程几乎相同的方式映射到容器中。以下是`docker-compose.prod.yml`文件中的相关部分：
- en: '`services:     api:       environment:         CERT_PRIVATE_KEY_FILE: ''/run/secrets/cert_private_key.pem''
            CERT_FILE: ''/config/svc.crt''       secrets:         - source: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1
              target: cert_private_key.pem           uid: ''1000''           gid:
    ''1000''           mode: 0400       # ... snip ...`'
  id: totrans-374
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`services:    api:        environment:            CERT_PRIVATE_KEY_FILE: ''/run/secrets/cert_private_key.pem''            CERT_FILE:
    ''/config/svc.crt''            secrets:            - source: ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1            target:
    cert_private_key.pem            uid: ''1000''            gid: ''1000''            mode:
    0400        # ... 省略 ...`'
- en: The `ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1` secret will be mapped into
    the container, in the file `cert_private_key.pem`. The default location for secret
    files is /run/secrets/. This application looks for the location of its private
    key and certificate in environment variables, so those are also defined with fully
    qualified paths to the files. For example, the `CERT_PRIVATE_KEY_FILE` environment
    variable’s value is set to `/run/secrets/cert_private_key.pem`.
  id: totrans-375
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ch12_greetings-svc-prod-TLS_PRIVATE_KEY_V1`密钥将被映射到容器中，文件名为`cert_private_key.pem`。密钥文件的默认位置是/run/secrets/。此应用程序在其私钥和证书的位置中查找环境变量，因此这些变量也使用文件的完全限定路径定义。例如，`CERT_PRIVATE_KEY_FILE`环境变量的值设置为`/run/secrets/cert_private_key.pem`。'
- en: 'The production `greetings` application also depends on a `ch12_greetings_svc-prod-TLS_CERT_V1`
    config resource. This config resource contains the public, nonsensitive, x.509
    certificate the `greetings` application will use to offer HTTPS services. The
    private and public keys of an x.509 certificate change together, which is why
    these secret and config resources are created as a pair. Define the certificate’s
    config resource now by running the following command:'
  id: totrans-376
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生产`greetings`应用程序还依赖于`ch12_greetings_svc-prod-TLS_CERT_V1`配置资源。此配置资源包含`greetings`应用程序将用于提供HTTPS服务的公共、非敏感的x.509证书。x.509证书的私钥和公钥一起更改，这就是为什么这些密钥和配置资源作为一对创建的原因。现在通过运行以下命令定义证书的配置资源：
- en: '`$ docker config create \     ch12_greetings_svc-prod-TLS_CERT_V1 api/config/insecure.crt
    5a1lybiyjnaseg0jlwj2s1v5m`'
  id: totrans-377
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker config create \    ch12_greetings_svc-prod-TLS_CERT_V1 api/config/insecure.crt
    5a1lybiyjnaseg0jlwj2s1v5m`'
- en: The `docker config create` command works like the secret creation command. In
    particular, a config resource can be created by specifying the path to a file,
    as we have done here with `api/config/insecure.crt`. The command completed successfully
    and printed the new config resource’s unique ID, `5a1lybiyjnaseg0jlwj2s1v5m`.
  id: totrans-378
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker config create`命令与密钥创建命令类似。特别是，可以通过指定文件的路径来创建配置资源，就像我们在这里使用`api/config/insecure.crt`所做的那样。命令成功完成并打印了新的配置资源的唯一ID，`5a1lybiyjnaseg0jlwj2s1v5m`。'
- en: 'Now, rerun the deploy command:'
  id: totrans-379
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，重新运行部署命令：
- en: '`$ DEPLOY_ENV=prod docker stack deploy \     --compose-file docker-compose.yml
    \     --compose-file docker-compose.prod.yml \     greetings_prod Creating service
    greetings_prod_api`'
  id: totrans-380
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ DEPLOY_ENV=prod docker stack deploy \    --compose-file docker-compose.yml
    \    --compose-file docker-compose.prod.yml \    greetings_prod Creating service
    greetings_prod_api`'
- en: 'This attempt should succeed. Run `docker service ps greetings_prod_api` and
    verify that the service has a single task in the running state:'
  id: totrans-381
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这次尝试应该会成功。运行`docker service ps greetings_prod_api`并验证服务是否有一个正在运行的单个任务：
- en: '`ID                NAME              IMAGE                     NODE DESIRED
    STATE        CURRENT STATE           ERROR               PORTS 93fgzy5lmarp       
    greetings_prod_api.1   dockerinaction/ch12_greetings:api docker-desktop      
    Running             Running 2 minutes ago`'
  id: totrans-382
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ID                NAME              IMAGE                     NODE DESIRED
    STATE        CURRENT STATE           ERROR               PORTS 93fgzy5lmarp       
    greetings_prod_api.1   dockerinaction/ch12_greetings:api docker-desktop      
    运行中             运行中 2分钟前`'
- en: 'Now that the production stack is deployed, we can check the service’s logs
    to see whether it found the TLS certificate and private key:'
  id: totrans-383
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在生产栈已部署，我们可以检查服务的日志以查看是否找到了TLS证书和私钥：
- en: '`docker service logs --since 1m greetings_prod_api`'
  id: totrans-384
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service logs --since 1m greetings_prod_api`'
- en: 'That command will print the `greetings` service application logs, which should
    look like this:'
  id: totrans-385
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该命令将打印`greetings`服务应用程序的日志，其外观应如下所示：
- en: '`Initializing greetings api server for deployment environment prod Will read
    TLS certificate private key from      ''/run/secrets/cert_private_key.pem'' chars
    in certificate private key 3272 Will read TLS certificate from ''/config/svc.crt''
    chars in TLS certificate 1960 Loading env-specific configurations from /config/config.common.yml
    Loading env-specific configurations from /config/config.prod.yml Greetings: [Hello
    World! Hola Mundo! Hallo Welt!] Initialization complete Starting https listener
    on :8443`'
  id: totrans-386
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`初始化prod部署环境的greetings api服务器 将从''/run/secrets/cert_private_key.pem''读取TLS证书私钥
    私钥证书中的字符数 3272 将从''/config/svc.crt''读取TLS证书 TLS证书中的字符数 1960 从/config/config.common.yml加载环境特定配置
    从/config/config.prod.yml加载环境特定配置 问候：[Hello World! Hola Mundo! Hallo Welt!] 初始化完成
    在:8443启动https监听器`'
- en: Indeed, the `greetings` application found the private key at /run/secrets/cert_private_
    key.pem and reported that the file has 3,272 characters in it. Similarly, the
    certificate has 1,960 characters. Finally, the `greetings` application reported
    that it is starting a listener for HTTPS traffic on port 8443 inside the container.
  id: totrans-387
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 的确，`greetings`应用程序在`/run/secrets/cert_private_key.pem`找到了私钥，并报告该文件包含3,272个字符。同样，证书有1,960个字符。最后，`greetings`应用程序报告它正在容器内部启动一个监听8443端口HTTPS流量的监听器。
- en: 'Use a web browser to open https://localhost:8443\. The example certificate
    is not issued by a trusted certificate authority, so you will receive a warning.
    If you proceed through that warning, you should see a response from the greetings
    service:'
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用网络浏览器打开https://localhost:8443。示例证书不是由受信任的证书颁发机构签发的，因此你会收到一个警告。如果你通过那个警告，你应该会看到来自greetings服务的响应：
- en: '`Welcome to the Greetings API Server! Container with id 583a5837d629 responded
    at 2019-04-17 22:35:07.3391735 +0000 UTC DEPLOY_ENV: prod`'
  id: totrans-389
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`欢迎使用Greetings API服务器！ID为583a5837d629的容器在2019-04-17 22:35:07.3391735 +0000
    UTC响应 DEPLOY_ENV: prod`'
- en: Woo-hoo! The greetings service is now serving traffic over HTTPS using TLS certificates
    delivered by Docker’s secret management facilities. You can request greetings
    from the service at https://localhost:8443/greeting as you did before. Notice
    that only the three greetings from the common config are served. This is because
    the application’s environment-specific configuration file for prod, config.prod.yml,
    does not add any greetings.
  id: totrans-390
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哇哦！现在greetings服务正在使用Docker的密钥管理设施提供的TLS证书通过HTTPS提供服务。你可以像以前一样在https://localhost:8443/greeting请求问候。注意，只提供了常见配置中的三个问候。这是因为应用程序针对prod环境的特定配置文件config.prod.yml没有添加任何问候。
- en: 'The `greetings` service is now using every form of configuration supported
    by Docker: files included in the application image, environment variables, config
    resources, and secret resources. You’ve also seen how to combine the usage of
    all these approaches to vary application behavior in a secure manner across several
    environments.'
  id: totrans-391
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`greetings`服务现在正在使用Docker支持的每种配置形式：包含在应用程序镜像中的文件、环境变量、配置资源和密钥资源。你也看到了如何结合使用所有这些方法，以安全的方式在多个环境中改变应用程序的行为。'
- en: SUMMARY
  id: totrans-392
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter described the core challenges of varying application behavior
    at deployment time instead of build time. We explored how you can model this variation
    with Docker’s configuration abstractions. The example application demonstrated
    using Docker’s config and secret resources to vary its behavior across environments.
    This culminated in a Docker Service serving traffic over https with an environment-specific
    dataset. The key points to understand from this chapter are:'
  id: totrans-393
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章描述了在部署时间而不是构建时间改变应用程序行为的核心挑战。我们探讨了如何使用Docker的配置抽象来模拟这种变化。示例应用程序展示了如何使用Docker的配置和秘密资源来改变其在不同环境中的行为。这最终导致了一个通过https提供流量并具有特定环境数据集的Docker服务。本章需要理解的关键点包括：
- en: Applications often must adapt their behavior to the environment they are deployed
    into.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序通常必须适应它们部署到的环境。
- en: Docker config and secret resources can be used to model and adapt application
    behavior to various deployment needs.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker配置和秘密资源可用于模拟和适应应用程序的各种部署需求。
- en: Secrets are a special kind of configuration data that is challenging to handle
    safely.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秘密是难以安全处理的一种特殊类型的配置数据。
- en: Docker Swarm establishes a chain of trust and uses Docker service identities
    to ensure that secrets are delivered correctly and securely to applications.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm建立信任链，并使用Docker服务身份来确保秘密被正确且安全地传递给应用程序。
- en: Docker provides config and secrets to services as files on a container-specific
    `tmpfs` filesystem that applications can read at startup.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker将配置和秘密作为容器特定的`tmpfs`文件系统上的文件提供给服务，应用程序可以在启动时读取。
- en: Deployment processes must use a naming scheme for config and secret resources
    that enables automation to update services.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署流程必须使用命名方案为配置和秘密资源命名，以便自动化更新服务。
- en: Chapter 13\. Orchestrating services on a cluster of Docker hosts with Swarm
  id: totrans-400
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第13章\. 使用Swarm在Docker主机集群上编排服务
- en: This chapter covers
  id: totrans-401
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How Docker application deployments work and options
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker应用程序部署的工作原理和选项
- en: Deploying a multitier application to Docker Swarm
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多层应用程序部署到Docker Swarm
- en: How Swarm attempts to converge the Docker application deployment to the desired
    state declared by operators
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm如何尝试将Docker应用程序部署收敛到操作者声明的所需状态
- en: How Swarm ensures the desired number of replicas are running around the cluster
    within the declared placement and resource constraints
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm如何确保在声明的放置和资源约束内，集群中运行着所需数量的副本
- en: Routing of request traffic from a cluster node to network service instances
    and how collaborating services reach each other using Docker networks
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从集群节点到网络服务实例的请求流量路由以及协作服务如何使用Docker网络相互连接
- en: Controlling placement of Docker Service containers within the cluster
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制Docker服务容器在集群中的放置
- en: 13.1\. CLUSTERING WITH DOCKER SWARM
  id: totrans-408
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.1\. 使用Docker Swarm进行集群
- en: Application developers and operators frequently deploy services onto multiple
    hosts to achieve greater availability and scalability. When an application is
    deployed across multiple hosts, the redundancy in the application’s deployment
    provides capacity that can serve requests when a host fails or is removed from
    service. Deploying across multiple hosts also permits the application to use more
    compute resources than any single host can provide.
  id: totrans-409
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用程序开发者和运维人员经常将服务部署到多个主机上，以实现更高的可用性和可伸缩性。当应用程序部署到多个主机上时，应用程序部署的冗余提供了容量，可以在主机失败或从服务中移除时处理请求。跨多个主机部署还允许应用程序使用比单个主机能提供的更多计算资源。
- en: For example, say you run an e-commerce site that usually performs well on a
    single host, but is a slow during big promotions that drive peak load twice as
    high as normal. This site might benefit from being redeployed onto three hosts.
    Then you should have enough capacity to handle peak traffic even if one host fails
    or is out of service for an upgrade. In this chapter, we will show you how to
    model and deploy a web API across a cluster of hosts managed with Swarm.
  id: totrans-410
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，假设你运行一个电子商务网站，该网站通常在单个主机上表现良好，但在大型促销活动期间（将峰值负载提高两倍）运行缓慢。这个网站可能从重新部署到三个主机中受益。然后你应该有足够的容量来处理峰值流量，即使一个主机失败或因升级而停机。在本章中，我们将向你展示如何使用Swarm在主机集群上模拟和部署一个Web
    API。
- en: Docker Swarm provides a sophisticated platform for deploying and operating a
    containerized application across a set of Docker hosts. Docker’s deployment tooling
    automates the process for deploying a new Docker service to the cluster or changes
    to an existing service. Service configuration changes may include anything declared
    in the service definition (docker-compose.yml) such as image, container command,
    resource limits, exposed ports, mounts, and consumed secrets. Once deployed, Swarm
    supervises the application so that problems are detected and repaired. Additionally,
    Swarm routes requests from the application’s users to the service’s containers.
  id: totrans-411
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm提供了一个复杂的平台，用于在多个Docker主机上部署和运行容器化应用程序。Docker的部署工具自动化了将新的Docker服务部署到集群或对现有服务进行更改的过程。服务配置更改可能包括在服务定义（docker-compose.yml）中声明的任何内容，例如镜像、容器命令、资源限制、暴露的端口、挂载和消耗的秘密。一旦部署，Swarm将监督应用程序，以便检测和修复问题。此外，Swarm将应用程序用户的请求路由到服务的容器中。
- en: In this chapter, we will examine how Docker Swarm supports each of these functions.
    We will build on the service, configuration, and secret resources explored in
    [chapters 11](index_split_092.html#filepos1146874) and [12](index_split_098.html#filepos1254019).
    We will also leverage your fundamental knowledge of Docker containers ([chapter
    2](index_split_024.html#filepos153412)), resource limits ([chapter 6](index_split_053.html#filepos544857)),
    and networking ([chapter 5](index_split_046.html#filepos458921)).
  id: totrans-412
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Docker Swarm如何支持这些功能中的每一个。我们将基于第11章和第12章中探讨的服务、配置和秘密资源。我们还将利用你对Docker容器（第2章）、资源限制（第6章）和网络（第5章）的基本知识。
- en: 13.1.1\. Introducing Docker Swarm mode
  id: totrans-413
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.1.1\. Docker Swarm模式的介绍
- en: Docker Swarm is a clustering technology that connects a set of hosts running
    Docker and lets you run applications built using Docker services across those
    machines. Swarm orchestrates the deployment and operation of Docker services across
    the collection of machines. Swarm schedules tasks according to the application’s
    resource requirements and machine capabilities. The Swarm clustering software
    is included in the Docker Engine and command-line tool. You can enable Swarm mode
    and start using Swarm without installing any additional components. [Figure 13.1](#filepos1352211)
    shows how the components of a Docker Swarm deployment relate to each other and
    how the machines of the cluster collaborate to run applications.
  id: totrans-414
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm是一种集群技术，它连接了一组运行Docker的主机，并允许你在这些机器上运行使用Docker服务构建的应用程序。Swarm协调Docker服务在机器集合上的部署和运行。Swarm根据应用程序的资源需求和机器能力来调度任务。Swarm集群软件包含在Docker
    Engine和命令行工具中。你可以启用Swarm模式并开始使用Swarm，而无需安装任何额外的组件。[图13.1](#filepos1352211)显示了Docker
    Swarm部署的组件如何相互关联以及集群的机器如何协作运行应用程序。
- en: Figure 13.1\. Swarm cluster deployment
  id: totrans-415
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![图13.1\. Swarm集群部署](#filepos1352211)'
- en: '![](images/00012.jpg)'
  id: totrans-416
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00012.jpg)'
- en: When you join a Docker Engine to a Swarm cluster, you specify whether that machine
    should be a manager or a worker. Managers listen for instructions to create, change,
    or remove definitions for entities such as Docker services, configuration, and
    secrets. Managers instruct worker nodes to create containers and volumes that
    implement Docker service instances. Managers continuously converge the cluster
    to the state you have declared it should be in. The control plane connecting the
    cluster’s Docker Engines depicts the communication of the desired cluster state
    and events related to realizing that state. Clients of a Docker service may send
    requests to any node of the cluster on the port published for that service. Swarm’s
    network mesh will route the request from whichever node received the request to
    a healthy service container that can handle it. Swarm deploys and manages lightweight,
    dedicated load-balancer and network routing components to receive and transport
    network traffic for each published port. [Section 13.3.1](index_split_106.html#filepos1426043)
    explains the Swarm network mesh in detail. Let’s deploy a cluster to work through
    the examples in this chapter.
  id: totrans-417
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您将Docker Engine加入Swarm集群时，您指定该机器应该是管理器还是工作者。管理器监听创建、更改或删除实体（如Docker服务、配置和机密）定义的指令。管理器指示工作节点创建实现Docker服务实例的容器和卷。管理器持续地将集群收敛到您声明的状态。连接集群Docker
    Engine的控制平面描述了所需集群状态的通信以及实现该状态相关的事件。Docker服务的客户端可以向集群中任何发布该服务的端口发送请求。Swarm的网络网格将路由从接收请求的任何节点到可以处理该请求的健康服务容器的请求。Swarm部署和管理轻量级、专用的负载均衡器和网络路由组件，以接收和传输每个发布端口的网络流量。[第13.3.1节](index_split_106.html#filepos1426043)详细解释了Swarm网络网格。让我们部署一个集群来处理本章中的示例。
- en: Swarm clusters can be deployed in many topologies. Each cluster has at least
    one manager to safeguard cluster state and orchestrate services across workers.
    Swarm managers require a majority of the managers to be available in order to
    coordinate and record a change to the cluster. Most production Swarm deployments
    should have three or five nodes in the manager role. Increasing the number of
    managers will improve availability of the Swarm control plane, but will also increase
    the time it takes for managers to acknowledge a change to the cluster. See the
    Swarm Admin Guide for a detailed explanation of the trade-offs ([https://docs.docker.com/engine/swarm/admin_guide/](https://docs.docker.com/engine/swarm/admin_guide/)).
    Swarm clusters can scale reliably to hundreds of worker nodes. The community has
    demonstrated tests of a single Swarm with thousands of worker nodes (see the Swarm3K
    project at [https://dzone.com/articles/docker-swarm-lessons-from-swarm3k](https://dzone.com/articles/docker-swarm-lessons-from-swarm3k)).
  id: totrans-418
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm集群可以部署在多种拓扑结构中。每个集群至少有一个管理器来保护集群状态并协调跨工作者的服务。Swarm管理器需要大多数管理器可用，以便协调并记录对集群的更改。大多数生产级Swarm部署应具有三个或五个节点担任管理角色。增加管理器的数量将提高Swarm控制平面的可用性，但也会增加管理器确认集群更改所需的时间。有关权衡的详细说明，请参阅Swarm管理员指南（[https://docs.docker.com/engine/swarm/admin_guide/](https://docs.docker.com/engine/swarm/admin_guide/))。Swarm集群可以可靠地扩展到数百个工作节点。社区已经展示了单个Swarm集群包含数千个工作节点的测试（请参阅[https://dzone.com/articles/docker-swarm-lessons-from-swarm3k](https://dzone.com/articles/docker-swarm-lessons-from-swarm3k)上的Swarm3K项目）。
- en: Swarm, the native clustered application deployment option provided by Docker,
    supports the Docker application model well. Many people will find Swarm simpler
    to deploy, use, and manage than other container clustering technologies. You may
    find it useful to deploy small Swarm clusters for an individual team or project.
    A large Swarm cluster can be partitioned into multiple zones by using labels,
    and then you can place service instances into the proper zone by using scheduling
    constraints. You can label cluster resources with metadata meaningful to your
    organization such as `environment=dev` or `zone=private` so the cluster’s actual
    management model matches your own terminology.
  id: totrans-419
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm是Docker提供的原生集群应用程序部署选项，很好地支持Docker应用程序模型。许多人会发现Swarm比其他容器集群技术更容易部署、使用和管理。您可能会发现为单个团队或项目部署小型Swarm集群很有用。您可以使用标签将大型Swarm集群划分为多个区域，然后通过使用调度约束将服务实例放置到适当的区域。您可以使用对组织有意义的元数据（如`environment=dev`或`zone=private`）对集群资源进行标记，以便集群的实际管理模型与您的术语相匹配。
- en: 13.1.2\. Deploying a Swarm cluster
  id: totrans-420
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.1.2\. 部署Swarm集群
- en: You have many options for building a swarm from a cluster of nodes. The examples
    in this chapter use a Swarm cluster with five nodes, though most of the examples
    work on a single node, such as Docker for Mac. You may provision a Swarm cluster
    however you like. Because of the wide variety of provisioning options and the
    rate of change, we recommend you follow an up-to-date guide to provision a Swarm
    cluster on your favorite infrastructure provider. Many people deploy test clusters
    with `docker-machine` on cloud providers such as DigitalOcean and Amazon Web Services.
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您有很多选项可以从节点集群构建swarm。本章的示例使用具有五个节点的Swarm集群，尽管大多数示例在单个节点上工作，例如Docker for Mac。您可以按自己的喜好配置Swarm集群。由于配置选项种类繁多且变化迅速，我们建议您遵循最新的指南，在您喜欢的基础设施提供商上配置Swarm集群。许多人使用`docker-machine`在云提供商（如DigitalOcean和Amazon
    Web Services）上部署测试集群。
- en: The examples in this chapter were created and tested using Play with Docker
    ([https://labs.play-with-docker.com/](https://labs.play-with-docker.com/)). On
    the Play with Docker site, you can experiment with Docker and learn about it for
    free. The cluster was created using the Play with Docker template that provisions
    three manager and two worker nodes. You will need at least two workers to complete
    all of the exercises in this chapter.
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章中的示例使用Play with Docker([https://labs.play-with-docker.com/](https://labs.play-with-docker.com/))创建和测试。在Play
    with Docker网站上，您可以免费实验Docker并了解它。集群使用Play with Docker模板创建，该模板配置了三个管理节点和两个工作节点。您至少需要两个工作节点来完成本章的所有练习。
- en: 'The general process for deploying a Swarm cluster is as follows:'
  id: totrans-423
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部署Swarm集群的一般过程如下：
- en: Deploy at least three nodes with Docker Engine installed and running, preferably
    five.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署至少三个已安装并运行Docker Engine的节点，最好是五个。
- en: 'Ensure that network traffic is permitted between the machines on the following
    ports and protocols:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保以下端口和协议之间允许机器之间的网络流量：
- en: TCP port 2377 for cluster management communications
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: TCP端口2377用于集群管理通信
- en: TCP and UDP port 7946 for communication among nodes
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: TCP和UDP端口7946用于节点间的通信
- en: UDP port 4789 for overlay network traffic
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: UDP端口4789用于覆盖网络流量
- en: Initialize a Swarm cluster by running `docker swarm init` on a manager.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在管理节点上运行`docker swarm init`来初始化Swarm集群。
- en: Record the Swarm cluster join tokens or display them again with `docker swarm
    join-token`.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录Swarm集群加入令牌或使用`docker swarm join-token`再次显示它们。
- en: Join the manager and then worker nodes to the cluster with `docker swarm join`.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`docker swarm join`将管理节点和工作节点加入集群。
- en: 13.2\. DEPLOYING AN APPLICATION TO A SWARM CLUSTER
  id: totrans-432
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.2\. 将应用程序部署到Swarm集群
- en: In this section, we will deploy an example web application with a common three-tier
    architecture. The application features a stateless API server connected to a PostgreSQL
    relational database. Both the API server and database will be managed as Docker
    services. The database will use a Docker volume to persist data across restarts.
    The API servers will communicate with the database over a private, secure network.
    This application will demonstrate how the Docker resources you have learned about
    in previous chapters translate to a deployment spanning multiple nodes.
  id: totrans-433
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本节中，我们将部署一个具有常见三层架构的示例Web应用程序。该应用程序具有一个无状态的API服务器，连接到PostgreSQL关系数据库。API服务器和数据库都将作为Docker服务进行管理。数据库将使用Docker卷在重启之间持久化数据。API服务器将通过一个私有、安全的网络与数据库通信。此应用程序将展示您在前面章节中学到的Docker资源如何转换为跨多个节点的部署。
- en: 13.2.1\. Introducing Docker Swarm cluster resource types
  id: totrans-434
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.2.1\. 介绍Docker Swarm集群资源类型
- en: Docker Swarm supports nearly all of the concepts discussed in this book, as
    illustrated in [figure 13.2](#filepos1361205). When using Swarm, these resources
    are defined and managed at the cluster level.
  id: totrans-435
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm支持本书中讨论的几乎所有概念，如图13.2所示。当使用Swarm时，这些资源在集群级别定义和管理。
- en: Figure 13.2\. Docker Swarm resource types
  id: totrans-436
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图13.2\. Docker Swarm资源类型
- en: '![](images/00007.jpg)'
  id: totrans-437
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00007.jpg)'
- en: 'The key Docker Swarm resource types are as follows:'
  id: totrans-438
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm的关键资源类型如下：
- en: Services—  A Docker service defines the application processes that run on the
    Swarm cluster’s nodes. Swarm managers interpret the service definition and create
    tasks that are executed on the cluster’s manager and worker nodes. Services are
    introduced in [chapter 11](index_split_092.html#filepos1146874).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务—— Docker服务定义了在Swarm集群节点上运行的应用程序进程。Swarm管理器解释服务定义并在集群的管理节点和工作节点上创建任务。服务在[第11章](index_split_092.html#filepos1146874)中介绍。
- en: Tasks—  Tasks define a containerized process that Swarm will schedule and run
    once until completion. A task that exits may be replaced by a new task, depending
    on the restart policy defined by the service. Tasks also specify dependencies
    on other cluster resources such as networks and secrets.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务— 任务定义了一个Swarm将调度并运行一次直到完成的过程化容器。一个退出的任务可能会根据服务定义的重启策略被新的任务替换。任务还指定了对其他集群资源（如网络和机密）的依赖。
- en: Networks—  Applications can use Docker overlay networks for traffic between
    services. Docker networks have low overhead, so you can create network topologies
    that suit your desired security model. [Section 13.3.2](index_split_106.html#filepos1438467)
    describes overlay networks.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络— 应用可以使用Docker overlay网络在服务之间进行流量传输。Docker网络开销低，因此你可以创建适合你所需安全模型的网络拓扑。[第13.3.2节](index_split_106.html#filepos1438467)描述了overlay网络。
- en: Volumes—  Volumes provide persistent storage to service tasks. These volumes
    are bound to a single node. Volumes and mounts are described in [chapter 4](index_split_037.html#filepos379268).
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷— 卷为服务任务提供持久存储。这些卷绑定到单个节点。卷和挂载在[第4章](index_split_037.html#filepos379268)中描述。
- en: Configs and secrets—  Configurations and secrets ([chapter 12](index_split_098.html#filepos1254019))
    provide environment-specific configurations to services deployed on the cluster.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和机密— 配置和机密([第12章](index_split_098.html#filepos1254019))为集群上部署的服务提供特定环境的配置。
- en: The example application uses each of these Docker resource types.
  id: totrans-444
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 示例应用使用了这些Docker资源类型中的每一个。
- en: 13.2.2\. Defining an application and its dependencies by using Docker services
  id: totrans-445
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.2.2\. 使用Docker服务定义应用及其依赖
- en: 'The example application we will work with in this chapter is a simple web application
    with three tiers: a load balancer, API server, and PostgreSQL database. We will
    model this application with Docker and then deploy it to our Swarm cluster. Logically,
    the application deployment will look like [figure 13.3](#filepos1365412).'
  id: totrans-446
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章我们将要处理的示例应用是一个简单的三层Web应用，包括负载均衡器、API服务器和PostgreSQL数据库。我们将使用Docker来模拟这个应用，并将其部署到我们的Swarm集群中。从逻辑上讲，应用部署将看起来像[图13.3](#filepos1365412)。
- en: Figure 13.3\. Logical architecture of example application
  id: totrans-447
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图13.3\. 示例应用的逻辑架构
- en: '![](images/00017.jpg)'
  id: totrans-448
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00017.jpg)'
- en: 'The application has an API server with two endpoints: `/` and `/counter`. The
    API service publishes a port to the cluster’s edge that is implemented by Swarm’s
    built-in load balancer. Requests to the `/` endpoint will return information about
    the container that handled the request. The `/counter` endpoint will increment
    an integer with each request. The value of the counter is stored in a PostgreSQL
    database.'
  id: totrans-449
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用有一个API服务器，包含两个端点：`/`和`/counter`。API服务将一个端口发布到集群的边缘，该端口由Swarm的内置负载均衡器实现。对`/`端点的请求将返回处理请求的容器信息。`/counter`端点将在每次请求中增加一个整数。计数器的值存储在PostgreSQL数据库中。
- en: Let’s define the application a piece at a time by using the Docker Compose version
    3 format and synthesize the concepts covered in previous chapters. After that,
    we will deploy it with the `docker stack` command. This application definition
    is available in full at [https://github.com/dockerinaction/ch13_multi_tier_app.git](https://github.com/dockerinaction/ch13_multi_tier_app.git)`.`
    Clone that repo to follow along as the application is explained piece by piece.
  id: totrans-450
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们分步骤使用Docker Compose版本3格式定义应用，并综合前几章中介绍的概念。之后，我们将使用`docker stack`命令部署它。这个应用定义的完整内容可以在[https://github.com/dockerinaction/ch13_multi_tier_app.git](https://github.com/dockerinaction/ch13_multi_tier_app.git)找到。`.
    克隆这个仓库，随着应用的逐步解释来跟随。
- en: 'The application uses two networks, a public network handling requests coming
    from external clients, and a private network that is more trusted. These networks
    are described in the docker-compose.yml application descriptor as follows:'
  id: totrans-451
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用使用了两个网络，一个公共网络处理来自外部客户端的请求，另一个是更受信任的私有网络。这些网络在`docker-compose.yml`应用描述符中如下描述：
- en: '`version: ''3.7''  networks:   public:     driver: overlay     driver_opts:
          encrypted: ''true''   private:     driver: overlay     driver_opts:      
    encrypted: ''true''     attachable: true`'
  id: totrans-452
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`version: ''3.7''  networks:   public:     driver: overlay     driver_opts:       encrypted:
    ''true''   private:     driver: overlay     driver_opts:       encrypted: ''true''       attachable:
    true`'
- en: '|    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-454
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: The value `true` is quoted for `driver_opts` because Docker requires a string
    or number. The value of `true` is unquoted for `attachable` because Docker requires
    a boolean.
  id: totrans-455
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`driver_opts`的值`true`被引号包围，因为Docker需要一个字符串或数字。而`attachable`的值`true`没有被引号包围，因为Docker需要一个布尔值。'
- en: '|    |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Both networks are defined by adding named entries to a top-level networks key
    in the application descriptor. The team that built this application has an end-to-end
    encryption requirement. The application definition satisfies a large portion of
    that requirement by encrypting all of the traffic on the networks used by the
    application. The only remaining work is to secure communications on the service’s
    published port by using TLS. [Section 13.3](index_split_106.html#filepos1425310)
    explains why applications should secure the published ports, and [chapter 12](index_split_098.html#filepos1254019)’s
    `greetings` application showed one way to do this. This highlights an interesting
    feature of Swarm: it is easy to satisfy many transport encryption requirements
    by using a relatively simple, auditable configuration in the deployment descriptor.'
  id: totrans-457
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过在应用程序描述符的顶级`networks`键中添加命名条目，定义了两个网络。构建此应用程序的团队有一个端到端加密的要求。应用程序定义通过加密应用程序使用的所有网络上的流量，满足了该要求的大部分。唯一剩下的工作是通过使用TLS来确保服务发布端口上的通信安全。[第13.3节](index_split_106.html#filepos1425310)解释了为什么应用程序应该确保发布端口的安全，[第12章](index_split_098.html#filepos1254019)的`greetings`应用程序展示了实现这一点的其中一种方法。这突出了Swarm的一个有趣特性：通过在部署描述符中使用相对简单且可审计的配置，可以轻松满足许多传输加密要求。
- en: 'Next, the database needs persistent storage for its data. We define a Docker
    volume under a top-level `volumes` key:'
  id: totrans-458
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 接下来，数据库需要持久存储来保存其数据。我们在顶级`volumes`键下定义一个Docker卷：
- en: '`volumes:   db-data:`'
  id: totrans-459
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`volumes:    db-data:`'
- en: Notice that no options are defined for this volume. Swarm will use Docker’s
    built-in `local` volume driver to create it. The volume will be local to that
    Swarm node and not replicated, backed up, or shared elsewhere. Some Docker volume
    plugins can create and manage volumes that persist and share data across nodes;
    the Docker Cloudstor and REX-Ray plugins are good examples.
  id: totrans-460
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，此卷没有定义任何选项。Swarm将使用Docker内置的`local`卷驱动程序来创建它。该卷将仅限于该Swarm节点，并且不会在其他地方复制、备份或共享。一些Docker卷插件可以创建和管理跨节点持久化和共享数据的卷；Docker
    Cloudstor和REX-Ray插件是很好的例子。
- en: 'Before we move on to the service definition, create a reference to the password
    that will be used by the API to access the PostgreSQL database. The password will
    be configured in the Swarm cluster as one of the first steps of the deployment
    process. Add a top-level `secrets` key that instructs the password to be retrieved
    from the cluster’s secret resources:'
  id: totrans-461
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们继续到服务定义之前，创建一个引用API将用于访问PostgreSQL数据库的密码。密码将在部署过程的最初步骤中将配置在Swarm集群中。添加一个顶级`secrets`键，指示从集群的秘密资源中检索密码：
- en: '`secrets   ch13_multi_tier_app-POSTGRES_PASSWORD:     external: true` `1`'
  id: totrans-462
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`secrets    ch13_multi_tier_app-POSTGRES_PASSWORD:    external: true` `1`'
- en: 1 Retrieves from cluster’s secret resources
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 从集群的秘密资源检索
- en: 'Now we are ready to define the application’s services. Let’s start with the
    database by defining a `postgres` service under a top-level `services` key:'
  id: totrans-464
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在我们已经准备好定义应用程序的服务。让我们从定义顶级`services`键下的`postgres`服务开始：
- en: '`services:   postgres:       image: postgres:9.6.6       networks:        
    - private       volumes:         - db-data:/var/lib/postgresql/data       secrets:
            - source: ch13_multi_tier_app-POSTGRES_PASSWORD` `1` `target: POSTGRES_PASSWORD
              uid: ''999''` `2` `gid: ''999''           mode: 0400       environment:
              POSTGRES_USER: ''exercise''           POSTGRES_PASSWORD_FILE: ''/run/secrets/POSTGRES_PASSWORD''
              POSTGRES_DB: ''exercise''       deploy:           replicas: 1` `3` `update_config:
                order: ''stop-first''           rollback_config:             order:
    ''stop-first''           resources:             limits:                cpus: ''1.00''
                  memory: 50M             reservations:               cpus: ''0.25''
                  memory: 50M`'
  id: totrans-465
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`services:    postgres:        image: postgres:9.6.6        networks:            -
    private            volumes:            - db-data:/var/lib/postgresql/data            secrets:            -
    source: ch13_multi_tier_app-POSTGRES_PASSWORD    `1`    `target: POSTGRES_PASSWORD    uid:
    ''999''    `2`    `gid: ''999''    mode: 0400    environment:            POSTGRES_USER:
    ''exercise''            POSTGRES_PASSWORD_FILE: ''/run/secrets/POSTGRES_PASSWORD''            POSTGRES_DB:
    ''exercise''            deploy:                replicas: 1    `3`    `update_config:                order:
    ''stop-first''                rollback_config:                order: ''stop-first''                resources:                    limits:                        cpus:
    ''1.00''                        memory: 50M                    reservations:                        cpus:
    ''0.25''                        memory: 50M`'
- en: 1 Injects PostgreSQL password from a cluster-managed secret
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 从集群管理的秘密中注入PostgreSQL密码
- en: '2 The postgres user (uid: 999) managed by the container needs to read the file.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 由容器管理的postgres用户（uid：999）需要读取该文件。
- en: 3 Ensures at-most one instance of PostgreSQL by limiting replicas to 1 and stopping
    after first update or rollback failure
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 通过限制副本数量为 1 并在第一次更新或回滚失败后停止，确保最多只有一个 PostgreSQL 实例
- en: This database service will use the official PostgreSQL image to start a database.
    That PostgreSQL container will attach to (only) the `private` network, mount the
    `db-data` volume, and use the `POSTGRES_*` environment variables to initialize
    the database. The `POSTGRES_DB` and `POSTGRES_USER` environment variables determine
    the name of the database and the user we will use to access the database, respectively.
    However, you should avoid providing secrets such as passwords to processes via
    environment variables because they are leaked easily.
  id: totrans-469
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此数据库服务将使用官方的 PostgreSQL 镜像来启动数据库。该 PostgreSQL 容器将连接到（仅）`private` 网络，挂载 `db-data`
    卷，并使用 `POSTGRES_*` 环境变量来初始化数据库。`POSTGRES_DB` 和 `POSTGRES_USER` 环境变量分别决定了数据库的名称和我们将用于访问数据库的用户。然而，您应该避免通过环境变量将密码等秘密信息提供给进程，因为这些信息很容易泄露。
- en: A better way is to read that secret from a file that is managed safely. Docker
    supports this directly with its secrets functionality. The PostgreSQL image also
    has support for reading sensitive data such as the `POSTGRES_PASSWORD` from a
    file. For this stack definition, Docker will retrieve the PostgreSQL password
    from the cluster’s `ch13_multi_tier_app-POSTGRES_PASSWORD` secret resource definition.
    Swarm places the secret’s value in a file mounted into the container at `/run/secrets/POSTGRES_PASSWORD`.
    The PostgreSQL process switches to a user with user ID `999` when it starts up,
    so the secret file’s owner is configured to be readable by that user.
  id: totrans-470
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种更好的方式是从一个安全管理的文件中读取那个秘密。Docker 通过其秘密功能直接支持这一点。PostgreSQL 镜像也支持从文件中读取敏感数据，例如
    `POSTGRES_PASSWORD`。对于此堆栈定义，Docker 将从集群的 `ch13_multi_tier_app-POSTGRES_PASSWORD`
    秘密资源定义中检索 PostgreSQL 密码。Swarm 将秘密的值放置在容器中挂载的文件 `/run/secrets/POSTGRES_PASSWORD`
    中。PostgreSQL 进程在启动时切换到用户 ID 为 `999` 的用户，因此秘密文件的拥有者被配置为可以被该用户读取。
- en: '|    |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Note
  id: totrans-472
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: All processes that execute inside a Docker container can access all the environment
    variables of that container. However, access to data in files is controlled by
    file permissions. So `nobody` can read a `$SECRET` environment variable, but not
    the /run/secrets/SECRET file unless file ownership and permissions permit reading
    by `nobody`. For details, see [chapter 12](index_split_098.html#filepos1254019),
    which explores Docker configurations and secrets in detail.
  id: totrans-473
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有在 Docker 容器内部执行的过程都可以访问该容器的所有环境变量。然而，对文件中数据的访问是由文件权限控制的。所以 `nobody` 无法读取 `$SECRET`
    环境变量，除非文件的所有权和权限允许 `nobody` 读取 `/run/secrets/SECRET` 文件。有关详细信息，请参阅[第 12 章](index_split_098.html#filepos1254019)，该章节详细探讨了
    Docker 配置和秘密。
- en: '|    |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Does it look like anything is missing from the `postgres` service definition?
    One thing that is not clear is how clients will connect to the database.
  id: totrans-475
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 `postgres` 服务定义中看起来是否遗漏了什么？不清楚的一点是客户端将如何连接到数据库。
- en: When using a Docker overlay network, applications connected to a given network
    will be able to communicate with each other on any port. No firewalls exist between
    applications attached to a Docker network. Because PostgreSQL listens on port
    5432 by default and no firewall is present, other applications that are also attached
    to that `private` network will be able to connect to the `postgres` service on
    that port.
  id: totrans-476
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当使用 Docker overlay 网络时，连接到给定网络的客户端将能够通过任何端口相互通信。在连接到 Docker 网络的应用程序之间不存在防火墙。因为
    PostgreSQL 默认监听端口 5432，且没有防火墙，所以连接到那个 `private` 网络的其他应用程序也将能够连接到该端口的 `postgres`
    服务。
- en: 'Now let’s add a service definition for the API under the `services` key:'
  id: totrans-477
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在让我们在 `services` 键下添加一个 API 服务定义：
- en: '`api:       image: ${IMAGE_REPOSITORY:-dockerinaction/ch13_multi_tier_app}:api
          networks:         - public          - private       ports:         - ''8080:80''
          secrets:         - source: ch13_multi_tier_app-POSTGRES_PASSWORD          
    target: POSTGRES_PASSWORD           mode: 0400       environment:         POSTGRES_HOST:
    ''postgres''         POSTGRES_PORT: ''5432''         POSTGRES_USER: ''exercise''
            POSTGRES_DB: ''exercise''         POSTGRES_PASSWORD_FILE: ''/run/secrets/POSTGRES_PASSWORD''
          depends_on:         - postgres       deploy:           replicas: 2          
    restart_policy:               condition: on-failure               max_attempts:
    10               delay: 5s           update_config:               parallelism:
    1               delay: 5s           resources:             limits:              
    cpus: ''0.50''               memory: 15M             reservations:              
    cpus: ''0.25''               memory: 15M`'
  id: totrans-478
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`api:   image: ${IMAGE_REPOSITORY:-dockerinaction/ch13_multi_tier_app}:api  
    networks:   - public   - private   ports:   - ''8080:80''   secrets:   - source:
    ch13_multi_tier_app-POSTGRES_PASSWORD   target: POSTGRES_PASSWORD   mode: 0400  
    environment:   POSTGRES_HOST: ''postgres''   POSTGRES_PORT: ''5432''   POSTGRES_USER:
    ''exercise''   POSTGRES_DB: ''exercise''   POSTGRES_PASSWORD_FILE: ''/run/secrets/POSTGRES_PASSWORD''  
    depends_on:   - postgres   deploy:   replicas: 2   restart_policy:   condition:
    on-failure   max_attempts: 10   delay: 5s   update_config:   parallelism: 1  
    delay: 5s   resources:   limits:   cpus: ''0.50''   memory: 15M   reservations:  
    cpus: ''0.25''   memory: 15M`'
- en: The API servers are attached to both the `public` and `private` network. Clients
    of the API server issue requests to port 8080 of the cluster. The Swarm network
    routing mesh will forward client requests from the edge of the network to a task
    and ultimately into an API server container on port 80\. The API servers connect
    to PostgreSQL, which is attached to only the `private` network. The API servers
    are configured to connect to PostgreSQL by using the information defined in the
    `POSTGRES_*` environment variables.
  id: totrans-479
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: API服务器连接到`public`和`private`网络。API服务器的客户端向集群的8080端口发送请求。Swarm网络路由网格会将客户端请求从网络的边缘转发到一个任务，最终进入端口80上的API服务器容器。API服务器连接到仅连接到`private`网络的PostgreSQL。API服务器配置为使用在`POSTGRES_*`环境变量中定义的信息连接到PostgreSQL。
- en: Notice that the PostgreSQL user’s password is also provided to the API server
    via a Docker secret. As with the `postgres` service, the secret is mounted into
    each API service container as a file. Though the API service uses an image built
    from scratch and includes only a static Golang binary, the secret mount still
    works because Docker manages the underlying `tmpfs` filesystem mount for you.
    Docker goes to great lengths to help you manage and use secrets safely.
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，PostgreSQL用户的密码也通过Docker密钥提供给API服务器。与`postgres`服务一样，密钥被挂载到每个API服务容器中作为一个文件。尽管API服务使用从头构建的镜像，并且只包含静态的Golang二进制文件，但密钥挂载仍然有效，因为Docker为您管理底层的`tmpfs`文件系统挂载。Docker会尽最大努力帮助您安全地管理和使用密钥。
- en: The rest of the API service definition manages the specifics of how Swarm should
    deploy the service. The `depends_on` key contains a list of other services that
    the API server depends on—in this case, `postgres`. When we deploy the stack,
    Swarm will start the `postgres` service before `api`. The `deploy` key declares
    how Swarm should deploy the `api` service across the cluster.
  id: totrans-481
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: API服务定义的其余部分管理了Swarm如何部署服务的具体细节。`depends_on`键包含了一个API服务器所依赖的其他服务的列表——在这个例子中，是`postgres`。当我们部署堆栈时，Swarm会在`api`服务之前启动`postgres`服务。`deploy`键声明了Swarm应该如何在集群中部署`api`服务。
- en: In this configuration, Swarm will deploy two replicas to the cluster and try
    to keep that many tasks running to support the service. The `restart_policy` determines
    how Swarm handles a service task exiting or entering a failed state, according
    to its health check.
  id: totrans-482
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个配置中，Swarm将在集群中部署两个副本，并尝试保持这么多任务运行以支持服务。`restart_policy`决定了Swarm如何根据其健康检查处理服务任务退出或进入失败状态。
- en: Here, Swarm will restart the task when it fails to start. Restart is a misnomer,
    as Swarm will actually start a new container rather than restart the failed container.
    Swarm restarts service tasks an infinite number of times by default. The API service’s
    configuration restarts tasks up to 10 times with a 5-second delay between each
    restart.
  id: totrans-483
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这里，当任务启动失败时，Swarm将重新启动任务。重启是一个误称，因为Swarm实际上会启动一个新的容器而不是重启失败的容器。Swarm默认无限次数地重启服务任务。API服务的配置在每次重启之间有5秒的延迟，最多重启任务10次。
- en: Service authors should think through their restart strategies carefully to determine
    how long and how many attempts Swarm should make to start a service. First, it’s
    rarely useful to try indefinitely. Second, infinite retry processes could exhaust
    cluster resources that are consumed when new containers start, but aren’t cleaned
    up quickly enough.
  id: totrans-484
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 服务作者应该仔细思考他们的重启策略，以确定Swarm启动服务所需的时间和尝试次数。首先，无限尝试通常是没有用的。其次，无限重试过程可能会耗尽集群资源，这些资源在启动新容器时被消耗，但清理不够快。
- en: The API service uses a simple `update_config` that limits the rollout of updates
    to the service to one task at a time. In this configuration, Swarm will update
    the service by shutting down a task with the old configuration, start one with
    the new configuration, and wait until the new task is healthy prior to moving
    on to replacing the next task in the service. The delay configuration introduces
    an interval between task replacement actions to keep the cluster and traffic to
    the service stable during the rollout.
  id: totrans-485
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: API服务使用一个简单的`update_config`，该配置将服务的更新推出限制为一次一个任务。在这个配置中，Swarm将通过关闭具有旧配置的任务，启动一个具有新配置的任务，并在新任务健康之前等待，来更新服务。延迟配置在任务替换操作之间引入了一个间隔，以保持集群和服务流量的稳定性，在推出过程中。
- en: Many configuration options exist for restart, update, and rollback configurations
    that were discussed in [chapter 11](index_split_092.html#filepos1146874). You
    can fine-tune these to complement the application’s behavior and create a robust
    deployment process.
  id: totrans-486
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在第11章中讨论了许多重启、更新和回滚配置的选项。你可以对这些选项进行微调，以补充应用程序的行为并创建一个健壮的部署过程。[第11章](index_split_092.html#filepos1146874)。
- en: 13.2.3\. Deploying the application
  id: totrans-487
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.2.3\. 部署应用程序
- en: In this section, we will deploy the application we’ve defined to a Swarm cluster.
    We will use the `docker stack` command introduced in [chapter 11](index_split_092.html#filepos1146874)
    to do that. [Figure 13.4](#filepos1394694) shows how this command will be communicated
    to the cluster.
  id: totrans-488
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本节中，我们将把我们定义的应用程序部署到Swarm集群中。我们将使用第11章中介绍的`docker stack`命令来完成这项工作。[图13.4](#filepos1394694)显示了该命令如何与集群进行通信。
- en: Figure 13.4\. Communication path for Docker control plane
  id: totrans-489
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图13.4\. Docker控制平面的通信路径
- en: '![](images/00083.jpg)'
  id: totrans-490
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00083.jpg)'
- en: Docker services, networks, and other swarm resources are managed by issuing
    the appropriate `docker` command to a manager node of the swarm cluster. When
    you issue a command with the `docker` CLI, it will connect to the Docker Engine
    API and request updates to the swarm cluster’s state. The leader of the swarm
    will orchestrate the changes required to converge the actual application resources
    on the cluster to the desired state.
  id: totrans-491
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过向Swarm集群的管理节点发出适当的`docker`命令来管理Docker服务、网络和其他Swarm资源。当你使用`docker` CLI发出命令时，它将连接到Docker
    Engine API并请求更新Swarm集群的状态。Swarm的领导者将编排所需的更改，以将集群上的实际应用程序资源收敛到所需状态。
- en: 'If you issue Docker commands to manage the cluster or its resources to a worker
    node, you will receive an error:'
  id: totrans-492
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你向工作节点发出Docker命令来管理集群或其资源，你将收到一个错误：
- en: '`[worker1] $ docker node ls Error response from daemon: This node is not a
    swarm manager. Worker nodes can''t be used to view or modify cluster state. Please
    run this command on a manager node or promote the current node to a manager.`'
  id: totrans-493
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[worker1] $ docker node ls 错误响应来自守护进程：此节点不是Swarm管理器。工作节点不能用于查看或修改集群状态。请在管理节点上运行此命令或提升当前节点为管理器。`'
- en: 'Open a command shell to any of the manager nodes in your cluster. List the
    cluster’s nodes with `docker node ls`:'
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在你的集群中的任何管理节点上打开一个命令外壳。使用`docker node ls`列出集群的节点：
- en: '`[manager1] $ docker node ls ID                            HOSTNAME           
    STATUS             AVAILABILITY        MANAGER STATUS      ENGINE VERSION 7baqi6gedujmycxwufj939r44
    *   manager1            Ready Active              Reachable           18.06.1-ce
    bbqicrevqkfu8w4f9wli1tjcr     manager2            Ready Active              Leader             
    18.06.1-ce hdpskn4q93f5ou1whw9ht8y01     manager3            Ready Active             
    Reachable           18.06.1-ce xle0g72ydvj24sf40vnaw08n0     worker1            
    Ready Active                                  18.06.1-ce l6fkyzqglocnwc0y4va2anfho    
    worker2             Ready Active                                  18.06.1-ce [manager1]
    $`'
  id: totrans-495
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[manager1] $ docker node ls ID                            HOSTNAME           
    STATUS             AVAILABILITY        MANAGER STATUS      ENGINE VERSION 7baqi6gedujmycxwufj939r44
    *   manager1            Ready Active              Reachable           18.06.1-ce
    bbqicrevqkfu8w4f9wli1tjcr     manager2            Ready Active              Leader             
    18.06.1-ce hdpskn4q93f5ou1whw9ht8y01     manager3            Ready Active             
    Reachable           18.06.1-ce xle0g72ydvj24sf40vnaw08n0     worker1            
    Ready Active                                  18.06.1-ce l6fkyzqglocnwc0y4va2anfho    
    worker2             Ready Active                                  18.06.1-ce [manager1]
    $`'
- en: In the preceding output, notice that the command was executed from the node
    named `manager1`. This node is operating in a manager role but is not currently
    the leader of the cluster. When a cluster management command is issued to this
    node, it will be forwarded to the leader for processing.
  id: totrans-496
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在前面的输出中，请注意命令是在名为`manager1`的节点上执行的。该节点正在运行管理角色，但不是当前集群的领导者。当向此节点发出集群管理命令时，它将被转发到领导者进行处理。
- en: 'Use Git to clone the application to the manager node and change into the `ch13_multi_tier_app`
    directory:'
  id: totrans-497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用Git将应用程序克隆到管理节点，并切换到`ch13_multi_tier_app`目录：
- en: '`git clone https://github.com/dockerinaction/ch13_multi_tier_app.git cd ch13_multi_tier_app`'
  id: totrans-498
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/dockerinaction/ch13_multi_tier_app.git cd ch13_multi_tier_app`'
- en: 'We are now ready to deploy the application with `docker stack`. The `stack`
    subcommand can deploy applications defined in two formats. The first format is
    the Docker Compose format we will be using. The second is the older and less popular
    Distributed Application Bundle (DAB) format. Because we are using the Docker Compose
    format, we will specify the path(s) to the compose file with `--compose-file`.
    Let’s deploy our Compose application to Swarm now:'
  id: totrans-499
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们现在可以使用`docker stack`部署应用程序了。`stack`子命令可以部署以两种格式定义的应用程序。第一种格式是我们将使用的Docker
    Compose格式。第二种是较旧且不太受欢迎的分布式应用程序包（DAB）格式。因为我们使用的是Docker Compose格式，所以我们将使用`--compose-file`指定组合文件的路径。现在让我们将我们的组合应用程序部署到Swarm：
- en: '`docker stack deploy --compose-file docker-compose.yml multi-tier-app`'
  id: totrans-500
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy --compose-file docker-compose.yml multi-tier-app`'
- en: 'The application deployment should fail with an error indicating that `ch13_multi_tier_app-POSTGRES_PASSWORD`
    was not found:'
  id: totrans-501
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用部署应失败并显示错误信息，指出未找到`ch13_multi_tier_app-POSTGRES_PASSWORD`：
- en: '`$ docker stack deploy --compose-file docker-compose.yml multi-tier-app Creating
    network multi-tier-app_private Creating network multi-tier-app_public service
    postgres: secret not found: ch13_multi_tier_app-POSTGRES_PASSWORD`'
  id: totrans-502
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker stack deploy --compose-file docker-compose.yml multi-tier-app Creating
    network multi-tier-app_private Creating network multi-tier-app_public service
    postgres: secret not found: ch13_multi_tier_app-POSTGRES_PASSWORD`'
- en: The `docker` command output shows Swarm was able to create the networks, but
    not the services. Swarm requires that all cluster-level resources that a service
    depends on exist prior to proceeding with the deployment. So Docker halted the
    application deployment when it determined a resource dependency was missing. The
    resources that were created have been left as is and can be used in subsequent
    deployment attempts. These predeployment checks help build robust application
    delivery processes. The fail-fast deployment behavior helped us quickly discover
    a missing dependency.
  id: totrans-503
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker`命令的输出显示Swarm能够创建网络，但不能创建服务。Swarm要求在部署之前，所有服务所依赖的集群级资源都必须存在。因此，当Docker确定资源依赖项缺失时，它会停止应用程序的部署。已创建的资源保持原样，可以在后续的部署尝试中使用。这些预部署检查有助于构建健壮的应用程序交付流程。快速失败的部署行为帮助我们迅速发现缺失的依赖项。'
- en: 'The missing cluster-level resource that this application depends on is the
    `ch13_multi_tier_app-POSTGRES_PASSWORD` secret. Recall that the application’s
    reference to that secret said that it was defined externally:'
  id: totrans-504
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此应用程序所依赖的缺失的集群级资源是`ch13_multi_tier_app-POSTGRES_PASSWORD`密钥。回想一下，应用程序对该密钥的引用表明它是外部定义的：
- en: '`secrets:   ch13_multi_tier_app-POSTGRES_PASSWORD:     external: true             
    # Retrieve from cluster''s secret resources`'
  id: totrans-505
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`secrets:   ch13_multi_tier_app-POSTGRES_PASSWORD:     external: true             
    # 从集群的密钥资源中检索`'
- en: 'In this context, `external` means defined outside the application deployment
    definition and provided by Swarm. Let’s store the application’s database password
    as a Docker secret in the Swarm cluster now:'
  id: totrans-506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在此上下文中，`external` 表示定义在应用程序部署定义之外，并由Swarm提供。现在让我们将应用程序的数据库密码作为Docker secret存储在Swarm集群中：
- en: '`echo ''mydbpass72'' | docker secret create \     ch13_multi_tier_app-POSTGRES_PASSWORD
    -`'
  id: totrans-507
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`echo ''mydbpass72'' | docker secret create \   ch13_multi_tier_app-POSTGRES_PASSWORD
    -`'
- en: '|    |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Note
  id: totrans-509
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: The only place this password is defined is in this Docker secret managed by
    Swarm. You can use any valid PostgreSQL password you want. Feel free to change
    it. This demonstrates how easy it is to safely handle secrets in distributed applications
    with Swarm.
  id: totrans-510
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个密码只在这个由Swarm管理的Docker secret中定义。你可以使用任何有效的PostgreSQL密码。请随意更改它。这展示了在Swarm中如何轻松安全地处理分布式应用程序的秘密。
- en: '|    |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: 'The `docker secret` command should succeed and print the random identifier
    that Docker assigned to manage the secret. You can verify that the secret was
    created by listing the secrets in your cluster:'
  id: totrans-512
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker secret` 命令应该成功并打印Docker分配的随机标识符来管理秘密。你可以通过列出集群中的秘密来验证秘密是否已创建：'
- en: '`docker secret ls --format "table {{.ID}} {{.Name}} {{.CreatedAt}}"` `1`'
  id: totrans-513
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker secret ls --format "table {{.ID}} {{.Name}} {{.CreatedAt}}"` `1`'
- en: 1 Formats output as a table with secret identifier, name, and time since creation
    (optional)
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 以秘密标识符、名称和自创建以来时间（可选）格式化输出为表格
- en: 'The listing should show the secret was created recently:'
  id: totrans-515
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 列表应显示秘密最近被创建：
- en: '`ID NAME CREATED <random id> ch13_multi_tier_app-POSTGRES_PASSWORD 6 seconds
    ago`'
  id: totrans-516
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ID NAME CREATED <random id> ch13_multi_tier_app-POSTGRES_PASSWORD 6 seconds
    ago`'
- en: 'Now let’s try deploying the stack again:'
  id: totrans-517
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在让我们再次尝试部署堆栈：
- en: '`[manager1] $ docker stack deploy \    --compose-file docker-compose.yml multi-tier-app
    Creating service multi-tier-app_postgres Creating service multi-tier-app_api`'
  id: totrans-518
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[manager1] $ docker stack deploy \   --compose-file docker-compose.yml multi-tier-app
    Creating service multi-tier-app_postgres Creating service multi-tier-app_api`'
- en: 'The `docker stack` command should report that it has created two Docker services
    for the multitier app: `multi-tier-app_postgres` and `multi-tier-app_api`. List
    the services and check their status:'
  id: totrans-519
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack` 命令应该报告它已为多级应用程序创建了两个Docker服务：`multi-tier-app_postgres` 和 `multi-tier-app_api`。列出服务并检查其状态：'
- en: '`docker service ls \     --format "table {{.Name}} {{.Mode}} {{.Replicas}}"`
    `1`'
  id: totrans-520
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ls \   --format "table {{.Name}} {{.Mode}} {{.Replicas}}"`
    `1`'
- en: 1 Formats output as a table with service name, mode, and replicas (optional)
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 以服务名称、模式（可选）和副本数（可选）格式化输出为表格
- en: 'That command will produce output like this:'
  id: totrans-522
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该命令将生成如下输出：
- en: '`NAME MODE REPLICAS multi-tier-app_api replicated 2/2 multi-tier-app_postgres
    replicated 1/1`'
  id: totrans-523
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`NAME MODE REPLICAS multi-tier-app_api replicated 2/2 multi-tier-app_postgres
    replicated 1/1`'
- en: Each of the services has the expected number of replicas. There is one task
    for PostgreSQL and two tasks for the API shown in the `REPLICAS` column.
  id: totrans-524
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个服务都有预期的副本数。PostgreSQL有一个任务，API在`REPLICAS`列中有两个任务。
- en: 'You can check that the `api` service started up correctly by inspecting logs:'
  id: totrans-525
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过检查日志来检查`api`服务是否正确启动：
- en: '`docker service logs --follow multi-tier-app_api`'
  id: totrans-526
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service logs --follow multi-tier-app_api`'
- en: 'Each `api` task should log a message saying that it is initializing, reading
    the PostgreSQL password from a file, and listening for requests. For example:'
  id: totrans-527
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个`api`任务都应该记录一条消息，说明它正在初始化，从文件中读取PostgreSQL密码，并监听请求。例如：
- en: '`$ docker service logs --no-task-ids multi-tier-app_api multi-tier-app_api.1@worker1   
    | 2019/02/02 21:25:22 \     Initializing api server multi-tier-app_api.1@worker1   
    | 2019/02/02 21:25:22 \     Will read postgres password from ''/run/secrets/POSTGRES_PASSWORD''
    multi-tier-app_api.1@worker1    | 2019/02/02 21:25:22 \     dial tcp: lookup postgres
    on 127.0.0.11:53: no such host multi-tier-app_api.1@worker1    | 2019/02/02 21:25:23
    \     dial tcp 10.0.0.12:5432: connect: connection refused multi-tier-app_api.1@worker1   
    | 2019/02/02 21:25:25 \     Initialization complete, starting http service multi-tier-app_api.2@manager1  
    | 2019/02/02 21:25:22 \     Initializing api server multi-tier-app_api.2@manager1   
    | 2019/02/02 21:25:22 \     Will read postgres password from ''/run/secrets/POSTGRES_PASSWORD''
    multi-tier-app_api.2@manager1    | 2019/02/02 21:25:22 \     dial tcp: lookup
    postgres on 127.0.0.11:53: no such host multi-tier-app_api.2@manager1    | 2019/02/02
    21:25:23 \     dial tcp: lookup postgres on 127.0.0.11:53: no such host multi-tier-app_api.2@manager1   
    | 2019/02/02 21:25:25 \     Initialization complete, starting http service`'
  id: totrans-528
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service logs --no-task-ids multi-tier-app_api multi-tier-app_api.1@worker1    |
    2019/02/02 21:25:22     初始化 api 服务器 multi-tier-app_api.1@worker1    | 2019/02/02
    21:25:22     将从 ''/run/secrets/POSTGRES_PASSWORD'' 读取 postgres 密码 multi-tier-app_api.1@worker1    |
    2019/02/02 21:25:22     dial tcp: lookup postgres on 127.0.0.11:53: no such host
    multi-tier-app_api.1@worker1    | 2019/02/02 21:25:23     dial tcp 10.0.0.12:5432:
    connect: connection refused multi-tier-app_api.1@worker1    | 2019/02/02 21:25:25     初始化完成，启动
    http 服务 multi-tier-app_api.2@manager1   | 2019/02/02 21:25:22     初始化 api 服务器
    multi-tier-app_api.2@manager1   | 2019/02/02 21:25:22     将从 ''/run/secrets/POSTGRES_PASSWORD''
    读取 postgres 密码 multi-tier-app_api.2@manager1   | 2019/02/02 21:25:22     dial
    tcp: lookup postgres on 127.0.0.11:53: no such host multi-tier-app_api.2@manager1   |
    2019/02/02 21:25:23     dial tcp: lookup postgres on 127.0.0.11:53: no such host
    multi-tier-app_api.2@manager1   | 2019/02/02 21:25:25     初始化完成，启动 http 服务`'
- en: The `docker service logs <service name>` command streams log messages from the
    node where service tasks are deployed to your terminal. You may view the logs
    of any service by issuing this command to Docker Engine on a manager node, but
    not a worker. When you view service logs, Docker Engine connects to the engines
    in the cluster where its tasks have run, retrieves the logs, and returns them
    to you.
  id: totrans-529
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service logs <service name>` 命令将服务任务部署的节点上的日志消息流式传输到您的终端。您可以通过在管理节点上对
    Docker 引擎发出此命令来查看任何服务的日志，但不能在工作节点上。当您查看服务日志时，Docker 引擎连接到集群中运行其任务的引擎，检索日志，并将其返回给您。'
- en: 'From the log messages, we can see that these `api` tasks appear to be running
    on the `worker1` and `manager1` nodes. Your service tasks may have started on
    different nodes. We can verify this with the `docker service ps` command, which
    lists a service’s tasks. Run this:'
  id: totrans-530
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从日志消息中，我们可以看到这些 `api` 任务似乎正在 `worker1` 和 `manager1` 节点上运行。您的服务任务可能已在不同节点上启动。我们可以通过
    `docker service ps` 命令来验证这一点，该命令列出了服务的任务。运行以下命令：
- en: '`docker service ps \     --format "table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}"
    \` `1` `multi-tier-app_api`'
  id: totrans-531
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps \    --format "table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}"
    \` `1` `multi-tier-app_api`'
- en: 1 Formats output as a table with essential task data (optional)
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 以表格格式输出必要任务数据（可选）
- en: 'This command will produce output like this:'
  id: totrans-533
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此命令将产生如下输出：
- en: '`ID NAME NODE CURRENT STATE 5jk32y4agzst multi-tier-app_api.1 worker1 Running
    16 minutes ago nh5trkrpojlc multi-tier-app_api.2 manager1 Running 16 minutes ago`'
  id: totrans-534
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ID NAME NODE CURRENT STATE 5jk32y4agzst multi-tier-app_api.1 worker1 运行 16
    分钟前 nh5trkrpojlc multi-tier-app_api.2 manager1 运行 16 分钟前`'
- en: The `docker service ps` command reports that two tasks are running for the `api`
    service, as expected. Notice that the tasks are named in the form `<stack name>_<service
    name>.<replica slot number>`; for example, `multi-tier-app_api.1`. Each task also
    gets a unique ID. The `docker service ps` command lists the tasks and their status
    for a service no matter where they are running on the cluster.
  id: totrans-535
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps` 命令报告说，`api` 服务正在运行两个任务，正如预期的那样。请注意，任务以 `<stack name>_<service
    name>.<replica slot number>` 的形式命名；例如，`multi-tier-app_api.1`。每个任务也都有一个唯一的 ID。`docker
    service ps` 命令列出了服务上的任务及其状态，无论它们在集群的哪个位置运行。'
- en: 'By contrast, when running `docker container ps` on the `manager1` node, it
    shows only the single container running on that node:'
  id: totrans-536
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相比之下，当在 `manager1` 节点上运行 `docker container ps` 时，它只显示该节点上运行的单个容器：
- en: '`$ docker container ps --format "table {{.ID}} {{.Names}} {{.Status}}" CONTAINER
    ID NAMES STATUS 4a95fa59a7f8 multi-tier-app_api.2.nh5trkrpojlc3knysxza3sffl \
        Up 27 minutes (healthy)`'
  id: totrans-537
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker container ps --format "table {{.ID}} {{.Names}} {{.Status}}" CONTAINER
    ID NAMES STATUS 4a95fa59a7f8 multi-tier-app_api.2.nh5trkrpojlc3knysxza3sffl     Up
    27 minutes (healthy)`'
- en: The container name for a service task is constructed from the task name and
    unique task ID. Both `ps` commands report that this task is running and healthy.
    The `api` server’s image defines a `HEALTHCHECK`, so we can be confident this
    is true.
  id: totrans-538
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 服务任务容器的名称由任务名称和唯一的任务ID组成。两个`ps`命令都报告说该任务正在运行且状态良好。`api`服务器的镜像定义了一个`HEALTHCHECK`，因此我们可以确信这是真的。
- en: Great—our application deployed successfully, and everything looks healthy!
  id: totrans-539
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 太好了——我们的应用程序成功部署，一切看起来都很健康！
- en: 'Open a web browser and point it to port 8080 on any node of your cluster. Play
    with Docker users should have an 8080 hyperlink at the top of the web console.
    You can also use a `curl` command to issue an HTTP request from one of the cluster
    nodes to port 8080:'
  id: totrans-540
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 打开网页浏览器，将其指向集群中任意节点的8080端口。Play with Docker用户应在网页控制台顶部看到一个8080超链接。您也可以使用`curl`命令从集群的某个节点向8080端口发起HTTP请求：
- en: '`curl http://localhost:8080`'
  id: totrans-541
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`curl http://localhost:8080`'
- en: 'The `api` server should respond with a simple message similar to the following:'
  id: totrans-542
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`api`服务器应该响应一个类似于以下简单的消息：'
- en: '`Welcome to the API Server! Container id 256e1c4fb6cb responded at 2019-02-03
    00:31:23.0915026 +0000 UTC`'
  id: totrans-543
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`欢迎来到API服务器！容器id 256e1c4fb6cb在2019-02-03 00:31:23.0915026 +0000 UTC响应`'
- en: '|    |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Tip
  id: totrans-545
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示
- en: If you are using Play with Docker, the detail page for each cluster node will
    have a link to ports published on that node. You can open that link or use it
    with `curl`.
  id: totrans-546
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您使用Play with Docker，每个集群节点的详细页面将有一个指向该节点上发布的端口的链接。您可以打开该链接或使用`curl`。
- en: '|    |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: 'When you make that request several times, you should see different container
    IDs serving your requests. This shell script will issue four HTTP requests and
    produce the output that follows:'
  id: totrans-548
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您多次发出该请求时，您应该看到不同的容器ID在为您提供服务。这个shell脚本将发出四个HTTP请求并产生以下输出：
- en: '``$ for i in `seq 1 4`; do curl http://localhost:8080; sleep 1; done;`` `1`
    `Welcome to the API Server!` `2` `Server 9c2eea9f140c responded at 2019-02-05
    17:51:41.2050856 +0000 UTC Welcome to the API Server! Server 81fbc94415e3 responded
    at 2019-02-05 17:51:42.1957773 +0000 UTC Welcome to the API Server! Server 9c2eea9f140c
    responded at 2019-02-05 17:51:43.2172085 +0000 UTC Welcome to the API Server!
    Server 81fbc94415e3 responded at 2019-02-05 17:51:44.241654 +0000 UTC`'
  id: totrans-549
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '``$ for i in `seq 1 4`; do curl http://localhost:8080; sleep 1; done;`` `1`
    `欢迎来到API服务器！` `2` `服务器9c2eea9f140c在2019-02-05 17:51:41.2050856 +0000 UTC响应欢迎来到API服务器！服务器81fbc94415e3在2019-02-05
    17:51:42.1957773 +0000 UTC响应欢迎来到API服务器！服务器9c2eea9f140c在2019-02-05 17:51:43.2172085
    +0000 UTC响应欢迎来到API服务器！服务器81fbc94415e3在2019-02-05 17:51:44.241654 +0000 UTC响应欢迎来到API服务器！`'
- en: 1 Bash shell commands to issue four requests to the application; you can replace
    “localhost” with the hostname of any cluster node.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 使用Bash shell命令向应用程序发出四个请求；您可以将“localhost”替换为任何集群节点的主机名。
- en: 2 Output from each HTTP request
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 每个HTTP请求的输出
- en: Here, the `curl` program issues an HTTP GET request to a cluster node. In the
    preceding example, the `curl` program runs on one of the cluster’s nodes and sends
    the request to that node, `localhost`, on port 8080\. As there are no firewalls
    preventing `curl` from opening a socket to that network location, Docker Swarm’s
    service mesh will handle the connection to port 8080 and route the request to
    a live container. We will investigate how requests are routed to Docker services
    in more detail next.
  id: totrans-552
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这里，`curl`程序向一个集群节点发出HTTP GET请求。在上面的例子中，`curl`程序在集群的某个节点上运行并向该节点上的8080端口发送请求。由于没有防火墙阻止`curl`打开到该网络位置的套接字，Docker
    Swarm的服务网格将处理到8080端口的连接并将请求路由到活动容器。我们将在下一节更详细地研究请求是如何路由到Docker服务的。
- en: 13.3\. COMMUNICATING WITH SERVICES RUNNING ON A SWARM CLUSTER
  id: totrans-553
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3. COMMUNICATING WITH SERVICES RUNNING ON A SWARM CLUSTER
- en: Docker makes it easy for clients outside a Swarm cluster to connect to services
    running in the cluster. Swarm also helps services running within the cluster to
    find and contact each other when they share a Docker network. In this section,
    we will first explore how Docker exposes services to the world outside the cluster.
    Then we will look at how Docker services communicate with each other by using
    Swarm’s service discovery and overlay networking features.
  id: totrans-554
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker使集群外部的客户端连接到集群中运行的服务变得容易。Swarm还帮助集群内部运行的服务在共享Docker网络时找到并相互联系。在本节中，我们将首先探讨Docker如何将服务暴露给集群外的世界。然后我们将查看Docker服务如何通过使用Swarm的服务发现和overlay网络功能相互通信。
- en: 13.3.1\. Routing client requests to services by using the Swarm routing mesh
  id: totrans-555
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3.1\. 使用Swarm路由网格路由客户端请求到服务
- en: The Swarm routing mesh provides a simple way to expose a service running on
    a container cluster with the outside world, which is one of Swarm’s most compelling
    features. The routing mesh combines several sophisticated network building blocks
    to publish a service port. [Figure 13.5](#filepos1426936) depicts the logical
    network topology Swarm creates for the example application.
  id: totrans-556
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm 路由网格提供了一种简单的方法，将运行在容器集群上的服务暴露给外部世界，这是 Swarm 最吸引人的特性之一。路由网格结合了几个复杂的网络构建块来发布服务端口。[图
    13.5](#filepos1426936) 描述了 Swarm 为示例应用程序创建的逻辑网络拓扑。
- en: Figure 13.5\. Swarm network components for example app
  id: totrans-557
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.5\. 示例应用程序的 Swarm 网络组件
- en: '![](images/00060.jpg)'
  id: totrans-558
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00060.jpg)'
- en: Swarm sets up a listener on each node of the cluster for each published service
    port. You can configure the port to listen for TCP, UDP, or both kinds of traffic.
    Client applications can connect to this port on any cluster node and issue requests.
  id: totrans-559
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm 为每个发布的端口在每个集群节点上设置一个监听器。您可以配置端口以监听 TCP、UDP 或两种类型的流量。客户端应用程序可以连接到任何集群节点上的此端口并发出请求。
- en: Swarm implements this listener with a combination of Linux `iptables` and `ipvs`
    features. An `iptables` rule redirects traffic to a dedicated virtual IP (VIP)
    allocated for the service. The service’s dedicated VIP is made available across
    the swarm cluster by using a Linux kernel feature called IP Virtual Server, `ipvs`.
    IPVS is a transport-layer load balancer that forwards requests for TCP or UDP
    services to their real endpoints. IPVS is not an application-layer load balancer
    for protocols such as HTTP. Swarm creates a VIP for each published Service port
    using `ipvs`. It then attaches the VIP to the `ingress` network, which is available
    across the Swarm cluster.
  id: totrans-560
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm 通过结合 Linux `iptables` 和 `ipvs` 功能来实现此监听器。一个 `iptables` 规则将流量重定向到为服务分配的专用虚拟
    IP (VIP)。通过使用 Linux 内核功能 IP 虚拟服务器，`ipvs`，服务的专用 VIP 可在 Swarm 集群中提供。IPVS 是一个传输层负载均衡器，它将
    TCP 或 UDP 服务的请求转发到其实际端点。IPVS 不是一个针对 HTTP 等协议的应用层负载均衡器。Swarm 使用 `ipvs` 为每个发布的 Service
    端口创建一个 VIP。然后，它将 VIP 绑定到 `ingress` 网络，该网络在 Swarm 集群中可用。
- en: Returning to our example application, when traffic reaches a cluster node on
    TCP port 8080, `iptables` reroutes that traffic to the `api` service VIP attached
    to the `ingress` network. IPVS forwards traffic from the VIP to the ultimate endpoints,
    which are Docker service tasks.
  id: totrans-561
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 回到我们的示例应用程序，当流量到达 TCP 端口 8080 的集群节点时，`iptables` 将该流量重定向到 `ingress` 网络上附加的 `api`
    服务 VIP。IPVS 将流量从 VIP 转发到最终端点，即 Docker 服务任务。
- en: Swarm’s routing mesh will handle the connection from the client, connect to
    a healthy service task, and forward the client’s request data to the service task.
    [Figure 13.6](#filepos1430433) shows how Swarm routs `curl`’s HTTP request to
    an API service task and back.
  id: totrans-562
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Swarm 的路由网格将处理来自客户端的连接，连接到健康的服务任务，并将客户端的请求数据转发到服务任务。[图 13.6](#filepos1430433)
    展示了 Swarm 如何将 `curl` 的 HTTP 请求路由到 API 服务任务并返回。
- en: Figure 13.6\. Routing an HTTP request to a service task
  id: totrans-563
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.6\. 将 HTTP 请求路由到服务任务
- en: '![](images/00090.jpg)'
  id: totrans-564
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00090.jpg)'
- en: When a program connects to a published port, Swarm will attempt to connect to
    a healthy task for that service. If the service has been scaled to zero replicas
    or no healthy tasks exist, the routing mesh will refuse to initiate the network
    connection. Once the TCP connection is established, the client may move on to
    the next stage of transmission. In the case of the API service, the client writes
    an HTTP GET request onto the TCP socket connection. The routing mesh receives
    that data and sends it to the task handling this connection.
  id: totrans-565
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当程序连接到发布的端口时，Swarm 将尝试连接到该服务的健康任务。如果服务已扩展到零个副本或不存在健康任务，路由网格将拒绝启动网络连接。一旦建立了 TCP
    连接，客户端就可以继续到传输的下一阶段。在 API 服务的例子中，客户端将 HTTP GET 请求写入 TCP 套接字连接。路由网格接收这些数据并将其发送到处理此连接的任务。
- en: 'It’s important to note that a service task does not need to be running on the
    node that handles the client’s connection. Publishing a port establishes a stable
    ingress point for a Docker service that is independent of the transient locations
    of that service’s tasks within the Swarm cluster. You can inspect the ports published
    by a service with `docker service inspect`:'
  id: totrans-566
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重要的是要注意，服务任务不需要在处理客户端连接的节点上运行。发布端口为 Docker 服务建立了一个稳定的入口点，该入口点独立于该服务任务在 Swarm
    集群中的临时位置。您可以使用 `docker service inspect` 检查服务发布的端口：
- en: '`$ docker service inspect --format="{{json .Endpoint.Spec.Ports}}" \     multi-tier-app_api
    [   {     "Protocol": "tcp",     "TargetPort": 80,     "PublishedPort": 8080,
        "PublishMode": "ingress"   } ]`'
  id: totrans-567
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service inspect --format="{{json .Endpoint.Spec.Ports}}" \    multi-tier-app_api
    [    {    "Protocol": "tcp",    "TargetPort": 80,    "PublishedPort": 8080,    "PublishMode":
    "ingress"    } ]`'
- en: This output indicates that `multi-tier-app_api` has a listener attached to the
    `ingress` network on TCP port 8080, and that traffic will be routed into service
    tasks on port 80\.
  id: totrans-568
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此输出指示 `multi-tier-app_api` 在 TCP 端口 8080 上连接到 `ingress` 网络的监听器，并且流量将被路由到端口号为
    80 的服务任务上。
- en: '|    |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Bypassing the routing mesh
  id: totrans-570
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 跳过路由网格
- en: An alternate `PublishMode` called `host` bypasses the routing mesh and attachment
    to the `ingress` network. When using this mode, clients connect directly to the
    service task on a given host. If a task is deployed there, it can handle the connection;
    otherwise, the connection attempt will fail.
  id: totrans-571
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一种名为 `host` 的 `PublishMode` 跳过了路由网格和 `ingress` 网络的连接。当使用此模式时，客户端将直接连接到指定主机上的服务任务。如果那里部署了任务，它可以处理连接；否则，连接尝试将失败。
- en: This `PublishMode` is likely most appropriate for services that are deployed
    in `global` mode so that there is one, and only one, task for a particular service
    on a cluster node. This ensures that a task is available to handle requests and
    avoids port collisions. Global services are explained in more detail in [section
    13.4.3](index_split_107.html#filepos1527907).
  id: totrans-572
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种 `PublishMode` 可能最适合在 `global` 模式下部署的服务，这样在集群节点上就只有一个任务，确保任务可以处理请求并避免端口冲突。全局服务在
    [第 13.4.3 节](index_split_107.html#filepos1527907) 中有更详细的解释。
- en: '|    |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Clients interact with the example application’s API service by using HTTP. HTTP
    is an application protocol (layer 7) that is transported over the TCP/IP (layer
    4) networking protocol. Docker also supports services that listen on UDP/IP (layer
    4). The Swarm routing mesh relies on IPVS, which routes and balances network traffic
    at layer 4\.
  id: totrans-574
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 客户端通过使用 HTTP 与示例应用程序的 API 服务进行交互。HTTP 是一种应用协议（第 7 层），它通过 TCP/IP（第 4 层）网络协议进行传输。Docker
    还支持监听 UDP/IP（第 4 层）的服务。Swarm 路由网格依赖于 IPVS，它在第 4 层路由和平衡网络流量。
- en: The distinction between routing at layer 4 versus layer 7 is important. Because
    Swarm routes and load-balances connections at the IP layer, it means client connections
    will be balanced across backend service tasks, not HTTP requests. When one client
    issues many requests over a single connection, all of those requests will go to
    a single task, and will not be distributed across all backend service tasks as
    you might expect. Note that Docker Enterprise Edition supports load balancing
    of the HTTP protocol (layer 7), and third-party solutions exist as well.
  id: totrans-575
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在第 4 层和第 7 层之间进行路由的区别很重要。因为 Swarm 在 IP 层路由和负载均衡连接，这意味着客户端连接将在后端服务任务之间进行平衡，而不是
    HTTP 请求。当一个客户端通过单个连接发出许多请求时，所有这些请求都将发送到单个任务，而不会像预期的那样分布到所有后端服务任务。请注意，Docker 企业版支持
    HTTP 协议（第 7 层）的负载均衡，也存在第三方解决方案。
- en: 13.3.2\. Working with overlay networks
  id: totrans-576
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3.2. 与 overlay 网络一起工作
- en: Docker Swarm offers a type of network resource called an overlay network, illustrated
    in [figure 13.7](#filepos1439530). This network, whose traffic is logically segmented
    from other networks, runs on top of another network. The Docker Engines of a Swarm
    cluster can create overlay networks that connect containers running on different
    Docker hosts. In a Docker overlay network, only the containers attached to that
    network can communicate with other containers on that network. An overlay network
    isolates the communication between containers attached to that network from other
    networks.
  id: totrans-577
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm 提供一种名为 overlay 网络的网络资源类型，如图 13.7 所示。[#filepos1439530](#filepos1439530)。此网络在逻辑上将其流量与其他网络分离，运行在另一个网络之上。Swarm
    集群的 Docker 引擎可以创建连接在不同 Docker 主机上运行的容器的 overlay 网络。在 Docker overlay 网络中，只有连接到该网络的容器可以与该网络上的其他容器通信。overlay
    网络将连接到该网络的容器之间的通信与其他网络隔离开来。
- en: Figure 13.7\. Layered view of overlay networks
  id: totrans-578
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.7. overlay 网络的分层视图
- en: '![](images/00070.jpg)'
  id: totrans-579
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00070.jpg)'
- en: One way to think about an overlay network is that it enhances the user-defined
    bridge networks described in [chapter 5](index_split_046.html#filepos458921) to
    span Docker hosts. Just as with a user-defined bridge network, all containers
    attached to an overlay network can communicate with each other directly as peers.
    A special example of an overlay network is the `ingress` network.
  id: totrans-580
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑覆盖网络的一种方式是，它增强了第 5 章中描述的用户定义的桥接网络，使其能够跨越 Docker 主机。就像用户定义的桥接网络一样，附加到覆盖网络的所有容器都可以作为对等节点直接相互通信。覆盖网络的一个特殊例子是
    `ingress` 网络。
- en: The `ingress` network is a special-purpose overlay network that it is created
    by Docker when you initialize a swarm. The `ingress` network’s only responsibility
    is to route traffic from external clients connected to ports published by Docker
    services within the cluster. This network is managed by Swarm, and only Swarm
    can attach containers to the `ingress` network. You should be aware that the default
    configuration of the `ingress` network is not encrypted.
  id: totrans-581
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ingress` 网络是一个特殊用途的覆盖网络，它是 Docker 在初始化一个集群时创建的。`ingress` 网络的唯一职责是将连接到集群内 Docker
    服务发布的端口的客户端流量进行路由。这个网络由 Swarm 管理，并且只有 Swarm 可以将容器附加到 `ingress` 网络。你应该知道，`ingress`
    网络的默认配置是没有加密的。'
- en: If your application needs end-to-end encryption, all services that publish ports
    should terminate their connections with TLS. TLS certificates can be stored as
    Docker secrets and retrieved by services on startup, just as we have demonstrated
    with passwords in this chapter and TLS certificates in [chapter 12](index_split_098.html#filepos1254019).
  id: totrans-582
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你的应用程序需要端到端加密，所有发布端口的应使用 TLS 终止它们的连接。TLS 证书可以存储为 Docker 机密，并在服务启动时检索，就像我们在本章中演示的密码，以及在
    [第 12 章](index_split_098.html#filepos1254019) 中的 TLS 证书一样。
- en: Next, we will explore how Docker helps services discover and connect to each
    other on a shared network.
  id: totrans-583
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨 Docker 如何帮助服务在共享网络上发现和连接到彼此。
- en: 13.3.3\. Discovering services on an overlay network
  id: totrans-584
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3.3\. 在覆盖网络中查找服务
- en: 'Docker services use the Domain Name System (DNS) to discover the location of
    other Docker services on a Docker network that they share. A program can connect
    to a Docker service if it knows the name of that service. In our example application,
    the `api` server is configured with the name of the database service via the `POSTGRES_HOST`
    environment variable:'
  id: totrans-585
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 服务使用域名系统 (DNS) 来发现它们在共享的 Docker 网络上其他 Docker 服务的位置。如果程序知道该服务的名称，它就可以连接到
    Docker 服务。在我们的示例应用程序中，`api` 服务器通过 `POSTGRES_HOST` 环境变量配置了数据库服务的名称：
- en: '`api:   # ... snip ...       environment:         POSTGRES_HOST: ''postgres''`'
  id: totrans-586
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`api:   # ... snip ...       environment:         POSTGRES_HOST: ''postgres''`'
- en: When an `api` task creates a connection to the PostgreSQL database, it will
    resolve the `postgres` name to an IP by using DNS. Containers attached to a Docker
    overlay network are automatically configured by Docker to perform DNS lookups
    via a special resolver, 127.0.0.11\. This is also true for user-defined bridge
    and MACVLAN networks. The Docker Engine handles DNS lookups made to 127.0.0.1\.
    If the name resolution request is for a Docker service that is present on that
    network, Docker will respond with the location of that service’s virtual IP. If
    the lookup is for another name, Docker will forward the request on to the normal
    DNS resolver for that container host.
  id: totrans-587
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一个 `api` 任务创建到 PostgreSQL 数据库的连接时，它将通过 DNS 将 `postgres` 名称解析为 IP。附加到 Docker
    覆盖网络的容器会自动由 Docker 配置，通过一个特殊的解析器 127.0.0.11 进行 DNS 查询。这也适用于用户定义的桥接网络和 MACVLAN
    网络。Docker 引擎处理发送到 127.0.0.1 的 DNS 查询。如果名称解析请求是针对该网络上存在的 Docker 服务，Docker 将响应该服务的虚拟
    IP 地址。如果查询是针对另一个名称，Docker 将将请求转发到该容器主机的正常 DNS 解析器。
- en: In our example application, that means when the `api` service looks up `postgres`,
    the Docker Engine on that host will respond with the virtual IP of the `postgres`
    service endpoint; for example, 10.0.27.2\. The `api` database connection driver
    can establish a connection to this virtual IP, and Swarm will route the connection
    to the `postgres` service task, which could be at 10.0.27.3\. You may have expected
    this convenient name resolution and network routing functionality to exist, but
    not all container orchestrators work this way.
  id: totrans-588
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的示例应用程序中，这意味着当 `api` 服务查找 `postgres` 时，该主机上的 Docker 引擎将响应 `postgres` 服务端点的虚拟
    IP，例如，10.0.27.2。`api` 数据库连接驱动程序可以连接到这个虚拟 IP，Swarm 将路由连接到 `postgres` 服务任务，该任务可能位于
    10.0.27.3。你可能期望这种方便的名称解析和网络路由功能存在，但并非所有容器编排器都以这种方式工作。
- en: If you recall [figure 13.5](#filepos1426936) shown previously, you may also
    have an explanation for something that looked unusual. [Figure 13.8](#filepos1446078)
    reproduces that diagram here.
  id: totrans-589
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你还记得之前显示的 [图 13.5](#filepos1426936)，你也许也能解释一些看起来不寻常的东西。[图 13.8](#filepos1446078)
    在这里重现了那个图表。
- en: Figure 13.8\. Swarm network components for example app
  id: totrans-590
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.8\. 示例应用的 Swarm 网络组件
- en: '![](images/00085.jpg)'
  id: totrans-591
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图片](images/00085.jpg)'
- en: 'The `api` service has three virtual IPs establishing its presence on each of
    three overlay networks it is attached to: `ingress`, `multi-tier-app_public`,
    and `multi-tier-app_private`. If you inspect the `api` service’s endpoints, you
    should see output that verifies this with `VirtualIPs` on those three networks:'
  id: totrans-592
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`api` 服务有三个虚拟 IP，分别在每个它附加的三个覆盖网络上建立其存在：`ingress`、`multi-tier-app_public` 和
    `multi-tier-app_private`。如果你检查 `api` 服务的端点，你应该看到输出验证了这三个网络上的 `VirtualIPs`：'
- en: '`docker service inspect --format ''{{ json .Endpoint.VirtualIPs }}'' \    
    multi-tier-app_api [   {     "NetworkID": "5oruhwaq4996xfpdp194k82td",` `1` `"Addr":
    "10.255.0.8/16"   },   {     "NetworkID": "rah2lj4tw67lgn87of6n5nihc",` `2` `"Addr":
    "10.0.2.2/24"   },   {     "NetworkID": "vc12njqthcq1shhqtk4eph697",` `3` `"Addr":
    "10.0.3.2/24"   } ]`'
  id: totrans-593
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service inspect --format ''{{ json .Endpoint.VirtualIPs }}'' \ multi-tier-app_api
    [ { "NetworkID": "5oruhwaq4996xfpdp194k82td", "Addr": "10.255.0.8/16" }, { "NetworkID":
    "rah2lj4tw67lgn87of6n5nihc", "Addr": "10.0.2.2/24" }, { "NetworkID": "vc12njqthcq1shhqtk4eph697",
    "Addr": "10.0.3.2/24" } ]`'
- en: 1 ingress network
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 入口网络层
- en: 2 multi-tier-app_private network
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 多层-app_private 网络层
- en: 3 multi-tier-app_public network
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 multi-tier-app_public 网络层
- en: 'Follow along with a little experiment that demonstrates the discoverability
    of services attached to a network and even the containers behind them. Start a
    shell and attach it to the `multi-tier-app_private` network:'
  id: totrans-597
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 跟随一个实验，该实验演示了附加到网络上的服务及其背后的容器的可发现性。启动一个 shell 并将其附加到 `multi-tier-app_private`
    网络上：
- en: '`docker container run --rm -it --network multi-tier-app_private \     alpine:3.8
    sh`'
  id: totrans-598
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker container run --rm -it --network multi-tier-app_private \ alpine:3.8
    sh`'
- en: 'We can attach our shell container to the application’s `private` network because
    it was defined as `attachable`:'
  id: totrans-599
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以将我们的 shell 容器附加到应用程序的 `private` 网络上，因为它被定义为 `attachable`：
- en: '`private:     driver: overlay     driver_opts:       encrypted: "true"    
    attachable: true`'
  id: totrans-600
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`private: driver: overlay driver_opts: encrypted: "true" attachable: true`'
- en: By default, only Swarm can attach containers for service tasks to a network.
    This `private` network was made attachable specifically for this service discovery
    exercise.
  id: totrans-601
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认情况下，只有 Swarm 可以将容器服务任务附加到网络。这个 `private` 网络被特别设置为可附加，以便进行此服务发现练习。
- en: 'Ping the `postgres` service once. You should see output like this:'
  id: totrans-602
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ping `postgres` 服务一次。你应该看到类似以下输出：
- en: '`/ # ping -c 1 postgres PING postgres (10.0.2.6): 56 data bytes 64 bytes from
    10.0.2.6: seq=0 ttl=64 time=0.110 ms  --- postgres ping statistics --- 1 packets
    transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.110/0.110/0.110
    ms`'
  id: totrans-603
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/ # ping -c 1 postgres PING postgres (10.0.2.6): 56 数据字节 64 字节来自 10.0.2.6:
    seq=0 ttl=64 time=0.110 ms  --- postgres ping 统计信息 --- 1 个数据包已传输，1 个数据包已接收，0%
    数据包丢失，往返时间最小/平均/最大 = 0.110/0.110/0.110 ms`'
- en: 'Now ping the `api` service:'
  id: totrans-604
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在ping `api` 服务：
- en: '`/ # ping -c 1 api PING api (10.0.2.2): 56 data bytes 64 bytes from 10.0.2.2:
    seq=0 ttl=64 time=0.082 ms  --- api ping statistics --- 1 packets transmitted,
    1 packets received, 0% packet loss round-trip min/avg/max = 0.082/0.082/0.082
    ms`'
  id: totrans-605
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/ # ping -c 1 api PING api (10.0.2.2): 56 数据字节 64 字节来自 10.0.2.2: seq=0 ttl=64
    time=0.082 ms  --- api ping 统计信息 --- 1 个数据包已传输，1 个数据包已接收，0% 数据包丢失，往返时间最小/平均/最大
    = 0.082/0.082/0.082 ms`'
- en: 'Let’s use Netcat to issue a request manually from your shell on the `private`
    network to the `api` service:'
  id: totrans-606
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们使用 Netcat 从 `private` 网络上的 shell 手动发出对 `api` 服务的请求：
- en: '`$ printf ''GET / HTTP/1.0\nHost: api\n\n'' | nc api 80` `1`'
  id: totrans-607
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ printf ''GET / HTTP/1.0\nHost: api\n\n'' | nc api 80`'
- en: 1 Creates an HTTP request and pipes through Netcat to the API
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 创建一个 HTTP 请求并通过 Netcat 发送到 API
- en: 'You should see output similar to that in the previous section:'
  id: totrans-609
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应该看到与上一节类似的输出：
- en: '`HTTP/1.0 200 OK Connection: close Content-Type: text/plain; charset=utf-8
    Date: Wed, 13 Feb 2019 05:21:43 GMT Content-Length: 98  Welcome to the API Server!
    Server 82f4ab268c2a responded at 2019-02-13 05:21:43.3537073 +0000 UTC`'
  id: totrans-610
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`HTTP/1.0 200 OK 连接: 关闭 内容类型: text/plain; charset=utf-8 日期: Wed, 13 Feb 2019
    05:21:43 GMT 内容长度: 98  欢迎来到 API 服务器！服务器 82f4ab268c2a 在 2019-02-13 05:21:43.3537073
    +0000 UTC 响应。`'
- en: 'We successfully issued a request to the `api` service from a shell attached
    to the `private` network. This works because the `api` service is attached to
    the `private` network in addition to the `public` and `ingress` networks. You
    can also connect to the PostgreSQL DB from your shell:'
  id: totrans-611
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们成功从连接到`private`网络的shell向`api`服务发出了请求。这是因为`api`服务除了连接到`public`和`ingress`网络外，还连接到了`private`网络。你还可以从你的shell连接到PostgreSQL数据库：
- en: '`/ # nc -vz postgres 5432 postgres (10.0.2.6:5432) open`'
  id: totrans-612
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/ # nc -vz postgres 5432 postgres (10.0.2.6:5432) open`'
- en: This Netcat command opens a socket to the `postgres` hostname on port 5432 and
    then closes it right away. Netcat’s output indicates that it succeeded in connecting
    to the `postgres` VIP, 10.0.2.6\. This might surprise you. After all, if you review
    the `postgres` service definition, you can confirm that we never published or
    exposed any ports. What’s going on here?
  id: totrans-613
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个Netcat命令在端口5432上打开到`postgres`主机名的套接字，然后立即关闭。Netcat的输出表明它成功连接到了`postgres` VIP，10.0.2.6。这可能会让你感到惊讶。毕竟，如果你回顾`postgres`服务定义，你可以确认我们从未发布或暴露过任何端口。这里发生了什么？
- en: Communication between containers attached to a given Docker network is completely
    open. There are no firewalls between containers on a Docker overlay network. Because
    the PostgreSQL server is listening on port 5432 and is attached to the `private`
    network, any other container attached to that network can connect to it.
  id: totrans-614
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 连接到给定Docker网络的容器之间的通信是完全开放的。在Docker overlay网络上的容器之间没有防火墙。因为PostgreSQL服务器正在监听端口5432，并且连接到`private`网络，所以任何连接到该网络的另一个容器都可以连接到它。
- en: This behavior might be convenient in some cases. However, you may need to approach
    access control between connected services differently than you are accustomed
    to. We will discuss some ideas for isolating service-to-service communications
    next.
  id: totrans-615
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在某些情况下，这种行为可能很方便。然而，你可能需要以不同于你习惯的方式处理连接服务之间的访问控制。接下来，我们将讨论一些隔离服务间通信的想法。
- en: 13.3.4\. Isolating service-to-service communication with overlay networks
  id: totrans-616
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3.4. 使用overlay网络隔离服务间通信
- en: Many people control access to a service by restricting the network connections
    that can be made to that service. For example, it is common to use a firewall
    that permits traffic to flow from service A to service B, but not permit traffic
    in the reverse direction from B to A. This approach does not translate well to
    Docker overlay networks because there are no firewalls between peers connected
    to a given network. Traffic flows freely in both directions. The only access-control
    mechanism available for an overlay network is attachment (or not) to the network.
  id: totrans-617
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多人通过限制可以连接到该服务的网络连接来控制对服务的访问。例如，使用允许从服务A到服务B流量流动但禁止从B到A反向流量的防火墙是很常见的。这种方法不适合Docker
    overlay网络，因为连接到给定网络的对等体之间没有防火墙。流量在两个方向上自由流动。overlay网络唯一可用的访问控制机制是（或不是）连接到网络。
- en: However, you can achieve substantial isolation of application traffic flows
    with Docker overlay networks. Overlay networks are lightweight and easy to create
    with Swarm so they can be used as a design tool to create secure application communication
    topologies. You can use fine-grained, application-specific networks for your application
    deployments and avoid sharing services to achieve isolation. The example application
    demonstrates this approach, with the exception of making the `private` network
    attachable.
  id: totrans-618
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，你可以通过使用Docker overlay网络来实现应用程序流量流的实质性隔离。Overlay网络轻量级且易于Swarm创建，因此可以用作设计工具来创建安全的应用程序通信拓扑。你可以为你的应用程序部署使用细粒度、特定于应用程序的网络，以避免共享服务来实现隔离。示例应用程序展示了这种方法，除了将`private`网络设置为可连接之外。
- en: The key point to remember is that while traffic flows on a tightly scoped network
    may be isolated to a few containers, there is no such thing as using a network
    identity to authenticate and authorize traffic. When an application needs to control
    access to its functionality, the application must verify the identity and authorization
    of clients at the application level. The example application controls access to
    the `postgres` database by using the PostgreSQL user and password. This ensures
    that only the `api` service can interact with the database in our deployment.
    The `api` service is meant to be used anonymously, so it does not implement authentication,
    but it certainly could.
  id: totrans-619
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要记住的关键点是，尽管在范围紧密的网络中流量可能仅限于几个容器，但并没有使用网络标识来验证和授权流量的说法。当应用程序需要控制对其功能的访问时，应用程序必须在应用级别验证客户端的身份和授权。示例应用程序通过使用
    PostgreSQL 用户名和密码来控制对 `postgres` 数据库的访问。这确保了只有 `api` 服务可以与我们的部署中的数据库交互。`api` 服务旨在匿名使用，因此它没有实现身份验证，但它当然可以。
- en: One challenge you may run into is integrating centralized, shared services such
    as a logging service. Suppose an application such as our example and a centralized
    logging service are attached to a shared network. Docker networks would enable
    the `logging` service to contact the `api` or `postgres` service if it (or an
    attacker) chooses to do so.
  id: totrans-620
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能会遇到的一个挑战是集成集中式、共享的服务，例如日志服务。假设我们的示例应用程序和集中式日志服务连接到共享网络。Docker 网络将允许 `logging`
    服务在需要时（或攻击者）联系 `api` 或 `postgres` 服务。
- en: The solution to this problem is to deploy the centralized logging service or
    other shared services as a Docker service that publishes a port. Swarm will set
    up a listener for the `logging` service on the `ingress` network. Clients running
    inside the cluster can connect to this service like any other published service.
    Connections from tasks and containers running inside the cluster will be routed
    to the logging service as described in [section 13.3.1](#filepos1426043). Because
    the logging service’s listener will be available on every node of the Swarm cluster,
    the logging service should authenticate its clients.
  id: totrans-621
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是部署集中式日志服务或其他共享服务作为发布端口的 Docker 服务。Swarm 将在 `ingress` 网络上为 `logging`
    服务设置监听器。运行在集群内部的客户端可以像连接其他已发布服务一样连接到该服务。来自集群内部运行的任务和容器的连接将按照[第 13.3.1 节](#filepos1426043)中描述的方式路由到日志服务。因为日志服务的监听器将在
    Swarm 集群的每个节点上可用，所以日志服务应该验证其客户端。
- en: 'Let’s demonstrate this idea with a simple `echo` service that replies with
    whatever input you send it. First create the service:'
  id: totrans-622
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们用一个简单的 `echo` 服务来演示这个想法，该服务会回复你发送的任何输入。首先创建该服务：
- en: '`docker service create --name echo --publish ''8000:8'' busybox:1.29 \    
    nc -v -lk -p 8 -e /bin/cat`'
  id: totrans-623
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service create --name echo --publish ''8000:8'' busybox:1.29 \ nc -v
    -lk -p 8 -e /bin/cat`'
- en: 'If you send data to the `echo` service using port 8000 of a cluster node using
    Netcat (`nc`):'
  id: totrans-624
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你通过集群节点的 8000 端口使用 Netcat (`nc`) 向 `echo` 服务发送数据：
- en: '`echo "Hello netcat my old friend, I''ve come to test connections again." \
        | nc -v -w 3 192.168.1.26 8000` `1`'
  id: totrans-625
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`echo "Hello netcat my old friend, I''ve come to test connections again." \
    | nc -v -w 3 192.168.1.26 8000` `1`'
- en: 1 Replace 192.168.1.26 with the IP of one of your cluster’s nodes or use $(hostname
    -i) to substitute the current host IP on Linux.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 将 192.168.1.26 替换为你的集群节点之一的 IP 地址，或者在 Linux 上使用 $(hostname -i) 来替换当前主机的 IP
    地址。
- en: 'Netcat should print a response similar to this:'
  id: totrans-627
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Netcat 应该打印出类似于以下内容的响应：
- en: '`192.168.1.26 (192.168.1.26:8000) open Hello netcat my old friend, I''ve come
    to test connections again.`'
  id: totrans-628
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`192.168.1.26 (192.168.1.26:8000) open Hello netcat my old friend, I''ve come
    to test connections again.`'
- en: 'Clients should connect to shared services by using a port published by that
    service. Switch to or reopen the shell we created in the previous section so we
    can verify a few things:'
  id: totrans-629
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 客户端应通过使用该服务发布的端口连接到共享服务。切换到或重新打开上一节中创建的shell，以便我们可以验证一些事情：
- en: '`docker container run --rm -it --network multi-tier-app_private \     alpine:3.8
    sh`'
  id: totrans-630
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker container run --rm -it --network multi-tier-app_private \ alpine:3.8
    sh`'
- en: 'Then, if you try to ping the `echo` service, the ping will report an error:'
  id: totrans-631
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后，如果你尝试ping `echo` 服务，ping 将报告错误：
- en: '`/ $ ping -c 1 echo ping: bad address ''echo''`'
  id: totrans-632
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/ $ ping -c 1 echo ping: bad address ''echo''`'
- en: 'The same occurs with `nslookup` when trying to resolve the hostname `echo`:'
  id: totrans-633
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当尝试解析主机名 `echo` 时，`nslookup` 也会出现相同的情况：
- en: '`/ $ nslookup echo nslookup: can''t resolve ''(null)'': Name does not resolve`'
  id: totrans-634
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/ $ nslookup echo nslookup: can''t resolve ''(null)'': Name does not resolve`'
- en: The `echo` service’s name doesn’t resolve when attached to the `multi-tier-app_private`
    network. The `api` service needs to connect to the port published by the `echo`
    service at the cluster’s edge, just like processes running outside the Swarm cluster.
    The only route to the `echo` service is through the `ingress` network.
  id: totrans-635
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当附加到`multi-tier-app_private`网络时，`echo`服务的名称无法解析。`api`服务需要连接到在集群边缘由`echo`服务发布的端口，就像在Swarm集群外运行的进程一样。到达`echo`服务的唯一途径是通过`ingress`网络。
- en: We can say a few good things about this design. First, all clients reach the
    `echo` service in a uniform way, through a published port. Second, because we
    didn’t join the `echo` service to any networks (besides the implicit `ingress`
    network join), it is isolated and cannot connect to other services, except for
    those that are published. Third, Swarm has pushed application authentication responsibilities
    into the application layer, where they belong.
  id: totrans-636
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以关于这个设计说一些好事。首先，所有客户端都以统一的方式到达`echo`服务，通过一个已发布的端口。其次，因为我们没有将`echo`服务连接到任何网络（除了隐含的`ingress`网络连接），所以它是隔离的，无法连接到其他服务，除了那些已发布的。第三，Swarm已经将应用程序认证责任推入应用层，这是它们应该所在的地方。
- en: One of the main implications with this design is that an application described
    with Docker Compose may rely on two sets of names for services and their locations.
    First, some services are scoped to and defined within the application’s deployment
    (for example, `api` depends on `postgres`). Second, there are services such as
    the `echo` service that an application may depend on, but that are managed with
    a different deployment life cycle and have a different scope. These latter services
    may be shared by many applications. This second kind of service needs to be registered
    with a registry such as the corporate-wide DNS so that applications can discover
    its location. Next we will examine how client connections are balanced behind
    a service VIP after its location has been discovered.
  id: totrans-637
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种设计的主要影响之一是，使用Docker Compose描述的应用程序可能依赖于两组服务及其位置名称。首先，一些服务被限定在应用程序的部署范围内，并在其中定义（例如，`api`依赖于`postgres`）。其次，有一些服务，如`echo`服务，应用程序可能依赖于，但它们使用不同的部署生命周期和范围进行管理。这些后者的服务可能被许多应用程序共享。这种第二种类型的服务需要在一个注册表中注册，如企业级的DNS，以便应用程序可以找到其位置。接下来，我们将检查在发现位置后，如何在服务VIP之后平衡客户端连接。
- en: 13.3.5\. Load balancing
  id: totrans-638
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.3.5. 负载均衡
- en: Let’s explore how Docker client connections are balanced across a Docker service’s
    tasks. Clients usually connect to Docker services through a virtual IP. Docker
    services have a property called `endpoint-mode` that defaults to `vip`. We have
    been using this default `vip` endpoint mode for all of our examples so far. When
    a service uses the `vip` endpoint mode, clients will access the service through
    the VIP. Connections to that VIP will be load-balanced automatically by Docker.
  id: totrans-639
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们探索Docker客户端连接如何在Docker服务的任务之间进行平衡。客户端通常通过虚拟IP连接到Docker服务。Docker服务有一个名为`endpoint-mode`的属性，默认为`vip`。到目前为止，我们一直在使用这个默认的`vip`端点模式。当服务使用`vip`端点模式时，客户端将通过VIP访问服务。到该VIP的连接将由Docker自动进行负载均衡。
- en: For example, in [section 13.3.3](#filepos1442260), we attached a shell to the
    `multi-tier-app_private` network and used Netcat to issue an HTTP request to the
    `api`. When Netcat resolved the `api` hostname to an IP, Docker’s internal DNS
    replied with the VIP of the `api` service. In that case, more than one healthy
    service task was available. Docker’s network routing implementation is responsible
    for distributing connections to the service VIP equally between the healthy tasks
    behind the VIP.
  id: totrans-640
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，在[第13.3.3节](#filepos1442260)中，我们将shell附加到`multi-tier-app_private`网络，并使用Netcat向`api`发出HTTP请求。当Netcat将`api`主机名解析为IP时，Docker的内部DNS回复了`api`服务的VIP。在这种情况下，有多个健康的服务任务可用。Docker的网络路由实现负责在VIP后面的健康任务之间平均分配连接。
- en: Docker’s network-based load-balancing implementation is used for all traffic
    routed through a VIP endpoint. That traffic could be from an internal overlay
    network or come in through a port published to the `ingress` network.
  id: totrans-641
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker基于网络的负载均衡实现被用于所有通过VIP端点路由的流量。这种流量可能来自内部覆盖网络，或者通过发布到`ingress`网络的端口进入。
- en: Docker does not guarantee which service task will handle a client’s request.
    Even when a client is running on the same node as a healthy service task, the
    client’s request may go to a healthy task on another node. This is true even for
    services deployed in `global` mode (distinct from endpoint mode), where an instance
    runs on each cluster node.
  id: totrans-642
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker 不保证哪个服务任务将处理客户端的请求。即使客户端与健康的任务运行在同一节点上，客户端的请求也可能被发送到另一个节点上的健康任务。这在 `global`
    模式（与端点模式不同）的服务中也是真实的，其中每个实例都运行在每个集群节点上。
- en: 13.4\. PLACING SERVICE TASKS ON THE CLUSTER
  id: totrans-643
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.4\. 在集群上放置服务任务
- en: In this section, we will investigate how Swarm places tasks around the cluster
    and tries to run the desired number of service replicas within declared constraints.
    First, we will introduce the coarse-grained controls Swarm has for managing task
    placement. Then we’ll show you how to control the placement of tasks by using
    affinity and anti-affinity to built-in and operator-specified node labels.
  id: totrans-644
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究 Swarm 如何在集群周围放置任务，并尝试在声明的约束内运行所需数量的服务副本。首先，我们将介绍 Swarm 管理任务放置的粗粒度控制。然后，我们将向您展示如何通过使用亲和力和反亲和力来控制任务放置，这些亲和力和反亲和力是通过内置和操作员指定的节点标签实现的。
- en: We will use a five-node swarm cluster created from a Play with Docker template,
    depicted in [figure 13.9](#filepos1473456).
  id: totrans-645
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将使用由 Play with Docker 模板创建的五个节点 Swarm 集群，如图 13.9 所示。
- en: Figure 13.9\. The test Swarm cluster
  id: totrans-646
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.9\. 测试 Swarm 集群
- en: '![](images/00079.jpg)'
  id: totrans-647
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00079.jpg)'
- en: 'This cluster has three manager nodes and two worker nodes, which are named:'
  id: totrans-648
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此集群有三个管理节点和两个工作节点，其名称如下：
- en: '`manager1`'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manager1`'
- en: '`manager2`'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manager2`'
- en: '`manager3`'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manager3`'
- en: '`worker1`'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`worker1`'
- en: '`worker2`'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`worker2`'
- en: 13.4.1\. Replicating services
  id: totrans-654
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.4.1\. 复制服务
- en: The default, and most commonly used, deployment mode for a Docker service is
    replicated. Swarm will try to keep the number of replicas specified in a service’s
    definition running at all times. Swarm continually reconciles the desired state
    of the service specified by the Docker Compose definition or `docker service`
    command, and the state of the service’s tasks on the cluster. This reconciliation
    loop, illustrated in [figure 13.10](#filepos1475667), will continuously start
    or stop tasks to match so that the service has the desired number of healthy replicas.
  id: totrans-655
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于 Docker 服务而言，默认且最常用的部署模式是复制模式。Swarm 将尝试始终保持服务定义中指定的副本数量运行。Swarm 会持续协调 Docker
    Compose 定义或 `docker service` 命令中指定的服务期望状态，以及集群上服务任务的状态。此协调循环，如图 13.10 所示，将不断启动或停止任务以匹配，以确保服务具有所需的健康副本数量。
- en: Figure 13.10\. Event reconciliation loop
  id: totrans-656
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 13.10\. 事件协调循环
- en: '![](images/00004.jpg)'
  id: totrans-657
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00004.jpg)'
- en: Replicating a service is useful because you can scale the service to as many
    replicas as needed to handle the load and that your cluster has resources to support.
  id: totrans-658
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 复制服务是有用的，因为你可以将服务扩展到所需的副本数量，以处理负载，并且你的集群有足够的资源来支持。
- en: In this mode, Swarm will schedule a service task to start on a cluster node
    that has sufficient compute resources (memory, CPU) and that satisfies the service’s
    labelled constraints. Swarm tries to spread the service’s tasks across the cluster’s
    nodes. This strategy is helpful for improving service availability and smoothing
    load across nodes. We will control where tasks run in the next section. For now,
    let’s see what happens when we start scaling our example application’s `api` service.
  id: totrans-659
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在此模式下，Swarm 将在具有足够计算资源（内存、CPU）并满足服务标签约束的集群节点上调度一个服务任务以启动。Swarm 尝试将服务的任务分散到集群的各个节点上。这种策略有助于提高服务的可用性并平衡节点间的负载。我们将在下一节中控制任务运行的位置。现在，让我们看看当我们开始扩展示例应用的
    `api` 服务时会发生什么。
- en: 'The `api` service is configured to have two replicas by default. The deployment
    definition also reserves and limits the CPU and memory resources that each container
    can use:'
  id: totrans-660
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认情况下，`api` 服务配置为具有两个副本。部署定义还预留和限制了每个容器可以使用的 CPU 和内存资源：
- en: '`      deploy:           replicas: 2           restart_policy:              
    condition: on-failure               max_attempts: 10               delay: 5s          
    update_config:               parallelism: 1               delay: 5s          
    resources:             limits:               cpus: ''0.50''               memory:
    15M             reservations:               cpus: ''0.25''               memory:
    15M`'
  id: totrans-661
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`      deploy:         replicas: 2         restart_policy:             condition:
    on-failure             max_attempts: 10             delay: 5s         update_config:            
    parallelism: 1             delay: 5s         resources:           limits:            
    cpus: ''0.50''             memory: 15M           reservations:             cpus:
    ''0.25''             memory: 15M`'
- en: When Swarm schedules each `api` task, it will look for a node with at least
    15 MB of memory and 0.25 CPUs that have not been reserved for other tasks. Once
    a node with sufficient resources has been identified, Swarm will create a container
    for the task that is limited to (again) 15 MB of memory and may use up to 0.5
    CPUs.
  id: totrans-662
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当 Swarm 为每个 `api` 任务进行调度时，它将寻找至少有 15 MB 内存和 0.25 个 CPU 的节点，这些资源尚未被其他任务保留。一旦识别到具有足够资源的节点，Swarm
    将为该任务创建一个容器，该容器限制为（再次）15 MB 内存，并且可能使用高达 0.5 个 CPU。
- en: 'In aggregate, the `api` service begins with two replicas that reserve a total
    of 0.5 CPUs and 30 MB of memory. Now let’s scale up our service a bit with five
    replicas:'
  id: totrans-663
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总体而言，`api` 服务开始时有两个副本，总共保留了 0.5 个 CPU 和 30 MB 内存。现在让我们将我们的服务扩展到五个副本：
- en: '`docker service scale multi-tier-app_api=5`'
  id: totrans-664
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service scale multi-tier-app_api=5`'
- en: 'The service now reserves 75 MB of memory and 1.25 CPUs in aggregate. Swarm
    was able to find resources for the `api` service’s tasks and spread them across
    this cluster:'
  id: totrans-665
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 服务现在总共保留了 75 MB 的内存和 1.25 个 CPU。Swarm 能够为 `api` 服务的任务找到资源，并将它们分散在这个集群中：
- en: '`$ docker service ps multi-tier-app_api \     --filter ''desired-state=running''
    \     --format ''table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}'' ID          
    NAME                 NODE     CURRENT STATE dekzyqgcc7fs multi-tier-app_api.1
    worker1  Running 4 minutes ago 3el58dg6yewv multi-tier-app_api.2 manager1 Running
    5 minutes ago qqc72ylzi34m multi-tier-app_api.3 manager3 Running about a minute
    ago miyugogsv2s7 multi-tier-app_api.4 manager2 Starting 4 seconds ago zrp1o0aua29y
    multi-tier-app_api.7 worker1  Running 17 minutes ago`'
  id: totrans-666
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service ps multi-tier-app_api \   --filter ''desired-state=running''
    \   --format ''table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}'' ID          
    NAME                 NODE     CURRENT STATE dekzyqgcc7fs multi-tier-app_api.1
    worker1  Running 4 minutes ago 3el58dg6yewv multi-tier-app_api.2 manager1 Running
    5 minutes ago qqc72ylzi34m multi-tier-app_api.3 manager3 Running about a minute
    ago miyugogsv2s7 multi-tier-app_api.4 manager2 Starting 4 seconds ago zrp1o0aua29y
    multi-tier-app_api.7 worker1  Running 17 minutes ago`'
- en: Now let’s demonstrate what it looks like when a service reserves all of a cluster’s
    resources. You should follow along only if you are using a cluster where it is
    OK for you to exhaust cluster resources and prevent other tasks from being scheduled.
    We’ll undo all of this when we’re done, but at some point, no new tasks that reserve
    CPU can be scheduled. We recommend that you do not run this resource exhaustion
    exercise on Play with Docker (PWD) because the underlying machines are shared
    by everyone using PWD.
  id: totrans-667
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在我们来演示一下当服务保留集群所有资源时的样子。只有在你使用的集群允许你耗尽集群资源并阻止其他任务调度的情况下，你才应该跟随操作。我们完成这些操作后，会撤销所有这些设置，但在某个时刻，将无法再调度任何保留
    CPU 的新任务。我们建议你不要在 Play with Docker (PWD) 上运行这种资源耗尽练习，因为底层的机器是由使用 PWD 的每个人共享的。
- en: 'First, let’s increase the CPU reservation of our `api` tasks from a quarter
    of a CPU to an entire CPU:'
  id: totrans-668
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，让我们将我们的 `api` 任务保留的 CPU 从四分之一 CPU 增加到整个 CPU：
- en: '`docker service update multi-tier-app_api --reserve-cpu 1.0 --limit-cpu 1.0`'
  id: totrans-669
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update multi-tier-app_api --reserve-cpu 1.0 --limit-cpu 1.0`'
- en: You will see Docker shuffling tasks around as it re-creates the containers for
    each task with the new limits on a node with capacity.
  id: totrans-670
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你会看到 Docker 在节点容量限制下为每个任务重新创建容器时，正在将任务在节点间调度。
- en: Now let’s try scaling the service to a larger number of replicas that will exhaust
    the cluster’s available resources. For example, if you’re running a five-node
    cluster and each node has 2 CPUs, then there should be 10 CPUs reservable in total.
  id: totrans-671
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在我们尝试将服务扩展到更多的副本数量，这将耗尽集群的可用资源。例如，如果你运行的是一个五节点集群，并且每个节点有 2 个 CPU，那么总共应该有 10
    个 CPU 可用。
- en: 'The following output comes from a cluster with 10 reservable CPUs. The `postgres`
    service has reserved 1 CPU. The `api` service can be scaled successfully to 9
    replicas:'
  id: totrans-672
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下输出来自一个有 10 个可用 CPU 的集群。`postgres` 服务已保留 1 个 CPU。`api` 服务可以成功扩展到 9 个副本：
- en: '`$ docker service scale multi-tier-app_api=9 multi-tier-app_api scaled to 9
    overall progress: 9 out of 9 tasks 1/9: running   [==================================================>]
    ... snip ... 9/9: running   [==================================================>]
    verify: Service converged`'
  id: totrans-673
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service scale multi-tier-app_api=9 multi-tier-app_api scaled to 9
    overall progress: 9 out of 9 tasks 1/9: running   [==================================================>]
    ... snip ... 9/9: running   [==================================================>]
    verify: Service converged`'
- en: 'All 10 CPUs are now reserved by the `api` and `postgres` services. When scaling
    the service to 10 replicas, the `docker` program appears to hang:'
  id: totrans-674
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有 10 个 CPU 现在都已被 `api` 和 `postgres` 服务保留。当将服务扩展到 10 个副本时，`docker` 程序似乎挂起了：
- en: '`docker service scale multi-tier-app_api=10 multi-tier-app_api scaled to 10
    overall progress: 9 out of 10 tasks 1/10: running   [==================================================>]
    ... snip ... 10/10: no suitable node (insufficient resources on 5 nodes)` `1`'
  id: totrans-675
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service scale multi-tier-app_api=10 multi-tier-app_api scaled to 10
    overall progress: 9 out of 10 tasks 1/10: running   [==================================================>]
    ... snip ... 10/10: no suitable node (insufficient resources on 5 nodes)` `1`'
- en: 1 Insufficient resources to create the 10th api task
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 创建第 10 个 api 任务时资源不足
- en: The output reports there are insufficient resources on the cluster’s five nodes
    to launch a 10th task. The trouble occurs when Swarm tries to schedule a task
    for the 10th `api` service task slot. When you run the cluster out of reservable
    resources, you will need to interrupt the `docker stack deploy` command with a
    `^C` keystroke to get your terminal back or wait for the command to time out.
    The Docker command will suggest that you run `docker service ps multi-tier-app_api`
    to get more info and check whether the service converges.
  id: totrans-677
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 输出报告集群的五个节点上资源不足，无法启动第 10 个任务。问题发生在 Swarm 尝试为第 10 个 `api` 服务任务槽调度任务时。当你耗尽可预订资源时，你需要使用
    `^C` 键盘操作中断 `docker stack deploy` 命令以获取终端或等待命令超时。Docker 命令将建议你运行 `docker service
    ps multi-tier-app_api` 以获取更多信息并检查服务是否收敛。
- en: Go ahead and do that now and verify that `api` tasks are distributed across
    all of the cluster nodes and Swarm is unable to schedule the last task. In this
    case, we know the cluster will never converge unless we increase cluster capacity
    or reduce the desired number of replicas. Let’s revert our changes.
  id: totrans-678
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在就去做，并验证 `api` 任务是否已分布到所有集群节点，Swarm 无法调度最后一个任务。在这种情况下，我们知道除非我们增加集群容量或减少期望的副本数量，否则集群永远不会收敛。让我们撤销我们的更改。
- en: '|    |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Autoscaling services
  id: totrans-680
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自动缩放服务
- en: Docker Swarm does not support autoscaling services by using built-in functionality.
    Third-party solutions can use resource usage metrics such as CPU or memory utilization
    or application-level metrics such as HTTP requests per task. The Docker Flow project
    is a good place to start, [https://monitor.dockerflow.com/auto-scaling/](https://monitor.dockerflow.com/auto-scaling/).
  id: totrans-681
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker Swarm 不支持使用内置功能自动缩放服务。第三方解决方案可以使用资源使用指标，如 CPU 或内存利用率，或应用程序级别的指标，如每任务的
    HTTP 请求。Docker Flow 项目是一个很好的起点，[https://monitor.dockerflow.com/auto-scaling/](https://monitor.dockerflow.com/auto-scaling/)。
- en: '|    |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: 'We have a few options for reverting the scaling changes. We can redeploy our
    stack from its source definition, roll back the service configuration changes
    with the `docker service rollback` subcommand, or “roll forward” and set the service
    scale directly to something that will work. Try rolling back:'
  id: totrans-683
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们有几种方法可以撤销缩放更改。我们可以从源定义重新部署我们的堆栈，使用 `docker service rollback` 子命令回滚服务配置更改，或者“向前滚动”并将服务缩放直接设置为可以工作的某个值。尝试回滚：
- en: '`$ docker service rollback multi-tier-app_api multi-tier-app_api rollback:
    manually requested rollback overall progress: rolling back update: 9 out of 9
    tasks ... snip ... verify: Service converged`'
  id: totrans-684
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service rollback multi-tier-app_api multi-tier-app_api rollback:
    manually requested rollback overall progress: rolling back update: 9 out of 9
    tasks ... snip ... verify: Service converged`'
- en: The `service rollback` subcommand shifts a service’s desired configuration back
    by one version. The previous configuration of `multi-tier-app_api` had nine replicas.
    You can confirm that this configuration has taken effect by running `docker service
    ls`. The output should show that the `multi-tier-app_api` service has the pre-exhaustion
    number of replicas running; for example, 9/9\. You might wonder what will happen
    if you run `rollback` again. If you execute another `rollback`, Docker will restore
    the config with 10 service replicas, exhausting resources again. That is, Docker
    will roll back the rollback, leaving us where we started. Since we’d like to undo
    multiple changes, we’ll need another method.
  id: totrans-685
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`service rollback` 子命令将服务的期望配置回滚一个版本。`multi-tier-app_api` 的先前配置有九个副本。你可以通过运行
    `docker service ls` 来确认此配置是否生效。输出应显示 `multi-tier-app_api` 服务正在运行的副本数已达到预耗尽数；例如，9/9。你可能想知道如果你再次运行
    `rollback` 会发生什么。如果你执行另一个 `rollback`，Docker 将恢复具有 10 个服务副本的配置，再次耗尽资源。也就是说，Docker
    将撤销回滚，使我们回到起点。由于我们想撤销多个更改，我们需要另一种方法。'
- en: 'The cleanest approach in our case is to redeploy the service from its source
    definition:'
  id: totrans-686
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的情况下，最干净的方法是从其源定义重新部署服务：
- en: '`docker stack deploy --compose-file docker-compose.yml multi-tier-app`'
  id: totrans-687
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker stack deploy --compose-file docker-compose.yml multi-tier-app`'
- en: 'Take a look at the service’s tasks with `docker service ps` to ensure that
    the service has returned to the state declared in the Docker Compose application
    definition:'
  id: totrans-688
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用`docker service ps`查看服务任务，以确保服务已返回Docker Compose应用程序定义中声明的状态：
- en: '`docker service ps multi-tier-app_api \     --filter ''desired-state=running''
    \     --format ''table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}'' ID          
    NAME                 NODE     CURRENT STATE h0to0a2lbm87 multi-tier-app_api.1
    worker1  Running about a minute ago v6sq9m14q3tw multi-tier-app_api.2 manager2
    Running about a minute ago`'
  id: totrans-689
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps multi-tier-app_api \      --filter ''desired-state=running''
    \      --format ''table {{.ID}} {{.Name}} {{.Node}} {{.CurrentState}}'' ID          
    NAME                 NODE     CURRENT STATE h0to0a2lbm87 multi-tier-app_api.1
    worker1  正在运行大约一分钟前 v6sq9m14q3tw multi-tier-app_api.2 manager2 正在运行大约一分钟前`'
- en: The manual scaling changes are gone. There are two `api` tasks, as expected.
  id: totrans-690
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 手动缩放更改已消失。正如预期的那样，有两个`api`任务。
- en: 'Notice that one task is running on the `worker1` node and the other is running
    on `manager2` nodes. This isn’t really the task placement we’d like for most deployments.
    Usually, we’d like to implement architectural goals like these:'
  id: totrans-691
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，一个任务正在`worker1`节点上运行，而另一个任务正在`manager2`节点上运行。这并不是我们大多数部署所期望的任务放置方式。通常，我们希望实现如下架构目标：
- en: Reserve manager nodes for running the Swarm control plane so they have dedicated
    compute resources
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为运行Swarm控制平面保留管理节点，以便它们有专门的计算资源
- en: Isolate services that publish ports because they are easier to attack than `private`
    services
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将发布端口的隔离服务，因为它们比`private`服务更容易受到攻击
- en: We’ll accomplish these goals and more using Swarm’s built-in features for constraining
    where tasks run in the next section.
  id: totrans-694
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将在下一节中使用Swarm内置的功能来限制任务运行的位置，以实现这些目标以及更多。
- en: 13.4.2\. Constraining where tasks run
  id: totrans-695
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.4.2\. 限制任务运行的位置
- en: We often want to control which nodes in a cluster that an application runs on.
    We might want to do this in order to isolate workloads into different environments
    or security zones, take advantage of special machine capabilities such as GPUs,
    or reserve a set of nodes for a critical function.
  id: totrans-696
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们经常想要控制应用程序在集群中的哪些节点上运行。我们可能想要这样做，以便将工作负载隔离到不同的环境或安全区域，利用特殊的机器能力，如GPU，或者为关键功能保留一组节点。
- en: Docker services provide a feature called placement constraints that allow you
    to control the nodes a service’s tasks can be assigned to. With placement constraints,
    you can say where service tasks should or should not run. The constraints can
    use both built-in and user-defined properties of your cluster nodes. We’ll work
    through examples of each.
  id: totrans-697
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker服务提供了一种名为放置约束的功能，允许您控制服务任务可以分配到的节点。使用放置约束，您可以指定服务任务应该或不应该运行的位置。约束可以使用集群节点的内置和用户定义属性。我们将通过每个示例来演示。
- en: In the previous section, we saw that the `api` service was distributed to all
    nodes when scaled up. The `api` service ran on manager nodes as well on the same
    node as the `postgres` database, as shown in [figure 13.11](#filepos1496991).
  id: totrans-698
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到当进行扩展时，`api`服务被分配到所有节点。`api`服务不仅运行在管理节点上，还与`postgres`数据库在同一节点上运行，如图13.11所示。
- en: Figure 13.11\. API service tasks are everywhere
  id: totrans-699
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图13.11\. API服务任务无处不在
- en: '![](images/00061.jpg)'
  id: totrans-700
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00061.jpg)'
- en: Many system architects would adjust this deployment architecture so that manager
    nodes are dedicated to the running Swarm cluster. This is a good idea for important
    clusters because if a service consumes resources such as CPU, Swarm might fall
    behind in supervising tasks and responding to operational commands that would
    affect the operation of all services on the cluster. Also, because Swarm managers
    control the cluster, access to those nodes (and the Docker Engine API) should
    be controlled tightly. We can use Swarm’s node availability and service placement
    constraints to do this.
  id: totrans-701
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多系统架构师会调整这种部署架构，以便管理节点专门用于运行Swarm集群。对于重要的集群来说，这是一个好主意，因为如果服务消耗如CPU等资源，Swarm可能会在监督任务和响应影响集群上所有服务操作的操作命令时落后。此外，由于Swarm管理器控制集群，因此应严格控制对这些节点（以及Docker
    Engine API）的访问。我们可以使用Swarm的节点可用性和服务放置约束来实现这一点。
- en: 'Let’s start by ensuring that our services do not run on manager nodes. All
    nodes in a Swarm cluster are available to run service tasks by default. However,
    we can reconfigure the availability of nodes by using the `docker node update`
    command’s `--availability` option. There are three availability options: `active`,
    `pause`, and `drain`. The `active` option means the schedule can assign new tasks
    to the node. The `pause` option means existing tasks will continue to run, but
    no new tasks will be scheduled to the node. The `drain` option means existing
    tasks will be shut down and restarted on another node, and no new tasks will be
    scheduled to that node.'
  id: totrans-702
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们先确保我们的服务不在管理节点上运行。Swarm集群中的所有节点默认情况下都可以运行服务任务。然而，我们可以通过使用`docker node update`命令的`--availability`选项来重新配置节点的可用性。有三个可用性选项：`active`、`pause`和`drain`。`active`选项表示调度程序可以分配新任务给该节点。`pause`选项表示现有任务将继续运行，但不会为新任务分配给该节点。`drain`选项表示现有任务将被关闭并在另一个节点上重新启动，并且不会为新任务分配给该节点。
- en: 'So we can set the availability of the manager nodes to `drain` to keep service
    tasks from running on them:'
  id: totrans-703
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，我们可以将管理节点的可用性设置为`drain`以防止服务任务在其上运行：
- en: '`docker node update --availability drain manager1 docker node update --availability
    drain manager2 docker node update --availability drain manager3`'
  id: totrans-704
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker node update --availability drain manager1 docker node update --availability
    drain manager2 docker node update --availability drain manager3`'
- en: 'Once you run those commands, the output of `docker node ls` should reflect
    the changes in availability:'
  id: totrans-705
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦运行了这些命令，`docker node ls`的输出应反映可用性的变化：
- en: '`docker node ls --format ''table {{ .ID }} {{ .Hostname }} {{ .Availability
    }}'' ID                        HOSTNAME AVAILABILITY ucetqsmbh23vuk6mwy9itv3xo
    manager1 Drain b0jajao5mkzdd3ie91q1tewvj manager2 Drain kxfab99xvgv71tm39zbeveglj
    manager3 Drain rbw0c466qqi0d7k4niw01o3nc worker1  Active u2382qjg6v9vr8z5lfwqrg5hf
    worker2  Active`'
  id: totrans-706
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker node ls --format ''table {{ .ID }} {{ .Hostname }} {{ .Availability
    }}'' ID                        HOSTNAME AVAILABILITY ucetqsmbh23vuk6mwy9itv3xo
    manager1 Drain b0jajao5mkzdd3ie91q1tewvj manager2 Drain kxfab99xvgv71tm39zbeveglj
    manager3 Drain rbw0c466qqi0d7k4niw01o3nc worker1  Active u2382qjg6v9vr8z5lfwqrg5hf
    worker2  Active`'
- en: 'We can verify that Swarm has migrated the `multi-tier-app` service tasks to
    the worker nodes:'
  id: totrans-707
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以验证Swarm是否已将`multi-tier-app`服务任务迁移到工作节点：
- en: '`docker service ps multi-tier-app_api multi-tier-app_postgres \     --filter
    ''desired-state=running'' \     --format ''table {{ .Name }} {{ .Node }}'' NAME                     
    NODE multi-tier-app_postgres.1 worker2 multi-tier-app_api.1      worker1 multi-tier-app_api.2     
    worker2`'
  id: totrans-708
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps multi-tier-app_api multi-tier-app_postgres \    --filter
    ''desired-state=running'' \    --format ''table {{ .Name }} {{ .Node }}'' NAME                     
    NODE multi-tier-app_postgres.1 worker2 multi-tier-app_api.1      worker1 multi-tier-app_api.2     
    worker2`'
- en: If you run `docker container ps` on the manager nodes, you should not see any
    containers related to service tasks, either.
  id: totrans-709
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您在管理节点上运行`docker container ps`，您不应看到任何与服务任务相关的容器。
- en: 'Placement constraints work by expressing that a service either should or should
    not run on a node based on certain metadata. The general form of a constraint
    is as follows:'
  id: totrans-710
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 放置约束通过表达服务基于某些元数据应在或不应在节点上运行来工作。约束的一般形式如下：
- en: '`<node attribute> equals or does not equal <value>`'
  id: totrans-711
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`<节点属性>` 等于或不等于 `<值>`'
- en: 'When a service is constrained to running in a node, we say it has affinity
    for that node. When it must not run on a node, we say it has anti-affinity for
    that node. You will see these terms used throughout this and other discussions
    of service placement. Swarm’s constraint language denotes equality (a match) with
    `==`, and inequality with `!=`. When a service defines multiple constraints, a
    node must satisfy all constraints for the task to be scheduled there. That is,
    multiple constraints are `AND`’d together. For example, suppose you want to run
    a service on swarm worker nodes that are not in the public security zone. Once
    you have configured the cluster’s zone metadata, you could achieve this by running
    the service with these constraints: `node.role == worker` and `node .labels.zone
    != public`.'
  id: totrans-712
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一个服务被限制在某个节点上运行时，我们说它对该节点有亲和力。当它必须不在某个节点上运行时，我们说它对该节点有反亲和力。您将在本讨论和其他关于服务放置的讨论中看到这些术语的使用。Swarm的约束语言用`==`表示相等（匹配），用`!=`表示不等。当服务定义了多个约束时，节点必须满足所有约束，任务才能被调度到那里。也就是说，多个约束是`AND`在一起的。例如，假设您想在不在公共安全区的Swarm工作节点上运行一个服务。一旦您配置了集群的区元数据，您可以通过运行具有以下约束的服务来实现这一点：`node.role
    == worker`和`node .labels.zone != public`。
- en: 'Docker supports several node attributes that can be used as the basis of constraints:'
  id: totrans-713
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Docker支持几个节点属性，可以用作约束的基础：
- en: '`node.id`—  The unique identifier for the node in the Swarm cluster (for example,
    `ucetqsmbh23vuk6mwy9itv3xo`)'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node.id`— 蜂群集群中节点的唯一标识符（例如，`ucetqsmbh23vuk6mwy9itv3xo`）'
- en: '`node.hostname`—  The node’s hostname, (for example, `worker2`)'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node.hostname`— 节点的计算机名，（例如，`worker2`）'
- en: '`node.role`—  The node’s role in the cluster, either `manager` or `worker`'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node.role`— 节点在集群中的角色，可以是`manager`或`worker`'
- en: '`node.labels.<label name>`—  A label applied to the node by an operator (for
    example, a node with a `zone=public` label would have a node attribute of `node.labels.zone=public`)'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node.labels.<label name>`— 由操作员应用于节点的标签（例如，具有`zone=public`标签的节点将具有节点属性`node.labels.zone=public`）'
- en: '`engine.labels`—  A set of labels describing key properties of the node and
    Docker Engine such as Docker version and operating system (for example, `engine.labels.operatingsystem==ubuntu
    16.04`)'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine.labels`— 描述节点和Docker Engine关键属性的标签集合，例如Docker版本和操作系统（例如，`engine.labels.operatingsystem==ubuntu
    16.04`）'
- en: Let’s continue organizing our system by separating the worker nodes of our cluster
    into a `public` and a `private` zone. Once we have these zones, we will update
    the `api` and `postgres` services so their tasks run only in the desired zone.
  id: totrans-719
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们继续组织我们的系统，通过将集群的worker节点分为`public`和`private`区域来组织系统。一旦我们有了这些区域，我们将更新`api`和`postgres`服务，以便它们的任务只在期望的区域中运行。
- en: You can label Swarm cluster nodes with your own metadata by using the `docker
    node update` command’s `--label-add` option. This option accepts a list of key/value
    pairs that will be added to the node’s metadata. There is also a `--label-rm`
    option to remove metadata from a node. This metadata will be available for use
    in constraining tasks to particular nodes.
  id: totrans-720
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以使用`docker node update`命令的`--label-add`选项通过自己的元数据标记Swarm集群节点。此选项接受一个键/值对列表，这些键/值对将被添加到节点的元数据中。还有一个`--label-rm`选项可以从节点中删除元数据。这些元数据将可用于将任务约束到特定的节点。
- en: 'Let’s identify `worker1` as part of the `private` zone, and `worker2` as part
    of the public zone:'
  id: totrans-721
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们将`worker1`识别为`private`区域的一部分，将`worker2`识别为公共区域的一部分：
- en: '`$ docker node update --label-add zone=private worker1 worker1 $ docker node
    update --label-add zone=public worker2 worker2`'
  id: totrans-722
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker node update --label-add zone=private worker1 worker1 $ docker node
    update --label-add zone=public worker2 worker2`'
- en: 'Now constrain the `api` service to the `public` zone. The `docker service create`
    and `update` commands have options to add and remove task-scheduling constraints,
    `--constraint-add` and `--constraint-rm`, respectively. The constraint we added
    to the service tells Swarm to schedule only `api` service tasks on nodes with
    a `zone` label that equals `public`:'
  id: totrans-723
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在将`api`服务约束到`public`区域。`docker service create`和`update`命令有选项可以添加和删除任务调度约束，分别是`--constraint-add`和`--constraint-rm`。我们添加到服务中的约束告诉Swarm只在具有`zone`标签等于`public`的节点上调度`api`服务任务：
- en: '`docker service update \     --constraint-add ''node.labels.zone == public''
    \     multi-tier-app_api`'
  id: totrans-724
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update \ --constraint-add ''node.labels.zone == public'' \
    multi-tier-app_api`'
- en: 'If all goes well, Docker will report that the `api` service’s tasks have converged
    to the new state:'
  id: totrans-725
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果一切顺利，Docker将报告`api`服务的任务已收敛到新状态：
- en: '`multi-tier-app_api overall progress: 2 out of 2 tasks 1/2: running   [==================================================>]
    2/2: running   [==================================================>] verify: Service
    converged`'
  id: totrans-726
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`multi-tier-app_api 总进度：2/2任务 1/2：运行   [==================================================>]
    2/2：运行   [==================================================>] 验证：服务收敛`'
- en: 'You can verify that the `api` tasks have been rescheduled to the `worker2`
    node:'
  id: totrans-727
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以验证`api`任务已被重新调度到`worker2`节点：
- en: '`docker service ps multi-tier-app_api \     --filter ''desired-state=running''
    \     --format ''table {{ .Name }} {{ .Node }}'' NAME NODE multi-tier-app_api.1
    worker2 multi-tier-app_api.2 worker2`'
  id: totrans-728
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps multi-tier-app_api \ --filter ''desired-state=running''
    \ --format ''table {{ .Name }} {{ .Node }}'' NAME NODE multi-tier-app_api.1 worker2
    multi-tier-app_api.2 worker2`'
- en: 'Unfortunately, we can’t display node label information in `docker service ps`
    output nor see the labels we’ve added with `docker node ls`. Currently, the only
    way to see a node’s labels is to inspect the node. Here’s a quick bash shell script
    to show the hostname, role, and label information for all nodes in a cluster:'
  id: totrans-729
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法在`docker service ps`输出中显示节点标签信息，也无法看到我们使用`docker node ls`添加的标签。目前，查看节点标签的唯一方法是检查节点。以下是一个快速bash
    shell脚本，用于显示集群中所有节点的计算机名、角色和标签信息：
- en: '``for node_id in `docker node ls -q | head`; do   docker node inspect \   --format
    ''{{.Description.Hostname}} {{.Spec.Role}} {{.Spec.Labels}}''\   "${node_id}";
    done;``'
  id: totrans-730
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '``for node_id in `docker node ls -q | head`; do docker node inspect \ --format
    ''{{.Description.Hostname}} {{.Spec.Role}} {{.Spec.Labels}}''\ "${node_id}"; done;``'
- en: 'This script should output the following:'
  id: totrans-731
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个脚本应该输出以下内容：
- en: '`manager1 manager map[] manager2 manager map[] manager3 manager map[] worker1
    worker map[zone:private] worker2 worker map[zone:public]`'
  id: totrans-732
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`manager1 manager map[] manager2 manager map[] manager3 manager map[] worker1
    worker map[zone:private] worker2 worker map[zone:public]`'
- en: This isn’t great, but it’s better than trying to recall which nodes have which
    labels.
  id: totrans-733
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这并不理想，但比试图回忆哪些节点有哪些标签要好。
- en: 'The final adjustment we need to make to this system is to relocate the `postgres`
    database to the `private` zone. Before doing that, issue some queries to the `api`
    service’s `/counter` endpoint with `curl`:'
  id: totrans-734
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要对这个系统进行的最后调整是将 `postgres` 数据库迁移到 `private` 区域。在这样做之前，使用 `curl` 向 `api` 服务的
    `/counter` 端点发出一些查询：
- en: '`curl http://127.0.0.1:8080/counter`'
  id: totrans-735
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`curl http://127.0.0.1:8080/counter`'
- en: 'The `/counter` endpoint inserts a record into a table with an auto-incrementing
    `id` column. When the `api` service responds, it prints out all of the IDs in
    the column. If you issue three requests to the endpoint, the output of the third
    response should be similar to the following:'
  id: totrans-736
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`/counter` 端点将一条记录插入一个具有自增 `id` 列的表中。当 `api` 服务响应时，它会打印出该列中的所有 ID。如果你向该端点发出三个请求，第三个响应的输出应类似于以下内容：'
- en: '`# curl http://127.0.0.1:8080/counter SERVER: c098f30dd3c4 DB_ADDR: postgres
    DB_PORT: 5432 ID: 1 ID: 2 ID: 3`'
  id: totrans-737
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`# curl http://127.0.0.1:8080/counter SERVER: c098f30dd3c4 DB_ADDR: postgres
    DB_PORT: 5432 ID: 1 ID: 2 ID: 3`'
- en: This may seem like a bit of a diversion, but inserting these records will help
    demonstrate a key point in a moment.
  id: totrans-738
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这可能看起来有点偏离主题，但插入这些记录将有助于在稍后展示一个关键点。
- en: 'Let’s constrain the `postgres` task to the `private` zone:'
  id: totrans-739
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们将 `postgres` 任务约束在 `private` 区域：
- en: '`$ docker service update --constraint-add ''node.labels.zone == private'' \
        multi-tier-app_postgres multi-tier-app_postgres overall progress: 1 out of
    1 tasks 1/1: running   [==================================================>] verify:
    Service converged`'
  id: totrans-740
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service update --constraint-add ''node.labels.zone == private'' \    multi-tier-app_postgres
    multi-tier-app_postgres overall progress: 1 out of 1 tasks 1/1: running   [==================================================>]
    verify: Service converged`'
- en: 'The `postgres` task is now running on the `worker1` node:'
  id: totrans-741
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`postgres` 任务现在正在 `worker1` 节点上运行：'
- en: '`$ docker service ps multi-tier-app_postgres \     --filter ''desired-state=running''
    \     --format ''table {{ .Name }} {{ .Node }}'' NAME                      NODE
    multi-tier-app_postgres.1 worker1`'
  id: totrans-742
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service ps multi-tier-app_postgres \    --filter ''desired-state=running''
    \    --format ''table {{ .Name }} {{ .Node }}'' NAME                      NODE
    multi-tier-app_postgres.1 worker1`'
- en: 'Now, if you issue a request to the `/counter` endpoint, you will see this:'
  id: totrans-743
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，如果你向 `/counter` 端点发出请求，你会看到以下内容：
- en: '`$ curl http://127.0.0.1:8080/counter SERVER: c098f30dd3c4 DB_ADDR: postgres
    DB_PORT: 5432 ID: 1`'
  id: totrans-744
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ curl http://127.0.0.1:8080/counter SERVER: c098f30dd3c4 DB_ADDR: postgres
    DB_PORT: 5432 ID: 1`'
- en: The counter has been reset. Where did our data go? It was lost because the `postgres`
    database used a `db-data` volume that is local to a cluster node. Strictly speaking,
    the data wasn’t lost. If the `postgres` task migrates back to the `worker2` node,
    it will mount the original volume and resume counting from 3\. If you were following
    along on your own cluster and didn’t notice data loss, it’s probably because `postgres`
    happened to deploy to `worker1` to start with. This lack of determinism and potential
    for data loss is not a good situation. What can we do about it?
  id: totrans-745
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 计数器已重置。我们的数据去哪了？它丢失了，因为 `postgres` 数据库使用了本地于集群节点的 `db-data` 卷。严格来说，数据并没有丢失。如果
    `postgres` 任务迁移回 `worker2` 节点，它将挂载原始卷并从 3 继续计数。如果你在自己的集群中跟随操作并且没有注意到数据丢失，那可能是因为
    `postgres` 最初恰好部署到了 `worker1`。这种缺乏确定性和可能的数据丢失并不是一个好的情况。我们能做些什么？
- en: The default Docker volume storage driver uses the node’s storage. This storage
    driver does not share or back up data across the Swarm cluster. There are Docker
    storage drivers that add features like this, including Docker CloudStor and Rex-Ray.
    Those drivers will enable you to create and share a volume across the cluster.
    You should investigate and test these drivers carefully before committing important
    data to them.
  id: totrans-746
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认的 Docker 卷存储驱动程序使用节点的存储。这个存储驱动程序不会在 Swarm 集群中共享或备份数据。有一些 Docker 存储驱动程序增加了这样的功能，包括
    Docker CloudStor 和 Rex-Ray。这些驱动程序将使你能够在集群中创建和共享卷。在将重要数据提交给它们之前，你应该仔细调查和测试这些驱动程序。
- en: 'Another approach to ensure that a task runs consistently on a given node is
    to constrain it to a specific node. Relevant constraint options are node hostname,
    node ID, or a user-defined label. For now, let’s constrain the `postgres` task
    to the `worker1` node to ensure that it doesn’t move from that node, even if the
    `private` zone expands:'
  id: totrans-747
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确保任务在给定的节点上持续运行的另一种方法是将其约束到特定的节点。相关的约束选项包括节点主机名、节点ID或用户定义的标签。现在，让我们将`postgres`任务约束到`worker1`节点，以确保它不会从该节点移动，即使`private`区域扩展：
- en: '`docker service update --constraint-add ''node.hostname == worker1'' \    
    multi-tier-app_postgres`'
  id: totrans-748
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service update --constraint-add ''node.hostname == worker1'' \    
    multi-tier-app_postgres`'
- en: 'Now, the `postgres` service will not move from that node. Inspection of the
    service’s placement constraints shows that two are active:'
  id: totrans-749
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，`postgres`服务将不会从该节点移动。检查服务的放置约束显示有两个是活动的：
- en: '`$ docker service inspect \     --format ''{{json .Spec.TaskTemplate.Placement.Constraints
    }}'' \     multi-tier-app_postgres ["node.hostname == worker1","node.labels.zone
    == private"]`'
  id: totrans-750
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$ docker service inspect \     --format ''{{json .Spec.TaskTemplate.Placement.Constraints
    }}'' \     multi-tier-app_postgres ["node.hostname == worker1","node.labels.zone
    == private"]`'
- en: 'If we wanted to continue with these placement constraints, we would specify
    these in the example application’s docker-compose.yml. Here is how to express
    the `postgres` service’s constraints:'
  id: totrans-751
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们想要继续使用这些放置约束，我们将在示例应用程序的docker-compose.yml中指定这些。以下是表达`postgres`服务约束的方法：
- en: '`services:   postgres:     # ... snip ...     deploy:       # ... snip ...
            placement:           constraints:             - node.labels.zone == private
                - node.hostname == worker1`'
  id: totrans-752
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`services:   postgres:     # ... 省略 ...     deploy:       # ... 省略 ...        
    placement:           constraints:             - node.labels.zone == private            
    - node.hostname == worker1`'
- en: Now the placement constraints won’t be lost the next time we deploy the application
    by using `docker stack deploy`.
  id: totrans-753
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在放置约束在下次使用`docker stack deploy`部署应用程序时不会丢失。
- en: '|    |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Note
  id: totrans-755
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to perform a lot of careful engineering in order to safely run
    databases with important data on container clusters such as Swarm. The exact strategy
    will likely be specific to the database implementation so that you can take advantage
    of the database’s specific replication, backup, and data recovery strengths.
  id: totrans-756
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了在像Swarm这样的容器集群上安全地运行包含重要数据的数据库，你需要进行大量的谨慎工程。具体的策略可能因数据库实现而异，以便你可以利用数据库特定的复制、备份和数据恢复优势。
- en: '|    |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '|    |'
- en: Now that we have explored how to constrain services to particular nodes, let’s
    go in the complete opposite direction and deploy a service everywhere with `global`
    services.
  id: totrans-758
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了如何将服务约束到特定的节点，让我们完全相反的方向，使用`global`服务部署服务到每个地方。
- en: 13.4.3\. Using global services for one task per node
  id: totrans-759
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.4.3\. 使用全局服务在每个节点上执行一个任务
- en: You can deploy one service task to each node in the Swarm cluster by declaring
    the service’s mode to be `global`. This is useful when you want to scale a service
    along with the size of the cluster. Common use cases include logging and monitoring
    services.
  id: totrans-760
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过声明服务的模式为`global`来在每个Swarm集群的节点上部署一个服务任务。当你想要随着集群的大小扩展服务时，这很有用。常见的用例包括日志记录和监控服务。
- en: 'Let’s deploy a second instance of our `echo` service as a `global` service:'
  id: totrans-761
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们部署第二个`echo`服务实例作为`global`服务：
- en: '`docker service create --name echo-global \   --mode global \` `1` `--publish
    ''9000:8'' \` `2` `busybox:1.29 nc -v -lk -p 8 -e /bin/cat`'
  id: totrans-762
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service create --name echo-global \   --mode global \` `1` `--publish
    ''9000:8'' \` `2` `busybox:1.29 nc -v -lk -p 8 -e /bin/cat`'
- en: 1 Uses “global” instead of “replicated”
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 使用“global”而不是“replicated”
- en: 2 Publishes port 9000 to avoid collision with echo service
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 发布端口9000以避免与echo服务冲突
- en: If you run `docker service ls`, you will see that the `echo-global` service
    is operating in `global` mode. The mode of the other services we have deployed
    so far will be `replicated`, the default.
  id: totrans-765
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你运行`docker service ls`，你会看到`echo-global`服务正在`global`模式下运行。我们迄今为止部署的其他服务的模式将是`replicated`，默认模式。
- en: 'You can verify that Swarm has deployed one task on each node that is available
    for task scheduling. This example uses the previous section’s Swarm cluster, in
    which only the worker nodes are available for tasks. The `docker service ps` command
    confirms there is one task on each of those nodes:'
  id: totrans-766
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以验证Swarm是否在每个可用于任务调度的节点上部署了一个任务。此示例使用上一节的Swarm集群，其中只有工作节点可用于任务。`docker service
    ps`命令确认每个节点上都有一个任务：
- en: '`docker service ps echo-global \     --filter ''desired-state=running'' \    
    --format ''table {{ .Name }} {{ .Node }}'' NAME                                 
    NODE echo-global.u2382qjg6v9vr8z5lfwqrg5hf worker2 echo-global.rbw0c466qqi0d7k4niw01o3nc
    worker1`'
  id: totrans-767
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker service ps echo-global \     --filter ''desired-state=running'' \    
    --format ''table {{ .Name }} {{ .Node }}'' NAME                                 
    NODE echo-global.u2382qjg6v9vr8z5lfwqrg5hf worker2 echo-global.rbw0c466qqi0d7k4niw01o3nc
    worker1`'
- en: 'You can interact with `echo-global` service just as you did the `echo` service.
    Send a few messages by using the following command:'
  id: totrans-768
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以像使用`echo`服务一样与`echo-global`服务进行交互。使用以下命令发送几条消息：
- en: '`[worker1] $  echo ''hello'' |  nc 127.0.0.1 -w 3 9000` `1`'
  id: totrans-769
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[worker1] $  echo ''hello'' |  nc 127.0.0.1 -w 3 9000` `1`'
- en: 1 Sends to echo-global service using published port
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 通过已发布的端口发送到echo-global服务
- en: Remember that client connections will be routed through the service’s virtual
    IP (see [section 13.3.5](index_split_106.html#filepos1469125)). Because of Docker
    networking’s routing behavior, a client of a global service may connect to a task
    on another node instead of its own. The probability of connecting to a global
    service task on another node goes up with size of the cluster because connections
    are balanced uniformly. You can see the connection-based load balancing happening
    if you inspect the logs with `docker service logs --follow --timestamps echo-global`
    and send messages to the service.
  id: totrans-771
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住客户端连接将通过服务的虚拟IP路由（见[第13.3.5节](index_split_106.html#filepos1469125)）。由于Docker网络的路由行为，全局服务的客户端可能会连接到另一个节点上的任务，而不是它自己的。随着集群规模的增加，连接到另一个节点上的全局服务任务的概率也会增加，因为连接是均匀分配的。如果您通过`docker
    service logs --follow --timestamps echo-global`检查日志并向服务发送消息，您可以看到基于连接的负载均衡正在发生。
- en: 'The following output was produced by connecting to `worker1` and sending messages
    1 second apart:'
  id: totrans-772
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下输出是通过连接到`worker1`并每隔1秒发送消息产生的：
- en: '`2019-02-23T23:51:01.042747381Z echo-global.0.rx3o7rgl6gm9@``worker2``    |
    connect to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40170 ([::ffff:10.255.0.3]:40170)
    2019-02-23T23:51:02.134314055Z echo-global.0.hp01yak2txv2@``worker1``    |  connect
    to [::ffff:10.255.0.``94``]:8 from [::ffff:10.255.0.3]:40172 ([::ffff:10.255.0.3]:40172)
    2019-02-23T23:51:03.264498966Z echo-global.0.rx3o7rgl6gm9@``worker2``    | connect
    to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40174 ([::ffff:10.255.0.3]:40174)
    2019-02-23T23:51:04.398477263Z echo-global.0.hp01yak2txv2@``worker1``    | connect
    to [::ffff:10.255.0.``94``]:8 from [::ffff:10.255.0.3]:40176 ([::ffff:10.255.0.3]:40176)
    2019-02-23T23:51:05.412948512Z echo-global.0.rx3o7rgl6gm9@``worker2``    | connect
    to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40178 ([::ffff:10.255.0.3]:40178)`'
  id: totrans-773
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`2019-02-23T23:51:01.042747381Z echo-global.0.rx3o7rgl6gm9@``worker2``    |
    connect to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40170 ([::ffff:10.255.0.3]:40170)
    2019-02-23T23:51:02.134314055Z echo-global.0.hp01yak2txv2@``worker1``    |  connect
    to [::ffff:10.255.0.``94``]:8 from [::ffff:10.255.0.3]:40172 ([::ffff:10.255.0.3]:40172)
    2019-02-23T23:51:03.264498966Z echo-global.0.rx3o7rgl6gm9@``worker2``    | connect
    to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40174 ([::ffff:10.255.0.3]:40174)
    2019-02-23T23:51:04.398477263Z echo-global.0.hp01yak2txv2@``worker1``    | connect
    to [::ffff:10.255.0.``94``]:8 from [::ffff:10.255.0.3]:40176 ([::ffff:10.255.0.3]:40176)
    2019-02-23T23:51:05.412948512Z echo-global.0.rx3o7rgl6gm9@``worker2``    | connect
    to [::ffff:10.255.0.``95``]:8 from [::ffff:10.255.0.3]:40178 ([::ffff:10.255.0.3]:40178)`'
- en: The `nc` client program sending messages was running on `worker1`. This log
    output shows that the client’s connections that were routed bounce between the
    task on `worker2`, with IP ending in `.95`, and the task on `worker1`, with IP
    ending in `.94`.
  id: totrans-774
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发送消息的`nc`客户端程序在`worker1`上运行。此日志输出显示客户端的连接在`worker2`上的任务（IP以`.95`结尾）和`worker1`上的任务（IP以`.94`结尾）之间弹跳。
- en: 13.4.4\. Deploying real applications onto real clusters
  id: totrans-775
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13.4.4\. 在真实集群上部署实际应用程序
- en: The preceding exercises have shown how Swarm tries to converge an application’s
    actual deployed resources to the desired state indicated in the application’s
    deployment descriptor.
  id: totrans-776
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前面的练习展示了Swarm如何尝试将应用程序的实际已部署资源收敛到应用程序部署描述符中指示的期望状态。
- en: The desired state of the cluster changes as applications are updated, cluster
    resources such as nodes and shared networks are added or removed, or new configurations
    and secrets are provided by operators. Swarm processes these events and updates
    state in the internal log replicated among the cluster’s manager nodes. When Swarm
    sees an event that changes the desired state, the leader of the managers issues
    commands to the rest of the cluster to converge to the desired state. Swarm will
    converge to the desired state by starting or updating service tasks, overlay networks,
    and other resources within the constraints specified by operators.
  id: totrans-777
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着应用程序的更新、集群资源（如节点和共享网络）的添加或删除，或操作员提供新的配置和机密信息，集群的所需状态会发生变化。Swarm处理这些事件，并在集群管理节点之间复制的内部日志中更新状态。当Swarm看到改变所需状态的事件时，管理器的领导者向集群的其他部分发出命令，以收敛到所需状态。Swarm将通过启动或更新服务任务、覆盖网络和其他资源来收敛到所需状态，这些资源由操作员指定的约束条件所限制。
- en: You may be wondering which features of Swarm to start with. First, make an inventory
    of the kinds of applications you want to run and the types of Swarm resources
    they will need. Second, think about how you want to organize those applications
    running on the cluster. Third, decide whether the cluster will support stateful
    services such as databases and determine a strategy for managing data safely.
  id: totrans-778
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能想知道从Swarm的哪些功能开始。首先，列出你想要运行的应用程序类型以及它们将需要的Swarm资源类型。其次，考虑你想要如何组织在集群上运行的应用程序。第三，决定集群是否将支持有状态服务（如数据库），并确定一个安全地管理数据的方法。
- en: Remember, you can start by deploying stateless services that use only the networking,
    configuration, and secret management features of Swarm. This approach provides
    the opportunity to learn more about Swarm and the way services operate in a multihost
    environment without putting data at risk.
  id: totrans-779
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，你可以从部署仅使用Swarm的网络、配置和机密管理功能的无状态服务开始。这种方法提供了深入了解Swarm和服务的多宿主环境操作方式的机会，而不会将数据置于风险之中。
- en: With thoughtful design and proactive monitoring of the cluster’s resources,
    you can ensure that applications have the resources they need, when they need
    it. You can also set expectations for which activities and data the cluster should
    host.
  id: totrans-780
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过对集群资源进行深思熟虑的设计和主动监控，你可以确保应用程序在需要时获得所需的资源。你还可以设定集群应托管哪些活动和数据的预期。
- en: SUMMARY
  id: totrans-781
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter explored many aspects of running Docker services on a cluster
    of hosts managed by Swarm. The exercises demonstrated how to deploy several kinds
    of applications using Swarm’s most important and commonly used features. We saw
    what happens when a cluster runs out of resources and showed how to recover from
    that condition. We also reconfigured a cluster and service deployment to implement
    user-defined architectural goals. The key points to understand from this chapter
    are:'
  id: totrans-782
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章探讨了在Swarm管理的宿主集群上运行Docker服务的一些方面。练习展示了如何使用Swarm最重要的和最常用的功能来部署多种类型的应用程序。我们看到了当集群资源耗尽时会发生什么，并展示了如何从该状态中恢复。我们还重新配置了集群和服务部署，以实现用户定义的架构目标。本章需要理解的关键点包括：
- en: Administrators define cluster-scoped resources such as networks, configurations,
    and secrets that are shared by services.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员定义了由服务共享的集群范围内的资源，如网络、配置和机密信息。
- en: Services define their own service-scoped resources in addition to using cluster-scoped
    resources.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务除了使用集群范围内的资源外，还定义了它们自己的服务范围资源。
- en: Swarm managers store and manage updates to the desired state of the cluster.
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm管理器存储和管理集群所需状态的更新。
- en: Swarm managers converge actual application and resource deployments to the desired
    state when sufficient resources are available.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有足够的资源可用时，Swarm管理器将实际的应用程序和资源部署收敛到所需状态。
- en: Service tasks are ephemeral, and service updates cause tasks to be replaced
    with new containers.
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务任务是非持久的，服务更新会导致任务被新的容器替换。
- en: Service tasks can be scaled to the desired state as long as the cluster has
    enough resources available on nodes that fulfill the service’s placement constraints.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要集群在满足服务放置约束条件的节点上有足够的可用资源，服务任务就可以扩展到所需状态。
