- en: Chapter 8\. Train and Optimize Models at Scale
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 大规模训练和优化模型
- en: Peter Drucker, one of Jeff Bezos’s favorite business strategists, once said,
    “If you can’t measure it, you can’t improve it.” This quote captures the essence
    of this chapter, which focuses on measuring, optimizing, and improving our predictive
    models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 彼得·德鲁克（Peter Drucker）是杰夫·贝佐斯最喜欢的商业战略家之一，他曾说过：“如果你不能衡量它，你就不能改进它。”这句话捕捉到了本章的核心，重点在于衡量、优化和改进我们的预测模型。
- en: In the previous chapter, we trained a single model with a single set of hyper-parameters
    using Amazon SageMaker. We also demonstrated how to fine-tune a pre-trained BERT
    model to build a review-text classifier model to predict the sentiment of product
    reviews in the wild from social channels, partner websites, etc.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用Amazon SageMaker训练了一个具有单一超参数集的单一模型。我们还展示了如何微调预训练的BERT模型，构建一个评估文本情感的分类器模型，以预测来自社交渠道、合作伙伴网站等的产品评论的情感。
- en: In this chapter, we will use SageMaker Experiments to measure, track, compare,
    and improve our models at scale. We also use SageMaker Hyper-Parameter Tuning
    to choose the best hyper-parameters for our specific algorithm and dataset. We
    also show how to perform distributed training using various communication strategies
    and distributed file systems. We finish with tips on how to reduce cost and increase
    performance using SageMaker Autopilot’s hyper-parameter-selection algorithm, SageMaker’s
    optimized pipe to S3, and AWS’s enhanced-networking hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用SageMaker实验来量化、跟踪、比较和改进我们的模型。我们还将使用SageMaker Hyper-Parameter Tuning选择特定算法和数据集的最佳超参数。我们还展示了如何使用各种通信策略和分布式文件系统进行分布式训练。最后，我们提供了如何通过SageMaker
    Autopilot的超参数选择算法、SageMaker优化的管道到S3和AWS增强网络硬件来降低成本和提高性能的建议。
- en: Automatically Find the Best Model Hyper-Parameters
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动找到最佳模型超参数
- en: Now that we understand how to track and compare model-training runs, we can
    automatically find the best hyper-parameters for our dataset and algorithm using
    a scalable process called hyper-parameter tuning (HPT) or hyper-parameter optimization
    (HPO). SageMaker natively supports HPT jobs. These tuning jobs are the building
    blocks for SageMaker Autopilot, discussed in [Chapter 3](ch03.html#automated_machine_learnin).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何跟踪和比较模型训练运行，我们可以使用名为超参数调优（HPT）或超参数优化（HPO）的可扩展过程自动找到适合我们数据集和算法的最佳超参数。SageMaker原生支持HPT作业。这些调优作业是SageMaker
    Autopilot的构建模块，详细讨论请见[第三章](ch03.html#automated_machine_learnin)。
- en: We have already learned that hyper-parameters control *how* our machine learning
    algorithm learns the model parameters during model training. When tuning our hyper-parameters,
    we need to define an objective to optimize, such as model accuracy. In other words,
    we need to find a set of hyper-parameters that meets or exceeds our given objective.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到超参数在模型训练期间控制我们的机器学习算法如何学习模型参数。在调整超参数时，我们需要定义一个优化的目标，例如模型准确度。换句话说，我们需要找到一组超参数，使其满足或超过我们给定的目标。
- en: After each HPT run, we evaluate the model performance and adjust the hyper-parameters
    until the objective is reached. Doing this manually is very time-consuming as
    model tuning often requires tens or hundreds of training jobs to converge on the
    best combination of hyper-parameters for our objective. SageMaker’s HPT jobs speed
    up and scale out the optimization process by running multiple training jobs in
    parallel using a given tuning strategy, as shown in [Figure 8-1](#sagemaker_hyper_parameter_optimization).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次HPT运行之后，我们评估模型性能并调整超参数，直到达到目标。手动执行此操作非常耗时，因为模型调优通常需要数十甚至数百个训练作业才能收敛于最佳超参数组合以实现我们的目标。SageMaker的HPT作业通过使用给定的调优策略并行运行多个训练作业，加速和扩展了优化过程，如[图8-1](#sagemaker_hyper_parameter_optimization)所示。
- en: '![](assets/dsaw_0801.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0801.png)'
- en: Figure 8-1\. SageMaker HPT supports common tuning strategies.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. SageMaker HPT支持常见的调优策略。
- en: SageMaker supports the random-search and Bayesian hyper-parameter optimization
    strategies. With random search, we randomly keep picking combinations of hyper-parameters
    until we find a well-performing combination. This approach is very fast and very
    easy to parallelize, but we might miss the best set of hyper-parameters as we
    are picking randomly from the hyper-parameter space. With Bayesian optimization,
    we treat the task as a regression problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持随机搜索和贝叶斯超参数优化策略。使用随机搜索时，我们会随机挑选超参数组合，直到找到表现良好的组合。这种方法非常快速且易于并行化，但可能会错过最佳超参数集，因为我们是从超参数空间随机选择的。使用贝叶斯优化时，我们将任务视为回归问题。
- en: Similar to how our actual model learns the model weights that minimize a loss
    function, Bayesian optimization iterates to find the best hyper-parameters using
    a surrogate model and acquisition function that performs an informed search over
    the hyper-parameter space using prior knowledge learned during previous optimization
    runs. Bayesian optimization is usually more efficient than manual, random, or
    grid search but requires that we perform some optimizations sequentially (versus
    in parallel) to build up the prior knowledge needed to perform the informed search
    across the hyper-parameter space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们实际模型如何学习最小化损失函数的模型权重，贝叶斯优化通过使用替代模型和获取函数来迭代，以使用在先前优化运行期间学习到的先验知识在超参数空间上进行信息搜索，以找到最佳超参数。贝叶斯优化通常比手动、随机或网格搜索更高效，但需要我们顺序执行一些优化（而不是并行执行），以建立所需的先验知识，以在超参数空间上进行信息搜索。
- en: What about grid search? With grid search, we would evaluate a grid of every
    possible hyper-parameter combination in our hyper-parameter space. This approach
    is often inefficient and takes orders of magnitude longer to complete relative
    to the random search and Bayesian optimization strategies. At the time of this
    writing, SageMaker HPT does not support the inefficient grid search optimization
    strategy. Instead, we recommend using the random-search and Bayesian optimization
    strategies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，网格搜索呢？使用网格搜索，我们将评估超参数空间中每一种可能的超参数组合的网格。这种方法通常效率低下，并且完成时间相对于随机搜索和贝叶斯优化策略需要花费数量级更长的时间。在撰写本文时，SageMaker
    HPT 不支持低效的网格搜索优化策略。相反，我们建议使用随机搜索和贝叶斯优化策略。
- en: Set Up the Hyper-Parameter Ranges
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置超参数范围
- en: 'Let’s use SageMaker HPT to find the best hyper-parameters for our BERT-based
    review classifier from the previous chapter. First, let’s create an `optimize`
    experiment tracker and associate it with our experiment:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 SageMaker HPT 来为前一章节中基于 BERT 的评论分类器找到最佳超参数。首先，让我们创建一个`optimize`实验追踪器，并将其与我们的实验关联起来。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To keep this example simple and avoid a combinatorial explosion of trial runs,
    we will freeze most hyper-parameters and explore only a limited set for this particular
    optimization run. In a perfect world with unlimited resources and budget, we would
    explore every combination of hyper-parameters. For now, we will manually choose
    some of the following hyper-parameters and explore the rest in our HPO run:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个例子保持简单，并避免试验运行的组合爆炸，我们将冻结大部分超参数，并仅在此次优化运行中探索有限的集合。在资源和预算无限的理想情况下，我们将探索每一种超参数的组合。但现在，我们将手动选择以下一些超参数，并在我们的HPO运行中探索其余的。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s set up the hyper-parameter ranges that we wish to explore. We are
    choosing these hyper-parameters based on intuition, domain knowledge, and algorithm
    documentation. We may also find research papers useful—or other prior work from
    the community. At this point in the life cycle of machine learning and predictive
    analytics, we can almost always find relevant information on the problem we are
    trying to solve.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们设置我们希望探索的超参数范围。我们根据直觉、领域知识和算法文档选择这些超参数。我们还可能发现研究论文或社区中的其他先前工作有用。在机器学习和预测分析的生命周期的这一阶段，我们几乎总能找到解决我们试图解决的问题的相关信息。
- en: If we still can’t find a suitable starting point, we should explore ranges logarithmically
    (versus linearly) to help gain a sense of the scale of the hyper-parameter. There
    is no point in exploring the set [1, 2, 3, 4] if our best hyper-parameter is orders
    of magnitude away in the 1,000s, for example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仍然找不到合适的起始点，我们应该对数范围进行探索（而不是线性范围），以帮助获得超参数规模的感觉。例如，如果我们最佳的超参数与数千的量级差异很大，那么探索集合[1,
    2, 3, 4]就没有意义。
- en: SageMaker Autopilot is another way to determine a baseline set of hyper-parameters
    for our problem and dataset. SageMaker Autopilot’s hyper-parameter selection process
    has been refined on many thousands of hours of training jobs across a wide range
    of datasets, algorithms, and use cases within Amazon.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Autopilot 是确定我们问题和数据集的基准超参数集的另一种方法。SageMaker Autopilot 的超参数选择过程已在亚马逊广泛的数据集、算法和用例中经过数千小时的训练作业的精炼。
- en: 'SageMaker HPT supports three types of parameter ranges: categorical, continuous,
    and integer. `Categorical` is used for discrete sets of values (e.g., `product_category`).
    `Continuous` is used for floats, and `Integer` is used for integers. We can also
    specify the scaling type for each type of hyper-parameter ranges. The scaling
    type can be set to `Linear`, `Logarithmic`, `ReverseLogarithmic`, or `Auto`, which
    allows SageMaker to decide. Certain hyper-parameters are better suited to certain
    scaling types. Here, we are specifying that the SageMaker Tuning Job should explore
    the continuous hyper-parameter, `learning_rate`, between the given range using
    a linear scale:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker HPT 支持三种参数范围类型：分类、连续和整数。`分类` 用于离散值集合（例如 `产品类别`）。`连续` 用于浮点数，`整数` 用于整数。我们还可以为每种超参数范围类型指定缩放类型。缩放类型可以设置为
    `线性`、`对数`、`反对数` 或 `自动`，这样 SageMaker 就可以决定。某些超参数更适合某些缩放类型。在这里，我们指定 SageMaker 调优作业应使用线性尺度在给定范围内探索连续超参数
    `learning_rate`：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If we do not have a suitable range for exploring a particular hyper-parameter—even
    after researching other algorithms that address the problem similar to ours—we
    can start with the `Logarithmic` scaling type for that hyper-parameter and narrow
    in on a range to subsequently explore linearly with the `Linear` scaling type.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有适合探索特定超参数的范围——即使在研究其他解决类似问题的算法之后——我们可以从 `对数` 缩放类型开始处理该超参数，并逐渐使用 `线性` 缩放类型探索范围。
- en: 'Finally, we need to define the objective metric that the HPT job is trying
    to optimize—in our case, validation accuracy. Remember that we need to provide
    the regular expression (regex) to extract the metric from the SageMaker container
    logs. We chose to also collect the training loss, training accuracy, and validation
    loss for informational purposes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义 HPT 作业尝试优化的客观指标——在我们的例子中是验证准确率。请记住，我们需要提供正则表达式（regex）来从 SageMaker
    容器日志中提取指标。我们选择还收集训练损失、训练准确率和验证损失以供信息目的使用：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Run the Hyper-Parameter Tuning Job
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行超参数调优作业
- en: 'We start by creating our TensorFlow estimator as in the previous chapter. Note
    that we are not specifying the `learning_rate` hyper-parameter in this case. We
    will pass this as a hyper-parameter range to the `HyperparameterTuner` in a moment:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先像前一章一样创建我们的 TensorFlow 评估器。请注意，在这种情况下，我们没有指定 `learning_rate` 超参数。我们将在稍后将其作为超参数范围传递给
    `HyperparameterTuner`：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we can create our HPT job by passing the TensorFlow estimator, hyper-parameter
    range, objective metric, tuning strategy, number of jobs to run in parallel/total,
    and an early stopping strategy. SageMaker will use the given optimization strategy
    (i.e., “Bayesian” or “Random”) to explore the values within the given ranges:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过传递 TensorFlow 评估器、超参数范围、客观指标、调优策略、并行/总作业数以及早停策略来创建我们的 HPT 作业。SageMaker
    将使用给定的优化策略（即“贝叶斯”或“随机”）来探索给定范围内的值：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we are using the `Bayesian` optimization strategy with 10 jobs
    in parallel and 100 total. By only doing 10 at a time, we give the `Bayesian`
    strategy a chance to learn from previous runs. In other words, if we did all 100
    in parallel, the `Bayesian` strategy could not use prior information to choose
    better values within the ranges provided.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了 `贝叶斯` 优化策略，并行进行了 10 个作业，总共有 100 个作业。每次只做 10 个作业，我们给 `贝叶斯` 策略一个机会从之前的运行中学习。换句话说，如果我们同时进行了所有
    100 个作业，`贝叶斯` 策略将无法利用先前的信息来选择范围内更好的值。
- en: By setting `early_stopping_type` to `Auto`, SageMaker will stop the tuning job
    if the tuning job is not going to improve upon the objective metric. This helps
    save time, reduces the potential for overfitting to our training dataset, and
    reduces the overall cost of the tuning job.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `early_stopping_type` 设置为 `Auto`，如果调优作业不能在客观指标上取得改进，SageMaker 将停止调优作业。这有助于节省时间，减少对训练数据集的过拟合可能性，并降低调优作业的总体成本。
- en: 'Let’s start the tuning job by calling `tuner.fit()` using the train, validation,
    and test dataset splits:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `tuner.fit()` 来启动调整作业，使用训练、验证和测试数据集拆分：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Analyze the Best Hyper-Parameters from the Tuning Job
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析调整作业中的最佳超参数
- en: 'Following are results of the tuning job to determine the best hyper-parameters.
    This tuning job resulted in a final training accuracy of 0.9416 for the best candidate,
    which is higher than 0.9394, the accuracy from [Chapter 7](ch07.html#train_your_first_model)
    using a set of manually chosen hyper-parameter values:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是确定最佳超参数的调整作业的结果。这次调整作业为最佳候选者实现了 0.9416 的最终训练精度，高于手动选择的超参数值集合 [第7章](ch07.html#train_your_first_model)
    中的 0.9394 精度：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '| freeze_bert_​layer | learning_​rate | train_batch_​size | TrainingJob​Name
    | TrainingJob​Status | FinalObjective​Value | TrainingElapsed​Time​Seconds |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| freeze_bert_​layer | learning_​rate | train_batch_​size | TrainingJob​Name
    | TrainingJob​Status | FinalObjective​Value | TrainingElapsed​Time​Seconds |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| “False” | 0.000017 | “128” | tensorflow-training-​210109-0222-​003-​cf95cdaa
    | Completed | 0.9416 | 11245.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| “False” | 0.000017 | “128” | tensorflow-training-​210109-0222-​003-​cf95cdaa
    | Completed | 0.9416 | 11245.0 |'
- en: '| … |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| “False” | 0.000042 | “128” | tensorflow-training-​210109-0222-​004-​48da4bab
    | Stopped | 0.8056 | 693.0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| “False” | 0.000042 | “128” | tensorflow-training-​210109-0222-​004-​48da4bab
    | Stopped | 0.8056 | 693.0 |'
- en: Given the results of this tuning job, the best combination of hyper-parameters
    is `learning_rate` 0.000017, `train_batch_size` 128, and `freeze_bert_layer` False.
    SageMaker stopped a job early because its combination of hyper-parameters was
    not improving the training-accuracy objective metric. This is an example of SageMaker
    saving us money by intelligently stopping jobs early when they are not adding
    value to our business objective.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这次调整作业的结果，最佳超参数组合是 `learning_rate` 为 0.000017，`train_batch_size` 为 128，`freeze_bert_layer`
    为 False。当超参数组合未能改善训练准确度目标度量时，SageMaker 提前停止了作业。这是 SageMaker 智能停止未能为业务目标增加价值的作业，从而节省了我们的成本。
- en: Show Experiment Lineage for Our SageMaker Tuning Job
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示我们 SageMaker 调整作业的实验谱系
- en: Once the HPT job has finished, we can analyze the results directly in our notebook
    or through SageMaker Studio.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: HPT 作业完成后，我们可以直接在笔记本或 SageMaker Studio 中分析结果。
- en: 'First, let’s update the experiment lineage to include the best hyper-parameters
    and objective metrics found by our HPT job:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们更新实验谱系，包括我们的 HPT 作业找到的最佳超参数和目标度量：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s summarize the experiment lineage up to this point. In [Chapter 9](ch09.html#deploy_models_to_production),
    we will deploy the model and further extend our experiment lineage to include
    model deployment. We will then tie everything together in an end-to-end pipeline
    with full lineage tracking in [Chapter 10](ch10.html#pipelines_and_mlops):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结到目前为止的实验谱系。在 [第9章](ch09.html#deploy_models_to_production) 中，我们将部署模型，并进一步扩展我们的实验谱系以包括模型部署。然后，我们将在
    [第10章](ch10.html#pipelines_and_mlops) 中将所有内容整合到一个端到端管道中，实现全面的谱系跟踪。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | … |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | … |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TrialComponent-​2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN | … |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN | … |'
- en: '| tensorflow-training-​2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    | … |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| tensorflow-training-​2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    | … |'
- en: '| TrialComponent-​2020-06-12-193933-bowu | optimize-1 | 64.0 | 0.000017 | 0.9416
    | … |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2020-06-12-193933-bowu | optimize-1 | 64.0 | 0.000017 | 0.9416
    | … |'
- en: In this example, we have optimized the hyper-parameters of our TensorFlow BERT
    classifier layer. SageMaker HPT also supports automatic HPT across multiple algorithms
    by adding a list of algorithms to the tuning job definition. We can specify different
    hyper-parameters and ranges for each algorithm. Similarly, SageMaker Autopilot
    uses multialgorithm tuning to find the best model across different algorithms
    based on our problem type, dataset, and objective function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们优化了 TensorFlow BERT 分类器层的超参数。SageMaker HPT 还支持通过将算法列表添加到调整作业定义来跨多个算法进行自动超参数调整。我们可以为每个算法指定不同的超参数和范围。类似地，SageMaker
    Autopilot 使用多算法调整来根据我们的问题类型、数据集和目标函数找到最佳模型。
- en: Use Warm Start for Additional SageMaker Hyper-Parameter Tuning Jobs
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Warm Start 进行额外的 SageMaker 超参数调整作业
- en: Once we have our best candidate, we can choose to perform yet another round
    of hyper-parameter optimization using a technique called “warm start.” Warm starting
    reuses the prior results from a previous HPT job—or set of jobs—to speed up the
    optimization process and reduce overall cost. Warm start creates a many-to-many
    parent–child relationship. In our example, we perform a warm start with a single
    parent, the previous tuning job, as shown in [Figure 8-2](#use_warm_start_to_start_an_additional_h).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳候选者，我们可以选择使用称为“热启动”的技术执行另一轮超参数优化。热启动重复使用先前 HPT 作业或一组作业的结果，以加快优化过程并降低总成本。热启动创建了多对多的父-子关系。在我们的示例中，我们与单个父作业——前一个调整作业——执行热启动，如图
    8-2 所示。
- en: '![](assets/dsaw_0802.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0802.png)'
- en: Figure 8-2\. Use warm start to start an additional HPT job from a previous tuning
    job.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 使用热启动从先前的调整作业开始一个额外的 HPT 作业。
- en: 'Warm start is particularly useful when we want to change the tunable hyper-parameter
    ranges from the previous job or add new hyper-parameters. Both scenarios use the
    previous tuning job to find the best model faster. The two scenarios are implemented
    with two warm start types: `IDENTICAL_DATA_AND_ALGORITHM` and `TRANSFER_LEARNING`.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要从上一个作业更改可调参数范围或添加新的超参数时，热启动尤其有用。两种情况都使用前一个调整作业来更快地找到最佳模型。这两种情况分别用两种热启动类型实现：`IDENTICAL_DATA_AND_ALGORITHM`和`TRANSFER_LEARNING`。
- en: If we choose `IDENTICAL_DATA_AND_ALGORITHM`, the new tuning job uses the same
    input data and training image as the parent job. We are allowed to update the
    tunable hyper-parameter ranges and the maximum number of training jobs. We can
    also add previously fixed hyper-parameters to the list of tunable hyper-parameters
    and vice versa—as long as the overall number of fixed plus tunable hyper-parameters
    remains the same. Upon completion, a tuning job with this strategy will return
    an additional field, `OverallBestTrainingJob`, containing the best model candidate,
    including this tuning job as well as the completed parent tuning jobs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择`IDENTICAL_DATA_AND_ALGORITHM`，新的调整作业将使用与父作业相同的输入数据和训练图像。我们可以更新可调参数范围和最大训练作业数。我们还可以将之前的固定超参数添加到可调超参数列表中，反之亦然——只要固定加可调超参数总数保持不变即可。完成后，采用这种策略的调整作业将返回一个额外字段，`OverallBestTrainingJob`，其中包含最佳模型候选者，包括此调整作业以及已完成的父调整作业。
- en: If we choose `TRANSFER_LEARNING`, we can use updated training data and a different
    version of the training algorithm. Perhaps we collected more training data since
    the last optimization run—and now we want to rerun the tuning job with the updated
    dataset. Or perhaps a newer version of the algorithm has been released and we
    want to rerun the optimization process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择`TRANSFER_LEARNING`，我们可以使用更新后的训练数据和不同版本的训练算法。也许自上次优化运行以来我们收集了更多的训练数据，现在我们想要使用更新后的数据集重新运行调整作业。或者可能发布了更新的算法版本，我们想要重新运行优化过程。
- en: Run HPT Job Using Warm Start
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行使用热启动的 HPT 作业
- en: 'We need to configure the tuning job with `WarmStartConfig` using one or more
    of the previous HPT jobs as parents. The parent HPT jobs must have finished with
    one of the following success or failure states: `Completed`, `Stopped`, or `Failed`.
    Recursive parent–child relationships are not supported. We also need to specify
    the `WarmStartType`. In our example, we will use `IDENTICAL_DATA_AND_ALGORITHM`
    as we plan to only modify the hyper-parameter ranges and not use an updated dataset
    or algorithm version.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用一个或多个先前的 HPT 作业作为父作业来配置调整作业的`WarmStartConfig`。父 HPT 作业必须以`Completed`、`Stopped`或`Failed`状态完成。不支持递归的父-子关系。我们还需要指定`WarmStartType`。在我们的示例中，我们将使用`IDENTICAL_DATA_AND_ALGORITHM`，因为我们计划仅修改超参数范围，而不使用更新后的数据集或算法版本。
- en: 'Let’s start with the setup of `WarmStartConfig`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设置`WarmStartConfig`开始：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s define the fixed hyper-parameters that we are not planning to tune:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义那些不打算调整的固定超参数：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'While we can choose to tune more hyper-parameters in this warm-start tuning
    job, we will simply modify the range of our `learning_rate` to narrow in on the
    best value found in the parent tuning job:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以选择在这个热启动调整作业中调整更多超参数，但我们将简单地修改我们的`learning_rate`范围，以便缩小到在父调整作业中找到的最佳值：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let’s define the objective metric, create the `HyperparameterTuner` with
    the preceding `warm_start_config`, and start the tuning job:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义目标度量，使用前述`warm_start_config`创建`HyperparameterTuner`，并启动调整作业：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, let’s configure the dataset splits and start our tuning job:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们配置数据集拆分并启动我们的调优作业：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Analyze the Best Hyper-Parameters from the Warm-Start Tuning Job
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析温启动调优作业的最佳超参数
- en: Following are results of the tuning job to determine the best hyper-parameters.
    The tuning job resulted in a best-candidate training accuracy of 0.9216, which
    is lower than 0.9416, the best-candidate training accuracy of the parent HPT job.
    After updating our experiment lineage with the warm-start HPT results, we will
    move forward with the hyper-parameters of the candidate that generated the highest
    training accuracy of 0.9416 from the parent tuning job.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于确定最佳超参数的调优作业结果。调优作业导致了0.9216的最佳候选训练精度，低于父HPT作业的最佳候选训练精度0.9416。在更新我们的实验谱系以包含温启动HPT结果后，我们将采用产生父调优作业中最高训练精度0.9416的候选超参数。
- en: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | … |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | … |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TrialComponent-​2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN | … |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-062410-pxuy | 准备 | 64.0 | NaN | NaN | … |'
- en: '| tensorflow-training-​2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    | … |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| tensorflow-training-​2021-01-09-06-24-12-989 | 训练 | 64.0 | 0.00001 | 0.9394
    | … |'
- en: '| TrialComponent-​2021-01-09-193933-bowu | optimize-1 | 64.0 | 0.000017 | 0.9416
    | … |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-193933-bowu | 优化-1 | 64.0 | 0.000017 | 0.9416
    | … |'
- en: '| TrialComponent-​2021-01-09-234445-dep | optimize-2 | 64.0 | 0.000013 | 0.9216
    | … |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-234445-dep | 优化-2 | 64.0 | 0.000013 | 0.9216 |
    … |'
- en: In this example, we have optimized the hyper-parameters of our TensorFlow BERT
    classifier layer. SageMaker HPT also supports automatic HPT across multiple algorithms
    by adding a list of algorithms to the tuning job definition. We can specify different
    hyper-parameters and ranges for each algorithm. Similarly, SageMaker Autopilot
    uses multialgorithm tuning to find the best model across different algorithms
    based on our problem type, dataset, and objective function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经优化了我们的TensorFlow BERT分类器层的超参数。SageMaker HPT还通过在调优作业定义中添加算法列表来支持多算法的自动HPT。我们可以为每个算法指定不同的超参数和范围。类似地，SageMaker
    Autopilot使用多算法调优来基于问题类型、数据集和目标函数找到最佳模型。
- en: The warm-start tuning job did not beat the accuracy of the parent tuning job’s
    best candidate. Therefore, the hyper-parameter found from the parent tuning is
    still the best candidate in this example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 温启动调优作业未能超越父调优作业的最佳候选精度。因此，在这个例子中，仍然选择父调优作业中找到的超参数作为最佳候选。
- en: Scale Out with SageMaker Distributed Training
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker分布式训练扩展
- en: Most modern AI and machine learning frameworks support some form of distributed
    processing to scale out the computation. Without distributed processing, the training
    job is limited to the resources of a single instance. While individual instance
    types are constantly growing in capabilities (RAM, CPU, and GPU), our modern world
    of big data requires a cluster to power continuous data ingestion, real-time analytics,
    and data-hungry machine learning models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代AI和机器学习框架都支持某种形式的分布式处理，以扩展计算能力。如果没有分布式处理，训练作业将受限于单个实例的资源。虽然单个实例类型在能力方面（RAM、CPU和GPU）不断增强，但我们现代的大数据世界需要一个集群来支持持续数据摄取、实时分析和数据密集型机器学习模型。
- en: Let’s run a distributed training job to build our reviews classifier model using
    the TensorFlow 2.x Keras API, BERT, and SageMaker’s native distributed-training
    support for TensorFlow.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个分布式训练作业，使用TensorFlow 2.x Keras API、BERT和SageMaker的本地分布式训练支持来构建我们的评论分类器模型。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While we do not include a PyTorch example in this chapter, SageMaker absolutely
    supports distributed PyTorch. Review our GitHub repository for the PyTorch and
    BERT examples. Additionally, the Hugging Face Transformers library natively supports
    SageMaker’s distributed training infrastructure for both TensorFlow and PyTorch.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章没有包含PyTorch的示例，但SageMaker绝对支持分布式PyTorch。请查看我们的GitHub存储库，了解PyTorch和BERT示例。此外，Hugging
    Face Transformers库原生支持SageMaker的分布式训练基础设施，适用于TensorFlow和PyTorch。
- en: Choose a Distributed-Communication Strategy
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择分布式通信策略
- en: Any distributed computation requires that the cluster instances communicate
    and share information with each other. This cluster communication benefits from
    higher-bandwidth connections between the instances. Therefore, the instances should
    be physically close to each other in the cloud data center, if possible. Fortunately,
    SageMaker handles all of this heavy lifting for us so we can focus on creating
    our review classifier and address our business problem of classifying product
    reviews in the wild. SageMaker supports distributed computations with many distributed-native
    frameworks, including Apache Spark, TensorFlow, PyTorch, and APache MXNet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分布式计算都要求集群实例之间进行通信并共享信息。这种集群通信受益于实例之间更高带宽的连接。因此，如果可能的话，实例应该在云数据中心内彼此物理靠近。幸运的是，SageMaker
    为我们处理了所有这些繁重的工作，因此我们可以专注于创建我们的评论分类器并解决我们在野外分类产品评论的业务问题。SageMaker 支持使用许多分布式原生框架进行分布式计算，包括
    Apache Spark、TensorFlow、PyTorch 和 Apache MXNet。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While most modern AI and machine learning frameworks like TensorFlow, PyTorch,
    and Apache MXNet are designed for distributed computations, many classic data
    science libraries such as scikit-learn and pandas do not natively support distributed
    communication protocols or distributed datasets. Dask is a popular runtime to
    help scale certain scikit-learn models to multiple nodes in a cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数现代 AI 和机器学习框架如 TensorFlow、PyTorch 和 Apache MXNet 都设计用于分布式计算，但许多经典的数据科学库如
    scikit-learn 和 pandas 并不原生支持分布式通信协议或分布式数据集。Dask 是一个流行的运行时，可帮助将某些 scikit-learn
    模型扩展到集群中的多个节点。
- en: “Parameter server” is a primitive distributed training strategy supported by
    most distributed machine learning frameworks. Remember that parameters are what
    the algorithm is learning. Parameter servers store the learned parameters and
    share them with every instance during the training process. Since parameter servers
    store the state of the parameters, SageMaker runs a parameter server on every
    instance for higher availability, as shown in [Figure 8-3](#parameter_server_cluster_communication).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “参数服务器”是大多数分布式机器学习框架支持的一种原始分布式训练策略。请记住，参数是算法学习的内容。参数服务器存储学习到的参数，并在训练过程中与每个实例共享。由于参数服务器存储参数的状态，SageMaker
    在每个实例上运行参数服务器以提高可用性，如 [图 8-3](#parameter_server_cluster_communication) 所示。
- en: '![](assets/dsaw_0803.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0803.png)'
- en: Figure 8-3\. Distributed communication with parameter servers.
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 参数服务器的分布式通信。
- en: Running stateful parameter servers on every instance helps SageMaker recover
    from failure situations or when Spot Instances are terminated and replaced during
    the training process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个实例上运行有状态的参数服务器有助于 SageMaker 在训练过程中从故障情况或当 Spot 实例被终止并替换时进行恢复。
- en: Another common distributed communication strategy rooted in parallel computing
    and message-passing interfaces (MPI) is “all-reduce.” All-reduce uses a ring-like
    communication pattern, as shown in [Figure 8-4](#all_reduce_distributed_communication_st),
    and increases overall training efficiency for very large clusters where the communication
    overhead between parameter servers becomes overwhelming.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种根植于并行计算和消息传递接口（MPI）的常见分布式通信策略是“all-reduce”。All-reduce 使用类似环形的通信模式，如 [图 8-4](#all_reduce_distributed_communication_st)
    所示，并提高了在通信开销对参数服务器之间的影响较大的非常大集群中的整体训练效率。
- en: '![](assets/dsaw_0804.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0804.png)'
- en: Figure 8-4\. All-reduce distributed communication strategy.
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. All-reduce 分布式通信策略。
- en: SageMaker’s all-reduce distributed training strategy is compatible with Horovod,
    a popular all-reduce and MPI implementation commonly used to scale TensorFlow
    and PyTorch training jobs to multiple instances in a cluster. If we are currently
    using Horovod for distributed training, we can easily transition to SageMaker’s
    all-reduce strategy. For our example, we will use SageMaker’s built-in distributed
    all-reduce communication strategy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 的 all-reduce 分布式训练策略与 Horovod 兼容，后者是一种流行的 all-reduce 和 MPI 实现，常用于将
    TensorFlow 和 PyTorch 训练作业扩展到集群中的多个实例。如果我们当前正在使用 Horovod 进行分布式训练，我们可以轻松过渡到 SageMaker
    的 all-reduce 策略。在我们的示例中，我们将使用 SageMaker 内置的分布式 all-reduce 通信策略。
- en: Choose a Parallelism Strategy
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择并行策略
- en: 'There are two main types of parallelism when performing distributed computations:
    data parallelism and model parallelism. Most of us are already familiar with data
    parallelism from classical map-reduce data-processing tools like Apache Spark
    that split up the dataset into “shards” and place them on separate instances.
    Each instance processes its split separately in the “map” phase, then combines
    the results in the “reduce” phase. Data parallelism is required when our dataset
    cannot fit on a single instance, as in the case with most modern big data processing
    and distributed machine learning. [Figure 8-5](#sharding_a_dataset_across_multiple_inst)
    shows how data parallelism splits up the data onto different instances for the
    model to process separately.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行分布式计算时，有两种主要的并行方式：数据并行和模型并行。我们大多数人已经熟悉数据并行，它来自于类似Apache Spark这样的经典Map-Reduce数据处理工具，将数据集分割成“分片”并将它们放置在单独的实例上。每个实例在“映射”阶段分别处理其分片，然后在“归约”阶段合并结果。当我们的数据集无法适应单个实例时，如现代大数据处理和分布式机器学习的情况下，就需要数据并行。[图 8-5](#sharding_a_dataset_across_multiple_inst)显示数据并行如何将数据分割到不同实例上，以便模型分别处理。
- en: '![](assets/dsaw_0805.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0805.png)'
- en: Figure 8-5\. Sharding a dataset across multiple instances for distributed training
    with data parallelism.
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。将数据集通过数据并行分片到多个实例上进行分布式训练。
- en: Model parallelism does the opposite and splits the processing onto separate
    instances and processes the entire dataset separately on each instance. It is
    quite a bit more complicated and is typically required when our model is too large
    to fit into the resources of a single instance due to memory constraints. [Figure 8-6](#sharding_a_model_across_multiple_instan)
    shows how model parallelism splits up the model onto different instances and processes
    the full dataset with each “model shard.”
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行性则相反，将处理分割到单独的实例上，并在每个实例上分别处理整个数据集。这要复杂得多，通常在我们的模型由于内存限制而无法适应单个实例资源时需要使用。[图 8-6](#sharding_a_model_across_multiple_instan)显示模型并行性如何将模型分割到不同实例上，并处理每个“模型分片”的完整数据集。
- en: '![](assets/dsaw_0806.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0806.png)'
- en: Figure 8-6\. Sharding a model across multiple instances for distributed training
    with model parallelism.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6。使用模型并行将模型分片到多个实例上进行分布式训练。
- en: SageMaker natively supports both data parallelism and model parallelism. For
    our BERT model, we will use data parallelism since our model fits into a single
    instance, so we will shard our dataset across the different instances, train on
    each shard, and combine the results through the all-reduce communication strategy
    built in to SageMaker distributed training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker原生支持数据并行和模型并行。对于我们的BERT模型，我们将使用数据并行，因为我们的模型适合单个实例，因此我们将把数据集分片到不同的实例上进行训练，并通过内置到SageMaker分布式训练中的全局归约通信策略合并结果。
- en: Choose a Distributed File System
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择分布式文件系统
- en: Typically, our distributed training clusters communicate directly with S3 to
    read and write our data. However, some frameworks and tools are not optimized
    for S3 natively—or only support POSIX-compatible filesystems. For these scenarios,
    we can use FSx for Lustre (Linux) or Amazon FSx for Windows File Server to expose
    a POSIX-compatible filesystem on top of S3\. This extra layer also provides a
    crucial cache-performance benefit that reduces training times to reasonable levels
    for larger datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们的分布式训练集群直接与S3进行数据读写。然而，某些框架和工具未经过S3本地优化，或者仅支持POSIX兼容的文件系统。对于这些情况，我们可以使用FSx
    for Lustre（Linux）或Amazon FSx for Windows File Server，在S3之上提供一个POSIX兼容的文件系统。这一额外层还提供了关键的缓存性能优势，可以将大型数据集的训练时间降至合理水平。
- en: Amazon FSx for Lustre is a high-performance, POSIX-compatible filesystem that
    natively integrates with S3\. FSx for Lustre is based on the open source Lustre
    filesystem designed for highly scalable, highly distributed, and highly parallel
    training jobs with petabytes of data, terabytes per second of aggregate I/O throughput,
    and consistent low latency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon FSx for Lustre是一个高性能的、与POSIX兼容的文件系统，能够与S3本地集成。FSx for Lustre基于开源Lustre文件系统设计，专为具有PB级数据、每秒TB级聚合I/O吞吐量和一致低延迟的高度可扩展、高度分布式和高度并行的训练作业而设计。
- en: There is also Amazon FSx for Windows File Server, which provides a Windows-compatible
    filesystem that natively integrates with S3 as well. However, we have chosen to
    focus on FSx for Lustre as our examples are Linux based. Both filesystems are
    optimized for machine learning, analytics, and high-performance computing workloads
    using S3\. And both filesystems offer similar features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还有Amazon FSx for Windows File Server，它提供与Windows兼容的文件系统，并与S3原生集成。然而，我们选择关注FSx
    for Lustre，因为我们的示例是基于Linux的。这两种文件系统都针对使用S3的机器学习、分析和高性能计算工作负载进行了优化。并且这两种文件系统都提供类似的功能。
- en: FSx for Lustre is a fully managed service that simplifies the complexity of
    setting up and managing the Lustre filesystem. Mounting an S3 bucket as a filesystem
    in minutes, FSx for Lustre lets us access data from any number of instances concurrently
    and caches S3 objects to improve performance of iterative machine learning workloads
    that pass over the dataset many times to fit a high-accuracy model. [Figure 8-7](#sagemaker_uses_fsx_for_lustre_to_increa)
    shows how SageMaker uses FSx for Lustre to provide fast, shared access to our
    S3 data and accelerate our training and tuning jobs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: FSx for Lustre是一个完全托管的服务，简化了设置和管理Lustre文件系统的复杂性。将S3存储桶挂载为文件系统后，FSx for Lustre让我们能够同时从任意数量的实例访问数据，并且缓存S3对象以提高迭代机器学习工作负载的性能，这些工作负载会多次处理数据集以拟合高精度模型。[图 8-7](#sagemaker_uses_fsx_for_lustre_to_increa)展示了SageMaker如何使用FSx
    for Lustre提供快速、共享的S3数据访问，加速我们的训练和调优任务。
- en: '![](assets/dsaw_0807.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dsaw_0807.png)'
- en: Figure 8-7\. SageMaker uses FSx for Lustre to increase training and tuning job
    performance.
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7。SageMaker使用FSx for Lustre提升训练和调优任务的性能。
- en: Our SageMaker training cluster instances access a file in FSx for Lustre using
    */mnt/data/file1.txt*. FSx for Lustre translates this request and issues a `GetObject`
    request to S3\. The file is cached and returned to the cluster instance. If the
    file has not changed, subsequent requests will return from FSx for Lustre’s cache.
    Since training data does not typically change during a training job run, we see
    huge performance gains as we iterate through our dataset over many training epochs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SageMaker训练集群实例使用*/mnt/data/file1.txt*访问FSx for Lustre中的文件。FSx for Lustre会转换此请求并向S3发出`GetObject`请求。文件将被缓存并返回到集群实例。如果文件未更改，则后续请求将从FSx
    for Lustre的缓存返回。由于训练数据在训练作业运行期间通常不会更改，因此在多个训练周期中迭代数据集时，我们看到了巨大的性能提升。
- en: 'Once we have set up the FSx for Lustre filesystem, we can pass the location
    of the FSx for Lustre filesystem into the training job as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了FSx for Lustre文件系统，我们可以将FSx for Lustre文件系统的位置传递给训练作业，如下所示：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we need to specify the `subnets` and `security_group_ids` used when
    we created our FSx for Lustre filesystem. We will dive deep into networking and
    security in [Chapter 12](ch12.html#secure_data_science_on_aws).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在创建我们的FSx for Lustre文件系统时，我们需要指定`subnets`和`security_group_ids`。我们将在[第12章](ch12.html#secure_data_science_on_aws)深入探讨网络和安全性。
- en: Another option for distributed training is Amazon Elastic File System (Amazon
    EFS). Amazon EFS is compatible with industry-standard Network File System protocols
    but optimized for AWS’s cloud-native and elastic environment, including networking,
    access control, encryption, and availability. In this section, we adapt our distributed
    training job to use both FSx for Lustre (Linux) and Amazon EFS. Amazon EFS provides
    centralized, shared access to our training datasets across thousands of instances
    in a distributed training cluster, as shown in [Figure 8-8](#amazon_elastic_file_system_left_parenth).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分布式训练选项是Amazon Elastic File System（Amazon EFS）。Amazon EFS兼容行业标准的网络文件系统协议，但针对AWS的云原生和弹性环境进行了优化，包括网络、访问控制、加密和可用性。在本节中，我们调整我们的分布式训练作业，同时使用FSx
    for Lustre（Linux）和Amazon EFS。Amazon EFS为分布式训练集群中的数千个实例提供集中、共享的训练数据集访问，如[图 8-8](#amazon_elastic_file_system_left_parenth)所示。
- en: '![](assets/dsaw_0808.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dsaw_0808.png)'
- en: Figure 8-8\. Amazon EFS with SageMaker.
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8。Amazon EFS与SageMaker。
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: SageMaker Studio uses Amazon EFS to provide centralized, shared, and secure
    access to code and notebooks across all team members with proper authorization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Studio使用Amazon EFS为所有团队成员提供集中、共享和安全的代码和笔记本访问权限。
- en: Data stored in Amazon EFS is replicated across multiple Availability Zones,
    which provides higher availability and read/write throughput. The Amazon EFS filesystem
    will scale out automatically as new data is ingested.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在Amazon EFS中的数据在多个可用区复制，提供更高的可用性和读/写吞吐量。随着新数据的摄入，Amazon EFS文件系统将自动扩展。
- en: 'Assuming we have mounted and populated the Amazon EFS filesystem with training
    data, we can pass the Amazon EFS mount into the training job using two different
    implementations: `FileSystemInput` and `FileSystemRecordSet`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经挂载并填充了Amazon EFS文件系统的训练数据，我们可以通过两种不同的实现将Amazon EFS挂载传递到训练作业中：`FileSystemInput`和`FileSystemRecordSet`。
- en: 'This example shows how to use the `FileSystemInput` implementation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例展示了如何使用`FileSystemInput`实现：
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we need to specify the `subnets` and `security_group_ids` used when
    we created our Amazon EFS filesystem. We will dive deep into networking and security
    in [Chapter 12](ch12.html#secure_data_science_on_aws).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在创建Amazon EFS文件系统时，我们需要指定用于其的`子网`和`安全组ID`。我们将深入研究[第12章](ch12.html#secure_data_science_on_aws)中的网络和安全性。
- en: For our example, we will use FSx for Lustre because of its S3-caching capabilities,
    which greatly increases our training performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用FSx for Lustre，因为其具有S3缓存功能，可以极大地提高我们的训练性能。
- en: Launch the Distributed Training Job
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动分布式训练作业
- en: 'SageMaker, following cloud-native principles, is inherently distributed and
    scalable in nature. In the previous chapter, we were using a single instance by
    specifying `train_instance_count=1`. Here, we will increase the `train_instance_count`
    and specify the `distribution` parameter in our TensorFlow estimator to enable
    SageMaker distributed training, as shown in the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker遵循云原生原则，本质上是分布式和可扩展的。在上一章中，我们使用了单个实例，通过指定`train_instance_count=1`。在这里，我们将增加`train_instance_count`并在我们的TensorFlow估算器中指定`distribution`参数，以启用SageMaker的分布式训练，如下所示：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: SageMaker automatically passes the relevant cluster information to TensorFlow
    to enable the all-reduce strategy and use distributed TensorFlow.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker会自动将相关集群信息传递给TensorFlow，以启用全减少策略并使用分布式TensorFlow。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: SageMaker also passes the same cluster information to enable distributed PyTorch
    and Apache MXNet, but we are only showing TensorFlow in this example.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker还将相同的集群信息传递给分布式PyTorch和Apache MXNet，但在本示例中仅显示TensorFlow。
- en: Reduce Cost and Increase Performance
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少成本并提高性能
- en: In this section, we discuss various ways to increase cost-effectiveness and
    performance using some advanced SageMaker features, including SageMaker Autopilot
    for baseline hyper-parameter selection, `ShardedByS3Key` to distribute input files
    across all training instances, and `Pipe` mode to improve I/O throughput. We also
    highlight AWS’s enhanced networking capabilities, including the Elastic Network
    Adapter (ENA) and Elastic Fabric Adapter (EFA) to optimize network performance
    between instances in our training and tuning cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了使用一些高级SageMaker功能来提高成本效益和性能的各种方法，包括SageMaker Autopilot用于基线超参数选择，`ShardedByS3Key`用于在所有训练实例之间分发输入文件，以及`Pipe`模式来提高I/O吞吐量。我们还强调了AWS增强的网络功能，包括弹性网络适配器（ENA）和弹性布线适配器（EFA），以优化训练和调整集群中实例之间的网络性能。
- en: Start with Reasonable Hyper-Parameter Ranges
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从合理的超参数范围开始
- en: By researching the work of others, we can likely find a range of hyper-parameters
    that will narrow the search space and speed up our SageMaker HPT jobs. If we don’t
    have a good starting point, we can use the `Logarithmic` scaling strategy to determine
    the scale within which we should explore. Just knowing the power of 10 can make
    a big difference in reducing the time to find the best hyper-parameters for our
    algorithm and dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究他人的工作，我们很可能可以找到一系列超参数的范围，这将缩小搜索空间并加快我们的SageMaker HPT作业。如果我们没有一个好的起点，我们可以使用`对数`缩放策略来确定我们应该探索的范围。仅仅知道10的幂次可以显著减少找到最佳超参数的时间，适用于我们的算法和数据集。
- en: Shard the Data with ShardedByS3Key
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`ShardedByS3Key`对数据进行分片
- en: 'When training at scale, we need to consider how each instance in the cluster
    will read the large training datasets. We can use a brute-force approach and copy
    all of the data to all of the instances. However, with larger datasets, this may
    take a long time and potentially dominate the overall training time. For example,
    after performing feature engineering, our tokenized training dataset has approximately
    45 `TFRecord` “part” files, as shown in the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模训练时，我们需要考虑集群中每个实例如何读取大型训练数据集。我们可以采用蛮力方法将所有数据复制到所有实例中。然而，对于更大的数据集，这可能需要很长时间，并且可能会主导整体训练时间。例如，在执行特征工程之后，我们的分词训练数据集大约有45个`TFRecord`“part”文件，如下所示：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Rather than load all 45 part files onto all instances in the cluster, we can
    improve startup performance by placing only 15 part files onto each of the 3 cluster
    instances for a total of 45 part files spread across the cluster. This is called
    “sharding.” We will use a SageMaker feature called `ShardedByS3Key` that evenly
    distributes the part files across the cluster, as shown in [Figure 8-9](#using_shardedbysthreekey_distribution_s).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将所有45个分片文件加载到集群中的所有实例中，我们可以通过将每个集群实例上仅放置15个分片文件来改善启动性能，总共在集群中分布了45个分片文件。这称为“分片”。我们将使用SageMaker的`ShardedByS3Key`功能，将分片文件均匀分布到集群中，如[图8-9](#using_shardedbysthreekey_distribution_s)所示。
- en: '![](assets/dsaw_0809.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0809.png)'
- en: Figure 8-9\. Using `ShardedByS3Key` distribution strategy to distribute the
    input files across the cluster instances.
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9\. 使用`ShardedByS3Key`分布策略将输入文件分布到集群实例中。
- en: 'Here we set up the `ShardedByS3Key` distribution strategy for our S3 input
    data including the train, validation, and test datasets:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为我们的S3输入数据（包括训练、验证和测试数据集）设置了`ShardedByS3Key`分布策略：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we call `fit()` with the input map for each of our dataset splits, including
    train, validation, and test:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每个数据集分片（包括训练、验证和测试）调用`fit()`：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, each instance in our cluster will receive approximately 15 files
    for each of the dataset splits.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们集群中的每个实例将收到每个数据集分片约15个文件。
- en: Stream Data on the Fly with Pipe Mode
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Pipe模式动态流式传输数据
- en: In addition to sharding, we can also use a SageMaker feature called `Pipe` mode
    to load the data on the fly and as needed. Up until now, we’ve been using the
    default `File` mode, which copies all of the data to all the instances when the
    training job starts. This creates a long pause at the start of the training job
    as the data is copied. `Pipe` mode provides the most significant performance boost
    when using large datasets in the 10, 100, or 1,000 GB range. If our dataset is
    smaller, we should use `File` mode.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分片之外，我们还可以使用SageMaker的`Pipe`模式，按需动态加载数据。到目前为止，我们一直在使用默认的`File`模式，在训练作业启动时将所有数据复制到所有实例，这会导致长时间的启动暂停。使用大型数据集（10、100或1,000
    GB范围内）时，`Pipe`模式提供了最显著的性能提升。如果我们的数据集较小，则应使用`File`模式。
- en: '`Pipe` mode streams data in parallel from S3 directly into the training processes
    running on each instance, which provides significantly higher I/O throughput than
    `File` mode. By streaming only the data that is needed when it’s needed, our training
    and tuning jobs start quicker, complete faster, and use less disk space overall.
    This directly leads to lower cost for our training and tuning jobs.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipe`模式可以并行地从S3直接流式传输数据到每个实例的训练过程中，其I/O吞吐量明显高于`File`模式。通过仅在需要时流式传输数据，我们的训练和调优作业启动更快、完成更快，并且总体上使用更少的磁盘空间。这直接降低了我们训练和调优作业的成本。'
- en: '`Pipe` mode works with S3 to pull the rows of training data as needed. Under
    the hood, `Pipe` mode is using Unix first-in, first-out (FIFO) files to read data
    from S3 and cache it locally on the instance shortly before the data is needed
    by the training job. These FIFO files are one-way readable. In other words, we
    can’t back up or skip ahead randomly.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipe`模式与S3一起按需获取训练数据的行。在幕后，`Pipe`模式使用Unix先进先出（FIFO）文件从S3读取数据并在数据即将被训练作业使用前在实例上缓存。这些FIFO文件是单向可读的，换句话说，我们无法随机备份或跳过。'
- en: 'Here is how we configure our training job to use `Pipe` mode:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们配置训练作业使用`Pipe`模式的方式：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since `Pipe` mode wraps our TensorFlow Dataset Reader, we need to change our
    TensorFlow code slightly to detect `Pipe` mode and use the `PipeModeDataset` wrapper:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`Pipe`模式包装了我们的TensorFlow数据集读取器，我们需要稍微修改我们的TensorFlow代码以检测`Pipe`模式并使用`PipeModeDataset`包装器：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Enable Enhanced Networking
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用增强型网络
- en: Training at scale requires super-fast communication between instances in the
    cluster. Be sure to select an instance type that utilizes ENA and EFA to provide
    high network bandwidth and consistent network latency between the cluster instances.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在规模化训练中，集群内实例之间需要超快的通信。务必选择一种能够利用ENA和EFA提供高网络带宽以及集群内实例之间一致网络延迟的实例类型。
- en: ENA works well with the AWS deep learning instance types, including the C, M,
    P, and X series. These instance types offer a large number of CPUs, so they benefit
    greatly from efficient sharing of the network adapter. By performing various network-level
    optimizations such as hardware-based checksum generation and software-based routing,
    ENA reduces overhead, improves scalability, and maximizes consistency. All of
    these optimizations are designed to reduce bottlenecks, offload work from the
    CPUs, and create an efficient path for the network packets.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ENA 与 AWS 深度学习实例类型（包括 C、M、P 和 X 系列）兼容良好。这些实例类型提供大量的 CPU，因此它们从网络适配器的高效共享中受益匪浅。通过执行各种网络级别的优化，如基于硬件的校验和生成和软件路由，ENA
    减少了开销，提高了可伸缩性，并最大化了一致性。所有这些优化措施旨在减少瓶颈，卸载 CPU 的工作，并为网络数据包创建有效路径。
- en: EFA uses custom-built, OS-level bypass techniques to improve network performance
    between instances in a cluster. EFA natively supports MPI, which is critical to
    scaling high-performance computing applications that scale to thousands of CPUs.
    EFA is supported by many of the compute-optimized instance types, including the
    C and P series.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: EFA 使用自定义构建的操作系统级旁路技术来提高集群内实例之间的网络性能。EFA 原生支持 MPI，这对于扩展到数千个 CPU 的高性能计算应用至关重要。EFA
    得到许多计算优化实例类型的支持，包括 C 和 P 系列。
- en: While not much concrete data exists to verify, some practitioners have noticed
    a performance improvement when running distributed SageMaker jobs in a Virtual
    Private Cloud (VPC). This is likely attributed to reduced network latency between
    cluster instances running in the same VPC. If our training jobs are particularly
    latency-sensitive, we might want to try running our training jobs in a VPC. We
    dive deep into VPCs and SageMaker in [Chapter 12](ch12.html#secure_data_science_on_aws).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有足够的具体数据来验证，但一些从业者注意到，在虚拟私有云（VPC）中运行分布式 SageMaker 作业时，性能有所提升。这很可能归因于在同一 VPC
    中运行的集群实例之间的网络延迟减少。如果我们的训练作业特别对延迟敏感，我们可能想尝试在 VPC 中运行我们的训练作业。我们在[第 12 章](ch12.html#secure_data_science_on_aws)深入探讨了
    VPC 和 SageMaker。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we used SageMaker Experiments and HPT to track, compare, and
    choose the best hyper-parameters for our specific algorithm and dataset. We explored
    various distributed communication strategies, such as parameter servers and all-reduce.
    We demonstrated how to use FSx for Lustre to increase S3 performance and how to
    configure our training job to use the Amazon EFS. Next, we explored a few ways
    to reduce cost and increase performance using SageMaker Autopilot’s hyper-parameter
    selection feature and SageMaker’s optimized data loading strategies like `ShardedByS3Key`
    and `Pipe` mode. Last, we discussed the enhanced networking features for compute-optimized
    instance types, including the ENA and EFA.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 SageMaker 实验和 HPT 来跟踪、比较和选择我们特定算法和数据集的最佳超参数。我们探讨了各种分布式通信策略，如参数服务器和全局减少。我们演示了如何使用
    FSx for Lustre 来增加 S3 的性能，并如何配置我们的训练作业以使用 Amazon EFS。最后，我们讨论了使用 SageMaker 自动驾驶的超参数选择功能和像
    `ShardedByS3Key` 和 `Pipe` 模式这样的 SageMaker 优化数据加载策略来降低成本和提高性能的几种方法。我们还讨论了计算优化实例类型，包括
    ENA 和 EFA 的增强网络功能。
- en: In [Chapter 9](ch09.html#deploy_models_to_production), we will deploy our models
    into production using various rollout, A/B testing, and multiarmed bandit strategies.
    We will discuss how to integrate model predictions into applications using real-time
    REST endpoints, offline batch jobs, and edge devices. We demonstrate how to auto-scale
    our endpoints based on built-in and custom CloudWatch metrics. We also dive deep
    into using SageMaker Model Monitor to detect drifts in data distributions, model
    bias, and model explainability of our live SageMaker Endpoints.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 9 章](ch09.html#deploy_models_to_production)，我们将使用各种推出、A/B 测试和多臂老虎机策略将我们的模型部署到生产环境中。我们将讨论如何使用实时
    REST 端点、离线批处理作业和边缘设备将模型预测集成到应用程序中。我们演示了如何基于内置和自定义 CloudWatch 指标自动扩展我们的端点。我们还深入探讨了如何使用
    SageMaker 模型监视器检测我们的实时 SageMaker 端点中数据分布的变化、模型偏差和模型可解释性的漂移。
