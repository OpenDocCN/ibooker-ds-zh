- en: Part 1\. Search meets deep learning
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分\. 搜索与深度学习相遇
- en: Setting up search engines to effectively react to users’ needs isn’t an easy
    task. Traditionally, many manual tweaks and adjustments had to be made to a search
    engine’s internals to get it to work decently on a real collection of data. On
    the other hand, deep neural networks are very good at learning useful information
    about vast amounts of data. In this first part of the book, we’ll start looking
    into how a search engine can be used in conjunction with a neural network to get
    around some common limitations and provide users with a better search experience.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 设置搜索引擎以有效地响应用户的需求并不是一项容易的任务。传统上，必须对搜索引擎的内部进行许多手动调整和修改，才能使其在真实的数据集上运行得体。另一方面，深度神经网络在从大量数据中学习有用信息方面非常出色。在这本书的第一部分，我们将开始探讨搜索引擎如何与神经网络结合使用，以克服一些常见的限制，并为用户提供更好的搜索体验。
- en: Chapter 1\. Neural search
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章\. 神经搜索
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: A gentle introduction to search fundamentals
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索基础知识的温和介绍
- en: Important problems in search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索中的重要问题
- en: Why neural networks can help search engines be more effective
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么神经网络可以帮助搜索引擎更有效
- en: Suppose you want to learn something about the latest research breakthroughs
    in artificial intelligence. What will you do to find information? How much time
    and work does it take to get the facts you’re looking for? If you’re in a (huge)
    library, you can ask the librarian what books are available on the topic, and
    they will probably point you to a few they know about. Ideally, the librarian
    will suggest particular chapters to browse in each book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想了解关于人工智能最新研究突破的信息。你将如何找到信息？找到你想要的事实需要多少时间和工作？如果你在一个（巨大的）图书馆里，你可以询问图书管理员关于这个主题有哪些可用的书籍，他们可能会指向他们知道的几本书。理想情况下，图书管理员会建议你在每本书中浏览特定的章节。
- en: That sounds easy enough. But the librarian generally comes from a different
    context than you do, meaning you and the librarian may have different opinions
    about what’s significant. The library could have books in various languages, or
    the librarian might speak a different language. Their information about the topic
    could be outdated, given that *latest* is a fairly relative point in time, and
    you don’t know when the librarian last read anything about artificial intelligence,
    or if the library regularly receives publications in the field. Additionally,
    the librarian may not understand your inquiry properly. The librarian may think
    you’re talking about intelligence from the psychology perspective,^([[1](#ch01fn01)])
    requiring a few iterations back and forth before you understand one another and
    get to the pieces of information you need.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来足够简单。但图书管理员通常来自与你不同的背景，这意味着你和图书管理员可能对什么是重要的有不同的看法。图书馆可能有各种语言的书籍，或者图书管理员可能说不同的语言。鉴于“最新”是一个相对的时间点，而且你不知道图书管理员最后一次阅读关于人工智能的内容是什么时候，或者图书馆是否定期接收该领域的出版物，他们关于这个主题的信息可能已经过时了。此外，图书管理员可能无法正确理解你的查询。图书管理员可能认为你在从心理学角度谈论智能，^([[1](#ch01fn01)])
    这需要你来回迭代几次，才能相互理解并找到你需要的信息片段。
- en: ¹
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This happened to me in real life.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是我现实生活中发生的事情。
- en: Then, after all this, you might discover the library doesn’t have the book you
    need; or the information may be spread among several books, and you have to read
    them all. Exhausting!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在所有这些之后，你可能会发现图书馆没有你需要的书籍；或者信息可能分散在几本书中，你必须全部阅读。太累人了！
- en: 'Unless you’re a librarian yourself, this is what often happens nowadays when
    you search for something on the internet. Although we can think of the internet
    as a single huge library, there are many different librarians out there to help
    you find the information you need: search engines. Some search engines are experts
    in certain topics; others know only a subset of a library, or only a single book.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你自己是图书管理员，否则现在你在互联网上搜索某样东西时，通常会发生这种情况。虽然我们可以将互联网想象成一个巨大的图书馆，但那里有许多不同的图书管理员可以帮助你找到你需要的信息：搜索引擎。一些搜索引擎是某些主题的专家；其他只知道图书馆的一个子集，或者只知道一本书。
- en: Now imagine that someone, let’s call him Robbie, who already knows about both
    the library and its visitors, can help you communicate with the librarian in order
    to better find what you’re looking for. That will help you get your answers more
    quickly. Robbie can help the librarian understand a visitor’s inquiry by providing
    additional context, for example. Robbie knows what the visitor usually reads about,
    so he skips all the books about psychology. Also, having read a lot of the books
    in the library, Robbie has better insight into what’s important in the field of
    artificial intelligence. It would be extremely helpful to have advisors like Robbie
    to help search engines work better and faster, and help users get more useful
    information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，如果有人，我们叫他Robbie，他既了解图书馆又了解其访客，可以帮助您与图书管理员沟通，以便更好地找到您想要的东西。这将帮助您更快地得到答案。Robbie可以通过提供额外的上下文来帮助图书管理员理解访客的询问，例如。Robbie知道访客通常阅读什么，因此跳过了所有关于心理学的书籍。而且，由于阅读了大量图书馆的书籍，Robbie对人工智能领域的重要性的洞察力更强。拥有像Robbie这样的顾问来帮助搜索引擎更好地工作，并帮助用户获得更有用的信息，将是非常有帮助的。
- en: This book is about using techniques from a machine learning field called *deep
    learning* (DL) to build models and algorithms that can influence the behavior
    of search engines, to make them more effective. Deep learning algorithms will
    play the role of Robbie, helping the search engine to provide a better search
    experience and to deliver more precise answers to end users.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是关于使用名为*深度学习*（DL）的机器学习领域的技巧来构建可以影响搜索引擎行为的模型和算法，使它们更有效。深度学习算法将扮演Robbie的角色，帮助搜索引擎提供更好的搜索体验，并向最终用户提供更精确的答案。
- en: One important thing to note is that DL isn’t the same as *artificial intelligence*
    (AI). As you can see in [figure 1.1](#ch01fig01), AI is a huge research field;
    machine learning is only part of it, and DL, in turn, is a sub-area of machine
    learning. Basically, DL studies how to make machines “learn” using the deep neural
    network computing model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要事项是，深度学习（DL）并不等同于*人工智能*（AI）。正如您在[图1.1](#ch01fig01)中可以看到的，AI是一个庞大的研究领域；机器学习只是其中的一部分，而深度学习（DL）又是机器学习的一个子领域。基本上，深度学习研究如何使用深度神经网络计算模型使机器“学习”。
- en: Figure 1.1\. Artificial intelligence, machine learning, and deep learning
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1\. 人工智能、机器学习和深度学习
- en: '![](Images/01fig01_alt.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1](Images/01fig01_alt.jpg)'
- en: 1.1\. Neural networks and deep learning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 神经网络和深度学习
- en: The goal of this book is to enable you to use deep learning in the context of
    search engines, to improve the search experience and results. Even if you’re not
    going to build the next Google search, you should be able to learn enough to use
    DL techniques within small or medium-sized search engines to provide a better
    experience to users. Neural search should help you automate work that you’d otherwise
    have to perform manually. For example, you’ll learn how to automate extraction
    of synonyms from search engine data, avoiding manual editing of synonym files
    ([chapter 2](kindle_split_013.xhtml#ch02)). This saves time while improving search
    effectiveness, regardless of the specific use case or domain. The same is true
    for having good related-content suggestions ([chapter 6](kindle_split_018.xhtml#ch06)).
    In many cases, users are satisfied with a combination of plain search together
    with the ability to navigate related content. We’ll also cover some more-specific
    use cases, such as searching content in multiple languages ([chapter 7](kindle_split_020.xhtml#ch07))
    and searching for images ([chapter 8](kindle_split_021.xhtml#ch08)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是使您能够在搜索引擎的背景下使用深度学习，以改善搜索体验和结果。即使您不打算构建下一个谷歌搜索引擎，您也应该能够学习到足够多的知识，以便在小型或中型搜索引擎中使用深度学习技术，为用户提供更好的体验。神经搜索应该能帮助您自动化那些您本需要手动执行的工作。例如，您将学习如何从搜索引擎数据中自动提取同义词，避免手动编辑同义词文件（[第2章](kindle_split_013.xhtml#ch02)）。这可以在不影响特定用例或领域的情况下节省时间，同时提高搜索效率。对于提供良好的相关内容建议（[第6章](kindle_split_018.xhtml#ch06)）也是如此。在许多情况下，用户对普通搜索和导航相关内容的能力的结合感到满意。我们还将涵盖一些更具体的用例，例如在多种语言中搜索内容（[第7章](kindle_split_020.xhtml#ch07)）和搜索图像（[第8章](kindle_split_021.xhtml#ch08)）。
- en: 'The only requirement for the techniques we’ll discuss is that they have enough
    data to feed into neural networks. But it’s difficult to define the boundaries
    of “enough data” in a generic way. Let’s instead summarize the minimum number
    of documents (text, images, and so on) that are generally needed for each of the
    problems addressed in the book: see [table 1.1](#ch01table01).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的技术唯一的要求是它们有足够的数据输入到神经网络中。但以通用方式定义“足够的数据”的边界是困难的。让我们总结一下，本书中解决每个问题的最小文档数量（文本、图像等）：参见[表1.1](#ch01table01)。
- en: Table 1.1\. Per-task requirements for neural search techniques
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1\. 神经搜索技术的每项任务需求
- en: '| Task | Minimum number of docs (range) | Chapter |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 最小文档数量（范围） | 章节 |'
- en: '| --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Learning word representations | 1,000–10,000 | 2, 5 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 学习词表示 | 1,000–10,000 | 2, 5 |'
- en: '| Text generation | 10,000–100,000 | 3, 4 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成 | 10,000–100,000 | 3, 4 |'
- en: '| Learning document representations | 1,000–10,000 | 6 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 学习文档表示 | 1,000–10,000 | 6 |'
- en: '| Machine translation | 10,000–100,000 | 7 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 机器翻译 | 10,000–100,000 | 7 |'
- en: '| Learning image representations | 10,000–100,000 | 8 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 学习图像表示 | 10,000–100,000 | 8 |'
- en: Note that [table 1.1](#ch01table01) isn’t meant to be strictly adhered to; these
    are numbers drawn from experience. For example, even if a search engine counts
    fewer than 10,000 documents, you can still try to implement the neural machine
    translation techniques from [chapter 7](kindle_split_020.xhtml#ch07); but you
    should take into account that it may be harder to get high-quality results (for
    example, perfect translations).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，[表1.1](#ch01table01)并不是要求严格遵循的；这些数字是从经验中得出的。例如，即使搜索引擎计算的文档少于10,000篇，你仍然可以尝试实现[第7章](kindle_split_020.xhtml#ch07)中的神经机器翻译技术；但你应该考虑到，可能更难获得高质量的结果（例如，完美的翻译）。
- en: As you read the book, you’ll learn a lot about DL as well as all the required
    search fundamentals to implement these DL principles in a search engine. So if
    you’re a search engineer or a programmer willing to learn neural search, this
    book is for you.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本书时，你将了解很多关于深度学习以及实现这些深度学习原理在搜索引擎中的所有必需的搜索基础。所以，如果你是一名搜索工程师或愿意学习神经搜索的程序员，这本书就是为你准备的。
- en: 'You aren’t expected to know what DL is or how it works, at this point. You’ll
    get to know more as we look at some specific algorithms one by one, when they
    become useful for solving particular types of search problems. For now, I’ll start
    you off with some basic definitions. Deep learning is a field of machine learning
    where computers are capable of learning to represent and recognize things incrementally,
    by using deep neural networks. Deep *artificial neural networks* are a computational
    paradigm originally inspired by the way the brain is organized into graphs of
    neurons (although the brain is much more complex than an artificial neural network).
    Usually, information flows into neurons in an *input layer*, then through a network
    of hidden neurons (forming one or more *hidden layers*), and then out through
    neurons in an *output layer*. Neural networks can also be thought of as black
    boxes: smart functions that can transform inputs into outputs, based on what each
    network has been trained for. A common neural network has at least one input layer,
    one hidden layer, and one output layer. When a network has more than one hidden
    layer, we call the network *deep*. In [figure 1.2](#ch01fig02), you can see a
    deep neural network with two hidden layers.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你不需要了解深度学习是什么，以及它是如何工作的。随着我们逐一查看一些特定的算法，当你发现它们对解决特定类型的搜索问题有用时，你会了解更多。现在，我将从一些基本定义开始。深度学习是机器学习的一个领域，其中计算机能够通过使用深度神经网络，逐步学习和表示以及识别事物。深度**人工神经网络**是一种计算范式，最初受到大脑以神经元图的形式组织方式的启发（尽管大脑比人工神经网络要复杂得多）。通常，信息流入**输入层**的神经元，然后通过一个隐藏神经元网络（形成一个或多个**隐藏层**），最后通过**输出层**的神经元流出。神经网络也可以被视为黑盒：智能函数，可以根据每个网络训练的内容将输入转换为输出。一个常见的神经网络至少包含一个输入层、一个隐藏层和一个输出层。当一个网络有多个隐藏层时，我们称这个网络为**深度**。在[图1.2](#ch01fig02)中，你可以看到一个具有两个隐藏层的深度神经网络。
- en: Figure 1.2\. A deep neural network with two hidden layers
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2\. 具有两个隐藏层的深度神经网络
- en: '![](Images/01fig02_alt.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig02_alt.jpg)'
- en: Before going into more detail about neural networks, let’s take a step back.
    I said deep learning is a subfield of machine learning, which is part of the broader
    area of artificial intelligence. But what is machine learning?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在更详细地介绍神经网络之前，让我们退一步。我说深度学习是机器学习的一个子领域，而机器学习是更广泛的人工智能领域的一部分。但机器学习是什么？
- en: 1.2\. What is machine learning?
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 什么是机器学习？
- en: An overview of basic machine learning concepts is useful here before diving
    into DL and search specifics. Many of the concepts that apply to learning with
    artificial neural networks, such as *supervised* and *unsupervised* learning,
    *training*, and *predicting*, come from machine learning. Let’s quickly go over
    some basic machine learning concepts that we’ll be using in DL (applied to search)
    during the course of this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨深度学习和搜索的细节之前，这里简要概述基本机器学习概念是有用的。许多适用于使用人工神经网络进行学习的概念，如**监督学习**和**无监督学习**、**训练**和**预测**，都来自机器学习。让我们快速回顾一些基本机器学习概念，这些概念将在本书的深度学习（应用于搜索）过程中使用。
- en: Machine learning (ML) is an automated approach to solving problems based on
    algorithms that can learn optimal solutions from previous experience. In many
    cases, this experience comes in the form of pairs made from what has been previously
    observed together with what you want the algorithm to infer from it. For example,
    an ML algorithm can be fed text pairs, where the input is some text and the output
    is a category that can be used to classify similar texts. Imagine you’re back
    in the library, but this time as the librarian; you’ve bought thousands of books,
    and you want to organize them on bookshelves so people can easily find them. To
    that end, you want to categorize them so that books belonging to the same category
    are placed close to each other on the same bookshelf (which perhaps has a small
    tag indicating the category). If you can spend a few hours categorizing the books
    manually, you’ll have built the experience your ML algorithm needs. Afterward,
    you can train an ML algorithm based on your wise judgment, and it will do the
    categorization of the remaining books on your behalf.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是一种基于算法的自动化问题解决方法，这些算法可以从以往的经验中学习最佳解决方案。在许多情况下，这种经验以观察到的数据对的形式出现，包括之前观察到的数据和你希望算法从中推断出的数据。例如，一个机器学习算法可以接收文本对，其中输入是某些文本，输出是一个可以用于分类相似文本的类别。想象一下，你回到了图书馆，但这次你是图书管理员；你已经购买了数千本书，你希望将它们组织在书架上，以便人们可以轻松找到它们。为此，你希望将它们分类，以便同一类别的书籍在同一个书架上靠近放置（可能有一个小标签指示类别）。如果你能花几个小时手动分类书籍，你将为你的机器学习算法构建所需的经验。之后，你可以根据你的明智判断训练一个机器学习算法，它将代表你进行剩余书籍的分类。
- en: This type of training, where you specify the desired output corresponding to
    each input, is called *supervised learning*. Each pair made of an input and its
    corresponding target output is called a *training sample*. [Table 1.2](#ch01table02)
    shows some of the categorizations a librarian might make manually, to help create
    a supervised learning algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的训练，其中你指定与每个输入相对应的期望输出，被称为**监督学习**。由输入及其相应的目标输出组成的每一对被称为**训练样本**。[表1.2](#ch01table02)展示了图书管理员可能手动进行的某些分类，以帮助创建一个监督学习算法。
- en: Table 1.2\. Example data for book categorization
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.2. 书籍分类的示例数据
- en: '| Title | Text | Categories |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 文本 | 类别 |'
- en: '| --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Taming Text* | If you’re reading this book, chances are you’re a programmer...
    | NLP, search |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| *Taming Text* | 如果你正在阅读这本书，那么你很可能是程序员... | 自然语言处理，搜索 |'
- en: '| *Relevant Search* | Getting a search engine to behave can be maddening ...
    | Search, relevance |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *Relevant Search* | 让搜索引擎表现良好可能会让人抓狂... | 搜索，相关性 |'
- en: '| *OAuth2 in Action* | If you’re a software developer on the web today ...
    | Security, OAuth |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| *OAuth2 in Action* | 如果你今天是一名网络软件开发者... | 安全，OAuth |'
- en: '| *The Lord of the Rings* | ... | Fantasy, novels |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| *The Lord of the Rings* | ... | 奇幻，小说 |'
- en: '| *Lucene in Action* | Lucene is a powerful Java search library that lets you
    ... | Lucene, search |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| *Lucene in Action* | Lucene是一个强大的Java搜索库，它让你可以... | Lucene，搜索 |'
- en: A supervised learning algorithm is fed data like the kind shown in the table
    during what’s called a *training phase*. During the training phase, an ML algorithm
    crunches the training set (the set of training samples) and learns how to map,
    for example, input text to output categories. *What* an ML algorithm learns depends
    on the task it’s used for; in the example case, it’s being used for a *document
    categorization* task. *How* an ML algorithm learns depends on how the algorithm
    itself is built. There isn’t only one algorithm to perform ML; there are various
    subfields of ML, and many different algorithms exist for each of them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法在所谓的 *训练阶段* 接收表格中所示的数据。在训练阶段，机器学习算法处理训练集（训练样本集），并学习如何将输入文本映射到输出类别。机器学习算法学习的内容取决于它所使用的任务；在示例情况下，它被用于一个
    *文档分类* 任务。机器学习算法的学习方式取决于算法本身的构建方式。执行机器学习不仅仅有一个算法；机器学习有多个子领域，并且针对每个子领域都有许多不同的算法。
- en: '|  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: DL is just one way of doing ML, by using neural networks. But a vast number
    of alternatives are available when it comes to deciding which kind of neural network
    is best suited for a certain task. In the course of this book, we’ll mostly cover
    ML topics via DL. Occasionally, though, we’ll quickly cover other types of ML
    algorithms, mostly for the sake of comparison and reasoning for real-world scenarios.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）只是进行机器学习的一种方式，通过使用神经网络。但在决定哪种神经网络最适合特定任务时，有大量的替代方案。在这本书的过程中，我们将主要通过深度学习来覆盖机器学习主题。不过，偶尔我们也会快速介绍其他类型的机器学习算法，主要是为了比较和为现实场景中的推理服务。
- en: '|  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Once the training phase has completed, you’ll usually end up with a *machine
    learning model*. You can think of this as the artifact that captures what the
    algorithm has learned during training. This artifact is then used to perform *predictions*.
    A prediction is performed when a model is given a new input without any attached
    desired output, and you ask the model to tell you the correct output, based on
    what it’s been learning during the training phase. Note that you need to provide
    a lot of data for training (not hundreds, but at least tens of thousands of training
    samples) if you want to obtain good results when predicting outputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练阶段完成，你通常会得到一个 *机器学习模型*。你可以将其视为在训练过程中算法所学习内容的捕获物。然后，这个捕获物被用来进行 *预测*。当模型接收到一个没有附带期望输出的新输入时，你要求模型告诉你正确的输出，基于它在训练阶段所学习的内容，这时就会进行预测。请注意，如果你想在进行输出预测时获得良好的结果，你需要提供大量的训练数据（不仅仅是几百个，但至少要有成千上万的训练样本）。
- en: 'In the book categorization example, when given the following text, the model
    will extract categories such as “search” and “Lucene”:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在书籍分类的例子中，当给出以下文本时，模型将提取出“搜索”和“Lucene”等类别：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These are the opening words from *Lucene in Action, 2nd edition*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是《Lucene in Action，第2版》的开篇词。
- en: As I mentioned, the extracted categories can be used to place books belonging
    to the same category on the same bookshelf in a library. Are there other ways
    to accomplish this, without first providing a training set with book texts labeled
    by category? It would be helpful if you could find a way to measure the similarity
    between books so that you could place similar ones near to each other without
    caring too much about the exact naming of each book category. To do that without
    categories, you can use *unsupervised learning* techniques to cluster similar
    documents together. In unsupervised learning, as opposed to supervised learning,
    an ML algorithm looks at the data with no information about any expected output
    and extracts patterns and data representations during the *learning phase*. During
    *clustering*, each piece of input data—in this case, the text of a book—is transformed
    into a point that’s placed on a graph. During the training phase, a clustering
    algorithm places points in clusters, assuming that nearby points are semantically
    similar. After training has completed, books belonging to the same clusters can
    be picked up and placed on bookshelves accordingly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所述，提取的类别可以用来将属于同一类别的书籍放置在图书馆同一书架上的同一位置。有没有其他方法可以完成这个任务，而不需要首先提供一个带有类别标签的书籍文本的训练集？如果你能找到一种方法来衡量书籍之间的相似性，以便你可以将相似的书籍放置在一起，而不太关心每个书籍类别的确切名称，那将很有帮助。为了在没有类别的情况下做到这一点，你可以使用*无监督学习*技术将相似的文档聚在一起。在无监督学习中，与监督学习相反，机器学习算法在没有任何关于预期输出的信息的情况下查看数据，并在*学习阶段*中提取模式和数据处理表示。在*聚类*过程中，每条输入数据——在这种情况下，是书籍的文本——被转换成一个放置在图上的点。在训练阶段，聚类算法将点放置在簇中，假设相邻的点在语义上是相似的。训练完成后，属于同一簇的书籍可以被挑选出来并相应地放置在书架上。
- en: In this case, the output of unsupervised learning is the set of clusters with
    their assigned points. Just as before, such models can be used for predictions,
    such as, “What cluster does this new book/point belong to?”
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，无监督学习的输出是带有分配点的簇集。正如之前一样，这些模型可以用于预测，例如，“这本书/点属于哪个簇？”
- en: ML can help solve a lot of different problems, including categorizing books
    and grouping similar texts together. Until the early 2000s, several different
    techniques were used to achieve decent results when trying to address these kinds
    of tasks. Then DL became mainstream, not just in research labs of universities,
    but also in industry. Many ML problems were better resolved with DL, so DL became
    better known and more frequently used. The success and wide use of DL has resulted
    in extracting more-accurate book categories and more-accurate clustering, and
    many other improvements.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以帮助解决许多不同的问题，包括对书籍进行分类和将相似文本分组。直到2000年代初，在尝试解决这类任务时，使用了多种不同的技术来获得相当好的结果。然后深度学习成为主流，不仅是在大学的实验室中，而且在工业界也是如此。许多机器学习问题通过深度学习得到了更好的解决，因此深度学习变得更加知名和频繁使用。深度学习的成功和广泛应用导致了更准确的书籍类别提取和更准确的聚类，以及许多其他改进。
- en: 1.3\. What deep learning can do for search
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 深度学习对搜索能做什么
- en: When deep artificial neural networks are used to help solve search problems,
    this field is called neural search. In this book, you’ll get to know how neural
    networks are composed, how they work, and how they can be used in practice, all
    in the context of search engines.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度人工神经网络被用来帮助解决搜索问题时，这个领域被称为神经搜索。在这本书中，你将了解神经网络是如何组成的，它们是如何工作的，以及它们如何在搜索引擎的背景下被实际应用。
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Neural search**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经搜索**'
- en: The term *neural search* is a less academic form of the term *neural information
    retrieval*, which first appeared during a research workshop at the SIGIR 2016
    conference ([www.microsoft.com/en-us/research/event/neuir2016](http://www.microsoft.com/en-us/research/event/neuir2016))
    focused on applying deep neural networks to the field of information retrieval.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: “神经搜索”这个术语是“神经信息检索”这个更学术术语的非正式形式，后者首次出现在SIGIR 2016会议的一个研究研讨会上，该研讨会专注于将深度神经网络应用于信息检索领域（[www.microsoft.com/en-us/research/event/neuir2016](http://www.microsoft.com/en-us/research/event/neuir2016)）。
- en: '|  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'You might be wondering why we need neural search: after all, we already have
    good search engines on the web, and we often manage to find what we need. So what’s
    the value proposition of neural search?'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们需要神经搜索：毕竟，我们已经在网上有了很好的搜索引擎，我们通常能够找到我们需要的。那么神经搜索的价值主张是什么呢？
- en: 'Deep neural networks are good at the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络擅长以下方面：
- en: Providing a representation of textual data that captures word and document semantics,
    allowing a machine to say which words and documents are semantically similar.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供文本数据的表示，它捕捉了单词和文档的语义，使机器能够说出哪些单词和文档在语义上是相似的。
- en: 'Generating text that’s meaningful in a certain context: for example, useful
    for creating chatbots.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成在特定上下文中具有意义的文本：例如，用于创建聊天机器人。
- en: Providing representations of images that pertain not to the pixels but rather
    to their composing objects. This allows us to build efficient face/object-recognition
    systems.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供与像素无关的图像表示，而是关于其组成对象的表示。这使我们能够构建高效的面对面识别系统。
- en: Performing machine translation efficiently.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效地进行机器翻译。
- en: Under certain assumptions, approximating *any* function.^([[2](#ch01fn02)])
    There’s theoretically no limit to the kinds of tasks deep neural networks can
    achieve.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些假设下，可以近似**任何**函数.^([[2](#ch01fn02)]) 深度神经网络能够完成的任务种类在理论上没有限制。
- en: ²
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See Kurt Hornik, Maxwell Stinchcombe, and Halbert White, “Multilayer Feedforward
    Networks Are Universal Approximators,” *Neural Networks* 2, no. 5 (1989): 359-366,
    [http://mng.bz/Mxg8](http://mng.bz/Mxg8).'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见库尔特·霍尼克、马克斯韦尔·斯廷奇科姆和哈伯特·怀特，《多层前馈网络是通用逼近器》，*神经网络* 2，第5期（1989年）：359-366，[http://mng.bz/Mxg8](http://mng.bz/Mxg8)。
- en: 'This might sound a bit abstract, so let’s look at how these capabilities can
    be useful for you as a search engineer and/or a user. Think of the major struggle
    points when using search engines. Most likely you’ll experience concerns like
    these:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有点抽象，那么让我们看看这些能力如何对你作为一个搜索工程师和/或用户有用。想想使用搜索引擎时的主要痛点。很可能会遇到以下这些担忧：
- en: 'I didn’t get good results: I found somewhat related documents, but not the
    one I was looking for.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有得到好的结果：我发现了一些相关文档，但不是我要找的那个。
- en: It took me too much time to find the information I was looking for (and then
    I gave up).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到我想要的信息花了我太多时间（然后我就放弃了）。
- en: I had to read through some of the provided results before getting a good understanding
    of the topic I wanted to learn about.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对想要学习的话题有一个良好的理解之前，我不得不阅读一些提供的结果。
- en: I was looking for content in my native language, but I could find good search
    results only in English.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在寻找的是用我的母语的内容，但只能找到英文的搜索结果。
- en: I was looking for a certain image I had once seen on a website, but I couldn’t
    find it again.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在寻找一张我曾经在网站上看到过的特定图像，但找不到它了。
- en: Such problems are common, and various solutions exist to mitigate each of them.
    But the exciting thing is that deep neural networks, if tailored properly, can
    help in all these cases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是常见的，并且存在各种解决方案来减轻每一个问题。但令人兴奋的是，如果深度神经网络被适当定制，可以在所有这些情况下提供帮助。
- en: With the help of DL algorithms, a search engine is able to
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习算法的帮助下，搜索引擎能够
- en: Provide more-relevant results to its end users, increasing user satisfaction.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为其最终用户提供更相关的结果，提高用户满意度。
- en: Search through binary content like images the same way we search text. Think
    of this as being able to search for an image with the phrase “picture of a leopard
    hunting an impala” (and you’re not Google).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像搜索文本一样搜索二进制内容，比如图像。想象一下，能够搜索包含短语“猎豹捕食羚羊的图片”的图像（而你并不是谷歌）。
- en: Serve content to users speaking different languages, allowing more users to
    access the data in the search system.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为讲不同语言的用户提供服务，让更多用户能够访问搜索系统中的数据。
- en: Generally become more sensitive to the data it serves, which means less chance
    for queries that give no results.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常对它提供的数据更加敏感，这意味着查询无结果的可能性更小。
- en: If you’ve ever worked on designing, implementing, or configuring a search engine,
    you’ve surely faced the problem of obtaining a solution that adapts to your data.
    DL will help a lot in providing solutions to these problems that are accurately
    based on your data, not on fixed rules or algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经参与过设计、实施或配置搜索引擎，你肯定遇到过获得适应你数据解决方案的问题。深度学习将大量帮助提供基于你数据的准确解决方案，而不是基于固定规则或算法。
- en: 'The quality of search results is crucial for end users. There’s one thing a
    search engine should do well: find out which of the possibly matching search results
    would be most useful for a specific user’s information needs. Well-ranked search
    results allow users to find important results more easily and quickly; that’s
    why we put a lot of emphasis on the topic of *relevant results*. In real life,
    this can make a huge difference. According to an article published in *Forbes*,
    “By providing better search results, Netflix estimates that it is avoiding canceled
    subscriptions that would reduce its revenue by $1B annually.”^([[3](#ch01fn03)])
    Deep neural networks can help by automatically tweaking the end user query under
    the hood based on past user queries or based on the search engine contents.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索结果的质量对最终用户至关重要。搜索引擎应该擅长做的一件事是：找出哪些可能匹配的搜索结果对特定用户的信息需求最有用。排名靠前的搜索结果使用户能够更容易、更快地找到重要的结果；这就是为什么我们非常重视**相关结果**这一主题。在现实生活中，这可以产生巨大的差异。根据《福布斯》杂志发表的一篇文章，“通过提供更好的搜索结果，Netflix估计它避免了每年减少10亿美元收入的取消订阅。”^([[3](#ch01fn03)])
    深度神经网络可以通过根据过去的用户查询或根据搜索引擎内容自动调整最终用户查询来提供帮助。
- en: ³
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Louis Columbus, “McKinsey’s State of Machine Learning and AI, 2017,” July 9,
    2017, [http://mng.bz/a7KX](http://mng.bz/a7KX).
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Louis Columbus，“麦肯锡2017年机器学习和人工智能状况”，2017年7月9日，[http://mng.bz/a7KX](http://mng.bz/a7KX)。
- en: People today are used to working with web search engines to retrieve images.
    If you search for “pictures of angry lions” on Google, for instance, you’ll get
    strongly relevant images. Before the advent of DL, such images had to be decorated
    with *metadata* (data about data) describing their contents before being put into
    the search engine. And that metadata usually had to be typed by a human. Deep
    neural networks can abstract a representation of an image that captures what’s
    in there so that no human intervention is required to put an image description
    in the search engine.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的人们习惯于使用网络搜索引擎来检索图像。例如，如果你在谷歌上搜索“愤怒的狮子图片”，你会得到高度相关的图像。在深度学习出现之前，这样的图像在放入搜索引擎之前必须用描述其内容的**元数据**（关于数据的数据）进行装饰。而且这些元数据通常需要由人工输入。深度神经网络可以抽象出图像的表示，捕捉其中的内容，这样就不需要人工干预来在搜索引擎中放入图像描述。
- en: For scenarios like web search (searching over all the websites on the internet),
    users can come from all over the world, so it’s best if they can search in their
    native languages. Additionally, the search engine could pick user profiles and
    return results in their language even if they search in English; this is a common
    scenario for tech queries, because lots of content is produced in English. An
    interesting application of deep neural networks is called *neural machine translation*,
    a set of techniques that use deep neural networks to translate a piece of text
    from a source language into another target language.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像网络搜索（在整个互联网上的所有网站上搜索）这样的场景，用户可以来自世界各地，因此如果他们能够用母语进行搜索那就最好不过了。此外，搜索引擎可以挑选用户资料，即使他们用英语搜索也能返回他们语言的结果；这在技术查询中是一个常见的场景，因为大量的内容是用英语生产的。深度神经网络的一个有趣应用被称为**神经机器翻译**，它是一套使用深度神经网络将一段文本从源语言翻译成另一种目标语言的技巧。
- en: Also exciting is the possibility of using deep neural networks to change the
    way search engines return relevant information to end users. Most commonly, a
    search engine will give a list of search results in response to a search query.
    DL techniques can be used to let the search engine return a single piece of text
    that should give all the information needed by a user.^([[4](#ch01fn04)]) This
    would save users from looking at each and every result to get all the knowledge
    they required. We could even aggregate all of these ideas and build a search engine
    serving both text and images seamlessly to users from all over the world, which,
    instead of returning search results, returns the single piece of text or image
    the user needs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个令人兴奋的可能性是使用深度神经网络改变搜索引擎向最终用户返回相关信息的方式。最常见的情况是，搜索引擎会针对搜索查询提供一个搜索结果列表。深度学习技术可以用来让搜索引擎返回一段应该包含用户所需所有信息的单一文本。这将使用户不必查看每个结果就能获取所有所需的知识。我们甚至可以汇总所有这些想法，构建一个无缝服务于来自世界各地的用户的搜索引擎，它不仅返回搜索结果，还返回用户需要的单一文本或图像。
- en: ⁴
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Christina Lioma et al., “Deep Learning Relevance: Creating Relevant Information
    (As Opposed to Retrieving It),” June 27, 2016, [https://arxiv.org/pdf/1606.07660.pdf](https://arxiv.org/pdf/1606.07660.pdf).'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Christina Lioma 等人，“深度学习相关性：创建相关信息（而不是检索它）”，2016年6月27日，[https://arxiv.org/pdf/1606.07660.pdf](https://arxiv.org/pdf/1606.07660.pdf)。
- en: These applications are all examples of *neural search*. As you can imagine,
    they have the potential to revolutionize the way we work and use search engines
    today.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用都是*神经搜索*的例子。正如你可以想象的那样，它们有可能彻底改变我们今天工作和使用搜索引擎的方式。
- en: There are many possibilities for how computers can help people obtain the information
    they need. Neural networks have been discussed for the past few years, but only
    recently have they become so popular; that’s because researchers have discovered
    how to make them much more effective than they used to be. In the early 2000s,
    for example, adding the help of more-powerful computers was a key advance. To
    take advantage of all the potential of deep neural networks, people interested
    in computer science—especially in the fields of natural language processing, computer
    vision, and informational retrieval—will need to know how such artificial neural
    networks work in practice.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机如何帮助人们获取所需信息的可能性有很多。神经网络在过去几年中一直被讨论，但直到最近才变得如此流行；这是因为研究人员发现了如何使它们比以前更有效。例如，在2000年代初，借助更强大的计算机是一项关键进步。为了充分利用深度神经网络的所有潜力，对计算机科学感兴趣的人——特别是在自然语言处理、计算机视觉和信息检索等领域——需要了解这些人工神经网络在实际中的工作方式。
- en: 'This book is intended for people interested in building smart search engines
    with the help of DL. This doesn’t necessarily mean you’re going to build the next
    Google search. It could mean making use of what you learn here to design and implement
    an efficient, effective search engine for your company, or expanding your knowledge
    base to apply DL techniques in larger projects that may include web search engines.
    The goal here is to enrich your skill set around search engines and DL, because
    these skills can be useful in numerous contexts. For example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为那些希望通过深度学习构建智能搜索引擎的人所写。这并不一定意味着你将构建下一个谷歌搜索引擎。这可能意味着利用你在这里学到的知识来设计和实现一个高效、有效的搜索引擎，用于你的公司，或者扩大你的知识库，将深度学习技术应用于可能包括网络搜索引擎在内的更大项目。这里的目的是丰富你在搜索引擎和深度学习方面的技能集，因为这些技能在许多情况下都可能是有用的。例如：
- en: Training a deep neural network to learn to recognize objects in images and use
    what the neural network has learned when searching for images
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度神经网络以学习在图像中识别对象，并在搜索图像时使用神经网络所学的知识
- en: Using neural networks to populate a “related content” bar in a search engine’s
    search results list
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络来填充搜索引擎搜索结果列表中的“相关内容”栏
- en: Training neural networks to learn to make the user query more specific (fewer
    but better search results) or broader (more search results, even if some may be
    less relevant)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络以学习使用户查询更具体（更少但更好的搜索结果）或更广泛（更多搜索结果，即使其中一些可能不太相关）
- en: 1.4\. A roadmap for learning deep learning
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4. 深度学习的路线图
- en: We’ll run our neural search examples on top of open source software written
    in Java with the help of Apache Lucene ([http://lucene.apache.org](http://lucene.apache.org)),
    an information retrieval library; and Deeplearning4j ([http://deeplearning4j.org](http://deeplearning4j.org)),
    a DL library. But we’ll focus as much as possible on principles rather than implementation,
    in order to make sure the techniques explained in this book can be applied with
    different technologies and/or scenarios. At the time of writing, Deeplearning4j
    is a widely used framework for DL in the enterprise communities; it’s part of
    the Eclipse Foundation. It also has good adoption because of integration with
    popular big data frameworks like Apache Spark. Full source code accompanying this
    book can be found at [www.manning.com/books/deep-learning-for-search](http://www.manning.com/books/deep-learning-for-search)
    and on GitHub at [https://github.com/dl4s/dl4s](https://github.com/dl4s/dl4s).
    Other DL frameworks exist, though; for example, TensorFlow (from Google) is popular
    among the Python and research communities. Almost every day, new tools are invented,
    so I decided to focus on a relatively easy-to-use DL framework that can be easily
    integrated with Lucene, which is one of the most widely adopted search libraries
    for the JVM. If you’re working with Python, you can find TensorFlow implementations
    of most of the DL code used in this book together with some instruction on GitHub
    at [https://github.com/dl4s/pydl4s](https://github.com/dl4s/pydl4s).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Apache Lucene（[http://lucene.apache.org](http://lucene.apache.org)），一个信息检索库，以及Deeplearning4j（[http://deeplearning4j.org](http://deeplearning4j.org)），一个深度学习库的帮助下，运行我们的神经网络搜索示例。但我们将尽可能关注原理而不是实现，以确保本书中解释的技术可以应用于不同的技术和/或场景。在撰写本书时，Deeplearning4j是企业社区中广泛使用的深度学习框架的一部分，它是Eclipse基金会的成员。它还因为与Apache
    Spark等流行的大数据框架的集成而得到了良好的采用。本书的完整源代码可以在[www.manning.com/books/deep-learning-for-search](http://www.manning.com/books/deep-learning-for-search)和GitHub上的[https://github.com/dl4s/dl4s](https://github.com/dl4s/dl4s)找到。尽管存在其他深度学习框架；例如，TensorFlow（来自谷歌）在Python和研究社区中很受欢迎。几乎每天都有新的工具被发明，所以我决定专注于一个相对容易使用且可以轻松与Lucene集成的深度学习框架，Lucene是最广泛采用的JVM搜索库之一。如果你使用Python，你可以在GitHub上找到本书中使用的大多数深度学习代码的实现，以及一些说明，网址为[https://github.com/dl4s/pydl4s](https://github.com/dl4s/pydl4s)。
- en: 'While planning this book, I decided to present chapters in a kind of ascending
    level of difficulty, so each chapter will teach a certain application of neural
    networks to a specific search problem, supported by well-known algorithms. We’ll
    keep an eye on state-of-the-art DL algorithms, but we’re also quietly conscious
    that we can’t cover everything. The aim is to provide good baselines that can
    be easily extended if a new and better neural network-based algorithm comes out
    next week. Key things we’ll improve with the help of deep neural networks are
    relevance, query understanding, image search, machine translation, and document
    recommendation. Don’t worry if you don’t know about any of these: I’ll introduce
    such tasks as they exist without any DL technique and then show when and how DL
    can help.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划这本书的过程中，我决定以某种递增的难度级别来呈现章节，这样每个章节都将教授神经网络在特定搜索问题上的某种应用，并辅以知名算法。我们将关注最先进的深度学习算法，但我们也在默默地意识到我们无法涵盖一切。目标是提供良好的基准，如果下周出现新的、更好的基于神经网络的算法，可以轻松扩展。我们将借助深度神经网络改进的关键事项包括相关性、查询理解、图像搜索、机器翻译和文档推荐。如果你对这些内容不熟悉，不要担心：我会介绍这些任务，就像它们存在时没有使用任何深度学习技术一样，然后展示何时以及如何使用深度学习来帮助。
- en: In [part 1](kindle_split_011.xhtml#part01) of this book, I’ll present an overview
    of how neural networks can help to improve search engines in general. I’ll do
    this first with an application where neural networks help the search engine build
    multiple versions of the same query by generating synonyms. In [part 2](kindle_split_014.xhtml#part02)
    of the book, we’ll mostly examine DL-based techniques to make search queries more
    expressive. This improved expressiveness will make the queries better fit user
    intent, and thus make the search engine return better (more relevant) results.
    Finally, in [part 3](kindle_split_019.xhtml#part03) of the book, we’ll work on
    more-complex things like searching over multiple languages and searching for images,
    and finally address performance aspects of neural search systems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的[第1部分](kindle_split_011.xhtml#part01)中，我将概述神经网络如何帮助改进搜索引擎。我将首先通过一个应用来做到这一点，其中神经网络帮助搜索引擎通过生成同义词构建相同查询的多个版本。在本书的[第2部分](kindle_split_014.xhtml#part02)中，我们将主要检查基于深度学习的技术，以使搜索查询更具表现力。这种改进的表现力将使查询更好地符合用户意图，从而使搜索引擎返回更好的（更相关的）结果。最后，在本书的[第3部分](kindle_split_019.xhtml#part03)中，我们将处理更复杂的事情，如跨多种语言搜索和图像搜索，并最终解决神经网络搜索系统的性能方面。
- en: Along the way, we’ll also pause to consider accuracy and how to measure the
    final results when we apply neural search. Without numbers constantly demonstrating
    what we think is good, we won’t go far. We need to measure how good our systems
    are, with and without fancy neural nets.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用神经网络搜索的过程中，我们也会停下来思考准确性和如何衡量最终结果。如果没有数字不断证明我们认为什么是好的，我们就无法走得太远。我们需要衡量我们的系统是好是坏，无论是带有还是不带复杂的神经网络。
- en: In this chapter, we’ll start with a look at the problems search engines try
    to solve and the most common techniques used to solve them. This survey will introduce
    you to the basics of how text is analyzed, ingested, and retrieved within a search
    engine, so you’ll get to know how queries hit search results, as well as some
    fundamentals of solving the problem of returning relevant results first. We’ll
    also uncover some weaknesses inherent in common search techniques, which sets
    up a discussion of what DL can be used for in the context of search. Then we’ll
    look at which tasks DL can help to solve and what the practical implications are
    of its applications in the search field. This will help paint a realistic picture
    of what you can and can’t expect from neural search in real-life scenarios.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从了解搜索引擎试图解决的问题和解决这些问题的最常用技术开始。这项调查将向你介绍搜索引擎内部文本分析、摄取和检索的基本原理，这样你就能了解查询如何触达搜索结果，以及解决首先返回相关结果的一些基本原理。我们还将揭示常见搜索技术中固有的某些弱点，这为讨论在搜索背景下深度学习可以用于什么提供了基础。然后，我们将探讨深度学习可以帮助解决哪些任务以及其在搜索领域的应用的实际影响。这将帮助你描绘出一个现实的情况，让你了解在现实场景中可以从神经网络搜索中期待什么，以及不能期待什么。
- en: 1.5\. Retrieving useful information
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5. 获取有用信息
- en: Let’s start by learning how to retrieve search results that are relevant to
    users’ needs. This will give you the search fundamentals you need to understand
    how deep neural networks can help build innovative search platforms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先学习如何检索与用户需求相关的搜索结果。这将为你提供理解深度神经网络如何帮助构建创新搜索平台所需的搜索基础。
- en: 'First question: what is a search engine? It’s a system, a program running on
    a computer, that people can use to retrieve information. The main value of a search
    engine is that whereas it ingests “data,” it’s expected to provide “information.”
    This goal means the search engine should do its best to make sense of the data
    it gets in order to provide something that can be easily consumed by its users.
    As users, we rarely need lots of data about a certain topic; we’re often looking
    for a specific piece of information and would be satisfied with just *one* answer,
    not hundreds or thousands of results to inspect.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题：什么是搜索引擎？它是一个系统，一个在计算机上运行的程序，人们可以使用它来检索信息。搜索引擎的主要价值在于，虽然它摄取“数据”，但它预期提供“信息”。这个目标意味着搜索引擎应该尽其所能理解它获得的数据，以便为用户提供易于消费的东西。作为用户，我们很少需要大量关于某个主题的数据；我们通常在寻找特定的信息，并且对只有一个答案就感到满意，而不是检查成百上千的结果。
- en: When it comes to search engines, most people tend to think to Google, Bing,
    Baidu, and other large, popular search engines that provide access to huge amounts
    of information coming from a lot of diverse sources. But there are also many smaller
    search engines that focus on content from a specific domain or topic. These are
    often called *vertical search engines* because they work on a constrained set
    of document types or topics, rather than the entire set of content that is online
    nowadays. Vertical search engines play an important role, too, because they’re
    often able to provide more-precise results about “their” data—because they’ve
    been tailored to that specific content. They often allow us to retrieve more-fine-grained
    results with higher accuracy (think of searching for an academic article on Google
    versus using Google Scholar). (For now, we won’t go into the details of what *accuracy*
    means; here, I’m talking about the general concept of the accuracy of an answer
    to an inquiry. But accuracy is also the name of a well-defined measure used to
    evaluate how good and precise an information retrieval system’s results are.)
    We’ll make no distinction at this point about the size of the data and user base,
    because all the concepts that follow apply to most of the existing search engines
    no matter how big or small.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当提到搜索引擎时，大多数人倾向于想到谷歌、必应、百度和其他大型、流行的搜索引擎，它们提供大量来自众多不同来源的信息访问。但也有一些较小的搜索引擎专注于特定领域或主题的内容。这些通常被称为
    *垂直搜索引擎*，因为它们在受限制的文档类型或主题集上工作，而不是如今在线的整个内容集。垂直搜索引擎也扮演着重要的角色，因为它们通常能够提供关于“他们”数据的更精确结果——因为它们已经针对特定内容进行了定制。它们通常允许我们以更高的准确性检索更精细的结果（比如在谷歌上搜索学术论文与使用谷歌学术搜索相比）。（目前，我们不会深入探讨*准确性*的含义；在这里，我谈论的是关于查询答案准确性的通用概念。但准确性也是一个用于评估信息检索系统结果好坏和精确度的明确度量名称。）在此阶段，我们不会区分数据量和用户基础的大小，因为以下所有概念都适用于大多数现有的搜索引擎，无论大小。
- en: 'Key responsibilities of a search engine usually involve the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎的关键职责通常包括以下内容：
- en: '***Indexing—*** Ingesting and storing data efficiently so that it can be retrieved
    quickly'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***索引——*** 高效地摄取和存储数据，以便可以快速检索'
- en: '***Querying—*** Providing retrieval functionality so that search can be performed
    by an end user'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***查询——*** 提供检索功能，以便最终用户可以进行搜索'
- en: '***Ranking—*** Presenting and ranking the results according to certain metrics
    to best satisfy users’ information needs'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***排名——*** 根据某些指标展示和排名结果，以最好地满足用户的信息需求'
- en: A key point in practice is also *efficiency*. If it takes too much time to get
    the information you’re looking for, it’s likely you’ll switch to another search
    engine next time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中的一个关键点是 *效率*。如果你花费太多时间去寻找你想要的信息，那么你很可能在下一次会切换到另一个搜索引擎。
- en: But how does a search engine work with pages, books, and similar kinds of text?
    In the following sections, you’ll get to know
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但搜索引擎是如何处理网页、书籍和类似文本的呢？在接下来的章节中，你将了解到
- en: How big chunks of text are split into smaller pieces for the search engine to
    take a given query and quickly retrieve a document
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将大块文本分割成更小的部分，以便搜索引擎可以接受特定的查询并快速检索文档
- en: Basics of how to capture the importance and relevance of search results, for
    a particular query
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获特定查询的搜索结果重要性和相关性的基础知识
- en: Let’s start with the fundamentals of information retrieval (indexing, querying,
    and ranking). Before diving into that, you need to understand how big streams
    of texts end up in a search engine; this is important, because it impacts the
    search engine’s capabilities of searching fast and providing sensitive results.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从信息检索的基础（索引、查询和排名）开始。在深入探讨之前，你需要了解大量文本是如何进入搜索引擎的；这很重要，因为它影响搜索引擎快速搜索和提供敏感结果的能力。
- en: 1.5.1\. Text, tokens, terms, and search fundamentals
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.5.1\. 文本、标记、术语和搜索基础
- en: Put yourself in the shoes of the librarian, who has just received an inquiry
    for books related to a certain topic. How would you be able to say that one book
    contains information about a certain topic? How would you know if a book even
    contains a certain word?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你是图书管理员，刚刚收到关于某个特定主题的书籍查询。你如何能够说一本书包含关于某个主题的信息？你如何知道一本书是否包含某个特定的词？
- en: Extracting the categories a certain book belongs to (high-level topics like
    “AI” and “DL”) is different from extracting all the words contained in the book.
    For example, categories make searching for a book about AI easier for a newbie,
    because no prior knowledge of AI-specific techniques or authors is required. A
    user will go to the search engine website and start browsing between the existing
    categories and look for something that’s close enough to the topic of AI. On the
    other hand, for an AI expert, knowing whether a book contains the words *gradient
    descent* or *backpropagation* allows results to be found that contain finer-grained
    information about certain techniques or problems in the field of AI.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 提取一本书所属的类别（如“AI”和“DL”这类高级主题）与提取书中包含的所有单词是不同的。例如，类别使得对于AI新手来说，搜索关于AI的书籍变得更加容易，因为不需要对AI特定技术或作者有任何先验知识。用户会访问搜索引擎网站，并在现有的类别之间浏览，寻找与AI主题足够接近的内容。另一方面，对于AI专家来说，知道一本书是否包含“梯度下降”或“反向传播”这样的单词，可以找到包含关于AI领域某些技术或问题的更细粒度信息的搜索结果。
- en: Humans generally have a hard time remembering all the words contained in a book,
    although we can easily tell a book’s topic by reading a few paragraphs from the
    book or even from looking at the preface or foreword. Computers tend to behave
    the opposite way. They can easily store large amounts of text and “remember” all
    the words contained in millions of pages so that they can be used while searching;
    on the other hand, they aren’t so good at extracting information that’s implicit,
    scattered, or not directly formulated in a given piece of text, such as which
    category a book belongs to. For example, a book about neural networks may never
    mention “artificial intelligence” (although it would probably refer to ML). But
    it would still belong to the broad category of “books about artificial intelligence.”
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通常很难记住一本书中包含的所有单词，尽管我们可以通过阅读书中的一些段落，甚至只看前言或序言，就轻易地判断出这本书的主题。而计算机则往往表现出相反的行为。它们可以轻松地存储大量文本，并“记住”数百万页中包含的所有单词，以便在搜索时使用；另一方面，它们在提取隐含的、分散的或未直接在给定文本中表述的信息方面并不擅长，例如一本书属于哪个类别。例如，一本关于神经网络的书可能永远不会提到“人工智能”（尽管它可能提到机器学习）。但它在“关于人工智能的书籍”这一广泛类别中仍然属于。
- en: 'Let’s first look at the task computers can do well already: extracting and
    storing text fragments (also known as *terms*) from streams of text. You can think
    of this process, called *text analysis*, as breaking down the text of a book into
    all of its constituent words. Imagine having a tape on which the contents of a
    book are written in a stream, and a machine (the text analysis algorithm) into
    which you insert such tape as input. You receive many pieces of such tape as output,
    and each of those output pieces contains a word or a sentence or a noun phrase
    (for example, “artificial intelligence”); you may realize that some of the words
    written on the input tape have been eaten by the machine and not outputted in
    any form.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看计算机已经能够很好地完成的任务：从文本流中提取和存储文本片段（也称为*术语*）。你可以将这个过程称为*文本分析*，即将一本书的文本分解成所有构成单词。想象一下，有一卷带子，上面写着这本书的内容，而你将这卷带子作为输入插入到一个机器（文本分析算法）中。你收到了许多这样的带子作为输出，每个输出片段都包含一个单词、一个句子或一个名词短语（例如，“人工智能”）；你可能意识到输入带子上写的一些单词被机器吃掉了，并且没有以任何形式输出。
- en: Because the final units to be created by the text analysis algorithm might be
    words but also might be group of words or sentences, or even portions of words,
    we refer to these fragments as terms. You can think of a term as the fundamental
    unit that a search engine uses to store data and, consequently, retrieve it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本分析算法最终要创建的单元可能是单词，也可能是单词组或句子，甚至可能是单词的部分，我们将这些片段称为术语。你可以将术语视为搜索引擎用来存储数据并因此检索数据的基本单元。
- en: 'That’s the basis of one of the most fundamental forms of search, *keyword search*:
    a user types a set of words and expects the search engine to return all the documents
    that contain some or all of the terms. This is how web search started decades
    ago. Although many search engines today are much smarter, many users keep composing
    queries based on the keywords they expect the search results to contain. This
    is what you’ll learn now: how the text entered by a user into a search box makes
    the search engine return results. A *query* is what we call the text the user
    enters in order to search for something. Although a query is just text, it conveys
    and encodes what the user needs and how the user expresses this possibly general
    or abstract need (for example, “I want to learn about the latest and greatest
    research in the field of artificial intelligence”) in a way that’s concise but
    still descriptive (for example, “latest research in ai,” as in [figure 1.3](#ch01fig03)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最基本的搜索形式之一的基础，*关键词搜索*：用户输入一组单词，并期望搜索引擎返回包含这些单词中的一些或全部的文档。这就是几十年前网络搜索开始的方式。尽管今天许多搜索引擎都变得更加智能，但许多用户仍然基于他们期望搜索结果中包含的关键词来编写查询。你现在将要学习的是：用户输入到搜索框中的文本是如何让搜索引擎返回结果的。*查询*是我们对用户为了搜索某物而输入的文本的称呼。尽管查询只是文本，但它传达并编码了用户的需求以及用户如何以简洁但仍然描述性的方式表达这种可能是一般或抽象的需求（例如，“我想了解人工智能领域的最新和最伟大的研究”），（例如，“最新的人工智能研究”，如[图1.3](#ch01fig03)所示）。
- en: Figure 1.3\. Searching and getting results
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3\. 搜索和获取结果
- en: '![](Images/01fig03_alt.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](Images/01fig03_alt.jpg)'
- en: 'If, as a user, you want to find documents that contain the word “search,” how
    would the search engine return such documents? A not-so-smart way of doing that
    could be to go over each document’s content from the beginning and scan it until
    the search engine finds a match. But it would be very expensive to perform such
    text scans for each query, especially with many large documents:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果作为用户，你想找到包含单词“搜索”的文档，搜索引擎会如何返回这样的文档？一种不太智能的方法可能是从每个文档的内容开始，扫描它，直到搜索引擎找到匹配项。但为每个查询执行这样的文本扫描将非常昂贵，尤其是对于许多大型文档：
- en: Many documents may not contain the word “search”; therefore, it would be a waste
    of computation resources to scan through them.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多文档可能不包含单词“搜索”；因此，扫描这些文档将是计算资源的浪费。
- en: Even if a document contains the word “search,” this word may occur toward the
    end of the document, requiring the search engine to “read” through all the preceding
    words before finding a match for “search.”
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使文档包含单词“搜索”，这个单词可能出现在文档的末尾，需要搜索引擎“阅读”所有前面的单词，才能找到“搜索”的匹配项。
- en: You have a *match* or a *hit* when one or more *terms* that are part of a query
    are found in a search result.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询中的一个或多个*术语*在搜索结果中找到时，你就有了一个*匹配*或*命中*。
- en: 'You need to find a way to compute this retrieval phase quickly. One fundamental
    method to accomplish that is to break down sentences like “I like search engines”
    into smaller units: in this case, [“I”, “like”, “search”, “engines”]. This is
    a prerequisite for efficient storage mechanisms called *inverted indexes*, which
    we’ll cover in the next section. A text analysis program is often organized as
    a pipeline: a chain of components, each of which takes the previous component’s
    output as its input. Such pipelines are usually composed of building blocks of
    two types:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找到一种快速计算检索阶段的方法。实现这一目标的一个基本方法是将像“我喜欢搜索引擎”这样的句子分解成更小的单元：在这种情况下，[“我”，“喜欢”，“搜索”，“引擎”]。这是高效存储机制*倒排索引*的前提条件，我们将在下一节中介绍。文本分析程序通常组织成一个管道：一系列组件，每个组件都以前一个组件的输出作为其输入。这样的管道通常由两种类型的构建块组成：
- en: '***Tokenizers—*** Components that break a stream of text into words, phrases,
    symbols, or other units called *tokens*'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分词器—*** 将文本流分解成单词、短语、符号或其他称为*标记*的单元的组件'
- en: '***Token filters—*** Components that accept a stream of tokens (from a tokenizer
    or another filter) and can modify, delete, or add new tokens'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***标记过滤器—*** 接受来自分词器或另一个过滤器的标记流，并可以修改、删除或添加新标记的组件'
- en: The output of such text analysis pipelines is a sequence of consecutive terms,
    as shown in [figure 1.4](#ch01fig04).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文本分析管道的输出是一系列连续的术语，如图[图1.4](#ch01fig04)所示。
- en: Figure 1.4\. Getting the words of “I like search engines” using a simple text
    analysis pipeline
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4\. 使用简单的文本分析管道获取“我喜欢搜索引擎”的单词
- en: '![](Images/01fig04_alt.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](Images/01fig04_alt.jpg)'
- en: 'You now know that text analysis is useful for performance reasons to build
    fast search engines. Another equally important aspect is that it controls how
    queries and the text to be put into the index match. Often, text analysis pipelines
    are used to filter some tokens that aren’t considered useful or needed for the
    search engine. For example, a common practice is to avoid storing common terms
    like articles or prepositions in the search engine, because those words exist
    in most text documents in languages like English, and you usually don’t want a
    query to return everything in the search engine: that wouldn’t give much value
    to the user. In such cases, you can create a token filter responsible for removing
    tokens like “the,” “a,” “an,” “of,” “in,” and so on, while letting all the other
    tokens flow out as the tokenizer produces them. In this simplistic example,'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道，出于性能原因，文本分析对于构建快速搜索引擎是有用的。另一个同样重要的方面是它控制了查询和要放入索引中的文本如何匹配。通常，文本分析管道被用来过滤一些被认为对搜索引擎无用的或不需要的令牌。例如，常见的做法是避免在搜索引擎中存储像冠词或介词这样的常见术语，因为这些词在像英语这样的语言的大多数文本文档中都存在，而且你通常不希望查询返回搜索引擎中的所有内容：这不会给用户带来太多价值。在这种情况下，你可以创建一个负责移除“the”、“a”、“an”、“of”、“in”等令牌的令牌过滤器，同时让所有其他令牌随着分词器产生它们时流出。在这个简单的例子中，
- en: The tokenizer will split tokens every time it encounters a whitespace character.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器每次遇到空白字符时都会分割令牌。
- en: The token filter will remove tokens that match a certain blacklist (also known
    as a *stopword list*).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令牌过滤器会移除与特定黑名单（也称为*停用词列表*）匹配的令牌。
- en: In real life, it’s common, especially when setting up a search engine for the
    first time, to build several different text analysis algorithms and try them on
    the data you want to put into the search engine. This allows you to visualize
    how content will be handled by such algorithms, such as which tokens are generated,
    which ones eventually are filtered out, and so on. You’ve built this text analysis
    chain (also called an *analyzer*) and want to make sure it works as expected and
    filters articles, prepositions, and so forth. Let’s try to pass a first piece
    of text to the simplistic analyzer and submit the sentence “the brown fox jumped
    over the lazy dog” to the pipeline; you expect articles to be removed. The generated
    output stream will look like [figure 1.5](#ch01fig05).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，特别是在第一次设置搜索引擎时，构建几个不同的文本分析算法并在你想要放入搜索引擎的数据上尝试它们是很常见的。这让你能够可视化这些算法将如何处理内容，例如哪些令牌被生成，哪些最终被过滤掉，等等。你已经构建了这个文本分析链（也称为*分析器*），并想确保它按预期工作并过滤掉冠词、介词等。让我们尝试将第一段文本传递给这个简单的分析器，并将句子“the
    brown fox jumped over the lazy dog”提交到管道中；你期望移除冠词。生成的输出流将类似于[图1.5](#ch01fig05)。
- en: Figure 1.5\. The traversed token graph
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5. 已遍历的令牌图
- en: '![](Images/01fig05_alt.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片1.5](Images/01fig05_alt.jpg)'
- en: The resulting token stream has “the” tokens removed, as expected; you can see
    that from the dotted arrows at the start of the graph and between the nodes “over”
    and “lazy.” The numbers beside the tokens represent the starting and ending positions
    (in number of characters) of each token. The important bit of this example is
    that a query for “the” won’t match, because the analyzer has removed all such
    tokens, and they won’t end up being part of the search engine contents. In real
    life, text analysis pipelines are often more complex; you’ll see some of them
    in the following chapters. Now that you’ve learned about text analysis, let’s
    see how search engines store the text (and terms) to be queried by end users.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，结果令牌流中移除了“the”令牌；你可以从图的最开始和“over”和“lazy”节点之间的虚线箭头中看到这一点。令牌旁边的数字代表每个令牌的起始和结束位置（以字符数计）。这个例子中的重要部分是，对于“the”的查询不会匹配，因为分析器已经移除了所有这样的令牌，它们最终不会成为搜索引擎内容的一部分。在现实生活中，文本分析管道通常更复杂；你将在接下来的章节中看到一些。现在你已经了解了文本分析，让我们看看搜索引擎如何存储最终用户要查询的文本（和术语）。
- en: Indexing
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 索引
- en: 'Although the search engine needs to split text into terms for the sake of fast
    retrieval, end users expect search results to be in the form of a single unit:
    a document. Think about search results from Google. If you search for “books,”
    you’ll receive a list of results, each composed of a title, a link, a text snippet
    of the result, and so on. Each of those results contains the term “books,” but
    what’s shown is a document that has lot more information and context than just
    the text snippet where the term matched. In practice, tokens resulting from text
    analysis are stored with a reference to the original piece of text they belong
    to.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管搜索引擎需要将文本拆分为术语以实现快速检索，但最终用户期望搜索结果以单个单元的形式呈现：一个文档。想想Google的搜索结果。如果你搜索“书籍”，你会收到一个由标题、链接、结果文本片段等组成的列表。每个结果都包含术语“书籍”，但显示的是一个包含比仅匹配文本片段更多信息和上下文的文档。在实践中，从文本分析中产生的标记会与它们所属的原始文本片段一起存储。
- en: This link between a term and a document makes it possible to
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种术语与文档之间的联系使得
- en: Match a keyword or search term from a query
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配查询中的关键字或搜索术语
- en: Return the referenced original text as a search result
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回引用的原始文本作为搜索结果
- en: This whole process of analyzing streams of text and storing the resulting terms
    (along with their referenced documents) in the search engine is usually referred
    to as *indexing*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 分析文本流并将结果术语（及其引用的文档）存储在搜索引擎中的整个过程通常被称为*索引*。
- en: 'The reason for such wording is that terms are stored in an *inverted index*:
    a data structure that maps a term into the text that originally contained it.
    Probably the easiest way to look at it is as the analytic index of an actual book,
    where each word entry points to the pages where it’s mentioned; in the case of
    the search engine, the words are the terms and the pages are the original pieces
    of text.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这样表述的原因是术语存储在*倒排索引*中：一种将术语映射到最初包含它的文本的数据结构。可能最容易理解的方式是将它看作是实际书籍的分析索引，其中每个单词条目都指向它被提及的页面；在搜索引擎的情况下，单词是术语，页面是原始文本片段。
- en: 'From now on, we’ll refer to the pieces of text to be indexed (pages, books)
    as *documents*. In order to visualize how documents end up after being indexed,
    let’s assume you have the following two very similar documents:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将被索引的文本片段（页面、书籍）称为*文档*。为了可视化索引后文档的最终形式，让我们假设你有以下两个非常相似的文档：
- en: “the brown fox jumped over the lazy dog” (document 1)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “the brown fox jumped over the lazy dog” (文档1)
- en: “a quick brown fox jumps over the lazy dog” (document 2)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “a quick brown fox jumps over the lazy dog” (文档2)
- en: Assuming you use the text analysis algorithm defined earlier (whitespace tokenization
    with stop words “a,” “an,” and “the”), [table 1.3](#ch01table03) shows a good
    approximation of an inverted index containing such documents.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用之前定义的文本分析算法（使用空格分词和停用词“a”、“an”和“the”），[表1.3](#ch01table03)显示了包含此类文档的倒排索引的良好近似。
- en: Table 1.3\. Inverted index table
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.3\. 倒排索引表
- en: '| Term | Document IDs |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 文档ID |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| brown | 1, 2 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 棕色 | 1, 2 |'
- en: '| fox | 1, 2 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 狐狸 | 1, 2 |'
- en: '| jumped | 1 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 跳跃 | 1 |'
- en: '| over | 1, 2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 过 | 1, 2 |'
- en: '| lazy | 1, 2 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 懒惰 | 1, 2 |'
- en: '| dog | 1, 2 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 狗 | 1, 2 |'
- en: '| quick | 2 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 快速 | 2 |'
- en: '| jumps | 1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 跳 | 1 |'
- en: 'As you can see, there’s no entry for the term “the” because the stopword-based
    token filter has removed such tokens. In the table, you can find the dictionary
    of terms in the first column and a *posting list*—a set of document identifiers—associated
    with each term for each row. With inverted indexes, retrieval of documents that
    contain a given term is very fast: the search engine picks the inverted index,
    looks for an entry for the search term, and eventually retrieves the documents
    contained in the posting list. With the example index, if you search for the term
    “quick,” the inverted index will return document 2 by looking into the posting
    list corresponding to the term “quick.” We’ve just gone through a quick example
    of indexing text into a search engine.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，没有“the”这个术语的条目，因为基于停用词的标记过滤器已经移除了这样的标记。在表中，您可以在第一列找到术语的字典，以及每行与每个术语相关联的*倒排列表*——一组文档标识符。使用倒排索引，检索包含给定术语的文档非常快：搜索引擎选择倒排索引，查找搜索术语的条目，并最终检索倒排列表中的文档。以示例索引为例，如果您搜索术语“quick”，倒排索引将通过查看与“quick”术语相对应的倒排列表来返回文档2。我们刚刚快速地通过一个例子展示了将文本索引到搜索引擎的过程。
- en: 'Let’s think about the steps that go into indexing a book. A book is composed
    of pages, the core content, but it also has a title, an author, an editor, a publication
    year, and so on. You can’t use the same text analysis pipeline for everything;
    you wouldn’t want to remove “the” or “an” from a book title. A user knowing a
    book’s title should be able to find it by exact matching it! If the text analysis
    chain removes “in” from the book title *Tika in Action*, a query for “Tika in
    action” won’t find it. On the other hand, you may want to avoid keeping such tokens
    for the book contents so you have a text analysis pipeline that’s more aggressive
    in filtering unwanted terms. If the text analysis chain removes “in” and “the”
    from the book title *Living in the Information Age*, it shouldn’t be problematic:
    it’s very unlikely that a user will search for “Living in the Information Age,”
    but they may search for “information age.” In this case, there’s little or no
    loss of information, but you get the benefit of storing smaller texts and, more
    important, improving relevance (we’ll talk about this in the next section). A
    common approach in real life is to have multiple inverted indexes that address
    indexing of different parts of a document, all within the same search engine.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下将一本书索引到索引中的步骤。一本书由页面、核心内容组成，但它还包括标题、作者、编辑、出版年份等等。您不能为所有内容使用相同的文本分析管道；您不希望从书名中删除“the”或“an”。知道一本书的标题的用户应该能够通过精确匹配找到它！如果文本分析链从书名《Tika
    in Action》中删除“in”，则查询“Tika in action”将无法找到它。另一方面，您可能希望避免在书的内容中保留这样的标记，以便您有一个更积极地过滤不需要术语的文本分析管道。如果文本分析链从书名《Living
    in the Information Age》中删除“in”和“the”，则不应有问题：用户不太可能搜索“Living in the Information
    Age”，但他们可能会搜索“information age”。在这种情况下，信息损失很小或没有，但您获得了存储更小文本的好处，更重要的是，提高了相关性（我们将在下一节中讨论这一点）。现实生活中的一种常见做法是在同一个搜索引擎中拥有多个倒排索引，以处理文档不同部分的索引。
- en: Searching
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 搜索
- en: 'Now that we have some content indexed in the search engine, we’ll look at searching.
    Historically, the first search engines allowed users to search with specific terms,
    also known as *keywords*, and, eventually, Boolean operators that let users determine
    which terms *must* match, *must not* match, or *can* match in the search results.
    Most commonly, a term in a query *should* match, but that isn’t mandatory. If
    you want search results that must contain such a term, you must add the relevant
    operator: for example, using + in front of the term. A query like “deep +learning
    for search” requires results that contain both “deep” and “learning” and optionally
    contain “for” and “search.” It’s also common to allow users to specify that they
    need entire phrases to match, instead of single terms. That allows users to search
    for an exact sequence of words instead of single terms. The previous query could
    be rephrased as ““deep learning” for search,” in order to return search results
    that must contain the sequence “deep learning” and optionally the terms “for”
    and “search.”'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在搜索引擎中索引了一些内容，我们将探讨搜索。历史上，最早的搜索引擎允许用户使用特定的术语进行搜索，这些术语也被称为*关键词*，最终，布尔运算符允许用户确定哪些术语*必须*匹配，*不能*匹配，或者*可以*匹配在搜索结果中。最常见的是，查询中的术语*应该*匹配，但这不是强制性的。如果您希望搜索结果必须包含此类术语，您必须添加相关运算符：例如，在术语前使用+。查询“deep
    +learning for search”需要包含“deep”和“learning”的结果，并且可选地包含“for”和“search”。也常见的是允许用户指定他们需要整个短语匹配，而不是单个术语。这使用户能够搜索单词的精确序列，而不是单个术语。前面的查询可以改写为““deep
    learning” for search”，以便返回必须包含“deep learning”序列的搜索结果，并且可选地包含“for”和“search”术语。
- en: It may sound surprising, but text analysis is also important during the search,
    or *retrieval*, phase. Suppose you want to search for this book, *Deep Learning
    for Search*, on top of the data you just indexed; assuming you have a web interface,
    you’d probably type a query like “deep learning for search.” The challenge in
    this retrieval phase is to make it possible to retrieve the right book. The first
    thing that sits between a user and a classic search engine UI is a *query parser*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来令人惊讶，但在搜索或*检索*阶段，文本分析也同样重要。假设您想在您刚刚索引的数据中搜索这本书，*深度学习搜索*；假设您有一个网页界面，您可能会输入查询“deep
    learning for search”。在这个检索阶段中的挑战是使检索到正确的书籍成为可能。用户和经典搜索引擎用户界面之间的第一件事是一个*查询解析器*。
- en: 'A query parser is responsible for transforming the text of the search query
    entered by the user into a set of clauses that indicate which terms the search
    engine should look for and how to use them when looking for a match in the inverted
    indexes. In the previous query examples, the query parser would be responsible
    for making sense of the symbols + and ”. Another widespread syntax allows you
    to put Boolean operators among query terms: “deep AND learning.” In this case,
    the query parser will give a special meaning to the “AND” operator: terms to the
    left and right of it are mandatory. A query parser can be thought as a function
    that takes some text and outputs a set of constraints to apply to the underlying
    inverted index(es) in order to find results. Let’s again pick an example query
    like “latest research in artificial intelligence.” A smart query parser would
    create clauses that reflect the semantics of words; for example, instead of having
    two clauses for “artificial” and “intelligence,” it should create only one clause
    for “artificial intelligence.” In addition, probably the term “latest” isn’t to
    be matched; you don’t want results containing the word “latest”; you instead want
    to retrieve results that have been “created” recently. So a good query parser
    would transform the “latest” term into a clause that can be expressed, for example,
    like “created between today and 2 months ago” in natural language. The query engine
    would encode such a clause in a way that’s more easily handled by a computer,
    such as `created < today() AND created > (today() - 60days)`; see [figure 1.6](#ch01fig06).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 查询解析器负责将用户输入的搜索查询文本转换成一系列子句，这些子句指示搜索引擎应查找哪些术语以及如何在倒排索引中查找匹配项时使用它们。在先前的查询示例中，查询解析器负责理解符号
    + 和 ”。另一种广泛使用的语法允许你在查询术语之间放置布尔运算符：“深度 AND 学习”。在这种情况下，查询解析器将给“AND”运算符赋予特殊含义：其左右两侧的术语是强制性的。查询解析器可以被视为一个函数，它接受一些文本并输出一组约束条件，以应用于底层倒排索引（或索引）以找到结果。让我们再次以“人工智能的最新研究”这样的查询为例。一个智能查询解析器会创建反映词语语义的子句；例如，对于“人工”和“智能”这两个词，它应该只创建一个“人工智能”的子句。此外，可能“最新”这个术语不需要匹配；你不想得到包含“最新”这个词的结果；你相反想要检索最近“创建”的结果。因此，一个好的查询解析器会将“最新”这个术语转换成一个可以用自然语言表达，例如“创建于今天和两个月前之间”的子句。查询引擎会将这样的子句编码成计算机更容易处理的方式，例如`created
    < today() AND created > (today() - 60days)`；参见[图1.6](#ch01fig06)。
- en: Figure 1.6\. Query parsing
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6\. 查询解析
- en: '![](Images/01fig06_alt.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig06_alt.jpg)'
- en: During indexing, a text analysis pipeline is used to split the input text into
    terms to be stored in the index; this is also called *index-time text analysis*.
    Similarly, text analysis can be applied during search on the query in order to
    break the query string into terms; this is therefore called *search-time text
    analysis*. A document is retrieved by the search engine when the search-time terms
    match a term in the inverted index referenced by that doc.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引过程中，使用文本分析管道将输入文本分割成要存储在索引中的术语；这被称为*索引时文本分析*。同样，在搜索查询时也可以应用文本分析，以将查询字符串分割成术语；因此这被称为*搜索时文本分析*。当搜索时的术语与文档引用的倒排索引中的术语匹配时，搜索引擎会检索文档。
- en: '[Figure 1.7](#ch01fig07) shows an index-time analysis on the left, which is
    used to split a document text into terms. These end up in the index, all referencing
    *doc 1*. The index-time analysis is composed of a whitespace tokenizer and two
    token filters: the former is used to remove unwanted stopwords (like “the”), and
    the latter is used to convert all the terms into lowercase (for example, “Fox”
    gets converted to “fox”). At upper right, the query “lazy foxes” is passed to
    the search-time analysis, which splits tokens using a whitespace tokenizer but
    filters using a lowercase filter and a *stemming* filter. A stemming filter transforms
    terms by reducing inflected or derived words to their root form; this means removing
    plural suffixes, *ing* form in verbs, and so on. In this case, “foxes” is transformed
    into “fox.”'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.7](#ch01fig07)展示了左边的索引时分析，它用于将文档文本分割成术语。这些术语最终存储在索引中，所有都引用*doc 1*。索引时分析由一个空白分词器（whitespace
    tokenizer）和两个标记过滤器（token filters）组成：前者用于移除不需要的停用词（如“the”），后者用于将所有术语转换为小写（例如，“Fox”被转换为“fox”）。在右上角，查询“lazy
    foxes”被传递到搜索时分析，它使用空白分词器分割标记，但使用小写过滤器和一个*词干化*过滤器进行过滤。词干化过滤器通过将屈折或派生词还原到其词根形式来转换术语；这意味着移除复数后缀、动词中的*ing*形式等。在这种情况下，“foxes”被转换成“fox”。'
- en: Figure 1.7\. Index, search-time analysis, and term matching
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7\. 索引、搜索时分析和术语匹配
- en: '![](Images/01fig07_alt.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig07_alt.jpg)'
- en: 'A common way to verify that indexing and search text analysis pipelines work
    as expected is to follow these steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 验证索引和搜索文本分析管道是否按预期工作的一种常见方法是遵循以下步骤：
- en: '**1**.  Take sample content.'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  获取样本内容。'
- en: '**2**.  Pass the content to the index-time text analysis chain.'
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  将内容传递给索引时文本分析链。'
- en: '**3**.  Take a sample query.'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  获取样本查询。'
- en: '**4**.  Pass the query to the search-time text analysis chain.'
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  将查询传递给搜索时文本分析链。'
- en: '**5**.  Check whether the produced terms match.'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  检查生成的术语是否匹配。'
- en: For example, it’s common to have stopword filters at indexing time, because
    performing the filtering then won’t have any performance impact on the retrieval
    phase. But it may be possible to have other filters within either the indexing
    or search phases. With index- and search-time text analysis chains and query parsing
    in place, we can look at how the process of retrieving search results works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在索引时通常会有停用词过滤器，因为那时进行过滤不会对检索阶段产生任何性能影响。但可能在索引或搜索阶段都有其他过滤器。在索引和搜索时文本分析链以及查询解析就位后，我们可以查看检索搜索结果的过程是如何工作的。
- en: 'You’ve learned one of the basic techniques at the core of every search engine:
    text analysis (tokenization and filtering) allows the system to break down text
    into the terms you expect users to type at query time and place them into a data
    structure called an inverted index, which allows efficient storage (space-wise)
    and retrieval (time-wise). As users, however, we don’t want to look into all the
    search results, so we need search engine to tell us which ones are supposed to
    be the best. Now, you may be wondering, what does *the best* mean? Is there a
    measure of how good a piece of information is, given our queries? The answer is
    yes: we call such a measure *relevance*. Ranking search results in an accurate
    way is one of the most important tasks a search engine has to accomplish. In the
    next section, we’ll have a brief look at how to address the problem of relevance.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了每个搜索引擎核心的基本技术之一：文本分析（分词和过滤）允许系统将文本分解成用户在查询时可能输入的术语，并将它们放入一个称为倒排索引的数据结构中，这允许高效的空间存储和检索。然而，作为用户，我们不想查看所有搜索结果，因此我们需要搜索引擎告诉我们哪些是最应该被考虑的。现在，你可能想知道，“最好”是什么意思？有没有一种衡量信息好坏的度量标准，基于我们的查询？答案是肯定的：我们称这种度量标准为*相关性*。以准确的方式对搜索结果进行排序是搜索引擎必须完成的最重要任务之一。在下一节中，我们将简要探讨如何解决相关性问题。
- en: 1.5.2\. Relevance first
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.5.2\. 优先考虑相关性
- en: You now know how search engines retrieve a document, given a query. In this
    section, you’ll learn how search engines rank the search results so that the most
    important results are returned first. This will give you a solid understanding
    of how common search engines work.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了搜索引擎如何根据查询检索文档。在本节中，你将学习搜索引擎如何对搜索结果进行排序，以便首先返回最重要的结果。这将为你提供一个对常见搜索引擎工作原理的坚实基础。
- en: '*Relevance* is a key concept in search; it’s a measure of how important a document
    is with respect to a certain search query. As humans, it’s often easy for us to
    tell why certain documents are more relevant than others with respect to a query.
    So, in theory, we could try to extract a set of rules to represent our knowledge
    about ranking the importance of a document. But in practice, such an exercise
    would probably fail:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*相关性*是搜索中的关键概念；它是衡量文档相对于某个特定搜索查询重要性的度量。作为人类，我们通常很容易判断为什么某些文档相对于查询来说比其他文档更相关。因此，从理论上讲，我们可以尝试提取一组规则来表示我们对文档排序重要性的知识。但在实践中，这样的练习可能会失败：'
- en: The amount of information we have doesn’t allow us to extract a set of rules
    applicable to most of the documents.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们拥有的信息量不允许我们提取适用于大多数文档的规则集。
- en: Documents in the search engine change a lot over time, and it’s a huge effort
    to keep adjusting the rules accordingly.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎中的文档会随着时间的推移而大量变化，相应地调整规则需要付出巨大的努力。
- en: Documents in the search engine can belong to diverse domains (for example, in
    web search), and it’s not possible to find a good set of rules that works for
    all types of information.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎中的文档可以属于不同的领域（例如，在网页搜索中），并且不可能找到适用于所有类型信息的良好规则集。
- en: 'One of the central themes in the field of information retrieval is to define
    a model that doesn’t require a search engineer to extract such rules. Such a *retrieval
    model* should capture the notion of relevance as accurately as possible. Given
    a set of search results, a retrieval model will *rank* each of them: the more
    relevant the result, the higher its score.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索领域的一个核心主题是定义一个模型，该模型不需要搜索工程师提取此类规则。这样的 *检索模型* 应尽可能准确地捕捉相关性的概念。给定一组搜索结果，检索模型将对每个结果进行
    *排序*：结果越相关，其得分就越高。
- en: Most of the time, as a search engineer, you won’t get perfect results by just
    choosing a retrieval model; relevance is a capricious beast! In real-life scenarios,
    you may have to continuously adjust your text analysis pipelines, as well as the
    retrieval model, and possibly make some fine-grained tuning to the search engine
    internals. But retrieval models help a lot by providing a solid baseline to obtain
    good relevance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，作为一个搜索工程师，仅通过选择检索模型是无法获得完美结果的；相关性是一个反复无常的生物！在实际场景中，你可能需要不断地调整你的文本分析管道，以及检索模型，并且可能需要对搜索引擎内部进行一些精细的调整。但检索模型通过提供一个坚实的基线来获得良好的相关性，起到了很大的帮助作用。
- en: 1.5.3\. Classic retrieval models
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.5.3\. 经典检索模型
- en: Probably one of the most commonly used information retrieval models is the *vector
    space model* (VSM).^([[5](#ch01fn05)]) In this model, each document and query
    is represented as a vector. You can think of a vector as an arrow in a coordinate
    plane; each arrow in VSM can represent a query or a document. The closer two arrows
    are, the more similar they are (see [figure 1.8](#ch01fig08)); each arrow’s direction
    is defined by the terms that compose the query/document.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最常用的信息检索模型之一是 *向量空间模型* (VSM)^([[5](#ch01fn05)]). 在这个模型中，每个文档和查询都表示为一个向量。你可以将向量想象为坐标平面上的一个箭头；VSM
    中的每个箭头都可以代表一个查询或文档。两个箭头越接近，它们就越相似（见[图 1.8](#ch01fig08)）；每个箭头的方向由构成查询/文档的术语定义。
- en: ⁵
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See G. Salton, A. Wong, and C. S. Yang, “A vector space model for automatic
    indexing,” *Communications of the ACM* 18, no. 11 (1975): 613-620, [http://mng.bz/gNxG](http://mng.bz/gNxG).'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见 G. Salton, A. Wong, 和 C. S. Yang, “A vector space model for automatic indexing,”
    *Communications of the ACM* 18, no. 11 (1975): 613-620, [http://mng.bz/gNxG](http://mng.bz/gNxG).'
- en: Figure 1.8\. Similarities between document and query vectors according to VSM
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.8\. 根据向量空间模型 (VSM) 的文档和查询向量之间的相似性
- en: '![](Images/01fig08_alt.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig08_alt.jpg)'
- en: 'In such a vector representation, each term is associated with a *weight*: a
    real number that tells how important that term is in that document/query with
    respect to the rest of the documents in the search engine. Such weights can be
    calculated in various ways. At this point, we won’t go too deep into the details
    of how these weights are calculated; I’ll mention that the most common algorithm
    is called *term frequency–inverse document frequency* (TF-IDF). The basic idea
    behind TF-IDF is that the more frequently a term appears in a single document
    (term frequency, or TF) the more important it is. At the same time, it states
    that the more common a term is among all the documents, the less important it
    is (*inverse document frequency*, or IDF). So in VSM, search results are ranked
    with respect to the query vector, so documents appear higher in the results list
    (get a higher rank/score) if they’re closer to such query vector.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种向量表示中，每个术语都与一个 *权重* 相关联：一个表示该术语在文档/查询中相对于搜索引擎中其他文档的重要性程度的实数。这些权重可以通过各种方式计算。在此阶段，我们不会深入探讨这些权重是如何计算的细节；我将提到最常用的算法被称为
    *词频-逆文档频率* (TF-IDF)。TF-IDF 的基本思想是，一个术语在单个文档中出现的频率越高（词频，或 TF），它就越重要。同时，它指出，一个术语在所有文档中越常见，它就越不重要（逆文档频率，或
    IDF）。因此，在 VSM 中，搜索结果是根据查询向量进行排序的，所以如果文档与查询向量更接近，它们就会在结果列表中显示得更高（获得更高的排名/得分）。
- en: Whereas VSM is an information retrieval model based on linear algebra, over
    the years alternate approaches based on probabilistic relevance models have emerged.
    Instead of calculating how near a document and a query vector are, a probabilistic
    model ranks search results based on an estimate of the probability that a document
    is relevant to a certain query. One of the most common ranking functions for such
    models is *Okapi BM25*. We won’t dive into its details, but it has shown good
    results, especially on texts that aren’t very long.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然VSM是一个基于线性代数的信息检索模型，但多年来，基于概率相关性的替代方法已经出现。与计算文档和查询向量之间的距离不同，概率模型根据对文档与特定查询相关性的概率估计来对搜索结果进行排序。这类模型中最常见的排名函数之一是*Okapi
    BM25*。我们不会深入其细节，但它已经显示出良好的结果，尤其是在不太长的文本上。
- en: 1.5.4\. Precision and recall
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.5.4\. 精确度和召回率
- en: We’ll look into how neural search can help address relevance in future chapters,
    but first we need to be able to measure relevance! A standard way of measuring
    how well an information retrieval system is doing is to calculate its precision
    and recall. *Precision* is the fraction of retrieved documents that are relevant.
    If a system has high precision, users will mostly find results they’re looking
    for at the top of the list of search results. *Recall* is the fraction of relevant
    documents that are retrieved. If a system has a good recall, users will find all
    results relevant for them in the search results, although they might not all be
    among the top results.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的章节中探讨神经搜索如何帮助解决相关性问题，但首先我们需要能够衡量相关性！衡量信息检索系统表现好坏的标准方法之一是计算其精确度和召回率。*精确度*是检索到的相关文档的比例。如果一个系统具有高精确度，用户将主要在搜索结果列表的顶部找到他们想要的结果。*召回率*是检索到的相关文档的比例。如果一个系统具有良好的召回率，用户将在搜索结果中找到所有与他们相关的结果，尽管它们可能并不都是顶部结果。
- en: As you may have noticed, measuring precision and recall requires someone to
    judge how relevant search results are. In small-scale scenarios, that’s an addressable
    task; but the effort required makes it hardly doable for huge collections of documents.
    An option to measure the effectiveness of search engines is to use publicly available
    datasets for information retrieval, like the ones from the National Institute
    of Standards and Technology (NIST) Text REtrieval Conference (also known as TREC^([[6](#ch01fn06)])),
    which contain lots of ranked queries to be used for testing precision and recall.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，测量精确度和召回率需要有人来判断搜索结果的相关性。在小规模场景中，这是一个可解决的问题；但所需的努力使得它对于大量文档集合几乎无法实现。衡量搜索引擎有效性的一个选择是使用公开可用的信息检索数据集，例如来自国家标准与技术研究院（NIST）文本检索会议（也称为TREC^([[6](#ch01fn06)]))的数据集，这些数据集包含大量用于测试精确度和召回率的排序查询。
- en: ⁶
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [http://trec.nist.gov/data.html](http://trec.nist.gov/data.html).
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅[http://trec.nist.gov/data.html](http://trec.nist.gov/data.html)。
- en: In this section, you’ve learned some basics of classic information retrieval
    models like VSM and probabilistic models. We’re now going to examine common issues
    that affect search engines. The rest of the book will discuss how to fix them
    with the help of DL.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已经学习了经典信息检索模型（如VSM和概率模型）的一些基础知识。现在，我们将探讨影响搜索引擎的常见问题。本书的其余部分将讨论如何借助深度学习来解决这些问题。
- en: 1.6\. Unsolved problems
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6\. 未解决的问题
- en: We’ve had a closer look at how a search engine works, in particular how it strives
    to retrieve information relevant to the end user’s needs. But let’s take a step
    back and try to see the problem from the perspective of how, as users, we use
    search engines every day. We’ll examine some of the problems that remain unresolved
    in many search scenarios, in order to better understand which issues we can hope
    to solve with the help of DL.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经更深入地了解了搜索引擎的工作原理，特别是它如何努力检索与最终用户需求相关的信息。但让我们退一步，尝试从我们作为用户每天如何使用搜索引擎的角度来看待这个问题。我们将探讨许多搜索场景中尚未解决的某些问题，以便更好地理解我们可以借助深度学习解决哪些问题。
- en: 'Filling a knowledge gap, as opposed to retrieving information, is a slightly
    more complex topic. Let’s again take the example of going to a library, because
    you want to know more about interesting recent research in the field of AI. As
    soon as you meet the librarian, you have a problem: how do you get the librarian
    to accurately understand what you need and what would be useful to you?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 填补知识空白，而不是检索信息，是一个稍微复杂一些的话题。让我们再次以去图书馆的例子来说明，因为你想要了解更多关于人工智能领域最近有趣的研究。一见到图书管理员，你就遇到了一个问题：你该如何让图书管理员准确理解你的需求以及对你有用的信息是什么？
- en: Although this sounds simple, the usefulness of a piece of information is hardly
    objective, but instead is rather subjective and based on context and opinion.
    You may assume that a librarian has enough knowledge and experience that what
    you receive is good. In real life, you would probably talk to the librarian to
    introduce yourself and share information about your background and why you need
    something; that would allow the librarian to use such context in order to
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这听起来很简单，但信息的实用性几乎不是客观的，而是相当主观的，并基于背景和观点。你可能认为图书管理员拥有足够的知识和经验，所以你得到的是好的信息。在现实生活中，你可能会与图书管理员交谈，介绍自己并分享你的背景以及为什么你需要某些信息；这将允许图书管理员利用这样的背景来
- en: Exclude some books before even trying to search
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试搜索之前就排除一些书籍
- en: Discard some books after having found them
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在找到书籍之后丢弃一些书籍
- en: Explicitly search in one or more areas that have a closer relation to your context
    (for example, coming from academia versus industry sources)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确搜索一个或多个与你所处环境有更紧密关系的领域（例如，来自学术界与工业界来源）
- en: You’ll be able to give feedback on the books given to you by the librarian afterward,
    although sometimes you can express concerns based on past experiences (for example,
    you don’t like books written by a certain author, so you advise the librarian
    to explicitly not consider them). Both context and opinion can vary considerably
    and consequently influence the relevance of information over time and among different
    people. How does a librarian cope with this mismatch?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在之后能够对图书管理员给你的书籍提供反馈，尽管有时你可以根据以往的经验表达担忧（例如，你不喜欢某个作者写的书，因此建议图书管理员明确不考虑这些书籍）。背景和观点可能会有很大的不同，从而影响信息的相关性随时间和不同人而变化。图书管理员如何应对这种不匹配？
- en: You as a user may not know the librarian, or at least not well enough to understand
    their context. The librarian’s background and opinions are important because they
    influence the results you get. Therefore, the better you understand the librarian,
    the faster you’ll get your information. So you need to know your librarian in
    order to get good results!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，你可能并不了解图书管理员，或者至少并不足够了解他们的背景。图书管理员的经验和观点很重要，因为它们会影响你得到的结果。因此，你越了解图书管理员，你就能更快地得到信息。所以，为了得到好的结果，你需要了解你的图书管理员！
- en: What if the librarian gives you a book about “deep learning techniques” in response
    to your first inquiry about “artificial intelligence”? If you don’t know the subject,
    you need to make a second inquiry about “an introduction to what deep learning
    is” and whether there’s a good book about it in the library. This process can
    repeat a number of times; the key thing to understand is that information is flowing
    incrementally—you don’t upload stuff into your brain the way characters do in
    *The Matrix*. Instead, if you want to know something about AI, you may realize
    you need to know a bit about DL first, and, for that, you discover you need to
    read about calculus and linear algebra, and so on. In other words, you don’t know
    all of what you need when you first ask.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图书管理员在你询问“人工智能”后给你推荐了一本关于“深度学习技术”的书，而你并不了解这个主题，你需要进行第二次询问，了解“深度学习是什么的介绍”以及图书馆里是否有关于它的好书。这个过程可能会重复多次；关键是要理解信息是逐步流动的——你不会像《黑客帝国》中的角色那样将东西上传到大脑中。相反，如果你想了解关于人工智能的某些内容，你可能会意识到你首先需要了解一些关于深度学习的内容，为此，你可能需要阅读关于微积分和线性代数等方面的书籍。换句话说，当你第一次询问时，你并不知道你需要所有这些信息。
- en: 'To sum up, the process of getting the information you’re looking for from the
    librarian has some flaws, caused by these situations:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，从图书管理员那里获取你所需信息的流程存在一些缺陷，这些缺陷是由以下情况造成的：
- en: The librarian doesn’t know you.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图书管理员并不了解你。
- en: You don’t know the librarian.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你并不了解图书管理员。
- en: You may need a few iterations in order to get everything you need.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能需要经过几次迭代才能获得所有你需要的信息。
- en: It’s important to identify these issues, because we want to use deep neural
    networks to help us build better search engines that can be more easily used—and
    we’d like DL to help fix those problems. Understanding these issues is the first
    step toward resolving them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 识别这些问题非常重要，因为我们希望使用深度神经网络帮助我们构建更好的搜索引擎，使其更容易使用——并且我们希望深度学习能够帮助解决这些问题。理解这些问题是解决它们的第一步。
- en: 1.7\. Opening the search engine black box
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7\. 打开搜索引擎的“黑箱”
- en: Now let’s try to understand how much of what the search engine is doing users
    can see. A crucial issue in creating effective search queries is which query language
    you use. Some years ago, you would enter one or more keywords in a search box
    to perform a query. Today, technology has evolved to the point that you can type
    queries in natural language. Some search engines index documents in multiple languages
    (for example, for web search) and allow subsequent querying. If you search for
    the same thing but express it with slightly different queries in a search engine
    like Google, you’ll observe surprisingly different results.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来尝试理解用户能看到搜索引擎做了多少事情。创建有效的搜索查询的一个关键问题是使用哪种查询语言。几年前，你会在搜索框中输入一个或多个关键词来执行查询。如今，技术已经发展到你可以用自然语言输入查询。一些搜索引擎可以索引多种语言的文档（例如，用于网络搜索）并允许后续查询。如果你在像Google这样的搜索引擎中搜索相同的内容，但用略有不同的查询表达，你会观察到令人惊讶的不同结果。
- en: Let’s run a little experiment to see how search results change when the same
    request is expressed using different queries. If you were talking to a human and
    asking the same question in different ways, you’d expect to always get the same
    kind of answer. For example, if you ask someone, “What are the ‘latest *breakthroughs*
    in artificial intelligence,’ in your opinion?” you’ll get an answer based on their
    opinion. If you ask that same person, “What are the ‘latest *advancements* in
    artificial intelligence,’ in your opinion?” you’ll likely get exactly the same
    answer, or one that’s semantically equivalent.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个小实验，看看当使用不同的查询表达相同的请求时，搜索结果会如何变化。如果你在与人类交谈并以不同的方式提出相同的问题，你期望总是得到相同类型的答案。例如，如果你问某人，“在你看来，人工智能的‘最新*突破*’是什么？”你会得到基于他们观点的答案。如果你问同一个人，“在你看来，人工智能的‘最新*进展*’是什么？”你很可能会得到完全相同的答案，或者一个语义上等效的答案。
- en: Today, this often isn’t the case with search engines. [Table 1.4](#ch01table04)
    shows the results of searching for “latest breakthroughs in artificial intelligence”
    and some variants on Google.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，这种情况通常不适用于搜索引擎。[表1.4](#ch01table04)显示了在Google上搜索“人工智能的最新突破”及其变体得到的结果。
- en: Table 1.4\. Comparing similar queries
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.4\. 比较相似查询
- en: '| Query | First result title |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 查询 | 第一条结果标题 |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Latest breakthroughs in artificial intelligence | Academic papers for “latest
    breakthroughs in artificial intelligence” (Google Scholar) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能的最新突破 | “人工智能最新突破”的学术论文（Google Scholar）|'
- en: '| Latest advancements in artificial intelligence | Google advancements artificial
    intelligence push with 2 top hires |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能的最新进展 | Google通过两位顶级人才推动人工智能进步 |'
- en: '| Latest advancements on artificial intelligence | Images related to “latest
    advancements on artificial intelligence” (Google Images) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能的最新进展 | 与“人工智能最新进展”相关的图片（Google 图片）|'
- en: '| Latest breakthroughs in AI | Artificial Intelligence News—ScienceDaily |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能的最新突破 | 科学日报的人工智能新闻 |'
- en: '| Più recenti sviluppi di ricerca sull’intelligenza artificiale | Intelligenza
    Artificiale (Wikipedia) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Più recenti sviluppi di ricerca sull’intelligenza artificiale | 人工智能（维基百科）|'
- en: 'Although the first result of the first query isn’t surprising, changing the
    term “breakthroughs” to one of its synonyms (“advancements”) produces a different
    result, which seems to suggest that the search engine has a different understanding
    of the information needed: you weren’t looking into how Google is improving AI!
    The third query gives a surprising result: images! We have no real explanation
    for this. Changing “artificial intelligence” to its acronym, “AI,” leads to a
    different, but still relevant, result. And when you use the Italian translation
    of the original query, you get a completely different result with respect to the
    query in English: a Wikipedia page about artificial intelligence. That seems generic,
    given the fact that, for example, Google Scholar indexes research papers in different
    languages.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第一次查询的第一个结果并不令人惊讶，但将“突破”一词改为其同义词之一（“进步”）会产生不同的结果，这似乎表明搜索引擎对所需信息的理解不同：你并不是在研究谷歌如何改进人工智能！第三次查询给出了一个令人惊讶的结果：图片！我们对此没有真正的解释。将“人工智能”改为其首字母缩写“AI”，会得到一个不同但仍然相关的结果。而当你使用原始查询的意大利语翻译时，与英语查询相比，你会得到一个完全不同的结果：一个关于人工智能的维基百科页面。鉴于例如谷歌学术索引不同语言的学术论文，这似乎很普遍。
- en: Search engine rankings can vary significantly, much like user opinions; although
    a search engineer could optimize a ranking to respond to a set of given queries,
    it’s difficult to adjust it for possibly tens or hundreds of similar queries.
    So in real life, we don’t manually adjust the rankings of search results; doing
    so would be nearly impossible and unlikely to result in a generally good ranking.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎排名可能会有很大的变化，就像用户意见一样；尽管搜索工程师可以优化排名以响应一组给定的查询，但调整它以适应可能成十或数百个类似的查询是困难的。所以，在现实生活中，我们不会手动调整搜索结果的排名；这样做几乎是不可能的，而且不太可能产生一个普遍良好的排名。
- en: 'Often, performing search is a trial-and-error process: you issue an initial
    query and get too many results; you issue a second query and still get too many;
    and a third query may return trivial results you’re not interested in. Expressing
    an informational need using a search query isn’t a trivial task. You often end
    up performing a bunch of queries just to get a high-level understanding of what
    you think the search engine can do with them. It’s like trying to look into a
    black box: you see almost nothing but try to make assumptions about what happens
    inside.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，进行搜索是一个试错的过程：你发出一个初始查询，得到太多的结果；你发出第二个查询，仍然得到太多的结果；第三个查询可能返回你不太感兴趣的琐碎结果。使用搜索查询表达信息需求并不是一个简单任务。你经常不得不执行一系列查询，只是为了获得对搜索引擎如何使用它们的初步理解。这就像试图看一个黑盒子：你几乎什么也看不到，但试图对内部发生的事情做出假设。
- en: In most cases, users don’t have the chance to understand what the search engine
    is doing. Even worse, things change a lot depending on the way the user expresses
    their request.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，用户没有机会了解搜索引擎在做什么。更糟糕的是，事情会根据用户表达请求的方式有很大的变化。
- en: Now that you understand how a search engine generally works and you’ve learned
    about some important problems that haven’t been completely solved yet in the search
    field, it’s time to meet DL and discover how it can help to solve or at least
    mitigate such issues. We’ll start with a high-level overview of the capabilities
    of deep neural networks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了搜索引擎通常是如何工作的，并且你已经了解了一些在搜索领域尚未完全解决的问题，是时候认识深度学习（DL）并了解它如何帮助解决或至少减轻这些问题了。我们将从深度神经网络的能力的高级概述开始。
- en: 1.8\. Deep learning to the rescue
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8 深度学习拯救
- en: So far, we’ve explored information retrieval themes that are necessary to prepare
    you for the journey through neural search. You’ll now start learning about DL,
    which can help create smarter search engines. This section will introduce you
    to basic DL concepts.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了信息检索主题，这些主题对于准备你通过神经搜索之旅是必要的。现在，你将开始学习深度学习（DL），这可以帮助创建更智能的搜索引擎。本节将向你介绍基本深度学习概念。
- en: In the past, a key difficulty in *computer vision* (a field of computer science
    that deals with processing and understanding visual data like pictures or videos),
    when working with images, was that it was nearly impossible to obtain an image
    representation containing information about the enclosed objects and visual structures.
    How can you make a computer tell whether an image represents a running lion, a
    refrigerator, a group of monkeys, and so on? DL helped to solve this problem with
    the creation of a special type of deep neural network that could learn image representations
    incrementally, one abstraction at a time, as exemplified in [figure 1.9](#ch01fig09).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，当与图像一起工作时，计算机视觉（一个处理和理解如图像或视频等视觉数据的计算机科学领域）的一个关键困难是几乎不可能获得包含关于封闭对象和视觉结构信息的图像表示。你如何让计算机判断一个图像代表的是奔跑的狮子、冰箱、一群猴子等等？深度学习通过创建一种特殊的深度神经网络来帮助解决这个问题，这种神经网络可以逐步学习图像表示，一次一个抽象，如[图1.9](#ch01fig09)所示。
- en: Figure 1.9\. Learning image abstractions incrementally
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9. 逐步学习图像抽象
- en: '![](Images/01fig09_alt.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/01fig09_alt.jpg)'
- en: As mentioned earlier in this chapter, DL is a subfield of ML that focuses on
    learning deep representations of text, images, or data in general by learning
    successive abstractions of increasingly meaningful representations. It does that
    by using deep neural networks ([figure 1.10](#ch01fig10) shows a deep neural network
    with three hidden layers). Remember that a neural network is considered *deep*
    when it has at least two hidden layers.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，深度学习是机器学习的一个子领域，它通过学习越来越有意义的表示的连续抽象来专注于学习文本、图像或数据的深度表示。它是通过使用深度神经网络来做到这一点的（[图1.10](#ch01fig10)显示了一个具有三个隐藏层的深度神经网络）。记住，当一个神经网络至少有两个隐藏层时，它被认为是“深度”的。
- en: Figure 1.10\. A deep feed-forward neural network with three hidden layers
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10. 具有三个隐藏层的深度前馈神经网络
- en: '![](Images/01fig10_alt.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/01fig10_alt.jpg)'
- en: At each step (or layer of the network), such deep neural networks are able to
    capture increasingly more complex structures in the data. It isn’t by chance that
    computer vision is one of the fields that fostered the development and research
    of representation-learning algorithms for images.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤（或网络的每一层）中，这样的深度神经网络能够捕捉到数据中越来越复杂的结构。计算机视觉是培养图像表示学习算法发展和研究的一个领域，这并非偶然。
- en: Researchers have discovered that it makes sense to use such deep networks especially
    on data that is highly compositional.^([[7](#ch01fn07)]) This means they can help
    immensely when you can think of something as being formed by smaller parts of
    similar constituents. Images and text are good examples of compositional data,
    because they can be divided into smaller units incrementally (for example, text
    → paragraphs → sentences → words). But (deep) neural networks aren’t useful only
    to learn representations; they can be used to perform a lot of different ML tasks.
    I mentioned that the document-categorization task can be solved via ML methods.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，在高度组合的数据上使用这样的深度网络是有意义的。[7](#ch01fn07) 这意味着当你能将某物视为由相似组成部分的较小部分组成时，它们可以极大地帮助。图像和文本是组合数据的良好例子，因为它们可以逐步分解成更小的单元（例如，文本→段落→句子→单词）。但是（深度）神经网络不仅对学习表示有用；它们还可以用于执行许多不同的机器学习任务。我提到，文档分类任务可以通过机器学习方法解决。
- en: ⁷
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See, for example, H. Mhaskar, Q. Liao, and T. Poggio, “When and Why Are Deep
    Networks Better Than Shallow Ones?” *Proceedings of the AAAI-17: Thirty-First
    AAAI Conference on Artificial Intelligence* (Center for Brain, Minds & Machines),
    [http://mng.bz/0Wrv](http://mng.bz/0Wrv).'
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，参见H. Mhaskar，Q. Liao，和T. Poggio的论文，“何时以及为什么深度网络比浅层网络更好？”*AAA-17第三十一届人工智能会议论文集*（大脑、心智与机器中心），[http://mng.bz/0Wrv](http://mng.bz/0Wrv)。
- en: 'Although there are many different ways a neural network can be architected,
    neural networks are commonly composed of the following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络的架构有很多不同的方式，但神经网络通常由以下部分组成：
- en: A set of neurons
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组神经元
- en: A set of connections between all or some of the neurons
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有或某些神经元之间的一组连接
- en: A weight (a real number) for each directed connection between two neurons
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元之间有向连接的权重（一个实数）
- en: One or more functions that map how each neuron receives and *propagates* signals
    toward its outgoing connections
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个函数，映射每个神经元如何接收和*传播*信号到其输出连接
- en: Optionally, a set of layers that group sets of neurons having similar connectivity
    in the neural network
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，一组将神经网络中具有相似连接性的神经元分组在一起的层
- en: In [figure 1.10](#ch01fig10), we can identify 20 neurons organized in a set
    of 5 layers. Each neuron within each layer is connected with all the neurons in
    the layers nearby (both the previous and the following layers), except for the
    first and last layers. Conventionally, information starts flowing within the network
    from left to right. The first layer that receives the inputs is called the *input
    layer*; and the last layer, called the *output layer*, outputs the results of
    the neural network. The layers in between are called *hidden layers*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 1.10](#ch01fig10)中，我们可以识别出组织在 5 层中的一组 20 个神经元。每个层中的每个神经元都与附近层（包括上一层和下一层）中的所有神经元相连，除了第一层和最后一层。传统上，信息从左到右在网络中流动。接收输入的第一个层被称为*输入层*；而最后一个层，称为*输出层*，输出神经网络的输出结果。介于输入层和输出层之间的层被称为*隐藏层*。
- en: 'Imagine that you could apply the same approach to text to learn representations
    of documents that capture increasingly higher abstractions within a document.
    DL-based techniques exist for such tasks, and over time these algorithms are becoming
    smarter: you can use them to extract word, sentence, paragraph, and document representations
    that can capture surprisingly interesting semantics.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你能够将同样的方法应用于文本，以学习能够捕捉文档中越来越高的抽象层次的文档表示。存在基于深度学习的技术来完成此类任务，并且随着时间的推移，这些算法变得越来越智能：你可以使用它们来提取能够捕捉到令人惊讶有趣语义的词、句子、段落和文档表示。
- en: When using a neural network algorithm to learn word representations within a
    set of text documents, closely related words lie near each other in the vector
    space. Think about creating a point on a two-dimensional plot for each word contained
    in a piece of text, and see how similar or closely related words lie close to
    one another, as in [figure 1.11](#ch01fig11). That can be achieved by using a
    neural network algorithm called *word2vec* to learn such vector representations
    for words (also called *word vectors*). Notice that the words “Information” and
    “Retrieval” lie close to each other. Similarly, “word2vec” and “Skip-gram,” terms
    that both relate to (shallow) neural network algorithms used to extract word vectors,
    are near each other.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用神经网络算法在文本文档集中学习词表示时，密切相关的词在向量空间中彼此靠近。想想为文本中的每个词在二维图上创建一个点，看看相似或密切相关的词如何彼此靠近，就像[图
    1.11](#ch01fig11)中所示。这可以通过使用名为 *word2vec* 的神经网络算法来学习这样的词向量表示（也称为 *词向量*）来实现。注意，“Information”和“Retrieval”这两个词彼此靠近。同样，“word2vec”和“Skip-gram”，这两个术语都与用于提取词向量的（浅层）神经网络算法相关，它们也彼此靠近。
- en: Figure 1.11\. Word vectors derived from the text of research articles on word2vec
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.11\. 从 word2vec 研究文章的文本中派生的词向量
- en: '![](Images/01fig11_alt.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig11_alt.jpg)'
- en: One of the key ideas of neural search is to use such representations to improve
    the effectiveness of search engines. It would be nice to have a retrieval model
    that relies on word and document vectors (also called *embeddings*) with these
    capabilities, so we could calculate and use document and word similarities efficiently
    by looking at the *nearest neighbors*. [Figure 1.12](#ch01fig12) shows a deep
    neural network used to create word representations of the words contained in indexed
    documents, which are then put back into the search engine; they can be used to
    adjust the order of search results.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 神经搜索的一个关键思想是使用这样的表示来提高搜索引擎的有效性。拥有一个依赖于具有这些能力的词和文档向量（也称为 *嵌入*）的检索模型将会很棒，这样我们就可以通过查看*最近邻*来有效地计算和使用文档和词的相似性。[图
    1.12](#ch01fig12) 展示了一个用于创建索引文档中包含的词的词表示的深度神经网络，这些表示随后被放回搜索引擎中；它们可以用来调整搜索结果的顺序。
- en: 'Figure 1.12\. A neural search application: using word representations generated
    by a deep neural network to provide more-relevant results'
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.12\. 一种神经搜索应用：使用由深度神经网络生成的词表示来提供更相关的结果
- en: '![](Images/01fig12_alt.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig12_alt.jpg)'
- en: '|  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Deep learning vs. deep neural networks**'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习与深度神经网络**'
- en: 'We need to make an important distinction. Deep learning is mostly about learning
    representations of words, text, documents, and images by using deep neural networks.
    Deep neural networks, however, have a wider adoption: they’re used, for example,
    in language modeling, machine translation, and many other tasks. In this book,
    I’ll make it clear when we’re using deep neural nets to learn representations
    and when we’re using them for other purposes. In addition to learning representations,
    deep neural networks can help solve a number of information retrieval tasks.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做出一个重要的区分。深度学习主要是通过使用深度神经网络来学习单词、文本、文档和图像的表示。然而，深度神经网络的应用范围更广：例如，它们被用于语言建模、机器翻译以及许多其他任务。在这本书中，我将清楚地说明我们是在使用深度神经网络来学习表示，还是在用于其他目的。除了学习表示之外，深度神经网络还可以帮助解决许多信息检索任务。
- en: '|  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The previous section analyzed the importance of context when compared to the
    complexity of expressing and understanding information needs via text queries.
    Good semantic representations of text are often built by using the context in
    which a word, sentence, or document appears, in order to infer the most appropriate
    representation. Let’s look at the previous example to briefly see how DL algorithms
    can help get better results with relevance. Consider the two queries “latest breakthroughs
    in artificial intelligence” and “latest breakthroughs in AI” from [table 1.4](#ch01table04),
    assuming we’re using the VSM. In such models, the similarity between queries and
    documents can vary a lot based on the text analysis chain. But this problem doesn’t
    affect vector representations of text generated with recent algorithms based on
    neural networks. Although “artificial intelligence” and “AI” might lie far apart
    in VSM, they will likely be placed close together when they’re plotted using word
    representations generated by neural nets. With such a simple change, we can boost
    the relevance of the search engine via more semantically grounded representations
    of words.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节分析了在通过文本查询表达和理解信息需求时，与复杂性的比较，上下文的重要性。好的语义文本表示通常是通过使用单词、句子或文档出现的上下文来构建的，以便推断最合适的表示。让我们回顾先前的例子，简要看看深度学习算法如何帮助获得更好的相关性结果。考虑来自[表1.4](#ch01table04)的两个查询“人工智能的最新突破”和“AI的最新突破”，假设我们使用的是向量空间模型（VSM）。在这样的模型中，查询与文档之间的相似性可能因文本分析链而大不相同。但这个问题不会影响基于神经网络算法生成的文本向量表示。尽管“人工智能”和“AI”在VSM中可能相距甚远，但使用神经网络生成的词表示进行绘图时，它们很可能会被放置得很近。通过这样的简单改变，我们可以通过更语义化的词表示来提高搜索引擎的相关性。
- en: Before diving deeper into neural search applications, let’s look at how search
    engines and neural networks can work together.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨神经搜索应用之前，让我们看看搜索引擎和神经网络如何协同工作。
- en: 1.9\. Index, please meet neuron
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9\. 索引，请认识神经元
- en: 'An artificial neural network can learn to predict outputs based on a training
    set with labeled data (supervised learning, where each input is provided with
    information about the expected output), or it can perform unsupervised learning
    (no information about the correct output for each input is given) in order to
    extract patterns and/or learn representations. A search engine’s typical workflow
    involves indexing and searching content; notably, such tasks can happen in parallel.
    Although this may sound like a technicality at this point, the way you integrate
    a search engine with a neural network is important in principle because it impacts
    the neural search design’s effectiveness and performance. You may have a super-accurate
    system, but if it’s slow, no one will want to use it! In this book, you’ll see
    several ways to integrate neural networks and search engines:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络可以学习根据带有标签数据的训练集预测输出（监督学习，其中每个输入都提供了关于预期输出的信息），或者它可以执行无监督学习（对于每个输入的正确输出没有提供信息），以提取模式或学习表示。搜索引擎的典型工作流程涉及索引和搜索内容；值得注意的是，这些任务可以并行发生。虽然这一点可能听起来像是一个技术细节，但将搜索引擎与神经网络集成的做法在原则上很重要，因为它会影响神经搜索设计的效果和性能。你可能有一个超级精确的系统，但如果它很慢，没有人会想使用它！在这本书中，你将看到几种将神经网络和搜索引擎集成的途径：
- en: '***Train-then-index—*** Train the network first on a collection of documents
    (texts, images), and then index the same data into the search engine and use the
    neural network in conjunction with the search engine at search time.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***训练后索引——*** 首先在文档集合（文本、图像）上训练网络，然后将相同的数据索引到搜索引擎中，并在搜索时与搜索引擎一起使用神经网络。'
- en: '***Index-then-train—*** Index a collection of documents into the search engine
    first; then train the neural network with the indexed data (eventually retraining
    when data changes); and then use the neural network in conjunction with the search
    engine at search time.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Index-then-train—*** 首先将文档集合索引到搜索引擎中；然后使用索引数据（数据变化时最终重新训练）训练神经网络；然后在搜索时将神经网络与搜索引擎结合使用。'
- en: '***Train-extract-index—*** Train the network first on a collection of documents,
    and use the trained network to create useful resources that will be indexed along
    with the data. Search happens as usual with only the search engine.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Train-extract-index—*** 首先在文档集合上训练网络，然后使用训练好的网络创建将随数据一起索引的有用资源。搜索过程与往常一样，只需使用搜索引擎即可。'
- en: You’ll see each of these options in this book, being applied in the right context.
    For example, the train-then-index option will be used in [chapter 3](kindle_split_015.xhtml#ch03)
    for text generation, and the index-then-train option will be used in [chapter
    2](kindle_split_013.xhtml#ch02) for synonym generation from the indexed data.
    The train-extract-index option makes sense when you use a neural network to learn
    something like a semantic representation of the data to be indexed; you’ll use
    such representations at search time without requiring any interaction with the
    neural network. This is the case for the scenario outlined in [chapter 8](kindle_split_021.xhtml#ch08)
    for image search. The last chapter of the book also briefly looks at how to handle
    situations where the data isn’t all available at first but rather arrives in a
    streaming fashion.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你将看到这些选项的每个应用，都适用于正确的上下文。例如，在[第3章](kindle_split_015.xhtml#ch03)中，将使用train-then-index选项进行文本生成，而在[第2章](kindle_split_013.xhtml#ch02)中，将使用index-then-train选项从索引数据中生成同义词。当你使用神经网络学习要索引的数据的语义表示时，train-extract-index选项是有意义的；你将在搜索时使用这些表示，而不需要与神经网络进行任何交互。这是[第8章](kindle_split_021.xhtml#ch08)中概述的场景。本书的最后一章也简要探讨了如何处理数据最初并不全部可用，而是以流式传输方式到达的情况。
- en: 1.10\. Neural network training
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.10\. 神经网络训练
- en: In order to use a neural net’s powerful learning capabilities, you need to train
    it. Training a network like the one shown in the previous section via supervised
    learning means providing inputs to the network input layer, comparing the network
    (predicted) outputs with the known (target) outputs, and letting the network learn
    from the discrepancies between predicted and target outputs. Neural networks can
    easily represent many interesting mathematical functions; that’s one of the reasons
    they can have very high accuracy. Such mathematical functions are governed by
    the connections’ *weights* and neurons’ *activation functions*. A neural network
    learning algorithm takes the discrepancies between desired and actual outputs
    and adjusts each layer’s weights to reduce the output error in the future. If
    you feed enough data to the network, it will be able to achieve a very small error
    rate and therefore perform well. Activation functions have an impact on a neural
    network’s ability to perform predictions and on how quickly it learns; the activation
    functions control when and how much the incoming signal to a neuron is propagated
    throughout to the output connections.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用神经网络强大的学习能力，你需要对其进行训练。通过监督学习训练如前节所示的网络意味着向网络输入层提供输入，比较网络（预测）输出与已知（目标）输出，并让网络从预测输出和目标输出之间的差异中学习。神经网络可以轻松表示许多有趣的数学函数；这也是它们可以具有非常高的准确率的原因之一。这些数学函数由连接的*权重*和神经元的*激活函数*控制。神经网络学习算法通过调整每一层的权重来减少未来的输出误差，从而减少所需输出和实际输出之间的差异。如果你向网络提供足够的数据，它将能够实现非常小的误差率，因此表现良好。激活函数会影响神经网络进行预测的能力以及其学习速度；激活函数控制输入信号何时以及如何传播到神经元的输出连接。
- en: The most commonly used learning algorithm for neural networks is called *backpropagation*.
    Given desired and actual outputs, the algorithm *backpropagates* each neuron’s
    *error* and consequently adjusts its internal state on each neuron’s connections,
    one layer at a time, from output to input (backward); see [figure 1.13](#ch01fig13).
    Each training example makes backpropagation “adjust” each neuron’s state and connections
    to reduce the amount of error produced by the network for that pair of specific
    input and desired output. This is a high-level description of how a backpropagation
    algorithm works; we’ll take a closer look in upcoming chapters, when you’re more
    familiar with neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中最常用的学习算法被称为*反向传播*。给定期望和实际输出，该算法会*反向传播*每个神经元的*误差*，并相应地调整每个神经元连接的内部状态，一次一层，从输出到输入（反向）；参见[图1.13](#ch01fig13)。每个训练示例都会使反向传播“调整”每个神经元的状
    态和连接，以减少网络为这对特定输入和期望输出产生的误差量。这是对反向传播算法工作原理的高级描述；我们将在接下来的章节中更深入地探讨，届时你将更熟悉神经网络。
- en: Figure 1.13\. Forward step (feeding input) and backward step (backpropagating
    an error)
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.13\. 前向步骤（输入）和反向步骤（反向传播误差）
- en: '![](Images/01fig13_alt.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/01fig13_alt.jpg)'
- en: 'Now that you understand how neural nets learn, you need to decide how to plug
    in to the search engine. Search engines can receive data to be indexed continuously;
    because new content is added, existing content is updated or even deleted. Although
    it’s relatively easy and quick to support this process in a search engine, many
    ML algorithms create *static* models that can’t be adapted quickly as data changes.
    A typical development workflow for an ML task involves these steps:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了神经网络是如何学习的，您需要决定如何将其连接到搜索引擎。搜索引擎可以持续接收要索引的数据；由于新内容被添加，现有内容被更新或甚至被删除。尽管在搜索引擎中支持此过程相对容易且快捷，但许多机器学习算法创建的模型是*静态的*，无法快速适应数据的变化。机器学习任务的典型开发工作流程包括以下步骤：
- en: '**1**.  Choosing and gathering data to be used as the training set'
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 选择和收集用于训练集的数据'
- en: '**2**.  Keeping some portions of the training set apart for evaluation and
    tuning (test and cross-validation sets)'
  id: totrans-294
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 将训练集的一部分保留用于评估和调整（测试集和交叉验证集）'
- en: '**3**.  Training a few ML models according to algorithms (feed-forward neural
    networks, support vector machines, and so on) and hyperparameters (for example,
    the number of layers and the number of neurons in each layer for neural networks)'
  id: totrans-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 根据算法（前馈神经网络、支持向量机等）和超参数（例如，神经网络的层数和每层的神经元数量）训练几个机器学习模型'
- en: '**4**.  Evaluating and tuning the model over test and cross-validation sets'
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. 在测试集和交叉验证集上评估和调整模型'
- en: '**5**.  Choosing the best-performing model and using it to solve the desired
    task'
  id: totrans-297
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**. 选择表现最佳的模型并使用它来解决所需的任务'
- en: 'As you can see, this process aims to generate a computational model to be used
    to solve a certain task or problem by using training data that is static; updates
    to the training sets (added or modified inputs and outputs) of such models often
    require the entire sequence of steps to be repeated. This conflicts with systems
    like search engines that deal with a constant stream of new data. For example,
    the search engine for an online newspaper will be updated with many different
    news items every day; you need to take this into account when architecting a neural
    search system. Neural networks are ML models: you may need to retrain the model
    or come up with solutions to allow your neural network to perform *online learning*
    (not require retraining).^([[8](#ch01fn08)])'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，此过程旨在生成一个计算模型，通过使用静态的训练数据来解决特定任务或问题；此类模型的训练集（添加或修改的输入和输出）更新通常需要重复整个步骤序列。这与处理不断流数据流如搜索引擎的系统相冲突。例如，在线报纸的搜索引擎每天都会更新许多不同的新闻条目；在架构神经网络搜索系统时，需要考虑这一点。神经网络是机器学习模型：您可能需要重新训练模型或提出解决方案，以允许您的神经网络执行*在线学习*（不需要重新训练）。^([[8](#ch01fn08)])
- en: ⁸
  id: totrans-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-300
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See, for example, Andrey Besedin et al., “Evolutive deep models for online
    learning on data streams with no storage” (Workshop on Large-scale Learning from
    Data Streams in Evolving Environments, 2017), [http://mng.bz/K14O](http://mng.bz/K14O);
    and Doyen Sahoo et al., “Online Deep Learning: Learning Deep Neural Networks on
    the Fly,” [https://arxiv.org/pdf/1711.03705.pdf](https://arxiv.org/pdf/1711.03705.pdf).'
  id: totrans-301
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，参见Andrey Besedin等人，“用于无存储数据流在线学习的演化深度模型”（在演变环境中的大规模数据流学习研讨会，2017年），[http://mng.bz/K14O](http://mng.bz/K14O)；以及Doyen
    Sahoo等人，“在线深度学习：实时学习深度神经网络”，[https://arxiv.org/pdf/1711.03705.pdf](https://arxiv.org/pdf/1711.03705.pdf)。
- en: 'Think of the evolution across time of the meaning of certain English words.
    For example, the word “cell” today commonly refers to mobile phones or cells from
    the biological perspective; before mobile phones were invented, the word “cell”
    primarily referred to either biological cells or ... prisons! Some concepts are
    tightly bound to words only in specific time windows: political offices change
    every few years, so Barack Obama was President of the United States between 2009
    and 2017, whereas the term “President of the United States” referred to John Fitzgerald
    Kennedy between 1961 and 1963\. If you think about the books contained in a library
    archive, how many of them contain the phrase “President of the United States”?
    They will rarely relate to the same person, because of the different times at
    which they were written.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 想想某些英语单词随时间的演变。例如，今天“cell”这个词通常指手机或从生物学的角度来看的细胞；在手机发明之前，“cell”这个词主要指生物细胞或...监狱！一些概念仅与特定时间窗口内的词语紧密相关：政治职务每隔几年就会改变，所以巴拉克·奥巴马在2009年至2017年期间是美国总统，而“美国总统”这个术语在1961年至1963年期间指的是约翰·菲茨杰拉德·肯尼迪。如果你考虑图书馆档案中的书籍，有多少包含“美国总统”这个短语？由于它们写作的时间不同，它们很少会与同一个人相关。
- en: I mentioned that neural networks can be used to generate *word vectors* that
    capture word semantics so that words with similar meanings will have word vectors
    close to one another. What do you expect to happen to the word vector of “President
    of the USA” if you train the model over news articles from the 1960s and compare
    it with word vectors generated by a model trained on news articles from 2009?
    Will the word vector “Barack Obama” from the latter model be placed close to the
    word vector “President of the USA” from the former one? Probably not, unless you
    instruct the neural network how to deal with the evolution of words over time.^([[9](#ch01fn09)])
    On the other hand, common search engines can easily deal with queries like “President
    of the USA” and return search results that contain such a phrase, regardless of
    when they were ingested into the inverted index.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到神经网络可以用来生成*词向量*，这些向量能够捕捉词义，使得具有相似意义的词会有接近的词向量。如果你在1960年代的新闻文章上训练模型，并将其与在2009年的新闻文章上训练的模型生成的词向量进行比较，你期望“美国总统”这个词向量会发生什么变化？后一个模型中的“巴拉克·奥巴马”词向量会被放置在先一个模型中的“美国总统”词向量附近吗？可能不会，除非你指导神经网络如何处理词语随时间演变的问题.^([[9](#ch01fn09)])
    另一方面，常见的搜索引擎可以轻松处理像“美国总统”这样的查询，并返回包含此类短语的搜索结果，无论它们何时被纳入倒排索引。
- en: ⁹
  id: totrans-304
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-305
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See, for example, Zijun Yao et al., “Dynamic Word Embeddings for Evolving Semantic
    Discovery” (International Conference on Web Search and Data Mining, 2018), [https://arxiv.org/abs/1703.00607](https://arxiv.org/abs/1703.00607).
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，参见Zijun Yao等人，“用于演化语义发现的动态词嵌入”（国际网络搜索与数据挖掘会议，2018年），[https://arxiv.org/abs/1703.00607](https://arxiv.org/abs/1703.00607)。
- en: 1.11\. The promises of neural search
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.11. 神经搜索的承诺
- en: Neural search is about integrating DL and deep neural networks into search at
    different stages. DL’s ability to capture deep semantics lets us obtain relevant
    models and ranking functions that adapt well to underlying data. Deep neural networks
    can learn image representations that give surprisingly good results in image search.
    Simple similarity measures like cosine distance can be applied to DL-generated
    representations of data to capture semantically similar words, sentences, paragraphs,
    and so on; this has a number of applications, such as in the text analysis phase
    and in recommending similar documents. At the same time, deep neural networks
    can do more than “just” learn representations; they can learn to generate or translate
    text, and how to optimize search engine performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 神经搜索涉及将深度学习和深度神经网络整合到搜索的不同阶段。深度学习捕捉深层语义的能力使我们能够获得与底层数据适应良好的相关模型和排名函数。深度神经网络可以学习图像表示，这在图像搜索中给出了令人惊讶的好结果。简单的相似度度量，如余弦距离，可以应用于深度学习生成的数据表示，以捕获语义上相似的单词、句子、段落等；这有许多应用，例如在文本分析阶段和在推荐相似文档时。同时，深度神经网络不仅能“仅仅”学习表示；它们还可以学习生成或翻译文本，以及如何优化搜索引擎的性能。
- en: As you’ll see throughout the book, a search system is made up of different components
    playing together. The most obvious parts are ingesting data into the search engine
    and searching for it. Neural networks can be used during indexing to enhance the
    data right before it enters the inverted index, or they can be used to broaden
    or specify the scope of a search query to provide a larger number of results or
    more-precise results. But neural networks can also be used to make smart suggestions
    to users to help them type queries or to translate their search queries under
    the hood and make a search engine work with multiple languages.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在整本书中将会看到的，搜索系统是由不同组件协同作用而成的。最明显的部分是将数据导入搜索引擎和搜索数据。神经网络可以在索引过程中用于增强数据，在它进入倒排索引之前，或者它们可以用于扩展或指定搜索查询的范围，以提供更多结果或更精确的结果。但神经网络还可以用于向用户提供智能建议，帮助他们输入查询，或者在幕后翻译他们的搜索查询，使搜索引擎能够处理多种语言。
- en: All this sounds awesome, but you can’t throw neural networks at a search engine
    and expect it to become automagically perfect. Every decision has to be made in
    context; and neural networks have some limitations, including the cost of training,
    upgrading models, and more. But applying neural search to a search engine is a
    great way to make it better for users. It also makes for a fascinating journey
    for the search engineers, who get to explore the beauty of neural networks.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些听起来都很棒，但你不能把神经网络扔给搜索引擎并期望它自动变得完美。每个决策都必须在特定情境下做出；神经网络也有一些局限性，包括训练成本、模型升级等。但将神经搜索应用于搜索引擎是使其更好地服务于用户的一个很好的方法。这也为搜索引擎工程师提供了一个迷人的旅程，他们得以探索神经网络的美丽。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Search is a hard problem: common approaches to information retrieval come with
    limitations and disadvantages, and both users and search engineers can have a
    difficult time making things work as expected.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索是一个难题：常见的信息检索方法都存在局限性和缺点，用户和搜索引擎工程师都可能难以使事情按预期工作。
- en: Text analysis is an important task in search, during both the indexing and search
    phases, because it prepares the data to be stored in inverted indexes and has
    a significant influence on the effectiveness of a search engine.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析是搜索过程中的一个重要任务，无论是在索引阶段还是在搜索阶段，因为它为存储在倒排索引中的数据做准备，并对搜索引擎的有效性产生重大影响。
- en: Relevance is the fundamental measure of how well the search engine responds
    to users’ information needs. Some information retrieval models can give a standardized
    measure of the importance of results with respect to queries, but there’s no silver
    bullet. Context and opinions can vary significantly among users, and therefore
    measuring relevance needs to be a continuous focus for search engineers.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性是衡量搜索引擎如何响应用户信息需求的基本指标。一些信息检索模型可以给出关于查询结果重要性的标准化度量，但没有一劳永逸的解决方案。用户之间的情境和观点可能存在很大差异，因此衡量相关性需要成为搜索引擎工程师持续关注的焦点。
- en: Deep learning is a field of machine learning that uses deep neural networks
    to learn (deep) representations of content (text like words, sentences, and paragraphs,
    but also images) that can capture semantically relevant similarity measures.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个领域，它使用深度神经网络来学习（深层的）内容（如文本中的单词、句子和段落，以及图像）的表示，这些表示可以捕捉语义上相关的相似度度量。
- en: Neural search stands as a bridge between search and deep neural networks, with
    the goal of using deep learning to help improve different tasks related to search.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经搜索作为搜索和深度神经网络之间的桥梁，旨在利用深度学习来帮助改善与搜索相关的不同任务。
- en: Chapter 2\. Generating synonyms
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章\. 生成同义词
- en: '*This chapter covers*'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Why and how synonyms are used in search
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么以及如何在搜索中使用同义词
- en: A brief introduction to Apache Lucene
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Lucene简介
- en: Fundamentals of feed-forward neural networks
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络的 fundamentals
- en: Using a word2vec algorithm
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用word2vec算法
- en: Generating synonyms using word2vec
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用word2vec生成同义词
- en: '[Chapter 1](kindle_split_012.xhtml#ch01) gave you a high-level overview of
    the kinds of possibilities that open up when deep learning is applied to search
    problems. Those possibilities include using deep neural networks to search for
    images via a text query based on its content, generating text queries in natural
    language, and so on. You also learned about the basics of search engines and how
    they conduct searches from queries and deliver relevant results. You’re now ready
    to start applying deep neural networks to solve search problems.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](kindle_split_012.xhtml#ch01)为你提供了当深度学习应用于搜索问题时，所开启的可能性的高级概述。这些可能性包括使用深度神经网络通过基于内容的文本查询搜索图像，生成自然语言的文本查询等。你还了解了搜索引擎的基本知识以及它们是如何从查询中进行搜索并返回相关结果的。你现在可以开始应用深度神经网络来解决搜索问题。'
- en: In this chapter, we’ll begin with a shallow (not deep) neural network that can
    help you identify when two words are similar in semantics. This seemingly easy
    task is crucial for giving a search engine the ability to understand language.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从浅层（非深层）神经网络开始，这种神经网络可以帮助你识别两个词在语义上是否相似。这项看似简单的任务对于赋予搜索引擎理解语言的能力至关重要。
- en: 'In information retrieval, a common technique to improve the number of relevant
    results for a query is to use *synonyms*. Synonyms allow you to expand the number
    of potential ways a query or piece of indexed document is expressed. For example,
    you can express the sentence “I like living in Rome” as “I enjoy living in the
    Eternal City”: the terms “living” and “enjoying” as well as “Rome” and “the Eternal
    City” are semantically similar, so the information conveyed by both sentences
    is mostly the same. Synonyms could help with the problem discussed in [chapter
    1](kindle_split_012.xhtml#ch01), of a librarian and a student looking for a book
    understanding one another. That’s because using synonyms allows people to express
    the same concept in different ways—and still retrieve the same search results!'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息检索中，提高查询相关结果数量的常见技术是使用*同义词*。同义词允许你扩展查询或索引文档表达的可能方式数量。例如，你可以将句子“我喜欢住在罗马”表达为“我喜欢住在永恒之城”：术语“居住”和“喜欢”以及“罗马”和“永恒之城”在语义上是相似的，因此两个句子所传达的信息大部分是相同的。同义词可以帮助解决[第一章](kindle_split_012.xhtml#ch01)中讨论的问题，即图书管理员和学生寻找书籍时相互理解。这是因为使用同义词允许人们以不同的方式表达相同的概念——并且仍然检索到相同的搜索结果！
- en: 'In this chapter, we’ll start working with synonyms using word2vec, one of the
    most common neural network–based algorithms for learning word representations.
    Learning about word2vec will give you a closer look at how neural networks work
    in practice. To do this, you’ll first get an understanding of how *feed-forward*
    neural networks work. Feed-forward neural networks, one of the most basic types
    of neural networks, are the basic building blocks of deep learning. Next, you’ll
    learn about two feed-forward neural network architectures: skip-gram and continuous-bag-of-words
    (CBOW). They make it possible to learn when two words are similar in meaning,
    and hence they’re a good fit for understanding whether two given words are synonyms.
    You’ll see how to apply them to improve search engine recall by helping the search
    engine avoid missing relevant search results.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始使用word2vec，这是一种最常见的基于神经网络的算法，用于学习词表示。了解word2vec将让你更深入地了解神经网络在实际中的工作原理。为此，你首先需要了解*前馈*神经网络是如何工作的。前馈神经网络是神经网络中最基本类型之一，是深度学习的基本构建块。接下来，你将学习关于两种前馈神经网络架构：skip-gram和连续词袋（CBOW）。它们使得学习两个词在意义上相似成为可能，因此非常适合理解两个给定的词是否为同义词。你将看到如何应用它们，通过帮助搜索引擎避免错过相关搜索结果来提高搜索引擎的召回率。
- en: Finally, you’ll measure how much the search engine can be enhanced this way
    and what trade-offs you’ll need to consider for production systems. Understanding
    these costs and benefits is important when deciding when and where to apply these
    techniques to real-life scenarios.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将衡量这种方式的搜索引擎可以提升多少，以及对于生产系统需要考虑哪些权衡。在决定何时何地将这些技术应用于实际场景时，理解这些成本和收益非常重要。
- en: 2.1\. Introduction to synonym expansion
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 同义词扩展简介
- en: 'In the previous chapter, you saw how important it is to have good algorithms
    for performing text analysis: these algorithms specify the way text is broken
    into smaller fragments or terms. When it comes to executing a query, the terms
    generated at indexing time need to match those extracted from the query. This
    matching allows a document to be found and then appear in the search results.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你看到了进行文本分析的良好算法是多么重要：这些算法指定了文本被分解成更小的片段或术语的方式。当执行查询时，索引时生成的术语需要与从查询中提取的术语匹配。这种匹配允许找到文档，并随后出现在搜索结果中。
- en: One of the most frequent hurdles that prevent matching is the fact that people
    can express a concept in multiple different ways. For example, “going for a walk
    in the mountains” can be also expressed using the words “hiking” or “trekking.”
    If the author of the text to be indexed uses “hike,” but the user doing the search
    enters “trek,” the user won’t find the document. This is why you need to make
    the search engine aware of synonyms.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 阻止匹配的最常见障碍之一是人们可以用多种不同的方式表达一个概念。例如，“去山里散步”也可以用“hiking”或“trekking”来表达。如果要索引的文本的作者使用“hike”，而进行搜索的用户输入“trek”，用户将找不到该文档。这就是为什么你需要让搜索引擎意识到同义词的存在。
- en: 'I’ll explain how you can use a technique called *synonym expansion* to make
    it possible to express the same information need in several ways. Although synonym
    expansion is a popular technique, it has some limitations: in particular, the
    need to maintain a dictionary of synonyms that will likely change over time and
    that often isn’t perfectly suited to the data to be indexed (such dictionaries
    are often obtained from publicly available data). You’ll see how you can use algorithms
    like word2vec to learn word representations that help generate synonyms accurately
    based on the data that needs to be indexed.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释如何使用一种称为*同义词扩展*的技术，使其能够以多种方式表达相同的信息需求。尽管同义词扩展是一种流行的技术，但它有一些局限性：特别是需要维护一个同义词字典，这个字典可能会随时间变化，并且通常并不完全适合要索引的数据（这类字典通常来源于公开可用的数据）。你将看到如何使用像word2vec这样的算法来学习词表示，这些表示有助于根据需要索引的数据准确生成同义词。
- en: By the end of the chapter, you’ll have a search engine that can use a neural
    network to generate synonyms that can then be used to *decorate* the text to be
    indexed. To show how this works, we’ll use an example in which a user sends the
    query “music is my aircraft” through the search engine user interface. (I’ll explain
    why the user is using that particular query in a moment.) [Figure 2.1](#ch02fig01)
    shows what you’ll end up with.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将拥有一个能够使用神经网络生成同义词的搜索引擎，这些同义词可以用来*装饰*要索引的文本。为了展示其工作原理，我们将使用一个例子，其中用户通过搜索引擎用户界面发送查询“music
    is my aircraft”。（我稍后会解释用户为什么使用这个特定的查询。）[图2.1](#ch02fig01)展示了你最终会得到的结果。
- en: Figure 2.1\. Synonym expansion at search time, with a neural network
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1\. 搜索时的同义词扩展，使用神经网络
- en: '![](Images/02fig01_alt.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig01_alt.jpg)'
- en: 'Here are the major steps, as shown in the figure. In the search engine, the
    query is first processed by the text analysis pipeline. A *synonym filter* in
    the pipeline uses a neural network to generate synonyms. In the example, the neural
    network returns “airplane,” “aeroplane,” and “plane” as synonyms of “aircraft.”
    The generated synonyms are then used together with the tokens from the user query
    to find matches in the inverted index. Finally, the search results are collected.
    That’s the big picture. Don’t worry: we’ll now go through each step in detail.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，以下是主要步骤。在搜索引擎中，查询首先由文本分析管道处理。管道中的*同义词过滤器*使用神经网络生成同义词。在示例中，神经网络返回“airplane”、“aeroplane”和“plane”作为“aircraft”的同义词。生成的同义词随后与用户查询中的标记一起用于在倒排索引中找到匹配项。最后，收集搜索结果。这就是整体情况。别担心：我们现在将详细解释每个步骤。
- en: 2.1.1\. Why synonyms?
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 为什么需要同义词？
- en: '*Synonyms* are words that differ in spelling and pronunciation, but that have
    the same or a very close meaning. For example, “aircraft” and “airplane” are both
    synonyms of the word “plane.” In information retrieval, it’s common to use synonyms
    to decorate text in order to increase the probability that an appropriate query
    will match. Yes, we’re talking about probability here, because we can’t anticipate
    all the possible ways of expressing an information need. This technique isn’t
    a silver bullet that will let you *understand* all user queries, but it will reduce
    the number of queries that give too few or zero results.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*同义词*是拼写和发音不同，但意义相同或非常接近的词。例如，“aircraft”和“airplane”都是“plane”的同义词。在信息检索中，通常使用同义词来装饰文本，以提高适当查询匹配的概率。是的，我们在这里谈论的是概率，因为我们无法预测所有可能表达信息需求的方式。这项技术并不是一个万能的银弹，可以让你*理解*所有用户查询，但它将减少给出太多或少于零结果的查询数量。'
- en: 'Let’s look at an example where synonyms can be useful. This has probably happened
    to you: you vaguely remember a short piece of a song, or you remember something
    about the meaning of a lyric, but not the exact wording from the song you have
    in mind. Suppose you liked a song whose chorus was along the lines of, “Music
    is my ... *something*.” What was it? A car? A boat? A plane? Now imagine you have
    a system that collects song lyrics, and you want users to be able to search through
    it. If you have synonym expansion enabled in the search engine, searching for
    “music is my plane” will yield the phrase you’re looking for: “music is my aeroplane”!
    In this case, using synonyms lets you find a relevant document (the song “Aeroplane”
    by Red Hot Chili Peppers) using a fragment and an incorrect word. Without synonym
    expansion, it wouldn’t have been possible to retrieve this relevant response with
    queries like “music is my boat,” “music is my plane,” and “music is my car.”'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个同义词可能有用的例子。这可能是你经历过的事情：你模糊地记得一首歌的片段，或者你记得关于歌词含义的一些内容，但不是你心中那首歌的确切歌词。假设你喜欢一首副歌大致是这样的，“音乐是我的...
    *某物*。”那是什么？一辆车？一艘船？一架飞机？现在想象你有一个收集歌词的系统，你希望用户能够搜索它。如果你在搜索引擎中启用了同义词扩展，搜索“音乐是我的飞机”将会找到你想要的短语：“music
    is my aeroplane”！在这种情况下，使用同义词让你能够通过一个片段和一个错误的词找到相关文档（红辣椒乐队的歌曲“Aeroplane”）。如果没有同义词扩展，使用“music
    is my boat”、“music is my plane”和“music is my car”这样的查询将无法检索到这个相关响应。
- en: This is considered an improvement in recall. *Recall*, briefly mentioned in
    [chapter 1](kindle_split_012.xhtml#ch01), is a number between 0 and 1 equal to
    the number of documents that are retrieved and relevant, divided by the number
    of relevant documents. If none of the retrieved documents are relevant, recall
    is 0\. And if all the retrieved documents are relevant, recall is 1.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这被认为是对召回率的改进。*召回率*，在[第1章](kindle_split_012.xhtml#ch01)中简要提及，是一个介于0和1之间的数字，等于检索到的相关文档数除以相关文档总数。如果检索到的所有文档都不相关，召回率为0。如果所有检索到的文档都相关，召回率为1。
- en: 'The overall idea of synonym expansion is that when the search engine receives
    a stream of terms, it can enrich them by adding their synonyms, if they exist,
    at the same position. In the “Aeroplane” example, synonyms of the query terms
    have been expanded: they were silently decorated with the word “aeroplane” at
    the same position as “plane” in the stream of text; see [figure 2.2](#ch02fig02).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词扩展的整体思路是，当搜索引擎接收到一系列术语时，如果存在同义词，它可以在相同位置添加这些同义词来丰富它们。在“飞机”的例子中，查询术语的同义词已经扩展：它们在文本流中“plane”一词的相同位置被无声地装饰上了“aeroplane”一词；参见[图2.2](#ch02fig02)。
- en: Figure 2.2\. Synonym expansion graph
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2. 同义词扩展图
- en: '![](Images/02fig02_alt.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig02_alt.jpg)'
- en: You can apply the same technique during indexing of the “Aeroplane” lyrics.
    Expanding synonyms at indexing time will make indexing slightly slower (because
    of the calls to word2vec), and the index will inevitably be bigger (because it
    will contain more terms to store). On the plus side, searching will be faster
    because the word2vec call won’t happen during search. The decision of whether
    to do index-time or search-time synonym expansion may have a noticeable impact
    on the performance of the system as its size and load grow.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在“飞机”歌词的索引过程中应用相同的技巧。在索引时扩展同义词会使索引稍微慢一些（因为调用word2vec），并且索引必然更大（因为它将包含更多要存储的术语）。有利的一面是，搜索将会更快，因为搜索期间不会发生word2vec调用。是否在索引时或搜索时进行同义词扩展的决定可能会对系统性能产生明显影响，随着其规模和负载的增长。
- en: Now that you’ve seen why synonyms are useful in the context of search, let’s
    look at how to implement synonym expansion, first by using common techniques and
    then by using word2vec. This will help you appreciate the advantages of using
    the latter rather than the former.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了同义词在搜索上下文中的有用性，让我们来看看如何实现同义词扩展，首先是通过使用常见技术，然后是通过使用 word2vec。这将帮助你欣赏使用后者而不是前者所带来的优势。
- en: 2.1.2\. Vocabulary-based synonym matching
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 基于词汇的同义词匹配
- en: 'Let’s start by seeing how to implement a search engine with synonym expansion
    enabled at indexing time. The simplest and most common approach for implementing
    synonyms is based on feeding the search engine a vocabulary that contains the
    mapping between all the words and their related synonyms. Such a vocabulary can
    look like a table, where each key is a word and the corresponding values are its
    synonyms:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从了解如何在索引时启用同义词扩展的搜索引擎实现开始。实现同义词的最简单和最常见的方法是基于向搜索引擎提供包含所有单词及其相关同义词之间映射的词汇表。这样的词汇表可以看起来像一张表，其中每个键是一个单词，相应的值是其同义词：
- en: '[PRE1]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Imagine that you feed the lyrics of “Aeroplane” into the search engine for
    indexing, and you use synonym expansion with the previous vocabulary. Let’s pick
    the chorus of the song—“music is my aeroplane”—and see how synonym expansion handles
    it. You have a simple text analysis pipeline composed of a tokenizer, which creates
    a token every time it encounters whitespace, resulting in creating a token for
    each of the words in the sentence. The index-time text analysis pipeline will
    thus create these tokens. Then you’ll use a *token filter* for synonym expansion:
    for each received token, it will look at the vocabulary of synonyms and see if
    any of the keywords (“aeroplane,” “boat,” “car”) is equal to the token text. The
    posting list for the fragment “music is my aeroplane” (sorted in ascending alphabetical
    order) will look like [table 2.1](#ch02table01).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你将“飞机”这首歌的歌词输入搜索引擎进行索引，并使用之前提供的词汇表进行同义词扩展。让我们挑选这首歌的副歌——“music is my aeroplane”——看看同义词扩展是如何处理它的。你有一个简单的文本分析管道，由一个分词器组成，每次遇到空白时都会创建一个标记，从而为句子中的每个单词创建一个标记。因此，索引时的文本分析管道将创建这些标记。然后，你将使用一个*标记过滤器*进行同义词扩展：对于每个接收到的标记，它将查看同义词词汇表，看看是否有任何关键词（“飞机”、“船”、“汽车”）等于标记文本。片段“music
    is my aeroplane”的倒排列表（按升序字母顺序排序）将看起来像[表 2.1](#ch02table01)。
- en: Table 2.1\. Posting list for the fragment “music is my aeroplane”
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.1\. 片段“music is my aeroplane”的倒排列表
- en: '| Term | Document(position) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 文档(位置) |'
- en: '| --- | --- |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| aeroplane | 1(12,17) |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | 1(12,17) |'
- en: '| aircraft | 1(12,17) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | 1(12,17) |'
- en: '| airplane | 1(12,17) |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | 1(12,17) |'
- en: '| is | 1(6,8) |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 1(6,8) |'
- en: '| music | 1(0,5) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 音乐 | 1(0,5) |'
- en: '| my | 1(9,11) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 我 | 1(9,11) |'
- en: '| plane | 1(12,17) |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | 1(12,17) |'
- en: This particular posting list also records information about the position of
    the occurrence of a term in a specific document. This information helps you visualize
    the fact that the terms “plane,” “airplane,” and “aircraft,” which weren’t included
    in the original text fragment, were added to the index with the same position
    as information attached to the original term (“aeroplane”).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的倒排列表还记录了关于特定文档中术语出现位置的信息。这些信息帮助你可视化这样一个事实：术语“飞机”、“飞机”和“飞机”，它们没有包含在原始文本片段中，被添加到索引中，位置与附加到原始术语（“飞机”）的信息相同。
- en: 'You can record the *positions* of the terms in an inverted index in order to
    reconstruct the order in which a term appears in the text of a document. If you
    look at the inverted index table and pick the terms that have the lower positions
    in ascending order, you’ll get “music is my aeroplane/aircraft/airplane/plane.”
    The synonyms can be seamlessly replaced with one another, so, in the index, you
    can imagine having four different pieces of text: “music is my aeroplane,” “music
    is my aircraft,” “music is my airplane,” and “music is my plane.” It’s important
    to emphasize that although you found four different forms in which the sentence
    can be indexed and searched, if any of them matches, only one document will be
    returned by a search engine: they all reference document 1 in the posting list.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在倒排索引中记录术语的 *位置*，以便重建术语在文档文本中出现的顺序。如果你查看倒排索引表并选择具有较低位置的术语，按升序排列，你会得到“music
    is my aeroplane/aircraft/airplane/plane。”同义词可以无缝替换，因此，在索引中，你可以想象有四段不同的文本：“music
    is my aeroplane”，“music is my aircraft”，“music is my airplane”，“music is my plane。”重要的是要强调，尽管你找到了句子可以索引和搜索的四种不同形式，但如果其中任何一个匹配，搜索引擎只会返回一个文档：它们都引用了帖子列表中的文档1。
- en: Now that you understand how synonyms can be indexed into the search engine,
    you’re ready to try things out and build your first Apache Lucene–based search
    engine that indexes lyrics, setting up proper text analysis with synonym expansion
    at indexing time.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了同义词如何被索引到搜索引擎中，你就可以尝试一些事情并构建你的第一个基于Apache Lucene的搜索引擎，该搜索引擎索引歌词，并在索引时设置适当的文本分析以及同义词扩展。
- en: '|  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Going forward, I’ll use *Lucene* and *Apache Lucene* interchangeably, but the
    proper trademarked name is Apache Lucene.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我会交替使用 *Lucene* 和 *Apache Lucene*，但正确的商标名称是Apache Lucene。
- en: '|  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: A quick look at Apache Lucene
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速了解Apache Lucene
- en: I’ll briefly introduce Lucene before diving into synonym expansion. This will
    allow you to focus more on the concepts rather than on the Lucene API and implementation
    details.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨同义词扩展之前，我将简要介绍Lucene。这将使你能够更多地关注概念，而不是Lucene API和实现细节。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Obtaining Apache Lucene**'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取Apache Lucene**'
- en: 'You can download the latest release of Apache Lucene at [https://lucene.apache.org/core/mirrors-core-latest-redir.html?](https://lucene.apache.org/core/mirrors-core-latest-redir.html?).
    You can download either a binary package (.tgz or .zip) or the source release.
    The binary distribution is recommended if you just want to use Lucene within your
    own project; the .tgz/.zip package contains the JAR files of the Lucene components.
    Lucene is made of various artifacts: the only mandatory one is `lucene-core`,
    and the others are optional parts that you can use if needed. You can find the
    basics you need to know to get started with Lucene in the official documentation,
    available at [https://lucene.apache.org/core/7_4_0/index.html](https://lucene.apache.org/core/7_4_0/index.html).
    The source package is suitable for developers who want to look at the code or
    enhance it. (Patches for improvements, new features, bug fixes, documentation,
    and so on are always welcome at [https://issues.apache.org/jira/browse/LUCENE](https://issues.apache.org/jira/browse/LUCENE).)
    If you use a build tool like Maven, Ant, or Gradle, you can include Lucene in
    your project, because all the components are released in public repositories like
    Maven Central ([http://mng.bz/vN1x](http://mng.bz/vN1x)).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://lucene.apache.org/core/mirrors-core-latest-redir.html?](https://lucene.apache.org/core/mirrors-core-latest-redir.html?)下载Apache
    Lucene的最新版本。你可以下载二进制包（.tgz或.zip）或源代码版本。如果你只是想在你的项目中使用Lucene，建议使用二进制分发；.tgz/.zip包包含Lucene组件的JAR文件。Lucene由各种工件组成：唯一必需的是`lucene-core`，其他都是可选部分，如果你需要可以使用。你可以在官方文档中找到开始使用Lucene所需的基本知识，该文档可在[https://lucene.apache.org/core/7_4_0/index.html](https://lucene.apache.org/core/7_4_0/index.html)找到。源代码包适合想要查看或增强代码的开发者。（改进、新功能、错误修复、文档等补丁始终欢迎在[https://issues.apache.org/jira/browse/LUCENE](https://issues.apache.org/jira/browse/LUCENE)上提交。）如果你使用Maven、Ant或Gradle等构建工具，你可以在你的项目中包含Lucene，因为所有组件都发布在公共仓库中，如Maven
    Central ([http://mng.bz/vN1x](http://mng.bz/vN1x))。
- en: '|  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Apache Lucene is an open source search library written in Java, licensed under
    Apache License 2\. In Lucene, the main entities to be indexed and searched are
    represented by `Document`s. A `Document`, depending on your use case, can represent
    anything: a page, a book, a paragraph, an image, and so on. Whatever it is, that’s
    what you’ll get in your search results. A `Document` is composed of a number of
    `Field`s, which can be used to capture different portions of the `Document`. For
    example, if your document is a web page, you can think of having a separate `Field`
    for the page title, the page contents, the page size, the creation time, and so
    on. The main reasons for the existence of fields are that you can do the following:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Lucene 是一个用 Java 编写的开源搜索库，根据 Apache License 2.0 许可。在 Lucene 中，要索引和搜索的主要实体由
    `Document`s 表示。`Document` 依赖于你的用例，可以代表任何东西：一页、一本书、一个段落、一张图片等等。无论是什么，你都会在搜索结果中得到它。`Document`
    由多个 `Field`s 组成，可以用来捕获 `Document` 的不同部分。例如，如果你的文档是一个网页，你可以想象有一个单独的字段用于页面标题、页面内容、页面大小、创建时间等等。字段存在的主要原因是你可以做以下事情：
- en: Configure per-field text analysis pipelines
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置每个字段的文本分析管道
- en: Configure indexing options, such as whether to store in the posting list the
    term positions or the value of the original text each term refers to
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置索引选项，例如是否在 postings 列表中存储每个术语的位置或每个术语所引用的原始文本的值
- en: 'A Lucene search engine can be accessed via a `Directory`: a list of files where
    the inverted indexes (and other data structures used, for example, to record positions)
    are persisted. A view on a `Directory` for reading an inverted index can be obtained
    by opening an `IndexReader`:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `Directory` 访问 Lucene 搜索引擎：一个包含倒排索引（以及用于记录位置等的数据结构）的文件的列表。可以通过打开 `IndexReader`
    来获取对 `Directory` 的视图以读取倒排索引：
- en: '[PRE2]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Target path where the inverted indexes are stored on the filesystem**'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 存储倒排索引的文件系统上的目标路径**'
- en: '***2* Obtains a read-only view of the search engine via an IndexReader**'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通过 IndexReader 获取对搜索引擎的只读视图**'
- en: '***3* Opens a Directory on the target path**'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在目标路径上打开目录**'
- en: 'You can use an `IndexReader` to obtain useful statistics for an index, such
    as the number of documents currently indexed, or if there are any documents that
    have been deleted. You can also obtain statistics for a field or a particular
    term. And, if you know the *identifier* of the document you want to retrieve,
    you can get `Document`s from an `IndexReader` directly:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `IndexReader` 获取索引的有用统计信息，例如当前索引的文档数量，或者是否有被删除的文档。你也可以获取字段或特定术语的统计信息。如果你知道要检索的文档的
    *标识符*，你可以直接从 `IndexReader` 获取 `Document`s：
- en: '[PRE3]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: An `IndexReader` is needed in order to search, because it lets you read an index.
    Therefore, you need an `IndexReader` to create an `IndexSearcher`. An `IndexSearcher`
    is the entry point for performing search and collecting results; the queries that
    will be performed via an `IndexSearcher` will run on the index data, exposed by
    the `IndexReader`.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行搜索，需要一个 `IndexReader`，因为它允许你读取索引。因此，你需要一个 `IndexReader` 来创建一个 `IndexSearcher`。`IndexSearcher`
    是执行搜索和收集结果的入口点；将通过 `IndexSearcher` 执行的查询将在由 `IndexReader` 暴露的索引数据上运行。
- en: 'Without getting too much into coding queries programmatically, you can run
    a user-entered query using a `QueryParser`. You need to specify (search-time)
    text analysis when searching. In Lucene, the text analysis task is performed by
    implementing the `Analyzer` API. An `Analyzer` can be made up of a `Tokenizer`
    and, optionally, `TokenFilter` components; or you can use out-of-the-box implementations,
    as in this example:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入编程查询，你可以使用 `QueryParser` 运行用户输入的查询。在搜索时，你需要指定（搜索时）文本分析。在 Lucene 中，文本分析任务是通过实现
    `Analyzer` API 来执行的。一个 `Analyzer` 可以由一个 `Tokenizer` 和可选的 `TokenFilter` 组件组成；或者你可以使用现成的实现，就像这个例子一样：
- en: '[PRE4]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* Creates a query parser for the title field with a WhitespaceAnalyzer**'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用 WhitespaceAnalyzer 创建标题字段的查询解析器**'
- en: '***2* Parses the user-entered query and obtains a Lucene Query**'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 解析用户输入的查询并获取一个 Lucene 查询**'
- en: 'In this case, you tell the query parser to split tokens when it finds whitespace
    and run queries against the field named `title`. Suppose a user types in the query
    “+Deep +search.” You pass it to the `QueryParser` and obtain a Lucene `Query`
    object. Now you can run the query:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你告诉查询解析器在找到空白时拆分标记，并对名为 `title` 的字段运行查询。假设用户输入查询“+Deep +search。”你将其传递给
    `QueryParser` 并获得一个 Lucene `Query` 对象。现在你可以运行查询：
- en: '[PRE5]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1* Performs the query against the IndexSearcher, returning the first 10
    documents**'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 对IndexSearcher执行查询，返回前10个文档**'
- en: '***2* Iterates over the results**'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历结果**'
- en: '***3* Retrieves a ScoreDoc, which holds the returned document identifier and
    its score (given by the underlying retrieval model)**'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取一个ScoreDoc，它包含返回的文档标识符及其分数（由底层检索模型给出）**'
- en: '***4* Obtains a Document in which you can inspect fields using the document
    ID**'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取一个Document，你可以使用document ID来检查字段**'
- en: '***5* Outputs the value of the title field of the returned document**'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 输出返回文档的标题字段的值**'
- en: If you run this, you’ll get no results, because you haven’t indexed anything
    yet! Let’s fix this and examine how to index `Document`s with Lucene. First, you
    have to decide which fields to put into your documents and how their (index-time)
    text analysis pipelines should look. We’ll use books for this example. Assume
    you want to remove some useless words from the books’ contents while using a simpler
    text analysis pipeline for the title that doesn’t remove anything.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个，你将得到没有结果，因为你还没有索引任何内容！让我们解决这个问题，并检查如何使用Lucene索引`Document`s。首先，你必须决定将哪些字段放入你的文档中，以及它们的（索引时）文本分析管道应该如何看起来。我们将使用书籍作为这个例子。假设你想要从书籍的内容中删除一些无用的词，同时为标题使用一个简单的文本分析管道，不删除任何内容。
- en: Listing 2.1\. Building per-field analyzers
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1\. 构建每个字段的Analyzer
- en: '[PRE6]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* Sets up a map where the keys are the names of fields and the values are
    the Analyzers to be used for the fields**'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 设置一个映射，其中键是字段名称，值是要用于字段的Analyzers**'
- en: '***2* Creates a stopword list of the tokens to remove from the books’ contents
    while indexing**'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个停用词列表，用于在索引书籍内容时删除的标记**'
- en: '***3* Uses a StopAnalyzer with the given stopwords for the pages field**'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用带有给定停用词的StopAnalyzer对页面字段进行处理**'
- en: '***4* Uses a WhitespaceAnalyzer for the title field**'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用WhitespaceAnalyzer对标题字段进行处理**'
- en: '***5* Creates a per-field Analyzer, which also requires a default analyzer
    (EnglishAnalyzer, in this case) for any other field that may be added to a Document**'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 创建一个针对字段的Analyzer，这也需要一个默认的Analyzer（在这个例子中是EnglishAnalyzer）以供任何可能添加到文档的其他字段使用**'
- en: The inverted indexes for a Lucene-based search engine are written on disk in
    a `Directory` by an `IndexWriter` that will persist `Document`s according to an
    `IndexWriterConfig`. This config contains many options, but for you the most important
    bit is the required index-time analyzer. Once the `IndexWriter` is ready, you
    can create `Document`s and add `Field`s.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Lucene的搜索引擎的反向索引是通过`IndexWriter`在`Directory`上写入的，该`IndexWriter`将根据`IndexWriterConfig`持久化`Document`s。此配置包含许多选项，但对你来说最重要的部分是必需的索引时Analyzer。一旦`IndexWriter`就绪，你就可以创建`Document`s并添加`Field`s。
- en: Listing 2.2\. Adding documents to the Lucene index
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2\. 向Lucene索引添加文档
- en: '[PRE7]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1* Creates a configuration for indexing**'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建索引配置**'
- en: '***2* Creates an IndexWriter to write Documents into a Directory, based on
    an IndexWriterConfig**'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个IndexWriter，将Documents写入Directory，基于IndexWriterConfig**'
- en: '***3* Creates Document instances**'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建Document实例**'
- en: '***4* Adds Fields, each of which has a name, a value, and an option to store
    the value with the terms**'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 添加字段，每个字段都有一个名称、一个值以及一个选项，可以选择存储值与术语**'
- en: '***5* Adds Documents to the search engine**'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将文档添加到搜索引擎**'
- en: 'After you’ve added a few documents to the `IndexWriter`, you can persist them
    on the filesystem by issuing a `commit`. Until you do, new `IndexReader`s won’t
    see the added documents:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在你向`IndexWriter`添加了一些文档之后，你可以通过发出一个`commit`命令在文件系统上持久化它们：直到你这样做，新的`IndexReader`s将看不到添加的文档：
- en: '[PRE8]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Commits the changes**'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 提交更改**'
- en: '***2* Closes the IndexWriter (releases resources)**'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 关闭IndexWriter（释放资源）**'
- en: 'Run the search code again, and this is what you’ll get:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行搜索代码，你将得到以下结果：
- en: '[PRE9]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The code finds a match for the query “+Deep +search” and prints its title and
    score.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 代码找到查询“+Deep +search”的匹配项，并打印其标题和分数。
- en: Now that you’ve been introduced to Lucene, let’s get back to the topic of synonym
    expansion.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Lucene，让我们回到同义词扩展这个话题。
- en: Setting up a Lucene index with synonym expansion
  id: totrans-419
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置带有同义词扩展的Lucene索引
- en: 'You’ll first define the algorithms to use for text analysis at indexing and
    search time. Then, you’ll add some lyrics to an inverted index. In many cases,
    it’s a good practice to use the same tokenizer at both indexing and search time,
    so the text is split according to the same algorithm. This makes it easier for
    queries to match fragments of documents. You’ll start simple and set up the following:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先定义在索引和搜索时用于文本分析的算法。然后，你将一些歌词添加到倒排索引中。在许多情况下，使用索引和搜索时相同的分词器是一个好习惯，这样文本就会根据相同的算法进行分割。这使得查询匹配文档片段变得更容易。你将从简单开始，设置以下内容：
- en: A search-time `Analyzer` that uses a tokenizer that splits tokens when it encounters
    a whitespace character (also called a *whitespace tokenizer*)
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个搜索时`Analyzer`，它使用一个分词器，在遇到空白字符时分割标记（也称为*空白分词器*）
- en: An index-time `Analyzer` that uses the whitespace tokenizer and a synonym filter
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用空白分词器和同义词过滤器的索引时`Analyzer`
- en: The reason for this is that you don’t need synonym expansion at both query time
    and index time. For two synonyms to match, it’s sufficient to do expansion once.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是你在查询时间和索引时间都不需要同义词扩展。为了使两个同义词匹配，只需要进行一次扩展。
- en: Assuming you have the two synonyms “aeroplane” and “plane,” the following listing
    will build a text analysis chain that can take a term from an original token (for
    example, “plane”) and generate another term for its synonym (for example, “aeroplane”).
    Both the original and the new term will be generated.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有两个同义词“aeroplane”和“plane”，以下列表将构建一个文本分析链，可以从原始标记（例如，“plane”）中提取一个术语，并为它的同义词生成另一个术语（例如，“aeroplane”）。原始术语和新术语都将被生成。
- en: Listing 2.3\. Configuring synonym expansion
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3\. 配置同义词扩展
- en: '[PRE10]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Programmatically defines synonyms**'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 程序化定义同义词**'
- en: '***2* Creates a custom Analyzer, for indexing**'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个用于索引的自定义Analyzer**'
- en: '***3* Creates a synonym filter that receives terms from the whitespace tokenizer
    and expands synonyms according to a map word, ignoring case**'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个接收来自空白分词器的术语并按映射词扩展同义词的同义词过滤器**'
- en: '***4* Whitespace analyzer for search time**'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 搜索时的空白分析器**'
- en: This simplistic example creates a synonym vocabulary with just one entry. Normally,
    you’ll have more entries, or you’ll read them from an external file so you don’t
    have to write the code for each synonym.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子创建了一个只有一个条目的同义词词汇表。通常，你会有更多的条目，或者你会从外部文件中读取它们，这样你就不必为每个同义词编写代码。
- en: 'You’re just about ready to put some song lyrics into the index using the `indexTimeAnalyzer`.
    Before doing that, let’s look at how song lyrics are structured. Each song has
    an author, a title, a publication year, lyrics text, and so on. As I said earlier,
    it’s important to examine the data to be indexed, to see what kind of data you
    have and possibly come up with reasoned text analysis chains that you expect to
    work well on that data. Here’s an example:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 你差不多准备好使用`indexTimeAnalyzer`将一些歌曲歌词放入索引中。在这样做之前，让我们看看歌曲歌词的结构。每首歌曲都有一个作者、标题、出版年份、歌词文本等。正如我之前所说的，检查要索引的数据非常重要，以了解你有什么样的数据，并可能提出合理的文本分析链，你期望它在那些数据上工作得很好。以下是一个例子：
- en: '[PRE11]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Can you keep track of such a structure in a search engine? Would doing so be
    useful?
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在搜索引擎中跟踪这样的结构吗？这样做会有用吗？
- en: In most cases, it’s handy to keep a lightweight document structure, because
    each part of it conveys different semantics, and therefore different requirements
    in the way it’s hit by search. For example, the year will always be a numeric
    value; it makes no sense to use a whitespace tokenizer on it, because it’s unlikely
    that any whitespace will appear in that field. For all the other fields, you can
    probably use the `Analyzer` you defined earlier for indexing. Putting it all together,
    you’ll have multiple inverted indexes (one for each attribute) that address indexing
    of different parts of a document, all within the same search engine; see [figure
    2.3](#ch02fig03).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，保持一个轻量级的文档结构很方便，因为文档的每一部分都传达不同的语义，因此在对搜索的响应中会有不同的要求。例如，年份将始终是一个数值；在它上面使用空白分词器是没有意义的，因为在这个字段中不太可能出现任何空白。对于所有其他字段，你可能会使用之前定义的`Analyzer`进行索引。将所有这些放在一起，你将拥有多个倒排索引（每个属性一个），它们处理文档的不同部分，所有这些都在同一个搜索引擎中；参见[图2.3](#ch02fig03)。
- en: Figure 2.3\. Splitting portions of the text depending on the type of data
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3\. 根据数据类型分割文本的部分
- en: '![](Images/02fig03.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig03.jpg)'
- en: With Lucene, you can define a field for each of the attributes in the example
    (`author`, `title`, `year`, `album`, `text`). You specify that you want a separate
    `Analyzer` for the `year` field that doesn’t touch the value; for all the other
    values, it will use the previously defined `indexTimeAnalyzer` with synonym expansion
    enabled.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Lucene，您可以定义一个字段来表示示例中的每个属性（`author`、`title`、`year`、`album`、`text`）。您指定您想要为`year`字段定义一个单独的`Analyzer`，该分析器不修改值；对于所有其他值，它将使用之前定义的`indexTimeAnalyzer`，并启用同义词扩展。
- en: Listing 2.4\. Separate analysis chains for indexing and search
  id: totrans-439
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4\. 索引和搜索的独立分析链
- en: '[PRE12]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* Opens a Directory for indexing**'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 为索引打开一个目录**'
- en: '***2* Creates a map whose keys are the names of the fields and the values in
    the corresponding analysis chain to be used**'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个映射，其键是字段名称，值是对应的分析链**'
- en: '***3* Sets up a different analyzer (keyword; doesn’t touch the value) for the
    year**'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为年份设置不同的分析器（关键字；不修改值）**'
- en: '***4* Creates a wrapping analyzer that can work with per-field analyzers**'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 创建一个包装分析器，它可以与字段分析器一起工作**'
- en: '***5* Builds all the above in a configuration object**'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在配置对象中构建所有上述内容**'
- en: '***6* Creates an IndexWriter to be used for indexing**'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 创建一个用于索引的IndexWriter**'
- en: This mechanism allows indexing to be flexible in the way content is analyzed
    before being written into the inverted indexes; it’s common to play with different
    `Analyzer`s for different portions of `Document`s and to change them several times
    before finding the best combination for a data corpus. Even then, in the real
    world, it’s likely that such configurations will need adjustments over time. For
    instance, you may only index English songs and then, at some later point, begin
    to add songs in Chinese. In this case, you’ll have to adjust the analyzers to
    work with both languages (for example, you can’t expect a whitespace tokenizer
    to work well on Chinese, Japanese, and Korean [CJK] languages, where words often
    aren’t separated by a space).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此机制允许在将内容写入倒排索引之前灵活地分析内容；通常会在`Document`的不同部分尝试不同的`Analyzer`，并在找到最佳组合之前多次更改它们。即便如此，在现实世界中，此类配置可能需要随着时间的推移进行调整。例如，您可能只索引英文歌曲，然后在某个时间点开始添加中文歌曲。在这种情况下，您必须调整分析器以支持两种语言（例如，您不能期望空白分词器在中文、日语和韩语（CJK）语言中表现良好，在这些语言中，单词通常不是由空格分隔的）。
- en: Let’s put your first document into the Lucene index.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将您的第一个文档放入Lucene索引中。
- en: Listing 2.5\. Indexing documents
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5\. 索引文档
- en: '[PRE13]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* Creates a document for the song “Aeroplane”**'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 为歌曲“Aeroplane”创建一个文档**'
- en: '***2* Adds all the fields from the song lyrics**'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 添加歌曲歌词中的所有字段**'
- en: '***3* Adds the document**'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 添加文档**'
- en: '***4* Persists the updated inverted index to the filesystem, making the changes
    durable (and searchable)**'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将更新的倒排索引持久化到文件系统，使更改持久（并可搜索）**'
- en: You create a document composed of multiple fields, one per song attribute, and
    then add it to the `writer`.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 您创建一个由多个字段组成的文档，每个字段对应一首歌曲的属性，然后将它添加到`writer`中。
- en: In order to search, you open the `Directory` (again) and obtain a view on the
    index, an `IndexReader`, on which you can search via an `IndexSearcher`. To make
    sure synonym expansion works as expected, enter a query with the word “plane”;
    you’ll expect the “Aeroplane” song to be retrieved.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行搜索，您再次打开`Directory`并获取索引的视图，即`IndexReader`，您可以通过`IndexSearcher`在其上进行搜索。为了确保同义词扩展按预期工作，输入包含单词“plane”的查询；您期望检索到“Aeroplane”这首歌。
- en: Listing 2.6\. Searching for the word “plane”
  id: totrans-457
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6\. 搜索单词“plane”
- en: '[PRE14]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* Opens a view on the index**'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在索引上打开一个视图**'
- en: '***2* Instantiates a searcher**'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 实例化一个搜索器**'
- en: '***3* Creates a query parser that uses the search-time analyzer with the user-entered
    query to produce search terms**'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个查询解析器，它使用搜索时分析器与用户输入的查询来生成搜索词**'
- en: '***4* Transforms a user-entered query (as a String) into a proper Lucene query
    object using the QueryParser**'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用QueryParser将用户输入的查询（作为字符串）转换为适当的Lucene查询对象**'
- en: '***5* Searches, and obtains the first 10 results**'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 搜索，并获取前10个结果**'
- en: '***6* Iterates over the results**'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 遍历结果**'
- en: '***7* Obtains the search result**'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 获取搜索结果**'
- en: '***8* Outputs the title and author of the returned song**'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 输出返回歌曲的标题和作者**'
- en: 'As expected, the result is as follows:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，结果如下：
- en: '[PRE15]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We’ve gone through a quick tour of how to set up text analysis for index and
    search, and how to index documents and retrieve them. You’ve also learned how
    to add synonym expansion capability. But it should be clear that this code can’t
    be maintained in real life:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经快速浏览了如何设置索引和搜索的文本分析，以及如何索引文档和检索它们。您还学习了如何添加同义词扩展功能。但应该清楚，在实际生活中，这段代码是无法维护的：
- en: You can’t write code for each and every synonym you want to add.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能为每个想要添加的同义词编写代码。
- en: You need a synonym vocabulary that can be plugged in and managed separately,
    to avoid having to modify the search code every time you need to update it.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要一个可以插入并独立管理的同义词词汇表，以避免每次更新时都需要修改搜索代码。
- en: You need to manage the evolution of languages—new words (and synonyms) are added
    constantly.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要管理语言的演变——新词（及其同义词）不断被添加。
- en: A first step toward resolving these issues is to write the synonyms into a file
    and let the synonym filter read them from there. You’ll do that by putting synonyms
    on the same line, separated by commas. You’ll build the `Analyzer` in a more compact
    way, by using a builder pattern (see [https://en.wikipedia.org/wiki/Builder_pattern](https://en.wikipedia.org/wiki/Builder_pattern)).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的第一步是将同义词写入文件，并让同义词过滤器从那里读取它们。您将通过将同义词放在同一行，并用逗号分隔来实现这一点。您将通过使用构建者模式（见[https://en.wikipedia.org/wiki/Builder_pattern](https://en.wikipedia.org/wiki/Builder_pattern)）以更紧凑的方式构建`Analyzer`。
- en: Listing 2.7\. Feeding synonyms from a file
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7\. 从文件中提供同义词
- en: '[PRE16]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Defines the file that contains the synonyms**'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义了包含同义词的文件**'
- en: '***2* Defines an analyzer**'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定义了一个分析器**'
- en: '***3* Lets the analyzer use a whitespace tokenizer**'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 允许分析器使用空白分词器**'
- en: '***4* Lets the analyzer use a synonym filter**'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 允许分析器使用同义词过滤器**'
- en: 'Set up synonyms in the synonyms file:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在同义词文件中设置同义词：
- en: '[PRE17]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This way, the code remains unchanged regardless of any change in the synonyms
    file; you can update the file as much as you need to. Although this is much better
    than having to write code for synonyms, you don’t want to write the synonyms file
    by hand, unless you know that you’ll have just a few fixed synonyms. Fortunately,
    these days there’s lots of data that you can use for free or for a very low cost.
    A good, large resource for natural language processing in general is the WordNet
    project ([http://wordnet.princeton.edu](http://wordnet.princeton.edu)), a lexical
    database for the English language from Princeton University. You can take advantage
    of WordNet’s large synonym vocabulary, which is constantly updated, and include
    it in your indexing analysis pipeline by downloading it as a file (called, for
    example, synonyms-wn.txt) and specifying that you want to use the WordNet format.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，代码在同义词文件有任何变化的情况下保持不变；您可以根据需要更新文件。尽管这比为同义词编写代码要好得多，但您不想手动编写同义词文件，除非您知道您只有几个固定的同义词。幸运的是，如今有很多您可以免费或以极低成本使用的数据。自然语言处理的一个很好的大型资源是WordNet项目（[http://wordnet.princeton.edu](http://wordnet.princeton.edu)），这是普林斯顿大学为英语语言创建的词汇数据库。您可以利用WordNet庞大的同义词词汇表，该词汇表不断更新，并通过将其作为文件（例如，称为synonyms-wn.txt）下载并将其指定为想要使用WordNet格式来将其包含在索引分析管道中。
- en: Listing 2.8\. Using synonyms from WordNet
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8\. 使用WordNet中的同义词
- en: '[PRE18]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Sets up a synonym file using the WordNet vocabulary**'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用WordNet词汇设置同义词文件**'
- en: '***2* Specifies the WordNet format for the synonym file**'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 指定了同义词文件的WordNet格式**'
- en: With the WordNet dictionary plugged in, you have a very large, high-quality
    source of synonym expansion that should work well for English. But there are still
    a few problems. First, there’s not a WordNet-type resource for every language.
    Second, even if you stick to English, the synonym expansion for a word is based
    on its *denotation* as defined by the rules of English grammar and dictionaries;
    this doesn’t take into account its *connotation* as defined by the context in
    which those words appear.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 通过插入WordNet词典，您拥有一个非常庞大、高质量的同义词扩展源，这对于英语应该效果很好。但仍然存在一些问题。首先，并不是每种语言都有WordNet类型的资源。其次，即使您坚持使用英语，一个词的同义词扩展也是基于英语语法和词典定义的*外延*；这并没有考虑到它在特定语境中出现的*内涵*。
- en: 'I’m describing the difference between what linguists define as a synonym, based
    on strict dictionary definitions (denotation), versus how people commonly use
    language and words in real life (connotation). In informal contexts like social
    networks, chat rooms, and meeting friends in real life, people may use two words
    as if they were synonyms even if, by grammar rules, they aren’t synonyms. To handle
    this issue, word2vec will kick in and provide a more advanced level of search
    than just expanding synonyms based on the strict syntax of a language. You’ll
    see that using word2vec enables you to build synonym expansions that are language
    agnostic; it learns from the data which words are similar, without caring too
    much about the language used and whether it’s formal or informal. This is a helpful
    feature of word2vec: words with similar contexts are considered similar exactly
    because of their context. There’s no grammar or syntax involved. For each word,
    word2vec looks at the surrounding words, assuming that semantically similar words
    will appear in similar contexts.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我在描述语言学家根据严格的字典定义（指示）定义的同义词与人们在现实生活中通常使用语言和词语的方式（内涵）之间的区别。在非正式的语境中，如社交网络、聊天室和现实生活中与朋友见面，人们可能会使用两个词，好像它们是同义词，即使根据语法规则，它们不是同义词。为了处理这个问题，word2vec将介入并提供比仅仅基于语言的严格句法扩展同义词更高级别的搜索。你会发现，使用word2vec可以使你构建语言无关的同义词扩展；它从数据中学习哪些词语是相似的，而不太关心所使用的语言以及它是正式的还是非正式的。这是word2vec的一个有用特性：具有相似上下文的词语被认为是相似的，正是因为它们的上下文。这里不涉及语法或句法。对于每个词，word2vec都会查看周围的词语，假设语义相似的词将出现在相似的环境中。
- en: 2.2\. The importance of context
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 上下文的重要性
- en: The main problem with the approach outlined so far is that synonym mappings
    are static and not bound to the indexed data. For example, in the case of WordNet,
    synonyms strictly obey English grammar semantics but don’t take into account slang
    or informal contexts where words are often used as synonyms even if they aren’t
    synonyms according to strict rules of grammar. Another example is acronyms used
    in chat sessions and emails. For instance, it’s not uncommon to see acronyms like
    ICYMI (“in case you missed it”) and AKA (“also known as”) in email. *ICYMI* and
    “in case you missed it” can’t be called synonyms, and you probably won’t find
    them in a dictionary, but they mean the same thing.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止所概述的方法的主要问题是同义词映射是静态的，并且与索引数据无关。例如，在WordNet的情况下，同义词严格遵循英语语法语义，但并没有考虑到俚语或非正式语境，在这些语境中，即使根据严格的语法规则，词语通常也被用作同义词。另一个例子是在聊天和电子邮件中使用的缩写词。例如，在电子邮件中看到像ICYMI（“in
    case you missed it”）和AKA（“also known as”）这样的缩写词并不罕见。*ICYMI*和“in case you missed
    it”不能被称为同义词，你可能也不会在字典中找到它们，但它们的意思是相同的。
- en: One approach to overcoming these limitations is to have a way to generate synonyms
    from the data to be ingested. The basic concept is that it should be possible
    to extract the *nearest neighbors* of a word by looking at the context of the
    word, which means analyzing the patterns of surrounding words that occur together
    with the word itself. A nearest neighbor of a word in this case should be its
    synonym, even if it’s not strictly a synonym from the grammar perspective.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这些限制的一种方法是有一种方法可以从要摄入的数据中生成同义词。基本概念是应该可以通过查看词语的上下文来提取词语的*最近邻*，这意味着分析与该词本身一起出现的周围词语的模式。在这种情况下，一个词的最近邻应该是它的同义词，即使它从语法角度来看不是严格意义上的同义词。
- en: 'This idea that words that are used, and occur, in the same contexts tend to
    have similar meanings is called the *distributional hypothesis* (see [https://aclweb.org/aclwiki/Distributional_Hypothesis](https://aclweb.org/aclwiki/Distributional_Hypothesis))
    and is the basis of many deep learning algorithms for text representations. The
    interesting thing about this idea is that it disregards language, slang, style,
    and grammar: every bit of information about a word is inferred from the word contexts
    that appear in the text. Think, for example, of how words representing cities
    (Rome, Cape Town, Oakland, and so on) are often used. Let’s look at a few sentences:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观点认为，在相同上下文中使用和出现的词语往往具有相似的意义，这被称为*分布假设*（见[https://aclweb.org/aclwiki/Distributional_Hypothesis](https://aclweb.org/aclwiki/Distributional_Hypothesis)）并且是许多用于文本表示的深度学习算法的基础。这个想法有趣的地方在于它忽略了语言、俚语、风格和语法：关于一个词的所有信息都是从文本中出现的词的上下文中推断出来的。例如，想想代表城市的词语（如罗马、开普敦、奥克兰等）通常是如何使用的。让我们看看几个句子：
- en: I like to live in Rome because ...
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我喜欢住在罗马，因为...
- en: People who love surfing should go to Cape Town because ...
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热爱冲浪的人应该去开普敦，因为...
- en: I would like to visit Oakland to see ...
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我想去奥克兰看看...
- en: Traffic is crazy in Rome ...
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罗马的交通非常疯狂...
- en: Often the city names are used near the word “in” or a short distance from verbs
    like “live,” “visit,” and so on. This is the basic intuition behind the fact that
    the context provides a lot of information about each word.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，城市名称会用在“in”这个词附近，或者距离像“live”、“visit”这样的动词不远的地方。这是上下文为每个单词提供大量信息的基本直觉。
- en: With this in mind, you want to learn word representations for the words in the
    data to be indexed, so that you can generate synonyms from the data rather than
    manually building or downloading a synonym vocabulary. In the library example
    in [chapter 1](kindle_split_012.xhtml#ch01), I mentioned that it’s best to have
    insight about what’s in the library; with this additional insight, the librarian
    could help you more effectively. A student coming to the library could ask the
    librarian for, say, “books about artificial intelligence.” Let’s also suppose
    the library has only one book on the topic, and it’s called *AI Principles*. If
    the librarian (or the student) were searching through book titles, they would
    miss this book, unless they knew that AI is an acronym (and, given previous assumptions,
    a synonym) for artificial intelligence. An assistant knowledgeable about these
    synonyms would be useful in this situation.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，你想要学习要索引的数据中的单词表示，以便你可以从数据中生成同义词，而不是手动构建或下载同义词词汇表。在第1章[的图书馆例子](kindle_split_012.xhtml#ch01)中，我提到最好对图书馆的内容有所了解；有了这个额外的见解，图书管理员可以更有效地帮助你。一个来到图书馆的学生可以向图书管理员要求，比如说，“关于人工智能的书籍。”假设图书馆只有一本关于这个主题的书，书名为*AI
    Principles*。如果图书管理员（或学生）正在搜索书名，他们可能会错过这本书，除非他们知道AI是缩写（并且根据之前的假设，是同义词）代表人工智能。在这种情况下，了解这些同义词的助手会有用。
- en: 'Let’s imagine two hypothetical types of such an assistant: John, an English
    language expert who has studied English grammar and syntax for years; and Robbie,
    another student who collaborates weekly with the librarian and has the chance
    to read most of the books. John couldn’t tell you that *AI* stands for *artificial
    intelligence*, because his background doesn’t give him this information. Robbie,
    on the other hand, has far less formal knowledge of English, but he’s an expert
    on the books in the library; he could easily tell you that *AI* stands for *artificial
    intelligence*, because he’s read the book *AI Principles* and knows it’s about
    the principles of artificial intelligence. In this scenario, John is acting like
    the WordNet vocabulary, and Robbie is the word2vec algorithm. Although John has
    proven knowledge of the language, Robbie may be more helpful in this particular
    situation.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象两种这样的假设助手类型：约翰，一位多年研究英语语法和语法的英语语言专家；还有罗比，另一位每周与图书管理员合作的学生，有机会阅读大部分书籍。约翰无法告诉你*AI*代表*人工智能*，因为他的背景没有提供这方面的信息。另一方面，罗比对英语的正式知识要少得多，但他对图书馆的书籍是专家；他可以轻易地告诉你*AI*代表*人工智能*，因为他读过《AI
    Principles》这本书，并知道它关于人工智能的原则。在这种情况下，约翰就像WordNet词汇表，而罗比则是word2vec算法。尽管约翰对语言有证明的知识，但罗比在这种情况下可能更有帮助。
- en: 'In [chapter 1](kindle_split_012.xhtml#ch01), I mentioned that neural networks
    are good at learning representations (in this case, representations of words)
    that are sensitive to the context. That’s the kind of capability you’ll use with
    word2vec. In short, you’ll use the word2vec neural network to learn a representation
    of the words that can tell you the most similar (or nearest neighbor) word for
    “plane”: “aeroplane.” Before we get deeper into that, let’s take a closer look
    at one of the simplest forms of neural networks: feed-forward. Feed-forward neural
    networks are the basis for most more-complex neural network architectures.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，我提到神经网络擅长学习对上下文敏感的表示（在这种情况下，是单词的表示）。这就是你将使用word2vec的能力。简而言之，你将使用word2vec神经网络来学习一个可以告诉你“plane”最相似（或最近邻）单词的表示：“aeroplane”。在我们深入探讨这一点之前，让我们更仔细地看看神经网络最简单的一种形式：前馈。前馈神经网络是大多数更复杂的神经网络架构的基础。
- en: 2.3\. Feed-forward neural networks
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 前馈神经网络
- en: Neural networks are the key tool for neural search, and many neural network
    architectures extend from feed-forward networks. A *feed-forward neural network*
    is a neural network in which information flows from the input layer to hidden
    layers, if any, and finally to the output layer; there are no loops, because the
    connections among neurons don’t form a cycle. Think of it as a magic black box
    with inputs and outputs. The magic mostly happens inside the net, thanks to the
    way neurons are connected to each other and how they react to their inputs. If
    you were looking for a house to buy in a specific country, for instance, you could
    use the “magic box” to predict a fair price you could expect to pay for a specific
    house. As you can see in [figure 2.4](#ch02fig04), the magic box would learn to
    make predictions using input features such as house size, location, and a rating
    given by the seller.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是神经搜索的关键工具，许多神经网络架构都源自前馈网络。*前馈神经网络*是一种神经网络，其中信息从输入层流向隐藏层（如果有），最终到达输出层；因为没有循环，因为神经元之间的连接不会形成一个循环。将其视为一个有输入和输出的魔法黑盒子。魔法主要发生在网络内部，归功于神经元之间如何相互连接以及它们如何对输入做出反应。例如，如果你在特定国家寻找购买房屋，你可以使用“魔法盒子”来预测你预期为特定房屋支付的价格。如[图2.4](#ch02fig04)所示，这个魔法盒子将学会使用输入特征（如房屋大小、位置和卖家给出的评级）进行预测。
- en: Figure 2.4\. Predicting price with a feed-forward neural network with three
    inputs, five hidden units, and one output unit
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4\. 使用三个输入、五个隐藏单元和一个输出单元的前馈神经网络预测价格
- en: '![](Images/02fig04_alt.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/02fig04_alt.jpg)'
- en: 'A feed-forward neural network is composed of the following:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络由以下部分组成：
- en: '***An input layer—*** Responsible for gathering the inputs provided by the
    user. These inputs are usually in the form of real numbers. In the example of
    predicting a house price, you have three inputs: house size, house location, and
    amount of money required by the seller. You’ll encode these inputs as three real
    numbers, so the input you’ll pass to the network will be a three-dimensional vector:
    `[size, location, price]`.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***输入层——*** 负责收集用户提供的输入。这些输入通常以实数的形式存在。在预测房屋价格的例子中，你有三个输入：房屋大小、房屋位置和卖家所需金额。你将把这些输入编码为三个实数，因此传递给网络的输入将是一个三维向量：`[size,
    location, price]`。'
- en: '***Optionally, one or more hidden layers—*** Represents a more mysterious part
    of the network. Think of it as the part of the network that allows it to be so
    good at learning and predicting. In the example, there are five units in the hidden
    layer, all of which are connected to the units in the input layer and also to
    the units in the output layer. The connectivity in the network plays a fundamental
    role in the network activity dynamics. Most of the time, all units in a layer
    (*x*) are fully connected (forward) to the units in the next layer (*x*+1).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可选，一个或多个隐藏层——*** 代表网络中更神秘的部分。将其视为网络能够如此擅长学习和预测的部分。在示例中，隐藏层有五个单元，它们都与输入层的单元以及输出层的单元相连。网络中的连接性在网络活动动态中起着基本作用。大多数情况下，一个层（*x*）中的所有单元都完全连接（正向）到下一层的单元（*x*+1）。'
- en: '***An output layer—*** Responsible for providing the final output of the network.
    In the house price example, it will provide a real number representing what the
    network estimates the right price should be.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***输出层——*** 负责提供网络的最终输出。在房屋价格示例中，它将提供一个代表网络估计的正确价格的实数。'
- en: '|  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Usually, it’s a good idea to scale inputs so they’re more or less in the same
    range of values—for example, between -1 and 1\. In the example, a house’s size
    in square meters is between 10 and 200, and its price range is in the order of
    tens of thousands. Preprocessing the input data so it’s all in similar ranges
    of values allows the network to learn more quickly.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将输入值缩放至大致相同的数值范围是一个好主意——例如，介于-1和1之间。在示例中，房屋的面积（平方米）介于10到200之间，价格范围在数万级别。将输入数据预处理到相似数值范围，可以使网络更快地学习。
- en: '|  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'How it works: Weights and activation functions'
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工作原理：权重和激活函数
- en: As you’ve seen, a feed-forward neural network receives inputs and produces outputs.
    The fundamental building blocks of these networks are called *neurons* (even though
    a brain neuron is much more complex). Every neuron in a feed-forward neural network
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，前馈神经网络接收输入并产生输出。这些网络的基本构建块被称为*神经元*（尽管大脑中的神经元要复杂得多）。前馈神经网络中的每个神经元
- en: Belongs to a layer
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于某一层
- en: Smooths each input by its incoming weight
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过其输入权重平滑每个输入
- en: Propagates its output according to an activation function
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据激活函数传播其输出
- en: In the feed-forward neural network in [figure 2.5](#ch02fig05), the second layer
    is composed of only one neuron. This neuron receives input from three neurons
    in layer 1 and propagates output to only one neuron in layer 3\. It has an associated
    activation function, and its incoming links with the previous layer have associated
    weights (often, real numbers between -1 and 1).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2.5](#ch02fig05)中的前馈神经网络中，第二层仅由一个神经元组成。这个神经元从第一层的三个神经元接收输入，并将输出传播到第三层的一个神经元。它有一个相关的激活函数，并且与前一层的输入链接有关联的权重（通常是介于-1和1之间的实数）。
- en: Figure 2.5\. Propagating signals through the network
  id: totrans-519
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5\. 通过网络传播信号
- en: '![](Images/02fig05.jpg)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/02fig05.jpg)'
- en: 'Let’s assume that all the incoming weights of the neuron in layer 2 are set
    to 0.3 and that it receives from the first layer the inputs 0.4, 0.5, and 0.6\.
    Each weight is multiplied by its input, and the results are summed together: 0.3
    × 0.4 + 0.3 × 0.5 + 0.3 × 0.6 = 0.45\. The activation function is applied to this
    intermediate result and then propagated to the outgoing links of the neuron. Common
    activation functions are hyperbolic tangent (`tanh`), `sigmoid`, and rectified
    linear unit (`ReLU`).'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第二层神经元的所有输入权重都设置为0.3，并且它从第一层接收输入0.4、0.5和0.6。每个权重乘以其输入，然后将结果相加：0.3 × 0.4 +
    0.3 × 0.5 + 0.3 × 0.6 = 0.45。激活函数应用于这个中间结果，然后传播到神经元的输出链接。常见的激活函数是双曲正切(`tanh`)、`sigmoid`和修正线性单元(`ReLU`)。
- en: 'In the current example, let’s use the `tanh` function. You’ll have `tanh(0.45)
    = 0.4218990053`, so the neuron in the third layer will receive this number as
    an input on its only incoming link. The output neuron will perform exactly the
    same steps the neuron from layer 2 does, using its own weights. For this reason,
    these networks are called *feed-forward*: each neuron transforms and propagates
    its inputs in order to feed the neurons in the next layer.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，让我们使用`tanh`函数。你会得到`tanh(0.45) = 0.4218990053`，因此第三层的神经元将在这个唯一的输入链接上接收这个数字。输出神经元将执行与第二层神经元完全相同的步骤，使用它自己的权重。因此，这些网络被称为*前馈*：每个神经元转换并传播其输入，以便为下一层的神经元提供输入。
- en: Backpropagation in a nutshell
  id: totrans-523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向传播的要点
- en: 'In [chapter 1](kindle_split_012.xhtml#ch01), I mentioned that neural networks
    and deep learning belong to the field of machine learning. I also touched on the
    main algorithm used for training neural networks: backpropagation. In this section,
    we’ll give it a closer look.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，我提到了神经网络和深度学习属于机器学习的领域。我还简要提到了用于训练神经网络的
    主要算法：反向传播。在本节中，我们将对其进行更深入的探讨。
- en: A fundamental point when discussing the rise of deep learning is related to
    how well and how quickly neural networks can learn. Although artificial neural
    networks are an old computing paradigm (circa 1950), they became popular again
    recently (around 2011) as modern computers’ performance improved to a level that
    allowed neural nets to perform effective learning in a reasonable time.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论深度学习兴起的一个基本点与神经网络学习得有多好以及有多快有关。尽管人工神经网络是一个古老的计算范式（约1950年），但随着现代计算机性能的提高（大约在2011年），它们最近又重新流行起来，这使得神经网络能够在合理的时间内进行有效的学习。
- en: In the previous section, you saw how a network propagates information from the
    input layer to the output layer in a feed-forward fashion. On the other hand,
    after a feed-forward pass, backpropagation lets the signal flow backward from
    the output layer to the input layer.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你看到了网络如何以前馈方式从输入层传播信息到输出层。另一方面，在完成前馈传递后，反向传播允许信号从输出层反向流向输入层。
- en: The values of the activations of the neurons in the output layer, generated
    by a feed-forward pass on an input, are compared the values in the desired output.
    This comparison is performed by a *cost function* that calculates a loss or cost
    and represents a measure of how much the network is wrong in that particular case.
    Such an error is sent backward through the incoming connections of the output
    neurons to the corresponding units in the hidden layer. You can see in [figure
    2.6](#ch02fig06) that the neuron in the output layer sends back its portion of
    error to the connected units in the hidden layer.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通过前向传播生成的输出层中神经元的激活值与期望输出的值进行比较。这种比较是通过一个*成本函数*来执行的，该函数计算损失或成本，并代表网络在特定情况下错误的程度。这种错误会通过输出神经元的输入连接反向传递到隐藏层中的相应单元。你可以在[图2.6](#ch02fig06)中看到，输出层中的神经元将其错误部分发送回连接的隐藏层单元。
- en: Figure 2.6\. Backpropagating a signal from the output layer to the hidden layer
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6\. 从输出层到隐藏层的反向传播信号
- en: '![](Images/02fig06_alt.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6](Images/02fig06_alt.jpg)'
- en: Once a unit receives an error, it updates its weights according to an *update
    algorithm*; usually, the algorithm used is *stochastic gradient descent*. This
    backward update of weights happens until the weights on the input layer connections
    are adjusted (note that updates are done only for output and hidden layer units,
    as input units don’t have weights), and then the update stops. So a run of backpropagation
    updates all the weights associated with the existing connections. The rationale
    behind this algorithm is that each weight is responsible for a portion of the
    error and, therefore, backpropagation tries to adjust such weights in order to
    reduce the error for that particular input/output pair.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一个单元收到错误，它就会根据一个*更新算法*更新其权重；通常使用的算法是*随机梯度下降*。这种权重的反向更新会一直进行，直到输入层连接上的权重被调整（注意，更新仅针对输出和隐藏层单元进行，因为输入单元没有权重），然后更新停止。因此，一次反向传播更新与现有连接相关联的所有权重。这个算法背后的原理是每个权重都负责一部分错误，因此反向传播试图调整这样的权重以减少特定输入/输出对的错误。
- en: 'The gradient descent algorithm (or any other update algorithm for adjusting
    the weights) decides *how* the weights are changed with respect to the portion
    of error each weight contributes. A lot of math is involved in this concept, but
    you can think of it as if the cost function defines a shape like the one in [figure
    2.7](#ch02fig07), where the height of the hill defines the amount of error. A
    very low point corresponds to the combination of the neural network weights having
    a very low error:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法（或任何其他用于调整权重的更新算法）决定了权重如何根据每个权重所贡献的错误部分进行变化。这个概念涉及到大量的数学，但你可以将其想象成成本函数定义了一个形状，就像[图2.7](#ch02fig07)中的那样，山的高度定义了错误的量。一个非常低的点对应于神经网络权重组合具有非常低的错误：
- en: '***Low—*** The point with the lowest possible error, having optimal values
    for the neural network weights'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***低—*** 具有最低可能错误的点，具有神经网络权重的最优值'
- en: '***High—*** A point with high error; gradient descent tries to perform descent
    toward points with lower error'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***高—*** 具有高错误的点；梯度下降试图向具有更低错误的点下降'
- en: Figure 2.7\. Geometric interpretation of backpropagation with gradient descent
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7\. 梯度下降的反向传播的几何解释
- en: '![](Images/02fig07_alt.jpg)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7](Images/02fig07_alt.jpg)'
- en: The coordinates of a point are given by the value of the weights in the neural
    network, so the gradient descent tries to find a value of the weights (a point)
    with very low error (a very low height) in the shape.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 一个点的坐标由神经网络中权重的值给出，因此梯度下降试图找到一个权重值（一个点）在形状中有非常低的错误（一个非常低的高度）。
- en: 2.4\. Using word2vec
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 使用word2vec
- en: 'Now that you understand what a generic feed-forward network is, we can focus
    on a more specific neural network algorithm based on feed-forward neural networks:
    word2vec. Although its basics are fairly easy to understand, it’s fascinating
    to see the good results (in terms of capturing the semantics of words in a text)
    you can achieve. But what does it do, and how is it useful for the synonym expansion
    use case?'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了什么是通用的前向网络，我们可以专注于基于前向神经网络的一个更具体的神经网络算法：word2vec。尽管其基本原理相对容易理解，但看到它能取得良好的结果（在捕捉文本中词语的语义方面）是非常令人着迷的。但它具体做什么，以及它对同义词扩展用例有何用途？
- en: Word2vec takes a piece of text and outputs a series of vectors, one for each
    word in the text. When the output vectors of word2vec are plotted on a two-dimensional
    graph, vectors whose words are very similar in terms of semantics are very close
    to one another. You can use a distance measures like the cosine distance to find
    the most similar words with respect to a given word. Thus, you can use this technique
    to find a word’s synonyms. In short, in this section you’ll set up a word2vec
    model, feed it the text of the song lyrics you want to index, get output vectors
    for each word, and use them to find synonyms.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 从一段文本中提取并输出一系列向量，每个向量对应文本中的一个单词。当将 word2vec 的输出向量绘制在二维图上时，语义上非常相似的单词的向量彼此非常接近。你可以使用诸如余弦距离这样的距离度量来找到与给定单词最相似的单词。因此，你可以使用这种技术来查找单词的同义词。简而言之，在本节中，你将设置一个
    word2vec 模型，给它提供你想要索引的歌曲歌词文本，为每个单词获取输出向量，并使用它们来查找同义词。
- en: '[Chapter 1](kindle_split_012.xhtml#ch01) discussed using vectors in the context
    of search, when we talked about the vector space model and term frequency-inverse
    document frequency (TF-IDF). In a sense, word2vec also generates a vector space
    model whose vectors (one for each word) are weighted by the neural network during
    the learning process. Word vectors generated by algorithms like word2vec are often
    referred to as *word embeddings* because they map static, discrete, high-dimensional
    word representations (such as TF-IDF or one-hot encoding) into a different (continuous)
    vector space with fewer dimensions involved.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](kindle_split_012.xhtml#ch01) 讨论了在搜索的背景下使用向量，当时我们讨论了向量空间模型和词频-逆文档频率（TF-IDF）。从某种意义上说，word2vec
    也生成一个向量空间模型，其向量（每个单词一个）在学习过程中由神经网络加权。由像 word2vec 这样的算法生成的词向量通常被称为 *词嵌入*，因为它们将静态、离散、高维度的词表示（如TF-IDF或独热编码）映射到涉及维度更少的（连续）向量空间中。'
- en: 'Let’s get back to the example of the song “Aeroplane.” If you feed its text
    to word2vec, you’ll get a vector for each word:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到“飞机”这首歌的例子。如果你将其文本输入到 word2vec 中，你将为每个单词得到一个向量：
- en: '[PRE19]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see these in the coordinate plan shown in [figure 2.8](#ch02fig08).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图2.8](#ch02fig08)所示的坐标图中看到这些。
- en: Figure 2.8\. Plotted word vectors for “Aeroplane”
  id: totrans-544
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8\. “飞机”的词向量图
- en: '![](Images/02fig08_alt.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig08_alt.jpg)'
- en: In the example output, two dimensions were used so those vectors are more easily
    plottable on a graph. But in practice, it’s common to use 100 or more dimensions,
    and to use a dimensionality reduction algorithm like Principal Component Analysis
    or t-SNE to obtain two- or three-dimensional vectors that can be more easily plotted.
    (Using many dimensions lets you capture more information as the amount of data
    grows.) At this point, we won’t discuss this tuning in detail, but we’ll return
    to it later in the book as you learn more about neural networks.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例输出中，使用了两个维度，因此这些向量更容易在图上绘制。但在实践中，通常使用100个或更多的维度，并使用主成分分析或t-SNE这样的降维算法来获得可以更容易绘制的二维或三维向量。（使用许多维度可以让你在数据量增长时捕获更多信息。）在此阶段，我们不会详细讨论这种调整，但当我们学习更多关于神经网络的知识时，我们将在本书的后面部分回到这个问题。
- en: 'Using cosine similarity to measure the distance among each of the generated
    vectors produces some interesting results:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余弦相似度来衡量生成向量的距离产生了一些有趣的结果：
- en: '[PRE20]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you can see, extracting the two nearest vectors for a few random vectors
    gives results, some good, and some, not so much:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，提取几个随机向量的两个最近向量给出了结果，有些很好，有些则不然：
- en: “Music” and “song” are very close semantically; you could even say they’re synonyms.
    But the same isn’t true for “view.”
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “音乐”和“歌曲”在语义上非常接近；甚至可以说它们是同义词。但“观看”的情况并非如此。
- en: “Looking” and “view” are related, but “better” has nothing to do with “looking.”
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “看”和“观看”有关联，但“更好”与“看”无关。
- en: “In,” “the,” and “like” aren’t close to each other.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在”、“the”和“like”彼此之间并不接近。
- en: “Sitting” and “turning” are both verbs of the “ing” form, but their semantics
    are loosely coupled. “Could” is also a verb, but it doesn’t have much else to
    do with “sitting.”
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “坐着”和“转动”都是“ing”形式的动词，但它们的语义松散地耦合。“可以”也是一个动词，但它与“坐着”没有太多关系。
- en: What’s the problem? Isn’t word2vec up to the task?
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是什么？word2vec 不是能胜任这项任务吗？
- en: 'There are two factors at play:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个因素在起作用：
- en: The number of dimensions (two) of the generated word vectors is probably too
    low.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成词向量的维度（两个）可能太低。
- en: Feeding the word2vec model the text of a single song probably doesn’t provide
    enough contexts for each of the words to come with an accurate representation.
    The model needs more examples of the contexts in which the words “better” and
    “view” occur.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向word2vec模型输入单首歌曲的文本可能不足以提供每个单词的准确表示的上下文。模型需要更多“better”和“view”单词出现的上下文示例。
- en: 'Let’s assume you again build the word2vec model, this time using 100 dimensions
    and a larger set of song lyrics taken from the Billboard Hot 100 dataset ([https://www.kaylinpavlik.com/50-years-of-pop-music](https://www.kaylinpavlik.com/50-years-of-pop-music)):'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你再次构建word2vec模型，这次使用100个维度和从Billboard Hot 100数据集（[https://www.kaylinpavlik.com/50-years-of-pop-music](https://www.kaylinpavlik.com/50-years-of-pop-music)）中选取的更大的歌曲歌词集：
- en: '[PRE21]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The results are much better and more appropriate: you could use almost all
    of them as synonyms in the context of search. You can imagine using such a technique
    at either query or indexing time. There would be no more dictionaries or vocabularies
    to keep up to date; the search engine could learn to generate synonyms from the
    data it handles.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 结果要好得多，也更合适：你几乎可以用它们中的所有作为搜索上下文中的同义词。你可以想象在查询或索引时使用这种技术。将不再需要更新词典或词汇表；搜索引擎可以从它处理的数据中学习生成同义词。
- en: 'You may have a couple of questions right about now: How does word2vec work?
    And how can you integrate it, in practice, into a search engine? The paper “Efficient
    Estimation of Word Representations in Vector Space”^([[1](#ch02fn01)]) describes
    two different neural network models for learning such word representations: *continuous-bag-of-words*
    (CBOW) and *continuous skip-gram*. We’ll discuss both of them, and how to implement
    them, in a moment. Word2vec performs unsupervised learning of word representations;
    the mentioned CBOW and skip-gram models just need to be fed a sufficiently large
    text, properly encoded. The main concept behind word2vec is that the neural network
    is given a piece of text, which is split into fragments of a certain size (also
    called *windows*). Every fragment is fed to the network as a pair consisting of
    a *target word* and a *context*. In the case of [figure 2.9](#ch02fig09), the
    target word is “aeroplane,” and the context consists of the words “music,” “is,”
    and “my.”'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能有几个问题：word2vec是如何工作的？你如何将其实际集成到搜索引擎中？论文“Efficient Estimation of Word Representations
    in Vector Space”^([[1](#ch02fn01)])描述了两种不同的神经网络模型用于学习这样的单词表示：*连续词袋*（CBOW）和*连续跳字*。我们将讨论这两个模型，以及如何实现它们，稍后我们将讨论。Word2vec执行单词表示的无监督学习；提到的CBOW和跳字模型只需要提供足够大的文本，并正确编码。word2vec背后的主要概念是神经网络被给了一段文本，该文本被分成一定大小的片段（也称为*窗口*）。每个片段作为由*目标词*和*上下文*组成的对输入到网络中。在[图2.9](#ch02fig09)的情况下，目标词是“aeroplane”，上下文包括单词“music”、“is”和“my”。
- en: ¹
  id: totrans-562
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-563
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tomas Mikolov et al. (2013), [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf).
  id: totrans-564
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tomas Mikolov等人（2013年），[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)。
- en: Figure 2.9\. Feeding word2vec (skip-gram model) text fragments
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9\. 向word2vec（skip-gram模型）输入文本片段
- en: '![](Images/02fig09_alt.jpg)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig09_alt.jpg)'
- en: The hidden layer of the network contains a set of weights (in this case, 11
    of them—the number of neurons in the hidden layer) for each word. These vectors
    will be used as the word representations when learning ends.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的隐藏层包含一组权重（在这个例子中，有11个——隐藏层中神经元的数量），每个单词都有一个权重。这些向量将在学习结束时用作单词表示。
- en: An important note about word2vec is that you don’t care much about the outputs
    of the neural network. Instead, you extract the internal state of the hidden layer
    at the end of the training phase, which yields exactly one vector representations
    for each word.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 关于word2vec的一个重要注意事项是，你并不太关心神经网络的输出。相反，你提取训练阶段结束时隐藏层的内部状态，这为每个单词产生一个精确的向量表示。
- en: 'During training, a portion of each fragment is used as target word, and the
    rest is used as context. With the CBOW model, the target word is used as the output
    of the network, and the remaining words of the text fragment (the context) are
    used as inputs. The opposite is true with the continuous skip-gram model: the
    target word is used as input and the context words as outputs (as in the example).
    In practice, both work well, but skip-gram is usually preferred because it works
    slightly better with infrequently used words.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个片段的一部分被用作目标词，其余部分被用作上下文。在CBOW模型中，目标词被用作网络的输出，而文本片段的剩余单词（上下文）被用作输入。相反，在连续跳过一元模型中：目标词被用作输入，上下文单词作为输出（如示例所示）。在实践中，它们都工作得很好，但跳过一元模型通常更受欢迎，因为它在处理不常用单词时表现略好。
- en: For example, given the text “she keeps moet et chandon in her pretty cabinet
    let them eat cake she says” from the song “Killer Queen” (by the band Queen),
    and a window of 5, a word2vec model based on CBOW will receive a sample for each
    five-word fragment. For example, for the fragment `| she | keeps | moet | et |
    chandon |`, the input will consist of the words `| she | keeps | et | chandon
    |` and the output will consist of the word `moet`.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定歌曲“Killer Queen”（乐队Queen演唱）中的文本“she keeps moet et chandon in her pretty
    cabinet let them eat cake she says”，以及一个窗口大小为5，基于CBOW的word2vec模型将为每个五词片段接收一个样本。例如，对于片段`|
    she | keeps | moet | et | chandon |`，输入将包括单词`| she | keeps | et | chandon |`，而输出将包括单词`moet`。
- en: As you can see from [figure 2.10](#ch02fig10), the neural network is composed
    of an input layer, a hidden layer, and an output layer. This kind of neural network,
    with one hidden layer, is referred to as *shallow*. Neural networks with more
    than one hidden layer are referred to as *deep*.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图2.10](#ch02fig10)所示，神经网络由输入层、隐藏层和输出层组成。这种只有一个隐藏层的神经网络被称为*浅层*。具有多个隐藏层的神经网络被称为*深层*。
- en: Figure 2.10\. Continuous-bag-of-words model
  id: totrans-572
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10\. 连续词袋模型
- en: '![](Images/02fig10_alt.jpg)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig10_alt.jpg)'
- en: The neurons in the hidden layer have no activation function, so they linearly
    combine weights and inputs (multiply each input by its weight and sum all of the
    results together). The input layer has a number of neurons equal to the number
    of words in the text for each word; word2vec requires each word to be represented
    as a *one-hot-encoded* vector.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的神经元没有激活函数，因此它们线性组合权重和输入（将每个输入乘以其权重并将所有结果相加）。输入层中的神经元数量等于文本中每个单词的单词数；word2vec要求每个单词都表示为一个*一热编码*向量。
- en: 'Let’s see what a one-hot-encoded vector looks like. Imagine that you have a
    dataset with three words: `[cat, dog, mouse]`. You have three vectors, each with
    all the values set to `0` except one, which is set to `1` (that one identifies
    that specific word):'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一热编码向量是什么样的。想象一下，你有一个包含三个单词的数据集：`[cat, dog, mouse]`。你有三个向量，每个向量中所有值都设置为`0`，只有一个设置为`1`（这个值标识了特定的单词）：
- en: '[PRE22]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If you add the word “lion” to the dataset, one-hot-encoded vectors for this
    dataset will have dimension 4:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将单词“lion”添加到数据集中，该数据集的一热编码向量将有维度4：
- en: '[PRE23]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you have 100 words in your input text, each word will be represented as a
    100-dimensional vector. Consequently, in the CBOW model, you’ll have 100 input
    neurons multiplied by the value of the `window` parameter minus 1\. So, if `window`
    is 4, you’ll have 300 input neurons.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的输入文本中有100个单词，每个单词将表示为一个100维的向量。因此，在CBOW模型中，你将会有100个输入神经元乘以`window`参数的值减1。所以，如果`window`是4，你将会有300个输入神经元。
- en: The hidden layer has a number of neurons equal to the desired dimensionality
    of the resulting word vectors. This parameter must be set by whoever sets up the
    network.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的神经元数量等于结果词向量的期望维度。此参数必须由设置网络的人设置。
- en: 'The size of the output layer is equal to the number of words in the input text:
    in this example, 100\. A word2vec CBOW model for an input text with 100 words,
    embeddings dimensionality equal to 50, and `window` set to 4 will have 300 input
    neurons, 50 hidden neurons, and 100 output neurons. Note that, while input and
    output dimensionalities depend on the size of the vocabulary (in this case, 100)
    and the `window` parameter, the dimensionality of the word embeddings generated
    by the CBOW model is a parameter, to be chosen by the user. For example, in [figure
    2.11](#ch02fig11) you can see the following:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的尺寸等于输入文本中的单词数量：在这个例子中，是100。对于一个包含100个单词的输入文本，词向量维度为50，`window`设置为4的word2vec
    CBOW模型将拥有300个输入神经元，50个隐藏神经元和100个输出神经元。请注意，虽然输入和输出维度依赖于词汇表的大小（在这个例子中，是100）和`window`参数，但CBOW模型生成的词向量维度是一个参数，由用户选择。例如，在[图2.11](#ch02fig11)中，你可以看到以下内容：
- en: The input layer has a dimensionality of `C` × `V`, where `C` is the length of
    the context (corresponding to the `window` parameter minus 1) and `V` is the size
    of the vocabulary.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层的维度为`C` × `V`，其中`C`是上下文的长度（对应于`window`参数减1），`V`是词汇表的大小。
- en: The hidden layer has a dimensionality of `N`, defined by the user.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的维度为`N`，由用户定义。
- en: The output layer has a dimensionality equal to `V`.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的维度等于`V`。
- en: Figure 2.11\. Continuous-bag-of-words model weights
  id: totrans-585
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11\. 连续词袋模型权重
- en: '![](Images/02fig11.jpg)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/02fig11.jpg)'
- en: For word2vec, CBOW model inputs are propagated through the network by first
    multiplying the one-hot-encoded vectors of the input words by their input-to-hidden
    weights; you can imagine that as a matrix containing a weight for each connection
    between an input and a hidden neuron. Those are combined (multiplied) with the
    hidden-to-output weights, producing the outputs; and these outputs are then passed
    through a softmax function. Softmax “squashes” a K-dimensional vector (the output
    vector) of arbitrary real values to a K-dimensional vector of real values in the
    range (0, 1) that add up to 1, so that they can represent a probability distribution.
    Your network tells you the probability that each output word will be selected,
    given the context (the network input).
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 对于word2vec，CBOW模型通过首先将输入单词的一热编码向量与它们的输入到隐藏权重相乘来通过网络传播输入；你可以想象这就像一个包含每个输入和隐藏神经元之间连接权重的矩阵。这些与隐藏到输出的权重相结合（相乘），产生输出；然后这些输出通过softmax函数。softmax函数将一个K维向量（输出向量）的任意实数值压缩成一个K维实数值向量，其范围在(0,
    1)之间，且总和为1，这样它们可以表示一个概率分布。你的网络告诉你，在给定上下文（网络输入）的情况下，每个输出单词被选中的概率。
- en: You now have a neural network that can predict the most likely word to appear
    in the text, given a context of a few words (the `window` parameter). This neural
    network can tell you that given a context like “I like eating,” you should expect
    the next word to be something like “pizza.” Note that because word order isn’t
    taken into account, you could also say that given the context “I eating pizza,”
    the next word most likely to appear in the text is “like.”
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有一个神经网络，可以根据几个单词的上下文（`window`参数）预测文本中最可能出现的单词。这个神经网络可以告诉你，给定上下文“我喜欢吃”，你应该期待下一个词可能是“披萨”这样的词。请注意，因为未考虑词序，你也可以说，给定上下文“我吃披萨”，文本中最可能出现的下一个词是“喜欢”。
- en: But the most important part of this neural network for the goal of generating
    synonyms isn’t learning to predict words given a context. The surprising beauty
    of this method is that, internally, the weights of the hidden layer adjust in
    a way that makes it possible to determine when two words are semantically similar
    (because they appear in the same or similar contexts).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于生成同义词的目标来说，这个神经网络最重要的部分并不是学习根据上下文预测单词。这个方法的惊人之处在于，在内部，隐藏层的权重以某种方式调整，使得可以确定两个单词在语义上是否相似（因为它们出现在相同的或相似上下文中）。
- en: After forward propagation, the backpropagation learning algorithm adjusts the
    weights of each neuron in the different layers, so the neural network will produce
    a more accurate result for each new fragment. When the learning process has finished,
    the hidden-to-output weights represent the vector representation (embedding) for
    each word in the text.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播之后，反向传播学习算法调整不同层中每个神经元的权重，以便神经网络为每个新的片段产生更准确的结果。当学习过程完成后，隐藏到输出的权重代表文本中每个单词的向量表示（嵌入）。
- en: 'Skip-gram looks reversed with respect to the CBOW model. The same concepts
    apply: the input vectors are one-hot encoded (one for each word), so the input
    layer has a number of neurons equals to the number of words in the input text.
    The hidden layer has the dimensionality of the desired resulting word vectors,
    and the output layer has a number of neurons equal to the number of words multiplied
    by `window` minus 1\. Using the same example as before, given the text “she keeps
    moet et chandon in her pretty cabinet let them eat cake she says” and a `window`
    value of 5, a word2vec model based on the skip-gram model will receive a first
    sample for `| she | keeps | moet | et | chandon |` with the input `moet` and the
    output `| she | keeps | et | chandon |` (see [figure 2.12](#ch02fig12)).'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram模型与CBOW模型相反。相同的概念适用：输入向量是one-hot编码的（每个词一个），因此输入层有与输入文本中单词数量相等的神经元数量。隐藏层具有所需结果词向量的维度，输出层有与单词数量乘以`window`减1相等的神经元数量。使用之前的例子，给定文本“she
    keeps moet et chandon in her pretty cabinet let them eat cake she says”和`window`值为5，基于skip-gram模型的word2vec模型将接收第一个样本`|
    she | keeps | moet | et | chandon |`，输入为`moet`，输出为`| she | keeps | et | chandon
    |`（见[图 2.12](#ch02fig12)）。
- en: Figure 2.12\. Skip-gram model
  id: totrans-592
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.12\. Skip-gram 模型
- en: '![](Images/02fig12_alt.jpg)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig12_alt.jpg)'
- en: '[Figure 2.13](#ch02fig13) is an example excerpt of word vectors calculated
    by word2vec for the text of the Hot 100 Billboard dataset. It shows a small subset
    of words plotted, for the sake of appreciating word semantics being expressed
    geometrically.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.13](#ch02fig13) 是 word2vec 为 Hot 100 Billboard 数据集文本计算出的词向量示例摘录。它展示了为欣赏词义以几何方式表达的小部分词的分布。'
- en: Figure 2.13\. Highlights of word2vec vectors for the Hot 100 Billboard dataset
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.13\. Hot 100 Billboard 数据集 word2vec 向量的亮点
- en: '![](Images/02fig13_alt.jpg)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig13_alt.jpg)'
- en: Notice the expected regularities between “me” and “my” with respect to “you”
    and “your.” Also note the groups of similar words, or words used in similar contexts,
    which are good candidates for synonyms.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 注意“me”和“my”与“you”和“your”之间的预期规律性。同时注意相似词组，或用于相似上下文中的词，它们是同义词的良好候选。
- en: Now that you’ve learned a bit about how the word2vec algorithm works, let’s
    write some code and see it in action. Then you’ll be able to combine it with the
    search engine for synonym expansion.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些关于 word2vec 算法的工作原理，让我们编写一些代码并看看它的实际应用。然后你将能够将其与搜索引擎结合以进行同义词扩展。
- en: '|  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Deeplearning4j**'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '**Deeplearning4j**'
- en: Deeplearning4j (DL4J) is a deep learning library for the Java Virtual Machine
    (JVM). It has good adoption among Java users and a not-too-steep learning curve
    for early adopters. It also comes with an Apache 2 license, which is handy if
    you want to use it within a company and include it in a possibly non-open source
    product. Additionally, DL4J has tools to import models created with other frameworks
    such as Keras, Caffe, TensorFlow, Theano, and so on.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: Deeplearning4j (DL4J) 是一个用于 Java 虚拟机 (JVM) 的深度学习库。它在 Java 用户中拥有良好的采用率，并且对于早期采用者来说学习曲线并不陡峭。它还附带
    Apache 2 许可证，如果你想在公司内部使用它并将其包含在可能不是开源产品中，这将非常方便。此外，DL4J 还提供了导入使用其他框架（如 Keras、Caffe、TensorFlow、Theano
    等）创建的模型的工具。
- en: '|  |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 2.4.1\. Setting up word2vec in Deeplearning4j
  id: totrans-603
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 在 Deeplearning4j 中设置 word2vec
- en: In this book, we’ll use DL4J to implement neural network–based algorithms. Let’s
    see how to use it to set up a word2vec model.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将使用 DL4J 来实现基于神经网络的算法。让我们看看如何使用它来设置 word2vec 模型。
- en: DL4J has an out-of-the-box implementation of word2vec, based on the skip-gram
    model. You need to set up its configuration parameters and pass the input text
    you want to feed the search engine.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 基于skip-gram模型提供了word2vec的现成实现。你需要设置其配置参数，并传递你想要输入搜索引擎的输入文本。
- en: Keeping the song lyrics use case in mind, let’s feed word2vec the Billboard
    Hot 100 text file. You want output word vectors of a suitable dimension, so set
    that configuration parameter to 100 and the window size to 5.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到歌曲歌词用例，让我们将 Billboard Hot 100 文本文件输入到 word2vec 中。你希望输出具有合适维度的词向量，因此将配置参数设置为
    100，并将窗口大小设置为 5。
- en: Listing 2.9\. DL4J word2vec example
  id: totrans-607
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.9\. DL4J word2vec 示例
- en: '[PRE24]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Reads the corpus of text containing the lyrics**'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 读取包含歌词的语料库**'
- en: '***2* Sets up an iterator over the corpus**'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 设置语料库的迭代器**'
- en: '***3* Creates a configuration for word2vec**'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为 word2vec 创建配置**'
- en: '***4* Sets the number of dimensions the vector representations should have**'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 设置向量表示应具有的维度数**'
- en: '***5* Sets the window parameter**'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 设置窗口参数**'
- en: '***6* Sets word2vec to iterate over the selected corpus**'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 设置word2vec遍历所选语料库**'
- en: '***7* Uses the CBOW model**'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 使用CBOW模型**'
- en: '***8* Performs training**'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 执行训练**'
- en: '***9* Obtains the closest words to an input word**'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 获取输入词的最近词**'
- en: '***10* Prints the nearest words**'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 打印最近词**'
- en: 'You obtain the following output, which seems good enough:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 您得到以下输出，看起来已经足够好了：
- en: '[PRE25]'
  id: totrans-620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that you can alternatively use the skip-gram model by changing the `elementsLearningAlgorithm`.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以通过更改`elementsLearningAlgorithm`来选择性地使用skip-gram模型。
- en: Listing 2.10\. Using the skip-gram model
  id: totrans-622
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.10. 使用skip-gram模型
- en: '[PRE26]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1* Uses the skip-gram model**'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用skip-gram模型**'
- en: As you can see, it’s straightforward to set up such a model and obtain results
    in a reasonable time (training the word2vec model took around 30 seconds on a
    “normal” laptop). Keep in mind that you’ll now aim to use this in conjunction
    with the search engine, which should yield a better synonym-expansion algorithm.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，设置此类模型并获得结果非常简单（在“普通”笔记本电脑上训练word2vec模型大约需要30秒）。请记住，您现在将尝试将其与搜索引擎结合使用，这应该会产生更好的同义词扩展算法。
- en: 2.4.2\. Word2vec-based synonym expansion
  id: totrans-626
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2. 基于Word2vec的同义词扩展
- en: Now that you have this powerful tool in your hands, you need to be careful!
    When using WordNet, you have a constrained set of synonyms, so you can’t blow
    up the index. With word vectors generated by word2vec, you can ask the model to
    return the closest words for each word to be indexed. This might not be acceptable
    from a performance perspective (for both runtime and storage), so you have to
    come up with a strategy for using word2vec responsibly. One thing you can do is
    constrain the types of words for which you ask word2vec to get the nearest words.
    In natural language processing, it’s common to tag each word as a *part of speech*
    (PoS) that labels its syntactic role in a sentence. Common parts of speech are
    `NOUN`, `VERB`, and `ADJ`; there are also finer-grained ones like `NP` and `NC`
    (proper and common noun, respectively). For example, you might decide to use word2vec
    only for words whose PoS is either `NC` or `VERB`, to avoid bloating the index
    with synonyms for adjectives. Another technique would be to look at how informative
    the document is. A short text has a relatively poor probability of being hit with
    a query, because it’s composed of only a few terms. So you might decide to focus
    on such documents and expand their synonyms, rather than focusing on longer documents.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您手中有了这个强大的工具，您需要小心！当使用WordNet时，您有一个受限的同义词集合，因此您不能使索引膨胀。通过word2vec生成的词向量，您可以要求模型为每个要索引的词返回最近的词。这可能在性能方面可能不可接受（无论是运行时间还是存储），因此您必须制定一个策略来负责任地使用word2vec。您可以做的事情之一是限制您要求word2vec获取最近词的词的类型。在自然语言处理中，通常将每个词标记为*词性*（PoS），以标记其在句子中的句法角色。常见的词性有`NOUN`、`VERB`和`ADJ`；还有更细粒度的，如`NP`和`NC`（分别代表专有名词和普通名词）。例如，您可能决定仅对那些词性为`NC`或`VERB`的词使用word2vec，以避免形容词的同义词使索引膨胀。另一种技术是查看文档的信息量。简短文本被查询击中的概率相对较低，因为它只由几个术语组成。因此，您可能决定专注于这样的文档并扩展它们的同义词，而不是专注于较长的文档。
- en: On the other hand, the “informativeness” of a document doesn’t only depend on
    its size. Thus you might use other techniques, such as looking at term *weights*
    (the number of times a term appears in a piece of text) and skipping those that
    have a low weight.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，文档的“信息量”并不仅仅取决于其大小。因此，您可能需要使用其他技术，例如查看术语*权重*（一个术语在一段文本中出现的次数）并跳过那些权重低的术语。
- en: You could also choose to use word2vec results only if they have a good similarity
    score. If you use cosine distance to measure the nearest neighbors of a word vector,
    such neighbors may be too far away (a low similarity score) but still be the nearest.
    In that case, you could decide not to use those neighbors.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以选择仅当它们具有良好的相似度得分时才使用word2vec的结果。如果您使用余弦距离来衡量词向量的最近邻，这些邻居可能距离太远（相似度得分低），但仍然是最近的。在这种情况下，您可以选择不使用这些邻居。
- en: Now that you’ve trained a word2vec model on the Hot 100 Billboard dataset using
    Deeplearning4j, let’s use it in conjunction with the search engine to generate
    synonyms. As explained in [chapter 1](kindle_split_012.xhtml#ch01), a token filter
    performs operations on the terms provided by a tokenizer, such as filtering them
    or, as in this case, adding other terms to be indexed. A Lucene `TokenFilter`
    is based on the `incrementToken` API, which returns a `boolean` value that is
    false at the end of the token stream. Implementors of this API consume one token
    at a time (for example, by filtering or expanding a token). [Figure 2.14](#ch02fig14)
    shows a diagram of how word2vec-based synonym expansion is expected to work.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经使用 Deeplearning4j 在 Hot 100 Billboard 数据集上训练了一个 word2vec 模型，让我们将其与搜索引擎结合使用以生成同义词。如
    [第 1 章](kindle_split_012.xhtml#ch01) 所述，标记过滤器对分词器提供的术语执行操作，例如过滤它们，或者在本例中，添加其他要索引的术语。Lucene
    `TokenFilter` 基于 `incrementToken` API，该 API 返回一个 `boolean` 值，在标记流结束时为 `false`。此
    API 的实现者一次消费一个标记（例如，通过过滤或扩展标记）。[图 2.14](#ch02fig14) 展示了基于 word2vec 的同义词扩展预期如何工作。
- en: Figure 2.14\. Synonym expansion at search time, with word2vec
  id: totrans-631
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.14\. 使用 word2vec 的搜索时间同义词扩展
- en: '![](Images/02fig14_alt.jpg)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/02fig14_alt.jpg)'
- en: You’re finished with word2vec training, so you can create a synonym filter that
    will use the learned model to predict term synonyms during filtering. You’ll build
    a Lucene `TokenFilter` that can use DL4J word2vec on input tokens. This means
    implementing the left side of [figure 2.14](#ch02fig14).
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了 word2vec 训练，因此可以创建一个同义词过滤器，该过滤器将使用学习到的模型在过滤过程中预测术语同义词。你将构建一个 Lucene `TokenFilter`，它可以使用
    DL4J word2vec 对输入标记进行处理。这意味着实现 [图 2.14](#ch02fig14) 的左侧。
- en: The Lucene APIs for token filtering require you to implement the `incrementToken`
    method. This method will return `true` if there are still tokens to consume from
    the token stream or `false` if there are no more tokens left to consider for filtering.
    The basic idea is that the token filter will return `true` for all original tokens
    and `false` for all the related synonyms you get from word2vec.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene 的标记过滤 API 要求你实现 `incrementToken` 方法。此方法将在还有可消费的标记从标记流中返回 `true`，如果没有更多标记可供过滤考虑则返回
    `false`。基本思路是标记过滤器将对所有原始标记返回 `true`，对所有从 word2vec 获得的相关同义词返回 `false`。
- en: Listing 2.11\. Word2vec-based synonym expansion filter
  id: totrans-635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.11\. 基于 word2vec 的同义词扩展过滤器
- en: '[PRE27]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1* Creates a token filter that takes an already-trained word2vec model**'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个接受已训练的 word2vec 模型的标记过滤器**'
- en: '***2* Implements the Lucene API for token filtering**'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 实现 Lucene 的标记过滤 API**'
- en: '***3* Adds cached synonyms to the token stream (see the next code listing)**'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将缓存的同义词添加到标记流中（参见下一代码列表）**'
- en: '***4* Expands a token only if it’s not a synonym (to avoid loops in the expansion)**'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 仅在标记不是同义词时扩展标记（以避免扩展中的循环）**'
- en: '***5* For each term, uses word2vec to find the closest words that have an accuracy
    higher than a minAcc (for example, 0.35)**'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对于每个术语，使用 word2vec 找到最接近的单词，其准确率高于 minAcc（例如，0.35）**'
- en: '***6* Records no more than two synonyms for each token**'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 每个标记记录不超过两个同义词**'
- en: '***7* Records the synonym value**'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 记录同义词值**'
- en: '***8* Records the current state of the original term (not the synonym) in the
    token stream (for example, starting and ending position)**'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在标记流中记录原始术语的当前状态（不是同义词）（例如，起始和结束位置）**'
- en: '***9* Creates an object to contain the synonyms to be added to the token stream
    after all the original terms have been consumed**'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 创建一个对象来包含在所有原始术语消耗完毕后要添加到标记流中的同义词**'
- en: This code traverses all the terms and, when it finds a synonym, puts the synonym
    in a list of pending outputs to expand (the `outputs` list). You apply those pending
    terms to be added (the actual synonyms) after each original term has been processed,
    as shown next.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码遍历所有术语，并在找到同义词时，将同义词放入待扩展输出列表（`outputs` 列表）。在处理完每个原始术语后，应用这些待添加的术语（实际的同义词），如下所示。
- en: Listing 2.12\. Expanding pending synonyms
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.12\. 扩展待定同义词
- en: '[PRE28]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* Gets the first pending output to expand**'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取要扩展的第一个待输出**'
- en: '***2* Retrieves the state of the original term, including its text, its position
    in the text stream, and so on**'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 获取原始术语的状态，包括其文本、其在文本流中的位置等**'
- en: '***3* Sets the synonym text to that given by word2vec and previously saved
    in the pending output**'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将同义词文本设置为 word2vec 给出并之前保存在待输出中的文本**'
- en: '***4* Sets the type of the term as synonym**'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将术语的类型设置为同义词**'
- en: 'You use the word2vec output results as synonyms only if they have an accuracy
    greater than a certain threshold, as discussed in the previous section. The filter
    picks only the two words closest to the given term (according to word2vec) having
    an accuracy of at least 0.35 (which isn’t that high), for each term passed by
    the tokenizer. If you pass the sentence “I like pleasure spiked with pain and
    music is my airplane” to the filter, it will expand the word “airplane” with two
    additional words: “airplanes” and “aeroplane” (see the final part of the expanded
    token stream shown in [figure 2.15](#ch02fig15)).'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 你只有在它们具有高于一定阈值的确切度时才使用word2vec输出结果作为同义词，如前节所述。过滤器只选择与给定术语最接近的两个词（根据word2vec），每个术语的准确度至少为0.35（这并不高），对于通过分词器的每个术语。如果你将句子“我喜欢痛苦中带点快乐的愉悦，音乐是我的飞机”传递给过滤器，它将用两个额外的词扩展“飞机”这个词：“airplanes”和“aeroplane”（见[图2.15](#ch02fig15)中扩展的标记流的最末部分）。
- en: Figure 2.15\. Token stream after word2vec synonym expansion
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15\. word2vec同义词扩展后的标记流
- en: '![](Images/02fig15_alt.jpg)'
  id: totrans-655
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/02fig15_alt.jpg)'
- en: 2.5\. Evaluations and comparisons
  id: totrans-656
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 评估和比较
- en: 'As mentioned in [chapter 1](kindle_split_012.xhtml#ch01), you can usually capture
    metrics, including precision, recall, query with zero results, and so on, both
    before and after the introduction of query expansion. It’s also usually good to
    determine the best configuration set for all the parameters of a neural network.
    A generic neural network has many parameters you can adjust:'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](kindle_split_012.xhtml#ch01)所述，你通常可以在引入查询扩展前后捕获指标，包括精确度、召回率、零结果查询等。通常，确定神经网络所有参数的最佳配置集也是很好的。一个通用的神经网络有许多你可以调整的参数：
- en: The general network architecture, such as using one or more hidden layers
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用网络架构，例如使用一个或多个隐藏层
- en: The transformations performed in each layer
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层执行的转换
- en: The number of neurons in each layer
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层的神经元数量
- en: The connections between neurons belonging to different layers
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于不同层的神经元之间的连接
- en: The number of times (also called *epochs*) the network should read through all
    the training sets in order to reach its final state (possibly with a low error
    and high accuracy)
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络应该读取所有训练集的次数（也称为*epoch*），以达到其最终状态（可能具有低错误率和高精度）
- en: These parameters also apply to other machine learning techniques. In the case
    of word2vec, you can decide
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数也适用于其他机器学习技术。在word2vec的情况下，你可以决定
- en: The size of the generated word embeddings
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的词嵌入的大小
- en: The window used to create fragments for unsupervised training of models
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于为模型的无监督训练创建片段的窗口
- en: 'Which architecture to use: CBOW or skip-gram'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用哪种架构：CBOW或skip-gram
- en: As you can see, there are many possible parameter settings to try.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，有许多可能的参数设置可以尝试。
- en: '*Cross validation* is a method of optimizing the parameters while making sure
    a machine learning model performs well enough on data that’s different from the
    one used for training. With cross validation, the original dataset is split into
    three subsets: a training set, a validation set, and a test set. The training
    set is used as the data source to train the model. In practice, it’s often used
    to train a bunch of separate models with different settings for the available
    parameters. The cross-validation set is used to select the model that has the
    best-performing parameters. This can be done, for example, by taking each pair
    of input and desired output in the cross-validation set and seeing whether a model
    gives results equal or close to the desired output, when given that particular
    input. The test set is used the same way as the cross-validation set, except it’s
    only used by the model selected by testing on the cross-validation set. The accuracy
    of results on the test set can be considered a good measure of the model’s overall
    effectiveness.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证*是一种在确保机器学习模型在用于训练的数据之外的数据上表现良好的同时优化参数的方法。使用交叉验证，原始数据集被分成三个子集：训练集、验证集和测试集。训练集用作训练模型的dataSource。在实践中，它通常用于训练多个具有不同参数设置的独立模型。交叉验证集用于选择具有最佳性能参数的模型。例如，可以通过在交叉验证集中取每一对输入和期望输出，并查看模型是否给出与特定输入相等或接近的期望输出来完成此操作。测试集的使用方式与交叉验证集相同，但它仅由在交叉验证集上测试后选定的模型使用。测试集上的结果准确性可以被认为是衡量模型整体有效性的良好指标。'
- en: 2.6\. Considerations for production systems
  id: totrans-669
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6\. 生产系统的考虑因素
- en: In this chapter, you’ve seen how to use word2vec to generate synonyms from data
    to be indexed and searched. Most existing production systems already contain lots
    of indexed documents, and in such cases it’s often impossible to access the original
    data as it existed before it was indexed. In the case of indexing the top 100
    songs of the year to build a search engine of song lyrics, you have to take into
    account that the rankings of the most popular songs change every day, week, month,
    and year. This implies that the dataset will change over time; therefore, if you
    don’t keep old copies in separate storage, you won’t be able to build a word2vec
    model for all indexed documents (song lyrics) later.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经看到了如何使用word2vec从要索引和搜索的数据中生成同义词。大多数现有的生产系统已经包含大量的索引文档，在这种情况下，通常无法访问索引之前存在的原始数据。在索引年度前100首歌曲以构建歌词搜索引擎的情况下，您必须考虑到最受欢迎的歌曲排名每天、每周、每月和每年都在变化。这意味着数据集会随时间变化；因此，如果您不在单独的存储中保留旧副本，您将无法在以后为所有索引文档（歌词）构建word2vec模型。
- en: 'The solution to this problem is to work with the search engine as the primary
    data source. When you set up word2vec using DL4J, you fetched sentences from a
    single file:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是使用搜索引擎作为主要的数据源。当您使用DL4J设置word2vec时，您从单个文件中获取句子：
- en: '[PRE29]'
  id: totrans-672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Given an evolving system that’s fed song lyrics from different files daily,
    weekly, or monthly, you’ll need to take the sentences directly from the search
    engine. For this reason, you’ll build a `SentenceIterator` that reads stored values
    from the Lucene index.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个每天、每周或每月从不同文件中获取歌曲歌词的动态系统，您需要直接从搜索引擎获取句子。因此，您将构建一个`SentenceIterator`，它从Lucene索引中读取存储的值。
- en: Listing 2.13\. Fetching sentences for word2vec from the Lucene index
  id: totrans-674
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.13\. 从Lucene索引中获取用于word2vec的句子
- en: '[PRE30]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '***1* View of the index used to fetch the document values**'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 用于获取文档值的索引视图**'
- en: '***2* Specific field to fetch the values from**'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从特定字段获取值**'
- en: '***3* The identifier of the current document being fetched, because this is
    an iterator**'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 正在获取的当前文档的标识符，因为这是一个迭代器**'
- en: '***4* First document ID is always 0**'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 第一份文档ID总是0**'
- en: In the example of the song lyrics search engine, the text of the lyrics were
    indexed into the `text` field. You therefore fetch the sentences and words to
    be used for training the word2vec model from that field.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 在歌曲歌词搜索引擎的例子中，歌词文本被索引到`text`字段。因此，您需要从该字段获取用于训练word2vec模型的句子和单词。
- en: Listing 2.14\. Reading sentences from the Lucene index
  id: totrans-681
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.14\. 从Lucene索引中读取句子
- en: '[PRE31]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once you’ve set things up, you pass this new `SentenceIterator` to the word2vec
    implementation:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好，您就将这个新的`SentenceIterator`传递给word2vec实现：
- en: '[PRE32]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: During the training phase, the `SentenceIterator` is asked to iterate over `String`s.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，`SentenceIterator`被要求遍历`String`s。
- en: Listing 2.15\. For each document, passing field values to word2vec for training
  id: totrans-686
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.15\. 对于每个文档，将字段值传递给word2vec进行训练
- en: '[PRE33]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '***1* The iterator has more sentences if the current document identifier isn’t
    bigger than the number of documents contained in the index.**'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果当前文档标识符不大于索引中包含的文档数量，迭代器将包含更多句子。**'
- en: '***2* Gets the document with the current identifier (only the field you need
    is fetched)**'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 获取具有当前标识符的文档（只获取您需要的字段）**'
- en: '***3* Gets the value of the text field from the current Lucene Document as
    a String**'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从当前Lucene文档中获取文本字段的值作为字符串**'
- en: '***4* Returns the sentence, which is preprocessed if you set a preprocessor
    (for example, to remove unwanted characters or tokens)**'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果您设置了预处理器（例如，用于删除不需要的字符或标记），则返回句子，该句子已进行预处理**'
- en: '***5* Increments the document ID for the next iteration**'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 为下一次迭代增加文档ID**'
- en: This way, word2vec can be retrained frequently on existing search engines without
    having to maintain the original data. The synonym expansion filter can be kept
    up to date as the data in the search engine is updated.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，word2vec可以频繁地在现有的搜索引擎上重新训练，而无需维护原始数据。同义词扩展过滤器可以随着搜索引擎中的数据更新而保持最新。
- en: 2.6.1\. Synonyms vs. antonyms
  id: totrans-694
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1\. 同义词与反义词
- en: 'Imagine that you have the following sentences: “I like pizza,” “I hate pizza,”
    “I like pasta,” “I hate pasta,” “I love pasta,” and “I eat pasta.” This would
    be a small set of sentences for word2vec to use to learn accurate embeddings in
    real life. But you can clearly see that the terms “I” on the left and “pizza”
    and “pasta” on the right all share verbs in between. Because word2vec learns word
    embeddings using similar text fragments, you may end up with similar word vectors
    for the verbs “like,” “hate,” “love,” and “eat.” So word2vec may report that “love”
    is close to “like” and “eat” (which is fine, given that the sentences are all
    related to food) but also to “hate,” which is definitely not a synonym for “love.”'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有以下句子：“我喜欢披萨”，“我讨厌披萨”，“我喜欢意大利面”，“我讨厌意大利面”，“我热爱意大利面”，“我吃意大利面”。这对于word2vec来说是一小批句子，用于在现实生活中学习准确的嵌入。但你可以清楚地看到，左侧的“我”和右侧的“披萨”和“意大利面”之间都共享动词。因为word2vec使用相似的文本片段来学习单词嵌入，你可能会得到“like”、“hate”、“love”和“eat”这些动词的相似单词向量。所以word2vec可能会报告“love”接近“like”和“eat”（考虑到句子都与食物相关，这是可以接受的），但也接近“hate”，这绝对不是“love”的同义词。
- en: In some cases, this issue may not be important. Suppose you want to go out to
    dinner, and you’re searching for a nice restaurant on the internet. You write
    the query “reviews of restaurants people love” in a search engine. If you get
    reviews about “restaurants people hate,” then you’ll know where *not* to go. But
    this is an edge case; generally, you don’t want antonyms (the opposite of a synonym)
    to be expanded like synonyms.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这个问题可能并不重要。假设你想外出吃饭，你正在互联网上搜索一家好餐厅。你在搜索引擎中输入查询“人们喜欢的餐厅评论”。如果你得到关于“人们讨厌的餐厅”的评论，那么你就会知道*不去*哪里。但这是一个边缘情况；通常，你不想像同义词一样扩展反义词。
- en: 'Don’t worry—usually, the text has enough information to tell you that although
    “hate” and “love” appear in similar contexts, they aren’t proper synonyms. The
    fact that this corpus of text is only made of sentences like “I hate pizza” or
    “I like pasta” makes it more difficult: usually, “hate” and “like” also appear
    in other contexts, which helps word2vec figure out that they aren’t similar. To
    see that, let’s evaluate the nearest words of the word “nice” together with their
    similarity:'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——通常，文本有足够的信息告诉你，尽管“hate”和“love”出现在相似的环境中，但它们并不是真正的同义词。这个文本语料库只由像“I hate
    pizza”或“I like pasta”这样的句子组成，这使得问题更加困难：通常，“hate”和“like”也会出现在其他环境中，这有助于word2vec确定它们并不相似。为了说明这一点，让我们一起评估单词“nice”的最邻近单词及其相似度：
- en: '[PRE34]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The similarity between word vectors can help you exclude nearest neighbors
    that aren’t similar enough. A sample word2vec run over the Hot 100 Billboard dataset
    indicates that the nearest words of the word “nice” are “cute,” “unfair,” and
    “real”:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量之间的相似性可以帮助你排除那些不够相似的最邻近邻居。在Hot 100 Billboard数据集上运行的一个word2vec示例表明，单词“nice”的最邻近单词是“cute”、“unfair”和“real”：
- en: '[PRE35]'
  id: totrans-700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: “Cute” is a synonym. “Unfair” isn’t an antonym but an adjective that expresses
    negative feelings; it’s not a good result, because it’s in contrast with the positive
    nature of “nice” and “cute.” “Real” also doesn’t express the same general semantics
    as “nice.” To fix this, you can, for example, filter out the nearest neighbors
    whose similarity is less than the absolute value 0.5, or less than the highest
    similarity minus 0.1\. You assume that the first nearest neighbor is usually good
    enough, as long as its similarity is greater than 0.5; once this applies, you
    exclude words that are too far from the nearest neighbor. In this case, filtering
    out words whose similarity is less than the highest nearest neighbor similarity
    (0.61) minus 0.1, you filter out both “unfair” and “real” (each has a similarity
    less than 0.60).
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: “Cute”是一个同义词。“Unfair”不是一个反义词，而是一个表示负面情绪的形容词；它不是一个好的结果，因为它与“nice”和“cute”的积极性质形成对比。“Real”也没有表达与“nice”相同的通用语义。为了解决这个问题，例如，你可以过滤掉相似度小于绝对值0.5的最近邻居，或者小于最高相似度减去0.1的邻居。你假设第一个最近邻居通常足够好，只要它的相似度大于0.5；一旦应用了这个条件，你就排除了与最近邻居距离太远的单词。在这种情况下，过滤掉相似度小于最高最近邻居相似度（0.61）减去0.1的单词，你过滤掉了“unfair”和“real”（每个的相似度都小于0.60）。
- en: Summary
  id: totrans-702
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Synonym expansion can be a handy technique to improve recall and make the users
    of your search engine happier.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词扩展可以是一个有用的技术，用于提高召回率并使你的搜索引擎用户更加满意。
- en: Common synonym-expansion techniques are based on static dictionaries and vocabularies
    that might require manual maintenance or are often far from the data they’re used
    for.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的同义词扩展技术基于静态字典和词汇表，可能需要手动维护，或者通常与它们所使用的数据相差甚远。
- en: Feed-forward neural networks are the basis of many neural network architectures.
    In a feed-forward neural network, information flows from an input layer to an
    output layer; in between these two layers, there may be one or more hidden layers.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络是许多神经网络架构的基础。在前馈神经网络中，信息从输入层流向输出层；在这两个层之间，可能有一个或多个隐藏层。
- en: Word2vec is a feed-forward neural network–based algorithm for learning vector
    representations for words that can be used to find words with similar meanings—or
    that appear in similar contexts—so it’s reasonable to use it for synonym expansion,
    too.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec是一种基于前馈神经网络的算法，用于学习单词的向量表示，可用于查找具有相似意义或出现在相似上下文中的单词，因此也合理地将其用于同义词扩展。
- en: You can either use the continuous-bag-of-words or skip-gram architecture for
    word2vec. In CBOW, the target word is used as the output of the network, and the
    remaining words of the text fragments are used as inputs. In the skip-gram model,
    the target word is used as input, and the context words are outputs. Both work
    well, but skip-gram is usually preferred, because it works better with infrequent
    words.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以选择使用连续词袋模型或跳字模型架构进行word2vec。在CBOW中，目标词被用作网络的输出，而文本片段的其余单词被用作输入。在跳字模型中，目标词被用作输入，上下文词被用作输出。两者都效果良好，但通常更倾向于跳字模型，因为它在处理不常见单词时表现更佳。
- en: Word2vec models can provide good results, but you need to manage word senses
    or parts of speech when using it for synonyms.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec模型可以提供良好的结果，但您在使用它进行同义词扩展时需要管理词义或词性。
- en: In word2vec, be careful to avoid letting antonyms be used as synonyms.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在word2vec中，请注意避免将反义词用作同义词。
