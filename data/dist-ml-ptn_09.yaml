- en: 9 A complete implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 完整实现
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing data ingestion component with TensorFlow
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现数据摄取组件
- en: Defining the machine learning model and submitting distributed model training
    jobs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义机器学习模型并提交分布式模型训练作业
- en: Implementing a single-instance model server as well as replicated model servers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现单实例模型服务器以及复制模型服务器
- en: Building an efficient end-to-end workflow of our machine learning system
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建我们机器学习系统的有效全流程
- en: 'In the previous chapter of the book, we learned the basics of the four core
    technologies that we will use in our project: TensorFlow, Kubernetes, Kubeflow,
    and Argo Workflows. We learned that TensorFlow performs data processing, model
    building, and model evaluation. We also learned the basic concepts of Kubernetes
    and started our local Kubernetes cluster, which we will use as our core distributed
    infrastructure. In addition, we successfully submitted distributed model training
    jobs to the local Kubernetes cluster using Kubeflow. At the end of the last chapter,
    we learned how to use Argo Workflows to construct and submit a basic “hello world”
    workflow and a complex DAG-structured workflow.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前一章中，我们学习了我们将用于项目的四个核心技术的基础知识：TensorFlow、Kubernetes、Kubeflow和Argo Workflows。我们了解到TensorFlow执行数据处理、模型构建和模型评估。我们还学习了Kubernetes的基本概念，并启动了我们的本地Kubernetes集群，我们将将其作为我们的核心分布式基础设施。此外，我们成功地将分布式模型训练作业提交到本地Kubernetes集群，使用了Kubeflow。在上一章的结尾，我们学习了如何使用Argo
    Workflows构建和提交一个基本的“hello world”工作流和一个复杂的DAG结构化工作流。
- en: In this chapter, we’ll implement the end-to-end machine learning system with
    the architecture we designed in chapter 7\. We will completely implement each
    component, which will incorporate the previously discussed patterns. We’ll use
    several popular frameworks and cutting-edge technologies, particularly TensorFlow,
    Kubernetes, Kubeflow, Docker, and Argo Workflows, which we introduced in chapter
    8 to build different components of a distributed machine learning workflow in
    this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用第7章中设计的架构来实现端到端机器学习系统。我们将完全实现每个组件，这些组件将结合之前讨论的模式。我们将使用几个流行的框架和尖端技术，特别是TensorFlow、Kubernetes、Kubeflow、Docker和Argo
    Workflows，这些我们在第8章中介绍，以在本章中构建分布式机器学习工作流的不同组件。
- en: 9.1 Data ingestion
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 数据摄取
- en: The first component in our end-to-end workflow is data ingestion. We’ll be using
    the Fashion-MNIST dataset introduced in section 2.2 to build the data ingestion
    component. Figure 9.1 shows this component in the dark box on the left of the
    end-to-end workflow.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们端到端工作流中的第一个组件是数据摄取。我们将使用第2.2节中介绍的Fashion-MNIST数据集来构建数据摄取组件。图9.1显示了在端到端工作流左侧深色框中的该组件。
- en: '![09-01](../../OEBPS/Images/09-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![09-01](../../OEBPS/Images/09-01.png)'
- en: Figure 9.1 The data ingestion component (dark box) in the end-to-end machine
    learning system
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 全流程机器学习系统中的数据摄取组件（深色框）
- en: Recall that this dataset consists of a training set of 60,000 examples and a
    test set of 10,000 examples. Each example is a 28 × 28 grayscale image representing
    one Zalando’s article image and associated with a label from 10 classes. In addition,
    the Fashion-MNIST dataset is designed to serve as a direct drop-in replacement
    for the original MNIST dataset for benchmarking machine learning algorithms. It
    shares the same image size and structure of training and testing splits. Figure
    9.2 is a screenshot of the collection of images for all 10 classes (T-shirt/top,
    trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot) from
    Fashion-MNIST, where each class takes three rows in the screenshot.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这个数据集包含一个包含60,000个示例的训练集和一个包含10,000个示例的测试集。每个示例是一个28 × 28的灰度图像，代表一个Zalando的商品图像，并关联到10个类别中的一个标签。此外，Fashion-MNIST数据集被设计为作为原始MNIST数据集的直接替换，用于基准测试机器学习算法。它共享相同的训练和测试分割的图像大小和结构。图9.2是Fashion-MNIST中所有10个类别（T恤/上衣、裤子、开衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴）的图像集合的截图，其中每个类别在截图中占据三行。
- en: '![09-02](../../OEBPS/Images/09-02.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![09-02](../../OEBPS/Images/09-02.png)'
- en: Figure 9.2 A screenshot of the collection of images from the Fashion-MNIST dataset
    for all 10 classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt,
    sneaker, bag, and ankle boot)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 从Fashion-MNIST数据集的所有10个类别（T恤/上衣、裤子、开衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴）收集的图像截图
- en: Figure 9.3 is a closer look at the first few example images in the training
    set together with their corresponding labels in text above each of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 是对训练集中前几个示例图像的仔细查看，以及每个图像上方的对应文本标签。
- en: '![09-03](../../OEBPS/Images/09-03.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![09-03](../../OEBPS/Images/09-03.png)'
- en: Figure 9.3 A closer look at the first few example images in the training set
    with their corresponding labels in text
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 仔细查看训练集中前几个示例图像及其对应的文本标签
- en: In section 9.1.1, we’ll go through the implementation of a single-node data
    pipeline that ingests the Fashion-MNIST dataset. Furthermore, section 9.1.2 will
    cover the implementation of the distributed data pipeline to prepare the data
    for our distributed model training in section 9.2.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 9.1.1 节中，我们将介绍单节点数据管道的实现，该管道用于处理 Fashion-MNIST 数据集。此外，9.1.2 节将涵盖分布式数据管道的实现，为
    9.2 节中的分布式模型训练准备数据。
- en: 9.1.1 Single-node data pipeline
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 单节点数据管道
- en: Let’s first take a look at how to build a single-node data pipeline that works
    locally on your laptop without using a local Kubernetes cluster. The best way
    for a machine learning program written in TensorFlow to consume data is through
    methods in tf.data module. The tf.data API allows users to build complex input
    pipelines easily. For example, the pipeline for an image model might aggregate
    data from files in various file systems, apply random transformations to each
    image, and create batches from the images for model training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看如何构建一个单节点数据管道，该管道在您的笔记本电脑上本地工作，而不使用本地 Kubernetes 集群。对于用 TensorFlow 编写的机器学习程序来说，通过
    tf.data 模块中的方法来消费数据是最佳方式。tf.data API 允许用户轻松构建复杂的输入管道。例如，图像模型的管道可能从各种文件系统中的文件中聚合数据，对每个图像应用随机转换，并从图像中创建用于模型训练的批次。
- en: The tf.data API enables it to handle large amounts of data, read from different
    data formats, and perform complex transformations. It contains a tf.data.Dataset
    abstraction that represents a sequence of elements, in which each element consists
    of one or more components. Let’s use the image pipeline to illustrate this. An
    element in an image input pipeline might be a single training example, with a
    pair of tensor components representing the image and its label.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data API 允许它处理大量数据，从不同的数据格式中读取，并执行复杂的转换。它包含一个 tf.data.Dataset 抽象，表示一系列元素，其中每个元素由一个或多个组件组成。让我们用图像管道来举例说明。图像输入管道中的一个元素可能是一个单独的训练示例，由一对张量组件表示图像及其标签。
- en: 'The following listing provides the code snippet to load the Fashion-MNIST dataset
    into a tf.data.Dataset object and performs some necessary preprocessing steps
    to prepare for our model training:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了代码片段，用于将 Fashion-MNIST 数据集加载到 tf.data.Dataset 对象中，并执行一些必要的预处理步骤以准备我们的模型训练：
- en: Scale the dataset from the range (0, 255] to (0., 1.].
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集的缩放范围从 (0, 255] 调整到 (0., 1.]。
- en: Cast the image multidimensional arrays into float32 type that our model can
    accept.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像的多维数组转换为模型可以接受的 float32 类型。
- en: Select the training data, cache it in memory to speed up training, and shuffle
    it with a buffer size of 10,000.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择训练数据，将其缓存在内存中以加快训练速度，并使用 10,000 个缓冲区大小对其进行洗牌。
- en: Listing 9.1 Loading the Fashion-MNIST dataset
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 加载 Fashion-MNIST 数据集
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we imported tensorflow_datasets module. The TensorFlow Datasets, which
    consists of a collection of datasets for various tasks such as image classification,
    object detection, document summarization, etc., can be used with TensorFlow and
    other Python machine learning frameworks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们导入了 tensorflow_datasets 模块。TensorFlow 数据集，它由一系列用于各种任务（如图像分类、目标检测、文档摘要等）的数据集组成，可以与
    TensorFlow 和其他 Python 机器学习框架一起使用。
- en: The tf.data.Dataset object is a shuffled dataset where each element consists
    of the images and their labels with the shape and data type information as in
    the following listing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset 对象是一个洗牌后的数据集，其中每个元素由图像及其标签组成，其形状和数据类型信息如下列表所示。
- en: Listing 9.2 Inspecting the tf.data object
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2 检查 tf.data 对象
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 9.1.2 Distributed data pipeline
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 分布式数据管道
- en: Now let’s look at how we can consume our dataset in a distributed fashion. We’ll
    be using tf.distribute.MultiWorkerMirroredStrategy for distributed training in
    the next section. Let’s assume we have instantiated a strategy object. We will
    instantiate our dataset inside the strategy’s scope via Python’s with syntax using
    the same function we previously defined for the single-node use case.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何以分布式方式消耗我们的数据集。在下一节中，我们将使用tf.distribute.MultiWorkerMirroredStrategy进行分布式训练。假设我们已经实例化了一个策略对象。我们将通过Python的with语法在策略的作用域内实例化我们的数据集，使用与之前为单节点使用情况定义的相同函数。
- en: We will need to tweak a few configurations to build our distributed input pipeline.
    First, we create repeated batches of data where the total batch size equals the
    batch size per replica times the number of replicas over which gradients are aggregated.
    This ensures that we will have enough records to train each batch in each of the
    model training workers. In other words, the number of replicas in sync equals
    the number of devices taking part in the gradient allreduce operation during model
    training. For instance, when a user or the training code calls next() on the distributed
    data iterator, a per replica batch size of data is returned on each replica. The
    rebatched dataset cardinality will always be a multiple of the number of replicas.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要调整一些配置来构建我们的分布式输入管道。首先，我们创建重复的数据批次，其中总批次大小等于每个副本的批次大小乘以聚合梯度的副本数量。这确保了我们将为每个模型训练工作者中的每个批次提供足够的记录。换句话说，同步的副本数量等于在模型训练期间参与梯度allreduce操作的设备数量。例如，当用户或训练代码在分布式数据迭代器上调用next()时，每个副本都会返回一个按副本划分的数据批次大小。重新批处理的数据集基数总是副本数量的倍数。
- en: In addition, we want to configure tf.data to enable automatic data sharding.
    Since the dataset is in the distributed scope, the input dataset will be sharded
    automatically in multiworker training mode. More specifically, each dataset will
    be created on the CPU device of the worker, and each set of workers will train
    the model on a subset of the entire dataset when tf.data.experimental.AutoShardPolicy
    is set to AutoShardPolicy.DATA. One benefit is that during each model training
    step, a global batch size of non-overlapping dataset elements will be processed
    by each worker. Each worker will process the whole dataset and discard the portion
    that is not for itself. Note that for this mode to partition the dataset elements
    correctly, the dataset needs to produce elements in a deterministic order, which
    should already be guaranteed by the TensorFlow Datasets library we use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想配置tf.data以启用自动数据分片。由于数据集在分布式范围内，在多工作者训练模式下，输入数据集将自动分片。更具体地说，每个数据集将在工作者的CPU设备上创建，并且当tf.data.experimental.AutoShardPolicy设置为AutoShardPolicy.DATA时，每组工作者将在整个数据集的子集上训练模型。一个好处是，在每次模型训练步骤中，每个工作者将处理全局批次大小的非重叠数据集元素。每个工作者将处理整个数据集，并丢弃不属于其自身的部分。请注意，为了正确分区数据集元素，数据集需要以确定性的顺序产生元素，这应该已经由我们使用的TensorFlow
    Datasets库保证。
- en: Listing 9.3 Configuring distributed data pipeline
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 配置分布式数据管道
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 9.2 Model training
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 模型训练
- en: We went through the implementation of the data ingestion component for both
    local-node and distributed data pipelines and discussed how we can shard the dataset
    properly across different workers so that it would work with distributed model
    training. In this section, let’s dive into the implementation details for our
    model training component. An architecture diagram of the model training component
    can be found in figure 9.4.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了本地节点和分布式数据管道的数据摄取组件的实施，以及我们如何在不同工作者之间正确地分片数据集，以便它能够与分布式模型训练一起工作。在本节中，让我们深入了解模型训练组件的实施细节。模型训练组件的架构图可以在图9.4中找到。
- en: '![09-04](../../OEBPS/Images/09-04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![09-04](../../OEBPS/Images/09-04.png)'
- en: Figure 9.4 A diagram of the model training component in the overall architecture.
    Three different model training steps are followed by a model selection step. These
    model training steps would train three different models—namely, CNN, CNN with
    dropout, and CNN with batch normalization—competing with each other for better
    statistical performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 整体架构中模型训练组件的示意图。在三个不同的模型训练步骤之后，有一个模型选择步骤。这些模型训练步骤将训练三个不同的模型——即CNN、带有dropout的CNN和带有批归一化的CNN——它们相互竞争以获得更好的统计性能。
- en: We will learn how to define those three models with TensorFlow in section 9.2.1
    and execute the distributed model training jobs with Kubeflow in section 9.2.2\.
    In section 9.2.3, we will implement the model selection step that picks the top
    model that will be used in the model serving component in our end-to-end machine
    learning workflow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第9.2.1节中学习如何使用TensorFlow定义这三个模型，并在第9.2.2节中执行分布式模型训练作业。在第9.2.3节中，我们将实现模型选择步骤，该步骤选择将用于我们端到端机器学习工作流程中模型服务组件的顶级模型。
- en: 9.2.1 Model definition and single-node training
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 模型定义和单节点训练
- en: Next, we’ll look at the TensorFlow code to define and initialize the first model,
    a convolutional neural network (CNN) model we introduced in previous chapters
    with three convolutional layers. We initialize the model with Sequential(), meaning
    we’ll add the layers sequentially. The first layer is the input layer, where we
    specify the shape of the input pipeline that we defined previously. Note that
    we also explicitly give a name to the input layer so we can pass the correct key
    in our inference inputs, which we will discuss in more depth in section 9.3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看TensorFlow代码来定义和初始化第一个模型，这是一个卷积神经网络（CNN）模型，我们在前面的章节中介绍过，具有三个卷积层。我们使用Sequential()初始化模型，这意味着我们将按顺序添加层。第一层是输入层，我们指定了之前定义的输入管道的形状。请注意，我们还明确地为输入层命名，以便我们可以在推理输入中传递正确的键，我们将在第9.3节中更深入地讨论这一点。
- en: After adding the input layer, three convolutional layers, followed by max-pooling
    layers and dense layers, are added to the sequential model. We’ll then print out
    a summary of the model architecture and compile the model with Adam as its optimizer,
    accuracy as the metric we use to evaluate the model, and sparse categorical cross-entropy
    as the loss function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加输入层、三个卷积层、随后是最大池化层和密集层之后，我们将打印出模型架构的摘要，并使用Adam作为其优化器、准确率作为我们用于评估模型的指标，以及稀疏分类交叉熵作为损失函数来编译模型。
- en: Listing 9.4 Defining the basic CNN model
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 定义基本CNN模型
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve successfully defined our basic CNN model. Next, we define two models based
    on the CNN model. One adds a batch normalization layer to force the pre-activations
    to have zero mean and unit standard deviation for every neuron (activation) in
    a particular layer. The other model has an additional dropout layer where half
    of the hidden units will be dropped randomly to reduce the complexity of the model
    and speed up computation. The rest of the code is the same as the basic CNN model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功定义了我们的基本CNN模型。接下来，我们基于CNN模型定义了两个模型。一个模型添加了一个批量归一化层，以强制特定层的每个神经元（激活）的预激活具有零均值和单位标准差。另一个模型有一个额外的dropout层，其中一半的隐藏单元将被随机丢弃，以减少模型的复杂性和加快计算速度。其余的代码与基本CNN模型相同。
- en: Listing 9.5 Defining the variations of the basic CNN model
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 定义基本CNN模型的变体
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the models are defined, we can train them locally on our laptops. Let’s
    use the basic CNN model as an example. We will create four callbacks that will
    be executed during model training:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了模型，我们就可以在我们的笔记本电脑上本地训练它们。让我们以基本的CNN模型为例。我们将创建四个将在模型训练期间执行的回调：
- en: PrintLR—Callback to print the learning rate at the end of each epoch
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PrintLR—在每个epoch结束时打印学习率的回调
- en: TensorBoard—Callback to start the interactive TensorBoard visualization to monitor
    the training progress and model architecture
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard—回调以启动交互式TensorBoard可视化，以监控训练进度和模型架构
- en: ModelCheckpoint—Callback to save model weights for model inference later
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ModelCheckpoint—回调以保存模型权重，以便稍后进行模型推理
- en: LearningRateScheduler—Callback to decay the learning rate at the end of each
    epoch
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LearningRateScheduler—在每个epoch结束时衰减学习率的回调
- en: Once these callbacks are defined, we’ll pass it to the fit() method for training.
    The fit() method trains the model with a specified number of epochs and steps
    per epoch. Note that the numbers here are for demonstration purposes only to speed
    up our local experiments and may not sufficiently produce a model with good quality
    in real-world applications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了这些回调，我们将将其传递给fit()方法进行训练。fit()方法使用指定的epoch数和每个epoch的步数来训练模型。请注意，这里的数字仅用于演示目的，以加快我们的本地实验，可能不足以在实际应用中生成高质量的模型。
- en: Listing 9.6 Modeling training with callbacks
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 使用回调进行模型训练
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ll see the model training progress like the following in the logs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在日志中看到以下模型训练进度：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Based on this summary, 93,000 parameters will be trained during the process.
    The shape and the number of parameters in each layer can also be found in the
    summary.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个总结，在过程中将训练 93,000 个参数。每个层的形状和参数数量也可以在总结中找到。
- en: 9.2.2 Distributed model training
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 分布式模型训练
- en: Now that we’ve defined our models and can train them locally in a single machine,
    the next step is to insert the distributed training logic in the code so that
    we can run model training with multiple workers using the collective communication
    pattern that we introduced in the book. We’ll use the tf.distribute module that
    contains MultiWorkerMirroredStrategy. It’s a distribution strategy for synchronous
    training on multiple workers. It creates copies of all variables in the model’s
    layers on each device across all workers. This strategy uses a distributed collective
    implementation (e.g., all-reduce), so multiple workers can work together to speed
    up training. If you don’t have appropriate GPUs, you can replace communication_options
    with other implementations. Since we want to ensure the distributed training can
    run on different machines that might not have GPUs, we’ll replace it with CollectiveCommunication.AUTO
    so that it will pick any available hardware automatically.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型并且可以在单台机器上本地训练它们，下一步是在代码中插入分布式训练逻辑，以便我们可以使用书中介绍的全局通信模式运行具有多个工作器的模型训练。我们将使用包含
    MultiWorkerMirroredStrategy 的 tf.distribute 模块。这是一个在多个工作器上进行同步训练的分布式策略。它在所有工作器的每个设备上创建了模型层中所有变量的副本。此策略使用分布式集体实现（例如，all-reduce），因此多个工作器可以一起工作以加快训练速度。如果您没有适当的
    GPU，您可以将 communication_options 替换为其他实现。由于我们希望确保分布式训练可以在可能没有 GPU 的不同机器上运行，我们将将其替换为
    CollectiveCommunication.AUTO，这样它将自动选择任何可用的硬件。
- en: Once we define our distributed training strategy, we’ll initiate our distributed
    input data pipeline (as mentioned previously in section 9.1.2) and the model inside
    the strategy scope. Note that defining the model inside the strategy scope is
    required since TensorFlow knows how to copy the variables in the model’s layers
    to each worker adequately based on the strategy. Here we define different model
    types (basic CNN, CNN with dropout, and CNN with batch normalization) based on
    the command-line arguments we pass to this Python script.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了我们的分布式训练策略，我们将在策略作用域内启动我们的分布式输入数据管道（如前所述，在 9.1.2 节中），以及策略作用域内的模型。请注意，在策略作用域内定义模型是必需的，因为
    TensorFlow 根据策略能够适当地在每个工作器中复制模型层的变量。在这里，我们根据传递给这个 Python 脚本的命令行参数定义不同的模型类型（基本
    CNN、带有 dropout 的 CNN 和带有批归一化的 CNN）。
- en: We’ll get to the rest of the flags soon. Once the data pipeline and the model
    are defined inside the scope, we can use fit() to train the model outside the
    distribution strategy scope.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会接触到其余的标志。一旦数据管道和模型在作用域内定义，我们就可以使用 fit() 函数在分布式策略作用域之外训练模型。
- en: Listing 9.7 Distributed model training logic
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.7 分布式模型训练逻辑
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once the model training is finished via fit() function, we want to save the
    model. One common mistake that users can easily make is saving models on all the
    workers, which may not save the completed model correctly and wastes computational
    resources and storage. The correct way to fix this problem is to save only the
    model on the chief worker. We can inspect the environment variable TF_CONFIG,
    which contains the cluster information, such as the task type and index, to see
    whether the worker is chief. Also, we want to save the model to a unique path
    across workers to avoid unexpected errors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过 fit() 函数完成模型训练，我们希望保存模型。用户可能会犯的一个常见错误是在所有工作器上保存模型，这可能会导致无法正确保存完成的模型，并浪费计算资源和存储空间。修复此问题的正确方法是只保存主工作器上的模型。我们可以检查环境变量
    TF_CONFIG，它包含集群信息，例如任务类型和索引，以查看工作器是否为主工作器。此外，我们希望将模型保存到工作器之间的唯一路径，以避免意外错误。
- en: Listing 9.8 Saving a model with a chief worker
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.8 使用主工作器保存模型
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So far, we’ve seen two command-line flags already—namely, saved_model_dir and
    model_type. Listing 9.9 provides the rest of the main function that will parse
    those command-line arguments. In addition to those two arguments, there’s another
    checkpoint_dir argument that we will use to save our model to the TensorFlow SavedModel
    format that can be easily consumed for our model serving component. We will discuss
    that in detail in section 9.3\. We also disabled the progress bar for the TensorFlow
    Datasets module to reduce the logs we will see.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了两个命令行标志——即saved_model_dir和model_type。列表9.9提供了解析这些命令行参数的其余主要函数。除了这两个参数之外，还有一个checkpoint_dir参数，我们将使用它将我们的模型保存到TensorFlow
    SavedModel格式，这种格式可以很容易地被我们的模型服务组件消费。我们将在第9.3节中详细讨论这一点。我们还禁用了TensorFlow Datasets模块的进度条，以减少我们将看到的日志。
- en: Listing 9.9 Entry point main function
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 入口点main函数
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ve just finished writing our Python script that contains the distributed
    model training logic. Let’s containerize it and build the image used to run distributed
    training in our local Kubernetes cluster. In our Dockerfile, we’ll use the Python
    3.9 base image, install TensorFlow and TensorFlow Datasets modules via pip, and
    copy our multiworker distributed training Python script.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成了包含分布式模型训练逻辑的Python脚本的编写。让我们将其容器化并构建用于在我们本地Kubernetes集群中运行分布式训练的镜像。在我们的Dockerfile中，我们将使用Python
    3.9基础镜像，通过pip安装TensorFlow和TensorFlow Datasets模块，并复制我们的多工作节点分布式训练Python脚本。
- en: Listing 9.10 Containerization
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.10 容器化
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We then build the image from the Dockerfile we just defined. We also need to
    import the image to k3d cluster since our cluster does not have access to our
    local image registry. We then set the current namespace to be “kubeflow”. Please
    read chapter 8 and follow the instructions to install the required components
    we need for this project.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从刚才定义的Dockerfile构建镜像。由于我们的集群无法访问我们的本地镜像仓库，我们还需要将镜像导入到k3d集群中。然后，我们将当前命名空间设置为“kubeflow”。请阅读第8章并按照说明安装我们为此项目所需的组件。
- en: Listing 9.11 Building and importing the docker image
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 构建和导入docker镜像
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once the worker Pods are completed, all files in the Pod will be recycled. Since
    we are running distributed model training across multiple workers in Kubernetes
    Pods, all the model checkpoints will be lost, and we don’t have a trained model
    for model serving. To address this problem, we’ll use PersistentVolume (PV) and
    PersistentVolumeClaim (PVC).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成工作Pod，Pod中的所有文件都将被回收。由于我们在Kubernetes Pods中的多个工作节点上运行分布式模型训练，所有模型检查点都将丢失，我们没有用于模型服务的训练模型。为了解决这个问题，我们将使用持久卷（PV）和持久卷声明（PVC）。
- en: PV is a storage in the cluster that has been provisioned by an administrator
    or dynamically provisioned. It is a resource in the cluster, just like a node
    is a cluster resource. PVs are volume plugins like Volumes, but have a life cycle
    independent of any individual Pod that uses the PV. In other words, PVs will persist
    and live even after the Pods are completed or deleted.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: PV是在集群中由管理员或动态配置的存储。它就像节点是集群资源一样，是集群中的资源。PV是类似于卷的卷插件，但它们的生命周期独立于使用PV的任何单个Pod。换句话说，PV将在Pod完成或删除后继续存在和存活。
- en: A PVC is a request for storage by a user. It is similar to a Pod. Pods consume
    node resources, and PVCs consume PV resources. Pods can request specific levels
    of resources (CPU and memory). Claims can request specific size and access modes
    (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, or ReadWriteMany).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PVC是用户对存储的请求。它与Pod类似。Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定的资源级别（CPU和内存）。声明可以请求特定的尺寸和访问模式（例如，它们可以是ReadWriteOnce、ReadOnlyMany或ReadWriteMany）。
- en: Let’s create a PVC to submit a request for storage that will be used in our
    worker Pods to store the trained model. Here we only submit a request for 1 Gi
    storage with ReadWriteOnce access mode.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个PVC，提交一个请求用于在我们的工作Pod中存储训练模型。在这里，我们只请求1 Gi的存储空间，使用ReadWriteOnce访问模式。
- en: Listing 9.12 Persistent volume claim
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.12 持久卷声明
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we’ll create the PVC.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建PVC。
- en: Listing 9.13 Creating the PVC
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.13 创建PVC
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, let’s define the TFJob spec we introduced in chapter 7 with the image
    we just built that contains the distributed training script. We pass the necessary
    command arguments to the container to train the basic CNN model. The volumes field
    in the Worker spec specifies the name of the persistent volume claim that we just
    created, and the volumeMounts field in the containers spec specifies what folder
    to mount the files between the volume to the container. The model will be saved
    in the /trained_model folder inside the volume.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义第7章中介绍的TFJob规范，使用我们刚刚构建的包含分布式训练脚本的镜像。我们将必要的命令参数传递到容器中，以训练基本的CNN模型。Worker规范中的volumes字段指定了我们刚刚创建的持久卷声明的名称，而containers规范中的volumeMounts字段指定了将文件从卷挂载到容器的文件夹。模型将保存在卷内的/trained_model文件夹中。
- en: Listing 9.14 Distributed model training job definition
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.14 分布式模型训练作业定义
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Then we can submit this TFJob to our cluster to start our distributed model
    training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这个TFJob提交到我们的集群以启动分布式模型训练。
- en: Listing 9.15 Submitting TFJob
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.15 提交TFJob
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the worker Pods are completed, we’ll notice the following logs from the
    Pods that indicate we trained the model in a distributed fashion and the workers
    communicated with each other successfully:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Worker Pods完成，我们会从Pod中注意到以下日志，表明我们以分布式方式训练了模型，并且工作者之间成功进行了通信：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 9.2.3 Model selection
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 模型选择
- en: So far, we’ve implemented our distributed model training component. We’ll eventually
    train three different models, as mentioned in section 9.2.1, and then pick the
    top model for model serving. Let’s assume that we have trained those models successfully
    by submitting three different TFJobs with different model types.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了我们的分布式模型训练组件。我们最终将训练三个不同的模型，正如9.2.1节中提到的，然后选择最佳模型用于模型服务。假设我们已经通过提交三个不同类型的TFJobs成功训练了这些模型。
- en: Next, we write the Python code that loads the testing data and trained models
    and then evaluate their performance. We will load each trained model from different
    folders by keras.models.load_model() function and execute model.evaluate(), which
    returns the loss and accuracy. Once we find the model with the highest accuracy,
    we can copy the model to a new version in a different folder—namely, 4—which will
    be used by our model serving component.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写Python代码来加载测试数据和训练好的模型，然后评估它们的性能。我们将通过keras.models.load_model()函数从不同的文件夹中加载每个训练好的模型，并执行model.evaluate()，它返回损失和准确率。一旦我们找到准确率最高的模型，我们就可以将其复制到不同文件夹中的新版本——即4号文件夹——这将被我们的模型服务组件使用。
- en: Listing 9.16 Model evaluation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.16 模型评估
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that the latest version, 4, in the trained_model/saved_model_versions folder
    will be picked up by our serving component. We will talk about that in the next
    section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在trained_model/saved_model_versions文件夹中的最新版本，即4号，将被我们的服务组件选中。我们将在下一节中讨论这一点。
- en: We then add this Python script to our Dockerfile, rebuild the container image,
    and create a Pod that runs the model selection component. The following is the
    YAML file that configures the model selection Pod.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这个Python脚本添加到我们的Dockerfile中，重新构建容器镜像，并创建一个运行模型选择组件的Pod。以下是为配置模型选择Pod的YAML文件。
- en: Listing 9.17 Model selection Pod definition
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.17 模型选择Pod定义
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When inspecting the logs, we see the third model has the highest accuracy,
    so we will copy it to a new version to be used for the model serving component:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查日志时，我们看到第三个模型的准确率最高，因此我们将它复制到新版本，用于模型服务组件：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 9.3 Model serving
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 模型服务
- en: Now that we have implemented our distributed model training as well as model
    selection among the trained models. The next component we will implement is the
    model serving component. The model serving component is essential to the end-user
    experience since the results will be shown to our users directly, and if it’s
    not performant enough, our users will know immediately. Figure 9.5 shows the model
    training component in the overall architecture.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了分布式模型训练以及训练模型之间的模型选择。接下来，我们将实现模型服务组件。模型服务组件对于最终用户体验至关重要，因为结果将直接展示给我们的用户，如果性能不足，我们的用户会立即知道。图9.5显示了整体架构中的模型训练组件。
- en: '![09-05](../../OEBPS/Images/09-05.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![09-05](../../OEBPS/Images/09-05.png)'
- en: Figure 9.5 Model serving component (dark boxes) in the end-to-end machine learning
    system
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 端到端机器学习系统中的模型服务组件（深色框）
- en: In figure 9.5, the model serving components are shown as the two dark boxes
    between the model selection and result aggregation steps. Let’s first implement
    our single-server model inference component in section 9.3.1 and then make it
    more scalable and performant in section 9.3.2.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 9.5 中，模型服务组件显示为模型选择和结果聚合步骤之间的两个深色框。让我们首先在 9.3.1 节中实现我们的单服务器模型推理组件，然后在 9.3.2
    节中使其更具可扩展性和性能。
- en: 9.3.1 Single-server model inference
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 单服务器模型推理
- en: The model inference Python code is very similar to the model evaluation code.
    The only difference is that we use the model.predict() method instead of evaluate()
    after we load the trained model. This is an excellent way to test whether the
    trained model can make predictions as expected.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 模型推理的 Python 代码与模型评估代码非常相似。唯一的区别是在加载训练模型后，我们使用 model.predict() 方法而不是 evaluate()。这是一种测试训练模型是否能够按预期进行预测的极好方式。
- en: Listing 9.18 Model prediction
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.18 模型预测
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Alternatively, you can start a TensorFlow Serving ([https://github.com/tensorflow/serving](https://github.com/tensorflow/serving))
    server locally like in the following listing once it’s installed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，一旦安装完成，你可以在以下列表中像本地启动 TensorFlow Serving ([https://github.com/tensorflow/serving](https://github.com/tensorflow/serving))
    服务器。
- en: Listing 9.19 TensorFlow Serving command
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.19 TensorFlow Serving 命令
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This seems straightforward and works well if we are only experimenting locally.
    However, there are more performant ways to build our model serving component that
    will pave our path to running distributed model serving that incorporates the
    replicated model server pattern that we introduced in previous chapters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很简单，如果我们在本地进行实验，效果很好。然而，还有更多高效的方式来构建我们的模型服务组件，这将为我们通往运行分布式模型服务铺平道路，该服务结合了我们之前章节中介绍的复制模型服务器模式。
- en: 'Before we dive into a better solution, let’s make sure our trained model can
    work with our prediction inputs, which will be a JSON-structured list of image
    bytes with the key "instances" and "image_bytes", like the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨更好的解决方案之前，让我们确保我们的训练模型可以与我们的预测输入一起工作，这些输入将是一个包含键 "instances" 和 "image_bytes"
    的 JSON 结构化图像字节列表，如下所示：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now is the time to modify our distributed model training code to make sure
    the model has the correct serving signature that’s compatible with our supplied
    inputs. We define the preprocessing function that does the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是修改我们的分布式模型训练代码的时候了，以确保模型具有与提供的输入兼容的正确服务签名。我们定义的预处理函数执行以下操作：
- en: Decodes the images from bytes
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从字节解码图像
- en: Resizes the image to 28 × 28 that’s compatible with our model architecture
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像调整大小到 28 × 28，以与我们的模型架构兼容
- en: Casts the images to tf.uint8
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为 tf.uint8
- en: Defines the input signature with string type and key as image_bytes
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入签名，类型为字符串，键为 image_bytes
- en: Once the preprocessing function is defined, we can define the serving signature
    via tf.TensorSpec() and then pass it to tf.saved_model.save() method to save the
    model that is compatible with our input format and preprocess it before TensorFlow
    Serving makes inference calls.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预处理函数被定义，我们可以通过 tf.TensorSpec() 定义服务签名，然后将其传递给 tf.saved_model.save() 方法以保存与我们的输入格式兼容的模型，并在
    TensorFlow Serving 进行推理调用之前对其进行预处理。
- en: Listing 9.20 Model serving signature definitions
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.20 模型服务签名定义
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once the distributed model training script is modified, we can rebuild our container
    image and retrain our model from scratch, following the instructions in section
    9.2.2.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦修改了分布式模型训练脚本，我们可以重新构建我们的容器镜像，并按照 9.2.2 节中的说明从头开始重新训练我们的模型。
- en: Next, we will use KServe, as we mentioned in the technologies overview, to create
    an inference service. Listing 9.21 provides the YAML to define the KServe inference
    service. We need to specify the model format so that KServe knows what to use
    for serving the model (e.g., TensorFlow Serving). In addition, we need to supply
    the URI to the trained model. In this case, we can specify the PVC name and the
    path to the trained model, following the format pvc://<pvc-name>/<model-path>.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 KServe，正如我们在技术概述中提到的，来创建一个推理服务。列表 9.21 提供了定义 KServe 推理服务的 YAML。我们需要指定模型格式，以便
    KServe 知道用于服务的模型（例如，TensorFlow Serving）。此外，我们需要提供训练模型的 URI。在这种情况下，我们可以指定 PVC 名称和训练模型的路径，格式为
    pvc://<pvc-name>/<model-path>。
- en: Listing 9.21 Inference service definition
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.21 推理服务定义
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s install KServe and create our inference service!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装 KServe 并创建我们的推理服务！
- en: Listing 9.22 Installing KServe and creating the inference service
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.22 安装 KServe 和创建推理服务
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can check its status to make sure it’s ready for serving.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查其状态以确保它已准备好提供服务。
- en: Listing 9.23 Getting the details of the inference service
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.23 获取推理服务的详细信息
- en: '[PRE26]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once the service is created, we port-forward it to local so that we can send
    requests to it locally.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务创建成功，我们将它端口转发到本地，以便我们可以在本地向其发送请求。
- en: Listing 9.24 Port-forwarding the inference service
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.24 端口转发推理服务
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should be able to see the following if the port-forwarding is successful:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果端口转发成功，你应该能看到以下内容：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Let’s open another terminal and execute the following Python script to send
    a sample inference request to our model serving service and print out the response
    text.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开另一个终端并执行以下 Python 脚本来向我们的模型服务服务发送一个示例推理请求并打印出响应文本。
- en: Listing 9.25 Using Python to send an inference request
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.25 使用 Python 发送推理请求
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The response from our KServe model serving service, which includes the predicted
    probabilities for each class in the Fashion-MNIST dataset, is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 KServe 模型服务服务的响应如下，它包括 Fashion-MNIST 数据集中每个类别的预测概率：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Alternatively, we can use curl to send requests.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 curl 发送请求。
- en: Listing 9.26 Using curl to send an inference request
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.26 使用 curl 发送推理请求
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output probabilities should be the same as the ones we just saw:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的概率应该与我们刚才看到的相同：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As mentioned previously, even though we specified the entire directory that
    contains the trained model in the KServe InferenceService spec, the model serving
    service that utilizes TensorFlow Serving will pick the latest version 4 from that
    particular folder, which is our best model we selected in section 9.2.3\. We can
    observe that from the logs of the serving Pod.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，尽管我们在 KServe InferenceService 规范中指定了包含训练模型的整个目录，但利用 TensorFlow Serving
    的模型服务服务将选择该特定文件夹中的最新版本 4，这是我们第 9.2.3 节中选择的最佳模型。我们可以从服务 Pod 的日志中观察到这一点。
- en: Listing 9.27 Inspecting the model server logs
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.27 检查模型服务器日志
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here’s the logs:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是日志：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 9.3.2 Replicated model servers
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 复制模型服务器
- en: In the previous section, we successfully deployed our model serving service
    in our local Kubernetes cluster. This might be sufficient for running local serving
    experiments, but it’s far from ideal if it’s deployed to production systems that
    serve real-world model serving traffic. The current model serving service is a
    single Kubernetes Pod, where the allocated computational resources are limited
    and requested in advance. When the number of model serving requests increases,
    the single-instance model server can no longer support the workloads and may run
    out of computational resources.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们在本地 Kubernetes 集群中成功部署了我们的模型服务服务。这可能对于运行本地服务实验来说是足够的，但如果它部署到生产系统中，这些系统服务于现实世界的模型服务流量，那就远远不够理想了。当前的模型服务服务是一个
    Kubernetes Pod，分配的计算资源有限且预先请求。当模型服务请求的数量增加时，单实例模型服务器将无法支持工作负载，并且可能会耗尽计算资源。
- en: To address the problem, we need to have multiple instances of model servers
    to handle a larger amount of dynamic model serving requests. Fortunately, KServe
    can autoscale based on the average number of in-flight requests per Pod, which
    uses the Knative Serving autoscaler.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要有多个模型服务器实例来处理更多的动态模型服务请求。幸运的是，KServe 可以根据每个 Pod 的平均飞行请求数量自动扩展，这使用了
    Knative Serving 自动扩展器。
- en: The following listing provides the inference service spec with autoscaling enabled.
    The scaleTarget field specifies the integer target value of the metric type the
    autoscaler watches for. In addition, the scaleMetric field defines the scaling
    metric type watched by autoscaler. The possible metrics are concurrency, RPS,
    CPU, and memory. Here we only allow one concurrent request to be processed by
    each inference service instance. In other words, when there are more requests,
    we will start a new inference service Pod to handle each additional request.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了启用自动扩展的推理服务规范。scaleTarget 字段指定了自动扩展器监视的指标类型的整数目标值。此外，scaleMetric 字段定义了自动扩展器监视的扩展指标类型。可能的指标有并发性、RPS、CPU
    和内存。在这里，我们只允许每个推理服务实例处理一个并发请求。换句话说，当有更多请求时，我们将启动一个新的推理服务 Pod 来处理每个额外的请求。
- en: Listing 9.28 Replicated model inference services
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.28 复制模型推理服务
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let’s assume there’s no request, and we should only see one inference service
    Pod that’s up and running. Next, let’s send traffic in 30-second spurts, maintaining
    five in-flight requests. We use the same service hostname and ingress address,
    as well as the same inference input and trained model. Note that we are using
    the tool hey, a tiny program that sends some load to a web application. Follow
    the instructions at [https://github.com/rakyll/hey](https://github.com/rakyll/hey)
    to install it before executing the following command.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设没有请求，我们应该只看到一个运行中的推理服务Pod。接下来，让我们以30秒的间隔发送流量，保持五个正在进行的请求。我们使用相同的服务主机名和入口地址，以及相同的推理输入和训练模型。请注意，我们正在使用工具hey，这是一个向Web应用程序发送一些负载的小程序。在执行以下命令之前，请按照[https://github.com/rakyll/hey](https://github.com/rakyll/hey)上的说明安装它。
- en: Listing 9.29 Sending traffic to test the load
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.29 发送流量以测试负载
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the expected output from the command, which includes a summary
    of how the inference service handled the requests. For example, the service has
    processed 230,160 bytes of inference inputs and 95.7483 requests per second. You
    can also find a nice response-time histogram and a latency distribution that might
    be useful:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的命令预期输出包括推理服务处理请求的摘要。例如，该服务处理了230,160字节的推理输入和每秒95.7483个请求。您还可以找到有用的响应时间直方图和延迟分布：
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As expected, we see five running inference service Pods processing the requests
    concurrently, where each Pod handles only one request.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们看到有五个正在运行的推理服务Pod并发处理请求，其中每个Pod只处理一个请求。
- en: Listing 9.30 Getting the list of model server Pods
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.30 获取模型服务器Pod列表
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Once the hey command is completed, we will only see one running Pod.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: hey命令完成后，我们只会看到一个正在运行的Pod。
- en: Listing 9.31 Getting the list of model server Pods again
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.31 再次获取模型服务器Pod列表
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 9.4 The end-to-end workflow
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 端到端工作流程
- en: We have just implemented all the components in the previous sections. Now it’s
    time to put things together! In this section, we’ll define an end-to-end workflow
    using Argo Workflows that includes the components we just implemented. Please
    go back to previous sections if you are still unfamiliar with all the components
    and refresh your knowledge of basic Argo Workflows from chapter 8.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了前几节中所有组件。现在，是时候将它们组合在一起了！在本节中，我们将使用Argo Workflows定义一个端到端工作流程，该工作流程包括我们刚刚实现的组件。如果您对所有组件仍然不熟悉，请回到前面的章节，并刷新第8章中关于基本Argo
    Workflows的知识。
- en: Here’s a recap of what the end-to-end workflow we will implement looks like.
    Figure 9.6 is a diagram of the end-to-end workflow that we are building. The diagram
    includes two model serving steps for illustration purposes, but we will only implement
    one step in our Argo workflow. It will autoscale to more instances based on requests
    traffic, as mentioned in section 9.3.2.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对我们将要实施的端到端工作流程的回顾。图9.6是我们正在构建的端到端工作流程的示意图。该图为了说明目的包含了两个模型服务步骤，但我们在Argo工作流程中只实现一个步骤。它将根据请求流量自动扩展到更多实例，如第9.3.2节所述。
- en: '![09-06](../../OEBPS/Images/09-06.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![09-06](../../OEBPS/Images/09-06.png)'
- en: Figure 9.6 An architecture diagram of the end-to-end machine learning system
    we are building
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 我们正在构建的端到端机器学习系统架构图
- en: In the next sections, we will define the entire workflow by connecting the steps
    sequentially with Argo and then optimize the workflow for future executions by
    implementing step memoization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过使用Argo按顺序连接步骤来定义整个工作流程，然后通过实现步骤记忆化来优化工作流程以供未来执行。
- en: 9.4.1 Sequential steps
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 顺序步骤
- en: 'First, let’s look at the entry point templates and the main steps involved
    in the workflow. The entry point template name is tfjob-wf, which consists of
    the following steps (for simplicity, each step uses a template with the same name):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看入口模板和涉及工作流程的主要步骤。入口模板的名称是tfjob-wf，它由以下步骤组成（为了简单起见，每个步骤使用具有相同名称的模板）：
- en: data-ingestion-step contains the data ingestion step, which we will use to download
    and preprocess the dataset before model training.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: data-ingestion-step 包含数据摄入步骤，我们将使用它来在模型训练之前下载和预处理数据集。
- en: distributed-tf-training-steps is a step group that consists of multiple substeps,
    where each substep represents a distributed model training step for a specific
    model type.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: distributed-tf-training-steps 是一个由多个子步骤组成的步骤组，其中每个子步骤代表特定模型类型的分布式模型训练步骤。
- en: model-selection-step is a step that selects the top model from among the different
    models we have trained in previous steps.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model-selection-step`是一个步骤，用于从我们在先前步骤中训练的不同模型中选择最佳模型。'
- en: create-model-serving-service creates the model serving serve via KServe.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`create-model-serving-service`通过KServe创建模型服务。'
- en: Listing 9.32 Workflow entry point templates
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.32 工作流入口模板
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that we specify the podGC strategy to be OnPodSuccess since we’ll be creating
    a lot of Pods for different steps within our local k3s cluster with limited computational
    resources, so deleting the Pods right after they are successful can free up computational
    resources for the subsequent steps. The OnPodCompletion strategy is also available;
    it deletes Pods on completion regardless of whether they failed or succeeded.
    We won’t use that since we want to keep failed Pods to debug what went wrong.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们指定了podGC策略为OnPodSuccess，因为我们将在有限的计算资源下创建大量Pod，用于我们本地k3s集群中的不同步骤，因此，在Pod成功后立即删除Pod可以释放后续步骤的计算资源。OnPodCompletion策略也是可用的；无论Pod失败还是成功，它都会在完成时删除Pod。我们不会使用它，因为我们希望保留失败的Pod以调试出错的原因。
- en: In addition, we also specify our volumes and PVC to ensure we can persist any
    files that will be used in the steps. We can save the downloaded dataset into
    the persistent volume for model training and then persist the trained model for
    the subsequent model serving step.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还指定了我们的卷和PVC，以确保我们可以持久化在步骤中使用的任何文件。我们可以将下载的数据集保存到持久卷中用于模型训练，然后持久化训练好的模型以供后续模型服务步骤使用。
- en: The first step, the data ingestion step, is very straightforward. It only specifies
    the container image and the data ingestion Python script to execute. The Python
    script is a one-line code with tfds.load(name='fashion_mnist') to download the
    dataset to the container’s local storage, which will be mounted to our persistent
    volume.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，数据摄入步骤，非常直接。它只指定了容器镜像和数据摄入的Python脚本以执行。这个Python脚本是一行代码，包含`tfds.load(name='fashion_mnist')`以将数据集下载到容器的本地存储，该存储将被挂载到我们的持久卷上。
- en: Listing 9.33 Data ingestion step
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.33 数据摄入步骤
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The next step is a step group that consists of multiple substeps, where each
    substep represents a distributed model training step for a specific model type
    (e.g., basic CNN, CNN with dropout, and CNN with batch norm). The following listing
    provides the template that defines all the substeps. Distributed training steps
    for multiple models dictate that these will be executed in parallel.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤是一个由多个子步骤组成的步骤组，其中每个子步骤代表特定模型类型（例如，基本CNN、带有dropout的CNN和带有批归一化的CNN）的分布式模型训练步骤。以下列表提供了定义所有子步骤的模板。对于多个模型的分布式训练步骤，这些步骤将并行执行。
- en: Listing 9.34 Distributed training step groups
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.34 分布式训练步骤组
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s use the first substep, which runs a distributed model training for the
    basic CNN model, as an example. The main content of this step template is the
    resource field, which includes the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以第一个子步骤为例，该子步骤运行基本CNN模型的分布式模型训练。此步骤模板的主要内容是资源字段，它包括以下内容：
- en: The custom resource definition (CRD) or manifest to take action upon. In our
    case, we create a TFJob as part of this step.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义（CRD）或清单，用于采取行动。在我们的案例中，我们创建一个TFJob作为此步骤的一部分。
- en: The conditions that indicate whether the CRD is created successfully. In our
    case, we ask Argo to watch the field status.replicaStatuses.Worker.succeeded and
    status.replicaStatuses.Worker.failed.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指示CRD是否成功创建的条件。在我们的案例中，我们要求Argo监视`status.replicaStatuses.Worker.succeeded`和`status.replicaStatuses.Worker.failed`字段状态。
- en: Inside the container spec in the TFJob definition, we specify the model type
    and save the trained model to a different folder so it’s easy to pick and save
    the best model for model serving in subsequent steps. We also want to make sure
    to attach the persistent volumes so the trained model can be persisted.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在TFJob定义中的容器规范内部，我们指定模型类型并将训练好的模型保存到不同的文件夹，以便在后续步骤中轻松选择和保存最佳模型用于模型服务。我们还想确保附加持久卷，以便训练好的模型可以被持久化。
- en: Listing 9.35 CNN model training step
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.35 CNN模型训练步骤
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: For the rest of the substeps in distributed-tf-training-steps, the spec is very
    similar, except the saved model directory and model type arguments are different.
    The next step is model selection, for which we will supply the same container
    image but execute the model selection Python script we implemented earlier.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`distributed-tf-training-steps`中的其余子步骤，规范非常相似，只是保存的模型目录和模型类型参数不同。下一个步骤是模型选择，我们将提供相同的容器镜像，但执行我们之前实现的模型选择Python脚本。
- en: Listing 9.36 Model selection step Caption here
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.36 模型选择步骤 标题在此处
- en: '[PRE44]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Make sure these additional scripts are included in your Dockerfile and that
    you have rebuilt the image and re-imported it to your local Kubernetes cluster.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些额外的脚本包含在你的Dockerfile中，并且你已经重建了镜像并将其重新导入到你的本地Kubernetes集群中。
- en: Once the model selection step is implemented, the last step in the workflow
    is the model serving step that starts a KServe model inference service. It’s a
    resource template similar to the model training steps but with KServe’s InferenceService
    CRD and a success condition that applies to this specific CRD.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型选择步骤被实现，工作流中的最后一步是模型服务步骤，它启动一个KServe模型推理服务。它是一个类似于模型训练步骤的资源模板，但带有KServe的InferenceService
    CRD和一个适用于此特定CRD的成功条件。
- en: Listing 9.37 The model serving step
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.37 模型服务步骤
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let’s submit this workflow now!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在提交这个工作流！
- en: Listing 9.38 Submitting the end-to-end workflow
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.38 提交端到端工作流
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Once the data ingestion step is completed, the associated Pod will be deleted.
    When we list the Pods again while it’s executing the distributed model training
    steps, we’ll see the Pods with names prefixed by tfjob-wf-f4bql-cnn-model-, which
    are the Pods responsible for monitoring the status of distributed model training
    for different model types. In addition, each model training for each model type
    contains two workers with the name matching the pattern multi-worker-training-*-worker-*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据摄取步骤完成，相关的Pod将被删除。当我们再次列出Pods，在它执行分布式模型训练步骤时，我们会看到以tfjob-wf-f4bql-cnn-model-为前缀的Pods，这些Pod负责监控不同模型类型的分布式模型训练状态。此外，每个模型类型的每个模型训练包含两个名为multi-worker-training-*-worker-*的工作者。
- en: Listing 9.39 Getting the list of Pods
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.39 获取Pod列表
- en: '[PRE47]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Once the remaining steps are completed, and the model serving has started successfully,
    the workflow should have a Succeeded status. We’ve just finished the execution
    of the end-to-end workflow.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦剩余的步骤完成，并且模型服务启动成功，工作流应该具有“成功”状态。我们刚刚完成了端到端工作流的执行。
- en: 9.4.2 Step memoization
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 步骤记忆化
- en: To speed up future executions of workflows, we can utilize cache and skip certain
    steps that have recently run. In our case, the data ingestion step can be skipped
    since we don’t have to download the same dataset again and again.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快工作流的未来执行，我们可以利用缓存并跳过最近运行过的某些步骤。在我们的案例中，数据摄取步骤可以被跳过，因为我们不需要反复下载相同的数据集。
- en: 'Let’s first take a look at the logs from our data ingestion step:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看我们数据摄取步骤的日志：
- en: '[PRE48]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The dataset has been downloaded to a path in the container. If the path is mounted
    to our persistent volume, it will be available for any future workflow runs. Let’s
    use the step memoization feature provided by Argo Workflows to optimize our workflow.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经被下载到容器中的某个路径。如果这个路径被挂载到我们的持久卷上，它将可供任何未来的工作流运行。让我们使用Argo Workflows提供的步骤记忆化功能来优化我们的工作流。
- en: Inside the step template, we supply the memoize field with the cache key and
    age of the cache. When a step is completed, a cache will be saved. When this step
    runs again in a new workflow, it will check whether the cache is created within
    the past hour. If so, this step will be skipped, and the workflow will proceed
    to execute subsequent steps. For our application, our dataset does not change
    so, theoretically, the cache should always be used, and we specify 1 hour here
    for demonstration purposes only. In real-world applications, you may want to adjust
    that according to how frequently the data is updated.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤模板内部，我们为memoize字段提供缓存键和缓存年龄。当步骤完成时，会保存一个缓存。当这个步骤在新工作流中再次运行时，它会检查缓存是否在过去一小时之内创建。如果是这样，这个步骤将被跳过，工作流将接着执行后续步骤。对于我们的应用，我们的数据集不会改变，所以理论上，缓存应该总是被使用，我们在这里只为了演示目的指定了1小时。在现实世界的应用中，你可能需要根据数据更新的频率来调整这个值。
- en: Listing 9.40 Memoization for the data ingestion step
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.40 数据摄取步骤的记忆化
- en: '[PRE49]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Let’s run the workflow for the first time and pay attention to the Memoization
    Status field in the workflow’s node status. The cache is not hit because this
    is the first time the step is run.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们第一次运行这个工作流，并注意工作流节点状态中的“记忆化状态”字段。由于这是第一次运行该步骤，缓存没有命中。
- en: Listing 9.41 Checking the node statuses of the workflow
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.41 检查工作流的节点状态
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'If we run the same workflow again within one hour, we will notice that the
    step is skipped (indicated by hit: true in the Memoization Status field):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们在一小时之内再次运行相同的流程，我们会注意到步骤被跳过（在“记忆化状态”字段中指示为hit: true）：'
- en: '[PRE51]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In addition, note that the Finished At and Started At timestamps are the same.
    That is, this step is completed instantly without having to re-execute from scratch.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意“完成时间”和“开始时间”戳是相同的。也就是说，这一步可以立即完成，无需从头开始重新执行。
- en: All the cache in Argo Workflows is saved in a Kubernetes ConfigMap object. The
    cache contains the node ID, step outputs, and cache creation timestamp, as well
    as the timestamp when this cache is last hit.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 中的所有缓存都保存在 Kubernetes ConfigMap 对象中。缓存包含节点 ID、步骤输出和缓存创建时间戳，以及此缓存最后被访问的时间戳。
- en: Listing 9.42 Checking the details of the configmap
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.42 检查 configmap 的详细信息
- en: '[PRE52]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The data ingestion component implements a distributed input pipeline for the
    Fashion-MNIST dataset with TensorFlow that makes it easy to integrate with distributed
    model training.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取组件使用 TensorFlow 实现了 Fashion-MNIST 数据集的分布式输入管道，这使得它很容易与分布式模型训练集成。
- en: Machine learning models and distributed model training logic can be defined
    in TensorFlow and then executed in a distributed fashion in the Kubernetes cluster
    with the help of Kubeflow.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型和分布式模型训练逻辑可以在 TensorFlow 中定义，然后借助 Kubeflow 在 Kubernetes 集群中以分布式方式执行。
- en: Both the single-instance model server and the replicated model servers can be
    implemented via KServe. The autoscaling functionality of KServe can automatically
    create additional model serving Pods to handle the increasing number of model
    serving requests.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单实例模型服务器和复制模型服务器都可以通过 KServe 实现。KServe 的自动扩展功能可以自动创建额外的模型服务 Pod 来处理不断增加的模型服务请求。
- en: We implemented our end-to-end workflow that includes all the components of our
    system in Argo Workflows and used step memoization to avoid time-consuming and
    redundant data ingestion step.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在 Argo Workflows 中实现了我们的端到端工作流程，该工作流程包括我们系统中的所有组件，并使用步骤记忆化来避免耗时且冗余的数据摄取步骤。
