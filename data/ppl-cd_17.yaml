- en: 13 Collecting continuous delivery metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 收集持续交付指标
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Monitoring Jenkins and its jobs effectively
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效监控 Jenkins 及其作业
- en: Forwarding Jenkins build logs to a centralized logging platform
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Jenkins 构建日志转发到集中式日志平台
- en: Parsing Jenkins logs into something structured and queryable
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Jenkins 日志解析成结构化和可查询的形式
- en: Exposing Jenkins internal metrics with Prometheus
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus 暴露 Jenkins 内部指标
- en: Building interactive dashboards with Grafana
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Grafana 构建交互式仪表板
- en: Creating metric-based alerts for Jenkins
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Jenkins 创建基于指标的警报
- en: 'In the previous chapters, you learned to design, build, and deploy a Jenkins
    cluster from scratch by using automation tools; you also learned to set up a fully
    working CI/CD pipeline for several cloud-native applications. In this chapter,
    we will dive into advanced Jenkins topics: monitoring a running Jenkins server
    and detecting anomalies and resource starvation. Along the way, we will cover
    how to build a centralized logging platform for Jenkins logs.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你学习了如何使用自动化工具从头开始设计、构建和部署 Jenkins 集群；你还学习了如何为几个云原生应用设置一个完全工作的 CI/CD 流水线。在本章中，我们将深入探讨高级
    Jenkins 主题：监控运行中的 Jenkins 服务器以及检测异常和资源耗尽。在这个过程中，我们将介绍如何为 Jenkins 日志构建一个集中式日志平台。
- en: 13.1 Monitoring Jenkins cluster health
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 监控 Jenkins 集群健康
- en: The cluster we built in chapter 5 consists of a Jenkins master and workers,
    with each node running inside an EC2 instance. Figure 13.1 shows a typical Jenkins
    node configuration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 5 章中构建的集群由一个 Jenkins 主节点和多个工作节点组成，每个节点都在 EC2 实例内部运行。图 13.1 显示了一个典型的 Jenkins
    节点配置。
- en: '![](Images/CH13_F01_Labouardy.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F01_Labouardy.png)'
- en: Figure 13.1 Jenkins distributed build architecture
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 Jenkins 分布式构建架构
- en: So far, the Jenkins cluster is working as expected. However, you should never
    take your IT infrastructure for granted. Your Jenkins master or workers one day
    will break and will need to be replaced. So, how do you know if your Jenkins cluster
    is working effectively if you aren’t monitoring it?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Jenkins 集群运行得如预期。然而，你永远不应该将你的 IT 基础设施视为理所当然。你的 Jenkins 主节点或工作节点总有一天会出故障并需要更换。那么，如果你不监控它，你怎么知道你的
    Jenkins 集群是否在有效运行呢？
- en: Monitoring Jenkins should become a crucial part of your IT management. Monitoring
    helps you look for abnormalities and spot issues on instances running the cluster,
    saves you money as it minimizes the network downtime, and enhances efficiency.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 Jenkins 应该成为你 IT 管理的关键部分。监控可以帮助你寻找异常并发现集群中运行的实例上的问题，通过最小化网络中断来节省金钱，并提高效率。
- en: In AWS, you can monitor Jenkins instances by using Amazon CloudWatch ([https://
    aws.amazon.com/cloudwatch](https://aws.amazon.com/cloudwatch)). The platform consumes
    data coming from all AWS services and allows the user to visualize, query, and
    take action on the data. By default, Amazon EC2 sends metrics data to CloudWatch.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 中，你可以使用 Amazon CloudWatch ([https://aws.amazon.com/cloudwatch](https://aws.amazon.com/cloudwatch))
    监控 Jenkins 实例。该平台消耗来自所有 AWS 服务的数据，并允许用户可视化、查询并对数据进行操作。默认情况下，Amazon EC2 将指标数据发送到
    CloudWatch。
- en: Note You can use Azure Monitor ([http://mng.bz/wQYQ](https://shortener.manning.com/wQYQ))
    or Google Cloud’s operations ([https://cloud.google.com/monitoring/quickstart-lamp](https://cloud.google.com/monitoring/quickstart-lamp))
    if you want to monitor the overall health and performance of Jenkins instances
    running in Azure or GCP environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你想在 Azure 或 GCP 环境中监控 Jenkins 实例的整体健康和性能，可以使用 Azure Monitor ([http://mng.bz/wQYQ](https://shortener.manning.com/wQYQ))
    或 Google Cloud 的操作 ([https://cloud.google.com/monitoring/quickstart-lamp](https://cloud.google.com/monitoring/quickstart-lamp))。
- en: Navigate to the Amazon CloudWatch console and jump to the All Metrics tab. Then,
    under EC2, look for instances running the cluster by typing their instance ID
    on the search bar, as shown in figure 13.2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到 Amazon CloudWatch 控制台并跳转到所有指标选项卡。然后，在 EC2 下，通过在搜索栏中输入它们的实例 ID 来查找运行集群的实例，如图
    13.2 所示。
- en: '![](Images/CH13_F02_Labouardy.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F02_Labouardy.png)'
- en: Figure 13.2 Key metrics for EC2 monitoring
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 EC2 监控的关键指标
- en: You will see a pretty long list of reported metrics for your Jenkins EC2 instances.
    You can scroll and select one or more metrics to display (for example, EC2 instance
    CPU utilization) and create a graph widget to display them, as shown in figure
    13.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到一份相当长的报告指标列表，针对你的 Jenkins EC2 实例。你可以滚动并选择一个或多个指标来显示（例如，EC2 实例 CPU 利用率），并创建一个图形小部件来显示它们，如图
    13.3 所示。
- en: '![](Images/CH13_F03_Labouardy.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F03_Labouardy.png)'
- en: Figure 13.3 The percentage of allocated EC2 compute units currently in use on
    the Jenkins instances
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 当前在Jenkins实例上使用的已分配EC2计算单元的百分比
- en: By default, EC2 reports metrics to CloudWatch in 5-minute intervals. However,
    if your Jenkins cluster is being extensively used (for example, hosting multiple
    jobs and scheduling many CI/CD pipelines), you can enable the enhanced monitoring
    feature ([http://mng.bz/GOZR](http://mng.bz/GOZR)) on each instance to get metrics
    in 1-minute intervals (though an additional cost applies).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，EC2以5分钟间隔向CloudWatch报告度量。然而，如果您的Jenkins集群被广泛使用（例如，托管多个作业和调度许多CI/CD管道），您可以在每个实例上启用增强监控功能([http://mng.bz/GOZR](http://mng.bz/GOZR))以获取1分钟间隔的度量（尽管会产生额外的费用）。
- en: CloudWatch also offers dashboards, which provide a quick view of how your instances
    are performing as well as tremendous flexibility in terms of data visualization—for
    example, zooming in or rescaling.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch也提供仪表板，可以快速查看您的实例性能，同时在数据可视化方面具有极大的灵活性——例如，放大或缩放。
- en: You can customize the dashboard and add additional graphs showing, for example,
    the number of bytes received and sent out on all network interfaces, or disk usage
    (bytes written and read from all instance store volumes), as demonstrated in figure
    13.4.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自定义仪表板并添加额外的图表，例如，显示所有网络接口接收和发送的字节数，或磁盘使用情况（从所有实例存储卷写入和读取的字节数），如图13.4所示。
- en: '![](Images/CH13_F04_Labouardy.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F04_Labouardy.png)'
- en: Figure 13.4 Building the CloudWatch dashboard to monitor Jenkins instances
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 构建CloudWatch仪表板以监控Jenkins实例
- en: Now you know how to monitor Jenkins instances using CloudWatch. However, it
    can be error-prone and tedious to set up CloudWatch monitoring for all your Jenkins
    instances (and remembering to do it for Jenkins workers created for scaling events).
    Additionally, some metrics are unavailable through CloudWatch (such as memory
    usage). Hence, we will use an advanced monitoring stack.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何使用CloudWatch监控Jenkins实例。然而，为所有Jenkins实例设置CloudWatch监控可能会出错且繁琐（以及记住为用于扩展事件的Jenkins工作节点执行此操作）。此外，一些度量通过CloudWatch不可用（例如，内存使用）。因此，我们将使用高级监控堆栈。
- en: Note The Amazon CloudWatch agent can be installed on EC2 instances to report
    additional and useful metrics. This feature is seldom used, but it is good to
    know it exists. Refer to the official guide at [http://mng.bz/q5J2](https://shortener.manning.com/q5J2)
    for instructions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Amazon CloudWatch代理可以安装在EC2实例上以报告额外的有用度量。这个功能很少使用，但了解它的存在是好的。有关说明，请参阅官方指南[http://mng.bz/q5J2](https://shortener.manning.com/q5J2)。
- en: Many tools, from open source to a commercial level, can help you monitor your
    infrastructure and notify you of any failure. (Section 13.3 covers how to set
    up alerts that will notify you in near real-time.) The good thing is that a powerful
    open source monitoring solution is available, thanks to the open source community
    that maintains it. Figure 13.5 summarizes the open source solution we’re going
    to implement.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具，从开源到商业级别，都可以帮助您监控您的基础设施并在出现任何故障时通知您。（第13.3节介绍了如何设置几乎实时通知您的警报。）好事是，由于维护它的开源社区，一个强大的开源监控解决方案可用。图13.5总结了我们将要实施的开源解决方案。
- en: '![](Images/CH13_F05_Labouardy.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F05_Labouardy.png)'
- en: Figure 13.5 Telegraf will collect metrics, store them in InfluxDB, and from
    there we can visualize them in Grafana.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 Telegraf将收集度量，将它们存储在InfluxDB中，然后我们可以在Grafana中可视化它们。
- en: 'This monitoring solution can be split into three parts:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此监控解决方案可以分为三个部分：
- en: '*Telegraf*—A metric collector agent, installed on each Jenkins instance. It
    collects the internal metrics and ships them to a time-series database.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Telegraf*—一个度量收集代理，安装在每一个Jenkins实例上。它收集内部度量并将它们发送到时间序列数据库。'
- en: '*InfluxDB*—An open source time-series database (TSDB), optimized for fast,
    high-availability storage. It consumes the telemetry coming from Telegraf agents.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*InfluxDB*—一个开源的时间序列数据库（TSDB），针对快速、高可用性存储进行了优化。它消耗来自Telegraf代理的遥测数据。'
- en: '*Grafana*—An open source visualization platform, used to build dynamic and
    interactive dashboards based on data stored in InfluxDB.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Grafana*—一个开源的可视化平台，用于基于存储在InfluxDB中的数据构建动态和交互式仪表板。'
- en: Now that the architecture is clear, we need to deploy an InfluxDB server on
    an EC2 instance. Check out the InfluxDB official documentation at [http://mng.bz/7lJy](https://shortener.manning.com/7lJy)
    for a step-by-step guide on how to install and configure InfluxDB.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在架构已经清晰，我们需要在 EC2 实例上部署一个 InfluxDB 服务器。请查看 InfluxDB 官方文档[http://mng.bz/7lJy](https://shortener.manning.com/7lJy)，以获取如何安装和配置
    InfluxDB 的分步指南。
- en: 'Once the instance is up and running, SSH to the InfluxDB instance and type
    the `influx` command on the terminal. The `influx` CLI, which is included in all
    InfluxDB packages, is a lightweight and simple way to interact with the database.
    We need to create two databases:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例启动并运行，通过 SSH 连接到 InfluxDB 实例，并在终端中输入 `influx` 命令。`influx` CLI 是包含在所有 InfluxDB
    软件包中的，它是一种轻量级且简单的方式与数据库交互。我们需要创建两个数据库：
- en: '*instances*—To store metrics about resource usage, such as CPU utilization,
    memory, network traffic, disk usage, and so forth.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instances*——用于存储有关资源使用的指标，例如 CPU 利用率、内存、网络流量、磁盘使用率等。'
- en: '*containers*—To store metrics about containers running in the Jenkins workers.
    The containers are basically build jobs scheduled for Jenkins workers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*containers*——用于存储在 Jenkins 工作节点中运行的容器的指标。容器基本上是安排给 Jenkins 工作节点的构建作业。'
- en: 'Create the databases by using the `CREATE` `DATABASE` Influx Query Language
    (InfluxQL) statement:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `CREATE` `DATABASE` Influx 查询语言 (InfluxQL) 语句创建数据库：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The databases can also be created by making raw HTTP requests to an InfluxDB
    API over port 8086 (see [http://mng.bz/m1z2](https://shortener.manning.com/m1z2)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过向端口 8086 上的 InfluxDB API 发送原始 HTTP 请求来创建数据库（见 [http://mng.bz/m1z2](https://shortener.manning.com/m1z2)）。
- en: Now that we have databases, InfluxDB is ready to accept queries and writes.
    To collect Jenkins instance metrics, we need to install a Telegraf agent on each
    server. One way to do this is to install Telegraf on the existing instances, but
    this solution won’t scale, as we need to install and configure a Telegraf agent
    each time a new Jenkins worker is deployed. Therefore, the best way is to ship
    Telegraf within the Jenkins AMI. Once again, we will use Packer to bake the Jenkins
    master and worker AMIs with a preinstalled and configured Telegraf agent.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据库，InfluxDB 准备接受查询和写入。要收集 Jenkins 实例指标，我们需要在每个服务器上安装一个 Telegraf 代理。一种方法是在现有实例上安装
    Telegraf，但这个解决方案无法扩展，因为每次部署新的 Jenkins 工作节点时，我们都需要安装和配置一个 Telegraf 代理。因此，最好的方法是将在
    Jenkins AMI 中打包 Telegraf。再次使用 Packer 将 Jenkins 主节点和工作节点 AMI 打包，其中预安装并配置了 Telegraf
    代理。
- en: Add the code in the next listing to the setup.sh (chapter13/telegraf/setup.sh)
    script provided in chapter 4, listings 4.4 and 4.5\. This code will install the
    latest stable version of Telegraf (at the time of writing this book, version 1.19.0
    is available).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将下一列表中的代码添加到第 4 章提供的 setup.sh（chapter13/telegraf/setup.sh）脚本中。此代码将安装 Telegraf
    的最新稳定版本（在撰写本书时，版本为 1.19.0）。
- en: Listing 13.1 Installing the Telegraf agent with the Yum utility
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.1 使用 Yum 工具安装 Telegraf 代理
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we tell Telegraf what metrics to collect, by creating a configuration
    file at /etc/telegraf/telegraf.conf. The config file consists of *inputs* (where
    the metrics come from) and *outputs* (where the metrics go). The following listing
    specifies three inputs (CPU memory usage, and Docker), and specifies InfluxDB
    as the output. The Docker input reads metrics about the Docker daemon and then
    outputs this data to InfluxDB.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们告诉 Telegraf 要收集哪些指标，通过在 /etc/telegraf/telegraf.conf 创建一个配置文件。该配置文件由 *inputs*（指标来源）和
    *outputs*（指标去向）组成。以下列表指定了三个输入（CPU 内存使用和 Docker），并将 InfluxDB 指定为输出。Docker 输入读取
    Docker 守护进程的指标，然后将这些数据输出到 InfluxDB。
- en: Listing 13.2 Telegraf configuration file with various inputs
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.2 Telegraf 配置文件，包含各种输入
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Overrides default hostname; if empty, use os.Hostname()
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 覆盖默认主机名；如果为空，则使用 os.Hostname()
- en: ❷ Gathers metrics on the system CPUs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 收集系统 CPU 的指标
- en: ❸ Gathers metrics about disk usage. By default, stats are gathered for all mount
    points, and setting Mount_points will restrict the stats to the root volume.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 收集磁盘使用情况的指标。默认情况下，统计信息会收集所有挂载点的信息，设置 Mount_points 将限制统计信息仅限于根卷。
- en: ❹ Collects system memory metrics
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 收集系统内存指标
- en: ❺ Collects system swap metrics
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 收集系统交换空间指标
- en: ❻ Gathers general stats on system load, uptime, and number of users logged in.
    It is similar to the Unix uptime command.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 收集系统负载、运行时间和登录用户数的通用统计信息。它与 Unix 的 uptime 命令类似。
- en: ❼ Uses the Docker Engine API to gather metrics on running Docker containers
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 Docker 引擎 API 收集运行中的 Docker 容器的指标
- en: ❽ Writes system metrics to the InfluxDB instance database
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将系统指标写入 InfluxDB 实例数据库
- en: ❾ Writes Docker metrics to the InfluxDB container database
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将 Docker 指标写入 InfluxDB 容器数据库
- en: Make sure to replace the `INFLUXDB_IP` variable with the IP address of the instance
    where the InfluxDB server is running.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将 `INFLUXDB_IP` 变量替换为运行 InfluxDB 服务器的实例的 IP 地址。
- en: Bake a new Jenkins AMI and redeploy a Jenkins cluster with the newly built image
    by following steps described in section 5.3\. Once the new Jenkins cluster is
    up and running, Telegraf will start collecting metrics and streaming them to InfluxDB
    for storage and indexing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按照第 5.3 节中描述的步骤制作一个新的 Jenkins AMI，并使用新构建的镜像重新部署 Jenkins 集群。一旦新的 Jenkins 集群启动并运行，Telegraf
    将开始收集指标并将它们流式传输到 InfluxDB 以进行存储和索引。
- en: To explore the metrics, we will use Grafana. You can install Grafana from a
    Yum repository or by running a Docker image. (Check out the Grafana official documentation
    at [http://mng.bz/5ZY1](https://shortener.manning.com/5ZY1) for more details.)
    Once Grafana is installed, head your browser to HOST_IP:3000\. On the login page,
    enter `admin` for the username and password.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索指标，我们将使用 Grafana。您可以从 Yum 仓库安装 Grafana 或通过运行 Docker 镜像来安装。（有关更多详细信息，请参阅 Grafana
    官方文档[http://mng.bz/5ZY1](https://shortener.manning.com/5ZY1)。一旦安装了 Grafana，请将浏览器导航到
    HOST_IP:3000。在登录页面，输入 `admin` 作为用户名和密码。
- en: Before we create a dashboard to monitor the overall health of the Jenkins instances,
    we need to link the InfluxDB databases to Grafana. To do so, we need to create
    a data source for each InfluxDB database.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建用于监控 Jenkins 实例整体健康状况的仪表盘之前，我们需要将 InfluxDB 数据库链接到 Grafana。为此，我们需要为每个 InfluxDB
    数据库创建一个数据源。
- en: 'In the side panel, click the cog icon and then click Configuration > Data Sources.
    Click the Add Data Source button, shown in figure 13.6\. Then fill the settings
    page with the following values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧面板中，点击齿轮图标，然后点击配置 > 数据源。点击图 13.6 所示的添加数据源按钮。然后在设置页面填写以下值：
- en: '*Name*—The data source name. (This is how you’ll refer to the data source in
    queries.)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名称*—数据源名称。（这是您在查询中引用数据源的方式。）'
- en: '*URL*—The HTTP, IP address, and port of your InfluxDB API. (By default, the
    InfluxDB API port is 8086.)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*URL*—您的 InfluxDB API 的 HTTP、IP 地址和端口号。（默认情况下，InfluxDB API 端口为 8086。）'
- en: '*Database*—Name of the InfluxDB database (*instances* or *containers* database).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据库*—InfluxDB 数据库的名称（*实例*或*容器*数据库）。'
- en: '![](Images/CH13_F06_Labouardy.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F06_Labouardy.png)'
- en: Figure 13.6 Configuring InfluxDB-based data sources in Grafana
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 在 Grafana 中配置基于 InfluxDB 的数据源
- en: 'With your InfluxDB connection configured, use Grafana and InfluxQL to query
    and visualize time-series data stored in InfluxDB. From the left panel, click
    Dashboards. From the top menu, click Home to get a list of dashboards. Click the
    Create New button at the bottom to create a new dashboard. To add a graph, just
    click the graph button in the panel filter. In the Query section, type the following
    InfluxQL statement:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好 InfluxDB 连接后，使用 Grafana 和 InfluxQL 查询并可视化存储在 InfluxDB 中的时序数据。从左侧面板点击仪表盘。从顶部菜单点击首页以获取仪表盘列表。点击底部的新建按钮创建一个新的仪表盘。要添加图表，只需在面板过滤器中点击图表按钮。在查询部分，输入以下
    InfluxQL 语句：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This query selects the memory usage from the `mem_vm` measurement and groups
    the results by Jenkins node. The query results in the graph in figure 13.7.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询从 `mem_vm` 测量中选择内存使用情况，并按 Jenkins 节点分组结果。查询结果如图 13.7 所示。
- en: '![](Images/CH13_F07_Labouardy.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F07_Labouardy.png)'
- en: Figure 13.7 Building a memory utilization gauge chart
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 构建内存利用率仪表图
- en: 'To monitor the Jenkins jobs build time, you can use the following statement:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控 Jenkins 作业的构建时间，可以使用以下语句：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This selects the uptime value (the amount of time the container is online and
    operational) from the `docker_container_status_docker` measurement and groups
    the results by the container name (figure 13.8).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询从 `docker_container_status_docker` 测量中选择容器在线和运行的时间（即在线时间）值，并按容器名称分组结果（如图
    13.8 所示）。
- en: '![](Images/CH13_F08_Labouardy.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F08_Labouardy.png)'
- en: Figure 13.8 Monitoring containers built within CI/CD pipelines
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 监控 CI/CD 管道内构建的容器
- en: 'Back to Grafana, you can create multiple graphs to monitor various metrics
    of the Jenkins cluster:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 Grafana，您可以为 Jenkins 集群的各个指标创建多个图表进行监控：
- en: CPU usage of Jenkins nodes (master and worker instances)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins 节点（主节点和工作节点）的 CPU 使用率
- en: Network traffic (in and out bytes)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络流量（进出字节数）
- en: Memory utilization of each Jenkins node
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 Jenkins 节点的内存利用率
- en: Number of running build jobs
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行构建作业的数量
- en: Overall health and number of workers
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体健康状态和工作者数量
- en: Figure 13.9 shows host-level details for the Jenkins cluster. The complete dashboard
    can be imported from the JSON file (chapter13/grafana/dashboard/influxdb.json).
    Refer to [http://mng.bz/6mGD](http://mng.bz/6mGD) for instructions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 显示了 Jenkins 集群的宿主级详细信息。完整的仪表板可以从 JSON 文件（chapter13/grafana/dashboard/influxdb.json）中导入。有关说明，请参阅
    [http://mng.bz/6mGD](http://mng.bz/6mGD)。
- en: '![](Images/CH13_F09_Labouardy.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F09_Labouardy.png)'
- en: Figure 13.9 Jenkins host metrics
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 Jenkins 宿主指标
- en: As mentioned earlier, monitoring the state of your instances is imperative to
    keeping your Jenkins cluster healthy, and by using the preceding metrics (and
    the many others) provided by Telegraf, you can achieve this with relative ease.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，监控实例的状态对于保持 Jenkins 集群健康至关重要，通过使用 Telegraf 提供的上述指标（以及许多其他指标），你可以相对容易地实现这一点。
- en: 'So far, you have seen how to monitor the Jenkins instances (server side). Let’s
    explore monitoring the Jenkins server itself (application side). As you may have
    already guessed, a monitoring plugin for Jenkins can provide a lot of data about
    what’s going on within Jenkins and about the tasks being performed by Jenkins.
    For example, the Metrics plugin ([https://plugins.jenkins.io/metrics/](https://plugins.jenkins.io/metrics/))
    provides health checks by exposing an API on the Jenkins server at the $JENKINS_URL/metrics
    endpoint. The API provides information on the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了如何监控 Jenkins 实例（服务器端）。现在让我们探索如何监控 Jenkins 服务器本身（应用端）。正如你可能已经猜到的，一个
    Jenkins 监控插件可以提供关于 Jenkins 内部发生的事情以及 Jenkins 执行的任务的大量数据。例如，指标插件 ([https://plugins.jenkins.io/metrics/](https://plugins.jenkins.io/metrics/))
    通过在 Jenkins 服务器上的 $JENKINS_URL/metrics 端点公开 API 来提供健康检查。该 API 提供以下信息：
- en: HTTP sessions and current HTTP requests
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP 会话和当前 HTTP 请求
- en: Detailed statistics of the build times and the build steps by period
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按周期详细统计构建时间和构建步骤
- en: Threads, process list of OS, and heap dumps
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程、操作系统进程列表和堆转储
- en: For instance, the API call in figure 13.10 returns statistics about the number
    of executors available to Jenkins.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图 13.10 中的 API 调用返回了 Jenkins 可用执行器的统计信息。
- en: '![](Images/CH13_F10_Labouardy.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F10_Labouardy.png)'
- en: Figure 13.10 Metrics API with health-check endpoints
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 指标 API 与健康检查端点
- en: To create a dashboard based on those metrics, we can write a custom script to
    save those values regularly to InfluxDB, or use a Prometheus metric plugin ([https://plugins.jenkins.io/prometheus/](https://plugins.jenkins.io/prometheus/))
    to expose an endpoint (the default is /prometheus) with metrics that a Prometheus
    server can scrape.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建基于这些指标的仪表板，我们可以编写一个自定义脚本来定期将这些值保存到 InfluxDB，或者使用 Prometheus 指标插件 ([https://plugins.jenkins.io/prometheus/](https://plugins.jenkins.io/prometheus/))
    来公开一个端点（默认为 /prometheus），Prometheus 服务器可以抓取这些指标。
- en: Prometheus ([https://prometheus.io/](https://prometheus.io/)) is an open source
    monitoring system with a dimensional data model, flexible query language, efficient
    time-series database, and modern alerting approach.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus ([https://prometheus.io/](https://prometheus.io/)) 是一个具有维度数据模型、灵活的查询语言、高效的时序数据库和现代警报方法的开源监控系统。
- en: Note The Packer template file and Terraform HCL files for baking and deploying
    a Prometheus server are available in the chapter13/prometheus folder.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于烘焙和部署 Prometheus 服务器的 Packer 模板文件和 Terraform HCL 文件位于 chapter13/prometheus
    文件夹中。
- en: First, install the Prometheus Metrics plugin ([https://plugins.jenkins.io/prometheus/](https://plugins.jenkins.io/prometheus/))
    from the Manage Plugins section. Once it’s installed, you can see the plugin’s
    output through JENKINS _URL/prometheus (figure 13.11).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从“管理插件”部分安装 Prometheus 指标插件 ([https://plugins.jenkins.io/prometheus/](https://plugins.jenkins.io/prometheus/))。安装完成后，你可以在
    JENKINS _URL/prometheus（图 13.11）中看到插件的输出。
- en: '![](Images/CH13_F11_Labouardy.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F11_Labouardy.png)'
- en: Figure 13.11 Prometheus endpoint serves a list of metrics
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 Prometheus 端点提供指标列表
- en: Then, you need to configure a Prometheus server to scrape metrics from Jenkins.
    Edit the configuration file at /etc/prometheus/prometheus.yml (listing 13.3).
    In the `scrape_configs` section, add a job for the Jenkins server. The format
    for writing this config file can be found at [http://mng.bz/o8Vr](https://shortener.manning.com/o8Vr).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要配置一个 Prometheus 服务器以从 Jenkins 抓取指标。编辑 /etc/prometheus/prometheus.yml 配置文件（列表
    13.3）。在 `scrape_configs` 部分添加一个针对 Jenkins 服务器的作业。此配置文件的编写格式可以在 [http://mng.bz/o8Vr](https://shortener.manning.com/o8Vr)
    找到。
- en: Listing 13.3 Configuring Prometheus to scrape metrics from Jenkins
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.3 配置 Prometheus 从 Jenkins 抓取指标
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: On the Prometheus dashboard (the default port is 9090), you can explore the
    metrics collected from Jenkins. You will be greeted will the screen in figure
    13.12.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 控制台（默认端口为 9090）中，您可以探索从 Jenkins 收集的指标。您将看到图 13.12 中的屏幕。
- en: '![](Images/CH13_F12_Labouardy.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F12_Labouardy.png)'
- en: Figure 13.12 Exploring Jenkins metrics from the Prometheus dashboard
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 从 Prometheus 控制台探索 Jenkins 指标
- en: 'Collected metrics are not very useful unless they are visualized. Connect Prometheus
    with Grafana by creating a new data source. To create a Prometheus data source
    in Grafana, follow these steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的指标如果没有可视化则不太有用。通过创建新的数据源将 Prometheus 与 Grafana 连接起来。要在 Grafana 中创建 Prometheus
    数据源，请按照以下步骤操作：
- en: Click the cogwheel icon in the side panel to open the Configuration menu.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击侧面板中的齿轮图标以打开配置菜单。
- en: Click Data Sources.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击数据源。
- en: Click Add Data Source.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“添加数据源”。
- en: Select Prometheus as the type.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 Prometheus 作为类型。
- en: Set the appropriate Prometheus server URL to [http://prometheus:9090](http://prometheus:9090).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置适当的 Prometheus 服务器 URL 为 [http://prometheus:9090](http://prometheus:9090)。
- en: Click Save & Test to save the new data source.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“保存并测试”以保存新的数据源。
- en: Then, create a dashboard based on the available metrics. The dashboard features
    application-level metrics (which track the total number of jobs in a queue, how
    many are pending, and how many are stuck or otherwise delayed), followed by internal
    operation metrics (JVM), and finally system-level metrics (disk I/O, network,
    memory, and so forth). Figure 13.13 shows a part of the dashboard.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据可用的指标创建一个仪表板。仪表板包括应用级指标（跟踪队列中作业的总数、待处理作业的数量以及卡住或延迟的作业数量），接着是内部操作指标（JVM），最后是系统级指标（磁盘
    I/O、网络、内存等）。图 13.13 展示了仪表板的一部分。
- en: '![](Images/CH13_F13_Labouardy.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F13_Labouardy.png)'
- en: Figure 13.13 Comprehensive Jenkins monitoring summary of jobs and builds
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 作业和构建的全面 Jenkins 监控摘要
- en: 'The complete dashboard can be imported from the following JSON file: chapter13/grafana/dashboard/prometheus.json.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的仪表板可以从以下 JSON 文件导入：chapter13/grafana/dashboard/prometheus.json。
- en: Another popular solution for monitoring Jenkins is the Monitoring plugin (previously
    called JavaMelody). This plugin produces comprehensive HTML reports about the
    state of Jenkins, including CPU and system load, average response time, and memory
    usage; see [https://plugins.jenkins.io/monitoring/](https://plugins.jenkins.io/monitoring/)
    for more details. Moreover, the reports are served from the Jenkins dashboard,
    as shown in figure 13.14.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的 Jenkins 监控解决方案是监控插件（之前称为 JavaMelody）。此插件生成关于 Jenkins 状态的全面 HTML 报告，包括
    CPU 和系统负载、平均响应时间和内存使用情况；有关更多详细信息，请参阅 [https://plugins.jenkins.io/monitoring/](https://plugins.jenkins.io/monitoring/)。此外，报告由
    Jenkins 控制台提供，如图 13.14 所示。
- en: '![](Images/CH13_F14_Labouardy.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F14_Labouardy.png)'
- en: Figure 13.14 Statistics of JavaMelody monitoring
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14 JavaMelody 监控的统计信息
- en: Great! You should now be able to monitor a Jenkins cluster running in production.
    To provide even further visibility into your Jenkins environment, you can collect
    and analyze Jenkins logs of real-time system and security events and correlate
    them with performance and server metrics to identify and resolve issues.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在您应该能够监控在生产环境中运行的 Jenkins 集群。为了进一步了解您的 Jenkins 环境，您可以收集和分析实时系统和安全事件的 Jenkins
    日志，并将它们与性能和服务器指标相关联，以识别和解决问题。
- en: 13.2 Centralized logging for Jenkins logs with ELK
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用 ELK 对 Jenkins 日志进行集中式记录
- en: 'By default, Jenkins logs are located at /var/log/jenkins/jenkins.log. To view
    those logs, SSH to the Jenkins master instance with the bastion host, and then
    issue the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Jenkins 日志位于 /var/log/jenkins/jenkins.log。要查看这些日志，使用堡垒主机 SSH 到 Jenkins
    主实例，然后执行以下命令：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Figure 13.15 shows the command output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15 显示了命令输出。
- en: '![](Images/CH13_F15_Labouardy.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F15_Labouardy.png)'
- en: Figure 13.15 Viewing Jenkins logs at /var/log/jenkins/jenkins.log
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15 查看 /var/log/jenkins/jenkins.log 中的 Jenkins 日志
- en: You can also view those logs from the web dashboard (figure 13.16). Head to
    the Jenkins dashboard and select System Log from the Manage Jenkins page.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从网络控制台（图 13.16）查看这些日志。转到 Jenkins 控制台，并在管理 Jenkins 页面上选择系统日志。
- en: '![](Images/CH13_F16_Labouardy.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F16_Labouardy.png)'
- en: Figure 13.16 Viewing Jenkins logs from the Jenkins dashboard
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.16 从 Jenkins 控制台查看 Jenkins 日志
- en: By default, Jenkins records every `INFO` log to stdout, but you can configure
    Jenkins to record logs of a specific Jenkins plugin by creating a custom log recorder.
    From the System Log page, click the Add New Log Recorder button and choose a name
    that makes sense to you. The example in figure 13.17 creates a log recorder for
    the Slack plugin (the Java package is located at jenkins.plugins.slack).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Jenkins 将每个 `INFO` 日志记录到 stdout，但您可以通过创建自定义日志记录器来配置 Jenkins 记录特定 Jenkins
    插件的日志。从系统日志页面，点击“添加新日志记录器”按钮并选择一个有意义的名称。图 13.17 中的示例创建了一个 Slack 插件的日志记录器（Java
    包位于 jenkins.plugins.slack）。
- en: '![](Images/CH13_F17_Labouardy.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F17_Labouardy.png)'
- en: Figure 13.17 Capturing the Slack plugin’s login with a custom log recorder
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17 使用自定义日志记录器捕获 Slack 插件的登录信息
- en: Now, if any Slack notification is sent from a Jenkins pipeline, a log should
    be captured as shown in figure 13.18.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果 Jenkins 管道发送任何 Slack 通知，应该会捕获如图 13.18 所示的日志。
- en: '![](Images/CH13_F18_Labouardy.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F18_Labouardy.png)'
- en: Figure 13.18 Display of Slack plugin’s logs
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.18 Slack 插件的日志显示
- en: You can also view the build logs for a particular job by navigating to the job
    item from the dashboard and clicking Console Output, or by viewing the content
    of the logfile at $JENKINS_HOME/jobs/$JOB_NAME/builds/$BUILD_NUMBER/log.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过导航到仪表板中的作业项并点击“控制台输出”，或查看 $JENKINS_HOME/jobs/$JOB_NAME/builds/$BUILD_NUMBER/log
    中的日志文件内容来查看特定作业的构建日志。
- en: Depending on a log rotation configuration, the logs could be saved for *X* number
    of builds (or days, and so forth), meaning the old job logs might be lost. That’s
    why you need to persist the logs in a centralized logging platform for auditing
    and potential troubleshooting.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 根据日志轮转配置，日志可能会保存 *X* 次构建（或天数等），这意味着旧的工作日志可能会丢失。这就是为什么您需要将日志持久化到集中式日志平台，以便进行审计和潜在的故障排除。
- en: Note You can enable the Discard Old Build plugin ([https://plugins.jenkins.io/discard-old-build/](https://plugins.jenkins.io/discard-old-build/))
    in each project or job configuration page to configure the interval to keep old
    builds (for example, once a month, once in 10 builds, and so forth).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以在每个项目或作业配置页面中启用“丢弃旧构建”插件 ([https://plugins.jenkins.io/discard-old-build/](https://plugins.jenkins.io/discard-old-build/))
    来配置保留旧构建的间隔（例如，每月一次，每 10 次构建一次等）。
- en: Additionally, analyzing Jenkins logs can provide a lot of information that helps
    with troubleshooting the root cause of pipeline job failure. Build logs contain
    a full set of records such as build name, number, execution time, and other things.
    However, to analyze those logs, you need to ship them to an external logging platform.
    That’s where a platform like the ELK stack (Elasticsearch, Logstash, and Kibana)
    comes into play.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，分析 Jenkins 日志可以提供大量信息，有助于排查管道作业失败的根本原因。构建日志包含完整的记录，如构建名称、编号、执行时间等。然而，要分析这些日志，您需要将它们发送到外部日志平台。这就是像
    ELK 堆栈（Elasticsearch、Logstash 和 Kibana）这样的平台发挥作用的地方。
- en: 13.2.1 Streaming logs with Filebeat
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用 Filebeat 流式传输日志
- en: Filebeat ([www.elastic.co/beats/filebeat](http://www.elastic.co/beats/filebeat)),
    a lightweight agent that will be installed on the Jenkins master instance, will
    ship the logs to Logstash ([www.elastic.co/logstash](http://www.elastic.co/logstash))
    for processing and aggregation. From there, the logs will be stored in Elasticsearch
    ([www.elastic.co/elasticsearch](http://www.elastic.co/elasticsearch)) and visualized
    in Kibana ([www.elastic.co/kibana](http://www.elastic.co/kibana)) through interactive
    dashboards. Figure 13.19 summarizes the entire workflow.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Filebeat ([www.elastic.co/beats/filebeat](http://www.elastic.co/beats/filebeat))，一个轻量级代理，将安装在
    Jenkins 主实例上，并将日志发送到 Logstash ([www.elastic.co/logstash](http://www.elastic.co/logstash))
    进行处理和聚合。从那里，日志将被存储在 Elasticsearch ([www.elastic.co/elasticsearch](http://www.elastic.co/elasticsearch))
    中，并通过交互式仪表板在 Kibana ([www.elastic.co/kibana](http://www.elastic.co/kibana)) 中可视化。图
    13.19 总结了整个工作流程。
- en: '![](Images/CH13_F19_Labouardy.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F19_Labouardy.png)'
- en: Figure 13.19 Shipping Jenkins logs to the ELK platform with Filebeat
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.19 使用 Filebeat 将 Jenkins 日志发送到 ELK 平台
- en: To deploy this architecture, we need to create a machine image for each component.
    You can use Packer to bake the AMIs (figure 13.20). The Packer templates are available
    in the GitHub repository at chapter13/COMPONENT_NAME/packer/template.json.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署此架构，我们需要为每个组件创建一个机器镜像。您可以使用 Packer 来烘焙 AMI（图 13.20）。Packer 模板可在 GitHub 仓库的
    chapter13/COMPONENT_NAME/packer/template.json 中找到。
- en: Once the AMIs are created, you can use Terraform to deploy the ELK stack. The
    template resources are available in the GitHub repository at chapter13/COMPONENT_
    NAME/terraform/*.tf.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了 AMI，你就可以使用 Terraform 来部署 ELK 堆栈。模板资源可在 GitHub 仓库 chapter13/COMPONENT_NAME/terraform/*.tf
    中找到。
- en: '![](Images/CH13_F20_Labouardy.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F20_Labouardy.png)'
- en: Figure 13.20 Logstash, Kibana, and Elasticsearch AMIs built with Packer
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.20 使用 Packer 构建的 Logstash、Kibana 和 Elasticsearch AMI
- en: By the end of the provisioning process, three EC2 instances should be created,
    as shown in figure 13.21.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置过程结束时，应该创建了三个 EC2 实例，如图 13.21 所示。
- en: '![](Images/CH13_F21_Labouardy.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F21_Labouardy.png)'
- en: Figure 13.21 Deployed ELK stack on AWS
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.21 在 AWS 上部署的 ELK 堆栈
- en: With the logging platform ready to consume incoming Jenkins logs, we need to
    install Filebeat on the Jenkins master instance. SSH to the Jenkins server, and
    run the commands in the following listing to install the latest stable version
    of Filebeat (at the time of writing this book, version 7.13.2 is available).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当日志平台准备好消费传入的 Jenkins 日志时，我们需要在 Jenkins 主实例上安装 Filebeat。通过 SSH 连接到 Jenkins 服务器，并运行以下列表中的命令来安装
    Filebeat 的最新稳定版本（在撰写本书时，版本为 7.13.2）。
- en: Listing 13.4 Installing the Filebeat agent on the Jenkins server
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.4 在 Jenkins 服务器上安装 Filebeat 代理
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we need to set the path of the log files that we want to forward to ELK.
    Here we want to forward logs to /var/log/jenkins/jenkins.log. Go to the configuration
    directory of Filebeat under the location /etc/filebeat, and update filebeat.yml
    with the following listing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置我们想要转发到 ELK 的日志文件的路径。这里我们希望将日志转发到 /var/log/jenkins/jenkins.log。转到
    /etc/filebeat 下的 Filebeat 配置目录，并使用以下列表更新 filebeat.yml。
- en: Listing 13.5 Filebeat input configuratio.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.5 Filebeat 输入配置
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Harvests lines from the /var/log/jenkins/jenkins.log file
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 /var/log/jenkins/jenkins.log 文件中提取行
- en: ❷ Adds a field called type to the output, so we can easily identify logs coming
    from Jenkins
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在输出中添加一个名为 type 的字段，以便我们能够轻松识别来自 Jenkins 的日志
- en: ❸ Configures Filebeat to handle a multiline message
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 配置 Filebeat 以处理多行消息
- en: ❹ Sends logs directly to Logstash
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 直接将日志发送到 Logstash
- en: ❺ Annotates each log event with relevant metadata from the host machine
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用主机机的相关元数据注释每个日志事件
- en: 'Multiline messages are common in Jenkins logs, especially for log messages
    containing Java stack traces. Here’s an example of a Java stack trace:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins 日志中常见的多行消息，尤其是包含 Java 栈跟踪的日志消息。以下是一个 Java 栈跟踪的示例：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To correctly handle these multiline messages, we use the `multiline` settings
    to specify which lines are part of a single log message.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确处理这些多行消息，我们使用 `multiline` 设置来指定哪些行是单个日志消息的一部分。
- en: 'Replace the `LOGSTASH_HOST` variable, with the IP address of the Logstash server.
    Then restart the Filebeat agent with the following command:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `LOGSTASH_HOST` 变量替换为 Logstash 服务器的 IP 地址。然后使用以下命令重新启动 Filebeat 代理：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Head to the Kibana dashboard (at `KIBANA_IP:5601`), jump to the Management tab,
    and to Index Patterns. We have to create a new index pattern. Creating an index
    pattern means mapping Kibana with an Elasticsearch index. Since Logstash stores
    incoming Jenkins logs to a series of indices in the format *jenkins-YYYY.MM.DD*,
    we will create an index pattern `jenkins-*` to explore all the logs, as shown
    in figure 13.22.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 Kibana 仪表板（在 `KIBANA_IP:5601`），跳转到管理标签，然后到索引模式。我们必须创建一个新的索引模式。创建索引模式意味着将
    Kibana 与 Elasticsearch 索引映射。由于 Logstash 将传入的 Jenkins 日志存储到一系列格式为 *jenkins-YYYY.MM.DD*
    的索引中，因此我们将创建一个索引模式 `jenkins-*` 来探索所有日志，如图 13.22 所示。
- en: Click the Next Step option. From the Time Filter Field Name drop-down, select
    @timestamp. Then click the Create Index Pattern button.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一步选项。从时间过滤器字段名称下拉菜单中选择 @timestamp。然后点击创建索引模式按钮。
- en: Now, to view logs, go to the Discover page. You can see your index data coming
    in (figure 13.23).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要查看日志，请转到发现页面。你可以看到你的索引数据正在传入（如图 13.23 所示）。
- en: '![](Images/CH13_F22_Labouardy.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F22_Labouardy.png)'
- en: Figure 13.22 Connecting an Elasticsearch index to Kibana
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.22 将 Elasticsearch 索引连接到 Kibana
- en: '![](Images/CH13_F23_Labouardy.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F23_Labouardy.png)'
- en: Figure 13.23 Visualizing Jenkins logs from Kibana
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.23 从 Kibana 可视化 Jenkins 日志
- en: 'Now you have a working pipeline that reads Jenkins logs. However, you’ll notice
    that the format of the log messages is not ideal. You want to parse the log messages
    to create specific, named fields from the logs. Let’s take, as an example, the
    following Jenkins log:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有一个可以读取 Jenkins 日志的工作管道。然而，你会注意到日志消息的格式并不理想。你希望解析日志消息以从日志中创建特定的命名字段。以下是一个
    Jenkins 日志的示例：
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The timestamp at the beginning of the line is easy to define as the level of
    the log (`INFO`, `WARNING`, `DEBUG`, etc.). To parse the line, we can write a
    Grok expression.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 行首的时间戳很容易定义为日志级别（`INFO`、`WARNING`、`DEBUG` 等）。要解析行，我们可以编写一个 Grok 表达式。
- en: Grok works by parsing text patterns, using regular expressions, and assigning
    them to an identifier. The syntax is `%{PATTERN:IDENTIFIER}`. We can write a sequence
    of Grok patterns and assign various pieces of the preceding log message to various
    identifiers, as you can see in the following listing.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Grok 通过解析文本模式，使用正则表达式，并将它们分配给一个标识符来工作。语法是 `%{PATTERN:IDENTIFIER}`。我们可以编写一系列
    Grok 模式并将前一条日志消息的各个部分分配给不同的标识符，如下面的列表所示。
- en: Listing 13.6 Grok expression to parse Jenkins log message
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.6 解析 Jenkins 日志消息的 Grok 表达式
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Grok comes with its own dictionary of patterns that you can use out of the box.
    But you can always define your own custom pattern, as shown in the following listing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Grok 随带自己的模式字典，你可以直接使用。但你可以始终定义自己的自定义模式，如下面的列表所示。
- en: Listing 13.7 Grok custom patterns definition
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.7 Grok 自定义模式定义
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can use the Kibana Grok Debugger console to debug the expression. This feature,
    which is automatically enabled in Kibana, is located on the DevTools tab.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Kibana Grok Debugger 控制台来调试表达式。此功能在 Kibana 中自动启用，位于 DevTools 选项卡上。
- en: Enter the log message in the Sample Data field, and the Grok expression in the
    Grok Pattern field. Then click Simulate. You will see the simulated event that
    results from applying the Grok pattern (figure 13.24).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在“样本数据”字段中输入日志消息，在“Grok 模式”字段中输入 Grok 表达式。然后点击模拟。你将看到应用 Grok 模式后产生的模拟事件（图 13.24）。
- en: '![](Images/CH13_F24_Labouardy.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F24_Labouardy.png)'
- en: Figure 13.24 Simulating Grok parsing with Grok Debugger tool
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.24 使用 Grok Debugger 工具模拟 Grok 解析
- en: Note that the Grok pattern references the `JAVACLASS` and `JOBNAME` custom patterns.
    They are defined in the Custom Patterns section. Each pattern definition is specified
    on its own line.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Grok 模式引用了 `JAVACLASS` 和 `JOBNAME` 自定义模式。它们在自定义模式部分定义。每个模式定义都在其自己的行上。
- en: Note If an error occurs, you can continue iterating over the custom pattern
    until the output matches the event that you expect.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果发生错误，你可以继续迭代自定义模式，直到输出匹配你期望的事件。
- en: The Grok expression is working, but we want the parsing mechanism to be done
    before storing logs to Elasticsearch. That’s why we will update the Logstash config
    (chapter13/logstash/packer/jenkins.conf) to parse incoming logs from Filebeat.
    The `filter` section will attempt to match messages coming from Jenkins with the
    Grok expression defined earlier, as shown in the following listing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Grok 表达式正在工作，但我们希望解析机制在将日志存储到 Elasticsearch 之前完成。这就是为什么我们将更新 Logstash 配置（chapter13/logstash/packer/jenkins.conf）以解析来自
    Filebeat 的传入日志。`filter` 部分将尝试匹配来自 Jenkins 的消息与之前定义的 Grok 表达式，如下面的列表所示。
- en: Listing 13.8 Parsing Jenkins logs at the Logstash level
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.8 在 Logstash 层解析 Jenkins 日志
- en: '[PRE14]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code takes the Jenkins logs collected by Filebeat, parses them into fields,
    and sends the fields to Elasticsearch. The `pattern_dir` setting tells Logstash
    where your custom patterns directory is. You can customize the parsing mechanism
    by adding more processing, such as dropping unused fields or renaming fields.
    See the Mutate Filter plugin at [http://mng.bz/J6Av](https://shortener.manning.com/J6Av)
    for more information.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将 Filebeat 收集的 Jenkins 日志解析成字段，并将字段发送到 Elasticsearch。`pattern_dir` 设置告诉 Logstash
    你的自定义模式目录在哪里。你可以通过添加更多处理来自定义解析机制，例如删除未使用的字段或重命名字段。有关更多信息，请参阅 [http://mng.bz/J6Av](https://shortener.manning.com/J6Av)
    上的 Mutate Filter 插件。
- en: Restart Logstash to reload the configuration. Your Jenkins logs will be gathered
    and structured into fields (figure 13.25). Right now, not much is in there because
    you are gathering only Jenkins logs. Here, you can search and browse through your
    logs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动 Logstash 以重新加载配置。你的 Jenkins 日志将被收集并结构化成字段（图 13.25）。目前里面没有多少内容，因为你只收集了 Jenkins
    日志。在这里，你可以搜索和浏览你的日志。
- en: '![](Images/CH13_F25_Labouardy.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F25_Labouardy.png)'
- en: Figure 13.25 Structuring Jenkins logs into separated queryable fields
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.25 将 Jenkins 日志结构化成可查询的字段
- en: Each log message coming from Jenkins will match and result in the fields listed
    in table 13.1.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Jenkins 的每个日志消息都将匹配并产生表 13.1 中列出的字段。
- en: Table 13.1 Jenkins index fields in Elasticsearch
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1 Elasticsearch 中的 Jenkins 索引字段
- en: '| Field | Description |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 描述 |'
- en: '| `time` | The data and time of the message in UTC format |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `time` | 消息的数据和时间，以 UTC 格式表示 |'
- en: '| `level` | The log message level (INFO, WARNING, DEBUG, FATAL, ERROR) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `level` | 日志消息级别（INFO、WARNING、DEBUG、FATAL、ERROR）|'
- en: '| `project` | The Jenkins job’s build name |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `project` | Jenkins作业的构建名称|'
- en: '| `buildNumber` | The build number of the job, which identifies how many times
    Jenkins runs this build process |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `buildNumber` | 作业的构建号，用于标识Jenkins运行此构建过程的次数|'
- en: '| `status` | The status of the build (FAILURE or SUCCESS) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| `status` | 构建状态（失败或成功）|'
- en: '| `execution` | The current state of the build (running, pending, terminated,
    or completed) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| `execution` | 构建当前状态（运行中、挂起、已终止或已完成）|'
- en: You can create a stacked bar chart showing the number of failed versus successful
    builds based on the `status` field over a period of time; see figure 13.26.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据`status`字段在一段时间内显示失败与成功构建数量的堆叠柱状图；见图13.26。
- en: '![](Images/CH13_F26_Labouardy.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F26_Labouardy.png)'
- en: Figure 13.26 Building interactive widgets based on Jenkins structured fields
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.26 基于Jenkins结构化字段构建交互式小部件
- en: You can save the bar chart as a widget and import it to a dashboard. With a
    dashboard, you can combine multiple visualizations onto a single page, and then
    filter them by providing a search query or by selecting filters by clicking elements
    in the visualization. Dashboards are useful when you want to get an overview of
    your Jenkins logs and make correlations among various visualizations and logs;
    see figure 13.27.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将柱状图保存为小部件并导入到仪表板中。使用仪表板，您可以将多个可视化组合到单个页面上，然后通过提供搜索查询或通过在可视化中单击元素来选择过滤器来过滤它们。仪表板在您想要获取Jenkins日志的概览并在不同可视化之间建立关联时非常有用；见图13.27。
- en: '![](Images/CH13_F27_Labouardy.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F27_Labouardy.png)'
- en: Figure 13.27 Analyzing Jenkins logs from a Kibana dashboard
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.27 从Kibana仪表板分析Jenkins日志
- en: 'The complete dashboard can be imported from the following JSON file: chapter13/kibana/dashboard/jenkins.json.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的仪表板可以从以下JSON文件导入：chapter13/kibana/dashboard/jenkins.json。
- en: That’s it! You’ve successfully created a pipeline that uses Filebeat to take
    Jenkins logs as input, forwards those logs to Logstash for parsing, and writes
    the parsed data to an Elasticsearch server.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！您已成功创建了一个使用Filebeat将Jenkins日志作为输入，将这些日志转发到Logstash进行解析，并将解析后的数据写入Elasticsearch服务器的管道。
- en: 13.2.2 Streaming logs with the Logstash plugin
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 使用Logstash插件流式传输日志
- en: You can skip the Filebeat and Logstash configurations by shipping Jenkins logs
    directly to an Elasticsearch instance via the Logstash plugin ([https://plugins.jenkins.io/logstash/](https://plugins.jenkins.io/logstash/))
    on Jenkins. This solution is ideal if you’re not already using external Logstash
    agents to stream your infrastructure or application logs to Elasticsearch, and
    if you don’t need to enrich the parsing mechanism of logs with custom Grok expressions.
    Plus, the Logstash plugin can stream the log data from a Jenkins instance to any
    indexer solution (including Redis, RabbitMQ, and Elasticsearch). In the current
    scenario, we will use Elasticsearch.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过Jenkins上的Logstash插件（[https://plugins.jenkins.io/logstash/](https://plugins.jenkins.io/logstash/)）直接将Jenkins日志发送到Elasticsearch实例来跳过Filebeat和Logstash的配置。如果您尚未使用外部Logstash代理将您的基础设施或应用程序日志流式传输到Elasticsearch，并且不需要使用自定义Grok表达式丰富日志解析机制，则此解决方案非常理想。此外，Logstash插件可以将Jenkins实例的日志数据流式传输到任何索引解决方案（包括Redis、RabbitMQ和Elasticsearch）。在当前场景中，我们将使用Elasticsearch。
- en: After successfully installing the Logstash plugin in the global configuration
    of the Jenkins dashboard, we need to configure the plugin with the target indexer.
    Configure the URI, where the Elasticsearch server is running, as shown in figure
    13.28.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jenkins仪表板的全球配置中成功安装Logstash插件后，我们需要使用目标索引器配置该插件。配置URI，即Elasticsearch服务器运行的地址，如图13.28所示。
- en: '![](Images/CH13_F28_Labouardy.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F28_Labouardy.png)'
- en: Figure 13.28 Configuring the Logstash plugin to stream logs to the Elasticsearch
    server
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.28 配置Logstash插件将日志流式传输到Elasticsearch服务器
- en: 'After configuring the Elasticsearch endpoint in the Logstash configuration,
    you can add the following block to your pipelines. That way, all the logs produced
    within the `logstash` step will be streamed into Elasticsearch:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在Logstash配置中配置Elasticsearch端点后，您可以在您的管道中添加以下块。这样，所有在`logstash`步骤中产生的日志都将流式传输到Elasticsearch：
- en: '[PRE15]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can view the streamed logs by accessing the Kibana dashboard, shown in figure
    13.29.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问如图13.29所示的Kibana仪表板来查看流式日志。
- en: '![](Images/CH13_F29_Labouardy.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH13_F29_Labouardy.png)'
- en: Figure 13.29 Example of a log message sent to Elasticsearch
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.29 发送到Elasticsearch的日志消息示例
- en: Now we are able to stream the log data from the Jenkins instance to Elasticsearch
    and finally to Kibana.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 Jenkins 实例的日志数据流式传输到 Elasticsearch，最终传输到 Kibana。
- en: 13.3 Creating alerts based on metrics
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 基于指标创建警报
- en: We can take the logging and monitoring solutions further and set up alerts.
    One of the most common use cases is DevOps teams getting notifications of events,
    such as when the failure build rate is significantly higher than usual. Needless
    to say, this issue can have a significant impact on the release of new features,
    hence having an impact on business and user experience.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将日志和监控解决方案进一步扩展并设置警报。最常见的用例是 DevOps 团队接收事件通知，例如当失败构建率显著高于正常水平时。不用说，这个问题可能会对新功能的发布产生重大影响，从而影响业务和用户体验。
- en: You can use Kibana to define a meaningful alert on a specified condition; see
    figure 13.30\. For instance, you can define an alert to periodically check the
    failure build rate. For the notification channel, you can use Slack, OpsGenie,
    or a simple email notification.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Kibana 在指定的条件下定义一个有意义的警报；参见图 13.30。例如，您可以定义一个警报来定期检查失败构建率。对于通知通道，您可以使用
    Slack、OpsGenie 或简单的电子邮件通知。
- en: '![](Images/CH13_F30_Labouardy.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F30_Labouardy.png)'
- en: Figure 13.30 Configuring an alert on Kibana
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.30 在 Kibana 上配置警报
- en: You can also create alerts based on metrics collected by Prometheus or Telegraf,
    by using the Grafana alerting feature.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过使用 Grafana 警报功能，根据 Prometheus 或 Telegraf 收集的指标创建警报。
- en: Note While it’s easy to set up and use Grafana alerting, it’s more limited in
    terms of the alert rules you can apply to your metrics queries. If you’re looking
    for an advanced solution, go with Prometheus Alertmanager ([https://prometheus.io/docs/alerting/latest/alertmanager/](https://prometheus.io/docs/alerting/latest/alertmanager/)).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然设置和使用 Grafana 警报很简单，但在将警报规则应用于您的指标查询方面更为有限。如果您正在寻找一个高级解决方案，请选择 Prometheus
    Alertmanager ([https://prometheus.io/docs/alerting/latest/alertmanager/](https://prometheus.io/docs/alerting/latest/alertmanager/))。
- en: Before creating monitoring alerts, we need to add the notification channel through
    which we will be notified. Here, we will be adding Slack as the notification channel.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建监控警报之前，我们需要添加将通过其接收通知的通知通道。在这里，我们将添加 Slack 作为通知通道。
- en: To set up Slack, you need to configure an incoming Slack webhook URL. Create
    a Slack application by going to [https://api.slack.com/apps/new](https://api.slack.com/apps/new).
    After creating the application, you’ll be redirected to the Settings page of the
    new app (figure 13.31). From there, enable the Incoming Webhook feature by switching
    the radio button to On.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Slack，您需要配置一个传入的 Slack webhook URL。通过访问 [https://api.slack.com/apps/new](https://api.slack.com/apps/new)
    创建 Slack 应用程序。创建应用程序后，您将被重定向到新应用的设置页面（图 13.31）。从那里，通过切换单选按钮到“开启”来启用“传入 webhook”功能。
- en: '![](Images/CH13_F31_Labouardy.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F31_Labouardy.png)'
- en: Figure 13.31 Enabling the incoming webhook on a Slack application
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.31 在 Slack 应用程序上启用传入 webhook
- en: Now that incoming webhooks are enabled, the Settings page should refresh, and
    some extra options will appear. One of those options will be a really helpful
    button marked Add New Webhook to Workspace, and you should click it.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，启用传入 webhook 后，设置页面应该刷新，并出现一些额外选项。其中之一将是一个非常有用的按钮，标记为“添加新 webhook 到 Workspace”，您应该点击它。
- en: Go ahead and pick a Slack channel that Grafana will post to, and then click
    Authorize Your App. You’ll be sent back to your app settings, where you should
    now see a new entry under the webhook URLs for the Your Workspace section, with
    a webhook URL. Copy it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续选择 Grafana 将要发布消息的 Slack 频道，然后点击“授权您的应用”。您将被返回到您的应用设置页面，在那里您现在应该在“您的 Workspace”部分的
    webhook URLs 下看到一个新条目，其中包含一个 webhook URL。复制它。
- en: 'After creating the webhook URL, you need to create a notification channel in
    Grafana. In the Grafana sidebar, hover your cursor over the Alerting icon and
    then click Notification Channels, as shown in figure 13.32\. Create a Slack notification
    channel as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 webhook URL 后，您需要在 Grafana 中创建一个通知通道。在 Grafana 侧边栏中，将鼠标悬停在“警报”图标上，然后单击“通知通道”，如图
    13.32 所示。按照以下步骤创建 Slack 通知通道：
- en: Input the name of the channel.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入频道的名称。
- en: Change Type to Slack and input a webhook URL that you have created.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将类型更改为 Slack 并输入您已创建的 webhook URL。
- en: '![](Images/CH13_F32_Labouardy.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F32_Labouardy.png)'
- en: Figure 13.32 Configuring a new Slack notification channel
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.32 配置新的 Slack 通知通道
- en: You can test the setup by clicking the Send Test button at the bottom. After
    setting up all the fields, just click the Save button.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击底部的发送测试按钮来测试设置。设置所有字段后，只需点击保存按钮。
- en: 'Now let’s create the alert. Select the panel where you want to create an alert.
    For instance, we can create an alert on the memory usage metric. Click the Alert
    tab and then click Create Alert. This will open a form for configuring the alert,
    where you can set the following options:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建警报。选择你想要创建警报的面板。例如，我们可以在内存使用指标上创建警报。点击警报选项卡，然后点击创建警报。这将打开一个配置警报的表单，你可以设置以下选项：
- en: '*Evaluate Every*—The time interval on which you want the alert rule to be evaluated.
    For this example, we can set the option to Evaluate Every 1m for 1m. It means
    that Grafana will evaluate the rule every minute. If the metrics violate the rule,
    Grafana will wait for 1 minute. If, after 1 minute, the metrics are not recovered,
    Grafana will trigger an alert.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估间隔*—你希望警报规则评估的时间间隔。对于本例，我们可以将选项设置为每 1 分钟评估 1 分钟。这意味着 Grafana 将每分钟评估一次规则。如果指标违反规则，Grafana
    将等待 1 分钟。如果 1 分钟后指标没有恢复，Grafana 将触发警报。'
- en: '*Conditions*—We can use the `avg()` function as we want to validate our rule
    against the average memory utilization.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*条件*—我们可以使用 `avg()` 函数，因为我们想验证我们的规则与平均内存利用率。'
- en: This alert will be triggered when the average memory utilization is above 90%,
    as shown in figure 13.33.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当平均内存利用率超过 90% 时，将触发此警报，如图 13.33 所示。
- en: '![](Images/CH13_F33_Labouardy.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F33_Labouardy.png)'
- en: Figure 13.33 Defining an alert rule for memory usage
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.33 定义内存使用警报规则
- en: Additionally, we need to add the notification channel where the alert needs
    to be sent, as well as the alert message. If the alert is triggered, you will
    see the message in figure 13.34 on your Slack channel.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要添加需要发送警报的通知渠道以及警报消息。如果警报被触发，你将在 Slack 频道上看到图 13.34 中的消息。
- en: '![](Images/CH13_F34_Labouardy.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH13_F34_Labouardy.png)'
- en: Figure 13.34 Slack notification upon memory threshold exceeded
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.34 超过内存阈值时的 Slack 通知
- en: Creating an alert to a messaging application like Slack is very beneficial.
    This ensures that you and your teammates get notifications immediately if something
    wrong happens. You can mention your team Slack group or use `@here` or `@channel`
    to make sure your team gets the message.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个发送到类似 Slack 的消息应用的警报非常有用。这确保了如果发生错误，你和你的团队成员会立即收到通知。你可以提及你的团队 Slack 群组或使用
    `@here` 或 `@channel` 确保你的团队收到消息。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can build a monitoring stack with Telegraf, InfluxDB, and Grafana to collect,
    store, and visualize Jenkins instance metrics.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 Telegraf、InfluxDB 和 Grafana 构建一个监控堆栈，以收集、存储和可视化 Jenkins 实例指标。
- en: You can collect and parse Jenkins logs into structured fields by writing Grok
    expressions.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过编写 Grok 表达式将 Jenkins 日志收集和解析到结构化字段中。
- en: The Prometheus plugin can be used to expose internal and client-side metrics
    in Jenkins.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 插件可用于在 Jenkins 中公开内部和客户端指标。
- en: The Logstash plugin is an easy way to integrate Jenkins logs with the ELK stack.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logstash 插件是集成 Jenkins 日志与 ELK 堆栈的简单方法。
- en: Filebeat can be installed as an agent on your Jenkins master instance to ship
    logs to Logstash for parsing. From there, logs will be stored in Elasticsearch
    and analyzed from Kibana within an interactive dashboard.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filebeat 可以作为代理安装在 Jenkins 主实例上，以便将日志发送到 Logstash 进行解析。从那里，日志将被存储在 Elasticsearch
    中，并在 Kibana 的交互式仪表板中进行分析。
