- en: 5 Advanced active learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 高级主动学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Combining uncertainty sampling and diversity sampling techniques
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合不确定性采样和多样性采样技术
- en: Using active transfer learning to sample the most uncertain and the most representative
    items
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主动迁移学习来采样最不确定和最具代表性的项目
- en: Implementing adaptive transfer learning within an active learning cycle
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主动学习周期内实现自适应迁移学习
- en: In chapters 3 and 4, you learned how to identify where your model is uncertain
    (what your model knows it doesn’t know) and what is missing from your model (what
    your model doesn’t know that it doesn’t know). In this chapter, you learn how
    to combine these techniques into a comprehensive active learning strategy. You
    also learn how to use transfer learning to adapt your models to predict which
    items to sample.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章和第4章中，你学习了如何识别模型的不确定性（模型知道它不知道的内容）以及模型缺失的内容（模型不知道它不知道的内容）。在本章中，你将学习如何将这些技术结合成一个全面的主动学习策略。你还将学习如何使用迁移学习来调整你的模型，以预测哪些项目需要采样。
- en: 5.1 Combining uncertainty sampling and diversity sampling
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 结合不确定性采样和多样性采样
- en: 'This section explores ways to combine all the active learning techniques that
    you have learned up to this point so that you can use them effectively them for
    your particular use cases. You will also learn one new active learning strategy:
    expected error reduction, which combines principles of uncertainty sampling and
    diversity sampling. Recall from chapter 1 that an ideal strategy for active learning
    tries to sample items that are near the decision boundary but are distant from
    one another, as shown in figure 5.1.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了将你迄今为止所学的所有主动学习技术结合起来的方法，以便你可以有效地将它们应用于特定的用例。你还将学习一种新的主动学习策略：预期误差减少，它结合了不确定性采样和多样性采样的原则。回顾第1章，理想的主动学习策略试图采样接近决策边界但彼此距离较远的项目，如图5.1所示。
- en: '![](../Images/CH05_F01_Munro.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F01_Munro.png)'
- en: Figure 5.1 One possible result of combining uncertainty sampling and diversity
    sampling. When these strategies are combined, items near diverse sections of the
    decision boundary are selected. Therefore, we are optimizing the chance of finding
    items that are likely to result in a changed decision boundary when they’re added
    to the training data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 结合不确定性采样和多样性采样的一种可能结果。当这些策略结合时，会选择接近决策边界不同部分的项目。因此，我们正在优化找到可能改变决策边界的项目的概率，当它们被添加到训练数据中时。
- en: You have learned to identify items that are near the decision boundary (uncertainty
    sampling) and distant from one another (cluster-based sampling and adaptive representative
    sampling). This chapter shows you how to sample items that are both near the decision
    boundary and diverse, like those shown in figure 5.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了识别接近决策边界（不确定性采样）和彼此之间距离较远的项目（基于聚类的采样和自适应代表性采样）。本章将向你展示如何采样既接近决策边界又具有多样性的项目，如图5.1所示。
- en: 5.1.1 Least confidence sampling with cluster-based sampling
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 基于聚类的最小置信度采样
- en: The most common way that uncertainty sampling and diversity sampling are combined
    in industry is takes a large sample from one method and further filter the sample
    with another method. This technique has no common name, despite its ubiquity,
    probably because so many companies have invented it independently by necessity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业界，不确定性采样和多样性采样最常见的结合方式是从一种方法中抽取大量样本，然后使用另一种方法进一步过滤这些样本。尽管这种技术无处不在，但它没有统一的名称，这可能是由于许多公司出于必要而独立发明了它。
- en: 'If you sampled the 50% most uncertain items with least confidence sampling
    and then applied cluster-based sampling to sample 10% of those items, you could
    end up with a sample of 5% of your data more or less like those in figure 5.1:
    a near-optimal combination of uncertainty and diversity. Figure 5.2 represents
    this result graphically. First, you sample the 50% most uncertain items; then
    you apply clustering to ensure diversity within that selection, sampling the centroid
    of each cluster.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用最不确定的50%项目进行最小置信度采样，然后应用基于聚类的采样来采样这些项目的10%，你最终可以得到大约5%的数据样本，类似于图5.1中的那些：不确定性和多样性的近似最优组合。图5.2以图形方式表示了这一结果。首先，你采样最不确定的50%项目；然后应用聚类确保所选项目中的多样性，采样每个聚类的质心。
- en: '![](../Images/CH05_F02_Munro.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F02_Munro.png)'
- en: Figure 5.2 An example combining least confidence and clustering-based sampling.
    First, uncertainty sampling finds items near the decision boundary; then clustering
    ensures diversity within that selection. In this figure, the centroids from each
    cluster are sampled. Alternatively, or in addition, you could select random members
    of outliers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 结合最小置信度和基于聚类的采样的示例。首先，不确定性采样找到决策边界附近的项；然后聚类确保该选择中的多样性。在此图中，每个聚类的质心被采样。或者，你也可以选择随机选择异常值成员。
- en: With the code you have already learned, you can see that combining least confidence
    sampling and clustering is a simple extension in advanced_active_learning.py within
    the same code repository that we have been using ([https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)),
    as shown in the following listing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你已经学到的代码，你可以看到结合最小置信度采样和聚类是我们在同一代码仓库中使用的`advanced_active_learning.py`的一个简单扩展（[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)），如下所示。
- en: Listing 5.1 Combining least confidence sampling and clustering
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 结合最小置信度采样和聚类
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Get a large sample of the most uncertain items.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取大量最不确定的样本。
- en: ❷ Within those uncertain items, use clustering to ensure a diverse sample.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这些不确定的样本中，使用聚类确保样本的多样性。
- en: 'Only two new lines of code are needed to combine the two approaches: one to
    get the most uncertain items and one to cluster them. If you are interested in
    the disaster-response text classification task, try it with this new command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 只需要两行新代码就可以结合这两种方法：一行用于获取最不确定的项，另一行用于聚类它们。如果你对灾难响应文本分类任务感兴趣，尝试使用这个新命令：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You’ll immediately see that data tends to fall near the divide of text that
    may or not be disaster-related and that the items are a diverse selection. You
    have many options for using uncertainty sampling to find items near the decision
    boundary and then apply cluster-based sampling to ensure diversity within those
    items. You can experiment with different types of uncertainty sampling, different
    thresholds for your uncertainty cutoff, and different parameters for clustering.
    In many settings, this combination of clustering and uncertainty sampling will
    be the fastest way to drill down on the highest-value items for active learning
    and should be one of the first strategies that you try for almost any use case.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会立即看到数据往往落在可能或可能不与灾难相关的文本的分割附近，并且这些项是一个多样化的选择。你有许多选项可以使用不确定性采样来找到决策边界附近的项，然后应用基于聚类的采样以确保这些项中的多样性。你可以尝试不同的不确定性采样类型，不同的不确定性截止阈值，以及不同的聚类参数。在许多情况下，这种聚类和不确定性采样的组合将是深入挖掘主动学习中最有价值项的最快方式，应该是最先尝试的策略之一。
- en: The simple methods of combining strategies rarely make it into academic papers;
    academia favors papers that combine methods into a single algorithm rather than
    chaining multiple simpler algorithms. This makes sense, because combining the
    methods is easy, as you have already seen; there is no need to write an academic
    paper about something that can be implemented in a few lines of code. But as a
    developer building real-world active learning systems, you should always implement
    the easy solutions before attempting more experimental algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 结合策略的简单方法很少出现在学术论文中；学术界更喜欢将方法结合成一个单一算法的论文，而不是将多个简单算法链在一起。这很有道理，因为结合方法是容易的，正如你已经看到的；没有必要为可以在几行代码中实现的事情写一篇学术论文。但是作为一个构建现实世界主动学习系统的开发者，你应该在尝试更实验性的算法之前，始终实现简单的解决方案。
- en: Another reason to try simple methods first is that you might need to keep supporting
    them in your applications for a long time. It will be easier to maintain your
    code if you can get 99% of the way there without having to invent new techniques.
    See the following sidebar for a great example of how early decisions matter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试简单方法的第一种原因是，你可能会需要长时间在你的应用程序中支持它们。如果你不需要发明新技术就能达到99%的效果，那么维护你的代码将会更容易。以下侧边栏提供了一个很好的例子，说明了早期决策的重要性。
- en: Your early data decisions continue to matter
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你的早期数据决策持续重要
- en: '*Expert anecdote by Kieran Snyder*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*凯里安·斯奈德的专家轶事*'
- en: 'The decisions that you make early in a machine learning project can influence
    the products that you are building for many years to come. This is especially
    true for data decisions: your feature-encoding strategies, labeling ontologies,
    and source data will have long-term impacts.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目的早期所做的决策可能会影响你未来多年所构建的产品。这一点对于数据决策尤其如此：你的特征编码策略、标签本体和源数据将产生长期影响。
- en: 'In my first job out of graduate school, I was responsible for building the
    infrastructure that allowed Microsoft software to work in dozens of languages
    around the world. This job included making fundamental decisions such as deciding
    on the alphabetical order of the characters in a language—something that didn’t
    exist for many languages at the time. When the 2004 tsunami devastated countries
    around the Indian Ocean, it was an immediate problem for Sinhalese-speaking people
    in Sri Lanka: there was no easy way to support searching for missing people because
    Sinhalese didn’t yet have standardized encodings. Our timeline for Sinhalese support
    went from several months to several days so that we could help the missing-persons
    service, working with native speakers to build solutions as quickly as possible.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我研究生毕业后第一次工作的时候，我负责构建基础设施，使得微软软件能够在世界各地的数十种语言中运行。这项工作包括做出基本决策，比如决定一个语言的字符的字母顺序——这在当时许多语言中是不存在的。当2004年的海啸摧毁了印度洋周边的国家时，对于斯里兰卡的僧伽罗语使用者来说，这是一个紧迫的问题：由于僧伽罗语还没有标准化的编码，因此没有简单的方法来支持搜索失踪人员。我们为僧伽罗语支持的时间表从几个月缩短到几天，以便我们能够帮助失踪人员服务，与本土语言使用者合作，尽可能快地构建解决方案。
- en: The encodings that we decided on at that time were adopted by Unicode as the
    official encodings for the Sinhalese language and now encode that language forever.
    You won’t always be working on such critical timelines, but you should always
    consider the long-term impact of your product decisions right from the start.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当时决定的编码被Unicode采纳为僧伽罗语的官方编码，现在永久地编码了这种语言。你并不总是会在如此关键的时间线上工作，但你应该始终从一开始就考虑你产品决策的长期影响。
- en: '*Kieran Snyder is CEO and co-founder of Textio, a widely used augmented writing
    platform. Kieran previously held product leadership roles at Microsoft and Amazon
    and has a PhD in linguistics from the University of Pennsylvani*a.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*基兰·斯奈德（Kieran Snyder）是Textio的CEO和联合创始人，Textio是一个广泛使用的增强写作平台。基兰之前在微软和亚马逊担任过产品领导角色，并在宾夕法尼亚大学获得了语言学博士学位*。'
- en: Don’t assume that a complicated solution is necessarily the best; you may find
    that a simple combination of least confidence and clustering is all you need for
    your data. As always, you can test different methods to see which results in the
    biggest change in accuracy against a baseline of random sampling.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不要假设复杂的解决方案一定是最好的；你可能会发现，对于你的数据来说，只需要简单地将最小置信度和聚类结合起来就足够了。像往常一样，你可以测试不同的方法，看看哪种方法在随机采样的基线准确性上产生了最大的变化。
- en: 5.1.2 Uncertainty sampling with model-based outliers
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 基于模型的异常值的不确定性采样
- en: When you combine uncertainty sampling with model-based outliers, you are maximizing
    your model’s current confusion. You are looking for items near the decision boundary
    and making sure that their features are relatively unknown to the current model.
    Figure 5.3 shows the kinds of samples that this approach might generate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将不确定性采样与基于模型的异常值相结合时，你正在最大化你模型当前的混淆。你正在寻找接近决策边界的项目，并确保它们的特征对于当前模型来说是相对未知的。图
    5.3 展示了这种方法可能生成的样本类型。
- en: '![](../Images/CH05_F03_Munro.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Munro.png)'
- en: Figure 5.3 This example of combining uncertainty sampling with model-based outliers
    selects items that are near the decision boundary but that are different from
    the current training data items and, therefore, different from the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 这个将不确定性采样与基于模型的异常值相结合的例子选择了接近决策边界但与当前训练数据项不同的项目，因此也与模型不同。
- en: Listing 5.2 Combining uncertainty sampling with model-based outliers
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 将不确定性采样与基于模型的异常值相结合
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Get the most uncertain items.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取最不确定的项目。
- en: ❷ Apply model-based outlier sampling to those items.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对这些项目应用基于模型的异常值采样。
- en: 'As in the example in listing 5.1, you need only two lines of code here to pull
    everything together. Although combining uncertainty sampling with model-based
    outliers is optimal for targeting items that are most likely to increase your
    model’s knowledge and overall accuracy, it can also sample similar items. You
    can try this technique with this command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如列表 5.1 中的示例一样，这里只需要两行代码就可以将所有内容整合在一起。虽然将不确定性采样与基于模型的异常值结合是针对最有可能增加你的模型知识和整体准确性的项目最优的，但它也可能采样相似的项目。你可以使用以下命令尝试此技术：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 5.1.3 Uncertainty sampling with model-based outliers and clustering
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 基于模型的异常值和聚类的不确定性采样
- en: Because the method in section 5.1.2 might oversample items that are close to
    one another, you may want to implement this strategy first and then apply clustering
    to ensure diversity. It takes only one line of code to add clustering to the end
    of the previous method, so you could implement it easily. Alternatively, if you
    have quick active learning iterations, this approach ensures more diversity when
    you combine uncertainty sampling and model-based outliers; you can sample a small
    number of items in each iteration.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 5.1.2 节中的方法可能会对彼此接近的项进行过采样，你可能希望首先实施此策略，然后应用聚类以确保多样性。只需一行代码即可将聚类添加到先前方法末尾，因此可以轻松实现。或者，如果你有快速的主动学习迭代，这种方法在结合不确定性采样和基于模型的异常值时可以确保更多的多样性；你可以在每个迭代中采样少量项目。
- en: 5.1.4 Representative sampling cluster-based sampling
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 代表性采样和基于簇的采样
- en: One shortcoming of the representative sampling technique that you learned in
    chapter 4 is that it treats the training data and target domain as single clusters.
    In reality, your data will often be multinodal in a way that a single cluster
    cannot optimally capture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第 4 章中学到的代表性采样技术的缺点是它将训练数据和目标域视为单个簇。实际上，你的数据通常会在特征空间中呈现多峰分布，而单个簇无法最优地捕捉。
- en: To capture this complexity, you can combine representative sampling and cluster-based
    sampling in a slightly more complicated architecture. You can cluster your training
    data and your unlabeled data independently, identify the clusters that are most
    representative of your unlabeled data, and oversample from them. This approach
    gives you a more diverse set of items than representative sampling alone (figure
    5.4).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉这种复杂性，你可以将代表性采样和基于簇的采样结合到一个稍微复杂一些的架构中。你可以独立地对你的训练数据和未标记数据进行聚类，识别出最能代表你的未标记数据的簇，并从这些簇中进行过采样。这种方法比单独的代表性采样提供了更多样化的项目（图
    5.4）。
- en: '![](../Images/CH05_F04_Munro.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F04_Munro.png)'
- en: Figure 5.4 An example (bottom) of combining representative sampling and cluster-based
    sampling. This method samples items that are most like your application domain
    relative to your current training data and also different from one another. By
    comparison, the simpler representative sampling method in chapter 4 treats each
    distribution as a single distribution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 结合代表性采样和基于簇的采样的一个示例（底部）。这种方法采样了相对于你的当前训练数据最像你的应用域的项目，并且彼此不同。相比之下，第 4 章中更简单的代表性采样方法将每个分布视为单个分布。
- en: As you can see in figure 5.4, your current training data and target domains
    may not be uniform distributions within your feature space. Clustering the data
    first will help you model your feature space more accurately and sample a more
    diverse set of unlabeled items. First, create the clusters for the training data
    and unlabeled data from the application domain.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图 5.4 中可以看到的，你的当前训练数据和目标域可能不是特征空间内的均匀分布。首先对数据进行聚类将帮助你更准确地建模特征空间，并采样更多样化的未标记项目。首先，从应用域创建训练数据和未标记数据的簇。
- en: Listing 5.3 Combining representative sampling and clustering
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 结合代表性采样和聚类
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Create clusters within the existing training data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在现有训练数据中创建簇。
- en: ❷ Create clusters within the unlabeled data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在未标记数据中创建簇。
- en: Then iterate each cluster of unlabeled data, and find the item in each cluster
    that is closest to the centroid of that cluster relative to training data clusters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后迭代每个未标记数据的簇，并找到每个簇中相对于训练数据簇的质心最近的项。
- en: Listing 5.4 Combining representative sampling and clustering, continued
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 结合代表性采样和聚类，继续
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Find the best-fit cluster within the unlabeled data clusters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在未标记数据簇中找到最佳拟合簇。
- en: ❷ Find the best-fit cluster within the training data clusters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练数据簇中找到最佳拟合簇。
- en: ❸ Record the difference between the two as our representativeness score.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记录两个之间的差异作为我们的代表性分数。
- en: 'In design, this code is almost identical to the representative sampling method
    that you implemented in chapter 4, but you are asking the clustering algorithm
    to create multiple clusters for each distribution instead of only one for training
    data and one for unlabeled data. You can try this technique with this command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计上，这个代码几乎与你第4章中实现的代表性采样方法相同，但你要求聚类算法为每个分布创建多个簇，而不是只为训练数据和未标记数据创建一个簇。你可以用这个命令尝试这个技术：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 5.1.5 Sampling from the highest-entropy cluster
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.5 从最高熵簇中采样
- en: If you have high entropy in a certain cluster, a lot of confusion exists about
    the right labels for items in that cluster. In other words, these clusters have
    the highest average uncertainty across all the items. These items, therefore,
    are most likely to change labels and have the most room for changes in label.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你某个簇中有高熵，那么关于该簇中项目的正确标签存在很多混淆。换句话说，这些簇在所有项目中的平均不确定性最高。因此，这些项目最有可能改变标签，并且有最大的标签变化空间。
- en: The example in figure 5.5 is the opposite of clustering for diversity in some
    ways, as it deliberately focuses on one part of the problem space. But sometimes,
    that focus is exactly what you want.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5中的例子在某些方面与为了多样性而进行聚类的相反，因为它故意关注问题空间的一部分。但有时，这种关注正是你所需要的。
- en: '![](../Images/CH05_F05_Munro.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F05_Munro.png)'
- en: Figure 5.5 This example of combining cluster-based sampling with entropy (bottom)
    samples items within the cluster that show the most confusion. You might think
    of this cluster as being the one that straddles the decision boundary most closely.
    In this example, random items are sampled in the cluster, but you could experiment
    by sampling the centroid, outliers, and/or oversampling items within the cluster
    that have the highest entropy. By comparison, simple clustering (top) samples
    items from every cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5这个结合基于簇的采样与熵（底部）的例子采样了簇内最混乱的项目。你可能认为这个簇是最接近决策边界的。在这个例子中，簇内随机采样项目，但你可以通过采样质心、异常值和/或对簇内具有最高熵的项目进行过采样来实验。相比之下，简单的聚类（顶部）从每个簇中采样项目。
- en: Note that this approach works best when you have data with accurate labels and
    are confident that the task can be solved with machine learning. If you have data
    that has a lot of inherent ambiguity, this method will tend to focus in those
    areas. To solve this problem, see how much of your existing training data falls
    into your high-entropy clusters. If the cluster is already well represented in
    your training data, you have good evidence that it is an inherently ambiguous
    part of your feature space and that additional labels will not help. The following
    listing shows the code for selecting the cluster with the highest average entropy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种方法在你有准确标签的数据并且有信心任务可以用机器学习解决时效果最好。如果你有大量固有模糊性的数据，这种方法往往会集中在那些区域。为了解决这个问题，看看你现有的训练数据中有多少落在高熵簇中。如果簇已经在你的训练数据中得到了很好的代表，那么你有很好的证据表明它是特征空间中固有的模糊部分，并且额外的标签不会有所帮助。以下列表显示了选择具有最高平均熵簇的代码。
- en: Listing 5.5 Sampling from the cluster with the highest entropy
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5从最高熵簇中采样
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Create the clusters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建簇。
- en: ❷ Calculate the average uncertainty (using entropy) for the items in each cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算每个簇中项目的平均不确定性（使用熵）。
- en: 'In this code example, we are taking the average entropy of all items in a cluster.
    You can try different aggregate statistics based on your sampling strategy. If
    you know that you are sampling only the top 100 items, for example, you could
    calculate the average entropy across the 100 most uncertain items in each cluster
    rather than across every item in the cluster. You can try this technique with
    this command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们正在计算一个簇中所有项目的平均熵。你可以尝试基于你的采样策略不同的聚合统计。例如，如果你知道你只采样了前100个项目，那么你可以计算每个簇中100个最不确定的项目之间的平均熵，而不是计算簇中每个项目之间的平均熵。你可以用这个命令尝试这个技术：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 5.1.6 Other combinations of active learning strategies
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.6 其他主动学习策略的组合
- en: 'There are too many possible combinations of active learning techniques to cover
    in this book, but by this stage, you should have a good idea of how to combine
    them. Here are some starting points:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中涵盖主动学习技术的所有可能组合太多了，但到这个阶段，你应该已经很好地了解了如何将它们组合起来。以下是一些起点：
- en: '*Combining uncertainty sampling and representative sampling*—You can sample
    items that are most representative of your target domains and are also uncertain.
    This approach will be especially helpful in later iterations of active learning.
    If you used uncertainty sampling for early iterations, your target domain will
    have items that are disproportionately far from the decision boundary and could
    be selected erroneously as representative.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结合不确定性采样与代表性采样*—您可以采样最能够代表您的目标领域且不确定的项目。这种方法在主动学习的后期迭代中特别有用。如果您在早期迭代中使用了不确定性采样，您的目标领域将包含与决策边界不成比例地远的项，可能会被错误地选为代表性。'
- en: '*Combining model-based outliers and representative sampling*—This method is
    the ultimate method for domain adaptation, targeting items that are unknown to
    your model today but are also relatively common in your target domain.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结合基于模型的异常值与代表性采样*—这是领域自适应的终极方法，针对的是今天您的模型未知但也在您的目标领域中相对常见的项目。'
- en: '*Combining clustering with itself for hierarchical clusters*—If you have some
    large clusters or want to sample for diversity within one cluster, you can take
    the items from one cluster and use them to create a new set of clusters.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将聚类与自身结合以创建层次聚类*—如果您有一些大型簇或希望在簇内进行多样性采样，您可以从一个簇中提取项目，并使用它们来创建一组新的簇。'
- en: '*Combining sampling from the highest-entropy cluster with margin of confidence
    sampling (or some other uncertainty metrics*—You can find the cluster with the
    highest entropy and then sample all the items within it that fall closest to a
    decision boundary.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结合从最高熵簇中进行采样与置信度边界采样（或某些其他不确定性指标）*—你可以找到具有最高熵的簇，然后从中采样所有最接近决策边界的项目。'
- en: '*Combining ensemble methods or dropouts with individual strategies*—You may
    be building multiple models and decide that a Bayesian model is better for determining
    uncertainty, but a neural model is better for determining model-based outliers.
    You can sample with one model and further refine with another. If you’re clustering
    based on hidden layers, you could adapt the dropout method from uncertainty sampling
    and randomly ignore some neurons while creating clusters. This approach will prevent
    the clusters from overfitting to the internal representation of your network.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结合集成方法或dropout与个体策略*—您可能正在构建多个模型，并决定贝叶斯模型更适合确定不确定性，而神经网络模型更适合确定基于模型的异常值。您可以使用一个模型进行采样，并使用另一个模型进一步细化。如果您基于隐藏层进行聚类，您可以从不确定性采样中采用dropout方法，并在创建簇时随机忽略一些神经元。这种方法将防止簇过度拟合到您网络的内部表示。'
- en: 5.1.7 Combining active learning scores
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.7 结合主动学习分数
- en: An alternative to piping the output from one sampling strategy to another is
    taking the scores from the different sampling strategies and finding the highest
    average score, which makes mathematical sense for all methods other than clustering.
    You could average each item’s score for margin of confidence, model-based outliers,
    and representative learning, for example, and then rank all items by that single
    aggregate score.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个采样策略的输出管道传输到另一个策略的替代方法是，从不同的采样策略中提取分数，并找到最高平均分数，这对于除了聚类以外的所有方法都有数学意义。例如，您可以平均每个项目的置信度边界分数、基于模型的异常值分数和代表性学习分数，然后根据这个单一的汇总分数对所有项目进行排名。
- en: Although all the scores should be in a [0–1] range, note that some of them may
    be clustered in small ranges and therefore not contribute as much to the average.
    If this is the case with your data, you can try converting all your scores to
    percentiles (quantiles), effectively turning all the sampling scores into stratified
    rank orders. You can use built-in functions from your math library of choice to
    turn any list of numbers into percentiles. Look for functions called `rank``()`,
    `percentile``(),` or `percentileofscore``()` in various Python libraries. Compared
    with the other methods that you are using for sampling, converting scores to percentiles
    is relatively quick, so don’t worry about trying to find the most optimal function;
    choose a function from a library that you are already using.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有分数都应该在[0–1]范围内，但请注意，其中一些可能集中在很小的范围内，因此对平均值的贡献可能不大。如果您的数据中存在这种情况，您可以尝试将所有分数转换为百分位数（分位数），这实际上将所有采样分数转换为分层排名顺序。您可以使用您选择的数学库中的内置函数将任何数字列表转换为百分位数。在各种Python库中寻找名为`rank()`、`percentile()`或`percentileofscore()`的函数。与您用于采样的其他方法相比，将分数转换为百分位数相对较快，因此无需担心尝试找到最优化函数；选择您已经使用的库中的一个函数即可。
- en: You could also sample via the union of the methods rather than filtering (which
    is a combination via intersection). This approach can be used for any methods
    and might make the most sense when you are combining multiple uncertainty sampling
    scores. You could sample the items that are in the most 10% uncertain by any of
    least confidence, margin of confidence, ratio of confidence, or entropy to produce
    a general “uncertain” set of samples, and then use those samples directly or refine
    the sampling by combining it with additional methods. There are many ways to combine
    the building blocks that you have learned, and I encourage you to experiment with
    them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过方法的并集来采样，而不是通过过滤（这是通过交集的组合）。这种方法可以用于任何方法，并且在你结合多个不确定性采样分数时可能最有意义。你可以通过最不自信、置信度边际、置信度比率或熵来采样最不确定的10%的项目，从而产生一个通用的“不确定”样本集，然后直接使用这些样本或通过结合其他方法来细化采样。你可以以许多方式组合你所学到的构建块，我鼓励你尝试它们。
- en: 5.1.8 Expected error reduction sampling
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.8 预期误差减少采样
- en: 'Expected error reduction is one of a handful of active learning strategies
    in the literature that aim to combine uncertainty sampling and diversity sampling
    into a single metric. This algorithm is included here for completeness, with the
    caveat that I have not seen it implemented in real-world situations. The core
    metric for expected error reduction sampling is how much the error in the model
    would be reduced if an unlabeled item were given a label.[¹](#pgfId-1006308) You
    could give each unlabeled item the possible labels that it could have, retrain
    the model with those labels, and then look at how the model accuracy changes.
    You have two common ways to calculate the change in model accuracy:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 预期误差减少是文献中旨在将不确定性采样和多样性采样结合成一个单一指标的一小部分积极学习策略之一。这个算法被包括在这里是为了完整性，但有一个前提，就是我还没有看到它在现实世界中的实现。预期误差减少采样的核心指标是，如果给一个未标记的项目分配一个标签，模型中的误差将减少多少。[¹](#pgfId-1006308)你可以给每个未标记的项目分配可能的标签，用这些标签重新训练模型，然后观察模型准确率的变化。你有两种常见的方式来计算模型准确率的变化：
- en: '*Overall accuracy*—What is the change in number of items predicted correctly
    if this item had a label?'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*总体准确率*——如果这个项目有一个标签，预测正确的项目数量会有什么变化？'
- en: '*Overall entropy*—What is the change in aggregate entropy if this item had
    a label? This method uses the definition of entropy that you learned in the uncertainty
    sampling chapter in sections 3.2.4 and 3.2.5\. It is sensitive to the confidence
    of the prediction, unlike the first method, which is sensitive only to the predicted
    label.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*总体熵*——如果这个项目有一个标签，总体熵会有什么变化？这种方法使用你在不确定性采样章节3.2.4和3.2.5中学到的熵的定义。与第一种方法相比，它对预测的置信度敏感，而第一种方法只对预测的标签敏感。'
- en: 'The score is weighted across labels by the frequency of each label. You sample
    the items that are most likely to improve the model overall. This algorithm has
    some practical problems, however:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分数是按照每个标签的频率加权计算的。你采样那些最有可能提高模型整体性能的项目。然而，这个算法存在一些实际问题：
- en: Retraining the model once for every unlabeled item multiplied by every label
    is prohibitively expensive for most algorithms.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大多数算法来说，为每个未标记的项目和每个标签重新训练模型一次是非常昂贵的。
- en: There can be so much variation when retraining a model that the change from
    one additional label could be indistinguishable from noise.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新训练模型时可能会有很大的变化，以至于一个额外标签的变化可能无法与噪声区分开来。
- en: The algorithm can oversample items a long way from the decision boundary, thanks
    to the high entropy for the labels that are a diminishingly small likelihood.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于标签的熵很高，对于可能性越来越小的标签，算法可能会过度采样远离决策边界的项目。
- en: So there are practical limitations to using this method with neural models.
    The original authors of this algorithm used incremental Naive Bayes, which can
    be adapted to new training items by updating the counts of a new item’s features,
    and is deterministic. Given this fact, expected error reduction works for the
    authors’ particular algorithm. The problem of oversampling items away from the
    decision boundary can be addressed by using the predicted probability of each
    label rather than the label frequency (prior probability), but you will need accurate
    confidence predictions from your model, which you may not have, as you learned
    in chapter 3.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用神经网络模型时，这种方法存在实际限制。这个算法的原始作者使用了增量朴素贝叶斯，可以通过更新新项的特征计数来适应新的训练项，并且是确定性的。鉴于这一事实，预期误差减少对作者们的特定算法是有效的。可以通过使用每个标签的预测概率而不是标签频率（先验概率）来解决远离决策边界的过度采样项的问题，但你可能需要从你的模型中获得准确的置信度预测，正如你在第3章中学到的，你可能没有。
- en: If you do try to implement expected error reduction, you could experiment with
    different accuracy measures and with uncertainty sampling algorithms other than
    entropy. Because this method uses entropy, which comes from information theory,
    you might see it called *information gain* in the literature on variations of
    this algorithm. Read these papers closely, because *gain* can mean *lower* information.
    Although the term is mathematically correct, it can seem counterintuitive to say
    that your model knows more when the predictions have less information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试实现预期误差减少，你可以尝试不同的准确度指标，以及除了熵之外的其它不确定性采样算法。因为这种方法使用熵，它来自信息论，你可能会在关于这个算法变体的文献中看到它被称为*信息增益*。仔细阅读这些论文，因为*增益*可能意味着*更低*的信息。尽管这个术语在数学上是正确的，但当预测信息较少时，说你的模型知道得更多可能会感觉反直觉。
- en: As stated at the start of this section, no one has (as far as I know) published
    on whether expected error reduction is better than the simple combination of methods
    through the intersection and/or union of sampling strategies. You could try implementing
    expected error reduction and related algorithms to see whether they help in your
    systems. You may be able to implement them by retraining only the final layer
    of your model with the new item, which will speed the process.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节开头所述，据我所知，没有人（至少）在是否预期误差减少比通过采样策略的交集和/或并集简单组合方法更好的问题上发表过文章。你可以尝试实现预期误差减少和相关算法，看看它们是否有助于你的系统。你可能只需要通过用新项重新训练你模型的最后一层来实现它们，这将加快这个过程。
- en: If you want to sample items with a goal similar to expected error reduction,
    you can cluster your data and then look at clusters with the highest entropy in
    the predictions, like the example in figure 5.4 earlier in this chapter. Expected
    error reduction has a problem, however, in that it might find items in only one
    part of the feature space, like the uncertainty sampling algorithms used in isolation.
    If you extend the example in figure 5.4 to sample items from the *N* highest entropy
    clusters, not only the single highest entropy cluster, you will have addressed
    the limitations of expected error reduction in only a few lines of code.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要采样具有与预期误差减少类似目标的项，你可以聚类你的数据，然后查看预测中熵最高的聚类，就像本章前面5.4节中的示例。然而，预期误差减少有一个问题，那就是它可能只找到特征空间中某一部分的项，就像在孤立中使用的不确定性采样算法。如果你将5.4节中的示例扩展到从*N*个最高熵聚类中采样项，而不仅仅是单个最高熵聚类，你将在几行代码中解决预期误差减少的局限性。
- en: Rather than try to handcraft an algorithm that combines uncertainty sampling
    and diversity sampling into one algorithm, however, you can let machine learning
    decide on that combination for you. The original expected error reduction paper
    was titled “Toward Optimal Active Learning through Sampling Estimation of Error
    Reduction” and is 20 years old, so this is likely the direction that the authors
    had in mind. The rest of this chapter builds toward machine learning models for
    the sampling process itself in active learning.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是尝试手工制作一个将不确定性采样和多样性采样结合成一个算法的算法，然而，你可以让机器学习为你决定这种组合。原始的预期误差减少论文的标题为“通过采样估计误差减少实现最优主动学习”，这篇论文已有20年历史，因此这很可能是作者们所考虑的方向。本章的其余部分将构建用于主动学习中的采样过程的机器学习模型。
- en: 5.2 Active transfer learning for uncertainty sampling
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 不确定性采样的主动迁移学习
- en: 'The most advanced active learning methods use everything that you have learned
    so far in this book: the sampling strategies for interpreting confusion that you
    learned in chapter 3, the methods for querying the different layers in your models
    that you learned in chapter 4, and the combinations of techniques that you learned
    in the first part of this chapter.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的主动学习方法使用了你在本书中学到的所有内容：第3章中学到的解释混淆的采样策略，第4章中学到的查询模型不同层的方法，以及本章第一部分中学到的技术组合。
- en: Using all these techniques, you can build a new model with the task of predicting
    where the greatest uncertainty occurs. First, let’s revisit the description of
    transfer learning from chapter 1, shown here in figure 5.6.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些技术，你可以构建一个新的模型，其任务是预测最大不确定性的位置。首先，让我们回顾一下第1章中关于迁移学习的描述，如图5.6所示。
- en: '![](../Images/CH05_F06_Munro.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F06_Munro.png)'
- en: Figure 5.6 We have a model that predicts a label as “A,” “B,” “C,” or “D” and
    a separate dataset with the labels “W,” “X,” “Y,” and “Z.” When we retrain only
    the last layer of the model, the model is able to predict labels “W,” “X,” “Y,”
    and “Z,” using far fewer human-labeled items than if we were training a model
    from scratch.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 我们有一个预测标签为“A”、“B”、“C”或“D”的模型，以及一个带有标签“W”、“X”、“Y”和“Z”的独立数据集。当我们只重新训练模型的最后一层时，模型能够使用比从头开始训练模型时更少的人工标记项目来预测标签“W”、“X”、“Y”和“Z”。
- en: In the example in figure 5.6, you can see how a model can be trained on one
    set of labels and then retrained on another set of labels by keeping the architecture
    the same and freezing part of the model, retraining only the last layer in this
    case. There are many more ways to use transfer learning and contextual models
    for human-in-the-loop machine learning. The examples in this chapter are variations
    on the type of transfer learning shown in figure 5.6.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.6的示例中，你可以看到模型如何在一个标签集上训练，然后通过保持相同的架构和冻结模型的一部分（在这种情况下仅重新训练最后一层）在另一个标签集上重新训练。还有许多其他方法可以使用迁移学习和上下文模型进行人机交互的机器学习。本章中的示例是图5.6中所示迁移学习类型的变体。
- en: 5.2.1 Making your model predict its own errors
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 让你的模型预测自己的错误
- en: 'The new labels from transfer learning can be any categories that you want,
    including information about the task itself. This fact is the core insight for
    active transfer learning: you can use transfer learning to ask your model where
    it is confused by making it predict its own errors. Figure 5.7 outlines this process.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习产生的新标签可以是任何你想要的类别，包括关于任务本身的信息。这一事实是主动迁移学习的核心洞察：你可以使用迁移学习来询问你的模型它在哪些方面感到困惑，通过让它预测自己的错误。图5.7概述了这一过程。
- en: '![](../Images/CH05_F07_Munro.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F07_Munro.png)'
- en: Figure 5.7 Validation items are predicted by the model and bucketed as “Correct”
    or “Incorrect” according to whether they were classified correctly. Then the last
    layer of the model is retrained to predict whether items are “Correct” or “Incorrect,”
    effectively turning the two buckets into new labels.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 模型预测验证项目，并根据它们是否被正确分类将它们分类为“正确”或“不正确”。然后重新训练模型的最后一层来预测项目是“正确”还是“不正确”，有效地将这两个桶转换为新的标签。
- en: 'As figure 5.7 shows, this process has several steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.7所示，这个过程有几个步骤：
- en: Apply the model to a validation dataset, and capture which validation items
    were classified correctly and incorrectly. This data is your new training data.
    Now your validation items have an additional label of “Correct” or “Incorrect.”
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于验证数据集，并捕获哪些验证项目被正确和错误地分类。这些数据是你的新训练数据。现在你的验证项目有了“正确”或“不正确”的额外标签。
- en: Create a new output layer for the model, and train that new layer on your new
    training data, predicting your new “Correct” and “Incorrect” labels.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为模型创建一个新的输出层，并使用你的新训练数据训练这个新层，预测你的新“正确”和“不正确”标签。
- en: Run your unlabeled data items through the new model, and sample the items that
    are predicted to be “Incorrect” with the highest confidence.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将未标记的数据项通过新模型运行，并采样预测为“不正确”且置信度最高的项目。
- en: Now you have a sample of items that are predicted by your model as the most
    likely to be incorrect and therefore will benefit from a human label.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经获得了一组项目样本，这些项目被你的模型预测为最有可能不正确，因此将受益于人工标记。
- en: 5.2.2 Implementing active transfer learning
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 实施主动迁移学习
- en: The simplest forms of active transfer learning can be built with the building
    blocks of code that you have already learned. To implement the architecture in
    figure 5.7, you can create the new layer as its own model and use the final hidden
    layer as the features for that layer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的主动迁移学习方法可以使用您已经学到的代码构建块来构建。要实现图5.7中的架构，您可以创建一个新的层作为其自己的模型，并使用最终的隐藏层作为该层的特征。
- en: Here are the three steps from section 5.2.1, implemented in PyTorch. First,
    apply the model to a validation dataset, and capture which validation items were
    classified correctly and incorrectly. This data is your new training data. Your
    validation items have an additional label of “Correct” or “Incorrect,” which is
    in the (verbosely but transparently named) `get_deep_active_transfer_learning_uncertainty_samples()`
    method.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是5.2.1节中的三个步骤，在PyTorch中实现。首先，将模型应用于验证数据集，并捕获哪些验证项被正确和错误地分类。这些数据是您的新训练数据。您的验证项有一个额外的标签“正确”或“错误”，这在（名称冗长但透明地命名的）`get_deep_active_transfer_learning_uncertainty_samples()`方法中。
- en: Listing 5.6 Active transfer learning
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6 主动迁移学习
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Store the hidden layer for this item to use later for our new model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将此项目的隐藏层存储起来，稍后用于我们的新模型。
- en: ❷ The item was correctly predicted, so it gets a “Correct” label in our new
    model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 项被正确预测，因此在新模型中它获得“正确”标签。
- en: ❸ The item was incorrectly predicted, so it gets an “Incorrect” label in our
    new model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 项被错误预测，因此在新模型中它获得“错误”标签。
- en: Second, create a new output layer for the model trained on your new training
    data, predicting your new “Correct” and “Incorrect” labels.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，为在您的新的训练数据上训练的模型创建一个新的输出层，预测新的“正确”和“错误”标签。
- en: Listing 5.7 Creating a new output layer
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7 创建新的输出层
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The code for training is similar to the other examples in this book.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练代码与本书中的其他示例类似。
- en: ❷ Here, we use the hidden layer from the original model as our feature vector.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这里，我们使用原始模型的隐藏层作为我们的特征向量。
- en: Finally, run your unlabeled data items through the new model, and sample the
    items that are predicted to be incorrect with the highest confidence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将您的未标记数据项通过新模型运行，并采样预测置信度最高的错误项。
- en: Listing 5.8 Predicting “Incorrect” labels
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 预测“错误”标签
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The code for evaluation is similar to the others in this book.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 评估代码与本书中的其他示例类似。
- en: ❷ First, we need to get the hidden layer from our original model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 首先，我们需要从我们的原始模型中获取隐藏层。
- en: ❸ Then we use that hidden layer as the feature vector for our new model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 然后，我们将那个隐藏层作为我们新模型的特征向量。
- en: 'If you are interested in the disaster-response text classification task, try
    it with this new method for active transfer learning:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对灾害响应文本分类任务感兴趣，请尝试使用这种新的主动迁移学习方法：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see in this code, we are not altering our original model for predicting
    whether a message is related to disaster response. Instead of replacing the final
    layer of that model, we are effectively adding a new output layer over the existing
    model. As an alternative, you could replace the final layer with the same result.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这段代码中所见，我们并没有改变用于预测消息是否与灾害响应相关的原始模型。我们不是替换该模型的最后一层，而是在现有模型之上有效地添加了一个新的输出层。作为替代，您也可以用相同的结果替换最后一层。
- en: 'This architecture is used in this book because it is nondestructive. The old
    model remains. This architecture prevents unwanted errors when you still want
    to use the original model, either in production or for other sampling strategies.
    You also avoid needing the extra memory to have two copies of the full model in
    parallel. Building a new layer or copying and modifying the model are equivalent,
    so choose whichever approach is right for your codebase. All this code is in the
    same file as the methods discussed earlier in this chapter: advanced_active_learning.py.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用这种架构是因为它是非破坏性的。旧模型仍然存在。这种架构防止了当您仍然想使用原始模型（无论是生产中还是用于其他采样策略）时出现不希望的错误。您还避免了需要额外的内存来并行拥有两个完整模型的副本。构建一个新层或复制并修改模型是等效的，因此请选择适合您代码库的方法。所有这些代码都位于本章前面讨论的方法相同的文件中：advanced_active_learning.py。
- en: 5.2.3 Active transfer learning with more layers
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 带有更多层的主动迁移学习
- en: You don’t need to limit active transfer learning to a single new layer or build
    on only the last hidden layer. As figure 5.8 shows, you can build multiple new
    layers, and they can connect directly with any hidden layer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要将主动迁移学习限制在单个新层或仅基于最后一个隐藏层。如图5.8所示，您可以构建多个新层，并且它们可以直接连接到任何隐藏层。
- en: '![](../Images/CH05_F08_Munro.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8](../Images/CH05_F08_Munro.png)'
- en: Figure 5.8 More-complicated active transfer learning architectures, using active
    transfer learning to create a prediction. The top example has a single neuron
    in the new output layer. The bottom example is a more-complicated architecture,
    with a new hidden layer that connects with multiple existing hidden layers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 更复杂的活跃迁移学习架构，使用活跃迁移学习来创建预测。顶部示例在新的输出层中有一个单神经元。底部示例是一个更复杂的架构，有一个新的隐藏层与多个现有隐藏层相连。
- en: The extension to the more-complicated architecture in figure 5.8 requires only
    a few lines of extra code. First, the new model to predict “Correct” or “Incorrect”
    needs a hidden layer. Then that new model will take its features from multiple
    hidden layers. You can append the vectors from the different layers to one another,
    and this flattened vector becomes the features for the new model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将扩展到图5.8中更复杂的架构只需要几行额外的代码。首先，用于预测“正确”或“错误”的新模型需要一个隐藏层。然后，这个新模型将从多个隐藏层中提取其特征。你可以将不同层的向量附加在一起，这个展平的向量成为新模型的特征。
- en: If you are familiar with contextual models for natural language processing (NLP)
    or convolutional models for computer vision, this process is a familiar one; you
    are extracting the activations of neurons from several parts of your network and
    flattening into one long feature vector. The resulting vector is often called
    a *representation* because you are using the neurons from one model to represent
    your features in another model. We will return to representations in chapter 9,
    where they are also important for some semi-automated methods for creating training
    data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉自然语言处理（NLP）中的上下文模型或计算机视觉中的卷积模型，这个过程是熟悉的；你正在从你的网络的不同部分提取神经元的激活并将其展平成一个长的特征向量。这个结果向量通常被称为*表示*，因为你在使用一个模型中的神经元来表示另一个模型中的特征。我们将在第9章中回到表示，在那里它们对于一些半自动化的创建训练数据的方法也很重要。
- en: The fact that you can build a more complicated model, however, doesn’t mean
    that you should build it. If you don’t have a lot of validation data, you are
    more likely to overfit a more-complicated model. It is a lot easier to avoid training
    errors if you are training only a single new output neuron. Use your instincts
    about how complicated your model needs to be, based on what you would normally
    build for that amount of data for a binary prediction task.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以构建一个更复杂的模型，但这并不意味着你应该构建它。如果你没有很多验证数据，你更有可能对更复杂的模型进行过拟合。如果你只训练一个新输出神经元，那么避免训练错误会容易得多。根据你通常为该数据量构建的二进制预测任务，使用你的直觉来判断你的模型需要有多复杂。
- en: 5.2.4 The pros and cons of active transfer learning
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 活跃迁移学习的优缺点
- en: 'Active transfer learning has some nice properties that make it suitable for
    a wide range of problems:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃迁移学习具有一些很好的特性，使其适用于广泛的问题：
- en: You are reusing your hidden layers, so you are building models directly based
    on your model’s current information state.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在重用你的隐藏层，因此你是在直接基于你的模型当前的信息状态构建模型。
- en: You don’t need too many labeled items for the model to be effective, especially
    if you are retraining only the last layer (handy if your validation data is not
    large).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要太多的标记项来使模型有效，特别是如果你只重新训练最后一层（如果你的验证数据量不大，这很方便）。
- en: It is fast to train, especially if you are retraining only the last layer.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度快，特别是如果你只重新训练最后一层。
- en: It works with many architectures. You may be predicting labels at document or
    image level, predicting objects within an image, or generating sequences of text.
    For all these use cases, you can add a new final layer or layers to predict “Correct”
    or “Incorrect.” (For more on active learning use cases, see chapter 6.)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与许多架构一起工作。你可能是在文档或图像级别预测标签，预测图像中的对象，或者生成文本序列。对于所有这些用例，你都可以添加一个新的最终层或几层来预测“正确”或“错误”。（有关主动学习用例的更多信息，请参阅第6章。）
- en: You don’t need to normalize the different ranges of activation across different
    neurons, because your model is going to work out that task for you.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要对不同神经元的不同激活范围进行归一化，因为你的模型将为你完成这项任务。
- en: 'The fifth point is especially nice. Recall that with model-based outliers,
    you need to quantize the activation with the validation data because some of the
    neurons could be arbitrarily higher or lower in their average activation. It is
    nice to be able to pass the information to another layer of the neurons and tell
    that new layer to figure out exactly what weight to apply to the activation of
    each existing neuron. Active transfer learning also has some drawbacks:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第五点特别不错。回想一下，使用基于模型的异常值时，你需要使用验证数据对激活进行量化，因为一些神经元在平均激活上可能任意地更高或更低。能够将信息传递给另一层的神经元，并告诉新层确定每个现有神经元应应用的精确权重，这很好。主动迁移学习也有一些缺点：
- en: Like other uncertainty sampling techniques, it can focus too much on one part
    of the feature space; therefore, it lacks diversity.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他不确定性采样技术一样，它可能会过分关注特征空间的一部分；因此，它缺乏多样性。
- en: You can overfit your validation data. If there aren’t many validation items,
    your model for predicting uncertainty may not generalize beyond your validation
    data to your unlabeled data.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能会过拟合你的验证数据。如果没有很多验证项目，你的预测不确定性的模型可能无法推广到你的未标记数据。
- en: The first problem can be partially addressed without additional human labels,
    as you see later in this chapter in section 5.3.2\. This fact is one of the biggest
    strengths of this approach compared with the other uncertainty sampling algorithms.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题可以在不添加额外人工标签的情况下部分解决，正如你在本章5.3.2节中稍后看到的。这个事实是与其他不确定性采样算法相比，这种方法最大的优势之一。
- en: The overfitting problem can be diagnosed relatively easily too, because it manifests
    itself as high confidence that an item is an error. If you have a binary prediction
    for your main model, and your error-prediction model is 95% confident that an
    item was classified incorrectly, your main model should have classified that item
    correctly in the first place.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合问题也可以相对容易地诊断，因为它表现为对项目是错误的极高信心。如果你对你的主要模型有一个二进制预测，并且你的错误预测模型有95%的信心认为一个项目被错误分类，那么你的主要模型最初就应该正确地分类那个项目。
- en: If you find that you are overfitting and that stopping the training earlier
    doesn’t help, you can try to avoid overfitting by getting multiple predictions,
    using the ensemble methods from section 3.4 of chapter 3\. These methods include
    training multiple models, using dropouts at inference (Monte Carlo sampling),
    and drawing from different subsets of the validation items and features.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现你正在过拟合，并且提前停止训练没有帮助，你可以尝试通过获取多个预测，使用第三章3.4节中的集成方法来避免过拟合。这些方法包括训练多个模型，在推理时使用dropout（蒙特卡洛采样），以及从不同的验证项目特征子集中抽取。
- en: 5.3 Applying active transfer learning to representative sampling
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 将主动迁移学习应用于代表性采样
- en: We can apply the same active transfer learning principles to representative
    sampling. That is, we can adapt our models to predict whether an item is most
    like the application domain of our model compared with the current training data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将相同的主动迁移学习原则应用于代表性采样。也就是说，我们可以调整我们的模型来预测一个项目与我们的模型的应用领域相比，是否更相似于当前的训练数据。
- en: This approach will help with domain adaptation, like the representative sampling
    methods that you learned in chapter 4\. In fact, representative sampling is not
    too different. In both chapter 4 and the example in the following sections, you
    are building a new model to predict whether an item is most representative of
    the data to which you are trying to adapt your model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于领域自适应，例如你在第四章中学到的代表性采样方法。实际上，代表性采样并没有太大的不同。在第四章以及下文中的示例中，你都在构建一个新的模型来预测一个项目是否是你试图适应的模型数据的代表性。
- en: 5.3.1 Making your model predict what it doesn’t know
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 让你的模型预测它不知道的内容
- en: In principle, you don’t need your existing model to predict whether an item
    is in your training data or in your unlabeled data. You can build a new model
    that uses both your training data and your unlabeled data as a binary prediction
    problem. In practice, it is useful to include features that are important for
    the machine learning task that you are trying to build.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，你不需要你的现有模型来预测一个项目是否在你的训练数据中或在你的未标记数据中。你可以构建一个新的模型，该模型将你的训练数据和未标记数据作为二进制预测问题。在实践中，包括对你要构建的机器学习任务重要的特征是有用的。
- en: Figure 5.9 shows the process and architecture for representative active transfer
    learning, showing how you can retrain your model to predict whether unlabeled
    items are more like your current training data or more like the application domain
    for your model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9展示了代表性主动迁移学习的过程和架构，展示了如何重新训练你的模型以预测未标记项是更类似于你的当前训练数据还是更类似于你的模型的应用域。
- en: '![](../Images/CH05_F09_Munro.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9](../Images/CH05_F09_Munro.png)'
- en: Figure 5.9 We can build a model to sample the items that are most unlike the
    current training data. To begin, we take validation data from the same distribution
    as the training data and give it a “Training” label. Then we take unlabeled data
    from our target domain and give it an “Application” label. We train a new output
    layer to predict the “Training” and “Application” labels, giving it access to
    all layers of the model. We apply the new model to the unlabeled data (ignoring
    the unlabeled items that we trained on), and sample the items that are most confidently
    predicted as “Application.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 我们可以构建一个模型来采样与当前训练数据最不相似的项。首先，我们从与训练数据相同分布中获取验证数据，并给它一个“训练”标签。然后，我们从我们的目标域中获取未标记数据，并给它一个“应用”标签。我们训练一个新的输出层来预测“训练”和“应用”标签，使其能够访问模型的全部层。我们将新模型应用于未标记数据（忽略我们在其上训练的未标记项），并采样那些最自信地预测为“应用”的项。
- en: As figure 5.9 shows, there are few differences from active transfer learning
    for uncertainty sampling. First, the original model predictions are ignored. The
    validation and unlabeled data can be given labels directly. The validation data
    is from the same distribution as the training data, so it is given a “Training”
    label. The unlabeled data from the target domain is given an “Application” label.
    Then the model is trained on these labels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.9所示，与不确定性采样的主动迁移学习相比，差异很少。首先，忽略原始模型的预测。验证和未标记数据可以直接给予标签。验证数据来自与训练数据相同的分布，因此被赋予“训练”标签。来自目标域的未标记数据被赋予“应用”标签。然后，模型在这些标签上训练。
- en: Second, the new model should have access to more layers. If you are adapting
    to a new domain, you may have many features that do not yet exist in your training
    data. In such a case, the only information that your existing model contains is
    the fact that these features exist in the input layer as features but have not
    contributed to any other layer in the previous model. The more-complicated type
    of architecture will capture this information.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，新模型应该能够访问更多层。如果你正在适应一个新的领域，你可能有很多在训练数据中尚未存在的特征。在这种情况下，你现有模型包含的唯一信息是这些特征存在于输入层作为特征，但之前模型中的任何其他层都没有贡献。更复杂的架构类型将捕捉到这些信息。
- en: 5.3.2 Active transfer learning for adaptive representative sampling
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 自适应代表性采样的主动迁移学习
- en: Just like representative sampling (chapter 4) can be adaptive, active transfer
    learning for representative sampling can be adaptive, meaning that you can have
    multiple iterations within one active learning cycle, as shown in figure 5.10.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 就像代表性采样（第4章）可以是自适应的，代表性采样的主动迁移学习也可以是自适应的，这意味着你可以在一个主动学习周期内进行多次迭代，如图5.10所示。
- en: '![](../Images/CH05_F10_Munro.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10](../Images/CH05_F10_Munro.png)'
- en: Figure 5.10 Because our sampled items will get a human label later, we can assume
    that they become part of the training data without needing to know what the label
    is. To begin, we take validation data from the same distribution as the training
    data and give it a “Training” label. We take unlabeled data from our target domain
    and give it an “Application” label. We train a new output layer to predict the
    “Training” and “Application” labels, giving it access to all layers of the model.
    We apply the new model to the unlabeled data (ignoring the unlabeled items that
    we trained on) and sample the items that are most confidently predicted as “Application.”
    We can assume that those items will later get labels and become part of the training
    data. So we can take those sampled items, change their label from “Application”
    to “Training,” and retrain our final layer(s) on the new dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 由于我们的采样项目将在之后获得人工标签，我们可以假设它们成为训练数据的一部分，而无需知道标签是什么。首先，我们从与训练数据相同的分布中获取验证数据，并给它一个“训练”标签。我们从目标域中获取未标记数据，并给它一个“应用”标签。我们训练一个新的输出层来预测“训练”和“应用”标签，并让它访问模型的全部层。我们将新模型应用于未标记数据（忽略我们在其上训练的未标记项目），并采样那些最自信地预测为“应用”的项目。我们可以假设这些项目将后来获得标签并成为训练数据的一部分。因此，我们可以取这些采样项目，将它们的标签从“应用”改为“训练”，并在新的数据集上重新训练我们的最终层（们）。
- en: The process in figure 5.10 starts like the non-adaptive version. We create new
    output layers to classify whether an item is in the existing training data or
    in the target domain, sampling the items that are most confidently predicted as
    “Application.” To extend the process to the adaptive strategy, we can assume that
    the sampled items will later get a label and become part of the training data.
    So we can take those sampled items, change their label from “Application” to “Training,”
    and retrain our final layer(s) on the new dataset. This process can be repeated
    until there are no more confident predictions for “Application” domain items,
    or until you reach the maximum number of items that you want to sample in this
    iteration of active learning.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10中的过程与非自适应版本类似。我们创建新的输出层来分类一个项目是否在现有训练数据中或在目标域中，采样那些最自信地预测为“应用”的项目。为了将这个过程扩展到自适应策略，我们可以假设采样项目将后来获得标签并成为训练数据的一部分。因此，我们可以取那些采样项目，将它们的标签从“应用”改为“训练”，并在新的数据集上重新训练我们的最终层（们）。这个过程可以重复进行，直到没有更多对“应用”域项目的自信预测，或者直到您达到在这个主动学习迭代中想要采样的项目最大数量。
- en: 5.3.3 The pros and cons of active transfer learning for representative sampling
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 代表性采样中主动迁移学习的优缺点
- en: The pros and cons of active transfer learning for representative sampling are
    the same as for the simpler representative sampling methods in chapter 4\. Compared
    with those methods, the pros can be more positive because you are using more powerful
    models, but some of the cons, such as the danger of overfitting, become bigger
    potential errors.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性采样中主动迁移学习的优缺点与第4章中更简单的代表性采样方法相同。与这些方法相比，优点可能更加积极，因为您正在使用更强大的模型，但一些缺点，如过拟合的风险，可能成为更大的潜在错误。
- en: 'To summarize those strengths and weaknesses again: representative sampling
    is effective when you have all the data in a new domain, but if you’re adapting
    to future data that you haven’t sampled yet, your model can wind up being stuck
    in the past. This method is also the most prone to noise of all the active learning
    strategies in this book. If you have new data that is corrupted text—text from
    a language that is not part of your target domain, corrupted image files, artifacts
    that arise from using different cameras, and so on—any of these factors could
    look different from your current training data, but not in an interesting way.
    Finally, active transfer learning for representative sampling can do more harm
    than good if you apply it in iterations after you use uncertainty sampling, because
    your application domain will have more items away from the decision boundary than
    your training data. For these reasons, I recommended that you deploy active transfer
    learning for representative sampling only in combination with other sampling strategies,
    as you learned in section 5.1.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 再次总结这些优势和劣势：代表性采样在当你拥有新领域中的所有数据时是有效的，但如果你正在适应尚未采样的未来数据，你的模型可能会陷入过去。这种方法也是本书中所有主动学习策略中最容易受到噪声影响的。如果你有新的数据，比如损坏的文本——来自不属于你的目标领域的语言文本，损坏的图像文件，由不同相机使用产生的伪影等等——这些因素中的任何一个都可能与你当前的训练数据不同，但不是以有趣的方式。最后，如果你在不确定性采样之后迭代使用代表性采样，主动迁移学习可能会产生比好处更多的坏处，因为你的应用领域将有更多远离决策边界的项目，而你的训练数据则没有。出于这些原因，我建议你仅在与其他采样策略结合使用的情况下部署主动迁移学习进行代表性采样，正如你在5.1节所学的那样。
- en: 5.4 Active transfer learning for adaptive sampling
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 主动迁移学习自适应采样
- en: The final algorithm for active learning in this book is also the most powerful;
    it is a form of uncertainty sampling that can be adaptive within one iteration
    of active learning. All the uncertainty sampling techniques that you learned in
    chapter 3 were non-adaptive. Within one active learning cycle, all these techniques
    risk sampling items from only one small part of the problem space.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的主动学习最终算法也是最强大的；它是一种可以在主动学习单次迭代中自适应的不确定性采样。你在第3章学到的所有不确定性采样技术都是非自适应的。在一个主动学习周期内，所有这些技术都存在从问题空间的一个小部分采样项的风险。
- en: '*Active transfer learning for adaptive sampling* (ATLAS) is an exception, allowing
    adaptive sampling within one iteration without also using clustering to ensure
    diversity. ATLAS is introduced here with the caveat that it is the least-tested
    algorithm in this book at the time of publication. I invented ATLAS in late 2019
    when I realized that active transfer learning had certain properties that could
    be exploited to make it adaptive. ATLAS has been successful on the data that I
    have been experimenting with, but it has not yet been widely deployed in industry
    or tested under peer review in academia. As you would with any new method, be
    prepared to experiment to be certain that this algorithm is right for your data.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*自适应采样主动迁移学习*（ATLAS）是一个例外，它允许在一个迭代中自适应采样，而不需要使用聚类来确保多样性。ATLAS在此处引入时有一个前提，即它是本书中在出版时测试最少的一个算法。我在2019年底发明了ATLAS，当时我意识到主动迁移学习具有某些可以利用的特性，使其变得自适应。ATLAS在我所实验的数据上取得了成功，但它尚未在工业界广泛应用或在学术界经过同行评审。正如你对待任何新方法一样，准备好进行实验，以确保这个算法适合你的数据。'
- en: 5.4.1 Making uncertainty sampling adaptive by predicting uncertainty
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 通过预测不确定性使不确定性采样自适应
- en: 'As you learned in chapter 3, most uncertainty sampling algorithms have the
    same problem: they can sample from one part of the feature space, meaning that
    all the samples are similar in one iteration of active learning. You can end up
    sampling items from only one small part of your feature space if you are not careful.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在第3章所学，大多数不确定性采样算法都有相同的问题：它们可以从特征空间的一部分采样，这意味着在主动学习的一次迭代中，所有样本都是相似的。如果你不小心，你最终可能只会从特征空间的一个小部分采样项。
- en: 'As you learned in section 5.1.1, you can address this problem by combining
    clustering and uncertainty sampling. This approach is still the recommended way
    to think about beginning your active learning strategy; you can try ATLAS after
    you have that baseline. You can exploit two interesting properties of active transfer
    learning for uncertainty sampling:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在5.1.1节所学，你可以通过结合聚类和不确定性采样来解决此问题。这种方法仍然是开始你的主动学习策略时推荐的思考方式；在有了这个基线之后，你可以尝试ATLAS。你可以利用主动迁移学习在不确定性采样中的两个有趣特性：
- en: You are predicting whether the model is correct, not the actual label.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你预测的是模型是否正确，而不是实际的标签。
- en: You can generally expect to predict the labels of your training data items correctly.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通常可以正确预测训练数据项的标签。
- en: Taken together, these two items mean that you can assume that your sampled items
    will be correct later, even if you don’t yet know the labels (figure 5.11).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这两项加在一起意味着你可以假设你的样本项稍后会正确，即使你目前还不知道标签（见图5.11）。
- en: '![](../Images/CH05_F11_Munro.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11](../Images/CH05_F11_Munro.png)'
- en: Figure 5.11 Because our sampled items will later get a human label and become
    part of the training data, we can assume that the model will later predict those
    items correctly, because models are typically the most accurate on the actual
    items on which they trained. To begin, validation items are predicted by the model
    and bucketed as “Correct” or “Incorrect,” according to whether they were classified
    correctly. The last layer of the model is retrained to predict whether items are
    “Correct” or “Incorrect,” effectively turning the two buckets into new labels.
    We apply the new model to the unlabeled data, predicting whether each item will
    be “Correct” or “Incorrect.” We can sample the most likely to be “Incorrect.”
    Then we can assume that those items will get labels later and become part of the
    training data, which will be labeled correctly by a model that predicted on that
    same data. So we can take those sampled items, change their label from “Incorrect”
    to “Correct,” and retrain our final layer(s) on the new dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 因为我们的采样项稍后会被人工标记并成为训练数据的一部分，我们可以假设模型稍后会正确预测这些项，因为模型通常在它们训练的实际项上最准确。首先，验证项由模型预测，并根据它们是否被正确分类分为“正确”或“错误”的类别。模型的最后一层被重新训练以预测项是“正确”还是“错误”，有效地将这两个类别转换为新的标签。我们将新模型应用于未标记的数据，预测每个项将是“正确”还是“错误”。我们可以采样最有可能被标记为“错误”的项。然后我们可以假设这些项稍后会被标记，并成为训练数据的一部分，这将由预测相同数据的模型正确标记。因此，我们可以将这些采样项的标签从“错误”改为“正确”，并在新的数据集上重新训练我们的最终层。
- en: The process in figure 5.11 starts like the non-adaptive version. We create new
    output layers to classify whether an item is “Correct” or “Incorrect,” sampling
    the items that are most confidently predicted as “Incorrect.” To extend this architecture
    to the adaptive strategy, we can assume that those sampled items will be labeled
    later and become part of the training data, and that they will be predicted correctly
    after they receive a label (whatever that label might be). So we can take those
    sampled items, change their label from “Incorrect” to “Correct,” and retrain our
    final layer(s) on the new dataset. This process can be repeated until we have
    no more confidence predictions for “Incorrect” domain items or reach the maximum
    number of items that we want to sample in this iteration of active learning. It
    takes only 10 lines of code to implement ATLAS as a wrapper for active learning
    for uncertainty sampling.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11中的过程与非自适应版本类似。我们创建新的输出层来分类一个项是“正确”还是“错误”，采样那些最有可能被预测为“错误”的项。为了将这种架构扩展到自适应策略，我们可以假设那些采样项稍后会被标记，并成为训练数据的一部分，并且它们在获得标签后将被正确预测（无论标签是什么）。因此，我们可以将这些采样项的标签从“错误”改为“正确”，并在新的数据集上重新训练我们的最终层。这个过程可以重复进行，直到我们没有更多关于“错误”域项的信心预测，或者达到我们在这个主动学习迭代中想要采样的最大项数。将ATLAS作为主动学习的不确定性采样包装器实现，只需要10行代码。
- en: Listing 5.9 Active transfer learning for adaptive sampling
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9主动迁移学习用于自适应采样
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The key line of code adds a copy of the sampled item to the validation data
    after each cycle. If you are interested in the disaster-response text classification
    task, try it with this new method for an implementation of ATLAS:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 关键代码行在每次循环后将样本项的副本添加到验证数据中。如果你对灾难响应文本分类任务感兴趣，尝试使用ATLAS的新方法进行实现：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Because you are selecting 10 items by default (`number_per_iteration=10`) and
    want 100 total, you should see the model retrain 10 times during the sampling
    process. Play around with smaller numbers per iteration for a more diverse selection,
    which will take more time to retrain.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你默认选择10个项（`number_per_iteration=10`）并且总共需要100个，你应该在采样过程中看到模型重新训练10次。尝试使用每次迭代更小的数字以获得更多样化的选择，这将需要更多时间来重新训练。
- en: Although ATLAS adds only one step to the active transfer learning for uncertainty
    sampling architecture that you first learned, it can take a little bit of time
    to get your head around it. There aren’t many cases in machine learning in which
    you can confidently give a label to an unlabeled item without human review. The
    trick is that we are not giving our items an actual label; we know that the label
    will come later.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ATLAS只向您最初学习的用于不确定性采样的活跃迁移学习架构中添加了一步，但要理解它可能需要一点时间。在机器学习中，你可以在没有人工审查的情况下自信地为未标记的项目分配标签的情况并不多见。诀窍在于我们没有为我们的项目分配实际的标签；我们知道标签将在以后到来。
- en: 5.4.2 The pros and cons of ATLAS
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 ATLAS的优缺点
- en: 'The biggest pro of ATLAS is that it addresses both uncertainty sampling and
    diversity sampling in one method. This method has another interesting advantage
    over the other methods of uncertainty sampling: it won’t get stuck in inherently
    ambiguous parts of your feature space. If you have data that is inherently ambiguous,
    that data will continue to have high uncertainty for your model. After you annotate
    the data in one iteration of active learning, your model might still find the
    most uncertainty in that data in the next iteration. Here, our model’s (false)
    assumption that it will get this data right later helps us. We need to see only
    a handful of ambiguous items for ATLAS to start focusing on other parts of our
    feature space. There aren’t many cases in which a model’s making a mistake will
    help, but this case is one of them.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ATLAS最大的优点是它通过一种方法解决了不确定性采样和多样性采样问题。这种方法相对于其他不确定性采样方法还有一个有趣的优点：它不会陷入特征空间中固有的模糊部分。如果你有固有的模糊数据，这些数据对你的模型来说将继续保持高不确定性。在你进行一次主动学习迭代并标注数据后，你的模型可能在下一次迭代中仍然在这部分数据中找到最大的不确定性。在这里，我们的模型（错误的）假设它将在以后正确处理这些数据，这有助于我们。我们只需要看到几个模糊的项目，ATLAS就会开始关注我们特征空间的其它部分。在模型犯错有帮助的情况下并不多见，但这种情况是其中之一。
- en: 'The biggest con is the flip side: sometimes, you won’t get enough labels from
    one part of your feature space. You won’t know for certain how many items you
    need from each part of your feature space until you get the actual labels. This
    problem is the equivalent of deciding how many items to sample from each cluster
    when combining clustering and uncertainty sampling. Fortunately, future iterations
    of active learning will take you back to this part of your feature space if you
    don’t have enough labels. So it is safe to underestimate if you know that you
    will have more iterations of active learning later.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最主要的缺点是其反面：有时，你不会从特征空间的一个部分中获得足够的标签。除非你得到实际的标签，否则你无法确定你需要从特征空间的每个部分获取多少项目。这个问题等同于在结合聚类和不确定性采样时决定从每个簇中采样多少项目。幸运的是，如果你知道你将在以后有更多的主动学习迭代，那么低估这个问题是安全的。
- en: The other cons largely come from the fact that this method is untested and has
    the most complicated architecture. You may need a fair amount of hyperparameter
    tuning to build the most accurate models to predict “Correct” and “Incorrect.”
    If you can’t automate that tuning and need to do it manually, this process is
    not an automated adaptive process. Because the models are a simple binary task,
    and you are not retraining all the layers, the models shouldn’t require much tuning.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其他缺点大多源于这种方法尚未经过测试，并且具有最复杂的架构。你可能需要相当数量的超参数调整来构建最准确的模型来预测“正确”和“错误”。如果你不能自动化这个调整而需要手动进行，这个过程就不是自动自适应的过程。因为模型是一个简单的二进制任务，你没有重新训练所有层，所以模型不应该需要太多的调整。
- en: 5.5 Advanced active learning cheat sheets
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 高级主动学习速查表
- en: For quick reference, figures 5.12 and 5.13 show cheat sheets for the advanced
    active learning strategies in section 5.1 and the active transfer learning techniques
    in sections 5.2, 5.3, and 5.4.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速参考，图5.12和5.13展示了5.1节中高级主动学习策略和5.2、5.3、5.4节中主动迁移学习技术的速查表。
- en: '![](../Images/CH05_F12_Munro.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 高级主动学习速查表](../Images/CH05_F12_Munro.png)'
- en: Figure 5.12 Advanced active learning cheat sheet
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 高级主动学习速查表
- en: '![](../Images/CH05_F13_Munro.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 高级主动学习速查表](../Images/CH05_F13_Munro.png)'
- en: Figure 5.13 Active transfer learning cheat sheet
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 主动迁移学习速查表
- en: 5.6 Further reading for active transfer learning
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 活跃迁移学习的进一步阅读
- en: 'As you learned in the chapter, there is little existing work on the advanced
    active learning techniques in which one method is used to sample a large number
    of items and a second method is used to refine the sample. Academic papers about
    combining uncertainty sampling and diversity sampling focus on single metrics
    that combine the two methods, but in practice, you can simply chain the methods:
    apply one method to get a large sample and then refine that sample with another
    method. The academic papers tend to compare the combined metrics to the individual
    methods in isolation, so they will not give you an idea of whether they are better
    than chaining the methods together (section 5.1).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本章中学到的，关于使用一种方法采样大量项目，而另一种方法用于细化样本的高级主动学习技术，现有的工作很少。关于结合不确定性采样和多样性采样的学术论文主要关注结合两种方法的单一指标，但在实践中，你可以简单地串联这些方法：应用一种方法得到一个大样本，然后用另一种方法细化这个样本。学术论文倾向于将组合指标与独立的方法进行比较，因此它们不会给你一个关于它们是否比串联方法更好的想法（第5.1节）。
- en: The active transfer learning methods in this chapter are more advanced than
    the methods currently reported in academic or industry-focused papers. I have
    given talks about the methods before publishing this book, but all the content
    in those talks appears in this chapter, so there is nowhere else to read about
    them. I didn’t discover the possibility of extending active transfer learning
    to adaptive learning until late 2019, while *I was* creating the PyTorch library
    to accompany this chapter. After this book is published, look for papers that
    cite ATLAS for the up-to-date research.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提到的主动迁移学习方法比目前在学术或行业论文中报道的方法更为先进。在出版本书之前，我已经就这些方法发表过演讲，但所有这些演讲的内容都包含在本章中，因此别无他处可以阅读到它们。我直到2019年底在创建与本章配套的PyTorch库时，才发现了将主动迁移学习扩展到自适应学习的可能性。本书出版后，请寻找引用ATLAS的最新研究论文。
- en: If you like the fact that ATLAS turns active learning into a machine learning
    problem in itself, you can find a long list of interesting research papers. For
    as long as active learning has existed, people have been thinking about how to
    apply machine learning to the process of sampling items for human review. One
    good recent paper that I recommend is “Learning Active Learning from Data,” by
    Ksenia Konyushkova, Sznitman Raphael, and Pascal Fua ([http://mng.bz/Gxj8](http://mng.bz/Gxj8)).
    Look for the most-cited works in this paper and more recent work that cites this
    paper for approaches to active learning that use machine learning. For a deep
    dive, look at the PhD dissertation of Ksenia Konyushkova, the first author of
    the NeurIPS paper, which includes a comprehensive literature review.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢ATLAS将主动学习本身转化为一个机器学习问题的事实，你可以找到一份长长的有趣研究论文列表。自从主动学习存在以来，人们一直在思考如何将机器学习应用于为人工审查采样项目的过程。我推荐的一篇很好的近期论文是“从数据中学习主动学习”，作者是Ksenia
    Konyushkova、Sznitman Raphael和Pascal Fua（[http://mng.bz/Gxj8](http://mng.bz/Gxj8)）。寻找这篇论文中最被引用的工作以及引用这篇论文的更近期的作品，以了解使用机器学习的主动学习方法。为了深入了解，请参阅NeurIPS论文的第一作者Ksenia
    Konyushkova的博士论文，其中包含全面的文献综述。
- en: For an older paper that looks at ways to combine uncertainty and representative
    sampling, I recommend “Optimistic Active Learning Using Mutual Information,” by
    Yuhong Guo and Russ Greiner ([http://mng.bz/zx9g](http://mng.bz/zx9g)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一篇较旧的论文，该论文探讨了结合不确定性和代表性采样的方法，我推荐“使用互信息进行乐观主动学习”，作者是Yuhong Guo和Russ Greiner（[http://mng.bz/zx9g](http://mng.bz/zx9g)）。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You have many ways to combine uncertainty sampling and diversity sampling. These
    techniques will help you optimize your active learning strategy to sample the
    items for annotation that will most help your model’s accuracy.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有多种方式可以将不确定性采样和多样性采样结合起来。这些技术将帮助你优化你的主动学习策略，以采样对模型精度最有帮助的项目。
- en: Combining uncertainty sampling and clustering is the most common active learning
    technique and is relatively easy to implement after everything that you have learned
    in this book so far, so it is a good starting point for exploring advanced active
    learning strategies.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不确定性采样和聚类结合起来是最常见的主动学习方法，在你学习本书到目前为止的所有内容之后，相对容易实现，因此它是探索高级主动学习策略的一个好起点。
- en: Active transfer learning for uncertainty sampling allows you to build a model
    to predict whether unlabeled items will be labeled correctly, using your existing
    model as the starting point for the uncertainty-predicting model. This approach
    allows you to use machine learning within the uncertainty sampling process.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活跃的迁移学习用于不确定性采样允许你构建一个模型来预测未标记的项目是否会被正确标记，使用你的现有模型作为不确定性预测模型的起点。这种方法允许你在不确定性采样过程中使用机器学习。
- en: Active transfer learning for representative sampling allows you to build a model
    to predict whether unlabeled items are more like your target domain than your
    existing training data. This approach allows you to use machine learning within
    the representative sampling process.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活跃的迁移学习用于代表性采样允许你构建一个模型来预测未标记的项目是否比你的现有训练数据更接近你的目标领域。这种方法允许你在代表性采样过程中使用机器学习。
- en: ATLAS allows you to extend active transfer learning for uncertainty sampling
    so that you are not oversampling items from one area of your feature space, combining
    aspects of uncertainty sampling and diversity sampling into a single machine learning
    model.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ATLAS 允许你扩展活跃的迁移学习用于不确定性采样，这样你就不需要从特征空间的一个区域过度采样项目，将不确定性采样和多样性采样的方面结合到一个机器学习模型中。
- en: '* * *'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)“Toward Optimal Active Learning through Sampling Estimation of Error Reduction,”
    by Nicholas Roy and Andrew McCallum ([https://dl.acm.org/doi/10.5555/645530.655646](https://dl.acm.org/doi/10.5555/645530.655646)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (1.)“通过采样估计误差减少实现最优主动学习”，作者：尼古拉斯·罗伊和安德鲁·麦卡卢姆 ([https://dl.acm.org/doi/10.5555/645530.655646](https://dl.acm.org/doi/10.5555/645530.655646)).
