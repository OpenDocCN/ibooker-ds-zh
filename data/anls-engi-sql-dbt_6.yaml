- en: Chapter 6\. Building an End-to-End Analytics Engineering Use Case
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。构建端到端的分析工程用例
- en: Welcome to the last chapter of our book on analytics engineering with dbt and
    SQL. In the previous chapters, we have covered various concepts, techniques, and
    best practices for turning raw data into actionable insights using analytics engineering.
    Now it’s time to pull these topics all together and embark on a practical journey
    to construct an end-to-end analytics engineering use case.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们关于使用dbt和SQL进行分析工程的书的最后一章。在前面的章节中，我们涵盖了将原始数据转化为可操作见解的各种概念、技术和最佳实践。现在是时候把这些主题整合起来，开始一个实用的旅程，构建一个端到端的分析工程用例。
- en: In this chapter, we will look at designing, implementing, and deploying a comprehensive
    analytics solution from start to finish. We will leverage the full potential of
    dbt and SQL to build a robust and scalable analytics infrastructure and also use
    data modeling for both operational and analytical purposes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨从头开始设计、实施和部署全面的分析解决方案。我们将充分利用dbt和SQL的潜力，构建一个强大和可扩展的分析基础设施，并且在操作和分析目的上使用数据建模。
- en: Our main goal is to show how the principles and methods covered in this book
    can be practically applied to solve real-world data problems. By combining the
    knowledge acquired in the previous chapters, we will build an analytics engine
    that spans all phases of the data lifecycle, from data ingestion and transformation
    to modeling and reporting. Throughout the chapter, we’ll address common challenges
    that arise during implementation and provide guidance on how to effectively overcome
    them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是展示本书涵盖的原则和方法如何实际应用于解决现实世界的数据问题。通过结合在前几章中获得的知识，我们将构建一个涵盖数据生命周期所有阶段的分析引擎，从数据摄入和转换到建模和报告。在整章中，我们将解决实施过程中常见的挑战，并提供如何有效克服这些挑战的指导。
- en: 'Problem Definition: An Omnichannel Analytics Case'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义：全渠道分析案例
- en: In this challenge, our goal is to enhance the customer experience by providing
    seamless and personalized interactions across multiple channels. To achieve this,
    we need a comprehensive dataset that captures valuable customer insights. We require
    customer information, including names, email addresses, and phone numbers, to
    build a robust customer profile. It is essential to track customer interactions
    across channels, such as our website, mobile app, and customer support, to understand
    their preferences and needs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个挑战中，我们的目标是通过在多个渠道上提供无缝和个性化的互动来增强客户体验。为了实现这一目标，我们需要一个全面的数据集，捕捉有价值的客户洞察。我们需要客户信息，包括姓名、电子邮件地址和电话号码，以构建一个强大的客户档案。跟踪客户在网站、移动应用和客户支持等渠道上的互动至关重要，以了解他们的偏好和需求。
- en: We also need to gather order details, including order dates, total amounts,
    and payment methods, to analyze order patterns and identify opportunities for
    cross-selling or upselling. Furthermore, including product information like product
    names, categories, and prices will enable us to tailor our marketing efforts and
    promotions effectively. By analyzing this dataset, we can uncover valuable insights,
    optimize our omnichannel strategy, enhance customer satisfaction, and drive business
    growth.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要收集订单详细信息，包括订单日期、总金额和付款方式，以分析订单模式，并识别跨销售或交叉销售的机会。此外，包括产品信息如产品名称、类别和价格，将使我们能够有效地定制我们的营销活动和促销活动。通过分析这些数据集，我们可以发现有价值的见解，优化我们的全渠道策略，增强客户满意度，并推动业务增长。
- en: Operational Data Modeling
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作数据建模
- en: In our pursuit of a holistic approach we begin our journey with the operational
    step. In this way, we aim to create a solid foundation for subsequent steps. Our
    approach involves using the carefully documented requirements for a management
    database, which will guide us. In line with industry best practices, we will diligently
    follow the three essential steps—conceptual, logical, and physical modeling—to
    meticulously build our database.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们追求整体方法的过程中，我们从操作步骤开始我们的旅程。通过这种方式，我们旨在为后续步骤打下坚实的基础。我们的方法涉及使用精心记录的管理数据库需求，这将指导我们。根据行业最佳实践，我们将认真遵循三个关键步骤——概念建模、逻辑建模和物理建模——来精心构建我们的数据库。
- en: Keep in mind, we are opting for a breadth-first strategy that covers all the
    steps but does not go deep in terms of detail. Thus, consider this an academic
    exercise with simplified requirements, intended to equip you with a better understanding
    of the processes of building an operational database, and not a comprehensive
    one.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们选择了广度优先策略，涵盖了所有步骤，但在细节方面并不深入。因此，请将其视为一种学术练习，具有简化的要求，旨在让您更好地理解构建操作数据库过程，而不是全面的过程。
- en: Conceptual Model
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念模型
- en: As we previously described, the first step, consisting of the conceptual modeling
    phase, allows us to conceptualize and define the overall structure and relationships
    within the database. This involves identifying the key entities, their attributes,
    and their associations. Through careful analysis and collaboration with stakeholders,
    we will capture the essence of the management system and translate it into a concise
    and comprehensive conceptual model ([Figure 6-1](#E2EConceptualDiagram)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前描述的，第一步，即概念建模阶段，使我们能够对数据库内部结构及其关系进行概念化和定义。这包括识别关键实体、它们的属性及其关联。通过仔细分析和与利益相关者的合作，我们将捕捉管理系统的本质，并将其转化为简明和全面的概念模型（[图6-1](#E2EConceptualDiagram)）。
- en: '![ch06_concept](assets/aesd_0601.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_concept](assets/aesd_0601.png)'
- en: Figure 6-1\. Conceptual diagram for our operational database
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 我们操作数据库的概念图
- en: In the conceptual model in [Figure 6-1](#E2EConceptualDiagram), we can observe
    three entities, Customer, Channel, and Products, with two key relationships, Buy
    and Visit. The first relationship enables us to track purchases of customers of
    certain products in certain channels. (Keep in mind, we need the channels for
    understanding performance across them.) The second relationship allows us to track
    interactions across channels. For each entity and relationship, we have defined
    a few attributes to make it a richer database.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图6-1](#E2EConceptualDiagram)的概念模型中，我们可以观察到三个实体：客户、渠道和产品，以及两个关键关系：购买和访问。第一个关系使我们能够追踪客户在某些渠道购买特定产品的情况。（请注意，我们需要渠道来了解其跨渠道的表现。）第二个关系允许我们跨渠道跟踪互动。对于每个实体和关系，我们定义了一些属性，使其成为一个更丰富的数据库。
- en: Logical Model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑模型
- en: 'As we previously mentioned, to convert the conceptual ERD exercise into a logical
    schema, we create a structured representation of entities, attributes, and their
    relationships. This schema acts as a foundation for implementing the database
    in a specific system. We turn the entities into tables, with their attributes
    becoming table columns. Relationships are handled differently based on the type:
    for N:1 relationships, we use foreign keys to connect tables, and for M:N relationships,
    we create a separate table to represent the connection. By following these steps,
    we ensure data integrity and efficient management of the database, almost normalizing
    implicitly our conceptual model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，为了将概念性ERD练习转换为逻辑模式，我们创建了实体、属性及其关系的结构化表示。此模式充当在特定系统中实施数据库的基础。我们将实体转换为表，其属性成为表列。根据关系类型不同处理关系：对于N:1关系，我们使用外键连接表，对于M:N关系，我们创建一个单独的表来表示连接。通过这些步骤，我们确保数据完整性和高效的数据库管理，几乎在隐式地规范化我们的概念模型。
- en: If we apply the previous rules to our concept, we should be able to come up
    with something similar to [Figure 6-2](#E2ELogicalDiagram).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将上述规则应用于我们的概念，我们应该能够得出类似于[图6-2](#E2ELogicalDiagram)的结果。
- en: '![ch06_logical](assets/aesd_0602.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_logical](assets/aesd_0602.png)'
- en: Figure 6-2\. Logical schema for our operational database
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 我们操作数据库的逻辑模式
- en: 'As you can see, we now have five tables: three of them represent the primary
    entities (Customers, Products, and Channels), while the remaining two tables represent
    the relationships. However, for the sake of simplicity, we have renamed the two
    relation tables from Buy and Visit to Purchase history and Visit history, respectively.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在我们有五个表：其中三个代表主要实体（客户、产品和渠道），而剩下的两个表则代表关系。但是为了简化起见，我们将这两个关系表从购买和访问改名为购买历史和访问历史。
- en: Physical Model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物理模型
- en: While the logical model primarily deals with the conceptual representation of
    the database, the physical model delves into the practical aspects of data management,
    assuming we have chosen a certain database engine. In our case, it will be MySQL.
    Thus, we need to translate the logical model into specific storage configurations,
    as per MySQL best practices and limitations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑模型主要处理数据库的概念表示，而物理模型则深入探讨数据管理的实际方面，假设我们选择了特定的数据库引擎。在我们的情况下，将是 MySQL。因此，我们需要按照
    MySQL 的最佳实践和限制将逻辑模型转换为具体的存储配置。
- en: '[Figure 6-3](#E2EPhysicalDiagram) shows our ERD diagram, representing a MySQL
    data types and constraints.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](#E2EPhysicalDiagram) 显示了我们的 ERD 图表，表示 MySQL 的数据类型和约束。'
- en: '![ch06_phisic](assets/aesd_0603.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_phisic](assets/aesd_0603.png)'
- en: Figure 6-3\. Physical diagram for our operational database in MySQL
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 我们在 MySQL 中操作数据库的物理图表。
- en: Now we can translate the previous model to a set of DDL scripts, starting by
    creating a new MySQL database to store our table structures ([Example 6-1](#db_oper_01)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将之前的模型翻译为一组 DDL 脚本，首先创建一个新的 MySQL 数据库以存储我们的表结构（[示例 6-1](#db_oper_01)）。
- en: Example 6-1\. Creating the primary tables
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 创建主表
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In [Example 6-2](#db_oper_012), we now handle the DDL code to create the three
    primary tables: `customers`, `products`, and `channels`.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 6-2](#db_oper_012) 中，我们现在处理创建三个主要表 `customers`、`products` 和 `channels`
    的 DDL 代码。
- en: Example 6-2\. Creating our operational database
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 创建我们的操作数据库
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `customers` table has columns such as `customer_id`, `name`, `date_birth`,
    `email_address`, `phone_number`, and `country`. The `customer_id` column acts
    as the primary key, uniquely identifying each customer. It is set to automatically
    increment its value for every new customer added. The other columns store relevant
    information about the customer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`customers` 表格包括诸如 `customer_id`、`name`、`date_birth`、`email_address`、`phone_number`
    和 `country` 的列。`customer_id` 列充当主键，唯一标识每个客户。它被设置为每添加一个新客户时自动递增其值。其他列存储客户的相关信息。'
- en: The `products` and `channels` tables follow a similar approach. However, `products`
    consists of columns such as `product_sku`, `product_name`, and `unit_price`, whereas
    `channels` contains only `channel_id` and `channel_name`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`products` 和 `channels` 表格采用类似的方法。然而，`products` 包括列如 `product_sku`、`product_name`
    和 `unit_price`，而 `channels` 只包含 `channel_id` 和 `channel_name`。'
- en: All tables’ creation code includes the `IF NOT EXISTS` clause available in MySQL,
    which ensures that the tables are created only if they do not already exist in
    the database. This helps prevent any errors or conflicts when executing the code
    multiple times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所有表格的创建代码都包括 MySQL 中的 `IF NOT EXISTS` 子句，这确保只有在数据库中不存在这些表时才创建它们。这有助于在多次执行代码时防止任何错误或冲突。
- en: We are using both `CREATED_AT` and `UPDATED_AT` columns in all our tables as
    this is a best practice. By adding these, often called *audit columns*, we make
    our database ready for incremental extractions of data in the future. This is
    also required for many CDC tools that handle this incremental extraction for us.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在所有表格中使用 `CREATED_AT` 和 `UPDATED_AT` 列，因为这是最佳实践。通过添加这些通常称为 *审计列* 的列，我们使我们的数据库准备好在将来进行增量数据提取。这也是许多处理此增量提取的
    CDC 工具所需的。
- en: We can now create the relationship tables, as seen in [Example 6-3](#db_oper_02).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建关系表，如 [示例 6-3](#db_oper_02) 所示。
- en: Example 6-3\. Creating the relationship tables
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 创建关系表
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `purchaseHistory` table, which is probably the heart of our purchase relationship,
    has columns such as `customer_id`, `product_sku`, `channel_id`, `quantity`, `discount`,
    and `order_date`. The `customer_id`, `product_sku`, and `channel_id` columns represent
    the foreign keys referencing the respective primary keys in the `customers`, `products`,
    and `channels` tables. These foreign keys establish relationships among the tables.
    The `quantity` column stores the quantity of products purchased, while the `discount`
    column holds the discount applied to the purchase (with a default value of 0 if
    not specified, assuming this is the norm). The `order_date` column records the
    date and time of the purchase and is marked `NOT NULL`, meaning it must always
    have a value.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`purchaseHistory` 表可能是我们购买关系的核心，包含诸如 `customer_id`、`product_sku`、`channel_id`、`quantity`、`discount`
    和 `order_date` 等列。`customer_id`、`product_sku` 和 `channel_id` 列分别表示引用 `customers`、`products`
    和 `channels` 表中对应主键的外键。这些外键建立了表之间的关系。`quantity` 列存储购买的产品数量，而 `discount` 列存储购买时应用的折扣（如果没有指定，默认值为0，假定这是标准）。`order_date`
    列记录购买的日期和时间，并标记为 `NOT NULL`，意味着它必须始终有值。'
- en: The `visitHistory` table is similar to `PurchaseHistory` and contains columns
    such as `customer_id`, `channel_id`, `visit_timestamp`, and `bounce_timestamp`.
    The `customer_id` and `channel_id` columns serve as foreign keys referencing the
    primary keys of the `customers` and `channels` tables. The `visit_timestamp` column
    captures the timestamp indicating when a customer visited a particular channel,
    while the `bounce_timestamp` records the timestamp if the visit resulted in a
    bounce (departure from the channel without any further action).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`visitHistory` 表类似于 `PurchaseHistory`，包含诸如 `customer_id`、`channel_id`、`visit_timestamp`
    和 `bounce_timestamp` 等列。`customer_id` 和 `channel_id` 列作为外键，引用 `customers` 和 `channels`
    表的主键。`visit_timestamp` 列记录客户访问特定渠道的时间戳，而 `bounce_timestamp` 记录如果访问导致跳出（在没有进一步操作的情况下离开渠道）的时间戳。'
- en: The `FOREIGN KEY` constraints enforce referential integrity, ensuring that the
    values in the foreign-key columns correspond to existing values in the referenced
    tables (`customers`, `products`, and `channels`). This helps maintain the integrity
    and consistency of the data within the database.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`FOREIGN KEY` 约束确保了外键列中的值与参考表（`customers`、`products` 和 `channels`）中的现有值相对应，从而维护了数据库内数据的完整性和一致性。'
- en: Modeling an operational database is a valuable aspect of an analyst’s skill
    set, even if they are not always directly responsible for designing the raw database
    structures. Understanding the principles and considerations underlying operational
    database modeling gives analysts a holistic perspective on the entire data pipeline.
    This knowledge helps them understand the origin and structure of the data they
    are working with, which in turn enables them to work more effectively with the
    data engineers responsible for the operational layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 建模运营数据库是分析师技能中宝贵的一部分，即使他们并非总是直接负责设计原始数据库结构。理解运营数据库建模背后的原则和考虑因素，使分析师能够全面了解整个数据管道。这种知识帮助他们理解他们正在处理的数据的起源和结构，从而使他们能够更有效地与负责运营层的数据工程师合作。
- en: While it is not the job of analysts and analytics engineers to design these
    databases from scratch, with knowledge of operational modeling, they can navigate
    the intricacies of the data sources and ensure that the data is structured and
    organized to meet their analytical needs. In addition, this understanding aids
    in troubleshooting and optimizing data pipelines, as analytics engineers can identify
    potential problems or opportunities for improvement within the operational database
    layer. In essence, familiarity with operational database modeling improves analytical
    skills and contributes to more efficient and collaborative data-driven workflows.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分析师和分析工程师的工作并非从零开始设计这些数据库，但了解运营建模可以帮助他们解决数据源的复杂性，并确保数据按照他们的分析需求进行结构化和组织。此外，这种理解有助于故障排除和优化数据管道，因为分析工程师可以在运营数据库层面识别潜在的问题或改进机会。总之，熟悉运营数据库建模可以提升分析技能，促进更高效和协作的数据驱动工作流程。
- en: High-Level Data Architecture
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级数据架构
- en: We have designed a lean data architecture to support the initial requirements
    of our omnichannel use case. We will start by developing a Python script to extract
    the data from MySQL, clean a few data types, and then send the data to our BigQuery
    project. [Figure 6-4](#SolutionArchitecture) illustrates our target solution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一个精简的数据架构来支持我们全渠道使用案例的初始需求。我们将从开发一个Python脚本开始，从MySQL提取数据，清理一些数据类型，然后将数据发送到我们的BigQuery项目。[图 6-4](#SolutionArchitecture)说明了我们的目标解决方案。
- en: '![ch06_diagram_arc](assets/aesd_0604.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_diagram_arc](assets/aesd_0604.png)'
- en: Figure 6-4\. Diagram for our lean data architecture to support our use case
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 用于支持我们用例的精简数据架构图
- en: Once the data lands on the raw environment, we will leverage dbt to transform
    our data and build the required models that will compose our star schema. Last
    but not least, we will analyze data in BigQuery by running SQL queries against
    our star schema data model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入原始环境，我们将利用dbt来转换数据并构建所需的模型，这些模型将组成我们的星型模式。最后但同样重要的是，我们将通过对我们的星型模式数据模型运行SQL查询来分析BigQuery中的数据。
- en: 'To extract the data from MySQL and load it to BigQuery, we have decided to
    simulate an ETL job ([Example 6-4](#db_etl_01)). The orchestrator code block containing
    a single function, `data_pipeline_mysql_to_bq`, performs a few steps: extracting
    data from a MySQL database, transforming it, and loading it into our target dataset
    in BigQuery. The code starts by importing the necessary modules, including `mysql.connector`
    for MySQL database connectivity and `pandas` for data manipulation. Another key
    library, *pandas_bq*, is also used later in our code structure.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从MySQL提取数据并将其加载到BigQuery，我们决定模拟一个ETL作业（[示例 6-4](#db_etl_01)）。包含单个函数`data_pipeline_mysql_to_bq`的编排器代码块执行几个步骤：从MySQL数据库中提取数据，转换数据，并将其加载到BigQuery的目标数据集中。代码从导入必要的模块开始，包括用于MySQL数据库连接的`mysql.connector`和用于数据操作的`pandas`。另一个关键库*pandas_bq*也在我们的代码结构中稍后使用。
- en: The `data_pipeline_mysql_to_bq` function takes keyword arguments (`**kwargs`)
    to receive configuration details required for the pipeline. In Python, `**kwargs`
    is a special syntax used to pass a variable number of keyword arguments to a function
    as a dictionary-like object Inside the function, a connection is established with
    the MySQL database using the provided connection details.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_pipeline_mysql_to_bq`函数采用关键字参数(`**kwargs`)来接收流水线所需的配置详细信息。在Python中，`**kwargs`是一种特殊的语法，用于将可变数量的关键字参数传递给函数，形式上类似于字典。在函数内部，使用提供的连接详细信息与MySQL数据库建立连接。'
- en: To automate the table extraction, given that we want all the tables in our source
    database, we create a simple routine using the `information_schema` of MySQL.
    This is a virtual database that provides access to metadata about the database
    server, databases, tables, columns, indexes, privileges, and other important information.
    It is a system schema that is automatically created and maintained by MySQL. We
    leverage the `information_schema` to get all the table names in our database,
    and the result is stored in a DataFrame named `df_tables`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动化表提取，考虑到我们想要源数据库中的所有表，我们使用MySQL的`information_schema`创建了一个简单的例程。这是一个虚拟数据库，提供了关于数据库服务器、数据库、表、列、索引、权限和其他重要信息的元数据访问。`information_schema`是MySQL自动创建和维护的系统模式。我们利用`information_schema`获取我们数据库中所有表的表名，并将结果存储在名为`df_tables`的DataFrame中。
- en: After this step, we initiate the core of our pipeline, calling an extraction,
    a transformation, and a load function to simulate the three steps in an ETL job.
    The code snippets in [Example 6-4](#db_etl_01) illustrate how we create these
    functions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，我们启动我们流水线的核心，调用提取、转换和加载函数来模拟ETL作业中的三个步骤。代码片段在[示例 6-4](#db_etl_01)中展示了我们如何创建这些函数。
- en: Example 6-4\. Loading data into BigQuery
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 将数据加载到BigQuery
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In [Example 6-5](#db_etl_02), we define the `extract_table_from_mysql` function
    that simulates the extraction step in an ETL job. This function is responsible
    for retrieving data from a specified table in a MySQL database. It takes two parameters:
    `table_name`, which represents the name of the table to be extracted, and `my_sql_connection`,
    which represents the connection object or connection details for the MySQL database.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-5](#db_etl_02)中，我们定义了`extract_table_from_mysql`函数，模拟了ETL作业中的提取步骤。该函数负责从MySQL数据库中指定的表中检索数据。它接受两个参数：`table_name`，表示要提取的表的名称，以及`my_sql_connection`，表示MySQL数据库的连接对象或连接详细信息。
- en: To perform the extraction, the function constructs a SQL query by concatenating
    the table name with the `select * from` statement. This is a very simple way to
    extract all the rows and works well in our example; however, you might want to
    extract this data incrementally by filtering records where `updated_at` or `created_at`
    are greater than the last extraction date (which you can store in a metadata table).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行提取操作，该函数通过将表名与 `select * from` 语句连接起来构造 SQL 查询。这是一种非常简单的提取所有行的方法，在我们的示例中运行良好；然而，您可能希望通过筛选
    `updated_at` 或 `created_at` 大于最后提取日期的记录（可以存储在元数据表中）来逐步提取这些数据。
- en: Next, the function utilizes the `pd.read_sql` function from the *pandas* library
    to execute the extraction query. It passes the query and the MySQL connection
    object (`my_sql_connection`) as arguments. The function reads the data from the
    specified table and loads it into a pandas DataFrame named `df_table_data`. Finally,
    it returns the extracted DataFrame, which contains the data retrieved from the
    MySQL table.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，该函数利用 *pandas* 库中的 `pd.read_sql` 函数执行提取查询。它将查询和 MySQL 连接对象 (`my_sql_connection`)
    作为参数。该函数从指定表中读取数据，并将其加载到名为 `df_table_data` 的 pandas DataFrame 中。最后，它返回提取的包含从 MySQL
    表中检索到的数据的 DataFrame。
- en: Example 6-5\. Loading data into BigQuery—extraction
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 加载数据到 BigQuery—提取
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In [Example 6-6](#db_etl_03), we define the `transform_data_from_table` function
    that represents the transformation step in an ETL job. This function is responsible
    for performing a specific transformation on a DataFrame called `df_table_data`.
    In this case, we do something simple: clean the dates in the DataFrame by converting
    them to strings to avoid conflicts with the *pandas_bq* library. To achieve this,
    the function identifies the columns with an object data type (string columns)
    by using the `select_dtypes` method. It then iterates over these columns and checks
    the data type of the first value in each column by converting it to a string representation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 6-6](#db_etl_03)，我们定义了 `transform_data_from_table` 函数，该函数代表了 ETL 作业中的转换步骤。该函数负责对名为
    `df_table_data` 的 DataFrame 执行特定的转换。在这种情况下，我们做了一些简单的事情：通过将日期转换为字符串来清理 DataFrame
    中的日期，以避免与 *pandas_bq* 库发生冲突。为了实现这一目标，该函数使用 `select_dtypes` 方法识别具有对象数据类型（字符串列）的列。然后，它迭代这些列，并通过将第一个值转换为字符串表示来检查每列的数据类型。
- en: If the data type is identified as `<class *datetime.date*>`, indicating that
    the column contains date values, the function proceeds to convert each date value
    to a string format. This is done by mapping each value to its string representation
    by using a `lambda` function. After performing the transformation, the function
    returns the modified DataFrame with the cleaned dates.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据类型被识别为 `<class *datetime.date*>`，表明该列包含日期值，则函数继续将每个日期值转换为字符串格式。这是通过使用 `lambda`
    函数将每个值映射到其字符串表示来完成的。在执行转换后，函数返回具有清理后日期的修改后的 DataFrame。
- en: Example 6-6\. Loading data into BigQuery—transformation
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 加载数据到 BigQuery—转换
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In [Example 6-7](#db_etl_04), we define the `load_data_into_bigquery` method,
    which provides a convenient way to load data from a pandas DataFrame into a specified
    BigQuery table by using the *pandas_gbq* library. It ensures that the existing
    table is replaced with the new data, allowing seamless data transfer and update
    within the BigQuery environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 6-7](#db_etl_04)，我们定义了 `load_data_into_bigquery` 方法，它提供了一种方便的方法，通过 *pandas_gbq*
    库将数据从 pandas DataFrame 加载到指定的 BigQuery 表中。它确保现有表格被新数据替换，允许在 BigQuery 环境中进行无缝数据传输和更新。
- en: 'The function takes four parameters: `bq_project_id` represents the project
    ID of the BigQuery project, while `dataset` and `table_name` specify the target
    dataset and table in BigQuery, respectively. The `df_table_data` parameter is
    a pandas DataFrame that contains the data to be loaded.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受四个参数：`bq_project_id` 表示 BigQuery 项目的项目 ID，`dataset` 和 `table_name` 分别指定
    BigQuery 中的目标数据集和表。`df_table_data` 参数是一个包含要加载的数据的 pandas DataFrame。
- en: Example 6-7\. Loading data into BigQuery—load
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. 加载数据到 BigQuery—加载
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In [Example 6-8](#db_etl_05), we execute the data pipeline by calling the `data_pipeline_mysql_to_bq`
    function with the specified keyword arguments. The code creates a dictionary named
    `kwargs` that holds the required keyword arguments for the function. This is a
    convenient way to pass multiple parameters in Python without having to add them
    all to the method signature. The `kwargs` dictionary includes values such as the
    BigQuery project ID, dataset name, MySQL connection details (host, username, password),
    and the name of the MySQL database containing the data to be transferred. However,
    the actual values for the BigQuery project ID, MySQL host information, username,
    and password need to be replaced with the appropriate values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-8](#db_etl_05)中，我们通过调用`data_pipeline_mysql_to_bq`函数并提供指定的关键字参数来执行数据管道。该代码创建了一个名为`kwargs`的字典，其中包含函数所需的关键字参数。这是在Python中传递多个参数的便捷方式，而无需将它们全部添加到方法签名中。`kwargs`字典包括BigQuery项目ID、数据集名称、MySQL连接详细信息（主机、用户名、密码）以及包含要传输数据的MySQL数据库的名称。但是，实际的BigQuery项目ID、MySQL主机信息、用户名和密码的值需要替换为适当的值。
- en: We call the function `data_pipeline_mysql_to_bq` by providing the `kwargs` dictionary
    contents as keyword arguments. This triggers the data pipeline that moves data
    from the specified MySQL database to the target BigQuery table.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供`kwargs`字典内容作为关键字参数调用函数`data_pipeline_mysql_to_bq`。这会触发数据管道，将数据从指定的MySQL数据库移动到目标BigQuery表中。
- en: Example 6-8\. Loading data into BigQuery—call orchestrator
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-8\. 加载数据到BigQuery — 调用编排器
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We should now have our raw data loaded into our target dataset on BigQuery,
    ready to be transformed into a dimensional model, using dbt as a tool to do so.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该已经将原始数据加载到BigQuery的目标数据集中，准备使用dbt工具将其转换为维度模型。
- en: Analytical Data Modeling
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析数据建模
- en: As we saw earlier in this book, analytical data modeling uses a systematic approach
    encompassing several crucial steps to create a compelling and meaningful representation
    of your business processes. The first step is identifying and understanding the
    business processes driving your organization. This involves mapping out the key
    operational activities, data flows, and interdependencies among departments. By
    fully understanding your business processes, you can pinpoint the critical touchpoints
    where data is generated, transformed, and utilized.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书前面所看到的，分析数据建模使用系统化的方法，包括若干关键步骤，以创建您业务流程的引人入胜且有意义的表示形式。第一步是确定和理解推动您组织的业务流程。这涉及映射关键运营活动、数据流和部门之间的相互依赖关系。通过充分理解您的业务流程，您可以确定生成、转换和利用数据的关键接触点。
- en: Once you have a clear picture of your business processes, the next step is identifying
    the facts and dimensions in your dimensional data model. Facts represent the measurable
    and quantifiable data points you want to analyze, such as sales figures, customer
    orders, or website traffic. On the other hand, dimensions provide the necessary
    context for these facts. They define the various attributes and characteristics
    that describe the facts. Identifying these facts and dimensions is essential for
    structuring your data model effectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您清楚了解了业务流程，下一步是确定维度数据模型中的事实和维度。事实代表您想要分析的可衡量和可量化的数据点，例如销售数字、客户订单或网站流量。另一方面，维度为这些事实提供必要的上下文。它们定义了描述事实的各种属性和特征。有效地构建数据模型的关键在于确定这些事实和维度。
- en: Once you have identified the facts and dimensions, the next step is to identify
    the attributes for each dimension. Attributes provide additional detail and enable
    a more profound analysis of the data. They describe specific characteristics or
    properties associated with each dimension. Using an example of the product dimension,
    attributes could include product color, size, weight, and price. Similarly, if
    we want a customer dimension, attributes might encompass demographic information
    such as age, gender, and location. By identifying relevant attributes, you enhance
    the richness and depth of your data model, enabling more insightful analysis.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了事实和维度，下一步是确定每个维度的属性。属性提供额外的细节，并且能够更深入地分析数据。它们描述与每个维度相关的特定特征或属性。以产品维度为例，属性可能包括产品的颜色、尺寸、重量和价格。同样，如果我们想要一个客户维度，属性可能涵盖诸如年龄、性别和位置等人口统计信息。通过确定相关属性，您可以增强数据模型的丰富性和深度，从而实现更深入的分析。
- en: Defining the granularity of business facts is the final step in analytical data
    modeling. *Granularity* refers to the level of detail at which you capture and
    analyze your business facts. Balancing capturing enough detail for meaningful
    analysis and avoiding unnecessary data complexity is essential. For instance,
    in retail sales analysis, the granularity could be defined at the transaction
    level, capturing individual customer purchases. In the alternative, we can have
    other higher-level granularities, such as daily, weekly, or monthly aggregates.
    The choice of granularity depends on your analytical objectives, data availability,
    and the level of detail necessary to derive valuable insights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定义商业事实的粒度是分析数据建模的最后一步。*粒度*指的是捕获和分析业务事实的详细级别。在平衡捕获足够详细的数据以进行有意义分析和避免不必要的数据复杂性方面至关重要。例如，在零售销售分析中，粒度可以定义为交易级别，捕获每位客户的购买情况。另一方面，我们也可以选择其他更高层次的粒度，如每日、每周或每月的汇总。粒度的选择取决于您的分析目标、数据可用性以及从中获得有价值洞见所需的详细程度。
- en: By following these steps in analytical data modeling, you establish a solid
    foundation for creating a data model that accurately represents your business,
    captures the essential facts and dimensions, includes relevant attributes, and
    defines an appropriate level of granularity. A well-designed data model allows
    you to unlock the power of your data, gain valuable insights, and make informed
    decisions to drive business growth and success.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在分析数据建模中遵循这些步骤，您将为创建一个准确代表您的业务、捕获关键事实和维度、包含相关属性并定义适当粒度的数据模型奠定坚实的基础。一个设计良好的数据模型能让您释放数据的潜力，获取宝贵的洞见，并做出明智的决策，推动业务的增长和成功。
- en: Identify the Business Processes
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别业务流程
- en: In the search to develop an effective analytical data model, the initial phase
    is to identify the business processes within the organization. After engaging
    in discussions with key stakeholders and conducting in-depth interviews, it becomes
    clear that one of the primary objectives is to track sales performance across
    channels. This critical piece of information will provide valuable insights into
    revenue generation and the effectiveness of various sales channels.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻求开发有效的分析数据模型过程中，首要阶段是识别组织内的业务流程。经过与关键利益相关者的讨论和深入面试后，明确了主要目标之一是跟踪各渠道的销售表现。这一关键信息将为收入生成和各种销售渠道的有效性提供宝贵的洞见。
- en: 'Exploring the organization’s goals, another significant requirement is also
    discovered: tracking visits and bounce rates per channel. This objective aims
    to shed light on customer engagement and website performance across channels.
    By measuring the number of visits and bounce rates, the organization could identify
    which channels are driving traffic, as well as areas for improvement to reduce
    bounce rates and increase user engagement.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 探索组织的目标时，另一个重要需求也显现出来：跟踪各渠道的访问量和跳出率。这一目标旨在揭示跨渠道的客户参与和网站性能。通过测量访问量和跳出率，组织可以确定哪些渠道带来了流量，以及改进的空间，以减少跳出率并增加用户参与度。
- en: 'Understanding the importance of these metrics, we recognize a need to focus
    on two distinct business processes: sales tracking and website performance analysis.
    The sales tracking process would capture and analyze sales data generated through
    various channels, such as mobile, mobile app, or Instagram. This process would
    provide a comprehensive view of sales performance, enabling the organization to
    make data-driven decisions regarding channel optimization and sales forecasting.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些指标的重要性，我们意识到需要专注于两个不同的业务流程：销售跟踪和网站性能分析。销售跟踪流程将捕获并分析通过各种渠道产生的销售数据，例如移动端、移动应用或Instagram。这一过程将提供销售表现的全面视角，使组织能够基于数据做出关于渠道优化和销售预测的决策。
- en: Simultaneously, the website performance analysis process would gather data on
    website visits and bounce rates across various channels. This would require implementing
    robust tracking mechanisms, such as web analytics tools, to monitor and measure
    user behavior on the organization’s website. By examining channel-specific visit
    patterns and bounce rates, the organization could identify trends, optimize user
    experience, and enhance website performance to improve overall customer engagement.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，网站性能分析过程将收集关于网站访问和跳出率的数据，跨多个渠道进行分析。这将需要实施强大的跟踪机制，如网络分析工具，以监测和衡量用户在组织网站上的行为。通过检查特定渠道的访问模式和跳出率，组织可以识别趋势，优化用户体验，并提升网站性能，从而提高整体客户参与度。
- en: Thus, identifying these two vital business processes—sales tracking and website
    performance analysis—emerged as a key milestone in the analytical data modeling
    journey. With this knowledge, we will become adequately equipped to proceed to
    the next phase, where we will dive deeper into understanding the data flows, interdependencies,
    and specific data points associated with these processes, shaping a comprehensive
    dimensional data model that aligns with the organization’s objectives and requirements.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在分析数据建模过程中，识别这两个关键的业务流程——销售追踪和网站性能分析——成为一个重要的里程碑。有了这些知识，我们将能够充分准备进入下一阶段，在那里我们将深入了解与这些流程相关的数据流、相互依赖关系和特定数据点，形成一个与组织目标和要求相一致的全面维度数据模型。
- en: Identify Facts and Dimensions in the Dimensional Data Model
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在维度数据模型中识别事实和维度
- en: Based on the identified business processes of sales tracking and website performance
    analysis, we have inferred the need for four dimensions and two corresponding
    fact tables. Let’s detail each of them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于销售追踪和网站性能分析的业务流程，我们推断需要四个维度和两个相应的事实表。让我们详细描述每一个。
- en: The first dimension is channels (`dim_channels`). This dimension represents
    the various sales and marketing channels through which the organization operates.
    The common channels identified are website, Instagram, and mobile app channels.
    By analyzing sales data across channels, the organization gains insights into
    the performance and effectiveness of each channel in generating revenue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个维度是渠道（`dim_channels`）。这个维度代表组织经营的各种销售和营销渠道。确定的常见渠道包括网站、Instagram 和移动应用渠道。通过跨渠道分析销售数据，组织可以洞察每个渠道在产生收入方面的表现和效果。
- en: The second dimension is products (`dim_products`). This dimension focuses on
    the organization’s product offerings. By including the product dimension, the
    organization is able to analyze sales patterns across product categories and identify
    top-selling products or areas for improvement.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个维度是产品（`dim_products`）。这个维度关注组织的产品提供。通过包含产品维度，组织能够分析产品类别中的销售模式，并识别畅销产品或需要改进的领域。
- en: The third dimension is customers (`dim_customers`). This dimension captures
    information about the organization’s customer base. The organization can gain
    insights into customer preferences, behavior, and purchasing patterns by analyzing
    sales data based on customer attributes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个维度是客户（`dim_customers`）。该维度捕获了关于组织客户群体的信息。通过分析基于客户属性的销售数据，组织可以洞察客户偏好、行为和购买模式。
- en: The fourth and final dimension is the date (`dim_date`). This dimension allows
    for the analysis of sales and website performance over time. Analyzing data based
    on the date dimension allows the organization to identify trends, seasonality,
    and any temporal patterns that may impact sales or website performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个也是最后一个维度是日期（`dim_date`）。这个维度允许对销售和网站性能进行时间分析。基于日期维度的数据分析允许组织识别趋势、季节性以及可能影响销售或网站性能的任何时间模式。
- en: Now let’s move on to the fact tables. The first fact table identified is the
    purchase history (`fct_purchase_history`). This table serves as the central point
    where the purchase transactions are captured and associated with the relevant
    dimensions—channels, products, customers, and date. It allows for detailed sales
    data analysis, enabling the organization to understand the correlation between
    sales and the dimensions. With the purchase history fact table, the organization
    gains insights into sales performance across channels, product categories, customer
    segments, and time periods.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向事实表。第一个确定的事实表是购买历史（`fct_purchase_history`）。这张表是捕获购买交易并与相关维度——渠道、产品、客户和日期——关联的中心点。它允许详细的销售数据分析，使组织能够了解销售与维度之间的相关性。通过购买历史事实表，组织可以深入了解跨渠道、产品类别、客户细分和时间段的销售表现。
- en: The second fact table is the visits history table (`fct_visit_history`). Unlike
    the purchase history, this table focuses on website performance analysis. It captures
    data related to website visits and is primarily associated with the dimensions
    of channels, customers, and date. By analyzing visit history, the organization
    can understand customer engagement, track traffic patterns across channels, and
    measure the effectiveness of different marketing campaigns or website features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个事实表是访问历史表（`fct_visit_history`）。与购买历史不同，这张表专注于网站性能分析。它捕获与网站访问相关的数据，主要与渠道、客户和日期等维度相关联。通过分析访问历史，组织可以了解客户参与度，跟踪跨渠道的流量模式，以及衡量不同营销活动或网站功能的有效性。
- en: With these dimensions and fact tables identified, we have established the foundation
    for your dimensional data model. This model allows you to efficiently analyze
    and derive insights from sales data across various dimensions, as well as track
    website performance metrics associated with different channels, customers, and
    time periods. As you proceed with the data modeling process, we will further refine
    and define the attributes within each dimension and establish the relationships
    and hierarchies necessary for comprehensive analysis, but for now, we already
    have conditions to design our star schema ([Figure 6-5](#ch06_use_case_star_schema)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些确定的维度和事实表，我们已经为您的维度数据模型奠定了基础。这个模型使您能够有效分析和从各个维度的销售数据中获取洞察，以及跟踪与不同渠道、客户和时间段相关的网站性能指标。随着数据建模过程的进行，我们将进一步完善和定义每个维度内的属性，并建立必要的关系和层次结构，以进行全面分析，但目前，我们已经具备条件来设计我们的星型模式（[图 6-5](#ch06_use_case_star_schema)）。
- en: '![ch06_star_schema_omnichannel](assets/aesd_0605.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_star_schema_omnichannel](assets/aesd_0605.png)'
- en: Figure 6-5\. Use case star schema model
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 使用案例星型模式
- en: 'The star schema incorporates four primary dimensions: channels, products, customers,
    and date. These dimensions serve as the pillars of analysis, providing valuable
    context to the data.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式包含四个主要维度：渠道、产品、客户和日期。这些维度作为分析的支柱，为数据提供了宝贵的背景信息。
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Three of the dimensions—`dim_channels`, `dim_customers`, and `dim_date`—are
    conformed dimensions. *Conformed dimensions* are shared across multiple fact tables,
    ensuring consistency and facilitating seamless integration among various analytical
    perspectives.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 三个维度——`dim_channels`、`dim_customers` 和 `dim_date`——是符合维度。*符合维度* 被多个事实表共享，确保一致性，并促进在多种分析视角之间的无缝集成。
- en: Identify the Attributes for Dimensions
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别维度的属性
- en: With the dimensions identified, it is time to detail each identified attribute
    within the dimension. The data model gains depth and completeness by incorporating
    these attributes with their respective dimensions. These attributes enrich the
    analysis and enable more granular insights, empowering decision-makers to derive
    valuable information from the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了维度后，现在是详细说明每个维度内识别的属性的时候了。通过将这些属性与各自的维度结合起来，数据模型变得更加深入和完整。这些属性丰富了分析，使决策者能够从数据中获得更加精细的洞察。
- en: When considering the channels dimension (`dim_channels`), several attributes
    were identified. First, the channel surrogate key (`sk_channel`) provides a unique
    identifier within the data model for each channel. Alongside it, the channel natural
    key (`nk_channel_id`), which represents the key from the source system, ensures
    seamless integration with external data sources. Additionally, the channel name
    attribute (`dsc_channel_name`) captures the descriptive name of each channel,
    enabling easy identification and understanding within the data model. This last
    one is potentially the most interesting for analytics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑渠道维度（`dim_channels`）时，识别了多个属性。首先，渠道代理键（`sk_channel`）为数据模型内每个渠道提供了唯一标识符。与之配套的渠道自然键（`nk_channel_id`）代表了源系统的键，确保与外部数据源的无缝集成。此外，渠道名称属性（`dsc_channel_name`）捕获了每个渠道的描述性名称，在数据模型内易于识别和理解。这可能是最令人感兴趣的分析部分。
- en: Moving on to the products dimension (`dim_products`), multiple key attributes
    were identified. The product surrogate key (`sk_product`) serves as a unique identifier
    for each product within the data model. Similarly, the product natural key (`n⁠k⁠_​p⁠r⁠o⁠d⁠u⁠c⁠t⁠_sku`)
    captures the key from the source system, allowing for consistent linking of product-related
    data. The product name attribute (`dsc_product_name`) provides a descriptive name
    for each product, helping with clarity and comprehension. Finally, the product
    unit price attribute (`mtr_unit_price`) records the price of each product, facilitating
    price analysis and revenue calculations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 转向产品维度（`dim_products`），识别了多个关键属性。产品代理键（`sk_product`）作为数据模型内每个产品的唯一标识符。类似地，产品自然键（`n⁠k⁠_​p⁠r⁠o⁠d⁠u⁠c⁠t⁠_sku`）捕获了来自源系统的键，确保产品相关数据的一致链接。产品名称属性（`dsc_product_name`）为每个产品提供了描述性名称，有助于清晰理解。最后，产品单价属性（`mtr_unit_price`）记录了每个产品的价格，促进了价格分析和收入计算。
- en: In the customers dimension (`dim_customers`), different attributes help provide
    a wide view of customer-related information. The customer surrogate key (`sk_customer`)
    is a unique identifier for each customer within the data model. The customer natural
    key (`nk_customer_id`) keeps the key from the source system, allowing seamless
    integration with external data sources. Additionally, attributes such as customer
    name (`dsc_name`), birth date (`dt_date_birth`), email address (`dsc_email_address`),
    phone number (`dsc_phone_number`), and country (`dsc_country`) capture important
    details related to individual customers. These attributes enable customer segmentation,
    personalized marketing, and in-depth customer behavior and demographics analysis.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户维度（`dim_customers`）中，不同的属性帮助提供了客户相关信息的广泛视角。客户代理键（`sk_customer`）是数据模型内每位客户的唯一标识符。客户自然键（`nk_customer_id`）保留了来自源系统的键，允许与外部数据源无缝集成。此外，客户姓名（`dsc_name`）、出生日期（`dt_date_birth`）、电子邮件地址（`dsc_email_address`）、电话号码（`dsc_phone_number`）和国家（`dsc_country`）等属性捕获了与个体客户相关的重要细节。这些属性使得客户分割、个性化营销以及深入的客户行为和人口统计分析成为可能。
- en: Finally, the date dimension (`dim_date`) includes a range of date-related attributes.
    These attributes enhance the understanding and analysis of temporal data. The
    date attribute itself captures the specific date. Attributes such as month, quarter,
    and year provide higher-level temporal information, facilitating aggregated analysis.
    By including these attributes, the data model enables comprehensive time-based
    analysis and pattern recognition.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，日期维度（`dim_date`）包括一系列与日期相关的属性。这些属性增强了对时间数据的理解和分析。日期属性本身捕获特定的日期。诸如月份、季度和年份等属性提供了更高级别的时间信息，促进了聚合分析。通过包含这些属性，数据模型实现了全面的基于时间的分析和模式识别。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '*Surrogate keys* are artificial identifiers assigned to records within a database
    table. They provide uniqueness, stability, and improved performance in data operations.
    Surrogate keys are independent of the data itself, ensuring that each record has
    a unique identifier and remains stable even if natural key values change. They
    simplify joins between tables, enhance data integration, and facilitate efficient
    indexing and querying.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*代理键* 是分配给数据库表中记录的人工标识符。它们提供了唯一性、稳定性和数据操作性能的提升。代理键独立于数据本身，确保每条记录都有一个唯一标识符，即使自然键值发生变化，也能保持稳定。它们简化了表之间的连接，增强了数据集成能力，并促进了高效的索引和查询。'
- en: Define the Granularity for Business Facts
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为业务事实定义粒度
- en: Having completed the earlier phases of analytical data modeling, we now move
    on to the last vital step, namely, identifying the granularity of our future business
    facts. The granularity refers to the level of detail at which data is captured
    and analyzed within the dimensional data model. Determining the appropriate granularity
    is essential to ensuring that the data model effectively supports the analytical
    requirements and objectives of the organization.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了分析数据建模的早期阶段后，我们现在转向最后一个关键步骤，即确定未来业务事实的细粒度。细粒度指的是在维度数据模型内捕获和分析数据的详细程度。确定适当的细粒度对于确保数据模型有效地支持组织的分析需求和目标至关重要。
- en: To define the granularity of our business facts, it is necessary to consider
    the specific needs of the analysis and strike a balance between capturing sufficient
    detail and avoiding excessive complexity. The chosen granularity should provide
    enough information for meaningful analysis while maintaining data manageability
    and performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义我们业务事实的细粒度，需要考虑分析的具体需求，并在捕获足够详细信息和避免过度复杂化之间取得平衡。选择的细粒度应提供足够的信息进行有意义的分析，同时保持数据的可管理性和性能。
- en: 'In the context of sales data, the granularity identified is determined at the
    transaction level, capturing individual customer purchases: `fct_purchase_history`.
    This level of granularity allows for detailed analysis of sales patterns, such
    as examining individual transactions, identifying trends in customer behavior,
    and conducting product-level analysis.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在销售数据的背景下，确定的细粒度是在交易级别确定的，捕获个别客户的购买情况：`fct_purchase_history`。这种细粒度允许对销售模式进行详细分析，如检查个别交易、识别客户行为趋势和进行产品级分析。
- en: 'For the other requirement, website performance analysis, the granularity is
    selected at the visit level, gathering individual customer visits and the channel
    from which they came into the platform: `fct_visit_history`. With this detail,
    the organization can understand customer engagement, track traffic patterns across
    channels, and measure the effectiveness of distinct marketing campaigns or website
    features.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他需求，网站性能分析，细粒度选定为访问级别，收集个别客户的访问以及他们进入平台的渠道：`fct_visit_history`。通过这些细节，组织可以了解客户参与度，跟踪跨渠道的流量模式，并衡量独特营销活动或网站功能的有效性。
- en: Alternatively, with the level of granularities defined, other units of analysis
    that are less granular can be determined, such as daily, weekly, or monthly aggregates.
    Aggregating the data allows for a more concise representation while providing
    valuable insights. This approach reduces data volume and simplifies analysis,
    making identifying broader trends, seasonal patterns, or overall performance across
    multiple dimensions easier.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以确定其他较少细粒度的分析单位，例如日常、每周或每月的聚合。对数据进行聚合可以提供更简洁的表达方式，同时提供有价值的见解。这种方法可以减少数据量并简化分析，使跨多个维度识别更广泛趋势、季节性模式或整体绩效更容易。
- en: By carefully defining the granularity of our business facts, the organization
    can ensure that the dimensional data model hits the right balance between capturing
    sufficient detail and maintaining data manageability and performance. This step
    sets the stage for meaningful analysis, enabling stakeholders to derive valuable
    insights and make informed decisions based on the data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精确定义我们业务事实的细粒度，组织可以确保维度数据模型在捕获足够详细信息的同时保持数据的可管理性和性能。这一步骤为有意义的分析奠定了基础，使利益相关者能够从数据中获取有价值的见解，并基于数据做出明智的决策。
- en: As we conclude this phase, we have successfully navigated the essential stages
    of analytical data modeling, including identifying business processes, facts,
    dimensions, and attributes, as well as defining the granularity of business facts.
    These foundational steps provide a solid framework for developing a comprehensive
    and effective dimensional data model that empowers data-driven decision making
    within the organization. In the next section, we will get our hands dirty and
    develop our models using dbt, but always with the foundation defined by analytical
    data modeling.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这一阶段时，我们已成功地完成了分析数据建模的关键阶段，包括识别业务流程、事实、维度和属性，并定义了业务事实的细粒度。这些基础步骤为开发全面有效的维度数据模型提供了坚实的框架，支持组织内基于数据的决策。
- en: Creating Our Data Warehouse with dbt
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过dbt创建我们的数据仓库
- en: With the analytical data modeling phase complete, it is time to venture into
    developing our data warehouse. The data warehouse serves as the central repository
    for structured and integrated data, supporting robust reporting, analysis, and
    decision-making processes within the organization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成分析数据建模阶段后，现在是时候开始开发我们的数据仓库了。数据仓库作为结构化和集成数据的中心存储库，支持组织内强大的报告、分析和决策过程。
- en: Overall, data warehouse development begins by establishing the necessary infrastructure.
    Long story short, we already did that earlier, in [“High-Level Data Architecture”](#ch06_high_level_architecture),
    by setting up our BigQuery. At this stage, we only need to set up our dbt project
    and connect it to BigQuery and GitHub. In [“Setting Up dbt Cloud with BigQuery
    and GitHub”](ch04.html#setting_up_dbt_cloud), we present a comprehensive step-by-step
    guide explaining how to do all the initial setup, so we will skip this phase in
    this section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库开发的整体目标是通过建立必要的基础设施来开始。简而言之，在[“高级数据架构”](#ch06_high_level_architecture)中，我们已经通过建立我们的BigQuery完成了这一点。在这个阶段，我们只需要设置我们的dbt项目并将其连接到BigQuery和GitHub。在[“使用BigQuery和GitHub设置dbt
    Cloud”](ch04.html#setting_up_dbt_cloud)中，我们提供了一个全面的逐步指南，说明如何进行所有初始设置，因此我们将在本节中跳过此阶段。
- en: 'Our main goal in this section is to develop all the dbt models crafted during
    the analytical data modeling phase, which serves as the blueprint for the design
    and construction of our data warehouse. In parallel with the models, we will also
    develop all the parametrization YAML files to ensure we leverage the `ref()` and
    `source()` functions and ultimately make our code DRY-er. In line with the stated
    goal, another step needs to be performed along with developing the YAML files:
    building our staging models area. These will be the seed for our dimensions and
    facts.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的主要目标是开发在分析数据建模阶段制定的所有dbt模型，这些模型作为设计和构建数据仓库的蓝图。与模型同时进行的是，我们还将开发所有参数化的YAML文件，以确保我们利用`ref()`和`source()`函数，最终使我们的代码更加DRY。与开发YAML文件的目标一致，还需要执行另一个步骤：构建我们的分阶段模型区域。这些将是我们维度和事实的种子。
- en: In addition to developing the data models, it is essential to establish consistent
    naming conventions within the data warehouse. These naming conventions provide
    a standardized approach to naming tables, columns, and other database objects,
    ensuring clarity and consistency across the data infrastructure. [Table 6-1](#dbt_use_case_naming_conventions)
    presents the naming conventions used to build the data warehouse with dbt.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开发数据模型之外，建立数据仓库内部的命名约定也是至关重要的。这些命名约定提供了一种标准化的方法来命名表、列和其他数据库对象，确保数据基础设施的清晰度和一致性。[表 6-1](#dbt_use_case_naming_conventions)展示了用于构建数据仓库的命名约定。
- en: Table 6-1\. Naming conventions
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. 命名约定
- en: '| Convention | Field type | Description |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 约定 | 字段类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| stg | Table/CTE | Staging tables or CTE |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| stg | Table/CTE | 分阶段表或CTE |'
- en: '| dim | Table | Dimension tables |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| dim | Table | 维度表 |'
- en: '| fct | Table | Fact tables |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| fct | Table | 事实表 |'
- en: '| nk | Column | Natural keys |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| nk | Column | 自然键 |'
- en: '| sk | Column | Surrogate keys |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| sk | Column | 代理键 |'
- en: '| mtr | Column | Metric columns (numeric values) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| mtr | Column | 指标列（数值） |'
- en: '| dsc | Column | Description columns (text values) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| dsc | Column | 描述列（文本值） |'
- en: '| dt | Column | Date and time columns |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| dt | Column | 日期和时间列 |'
- en: To build our first models, we must ensure that our dbt project is set up and
    that we have the proper folder structure. During this part of the use case, we
    will keep it simple and build only *staging* and *marts* directories. So once
    you have your dbt project initialized, create the specified folders. The models
    folders directory should look like [Example 6-9](#dbt_use_case_models_dir).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的第一个模型，我们必须确保我们的dbt项目已设置好，并且具备适当的文件夹结构。在此使用案例的这一部分中，我们将保持简单，仅构建*分阶段*和*marts*目录。因此，一旦初始化您的dbt项目，创建指定的文件夹。模型文件夹的目录应如[示例 6-9](#dbt_use_case_models_dir)。
- en: Example 6-9\. Omnichannel data warehouse, models directory
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-9\. 全渠道数据仓库，模型目录
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have built our initial project and folders, the next step is creating
    our staging YAML files. As per the segregation of YAML files best practices we
    discussed in [“YAML Files”](ch04.html#structure_of_dbt_chapter_yaml), we will
    have one YAML file for sources and another for models. For building our staging
    layer, let’s, for now, focus only on our `source` YAML file. This file must be
    inside the *staging* directory and should look like [Example 6-10](#dbt_use_case_staging_yml_source).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了初始项目和文件夹，下一步是创建我们的分期 YAML 文件。按照我们在 [“YAML Files”](ch04.html#structure_of_dbt_chapter_yaml)
    中讨论的 YAML 文件最佳实践的分离，我们将为源和模型各有一个 YAML 文件。为了构建我们的分期层，我们现在只关注我们的 `source` YAML 文件。该文件必须位于
    *staging* 目录内，看起来应该像 [示例 6-10](#dbt_use_case_staging_yml_source)。
- en: Example 6-10\. _omnichannel_raw_sources.yml file configuration
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-10\. _omnichannel_raw_sources.yml 文件配置
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The use of this file will allow you to leverage the `source()` function to
    work with the raw data available in your data platform. Five tables were specified
    under the `omnichannel_raw` schema: `Channels`, `Customers`, `Products`, `VisitHistory`,
    and `PurchaseHistory`. These correspond to the relevant source tables used to
    make our staging layer, and dbt will interact with these to build the staging
    data models.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个文件将允许你利用 `source()` 函数来处理你数据平台中的原始数据。在 `omnichannel_raw` 模式下指定了五张表：`Channels`、`Customers`、`Products`、`VisitHistory`
    和 `PurchaseHistory`。这些表对应于用来构建我们分期层的相关源表，dbt 将与这些表进行交互以构建分期数据模型。
- en: Let’s kick off construction of our staging models. The primary idea here is
    to have one staging model for each source table—`Channels`, `Customers`, `Products`,
    `VisitHistory`, and `PurchaseHistory`. Keep in mind that each new staging model
    needs to be created inside the *staging* directory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的分期模型。这里的主要想法是为每个源表创建一个分期模型 — `Channels`、`Customers`、`Products`、`VisitHistory`
    和 `PurchaseHistory`。请记住，每个新的分期模型都需要在 *staging* 目录内创建。
- en: Examples [6-11](#dbt_use_case_stg_channels) through [6-15](#dbt_use_case_visit_history)
    show the code snippets to build each one of our staging models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [6-11](#dbt_use_case_stg_channels) 至 [6-15](#dbt_use_case_visit_history)
    展示了构建每个分期模型的代码片段。
- en: Example 6-11\. `stg_channels`
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-11\. `stg_channels`
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Example 6-12\. `stg_customers`
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-12\. `stg_customers`
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 6-13\. `stg_products`
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-13\. `stg_products`
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Example 6-14\. `stg_purchase_history`
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-14\. `stg_purchase_history`
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Example 6-15\. `stg_visit_history`
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-15\. `stg_visit_history`
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In summary, each of these dbt models extracts data from the respective source
    tables and stages it in separate CTEs. These staging tables serve as intermediate
    storage for further data transformations before loading the data into the final
    destination tables of the data warehouse.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，每个 dbt 模型从相应的源表中提取数据，并在单独的 CTE 中进行分期存储。这些分期表在加载数据到数据仓库的最终目标表之前，作为进一步数据转换的中间存储。
- en: After successfully creating the staging models, the next phase is to set the
    YAML file for the staging layer. The staging layer YAML file will serve as a configuration
    file that references the staging models and specifies their execution order and
    dependencies. This file provides a clear and structured view of the staging layer’s
    setup, allowing for consistent integration and management of the staging models
    within the overall data modeling process. [Example 6-16](#dbt_use_case_staging_yml_models)
    shows how the YAML file should look in your staging layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 成功创建分期模型后，下一阶段是为分期层设置 YAML 文件。分期层的 YAML 文件将作为一个配置文件，引用分期模型并指定它们的执行顺序和依赖关系。该文件清晰地展示了分期层设置的结构化视图，允许在整体数据建模过程中一致地集成和管理分期模型。[示例
    6-16](#dbt_use_case_staging_yml_models) 展示了分期层中 YAML 文件的样式。
- en: Example 6-16\. _omnichannel_raw_models.yml file configuration
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-16\. _omnichannel_raw_models.yml 文件配置
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Once the staging layer YAML file is in place, it’s time to move forward with
    building the dimension models. *Dimension models* are an essential component of
    the data warehouse, representing the business entities and their attributes. These
    models capture the descriptive information that provides context to the fact data
    and allows for deeper analysis. Dimension tables, such as channels, products,
    customers, and date, will be constructed based on the previously defined dimensions
    and their attributes, derived from [“Identify Facts and Dimensions in the Dimensional
    Data Model”](#ch06_facts_and_dimensions_dim_model) and [“Identify the Attributes
    for Dimensions”](#ch06_dimensions_attributes). These tables are populated with
    the relevant data from the staging layer, which ensures consistency and accuracy.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分段层 YAML 文件就位，就是前进构建维度模型的时候了。*维度模型*是数据仓库的一个重要组成部分，代表业务实体及其属性。这些模型捕捉提供上下文给事实数据并允许深入分析的描述性信息。维度表，如渠道、产品、客户和日期，将根据之前定义的维度和它们的属性构建，这些信息来自于[“在维度数据模型中识别事实和维度”](#ch06_facts_and_dimensions_dim_model)和[“识别维度的属性”](#ch06_dimensions_attributes)。这些表将使用来自分段层的相关数据填充，以确保一致性和准确性。
- en: Let’s proceed with our dimension model creation. Create the respective models
    in Examples [6-17](#dbt_use_case_dim_channels) through [6-20](#dbt_use_case_dim_date)
    in your *marts* directory.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的维度模型创建。在*marts*目录中创建示例[6-17](#dbt_use_case_dim_channels)到[6-20](#dbt_use_case_dim_date)中的相应模型。
- en: Example 6-17\. `dim_channels`
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-17\. `dim_channels`
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Example 6-18\. `dim_customers`
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-18\. `dim_customers`
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Example 6-19\. `dim_products`
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-19\. `dim_products`
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Example 6-20\. `dim_date`
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-20\. `dim_date`
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In summary, each code block defines a dbt model for a specific dimension table.
    The first three models, `dim_channels`, `dim_customers`, and `dim_products`, retrieve
    data from the corresponding staging tables and transform it into the desired structure
    for the dimension tables. In each one of the models, we’ve included the surrogate
    keys generation from the natural key. To do that, we’ve resorted to the `dbt_utils`
    package, specifically the `generate_surrogate_key()` function. This function takes
    an array of column names as an argument, representing the dimension table’s natural
    keys (or business keys), and generates a surrogate key column based on those columns.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，每个代码块定义了一个特定维度表的 dbt 模型。前三个模型，`dim_channels`、`dim_customers` 和 `dim_products`，从对应的分段表中检索数据，并将其转换为维度表的所需结构。在每一个模型中，我们都包括了从自然键生成伪码的步骤。为此，我们借助了
    `dbt_utils` 包，具体来说是 `generate_surrogate_key()` 函数。该函数接受一个列名数组作为参数，表示维度表的自然键（或业务键），并基于这些列生成一个伪码列。
- en: The last dimension, `dim_date`, is different since it doesn’t arrive from the
    staging layer. Instead, it is entirely produced using the `get_date_dimension()`
    function from the `dbt_date` package. The `get_date_dimension()` function handles
    the generation of the date dimension table, including creating all the necessary
    columns and the data for each based on the specified date range. In our case,
    we choose the data range from 2022-01-01 to 2024-12-31.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个维度，`dim_date`，与其他不同，因为它并不来自分段层。相反，它完全使用 `dbt_date` 包中的 `get_date_dimension()`
    函数生成。`get_date_dimension()` 函数负责生成日期维度表，包括创建所有必要列和根据指定日期范围为每个列填充数据。在我们的案例中，我们选择的日期范围是从
    2022-01-01 到 2024-12-31。
- en: Finally, remember that we are now using packages. To successfully build our
    project at this stage, we need to install them, so add the [Example 6-21](#dbt_use_case_packages_yml)
    configurations to your *dbt_packages.yml* file. Then execute **`dbt deps`** and
    **`dbt build`** commands and look at your data platform to check that we have
    the new dimensions created.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住我们现在在使用包。为了顺利在这个阶段构建项目，我们需要安装它们，所以将[示例 6-21](#dbt_use_case_packages_yml)配置添加到你的*dbt_packages.yml*文件中。然后执行**`dbt
    deps`**和**`dbt build`**命令，查看你的数据平台，确认我们已经创建了新的维度。
- en: Example 6-21\. packages.yml file configuration
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-21\. packages.yml 文件配置
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The final step is the creation of the models from fact tables that were earlier
    identified as necessary to analyze the new business processes. These tables are
    an integral part of a data warehouse and represent the measurable, numerical data
    that captures the business events or transactions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是根据早前确定需要分析新业务流程的事实表创建模型。这些表是数据仓库的一个组成部分，代表捕捉业务事件或交易的可度量、数值数据。
- en: Examples [6-22](#dbt_use_case_fct_purchase_history) and [6-23](#dbt_use_case_fct_visit_history)
    represent the new fact tables to be developed. Create them inside the *marts*
    directory.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 示例[6-22](#dbt_use_case_fct_purchase_history)和[6-23](#dbt_use_case_fct_visit_history)代表了需要开发的新事实表。请在*marts*目录内创建它们。
- en: Example 6-22\. `fct_purchase_history`
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-22\. `fct_purchase_history`
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`fct_purchase_history` aims to answer to the first business process identified,
    namely, to track sales performance across channels. Next, we gather sales data
    from the `stg_purchase_history` model and join it with the respective channel,
    customer, and product dimensions to capture the respective surrogate key, using
    the `COALESCE()` function to handle cases where the natural key does not match
    a dimension table entry. By including the relationship between this fact and the
    respective dimensions, the organization would be equipped with valuable insights
    from revenue generation and the effectiveness of various sales channels per customer
    and product.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`fct_purchase_history`旨在回答首个识别的业务流程，即跟踪跨渠道的销售业绩。接下来，我们从`stg_purchase_history`模型中收集销售数据，并将其与相应的渠道、客户和产品维度结合，以捕获相应的代理键，使用`COALESCE()`函数处理自然键与维度表条目不匹配的情况。通过包括此事实与相应维度之间的关系，组织将能够从收入生成和各销售渠道在客户和产品方面的效果中获得有价值的见解。'
- en: To fully meet the requirements, two additional calculated metrics, `m⁠t⁠r⁠_⁠t⁠o⁠t⁠a⁠⁠l​_⁠a⁠m⁠o⁠u⁠n⁠t⁠_⁠g⁠r⁠o⁠s⁠s`
    and `mtr_total_amount_net`, are computed based on the product quantity bought
    (`mtr_quantity`), the unit price of each product (`mtr_unit_price`), and the applied
    discount (`mtr_discount`).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全满足要求，还需计算两个额外的计算指标，即`m⁠t⁠r⁠_⁠t⁠o⁠t⁠a⁠⁠l​_⁠a⁠m⁠o⁠u⁠n⁠t⁠_⁠g⁠r⁠o⁠s⁠s`和`mtr_total_amount_net`，它们基于购买的产品数量(`mtr_quantity`)、每个产品的单价(`mtr_unit_price`)以及应用的折扣(`mtr_discount`)计算而得。
- en: In summary, [Example 6-22](#dbt_use_case_fct_purchase_history) demonstrates
    the process of transforming the staging data into a structured fact table that
    captures relevant purchase history information. By joining dimension tables and
    performing calculations, the fact table provides a consolidated view of purchase
    data, enabling valuable insights and analysis.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，[Example 6-22](#dbt_use_case_fct_purchase_history)展示了将分期数据转换为捕获相关购买历史信息的结构化事实表的过程。通过联接维度表并执行计算，事实表提供了购买数据的综合视图，从而实现有价值的见解和分析。
- en: Moving to the last fact table, let’s have a look at [Example 6-23](#dbt_use_case_fct_visit_history).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 转向最后一个事实表，让我们看看[Example 6-23](#dbt_use_case_fct_visit_history)。
- en: Example 6-23\. `fct_visit_history`
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-23\. `fct_visit_history`
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`fct_visit_history` answers to the other business process identified: tracking
    visits and bounce rates per channel to shed light on customer engagement and website
    performance across channels. To create it, we gather visit data from the `stg_v⁠i⁠s⁠i⁠t​_h⁠i⁠s⁠t⁠o⁠r⁠y`
    model and join it with the customers and channels dimensions to catch the respective
    surrogate key, using the `COALESCE()` function to handle cases where the natural
    key does not match with a dimension table entry. With this fact and dimension
    relationship established, the organization will be able to identify the channels
    that are driving more traffic. An additional calculated metric was added, `mtr_length_of_stay_minutes`,
    to understand the length of stay of a particular visit. This calculated metric
    leverages the `DATE_DIFF()` function to compute the difference between the bounce
    and visit date, aiming to support the organization in identifying areas for improvement
    to reduce bounce rates and increase user engagement.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`fct_visit_history`回答了另一个识别的业务流程：跟踪每个渠道的访问和跳出率，以揭示客户参与和网站性能的情况。为了创建它，我们从`stg_v⁠i⁠s⁠i⁠t​_h⁠i⁠s⁠t⁠o⁠r⁠y`模型中收集访问数据，并将其与客户和渠道维度结合，以获取相应的代理键，使用`COALESCE()`函数处理自然键与维度表条目不匹配的情况。通过建立这种事实与维度的关系，组织将能够确定推动更多流量的渠道。还添加了一个额外的计算指标`mtr_length_of_stay_minutes`，以了解特定访问的停留时间。此计算指标利用`DATE_DIFF()`函数计算跳出和访问日期之间的差异，旨在支持组织识别改进的领域，以减少跳出率并增加用户参与度。'
- en: In conclusion, the `fct_visit_history` fact table transforms the staging data
    into a structured fact table that captures relevant visit history information.
    By joining with dimension tables and performing calculations, as we did for both
    facts, the `fct_visit_history` table provides a compact view of visit data, enabling
    valuable insights and analysis.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`fct_visit_history` 事实表将分期数据转换为一个结构化的事实表，记录了相关的访问历史信息。通过与维度表的连接和计算，就像我们为两个事实表所做的那样，`fct_visit_history`
    表提供了访问数据的紧凑视图，能够提供有价值的洞察和分析。
- en: In the next section, we will continue our journey and develop tests and documentation
    and finally deploy to production. These will be the last steps inside dbt, aiming
    to guarantee the data model’s reliability, usability, and ongoing availability
    and support data-driven decision making within the organization.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将继续我们的旅程，开发测试和文档，并最终部署到生产环境。这些将是 dbt 内的最后步骤，旨在确保数据模型的可靠性、可用性，并支持组织内基于数据的决策。
- en: Tests, Documentation, and Deployment with dbt
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 dbt 进行测试、文档编制和部署
- en: As the development of our data warehouse nears completion, it’s crucial to ensure
    the accuracy, reliability, and usability of the implemented data models. This
    section focuses on testing, documentation, and going live with your data warehouse.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们数据仓库开发的完成，确保实施的数据模型的准确性、可靠性和可用性至关重要。本节侧重于测试、文档编制以及数据仓库的投入使用。
- en: As previously noted, when using dbt, tests and documentation should be created
    while developing models. We took that approach, but opted to split it into two
    parts for clarity. This division allows for a clearer understanding of what we
    accomplished within dbt for model development, as well as our processes for testing
    and documentation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，在使用 dbt 时，应在开发模型的同时创建测试和文档。我们采取了这种方法，但为了清晰起见，选择将其分为两部分。这种分割允许更清晰地了解我们在
    dbt 中为模型开发所取得的成果，以及我们在测试和文档编制方面的流程。
- en: As a brief summary, testing is essential in validating the data model’s functionality
    and integrity. Testing verifies the relationships between dimension and fact tables,
    checking data consistency, and validating the accuracy of calculated metrics.
    By conducting tests, you can identify and rectify any issues or discrepancies
    in the data, thus ensuring the reliability of your analytical outputs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，测试在验证数据模型功能和完整性方面至关重要。测试验证维度表和事实表之间的关系，检查数据一致性，并验证计算指标的准确性。通过进行测试，您可以识别和纠正数据中的任何问题或不一致性，从而确保分析输出的可靠性。
- en: It is important to perform both singular and generic tests. Singular tests target
    specific aspects of the data models, such as verifying the accuracy of a specific
    metric calculation or validating the relationships between a particular dimension
    and fact table. These tests provide focused insights into individual components
    of the data models.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单一和通用测试都很重要。单一测试针对数据模型的特定方面，例如验证特定指标计算的准确性或验证特定维度与事实表之间的关系。这些测试提供了对数据模型各个组成部分的集中洞察。
- en: On the other hand, generic tests cover a broader range of scenarios and monitor
    the overall behavior of the data models. These tests aim to ensure that the data
    models function correctly across various dimensions, time periods, and user interactions.
    Generic tests help uncover potential issues that might arise during real-world
    usage and provide confidence in the data models’ ability to handle various scenarios.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通用测试涵盖更广泛的场景，并监视数据模型的整体行为。这些测试旨在确保数据模型在各种维度、时间段和用户交互中正确运行。通用测试有助于发现在实际使用过程中可能出现的潜在问题，并提供对数据模型处理各种场景能力的信心。
- en: Simultaneously, documenting the data models and related processes is required
    for knowledge transfer, collaboration, and future maintenance. Documenting the
    data models involves capturing information about the purpose, structure, relationships,
    and assumptions underlying the models. It includes details on the source systems,
    transformation logic, business rules applied, and any other relevant information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，为了知识转移、协作和未来维护的需要，记录数据模型及其相关流程至关重要。记录数据模型涉及捕获关于模型目的、结构、关系和基础假设的信息。这包括源系统的细节、转换逻辑、应用的业务规则以及任何其他相关信息。
- en: To document the data models, it is recommended to update the corresponding YAML
    files with comprehensive explanations and metadata. YAML files serve as a centralized
    location for the configuration and documentation of dbt models, making it easier
    to track changes and understand the purpose and usage of each model. Documenting
    the YAML files ensures that future team members and stakeholders have a clear
    understanding of the data models and can effectively work with them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了记录数据模型，建议更新相应的 YAML 文件，包括详细的说明和元数据。YAML 文件作为配置和 dbt 模型文档的集中位置，使跟踪更改和理解每个模型的目的和用法变得更容易。记录
    YAML 文件确保未来的团队成员和利益相关者清楚地了解数据模型，并能够有效地使用它们。
- en: Once testing and documentation are complete, the final step in this section
    is to prepare for the go-live of your data warehouse. This involves deploying
    the data models to the production environment, ensuring data pipelines are established,
    and setting up scheduled data updates. It’s important to monitor the data warehouse’s
    performance, availability, and data quality during this phase. Conducting thorough
    testing in a production-like environment and obtaining end-user feedback can help
    identify any remaining issues before the data warehouse is fully operational.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦测试和文档编写完成，本节的最后一步是准备您的数据仓库进行上线。这涉及将数据模型部署到生产环境，确保建立数据管道，并设置定期数据更新。在此阶段监视数据仓库的性能、可用性和数据质量非常重要。在数据仓库完全运行之前，通过在类似生产的环境中进行彻底的测试并获得最终用户的反馈，可以帮助识别任何剩余的问题。
- en: Let’s start with the tests. Our first batch of tests will focus on generic tests.
    The first use case is to ensure for all dimensions that the surrogate key is unique
    and doesn’t have null values. For the second use case, we must also grant that
    every surrogate key in a fact table exists in the specified dimension. Let’s first
    create the respective YAML file in the marts layer, using the [Example 6-24](#dbt_use_case_generic_test)
    code block to achieve all we’ve stated.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始测试。我们的第一批测试将专注于通用测试。第一个用例是确保所有维度的代理键是唯一且不为空。对于第二个用例，我们还必须确保事实表中的每个代理键存在于指定的维度中。让我们首先在
    marts 层创建相应的 YAML 文件，使用 [Example 6-24](#dbt_use_case_generic_test) 代码块来实现我们所述的所有内容。
- en: Example 6-24\. _omnichannel_marts.yml file configuration
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-24\. _omnichannel_marts.yml 文件配置
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let’s now execute our **`dbt test`** command to see if all tests executed successfully.
    Everything went well if logs are as shown in [Figure 6-6](#dbt_use_case_generic_test_output).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们执行我们的 **`dbt test`** 命令，看看是否所有测试都成功执行。如果日志如 [图 6-6](#dbt_use_case_generic_test_output)
    所示，一切顺利。
- en: '![c06_use_case_generic_tests](assets/aesd_0606.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![c06_use_case_generic_tests](assets/aesd_0606.png)'
- en: Figure 6-6\. Logs for generic tests
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 通用测试的日志
- en: Now let’s go to our second iteration of tests and develop some singular tests.
    Here we will focus on our fact table metrics. The first singular use case is to
    make sure our `mtr_total_amount_gross` metric from `fct_purchase_history` has
    only positive values. For that, let’s create a new test, *assert_mtr_total_amount_gross_is_positive.sql*,
    in the *tests* folder with the code in [Example 6-25](#dbt_use_case_singular_test_01).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行第二轮测试，并开发一些单独的测试。在这里，我们将专注于事实表的度量。第一个单独用例是确保我们来自 `fct_purchase_history`
    的 `mtr_total_amount_gross` 度量仅具有正值。为此，让我们在 *tests* 文件夹中创建一个新的测试，*assert_mtr_total_amount_gross_is_positive.sql*，其中包含
    [Example 6-25](#dbt_use_case_singular_test_01) 中的代码。
- en: Example 6-25\. assert_mtr_total_amount_gross_is_positive.sql
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-25\. assert_mtr_total_amount_gross_is_positive.sql
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The next test we want to do is to confirm that `mtr_unit_price` is always lower
    or equal to `mtr_total_amount_gross`. Note that the same test could not be applied
    to `mtr_total_amount_net` since a discount was also applied. To develop this test,
    first, create the file *a⁠s⁠s⁠e⁠r⁠t⁠_⁠m⁠t⁠r⁠_⁠u⁠n⁠i⁠t⁠_⁠p⁠r⁠i⁠c⁠e⁠_⁠i⁠s⁠_⁠e⁠q⁠u⁠a⁠l⁠_⁠o⁠r⁠_​l⁠o⁠w⁠e⁠r⁠_⁠t⁠h⁠a⁠n⁠_⁠m⁠t⁠r⁠_⁠t⁠o⁠t⁠a⁠l⁠_⁠a⁠mount⁠_⁠g⁠r⁠o⁠s⁠s⁠.⁠s⁠q⁠l*
    and paste the code in [Example 6-26](#dbt_use_case_singular_test_02).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要进行的测试是确认 `mtr_unit_price` 始终低于或等于 `mtr_total_amount_gross`。请注意，同样的测试无法应用于
    `mtr_total_amount_net`，因为还应用了折扣。为了开发这个测试，首先创建文件 *a⁠s⁠s⁠e⁠r⁠t⁠_⁠m⁠t⁠r⁠_⁠u⁠n⁠i⁠t⁠_⁠p⁠r⁠i⁠c⁠e⁠_⁠i⁠s⁠_⁠e⁠q⁠u⁠a⁠l⁠_⁠o⁠r⁠_​l⁠o⁠w⁠e⁠r⁠_⁠t⁠h⁠a⁠n⁠_⁠m⁠t⁠r⁠_⁠t⁠o⁠t⁠a⁠l⁠_⁠a⁠m⁠o⁠u⁠n⁠t⁠_⁠g⁠r⁠o⁠s⁠s⁠.⁠s⁠q⁠l*，并粘贴
    [Example 6-26](#dbt_use_case_singular_test_02) 中的代码。
- en: Example 6-26\. assert_mtr_unit_price_is_equal_or_lower_than_mtr_total_amount_gross.sql
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-26\. assert_mtr_unit_price_is_equal_or_lower_than_mtr_total_amount_gross.sql
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With all the singular tests created, we can now execute them and check the output.
    To avoid executing all the tests, including the generic ones, execute the `**dbt
    test --select test_type:singular**` command. This command will execute the tests
    from the type `singular`, ignoring any that are `generic`. [Figure 6-7](#dbt_use_case_singular_test_output)
    shows the expected log output.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所有独立测试创建完成后，我们现在可以执行它们并检查输出。为了避免执行所有测试，包括通用测试，请执行**`dbt test --select test_type:singular`**命令。此命令将执行类型为`singular`的测试，忽略任何`generic`测试。[图 6-7](#dbt_use_case_singular_test_output)显示了预期的日志输出。
- en: '![c06_use_case_singular_tests](assets/aesd_0607.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![c06_use_case_singular_tests](assets/aesd_0607.png)'
- en: Figure 6-7\. Logs for singular tests
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 独立测试的日志
- en: The last singular test we want to do is to confirm that, in `fct_visit_history`,
    the `mtr_length_of_stay_minutes` metric is always positive. This test will tell
    us if we have records with a bouncing date that is earlier than the visit date,
    which can never happen. To perform it, create the *assert_mtr_length_of_stay_is_positive.sql*
    file with the code in [Example 6-27](#dbt_use_case_singular_test_03).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望进行的最后一个独立测试是确认在`fct_visit_history`中，`mtr_length_of_stay_minutes`指标始终为正。这个测试将告诉我们是否有记录的弹跳日期早于访问日期，这是不可能发生的。为了执行这个测试，请创建*assert_mtr_length_of_stay_is_positive.sql*文件，其中包含[示例 6-27](#dbt_use_case_singular_test_03)中的代码。
- en: Example 6-27\. assert_mtr_length_of_stay_is_positive.sql
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-27\. assert_mtr_length_of_stay_is_positive.sql
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: By implementing tests, you can validate the integrity of the data, verify calculations
    and transformations, and ensure compliance to defined business rules. dbt offers
    a complete testing framework that allows you to conduct both singular and generic
    tests, covering various aspects of your data models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施测试，您可以验证数据的完整性，验证计算和转换，并确保符合定义的业务规则。dbt提供了一个完整的测试框架，允许您进行单独和通用测试，涵盖数据模型的各个方面。
- en: 'With all the tests successfully executed, we now move on to the next vital
    aspect of the data warehouse development process: documentation. In dbt, a good
    portion of the documentation is done using the same YAML files we use to configure
    our models or do our tests. Let’s use the marts layer to document all the tables
    and columns. Let’s refer to the *_omnichannel_marts.yml* file and replace it with
    the code in [Example 6-28](#dbt_use_case_documentation). As a note, we document
    only the columns used for the generic tests to make the example cleaner, yet the
    premise is the same for all other columns.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有测试成功执行后，我们现在转向数据仓库开发过程的下一个重要方面：文档编制。在dbt中，大部分文档工作都是使用我们用来配置模型或执行测试的相同YAML文件完成的。让我们使用marts层来记录所有表和列。让我们参考*_omnichannel_marts.yml*文件，并将其替换为[示例 6-28](#dbt_use_case_documentation)中的代码。需要注意的是，我们只记录用于通用测试的列，以使示例更加清晰，但基本原理对所有其他列也适用。
- en: Example 6-28\. _omnichannel_marts.yml file configuration with documentation
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-28\. _omnichannel_marts.yml文件配置及文档
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With the YAML file updated, execute **`dbt docs generate`**, and let’s have
    a look at the new documentation available. For example, if your `fct_purchase_history`
    page looks similar to [Figure 6-8](#dbt_use_case_doc_page), you are good to go.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 更新YAML文件后，执行**`dbt docs generate`**，我们来看看可用的新文档。例如，如果您的`fct_purchase_history`页面类似于[图 6-8](#dbt_use_case_doc_page)，那就准备好了。
- en: '![ch06_use_case_documentation](assets/aesd_0608.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_use_case_documentation](assets/aesd_0608.png)'
- en: Figure 6-8\. `fct_purchase_history` documentation page
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. `fct_purchase_history`文档页面
- en: And that’s it. The final step is deploying what we’ve been doing into the production
    environment. For that, we need to create an environment in dbt, similar to the
    one presented in [Figure 6-9](#dbt_use_case_prod_env).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。最后一步是将我们所做的部署到生产环境中。为此，我们需要在dbt中创建一个类似于[图 6-9](#dbt_use_case_prod_env)中展示的环境。
- en: '![ch06_use_case_env_creation](assets/aesd_0609.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_use_case_env_creation](assets/aesd_0609.png)'
- en: Figure 6-9\. Creating a production environment
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. 创建生产环境
- en: Note that we named our production dataset `omnichannel_analytics`. We’ll use
    this dataset in [“Data Analytics with SQL”](#ch06_analytics_sql). After creating
    the environment, it is time to configure our job. To make it straightforward,
    in Create Job, provide the Job name, set the environment to Production (the one
    that you’ve just created), check the box “Generate docs on run,” and finally,
    in the Commands section, include the command **`dbt test`** below the `dbt build`
    command. Leave all the rest by default.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将生产数据集命名为`omnichannel_analytics`。我们将在[“使用SQL进行数据分析”](#ch06_analytics_sql)中使用此数据集。创建环境后，现在是配置作业的时候。为了简化操作，在创建作业时，请提供作业名称，将环境设置为生产环境（刚刚创建的环境），勾选“运行时生成文档”选项，并最后，在命令部分，包含**`dbt
    build`**命令下方的**`dbt test`**命令。其余选项保持默认。
- en: After the job creation, manually execute the job, and let’s check the logs.
    It is a good indicator if they are similar to [Figure 6-10](#dbt_use_case_job_execution).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业创建后，手动执行该作业，然后检查日志。如果它们类似于[图 6-10](#dbt_use_case_job_execution)，这是一个良好的指标。
- en: '![ch06_use_case_job_run](assets/aesd_0610.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_use_case_job_run](assets/aesd_0610.png)'
- en: Figure 6-10\. Job execution log
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 作业执行日志
- en: Let’s have a look at our data platform, which in our case is BigQuery, and check
    if everything ran successfully. The models in BigQuery should be the ones presented
    in [Figure 6-11](#dbt_use_case_bigquery).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的数据平台，这里我们使用的是BigQuery，并检查一下是否一切运行成功。BigQuery中的模型应该与[图 6-11](#dbt_use_case_bigquery)中展示的模型一致。
- en: '![ch06_use_case_bigquery_models](assets/aesd_0611.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_use_case_bigquery_models](assets/aesd_0611.png)'
- en: Figure 6-11\. Models in BigQuery
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-11\. BigQuery 中的模型
- en: In conclusion, the final part of building a data warehouse focused on testing,
    documentation, and the go-live process. You could ensure your data warehouse’s
    accuracy, reliability, and usability by conducting extensive tests, documenting
    the data models, and preparing for production deployment. In the next section,
    we will dive into data analytics with SQL, taking our data warehouse to the next
    level.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，数据仓库建设的最后一部分专注于测试、文档编制和上线过程。通过进行广泛测试、文档化数据模型，并为生产部署做准备，您可以确保数据仓库的准确性、可靠性和可用性。在下一节中，我们将深入探讨使用SQL进行数据分析，将我们的数据仓库推向新的高度。
- en: Data Analytics with SQL
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SQL进行数据分析
- en: With our star schema model completed, we can now start the analytics discovery
    phase and develop queries to answer specific business questions. As previously
    mentioned, this type of data modeling technique makes it easy to select specific
    metrics from fact tables and enrich them with attributes that come from dimensions.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 完成我们的星型模式模型后，我们现在可以开始分析发现阶段，并开发查询来回答具体的业务问题。正如先前提到的，这种数据建模技术使得可以轻松从事实表中选择特定指标，并丰富来自维度的属性。
- en: In [Example 6-29](#db_da_01), we start by creating a query for “Total amount
    sold per quarter with discount.” In it, we fetch data from two tables, `fct_purchase_history`
    and `dim_date`, and perform calculations on the retrieved data. This query aims
    to obtain information about the sum of total amounts for each quarter of the year.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-29](#db_da_01)中，我们首先创建了一个查询，用于“每个季度销售总额（包含折扣）”。在这个查询中，我们从两个表`fct_purchase_history`和`dim_date`中获取数据，并对检索到的数据进行计算。此查询旨在获取每年每个季度的总金额信息。
- en: Example 6-29\. Total amount sold per quarter with discount
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-29\. 每个季度销售总额（包含折扣）
- en: '[PRE28]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: By analyzing the results of running this query ([Figure 6-12](#ch06_query_1)),
    we could conclude that the second quarter of 2023 was the best, while the worst
    was the first quarter of 2024.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析运行此查询的结果（[图 6-12](#ch06_query_1)），我们可以得出结论：2023年第二季度是最好的，而2024年第一季度是最差的。
- en: '![ch06_query_1](assets/aesd_0612.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_query_1](assets/aesd_0612.png)'
- en: Figure 6-12\. Analytical query for obtaining total amount sold per quarter with
    discount
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-12\. 获取每个季度销售总额（包含折扣）的分析查询
- en: In [Example 6-30](#db_da_02), we calculate the average length of stay (in minutes)
    for each channel by using our star schema model. It selects the channel name (`dc.dsc_channel_name`)
    and the average length of stay in minutes, which is calculated with the function
    `ROUND(AVG(mtr_length_of_stay_minutes),2)`. The `dc.dsc_channel_name` refers to
    the `channel_name` attribute from the `dim_channels` dimension table.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-30](#db_da_02)中，我们通过星型模式模型计算每个频道的平均停留时间（以分钟计）。它选择频道名称（`dc.dsc_channel_name`）和以函数`ROUND(AVG(mtr_length_of_stay_minutes),2)`计算的平均停留时间（分钟）。`dc.dsc_channel_name`指的是来自`dim_channels`维度表的`channel_name`属性。
- en: The `ROUND(AVG(mtr_length_of_stay_minutes),2)` calculates the average length
    of stay in minutes by using the `AVG` function on the `mtr_length_of_stay_minutes`
    column from the `fct_visit_history` fact table. The `ROUND()` function is used
    to round the result to two decimal places. The alias `avg_length_of_stay_minutes`
    is assigned to the calculated average value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`ROUND(AVG(mtr_length_of_stay_minutes),2)`通过在`fct_visit_history`事实表的`mtr_length_of_stay_minutes`列上使用`AVG`函数计算平均逗留时间（以分钟为单位）。`ROUND()`函数用于将结果四舍五入为两位小数。给计算的平均值分配别名`avg_length_of_stay_minutes`。'
- en: Example 6-30\. Average time spent per visit on each channel
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-30\. 每个渠道访问时间的平均值
- en: '[PRE29]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: By analyzing the results of running this query ([Figure 6-13](#ch06_query_2)),
    we could conclude that users spend more time on the website than on the mobile
    app or the company’s Instagram account.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析运行此查询的结果（[Figure 6-13](#ch06_query_2)），我们可以得出结论，用户在网站上花费的时间比在移动应用或公司的Instagram帐户上的时间更多。
- en: '![ch06_query_2](assets/aesd_0613.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_query_2](assets/aesd_0613.png)'
- en: Figure 6-13\. Analytical query for obtaining average time spent per visit on
    each channel
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-13\. 分析查询以获取每个渠道的访问时间平均值
- en: In [Example 6-31](#db_da_03), we take our model to an advanced use case. We
    are now interested in getting the top three products per channel. As we have three
    distinct channels, namely mobile app, website, and Instagram, we are interested
    in obtaining nine rows, three for each of the top three best sellers per channel.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Example 6-31](#db_da_03)中，我们将模型带到了一个高级用例。我们现在对每个渠道获取前三个产品感兴趣。由于我们有三个不同的渠道，即移动应用程序、网站和Instagram，我们有兴趣获取九行数据，每个渠道的前三个畅销产品各三行。
- en: To do so, we leverage the structural benefits of CTEs and start with a base
    query that will return the `sum_total_amount` per product and channel. With it,
    we can now create a second CTE, starting from the previous one, and rank the total
    amount descending per channel, which means the performance order of each product
    across channels. To obtain this rank, we default to window functions, specifically
    the `RANK()` function, which will score the rows based on the previously mentioned
    rule.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们利用CTE的结构优势，并从将返回每个产品和渠道的`sum_total_amount`的基本查询开始。现在，我们可以创建第二个CTE，从上一个CTE开始，并按渠道降序排列总金额，这意味着每个产品在各个渠道中的性能顺序。为了获得这个排名，我们默认使用窗口函数，特别是`RANK()`函数，它将根据先前提到的规则对行进行评分。
- en: Example 6-31\. Top three products per channel
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-31\. 每个渠道的前三个产品
- en: '[PRE30]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: By analyzing the results of running this query ([Figure 6-14](#ch06_query_3)),
    we conclude that our top performer for mobile app is the Men Bomber Jacket with
    Detachable Hood with €389.97 in sales, for website it’s Leather Crossbody Handbag
    with €449.97 in sales, and for Instagram it’s Unisex Running Sneakers with €271.97
    in sales.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析运行此查询的结果（[Figure 6-14](#ch06_query_3)），我们得出结论，我们移动应用程序的表现最佳产品是带可拆卸兜帽的男式轰炸机夹克，销售额为€389.97；网站的表现最佳产品是皮质斜挎手提包，销售额为€449.97；Instagram的表现最佳产品是男女款跑步鞋，销售额为€271.97。
- en: '![ch06_query_3](assets/aesd_0614.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_query_3](assets/aesd_0614.png)'
- en: Figure 6-14\. Analytical query for obtaining the top three products per channel
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-14\. 分析查询以获取每个渠道的前三个产品
- en: In [Example 6-32](#db_da_4), we conclude our roadshow through supporting business
    questions with SQL by leveraging our recently created star schema data model to
    perform an analysis of the top customers in 2023 on our mobile app. Once again,
    we leverage CTEs to structure our query correctly, but instead of window functions,
    this time we combine the `ORDER BY` clause with the `LIMIT` modifier to get the
    top three buyers in terms of money spent on purchases.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Example 6-32](#db_da_4)中，我们通过利用最近创建的星型模式数据模型来分析我们2023年移动应用程序上的顶级客户，以支持业务问题。再次利用CTE正确构造查询，但这次我们结合`ORDER
    BY`子句和`LIMIT`修饰符，以获取在购买支出方面排名前三的买家。
- en: Example 6-32\. Top three customers in 2023 on the mobile app
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-32\. 2023年移动应用程序上的前三名客户
- en: '[PRE31]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: By analyzing the results of running this query ([Figure 6-15](#ch06_query_4)),
    we conclude that our top buyer is Sophia Garcia with €389.97 spent. We can send
    an email to her at *sophia.garcia@emailaddress.com*, thanking her for being such
    a special customer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析运行此查询的结果（[Figure 6-15](#ch06_query_4)），我们得出结论，我们的顶级买家是Sophia Garcia，花费了€389.97。我们可以通过*sophia.garcia@emailaddress.com*向她发送电子邮件，感谢她成为如此特别的客户。
- en: '![ch06_query_4](assets/aesd_0615.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![ch06_query_4](assets/aesd_0615.png)'
- en: Figure 6-15\. Analytical query for obtaining the top customers for a mobile
    app
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-15\. 分析查询以获取移动应用程序的前三名客户
- en: By demonstrating these queries, we aim to highlight the inherent simplicity
    and effectiveness of using a star schema to answer complex business questions.
    By using this schema design, organizations can gain valuable insights and make
    data-driven decisions more efficiently.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展示这些查询，我们旨在突显使用星型模式来回答复杂业务问题的固有简单性和有效性。通过使用这种模式设计，组织可以获得宝贵的洞见，并更高效地做出基于数据的决策。
- en: While the preceding queries demonstrate the simplicity of each query, the real
    power lies in the ability to combine them with CTEs. This strategic use of CTEs
    allows queries to be optimized and organized in a structured and easy-to-understand
    manner. By using CTEs, analysts can streamline their workflows, improve code readability,
    and facilitate the reuse of intermediate results.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的查询展示了每个查询的简单性，但真正的力量在于能够将它们与公共表达式（CTE）结合起来。这种策略性使用CTE使得查询可以在结构化和易于理解的方式下进行优化和组织。通过使用CTE，分析师可以简化他们的工作流程，提高代码的可读性，并促进中间结果的重复使用。
- en: Also, implementing window functions brings an extra layer of efficiency to data
    analysis. Using window functions, analysts can efficiently calculate aggregated
    results across specific data partitions or windows, providing valuable insights
    into trends, rankings, and comparative analyzes. Analysts can efficiently derive
    meaningful conclusions from large datasets by using these functions, accelerating
    the decision-making process.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，实现窗口函数为数据分析带来了额外的效率层级。通过使用窗口函数，分析师可以在特定数据分区或窗口中高效计算聚合结果，为趋势、排名和比较分析提供宝贵见解。分析师可以通过这些函数高效地从大数据集中推导出有意义的结论，加速决策过程。
- en: By writing this section, we intend to encapsulate the significance of the topics
    covered in this book. It emphasizes the importance of having a solid command of
    SQL, proficient data modeling skills, and a comprehensive understanding of the
    technical landscape surrounding data technologies like dbt to grow your analytics
    engineering skills. Acquiring these competencies enables professionals to effectively
    navigate the vast quantities of data generated within businesses or personal projects.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过撰写本节，我们意在概括本书涵盖的主题的重要性。它强调了掌握SQL、熟练的数据建模技能以及全面理解围绕数据技术如dbt的技术环境的重要性，以提升您的分析工程技能。掌握这些能力使专业人士能够有效地在企业或个人项目中处理产生的大量数据。
- en: Conclusion
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The landscape of analytics engineering is as vast and varied as the limits of
    human imagination. Just as Tony Stark harnessed cutting-edge technology to transform
    into Iron Man, we are empowered by databases, SQL, and dbt that make us not just
    spectators but active heroes in the data-driven age. Like Stark’s array of armor,
    these tools provide us with the flexibility, strength, and precision to face the
    most intricate of challenges head-on.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工程的风景如同人类想象力的边界一样广阔而多样化。正如托尼·斯塔克利用尖端技术变身为钢铁侠一样，我们受到数据库、SQL和dbt的赋能，使我们不仅仅是观众，而是数据驱动时代中的积极英雄。就像斯塔克的一系列盔甲一样，这些工具为我们提供了灵活性、力量和精度，使我们能够直面最复杂的挑战。
- en: Databases and SQL have always been our foundational pillars in data-driven strategies,
    providing stability and reliability. However, as the demands and complexities
    have grown, analytics has expanded to integrate complex data modeling practices.
    This transition goes beyond just mechanics, emphasizing crafting business narratives,
    anticipating analytical trends, and forward planning for future requirements.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库和SQL一直是我们在数据驱动策略中的基础支柱，提供稳定性和可靠性。然而，随着需求和复杂性的增长，分析已扩展到整合复杂的数据建模实践。这种转变不仅仅是技术上的，还强调了制定业务叙事、预见分析趋势和为未来需求进行前瞻规划的重要性。
- en: dbt has emerged as a transformative element in this dynamic landscape. More
    than just a complement to SQL, it redefines how we approach collaboration, testing,
    and documentation. With dbt, the processing of raw and fragmented data becomes
    more refined, leading to actionable models that support informed decision making.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: dbt在这个充满活力的领域中崭露头角。它不仅仅是SQL的补充，而是重新定义了我们在协作、测试和文档化方面的方法。借助dbt，原始和碎片化数据的处理变得更加精细化，从而形成支持决策的可操作模型。
- en: Analytical engineering blends both traditional practices and innovative advancements.
    While there are established principles of data modeling, rigorous testing, and
    transparent documentation, tools like dbt introduce fresh approaches and possibilities.
    Every challenge faced is a learning opportunity for both those stepping into this
    field and those with experience. Each database and query offers a unique perspective
    and a potential solution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工程既融合了传统实践又结合了创新进展。虽然数据建模、严格测试和透明文档等原则已被确立，但像dbt这样的工具引入了新的方法和可能性。每一个面临的挑战都是新人和老手们的学习机会。每个数据库和查询都提供了独特的视角和潜在的解决方案。
- en: Just as Holmes weaves labyrinthine stories from fragmentary evidence, analytics
    engineers can create compelling data models from highly fragmented data points.
    The tools at their disposal aren’t just mechanical, but empower them to anticipate
    analytic trends, align data models with business needs, and, like Holmes, become
    storytellers of the data-driven age. In this context, dbt is their Watson, fostering
    collaboration and efficiency, much like the famous detective’s trusty companion.
    The parallels to Holmes are striking, as both have made it their mission to uncover
    secrets hidden in cryptic cases or complex datasets. It’s our hope that through
    this book’s chapters, you’ve gained insights and a clearer understanding of this
    evolving discipline.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如福尔摩斯从零碎的证据中编织复杂的故事一样，分析工程师可以从高度分散的数据点中创建引人入胜的数据模型。他们手中的工具不仅仅是机械的，而是赋予他们预测分析趋势、将数据模型与业务需求对齐，并且像福尔摩斯一样，成为数据驱动时代的讲故事者的力量。在这个背景下，dbt就像他们的沃森，促进协作和效率，就像那位著名侦探可靠的伙伴一样。与福尔摩斯的相似之处令人震撼，因为两者都以揭示隐藏在复杂数据集中的秘密为使命。我们希望通过本书的章节，您能够深入了解这一不断发展的学科，并获得洞见和更清晰的理解。
