- en: Chapter 1\. The Truth About AI Bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 真相关于AI偏见
- en: Cassie Kozyrkov
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassie Kozyrkov
- en: '![](Images/cassie_kozyrkov.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/cassie_kozyrkov.png)'
- en: Chief Decision Scientist, Google Cloud
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云的首席决策科学家
- en: No technology is free of its creators. Despite our fondest sci-fi wishes, there’s
    no such thing as AI systems that are truly separate and autonomous...because they
    start with *us*. Though its effect can linger long after you’ve pressed a button,
    all technology is an echo of the wishes of whomever built it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 没有技术是免于其创造者的。尽管我们对科幻小说充满期望，但并不存在真正分离和自主的AI系统……因为它们始于*我们*。尽管其影响可能在你按下按钮后久久不散，所有技术都是其建造者愿望的回声。
- en: Data and Math Don’t Equal Objectivity
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据和数学并不等同于客观性
- en: If you’re looking to AI as your savior from human foibles, tread carefully.
    Sure, data and math can increase the amount of information you use in decision
    making and/or save you from heat-of-the-moment silliness, but how you use them
    is still up to you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将AI视为从人类缺陷中拯救你的救世主，要小心行事。当然，数据和数学可以增加你在决策中使用的信息量，和/或者避免你在当下热情时的愚蠢，但是如何使用它们仍然取决于你。
- en: 'Look, I know sci-fi sells. It’s much flashier to say “The AI learned to do
    this task all by itself” than to tell the truth: *People used a tool with a cool
    name to help them write code. They fed in examples they considered appropriate,
    found some patterns in them, and turned those patterns into instructions. Then
    they checked whether they liked what those instructions did for them.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 听着，我知道科幻小说卖座。说“AI学会了自己完成这项任务”比告诉真相更加耀眼：“人们使用一个名字酷炫的工具来帮助他们编写代码。他们输入他们认为合适的例子，发现其中的模式，并将这些模式转化为指令。然后，他们检查这些指令对他们的影响是否令他们满意。”
- en: The truth drips with human subjectivity—look at all those little choices along
    the way that are left up to people running the project. *What shall we apply AI
    to? Is it worth doing? In which circumstances? How shall we define success? How
    well does it need to work?* [The list goes on and on](https://oreil.ly/FkHtg).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 真相充满了人类的主观性——看看沿途所有那些留给项目负责人自行决定的小选择吧。*我们应该将AI应用于什么？这值得吗？在什么情况下？我们如何定义成功？它需要多好地运行？*
    [问题层出不穷](https://oreil.ly/FkHtg)。
- en: Tragicomically, adding data to the mix obscures the ever-present human element
    and creates an illusion of objectivity. Wrapping a glamorous coat of math around
    the core doesn’t make it any less squishy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 令人哀笑的是，将数据添加到混合物中掩盖了永存的人类因素，创造了客观性的幻象。在核心周围包裹一层华丽的数学外衣，并不会使其变得不那么“软软的”。
- en: Technology always comes from and is designed by people, which means it’s no
    more objective than we are.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 技术始终来源于并由人设计，这意味着它不比我们更客观。
- en: What Is Algorithmic Bias?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是算法偏见？
- en: 'Algorithmic bias refers to situations in which a computer system reflects the
    implicit values of the people who created it. By this definition, even the most
    benign computer systems are biased; when we apply math toward a purpose, that
    purpose is shaped by the sensibilities of our times. Is AI exempt? Not at all.
    Stop thinking of AI as an entity and see it for what it really is: an excellent
    tool for writing code.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见指的是计算机系统反映其创建者的隐含价值观的情况。按照这一定义，即使是最良性的计算机系统也是有偏见的；当我们将数学应用于某一目的时，该目的受我们这个时代的感性影响。AI豁免吗？一点也不。停止将AI视为一个实体，看清它真正的面貌：写代码的极佳工具。
- en: The whole point of AI is to let you explain your wishes to a computer using
    examples (data!) instead of instructions. Which examples? That depends on what
    you’re trying to teach your system to do. Think of your dataset as the textbook
    you’re asking your machine student to learn from.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI的整体目的是让你用例子（数据！）而不是指令向计算机解释你的意愿。哪些例子？这取决于你试图教会你的系统做什么。把你的数据集想象成你要求机器学生学习的教科书。
- en: Datasets Have Human Authors
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集由人类作者编写
- en: When I’ve said that “AI bias doesn’t come from AI algorithms, it comes from
    people,” some folks have written to tell me that I’m wrong because bias comes
    from data. Well, we can both be winners...because people make the data. Like textbooks,
    datasets reflect the biases of their authors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说“AI偏见不是来自AI算法，而是来自人”，一些人写信告诉我我错了，因为偏见来自数据。好吧，我们都可以是赢家……因为数据是由人创造的。就像教科书一样，数据集反映了其作者的偏见。
- en: Consider the following image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下图片。
- en: '![Image](Images/aeds_01in01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/aeds_01in01.png)'
- en: Was your first thought “bananas”? Why didn’t you mention the plastic bag roll,
    or the color of the bananas? This example comes from Google’s AI Fairness training
    course and demonstrates that although all three answers are technically correct,
    you have a bias to prefer one of them. Not all people would share that bias; what
    we perceive and how we respond is influenced by our norms. If you live on a planet
    where all bananas are blue, you might answer “yellow bananas” here. If you’ve
    never seen a banana before, you might say “shelves with yellow stuff on them.”
    Both answers are also correct.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一个想法是“香蕉”吗？为什么你没有提到塑料袋卷或香蕉的颜色？这个例子来自谷歌的AI公平培训课程，它表明，尽管这三个答案在技术上都是正确的，但你有偏好倾向于更喜欢其中一个。并非所有人都会分享这种偏好；我们的感知和反应受到我们的规范的影响。如果你生活在一个所有香蕉都是蓝色的星球上，你可能会在这里回答“黄色的香蕉”。如果你以前从未见过香蕉，你可能会说“架子上放着黄色的东西”。这两个答案也是正确的。
- en: The data you create for your system to learn from will be biased by how you
    see the world.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你为系统创造的数据将受到你如何看待世界的偏见的影响。
- en: This Is No Excuse to Be a Jerk
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这并不是做一个混蛋的借口
- en: Philosophical arguments invalidating the existence of truly unbiased and objective
    technology don’t give anyone an excuse to be a jerk. If anything, the fact that
    you can’t pass the ethical buck to a machine puts more responsibility on your
    shoulders, not less.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 哲学上的论证无效化真正无偏和客观技术的存在并不意味着任何人可以以此为借口成为一个混蛋。如果有什么，你不能把伦理责任推给机器的事实更多地把责任放在你的肩膀上，而不是减少。
- en: Sure, our perceptions are shaped by our times. Societal ideas of virtue, justice,
    kindness, fairness, and honor aren’t the same today as they were for people living
    a few thousand years ago, and they may keep evolving. That doesn’t make these
    ideas unimportant; it only means we can’t outsource them to a heap of wires. They’re
    the responsibility of all of us, together.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的观念是由我们所处的时代塑造的。关于美德、正义、善良、公平和荣誉的社会观念与几千年前居住在此地的人们的观念并不相同，它们可能会不断演变。这并不意味着这些观念不重要；这只意味着我们不能把它们外包给一堆电线。它们是我们所有人共同的责任。
- en: Fairness in AI
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性在人工智能中
- en: Once you appreciate that *you* are responsible for how you use your tools and
    where you point them, strive to make yourself aware of how your choices affect
    the rest of humanity. For example, deciding which application to pursue is a choice
    that affects other people. Think it through.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你意识到*你*要负责如何使用你的工具以及你将它们指向何处，就努力让自己意识到你的选择如何影响其他人类。例如，决定追求哪个应用程序是一种影响其他人的选择。要仔细考虑。
- en: Another choice you have is which data to use for AI. You should expect better
    performance on examples that are similar to what your system learned from. If
    you choose not to use data from people like me, your system is more likely to
    make a mistake when I show up as your user. It’s your duty to think about the
    pain you could cause when that happens.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你另一个选择是使用哪些数据来进行AI。你应该期望在与你的系统学习相似的示例上表现更好。如果你选择不使用像我这样的人群的数据，当我作为你的用户出现时，你的系统更有可能出错。当这种情况发生时，你有责任考虑可能造成的痛苦。
- en: At a bare minimum, I hope you’d have the common sense to check whether the distribution
    of your user population matches the distribution in your data. For example, if
    100% of your training examples come from residents of a single country, but your
    target users are global...expect a mess.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，我希望你有常识去检查你的用户群体的分布是否与你的数据分布相匹配。例如，如果你的所有训练示例都来自一个国家的居民，但你的目标用户是全球的...预料到会出现问题。
- en: Fair and Aware
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平与意识
- en: I’ve written a lot of words here, when I could have just told you that most
    of the research on the topic of bias and fairness in AI is about making sure that
    your system doesn’t have a disproportionate effect on some group of users relative
    to other groups. The primary focus of AI ethics is on distribution checks and
    similar analytics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里写了很多字，当我本可以告诉你，在AI偏见和公平性的研究中，主要关注的是确保你的系统对某些用户群体相对于其他群体没有不成比例的影响。AI伦理的主要焦点是分布检查和类似的分析。
- en: The reason I wrote so much is that I want you to do even better. Automated distribution
    checks go only so far. No one knows a system better than its creators, so if you’re
    building one, take the time to think about whom your actions will affect and how,
    and do your best to give those people a voice to guide you through your blind
    spots.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我写这么多的原因是希望你能做得更好。自动化分布检查只能做到这么多。没有人比系统的创造者更了解这个系统，所以如果你正在构建一个系统，请花时间考虑你的行动将影响谁以及如何影响，并尽力让那些受影响的人有发言权，以引导你解决盲点。
