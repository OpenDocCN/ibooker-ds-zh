- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Thank you for buying our book. We hope that it provides you with a look under
    the hood of deep learning (*DL*) and gives you some inspirations on how to use
    probabilistic DL methods for your work.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您购买我们的书籍。我们希望这本书能为您揭示深度学习（*DL*）的内部机制，并为您提供一些关于如何在工作中使用概率深度学习方法灵感的启示。
- en: All three of us, the authors, have a background in statistics. We started our
    journey in DL together in 2014\. We got so excited about it that DL is still in
    the center of our professional lives. DL has a broad range of applications, but
    we are especially fascinated by the power of combining DL models with probabilistic
    approaches as used in statistics. In our experience, a deep understanding of the
    potential of probabilistic DL requires both insight into the underlying methods
    and practical experience. Therefore, we tried to find a good balance of both ingredients
    in this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们三位作者在统计学方面都有背景。我们于2014年一起开始了深度学习的旅程。我们对它如此着迷，以至于深度学习至今仍然是我们职业生涯的中心。深度学习有着广泛的应用范围，但我们特别着迷于将深度学习模型与统计学中使用的概率方法相结合的力量。根据我们的经验，对概率深度学习潜力的深入理解需要深入了解底层方法和实践经验。因此，我们在本书中试图在这两种成分之间找到一个良好的平衡。
- en: In this book, we aimed to give some clear ideas and examples of applications
    before discussing the methods involved. You also have the chance to make practical
    use of all discussed methods by working with the accompanying Jupyter notebooks.
    We hope you learn as much by reading this book as we learned while writing it.
    Have fun and stay curious!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们旨在在讨论涉及的方法之前，提供一些清晰的想法和示例应用。你们也有机会通过使用随书附带的Jupyter笔记本，实际应用所有讨论的方法。我们希望你们通过阅读这本书学到的东西，能和我们写作时学到的一样多。祝你们玩得开心，保持好奇心！
- en: acknowledgments
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'We want to thank all the people who helped us in writing this book. A special
    thanks go out to our development editor, Marina Michaels, who managed to teach
    a bunch of Swiss and Germans how to write sentences shorter than a few hundred
    words. Without her, you would have no fun deciphering the text. Also, many thanks
    to our copyeditor, Frances Buran, who spotted uncountable errors and inconsistencies
    in the text (and also in the formulas, kudos!). We also got much support on the
    technical side from Al Krinkler and Hefin Rhys to make the text and code in the
    notebooks more consistent and easier to understand. Also, thank you to our project
    editor, Deirdre Hiam; our proofreader, Keri Hales; and our review editor, Aleksandar
    Dragosavljevic´. We would also like to thank the reviewers, which at various stages
    of the book helped with their very valuable feedback: Bartek Krzyszycha, Brynjar
    Smári Bjarnason, David Jacobs, Diego Casella, Francisco José Lacueva Pérez, Gary
    Bake, Guillaume Alleon, Howard Bandy, Jon Machtynger, Kim Falk Jorgensen, Kumar
    Kandasami, Raphael Yan, Richard Vaughan, Richard Ward, and Zalán Somogyváry.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要感谢所有帮助我们撰写这本书的人。特别感谢我们的开发编辑玛琳娜·迈克尔斯，她设法教会了一群瑞士人和德国人如何写出几百字以下的句子。没有她，你们就没有乐趣去解读文本。还要感谢我们的校对员弗朗西斯·伯兰，她在文本（以及公式中）发现了无数的错误和不一致（也要感谢你！）。我们在技术方面也得到了Al
    Krinkler和Hefin Rhys的大力支持，使笔记本中的文本和代码更加一致，更容易理解。还要感谢我们的项目编辑迪尔德丽·希姆；我们的校对员凯里·黑尔斯；以及我们的审稿编辑亚历山大·德拉戈萨夫利奇。我们还要感谢审稿人，他们在书的各个阶段提供了非常有价值的反馈：Bartek
    Krzyszycha、Brynjar Smári Bjarnason、David Jacobs、Diego Casella、Francisco José Lacueva
    Pérez、Gary Bake、Guillaume Alleon、Howard Bandy、Jon Machtynger、Kim Falk Jorgensen、Kumar
    Kandasami、Raphael Yan、Richard Vaughan、Richard Ward和Zalán Somogyváry。
- en: Finally, we would also like to thank Richard Sheppard for the many excellent
    graphics and drawings making the book less dry and friendlier.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还想感谢理查德·谢泼德，他为本书提供了许多优秀的图形和插图，使书籍不那么枯燥，更加友好。
- en: I, Oliver, would like to thank my partner Lena Obendiek for her patience as
    I worked on the book for many long hours. I also thank my friends from the “Tatort”
    viewing club for providing food and company each Sunday at 8:15 pm and for keeping
    me from going crazy while writing this book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我，奥利弗，想感谢我的合作伙伴莱娜·奥本代克，在我长时间工作于这本书时，她的耐心。我还感谢来自“Tatort”观看俱乐部的朋友们，每周日晚上8:15提供食物和陪伴，并在写作这本书时帮助我避免发疯。
- en: I, Beate, want to thank my friends, not so much for helping me to write the
    book, but for sharing with me a good time beyond the computer screen--first of
    all my partner Michael, but also the infamous Limmat BBQ group and my friends
    and family outside of Zurich who still spend leisure time with me despite the
    Rösti-Graben, the country border to the big canton, or even the big pond in between.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我，Beate，想要感谢我的朋友们，不仅仅是因为他们帮助我写这本书，而是因为他们与我分享了屏幕之外的快乐时光——首先是我的伴侣Michael，还有臭名昭著的Limmat
    BBQ小组，以及苏黎世之外的朋友们和家人，尽管有Rösti-Graben，即通往大州的国界，甚至还有中间的大湖，他们仍然与我共度休闲时光。
- en: I, Elvis, want to thank everyone who supported me during the exciting time of
    writing this book, not only professionally, but also privately during a good glass
    of wine or a game of football.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我，Elvis，想要感谢在撰写这本书的激动人心时刻支持我的人，不仅在专业上，而且在私下里，无论是品一杯美酒还是踢一场足球。
- en: We, the Tensor Chiefs, are happy that we made it together to the end of this
    book. We look forward to new scientific journeys, but also to less stressful times
    where we not only meet for work, but also for fun.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们，Tensor Chiefs，很高兴我们一起走到了这本书的结尾。我们期待新的科学旅程，但也期待不那么紧张的时间，那时我们不仅为了工作，也为了乐趣而相聚。
- en: about this book
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于这本书
- en: In this book, we hope to bring the probabilistic principles underpinning deep
    learning (*DL*) to a broader audience. In the end (almost), all neural networks
    (NNs) in DL are probabilistic models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们希望将支撑深度学习（*DL*）的概率原理带给更广泛的读者。最终（几乎），深度学习（DL）中的所有神经网络（NNs）都是概率模型。
- en: 'There are two powerful probabilistic principles: maximum likelihood and Bayes.
    Maximum likelihood (fondly referred to as MaxLike) governs all traditional DL.
    Understanding networks as probabilistic models trained with the maximum likelihood
    principle helps you to boost the performance of your networks (as Google did when
    going from WaveNet to WaveNet++) or to generate astounding applications (like
    OpenAI did with Glow, a net that generates realistic looking faces). Bayesian
    methods come into play in situations where networks need to say, “I’m not sure.”
    (Strangely, traditional NNs cannot do this.) The subtitle for the book, “with
    Python, Keras, and TensorFlow Probability,” reflects the fact that you really
    should get your hands dirty and do some coding.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个强大的概率原理：最大似然和贝叶斯。最大似然（亲切地称为MaxLike）支配着所有传统深度学习（DL）。将网络视为使用最大似然原理训练的概率模型，可以帮助你提升网络的性能（就像谷歌从WaveNet过渡到WaveNet++时所做的那样）或生成惊人的应用（例如，OpenAI使用Glow，一个生成逼真面孔的网络）。在需要网络表示“我不确定”的情况下，贝叶斯方法就派上用场了。（奇怪的是，传统的神经网络无法做到这一点。）本书的副标题“使用Python、Keras和TensorFlow
    Probability”反映了这样一个事实，即你真的应该亲自动手编写一些代码。
- en: Who should read this book
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应该阅读这本书的人
- en: This book is written for people who like to understand the underlying probabilistic
    principles of DL. Ideally, you should have some experience with DL or machine
    learning (ML) and should not be too afraid of a bit of math and Python code. We
    did not spare the math and always included examples in code. We believe math goes
    better with code.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为那些喜欢理解深度学习（DL）底层概率原理的人所写。理想情况下，你应该有一些深度学习（DL）或机器学习（ML）的经验，并且不应该对数学和Python代码感到过于恐惧。我们没有省略数学，并且在代码中总是包含了示例。我们相信数学与代码更相得益彰。
- en: 'How this book is organized: A roadmap'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这本书的组织结构：路线图
- en: The book has three parts that cover eight chapters. Part 1 explains traditional
    deep learning (*DL*) architectures and how the training of neural networks (NNs)
    is done technically.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三部分，共涵盖八章。第一部分解释了传统的深度学习（*DL*）架构以及神经网络（NNs）的技术训练过程。
- en: Chapter 1--Sets the stage and introduces you to probabilistic DL.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一章--设定场景，并介绍概率深度学习（DL）。
- en: Chapter 2--Talks about network architectures. We cover fully connected neural
    networks (fcNNs), which are kind of all-purpose networks, and convolutional neural
    networks (CNNs), which are ideal for images.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二章--讨论网络架构。我们涵盖了全连接神经网络（fcNNs），这是一种全能型网络，以及卷积神经网络（CNNs），它们非常适合图像处理。
- en: Chapter 3--Shows you how NNs manage to fit millions of parameters. We keep it
    easy and show gradient descent and backpropagation on the simplest network one
    can think of--linear regression.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三章--展示了神经网络如何拟合数百万个参数。我们尽量简化，展示了在可以想到的最简单网络上的梯度下降和反向传播--线性回归。
- en: Part 2 focuses on using NNs as probabilistic models. In contrast to part 3,
    we discuss maximum likelihood approaches. These are behind all traditional DL.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分专注于将神经网络作为概率模型使用。与第三部分相比，我们讨论了最大似然方法。这些方法背后是所有传统深度学习（DL）。
- en: Chapter 4--Explores maximum likelihood (MaxLike), the underlying principle of
    ML and DL. We start by applying this principle to classification and (simple regression
    problems).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四章——探讨了最大似然（MaxLike），这是机器学习和深度学习的基本原理。我们首先将这个原理应用于分类和（简单的回归问题）。
- en: Chapter 5--Introduces TensorFlow Probability (TFP), a framework to build deep
    probabilistic models. We use it for not-so-simple regression problems like count
    data.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五章——介绍了 TensorFlow Probability (TFP)，这是一个用于构建深度概率模型的框架。我们用它来解决像计数数据这样的不太简单的回归问题。
- en: Chapter 6--Begins with more complex regression models. At the end, we explain
    how you can use probabilistic models to master complex distributions like describing
    images of human faces.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第六章——从更复杂的回归模型开始。最后，我们解释了如何使用概率模型来掌握描述人类面部图像等复杂分布。
- en: Part 3 introduces Bayesian NNs. Bayesian NNs allow you to handle uncertainty.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分介绍了贝叶斯神经网络。贝叶斯神经网络允许你处理不确定性。
- en: Chapter 7--Motivates the need for Bayesian DL and explains its principles. We
    again look at the simple example of linear regression to explain the Bayesian
    principle.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第七章——阐述了贝叶斯深度学习的必要性，并解释了其原理。我们再次通过简单的线性回归示例来解释贝叶斯原理。
- en: Chapter 8--Shows you how to build Bayesian NNs. Here we cover two approaches
    called MC (Monte Carlo) dropout and variational inference.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第八章——展示了如何构建贝叶斯神经网络。在这里，我们涵盖了两种称为 MC（蒙特卡洛）dropout 和变分推理的方法。
- en: If you already have experience with DL, you can skip the first part. Also, the
    second part of chapter 6 (starting with section 6.3) describes normalizing flows.
    You do not need to know these to understand the material in part 3\. Section 6.3.5
    is a bit heavy on math, so if this is not your cup of tea, you can skip it. The
    same holds true for sections 8.2.1 and 8.2.2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有深度学习的经验，你可以跳过第一部分。此外，第六章的第二部分（从 6.3 节开始）描述了正态流。你不需要了解这些来理解第三部分的内容。6.3.5
    节在数学上有点复杂，所以如果你不感兴趣，可以跳过。8.2.1 和 8.2.2 节也是如此。
- en: About the code
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于代码
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a `fixed-width`
    `font, like` `this` to separate it from ordinary text.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含许多源代码示例，无论是编号列表还是与普通文本混排。在两种情况下，源代码都以 `fixed-width` `font`，如 `this` 的格式呈现，以将其与普通文本区分开来。
- en: 'The code samples are taken from Jupyter notebooks. These notebooks include
    additional explanations and most include little exercises you should do for a
    better understanding of the concepts introduced in this book. You can find all
    the code in this directory in GitHub: [https://github.com/tensorchiefs/dl_book/](https://github.com/tensorchiefs/dl_book/)
    . A good place to start is in the directory [https://tensorchiefs.github.io/dl_book/](https://tensorchiefs.github.io/dl_book/)
    , where you’ll find links to the notebooks. The notebooks are numbered according
    to the chapters. So, for example, nb_ch08_02 is the second notebook in chapter
    8.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码示例是从 Jupyter 笔记本中提取的。这些笔记本包含额外的解释，并且大多数都包括一些你应该做的练习，以更好地理解本书中介绍的概念。你可以在 GitHub
    上的这个目录中找到所有代码：[https://github.com/tensorchiefs/dl_book/](https://github.com/tensorchiefs/dl_book/)。一个不错的起点是在这个目录
    [https://tensorchiefs.github.io/dl_book/](https://tensorchiefs.github.io/dl_book/)，在那里你可以找到笔记本的链接。笔记本是按照章节编号的。例如，nb_ch08_02
    是第 8 章的第二本笔记本。
- en: All the examples in this book, except nb_06_05, are tested with the TensorFlow
    v2.1 and TensorFlow Probability (TFP) v0.8\. The notebooks nb_ch03_03 and nb_ch03_04,
    describing the computation graphs, are easier to understand in TensorFlow v1\.
    For these notebooks, we also include both versions of TensorFlow. The nb_06_05
    notebook only works with TensorFlow v1 because we need weights that are only provided
    in that version of TensorFlow.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有示例，除了 nb_06_05，都是使用 TensorFlow v2.1 和 TensorFlow Probability (TFP) v0.8
    测试的。描述计算图的笔记本 nb_ch03_03 和 nb_ch03_04 在 TensorFlow v1 中更容易理解。对于这些笔记本，我们还包含了 TensorFlow
    的两个版本。nb_06_05 笔记本只适用于 TensorFlow v1，因为我们需要在该版本的 TensorFlow 中提供的权重。
- en: You can execute the notebooks in Google’s Colab or locally. Colab is great;
    you can simply click on a link and then play with the code in the cloud. No installation--you
    just need a browser. We definitely suggest that you go this way.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Google 的 Colab 或本地执行这些笔记本。Colab 非常棒；你只需点击一个链接，然后在云端玩转代码。无需安装——你只需要一个浏览器。我们强烈建议你这样去做。
- en: TensorFlow is still fast-evolving, and we cannot guarantee the code will run
    in several years’ time. We, therefore, provide a Docker container ([https://github.com
    oduerr/ dl_book_docker/](https://github.com/oduerr/dl_book_docker/)) that you
    can use to execute all notebooks except nb_06_05 and the TensorFlow 1.0 versions
    of nb_ch03_03 and nb_ch03_04\. This Docker container is the way to go if you want
    to use the notebooks locally.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow仍在快速发展，我们无法保证代码在几年后仍能运行。因此，我们提供了一个Docker容器（[https://github.com/oduerr/dl_book_docker/](https://github.com/oduerr/dl_book_docker/)），您可以使用它来执行所有notebooks，除了nb_06_05以及nb_ch03_03和nb_ch03_04的TensorFlow
    1.0版本。如果您想在本地使用notebooks，这是必走之路。
- en: liveBook discussion forum
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: liveBook讨论论坛
- en: Purchase of Probabilistic Deep Learning includes free access to a private web
    forum run by Manning Publications where you can make comments about the book,
    ask technical questions, and receive help from the authors and from other users.
    To access the forum, go to [https://livebook.manning.com/book/probabilistic-deep-learning-with-python/welcome/v-6/](https://livebook.manning.com/book/probabilistic-deep-learning-with-python/welcome/v-6/)
    . You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)
    .
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 购买《概率深度学习》包括免费访问由Manning Publications运营的私人网络论坛，您可以在论坛上对书籍发表评论，提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请访问[https://livebook.manning.com/book/probabilistic-deep-learning-with-python/welcome/v-6/](https://livebook.manning.com/book/probabilistic-deep-learning-with-python/welcome/v-6/)。您还可以在[https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)上了解更多关于Manning论坛和行为准则的信息。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the authors can take
    place. It is not a commitment to any specific amount of participation on the part
    of the authors, whose contribution to the forum remains voluntary (and unpai*D*).
    We suggest you try asking the authors some challenging questions lest their interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 曼宁对读者的承诺是提供一个平台，在这里读者之间以及读者与作者之间可以进行有意义的对话。这并不是对作者参与特定数量活动的承诺，作者对论坛的贡献仍然是自愿的（并且**未付费**）。我们建议您尝试向作者提出一些挑战性的问题，以免他们的兴趣转移！只要书籍在印刷中，论坛和先前讨论的存档将可通过出版社的网站访问。
- en: about the authors
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: Oliver Dürr is professor for data science at the University of Applied Sciences
    in Konstanz, Germany. Beate Sick holds a chair for applied statistics at ZHAW,
    and works as a researcher and lecturer at the University of Zurich, and as a lecturer
    at ETH Zurich. Elvis Murina is a research scientist, responsible for the extensive
    exercises that accompany this book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Oliver Dürr是德国康斯坦茨应用科学大学的教授，教授数据科学。Beate Sick在ZHAW担任应用统计学教授，并在苏黎世大学担任研究员和讲师，同时在苏黎世联邦理工学院（ETH
    Zurich）担任讲师。Elvis Murina是一位研究科学家，负责本书伴随的广泛练习。
- en: Dürr and Sick are both experts in machine learning and statistics. They have
    supervised numerous bachelor’s, master’s, and PhD theses on the topic of deep
    learning, and planned and conducted several postgraduate- and master’s-level deep
    learning courses. All three authors have worked with deep learning methods since
    2013, and have extensive experience in both teaching the topic and developing
    probabilistic deep learning models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Dürr和Sick都是机器学习和统计学的专家。他们监督了关于深度学习的众多学士、硕士和博士论文，并计划并实施了多个研究生和硕士学位的深度学习课程。所有三位作者自2013年以来一直在使用深度学习方法，并在教授该主题以及开发概率深度学习模型方面拥有丰富的经验。
- en: about the cover illustration
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于封面插图
- en: The figure on the cover of Probabilistic Deep Learning is captioned “Danseuse
    de l’Isle O-tahiti,” or A dancer from the island of Tahiti. The illustration is
    taken from a collection of dress costumes from various countries by Jacques Grasset
    de Saint-Sauveur (1757-1810), titled Costumes de Différents Pays, published in
    France in 1788\. Each illustration is finely drawn and colored by hand. The rich
    variety of Grasset de Saint-Sauveur’s collection reminds us vividly of how culturally
    apart the world’s towns and regions were just 200 years ago. Isolated from each
    other, people spoke different dialects and languages. In the streets or in the
    countryside, it was easy to identify where they lived and what their trade or
    station in life was just by their dress.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 《概率深度学习》封面上的插图被标注为“丹萨·德·伊勒·奥塔希提”，或塔希提岛的舞者。这幅插图取自雅克·Grasset de Saint-Sauveur（1757-1810）的作品集，名为《不同国家的服饰》，1788年在法国出版。每一幅插图都是手工精心绘制和着色的。Grasset
    de Saint-Sauveur收藏中的丰富多样性生动地提醒我们，200年前世界的城镇和地区在文化上是如何截然不同的。彼此孤立，人们说着不同的方言和语言。在街道或乡村，仅凭他们的服饰，就可以轻易地识别出他们居住的地方以及他们的职业或社会地位。
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life--certainly for a more
    varied and fast-paced technological life.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自那以后，我们的着装方式已经改变，而当时区域间的多样性，如此丰富，现在已经逐渐消失。现在很难区分不同大陆的居民，更不用说不同的城镇、地区或国家了。也许我们是以更丰富多彩的个人生活——当然，是更丰富多彩、节奏更快的技术生活——为代价，换取了文化多样性。
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在难以区分一本计算机书与另一本计算机书的时代，曼宁通过基于两百年前丰富多样的区域生活所设计的书封面，庆祝了计算机行业的创新精神和主动性，这些封面由Grasset
    de Saint-Sauveur的画作赋予新生。
