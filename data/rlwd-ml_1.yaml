- en: Part 2\. Practical application
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分。实际应用
- en: In [part 2](#part02), you’ll go beyond a basic ML workflow to look at how to
    extract features from text, images, and time-series data to improve the accuracy
    of models even further, and to scale your ML system to larger data volumes. In
    addition, you’ll go through three full example chapters to see everything in action.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二部分](#part02)中，你将超越基本的机器学习工作流程，了解如何从文本、图像和时间序列数据中提取特征，以进一步提高模型的准确性，并将你的机器学习系统扩展到更大的数据量。此外，你将阅读三个完整的示例章节，以看到一切的实际应用。
- en: In [chapter 6](kindle_split_017.html#ch06), our first full example chapter,
    you’ll try to predict the tipping behavior of NYC taxis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_017.html#ch06)，我们的第一个完整示例章节，你将尝试预测纽约市出租车的打赏行为。
- en: In [chapter 7](kindle_split_018.html#ch07), you’ll look at advanced feature-engineering
    processes that allow you to extract value out of natural language text, images,
    and time series data. A lot of modern ML and artificial intelligence applications
    are based on these techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](kindle_split_018.html#ch07)中，你将了解高级特征工程过程，这些过程允许你从自然语言文本、图像和时间序列数据中提取价值。许多现代机器学习和人工智能应用都基于这些技术。
- en: 'In [chapter 8](kindle_split_019.html#ch08), you’ll use this advanced feature-engineering
    knowledge in another full example: predicting the sentiment of online movie reviews.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](kindle_split_019.html#ch08)中，你将使用这种高级特征工程知识在另一个完整的示例中：预测在线电影评论的情感。
- en: In [chapter 9](kindle_split_020.html#ch09), you’ll learn techniques for scaling
    ML systems to larger volumes of data, higher prediction throughput, and lower
    prediction latency. These are all important aspects of many modern ML deployments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](kindle_split_020.html#ch09)中，你将学习将机器学习系统扩展到更大数据量、更高预测吞吐量和更低预测延迟的技术。这些都是许多现代机器学习部署的重要方面。
- en: In [chapter 10](kindle_split_021.html#ch10), you’ll walk through a full example
    of building a model—on large amounts of data—that predicts online digital display
    advertisement clicks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](kindle_split_021.html#ch10)中，你将逐步构建一个模型——在大量数据上——该模型可以预测在线数字展示广告的点击量。
- en: 'Chapter 6\. Example: NYC taxi data'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章。示例：纽约市出租车数据
- en: '*This chapter covers*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Introducing, visualizing, and preparing a real-world dataset about NYC taxi
    trips
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍、可视化和准备关于纽约市出租车行程的真实世界数据集
- en: Building a classification model to predict passenger tipping habits
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个分类模型来预测乘客打赏习惯
- en: Optimizing an ML model by tuning model parameters and engineering features
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调整模型参数和工程特征来优化机器学习模型
- en: Building and optimizing a regression model to predict tip amount
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立和优化回归模型以预测小费金额
- en: Using models to gain a deeper understanding of data and the behavior it describes
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型来深入理解数据和它所描述的行为
- en: In the previous five chapters, you learned how to go from raw, messy data to
    building, validating, and optimizing models by tuning parameters and engineering
    features that capture the domain knowledge of the problem. Although we’ve used
    a variety of minor examples throughout these chapters to illustrate the points
    of the individual sections, it’s time for you to use the knowledge you’ve acquired
    and work through a full, real-world example. This is the first of three chapters
    (along with [chapters 8](kindle_split_019.html#ch08) and [10](kindle_split_021.html#ch10))
    entirely dedicated to a full, real-world example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前五章中，你学习了如何从原始、混乱的数据开始，通过调整参数和工程特征来构建、验证和优化模型。尽管我们在这些章节中使用了各种小例子来阐述各个部分的观点，但现在是你运用所获得的知识并解决一个完整、真实世界例子的时候了。这是三个章节（连同[第8章](kindle_split_019.html#ch08)和[第10章](kindle_split_021.html#ch10)）中完全致力于一个完整、真实世界例子的第一个。
- en: In the first section of this chapter, you’ll take a closer look at the data
    and various useful visualizations that help you gain a better understanding of
    the possibilities of the data. We explain how the initial data preparation is
    performed, so the data will be ready for the modeling experiments in the subsequent
    sections. In the second section, you’ll set up a classification problem and improve
    the performance of the model by tuning model parameters and engineering new features.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，你将更仔细地查看数据以及各种有用的可视化，这些可视化有助于你更好地理解数据的可能性。我们解释了如何进行初始数据准备，以便数据为后续章节中的建模实验做好准备。在第二部分，你将设置一个分类问题，并通过调整模型参数和工程新特征来提高模型的性能。
- en: '6.1\. Data: NYC taxi trip and fare information'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1。数据：纽约市出租车行程和费用信息
- en: With companies and organizations producing more and more data, a large set of
    rich and interesting datasets has become available in recent years. In addition,
    some of these organizations are embracing the concept of *open data*, enabling
    the public dissemination and use of the data by any interested party.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司和组织产生越来越多的数据，近年来出现了一大批丰富且有趣的数据集。此外，一些这些组织正在拥抱*开放数据*的概念，允许任何感兴趣的各方公开传播和使用这些数据。
- en: 'Recently, the New York State Freedom of Information Law (FOIL) made available
    an extremely detailed dataset of New York City taxi trip records from every taxi
    trip of 2013.^([[1](#ch06fn01)]) This dataset collected various sets of information
    on each individual taxi trips including the pickup and drop-off location, time
    and duration of the trip, distance travelled, and fare amount. You’ll see that
    this data qualifies as real-world data, not only because of the way it has been
    generated but also in the way that it’s messy: there are missing data, spurious
    records, unimportant columns, baked-in biases, and so on.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，纽约州信息自由法（FOIL）公布了一个极其详细的数据集，包含了2013年所有出租车行程的记录。[1](#ch06fn01)这个数据集收集了每个单独出租车行程的各种信息，包括接车和下车位置、行程时间和持续时间、行驶距离和费用金额。您会看到这些数据符合现实世界数据的特征，不仅因为其生成方式，还因为其混乱：存在缺失数据、虚假记录、不重要的列、内置偏见等。
- en: ¹
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Initially released in a blog post by Chris Wong: [http://chriswhong.com/open-data/foil_nyc_taxi/](http://chriswhong.com/open-data/foil_nyc_taxi/).'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该数据最初由Chris Wong在博客文章中发布：[http://chriswhong.com/open-data/foil_nyc_taxi/](http://chriswhong.com/open-data/foil_nyc_taxi/)。
- en: And speaking of data, there’s a lot of it! The full dataset is over 19 GB of
    CSV data, making it too large for many machine-learning implementations to handle
    on most systems. For simplicity, in this chapter you’ll work with a smaller subset
    of the data. In [chapters 9](kindle_split_020.html#ch09) and [10](kindle_split_021.html#ch10),
    you’ll investigate methods that are able to scale to sizes like this and even
    larger, so by the end of the book you’ll know how to analyze all 19 GB of data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 说到数据，数据量很大！完整的数据集超过19 GB的CSV数据，这使得许多机器学习实现无法在大多数系统上处理。为了简化，在本章中，您将使用数据的一个较小子集。在[第9章](kindle_split_020.html#ch09)和[第10章](kindle_split_021.html#ch10)中，您将研究能够扩展到这种大小甚至更大的方法，因此到本书结束时，您将知道如何分析全部19
    GB的数据。
- en: The data is available for download at [www.andresmh.com/nyctaxitrips/](http://www.andresmh.com/nyctaxitrips/).
    The dataset consists of 12 pairs of trip/fare compressed CSV files. Each file
    contains about 14 million records, and the trip/fare files are matched line by
    line.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可在[www.andresmh.com/nyctaxitrips/](http://www.andresmh.com/nyctaxitrips/)下载。数据集由12对行程/票价压缩CSV文件组成。每个文件包含约1400万条记录，行程/票价文件是逐行匹配的。
- en: 'You’ll follow our basic ML workflow: analyzing the data; extracting features;
    building, evaluating, and optimizing models; and predicting on new data. In the
    next subsection, you’ll look at the data by using some of the visualization methods
    from [chapter 2](kindle_split_012.html#ch02).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您将遵循我们的基本机器学习工作流程：分析数据；提取特征；构建、评估和优化模型；以及在新的数据上进行预测。在下一小节中，您将通过使用[第2章](kindle_split_012.html#ch02)中的一些可视化方法来查看数据。
- en: 6.1.1\. Visualizing the data
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1. 可视化数据
- en: As you get started with a new problem, the first step is to gain an understanding
    of what the dataset contains. We recommend that you start by loading the dataset
    and viewing it in tabular form. For this chapter, we’ve joined the trip/fare lines
    into a single dataset. [Figure 6.1](#ch06fig01) shows the first six rows of data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始解决一个新问题时，第一步是了解数据集包含的内容。我们建议您首先加载数据集，并以表格形式查看它。对于本章，我们将行程/票价数据合并为一个单一的数据集。[图6.1](#ch06fig01)显示了数据的前六行。
- en: Figure 6.1\. The first six rows of the NYC taxi trip and fare record data. Most
    of the columns are self-explanatory, but we introduce some of them in more detail
    in the text that follows.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1. 纽约市出租车行程和票价记录数据的前六行。大多数列都是自解释的，但我们在随后的文本中会详细介绍其中的一些。
- en: '![](06fig01_alt.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig01_alt.jpg)'
- en: The `medallion` and `hack_license` columns look like simple ID columns that
    are useful for bookkeeping but less interesting from an ML perspective. From their
    column names, a few of the columns look like categorical data, like `vendor_id`,
    `rate_code`, `store_and_fwd_flag`, and `payment_type`. For individual categorical
    variables, we recommend visualizing their distributions either in tabular form
    or as bar plots. [Figure 6.2](#ch06fig02) uses bar plots to show the distribution
    of values in each of these categorical columns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`medallion`和`hack_license`列看起来像简单的ID列，对簿记有用，但从机器学习的角度来看不太有趣。从它们的列名来看，一些列看起来像分类数据，如`vendor_id`、`rate_code`、`store_and_fwd_flag`和`payment_type`。对于单个分类变量，我们建议以表格形式或条形图形式可视化它们的分布。[图6.2](#ch06fig02)使用条形图显示了这些分类列中每个值的分布。'
- en: Figure 6.2\. The distribution of values across some of the categorical-looking
    columns in our dataset
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2. 我们数据集中一些看似分类的列的值分布
- en: '![](06fig02_alt.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig02_alt.jpg)'
- en: Next, let’s look at some of the numerical columns in the dataset. It’s interesting
    to validate, for example, that correlations exist between things like trip duration
    (`trip_time_in_secs`), distance, and total cost of a trip. [Figure 6.3](#ch06fig03)
    shows scatter plots of some of these factors plotted against each other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看数据集中的一些数值列。例如，验证诸如行程时长(`trip_time_in_secs`)、距离和行程总费用之间存在相关性是有趣的。[图6.3](#ch06fig03)显示了这些因素相互之间的散点图。
- en: Figure 6.3\. Scatter plots of taxi trips for the time in seconds versus the
    trip distance, and the time in seconds versus the trip amount (USD), respectively.
    A certain amount of correlation exists, as expected, but the scatter is still
    relatively high. Some less-logical clusters also appear, such as a lot of zero-time
    trips, even expensive ones, which may indicate corrupted data entries.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3. 出租车行程的散点图，分别显示了以秒为单位的时间与行程距离，以及以秒为单位的时间与行程金额（美元）的关系。正如预期的那样，存在一定程度的关联，但散点仍然相对较高。还出现了一些不太合理的簇，例如很多零时间行程，甚至一些昂贵的行程，这可能表明数据条目有误。
- en: '![](06fig03_alt.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig03_alt.jpg)'
- en: Finally, in [figure 6.4](#ch06fig04), you can visualize the pickup locations
    in the latitude/longitude space, defining a map of NYC taxi trips. The distribution
    looks reasonable, with most pickup locations occurring in downtown Manhattan,
    many occurring in the other boroughs, and surprisingly a few happening in the
    middle of the East River!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[图6.4](#ch06fig04)中，你可以可视化纬度/经度空间中的接车位置，定义纽约市出租车行程的地图。分布看起来是合理的，大多数接车位置发生在曼哈顿市中心，许多发生在其他区，令人惊讶的是，还有少数发生在东河中间！
- en: Figure 6.4\. The latitude/longitude of pickup locations. Note that the x-axis
    is flipped, compared to a regular map. You can see a huge number of pickups in
    Manhattan, falling off as you move away from the city center.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4. 接车位置的纬度/经度。请注意，与常规地图相比，x轴被翻转了。你可以看到曼哈顿有大量的接车点，随着你远离市中心，数量逐渐减少。
- en: '![](06fig04_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig04_alt.jpg)'
- en: With a fresh perspective on the data you’re dealing with, let’s go ahead and
    dream up a realistic problem that you can solve with this dataset by using machine
    learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以对所处理数据的全新视角，让我们继续设想一个可以通过使用机器学习解决的现实问题，这个问题可以用这个数据集来解决。
- en: 6.1.2\. Defining the problem and preparing the data
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2. 定义问题和准备数据
- en: 'When we first looked at this data, a particular column immediately grabbed
    our attention: `tip_amount`. This column stores the information about the amount
    of the tip (in US dollars) given for each ride. It would be interesting to understand,
    in greater detail, what factors most influence the amount of the tip for any given
    NYC taxi trip.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们最初查看这些数据时，一个特定的列立即引起了我们的注意：`tip_amount`。这个列存储了每次行程给小费的金额（美元）的信息。了解哪些因素最影响任何给定纽约市出租车行程的小费金额将是有趣的。
- en: To this end, you might want to build a classifier that uses all of the trip
    information to try to predict whether a passenger will tip a driver. With such
    a model, you could predict tip versus no tip at the end of each trip. A taxi driver
    could have this model installed on a mobile device and would get no-tip alerts
    and be able to alter the situation before it was too late. While you wait for
    approval for having your app installed in all NYC taxis, you can use the model
    to give you insight into which parameters are most important, or predictive, of
    tip versus no tip in order to attempt to boost overall tipping on a macro level.
    [Figure 6.5](#ch06fig05) shows a histogram of the tip amount across all taxi trips.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，你可能想要构建一个分类器，该分类器使用所有行程信息来尝试预测乘客是否会给司机小费。有了这样的模型，你可以在每次行程结束时预测是否给小费。出租车司机可以在移动设备上安装这个模型，并在小费未给的情况下收到警报，从而在情况变得太晚之前改变现状。在你等待所有纽约出租车安装你的应用程序获得批准的同时，你可以使用这个模型来了解哪些参数对于小费与否最为重要，或者具有预测性，以便在宏观层面上尝试提高整体小费率。[图6.5](#ch06fig05)显示了所有出租车行程中小费金额的直方图。
- en: Figure 6.5\. The distribution of tip amount. Around half the trips yielded $0
    tips, which is more than we’d expect intuitively.
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5。小费金额的分布。大约一半的行程没有小费，这比我们直观上预期的要多。
- en: '![](06fig05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5](06fig05.jpg)'
- en: 'So the plan for our model is to predict which trips will result in no tip,
    and which will result in a tip. This is a job for a binary classifier. With such
    a classifier, you’ll to be able to do the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们模型的计划是预测哪些行程将导致不给小费，哪些将导致给小费。这是一项二元分类器的工作。有了这样的分类器，你将能够做到以下几点：
- en: Assist the taxi driver by providing an alert to predicted no-tip situations
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供预测不给小费情况的警报来帮助出租车司机
- en: Gain understanding of how and why such a situation might arise by using the
    dataset to uncover the driving factors (pun intended!) behind incidence of tipping
    in NYC taxi rides
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用数据集来揭示纽约出租车行程中给小费背后的驱动因素（有意为之！），来了解这种情况是如何以及为什么会出现。
- en: A story from the real world
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一个来自现实世界的故事
- en: 'Before you start building this model, we’ll tell you the real story of how
    our first attempt at tackling this problem was quite unsuccessful, disguised as
    very successful—the worst kind of unsuccessful—and how we fixed it. This type
    of detour is extremely common when working with real data, so it’s helpful to
    include the lessons learned here. When working with machine learning, it’s critical
    to watch out for two pitfalls: *too-good-to-be-true scenarios* and making *premature
    assumptions* that aren’t rooted in the data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始构建这个模型之前，我们将告诉你我们第一次尝试解决这个问题时的真实故事，这个尝试非常不成功，却伪装成非常成功——最糟糕的不成功——以及我们是如何纠正它的。这种类型的偏离在处理真实数据时非常常见，因此包括在这里学到的教训是有帮助的。在处理机器学习时，密切关注两个陷阱至关重要：*过于美好以至于不可能是真的*的场景和基于数据的*过早的假设*。
- en: As a general rule in ML, if the cross-validated accuracy is higher than you’d
    have expected, chances are your model is cheating somewhere. The real world is
    creative when trying to make your life as a data scientist difficult. When building
    initial tip/no-tip classification models, we quickly obtained a very high cross-validated
    predictive accuracy of the model. Because we were so excited about the model performance
    on this newly acquired dataset—we nailed it—we temporarily ignored the warnings
    of a cheating model. But having been bitten by such things many times before,
    the overly optimistic results caused us to investigate further.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的一般规则中，如果交叉验证的准确率高于你预期的，那么很可能是你的模型在某个地方作弊。现实世界在试图让数据科学家的生活变得困难时非常具有创造性。在构建初始的小费/不给小费分类模型时，我们迅速获得了非常高的交叉验证预测准确率。因为我们对于这个新获得的数据集上的模型性能非常兴奋——我们做到了——我们暂时忽略了作弊模型的警告。但是，由于之前多次被这种事情咬过，过于乐观的结果迫使我们进一步调查。
- en: 'One of the things we looked at was the importance of the input features (as
    you’ll see in more detail in later sections). In our case, a certain feature totally
    dominated in terms of feature importance in the model: *payment type*.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的一件事是输入特征的重要性（你将在后面的章节中更详细地看到）。在我们的案例中，一个特定的特征在模型中的特征重要性方面完全占主导地位：*支付类型*。
- en: From our own taxi experience, this could make sense. People paying with credit
    cards (in the pre-Square era) may have a lower probability of tipping. If you
    pay with cash, you almost always round up to whatever you have the bills for.
    So we started segmenting the number of tips versus no tips for people paying with
    a credit card rather than cash. Alas, it turned out that the vast majority (more
    than 95%) of the millions of passengers paying with a credit card did tip. So
    much for that theory.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们自己的出租车经验来看，这可能是有道理的。在Square时代之前使用信用卡支付的人可能给小费的可能性较低。如果你用现金支付，你几乎总是会把零钱凑成整数。所以我们开始将使用信用卡支付而不是现金的人的小费与不支付小费的人数进行细分。然而，结果令人惊讶，超过95%的数百万名使用信用卡支付的车费乘客都给了小费。那么，那个理论就到此为止了。
- en: So how many people paying with cash tipped? *All* of them?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，用现金支付的人有多少人给了小费？*所有人*？
- en: In actuality, *none* of the passengers paying with cash had tipped! Then it
    quickly became obvious. Whenever a passenger paid with cash and gave a tip, the
    driver didn’t register it in whatever way was necessary for it to be included
    as part of our data. By going through our ML sanity checks, we unearthed millions
    of instances of potential fraud in the NYC taxi system!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，用现金支付的所有乘客都没有给小费！然后很快就变得很明显。每当乘客用现金支付并给小费时，司机都没有以任何必要的方式将其记录下来，以便将其包含在我们的数据中。通过进行我们的机器学习合理性检查，我们发现了数百万个潜在的欺诈案例，这些案例发生在纽约市出租车系统中！
- en: 'Returning to the implications for our ML model: in a situation like this, when
    there’s a problem in the generation of the data, there’s simply no way to trust
    that part of the data for building an ML model. If the answers are incorrect in
    nefarious ways, then what the ML model learns may be completely incorrect and
    detached from reality.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们机器学习模型的影响：在这种情况下，当数据生成存在问题的时候，我们根本无法相信这部分数据用于构建机器学习模型。如果答案以恶意的方式不正确，那么机器学习模型学到的可能完全错误，与现实脱节。
- en: 'Ultimately, to sidestep the problem, we opted to remove from the dataset all
    trips paid for with cash. This modified the objective: to predict the incidence
    of tipping for only noncash payers. It always feels wrong to throw away data,
    but in this case we decided that under the new data-supported assumption that
    all cash-payment data was untrustworthy, the best option was to use the noncash
    data to answer a slightly different problem. Of course, there’s no guarantee that
    other tip records aren’t wrong as well, but we can at least check the new distribution
    of tip amounts. [Figure 6.6](#ch06fig06) shows the histogram of tip amounts after
    filtering out any cash-paid trips.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，为了规避这个问题，我们决定从数据集中移除所有用现金支付的车费行程。这改变了目标：只预测非现金支付者的给小费发生率。丢弃数据总是感觉不正确，但在这个案例中，我们决定在新的基于数据支持的假设下，即所有现金支付数据都是不可信的，最佳选择是使用非现金数据来回答一个稍微不同的问题。当然，不能保证其他小费记录也是正确的，但我们至少可以检查小费金额的新分布。[图6.6](#ch06fig06)显示了过滤掉任何现金支付行程后的小费金额直方图。
- en: Figure 6.6\. The distribution of tip amounts when omitting cash payments (after
    discovering that cash tips are never recorded in the system)
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 忽略现金支付时的小费金额分布（在发现现金小费从未在系统中记录后）
- en: '![](06fig06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6](06fig06.jpg)'
- en: 'With the bad data removed, the distribution is looking much better: only about
    5% of trips result in no tip. Our job in the next section is to find out why.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在移除不良数据后，分布看起来要好得多：只有大约5%的行程没有小费。在下一节中，我们的任务是找出原因。
- en: 6.2\. Modeling
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 建模
- en: With the data prepared for modeling, you can easily use your knowledge from
    [chapter 3](kindle_split_013.html#ch03) to set up and evaluate models. In the
    following subsections, you’ll build different versions of models, trying to improve
    the performance with each iteration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好用于建模的数据后，你可以轻松地运用你在[第3章](kindle_split_013.html#ch03)中学到的知识来设置和评估模型。在接下来的小节中，你将构建不同版本的模型，并尝试在每次迭代中提高性能。
- en: 6.2.1\. Basic linear model
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 基本线性模型
- en: You’ll start this modeling endeavor as simply as possible. You’ll work with
    a simple, logistic regression algorithm. You’ll also restrict yourself initially
    to the numerical values in the dataset, because those are handled by the logistic
    regression algorithm naturally, without any data preprocessing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你将以尽可能简单的方式开始这个建模工作。你将使用一个简单的逻辑回归算法。你最初也将自己限制在数据集中的数值上，因为这些数值可以由逻辑回归算法自然处理，无需任何数据预处理。
- en: You’ll use the scikit-learn and pandas libraries in Python to develop the model.
    Before building the models, we shuffled the instances randomly and split them
    into 80% training and 20% holdout testing sets. You also need to scale the data
    so no column is considered more important than others a priori. If the data has
    been loaded into a pandas `DataFrame`, the code to build and validate this model
    looks something like the following listing.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用Python中的scikit-learn和pandas库来开发模型。在构建模型之前，我们随机打乱了实例并将它们分成80%的训练集和20%的持有样本测试集。你还需要对数据进行缩放，这样就没有任何列在事先被认为是比其他列更重要。如果数据已经被加载到pandas的`DataFrame`中，构建和验证此模型的代码看起来如下所示。
- en: Listing 6.1\. Logistic regression tip-prediction model
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1。逻辑回归小费预测模型
- en: '![](ch06ex01-0.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex01-0.jpg)'
- en: '![](ch06ex01-1.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex01-1.jpg)'
- en: The last part of [listing 6.1](#ch06ex01) plots the ROC curve for this first,
    simple classifier. The holdout ROC curve is shown in [figure 6.7](#ch06fig07).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.1](#ch06ex01)的最后部分绘制了第一个简单分类器的ROC曲线。持有样本的ROC曲线显示在[图6.7](#ch06fig07)中。'
- en: Figure 6.7\. The receiver operating characteristic (ROC) curve of the logistic
    regression tip/no-tip classifier. With an area under the curve (AUC) of 0.5, the
    model seems to perform no better than random guessing. Not a good sign for our
    model.
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7。逻辑回归小费/无小费分类器的接收者操作特征（ROC）曲线。曲线下的面积（AUC）为0.5，模型似乎并不比随机猜测表现得好。这对我们的模型来说不是一个好迹象。
- en: '![](06fig07_alt.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig07_alt.jpg)'
- en: 'There’s no way around it: the performance of this classifier isn’t good! With
    a holdout AUC of 0.51, the model is no better than random guessing (flipping a
    coin weighted 95% “tip” and 5% “no tip” to predict each trip), which is, for obvious
    reasons, not useful. Luckily, we started out simply and have a few ways of trying
    to improve the performance of this model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 没有办法绕过这个问题：这个分类器的性能并不好！持有样本的AUC为0.51，模型的表现并不比随机猜测（抛一个95%“小费”和5%“无小费”的硬币来预测每次行程）好，这显然是没有用的。幸运的是，我们一开始就很简单，有几种方法可以尝试提高这个模型的表现。
- en: 6.2.2\. Nonlinear classifier
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2. 非线性分类器
- en: The first thing you’ll try is to switch to a different algorithm—one that’s
    nonlinear. Considering how poor the first attempt was, it seems that a linear
    model won’t cut it for this dataset; simply put, tipping is a complicated process!
    Instead, you’ll use a nonlinear algorithm called *random forest*, well known for
    its high level of accuracy on real-world datasets. You could choose any of a number
    of other algorithms (see the [appendix](kindle_split_022.html#app01)), but we’ll
    leave it as an exercise for you to evaluate and compare different algorithms.
    Here’s the code (relative to the previous model) for building this model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先尝试的是切换到一个不同的算法——一个非线性算法。考虑到第一次尝试的结果很差，似乎线性模型对于这个数据集来说是不够用的；简单来说，小费是一个复杂的过程！相反，你将使用一个名为*随机森林*的非线性算法，它在现实世界数据集上以其高精度而闻名。你可以选择许多其他算法（见[附录](kindle_split_022.html#app01)），但我们将其留给你作为评估和比较不同算法的练习。以下是构建此模型的代码（相对于前一个模型）。
- en: Listing 6.2\. Random forest tip-prediction model
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2。随机森林小费预测模型
- en: '![](139fig01_alt.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](139fig01_alt.jpg)'
- en: The results of running the code in [listing 6.2](#ch06ex02) are shown in [figure
    6.8](#ch06fig08). You can see a significant increase in holdout accuracy—the holdout
    AUC is now 0.64—showing clearly that there’s a predictive signal in the dataset.
    Some combinations of the input features are capable of predicting whether a taxi
    trip will yield any tips from the passenger. If you’re lucky, further feature
    engineering and optimization will be able to boost the accuracy levels even higher.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.2](#ch06ex02)中运行代码的结果显示在[图6.8](#ch06fig08)中。你可以看到持有样本准确率有显著提高——持有样本的AUC现在为0.64——清楚地表明数据集中存在预测信号。一些输入特征的组合能够预测出租车行程是否会收到乘客的小费。如果你很幸运，进一步的特征工程和优化甚至能够进一步提高准确率水平。'
- en: 'Figure 6.8\. The ROC curve of the nonlinear random forest model. The AUC is
    significantly better: at 0.64, it’s likely that there’s a real signal in the dataset.'
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8。非线性随机森林模型的ROC曲线。AUC显著提高：达到0.64，数据集中很可能存在真实信号。
- en: '![](06fig08_alt.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig08_alt.jpg)'
- en: 'You can also use the model to gain insight into what features are most important
    in this moderately predictive model. This exercise is a crucial step for a couple
    of reasons:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用这个模型来了解在这个中等预测模型中哪些特征最重要。这个练习有几个关键原因：
- en: It enables you to identify any cheating features (for example, the problem with
    noncash payers) and to use that as insight to rectify any issues.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使你能够识别任何作弊特征（例如，非现金支付者的问题），并利用这些作为洞察来纠正任何问题。
- en: It serves as a launching point for further feature engineering. If, for instance,
    you identify latitude and longitude as the most important features, you can consider
    deriving other features from those metrics, such as distance from Times Square.
    Likewise, if there’s a feature that you thought would be important but it *doesn’t*
    appear on the top feature list, then you’ll want to analyze, visualize, and potentially
    clean up or transform that feature.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它作为进一步特征工程的一个起点。例如，如果你将经纬度识别为最重要的特征，你可以考虑从这些指标中推导出其他特征，例如从时代广场的距离。同样，如果你认为某个特征很重要，但它*没有*出现在顶级特征列表中，那么你将想要分析、可视化和可能清理或转换该特征。
- en: '[Figure 6.9](#ch06fig09) (also generated by the code in [listing 6.2](#ch06ex02))
    shows the list of features and their relative importance for the random forest
    model. From this figure, you can see that the location features are the most important,
    along with time, trip distance, and fare amount. It may be that riders in some
    parts of the city are less patient with slow, expensive rides, for example. You’ll
    look more closely at the potential insights gained in [section 6.2.5](#ch06lev2sec7).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.9](#ch06fig09)（由[列表6.2](#ch06ex02)中的代码生成）显示了随机森林模型的特征列表及其相对重要性。从这张图中，你可以看到位置特征是最重要的，其次是时间、行程距离和车费金额。可能的情况是，城市某些地区的乘客对缓慢、昂贵的行程不太有耐心。你将在[第6.2.5节](#ch06lev2sec7)中更仔细地研究获得的可能见解。'
- en: Figure 6.9\. The important features of the random forest model. The drop-off
    and pickup location features seem to dominate the model.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9。随机森林模型的重要特征。上下车位置特征似乎主导了模型。
- en: '![](06fig09.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig09.jpg)'
- en: Now that you’ve chosen the algorithm, let’s make sure you’re using all of the
    raw features, including categorical columns and not just plain numerical columns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经选择了算法，让我们确保你正在使用所有原始特征，包括分类列，而不仅仅是普通的数值列。
- en: 6.2.3\. Including categorical features
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3。包括分类特征
- en: Without going deeper into the realm of feature engineering, you can perform
    some simple data preprocessing to increase the accuracy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在不深入特征工程领域的情况下，你可以执行一些简单的数据预处理来提高准确性。
- en: 'In [chapter 2](kindle_split_012.html#ch02), you learned how to work with categorical
    features. Some ML algorithms work with categorical features directly, but you’ll
    use the common trick of “Booleanizing” the categorical features: creating a column
    of value 0 or 1 for each of the possible categories in the feature. This makes
    it possible for any ML algorithm to handle categorical data without changes to
    the algorithm itself.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_012.html#ch02)中，你学习了如何处理分类特征。一些机器学习算法可以直接处理分类特征，但你将使用常见的“布尔化”分类特征的技巧：为特征中每个可能的类别创建一个值为0或1的列。这使得任何机器学习算法都能在不改变算法本身的情况下处理分类数据。
- en: The code for converting all of the categorical features is shown in the following
    listing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 转换所有分类特征的代码如下所示。
- en: Listing 6.3\. Converting categorical columns to numerical features
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3。将分类列转换为数值特征
- en: '![](141fig01_alt.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](141fig01_alt.jpg)'
- en: After creating the Booleanized columns, you run the data through [listing 6.2](#ch06ex02)
    again and obtain the ROC curve and feature importance list shown in [figure 6.10](#ch06fig10).
    Note that your holdout AUC has risen slightly, from 0.64 to 0.656.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建布尔化列之后，你再次将数据通过[列表6.2](#ch06ex02)运行，并获得图6.10中显示的ROC曲线和特征重要性列表。请注意，你的保留AUC略有上升，从0.64上升到0.656。
- en: Figure 6.10\. The ROC curve and feature importance list of the random forest
    model with all categorical variables converted to Boolean (0/1) columns, one per
    category per feature. The new features are bringing new useful information to
    the table, because the AUC is seen to increase from the previous model without
    categorical features.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10。将所有分类变量转换为布尔（0/1）列的随机森林模型的ROC曲线和特征重要性列表。新特征为表格带来了新的有用信息，因为AUC从之前没有分类特征的模型中观察到有所增加。
- en: '![](06fig10_alt.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig10_alt.jpg)'
- en: As model performance increases, you can consider additional factors. You haven’t
    done any real feature engineering, of course, because the data transformations
    applied so far are considered basic data preprocessing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型性能的提高，你可以考虑额外的因素。当然，你还没有进行任何真正的特征工程，因为到目前为止应用的数据转换被认为是基本的数据预处理。
- en: 6.2.4\. Including date-time features
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4\. 包含日期时间特征
- en: At this point, it’s time to start working with the data to produce new features,
    what you’ve previously known as *feature engineering*. In [chapter 5](kindle_split_015.html#ch05),
    we introduced a set of date-time features transforming date and timestamps into
    numerical columns. You can easily imagine the time of the day or day of the week
    to have some kind of influence on how a passenger will tip.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，是时候开始使用数据来生成新的特征了，你之前所知道的称为*特征工程*。在[第 5 章](kindle_split_015.html#ch05)中，我们介绍了一系列将日期和时间戳转换为数值列的日期时间特征。你可以很容易地想象一天中的时间或一周中的某一天可能会对乘客的小费产生影响。
- en: The code for calculating these features is presented in the following listing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这些特征的代码在下面的列表中给出。
- en: Listing 6.4\. Date-time features
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4\. 日期时间特征
- en: '![](142fig01_alt.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](142fig01_alt.jpg)'
- en: With these date-time features, you can build a new model. You run the data through
    the code in [listing 6.2](#ch06ex02) once again and obtain the ROC curve and feature
    importance shown in [figure 6.11](#ch06fig11).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些日期时间特征，你可以构建一个新的模型。你再次将数据通过[列表 6.2](#ch06ex02)中的代码运行，并获取[图 6.11](#ch06fig11)中显示的
    ROC 曲线和特征重要性。
- en: Figure 6.11\. The ROC curve and feature importance list for the random forest
    model, including all categorical features and additional date-time features
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.11\. 包含所有分类特征和附加日期时间特征的随机森林模型的 ROC 曲线和特征重要性列表
- en: '![](06fig11_alt.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig11_alt.jpg)'
- en: 'You can see an evolution in the accuracy of the model with additional data
    preprocessing and feature engineering. At this point, you’re able to predict whether
    a passenger will tip the driver with an accuracy significantly above random. Up
    to now, you’ve looked only at improving the data in order to improve the model,
    but you can try to improve this model in two other ways:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，随着额外的数据预处理和特征工程的加入，模型的准确性有所提高。到目前为止，你只关注了改进数据以改进模型，但你还可以尝试以两种其他方式改进这个模型：
- en: Vary the model parameters to see whether the default values aren’t necessarily
    the most optimal
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型参数以查看默认值是否一定是最佳选择
- en: Increase the dataset size
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加数据集大小
- en: In this chapter, we’ve been heavily subsampling the dataset in order for the
    algorithms to handle the dataset, even on a 16 GB–memory machine. We’ll talk more
    about scalability of methods in [chapters 9](kindle_split_020.html#ch09) and [10](kindle_split_021.html#ch10),
    but in the meantime we’ll leave it to you to work with this data to increase the
    cross-validated accuracy even further!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为了使算法能够处理数据集，甚至是在 16 GB 内存机器上，对数据集进行了大量子采样。我们将在[第 9 章](kindle_split_020.html#ch09)和[第
    10 章](kindle_split_021.html#ch10)中更多地讨论方法的可扩展性，但在此期间，我们将这项工作留给你，以便你使用这些数据进一步提高交叉验证的准确性！
- en: 6.2.5\. Model insights
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5\. 模型洞察
- en: It’s interesting to gain insight about the data through the act of building
    a model to predict a certain answer. From the feature importance list, you can
    understand which parameters have the most predictive power, and you use that to
    look at the data in new ways. In our initial unsuccessful attempt, it was because
    of inspection of the feature importance list that we discovered the problem with
    the data. In the current working model, you can also use the list to inspire some
    new visualizations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建一个预测特定答案的模型来深入了解数据是很有趣的。从特征重要性列表中，你可以了解哪些参数具有最大的预测能力，并利用这一点以新的方式查看数据。在我们最初的失败尝试中，正是因为检查了特征重要性列表，我们发现了数据中的问题。在当前的工作模型中，你也可以使用这个列表来启发一些新的可视化。
- en: At every iteration of our model in this section, the most important features
    have been the pickup and drop-off location features. [Figure 6.12](#ch06fig12)
    plots the geographical distribution of drop-offs that yield tips from the passenger,
    as well as drop-offs from trips that don’t.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中我们模型的每一次迭代中，最重要的特征都是接车和下车的位置特征。[图 6.12](#ch06fig12)绘制了产生小费的乘客的下车地点的地理分布，以及没有产生小费的行程的下车地点。
- en: Figure 6.12\. The geographical distribution of drop-offs
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.12\. 乘客上下车的地理分布
- en: '![](06fig12_alt.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig12_alt.jpg)'
- en: '[Figure 6.12](#ch06fig12) shows an interesting trend of not tipping when being
    dropped off closer to the center of the city. Why is that? One possibility is
    that the traffic situation creates many slow trips, and the passenger isn’t necessarily
    happy with the driver’s behavior. As a non–US-citizen, I have another theory.
    This particular area of the city has a high volume of both financial workers and
    tourists. We’d expect the financial group to be distributed farther south on Manhattan.
    There’s another reason that tourists are the most likely cause of this discrepancy,
    in my mind: many countries have vastly different rules for tipping than in the
    United States. Some Asian countries almost never tip, and many northern European
    countries tip much less, and rarely in taxis. You can make many other interesting
    investigations based on this dataset. The point is, of course, that real-world
    data can often be used to say something interesting about the real world and the
    people generating the data.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12](#ch06fig12) 显示了在靠近城市中心下车时不给小费的有趣趋势。为什么会有这种情况？一个可能的原因是交通状况造成了许多缓慢的行程，乘客可能并不满意司机的行为。作为一个非美国公民，我还有另一个理论。这个城市的特定区域金融工作者和游客数量都很多。我们预计金融群体会在曼哈顿南部更远的地方分布。在我看来，游客很可能是造成这种差异的最可能原因：许多国家对于小费的规定与美国大相径庭。一些亚洲国家几乎从不给小费，许多北欧国家小费较少，且很少在出租车中给小费。你可以基于这个数据集进行许多其他有趣的调查。当然，重点是，现实世界的数据经常可以用来对现实世界以及生成数据的人说些有趣的事情。'
- en: 6.3\. Summary
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 概述
- en: 'This chapter introduced a dataset from the real world and defined a problem
    suitable for the machine-learning knowledge that you’ve built up over the previous
    five chapters. You went through the entire ML workflow, including initial data
    preparation, feature engineering, and multiple iterations of model building, evaluation,
    optimization, and prediction. The main takeaways from the chapter are these:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一个来自现实世界的数据集，并定义了一个适合你在前五章中积累的机器学习知识的问题。你经历了整个机器学习工作流程，包括初始数据准备、特征工程以及模型构建、评估、优化和预测的多次迭代。本章的主要收获如下：
- en: With more organizations producing vast amounts of data, increasing amounts of
    data are becoming available within organizations, if not publicly.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着更多组织产生大量数据，组织内部可用的数据量也在不断增加，即使不是公开的。
- en: Records of all taxi trips from NYC in 2013 have been released publicly. A lot
    of taxi trips occur in NYC in one year!
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2013年纽约市所有出租车行程的记录已公开发布。纽约市一年中发生的出租车行程数量真是惊人！
- en: Real-world data can be messy. Visualization and knowledge about the domain helps.
    Don’t get caught in too-good-to-be-true scenarios and don’t make premature assumptions
    about the data.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的数据可能很混乱。可视化和对领域的了解有帮助。不要陷入过于完美的场景，也不要对数据做出过早的假设。
- en: Start iterating from the simplest possible model. Don’t spend time on premature
    optimization. Gradually increase complexity.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最简单的模型开始迭代。不要在过早优化上浪费时间。逐渐增加复杂性。
- en: Make choices and move on; for example, choose an algorithm early on. In an ideal
    world, you’d try all combinations at all steps in the iterative process of building
    a model, but you’d have to fix some things in order to make progress.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 做出选择并继续前进；例如，尽早选择一个算法。在理想的世界里，你会在构建模型的迭代过程的每一步都尝试所有组合，但你必须固定一些东西才能取得进展。
- en: Gain insights into the model and the data in order to learn about the domain
    and potentially improve the model further.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了了解领域并可能进一步改进模型，深入了解模型和数据。
- en: 6.4\. Terms from this chapter
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4. 本章术语
- en: '| Word | Definition |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 词汇 | 定义 |'
- en: '| --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| open data | Data made available publicly by institutions and organizations.
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 开放数据 | 由机构和组织公开提供的数据。|'
- en: '| FOIL | Freedom of Information Law. (The federal version is known as the Freedom
    of Information Act, or FOIA.) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| FOIL | 信息自由法。（联邦版本被称为信息自由法案，或FOIA。）|'
- en: '| too-good-to-be-true scenario | If a model is extremely accurate compared
    to what you would have thought, chances are that some features in the model, or
    some data peculiarities, are causing the model to “cheat.” |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 过于完美的场景 | 如果一个模型与你的预期相比极其准确，那么很可能是模型中的某些特征或某些数据特性导致模型“作弊”。|'
- en: '| premature assumptions | Assuming something about the data without validation,
    risking biasing your views of the results. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 过早的假设 | 在未经验证的情况下对数据进行假设，可能会使你对结果的看法产生偏差。|'
- en: Chapter 7\. Advanced feature engineering
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. 高级特征工程
- en: '*This chapter covers*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using advanced feature-engineering concepts to increase the accuracy of your
    machine-learning system
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级特征工程概念来提高你的机器学习系统的准确性
- en: Extracting valuable features from text by using natural-language-processing
    techniques
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用自然语言处理技术从文本中提取有价值的特征
- en: Extracting meaning from images and using them as features in your machine-learning
    project
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中提取意义并在你的机器学习项目中使用它们作为特征
- en: You explored the basic concepts behind feature engineering in [chapter 5](kindle_split_015.html#ch05)
    and applied simple feature-engineering techniques to real-world data in [chapter
    6](kindle_split_017.html#ch06). In this chapter, you’ll look at more-sophisticated
    techniques that you can use when faced with types of data that have become common
    in today’s world. The two most important of these are text and images. This chapter
    presents advanced techniques for extracting features from text and image data,
    in order to use this data in your machine-learning pipelines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[第5章](kindle_split_015.html#ch05)中探索了特征工程的基本概念，并在[第6章](kindle_split_017.html#ch06)中应用了简单的特征工程技术到现实世界的数据中。在本章中，你将了解更复杂的技巧，这些技巧可以用于面对当今世界常见的各种数据类型。其中最重要的两种是文本和图像。本章介绍了从文本和图像数据中提取特征的高级技术，以便在机器学习管道中使用这些数据。
- en: 7.1\. Advanced text features
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 高级文本特征
- en: You already looked at simple feature engineering for text data in [chapter 5](kindle_split_015.html#ch05).
    This section provides more details about the ideas behind these techniques, and
    presents more-advanced concepts that can improve the accuracy of your models even
    further.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在[第5章](kindle_split_015.html#ch05)中了解了文本数据的简单特征工程，本节提供了关于这些技术背后的思想的更多细节，并介绍了可以进一步提高模型准确性的更高级概念。
- en: Recall that your mission in extracting features from text is to somehow convert
    texts of various lengths and words into a common set of features. In [chapter
    5](kindle_split_015.html#ch05), you learned about the bag-of-words representation,
    in which you count the occurrences of words across all texts and use the counts
    of the top-N words as N new features. This work of transforming natural-language
    text into machine-usable data is commonly referred to as *natural language processing*,
    or NLP.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，从文本中提取特征的任务是将各种长度和单词的文本转换为一种共同的特征集。在[第5章](kindle_split_015.html#ch05)中，你学习了词袋表示法，其中你统计了所有文本中单词的出现次数，并使用前N个单词的计数作为N个新的特征。将自然语言文本转换为机器可用的数据这一工作通常被称为*自然语言处理*，或NLP。
- en: 7.1.1\. Bag-of-words model
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 词袋模型
- en: '*Bag of words* is one of the simplest but also most widely used techniques
    in NLP. It’s a great approach to start with for any text-based problem. It’s also
    the basis of many other more advanced methods that you’ll look at later in this
    chapter. You’ll learn about this model in two parts: first, tokenization and transformation,
    and then vectorization.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*词袋*是NLP中最简单但也是最广泛使用的技术之一。对于任何基于文本的问题，它都是一个很好的起点。它也是本章后面将要探讨的许多其他更高级方法的基础。你将分两部分了解这个模型：首先，标记化和转换，然后是向量化。'
- en: Tokenization and Transformation
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标记化和转换
- en: The splitting of a text into pieces is known as *tokenization*. The most common
    way to split is on words, but in some cases (for example, in character-based languages),
    you may want to split on characters or split on pairs or groups of words or even
    something more advanced. Groups of words in a split are known as *n-grams*. Two-
    or three-word combinations are known as *bigrams* and *trigrams*, respectively
    (and they’re the most common after one-word *unigrams*). Bigrams in the example
    in [figure 7.1](#ch07fig01) include “the lazy,” “brown fox,” and so forth. Trigrams
    include “brown fox jumps” and “jumps over the.”
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本分割成片段的过程称为*标记化*。最常见的分割方式是基于单词，但在某些情况下（例如，在基于字符的语言中），你可能希望基于字符或基于单词对或单词组进行分割，甚至更高级的分割。分割中的单词组称为*n-gram*。两个或三个单词的组合分别称为*bigram*和*trigram*（它们在单词*unigram*之后是最常见的）。[图7.1](#ch07fig01)中的例子中的bigram包括“the
    lazy”、“brown fox”等等。trigram包括“brown fox jumps”和“jumps over the”。
- en: Figure 7.1\. The initial steps in the bag-of-words extraction algorithm
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1\. 词袋提取算法的初始步骤
- en: '![](07fig01_alt.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig01_alt.jpg)'
- en: Expanding to multiple words may help your models in some cases, by offering
    more contextualization of the text. But using multiple words also typically inflates
    the number of features quite dramatically. In practice, you usually start with
    only unigram representations. If you want to move to higher-grade grams, you have
    to make sure to use an ML algorithm that handles sparse data. You’ll learn more
    about that in the following subsection.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，扩展到多个单词可能有助于你的模型，因为它提供了更多的文本上下文。但使用多个单词通常也会显著增加特征数量。在实践中，你通常只从单词表示开始。如果你想转向更高阶的语法，你必须确保使用能够处理稀疏数据的ML算法。你将在下一个小节中了解更多关于这一点。
- en: The next step in our bag-of-words algorithm is to make any *transformations*
    necessary to the tokens extracted from the text. A good example of a transformation
    is converting all words to lowercase, such that you don’t produce features for
    both “fox” and “Fox,” which may add to the noise of the model. In some cases,
    however, you may want to preserve the case, if it makes sense in your project
    (for example, if proper names are common in the text and highly predictive, or
    if ALL CAPS is meaningful). *Stemming*—which strips word suffixes—can also be
    a powerful transformation for extracting more signals out of different words with
    similar meanings. Using stemming, for instance, causes the words “jump,” “jumping,”
    “jumps,” and “jumped” to all be expressed as the token “jump” in your dictionary.
    Other transformations such as custom handling of numbers, punctuation, and special
    characters can also be useful, depending on the text at hand.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们词袋算法的下一步是对从文本中提取的标记进行必要的*转换*。一个很好的转换例子是将所有单词转换为小写，这样你就不会为“fox”和“Fox”产生特征，这可能会增加模型的噪声。然而，在某些情况下，你可能想保留大小写，如果这在你的项目中是有意义的（例如，如果文本中常见的专有名词具有高度预测性，或者如果ALL
    CAPS有意义）。*词干提取*——去除单词后缀——对于从具有相似意义的不同单词中提取更多信号也是一种强大的转换。例如，使用词干提取，单词“jump”、“jumping”、“jumps”和“jumped”在你的字典中都会表示为标记“jump”。其他如自定义处理数字、标点符号和特殊字符的转换也可能很有用，具体取决于文本内容。
- en: Next, you can define the dictionary that you’ll generate your text features
    from. For machine-learning projects, it’s common to set a limit on the number
    of features, hence the number of words, in your dictionary. This is usually done
    by sorting by the word occurrences and using only the top-N words.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以定义你将从中生成文本特征的字典。在机器学习项目中，通常会对字典中的特征数量（即单词数量）设置一个限制。这通常是通过按单词出现次数排序，并仅使用前N个单词来完成的。
- en: Vectorization
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量化
- en: You can use your bag-of-words dictionary to generate features to use in your
    ML models. After defining the dictionary, you can convert any text to a set of
    numbers corresponding to the occurrences of each dictionary word in the text.
    [Figure 7.2](#ch07fig02) shows this process, which is called *vectorization*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你的词袋字典来生成用于你的ML模型的特征。在定义字典后，你可以将任何文本转换为与文本中每个字典单词出现次数相对应的数字集合。[图7.2](#ch07fig02)显示了这一过程，这个过程被称为*向量化*。
- en: Figure 7.2\. Using the vocabulary, you can now represent each text as a list
    of numbers. The rows show the count for the two small texts in [figure 7.1](#ch07fig01)
    and the count for the Wikipedia page about the sentence “The quick brown fox jumps
    over the lazy dog,” which is an English pangram (it includes all letters in the
    English alphabet).
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 使用词汇表，你现在可以将每个文本表示为一个数字列表。行显示了[图7.1](#ch07fig01)中两个小文本的计数以及关于句子“The quick
    brown fox jumps over the lazy dog”的维基百科页面的计数，这是一个英语回文句（它包含了英语字母表中的所有字母）。
- en: '![](07fig02_alt.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![07fig02_alt.jpg](07fig02_alt.jpg)'
- en: But there’s a problem that we haven’t discussed yet. Most natural-language texts
    include many words that aren’t important for understanding the topic, but are
    simply “filling.” These include words such as “the,” “is,” and “and.” In NLP research,
    these are called *stop words*, and they’re usually removed from the dictionary
    as they typically aren’t highly predictive of anything interesting and can dilute
    the more meaningful words that are important from an ML perspective. With our
    words already sorted by occurrences, the usual way to remove stop words is to
    throw away all words with more occurrences than a certain word-count threshold.
    [Figure 7.2](#ch07fig02) shows an example; a larger text (the third row in the
    figure) has a much larger count of the word “the” than any of the other words.
    The challenge, then, is to define the threshold at which a particular word is
    a stop word and not a meaningful word. Most NLP libraries, such as the NLTK Python
    library, include prebuilt stop-word lists for a range of languages so you don’t
    have to do this every time. In some cases, though, the list of stop words will
    be different for your specific project, and you’ll need to choose a stop-word
    threshold (a standard choice is to exclude any words that appear in more than
    90% of all documents).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一个问题我们尚未讨论。大多数自然语言文本包含许多对理解主题不重要的词，它们只是“填充”的。这些词包括“the”、“is”和“and”等。在自然语言处理研究中，这些被称为*停用词*，通常从字典中移除，因为它们通常不高度预测任何有趣的内容，并且可能会稀释从机器学习角度来看重要的更有意义的词。在我们的单词已经按出现次数排序的情况下，移除停用词的常用方法是丢弃所有出现次数超过某个词数阈值的单词。[图7.2](#ch07fig02)展示了示例；较大的文本（图中的第三行）中“the”这个词的出现次数比其他任何词都要多。因此，挑战在于定义一个阈值，以确定一个特定的词是停用词而不是有意义的词。大多数自然语言处理库，如NLTK
    Python库，都包含一系列语言的预构建停用词列表，这样你就不必每次都这样做。然而，在某些情况下，停用词列表将因你的特定项目而异，你需要选择一个停用词阈值（一个标准的选择是排除任何在所有文档中出现超过90%的单词）。
- en: Although not apparent in [figure 7.2](#ch07fig02), any realistic dictionary
    will have many words, and usually only a small subset of those will be present
    in the texts that you’re generating features for. This combination usually makes
    text features include lots of zeros. Only a small number of the dictionary words
    will be found in a given text, so we call the bag-of-words features *sparse*.
    If you have many sparse features (it’s common to have 1,000 features with only
    a small percent nonzero elements), it’s a good idea to choose an ML algorithm
    that can handle sparse features natively, or an algorithm that can deal with many
    low-significance features without sacrificing accuracy. The naïve Bayes algorithms
    in the scikit-learn Python library handle sparse data natively, and are therefore
    well suited for text-classification problems. Algorithms such as random forest
    are known to handle lots of low-significance features well, although your mileage
    may vary. You should always test the efficacy of different methods by using the
    evaluation and optimization techniques discussed in [chapter 4](kindle_split_014.html#ch04).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在[图7.2](#ch07fig02)中不明显，但任何现实的字典都会有许多词，通常只有其中的一小部分会出现在你为生成特征而创建的文本中。这种组合通常会使文本特征包含很多零。在给定的文本中，只有少数字典词会被找到，因此我们称这些词为“词袋”特征*稀疏的*。如果你有很多稀疏特征（通常有1,000个特征，只有一小部分非零元素），选择一个可以原生处理稀疏特征的机器学习算法，或者一个可以处理许多低显著性特征而不牺牲准确性的算法是个好主意。scikit-learn
    Python库中的朴素贝叶斯算法可以原生处理稀疏数据，因此非常适合文本分类问题。像随机森林这样的算法已知可以很好地处理许多低显著性特征，尽管效果可能会有所不同。你应该始终通过使用第4章中讨论的评估和优化技术来测试不同方法的功效。
- en: 7.1.2\. Topic modeling
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 主题建模
- en: The bag-of-words method is simple to understand and implement. But other, more-advanced
    methods could lead to big increases in ML model accuracy. This section introduces
    three of those methods.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋方法简单易懂且易于实现。但其他更高级的方法可能导致机器学习模型准确性的大幅提升。本节介绍了这三种方法中的三种。
- en: One problem with the bag-of-words model is the nature of simple word counts.
    If a certain word (not a stop word) is common in a corpus—for example, the word
    “data” in a corpus of ML papers—it’s not necessarily informative to know that
    the word also appears in a new text. Instead, you’d do better by focusing on relatively
    rare words that are more highly predictive of the outcome of interest. To this
    end, it’s common to scale the word counts by the inverse of the total count of
    that word in the corpus. Because you want to describe a text the best you can
    using only numbers, and a word that isn’t abundant in the training corpus but
    *is* abundant in a new document is likely more indicative of the meaning of the
    new document, you’re better off giving preferential treatment to that rare word.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型的一个问题是简单词频的性质。如果某个词（不是停用词）在语料库中很常见——例如，在机器学习论文语料库中的“数据”一词——知道这个词也出现在新文本中并不一定有信息量。相反，你最好关注那些相对罕见且更能预测感兴趣的结果的词。为此，通常通过该词在语料库中的总计数倒数来缩放词频。因为你只想用数字尽可能好地描述文本，而在训练语料库中不常见但在新文档中常见的词可能更能表明新文档的意义，所以你最好优先考虑这个罕见的词。
- en: Term frequency-inverse document frequency
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 词频-逆文档频率
- en: A commonly used algorithm that tries to solve this exact problem is called *term
    frequency–inverse document frequency*, or *tf-idf* for short. This algorithm is
    calculated as a product of the term frequency (tf) and the inverse document frequency
    (idf).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一种试图解决这个确切问题的常用算法被称为**词频-逆文档频率**，简称**tf-idf**。这个算法是通过词频（tf）和逆文档频率（idf）的乘积来计算的。
- en: The tf can be calculated in different ways, but the simplest is to use the number
    of times a word occurs in a particular document. It’s also common to use other
    versions of the tf factor, such as binary (1 if the word is in a document, and
    0 otherwise) and logarithmic (1 + log[tf]).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: tf 可以用不同的方式计算，但最简单的是使用一个词在特定文档中出现的次数。也常见使用 tf 因子的其他版本，例如二进制（如果词在文档中，则为 1，否则为
    0）和对数（1 + log[tf]）。
- en: 'The inverse document frequency is calculated as the logarithm of the total
    number of documents, divided by the number of documents that contain the term,
    so that relatively uncommon words attain higher values. In its simplest form,
    the tf-idf equation looks like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率是通过总数为文档总数的对数除以包含该词的文档数来计算的，这样相对不常见的词就能获得更高的值。在它的最简单形式中，tf-idf 方程看起来是这样的：
- en: '![](150equ01_alt.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](150equ01_alt.jpg)'
- en: Tf-idf can be powerful for generating good ML features from any corpus of text.
    It can also be useful in other areas, such as search. Because you’re generating
    a vector of numbers for any document, you can also find “distances” between documents,
    as distances between their tf-idf vector representations. If the user search query
    is a document, you can find the distances between any other documents in your
    dataset in this way, and hence return a ranked list of documents to the user based
    on the query. [Listing 7.1](#ch07ex01) in the next section shows how to use the
    scikit-learn Python library to generate tf-idf vectors from documents, along with
    a more advanced technique called *latent semantic indexing*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Tf-idf 可以从任何文本语料库中生成好的机器学习特征。它也可以在其他领域有用，例如搜索。因为为任何文档生成一个数字向量，你也可以找到文档之间的“距离”，就像它们
    tf-idf 向量表示之间的距离一样。如果用户的搜索查询是一个文档，你可以以这种方式找到数据集中任何其他文档之间的距离，从而根据查询返回文档的排名列表。[下一节中的列表
    7.1](#ch07ex01) 展示了如何使用 scikit-learn Python 库从文档中生成 tf-idf 向量，以及一种更高级的技术，称为**潜在语义索引**。
- en: Latent semantic analysis
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隐含语义分析
- en: '*Latent semantic analysis*, or LSA (also commonly called *latent semantic indexing*,
    or LSI) is a more sophisticated method of topic modeling. It’s also more advanced
    both conceptually and computationally. The idea is to use the bag-of-word counts
    to build a term-document matrix, with a row for each term and a column for each
    document. The elements of this matrix are then normalized similarly to the tf-idf
    process in order to avoid frequent terms dominating the power of the matrix.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义分析**，或 LSA（也常称为**潜在语义索引**，或 LSI），是一种更复杂的话题建模方法。它在概念上和计算上也都更先进。想法是使用词袋计数来构建一个词-文档矩阵，其中每一行代表一个词，每一列代表一个文档。然后，这个矩阵的元素被规范化，类似于
    tf-idf 过程，以避免频繁的词主导矩阵的权重。'
- en: The main trick of the LSA algorithm is in its notion of a concept. A *concept*
    is a pattern of similar terms in the document corpus. For example, the concept
    of “dog” may have related terms (words, in this case) of “barking,” “leash,” and
    “kennel.” The algorithm doesn’t label the concept “dog” but instead figures out
    which words are related by their co-occurrence in documents and then ascertains
    that these words are connected through a certain abstract concept. The word “dog”
    may itself be an important term related to the “dog” concept. These topics are
    considered *hidden* or *latent* in the data, hence the name *latent* semantic
    analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LSA算法的主要技巧在于其对概念的理解。**概念**是文档语料库中相似术语的模式。例如，“dog”的概念可能有关联术语（在这种情况下是单词）如“barking”，“leash”和“kennel”。算法不会标记“dog”这个概念，而是通过分析文档中术语的共现来找出哪些单词相关联，然后确定这些单词通过某种抽象概念相连。单词“dog”本身可能是一个与“dog”概念相关的重要术语。这些主题被认为是数据中的**隐藏**或**潜在**的，因此得名**潜在语义分析**。
- en: LSA uses *singular value decomposition* (SVD)^([[1](#ch07fn01)])—a well-known
    mathematical tool—to split the term-document matrix (*A*) into three matrices
    (*T*,*S*,*D*). *T* is the term-concept matrix that relates the terms (for example,
    “barking” and “kennel”) to concepts (for example, “dog”), and *D* is the concept-document
    matrix that relates individual documents to concepts that you’ll later use to
    extract the features from the LSA model. The *S* matrix holds the singular values.
    In LSA, these denote the relative importance that a term has to a document. In
    the same way as you restricted the number of features in the bag-of-words and
    tf-idf algorithms, you can now select the top singular values and restrict the
    feature space to something more manageable; recall that the term-document matrix
    (*[A]*) can be extremely large and sparse.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: LSA使用**奇异值分解**（SVD）^([[1](#ch07fn01)])——一种著名的数学工具——将词-文档矩阵（*A*）分解为三个矩阵（*T*，*S*，*D*）。*T*是词-概念矩阵，它将术语（例如，“barking”和“kennel”）与概念（例如，“dog”）相关联，而*D*是概念-文档矩阵，它将单个文档与你在以后用于从LSA模型中提取特征的概念相关联。*S*矩阵持有奇异值。在LSA中，这些表示一个术语对一个文档的相对重要性。与你在词袋模型和tf-idf算法中限制特征数量相同，你现在可以选择前几个奇异值，并将特征空间限制在更易于管理的范围内；回想一下，词-文档矩阵（*[A]*）可以非常大且稀疏。
- en: ¹
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For readers familiar with principal component analysis (which is presented later
    in this chapter), SVD is the same technique that enables you to compute PCA coordinates
    from a dataset. You can think of LSA as “PCA for bag of words.”
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于熟悉主成分分析（PCA，本章后面将介绍）的读者来说，SVD是使你能够从数据集中计算PCA坐标的相同技术。你可以将LSA视为“词袋的PCA”。
- en: 'Using the top-N components of the SVD, you generate N features for your ML
    model by taking the corresponding rows from the concept-document matrix (*D*).
    When new documents come in for prediction, you can generate a new set of features
    from the previously learned LSA model by performing the matrix multiplication:
    *D* = *A^TTS*^(–1). Here *A^T* is the word count (or tf-idf), using the defined
    dictionary, for the new document, and *T* and *S* are the term-concept and singular-value
    matrices from the SVD.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVD的前N个成分，你可以通过从概念-文档矩阵（*D*）中取出相应的行来为你的机器学习模型生成N个特征。当有新的文档用于预测时，你可以通过执行矩阵乘法：*D*
    = *A^TTS*^(–1)来从先前学习的LSA模型中生成一组新的特征。在这里，*A^T*是新文档的词频（或tf-idf），使用定义的词典，而*T*和*S*是SVD中的词-概念和奇异值矩阵。
- en: Although it’s useful to understand the principles of LSA, not everyone knows
    linear algebra well enough to do these calculations. Luckily, plenty of implementations
    can readily be used in your ML project. The scikit-learn Python library includes
    the functionality needed to run LSA by (1) using tf-idf to generate the term-document
    matrix, (2) performing the matrix decomposition, and (3) transforming the documents
    to vectors, as shown in the following listing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理解LSA的原则很有用，但并不是每个人都足够熟悉线性代数来做这些计算。幸运的是，有很多实现可以方便地用于你的机器学习项目。scikit-learn
    Python库包括运行LSA所需的功能，通过（1）使用tf-idf生成词-文档矩阵，（2）执行矩阵分解，（3）将文档转换为向量，如下所示。
- en: Listing 7.1\. Latent semantic analysis using scikit-learn
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. 使用scikit-learn进行潜在语义分析
- en: '![](151fig01_alt.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](151fig01_alt.jpg)'
- en: Next, you’ll look at a few advanced extensions to LSA that have recently become
    popular in the field of topic modeling.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将了解一些最近在主题建模领域变得流行的LSA（潜在语义分析）的高级扩展。
- en: Probabilistic methods
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 概率方法
- en: LSA is based on linear algebra (math with vectors and matrices), but an equivalent
    analysis can be done using probabilistic methods that model each document as a
    statistical mixture of topic distributions. These concepts are all relatively
    advanced, and we won’t go into the mathematical details here, but the probabilistic
    approach can perform better in terms of model accuracy for some datasets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LSA基于线性代数（涉及向量和矩阵的数学），但可以使用对每个文档作为主题分布的统计混合模型进行概率方法的等效分析。这些概念都相对高级，我们在这里不会深入数学细节，但概率方法在某些数据集上可以更好地提高模型精度。
- en: The probabilistic analogue to LSA is known as pLSA (for probabilistic). A more
    widely used version of this is called *latent Dirichlet analysis* (LDA), in which
    specific assumptions are made on the distribution of topics. You build in the
    assumption that a document can be described by a small set of topics and that
    any term (word) can be attributed to a topic. In practice, LDA can perform well
    on diverse datasets. The following code listing highlights how LDA can be used
    in Python using the Gensim library.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: LSA的概率对应物被称为pLSA（代表概率）。这种更广泛使用的版本被称为*潜在狄利克雷分析*（LDA），其中对主题的分布做出了特定假设。您假设一个文档可以由一组小的主题来描述，并且任何术语（单词）都可以归因于一个主题。在实践中，LDA可以在各种数据集上表现良好。以下代码示例突出了如何使用Gensim库在Python中应用LDA。
- en: Listing 7.2\. Latent Dirichlet analysis in Python using Gensim
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2. 使用Gensim在Python中进行的潜在狄利克雷分析
- en: '![](152fig01_alt.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](152fig01_alt.jpg)'
- en: 'The number of topics used in the LDA model is a parameter that needs to be
    tuned to the data and problem at hand. We encourage you to define your performance
    metric and use the techniques in [chapter 4](kindle_split_014.html#ch04) to optimize
    your model. It’s also worth noting that the LDA in Gensim can be updated on the
    fly with new documents if new training data is coming in continuously. We encourage
    you to check out the many other interesting natural-language and topic-modeling
    algorithms in Gensim. In [chapter 10](kindle_split_021.html#ch10), you’ll use
    some of these advanced text-feature-extraction techniques to solve a real-world
    machine-learning problem. The next section introduces a completely different method
    for text-feature extraction: expanding the content of the text.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在LDA模型中使用的话题数量是一个需要根据数据和问题进行调整的参数。我们鼓励您定义您的性能指标，并使用[第4章](kindle_split_014.html#ch04)中的技术来优化您的模型。还值得注意的是，如果新的训练数据持续到来，Gensim中的LDA可以实时更新。我们鼓励您探索Gensim中许多其他有趣的自然语言和主题建模算法。在[第10章](kindle_split_021.html#ch10)中，您将使用这些高级文本特征提取技术来解决一个实际的机器学习问题。下一节介绍了一种完全不同的文本特征提取方法：扩展文本内容。
- en: 7.1.3\. Content expansion
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3. 内容扩展
- en: We now turn to a completely different concept for extracting features from text.
    The methods of this section don’t represent the text with numbers, but rather
    expand the text content to include more text (which can then be featurized) or
    to introduce other useful information for the specific ML problem. The following
    are some common content-expansion methods.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向一个完全不同的概念，用于从文本中提取特征。本节的方法不是用数字表示文本，而是扩展文本内容以包含更多文本（然后可以进行特征化）或引入对特定ML问题有用的其他信息。以下是一些常见的内容扩展方法。
- en: Follow links
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跟随链接
- en: If you’re looking to build an ML classifier by extracting text features from
    tweets (for instance, for a Twitter sentiment analysis that classifies a post
    as positive or negative in sentiment), you’ll often find the 140-character limit
    problematic. You might not have enough information to obtain the desired accuracy
    of the model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望通过从推文中提取文本特征来构建ML分类器（例如，用于Twitter情感分析，将帖子分类为正面或负面情感），您通常会发现140个字符的限制是一个问题。您可能没有足够的信息来获得模型所需的准确度。
- en: Many tweets contain links to external web pages that can hold much more text,
    and that you could *expand* the tweet with the text from the link in order to
    improve the quality of the data. You could even follow links deeper on the web
    page to build a larger corpus of text.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 许多推文包含指向外部网页的链接，这些网页可以包含更多的文本，您可以用链接中的文本来*扩展*推文，从而提高数据质量。您甚至可以深入网页上的链接，以构建更大的文本语料库。
- en: Knowledge-base expansion
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识库扩展
- en: A more advanced text-extension method is to detect named entities in the text
    and extend the original text with information about each named entity in an online
    knowledge base, such as Wikipedia. In this situation, named entities would be
    anything that you could look up on Wikipedia. You’d then grab the text from the
    Wikipedia entry for that named entity and perform any of the text-extraction algorithms
    from [section 7.1.2](#ch07lev2sec2).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更高级的文本扩展方法是在文本中检测命名实体，并使用在线知识库（如维基百科）中每个命名实体的信息来扩展原始文本。在这种情况下，命名实体可以是你在维基百科上可以查找的任何东西。然后，你会从该命名实体的维基百科条目中获取文本，并执行[第7.1.2节](#ch07lev2sec2)中提到的任何文本提取算法。
- en: Extracting named entities isn’t a trivial task, and has been the subject of
    several research groups. One of the issues stems from ambiguous names. If one
    word could have multiple meanings, you risk expanding your feature set with completely
    wrong information. One possible solution is to disambiguate the named entities
    again by using a knowledge base like Wikipedia. First of all, you could assume
    that any other words in the tweet, for example, would also be common in the knowledge-base
    text. You could also use the Wikipedia link graph to find how close two named
    entities fall in the knowledge base. An example is a tweet that includes the named
    entity “Tesla.” Some tweets will relate to the electronic car company, whereas
    others will be about inventor Nikola Tesla. If the tweet contains the word “car”
    or “model,” it’s most likely about Tesla, the company. If it contains the related
    entity of “Edison,” it might be about the person (Tesla and Edison worked together
    in NYC in 1884).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提取命名实体并非易事，一直是多个研究小组的研究课题。其中一个问题是源于名称的歧义性。如果一个单词有多种含义，你可能会因为包含完全错误的信息而扩大你的特征集。一个可能的解决方案是使用像维基百科这样的知识库再次对命名实体进行消歧。首先，你可以假设推文中出现的任何其他单词，例如，在知识库文本中也很常见。你也可以使用维基百科的链接图来找出两个命名实体在知识库中的接近程度。一个例子是包含命名实体“Tesla”的推文。一些推文会与电子汽车公司相关，而另一些则可能关于发明家尼古拉·特斯拉。如果推文中包含“car”或“model”等单词，那么它很可能是关于特斯拉公司。如果它包含与“Edison”相关的实体，那么它可能是关于这个人（特斯拉和爱迪生在1884年纽约市一起工作过）。
- en: Text meta-features
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本元特征
- en: Another technique for extending the text features with valuable data is to analyze
    the text for *meta-features*. Unlike the previously discussed techniques, these
    types of features are problem-dependent.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种通过分析文本以扩展具有价值数据的技巧是分析文本的元特征。与之前讨论的技术不同，这些类型的特征是问题相关的。
- en: Let’s take the example of tweets again. A tweet contains all sorts of valuable
    data that’s particular to tweets and can be extracted, such as hashtags and mentions,
    as well as meta-information from Twitter, such as counts of retweets and favorites.
    As another example for web-based text, you could extract basic information from
    link text, such as the top-level domain. In general text, you could extract the
    count of words or characters or the number of special characters in different
    languages. Extracting the language could be an ML classifier itself that provides
    the answer as a feature to another classifier.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次以推文为例。推文包含所有 sorts of 有价值的数据，这些数据特定于推文并且可以提取，例如标签和提及，以及来自Twitter的元信息，例如转发和喜欢的计数。作为基于网络的文本的另一个例子，你可以从链接文本中提取基本信息，例如顶级域名。在一般文本中，你可以提取单词或字符的计数，或不同语言中特殊字符的数量。提取语言本身可能就是一个机器学习分类器，它将答案作为特征提供给另一个分类器。
- en: To choose the right text meta-features, you should use your imagination and
    knowledge of the problem at hand. Remember that the ML workflow is an iterative
    process; you can develop a new feature, go back through the pipeline, and analyze
    how the accuracy is improved over time.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择正确的文本元特征，你应该发挥想象力，并了解手头的问题。记住，机器学习工作流程是一个迭代过程；你可以开发一个新的特征，然后回到管道中，分析随着时间的推移准确率是如何提高的。
- en: You can use the text to get at other types of data as well. The text might include
    dates and times that could be useful for the ML model to understand, or there
    may be time information in the metadata of the text. [Chapter 5](kindle_split_015.html#ch05)
    presented date-time feature extractions, which can be used in this context as
    well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用文本来获取其他类型的数据。文本可能包含对机器学习模型理解有用的日期和时间，或者文本的元数据中可能包含时间信息。[第五章](kindle_split_015.html#ch05)介绍了日期时间特征提取，这些也可以在此背景下使用。
- en: If you’re analyzing a web page, or there’s a URL in the text, you may have access
    to images or videos that are important for understanding the context of the text.
    Extracting features from images and videos requires even more-advanced techniques,
    which you’ll investigate next.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在分析一个网页，或者文本中有一个URL，你可能可以访问对理解文本上下文很重要的图像或视频。从图像和视频中提取特征需要更高级的技术，你将在下一节中探讨。
- en: 7.2\. Image features
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 图像特征
- en: 'One of the strongholds of human intelligence is our visual and spatial sense
    and our ability to recognize patterns and objects in images and the 3D scenes
    we navigate every day. Much of the way we think is based on these abilities. Computers,
    on the other hand, think in bits and their visual analogue, pixels. Historically,
    this fact has severely limited computers’ ability to match human levels of cognition
    when it comes to visual pattern recognition. Only with the advent of sophisticated
    algorithms in computer vision and artificial intelligence—from which machine learning
    has arguably sprung—are researchers and practitioners getting closer to reaching
    human levels, although most often in narrowly specified areas. On the other hand,
    if you can get close to matching human-level pattern recognition accuracy with
    computer vision and machine-learning techniques, you can reap some of the benefits
    of most computational systems: scalability, availability, and reproducibility.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 人类智能的坚强堡垒之一是我们的视觉和空间感，以及我们识别图像和日常生活中导航的3D场景中的模式和对象的能力。我们思考的大部分方式都基于这些能力。另一方面，计算机以比特和它们的视觉类比——像素来思考。从历史上看，这一事实严重限制了计算机在视觉模式识别方面达到人类认知水平的能力。只有随着计算机视觉和人工智能中复杂算法的出现——机器学习可以说是由此产生的——研究人员和从业者才越来越接近达到人类水平，尽管大多数情况下是在狭窄指定的领域。另一方面，如果你能通过计算机视觉和机器学习技术接近匹配人类水平的模式识别准确性，你就可以获得大多数计算系统的一些好处：可扩展性、可用性和可重复性。
- en: This section presents a few ways to extract features from images that can be
    used in your ML workflows. First, you’ll look at simple image features including
    raw pixels, colors, and image metadata.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了几种从图像中提取特征的方法，这些特征可以用于你的机器学习工作流程。首先，你将查看简单的图像特征，包括原始像素、颜色和图像元数据。
- en: 7.2.1\. Simple image features
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 简单图像特征
- en: The simplest way to deal with images is worth mentioning, not only because it
    may sometimes be enough, but also because it shows the true power of the machine-learning
    approach, as compared to manual or conventional statistical approaches. You treat
    the values of pixels in the image as the features that go into your ML model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像的最简单方法值得提及，不仅因为它有时可能足够，而且还因为它展示了机器学习方法的真正力量，与手动或传统统计方法相比。你将图像中像素的值视为进入你的机器学习模型的特征。
- en: In practice, you make a single row with all the pixels, converting the two-dimensional
    image into one dimension. If it’s a color image, you have basically three images
    in one (red, blue, green channels). Normal pixel values are 0.0 to 1.0, or 0 to
    255 (for 8-bit images). You may have guessed that for any modern image, this creates
    thousands or millions of features that will increase the computational requirements
    and potentially lead to overfitting, hence affecting the accuracy. That’s why
    this approach isn’t often used in practice. Still, you’d probably be surprised
    how well this can work without any sophisticated feature engineering for some
    ML problems, such as classifying indoor versus outdoor images.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你将所有像素排成一行，将二维图像转换为一维。如果是一个彩色图像，你基本上有三个图像（红色、蓝色、绿色通道）。正常像素值是0.0到1.0，或者0到255（对于8位图像）。你可能已经猜到，对于任何现代图像，这会创建成千上万的特征，这将增加计算需求，并可能导致过拟合，从而影响准确性。这就是为什么这种方法在实践中并不常用。然而，你可能会惊讶地发现，对于某些机器学习问题，如区分室内和室外图像，这种方法在没有任何复杂特征工程的情况下也能很好地工作。
- en: In principle, all the information is encoded in the pixels. If you’re not going
    to use the raw pixels for performance reasons (computationally or accuracy-wise),
    you have to find a way to represent the image with fewer features that works well
    enough for your specific problem. This is exactly the same problem you were solving
    in the previous section on text features and many other feature-engineering techniques.
    Toward the end of [section 7.2.2](#ch07lev2sec5), we introduce some new methods
    for automatic feature extraction, but most current practical ML projects on images
    use some of the techniques described in this section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，所有信息都编码在像素中。如果您出于性能原因（计算或准确性方面）不打算使用原始像素，您必须找到一种方法，用更少的特征来表示图像，这些特征对您特定的任务来说足够好。这正是您在前一节关于文本特征和许多其他特征工程技术中解决的问题。在[7.2.2节](#ch07lev2sec5)的末尾，我们介绍了一些新的自动特征提取方法，但大多数当前的实际图像机器学习项目都使用本节中描述的一些技术。
- en: Color features
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 颜色特征
- en: Let’s say you’re trying to classify images into categories based on the landscape
    of the images. Categories could be *sky*, *mountain*, or *grass*, for example.
    In this case, it sounds useful to represent the images by the constituent colors.
    You can calculate simple color statistics of each color channel of the image,
    such as *mean*, *median*, *mode*, *standard deviation*, *skewness*, and *kurtosis*.
    This leads to 6 x 3 = 18 features for common RGB (red-green-blue channel) images.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在尝试根据图像的风景将图像分类到不同的类别中。例如，类别可以是*天空*、*山脉*或*草地*。在这种情况下，使用构成颜色来表示图像似乎很有用。您可以计算图像每个颜色通道的简单颜色统计信息，例如*平均值*、*中位数*、*众数*、*标准差*、*偏度*和*峰度*。这为常见的RGB（红-绿-蓝通道）图像产生了6
    x 3 = 18个特征。
- en: Another set of features representing colors in the images are the color ranges
    of the image. [Table 7.1](#ch07table01) shows a list of possible color ranges
    that will cover much of the color space.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组表示图像中颜色的特征是图像的颜色范围。[表7.1](#ch07table01)显示了一个可能的颜色范围列表，它将覆盖大部分颜色空间。
- en: Table 7.1\. Examples of color-range features. You add 1 to the divisors to avoid
    producing missing values from dividing by 0.
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1\. 颜色范围特征示例。您将除数加1，以避免除以0产生缺失值。
- en: '| Color range | Definition |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 颜色范围 | 定义 |'
- en: '| --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Red range | Max value in red channel minus min value in red channel |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 红色范围 | 红色通道最大值减去红色通道最小值 |'
- en: '| Red-to-blue range | Red range / (max value in blue channel minus min value
    in blue channel plus 1) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 红到蓝范围 | 红色范围 / (蓝色通道最大值减去蓝色通道最小值加1) |'
- en: '| Blue-to-green range | (Min value in blue channel minus max value in blue
    channel) / (min value in green channel minus max value in green channel plus 1)
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 蓝到绿范围 | (蓝色通道最小值减去蓝色通道最大值) / (绿色通道最小值减去绿色通道最大值加1) |'
- en: '| Red-to-green range | Red range / (max value in green channel minus min value
    in green channel plus 1) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 红到绿范围 | 红色范围 / (绿色通道最大值减去绿色通道最小值加1) |'
- en: Image metadata features
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像元数据特征
- en: In addition to color information, the image may contain metadata that’s helpful
    for your problem. Most photographs, for example, include EXIF data that’s recorded
    by the camera at the time the picture was taken. If you’re building a model to
    predict whether an image is considered interesting or beautiful to a user, the
    algorithm could use the brand of the camera and the lens, the value of the aperture,
    and the zoom level. [Table 7.2](#ch07table02) outlines image metadata features
    that may be useful.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了颜色信息外，图像可能包含对您的问题有帮助的元数据。例如，大多数照片都包含相机在拍照时记录的EXIF数据。如果您正在构建一个预测用户认为图像是否有趣或美丽的模型，算法可以使用相机的品牌和镜头、光圈值和变焦级别。[表7.2](#ch07table02)概述了可能对图像元数据特征有用的内容。
- en: Table 7.2\. Image metadata features that can be included in the ML pipeline
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2\. 可包含在机器学习流程中的图像元数据特征
- en: '| Feature | Definition |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 定义 |'
- en: '| --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Manufacturer | The company that made the camera |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 制造商 | 制作相机的公司 |'
- en: '| Orientation | The orientation of the camera (landscape or portrait) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方向 | 相机的方向（横向或纵向） |'
- en: '| Date-time | Time of the shooting (use the date-time features introduced in
    [chapter 5](kindle_split_015.html#ch05)) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 日期时间 | 拍摄时间（使用第5章中介绍的日期时间功能[chapter 5](kindle_split_015.html#ch05)） |'
- en: '| Compression | How the image is compressed (usually JPEG or RAW) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 压缩 | 图像的压缩方式（通常是JPEG或RAW） |'
- en: '| Resolution | The number of pixels in the width and height dimensions |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 分辨率 | 宽度和高度维度的像素数量 |'
- en: '| Aspect ratio | A measurement indicated by dividing the height and width resolutions
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 长宽比 | 通过除以高度和宽度分辨率来表示的测量值 |'
- en: '| Exposure time | The number or fraction of seconds of exposure |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 曝光时间 | 曝光秒数或分数 |'
- en: '| Aperture | The f-number representing the aperture (for example, 2.8 or 4.0)
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 光圈 | 表示光圈（例如，2.8或4.0）的f数 |'
- en: '| Flash | Whether the flash was on |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 闪光灯 | 闪光灯是否开启 |'
- en: '| Focal length | The distance from the lens to the point of focus |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 焦距 | 从镜头到焦点的距离 |'
- en: With these simple features, you might be able to solve quite a few machine-learning
    problems that have images as part of the data. Of course, you haven’t represented
    any of the shapes or objects in the image, which will, for obvious reasons, be
    important for many image-classification problems! The next section introduces
    more-advanced computer-vision techniques commonly used to represent objects and
    shapes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些简单的特征，你可能能够解决许多具有图像数据部分的机器学习问题。当然，你还没有在图像中表示任何形状或对象，这在许多图像分类问题中显然是重要的！下一节将介绍更多常用的计算机视觉技术，这些技术通常用于表示对象和形状。
- en: 7.2.2\. Extracting objects and shapes
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2. 提取对象和形状
- en: So far, you haven’t considered objects or shapes when extracting information
    from images. In this subsection, you’ll look at a few ways to represent shapes
    with numerical features that can be automatically extracted via statistical and
    computational methods.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你在从图像中提取信息时还没有考虑对象或形状。在本小节中，你将了解几种使用可以通过统计和计算方法自动提取的数值特征来表示形状的方法。
- en: Edge detection
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 边缘检测
- en: Probably the simplest way to represent shapes in images is to find their edges
    and build features on those. [Figure 7.3](#ch07fig03) shows an example of *edge
    detection* in an image.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 可能表示图像中形状的最简单方法就是找到它们的边缘，并在这些边缘上构建特征。[图7.3](#ch07fig03) 展示了图像中边缘检测的一个示例。
- en: Figure 7.3\. Applying the Canny edge-detection algorithm to a photo of a girl
    (input on left) produces a new binary image (on right) with only the edges traced.
    (Image by JonMcLoone at English Wikipedia, CC BY-SA 3.0, [https://commons.wikimedia.org/w/index.php?curid=44894482](https://commons.wikimedia.org/w/index.php?curid=44894482).)
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3. 将Canny边缘检测算法应用于女孩的照片（左侧为输入）生成一个新的二值图像（右侧），其中只追踪了边缘。（图片由JonMcLoone在英文维基百科提供，CC
    BY-SA 3.0，[https://commons.wikimedia.org/w/index.php?curid=44894482](https://commons.wikimedia.org/w/index.php?curid=44894482)）
- en: '![](07fig03_alt.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig03_alt.jpg)'
- en: Several well-known algorithms can find edges in an image. Some of the most commonly
    used are the *Sobel* and *Canny* edge-detection algorithms. [Figure 7.3](#ch07fig03)
    shows the Canny algorithm.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 几种著名的算法可以在图像中找到边缘。其中最常用的有 *Sobel* 和 *Canny* 边缘检测算法。[图7.3](#ch07fig03) 展示了Canny算法。
- en: '|  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Image processing in Python with scikit-image**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用scikit-image进行Python图像处理**'
- en: We’ve mentioned the scikit-learn Python library a few times in this book already,
    as it provides an easy way to try many machine-learning algorithms. The analogue
    to this in the computer-vision and image-processing world is scikit-image. This
    is an equally useful way to try algorithms that we talk about in this section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中已经提到了几次scikit-learn Python库，因为它提供了一种尝试许多机器学习算法的简单方法。在计算机视觉和图像处理世界中，与此类似的是scikit-image。这是尝试本节中讨论的算法的同样有用的方法。
- en: 'If you’re using Pip, scikit-image can easily be installed with the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Pip，可以使用以下命令轻松安装scikit-image：
- en: '[PRE0]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s a simple example of using this library for edge detection:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用此库进行边缘检测的简单示例：
- en: '[PRE1]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Now that you’ve extracted edges from images, you can extract features from
    those edges. The simplest way is to calculate a number that represents the total
    number of edges in an image. If `edges` is your edge images and `res` is the resolution
    of the image, the equation is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经从图像中提取了边缘，你可以从这些边缘中提取特征。最简单的方法是计算一个表示图像中边缘总数的数字。如果`edges`是你的边缘图像，而`res`是图像的分辨率，则方程如下：
- en: '![](157equ01.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](157equ01.jpg)'
- en: Together with other features, this may be useful in determining objects of interest.
    You can define other edge-based features depending on your use case. For example,
    you could choose to calculate the preceding edge score for multiple parts of the
    image in a grid.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他特征一起，这可能有助于确定感兴趣的对象。你可以根据你的用例定义其他基于边缘的特征。例如，你可以在网格中为图像的多个部分计算前面的边缘得分。
- en: Advanced shape features
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高级形状特征
- en: More-sophisticated feature-extraction algorithms that can be used to detect
    particular shapes and objects exist. One of these is the *histogram of oriented
    gradients* (HOG). In machine learning, these algorithms can be used to detect
    human faces or particular animals in images, for example.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 存在更多复杂的特征提取算法，可以用来检测特定的形状和物体。其中之一是*方向梯度直方图*（HOG）。在机器学习中，这些算法可以用来检测图像中的人类面部或特定的动物，例如。
- en: 'The HOG algorithm is a multistep process of various image-processing techniques.
    The goal of the algorithm is to describe shapes and objects in image regions that
    aren’t too sensitive to small changes in scale and orientation. This is achieved
    as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: HOG 算法是一系列多步骤的图像处理技术。该算法的目标是在对图像区域中的形状和物体进行描述时，对尺度和小方向变化不太敏感。这是通过以下方式实现的：
- en: Calculate the gradient image (which direction the edges of the image are “moving”)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度图像（图像边缘“移动”的方向）
- en: Divide the image into small blocks called *cells*
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像划分为称为*单元格*的小块
- en: Calculate the orientation of the gradients inside those cells
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算那些细胞内梯度的方向
- en: Calculate the histogram of those orientations in the individual cells
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些方向在单个细胞中的直方图
- en: Usually, larger blocks of the image are defined from the smaller cells and used
    for normalization of the gradient values in the cells. In this way, you can avoid
    being too sensitive to changes in lighting or shadows. Each cell can then be flattened
    into a list of features that describe the shapes in the image and can be used
    in the ML pipeline.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，较大的图像块是从较小的单元格定义的，并用于单元格中梯度值的归一化。这样，你可以避免对光照或阴影的变化过于敏感。然后，每个单元格可以被展平成一个特征列表，这些特征描述了图像中的形状，并可用于机器学习流程。
- en: As usual, you’re concerned with understanding the usefulness of the algorithms
    from a practical perspective, and so you can go ahead and use an already implemented
    library for HOG features. The scikit-image Python library has an easy-to-use version
    of HOG. The following listing shows how to calculate HOG features for an image.
    [Figure 7.4](#ch07fig04) shows the result of the HOG transformation applied to
    a photograph of American astronaut Eileen Collins, the first female commander
    of a Space Shuttle.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你关注的是从实用角度理解算法的有用性，因此你可以继续使用已经实现的 HOG 特征库。scikit-image Python 库提供了一个易于使用的
    HOG 版本。以下列表显示了如何计算图像的 HOG 特征。[图 7.4](#ch07fig04) 展示了将 HOG 变换应用于美国宇航员 Eileen Collins
    照片的结果，她是第一位太空船指挥官女性。
- en: Figure 7.4\. Applying the HOG transformation. This image is from the HOG example
    page on scikit-image documentation ([http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py)).
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.4\. 应用 HOG 变换。此图像来自 scikit-image 文档中的 HOG 示例页面 ([http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py))。
- en: '![](07fig04_alt.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig04_alt.jpg)'
- en: Listing 7.3\. Histogram of oriented gradients in Python with scikit-image
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3\. 使用 scikit-image 在 Python 中计算方向梯度直方图
- en: '[PRE2]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here you see how to calculate HOG features easily while defining the number
    of orientations to consider, the size of the cells in pixels, the size of the
    blocks in cells, and whether to normalize and visualize the result.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到如何轻松地计算 HOG 特征，同时定义要考虑的方向数量、单元格的像素大小、单元格中块的大小，以及是否对结果进行归一化和可视化。
- en: With HOG features, you have a powerful way to find objects in images. As with
    everything, in certain cases, HOG doesn’t work well—for instance, when the object
    changes orientation significantly. You should make proper tests of the ML system
    as usual to determine usefulness for the problem at hand.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HOG 特征，你有一个强大的方法在图像中找到物体。就像所有的事情一样，在某些情况下，HOG 并不奏效——例如，当物体显著改变方向时。你应该像往常一样对机器学习系统进行适当的测试，以确定它对当前问题的有用性。
- en: Dimensionality reduction
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维度降低
- en: We’re almost always in the game of dimensionality reduction when performing
    feature extraction, except perhaps for the content-expansion methods in the previous
    section. But a few techniques are commonly used for dimensionality reduction in
    general, and the most widely used is called *principal component analysis* (PCA).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行特征提取时，我们几乎总是在进行降维的游戏，除了上一节中的内容扩展方法。但有一些技术通常用于一般的降维，其中最广泛使用的是称为*主成分分析*（PCA）。
- en: PCA allows you to take a set of images and find “typical” images that can be
    used as building blocks to represent the original images. Combining the first
    couple of principal components enables you to rebuild a large portion of the training
    images, whereas subsequent components will cover less-frequent patterns in the
    images. Features for a new image are generated by finding the “distance” from
    a principal image, thus representing the new image by a single number per principal
    image. You can use as many principal components as make sense in your ML problem.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）允许您从一组图像中找到“典型”图像，这些图像可以用作构建块来表示原始图像。结合前几个主成分可以使您重建大量训练图像，而后续成分将覆盖图像中较少出现的模式。对于新图像的特征是通过找到与主图像的“距离”来生成的，因此每个主图像代表新图像的单个数字。您可以在您的机器学习问题中使用尽可能多的主成分。
- en: PCAs are known to be linear algorithms; they can’t represent inherently nonlinear
    data. There are several extensions to PCA or other types of nonlinear dimensionality
    reduction. An example that we’ve had good experiences with is *diffusion maps*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCAs）被认为是线性算法；它们无法表示本质上非线性的数据。PCA或其他类型的非线性降维方法有几种扩展。我们有过良好经验的例子是*扩散映射*。
- en: Automatic feature extraction
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自动特征提取
- en: A renaissance has occurred in the world of artificial neural networks. Invented
    in the ’80s and inspired by the biology of the brain, these networks were at the
    center of the artificial intelligence field that has evolved into the machine-learning
    field we know today. For a few decades, they were considered useful methods for
    some ML problems. But because they were hard to configure and interpret, had problems
    with overfitting, and were less computationally scalable, they ended up as a last
    resort when real-world problems needed solving. Now, several breakthroughs in
    machine-learning research have mostly taken care of these issues. *Deep neural
    nets* (DNNs) are now considered state of the art for many ML problems, but especially
    those that deal with images, video, or voice. [Figure 7.5](#ch07fig05) shows the
    layout of a neural net.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的世界经历了一次复兴。这些网络在20世纪80年代发明，并受到大脑生物学的启发，曾是人工智能领域（该领域已发展成为我们今天所知的机器学习领域）的中心。几十年来，它们被认为是某些机器学习问题有用的方法。但由于它们难以配置和解释，存在过拟合问题，并且计算扩展性较差，最终成为解决现实世界问题时的一种最后手段。现在，机器学习研究中的几个突破主要解决了这些问题。*深度神经网络*（DNNs）现在被认为是许多机器学习问题的最佳实践，尤其是那些处理图像、视频或声音的问题。[图7.5](#ch07fig05)显示了神经网络的布局。
- en: Figure 7.5\. A simple artificial neural network. Deep neural nets are made of
    many layers of these simple networks. (Image from Wikipedia.)
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5. 一个简单的人工神经网络。深度神经网络由许多这样的简单网络层组成。（图片来自维基百科。）
- en: '![](07fig05.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig05.jpg)'
- en: In DNNs, each layer is capable of defining a set of new features that are useful
    for the problem at hand. The weights between nodes then define the importance
    of those features for the next layer, and so forth. This approach was traditionally
    prone to overfitting, but recently developed techniques allow for the removal
    of node connections in a way that the accuracy is maintained while decreasing
    the risk of overfitting.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络（DNNs）中，每一层都能够定义一组对当前问题有用的新特征。节点之间的权重定义了这些特征对下一层的重要性，以此类推。这种方法传统上容易过拟合，但最近开发的技术允许以保持准确性的方式移除节点连接，从而降低过拟合的风险。
- en: The use of DNNs, also known as *deep belief networks* or *deep learning*, is
    still a relatively new field. We encourage you to follow its development.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs），也称为*深度信念网络*或*深度学习*，仍然是一个相对较新的领域。我们鼓励您关注其发展。
- en: 7.3\. Time-series features
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 时间序列特征
- en: Many datasets that are amassed by modern data-collection systems come in the
    form of *time series*, measurements of a process or set of processes across time.
    Time-series data is valuable because it provides a window into the time-varying
    characteristics of the subjects at hand and enables ML practitioners to move beyond
    employing static snapshots of these subjects to make predictions. But fully extracting
    the value out of time-series data can be difficult. This section describes two
    common types of time-series data—classical time series and point processes (event
    data)—and details some of the most widely used time-series features.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 许多由现代数据收集系统收集的数据集以*时间序列*的形式出现，这是对过程或一系列过程随时间变化的测量。时间序列数据很有价值，因为它提供了对当前主题随时间变化特性的窗口，并使机器学习从业者能够超越仅使用这些主题的静态快照来做出预测。但完全提取时间序列数据的价值可能很困难。本节描述了两种常见的时间序列数据类型——经典时间序列和点过程（事件数据）——并详细介绍了最广泛使用的时间序列特征。
- en: 7.3.1\. Types of time-series data
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1\. 时间序列数据的类型
- en: 'There are two main types of time-series data: classical time series and point
    processes. *Classical time series* consist of numerical measurements that are
    taken over time. Typically, these measurements are evenly spaced over time (hourly,
    daily, weekly, and so forth) but can also consist of irregularly sampled data.
    These are examples of classical time-series data:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据主要有两种类型：经典时间序列和点过程。*经典时间序列*由随时间进行的数值测量组成。通常，这些测量在时间上是均匀分布的（每小时、每日、每周等），但也可以是不规则采样的数据。以下是一些经典时间序列数据的例子：
- en: The value of the stock market, in billions of dollars (for example, measured
    hourly, daily, or weekly)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票市场的价值，以十亿美元为单位（例如，按小时、每日或每周衡量）
- en: The day-to-day energy consumption of a commercial building or residential home
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业建筑或住宅的日常能源消耗
- en: The value, in dollars, of a client’s bank account over time
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户银行账户随时间变化的美元价值
- en: Sets of diagnostics monitored in an industrial manufacturing plant (for example,
    physical performance measurements of different parts or measurements of plant
    output over time)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工业制造厂中监控的诊断集（例如，不同部件的物理性能测量或随时间变化的工厂产出测量）
- en: '*Point processes*, on the other hand, are collections of events that occur
    over time. As opposed to measuring numerical quantities over time, point processes
    consist of a timestamp for each discrete event that happens, plus (optionally)
    other metadata about the event such as category or value. For this reason, point
    processes are also commonly referred to as *event streams*. Examples of point
    processes include the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*点过程*是随时间发生的事件集合。与在时间上测量数值量不同，点过程由每个离散事件的时间戳组成，加上（可选的）关于事件的元数据，如类别或值。因此，点过程也通常被称为*事件流*。以下是一些点过程的例子：
- en: The activity of a web user, measuring the time and type of each click (this
    is also called *clickstream data*)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络用户的活动，衡量每次点击的时间和类型（这通常也称为*点击流数据*）
- en: Worldwide occurrences of earthquakes, hurricanes, disease outbreak, and so forth
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球地震、飓风、疾病爆发等事件的发生
- en: The individual purchases made by a customer throughout the history of their
    account
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户在其账户历史中进行的个别购买
- en: Event logs in a manufacturing plant, recording every time an employee touches
    the system and every time a step in the manufacturing process is completed
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造厂中的事件日志，记录每次员工接触系统和每次制造过程步骤完成的时间
- en: An astute reader may note that for some time series, a one-to-one mapping exists
    between the classical time-series representation and the underlying point process.
    For example, a customer’s bank account can easily be viewed either as the value
    of the account over time (classical time series) or as a list of the individual
    transactions (point process). This correspondence can be useful in creating various
    types of time-series features on a single dataset. But the conversion isn’t always
    possible. (For example, it’s difficult to imagine what a classical time-series
    related to simple web clicks would be.)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一个敏锐的读者可能会注意到，对于某些时间序列，经典时间序列表示和底层点过程之间存在一对一的映射。例如，客户的银行账户可以很容易地看作是账户随时间的变化值（经典时间序列）或一系列个别交易（点过程）。这种对应关系在创建单个数据集上的各种类型的时间序列特征时可能很有用。但转换并不总是可能的。（例如，很难想象与简单网络点击相关的经典时间序列会是什么样子。）
- en: To make this more concrete, let’s look at time-series data that can be just
    as easily viewed as a point process or a time series. [Table 7.3](#ch07table03)
    shows the first few rows of a crime dataset from San Francisco, collected between
    2003 and 2014 (dataset publicly available at [https://data.sfgov.org](https://data.sfgov.org)).
    In all, the dataset consists of more than 1.5 million crimes that occurred in
    the city. For each crime, the data includes the exact date and time of the crime,
    type of crime, and location.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加具体，让我们看看可以同样容易地被视为点过程或时间序列的时间序列数据。[表7.3](#ch07table03)显示了2003年至2014年间收集的旧金山犯罪数据集的前几行（数据集在[https://data.sfgov.org](https://data.sfgov.org)公开可用）。总的来说，该数据集包括城市中发生的超过150万起犯罪事件。对于每起犯罪，数据包括犯罪的确切日期和时间、犯罪类型和地点。
- en: Table 7.3\. San Francisco crime data in its raw form, as a sequence of events
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.3\. 旧金山的原始犯罪数据，作为事件序列
- en: '| Incident number | Date | Time | District | Category |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 事件编号 | 日期 | 时间 | 区域 | 类别 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **80384498** | 04/13/2008 | 00:54 | NORTHERN | DRUNKENNESS |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| **80384498** | 04/13/2008 | 00:54 | NORTHERN | 酒精中毒'
- en: '| **80384147** | 04/13/2008 | 00:55 | CENTRAL | NONCRIMINAL |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| **80384147** | 04/13/2008 | 00:55 | CENTRAL | 非犯罪'
- en: '| **80384169** | 04/13/2008 | 00:56 | BAYVIEW | ASSAULT |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| **80384169** | 04/13/2008 | 00:56 | BAYVIEW | 攻击'
- en: '| **80384169** | 04/13/2008 | 00:56 | BAYVIEW | DRUG/NARCOTIC |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| **80384169** | 04/13/2008 | 00:56 | BAYVIEW | 毒品/麻醉品'
- en: '| **80384153** | 04/13/2008 | 00:57 | BAYVIEW | OTHER |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| **80384153** | 04/13/2008 | 00:57 | BAYVIEW | 其他'
- en: '| **80384175** | 04/13/2008 | 01:00 | CENTRAL | ASSAULT |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| **80384175** | 04/13/2008 | 01:00 | CENTRAL | 攻击'
- en: '| **80384943** | 04/13/2008 | 01:00 | CENTRAL | LARCENY/THEFT |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| **80384943** | 04/13/2008 | 01:00 | CENTRAL | 盗窃/盗窃'
- en: '| **80392532** | 04/13/2008 | 01:00 | INGLESIDE | LARCENY/THEFT |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **80392532** | 04/13/2008 | 01:00 | INGLESIDE | 盗窃/盗窃'
- en: '| **80384943** | 04/13/2008 | 01:00 | CENTRAL | FRAUD |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **80384943** | 04/13/2008 | 01:00 | CENTRAL | 诈骗'
- en: '| **80384012** | 04/13/2008 | 01:15 | NORTHERN | SUSPICIOUS OCC |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **80384012** | 04/13/2008 | 01:15 | NORTHERN | 可疑事件'
- en: 'You can aggregate this raw data into classical time-series data in a multitude
    of ways: by year, by month, by day of week, and so on, potentially with a different
    time series for each district or category. [Listing 7.4](#ch07ex04) demonstrates
    how to aggregate the raw event data into a time series of the monthly number of
    crimes in San Francisco. The resulting time series of integer crime count by month
    is plotted in [figure 7.6](#ch07fig06). The data shows a marked decline from the
    rate of 13,000 crimes per month in 2003, and a recent uptick in crime activity.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过多种方式将这原始数据聚合为经典的时间序列数据：按年、按月、按星期几等，每个区域或类别可能都有不同的时间序列。[列表7.4](#ch07ex04)演示了如何将原始事件数据聚合为旧金山每月犯罪数量的时间序列。通过月份整数犯罪计数得到的时间序列在[图7.6](#ch07fig06)中绘制。数据显示，从2003年每月13,000起犯罪的比率有显著下降，最近犯罪活动有所上升。
- en: Figure 7.6\. Classical time series of monthly crime count in San Francisco.
    This data was processed from the raw event data. For ML modeling, you can derive
    features from the event data, the classical time series, or both.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. 旧金山每月犯罪计数的经典时间序列。这些数据是从原始事件数据中处理得到的。对于机器学习建模，你可以从事件数据、经典时间序列或两者中提取特征。
- en: '![](07fig06_alt.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig06_alt.jpg)'
- en: Listing 7.4\. Converting SF crime event data to classical time series
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4\. 将旧金山犯罪事件数据转换为经典时间序列
- en: '![](162fig01_alt.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](162fig01_alt.jpg)'
- en: 7.3.2\. Prediction on time-series data
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2\. 时间序列数据预测
- en: 'Just as there are two common types of time-series data, there are also two
    common types of predictions that you can make from time-series data. The first
    is *time-series forecasting*, which attempts to predict future values of the time
    series (or times of future events) based on past measurements. Time-series forecasting
    problems include the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 正如存在两种常见的时间序列数据类型一样，你也可以从时间序列数据中做出两种常见的预测。第一种是 *时间序列预测*，它试图根据过去的测量值预测时间序列的未来值（或未来事件的时刻）。时间序列预测问题包括以下内容：
- en: Predicting tomorrow’s price of a stock
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股票明天的价格
- en: Predicting tomorrow’s temperature in Phoenix, Arizona
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测亚利桑那州凤凰城的明天温度
- en: Forecasting next year’s energy consumption in Denmark
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测明年丹麦的能源消耗
- en: Forecasting the date of the next major hurricane in North America
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测北美下一次主要飓风的日期
- en: The first three of these tasks involve predicting future values of a classical
    time series, whereas the fourth is a prediction on a point-process dataset. The
    common thread is that each task involves analyzing the values of a single time
    series to make predictions about the future. Note that the vast majority of literature
    on time-series forecasting falls under the branch of time-series analysis, whereas
    comparatively little attention has been focused here by ML practitioners (though
    that is changing). For further details, any Google or Amazon search will reveal
    an abundance of results!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个任务中的前三个涉及预测经典时间序列的未来值，而第四个是在点过程数据集上的预测。共同点是每个任务都涉及分析单个时间序列的值来预测未来。请注意，关于时间序列预测的大多数文献都属于时间序列分析的分支，而机器学习实践者在这里的关注相对较少（尽管这种情况正在改变）。有关更多详细信息，任何Google或Amazon搜索都会揭示大量结果！
- en: 'The second common type of time-series prediction is *time-series classification
    or regression*. Instead of predicting future values of a single time series, the
    aim here is to classify (or predict a real-valued output on) hundreds or thousands
    of time series. Examples of this type of problem include the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种常见的时间序列预测类型是 *时间序列分类或回归*。这里的目标不是预测单个时间序列的未来值，而是对数百或数千个时间序列进行分类（或预测一个实值输出）。这类问题的例子包括以下内容：
- en: Using each user’s online clickstream to predict whether each user will click
    a specific ad
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每个用户的在线点击流来预测每个用户是否会点击特定的广告
- en: Employing a time series of QA measurements to determine which of a set of manufactured
    goods (for example, lightbulbs) are most likely to fail in the next month
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用QA测量值的时间序列来确定一组制造产品（例如，灯泡）中哪些最有可能在下一个月失效
- en: Predicting the lifetime value of each user of an online app based on each user’s
    in-app activity stream from the first week after sign-up
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户在注册后第一周内的应用活动流预测每个在线应用的用户的终身价值
- en: Predicting which patients are most likely to suffer post-op complications based
    on their medical records
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据患者的病历预测哪些患者最有可能遭受术后并发症
- en: Unlike time-series forecasting, ML has had a large influence on time-series
    classification and regression. The following section focuses primarily on creating
    time-series features for classification/regression purposes, but many of those
    methods can also be applied for time-series forecasting.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与时间序列预测不同，机器学习对时间序列分类和回归产生了重大影响。下一节主要关注为分类/回归目的创建时间序列特征，但许多这些方法也可以应用于时间序列预测。
- en: 7.3.3\. Classical time-series features
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.3\. 经典时间序列特征
- en: This section describes several of the most common feature-engineering approaches
    for classical time series. We start with the simplest time-series metrics and
    describe progressively more complicated and sophisticated approaches.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了经典时间序列中最常见的特征工程方法。我们从最简单的时间序列度量开始，并逐步描述更复杂和高级的方法。
- en: Simple time-series features
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 简单时间序列特征
- en: 'It may sound absurd, but the simplest time-series metrics involve ignoring
    the time axis altogether! Analyzing the distribution of measurements without considering
    the timestamps can often provide useful information for classification, regression,
    or forecasting. For discussion purposes, we outline four simple (yet powerful)
    metrics that involve only the marginal distribution of time-series measurements:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能有些荒谬，但最简单的时间序列度量可能完全忽略时间轴！在分析测量值的分布时，不考虑时间戳，通常可以提供用于分类、回归或预测的有用信息。为了讨论的目的，我们概述了四个简单（但强大）的度量，这些度量仅涉及时间序列测量值的边缘分布：
- en: '***Average—*** The mean or median of the measurements can uncover tendencies
    in the average value of a time series.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***平均—*** 测量值的平均值或中位数可以揭示时间序列平均值的趋势。'
- en: '***Spread—*** Measurements of the spread of a distribution, such as standard
    deviation, median absolute deviation, or interquartile range, can reveal trends
    in the overall variability of the measurements.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分散—*** 分布的分散度测量，如标准差、中位数绝对偏差或四分位数范围，可以揭示测量值的整体变异趋势。'
- en: '***Outliers—*** The frequency of time-series measurements that fall outside
    the range of the typical distribution (for example, larger than two, three, or
    four standard deviations from the mean) can carry predictive power in many use
    cases, such as prediction of process-line interruptions or failures.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***异常值—*** 落在典型分布范围之外的时间序列测量值的频率（例如，比平均值大两个、三个或四个标准差），在许多用例中可以携带预测能力，例如预测生产线中断或故障。'
- en: '***Distribution—*** Estimating the higher-order characteristics of the marginal
    distribution of the time-series measurements (for example, skew or kurtosis),
    or going a step further and running a statistical test for a named distribution
    (for example, normal or uniform), can be predictive in some scenarios.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分布—*** 估计时间序列测量值的边缘分布的高阶特征（例如，偏度或峰度），或者更进一步，对命名的分布（例如，正态分布或均匀分布）进行统计检验，在某些情况下可能是预测性的。'
- en: You can make things more sophisticated by computing *windowed statistics*, which
    entails calculating the preceding summary metrics within a specified time window.
    For instance, the mean or standard deviation of only the last week of measurements
    may be highly predictive. From there, you can also compute *windowed differences*,
    which would be the difference in those metrics from one time window to the next.
    The following listing presents a code example of computing those features.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过计算**窗口统计**来使事物更加复杂，这涉及到在指定时间窗口内计算前述汇总指标。例如，仅测量数据的最后一周的平均值或标准差可能具有高度预测性。从那里，您还可以计算**窗口差异**，这将是从一个时间窗口到下一个时间窗口这些指标之间的差异。以下列表展示了计算这些特征的一个代码示例。
- en: Listing 7.5\. Windowed statistics and differences
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5\. 窗口统计和差异
- en: '![](164fig01_alt.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图片](164fig01_alt.jpg)'
- en: Advanced time-series features
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高级时间序列特征
- en: Next, you move to more-sophisticated classical time-series features. *Autocorrelation*
    features measure the statistical correlation of a time series with a *lagged*
    version of itself. For example, the one-autocorrelation feature of a time series
    takes the original time series and correlates it with the same time series shifted
    over by one time bin to the left (with nonoverlapping portions removed). By shifting
    the time series like this, you can capture the presence of periodicity and other
    statistical structure in the time series. The shape of the autocorrelation function
    (autocorrelation computed over a grid of time lags) captures the essence of the
    structure of the time series. In Python, the `statsmodels` module contains an
    easy-to-use autocorrelation function. [Figure 7.7](#ch07fig07) shows how the autocorrelation
    is computed and plots an autocorrelation function for the SF crime data.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将转向更复杂的经典时间序列特征。**自相关**特征衡量时间序列与其**滞后**版本之间的统计相关性。例如，时间序列的一个自相关特征会将原始时间序列与向左移动一个时间间隔的相同时间序列（移除非重叠部分）进行相关性分析。通过以这种方式移动时间序列，您可以捕捉到时间序列中的周期性和其他统计结构的存在。自相关函数的形状（在时间滞后网格上计算的自相关）捕捉了时间序列结构的核心。在Python中，`statsmodels`模块包含一个易于使用的自相关函数。[图7.7](#ch07fig07)展示了自相关是如何计算的，并绘制了SF犯罪数据的自相关函数。
- en: 'Figure 7.7\. Top: Correlation of the original time series and 12-month lagged
    time series defines the 12-month autocorrelation. Bottom: The autocorrelation
    function for the SF crime data. The autocorrelation is high for short time scales,
    showing high dependence of any month’s crime on the previous months’ values.'
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7\. 顶部：原始时间序列与12个月滞后时间序列的相关性定义了12个月的自相关。底部：SF犯罪数据的自相关函数。对于短期尺度，自相关较高，表明任何月份的犯罪值对前几个月的值有高度依赖性。
- en: '![](07fig07_alt.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig07_alt.jpg)'
- en: '*Fourier analysis* is one of the most commonly used tools for time-series feature
    engineering. The goal of Fourier analysis is to decompose a time series into a
    sum of sine and cosine functions on a range of frequencies, which are naturally
    occurring in many real-world datasets. Performing this decomposition enables you
    to quickly identify periodic structure in the time series. The Fourier decomposition
    is achieved by using the discrete Fourier transform, which computes the *spectral
    density* of the time series—how well it correlates to a sinusoidal function at
    each given frequency—as a function of frequency. The resulting decomposition of
    a time series into its component spectral densities is called a *periodogram*.
    [Figure 7.8](#ch07fig08) shows the periodogram of the San Francisco crime data,
    computed using the `scipy.signal.periodogram` function (several Python modules
    have methods for periodogram estimation). From the periodogram, various ML features
    can be computed, such as the spectral density at specified frequencies, the sum
    of the spectral densities within frequency bands, or the location of the highest
    spectral density (which describes the fundamental frequency of oscillation of
    the time series). The following listing provides example code for periodogram
    computation and features.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*傅里叶分析*是时间序列特征工程中最常用的工具之一。傅里叶分析的目标是将时间序列分解为一系列频率上的正弦和余弦函数之和，这些频率在许多现实世界的数据集中自然存在。执行这种分解可以使你快速识别时间序列中的周期性结构。傅里叶分解是通过使用离散傅里叶变换来实现的，它计算时间序列的*谱密度*——即它在每个给定频率上与正弦函数的相关程度——作为频率的函数。将时间序列分解为其组成谱密度的结果称为*频谱图*。[图7.8](#ch07fig08)显示了使用`scipy.signal.periodogram`函数（几个Python模块有频谱估计的方法）计算出的旧金山犯罪数据的频谱图。从频谱图中，可以计算各种机器学习特征，如指定频率的谱密度、频率带内谱密度的总和，或最高谱密度的位置（这描述了时间序列振荡的基本频率）。以下列表提供了频谱计算和特征示例代码。'
- en: 'Figure 7.8\. Left: Periodogram of the San Francisco crime data, showing the
    spectral density as a function of frequency. Right: The same periodogram with
    the x-axis transformed from frequency to period.'
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8. 左：旧金山犯罪数据的频谱图，显示了频率作为函数的谱密度。右：将x轴从频率转换为周期的相同频谱图。
- en: '![](07fig08_alt.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig08_alt.jpg)'
- en: Listing 7.6\. Periodogram features
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6. 频谱特征
- en: '![](165fig01_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](165fig01_alt.jpg)'
- en: 'Several classical time-series models are commonly used in the time series analysis
    literature. The purpose of these models is to describe each value of the time
    series as a function of the past values of the time series. The models themselves
    have been widely used for time-series forecasting for decades. Now, as machine
    learning has become a mainstay in time-series data analysis, they’re often used
    for prediction in conjunction with more-sophisticated ML models such as SVMs,
    neural nets, and random forests. Examples of time-series models include the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 几种经典的时间序列模型在时间序列分析文献中常用。这些模型的目的是将时间序列的每个值描述为时间序列过去值的函数。这些模型本身已经广泛用于数十年的时间序列预测。现在，随着机器学习成为时间序列数据分析的主流，它们通常与更复杂的机器学习模型（如SVMs、神经网络和随机森林）一起用于预测。时间序列模型的例子包括以下内容：
- en: '***Autoregressive (AR) model—*** Each value in the time series is modeled as
    a linear combination of the last *p* values, where *p* is a free parameter to
    be estimated.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归 (AR) 模型**—时间序列中的每个值被建模为最后*p*个值的线性组合，其中*p*是一个待估计的自由参数。'
- en: '***Autoregressive–moving average (ARMA) model—*** Each value is modeled as
    the sum of two polynomial functions: the AR model and a moving-average (MA) model
    that’s a linear combination of the previous *q* error terms.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归-移动平均 (ARMA) 模型**—每个值被建模为两个多项式函数的和：AR模型和一个移动平均（MA）模型，该模型是前*q*个误差项的线性组合。'
- en: '***GARCH model—*** A model commonly used in financial analysis that describes
    the random noise terms of a time series using an ARMA model.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义自回归条件异方差 (GARCH) 模型**—一种在金融分析中常用的模型，它使用ARMA模型描述时间序列的随机噪声项。'
- en: '***Hidden Markov model (HMM)—*** A probabilistic model that describes the observed
    values of the time series as being drawn from a series of hidden states, which
    themselves follow a Markov process.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐马尔可夫模型 (HMM)**—一种概率模型，描述时间序列的观测值是从一系列隐藏状态中抽取的，这些隐藏状态本身遵循马尔可夫过程。'
- en: 'You can use these models to compute time-series features in various ways, including
    these:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这些模型以各种方式计算时间序列特征，包括以下这些：
- en: Using the predicted values from each of the models (and the differences between
    the predictions) as features themselves
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每个模型的预测值（以及预测之间的差异）本身作为特征
- en: Using the best-fit parameters of the models (for example, the values of *p*
    and *q* in an ARMA(*p*,*q*) model) as features
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型的最佳拟合参数（例如，ARMA(*p*,*q*)模型中的*p*和*q*的值）作为特征
- en: Calculating the statistical goodness-of-fit (for example, mean-square error)
    of a model and using it as a feature
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算模型的统计拟合优度（例如，均方误差）并将其用作特征
- en: 'In this way, a blend of classical time-series models and state-of-the-art machine-learning
    methodologies can be achieved. You can attain the best of both worlds: if an ARMA
    model is already highly predictive for a certain time series, the ML model that
    uses those predictions will also be successful; but if the ARMA model doesn’t
    fit well (as for most real-world datasets), the flexibility that the ML model
    provides can still produce highly accurate predictions.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，可以实现经典时间序列模型和最先进的机器学习方法的结合。你可以达到两者的最佳效果：如果ARMA模型对某个时间序列已经具有高度的预测性，那么使用这些预测的机器学习模型也将是成功的；但如果ARMA模型拟合不佳（如大多数实际数据集），机器学习模型提供的灵活性仍然可以产生高度准确的预测。
- en: 7.3.4\. Feature engineering for event streams
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.4\. 事件流的特征工程
- en: This section presents a brief look at feature engineering for event streams.
    As shown previously in [listing 7.4](#ch07ex04), event data can be converted to
    a classical time series. This enables you to employ all the feature-engineering
    processes described in the preceding two sections to extract classical time-series
    data on point-process data. But a number of additional features can be computed
    on event data because of its finer granularity.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了事件流的特征工程。正如之前在[代码列表7.4](#ch07ex04)中所示，事件数据可以转换为经典的时间序列。这使得你可以使用前两节中描述的所有特征工程过程来从点过程数据中提取经典时间序列数据。但由于事件数据的粒度更细，因此可以计算许多额外的特征。
- en: Analogous to the windowed statistics described in [section 7.1.3](#ch07lev2sec3),
    you can compute simple windowed and difference statistics on event data. But because
    point-process data allows an individual timestamp of each and every event, you
    can compute these statistics on any time window that you want, down to an extremely
    fine granularity. Further, statistics such as “time since last event,” “number
    of events in the past 48 hours,” and “average length of time between events” suddenly
    become possible.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第7.1.3节](#ch07lev2sec3)中描述的窗口统计，你可以在事件数据上计算简单的窗口和差异统计。但由于点过程数据允许每个事件都有单独的时间戳，因此你可以在任何你想要的时间窗口上计算这些统计，甚至达到极细的粒度。此外，“自上次事件以来时间”、“过去48小时内事件数量”和“事件之间的平均时间长度”等统计信息突然变得可行。
- en: 'Finally, just as classical time series are often modeled with statistical models
    like ARMA and HMM, point-process data is often described with models such as Poisson
    processes and nonhomogeneous Poisson processes. In a nutshell, these models describe
    the rate of incoming events as a function of time and enable you to predict the
    expected time until the next event. Feel free to explore these methods more on
    your own! Just as with the classical time-series models, machine-learning features
    can be derived from point-process models in three ways: using the predictions
    from the model, the parameters of the model, and the statistical goodness-of-fit
    of the model.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如经典时间序列通常使用ARMA和HMM等统计模型进行建模一样，点过程数据通常使用泊松过程和非齐次泊松过程等模型进行描述。简而言之，这些模型将事件到达率描述为时间的函数，并使你能够预测下一次事件发生的预期时间。请随意探索这些方法！就像经典时间序列模型一样，可以从点过程模型中通过三种方式推导出机器学习特征：使用模型的预测、模型的参数以及模型的统计拟合优度。
- en: 7.4\. Summary
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 概述
- en: 'In this chapter, you looked at methods for generating features from text and
    images. You can use these features in your ML algorithms to build models that
    are capable of “reading” or “seeing” with human-level perception. The main takeaways
    are as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了从文本和图像中生成特征的方法。你可以在你的机器学习算法中使用这些特征来构建具有人类水平感知能力的“阅读”或“观察”模型。主要收获如下：
- en: 'For text-based datasets, you need to transform variable-length documents to
    a fixed-length number of features. Methods for this include the following:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于文本的数据集，你需要将可变长度的文档转换为固定长度的特征数量。这些方法包括以下几种：
- en: Simple bag-of-words methods, in which particular words are counted for each
    document.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的词袋方法，其中对每个文档中的特定单词进行计数。
- en: The tf-idf algorithm, which takes into account the frequency of words in the
    entire corpus to avoid biasing the dictionary toward unimportant-but-common words.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf-idf算法，它考虑整个语料库中单词的频率，以避免将字典偏向于不重要但常见的单词。
- en: More-advanced algorithms for topic modeling, such as latent semantic analysis
    and latent Dirichlet analysis.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高级的主题建模算法，如潜在语义分析和潜在狄利克雷分析。
- en: Topic-modeling techniques can describe documents as a set of topics, and topics
    as a set of words. This allows sophisticated semantic understanding of documents
    and can help build advanced search engines, for example, in addition to the usefulness
    in the ML world.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模技术可以将文档描述为一组主题，主题为一组单词。这允许对文档进行复杂的语义理解，并有助于构建高级搜索引擎，例如，除了在机器学习世界中的有用性之外。
- en: You can use the scikit-learn and Gensim Python libraries for many interesting
    experiments in the field of text extraction.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用scikit-learn和Gensim Python库进行许多有趣的文本提取领域的实验。
- en: 'For images, you need to be able to represent characteristics of the image with
    numeric features:'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像，你需要能够用数值特征来表示图像的特征：
- en: You can extract information about the colors in the image by defining color
    ranges and color statistics.
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过定义颜色范围和颜色统计来提取图像中关于颜色的信息。
- en: You can extract potentially valuable image metadata from the image file itself;
    for example, by tapping into the EXIF metadata available in most image files.
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从图像文件本身中提取可能具有价值的图像元数据；例如，通过利用大多数图像文件中可用的EXIF元数据。
- en: 'In some cases, you need to be able to extract shapes and objects from images.
    You can use the following methods:'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，你需要能够从图像中提取形状和对象。你可以使用以下方法：
- en: Simple edge-detection-based algorithms using Sobel or Canny edge-detection filters
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sobel或Canny边缘检测滤波器进行简单的基于边缘检测的算法
- en: Sophisticated shape-extraction algorithms such as histogram of oriented gradients
    (HOG)
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如直方图方向梯度（HOG）等复杂的形状提取算法
- en: Dimensionality reduction techniques such as PCA
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如PCA之类的降维技术
- en: Automated feature extraction by using deep neural nets
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用深度神经网络进行自动特征提取
- en: 'Time-series data comes in two flavors: classical time series and point processes.
    A plethora of ML features can be estimated from this data.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列数据分为两种类型：经典时间序列和点过程。可以从这些数据中估计出大量的机器学习特征。
- en: 'Two principal machine-learning tasks are performed on time-series data:'
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间序列数据上执行两个主要的机器学习任务：
- en: Forecasting the value of a single time series
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测单个时间序列的值
- en: Classifying a set of time series
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一组时间序列进行分类
- en: For classical time series, the simplest features involve computing time-windowed
    summary statistics and windowed differences.
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于经典时间序列，最简单的特征涉及计算时间窗口的汇总统计量和窗口差异。
- en: More-sophisticated features involve the statistical characterization of the
    time series, using tools such as autocorrelation and Fourier analysis.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更复杂的特征涉及使用自相关和傅里叶分析等工具对时间序列进行统计分析。
- en: Various classical time-series models can be used to derive features. These include
    AR, ARMA, GARCH, and HMM.
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用各种经典时间序列模型来推导特征。这些包括AR，ARMA，GARCH和HMM。
- en: From point-process data, you can compute all these features and more, because
    of the finer granularity of the data.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据的更细粒度，从点过程数据中可以计算所有这些特征以及更多。
- en: Common models for point-process data include Poisson processes and nonhomogeneous
    Poisson processes.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点过程数据的常见模型包括泊松过程和非齐次泊松过程。
- en: 7.5\. Terms from this chapter
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5. 本章术语
- en: '| Word | Definition |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| feature engineering | Transforming input data to extract more value and improve
    the predictive accuracy of ML models. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程 | 将输入数据转换为提取更多价值并提高机器学习模型预测准确性的过程。|'
- en: '| natural language processing | The field that aims to make computers understand
    natural language. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言处理 | 旨在使计算机理解自然语言的领域。|'
- en: '| bag of words | A method for transforming text into numbers; counting the
    number of occurrences of a particular word in a document. |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 词袋模型 | 一种将文本转换为数字的方法；计算文档中特定单词出现的次数。|'
- en: '| stop words | Words that are common but not useful as a feature (for example,
    “the,” “is,” “and”). |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 停用词 | 作为特征不常用但无用的词（例如，“the”，“is”，“and”）|'
- en: '| sparse data | When data consists of mostly 0s and few data cells, we call
    the data sparse. Most NLP algorithms produce sparse data, which you need to use
    or transform for your ML algorithms. |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| sparse data | 当数据主要由0组成且数据单元格很少时，我们称数据为稀疏。大多数NLP算法产生稀疏数据，你需要使用或转换这些数据以用于你的ML算法。|'
- en: '| tf-idf | Term-frequency, inverse-document frequency. A bag-of-words method
    that’s normalized by text from the entire corpus. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| tf-idf | 词频，逆文档频率。一种通过整个语料库中的文本进行归一化的词袋方法。|'
- en: '| latent semantic analysis | A method for finding topics of interest in documents
    and connecting them to a set of words. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| latent semantic analysis | 一种在文档中寻找感兴趣的主题并将它们与一组单词连接起来的方法。|'
- en: '| latent Dirichlet analysis | An extension of the idea from LSA that works
    well with many text problems in practice. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| latent Dirichlet analysis | LSA思想的扩展，在实际中与许多文本问题表现良好。|'
- en: '| content expansion | The process of expanding the original content into more
    data (for example, by following links in a document). |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| content expansion | 将原始内容扩展为更多数据的过程（例如，通过跟随文档中的链接）。|'
- en: '| meta-features | A set of features that aren’t extracted from the content
    itself, but some connected metadata. |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| meta-features | 一组不是从内容本身提取，而是从一些相关元数据中提取的特征。|'
- en: '| EXIF data | A standard for defining metadata on images. Includes information
    about the photo (for example, manufacturer of the camera, resolution, aperture).
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| EXIF data | 定义图像元数据的标准。包括有关照片的信息（例如，相机制造商、分辨率、光圈）。|'
- en: '| edge detection | The process of detecting edges in images to remove the noise
    of most images. |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| edge detection | 检测图像边缘的过程，以去除大多数图像的噪声。|'
- en: '| HOG | Histogram of oriented gradients. An approach to image features that
    understands particular shapes and objects. |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| HOG | 直方图导向梯度。一种理解特定形状和对象图像特征的方法。|'
- en: '| PCA | Principal component analysis. A way to represent images by simpler,
    typical images, thus reducing the number of dimensions in images. Instead of 100
    pixels, an image can be approximated by two numbers: the distance to the two most
    principal components. |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| PCA | 主成分分析。一种通过更简单、典型的图像来表示图像的方法，从而减少图像的维度数。而不是100个像素，一个图像可以通过两个数字来近似：到两个主成分的距离。|'
- en: '| deep neural nets | An extension to artificial neural nets that has recently
    shown to perform well for machine learning on audiovisual data. |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| deep neural nets | 人工神经网络的扩展，最近在音频视觉数据的机器学习上表现出色。|'
- en: '| classical time series | Series of numerical measurements over time. |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| classical time series | 随时间进行的数值测量序列。|'
- en: '| point process | Series of events collected over time, each with a precise
    timestamp known. |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| point process | 在时间上收集的事件序列，每个事件都有一个精确的时间戳。|'
- en: '| time-series forecasting | Predicting future values of an individual time
    series. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| time-series forecasting | 预测单个时间序列的未来值。|'
- en: '| periodogram | Plot of the Fourier power spectral density of a time series
    as a function of frequency of oscillation. This technique can reveal the fundamental
    modes of oscillation and is a useful feature-engineering tool for time-series
    data. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| periodogram | 时间序列的傅里叶功率谱密度作为振荡频率的函数的图。这项技术可以揭示基本振荡模式，是时间序列数据的有用特征工程工具。|'
- en: 'Chapter 8\. Advanced NLP example: movie review sentiment'
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章\. 高级NLP示例：电影评论情感
- en: '*This chapter covers*'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using a real-world dataset for predicting sentiment from movie reviews
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实世界数据集预测电影评论的情感
- en: Exploring possible use cases for this data and the appropriate modeling strategy
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索此数据可能的用例和适当的建模策略
- en: Building an initial model using basic NLP features and optimizing the parameters
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基本的NLP特征构建初始模型并优化参数
- en: Improving the accuracy of the model by extracting more-advanced NLP features
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提取更高级的NLP特征来提高模型的准确性
- en: Scaling and other deployment aspects of using this model in production
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产中使用此模型的扩展和部署方面
- en: In this chapter, you’ll use some of the advanced feature-engineering knowledge
    acquired in the previous chapter to solve a real-world problem. Specifically,
    you’ll use advanced text and NLP feature-engineering processes to build and optimize
    a model based on user-submitted reviews of movies.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用在前一章中获得的某些高级特征工程知识来解决一个实际问题。具体来说，你将使用高级文本和NLP特征工程过程来构建和优化一个基于用户提交的电影评论的模型。
- en: As always, you’ll start by investigating and analyzing the dataset at hand to
    understand the feature and target columns so you can make the best decisions about
    which feature-extraction and ML algorithms to use. You’ll then build the initial
    model from the simplest feature-extraction algorithms to see how you can quickly
    get a useful model with only a few lines of code. Next, you’ll dig a little deeper
    into the library of feature-extraction and ML modeling algorithms to improve the
    accuracy of the model even further. You’ll conclude by exploring various deployment
    and scalability aspects of putting the model into production.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，您将从调查和分析手头的数据集开始，以了解特征和目标列，这样您就可以做出最佳决定，选择使用哪些特征提取和机器学习算法。然后，您将从最简单的特征提取算法开始构建初始模型，看看您如何只用几行代码就能快速得到一个有用的模型。接下来，您将进一步深入研究特征提取和机器学习建模算法的库，以进一步提高模型的准确性。最后，您将探索将模型投入生产的各种部署和可扩展性方面。
- en: 8.1\. Exploring the data and use case
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 探索数据和用例
- en: In this chapter, you’ll use data from a competition on *Kaggle*—a data-science
    challenge site where data scientists from around the world work on solving well-defined
    problems posed by companies to win prizes. You’ll work with this data as you learn
    to use the tools developed in the previous chapters to solve a real-world problem
    via machine learning.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使用来自*Kaggle*竞赛的数据——这是一个数据科学挑战网站，全球的数据科学家在这里解决公司提出的明确问题以赢得奖品。您将使用这些数据，在学习使用前几章开发的工具的同时，通过机器学习解决一个现实世界的问题。
- en: The data used in this chapter is from the Bag of Words Meets Bags of Popcorn
    competition ([www.kaggle.com/c/word2vec-nlp-tutorial](http://www.kaggle.com/c/word2vec-nlp-tutorial)).
    You need to create an account on the Kaggle platform to download the data, but
    that’s probably a good thing because you might want to try your newly acquired
    ML skills on a big-prize competition anyway!
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的数据来自“单词袋遇见爆米花袋”竞赛([www.kaggle.com/c/word2vec-nlp-tutorial](http://www.kaggle.com/c/word2vec-nlp-tutorial))。您需要在Kaggle平台上创建一个账户来下载数据，但这可能是一件好事，因为您可能仍然想在一个大奖竞赛中尝试您新获得的机器学习技能！
- en: In the following sections, we begin by describing the dataset, what the individual
    columns mean, and how the data was generated. Next, we dive a level deeper, present
    the data attributes, and make some initial observations about the data that we
    have. From here, we brainstorm possible use cases that we could solve with the
    dataset at hand and review the data requirements and real-world implications of
    each potential use case. Finally, we use this discussion to select a single use
    case that we’ll solve in the remainder of the chapter.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们首先描述数据集，说明各个列的含义以及数据是如何生成的。然后，我们进一步深入，展示数据属性，并对我们拥有的数据进行一些初步观察。从这里，我们头脑风暴可能的用例，这些用例我们可以用手头的数据集来解决，并审查每个潜在用例的数据需求和现实世界影响。最后，我们使用这次讨论来选择一个将在本章剩余部分解决的单一用例。
- en: Note that although we structure this section to first describe and explore the
    data and then to figure out a use case to solve, typically the steps are taken
    in reverse order. Usually an ML practitioner will start with a use case, hypothesis,
    or set of questions to answer and then search for and explore data to appropriately
    solve the problem at hand. This is the preferred methodology, because it forces
    the practitioner to think hard about the use case and the data required before
    going “in the weeds” of the dataset. That said, it’s not uncommon to be handed
    a dataset and be asked to build something cool!
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管我们本节的结构是先描述和探索数据，然后找出一个用例来解决问题，但通常步骤是按照相反的顺序进行的。通常，机器学习实践者会从一个用例、假设或要回答的问题集开始，然后寻找并探索数据，以适当地解决手头的问题。这是首选的方法，因为它迫使实践者在深入数据集的细节之前，认真思考用例和所需的数据。尽管如此，接到一个数据集并被要求构建一些酷炫的东西的情况并不少见！
- en: 8.1.1\. A first glance at the dataset
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1. 初步查看数据集
- en: Our dataset consists of written movie reviews from the Internet Movie Database,
    IMDb ([www.imdb.com](http://www.imdb.com)). The training data consists of 50,000
    reviews, selected so that each movie has no more than 30 reviews in the dataset.
    For each review, the outcome variable is encoded as a binary feature, with the
    value 1 if the manual IMDb rating for that review is greater than 6, and the value
    0 if the rating is less than 5\. No reviews in the intermediate ratings of 5–6
    are included in the dataset.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库由来自互联网电影数据库（IMDb，[www.imdb.com](http://www.imdb.com)）的书面电影评论组成。训练数据包括50,000条评论，选择时确保每部电影在语料库中的评论不超过30条。对于每条评论，结果变量被编码为一个二进制特征，如果该评论的手动IMDb评分大于6，则值为1，如果评分小于5，则值为0。介于5到6之间的中间评分的评论不包括在语料库中。
- en: The challenge with this dataset is to devise an ML system to learn the patterns
    and structure of language that constitute positive reviews versus those that constitute
    negative reviews. Critically, you’ll train your model to learn only from the text
    of the reviews and not from other contextual data such as the movie actors, director,
    genre, or year of release. Presumably, that data would help the accuracy of your
    model predictions, but it isn’t available in this dataset.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的挑战是设计一个机器学习系统来学习构成正面评论和负面评论的语言模式和结构。关键的是，你需要训练你的模型只从评论的文本中学习，而不是从其他上下文数据中学习，例如电影演员、导演、类型或发行年份。假设这些数据会帮助提高你模型预测的准确性，但在这个数据集中这些数据是不可用的。
- en: In addition to a training dataset, a separate testing dataset of 25,000 reviews
    of movies that don’t appear in the training dataset is provided. In principle,
    this set of data could be used to validate the performance of your model and to
    estimate how well the model will perform when deployed to a real-world production
    setting. But Kaggle doesn’t supply the labels for the testing set. Therefore,
    you’ll construct your own testing set by splitting Kaggle’s training set into
    70% training and 30% testing.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练数据集外，还提供了一个包含25,000条电影评论的独立测试数据集，这些电影不在训练数据集中。原则上，这组数据可以用来验证你模型的性能，并估计模型在实际部署到现实世界生产环境中的表现。但是Kaggle没有提供测试集的标签。因此，你需要通过将Kaggle的训练集分成70%的训练集和30%的测试集来构建自己的测试集。
- en: Note the importance of ensuring that no movies in the training set appear in
    the testing set.^([[1](#ch08fn01)]) If, for instance, reviews from the same movies
    were included in both the training and testing sets, then your model could learn
    which movie titles were good and bad, instead of focusing on the language constituting
    positivity and negativity. But in production you’ll be applying this ML model
    to new movies, with titles you’ve never seen. This leakage of movies from the
    training to the testing set could lead you to believe that your model is better
    than it is when predicting the sentiment of reviews of new movies. For this reason,
    we recommend that holdout testing sets always be constructed with temporal cutoffs,
    so that the testing set consists of instances that are newer than the training
    instances.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 注意确保训练集中没有电影出现在测试集中的重要性.^([[1](#ch08fn01)]) 例如，如果同一电影的评论同时包含在训练集和测试集中，那么你的模型可能会学习到哪些电影标题是好是坏，而不是专注于构成积极和消极的语言。但在生产中，你将应用这个机器学习模型到新的电影上，这些电影标题是你从未见过的。从训练集到测试集的电影泄露可能会导致你相信你的模型在预测新电影评论的情感时比实际情况更好。因此，我们建议始终使用时间截止点来构建保留测试集，以便测试集包含比训练实例更新的实例。
- en: ¹
  id: totrans-415
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-416
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our training set, we don’t have an indicator of which movie each review describes.
    Therefore, we make the assumption that the training set is provided presorted
    by date, and we divide the set so that multiple reviews of the same movie fall
    together in the training or testing set.
  id: totrans-417
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的训练集中，我们没有指示每条评论描述的是哪部电影。因此，我们假设训练集是按日期预先排序提供的，并且我们将集合分割，使得同一电影的多个评论在训练集或测试集中聚集在一起。
- en: 8.1.2\. Inspecting the dataset
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2. 检查数据集
- en: The individual reviews in this dataset vary in length, from a single sentence
    up to several pages of text. Because the reviews are pulled from dozens of film
    critics, the vocabulary can vary dramatically from review to review. The key is
    to build a machine-learning model that can detect and exploit the differences
    between the positive and negative reviews so that it can accurately predict the
    sentiment of new reviews.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集中的单个评论在长度上有所不同，从一句话到几页的文本。由于评论是从几十位电影评论家那里收集的，因此评论之间的词汇量可能会有很大差异。关键是构建一个机器学习模型，它可以检测和利用正面和负面评论之间的差异，以便它可以准确地预测新评论的情感。
- en: 'The first step of the ML process is to look at the data to see what’s there
    and to begin thinking about the other steps of the ML process, such as model type
    and featurization. To start the data review process, take a look at the 10 shortest
    reviews in [figure 8.1](#ch08fig01). Look at the first row (`id = 10962_3`). This
    particular review demonstrates how nuanced this problem can be: although the review
    clearly states that the “movie is terrible,” it also says that there are “good
    effects.” Despite the use of the word *good*, any person would clearly agree that
    this is a negative movie review. The challenge now is to teach the ML model that
    even if positive words such as *good* are used, the use of the phrase “movie is
    terrible” trumps all!'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习过程的第一步是查看数据，看看有什么，并开始思考机器学习过程的其他步骤，如模型类型和特征化。为了开始数据审查过程，请查看[图8.1](#ch08fig01)中的10个最短的评论。查看第一行（`id
    = 10962_3`）。这个特定的评论展示了这个问题有多么微妙：尽管评论明确表示“电影很糟糕”，但它也说有“好的效果”。尽管使用了“好”这个词，但任何人都清楚地同意这是一篇负面电影评论。现在的挑战是教会机器学习模型，即使使用了像“好”这样的积极词汇，短语“电影很糟糕”仍然占上风！
- en: Figure 8.1\. Ten example reviews in the training set, chosen from the shortest
    reviews. For each review, you’re provided only an ID, the binary sentiment, and
    the text of the review.
  id: totrans-421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1. 训练集中的10个示例评论，从最短的评论中选择。对于每个评论，你只提供了ID、二元情感和评论文本。
- en: '![](08fig01_alt.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![08fig01_alt.jpg](08fig01_alt.jpg)'
- en: Similarly, these 10 sample reviews include several examples of negative statements.
    Phrases such as “never get tired” and “no wasted moments” clearly indicate positive
    qualities of movies, even if the component words are all negative in nature. This
    demonstrates that to do well in predicting sentiment, you must combine information
    across multiple (neighboring) words.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这10个样本评论包括几个负面陈述的例子。像“永远不会感到疲倦”和“没有浪费的时刻”这样的短语明显表明了电影的积极品质，即使构成这些短语的单词在本质上都是消极的。这表明，为了在预测情感方面做得好，你必须结合多个（相邻）单词的信息。
- en: Looking through a few of the other (longer) reviews, it’s apparent that these
    reviews typically consist of verbose, descriptive, flowery language. The language
    is often sarcastic, ironic, and witty. This makes it a great dataset to demonstrate
    the power of ML to learn nuanced patterns from real data and to make accurate
    predictions under uncertainty.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 翻阅一些其他（较长的）评论，很明显，这些评论通常由冗长、描述性、华丽的语言组成。语言通常是讽刺的、讽刺的、机智的。这使得它成为一个很好的数据集，可以展示机器学习从真实数据中学习细微模式并在不稳定的情况下做出准确预测的能力。
- en: 8.1.3\. So what’s the use case?
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.3. 那么用例有什么用？
- en: 'Often practitioners of (non-real-world) machine learning dive into a problem
    without thinking hard about the practical use of their ML model. This is a mistake,
    because the choice of use case can help determine how you structure the problem
    and solution, including the following:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 经常情况下，(非现实世界)机器学习的实践者在没有深思熟虑他们的机器学习模型的实际用途的情况下就一头扎进了一个问题。这是一个错误，因为用例的选择可以帮助确定你如何构建问题和解决方案，包括以下方面：
- en: How to encode the target variable (for example, binary versus multiclass versus
    real value)
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何编码目标变量（例如，二元、多类或实值）
- en: Which evaluation criterion to optimize
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化哪个评估标准
- en: What kinds of learning algorithms to consider
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应考虑哪些学习算法
- en: Which data inputs you should and should not use
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该使用和不应该使用哪些数据输入
- en: So before you get started with ML modeling, you first need to determine what
    real-world use case you want to solve with this dataset.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你开始机器学习建模之前，你首先需要确定你想要用这个数据集解决哪个现实世界的用例。
- en: 'For each of three possible use cases, you’ll consider the following:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三个可能的用例中的每一个，你将考虑以下方面：
- en: Why would the use case be valuable?
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么用例会有价值？
- en: What kind of training data would you need?
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要什么样的训练数据？
- en: What would an appropriate ML modeling strategy be?
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的机器学习建模策略是什么？
- en: What evaluation metric should you use for your predictions?
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该使用什么评估指标来预测？
- en: Is the data you have sufficient to solve this use case?
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你拥有的数据是否足够解决这个用例？
- en: Based on the answers to those questions, you’ll choose a single use case, which
    you’ll spend the remainder of the chapter solving.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对这些问题的回答，你将选择一个单一用例，你将在本章的剩余部分解决它。
- en: 'Use case 1: ranking new movies'
  id: totrans-439
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用例 1：对电影进行排名
- en: 'The first and most obvious use case for a movie review dataset is to automatically
    rank all new movies based on the text of all their reviews:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电影评论数据集的第一个也是最明显的用例是，根据所有评论的文本自动对所有新电影进行排名：
- en: '*Why would the use case be valuable?* This could be a powerful way to decide
    which movie to watch this weekend. Scoring individual reviews is one thing, but
    obviously the more valuable use case is to score each movie on the overall positivity
    of its reviews. Sites such as Rotten Tomatoes get heavy traffic because of their
    ability to reliably rate each movie.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么这个用例会有价值？* 这可能是一个决定这个周末要看哪部电影的有力方式。对个别评论进行评分是一回事，但显然更有价值的用例是对每部电影的评论整体积极性进行评分。像
    Rotten Tomatoes 这样的网站因其能够可靠地评估每部电影而获得大量流量。'
- en: '*What kind of training data would you need?* The basic necessities would be
    the review text, an indication of the sentiment of each review, and knowledge
    about which movie each review refers to. With these three components, building
    a movie-ranking system would be feasible.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你需要什么样的训练数据？* 基本需求包括评论文本、每个评论的情感指示以及关于每条评论所指电影的了解。有了这三个组成部分，构建一个电影排名系统是可行的。'
- en: '*What would an appropriate ML modeling strategy be?* There are a couple of
    options: (a) You could treat each movie as an ML instance, aggregate the individual
    reviews for each movie, and roll up the review sentiment into either an average
    score or a multiclass model. (b) You could continue to treat each review as an
    ML instance, score every new review on its positivity, and then assign each new
    movie its average positivity score. We prefer option (b), because aggregating
    all reviews for a single movie together could result in some confusing patterns
    for ML—particularly if the individual reviews are highly polarized! Scoring the
    individual results and then averaging them into a “metascore” is a more straightforward
    approach.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合适的机器学习建模策略会是什么？* 有几个选择：(a) 你可以将每部电影视为一个机器学习实例，汇总每部电影的个别评论，并将评论情感汇总为平均分数或多类模型。(b)
    你可以继续将每条评论视为一个机器学习实例，对每条新评论的积极性进行评分，然后为每部新电影分配其平均积极性分数。我们更喜欢选项 (b)，因为将单一电影的全部评论汇总在一起可能会对机器学习产生一些令人困惑的模式——尤其是如果个别评论高度两极分化！对个别结果进行评分，然后将其平均到“综合评分”中是一个更直接的方法。'
- en: '*What evaluation metric should you use for your predictions?* Assume here that
    you have a binary outcome variable for each review and that your ML algorithm
    assigns a score to each review on its likelihood of being a positive review, which
    you aggregate into a single score per movie. What you care about here is how closely
    your score matches the true average rating for that movie (for example, percentage
    of positive reviews), which could lead you to use a metric such as R². But you
    could imagine using a different evaluation metric that focuses more weight at
    the top of the ranking list. In reality, you’re probably interested in a movie
    ranking in order to pick a flick from the top of the list to see this Saturday.
    Therefore, you’d instead select a metric that focuses on your ability to get the
    top of the ranking list right. In this case, you’d select a metric such as the
    true-positive rate at a small false-positive rate (for example, 5% or 10%).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你应该使用什么评估指标来预测？* 假设你为每条评论都有一个二元结果变量，并且你的机器学习算法为每条评论分配一个分数，表示其可能是正面评论的可能性，并将其汇总为每部电影的单一分数。你关心的是你的分数与该电影真实平均评分的匹配程度（例如，正面评论的百分比），这可能导致你使用如
    R² 这样的指标。但你可以想象使用一个更侧重于排名列表顶部的评估指标。在现实中，你可能对电影排名感兴趣，以便从列表顶部挑选一部电影在本周六观看。因此，你会选择一个侧重于你正确获取排名列表顶部能力的指标。在这种情况下，你会选择一个如小错误正率下的真正正率（例如，5%
    或 10%）这样的指标。'
- en: '*Is the data you have sufficient to solve this use case?* Unfortunately, no.
    You have everything you need except knowledge of which movie each review is describing!'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你拥有的数据是否足够解决这个用例？* 不幸的是，不是。你拥有所有你需要的东西，除了知道每条评论描述的是哪部电影！'
- en: 'Use case 2: rating each review from 1 to 10'
  id: totrans-446
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用例 2：对每条评论进行 1 到 10 的评分
- en: 'A second possible use case is to auto-rate each review on a scale of 1 to 10
    (the IMDb scale) based on the set of user reviews about each movie:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个可能用例是根据每部电影的用户评论集自动对每个评论进行 1 到 10 级的评分（IMDb 评分）：
- en: '*Why would the use case be valuable?* Any new review could be automatically
    assigned a rating without any manual reading or scoring. This would cut down on
    a lot of manual labor that’s required to curate the IMDb website and movie ratings;
    or, if users are providing a score along with their rating, it could provide a
    more objective score based on the text of the user’s review.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么这个用例会有价值？* 任何新的评论都可以自动分配一个评分，而无需任何人工阅读或评分。这将减少许多需要用于维护 IMDb 网站 和电影评分的体力劳动；或者，如果用户在他们的评分中提供了分数，它可以根据用户评论的文本提供更客观的评分。'
- en: '*What kind of training data would you need?* Just the text of each review and
    a score, from 1 to 10, for each review.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*您需要什么样的训练数据？* 只需每个评论的文本和每个评论的分数，从 1 到 10。'
- en: '*What would an appropriate ML modeling strategy be?* Again, there are two options:
    (a) Treat the outcome variable as a real-valued number and fit a regression model.
    (b) Treat the outcome variable as categorical and fit a multiclass classification
    model. In this case, we much prefer option (a) because, unlike classification,
    it considers the scores on a numerical scale.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合适的机器学习建模策略是什么？* 同样，有两种选择：(a) 将结果变量视为实数值并拟合回归模型。(b) 将结果变量视为分类变量并拟合多类分类模型。在这种情况下，我们更倾向于选择
    (a)，因为与分类不同，它考虑了数值尺度上的分数。'
- en: '*What evaluation metric should you use for your predictions?* If you choose
    to run a regression model, the typical regression evaluation metrics such as R²
    or mean squared error are natural choices.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*您应该使用什么评估指标来评估您的预测？* 如果您选择运行回归模型，典型的回归评估指标，如 R² 或均方误差，是自然的选择。'
- en: '*Is the data you have sufficient to solve this use case?* Again, it’s not.
    You have only a Boolean version of each review (positive versus negative) and
    not the finely grained numerical score.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*您拥有的数据是否足够解决这个用例？* 同样，也不够。您只有每个评论的布尔版本（正面与负面），而没有精细的数值分数。'
- en: 'Use case 3: separating the positive from the negative reviews'
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用例 3：区分正面评论和负面评论
- en: 'The final use case to consider is separating all the positive reviews from
    the rest:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的最后一个用例是将所有正面评论与其他评论分开：
- en: '*Why would the use case be valuable?* This use case would represent a less
    granular version of use case 2, whereby each new review could be automatically
    classified as positive or negative (instead of scored from 1 to 10). This classification
    could be useful for IMDb to detect the positive reviews, which it could then promote
    to its front page or (better yet) sell to movie producers to use as quotes on
    their movie posters.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么这个用例会有价值？* 这个用例将代表用例 2 的一个更细粒度的版本，其中每个新的评论可以自动分类为正面或负面（而不是从 1 到 10 进行评分）。这种分类对
    IMDb 来说可以用来检测正面评论，然后它可以将其推广到首页或（更好的是）出售给电影制片商，用于电影海报上的引用。'
- en: '*What kind of training data would you need?* Only the review text and the binary
    positive versus negative indicator.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*您需要什么样的训练数据？* 只需评论文本和正负二值指示器。'
- en: '*What would an appropriate ML modeling strategy be?* You’d fit a binary classification
    model. From there, you could assign a prediction score for each new review on
    the likelihood that it’s a positive review.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合适的机器学习建模策略是什么？* 您将拟合一个二元分类模型。从那里，您可以为每个新的评论分配一个预测分数，表示它是正面评论的可能性。'
- en: '*What evaluation metric should you use for your predictions?* It depends on
    how you want to use your predictions. If the use case is to automatically pull
    out the 10 most positive reviews of the week (for example, to use on the IMDb
    front page), then a good evaluation metric would be the true-positive rate at
    a very small false-positive rate (for example, 1%). But if the goal is to try
    to find *all* positive reviews while ignoring the negative reviews (for example,
    for complete automated sentiment tagging of every review), then a metric such
    as accuracy or area under the curve (AUC) would be appropriate.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*您应该使用什么评估指标来评估您的预测？* 这取决于您如何使用您的预测。如果用例是自动提取每周最积极的 10 条评论（例如，用于 IMDb 首页），那么一个好的评估指标将是极低的假阳性率下的真正阳性率（例如，1%）。但如果目标是尝试找到
    *所有* 正面评论而忽略负面评论（例如，用于对每条评论进行完整的自动情感标记），那么准确率或曲线下面积（AUC）这样的指标将是合适的。'
- en: '*Is the data you have sufficient to solve this use case?* Finally, yes! You
    have a training set of the movie review text and the binary sentiment variable.
    In the remainder of the chapter, you’ll build out a machine-learning solution
    for this use case.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你拥有的数据是否足够解决这个用例？* 最后，是的！你有一个包含电影评论文本和二元情感变量的训练集。在本章的剩余部分，你将为这个用例构建一个机器学习解决方案。'
- en: 'To recap, you first learned the basic details about the dataset: hand-written
    movie ratings from IMDb. Then, you dove a little deeper to explore some of the
    patterns and trends in the data. Finally, you considered possible ML use cases.
    For each use case, you explored the value of a machine-learning solution to the
    problem, the basic data requirements to build out an ML solution, and how to go
    about putting together a solution.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，你首先了解了数据集的基本细节：来自IMDb的手写电影评分。然后，你深入探索了一些数据中的模式和趋势。最后，你考虑了可能的机器学习用例。对于每个用例，你探讨了机器学习解决方案对问题的价值、构建机器学习解决方案的基本数据需求以及如何构建解决方案。
- en: Next, you’ll build out an ML solution to separate positive from negative movie
    reviews.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将构建一个机器学习解决方案来区分正面和负面的电影评论。
- en: 8.2\. Extracting basic NLP features and building the initial model
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 提取基本NLP特征和构建初始模型
- en: 'Because the movie review dataset consists of only the review text, you need
    to use text and natural-language features to build a meaningful dataset for your
    sentiment model. In the previous chapter, we introduced various methods for extracting
    features from text, and we use this chapter to discuss various practical aspects
    of working with ML and free-form text. The steps you’ll go through in this section
    are as follows:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 由于电影评论数据集仅包含评论文本，你需要使用文本和自然语言特征来构建一个对你的情感模型有意义的数据集。在上一章中，我们介绍了从文本中提取特征的多种方法，我们使用本章来讨论与机器学习和自由文本工作相关的各种实际方面。在这一节中，你将经历的步骤如下：
- en: Extracting features from movie reviews with the bag-of-words method
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词袋方法从电影评论中提取特征
- en: Building an initial model using the naïve Bayes ML algorithm
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯机器学习算法构建初始模型
- en: Improving your bag-of-words features with the tf-idf algorithm
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用tf-idf算法改进你的词袋特征
- en: Optimizing model parameters
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化模型参数
- en: 8.2.1\. Bag-of-words features
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 词袋特征
- en: 'As you may recall from our discussion of NLP features in the previous chapter,
    we started out with a simple technique to featurize natural-language data: bag
    of words. This method analyzes the entire corpus of text, builds a dictionary
    of all words, and translates every instance in the dataset into a list of numbers,
    counting how many times each word appears in the document. To refresh your memory,
    let’s revisit bag of words in [figure 8.2](#ch08fig02).'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从上一章关于自然语言处理特征的讨论中可能记得，我们最初使用一种简单的技术来对自然语言数据进行特征化：词袋。这种方法分析整个文本语料库，构建一个包含所有单词的词典，并将数据集中的每个实例转换为一个数字列表，统计每个单词在文档中出现的次数。为了刷新您的记忆，让我们回顾一下[图8.2](#ch08fig02)中的词袋。
- en: Figure 8.2\. The bag-of-words vectorization algorithm. From a dictionary of
    words, you can transform any new document (for example, Text 1, Text 2 in the
    figure) into a list of numbers that counts how many times each word appears in
    the document.
  id: totrans-470
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2\. 词袋向量化算法。从一个单词词典中，你可以将任何新的文档（例如图中的文本1、文本2）转换为一个数字列表，该列表统计文档中每个单词出现的次数。
- en: '![](08fig02_alt.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2](08fig02_alt.jpg)'
- en: In [listing 8.1](#ch08ex01), you load the dataset, create a 70%–30% train-test
    split, and use a simple word-count method for extracting features. An important
    point to realize in this process is that you can’t contaminate the bag-of-words
    dictionary with words from the test set. This is why you split the dataset into
    training and testing subsets *before* you build the vectorizer dictionary—to get
    a realistic estimate of the accuracy of the model on previously *unseen* data.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表8.1](#ch08ex01)中，你加载数据集，创建70%–30%的训练-测试分割，并使用简单的词频方法提取特征。在这个过程中，一个重要的认识是，你不能用测试集中的单词污染词袋词典。这就是为什么你在构建向量器词典之前将数据集分割成训练集和测试集——为了对模型在之前*未见过*的数据上的准确性进行现实估计。
- en: Listing 8.1\. Building word-count features from the movie review dataset
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1\. 从电影评论数据集中构建词频特征
- en: '![](179fig01_alt.jpg)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![图179.1](179fig01_alt.jpg)'
- en: Take a look at a subset of the features generated in [figure 8.3](#ch08fig03).
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 查看在[图8.3](#ch08fig03)中生成的特征子集。
- en: Figure 8.3\. A small 7 × 10 subset view of the word-count features that you’ll
    use for building the model. The full dataset is a sparse matrix of size 17,500
    × 65,005 (17,500 documents in the training set by 65,005 unique words in the training
    set). A sparse matrix is useful when most of the values are 0, which is the case
    in most bag-of-words– based features; in the full dictionary of words, individual
    words are unlikely to appear in a particular document.
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3\. 用于构建模型的词频特征的小型7 × 10子集视图。完整的数据集是一个大小为17,500 × 65,005的稀疏矩阵（训练集中有17,500个文档，65,005个唯一的单词）。当大多数值都是0时，稀疏矩阵是有用的，这在大多数基于词袋的特征中都是这种情况；在完整的单词字典中，单个单词不太可能出现在特定的文档中。
- en: '![](08fig03_alt.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig03_alt.jpg)'
- en: From [figure 8.3](#ch08fig03), it’s clear that the dataset consists of mostly
    zeros with only a few exceptions. We call such a dataset *sparse*, a common attribute
    of NLP datasets. This has consequences when you want to use the dataset for features
    in an ML model, something we discuss in the next section before building an actual
    model to predict the sentiment of reviews.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图8.3](#ch08fig03)中可以看出，数据集主要由零组成，只有少数例外。我们称这样的数据集为“稀疏”，这是NLP数据集的常见属性。当你想使用数据集作为机器学习模型中的特征时，这会产生影响，我们将在构建实际模型来预测评论情感之前讨论这个问题。
- en: 8.2.2\. Building the model with the naïve Bayes algorithm
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. 使用朴素贝叶斯算法构建模型
- en: Now that you have a proper featurized dataset, you can use the features to build
    the model as usual. For highly sparse datasets like this, some ML algorithms work
    much better than others. Specifically, some algorithms have built-in support for
    sparse data, and those algorithms are generally much more efficient, at least
    in memory usage but often also in CPU usage and time to build. If you inspect
    the generated feature set from [listing 8.1](#ch08ex01), you’ll find that only
    0.2% of the cells in the dataset have nonzero elements. Using the dense representation
    of the dataset would significantly increase the size of the data in memory.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有一个合适的特征化数据集，你可以像往常一样使用这些特征来构建模型。对于像这样高度稀疏的数据集，一些机器学习算法比其他算法工作得更好。具体来说，一些算法内置了对稀疏数据的支持，这些算法通常在内存使用上，甚至在CPU使用和时间上都要高效得多。如果你检查[列表8.1](#ch08ex01)中生成的特征集，你会发现数据集中只有0.2%的单元格具有非零元素。使用数据集的密集表示将显著增加内存中数据的大小。
- en: '|  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The basics of the naïve Bayes classifier**'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯分类器的基本原理**'
- en: The naïve Bayes (NB) classifier algorithm is a simple ML algorithm that was
    created for use in text classification, an area of ML where it can still be competitive
    with more-advanced general-purpose algorithms. The name stems from the fact that
    the Bayes formula is applied to the data with very “naïve” assumptions about independence.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯公式的朴素贝叶斯（NB）分类器算法是一种简单的机器学习算法，它最初是为文本分类而创建的，在机器学习的这个领域中，它仍然可以与更高级的通用算法竞争。这个名字来源于贝叶斯公式应用于数据时，对独立性的假设非常“朴素”。
- en: This assumption is what usually makes the algorithm less useful for general
    (dense) problems, because the features are rarely anywhere near independent. For
    sparse-text features, this assumption still isn’t true, but it’s true enough for
    the algorithm to work surprisingly well in practice. The NB classifier is one
    of the few ML algorithms that’s simple enough to derive in a few lines, and we
    explain some of the highlights in this sidebar.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 这种假设通常使得算法对于一般的（密集）问题不太有用，因为特征很少是相互独立的。对于稀疏文本特征，这个假设仍然不成立，但在实践中，这个假设足够准确，使得算法可以出奇地好地工作。朴素贝叶斯分类器是少数几个足够简单，可以在几行中推导出来的机器学习算法之一，我们将在侧边栏中解释一些亮点。
- en: 'In this chapter, our goal is to classify a review by finding the probability
    *p*(*C[k]*|*x*) of the review sentiment being “bad” (k = 0) or “good” (k = 1)
    based on the features x of the instance. In probability theory using the Bayes
    formula, this can be written like so:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是通过对实例的特征x进行分类，找到评论情感为“差”（k = 0）或“好”（k = 1）的概率 *p*(*C[k]*|*x*)。在概率论中使用贝叶斯公式，这可以写成如下形式：
- en: '*p*(*C[k]*|*x*) ~ *p*(*C[k]*)*p*(*x*|*C[k]*)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*C[k]*|*x*) ~ *p*(*C[k]*)*p*(*x*|*C[k]*)'
- en: '*p*(*x*|*C[k]*) is known as the joint probability of the features x if the
    instance was of class *C[k]*. Because of the independence assumption (the *naïve*
    part), there’s no cross-feature probability, and this becomes simply the product
    of the probability of each of the features given the class:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*x*|*C[k]*)被称为如果实例属于类别 *C[k]* 的特征的联合概率。由于独立性假设（“朴素”部分），没有交叉特征概率，这变成了每个特征给定类别的概率的乘积：'
- en: '*p*(*C[k]*|*x*) ~ *p*(*C[k]*)*p*(*x*[1]|*C[k]*)*p*(*x*[2]|*C[k]*)*p*(*x*[3]|*C[k]*)*p*(*x*[4]|*C[k]*)...'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*C[k]*|*x*) ~ *p*(*C[k]*)*p*(*x*[1]|*C[k]*)*p*(*x*[2]|*C[k]*)*p*(*x*[3]|*C[k]*)*p*(*x*[4]|*C[k]*)...'
- en: '![](181equ01.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![图片](181equ01.jpg)'
- en: Because *p*(*C[k]*) is the marginal class distribution—the overall breakdown
    of good and bad sentiment reviews—which you can easily find from the data, you
    only need to figure out what *p*(*x[i]*|*C[k]*) is. You can read this expression
    as “the probability of a specific feature for a specific class.” For example,
    you’d expect the probability of having the word *great* in a good-sentiment review
    being higher than in a bad-sentiment review.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 *p*(*C[k]*) 是边缘类别分布——好和坏情感评论的整体分布，你可以很容易地从数据中找到，所以你只需要弄清楚 *p*(*x[i]*|*C[k]*)
    是什么。你可以将这个表达式读作“特定类别的特定特征的概率”。例如，你预计在正面情感评论中包含单词 *great* 的概率要高于在负面情感评论中。
- en: You can imagine learning this from the data by counting the feature (word) presence
    across all documents in each class. The probability distribution that generates
    such counts is called the *multinomial* distribution, and *p*(*x[i]*|*C[k]*) becomes
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象通过计算每个类别中所有文档中的特征（词）出现次数来从数据中学习这一点。生成这种计数的概率分布称为 *多项式* 分布，*p*(*x[i]*|*C[k]*)
    变为
- en: '![](181equ02.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![图片](181equ02.jpg)'
- en: 'You use this in the previous equation and move to log space for convenience:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 你将这个用于前面的方程，并移动到对数空间以方便计算：
- en: '![](181equ03.jpg)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![图片](181equ03.jpg)'
- en: Here *b* is log[*p*(*C[k]*)] (known from the data), *x* represents the features
    of the instance you want to predict, and *w[k]* is log(*p[k[i]]*)—the fraction
    of times a word appears in a good or bad document, which you’ll learn at model
    build time. Please note that we’ve left out various constants throughout this
    calculation, and there are multiple implementation details to consider when coding
    this algorithm from scratch, but the basics outlined here remain true.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 *b* 是 log[*p*(*C[k]*)]（从数据中得知），*x* 代表你想要预测的实例的特征，而 *w[k]* 是 log(*p[k[i]]*)—一个词在好文档或坏文档中出现的频率，你将在模型构建时学习到。请注意，我们在整个计算中省略了各种常数，当从头开始编写此算法时，需要考虑多个实现细节，但这里概述的基本内容仍然是正确的。
- en: '|  |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: One of the algorithms that works well for classification with sparse natural
    language processing (NLP) features is the naïve Bayes algorithm, specifically
    the multinomial (see the sidebar). In the following listing, you build the model
    on the features from [listing 8.1](#ch08ex01).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有稀疏自然语言处理（NLP）特征的分类，有一个算法效果很好，那就是朴素贝叶斯算法，特别是多项式（见侧边栏）。在下面的列表中，你将在 [列表 8.1](#ch08ex01)
    中的特征上构建模型。
- en: Listing 8.2\. Building the first review sentiment model using multinomial naïve
    Bayes
  id: totrans-498
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 使用多项式朴素贝叶斯构建第一个评论情感模型
- en: '[PRE3]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To evaluate the performance of the model, you define a function in [listing
    8.3](#ch08ex03) and call it on the initial model predictions. The accuracy metrics
    that you’ll report in this chapter are the general classification accuracy (fraction
    of correctly classified documents), the receiver operating characteristic (ROC)
    curve, and the corresponding area under the curve (AUC) number. These were all
    introduced in [chapter 4](kindle_split_014.html#ch04) and used in many of our
    examples.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的性能，你在 [列表 8.3](#ch08ex03) 中定义一个函数，并在初始模型预测上调用它。本章中你将报告的准确度指标是通用分类准确率（正确分类的文档比例）、接收者操作特征（ROC）曲线以及相应的曲线下面积（AUC）数值。这些内容都在
    [第 4 章](kindle_split_014.html#ch04) 中介绍过，并在我们的大量示例中使用过。
- en: Listing 8.3\. Evaluating the initial model
  id: totrans-501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 评估初始模型
- en: '[PRE4]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result of running this code is shown in [figure 8.4](#ch08fig04).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码的结果显示在 [图 8.4](#ch08fig04) 中。
- en: Figure 8.4\. ROC curve of the classification performance of the simple bag-of-words
    model. The classification accuracy—the fraction of correctly classified reviews—as
    well as the AUC (area under the ROC curve) metrics are printed in the figure.
    The accuracy shows that you’d expect to correctly classify 88% of the reviews
    with this model, but by using the ROC curve, you can trade false-positive rate
    (FPR) for true-positive rate (TPR), and vice versa. If there were many reviews
    that humans needed to look through based on this classification, you might want
    to fix the FPR at a low value, which would in turn lower the true-positive detection
    rate.
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4\. 简单词袋模型的分类性能ROC曲线。图中的分类准确率——正确分类评论的比例——以及AUC（ROC曲线下的面积）指标被打印出来。准确率表明，你预计使用这个模型可以正确分类88%的评论，但通过使用ROC曲线，你可以用假正率（FPR）交换真正正率（TPR），反之亦然。如果有很多评论需要根据这个分类进行人工检查，你可能希望将FPR设定在一个较低的水平，这反过来会降低真正正检测率。
- en: '![](08fig04_alt.jpg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig04_alt.jpg)'
- en: Looking at [figure 8.4](#ch08fig04), you can see that the performance of your
    bare-bones model isn’t bad at all. You classify 88% of the reviews correctly,
    but you can dial the number of false positives versus true positives up or down,
    depending on your preference for more noise or better detection rate.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[图8.4](#ch08fig04)，你可以看到你的基础模型的表现并不差。你正确分类了88%的评论，但你可以根据你对更多噪声或更好检测率的偏好，调整假正与真正正的比例。
- en: 'Let’s try this with a few new example reviews by passing some text through
    the vectorizer and model for sentiment predictions:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将一些文本通过向量化和模型进行情感预测来尝试几个新的示例评论：
- en: '[PRE5]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A positive sentiment is indicated by 1, so this sounds about right. Let’s try
    another one:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 正面情绪表示为1，所以这听起来很正确。让我们再试一个：
- en: '[PRE6]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A negative sentiment is indicated by 0, so again this is indeed correct. Okay,
    let’s try to trick the model:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 负面情绪表示为0，所以这确实是对的。好的，让我们尝试欺骗模型：
- en: '[PRE7]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: No luck, the prediction is still correct. Maybe if you introduce more positive
    words into the negative review?
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 没有成功，预测仍然是正确的。也许如果你在负面评论中引入更多的正面词汇？
- en: '[PRE8]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Nope, this is one clever model. The word *bad* must have a strong influence
    on the classification, so perhaps you can cheat the model by using that in a positive
    review:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，这是一个聪明的模型。单词*bad*对分类有很强的影响，所以你可能可以通过在正面评论中使用这个词来欺骗模型：
- en: '[PRE9]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, you succeed in somewhat cheating the model. This little exercise is
    fun, but it also shows the power of the model in understanding arbitrary natural
    language in the movie review domain. In the next section, you’ll try to improve
    the initial model by going a bit further than our simple word-count features and
    by finding better values for the parameters of the feature and modeling algorithms.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你成功地在某种程度上欺骗了模型。这个小练习很有趣，但也展示了模型在理解电影评论领域任意自然语言方面的力量。在下一节中，你将通过比我们简单的词频特征更进一步，并找到特征和建模算法参数的更好值来尝试改进初始模型。
- en: 8.2.3\. Normalizing bag-of-words features with the tf-idf algorithm
  id: totrans-518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3\. 使用tf-idf算法对词袋特征进行归一化
- en: In the previous chapter, we introduced tf-idf as an upgrade to simple word-count
    features. In essence, tf-idf normalizes the word counts based on the frequency
    of how often each word appears across the documents. The main idea is that common
    words get smaller weighting factors, and relatively rare words get larger weighting
    factors, which enables you to dig deeper into the (often highly informative) words
    that appear less often in the dataset.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了tf-idf作为简单词频特征的升级。本质上，tf-idf根据每个单词在文档中出现的频率对词频进行归一化。主要思想是，常见单词获得较小的权重因子，而相对罕见的单词获得较大的权重因子，这使你能够深入挖掘（通常是高度信息丰富的）在数据集中出现频率较低的单词。
- en: In this section, you’ll use tf-idf for your features to see whether you can
    gain extra accuracy. The change is easy with scikit-learn, because you simply
    need to switch out your `CountVectorizer` for a `TfidfVectorizer`. The code is
    shown in the next listing.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用tf-idf作为你的特征来查看你是否可以获得额外的准确性。使用scikit-learn进行更改很容易，因为你只需要将你的`CountVectorizer`替换为`TfidfVectorizer`。代码将在下一列表中展示。
- en: Listing 8.4\. Using tf-idf features in your model
  id: totrans-521
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4\. 在模型中使用tf-idf特征
- en: '![](184fig01_alt.jpg)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![](184fig01_alt.jpg)'
- en: The performance of the tf-idf model is shown in [figure 8.5](#ch08fig05). You
    can see how the tf-idf features improved the model accuracy slightly. Specifically,
    the ROC curve shows that it should be better at avoiding false positives. Imagine
    that you had numerous reviews coming in but wanted to flag bad reviews for human
    inspection. A lower false-positive rate would present fewer reviews to the reviewer
    that were actually positive, so they could work through the queue faster.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf模型的性能在[图 8.5](#ch08fig05)中显示。你可以看到tf-idf特征如何略微提高了模型准确度。具体来说，ROC曲线显示它应该更好地避免误报。想象一下，你有很多评论进来，但想标记出需要人类检查的不良评论。更低的误报率将向审阅者展示更少的实际上是正面的评论，这样他们可以更快地处理队列。
- en: Figure 8.5\. ROC curves for the tf-idf model on top of the previous bag-of-words
    model. You can see a slight improvement in both classification accuracy and AUC
    (area under the ROC curve). The tf-idf model curve specifically shows improvements
    in the low FPR range; the model would yield fewer false positives for the same
    number of correctly classified reviews. If humans were in the classification review
    loop, you’d have less noise to sift through.
  id: totrans-524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.5\. 在之前的词袋模型之上tf-idf模型的ROC曲线。你可以看到分类准确性和AUC（ROC曲线下的面积）都有轻微的提高。tf-idf模型曲线特别显示了在低FPR范围内的改进；对于相同数量的正确分类评论，模型将产生更少的误报。如果人类在分类审查循环中，你将需要筛选的噪声会更少。
- en: '![](08fig05_alt.jpg)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig05_alt.jpg)'
- en: Both our tf-idf NLP feature-extraction algorithm and our naïve Bayes modeling
    algorithm have knobs that can be turned to tune the algorithm for specific details
    in the dataset. We call such knobs *hyperparameters*. This comes from the fact
    that the variables (features) of the model can be considered parameters as well,
    whereas these algorithm parameters work at a higher level. Before you accept your
    model performance, it’s important that you try different values for these parameters,
    and this is the topic of the next section.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两个tf-idf NLP特征提取算法和朴素贝叶斯建模算法都有可以调整以调整算法以适应数据集特定细节的旋钮。我们称这些旋钮为*超参数*。这源于模型变量（特征）也可以被视为参数，而算法参数则工作在更高的层次。在你接受模型性能之前，尝试这些参数的不同值是非常重要的，这也是下一节的主题。
- en: 8.2.4\. Optimizing model parameters
  id: totrans-527
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.4\. 优化模型参数
- en: 'The simplest way to find the best parameters of a model is to try to build
    a bunch of models with different parameters and look at the performance metric
    of interest. The problem is that you can’t assume that the parameters are independent
    of each other—varying one parameter may affect the optimal value of another. This
    can be solved in a brute-force way by building a model for any combination of
    parameters. But if there are many parameters, this quickly becomes intractable,
    especially if it takes a while to build the model just once. We discussed some
    solutions in [chapter 4](kindle_split_014.html#ch04), but you’ll probably be surprised
    by how often ML practitioners still rely on the brute-force way. You’ll need to
    build up intuition about which parameters may be more independent of each other
    and which have the largest effect on which types of dataset. For this exercise,
    you have three parameters to optimize: two tf-idf parameters (`max_features`,
    `min_df`) and one naïve Bayes parameter (`nb_alpha`).'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 找到模型最佳参数的最简单方法就是尝试构建具有不同参数的一组模型，并查看感兴趣的指标。问题是你不能假设参数之间是独立的——改变一个参数可能会影响另一个参数的最佳值。这可以通过为任何参数组合构建模型的方式来解决。但如果有很多参数，这很快就会变得难以处理，尤其是如果构建模型本身就需要花费一些时间。我们在[第4章](kindle_split_014.html#ch04)中讨论了一些解决方案，但你可能会惊讶地发现，ML从业者仍然经常依赖于这种方法。你需要培养对哪些参数可能更独立以及哪些参数对哪种类型的数据集影响最大的直觉。对于这个练习，你有三个参数需要优化：两个tf-idf参数（`max_features`，`min_df`）和一个朴素贝叶斯参数（`nb_alpha`）。
- en: The first thing you need is a function that you can call repeatedly to build
    a model and return the parameters and the metric of interest (in this case, the
    AUC). The following listing defines this function.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要的是一个你可以反复调用来构建模型并返回参数和感兴趣指标（在这种情况下，是AUC）的函数。下面的列表定义了这个函数。
- en: Listing 8.5\. Model building method useful for parameter optimization
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.5\. 用于参数优化的模型构建方法
- en: '[PRE10]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the repeatable model building function defined in [listing 8.5](#ch08ex05),
    you can go ahead and run your optimization pipeline by defining the possible values
    of your parameters (chosen randomly or by intuition) and run the loop. This is
    done in the next listing.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在[列表8.5](#ch08ex05)中定义的可重复模型构建函数，你可以通过定义参数的可能值（随机选择或凭直觉选择）并运行循环来运行你的优化管道。这将在下一个列表中完成。
- en: Listing 8.6\. Parameter optimization loop
  id: totrans-533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6\. 参数优化循环
- en: '![](186fig01_alt.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图片](186fig01_alt.jpg)'
- en: 'The parameters you optimize over are these:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 你要优化的参数如下：
- en: '*`max_features`*—The maximum number of word columns for the tf-idf algorithm
    to create. From looking at the data, you know that all words amount to about 65,000
    columns, so you try out a number of a similar size in a range. `None` specifies
    to use all words.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`max_features`*—tf-idf算法要创建的最大词列数。从数据中可以看出，所有词加起来大约有65,000列，所以你尝试在类似大小的范围内尝试一些数值。`None`指定使用所有词。'
- en: '*`min_df`*—The minimum number of times a word must appear in the dataset to
    be included in the features. This is an example of potential parameter dependency,
    because the number of words in the dictionary (and hence `max_features`) could
    be changed by changing `min_df`.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`min_df`*—一个词必须出现在数据集中多少次才能被包含在特征中。这是一个潜在的参数依赖性的例子，因为通过改变`min_df`，字典中的词数（以及`max_features`）可能会改变。'
- en: '*`nb_alpha`*—The alpha (smoothing) parameter of the naïve Bayes classifier.
    This is the only parameter that you can tune on this specific ML algorithm. The
    values to choose here require a bit more research into what the parameter means
    and how others have been using it in other circumstances.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`nb_alpha`*—朴素贝叶斯分类器的alpha（平滑）参数。这是你可以调整的特定ML算法的唯一参数。这里要选择的值需要更多研究参数的含义以及其他人如何在其他情况下使用它。'
- en: The last thing to mention about the code in [listing 8.6](#ch08ex06) is the
    use of the `product` function from the `itertools` module—a collection of Python
    functions that makes it easier to work with data. This function is a clever way
    to generate all combinations of a set of lists (Cartesian product). The results
    from running the code in [listing 8.6](#ch08ex06) are shown in [figure 8.6](#ch08fig06).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[列表8.6](#ch08ex06)中的代码的最后一点是使用来自`itertools`模块的`product`函数——一组Python函数，使处理数据更加容易。这个函数是一种巧妙的方法来生成一组列表的所有组合（笛卡尔积）。[列表8.6](#ch08ex06)中代码运行的结果显示在[图8.6](#ch08fig06)中。
- en: Figure 8.6\. A subset of results from the parameter optimization loop. The parameter
    combination in iteration 27 produces the best model overall.
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6\. 参数优化循环的一些结果子集。第27次迭代的参数组合总体上产生了最佳模型。
- en: '![](08fig06.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig06.jpg)'
- en: '[Figure 8.6](#ch08fig06) shows the output of some of the optimization runs.
    You had only three parameters with 36 possible value combinations, so this didn’t
    take more than 10 minutes because the naïve Bayes training time is relatively
    low, but you could easily imagine wanting to try many more values of many more
    parameters, and the optimization would take a long time. Another trick for finding
    the optimal parameters is to start with a broad range of values and then dive
    more deeply into the optimal value range with subsequent optimization runs over
    different parameter values. It’s clear from the table how different parameters
    seem to improve the AUC of the model. Iteration 27 had the best results with these
    values:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.6](#ch08fig06)显示了某些优化运行的结果。你只有三个参数，36种可能的值组合，所以这没有超过10分钟，因为朴素贝叶斯训练时间相对较低，但你很容易想象想要尝试更多参数的更多值，优化将需要很长时间。找到最佳参数的另一个技巧是，从广泛的值开始，然后在随后的优化运行中更深入地探索最佳值范围。从表中可以看出，不同的参数似乎提高了模型的AUC。第27次迭代使用这些值取得了最佳结果：'
- en: '*`max_features`*—None (all words, default)'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`max_features`*—None（所有词，默认）'
- en: '*`min_df`*—1 (default)'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`min_df`*—1（默认）'
- en: '*`nb_alpha`*—0.01'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`nb_alpha`*—0.01'
- en: So, interestingly, you managed to improve on the model performance quite a bit
    by finding a better value for the alpha parameter of the naïve Bayes algorithm.
    Let’s look at the evolution of the AUC when varying each parameter (fixing the
    others at their optimal values) in [figure 8.7](#ch08fig07).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有趣的是，你通过找到朴素贝叶斯算法的alpha参数的更好值，成功地显著提高了模型性能。让我们看看当改变每个参数（固定其他参数在其最优值）时AUC的变化情况（[图8.7](#ch08fig07)）。
- en: Figure 8.7\. The AUC improvements from varying three parameters of the feature
    and ML algorithms. You can see that (a) a higher `max_features` gives a better
    AUC, (b) a lower `min_df` gives a better AUC, and (c) a lower alpha gives a better
    AUC. This doesn’t mean that the best values for each of them individually necessarily
    yields the best combined. The best combined parameters from our optimization run
    are `max_features=None` (all words, default), `min_df=1` (minimum, default), `alpha=0.01`
    (main reason for improvement). The best AUC is 0.974\. All graphs shown can be
    reproduced using the code in the accompanying Python notebook.
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7. 通过改变特征和ML算法的三个参数带来的AUC改进。您可以看到：(a)更高的`max_features`给出了更好的AUC，(b)更低的`min_df`给出了更好的AUC，(c)更低的alpha给出了更好的AUC。这并不意味着每个参数的最佳值必然会产生最佳的组合。我们优化运行中最佳组合参数是`max_features=None`（所有单词，默认值），`min_df=1`（最小值，默认值），`alpha=0.01`（改进的主要原因）。最佳AUC是0.974。所有显示的图表都可以使用附带的Python笔记本中的代码重新生成。
- en: '![](08fig07_alt.jpg)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig07_alt.jpg)'
- en: Each of these plots is only one perspective on the AUC evolution, because you’d
    need a four-dimensional plot to plot the AUC as a function of all the parameters.
    But it’s still interesting to see how the model responds to varying each value.
    For instance, the higher the number of features, the better (the largest possible
    value won). The smaller the number of `min_df`, the better (the smallest possible
    value won). And then, the smaller the `nb_alpha`, the better. Because this has
    no theoretical lower limit, this should prompt you to try even lower values in
    another run. We leave this as an exercise for you (but, anecdotally, we weren’t
    able to find a much better value).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表只是AUC演变的单一视角，因为您需要一个四维图表来将AUC作为所有参数的函数来绘制。但仍然很有趣地看到模型对每个值的变化的响应。例如，特征数量越多，效果越好（赢得了最大可能值）。`min_df`的数量越小，效果越好（赢得了最小可能值）。然后，`nb_alpha`越小，效果越好。因为这一项没有理论上的下限，这应该会促使您在另一次运行中尝试更低的值。我们将这留作您的练习（但根据经验，我们并没有找到更好的值）。
- en: 'The ROC curve of the optimized model is plotted with the previous models in
    [figure 8.8](#ch08fig08). You can see a substantial improvement in model performance
    for both metrics and all points on the ROC curve. This is a great example of how
    it can pay off to tune your model hyperparameters to gain extra prediction power.
    One last thing to note here: you could, of course, imagine that new choices of
    model parameters could, in turn, affect which feature and modeling algorithms
    (for example, word count versus tf-idf) would perform best, and each algorithm
    would potentially have a new set of parameters to optimize. To be fully rigorous,
    you’d need to optimize across all choices of algorithms and their parameters,
    but this is infeasible for most real-world problems, and the trade-off here is
    to go through your optimization in milestones. For example, first you fix the
    NLP algorithm to use and then the ML model, and then you optimize those parameters.
    Your project could require a different set of milestones—again, you’ll develop
    intuition about these things as you build successive ML models.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 优化模型的ROC曲线与先前模型一起绘制在[图8.8](#ch08fig08)中。您可以看到模型性能在两个指标和ROC曲线的所有点上都有显著提高。这是一个很好的例子，说明调整模型超参数以获得额外的预测能力是有益的。最后要注意的一点是：当然，您可以想象新的模型参数选择可能会反过来影响哪些特征和建模算法（例如，词频与tf-idf）将表现最好，每种算法都可能有一组新的参数需要优化。为了完全严谨，您需要优化所有算法及其参数的选择，但对于大多数现实世界问题来说这是不可行的，这里的权衡是在里程碑中完成优化。例如，首先确定要使用的NLP算法，然后是ML模型，然后优化这些参数。您的项目可能需要不同的里程碑——再次强调，您将在构建连续的ML模型的过程中对这些事情产生直觉。
- en: Figure 8.8\. The ROC curve of the optimized model versus the previous models.
    In our test set evaluation, this model seems to be universally better (at every
    point on the curve), and the expected accuracy increased considerably.
  id: totrans-551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8. 优化模型与先前模型的ROC曲线。在我们的测试集评估中，这个模型似乎在所有曲线上都表现得更好，预期准确率显著提高。
- en: '![](08fig08_alt.jpg)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig08_alt.jpg)'
- en: The ROC curves in [figure 8.8](#ch08fig08) conclude our initial modeling experiments.
    From basic algorithms and very little code, you’ve managed to build a model with
    pretty good accuracy on natural-language data alone. In the next section, you’ll
    go a step further in your feature-engineering and modeling efforts and see various
    aspects of deploying such a model into a real-world production-ready system.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.8](#ch08fig08)中的ROC曲线总结了我们的初步建模实验。仅从基本算法和少量代码中，你就成功地构建了一个在自然语言数据上具有相当高准确性的模型。在下一节中，你将在特征工程和建模工作中更进一步，并了解将此类模型部署到实际生产就绪系统中的各个方面。'
- en: 8.3\. Advanced algorithms and model deployment considerations
  id: totrans-554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3. 高级算法和模型部署考虑因素
- en: In the previous section, we were concerned with building a model using relatively
    simple features and ML algorithms. The accuracy of any of the models in that section
    may have been good enough for our needs. You can try the next idea for optimizing
    the model, but there’s always a trade-off between the time you spend and the potential
    value brought by incremental improvements in model accuracy. We encourage you
    to get a handle on the value of each percentage improvement, for example, in the
    form of saved human-reviewer time, and how much you can afford to spend up front.
    As you saw, our very first model was certainly capable of understanding review
    sentiment in many cases and may well have been a good enough model to begin with.
    Often it’s more valuable to put a slightly lower-accuracy model into production
    and get live feedback from the system if possible.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们关注的是使用相对简单的特征和机器学习算法构建模型。该节中任何模型的准确性可能已经足够满足我们的需求。你可以尝试下一个优化模型的想法，但时间和模型准确度增量改进带来的潜在价值之间总是存在权衡。我们鼓励你掌握每个百分比改进的价值，例如，以节省人工审查时间的形式，以及你愿意前期投入多少。正如你所见，我们的第一个模型在许多情况下确实能够理解审查情感，并且可能一开始就是一个足够好的模型。通常，将略微低精度的模型投入生产并获得系统的实时反馈更有价值。
- en: 'With that advice out of the way, let’s go against it and try to optimize this
    model a bit further. Next, you’ll look into generating features from a new natural-language
    modeling technique, originally developed by Google: *word2vec*. After you’ve extracted
    the word2vec features, you’ll switch to the random forest algorithm to better
    support the new features.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循上述建议后，让我们反其道而行之，尝试进一步优化这个模型。接下来，你将研究从谷歌最初开发的新自然语言建模技术中生成特征：*word2vec*。在提取word2vec特征后，你将切换到随机森林算法以更好地支持新特征。
- en: 8.3.1\. Word2vec features
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1. Word2vec特征
- en: A relatively new approach to natural language processing has been introduced
    by Google in the form of the word2vec project. A word2vec model is itself an ML
    model that’s built using deep neural networks, a branch of ML that has recently
    been producing state-of-the-art results, especially on human-related domains such
    as natural language, speech, and images.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过word2vec项目引入了一种相对较新的自然语言处理方法。word2vec模型本身就是一个使用深度神经网络构建的机器学习模型，深度神经网络是机器学习的一个分支，最近在自然语言、语音和图像等与人类相关的领域产生了最先进的结果。
- en: To build a word2vec model on your training set, you’ll use the Gensim NLP library
    for Python, which has a nice word2vec implementation built in. You previously
    used Gensim in [chapter 7](kindle_split_018.html#ch07) to work with LDA, another
    topic model similar to word2vec.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练集上构建word2vec模型，你将使用Python的Gensim NLP库，该库内置了一个很好的word2vec实现。你之前在[第7章](kindle_split_018.html#ch07)中使用Gensim处理LDA，另一个类似于word2vec的主题模型。
- en: In Gensim, you need to do a bit of extra work to prepare your documents for
    modeling, because the Gensim algorithms work on sentences (lists of words already
    split up) instead of arbitrary documents. This can be more work up front, but
    it also gives you a better understanding of what goes into your model. In [listing
    8.7](#ch08ex07), you’ll build a simple tokenization function that removes stop
    words and punctuation characters, and converts all words to lowercase. Note that
    this was all done automatically in the scikit-learn word vectorizers; we could
    have used the same functionality or similar functions from the NLTK Python NLP
    toolkit, but we chose to write it out ourselves here for educational purposes.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gensim中，您需要做一些额外的工作来准备您的文档以进行建模，因为Gensim算法在句子（已经分割成单词的列表）上工作，而不是任意文档。这可能会在前期增加工作量，但它也使您更好地了解模型中包含的内容。在[列表8.7](#ch08ex07)中，您将构建一个简单的标记化函数，该函数删除停用词和标点符号，并将所有单词转换为小写。请注意，这都是在scikit-learn词向量器中自动完成的；我们可以使用相同的功能或类似的功能从NLTK
    Python NLP工具包中，但我们选择在这里自己编写它，以教育目的。
- en: Listing 8.7\. Document tokenization
  id: totrans-561
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7\. 文档标记化
- en: '![](ch08ex07-0.jpg)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![](ch08ex07-0.jpg)'
- en: '![](ch08ex07-1.jpg)'
  id: totrans-563
  prefs: []
  type: TYPE_IMG
  zh: '![](ch08ex07-1.jpg)'
- en: From this function, you can tokenize any list of documents, and you can now
    proceed to build your first word2vec model. For more information on the parameters
    of the algorithm, please see the Gensim documentation.^([[2](#ch08fn02)])
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个函数中，您可以标记化任何文档列表，并且现在您可以继续构建您的第一个word2vec模型。有关算法参数的更多信息，请参阅Gensim文档.^([[2](#ch08fn02)])
- en: ²
  id: totrans-565
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-566
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
  id: totrans-567
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
- en: Listing 8.8\. Word2vec model
  id: totrans-568
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8\. Word2vec模型
- en: '![](192fig01_alt.jpg)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![](192fig01_alt.jpg)'
- en: You can see how a single word is represented as a vector (of 300 numbers, in
    this case). In order to use the word2vec model to generate features for your ML
    algorithm, you need to convert your reviews into feature vectors. You know how
    to represent single words as vectors, so a simple idea is to represent a review
    document (list of words) as the average vector of all the words in the document.
    In the next listing, you’ll build a function to do exactly this.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到单个单词是如何表示为一个向量（在这个例子中是300个数字）。为了使用word2vec模型为您的人工智能算法生成特征，您需要将您的评论转换为特征向量。您知道如何将单个单词表示为向量，因此一个简单的方法是将评论文档（单词列表）表示为文档中所有单词的平均向量。在下一个列表中，您将构建一个函数来完成这项工作。
- en: Listing 8.9\. Word2vec featurization
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9\. Word2vec特征化
- en: '![](192fig02_alt.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![](192fig02_alt.jpg)'
- en: You’re now ready to build a model on your newly generated word2vec features.
    As you may recall from our ML algorithm discussions in [section 8.2.2](#ch08lev2sec5),
    the naïve Bayes classifier works well with sparse data but not so well with dense
    data. The word2vec features have indeed converted your documents from the ~65,000
    sparse word-count features into only hundreds of dense features. The deep-learning
    model has learned higher-level topics of the model ([listing 8.8](#ch08ex08)),
    and each document can be represented as a combination of topics ([listing 8.9](#ch08ex09)).
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以构建一个基于您新生成的word2vec特征的模型。如您从我们的机器学习算法讨论中回忆起[第8.2.2节](#ch08lev2sec5)，朴素贝叶斯分类器在稀疏数据上表现良好，但在密集数据上表现不佳。word2vec特征确实将您的文档从65,000个稀疏词频特征转换为仅几百个密集特征。深度学习模型已经学习了模型的高级主题([列表8.8](#ch08ex08))，每个文档都可以表示为主题的组合([列表8.9](#ch08ex09))。
- en: 8.3.2\. Random forest model
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2\. 随机森林模型
- en: 'The multinomial naïve Bayes algorithm introduced in the previous section is
    incompatible with the new word2vec features, because they can’t be considered
    generated by a multinomial distribution. You could use other distributions to
    continue to work with the naïve Bayes algorithm, but you’ll instead rely on an
    old friend of ours: the random forest algorithm. In the following listing, you’ll
    build a 100-tree random forest model on the word2vec features and analyze the
    performance as usual on the test set.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中引入的多项式朴素贝叶斯算法与新的word2vec特征不兼容，因为它们不能被认为是多项式分布生成的。您可以使用其他分布继续使用朴素贝叶斯算法，但您将依赖我们的一位老朋友：随机森林算法。在下面的列表中，您将在word2vec特征上构建一个100棵树的随机森林模型，并像往常一样在测试集上分析性能。
- en: Listing 8.10\. Building a random forest model on the word2vec features
  id: totrans-576
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10\. 在word2vec特征上构建随机森林模型
- en: '[PRE11]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The performance of the word2vec random forest model is compared to your previous
    models in [figure 8.9](#ch08fig09). You can see how your new model indeed improves
    the model accuracy in your chosen evaluation metric and across all points on the
    ROC curve.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8.9](#ch08fig09)中比较了word2vec随机森林模型与您之前模型的性能。您可以看到，您的新模型确实在您选择的评估指标和ROC曲线的所有点上提高了模型精度。
- en: Figure 8.9\. The ROC curve of the word2vec model along with previous models.
    You can see an improvement for all values of the ROC curve, also reflected in
    the increased accuracy and AUC numbers.
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9。word2vec模型及其之前模型的ROC曲线。您可以看到ROC曲线的所有值都有所改进，这也反映在增加的准确性和AUC数值上。
- en: '![](08fig09_alt.jpg)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9的替代文本](08fig09_alt.jpg)'
- en: With your final model illustrated in [figure 8.9](#ch08fig09), you’re satisfied
    with the performance and will stop optimization work for now. You could try many
    more things to improve the accuracy even further. Most likely, not even humans
    would be capable of correctly classifying the sentiment of all the reviews; there
    may be some incorrect labels or some reviews for which the sentiment isn’t easily
    understandable.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8.9](#ch08fig09)中展示了您的最终模型后，您对性能感到满意，现在将停止优化工作。您还可以尝试更多的事情来进一步提高准确率。很可能是，即使是人类也无法正确分类所有评论的情感；可能存在一些错误的标签，或者有些评论的情感不容易理解。
- en: 'But the model can likely get much better than what you’ve achieved so far.
    We’ll leave you, dear reader, with an initial list of things that we would try
    out, in rough order of priority:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个模型可能比您迄今为止所取得的成果要好得多。我们将给您，亲爱的读者，一个初步的清单，其中列出了我们打算尝试的一些事情，按照优先级的大致顺序：
- en: '*Use unlabeled data to build a better topic model.* The data section of the
    Kaggle competition website contains an unlabeled set of reviews that you can use
    for training. Because you’re building a supervised model, they don’t seem useful
    at first. But because you’re building a word2vec model that needs to learn the
    nuances of the world of IMDb movie reviews—and especially the connections between
    different words and concepts—it would be beneficial to use this data in order
    to improve your word2vec model that goes into the features of your training set
    (the one that has labels) before you build the model.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用未标记的数据构建更好的主题模型*。Kaggle竞赛网站的数据部分包含一组未标记的评论，您可以使用这些评论进行训练。因为您正在构建一个监督模型，它们最初看起来似乎没有用。但是，因为您正在构建一个需要学习IMDb电影评论世界细微差别（尤其是不同单词和概念之间的联系）的word2vec模型，所以使用这些数据来改进您将要进入训练集特征（带有标签的那个）的word2vec模型将是有益的。'
- en: '*Optimize parameters.* You saw great improvement in model performance in the
    initial models of this chapter after finding better values for the hyperparameters
    of the model. We since introduced a new NLP model (word2vec) and ML algorithm
    (random forest), so there are many new parameters to optimize.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化参数*。您在找到模型超参数的更好值之后，在本章的初始模型中看到了模型性能的显著提升。我们随后引入了一个新的NLP模型（word2vec）和ML算法（随机森林），因此有许多新的参数需要优化。'
- en: '*Detect phrases*. The Gensim library includes support for detecting phrases
    in text, such as “New York City,” which would be missed in our “dump” word-only
    tokenization function. The English language tends to include multiword concepts,
    so this could be an interesting thing to include in your sentence-generation function.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检测短语*。Gensim库包括在文本中检测短语的支持，例如“纽约市”，这在我们的“dump”仅单词标记化函数中可能会被遗漏。英语语言往往包含多词概念，所以这可能是您句子生成函数中一个有趣的东西。'
- en: '*Handle multiple languages.* If you were uncertain about all the reviews being
    in a single language (in this case, English), you’d have to deal with multiple
    languages in various places of the modeling pipeline. First, you’d need to know
    which language the review was in, or you’d need to detect the language (for which
    there are several libraries of varying quality available). Then you’d need to
    use this information in your tokenization process to use different stop words
    and, potentially, punctuation characters. If you were really unlucky, you’d even
    have to deal with totally different sentence structures, such as Chinese text,
    where you can’t just split the words when there’s a whitespace.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理多种语言。* 如果你对所有评论是否都使用单一语言（在这种情况下，英语）不确定，你将不得不在建模流程的各个地方处理多种语言。首先，你需要知道评论使用的是哪种语言，或者你需要检测语言（有几种质量各异的库可用）。然后，你需要在分词过程中使用这些信息，以使用不同的停用词和，可能的话，不同的标点符号。如果你真的很不幸，甚至可能不得不处理完全不同的句子结构，例如中文文本，在这种情况下，当存在空白时不能简单地分割单词。'
- en: 'Now, imagine you’re satisfied with the model at hand. If this were a real-world
    use case, you’d want to put the model into production. You should then consider
    some of the following aspects, depending on the exact use case:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你对当前的模型感到满意。如果这是一个现实世界的用例，你将希望将模型投入生产。然后，根据具体用例，你应该考虑以下一些方面：
- en: '*How much training data do you have, and does the model get better with more
    training data?* This can affect the choice of ML algorithm because you need to
    pick a model that scales well with more training data. For example, the naïve
    Bayes classifier supports partial training, also known as online learning, whereas
    the random forest algorithm can be difficult to scale to larger datasets.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你有多少训练数据，更多的训练数据是否会使模型变得更好？* 这可能会影响机器学习算法的选择，因为你需要选择一个能够随着更多训练数据而良好扩展的模型。例如，朴素贝叶斯分类器支持部分训练，也称为在线学习，而随机森林算法可能难以扩展到更大的数据集。'
- en: '*What is the volume of predictions, and do they need to be delivered in real
    time?* We’ll talk a great deal more about scaling up predictions with volume and
    speed in the next chapter, but the takeaway is that this can have consequences
    for the choice of algorithm and the infrastructure in which it’s deployed.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测量有多大，是否需要实时交付？* 我们将在下一章中详细讨论如何通过规模和速度来扩展预测，但关键是要认识到这可能会对算法的选择和部署的基础设施产生影响。'
- en: 8.4\. Summary
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 摘要
- en: 'In this chapter, you learned how to go end to end on a real machine-learning
    use case, along with the basics of natural language processing and optimizing
    model parameters. Key takeaways for this chapter included the following:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何在真实机器学习用例中从头到尾进行操作，以及自然语言处理和优化模型参数的基础知识。本章的关键要点包括以下内容：
- en: It’s essential to focus on the right problem. You should always start by asking,
    for each possible use case, “What’s the value of solving this problem?”
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于正确的问题是至关重要的。你应该始终针对每个可能的用例问自己，“解决这个问题的价值是什么？”
- en: For each use case, you need to inspect the data and systematically determine
    whether the data is sufficient to solve the problem at hand.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个用例，你需要检查数据，并系统地确定数据是否足以解决手头的问题。
- en: Start with simple off-the-shelf algorithms to build an initial model whenever
    possible. In our example, we predicted review sentiment with almost 90% accuracy.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下，始终从简单的现成算法开始，以构建初始模型。在我们的例子中，我们以几乎90%的准确率预测了评论的情感。
- en: Accuracy can be improved by testing and evaluating alternative models and combinations
    of model parameters.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过测试和评估替代模型和模型参数的组合，可以提高准确性。
- en: There are often trade-offs between different model parameters and evaluation
    criteria. We looked at how the trade-off between false positive and false negative
    rates for movie reviews is represented by the model’s ROC curve.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同的模型参数和评估标准之间，常常需要做出权衡。我们研究了电影评论中错误接受率和错误拒绝率之间的权衡是如何通过模型的ROC曲线来表示的。
- en: State-of-the-art natural-language and ML modeling techniques like word2vec are
    examples of how advanced feature engineering may enable you to improve your models.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如word2vec之类的最先进的自然语言和机器学习建模技术是高级特征工程如何帮助你改进模型的例子。
- en: Your choice of algorithms may depend on factors other than model accuracy, such
    as training time and the need to incorporate new data or perform predictions in
    near-real time.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你选择的算法可能取决于除了模型准确性之外的其他因素，例如训练时间和需要整合新数据或进行近实时预测的需求。
- en: In the real world, models can always be improved.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界中，模型总是可以改进的。
- en: 8.5\. Terms from this chapter
  id: totrans-600
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5\. 本章术语
- en: '| Word | Definition |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| Word | 定义 |'
- en: '| --- | --- |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| word2vec | An NLP modeling framework, initially released by Google and used
    in many state-of-the-art machine-learning systems involving natural language |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| word2vec | 一种自然语言处理建模框架，最初由谷歌发布，并被用于许多涉及自然语言的先进机器学习系统中 |'
- en: '| hyperparameter optimization | Various techniques for choosing parameters
    that control ML algorithms’ execution to maximize their performance |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| hyperparameter optimization | 选择控制机器学习算法执行以最大化其性能的参数的各种技术 |'
- en: Chapter 9\. Scaling machine-learning workflows
  id: totrans-605
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章\. 扩展机器学习工作流程
- en: '*This chapter covers*'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Determining when to scale up workflows for model accuracy and prediction throughput
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定何时扩展工作流程以提高模型准确性和预测吞吐量
- en: Avoiding unnecessary investments in complex scaling strategies and heavy infrastructure
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免在复杂的扩展策略和重型基础设施上进行不必要的投资
- en: Ways to scale linear ML algorithms to large amounts of training data
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线性机器学习算法扩展到大量训练数据的方法
- en: Approaches to scaling nonlinear ML algorithms—usually a much greater challenge
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展非线性机器学习算法的方法——通常是一个更大的挑战
- en: Decreasing latency and increasing throughput of predictions
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低预测延迟并提高预测吞吐量
- en: In real-world machine-learning applications, scalability is often a primary
    concern. Many ML-based systems are required to quickly crunch new data and produce
    predictions, because the predictions become useless after a few milliseconds (for
    instance, think of real-time applications such as the stock market or clickstream
    data). On the other hand, other machine-learning applications need to be able
    to scale during model training, to learn on gigabytes or terabytes of data (think
    about learning a model from an internet-scale image corpus).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习应用中，可扩展性通常是一个主要关注点。许多基于机器学习的系统需要快速处理新数据并生成预测，因为预测在几毫秒后就会变得无用（例如，考虑实时应用，如股市或点击流数据）。另一方面，其他机器学习应用需要在模型训练期间进行扩展，以便在千兆或太字节的数据上学习（考虑从互联网规模的图像语料库中学习模型）。
- en: In previous chapters, you worked mostly with data that’s small enough to fit,
    process, and model on a single machine. For many real-world problems, this may
    be sufficient to solve the problem at hand, but plenty of applications require
    scaling to multiple machines and sometimes hundreds of machines in the cloud.
    This chapter is about deciding on a scaling strategy and learning about the technologies
    involved.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你主要处理的是足够小，可以单机拟合、处理和建模的数据。对于许多现实世界的问题，这可能足以解决当前的问题，但许多应用需要扩展到多台机器，有时甚至需要扩展到云中的数百台机器。本章是关于决定扩展策略并了解涉及的技术。
- en: In the first part of this chapter, we introduce the various dimensions to consider
    when facing a large dataset or a requirement for high-volume predictions. We present
    ways that you can avoid investing a lot of time and resources in a fully scalable
    approach, and some technologies to consider if there’s no way around it. The following
    section goes more deeply into the process of scaling up the ML workflow for training
    models on large datasets. Finally, we focus on scaling the prediction workflow
    to large volumes or decreased latency.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们介绍了面对大数据集或高量预测需求时需要考虑的各种维度。我们展示了你可以避免在完全可扩展的方法上投入大量时间和资源的方法，以及如果无法避免这种情况需要考虑的一些技术。下一节将更深入地探讨扩展机器学习工作流程的过程，以便在大型数据集上训练模型。最后，我们将关注扩展预测工作流程以处理大量数据或降低延迟。
- en: In the next chapter, you’ll get to use everything you’ve learned in order to
    solve a real-world big-data example, so hang on as you get through the basics
    in this chapter.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将能够使用本章中学到的所有知识来解决一个现实世界的海量数据示例，所以请耐心等待，当你完成本章的基础知识后。
- en: 9.1\. Before scaling up
  id: totrans-616
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 扩展前的准备
- en: The type of scalability required for any given problem ultimately depends on
    the use case and the computational constraints that exist. This section starts
    by describing the kinds of scalability that are commonly required in modern machine-learning
    applications. You’ll step through the various dimensions to consider and identify
    which could be bottlenecks in your ML code. Later, after you’ve identified the
    types of scalability required, you’ll learn about standard techniques to ensure
    that your ML applications can handle real-world data rates and volumes.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何给定问题所需的可扩展性类型最终取决于用例和存在的计算约束。本节首先描述在现代机器学习应用中通常所需的可扩展性类型。你将逐步了解需要考虑的各种维度，并确定哪些可能是你ML代码的瓶颈。在你确定了所需的可扩展性类型之后，你将了解确保你的ML应用能够处理现实世界的数据速率和体积的标准技术。
- en: Instead of diving right into specific methods to scale ML applications, we start
    with a high-level overview. Using our ML workflow as a guide, let’s begin with
    a systems view of ML scalability.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不直接深入到具体的方法来扩展ML应用，而是从高层次概述开始。以我们的ML工作流程为指导，让我们从ML可扩展性的系统视角开始。
- en: 9.1.1\. Identifying important dimensions
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. 识别重要维度
- en: 'Let’s first deconstruct our machine-learning workflow into the two primary
    routines: model training and model prediction. For these two systems, how could
    resource constraints affect the workflow, and how could these inhibit or break
    the system? Consider [table 9.1](#ch09table01).'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将我们的机器学习工作流程分解为两个主要流程：模型训练和模型预测。对于这两个系统，资源约束如何影响工作流程，以及它们如何抑制或破坏系统？考虑[表9.1](#ch09table01)。
- en: Table 9.1\. Problems in model building that can occur due to lack of scalability,
    plus their ultimate consequences
  id: totrans-621
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表9.1\. 由于缺乏可扩展性可能发生的模型构建问题，以及它们的最终后果
- en: '| Scalability problem | Consequence |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性问题 | 后果 |'
- en: '| --- | --- |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Training dataset is too large to fit a model. | No model is fitted, so no
    predictions can be made. |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据集太大，无法拟合模型。 | 没有模型被拟合，因此无法进行预测。 |'
- en: '| Training dataset is so large that model fitting is slow. | Model optimization
    is infeasible (or impractical), so a suboptimal model is used, sacrificing accuracy.
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据集太大，模型拟合速度慢。 | 模型优化不可行（或实际不可行），因此使用次优模型，牺牲了准确性。 |'
- en: 'During model building, the scalability issues that you’ll face stem from large
    training sets. At one extreme, if your training dataset is so large that you can’t
    even fit a model (for example, the data doesn’t fit in memory), then this is a
    problem that you *must* find a way around. You can choose from three approaches:
    (1) find a smaller subset of the data that you can learn on, (2) use a machine
    with more RAM, or (3) use a more memory-efficient learning algorithm.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建过程中，你将面临的可扩展性问题源于大规模的训练集。在一种极端情况下，如果你的训练数据集如此之大以至于你甚至无法拟合一个模型（例如，数据无法适应内存），那么这是一个你必须找到解决方案的问题。你可以选择以下三种方法：（1）找到可以学习的小数据子集，（2）使用具有更多RAM的机器，或者（3）使用更内存高效的算法。
- en: In a bit, we describe a few quick ways to reduce your dataset size without significantly
    impacting model quality. We follow this up with a discussion of how to scale compute
    cycles to fit your problem via scalable data systems. Later in the chapter, we
    introduce scalable learning algorithms, which can allow you to scale ML to your
    data without relying on shortcuts or extra hardware.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 在稍后，我们将描述几种快速减少数据集大小的方法，而不会显著影响模型质量。接着，我们将讨论如何通过可扩展的数据系统来扩展计算周期以适应你的问题。在本章的后面部分，我们将介绍可扩展的学习算法，这些算法可以让你在不依赖捷径或额外硬件的情况下将ML扩展到你的数据。
- en: For slightly smaller datasets, it may be possible to fit only relatively simple
    models (such as linear/logistic regression) in lieu of more-sophisticated ones
    (such as boosting), because of the extra computational complexity and memory footprint
    of the latter. In this case, you may be sacrificing accuracy by not fitting more-sophisticated
    learning algorithms, but at least you’re able to fit a model. In this case, the
    same options presented previously are viable approaches to try.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稍微小一些的数据集，可能只能拟合相对简单的模型（如线性/逻辑回归），而不是更复杂的模型（如提升），因为后者有额外的计算复杂性和内存占用。在这种情况下，你可能通过不拟合更复杂的学习算法而牺牲了准确性，但至少你能够拟合一个模型。在这种情况下，之前提出的相同选项是可行的尝试方法。
- en: In a related scenario, the massive size of your training dataset could cause
    model fitting, and in turn model optimization, to be slow. Like the previous scenario,
    this can cause you to use a less accurate model, because you’re forced to employ
    a coarse tuning-parameter-optimization strategy or to forego tuning altogether.
    But unlike the preceding situation, this predicament can be solved by spinning
    up more nodes (horizontal scaling) and fitting models (with different tuning parameter
    choices) on the different machines. We touch more on horizontal scaling in [section
    9.1.3](#ch09lev2sec3).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关场景中，你的训练数据集的巨大规模可能导致模型拟合，进而导致模型优化变慢。与前面的场景一样，这可能导致你使用一个不太准确的模型，因为你被迫采用粗略的调整参数优化策略，或者完全放弃调整。但与前面情况不同，这种困境可以通过启动更多节点（横向扩展）并在不同机器上（使用不同的调整参数选择）拟合模型来解决。我们将在[9.1.3节](#ch09lev2sec3)中更多地讨论横向扩展。
- en: In the prediction workflow, the scalability issues you face stem from data that
    comes in very fast, prediction or feature-engineering processes that are CPU-intensive,
    or prediction data batches that are very large. Consider [table 9.2](#ch09table02).
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测工作流程中，你面临的扩展性问题源于非常快速到来的数据、CPU密集型的预测或特征工程过程，或者非常大的预测数据批量。考虑[表9.2](#ch09table02)。
- en: Table 9.2\. Problems in ML prediction that can occur due to lack of scalability,
    plus their ultimate consequences
  id: totrans-631
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表9.2\. 由于缺乏可扩展性可能导致机器学习预测中出现的问题，以及它们的最终后果
- en: '| Scalability problem | Consequence |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| 扩展性问题 | 后果 |'
- en: '| --- | --- |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Data rates (streaming) are too fast for the ML system to keep up. | The backlog
    of data to predict on grows and grows until ultimately breaking. |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| 数据速率（流式）对于机器学习系统来说太快了。 | 预测所需的数据积压不断增长，最终导致崩溃。 |'
- en: '| Feature-engineering code and/or prediction processes are too slow to generate
    timely predictions. | The potential value of the predictions is lost, particularly
    in real-time use cases. |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程代码和/或预测过程太慢，无法生成及时的预测。 | 预测的潜在价值丢失，尤其是在实时用例中。 |'
- en: '| Data sizes (batch) are too large to process with the model. | The prediction
    system breaks, and no predictions are made. |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| 数据大小（批量）太大，无法用模型处理。 | 预测系统崩溃，无法进行预测。 |'
- en: 'Luckily, all three of these challenges can be resolved with the same strategy:
    spinning up more machines. The advantage of prediction, as opposed to model training,
    is that in the vast majority of use cases, predictions can be made independently
    for each data instance.^([[1](#ch09fn01)]) To generate predictions, at any one
    time you need to hold in memory only the features for a single instance (and the
    ML model that you’ve built). Contrast that scenario to model training: typically,
    the entire training set needs to be loaded into memory. Thus, unlike the scalability
    problems during model training, prediction scalability issues don’t require larger
    machines; they just require more of them—and, of course, an efficient data management
    system to control them (more on this later).'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这三个挑战都可以通过相同的策略来解决：增加更多的机器。与模型训练相比，预测的优势在于在绝大多数用例中，预测可以独立地对每个数据实例进行。为了生成预测，在任何时候你只需要在内存中保留单个实例的特征（以及你构建的机器学习模型）。将这种情况与模型训练进行对比：通常，整个训练集需要被加载到内存中。因此，与模型训练期间的扩展性问题不同，预测的扩展性问题不需要更大的机器；只需要更多的机器——当然，还需要一个高效的数据管理系统来控制它们（关于这一点稍后还会详细介绍）。
- en: ¹
  id: totrans-638
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-639
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that in a handful of ML use cases, predictions can’t be made on separate
    instances independently. For example, a time-series forecasting model, such as
    a financial or climate model, may rely on the predictions from multiple timestamps
    in generating a single forecast.
  id: totrans-640
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，在少数机器学习用例中，预测不能独立地对单独的实例进行。例如，一个时间序列预测模型，如金融或气候模型，可能依赖于多个时间戳的预测来生成单个预测。
- en: Whether you need to generate predictions more quickly, handle a higher volume
    of instances, or deal with slow feature-engineering or prediction processes, the
    solution is to spin up more machines and send out different subsets of instances
    on each node for processing. Then, assuming that the fitted model is distributed
    on all the nodes, you can generate predictions in parallel across all machines
    and return them to a central database.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你需要更快地生成预测、处理更高容量的实例，还是处理缓慢的特征工程或预测过程，解决方案都是启动更多机器，并将不同子集的实例发送到每个节点进行处理。然后，假设拟合的模型分布在所有节点上，你可以在所有机器上并行生成预测，并将它们返回到中央数据库。
- en: In [section 9.3](#ch09lev1sec3), you’ll dive deeply into prediction systems.
    There, you’ll explore a few approaches to building computational systems for fast
    and scalable ML prediction.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9.3节](#ch09lev1sec3)中，你将深入探讨预测系统。在那里，你将探索一些构建快速且可扩展的机器学习预测计算系统的方法。
- en: 9.1.2\. Subsampling training data in lieu of scaling?
  id: totrans-643
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2\. 用子采样代替扩展？
- en: In some cases, model training may be infeasible with the entire training set
    and the available CPU resources. If you’re up against this challenge and no other
    option is viable, then *as a method of last resort*, you may consider subsampling
    the training data before model building.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，使用整个训练集和可用的CPU资源进行模型训练可能不可行。如果你面临这个挑战，且没有其他可行的选择，那么作为最后的手段，你可以在模型构建之前考虑对训练数据进行子采样。
- en: 'Although in general we discourage subsampling data (you might lose important
    signals), some ways of discarding data are better than others. Some might even
    improve your model, depending on the ML algorithm at hand. You can throw away
    data in two ways: discard features or discard instances. For each option, we’ll
    describe a statistically rigorous method to reduce the size of your training data.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们通常不鼓励对数据进行子采样（你可能会丢失重要的信号），但某些丢弃数据的方法比其他方法更好。在某些情况下，这些方法甚至可能改善你的模型，这取决于手头的机器学习算法。你可以通过两种方式丢弃数据：丢弃特征或丢弃实例。对于每种选项，我们将描述一种统计上严格的方法来减少你的训练数据量。
- en: Feature selection
  id: totrans-646
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征选择
- en: Often, the broadness of a dataset creates the computational bottleneck. For
    example, in genome data, a training set may contain data for millions of genes
    (features) but for only hundreds of patients (instances). Likewise, for text analysis,
    the featurization of data into n-grams can result in training sets containing
    upward of millions of features. In these cases, you can make your model training
    scale by first eliminating unimportant features in a process called *feature selection*.
    [Figure 9.1](#ch09fig01) shows a schematic of how feature selection works.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据集的广泛性会形成计算瓶颈。例如，在基因组数据中，一个训练集可能包含数百万个基因（特征）的数据，但只有数百个患者（实例）。同样，对于文本分析，将数据特征化为n-gram可能会导致包含数百万个特征的训练集。在这些情况下，你可以通过首先在称为*特征选择*的过程中消除不重要的特征来使你的模型训练可扩展。[图9.1](#ch09fig01)展示了特征选择是如何工作的示意图。
- en: Figure 9.1\. Feature selection using Lasso to reduce the dimensionality of a
    large dataset to train a machine-learning model
  id: totrans-648
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1\. 使用Lasso降低大型数据集的维度以训练机器学习模型
- en: '![](09fig01.jpg)'
  id: totrans-649
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig01.jpg)'
- en: As we discussed in [chapters 4](kindle_split_014.html#ch04) and [5](kindle_split_015.html#ch05),
    feature selection can lead to better models in some cases. By intelligently removing
    features, you can make the learning algorithms hone in on the important signals
    without becoming distracted by the features that don’t matter. The actual loss
    or gain of feature selection depends on the choice of ML model and on how much
    information is unknowingly lost because you’re throwing away data, so you should
    always test your changes by validating your model appropriately. In this section,
    we talk about feature selection primarily as a way of working with large datasets.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](kindle_split_014.html#ch04)和[第5章](kindle_split_015.html#ch05)中讨论的那样，特征选择在某些情况下可以导致更好的模型。通过智能地去除特征，你可以让学习算法专注于重要的信号，而不会被那些无关的特征所分散。特征选择的实际损失或收益取决于所选的机器学习模型以及由于你丢弃数据而无意中丢失的信息量，因此你应该始终通过适当验证你的模型来测试你的更改。在本节中，我们主要讨论特征选择作为处理大型数据集的一种方式。
- en: For massive training sets, our recommended method of feature selection is *Lasso*.
    Lasso is an efficient linear learning algorithm that automatically searches for
    the most predictive subset of features. Computing the entire trace of the algorithm
    is efficient, allowing the user insight into the entire ordering of all the features
    in terms of their predictive power in the linear model. Moreover, the best subset
    of features, in terms of the linear model predictions, is provided.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模的训练集，我们推荐的特征选择方法是*Lasso*。Lasso是一种高效的线性学习算法，它自动搜索最具预测力的特征子集。计算整个算法的迹是高效的，使用户能够洞察所有特征在线性模型中预测力方面的整个排序。此外，根据线性模型预测，还提供了最佳特征子集。
- en: If you’re (un)lucky enough to have such a large dataset that you can’t even
    fit a Lasso model, you may consider fitting the Lasso to subsets of the instances
    in your training set (and potentially averaging across runs of the algorithm).
    This can give you a good sense of which features can be removed from the model
    without degrading the statistical performance of your ML algorithm.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你非常幸运（或不幸运）拥有如此大的数据集，以至于你甚至无法拟合Lasso模型，你可以考虑将Lasso拟合到训练集中实例的子集（并且可能跨算法运行的平均值）。这可以给你一个很好的感觉，了解哪些特征可以从模型中移除，而不会降低你的机器学习算法的统计性能。
- en: The obvious downside to Lasso feature selection is that it uses a linear model
    to gauge the importance of each feature. A feature that’s selected out via Lasso
    could indeed have a nonlinear relationship with the target variable that may not
    be appropriately captured by Lasso. As an alternative, nonparametric approaches
    to feature selection exist, such as random forest feature importance, but those
    methods typically don’t scale to large datasets.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso特征选择的明显缺点是它使用线性模型来衡量每个特征的重要性。通过Lasso选出的特征确实可能与目标变量有非线性关系，这可能无法被Lasso适当地捕捉。作为替代，存在非参数特征选择方法，如随机森林特征重要性，但这些方法通常无法扩展到大型数据集。
- en: Instance clustering
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实例聚类
- en: If after feature selection your training data is *still* too large to fit a
    model on, you may consider subselecting instances. As an absolute method of last
    resort, you can use statistical clustering algorithms to identify and remove redundancies
    in your training instances.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在特征选择后，你的训练数据仍然太大，无法拟合模型，你可以考虑子选择实例。作为最后的绝对方法，你可以使用统计聚类算法来识别并删除训练实例中的冗余。
- en: For this type of data reduction, we recommend using an agglomerative hierarchical
    clustering algorithm. This approach will initialize with each training set instance
    as the sole member of its own cluster. Then, the two closest clusters are subsequently
    joined (using a predefined distance measure to determine “closeness”). This joining
    of nearby clusters continues until a stopping criterion (for example, number of
    clusters) is reached. We recommend stopping this process as early as possible
    so you don’t reduce too dramatically the information content of your data. The
    final reduced training set consists of a single instance for each of the resulting
    clusters.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种数据缩减，我们建议使用聚集层次聚类算法。这种方法将以每个训练集实例作为其自己聚类的唯一成员开始。然后，将最近的两个聚类随后合并（使用预定义的距离度量来确定“接近度”）。这种邻近聚类的合并会一直持续到达到停止标准（例如，聚类数量）为止。我们建议尽可能早地停止这个过程，以免过于大幅度地减少数据的信息内容。最终的缩减训练集由每个结果聚类的单个实例组成。
- en: 9.1.3\. Scalable data management systems
  id: totrans-657
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.3. 可扩展的数据管理系统
- en: Independent of the strategy you want to take for scaling up your ML workflow,
    you need to be able to handle the data first. In the past decade, we’ve seen tremendous
    focus on so-called *big-data* technologies. In this book, we use the term *big
    data* to mean any data that’s too large to be processed by a single machine in
    a reasonable amount of time. Here, we introduce some of the most successful big-data
    projects and how they can be used in an ML framework.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你打算采取何种策略来扩展你的机器学习工作流程，你首先需要能够处理数据。在过去十年中，我们对所谓的*大数据*技术给予了极大的关注。在这本书中，我们使用“大数据”一词来指代任何在合理时间内无法由单个机器处理的数据。在这里，我们介绍了一些最成功的的大数据项目以及它们如何在机器学习框架中使用。
- en: The basic principle in modern big-data systems is that you need to be able to
    handle more data by adding more machines. This is known as *horizontal* scalability.
    In contrast, the alternative way of handling larger resource requirements is *vertical*
    scaling, whereby you upgrade the small number of machines you have with more disk,
    memory, or CPU cores. [Figure 9.2](#ch09fig02) compares horizontal and vertical
    scalability.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 现代大数据系统中的基本原则是，你需要通过增加更多机器来处理更多的数据。这被称为*水平扩展性*。相比之下，处理更大资源需求的其他方法是*垂直扩展性*，即通过增加磁盘、内存或CPU核心来升级你拥有的少量机器。[图9.2](#ch09fig02)比较了水平和垂直扩展性。
- en: Figure 9.2\. Horizontal vs. vertical scalability for big-data systems. In horizontal
    systems, you add new nodes (machines) to your infrastructure to handle more data
    or computation, as the load is distributed relatively evenly among nodes. An example
    of such a system is Apache Hadoop. In vertically scaling systems, you add more
    resources to your existing machines in order to handle higher loads. This approach
    is usually more efficient initially, but there’s a limit to the amount of resources
    you can add. Examples of databases that work well with this approach are SQL servers
    such as PostgreSQL.
  id: totrans-660
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2\. 大数据系统的水平与垂直可扩展性。在水平系统中，你向你的基础设施添加新的节点（机器）来处理更多的数据或计算，因为负载在节点之间相对均匀地分布。Apache
    Hadoop就是这样一个系统的例子。在垂直扩展系统中，你向现有的机器添加更多资源以处理更高的负载。这种方法最初通常更有效，但你可以添加的资源量是有限的。与这种方法很好地工作的数据库示例是PostgreSQL这样的SQL服务器。
- en: '![](09fig02_alt.jpg)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig02_alt.jpg)'
- en: Sometimes, and perhaps more often than you might think, upgrading your machines
    will be enough to scale up your machine-learning workflow. As stated in the previous
    sections, after the raw data has been processed and readied for your classification
    or regression problem, the data may not be big enough to warrant the complexity
    of a true big-data system. But in some cases, when dealing with data from popular
    websites, mobile apps, games, or a large number of physical sensors, it’s necessary
    to use a horizontally scalable system. From now on, this is what we’ll assume.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，也许比你想象的更频繁，升级你的机器就足以扩展你的机器学习工作流程。正如前几节所述，在原始数据经过处理并准备好用于分类或回归问题之后，数据可能不足以证明真正大数据系统的复杂性。但在某些情况下，当处理来自流行网站、移动应用、游戏或大量物理传感器的数据时，使用水平可扩展的系统是必要的。从现在开始，我们将假设这一点。
- en: 'Horizontally scalable big-data systems have two main layers: storage and computation.
    In the *storage layer*, data is stored and passed on to the *computational layer*,
    where data is processed. One of the most popular big-data software projects is
    Apache Hadoop, which is still widely used in science and industry and is based
    on ideas from a previously unseen level of scalability obtained at Google and
    other web-scale companies in the early 2000s.'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 水平可扩展的大数据系统有两个主要层：存储和计算。在*存储层*中，数据被存储并传递到*计算层*，在那里数据被处理。最受欢迎的大数据软件项目之一是Apache
    Hadoop，它仍然在科学和工业中得到广泛应用，并基于2000年代初谷歌和其他Web规模公司在获得前所未有的可扩展性水平时的想法。
- en: The storage layer in Hadoop is called the *Hadoop Distributed File System* (HDFS).
    Datasets are partitioned and distributed over multiple machines so they can be
    processed in parallel. Also, each partition is replicated so data is unlikely
    to be lost in the event of hardware or software failures.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中的存储层被称为*Hadoop分布式文件系统*（HDFS）。数据集被分区并分布在多台机器上，以便可以并行处理。此外，每个分区都被复制，因此数据在硬件或软件故障的情况下不太可能丢失。
- en: The computing layer of Hadoop uses a simple algorithm called *MapReduce* to
    distribute computation among the nodes in the cluster. In the MapReduce framework,
    the map step distributes data from HDFS onto workers that transform the data in
    some way, usually keeping the number of data rows the same. This is similar to
    our feature-engineering processes in earlier chapters, where you add new columns
    to each row of input data. In the reduce step, the mapped data is filtered and
    aggregated into its final form. Many data-processing algorithms can be transformed
    into MapReduce jobs. When algorithms are transformed to this framework, systems
    such as Hadoop will take care of the distribution of work among any number of
    machines in your cluster.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的计算层使用一种简单的算法，称为*MapReduce*，在集群中的节点之间分配计算。在MapReduce框架中，map步骤将数据从HDFS分布到工人节点上，这些节点以某种方式转换数据，通常保持数据行数不变。这与我们之前章节中的特征工程过程类似，你在输入数据的每一行中添加新的列。在reduce步骤中，映射后的数据被过滤并聚合成最终形式。许多数据处理算法可以转换为MapReduce作业。当算法转换为这个框架时，系统如Hadoop将负责在集群中的任何数量的机器之间分配工作。
- en: 'In principle, the storage and computational layers need not be integrated.
    Many organizations use a storage system from a cloud provider, such as the S3
    service in the Amazon Web Services (AWS) cloud infrastructure, coupled with the
    Hadoop MapReduce framework for computation. This has the benefit that AWS manages
    your large volumes of data, but you lose one of the main points of the tight integration
    between HDFS and MapReduce: *data locality*. With data locality, your system becomes
    more efficient because computational tasks are performed on subsets of the data
    close to where that data is stored.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，存储和计算层不需要集成。许多组织使用来自云服务提供商的存储系统，例如亚马逊网络服务 (AWS) 云基础设施中的 S3 服务，与 Hadoop
    MapReduce 框架结合用于计算。这有一个好处，即 AWS 管理你的大量数据，但你失去了 HDFS 和 MapReduce 之间紧密集成的主要优点：*数据局部性*。有了数据局部性，你的系统变得更加高效，因为计算任务是在数据存储附近的数据子集上执行的。
- en: The Hadoop community has developed a machine-learning library called *Mahout*
    that implements a range of popular ML algorithms that work with HDFS and MapReduce
    in the Hadoop framework. If your data is in Hadoop, Mahout may be worth looking
    into for your machine-learning needs. Mahout is moving away from the simplistic
    MapReduce framework into more-advanced distributed computing approaches based
    on Apache Spark. *Apache Spark*, a more recent and widely popular framework based
    on the ideas of Hadoop, strives to achieve better performance by working on data
    in memory. Spark has its own library of machine-learning algorithms in the *MLlib*
    library included with the framework. [Figure 9.3](#ch09fig03) shows a simple diagram
    of the Apache Spark ecosystem.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 社区开发了一个名为 *Mahout* 的机器学习库，该库实现了多种流行的 ML 算法，这些算法可以在 Hadoop 框架中的 HDFS 和
    MapReduce 上运行。如果你的数据存储在 Hadoop 中，那么 Mahout 可能值得你考虑用于你的机器学习需求。Mahout 正在从简单的 MapReduce
    框架转向基于 Apache Spark 的更高级的分布式计算方法。*Apache Spark* 是一个基于 Hadoop 思想的更近和更受欢迎的框架，它通过在内存中处理数据来努力实现更好的性能。Spark
    在框架中包含了名为 *MLlib* 的机器学习算法库。[图 9.3](#ch09fig03) 展示了 Apache Spark 生态系统的简单图示。
- en: Figure 9.3\. The Apache Spark ecosystem based on the Spark core for distributed
    computation. Spark SQL allows you to work with tables using Python pandas or R
    data frames. Spark Streaming allows you to process data in real time as it arrives,
    in contrast to the batch-processing nature of Hadoop and classic Spark. MLlib
    is the machine-learning library that includes a range of ML algorithms optimized
    for the Spark engine, and GraphX is a library allowing efficient computation on
    large graphs such as the social graph of a social network.
  id: totrans-668
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.3\. 基于 Spark 核心的 Apache Spark 生态系统，用于分布式计算。Spark SQL 允许你使用 Python pandas
    或 R 数据框来处理表格。Spark Streaming 允许你在数据到达时实时处理数据，与 Hadoop 和经典 Spark 的批处理特性形成对比。MLlib
    是一个机器学习库，它包含了一系列针对 Spark 引擎优化的 ML 算法，而 GraphX 是一个允许在大型图（如社交网络的社会图）上进行高效计算的库。
- en: '![](09fig03_alt.jpg)'
  id: totrans-669
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig03_alt.jpg)'
- en: Scalable ML algorithms are often linear for natural reasons. Both Mahout and
    MLlib include mostly linear ML algorithms or approximations to nonlinear algorithms.
    In the next section, you’ll look at how to approach scaling with both types of
    algorithms.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展的 ML 算法通常由于自然原因而通常是线性的。Mahout 和 MLlib 主要包括线性的 ML 算法或非线性算法的近似。在下一节中，你将了解如何使用这两种类型的算法进行扩展。
- en: 9.2\. Scaling ML modeling pipelines
  id: totrans-671
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 扩展 ML 模型管道
- en: In the first section of this chapter, you looked at things that are good to
    know before you take the plunge and invest in scaling up your workflow to handle
    larger datasets. In this section, we assume that you’ve made the decision to scale
    out your ML workflow and chosen a big-data processing system to use. [Figure 9.4](#ch09fig04)
    updates our familiar ML workflow diagram to the world of big data.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一节中，你了解了在投入大量资金扩大你的工作流程以处理更大的数据集之前应该知道的事情。在本节中，我们假设你已经做出了扩大你的 ML 工作流程的决定，并选择了一个大数据处理系统来使用。[图
    9.4](#ch09fig04) 更新了我们熟悉的 ML 工作流程图，以适应大数据的世界。
- en: Figure 9.4\. The modeling part of our familiar ML workflow diagram with scalable
    components
  id: totrans-673
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.4\. 我们熟悉的 ML 工作流程图中建模部分的扩展组件
- en: '![](09fig04.jpg)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig04.jpg)'
- en: In [section 9.1.3](#ch09lev2sec3), we introduced a few big-data-capable systems
    that can be used to manage and process data of almost any size. Because they work
    on an instance-by-instance basis, the feature-engineering processes that we’ve
    talked about in the book so far can be done with simple map calls that are available
    in any of those systems. Next, you’ll look at how some popular linear and nonlinear
    ML algorithms scale in the face of big data.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9.1.3节](#ch09lev2sec3)中，我们介绍了一些能够用于管理和处理几乎任何大小数据的大数据系统。由于它们基于实例进行工作，因此我们在这本书中讨论过的特征工程过程可以通过任何这些系统中的简单map调用来完成。接下来，您将了解一些流行的线性和非线性机器学习算法如何在大数据面前进行扩展。
- en: 9.2.1\. Scaling learning algorithms
  id: totrans-676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. 扩展学习算法
- en: In the beginning of the chapter, you saw that during the learning phase, the
    fundamental scalability challenge is dealing with the size, in memory, of very
    large training sets. To circumvent that problem, one option is to look for implementations
    of ML algorithms that either (a) use a smaller memory footprint than competing
    implementations of the same algorithm, or (b) can train over distributed systems
    in which each node requires only a subset of the entire dataset.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，您看到在学习阶段，基本的可扩展性挑战是处理非常大的训练集在内存中的大小。为了规避这个问题，一个选择是寻找机器学习算法的实现，这些实现要么（a）比相同算法的竞争实现占用更小的内存足迹，要么（b）可以在每个节点只需要整个数据集子集的分布式系统中进行训练。
- en: Out in the wild, countless implementations of the most common ML learning algorithms
    exist. From scikit-learn to mlpack, these implementations are continually stretching
    the frontiers of memory efficiency (and thus increasing the dataset size that
    can be trained on a single computer with a fixed amount of RAM). Yet, data volumes
    are still outpacing the gains in ML software and computer hardware. For some training
    sets, the only option is horizontally scalable machine learning.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 在野外，存在无数最常见机器学习学习算法的实现。从scikit-learn到mlpack，这些实现不断扩展内存效率的边界（从而增加了可以在固定数量的RAM上训练的数据集大小）。然而，数据量仍在超过机器学习软件和计算机硬件的进步。对于某些训练集，唯一的选项是水平可扩展的机器学习。
- en: The most commonly used distributed learning algorithm is linear (and logistic)
    regression. The *Vowpal Wabbit* (VW) library popularized this approach, and has
    been a mainstay for scalable linear learning across multiple machines. The basic
    way that distributed linear regression works is to first send subsets of the training
    data (subset by dataset rows) to the various machines in the cluster. Then, in
    an iterative manner, each machine performs an optimization problem on the subset
    of data on hand, sending back the result of the optimization to the central node.
    There, that information is combined to come up with the best overall solution.
    After a small number of iterations of this procedure, the final model is guaranteed
    to be close to the overall optimal model (if a single model were fit to all the
    data at once). Hence, linear models can be fit in a distributed way to terabytes
    or more of data!
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的分布式学习算法是线性（和逻辑）回归。*Vowpal Wabbit*（VW）库推广了这种方法，并且已经成为跨多台机器可扩展线性学习的主要支柱。分布式线性回归的基本工作方式是首先将训练数据集的子集（按数据集行进行子集划分）发送到集群中的各个机器。然后，以迭代的方式，每台机器对其手头的子集数据进行优化问题求解，并将优化结果发送回中央节点。在那里，这些信息被组合起来，以得出最佳的整体解决方案。经过几次迭代后，最终模型将保证接近整体最优模型（如果一次性将所有数据拟合到一个模型中）。因此，线性模型可以以分布式方式拟合到数太字节或更多的数据！
- en: As we’ve discussed numerous times in this book, linear algorithms aren’t necessarily
    adequate for modeling the nuances of data for accuracy predictions. In these cases,
    it can be helpful to turn to nonlinear models. Nonlinear models usually require
    more computational resources, and horizontal scalability isn’t always possible
    with nonlinear models. This can be understood loosely by thinking of nonlinear
    models as also considering complex interactions between features, thus requiring
    a larger portion of the dataset at any given node.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中多次讨论的那样，线性算法并不一定足以模拟数据的细微差别以进行准确预测。在这些情况下，转向非线性模型可能会有所帮助。非线性模型通常需要更多的计算资源，并且非线性模型不一定总是可以实现水平可扩展性。这可以通过将非线性模型视为也考虑特征之间的复杂交互来粗略理解，因此需要任何给定节点上更大比例的数据集。
- en: In many cases, it’s more feasible to upgrade your hardware or find more-efficient
    algorithms or more-efficient implementations of the algorithms you’ve chosen.
    But in other situations, scaling a nonlinear model is needed, and in this section
    we discuss a few ways to approach this.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，升级硬件或找到更高效的算法或更高效的算法实现可能更可行。但在其他情况下，需要扩展非线性模型，本节我们将讨论几种处理此问题的方法。
- en: Polynomial features
  id: totrans-682
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多项式特征
- en: 'One of the most widely used tricks to model nonlinear feature interactions
    is to create new features that are combinations of the existing features and then
    train a linear model including the nonlinear features. A common way to combine
    features is to multiply features in various combinations, such as *feature 1 times
    feature 2*, *feature 2 squared*, or *feature 1 times feature 2 times feature 5*.
    Say a dataset consists of two features, f1 = 4 and f2 = 15\. In addition to using
    f1 and f2 in your model, you can generate new features f1 × f2 = 60, f1 ^ ² =
    16 and f2 ^ ² = 225\. Datasets usually contain a lot more than two features, so
    this technique can generate a huge number of new features. These features are
    nonlinear combinations of existing features. We call them *polynomial features*.
    The following listing shows how this can be achieved with the scikit-learn Python
    library. The results of running the code in this listing show the accuracy gained
    when adding polynomial features to a standard Iris flower classification model:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟非线性特征交互的最常用技巧之一是创建新的特征，这些特征是现有特征的组合，然后训练一个包含非线性特征的线性模型。组合特征的一种常见方式是将特征以各种组合相乘，例如
    *特征 1 乘以特征 2*，*特征 2 的平方*，或 *特征 1 乘以特征 2 乘以特征 5*。假设一个数据集包含两个特征，f1 = 4 和 f2 = 15。除了在模型中使用
    f1 和 f2 之外，你还可以生成新的特征 f1 × f2 = 60，f1 ^ ² = 16 和 f2 ^ ² = 225。数据集通常包含远不止两个特征，因此这种技术可以生成大量新的特征。这些特征是现有特征的非线性组合。我们称它们为
    *多项式特征*。以下列表展示了如何使用 scikit-learn Python 库实现这一点。此列表中代码的运行结果显示了向标准 Iris 花分类模型添加多项式特征时获得的准确度：
- en: '[PRE12]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Listing 9.1\. Making a linear model nonlinear by using polynomial features
  id: totrans-685
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. 通过使用多项式特征将线性模型非线性化
- en: '![](205fig01_alt.jpg)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](205fig01_alt.jpg)'
- en: An example of another machine-learning toolkit that has polynomial feature extraction
    integrated is the Vowpal Wabbit library. VW can be used to build models on large
    datasets on single machines because all computation is done iteratively and *out
    of core*, meaning that only the data used in the particular iteration needs to
    be kept in memory. VW uses stochastic gradient descent and feature hashing to
    deal with unstructured and sparse data in a scalable fashion. VW can generate
    nonlinear models by supplying the `–q` and `–cubic` flags to generate quadratic
    or cubic features, corresponding to polynomial features where all pairs or all
    triplets of features have been multiplied together.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个集成了多项式特征提取的机器学习工具包示例是 Vowpal Wabbit 库。VW 可以用于在单机上构建大型数据集上的模型，因为所有计算都是迭代性地并且**离线**完成的，这意味着只需要保留特定迭代中使用的数据。VW
    使用随机梯度下降和特征哈希来以可扩展的方式处理非结构化和稀疏数据。VW 可以通过提供 `–q` 和 `–cubic` 标志来生成二次或三次特征，对应于所有特征对或所有特征三联乘积的多项式特征。
- en: Data and algorithm approximations
  id: totrans-688
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据和算法近似
- en: As you saw in the preceding section, the polynomial feature approach has the
    ability to increase the accuracy of the model significantly, but also increases
    the number of features polynomially. That might not be feasible for a large number
    of input features, so here you’ll look at a few nonlinear algorithms that have
    well-known approximations useful for scalable implementations. Other algorithms
    may have their own approximations for scalability, so we encourage you to investigate
    your favorite algorithm further.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一节中看到的，多项式特征方法能够显著提高模型的准确度，但也以多项式的方式增加了特征数量。这可能对于大量输入特征来说并不可行，因此在这里，你将了解一些具有已知近似值且适用于可扩展实现的非线性算法。其他算法可能有它们自己的可扩展近似，所以我们鼓励你进一步研究你喜欢的算法。
- en: A widely used nonlinear learning algorithm is random forest, which you’ve already
    read about in previous chapters. The random forest model consists of numerous
    decision trees, and on first sight it may look trivial to scale random forest
    to many machines by building only a subset of the trees on each node. Be aware
    that if the data subsamples available at each node aren’t sufficiently similar,
    the accuracy of the model can suffer. But building more trees or splitting the
    data more intelligently could mitigate the loss in accuracy.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广泛使用的非线性学习算法是随机森林，你已经在之前的章节中了解过。随机森林模型由许多决策树组成，乍一看，通过在每个节点上仅构建树的一个子集，似乎可以轻松地将随机森林扩展到多台机器。但请注意，如果每个节点可用的数据子样本不充分相似，模型的准确性可能会受到影响。但构建更多树或更智能地分割数据可以减轻准确性损失。
- en: 'Another approximation that can be used to scale random forests and other algorithms
    is a *histogram approximation*: each column in the dataset is replaced with the
    histogram of that column, which usually decreases the number of values in the
    column significantly. If the number of bins in the histogram is too small, a lot
    of nuance may be lost and model performance suffers.'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于扩展随机森林和其他算法的另一个近似方法是*直方图近似*：数据集中的每一列都被替换为该列的直方图，这通常会显著减少列中的值数。如果直方图中的箱数太少，可能会丢失很多细微差别，从而导致模型性能下降。
- en: Another algorithm that has natural approximations is k-nearest neighbors; special
    approximate tree structures can be used to increase the scalability of the model.
    Support vector machines have seen multiple approximation methods to make the nonlinear
    versions more scalable, including Budgeted Stochastic Gradient Descent (BSGD)
    and Adaptive Multi-hyperplane Machines (AMM).
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个具有自然近似的方法的算法是k-最近邻；可以使用特殊的近似树结构来提高模型的可扩展性。支持向量机已经看到了多种近似方法，以使非线性版本更具可扩展性，包括预算随机梯度下降（BSGD）和自适应多超平面机器（AMM）。
- en: Deep neural nets
  id: totrans-693
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: A recent revolution in neural network research has spawned a new field of deep
    learning that produces highly nonlinear models and has proven to be scalable to
    very large datasets. In the early days of machine learning, neural networks (NNs)
    were researched heavily and applied widely in science and industry. Later, with
    the advent of algorithms that were easier to reason about mathematically, NNs
    were used less frequently. Recently, NNs again started producing state-of-the-art
    results on large and diverse datasets after going through a few important evolutionary
    steps and entering the realm of deep learning.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络研究最近的一次革命催生了一个新的领域——深度学习，该领域产生了高度非线性的模型，并且已被证明能够扩展到非常大的数据集。在机器学习的早期，神经网络（NNs）被广泛研究并应用于科学和工业领域。后来，随着易于数学推理的算法的出现，神经网络的使用频率降低。最近，经过几个重要的进化步骤并进入深度学习领域后，神经网络再次在大型和多样化的数据集上产生了最先进的结果。
- en: '*Deep learning* refers to a family of algorithms that extends the traditional
    neural network. Commonly, these models include many hidden layers in the neural
    network or many single-layer networks combined. [Figure 9.5](#ch09fig05) illustrates
    an example neural network.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*指的是一系列扩展传统神经网络的算法。通常，这些模型包括神经网络中的许多隐藏层或许多单层网络的组合。[图9.5](#ch09fig05)展示了示例神经网络。'
- en: Figure 9.5\. A neural network of two hidden layers. Loosely modeled on the human
    brain, the neurons (circles in each layer) are connected with weights that are
    learned during model training. The output variables can be predicted by running
    the input variables through the weighted connections. In deep learning, this classical
    neural network concept is expanded to include more hidden layers of various shapes
    and various degrees of connectivity between layers.
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5. 具有两个隐藏层的神经网络。松散地模拟人脑，神经元（每层的圆圈）通过在模型训练期间学习的权重相互连接。输出变量可以通过运行输入变量通过加权连接来预测。在深度学习中，这个经典的神经网络概念被扩展到包括更多形状和层间连接程度各异的隐藏层。
- en: '![](09fig05.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig05.jpg)'
- en: One of the disadvantages of deep neural nets are that even on GPU hardware,
    the computational resources needed to build and optimize models can take a long
    time. In practice, you may be able to get just as good performance with other
    algorithms, such as random forests, using far less time and resources. This depends
    on the dataset and problem at hand, as always. Another disadvantage is that it
    can be difficult to understand what’s going on under the hood of these neural
    net models. Some refer to them as *black-box models*, because you have to trust
    the results of your statistical analysis of the models without doing any introspection
    of their internals. This again depends on the use case. If you’re working with
    images, for example, the neurons can take on intuitive representations of various
    visual patterns that lead to specific predictions.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的一个缺点是，即使在GPU硬件上，构建和优化模型所需的计算资源也可能需要很长时间。在实践中，你可能只需使用其他算法，如随机森林，就能获得几乎相同的表现，同时花费更少的时间和资源。这始终取决于数据集和问题。另一个缺点是，理解这些神经网络模型内部发生的事情可能很困难。有些人称它们为*黑盒模型*，因为你必须相信你对模型进行的统计分析结果，而不需要对其内部进行任何反思。这同样取决于用例。例如，如果你在处理图像，神经元可以承担各种视觉模式的直观表示，这些模式导致特定的预测。
- en: Many deep-learning methods have shown to scale to large datasets, sometimes
    by using modern graphic cards (GPUs) for performing certain computations. For
    a deep-learning library in Python that supports GPUs, take a look at *Theano*
    ([http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))
    or *Keras* ([http://keras.io/](http://keras.io/)), which is based on Theano).
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习方法已经证明可以扩展到大型数据集，有时通过使用现代图形卡（GPU）来执行某些计算。对于支持GPU的Python深度学习库，请查看*Theano*
    ([http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))或*Keras*
    ([http://keras.io/](http://keras.io/))，它基于Theano)。
- en: 9.3\. Scaling predictions
  id: totrans-700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 扩展预测
- en: Scaling ML isn’t only about scaling to larger datasets. Imagine you’re building
    an email service, and you suddenly have millions of users. You built a nice spam-detection
    model, and it even scales to large datasets, but now you need to make hundreds
    of millions of predictions per day. That’s more than 10 thousand per second! [Figure
    9.6](#ch09fig06) illustrates this common pattern. In this section, we discuss
    ways to scale the volume of predictions and scale the velocity when predictions
    need to be used in real time.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展机器学习不仅仅是扩展到更大的数据集。想象一下你正在构建一个电子邮件服务，突然有数百万用户。你构建了一个很好的垃圾邮件检测模型，它甚至可以扩展到大型数据集，但现在你每天需要做出数亿次的预测。这超过了每秒10,000次！[图9.6](#ch09fig06)说明了这种常见的模式。在本节中，我们讨论了扩展预测量以及当需要实时使用预测时的扩展速度的方法。
- en: Figure 9.6\. Scaling the prediction part of the ML workflow to high volumes
    or high-velocity predictions
  id: totrans-702
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6\. 将机器学习工作流程中的预测部分扩展到高容量或高速预测
- en: '![](09fig06_alt.jpg)'
  id: totrans-703
  prefs: []
  type: TYPE_IMG
  zh: '![09fig06_alt.jpg](09fig06_alt.jpg)'
- en: First, you’ll look at infrastructure architectures for scaling with the volume
    of predictions so you can handle the large user base of your email client, for
    example. Next, you’ll look at how to scale the velocity of predictions and guarantee
    an answer within a given timeframe. This is important when your ML models are
    used in the real-time feedback loop of, for example, humans on web or mobile devices.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将查看用于通过预测量扩展的基础设施架构，这样你就可以处理电子邮件客户端等的大用户群。接下来，你将了解如何扩展预测速度并保证在给定时间内得到答案。当你的机器学习模型用于例如人类在网页或移动设备上的实时反馈循环时，这一点很重要。
- en: 9.3.1\. Scaling prediction volume
  id: totrans-705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1\. 扩展预测量
- en: In order to handle many predictions, you can use patterns known from computational
    architecture for scaling workers to support any number of requests. The traditional
    approach is to have a queue of prediction jobs from which a number of worker nodes
    pull predictions, load the model (if needed), make the prediction, and push back
    the results in whatever way makes sense for the application. [Figure 9.7](#ch09fig07)
    shows how this architecture might look for scaling predictions by volume.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理大量预测，你可以使用来自计算架构中已知的模式来扩展工作节点以支持任意数量的请求。传统的方法是有一个预测作业队列，其中多个工作节点从中提取预测，加载模型（如果需要），进行预测，并以任何对应用程序有意义的方式推送结果。[图9.7](#ch09fig07)展示了通过容量扩展预测的这种架构可能的样子。
- en: Figure 9.7\. A possible infrastructure for a scalable prediction service. Prediction
    requests are sent from the consumer to a queue, which delegates the job to a prediction
    worker. The worker stores the prediction in a database and delivers it back to
    the client when done. If the queue is clogging up when more predictions are streaming
    in than the workers can handle, more workers can be spun up.
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7. 可扩展预测服务的可能基础设施。预测请求从消费者发送到一个队列，队列将任务委托给预测工人。工人在数据库中存储预测结果，并在完成后将其发送回客户端。如果队列因流入的预测请求多于工人能处理的数量而拥堵，可以启动更多的工人。
- en: '![](09fig07_alt.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig07_alt.jpg)'
- en: 'This approach requires that the model can be loaded on all worker nodes, of
    course, and that there are enough workers (or an autoscaling solution in place)
    to handle the number of predictions coming in. You can easily calculate the number
    of workers needed if you know the mean prediction time for a worker and the velocity
    of requests coming in:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法要求模型可以加载到所有工作节点上，当然，还需要有足够的工人（或已部署的自动扩展解决方案）来处理涌入的预测数量。如果你知道一个工人的平均预测时间和请求的速度，你可以轻松计算出所需的工人数量：
- en: '[PRE13]'
  id: totrans-710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For example, if you have 10 prediction requests coming in per second, and your
    workers take 2 seconds to finish, you need at least 20 workers to keep up. The
    optimal autoscaling solution here is to be able to spawn new workers from the
    number of requests waiting in the queue over a certain period of time.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你每秒有10个预测请求进入，而你的工人需要2秒来完成，你需要至少20个工人来跟上。最佳的自动扩展解决方案是能够在一定时间内根据队列中等待的请求数量启动新的工人。
- en: 9.3.2\. Scaling prediction velocity
  id: totrans-712
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2. 扩展预测速度
- en: In some cases, you need your predictions to be returned within a certain time
    after the request was made by a client. Prediction velocity can be important,
    for example, when predictions are made in response to a user action. Users expect
    feedback in real time, and waiting even a few seconds can be detrimental to the
    user experience. Imagine a Google search that takes 20 seconds—likely, you’d be
    long gone. Or, if you’re making predictions about financial transactions, mere
    milliseconds could mean making or losing a lot of money.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你需要预测在客户端发出请求后的一定时间内返回。预测速度可能很重要，例如，当预测是对用户操作的反应时。用户期望实时反馈，等待几秒钟都可能对用户体验产生不利影响。想象一下，一个谷歌搜索需要20秒——很可能你已经离开了。或者，如果你正在对金融交易进行预测，仅仅几毫秒可能意味着赚或亏很多钱。
- en: Various approaches are available to make your predictions faster, such as upgrading
    your hardware or using more-efficient algorithms or implementations of an algorithm.
    You can also optimize the network and make sure that the client is as physically
    close to your servers as possible. In addition, you shouldn’t call any other service
    that may introduce additional latency, such as recording the predictions to a
    database, or waiting for the data to be written to disk and replicated across
    a cluster. In the following example, we’ll assume that you’ve already considered
    these points. Now you’ll take a look at two architectures for serving real-time
    predictions.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以使预测更快，例如升级硬件或使用更高效的算法或算法的实现。你还可以优化网络，确保客户端尽可能靠近你的服务器。此外，你不应该调用任何可能引入额外延迟的其他服务，例如将预测记录到数据库中，或等待数据被写入磁盘并在集群中复制。在下面的例子中，我们假设你已经考虑了这些点。现在，你将查看两个用于提供实时预测的架构。
- en: The first architecture for fast predictions is similar to the architecture introduced
    in the preceding scale-by-volume section, but requires more workers. The basic
    idea is that each prediction request is sent to multiple workers at once, and
    the first prediction that finishes is sent back to the customer. [Figure 9.8](#ch09fig08)
    shows an example of this architecture.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 快速预测的第一个架构与前面按体积缩放部分中介绍的架构类似，但需要更多的工人。基本思路是，每个预测请求同时发送给多个工人，第一个完成的预测结果被发送回客户。[图9.8](#ch09fig08)展示了这种架构的一个示例。
- en: Figure 9.8\. A possible architecture for a prediction pipeline with low-latency
    requirements. A prediction dispatcher sends the prediction job to multiple workers,
    hoping that at least one will return predictions in time. It will return the first
    one that comes back to the client, and afterward record it in a log or database
    for later inspection and analytics (in the background, possibly while already
    working on the next prediction).
  id: totrans-716
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8\. 具有低延迟要求的预测管道的可能架构。预测调度器将预测任务发送到多个工作者，希望至少有一个能及时返回预测。它将返回第一个返回的客户，之后将其记录在日志或数据库中，以供后续检查和分析（在后台，可能在已经处理下一个预测的同时）。
- en: '![](09fig08.jpg)'
  id: totrans-717
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig08.jpg)'
- en: Another approach to real-time predictions is to make predictions in parts so
    the computation can be distributed among multiple machines. Ensemble methods comprise
    a class of algorithms that lend themselves well to this approach. We’ll again
    use random forests as an example here.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实时预测的方法是将预测分成几部分，以便计算可以在多台机器之间分配。集成方法是一类适合这种方法的算法。我们再次以随机森林为例。
- en: Random forest models consist of an ensemble of decision trees. The algorithm
    makes a prediction from each tree and (in the case of classification) counts the
    votes from each tree into the final probability. For example, if there are 10
    trees and 6 of them vote yes for a particular prediction instance, the forest
    returns 6/10, or 60%, as the answer. Usually, the larger the total number of trees
    queried, the more accurate and confident the results. This can be used in a real-time
    prediction system to trade accuracy for speed. If each prediction node is responsible
    for a tree or list of trees from the forest, you ask each for a prediction. Whenever
    a node finishes predicting on its own trees, the result is returned to a collector
    service that collects results from all nodes and makes the final prediction. The
    collector can observe a time limit and at any time return the prediction in its
    current state, if necessary. For example, if only 20 of 1,000 trees have returned
    anything, the user gets an answer, but it’s not as accurate as it could have been
    had all 1,000 trees had time to return an answer.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型由一系列决策树组成。该算法从每棵树中进行预测，并在分类的情况下，将每棵树的投票计入最终概率。例如，如果有10棵树，其中6棵树对一个特定的预测实例投赞成票，那么森林返回6/10，即60%作为答案。通常，查询的总树数越多，结果越准确、越有信心。这可以用于实时预测系统，以速度换取准确性。如果每个预测节点负责森林中的一棵树或树列表，那么你会要求每个节点进行预测。每当节点完成其树的预测后，结果会返回给收集服务，该服务收集所有节点的结果并做出最终预测。收集器可以观察时间限制，并在必要时在任何时候返回当前状态的预测。例如，如果只有20棵树中的1,000棵返回了任何内容，用户会得到一个答案，但它的准确性不如所有1,000棵树都有时间返回答案的情况。
- en: '[Figure 9.9](#ch09fig09) shows a diagram of this architecture in action.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.9](#ch09fig09)显示了该架构的实际操作图。'
- en: Figure 9.9\. Suggested architecture for a prediction pipeline that’s guaranteed
    to return within a certain time, potentially sacrificing prediction accuracy and
    confidence if some of the partial predictions haven’t returned yet. Prediction
    requests are shipped to workers by the producer, while a consumer service collects
    partial predictions ready to return to the client if time is up.
  id: totrans-721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.9\. 确保在特定时间内返回的预测管道的建议架构，如果一些部分预测尚未返回，可能会牺牲预测的准确性和信心。预测请求由生产者发送到工作者，而消费者服务收集准备返回给客户的部分预测，如果时间到了。
- en: '![](09fig09.jpg)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig09.jpg)'
- en: 'A few systems are showing promise in supporting these scalable, real-time systems.
    One is part of the previously mentioned Apache Spark ecosystem: Spark Streaming.
    With Spark Streaming, you get a set of tools and libraries that makes it easier
    to build real-time, stream-oriented data-processing pipelines. Don’t forget that
    any prediction made usually has to go through the same feature-engineering processes
    that the training data went through at model-building time.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统在支持这些可扩展的实时系统方面显示出希望。一个是之前提到的Apache Spark生态系统的一部分：Spark Streaming。使用Spark
    Streaming，你获得了一套工具和库，使构建实时、面向流的数据处理管道变得更容易。别忘了，任何预测通常都必须经过与模型构建时训练数据相同的特征工程过程。
- en: Other projects include Apache Storm, Apache Kafka, AWS Kinesis, and Turi. Each
    project has pros and cons for particular use cases, so we encourage you to investigate
    the appropriate tool for your needs.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 其他项目包括Apache Storm、Apache Kafka、AWS Kinesis和Turi。每个项目都有针对特定用例的优缺点，所以我们鼓励你调查适合你需求的适当工具。
- en: 9.4\. Summary
  id: totrans-725
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4. 摘要
- en: 'In this chapter, you’ve investigated various ways to scale machine-learning
    systems to large datasets by transforming the data or building a horizontally
    scalable multimachine infrastructure. The main takeaways from the chapter are
    as follows:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经研究了通过转换数据或构建水平可扩展的多机基础设施来扩展机器学习系统到大型数据集的各种方法。本章的主要收获如下：
- en: 'Scaling up your machine-learning system is sometimes necessary. These are some
    common reasons:'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的机器学习系统有时是必要的。以下是一些常见原因：
- en: The training data doesn’t fit on a single machine.
  id: totrans-728
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据无法适应单个机器。
- en: The time to train a model is too long.
  id: totrans-729
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型的时间太长。
- en: The volume of data coming in is too high.
  id: totrans-730
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入的数据量太大。
- en: The latency requirements for predictions are low.
  id: totrans-731
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的延迟要求很低。
- en: 'Sometimes you can avoid spending time and resources on a scalable infrastructure
    by doing the following:'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，你可以通过以下方式避免在可扩展基础设施上花费时间和资源：
- en: Choosing a different ML algorithm that’s fast or lean enough to work on a single
    machine without sacrificing accuracy
  id: totrans-733
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个足够快或足够精简的机器学习算法，可以在不牺牲准确性的情况下在单个机器上工作
- en: Subsampling the data
  id: totrans-734
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行子采样
- en: Scaling up vertically (upgrading the machine)
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直扩展（升级机器）
- en: Sacrificing accuracy or easing other constraints if it’s still cheaper than
    scaling up
  id: totrans-736
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果仍然比扩展更便宜，则可以牺牲准确性或放宽其他约束
- en: 'If it’s not possible to avoid scaling up in a horizontal fashion, widely used
    systems are available for setting up a scalable data-management and processing
    infrastructure:'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果无法避免以横向方式扩展，则广泛使用的系统可用于设置可扩展的数据管理和处理基础设施：
- en: The Hadoop ecosystem with the Mahout machine-learning framework
  id: totrans-738
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有 Mahout 机器学习框架的 Hadoop 生态系统
- en: The Spark ecosystem with the MLlib machine-learning library
  id: totrans-739
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有 MLlib 机器学习库的 Spark 生态系统
- en: The Turi (formerly GraphLab) framework
  id: totrans-740
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turi（以前称为 GraphLab）框架
- en: Streaming technologies such as Spark Streaming, Apache Storm, Apache Kafka,
    and AWS Kinesis
  id: totrans-741
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式技术，如 Spark Streaming、Apache Storm、Apache Kafka 和 AWS Kinesis
- en: 'When scaling up a model-building pipeline, consider the following:'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在扩展模型构建管道时，请考虑以下因素：
- en: Choosing a scalable algorithm such as logistic regression or linear SVMs
  id: totrans-743
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个可扩展的算法，如逻辑回归或线性 SVM
- en: Scaling up other (for example, nonlinear) algorithms by making data and algorithm
    approximations
  id: totrans-744
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对数据和算法进行近似来扩展其他（例如，非线性）算法
- en: Building a scalable version of your favorite algorithm using a distributed computing
    infrastructure
  id: totrans-745
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布式计算基础设施构建你喜欢的算法的可扩展版本
- en: 'Predictions can be scaled in both volume and velocity. Useful approaches include
    the following:'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测可以在数量和速度上进行扩展。以下是一些有用的方法：
- en: Building your infrastructure so that it allows you to scale up the number of
    workers with the prediction volume
  id: totrans-747
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建你的基础设施，使其能够随着预测量的增加而扩展工作者的数量
- en: Sending the same prediction to multiple workers and returning the first one
    in order to optimize prediction velocity
  id: totrans-748
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相同的预测发送到多个工作者，并返回第一个以优化预测速度
- en: Choosing an algorithm that allows you to parallelize predictions across multiple
    machines
  id: totrans-749
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个允许你在多台机器上并行化预测的算法
- en: 9.5\. Terms from this chapter
  id: totrans-750
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5. 本章术语
- en: '| Word | Definition |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| 词汇 | 定义 |'
- en: '| --- | --- |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| big data | A broad term usually used to denote data management and processing
    problems that can’t fit on single machines. |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| 大数据 | 一个广泛使用的术语，通常用来表示无法在单个机器上处理的数据管理和处理问题。 |'
- en: '| horizontal/vertical scaling | Scaling out horizontally means adding more
    machines to handle more data. Scaling up vertically means upgrading the hardware
    of your machines. |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| 水平/垂直扩展 | 水平扩展意味着添加更多机器来处理更多数据。垂直扩展意味着升级你的机器的硬件。 |'
- en: '| Hadoop, HDFS, MapReduce, Mahout | The Hadoop ecosystem is widely used in
    science and industry for handling and processing large amounts of data. HDFS and
    MapReduce are the distributed storage and parallel processing systems respectively,
    and Mahout is the machine-learning component of the Hadoop ecosystem. |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop, HDFS, MapReduce, Mahout | Hadoop 生态系统在科学和工业中广泛用于处理和加工大量数据。HDFS 和
    MapReduce 分别是分布式存储和并行处理系统，而 Mahout 是 Hadoop 生态系统中的机器学习组件。 |'
- en: '| Apache Spark, MLlib | Apache Spark is a newer project that tries to keep
    data in memory to make it much more efficient than the disk-based Hadoop. MLlib
    is the machine-learning library that comes with Spark. |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| Apache Spark, MLlib | Apache Spark 是一个较新的项目，它试图将数据保留在内存中，使其比基于磁盘的 Hadoop
    更高效。MLlib 是 Spark 伴随的机器学习库。 |'
- en: '| data locality | Doing computation on the data where it resides. Data transfer
    can often be the bottleneck in big-data projects, so avoiding transferring data
    can result in a big gain in resource requirements. |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| 数据局部性 | 在数据所在的位置进行计算。在大数据项目中，数据传输往往成为瓶颈，因此避免数据传输可以大大提高资源需求。 |'
- en: '| polynomial features | A trick to extend linear models to include nonlinear
    polynomial feature interaction terms without losing the scalability of linear
    learning algorithms. |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| 多项式特征 | 一种技巧，可以将线性模型扩展到包括非线性多项式特征交互项，同时不失去线性学习算法的可扩展性。 |'
- en: '| Vowpal Wabbit | An ML tool for building models efficiently on large datasets
    without necessarily using a full big-data system such as Hadoop. |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| Vowpal Wabbit | 一种机器学习工具，可以在大型数据集上高效地构建模型，而不必使用像Hadoop这样的完整大数据系统。 |'
- en: '| out-of-core | Computations are done out of core if you need to keep only
    the current iteration of data in memory. |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| 离核计算 | 如果您只需要在内存中保留当前迭代的数据，则进行离核计算。 |'
- en: '| histogram approximations | Approximations of the training data that convert
    all columns to histograms for the learning process. |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| 直方图近似 | 对训练数据的近似，将所有列转换为学习过程中的直方图。 |'
- en: '| feature selection | Process of reducing the size of training data by selecting
    and retaining the best (most predictive) subset of features. |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '| 特征选择 | 通过选择和保留最佳（最具预测性）的特征子集来减少训练数据大小的过程。 |'
- en: '| Lasso | Linear algorithm that selects the most predictive subset of features.
    Very useful for feature selection. |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '| Lasso | 一种线性算法，选择最具预测性的特征子集。在特征选择方面非常有用。 |'
- en: '| deep neural nets | An evolution of neural nets that scales to larger datasets
    and achieves state-of-the-art accuracy. Requires more knowledge and computational
    resources in practice than other algorithms, depending on the dataset and problem
    at hand. |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| 深度神经网络 | 神经网络的一种演变，可以扩展到更大的数据集并实现最先进的准确性。在实际应用中，相对于其他算法，它需要更多的知识和计算资源，这取决于数据集和问题。
    |'
- en: '| prediction volume/velocity | Scaling prediction volume means being able to
    handle a lot of data. Scaling velocity means being able to do it fast enough for
    a specific real-time use case. |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| 预测量/速度 | 扩展预测量意味着能够处理大量数据。扩展速度意味着能够足够快地处理特定实时用例。 |'
- en: '| accuracy vs. speed | For real-time predictions, you can sometimes trade accuracy
    of the prediction for the speed with which the prediction is made. |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| 准确度与速度 | 对于实时预测，有时可以以预测速度为代价来换取预测的准确性。 |'
- en: '| Spark Streaming, Apache Storm, Apache Kafka, AWS Kinesis | Upcoming technologies
    for building real-time streaming systems. |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| Spark Streaming、Apache Storm、Apache Kafka、AWS Kinesis | 建立实时流系统的即将到来的技术。
    |'
- en: 'Chapter 10\. Example: digital display advertising'
  id: totrans-768
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章。示例：数字展示广告
- en: '*This chapter covers*'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Visualizing and preparing a real-world dataset
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和准备一个现实世界的数据集
- en: Building a predictive model of the probability that users will click a digital
    display advertisement
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立用户点击数字展示广告概率的预测模型
- en: Comparing the performance of several algorithms in both training and prediction
    phases
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较几个算法在训练和预测阶段的表现
- en: Scaling by dimension reduction and parallel processing
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过降维和并行处理进行扩展
- en: '[Chapter 9](kindle_split_020.html#ch09) presented techniques that enable you
    to scale your machine-learning workflow. In this chapter, you’ll apply those techniques
    to a large-scale real-world problem: optimizing an online advertising campaign.
    We begin with a short introduction to the complex world of online advertising,
    the data that drives it, and some of the ways it’s used by advertisers to maximize
    *return on advertising spend* (ROAS). Then we show how to put some of the techniques
    in [chapter 9](kindle_split_020.html#ch09) to use in this archetypal big-data
    application.'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](kindle_split_020.html#ch09)介绍了使您能够扩展机器学习工作流程的技术。在本章中，您将应用这些技术来解决一个大规模的实际情况：优化在线广告活动。我们从对在线广告复杂世界的简要介绍开始，包括驱动它的数据，以及广告商用来最大化*广告支出回报率*（ROAS）的一些方式。然后我们展示如何将第9章中的一些技术应用于这个典型的大数据应用。'
- en: We employ several datasets in our example. Unfortunately, only a few large datasets
    of this type are available to the public. The primary dataset in our example isn’t
    available for download, and even if it were, it would be too large for personal
    computing.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了几个数据集。不幸的是，此类大型数据集只有少数对公众开放。我们示例中的主要数据集不可下载，即使可以下载，也太大，不适合个人计算机使用。
- en: One dataset that can be downloaded and used for noncommercial purposes is from
    the Kaggle Display Advertising Challenge sponsored by Criteo, a company whose
    business is optimizing the performance of advertising campaigns. The Criteo dataset
    contains more than 45 million observations of 39 features, of which 13 are numerical
    and 26 categorical. Unfortunately, as is common for datasets used in data science
    competitions, the meaning of the features is obfuscated. The variable names are
    V1 through V40. V1 is the label, and V2 through V40 are features. In the real
    world, you’d have the benefit of knowing what each feature measures or represents.
    But as the competition proved, you can nonetheless explore their predictive value
    and create useful models.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 可以下载并用于非商业目的的一个数据集来自由Criteo赞助的Kaggle展示广告挑战赛。Criteo数据集包含超过4500万个关于39个特征的观测值，其中13个是数值型，26个是分类型。不幸的是，正如在数据科学竞赛中使用的数据集常见的那样，特征的含义是模糊的。变量名称从V1到V40。V1是标签，V2到V40是特征。在现实世界中，你会知道每个特征衡量或代表什么。但正如比赛证明的那样，你仍然可以探索它们的预测价值并创建有用的模型。
- en: The Criteo dataset is available at [https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz](https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz).
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: Criteo数据集可在[https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz](https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz)获取。
- en: 10.1\. Display advertising
  id: totrans-778
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1. 展示广告
- en: Half the money I spend on advertising is wasted; the trouble is, I don’t know
    which half.
  id: totrans-779
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我花在广告上的钱有一半是浪费的；麻烦的是，我不知道是哪一半。
- en: ''
  id: totrans-780
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*John Wannamaker*'
  id: totrans-781
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*约翰·万纳梅克*'
- en: In the days of *Mad Men*, this was an inescapable truth. But with digital advertising
    comes the opportunity to discover what works and what doesn’t via the data collected
    as users interact with online ads.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *广告狂人* 时代，这是一个无法避免的事实。但随着数字广告的出现，我们可以通过收集用户与在线广告互动的数据来发现哪些有效，哪些无效。
- en: Online advertising is delivered through a myriad of media. *Display ads* appear
    within web pages rendered in browsers, usually on personal computers or laptops.
    Because the rules for identifying users and the handling of internet cookies are
    different on mobile browsers, mobile ad technology relies on a different set of
    techniques and generates quite different historical data. *Native ads*, embedded
    in games and mobile apps, and *pre-roll ads* that precede online video content,
    are based on distinct delivery technologies and require analyses tailored to their
    unique processes. Our examples are limited to *traditional* display advertising.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 在线广告通过众多媒体进行传递。*展示广告*出现在浏览器中渲染的网页内，通常在个人电脑或笔记本电脑上。由于移动浏览器上识别用户和处理互联网cookie的规则不同，移动广告技术依赖于不同的一套技术，并生成相当不同的历史数据。*原生广告*嵌入在游戏和移动应用中，以及*预播放广告*，它们在在线视频内容之前播放，基于不同的传递技术，并需要针对其独特流程的分析。我们的例子仅限于*传统*展示广告。
- en: Much of the terminology of display advertising was inherited from the print
    advertising business. The websites on which ads can be purchased are known as
    *publications*, within which advertising space is characterized by size and format,
    or *ad unit*, and location within the site and page is referred to as *placement.*
    Each presentation of an ad is called an *impression*. Ads are sold in lots of
    1,000 impressions, the price of which is known as CPM, (cost per thousand).
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 展示广告的大部分术语都是从印刷广告业务中继承下来的。可以购买广告的网站被称为 *出版物*，其中的广告空间以尺寸和格式为特征，或称为 *广告单元*，而广告在网站和页面中的位置被称为
    *投放位置*。每次广告展示都称为 *印象*。广告以每1000次印象的批量出售，这种价格被称为CPM（每千次成本）。
- en: When a user browses to a web page—say, xyz.com—it appears that the publisher
    of xyz.com delivers the entire page. In reality, the page contains placeholders
    for advertisements that are filled in by various advertisers through a complex
    network of intermediaries. Each web server that delivers ads maintains logs that
    include information about each impression, including the publisher, the internet
    address of the user, and information contained in internet *cookies*, where information
    about previous deliveries from the advertiser’s server may be stored. In the next
    section, you’ll look at the sorts of data that’s captured during a display ad
    campaign.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户浏览到网页时——比如说，xyz.com——看起来是 xyz.com 的发布者提供了整个页面。实际上，页面包含由各种广告商通过复杂的中间商网络填充的广告占位符。每个提供广告的Web服务器都维护日志，包括有关每次印象的信息，包括发布者、用户的互联网地址以及包含在互联网
    *cookies* 中的信息，其中可能存储了来自广告商服务器的先前交付的信息。在下一节中，您将了解在展示广告活动中捕获的数据类型。
- en: 10.2\. Digital advertising data
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 数字广告数据
- en: 'Web servers capture data for each user request, including the following:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器为每个用户请求捕获数据，包括以下内容：
- en: '***Client address—*** The IP address of the computer that made the request.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***客户端地址—*** 发起请求的计算机的IP地址。'
- en: '***Request—*** The URL and parameters (for example, [http://www.abc.com?x=1234&y=abc01](http://www.abc.com?x=1234&y=abc01)).'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***请求—*** URL和参数（例如，[http://www.abc.com?x=1234&y=abc01](http://www.abc.com?x=1234&y=abc01)）。'
- en: '***Status—*** The response code issued by the server; usually 200, indicating
    successful response.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***状态—*** 服务器发出的响应代码；通常是200，表示成功响应。'
- en: '***Referrer—*** The web page from which the user linked to the current page.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***来源网站—*** 用户链接到当前网页的网页。'
- en: '***User agent—*** A text string that identifies the browser and operating system
    making the request.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户代理—*** 识别发起请求的浏览器和操作系统的文本字符串。'
- en: '***Cookie—*** A small file stored when a browser visits a website. When the
    site is visited again, the file is sent along with the request.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Cookie—*** 当浏览器访问网站时存储的小文件。当再次访问该网站时，文件会随请求一起发送。'
- en: 'In addition, many modern advertisements are served in conjunction with measurement
    programs—small JavaScript programs that capture information such as the following:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多现代广告都与测量程序一起提供——捕获如下信息的小型JavaScript程序：
- en: '***Viewability—*** Whether and for how long the advertisement was displayed.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可视化—*** 广告是否以及显示了多长时间。'
- en: '***User ID—*** Browser cookies are used to leave behind unique identifiers
    so that users can be recognized when encountered again.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户ID—*** 浏览器cookies用于留下唯一标识符，以便在再次遇到时识别用户。'
- en: '***Viewable seconds—*** Number of seconds advertisement was in view.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可视化秒数—*** 广告显示的秒数。'
- en: '[Figure 10.1](#ch10fig01) shows sample data from a campaign. Viewability data
    is extracted from a query string, and `user_id` is a randomly generated identifier
    that associates users with previous visits.'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.1](#ch10fig01) 展示了活动样本数据。可视化数据是从查询字符串中提取的，而 `user_id` 是随机生成的标识符，用于将用户与之前的访问关联起来。'
- en: Figure 10.1\. Impression data. Domain names are randomly generated substitutes
    for the real names.
  id: totrans-799
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1. 印象数据。域名是随机生成的真实名称的替代品。
- en: '![](10fig01_alt.jpg)'
  id: totrans-800
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig01_alt.jpg)'
- en: 10.3\. Feature engineering and modeling strategy
  id: totrans-801
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3. 特征工程和建模策略
- en: '*Click* is our target variable. You want to predict the likelihood that impressions
    will result in clicks (sometimes called *click-throughs* or *click-thrus*). More
    specifically, given a specific user visiting a particular site, you’d like to
    know the probability that the user will click the advertisement. You have several
    choices in formulating the problem. You can try to predict the probability that
    a given user will click through, and you can try to predict the click-through
    rate (CTR) for each publisher that presents the ad.'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '*点击* 是我们的目标变量。您希望预测印象导致点击的可能性（有时称为 *点击率* 或 *点击通过率*）。更具体地说，给定一个特定用户访问特定网站，您想了解用户点击广告的概率。在制定问题方面，您有几个选择。您可以尝试预测给定用户点击通过的概率，也可以尝试预测每个展示广告的发布者的点击通过率（CTR）。'
- en: 'As is often the case, precisely what you model and the precise values you endeavor
    to predict will ultimately be driven by asking these questions: *How will the
    prediction be used? In what manner will it be acted on?* In this case, our advertiser
    has the option of blacklisting certain publications, so the advertiser’s primary
    concern is identifying the publications least likely to yield clicks. In recent
    years, *real-time bidding* technologies have been developed that enable advertisers
    to bid for individual impressions based on user and publication features provided
    by the bidding system, but our example advertiser hasn’t adopted real-time bidding
    yet.'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 正如通常情况一样，你模型的具体内容和你努力预测的确切值最终将由提出这些问题来驱动：*预测将如何被使用？它将以何种方式被实施？*在这种情况下，我们的广告商有选择性地将某些出版物列入黑名单的选项，因此广告商的主要关注点是识别最不可能产生点击的出版物。近年来，已经开发出*实时竞价*技术，使广告商能够根据竞价系统提供的用户和出版物特征对单个展示进行竞价，但我们的示例广告商尚未采用实时竞价。
- en: You might wonder at this point why the advertiser doesn’t just look at some
    historical data for all the publications and blacklist those with low CTRs. The
    problem is that when the overall CTR for a campaign is in the neighborhood of
    0.1%, the expected value of clicks for a publication with only a few impressions
    is zero. The absence of clicks doesn’t indicate a low CTR. Further, when we aggregate
    the best-performing, low-volume publications, we often observe above-average CTR
    (so just blacklisting all the low-volume pubs isn’t a good strategy). You’re looking
    for a model that will enable you to predict publications’ performance without
    the benefit of a great deal of performance history.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在这个时候感到疑惑，为什么广告商不直接查看所有出版物的一些历史数据，并将CTR低的出版物列入黑名单。问题是，当一场活动的整体CTR在0.1%左右时，只有少量展示的出版物的点击预期值是零。没有点击并不表示CTR低。此外，当我们汇总表现最好的低量级出版物时，我们经常观察到高于平均水平的CTR（因此仅仅将所有低量级出版物列入黑名单并不是一个好的策略）。你正在寻找一个模型，它将使你能够在没有大量性能历史的情况下预测出版物的表现。
- en: At first glance, you might imagine you don’t have much to work with. You can
    count impressions, clicks, and views for users, publishers, and operating systems.
    Maybe time of day or day of the week has some effect. But on further reflection,
    you realize that the domains a user visits are features that describe the user,
    and the users who visit a domain are features of the domain. Suddenly, you have
    a wealth of data to work with and a real-world opportunity to experience *the
    curse of dimensionality*—a phrase used to describe the tribulations of working
    in *high-dimensional space.* As you explore the data, you’ll see that a wealth
    of features can be, if not a curse, a mixed blessing.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，你可能认为你没有什么可以利用的。你可以统计用户、出版商和操作系统的展示、点击和观看次数。也许一天中的某个时间或一周中的某一天会有一些影响。但进一步思考后，你意识到用户访问的域名是描述用户的功能，访问域名的用户是域名特征。突然之间，你有了大量可以工作的数据，以及一个真实世界的体验*维度诅咒*的机会——这是一个用来描述在高维空间中工作所遇到的困境的短语。当你探索数据时，你会发现大量特征，如果不是诅咒，也是好坏参半的祝福。
- en: 'You may recognize the logic you’ll apply here as the basis of *recommenders*,
    the systems that suggest movies on Netflix, products on Amazon, and restaurants
    on Yelp. The idea of characterizing users as collections of items, and items as
    collections of users, is the basis of *collaborative filtering*, in which users
    are clustered based on common item preferences, and items are clustered based
    on the affinities of common users. Of course, the motivation for recommenders
    is to present users with items they’re likely to purchase. The advertising problem
    is a variation; instead of many items, the same advertisement is presented in
    a wide variety of contexts: the publications. The driving principle is that the
    greatest likelihood of achieving user responses (clicks) will be on publications
    that are similar to those that have a history of achieving responses. And because
    similarity is based on common users, pubs chosen in this manner will attract people
    who are similar in their preferences to past responders.'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认出在这里应用的逻辑是*推荐系统*的基础，这些系统在Netflix上推荐电影，在Amazon上推荐产品，在Yelp上推荐餐厅。将用户描述为物品集合，将物品描述为用户集合的想法是*协同过滤*的基础，其中用户根据共同物品偏好进行聚类，物品根据共同用户的亲和力进行聚类。当然，推荐系统的动机是向用户提供他们可能购买的项目。广告问题是一个变体；不是许多项目，而是在广泛的上下文中展示相同的广告：出版物。驱动原则是，在类似那些有响应历史记录的出版物上实现用户响应（点击）的可能性最大。而且因为相似性基于共同用户，以这种方式选择的出版物将吸引与过去响应者偏好相似的人。
- en: 10.4\. Size and shape of the data
  id: totrans-807
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4. 数据的大小和形状
- en: You’ll start with a sample of 9 million observations, a small-enough sample
    to fit into memory so you can do some quick calculations of cardinality and distributions.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从900万个观测值的样本开始，这是一个足够小的样本，可以放入内存中，这样你就可以做一些关于基数和分布的快速计算。
- en: Listing 10.1\. A first look at the data
  id: totrans-809
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1. 数据的第一印象
- en: '![](218fig01_alt.jpg)'
  id: totrans-810
  prefs: []
  type: TYPE_IMG
  zh: '![图片218fig01_alt](218fig01_alt.jpg)'
- en: Fortunately, most users never visit most of the domains, so the user/item matrix
    is sparsely populated, and you have tools at your disposal for dealing with large,
    sparse matrices. And nobody said that users and domains must be the rows and columns
    of a gigantic matrix, but it turns out that some valuable algorithms work exceptionally
    well when it’s possible to operate on a user/item matrix in memory.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数用户从未访问过大多数域名，因此用户/项目矩阵是稀疏的，你可以使用工具来处理大型、稀疏的矩阵。没有人说过用户和域名必须是巨大矩阵的行和列，但事实是，当可以在内存中操作用户/项目矩阵时，一些有价值的算法表现得特别出色。
- en: 'Oh, and one more thing: the 9 million observations referenced in [listing 10.1](#ch10ex01)
    represent roughly 0.1% of the data. Ultimately, you need to process roughly 10
    billion impressions, and that’s just one week’s worth of data. We loaded the data
    from 9 million impressions into about 53% of the memory on an Amazon Web Services
    (AWS) instance with 32 GB of RAM, so this will certainly get more interesting
    as you go.'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，还有一件事：在[列表10.1](#ch10ex01)中提到的900万观测值大约占数据的0.1%。最终，你需要处理大约100亿次的印象，而这只是数据的一周量。我们将900万次印象的数据加载到亚马逊网络服务（AWS）实例的约53%内存中，该实例有32GB的RAM，所以随着你的深入，这肯定会更有趣。
- en: Next, let’s look at how the data is distributed over the categorical variables.
    In [listing 10.1](#ch10ex01), we already started this process by computing the
    cardinality of `pub_domain` and `user_id`.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看数据如何在分类变量上分布。在[列表10.1](#ch10ex01)中，我们已经通过计算`pub_domain`和`user_id`的基数开始了这个过程。
- en: Listing 10.2\. Distributions
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2. 分布
- en: '![](ch10ex02-0.jpg)'
  id: totrans-815
  prefs: []
  type: TYPE_IMG
  zh: '![图片ch10ex02-0](ch10ex02-0.jpg)'
- en: '![](ch10ex02-1.jpg)'
  id: totrans-816
  prefs: []
  type: TYPE_IMG
  zh: '![图片ch10ex02-1](ch10ex02-1.jpg)'
- en: '[Figure 10.2](#ch10fig02) shows that many domains have a small number of impressions,
    and a few have large numbers of impressions. So that you can see the distribution
    graphically, we plotted the base 10 log rather than the raw frequencies (we use
    base 10 so you can think of the x-axis as 10⁰, 10¹, 10²...).'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.2](#ch10fig02)显示，许多域的印象数量很少，而少数域的印象数量很多。为了你能直观地看到分布图，我们绘制了以10为底的对数而不是原始频率（我们使用10为底，这样你可以将x轴视为10⁰、10¹、10²...）。'
- en: Figure 10.2\. The histogram of impression data shows that the distribution of
    the number of impressions over publisher domains is heavily skewed.
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2. 印象数据的直方图显示，出版域中印象数量的分布严重偏斜。
- en: '![](10fig02.jpg)'
  id: totrans-819
  prefs: []
  type: TYPE_IMG
  zh: '![图片10fig02](10fig02.jpg)'
- en: Perhaps most significantly, you can see that clicks are relatively rare, only
    0.12%, or 0.0012\. This is a respectable overall click-through rate. But for this
    example, you need large datasets in order to have enough target examples to build
    your model. This isn’t unusual. We’re often trying to predict relatively rare
    phenomena. The capacity to process huge datasets by using big-data technologies
    has made it possible to apply machine learning to many whole new classes of problems.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最显著的是，你可以看到点击率相对较低，仅为0.12%，或者说0.0012。这是一个令人尊重的整体点击率。但在这个例子中，你需要大量的数据集，以便有足够的目标示例来构建你的模型。这并不罕见。我们经常试图预测相对罕见的现象。通过使用大数据技术处理大量数据集的能力，使得将机器学习应用于许多全新的问题类别成为可能。
- en: Similarly, impression frequency by `user_id` is highly skewed. An average user
    has 2.46 impressions, but the median is 1, so a few heavy hitters pull the mean
    higher.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，按`user_id`计算的印象频率高度倾斜。平均用户有2.46次印象，但中位数是1，所以少数几个高点击量将平均值拉高。
- en: 10.5\. Singular value decomposition
  id: totrans-822
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5. 奇异值分解
- en: '[Chapters 3](kindle_split_013.html#ch03) and [7](kindle_split_018.html#ch07)
    mentioned principal component analysis, or PCA, an unsupervised ML technique often
    used to reduce dimensions and extract features. If you look at each user as a
    feature of the publications they’ve interacted with, you have approximately 3.6
    million features per publication, 150 billion values for your exploratory sample
    of data. Obviously, you’d like to work with fewer features, and fortunately you
    can do so fairly easily.'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章和第7章提到了主成分分析，或PCA，这是一种无监督的机器学习技术，常用于降维和提取特征。如果你将每个用户视为他们互动的出版物的一个特征，那么每个出版物大约有360万个特征，你的数据探索样本有1500亿个值。显然，你希望使用更少的特征，幸运的是，你可以相当容易地做到这一点。
- en: As it turns out, PCA has several algorithms, one of which is *singular value
    decomposition*, or SVD. You can explain and interpret SVD mathematically in various
    ways, and mathematicians will recognize that our explanation here leaves out some
    of the beauty of the underlying linear algebra. Fortunately, like the latent semantic
    analysis covered in [chapter 7](kindle_split_018.html#ch07), SVD has an excellent
    implementation in the scikit-learn Python library. But this time, let’s do just
    a little bit of the matrix algebra. If you’ve done matrix multiplication, you
    know that dimensions are important. If A[[n x p]] denotes an n-by-p matrix, you
    can multiple A by another matrix whose dimensions are p by q (for example, B[[p
    x q]]), and the result will have dimensions of n by q (say, C[[n x q]]). It turns
    out that any matrix can be factored into three components, called the left and
    right singular vectors and the singular values, respectively.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，PCA有几个算法，其中之一是*奇异值分解*，或称为SVD。你可以用各种方式从数学上解释和解释SVD，数学家会认识到我们在这里的解释忽略了底层线性代数的一些美感。幸运的是，就像在第7章中介绍的潜在语义分析一样，SVD在scikit-learn
    Python库中有一个出色的实现。但这次，让我们只做一点矩阵代数。如果你做过矩阵乘法，你就会知道维度很重要。如果A[[n x p]]表示一个n行p列的矩阵，你可以将A与另一个维度为p行q列的矩阵相乘（例如，B[[p
    x q]]），结果将具有n行q列的维度（例如，C[[n x q]]）。实际上，任何矩阵都可以分解为三个组成部分，分别称为左奇异向量、右奇异向量和奇异值。
- en: 'In this example, n is the number of users, each of which is represented by
    a row in matrix A, and p is the number of pubs, each of which is represented by
    a column:'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，n是用户数量，每个用户由矩阵A中的一行表示，p是出版物数量，每个出版物由矩阵A中的一列表示：
- en: '![](220fig01.jpg)'
  id: totrans-826
  prefs: []
  type: TYPE_IMG
  zh: '![图片](220fig01.jpg)'
- en: What makes this interesting is that the singular values tell you something about
    the importance of the features represented by the left and right singular vectors
    (the vectors are the rows of U and V^T). In particular, the singular values tell
    you the extent to which the corresponding feature vectors are independent. Consider
    the implication of interdependent or *covariant* features. Or to make it a bit
    easier, imagine that two features, A and B, are identical. After feature A has
    been considered by the model, feature B has nothing to contribute. It contains
    no new information. As builders of predictive models, the features you want are
    independent, and each one is at least a weak predictor of your target. If you
    have many weak predictors, so long as their predictions are better than random,
    in combination they gain strength. But this phenomenon, *the ensemble effect*,
    works only when features are independent.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 使这变得有趣的是，奇异值告诉你关于由左右奇异向量（这些向量是U和V^T的行）表示的特征的重要性。特别是，奇异值告诉你相应的特征向量独立到什么程度。考虑相互依赖或*协变*特征的含义。或者，为了使它更容易理解，想象两个特征，A和B，是相同的。在模型考虑了特征A之后，特征B就没什么可以贡献的了。它不包含任何新信息。作为预测模型的构建者，你想要的特征是独立的，并且每个特征至少是目标的一个弱预测器。如果你有很多弱预测器，只要它们的预测比随机预测好，结合起来它们就会增强力量。但这种现象，*集成效应*，只有在特征独立时才会起作用。
- en: Let’s run SVD on our advertising data and have a look at the resulting singular
    values.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的广告数据上运行SVD，并查看产生的奇异值。
- en: Listing 10.3\. SVD on advertising data
  id: totrans-829
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3\. 广告数据的SVD
- en: '![](221fig01_alt.jpg)'
  id: totrans-830
  prefs: []
  type: TYPE_IMG
  zh: '![图片](221fig01_alt.jpg)'
- en: When you ran SVD, you used the k = *maximum singular values* parameter to limit
    the calculation to the 1,550 largest singular values. [Figure 10.3](#ch10fig03)
    shows their magnitude; you can see that there are about 1,425 nonzero values,
    and that beyond the 450 most independent feature vectors, the rest are highly
    covariant. This isn’t surprising. Although there are over 3 million users, remember
    that most of them interact with very few pubs. Consider that of these, 136,000
    were observed exactly once (on ebay.com, by the way). So if each user vector is
    a feature of the pub, ebay.com has 136,000 features that are identical.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行SVD时，你使用了k = *最大奇异值*参数来限制计算只到1,550个最大的奇异值。[图10.3](#ch10fig03)显示了它们的幅度；你可以看到大约有1,425个非零值，并且超过450个最独立的特征向量之外，其余的都是高度协变的。这并不奇怪。尽管有超过300万用户，但请记住，他们中的大多数只与非常少的pub互动。考虑到这些，其中13.6万个用户（顺便说一句，在ebay.com）只被观察过一次。所以如果每个用户向量是pub的一个特征，ebay.com就有13.6万个相同的特征。
- en: Figure 10.3\. Singular values for advertising data
  id: totrans-832
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3\. 广告数据的奇异值
- en: '![](10fig03_alt.jpg)'
  id: totrans-833
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig03_alt.jpg)'
- en: Our SVD reduced more than 3 million features to around 7 thousand, a 400:1 reduction.
    Knowing this, you have a much better sense of the resources that will be needed.
    In the next section, you’ll look at ways to size and optimize the resources necessary
    to train your models.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SVD将超过300万个特征减少到大约7千个，减少了400:1。了解这一点后，你对所需的资源有了更好的感觉。在下一节中，你将了解如何确定和优化训练模型所需的资源。
- en: 10.6\. Resource estimation and optimization
  id: totrans-835
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6\. 资源估计和优化
- en: So far, you’ve looked at the cardinalities and distributions that characterize
    your data and done some feature engineering. In this section, you’ll assess the
    task at hand in terms of the computational workload relative to the resources
    you have at your disposal.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经查看了你数据的基数和分布，并进行了某些特征工程。在本节中，你将根据你拥有的资源相对于计算工作量的任务进行评估。
- en: To estimate resource requirements, you need to start with some measurements.
    First let’s look at your available resources. So far, you’ve been using a single
    m4.2xlarge Amazon EC2 instance. Let’s decode that quickly. EC2 is Amazon’s *Elastic
    Compute Cloud*. Each instance is a virtual server with dedicated CPU, random access
    memory (RAM), and disk or solid-state online storage. The *m4.2xlarge* designation
    means a server with eight cores and 32 GB of memory. Disk space is provisioned
    separately. Our single instance has 1 terabyte of *elastic block storage (EBS)*.
    EBS is virtualized storage, set up so that it appears that your instance has a
    dedicated 1 TB disk volume. You’ve set up your instance to run Linux. Depending
    on your needs, you can easily upgrade your single instance to add cores or memory,
    or you can provision more instances.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计资源需求，你需要从一些测量开始。首先让我们看看你的可用资源。到目前为止，你一直在使用单个m4.2xlarge亚马逊EC2实例。让我们快速解码一下。EC2是亚马逊的弹性计算云。每个实例都是一个具有专用CPU、随机存取内存（RAM）和磁盘或固态在线存储的虚拟服务器。"m4.2xlarge"标识符意味着一个具有八个核心和32GB内存的服务器。磁盘空间是单独配置的。我们的单个实例配备了1TB的弹性块存储（EBS）。EBS是虚拟化存储，设置为看起来你的实例有一个专用的1TB磁盘卷。你已经设置了你的实例以运行Linux。根据你的需求，你可以轻松地将单个实例升级以添加核心或内存，或者你可以配置更多的实例。
- en: Next, let’s have a look at your workload. Your raw data resides in transaction
    files on Amazon’s *Simple Storage Service,* S3, which is designed to store large
    quantities of data inexpensively. But access is a lot slower than a local disk
    file. Each file contains around 1 million records. You can read approximately
    30,000 records per second from S3, so if you process them one at a time, 10 billion
    will take about 92 hours. Downloading from S3 can be speeded up by around 75%,
    by processing multiple downloads in parallel (on a single instance), so that gets
    you down to 23 hours.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看你的工作负载。你的原始数据存储在亚马逊的简单存储服务（S3）的交易文件中，S3旨在以低廉的价格存储大量数据。但访问速度比本地磁盘文件慢得多。每个文件大约包含100万条记录。你可以从S3中每秒读取大约30,000条记录，所以如果你逐个处理它们，10亿条将需要大约92小时。通过并行处理多个下载（在单个实例上），可以从S3中加速下载，大约可以提高75%，这样可以将时间缩短到23小时。
- en: But speed isn’t your only problem. Based on your earlier observation that 10
    million records loaded into memory consume 53% of your 32 GB of memory, it would
    take 1.7 terabytes of memory to load your entire dataset. Even if you could afford
    it, Amazon doesn’t have an instance with that much RAM.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 但速度并不是你唯一的问题。根据你之前的观察，将1000万条记录加载到内存中会消耗你32GB内存的53%，要加载整个数据集需要1.7TB的内存。即使你能负担得起，亚马逊也没有那么多RAM的实例。
- en: Fortunately, you don’t need all the data in memory. Furthermore, your requirement
    isn’t just a function of the size of the data, but of its shape—by which we mean
    the cardinality of its primary keys. It turns out that there are 10 billion records,
    but only about 10 million users and around 300 thousand pubs, which means the
    user/pub matrix is around 3 trillion entries. But when you populated your sparse
    matrix, there were values in only about 0.01% of the cells, so 3 trillion is reduced
    to 300 million. Assuming one 64-bit floating-point number per value, your user/pub
    matrix will fit in about 2.5 of your 32 GB.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你不需要将所有数据都存储在内存中。此外，你的需求并不仅仅取决于数据的大小，还取决于其形状——我们指的是其主键的基数。结果显示，有100亿条记录，但只有大约1000万用户和大约30万个出版物，这意味着用户/出版物矩阵大约有3万亿条条目。但是，当你填充你的稀疏矩阵时，只有大约0.01%的单元格中有值，所以3万亿减少到3亿。假设每个值有一个64位的浮点数，你的用户/出版物矩阵将大约占用你32GB中的2.5GB。
- en: To cut processing time, you need to look at doing things in parallel. [Figure
    10.4](#ch10fig04) illustrates using worker nodes (additional EC2 instances, in
    this case) to ingest the raw data in parallel.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少处理时间，你需要考虑并行处理。图10.4[图](#ch10fig04)说明了使用工作节点（额外的EC2实例，在这种情况下）并行获取原始数据。
- en: Figure 10.4\. Parallel processing scales the initial data acquisition.
  id: totrans-842
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4。并行处理扩展了初始数据获取。
- en: '![](10fig04_alt.jpg)'
  id: totrans-843
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig04_alt.jpg)'
- en: The worker nodes do more than read the data from S3\. Each one independently
    builds a sparse matrix of users and items. When all the workers are finished with
    their jobs, these are combined by your compute node.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点不仅从S3读取数据。每个节点独立构建用户和物品的稀疏矩阵。当所有工作节点完成他们的工作后，这些矩阵将由你的计算节点组合。
- en: '[Chapter 9](kindle_split_020.html#ch09) described some big-data technologies:
    Hadoop, MapReduce, and Apache Spark. The processes described here are a highly
    simplified version of what happens in a MapReduce job. A large task is broken
    into small units, each of which is dispatched (mapped) to a worker. As workers
    complete their subtasks, the results are combined (reduced), and that result is
    returned to the requestor. Hadoop optimizes this process in several ways. First,
    rather than having the workers retrieve data over a network, each worker node
    stores part of the data locally. Hadoop optimizes the assignment of tasks so that
    whenever possible, each node works on data that’s already on a local volume. Spark
    goes one step further by having the worker nodes load the data into memory so
    they don’t need to do any I/O operations in order to process the tasks they’re
    assigned.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](kindle_split_020.html#ch09)描述了一些大数据技术：Hadoop、MapReduce和Apache Spark。这里描述的过程是MapReduce作业中发生的事情的高度简化版本。一个大任务被分解成小单元，每个单元被分配（映射）到工作节点。当工作节点完成它们的子任务时，结果被合并（减少），并将结果返回给请求者。Hadoop通过几种方式优化了这个过程。首先，而不是让工作节点通过网络检索数据，每个工作节点将部分数据存储在本地。Hadoop优化任务分配，以便尽可能让每个节点处理已经存储在本地卷上的数据。Spark更进一步，让工作节点将数据加载到内存中，这样它们在处理分配的任务时就不需要进行任何I/O操作。'
- en: Although this example problem is large enough to require a little parallel processing,
    it’s probably not worth the effort required to implement one of these frameworks.
    You need to run your entire workflow only once per day, and you could easily add
    a few more instances and get the whole process down to an hour or less. But you
    can easily imagine an application requiring you to run a variety of processes
    at a greater frequency, where having the worker nodes retain the raw data in memory
    over the course of many processing cycles would boost performance by orders of
    magnitude.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个示例问题足够大，需要一点并行处理，但实现这些框架之一所付出的努力可能并不值得。你每天只需要运行整个工作流程一次，你很容易增加几个实例，并将整个过程缩短到一小时或更少。但你可以很容易地想象一个需要你以更高频率运行各种过程的应用程序，其中工作节点在多个处理周期中保留原始数据在内存中，将性能提升几个数量级。
- en: 10.7\. Modeling
  id: totrans-847
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7. 模型构建
- en: Your goal for the model is to predict CTR for each pub. You started with user
    interactions as features and used SVD to reduce the feature space. From here,
    there are several approaches to making predictions. Your first model will be a
    *k-nearest neighbors* (KNN) model. This is a simple but surprisingly effective
    recommender model.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是预测每个出版物的点击率（CTR）。你从用户交互作为特征开始，并使用奇异值分解（SVD）来降低特征空间。从这里，有几种方法可以做出预测。你的第一个模型将是一个
    *k-最近邻算法*（KNN）模型。这是一个简单但出奇有效的推荐模型。
- en: You’ll also train a *random forest regressor*. Random forests are a form of
    decision-tree-based learning; many random samples of data and random subsets of
    the feature set are selected, and decision trees are constructed for each selection.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将训练一个 *随机森林回归器*。随机森林是一种基于决策树的机器学习方法；它选择了数据的多组随机样本和特征集的随机子集，并为每个选择构建决策树。
- en: 10.8\. K-nearest neighbors
  id: totrans-850
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.8. K-最近邻算法
- en: '[Figure 10.5](#ch10fig05) shows simplified user/item and dissimilarity matrices.
    Notice that the diagonal of the dissimilarity matrix is all zeros because each
    pub’s user vector (column in the user/item matrix) is identical to itself, and
    therefore zero distance from itself. You can see that the distance between pub3,
    pub4, and pub7 is zero, as you’d expect, because their respective columns in the
    user/item matrix are identical. Also note that pub1’s distance to pub5 is the
    same as pub5’s distance to pub1\. In other words, dissimilarity is symmetric.
    Interestingly, some recommender algorithms don’t define distance symmetrically.
    Item A may be like item B, but item B isn’t like item A.'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.5](#ch10fig05)显示了简化的用户/项目和相似度矩阵。注意，相似度矩阵的对角线都是零，因为每个出版物的用户向量（用户/项目矩阵中的列）与其自身相同，因此与自身之间的距离为零。你可以看到pub3、pub4和pub7之间的距离为零，正如你所期望的，因为它们在用户/项目矩阵中的相应列是相同的。此外，请注意pub1与pub5的距离与pub5与pub1的距离相同。换句话说，不相似性是对称的。有趣的是，一些推荐算法没有定义对称的距离。项目A可能类似于项目B，但项目B可能不类似于项目A。'
- en: Figure 10.5\. The dissimilarity, or distance, matrix shows the extent to which
    user interactions are similar or different. In this example, the user/item matrix
    is binary, indicating whether the user has interacted with the pub.
  id: totrans-852
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5\. 相似度，或距离，矩阵显示了用户交互的相似或不同程度。在这个例子中，用户/项目矩阵是二元的，表示用户是否与pub互动。
- en: '![](10fig05.jpg)'
  id: totrans-853
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig05.jpg)'
- en: You compute the similarity (actually, *dis*similarity, or distance) between
    each pair of pubs, using one of several available measures. You then choose the
    most common, the *Euclidean distance*.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用几种可用的度量之一计算每对pub之间的相似度（实际上，*dis*similarity，或距离）。然后你选择最常见的方法，即 *欧几里得距离*。
- en: After you’ve computed pairwise distances, the next step is to compute your predicted
    CTR for each pub. In KNN, the predicted target value is calculated by averaging
    the values of the target values for k-nearest neighbors, presuming that each example
    observation will be most similar to its nearest neighbors. There are several important
    questions at this juncture. First, what should you choose for the value of k?
    How many neighbors should be considered? Also, it’s common to give greater weight
    to the closest neighbors, usually by weighting the calculation of the mean target
    value by 1/*distance* or [1/*distance*]².
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完成对距离之后，下一步是计算每个pub的预测CTR。在KNN中，预测的目标值是通过计算k个最近邻的目标值的平均值来得到的，假设每个示例观测值将与其最近的邻居最相似。在这个阶段有几个重要的问题。首先，你应该选择k的什么值？应该考虑多少个邻居？此外，通常会给最近的邻居更大的权重，通常是通过将平均目标值的计算加权为1/*距离*或[1/*距离*]²。
- en: '[Listing 10.4](#ch10ex04) shows a calculation of predicted values for a range
    of possible values of k by using scikit-learn `NearestNeighbors`. Here you try
    three weighting formulas, each of 20 values of k. [Figure 10.6](#ch10fig06) shows
    that the best predictors are one or two nearest neighbors, and averaging over
    a larger range offers no real improvement. This is probably because our data is
    sparse, and nearest neighbors are often fairly distant. Note that the variation
    over the values of k is also small. In any case, the normalized RMSE for our test
    set predictions is in the range of 5%. Not bad!'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表10.4](#ch10ex04)展示了使用scikit-learn `NearestNeighbors`计算一系列可能的k值预测值的计算。在这里，你尝试了三种加权公式，每个公式有20个k值。[图10.6](#ch10fig06)显示，最佳预测者是一个或两个最近邻，对更大范围的平均并没有真正的改进。这可能是由于我们的数据稀疏，最近邻通常相当遥远。注意，k值的变异性也较小。无论如何，我们的测试集预测的标准化RMSE在5%的范围内。还不错！'
- en: Figure 10.6\. RMSE for three weighting functions and values of k = 1 to k =
    30
  id: totrans-857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6\. 三种加权函数和k = 1到k = 30的RMSE值
- en: '![](10fig06_alt.jpg)'
  id: totrans-858
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig06_alt.jpg)'
- en: Listing 10.4\. KNN predictions
  id: totrans-859
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表10.4\. KNN预测
- en: '![](ch10ex04-0.jpg)'
  id: totrans-860
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex04-0.jpg)'
- en: '![](ch10ex04-1.jpg)'
  id: totrans-861
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex04-1.jpg)'
- en: 10.9\. Random forests
  id: totrans-862
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.9\. 随机森林
- en: In the training phase of random forests, data is sampled repeatedly, with replacement,
    in a process called *bagging*, sometimes called *bootstrap aggregating*. For each
    sample, a decision tree is constructed using a randomly selected subset of the
    features. To make predictions on unseen data, each decision tree is evaluated
    independently, and the results are averaged (for regression) or each tree “votes”
    for classification. For many applications, random forests may be outperformed
    by other algorithms such as boosted trees or support vector machines, but random
    forests have the advantages that they’re easy to apply, their results are easy
    to interpret and understand, and the training of many trees is easily parallelized.
    Once again, you’ll use scikit-learn; see [figure 10.7](#ch10fig07).
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林的训练阶段，数据会通过一个称为 *bagging* 的过程（有时也称为 *bootstrap aggregating*）进行重复采样，并且是有放回的。对于每个样本，使用随机选择的特征子集构建一个决策树。为了对未见过的数据进行预测，每个决策树都是独立评估的，并且结果会被平均（对于回归）或者每个树对分类进行“投票”。对于许多应用来说，随机森林可能不如其他算法（如提升树或支持向量机）表现好，但随机森林有易于应用、结果易于解释和理解、以及多棵树的训练易于并行化的优点。再次强调，你将使用
    scikit-learn；参见[图10.7](#ch10fig07)。
- en: Figure 10.7\. Variable importance for the random forest regression
  id: totrans-864
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7\. 随机森林回归的变量重要性
- en: '![](10fig07_alt.jpg)'
  id: totrans-865
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig07_alt.jpg)'
- en: Listing 10.5\. Random forest regression
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表10.5\. 随机森林回归
- en: '![](ch10ex05-0.jpg)'
  id: totrans-867
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex05-0.jpg)'
- en: '![](ch10ex05-1.jpg)'
  id: totrans-868
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex05-1.jpg)'
- en: The optimized random forest regression provides a useful prediction of CTR,
    but it’s not as good as the KNN prediction. Your next steps might be to explore
    ways to combine these, and possibly other, models. Methods that combine models
    in this way are called *ensemble methods*. Random forests are, in their own right,
    an ensemble method, as bagging is a way of generating multiple models. To combine
    entirely different models such as the two in this example, you might employ *stacking*,
    or stacked generalization, in which the predictions from multiple models become
    features that are combined by training and prediction using yet another ML model,
    usually logistic regression.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的随机森林回归提供了有用的点击率预测，但它不如KNN预测好。你的下一步可能就是探索将这些模型以及其他模型结合起来的方法。以这种方式结合模型的方法被称为*集成方法*。随机森林本身就是一种集成方法，因为袋装是一种生成多个模型的方式。为了结合完全不同的模型，例如本例中的两个模型，你可能需要采用*堆叠*或堆叠泛化，在这种方法中，多个模型的预测成为由另一个机器学习模型（通常是逻辑回归）训练和预测时结合的特征。
- en: 10.10\. Other real-world considerations
  id: totrans-870
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.10\. 其他现实世界的考虑因素
- en: 'You looked at the real-world issues that come with big data: high dimensionality,
    computing resources, storage, and network data transfer constraints. As we mentioned
    briefly, the entire process may be replicated for several species of digital ads:
    mobile, video, and native. Real-time bidding and user-level personalization have
    an entirely different set of concerns. The data at your disposal may vary widely
    from one program to the next, and the models that work perfectly in one situation
    may fail entirely for another.'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 你考虑了大数据带来的现实世界问题：高维度、计算资源、存储和网络数据传输限制。正如我们简要提到的，整个过程可能适用于几种数字广告类型：移动、视频和原生。实时竞价和用户级个性化有一套完全不同的关注点。你拥有的数据可能因程序而异，而在一种情况下完美工作的模型可能在另一种情况下完全失败。
- en: In our example, we had a large historical dataset to start with. But our recommender-like
    approach has an issue known as the *cold-start problem*. When a new user or a
    new product enters the system with no history to rely on, you have no basis for
    building associations. For our purposes, a few unknowns don’t matter, but when
    a new campaign starts from scratch, you have no history at all to work with. Models
    built on the basis of other similar campaigns may or may not be effective.
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们一开始有一个庞大的历史数据集。但我们的推荐系统方法存在一个被称为*冷启动问题*的问题。当一个新用户或新产品进入系统而没有历史记录可依赖时，你没有任何基础来建立关联。对我们来说，一些未知因素并不重要，但当一个新的活动从头开始时，你完全没有历史记录可以工作。基于其他类似活动的模型构建的模型可能有效也可能无效。
- en: In the real world, there’s a great advantage to having a variety of tools and
    models that can be employed. The larger and more complex the environment, the
    greater the benefit of having such a suite of feature-building, data-reduction,
    training, prediction, and assessment tools well organized and built into a coherent
    automated workflow.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，拥有各种可以使用的工具和模型具有很大的优势。环境越大、越复杂，拥有这样一套特征构建、数据减少、训练、预测和评估工具的好处就越大，这些工具应该组织良好并集成到一个连贯的自动化工作流程中。
- en: Advertising is a great example of a business in which externalities may diminish
    the effectiveness of your predictive models. As technology and business practices
    change, behaviors change. The growth of mobile devices has changed the digital
    landscape dramatically. Real-time bidding completely changes the level on which
    you apply optimization. New forms of fraud, ad blockers, new browsers, and new
    web technology all change the dynamics that you’re modeling. In the real world,
    models are built, tested, deployed, measured, rebuilt, retested, redeployed, and
    measured again.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 广告是一个很好的例子，说明外部性可能会降低你的预测模型的有效性。随着技术和商业实践的变革，行为也在改变。移动设备的增长极大地改变了数字景观。实时竞价完全改变了你应用优化的层级。新的欺诈形式、广告拦截器、新的浏览器和新的网络技术都改变了你正在建模的动态。在现实世界中，模型被构建、测试、部署、衡量、重建、重新测试、重新部署，并再次衡量。
- en: Digital advertising is a multibillion-dollar business, and for the brands that
    rely on it, optimizations that reduce wasted expenditures, even a little, can
    have a significant return on investment. Each wasted impression you can eliminate
    saves money, but when replaced with one that results in gaining a customer, the
    benefit will be far greater than the cost savings—and will more than justify the
    effort to overcome the many challenges of this dynamic business.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 数字广告是一个价值数十亿美元的业务，对于依赖它的品牌来说，即使是减少一点浪费支出，也能带来显著的回报。你可以消除的每一处浪费印象都能节省金钱，但当你用能带来客户获取的印象替换时，其好处将远远大于成本节省，并且将远远超过克服这个动态业务众多挑战的努力。
- en: 10.11\. Summary
  id: totrans-876
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.11\. 摘要
- en: 'This chapter covered elements of a real-world machine-learning problem somewhat
    more broadly than just choosing algorithms, training, and testing models. Although
    these are the heart of the discipline of machine learning, their success often
    depends on surrounding practicalities and trade-offs. Here are some of the key
    points from this chapter’s example:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不仅涵盖了选择算法、训练和测试模型等机器学习问题的要素，而且比仅仅选择算法、训练和测试模型更为广泛。尽管这些是机器学习学科的核心，但它们的成功往往取决于周围的实践性和权衡。以下是本章示例中的关键点：
- en: The first step is always to understand the business or activity you’re modeling,
    its objectives, and how they’re measured. It’s also important to consider how
    your predictions can be acted on—to anticipate what adjustments or optimizations
    can be made based on the insight you deliver.
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步始终是理解你正在建模的业务或活动，其目标以及如何衡量它们。考虑你的预测如何被采取行动也很重要——预测基于你提供的洞察力可以做出哪些调整或优化。
- en: Different feature-engineering strategies may yield very different working datasets.
    Casting a wide net and considering a range of possibilities can be beneficial.
    In the first model, you expanded the feature set vastly and then reduced it using
    SVD. In the second, you used simple aggregations. Which approach works best depends
    on the problem and the data.
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的特征工程策略可能会产生非常不同的工作数据集。撒大网并考虑多种可能性是有益的。在第一个模型中，你大大扩展了特征集，然后使用奇异值分解（SVD）进行缩减。在第二个模型中，你使用了简单的聚合。哪种方法最有效取决于问题和数据。
- en: After exploring a subsample of data, you were able to estimate the computing
    resources needed to perform your analyses. In our example, the bottleneck wasn’t
    the ML algorithms themselves, but rather the collection and aggregation of raw
    data into a form suitable for modeling. This isn’t unusual, and it’s important
    to consider both prerequisite and downstream workflow tasks when you consider
    resource needs.
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在探索数据子样本之后，你能够估计执行分析所需的计算资源。在我们的例子中，瓶颈不是机器学习算法本身，而是将原始数据收集和汇总成适合建模的形式。这种情况并不少见，当你考虑资源需求时，考虑先决条件和下游工作流程任务同样重要。
- en: Often, the best model isn’t a single model, but an ensemble of models, the predictions
    of which are aggregated by yet another predictive model. In many real-world problems,
    practical trade-offs exist between the best possible ensembles and the practicality
    of creating, operating, and maintaining complex workflows.
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，最好的模型不是一个单一的模型，而是一系列模型，这些模型的预测由另一个预测模型汇总。在许多现实世界的问题中，最佳可能的集成与创建、操作和维护复杂工作流程的实用性之间存在实际权衡。
- en: In the real world, there are often a few, and sometimes many, variations on
    the problem at hand. We discussed some of these for advertising, and they’re common
    in any complex discipline.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界中，往往只有少数，有时很多，对当前问题的变体。我们讨论了其中一些用于广告的变体，它们在任何复杂学科中都很常见。
- en: The underlying dynamics of the phenomena you model often aren’t constant. Business,
    markets, behaviors, and conditions change. When you use ML models in the real
    world, you must constantly monitor their performance and sometimes go back to
    the drawing board.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你所建模的现象的潜在动态通常不是恒定的。商业、市场、行为和条件都在变化。当你在现实世界中使用机器学习模型时，你必须不断监控它们的性能，有时甚至需要回到起点重新设计。
- en: 10.12\. Terms from this chapter
  id: totrans-884
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.12\. 本章术语
- en: '| Word | Definition |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| recommender | A class of ML algorithms used to predict users’ affinities
    for various items. |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| 推荐算法 | 一类用于预测用户对各种物品亲和力的机器学习算法。 |'
- en: '| collaborative filtering | Recommender algorithms that work by characterizing
    users via their item preferences, and items by the preferences of common users.
    |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '| 协同过滤 | 通过描述用户的物品偏好和物品的常见用户偏好来工作的推荐算法。 |'
- en: '| ensemble method | An ML strategy in which multiple models’ independent predictions
    are combined. |'
  id: totrans-889
  prefs: []
  type: TYPE_TB
  zh: '| 集成方法 | 一种机器学习策略，其中多个模型的独立预测被组合在一起。|'
- en: '| ensemble effect | The tendency of multiple combined models to yield better
    predictive performance than the individual components. |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '| 集成效应 | 多个组合模型倾向于产生比单个组件更好的预测性能的趋势。|'
- en: '| k-nearest neighbors | An algorithm that bases predictions on the nearest
    observations in the training space. |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
  zh: '| k最近邻 | 一种基于训练空间中最近观察值的预测算法。|'
- en: '| Euclidean distance | One of many ways of measuring distances in feature space.
    In two-dimensional space, it’s the familiar distance formula. |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '| 欧几里得距离 | 在特征空间中测量距离的许多方法之一。在二维空间中，它是熟悉的距离公式。|'
- en: '| random forest | An ensemble learning method that fits multiple decision tree
    classifiers or regressors to subsets of the training data and features and makes
    predictions based on the combined model. |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 一种集成学习方法，它将多个决策树分类器或回归器拟合到训练数据的子集和特征中，并根据组合模型进行预测。|'
- en: '| bagging | The process of repeated sampling with replacement used by random
    forests and other algorithms. |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
  zh: '| 折叠法 | 随机森林和其他算法使用的重复有放回抽样的过程。|'
- en: '| stacking | Use of a machine-learning algorithm, often logistic regression,
    to combine the predictions of other algorithms to create a final “consensus” prediction.
    |'
  id: totrans-895
  prefs: []
  type: TYPE_TB
  zh: '| 堆叠 | 使用机器学习算法（通常是逻辑回归）结合其他算法的预测，以创建最终的“共识”预测。|'
- en: 10.13\. Recap and conclusion
  id: totrans-896
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.13. 回顾与结论
- en: 'The first goal in writing this book was to explain machine learning as it’s
    practiced in the real world, in an understandable and interesting way. Another
    was to enable you to recognize when machine learning can solve your real-world
    problems. Here are some of the key points:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 写这本书的第一个目标是以一种可理解且有趣的方式解释现实世界中实践中的机器学习。另一个目标是让你能够识别出机器学习何时可以解决你的现实世界问题。以下是一些关键点：
- en: Machine-learning methods are truly superior for certain data-driven problems.
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习方法在解决某些数据驱动问题方面确实具有优势。
- en: A basic machine-learning workflow includes data preparation, model building,
    model evaluation, optimization, and prediction.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的机器学习工作流程包括数据准备、模型构建、模型评估、优化和预测。
- en: Data preparation includes ensuring that a sufficient quantity of the right data
    has been collected, visualizing the data, exploring the data, dealing with missing
    data, recoding categorical features, performing feature engineering, and always
    watching out for bias.
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备包括确保收集了足够数量的正确数据，可视化数据，探索数据，处理缺失数据，重新编码分类特征，执行特征工程，并始终注意偏差。
- en: Machine learning uses many models. Broad classes are linear and nonlinear, parametric
    and nonparametric, supervised and unsupervised, and classification and regression.
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习使用了许多模型。广泛的类别包括线性和非线性、参数和非参数、监督和非监督，以及分类和回归。
- en: Model evaluation and optimization involves iterative cross-validation, performance
    measurement, and parameter tuning.
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估和优化涉及迭代交叉验证、性能测量和参数调整。
- en: Feature engineering enables application of domain knowledge and use of unstructured
    data. It can often improve the performance of models dramatically.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程使得应用领域知识和使用非结构化数据成为可能。它通常可以显著提高模型的性能。
- en: Scale isn’t just about big data. It involves the partitioning of work, the rate
    at which new data is ingested, training time, and prediction time, all in the
    context of business or mission requirements.
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模不仅仅是关于大数据。它涉及工作划分、新数据摄入速率、训练时间和预测时间，所有这些都在业务或任务需求背景下进行。
- en: The mathematics and computer science of machine learning have been with us for
    50 years, but until recently they were confined to academia and a few esoteric
    applications. The growth of giant internet companies and the propagation of data
    as the world has gone online have opened the floodgates. Businesses, governments,
    and researchers are discovering and developing new applications for machine learning
    every day. This book is primarily about these applications, with just enough of
    the foundational mathematics and computer science to explain not just *what* practitioners
    do, but *how* they do it. We’ve emphasized the essential techniques and processes
    that apply regardless of the algorithms, scale, or application. We hope we’ve
    helped to demystify machine learning and in so doing helped to advance its use
    to solve important problems.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的数学和计算机科学已经伴随着我们50年，但直到最近，它们都局限于学术界和一些神秘的应用。随着大型互联网公司的增长和世界上网的数据传播，大门已经打开。企业、政府和研究人员每天都在发现和发展机器学习的新应用。这本书主要关于这些应用，只包含足够的基础数学和计算机科学知识，不仅解释了从业者做什么，还解释了他们是如何做的。我们强调了无论算法、规模或应用如何，都适用的基本技术和流程。我们希望我们已经帮助消除了机器学习的神秘感，并在这样做的同时，帮助推进了其解决重要问题的应用。
- en: Progress comes in waves. The computer automation wave changed our institutions.
    The internet tidal wave changed our lives and our culture. There are good reasons
    to expect that today’s machine learning is but a preview of the next wave. Will
    it be a predictable rising tide, a rogue wave, or a tsunami? It’s too soon to
    say, but adoption isn’t just proceeding; it’s accelerating. At the same time,
    advances in machine-learning tools are impressive, to say the least. Computer
    systems are advancing in entirely new ways as we program them to learn progressively
    more-abstract skills. They’re learning to see, hear, speak, translate languages,
    drive our cars, and anticipate our needs and desires for goods, services, knowledge,
    and relationships.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 进步如同波浪般涌现。计算机自动化浪潮改变了我们的机构。互联网的浪潮改变了我们的生活和文化。有充分的理由预期，今天的机器学习只是下一波浪潮的预览。它将是一个可预测的涨潮，还是一场狂风暴雨，或者是一场海啸？现在说还为时尚早，但采用率不仅正在推进，而且正在加速。与此同时，机器学习工具的进步令人印象深刻。我们编程让计算机系统以全新的方式进步，它们正在学习看、听、说话、翻译语言、驾驶我们的汽车，并预测我们对商品、服务、知识和关系的需求和欲望。
- en: Arthur C. Clark said that any sufficiently advanced technology is indistinguishable
    from magic (Clark’s third law). When machine learning was first proposed, it did
    sound like magic. But as it has become more commonplace, we’ve begun to understand
    it as a tool. As we see many examples of its application, we can generalize (in
    the human sense) and imagine other uses without knowing all the details of its
    internal workings. Like other advanced technologies that were once seen as magic,
    machine learning is coming into focus as a natural phenomenon, in the end more
    subtle and beautiful than magic.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·C·克拉克曾说，任何足够先进的技术都与魔法无异（克拉克第三定律）。当机器学习最初被提出时，确实听起来像是魔法。但随着它变得越来越普遍，我们开始将其视为一种工具。当我们看到许多它的应用实例时，我们可以从人类的角度进行概括，并想象其他用途，而无需了解其内部运作的所有细节。像其他曾经被视为魔法的先进技术一样，机器学习正在成为自然现象，最终比魔法更加微妙和美丽。
- en: '|  |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Further reading**'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: For those of you who’d like to learn more about using ML tools in the Python
    language, we recommend *Machine Learning in Action* by Peter Harrington (Manning,
    2012).
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想了解如何在Python语言中使用ML工具的人来说，我们推荐Peter Harrington的《机器学习实战》（Manning，2012年）。
- en: For a deep dive with examples in the R language, consider *Applied Predictive
    Modeling* by Max Kuhn and Kjell Johnson (Springer, 2013).
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在R语言中深入了解示例，可以考虑Max Kuhn和Kjell Johnson合著的《应用预测建模》（Springer，2013年）。
- en: 'Cathy O’Neil describes her and Rachel Schutt’s book, *Doing Data Science: Straight
    Talk from the Frontline* (O’Reilly Media, 2013) as “a course I wish had existed
    when I was in college.” We agree.'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 凯西·奥尼尔描述她和雷切尔·舒特的书籍《做数据科学：前线直言》（O'Reilly Media，2013年）为“我上大学时希望存在的课程。”我们同意。
- en: If you’re interested in the implications of big data and machine learning for
    businesses and society, consider *Big Data, A Revolution That Will Transform How
    We Live, Work, and Think* by Viktor Mayer-Schönberger and Kenneth Cukier (Houghton
    Mifflin Harcourt, 2013).
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣于大数据和机器学习对商业和社会的影响，可以考虑维克托·迈尔-舍恩伯格和肯尼思·库克耶合著的《大数据：一场将改变我们生活、工作和思考方式的革命》（Houghton
    Mifflin Harcourt，2013年）。
- en: 'Online resources include the following:'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 网络资源包括以下内容：
- en: '[www.predictiveanalyticstoday.com](http://www.predictiveanalyticstoday.com)—For
    industry news'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[www.predictiveanalyticstoday.com](http://www.predictiveanalyticstoday.com)—行业新闻'
- en: '[www.analyticbridge.com](http://www.analyticbridge.com) and its parent site,
    [www.datasciencecentral.com](http://www.datasciencecentral.com)'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[www.analyticbridge.com](http://www.analyticbridge.com) 及其母站 [www.datasciencecentral.com](http://www.datasciencecentral.com)'
- en: '[www.analyticsvidhya.com](http://www.analyticsvidhya.com)—Analytics news focused
    on learning'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[www.analyticsvidhya.com](http://www.analyticsvidhya.com)—专注于学习的分析新闻'
- en: '[www.reddit.com/r/machinelearning](http://www.reddit.com/r/machinelearning)—Machine-learning
    discussion'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[www.reddit.com/r/machinelearning](http://www.reddit.com/r/machinelearning)—机器学习讨论'
- en: '[www.kaggle.com](http://www.kaggle.com)—Competitions, community, scripts, job
    board'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[www.kaggle.com](http://www.kaggle.com)—竞赛、社区、脚本、招聘板'
- en: '|  |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
  zh: '|  |'
