- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Clean and prepare
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 清理和准备
- en: '**This chapter covers**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Understanding the types of errors that you might find in your data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解你可能在数据中找到的错误类型
- en: Identifying problems in your data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据中的问题
- en: Implementing strategies for fixing or working around bad data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施修复或绕过不良数据的策略
- en: Preparing your data for effective use in production
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为在生产中有效使用数据做准备
- en: When we’re working with data, it’s crucial that we can trust our data and work
    with it effectively. Almost every data-wrangling project is front-loaded with
    an effort to fix problems and prepare the data for use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理数据时，我们能够信任数据并有效地与之工作至关重要。几乎每个数据整理项目都预先加载了修复问题和准备数据以供使用的工作。
- en: You may have heard that cleanup and preparation equal 80% of the work! I’m not
    sure about that, but certainly preparation is often a large proportion of the
    total work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过清理和准备等于80%的工作！我不确定这一点，但确实准备通常是总工作量的大部分。
- en: 'Time invested at this stage helps save us from later discovering that we’ve
    been working with unreliable or problematic data. If this happens to you, then
    much of your work, understanding, and decisions are likely to be based on faulty
    input. This isn’t a good situation: you must now backtrack and fix those mistakes.
    This is an expensive process, but we can mitigate against this risk by paying
    attention early in the cleanup phase.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段投入的时间可以帮助我们避免在后期发现我们一直在处理不可靠或有问题数据的情况。如果你遇到这种情况，那么你的大部分工作、理解和决策可能都是基于错误输入的。这不是一个好的情况：你现在必须回溯并修复这些错误。这是一个昂贵的过程，但我们可以通过在清理阶段早期注意来减轻这种风险。
- en: In this chapter, we’ll learn how to identify and fix bad data. You’ll see so
    many different ways that data can go wrong, so we can’t hope to look at them all.
    Instead, we’ll look at general strategies for addressing bad data and apply these
    to specific examples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何识别和修复不良数据。你会看到数据出错的不同方式，所以我们无法期望查看所有这些方式。相反，我们将查看处理不良数据的一般策略，并将这些策略应用于具体示例。
- en: 6.1 Expanding our toolkit
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 扩展我们的工具包
- en: In this chapter, we take a closer look at JavaScript and Data-Forge functions
    for slicing, dicing, and transforming your data. We’ll also rely on our toolkit
    from chapter 3, using our `importCsvFile` and `exportCsvFile` to load and save
    CSV files.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地了解JavaScript和Data-Forge函数，用于切片、切块和转换数据。我们还将依赖第3章的工具包，使用`importCsvFile`和`exportCsvFile`来加载和保存CSV文件。
- en: '[Table 6.1](#table6.1) lists the various tools that we cover in this chapter.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.1](#table6.1) 列出了本章中涵盖的各种工具。'
- en: Table 6.1 Tools used in chapter 6
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 第6章使用的工具
- en: '| **API/Library** | **Function/Operator** | **Notes** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **API库** | **函数/操作符** | **说明** |'
- en: '| --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| JavaScript | `Map` | Builds a new array after transforming every element
    of the input array |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| JavaScript | `Map` | 在转换输入数组的每个元素后构建一个新的数组 |'
- en: '|  | `Filter` | Builds a new array filtering out unwanted elements |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | `Filter` | 通过过滤掉不需要的元素构建一个新的数组 |'
- en: '|  | `Concat` | Concatenates two or more arrays into a single array |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | `Concat` | 将两个或多个数组连接成一个数组 |'
- en: '|  | `Delete` | JavaScript operator that deletes a field from a JavaScript
    object |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | `Delete` | JavaScript操作符，用于从JavaScript对象中删除字段 |'
- en: '|  | `Reduce` | Collapses an array to a single value; can be used to aggregate
    or summarize a data set |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | `Reduce` | 将数组折叠成一个单一值；可用于聚合或总结数据集 |'
- en: '| Data-Forge | `select` | Similar to JavaScript `map` function, builds a new
    DataFrame after transforming every row of the input DataFrame |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Data-Forge | `select` | 与JavaScript `map`函数类似，在转换输入DataFrame的每一行后构建一个新的DataFrame
    |'
- en: '|  | `where` | Similar to JavaScript `filter` function, builds a new DataFrame
    filtering out unwanted rows of data |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | `where` | 与JavaScript `filter`函数类似，构建一个新的DataFrame，过滤掉不需要的数据行 |'
- en: '|  | `concat` | Similar to JavaScript `concat` function, concatenates two or
    more DataFrames into a single DataFrame |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | `concat` | 与JavaScript `concat`函数类似，将两个或多个DataFrame连接成一个DataFrame |'
- en: '|  | `dropSeries` | Removes an entire named series from a DataFrame. Use this
    to remove entire columns of data from your data set. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | `dropSeries` | 从DataFrame中删除整个命名序列。使用此功能可以从数据集中删除整个数据列 |'
- en: '|  | `groupBy` | Organizes rows of data into groups by criteria that you specify
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | `groupBy` | 根据你指定的标准将数据行组织成组 |'
- en: '|  | `aggregate` | Similar to the JavaScript `reduce` function, collapses a
    DataFrame to a single value; can be used to aggregate or summarize a data set
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | `aggregate` | 与JavaScript `reduce`函数类似，将DataFrame折叠成一个单一值；可用于聚合或总结数据集
    |'
- en: '| Globby | `globby` | Function used to read the file system and determine which
    files match a particular wildcard. We’ll use this to merge multiple files into
    a single file. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Globby | `globby` | 用于读取文件系统并确定哪些文件与特定通配符匹配的函数。我们将使用它将多个文件合并成一个文件。|'
- en: Our main mental tool here is that of the data pipeline. As we look at the different
    ways we can transform data, keep in mind that we’re working toward building a
    flexible data pipeline. How you structure this, well, that’s up to you, but by
    the end of the chapter, I’ll show you an elegant and flexible way of chaining
    your data transforms using Data-Forge.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的主要心智工具是数据管道。当我们考虑不同的数据转换方式时，请记住我们正在努力构建一个灵活的数据管道。如何构建它，那完全取决于你，但到本章结束时，我会向你展示一种优雅且灵活的方法，使用
    Data-Forge 来串联你的数据转换。
- en: 6.2 Preparing the reef data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 准备珊瑚数据
- en: When we acquire data, it isn’t always going to come in as we’d like it to be.
    Let’s return to our reef data set that we saw in chapters 1 and 2\. We have several
    problems with this data that we might want to fix before we start to use it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们获取数据时，它并不总是以我们希望的方式出现。让我们回到我们在第1章和第2章中看到的珊瑚数据集。我们在使用它之前可能需要修复这个数据集的几个问题。
- en: First, though, let’s work through several of the general issues relating to
    data cleanup and preparation. We’ll look at where bad data comes from and how
    we go about identifying it. Then we’ll cover general techniques for dealing with
    problematic data. After that, we’ll look at specific examples based on the reef
    data set.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们先解决几个与数据清理和准备相关的一般性问题。我们将探讨不良数据的来源以及我们如何识别它。然后，我们将介绍处理问题数据的一般技术。之后，我们将基于珊瑚数据集查看具体示例。
- en: I should say that we don’t necessarily need our data to be perfect! Besides
    being difficult to achieve that (who gets to define perfection?), our data only
    needs to be fit for the purpose. We’d like to work effectively with data that’s
    problem-free to the extent that it’s accurate for our business needs. Let’s get
    into it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该说的是，我们并不一定需要我们的数据完美无瑕！除了实现这一点可能很困难（谁有资格定义完美？），我们的数据只需要适合目的。我们希望有效地处理那些尽可能没有问题、符合我们业务需求的数据。让我们开始吧。
- en: 6.3 Getting the code and data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 获取代码和数据
- en: The code and data are available in the Chapter-6 repository in GitHub at [https://github.com/data-wrangling-with-javascript/chapter-6](https://github.com/data-wrangling-with-javascript/chapter-6).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 代码和数据可在GitHub的Chapter-6存储库中找到，网址为[https://github.com/data-wrangling-with-javascript/chapter-6](https://github.com/data-wrangling-with-javascript/chapter-6)。
- en: The example data is located under the *data* directory in the repository. Output
    generated by code is located under the *output* directory (but isn’t included
    in the repo).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据位于存储库中的 *data* 目录下。由代码生成的输出位于 *output* 目录下（但不在存储库中）。
- en: Refer to “Getting the code and data”*in chapter 2 for help getting the code
    and data.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考第2章中的“获取代码和数据”以获取帮助获取代码和数据。
- en: '*## 6.4 The need for data cleanup and preparation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 6.4 数据清理和准备的需求'
- en: 'Why do we need to clean up and prepare our data? Ultimately, it’s about fixing
    problems in the data. We need to do this for the following reasons:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要清理和准备我们的数据？最终，这关乎解决数据中的问题。我们需要出于以下原因来做这件事：
- en: To make sure we don’t draw the wrong conclusions and make bad decisions based
    on broken or inaccurate data.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保我们不会基于错误或不准确的数据得出错误的结论并做出糟糕的决定。
- en: To avoid negative business impact—for example, losing trust with customers/clients
    who notice broken data.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免产生负面影响——例如，失去那些注意到数据错误的客户/客户的信任。
- en: Working with data that’s clean, accurate, and reliable makes our job easier
    and more straightforward.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与干净、准确和可靠的数据一起工作可以使我们的工作更简单、更直接。
- en: We should fix data problems early, when they’re cheap to fix. The longer you
    leave them, the more expensive they are to rectify.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该在问题容易解决的时候修复数据问题。你留得越久，解决它们就越昂贵。
- en: We may need to prepare our data offline for efficient use in production. To
    get timely results so that we can take quick action, we need data that’s already
    in the best format to be used with adequate performance.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能需要在生产中离线准备我们的数据以实现高效使用。为了及时获得结果，以便我们可以迅速采取行动，我们需要数据已经以最佳格式存在，以便能够提供足够的性能。
- en: 'We have a variety of reasons why we must put effort into fixing our data, but
    that begs the question: Why is data broken in the first place?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有许多原因需要投入精力来修复我们的数据，但这引发了一个问题：数据最初为什么会出错？
- en: 6.5 Where does broken data come from?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 破坏数据从何而来？
- en: Data can have errors for any number of reasons. We don’t often control the source,
    although if we do, we can ensure that we have good validation at the collection
    point. We can save time and effort by ensuring that data is clean at the moment
    it’s collected.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能由于任何原因出现错误。我们通常无法控制数据源，尽管如果我们能控制，我们可以在收集点确保良好的验证。我们可以在收集数据时确保数据清洁，从而节省时间和精力。
- en: However, even when we control the source, we can’t always achieve good data
    quality. For example, if we read data from electronic sensors, they might occasionally
    return spurious or faulty readings. They might have intermittent problems and
    drop out for periods of time, leaving gaps in the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使我们控制了数据源，我们也不能总是达到良好的数据质量。例如，如果我们从电子传感器读取数据，它们可能会偶尔返回虚假或错误的数据。它们可能会有间歇性问题，并在一段时间内失效，导致数据中断。
- en: We could have software that’s responsible for collecting or synthesizing our
    data. Latent bugs in that software might be generating bad data, and we don’t
    even know it yet! Bugs such as these might go unnoticed for significant periods
    of time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能有负责收集或合成数据的软件。该软件中的潜在错误可能会生成不良数据，而我们甚至还没有意识到这一点！这样的错误可能长时间未被察觉。
- en: Maybe we’re generating data with buggy software, and we know bugs are causing
    bad data. Are we in a position to fix them? We might not be able to! Various reasons
    exist why we might be unable to fix the bugs in the program. For a start, we might
    not have access to the source code, and therefore, we can’t update the program.
    Or we might be working with complex legacy code and are hesitant to make changes—changes
    that potentially cause more bugs (you know what I mean if you’ve ever worked with
    legacy code). When you can’t change the code, or changing the code is too hard,
    the only other option is to work around the bad data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们正在使用有缺陷的软件生成数据，我们知道这些缺陷正在导致不良数据。我们是否处于修复它们的位置？我们可能无法做到！可能存在各种原因，使我们可能无法修复程序中的错误。首先，我们可能无法访问源代码，因此无法更新程序。或者我们可能在与复杂的遗留代码一起工作，并犹豫是否要做出更改——这些更改可能会产生更多的错误（如果你曾经与遗留代码一起工作，你应该知道我的意思）。当你无法更改代码，或者更改代码太难时，唯一的选择是绕过不良数据。
- en: Most often we’ll acquire our data from external sources over which we have no
    control. We must therefore expect that our data contains any number of problems
    that must be fixed before we can work with it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常无法控制数据来源，因此我们获取的数据可能存在任何数量的问题，在我们可以使用之前必须解决这些问题。
- en: Whichever way we acquire data, it seems impossible to avoid bad data, hence
    the need for data cleanup and preparation. We must invest time to check our data
    for errors and, when necessary, fix problems and prepare our data for efficient
    usage in production.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们以何种方式获取数据，似乎都无法避免不良数据，因此需要数据清理和准备。我们必须投入时间检查数据中的错误，并在必要时修复问题，为生产中的高效使用准备数据。
- en: 6.6 How does data cleanup fit into the pipeline?
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 数据清理如何融入管道？
- en: In chapter 3 I introduced the core data representation (CDR) design pattern.
    This is the idea that we can piece together flexible data pipelines by connecting
    the stages with a shared data representation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我介绍了核心数据表示（CDR）设计模式。这是通过连接具有共享数据表示的阶段来构建灵活数据管道的想法。
- en: At the end of chapter 3, the conceptual model of our data conversion pipeline
    looked like [figure 6.1](#figure6.1). The import code produces data in the core
    data representation that’s fed into the export code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章结束时，我们的数据转换管道的概念模型看起来像[图6.1](#figure6.1)。导入代码产生核心数据表示的数据，然后输入到导出代码中。
- en: '![c06_01.eps](Images/c06_01.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![c06_01.eps](Images/c06_01.png)'
- en: '[Figure 6.1](#figureanchor6.1) A basic data pipeline: data is converted from
    one format to another through the core data representation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.1](#figureanchor6.1) 基本数据管道：数据通过核心数据表示从一种格式转换为另一种格式。'
- en: '![c06_02.eps](Images/c06_02.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![c06_02.eps](Images/c06_02.png)'
- en: Figure 6.2 A more complete data pipeline with the addition of cleanup and preparation
    stages
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 添加了清理和准备阶段的更完整的数据管道。
- en: '![c06_03.eps](Images/c06_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![c06_03.eps](Images/c06_03.png)'
- en: Figure 6.3 Phases in the data pipeline are linked together using the core data
    representation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 数据管道中的各个阶段通过核心数据表示连接在一起。
- en: In this chapter, we extend the conceptual model of our data pipeline to include
    multiple transformation stages to clean up, prepare, and transform our data. [Figure
    6.2](#figure6.2) shows how arbitrary cleanup and preparation stages fit into the
    pipeline. It demonstrates how we can include any number of data transformation
    stages between import and export. We can use this model to build a data pipeline
    that can import from any one format, *transform* the data through multiple stages,
    and then export to any other format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了我们数据管道的概念模型，包括多个转换阶段来清理、准备和转换我们的数据。[图6.2](#figure6.2) 展示了任意清理和准备阶段如何融入管道。它演示了如何在导入和导出之间包含任意数量的数据转换阶段。我们可以使用这个模型来构建一个可以从任何一种格式导入，通过多个阶段*转换*数据，然后导出到任何其他格式的数据管道。
- en: The space between the transformation stages is where we use the core data representation.
    [Figure 6.3](#figure6.3) illustrates how the core data representation connects
    our modular data transformation stages. The input and output to any transformation
    stage are a blob of data in the shared format. We can link together multiple stages
    and build flexible data pipelines from reusable code modules.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 转换阶段之间的空间是我们使用核心数据表示的地方。[图6.3](#figure6.3) 阐述了核心数据表示如何连接我们的模块化数据转换阶段。任何转换阶段的输入和输出都是共享格式中的数据块。我们可以链接多个阶段，并从可重用的代码模块中构建灵活的数据管道。
- en: 6.7 Identifying bad data
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 识别不良数据
- en: 'You might well ask: How do we detect bad data? You can approach this in various
    ways.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：我们如何检测不良数据？你可以用各种方式来处理这个问题。
- en: Early on we can look at the data in a text editor or viewer and find problems
    by eye. We need to do this anyway to get a feel for the shape of our data, but
    it can also help us quickly detect any obvious issues. This approach can get us
    started, and it can work for a small data set, but obviously it doesn’t scale
    to large data sets. The human eye is good at picking out problems, but it’s also
    fallible, so we can miss problems easily.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，我们可以在文本编辑器或查看器中查看数据，并通过肉眼发现问题。我们无论如何都需要这样做，以便对数据的形状有一个感觉，但这也可以帮助我们快速检测任何明显的问题。这种方法可以让我们开始，并且对于小数据集来说可能有效，但显然它不能扩展到大数据集。人眼擅长发现问题，但它也有局限性，所以我们很容易错过问题。
- en: My approach is to analyze a small portion of data by eye, then make assumptions
    about how it’s structured and formatted. I then write a script to check those
    assumptions across the entire data set. This is the assumption-checking script
    that we talked about in chapter 5\. It can take a significant amount of time to
    run this script, but it’s worth it because you’ll know then if your assumptions
    bear out or not. The job of this script is to tell you if problems exist in your
    data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我的做法是先通过肉眼分析一小部分数据，然后对其结构和格式做出假设。然后我编写一个脚本来在整个数据集中检查这些假设。这是我们第5章中提到的假设检查脚本。运行这个脚本可能需要相当长的时间，但这是值得的，因为这样你就可以知道你的假设是否成立。这个脚本的工作是告诉你你的数据中是否存在问题。
- en: It might be worthwhile to optimize your assumption-checking script to speed
    up the process, especially because you might want to run your assumption-checking
    script in production and so you can accept streaming data updates into your live
    data pipeline. We’ll talk more about live data pipelines in chapter 12.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可能值得优化你的假设检查脚本以加快处理速度，尤其是因为你可能想在生产环境中运行你的假设检查脚本，以便接受实时数据更新到你的数据管道中。我们将在第12章中更多地讨论实时数据管道。
- en: 'One final way to detect bad data that you might want to consider is to *crowd-source*
    the problem and allow your users to find and report broken data. You might want
    to consider canarying your production release, which is making a new version available
    to a subset of users who’ll help you find problems before it’s generally released.
    Whether this approach makes sense depends on your product: you’ll need a huge
    data set (otherwise why would you need to do this) and a large and active user
    base.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想考虑的一种检测不良数据的方法是*众包*问题，并允许你的用户找到并报告损坏的数据。你可能还想考虑在生产版本中引入金丝雀测试，即将新版本提供给一小部分用户，以便在它广泛发布之前帮助你找到问题。这种方法是否合理取决于你的产品：你需要一个巨大的数据集（否则你为什么要这样做）和一个庞大且活跃的用户群体。
- en: 6.8 Kinds of problems
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 种类的问题
- en: 'The kinds of problems we might see in data are many and varied. Here are several
    examples for illustration:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能在数据中看到的问题种类繁多。以下是一些示例，以供说明：
- en: '*Extra* *white space—*Blank rows or whitespace around field values.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*额外* *空白空间—*字段值周围的空白行或空白。'
- en: '*Missing data—*Empty, null, or NaN fields.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺失数据—*空、null或NaN字段。'
- en: '*Unexpected data* —Can your code handle new and unexpected values?'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*意外数据* —你的代码能处理新的和意外的值吗？'
- en: '*Inaccurate data—*Sensor readings that are off by a certain amount.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不准确的数据—*传感器读数偏离一定量。'
- en: '*Inconsistencies—*Street and St, Mister and Mr, data in different currencies,
    inconsistent capitalization.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不一致性—*街道和St，先生和Mr，不同货币中的数据，不一致的大小写。'
- en: '*Badly formatted fields—*Email, phone number, misspelled categories, and so
    on.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*格式错误的字段—*电子邮件、电话号码、拼写错误的类别等。'
- en: '*Broken data—*Date/time with missing time zone or faulty sensor readings.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损坏的数据—*缺少时区或传感器读数错误的日期/时间。'
- en: '*Irrelevant data—*Data that isn’t useful to us.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无关数据—*对我们无用的数据。'
- en: '*Redundant data—*Data that’s duplicated.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*冗余数据—*重复的数据。'
- en: '*Inefficient data—*Data that isn’t organized for effective use.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*低效的数据—*没有为有效使用而组织的数据。'
- en: '*Too much data—*We have more data than we can deal with.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据过多—*我们处理不过来的数据。'
- en: Soon we’ll delve into specific examples for code to fix several of these problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将深入研究修复这些问题的具体代码示例。
- en: 6.9 Responses to bad data
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.9 对坏数据的响应
- en: We’ve identified bad data, but how do we respond to it?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经识别出坏数据，但我们如何应对它？
- en: 'This depends on your situation and the scale of your data, but we have various
    responses to bad data at our disposal that we can deploy. Consider the following
    options:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你的情况和数据规模，但我们有各种应对坏数据的策略可供部署。考虑以下选项：
- en: '*We can fix the data—*If that’s possible.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以修复数据—*如果可能的话。'
- en: '*We can optimize the data—*If it’s in an ineffective or inefficient format.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以优化数据—*如果它在无效或不高效的格式中。'
- en: '*We could ignore the problem—*We need to ask: what’s the worst that could happen?'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以忽略这个问题—*我们需要问：最坏的情况会怎样？'
- en: '*We can work around the problem—*Maybe we can deal with the problem in production,
    rather than offline?'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以绕过问题—*也许我们可以在生产中处理这个问题，而不是离线？'
- en: '*We could filter out the broken data—*Maybe it costs more to fix than it’s
    worth to us.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以过滤掉损坏的数据—*也许修复它比对我们来说更有价值。'
- en: '*We could generate the data again—*If possible, maybe we can fix the source
    of the problem and then capture or generate the data from scratch. If the data
    was cheap to generate in the first place, regeneration might be less expensive
    than trying to fix the data.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以重新生成数据—*如果可能的话，也许我们可以修复问题的源头，然后从头开始捕获或生成数据。如果最初生成数据很便宜，重新生成可能比试图修复数据更便宜。'
- en: When we talk about responding to bad data, we must also consider *where* we’ll
    respond to it. Most of this chapter assumes that we’ll fix our data *offline*,
    although it’s useful to note that most of these techniques will also work *online*
    in a live data pipeline, such as the example we’ll cover in chapter 12.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论对坏数据的响应时，我们也必须考虑*在哪里*我们将对其进行响应。本章的大部分内容假设我们将数据*离线*修复，尽管值得注意的是，这些技术中的大多数也适用于实时数据管道，例如我们将在第12章中涵盖的示例。
- en: Shouldn’t we always fix our data offline? It’s certainly better for the performance
    of our production system if we do fix our data offline, but cases exist where
    doing so might not be feasible. For example, imagine that you have a huge data
    set and it has errors, but the errors are only pertinent to a small number of
    users and access is infrequent. In this case it might be more effective to have
    the live system fix such errors just in time, the so-called lazy pattern, and
    then bake the fixed records back into the database. This allows our production
    system to slowly rectify itself over time without needing large amounts of offline
    time and resources and without unduly affecting our user base.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是应该总是离线修复我们的数据吗？如果我们这样做，确实会提高我们生产系统的性能，但存在一些情况，这样做可能不可行。例如，想象一下你有一个巨大的数据集，它有错误，但这些错误仅与少数用户相关，并且访问频率不高。在这种情况下，让实时系统及时修复这些错误可能更有效，这就是所谓的懒惰模式，然后将修复后的记录重新烘焙回数据库。这允许我们的生产系统随着时间的推移缓慢地自行纠正，而不需要大量的离线时间和资源，并且不会过度影响我们的用户群。
- en: Techniques for fixing bad data
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复坏数据的技巧
- en: We haven’t yet addressed what we need to do to fix broken data. A huge number
    of problems can occur in data; fortunately, we have a simple set of strategies
    that we can deploy to fix broken data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有解决修复损坏数据需要做什么。数据中可能发生大量问题；幸运的是，我们有一套简单的策略可以部署来修复损坏的数据。
- en: '[Table 6.2](#table6.2) lists the techniques for fixing bad data that we’ll
    now add to our toolkit.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.2](#table6.2) 列出了我们将现在添加到工具包中修复坏数据的技巧。'
- en: Table 6.2 Techniques for fixing bad data
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 修复不良数据的技巧
- en: '| **Technique** | **How?** | **Why?** |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **技巧** | **如何？** | **为什么？** |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Modify the data | Iterate and update rows and columns. | For normalizing
    and standardizing data |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 修改数据 | 迭代并更新行和列。 | 用于数据归一化和标准化 |'
- en: '|  |  | For fixing broken data |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 用于修复损坏的数据 |'
- en: '| Remove the data | Filter out rows and columns. | To remove irrelevant and
    redundant data |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 删除数据 | 过滤行和列。 | 用于删除不相关和冗余的数据 |'
- en: '|  |  | To reduce data when we have too much |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 当我们数据过多时减少数据 |'
- en: '| Aggregating data | To merge, combine, and summarize data | To optimize data
    for efficient access |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 数据聚合 | 合并、组合和汇总数据 | 优化数据以实现高效访问 |'
- en: '|  |  | To reduce data when we have too much |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 当我们数据过多时减少数据 |'
- en: '| Splitting data | Separating data out into separate data sets | For efficient
    access |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 分割数据 | 将数据分离成单独的数据集 | 用于高效访问 |'
- en: We’ll spend the rest of the chapter exploring code examples of these techniques.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的剩余部分探索这些技术的代码示例。
- en: Cleaning our data set
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理我们的数据集
- en: 'It’s time to get into code examples! We’ll first look at what’s probably the
    most common technique: rewriting rows of data to fix the issues we found. Then
    we’ll look at a common alternative: filtering out rows or columns to remove broken
    or irrelevant data.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入代码示例了！我们首先将查看最常见的技术之一：重写数据行以修复我们发现的问题。然后我们将查看一个常见的替代方案：过滤行或列以删除损坏或不相关的数据。
- en: We’ll use important JavaScript functions in these examples, so please pay attention.
    I’ll also show how to do this kind of work in Data-Forge. To load data, we’ll
    fall back on our toolkit functions for importing and export CSV files that we
    created in chapter 3.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这些示例中使用重要的JavaScript函数，所以请务必注意。我还会展示如何在Data-Forge中完成这类工作。为了加载数据，我们将回退到我们在第三章中创建的用于导入和导出CSV文件的工具包函数。
- en: 6.11.1 Rewriting bad rows
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11.1 重写不良行
- en: Our first problem to fix in the reef data is a date/time problem. Working with
    date/time values can cause many problems, although the solutions are often easy
    after you understand the problem. In this case, the problem is that the date/time
    is stored as a string representation that doesn’t include time zone information
    (see [figure 6.4](#figure6.4)). The reef database contains records from many different
    time zones, so it’s important that we have the correct time zone encoded in our
    dates. Many production issues have been caused by dates that are in the wrong
    time zone for users of our products.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在礁石数据中，我们首先要解决的问题是一个日期/时间问题。处理日期/时间值可能会引起许多问题，尽管在理解了问题之后，解决方案通常很容易找到。在这种情况下，问题在于日期/时间被存储为不包含时区信息的字符串表示（见[图6.4](#figure6.4)）。礁石数据库包含来自许多不同时区的记录，因此，在我们的日期中正确编码时区非常重要。由于我们的产品用户中存在日期位于错误时区的情况，这已经导致了许多生产问题。
- en: '![c06_04.eps](Images/c06_04.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![c06_04.eps](Images/c06_04.png)'
- en: '[Figure 6.4](#figureanchor6.4) Dates and time zones are stored in separate
    columns.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.4](#figureanchor6.4) 日期和时区存储在不同的列中。'
- en: Our aim here is to convert all the date/time values to the standard UTC format
    with the correct time zone encoded (shown in [figure 6.5](#figure6.5)). We’ll
    use the JavaScript date/time library moment to achieve this. It’s one of the handiest
    JavaScript libraries you’ll ever find. You might remember that we first installed
    it in chapter 2 and used it again in chapter 4\. It’s an invaluable tool for dealing
    with date and time values.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将所有日期/时间值转换为带有正确时区编码的标准UTC格式（如图6.5所示）。我们将使用JavaScript日期/时间库moment来实现这一点。这是你将找到的最实用的JavaScript库之一。你可能记得我们第一次在第二章中安装它，并在第四章中再次使用它。它是处理日期和时间值的无价之宝。
- en: '![c06_05.png](Images/c06_05.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![c06_05.png](Images/c06_05.png)'
- en: '[Figure 6.5](#figureanchor6.5) Separate date and time zone columns are merged
    into a UTC formatted date that includes the time zone.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.5](#figureanchor6.5) 将日期和时区列合并为一个包含时区的UTC格式日期。'
- en: In this case we have all the information we need already because each record
    encodes the time zone as a separate field. We need to combine these two fields
    into a single international date/time value that reflects the right date/time
    in the right time zone. We can easily do this using moment as indicated in [figure
    6.5](#figure6.5).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们已经有所有需要的信息，因为每个记录都将时区编码为单独的字段。我们需要将这些两个字段合并成一个单一的国际化日期/时间值，以反映正确时区的正确日期/时间。我们可以很容易地使用moment（如[图6.5](#figure6.5)所示）来完成这项工作。
- en: To rewrite every row in our data set, we’ll use the JavaScript `map` function.
    This function accepts as input an array—our input data set. We also pass a transformation
    function into the `map` function. This function applies a modification to each
    record in our data set. The output of the `map` function is a modified data set—the
    result of transforming each record and building a new array.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要重写我们数据集中的每一行，我们将使用 JavaScript 的 `map` 函数。这个函数接受一个数组作为输入——我们的输入数据集。我们还向 `map`
    函数传递一个转换函数。这个函数对我们的数据集中的每条记录应用修改。`map` 函数的输出是一个修改后的数据集——转换每条记录并构建新数组的结果。
- en: We can say that the `map` function *rewrites* our data set by applying the specified
    modification to every record. You can see in [figure 6.6](#figure6.6) how the
    `transformRow` function is applied to every element of the input array to build
    the output array.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，`map` 函数通过将指定的修改应用于每条记录来 *重写* 我们的数据集。您可以在 [图 6.6](#figure6.6) 中看到 `transformRow`
    函数是如何应用于输入数组的每个元素以构建输出数组的。
- en: '![c06_06.eps](Images/c06_06.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![c06_06.eps](Images/c06_06.png)'
- en: '[Figure 6.6](#figureanchor6.6) Using the JavaScript map function to transform
    an array of data from one structure to another'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.6](#figureanchor6.6) 使用 JavaScript `map` 函数将数据从一种结构转换为另一种结构'
- en: '[Listing 6.1](#listing6.1) shows the code that uses the `map` function to fix
    date/time values in our reef data set. The important functions to look at are
    `transformData` and `transformRow. transformData` transforms the entire data set.
    `transformRow` fixes each record in the data set. We use the moment library to
    combine the string representation of the date/time with the time zone value from
    each record.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.1](#listing6.1) 展示了使用 `map` 函数修复我们珊瑚数据集中日期/时间值的代码。需要关注的重要函数是 `transformData`
    和 `transformRow`。`transformData` 转换整个数据集。`transformRow` 修复数据集中的每条记录。我们使用 moment
    库将日期/时间的字符串表示与每条记录中的时区值结合起来。'
- en: The `map` function essentially splits apart the input array and then modifies
    each record by passing it through `transformRow`. Finally, it glues the modified
    records together into a new array, outputting a new data set with the broken data
    repaired. After you run the following listing, and it generates the output file
    (surveys-with-fixed-dates.csv), load the file in Excel or a text editor to verify
    it came out correctly.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`map` 函数本质上将输入数组拆分开来，然后通过 `transformRow` 修改每条记录。最后，它将修改后的记录粘合在一起形成一个新的数组，输出一个修复了损坏数据的新数据集。运行以下列表后，它将生成输出文件（surveys-with-fixed-dates.csv），然后在
    Excel 或文本编辑器中加载该文件以验证其正确性。'
- en: Listing 6.1 Rewriting rows to fix bad data (listing-6.1.js)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 重写行以修复不良数据（listing-6.1.js）
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note in [listing 6.1](#listing6.1) how we reused the CSV import and export functions
    that we created back in chapter 3\. We use these now to load the input data from
    the CSV file surveys.csv and then, after the broken data has been repaired, we
    save the data to the new CSV file surveys-with-fixed-dates.csv*.*
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 [列表 6.1](#listing6.1) 中，我们如何重用了我们在第 3 章中创建的 CSV 导入和导出函数。我们现在使用这些函数从 CSV
    文件 surveys.csv 中加载数据，然后在损坏的数据被修复后，将数据保存到新的 CSV 文件 surveys-with-fixed-dates.csv
    中*.*。
- en: This technique can be used to rewrite entire rows or, as we did in [listing
    6.1](#listing6.1), to rewrite specific individual fields. We used this technique
    to fix our data, but you might also say we did this to make our production code
    a bit simpler because now it only has to deal with the combined date/time value.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可以用来重写整个行，或者，就像我们在 [列表 6.1](#listing6.1) 中做的那样，重写特定的单个字段。我们使用这项技术来修复我们的数据，但您也可以说我们这样做是为了使我们的生产代码更简单，因为现在它只需要处理组合的日期/时间值。
- en: General pattern for row transformation
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 行转换的通用模式
- en: We can generalize a reusable pattern from this technique so that we can use
    it for rewriting any tabular data set. The following listing shows the generalized
    pattern. Slot your own code into the `transformRow` function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这项技术中提炼出一个可重用的模式，以便我们可以用它来重写任何表格数据集。以下列表显示了通用模式。将您自己的代码放入 `transformRow`
    函数中。
- en: Listing 6.2 General pattern for rewriting bad rows (extract from listing-6.2.js)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 重写不良行的通用模式（摘自 listing-6.2.js）
- en: '[PRE1]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using Data-Forge to rewrite broken data
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Data-Forge 重写损坏的数据
- en: We can also use Data-Forge to rewrite our data set in a way that looks similar
    to plain old JavaScript.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 Data-Forge 以类似于传统 JavaScript 的方式重写我们的数据集。
- en: Why should we use Data-Forge for this? Because data transformations like this
    fit nicely into a flexible, convenient, and elegant Data-Forge data pipeline.
    At the end of the chapter, you’ll see a more complete Data-Forge example to show
    you this all fits together in the context of a bigger data pipeline, but for now
    let’s rewrite [listing 6.1](#listing6.1) using Data-Forge.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么应该使用 Data-Forge 呢？因为像这样的数据转换非常适合灵活、方便且优雅的 Data-Forge 数据管道。在章节的结尾，你将看到一个更完整的
    Data-Forge 示例，展示这一切是如何在大数据管道的背景下结合在一起的，但就目前而言，让我们使用 Data-Forge 重新编写[列表 6.1](#listing6.1)。
- en: 'You’ll notice that [listing 6.3](#listing6.3) is similar to [listing 6.1](#listing6.1).
    We have the familiar `transformData` and `transformRow` functions. In fact, `transformRow`
    is exactly the same as in [listing 6.1](#listing6.1). However, `transformData`
    is different. In this case, it accepts a Data-Forge DataFrame as input and returns
    a new, modified DataFrame as output. Instead of JavaScript’s `map` function, we
    are using Data-Forge’s `select` function to transform the data set. `map` and
    `select` are conceptually equivalent: they both pull apart a sequence of data,
    modify each record, and then merge the output to create a new sequence. You can
    run the following listing, and it will output the file surveys-with-fixed-dates-using-data-forge.csv.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 [列表 6.3](#listing6.3) 与 [列表 6.1](#listing6.1) 类似。我们有熟悉的 `transformData`
    和 `transformRow` 函数。实际上，`transformRow` 与 [列表 6.1](#listing6.1) 中的完全相同。然而，`transformData`
    是不同的。在这种情况下，它接受一个 Data-Forge DataFrame 作为输入，并返回一个新的、修改后的 DataFrame 作为输出。我们不是使用
    JavaScript 的 `map` 函数，而是使用 Data-Forge 的 `select` 函数来转换数据集。`map` 和 `select` 在概念上是等效的：它们都拆分一个数据序列，修改每个记录，然后将输出合并以创建一个新的序列。你可以运行以下列表，它将输出文件
    surveys-with-fixed-dates-using-data-forge.csv。
- en: Listing 6.3 Using Data-Forge to rewrite bad records (listing-6.3.js)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 使用 Data-Forge 重新编写不良记录（listing-6.3.js）
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 6.3](#listing6.3) isn’t so different from [listing 6.1](#listing6.1),
    and it doesn’t yet show you the power of Data-Forge. One of the benefits of Data-Forge,
    among others, is that it’s easy to chain data transformations and build a pipeline.
    Let’s work through the remainder of the examples before we see how they can be
    chained together into a more complex pipeline using Data-Forge.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.3](#listing6.3) 与 [列表 6.1](#listing6.1) 并没有太大的不同，它还没有展示出 Data-Forge 的强大功能。Data-Forge
    的好处之一，以及其他好处，是它很容易链式调用数据转换并构建管道。在我们看到它们如何使用 Data-Forge 链接成一个更复杂的管道之前，让我们先处理完剩余的示例。'
- en: 6.11.2 Filtering rows of data
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11.2 过滤数据行
- en: Our second problem to fix in the reef data is that we’re only interested in
    Australian reefs. That’s what we’re focusing on, and the rest of the data isn’t
    relevant to our data analysis, so let’s remove the rows in which we have no interest.
    We can filter out data when it isn’t useful to us or when we detect duplication
    or redundancy. We might also want to filter out data that’s broken when we have
    no cost-effective way of fixing it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在珊瑚礁数据中需要解决的第二个问题是，我们只对澳大利亚的珊瑚礁感兴趣。这就是我们的关注点，其余的数据与我们的数据分析无关，所以让我们删除我们不感兴趣的行。当我们发现数据无用或检测到重复或冗余时，我们可以过滤掉数据。我们可能还希望过滤掉那些我们没有成本效益的修复方法的数据。
- en: As we already discussed in chapter 5, working with a cut-down data set is going
    to make our process quicker and more streamlined. Also, the data you are interested
    in will be clearer because it’s not cluttered up with additional data that’s not
    relevant. You should definitely remove the parts of the data that you don’t need.
    As always, take care not to overwrite your source data. The data that you are
    about to remove might be needed one day, so be careful to stash aside a copy of
    the original unmodified data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 5 章中已经讨论过的，使用精简的数据集将使我们的过程更快、更流畅。此外，你感兴趣的数据将更加清晰，因为它没有被无关的额外数据所杂乱。你绝对应该删除你不需要的数据部分。一如既往，请注意不要覆盖你的源数据。你即将删除的数据可能在将来某一天需要，所以请小心保留原始未修改数据的副本。
- en: Our aim here is to remove data for reefs that aren’t in Australia. We’ll use
    the JavaScript `filter` function to achieve this. We’ll call the `filter` function
    on our array of data and pass in a user-defined *predicate* function that specifies
    which records to filter out. The predicate function must return Boolean `true`
    to keep the record or `false` to remove it. Like the `map` function that we examined
    earlier, the `filter` function pulls apart the input array and then, based on
    the results of the predicate function, it stitches together a new array but minus
    any records that were filtered out.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是移除不在澳大利亚的珊瑚礁数据。我们将使用JavaScript `filter` 函数来实现这一点。我们将在数据数组上调用 `filter`
    函数，并传入一个用户定义的 *谓词* 函数，该函数指定要过滤掉的记录。谓词函数必须返回布尔 `true` 以保留记录或 `false` 以移除它。与之前检查的
    `map` 函数类似，`filter` 函数将输入数组拆分，然后根据谓词函数的结果，它将拼接一个新的数组，但减去任何被过滤掉的记录。
- en: We can say that the `filter` function *rewrites* our data set by removing the
    records that we no longer want. You can see in [figure 6.7](#figure6.7) how the
    `filterRow` predicate function is applied to every element of the input array
    to determine if the record should be included in the output array.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，`filter` 函数通过删除我们不再想要的记录来 *重写* 我们的数据集。你可以在 [图6.7](#figure6.7) 中看到 `filterRow`
    谓词函数是如何应用于输入数组的每个元素，以确定记录是否应包含在输出数组中。
- en: '[Listing 6.4](#listing6.4) demonstrates use of the JavaScript `filter` function
    to remove rows from our reef data set. We see here again the `transformData` function
    from previous listings, although this time we use the `filter` function, instead
    of the `map` function, to transform the data set.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.4](#listing6.4) 展示了使用JavaScript `filter` 函数从我们的珊瑚礁数据集中删除行。我们在这里再次看到了之前列表中的
    `transformData` 函数，尽管这次我们使用 `filter` 函数而不是 `map` 函数来转换数据集。'
- en: 'Notice the `filterRow` function: this is our predicate function that’s called
    for each record and determines whether the record should stay or go. `filterRow`
    returns `true` for each record that’s in Australia and so it keeps those records.
    On the flip side, it returns `false` for every other record and removes those
    records not in Australia.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `filterRow` 函数：这是我们为每个记录调用的谓词函数，它确定记录是否应该保留或删除。`filterRow` 对每个位于澳大利亚的记录返回
    `true`，因此它保留了这些记录。另一方面，它对每个其他记录返回 `false`，并删除了不在澳大利亚的记录。
- en: The `filter` function splits apart the input array, and it calls `filterRow`
    for each record. It produces a new array containing only those records that passed
    the filter—the output array only contains records for which `filterRow` returned
    `true`. It outputs a new data set, not including the records that we wanted removed.
    You should run the following listing and inspect the file surveys-but-only-Australia.csv
    that it outputs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 函数将输入数组拆分，并为每个记录调用 `filterRow`。它生成一个只包含通过过滤的记录的新数组——输出数组只包含 `filterRow`
    返回 `true` 的记录。它输出一个新数据集，不包括我们想要删除的记录。你应该运行以下列表并检查它输出的文件 surveys-but-only-Australia.csv。'
- en: '![c06_07.eps](Images/c06_07.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![c06_07.eps](Images/c06_07.png)'
- en: Figure 6.7 Using the JavaScript filter function to produce a new array with
    certain elements filtered out
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 使用JavaScript过滤函数生成一个新数组，其中包含过滤掉某些元素
- en: Listing 6.4 Filtering out unwanted or bad data (extract from listing-6.4.js)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 过滤掉不需要或不良数据（摘自列表-6.4.js）
- en: '[PRE3]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: General pattern for filtering rows
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过滤行的一般模式
- en: We can make a general pattern for filtering out rows of data from our data sets.
    [Listing 6.5](#listing6.5) is a template for this, and you can insert your own
    filtering code. Remember that your predicate function must return `true` for the
    records that you want to keep and `false` for the records you want removed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为从我们的数据集中过滤掉数据行创建一个通用模式。[列表6.5](#listing6.5) 是这个模式的模板，你可以插入你自己的过滤代码。记住，你的谓词函数必须为你要保留的记录返回
    `true`，为你要删除的记录返回 `false`。
- en: Listing 6.5 General pattern for filtering out bad data (listing-6.5.js)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 过滤掉不良数据的一般模式（列表-6.5.js）
- en: '[PRE4]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using Data-Forge to filter rows
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Data-Forge过滤行
- en: Let’s look again at Data-Forge, and this time we’ll learn how we can use it
    to filter rows of data. What we see here is similar to how this is achieved in
    plain old JavaScript. Because it’s so similar, you might wonder why we’d bother
    using Data-Forge? The reason for this should become clear at the end of the chapter
    when I show you how to chain together multiple Data-Forge functions to build a
    more complex data pipeline.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看Data-Forge，这次我们将学习如何使用它来过滤数据行。我们看到的情况与在纯JavaScript中实现的方式相似。因为它如此相似，你可能会想知道为什么我们要费心使用Data-Forge？这个原因应该在章节结束时变得清晰，届时我将向您展示如何将多个Data-Forge函数链接起来以构建更复杂的数据管道。
- en: '[Listing 6.6](#listing6.6) has the same `filterRow` function as [listing 6.4](#listing6.4).
    Its `transformData` function, however, uses Data-Forge’s `where` function to filter
    out records instead of the JavaScript `filter` function that we used in [listing
    6.4](#listing6.4). Both `where` and `filter` functions perform the same conceptual
    task: they execute a predicate function for each record that determines which
    records should remain and which are to be removed. Our `transformData` function
    in [listing 6.6](#listing6.6) accepts a DataFrame as input and returns a new,
    modified DataFrame as output. The output DataFrame retains only the records that
    we wanted to keep; all others have been filtered out. When you run this code,
    it produces the output file surveys-but-only-Australia-using-data-forge.csv. Inspect
    the output file, and you’ll see that it’s the same as that produced by [listing
    6.4](#listing6.4).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.6](#listing6.6) 与[列表6.4](#listing6.4) 有相同的`filterRow`函数。然而，它的`transformData`函数使用Data-Forge的`where`函数来过滤记录，而不是我们在[列表6.4](#listing6.4)中使用的JavaScript
    `filter`函数。`where`和`filter`函数执行相同的概念性任务：它们为每个记录执行一个谓词函数，以确定哪些记录应该保留，哪些应该被删除。我们的[列表6.6](#listing6.6)中的`transformData`函数接受一个DataFrame作为输入，并返回一个新的、修改后的DataFrame作为输出。输出DataFrame仅保留我们想要保留的记录；所有其他记录都已过滤掉。当你运行此代码时，它会产生输出文件surveys-but-only-Australia-using-data-forge.csv。检查输出文件，你会发现它与[列表6.4](#listing6.4)产生的相同。'
- en: Listing 6.6 Using Data-Forge to filter out unwanted or bad data (extract from
    listing-6.6.js)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 使用Data-Forge过滤掉不需要或不良数据（摘自列表-6.6.js）
- en: '[PRE5]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We haven’t yet seen the real power of Data-Forge. Hold tight; that’s coming
    soon!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有看到Data-Forge的真正力量。请耐心等待；它很快就会到来！
- en: 6.11.3 Filtering columns of data
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11.3 过滤数据列
- en: Our third problem to fix in the reef data involves removing columns. This is
    similar to the previous problem where we wanted to remove rows of data. This time,
    though, rather than remove entire records, we want to remove individual fields
    from each record but leave the remainder of each record intact.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在礁石数据中需要解决的第三个问题是删除列。这与之前的问题类似，那时我们想要删除数据行。然而，这次，我们不是要删除整个记录，而是要从每个记录中删除单个字段，但保留每个记录的其余部分。
- en: 'We do this for the same reason that we remove rows: to remove broken, irrelevant,
    or redundant data and also to make the data set more compact and easier to work
    with. Again, please take care not to overwrite your source data, and stash a copy
    of it somewhere for safe keeping.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的原因与删除行相同：为了删除损坏的、无关的或冗余的数据，并使数据集更加紧凑，更容易处理。再次提醒，请务必不要覆盖您的源数据，并将它的副本保存在安全的地方。
- en: Our aim here is to remove the `reef_type` field from each record, which removes
    the `reef_type` column from our entire data set. We don’t need this column, and
    it’s cluttering up our data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是从每个记录中删除`reef_type`字段，这将从我们的整个数据集中删除`reef_type`列。我们不需要这个列，它使我们的数据变得杂乱。
- en: 'Removing a field from every item in an array isn’t as convenient as filtering
    out the entire item the way we did with the JavaScript `filter` function; however,
    JavaScript does provide a `delete` operator that does what we need: it removes
    a field from a JavaScript object (see [figure 6.8](#figure6.8)).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从数组的每个项目中删除字段并不像我们使用JavaScript `filter`函数那样过滤整个项目那样方便；然而，JavaScript确实提供了一个`delete`运算符，它可以完成我们需要的功能：从一个JavaScript对象中删除一个字段（参见[图6.8](#figure6.8)）。
- en: '![c06_08.eps](Images/c06_08.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![c06_08.eps](Images/c06_08.png)'
- en: '[Figure 6.8](#figureanchor6.8) Deleting a field from each element in the array
    has the effect of deleting a “column” from our tabular data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.8](#figureanchor6.8) 从数组的每个元素中删除字段的效果是从我们的表格数据中删除一个“列”。'
- en: To use the `delete` operator, we must iterate over our data set and apply it
    to each record as shown in [listing 6.7](#listing6.7). Note in `transformData`
    that we’re again using the `map` function to transform the entire array of data.
    The `transformRow` function visits each record and uses the `delete` operator
    to remove the `reef_type` field. Run this code, and it will produce the output
    file surveys-with-no-reef-type.csv. The output data is the same as the input,
    but with the desired column removed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`delete`运算符，我们必须遍历我们的数据集并应用于每个记录，如[列表6.7](#listing6.7)所示。注意在`transformData`中，我们再次使用`map`函数来转换整个数据数组。`transformRow`函数访问每个记录并使用`delete`运算符删除`reef_type`字段。运行此代码，将生成输出文件surveys-with-no-reef-type.csv。输出数据与输入数据相同，但已删除所需的列。
- en: Listing 6.7 Removing an entire column (extract from listing-6.7.js)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 删除整个列（列表-6.7.js的摘录）
- en: '[PRE6]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using Data-Forge to filter columns
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Data-Forge过滤列
- en: Continuing our theme of doing it in plain JavaScript, then in Data-Forge, we
    can also use Data-Forge to remove entire columns from our data set. In previous
    examples, using Data-Forge hasn’t been much different from using plain old JavaScript,
    but in this example our task becomes somewhat simpler.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的主题，先在纯JavaScript中实现，然后在Data-Forge中，我们也可以使用Data-Forge从我们的数据集中删除整个列。在先前的例子中，使用Data-Forge与使用纯JavaScript并没有太大的区别，但在这个例子中，我们的任务变得稍微简单一些。
- en: '[Listing 6.8](#listing6.8) shows use of the Data-Forge `dropSeries` function
    to remove a named series (for example, a column of data) from our DataFrame. This
    is easier than removing the field individually from each separate record. When
    you run this code, it produces the output file surveys-with-no-reef-type-using-data-forge.csv.
    This is the same output as produced by [listing 6.7](#listing6.7) but generated
    more conveniently using Data-Forge.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.8](#listing6.8) 展示了使用Data-Forge的`dropSeries`函数从我们的DataFrame中删除一个命名序列（例如，数据列）。这比从每个单独的记录中逐个删除字段要简单。当你运行此代码时，将生成输出文件surveys-with-no-reef-type-using-data-forge.csv。这是与[列表6.7](#listing6.7)生成的相同输出，但使用Data-Forge生成更为方便。'
- en: Listing 6.8 Removing an entire column using Data-Forge (extract from listing-6.8.js)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 使用Data-Forge删除整个列（列表-6.8.js的摘录）
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the first good example of how Data-Forge can simplify and streamline
    the process of working with data, but we’re just getting started and Data-Forge
    has many more functions that help make short work of carving up, transforming,
    and reintegrating our data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Data-Forge如何简化并简化数据处理过程的第一个好例子，但我们才刚刚开始，Data-Forge还有许多更多功能可以帮助我们轻松分割、转换和重新整合我们的数据。
- en: Preparing our data for effective use
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备我们的数据以有效使用
- en: We cleaned up and fixed various problems that we identified in our data. However,
    there may still be work to do to prepare the data for effective use. We might
    still have too much data and need to reduce it, or our data might not be amenable
    to analysis. Let’s now look at several examples of how we can aggregate or divide
    our data to make it easier to work with.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们清理并修复了我们在数据中识别出的各种问题。然而，为了有效使用数据，可能还需要做更多的工作。我们可能仍然有太多的数据需要减少，或者我们的数据可能不适合分析。现在让我们看看几个如何聚合或划分我们的数据以使其更容易处理的例子。
- en: 6.12.1 Aggregating rows of data
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.12.1 数据行聚合
- en: Let’s look at aggregating our data by reef name. If we want to look at statistics
    for each reef, it only makes sense that all records for a particular reef should
    be collapsed down to one summary record per reef.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何按珊瑚礁名称聚合我们的数据。如果我们想查看每个珊瑚礁的统计数据，那么将特定珊瑚礁的所有记录合并成一个珊瑚礁的总结记录是有意义的。
- en: We’ll keep things simple here and look at the cumulative distance that was traveled
    for each reef. We need to sum the `transects_length` field across all records
    from each reef. This is simple in terms of data analysis, but it’s all we need
    for the example in this chapter. Later in chapter 9 we’ll investigate more advanced
    data analysis techniques.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将保持简单，并查看每个珊瑚礁所行驶的累积距离。我们需要对每个珊瑚礁的所有记录中的`transects_length`字段进行求和。从数据分析的角度来看，这很简单，但这是我们本章示例中所需的所有内容。在第9章的后面，我们将探讨更高级的数据分析技术。
- en: '[Figure 6.9](#figure6.9) shows a portion of the source data and how it compares
    to the aggregated data. Notice how each row of data on the left has multiple records
    per reef, but on the right, it has been condensed into a single row per reef.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.9](#figure6.9) 展示了源数据的一部分以及它与聚合数据的比较。注意左侧每行数据在每个珊瑚礁中都有多个记录，但在右侧，它们已经被压缩成每个珊瑚礁一行。'
- en: '![c06_09.eps](Images/c06_09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![c06_09.eps](Images/c06_09.png)'
- en: '[Figure 6.9](#figureanchor6.9) Aggregating data: grouping by reef name and
    then summing the transects_length for each group'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.9](#figureanchor6.9) 聚合数据：按礁石名称分组然后对每个组的transects_length求和'
- en: 'To aggregate our data, we perform the following steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了聚合我们的数据，我们执行以下步骤：
- en: The source data is organized into buckets based on the `reef_name` field.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源数据根据`reef_name`字段组织到桶中。
- en: For each group of records, we compute the sum of the `transects_length` field.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个记录组，我们计算`transects_length`字段的和。
- en: Finally, a new data set is created with a single record per reef and containing
    the aggregated data.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建一个新的数据集，每个礁石一条记录，包含聚合数据。
- en: '[Listing 6.9](#listing6.9) shows the code to aggregate our reef data. Note
    the call to Data-Forge’s `groupBy` function: this transforms our DataFrame into
    a series of groups. The function passed to `groupBy` specifies how to organize
    our data into groups. This says that we wish to group the data by `reef_name`*.*
    The output from `groupBy` is a Data-Forge Series object that represents the series
    of groups. Each group is itself a DataFrame containing a subset of the original
    data. We then call `select` to transform the groups into a new set of summary
    records. Here we call the `sum` function to sum the `transects_length` fields
    for the group.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.9](#listing6.9) 展示了聚合礁石数据的代码。注意对Data-Forge的`groupBy`函数的调用：这将我们的DataFrame转换成一系列组。传递给`groupBy`的函数指定了如何组织我们的数据到组中。这表示我们希望按`reef_name`分组。`groupBy`的输出是一个表示一系列组的Data-Forge
    Series对象。每个组本身是一个包含原始数据子集的DataFrame。然后我们调用`select`将组转换成一组新的汇总记录。在这里，我们调用`sum`函数对组的`transects_length`字段求和。'
- en: There’s quite a lot going on right here, so please take time to read the code
    and let this sink in. You can run this code, and it will generate the file surveys-aggregated.csv
    like the example shown on the right-hand side of [figure 6.9](#figure6.9).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中，所以请花时间阅读代码，并让这些内容深入人心。您可以运行此代码，它将生成类似于右侧[图6.9](#figure6.9)所示的surveys-aggregated.csv文件。
- en: Listing 6.9 Aggregating data using Data-Forge (extract from listing-6.9.js)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 使用Data-Forge聚合数据（列表-6.9.js的摘录）
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is another example that only uses Data-Forge. You could write this code
    in plain old JavaScript, but the code would be longer and more difficult to read.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个仅使用Data-Forge的另一个示例。您可以用普通的JavaScript编写此代码，但代码会更长，更难阅读。
- en: Using Data-Forge allows us to express transformations such as this more concisely.
    Less code means fewer bugs, so that’s a good thing. Notice how the functions `parseFloats`,
    `groupBy`, and `select` are all chained one after the other? We’ve glimpsed how
    Data-Forge functions can be chained one after the other to quickly build data
    pipelines.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Data-Forge允许我们更简洁地表达这种类型的转换。代码越少，错误越少，所以这是一个好事。注意函数`parseFloats`、`groupBy`和`select`是如何一个接一个地链在一起的？我们已经瞥见了Data-Forge函数是如何一个接一个地链在一起以快速构建数据管道的。
- en: 6.12.2 Combining data from different files using globby
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.12.2 使用globby从不同文件中组合数据
- en: Let’s imagine now that we have received our reef data as a set of files. Say
    that the reef data is separated out by country with files Australia.csv, United
    States.csv, and so on. We need to load these files from our local filesystem and
    combine them before we can work with the data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们已经以一组文件的形式收到了我们的礁石数据。比如说，礁石数据按国家分开，有文件Australia.csv、United States.csv等等。在我们能够处理这些数据之前，我们需要从本地文件系统中加载这些文件并将它们合并。
- en: 'Various methods exist to combine data such as this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种方法来组合此类数据：
- en: Concatenate the rows of the files.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接文件的行。
- en: Merge row by row.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行行合并。
- en: Join the data by matching a field (as with a SQL join operation).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过匹配字段（类似于SQL连接操作）来合并数据。
- en: In this section, we’ll keep it simple and focus on the concatenation method.
    We aim to read multiple files into memory, concatenate them in memory, and then
    write them out to a single, large data file. We’ll use a JavaScript library called
    globby to find the files. We already have file import and export capability using
    our toolkit functions. To do the concatenation, we’ll use JavaScript’s array `concat`
    function. The process is shown in [figure 6.10](#figure6.10).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将保持简单，并专注于连接方法。我们的目标是读取多个文件到内存中，在内存中连接它们，然后将它们写入一个单一的大型数据文件。我们将使用一个名为globby的JavaScript库来查找文件。我们已经有文件导入和导出功能，使用我们的工具函数。为了进行连接，我们将使用JavaScript的数组`concat`函数。这个过程在[图6.10](#figure6.10)中展示。
- en: '![c06_10.eps](Images/c06_10.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![c06_10.eps](Images/c06_10.png)'
- en: Figure 6.10 Aggregating multiple input files into a single output file
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 将多个输入文件聚合到一个输出文件中
- en: 'To concatenate multiple files, we perform the following process:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接多个文件，我们执行以下过程：
- en: Locate and read multiple CSV files into memory.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位并读取多个 CSV 文件到内存中。
- en: Use the JavaScript array `concat` function to concatenate all records into a
    single array.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 JavaScript 数组的 `concat` 函数将所有记录连接成一个单独的数组。
- en: Write the concatenated array into a single combined output file.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连接后的数组写入一个单一的合并输出文件。
- en: 'If you installed the dependencies for the Chapter-6 code repository, you already
    have globby installed in your project; otherwise, you can install it in a fresh
    Node.js project as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经为第 6 章代码仓库安装了依赖项，那么你的项目中已经安装了 globby；否则，你可以在一个新的 Node.js 项目中按照以下方式安装它：
- en: '[PRE9]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 6.10](#listing6.10) shows the code that uses globby and our toolkit
    function `importCsvFile` to load multiple files into memory. We use the JavaScript
    `reduce` function to *reduce* the selection of imported files into a single concatenated
    JavaScript array. For each imported file, we call the `concat` function to append
    the imported records into the combined array. You should run this code and look
    at the merged output file surveys-aggregated-from-separate-files.csv that it creates.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.10](#listing6.10) 展示了使用 globby 和我们的工具函数 `importCsvFile` 将多个文件加载到内存中的代码。我们使用
    JavaScript 的 `reduce` 函数将导入文件的选择 *reduce* 到一个单一的连接 JavaScript 数组。对于每个导入的文件，我们调用
    `concat` 函数将导入的记录追加到合并数组中。你应该运行此代码并查看它创建的合并输出文件 surveys-aggregated-from-separate-files.csv。'
- en: Listing 6.10 Aggregating multiple files using globby (listing-6.10.js)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.10 使用 globby 聚合多个文件（listing-6.10.js）
- en: '[PRE10]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note in [listing 6.10](#listing6.10) that all imported files are loaded asynchronously.
    The main purpose of using `reduce` here is to merge the sequence of asynchronous
    operations into a single promise; this allows us to use that one promise to manage
    the whole chain of async operations. We could have used `Promise.all` here as
    well and processed the files in parallel rather than in sequential order, but
    I wanted to demonstrate how to use the `reduce` function in this way. If you’re
    having trouble with this, please refer back to the primer on asynchronous coding
    and promises in chapter 2.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 [列表 6.10](#listing6.10) 中，所有导入的文件都是异步加载的。在这里使用 `reduce` 的主要目的是将一系列异步操作合并成一个单一的
    promise；这允许我们使用这个单一的 promise 来管理整个异步操作链。我们也可以在这里使用 `Promise.all` 并并行处理文件，而不是按顺序处理，但我想要展示如何以这种方式使用
    `reduce` 函数。如果你在这方面遇到困难，请参考第 2 章中关于异步编码和 promises 的入门指南。
- en: Please note that Data-Forge has a `concat` function that you can use to concatenate
    the contents of multiple DataFrames.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Data-Forge 有一个 `concat` 函数，你可以使用它来连接多个 DataFrame 的内容。
- en: 6.12.3 Splitting data into separate files
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.12.3 将数据拆分到单独的文件中
- en: 'We learned how to merge multiple input files into a single data set. Let’s
    now look at the opposite of this: splitting out a big data set into multiple files.
    We might want to do this so that we can work with a smaller partitioned section
    of the data, or maybe it makes our job easier if we can work with data that is
    split up based on some criteria.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何将多个输入文件合并成一个数据集。现在让我们看看这个过程的反面：将大数据集拆分成多个文件。我们可能想要这样做，以便我们可以处理数据的一个较小的分区部分，或者如果我们能够根据某些标准拆分数据来处理数据，这可能使我们的工作变得更简单。
- en: For this example, we’ll do the exact opposite of the previous example, splitting
    up our data based on the country, as shown in [figure 6.11](#figure6.11). This
    gives us more flexibility on how we work with the data. In this example, let’s
    say that we want to work with the data for each country individually. Or if we
    have a large amount of data, it might be more productive to work on a single batch
    at a time, which is a technique we’ll look at again in chapter 8.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将做与上一个例子完全相反的事情，根据国家拆分我们的数据，如图 6.11 所示。这使我们能够更灵活地处理数据。在这个例子中，让我们假设我们想要单独处理每个国家的数据。或者，如果我们有大量的数据，一次处理一个批次可能更有效率，这是我们将在第
    8 章再次探讨的技术。
- en: The code in [listing 6.11](#listing6.11) defines a function called `splitDataByCountry`.
    It starts by calling the `getCountries` function, which queries the data to determine
    the unique list of countries that are represented. Then, for each country, it
    filters the data set for that country and exports a new CSV file that contains
    only the filtered data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.11](#listing6.11) 中的代码定义了一个名为 `splitDataByCountry` 的函数。它首先调用 `getCountries`
    函数，查询数据以确定表示的唯一国家列表。然后，对于每个国家，它过滤该国家的数据集，并导出一个只包含过滤数据的新的 CSV 文件。'
- en: 'The filter and export logic here is similar to what we saw in [listing 6.6](#listing6.6),
    the Data-Forge example for filtering rows, although we added another layer here
    that iterates over all the countries and exports a separate CSV file for each.
    If you run this code, it will produce an output for each country: Australia.csv,
    United States.csv, and so on.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的过滤和导出逻辑与我们在 [列表 6.6](#listing6.6) 中看到的类似，即 Data-Forge 过滤行的示例，尽管我们在这里添加了一个额外的层，它遍历所有国家并为每个国家导出单独的
    CSV 文件。如果你运行此代码，它将为每个国家生成输出：Australia.csv、United States.csv 等。
- en: Listing 6.11 Splitting data into multiple files (listing-6.11.js)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.11 将数据拆分到多个文件中（listing-6.11.js）
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In [listing 6.11](#listing6.11), please note the use of the Data-Forge `aggregate`
    function. This works in a similar way to the JavaScript `reduce` function that
    we saw earlier in this chapter, and we use it here for the same reason: to sequence
    a series of asynchronous operations into a single combined promise. Please refer
    to chapter 2 for a refresher on asynchronous coding and promises.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.11](#listing6.11) 中，请注意 Data-Forge `aggregate` 函数的使用。这与我们在本章早些时候看到的
    JavaScript `reduce` 函数的工作方式类似，我们在这里使用它的原因也是相同的：将一系列异步操作序列化成一个单一的合并承诺。请参考第 2 章以复习异步编码和承诺。
- en: Building a data processing pipeline with Data-Forge
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Data-Forge 构建数据处理管道
- en: One of the main reasons I use Data-Forge is its ability to chain operations
    to quickly build flexible data pipelines. I say flexible because the syntax of
    how Data-Forge functions are chained is easy to rearrange and extend. We can easily
    plug in new data transformations, remove ones we no longer need, or modify the
    existing ones.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 Data-Forge 的一个主要原因是它能够将操作链式连接起来，快速构建灵活的数据管道。我说灵活，是因为 Data-Forge 函数链的语法很容易重新排列和扩展。我们可以轻松地插入新的数据转换，移除不再需要的，或者修改现有的。
- en: '![c06_11.eps](Images/c06_11.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![c06_11.eps](Images/c06_11.png)'
- en: Figure 6.11 Splitting a single file into multiple files by country
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 按国家拆分单个文件到多个文件
- en: Throughout this chapter you have been building an understanding of how Data-Forge
    chaining works and, I hope, an appreciation of the power it can bring to your
    data-wrangling toolkit, but I’d like to make this more explicit now. Let’s look
    at a new Data-Forge example that’s combined from a number of the previous code
    listings. It shows how these transformations can be chained together into a single
    data pipeline.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你一直在构建对 Data-Forge 链式操作的理解，我希望你也能欣赏到它能为你的数据处理工具箱带来的力量，但现在我想使这一点更加明确。让我们看看一个新的
    Data-Forge 示例，它是从多个之前的代码列表中组合而成的。它展示了这些转换如何被链式组合成一个单一的数据管道。
- en: 'The code for the more complex data pipeline is shown in [listing 6.12](#listing6.12).
    You can see many of the functions we’ve looked at so far in this chapter: `where`,
    `groupBy`, `select`, plus several others. You can run the following listing and
    check the output file data-pipeline-output.csv that it generates.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的数据管道的代码显示在 [列表 6.12](#listing6.12) 中。你可以看到本章中我们探讨过的许多函数：`where`、`groupBy`、`select`
    以及其他几个。你可以运行以下列表并检查它生成的输出文件 data-pipeline-output.csv。
- en: Listing 6.12 A more complex data pipeline constructed with Data-Forge (extract
    from listing-6.12.js)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.12 使用 Data-Forge 构建更复杂的数据管道（从 listing-6.12.js 中提取）
- en: '[PRE12]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this chapter, we covered quite a bit of ground, and we learned various techniques
    for cleaning and preparing our data before trying to use it for analysis or move
    it to production. Later in chapter 9 we’ll get into the actual data analysis,
    but first we need to deal with something we’ve avoided until now: How can we cope
    with a huge amount of data? That’s the topic of chapters 7 and 8, coming up next.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们覆盖了相当多的内容，我们学习了在尝试用于分析或将其移至生产之前清理和准备数据的各种技术。在第 9 章中，我们将进入实际的数据分析，但首先我们需要处理我们至今为止一直避免的事情：我们如何应对大量数据？这是第
    7 章和第 8 章的主题，接下来即将到来。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: You learned to use the JavaScript `map` function and the Data-Forge `select`
    function to rewrite your data set to repair bad data.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经学会了使用 JavaScript 的 `map` 函数和 Data-Forge 的 `select` 函数来重写你的数据集以修复坏数据。
- en: You learned to use various other functions to filter out problematic or irrelevant
    data. We looked at the JavaScript `filter` function, `delete` operator, and the
    Data-Forge `where` and `dropSeries` functions.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经学会了使用各种其他函数来过滤掉有问题的或不相关的数据。我们探讨了 JavaScript 的 `filter` 函数、`delete` 操作符以及
    Data-Forge 的 `where` 和 `dropSeries` 函数。
- en: We looked at examples of aggregation to summarize and reduce your data set.
    We used the JavaScript `reduce` function and Data-Forge’s `groupBy` and `aggregate`
    functions.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们研究了聚合的示例，以总结和减少你的数据集。我们使用了JavaScript的`reduce`函数以及Data-Forge的`groupBy`和`aggregate`函数。
- en: We merged data from multiple files using the `globby`*library.*
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`globby`*库*将来自多个文件的数据合并在一起。
- en: '**   We split data out to multiple files based on criteria. We used the JavaScript
    `filter` function and the Data-Forge `where` function.**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**   我们根据标准将数据拆分到多个文件中。我们使用了JavaScript的`filter`函数和Data-Forge的`where`函数。**'
