- en: 5 Routing log events
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 路由日志事件
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Copying log events to send to multiple outputs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志事件复制到发送到多个输出
- en: Routing log events using tags and labels
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标签和标签路由日志事件
- en: Observing filters and handling errors in Fluentd
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Fluentd中观察过滤器并处理错误
- en: Applying inclusions to enable reuse of configurations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用包含以实现配置的重用
- en: Injecting extra context information into log events
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将额外的上下文信息注入日志事件
- en: So far in this book, we have seen how to capture and store log events. But in
    all the examples, routing was simply all events going to the same output. However,
    this can be far from ideal. As described in chapter 1, we may want log events
    to go to different tools, depending on the type of log event. It may be desirable
    to send a log event to multiple locations or none. In this chapter, we will, therefore,
    examine the different ways we can route events. In addition, we will look at some
    smaller features that can contribute to solving the challenges of routing, such
    as adding information into the log event to ensure the origin of the log event
    is not lost along the way.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经看到了如何捕获和存储日志事件。但在所有示例中，路由都是简单地所有事件都发送到同一个输出。然而，这可能远非理想。正如第1章所述，我们可能希望日志事件根据日志事件的类型发送到不同的工具。可能希望将日志事件发送到多个位置或一个都不发送。因此，在本章中，我们将探讨我们可以路由事件的不同方式。此外，我们还将查看一些较小的功能，这些功能可以帮助解决路由的挑战，例如在日志事件中添加信息以确保日志事件的来源在传输过程中不会丢失。
- en: Routing often aligns with how work is split among individuals or teams. As we
    will see, the use of inclusions supports how multiple teams can each work on their
    part of a Fluentd configuration without interrupting others and injecting specific
    configuration values. For example, we have seen the security team needing to apply
    routing and filtering of log events to their tool (and exclude events they’re
    not interested in). In contrast, the Ops team needs the log events in a different
    tool. With the routing and inclusion features, we can quickly achieve this.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 路由通常与个人或团队之间如何分配工作相一致。正如我们将看到的，使用包含功能支持多个团队可以各自在其Fluentd配置的部分上工作，而不会打扰他人并注入特定的配置值。例如，我们已经看到安全团队需要将日志事件的路由和过滤应用于他们的工具（并排除他们不感兴趣的事件）。相比之下，运维团队需要不同的工具中的日志事件。通过路由和包含功能，我们可以快速实现这一点。
- en: The one aspect of routing we will not address in this chapter is the idea of
    forwarding log events to other Fluentd nodes, as that is best addressed when we
    look at scaling later in the book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会涉及路由的一个方面是将日志事件转发到其他Fluentd节点，因为这最好在我们稍后查看扩展时解决。
- en: 5.1 Reaching multiple outputs by copying
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 通过复制达到多个输出
- en: One way to get log events to all the correct output(s) is to ensure that all
    outputs receive the event, and each output includes one or more filters to stop
    unwanted content from being output. We’ll focus on copying in this section and
    will address filtering later, as before we filter things, we need to get the log
    events to the right place.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志事件发送到所有正确输出（s）的一种方法是通过确保所有输出都接收事件，并且每个输出包含一个或多个过滤器以阻止不需要的内容被输出。在本节中，我们将重点关注复制，稍后我们将讨论过滤，因为在过滤事物之前，我们需要将日志事件带到正确的位置。
- en: As described in chapter 2, log events are, by default, consumed by the first
    appropriate *match* directive, containing the output plugin. To allow a log event
    to reach more than one output plugin within a match directive, we need to use
    the copy plugin (*@copy)*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如第2章所述，日志事件默认情况下由第一个合适的*match*指令消费，其中包含输出插件。为了允许日志事件在匹配指令内到达多个输出插件，我们需要使用复制插件（*@copy*）。
- en: Each destination is held within a *store* declaration defined with XML style
    tags `<store>` and `</store>` within the match directive. While *store* may not
    always seem intuitive as a plugin name (many outputs are for solutions we wouldn’t
    associate with storage, like Grafana), it is worth remembering that more of the
    Fluentd plugins address the retrieval and storage of log events than anything
    else. The diagram in figure 5.1 illustrates how the directive and plugins relate
    to each other both logically and in the way the configuration file is written.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个目的地都包含在匹配指令中定义的*store*声明中，该声明使用XML风格的标签`<store>`和`</store>`。虽然*store*可能并不总是作为一个插件名称看起来直观（许多输出是为了我们不会将其与存储关联的解决方案，如Grafana），但值得记住的是，Fluentd的更多插件是针对日志事件的检索和存储，而不是其他任何事情。图5.1中的图示说明了指令和插件在逻辑上以及配置文件编写方式上的相互关系。
- en: '![](../Images/CH05_F01_Wilkins.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F01_Wilkins.png)'
- en: Figure 5.1 Visualization of the hierarchy of elements for a match directive
    using @copy and Store. Reading from left to right, we see the blocks of configuration
    with increasing detail and focus (i.e., Buffer or Formatter for a specific plugin
    type). The store configuration block can occur one or more times within the copy
    plugin.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 使用 @copy 和 Store 的匹配指令元素层次结构可视化。从左到右阅读，我们看到配置块越来越详细和专注（即特定插件类型的缓冲区或格式化器）。存储配置块可以在复制插件内部出现一次或多次。
- en: Within each store configuration block, we can configure the use of a plugin.
    Typically, this is going to be an output plugin but could easily be a filter plugin.
    The store plugin’s attributes can be configured just as they would if used directly
    within a *match* directive, as we have done previously. This includes using helper
    plugins, such as *buffers*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个存储配置块中，我们可以配置插件的使用。通常，这将是一个输出插件，但也可以很容易地是一个过滤器插件。存储插件的属性可以像在 `match` 指令中直接使用时一样进行配置。这包括使用辅助插件，如
    `buffers`。
- en: To illustrate this, we’re going to take a file input, and rather than send the
    log events from one file to another file, as we did in chapter 3, we will extend
    the configuration to send the output to both a file and a stdout (console). We
    can see a representation of this in figure 5.2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将使用文件输入，而不是像第 3 章中那样将日志事件从一个文件发送到另一个文件，我们将扩展配置以将输出发送到文件和控制台。我们可以在图
    5.2 中看到这种表示。
- en: '![](../Images/CH05_F02_Wilkins.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F02_Wilkins.png)'
- en: Figure 5.2 Visualization of a configuration file using store and copy to send
    log events to multiple destinations
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 使用存储和复制将日志事件发送到多个目标配置文件的可视化
- en: To implement this, we need to edit the `match` directive. The easiest way to
    do this is to first wrap the existing output plugin attributes within the `store`
    tags and then add the next `store` start and end tags. With the `store` start
    and end tags in place, each of the output plugins can be configured. Finally,
    introduce the *@copy* at the start of the `match` directive. The modified configuration
    is shown in the following listing, which contains the two `store` blocks, each
    holding an output plugin (`file` and `stdout`). You’ll also see a third `store`
    block with the output plugin type of `null`, followed by an `@include` directive.
    We will explain these shortly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现此功能，我们需要编辑 `match` 指令。最简单的方法是首先将现有的输出插件属性包裹在 `store` 标签内，然后添加下一个 `store`
    开始和结束标签。有了 `store` 开始和结束标签，就可以配置每个输出插件。最后，在 `match` 指令的开始处引入 `@copy`。修改后的配置如下所示，其中包含两个
    `store` 块，每个块包含一个输出插件（`file` 和 `stdout`）。您还会看到一个具有 `null` 输出插件类型的第三个 `store` 块，后面跟着一个
    `@include` 指令。我们将在稍后解释这些。
- en: Listing 5.1 Chapter5/Fluentd/file-source-multi-out.conf—copy to multiple outputs
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 Chapter5/Fluentd/file-source-multi-out.conf——复制到多个输出
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Declaring the plugin to be used
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 声明要使用的插件
- en: ❷ Start of the store block—each store reflects the action to take. This is often
    done to store a log event using a plugin or forward to another Fluentd node. In
    this case, we’re simply writing to the console.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 存储块的开始——每个存储反映要采取的操作。这通常是为了使用插件存储日志事件或将事件转发到另一个 Fluentd 节点。在这种情况下，我们只是写入控制台。
- en: ❸ Third store routes to a file
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第三个存储路由到文件
- en: 'Let’s see the result of the configuration. As this uses a file source, we need
    to run the LogSimulator as well. So, to run the example, the following commands
    are needed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看配置的结果。由于这使用文件源，我们需要运行 LogSimulator。因此，要运行此示例，需要以下命令：
- en: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out.conf`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out.conf`'
- en: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
- en: After running these commands, log events will appear very quickly on the console.
    Once the buffer reaches the point of writing, files will appear with the name
    `fluentd-file-output.<date>_<number>.log`. It is worth comparing the content in
    the file to the console, as we have included additional attributes into the payload.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些命令后，日志事件将很快出现在控制台上。一旦缓冲区达到写入点，将出现名为 `fluentd-file-output.<date>_<number>.log`
    的文件。将文件内容与控制台内容进行比较是值得的，因为我们已经将额外的属性包含在有效负载中。
- en: 5.1.1 Copy by reference or by value
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 按引用或按值复制
- en: In most, perhaps even all programming languages, there is the idea of shallow
    and deep copying, sometimes called *copy by reference* (illustrated in figure
    5.3) and *copy value* (illustrated by figure 5.4). Whichever terminology you are
    used to, *copy by reference* means that the copy of the log event is achieved
    by each copy referring to the same piece of memory holding the log event. If the
    log event is modified, then that change impacts all subsequent uses for all copies.
    *Copying by value* means grabbing a new piece of memory and making a wholesale
    copy of the content. This means if one copy is modified, the other will not be
    because it is an outright clone. While we have not yet seen a reason to do anything
    other than use the default behavior, in the next chapter, we’ll see that it is
    possible to manipulate the contents of a log event.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数，也许甚至是所有编程语言中，都存在浅拷贝和深拷贝的概念，有时称为*按引用复制*（如图5.3所示）和*按值复制*（如图5.4所示）。无论您习惯于哪种术语，*按引用复制*意味着每个副本通过引用同一块内存来达到日志事件的复制。如果日志事件被修改，那么这种变化会影响所有后续使用的所有副本。*按值复制*意味着获取一个新的内存块，并复制其内容。这意味着如果一个副本被修改，另一个副本不会受到影响，因为它是一个完整的克隆。虽然我们还没有看到需要做任何其他事情的理由，但在下一章中，我们将看到可以操纵日志事件的内容。
- en: '![](../Images/CH05_F03_Wilkins.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F03_Wilkins.png)'
- en: Figure 5.3 How objects reside in memory when copied by reference
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 按引用复制时对象在内存中的存储方式
- en: As shown in figure 5.3, when Object B is created as a shallow copy of Object
    A, then they both refer to the same memory holding the inner object (Object 1).
    So if we change Object 1 when updating through Object B, we will impact Object
    A as well.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.3所示，当对象B被创建为对象A的浅拷贝时，它们都指向包含内部对象（对象1）的同一内存。因此，如果我们通过对象B更新对象1，那么也会影响对象A。
- en: '![](../Images/CH05_F04_Wilkins.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F04_Wilkins.png)'
- en: Figure 5.4 How objects reside in memory when copied by value
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 按值复制时对象在内存中的存储方式
- en: 'Within the copy configuration, we can control this behavior with the `copy_mode`
    attribute. Copy mode has several settings that range in behavior from a copy by
    reference to a copy by value:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制配置中，我们可以通过`copy_mode`属性来控制这种行为。复制模式有几种设置，其行为范围从按引用复制到按值复制：
- en: '*no_copy*—The default state, and effectively copy by reference.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*no_copy*—默认状态，实际上是按引用复制。'
- en: '*Shallow*—This deep-copies the first layer of values. If those objects, in
    turn, reference objects, they are still referencing the same memory as the original.
    Under the hood, this uses Ruby’s *dup* method. While faster than a deep copy,
    the use of dup needs to be used with care; it is comparable to *no_copy* of nested
    objects.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*浅拷贝*—这会深度复制第一层值。如果那些对象反过来又引用对象，它们仍然引用与原始对象相同的内存。在底层，这使用了Ruby的*dup*方法。虽然比深度复制快，但使用dup需要谨慎；它类似于嵌套对象的*no_copy*。'
- en: '*Deep*—This is a proper copy by value, leveraging Ruby’s *msgpack* gem. If
    in doubt, this is the approach we recommend.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深拷贝*—这是一个正确的按值复制，利用Ruby的*msgpack* gem。如果有疑问，这是我们推荐的方法。'
- en: '*Marshal*—When Ruby’s msgpack cannot be used, then native language object marshaling
    can be used. The object is marshaled (serialized) into a byte stream representation.
    Then the byte stream is unmarshaled (deserialized), and an object representing
    the byte stream is produced.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Marshal*—当Ruby的msgpack无法使用时，可以使用本地语言的物体序列化。对象被序列化（序列化）成一个字节流表示形式。然后字节流被反序列化（反序列化），并产生一个代表字节流的对象。'
- en: How copy operations work
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 复制操作的工作原理
- en: 'The following will help you better understand how copy behaviors work:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下将帮助您更好地理解复制行为的工作原理：
- en: 'Ruby dup (shallow copy): [http://mng.bz/mxOP](http://mng.bz/mxOP)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruby dup（浅拷贝）：[http://mng.bz/mxOP](http://mng.bz/mxOP)
- en: 'msgpack-ruby (deep copy): [https://rubydoc.info/gems/msgpack](https://rubydoc.info/gems/msgpack)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: msgpack-ruby（深拷贝）：[https://rubydoc.info/gems/msgpack](https://rubydoc.info/gems/msgpack)
- en: 'msgpack serialization (marshal): [https://ruby-doc.org/core-2.6.3/Marshal.html](https://ruby-doc.org/core-2.6.3/Marshal.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: msgpack序列化（marshal）：[https://ruby-doc.org/core-2.6.3/Marshal.html](https://ruby-doc.org/core-2.6.3/Marshal.html)
- en: Ideally, we shouldn’t need to worry about copying by value, as log events are
    received in a well-structured manner with all the necessary state information,
    so content manipulation becomes unnecessary. Sadly, the world is not ideal; when
    using the copy feature, consider whether the default option is appropriate; for
    example, do we need to manipulate the log event for one destination and not another?
    The use of labels to create “pipelines” of log event processing will increase
    the possibility of needing to consider how we copy as well, as we will see later
    in this chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们不需要担心按值复制，因为日志事件以良好的结构接收，并包含所有必要的状态信息，因此内容操作变得不必要。遗憾的是，世界并不完美；在使用复制功能时，请考虑默认选项是否合适；例如，我们是否需要操作一个目标而不是另一个目标的日志事件？使用标签创建“日志事件处理管道”会增加需要考虑如何复制的机会，正如我们将在本章后面看到的那样。
- en: Another consideration to be aware of is that when copying log events for different
    stores, if a log event can carry sensitive data, we may wish to redact or mask
    values for most cases, but not for the log events sent to the security department.
    If security does not wish to be impacted by any data masking or redaction, they
    will need a deep copy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的考虑因素是，在为不同的存储复制日志事件时，如果日志事件可以携带敏感数据，我们可能希望在大多数情况下对值进行编辑或屏蔽，但对于发送到安全部门的日志事件则不是这样。如果安全部门不希望受到任何数据屏蔽或编辑的影响，他们需要深度复制。
- en: 5.1.2 Handling errors when copying
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 复制时的错误处理
- en: In the example configuration we provided, both outputs are to the same local
    hardware, and it would need a set of unique circumstances that impacts one file
    and not the other. However, suppose the output is being sent to a remote service,
    such as a database or Elasticsearch. In that case, the chance of an issue impacting
    one output and not another is significantly higher. For example, if one of the
    destination services has been shut down or network issues prevent communication,
    what happens to our outputs? Does Fluentd send the log events to just the available
    stores, or to none of them unless they are all available?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提供的示例配置中，两个输出都指向相同的本地硬件，并且需要一些独特的情况才会影响一个文件而不影响另一个。然而，假设输出被发送到远程服务，例如数据库或Elasticsearch。在这种情况下，一个输出出现问题而另一个输出不受影响的可能性显著增加。例如，如果其中一个目标服务已被关闭或网络问题阻止了通信，我们的输出会发生什么？Fluentd是否会将日志事件发送到可用的存储，或者除非它们全部可用，否则不发送到任何存储？
- en: Fluentd does not try to apply *XA transactions* (also known as *two-phase commit*),
    allowing an all-or-nothing behavior because the coordination of such transactions
    is resource-intensive, and coordination takes time. However, by default, it does
    apply the next best thing; in the event of one output failing, subsequent outputs
    will be abandoned. For example, if we copy to three stores called Store A, Store
    B, and Store C, which are defined in the configuration in that order, and we fail
    to send a log event to Store A, then none of the stores will get the event (see
    the first part of figure 5.5). If the problem occurred with Store B, then Store
    A would keep the log event, but Store C would be abandoned (see the second part
    of figure 5.5.).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd不会尝试应用*XA事务*（也称为*两阶段提交*），因为它允许全有或全无的行为，因为此类事务的协调是资源密集型的，且协调需要时间。然而，默认情况下，它确实应用了次优方案；在某个输出失败的情况下，后续的输出将被放弃。例如，如果我们复制到三个存储，分别称为Store
    A、Store B和Store C，它们在配置中按此顺序定义，并且我们未能将日志事件发送到Store A，那么没有任何存储会收到该事件（参见图5.5的第一部分）。如果问题发生在Store
    B，那么Store A将保留日志事件，但Store C将被放弃（参见图5.5的第二部分）。
- en: '![](../Images/CH05_F05_Wilkins.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F05_Wilkins.png)'
- en: Figure 5.5 How a store error impacts a containing copy. The bar across the bottom
    of each diagram indicates which store would get the data value, which store failed,
    and which store was not actioned. For example, in the middle example, if Store
    B failed, then Store A will have got the log event, Store B wouldn’t have the
    event, and Store C would not be communicated with.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 存储错误如何影响包含的复制。每个图底部的条形图表示哪个存储会接收到数据值，哪个存储失败，以及哪个存储未执行操作。例如，在中间示例中，如果Store
    B失败，那么Store A将接收到日志事件，Store B不会接收到事件，而Store C则不会进行通信。
- en: But if you have a buffer as part of the output configuration, this may mask
    an issue, as the buffer may operate asynchronously and include options such as
    fallback and retry. As a result, an error, such as giving up retrying, may not
    impact the copy process, as described. Given this approach, there is the option
    to sequence the copy blocks to reflect the priority of the output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你在输出配置中有一个缓冲区，这可能会掩盖问题，因为缓冲区可能异步操作并包括回退和重试等选项。结果，如上所述，放弃重试等错误可能不会影响复制过程。鉴于这种方法，有选项可以按顺序排列复制块以反映输出优先级。
- en: The downside is that if you use asynchronous buffering with retries, the buffer
    will allow the execution to continue to the next store. But if it subsequently
    hits the maximum retries, it will fail that store, but subsequent store actions
    may have been successful.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用带有重试的异步缓冲区，缓冲区将允许执行继续到下一个存储。但如果它随后达到最大重试次数，它将失败该存储，但后续的存储操作可能已经成功。
- en: How priority/order is applied should be a function of the value of the log event
    and the output capability. For example, the use of the output plugin allows a
    *secondary* helper plugin such as *secondary_file*. If the log events are so critical
    that they cannot be lost, it is best to prioritize the local I/O options first.
    If the log event priority is to get it to a remote central service quickly (e.g.,
    Kafka or Splunk) and is failing, then that means the event is of little further
    help elsewhere (e.g., Prometheus for contributing to metrics calculations); therefore,
    it’s best to lead off with the highest priority destination.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如何应用优先级/顺序应取决于日志事件的值和输出能力。例如，使用输出插件允许使用*二级*辅助插件，如*secondary_file*。如果日志事件至关重要，以至于不能丢失，那么首先优先考虑本地I/O选项是最好的。如果日志事件的优先级是快速将其发送到远程中央服务（例如，Kafka或Splunk）并且失败，那么这意味着该事件在其他地方（例如，Prometheus用于指标计算贡献）的帮助很小；因此，最好以最高优先级的目的地开始。
- en: Fluentd does offer another option to tailor this behavior. Within the `<store>`
    declaration, it is possible to add the argument `ignore_error (`e.g., `<store
    ignore_ error>`). Then, if output in that store block does cause an error, it
    is prevented from cascading the error that would trigger subsequent store blocks
    from being abandoned. Using our example three stores again, setting `ignore_error`
    on Store A would mean that regardless of sending the event to Store A, we would
    continue to try with Store B. But if Store B failed, then Store C would not receive
    the event.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd还提供另一种选项来定制此行为。在`<store>`声明中，可以添加`ignore_error`（例如，`<store ignore_error>`）参数。然后，如果该存储块中的输出导致错误，它将阻止错误级联，从而触发后续存储块被放弃。使用我们的示例三个存储，在存储A上设置`ignore_error`意味着无论是否将事件发送到存储A，我们都会继续尝试使用存储B。但如果存储B失败，那么存储C将不会收到该事件。
- en: 5.2 Configuration reuse and extension through inclusion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 通过包含配置重用和扩展
- en: As Fluentd configurations develop, mature, and potentially introduce multiple
    processing routes for log events, our Fluentd configuration files grow in size
    and complexity. Along with this growth, we’re also likely to discover that some
    configurations could be reused (e.g., different match definitions want to reuse
    the same filter or formatter), particularly when trying to achieve the ideal of
    *DRY* (Don’t Repeat Yourself). So in this section, let’s explore how to address
    the challenges of larger configuration files and maximize reuse.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Fluentd配置的发展、成熟以及可能引入多个日志事件处理路由，我们的Fluentd配置文件在大小和复杂性上都会增长。随着这种增长，我们也很可能发现一些配置可以重用（例如，不同的匹配定义想要重用相同的过滤器或格式化器），尤其是在尝试实现*DRY*（不要重复自己）的理想时。因此，在本节中，让我们探讨如何解决大型配置文件带来的挑战并最大化重用。
- en: We could try to solve this by ensuring that the Fluentd configuration actions
    appear in the correct order. The use of tags to filter events may work. This can
    get rather messy as an approach, and small changes could disrupt the flow of log
    events in unexpected ways.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过确保Fluentd配置操作以正确的顺序出现来尝试解决这个问题。使用标签过滤事件可能有效。这种方法可能会相当混乱，并且小的更改可能会以意想不到的方式干扰日志事件的流程。
- en: The alternative is trying to massage configuration files to allow bits to be
    reused in different contexts. The first step is to isolate the Fluentd configuration
    that needs to be reused into its own file and then use the `@include` directive
    with a file name and path for wherever that configuration was needed. With the
    `include` statement, the referenced file is merged into the parent configuration
    file. This means we can reuse configurations and incorporate inclusions so that
    the Fluentd configuration doesn’t need to be manipulated, so the sequencing of
    directives is not a problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是尝试调整配置文件，以便在不同的上下文中重复使用部分内容。第一步是将需要重复使用的 Fluentd 配置隔离到自己的文件中，然后使用 `@include`
    指令和文件名及路径，在需要该配置的任何地方使用。使用 `include` 语句，引用的文件将被合并到父配置文件中。这意味着我们可以重复使用配置和包含，这样就不需要操作
    Fluentd 配置，指令的顺序也就不是问题。
- en: During Fluentd’s startup, the configuration file is parsed and has the inclusion
    directive replaced with a copy of the included file’s contents. This way, we can
    include the same configuration file wherever it is needed. For example, if we
    have an enterprise-wide setup for Elasticsearch, then all the different Fluentd
    configurations can reference a single file for using the enterprise Elasticsearch,
    and changes, for instance, optimizing connection settings, can then be applied
    to one file. Everyone inherits the change when the configuration file is deployed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fluentd 启动期间，配置文件会被解析，并将包含指令替换为所包含文件的副本内容。这样，我们可以在需要的地方包含相同的配置文件。例如，如果我们有一个针对
    Elasticsearch 的企业级设置，那么所有不同的 Fluentd 配置都可以引用单个文件来使用企业 Elasticsearch，并且可以针对单个文件应用更改，例如优化连接设置。当配置文件部署时，每个人都会继承这些更改。
- en: An *inclusion* does not have to contain a complete configuration; it can easily
    contain a single attribute. An excellent example of this is where you want to
    reuse some common Ruby (e.g., retrieving some security credentials) logic into
    the Fluentd configuration, as we’ll discuss later. Equally, an inclusion file
    may be used to inject a block of configuration, such as a `store` block or even
    an entire additional configuration file that could also be used independently.
    In listing 5.2, we have added several inclusions by introducing `@include additionalStore.conf`
    after the last `store` tag defines additional store configurations from a separate
    file. This means we could define a common destination for all our log events and
    repeat the configuration across this and other configuration files to log all
    events in a common place, and then allow the configuration to focus on the destinations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*包含* 不必包含完整的配置；它可以很容易地包含单个属性。一个很好的例子是，当你想要将一些常见的 Ruby 逻辑（例如，检索一些安全凭证）重复使用到
    Fluentd 配置中，正如我们稍后将要讨论的。同样，包含文件也可以用来注入配置块，例如 `store` 块，甚至是一个完整的附加配置文件，该文件也可以独立使用。在列表
    5.2 中，我们通过在最后一个 `store` 标签定义的附加存储配置之后引入 `@include additionalStore.conf`，添加了几个包含项。这意味着我们可以为所有日志事件定义一个共同的目的地，并在其他配置文件中重复配置，以便在共同的位置记录所有事件，然后允许配置专注于目的地。'
- en: Listing 5.2 Chapter5/Fluentd/file-source-multi-out2.conf—illustration of inclusion
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 Chapter5/Fluentd/file-source-multi-out2.conf—包含示例
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Incorporates the external file into the configuration providing an additional
    store declaration
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将外部文件整合到配置中，提供额外的存储声明
- en: ❷ Brings a complete configuration set that could be separately run if desired
    or could be reused
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提供了一个完整的配置集，如果需要可以单独运行，或者可以重复使用
- en: We have also added an inclusion directive referencing the file `record-origin
    .conf`. This illustrates the possibility that when multiple teams contribute functionality
    into a single run-time environment (e.g., a J2EE server), rather than all the
    teams trying to maintain a single configuration file and handling change collisions,
    each team has its own configuration file. But come execution time, a single configuration
    file uses inclusions to bring everything together. As a result, the Fluentd node
    needs to merge all the configurations together during startup. Within the `record-origin
    .conf` (if you review the content of `record-origin.conf`), we have introduced
    some new plugins, which we will cover later in the chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了一个引用文件 `record-origin .conf` 的包含指令。这说明了当多个团队向单个运行时环境（例如，J2EE 服务器）贡献功能时，而不是所有团队都试图维护单个配置文件并处理冲突，每个团队都有自己的配置文件。但在执行时间，单个配置文件使用包含来将所有内容组合在一起。因此，Fluentd
    节点需要在启动时合并所有配置。在 `record-origin .conf`（如果您查看 `record-origin.conf` 的内容），我们引入了一些新的插件，我们将在本章后面介绍。
- en: 'Let’s see the result of the configuration. As this uses a file source, we need
    to run the LogSimulator as well. So, to run the example, the following commands
    are needed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看配置的结果。由于这使用文件源，我们需要运行 LogSimulator。因此，要运行此示例，需要以下命令：
- en: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out2.conf`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out2.conf`'
- en: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
- en: NOTE It is important to remember that the content of an inclusion can have an
    impact on the configuration, which has the `include` declaration. So the placement
    and use of inclusions must be done with care, as the finalized order of directives
    and their associated plugins is still applicable, as highlighted in chapter 2.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：重要的是要记住，包含的内容可能会影响具有 `include` 声明的配置。因此，包含的位置和使用必须谨慎处理，因为指令和它们相关插件的最终顺序仍然适用，如第
    2 章中所述。'
- en: If the path to the included file is relative in the `include` statement, then
    the point of reference is the file’s location with the `include` directive. The
    `include` directive can use a comma-separated list, in which case the list order
    relates to the insertion sequence—for example, `@include file.conf, file2.conf`
    means `file.conf` is included before `file2.conf`. If the `include` directive
    uses wildcards (e.g., `@include *.conf`), then the order of insertion is alphabetical.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `include` 语句中包含文件的路径是相对的，那么参考点是带有 `include` 指令的文件位置。`include` 指令可以使用逗号分隔的列表，在这种情况下，列表顺序与插入顺序相关——例如，`@include
    file.conf, file2.conf` 表示 `file.conf` 在 `file2.conf` 之前被包含。如果 `include` 指令使用通配符（例如，`@include
    *.conf`），则插入顺序是按字母顺序的。
- en: Figure 5.6 shows the dry-run output and highlights where `include` declarations
    have been replaced with the included configuration or configuration fragment contents.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 显示了干运行输出，并突出显示了 `include` 声明已被替换为包含的配置或配置片段内容。
- en: '![](../Images/CH05_F06_Wilkins.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Wilkins 图](../Images/CH05_F06_Wilkins.png)'
- en: Figure 5.6 Configuration file with the include resolved (highlighted in the
    box) as Fluentd starts up
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 显示了 Fluentd 启动时解析的包含文件（框内高亮显示）
- en: NOTE As the process is a purely textual substitution, it does mean that the
    inclusion can easily be an empty placeholder file or a configuration fragment.
    If the inclusion is injected in the wrong place within a file, it can invalidate
    the entire configuration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：由于这个过程是纯文本替换，这意味着包含可以很容易地是一个空的占位符文件或配置片段。如果包含在文件中的位置不正确，可能会使整个配置无效。'
- en: 5.2.1 Place holding with null output
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 使用空输出进行占位
- en: In listing 5.2, the additional inclusion fragment (`@include additionalStore
    .conf`) provided the configuration fragment shown in listing 5.3\. This `store`
    definition uses the null output plugin; it simply discards the log events it receives.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 5.2 中，提供的附加包含片段 (`@include additionalStore .conf`) 如列表 5.3 所示。这个 `store`
    定义使用了空输出插件；它简单地丢弃接收到的日志事件。
- en: Placing null plugins when working in an environment where different teams may
    wish to output log events to different tools allows developers to build a service
    to put the placeholder in the Fluentd configuration ready for the other team(s)
    to replace. In many respects, the use of null is the nearest thing to adding a
    `TODO` code comment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个可能希望将日志事件输出到不同工具的不同团队的环境中工作时，放置空插件允许开发者构建一个服务，在 Fluentd 配置中放置占位符，以便其他团队替换。在许多方面，使用空插件是添加
    `TODO` 代码注释的近似方法。
- en: NOTE `TODO` is a common tag used in code to flag when something still needs
    to be done.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `TODO` 是在代码中标记还有事情要做的一个常用标签。
- en: Listing 5.3 Chapter5/Fluentd/additionalStore.conf—include configuration fragment
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 Chapter5/Fluentd/additionalStore.conf—包含配置片段
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 5.2.2 Putting inclusions with a MongoDB output into action
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 将 MongoDB 输出包含到实际操作中
- en: Let’s apply some of the insights to this scenario. Knowing where best to apply
    effort is best driven by analytical insights. Directing error events into a database
    makes it easy to get statistics over time showing what errors occur and how frequently.
    When combined with an appreciation of the impact of an error, the effort can be
    targeted with maximum value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将一些见解应用到这个场景中。知道在哪里最好地应用努力最好是由分析洞察驱动的。将错误事件直接导向数据库使得获取随时间显示发生错误及其频率的统计数据变得容易。当与对错误影响的欣赏相结合时，可以以最大价值的目标来定位努力。
- en: We need to apply this to `Chapter5/Fluentd/file-source-multi-out.conf`. To help
    with this, the work from chapter 4, where we used Fluentd with a MongoDB plugin,
    can be leveraged. We can capitalize on it to see the impact of copy errors and
    the use of the `ignore_error` option. To do this, create a copy of the `Chapter5/Fluentd/file-source-multi-out.conf`
    that can be safely modified. For simplicity, let’s call this copy `Chapter5/Fluentd/file-source-multi-out-exercise
    .conf`. We need to replace the `@type null` with the configuration for MongoDB
    output. The commands you will need to run the scenario are
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将此应用到 `Chapter5/Fluentd/file-source-multi-out.conf`。为此，可以利用第 4 章中的工作，当时我们使用了
    MongoDB 插件的 Fluentd。我们可以利用它来观察复制错误和使用 `ignore_error` 选项的影响。为此，创建一个 `Chapter5/Fluentd/file-source-multi-out.conf`
    的副本，可以安全地进行修改。为了简单起见，让我们称这个副本为 `Chapter5/Fluentd/file-source-multi-out-exercise.conf`。我们需要将
    `@type null` 替换为 MongoDB 输出配置。运行场景所需的命令是
- en: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out-exercise.conf`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter5/Fluentd/file-source-multi-out-exercise.conf`'
- en: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy ./Chapter5/SimulatorConfig/log-source-1.properties`'
- en: 'With the changes applied, we should be able to complete the following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 应用更改后，我们应该能够完成以下步骤：
- en: Check the configuration using the dry-run capability. This should yield a valid
    result.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用干运行功能检查配置。这应该会产生一个有效结果。
- en: Confirm that the modified configuration produces the desired result by starting
    MongoDB and rerunning the LogSimulator and Fluentd.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过启动 MongoDB 并重新运行 LogSimulator 和 Fluentd 来确认修改后的配置产生了预期的结果。
- en: Verify the behavior is as expected if we cannot connect to MongoDB, and repeat
    the same actions for running the LogSimulator and Fluentd.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们无法连接到 MongoDB，请验证行为是否符合预期，并重复执行运行 LogSimulator 和 Fluentd 的相同操作。
- en: The previous step should have highlighted the absence of the `ignore_error`
    option. Modify the Fluentd configuration adding the `ignore_error` option to the
    console output configuration. Rerun the configuration and LogSimulator. Confirm
    that the desired behavior is now correct.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前一个步骤应该已经突出了 `ignore_error` 选项的缺失。修改 Fluentd 配置，将 `ignore_error` 选项添加到控制台输出配置中。重新运行配置和
    LogSimulator。确认现在所需的行为是正确的。
- en: Answers
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The modified Fluentd configuration file should now look like `Chapter5/ExerciseResults/file-source-multi-out-Answer1.conf`
    and yield a successful dry run.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改后的 Fluentd 配置文件现在应该看起来像 `Chapter5/ExerciseResults/file-source-multi-out-Answer1.conf`，并且能够成功执行干运行。
- en: With MongoDB running, the database should continue to fill with events that
    reflect the content sent to the file, and the console will still display content.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 MongoDB 运行时，数据库应继续填充反映发送到文件的内容的事件，并且控制台仍然会显示内容。
- en: With MongoDB stopped, the output plugin will start realizing errors, as there
    is no configuration to ensure the issue does not cascade to impact other plugins.
    None of the output streams will produce log events. This is because of the default
    position that subsequent output plugins should not be executed once an error occurs.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当MongoDB停止时，输出插件将开始出现错误，因为没有配置来确保问题不会级联影响其他插件。没有任何输出流会产生日志事件。这是因为默认位置是，一旦发生错误，后续输出插件不应执行。
- en: With the `ignore_error` added to the configuration, the configuration should
    now resemble `Chapter5/ExerciseResults/file-source-multi-out-Answer2.conf`. With
    the MongoDB still stopped, the MongoDB output will fail, but the failure will
    not stop output to the console but will inhibit output to the file.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置中添加了`ignore_error`后，配置现在应类似于`Chapter5/ExerciseResults/file-source-multi-out-Answer2.conf`。由于MongoDB仍然停止，MongoDB输出将失败，但失败不会停止控制台输出，但会阻止文件输出。
- en: 5.3 Injecting context into log events
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 将上下文注入日志事件
- en: Providing more information and context can help us work with log events. To
    do this, we may need to manipulate the predefined log event attributes and capture
    additional Fluentd values. This section looks at this in more detail.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 提供更多信息和环境可以帮助我们处理日志事件。为此，我们可能需要操作预定义的日志事件属性并捕获额外的Fluentd值。本节将更详细地探讨这一点。
- en: By injecting this information into the log event as identifiable log event attributes,
    we can then reference the values explicitly when trying to exclude directives
    with a filter, which will prevent log events from being processed any further
    in a sequence of events. For example, suppose log events associated with a specific
    host are deemed unnecessary to be forwarded by comparing the attribute set with
    the hostname. In that case, we can apply a filter with an exclude directive to
    stop the information from going anywhere.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此信息注入日志事件作为可识别的日志事件属性，我们可以在尝试使用过滤器排除指令时显式引用这些值，这将防止日志事件在事件序列中进一步处理。例如，假设与特定主机关联的日志事件被认为是不必要的，可以通过将属性集与主机名进行比较来停止信息传递。在这种情况下，我们可以应用一个带有排除指令的过滤器来阻止信息传递。
- en: The *inject* operation can only be used with match and filter directives, which
    is unfortunate, as we might want to apply it at the source. That said, it is not
    a significant challenge to overcome if desired, as we will see shortly. Using
    our example configuration `Chapter5/Fluentd/record-origin.conf,` we can see the
    injection at work in listing 5.4.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*注入*操作只能与匹配和过滤指令一起使用，这很遗憾，因为我们可能希望在源处应用它。尽管如此，如果需要，这并不是一个重大的挑战，正如我们很快就会看到的。使用我们的示例配置`Chapter5/Fluentd/record-origin.conf`，我们可以在列表5.4中看到注入的工作情况。'
- en: When configuring the injection of time data, it is possible to configure different
    representations of the time. This is covered by the `time_type` attribute, which
    accepts values for
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置时间数据注入时，可以配置时间的不同表示形式。这由`time_type`属性处理，它接受以下值
- en: '*String*—Allows a textual representation to be used and defers to the `time_
    format` attribute for the representation. The `time_format` uses the standard
    notation, as described in appendix A.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字符串*—允许使用文本表示，并委托给`time_format`属性进行表示。`time_format`使用标准表示法，如附录A所述。'
- en: '*Float*—Seconds and nanoseconds from the epoch (e.g., `1510544836 .154709804`).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*浮点数*—从纪元开始的秒和纳秒（例如，`1510544836.154709804`）。'
- en: '*Unixtime*—This is the traditional seconds from epoch representation.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Unix时间*—这是从纪元开始的传统的秒表示。'
- en: In listing 5.4, we have gone for the most readable format of the string. In
    addition to describing the time data format, it is possible to specify the time
    as *localtime* or as *UTC* time by including the attributes `localtime` and `utc`,
    which take Boolean values. Trying to set both attributes could be the source of
    a lot of problems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.4中，我们选择了最易读的字符串格式。除了描述时间数据格式外，还可以通过包含`localtime`和`utc`属性来指定时间作为本地时间或UTC时间，这些属性接受布尔值。尝试设置这两个属性可能是许多问题的来源。
- en: Listing 5.4 Chapter5/Fluentd/record-origin.conf—Inject declaration
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 Chapter5/Fluentd/record-origin.conf—注入声明
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The inject declaration within the generic match
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在通用匹配中注入声明
- en: ❷ Adds the name of the host of Fluentd and calls the value the name provided
    (e.g., hostName)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加Fluentd的主机名并按提供的名称（例如，hostName）调用值
- en: ❸ Adds the worker_id and calls it by the name provided. This helps when Fluentd
    has support processes to share the work across.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加worker_id并按提供的名称调用它。这有助于当Fluentd有支持进程来分担工作时。
- en: ❹ Puts the tag into the record output and uses the name provided
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将标签放入记录输出并使用提供的名称
- en: ❺ Provides the name for the time to be included with
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 提供要包含的时间名称
- en: ❻ Defines how the date-time should be represented. Here we are saying to provide
    a textual representation, but as we’ve omitted a value for time_format to define
    the format, use the standard format.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义日期时间的表示方式。在这里，我们说的是提供文本表示，但由于我们省略了用于定义格式的 `time_format` 值，因此使用标准格式。
- en: The properties for the `inject` configuration relate to the mapping of known
    values like `hostname`, `tag`, and so on, to attributes in the log event record.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`inject` 配置的属性与将已知值（如 `hostname`、`tag` 等）映射到日志事件记录中的属性相关。'
- en: To see this configuration in action, we have used the `monitor_agent` and `stdout`,
    so all we need to do is run Fluentd with the command `fluentd` `-c ./Chapter5/fluentd/record-origin.conf`.
    The outcome will appear in the console, something like
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这个配置的实际效果，我们使用了 `monitor_agent` 和 `stdout`，所以我们只需要运行 Fluentd，命令为 `fluentd
    -c ./Chapter5/fluentd/record-origin.conf`。结果将出现在控制台，类似于以下内容
- en: '[PRE4]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Within this output, you will see that the injected values appear at the end
    of the JSON structure using the names defined by the attributes; for example,
    `"hostName": "Cohen`", where `Cohen` is the PC used to write this book.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个输出中，您将看到注入的值使用属性定义的名称出现在 JSON 结构的末尾；例如，`"hostName": "Cohen"`，其中 `Cohen`
    是编写这本书所使用的电脑。'
- en: 5.3.1 Extraction of values
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 值提取
- en: 'If we can inject certain values into the log event’s record, then it seems
    obvious that there should be a counter capability for extracting values from the
    record to set the tag and timestamp of the log event. This ability can be exploited
    by plugins that work with source, filter, and match directives. This gives us
    a helpful means to set tags dynamically based on the log event record content.
    Dynamically setting tags makes tag-based routing very flexible. For example, if
    the log event had an attribute called `source`, and we wanted to use that as a
    means to perform routing, we could use the *extract* operation. For example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将某些值注入到日志事件的记录中，那么很明显，应该有一个计数器功能来从记录中提取值以设置日志事件的标签和时间戳。这种能力可以通过与源、过滤器和匹配指令一起工作的插件来利用。这为我们提供了一种根据日志事件记录内容动态设置标签的有用手段。动态设置标签使得基于标签的路由非常灵活。例如，如果日志事件有一个名为
    `source` 的属性，而我们想将其用作路由的手段，我们可以使用 *extract* 操作。例如：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Unfortunately, only a subset of the plugins available takes advantage of the
    *extract* helper. One of the core plugins that does incorporate this *exec, which*
    we have not covered yet. So as we explore tag-based routing in the next section,
    we’ll use `exec`, and we will explore the interesting opportunities it offers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，只有一小部分可用的插件利用了 *extract* 辅助功能。其中一个是核心插件，它确实包含了我们尚未涉及的 *exec, which*。因此，当我们探索下一节的基于标签的路由时，我们将使用
    `exec`，并探索它提供的有趣机会。
- en: 5.4 Tag-based routing
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 基于标签的路由
- en: In all the chapters so far, we have always had wildcards in the *match* declarations
    (e.g., `<match *>`), but we have had the opportunity to define and change the
    tag values at different stages. We have seen the tag being manipulated in contexts
    ranging from taking the tag value from the URI to setting the tag within the configuration
    and even extracting the tag from the log event record, as just discussed. We can
    use the tags to control which directives are actioned, which is the subject of
    this section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在到目前为止的所有章节中，我们总是在 *match* 声明中使用了通配符（例如，`<match *>`），但我们有机会在不同的阶段定义和更改标签值。我们已经看到标签在从
    URI 中获取标签值到在配置中设置标签，甚至是从日志事件记录中提取标签的上下文中被操作。我们可以使用标签来控制哪些指令被执行，这是本节的主题。
- en: We can control which directives will process and consume log events by defining
    the match values more explicitly. For example, a configuration for two inputs
    called AppA and AppB includes the `tag` attribute setting the respective tags
    to be `AppA` and `AppB`. Now, rather than `match *`, we set the directives to
    be `<match AppA>` and `<match AppB>`. With this change, the match directives will
    only process log events from the associated source.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过更明确地定义匹配值来控制哪些指令将处理和消费日志事件。例如，一个名为 AppA 和 AppB 的两个输入配置包括设置相应标签为 `AppA`
    和 `AppB` 的 `tag` 属性。现在，而不是 `match *`，我们将指令设置为 `<match AppA>` 和 `<match AppB>`。随着这一变化，匹配指令将只处理来自相关源的日志事件。
- en: In our example, to keep the sources simple, we have configured two occurrences
    of the `dummy` source plugin to generate log events. We have added additional
    attributes to control the behavior to repeat at different frequencies (with the
    `rate` attribute representing the number of seconds between each log event generated)
    and different messages (`dummy` attribute).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，为了保持源简单，我们已配置了两个 `dummy` 源插件的实例以生成日志事件。我们添加了额外的属性来控制在不同频率（`rate` 属性表示每个生成的日志事件之间的秒数）和不同消息（`dummy`
    属性）上的行为。
- en: In the following listing, we show the key elements of the configuration (we
    have removed some configuration elements for clarity; this can be seen with the
    use of an ellipsis [. . .]).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们展示了配置的关键元素（为了清晰起见，我们删除了一些配置元素；这可以通过省略号 [...] 来看到）。
- en: Listing 5.5 Chapter5/Fluentd/monitor-file-out-tag-match.conf—tag matching
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 第 5 章/Fluentd/monitor-file-out-tag-match.conf—标签匹配
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The first of two source definitions in this configuration file, but note that
    the port numbers are different, along with several other configuration attributes,
    so the sources are easy to distinguish.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 此配置文件中的两个源定义中的第一个，但请注意端口号不同，以及几个其他配置属性，因此源很容易区分。
- en: ❷ The second self_monitor source configuration. Most crucially, note the tag
    name differences between the sources.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个 self_monitor 源配置。最重要的是，注意源之间的标签名称差异。
- en: ❸ The first of two match declarations. Note how we can use wildcard characters
    so partial name matching can be defined.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 两个匹配声明中的第一个。注意我们如何使用通配符字符来定义部分名称匹配。
- en: ❹ File output configuration mapping to different output files for each match
    (compare the path attributes)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将文件输出配置映射到每个匹配的不同输出文件（比较路径属性）
- en: ❺ The second match, this time without any wildcarding
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第二次匹配，这次没有任何通配符
- en: This setup can be run with the command
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令运行此设置
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output files should reflect the different dummy messages, as the routing
    will have directed from the relevant source.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件应反映不同的虚拟消息，因为路由将已从相关源导向。
- en: Despite the naming, it is still possible to use selective wildcarding with the
    tags. If we extend this example by adding an additional source and tagging it
    `AppAPart2`, we could catch `AppA` and `AppAPart2`. This is done by modifying
    the `<match AppA>` to become `<match AppA*>`. The log events captured from the
    new source would be incorporated into the `AppA` output.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管命名如此，仍然可以使用选择性的通配符与标签一起使用。如果我们通过添加一个额外的源并标记为 `AppAPart2` 来扩展此示例，我们可以捕获 `AppA`
    和 `AppAPart2`。这是通过将 `<match AppA>` 修改为 `<match AppA*>` 来实现的。从新源捕获的日志事件将被纳入 `AppA`
    输出。
- en: This is illustrated in listing 5.6\. If we do not want to reintroduce wildcard
    use, we can also utilize a comma-separated tag list in the match declaration;
    for example, `<match AppA, AppAPart2>`. To illustrate the wildcard behavior, this
    time we have introduced another source plugin called *exec*. The exec plugin allows
    us to call OS scripts and capture the result. We are simply using the more command
    (as it behaves the same way for Linux and Windows) within the exec statement.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这在列表 5.6 中得到了说明。如果我们不想重新引入通配符的使用，我们也可以在匹配声明中使用逗号分隔的标签列表；例如，`<match AppA, AppAPart2>`。为了说明通配符的行为，这次我们引入了另一个名为
    *exec* 的源插件。exec 插件允许我们调用 OS 脚本并捕获结果。我们只是在 exec 语句中使用 more 命令（因为它在 Linux 和 Windows
    上表现相同）。
- en: Listing 5.6 Chapter5/Fluentd/monitor-file-out-tag-match2.conf—tag matching
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 第 5 章/Fluentd/monitor-file-out-tag-match2.conf—标签匹配
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Original source, which remains unaltered
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 原始源，保持未更改
- en: ❷ The additional source, using the exec source plugin
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 exec 源插件添加的额外源
- en: ❸ The original AppB source, which remains unchanged
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 原始 AppB 源，保持不变
- en: ❹ The match for AppB remains unmodified.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ AppB 的匹配保持未修改。
- en: ❺ The original match for AppA has now been modified to include the wildcard,
    which means both AppA and AppAPart2 will be matched. This could described also
    be expressed as <match AppA, AppAPart2>.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ AppA 的原始匹配现在已被修改以包含通配符，这意味着 AppA 和 AppAPart2 都将被匹配。这也可以表示为 <match AppA, AppAPart2>。
- en: This setup can be run with the command
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令运行此设置
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output files should reflect the different dummy messages, but the `AppA`
    output should now include the outcome of executing the OS command on a predefined
    test data file`.`
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件应反映不同的虚拟消息，但 `AppA` 输出现在应包括在预定义测试数据文件上执行 OS 命令的结果。
- en: Tag naming convention
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 标签命名约定
- en: Despite using wildcard characters to help select tags for different directives
    regardless of the position, there is a convention normally applied. Tag naming
    typically follows a namespace-like hierarchy using the dot to break the hierarchy
    tiers (e.g., `AppA.ComponentB.SubComponentC`). Now the wildcard can filter the
    different namespaces (e.g., `AppA.*` or `AppA.ComponentB.*`). For example, if
    we had a web server hosting a domain with several different services, with each
    service potentially having one or more log outputs, we might see a convention
    of `webserver.service .outputName` in the tag convention.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用通配符字符来帮助选择不同指令的标签，而不考虑其位置，但通常有一个约定。标签命名通常遵循类似于命名空间的层次结构，使用点来分隔层次结构层（例如，`AppA.ComponentB.SubComponentC`）。现在通配符可以过滤不同的命名空间（例如，`AppA.*`
    或 `AppA.ComponentB.*`）。例如，如果我们有一个托管多个不同服务的域的 Web 服务器，每个服务可能有一个或多个日志输出，我们可能会在标签约定中看到
    `webserver.service .outputName` 的约定。
- en: 5.4.1 Using exec output plugin
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 使用 exec 输出插件
- en: 'The exec plugin illustrated in listing 5.6 creates some interesting opportunities.
    When plugins cannot help us get the information required, we have several options:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 中所示的 exec 插件提供了一些有趣的机会。当插件无法帮助我们获取所需的信息时，我们有几种选择：
- en: Build a custom plugin (which will be explored later in the book).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个自定义插件（本书稍后将会探讨）。
- en: Create an independent utility that can feed data to Fluentd directly via HTTP,
    UDP, forward plugins.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个独立的实用程序，可以直接通过 HTTP、UDP、转发插件将数据馈送到 Fluentd。
- en: Produce a small script that can be invoked by the exec plugin.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个可以由 exec 插件调用的小型脚本。
- en: Using the exec plugin makes it easy to retrieve environment-specific information
    or perform things like grabbing web page output using utilities like *Wget* and
    *cURL*—a modern version of screen scraping. The latter is particularly interesting,
    as it is possible to extract information from web interfaces or web endpoints—for
    example, if a third party provided a microservice (which therefore has to be treated
    as a black box)—and could still be effectively monitored. If the third party has
    followed the best practice of providing a `/health` endpoint (see [http://mng.bz/5KQz](http://mng.bz/5KQz)
    for more information), we could run a script to extract the necessary values from
    the response to a Wget or cURL call to `/health`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 exec 插件可以轻松检索特定环境的信息或执行类似使用 *Wget* 和 *cURL* 等工具抓取网页输出的操作——这是屏幕抓取的现代版本。后者尤其有趣，因为可以从
    Web 接口或 Web 端点中提取信息——例如，如果第三方提供了一个微服务（因此必须将其视为黑盒）——并且仍然可以有效地进行监控。如果第三方遵循了提供 `/health`
    端点的最佳实践（更多信息请参阅 [http://mng.bz/5KQz](http://mng.bz/5KQz)），我们可以运行一个脚本来从对 `/health`
    的 Wget 或 cURL 调用的响应中提取必要的值。
- en: 'The exec plugin does need to be used with some care. Each exec process is executed
    in its own thread so that it does not adversely impact the consumption of other
    logging events whenever triggered. However, if the process is too slow, then we
    could experience the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 exec 插件确实需要小心谨慎。每个 exec 进程都在自己的线程中执行，这样就不会在其他日志事件被触发时对其他日志事件的消费产生不利影响。然而，如果进程太慢，我们可能会遇到以下情况：
- en: The exec plugin will likely be triggered again before the last one has completed,
    which risks creating out-of-sequence events (due to how resources get shared across
    threads).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: exec 插件可能会在最后一个尚未完成之前再次被触发，这可能导致创建出序事件（由于资源如何在线程之间共享，这种情况可能会发生）。
- en: Thread death could occur because there are too many threads demanding too many
    resources (this kind of issue could come about if the buffer ends up with too
    many threads).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程死亡可能发生，因为存在太多线程需要太多资源（如果缓冲区最终有太多线程，可能会出现此类问题）。
- en: Events start being backed up, as logic will wait for threads to complete to
    allocate to another exec.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件开始积压，因为逻辑会等待线程完成以分配给另一个 exec。
- en: The takeaway is to think about what exec is doing; if it is slow or computationally
    demanding, then it’s probably unwise to run it within Fluentd. We could consider
    independently running the exec process that writes the results to a file, and
    log management should be relatively lightweight compared to the core business
    process.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训是思考 exec 在做什么；如果它运行缓慢或计算密集，那么在 Fluentd 中运行它可能是不明智的。我们可以考虑独立运行将结果写入文件的 exec
    进程，并且与核心业务流程相比，日志管理应该相对较轻量。
- en: 5.4.2 Putting tag naming conventions into action
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 将标签命名约定付诸实践
- en: A decision has been made by the team that the logging configuration should reflect
    a naming convention of the `domain.service.source`. The current configuration
    does not reflect the domain being called Demo, and the services are called `AppA`
    and `AppB`, with `AppA` having two components of `Part1` and `Part2`. You have
    been asked to update the configuration file `monitor-file-out-tag-match2.conf`
    to align with this convention. Change the match directive for `AppA` so that only
    `Part1` is captured in the `AppA` file. Note the additional input, as the exec
    source is not yet needed in the output.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 团队已经决定，日志配置应该反映 `domain.service.source` 的命名约定。当前的配置没有反映被调用的域为 Demo，服务被命名为 `AppA`
    和 `AppB`，其中 `AppA` 有两个组件 `Part1` 和 `Part2`。你被要求更新配置文件 `monitor-file-out-tag-match2.conf`
    以符合此约定。更改 `AppA` 的匹配指令，以便仅在 `AppA` 文件中捕获 `Part1`。注意额外的输入，因为 exec 源在输出中尚不需要。
- en: Answer
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The outcome should result in a modified configuration that should look something
    like `Chapter5/ExerciseResults/monitor-file-out-tag-match-Answer.conf`. Note how
    the match condition has changed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该得到一个修改后的配置，看起来类似于 `Chapter5/ExerciseResults/monitor-file-out-tag-match-Answer.conf`。注意匹配条件是如何改变的。
- en: 5.4.3 Putting dynamic tagging with extract into action
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 将动态标签与提取结合使用
- en: In section 5.3.1, we saw an explanation of how tags can be set dynamically.
    We should improve and rerun `monitor-file-out-tag-match2.conf` so that the exec
    sources set the tags based on the retrieved file value.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 5.3.1 节中，我们看到了如何动态设置标签的解释。我们应该改进并重新运行 `monitor-file-out-tag-match2.conf`，以便
    exec 源根据检索到的文件值设置标签。
- en: Answer
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: We should end up with a configuration that looks something like `Chapter5/ExerciseResults/monitor-file-out-tag-match-Answer2.conf`.
    Note that when we run this, the contents of the log events using the exec source
    will no longer reach the output because we’ve changed the tag, so it fails the
    match clause.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到一个类似于 `Chapter5/ExerciseResults/monitor-file-out-tag-match-Answer2.conf`
    的配置。注意，当我们运行这个配置时，使用 exec 源的日志事件内容将不再达到输出，因为我们已经更改了标签，所以它无法通过匹配条件。
- en: 5.5 Tag plugins
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 标签插件
- en: There are plugins available to further help with routing using tags; let’s look
    at some certified plugins outside the core Fluentd (table 5.1).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有可用的插件可以进一步帮助使用标签进行路由；让我们看看一些核心 Fluentd 之外的认证插件（表 5.1）。
- en: When plugins are described as “certified,” it means they come from recognized
    and trusted contributors to the Fluentd community. As these plugins are not part
    of the core Fluentd, it does mean that to use these plugins, you will need to
    install them, just as we did for MongoDB in chapter 4.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当插件被描述为“认证”时，这意味着它们来自被 Fluentd 社区认可和信任的贡献者。由于这些插件不是 Fluentd 的核心部分，这意味着要使用这些插件，你需要安装它们，就像我们在第
    4 章中为 MongoDB 所做的那样。
- en: Table 5.1 Additional tag-based routing plugins that can help with routing
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 可帮助进行路由的基于标签的附加路由插件
- en: '| Plugin name and link | Description |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 插件名称和链接 | 描述 |'
- en: '| *rewrite-tag-filter*[https://github.com/fluent/fluent-plugin-rewrite-tag-filter](https://github.com/fluent/fluent-plugin-rewrite-tag-filter)
    | With one or more rules in the `match` directive, the log event has a regular
    expression applied to it by the plugin. Then, depending on the result, the tag
    is changed to a specified value. The rule can be set such that you can choose
    whether the rewrite is applied to a true or false outcome from the regex. The
    log event is re-emitted to continue beyond the match event using the new tag if
    a successful outcome is achieved. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| *rewrite-tag-filter*[https://github.com/fluent/fluent-plugin-rewrite-tag-filter](https://github.com/fluent/fluent-plugin-rewrite-tag-filter)
    | 通过 `match` 指令中的一个或多个规则，插件会对日志事件应用正则表达式。然后，根据结果，标签会更改到指定的值。规则可以设置为可以选择是否将重写应用于正则表达式的真或假结果。如果成功，则使用新标签重新发出日志事件以继续匹配事件之后的过程。
    |'
- en: '| *route*[https://github.com/tagomoris/fluent-plugin-route](https://github.com/tagomoris/fluent-plugin-route)
    | The route plugin allows tags to direct the log events to one or more operations,
    such as manipulating the log event and copying it to intercept it by another directive.
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| *route*[https://github.com/tagomoris/fluent-plugin-route](https://github.com/tagomoris/fluent-plugin-route)
    | 路由插件允许标签将日志事件定向到一个或多个操作，例如操纵日志事件并将其复制以通过另一个指令拦截。 |'
- en: '| *rewrite*[https://github.com/kentaro/fluent-plugin-rewrite](https://github.com/kentaro/fluent-plugin-rewrite)
    | This enables tags to be modified using one or more rules, such as if an attribute
    of the log event record matches a regular expression. As a result, performing
    specific tasks based on the log event becomes very easy. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| *rewrite*[https://github.com/kentaro/fluent-plugin-rewrite](https://github.com/kentaro/fluent-plugin-rewrite)
    | 这使得可以使用一个或多个规则修改标签，例如如果日志事件记录的属性与正则表达式匹配。因此，根据日志事件执行特定任务变得非常容易。 |'
- en: '5.6 Labels: Taking tags to a new level'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 标签：将标签提升到新的水平
- en: 'As we will see in this section, the label directive uses the basic idea of
    routing with tags and takes it to a whole new level. Ideally, we should be able
    to group a set of directives together clearly and distinctly for a particular
    group of log events, but this can become challenging. Labels allow us to overcome
    that. They have two aspects: first, an additional attribute using `@label` can
    be linked to a log event, in much the same way that tags are linked (although,
    unlike a tag, a label is not part of the log event data structure). Second, labels
    offer a directive `(<label labelName> . . . </label>`) that we use to group other
    directives (e.g., match and filter) that are executed in sequence. In effect,
    we are defining a pipeline of actions. To differentiate the two for the rest of
    the book, we will talk about labels as attributions to log events and directives
    as linking one or more directives together as a pipeline or a label pipeline.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本节中看到的那样，标签指令使用了基于标签的路由的基本思想，并将其提升到了全新的水平。理想情况下，我们应该能够清楚地将一组指令组合在一起，以区分特定的日志事件组，但这可能具有挑战性。标签允许我们克服这一点。它们有两个方面：首先，可以使用`@label`属性将一个额外的属性链接到日志事件，这与标签链接的方式非常相似（尽管，与标签不同，标签不是日志事件数据结构的一部分）。其次，标签提供了一个指令`(<label
    labelName> ... </label>)`，我们使用它来分组其他按顺序执行的指令（例如，匹配和过滤器）。实际上，我们正在定义一个动作管道。为了在本书的其余部分区分这两个概念，我们将谈论标签作为日志事件的属性，并将指令作为将一个或多个指令链接在一起作为管道或标签管道的方式。
- en: 'There is one constraint for labels when compared to tags. It is possible to
    create a comma-separated list of tags (e.g., `<match basicFile,basicFILE2>`),
    but labels can have only a single label associated with that pipeline (e.g., `<label
    myLabel>`). You will find that trying to match multiple labels in the same way
    will result in an error—for example, `''find_label'': common label not found (ArgumentError)`.
    This comes about as Fluentd does check that each label declaration can be executed
    during startup.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '与标签相比，标签有一个限制。可以创建一个以逗号分隔的标签列表（例如，`<match basicFile,basicFILE2>`），但标签只能与该管道关联一个标签（例如，`<label
    myLabel>`）。你会发现尝试以相同方式匹配多个标签会导致错误——例如，`''find_label'': common label not found
    (ArgumentError)`。这种情况发生是因为Fluentd确实会在启动时检查每个标签声明是否可以执行。'
- en: NOTE Unlike tags, the naming convention is usually more functional in meaning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与标签不同，命名约定通常在意义上更具有功能性。
- en: 5.6.1 Using a stdout filter to see what is happening
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 使用stdout过滤器查看发生了什么
- en: To help illustrate the point, we will introduce a special filter configuration.
    The important thing about filters with stdout, unlike match directives, is that
    even if the event satisfies the filter rule, it is emitted by the plugin to be
    consumed by whatever follows. This setup for a filter is a bit like a developer’s
    `println` for helping to see what is happening during code development. We will
    look more closely at filters in the next chapter, but for now, let’s see how the
    stdout plugin behaves in a filter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助说明这一点，我们将引入一个特殊的过滤器配置。与匹配指令不同，filters with stdout的重要之处在于，即使事件满足过滤器规则，它也会由插件发出，以便被后续的内容消费。这种过滤器设置有点像开发者的`println`，有助于在代码开发过程中查看发生了什么。我们将在下一章更详细地探讨过滤器，但现在，让我们看看stdout插件在过滤器中的行为。
- en: 'The stdout plugin effectively accepts all events; thus, the following filter
    will let everything pass through and send the details to the console:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: stdout插件有效地接受所有事件；因此，以下过滤器将允许所有内容通过并发送详细信息到控制台：
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This configuration is typically referred to as *filter_stdout*. Using this as
    an additional step will help us illustrate the label pipeline behavior. This is
    another handy way of peeking at what is happening within a Fluentd configuration.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置通常被称为 *filter_stdout*。使用此作为额外步骤将帮助我们说明标签管道的行为。这是查看Fluentd配置内部发生情况的一种便捷方式。
- en: 5.6.2 Illustrating label and tag routing
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 标签和标签路由的示例
- en: To illustrate a label-based pipeline, we have created a configuration that tails
    two separate files (from two different log simulators). The configuration of the
    simulator output results in two differing message structures (although both are
    derived from the same source data). To observe the differences, compare `basic-file.txt`
    and `basic-file2.txt` once the simulators are running.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明基于标签的管道，我们创建了一个配置，该配置跟踪两个不同的文件（来自两个不同的日志模拟器）。模拟器输出结果的配置导致两种不同的消息结构（尽管两者都源自相同的数据源）。要观察差异，请在模拟器运行后比较
    `basic-file.txt` 和 `basic-file2.txt`。
- en: The configuration will illustrate the use of a label being applied to one source
    and not another. Then, within the label “pipeline,” one source (file) will be
    subject to both the stdout filter (as explained in section 5.6.1) and a file output
    that is separate from the output of the other file. This is illustrated in the
    following listing. As with other larger configurations, we have replaced sections
    with ellipses, so relevant aspects of the configuration are easier to read.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置将说明将标签应用于一个源而不是另一个源的使用。然后，在“pipeline”标签内，一个源（文件）将同时受到 stdout 过滤器（如第 5.6.1
    节所述）和与另一个文件输出分开的文件输出的影响。以下列表中展示了这一点。与其他较大的配置一样，我们用省略号替换了部分内容，以便更容易阅读配置的相关方面。
- en: Listing 5.7 Chapter5/Fluentd/file-source-file-out-label-pipeline.conf label
    pipeline
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 第 5 章/Fluentd/file-source-file-out-label-pipeline.conf 标签管道
- en: '[PRE11]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Our source attaches a label to the events it creates; in this case, labelPipeline.
    This will mean the step operation performed on these events will be in the <label
    labelPipeline> block.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将源附加到它创建的事件上；在这种情况下，标签为 labelPipeline。这意味着对这些事件执行的操作步骤将在 <label labelPipeline>
    块中执行。
- en: ❷ This source is unlabeled. As a result, its log events will be intercepted
    by the next match blog that can consume the tag basicFILE2.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 此源未标记。因此，其日志事件将被下一个可以消耗标签 basicFILE2 的匹配博客拦截。
- en: ❸ At the start of the label block, any log events with a label that match will
    pass through this sequence of operations, assuming the processing allows the event
    to output from the plugin.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在标签块的开始处，任何具有匹配标签的日志事件将通过此操作序列，假设处理允许事件从插件输出。
- en: ❹ Use the stdout filter to push the log events to stdout and output to the next
    plugin.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 stdout 过滤器将日志事件推送到 stdout 并输出到下一个插件。
- en: ❺ Use the match to direct content to a file.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用匹配将内容定向到文件。
- en: ❻ We will never see any result from this stdout filter, as the preceding match
    will have consumed the log event. To send log events to both stdout and the file
    would require the use of the copy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 由于前面的匹配将消耗日志事件，因此我们永远不会看到此 stdout 过滤器的任何结果。要将日志事件发送到 stdout 和文件，需要使用复制功能。
- en: ❼ Defines the end of the label series of events
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义事件标签系列的结束
- en: ❽ Outside of a label, the match will be applied to all no label events.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在标签之外，匹配将应用于所有无标签事件。
- en: To run this configuration, we need to run the commands
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此配置，我们需要运行以下命令
- en: '`fluentd -c Chapter5/Fluentd/file-source-file-out-label-pipeline.conf`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter5/Fluentd/file-source-file-out-label-pipeline.conf`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file.properties`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file.properties`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file2.properties`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file2.properties`'
- en: When running this setup, the log events can be seen in `basic-file.txt` and
    on the console. Additionally, there will be two more files, as the log content
    is output to `label-pipeline-file-output.*_*.log` and `alt-file-output.*_*.log`
    (wild- cards represent the date and file increment number). Neither file should
    have tags mixing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行此设置时，日志事件可以在 `basic-file.txt` 和控制台上看到。此外，还将有两个更多文件，因为日志内容输出到 `label-pipeline-file-output.*_*.log`
    和 `alt-file-output.*_*.log`（通配符代表日期和文件增量编号）。这两个文件都不应混合标签。
- en: While the match expression defined continues to use a wildcard within the label
    pipeline, it is possible to still apply the tag controls on the directives within
    the pipeline. If you edit the configuration setting to align the match clause
    with `<match basicFILE2>`, you will see the logs displayed on the console but
    not in the file.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然定义的匹配表达式在标签管道内继续使用通配符，但仍然可以在管道内的指令上应用标签控制。如果您将配置设置编辑为与 <match basicFILE2>
    对齐，您将看到日志在控制台上显示，但不在文件中。
- en: 5.6.3 Connecting pipelines
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.3 连接管道
- en: As configurations become more sophisticated, you will likely need to create
    pipelines and link them together. This can be done using the *relabel* plugin.
    Relabel does what it says; it changes the label associated with the log event.
    As relabel is an output plugin, the log event can change the label and emit the
    log event rather than consume it. For example, you might have a label with several
    directives that can manipulate a log event into a human-friendly representation
    and send it to a social platform such as Slack. But before you use your label
    to do that, you may wish to take the log events through a labeled pipeline of
    filters that exclude all log events representing business-as-usual events.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 随着配置变得更加复杂，您可能需要创建管道并将它们链接在一起。这可以通过使用*relabel*插件来完成。Relabel做它所说的；它更改与日志事件关联的标签。由于relabel是一个输出插件，日志事件可以更改标签并发出日志事件而不是消费它。例如，您可能有一个包含多个指令的标签，可以将日志事件转换为人类友好的表示并发送到像Slack这样的社交平台。但在您使用标签做那之前，您可能希望将日志事件通过一个标记管道的过滤器，排除所有代表常规业务事件的日志事件。
- en: As our Fluentd configuration structures become more complex with pipelines,
    it helps to visualize what is happening, as shown in figure 5.7\. As you can see,
    we have now made the match that feeds the `alt-file-output` a labeled pipeline
    called `common`. To illustrate the use of relabel, the match in our original `labelPipeline`
    (as we saw in listing 5.7) has been modified. We have introduced a *copy* plugin
    to ensure that the log event goes to both the output and relabel (highlighting
    the store declaration can be done for more than just storage plugins). When we
    run this configuration, `alt-file-output` files will now contain both sources.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的Fluentd配置结构随着管道变得更加复杂，可视化正在发生的事情很有帮助，如图5.7所示。正如您所看到的，我们现在已经将`alt-file-output`的匹配项改为一个名为`common`的标记管道。为了说明relabel的使用，我们原始的`labelPipeline`（如我们在列表5.7中看到的）已经被修改。我们引入了一个*copy*插件来确保日志事件同时发送到输出和relabel（突出显示存储声明不仅可以用于存储插件）。当我们运行此配置时，`alt-file-output`文件将现在包含两个来源。
- en: '![](../Images/CH05_F07_Wilkins.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F07_Wilkins.png)'
- en: Figure 5.7 Label routing example, with two label pipelines connected (labelPipeline
    and common)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 标签路由示例，两个标签管道连接（labelPipeline和common）
- en: We can run the configuration using the commands
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令运行配置
- en: '`fluentd -c Chapter5/Fluentd/file-source-multi-out-label-pipelines.conf`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter5/Fluentd/file-source-multi-out-label-pipelines.conf`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file.properties`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file.properties`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file2.properties`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log -file2.properties`'
- en: The following listing shows the configuration with the application of `relabel`.
    Note the use of the ellipsis again, so you can focus on the key elements.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了应用`relabel`的配置。注意再次使用省略号，这样您可以专注于关键元素。
- en: Listing 5.8 Chapter5/Fluentd/file-source-multi-out-label-pipelines.conf use
    of relabel
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 Chapter5/Fluentd/file-source-multi-out-label-pipelines.conf中使用relabel
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ This links this source’s log events to a label, as we did in the previous
    example.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这将此来源的日志事件链接到一个标签，就像我们在前面的例子中所做的那样。
- en: ❷ Sets the second source to use the common label rather than trust directives
    catching log events
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将第二个来源设置为使用common标签而不是信任指令捕获日志事件
- en: ❸ Using the copy plugin within the match, we can cause the log event to be consumed
    by more than one output plugin.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在match中使用copy插件，我们可以使日志事件被多个输出插件消费。
- en: ❹ The log event will get pushed to a file output plugin first.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 日志事件将首先被推送到文件输出插件。
- en: ❺ As the copy directive is being used, we can force the log event to be processed
    by further operations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 由于使用了copy指令，我们可以强制日志事件被进一步的操作处理。
- en: ❻ Using the relabel plugin to change the log event's label until this operation
    is 'labelPipeline'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用relabel插件更改日志事件的标签，直到此操作为'labelPipeline'
- en: ❼ The label is now set to be common; as we leave this match and label block,
    the event will be consumed by the label directive called common.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 标签现在设置为common；当我们离开这个匹配和标签块时，事件将被名为common的标签指令消费。
- en: ❽ The label common starts, which will now receive log events from both sources.
    One source’s event comes directly, and another passes through the labelPipeline
    before reaching the common pipeline.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 标签common开始，现在将接收来自两个来源的日志事件。一个来源的事件直接到达，另一个通过labelPipeline到达common管道。
- en: 5.6.4 Label sequencing
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.4 标签排序
- en: Unlike tags, the relative positioning of label directives does not matter, taking
    our current configuration shown in figure 5.7 as an example. While `labelPipeline`
    will trigger the use of the `common` label using relabel, the `common` label could
    be declared before the `labelPipeline`. The steps within a label pipeline are
    still sequential in execution. You can see and try this with the provided configuration
    file `Chapter5/Fluentd/file-source-multi-out-label-pipelines2.conf`. In the configuration,
    you can see
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签不同，标签指令的相对位置并不重要，以我们当前在图5.7中显示的配置为例。虽然`labelPipeline`将触发使用`common`标签的重新标记，但`common`标签可以在`labelPipeline`之前声明。标签管道内的步骤在执行时仍然是顺序的。您可以通过提供的配置文件`Chapter5/Fluentd/file-source-multi-out-label-pipelines2.conf`看到并尝试这一点。在配置中，您可以看到
- en: A relabel declaration that used to be for common, but has been changed to use
    the label `outOfSequence.` With it, we have moved the filter out to the new label
    section.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个曾经用于通用但已更改为使用标签`outOfSequence.`的重新标记声明。有了它，我们将过滤器移动到了新的标签部分。
- en: The `outOfSequence` label pipeline then redirects to the common, as we had previously
    and as illustrated in figure 5.8\. The configuration file order actually reflects
    the appearance in the figure when reading the diagram left to right (ignoring
    the flow shown in the diagram).
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`outOfSequence`标签管道随后重定向到通用，就像我们之前所做的那样，如图5.8所示。配置文件的实际顺序反映了从左到右阅读图中的外观（忽略图中显示的流程）。'
- en: '![](../Images/CH05_F08_Wilkins.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8](../Images/CH05_F08_Wilkins.png)'
- en: Figure 5.8 This configuration illustrates that labels are not order-sensitive.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 此配置说明标签不是顺序敏感的。
- en: The scenario can be executed using the commands
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 该场景可以使用以下命令执行
- en: '`fluentd -c Chapter5/Fluentd/file-source-multi-out-label-pipelines2.conf`'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter5/Fluentd/file-source-multi-out-label-pipelines2.conf`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log-file-small.properties`'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log-file-small.properties`'
- en: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log-file2-small.properties`'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter5/SimulatorConfig/basic-log-file2-small.properties`'
- en: The simulator properties have been configured with a small data set to make
    it easier to confirm, and we do not get any accidental looping.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器属性已配置为使用小数据集，以便更容易确认，并且我们没有得到任何意外的循环。
- en: NOTE While we have illustrated that labeling sections out of order is possible,
    we would not necessarily advocate it as good practice. As labels could be compared
    to *goto* statements in some respects, it is preferable to try to structure the
    configuration files to be as linear as practical.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然我们已经说明了在不按顺序标记部分是可能的，但我们并不一定提倡这种做法。从某些方面来看，标签可以与*goto*语句相提并论，因此最好尝试将配置文件结构得尽可能线性。
- en: 5.6.5 Special labels
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.5 特殊标签
- en: Fluentd’s label feature includes a predefined `@Error` label. This means we
    can use the label directive to define a pipeline to process errors; those errors
    can be raised within our configuration or within a plugin executing the configuration.
    This does rely on the plugin implementation to use a specific API (`emit_error_event`).
    We can be confident that core Fluentd plugin implementations will use this API,
    but it might be worth checking to see if a third-party plugin uses the feature
    rather than simply writing to stdout. We will see this later in the book when
    we look at building our own plugin.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd的标签功能包括一个预定义的`@Error`标签。这意味着我们可以使用标签指令来定义一个处理错误的管道；这些错误可以在我们的配置中或在执行配置的插件中引发。这确实依赖于插件实现使用特定的API（`emit_error_event`）。我们可以确信核心Fluentd插件实现将使用此API，但检查第三方插件是否使用此功能而不是简单地写入stdout可能是有价值的。我们将在本书后面查看构建我们自己的插件时看到这一点。
- en: We could therefore build upon our existing Fluentd configuration steps to capture
    these errors. With this, we could do things like relabel the log event so that
    it gets picked up by a common pipeline or simply direct it to its own destination.
    In the next example, we’ve added a new label pipeline to our previous configuration
    that writes to its own file, as illustrated in the following listing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以基于现有的Fluentd配置步骤来捕获这些错误。有了这个，我们可以做一些事情，比如重新标记日志事件，使其被常见的管道捕获，或者简单地将其直接发送到其自己的目的地。在下一个示例中，我们向之前的配置中添加了一个新的标签管道，该管道将写入其自己的文件，如下面的列表所示。
- en: Listing 5.9 Fluentd/file-source-multi-out-label-pipelines-error.conf using @Error
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 使用@Error的Fluentd/file-source-multi-out-label-pipelines-error.conf
- en: '[PRE13]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The start of the pipeline using the predefined @Error label. As a result,
    any plugin that sets this label will have its error(s) handled in this pipeline.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用预定义的 @Error 标签开始管道。因此，任何设置此标签的插件都将在此管道中处理其错误。
- en: ❷ Matches all tags to write errors to file once log events are labeled with
    Error. But we could get clever and handle errors associated with different tags
    differently. For example, specific tags that experience an error that needs more
    urgent attention could be sent to a notification or collaboration tool.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 匹配所有标签，一旦日志事件被标记为错误，就将其写入文件。但我们可以变得聪明一些，对不同标签关联的错误进行不同的处理。例如，需要更多紧急关注的特定标签错误可以发送到通知或协作工具。
- en: Tip It is always useful to create a simple generic pipeline for handling errors.
    The pipeline raises some sort of problem ticket, a social notification (e.g.,
    Slack, Teams), or simply an email to notify the event within its own Fluentd configuration.
    Then use `include` as standard practice in all of your configurations. So, if
    you don’t have any specific error-handling configuration, then the generic answer
    will kick in for you.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：创建一个简单的通用管道来处理错误总是很有用的。该管道会引发某种问题票、社交通知（例如 Slack、Teams）或简单地发送电子邮件来通知 Fluentd
    配置中的事件。然后，在所有配置中作为标准实践使用 `include`。因此，如果您没有特定的错误处理配置，则通用的答案将为您启动。
- en: 5.6.6 Putting a common pipeline into action
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.6 将通用管道投入实际应用
- en: You have been asked to refactor the configuration referenced in listing 5.8
    to have a single pipeline. This will allow the service to be incorporated into
    the main configuration through inclusion. This change will allow all of the different
    log event routes being developed by other teams to occur more safely. To help
    those teams, you should create an additional pipeline with a null output as a
    template.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您被要求重构列表 5.8 中引用的配置，使其具有单个管道。这将允许通过包含将其服务纳入主配置。此更改将允许其他团队开发的所有不同日志事件路由更安全地进行。为了帮助这些团队，您应该创建一个包含空输出作为模板的额外管道。
- en: Answer
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The result actually involves three files. The core is `Chapter5/ExerciseResults/file-source-multi-out-label-pipelines-Answer.conf`.
    This uses `@include` to bring in the configurations `Chapter5/ExerciseResults/label-pipeline-Answer.conf`,
    which contains the refactored logic, and a template containing the null output
    defined by `Chapter5/ExerciseResults/label-pipeline-template-Answer.conf`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 结果实际上涉及三个文件。核心是 `Chapter5/ExerciseResults/file-source-multi-out-label-pipelines-Answer.conf`。它使用
    `@include` 引入配置 `Chapter5/ExerciseResults/label-pipeline-Answer.conf`，其中包含重构的逻辑，以及一个包含由
    `Chapter5/ExerciseResults/label-pipeline-template-Answer.conf` 定义的空输出的模板。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Fluentd provides a null output plugin that can be used as a placeholder for
    output plugins. The plugin will simply delete the received log events.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 提供了一个空输出插件，可以用作输出插件的占位符。该插件将简单地删除接收到的日志事件。
- en: Fluentd provides an exec plugin that allows us to incorporate the triggering
    of external processes, such as scripts, into the configuration. The external process
    can be invoked with arguments from the log event.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 提供了一个 exec 插件，允许我们将外部进程的触发（如脚本）纳入配置。外部进程可以使用日志事件的参数来调用。
- en: Fluentd provides several mechanisms to route log events through a configuration.
    The simplest of these is using the tag associated with each log event.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 提供了多种机制来通过配置路由日志事件。其中最简单的是使用与每个日志事件关联的标签。
- en: For more sophisticated routing where we want a “pipeline” of actions, Fluentd
    provides a label construct. The use of labels can also help us simplify the complexity
    in a configuration file when it comes to ordering configuration values.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更复杂的路由，我们希望有一个“管道”式的动作，Fluentd 提供了标签构造。标签的使用还可以帮助我们简化配置文件中的配置值排序的复杂性。
- en: Fluentd provides naming conventions by using the conventions we can group and
    `namespace` tags. We can use wildcards in the configuration to group tags together.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 通过使用我们可以分组和 `namespace` 标签的约定来提供命名约定。我们可以在配置中使用通配符来分组标签。
- en: A Fluentd configuration can be assembled from multiple files through the use
    of the `@include` directive.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 配置可以通过使用 `@include` 指令从多个文件组装。
- en: If Fluentd experiences an error (e.g., can’t connect to an instance of Elasticsearch),
    we can catch the error and perform actions using the custom `@Error` label.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Fluentd 发生错误（例如，无法连接到 Elasticsearch 的实例），我们可以捕获错误并使用自定义 `@Error` 标签执行操作。
