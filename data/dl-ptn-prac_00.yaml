- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前置内容
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: As a Googler, one of my duties is to educate software engineers on how to use
    machine learning. I already had experience creating online tutorials, meetups,
    conference presentations, training workshops, and coursework for private coding
    schools and university graduate studies, but I am always looking for new ways
    to effectively teach.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名谷歌员工，我的职责之一是教育软件工程师如何使用机器学习。我已经有了创建在线教程、聚会、会议演示、培训研讨会和为私人编码学校和大学研究生课程编写课程的经验，但我总是寻找新的有效教学方法。
- en: Prior to Google, I worked in Japanese IT as a principal research scientist for
    20 years—all without deep learning. Almost everything I see today, we were doing
    in innovation labs 15 years ago; the difference is we needed a room full of scientists
    and a vast budget. It’s incredible how things have so rapidly changed as a result
    of deep learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌之前，我在日本IT行业担任了20年的首席研究科学家——这一切都没有涉及深度学习。今天我所看到的一切，我们15年前在创新实验室里就已经在做；区别在于我们需要一个满屋子的科学家和庞大的预算。深度学习带来的变化如此之快，真是令人难以置信。
- en: Back in the late 2000s, I was working with small structured datasets with geospatial
    data from national and international sources all over the world. Coworkers called
    me a data scientist, but nobody knew what a data scientist really was. Then came
    big data, and I didn’t know the big data tools and frameworks, and suddenly I
    wasn’t a data scientist. What? I had to scramble and learn the tools and concepts
    behind big data and once again I was a data scientist.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2000年代末，我正在使用来自世界各地国家和国际来源的小型结构化数据集和地理空间数据。同事们称我为数据科学家，但没有人知道数据科学家究竟是什么。然后出现了大数据，我不了解大数据工具和框架，突然间我不再是数据科学家。什么？我不得不匆忙学习大数据背后的工具和概念，再次成为数据科学家。
- en: Then emerged machine learning on big datasets, like linear/logistic regression
    and CART analysis, and I hadn’t used statistics since graduate school decades
    ago, and once again I was not a data scientist. What? I had to scramble to learn
    statistics all over again, and once again I was a data scientist. Then came deep
    learning, and I didn’t know the theory and frameworks for neural networks and
    suddenly I wasn’t a data scientist. What? I scrambled again and learned deep learning
    theory and other deep learning frameworks. And once again, I am a data scientist.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随后出现了在大数据集上的机器学习，如线性/逻辑回归和CART分析，而我自从几十年前研究生毕业以来就没有使用过统计学，再次我不是数据科学家。什么？我不得不再次匆忙学习统计学，再次成为数据科学家。然后出现了深度学习，我不了解神经网络的理论和框架，突然间我不再是数据科学家。什么？我再次匆忙学习深度学习理论和其他深度学习框架。再次，我成为数据科学家。
- en: acknowledgments
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: I would like to thank all those at Manning who helped throughout this process.
    Frances Lefkowitz, my development editor; Deirdre Hiam, my project editor; Sharon
    Wilkey, my copyeditor; Keri Hales, my proofreader; and Aleksandar Dragosavljević,
    my reviewing editor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢所有在Manning出版社帮助我完成这个过程的人。Frances Lefkowitz，我的开发编辑；Deirdre Hiam，我的项目编辑；Sharon
    Wilkey，我的校对编辑；Keri Hales，我的校对员；以及Aleksandar Dragosavljević，我的审稿编辑。
- en: 'To all the reviewers: Ariel Gamino, Arne Peter Raulf, Barry Siegel, Brian R.
    Gaines, Christopher Marshall, Curtis Bates, Eros Pedrini, Hilde Van Gysel, Ishan
    Khurana, Jen Lee, Karthikeyarajan Rajendran, Michael Kareev, Muhammad Sohaib Arif,
    Nick Vazquez, Ninoslav Cerkez, Oliver Korten, Piyush Mehta, Richard Tobias, Romit
    Singhai, Sayak Paul, Sergio Govoni, Simone Sguazza, Udendran Mudaliyar, Vishwesh
    Ravi Shrimali, and Viton Vitanis, your suggestions helped make this a better book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 向所有审稿人表示感谢：Ariel Gamino、Arne Peter Raulf、Barry Siegel、Brian R. Gaines、Christopher
    Marshall、Curtis Bates、Eros Pedrini、Hilde Van Gysel、Ishan Khurana、Jen Lee、Karthikeyarajan
    Rajendran、Michael Kareev、Muhammad Sohaib Arif、Nick Vazquez、Ninoslav Cerkez、Oliver
    Korten、Piyush Mehta、Richard Tobias、Romit Singhai、Sayak Paul、Sergio Govoni、Simone
    Sguazza、Udendran Mudaliyar、Vishwesh Ravi Shrimali和Viton Vitanis，你们的建议帮助这本书变得更好。
- en: To all Google Cloud AI staff who have shared their personal and customer insights,
    your insights helped the book cover a broader audience.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 向所有分享个人和客户见解的谷歌云AI团队表示感谢，你们的见解帮助这本书覆盖了更广泛的受众。
- en: about this book
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于这本书
- en: Who should read this book
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应该阅读这本书的人
- en: Welcome to my latest endeavor, *Deep Learning Patterns and Practices*. This
    book is for software engineers; machine learning engineers; and junior, mid-level,
    and senior data scientists. Although you might assume that the initial chapters
    would be redundant for the latter group, my unique approach will likely leave
    you with additional insight and a welcomed refresher. The book is structured so
    that every reader reaches the point of “ignition” and is able to self-propel forward
    into deep learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我的最新尝试，*深度学习模式与实践*。这本书是为软件工程师、机器学习工程师以及初级、中级和高级数据科学家所写。尽管你可能认为对于后者来说，最初章节可能有些冗余，但我的独特方法可能会让你获得额外的洞察和受欢迎的复习。本书的结构使得每位读者都能达到“点燃”的点，并能够自我推动进入深度学习。
- en: I teach the design patterns and practices mostly in the context of computer
    vision, as this is where design patterns for deep learning first evolved. Developments
    in natural-language understanding and structured data models lagged behind and
    continued to be focused on classical approaches. But as they caught up, these
    fields developed their own deep learning design patterns, and I discuss those
    patterns and practices throughout the book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我主要在计算机视觉的背景下教授设计模式和惯例，因为深度学习的设计模式最初就是在这里演化的。自然语言理解和结构化数据模型的发展落后，并且继续专注于经典方法。但随着它们赶上，这些领域发展了自己的深度学习设计模式，我在本书中也讨论了这些模式和惯例。
- en: 'Though I provide code for the computer vision models, my emphasis is on the
    concepts underlying the approaches and innovations: how they are set up and why
    they are set up that way. These underlying concepts are applicable to natural-language
    processing, structured data, signal processing, and other domains, and by generalizing,
    you should be able to adapt the concepts, methods, and techniques to the problems
    in your field. Many of the models and techniques I discuss are domain-agnostic,
    and throughout the book I also discuss key innovations in natural-language processing,
    natural-language understanding, and structured data domains where appropriate.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我为计算机视觉模型提供了代码，但我强调的是这些方法和创新背后的概念：它们是如何设置的以及为什么这样设置。这些基本概念适用于自然语言处理、结构化数据、信号处理和其他领域，通过概括，你应该能够将概念、方法和技术适应你领域中的问题。我讨论的许多模型和技术都是领域无关的，在本书中，我也讨论了自然语言处理、自然语言理解和结构化数据领域中的关键创新，在适当的地方。
- en: As for general background, you should know at least the basics of Python. It’s
    OK if you still struggle with what a comprehension is or what a generator is,
    or if you still have some confusion about the weird multidimensional array slicing,
    and this thing about which objects are mutable and nonmutable on the heap. For
    this book, that’s OK.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 至于一般背景，你应该至少了解Python的基础知识。如果你还在为理解什么是可理解性或什么是生成器而感到困惑，或者如果你对奇怪的多元数组切片以及堆上哪些对象是可变的和不可变的还有一些困惑，那都是可以的。对于这本书来说，这些都是可以的。
- en: For those software engineers wanting to become machine learning engineers—what
    does that mean? A machine learning engineer (MLE) is an applied engineer. You
    don’t need to know statistics (really, you don’t!), and you don’t need to know
    computational theory. If you fell asleep in your college calculus class on what
    a derivative is, that’s OK, and if somebody asks you to do a matrix multiplication,
    feel free to ask, “Why?”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想要成为机器学习工程师的软件工程师——那意味着什么？机器学习工程师（MLE）是应用工程师。你不需要知道统计学（真的，你不需要！），你也不需要知道计算理论。如果你在大学微积分课上对导数是什么感到困倦，那没关系，如果有人让你做矩阵乘法，你可以自由地问，“为什么？”
- en: Your job is to learn the knobs and levers of a framework, and apply your skills
    and experience to produce solutions for real-world problems. That’s what I am
    going to help you with, and that’s what the design patterns using TF.Keras are
    about.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是学习框架的旋钮和杠杆，并将你的技能和经验应用于解决现实世界的问题。这正是我将帮助你做到的，也是TF.Keras中使用的模式设计所关注的。
- en: This book is designed for machine learning engineers and data scientists at
    comparable levels. For those pursuing the data scientist route, I encourage you
    to study supplemental statistics-related material.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是为处于相当水平的机器学习工程师和数据科学家设计的。对于那些追求数据科学家路线的人，我鼓励你学习补充的与统计学相关的材料。
- en: 'Before we get started, I want to explain how you will learn, so this first
    section explains more about my philosophy and approach to teaching. Then we’ll
    review some foundational material, including terminology, the progression from
    classical or semantic AI to narrow or statistical AI, and an overview of the basic
    steps in machine learning. Finally, we’ll take a high-level look at what this
    book is all about: the modern model amalgamation approach to machine learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我想解释你将如何学习，所以这个第一部分更多地解释了我的教学哲学和方法。然后我们将回顾一些基础知识，包括术语、从经典或语义AI到窄或统计AI的演变，以及机器学习的基本步骤概述。最后，我们将从高层次的角度看看这本书的内容：机器学习的现代模型融合方法。
- en: I don’t use the traditional Western approach of rote memorization, iterate,
    iterate, test for correct answers, and then vertically advance. Beyond my view
    that it is less effective, I believe it is unintentionally discriminatory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我不使用传统的西方死记硬背的方法，重复，重复，测试正确答案，然后垂直提升。除了我认为这种方法效果较差之外，我还认为它无意中具有歧视性。
- en: 'Instead, I have had the benefit of teaching engineering and science across
    diverse cultures and teaching methods, and have developed a unique teaching style
    that uses what I call a *lateral approach*: I start with core concepts, and then
    spiral out using what I call *abstraction*. When questions are asked, I will gradually
    start to point to other students on their thoughts to the answer, before I reflect
    on their thoughts. I don’t do quizzes, where students try to get a 100%. Instead,
    I give assignments that each student will fail. I let the students pound away
    at the problem, struggling, and in doing so they will start to discover the underlying
    principles of what they need to learn. For example, I might give an assignment
    to train a CIFAR-10 dataset using a stock ResNet50 model, noting that the authors
    of the corresponding ResNet papers achieved 97% accuracy on CIFAR-10\. Every student
    will fail, the model won’t converge, they won’t get over 70%, and so forth.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我有幸在多元文化和教学方法中教授工程和科学，并发展了一种独特的教学方法，我称之为“横向方法”：我从核心概念开始，然后通过我所说的“抽象”螺旋式扩展。当有问题被问起时，我会在反思他们的想法之前，逐渐开始指向其他学生的想法，指向答案。我不进行测验，学生试图得到100%。相反，我给学生布置作业，每个学生都会失败。我让学生不断打击问题，挣扎，在这个过程中，他们将会开始发现他们需要学习的潜在原理。例如，我可能会布置一个作业，要求使用标准的ResNet50模型训练CIFAR-10数据集，并指出相应的ResNet论文的作者在CIFAR-10上实现了97%的准确率。每个学生都会失败，模型不会收敛，他们不会超过70%，等等。
- en: I then assemble the students into teams to tackle the problems together. By
    conferring with each other, they learn to generalize together. And before they
    can get it, I do the leap where I present the students with another challenge
    that will be difficult to solve—and the process starts again. I never give the
    students a chance to rote memorize.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我将学生组成团队一起解决问题。通过相互讨论，他们学会了一起概括。在他们能够理解之前，我会进行跳跃，给学生提出另一个难以解决的问题——然后这个过程再次开始。我从不给学生死记硬背的机会。
- en: Using my example, I might put on the whiteboard four possible solutions, like
    1) image augmentation, 2) more regularization, 3) more hyperparameter search,
    4) defer downsampling deep into the neural network (that’s the right answer).
    Then in the middle, I will stop the students and have each team state which solution
    they tried and what they learned so far. I then explain the why/why not of each
    solution, and then change the problem again.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以我的例子来说，我可能在黑板上写下四种可能的解决方案，比如1）图像增强，2）更多的正则化，3）更多的超参数搜索，4）延迟下采样到神经网络深处（这是正确答案）。然后在中途，我会停止学生，让每个团队陈述他们尝试的解决方案以及到目前为止学到了什么。然后我会解释每个解决方案为什么可行/不可行，然后再次改变问题。
- en: As the students progress to more advanced levels, I switch from the teacher
    role to what I call the master-student role and participate in the learning. The
    students are teaching me and each other as I am teaching them. I observe every
    student and look for what I call *ignition*—where the student will self-propel
    as a learner, and that is when the student is continuously learning. I find in
    my teaching method that the entire classroom comes up together, and no student
    is left behind.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当学生进步到更高级的水平时，我从教师角色转变为所谓的师生角色，并参与学习。学生们在教我，也互相教对方，就像我在教他们一样。我观察每个学生，寻找我所说的“点燃”——学生作为学习者自我推动的时刻，那就是学生持续学习的时候。我发现我的教学方法中，整个课堂一起进步，没有学生被落下。
- en: From time to time, a school administrator would sit in on one of my sessions
    to observe. They would hear chatter trickling up from students and want to observe
    how it works. Of course, administrators need to give a name to everything. At
    one private coding school, the administrator described it as “everyone is a learner.”
    The students learn from the teacher, the teacher learns from the students, and
    the students learn from the students. The administrator called it “Let’s Learn
    Together.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不时地，学校管理员会参加我的某个课程来观察。他们会听到学生们的低语，并想观察它是如何运作的。当然，管理员需要给每件事都起个名字。在一所私立编码学校，管理员将其描述为“每个人都是学习者”。学生们从老师那里学习，老师从学生那里学习，学生们也从彼此那里学习。管理员称之为“让我们共同学习”。
- en: I have my own name for my teaching methodology, which I call “I believe in myself.”
    I tell my students, how can you believe in me (“the teacher”), if you don’t first
    believe in yourself?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我为自己的教学方法取了一个名字，我称之为“我相信自己”。我告诉我的学生，如果你自己（学生）首先不相信自己，你怎么能相信我（老师）呢？
- en: 'How this book is organized: A roadmap'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书是如何组织的：路线图
- en: 'This book is organized into three parts: fundamentals, design patterns, and
    design patterns for training and deployment for production.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三个部分：基础知识、设计模式和用于生产训练和部署的设计模式。
- en: Part 1, “Deep learning fundamentals,” provides readers with a refresher on deep
    learning that includes an introduction to convolutional neural networks, as well
    as a discussion of the concepts and terminology that are mainstream today for
    all domains—computer vision, natural language processing, and structured data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分“深度学习基础知识”为读者提供了对深度学习的复习，包括卷积神经网络简介，以及讨论了今天所有领域——计算机视觉、自然语言处理和结构化数据——的主流概念和术语。
- en: The design patterns for models are presented in Part 2, “Basic design patterns.”
    In chapters 5 through 7, I introduce modern design patterns, and how they are
    applied to many current and former state-of-the-art deep learning models. I cover
    the procedural reuse design pattern, which has been the prevailing approach for
    hand-engineered models. I teach the design approaches, refinements, and pros/cons
    for large models discovered by researchers for going deeper in layers (chapter
    5), wider in layers (chapter 6), and using alternative or out-of-the-box connectivity
    patterns (chapter 7).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型设计模式在第2部分“基本设计模式”中呈现。在第5章到第7章中，我介绍了现代设计模式，以及它们如何应用于许多当前和以前最先进的深度学习模型。我涵盖了过程性重用设计模式，这是手工制作模型的主要方法。我教授了研究人员发现的设计方法、改进以及大模型在层中更深入（第5章）、更宽（第6章）以及使用替代或现成连接模式（第7章）的优缺点。
- en: Chapter 5 looks at a procedural design pattern for convolutional neural networks,
    as well as the development of residual blocks with identity links to attention
    in transformers for natural language understanding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章探讨了卷积神经网络的过程性设计模式，以及残差块在自然语言理解中的注意力机制中的身份链接开发。
- en: Chapter 6 expands on the procedural design pattern for convolutional neural
    networks and how researchers explored going wide in layers as an alternative to
    going deep. I show how this approach, such as ResNeXt, resulted in achieving comparing
    accuracy when compared to deep layers with less exposure to memorization and vanishing
    gradients. I also explore how wide convolutional neural networks are relevant
    to developments in wide and deep and TabNet models for structured data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章扩展了卷积神经网络的过程性设计模式，以及研究人员如何探索在层中变宽作为深入的一种替代方案。我展示了这种方法，如ResNeXt，与深度层相比，在减少对记忆和消失梯度的暴露时实现了比较高的准确性。我还探讨了宽卷积神经网络与宽度和深度以及结构化数据中的TabNet模型的发展的相关性。
- en: Chapter 7 covers model design patterns that explored other alternative layer
    connections to going deeper or wider in layers to increase accuracy, reduce the
    number of parameters, and increase information gain at intermediate latent space
    within the model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章涵盖了模型设计模式，这些模式探索了其他替代层连接，以在层中更深入或更广泛地增加准确性，减少参数数量，并在模型中间潜在空间中增加信息增益。
- en: Chapter 8 examines the unique design considerations and special constraints
    for mobile convolutional neural networks. Because of the memory constraints of
    these devices, tradeoffs between and size and accuracy had to be considered. I
    will teach the progression in these tradeoffs, the pros/cons, and how the designs
    of mobile networks differ from their large model counterparts to accommodate the
    tradeoffs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章探讨了移动卷积神经网络独特的考虑因素和特殊约束。由于这些设备的内存限制，必须在大小和准确性之间进行权衡。我将讲解这些权衡的进展、优缺点，以及移动网络的设计如何与大型模型对应物不同，以适应这些权衡。
- en: Chapter 9 introduces autoencoders for unsupervised learning. As standalone models,
    the practical application of autoencoders is very narrow. But the discoveries
    from autoencoders have contributed to advancements in pretraining models. These
    models better generalize to out-of-distribution serving—that is, prediction requests
    in a production-deployed model that have a different distribution then the data
    the model was trained on. I also explore how autoencoders are comparable to embeddings
    in natural language understanding.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章介绍了用于无监督学习的自动编码器。作为独立模型，自动编码器的实际应用范围非常有限。但自动编码器的发现对预训练模型的进步做出了贡献。这些模型更好地泛化到分布外的服务——即，在生产部署的模型中，预测请求的分布与模型训练的数据不同。我还探讨了自动编码器在自然语言理解中与嵌入的相似性。
- en: All the models in the second part of this book made seminal advancements to
    the research and development of deep learning, and continue to either be in use
    today, or their contributions have been incorporated in today’s models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第二部分的所有模型都对深度学习的研发做出了开创性的贡献，并且至今仍在使用，或者它们的贡献已被纳入今天的模型中。
- en: Part 3, “Working with the pipelines,” looks at design patterns and practices
    for production pipelines. In chapter 10, we look at hyperparameter tuning, both
    manual and automatic. I teach the design decisions, pros/cons, and best practices
    for specifying the search space, and the patterns for searching it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分，“与管道协同工作”，探讨了生产管道的设计模式和最佳实践。在第10章中，我们探讨了超参数调整，包括手动和自动调整。我讲解了指定搜索空间的设计决策、优缺点以及最佳实践，以及搜索该搜索空间的模式。
- en: Chapter 11 discusses transfer learning and introduces the concepts and methods
    of handling weight transfers and tuning for similar and distant tasks. I also
    look at the application to domain transfer for weight reuse during pretraining
    for models that are fully trained from scratch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章讨论了迁移学习，并介绍了处理权重迁移和调整相似和远程任务的概念和方法。我还探讨了在从头开始完全训练的模型中，预训练期间权重重用的应用。
- en: Chapters 12 through 14 take a high-level look at production pipelines. We dive
    deep into the data side in chapters 12 and 13\. Chapter 12, which introduces data
    distributions, is the only chapter that goes into some detail about statistics.
    A lot has changed since 2017, when one was expected to have PhD-level knowledge
    of statistics. Today, much of that is hidden or otherwise automated in deep learning
    frameworks like TensorFlow. Understanding data distributions and search spaces
    remains one of the prevailing areas of expected knowledge in statistics, and can
    substantially influence the cost of training and the ability of the model to generalize
    when deployed into production.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第12章至第14章从高层次上审视了生产管道。我们在第12章和第13章深入探讨了数据端。第12章介绍了数据分布，是唯一一个详细讨论统计学的章节。自2017年以来，统计学领域发生了很大变化，当时人们期望拥有博士级别的统计学知识。如今，这些知识的大部分都隐藏在深度学习框架（如TensorFlow）中，或者被自动化。理解数据分布和搜索空间仍然是统计学中预期知识的主要领域之一，并且可以显著影响训练成本以及模型在生产部署时的泛化能力。
- en: Finally, chapters 13 and 14 turn from the data side to the deployment side.
    I cover the concepts and best practices for constructing the data side and then
    training side of a production pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第13章和第14章从数据端转向部署端。我介绍了构建数据端和训练端的概念和最佳实践。
- en: About the code
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于代码
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a `fixed-width`
    `font like` `this` to separate it from ordinary text. Sometimes code is also **`in
    bold`** to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含许多源代码示例，无论是编号列表还是与普通文本并列。在这两种情况下，源代码都格式化为 `fixed-width` `font like` `this`
    以便与普通文本区分。有时代码也会被**加粗**以突出显示与章节中先前步骤不同的代码，例如当新功能添加到现有代码行时。
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. Additionally, comments in the source code have often been removed from the
    listings when the code is described in the text. Code annotations accompany many
    of the listings, highlighting important concepts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，原始源代码已被重新格式化；我们添加了换行并重新调整了缩进以适应书籍中的可用页面空间。此外，当代码在文本中描述时，源代码中的注释通常已从列表中删除。代码注释伴随着许多列表，突出显示重要概念。
- en: All the code samples in the book are written in Python and are working code;
    albeit they may be absent of import statements. In many cases, the code samples
    are part of a larger code component, such as a model. In these cases, the entire
    code is available in my Google Cloud AI Developer Relations public GitHub repo
    ([https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的所有代码示例都是用 Python 编写的，并且是可运行的代码；尽管它们可能缺少导入语句。在许多情况下，代码示例是更大代码组件的一部分，例如一个模型。在这些情况下，整个代码都可在我的
    Google Cloud AI 开发者关系公共 GitHub 仓库 ([https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo))
    中找到。
- en: liveBook discussion forum
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: liveBook 讨论论坛
- en: Purchase of *Deep Learning Patterns and Practices* includes free access to a
    private web forum run by Manning Publications where you can make comments about
    the book, ask technical questions, and receive help from the author and from other
    users. To access the forum, go to [https://livebook.manning.com/#!/book/deep-learning-patterns-and-practices/discussion](https://livebook.manning.com/#!/book/deep-learning-patterns-and-practices/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 购买《深度学习模式与实践》包括免费访问由 Manning 出版公司运行的私人网络论坛，您可以在论坛中就书籍发表评论、提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请访问
    [https://livebook.manning.com/#!/book/deep-learning-patterns-and-practices/discussion](https://livebook.manning.com/#!/book/deep-learning-patterns-and-practices/discussion)。您还可以在
    [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)
    了解更多关于 Manning 论坛和行为准则的信息。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 曼宁对读者的承诺是提供一个平台，在这里读者之间以及读者与作者之间可以进行有意义的对话。这并不是对作者参与特定数量活动的承诺，作者对论坛的贡献仍然是自愿的（且未支付报酬）。我们建议您尝试向作者提出一些挑战性的问题，以免他的兴趣转移！只要书籍在印刷中，论坛和先前讨论的存档将可通过出版社的网站访问。
- en: Other online resources
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他在线资源
- en: For a framework, I use TensorFlow 2.*x*, which has incorporated the Keras model
    API. I think that the combination of the two is a fantastic vehicle for education
    beyond its production value.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于框架，我使用 TensorFlow 2.*x*，它已经集成了 Keras 模型 API。我认为这两个的结合是超越其生产价值的教育工具。
- en: The material is multimodal. In addition to the book and full code samples in
    repo, there are presentation slides, workshops, labs, and prerecorded lectures
    for each chapter on my Google Cloud AI Developer Relations YouTube account ([www.youtube.com/channel/UC8OV0VkzHTp8_PUwEdzlBJg](https://www.youtube.com/channel/UC8OV0VkzHTp8_PUwEdzlBJg)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 材料是多模态的。除了书籍和仓库中的完整代码示例外，还有针对每个章节的演示文稿、研讨会、实验室和预先录制的讲座，这些都可以在我的 Google Cloud
    AI 开发者关系 YouTube 账号 ([www.youtube.com/channel/UC8OV0VkzHTp8_PUwEdzlBJg](https://www.youtube.com/channel/UC8OV0VkzHTp8_PUwEdzlBJg))
    上找到。
- en: about the author
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: I strongly believe that my lifelong experience makes me one of the most ideal
    individuals to teach concepts of deep learning. When this book is first printed,
    I will be nearly 60 years old. I have a wealth of knowledge and experience that
    translates to expectations today in the workforce. In 1987, I got my advanced
    degree in artificial intelligence. I specialized in natural language processing.
    When I got out of college, I thought I would be writing talking books. Well, it
    was the AI winter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信我终身积累的经验使我成为教授深度学习概念的理想人选之一。当这本书首次印刷时，我将近60岁。我拥有丰富的知识和经验，这些在当今的劳动力市场中得到了体现。1987年，我获得了人工智能的高级学位。我专攻自然语言处理。当我从大学毕业时，我以为我会写有声读物。嗯，那是人工智能的冬天。
- en: I took other directions in my early career. First, I became an expert in government
    security for mainframe computers. As I became more proficient designing and coding
    in operating system kernels, I became a kernel developer for UNIX, being one of
    the contributors to today’s heavyweight UNIX kernel. In those same years, I participated
    in shareware (before open source) and was the founder of WINNIX, a shareware program
    that competed with the commercial MKS Toolkit for running the UNIX shell and commands
    in a DOS environment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的早期职业生涯中，我还采取了其他方向。首先，我成为了主框架计算机政府安全方面的专家。随着我在操作系统内核设计和编码方面的技能越来越熟练，我成为了UNIX的内核开发者，是当今重型UNIX内核的众多贡献者之一。在那些同样的年份里，我参与了共享软件（在开源之前）并创立了WINNIX，这是一个与商业MKS
    Toolkit竞争在DOS环境中运行UNIX外壳和命令的共享软件程序。
- en: Subsequently, I developed low-level object code tooling. I became an expert
    at both secured-level computing and compiler/assemblers for massively parallel
    computers in the early 1990s. I developed MetaC, which provided instrumentation
    into the operating system kernels of both conventional operating systems and highly
    secured and massively parallel computers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，我开发了低级目标代码工具。在20世纪90年代初，我成为了安全级计算和大规模并行计算机的编译器/汇编器的专家。我开发了MetaC，它为传统操作系统和高度安全和大规模并行计算机的操作系统内核提供了仪器。
- en: In the late 1990s, I made a career change and became a research scientist for
    Sharp Corporation of Japan. Within a couple of years, I became the company’s principal
    research scientist in North America. Over a 20-year period, Sharp filed over 200
    US patent applications on my research, with 115 granted. My patents covered areas
    for solar energy, teleconferencing, imaging, digital interactive signage, and
    autonomous vehicles. Additionally, in 2014–2015 I was recognized as a leading
    world expert on open data and data ontologies, and founded the organization opengeocode.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代末，我改变了职业道路，成为日本夏普公司的研究科学家。在几年之内，我成为了公司在北美的主研科学家。在20年的时间里，夏普根据我的研究提交了200多项美国专利申请，其中115项获得批准。我的专利涵盖了太阳能、远程会议、成像、数字交互式标牌和自动驾驶汽车等领域。此外，在2014-2015年间，我被认定为开放数据和数据本体领域的世界级领先专家，并创立了opengeocode组织。
- en: In March of 2017, at a nudging of a friend of mine, I looked into “what’s this
    thing called deep learning?” It was natural for me. I had a big data background,
    had worked as an imaging scientist and research scientist, had an AI graduate
    degree, worked on autonomous vehicles—it all seemed to align. So, I made the leap.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年3月，在我的一个朋友的推动下，我开始了解“这个被称为深度学习的东西是什么？”对我来说这是自然而然的。我有大数据背景，曾担任成像科学家和研究员，拥有人工智能研究生学位，从事自动驾驶汽车的工作——这一切似乎都相得益彰。因此，我做出了这个跳跃。
- en: In the summer of 2018, Google approached me about being a staff member in Google
    Cloud AI. I accepted a position that October. It’s been a great experience at
    Google. Today, I work with vast numbers of AI experts within both Google and Google’s
    enterprise clients, teaching, mentoring, advising, and solving challenges to make
    deep learning operational on a large production scale.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年的夏天，谷歌向我提出担任谷歌云AI的员工。我在那年的10月接受了这个职位。在谷歌的工作经历非常棒。今天，我与谷歌及其企业客户中的大量AI专家合作，教授、指导、咨询，并解决挑战，以在大规模生产规模上使深度学习得以实施。
- en: about the cover illustration
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于封面插图
- en: The figure on the cover of *Deep Learning Patterns and Practices* is captioned
    “Indien,” or a man from India. The illustration is taken from a collection of
    dress costumes from various countries by Jacques Grasset de Saint-Sauveur (1757–1810),
    titled *Costumes de Différents Pays*, published in France in 1784\. Each illustration
    is finely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s
    collection reminds us vividly of how culturally apart the world’s towns and regions
    were just 200 years ago. Isolated from each other, people spoke different dialects
    and languages. In the streets or in the countryside, it was easy to identify where
    they lived and what their trade or station in life was just by their dress.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 《深度学习模式与实践》的封面图题为“Indien”，或印度人。这幅插图取自雅克·格拉塞·德·圣索沃尔（1757–1810）的作品集，名为《不同国家的服饰》，于1784年在法国出版。每一幅插图都是手工精心绘制和着色的。格拉塞·德·圣索沃尔收藏的丰富多样性生动地提醒我们，200年前世界的城镇和地区在文化上有多么不同。人们彼此孤立，说着不同的方言和语言。在街道或乡村，仅凭他们的服饰就能轻易识别他们居住的地方以及他们的职业或社会地位。
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 自那以后，我们的着装方式发生了变化，当时地区间的多样性已经消失。现在很难区分不同大陆、不同城镇、地区或国家的人们。也许我们用文化多样性换取了更加丰富多彩的个人生活——当然，是为了更加多样化和快节奏的技术生活。
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在难以区分一本计算机书与另一本计算机书的今天，曼宁通过基于两百年前区域生活的丰富多样性，并由格拉塞·德·圣索沃尔的图画使之重现的书封面，庆祝了计算机行业的创新精神和主动性。
