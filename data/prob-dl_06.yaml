- en: 4 Building loss functions with the likelihood approach
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用似然方法构建损失函数
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using the maximum likelihood approach for estimating model parameters
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最大似然方法估计模型参数
- en: Determining a loss function for classification problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定分类问题的损失函数
- en: Determining a loss function for regression problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定回归问题的损失函数
- en: '![](../Images/4-unnumb-1.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-1.png)'
- en: In the last chapter, you saw how you can determine parameter values through
    optimizing a loss function using stochastic gradient descent (SGD). This approach
    also works for DL models that have millions of parameters. But how did we arrive
    at the loss function? In the linear regression problem (see sections 1.4 and 3.1),
    we used the mean squared error (MSE) as a loss function. We don’t claim that it
    is a bad idea to minimize the squared distances of the data points from the curve.
    But why use squared and not, for example, the absolute differences?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你看到了如何通过使用随机梯度下降（SGD）优化损失函数来确定参数值。这种方法也适用于具有数百万个参数的深度学习模型。但我们是如何得到损失函数的呢？在线性回归问题中（见第1.4节和第3.1节），我们使用了均方误差（MSE）作为损失函数。我们不认为最小化数据点与曲线之间的平方距离是一个坏主意。但为什么使用平方而不是，例如，绝对差异呢？
- en: It turns out that there is a generally valid approach for deriving the loss
    function when working with probabilistic models. This approach is called the maximum
    likelihood approach (MaxLike). You’ll see that the MaxLike approach yields for
    the linear regression the MSE as loss function for some assumptions, which we
    discuss in detail in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在处理概率模型时，有一个普遍有效的方法来推导损失函数。这种方法被称为最大似然方法（MaxLike）。你会发现，对于线性回归，MaxLike方法在某种假设下给出了均方误差（MSE）作为损失函数，我们将在本章详细讨论。
- en: Concerning classification, you used a loss function called categorical cross
    entropy (see section 2.1). What is categorical cross entropy and how do you get
    to it in the first place? You probably can guess by which approach you can derive
    this loss function. It turns out that the likelihood function is your friend.
    It yields cross entropy as an appropriate loss function in classification tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分类，你使用了一个称为分类交叉熵的损失函数（见第2.1节）。什么是分类交叉熵？你最初是如何得到它的？你可能能猜到通过哪种方法可以推导出这个损失函数。结果是，似然函数是你的朋友。它在分类任务中提供了交叉熵作为合适的损失函数。
- en: '4.1 Introduction to the MaxLike principle: The mother of all loss functions'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 MaxLike原理简介：所有损失函数之母
- en: The fact that the MaxLike is the key to the “secret” behind almost all DL and
    ML applications is sketched in figure 4.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MaxLike是几乎所有深度学习（DL）和机器学习（ML）应用背后“秘密”的关键，这在图4.1中有所体现。
- en: '![](../Images/4-1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-1.png)'
- en: Figure 4.1 Unmasking the secrets of almost all loss functions in machine learning
    (ML in the figure) and in deep learning (*DL*). After [https://www.instagram.com/neuralnetmemes/](https://www.instagram.com/neuralnetmemes/)
    .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 揭示了机器学习（图中为ML）和深度学习（*DL*）中几乎所有损失函数的秘密。图片来源：[https://www.instagram.com/neuralnetmemes/](https://www.instagram.com/neuralnetmemes/)
- en: To demonstrate this principle, we start with a simple example far away from
    DL. Consider a die with one face showing a dollar sign ($) and the remaining five
    faces displaying dots (see figure 4.2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个原理，我们从深度学习（DL）领域之外的一个简单例子开始。考虑一个骰子，其中一个面显示美元符号（$），其余五个面显示点（见图4.2）。
- en: '![](../Images/4-2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-2.png)'
- en: Figure 4.2 A die with one side showing a dollar sign and the others displaying
    a dot
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 一个骰子，一面显示美元符号，其余面显示点
- en: 'What is the probability that the dollar sign comes up if you throw the die
    (we assume fair die here)? On average, you get a dollar sign in one out of six
    cases. The probability of seeing a dollar sign is thus p = 1/6\. The probability
    that this won’t happen and you see a dot is 5/6 = 1 - p. Let’s throw the die ten
    times. What is the probability that you see the dollar sign only one time and
    the dot nine times? Let’s assume first, that you see the dollar sign in the first
    throw and you see dots in the next nine throws. You could write this in a string
    as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你掷骰子，美元符号出现的概率是多少（我们这里假设骰子是公平的）？平均来说，每六次中出现一次美元符号。因此，看到美元符号的概率是 p = 1/6。不出现美元符号而看到点的概率是
    5/6 = 1 - p。让我们掷骰子十次。你只看到一次美元符号，九次看到点的概率是多少？首先，假设你在第一次掷骰子时看到美元符号，在接下来的九次掷骰子时看到点。你可以将这个情况写成字符串：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The probability for that particular sequence to happen would be ⅙ ⋅ ⅚ ⋅ ⅚ ⋅
    ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ = ⅙ ⋅ ⅚⁹ = 0.032 or with *p* =⅙ as *p*¹ ∗(1 −*p*)^(10−1)
    . If we only ask for the probability that a single dollar sign and nine dots occur
    in the ten throws regardless of the position, we have to take all of the following
    ten results into account:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该特定序列发生的概率将是 ⅙ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ ⋅ ⅚ = ⅙ ⋅ ⅚⁹ = 0.032，或者用 *p*
    =⅙ 表示为 *p*¹ ∗(1 −*p*)^(10−1) 。如果我们只要求在十次掷币中出现一个美元符号和九个点（无论位置如何）的概率，我们必须考虑以下所有十个结果：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To occur, each of these ten distinct sequences have the same probability of
    *p* ⋅(1 −*p*)⁹ . This yields a probability of 10 ⋅ *p* ⋅(1 −*p*)⁹ for the event
    that you observe one out of these ten sequences. In our example, we use p = 1/6
    and we get 0.323 for the probability that one dollar sign occurs during the ten
    throws. You might get curious and ask yourself what is the probability for two
    dollar signs in ten throws? The probability for a particular ordering (say `$$........`)
    is *p*² ⋅ (1 −*p* )⁸ . It turns out that there are now 45 possible ways[1](#pgfId-1044962)
    to rearrange a string like `$.$.......` or `$..$.......` : the total probability
    of two dollar signs and eight dots is thus 45 ⋅(⅙)² ⋅(⅚)⁸ = 0.2907 .'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要发生，这十个不同的序列中每一个都有相同的概率 *p* ⋅(1 −*p*)⁹ 。这意味着观察到的十个序列中有一个的概率为 10 ⋅ *p* ⋅(1 −*p*)⁹
    。在我们的例子中，我们使用 p = 1/6，得到0.323作为在十次掷币中发生一次美元符号的概率。你可能会产生好奇心，想知道在十次掷币中出现两个美元符号的概率是多少？特定顺序（例如
    `$$........`）的概率是 *p*² ⋅ (1 −*p* )⁸ 。结果是，现在有45种可能的方式[1](#pgfId-1044962)来重新排列像
    `$.$.......` 或 `$..$.......` 这样的字符串：因此，两个美元符号和八个点的总概率是 45 ⋅(⅙)² ⋅(⅚)⁸ = 0.2907
    。
- en: 'The die-throwing experiment where we count the successful throws with upcoming
    dollar signs is an example of a general class of experiments called binomial experiments.
    In a binomial experiment, you count the number of successes (here seeing a $)
    in n trials (here throwing a die), where all the trials are independent from each
    other and the probability for success is the same in each trial. The number of
    successes in a binomial experiment with n trials is not fixed but can usually
    take any values between 0 and n (if p is not exactly 0 or 1). For this reason,
    the number of successes, k, is called a random variable. To emphasize that k is
    a random variable coming from a binomial distribution, one writes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计数掷出带有美元符号的成功次数的掷币实验是称为二项实验的一般类实验的例子。在二项实验中，你计数 n 次试验中的成功次数（在这里是掷骰子），其中所有试验都是相互独立的，并且每次试验成功的概率相同。在
    n 次试验的二项实验中，成功次数不是固定的，但通常可以取0到 n 之间的任何值（如果 p 不是正好为0或1）。因此，成功次数 k 被称为随机变量。为了强调
    k 是来自二项分布的随机变量，可以写成：
- en: '*k* ∼ binom(*n*, *p*)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* ∼ binom(*n*, *p*)'
- en: The ~ symbol reads “stems from” or “is distributed like” a binomial with n equal
    to the number of tries and p equal to the probability of success in a single try.
    In the context of this book, it is not so important how to derive the probability
    of a certain k. But there is a SciPy function called `binom.pmf` to calculate
    this with the arguments `k` equals the number of successes, `n` equals the number
    of tries, and `p` equals the probability for success in a single try. Using this
    function, we can plot the probability that 0 to 10 dollar signs occur in 10 throws.
    See listing 4.1 for the code and figure 4.3 for the result.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ~ 符号读作“来自”或“分布类似于”一个二项分布，其中 n 等于尝试次数，p 等于单次尝试成功的概率。在本书的上下文中，如何推导出某个 k 的概率并不那么重要。但有一个名为
    `binom.pmf` 的SciPy函数可以用来计算这个概率，其参数 `k` 等于成功次数，`n` 等于尝试次数，`p` 等于单次尝试成功的概率。使用这个函数，我们可以绘制在10次掷币中出现0到10个美元符号的概率图。请参见列表4.1中的代码和图4.3中的结果。
- en: '![](../Images/4-3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-3.png)'
- en: Figure 4.3 Probability distribution for the number of observed dollar signs
    in 10 die throws. The probability of a dollar sign in an individual throw is p
    = 1/6\. The probabilities for one and two dollar signs turning up (0.323 and 0.2907)
    is the same as that calculated by hand. This figure is created using the code
    in listing 4.1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 10次掷骰子中观察到的美元符号数量的概率分布。单个掷币中出现美元符号的概率为 p = 1/6。一个和两个美元符号出现的概率（0.323和0.2907）与手工计算的结果相同。此图使用列表4.1中的代码创建。
- en: Listing 4.1 The probability for 0 to 10 thrown dollar signs using the `binom`
    function
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 使用`binom`函数计算0到10次掷出美元符号的概率
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The numbers of successes (thrown dollar signs), 0 to 10 as 11 integers
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 成功次数（掷出的美元符号），从0到10共11个整数
- en: ❷ The probability of throwing 0, 1, 2 . . ., dollar signs, each with the probability
    p = 1/6
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 投掷出0、1、2……美元符号的概率，每个符号的概率为p = 1/6
- en: 'So far, so good. You might remember this from your probability class. Now we
    turn the tables. Consider the following situation: you are in a casino and play
    a game in which you win if the dollar sign appears. You know that there are a
    certain number of faces (0 to 6) with the dollar sign, but you don’t know how
    many. You observe ten throws of the die and two dollar signs come up in those
    throws. What do you guess for the number of dollar signs on the die? Surely it
    cannot be zero because you observed a dollar sign, and on the other hand, it cannot
    be six either because you would have observed no dots. But what would be a good
    guess?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。你可能记得这是从你的概率课程中学到的。现在我们换个角度。考虑以下情况：你在一个赌场里玩一个游戏，如果你看到美元符号，你就能赢。你知道有特定数量的面（0到6）带有美元符号，但你不知道具体有多少。你观察到十次投掷骰子的结果，其中有两次出现了美元符号。你会猜骰子上有多少个美元符号？当然不可能是零，因为你观察到了美元符号，另一方面，也不可能是六，因为你没有观察到点数。但什么是一个好的猜测呢？
- en: Looking at listing 4.1 again, you suddenly have a genius idea. Simply calculate
    the probabilities to observe two dollar signs in ten throws again, but this time
    assume that your die has not only one face with a dollar sign, but it’s on two
    faces. Then assume your die has a dollar sign on three faces and again determine
    the probability to see two dollar signs in ten throws and so on. Your observed
    data is fixed (two dollar signs in ten throws), but your assumed model of how
    the data is generated changes from a die with zero dollar faces to a die with
    1, 2, 3, 4, 5, or 6 dollar faces. The probability to observe a dollar sign in
    one throw can be seen as a parameter in your model of the die. This parameter
    takes the values p = 0/6, 1/6, 2/6, . . . , 6/6 for your different die models.
    For each of these die models, you can determine the probability to observe two
    dollar signs in ten throws and plot it in a chart (see figure 4.4).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看列表4.1，你突然有了一个天才的想法。简单地再次计算在十次投掷中观察到两个美元符号的概率，但这次假设你的骰子不仅有带美元符号的一面，而是有两面。然后假设你的骰子有三面带美元符号，再次确定在十次投掷中看到两个美元符号的概率，依此类推。你的观察数据是固定的（十次投掷中出现两个美元符号），但你的数据生成假设模型从没有美元符号的骰子变为有1、2、3、4、5或6个美元符号的骰子。在一次投掷中观察到美元符号的概率可以看作是骰子模型中的一个参数。这个参数对于不同的骰子模型取值为p
    = 0/6, 1/6, 2/6, ……, 6/6。对于这些骰子模型中的每一个，你可以确定在十次投掷中观察到两个美元符号的概率，并在图表中绘制出来（见图4.4）。
- en: '| ![](../Images/computer-icon.png) | Hands-on timeOpen [http://mng.bz/eQv9](http://mng.bz/eQv9)
    . Work through the code until you reach the first exercise. For this exercise,
    it is your task to determine the probability of observing a dollar sign twice
    in 10 die throws if you consider a die that has dollar signs on 0, 1, 2, 3, 4,
    5, or all 6 faces. Plotting the computed probabilities yields the plot in figure
    4.4. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/computer-icon.png) | 实践时间打开[http://mng.bz/eQv9](http://mng.bz/eQv9)。运行代码直到你达到第一个练习。对于这个练习，你的任务是确定如果你考虑一个有0、1、2、3、4、5或所有6个面都带有美元符号的骰子，在10次骰子投掷中观察到美元符号两次的概率。绘制计算出的概率会产生图4.4中的图表。
    |'
- en: '![](../Images/4-4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-4.png)'
- en: Figure 4.4 Likelihoods for observing k = 2 dollar signs in n = 10 throws for
    the different number of dollar signs on the die
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：不同骰子上的美元符号数量在n=10次投掷中观察到k=2个美元符号的可能性
- en: What do we see in figure 4.4? Starting from the left, if you have zero dollar
    signs on the die then the probability that you observe two dollar signs in ten
    throws is zero. Well, that was expected. Next, compute the probability to observe
    two times a dollar sign in ten throws, assuming that you have only one dollar
    sign on the die (p = 1/6). This is nearly 0.3\. If you assume a die with two dollar
    signs, then the probability to observe two dollar signs in ten throws is approximately
    0.20 and so on. What would you guess is the number of dollar signs on the die?
    You would guess one because a die with one dollar sign has the highest probability
    to yield two dollar signs in ten throws. Congratulations! You have just discovered
    the MaxLike principle.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图4.4中看到了什么？从左边开始，如果你掷出的骰子上没有美元符号，那么你在十次投掷中观察到两个美元符号的概率为零。嗯，这是预期的。接下来，计算在骰子上只有一个美元符号（p
    = 1/6）的情况下，在十次投掷中观察到两次美元符号的概率。这个概率接近0.3。如果你假设骰子上有两个美元符号，那么在十次投掷中观察到两个美元符号的概率大约为0.20，依此类推。你会猜骰子上有多少个美元符号？你会猜一个，因为只有一个美元符号的骰子在十次投掷中产生两个美元符号的概率最高。恭喜！你刚刚发现了最大似然原理。
- en: The MaxLike mantra Choose the model’s parameter(s) so that the observed data
    has the highest likelihood.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MaxLike 口诀：选择模型的参数（s），使得观察到的数据具有最高的似然。
- en: 'In our case, the parametric model is the binomial distribution that has two
    parameters: the success probability, p, per trial and the number of conducted
    trials, n. We observed the data k = 2 dollar signs at n = 10 throws. The parameter
    of the model is p, the probability that a single die throw shows a dollar sign.
    The likelihood for different numbers of dollar signs on the die is shown in figure
    4.4\. We choose the value with maximal likelihood (p = 1/6), corresponding to
    one dollar sign on the die.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，参数模型是二项分布，它有两个参数：每次试验的成功概率 p 和进行的试验次数 n。我们观察到在 n = 10 次投掷中有 k = 2 个美元符号。模型的参数是
    p，即单次掷骰子显示美元符号的概率。不同数量美元符号的似然在图 4.4 中显示。我们选择最大似然值（p = 1/6），对应于骰子上有一个美元符号。
- en: 'There is one small subtlety. The probabilities in figure 4.4 are unnormalized
    probabilities in the sense that these do not add up to 1 but to a constant factor
    instead. In our case, this factor is 0.53\. Therefore, the probabilities are not
    probabilities in the strictest sense, which must add up to 1\. This is the reason
    we speak of likelihoods instead of probabilities. But we can still use these likelihoods
    for ranking, and we pick the model yielding the highest likelihood as the most
    probable model. Also, for simple cases such as ours, we could divide each likelihood
    by the sum of all likelihoods to transform those into proper probabilities. Let’s
    recap the steps in this MaxLike approach for determining the best parameter values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个小的细微差别。图 4.4 中的概率是未归一化的概率，因为这些概率相加不等于 1，而是等于一个常数因子。在我们的例子中，这个因子是 0.53。因此，这些概率在严格意义上不是概率，因为它们必须加起来等于
    1。这就是我们说似然而不是概率的原因。但我们可以仍然使用这些似然进行排序，我们选择产生最高似然的模型作为最可能的模型。此外，对于像我们这样的简单情况，我们可以将每个似然除以所有似然的和，将它们转换为适当的概率。让我们回顾一下在
    MaxLike 方法中确定最佳参数值的步骤：
- en: You need a model for the probability distribution of the observed data that
    has one or several parameters.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要一个模型来描述观察数据的概率分布，该模型有一个或多个参数。
- en: Here the data was the number of times you saw the dollar sign when throwing
    the die ten times. The parameter p of the binomial distribution was the probability
    p of the die showing a dollar sign (the number of faces on the die showing dollar
    signs divided by six).
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，数据是掷骰子十次时看到美元符号的次数。二项分布的参数 p 是骰子显示美元符号的概率 p（显示美元符号的骰子面数除以六）。
- en: You use the model to determine the likelihood to get the observed data when
    assuming different values of the parameter in the model.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你使用该模型来确定在假设模型中参数的不同值时，得到观察数据的似然。
- en: Here you computed the probability to get two dollar signs in ten throws when
    your assumed die has either 0, 1, 2, 3, 4, 5, or 6 dollar faces.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你计算了当假设骰子有 0、1、2、3、4、5 或 6 个美元面时，在十次投掷中得到两个美元符号的概率。
- en: You take the parameter value for which the likelihood to get the observed data
    is maximal as the optimal parameter value. This is also called the MaxLike estimator.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择使观察到的数据的似然最大的参数值作为最佳参数值。这也被称为 MaxLike 估计器。
- en: Here the ML estimator is that the die has one side with a dollar sign.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，ML 估计器是骰子有一面是美元符号。
- en: 4.2 Deriving a loss function for a classification problem
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 为分类问题推导损失函数
- en: In this section, we show you how to apply the MaxLike principle to derive the
    loss function for classification problems and to demystify the ten-dollar word
    categorical cross entropy. It turns out that this quantity is quite simple to
    calculate, and you will see how you can use it for sanity checking your models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们向您展示如何应用 MaxLike 原则来推导分类问题的损失函数，并揭开“分类交叉熵”这个十美元词的神秘面纱。结果证明，这个量计算起来相当简单，您将看到如何使用它来对您的模型进行合理性检查。
- en: 4.2.1 Binary classification problem
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 二分类问题
- en: Let’s revisit the fake banknote example from chapter 2 (see listing 2.2, which
    we repeat here).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第二章中的假钞例子（参见列表 2.2，此处重复列出）。
- en: Listing 2.2 Definition of a classification network with two hidden layers
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 定义具有两个隐藏层的分类网络
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ This loss, the 'categorical_crossentropy', from listing 2.2 is now explained.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里解释了列表 2.2 中的 'categorical_crossentropy' 损失。
- en: 'You also used `''categorical_crossentropy''` for all classification problems
    in chapter 2: the fcNN, the CNN applied to the MNIST handwritten digits classification
    problem, and the CNN for detecting stripes in an art piece. This loss function
    is generally used for classification problems in DL.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，你也为所有分类问题使用了`'categorical_crossentropy'`：全连接神经网络（fcNN）、应用于MNIST手写数字分类问题的卷积神经网络（CNN）以及用于检测艺术品条纹的CNN。这个损失函数通常用于深度学习中的分类问题。
- en: To explain `'categorical_crossentropy'` , let’s start with the banknote example.
    In that case, the output of the first neuron (labeled p0 in figure 4.5) is the
    probability that the model “thinks” a given input *x* belongs to a class 0 (a
    real banknote). The output of the other neuron (labeled p1 in the figure) is the
    probability that *x* describes a fake class. Of course, p0 and p1 add up to one.
    This is assured by the `softmax` activation function(see chapter 2).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释`'categorical_crossentropy'`，让我们从纸币的例子开始。在这种情况下，第一个神经元（在图4.5中标为p0）的输出是模型“认为”给定输入*x*属于类别0（真实纸币）的概率。其他神经元的输出（在图中标为p1）是*x*描述假类别的概率。当然，p0和p1的总和为1。这是由`softmax`激活函数（见第2章）保证的。
- en: '![](../Images/4-5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-5.png)'
- en: Figure 4.5 The classification network for banknotes described by the features
    *x*[1] and *x*[2] with two outputs yielding the probability for a real banknote
    (*p*[0]) or a fake banknote (*p*[1]). This is the same as figure 2.8 in chapter
    2.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5展示了由特征*x*[1]和*x*[2]描述的用于识别纸币的分类网络，该网络有两个输出，分别给出真实纸币的概率(*p*[0])和假币的概率(*p*[1])。这与第2章中的图2.8相同。
- en: We now use the MaxLike principle to derive the loss function. What is the likelihood
    of the observed data? The training data in classification problems comes in pairs
    (*x**[i]* and *y**[i]* ). In the banknote example, *x**[i]* is a vector with two
    entries and *y**[i]* is the true class of the example (banknote is fake or real).
    The CNN (see figure 4.5) takes an input *x* and outputs a probability for each
    possible class. These probabilities define the conditional probability distribution(CPD)
    for a given *x*(see the figure in the following sidebar). The likelihood of the
    observed (true) outcome class, *y**[i]* , is given by the CPD for the true class
    *y**[i]* . For an NN with known weights and a given input *x**[i]* , the likelihood
    is given by p0(*x**[i]* ) if the true class is *y**[i]* = 0 or by the output p1(xi)
    if the true class is *y**[i]* = 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用最大似然原理来推导损失函数。观察到的数据的似然性是什么？在分类问题中，训练数据以成对的形式出现(*x**[i]* 和 *y**[i]* )。在纸币的例子中，*x**[i]*
    是一个有两个条目的向量，*y**[i]* 是示例的真实类别（纸币是假的还是真的）。CNN（见图4.5）接收输入*x*并输出每个可能类别的概率。这些概率定义了给定*x*的条件概率分布（CPD）（见下文侧边栏中的图）。观察到的（真实）结果类别*y**[i]*
    的似然性由真实类别*y**[i]* 的CPD给出。对于具有已知权重和给定输入*x**[i]* 的NN，如果真实类别是*y**[i]* = 0，则似然性由p0(*x**[i]*
    )给出；如果真实类别是*y**[i]* = 1，则似然性由输出p1(xi)给出。
- en: 'Important Keep this in mind: the likelihood of a classification model for a
    given training example (*x**[i]* , *y**[i]* ) is simply the probability the network
    assigns to the correct class yi.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要：请记住：对于给定的训练示例(*x**[i]* , *y**[i]* )，分类模型的似然性就是网络分配给正确类别yi的概率。
- en: As an example, a well-trained network returns a high value for p1 if the training
    example is from class 1 (fake banknote). What is the probability for the training
    set as a whole? Here we make the assumption that all the examples in the training
    are independent from each other. Then the probability of the whole training data
    set is simply the product of the individual probabilities. For example, imagine
    you throw a regular die twice and you ask for the probability to have a 1 in the
    first throw and a 2 in the second. This is simply 1/6 · 1/6 because the throws
    are independent. This carries on and, in general, the likelihood of the whole
    training set is the product of all individual examples. We go from one training
    example after another and look at what is the probability for the true class.
    Then we take the product of all those probabilities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个训练良好的网络在训练示例来自类别1（假币）时，会返回一个高p1值。整个训练集的概率是多少？这里我们假设训练集中的所有示例都是相互独立的。因此，整个训练数据集的概率就是各个概率的乘积。例如，想象你掷一个标准的骰子两次，并询问第一次掷出1和第二次掷出2的概率。这很简单，因为1/6
    · 1/6，因为掷骰子是独立的。这可以一直进行下去，一般来说，整个训练集的似然性是所有单个示例的乘积。我们一个接一个地看每个训练示例的概率，然后取所有这些概率的乘积。
- en: 'We could also order the probabilities as follows: first, in our example, we
    take the real banknotes (for which *y* = 0) and multiply the predictions p0 together.
    Then, we take the fake ones and multiply p1 together. Let’s say you have five
    banknotes in your training set. The first three examples come from real banknotes,
    the last two examples from fake ones. You get from the NN for all banknotes the
    probabilities to belong to class zero (*p*[0]) or to class one (*p*[1]). Then
    the likelihood of all five banknotes is'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以按以下顺序排列概率：首先，在我们的例子中，我们取真实纸币（对于*y* = 0）并将预测p0相乘。然后，我们取假币并将p1相乘。假设你在训练集中有五张纸币。前三个例子来自真实纸币，最后两个例子来自假币。从NN为所有纸币得到属于类别零(*p*[0])或类别一(*p*[1])的概率。然后，五张纸币的似然度为
- en: '![](../Images/equation_4-09.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-09.png](../Images/equation_4-09.png)'
- en: 'The *Π* in the equation tells you to take the product, and *σ* tells you to
    take the sum. In general, this can be written as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式中的*Π*表示取乘积，而*σ*表示取和。一般来说，这可以写成：
- en: '![](../Images/equation_4-10.png)                     Equation 4.1'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![方程式4.1](../Images/equation_4-10.png)                     方程式4.1'
- en: We can also explain equation 4.1 in a slightly different way, based on formulating
    the probability distribution for an outcome *y*(see the sidebar). Because this
    point of view can help you to digest the ML approach from a more general perspective,
    we give this explanation in the following sidebar.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用稍微不同的方式解释方程4.1，基于对输出*y*的概率分布进行公式化（参见侧边栏）。因为这种观点可以帮助你从更一般的角度理解机器学习方法，所以我们将在以下侧边栏中给出这种解释。
- en: MaxLike approach for the classification loss using a parametric probability
    model
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数概率模型进行分类损失的MaxLike方法
- en: 'The NN with fixed weights in figure 4.5 outputs the probabilities for all possible
    class labels *y* when feeding in a certain input *x*. This can be written as the
    probability distribution for the outcome *y*, which depends on the input value
    *x* and the weights of the NN:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5中具有固定权重的NN在输入特定输入*x*时，输出所有可能的类别标签*y*的概率。这可以写成输出*y*的概率分布，它取决于输入值*x*和NN的权重：
- en: '![](../Images/equation_4-11s.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-11s](../Images/equation_4-11s.png)'
- en: Y = k states that the random variable *y* takes a specific value k. In the equation,
    you can further read the vertical bar as “with given” or “conditioned on.” To
    the right of the bar comes all given information, which is used to determine the
    probability of the variable to the left of the bar. Here you need to know the
    value of the input *x* and the weight W of the NN to compute the outputs of the
    NN, which are p0 and p1\. Because the probability distribution depends on *x*,
    it is called a conditional probability distribution (CPD). Often you see a simpler
    version of this equation that skips the part to the right of the bar (either only
    W = w or both *x* = *x* and W = w) and that assumes this is self-evident.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Y = k 表示随机变量*y*取特定值k。在方程中，你可以进一步将竖线读作“给定”或“条件”。竖线右侧是所有给定信息，这些信息用于确定竖线左侧变量的概率。在这里，你需要知道输入*x*和NN的权重W的值来计算NN的输出，即p0和p1。因为概率分布依赖于*x*，所以它被称为条件概率分布（CPD）。通常你会看到这个方程的简化版本，它省略了竖线右侧的部分（要么只有W
    = w，要么*x* = *x*和W = w），并假设这是不言而喻的。
- en: '![](../Images/equation_4-12s.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-12s.png](../Images/equation_4-12s.png)'
- en: This probability distribution where the outcome *y* can only take the value
    0 or 1 is known as Bernoulli distribution, which has only one parameter p. From
    this, you can directly compute p0 = 1 - p1\. The following figure shows this probability
    distribution for the binary outcome *y*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种输出*y*只能取0或1值的概率分布称为伯努利分布，它只有一个参数p。据此，你可以直接计算p0 = 1 - p1。以下图显示了二元输出*y*的概率分布。
- en: '![](../Images/4-unnumb-2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-14s.png](../Images/4-unnumb-2.png)'
- en: The probability distribution of a binary variable *y*, also known as Bernoulli
    distribution
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 二元变量*y*的概率分布，也称为伯努利分布
- en: 'The NN in figure 4.5 computes for each input p0 and p1\. For n data points,
    the probability or likelihood of the data is given by the product of the computed
    probabilities for the correct class (see equation 4.1). The MaxLike principle
    tells you that you should tune the network weights w such that the likelihood
    gets maximized like so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5中的NN为每个输入计算p0和p1。对于n个数据点，数据的概率或似然度是计算出的正确类别的概率的乘积（见方程式4.1）。MaxLike原则告诉你应该调整网络权重w，使得似然度最大化，如下所示：
- en: '![](../Images/equation_4-13s.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-13s.png](../Images/equation_4-13s.png)'
- en: In principle, we are now done. We could maximize equation 4.1 by tuning the
    weights of the network. You do not need to do this by hand, but you can use any
    framework such as TensorFlow or Keras. Both of these can do (stochastic) gradient
    descent.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，我们现在已经完成了。我们可以通过调整网络的权重来最大化方程4.1。您不需要手动完成这项工作，但可以使用任何框架，如TensorFlow或Keras。这两个框架都可以进行（随机）梯度下降。
- en: There is still a practical issue remaining. The p0 and p1 in equation 4.1 are
    numbers between 0 and 1, and some of these might be small. Multiplying many numbers
    in the range 0 to 1 yields numerical problems (see listing 4.2).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然存在一个实际问题。方程4.1中的p0和p1是介于0和1之间的数，其中一些可能很小。在0到1的范围内乘以许多数会导致数值问题（参见列表4.2）。
- en: Listing 4.2 Numerical instabilities when multiplying many numbers between 0
    and 1
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 在0到1之间乘以许多数时的数值不稳定性
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Multiplies 100 numbers randomly between 0 and 1.0
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机乘以0到1.0之间的100个数。
- en: ❷ Multiplies 1,000 numbers randomly between 0 and 1.0
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机乘以0到1.0之间的1,000个数。
- en: ❸ A typical result (7.147335361549675e-43, 0.0) for 1,000 numbers gives you
    0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于1,000个数的一个典型结果（7.147335361549675e-43, 0.0）给出的是0。
- en: If we take more than a few hundred examples, the product is so close to zero
    that it is treated as a zero due to the limited precision of the floating-point
    numbers in a computer. DL uses typical floating-point numbers of type float32,
    and for those, the smallest number (next to zero) is approximately 10-45.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取超过几百个例子，乘积接近零，由于计算机中浮点数的有限精度，它被视为零。DL使用典型的float32浮点数类型，对于这些类型，紧邻零的最小数大约是10^-45。
- en: There is a trick to fix this issue. Instead of *P*(Training) in equation 4.1,
    you can take the logarithm of the likelihood. Taking the logarithm changes the
    values of a function but does not change the position at which the maximum is
    reached. As a side note, the property that the maximum stays the same is due to
    the fact that the logarithm of *x* is a function that strictly grows as *x* gets
    larger. Functions with this property are called strictly monotonic functions.
    In figure 4.6, you see an arbitrary function *f*(*x*) and its logarithm log(*f*(*x*)
    ). The maximum of both *f*(*x*) and log(*f*(*x*)) is reached at *x* ≈ 500 .
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个技巧可以解决这个问题。在方程4.1中，您可以将似然的对数替换掉*P*(Training)。取对数会改变函数的值，但不会改变达到最大值的位置。作为旁注，最大值保持不变的性质是由于对数函数*x*是一个随着*x*增大而严格增长的函数。具有这种性质的函数被称为严格单调函数。在图4.6中，您可以看到一个任意函数*f*(*x*)及其对数log(*f*(*x*))。*f*(*x*)和log(*f*(*x*))的最大值都在*x*
    ≈ 500处达到。
- en: '![](../Images/4-6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-6.png)'
- en: Figure 4.6 An arbitrary function f(x) with non-negative values (solid line)
    and the logarithm of that function (Dashed line). While the maximum value (approximately
    2) changes if you take the logarithm, the position at which the maximum is reached
    (approximately 500) stays the same regardless of whether or not you take the logarithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 一个具有非负值的任意函数f(x)（实线）及其对数（虚线）。虽然取对数后最大值（大约为2）会变化，但达到最大值的位置（大约为500）无论是否取对数都保持不变。
- en: 'What have you gained by taking the logarithm? The log of the product of any
    numbers is the sum of the logs, which means log( *A* ⋅ *B* ) = *log* ( *A* ) +
    *log* ( *B* ) . This formula can be extended to an arbitrary number of terms:
    *log* ( *A* ⋅ *B* ⋅ *C* ⋅ ...) = *log* (A) + *log* ( *B* ) + *log* ( *C* ) , so
    the products in equation 4.1 become the sums of the logarithms. Let’s look at
    the consequences for numerical stability.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过取对数，您获得了什么？任何数的乘积的对数是这些数的对数之和，这意味着log(*A* ⋅ *B*) = *log* (*A*) + *log* (*B*)。这个公式可以扩展到任意多个项：*log*
    (*A* ⋅ *B* ⋅ *C* ⋅ ...) = *log* (A) + *log* (*B*) + *log* (*C*)，因此方程4.1中的乘积变成了对数之和。让我们看看这对数值稳定性的影响。
- en: You now add the log of numbers between 0 and 1\. For 1, you get *log* (1) =
    0 ; for 0.0001, you get *log* (0.0001) ≈ −4 . The only numerical problem arises
    if you really get a probability of 0 for the correct class, then the log of zero
    is minus infinity. To prevent this, sometimes a small number like 10E−20 is added
    to the probabilities. But let’s not bother here about this extremely unlikely
    situation. What happens if you change listing 4.2 from products to sums of logs?
    You get a numerically stable result (see listing 4.3).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在添加0到1之间的数字的对数。对于1，你得到*对数* (1) = 0；对于0.0001，你得到*对数* (0.0001) ≈ −4。唯一可能出现的数值问题是，如果你真的得到了正确类别的概率为0，那么零的对数是负无穷大。为了防止这种情况，有时会在概率中添加一个非常小的数，比如10E−20。但在这里我们不必担心这种极不可能的情况。如果你将列表4.2从乘积改为对数和，会发生什么？你得到一个数值稳定的计算结果（见列表4.3）。
- en: Listing 4.3 Fixing numerical instabilities by taking the log
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 通过取对数来修复数值不稳定性
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The product based on the same sampled numbers as in listing 4.2 becomes the
    sum of the logs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与列表4.2中相同的采样数字的乘积变成了对数和。
- en: ❷ A typical result (-89.97501927715012, -987.8053926732027) for 1,000 numbers
    gives you a valid value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于1,000个数字的典型结果（-89.97501927715012，-987.8053926732027）给出了一个有效值。
- en: 'Using the log trick transforms the MaxLike equation (4.1) to the maximum log
    likelihood equation shown here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数技巧将最大似然方程（4.1）转换为这里所示的最大对数似然方程：
- en: '![](../Images/equation_4-19.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-19](../Images/equation_4-19.png)'
- en: This log trick does not depend on the bases you work with, but it is useful
    to know that Keras uses the natural logarithm to calculate the loss function.
    We are now nearly at the end, just two more minor details.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对数技巧不依赖于你使用的底数，但了解Keras使用自然对数来计算损失函数是有用的。我们现在几乎到了终点，只剩下两个小细节。
- en: 'In equation 4.2, you add as many numbers as training data. Equation 4.2 therefore
    depends on the number of training data n. To have a quantity that does not depend
    systematically on the number of training data, you can divide the equation by
    n. In this case, you consider the mean log-likelihood per observation. The last
    point is instead of maximizing, DL frameworks are usually built to minimize a
    loss function. So instead of maximizing log(P(Training)), you minimize log(P(Training)).
    And voilà! You have derived the negative log likelihood(NLL) function for a binary
    classification model, which is also called cross entropy. You will see in the
    next section where the name cross entropy comes from. After achieving our goal
    to derive the loss function of a binary classifier, let’s write it down once more:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程4.2中，你添加了与训练数据一样多的数字。因此，方程4.2依赖于训练数据数量n。为了得到一个不系统依赖于训练数据数量的量，你可以将方程除以n。在这种情况下，你考虑每个观察的平均对数似然。最后一点是，深度学习框架通常被构建为最小化损失函数，而不是最大化。因此，你最小化log(P(Training))而不是最大化log(P(Training))。哇！你已经推导出了二元分类模型的负对数似然(NLL)函数，这也被称为交叉熵。你将在下一节中看到交叉熵这个名字的由来。在达到推导二元分类器损失函数的目标后，让我们再次写下它：
- en: '![](../Images/equation_4-20.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-20](../Images/equation_4-20.png)'
- en: You can now verify that this is indeed the quantity that gets minimized in DL.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以验证这确实是深度学习中需要最小化的量。
- en: '| ![](../Images/computer-icon.png) | Hands-on time[Open](http://mng.bz/pBY5)
    http://mng.bz/pBY5 and run the code where you create a subset of the MNIST digits
    with only two classes (0 and 1). Your task in this first exercise is to use the
    function `model.evaluate` to determine the cross entropy loss of the untrained
    model and to explain the value that you obtain. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间[打开](http://mng.bz/pBY5) http://mng.bz/pBY5
    并运行代码，其中你创建了一个包含仅两个类别（0和1）的MNIST数字子集。在这个第一个练习中，你的任务是使用`model.evaluate`函数确定未训练模型的交叉熵损失，并解释你获得的价值。|'
- en: With the untrained model, you achieve a cross entropy of around 0.7\. Of course,
    this is a random result because the weights of the network have their initial
    random values and no training has happened yet. Expect some random deviation from
    0.7\. Can you explain that value of 0.7 with what you just learned? The network
    knows nothing at the beginning. What mean hitting rate would you expect? About
    50% right? Let’s calculate *ln*(0.5). That’s 0.69, which looks about right. (Keras
    uses the natural logarithm to calculate the loss function.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用未训练的模型，你达到了大约0.7的交叉熵。当然，这是一个随机结果，因为网络的权重有它们的初始随机值，还没有进行过训练。期望从0.7有一些随机偏差。你能用你刚刚学到的知识解释0.7这个值吗？网络一开始一无所知。你期望的命中率是多少？大约50%对吗？让我们计算*ln*(0.5)。那是0.69，看起来很合适。（Keras使用自然对数来计算损失函数。）
- en: Loss function for classification of two classes with a single output
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 具有一个输出节点的两类分类损失函数
- en: 'For the special case where you have two classes, like in the banknote example,
    there is the possibility of having a network with one output neuron. In that case,
    the output is the probability for class p1\. The probability for the other class,
    *p*[0], is then given by *p*[0] = 1 - *p*[1]. Because of that, we do not need
    a one-hot encoding for yi. It is either *y**[i]* = 0 for class 0 or *y**[i]* =
    1 for class 1\. Using that we can rewrite equation 4.3 as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像银行票据示例中那样的两个特殊类别，存在一个网络只有一个输出神经元的可能性。在这种情况下，输出是类别p1的概率。其他类别的概率，*p*[0]，由 *p*[0]
    = 1 - *p*[1] 给出。因此，我们不需要对yi进行one-hot编码。它要么是类别0的 *y**[i]* = 0，要么是类别1的 *y**[i]*
    = 1。利用这一点，我们可以将方程4.3重写为：
- en: '![](../Images/equation_4-22s.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/equation_4-22s.png)'
- en: In contrast to this equation, we don’t have to check to which class an example
    belongs. If example i belongs to class 1 (*y**[i]* = 1), the first part is taken
    containing *p*[1]. Otherwise, if example i belongs to class 0 (*y**[i]* = 0),
    the second part where *p*[0] = 1 - *p*[1] is active.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与此方程不同，我们不需要检查示例属于哪个类别。如果示例i属于类别1（*y**[i]* = 1），则取包含 *p*[1] 的第一部分。否则，如果示例i属于类别0（*y**[i]*
    = 0），则激活第二部分，其中 *p*[0] = 1 - *p*[1]。
- en: 4.2.2 Classification problems with more than two classes
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 具有两个以上类别的分类问题
- en: 'What happens if you have more than two classes? Nothing special, you might
    think, and you are right. You’ve already done this in several exercises in chapter
    2 where you tackled the task of discriminating between the ten digits in the MNIST
    data set. Recall, when setting up the DL models for the MNIST task, you used the
    same loss as in a binary classification model: `loss=''categorical_crossentropy''`
    . Let’s check out how you can use the MaxLike approach to derive the loss function
    and show that using cross entropy is appropriate.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多于两个类别会发生什么？你可能认为没有什么特别的，你是对的。你已经在第2章的几个练习中做过这件事，当时你处理了在MNIST数据集中区分十个数字的任务。回想一下，在为MNIST任务设置深度学习模型时，你使用了与二分类模型相同的损失：`loss='categorical_crossentropy'`。让我们看看你如何使用最大似然方法推导损失函数，并证明使用交叉熵是合适的。
- en: 'Recall the probabilistic modeling you did for the binary classification task.
    You used an NN with two output nodes (see figure 4.5) yielding, for each input,
    the corresponding probabilities p 0 and p 1 for the classes 0 and 1, respectively.
    You can interpret these two probabilities as the parameters of the CPD for a binary
    classification task (see the figure in the first sidebar of this chapter). The
    model of this CPD is the Bernoulli distribution (see the first sidebar in this
    chapter). In principle, the Bernoulli distribution only needs one parameter: the
    probability of class one, p 1\. This parameter is given by the second output node.
    You can derive the probability for class 0 from p 1 by p 0 = 1 - p 1; the usage
    of the `softmax` activation function ensures that the first output of the NN returns
    p 0\. When following the MaxLike approach, the loss for a binary classification
    task is given by the mean NLL of the Bernoulli distribution (see equations 4.2
    and 4.3).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你在二分类任务中进行的概率建模。你使用了一个有两个输出节点的神经网络（见图4.5），为每个输入提供了对应于类别0和1的p 0和p 1概率。你可以将这些两个概率解释为二分类任务的CPD参数（参见本章第一个侧边栏中的图）。这个CPD的模型是伯努利分布（参见本章第一个侧边栏）。原则上，伯努利分布只需要一个参数：类别一的概率，p
    1。这个参数由第二个输出节点给出。你可以通过p 0 = 1 - p 1从p 1推导出类别0的概率；`softmax`激活函数的使用确保了神经网络的第一输出返回p
    0。在遵循最大似然方法时，二分类任务的损失由伯努利分布的均方误差NLL给出（参见方程4.2和4.3）。
- en: Let’s use the same procedure for a classification task with more than two classes.
    In the MNIST example, you have ten classes and you use an NN with ten output nodes,
    one for each class. Recall, for example, the architecture that we used in chapter
    2 for the MNIST handwritten digit classification task shown in figure 2.12, which
    we repeat in figure 4.7.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用相同的程序处理多于两个类别的分类任务。在MNIST示例中，你有十个类别，你使用一个有十个输出节点的神经网络，每个类别一个。回想一下，例如，我们在第2章中用于MNIST手写数字分类任务的架构，如图2.12所示，我们在图4.7中重复了它。
- en: '![](../Images/4-7.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-7.png)'
- en: Figure 4.7 A fully connected NN (fcNN) with two hidden layers. For the MNIST
    example, the input layer has 784 values for the 28 × 28 pixels. The output layer
    has ten nodes, one for each class.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 一个具有两个隐藏层的全连接神经网络（fcNN）。对于MNIST示例，输入层有784个值，对应于28 × 28像素。输出层有十个节点，每个类别一个。
- en: 'In this MNIST task, you want to discriminate between ten classes (0, 1, . . . ,
    9). Therefore, you set up an NN with ten output nodes, each providing the probability
    that the input corresponds to the respective class. These ten probabilities define
    the ten parameters of the CPD in the MNIST classification model. The model of
    the classification CPD is called multinomial distribution, which is an extension
    of the Bernoulli distribution to more than two classes. In the case of the MNIST
    classification task with ten classes, the multinomial CPD can be expressed as
    follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个MNIST任务中，你想要区分十个类别（0，1，……，9）。因此，你设置了一个具有十个输出节点的神经网络，每个节点提供输入对应于相应类别的概率。这十个概率定义了MNIST分类模型中CPD的十个参数。分类CPD的模型称为多项式分布，它是伯努利分布扩展到多于两个类别的结果。在十个类别的MNIST分类任务中，多项式CPD可以表示如下：
- en: '![](../Images/equation_4-23.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/equation_4-23.png)'
- en: Depending on the weights w, the NN yields for each input image *x* ten output
    values that define the parameters of the corresponding multinomial CPD that assigns
    a probability to each possible outcome.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据权重w，神经网络为每个输入图像 *x* 产生十个输出值，这些值定义了分配给每个可能结果的相应多项式CPD的参数。
- en: Before training the NN, you initiate the weights with small and random values.
    The untrained NN assigns a probability close to 1/10 to each class similar to
    a uniform distribution (see figure 4.8), regardless of the image that is passed
    through the NN.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络之前，你用小的随机值初始化权重。未训练的神经网络将每个类别的概率分配得接近1/10，类似于均匀分布（见图4.8），无论通过神经网络的图像是什么。
- en: '![](../Images/4-8.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-8.png)'
- en: Figure 4.8 A uniform probability distribution that assigns a probability of
    0.1 to each of the ten class labels. An untrained classification NN for ten class
    labels results in a CPD similar to this uniform distribution, regardless of the
    label of the classified image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8展示了一个均匀概率分布，将概率0.1分配给十个类别标签。对于十个类别标签的未训练分类神经网络，其CPD将与这种均匀分布相似，无论分类图像的标签是什么。
- en: If you then train the NN with a couple of labeled images, the CPD already carries
    some information. A CPD that results after passing through an image with the label
    2 can, for example, look like what’s shown in figure 4.9.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你然后用几个带标签的图像来训练神经网络，CPD已经包含了一些信息。例如，通过标签2的图像得到的CPD可能看起来像图4.9中展示的那样。
- en: '![](../Images/4-9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-9.png)'
- en: Figure 4.9 The multinomial CPD that corresponds to an image of label 2, which
    was passed through a NN (Distribution with solid lines). Here the distribution
    of the ground truth (the dotted line) assigns a probability of 1 to the true label
    2.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了与标签2对应的多项式条件概率分布（CPD），该图像通过了一个神经网络（实线表示的分布）。在这里，真实标签的分布（虚线）将概率1分配给真实的标签2。
- en: 'Let’s look at figure 4.9\. What is the likelihood of this observed image with
    label 2? The likelihood is the probability that the CPD assigns to the label,
    here approximately 0.3\. Note that only the probability that the CPD assigns to
    the correct label contributes to the likelihood. If you have classified several
    images with your NN and you know the true label of each image, you can determine
    the joint NLL. To do so for each image, evaluate the probability that the CPD
    assigns to the correct class label with this equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图4.9。这个观察到的标签2图像的似然性是什么？似然性是CPD分配给标签的概率，这里大约是0.3。请注意，只有CPD分配给正确标签的概率才对似然性有贡献。如果你用你的神经网络分类了几个图像，并且知道每个图像的真实标签，你可以确定联合负对数似然（NLL）。为此，对每个图像，使用以下方程评估CPD分配给正确类别标签的概率：
- en: '![](../Images/equation_4-24.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/equation_4-24.png)'
- en: 'The mean NLL per observation is the NLL divided by the number of samples. This
    again results in the formula of the cross entropy:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每个观察的平均NLL是NLL除以样本数。这再次导致了交叉熵的公式：
- en: '![](../Images/equation_4-25.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/equation_4-25.png)'
- en: You can write this more compactly if you use the one-hot encoded vector *^(true)*
    *p**[i]* for the true class of example i. The vector *^(true)* *p**[i]* is 1 for
    the true class of the training example i and 0 for the remaining components (see
    figure 4.9).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用一个-hot编码向量 *^(true)* *p**[i]* 来表示示例i的真实类别，你可以将这个表达式写得更紧凑。向量 *^(true)* *p**[i]*
    对于训练示例i的真实类别是1，对于其他成分是0（见图4.9）。
- en: '![](../Images/equation_4-27.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/equation_4-27.png)'
- en: What value for the loss do you expect from an untrained NN that was set up for
    the MNIST task? Pause for a second and try to find the answer by yourself before
    you read on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你期望从为MNIST任务设置的未训练的NN中损失值是多少？暂停一下，在你继续阅读之前，试着自己找出答案。
- en: In the full MNIST data set, you have ten classes. An untrained network would
    assign ~1/10 to each class. This leads to *p**[i]* ≈ 1/10 for all classes, and
    we get −*log* (1/10) ≈ 2.3 for the loss. To debug your training, it’s good practice
    to always check this number for classification problems.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整的MNIST数据集中，你有十个类别。一个未训练的网络会将大约1/10分配给每个类别。这导致所有类别的 *p**[i]* ≈ 1/10，我们得到损失的值约为2.3。为了调试你的训练，始终检查这个数字对于分类问题来说是一个好的实践。
- en: '| ![](../Images/computer-icon.png) | Hands-on timeOpen [http://mng.bz/OMJK](http://mng.bz/OMJK)
    and work through the code until you reach the exercise indicated by a pen icon,
    then do the exercise. Your task in the exercise is to use the digit predictions
    on MNIST images made by an untrained CNN and compute the value of the loss by
    hand. You’ll see that you get a value close to the value 2.3 that we previously
    calculated. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间打开[http://mng.bz/OMJK](http://mng.bz/OMJK)并运行代码，直到你达到由笔形图标指示的练习，然后做这个练习。在练习中，你的任务是使用未训练的CNN在MNIST图像上做出的数字预测，并手动计算损失值。你会发现你得到一个接近我们之前计算出的2.3的值。|'
- en: 4.2.3 Relationship between NLL, cross entropy, and Kullback-Leibler divergence
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 NLL、交叉熵和Kullback-Leibler散度之间的关系
- en: You might have wondered why DL people call the NLL in classification problems
    cross entropy. Maybe you have also wondered if it is possible in classification
    problems to quantify the “badness” of a fit by a difference between the true and
    a predicted value, similar to the MSE in regression. In this section, you learn
    about the answers to both questions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么深度学习的人把分类问题中的NLL称为交叉熵。你可能也想知道在分类问题中，是否可以通过真实值和预测值之间的差异来量化拟合的“坏度”，类似于回归中的MSE。在本节中，你将了解这两个问题的答案。
- en: What entropy means in statistics and information theory
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学和信息理论中熵的含义
- en: We use the term entropy in different disciplines including information theory,
    statistical physics, and statistics. Here you’ll learn about the first instance,
    where it’s used to describe the information content of a probability distribution.
    You’ll see it’s useful to understand the origin of the term cross entropy for
    the loss in classification.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在信息理论、统计物理和统计学等不同学科中使用熵这个术语。在这里，你将了解第一个实例，其中它被用来描述概率分布的信息内容。你会发现，了解分类中损失函数的交叉熵术语的起源是有用的。
- en: Let’s start with the term entropy. The basic idea behind entropy is how much
    does the distribution tell you about the quantity at hand and how much uncertainty
    or surprise is left? In the following equation, you can measure the entropy H
    by the spread or “roughness” of a distribution. It’s defined as
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从熵这个术语开始。熵的基本思想是分布能告诉你多少关于手头数量的信息，以及还剩下多少不确定或惊喜？在下面的方程中，你可以通过分布的扩散或“粗糙度”来衡量熵H。它被定义为
- en: '![](../Images/equation_4-29s.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-29s](../Images/equation_4-29s.png)'
- en: Take a look at the two distributions in figure 4.9\. The CPD (black) is quite
    flat. You don’t learn too much about the possible class label. You can take this
    to the extreme and consider uniform distribution (see figure 4.8). Indeed, it
    can be shown that the uniform distribution has the maximum entropy. For 10 classes
    with *p**[i]* = 1/10 each, the entropy is equal to *H* = −10 ⋅ 1/10 ⋅ *log* (1/10)
    ≈ 2.3 . The ground-truth distribution (figure 4.9, dotted line) tells you as much
    about the labels as possible. The distribution has only one peak, and you can
    be 100% sure that the true class label is 2\. The corresponding entropy is H =
    0, because for the correct class, you have pi = 1 and with that *p**[i]* ⋅ *log*(*p**[i]*
    ) = 1 ⋅ 0 = 0 .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图4.9中的两个分布。CPD（黑色）相当平坦。你不会学到太多关于可能的类别标签。你可以将其推向极端，并考虑均匀分布（见图4.8）。实际上，可以证明均匀分布具有最大的熵。对于每个
    *p**[i]* = 1/10 的10个类别，熵等于 *H* = −10 ⋅ 1/10 ⋅ *log* (1/10) ≈ 2.3。真实分布（图4.9，虚线）告诉你尽可能多的关于标签的信息。分布只有一个峰值，你可以100%确定真实类别标签是2。相应的熵是
    H = 0，因为对于正确的类别，你有一个 pi = 1，并且因此 *p**[i]* ⋅ *log*(*p**[i]* ) = 1 ⋅ 0 = 0 。
- en: Cross entropy and Kullback-Leibler divergence
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵和Kullback-Leibler散度
- en: 'Cross entropy comes into play if you have two distributions q and p and you
    calculate the expected value of log(q) given the distribution p. For the discrete
    case, this is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有两个分布 q 和 p，并且根据分布 p 计算给定分布 q 的对数期望值，交叉熵就会发挥作用。对于离散情况，如下所示：
- en: '![](../Images/equation_4-33s.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-33s](../Images/equation_4-33s.png)'
- en: With cross entropy, you can compare the two distributions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉熵，你可以比较两个分布。
- en: Kullback-Leibler divergence as the MSE pendant in classification
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度作为分类中的 MSE 对应物
- en: Let’s again inspect figure 4.9, which shows the predicted CPD (black line) and
    the ground-truth distribution (Dotted line), which is the one-hot encoded ground-truth
    label. If the model is perfect, the predicted CPD matches the ground-truth distribution.
    Let’s try to quantify the “badness” of the model, which should be some measure
    of the distance between the current predicted CPD and the ground truth. A naive
    way is to determine at each class label the difference of its CPD and ground-truth
    probabilities, maybe square them, and then take the mean. This would mimic MSE
    in regression, which is the expected value of the squared differences between
    true and predicted values.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查图 4.9，它显示了预测的 CPD（黑色线条）和真实分布（虚线），这是经过 one-hot 编码的真实标签。如果模型是完美的，预测的 CPD
    将与真实分布相匹配。让我们尝试量化模型的“坏处”，这应该是当前预测 CPD 与真实分布之间距离的某种度量。一种简单的方法是在每个类别标签上确定其 CPD 与真实概率的差异，可能将其平方，然后取平均值。这将在回归中模仿
    MSE，即真实值与预测值之间平方差的期望值。
- en: 'But be aware that the MSE doesn’t give you the MaxLike estimate for classification.
    For classification, subtracting the true versus the predicted value doesn’t correspond
    to the MaxLike principle. Here, you need to compare two distributions: the true
    distribution and the predicted distribution. For this purpose, the Kullback-Leibler
    (KL) divergence is often used. The KL divergence is the expected value of the
    differences of the log probabilities. By using some basic log calculus rules and
    the definition of the expected value, you can show that the KL divergence is the
    same as the cross entropy:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，MSE 并不能给出分类的最大似然估计。对于分类，减去真实值与预测值并不对应于最大似然原理。在这里，你需要比较两个分布：真实分布和预测分布。为此，通常使用
    Kullback-Leibler (KL) 散度。KL 散度是概率对数差异的期望值。通过使用一些基本的对数微积分规则和期望值的定义，你可以证明 KL 散度与交叉熵相同：
- en: '![](../Images/equation_4-34s.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-34s](../Images/equation_4-34s.png)'
- en: 'As you can see in the preceding derivation, the KL divergence between the ground
    truth and the predicted distribution strips down to the sum of the entropy of
    the ground truth and the cross entropy. Because the first term (entropy of the
    ground truth) is zero (Dotted distribution in figure 4.9), you indeed minimize
    the KL divergence if you minimize the cross entropy. You’ll encounter the KL divergence
    again in this book. But at the moment, let’s appreciate that the KL divergence
    is to classification what the MSE is to regression. As with cross entropy and
    the KL divergence: the two probability distributions don’t have the same role,
    and you get a different result if you swap the two distributions. To indicate
    that *KL* (*^(true)* *p* ||*^(pred)* *p* ) ≠ *KL* (*^(pred)* *p* ||*^(true)* *p*
    ) , we write it with two bars. By the way, that wouldn’t be a good idea. Why?
    (Try to answer the question before reading on.) Well, *^(true)* *p**[i]* is mostly
    zero, and taking the log of zero is not such a good idea because it returns minus
    infinity.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的推导中所示，真实分布与预测分布之间的 KL 散度简化为真实分布的熵与交叉熵之和。因为第一个项（真实分布的熵）为零（图 4.9 中的虚线分布），所以如果你最小化交叉熵，你实际上是在最小化
    KL 散度。你将在本书中再次遇到 KL 散度。但在此刻，让我们欣赏 KL 散度对于分类来说就像 MSE 对于回归一样。与交叉熵和 KL 散度一样：两个概率分布有不同的角色，如果你交换这两个分布，你会得到不同的结果。为了表示
    *KL* (*^(true)* *p* ||*^(pred)* *p* ) ≠ *KL* (*^(pred)* *p* ||*^(true)* *p* )
    ，我们用两个竖线表示。顺便说一下，这不是一个好主意。为什么？（在继续阅读之前尝试回答这个问题。）好吧，*^(true)* *p**[i]* 主要为零，取零的对数不是一个好主意，因为它返回负无穷大。
- en: 4.3 Deriving a loss function for regression problems
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 推导回归问题的损失函数
- en: In this section, you use the MaxLike principle to derive the loss function for
    regression problems. You start off by revisiting the blood pressure example from
    chapter 3, where the input was the age of an American healthy woman and the output
    was a prediction of her systolic blood pressure (SBP). In chapter 3, you used
    a simple NN without hidden layers to model a linear relationship between input
    and output. As the loss function, you used the MSE. This choice of the loss function
    was explained by some hand-waving arguments. No hard facts were given to prove
    that this loss function is a good choice. In this section, you will see that the
    MSE as a loss function directly results from the MaxLike principle. Further, using
    the MaxLike principle, we can go beyond the MSE loss and model data with nonconstant
    noise, known under the scary name of heteroscedasticity in the statistics community.
    Don’t be afraid; understanding the MaxLike principle makes modeling (not spelling)
    heteroscedasticity a piece of cake.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你使用最大似然原理推导回归问题的损失函数。你首先回顾了第3章中的血压示例，其中输入是一个美国健康女性的年龄，输出是她收缩压（SBP）的预测。在第3章中，你使用了一个没有隐藏层的简单神经网络来建模输入和输出之间的线性关系。作为损失函数，你使用了均方误差（MSE）。这种损失函数的选择是通过一些手舞足蹈的论据来解释的。没有给出任何硬事实来证明这个损失函数是一个好的选择。在本节中，你将看到MSE作为损失函数直接源于最大似然原理。此外，使用最大似然原理，我们可以超越MSE损失，并使用非恒定噪声来建模数据，这在统计学界被称为异方差性。不要害怕；理解最大似然原理会使建模（而不是拼写）异方差性变得轻而易举。
- en: 4.3.1 Using a NN without hidden layers and one output neuron for modeling a
    linear relationship between input and output
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 使用没有隐藏层和一个输出神经元的神经网络来建模输入和输出之间的线性关系
- en: 'Let’s go back to the blood pressure example from chapter 3\. In that application,
    you used a simple linear regression model, *ŷ =a* ⋅ *x* − *b* , to estimate the
    SBP *y* when given the age *x* of the woman. Training or fitting this model requires
    you to tune the parameters a and *b* so that the resulting model “best fits” to
    the observed data. In figure 4.10, you can see the observed data together with
    a linear regression line that goes quite well through the data but might not be
    the best model. In chapter 3, you used the MSE as a loss function that quantifies
    how poorly the model fits the data. Recall equation 3.1:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第3章中的血压示例。在那个应用中，你使用了一个简单的线性回归模型 *ŷ =a* ⋅ *x* − *b* ，来估计当给定女性的年龄 *x* 时收缩压
    *y* 的值。训练或拟合这个模型需要你调整参数 a 和 *b*，使得得到的模型“最佳拟合”观察到的数据。在图4.10中，你可以看到观察到的数据以及一条相当好地穿过数据的线性回归线，但它可能不是最好的模型。在第3章中，你使用了MSE作为损失函数来量化模型拟合数据的好坏。回想一下方程式3.1：
- en: '![](../Images/equation_4-38.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-38](../Images/equation_4-38.png)'
- en: This loss function relies on the idea that deviations between model and data
    should be quantified by summing up the squared residuals. Given this loss function,
    you determined the optimal parameter values by SGD.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数依赖于这样一个观点：模型与数据之间的偏差应该通过求和平方残差来量化。给定这个损失函数，你通过随机梯度下降法（SGD）确定了最优的参数值。
- en: In chapter 3, we introduced the MSE loss with the hand-waving argument that
    a fit is optimal if the sum of the squared residuals is minimal. In the following,
    you see how you can use the MaxLike principle to derive the appropriate loss for
    a linear regression task in a theoretically sound way.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们通过一个论据介绍了MSE损失，即如果平方残差之和最小，则拟合是最优的。在下面的内容中，你将看到如何使用最大似然原理以理论上的方式推导线性回归任务的适当损失。
- en: Spoiler alert You will see that the MaxLike approach leads to the MSE loss.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 提前剧透：你会发现最大似然法会导致MSE损失。
- en: 'Let’s derive the loss function for the regression task by the MaxLike approach.
    For a simple regression model, we need only a simple NN (see figure 4.11) without
    a hidden layer. When using a linear activation function, this NN encodes the linear
    relationship between the input *x* and the output: out *= a · x + b* .'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过最大似然法（MaxLike）推导回归任务的损失函数。对于一个简单的回归模型，我们只需要一个简单的神经网络（见图4.11），没有隐藏层。当使用线性激活函数时，这个神经网络编码了输入
    *x* 和输出之间的线性关系：out *= a · x + b* 。
- en: '![](../Images/4-10.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![4-10](../Images/4-10.png)'
- en: Figure 4.10 Scatter plot and regression model for the blood pressure example.
    The dots are the measured data points, and the straight line is the linear model.
    The vertical differences between data points and model are the residuals.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10展示了血压示例的散点图和回归模型。点代表测量数据点，直线是线性模型。数据点和模型之间的垂直差异是残差。
- en: '![](../Images/4-11.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![4-11](../Images/4-11.png)'
- en: Figure 4.11 Simple linear regression as an fcNN without a hidden layer. This
    model computes the output directly from the input as out = *a* · *x* + *b*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 简单线性回归作为一个没有隐藏层的 fcNN。该模型直接从输入计算输出，即 out = *a* · *x* + *b*。
- en: The training data in regression problems comes in n pairs (*x**[i]* , *y**[i]*
    ). In the blood pressure example, *x**[i]* holds the age of the i-th woman and
    *y**[i]* is the true systolic blood pressure of the i-th woman as well. If you
    choose certain numbers for the weights of the NN, say *a* = 1 and *b* = 100, then
    for a given input you can compute, say *x* = 50 and the fitted value *ŷ* = 1 ⋅
    50 + 100 = 150 . You can understand this as the best guess of the model. In our
    data set, we have a woman age 50, but her blood pressure is 183, not 150\. This
    does not imply that our model is wrong or could be further improved because you
    do not expect that all women with the same age have the same blood pressure.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题中的训练数据以 n 对 (*x**[i]* , *y**[i]* ) 的形式出现。在血压的例子中，*x**[i]* 表示第 i 个女性的年龄，而
    *y**[i]* 是第 i 个女性的真实收缩压。如果你为神经网络（NN）的权重选择某些数字，比如 *a* = 1 和 *b* = 100，那么对于给定的输入，你可以计算出，比如
    *x* = 50 和拟合值 *ŷ* = 1 ⋅ 50 + 100 = 150。你可以把这理解为模型的最佳猜测。在我们的数据集中，我们有一个 50 岁的女性，但她的血压是
    183，而不是 150。这并不意味着我们的模型是错误的或者可以进一步改进，因为你不会期望所有相同年龄的女性都有相同的血压。
- en: As in classification, also in regression. The output of the NN is not the value
    *y* you expect to observe when the input has a specific value *x*. In classifications,
    the output of the NN is not a class label but the probabilities for all possible
    class labels, which are the parameters for the fitted probability distribution
    (see the figure in the first sidebar). In regression, the output of the NN is
    not the specific value *y* itself, but again the parameter(s) of the fitted continuous
    probability distribution. Here we use a Normal distribution; later in chapter
    5, we also use different ones like the Poissonian distribution. To recap the properties
    of a Normal distribution, see the next sidebar.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如分类一样，在回归中也是如此。神经网络的输出不是当输入具有特定值 *x* 时你期望观察到的值 *y*。在分类中，神经网络的输出不是一个类标签，而是所有可能类标签的概率，这些概率是拟合概率分布的参数（见第一个侧边栏中的图）。在回归中，神经网络的输出不是具体的值
    *y* 本身，而是拟合的连续概率分布的参数。在这里，我们使用正态分布；在第五章中，我们还会使用不同的分布，如泊松分布。为了回顾正态分布的性质，请参见下一个侧边栏。
- en: Recap on Normal distributions
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布的回顾
- en: 'Let’s recap how to deal with a continuous variable *y* that follows a Normal
    distribution. First, have a closer look at the density of a Normal distribution:
    *N*(*μ, σ*). The parameter *μ* determines the center of the distribution, and
    *σ* determines the spread of the distribution (see the following figure). You
    often see something like this'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何处理遵循正态分布的连续变量 *y*。首先，更仔细地看看正态分布的密度：*N*(*μ, σ*)。参数 *μ* 决定了分布的中心，而 *σ*
    决定了分布的扩散（见图以下）。你经常看到类似的东西
- en: '*Y* ~ *N*(*μ, σ*)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* ~ *N*(*μ, σ*)'
- en: 'which states the random variable *y*(blood pressure at a certain age, for example)
    follows a Normal distribution. Such a random variable *y* has the following probability
    density function:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明随机变量 *y*（例如，一定年龄的血压）遵循正态分布。这样的随机变量 *y* 有以下概率密度函数：
- en: '![](../Images/equation_4-42s.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../Images/equation_4-42s.png)'
- en: 'This is visualized in the next figure:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下图中得到了可视化：
- en: '![](../Images/4-unnumb-3.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-3.png)'
- en: The density of a Normal distribution where *μ* is the center and *σ* is the
    spread of the distribution
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布的密度，其中 *μ* 是中心，*σ* 是分布的扩散
- en: Looking at the figure gives one the foresight that *y* takes high probability
    values close to *μ* and small probability values further away from *μ* . This
    intuition is correct. But still, it is a bit harder to read the probability distribution
    of a continuous variable *y*(see the previous figure) than the probability distribution
    of a discrete variable (see figure 4.2).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个图可以让人预见到 *y* 在 *μ* 附近有高概率值，而在 *μ* 附近有低概率值。这种直觉是正确的。但是，与离散变量的概率分布（见图 4.2）相比，读取连续变量
    *y* 的概率分布（见前一个图）还是有点困难。
- en: In the case of a discrete variable, the probability distribution consists of
    separated bars, corresponding to the discrete outcome values. For discrete probability
    distributions, the height of the bars directly corresponds to the probabilities,
    and these probabilities add up to one. A continuous variable can take infinitely
    many possible values, and the probability for exactly one value like *π* ~ = 3.14159265359
    is zero. A probability can, therefore, only be defined for a region of values.
    The probability to observe a value *y* in the range between *a* and *b*, *y* ∈
    *a,b*], is given by the area under the density curve between a and *b* (see the
    shaded region in the following figure). The range of all possible values has a
    probability of 1; therefore, the area under a probability density curve is always
    1.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散变量的情况下，概率分布由分离的柱状图组成，对应于离散的输出值。对于离散概率分布，柱状图的高度直接对应于概率，这些概率加起来为1。连续变量可以取无限多个可能值，对于像
    *π* ~ = 3.14159265359 这样的确切值，概率为零。因此，概率只能定义在值的一个区域内。观察值 *y* 在 *a* 和 *b* 之间的概率，*y*
    ∈ *a,b*]，由密度曲线在 *a* 和 *b* 之间的面积给出（见图中阴影区域）。所有可能值的范围有一个概率为1；因此，概率密度曲线下的面积总是1。
- en: '![](../Images/4-unnumb-4.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-4.png)'
- en: The density of a Normal distributed variable *y*, where the shaded area under
    the density curve gives the probability that *y* takes values between *a* and
    *b*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布变量 *y* 的密度，其中密度曲线下的阴影区域给出了 *y* 在 *a* 和 *b* 之间取值的概率
- en: You can use the MaxLike principle to tune the two weights w = (*a, b*) of the
    NN used to perform linear regression (see figure 4.11). But what is the likelihood
    of the observed data? For regression, it is just a little bit harder to answer
    this question than for classification. Remember, in classification, you can determine
    the probability or likelihood for each observed value from the probability distribution
    with the parameters pi. The NN controls these parameters (see figure 4.11). In
    regression, the observed value *y**[i]* is continuous. Here you need to work with
    a Normal distribution. (Other distributions besides Normal are also sometimes
    adequate and we deal with those later in chapter 5, but for now, we stick to the
    Normal distribution.)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用MaxLike原理来调整用于执行线性回归的NN的两个权重w = (*a, b*)（见图4.11）。但观察到的数据的似然性是什么？对于回归，回答这个问题比分类稍微困难一些。记住，在分类中，您可以从具有参数pi的概率分布中确定每个观察值的概率或似然性。NN控制这些参数（见图4.11）。在回归中，观察值
    *y**[i]* 是连续的。在这里，您需要与正态分布一起工作。（除了正态分布之外，有时其他分布也足够好，我们将在第5章中处理那些，但现在我们坚持使用正态分布。）
- en: 'Normal distribution has two parameters: *μ* and *σ* . To start with, we leave
    the parameter *σ* fixed (let’s say, we set it to *σ* = 20) and let the NN control
    only the parameter *μ* *x* . Here, the subscript *x* reminds us that this parameter
    depends on *x*. It is determined by the network and, thus, *μ* *x* depends on
    the parameters (weights) of the network. The simple network in figure 4.11 produces
    a linear dependency of *μ**[x]* = *ax + b* on *x*. Figure 4.12 shows this with
    a bold line. Older women have, on average, a higher blood pressure. The weights
    (*a, b*) themselves are determined (fitte*D*) to maximize the likelihood of the
    data. What is the likelihood of the data? We start with a single data point (*x**[i]*
    , *y**[i]* ). Inspect, for example, the data point of the 22-year-old woman who
    has an SBP of 131\. For that age, the network predicts an average value of *μ**[x]*
    = 111 ; the spread *σ* is fixed.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布有两个参数：*μ* 和 *σ* 。首先，我们固定参数 *σ*（比如说，我们将其设置为 *σ* = 20）并让NN只控制参数 *μ* *x* 。在这里，下标
    *x* 提醒我们这个参数依赖于 *x*。它由网络确定，因此 *μ* *x* 依赖于网络的参数（权重）。图4.11中的简单网络产生 *μ**[x]* = *ax
    + b* 对 *x* 的线性依赖。图4.12用粗线展示了这一点。老年女性的平均血压较高。权重 (*a, b*) 本身是通过（拟合）最大化数据的似然性来确定的。数据的似然性是什么？我们从单个数据点
    (*x**[i]* , *y**[i]* ) 开始。例如，检查22岁女性收缩压为131的数据点。对于那个年龄，网络预测的平均值为 *μ**[x]* = 111；*σ*
    的范围是固定的。
- en: '![](../Images/4-12.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-12.png)'
- en: Figure 4.12 Scatter plot and regression model for the blood pressure example.
    The dots are the measured data points and the straight line is the linear model.
    The bell-shaped curves are the conditional probability distributions of the outcome
    *y* conditioned on the observed value *x*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 血压示例的散点图和回归模型。点代表测量的数据点，直线是线性模型。钟形曲线是条件概率分布，条件是观察到的值 *x*。
- en: In other words, a woman of age 22 has (according to the model) most likely a
    blood pressure of 111\. But other values are also possible. The probability density
    *f* (*y, μ*) = 111, *σ* = 20) for different values of the blood pressure *y* is
    distributed around the value 111 by a Normal (shown in shaded grey area in figure
    4.12 and again in figure 4.13).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一个22岁的女性的血压（根据模型）最有可能是111。但其他值也是可能的。血压值 *y* 的概率密度 *f* (*y, μ*) = 111, *σ*
    = 20) 在不同的血压值 *y* 上分布，围绕111这个值，通过正态分布（如图4.12中阴影灰色区域所示，再次在图4.13中显示）。
- en: '![](../Images/4-13.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图4-13](../Images/4-13.png)'
- en: Figure 4.13 The conditional Normal density function f. The height of the vertical
    bar indicates the likelihood of the specific value under this model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 条件正态密度函数 f。垂直条的高度表示在这个模型下特定值的可能性。
- en: 'The woman in our observation has a blood pressure of 131\. As in the discrete
    case, where we reinterpreted the probability *p* (*y* | *x, a, b*) = *p* (*y*
    | *x, w*) as a likelihood for the data to occur given the parameters *w*, we now
    interpret the probability density *f* ( *y* | *x, μ, σ* ) = *f* (*y* | *x, w*)
    as the likelihood for the observed data to occur. Because it is derived from a
    probability density, the likelihood in the continuous case is a continuous function
    as well. In our concrete case, the likelihood of this observation is given by
    the density as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的观察中，这位女性的血压为131。与离散情况一样，我们将概率 *p* (*y* | *x, a, b*) = *p* (*y* | *x, w*)
    重新解释为给定参数 *w* 的数据发生的可能性，我们现在将概率密度 *f* ( *y* | *x, μ, σ* ) = *f* (*y* | *x, w*)
    解释为观察到的数据发生的可能性。因为它来自概率密度，所以在连续情况下，可能性也是一个连续函数。在我们的具体情况下，这个观察到的可能性由以下密度给出：
- en: '![](../Images/equation_4-52.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-52](../Images/equation_4-52.png)'
- en: 'See also the vertical bar in figure 4.13\. For each value of the input *x*,
    the output *y* follows another Normal distribution. For example, for a woman aged
    47 with an SBP of 110, the parameter is *μ**[x]* = 139 . The corresponding likelihood
    of that given blood pressure is determined by *f* ( *y* = 110; *μ* = 139, *σ*
    = 20) . Because the normal probability distribution depends via *μ**[x[i]]* =
    *a* ⋅ *x**[i]* + *b* on the value *x**[i]* , it is often called a conditional
    probability distribution (CPD). As before in the classification case, the likelihood
    of all points (we assume independence) is given by the product of the individual
    likelihoods as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 参见图4.13中的垂直条。对于每个输入值 *x*，输出 *y* 遵循另一个正态分布。例如，对于一个47岁的女性，收缩压为110，参数是 *μ**[x]*
    = 139。对应于这个血压的可能性的确定由 *f* ( *y* = 110; *μ* = 139, *σ* = 20) 决定。因为正态概率分布通过 *μ**[x[i]]*
    = *a* ⋅ *x**[i]* + *b* 依赖于值 *x**[i]*，它通常被称为条件概率分布（CPD）。与分类情况一样，所有点（我们假设独立性）的可能性由单个可能性的乘积给出：
- en: '![](../Images/equation_4-56.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-56](../Images/equation_4-56.png)'
- en: 'This likelihood depends only on the parameters *a* and *b*. The values â and
    bˆ maximizing it are our best guess and are known under the name MaxLike estimates.
    (That’s also the reason they get their hat.) Practically, we again take the log
    and minimize the NLL:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可能性只取决于参数 *a* 和 *b*。最大化它的值 *â* 和 *bˆ* 是我们最好的猜测，并且被称为MaxLike估计。这也是它们为什么被称为“帽子”的原因。实际上，我们再次取对数并最小化NLL：
- en: '![](../Images/equation_4-57.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-57](../Images/equation_4-57.png)'
- en: You saw in chapter 3 that you can find the network weights that minimize the
    loss function by SGD. Let’s do this now. We have to define a novel loss function
    in Keras. You can implement a custom loss function in Keras by defining a function
    that takes as input the true values and the prediction of the network. As shown
    in figure 4.11, linear regression is defined as a simple network, which predicts
    *a* ⋅ *x**[i]* + *b* with one weight a that gives the slope and the bias *b* that
    gives the intercept. The loss from the loss function in equation 4.4 is then coded
    as shown in listing 4.4\. (you can find this code in the notebook.)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第3章中看到，你可以通过SGD找到最小化损失函数的网络权重。现在让我们这样做。我们必须在Keras中定义一个新的损失函数。你可以在Keras中通过定义一个函数来实现自定义损失函数，该函数接受真实值和网络预测作为输入。如图4.11所示，线性回归被定义为一个简单的网络，它预测*a*
    ⋅ *x**[i]* + *b*，其中权重*a*给出斜率，偏置*b*给出截距。方程式4.4中的损失函数的损失编码如下所示（你可以在笔记本中找到此代码）。
- en: '| ![](../Images/computer-icon.png) | Hands-on timeOpen [http://mng.bz/YrJo](http://mng.bz/YrJo)
    and work through the code to see how to use the MaxLike approach to determine
    the parameter values in a linear regression model. For this, the NLL is defined
    as a loss function that is minimized via SGD. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间打开[http://mng.bz/YrJo](http://mng.bz/YrJo)并运行代码，以了解如何使用MaxLike方法确定线性回归模型中的参数值。为此，NLL被定义为通过SGD最小化的损失函数。
    |'
- en: Listing 4.4 Estimating the MaxLike solution
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 估计MaxLike解
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Defines a custom loss function
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个自定义损失函数
- en: ❷ Calculates the sum of all losses (see equation 4.4)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算所有损失的加和（见方程式 4.4）
- en: ❸ Sets up an NN equivalent to linear regression; includes one linear activation
    and a bias term
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置一个与线性回归等价的神经网络；包含一个线性激活和一个偏置项
- en: In chapter 3, you already minimized the MSE loss function via SGD, and you received
    *a* = 1.1 and *b* = 87.8 as optimal parameter values. And, indeed, the MaxLike
    estimate shown here is identical with the MSE approach of chapter 3\. For the
    detailed derivation, see the following sidebar.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，你已经通过SGD最小化了MSE损失函数，并得到了*a* = 1.1和*b* = 87.8作为最优参数值。实际上，这里显示的MaxLike估计与第3章的MSE方法相同。对于详细的推导，请参阅以下侧边栏。
- en: MaxLike-based derivation of the MSE loss in linear regression
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基于MaxLike的线性回归均方误差损失函数推导
- en: 'Let’s follow step-by-step the MaxLike approach to derive the loss for a classical
    linear regression task. The MaxLike approach tells you that you need to find those
    values for the weights w in the NN. Here w = (*a, b*) (see the figure in the sidebar
    entitled “MaxLike approach for the classification loss using a parametric probability
    model”), which maximize the likelihood for the observed data. The data are provided
    as n pairs (xi, *y**[i]* ). The following product shows the likelihood of the
    data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地跟随MaxLike方法来推导经典线性回归任务的损失。MaxLike方法告诉你，你需要找到神经网络中权重w的这些值。这里w = (*a,
    b*)（参见侧边栏中标题为“使用参数概率模型的MaxLike方法对分类损失进行分类”的图），这些值最大化了观察数据的似然。数据以n对（xi, *y**[i]*
    ）的形式提供。以下乘积显示了数据的似然：
- en: '![](../Images/equation_4-59s.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-59s](../Images/equation_4-59s.png)'
- en: Maximizing this product leads to the same results as minimizing the sum of the
    corresponding NLL(see section 4.1).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化这个乘积会导致与最小化相应的NLL（见第4.1节）相同的结果。
- en: '![](../Images/equation_4-60s.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-60s](../Images/equation_4-60s.png)'
- en: Now you plug in the expression for the Normal density function (see the second
    equation in the sidebar, “Recap on Normal distributions”).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将正态密度函数的表达式（参见侧边栏中的第二个方程，“正态分布回顾”）代入。
- en: '![](../Images/equation_4-61s.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-61s](../Images/equation_4-61s.png)'
- en: 'Then let’s use the rule that log( *c* ⋅ *d* ) *=log* ( *c* ) *+* log( *d* )
    and log( *e**^g* ) *= g* , and the fact that ( *c* − *d* )² =( *d* − *c* )² :'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们使用规则log( *c* ⋅ *d* ) *=log* ( *c* ) *+* log( *d* ) 和 log( *e**^g* ) *=
    g* ，以及 ( *c* − *d* )² =( *d* − *c* )² 的事实：
- en: '![](../Images/equation_4-65s.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-65s](../Images/equation_4-65s.png)'
- en: 'Adding a constant does not change the position of the minimum and, because
    the first term is constant with respect to ![](../Images/equation_4-66s-span.png)*a*
    and *b*, we can drop it:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个常数不会改变最小值的位置，并且因为第一个项相对于 ![方程式 4-66s-span](../Images/equation_4-66s-span.png)*a*
    和 *b* 是常数，我们可以省略它：
- en: '![](../Images/equation_4-67s.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-67s](../Images/equation_4-67s.png)'
- en: '![](../Images/equation_4-68s.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-68s](../Images/equation_4-68s.png)'
- en: 'Multiplying with a constant factor also does not change the position of the
    minimum. We can freely multiply with the constant factor 2 ⋅ *σ*²/*n* so that
    we finally obtain the formula for the MSE loss:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以一个常数因子也不会改变最小值的位置。我们可以自由地乘以常数因子 2 ⋅ *σ*²/*n*，这样我们最终得到均方误差损失公式：
- en: '![](../Images/equation_4-70s.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-70s](../Images/equation_4-70s.png)'
- en: 'With this, we have derived the loss, which we need to minimize to find the
    optimal values of the weights. Note that you only need to assume that *σ*[2] is
    constant; you do not need to derive the value of *σ*[2] to derive the loss function
    of a classical linear regression model:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经推导出了损失函数，我们需要最小化它以找到权重的最优值。请注意，你只需要假设 *σ*[2] 是常数；你不需要推导出 *σ*[2] 的值来推导经典线性回归模型的损失函数：
- en: '![](../Images/equation_4-71s.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-71s](../Images/equation_4-71s.png)'
- en: 'Ta-da! We ended the task to find the parameter values for *a* and *b*, which
    minimize the sum of the squared residuals. This can be done by minimizing the
    MSE. The MaxLike approach did indeed lead us to a loss function that is nothing
    else other than the MSE! In the case of simple linear regression, the fitted value
    is *ŷ**[i]* = *μ**[x[i]]* = *a* ⋅ *x**[i]* + *b* yielding:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们完成了寻找 *a* 和 *b* 参数值以最小化平方残差之和的任务。这可以通过最小化均方误差（MSE）来实现。最大似然（MaxLike）方法确实引导我们得到了一个损失函数，这个损失函数实际上就是均方误差（MSE）！在简单线性回归的情况下，拟合值是
    *ŷ**[i]* = *μ**[x[i]]* = *a* ⋅ *x**[i]* + *b*，从而得到：
- en: '![](../Images/equation_4-73s.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-73s](../Images/equation_4-73s.png)'
- en: 'Let’s take a step back and reflect on what we have done so far. First, we used
    an NN to determine the parameters of a probability distribution. Second, we chose
    a Normal distribution to model our data. A Normal probability distribution has
    two parameters: *μ* and σ . We keep *σ* fixed and only model *μ*[*x[i]*] using
    the simplest model possible, the linear regression: *μ**[x[i]]* = *a* ⋅ *x**[i]*
    + *b* . The corresponding *y* value (SBP) to an *x* value (age) is distributed
    like a Normal:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止我们所做的工作。首先，我们使用神经网络来确定概率分布的参数。其次，我们选择正态分布来模拟我们的数据。正态概率分布有两个参数：*μ*
    和 *σ*。我们保持 *σ* 不变，并仅使用最简单的模型——线性回归来模拟 *μ*[*x[i]*]，即：*μ**[x[i]]* = *a* ⋅ *x**[i]*
    + *b*。对应于 *x* 值（年龄）的 *y* 值（SBP）分布类似于正态分布：
- en: '*Y**[x[i]]* ~ *N μ**[x[i]]* =*a* ⋅ *x**[i]* + *b* , *σ*²'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y**[x[i]]* ~ *N μ**[x[i]]* =*a* ⋅ *x**[i]* + *b* , *σ*²'
- en: 'This reads *y* is a random variable coming from a Normal distribution with
    a mean *μ*[*x[i]*] and a standard deviation *σ*. We can extend this approach in
    several ways:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *y* 是一个来自具有均值 *μ*[*x[i]*] 和标准差 *σ* 的正态分布的随机变量。我们可以以几种方式扩展这种方法：
- en: We can choose a different probability distribution beside a Normal. As it turns
    out, there are certain situations where a Normal distribution is not adequate.
    Take count data for example. A Normal distribution always includes negative values.
    But some data-like count data does not have negative values. We deal with those
    cases in chapter 5.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以选择除了正态分布以外的其他概率分布。实际上，在某些情况下，正态分布是不够的。以计数数据为例。正态分布总是包含负值。但某些数据，如计数数据，没有负值。我们在第5章中处理这些情况。
- en: We can use a full-blown NN instead of linear regression to model *μ*[*x[i]*]
    , which we do in the next section.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用一个完整的神经网络（NN）来代替线性回归来建模 *μ*[*x[i]*]，这在下一节中会进行说明。
- en: We do not need to stick to the assumption that the variability of the data is
    constant over the whole input range, but can also model *σ* by the NN and allow,
    for example, the uncertainty to increase. We do this in section 4.3.3.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不需要坚持数据在整个输入范围内的可变性是恒定的假设，也可以通过神经网络来模拟 *σ*，并允许，例如，不确定性增加。我们在4.3.3节中这样做。
- en: 4.3.2 Using a NN with hidden layers to model non-linear relationships between
    input and output
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 使用具有隐藏层的神经网络来模拟输入和输出之间的非线性关系
- en: 'An NN without hidden layers (see figure 4.11) models a linear relationship
    between input and output: out *= a* ⋅ *x + b* . Now you can extend this model
    and use an NN with one or more hidden layers to model *μ**[x]*. Let’s still assume
    that the variance *σ*² is constant. With the NN in figure 4.15, you model for
    each input *x* a whole CPD for the output given by:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 没有隐藏层的神经网络（见图4.11）模拟输入和输出之间的线性关系：out *= a* ⋅ *x + b*。现在你可以扩展这个模型，并使用一个或多个隐藏层的神经网络来模拟
    *μ**[x]*。我们仍然假设方差 *σ*² 是常数。使用图4.15中的神经网络，对于每个输入 *x*，你模拟了由以下给出的输出整个条件概率分布（CPD）：
- en: '*Y**[x[i]]* ∼ *N*(*μ**[x[i]]* , *σ*²)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y**[x[i]]* ∼ *N*(*μ**[x[i]]* , *σ*²)'
- en: If you add at least one hidden layer to your NN, you see that the mean *μ*[*x[i]*]
    of these CPDs do not need to be along a straight line (see figure 4.15). In listing
    4.5, you see how you can simulate some data from a function with a sinusoidal
    shape and fit an NN with three hidden layers and an MSE loss to the data, resulting
    in a well-fitting, non-linear curve (see figure 4.14).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在您的 NN 中添加至少一个隐藏层，您会看到这些 CPD 的均值 *μ*[*x[i]*] 不需要沿直线（参见图 4.15）。在列表 4.5 中，您可以看到如何从一个具有正弦形状的函数中模拟一些数据，并将具有三个隐藏层和
    MSE 损失函数的 NN 拟合到数据，从而得到一个拟合良好的非线性曲线（参见图 4.14）。
- en: Listing 4.5 Using MSE loss to model non-linear relationships in an fcNN
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 使用 MSE 损失函数来模拟 fcNN 中的非线性关系
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Creates some random data (see figure 4.14)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一些随机数据（参见图 4.14）
- en: ❷ Defines the fcNN with 3 hidden layers and ReLU activations
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义具有 3 个隐藏层和 ReLU 激活的 fcNN
- en: ❸ Fits the NN using the MSE loss
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 MSE 损失函数拟合 NN
- en: With this extension, you can model arbitrary, complicated non-linear relationships
    between input and output such as, for example, a sinus (see figure 4.14).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种扩展，您可以模拟输入和输出之间的任意复杂非线性关系，例如，例如正弦波（参见图 4.14）。
- en: '![](../Images/4-14.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-14.png)'
- en: Figure 4.14 A sinus-shaped curve (solid line) fitted to the data points (see
    dots) by using an fcNN with three hidden layers and an MSE loss
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 使用具有三个隐藏层和 MSE 损失函数的 fcNN 将正弦曲线（实线）拟合到数据点（见点）
- en: How does this work? The model in figure 4.11 was only able to draw a straight-line
    model. Why is the slightly extended NN (see figure 4.15) able to model such a
    complex curve? In chapter 2, we discussed that hidden layers allow us to construct
    in a non-linear manner new features from the input feature. For example, an NN
    with one hidden layer holding eight neurons (see figure 4.15) allows the NN to
    construct eight new features from the input *x*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？图 4.11 中的模型只能绘制直线模型。为什么稍微扩展的 NN（参见图 4.15）能够模拟如此复杂的曲线？在第 2 章中，我们讨论了隐藏层如何使我们能够以非线性方式从输入特征构造新的特征。例如，一个具有一个隐藏层和八个神经元的
    NN（参见图 4.15）允许 NN 从输入 *x* 构造八个新的特征。
- en: '![](../Images/4-15.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-15.png)'
- en: Figure 4.15 Extended linear regression. The eight neurons in the hidden layer
    give the features from which the output “out” is computed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 扩展线性回归。隐藏层中的八个神经元给出了计算输出“out”的特征。
- en: 'Then the NN models a linear relationship between these new features and the
    outcome. The derivation of the loss function stays the same and leads to the MSE
    loss formula:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，NN 模拟这些新特征与结果之间的线性关系。损失函数的推导保持不变，并导致 MSE 损失公式：
- en: '![](../Images/equation_4-77.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_4-77.png)'
- en: In the case of an NN with a hidden layer (see, for example, figure 4.12), the
    modeled output *ŷ**[i]* = *f**[NN]*[4.11](*x**[i]* ,*w*) is a quite complicated
    function of the input *x**[i]* and all weights in the NN. This is the only difference
    to a simple linear model that is encoded by an NN without a hidden layer (see
    figure 4.11), where the fitted value is a simple linear function of the weights
    and the input *ŷ**[i]* = *f**[NN]*[4.6](*x**[i]* , *a, b*) = *a* ⋅ *x**[i]* +
    *b* .
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有隐藏层的神经网络的情况下（例如，参见图 4.12），模拟的输出 *ŷ**[i]* = *f**[NN]*[4.11](*x**[i]* ,*w*)
    是输入 *x**[i]* 和神经网络中所有权重的一个相当复杂的函数。这与由没有隐藏层的神经网络编码的简单线性模型（参见图 4.11）的唯一区别是，其中拟合值是权重和输入的简单线性函数
    *ŷ**[i]* = *f**[NN]*[4.6](*x**[i]* , *a, b*) = *a* ⋅ *x**[i]* + *b* 。
- en: 4.3.3 Using an NN with additional output for regression tasks with nonconstant
    variance
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 使用具有额外输出的神经网络进行具有非恒定方差的回归任务
- en: One assumption in classical linear regression is homoscedasticity, meaning that
    the variance of the outcome does not depend on the input value *x*. Therefore,
    you need only one output node to compute the first parameter *μ* *x* of the conditional
    Normal distribution (see figures 4.11 and 4.15). If you also allow the second
    parameter, *σ*x , to depend on *x*, then you need an NN with a second output node.
    If the variance of the output’s CPD is not constant but dependent on *x*, we talk
    about heteroscedasticity.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 经典线性回归中的一个假设是同方差性，意味着结果的方差不依赖于输入值 *x*。因此，您只需要一个输出节点来计算条件正态分布的第一个参数 *μ* *x*（参见图
    4.11 和 4.15）。如果您还允许第二个参数，*σ*x ，依赖于 *x*，那么您需要一个具有第二个输出节点的神经网络。如果输出 CPD 的方差不是常数而是依赖于
    *x*，我们称之为异方差性。
- en: Technically, you can easily realize this by adding a second output node (see
    figure 4.16). Because the NN in figure 4.15 also has a hidden layer, it allows
    for a non-linear relationship between the input and the output. The two nodes
    in the output layer provide the parameter values *μ* *x* and *σ*x of the CPD *N*(*μ**[x]*
    , *σ**[x]*²). When working with a linear activation function in the output layer,
    you can get negative and positive output values. Thus, the second output is not
    directly taken as standard deviation *σ*x , but as log(*σ**[x]*) . The standard
    deviation is then computed from out2 via *σ**[x]* = *e*^(out[2]) , ensuring that
    *σ*x is a non-negative number.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你可以通过添加第二个输出节点（见图 4.16）轻松实现这一点。因为图 4.15 中的神经网络也有一个隐藏层，它允许输入和输出之间存在非线性关系。输出层中的两个节点提供了
    CPD *N*(*μ**[x]* , *σ**[x]*²) 的参数值 *μ* *x* 和 *σ*x。当在输出层中使用线性激活函数时，你可以得到负的和正的输出值。因此，第二个输出不是直接作为标准差
    *σ*x，而是作为 log(*σ**[x]*)。然后从 out2 计算标准差，即 *σ**[x]* = *e*^(out[2])，确保 *σ*x 是一个非负数。
- en: '![](../Images/4-16.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16](../Images/4-16.png)'
- en: Figure 4.16 You can use an NN with two output nodes to control the parameters
    *μ* *x* and *σ* *x* of the conditional outcome distribution *N*( *μ* *x*, *σ*
    *x* ) for regression tasks with nonconstant variance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 你可以使用具有两个输出节点的神经网络来控制条件结果分布 *N*( *μ* *x*, *σ* *x*) 的参数 *μ* *x* 和 *σ*
    *x*，以进行具有非常数方差的回归任务。
- en: Because the classical linear regression assumes that the variance *σ*[2] is
    constant (called the homoscedasticity assumption), you might suspect that it makes
    things much more complicated if you want to allow for varying *σ*[2](called heteroscedasticity).
    But luckily, this is not the case. The homoscedasticity assumption is only used
    in the derivation of the loss function to get rid of the terms containing *σ*[2]
    leading to the MSE loss. But if you do not assume constant variance, you can’t
    do this step. The loss is still given by the NLL defined in equation 4.4
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因为经典的线性回归假设方差 *σ*[2] 是常数（称为同方差性假设），你可能会怀疑，如果你想要允许 *σ*[2]（称为异方差性）变化，这会使事情变得更加复杂。但幸运的是，情况并非如此。同方差性假设仅在损失函数的推导中使用，以消除包含
    *σ*[2] 的项，导致 MSE 损失。但如果你不假设方差是常数，你就不能进行这一步。损失仍然由方程 4.4 中定义的 NLL 给出
- en: '![](../Images/equation_4-83.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-83](../Images/equation_4-83.png)'
- en: with the loss
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下损失
- en: '![](../Images/equation_4-84.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-84](../Images/equation_4-84.png)'
- en: If you want to analytically solve this loss as it is done in traditional statistics,
    the fact that *σ* is nonconstant causes problems. But if you renounce from a closed-form
    solution, optimizing this loss is not a problem at all. You can again use the
    SGD machinery to tune the weights for a minimal loss. In TensorFlow or Keras,
    you can realize this by defining a custom loss and then using this custom loss
    for the fitting procedure (see listing 4.6), which uses the loss function from
    equation 4.8.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要像传统统计学那样解析地解决这个问题，*σ* 非常数的事实会导致问题。但如果你放弃闭式解，优化这个损失根本不是问题。你又可以再次使用 SGD 机制来调整权重以实现最小损失。在
    TensorFlow 或 Keras 中，你可以通过定义一个自定义损失，然后使用这个自定义损失进行拟合过程（参见列表 4.6），该过程使用方程 4.8 中的损失函数。
- en: Why you do not need to know the ground truth for the variance
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你不需要知道方差的真实值
- en: 'Presenting the MaxLike principle to students, we are frequently asked the question,
    “How can you determine the variance if you don’t have a ground truth for it?”
    Let’s have a look again at the architecture in figure 4.16\. The network has two
    outputs: one directly corresponds to the expected value *μ*[*x[i]*] of the outcome
    distribution and the other corresponds to a transformation of the standard deviation
    *σ* *x**[i]* of the outcome distribution. While it somehow feels natural to choose
    the *μ*[*x[i]*] that is closer to the values yi, you might wonder how you can
    fit, for example, the standard deviation of the outcome distribution without having
    this information given as ground truth. You only have for each input *x**[i]*
    the observed outcome yi. But the MaxLike approach is beautiful and is doing the
    trick for you!'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在向学生介绍 MaxLike 原则时，我们经常被问到这样的问题：“如果没有地面真实值，你如何确定方差？”让我们再次看看图 4.16 中的架构。该网络有两个输出：一个直接对应于结果分布的期望值
    *μ*[*x[i]*]，另一个对应于结果分布的标准差 *σ* *x**[i]* 的变换。虽然选择更接近 yi 值的 *μ*[*x[i]*] 感觉上似乎是自然的，但你可能会想知道在没有给出作为地面真实值的信息的情况下，你如何拟合，例如，结果分布的标准差。你只有每个输入
    *x**[i]* 的观察结果 yi。但 MaxLike 方法非常美妙，并且正在为你完成这项工作！
- en: 'Recall the blood pressure data set (see figure 4.12), where it was assumed
    that the data spread is the same over the whole range of ages. Let’s forget the
    regression task for a moment and turn to an easier task: you only want to model
    the blood pressure for 45-year-old women by a Normal distribution. Let’s imagine
    that all four women have a blood pressure of roughly 131 (say, 130.5, 130.7, 131,
    131.8). As a distribution for blood pressure at age 45, you would probably want
    to use a Gaussian bell curve that has the parameter values *μ*[*x[i]*] at the
    observed (mean) value 131 and *σ* *x**[i]* close to zero. This should yield a
    maximum likelihood for observations at 131 (see the left panel in the following
    figure). It feels kind of natural that *μ*[*x[i]*] is determined by the ground
    truth yi. But how would you handle a situation where your data shows high variability?'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下血压数据集（见图4.12），其中假设数据在整个年龄范围内的分布是相同的。让我们暂时忘记回归任务，转向一个更简单的任务：你只想通过正态分布来建模45岁女性的血压。让我们想象这四位女性血压大约都是131（比如，130.5、130.7、131、131.8）。作为45岁年龄段的血压分布，你可能想使用一个具有参数值
    *μ*[*x[i]*] 在观测（均值）值131的正态钟形曲线，且 *σ* *x**[i]* 接近零。这应该会在131的观测值上产生最大可能性（参见下图中左侧面板）。感觉上很自然，*μ*[*x[i]*]
    是由真实值 yi 决定的。但是，你将如何处理数据表现出高变异性的情况？
- en: Imagine for example that the four 45-year-old women in your data set have blood
    pressures of 82, 114, 117, and 131\. In such situations, you would probably not
    like to use the Gaussian bell curve shown in the left panel of the figure because
    only the observation with the blood pressure 131 has a high likelihood; the other
    three observations have tiny likelihoods leading to a small overall (joint) likelihood.
    To maximize the joint likelihood, it is much better to use a Gaussian curve where
    all four observations have reasonably high likelihoods (see the right panel of
    the following figure).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你数据集中的四位45岁女性的血压分别为82、114、117和131。在这种情况下，你可能不会想使用图左侧面板所示的高斯钟形曲线，因为只有血压为131的观测值有较高的可能性；其他三个观测值有极小的可能性，导致整体（联合）可能性很小。为了最大化联合可能性，使用所有四个观测值都有合理高可能性的高斯曲线会更好（参见下图中右侧面板）。
- en: '![](../Images/4-unnumb-5.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb-5.png)'
- en: The Normal distribution that maximizes the joint likelihood of four observed
    SBP values in the case of almost no data variability (left), where all four observations
    have values close to 131, or a large data variability (right), where the four
    observed SBP values of 82, 114, 117, and 131 are quite different. The height of
    the line indicates the likelihood of the observed SBP values.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎没有任何数据变异性的情况下，最大化四个观测值SBP的联合可能性（左侧），其中所有四个观测值都接近131，或者在大数据变异性的情况下（右侧），四个观测值SBP为82、114、117和131，差异很大。线的长度表示观测SBP值的可能性。
- en: '| ![](../Images/computer-icon.png) | Optional exercise Open [http://mng.bz/YrJo](http://mng.bz/YrJo)
    and step through the code until you reach exercise 2, then work on exercise 2\.
    Plot a Normal distribution along with the likelihood of the observed values as
    shown in the figure in the previous sidebar. Manually adapt the parameter values
    of a Normal distribution to achieve a maximal joint likelihood or minimal NLL.
    Develop code which determines the optimal parameter via gradient descent. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ![电脑图标](../Images/computer-icon.png) | 可选练习 打开 [http://mng.bz/YrJo](http://mng.bz/YrJo)
    并逐步执行代码，直到你达到练习2，然后完成练习2。绘制一个正态分布，并像前一个侧边栏中的图所示，绘制观测值的可能性。手动调整正态分布的参数值以实现最大联合可能性或最小NLL。开发代码，通过梯度下降确定最佳参数。
    |'
- en: If you did the exercise, you saw that the likelihood is maximized when the curve
    has a similar spread to the data, and indeed, it is at its maximum when you use
    the standard deviation of the four observations as parameter *σ*x . This means
    that the distribution *N*(*μ**[x[i]]* , *σ**[x[i]]*) you are modeling with your
    NN reflects the distribution of the data as well as possible. In case of a regression,
    the situation is more complicated because you have neighboring points at different
    *x* values. The parameter *σ*[*x[i]*] cannot have a completely different value
    at the neighboring *x* values, but it should be a smooth curve. If the network
    has enough flexibility, it holds that in regions where the spread of the observed
    outcomes is large (like at around *x* = 5 and *x* = 15 in figure 4.17), you should
    use a quite broad conditional Normal distribution that is a larger parameter *σ*[*x[i]*]
    . In contrast, in regions where there is a small spread (for example, around *x*
    = 0 in figure 4.17), *σ*[*x[i]*] should be small. In this way, the likelihood
    approach allows you to estimate the parameters of the conditional Normal distribution
    without having these values as ground truth.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你做了练习，你会看到当曲线的分布与数据相似时，似然性最大化，确实，当使用四个观察值的方差作为参数*σ*x时，它达到最大。这意味着你用神经网络建模的*N*(*μ**[x[i]]*，*σ**[x[i]]*)分布尽可能反映了数据的分布。在回归的情况下，情况更复杂，因为你在不同的*x*值上有相邻的点。参数*σ*[*x[i]*]在相邻的*x*值上不能有完全不同的值，但它应该是一个平滑的曲线。如果网络有足够的灵活性，那么在观察结果分布较大的区域（例如，图4.17中*x*
    = 5和*x* = 15附近），你应该使用一个相当宽的条件正态分布，即较大的参数*σ*[*x[i]*]。相反，在分布较小的区域（例如，图4.17中*x* =
    0附近），*σ*[*x[i]*]应该较小。这样，似然方法允许你估计条件正态分布的参数，而不需要这些值作为确切的值。
- en: Let’s hold on and recall the original question. Why did you think you had a
    ground truth for the mean and not for the variance in the first place? All you
    have is data and a model from which you assume the data has been created. In our
    case, the model is the Gaussian distribution *N*(*μ**[x[i]]* , *σ**[x[i]]*) and
    the network determines its parameters *μ*[*x[i]*] , *σ*[*x[i]*] . You optimize
    the weights of the NN so that the likelihood of the data is maximal. There is
    no such thing as a ground truth for the mean either. An NN would just estimate
    *μ* and *σ* for the conditional distribution, and both are indeed just estimates.
    There is no ground truth neither for *μ* nor for *σ* . There is no spoon.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，回顾一下原始问题。你为什么一开始就认为你有均值的确切值，而不是方差？你拥有的只是数据和从数据中假设生成的模型。在我们的情况下，模型是高斯分布*N*(*μ**[x[i]]*，*σ**[x[i]]*)，网络确定其参数*μ*[*x[i]*]，*σ*[*x[i]*]。你优化神经网络的权重，使数据的似然性最大化。均值也没有确切的值。神经网络只会估计条件分布的*μ*和*σ*，这两个都是估计值。*μ*和*σ*都没有确切的值。没有勺子。
- en: Listing 4.6 Non-linear heteroscedastic regression model from equation 4.5
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 来自方程4.5的非线性异方差回归模型
- en: '[PRE8]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Defines a custom loss
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义自定义损失
- en: ❷ Extracts the first column for *μ*
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取第一列用于*μ*
- en: ❸ Extracts the second column for *σ*
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取第二列用于*σ*
- en: ❹ Defines an NN with 3 hidden layers as in listing 4.4 but now with 2 outcome
    nodes
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义一个具有3个隐藏层的神经网络，如列表4.4所示，但现在有2个输出节点
- en: ❺ Uses the custom loss for the fitting
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用自定义损失进行拟合
- en: You can now graph the fit by not only plotting the curve of the fitted values
    but also the curves for the fitted values plus/minus 1 or 2 times the fitted standard
    deviation. This illustrates the varying spread of the fitted CPD (see figure 4.17).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以通过不仅绘制拟合值的曲线，还绘制拟合值加减1或2倍拟合标准差的曲线来绘制拟合。这说明了拟合CPD（见图4.17）的分布变化。
- en: '![](../Images/4-17.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图4-17](../Images/4-17.png)'
- en: Figure 4.17 The fitted values follow a sinus-shaped curve. The solid middle
    line gives the position of fitted *μ* *x* with varying standard deviation. The
    two thin outer lines correspond to a 95% prediction interval ( *μ* - 2*σ* , *μ*
    + 2*σ* ). We use an NN with three hidden layers, two output nodes, and a customized
    loss to fit the data points (see the dots).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 拟合值遵循正弦形状的曲线。实线中间线给出拟合的*μ* *x*的位置，其标准差在变化。两条细的外线对应于95%预测区间（*μ* - 2*σ*，*μ*
    + 2*σ*）。我们使用具有三个隐藏层、两个输出节点和自定义损失的神经网络来拟合数据点（见点）。
- en: You can design this network arbitrarily deep and wide to model complex relationships
    between *x* and *y*. If you only want to allow for linear relationships between
    inputs and outputs, you should use an NN without hidden layers.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以任意设计这个网络深度和宽度，以模拟*x*和*y*之间的复杂关系。如果你只想允许输入和输出之间存在线性关系，你应该使用没有隐藏层的NN。
- en: '| ![](../Images/computer-icon.png) | Optional exercise Open [http://mng.bz/GVJM](http://mng.bz/GVJM)
    and work through the code until you reach the first exercise indicated by a pen
    icon. You’ll see how to simulate the data shown in figure 4.17 and how to fit
    different regression models. Your task in this exercise is to experiment with
    different activation functions. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 可选练习 打开[http://mng.bz/GVJM](http://mng.bz/GVJM)并运行代码，直到你达到由笔形图标指示的第一个练习。你将看到如何模拟图4.17中显示的数据以及如何拟合不同的回归模型。在这个练习中，你的任务是尝试不同的激活函数。'
- en: You have now seen how to derive a loss function with the MaxLike approach. You
    only need a parametric model for your data. If you want to develop a prediction
    model, then you need to pick a model for the CPD *p*(*y*|*x*) . The CPD yields
    the likelihood of the observed outcomes. To follow this modeling approach in a
    DL manner, you design an NN that outputs the parameters of the probability distribution
    (or value from which these parameters can be derive*D*). The rest is done by TensorFlow
    or Keras. You use SGD to find the value of weights in the NN, which lead to modeled
    parameter values that minimize the NLL. In many cases, the NLL is provided as
    a predefined loss function, such as the cross entropy or the MSE loss. But you
    can also work with arbitrary likelihoods by defining a customized loss function
    that corresponds to the NLL. You saw in listing 4.6 a customized loss function
    defining the NLL for regression problems with nonconstant variance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经看到了如何使用MaxLike方法推导损失函数。你只需要为你的数据定义一个参数模型。如果你想开发一个预测模型，那么你需要选择一个CPD *p*(*y*|*x*)的模型。CPD给出了观测结果的似然。要按照深度学习的方式遵循这种建模方法，你设计一个NN，该NN输出概率分布的参数（或可以从这些参数推导出的值）。其余的工作由TensorFlow或Keras完成。你使用SGD来找到NN中权重的值，这些权重导致模型参数值最小化NLL。在许多情况下，NLL被作为预定义的损失函数提供，例如交叉熵或MSE损失。但你也可以通过定义一个与NLL对应的自定义损失函数来处理任意似然。你在列表4.6中看到了一个自定义损失函数，它定义了具有非常数方差的回归问题的NLL。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In the maximum likelihood (MaxLike) approach, you tune the parameter of a model
    such that the resulting model can produce the observed data with a higher probability
    than all other models with different parameter values.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最大似然（MaxLike）方法中，你调整模型的参数，使得产生的模型以比所有具有不同参数值的其他模型更高的概率产生观测数据。
- en: The MaxLike approach is a versatile tool to fit the parameters of models. It
    is widely used in statistics and provides a sound theoretical framework to derive
    loss functions.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MaxLike方法是一个多功能的工具，用于拟合模型的参数。它在统计学中得到广泛应用，并提供了一个坚实的理论框架来推导损失函数。
- en: To use the MaxLike approach, you need to define a parametric probability distribution
    for the observed data.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用MaxLike方法，你需要为观测数据定义一个参数概率分布。
- en: The likelihood of a discrete variable is given by a discrete probability distribution.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散变量的似然由离散概率分布给出。
- en: The likelihood of a continuous outcome is given by a continuous density function.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续结果的似然由连续密度函数给出。
- en: 'The MaxLike approach involves:'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MaxLike方法包括：
- en: Defining a parametric model for the (Discrete or continuous) probability distribution
    of the observed data
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义观测数据的（离散或连续）概率分布的参数模型
- en: Maximizing the likelihood (or minimizing the NLL) for the observed data
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于观测数据最大化似然（或最小化NLL）
- en: To develop a prediction model, you need to pick a model for the conditional
    probability distribution (CPD) of the outcome *y* given the input *x*.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要开发一个预测模型，你需要选择一个给定输入*x*的输出*y*的条件概率分布（CPD）的模型。
- en: Using the MaxLike approach for a classification task is based on a Bernoulli
    or multinomial probability distribution like CPD and leads to the standard loss
    for the classification known as cross entropy in Keras and TensorFlow.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MaxLike方法进行分类任务基于伯努利或多项式概率分布，如CPD，导致Keras和TensorFlow中已知的分类标准损失，即交叉熵。
- en: The Kullback-Leibler (KL) divergence is a measure between the predicted and
    the ground-truth CPD. Minimizing it has the same effect as minimizing cross entropy.
    In that sense, the KL divergence is the pendent of the MSE for a classification
    model.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback-Leibler（KL）散度是预测和真实CPD之间的度量。最小化它具有与最小化交叉熵相同的效果。从这个意义上讲，KL散度是分类模型中均方误差（MSE）的对应物。
- en: Using the MaxLike approach for a linear regression task that is based on a Normal
    probability distribution like CPD leads to the mean squared error (MSE) loss.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于正态概率分布（如CPD）的线性回归任务的MaxLike方法会导致均方误差（MSE）损失。
- en: For linear regression with a constant variance, we can interpret the output
    of the network with input *x* as the parameter *μ**[x]* of a conditional Normal
    distribution *N*(*μ**[x]*, *σ*).
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有常数方差的线性回归，我们可以将网络的输入 *x* 的输出解释为条件正态分布 *N*(*μ**[x]*, *σ*) 的参数 *μ**[x]*。
- en: Regression with nonconstant variance can be fitted with a loss that corresponds
    to the negative log-likelihood (NLL) of a normal probability model, where both
    parameters (mean and standard deviation) depend on the input *x* and can be computed
    by an NN with two outputs yielding the CPD *N*(*μ**[x]*, *σ**[x]*).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常数方差回归可以通过与正态概率模型的负对数似然（NLL）相对应的损失函数来拟合，其中两个参数（均值和标准差）都依赖于输入 *x*，并且可以通过具有两个输出的神经网络来计算，该输出产生条件概率分布（CPD）*N*(*μ**[x]*,
    *σ**[x]*)。
- en: Non-linear relationships can be fitted with the MSE loss by introducing hidden
    layers.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过引入隐藏层，可以使用均方误差（MSE）损失来拟合非线性关系。
- en: Generally, likelihoods of arbitrary CPDs can be maximized in the framework of
    NNs by interpreting the output of the NN as the parameter of the probability distributions.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，可以在神经网络框架中通过将神经网络的输出解释为概率分布的参数来最大化任意CPD的似然。
- en: 1.Although it’s not important for the rest of the book, if you are curious how
    to get the number 45, it is the number of all permutations 10! corrected by (Divided
    by) the number of indistinguishable permutations. That is 10! / (2! · 8!) = 45
    in our case. For more details, see for example, [http://mng.bz/gyQe](http://mng.bz/gyQe)
    .
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 1.虽然这对本书的其余部分并不重要，但如果您对如何得到数字45感兴趣，它是所有排列10!经过（除以）不可区分排列数校正后的数量。也就是说，10! / (2!
    · 8!) = 45在我们的情况下。更多细节，例如，请参阅[http://mng.bz/gyQe](http://mng.bz/gyQe)。
