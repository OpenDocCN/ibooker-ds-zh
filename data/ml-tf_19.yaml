- en: 16 Recurrent neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 循环神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the components of a recurrent neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解循环神经网络组件
- en: Designing a predictive model of time-series data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计时间序列数据的预测模型
- en: Using the time-series predictor on real-world data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实际数据上使用时间序列预测器
- en: Back in school, I remember my sigh of relief when one of my midterm exams consisted
    of only true-or-false questions. I can’t be the only one who assumed that half
    the answers would be true and the other half would be false.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 回到学校的时候，我记得当我发现一次期中考试只有是非题时，我如释重负。我不可能是唯一一个认为答案中一半是对的，另一半是错的的人。
- en: I figured out answers to most of the questions and left the rest to guessing.
    But that guessing was based on something clever, a strategy that you might have
    employed as well. After counting my number of true answers, I realized that a
    disproportionate number of false answers were lacking. So a majority of my guesses
    were false to balance the distribution.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我找到了大多数问题的答案，其余的留给了猜测。但那种猜测是基于一些聪明的策略，你可能也使用过这种策略。在数出我的正确答案数量后，我意识到有相当一部分错误答案缺乏。所以，我大多数的猜测都是错误的，以平衡分布。
- en: It worked. I sure felt sly in the moment. What is this feeling of craftiness
    that makes us feel so confident in our decisions, and how can we give a neural
    network the same power?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它起作用了。那一刻，我确实感觉到了狡猾。是什么感觉让我们对自己的决定如此自信，我们如何才能赋予神经网络同样的力量？
- en: One answer is to use context to answer questions. Contextual cues are important
    signals that can improve the performance of machine-learning algorithms. Suppose
    that you want to examine an English sentence and tag the part of speech of each
    word (a problem that may be more familiar to you after chapter 10).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个答案是使用上下文来回答问题。上下文线索是重要的信号，可以提高机器学习算法的性能。假设你想检查一个英语句子，并标记每个单词的词性（在第10章之后，你可能对这个问题更熟悉）。
- en: The naïve approach is to classify each word individually as a noun, an adjective,
    and so on without acknowledging the neighboring words. Consider trying that technique
    on the words in *this* sentence. The word *trying* is used as a verb, but depending
    on the context, you can also use it as an adjective, making parts-of-speech tagging
    a *trying* problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 天真的方法是对每个单词单独进行分类，如名词、形容词等，而不承认邻近的单词。考虑一下将这种技术应用于这个句子中的单词。单词*trying*被用作动词，但根据上下文，你也可以将其用作形容词，这使得词性标注成为一个*棘手*的问题。
- en: A better approach considers the context. To bestow contextual cues on neural
    networks, you’ll study an architecture called a *recurrent neural network* (RNN).
    Instead of natural language data, you’ll be dealing with continuous time-series
    data, such as stock market prices. By the end of the chapter, you’ll be able to
    model the patterns in time-series data to predict future values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法考虑上下文。为了向神经网络提供上下文线索，你将学习一种称为**循环神经网络**（RNN）的架构。你将处理的是连续的时间序列数据，例如股票市场价格，而不是自然语言数据。到本章结束时，你将能够对时间序列数据中的模式进行建模，以预测未来的值。
- en: 16.1 Introduction to RNNs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 循环神经网络简介
- en: To understand RNNs, look at the simple architecture in figure 16.1\. This architecture
    takes as input a vector *X*(*t*) and generates as output a vector *Y*(*t*) at
    some time (*t*). The circle in the middle represents the hidden layer of the network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解RNN，请查看图16.1中的简单架构。这个架构将向量*X*(*t*)作为输入，并在某个时间(*t*)生成输出向量*Y*(*t*)。中间的圆圈代表网络的隐藏层。
- en: '![CH16_F01_Mattmann2](../Images/CH16_F01_Mattmann2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F01_Mattmann2](../Images/CH16_F01_Mattmann2.png)'
- en: Figure 16.1 A neural network with the input and output layers labeled X(t) and
    Y(t), respectively
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 一个神经网络，输入层和输出层分别标记为X(t)和Y(t)
- en: With enough input/output examples, you can learn the parameters of the network
    in TensorFlow. Let’s refer to the input weights as a matrix *W*[in] and the output
    weights as a matrix *W*[out]. Assume that there’s one hidden layer, referred to
    as a vector *Z*(*t*).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有足够的输入/输出示例，你可以在TensorFlow中学习网络的参数。让我们将输入权重称为矩阵*W*[in]，将输出权重称为矩阵*W*[out]。假设有一个隐藏层，称为向量*Z*(*t*)。
- en: As shown in figure 16.2, the first half of the neural network is characterized
    by the function *Z*(*t*) = *X*(*t*) × *W*[in], and the second half of the neural
    network takes the form *Y*(*t*) = *Z*(*t*) × *W*[out]. Equivalently, if you prefer,
    the whole neural network is the function *Y*(*t*) = (*X*(*t*) × *W*[in]) × *W*[out].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如图16.2所示，神经网络的前半部分由函数 *Z*(*t*) = *X*(*t*) × *W*[in] 描述，而神经网络的第二半部分的形式为 *Y*(*t*)
    = *Z*(*t*) × *W*[out]。等价地，如果你愿意，整个神经网络是函数 *Y*(*t*) = (*X*(*t*) × *W*[in]) × *W*[out]。
- en: '![CH16_F02_Mattmann2](../Images/CH16_F02_Mattmann2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F02_Mattmann2](../Images/CH16_F02_Mattmann2.png)'
- en: Figure 16.2 The hidden layer of a neural network can be thought of as a hidden
    representation of the data, which is encoded by the input weights and decoded
    by the output weights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 神经网络的隐藏层可以被视为数据的隐藏表示，它由输入权重编码并由输出权重解码。
- en: After spending nights fine-tuning the network, you probably want to start using
    your learned model in a real-world scenario. Typically, that process implies calling
    the model multiple times, as depicted in figure 16.3.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在花费了无数个夜晚微调网络之后，你可能想开始在现实场景中使用你学习到的模型。通常，这个过程意味着多次调用模型，如图16.3所示。
- en: '![CH16_F03_Mattmann2](../Images/CH16_F03_Mattmann2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F03_Mattmann2](../Images/CH16_F03_Mattmann2.png)'
- en: Figure 16.3 Often, you end up running the same neural network multiple times
    without using knowledge about the hidden states of the previous runs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 通常，你会在不知道之前运行隐藏状态的情况下多次运行相同的神经网络。
- en: At each time *t*, when calling the learned model, this architecture doesn’t
    take into account knowledge about the previous runs. This process is like predicting
    stock-market trends by looking only at data from the current day. A better idea
    is to exploit overarching patterns from a week’s or month’s worth of data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间 *t*，当调用学习到的模型时，这种架构没有考虑之前运行的知识。这个过程就像只看当天的数据来预测股市趋势。更好的想法是利用一周或一个月的数据中的总体模式。
- en: A RNN is different from a traditional neural network because it introduces a
    transition weight *W* to transfer information over time. Figure 16.4 shows the
    three weight matrices that must be learned in an RNN. The introduction of the
    transition weight means that the next state is dependent on the previous model
    as well as the previous state, so your model has a “memory” of what it did.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RNN与传统的神经网络不同，因为它引入了一个过渡权重 *W* 来在时间上传递信息。图16.4显示了在RNN中必须学习的三个权重矩阵。引入过渡权重意味着下一个状态不仅依赖于前一个模型，还依赖于前一个状态，因此你的模型有“记忆”它所做的事情。
- en: '![CH16_F04_Mattmann2](../Images/CH16_F04_Mattmann2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F04_Mattmann2](../Images/CH16_F04_Mattmann2.png)'
- en: Figure 16.4 RNN architecture can use the previous states of the network to its
    advantage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 RNN架构可以利用网络的先前状态来获得优势。
- en: Diagrams are nice, but you’re here to get your hands dirty. Let’s get right
    to it! Section 16.2 shows how to use TensorFlow’s built-in RNN models. Then you’ll
    use an RNN on real-world time-series data to predict the future.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图表很棒，但你来这里是为了亲自动手。让我们直接进入正题！第16.2节展示了如何使用TensorFlow内置的RNN模型。然后你将使用RNN对实际的时间序列数据进行预测。
- en: 16.2 Implementing a recurrent neural network
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 实现循环神经网络
- en: As you implement the RNN, you’ll use TensorFlow to do much of the heavy lifting.
    You won’t need to build a network manually, as shown in figure 16.4, because the
    TensorFlow library already supports some robust RNN models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现RNN时，你将使用TensorFlow来完成大部分繁重的工作。你不需要像图16.4所示那样手动构建网络，因为TensorFlow库已经支持一些强大的RNN模型。
- en: Note For TensorFlow library information on RNNs, see [https://www.svds .com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关TensorFlow库中RNN的信息，请参阅[https://www.svds.com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)。
- en: One type of RNN model is *long short-term memory* (LSTM)—a fun name that means
    exactly what it sounds like. Short-term patterns aren’t forgotten in the long
    term.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种RNN模型是*长短期记忆*（LSTM）——一个有趣的名字，意味着它听起来那样。短期模式在长期不会被遗忘。
- en: The precise implementation details of LSTM are beyond the scope of this book.
    Trust me, a thorough inspection of the LSTM model would distract from the chapter
    because there’s no definite standard yet. TensorFlow comes to the rescue by taking
    care of how the model is defined so you can use it out of the box. And as TensorFlow
    is updated, you’ll be able to take advantage of improvements in the LSTM model
    without modifying your code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的确切实现细节超出了本书的范围。相信我，对LSTM模型的彻底检查会分散本章的注意力，因为没有明确的行业标准。TensorFlow通过处理模型的定义来提供帮助，这样你就可以直接使用它。随着TensorFlow的更新，你将能够利用LSTM模型中的改进，而无需修改你的代码。
- en: 'TIP To see how to implement LSTM from scratch, I suggest the web page [https://apaszke.github.io/lstm-explained.html](https://apaszke.github.io/lstm-explained.html).
    The paper that describes the implementation of regularization used in this chapter’s
    listings is available at [https://arxiv.org/abs/1409.2329](https://arxiv.org/abs/1409.2329).
    Finally, this tutorial for RNNs and LSTMs provides some real notebooks and code
    to try: [https://www.svds.com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：要了解如何从头开始实现LSTM，我建议访问网页[https://apaszke.github.io/lstm-explained.html](https://apaszke.github.io/lstm-explained.html)。描述本章列表中使用的正则化实现的论文可在[https://arxiv.org/abs/1409.2329](https://arxiv.org/abs/1409.2329)找到。最后，这个关于RNN和LSTMs的教程提供了一些真实的笔记本和代码以供尝试：[https://www.svds.com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)。
- en: Begin by writing your code in a new file called simple_regression.py. Then import
    the relevant libraries, as shown in listing 16.1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在一个名为simple_regression.py的新文件中编写你的代码。然后，如列表16.1所示，导入相关库。
- en: Listing 16.1 Importing relevant libraries
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.1 导入相关库
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, define a class called `SeriesPredictor`. The constructor, shown in listing
    16.2, will set up model hyperparameters, weights, and the `cost` function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个名为`SeriesPredictor`的类。构造函数，如列表16.2所示，将设置模型超参数、权重和`成本`函数。
- en: Listing 16.2 Defining a class and its constructor
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.2 定义类及其构造函数
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Hyperparameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 超参数
- en: ❷ Weight variables and input placeholders
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 权重变量和输入占位符
- en: ❸ Cost optimizer
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 成本优化器
- en: ❹ Auxiliary ops
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 辅助操作
- en: Next, use TensorFlow’s built-in RNN model `BasicLSTMCell`. The hidden dimension
    of the cell passed into the `BasicLSTMCell` object is the dimension of the hidden
    state that gets passed through time. You can run this cell with data by using
    the `rnn.dynamic_rnn` function to retrieve the output results. Listing 16.3 details
    how to use TensorFlow to implement a predictive model with LSTM.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用TensorFlow的内置RNN模型`BasicLSTMCell`。传递给`BasicLSTMCell`对象的隐藏维度是随时间传递的隐藏状态的维度。你可以通过使用`rnn.dynamic_rnn`函数来运行这个单元，以检索输出结果。列表16.3详细说明了如何使用TensorFlow实现带有LSTM的预测模型。
- en: Listing 16.3 Defining the RNN model
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.3 定义RNN模型
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Creates an LSTM cell
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个LSTM单元
- en: ❷ Runs the cell on the input to obtain tensors for outputs and states
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在输入上运行单元以获得输出和状态的张量
- en: ❸ Computes the output layer as a fully connected linear function
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将输出层计算为全连接线性函数
- en: With a model and `cost` function defined, you can implement the training function,
    which will learn the LSTM weights, given example input/output pairs. As listing
    16.4 shows, you open a session and repeatedly run the optimizer on the training
    data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了模型和`成本`函数后，你可以实现训练函数，该函数将根据示例输入/输出对学习LSTM权重。如列表16.4所示，你打开一个会话，并在训练数据上反复运行优化器。
- en: Note You can use cross-validation to figure out how many iterations you need
    to train the model. In this case, you assume a fixed number of epochs. Some good
    insights and answers are available at Q&A sites such as ResearchGate ([http://mng.bz/lB92](http://mng.bz/lB92)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以使用交叉验证来确定训练模型所需的迭代次数。在这种情况下，你假设一个固定的epoch数量。在ResearchGate（[http://mng.bz/lB92](http://mng.bz/lB92)）等问答网站上可以找到一些有价值的见解和答案。
- en: After training, save the model to a file so you can load it later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，将模型保存到文件中，以便以后加载。
- en: Listing 16.4 Training the model on a dataset
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.4 在数据集上训练模型
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Runs the train op 1,000 times
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 运行train op 1,000次
- en: Let’s say that all went well, and your model has learned parameters. Next, you’d
    like to evaluate the predictive model on other data. Listing 16.5 loads the saved
    model and runs the model in a session by feeding in test data. If a learned model
    doesn’t perform well on testing data, you can try tweaking the number of hidden
    dimensions of the LSTM cell.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一切顺利，并且您的模型已经学习到了参数。接下来，您可能希望在其他数据上评估预测模型。列表16.5加载了保存的模型，并通过输入测试数据在会话中运行模型。如果学习到的模型在测试数据上表现不佳，您可以尝试调整LSTM单元的隐藏维数数量。
- en: Listing 16.5 Testing the learned model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.5 测试学习到的模型
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It’s done! But to convince yourself that it works, make up some data to try
    to train the predictive model. In listing 16.6, you’ll create input sequences
    (`train_x`) and corresponding output sequences (`train_y`).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！但为了确信它工作正常，您可以创建一些数据来尝试训练预测模型。在列表16.6中，您将创建输入序列（`train_x`）和相应的输出序列（`train_y`）。
- en: Listing 16.6 Training and testing on dummy data
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.6 在虚拟数据上进行训练和测试
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Predicted result should be 1, 3, 5, 7.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预测结果应为1，3，5，7。
- en: ❷ Predicted result should be 4, 9, 11, 13.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测结果应为4，9，11，13。
- en: You can treat this predictive model as a black box and train it with real-world
    time-series data for prediction. In section 16.3, you’ll get data to work with.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以将这个预测模型视为一个黑盒，并用真实世界的时间序列数据进行训练以进行预测。在第16.3节中，您将获得可以处理的数据。 '
- en: 16.3 Using a predictive model for time-series data
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 使用预测模型处理时间序列数据
- en: Time-series data is abundantly available online. For this example, you’ll use
    data about international airline passengers for a specific period. You can obtain
    this data from [http://mng.bz/ggOV](http://mng.bz/ggOV). Clicking that link takes
    you to a nice plot of the time-series data, shown in figure 16.5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据在网上大量可用。对于这个例子，您将使用特定时期国际航空公司乘客的数据。您可以从[http://mng.bz/ggOV](http://mng.bz/ggOV)获取这些数据。点击该链接将带您到一个时间序列数据的好图，如图16.5所示。
- en: '![CH16_F05_Mattmann2](../Images/CH16_F05_Mattmann2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F05_Mattmann2](../Images/CH16_F05_Mattmann2.png)'
- en: Figure 16.5 Raw data showing the number of international airline passengers
    throughout the years
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 显示了多年来国际航空公司乘客数量的原始数据
- en: You can download the data by clicking the Data tab and then selecting CSV. You’ll
    have to edit the CSV file manually to remove the header line and the additional
    footer line.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击数据选项卡然后选择CSV来下载数据。您将需要手动编辑CSV文件以删除标题行和额外的页脚行。
- en: In a file called data_loader.py, add the code in listing 16.7.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在名为`data_loader.py`的文件中，添加列表16.7中的代码。
- en: Listing 16.7 Loading data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.7 加载数据
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Loops through the lines of the file and converts to a floating-point number
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历文件中的行并将其转换为浮点数
- en: ❷ Preprocesses the data by mean-centering and dividing by standard deviation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过均值中心化和除以标准差来预处理数据
- en: ❸ Calculates training data samples
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算训练数据样本
- en: ❹ Splits the dataset into training and testing
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将数据集分为训练集和测试集
- en: 'Here, you define two functions: `load_series` and `split_data`. The first function
    loads the time-series file on disk and normalizes it; the other function divides
    the dataset into two components for training and testing.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您定义了两个函数：`load_series`和`split_data`。第一个函数加载磁盘上的时间序列文件并对其进行归一化；另一个函数将数据集分为两个部分，用于训练和测试。
- en: Because you’ll be evaluating the model multiple times to predict future values,
    let’s modify the `test` function from `SeriesPredictor` to take a session as an
    argument instead of initializing the session on every call. See listing 16.8 for
    this tweak.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您将多次评估模型以预测未来的值，让我们修改`SeriesPredictor`中的`test`函数，使其接受一个会话作为参数，而不是在每次调用时初始化会话。请参阅列表16.8中的此调整。
- en: Listing 16.8 Modifying the `test` function to pass in the session
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.8 修改`test`函数以传入会话
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now you can train the predictor by loading the data in the acceptable format.
    Listing 16.9 shows how to train the network and then use the trained model to
    predict future values. You’ll generate the training data (`train_x` and `train_y`)
    to look like the data shown earlier in listing 16.6.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以通过加载可接受格式的数据来训练预测器。列表16.9展示了如何训练网络，然后使用训练好的模型来预测未来的值。您将生成训练数据（`train_x`和`train_y`），使其看起来像列表16.6中早些时候显示的数据。
- en: Listing 16.9 Generate training data
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.9 生成训练数据
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The dimension of each element of the sequence is a scalar (1D).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 序列中每个元素的维度是一个标量（1D）。
- en: ❷ Length of each sequence
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个序列的长度
- en: ❸ Size of the RNN hidden dimension
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ RNN隐藏维度的尺寸
- en: ❹ Loads the data
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 加载数据
- en: ❺ Slides a window through the time-series data to construct the training dataset
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在时间序列数据上滑动窗口以构建训练数据集
- en: ❻ Uses the same window-sliding strategy to construct the test dataset
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用相同的窗口滑动策略来构建测试数据集
- en: ❼ Trains a model on the training dataset
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在训练数据集上训练模型
- en: ❽ Visualizes the model’s performance
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 可视化模型的性能
- en: The predictor generates two graphs. The first graph is prediction results of
    the model, given ground-truth values (figure 16.6).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器生成两个图表。第一个图表是模型的预测结果，给定真实值（图16.6）。
- en: '![CH16_F06_Mattmann2](../Images/CH16_F06_Mattmann2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F06_Mattmann2](../Images/CH16_F06_Mattmann2.png)'
- en: Figure 16.6 The predictions match trends fairly well when tested against ground-truth
    data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 当与真实数据测试时，预测结果与趋势匹配得相当好。
- en: The other graph shows the prediction results when only the training data is
    given (blue line)—nothing else (figure 16.7). This procedure has less information
    available, but it still does a good job of matching data trends.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个图表显示了仅提供训练数据时的预测结果（蓝色线）——没有其他任何东西（图16.7）。这个过程可用的信息较少，但它仍然很好地匹配了数据趋势。
- en: '![CH16_F07_Mattmann2](../Images/CH16_F07_Mattmann2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH16_F07_Mattmann2](../Images/CH16_F07_Mattmann2.png)'
- en: Figure 16.7 If the algorithm uses previously predicted results to make further
    predictions, the general trend matches well, but not specific bumps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 如果算法使用先前预测的结果来做出进一步的预测，总体趋势匹配得很好，但不是具体的峰值。
- en: You can use time-series predictors to reproduce realistic fluctuations in data.
    Imagine predicting market boom-and-bust cycles based on the tools you’ve learned
    so far. What are you waiting for? Grab some market data, and learn your own predictive
    model!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用时间序列预测器来重现数据中的真实波动。想象一下，基于你迄今为止学到的工具来预测市场繁荣与萧条周期。你在等什么？抓取一些市场数据，并学习你自己的预测模型！
- en: 16.4 Applying RNNs
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 应用RNNs
- en: RNNs are meant to be used with sequential data. Because audio signals are a
    dimension lower than video (linear signal versus 2D pixel array), it’s a lot easier
    to get started with audio time-series data. Consider how much speech recognition
    has improved over the years; it’s becoming a tractable problem.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs旨在用于序列数据。由于音频信号比视频（线性信号与2D像素数组）低一个维度，因此开始处理音频时间序列数据要容易得多。考虑一下语音识别在过去几年里取得了多大的进步；它正成为一个可处理的问题。
- en: 'Like the audio histogram analysis on clustering audio data that you conducted
    in chapter 7, most speech recognition preprocessing involves representing the
    sound in a chromagram of sorts. A common technique is to use *mel-frequency cepstral
    coefficients* (MFCCs). A good introduction is outlined in this blog post: [http://mng.bz/411F](http://mng.bz/411F).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在第7章中进行的音频数据聚类分析中的音频直方图分析类似，大多数语音识别预处理都涉及以某种形式的音程图来表示声音。一种常见的技术是使用 *梅尔频率倒谱系数*
    (MFCCs)。本博客文章概述了一个很好的介绍：[http://mng.bz/411F](http://mng.bz/411F)。
- en: 'Next, you’ll need a dataset to train your model. A few popular ones include
    the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要一个数据集来训练你的模型。以下是一些流行的数据集：
- en: LibriSpeech ([www.openslr.org/12](http://www.openslr.org/12))
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LibriSpeech ([www.openslr.org/12](http://www.openslr.org/12))
- en: TED-LIUM ([www.openslr.org/7](http://www.openslr.org/7))
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TED-LIUM ([www.openslr.org/7](http://www.openslr.org/7))
- en: VoxForge ([www.voxforge.org](http://www.voxforge.org/))
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VoxForge ([www.voxforge.org](http://www.voxforge.org/))
- en: An in-depth walkthrough of a simple speech-recognition implementation in TensorFlow
    that uses these datasets is available at [https://svds.com/tensorflow-rnn-tutorial](https://svds.com/tensorflow-rnn-tutorial).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中使用这些数据集进行简单语音识别实现的详细说明可在[https://svds.com/tensorflow-rnn-tutorial](https://svds.com/tensorflow-rnn-tutorial)找到。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A recurrent neural network (RNN) uses information from the past. That way, it
    can make predictions by using data with high temporal dependencies.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个循环神经网络（RNN）使用过去的信息。这样，它可以通过使用具有高时间依赖性的数据来做出预测。
- en: TensorFlow comes with RNN models out of the box.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow自带RNN模型。
- en: Time-series prediction is a useful application for RNNs because of temporal
    dependencies in the data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据中的时间依赖性，时间序列预测是RNNs的一个有用应用。
