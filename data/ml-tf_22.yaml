- en: 19 Utility landscape
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19 效用景观
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Implementing a neural network for ranking
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现用于排名的神经网络
- en: Image embedding with VGG16
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VGG16进行图像嵌入
- en: Visualizing utility
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化效用
- en: Processing sensory input enables robots to adjust their model of the world around
    them. In the case of a vacuum-cleaner robot, the furniture in the room may change
    day to day, so the robot must be able to adapt to chaotic environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 处理感官输入使机器人能够调整他们对周围世界的模型。在吸尘机器人案例中，房间里的家具可能会每天变化，因此机器人必须能够适应混乱的环境。
- en: Let’s say you own a futuristic housemaid robot, which comes with a few basic
    skills but also with the ability to learn new skills from human demonstrations.
    Maybe you’d like to teach it how to fold clothes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有一台未来派的家用机器人女仆，它具备一些基本技能，同时也能从人类的演示中学习新技能。也许你想教它如何叠衣服。
- en: 'Teaching a robot how to accomplish a new task is a tricky problem. Some immediate
    questions come to mind:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 教机器人完成一项新任务是棘手的问题。一些直接的问题会浮现在脑海中：
- en: Should the robot simply mimic a human’s sequence of actions? Such a process
    is referred to as *imitation learning*.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人是否应该简单地模仿人类的动作序列？这样的过程被称为*模仿学习*。
- en: How do a robot’s arms and joints match up to human poses? This dilemma is often
    referred to as the *correspondence problem*.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人的手臂和关节如何与人类姿势相匹配？这个问题通常被称为*对应问题*。
- en: In this chapter, you’re going to model a task from human demonstrations while
    avoiding both imitation learning and the correspondence problem. Lucky you! You’ll
    achieve this task by studying a way to rank states of the world with a *utility
    function*, which takes a state and returns a real value representing its desirability.
    You’ll not only steer clear of imitation as a measure of success, but also bypass
    the complications of mapping a robot’s set of actions to that of a human (the
    correspondence problem).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将模拟一个从人类演示中学习到的任务，同时避免模仿学习和对应问题。真是太幸运了！你将通过研究一种使用*效用函数*对世界状态进行排名的方法来完成这个任务，该函数接受一个状态并返回一个表示其可取性的实数值。你不仅将避免将模仿作为成功衡量标准，还将绕过将机器人的动作集映射到人类动作集的复杂性（对应问题）。
- en: Exercise 19.1
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 练习19.1
- en: The goal of imitation learning is for the robot to reproduce the action sequences
    of the demonstrator. This goal sounds good on paper, but what are the limitations
    of such an approach?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习的目标是让机器人重现演示者的动作序列。这个目标在纸上听起来不错，但这种方法有哪些局限性？
- en: '**Answer**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Mimicking human actions is a naïve approach to learning from human demonstrations.
    Instead, the agent should identify the hidden goal behind a demonstration. When
    someone folds clothes, for example, the goal is to flatten and compress them,
    which are concepts independent of a human’s hand motions. By understanding why
    the human is producing their action sequence, the agent is better able to generalize
    the skill it’s being taught.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿人类动作是从人类演示中学习的一种天真方法。相反，代理应该识别演示背后的隐藏目标。例如，当有人叠衣服时，目标是使衣物变平并压缩，这些概念与人类的手部动作无关。通过理解人类为何产生他们的动作序列，代理能够更好地概括它所教授的技能。
- en: In section 19.1, you’ll learn how to implement a utility function over the states
    of the world obtained through videos of human demonstrations of a task. The learned
    utility function is a model of preferences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第19.1节中，你将学习如何通过人类演示任务的视频实现世界状态的效用函数。学习到的效用函数是偏好模型。
- en: You’ll explore the task of teaching a robot how to fold articles of clothing.
    A wrinkled article of clothing is almost certainly in a configuration that has
    never been seen before. As shown in figure 19.1, the utility framework has no
    limitations on the size of the state space. The preference model is trained specifically
    on videos of people folding T-shirts in various ways.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你将探索如何教机器人如何折叠衣物。一件皱巴巴的衣物几乎肯定处于一种以前从未见过的配置。如图19.1所示，效用框架对状态空间的大小没有限制。偏好模型专门在人们以各种方式折叠T恤的视频上训练。
- en: '![CH19_F01_Mattmann2](../Images/CH19_F01_Mattmann2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F01_Mattmann2](../Images/CH19_F01_Mattmann2.png)'
- en: Figure 19.1 Wrinkled clothes in a less-favorable state than well-folded clothes.
    This diagram shows how you might score each state of a piece of cloth; higher
    scores represent a more-favorable state.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1 穿着皱巴巴的衣服比叠得好的衣服状态更差。此图展示了如何评估一块布的每种状态；分数越高代表状态越佳。
- en: The utility function generalizes across states (wrinkled T-shirt in novel configuration
    versus folded T-shirt in familiar configuration) and reuses knowledge across clothes
    (T-shirt folding versus pants folding).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 效用函数可以推广到各种状态（新颖配置的皱巴巴的T恤与熟悉配置的折叠T恤）并在服装（T恤折叠与裤子折叠）之间重用知识。
- en: 'We can further illustrate the practical applications of a good utility function
    with the following argument: in real-world situations, not all visual observations
    are optimized toward learning a task. A teacher demonstrating a skill may perform
    irrelevant, incomplete, or even incorrect actions, yet humans are capable of ignoring
    the mistakes.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下论点进一步说明良好效用函数的实际应用：在现实世界中，并非所有视觉观察都是针对学习任务进行优化的。演示技能的教师可能执行无关、不完整甚至错误的行为，但人类能够忽略这些错误。
- en: When a robot watches human demonstrations, you want it to understand the causal
    relationships that go into achieving a task. Your work enables the learning phase
    to be interactive, where the robot is actively skeptical of human behavior, to
    refine the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器人观看人类演示时，你希望它能理解完成任务所涉及的因果关系。你的工作使得学习阶段可以互动，机器人对人类行为持积极怀疑态度，以完善训练数据。
- en: To accomplish this goal, you first learn a utility function from a small number
    of videos to rank the preferences of various states. Then, when the robot is shown
    a new instance of a skill through human demonstration, it consults the utility
    function to verify that the expected utility increases over time. Finally, the
    robot interrupts the human demonstration to ask whether the action was essential
    for learning the skill.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个目标，你首先从少量视频中学习一个效用函数来对各种状态的偏好进行排名。然后，当机器人通过人类演示展示一项新技能的实例时，它会咨询效用函数以验证预期的效用随时间增加。最后，机器人中断人类演示，询问该动作是否对于学习技能是必要的。
- en: 19.1 Preference model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1 偏好模型
- en: We assume that human preferences are derived from a *utilitarian* perspective,
    meaning that a number determines the rank of items. Suppose that you surveyed
    people to rank the fanciness of various foods (such as steak, hot dog, shrimp
    cocktail, and burger).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设人类的偏好是从一个*功利主义*的角度得出的，这意味着一个数字决定了物品的排名。假设你调查了人们对各种食物的精致程度（如牛排、热狗、虾尾和汉堡）进行排名。
- en: Figure 19.2 shows a couple of possible rankings between pairs of food. As you
    might expect, steak is ranked higher than hot dog and shrimp cocktail higher than
    burger on the fanciness scale.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2展示了食物成对之间的一些可能排名。正如你所预期的那样，在精致程度上，牛排比热狗排名更高，虾尾比汉堡排名更高。
- en: '![CH19_F02_Mattmann2](../Images/CH19_F02_Mattmann2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F02_Mattmann2](../Images/CH19_F02_Mattmann2.png)'
- en: 'Figure 19.2 A possible set of pairwise rankings between objects. Specifically,
    you have four food items, and you want to rank them by fanciness, so you employ
    two pairwise ranking decisions: steak is a fancier meal than a hot dog, and shrimp
    cocktail is a fancier meal than a burger.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2 展示了一组可能的对象成对排名。具体来说，你有四种食物，你想根据精致程度对它们进行排名，因此你采用了两个成对排名决策：牛排比热狗更精致，虾尾比汉堡更精致。
- en: Fortunately for the individuals being surveyed, not every pair of items needs
    to be ranked. It might not be so obvious which is fancier between hotdog and burger
    or between steak and shrimp cocktail. There’s a lot of room for disagreement.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于被调查的个人来说，并不是每一对物品都需要进行排名。可能并不明显的是，热狗和汉堡或牛排和虾尾之间哪个更精致。存在很多不同的意见空间。
- en: If a state *s*[1] has a higher utility than another state *s*[2], the corresponding
    ranking is denoted *s*[1] > *s*[2], implying that the utility of *s*[1] is greater
    than the utility of *s*[2]. Each video demonstration contains a sequence of *n*
    states *s*[0], *s*[1], ..., *s*[n], which offers *n*(*n* - 1)/2 possible ordered
    pairs ranking constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态 *s*[1] 的效用高于另一个状态 *s*[2]，则相应的排名表示为 *s*[1] > *s*[2]，这意味着 *s*[1] 的效用大于 *s*[2]
    的效用。每个视频演示包含一个由 *n* 个状态 *s*[0]，*s*[1]，...，*s*[n] 组成的序列，提供了 *n*(*n* - 1)/2 个可能的有序对排名约束。
- en: Let’s implement our own neural network capable of ranking. Open a new source
    file, and use listing 19.1 to import the relevant libraries. You’re about to create
    a neural network to learn a utility function based on pairs of preferences.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个能够进行排名的自己的神经网络。打开一个新的源文件，并使用列表19.1导入相关库。你即将创建一个神经网络，根据偏好对来学习效用函数。
- en: Listing 19.1 Importing relevant libraries
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.1 导入相关库
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To learn a neural network for ranking states based on a utility score, you’ll
    need training data. Let’s create dummy data to begin with; you’ll replace it with
    something more realistic later. Reproduce the 2D data in figure 19.3 by using
    listing 19.2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习基于效用分数对状态进行排名的神经网络，你需要训练数据。让我们先创建一些虚拟数据；你将在稍后用更真实的数据替换它。通过使用列表19.2重现图19.3中的2D数据。
- en: '![CH19_F03_Mattmann2](../Images/CH19_F03_Mattmann2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F03_Mattmann2](../Images/CH19_F03_Mattmann2.png)'
- en: Figure 19.3 Example data that you’ll work with. The circles represent more-favorable
    states, whereas the crosses represent less-favorable states. You have an equal
    number of circles and crosses because the data comes in pairs; each pair is a
    ranking, as in figure 19.2.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.3 你将工作的示例数据。圆圈代表更受欢迎的状态，而十字代表不太受欢迎的状态。由于数据成对出现，因此圆圈和十字的数量相等；每一对都是一个排名，如图19.2所示。
- en: Listing 19.2 Generating dummy training data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.2 生成虚拟训练数据
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ You’ll generate 2D data so that you can visualize it easily.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你将生成2D数据，这样你可以轻松地可视化它。
- en: ❷ The set of points that should yield higher utility
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 应该产生更高效用值的点的集合
- en: ❸ The set of points that are less preferred
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 不太受欢迎的点的集合
- en: Next, you need to define hyperparameters. In this model, let’s stay simple by
    keeping the architecture shallow. You’ll create a network with one hidden layer.
    The corresponding hyperparameter that dictates the hidden layer’s number of neurons
    is
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要定义超参数。在这个模型中，让我们保持简单，通过保持架构浅层。你将创建一个包含一个隐藏层的网络。决定隐藏层神经元数量的相应超参数是
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The ranking neural network will receive pairwise input, so you’ll need to have
    two separate placeholders—one for each part of the pair. Moreover, you’ll create
    a placeholder to hold the `dropout` parameter value. Continue by adding listing
    19.3 to your script.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 排名神经网络将接收成对输入，因此你需要有两个单独的占位符——每个部分一个。此外，你将创建一个占位符来保存`dropout`参数值。继续通过将列表19.3添加到你的脚本中。
- en: Listing 19.3 Placeholders
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.3 占位符
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Input placeholder for preferred points
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 受欢迎点的输入占位符
- en: ❷ Input placeholder for nonpreferred points
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不受欢迎点的输入占位符
- en: The ranking neural network will contain only one hidden layer. In listing 19.4,
    you define the weights and biases, and then reuse these weights and biases on
    each of the two input placeholders.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 排名神经网络将只包含一个隐藏层。在列表19.4中，你定义了权重和偏差，然后在这些两个输入占位符的每个上重用这些权重和偏差。
- en: Listing 19.4 Hidden layer
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.4 隐藏层
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The goal of the neural network is to calculate a score for the two inputs provided.
    In listing 19.5, you define the weights, biases, and fully connected architecture
    of the output layer of the network. You’ll be left with two output vectors: `s1`
    and `s2`, representing the scores for the pairwise input.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的目标是计算两个输入的分数。在列表19.5中，你定义了网络的输出层的权重、偏差和全连接架构。你将剩下两个输出向量：`s1`和`s2`，代表成对输入的分数。
- en: Listing 19.5 Output layer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.5 输出层
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Utility score of input x1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入x1的效用分数
- en: ❷ Utility score of input x2
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入x2的效用分数
- en: You’ll assume that when training the neural network, `x1` should contain the
    less-favorable items. `s1` should be scored lower than `s2`, so the difference
    between `s1` and `s2` should be negative. As listing 19.6 shows, the `loss` function
    tries to guarantee a negative difference by using the softmax cross-entropy loss.
    You’ll define a `train_op` to minimize the `loss` function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你将假设在训练神经网络时，`x1`应包含不太受欢迎的项目。`s1`应得分低于`s2`，因此`s1`和`s2`之间的差异应该是负数。正如列表19.6所示，`loss`函数通过使用softmax交叉熵损失来尝试保证负数差异。你将定义一个`train_op`来最小化`loss`函数。
- en: Listing 19.6 Loss and optimizer
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.6 损失和优化器
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now follow listing 19.7 to set up a TensorFlow session, which involves initializing
    all variables and preparing TensorBoard debugging by using a summary writer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按照列表19.7设置TensorFlow会话，这涉及到初始化所有变量，并使用摘要编写器准备TensorBoard调试。
- en: Note You used a summary writer at the end of chapter 2, when you were introduced
    to TensorBoard.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你在第2章的结尾使用了摘要编写器，当时你被介绍到了TensorBoard。
- en: Listing 19.7 Preparing a session
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.7 准备会话
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You’re ready to train the network! Run `train_op` on the dummy data you generated
    to learn the parameters of the model (listing 19.8).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好训练网络了！在生成的虚拟数据上运行`train_op`来学习模型的参数（列表19.8）。
- en: Listing 19.8 Training the network
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.8 训练网络
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Training dropout keep_prob is 0.5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练dropout keep_prob为0.5。
- en: ❷ Preferred points
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 受欢迎的点
- en: ❸ Nonpreferred points
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 不受欢迎的点
- en: ❹ Testing dropout keep_prob should always be 1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 测试dropout的keep_prob应该始终为1。
- en: Finally, visualize the learned score function. As shown in listing 19.9, append
    2D points to a list.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可视化学习到的得分函数。如图19.9所示，将二维点添加到列表中。
- en: Listing 19.9 Preparing test data
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.9 准备测试数据
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Loops through the rows
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历行
- en: ❷ Loops through the columns
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历列
- en: You’ll run the `s1` op on the test data to obtain the utility values of each
    state and visualize this data as shown in figure 19.4\. Use listing 19.10 to generate
    the visualization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在测试数据上运行`s1`操作以获得每个状态的效用值，并如图19.4所示可视化这些数据。使用列表19.10生成可视化。
- en: '![CH19_F04_Mattmann2](../Images/CH19_F04_Mattmann2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F04_Mattmann2](../Images/CH19_F04_Mattmann2.png)'
- en: Figure 19.4 The landscape of scores learned by the ranking neural network
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.4 排名神经网络学习到的得分景观
- en: Listing 19.10 Visualizing results
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.10 可视化结果
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Computes the utility of all the points
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算所有点的效用
- en: ❷ Reshapes the utilities to a matrix so you can visualize an image by using
    Matplotlib
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将效用重塑为矩阵，以便您可以使用Matplotlib可视化图像
- en: 19.2 Image embedding
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.2 图像嵌入
- en: In chapter 18, you summoned the hubris to feed a neural network some natural
    language sentences. You did so by converting words or letters in a sentence to
    numeric forms, such as vectors. Each symbol (whether it was a word or letter)
    was embedded in a vector by means of a lookup table.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第18章中，你大胆地给神经网络喂了一些自然语言句子。你是通过将句子中的单词或字母转换为数值形式，如向量来做到这一点的。每个符号（无论是单词还是字母）都通过查找表嵌入到向量中。
- en: Exercise 19.2
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 练习19.2
- en: Why is a lookup table that converts a symbol to a vector representation called
    an embedding matrix?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将符号转换为向量表示的查找表称为嵌入矩阵？
- en: '**Answer**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: The symbols are being embedded in a vector space.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 符号正在被嵌入到向量空间中。
- en: 'Fortunately, images are already in numeric form, represented as a matrix of
    pixels. If the image is grayscale, perhaps the pixels take on scalar values indicating
    luminosity. For colored images, each pixel represents color intensities (usually
    three: red, green, and blue). Either way, an image can easily be represented by
    numeric data structures, such as a tensor, in TensorFlow.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，图像已经是数值形式，表示为像素矩阵。如果图像是灰度的，像素可能代表亮度值的标量。对于彩色图像，每个像素代表颜色强度（通常是三个：红色、绿色和蓝色）。无论如何，图像都可以通过数值数据结构，如TensorFlow中的张量，轻松表示。
- en: Exercise 19.3
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 练习19.3
- en: Take a photo of a household object, such as a chair. Scale the image smaller
    and smaller until you can no longer identify the object. By what factor did you
    end up shrinking the image? What’s the ratio of the number of pixels in the original
    image to the number of pixels in the smaller image? This ratio is a rough measure
    of redundancy in the data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 拍摄一个家庭用品的照片，比如一把椅子。将图像缩小，直到你不能再识别物体。你最终缩小了多少倍？原始图像中的像素数与较小图像中的像素数之比是多少？这个比率是数据冗余的粗略度量。
- en: '**Answer**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: A typical 5 MP camera produces images at a resolution of 2560 × 1920, yet the
    content of that image might still be decipherable when you shrink it by a factor
    of 40 (resolution 64 × 48).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一台典型的500万像素相机以2560 × 1920的分辨率生成图像，但当你将其缩小40倍（分辨率64 × 48）时，图像的内容可能仍然可以辨认。
- en: Feeding a neural network a large image—say, of size 1280 × 720 (almost 1 million
    pixels)—increases the number of parameters and, consequently, escalates the risk
    of overfitting the model. The pixels in an image are highly redundant, so you
    can try to capture the essence of an image in a more-succinct representation.
    Figure 19.5 shows the clusters formed in a 2D embedding of images of clothes being
    folded.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 向神经网络输入一个大图像——比如说，大小为1280 × 720（近100万个像素）——会增加参数的数量，从而增加模型过拟合的风险。图像中的像素高度冗余，因此您可以尝试以更简洁的表示捕捉图像的精髓。图19.5显示了折叠衣物图像的2D嵌入中形成的簇。
- en: '![CH19_F05_Mattmann2ver2](../Images/CH19_F05_Mattmann2ver2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F05_Mattmann2ver2](../Images/CH19_F05_Mattmann2ver2.png)'
- en: Figure 19.5 Images can be embedded in much lower dimensions, such as 2D (shown
    here). Notice that points representing similar states of a shirt occur in nearby
    clusters. Embedding images allows you to use the ranking neural network to learn
    a preference between the states of a piece of cloth.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.5 图像可以嵌入到更低的维度，例如2D（如图所示）。注意，表示衬衫相似状态的点出现在附近的簇中。嵌入图像允许您使用排名神经网络学习布料状态的偏好。
- en: You saw in chapters 11 and 12 how to use autoencoders to reduce the dimensionality
    of images. Another common way to accomplish low-dimensional embedding of images
    is to use the penultimate layer of a deep convolutional neural network image classifier.
    Let’s explore the latter in more detail.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你在11章和12章中看到了如何使用自编码器来降低图像的维度。另一种常见的实现图像低维嵌入的方法是使用深度卷积神经网络图像分类器的倒数第二层。让我们更详细地探讨后者。
- en: Because designing, implementing, and learning a deep image classifier isn’t
    the primary focus of this chapter (see chapters 14 and 15 for CNNs), you’ll instead
    use an off-the-shelf pretrained model. A common go-to image classifier that many
    computer-vision research papers cite is VGG16; *you used it in chapter 15 to build
    a facial recognition system.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因为设计、实现和学习深度图像分类器不是本章的主要焦点（有关CNNs，请参阅第14章和第15章），所以你将使用现成的预训练模型。许多计算机视觉研究论文中引用的一个常见图像分类器是VGG16；*你在第15章中使用它构建了人脸识别系统*。
- en: Many online implementations of VGG16 exist for TensorFlow. I recommend using
    the one by Davi Frossard ([https://www.cs.toronto.edu/~frossard/post/vgg16](https://www.cs.toronto.edu/~frossard/post/vgg16/)).
    You can download the vgg16.py TensorFlow code and the vgg16_weights.npz pretrained
    model parameters from his website.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TensorFlow，VGG16的许多在线实现都存在。我推荐使用Davi Frossard的版本（[https://www.cs.toronto.edu/~frossard/post/vgg16](https://www.cs.toronto.edu/~frossard/post/vgg16/)）。你可以从他的网站下载`vgg16.py`
    TensorFlow代码和`vgg16_weights.npz`预训练模型参数。
- en: Figure 19.6 is a depiction of the VGG16 neural network from Frossard’s page.
    As you see, it’s a deep neural network, with many convolutional layers. The last
    few are the usual fully connected layers, and the output layer is a 1000D vector
    indicating the multiclass classification probabilities.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.6展示了Frossard页面上的VGG16神经网络。正如你所见，它是一个深度神经网络，包含许多卷积层。最后几层是常规的完全连接层，输出层是一个1000D向量，表示多类分类的概率。
- en: '![CH19_F06_Mattmann2](../Images/CH19_F06_Mattmann2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F06_Mattmann2](../Images/CH19_F06_Mattmann2.png)'
- en: Figure 19.6 The VGG16 architecture is a deep convolutional neural network used
    for classifying images. This particular diagram is from [https://www.cs.toronto.edu/
    ~frossard/post/vgg16.](https://www.cs.toronto.edu/~frossard/post/vgg16/)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.6 VGG16架构是一个用于图像分类的深度卷积神经网络。这个特定的图表来自[https://www.cs.toronto.edu/~frossard/post/vgg16.](https://www.cs.toronto.edu/~frossard/post/vgg16/)。
- en: Learning how to navigate other people’s code is an indispensable skill. First,
    make sure you have vgg16.py and vgg16_weights.npz downloaded, and test that you’re
    able to run the code by using python vgg16.py my_image.png.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何导航他人的代码是一项不可或缺的技能。首先，确保你已经下载了`vgg16.py`和`vgg16_weights.npz`，并通过使用`python
    vgg16.py my_image.png`测试你能否运行代码。
- en: note You may need to install SciPy and Pillow to get the VGG16 demo code to
    run without issues. You can download both via pip.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能需要安装SciPy和Pillow来确保VGG16演示代码无问题地运行。你可以通过pip下载这两个库。
- en: 'Let’s start by adding TensorBoard integration to visualize what’s going on
    in this code. In the main function, after creating a session variable `sess`,
    insert the following line of code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们添加TensorBoard集成来可视化代码中的情况。在主函数中，在创建会话变量`sess`之后，插入以下代码行：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now running the classifier once again (python vgg16.py my_image.png) will generate
    a directory called tb_files, to be used by TensorBoard. You can run TensorBoard
    to visualize the computation graph of the neural network. The following command
    runs TensorBoard:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在再次运行分类器（`python vgg16.py my_image.png`）将生成一个名为`tb_files`的目录，该目录将由TensorBoard使用。你可以运行TensorBoard来可视化神经网络的计算图。以下命令运行TensorBoard：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Open TensorBoard in your browser, and navigate to the Graphs tab to see the
    computation graph, shown in figure 19.7\. At a glance, you can get an idea of
    the types of layers involved in the network; the last three layers are fully connected
    dense layers labeled fc1, fc2, and fc3.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开TensorBoard，导航到“图形”选项卡以查看计算图，如图19.7所示。一眼望去，你可以了解网络中涉及的层类型；最后三层是标记为fc1、fc2和fc3的完全连接密集层。
- en: '![CH19_F07_Mattmann2](../Images/CH19_F07_Mattmann2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F07_Mattmann2](../Images/CH19_F07_Mattmann2.png)'
- en: Figure 19.7 A small segment of the computation graph shown in TensorBoard for
    the VGG16 neural network. The topmost node is the softmax operator used for classification.
    The three fully connected layers are labeled fc1, fc2, and fc3.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.7展示了TensorBoard中VGG16神经网络的计算图的一个小片段。最顶部的节点是用于分类的softmax运算符。三个完全连接的层分别标记为fc1、fc2和fc3。
- en: 19.3 Ranking images
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3 图像排名
- en: You’ll use the VGG16 code in section 19.2 to obtain a vector representation
    of an image. That way, you can rank two images efficiently in the ranking neural
    network you designed in section 12.1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用第19.2节中的VGG16代码来获取图像的向量表示。这样，你可以在第12.1节中设计的排名神经网络中有效地对两个图像进行排名。
- en: Consider videos of shirt-folding, as shown in figure 19.8\. You’ll process videos
    frame by frame to rank the states of the images. That way, in a novel situation,
    the algorithm can understand whether the goal of cloth-folding has been reached.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图19.8所示的衬衫折叠视频。你将逐帧处理视频以对图像的状态进行排名。这样，在新的情况下，算法可以理解布折叠的目标是否已经达到。
- en: '![CH19_F08_Mattmann2](../Images/CH19_F08_Mattmann2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F08_Mattmann2](../Images/CH19_F08_Mattmann2.png)'
- en: Figure 19.8 Videos of folding a shirt reveal how the cloth changes form through
    time. You can extract the first state and the last state of the shirt as your
    training data to learn a utility function to rank states. Final states of a shirt
    in each video should be ranked with a higher utility than the one used for those
    shirts near the beginning of the video.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.8 衬衫折叠的视频展示了布料随时间如何改变形状。你可以将衬衫的第一个状态和最后一个状态作为你的训练数据来学习一个用于排名的状态效用函数。每个视频中的衬衫最终状态应该比视频开头附近的衬衫具有更高的效用。
- en: First, download the cloth-folding dataset from [http://mng.bz/eZsc](http://mng.bz/eZsc).
    Extract the zip file. Keep note of where you extract file; you’ll call that location
    `DATASET_DIR` in the code listings.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[http://mng.bz/eZsc](http://mng.bz/eZsc)下载布折叠数据集。解压zip文件。注意你解压文件的位置；在代码列表中，你将称该位置为`DATASET_DIR`。
- en: Open a new source file, and import the relevant libraries in Python (listing
    19.11).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新的源文件，并在Python中导入相关库（列表19.11）。
- en: Listing 19.11 Importing libraries
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.11 导入库
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For each video, you’ll remember the first and last images. That way, you can
    train the ranking algorithm by assuming that the last image is of a higher preference
    than the first image. In other words, the last state of cloth-folding brings you
    to a higher-valued state than the first state of cloth-folding. Listing 19.12
    shows how to load the data into memory.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个视频，你将记住第一个和最后一个图像。这样，你可以通过假设最后一个图像比第一个图像更受偏好来训练排名算法。换句话说，布折叠的最后一个状态比布折叠的第一个状态带给你更高的价值状态。列表19.12显示了如何将数据加载到内存中。
- en: Listing 19.12 Preparing the training data
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.12 准备训练数据
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Directory of downloaded files
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载文件目录
- en: ❷ Number of videos to load
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要加载的视频数量
- en: ❸ Gets the starting and ending image of a video
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取视频的开始和结束图像
- en: 'Running listing 19.12 results in the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表19.12的结果如下：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Use listing 19.13 to create an input placeholder for the image that you’ll be
    embedding.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表19.13创建一个用于嵌入的图像输入占位符。
- en: Listing 19.13 Placeholder
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.13 占位符
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Copy over the ranking neural network code from listings 19.3-19.7; you’ll reuse
    it to rank images. Then prepare the session in listing 19.14.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表19.3-19.7中的排名神经网络代码复制过来；你将重用它来排名图像。然后准备列表19.14中的会话。
- en: Listing 19.14 Preparing the session
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.14 准备会话
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, you’ll initialize the VGG16 model by calling the constructor. Doing so,
    as shown in listing 19.15, loads all the model parameters from disk to memory.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将通过调用构造函数来初始化VGG16模型。这样做，如列表19.15所示，将从磁盘将所有模型参数加载到内存中。
- en: Listing 19.15 Loading the VGG16 model
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.15 加载VGG16模型
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, prepare training and testing data for the ranking neural network. As shown
    in listing 19.16, you’ll feed the VGG16 model your images; then you’ll access
    a layer near the output (in this case, `fc1`) to obtain the image embedding.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为排名神经网络准备训练和测试数据。如列表19.16所示，你将向VGG16模型提供你的图像；然后你将访问输出附近的一层（在这种情况下，`fc1`）以获取图像嵌入。
- en: 'In the end, you’ll have a 4096D embedding of your images. Because you have
    a total of 45 videos, you’ll split them, using some for training and some for
    testing:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将得到一个4096D的图像嵌入。因为你总共有45个视频，所以你需要将它们分割，一些用于训练，一些用于测试：
- en: Train
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练
- en: 'Start-frame size: (33, 4096)'
  id: totrans-140
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始帧大小：(33, 4096)
- en: 'End-frame size: (33, 4096)'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束帧大小：(33, 4096)
- en: Test
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: 'Start-frame size: (12, 4096)'
  id: totrans-143
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始帧大小：(12, 4096)
- en: 'End-frame size: (12, 4096)'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束帧大小：(12, 4096)
- en: Listing 19.16 Preparing data for ranking
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.16 准备排名数据
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With your training data ready for ranking, run `train_op` an `epoch` number
    of times (listing 19.17). After training the network, run the model on the test
    data to evaluate your results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的排名训练数据准备好后，运行`train_op`一个`epoch`次数（列表19.17）。在训练网络后，在测试数据上运行模型以评估你的结果。
- en: Listing 19.17 Training the ranking network
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19.17 训练排名网络
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that accuracy approaches 100% over time. Your ranking model learns that
    the images that occur at the end of the video are more favorable than the images
    that occur near the beginning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着时间的推移，准确性接近 100%。你的排名模型学习到视频末尾出现的图像比接近视频开头出现的图像更有利。
- en: Out of curiosity, let’s see the utility over time of a single video, frame by
    frame, as shown in figure 19.9\. The code to reproduce figure 19.9 requires loading
    all the images in a video, as outlined in listing 19.18.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们看看单个视频随时间变化的效用，如图 19.9 所示，逐帧展示。重现图 19.9 的代码需要加载视频中的所有图像，如列表 19.18 所述。
- en: '![CH19_F09_Mattmann2](../Images/CH19_F09_Mattmann2.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![CH19_F09_Mattmann2](../Images/CH19_F09_Mattmann2.png)'
- en: Figure 19.9 The utility increases over time, indi-cating that the goal is being
    accomplished. The utility of the cloth near the beginning of the video is near
    0, but it increases dramatically—to 120,000 units—by the end.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.9 随着时间的推移，效用增加，表明目标正在实现。视频开头附近的布料效用接近 0，但增加到 120,000 单位。
- en: Listing 19.18 Preparing image sequences from video
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19.18 从视频中准备图像序列
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can use your VGG16 model to embed the images and then run the ranking network
    to compute the scores, as shown in listing 19.19.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你的 VGG16 模型嵌入图像，然后运行排名网络来计算分数，如列表 19.19 所示。
- en: Listing 19.19 Computing the utility of images
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19.19 计算图像的效用
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Visualize your results to reproduce figure 19.9 (listing 19.20).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化你的结果以重现图 19.9（列表 19.20）。
- en: Listing 19.20 Visualizing utility scores
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19.20 可视化效用分数
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can rank states by representing objects as vectors and learning a utility
    function over such vectors.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过将对象表示为向量并学习这样的向量上的效用函数来对状态进行排名。
- en: Because images contain redundant data, you used the VGG16 neural network to
    reduce the dimensionality of your data so that you can use the ranking network
    with real-world images.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于图像包含冗余数据，你使用了 VGG16 神经网络来降低数据的维度，以便可以使用真实世界图像的排名网络。
- en: You learned how to visualize the utility of images over time in a video to verify
    that the video demonstration increases the utility of the cloth.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你学习了如何在视频中可视化图像随时间的变化，以验证视频演示增加了布料的效用。
- en: What’s next
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来是什么
- en: 'You’ve finished your TensorFlow journey! The 19 chapters of this book approached
    machine learning from different angles, but together, they taught you the concepts
    required to master these skills:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了你的 TensorFlow 之旅！本书的 19 章从不同的角度探讨了机器学习，但它们共同教会了你掌握这些技能所需的概念：
- en: Formulating an arbitrary real-world problem into a machine-learning framework
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任意真实世界问题表述为机器学习框架
- en: Understanding the basics of many machine-learning problems
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解许多机器学习问题的基本原理
- en: Using TensorFlow to solve these problems
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 解决这些问题
- en: Visualizing a machine-learning algorithm and speaking the lingo
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化机器学习算法并使用专业术语
- en: Using real-world data and problems to show off what you learned
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实世界的数据和问题来展示你所学的知识
- en: Because the concepts taught in this book are timeless, the code listings should
    be too. To ensure the most up-to-date library calls and syntax, I actively manage
    a GitHub repository at [http://mng.bz/Yx5A](http://mng.bz/Yx5A). Please feel free
    to join the community there and file bugs or send me pull requests.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中教授的概念是永恒的，代码列表也应该是。为确保使用最新的库调用和语法，我积极管理一个位于 [http://mng.bz/Yx5A](http://mng.bz/Yx5A)
    的 GitHub 仓库。请随时加入那里的社区，报告错误或发送拉取请求。
- en: Tip TensorFlow is in a state of rapid development, so more functionality will
    become available all the time.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 TensorFlow 正处于快速发展阶段，因此更多功能将不断可用。
