- en: 10 Attention mechanisms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 注意力机制
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding attention mechanisms and when to use them
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解注意力机制及其使用时机
- en: Adding context to an attention mechanism for context-sensitive results
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为注意力机制添加上下文以实现上下文敏感的结果
- en: Handling variable-length items with attention
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意力处理可变长度项目
- en: Imagine having a conversation with a couple of friends at a busy coffee shop.
    Around you are other conversations and people placing orders and talking on their
    cell phones. Despite all this noise, you, with your complex and sophisticated
    brain and ears, can pay *attention* to *only what is important* (your friends!),
    and selectively ignore the things occurring around you that are not relevant.
    The important thing here is that your *attention is adaptive* to the situation.
    You *ignore* the background sounds to listen to your friends only when there is
    nothing more important happening. If a fire alarm goes off, you stop paying attention
    to your friends and focus your attention on this new, important sound. Thus, attention
    is about adapting to the relative importance of inputs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在繁忙的咖啡馆和几个朋友聊天。周围有其他对话和人们下单以及用手机交谈。尽管有所有这些噪音，但你，凭借你复杂而精致的大脑和耳朵，可以只关注*重要的东西*（你的朋友！），并选择性地忽略你周围发生的、不相关的事情。这里重要的是你的*注意力是适应性的*。只有当没有更重要的事情发生时，你才会忽略背景声音，只听你的朋友。如果响起火灾警报，你会停止关注你的朋友，将注意力集中在这个新的、重要的事情上。因此，注意力是关于适应输入的相对重要性。
- en: Deep learning models can also learn to pay attention to some input, or features,
    and ignore other features. They do this with *attention mechanisms*, which are
    another type of prior belief we can impose on our network. Attention mechanisms
    help us deal with situations where part of our input may be irrelevant, or when
    we need to focus on one feature of many features being fed into the model. For
    example, if you are translating a book from English to French, you do not need
    to understand the entire book to translate the first sentence. Each word you should
    output in the English translation will depend on only a few nearby words in the
    same sentence, and you can ignore most of the surrounding French sentences and
    content.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型也可以学习关注某些输入或特征，而忽略其他特征。它们通过*注意力机制*来完成这项任务，这是我们可以强加给网络的另一种先验信念。注意力机制帮助我们处理输入中可能部分无关的情况，或者当我们需要关注模型中输入的多个特征中的一个时。例如，如果你正在将一本书从英语翻译成法语，你不需要理解整本书就能翻译第一句话。你将在英语翻译中输出的每个单词将仅取决于同一句子中附近的一两个单词，你可以忽略大多数周围的法语句子和内容。
- en: We would like to endow our networks with the ability to ignore superfluous and
    distracting inputs to focus on the most important portions, and that’s the goal
    of an attention mechanism. If you believe some of your input features are more
    or less important than other features, you should consider using an attention-based
    approach in your model. If you want state-of-the-art results on speech recognition,
    object detection, a chat bot, or machine translation, for instance, you will probably
    be using an attention mechanism.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望赋予我们的网络忽略多余和分散注意力的输入，专注于最重要的部分的能力，这就是注意力机制的目标。如果你认为你的某些输入特征相对于其他特征更重要或更不重要，你应该考虑在你的模型中使用基于注意力的方法。例如，如果你想实现语音识别、目标检测、聊天机器人或机器翻译等领域的最先进结果，你可能会使用注意力机制。
- en: In this chapter, we see how attention works on some toy problems so that in
    the next chapter, we can build something far more sophisticated. First, we create
    a toy problem out of the MNIST dataset that is too hard for a normal network but
    is easily and better solved with a simple kind of attention that learns how to
    score the *importance* of each item in the input. Then we improve the simple attention
    into a full-fledged approach that takes into account some *context* to better
    infer the importance of items in the input. Doing so will also allow us to make
    the attention mechanism work with variable-length data so we can work on padded
    data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过一些玩具问题来了解注意力机制的工作原理，以便在下一章中，我们可以构建更加复杂和高级的东西。首先，我们从MNIST数据集中创建一个玩具问题，这个问题对于一个普通网络来说太难解决，但通过一种简单的注意力机制可以轻松且更好地解决，这种机制学会了如何评估输入中每个项目的*重要性*。然后，我们将简单的注意力机制改进为一个完整的方案，该方案考虑了一些*上下文*，以更好地推断输入中项目的重要性。这样做也将使我们能够使注意力机制适用于可变长度的数据，这样我们就可以处理填充数据。
- en: 10.1 Attention mechanisms learn relative input importance
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 注意力机制学习相对输入的重要性
- en: 'Now that we’ve talked about the intuition behind attention, let’s create a
    toy dataset. We will modify the MNIST dataset to create a new kind of task, so
    let’s quickly load that up:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了注意力的直觉，让我们创建一个玩具数据集。我们将修改MNIST数据集以创建一种新的任务，所以让我们快速加载它：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Attention mechanisms are most useful when we have *multiple items* as inputs
    into our model. Since MNIST is a single digit, we will augment each item in MNIST
    to become a bag of digits. We use fully connected layers for this (i.e., flattened
    MNIST, ignoring the image nature), so instead of having a batch of digits (*B*,*D*),
    we have T digits (*B*,*T*,*D*). So why did I call this a *bag* instead of a sequence?
    Because we don’t care in what order the digits are presented in the tensor. We
    just need a tensor large enough to hold everything in the bag.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在我们有 *多个项目* 作为模型输入时最有用。由于MNIST是一个单个数字，我们将MNIST中的每个项目增强为数字袋子。我们为此使用全连接层（即平展的MNIST，忽略图像性质），因此我们不再有一个数字批（*B*，*D*），而是一个
    T 个数字（*B*，*T*，*D*）。那么我为什么叫它*袋子*而不是序列呢？因为我们不关心数字在张量中的呈现顺序。我们只需要一个足够大的张量来容纳袋子中的所有内容。
- en: 'Given a bag of digits **x**[1], **x**[2], …, **x**[T], we have a label y that
    is equal to the *largest* digit in the bag. If our bag contains 0, 2, 9, the label
    for the bag is “9.” The following code implements a `LargestDigit` class to wrap
    an input dataset and create new items by randomly filling a bag of `toSample`
    items and selecting the maximum label value:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含数字 **x**[1]，**x**[2]，…，**x**[T] 的数字袋子，我们有一个标签 y，它等于袋子中的*最大*数字。如果我们的袋子包含
    0，2，9，那么这个袋子的标签就是“9。”以下代码实现了一个 `LargestDigit` 类来包装输入数据集，并通过随机填充一个包含 `toSample`
    个项目的袋子并选择最大标签值来创建新项目：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Randomly selects n=self.toSample items from the dataset
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从数据集中随机选择 n=self.toSample 个项目
- en: ❷ Stacks the n items of shape (B, *) into (B, n, *)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将形状为 (B, *) 的 n 个项目堆叠成 (B, n, *)
- en: ❸ Label is the maximum label.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 标签是最大标签。
- en: ❹ Returns (data, label) pair!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回 (data, label) 对！
- en: Note Why not call this a *set* of items? A set implies that no repetitions are
    allowed, whereas a bag allows repetitions to occur. This matches the behavior
    of the Python `set` class, where duplicate items are automatically removed from
    the set. The type of problem we are creating is similar to a niche research area
    called multi-instance learning,[¹](#fn37) if you want to learn about other types
    of models that work on bags of data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为什么不说这是一个*集合*呢？集合意味着不允许重复，而一个袋子允许重复发生。这符合Python `set`类的行为，其中重复的项会自动从集合中移除。我们正在创建的问题类型类似于一个名为多实例学习的研究领域，[¹](#fn37)，如果你想了解其他在数据袋上工作的模型类型。
- en: This is a much harder version of the MNIST dataset for a model to learn from.
    Given a bag with a label, the model has to infer, on its own, which item in the
    input is the largest, use that information to slowly learn to recognize all 10
    digits, also learn that the digits are ordered, and return the largest digit in
    a bag.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对模型来说更难的一个版本，需要从MNIST数据集中学习。给定一个带有标签的袋子，模型必须自行推断输入中的哪个项目是最大的，使用这个信息来逐渐学会识别所有10个数字，并学会这些数字是有序的，然后返回袋子中的最大数字。
- en: 10.1.1  Training our baseline model
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1  训练我们的基线模型
- en: 'The following block of code sets up our training/testing loaders and uses a
    batch size of *B* = 128 items and 10 epochs of training:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块设置了我们的训练/测试加载器，并使用批大小 *B* = 128 个项目和 10 个训练周期：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we plot an item from the dataset, we should see the modified dataset correspond
    to what we have described. The following code samples a random item from the dataset
    and gets the digits 8, 2, and 6\. The “8” is the largest label, so 8 is the correct
    answer. In this case, the digits 2 and 6 *do not matter* because 2 < 8 and 6 <
    8\. They could have been any digit less than 8, in any order, and the results
    would not change. We want our model to learn to *ignore* the smaller digits:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制数据集中的一个项目，我们应该看到修改后的数据集与我们描述的相匹配。以下代码从数据集中随机抽取一个样本并获取数字 8，2 和 6\. “8”
    是最大的标签，所以 8 是正确答案。在这种情况下，数字 2 和 6 *并不重要*，因为 2 < 8 和 6 < 8\. 它们可以是任何小于 8 的数字，以任何顺序排列，结果都不会改变。我们希望我们的模型学会*忽略*较小的数字：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/CH10_UN01_Raff.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN01_Raff.png)'
- en: 'Now that we have this toy problem, let’s train a simple fully connected network
    and treat this like any other classification problem we might attempt. This will
    be our baseline and show how hard the new version of MNIST is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这个玩具问题，让我们训练一个简单的全连接网络，并将其视为我们可能尝试的任何其他分类问题。这将是我们的基线，并展示新版本的MNIST有多难：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ 784*3 because there are 784 pixels in an image and 3 images in the bag
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 784*3，因为一个图像中有784个像素，捆绑中有3个图像
- en: 'We have trained up the model and can plot the results. Since this is MNIST,
    we can get 98% accuracy without too much effort using just fully connected layers.
    But this bagged dataset results in a fully connected network barely reaching 92%,
    even with our fanciest tricks like LeakyReLUs and batch normalization:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练好了模型，可以绘制结果。由于这是MNIST，我们只需使用全连接层就可以轻松地获得98%的准确率。但是，这个捆绑数据集导致全连接网络几乎只能达到92%的准确率，即使我们使用了像LeakyReLUs和批量归一化这样的花哨技巧：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/CH10_UN02_Raff.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN02_Raff.png)'
- en: 10.1.2  Attention mechanism mechanics
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2  注意力机制机制
- en: Next, we’ll design a very simple kind of attention mechanism that takes a step
    toward the more complete attention we learn about later in this chapter. The primary
    tool we use is the softmax function *s**m*(*x*) from chapter 2\. Remember that
    once we compute **p** = *s**m*(**x**), p represents a probability distribution.
    A probability distribution has all values greater than or equal to zero, and the
    sum of all the values is one. The mathy way to say this is 0 ≤ **p**[i] ≤ 1 and
    ∑[i]**p**[i] = 1. Remember that a primary function of attention is to ignore parts
    of the input, and we do that by multiplying the parts to ignore with small values
    at or near zero. If we multiply something by zero, we get zero, effectively erasing
    it from the input. Thanks to the softmax computation, we can learn to multiply
    inputs by small values at or near 0, effectively learning how to ignore them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设计一种非常简单的注意力机制，它将朝着我们在本章后面学习到的更完整的注意力机制迈出一步。我们使用的主要工具是第二章中的softmax函数
    *s**m*(*x*)。记住，一旦我们计算出 **p** = *s**m*(**x**)，p 就代表一个概率分布。概率分布的所有值都大于或等于零，所有值的总和为1。用数学方式表达就是
    0 ≤ **p**[i] ≤ 1 和 ∑[i]**p**[i] = 1。记住，注意力机制的主要功能是忽略输入的一部分，我们通过将忽略的部分与接近零的小值相乘来实现这一点。如果我们把某物乘以零，我们得到零，实际上是从输入中删除它。多亏了softmax计算，我们可以学习如何将输入乘以接近零的小值，从而有效地学习如何忽略它们。
- en: 'Figure 10.1 shows the three primary steps of an attention mechanism. The first
    two steps are trivial and will change depending on your network and problem: simply
    get your features into your network and have some initial hidden layers before
    the attention mechanism is applied so that they can learn a useful representation.
    Then the final steps occur that perform the attention:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1展示了注意力机制的三步主要步骤。前两个步骤是微不足道的，并且将根据你的网络和问题而变化：简单地将你的特征输入到网络中，并在应用注意力机制之前有一些初始隐藏层，这样它们就可以学习到一个有用的表示。然后进行最后的步骤，执行注意力操作：
- en: Assign a score to every input **x**[t] using a *score* function.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *得分* 函数为每个输入 **x**[t] 分配一个得分。
- en: Compute a softmax over all scores.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有得分计算softmax。
- en: Multiply each item by its softmax score, and then add all the results together
    into an output x̄.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个项目乘以其softmax得分，然后将所有结果相加得到输出 x̄。
- en: '![](../Images/CH10_F01_Raff.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F01_Raff.png)'
- en: Figure 10.1 Diagram of how attention mechanisms work. The input sequence **x**[t]
    can be variable in number (i.e., you could have *T* = 1 items, *T* = 13, or *T*=
    any number you like). A *score* function assigns a raw importance value to each
    item t. The softmax function creates relative scores. The output is then the weighted
    average of the inputs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 展示了注意力机制的工作原理图。输入序列 **x**[t] 的数量可以是可变的（即，你可以有 *T* = 1 个项目，*T* = 13，或者
    *T* = 你喜欢的任何数量）。一个 *得分* 函数为每个项目 t 分配一个原始重要性值。softmax函数创建相对得分。输出然后是输入的加权平均值。
- en: Let’s restate some of this using a more mathematical notation to fill in some
    details. The way attention mechanisms usually work is that we represent the input
    as T different components **x**[1], **x**[2], …, **x**[T], each of which has a
    tensor representation **x**[t] ∈ ℝ^D. These T items could be natural breaks in
    the input (e.g., we naturally have different images in this toy problem), or they
    could be forced (e.g., we could split a single image into sub-images). Each input
    **x**[t] is transformed into a new tensor **h**[t] = *F*(**x**[t]), where *F*(⋅)
    is a neural network. This means the inputs to the attention do not have to be
    the first inputs into the model but can be the result after some computation that
    has already been done.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用更数学的符号重新表述一些内容，以填补一些细节。注意力机制通常的工作方式是，我们将输入表示为 T 个不同的组件 **x**[1], **x**[2],
    …, **x**[T]，每个组件都有一个张量表示 **x**[t] ∈ ℝ^D。这 T 个项目可以是输入中的自然断点（例如，在这个玩具问题中我们自然有不同的图像），或者它们可以是强制的（例如，我们可以将单个图像分割成子图像）。每个输入
    **x**[t] 被转换成一个新的张量 **h**[t] = *F*(**x**[t])，其中 *F*(⋅) 是一个神经网络。这意味着注意力机制的输入不必是模型中的第一个输入，但可以是已经完成的一些计算的结果。
- en: Note This also means **x**[t] does not have to be a one-dimensional tensor.
    For example, the network *F*(⋅) could be a CNN and **x**[t] ∈ ℝ^((*C*,*W*,*H*)).
    We are making it 1D right now for simplicity. However, we generally do want **h**[t]
    to be one-dimensional.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这也意味着 **x**[t] 不必是一个一维张量。例如，网络 *F*(⋅) 可能是一个卷积神经网络，**x**[t] ∈ ℝ^((*C*,*W*,*H*)).
    我们现在将其简化为一维是为了方便。然而，我们通常确实希望 **h**[t] 是一维的。
- en: So the processed sequence **h**[1], **h**[2], …, **h**[T] is the actual input
    to the attention mechanism. Next, we need to learn an importance score *α̃*[i]
    for each input **x**[i]. Let’s say a different function *α̃*[i] = score(*F*(**x**[i]))
    learns to compute that. Again, the function score(⋅) is *another* neural network.
    That means our model itself will learn how to score the inputs for us.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，处理后的序列 **h**[1], **h**[2], …, **h**[T] 是注意力机制的真正输入。接下来，我们需要为每个输入 **x**[i]
    学习一个重要性分数 *α̃*[i]。假设一个不同的函数 *α̃*[i] = score(*F*(**x**[i])) 学习计算这个分数。再次，函数 score(⋅)
    是 *另一个* 神经网络。这意味着我们的模型本身将学会如何为我们评分输入。
- en: 'Then we want to normalize the importances into a probability, so we get **α**
    = *α*[1], *α*[2], …, *α*[T] = *s**m*(*α̃*[1],*α̃*[2],…,*α̃*[T]). With these combined,
    we can now compute a weighted average of the representations **h**[t]. Specifically,
    we first compute the softmax scores represented by α:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们想要将重要性归一化为一个概率，所以我们得到 **α** = *α*[1], *α*[2], …, *α*[T] = *s**m*(*α̃*[1],*α̃*[2],…,*α̃*[T])。有了这些组合，我们现在可以计算表示
    **h**[t] 的加权平均。具体来说，我们首先计算由 α 表示的softmax分数：
- en: '![](../Images/ch10-eqs-to-illustrator0x.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ch10-eqs-to-illustrator0x.png)'
- en: 'Next we compute the output of the attention mechanism x̄:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们计算注意力机制的输出 x̄：
- en: '![](../Images/ch10-eqs-to-illustrator1x.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ch10-eqs-to-illustrator1x.png)'
- en: If the jth item **x**[j] is not important, hopefully our network will learn
    to give the item j a value of *α*[j] ≈ 0, in which case it will successfully ignore
    the jth item! This idea is not too complicated, especially compared to some of
    the early items like RNNs that we have learned about. But this simple approach
    has proven to be very powerful and can provide significant improvements to a number
    of problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第 j 个项目 **x**[j] 不重要，希望我们的网络学会将项目 j 的值设置为 *α*[j] ≈ 0，在这种情况下，它将成功地忽略第 j 个项目！这个想法并不太复杂，特别是与一些我们之前学过的早期项目（如RNNs）相比。但这种方法已被证明非常强大，并且可以为许多问题带来显著的改进。
- en: 10.1.3  Implementing a simple attention mechanism
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 实现简单的注意力机制
- en: 'Now that we understand how attention mechanisms work, let’s work on implementing
    a simple one for our bagged MNIST problem. We are not considering the convolutional
    nature of our data yet, so if we want to convert our bag of images (*B*,*T*,*C*,*W*,*H*)
    into a bag of feature vectors (*B*,*T*,*C*⋅*W*⋅*H*), we need a new version of
    the `nn.Flatten` function that leaves the first two axes of our tensor alone.
    This is pretty easy to do with the following `Flatten2` class. It just creates
    a `view` of the input but explicitly uses the first two axes as the start of the
    view and puts all leftover values at the end:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了注意力机制的工作原理，让我们为我们的捆绑MNIST问题实现一个简单的注意力机制。我们还没有考虑我们数据卷积的特性，所以如果我们想将我们的图像捆绑
    (*B*,*T*,*C*,*W*,*H*) 转换为特征向量捆绑 (*B*,*T*,*C*⋅*W*⋅*H*)，我们需要一个新的 `nn.Flatten` 函数版本，它保留张量的前两个轴。这通过以下
    `Flatten2` 类很容易实现。它只是创建输入的一个视图，但明确使用前两个轴作为视图的起始点，并将所有剩余的值放在末尾：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step is to create some classes for implementing our attention mechanism.
    The primary thing we need is a `Module` that takes the attention weights α with
    the extracted feature representations **h**[1], **h**[2], …, **h**[T] and computes
    the weighted average **x̄** = Σ[i]^T[=1] *α*[i] · **h**[i]. We call this the `Combiner`.
    We take in a network `featureExtraction` that computes **h**[t] = *F*(**x**[t])
    for each item in the input, and a network `weightSelection` that computes α from
    the extracted features.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一些类来实现我们的注意力机制。我们需要的主要是一个 `Module`，它接受注意力权重 α 和提取的特征表示 **h**[1]，**h**[2]，…，**h**[T]，并计算加权平均值
    **x̄** = Σ[i]^T[=1] *α*[i] · **h**[i]。我们称这个为 `Combiner`。我们接受一个网络 `featureExtraction`，它为输入中的每个项目计算
    **h**[t] = *F*(**x**[t])，以及一个网络 `weightSelection`，它从提取的特征中计算 α。
- en: Defining the combiner module
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 定义组合模块
- en: 'The forward function for this `Combiner` is pretty simple. We compute the `features`
    and the `weights` as we have described. We do a small amount of tensor manipulation
    to make sure `weights` is in a shape that will allow us to pairwise multiply with
    the features, because pairwise multiplication requires tensors to have the same
    number of axes. Also notice that we are making sure to add comments about the
    shape of each tensor on every line. Attention mechanisms involve many changing
    shapes, so it is a good idea for you to include comments like these:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `Combiner` 的前向函数相当简单。我们计算 `features` 和 `weights`，正如我们描述的那样。我们进行一些张量操作，以确保
    `weights` 的形状允许我们与特征进行成对乘法，因为成对乘法需要张量具有相同数量的轴。此外，请注意，我们在每一行都添加了关于每个张量形状的注释。注意力机制涉及许多变化的形状，因此包含这样的注释是个好主意：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ (B, T, D) **h**[i] = *F*(**x**[i])
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T, D) **h**[i] = *F*(**x**[i])
- en: ❷ (B, T) or (B, T, 1) for α
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T) 或 (B, T, 1) 用于 α
- en: ❸ (B, T) shape
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T) 形状
- en: ❹ Now (B, T, 1) shape
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在 (B, T, 1) 形状
- en: ❺ (B, T, D); computes *α*[i] ⋅ **h**[i]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, T, D)；计算 *α*[i] ⋅ **h**[i]
- en: ❻ Sums over the T dimension, giving the (B, D) final shape
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对 T 维度求和，得到 (B, D) 最终形状
- en: Defining a backbone network
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义骨干网络
- en: 'Now we are ready to define our attention-based model for this problem. First,
    we quickly define two variables—`T` for how many items are in our bag, and `D`
    for the number of features:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好为这个问题定义基于注意力的模型。首先，我们快速定义两个变量——`T` 表示包中项目的数量，`D` 表示特征的数量：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first network to define is the feature extraction network. We could also
    call this the backbone network, as it follows the same intuition as the backbone
    from Faster R-CNN in chapter 8\. This network will do all the heavy lifting to
    learn a good representation **h**[i] for every item in the input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要定义的是特征提取网络。我们也可以称这个网络为骨干网络，因为它遵循与第 8 章中 Faster R-CNN 的骨干网络相同的直觉。这个网络将承担所有繁重的工作，为输入中的每个项目学习一个良好的表示
    **h**[i]。
- en: 'The trick to making this easy to implement is to remember that the `nn.Linear`
    layer can work on tensors of shape (*B*,*T*,*D*). If you have an input of that
    shape, the `nn.Linear` layer will be applied to all T items *independently*, as
    if you wrote a `for` loop like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使其易于实现的小技巧是记住 `nn.Linear` 层可以作用于形状为 (*B*,*T*,*D*) 的张量。如果你有一个这种形状的输入，`nn.Linear`
    层将独立地应用于所有 T 个项目，就像你写了一个这样的 `for` 循环：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ (B, D)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, D)
- en: ❷ Makes (B, 1, D)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使其变为 (B, 1, D)
- en: ❸ (B, T, D)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T, D)
- en: 'So we can use `nn.Linear` followed by any activation function to apply this
    backbone to every input separately:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用 `nn.Linear` 后跟任何激活函数来分别应用这个骨干到每个输入：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Shape is now (B, T, D)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状现在是 (B, T, D)
- en: ❷ Shape becomes (B, T, neurons)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 形状变为 (B, T, 神经元)
- en: ❸ Still (B, T, neurons) on the way out
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仍然 (B, T, 神经元) 在输出路径上
- en: Defining an attention subnetwork
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个注意力子网络
- en: 'Now we need a network to compute the attention mechanism weights α. Following
    the backbone logic, we assume that the feature extraction network has done the
    heavy lifting, so our attention subnetwork can be on the small side. We have one
    hidden layer and then a second layer that explicitly has an output size of `1`.
    We need this because every item in the input gets *one* score. Then we apply the
    softmax over the T dimension to normalize these scores over each bag’s group:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个网络来计算注意力机制权重 α。遵循骨干逻辑，我们假设特征提取网络已经完成了繁重的工作，因此我们的注意力子网络可以相对较小。我们有一个隐藏层，然后是一个显式输出大小为
    `1` 的第二层。我们需要这样做，因为输入中的每个项目都会得到 *一个* 分数。然后我们在 T 维度上应用 softmax 来归一化每个包组的这些分数：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Shape is (B, T, neurons)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状是 (B, T, 神经元)
- en: ❷ (B, T, 1)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, 1)
- en: Training a simple attention model and results
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个简单的注意力模型及其结果
- en: 'With the feature extracting backbone and the weight computing attention mechanism,
    we can now define a complete attention-based network. It starts with a `Combiner`
    that takes in the two subnetworks we’ve defined, followed by any number of fully
    connected layers we desire. Usually the backbone has already done a lot of work,
    so you can often use just two or three hidden layers at this step. Then we train
    the model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征提取主干和权重计算注意力机制，我们现在可以定义一个完整的基于注意力的网络。它从一个`Combiner`开始，该`Combiner`接收我们定义的两个子网络，然后是任意数量的我们想要的完全连接层。通常，主干已经做了很多工作，所以在这个步骤中你通常只需要两到三个隐藏层。然后我们训练模型：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Input is (B, T, C, W, H). The combiner uses the backbone and attention to
    process. The result is (B, neurons).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入是(B, T, C, W, H)。组合器使用主干和注意力进行处理。结果是(B, 神经元)。
- en: 'With training complete, we can look at the accuracy of our model. After just
    one epoch, our simple attention network is already doing better than the regular
    network:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以查看我们模型的准确率。仅仅经过一个epoch，我们简单的注意力网络就已经比常规网络表现更好：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/CH10_UN03_Raff.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN03_Raff.png)'
- en: 'We can also select random samples from the dataset and look at how the attention
    mechanism selects the inputs. Here is some simple code to run a sample through;
    the following graphic shows the attention weight in red on top of each digit.
    With “0, 9, 0” as the input, the attention mechanism correctly placed almost all
    of the weight on the digit 9, allowing it to make an accurate classification:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从数据集中选择随机样本，并查看注意力机制如何选择输入。以下是一些简单的代码来运行一个样本；以下图形显示了红色覆盖在每个数字上的注意力权重。以“0,
    9, 0”作为输入，注意力机制正确地将几乎所有的权重放在数字9上，允许它做出准确的分类：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Selects a data point (which is a bag)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择一个数据点（它是一个包）
- en: ❷ Moves it to the compute device
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将其移动到计算设备
- en: ❸ Applies score(F(x))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用得分(F(x))
- en: ❹ Converts to a NumPy array
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 转换为NumPy数组
- en: ❺ Makes a plot for all three digits
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为所有三个数字绘制一个图
- en: ❻ Plots the digit
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 绘制数字
- en: ❼ Draws the attention score in the top left
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在左上角绘制注意力得分
- en: '![](../Images/CH10_UN04_Raff.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN04_Raff.png)'
- en: We have now seen how this simple attention can help us learn faster and better
    networks when only a subset of the inputs is important. But there are two lingering
    problems. First, we have the naive expectation that *everything* in a batch is
    the same size. We will need some padding to fix this, just as we used in our RNNs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到这个简单的注意力如何帮助我们更快、更好地学习当只有输入子集重要时的网络。但有两个悬而未决的问题。首先，我们有一个天真的期望，即批处理中的*所有内容*大小相同。我们需要一些填充来解决这个问题，就像我们在我们的RNNs中使用的那样。
- en: The second issue is that our scores lack *context*. The importance of one item
    depends on the other items. Imagine you are watching a movie. You pay attention
    to the movie and not the birds outside or the running dishwasher because the movie
    is more important. But that changes if you hear a fire alarm. Right now, our `attentionMechanism`
    network looks at each item independently, so both a movie and a fire alarm may
    receive high scores—but in reality, their scores should be relative to the other
    things that are present.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是我们的得分缺乏*上下文*。一个项目的重要性取决于其他项目。想象你正在看电影。你关注的是电影，而不是外面的鸟或正在运行的洗碗机，因为电影更重要。但如果你听到火灾警报，情况就会改变。现在，我们的`attentionMechanism`网络独立地查看每个项目，所以电影和火灾警报都可能得到高分数——但在现实中，它们的分数应该相对于其他存在的事物是相对的。
- en: 10.2 Adding some context
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 添加一些上下文
- en: The first thing we improve about our approach to attention mechanisms is adding
    context to the scores. We leave all the other parts alone. By *context*, we mean
    the score for an item **x**[i] should depend on all the other items **x**[*j*
    ≠ *i*]. Right now we have a backbone feature extractor *F*(⋅) and an importance
    calculator score(⋅). But that means the importance of any input **x**[i] is determined
    without any context about what the other inputs are, because score(⋅) is applied
    to everything independently.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对注意力机制方法的第一项改进是向得分中添加上下文。我们保留所有其他部分不变。通过*上下文*，我们指的是项目**x**[i]的得分应该依赖于所有其他项目**x**[*j*
    ≠ *i*]。现在我们有一个主干特征提取器*F*(⋅)和一个重要性计算器得分(⋅)。但这意味着任何输入**x**[i]的重要性是在没有任何关于其他输入的上下文的情况下确定的，因为得分(⋅)是独立应用于一切的。
- en: To get an idea of why we want context, let’s return to that noisy coffee shop
    where you are chatting with friends. Why are you ignoring all the background noise?
    Because you have the *context* that your friend is talking and they are more important
    to you than other conversations. But if your friend is not present, you may find
    yourself listening in on random conversations around you.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们为什么需要上下文，让我们回到那个嘈杂的咖啡馆，你在那里和朋友聊天。你为什么忽略所有的背景噪音？因为你有一个**上下文**，知道你的朋友在说话，他们对你来说比其他对话更重要。但如果你的朋友不在场，你可能会发现自己正在倾听周围随机的对话。
- en: In other words, the attention mechanism uses *global* information to make *local*
    decisions. The local decisions came in the form of the weights α, and the context
    provides the global information.[²](#fn38)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，注意力机制使用**全局**信息来做出**局部**决策。局部决策以权重α的形式出现，上下文提供了全局信息。[²](#fn38)
- en: How can we add context to the attention mechanism? Figure 10.2 shows how we
    make this happen.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将上下文添加到注意力机制中？图10.2展示了我们是如何实现这一点的。
- en: '![](../Images/CH10_F02_Raff.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_Raff.png)'
- en: 'Figure 10.2 The attention mechanism process on the left remains unchanged except
    for the score component. The right is zoomed in on how scores work. The score
    block takes in the results **h**[…] from the backbone/feature extraction network.
    For each score *α̃*[t], two inputs are used: **h**[t], for which we want to compute
    the score, and ![](../Images/bar_h.png), which represents a context of all the
    inputs **h**[…]. This context ![](../Images/bar_h.png) can be a simple average
    of all the items.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 左侧的注意力机制过程保持不变，除了分数组件。右侧放大了分数的工作方式。分数块接收来自骨干/特征提取网络的**h**[...]结果。对于每个分数**α̃**[t]，使用两个输入：**h**[t]，我们想要计算其分数，以及![bar_h.png](../Images/bar_h.png)，它代表所有输入**h**[...]的上下文。这个上下文![bar_h.png]可以是所有项目的简单平均值。
- en: 'We make score(⋅,⋅) a network that takes in two inputs: the tensor of inputs
    (*B*,*T*,*H*) and a second tensor of shape (*B*,*H*) containing a context for
    each sequence. We use H here to represent the number of features in the states
    **h**[t] = *F*(**x**[t]), to make them distinct from the size D of the original
    inputs **x**[i]. This second context tensor of shape (*B*,*H*) has no time/sequence
    dimension T because this vector is intended to be used as the context for all
    T items of the first input. This means every item in a bag will get the same context
    for computing its score.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将score(⋅,⋅)设计成一个网络，它接收两个输入：输入的张量(*B*,*T*,*H*)和第二个形状为(*B*,*H*)的张量，包含每个序列的上下文。我们在这里使用H来表示状态**h**[t]
    = *F*(**x**[t])中的特征数量，以使其与原始输入**x**[i]的大小D区分开来。这个形状为(*B*,*H*)的第二个上下文张量没有时间/序列维度T，因为这个向量打算用作第一个输入的所有T个项目的上下文。这意味着包中的每个项目在计算其分数时都会得到相同的上下文。
- en: 'The simplest form of context we can use is the average value of all the features
    extracted. So if we have **h**[i] = *F*(**x**[i]), we can compute the average
    ![](../Images/bar_h.png) from all of these to give the model a rough idea of all
    the options it has to choose from. This makes our attention computation occur
    in three steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用最简单的上下文形式是所有提取特征的平均值。所以如果我们有**h**[i] = *F*(**x**[i])，我们可以从所有这些计算平均![bar_h.png]，给模型一个大致的了解，它可以选择的所有选项。这使得我们的注意力计算分为三个步骤：
- en: 'Compute the average of the features extracted from each input. This treats
    everything as being equal in importance:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从每个输入提取的特征的平均值。这把所有东西都视为同等重要：
- en: '![](../Images/CH10_F02_EQ01.png)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_EQ01.png)'
- en: 'Compute the attention scores α for the T inputs. The only change here is that
    we include ![](../Images/bar_h.png) as a second argument to each score computation:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算T个输入的注意力分数α。这里唯一的改变是我们将![bar_h.png]作为每个分数计算的第二个参数：
- en: '![](../Images/CH10_F02_EQ02.png)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_EQ02.png)'
- en: 'Compute the weighted average of the extracted features. This part is identical
    to before:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算提取特征的加权平均值。这部分与之前相同：
- en: '![](../Images/CH10_F02_EQ03.png)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_EQ03.png)'
- en: This simple process gives us a new framework for attention scores, allowing
    us to alter the score of an item based on the other items present. We still want
    our score networks to be very simple and lightweight, because the backbone *F*(⋅)
    will do the heavy lifting. There are three common ways to compute the score from
    the representation h and the context ![](../Images/bar_h.png), commonly called
    the *dot*, *general*, and *additive scores*. Note that none of these is generally
    considered better than the others—attention mechanisms are still very new, and
    each of the following approaches works better or worse than others in different
    situations. The best advice I can give you right now is to try all three and see
    which works best for your problem. We will talk about each of them, since they
    are simple, and then run all of them on our toy problem to see what happens. In
    the descriptions, I use H to refer to the number of dimensions/neurons coming
    into the attention mechanism.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的流程为我们提供了一个新的注意力分数框架，使我们能够根据其他存在的项目来调整项目的分数。我们仍然希望我们的分数网络非常简单和轻量级，因为骨干 *F*(⋅)
    将会承担繁重的工作。从表示 h 和上下文 ![](../Images/bar_h.png) 计算分数有三种常见方法，通常称为 *点积*、*通用* 和 *加法*
    分数。请注意，这些方法中没有一种是普遍认为比其他更好的——注意力机制仍然非常新颖，以下每种方法在不同的情境下可能比其他方法更有效或更无效。我现在能给你的最好建议是尝试所有三种方法，看看哪一种最适合你的问题。我们将逐一讨论它们，因为它们很简单，然后在我们的小问题实例上运行所有这些方法，看看会发生什么。在描述中，我使用
    H 来指代进入注意力机制的维度/神经元的数量。
- en: 10.2.1  Dot score
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1  点积分数
- en: 'The dot score is one of the simplest scoring methods but can also be one of
    the most effective. The idea is that if we have an item **h**[t] and a context
    ![](../Images/bar_h.png), we take their dot product, ***h**[t]*^⊤ ![](../Images/bar_h.png),
    to get a score. We take the dot product because it measures how aligned in direction
    and size two vectors are. In chapter 7, we talked about vectors being orthogonal:
    that concept can help you understand this. If two vectors are orthogonal, their
    dot product is 0, meaning they have no relationship. The less orthogonal they
    are, the larger the dot product. If it is a large positive value, they are very
    similar, and if it is a large negative value, they are very different. In addition
    to this trick, we divide the result by the square root of the dimension of the
    vectors H. That gives us the following equation for the dot score:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 点积分数是一种最简单的评分方法，但也可以是最有效的。其想法是，如果我们有一个项目 **h**[t] 和一个上下文 ![](../Images/bar_h.png)，我们取它们的点积
    ***h**[t]*^⊤ ![](../Images/bar_h.png)，以得到一个分数。我们取点积是因为它衡量两个向量在方向和大小上的对齐程度。在第7章中，我们讨论了向量是正交的：这个概念可以帮助你理解这一点。如果两个向量是正交的，它们的点积是
    0，这意味着它们没有关系。它们越不正交，点积就越大。如果它是一个大的正数值，它们非常相似；如果它是一个大的负数值，它们非常不同。除了这个技巧之外，我们还把结果除以向量的维度
    H 的平方根。这给我们带来了点积分数的以下方程：
- en: '![](../Images/CH10_F02_EQ04.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F02_EQ04.png)'
- en: Why divide by √*H*? The concern is that the normal dot product calculation may
    lead to larger-magnitude values (very big positive or big negative values). Large
    values will cause vanishing gradients when the softmax is computed, just as we
    saw with the tanh and sigmoid (σ) activation functions. So we divide by √*H* to
    try to avoid large values—the specific choice of using the square root was made
    heuristically because it tended to work.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要除以 √*H*？担忧的是，正常的点积计算可能会导致更大的数值（非常大的正数或大负数）。大数值在计算softmax时会导致梯度消失，正如我们在tanh和sigmoid（σ）激活函数中看到的那样。因此，我们除以√*H*来尝试避免大数值——使用平方根的具体选择是启发式地做出的，因为它往往有效。
- en: 'Implementing this approach is pretty simple. We need to use the batch matrix
    multiply method `torch.bmm`, since we will compute the scores for all T time steps
    at once. Each item has a shape of (*T*,*H*), and the context—after we use the
    `unsqueeze` function to add a dimension of size 1 to the end of it—will have a
    shape of (*D*,1). Multiplying two matrices of shapes (*T*,*H*) × (*H*,1) = (*T*,1),
    which is the shape we need: one score for each item. `torch.bmm` applies this
    for every item in the batch, so `torch.bmm((B, T, H), (B, H, 1))` has an output
    of shape (*B*,*T*,1). The following `DotScore` implements this as a reusable `Module`,
    where the `forward` function takes in `states` and a `context`. We’ll reuse this
    pattern for the other two score methods:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这种方法相当简单。我们需要使用批矩阵乘法方法 `torch.bmm`，因为我们将会一次性计算所有 T 个时间步的得分。每个项目具有 (*T*,*H*)
    的形状，而上下文——在我们使用 `unsqueeze` 函数向其末尾添加一个大小为 1 的维度之后——将具有 (*D*,1) 的形状。将形状为 (*T*,*H*)
    × (*H*,1) 的两个矩阵相乘得到 (*T*,1)，这正是我们需要的形状：每个项目一个得分。`torch.bmm` 对批处理中的每个项目应用此操作，因此
    `torch.bmm((B, T, H), (B, H, 1))` 的输出形状为 (*B*,*T*,1)。接下来的 `DotScore` 将其实现为一个可重用的
    `Module`，其中 `forward` 函数接受 `states` 和 `context`。我们将重用此模式来处理其他两种得分方法：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Computes ![](../Images/eq_hh_t-top_bar_h.png). (B, T, H) -> (B, T, 1).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算 ![图片](../Images/eq_hh_t-top_bar_h.png)。 (B, T, H) -> (B, T, 1)。
- en: 10.2.2  General score
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2  一般得分
- en: Since the dot score is so effective, can we improve it? What if one of the features
    in ![](../Images/bar_h.png) is not very useful? Can we learn to not use it as
    much? That’s the idea behind the *general* score. Instead of simply computing
    the dot product between each item **h**[t] with the context ![](../Images/bar_h.png),
    we add a matrix W in between them. This gives us
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点积得分非常有效，我们能否改进它？如果 ![图片](../Images/bar_h.png) 中的一个特征不是非常有用，我们能否学习不使用它？这就是
    *一般* 得分背后的想法。我们不是简单地计算每个项目 **h**[t] 与上下文 ![图片](../Images/bar_h.png) 之间的点积，而是在它们之间添加一个矩阵
    W。这给我们
- en: '![](../Images/CH10_F02_EQ05.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_EQ05.png)'
- en: where W is a *H* × *H* matrix. The general score can be called a generalization
    of the dot product score because it can learn the same solution as the dot score,
    but the general score can also learn solutions the dot score can’t. This occurs
    if the model learns to set W equal to the identity matrix, I, because for any
    possible input z, **I****z** = **z**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 是一个 *H* × *H* 矩阵。一般得分可以称为点积得分的推广，因为它可以学习与点积得分相同的解，但一般得分也可以学习点积得分无法学习的解。这种情况发生在模型学习将
    W 设置为单位矩阵 I 时，因为对于任何可能的输入 z，**I****z** = **z**。
- en: Again, this implementation is simple because the general score has what is called
    a *bilinear relationship*. A bilinear function looks like
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这种实现很简单，因为一般得分有一个称为 *双线性关系* 的特性。双线性函数看起来像
- en: '![](../Images/CH10_F02_EQ06.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_EQ06.png)'
- en: where **x**[1] ∈ ℝ^(*H*1) and **x**[2] ∈ ℝ^(*H*1) are the inputs, and b and
    W are the parameters to learn. PyTorch provides a `nn.Bilinear(H1, H2, out_size)`
    function that can compute this for us. In our case, *H*1 = *H*2 = *H*, and our
    output size is 1 (a single score value for each item).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **x**[1] ∈ ℝ^(*H*1) 和 **x**[2] ∈ ℝ^(*H*1) 是输入，b 和 W 是需要学习的参数。PyTorch 提供了一个
    `nn.Bilinear(H1, H2, out_size)` 函数，可以为我们计算这个值。在我们的情况下，*H*1 = *H*2 = *H*，我们的输出大小是
    1（每个项目一个得分值）。
- en: 'We need to know about two tricks to implement this in one call to compute all
    T scores. We have a tensor of shape (*B*,*T*,*H*) for our items **h**[1], **h**[2],
    …, **h**[T] but a tensor of only (*B*,*H*) for our context ![](../Images/bar_h.png).
    We need both tensors to have the same number of axes to be used in the bilinear
    function. The trick is to stack the context into multiple copies T times, so we
    can create a matrix of shape (*B*,*T*,*H*):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解两个技巧来实现这个方法，以便在一次调用中计算所有 T 个得分。我们有一个形状为 (*B*,*T*,*H*) 的张量用于我们的项目 **h**[1]，**h**[2]，…，**h**[T]，但一个形状为
    (*B*,*H*) 的张量用于我们的上下文 ![图片](../Images/bar_h.png)。我们需要这两个张量具有相同数量的轴，以便在双线性函数中使用。技巧是将上下文堆叠成
    T 个副本，这样我们就可以创建一个形状为 (*B*,*T*,*H*) 的矩阵：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ stores W
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 存储 W
- en: ❷ Repeats the values T times. (B, H) -> (B, T, H).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重复值 T 次。 (B, H) -> (B, T, H)。
- en: ❸ Computes ***h**[t]*^⊤ *W*![](../Images/bar_h.png). (B, T, H) -> (B, T, 1).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算 ***h**[t]*^⊤ *W*![图片](../Images/bar_h.png)。 (B, T, H) -> (B, T, 1)。
- en: Note We introduced `GeneralScore` as trying to improve `DotScore`, which is
    reasonable because of how related they are. Yet today, neither appears to be better
    than the other in practice. As we mentioned, attention mechanisms are very new,
    and we as a community are still figuring them out. Sometimes you’ll see `Dot`
    do better than `General`, and sometimes not.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们引入 `GeneralScore` 是为了尝试改进 `DotScore`，这是合理的，因为它们之间有很强的相关性。然而，在实践中，今天似乎没有哪一个比另一个更好。正如我们提到的，注意力机制非常新，我们作为一个社区仍在探索它们。有时你会看到
    `Dot` 比 `General` 表现得更好，有时则不是。
- en: 10.2.3  Additive attention
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3  加性注意力
- en: 'The last score we’ll discuss is often called *additive* or *concat* attention.
    For a given item and its context, we use a vector v and a matrix W as the parameters
    for the layer, making a small neural network, as shown in the following equation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一种得分通常被称为 *加性* 或 *连接* 注意力。对于给定项及其上下文，我们使用向量 v 和矩阵 W 作为层的参数，形成一个小的神经网络，如下面的方程所示：
- en: '![](../Images/CH10_UN05_Raff.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_UN05_Raff.png)'
- en: This equation is a simple one-layer neural network. W is the hidden layer, followed
    by tanh activation, and v is the output layer with one output,[³](#fn39) which
    is necessary because the score should be a single value. The context ![](../Images/bar_h.png)
    is incorporated into the model by simply concatenating it with the item **h**[t]
    so that the item and its context are the inputs into this fully connected network.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是一个简单的一层神经网络。W 是隐藏层，后面跟着 tanh 激活，v 是输出层，有一个输出[³](#fn39)，这是必要的，因为得分应该是一个单一值。上下文
    ![图片](../Images/bar_h.png) 通过简单地将其与项目 **h**[t] 连接起来纳入模型，这样项目和它的上下文就成为了这个全连接网络的输入。
- en: 'The idea behind the additive layer is fairly simple: let a small neural network
    figure out the weighting for us. Implementing it requires a little cleverness,
    though. What operations would you expect to need? Both v and W can be taken care
    of with `nn.Linear` layers, and you need to use the `torch.cat` function to concatenate
    the two inputs **h**[t] and ![](../Images/bar_h.png) together, so you might annotate
    this previous equation as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 加性层的背后的思想相当简单：让一个小型神经网络为我们确定权重。虽然实现它需要一点巧妙，但操作预期是什么呢？v 和 W 都可以用 `nn.Linear`
    层来处理，并且你需要使用 `torch.cat` 函数将两个输入 **h**[t] 和 ![图片](../Images/bar_h.png) 连接起来，所以你可能这样注释前面的方程：
- en: '![](../Images/ch10-eqs-to-illustrator4x.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch10-eqs-to-illustrator4x.png)'
- en: 'While this would work as described, it would be computationally inefficient.
    It is slow because we don’t have just **h**[t]—we have **h**[1], **h**[2], …,
    **h**[T] in one larger tensor of shape (*B*,*T*,*H*). So rather than split this
    into T different items and call the attention function multiple times, we want
    to implement this in such a way that the T attention scores are calculated all
    at once, and we can do this by using the same trick we used with the general score.
    Just be sure to do the concatenation after the stacking trick so the tensors are
    the same shape—and use `dim=2` so they are concatenated along the feature dimension
    H rather than the time dimension T. A diagram of this process is shown in figure
    10.3, and the following code shows how to implement it:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样描述是可行的，但它在计算上效率不高。它之所以慢，是因为我们不仅仅有 **h**[t]——我们有一个形状为 (*B*,*T*,*H*) 的大张量，其中包含
    **h**[1]，**h**[2]，…，**h**[T]。因此，我们不想将其分成 T 个不同的项目并多次调用注意力函数，我们希望以这种方式实现它，即一次计算
    T 个注意力得分，我们可以通过使用与一般得分相同的技巧来实现这一点。只需确保在堆叠技巧之后进行连接，以便张量具有相同的形状——并使用 `dim=2` 以便它们在特征维度
    H 而不是时间维度 T 上进行连接。这个过程的一个图示如图 10.3 所示，以下代码显示了如何实现它：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ 2*H because we concatenate two inputs
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 2*H 因为我们要连接两个输入
- en: ❷ Repeats the values T times
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重复值 T 次
- en: ❸ (B, H) -> (B, T, H)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, H) -> (B, T, H)
- en: ❹ (B, T, H) + (B, T, H) -> (B, T, 2*H)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, T, H) + (B, T, H) -> (B, T, 2*H)
- en: ❺ (B, T, 2*H) -> (B, T, 1)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, T, 2*H) -> (B, T, 1)
- en: '![](../Images/CH10_F03_Raff.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F03_Raff.png)'
- en: Figure 10.3 Diagram of how to implement additive attention. The states and context
    are represented on the left, and the context is reshaped (via copying) to have
    the same shape as the states. This make it easy to concatenate them together to
    then be fed into a neural network.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 加性注意力的实现图。状态和上下文在左侧表示，上下文通过复制重塑以具有与状态相同的形状。这使得它们很容易连接起来，然后输入到神经网络中。
- en: 10.2.4  Computing attention weights
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4  计算注意力权重
- en: 'Now that we have the various attention scores we want, we’ll define a simple
    helper `Module` that takes in the raw scores ![](../Images/tilde_alpha.png) and
    extracted representations **h**[…] and computes the final output x̄. This will
    replace our `Combiner` module by adding one new step: applying a *mask* to the
    inputs.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了我们想要的各个注意力得分，我们将定义一个简单的辅助`Module`，它接受原始得分 ![](../Images/tilde_alpha.png)
    和提取的表示 **h**[…] 并计算最终的输出 x̄。这将通过添加一个新步骤来替换我们的`Combiner`模块：对输入应用*掩码*。
- en: Our toy MNIST example started with all bags having the same size, which never
    happens in real life. Any dataset you work with will most likely have inputs containing
    variable numbers of items (e.g., words in a sentence to translate), and if we
    train on batches of data, we have to deal with the inconsistency. This is why
    we need extra logic to compute the final weights α to take a Boolean *mask* as
    input. The mask tells us which parts of the input are real (`True`) and which
    are padded (`False`) to make the tensor shape consistent. This works almost the
    same as the RNNs in chapter 4 when we padded the smaller items to the same size
    as the largest bag/sequence in a batch. The mask replaces the role of the packing
    code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的玩具MNIST示例开始时所有袋子的大小都相同，这在现实生活中从未发生过。你将与之工作的任何数据集很可能包含包含可变数量项的输入（例如，要翻译的句子中的单词），如果我们对数据批次进行训练，我们必须处理这种不一致性。这就是为什么我们需要额外的逻辑来计算最终的权重α，以布尔*掩码*作为输入。掩码告诉我们输入的哪些部分是真实的（`True`）以及哪些是填充的（`False`），以使张量形状保持一致。这与第4章中我们填充较小项以与批次中最大的袋子/序列大小相同时的RNN几乎相同。掩码取代了打包代码的角色。
- en: 'The trick we use here is to manually set the score for every item that has
    a `False` value to be a very large negative number like –1000\. We do this because
    exp (−1000) ≈ 5.076 × 10^(−435), so this will most likely underflow to zero and
    cause *desirable* vanishing gradients. When the calculation underflows to zero,
    its gradient and contributions are zero, effectively eliminating its impact from
    the model. The following code does this with a new `ApplyAttention` class:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的技巧是手动将每个具有`False`值的项的得分设置为非常大的负数，例如-1000。我们这样做是因为exp(-1000) ≈ 5.076
    × 10^(-435)，因此这很可能会下溢到零并导致*期望的*梯度消失。当计算下溢到零时，其梯度及其贡献为零，有效地消除了其对模型的影响。以下代码使用新的`ApplyAttention`类来完成此操作：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Sets everything not present to a large negative value that causes vanishing
    gradients
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有未出现的项设置为一个大负值，导致梯度消失
- en: ❷ Computes the weight for each score. (B, T, 1) still, but sum(T) = 1
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为每个得分计算权重。（B，T，1）仍然如此，但sum(T) = 1
- en: ❸ (B, T, D) * (B, T, 1) -> (B, D)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T, D) * (B, T, 1) -> (B, D)
- en: 'Working with masks is very common with attention mechanisms. To make that easier,
    we can define a nice helper function for computing masks from any tensor with
    a shape (*B*,*T*,…). The idea is that we expect any missing item or non-present
    item to have its tensor filled with a constant value, usually zero. So we look
    for any dimension over time T that has values equal to this constant. If everything
    is equal to that constant, a value of `False` should be returned; otherwise, we
    want something that is `True` to indicate that the values present are good to
    use. The following `getMaskByFill` function does this. The input `x` is what we
    want to get a mask for, `time_dimension` tells us which dimension of the tensor
    is used to represent T, and `fill` indicates the special constant used to denote
    padding/invalid parts of the input:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中与掩码一起工作非常常见。为了使其更容易，我们可以为从任何具有形状(*B*,*T*,…)的张量计算掩码定义一个很好的辅助函数。想法是，我们期望任何缺失项或未出现的项都将其张量填充为常数值，通常是零。因此，我们寻找时间T维度上具有此常数值的任何维度。如果一切等于该常数，则应返回`False`值；否则，我们想要一个`True`值来指示现有的值是可用的。以下`getMaskByFill`函数就是这样做的。输入`x`是我们想要为其获取掩码的，`time_dimension`告诉我们张量的哪个维度用于表示T，而`fill`表示用于表示输入填充/无效部分的特殊常数：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Skips the first dimension 0 because that is the batch dimension
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 跳过第一个维度0，因为那是批次维度
- en: ❷ (x!=fill) determines locations that might be unused because they are missing
    the fill value we are looking for to indicate lack of use. We then count the number
    of non-fill values over everything in that time slot (reducing changes the shape
    to (B, T)). If any entry is not equal to this value, the item represent must be
    in use and so returns a value of true.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (x!=fill) 确定可能未使用的位置，因为这些位置缺少我们用来表示未使用的填充值。然后我们计算该时间槽中所有非填充值的数量（减少改变形状为（B，T））。如果任何条目不等于此值，则表示该项必须在使用中，因此返回true值。
- en: 'Let’s see a quick example of how this new function works. We create an input
    matrix with a batch of *B* = 5 inputs, *T* = 3 time steps, and a 7 × 7 image with
    one input channel. That’s be a tensor of shape (*B*=5,*T*=3,1,7,7), and we make
    it so that the last item in the first batch is unused and the entire fourth item
    is unused. Our mask should look like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看这个新函数是如何工作的。我们创建一个输入矩阵，包含一个批次的*B* = 5个输入，*T* = 3个时间步长，以及一个7 × 7的单通道图像。这将是一个形状为(*B*=5,*T*=3,1,7,7)的张量，我们使其如此，即第一个批次的最后一个项目未使用，整个第四个项目未使用。我们的mask应该看起来像这样：
- en: '![](../Images/CH10_F03_EQ01.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F03_EQ01.png)'
- en: 'The following code creates this hypothetical data and computes the appropriate
    mask and then prints it out:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了这个假设数据，并计算了适当的mask，然后将其打印出来：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Don’t use the last item in the first input.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不要使用第一个输入的最后一个项目。
- en: ❷ Don’t use any of the fourth item!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不要使用第四项中的任何一项！
- en: ❸ Makes it look like we aren’t using part of the fifth item, but we still are!
    This line was added to show that this works even on tricky inputs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 让它看起来我们好像没有使用第五项的一部分，但实际上我们还在使用！这一行被添加进来以表明即使在复杂的输入上这也有效。
- en: The mask returned the correct output. This one function allows us to create
    masks for text sequences, sequences/bags of images, and just about any kind of
    unusual tensor shape.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的mask产生了正确的输出。这个函数允许我们为文本序列、序列/图像包以及几乎任何类型的非标准张量形状创建mask。
- en: '10.3 Putting it all together: A complete attention mechanism with context'
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 将所有内容组合起来：一个带有上下文的完整注意力机制
- en: With a score method (e.g., dot), `getMaskByFill` function, and our new `ApplyAttention`
    function, we can define a complete attention-based network that is adaptive to
    the input. For most attention-based models, we do not use `nn.Sequential` for
    defining the main model because of all the non-sequential steps involved with
    attention. Instead, we use `nn.Sequential` to define and organize the subnetworks
    that our larger network uses.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分数方法（例如，点积）、`getMaskByFill`函数和我们的新`ApplyAttention`函数，我们可以定义一个完整的基于注意力的网络，该网络对输入是自适应的。对于大多数基于注意力的模型，我们由于涉及注意力的所有非顺序步骤，不使用`nn.Sequential`来定义主模型。相反，我们使用`nn.Sequential`来定义和组织较大网络使用的子网络。
- en: 'Let’s define a `SmarterAttentionNet` that is our entire network. Inside the
    constructor, we defines a network for our `backbone` that computes **h**[i] =
    *F*(**x**[i]), and a `prediction_net` that computes our prediction *ŷ* = *f*(
    **x̄**). The arguments `input_size`, `hidden_size`, and `out_size` define the
    number of features in the input (D), hidden neurons (H), and predictions our model
    should make (10 classes for MNIST). We also add an optional argument to choose
    which attention score is used as the `score_net`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`SmarterAttentionNet`，这是我们整个网络。在构造函数内部，我们定义了一个用于我们的`backbone`的网络，该网络计算**h**[i]
    = *F*(**x**[i])，以及一个`prediction_net`，该网络计算我们的预测*ŷ* = *f*( **x̄**)。参数`input_size`、`hidden_size`和`out_size`定义了输入中的特征数量（D）、隐藏神经元（H）以及我们的模型应该做出的预测（MNIST的10个类别）。我们还添加了一个可选参数来选择哪个注意力分数用作`score_net`：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Shape is now (B, T, D)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状现在是 (B, T, D)
- en: ❷ Shape becomes (B, T, H)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 形状变为 (B, T, H)
- en: ❸ Returns (B, T, H)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回 (B, T, H)
- en: ❹ Try changing this and see how the results change!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 尝试更改它并看看结果如何变化！
- en: ❺ (B, H), x̄ as input
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, H), x̄ 作为输入
- en: ❻ (B, H)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ (B, H)
- en: 'Next we define the `forward` function that computes the result of the network.
    We compute the `mask` from the input followed by the hidden states **h**[…] from
    the backbone on the same input. We use the average vector ![](../Images/bar_h.png)
    as the context `h_context`. Instead of computing this with `torch.mean`, we compute
    the mean ourselves in two steps so that we divide by only the number of valid
    items. Otherwise, an input with 2 valid items but 8 padded items would be divided
    by 10 instead of 2, and that’s not correct. This calculation also include `+1e-10`
    to add a tiny value to the numerator: this addition is some defensive coding so
    that if we ever receive a bag of *zero* items, we do not perform a division by
    zero. That would produce a `NaN` (Not a Number), which would cause silent failures.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义一个`forward`函数来计算网络的输出结果。我们从输入中计算`mask`，然后是来自骨干网络的隐藏状态**h**[...]，这些状态是在相同输入上计算的。我们使用平均向量
    ![](../Images/bar_h.png) 作为上下文`h_context`。我们不是用`torch.mean`来计算这个值，而是分两步自己计算，这样我们只除以有效项目的数量。否则，一个有2个有效项目但8个填充项目的输入将被除以10而不是2，这是不正确的。这个计算还包括`+1e-10`来给分子添加一个微小的值：这个添加是一种防御性编码，以防我们收到一个包含*零*项目的包，我们不会执行除以零的操作。这将产生一个`NaN`（不是一个数字），这会导致静默失败。
- en: 'Here’s the code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE22]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ (B, T, D) -> (B, T, H)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T, D) -> (B, T, H)
- en: ❷ h_context = torch.mean(h, dim=1) computes torch.mean but ignores the masked-out
    parts. First, add together all the valid items. (B, T, H) -> (B, H).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ h_context = torch.mean(h, dim=1) 计算torch.mean但忽略被屏蔽的部分。首先，将所有有效的项目加在一起。(B,
    T, H) -> (B, H)。
- en: ❸ Divides by the number of valid items, plus a small value in case a bag was
    all empty
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数量除以有效项目的数量，并在袋子全部为空的情况下加上一个小的值
- en: ❹ (B, T, H) , (B, H) -> (B, T, 1)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, T, H) , (B, H) -> (B, T, 1)
- en: ❺ Result is (B, H) shape
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 结果是 (B, H) 形状
- en: ❻ (B, H) -> (B, classes)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ (B, H) -> (B, classes)
- en: The last three lines of the `forward` function are again pretty simple. The
    `score_net` computes the `scores` α, the `apply_attn` function applies whichever
    attention function we have chosen, and `prediction_net` makes a prediction from
    the `final_context` x̄.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 函数的最后三行再次相当简单。`score_net` 计算得分 α，`apply_attn` 函数应用我们选择的任何注意力函数，`prediction_net`
    从 `final_context` x̄ 中做出预测。'
- en: 'Let’s train up some better attention models. The following block creates an
    attention network for each option, `DotScore`, `GeneralScore`, and `AdditiveAttentionScore`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一些更好的注意力模型。以下代码块为每个选项创建一个注意力网络，`DotScore`，`GeneralScore` 和 `AdditiveAttentionScore`：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'All three results are plotted by the following code. The results follow a trend
    that all three improved attention scores have similar performance. They all start
    out learning faster and converging faster than the simple attention we started
    with, and maybe the new methods are converging to something better. You will see
    some variation from run to run, but this dataset is not big or difficult enough
    to cause really interesting changes in behavior:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制了这三个结果。所有三个改进的注意力得分趋势相似。它们一开始比我们开始的简单注意力学习得更快，收敛得更快，也许新的方法正在收敛到一个更好的结果。你将看到每次运行之间的一些变化，但这个数据集还不够大或复杂，不足以引起行为上的真正有趣的变化：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/CH10_UN06_Raff.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN06_Raff.png)'
- en: The other benefit of our new code is the ability to handle variable-sized inputs,
    where the shorter/smaller items are padded to match the length of the longest/largest
    item in a batch. To make sure it’s working, we define a new `LargestDigitVariable`
    dataset that picks a random number of items to put in each bag, up to some specified
    maximum number. This will also make the training problem even more challenging
    because the network needs to determine whether there is any relationship between
    the number of items in a bag and the bag’s label.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新代码的另一个好处是能够处理可变大小的输入，其中较短的/较小的项目被填充以匹配批次中最长/最大的项目的长度。为了确保它正常工作，我们定义了一个新的
    `LargestDigitVariable` 数据集，该数据集随机选择放入每个袋子中的项目数量，最多达到某个指定的最大数量。这将使训练问题变得更加具有挑战性，因为网络需要确定袋子中项目的数量与袋子标签之间是否存在任何关系。
- en: 'The following code does this, with only two real changes. First, our `__getitem__`
    method computes `how_many` items to sample. Second, we implement padding inside
    our dataset object for simplicity, so if `how_many < maxToSample`, we pad the
    input with zero values to get everything the same size:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码做了这件事，只有两个真正的变化。首先，我们的 `__getitem__` 方法计算要采样多少个项目。其次，我们为了简单起见在数据集对象内部实现填充，所以如果
    `how_many < maxToSample`，我们用零值填充输入以使所有内容大小相同：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '❶ New: how many items should we select?'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新的：我们应该选择多少个项目？
- en: ❷ Randomly selects n=self.toSample items from the dataset
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机从数据集中选择 n=self.toSample 个项目
- en: '❸ Stacks the n items of shape (B, *) into (B, n, *). New: pad with zero values
    up to the max size.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将形状为 (B, *) 的 n 个项目堆叠成 (B, n, *)。新的：用零值填充到最大大小。
- en: ❹ Label is the maximum label.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 标签是最大标签。
- en: ❺ Returns (data, label) pair
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回 (data, label) 对
- en: 'The next block of code creates the training and test set loaders and should
    look very familiar to you by this point in the book:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块创建训练和测试集加载器，到这本书的这一部分你应该已经很熟悉了：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We would *usually* train our new models on this new dataset. But here is the
    cool thing: our new attention mechanism’s design can handle a variable number
    of inputs thanks to padding, and the attention prior includes the concept that
    only a subset of the inputs are relevant. So even though our data now has a new,
    larger shape, with more possible inputs, we can reuse the same network we have
    already trained.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会在这个新的数据集上训练我们的新模型。但这里有一个酷的地方：我们新的注意力机制设计可以处理可变数量的输入，这要归功于填充，并且注意力先验包括只关注输入子集的概念。所以尽管我们的数据现在有一个新的、更大的形状，有更多的可能输入，我们仍然可以重复使用我们已经训练过的相同网络。
- en: 'Instead of training a new model, the following code runs through the test set
    and makes predictions using one of our models trained on only bags of size 3,
    and it will hopefully work well on bags of size 1 through 6:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是训练一个新的模型，以下代码通过测试集运行并使用仅针对大小为3的包训练的我们的模型进行预测，并希望它在大小为1到6的包上也能很好地工作：
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You should see something around 96 or 97% accuracy on this new problem, and
    we trained on an easier version of it. This gives you an idea of how powerful
    the attention approach can be. It can often generalize very well to changes in
    the input data and sequence length, which is part of what has made attentions
    very successful. They have quickly become a go-to tool for most natural language
    processing tasks today.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在这个新问题中看到大约96%或97%的准确率，我们训练的是它的一个更容易的版本。这让你对注意力方法的力量有了概念。它通常能够很好地泛化到输入数据的变化和序列长度的变化，这也是注意力非常成功的一部分。它们已经迅速成为今天大多数自然语言处理任务的常用工具。
- en: It is not *always* possible to generalize like this. If we keep making the inputs
    longer than we trained on (increasing the `maxToSample` argument), the accuracy
    will eventually start to decrease. But we can counteract that by training on inputs
    that are as long as we expect to receive.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总是可以像这样泛化。如果我们继续将输入长度增加到我们训练的长度（增加`maxToSample`参数），准确率最终会开始下降。但我们可以通过在预期的输入长度上训练来抵消这一点。
- en: The important thing to be aware of here is that the attention approach can handle
    a variable number of inputs and generalize beyond what was given. This, combined
    with the ability to be selective with respect to the inputs, makes the approach
    very powerful.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要事情是，注意力方法可以处理可变数量的输入并泛化到所提供的内容之外。这，加上对输入的选择性，使得这种方法非常强大。
- en: Exercises
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台上的Inside Deep Learning Exercises（[https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)）分享和讨论你的解决方案。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最佳的。
- en: Train a new model for the `LargestDigit` problem that is convolutional. To make
    this work, you’ll need to convert your data from shape (*B*,*T*,*C*,*W*,*H*) to
    (*B***T*,*C*,*W*,*H*), essentially enlarging the batch size to run your 2D convolutions.
    Once you are done convolving, you can go back to a shape of (*B*,*T*,*C*,*W*,*H*).
    What accuracy are you able to get with this approach?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`LargestDigit`问题训练一个新的卷积模型。为了使这可行，你需要将你的数据从形状(*B*,*T*,*C*,*W*,*H*)转换为(*B***T*,*C*,*W*,*H*)，这实际上是将批大小扩大以运行你的2D卷积。一旦你完成卷积，你可以回到(*B*,*T*,*C*,*W*,*H*)的形状。你能够用这种方法获得多高的准确率？
- en: '`GeneralScore` uses a matrix W that starts with random values. But you could
    instead initialize it as *W* = I/√*H* + ε, where ϵ is just some small random values
    (say in the range of –0.01 to 0.01) to avoid hard zeros. This would make `GeneralScore`
    start with the behavior of `DotScore`. Try to implement this yourself.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`GeneralScore`使用一个以随机值开始的矩阵W。但你可以将其初始化为*W* = I/√*H* + ε，其中ε是某些小的随机值（例如在-0.01到0.01的范围内），以避免硬零。这将使`GeneralScore`以`DotScore`的行为开始。尝试自己实现这一点。'
- en: Using the `tanh` activation in the additive attention score is somewhat arbitrary.
    Try multiple different versions of the additive attention by replacing the `tanh`
    activation with other options in PyTorch. Are there any activations you think
    may work particularly well or poorly?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加性注意力分数中使用`tanh`激活函数是有些任意的。尝试通过在PyTorch中将`tanh`激活函数替换为其他选项来使用多个不同的加性注意力版本。你认为哪些激活函数可能特别有效或无效？
- en: Let’s implement an even harder version of the `LargestDigitVariable` dataset.
    To determine the label of a bag, take the sum of the labels of the bag’s contents
    (e.g., 3, 7, and 2 are in bag = “12”). If the sum is even, return the largest
    label; and if the sum is odd, return the smallest. For example “3”+“7”+“2” = 12,
    so the label should be “7.” But if the bag was “4”+“7”+“2”= 13, the label should
    be “2.”
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一个更难的`LargestDigitVariable`数据集版本。为了确定一个包的标签，将包内容的标签相加（例如，3、7和2在包“12”中）。如果总和是偶数，则返回最大的标签；如果是奇数，则返回最小的标签。例如，“3”+“7”+“2”=
    12，所以标签应该是“7”。但如果包是“4”+“7”+“2”= 13，则标签应该是“2”。
- en: You do not have to use the average of the inputs as your context; you can use
    *anything* that you think might work better. To demonstrate this, implement an
    attention network that uses an *attention subnetwork* to compute the context.
    This means there will be an initial context that is the average ![](../Images/bar_h.png),
    which is used to compute a *new* context *x̄*[*c**n**t**x*], which is then used
    to compute the *final* output x̄. Does this have any impact on your accuracy in
    exercise 4?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你不必使用输入的平均值作为你的上下文；你可以使用你认为可能工作得更好的任何东西。为了演示这一点，实现一个使用 *注意力子网络* 来计算上下文的注意力网络。这意味着将有一个初始上下文，它是平均
    ![bar_h.png](../Images/bar_h.png)，用于计算 *新的* 上下文 *x̄*[*c**n**t**x*]，然后用于计算 *最终*
    输出 x̄。这对你在练习 4 中的准确性有影响吗？
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Attention mechanisms encode a prior that some items in a sequence or set are
    more important than others.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制编码了一个先验，即序列或集合中的某些项目比其他项目更重要。
- en: Attention mechanisms use a context to decide which items are more or lessimportant.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制使用上下文来决定哪些项目更重要或不太重要。
- en: A score function is needed to determine how important each input is, given the
    context. The dot, general, and additive scores are all popular.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个得分函数来确定在给定上下文中每个输入的重要性。点积、一般和加法得分都很受欢迎。
- en: Attention mechanisms can handle variable-length inputs and are particularly
    good at generalizing over them.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制可以处理可变长度的输入，并且在泛化它们方面特别出色。
- en: Like RNNs, attentions require masking to handle batches of data.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 RNNs 类似，注意力机制需要掩码来处理数据批次。
- en: '* * *'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ See J. Foulds and E. Frank, “A review of multi-instance learning assumptions,”
    *The Knowledge Engin-eering Review*, vol. 25, no. 1, pp. 1–25, 2010, [https://www.cs.waikato.ac.nz/~eibe/pubs/FouldsAndFrankMIreview.pdf](https://www.cs.waikato.ac.nz/~eibe/pubs/FouldsAndFrankMIreview.pdf).[↩](#fnref37)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 请参阅 J. Foulds 和 E. Frank 的文章，“多实例学习假设综述”，*《知识工程评论》*，第 25 卷，第 1 期，第 1-25 页，2010
    年，[https://www.cs.waikato.ac.nz/~eibe/pubs/FouldsAndFrankMIreview.pdf](https://www.cs.waikato.ac.nz/~eibe/pubs/FouldsAndFrankMIreview.pdf)。[↩](#fnref37)
- en: ² If you want to get fancy, the context can be *anything* you think is valid.
    But it’s easiest to make the context a function of all inputs.[↩](#fnref38)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ² 如果你想变得复杂一些，上下文可以是你认为有效的任何东西。但最简单的方法是将上下文作为所有输入的函数。[↩](#fnref38)
- en: ³ Because v has one output, it is a common convention to write it as a vector
    instead of a matrix with one column.[↩](#fnref39)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 v 只有一个输出，所以通常将其写成向量而不是只有一个列的矩阵。[↩](#fnref39)
