- en: 14 Building custom ML transformers and estimators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 构建自定义ML转换器和评估器
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Creating your own transformers using Params for parameterization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Params进行参数化创建自己的转换器
- en: Creating your own estimators using the companion model approach
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用伴随模型方法创建自己的评估器
- en: Integrating custom transformers and estimators in an ML Pipeline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ML管道中集成自定义转换器和评估器
- en: In this chapter, we cover how to create and use custom transformers and estimators.
    While the ecosystem of transformers and estimators provided by PySpark covers
    a lot of frequent use cases and each version brings new ones to the table, sometimes
    you just need to go off trail and create your own. The alternative is to cut your
    pipeline in half and insert a data transformation function into the mix. This
    basically nullifies all the advantages (portability, self-documentation) of the
    ML pipeline that we covered in chapters 12 and 13.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将介绍如何创建和使用自定义的转换器和评估器。虽然PySpark提供的转换器和评估器生态系统涵盖了大量的常用用例，并且每个版本都会带来新的用例，但有时你可能需要另辟蹊径，创建自己的。另一种选择是将你的管道一分为二，并在其中插入一个数据转换函数。这基本上抵消了我们已在第12章和第13章中介绍过的ML管道的所有优势（可移植性、自文档化）。
- en: Because of how similar transformers and estimators are, we start with in-depth
    coverage of the transformer and its fundamental building block, the Param. We
    then move on to creating estimators, focusing on the differences in the transformer.
    Finally, we conclude with the integration of custom transformers and estimators
    in an ML pipeline, paying attention to serialization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换器和评估器非常相似，我们首先深入探讨转换器及其基本构建块——参数。然后，我们继续创建评估器，重点关注转换器之间的差异。最后，我们总结如何在ML管道中集成自定义转换器和评估器，并注意序列化。
- en: Before jumping into the content of this chapter, I strongly recommend reading
    chapters 12 and 13 and working through the examples and exercises. It is much
    easier to build a robust and useful transformer/estimator if you know how it is
    being used. I see custom transformers and estimators as a tool best used sparingly;
    always leverage the predefined PySpark components. Should you need to go off script,
    this chapter will guide you.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入本章内容之前，我强烈建议您阅读第12章和第13章，并完成示例和练习。如果您知道它是如何被使用的，那么构建一个健壮且有用的转换器/评估器会容易得多。我认为自定义转换器和评估器是一个最好谨慎使用的工具；始终利用预定义的PySpark组件。如果您需要另辟蹊径，本章将为您指引方向。
- en: 14.1 Creating your own transformer
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 创建自己的转换器
- en: This section covers how to create and use a custom transformer. We implement
    a `ScalarNAFiller` transformer that fills the `null` values of a column with a
    scalar value instead of the `mean` or `median` when using the `Imputer`. Thanks
    to this, our dessert pipeline from chapter 13 will have a `ScalarNAFiller` stage
    that we’ll be able to use when running different scenarios—when optimizing hyperparameters,
    for instance—without changing the code itself. This improves the flexibility and
    robustness of our ML experiments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何创建和使用自定义转换器。我们实现了一个`ScalarNAFiller`转换器，该转换器在`Imputer`中使用时，用标量值填充列中的`null`值，而不是使用`mean`或`median`。因此，我们的第13章中的甜点管道将有一个`ScalarNAFiller`阶段，我们可以在运行不同场景时使用，例如在优化超参数时，而无需更改代码本身。这提高了我们ML实验的灵活性和鲁棒性。
- en: 'Creating a custom transformer is not hard, but there are a lot of moving parts
    and a set of conventions to follow to make it consistent with the other transformers
    provided by PySpark. Our blueprint for this section follows this plan:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义转换器并不难，但有很多组成部分和一套需要遵循的约定，以确保它与PySpark提供的其他转换器保持一致。本节中的蓝图遵循以下计划：
- en: 'Design our transformer: Params, inputs, and outputs.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计我们的转换器：参数、输入和输出。
- en: Create the Params, inheriting some preconfigured ones as necessary.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Params，根据需要继承一些预配置的。
- en: Create the necessary getters and setters to get.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建必要的获取器和设置器以获取。
- en: Create the initialization function to instantiate our transformer.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建初始化函数以实例化我们的转换器。
- en: Create the transformation function.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建转换函数。
- en: '![](../Images/14-01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-01.png)'
- en: Figure 14.1 Our custom `ScalarNAFiller` blueprint, step 1
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 我们的定制`ScalarNAFiller`蓝图，步骤1
- en: Note Because we will be implementing the class in stages, some of the code blocks
    will not be able to be run in the REPL as is. Refer to the end of each section
    for an updated definition of the transformer with the new elements.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于我们将分阶段实现类，因此某些代码块将无法直接在REPL中运行。请参考每个部分的末尾，以获取包含新元素的转换器的更新定义。
- en: The PySpark `Transformer` class (`pyspark.ml.Transformer`; [http://mng.bz/y4Jq](http://mng.bz/y4Jq))
    provides many of the methods we used in chapter 13, such as `explainParams()`
    and `copy()`, plus a handful of other methods that will prove useful for implementing
    our own transformers. By sub-classing `Transformer`, we inherit all of this functionality
    for free, like we do in the following listing. This gives us a starting point!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的 `Transformer` 类 (`pyspark.ml.Transformer`; [http://mng.bz/y4Jq](http://mng.bz/y4Jq))
    提供了许多我们在第 13 章中使用的方法，例如 `explainParams()` 和 `copy()`，以及一些对我们实现自己的转换器非常有用的其他方法。通过继承
    `Transformer`，我们免费继承了所有这些功能，就像我们在下面的列表中所做的那样。这为我们提供了一个起点！
- en: Listing 14.1 The shell for the `ScalarNAFiller` transformer
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.1 `ScalarNAFiller` 转换器的壳
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The ScalarNAFiller class sub-classes the Transformer class, inheriting its
    generic methods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `ScalarNAFiller` 类是 `Transformer` 类的子类，继承了其通用方法。
- en: Before we start to code the rest of the transformer, let’s outline its parameterization
    and functionality. The next section reviews how to design a great transformer
    using Params and the transformation function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写转换器的其余代码之前，让我们概述其参数化和功能。下一节将回顾如何使用参数和转换函数设计一个优秀的转换器。
- en: '14.1.1 Designing a transformer: Thinking in terms of Params and transformation'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 设计转换器：从参数和转换的角度思考
- en: This section explains the relationship between the transformer, its Params,
    and the transformation function. By designing a transformer using these moving
    parts, we can ensure that our transformer is correct, robust, and consistent with
    the rest of the API pipeline.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了转换器、其参数和转换函数之间的关系。通过使用这些组成部分设计转换器，我们可以确保我们的转换器是正确的、健壮的，并且与 API 管道中的其他部分保持一致。
- en: In chapters 12 and 13, we saw that a transformer (and, by extension, an estimator)
    is configured through a collection of Params. The `transform()` function always
    takes a data frame as an input and returns a transformed data frame. We want to
    stay consistent with our design to avoid problems at use time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 12 章和第 13 章中，我们看到了转换器（以及由此扩展的估计器）是通过一组参数进行配置的。`transform()` 函数始终以数据框作为输入，并返回一个转换后的数据框。我们希望保持设计的一致性，以避免使用时出现问题。
- en: When designing a custom transformer, I always start by implementing a function
    that reproduces the behavior of my transformer. For the `ScalarNAFiller`, we leverage
    the `fillna()` function. I also create a sample data frame to test the behavior
    of my function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计自定义转换器时，我总是先实现一个函数来重现我的转换器的行为。对于 `ScalarNAFiller`，我们利用 `fillna()` 函数。我还创建了一个样本数据框来测试我的函数的行为。
- en: Listing 14.2 Creating a function that reproduces the transformer’s desired behavior
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.2 创建一个重现转换器期望行为的函数
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ null in column three has been replaced by -99, our filler value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第三列中的 `null` 已被替换为我们的填充值 -99。
- en: 'Through our design of the transformation function (which will prove useful
    in section 14.1.5), we immediately see that we need three Params in our `ScalarNAFiller`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们对转换函数的设计（这在第 14.1.5 节中将证明是有用的），我们立即看到在 `ScalarNAFiller` 中我们需要三个参数：
- en: '`inputCol` and `outputCol` are for the input and output columns, following
    the same behavior as the other transformers and estimators we’ve encountered thus
    far.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputCol` 和 `outputCol` 用于输入和输出列，遵循我们迄今为止遇到的其他转换器和估计器的相同行为。'
- en: '`filler` contains a floating-point number for the value that `null` will be
    replaced with during the transform() method.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filler` 包含一个浮点数，用于在 `transform()` 方法中将 `null` 替换为该值。'
- en: The data frame (`df` in listing 14.2) would get passed as an argument to the
    `transform()` method. Should we want to map this into the transformer blueprint
    introduced in chapter 13, it would look like figure 14.2.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框（列表 14.2 中的 `df`）将被传递给 `transform()` 方法作为参数。如果我们想将其映射到第 13 章中介绍的转换器蓝图，它将看起来像图
    14.2。
- en: '![](../Images/14-02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-02.png)'
- en: Figure 14.2 A blueprint for `ScalarNAFiller`. Three Params (`inputCol`, `outputCol`,
    `filler`) are necessary to configure its behavior. The `transform()` method provides
    the data frame, as with other transformers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 `ScalarNAFiller` 的蓝图。需要三个参数（`inputCol`、`outputCol`、`filler`）来配置其行为。`transform()`
    方法提供了数据框，就像其他转换器一样。
- en: I believe we are now ready to start coding on the `ScalarNAFiller` class. In
    this section, we designed our transformer by outlining the Params and created
    a function that reproduced the expected behavior of the `transform()` function.
    In the next section, we create the Params necessary for our transformer to operate.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们现在可以开始对`ScalarNAFiller`类进行编码了。在本节中，我们通过概述Params设计了我们的转换器，并创建了一个复制`transform()`函数预期行为的函数。在下一节中，我们将创建转换器操作所需的Params。
- en: 14.1.2 Creating the Params of a transformer
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 创建转换器的Params
- en: In this section, we create the three Params (`inputCol`, `outputCol`, `filler`)
    for the `ScalarNAFiller` transformer. We learn how to define a Param from scratch
    that will play well with other Params. We also leverage the predefined Param classes
    PySpark provides for common Params. Params drive the behavior of the transformer
    and estimator, and allow for easy customization when running a pipeline (e.g.,
    the cross-validation in chapter 13 where we provide Param maps to test different
    ML hyperparameters). It is therefore very important that we create them in a way
    that allows for that customization and self-documentation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们为`ScalarNAFiller`转换器创建了三个Params（`inputCol`、`outputCol`、`filler`）。我们学习了如何从头开始定义一个与其它Params良好协作的Param。我们还利用了PySpark为常见Params提供的预定义Param类。Params驱动转换器和估计器的行为，并在运行管道（例如，第13章中的交叉验证，我们提供了Param映射来测试不同的ML超参数）时允许轻松定制。因此，我们以允许这种定制和自文档化的方式创建它们非常重要。
- en: 'First, we start with the creation of a custom Param, our filling value `filler`.
    To create a custom Param, PySpark provides a `Param` class with four attributes:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从创建一个自定义Param开始，我们的填充值`filler`。为了创建一个自定义Param，PySpark提供了一个具有四个属性的`Param`类：
- en: A `parent`, which carries the value of the transformer once the transformer
    is instantiated.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`parent`，它在转换器实例化后携带转换器的值。
- en: A `name`, which is the name of our Param. By convention, we set it to the same
    name as our Param.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`name`，这是我们的Param的名称。按照惯例，我们将其设置为与我们的Param相同的名称。
- en: A `doc`, which is the documentation of our Param. This allows us to embed documentation
    for our Param when the transformer will be used.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`doc`，这是我们的Param的文档。这允许我们在转换器被使用时嵌入我们的Param的文档。
- en: A `typeConverter`, which governs the type of the Param. This provides a standardized
    way to convert an input value to the right type. It also gives a relevant error
    message if, for example, you expect a floating-point number, but the user of the
    transformer provides a string.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`typeConverter`，它控制Param的类型。这提供了一种标准化的方式将输入值转换为正确的类型。如果，例如，你期望一个浮点数，但转换器的用户提供了一个字符串，它还会给出相关的错误信息。
- en: In listing 14.3, we create a fully configured `filler`. Every custom Param we
    create needs to have `Params._dummy()` as a parent; this ensures that PySpark
    will be able to copy and change transformers' Params when you use or change them,
    for instance, during cross-validation (chapter 13). The name and doc are self-explanatory,
    so let’s spend a little more time on the `typeConverter`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在14.3列表中，我们创建了一个完全配置的`filler`。我们创建的每个自定义Param都需要以`Params._dummy()`作为父类；这确保了当你在使用或更改它们时，例如在交叉验证（第13章）期间，PySpark能够复制和更改转换器的Params。名称和文档是自解释的，所以让我们花更多的时间在`typeConverter`上。
- en: 'Type converters are the way we instruct the Param about the type of value it
    should expect. Think of them like value annotations in Python, but with the option
    to try to convert the value. In the case of the `filler`, we want a floating-point
    number, so we use `TypeConverters.toFloat`. There are many other options that
    are available. (Check the API reference to find the right one for your use case:
    [http://mng.bz/M2vn.](http://mng.bz/M2vn))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换器是我们指导Param期望值的类型的方式。想想看，它们就像Python中的值注解，但具有尝试转换值的选项。在`filler`的情况下，我们想要一个浮点数，所以我们使用`TypeConverters.toFloat`。还有许多其他选项可供选择。（检查API参考以找到适合您用例的正确选项：[http://mng.bz/M2vn.](http://mng.bz/M2vn))
- en: Listing 14.3 Creating the `filler` Param using the `Param` class
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3 使用`Param`类创建`filler` Param
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The parent is set to Params._dummy() to be consistent with other Params.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了与其他Params保持一致，父类被设置为Params._dummy()。
- en: ❷ Our Param name is set to the string value of the variable (filler).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的Param名称被设置为变量（filler）的字符串值。
- en: ❸ This is the documentation for our Param.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这是我们的Param的文档。
- en: ❹ We expect a floating point-like value for our Param.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们期望我们的Param参数是一个类似于浮点数的值。
- en: With three Params for our transformer, we would expect to repeat this process
    two more times, which is tedious, but feasible. Fortunately for us, PySpark provides
    some accelerated means to include commonly used Params without writing custom
    Param definitions. Since every transformer needs input and output columns, `inputCol`
    and `outputCol`, they belong in that category.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的转换器有三个参数，我们预计还需要重复这个过程两次，这虽然很繁琐，但却是可行的。幸运的是，PySpark 提供了一些加速手段，可以在不编写自定义参数定义的情况下包含常用参数。由于每个转换器都需要输入和输出列，即
    `inputCol` 和 `outputCol`，它们属于这一类别。
- en: 'Commonly used Params are defined in special classes called *Mixin* under the
    `pyspark.ml.param.shared` module. There is, at time of writing, no public documentation
    about this module, so you have to resort to reading the source code to see the
    Mixins available ([http://mng.bz/aDZB](http://mng.bz/aDZB)). As of Spark 3.2.0,
    34 are defined. The class for `inputCol` and `outputCol` Params are `HasInputCol`
    and `HasOutputCol`, respectively. The class in itself is nothing magical: it defines
    the Param (see the next listing for the complete code for `HasInputCol`) and provides
    an initialization and a getter function, which we cover in section 14.1.3.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 常用参数定义在名为 *Mixin* 的特殊类中，位于 `pyspark.ml.param.shared` 模块下。截至编写本文时，该模块没有公开文档，因此您必须求助于阅读源代码以查看可用的
    Mixins ([http://mng.bz/aDZB](http://mng.bz/aDZB))。截至 Spark 3.2.0，已定义了 34 个。`inputCol`
    和 `outputCol` 参数的类分别是 `HasInputCol` 和 `HasOutputCol`。这个类本身并没有什么神奇之处：它定义了参数（见下一列表中
    `HasInputCol` 的完整代码）并提供了一个初始化和一个获取函数，这些内容我们在 14.1.3 节中进行了介绍。
- en: Listing 14.4 The `HasInputCol` class looks very familiar
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.4 `HasInputCol` 类看起来非常熟悉
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The Param definition follows the same set of conventions as seen in a custom
    Param.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 参数定义遵循与自定义参数中看到的一套相同的约定。
- en: 'To use these accelerated Param definitions, we simply have to sub-class them
    in our transformer class definition. Our updated class definition now has all
    three Params defined: two of them through a Mixin (`inputCol`, `outputCol`), and
    one custom (`filler`).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些加速的参数定义，我们只需在我们的转换器类定义中对其进行子类化。我们的更新后的类定义现在已定义了所有三个参数：其中两个通过 Mixin（`inputCol`、`outputCol`），一个自定义（`filler`）。
- en: Listing 14.5 The `ScalarNAFiller` transformer with its three Params defined
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.5 定义了三个参数的 `ScalarNAFiller` 转换器
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ inputCol and outputCol are defined through Mixin classes, HasInputCol and
    HasOutputCol.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `inputCol` 和 `outputCol` 通过 Mixin 类，即 `HasInputCol` 和 `HasOutputCol` 定义。
- en: ❷ filler, as defined, has a custom Param.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如定义，填充器有一个自定义参数。
- en: Our `ScalarNAFiller` is getting closer to being usable now that the Params are
    defined. Following our plan outlined at the beginning of the section, the next
    logical step—and the subject of the next section—is to create the different getters
    and setters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `ScalarNAFiller` 的参数定义完成，我们现在可以更接近地使用它了。按照本节开头概述的计划，下一步逻辑步骤——也是下一节的主题——是创建不同的获取器和设置器。
- en: Tip What if you need more than one input/output column? See section 14.3.1,
    where we expand `ScalarNAFiller` to work on more than one column.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您需要超过一个输入/输出列，请参阅 14.3.1 节，其中我们将 `ScalarNAFiller` 扩展到可以处理多个列。
- en: '14.1.3 Getters and setters: Being a nice PySpark citizen'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.3 获取器和设置器：成为 PySpark 的好公民
- en: This section covers how to create getters and setters for a custom transformer.
    As seen in chapter 13, getters and setters are useful when we want to get or change
    the value of a Param. They provide a consistent interface to interact with the
    Param-eterization of a transformer or estimator.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何为自定义转换器创建获取器和设置器。如第 13 章所示，当我们想要获取或更改参数的值时，获取器和设置器非常有用。它们提供了一个一致的接口来与转换器或估计器的参数化进行交互。
- en: 'Based on the design of every PySpark transformer we have used so far, the simplest
    way to create setters is as follows: we first create a general method, `setParams()`,
    that allows us to change multiple parameters passed as keyword arguments (seen
    on the `continuous_assembler` transformer in chapter 13). Then, creating the setter
    for any other Param will simply call `setParams()` with the relevant keyword argument.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们迄今为止使用的每个 PySpark 转换器的设计，创建设置器的最简单方法是：我们首先创建一个通用方法 `setParams()`，它允许我们更改作为关键字参数传递的多个参数（在第
    13 章的 `continuous_assembler` 转换器中可以看到）。然后，为任何其他参数创建设置器只需调用 `setParams()` 并传递相关的关键字参数即可。
- en: The `setParams()` method is difficult to get right at first; it needs to accept
    any Params our transformer has and then update only those we are passing as arguments.
    Fortunately for us, we can leverage the approach the PySpark developers have used
    for other transformers and estimators. In listing 14.6, I provide the code for
    `setParams()`, adjusted for the `ScalarNAFiller`. If you look at the source code
    for any transformer or estimator provided by PySpark, you’ll see the same body
    of code, but with different arguments to the function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`setParams()`方法一开始很难正确实现；它需要接受我们的转换器拥有的任何Params，然后只更新我们作为参数传递的那些。幸运的是，我们可以利用PySpark开发者为其他转换器和估计器使用的方案。在列表14.6中，我提供了针对`ScalarNAFiller`调整的`setParams()`代码。如果你查看PySpark提供的任何转换器或估计器的源代码，你会看到相同的代码体，但函数的参数不同。'
- en: The `keyword_only` decorator provides the attribute `_input_kwargs`, which is
    a dictionary of the arguments passed to the function. For instance, if we were
    to call `setParams(inputCol="input",` `filler=0.0)`, `_input_kwargs` would be
    equal to `{"inputCol":` `"input",` `"filler":` `0.0}`. This attribute allows us
    to capture only the arguments we pass explicitly to `setParams()`, even if we
    pass `None` explicitly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`keyword_only`装饰器提供了属性`_input_kwargs`，它是一个包含传递给函数的参数的字典。例如，如果我们调用`setParams(inputCol="input",
    filler=0.0)`，则`_input_kwargs`将等于`{"inputCol": "input", "filler": 0.0}`。这个属性允许我们只捕获我们显式传递给`setParams()`的参数，即使我们显式地传递`None`。'
- en: The `Transformer` class[¹](#pgfId-1014334) has a `_set()` method that will update
    the relevant Params when passed a dictionary in the format `_input_kwargs` accepts.
    Handy!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`类[¹](#pgfId-1014334)有一个`_set()`方法，当传入一个符合`_input_kwargs`接受的格式字典时，会更新相关的Params。方便！'
- en: Listing 14.6 The `setParams()` method for the `ScalarNAFiller`
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.6 `ScalarNAFiller`的`setParams()`方法
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The keyword_only decorator provides the _input_kwargs attribute containing
    a dictionary of the arguments provided to setParams().
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `keyword_only`装饰器提供了包含传递给`setParams()`的参数字典的`_input_kwargs`属性。
- en: ❷ Our setParams() signature contains only the Params ScalarNAFiller has.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的`setParams()`签名只包含ScalarNAFiller拥有的Params。
- en: ❸ We finally use the _set() method provided by the superclass to update every
    Params from the _input_kwargs dictionary.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们最终使用由超类提供的`_set()`方法来更新从`_input_kwargs`字典中获取的每个Params。
- en: The setParams() method may look like magic when you first encounter it. Why
    not just use `**kwargs` and `_set()` directly? I think that the signature of `setParams()`
    is more clear when it contains only the Params that our transformer has. Also,
    if we input a typo (I often wrongly type `inputcol`—no capital letter—instead
    of `inputCol`), it’ll be caught at the function call rather than much later when
    we call `_set()`. I think that the trade-offs are worthwhile.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次遇到`setParams()`方法时，它可能看起来像魔法。为什么不直接使用`**kwargs`和`_set()`呢？我认为当`setParams()`只包含我们的转换器拥有的Params时，它的签名更清晰。此外，如果我们输入了拼写错误（我经常错误地输入`inputcol`——没有大写字母——而不是`inputCol`），它将在函数调用时被捕获，而不是在我们调用`_set()`时很久之后。我认为这种权衡是值得的。
- en: 'Tip If you create a custom transformer and you forget how to create `setParams()`,
    check out any transformer in PySpark’s source code: they all implement this method
    the same way!'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你创建了一个自定义转换器并且忘记了如何创建`setParams()`，可以查看PySpark源代码中的任何转换器：它们都以相同的方式实现这个方法！
- en: 'With `setParams()` cleared out, it’s time to create the individual setters.
    That couldn’t be easier: simply call `setParams()` with the appropriate argument!
    In listing 14.4, we saw that, while the getter for `inputCol` is provided, the
    setter is not because it would imply creating a generic `setParams()` that we’d
    override anyway. Fear not, it’s just a few more lines of boilerplate code.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`setParams()`清理完毕后，是时候创建单独的设置器了。这很简单：只需使用适当的参数调用`setParams()`！在列表14.4中，我们看到了虽然提供了`inputCol`的获取器，但设置器没有提供，因为这会意味着创建一个通用的`setParams()`，我们最终会覆盖它。不用担心，这只需要几行样板代码。
- en: Listing 14.7 Individual setters for `ScalarNAFiller`
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.7 `ScalarNAFiller`的单独设置器
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ All three setX() methods use the setParams() blueprint.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有三个`setX()`方法都使用`setParams()`蓝图。
- en: The setters are done! Now it’s time for the getters. Unlike setters, getters
    for Mixin are already provided, so we only have to create `getFiller()`. We also
    do not have to create a generic `getParams()`, since the `Transformer` class provides
    `explainParam` and `explainParams` instead.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 设置器已完成！现在轮到获取器了。与设置器不同，Mixin的获取器已经提供，所以我们只需要创建`getFiller()`。我们也不必创建通用的`getParams()`，因为`Transformer`类提供了`explainParam`和`explainParams`。
- en: The Mixin definition of listing 14.4 kind of spoiled the surprise for us by
    providing a blueprint for the getter’s syntax. We leverage the `getOrDefault()`
    method provided by the superclass to return the relevant value to the caller in
    the next listing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.4 中的 Mixin 定义通过提供获取器语法的蓝图，有点破坏了惊喜。我们利用超类提供的 `getOrDefault()` 方法在下一列表中返回相关值给调用者。
- en: Listing 14.8 The `getFiller()` method of `ScalarNAFiller`
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.8 `ScalarNAFiller` 的 `getFiller()` 方法
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If we put out code together, our transformer looks like the code in listing
    14.9\. We have our Param definition (elided to avoid cluttering the listing),
    as well as a general setter (`setParam()`), three individual setters (`setInputCol()`,
    `setOutputCol()`, `setFiller()`), and an explicit getter (`getFiller()`; `getInputCol()`
    and `getOutputCol()` are provided by the Mixin classes).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将代码放在一起，我们的转换器看起来就像列表 14.9 中的代码。我们有自己的 Param 定义（省略以避免列表混乱），以及一个通用设置器（`setParam()`），三个单独的设置器（`setInputCol()`、`setOutputCol()`、`setFiller()`），以及一个显式的获取器（`getFiller()`；`getInputCol()`
    和 `getOutputCol()` 由 Mixin 类提供）。
- en: Listing 14.9 The `ScalarNAFiller` with its getters and setters defined
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.9 定义了 `ScalarNAFiller` 及其获取器和设置器
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this section, we covered the creation of getters and setters for our custom
    transformer, leveraging some of the templates and Mixin classes PySpark provides
    to reduce boilerplate code. The next section covers the initialization function
    that will enable us to instantiate and therefore use our transformer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了为我们的自定义转换器创建获取器和设置器，利用 PySpark 提供的一些模板和 Mixin 类来减少样板代码。下一节将介绍初始化函数，这将使我们能够实例化并因此使用我们的转换器。
- en: 14.1.4 Creating a custom transformer’s initialization function
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.4 创建自定义转换器的初始化函数
- en: This section covers the initialization code for our transformer. If we want
    to create an instance of a class in Python, an initialization method is the simplest
    and most common way to proceed. We cover how to interact with the Param map and
    how to use the PySpark helper function to create an API consistent with the other
    PySpark transformers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了我们的转换器的初始化代码。如果我们想在 Python 中创建一个类的实例，初始化方法是最简单和最常见的方法。我们介绍了如何与 Param 映射交互以及如何使用
    PySpark 辅助函数创建与其他 PySpark 转换器一致的 API。
- en: At the core, initializing a transformer means nothing more than initializing
    the superclasses of the transformer and setting the Param map accordingly. Just
    like `setParams()`, `__init__()` is defined for every transformer and estimator,
    so we can take inspiration from the ones provided by PySpark.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，初始化转换器意味着 nothing more than 初始化转换器的超类并相应地设置 Param 映射。就像 `setParams()` 一样，`__init__()`
    为每个转换器和估计器定义，因此我们可以从 PySpark 提供的示例中汲取灵感。
- en: 'The `__init__()` method of the `SparkNAFiller`, shown in listing 14.10, performs
    the following tasks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.10 中所示的 `SparkNAFiller` 的 `__init__()` 方法执行以下任务：
- en: Instantiate every superclass `ScalarNAFiller` inherits from via the `super()`
    function.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `super()` 函数实例化 `ScalarNAFiller` 继承的每个超类。
- en: Call `setDefault()` on the custom Param we created. Because of the `keyword_
    only` decorator, we need `setDefault()` to set the default value for the `filler`
    Param. `inputCol` and `outputCol` are covered by the `__init__()` method in `HasInputCol`
    and `HasOutputCol`, respectively (see listing 14.4).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们创建的自定义 Param 上调用 `setDefault()`。由于 `keyword_only` 装饰器，我们需要 `setDefault()`
    来设置 `filler` Param 的默认值。`inputCol` 和 `outputCol` 分别由 `HasInputCol` 和 `HasOutputCol`
    中的 `__init__()` 方法处理（参见列表 14.4）。
- en: Extract the `_input_kwargs` and call `setParams()` to set the Params passed
    to the `__init__()` method to set the Params to the value passed to the class
    constructor.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取 `_input_kwargs` 并调用 `setParams()` 来设置传递给 `__init__()` 方法的 Params，以便将 Params
    设置为传递给类构造函数的值。
- en: Listing 14.10 The `ScalarNAFiller` initializer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.10 `ScalarNAFiller` 的初始化器
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ This will call the __init__() method of Transformer, then HasInputCol, then
    HasOutputCol.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这将调用 Transformer 的 __init__() 方法，然后是 HasInputCol，然后是 HasOutputCol。
- en: ❷ We set the default value for the Param filler, since keyword_only hijacks
    the regular default argument capture.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们为 Param 填充器设置了默认值，因为 `keyword_only` 会拦截常规默认参数捕获。
- en: ❸ Here, we could have called _set(), but other PySpark transformers use setParams().
    Both would work.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这里，我们可以调用 _set()，但其他 PySpark 转换器使用 setParams()。两者都行得通。
- en: All the boilerplate code for our custom transformer is done! Just like for getters
    and setters, taking inspiration from the existing PySpark transformers ensures
    that our code is consistent and easy to deduce. The next section covers the transformation
    function; our transformer will then be fully functional!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自定义转换器的所有样板代码都已经完成了！就像对于获取器和设置器一样，从现有的PySpark转换器中汲取灵感确保了我们的代码是一致的并且易于推断。下一节将涵盖转换函数；然后我们的转换器将完全可用！
- en: 14.1.5 Creating our transformation function
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.5 创建我们的转换函数
- en: This section covers the creation of our transformation function. This function
    is certainly the most important of our transformer, as it performs the actual
    work of transforming the data frame. I explain how to create a robust transformation
    function using the Params values and how to deal with improper inputs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了我们的转换函数的创建。这个函数无疑是我们的转换器中最重要的一部分，因为它执行了实际的数据帧转换工作。我解释了如何使用Params值创建一个健壮的转换函数，以及如何处理不合适的输入。
- en: 'In chapters 12 and 13, I explained how a transformer modifies the data via
    the `transform()` method. The `Transformer` class, on the other hand, expects
    the programmer to provide a `_transform()` method (note the trailing underscore).
    The difference is subtle: PySpark provides a default implementation for `transform()`
    that allows for an optional argument, `params`, in case we want to pass a Param
    map at transformation time (similar to the Param maps we encountered in chapter
    13 with the `ParamGridBuilder` and the `CrossValidator`). `transform()` ends up
    calling `_transform()`, which takes a single argument, `dataset`, and performs
    the actual data transformation.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章和第13章中，我解释了转换器如何通过`transform()`方法修改数据。另一方面，`Transformer`类期望程序员提供一个`_transform()`方法（注意尾随的下划线）。区别是微妙的：PySpark为`transform()`提供了一个默认实现，允许在转换时传递一个可选的参数`params`，以防我们想在转换时传递一个Param映射（类似于我们在第13章中遇到的`ParamGridBuilder`和`CrossValidator`中的Param映射）。`transform()`最终会调用`_transform()`，它接受一个参数`dataset`并执行实际的数据转换。
- en: Because we already have a working function (`scalarNAFillerFunction` that we
    created in listing 14.2), implementing the `_transform()` method is a piece of
    cake! The method is shown in listing 14.11, with a few details worth going over.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有了一个工作函数（在14.2列表中创建的`scalarNAFillerFunction`），实现`_transform()`方法就轻而易举了！该方法在14.11列表中展示，其中有一些值得注意的细节。
- en: First, should we want to validate any Param (e.g., making sure that `inputCol`
    is set), we would do it at `_transform()` time by using the `isSet()` method (provided
    by the superclass) and throwing an exception if it hasn’t been explicitly set.
    If we do it earlier, we risk running into problems when writing/loading custom
    transformers, like we do in section 14.3.2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果我们想验证任何Params（例如，确保`inputCol`已设置），我们将在`_transform()`时通过使用由超类提供的`isSet()`方法来完成，如果没有明确设置，则抛出异常。如果我们提前这样做，我们可能会在编写/加载自定义转换器时遇到问题，就像我们在14.3.2节中所做的那样。
- en: We then set some explicit variables for the three Params of the transformer
    using the individuals getters. `output_column` and `na_filler` represent the `outputCol`
    and `filler` Params, respectively. For `input_column`, representing the `inputCol`
    Param value (a string), we promote it to a column object on the `dataset` using
    the bracket notation; this makes it consistent with our prototype function and
    simplifies the `return` clause of our method. Since the `filler` Param is meant
    to be a double, I explicitly cast `input_column` as a `double` to ensure that
    the `fillna()` method will work. Since `outputCol` and `filler` have default values,
    we just need to test for `inputCol` being set by the user, throwing an exception
    if not.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用个体获取器为转换器的三个Params设置了一些明确的变量。`output_column`和`na_filler`分别代表`outputCol`和`filler`Params。对于代表`inputCol`Params值（一个字符串）的`input_column`，我们使用括号符号将其提升为`dataset`上的列对象；这使得它与我们的原型函数保持一致，并简化了方法中的`return`子句。由于`filler`Params预期是一个双精度浮点数，我明确地将`input_column`转换为`double`以确保`fillna()`方法能够工作。由于`outputCol`和`filler`有默认值，我们只需要检查用户是否设置了`inputCol`，如果没有设置，则抛出异常。
- en: Listing 14.11 The `_transform()` method of the `ScalarNAFiller` transformer
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.11 `ScalarNAFiller`转换器的`_transform()`方法
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ We raise a ValueError if inputCol isn’t set by the user.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果用户没有设置inputCol，我们将引发一个ValueError。
- en: With the `_transform()` method done, we have a fully functional transformer!
    The entire code is displayed in the next listing. The next section demonstrates
    that our transformer works as expected, so we can congratulate ourselves on a
    job well done.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`_transform()` 方法完成后，我们有一个完全功能的转换器！整个代码在下一列表中显示。下一节将演示我们的转换器按预期工作，因此我们可以对自己完成的工作表示祝贺。'
- en: Listing 14.12 The source code for the `ScalarNAFiller`
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.12 `ScalarNAFiller` 的源代码
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Custom Param definition (section 14.1.2)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 自定义参数定义（第 14.1.2 节）
- en: ❷ Initializer method (section 14.1.4)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化方法（第 14.1.4 节）
- en: ❸ General setParams() method (section 14.1.3)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一般的 `setParams()` 方法（第 14.1.3 节）
- en: ❹ Individuals setters (section 14.1.3)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 单个 setter（第 14.1.3 节）
- en: ❺ Individual getter (only for the custom Param, section 14.1.3)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 单个 getter（仅针对自定义参数，第 14.1.3 节）
- en: ❻ Transformation method (section 14.1.5)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 转换方法（第 14.1.5 节）
- en: 14.1.6 Using our transformer
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.6 使用我们的转换器
- en: Now that we have a custom transformer in our pocket, it’s time to use it! In
    this section, we ensure that the `ScalarNAFiller` transformer works as expected.
    To do so, we’ll instantiate it, set its Params, and use the transformation method.
    I don’t think I need to convince you that you need try your code once written.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个自定义转换器在手中，是时候使用它了！在本节中，我们确保 `ScalarNAFiller` 转换器按预期工作。为此，我们将实例化它，设置其参数，并使用转换方法。我认为我无需说服你，一旦代码编写完成，你需要亲自尝试一下。
- en: We already saw how a transformer is instantiated and used in chapter 12 and
    13, so we can jump right in.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第 12 章和第 13 章中看到了转换器的实例化和使用方法，因此我们可以直接进入。
- en: Listing 14.13 Instantiating and testing the `ScalarNAFiller` transformer
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.13 实例化和测试 `ScalarNAFiller` 转换器
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Because we inherit from `HasInputCol` and `HasOutputCol`, for conciseness,
    I skip the testing of changing `inputCol` or `outputCol` and instead focus on
    `filler`. In listing 14.14 and figure 14.3, I show two methods of changing the
    Params, which should yield the same behavior:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们继承了 `HasInputCol` 和 `HasOutputCol`，为了简洁起见，我跳过了更改 `inputCol` 或 `outputCol`
    的测试，而是专注于 `filler`。在列表 14.14 和图 14.3 中，我展示了两种更改参数的方法，它们应该产生相同的行为：
- en: Using the explicit `setFiller()`, which calls `setParams()` under the hood
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用显式的 `setFiller()`，它内部调用 `setParams()`
- en: Passing a Param map to the `transform()` method, which overrides the default
    Param map
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将参数映射传递给 `transform()` 方法，该映射覆盖默认参数映射
- en: '![](../Images/14-03.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-03.png)'
- en: Figure 14.3 By setting the `filler` Param explicitly, we modify the transformer
    in place permanently. We can also temporarily set new Params in the `transform()`
    method to test different scenarios without modifying the original transformer.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 通过显式设置 `filler` 参数，我们永久地修改了转换器。我们还可以在 `transform()` 方法中临时设置新的参数，以测试不同的场景而不修改原始转换器。
- en: In practice, both scenarios yield the same results; the difference is in what
    the transformer looks like after the operation. When using `setFiller()` explicitly,
    we modify `test_ScalarNAFiller` in place, setting `filler` to 17 before performing
    the transformation. In the `transform()` approach, with a Param map, we temporarily
    override the `filler` Param without changing the `test_ScalarNAFiller` in place.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这两种场景都产生相同的结果；区别在于操作后转换器的样子。当显式使用 `setFiller()` 时，我们在位置上修改了 `test_ScalarNAFiller`，在执行转换之前将
    `filler` 设置为 17。在 `transform()` 方法中，使用参数映射，我们临时覆盖了 `filler` 参数，而没有更改 `test_ScalarNAFiller`
    的位置。
- en: Listing 14.14 Testing changes to the `filler` Param
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.14 测试对 `filler` 参数的更改
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ We modify test_ScalarNAFiller in place.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在位置上修改了 `test_ScalarNAFiller`。
- en: ❷ We temporarily override the filler Param without changing test_ScalarNAFiller
    in place.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们临时覆盖了填充参数，而没有更改 `test_ScalarNAFiller` 的位置。
- en: The transformer is done! Not only have we learned how to create a custom transformer
    from scratch, but we have a head start on the next section, where we cover how
    to create a custom estimator.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器已完成！我们不仅学习了如何从头开始创建自定义转换器，而且为下一节打下了基础，下一节将介绍如何创建自定义估计器。
- en: 14.2 Creating your own estimator
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 创建自己的估计器
- en: Transformers and estimators goes hand in hand in an ML pipeline. In this section,
    we build on the knowledge of creating custom transformers (Params, getters/setters,
    initializers, and transform functions) to build a custom estimator. Custom estimators
    are useful when you outgrow the set of estimators provided by PySpark, but still
    want to keep all your steps within an ML pipeline. Just like in chapter 13, we
    focus on custom estimators by giving more attention to where they differ from
    custom transformers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习管道中，转换器和估计器是相辅相成的。在本节中，我们基于创建自定义转换器的知识（参数、获取器/设置器、初始化器和转换函数）来构建自定义估计器。当你的需求超出PySpark提供的估计器集时，自定义估计器非常有用，但你仍然希望将所有步骤保持在机器学习管道内。就像在第13章中一样，我们通过更多地关注它们与自定义转换器的不同之处来关注自定义估计器。
- en: In this section, we create an `ExtremeValueCapper` estimator. This estimator
    is similar to the capping operation we’ve done on calories, protein, and fat when
    preparing data for our dessert classification model (chapter 12), but rather than
    using the 99th percentile, `ExtremeValueCapper` caps the values that are beyond
    the average of the column, plus or minus a multiple of the standard deviation.
    For instance, if the average of the values in our column is 10, the standard deviation
    is 2, and the multiple is 3, we will floor the values lower than 4 (or 10 – 2
    × 3) and cap the values larger than 16 (or 10 + 2 × 3). Since the computation
    of the average and standard deviation is dependent on the input column, we need
    an estimator rather than a transformer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了一个`ExtremeValueCapper`估计器。这个估计器与我们为准备甜点分类模型（第12章）数据时对卡路里、蛋白质和脂肪进行的上限操作类似，但`ExtremeValueCapper`不是使用第99百分位数，而是将超出列平均值的值以及标准差的倍数上限。例如，如果我们的列中值的平均值为10，标准差为2，倍数为3，我们将对低于4（或10
    - 2 × 3）的值进行下限处理，并对高于16（或10 + 2 × 3）的值进行上限处理。由于平均数和标准差的计算依赖于输入列，我们需要一个估计器而不是转换器。
- en: 'Our action plan for this section is very similar to the transformer:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本节的活动计划与转换器非常相似：
- en: 'Outline the design or the estimator, taking into account the resulting model:
    inputs, outputs, `fit()`, and `transform()`.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概述估计器的设计，考虑到结果模型：输入、输出、`fit()`和`transform()`。
- en: Create the companion model class as a `Model` (which is a specialized `Transformer`)
    subclass.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个作为`Model`（这是一个特殊的`Transformer`）子类的伴随模型类。
- en: Create the estimator as an `Estimator` subclass.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将估计器创建为`Estimator`子类。
- en: Let’s start with the design.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设计开始。
- en: '14.2.1 Designing our estimator: From model to params'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.1 设计我们的估计器：从模型到参数
- en: This section covers the design of the estimator before we start coding. Just
    like the design of a custom transformer (section 14.1.1), I cover how to design
    your estimator from the desired outputs to the inputs, making sure that the design
    is logical and sound.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了我们在开始编码之前估计器的设计。就像自定义转换器的设计（第14.1.1节）一样，我介绍了如何从期望的输出到输入设计你的估计器，确保设计是逻辑上合理和可靠的。
- en: For an estimator to be a transformer-creating machine, we need the `fit()` method
    to return a fully parameterized transformer. When designing an estimator, it therefore
    makes sense to start the design of an estimator by building the returned `Model`,
    which I call the “companion model,” which dictates how the estimator needs to
    be configured, not the other way around.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要使估计器成为一个创建转换器的机器，我们需要`fit()`方法返回一个完全参数化的转换器。因此，在设计估计器时，从构建返回的`Model`开始设计估计器是有意义的，我称之为“伴随模型”，它决定了估计器应该如何配置，而不是反过来。
- en: 'In the case of our `ExtremeValueCapper`, the resulting transformer is akin
    to a boundary guard: given a floor and a cap value'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`ExtremeValueCapper`的情况下，生成的转换器类似于边界守护者：给定一个底值和一个上限值
- en: Any value in our column lower than the floor will be changed with the floor
    value
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们列中低于底值的任何值都将被底值替换
- en: Any value in our column higher than the cap will be changed with the cap value
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们列中高于上限的任何值都将被上限值替换
- en: 'In figure 14.4, I draw my transformer flow, called `ExtremeValueCapperModel`,
    illustrating the relevant Params, named `inputCol`, `outputCol`, `cap`, and `floor`.
    I also highlight the Params in two categories:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在图14.4中，我绘制了我的转换器流程，称为`ExtremeValueCapperModel`，展示了相关的参数，名为`inputCol`、`outputCol`、`cap`和`floor`。我还突出了两个类别的参数：
- en: The *implicit* Params, which are inferred from the data itself
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐式*参数，这些参数是从数据本身推断出来的'
- en: the *explicit* Params, which are not data-dependent and need to be provided
    explicitly via the construction of the estimator
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*明确的* 参数，它们不依赖于数据，并且需要通过估计器的构建显式提供。'
- en: '![](../Images/14-04.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-04.png)'
- en: Figure 14.4 The high-level design of `ExtremeValueCapperModel`, showing the
    explicit and implicit Params
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 `ExtremeValueCapperModel` 的高级设计，显示了显式和隐式参数
- en: For the `ExtremeValueCapperModel`, `cap` and `floor` are implicit Params since
    they are computed using the average and standard deviation of the input column.
    The `inputCol` and `outputCol` Params are explicit.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `ExtremeValueCapperModel`，`cap` 和 `floor` 是隐式参数，因为它们是使用输入列的平均值和标准差计算的。`inputCol`
    和 `outputCol` 参数是显式的。
- en: 'With the model design out of the way, we can backtrack to the estimator design
    itself. Our estimator needs to have all the explicit Params required by the companion
    model, so it can pass them through. For our model-implicit Params, they need to
    be computed from the input column(s) and the Params of the estimator. In our case,
    we only need a single additional Param, which I name `boundary`, to compute both
    the `cap` and `floor` of the `ExtremeValueCapperModel`. The design is displayed
    in figure 14.5, and the test functions (just like we did for the transformer in
    section 14.1.1) are in listing 14.15\. This time, I create two functions: one
    for the functionality of the `fit()` method of the estimator and one for the functionality
    of the `transform()` method of the companion model.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型设计完成之后，我们可以回溯到估计器设计本身。我们的估计器需要拥有伴随模型所需的所有显式参数，以便将其传递出去。对于我们的模型隐式参数，它们需要从输入列和估计器的参数中计算得出。在我们的例子中，我们只需要一个额外的参数，我将其命名为
    `boundary`，用于计算 `ExtremeValueCapperModel` 的 `cap` 和 `floor`。设计在图 14.5 中显示，测试函数（就像我们在
    14.1.1 节中为转换器所做的那样）在列表 14.15 中。这次，我创建了两个函数：一个用于估计器的 `fit()` 方法的功能，另一个用于伴随模型的 `transform()`
    方法的功能。
- en: '![](../Images/14-05.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-05.png)'
- en: Figure 14.5 The design of the `ExtremeValueCapper` estimator, along with its
    Params and the resulting companion model
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 `ExtremeValueCapper` 估计器的设计，包括其参数和生成的伴随模型
- en: The `test_ExtremeValueCapper_transform()` function takes all four Params, `inputCol`,
    `outputCol`, `cap`, and `floor`, as arguments (plus the data frame `df`), and
    returns a data frame with an additional column floored and capped to the proper
    values. The `test_ExtremeValueCapper_fit()` function takes `inputCol`, `outputCol`,
    and `boundary` as arguments (plus the data frame `df`) and computes `cap` and
    `floor` using the average (`avg`) and standard deviation (`stddev`) of the input
    column. The function returns `test_ExtremeValueCapper_transform()` applied to
    the same data frame, with all the Params, both implicit and explicit, computed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_ExtremeValueCapper_transform()` 函数接受所有四个参数，`inputCol`、`outputCol`、`cap`
    和 `floor`（以及数据框 `df`），并返回一个包含额外列的、已对正确值进行地板和上限的 DataFrame。`test_ExtremeValueCapper_fit()`
    函数接受 `inputCol`、`outputCol` 和 `boundary` 作为参数（以及数据框 `df`），并使用输入列的平均值 (`avg`) 和标准差
    (`stddev`) 计算出 `cap` 和 `floor`。该函数返回应用于相同数据框的 `test_ExtremeValueCapper_transform()`，其中计算了所有参数，包括隐式和显式参数。'
- en: 'Tip If we wanted `fit()` to return a `test_ExtremeValueCapper_transform()`
    function pre-Param-eterized and ready to apply to a new data frame, we could have
    used the same mechanics as a transform-enabled function. This very useful functionality
    requires a little more Python gymnastics and is covered in appendix C, under “Transform-Enabled
    Function: Functions Returning Functions.”'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果我们想让 `fit()` 返回一个预先参数化并准备好应用于新数据框的 `test_ExtremeValueCapper_transform()`
    函数，我们可以使用与启用转换功能的函数相同的机制。这个非常有用的功能需要一些额外的 Python 技巧，并在附录 C 中介绍，标题为“转换启用函数：返回函数的函数”。
- en: Listing 14.15 Blueprint functions for the `ExtremeValueCapper` companion model
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.15 `ExtremeValueCapper` 伴随模型的蓝图函数
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Using when() makes our code more verbose, but more explicit than using F.min(F.max(inputCol,
    floor), cap).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 when() 使得我们的代码更加冗长，但比使用 F.min(F.max(inputCol, floor), cap) 更加明确。
- en: ❷ head() returns the first (and only) record of the aggregated data frame as
    a Row object, which we can bind to each field using de-structuring.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ head() 返回聚合数据框的第一个（也是唯一一个）记录作为一个 Row 对象，我们可以使用解构将其绑定到每个字段。
- en: ❸ We compute cap and floor using avg and stddev, which depend on the data, and
    boundary, one of the estimator’s Params.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们使用平均值和标准差计算 cap 和 floor，这些值依赖于数据，以及边界，这是估计器参数之一。
- en: ❹ We return the application of the companion model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们返回伴随模型的运用。
- en: With the design of both our estimator and companion model in the bag, we can
    now get to coding. In the next section, we implement the companion model class,
    using a trick for separating Params from implementation code.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的估计器和伴随模型的设计都准备好之后，我们现在可以开始编码了。在下一节中，我们将实现伴随模型类，使用一个技巧来分离Params和实现代码。
- en: '14.2.2 Implementing the companion model: Creating our own Mixin'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.2 实现伴随模型：创建我们自己的Mixin
- en: In this section, we implement the companion model, `ExtremeValueCapperModel`,
    which is akin to a transformer. Because this is an identical process to the implementation
    of the `ScalarNAFiller` from section 14.1.1, we introduce an additional trick
    to make our code more modular by separating Params from implementation and creating
    our own Param Mixin. When creating an estimator and its companion model, it is
    customary to propagate the Params of the estimator to the companion model, even
    if they are not used. In our case, it means that `boundary` will be added to the
    Params of `ExtremeValueCapperModel`. In order to make our code clearer and more
    terse, we can implement a Mixin (which in Python is a regular class) for both
    `ExtremeValueCapper` and `ExtremeValueCapperModel` to inherit from.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现了伴随模型`ExtremeValueCapperModel`，它类似于一个Transformer。因为这个过程与14.1.1节中`ScalarNAFiller`的实现是相同的，我们引入了一个额外的技巧，通过将Params与实现分离并创建我们自己的Param
    Mixin来使我们的代码更加模块化。在创建估计器和其伴随模型时，通常会将估计器的Params传播到伴随模型，即使它们没有被使用。在我们的例子中，这意味着`boundary`将被添加到`ExtremeValueCapperModel`的Params中。为了使我们的代码更清晰和简洁，我们可以为`ExtremeValueCapper`和`ExtremeValueCapperModel`实现一个Mixin（在Python中是一个常规类），以便它们可以继承。
- en: Note We won’t implement a `setBoundary()` method on the companion model, as
    we do not want to change that Param once we’ve computed the cap and floor values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们不会在伴随模型上实现`setBoundary()`方法，因为我们不希望在计算了上限和下限值之后改变这个Param。
- en: 'Creating a Mixin is very similar to creating half a transformer:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Mixin与创建半个Transformer非常相似：
- en: Inherit from any Mixin we wish to add the Params from (e.g. `HasInputCol`, `HasOutputCol`).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们希望添加Params的任何Mixin继承（例如，`HasInputCol`，`HasOutputCol`）。
- en: Create the custom Param(s) and their getter(s).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建自定义Param及其getter。
- en: Create the `__init__()` function.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`__init__()`函数。
- en: '![](../Images/14-06.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-06.png)'
- en: Figure 14.6 The design of our `_ExtremeValueCapperParams` Mixin, complete with
    inheritance from two Mixins
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：我们的`_ExtremeValueCapperParams` Mixin的设计，包括从两个Mixin继承
- en: The only slight difference here is in the signature of the `__init__()` method.
    Because this Mixin will not be directly called—the call will come from when we
    call `super()` in a class that inherits from our Mixin—we need to accept the arguments
    of any downstream transformer, model, or estimator. In Python, we simply do this
    by passing `*args` to our initializer. Because each transformer calling this Mixin
    as a superclass might have different arguments (and we don’t use them), we capture
    them under `*args` and call `super()` with those same arguments. Finally, we `_setDefault()`
    for our custom Param `boundary`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的唯一细微差别在于`__init__()`方法的签名。因为这个Mixin不会被直接调用——调用将在我们调用继承自我们的Mixin的类的`super()`时发生——我们需要接受任何下游Transformer、模型或估计器的参数。在Python中，我们通过将`*args`传递给初始化器来简单地做到这一点。因为每个调用这个Mixin作为超类的Transformer可能有不同的参数（而我们并不使用它们），所以我们把它们捕获在`*args`下，并使用相同的参数调用`super()`。最后，我们为我们的自定义Param
    `boundary`调用`_setDefault()`。
- en: Listing 14.16 The `_ExtremeValueCapperParams` Mixin implementation
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.16：`_ExtremeValueCapperParams` Mixin实现
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Ensure proper superclass call hierarchy by capturing them all under *args.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过将它们全部捕获在*args下，确保适当的超类调用层次结构。
- en: ❷ Just like when initializing a transformer, we use _setDefault() to set the
    default value of the boundary Param.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 就像初始化一个Transformer一样，我们使用setDefault()来设置边界Param的默认值。
- en: ❸ Mixin customarily provides the getter as part of the class definition. We
    reuse the same plumbing as with any getter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Mixin通常作为类定义的一部分提供getter。我们重用与任何getter相同的管道。
- en: Tip The syntax for the `__init()__` method for a Mixin is the same (besides
    `_setDefault()`, which will take the Param(s) of the Mixin(s) for every one of
    them, including those provided by PySpark). You can refer to the source code of
    an existing Mixin as a reminder.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：Mixin的`__init()__`方法的语法是相同的（除了`_setDefault()`，它将为每个Mixin（包括由PySpark提供的）的Param(s)取值）。您可以参考现有Mixin的源代码作为提醒。
- en: Now we can now implement the full model, inheriting from `Model` (rather than
    the transformer, since we want the added knowledge that this is a model) and `_ExtremeValueCapperParams`.
    To keep things a little more clean, I’ve elided the getters and setters. The full
    code for every transformer and estimator in this chapter is available in the book’s
    companion repository.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现完整的模型了，它继承自 `Model`（而不是 transformer，因为我们希望添加这样的知识，即这是一个模型）以及 `_ExtremeValueCapperParams`。为了使代码更加简洁，我已经省略了获取器和设置器。本章中每个
    transformer 和估计器的完整代码都可以在本书的配套仓库中找到。
- en: Listing 14.17 The source code for the `ExtremeValueCapperModel`
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.17 `ExtremeValueCapperModel` 的源代码
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ We inherit from the Model superclass, as well as our Mixin (which includes
    HasInputCol and HasOutputCol), so we don’t have to list them again here.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们从 Model 超类以及我们的 Mixin（包括 HasInputCol 和 HasOutputCol）继承，因此我们在这里不再列出它们。
- en: Although our model is not meant to be used as is—it should only be the output
    of the `fit()` method—nothing prevents our users or ourselves from importing `ExtremeValueCapperModel`
    and using it directly, passing direct values to `cap` and `floor` rather than
    computing them. Because of this, I code my companion models just like any free-standing
    transformer, checking the appropriate Params in the `transform()` method.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的模型不是为了直接使用——它应该是 `fit()` 方法的输出——但没有任何东西阻止我们的用户或我们自己导入 `ExtremeValueCapperModel`
    并直接使用它，将直接值传递给 `cap` 和 `floor` 而不是计算它们。正因为如此，我像任何独立的 transformer 一样编写我的伴随模型，在
    `transform()` 方法中检查适当的 Params。
- en: In this section, we created the companion model `ExtremeValueCapperModel`, as
    well as an `_ExtremeValueCapperParams` Mixin. Now we are ready to tackle the creation
    of the `ExtremeValueCapper` estimator.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了伴随模型 `ExtremeValueCapperModel` 以及 `_ExtremeValueCapperParams` Mixin。现在我们准备好处理
    `ExtremeValueCapper` 估计器的创建。
- en: 14.2.3 Creating the ExtremeValueCapper estimator
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.3 创建 `ExtremeValueCapper` 估计器
- en: 'This section covers the creation of the `ExtremeValueCapper`. Just like with
    creating a transformer/companion model class, the estimator borrows heavily from
    the set of conventions we have encountered so far. The only difference is in the
    return value of the `fit()` method: instead of returning a transformed data frame,
    we return a fully-Param-eterized model. And, just like with transformers, custom
    estimators allow for the implementation of functionality not provided outright
    by PySpark. This makes our ML pipelines more clean and robust.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 `ExtremeValueCapper` 的创建。就像创建 transformer/伴随模型类一样，估计器大量借鉴了我们迄今为止遇到的一系列约定。唯一的区别在于
    `fit()` 方法的返回值：我们返回一个完全参数化的模型，而不是转换后的数据框。而且，就像 transformers 一样，自定义估计器允许实现 PySpark
    直接未提供的功能。这使得我们的机器学习管道更加简洁和健壮。
- en: We are already working with a significant body of raw material. We have our
    Params defined in a Mixin and have a companion model handy (section 14.2.2). We
    simply have to provide the `__init__()` method, the setters, and the `fit()` method.
    Because the first two are completed the same way as the transformer, our focus
    will be on the `fit()` method.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经与大量的原始材料一起工作。我们在 Mixin 中定义了 Params，并且有一个现成的伴随模型（第 14.2.2 节）。我们只需提供 `__init__()`
    方法、设置器和 `fit()` 方法。由于前两者与 transformer 完成方式相同，我们的重点将放在 `fit()` 方法上。
- en: For the `fit()` method, illustrated in figure 14.7, we already have a sample
    function we can borrow heavily from in listing 14.15\. In listing 14.18, the `fit()`
    method reproduces the functionality of our sample function, using the Params of
    the estimator as necessary. The return value is a fully Param-eterized `ExtremeValueCapperModel`.
    Note that, just like `_transform()`, PySpark asks us to create the `_fit()` method,
    providing a `fit()` wrapped, which allows for overriding Params at call time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `fit()` 方法，如图 14.7 所示，我们已经在列表 14.15 中有一个可以大量借鉴的示例函数。在列表 14.18 中，`fit()` 方法使用估计器的
    Params 重新实现了示例函数的功能。返回值是一个完全参数化的 `ExtremeValueCapperModel`。请注意，就像 `_transform()`
    一样，PySpark 要求我们创建 `_fit()` 方法，提供一个包装的 `fit()`，这允许在调用时覆盖 Params。
- en: '![](../Images/14-07.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7](../Images/14-07.png)'
- en: Figure 14.7 The `fit` method of the `ExtremeValueCapper` estimator. For the
    companion model, we generate the `cap` and `floor` Params based on the data passed
    as input, where `inputCol` and `outputCol` are passed verbatim by the estimator
    instantiation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 `ExtremeValueCapper` 估计器的 `fit` 方法。对于伴随模型，我们根据输入数据生成 `cap` 和 `floor`
    Params，其中 `inputCol` 和 `outputCol` 由估计器实例化直接传递。
- en: Listing 14.18 The `_fit()` method of the `ExtremeValueCapper`
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.18 `ExtremeValueCapper` 的 `_fit()` 方法
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ We inherit from the Estimator class and the _ExtremeValueCapperParams Mixin
    to reduce boilerplate code.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们从 Estimator 类和 _ExtremeValueCapperParams 混合类继承，以减少样板代码。
- en: ❷ We set the relevant variables from the Params.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们从 Params 设置相关变量。
- en: ❸ We compute the average (avg) and the standard deviation (stddev) from the
    data frame passed as argument.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们从作为参数传递的数据框中计算平均值（avg）和标准差（stddev）。
- en: ❹ We return a fully Param-eterized ExtremeValueCapperModel as an output of the
    method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们将完全参数化的 ExtremeValueCapperModel 作为方法输出。
- en: Just like the `ExtremeValueCapperModel`, the full source code for the `ExtremeValueCapper`
    is available in the book’s companion repository under `code/Ch14/custom_ feature.py`
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `ExtremeValueCapperModel` 一样，`ExtremeValueCapper` 的完整源代码可在本书的配套仓库中的 `code/Ch14/custom_feature.py`
    下找到。
- en: In this section, we’ve implemented the `ExtremeValueCapper` estimator, closing
    the loop in our custom estimator journey. In the next section, we test-drive our
    estimator on a sample data set to sanity-check the plumbing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现了 `ExtremeValueCapper` 估计器，完成了我们自定义估计器之旅的闭环。在下一节中，我们将我们的估计器在一个样本数据集上进行测试，以进行合理性检查。
- en: 14.2.4 Trying out our custom estimator
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.4 尝试我们的自定义估计器
- en: This section takes the `ExtremeValueCapper` estimator and applies it to a sample
    data frame. Just like with a custom transformer, making sure that our custom estimator
    works as expected is paramount before using it in an ML pipeline.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将 `ExtremeValueCapper` 估计器应用于一个样本数据框。就像自定义转换器一样，在使用它之前确保我们的自定义估计器按预期工作至关重要。
- en: In the next listing, we use the `test_df` data frame (defined at the beginning
    of the chapter) to try the `ExtremeValueCapper`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们使用本章开头定义的 `test_df` 数据框来尝试 `ExtremeValueCapper`。
- en: Listing 14.19 Trying out the `ExtremeValueCapper` on a sample data frame
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.19 在样本数据框上尝试 `ExtremeValueCapper`
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ fit() returns a Param-eterized ExtremeValueCapperModel, whose transform()
    method is then called. The result is a transformed data frame.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `fit()` 返回一个参数化的 ExtremeValueCapperModel，然后调用其 `transform()` 方法。结果是转换后的数据框。
- en: ❷ 1 is lower than the floor, while 11 is higher than the cap. Both cases work
    as expected.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 1 小于下限，而 11 大于上限。这两种情况都按预期工作。
- en: In this very short section, we ensured that our `ExtremeValueCapper` was functioning
    as expected. With the development of two new pipeline member candidates, the next
    section covers their inclusion in our original dessert prediction model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个非常简短的章节中，我们确保了我们的 `ExtremeValueCapper` 正如预期那样工作。随着两个新的流程成员候选者的开发，下一节将介绍它们如何包含在我们的原始甜点预测模型中。
- en: 14.3 Using our transformer and estimator in an ML pipeline
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 在机器学习流程中使用我们的转换器和估计器
- en: What’s the point of creating custom transformers and estimators if we don’t
    plan on using them? In this section, we apply both `ScalarNAFiller` and `ExtremeValueCapper`
    to the dessert classification modeling pipeline. This custom transformer and estimator
    will help make our ML pipeline more portable and remove some of the pre-processing
    work we need to perform (filling `null` and capping numerical values) before we
    can run the pipeline.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打算不使用它们，创建自定义转换器和估计器有什么意义？在本节中，我们将 `ScalarNAFiller` 和 `ExtremeValueCapper`
    应用于甜点分类建模流程。这个自定义转换器和估计器将帮助使我们的机器学习流程更便携，并移除我们在运行流程之前需要执行的一些预处理工作（填充 `null` 和数值上限）。
- en: When writing an ML program, we get to choose whether we want to integrate an
    operation into a pipeline (through custom transformers/estimators) or leave it
    as a data transformation. I like the testability and the portability of pipelines
    and tend to err on the “pipeline more than less” camp. When building ML models,
    we often want to cap/floor values, or impute a scalar value for `null` ones; with
    our custom transformer/estimator, there is no need to rewrite that transformation
    code.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写机器学习程序时，我们可以选择是否将操作集成到流程中（通过自定义转换器/估计器）或将其作为数据转换保留。我喜欢流程的可测试性和可移植性，并倾向于“更多流程而不是更少”的观点。在构建机器学习模型时，我们经常想要对值进行上限/下限处理，或者为
    `null` 值填充一个标量值；使用我们的自定义转换器/估计器，无需重写该转换代码。
- en: If we were to use `ScalarNAFiller` as is, we would have to apply one transformer
    for every binary column we wished to fill. Not on my watch! We start this section
    by extending the `ScalarNAFiller` to accept more than one column.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接使用 `ScalarNAFiller`，我们不得不为每个我们希望填充的二进制列应用一个转换器。这可不是我想要的！我们从这个部分开始，扩展 `ScalarNAFiller`
    以接受多个列。
- en: 14.3.1 Dealing with multiple inputCols
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 处理多个 inputCols
- en: Note From this section onward, I use the word *transformer* for succinctness.
    The concepts apply identically to transformers, estimators, and companion models.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 从本节开始，我将使用“转换器”一词以简洁起见。这些概念同样适用于转换器、估计器和伴随模型。
- en: In this section, we tackle the common issue of dealing with multiple input and
    output columns when building custom transformers. We introduce the `HasInputCols`
    and `HasInputCols` Mixins and how to deal with transformers that can accept one
    or more columns as inputs or outputs. A transformer accepting multiple columns
    as inputs or outputs yields less repetition versus using one transformer for each
    column. Furthermore, the `VectorAssembler`, encountered for the first time in
    chapter 12, requires, by definition, multiple input columns (`inputCols`) and
    a single Vector output column (`outputCol`). At the end of this section, you’ll
    be able to create robust transformers that work on single and multiple columns.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解决在构建自定义转换器时处理多个输入和输出列的常见问题。我们介绍了 `HasInputCols` 和 `HasOutputCols` 混合模式和如何处理可以接受一个或多个列作为输入或输出的转换器。接受多个列作为输入或输出的转换器比每个列使用一个转换器产生的重复性更少。此外，第
    12 章中首次遇到的 `VectorAssembler`，根据定义，需要多个输入列（`inputCols`）和一个单独的 Vector 输出列（`outputCol`）。在本节结束时，你将能够创建适用于单列和多列的健壮转换器。
- en: Just like `HasInputCol` and `HasOutputCol`, PySpark provides the `HasInputCols`
    and `HasOutputCols` Mixins that we can use. In listing 14.20, we get the new class
    definition for `ScalarNAFiller` with the additional Mixins' inheritance. Since
    we want `ScalarNAFiller` to work with either a single column as inputs/outputs
    or with multiple columns, we inherit from both the singular and plural Mixins.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `HasInputCol` 和 `HasOutputCol` 一样，PySpark 提供了 `HasInputCols` 和 `HasOutputCols`
    混合模式，我们可以使用。在列表 14.20 中，我们获得了带有额外混合模式继承的新 `ScalarNAFiller` 类定义。由于我们希望 `ScalarNAFiller`
    能够以单列作为输入/输出或以多列作为输入/输出工作，因此我们继承自单数和复数混合模式。
- en: Listing 14.20 Adding `HasInputCols` and `HasOutputCols` to `ScalarNAFiller`
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.20 向 `ScalarNAFiller` 添加 `HasInputCols` 和 `HasOutputCols`
- en: '[PRE19]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Just like their singular counterparts, accepting multiple columns is just
    about inheriting from the appropriate Mixins.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 就像它们的单数对应物一样，接受多个列只是继承适当的混合模式。
- en: Note We need to create the appropriate setters and update the arguments to `setParams()`
    and `__init__` as well. The code for the full `ScalarNAFiller` is available in
    the book’s companion repository, under `code/Ch14/custom_ feature.py`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们需要创建适当的设置器，并更新 `setParams()` 和 `__init__` 的参数。完整的 `ScalarNAFiller` 代码可在本书的配套仓库中找到，位于
    `code/Ch14/custom_feature.py`。
- en: Dealing with `inputCol/inputCols/outputCol/outputCols` means that we have to
    ensure we are using the right Params at the right time. This also means that we
    have to validate that
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 处理 `inputCol/inputCols/outputCol/outputCols` 意味着我们必须确保在正确的时间使用正确的 Params。这也意味着我们必须验证
- en: The right Params are defined
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的 Params 已定义
- en: We can unambiguously determine which ones should be used
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以明确地确定应该使用哪些。
- en: 'In the case of the `ScalarNAFiller`, we either want to apply the transformer
    on a single column (`inputCol`/`outputCol`) or multiple columns (`inputCols`/`outputCols`).
    From this, we can derive three use cases we want to defend ourselves against:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ScalarNAFiller` 的情况下，我们希望将转换器应用于单个列（`inputCol`/`outputCol`）或多个列（`inputCols`/`outputCols`）。据此，我们可以推导出我们想要防御的三个用例：
- en: If `inputCol` and `inputCols` are both set, we should raise an error, as we
    don’t know if the transformer should be applied to a single or multiple columns.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `inputCol` 和 `inputCols` 都已设置，则应引发错误，因为我们不知道转换器应该应用于单个列还是多个列。
- en: Inversely, we should also raise an error if neither of them are set.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，如果两者都没有设置，我们也应该引发错误。
- en: Finally, if `inputCols` is set, `outputCols` should be set as a list of the
    same length (*N* columns in, *N* columns out).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果设置了 `inputCols`，则 `outputCols` 应该设置为相同长度的列表（输入 *N* 列，输出 *N* 列）。
- en: Note `outputCol` is set to a default value, so we don’t need to test for `isSet()`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `outputCol` 已设置为默认值，因此我们不需要测试 `isSet()`。
- en: We wrap those three test cases in a `checkParams()` method within the transformer
    definition.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些三个测试案例包装在转换器定义中的 `checkParams()` 方法内。
- en: Listing 14.21 Checking for the validity of the Params
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.21 检查 Params 的有效性
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '❶ Test 1: Either inputCol or inputCols can be set, but not both.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 测试 1：可以设置 inputCol 或 inputCols，但不能同时设置。
- en: '❷ Test 2: At least one (inputCol or inputCols) must be set.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 测试 2：至少必须设置一个（inputCol 或 inputCols）。
- en: '❸ Test 3: If inputCols is set, then outputCols must be a list of the same len().'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 测试3：如果设置了inputCols，则outputCols必须是长度相同的列表。
- en: The third aspect of the updated `ScalarNAFiller` is the `_transform()` method
    itself. In listing 14.22, the new method has a few new moving parts.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的`ScalarNAFiller`的第三个方面是它自己的`_transform()`方法。在列表14.22中，新的方法有几个新的组成部分。
- en: First, we `checkParams()` using the method defined in listing 14.21\. I like
    putting all the checks under a single method so the `_transform()` method is more
    squarely focused on the actual transformation work.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用列表14.21中定义的方法`checkParams()`进行检查。我喜欢将所有检查放在一个单独的方法下，这样`_transform()`方法就可以更专注于实际的转换工作。
- en: Second, since `inputCols/outputCols` are lists of strings and `inputCol/outputCol`
    are strings, and our transformation routine needs to accommodate both, we wrap
    the singular Param (when used) in a single-item list that we can iterate over
    later. This way, we can use a for loop over `input_columns`/`output_columns` without
    worrying if we are in the singular or plural case.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于`inputCols/outputCols`是字符串列表，而`inputCol/outputCol`是字符串，并且我们的转换例程需要适应这两种情况，我们将单个Param（如果使用）包裹在一个单元素列表中，以便稍后迭代。这样，我们可以使用for循环遍历`input_columns`/`output_columns`，而不用担心我们是在单数还是复数情况下。
- en: 'Finally, in the transformation routine itself, we first test to see if `input_columns`
    is identical to `output_columns`: when this is the case, we have no need to create
    new columns with `withColumn()` as they already exist in the data frame. We will
    address all the columns in the `output_columns` list with `na_filler`.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在转换例程本身中，我们首先测试`input_columns`是否与`output_columns`相同：当这种情况发生时，我们不需要使用`withColumn()`创建新列，因为它们已经在数据帧中存在。我们将使用`na_filler`处理`output_columns`列表中的所有列。
- en: Listing 14.22 The modified `_transform()` method
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.22 修改后的`_transform()`方法
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ We check for the validity of the Params first before performing any work.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在执行任何工作之前首先检查Params的有效性。
- en: ❷ Because the plural Params are in lists, we keep the same behavior by wrapping
    the singular Param in a (single-item) list so that we can iterate over it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于复数Params在列表中，我们通过将单个Param包裹在一个（单元素）列表中来保持相同的行为，这样我们就可以迭代它。
- en: ❸ To save a few operations, when input_columns equal output_columns, we overwrite
    the existing columns; there is no need to create new ones.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了节省一些操作，当input_columns等于output_columns时，我们覆盖现有的列；没有必要创建新的列。
- en: Transitioning a single-column transformer to a multiple-columns one is pretty
    straightforward; we still need to make sure we design the usage of Params appropriately
    so they work appropriately, or fail with an informative error message. In the
    next section, we put our custom transformers to good use in our dessert prediction
    pipeline.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 将单列transformer转换为多列transformer相当直接；我们仍然需要确保我们适当地设计Params的使用方式，以便它们能够正确工作，或者在出现错误时提供有信息的错误消息。在下一节中，我们将我们的自定义transformer在我们的甜点预测管道中发挥良好作用。
- en: '14.3.2 In practice: Inserting custom components into an ML pipeline'
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.2 实际应用：将自定义组件插入到ML管道中
- en: In this last section of the chapter, we look into applying a custom transformer/estimator
    to our dessert ML pipeline, introduced in chapter 13\. Furthermore, we look at
    serializing and deserializing an ML pipeline containing custom transformers and
    estimators, ensuring the same portability as the made-out-of-stock components.
    This section has a faster pace because we reuse the same plumbing encountered
    in chapter 13; as a matter of fact, this shows how consistent our custom transformers
    and estimators are with the PySpark ML API.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们探讨如何将自定义的transformer/estimator应用到第13章中介绍过的我们的甜点机器学习管道中。此外，我们还将研究如何序列化和反序列化包含自定义transformer和estimator的机器学习管道，确保其与现成组件相同的可移植性。由于我们重用了第13章中遇到的同一种管道，这一部分的内容节奏更快；实际上，这也展示了我们的自定义transformer和estimator与PySpark
    ML API的一致性。
- en: To instantiate custom transformers and estimators, we simply call the class
    constructor with the relevant parameters. Just like with any PySpark stock components,
    our custom ones take fully keyworded attributes, just like in the next listing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化自定义transformer和estimator，我们只需使用相关参数调用类构造函数。就像使用任何PySpark现成组件一样，我们的自定义组件接受完全关键字属性，就像在下一个列表中所示。
- en: Listing 14.23 Instantiating custom transformers and estimators for our pipeline
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.23 实例化用于我们的管道的自定义transformer和estimator
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ inputCols, outputCols, and filler are passed as explicit keyworded arguments.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ inputCols、outputCols和filler作为显式的关键字参数传递。
- en: Now we can define (in listing 14.24) a `food_pipeline`, which contains our new
    components as stages. Our new pipeline contains a handful of new stages because
    of our custom transformers and estimators, but the rest is copied on the one we
    used in chapter 13.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在列表 14.24 中定义一个 `food_pipeline`，它包含我们的新组件作为阶段。由于我们的自定义转换器和评估器，我们的新管道包含了一些新的阶段，但其余部分与我们在第
    13 章中使用的是相同的。
- en: Listing 14.24 The new and improved `food_pipeline`
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.24 新的改进版 `food_pipeline`
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The new processing stages are listed like any other stage.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新的处理阶段像任何其他阶段一样列出。
- en: 'Unsurprisingly, the updated `food_pipeline` works using the same methods (`fit()`/
    `transform()`). In listing 14.25, we follow the same logical steps as when running
    our previous version of the pipeline:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，更新后的 `food_pipeline` 使用相同的方法（`fit()`/ `transform()`）工作。在列表 14.25 中，我们遵循与运行我们之前版本的管道相同的逻辑步骤：
- en: Split the data set into `train` and `test` partitions.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为 `train` 和 `test` 分区。
- en: Fit the pipeline on the `train` data frame.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `train` 数据框上拟合管道。
- en: Classify the observations on the `test` data frame.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `test` 数据框上对观测值进行分类。
- en: Evaluate the AUC (area under the curve) and print the results.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 AUC（曲线下面积）并打印结果。
- en: Listing 14.25 Transforming `food_pipeline` on our training data set
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.25 在我们的训练数据集上转换 `food_pipeline`
- en: '[PRE24]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Confident with our pipeline working from beginning to end, let’s look at serialization
    and deserialization. In the next listing, our pipeline does not save, throwing
    a `ValueError` with an informative message: one stage (`ScalarNAFiller`) is not
    `MLWritable`. So close!'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的管道从开始到结束都运行正常的情况下，让我们看看序列化和反序列化。在下一个列表中，我们的管道没有保存，抛出了一个带有有用信息的 `ValueError`：一个阶段（`ScalarNAFiller`）不是
    `MLWritable`。所以，快关上！
- en: Listing 14.26 Attempting to serialize our model to disk
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.26 尝试将我们的模型序列化到磁盘
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Fortunately for us, we can add the capability of serializing a transformer (or
    estimator) just by inheriting from a Mixin. In this specific case, we want our
    custom components to inherit from `DefaultParamsReadable` and `DefaultParamsWritable`,
    both from the `pyspark.ml.util` module. In listing 14.27, we add those Mixins
    to `ScalarNAFiller` and `_ExtremeValueCapperParams` so that both the `ExtremeValueCapper`
    estimator and its companion model inherit from them. Doing so takes care of serializing
    the metadata of the transformer or estimator so that another instance of Spark
    can read them back and then apply the Param-eterization from your pipeline definition
    or fitting.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们只需通过从 Mixin 继承即可添加序列化转换器（或评估器）的功能。在这个特定的情况下，我们希望我们的自定义组件从 `pyspark.ml.util`
    模块中的 `DefaultParamsReadable` 和 `DefaultParamsWritable` 继承。在列表 14.27 中，我们将这些 Mixin
    添加到 `ScalarNAFiller` 和 `_ExtremeValueCapperParams`，这样 `ExtremeValueCapper` 评估器和它的伴随模型就都从它们继承了。这样做可以处理序列化转换器或评估器的元数据，以便另一个
    Spark 实例可以读取它们，然后应用来自管道定义或拟合的参数化。
- en: Listing 14.27 Adding the two Mixins for writing/reading the transformer
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.27 添加用于写入/读取转换器的两个 Mixin
- en: '[PRE26]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This takes care of the serialization. What about reading back a pipeline with
    custom components? PySpark, when reading a serialized pipeline, will perform the
    following steps:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这就处理了序列化。那么，如何读取包含自定义组件的管道呢？当 PySpark 读取一个序列化的管道时，将执行以下步骤：
- en: Create a shell of the pipeline, with the default Param-eterization.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有默认参数化的管道外壳。
- en: For each component, apply the Param-eterization from the serialized configuration.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个组件，应用序列化配置中的参数化。
- en: In many cases, the serialization environment is not the same as the deserialized
    one. For instance, we usually train an ML pipeline on a powerful Spark cluster,
    serializing the fitted pipeline. We can then predict, on a different (less powerful
    and/or costly) setup, as needed. In those scenarios, you need to provide the deserialized
    Spark environment an indication about where to find the classes that implement
    transformers and estimators. Those included with PySpark will be found without
    explicit importing, but any custom ones need to be imported explicitly.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，序列化环境与反序列化环境并不相同。例如，我们通常在一个强大的 Spark 集群上训练一个机器学习管道，并序列化已拟合的管道。然后，我们可以根据需要，在不同的（更弱或更昂贵）设置上进行预测。在这些场景中，你需要为反序列化的
    Spark 环境提供一个指示，告诉它在哪里可以找到实现转换器和评估器的类。PySpark 包含的类将无需显式导入即可找到，但任何自定义的类都需要显式导入。
- en: Listing 14.28 Reading the serialized pipeline from disk
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.28 从磁盘读取序列化的管道
- en: '[PRE27]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this section, we reviewed practical steps in using custom transformers and
    estimators. By taking a few precautions, such as making sure to inherit the appropriate
    Mixins and importing the necessary custom classes, we can ensure that our pipelines
    are portable and therefore usable in multiple Spark environments.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了使用自定义转换器和估计器的实际步骤。通过采取一些预防措施，例如确保继承适当的Mixins和导入必要的自定义类，我们可以确保我们的Pipeline是可移植的，因此可以在多个Spark环境中使用。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Behind transformers and estimators, PySpark has the concept of Param/Params,
    self-documenting attributes that govern how a transformer or estimator behaves.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在转换器和估计器后面，PySpark有Param/Params的概念，这是自我文档化的属性，它决定了转换器或估计器的行为方式。
- en: When creating custom transformers/estimators, we create their Param first, and
    then use them in `transform()`-/`fit()`-like instance attributes. PySpark provides
    standard Params for frequent use cases in the `pyspark.ml.param .shared` module.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当创建自定义转换器/估计器时，我们首先创建它们的Param，然后在使用`transform()`-/`fit()`-样实例属性时使用它们。PySpark在`pyspark.ml.param.shared`模块中提供了标准Param，用于频繁使用的情况。
- en: For often-used Params or functionalities, such as writing and reading, PySpark
    provides Mixins, classes containing specific methods to simplify and reduce boilerplate
    code for transformers and estimators.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于常用Param或功能，例如写入和读取，PySpark提供了Mixins，这些是包含特定方法的类，用于简化转换器和估计器的样板代码。
- en: When deserializing a Pipeline containing custom stages, you need to ensure the
    underlying classes are imported within the program’s name space.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当反序列化包含自定义阶段的Pipeline时，你需要确保程序命名空间内导入了底层类。
- en: 'Conclusion: Have data, am happy!'
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：有数据，我就开心！
- en: 'This concludes our overview of the PySpark ecosystem for data analysis. I hope
    that, through the different use cases and questions we asked ourselves, you gained
    an appreciation for the Spark data model and operating engine. At the beginning
    of this book, I summarized every data job as akin to an *ingest, transform, and
    export* process. Throughout this book, we did the following:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对PySpark数据分析生态系统的概述。我希望通过我们提出的不同用例和问题，你能够对Spark数据模型和操作引擎有所欣赏。在本书的开头，我把每个数据工作总结为类似于一个*摄取、转换和导出*的过程。在整个本书中，我们做了以下几件事：
- en: Ingested a variety of data sources, from text (chapter 2) to CSV (chapter 4)
    to JSON (chapter 6) to parquet (chapter 10)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本（第2章）到CSV（第4章）到JSON（第6章）再到parquet（第10章）摄取了各种数据源
- en: Transformed data using a SQL-esque data manipulation framework (chapters 4 and
    5), even resorting to actual SQL code (chapter 7). We also used Python and pandas
    code (chapters 8 and 9) to combine the power of Python and the scalability of
    Spark.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类似SQL的数据操作框架（第4章和第5章）转换数据，甚至求助于实际的SQL代码（第7章）。我们还使用了Python和pandas代码（第8章和第9章）来结合Python的强大功能和Spark的可扩展性。
- en: Learned about Spark data types, schema, and how to build multidimensional data
    models using the data frame (chapter 6).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习了Spark数据类型、模式以及如何使用数据框构建多维数据模型（第6章）。
- en: Flipped the data frame model on its head and delved into the lower-level RDD,
    gaining full control over the distributed data model. We understood the trade-offs
    between complexity, performance, and flexibility using the RDD versus the data
    frame (chapter 8).
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据框模型颠倒过来，深入到底层的RDD，从而完全控制分布式数据模型。我们通过RDD与数据框之间的对比，理解了复杂性、性能和灵活性之间的权衡（第8章）。
- en: Analyzed how Spark processes data and manages compute and memory resources through
    the Spark UI (chapter 11).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Spark UI分析了Spark处理数据和管理计算及内存资源的方式（第11章）。
- en: Prepared data for machine learning (chapter 12), built ML pipelines for reproducible
    ML experimentation (chapter 13), and created custom components for more flexible
    and powerful pipelines (this chapter).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习准备数据（第12章），构建可重复的ML实验Pipeline（第13章），并创建自定义组件以实现更灵活和强大的Pipeline（本章）。
- en: While PySpark is a moving target, I hope that the information in this book will
    make using PySpark today (and tomorrow) easier, more productive, and more enjoyable.
    As data continues to grows faster than our hardware, I believe that distributed
    processing has much more value to provide.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PySpark是一个不断发展的目标，但我希望本书中的信息将使您今天（和明天）使用PySpark变得更加容易、更高效、更有趣。随着数据增长速度超过我们的硬件，我相信分布式处理有更多的价值可以提供。
- en: Thank you for giving me the chance to accompany you on this journey. I look
    forward to hearing about the insights you derive from data using Python and PySpark.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您给我机会陪伴您走过这段旅程。我期待着了解您使用Python和PySpark从数据中获得的见解。
- en: '* * *'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ Theoretically, it is provided by the `Params` class, from which both `Transformer`
    and `Estimator` inherit.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 理论上，它由 `Params` 类提供，`Transformer` 和 `Estimator` 都继承自该类。
