- en: 10 Visual embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 视觉嵌入
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Expressing similarity between images via loss functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过损失函数表达图像之间的相似性
- en: Training CNNs to achieve a desired embedding function with high accuracy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练CNN以实现高精度的期望嵌入函数
- en: Using visual embeddings in real-world applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实际应用中使用视觉嵌入
- en: by Ratnesh Kumar
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由拉特内什·库马尔编写
- en: Obtaining meaningful relationships between images is a vital building block
    for many applications that touch our lives every day, such as face recognition
    and image search algorithms. To tackle such problems, we need to build an algorithm
    that can extract relevant features from images and subsequently compare them using
    their corresponding features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 获取图像之间有意义的关联是许多与我们日常生活息息相关的应用的关键构建块，例如人脸识别和图像搜索算法。为了解决这类问题，我们需要构建一个算法，可以从图像中提取相关特征，并随后使用相应的特征进行比较。
- en: 'Ratnesh Kumar obtained his PhD from the STARS team at Inria, France, in 2014\.
    While working on his PhD, he focused on problems in video understanding: video
    segmentation and multiple object tracking. He also has a Bachelor of Engineering
    from Manipal University, India, and a Master of Science from the University of
    Florida at Gainesville. He has co-authored several scientific publications on
    learning visual embedding for re-identifying objects in camera networks.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 拉特内什·库马尔于2014年在法国Inria的STARS团队获得博士学位。在攻读博士学位期间，他专注于视频理解问题：视频分割和多目标跟踪。他还拥有印度马普尔大学的工程学士学位和佛罗里达大学盖恩斯维尔分校的科学硕士学位。他共同撰写了多篇关于在相机网络中重新识别物体时学习视觉嵌入的科学出版物。
- en: In the previous chapters, we learned that we can use convolutional neural networks
    (CNNs) to extract meaningful features for an image. This chapter will use our
    understanding of CNNs to train (jointly) a visual embedding layer. In this chapter’s
    context, visual embedding refers to the last fully connected layer (prior to a
    loss layer) appended to a CNN. Joint training refers to training both the embedding
    layer and the CNN parameters jointly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解到我们可以使用卷积神经网络（CNN）来提取图像的有意义特征。本章将利用我们对CNN的理解来共同训练（联合）一个视觉嵌入层。在本章的上下文中，视觉嵌入指的是附加到CNN的最后一个完全连接层（在损失层之前）。联合训练指的是共同训练嵌入层和CNN参数。
- en: This chapter explores the nuts and bolts of training and using visual embeddings
    for large-scale, image-based query-retrieval systems such as applications of visual
    embeddings (see figure 10.1). To perform this task, we first need to project (embed)
    our database of images onto a vector space (embedding). This way, comparisons
    between images can be performed by measuring their pairwise distances in this
    embedding space. This is the high-level idea of visual embedding systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了训练和使用视觉嵌入进行大规模基于图像的查询检索系统（如图10.1所示的应用）的细节。为了执行这项任务，我们首先需要将我们的图像数据库投影（嵌入）到向量空间（嵌入）。这样，图像之间的比较可以通过测量它们在这个嵌入空间中的成对距离来完成。这是视觉嵌入系统的高级概念。
- en: '![](../Images/10-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-1.png)'
- en: 'Figure 10.1 Example applications we encounter in everyday life when working
    with images: a machine comparing two images (left); querying the database to find
    images similar to the input image (right). Comparing two images is a non-trivial
    task and is key to many applications relating to meaningful image search.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 在处理图像时，我们在日常生活中遇到的示例应用：一台机器比较两张图像（左）；查询数据库以找到与输入图像相似的图像（右）。比较两张图像是一个非平凡的任务，并且是许多与有意义图像搜索相关的应用的关键。
- en: DEFINITION An embedding is a vector space, typically of lower dimension than
    the input space, which preserves relative dissimilarity (in the input space).
    We use the terms vector space and embedding space interchangeably. In the context
    of this chapter, the last fully connected layer of a trained CNN is this vector
    (embedding) space. As an example, a fully connected layer of 128 neurons corresponds
    to a vector space of 128 dimensions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 定义嵌入是一个向量空间，通常比输入空间维度低，它保留了相对不相似性（在输入空间中）。我们使用向量空间和嵌入空间这两个术语可以互换。在本章的上下文中，训练好的CNN的最后完全连接层就是这个向量（嵌入）空间。例如，一个有128个神经元的完全连接层对应于一个128维的向量空间。
- en: For a reliable comparison among images, the embedding function needs to capture
    a desired input similarity measure. This embedding function can be learned using
    various approaches; one of the popular ways is to use a deep CNN. Figure 10.2
    illustrates a high-level process of using CNNs to create an embedding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像之间进行可靠的比较，嵌入函数需要捕捉到期望的输入相似度度量。可以使用各种方法学习嵌入函数；其中一种流行的方法是使用深度卷积神经网络。图10.2说明了使用卷积神经网络创建嵌入的高级过程。
- en: '![](../Images/10-2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-2.png)'
- en: Figure 10.2 Using CNNs to obtain an embedding from an input image
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 使用卷积神经网络从输入图像中获取嵌入
- en: 'In the following section, we explore some example applications of using visual
    embeddings for large-scale query-retrieval systems. Then we will dive deeper into
    the different components of the visual embedding systems: loss functions, mining
    informative data, and training and testing the embedding network. Subsequently,
    we will use these concepts to solve our chapter project on building visual embedding-based
    query-retrieval systems. Thereafter, we will explore approaches to push the boundaries
    of the project’s network accuracy. By the end of this chapter, you will be able
    to train a CNN to obtain a reliable and meaningful embedding and use it in real-world
    applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些使用视觉嵌入进行大规模查询检索系统的示例应用。然后我们将深入探讨视觉嵌入系统不同组件的不同方面：损失函数、挖掘信息数据以及嵌入网络的训练和测试。随后，我们将使用这些概念来解决我们关于构建基于视觉嵌入的查询检索系统的章节项目。之后，我们将探索推动项目网络准确性的边界的方法。到本章结束时，你将能够训练一个卷积神经网络以获得可靠且有意义的嵌入，并将其用于实际应用中。
- en: 10.1 Applications of visual embeddings
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 视觉嵌入的应用
- en: Let’s look at some practical day-to-day information-retrieval algorithms that
    use the concept of visual embeddings. Some of the prominent applications for retrieving similar images
    given an input query include face recognition (FR), image recommendation, and
    object re-identification systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些实际日常信息检索算法，这些算法使用了视觉嵌入的概念。在给定输入查询的情况下，检索相似图像的一些突出应用包括面部识别（FR）、图像推荐和物体重识别系统。
- en: 10.1.1 Face recognition
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 面部识别
- en: 'FR is about automatically identifying or tagging an image with the exact identities
    of persons in the image. Day-to-day applications include searching for celebrities
    on the web, auto-tagging friends and family in images, and many more. Recognition
    is a form of fine-grained classification. The Handbook of Face Recognition [1]
    categorizes two modes of a FR system (figure 10.3 compares them):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FR是关于自动识别或标记图像，以精确地标识图像中的人物身份。日常应用包括在网络上搜索名人、自动标记照片中的朋友和家人等。识别是一种细粒度分类。人脸识别手册[1]将FR系统的两种模式进行了分类（图10.3进行了比较）：
- en: Face identification --One-to-many matches that compare a query face image against
    all the template images in the database to determine the identity of the query
    face. For example, city authorities can check a watch list to match a query to
    a list of suspects (one-to-few matches). Another fun example is automatically
    tagging users to photos they appear in, a feature implemented by major social
    network platforms.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部识别——一种将查询人脸图像与数据库中所有模板图像进行一对一匹配，以确定查询人脸身份的方法。例如，城市当局可以检查观察名单，将查询与嫌疑人名单进行匹配（一对一匹配）。另一个有趣的例子是自动标记出现在照片中的用户，这是由主要社交网络平台实现的功能。
- en: Face verification --One-to-one match that compares a query face image against
    a template face image whose identity is being claimed.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部验证——一种将查询人脸图像与声称身份的模板人脸图像进行一对一匹配的方法。
- en: '![](../Images/10-3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-3.png)'
- en: 'Figure 10.3 Face-verification and face-recognition systems: an example of a
    face-verification system comparing one-on-one matches to identify whether or not
    the image is Sundar (left); an example of a face-identification system comparing
    one-to-many matches to identify all images (right). Despite the objective-level
    difference between recognition and identification, they both rely on a good embedding
    function that captures meaningful differences between faces. (The figure was inspired
    by [2].)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 面部验证和面部识别系统：一个面部验证系统的例子，通过一对一匹配来识别图像是否为Sundar（左）；一个面部识别系统的例子，通过一对多匹配来识别所有图像（右）。尽管识别和识别在目标层面上存在差异，但它们都依赖于一个良好的嵌入函数，该函数能够捕捉到人脸之间的有意义差异。（该图受到了[2]的启发。）
- en: 10.1.2 Image recommendation systems
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 图像推荐系统
- en: In this task, the user seeks to find similar images with respect to a given
    query image. Shopping websites provide product suggestions (via images) based
    on the selection of a particular product, such as showing all kinds of shoes that
    are similar to the ones a user selected. Figure 10.4 shows an example in the context
    of apparel search.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，用户试图根据给定的查询图像找到相似图像。购物网站根据特定产品的选择提供产品建议（通过图像），例如显示与用户所选鞋子相似的所有类型的鞋子。图10.4展示了服装搜索的示例。
- en: '![](../Images/10-4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-4.png)'
- en: Figure 10.4 Apparel search. The leftmost image in each row is the query image,
    and the subsequent columns show various apparel that look similar to it. (Images
    in this figure are taken from [3].)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 服装搜索。每行的最左边的图像是查询图像，随后的列显示了与它相似的多种服装。（此图中的图像来自[3]。）
- en: 'Note that the similarity between two images varies depending on the context
    of choosing the similarity measure. The embedding of an image differs based on
    the type of similarity measure chosen. Some examples of similarity measures are
    color similarity and semantic similarity:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，两个图像之间的相似性取决于选择相似性度量的上下文。图像的嵌入根据选择的相似性度量类型而有所不同。一些相似性度量的例子包括颜色相似性和语义相似性：
- en: Color similarity --The retrieved images have similar colors, as shown in figure
    10.5\. This measure is used in applications like retrieving similarly colored
    paintings, similarly colored shoes (not necessarily determining style), and many
    more.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色相似性 --检索到的图像具有相似的颜色，如图10.5所示。这个度量在检索类似颜色的画作、类似颜色的鞋子（不一定确定风格）等应用中使用。
- en: '![](../Images/10-5.png)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/10-5.png)'
- en: Figure 10.5 Similarity example where cars are differentiated by their color.
    Notice that the similarly colored cars are closer in this illustrative two-dimensional
    embedding space.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.5相似性示例，其中汽车通过颜色进行区分。注意，在这个说明性的二维嵌入空间中，颜色相似的汽车更接近。
- en: Semantic similarity --The retrieved image has the same semantic properties,
    as shown in figure 10.6\. In our earlier example of shoe retrieval, the user expects
    to see suggestions of shoes having the same semantics as high-heeled shoes. You
    can be creative and decide to incorporate color similarity with semantics for
    more meaningful suggestions.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义相似性 --检索到的图像具有相同的语义属性，如图10.6所示。在我们之前的鞋类检索示例中，用户期望看到与高跟鞋具有相同语义的鞋类建议。你可以发挥创意，决定将颜色相似性与语义结合，以提供更有意义的建议。
- en: '![](../Images/10-6.png)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/10-6.png)'
- en: Figure 10.6 Example of identity embeddings. Cars with similar features are projected
    closer to each other in the embedding space.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.6示例：身份嵌入。具有相似特征的汽车在嵌入空间中投影得更近。
- en: 10.1.3 Object re-identification
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 物体重识别
- en: An example of object re-identification is security camera networks (CCTV monitoring),
    as depicted in figure 10.7\. The security operator may be interested in querying
    a particular person and finding out their location in all the cameras. The system
    is required to identify a moving object in one camera and then re-identify the
    object across cameras to establish consistent identity.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 物体重识别的一个例子是安全摄像头网络（CCTV监控），如图10.7所示。安全操作员可能希望查询特定人员并找出他们在所有摄像头中的位置。系统需要在一个摄像头中识别移动对象，然后跨摄像头重新识别对象以建立一致的标识。
- en: '![](../Images/10-7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-7.png)'
- en: 'Figure 10.7 Multi-camera dataset showing the presence of a person (queried)
    across cameras. (Source: [4].)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7多摄像头数据集显示了人员在摄像头之间的存在。（来源：[4]。）
- en: This problem is commonly known as person re-identification. Notice that it is
    similar to a face-verification system where we are interested in capturing whether
    any two people in separate cameras are the same, without needing to know exactly
    who a person is.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通常被称为人员重识别。请注意，它与面部验证系统类似，我们感兴趣的是捕捉在单独摄像头中的任何两个人是否相同，而不需要确切知道一个人的身份。
- en: One of the central aspects in all these applications is the reliance on an embedding
    function that captures and preserves the input’s similarity (and dissimilarity)
    to the output embedding space. In the following sections, we will delve into designing
    appropriate loss functions and sampling (mining) informative data points to guide
    the training of a CNN for a high-quality embedding function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些应用中，一个核心方面是依赖于一个嵌入函数，该函数能够捕捉并保留输入与输出嵌入空间之间的相似性（和差异性）。在接下来的章节中，我们将深入探讨设计适当的损失函数和采样（挖掘）信息数据点，以指导CNN的训练，以实现高质量的嵌入函数。
- en: 'But before we jump into the details of creating an embedding, let’s answer
    this question: why do we need to embed--can’t we just use the images directly?
    Let’s review the bottlenecks with this naive approach of directly using image
    pixel values as an embedding. Embedding dimensionality in this approach (assuming
    all images are high definition) would be 1920 × 1080, represented in a computer’s
    memory in double precision, which is computationally prohibitive for both storage
    and retrieval given any meaningful time requirements. Moreover, most embeddings
    need to be learned in a supervised setting, as a priori semantics for comparison
    are not known (that is when we unleash the power of CNNs to extract meaningful
    and relevant semantics). Any learning algorithm on such a high-dimensional embedding
    space will suffer from the curse of dimensionality: as the dimensionality increases,
    the volume of the space increases so fast that the available data becomes sparse.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们深入探讨创建嵌入的细节之前，让我们回答这个问题：为什么我们需要嵌入——我们难道不能直接使用图像吗？让我们回顾一下直接使用图像像素值作为嵌入的这种天真方法的瓶颈。在这种方法中，嵌入的维度（假设所有图像都是高分辨率的）将是1920
    × 1080，以双精度在计算机内存中表示，这给存储和检索带来了计算上的限制，考虑到任何有意义的时要求。此外，大多数嵌入都需要在监督设置中学习，因为事先的语义比较是未知的（这就是我们释放CNN提取有意义和相关性语义的力量的时候）。任何在这种高维嵌入空间上的学习算法都将受到维度灾难的影响：随着维度的增加，空间的体积增加得如此之快，以至于可用的数据变得稀疏。
- en: 'The geometry and data distribution of natural data are non-uniform and concatenate
    around low-dimensional structures. Hence, using an image size as data dimensions
    is overkill (let alone the exorbitant computational complexity and redundancy).
    Therefore, our goal in learning embedding is twofold: learning the required semantics
    for comparison, and achieving a low(er) dimensionality of the embedding space.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自然数据的几何和数据分布是非均匀的，围绕低维结构拼接。因此，使用图像大小作为数据维度是过度的（更不用说巨大的计算复杂性和冗余了）。因此，我们在学习嵌入时的目标是双重的：学习所需的语义以进行比较，并实现嵌入空间低（或更低）的维度。
- en: 10.2 Learning embedding
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 学习嵌入
- en: Learning an embedding function involves defining a desired criterion to measure
    a similarity; it can be based on color, semantics of the objects present in an
    image, or purely data-driven in a supervised form. Since a priori knowing the
    right semantics (for comparing images) is difficult, supervised learning is more
    popular. Instead of hand-crafting similarity criteria features, in this chapter
    we will focus on the supervised data-driven learning of embeddings wherein we
    assume we are given a training set. Figure 10.8 depicts a high-level architecture
    to learn an embedding using a deep CNN.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 学习嵌入函数涉及定义一个期望的准则来衡量相似性；它可以基于颜色、图像中存在的对象的语义，或者纯粹在监督形式中数据驱动。由于事先知道正确的语义（用于比较图像）很困难，因此监督学习更受欢迎。我们不会手动制作相似性准则特征，在本章中，我们将专注于监督数据驱动的嵌入学习，其中我们假设我们被给定一个训练集。图10.8描述了使用深度CNN学习嵌入的高级架构。
- en: '![](../Images/10-8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-8.png)'
- en: Figure 10.8 An illustration of learning machinery (top); the (test) process
    outline (bottom).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 学习机器的示意图（顶部）；（测试）过程概述（底部）。
- en: 'The process to learn an embedding is straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 学习嵌入的过程很简单：
- en: Choose a CNN architecture. Any suitable CNN architecture can be used. In practice,
    the last fully connected layer is used to determine the embedding. Hence the size
    of this fully connected layer determines the dimension of the embedding vector
    space. Depending on the size of the training dataset, it may be prudent to use
    pretraining with, for example, the ImageNet dataset.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个卷积神经网络（CNN）架构。任何合适的CNN架构都可以使用。在实践中，最后一层全连接层用于确定嵌入。因此，这个全连接层的大小决定了嵌入向量空间的大小。根据训练数据集的大小，使用例如ImageNet数据集进行预训练可能是明智的。
- en: Choose a loss function. Popular loss functions are contrastive and triplet loss.
    (These are explained in section 10.3.)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个损失函数。流行的损失函数是对比损失和三元组损失。（这些在10.3节中解释。）
- en: Choose a dataset sampling (mining) method. Naively feeding all possible samples
    from the dataset is wasteful and prohibitive. Hence we need to resort to sampling
    (mining) informative data points to train our CNN. We will learn various sampling
    techniques in section 10.4.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择数据集采样（挖掘）方法。天真地提供数据集中所有可能的样本是浪费且不可行的。因此，我们需要求助于采样（挖掘）信息数据点来训练我们的CNN。我们将在第10.4节学习各种采样技术。
- en: During test time, the last fully connected layer acts as the embedding of the
    corresponding image.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试时，最后一层全连接层充当相应图像的嵌入。
- en: Now that we have reviewed the big picture of the training and inference process
    for learning embedding, we will delve into defining useful loss functions to express
    our desired embedding objectives.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了学习嵌入的训练和推理过程的大致情况，我们将深入探讨定义有用的损失函数以表达我们期望的嵌入目标。
- en: 10.3 Loss functions
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 损失函数
- en: 'We learned in chapter 2 that optimization problems require the definition of
    a loss function to minimize. Learning embedding is not different from any other
    DL problem: we first define a loss function that we need to minimize, and then
    we train a neural network to choose the parameter (weights) values that yield
    the minimum error value. In this section, we will look more deeply at key embedding
    loss functions: cross-entropy, contrastive, and triplet.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二章中了解到，优化问题需要定义一个损失函数来最小化。学习嵌入与其他深度学习问题没有不同：我们首先定义一个需要最小化的损失函数，然后训练一个神经网络来选择参数（权重）值，以产生最小的误差值。在本节中，我们将更深入地探讨关键的嵌入损失函数：交叉熵、对比和三元组。
- en: First we will formalize the problem setup. Then we will explore the different
    loss functions and their mathematical formulas.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将形式化问题设置。然后，我们将探讨不同的损失函数及其数学公式。
- en: 10.3.1 Problem setup and formalization
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 问题设置和形式化
- en: 'To understand loss functions for learning embedding and eventually train a
    CNN (for this loss), let’s first formalize the input ingredients and desired output
    characteristics. This formalization will be used in later sections to understand
    and categorize various loss functions in a succinct manner. For the purposes of
    this conversation, our dataset can be represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解学习嵌入的损失函数并最终训练一个CNN（为此损失），我们首先形式化输入成分和期望的输出特性。这种形式化将在后面的章节中以简洁的方式理解和分类各种损失函数。为了这次对话的目的，我们的数据集可以表示如下：
- en: χ = {(*x[i]* , *y[i]*)}*[i]^N* =1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: χ = {(*x[i]* , *y[i]*)}*[i]^N* =1
- en: '*N* is the number of training images, *x*[i] is the input image, and *y*[i]
    is its corresponding label. Our objective is to create an embedding'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*是训练图像的数量，*x*[i]是输入图像，*y*[i]是它的对应标签。我们的目标是创建一个嵌入'
- en: '*f* (*x* ; θ): ℝ*^D* → ℝ*^F*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* (*x* ; θ): ℝ*^D* → ℝ*^F*'
- en: to map images in ℝ*^D* onto a feature (embedding) space in ℝ*^D* such that images
    of similar identity are metrically close in this feature space (and vice versa
    for images of dissimilar identities)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像从ℝ*^D*映射到ℝ*^D*的特征（嵌入）空间，使得具有相似身份的图像在这个特征空间中是度量上接近的（反之亦然，对于具有不同身份的图像而言）。
- en: θ* = arg [θ] minℒ( *f* (θ; χ ))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: θ* = arg [θ] minℒ( *f* (θ; χ ))
- en: where θ is the parameter set of the learning function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中θ是学习函数的参数集。
- en: Let
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让
- en: '*D*(*x[i]*, *x[j]*) : ℝ*^F* *X* ℝ^F → ℝ'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*D*(*x[i]*, *x[j]*) : ℝ*^F* *X* ℝ^F → ℝ'
- en: be the metric measuring distance of images *x[i]*, *x[j]* in the embedding space.
    For simplicity, we drop the input labels and denote *D*(*x[i]*, *x[j]*) as *D[ij]*
    · *y[ij]* = 1\. Both samples ( i ) and ( j ) belong to the same class, and the
    value *y[ij]* = 0 indicates samples of different classes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为嵌入空间中图像*x[i]*和*x[j]*之间的距离度量。为了简单起见，我们省略了输入标签，并将*D*(*x[i]*, *x[j]*)表示为*D[ij]*
    · *y[ij]* = 1。这两个样本（i）和（j）属于同一类，且*y[ij]* = 0表示不同类别的样本。
- en: 'Once we train an embedding network for its optimal parameters, we desire the
    learned function to have the following characteristics:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了一个嵌入网络的优化参数，我们希望学习到的函数具有以下特性：
- en: An embedding should be invariant to viewpoints, illumination, and shape changes
    in the object.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入应该对视图、光照和形状变化不敏感。
- en: From a practical application deployment, computation of embedding and ranking
    should be efficient. This calls for a low-dimension vector space (embedding).
    The bigger this space is, the more computation is required to compare any two
    images, which in turn affects the time complexity.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实际应用部署的角度来看，嵌入和排名的计算应该是高效的。这要求一个低维向量空间（嵌入）。这个空间越大，比较任意两个图像所需的计算就越多，这反过来又影响了时间复杂度。
- en: Popular choices for learning an embedding are cross-entropy loss, contrastive
    loss, and triplet loss. The subsequent sections will introduce and formalize these
    losses.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 学习嵌入的流行选择是交叉熵损失、对比损失和三元组损失。接下来的章节将介绍和形式化这些损失。
- en: 10.3.2 Cross-entropy loss
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 交叉熵损失
- en: 'Learning an embedding can also be formulated as a fine-grained classification
    problem, and the corresponding CNN can be trained using the popular cross-entropy
    loss (explained in detail in chapter 2). The following equation expresses cross-entropy
    loss, where *p*(*y*[ij] | *f* (*x; θ* )) represents the posterior class probability.
    In CNN literature, softmax loss implies a softmax layer trained in a discriminative
    regime using cross-entropy loss:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 学习嵌入也可以被表述为一个细粒度的分类问题，相应的卷积神经网络可以使用流行的交叉熵损失（在第2章中详细解释）。以下方程表达了交叉熵损失，其中 *p*(*y*[ij]
    | *f* (*x; θ* )) 表示后验类别概率。在CNN文献中，softmax损失意味着使用交叉熵损失在判别性区域训练的softmax层：
- en: '![](../Images/10-8_E1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-8_E1.png)'
- en: During training, a fully connected (embedding) layer is added prior to the loss
    layer. Each identity is considered a separate category, and the number of categories
    is equal to the number of identities in the training set. Once the network is
    trained using classification loss, the final classification layer is stripped
    off and an embedding is obtained from the new final layer of the network (figure
    10.9).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，在损失层之前添加了一个全连接（嵌入）层。每个身份被视为一个单独的类别，类别的数量等于训练集中身份的数量。一旦使用分类损失训练了网络，最终的分类层就被移除，并从网络的新最终层中获得嵌入（图10.9）。
- en: '![](../Images/10-9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-9.png)'
- en: Figure 10.9 An illustration of how cross-entropy loss is used to train an embedding
    layer (fully connected). The right side demonstrates the inference process and
    outlines the disconnect in training and inference in straightforward usage of
    cross-entropy loss for learning an embedding. (This figure is adapted from [5].)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9展示了如何使用交叉熵损失来训练嵌入层（全连接）。右侧展示了推理过程，并概述了在直接使用交叉熵损失学习嵌入时的训练和推理之间的不连续性。（此图改编自[5]）
- en: By minimizing the cross-entropy loss, the parameters (θ) of the CNN are chosen
    such that the estimated probability is close to 1 for the correct class and close
    to 0 for all other classes. Since the target of the cross-entropy loss is to categorize
    features into predefined classes, usually the performance of such a network is
    poor when compared to losses incorporating similarity (and dissimilarity) constraints
    directly in the embedding space during training. Furthermore, learning becomes
    computationally prohibitive when considering datasets of, for example, 1 million identities.
    (Imagine a loss layer with 1 million neurons!) Nevertheless, pretraining a network
    with cross-entropy loss (on a viable subset of the dataset, such as a subset of
    1,000 identities) is a popular strategy used to pretrain the CNN, which in turn
    makes embedding losses converge faster. We will explore this further while mining
    informative samples during training in section 10.4.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化交叉熵损失，CNN的参数（θ）被选择，使得对于正确类别，估计概率接近1，而对于所有其他类别，估计概率接近0。由于交叉熵损失的目标是将特征分类到预定义的类别中，因此，与在训练过程中直接在嵌入空间中包含相似性（和差异性）约束的损失相比，这种网络的性能通常较差。此外，当考虑例如1百万个身份的数据集时，学习变得计算上不可行。（想象一个有1百万个神经元的损失层！）尽管如此，使用交叉熵损失（在数据集的可操作子集上，例如1,000个身份的子集）预训练网络是一种流行的策略，这反过来又使得嵌入损失更快地收敛。我们将在第10.4节中进一步探讨这一点，在训练过程中挖掘信息样本。
- en: NOTE One of the disadvantages of the cross-entropy loss is the disconnect between
    training and inference. Hence, it generally performs poorly when compared with
    embedding learning losses (contrastive and triplet). These losses explicitly try
    to incorporate the relative distance preservation from the input image space to
    the embedding space.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：交叉熵损失的一个缺点是训练和推理之间的不连续性。因此，与嵌入学习损失（对比和三元组）相比，它通常表现较差。这些损失明确尝试将输入图像空间到嵌入空间的相对距离保持下来。
- en: 10.3.3  Contrastive loss
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 对比损失
- en: Contrastive loss optimizes the training objective by encouraging all similar
    class instances to come infinitesimally closer to each other, while forcing instances
    from other classes to move far apart in the output embedding space (we say infinitesimally
    here because a CNN can’t be trained with exactly zero loss). Using our problem
    formalization, this loss is defined as
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失通过鼓励所有相似类实例无限接近彼此，同时强制其他类别的实例在输出嵌入空间中远离彼此来优化训练目标（我们在这里说无限接近，因为CNN不能使用精确为零的损失进行训练）。使用我们的问题形式化，这种损失定义为
- en: be the metric measuring distance of images *x[i]*, *x[j]* in the embedding space.
    For simplicity, we drop the input labels and denote *D*(*x[i]*, *x[j]*) as *D[ij]*
    · *y[ij]* = 1\. Both samples ( i ) and ( j ) belong to the same class, and the
    value *y[ij]* = 0 indicates samples of different classes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 是衡量嵌入空间中图像 *x[i]* 和 *x[j]* 距离的度量。为了简单起见，我们省略了输入标签，并将 *D*(*x[i]*, *x[j]*) 表示为
    *D[ij]* · *y[ij]* = 1。两个样本 ( i ) 和 ( j ) 属于同一类别，且 *y[ij]* = 0 的值表示不同类别的样本。
- en: '*l[contrastive]* (*i, j*) = *y[ij] D²[ij]* + (1 - *y[ij]*)[*α - D²[ij]*][+]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*对比损失* (*i, j*) = *y[ij] D²[ij]* + (1 - *y[ij]*)[*α - D²[ij]*][+]'
- en: Note that [.]+ = max(0,.) in the loss function indicates hinge loss, and α is
    a predetermined threshold (margin) determining the max loss for when the two samples
    i and j are in different classes. Geometrically, this implies that two samples
    of different classes contribute to the loss only if the distance between them
    in the embedding space is less than this magin. Dij, as noted in the formulation,
    refers to the distance between two samples i and j in the embedding space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在损失函数中的 [.]+ = max(0,.) 表示hinge损失，α是一个预定的阈值（margin），用于确定当两个样本i和j属于不同类别时的最大损失。从几何上讲，这意味着只有当两个样本在嵌入空间中的距离小于这个margin时，不同类别的两个样本才会对损失做出贡献。Dij，如公式中所述，指的是两个样本i和j在嵌入空间中的距离。
- en: This loss is also known as Siamese loss, because we can visualize this as a
    twin network with shared parameters; each of the two CNNs is fed an image. Contrastive
    loss was employed in the seminal work by Chopra et al. [6] for the face-verification
    problem, where the objective is to verify whether two presented faces belong to
    the same identity. An illustration of this loss is provided in the context of
    face recognition in figure 10.10.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失也被称为Siamese损失，因为我们可以将其可视化为具有共享参数的孪生网络；两个CNN各自输入一张图像。对比损失在Chopra等人[6]的开创性工作中被用于人脸验证问题，其目标是验证两个展示的面孔是否属于同一身份。这种损失的示例在图10.10中提供了人脸识别的上下文。
- en: '![](../Images/10-10.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-10.png)'
- en: Figure 10.10 Computing contrastive loss requires two images. When the two images
    are of the same class, the optimization tries to put them closer in the embedding
    space, and vice versa when the images belong to different classes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 计算对比损失需要两张图像。当两张图像属于同一类别时，优化尝试在嵌入空间中将它们放得更近，反之亦然，当图像属于不同类别时。
- en: 'Notice that the choice of the margin α is the same for all dissimilar classes.
    Manmatha et al. [7] analyze the impact: this choice of α implies that for dissimilar
    identities, visually diverse classes are embedded in the same feature space as
    the visually similar ones. This assumption is stricter when compared to triplet
    loss (explained next) and restricts the structure of the embedding manifold, which
    subsequently makes learning tougher. The training complexity per epoch is O(N
    2) for a dataset of N samples, as this loss requires traversing a pair of samples
    to compute the contrastive loss.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有不同类别的 *margin α* 的选择是相同的。Manmatha等人[7]分析了其影响：这种α的选择意味着对于不同的身份，视觉上不同的类别被嵌入到与视觉上相似的类别相同的特征空间中。与三元组损失（下文将解释）相比，这种假设更为严格，并限制了嵌入流形的结构，这随后使得学习变得更加困难。对于包含N个样本的数据集，每个epoch的训练复杂度为O(N²)，因为这种损失需要遍历一对样本来计算对比损失。
- en: 10.3.4  Triplet loss
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.4  三元组损失
- en: 'Inspired by the seminal work on metric learning for nearest neighbor classification
    by Weinberger et al. [8], FaceNet (Schroff et al. [9]) proposed a modification
    suited for query-retrieval tasks called triplet loss. Triplet loss forces data
    points from the same class to be closer to each other than they are to a data
    point from another class. Unlike contrastive loss, triplet loss adds context to
    the loss function by considering both positive and negative pair distances from
    the same point. Mathematically, with respect to our problem formalization from
    earlier, triplet loss can be formulated as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 受Weinberger等人[8]关于最近邻分类的度量学习开创性工作的启发，FaceNet（Schroff等人[9]）提出了一种适用于查询检索任务的修改版，称为三元组损失。三元组损失通过考虑来自同一点的正负对距离，为损失函数添加了上下文。从数学上讲，与之前的问题形式化相关，三元组损失可以表示如下：
- en: '*l[triplet]* (*a, p, n*) = [*D[ap] − D[an] + α*][+]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*三元组* (*a, p, n*) = [*D[ap] − D[an] + α*][+]'
- en: Note that Dap represents the distance between the anchor and a positive sample,
    while Dan is the distance between the anchor and a negative sample. Figure 10.11
    illustrates the computation of the loss term using an anchor, a positive sample,
    and a negative sample. Upon successful training, the hope is that we will get
    all the same class pairs closer than pairs from different classes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Dap代表锚点和正样本之间的距离，而Dan是锚点和负样本之间的距离。图10.11展示了使用锚点、正样本和负样本计算损失项的过程。在成功训练后，我们希望所有同一类别的对比都会比不同类别的对对比得更近。
- en: '![](../Images/10-11.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-11.png)'
- en: Figure 10.11 Computing triplet loss requires three samples. The goal of learning
    is to embed the samples of the same class closer than samples belonging to different
    classes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 计算三元组损失需要三个样本。学习的目标是使同一类别的样本嵌入比不同类别的样本更近。
- en: Because computing triplet loss requires three terms, the training complexity
    per epoch is *O*(*N*³), which is computationally prohibitive on practical datasets.
    High computational complexity in triplet and contrastive losses have motivated
    a host of sampling approaches for efficient optimization and convergence. Let’s
    review the complexity of implementing these losses in a naive and straightforward
    manner.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算三元组损失需要三个项，每个epoch的训练复杂度是*O*(*N*³)，这在实际数据集上计算上是不可行的。三元组和对比损失中的高计算复杂度激发了许多用于高效优化和收敛的采样方法。让我们回顾一下以天真和直接的方式实现这些损失函数的复杂度。
- en: 10.3.5  Naive implementation and runtime analysis of losses
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.5  损失函数的简单实现和运行时分析
- en: 'Consider a toy example with the following specifications:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下规格的玩具示例：
- en: 'Number of identities (*N*): 100'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份数（*N*）：100
- en: 'Number of samples per identity (*S*): 10'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个身份的样本数（*S*）：10
- en: 'If we implement the losses in a naive manner (see figure 10.12), it leads to
    per-epoch (inner `for` loop,[1](#pgfId-1198704) in figure 10.12) training complexity:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以天真方式实现损失函数（参见图10.12），会导致每个epoch（图10.12中的内部`for`循环[1](#pgfId-1198704)）的训练复杂度：
- en: '*Cross-entropy loss* --This is a relatively straightforward loss. In an epoch,
    it just needs to traverse all samples. Hence our complexity here is *O*(*N × S*)
    = *O*(10³).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉熵损失* --这是一个相对简单的损失。在一个epoch中，它只需要遍历所有样本。因此，这里的复杂度是*O*(*N × S*) = *O*(10³)。'
- en: '*Contrastive loss* --This loss visits all pairwise distances, so complexity
    is quadratic in terms of number of samples (*N × S*): that is, *O*(100 × 10 ×
    100 × 10) = *O*(10⁶).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对比损失* --这个损失会访问所有成对距离，所以从样本数量来看，复杂度是二次的（*N × S*）：即*O*(100 × 10 × 100 × 10)
    = *O*(10⁶)。'
- en: '*Triplet loss* --For every loss computation, we need to visit three samples,
    so the worst-case complexity is cubic. In terms of total number of samples, that
    is *O*(10⁹).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*三元组损失* --对于每次损失计算，我们需要访问三个样本，所以最坏情况下的复杂度是立方。从样本总数来看，那就是*O*(10⁹)。'
- en: '![](../Images/10-12.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-12.png)'
- en: Figure 10.12 Algorithm 1, for a naive implementation
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 算法1，对于简单实现
- en: Despite the ease of computing cross-entropy loss, its performance is relatively
    low when compared to other embedding losses. Some intuitive explanations are pointed
    out in section 10.3.2\. In recent academic works (such as [10, 11, 13]), triplet
    loss has generally given better results than contrastive loss when provided with
    appropriate hard data mining, which we will explain in the next section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算交叉熵损失很容易，但与其他嵌入损失相比，其性能相对较低。一些直观的解释在10.3.2节中提出。在最近的学术作品中（如[10, 11, 13]），三元组损失在提供适当的硬数据挖掘时，通常比对比损失给出更好的结果，我们将在下一节中解释。
- en: NOTE In the following sections, we refer to triplet loss, owing to its high
    performance over contrastive loss in several academic works.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在以下章节中，我们提到三元组损失，因为它在多个学术作品中对比损失具有更高的性能。
- en: 'One important point to notice is that not many of the triplets of the O(109)
    contribute to the loss in a strong manner. In practice, during a training epoch,
    most of the triplets are trivial: that is, the current network is already at a
    low loss on these, and hence anchor-positive pairs of these trivial triplets are
    much closer (in the embedding space) than anchor-negative pairs. These trivial
    triplets do not add meaningful information to update the network parameters, thereby
    stagnating convergence. Furthermore, there are far fewer informative triplets
    than trivial triplets, which in turn leads to washing out the contribution of
    informative triplets.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的重要点是，在O(10^9)的三元组中，并不是很多对损失有显著的贡献。在实践中，在一个训练周期内，大多数的三元组都是微不足道的：也就是说，当前网络在这些数据上的损失已经很低，因此这些微不足道三元组的锚正对（在嵌入空间中）比锚负对更接近。这些微不足道的三元组不会为更新网络参数提供有意义的信息，从而阻碍了收敛。此外，信息三元组比微不足道三元组要少得多，这反过来又导致了信息三元组贡献的稀释。
- en: To improve the computational complexity of triplet enumeration and convergence,
    we need to come up with an efficient strategy for enumerating triplets and feed
    the CNN (during training) informative triplet samples (without trivial triplets). This
    process of selecting informative triplets is called mining. Informative data points
    is the essence of this chapter and is discussed in the following sections.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高三元组枚举的计算复杂度和收敛性，我们需要提出一种有效的策略来枚举三元组，并在训练过程中向CNN（无微不足道三元组）提供信息三元组样本。这个过程选择信息三元组被称为挖掘。信息数据点是本章的核心，将在以下几节中讨论。
- en: 'A popular strategy to tackle this cubic complexity is to enumerate triplets
    in the following manner:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理这种立方复杂度的流行策略是以下方式枚举三元组：
- en: Construct a triplet set using only the current batch constructed by the dataloader.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据加载器构建的当前批次仅构建三元组集合。
- en: Mine an informative triplet subset from this set.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个集合中挖掘一个信息三元组子集。
- en: The next section looks at this strategy in detail.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将详细探讨这种策略。
- en: 10.4  Mining informative data
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4  挖掘信息数据
- en: So far, we have looked at how triplet and contrastive losses are computationally
    prohibitive for practical dataset sizes. In this section, we take a deep dive
    into understanding the key steps during training a CNN for triplet loss and learn
    how to improve the training convergence and computational complexity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了三元组和对比损失在计算上对实际数据集大小的不切实际。在本节中，我们将深入了解训练CNN进行三元损失时的关键步骤，并学习如何提高训练收敛性和计算复杂度。
- en: The straightforward implementation in figure 10.12 is classified under offline
    training, as the selection of a triplet must consider the full dataset and therefore
    cannot be done on the fly while training a CNN. As we noted earlier, this approach
    of computing valid triplets is inefficient and is computationally infeasible for
    DL datasets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12中的直接实现被归类为离线训练，因为三元组的选取必须考虑整个数据集，因此在训练CNN时不能即时完成。正如我们之前提到的，这种计算有效三元组的方法效率低下，对于深度学习数据集来说在计算上是不可行的。
- en: To deal with this complexity, FaceNet [9] proposes using online batch-based
    triplet mining. The authors construct a batch on the fly and perform mining of
    triplets for this batch, ignoring the rest of the dataset outside this batch.
    This strategy proved effective and led to state-of-the-art accuracy in face recognition.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种复杂性，FaceNet [9] 提出使用基于在线批次的三元组挖掘。作者动态构建一个批次，并对该批次进行三元组的挖掘，忽略该批次之外的数据集。这种策略被证明是有效的，并导致了人脸识别中的最先进准确率。
- en: Let’s summarize this information flow during a training epoch (see figure 10.13).
    During training, mini-batches are constructed from the dataset, and valid triplets
    are subsequently identified for each sample in the mini-batch. These triplets
    are then used to update the loss, and the process iterates until all the batches
    are exhausted, thereby completing an epoch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下训练周期中的信息流（见图10.13）。在训练过程中，从数据集中构建小批量，然后为小批量中的每个样本识别有效三元组。然后使用这些三元组来更新损失，这个过程迭代进行，直到所有批次耗尽，从而完成一个周期。
- en: '![](../Images/10-13.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-13.png)'
- en: Figure 10.13 Information flow during an online training process. The dataloader
    samples a random subset of training data to the GPU. Subsequently, triplets are
    computed to update the loss.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 在线训练过程中的信息流。数据加载器从训练数据中采样一个随机子集到GPU。随后，计算三元组以更新损失。
- en: Similar to FaceNet, OpenFace [37] proposed a training scheme wherein the dataloader
    constructs a training batch of predefined statistics, and embeddings for the batch
    are computed on the GPU. Subsequently, valid triplets are generated on the CPU
    to compute the loss.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与FaceNet类似，OpenFace [37] 提出了一种训练方案，其中数据加载器构建一个具有预定义统计信息的训练批次，并在GPU上计算批次的嵌入。随后，在CPU上生成有效三元组以计算损失。
- en: In the next subsection, we look into an improved dataloader that can give us
    good batch statistics to mine triplets. Subsequently, we will explore how we can
    efficiently mine good, informative triplets to improve training convergence.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将探讨一个改进的数据加载器，它可以为我们提供良好的批次统计信息以挖掘三元组。随后，我们将探讨如何有效地挖掘好的、信息丰富的三元组以改善训练收敛性。
- en: 10.4.1  Dataloader
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 数据加载器
- en: Let’s examine the dataloader’s setup and its role in training with triplet loss.
    The dataloader selects a random subset from the dataset and is crucial to mining
    informative triplets. If we resort to a trivial dataloader to choose a random
    subset (mini-batch) of the dataset, it may not result in good class diversity
    for finding many triplets. For example, randomly selecting a batch with only one
    category will not have any valid triplets and thus will result in a wasteful batch
    iteration. We must take care at the dataloader level to have well distributed
    batches to mine triplets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查数据加载器的设置及其在三元组损失训练中的作用。数据加载器从数据集中选择一个随机子集，对于挖掘信息三元组至关重要。如果我们依赖于一个平凡的数据加载器来选择数据集的随机子集（迷你批次），则可能不会导致良好的类别多样性，从而无法找到许多三元组。例如，随机选择仅包含一个类别的批次将没有任何有效三元组，因此会导致无效的批次迭代。我们必须在数据加载器级别上确保批次分布良好，以便挖掘三元组。
- en: NOTE The requirement for better convergence at the dataloader level is to form
    a batch with enough class diversity to facilitate the triplet mining step in figure
    10.11.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在数据加载器级别上实现更好收敛的要求是形成一个具有足够类别多样性的批次，以促进图10.11中的三元组挖掘步骤。
- en: A general and effective approach to training is to first mine a set of triplets
    of size B, so that B terms contribute to the triplet loss. Once set B is chosen,
    their images are stacked to form a batch size of 3B images (B anchors, B positives,
    and B negatives), and subsequently 3B embeddings are computed to update the loss.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一般有效方法是首先挖掘一组大小为B的三元组，以便B个项对三元组损失做出贡献。一旦选择了B，它们的图像将被堆叠以形成一个3B图像的批次大小（B个锚点，B个正样本和负样本），然后计算3B个嵌入以更新损失。
- en: Hermans et al. [11], in their impressive work on revisiting triplet loss, realize
    the under-utilization of valid triplets in online generation presented in the
    previous section. In a set of 3B images (B anchors, B positives, B negatives),
    we have a total of 6B 2 - 4B valid triplets, so using only B triplets is under-utilization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Hermans等人[11]在他们对重新审视三元组损失所做的令人印象深刻的工作中，意识到在前一节中在线生成中有效三元组的低利用率。在一组3B张图像（B个锚点，B个正样本，B个负样本）中，我们总共有6B个2
    - 4B个有效三元组，因此仅使用B个三元组是低效的。
- en: Computing the number of valid triplets in stacked 3B images of B triplets
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 计算堆叠的3B张B三元组图像中的有效三元组数量
- en: To understand the computation of the number of valid triplets in a stack of
    3B images (that is, B anchors, B positives, B negatives), let’s assume we have
    exactly one pair of the same class. This implies we could choose 3B - 2 negatives
    for an (anchor, positive) pair. There are 2B possible anchor-positive pairs in
    this set, leading to a total of 2B (3B - 2) valid triplets. The following figure
    shows an example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解在3B图像堆叠中计算有效三元组数量（即B个锚点，B个正样本和负样本），让我们假设我们恰好有一对相同的类别。这意味着我们可以为（锚点，正样本）对选择3B
    - 2个负样本。在这个集合中有2B个可能的锚点-正样本对，导致总共2B (3B - 2)个有效三元组。以下图示了一个示例。
- en: '![](../Images/10-unnumb-1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-unnumb-1.png)'
- en: An example with B = 3\. Circles with the same pattern are of the same class.
    Since only the first two columns have a possible positive sample, there are a
    total of 2B (six) anchors. After selecting an anchor, we are left with 3B - 2
    (seven) negatives, implying a sum total of 2B (3B - 2) triplets.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: B = 3的示例。具有相同模式的圆圈属于同一类别。由于只有前两列有可能的正样本，总共有2B（六个）锚点。选择一个锚点后，我们剩下3B - 2（七个）负样本，这意味着总共有2B
    (3B - 2)个三元组。
- en: 'In light of the previous discussion, to use the triplets more efficiently,
    Hermans et al. propose a key organizational modification at the dataloader level:
    construct a batch by randomly sampling P identities from dataset *x* and subsequently
    sampling K images (randomly) for each identity, thus resulting in a batch size
    of PK images. Using this dataloader (with appropriate triplet mining), the authors
    demonstrate state-of-the-art accuracy on the task of person re-identification.
    We look more at the mining techniques introduced in [11] in the following subsections.
    Using this organizational modification, Kumar et al. [10, 12] demonstrate state-of-the-art
    results for the task of vehicle re-identification across many diverse datasets.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前的讨论，为了更有效地使用三元组，Hermans 等人提出在数据加载器级别进行一项关键的组织性修改：通过从数据集 *x* 中随机采样 P 个身份，然后为每个身份采样
    K 张图像（随机），从而得到一个大小为 PK 的批次。使用这个数据加载器（适当的三元组挖掘），作者展示了在人员重识别任务上达到最先进的准确率。我们将在接下来的子节中更详细地探讨[11]中引入的挖掘技术。使用这种组织性修改，Kumar
    等人[10, 12]在许多不同的数据集上展示了车辆重识别任务的最先进结果。
- en: Owing to the superior results on re-identification tasks, [11] has become one
    of the mainstays in recognition literature, and the batch construction (dataloader)
    is now a standard in practice. The default recommendation for the batch size is
    P = 18, K = 4, leading to 42 samples.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在重识别任务上取得了优越的结果，[11]已成为识别文献中的主要支柱，批次构建（数据加载器）现在已成为实践中的标准。默认推荐批次大小为 P = 18，K
    = 4，导致 42 个样本。
- en: Computing the number of valid triplets
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 计算有效三元组的数量
- en: 'Let’s make this concept clearer with a working example of computing the number
    of valid triplets in a batch. Assuming we have selected a random batch of size
    PK:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个计算批次中有效三元组数量的工作示例来使这个概念更清晰。假设我们随机选择了一个大小为 PK 的批次：
- en: P = 10 different classes
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P = 10 个不同的类别
- en: K = 4 samples per class
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个类别的样本数 = 4 个样本
- en: 'Using these values, we have the following batch statistics:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值，我们得到以下批次统计信息：
- en: Total number of anchors = 40 = (PK)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锚点的总数 = 40 = (PK)
- en: Number of positive samples per anchor = 3 = (K - 1)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个锚点的正样本数量 = 3 = (K - 1)
- en: Number of negative samples per anchor = 9 × 4 = (K(P - 1))
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个锚点的负样本数量 = 9 × 4 = (K(P - 1))
- en: Total number of valid triplets = products of the previous results = 40 × 3 ×
    (9 × 4)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效三元组的总数 = 前述结果的乘积 = 40 × 3 × (9 × 4)
- en: Taking a peek at upcoming concepts on mining informative triplets, notice that
    for each anchor, we have a set of positive samples and a set of negative samples.
    We argued earlier that many triplets are non-informative, and hence in the subsequent
    sections we look at various ways to filter out important triplets. More precisely,
    we examine techniques that help filter out informative subsets of positive and
    negative samples (for an anchor).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在挖掘信息三元组的前瞻概念中窥视，注意到对于每个锚点，我们都有一个正样本集和一个负样本集。我们之前已经论证了许多三元组是非信息性的，因此在后续章节中，我们将探讨各种过滤重要三元组的方法。更确切地说，我们检查帮助过滤正负样本集中信息子集的技术（对于一个锚点）。
- en: Now that we have built an efficient dataloader for mining triplets, we are ready
    to explore various techniques for mining informative triplets while training a
    CNN. In the following sections, we first look at hard data mining in general and
    subsequently focus on online generation (mining) of informative triplets following
    the batch construction approach in [11].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个用于挖掘三元组的有效数据加载器，我们准备探索在训练 CNN 时挖掘信息三元组的各种技术。在接下来的章节中，我们首先概述硬数据挖掘，然后专注于在线生成（挖掘）信息三元组，这遵循了[11]中的批次构建方法。
- en: '10.4.2  Informative data mining: Finding useful triplets'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2  信息性数据挖掘：寻找有用的三元组
- en: Mining informative samples while training a machine learning model is an important
    problem, and many solutions exist in academic literature. We take a quick peek
    at them here.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时挖掘信息样本是一个重要问题，学术文献中存在许多解决方案。我们在这里简要地看一下它们。
- en: 'A popular sampling approach to find informative samples is hard data mining,
    which is used in many CV applications such as object detection and action localization.
    Hard data mining is a bootstrapping technique used in iterative training of a
    model: at every iteration, the current model is applied on a validation set to
    mine hard data on which this model performs poorly. Only this hard data is then
    presented to the optimizer, which increases the ability of the model to learn
    effectively and converge faster to an optimum. On the flip side, if a model is
    only presented with hard data, which could consist of outliers, its ability to
    discriminate outliers with respect to normal data suffers, stalling the training
    progress. An outlier in a dataset could be a result of mislabeling or a sample
    captured with poor image quality.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的采样方法来寻找信息样本是困难数据挖掘，它在许多计算机视觉应用中如目标检测和动作定位中被使用。困难数据挖掘是一种用于模型迭代训练的自举技术：在每次迭代中，当前模型应用于验证集以挖掘模型表现不佳的困难数据。然后只将这组困难数据呈现给优化器，这增加了模型有效学习和更快收敛到最优的能力。另一方面，如果模型只接触到困难数据，这些数据可能包含异常值，那么它对正常数据的异常值判别能力会受到影响，从而阻碍训练进度。数据集中的异常值可能是标签错误或图像质量差的样本捕获的结果。
- en: In the context of triplet loss, a hard negative sample is one that is closer
    to the anchor (as this sample would incur a high loss). Similarly, a hard positive
    sample is one that is far from an anchor in embedding space.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在三元损失的上下文中，一个困难负样本是离锚点更近的样本（因为这个样本会导致高损失）。同样，一个困难正样本是在嵌入空间中远离锚点的样本。
- en: 'To deal with outliers during hard data sampling, FaceNet [9] proposed semi-hard
    sampling that mines moderate triplets that are neither too hard nor too trivial
    for getting meaningful gradients during training. This is done by using the margin
    parameter: only negatives that lie in the margin and are farther from the selected
    positive for an anchor are considered (see figure 10.14), thereby ignoring negatives
    that are too easy and too hard. However, this in turn adds additional burden on
    training for tuning an additional hyperparameter. This ad hoc strategy of semi-hard
    negatives is put into practice in a large batch size of 1,800 images, thereby
    enumerating triplets on the CPU. Notice that with the default batch size (42 images)
    in [11], it is possible to enumerate the set of valid triplets efficiently on
    the GPU.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在困难数据采样期间处理异常值，FaceNet [9] 提出了半困难采样，它挖掘既不太困难也不太平凡的三元组，以便在训练期间获得有意义的梯度。这是通过使用边界参数来实现的：只有位于边界并且比选定的正样本对锚点更远的负样本被考虑（见图10.14），从而忽略了太容易和太困难的负样本。然而，这反过来又增加了调整额外超参数的训练负担。这种半困难负样本的临时策略在大批量的1,800张图像中得到了实践，从而在CPU上枚举三元组。请注意，在[11]中的默认批量大小（42张图像）中，可以在GPU上有效地枚举有效三元组的集合。
- en: '![](../Images/10-14.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-14.png)'
- en: 'Figure 10.14 Margin: grading triplets into hard, semi-hard, and easy. This
    illustration (in the context of face recognition) is for an anchor and a corresponding
    negative sample. Therefore, negative samples that are closer to the anchor are
    hard.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 边界：将三元组分为困难、半困难和容易。这个插图（在人脸识别的上下文中）是一个锚点和相应的负样本。因此，靠近锚点的负样本是困难的。
- en: 'Figure 10.15 illustrates the hardness of a triplet. Remember that a positive
    sample is harder if the network (at a training epoch) puts this sample far from
    its anchor in the embedding space. Similarly, in a plot of distances from the
    anchor to negative data, the samples closer (less distant) to the anchor are harder.
    As a reminder, here is the triplet loss function for an anchor (*a*), positive
    (*p*), and negative (*n*):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15说明了三元组的困难程度。记住，如果一个正样本在训练时间步的网络中离其锚点在嵌入空间中很远，那么这个正样本就更加困难。同样，在从锚点到负数据的距离图中，靠近（距离更短）锚点的样本更困难。作为提醒，以下是锚点（*a*）、正样本（*p*）和负样本（*n*）的三元损失函数：
- en: '*l[triplet]* (*a, p, n*) = [*D[ap] − D[an] + α*][+]'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*l[triplet]* (*a, p, n*) = [*D[ap] − D[an] + α*][+]'
- en: '![](../Images/10-15.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-15.png)'
- en: Figure 10.15 Hard-positive and hard-negative data. The plot shows the distances
    of positive samples (top) and negative samples (bottom) with respect to an anchor
    (at a particular epoch). The hardness of samples increases as we move from left
    to right on both plots.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 困难正样本和困难负样本数据。图显示了正样本（顶部）和负样本（底部）相对于锚点（在特定时间步）的距离。样本的困难程度随着我们在两个图上从左到右移动而增加。
- en: 'Having explored the concept of hard data and its pitfalls, we will now explore
    various online triplet mining techniques for our batch. Once a batch (of size
    PK ) is constructed by the dataloader, there are PK possible anchors. How to find
    positive and negative data for these anchors is the crux of mining techniques. First
    we look at two simple and effective online triplet mining techniques: batch all
    (BA) and batch hard (BH ).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了硬数据及其缺陷的概念之后，我们现在将探讨针对我们的批次的多种在线三元组挖掘技术。一旦数据加载器构建了一个批次（大小为PK），就有PK个可能的锚点。如何为这些锚点找到正负数据是挖掘技术的关键。首先，我们来看两种简单而有效的在线三元组挖掘技术：批全（BA）和批硬（BH）。
- en: 10.4.3  Batch all (BA)
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 批全（BA）
- en: In the context of a batch, batch all (BA ) refers to using all possible and
    valid triplets; that is, we are not performing any ranking or selection of triplets.
    In implementation terms, for an anchor, this loss is computed by summing across
    all possible valid triplets. For a batch size of PK images, since BA selects all
    triplets, the number of terms updating the triplet loss is PK(K - 1)(K(P - 1)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在批次的上下文中，批全（BA）指的是使用所有可能的和有效的三元组；也就是说，我们不会对三元组进行任何排序或选择。在实现方面，对于一个锚点，这个损失是通过对所有可能的有效三元组求和来计算的。对于一个包含PK个图像的批次，由于BA选择了所有三元组，因此更新三元组损失的项数是PK(K
    - 1)(K(P - 1))。
- en: Using this approach, all samples (triplets) are equally important; hence this
    is straightforward to implement. On the other hand, BA can potentially lead to
    information averaging out. In general, many valid triplets are trivial (at a low
    loss or non-informative), and only a few are informative. Summing across all valid
    triplets with equal weights leads to averaging out the contribution of the informative
    triplets. Hermans et al. [11] experienced this averaging out and reported it in
    the context of person re-identification.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，所有样本（三元组）都同等重要；因此，这很容易实现。另一方面，BA可能导致信息平均化。一般来说，许多有效的三元组是平凡的（损失低或非信息性），而只有少数是有信息的。对所有有效的三元组使用相同的权重求和会导致有信息三元组的贡献平均化。Hermans等人[11]经历了这种平均化，并在人重识别的背景下进行了报告。
- en: 10.4.4  Batch hard (BH)
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.4 批硬（BH）
- en: As opposed to BA, batch hard (BH ) considers only the hardest data for an anchor.
    For each possible anchor in a batch, BH computes the loss with exactly one hardest
    positive data item and one hardest negative data item. Notice that here, the hardness
    of a datapoint is relative to the anchor. For a batch size of PK images, since
    BH selects only one positive and one negative per anchor, the number of terms
    updating the triplet loss is PK (total number of possible anchors).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与BA不同，批硬（BH）只考虑锚点的最硬数据。对于批次中的每个可能的锚点，BH使用一个最硬的正数据项和一个最硬的负数据项来计算损失。请注意，在这里，数据点的硬度相对于锚点是相对的。对于一个包含PK个图像的批次，由于BH对每个锚点只选择一个正样本和一个负样本，因此更新三元组损失的项数是PK（可能的锚点总数）。
- en: 'BH is robust to information averaging out, because trivial (easier) samples
    are ignored. However, it is potentially difficult to disambiguate with respect
    to outliers: outliers can creep in due to incorrect annotations, and the model
    tries hard to converge on them, thereby jeopardizing training quality. In addition,
    when a not-pretrained network is used prior to using BH, the hardness of a sample
    (with respect to an anchor) cannot be determined reliably. There is no way to
    gain this information during training, because the hardest sample is now any random
    sample, and this can lead to a stall in training. This is reported in [9] and
    when BH is applied to train a network from scratch in the context of vehicle re-identification
    in [10].'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BH对信息平均化具有鲁棒性，因为简单的（较容易的）样本被忽略了。然而，它可能难以区分异常值：异常值可能由于错误的标注而渗入，并且模型会努力收敛到它们，从而危及训练质量。此外，在使用BH之前使用未预训练的网络时，样本的硬度（相对于锚点）无法可靠地确定。在训练过程中无法获得此类信息，因为最硬的样本现在是任何随机样本，这可能导致训练停滞。这已在[9]中报告，并且在[10]中将BH应用于从零开始训练车辆重识别网络时也出现了这种情况。
- en: To visually understand BA and BH, let’s look again at our figure illustrating
    the distances of the anchor to all positive and negative data (figure 10.16).
    BA performs no selection and uses all five samples to compute a final loss, whereas
    BH uses only the hardest available data (ignoring all the rest). Figure 10.17
    shows the algorithm outline for computing BH and BA.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解BA和BH，让我们再次查看我们用来表示锚点与所有正负数据距离的图（图10.16）。BA不进行选择，使用所有五个样本来计算最终的损失，而BH只使用最硬的数据（忽略所有其他数据）。图10.17显示了计算BH和BA的算法概述。
- en: An alternative formalization of triplet loss
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 三重损失函数的另一种形式化
- en: Ristani et al., in their famous paper on features for multi-camera re-identification
    [13], unify various batch-sampling techniques under one expression. In a batch,
    let a be an anchor sample and N(*a*) and P(*a*) represent a subset of negative
    and positive samples for the corresponding anchor a. The triplet loss can then
    be written as
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Ristani等人[13]在他们关于多摄像头重识别特征的著名论文中，将各种批量采样技术统一在一个表达式中。在一个批次中，设a为一个锚点样本，N(*a*)和P(*a*)代表对应锚点a的负样本和正样本的子集。然后三重损失可以写成
- en: '![](../Images/10-17_Ea.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图10-17_Ea](../Images/10-17_Ea.png)'
- en: For an anchor sample a, wp represents the weight (importance) of a positive
    sample p; similarly, wn signifies the importance of a negative sample n. The total
    loss in an epoch is then obtained by
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于锚点样本a，wp代表正样本p的权重（重要性）；同样，wn表示负样本n的重要性。在一个时间步的总损失通过以下方式获得
- en: '![](../Images/10-17_Eb.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图10-17_Eb.png]'
- en: In this formulation, BA and BH could be integrated as shown in the following
    figure (see also table 10.1 in the following section). The Y-axis in this figure
    represents selection weight-age.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，BA和BH可以像以下图所示那样集成（也参见下一节中的表10.1）。这个图中的Y轴表示选择权重。
- en: '![](../Images/10-unnumb-2.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图10-unnumb-2](../Images/10-unnumb-2.png)'
- en: Plot showing selection weights for positive samples with respect to an anchor.
    For BA, all samples are equally important, while BH gives importance to only the
    hardest samples (the rest are ignored).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了正样本相对于锚点的选择权重。对于BA，所有样本同等重要，而BH只重视最困难的样本（其余的都被忽略）。
- en: '![](../Images/10-16.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图10-16](../Images/10-16.png)'
- en: 'Figure 10.16 Illustration of hard data: distances of positive samples from
    an anchor (at a particular epoch) (left); distances of negative samples from an
    anchor (right). BA takes all samples into account, whereas BH takes samples only
    at the far-right bar (the hardest-positive data for this mini-batch).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16 困难数据的说明：正样本与锚点（在特定时间步）的距离（左）；负样本与锚点的距离（右）。BA考虑了所有样本，而BH只考虑最右侧的条形（这个批次的最难正数据）。
- en: '![](../Images/10-17.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图10-17](../Images/10-17.png)'
- en: Figure 10.17 Algorithm for computing BA and BH
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17 计算BA和BH的算法
- en: 10.4.5  Batch weighted (BW)
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.5 批量加权（BW）
- en: 'BA is a straightforward sampling that weights all samples uniformly. This uniform
    weight distribution can ignore the contribution of important tough samples, as
    these samples are typically outnumbered by trivial, easy samples. To mitigate
    this issue with BA, Ristani et al. [13] employ a batch weighted (BW ) weighting
    scheme: a sample is weighted based on its distance from the corresponding anchor,
    thereby giving more importance to informative (harder) samples than trivial samples.
    Corresponding weights for positive and negative data are shown in table 10.1\.
    Figure 10.18 demonstrates the weighting of samples in this technique.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: BA是一种简单的采样，对所有样本进行均匀加权。这种均匀的权重分布可能会忽略重要困难样本的贡献，因为这些样本通常被琐碎的简单样本所超过。为了缓解BA中的这个问题，Ristani等人[13]采用了一种批量加权（BW）的加权方案：一个样本的权重基于其与相应锚点的距离，从而比简单样本赋予信息性（更困难）样本更多的重视。正负数据的相应权重在表10.1中显示。图10.18展示了这种技术中样本的加权情况。
- en: Table 10.1 Snapshot of various ways to mine good positive xp and negative xn
    [10]. BS and BW are explored in the upcoming section with examples.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 矿掘良好正xp和负xn的各种方式的快照[10]。BS和BW将在下一节中通过示例进行探讨。
- en: '| Mining | Positive weight: wp | Negative weight: wn | Comments |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 矿掘 | 正样本权重：wp | 负样本权重：wn | 评论 |'
- en: '| All (BA) | 1 | 1 | All samples are weighted uniformly. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 所有（BA） | 1 | 1 | 所有样本都进行均匀加权。 |'
- en: '| Hard (BH) | ![](../Images/10-17_ETa.png)  | ![](../Images/10-17_ETb.png)  |
    Pick one hardest sample. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 困难（BH） | ![图10-17_ETa](../Images/10-17_ETa.png)  | ![图10-17_ETb](../Images/10-17_ETb.png)  |
    选择一个最困难的样本。 |'
- en: '| Sample (BS) | ![](../Images/10-17_ETb.png)  | ![](../Images/10-17_ETd.png)  |
    Pick one from the multinomial distribution. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 样本（BS） | ![图10-17_ETb](../Images/10-17_ETb.png)  | ![图10-17_ETd](../Images/10-17_ETd.png)  |
    从多项式分布中选择一个。 |'
- en: '| Weighted (BW) | ![](../Images/10-17_Ec.png) | ![](../Images/10-17_Ed.png)
    | Weights are sampled based on their distance from the anchor. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 加权（BW） | ![图10-17_Ec](../Images/10-17_Ec.png) | ![图10-17_Ed](../Images/10-17_Ed.png)
    | 权重基于其与锚点的距离进行采样。 |'
- en: '![](../Images/10-18.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图10-18](../Images/10-18.png)'
- en: Figure 10.18 BW illustration of selecting positive data for the anchor in the
    left plot. In this case, all five positive samples are used (as in BA), but a
    weight-age is assigned to each sample. Unlike BA, which weighs every sample equally,
    the plot at right weighs each sample in proportion to the corresponding distance
    from the anchor. This effectively means we are paying more attention to a positive
    sample that is farther from the anchor (and thus is harder and more informative).
    Negative data for this anchor is chosen in the same manner, but with reverse weight-age.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18 BW选择左图中锚点正数据的BW示意图。在这种情况下，使用了所有五个正样本（如BA），但每个样本都被分配了权重。与BA不同，BA对每个样本的权重相同，而右边的图按与锚点相应的距离比例对每个样本进行加权。这意味着我们更加关注远离锚点的正样本（因此更难且更有信息量）。对于这个锚点的负数据也是以同样的方式选择的，但权重相反。
- en: 10.4.6  Batch sample (BS)
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.6 批量样本（BS）
- en: Another sampling technique is batch sample (BS ); it is actively discussed in
    the implementation page of Hermans el al. [11] and has been used for state-of-the-art
    vehicle re-identification by Kumar et al. [10]. BS uses the distribution of anchor-to-sample
    distances to mine[2](#pgfId-1198984) positive and negative data for an anchor
    (see figure 10.19). This technique thereby avoids sampling outliers when compared
    with BH, and it also hopes to determine the most relevant sample as the sampling
    is done using a distances-to-anchor distribution.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种采样技术是批量样本（BS）；它在Hermans等人[11]的实现页面上被积极讨论，并且已经被Kumar等人[10]用于最先进的车辆再识别。BS使用锚点到样本的距离分布来挖掘[2](#pgfId-1198984)个正负数据样本用于锚点（见图10.19）。与BH相比，这种技术避免了采样异常值，并且它还希望确定最相关的样本，因为采样是使用距离到锚点的分布进行的。
- en: '![](../Images/10-19.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图10.19](../Images/10-19.png)'
- en: Figure 10.19 BS illustration of selecting a positive data for an anchor. Similarly
    to BH, the aim is to find one positive data item (for the anchor in the left plot)
    that is informative and not an outlier. BH would take the hardest data item, which
    could lead to finding outliers. BS uses the distances as a distribution to mine
    a sample in a categorical fashion, thereby selecting a sample that is informative
    and that may not be an outlier. (Note that this is a random multinomial selection;
    we chose the third sample here just to illustrate the concept.)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19 BS选择锚点正数据的示意图。与BH类似，目标是找到一个信息丰富且不是异常值的数据项（对于左图中的锚点）。BH会选择最困难的数据项，这可能导致找到异常值。BS使用距离作为分布来以分类方式挖掘样本，从而选择一个信息丰富且可能不是异常值的样本。（注意，这是一个随机多项式选择；我们在这里选择第三个样本只是为了说明这个概念。）
- en: Now, let’s unpack these ideas by working through a project and diving deeper
    into the machinery required for training and testing a CNN for an embedding.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个项目和深入了解训练和测试CNN进行嵌入所需的机制来解开这些想法。
- en: '10.5  Project: Train an embedding network'
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 项目：训练嵌入网络
- en: 'In this project, we put our concepts into practice by building an image-based
    query retrieval system. We chose two problems that are popular in the visual embedding
    literature and have been actively studied to find better solutions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们通过构建一个基于图像的查询检索系统将我们的概念付诸实践。我们选择了在视觉嵌入文献中流行的两个问题，并且已经积极研究以找到更好的解决方案：
- en: Shopping dilemma --Find me apparel that is similar to a query item.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购物困境--找到与我查询项目相似的服装。
- en: Re-identification --Find similar cars in a database; that is, identify a car
    from different viewpoints (cameras).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再识别--在数据库中找到相似车辆；即从不同视角（摄像头）识别一辆车。
- en: 'Regardless of the tasks, the training, inference, and evaluation processes
    are the same. Here are some of the ingredients for successfully training an embedding
    network:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪些任务，训练、推理和评估过程都是相同的。以下是成功训练嵌入网络的一些关键要素：
- en: Training set --We follow a supervised learning approach with annotations underlining
    the inherent similarity measure. The dataset can be organized into a set of folders
    where each folder determines the identity/category of the images. The objective
    is that images belonging to the same category are kept closer to one another in
    the embedding space, and vice versa for images in separate categories.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集--我们遵循一个带有注释的监督学习方法，这些注释强调了固有的相似性度量。数据集可以被组织成一系列文件夹，每个文件夹确定图像的标识/类别。目标是使属于同一类别的图像在嵌入空间中彼此更接近，反之亦然。
- en: 'Testing set --The test set is usually split into two sets: query and gallery
    (often, academic papers refer to the gallery set as the test set). The query set
    consists of images that are used as queries. Each image in the gallery set is
    ranked (retrieved) against every query image. If the embedding is learned perfectly,
    the top-ranked (retrieved) items for a query all belong to the same class.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集--测试集通常分为两个集合：查询集和图库集（通常，学术论文将图库集称为测试集）。查询集由用作查询的图像组成。图库集中的每张图像都与每张查询图像进行排名（检索）。如果嵌入被完美学习，则查询的最高排名（检索）物品都属于同一类。
- en: Distance metric --To express similarity between two images in an embedding space,
    we use the Euclidean (L2) distance between the respective embeddings.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离度量--为了在嵌入空间中表达两张图像之间的相似性，我们使用相应的嵌入之间的欧几里得（L2）距离。
- en: Evaluation --To quantitatively evaluate a trained model, we use the top-k accuracy
    and mean average precision (mAP) metrics explained in chapters 4 and 7, respectively.
    For each object in a query set, the aim is to retrieve a similar identity from
    the test set (gallery set). AP(q) for a query image q is defined as
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估--为了定量评估训练好的模型，我们使用第 4 章和第 7 章中解释的 top-k 准确率和平均精度（mAP）指标。对于查询集中的每个对象，目标是检索测试集（图库集）中相似的身份。对于查询图像
    q 的 AP(q) 定义为
- en: '![](../Images/10-19_Ea.png)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/10-19_Ea.png)'
- en: where P(k) represents precision at rank k, Ngt(q) is the total number of true
    retrievals for q, and δk is a Boolean indicator function. So, its value is 1 when
    the matching of query image q to a test image is correct at rank £ k. Correct
    retrieval means the ground-truth label for both query and test is the same.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 P(k) 表示排名 k 的精确度，Ngt(q) 是查询 q 的总真实检索数，δk 是一个布尔指示函数。所以，当查询图像 q 与测试图像的正确匹配发生在排名
    k 时，其值为 1。正确的检索意味着查询和测试的地面真实标签是相同的。
- en: The mAP is then computed as an average over all query images
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后计算 mAP 作为所有查询图像的平均值
- en: '![](../Images/10-19_Eb.png)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/10-19_Eb.png)'
- en: where Q is the total number of query images. The following sections look at
    both tasks in more detail.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 Q 是查询图像的总数。以下几节将更详细地探讨这两个任务。
- en: '10.5.1  Fashion: Get me items similar to this'
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 时尚：给我找类似这个的物品
- en: The first task is to determine whether two images taken in a shop belong to
    the same clothing item. Shopping objects (clothes, shoes) related to fashion are
    key areas of visual search in industrial applications such as image-recommendation
    engines that recommend products similar to what a shopper is looking for. Liu et
    al. [3] introduced one of the largest datasets (DeepFashion) for shopping image-retrieval
    tasks. This benchmark contains 54,642 images of 11,735 clothing items from the
    popular Forever 21 catalog. The dataset comprises 25,000 training images and about
    26,000 test images, split across query and gallery sets; figure 10.20 shows sample
    images.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是确定在商店拍摄的两张图片是否属于同一服装物品。与时尚相关的购物对象（衣服、鞋子）是工业应用中视觉搜索的关键领域，例如图像推荐引擎，这些引擎推荐与购物者所寻找的产品相似的产品。刘等人
    [3] 为购物图像检索任务引入了最大的数据集之一（DeepFashion）。该基准包含来自流行的Forever 21 目录的 11,735 件服装的 54,642
    张图片。该数据集包括 25,000 张训练图像和大约 26,000 张测试图像，分为查询集和图库集；图 10.20 显示了样本图像。
- en: '![](../Images/10-20.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-20.png)'
- en: Figure 10.20 Each row indicates a particular category and corresponding similar
    images. A perfectly learned embedding would make embeddings of images in each
    row closer to each other than any two images across columns (which belong to different
    apparel categories). (Images in this figure are taken from the DeepFashion dataset
    [3].)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.20 每一行表示一个特定的类别及其相应的相似图像。一个完美学习的嵌入将使每行中图像的嵌入彼此更接近，而不是任何两列图像（属于不同的服装类别）之间的图像。（此图中的图像取自
    DeepFashion 数据集 [3]。）
- en: 10.5.2  Vehicle re-identification
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 车辆重新识别
- en: Re-identification is the task of matching the appearance of objects in and across
    camera networks. A usual pipeline here involves a user seeking all instances of
    a query object’s presence in all cameras within a network. For example, a traffic
    regulator may be looking for a particular car across a city-wide camera network.
    Other examples are person and face re-identification, which are mainstays in security
    and biometrics.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 重新识别是匹配相机网络中及跨相机网络中物体外观的任务。通常的流程涉及用户在网络的全部相机中寻找查询物体出现的所有实例。例如，交通管理员可能在全市范围的相机网络中寻找一辆特定的汽车。其他例子包括人和面部重新识别，这些在安全和生物识别领域是主流。
- en: This task uses the famous VeRi dataset from Liu et al. [14, 36]. This dataset
    encompasses 40,000 bounding-box annotations of 776 cars (identities) across 20
    cameras in traffic surveillance scenes; figure 10.21 shows sample images. Each
    vehicle is captured by 2 to 18 cameras in various viewpoints and varying illuminations.
    Notably, the viewpoints are not restricted to only front/rear but also include
    side views, thereby making this a challenging dataset. The annotations include
    make and model of vehicles, color, and inter-camera relations and trajectory information.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本任务使用来自 Liu 等人的著名 VeRi 数据集 [14, 36]。该数据集包含 776 辆车（身份）在交通监控场景中 20 个摄像头下的 40,000
    个边界框标注；图 10.21 展示了样本图像。每辆车由 2 到 18 个摄像头从不同的视角和不同的光照条件下捕捉。值得注意的是，视角不仅限于前/后视图，还包括侧面视图，从而使这个数据集更具挑战性。标注包括车辆的品牌和型号、颜色以及相机间关系和轨迹信息。
- en: '![](../Images/10-21.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-21.png)'
- en: Figure 10.21 Each row indicates a vehicle class. Similar to the apparel task,
    the goal (training an embedding CNN) here is to push the embeddings of the same
    class closer than the embeddings of different classes. (Images in this figure
    are from the VeRi dataset [14].)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.21 每一行表示一个车辆类别。与服装任务类似，这里的目的是（训练嵌入 CNN）使同一类别的嵌入比不同类别的嵌入更接近。（此图中的图像来自 VeRi
    数据集 [14]。）
- en: We will use only category (or identity) level annotations; we will not use attributes
    like make, model, and spatio-temporal location. Incorporating more information
    during training could help gain accuracy, but this is beyond the scope of this
    chapter. However, the last part of the chapter references some cool new developments
    in incorporating multi-source information for learning embeddings.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用类别（或身份）级别的标注；我们不会使用品牌、型号和时空位置等属性。在训练过程中加入更多信息可能有助于提高准确性，但这超出了本章的范围。然而，本章的最后部分引用了一些关于在嵌入学习中结合多源信息的新发展。
- en: 10.5.3  Implementation
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.3 实现方法
- en: This project uses the GitHub codebase of triplet learning ([https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling](https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling))
    attached to [11]. Dataset preprocessing and a summary of steps are available with
    the book’s downloadable code; go to the project’s Jupyter notebook to follow along
    with a step-by-step tutorial of the project implementation. TensorFlow users are
    encouraged to look at the blog post “Triplet Loss and Online Triplet Mining in
    TensorFlow” by Olivier Moindrot ([https://omoindrot.github.io/triplet-loss](https://omoindrot.github.io/triplet-loss))
    to understand various ways of implementing triplet loss.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目使用与 [11] 相关的 GitHub 代码库的 triplet learning（[https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling](https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling)）。数据预处理和步骤总结可在本书的可下载代码中找到；前往项目的
    Jupyter notebook，可以跟随项目实现的逐步教程。鼓励 TensorFlow 用户查看 Olivier Moindrot 的博客文章“TensorFlow
    中的 Triplet Loss 和在线 Triplet Mining”（[https://omoindrot.github.io/triplet-loss](https://omoindrot.github.io/triplet-loss)），以了解实现
    triplet loss 的各种方法。
- en: 'Training a deep CNN involves several key hyperparameters, and we briefly discuss
    them here. Following is a summary of the hyperparameters we set for this project:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度卷积神经网络（CNN）涉及几个关键的超参数，我们在此简要讨论它们。以下是本项目设置的超参数总结：
- en: Pre-training is performed on the ImageNet dataset [15].
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练在 ImageNet 数据集 [15] 上进行。
- en: Input image size is 224 × 224.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像大小为 224 × 224。
- en: 'Meta-architecture : We use Mobilenet-v1 [16], which has 569 million MACs and
    measures the number of fused multiplication and addition operations. This architecture
    has 4.24 million parameters and achieves a top-1 accuracy of 70.9% on ImageNet’s
    image classification benchmark, with input image size of 224 × 224.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元架构：我们使用 Mobilenet-v1 [16]，它有 569 百万 MACs，并测量融合的乘法和加法操作的数量。这个架构有 424 万个参数，在
    ImageNet 的图像分类基准测试中实现了 70.9% 的 top-1 准确率，输入图像大小为 224 × 224。
- en: 'Optimizer : We use the Adam optimizer [17] with default hyperparameters (ε
    = 10-3, β 1 = 0.9, β 2 = 0.999). Initial learning rate is set to 0.0003.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器：我们使用默认超参数的 Adam 优化器 [17]（ε = 10^-3，β1 = 0.9，β2 = 0.999）。初始学习率设置为 0.0003。
- en: Data augmentation is performed in an online fashion using a standard image-flip
    operation.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强通过在线方式使用标准的图像翻转操作进行。
- en: Batch size is 18 (P ) randomly sampled identities, with 4 (K ) samples per identity,
    for a total of 18 × 4 samples in a batch.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小为 18（P）随机采样的身份，每个身份 4（K）个样本，因此每个批次共有 18 × 4 个样本。
- en: 'Margin : The authors replaced the hinge loss [.]+ with a smooth variation called
    softplus: ln(1 + .). Our experiments also apply softplus instead of using a hard
    margin.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边距：作者将 hinge 损失 [.]+ 替换为一种称为 softplus 的平滑变化：ln(1 + .)。我们的实验也应用 softplus 而不是使用硬边距。
- en: Embedding dimension corresponds to the dimension of the last fully connected
    layer. We fix this to 128 units for all experiments. Using a lower embedding size
    is helpful for computational efficiency.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度对应于最后一个全连接层的维度。我们将其固定为所有实验的 128 个单位。使用较小的嵌入大小有助于提高计算效率。
- en: DEFINITION In computing, the multiply-accumulate operation is a common step
    that computes the product of two numbers and adds that product to an accumulator.
    The hardware unit that performs the operation is known as a multiplier-accumulator
    (MAC, or MAC unit ); the operation itself is also often referred to as MAC or
    a MAC operation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：在计算机科学中，乘累加操作是一个常见的步骤，它计算两个数字的乘积并将该乘积加到累加器中。执行此操作的硬件单元称为乘累加器（MAC，或 MAC 单元）；该操作本身也常被称为
    MAC 或 MAC 操作。
- en: A note on comparisons to state-of-the-art approaches
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 关于与最先进方法的比较的说明
- en: 'Before diving into comparisons, remember that training a deep neural network
    requires tuning several hyperparameters. This may in turn lead to pitfalls while
    comparing several algorithms: for example, an approach could perform better if
    the underlying CNN performs favorably on the same pretrained dataset(s). Other
    similar hyperparameters are the training algorithm choice (such as vanilla SGD
    or a more sophisticated Adam) and many other parameters that we have seen throughout
    this book. You must delve deeper into an algorithm’s machinery to see the complete
    picture.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入比较之前，请记住，训练一个深度神经网络需要调整几个超参数。这反过来可能导致比较几个算法时的陷阱：例如，如果底层 CNN 在相同的预训练数据集上表现良好，则一种方法可能会表现得更好。其他类似的超参数包括训练算法的选择（例如
    vanilla SGD 或更复杂的 Adam）以及我们在本书中看到的许多其他参数。你必须深入了解算法的机制，才能看到完整的图景。
- en: 10.5.4  Testing a trained model
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.4  测试训练好的模型
- en: 'To test a trained model, each dataset presents two files: a query set and a
    gallery set. These sets can be used to compute the evaluation metrics mentioned
    earlier: mAP and top-k accuracy. While evaluation metrics are a good summary,
    we also look at the results visually. To this end, we take random images in a
    query set and find (plot) the top-k retrievals from the gallery set. The following
    subsections show quantitative and qualitative results of using various mining
    techniques from this chapter.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试一个训练好的模型，每个数据集都包含两个文件：一个查询集和一个画廊集。这些集合可以用来计算前面提到的评估指标：mAP 和 top-k 准确率。虽然评估指标是一个很好的总结，但我们也会从视觉上查看结果。为此，我们从查询集中随机选取图像，并从画廊集中找到（绘制）top-k
    检索结果。以下小节展示了使用本章中各种挖掘技术的定量和定性结果。
- en: 'Task 1: In-shop retrieval'
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务 1：店内检索
- en: 'Let’s look at sample retrievals from the learned embeddings in figure 10.22\.
    The results look visually pleasing: the top retrievals are from the same class
    as the query. The network does reasonably well at inferring different views of
    the same query in the top ranks.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图 10.22 中的学习嵌入的样本检索。结果看起来视觉上很吸引人：top 检索来自与查询相同的类别。网络在推断排名靠前的相同查询的不同视图方面表现合理。
- en: '![](../Images/10-22.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-22.png)'
- en: Figure 10.22 Sample retrievals from the fashion dataset using various embedding
    approaches. Each row indicates the query image and top-5 retrievals for this query
    image. An *x* indicates an incorrect retrieval.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.22 使用各种嵌入方法从时尚数据集中检索的样本。每一行表示查询图像及其查询图像的 top-5 检索。一个 *x* 表示一个错误的检索。
- en: 'Table 10.2 outlines the performance of triplet loss under various sampling
    scenarios. BW outperforms all other sampling approaches. Top-1 accuracy is quite
    good in this case: we were able to retrieve the same class of fashion object in
    the very first retrieval, with accuracy over 87%. Notice that with the evaluation
    setup, the top-k accuracy for k > 1 is higher (monotonically).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 概述了在各种采样场景下 triplet 损失的性能。BW 在所有采样方法中表现最佳。在这种情况下，top-1 准确率相当好：我们能够在第一次检索中检索到相同类别的时尚物品，准确率超过
    87%。请注意，在评估设置中，k > 1 的 top-k 准确率更高（单调递增）。
- en: Table 10.2 Performance of various sampling approaches on the in-shop retrieval
    task
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 各种采样方法在店内检索任务上的性能
- en: '| Method | top-1 | top-2 | top-5 | top-10 | top-20 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | top-1 | top-2 | top-5 | top-10 | top-20 |'
- en: '| Batch all | 83.79 | 89.81 | 94.40 | 96.38 | 97.55 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 批量所有 | 83.79 | 89.81 | 94.40 | 96.38 | 97.55 |'
- en: '| Batch hard | 86.40 | 91.22 | 95.43 | 96.85 | 97.83 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 批量硬匹配 | 86.40 | 91.22 | 95.43 | 96.85 | 97.83 |'
- en: '| Batch sample | 86.62 | 91.36 | 95.36 | 96.72 | 97.84 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 批量采样 | 86.62 | 91.36 | 95.36 | 96.72 | 97.84 |'
- en: '| Batch weighted | 87.70 | 92.26 | 95.77 | 97.22 | 98.09 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 批量加权 | 87.70 | 92.26 | 95.77 | 97.22 | 98.09 |'
- en: '| Capsule embeddings  | 33.90 | - | - | 75.20 | 84.60 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 胶囊嵌入  | 33.90 | - | - | 75.20 | 84.60 |'
- en: '| ABE [18] | 87.30 | - | - | 96.70 | 97.90 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| ABE [18] | 87.30 | - | - | 96.70 | 97.90 |'
- en: '| BIER [19] | 76.90 | - | - | 92.80 | 95.20 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| BIER [19] | 76.90 | - | - | 92.80 | 95.20 |'
- en: Our results compare favorably with the state-of-the-art results. Using attention-based
    ensemble (ABE) [18], a diverse set of ensembles are trained that attend to parts
    of the image. Boosting independent embeddings robustly (BIER) [19] trains an ensemble
    of metric CNNs with a shared feature representation as an online gradient boosting
    problem. Noticeably, this ensemble framework does not introduce any additional
    parameters (and works with any differential loss).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果与最先进的结果相比表现良好。使用基于注意力的集成（ABE）[18]，训练了多种关注图像不同部分的集成。通过增强独立嵌入的鲁棒性（BIER）[19]，将具有共享特征表示的度量
    CNN 集成训练为一个在线梯度提升问题。值得注意的是，这个集成框架不引入任何额外的参数（并且与任何差异损失一起工作）。
- en: 'Task 2: Vehicle re-identification'
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务 2：车辆重识别
- en: Kumar et al. [12] recently performed an exhaustive evaluation of the sampling
    variants for optimizing triplet loss. The results are summarized in table 10.3
    with comparisons from several state-of-the-art approaches. Noticeably, the authors
    perform favorably compared to state-of-the-art approaches without using any other
    information sources, such as spatio-temporal distances and attributes. Qualitative
    results are shown in figure 10.23, demonstrating the robustness of embeddings
    with respect to the viewpoints. Notice that the retrieval has the desired property
    of being viewpoint-invariant, as different views of the same vehicle are retrieved
    into top-5 ranks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Kumar 等人 [12] 最近对优化三元组损失的采样变体进行了彻底评估。结果总结在表 10.3 中，并与几种最先进的方法进行了比较。值得注意的是，作者在没有使用任何其他信息源（如时空距离和属性）的情况下，与最先进的方法相比表现良好。定性结果如图
    10.23 所示，展示了嵌入对视角的鲁棒性。请注意，检索具有所需的视角不变性属性，因为同一辆车的不同视角被检索到 top-5 排名中。
- en: Table 10.3 Comparison of various proposed approaches on the VeRi dataset. An
    asterisk (*) indicates the usage of spatio-temporal information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.3 在 VeRi 数据集上比较各种提出的方法。星号 (*) 表示使用了时空信息。
- en: '| Method | mAP | top-1 | top-5 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | mAP | top-1 | top-5 |'
- en: '| Batch sample | 67.55 | 90.23 | 96.42 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 批量采样 | 67.55 | 90.23 | 96.42 |'
- en: '| Batch hard | 65.10 | 87.25 | 94.76 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 批量硬匹配 | 65.10 | 87.25 | 94.76 |'
- en: '| Batch all | 66.91 | 90.11 | 96.01 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 批量全部 | 66.91 | 90.11 | 96.01 |'
- en: '| Batch weighted | 67.02 | 89.99 | 96.54 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 批量加权 | 67.02 | 89.99 | 96.54 |'
- en: '| GSTE [20] | 59.47 | 96.24 | 98.97 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GSTE [20] | 59.47 | 96.24 | 98.97 |'
- en: '| VAMI [21] | 50.13 | 77.03 | 90.82 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| VAMI [21] | 50.13 | 77.03 | 90.82 |'
- en: '| VAMI+ST * [21] | 61.32 | 85.92 | 91.84 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| VAMI+ST * [21] | 61.32 | 85.92 | 91.84 |'
- en: '| Path-LSTM * [22] | 58.27 | 83.49 | 90.04 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Path-LSTM * [22] | 58.27 | 83.49 | 90.04 |'
- en: '| PAMTRI (RS) [23] | 63.76 | 90.70 | 94.40 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| PAMTRI (RS) [23] | 63.76 | 90.70 | 94.40 |'
- en: '| PAMTRI (All) [23] | 71.88 | 92.86 | 96.97 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| PAMTRI (All) [23] | 71.88 | 92.86 | 96.97 |'
- en: '| MSVR [24] | 49.30 | 88.56 | - |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| MSVR [24] | 49.30 | 88.56 | - |'
- en: '| AAVER [25] | 61.18 | 88.97 | 94.70 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| AAVER [25] | 61.18 | 88.97 | 94.70 |'
- en: '![](../Images/10-23.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-23.png)'
- en: Figure 10.23 Sample retrievals on the VeRi dataset using various embedding approaches.
    Each row indicates a query image and the top-5 retrievals for it. An *x* indicates
    an incorrect retrieval.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.23 使用各种嵌入方法在 VeRi 数据集上的样本检索。每一行表示一个查询图像及其 top-5 检索结果。星号 (*) 表示一个错误的检索。
- en: 'To gauge the pros and cons of various approaches in the literature, let’s conceptually
    examine the competing approaches in vehicle re-identification:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估文献中各种方法的优缺点，让我们从概念上考察车辆重识别中的竞争方法：
- en: Kanaci et al. [26] proposed cross-level vehicle re-identification (CLVR ) on
    the basis of using classification loss with model labels (see figure 10.24) to
    train a fine-grained vehicle categorization network. This setup is similar to
    the one we saw in section 10.3.2 and figure 10.9\. The authors did not perform
    an evaluation on the VeRi dataset. You are encouraged to refer to this paper to
    understand the performance on other vehicle re-identification datasets.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanaci 等人 [26] 基于使用模型标签的损失函数（见图 10.24）提出了基于交叉级别的车辆重识别（CLVR）。这种设置与我们第 10.3.2
    节和图 10.9 中看到的是相似的。作者没有在 VeRi 数据集上进行评估。鼓励您参考这篇论文以了解在其他车辆重识别数据集上的性能。
- en: '![](../Images/10-24.png)'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/10-24.png)'
- en: 'Figure 10.24 Cross-level vehicle re-identification (CLVR). (Source: [24].)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.24跨级车辆重识别（CLVR）。(来源：[24]。)
- en: Group-sensitive triplet embedding (GSTE ) by Bai et al. [20] is a novel training
    process that clusters intra-class variations using K-Means. This helps with more
    guided training at the expense of an additional parameter, K-Means clustering.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等人[20]提出的组敏感三元组嵌入（GSTE）是一种新颖的训练过程，它使用K-Means聚类来聚类类内变化。这以增加一个额外参数K-Means聚类为代价，帮助进行更有指导性的训练。
- en: Pose aware multi-task learning (PAMTRI ) by Zheng et al. [23] trains a network
    for embedding in a multi-task regime using keypoint annotations in conjunction
    with synthetic data (thereby tackling keypoint annotation requirements). PAMTRI
    (All) achieves the best results on this dataset. PAMTRI (RS) uses a mix of real
    and synthetic data for learning embedding, and PAMTRI (All) additionally uses
    vehicle keypoints and attributes in a multi-task learning framework.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人[23]提出的姿态感知多任务学习（PAMTRI）通过结合合成数据（从而解决关键点标注需求）和关键点标注来训练一个多任务环境下的嵌入网络。PAMTRI（All）在此数据集上取得了最佳结果。PAMTRI（RS）使用真实和合成数据的混合来学习嵌入，而PAMTRI（All）还额外在多任务学习框架中使用车辆关键点和属性。
- en: Adaptive attention for vehicle re-identification (AAVER ) by Khorramshahi et
    al. [25] is a recent work wherein the authors construct a dual-path network for
    extracting global and local features. These are then concatenated to form a final
    embedding. The proposed embedding loss is minimized using identity and keypoint
    orientation annotations.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Khorramshahi等人[25]提出的自适应注意力车辆重识别（AAVER）是一种近期的研究成果，其中作者构建了一个双路径网络以提取全局和局部特征。这些特征随后被连接起来形成一个最终的嵌入。所提出的嵌入损失是通过身份和关键点方向注释来最小化的。
- en: A training procedure for viewpoint attentive multi-view inference (VAMI ) by
    Zhou et al. [21] includes a generative adversarial network (GAN) and multi-view
    attention learning. The authors’ conjecture that being able to synthesize (generate
    using GAN) multiple viewpoint views would help learn a better final embedding.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人[21]提出了一种用于视角注意力多视角推理（VAMI）的训练过程，包括生成对抗网络（GAN）和多视角注意力学习。作者推测，能够合成（使用GAN生成）多个视角视图将有助于学习更好的最终嵌入。
- en: With Path-LSTM, Shen et al. [22] employ a generation of several path proposals
    for their spatio-temporal regularization and require an additional LSTM to rank
    these proposals.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Path-LSTM，Shen等人[22]为他们的时空正则化生成多个路径建议，并需要一个额外的LSTM来对这些建议进行排序。
- en: Kanaci et al. [24] proposed multi-scale vehicle representation (MSVR ) for re-identification
    by exploiting a pyramid-based DL method. MSVR learns vehicle re-identification
    sensitive feature representations from an image pyramid with a network architecture
    of multiple branches, all of which are optimized concurrently.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanaci等人[24]提出了基于金字塔的深度学习方法的多尺度车辆表示（MSVR）用于重识别。MSVR通过具有多个分支的网络架构从图像金字塔中学习车辆重识别敏感的特征表示，所有这些分支都是同时优化的。
- en: A snapshot summary of these approaches with respect to the key hyperparameters
    is summarized in table 10.4.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法关于关键超参数的快照总结见表10.4。
- en: Table 10.4 Summary of some important hyperparameters and labeling used during
    training
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.4总结了训练过程中使用的一些重要超参数和标签。
- en: '| Method | ED | Annotations |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ED | 标注 |'
- en: '| Ours | 0128 | ID |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0128 | ID |'
- en: '| GSTE [20] | 1024 | ID |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| GSTE [20] | 1024 | ID |'
- en: '| VAMI  [21] | 2048 | ID + A |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| VAMI [21] | 2048 | ID + A |'
- en: '| PAMTRI (All)  [23] | 1024 | ID + K + A |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PAMTRI (All) [23] | 1024 | ID + K + A |'
- en: '| MSVR [24] | 2048 | ID |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| MSVR [24] | 2048 | ID |'
- en: '| AAVER  [25] | 2048 | ID + K |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| AAVER [25] | 2048 | ID + K |'
- en: '| Note: ED = embedding dimension; K = keypoints; A = attributes. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 备注：ED = 嵌入维度；K = 关键点；A = 属性。|'
- en: Usually, license plates are a global unique identifier. However, with the standard
    installation of traffic cameras, license plates are difficult to extract; hence,
    visual-based features are required for vehicle re-identification. If two cars
    are of the same make, model, and color, then visual features cannot disambiguate
    them (unless there are some distinctive marks such as text or scratches). In these
    tough scenarios, only spatio-temporal information  (like GPS information) can
    help. To learn more, you are encouraged to look into recent proposed datasets
    by Tang et al. [27].
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，车牌是一个全局唯一标识符。然而，由于标准交通摄像机的安装，车牌难以提取；因此，车辆再识别需要基于视觉的特征。如果两辆车是同一品牌、型号和颜色，那么视觉特征无法区分它们（除非有一些独特的标记，如文字或划痕）。在这些困难场景中，只有时空信息（如GPS信息）才能有所帮助。要了解更多信息，鼓励大家查阅Tang等人[27]最近提出的提议数据集。
- en: 10.6  Pushing the boundaries of current accuracy
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.6 推动当前准确性的边界
- en: 'Deep learning is an evolving field, and novel approaches to training are being
    introduced every day. This section provides ideas for improving the current level
    of embeddings and some recently introduced tips and tricks to train a deep CNN:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个不断发展的领域，每天都有新的训练方法被引入。本节提供了提高当前嵌入水平的一些想法，以及一些最近提出的用于训练深度CNN的技巧和窍门：
- en: Re-ranking --After obtaining an initial ranking of gallery images (to an input
    query image), re-ranking uses a post-processing step with the aim of improving
    the ranking of relevant images. This is a powerful, widely used step in many re-identification
    and information-retrieval systems.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序 -- 在获得画廊图像的初始排名（针对输入查询图像）后，重新排序使用后处理步骤，目的是提高相关图像的排名。这是许多再识别和信息检索系统中广泛使用的一个强大步骤。
- en: A popular approach in re-identification is by Zhong et al. [28] (see figure
    10.25). Given a probe p and a gallery set, the appearance feature (embedding)
    and k-reciprocal feature are extracted for each person. The original distance
    d and Jaccard distance Jd are calculated for each pair of a probe person and a
    gallery person. The final distance is then computed as the combination of d and
    Jd and used to obtain the proposed ranking list.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再识别中一个流行的方法是Zhong等人[28]提出的（见图10.25）。给定一个探测图像p和一个画廊集，为每个人提取外观特征（嵌入）和k-互反特征。计算每个探测图像和画廊图像对的原始距离d和Jaccard距离Jd。最终距离是d和Jd的组合，并用于获得提出的排名列表。
- en: '![](../Images/10-25.png)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/10-25.png)'
- en: 'Figure 10.25 Re-ranking proposal by Zhong et al. (Source: [28].)'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.25 Zhong等人提出的重新排序建议（来源：[28]）
- en: A recent work in vehicle re-identification, AAVER [25] boosts mAP accuracy by
    5% by post-processing using re-ranking.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近在车辆再识别领域的一项工作，AAVER [25] 通过后处理使用重新排序提升了5%的mAP准确率。
- en: DEFINITION The Jaccard distance is computed among two sets of data and expresses
    the intersection over the union of the two sets.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义 Jaccard距离是在两组数据之间计算的，表示两个集合的交集与并集的比例。
- en: Tips and tricks --Luo et al. [29] demonstrated powerful baseline performance
    on the task of person re-identification. The authors follow the same batch construction
    from Hermans et al. [11] (studied in this chapter) and use tricks for data augmentation,
    warm-up learning rate, and label smoothing, to name a few. Noticeably, the authors
    perform favorably compared to many state-of-the-art methods. You are encouraged
    to apply these general tricks for training a CNN for any recognition-related tasks.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技巧和窍门 -- Luo等人[29]在人员再识别任务上展示了强大的基线性能。作者遵循了Hermans等人[11]（本章研究）相同的批量构建方法，并使用了数据增强、预热学习率和标签平滑等技巧，仅举几例。值得注意的是，作者的表现优于许多最先进的方法。鼓励大家将这些通用技巧应用于训练任何与识别相关的CNN任务。
- en: DEFINITIONS The warm-up learning rate refers to a strategy with a learning rate
    scheduler that modulates the learning rate linearly with respect to a predefined
    number of initial training epochs. Label smoothing modulates the cross-entropy
    loss so the resulting loss is less overconfident on the training set, thereby
    helping with model generalization and preventing overfitting. This is particularly
    useful in small-scale datasets.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义 预热学习率是指一个学习率调度策略，该策略将学习率线性地与预定义的初始训练epoch数相关联。标签平滑调整交叉熵损失，使得结果损失对训练集的置信度较低，从而有助于模型泛化并防止过拟合。这在小规模数据集中特别有用。
- en: 'Attention --In this chapter, we focused on learning embedding in a global fashion:
    that is, we did not explicitly guide the network to attend to, for example, discriminative
    parts of an object. Some of the prominent works employing attention are Liu et
    al. [30] and Chen et al. [31]. Employing attention could also help improve the
    cross-domain performance of a re-identification network, as demonstrated in [32].'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意 --在本章中，我们专注于以全局方式学习嵌入：也就是说，我们没有明确引导网络关注，例如，对象的判别部分。一些采用注意力的突出工作包括 Liu 等人
    [30] 和 Chen 等人 [31]。采用注意力还可以帮助提高重识别网络的跨域性能，如 [32] 中所示。
- en: 'Guiding training with more information --The state-of-the-art comparisons in
    table 10.3 briefly touched on works incorporating information from multiple sources:
    identity, attributes (such as the make and model of a vehicle), and spatio-temporal
    information (GPS location of each query and gallery image). Ideally, including
    more information helps obtain higher accuracy. However, this comes at the expense
    of labeling data with annotations. A reasonable approach for training with a multi-attribute
    setup is to use multi-task learning (MTL). Often, the loss becomes conflicting;
    this is resolved by weighting the tasks appropriately (using cross validation).
    A MTL framework to resolve this conflicting loss scenario using multi-objective
    optimization is by Sener al. [32].'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多信息指导训练 --表 10.3 中的最先进比较简要地提到了结合来自多个来源的信息的工作：身份、属性（例如车辆的制造商和型号）以及时空信息（每个查询图像和图库图像的
    GPS 位置）。理想情况下，包括更多信息有助于获得更高的准确性。然而，这需要为数据添加标注。在多属性设置中进行训练的合理方法是使用多任务学习（MTL）。通常，损失会变得冲突；这通过适当的任务权重（使用交叉验证）来解决。通过多目标优化解决这种冲突损失场景的多任务学习框架是由
    Sener 等人 [32] 提出的。
- en: Some popular works of MTL in the context of face, person, and vehicle categorization
    are by Ranjan et al. [34], Ling et al. [35], and Tang [23].
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在人脸、人员和车辆分类的上下文中，MTL的一些流行作品由 Ranjan 等人 [34]、Ling 等人 [35] 和 Tang [23] 完成。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Image-retrieval systems require the learning of visual embeddings (a vector
    space). Any pair of images can be compared using their geometric distance in this
    embedding space.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像检索系统需要学习视觉嵌入（一个向量空间）。在这个嵌入空间中，任何一对图像都可以通过它们的几何距离进行比较。
- en: 'To learn embeddings using a CNN, there are three popular loss functions: cross-entropy,
    triplet, and contrastive.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用 CNN 学习嵌入，有三种流行的损失函数：交叉熵、三重损失和对比损失。
- en: 'Naive training of triplet loss is computationally prohibitive. Hence we use
    batch-based informative data minings: batch all, batch hard, batch sample, and
    batch weighted.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三重损失的三种简单训练方法在计算上难以承受。因此，我们使用基于批次的基于信息的数据挖掘：全部批次、硬批次、样本批次和加权批次。
- en: References
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: S.Z. Li and A.K. Jain. 2011\. Handbook of Face Recognition. Springer Science
    & Business Media. [https://www.springer.com/gp/book/9780857299314](https://www.springer.com/gp/book/9780857299314).
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S.Z. Li 和 A.K. Jain. 2011\. 《人脸识别手册》. Springer Science & Business Media. [https://www.springer.com/gp/book/9780857299314](https://www.springer.com/gp/book/9780857299314).
- en: 'V. Gupta and S. Mallick. 2019\. “Face Recognition: An Introduction for Beginners.”
    Learn OpenCV. April 16, 2019\. [https://www.learnopencv.com/face-recognition-an-introduction-for-beginners](https://www.learnopencv.com/face-recognition-an-introduction-for-beginners).'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: V. Gupta 和 S. Mallick. 2019\. “面向初学者的面部识别介绍。” Learn OpenCV. 2019年4月16日。 [https://www.learnopencv.com/face-recognition-an-introduction-for-beginners](https://www.learnopencv.com/face-recognition-an-introduction-for-beginners).
- en: 'Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. 2016\. “Deepfashion: Powering
    robust clothes recognition and retrieval with rich annotations.” IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). [http://mmlab.ie .cuhk.edu.hk/projects/DeepFashion.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html).'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Z. Liu, P. Luo, S. Qiu, X. Wang, 和 X. Tang. 2016\. “Deepfashion: 利用丰富注释实现鲁棒的服装识别和检索。”
    IEEE 计算机视觉与模式识别会议 (CVPR)。 [http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html).'
- en: T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. 2016\. “Joint Detection and Identification
    Feature Learning for Person Search.” [http://arxiv.org/abs/1604.01850](http://arxiv.org/abs/1604.01850).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Xiao, S. Li, B. Wang, L. Lin, 和 X. Wang. 2016\. “人员搜索的联合检测和识别特征学习。” [http://arxiv.org/abs/1604.01850](http://arxiv.org/abs/1604.01850).
- en: Y. Zhai, X. Guo, Y. Lu, and H. Li. 2018\. “In Defense of the Classification
    Loss for Person Re-Identification.” [http://arxiv.org/abs/1809.05864](http://arxiv.org/abs/1809.05864).
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Zhai, X. Guo, Y. Lu, 和 H. Li. 2018\. “为行人重识别的分类损失辩护。” [http://arxiv.org/abs/1809.05864](http://arxiv.org/abs/1809.05864).
- en: 'S. Chopra, R. Hadsell, and Y. LeCun. 2005\. “Learning a Similarity Metric Discriminatively,
    with Application to Face Verification.” In 2005 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’05 ), 1: 539-46 vol. 1\. [https://doi.org/10.1109/CVPR.2005.202](https://doi.org/10.1109/CVPR.2005.202).'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'S. Chopra, R. Hadsell, and Y. LeCun. 2005\. “Learning a Similarity Metric Discriminatively,
    with Application to Face Verification.” In 2005 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’05), 1: 539-46 vol. 1\. [https://doi.org/10.1109/CVPR.2005.202](https://doi.org/10.1109/CVPR.2005.202).'
- en: C-Y. Wu, R. Manmatha, A.J. Smola, and P. Krähenbühl. 2017\. “Sampling Matters
    in Deep Embedding Learning.” [http://arxiv.org/abs/1706.07567](http://arxiv.org/abs/1706.07567).
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C-Y. Wu, R. Manmatha, A.J. Smola, and P. Krähenbühl. 2017\. “Sampling Matters
    in Deep Embedding Learning.” [http://arxiv.org/abs/1706.07567](http://arxiv.org/abs/1706.07567).
- en: 'Q. Weinberger and L.K. Saul. 2009\. “Distance Metric Learning for Large Margin
    Nearest Neighbor Classification.” The Journal of Machine Learning Research 10:
    207-244. [https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf.](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Q. Weinberger and L.K. Saul. 2009\. “Distance Metric Learning for Large Margin
    Nearest Neighbor Classification.” The Journal of Machine Learning Research 10:
    207-244. [https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf).'
- en: 'F. Schroff, D. Kalenichenko, and J. Philbin. 2015\. “FaceNet: A Unified Embedding
    for Face Recognition and Clustering.” In 2015 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR ), 815-23\. [https://ieeexplore.ieee.org/ document/7298682](https://ieeexplore.ieee.org/document/7298682).'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'F. Schroff, D. Kalenichenko, and J. Philbin. 2015\. “FaceNet: A Unified Embedding
    for Face Recognition and Clustering.” In 2015 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), 815-23\. [https://ieeexplore.ieee.org/document/7298682](https://ieeexplore.ieee.org/document/7298682).'
- en: 'R. Kumar, E. Weill, F. Aghdasi, and P. Sriram. 2019\. “Vehicle Re-Identification:
    An Efficient Baseline Using Triplet Embedding.” [https://arxiv.org/pdf/1901 .01015.pdf](https://arxiv.org/pdf/1901.01015.pdf).'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R. Kumar, E. Weill, F. Aghdasi, and P. Sriram. 2019\. “Vehicle Re-Identification:
    An Efficient Baseline Using Triplet Embedding.” [https://arxiv.org/pdf/1901.01015.pdf](https://arxiv.org/pdf/1901.01015.pdf).'
- en: A. Hermans, L. Beyer, and B. Leibe. 2017\. “In Defense of the Triplet Loss for
    Person Re-Identification.” [http://arxiv.org/abs/1703.07737](http://arxiv.org/abs/1703.07737).
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Hermans, L. Beyer, and B. Leibe. 2017\. “In Defense of the Triplet Loss for
    Person Re-Identification.” [http://arxiv.org/abs/1703.07737](http://arxiv.org/abs/1703.07737).
- en: 'R. Kumar, E. Weill, F. Aghdasi, and P. Sriram. 2020\. “A Strong and Efficient
    Baseline for Vehicle Re-Identification Using Deep Triplet Embedding.” Journal
    of Artificial Intelligence and Soft Computing Research 10 (1): 27-45\. [https://content
    .sciendo.com/view/journals/jaiscr/10/1/article-p27.xml](https://content.sciendo.com/view/journals/jaiscr/10/1/article-p27.xml).'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R. Kumar, E. Weill, F. Aghdasi, and P. Sriram. 2020\. “A Strong and Efficient
    Baseline for Vehicle Re-Identification Using Deep Triplet Embedding.” Journal
    of Artificial Intelligence and Soft Computing Research 10 (1): 27-45\. [https://content.sciendo.com/view/journals/jaiscr/10/1/article-p27.xml](https://content.sciendo.com/view/journals/jaiscr/10/1/article-p27.xml).'
- en: E. Ristani and C. Tomasi. 2018\. “Features for Multi-Target Multi-Camera Tracking
    and Re-Identification.” [http://arxiv.org/abs/1803.10859](http://arxiv.org/abs/1803.10859).
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E. Ristani and C. Tomasi. 2018\. “Features for Multi-Target Multi-Camera Tracking
    and Re-Identification.” [http://arxiv.org/abs/1803.10859](http://arxiv.org/abs/1803.10859).
- en: 'X. Liu, W. Liu, T. Mei, and H. Ma. 2018\. “PROVID: Progressive and Multimodal
    Vehicle Reidentification for Large-Scale Urban Surveillance.” IEEE Transactions
    on Multimedia 20 (3): 645-58\. [https://doi.org/10.1109/TMM.2017.2751966](https://doi.org/10.1109/TMM.2017.2751966).'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Liu, W. Liu, T. Mei, and H. Ma. 2018\. “PROVID: Progressive and Multimodal
    Vehicle Reidentification for Large-Scale Urban Surveillance.” IEEE Transactions
    on Multimedia 20 (3): 645-58\. [https://doi.org/10.1109/TMM.2017.2751966](https://doi.org/10.1109/TMM.2017.2751966).'
- en: 'J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009\. “ImageNet:
    A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer
    Vision and Pattern Recognition, 248-55\. [http://ieeexplore.ieee.org/lpdocs/epic03/
    wrapper.htm?arnumber=5206848](http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848).'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009\. “ImageNet:
    A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer
    Vision and Pattern Recognition, 248-55\. [http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848](http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848).'
- en: 'A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam. 2017\. “MobileNets: Efficient Convolutional Neural Networks for Mobile
    Vision Applications.” [http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861).'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam. 2017\. “MobileNets: Efficient Convolutional Neural Networks for Mobile
    Vision Applications.” [http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861).'
- en: 'D.P. Kingma and J. Ba. 2014\. “Adam: A Method for Stochastic Optimization.”
    [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'D.P. Kingma 和 J. Ba. 2014\. “Adam: 随机优化的方法。” [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).'
- en: W. Kim, B. Goyal, K. Chawla, J. Lee, and K. Kwon. 2018\. “Attention-based ensemble
    for deep metric learning.” In 2018 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR ), 760-777, [https://arxiv.org/abs/1804.00382](https://arxiv.org/abs/1804.00382).
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: W. Kim, B. Goyal, K. Chawla, J. Lee, 和 K. Kwon. 2018\. “基于注意力的深度度量学习集成。” 在 2018
    IEEE 计算机视觉与模式识别会议 (CVPR), 760-777, [https://arxiv.org/abs/1804.00382](https://arxiv.org/abs/1804.00382).
- en: M. Opitz, G. Waltner, H. Possegger, and H. Bischof. 2017\. “BIER--Boosting Independent
    Embeddings Robustly.” In 2017 IEEE International Conference on Computer Vision
    (ICCV ), 5199-5208\. [https://ieeexplore.ieee.org/document/8237817](https://ieeexplore.ieee.org/document/8237817).
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Opitz, G. Waltner, H. Possegger, 和 H. Bischof. 2017\. “BIER--稳健提升独立嵌入。” 在
    2017 IEEE 国际计算机视觉会议 (ICCV), 5199-5208\. [https://ieeexplore.ieee.org/document/8237817](https://ieeexplore.ieee.org/document/8237817).
- en: 'Y. Bai, Y. Lou, F. Gao, S. Wang, Y. Wu, and L. Duan. 2018\. “Group-Sensitive
    Triplet Embedding for Vehicle Reidentification.” IEEE Transactions on Multimedia
    20 (9): 2385-99\. [https://ieeexplore.ieee.org/document/8265213](https://ieeexplore.ieee.org/document/8265213).'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Y. Bai, Y. Lou, F. Gao, S. Wang, Y. Wu, 和 L. Duan. 2018\. “针对车辆重识别的组敏感三元组嵌入。”
    IEEE 传输多媒体 20 (9): 2385-99\. [https://ieeexplore.ieee.org/document/8265213](https://ieeexplore.ieee.org/document/8265213).'
- en: Y. Zhouy and L. Shao. 2018\. “Viewpoint-Aware Attentive Multi-View Inference
    for Vehicle Re-Identification.” In 2018 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 6489-98\. [https://ieeexplore.ieee.org/document/8578777](https://ieeexplore.ieee.org/document/8578777).
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Zhouy 和 L. Shao. 2018\. “针对车辆重识别的视角感知多视图推理。” 在 2018 IEEE/CVF 计算机视觉与模式识别会议，6489-98\.
    [https://ieeexplore.ieee.org/document/8578777](https://ieeexplore.ieee.org/document/8578777).
- en: Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang. 2017\. “Learning Deep Neural Networks
    for Vehicle Re-ID with Visual-Spatio-Temporal Path Proposals.” In 2017 IEEE International
    Conference on Computer Vision (ICCV ), 1918-27\. [https://ieeexplore .ieee.org/document/8237472](https://ieeexplore.ieee.org/document/8237472).
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Shen, T. Xiao, H. Li, S. Yi, 和 X. Wang. 2017\. “使用视觉时空路径提议学习车辆重识别的深度神经网络。”
    在 2017 IEEE 国际计算机视觉会议 (ICCV), 1918-27\. [https://ieeexplore.ieee.org/document/8237472](https://ieeexplore.ieee.org/document/8237472).
- en: 'Z. Tang, M. Naphade, S. Birchfield, J. Tremblay, W. Hodge, R. Kumar, S. Wang,
    and X. Yang. 2019\. “PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification
    Using Highly Randomized Synthetic Data.” In Proceedings of the IEEE International
    Conference on Computer Vision, 211-20\. [http://openaccess.thecvf .com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning
    _for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper .html](http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.html).'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Z. Tang, M. Naphade, S. Birchfield, J. Tremblay, W. Hodge, R. Kumar, S. Wang,
    和 X. Yang. 2019\. “PAMTRI: 基于姿态的多任务学习，用于使用高度随机化合成数据的车辆重识别。” 在 IEEE 国际计算机视觉会议论文集，211-20\.
    [http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.html](http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.html).'
- en: A. Kanacı, X. Zhu, and S. Gong. 2017\. “Vehicle Reidentification by Fine-Grained
    Cross-Level Deep Learning.” In BMVC AMMDS Workshop, 2:772-88\. [https://arxiv.org/abs/1809.09409](https://arxiv.org/abs/1809.09409).
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Kanacı, X. Zhu, 和 S. Gong. 2017\. “通过细粒度跨层深度学习进行车辆重识别。” 在 BMVC AMMDS Workshop,
    2:772-88\. [https://arxiv.org/abs/1809.09409](https://arxiv.org/abs/1809.09409).
- en: P. Khorramshahi, A. Kumar, N. Peri, S.S. Rambhatla, J.-C. Chen, and R. Chellappa.
    2019\. “A Dual-Path Model With Adaptive Attention For Vehicle Re-Identification.”
    [http://arxiv.org/abs/1905.03397](http://arxiv.org/abs/1905.03397).
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Khorramshahi, A. Kumar, N. Peri, S.S. Rambhatla, J.-C. Chen, 和 R. Chellappa.
    2019\. “具有自适应注意力的双路径模型，用于车辆重识别。” [http://arxiv.org/abs/1905.03397](http://arxiv.org/abs/1905.03397).
- en: A. Kanacı, X. Zhu, and S. Gong. 2017\. “Vehicle Reidentification by Fine-Grained
    Cross-Level Deep Learning.” In BMVC AMMDS Workshop, 2:772-88\. [http://www .eecs.qmul.ac.uk/~xiatian/papers](http://www.eecs.qmul.ac.uk/~xiatian/papers).
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Kanacı, X. Zhu, 和 S. Gong. 2017\. “通过细粒度跨层深度学习进行车辆重识别。” 在 BMVC AMMDS Workshop,
    2:772-88\. [http://www.eecs.qmul.ac.uk/~xiatian/papers](http://www.eecs.qmul.ac.uk/~xiatian/papers).
- en: 'Z. Tang, M. Naphade, M.-Y. Liu, X. Yang, S. Birchfield, S. Wang, R. Kumar,
    D. Anastasiu, and J.-N. Hwang. 2019\. “CityFlow: A City-Scale Benchmark for Multi-Target
    Multi-Camera Vehicle Tracking and Re-Identification.” In 2019 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR ). [http://arxiv.org/ abs/1903.09254](http://arxiv.org/abs/1903.09254).'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Z. Tang, M. Naphade, M.-Y. Liu, X. Yang, S. Birchfield, S. Wang, R. Kumar, D.
    Anastasiu, and J.-N. Hwang. 2019\. “CityFlow：用于多目标多摄像头车辆跟踪和重识别的城市规模基准.” 在2019年IEEE计算机视觉和模式识别会议（CVPR）中.
    [http://arxiv.org/abs/1903.09254](http://arxiv.org/abs/1903.09254).
- en: Z. Zhong, L. Zheng, D. Cao, and S. Li. 2017\. “Re-Ranking Person Re-Identification
    with K-Reciprocal Encoding.” In 2017 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR ), 3652-3661, [https://arxiv.org/abs/1701.08398](https://arxiv.org/abs/1701.08398).
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Z. Zhong, L. Zheng, D. Cao, and S. Li. 2017\. “使用K-互反编码对人员重识别进行重新排序.” 在2017年IEEE计算机视觉和模式识别会议（CVPR）中,
    3652-3661, [https://arxiv.org/abs/1701.08398](https://arxiv.org/abs/1701.08398).
- en: H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang. 2019\. “Bag of Tricks and A Strong
    Baseline for Deep Person Re-Identification.” In 2019 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR ) Workshops. [https://arxiv.org/abs/ 1903.07071](https://arxiv.org/abs/1903.07071).
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang. 2019\. “深度人员重识别的技巧集合和强大基线.” 在2019年IEEE计算机视觉和模式识别会议（CVPR）研讨会中.
    [https://arxiv.org/abs/1903.07071](https://arxiv.org/abs/1903.07071).
- en: 'H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan. 2016\. “End-to-End Comparative
    Attention Networks for Person Re-Identification.” IEEE Transactions on Image Processing
    26 (7): 3492-3506\. [https://arxiv.org/abs/1606.04404](https://arxiv.org/abs/1606.04404).'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan. 2016\. “用于人员重识别的端到端比较注意力网络.”
    IEEE图像处理杂志 26 (7): 3492-3506\. [https://arxiv.org/abs/1606.04404](https://arxiv.org/abs/1606.04404).'
- en: G. Chen, C. Lin, L. Ren, J. Lu, and J. Zhou. 2019\. “Self-Critical Attention
    Learning for Person Re-Identification.” In Proceedings of the IEEE International
    Conference on Computer Vision, 9637-46\. [http://openaccess.thecvf.com/content_ICCV_
    2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification _ICCV_2019_paper.html](http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html).
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Chen, C. Lin, L. Ren, J. Lu, and J. Zhou. 2019\. “用于人员重识别的自我批判注意力学习.” 在IEEE国际计算机视觉会议论文集中,
    9637-46\. [http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html](http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html).
- en: 'H. Liu, J. Cheng, S. Wang, and W. Wang. 2019\. “Attention: A Big Surprise for
    Cross-Domain Person Re-Identification.” [http://arxiv.org/abs/1905.12830](http://arxiv.org/abs/1905.12830).'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Liu, J. Cheng, S. Wang, and W. Wang. 2019\. “注意力：跨域人员重识别的一个大惊喜.” [http://arxiv.org/abs/1905.12830](http://arxiv.org/abs/1905.12830).
- en: O. Sener and V. Koltun. 2018\. “Multi-Task Learning as Multi-Objective Optimization.”
    In Proceedings of the 32nd International Conference on Neural Information Processing
    Systems, 525-36\. [http://dl.acm.org/citation.cfm?id=3326943.3326992](http://dl.acm.org/citation.cfm?id=3326943.3326992).
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O. Sener and V. Koltun. 2018\. “多任务学习作为多目标优化.” 在第32届国际神经网络信息处理系统会议论文集中, 525-36\.
    [http://dl.acm.org/citation.cfm?id=3326943.3326992](http://dl.acm.org/citation.cfm?id=3326943.3326992).
- en: R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chellappa. 2017\. “An
    All-In-One Convolutional Neural Network for Face Analysis.” In 2017 12th IEEE
    International Conference on Automatic Face Gesture Recognition (FG 2017 ), 17-24\.
    [https://arxiv.org/abs/1611.00851](https://arxiv.org/abs/1611.00851).
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chellappa. 2017\. “用于面部分析的全能卷积神经网络.”
    在2017年第12届IEEE自动面部和手势识别会议（FG 2017）中, 17-24\. [https://arxiv.org/abs/1611.00851](https://arxiv.org/abs/1611.00851).
- en: 'H. Ling, Z. Wang, P. Li, Y. Shi, J. Chen, and F. Zou. 2019\. “Improving Person
    Re-Identification by Multi-Task Learning.” Neurocomputing 347: 109-118\. [https://
    doi.org/10.1016/j.neucom.2019.01.027](https://doi.org/10.1016/j.neucom.2019.01.027).'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Ling, Z. Wang, P. Li, Y. Shi, J. Chen, and F. Zou. 2019\. “通过多任务学习提高人员重识别.”
    Neurocomputing 347: 109-118\. [https://doi.org/10.1016/j.neucom.2019.01.027](https://doi.org/10.1016/j.neucom.2019.01.027).'
- en: X. Liu, W. Liu, T. Mei, and H. Ma. 2016\. “A Deep Learning-Based Approach to
    Progressive Vehicle Re-Identification for Urban Surveillance.” In Computer Vision
    - ECCV 2016, 869-84\. [https://doi.org/10.1007/978-3-319-46475-6_53](https://doi.org/10.1007/978-3-319-46475-6_53).
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X. Liu, W. Liu, T. Mei, and H. Ma. 2016\. “基于深度学习的城市监控车辆渐进式重识别方法.” 在计算机视觉 -
    ECCV 2016, 869-84\. [https://doi.org/10.1007/978-3-319-46475-6_53](https://doi.org/10.1007/978-3-319-46475-6_53).
- en: 'B. Amos, B. Ludwiczuk, M. Satyanarayanan, et al. 2016\. “Openface: A General-Purpose
    Face Recognition Library with Mobile Applications.” CMU School of Computer Science
    6: 2\. [http://elijah.cs.cmu.edu/DOCS/CMU-CS-16-118.pdf](http://elijah.cs.cmu.edu/DOCS/CMU-CS-16-118.pdf).'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'B. Amos, B. Ludwiczuk, M. Satyanarayanan, 等人. 2016\. “Openface: 一个具有移动应用的通用人脸识别库.”
    卡内基梅隆大学计算机科学学院 6: 2\. [http://elijah.cs.cmu.edu/DOCS/CMU-CS-16-118.pdf](http://elijah.cs.cmu.edu/DOCS/CMU-CS-16-118.pdf).'
- en: '* * *'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.In practice, this step gets unwound into two `for` loops due to host memory
    limitations.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 在实践中，由于主机内存限制，这一步被分解为两个 `for` 循环。
- en: 2.Categorically. For an example in Tensorflow, see [http://mng.bz/zjvQ](http://mng.bz/zjvQ).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 坚定地。例如，在Tensorflow中，请参阅[http://mng.bz/zjvQ](http://mng.bz/zjvQ).
