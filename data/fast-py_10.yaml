- en: 7 High-performance pandas and Apache Arrow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 高性能的 pandas 和 Apache Arrow
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Optimizing memory usage with pandas’ data frame creation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 的数据框创建优化内存使用
- en: Decreasing computational cost of pandas operations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低 pandas 操作的计算成本
- en: Using Cython, NumExpr, and Numpy to accelerate pandas operations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Cython、NumExpr 和 Numpy 加速 pandas 操作
- en: Optimizing pandas with Apache Arrow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Arrow 优化 pandas
- en: 'Data analytics is essentially synonymous with using pandas. pandas is a data
    frame library, or a library to process tabular data. pandas is the de facto standard
    in the Python world to process in-memory tabular data. In this chapter, we will
    discuss approaches to optimize pandas usage. This will be a two-pronged approach:
    we will optimize pandas usage directly, and we will also optimize it using Apache
    Arrow.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析本质上等同于使用 pandas。pandas 是一个数据框库，或者是一个处理表格数据的库。pandas 是 Python 世界中处理内存中表格数据的既定标准。在本章中，我们将讨论优化
    pandas 使用的途径。这将是一个双管齐下的方法：我们将直接优化 pandas 的使用，并且我们还将使用 Apache Arrow 来优化它。
- en: Apache Arrow provides language-agnostic functionality to efficiently access
    columnar data, to share these data across different language implementations,
    and to transfer data to different processes and even to different computers. It
    can complement pandas from a performance perspective by introducing faster algorithms
    to perform basic operations, such as reading CSV files, translating pandas data
    frames to the format of lower-level languages for faster processing, and enhancing
    serialization mechanisms to transfer data frames across different computers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow 提供了语言无关的功能，以有效地访问列式数据，以便在不同语言实现之间共享这些数据，并将数据传输到不同的进程甚至不同的计算机。它可以通过引入更快的算法来执行基本操作，如读取
    CSV 文件，将 pandas 数据框转换为低级语言的格式以进行更快的处理，以及增强序列化机制以在不同计算机之间传输数据框。
- en: We will start by considering some techniques to optimize pandas usage. Here,
    we’ll divide our attention between time and memory. Given that pandas is an in-memory
    library, we want to make sure we have the smallest memory footprint possible,
    which allows us to not only do complex analytics over data but also load as much
    data as possible before we need to consider on-disk implementations (we’ll talk
    more about disk implementation in chapter 10).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先考虑一些优化 pandas 使用的技术。在这里，我们将注意力分配在时间和内存之间。鉴于 pandas 是一个内存库，我们希望确保我们拥有尽可能小的内存占用，这不仅允许我们进行复杂的数据分析，而且在我们需要考虑磁盘实现之前，可以加载尽可能多的数据（我们将在第
    10 章中更多关于磁盘实现的内容）。
- en: Next, we will use previously studied libraries—NumPy, Cython and NumExpr—to
    optimize the processing of pandas data frames. As pandas is based on NumPy, it
    is actually quite easy to use Cython and NumExpr to optimize pandas.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用之前学习过的库——NumPy、Cython 和 NumExpr 来优化 pandas 数据框的处理。由于 pandas 基于 NumPy，实际上使用
    Cython 和 NumExpr 来优化 pandas 是相当容易的。
- en: Then we will get to know Apache Arrow and discuss it from two points of view.
    First, we’ll see how Apache Arrow provides alternative implementations of standard
    algorithms to pandas. For example, is there any performance advantage to reading
    a large CSV file in Arrow and converting it to pandas as opposed to reading it
    directly in pandas? Second, we will use Arrow to efficiently transfer data frames
    to other, lower-level programming languages so we can speed up the processing
    by using more efficient implementations of algorithms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将了解 Apache Arrow，并从两个角度进行讨论。首先，我们将看到 Apache Arrow 如何为 pandas 提供标准算法的替代实现。例如，在
    Arrow 中读取大型 CSV 文件并将其转换为 pandas，与直接在 pandas 中读取相比，是否有性能优势？其次，我们将使用 Arrow 高效地将数据框传输到其他低级编程语言，这样我们就可以通过使用更高效的算法实现来加速处理。
- en: 'Let’s start with optimizing data loading with standard pandas. If you are using
    conda, you should install PyArrow: at the moment I am writing this book, `pip
    install pyarrow` seems to be the most functional solution. If you use Docker,
    please use the image `tiagoantao/python-performance-dask`.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用标准 pandas 优化数据加载开始。如果你使用 conda，你应该安装 PyArrow：在我写这本书的时候，`pip install pyarrow`
    似乎是最有效的解决方案。如果你使用 Docker，请使用镜像 `tiagoantao/python-performance-dask`。
- en: 7.1 Optimizing memory and time when loading data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 加载数据时的内存和时间优化
- en: Our first task is to optimize memory usage and the speed of pandas data frame
    loading. In the next section, we will optimize data analysis operations. For our
    example, we will use the records of trips by the famous yellow cabs of New York
    City. The New York City Taxi and Limousine Commission (TLC) makes available a
    public dataset of trips at [http://mng.bz/516D](http://mng.bz/516D). We’ll use
    the Yellow Car data for January 2020\. We have information available for each
    cab ride, including start and end times, the number of passengers, fare amount,
    tip, and more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是优化内存使用和pandas数据框的加载速度。在下一节中，我们将优化数据分析操作。在我们的例子中，我们将使用纽约市著名的黄色出租车行程记录。纽约市出租车和豪华轿车委员会（TLC）在[http://mng.bz/516D](http://mng.bz/516D)提供了一个公开的行程数据集。我们将使用2020年1月的黄色出租车数据。我们提供了每趟出租车行程的信息，包括开始和结束时间、乘客数量、车费金额、小费等。
- en: 'We will start by downloading the data once locally. While pandas can download
    directly from a remote source, we do not want to wait every time we load the data
    frame from the network, as this would cost a lot of time; nor do we want to consistently
    access the data server. Our objective will be twofold: determine how much memory
    pandas requires to load the whole table and different columns and reduce memory
    usage. You can download the 566 MB of data by using wget ([https://tiago.org/yellow_tripdata_
    2020-01.csv.gz](https://tiago.org/yellow_tripdata_2020-01.csv.gz)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在本地上下载数据。虽然pandas可以直接从远程源下载，但我们不希望每次从网络上加载数据框时都等待，因为这会花费很多时间；我们也不想持续访问数据服务器。我们的目标将是双重的：确定pandas加载整个表和不同列所需的内存量，并减少内存使用。您可以使用wget下载566
    MB的数据（[https://tiago.org/yellow_tripdata_2020-01.csv.gz](https://tiago.org/yellow_tripdata_2020-01.csv.gz)）。
- en: 7.1.1 Compressed vs. uncompressed data
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 压缩数据与未压缩数据对比
- en: 'Let’s start by loading the data (the code in this section is available in `07-pandas/
    sec1-intro/read_csv.py`):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据开始（本节中的代码可在`07-pandas/sec1-intro/read_csv.py`中找到）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'On my computer, this takes around 10 s. As you have seen in previous chapters,
    compressing the data may have a positive effect on time to process. Let’s try
    to compress the file with xz and load it. You will need to have xz installed,
    and then you can use `yellow_ tripdata_2020-01.csv`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的电脑上，这需要大约10秒。正如您在前几章中看到的，压缩数据可能会对处理时间产生积极影响。让我们尝试使用xz压缩文件并加载它。您需要安装xz，然后可以使用`yellow_tripdata_2020-01.csv`：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: pandas is smart enough to infer the compression type by extension, although
    you can override it. On my computer it took 15 s. While the number is worse than
    the uncompressed version, the file size is now only 74 MB, a seven-fold reduction.
    In this case, we were unable to reduce the time, so we will need to make a compromise
    between disk space and time to open the file. How you balance the two will depend
    on the requirements of your specific problem. We will revisit this with Apache
    Arrow; for now, table 7.1 provides the times and sizes for different algorithms.
    Times, of course, are dependent on the hardware on which this is run, but the
    relationship between different compression programs is what matters. Depending
    on your use case, you might need to read as fast as possible, or maybe size on
    disk is most important.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: pandas足够智能，可以根据扩展名推断压缩类型，尽管您可以覆盖它。在我的电脑上，这需要15秒。虽然这个数字比未压缩版本要差，但文件大小现在只有74 MB，减少了七倍。在这种情况下，我们无法减少时间，因此我们需要在磁盘空间和时间打开文件之间做出妥协。您如何平衡这两者将取决于您特定问题的需求。我们将使用Apache
    Arrow重新审视这个问题；目前，表7.1提供了不同算法的时间和大小。当然，时间取决于运行此硬件，但不同压缩程序之间的关系才是关键。根据您的使用情况，您可能需要尽可能快地读取，或者可能磁盘上的大小最重要。
- en: Table 7.1 Effect of CSV data compression on file size and time to open with
    pandas
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 CSV数据压缩对文件大小和pandas打开时间的影响
- en: '| Application | Time to read (s) | Size (MB) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 读取时间（秒） | 大小（MB） |'
- en: '| None | `10` | `566` |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| None | `10` | `566` |'
- en: '| `gip` | `12` | `105` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `gip` | `12` | `105` |'
- en: '| `bzip2` | `26` | `103` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `bzip2` | `26` | `103` |'
- en: '| `xz` | `15` | `74` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `xz` | `15` | `74` |'
- en: Do not take the relative size of the files across compression algorithms as
    a holy grail, however. Be sure to test with your data to see which ratios you
    get.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不要将不同压缩算法之间的文件相对大小视为神圣的法则。请确保使用您的数据进行测试，以查看您能得到哪些比率。
- en: The takeaway
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节要点
- en: With this example and throughout this chapter, the important takeaway is not
    the relative numbers presented, but the insight that different implementations
    and algorithms can have substantially different results. Two points are crucial
    to remember about optimizing memory and time (or, realistically, memory or time)
    when loading data. First, having an understanding of the underlying algorithms,
    rather than looking at them as a black box, will allow you to develop appropriate
    expectations of performance. Second, of course, you should profile for your specific
    conditions and determine whether it’s more important to conserve time or memory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子和本章的整个过程中，重要的不是相对数值，而是不同实现和算法可以产生实质上不同的结果的见解。在加载数据时优化内存和时间（或更现实地说，内存或时间）时，有两个要点至关重要。首先，了解底层算法，而不是将其视为黑盒，将允许你发展适当的性能预期。其次，当然，你应该针对你的特定条件进行性能分析，并确定是节省时间还是内存更重要。
- en: 7.1.2 Type inference of columns
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 列的类型推断
- en: 'When you load the data, you will get the following warning, at least with pandas
    version 1.0.5:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你加载数据时，你将至少得到以下警告，至少在 pandas 版本 1.0.5 中：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This message indicates that the data loader was not able to correctly infer
    the types of all columns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这条信息表明数据加载器无法正确推断所有列的类型。
- en: Warning Resist the temptation to set `low_memory=False` as suggested in pandas’s
    warning; with large data, that your code will very likely run out of memory and
    crash.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：抵制住按照 pandas 警告中建议的将 `low_memory=False` 设置为 False 的诱惑；对于大量数据，你的代码很可能耗尽内存并崩溃。
- en: The warning message is typically a smoking gun for the fact that some columns
    are being loaded with data types that are too general. For instance, a column
    that is an integer is promoted to an object, with the equivalent increase in memory
    required. We will look at concrete examples later in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 警告信息通常是某些列正在使用过于通用的数据类型加载的事实的一个明显的迹象。例如，一个整数列被提升为对象，所需的内存也相应增加。我们将在本章后面具体讨论这些例子。
- en: 'Before going into each column, let’s start by determining how much memory the
    whole data frame uses. pandas provides a more specific method than the general
    ones presented in previous chapters:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入到每一列之前，我们先来确定整个数据框占用了多少内存。pandas 提供了一种比之前章节中介绍的一般方法更具体的方法：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The abridged output is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 简化输出如下：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We get information for each column type, the number of entries, and memory usage.
    In our case, our 566 MB file is expanding to 2 GB! Given that we are talking about
    an input in text format, this seems oddly excessive.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了每个列类型、条目数量和内存使用情况的信息。在我们的例子中，我们的 566 MB 文件正在扩展到 2 GB！鉴于我们讨论的是文本格式的输入，这似乎有些过分。
- en: 'Let’s check the occupation of each column along with the number of unique values:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查每个列的占用情况以及唯一值的数量：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The abridged output is:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简化输出如下：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Columns of type object occupy more the 400 MB each (for details about the size,
    see chapter 2). `float64` requires 64 bits per float—hence, 8 bytes per value
    and thus 48 MB. The same is valid for `int64`. Can we reduce memory occupation
    by changing the types of columns? Yes, we can—dramatically.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 类型为对象的列每个占用超过 400 MB（关于大小的详细信息，请参阅第 2 章）。`float64` 每个浮点数需要 64 位——因此，每个值占用 8
    字节，从而占用 48 MB。对于 `int64` 也是如此。我们能否通过更改列的类型来减少内存占用？是的，我们可以——大幅减少。
- en: 'Let’s start with `tpep_pickup_datetime` and `tpep_dropoff_datetime`. Their
    values are, as the names imply, dates with times. You can inspect these with `df["tpep_pickup_
    datetime"].head()`. Let’s convert these columns to `datetime` format:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `tpep_pickup_datetime` 和 `tpep_dropoff_datetime` 开始。它们的值，正如名称所暗示的，是带有时间的日期。你可以使用
    `df["tpep_pickup_datetime"].head()` 来检查这些值。让我们将这些列转换为 `datetime` 格式：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Just this simple change reduced each column from 464 to 48 MB and the data frame
    from 2 GB to 1.2 GB. I hope this is enough to convince you of the importance of
    loading the correct data types.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样简单的更改就将每个列的占用从 464 MB 减少到 48 MB，并将数据框从 2 GB 减少到 1.2 GB。我希望这足以让你相信正确加载数据类型的重要性。
- en: 'Also, several variables are discrete and have a small number of possible values.
    For example, `payment_type` can only have six numeric values but is implemented
    with an 8-byte `float64`. Let’s recode it to a single byte:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有几个变量是离散的，并且具有少量可能的值。例如，`payment_type` 只能具有六个数值，但使用的是 8 字节的 `float64`。让我们将其重新编码为单字节：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The type for 8-bit signed integers comes from NumPy. pandas is, of course, dependent
    on NumPy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 8 位有符号整数的类型来自 NumPy。pandas 当然依赖于 NumPy。
- en: 'Unfortunately, this attempt will fail. If you inspect the column, you will
    see that there are a few missing data (NA) values along with the numeric values.
    We can recode the NAs as 0 because the value 0 is not otherwise used; if 0 is
    used, we would need to choose another value:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这次尝试将失败。如果你检查该列，你会看到其中有一些缺失数据（NA）值以及数值。我们可以将NA重新编码为0，因为0值没有被其他方式使用；如果使用了0，我们就需要选择另一个值：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This change reduces the columns from 48 to 6 MB as expected—for 8 bytes per
    value to 1 byte per value. We have six columns that can be downsized from 64 to
    8 bits and two that require 16 bits. This means another 450 MB cut. We are now
    down to roughly 750 MB.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这个变化将列的大小从48 MB减少到6 MB——从每个值8字节到每个值1字节。我们有六个列可以从64位缩小到8位，还有两个需要16位。这意味着又减少了450
    MB。我们现在大约只剩下750 MB。
- en: Don’t be fooled by how easy it was to solve that example; encoding and dealing
    with missing values is a complex problem and generally not so easily solved. While
    this example was easy to address, not all of the columns are that simple.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被解决那个例子有多容易所欺骗；编码和处理缺失值是一个复杂的问题，通常并不容易解决。虽然这个例子很容易解决，但并非所有列都那么简单。
- en: '`store_and_fwd_flag` is one example of a more complex column. It is a Boolean
    flag indicating whether the fare was held in vehicle memory because the server
    that stores fares was unavailable. For the many records that are missing values,
    it is unknown (i.e., we do not know whether it is true or false). If no values
    were missing, we could represent the column as a `boolean`, which would take 1
    bit per value. Given that we need to represent the third state, we have to use
    the next available data container which is 8-bit wide. Thus, dealing with the
    missing values costs us an eight-fold increase in memory allocation for this column.
    We end up doing:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`store_and_fwd_flag` 是一个更复杂列的例子。它是一个布尔标志，表示票价是否因为存储票价的服务器不可用而被保留在车辆内存中。对于许多缺失值的记录，它是未知的（即，我们不知道它是真还是假）。如果没有缺失值，我们可以将列表示为
    `boolean`，每个值占用1位。鉴于我们需要表示第三种状态，我们必须使用下一个可用的数据容器，它是8位宽。因此，处理缺失值使我们在这个列上的内存分配增加了八倍。我们最终这样做：'
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We convert NAs to spaces and get the ASCII code value of each character: 32
    for space, 78 for N, and 89 for Y. With the index function, we can code `NAs(32)`
    as `-1`, `N(78)` as `0,` and `Y(89)` as `1`.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将NA转换为空格，并获取每个字符的ASCII码值：空格为32，N为78，Y为89。使用索引函数，我们可以将 `NAs(32)` 编码为 `-1`，`N(78)`
    编码为 `0`，`Y(89)` 编码为 `1`。
- en: The takeaway
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经验总结
- en: 'We will revisit the complex topic of the representation of missing values throughout
    the remainder of the book, especially in the next chapter when we discuss persistence
    and the Parquet file format. For now, it’s enough to know that a common waste
    of memory (and thus the speed of operations) is columnar data of a more general
    type than necessary: the broader the data type, the bigger the memory footprint,
    and the slower the speed of operations. Changing the column types is not always
    a simple task, but it can dramatically reduce the memory footprint.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的剩余部分重新审视表示缺失值的复杂主题，特别是在下一章讨论持久性和Parquet文件格式时。现在，知道一个常见的内存浪费（以及因此的操作速度）是比必要的更通用的列数据类型：数据类型越宽，内存占用就越大，操作速度就越慢。更改列类型并不总是简单的事情，但它可以显著减少内存占用。
- en: 7.1.3 The effect of data type precision
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 数据类型精度的效果
- en: 'Another technique that we can use to reduce the memory footprint is using the
    same data type but with lower precision. For example, we can convert some of the
    cash values from `float64` to `float32` (i.e., double to single precision) for
    a 50% reduction in memory usage:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的一种减少内存占用的技术是使用相同的数据类型但具有更低的精度。例如，我们可以将一些现金值从 `float64` 转换为 `float32`（即，双精度到单精度），从而减少50%的内存使用：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we need to assess the effect of lowering precision on our ability to represent
    values. A simple approach here could be:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要评估降低精度对我们表示值的能力的影响。这里的一个简单方法可以是：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can compute the difference between the double and single precision. We must
    be careful to get the absolute values before adding them together to avoid cancellations.
    The total error on the whole data frame is a whopping $0.063:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算双精度和单精度之间的差异。我们必须在相加之前获取绝对值，以避免抵消。整个数据框的总误差高达0.063：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① We specify different types per column.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们为每个列指定不同的类型。
- en: ② We restrict some columns from 64-bit to 8-bit integers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们将一些列从64位整数限制为8位整数。
- en: ③ There is a slightly different way to specify date types.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 有一种稍微不同的方式来指定日期类型。
- en: ④ We create several converters, mostly to recode NAs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们创建了几个转换器，主要是为了重新编码NAs。
- en: ⑤ We are explicitly converting VendorID to np.int8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 我们明确地将VendorID转换为np.int8。
- en: At this stage, `df.info(memory_usage="deep")` will report a memory usage of
    757.4 MB. Notice, however, that most numeric types will report a 64-bit length,
    including `VendorID`, which we wrapped in a `np.int8` call—clearly to no avail.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，`df.info(memory_usage="deep")` 将报告757.4 MB的内存使用量。注意，然而，大多数数值类型将报告64位长度，包括我们用
    `np.int8` 调用包装的 `VendorID`——显然没有效果。
- en: 'We have several columns that clearly could still be smaller. Because we choose
    to be less precise, we will reduce all 64-bit floats to 16-bit ones. We will also
    reduce all 64-bit integers to 8-bit ones, as the range -128 to 127 of 8-bit integers
    is sufficient *in this specific case*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个列显然还可以更小。因为我们选择不那么精确，所以我们将所有64位浮点数减少到16位。我们还将所有64位整数减少到8位，因为8位整数的范围-128到127对于这个特定情况来说是足够的：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Our memory footprint is now down to 250.4 MB. Remember, we started with 2 GB.
    Not bad.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的内存占用已经降至250.4 MB。记住，我们最初是2 GB。还不错。
- en: 7.1.4 Recoding and reducing data
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 重新编码和减少数据
- en: 'If you really need, you can try to reduce the operation even further. For example,
    several numeric columns make use of a small number of different values. Let’s
    try to find those:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的需要，你可以尝试进一步减少操作。例如，几个数值列使用了少量不同的值。让我们尝试找到那些：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① value_counts returns the number of times each value is repeated.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ① `value_counts` 返回每个值重复的次数。
- en: ② We print all columns that have less than 10 distinct values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们打印所有具有不到10个不同值的列。
- en: 'Here we choose to print all columns that have less than 10 distinct values—10
    is an arbitrary value that you can change. We already improved some of these columns,
    but two are represented by 16-bit floating points and can be reduced. `improvement_surcharge`
    has only three distinct values: 0, 0.3, and -0.3\. These can easily be recoded
    to, say, 0, -1, and 1 and then reconverted. `congestion_surcharge` only has the
    values -2.5, -0.75, 0.5, 0.0, 0.5, 0.75, 2.0, 2.5, and 2.5\. Although you can
    make a table of some sort, if you multiply all the values by 4, they become integers.
    You can use an 8-bit integer representation and encode them by multiplying by
    4 and decode them by dividing equally by 4.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择打印所有具有不到10个不同值的列——10是一个任意值，你可以更改。我们已经改进了一些这些列，但有两个由16位浮点数表示，可以减少。`improvement_surcharge`
    只有三个不同的值：0，0.3和-0.3。这些可以很容易地重新编码为，比如说，0，-1和1，然后再重新转换。`congestion_surcharge` 只有以下值：-2.5，-0.75，0.5，0.0，0.5，0.75，2.0，2.5和2.5。虽然你可以制作某种表格，但如果将所有值乘以4，它们就变成了整数。你可以使用8位整数表示，并通过乘以4来编码它们，通过平均除以4来解码它们。
- en: 'Finally, there is the ultimate solution to conserve memory: avoid loading the
    parts of the data that we do not need. For our next task, we will be fine if we
    only load pickup and drop-off date times along with the congestion surcharge.
    We can do this easily with pandas:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有一个终极解决方案来节省内存：避免加载我们不需要的数据部分。对于我们的下一个任务，如果我们只加载接单和派单日期时间以及拥堵附加费就足够了。我们可以用pandas轻松做到这一点：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This code only requires 109.9 MB, around 5% of the original 2 GB requirement.
    We did this by reducing the number of columns and converting the load of some
    of them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码只需要109.9 MB，大约是原始2 GB需求的5%。我们是通过减少列数和转换其中一些列的负载来做到这一点的。
- en: The false safety of using `inplace=True`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `inplace=True` 的虚假安全性
- en: Most pandas methods have the ability to change the existing data structure in
    place, rather than returning a new data frame/series. You can save half of the
    memory at the expense of losing the original data. For example, you can drop all
    rows with NAs with
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数pandas方法都有在原地更改现有数据结构的能力，而不是返回一个新的数据框/序列。你可以通过牺牲原始数据来节省一半的内存。例如，你可以通过以下方式删除所有包含NAs的行：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'but you end up with three data frames, occupying twice the memory. Alternatively,
    you can use:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但最终你会有三个数据框，占用两倍的内存。或者，你可以使用：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will change the state of the original data frame so it won’t work in all
    situations, but in many cases, it can be a simple solution to reduce memory consumption
    by half.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将改变原始数据框的状态，因此在所有情况下可能都不会工作，但在许多情况下，这可以是一个简单的解决方案，将内存消耗减半。
- en: 'Be careful though: during the execution of the operation, pandas will allocate
    space for both arrays. So, during the execution, memory requirements will double.
    In a sense, this is mostly a convenience functionality that you can replicate
    with a `del` after using the default call without `inplace`.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是要小心：在操作执行过程中，pandas将为两个数组分配空间。因此，在执行过程中，内存需求将加倍。从某种意义上说，这主要是一个便利功能，你可以在使用默认调用而不使用`inplace`之后用`del`来复制。
- en: Arrow provides a much more interesting approach to lean memory management with
    the `self_destruct` parameter, which we will discuss later in the chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow通过`self_destruct`参数提供了一个更有趣的内存管理方法，我们将在本章稍后讨论。
- en: 'The code in this section demonstrates the effect of making the right choices
    for the data representation of columns. In practice, the pandas reader can do
    all this at the start:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的代码展示了为列的数据表示做出正确选择的影响。在实践中，pandas读取器可以在开始时完成所有这些操作：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① We can specify the desired type of some columns.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们可以指定某些列的期望类型。
- en: ② Dates are handled separately from other types.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ② 日期与其他类型分开处理。
- en: ③ We can convert on load.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们可以在加载时进行转换。
- en: Note that integer and float types will always be of the larger type, so you
    might need to downcast. Now that we have loaded the data in a memory efficiently,
    let’s see what can speed up data analysis with pandas.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，整数和浮点类型始终是较大的类型，因此你可能需要降级。现在我们已经以内存高效的方式加载数据，让我们看看pandas如何可以加快数据分析速度。
- en: The takeaway
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: We have gone through these exercises to demonstrate that careful choices of
    data types can decrease the amount of memory used. Narrowing the data type and
    broadening the precision are two approaches with relatively few tradeoffs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进行了这些练习，以证明仔细选择数据类型可以减少使用的内存量。缩小数据类型和扩大精度是两种具有相对较少权衡的方法。
- en: There’s more to understand about these general approaches, and we will return
    to them later. Reducing the amount and representation of data will be a big part
    of the last chapter of this book.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些通用方法，还有更多需要了解的，我们将在稍后回到它们。减少数据和数据表示的数量将是本书最后一章的主要内容。
- en: The good news about changing data types is that generally you don’t need to
    convert the data after loading, as pandas will do that for you. In the next example,
    we will use `read_csv` to do most of the conversions on loading.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 关于更改数据类型的好消息是，通常你不需要在加载后转换数据，因为pandas会为你完成这个工作。在下一个示例中，我们将使用`read_csv`在加载时进行大部分转换。
- en: 7.2 Techniques to increase data analysis speed
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 提高数据分析速度的技术
- en: Let’s now access records of NYC trips to do some statistical analysis. For example,
    we will determine the fraction of the payment that is a tip. We are not going
    to concentrate on the statistical analysis per se, as this is not a book about
    data science. Instead, we want to figure out how to access information efficiently
    so we can perform that kind of analysis if we choose. Here we will consider data
    frame indexing techniques and strategies to iterate over rows.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来访问纽约市的行程记录以进行一些统计分析。例如，我们将确定支付中作为小费的比率。我们不会专注于统计分析本身，因为这不是一本关于数据科学的书。相反，我们想要找出如何高效地访问信息，这样我们就可以在需要时进行这种分析。在这里，我们将考虑数据框索引技术和遍历行的策略。
- en: 'We start by loading the data. We just need three fields (the code is in `07-pandas/
    sec2-intro/index.py`):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据。我们只需要三个字段（代码在`07-pandas/ sec2-intro/index.py`中）：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 7.2.1 Using indexing to accelerate access
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 使用索引加速访问
- en: 'Let’s access all records with a certain pickup time:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们访问具有特定接车时间的所有记录：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For my machine, `timeit` reports a mean of 17.1 ms. We can try to sort the
    data frame by the pickup column:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的机器，`timeit`报告的平均值为17.1毫秒。我们可以尝试按接车列对数据框进行排序：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Unfortunately, from a timing perspective, the result is on the same order of
    magnitude: pandas ignores column sorting to fetch rows. We can expect a completely
    different execution time if we use the index:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，从时间角度来说，结果在同一数量级：pandas在获取行时忽略了列排序。如果我们使用索引，我们可以期望得到完全不同的执行时间：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this case, we index the data frame on `tpep_pickup_datetime`. If we do not
    sort the data frame, as is the case in `df_pickup`, nothing is gained. But for
    `df_pickup_sorted`, which is now indexed *and* sorted by `tpep_pickup_datetime`,
    we go to 395 microseconds, which is more than 40 times faster.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们在`tpep_pickup_datetime`上索引数据框。如果我们不排序数据框，就像`df_pickup`中那样，我们什么也得不到。但对于`df_pickup_sorted`，它现在被索引并且按`tpep_pickup_datetime`排序，我们达到了395微秒，这比之前快了40多倍。
- en: 'This solution comes with plenty of caveats, the obvious one being that it can
    only be used on the index. So, if you want to use another field, you would have
    to index on the other column or have an index with multiple columns. Thus, using
    the index is not a general solution, but our example suggests that you should
    carefully choose how you construct your index for performance. When you rely on
    the index—much pandas usage ignores the index—you are trading consistency of querying
    for potential speed gains. For example, if you want to know all rides that paid
    the congestion surcharge you can use this code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案有很多注意事项，最明显的一个是它只能用于索引。所以，如果你想使用另一个字段，你将不得不在另一列上索引，或者有一个多列的索引。因此，使用索引不是一个通用的解决方案，但我们的例子表明，你应该仔细选择如何构建你的索引以获得性能。当你依赖于索引——许多
    pandas 使用忽略了索引——你是在用查询的一致性换取潜在的加速。例如，如果你想了解所有支付拥堵费的车次，你可以使用以下代码：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice that the query language treats both columns with the same language.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，查询语言对待这两个列使用的是相同的语言。
- en: 'But if one is indexed, you can, for example, use this code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果有一个索引，例如，你可以使用以下代码：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip All the arguments presented here for indexes can be used when joining frames
    (`df.join`) with an even greater effect on performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：这里提出的所有关于索引的参数都可以在连接框架（`df.join`）时使用，对性能的影响更大。
- en: Now that we have a notion of the effect of using indexes, let’s actually use
    the whole dataset to compute the mean fraction of the amount paid that is a tip.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了使用索引的影响，让我们实际使用整个数据集来计算支付金额的平均小费比例。
- en: 7.2.2 Row iteration strategies
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 行迭代策略
- en: We will now consider different approaches to traverse a data frame. We will
    compute the fraction of the total amount paid that is a tip for our dataset, which
    will require going over all records to get the tip and total amounts.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑不同的方法来遍历数据帧。我们将计算我们数据集中支付金额中作为小费的比例，这将需要遍历所有记录以获取小费和总金额。
- en: 'We will start by reading the data and removing all total amounts that are zero
    (see `07-pandas/sec2-speed/traversing.py`):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先读取数据，并删除所有总金额为零的记录（见 `07-pandas/sec2-speed/traversing.py`）：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that we subsample the main data frame at 10% and 1%, which will help
    us later with performance testing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将主数据帧的样本量减少到 10% 和 1%，这将在我们稍后的性能测试中有所帮助。
- en: 'Let’s start by using a conventional Python technique (i.e., not using pandas-
    or NumPy-based approaches like vectorization): a `for` loop over all records:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先使用一种传统的 Python 技术（即，不使用基于 pandas 或 NumPy 的方法，如向量化）：对所有记录进行 `for` 循环遍历：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① A typical Python for loop using the number of rows
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用行数进行典型的 Python `for` 循环
- en: ② Accesses the row by position
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过位置访问行
- en: 'This code represents what a Python developer with no pandas or NumPy experience
    would typically write. The performance is, quite frankly, horrendous: measured
    in *minutes* on my computer as reported by `timeit`.[¹](#pgfId-1014608)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码代表了一个没有 pandas 或 NumPy 经验的 Python 开发者通常会编写的代码。坦白说，性能非常糟糕：在我的计算机上，`timeit`
    报告的测量结果是 *分钟*。[¹](#pgfId-1014608)
- en: 'There are two `for`-based approaches that provide better results. The first
    one is based on the `iterrows` method of a data frame:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种基于 `for` 的方法可以提供更好的结果。第一种是基于数据帧的 `iterrows` 方法：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This code is still a `for` loop, but in this case, we are using a pandas iterator
    that returns the current position and a row. The time is slightly better but still
    really bad.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码仍然是一个 `for` 循环，但在这个例子中，我们使用了一个 pandas 迭代器，它返回当前位置和一行。时间略有改善，但仍然非常糟糕。
- en: Tip If you are just starting with pandas and NumPy, it is perfectly normal for
    you to be used to a `for`-loop approach to perform many computations. While your
    medium-term approach should consider other techniques like vectorization (we will
    see a few examples later), in the short term, while you are not used to more efficient
    approaches, consider avoiding both the explicit and `iterrows`-based idioms. Of
    all the `for`-based approaches, `itertuples` will most probably save you the most
    time while still using an approach that you are comfortable with. You should,
    in any case, train yourself to use explicit iteration with pandas as soon as possible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如果你刚开始使用 pandas 和 NumPy，习惯使用 `for` 循环来进行大量计算是完全正常的。虽然你的中期方法应该考虑其他技术，如向量化（我们稍后会看到一些例子），但在短期内，在你不习惯更高效的方法时，考虑避免使用显式和基于
    `iterrows` 的惯用用法。在所有基于 `for` 的方法中，`itertuples` 可能会为你节省最多的时间，同时仍然使用你感到舒适的方法。无论如何，你应该尽快训练自己使用
    pandas 的显式迭代。
- en: 'Our final `for`-based approach is based on `itertuples`, where we use an iterator
    that returns one tuple per row:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的基于 `for` 的方法基于 `itertuples`，其中我们使用一个每次返回一行元组的迭代器：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: While the idiom is still the same, the mean time falls to 18 s!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然习惯用法仍然相同，但平均时间下降到 18 秒！
- en: 'We are now going to consider pandas-based dialects. We will start by using
    `apply`, which is conceptually similar to using a map function, where each row
    is processed individually:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑基于 pandas 的方言。我们将首先使用 `apply`，这在概念上类似于使用 map 函数，其中每一行都是单独处理的：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① Apply can be called for each column, which is the default, or for each row,
    which is what we will be using. The default axis of 0 will do column processing,
    and the axis of 1 will do row processing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ① 可以对每一列调用 `apply`，这是默认的，或者对每一行调用，这是我们将会使用的。默认的轴 0 将执行列处理，而轴 1 将执行行处理。
- en: ② We use the data frame mean function to compute the final value.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们使用数据框的均值函数来计算最终值。
- en: Using `apply` reduced the time to 9.5 s, which is twice as fast as the previous
    solution—good but not great.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `apply` 将时间减少到 9.5 秒，这是之前解决方案的两倍快——不错，但还不够好。
- en: 'Before we discuss the vectorization-based solution, let’s try a slightly different
    dialect for `apply`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论基于矢量的解决方案之前，让我们尝试一个稍微不同的 `apply` 语法：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The difference here is that we use `row.tip_amount` and `row.total_amount` instead
    of `row["tip_amount"]` and `row["total_amount"]`. Our object attribute–based approach
    is actually slower compared to the dictionary approach. The mean cost on my computer
    is 14 s.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区别在于我们使用 `row.tip_amount` 和 `row.total_amount` 而不是 `row["tip_amount"]` 和
    `row["total_amount"]`。我们的基于对象属性的方法实际上比字典方法要慢。在我的电脑上平均成本是 14 秒。
- en: Tip With different pandas versions, the performance relationship between different
    methods might change, as there is no guarantee that algorithms stay the same.
    This is valid both for accessing values on row objects, as well as for different
    algorithms. So, if performance is not good enough, always benchmark different
    algorithm approaches (`for`, `apply`, vectorization, etc.) and object access patterns
    (object attribute lookup versus dictionary lookup). Do not take the relationships
    presented in this book as gospel.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：随着 pandas 版本的更新，不同方法之间的性能关系可能会改变，因为没有保证算法保持不变。这对于访问行对象的值以及不同的算法都是有效的。所以，如果性能不够好，总是基准测试不同的算法方法（`for`、`apply`、矢量化等）和对象访问模式（对象属性查找与字典查找）。不要将本书中呈现的关系视为圣经。
- en: 'Now let’s see the effect of using pandas best practices for computation. We
    will start with a vectorization approach:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看使用 pandas 最佳实践进行计算的影响。我们将从矢量化方法开始：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We take the `tip_amount` series from the data frame and divide it by the `total_amount`
    series. We then use the mean. The time required fell by several orders of magnitude:
    it now takes a mean time of 32 ms.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据框中提取 `tip_amount` 序列，并将其除以 `total_amount` 序列。然后我们使用均值。所需时间减少了几个数量级：现在平均时间为
    32 毫秒。
- en: Remember, the example we worked on here is a simple one, to illustrate the importance
    of different strategies for iterating over rows. For more complex computations,
    try to come up with a vectorized approach. If that is not possible, you may be
    able to split the computation into parts, so you have isolated a vectorized (i.e.,
    fast) part. Then you can reduce the nonvectorized part to the smallest cost possible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们在这里工作的例子是一个简单的例子，用来说明不同策略遍历行的重要性。对于更复杂的计算，尝试提出一个矢量化方法。如果这不可能，你可能能够将计算分成几个部分，这样你就有了一个独立的矢量化（即快速）部分。然后你可以将非矢量化部分减少到可能的最小成本。
- en: The takeaway
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: The data-loading process is often overlooked as a task that can contribute to
    memory usage and thus speed of operations further on down the line. Correct typing
    of columns is the main way to optimize memory while loading, although broadening
    precision parameters can also help. pandas can handle the typing as it loads so
    you don’t have to do it afterward.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载过程通常被忽视，因为它可以导致内存使用增加，从而进一步影响操作的效率。在加载时正确设置列类型是优化内存的主要方式，尽管扩展精度参数也可以有所帮助。pandas
    在加载时可以处理类型，所以你不需要之后再进行操作。
- en: 'Accessing data once it is loaded is another process that can be examined for
    potential time-savings during operations. There are two general strategies for
    improving efficiency while accessing data. First, using indexing can be helpful
    but it also comes with a number of drawbacks. You can use row iteration. Some
    methods of iterating over data frames are faster than others. The general rule
    of thumb is that row-based analysis should usually be approached in a declarative
    way: vectorizing if possible but at least avoiding explicit iteration. Now that
    we have considered some pandas-native approaches to optimization, let’s consider
    alternatives using lower-level techniques.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据后访问数据是另一个可以检查以节省操作时间的潜在过程。在访问数据时提高效率有两种一般策略。首先，使用索引可能会有所帮助，但它也带来了一些缺点。您可以使用行迭代。一些遍历数据帧的方法比其他方法更快。一般规则是，基于行的分析通常应以声明性方式处理：如果可能，则向量化，至少避免显式迭代。现在我们已经考虑了一些pandas原生优化方法，让我们考虑使用更低级技术的方法。
- en: 7.3 pandas on top of NumPy, Cython, and NumExpr
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 在NumPy、Cython和NumExpr之上使用pandas
- en: For the next few sections, we will draw on discussions we’ve had in previous
    chapters about NumPy (chapter 4), Cython (chapter 5), and NumExpr (chapter 6),
    so here we will just do a whirlwind tour of these technologies as they apply to
    Python. If you need any reminders of the basics, please feel free to turn back
    to those chapters at any point necessary.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将借鉴前面章节（第4章NumPy、第5章Cython和第6章NumExpr）中关于这些技术的讨论，因此在这里我们只需对这些技术如何应用于Python进行快速浏览。如果您需要回顾基础知识，请随时在需要时回到那些章节。
- en: 'The goal of this section is to investigate NumPy, Cython, and NumExpr from
    a pandas perspective and to see how they can improve performance for data analysis.
    To conduct this investigation, we will revisit the example we used in the previous
    section: computing the portion of the paid amount of each fare that is a tip.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是从pandas的角度研究NumPy、Cython和NumExpr，并了解它们如何提高数据分析的性能。为了进行这项调查，我们将回顾上一节中使用的示例：计算每次车费中作为小费的支付金额的比例。
- en: 7.3.1 Explicit use of NumPy
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 显式使用NumPy
- en: 'The approaches we used in the previous section are all, implicitly, NumPy approaches,
    in the sense that pandas sits on top of NumPy. It is also possible to use NumPy
    explicitly. We start by getting the NumPy underlying representation of a series
    and then using a NumPy operation (see `07-pandas/sec3-numpy-numpexpr-cython/traversing.py`):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中使用的方法都是隐式地NumPy方法，因为pandas位于NumPy之上。也可以显式地使用NumPy。我们首先获取序列的NumPy底层表示，然后使用NumPy操作（见`07-pandas/sec3-numpy-numpexpr-cython/traversing.py`）：
- en: '[PRE33]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ① to_numpy references the underlying NumPy array.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ① to_numpy引用了底层的NumPy数组。
- en: ② The type is now a numpy.ndarray, not pandas.Series.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ② 类型现在是numpy.ndarray，而不是pandas.Series。
- en: ③ We divide using a vectorizing division over arrays, not series.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们使用数组上的向量化除法进行除法，而不是使用序列。
- en: ④ We use the mean from NumPy, not pandas.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们使用NumPy的均值，而不是pandas。
- en: The NumPy-based vectorized code runs in 11 ms on my computer, compared to 35
    ms on the vectorized pandas version. The previous operation works on NumPy arrays,
    but not pandas series.[²](#pgfId-1015434)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NumPy的向量化代码在我的电脑上运行时间为11毫秒，而向量化pandas版本则需要35毫秒。之前的操作适用于NumPy数组，但不适用于pandas序列。[²](#pgfId-1015434)
- en: Tip The `to_numpy` method returns a reference to the underlying pandas array.
    If you prefer a copy because you might want to make changes that are not reflected
    on the original, then use the `to_numpy(copy=True)` method. Remember, if you copy,
    memory usage will double and there will be a time cost for doing the copy.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 `to_numpy` 方法返回对底层pandas数组的引用。如果您更喜欢副本，因为您可能想要进行不会反映在原始数据上的更改，那么请使用 `to_numpy(copy=True)`
    方法。记住，如果您复制，内存使用量将加倍，并且复制操作会有时间成本。
- en: 7.3.2 pandas on top of NumExpr
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 在NumExpr之上使用pandas
- en: We can use NumExpr to query the data instead of using pandas’ querying engine.
    NumExpr is an expression evaluator that can perform substantially better than
    NumPy, due not only to its highly efficient multithreaded implementation, but
    also because of its judicious use of intermediate memory, allowing many computations
    to be done mostly based on CPU cache. For more details, see the previous chapter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumExpr查询数据，而不是使用pandas的查询引擎。NumExpr是一个表达式评估器，其性能可以显著优于NumPy，这不仅归功于其高度高效的线程化实现，还归功于其对中间内存的明智使用，使得许多计算主要基于CPU缓存完成。更多详情，请参阅上一章。
- en: 'Here is a simple implementation of the fraction of payment as the tip:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对支付比例作为小费的简单实现：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As expected, pandas expands NumExpr’s language with support for data frames
    and series. In the previous case, we refer to the columns `tip_amount` and `total_amount`
    in our evaluation string, which are resolved to the related pandas columns. It
    is also possible to use pandas’ eval in the local namespace. For example, the
    previous code could be implemented as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，pandas通过支持数据框和序列扩展了NumExpr的语言。在前一个案例中，我们在评估字符串中引用了`tip_amount`和`total_amount`列，这些列被解析为相关的pandas列。也可以在局部命名空间中使用pandas的eval。例如，前面的代码可以实施如下：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This would allow you to refer to more than a single data frame in an eval call.
    You could also use non-pandas variables; for details, see the eval documentation
    on the pandas website ([http://mng.bz/aMAX](http://mng.bz/aMAX)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这将允许你在eval调用中引用多个数据框。你也可以使用非pandas变量；有关详细信息，请参阅pandas网站上的eval文档([http://mng.bz/aMAX](http://mng.bz/aMAX))。
- en: What about performance? It is in the same league as the vectorized pandas solution—around
    35 ms—hence substantially slower than the NumPy solution at 11 ms. What is going
    on?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关于性能？它与矢量化pandas解决方案处于同一水平——大约35毫秒——因此比11毫秒的NumPy解决方案慢得多。发生了什么？
- en: 'First, we pay a price by parsing the string into executable code. As we saw
    in the previous chapter, this means that *NumExpr is only useful when the amount
    of data to process is large enough to justify the overhead*. Determining what
    is “large enough” will require some evaluation on a case-by-case basis: you will
    have to do some profiling of your data. In this specific case, we are using a
    large enough dataset. What is going on then?'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过将字符串解析成可执行代码来付出代价。正如我们在上一章所看到的，这意味着*NumExpr只有在处理的数据量足够大，足以证明开销是合理的时才有效用*。确定“足够大”需要逐个案例进行评估：你将不得不对你的数据进行一些分析。在这个特定案例中，我们使用了一个足够大的数据集。那么发生了什么？
- en: 'The ability of NumExpr to generate efficient computation strategies *increases*
    with the complexity of the formula, as cache hits become more common. Let’s consider
    the, arguably contrived, example of summing the fraction that we are using four
    times:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公式复杂性的增加，NumExpr生成高效计算策略的能力也增加，因为缓存命中变得更加常见。让我们考虑一个，可以说是人为的，例子，即我们使用四次求和的分数：
- en: tip_amount / total_amount + tip_amount / total_amount + tip_amount / total_amount
    + tip_amount / total_amount
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: tip_amount / total_amount + tip_amount / total_amount + tip_amount / total_amount
    + tip_amount / total_amount
- en: 'Here are the NumPy and NumExpr implementations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是NumPy和NumExpr的实现：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The only thing that changed was the complexity of the formula. The NumPy solution
    jumps, slightly worse than linearly, to a mean of 55 ms. NumExpr stays exactly
    the same at 35 ms! This is not to imply that the performance of NumExpr makes
    the complexity of the formula irrelevant, but it strongly suggests what we observed
    in the previous chapter: if our implementation can rely on the CPU cache as much
    as possible and avoid going to DRAM, then we can get substantial performance increases
    that are apparently counterintuitive.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一发生变化的是公式的复杂性。NumPy的解决方案略微下降，比线性略差，平均为55毫秒。而NumExpr则保持在35毫秒！这并不是说NumExpr的性能使得公式的复杂性变得无关紧要，但它强烈暗示了我们之前章节中观察到的现象：如果我们的实现尽可能依赖CPU缓存并避免访问DRAM，那么我们可以获得显著的性能提升，这看起来似乎是反直觉的。
- en: The takeaway
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: NumExpr is more efficient with lots of data *and* complex formulas, which means
    it will be helpful in the most complicated cases. As you have seen, in our first
    approach, NumExpr underperformed NumPy and pandas. This is yet another example
    suggesting that you should not always use the “best” technique, but instead, evaluate
    for your dataset and algorithm to determine the most efficient solution. One size
    doesn’t fit all, and the most sophisticated and elegant solution presented in
    this book, which NumExpr arguably is, should not be taken as gospel. Let’s now
    see how we can use Cython with pandas to improve the performance of our exercise.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: NumExpr在处理大量数据和复杂公式时更有效率，这意味着它将在最复杂的情况下有所帮助。正如你所看到的，在我们的第一种方法中，NumExpr的表现不如NumPy和pandas。这又是另一个例子，表明你不应该总是使用“最佳”技术，而应该评估你的数据集和算法，以确定最有效的解决方案。一种方法并不适合所有人，这本书中提出的最复杂和最优雅的解决方案，即NumExpr，不应被视为绝对真理。现在让我们看看如何使用Cython与pandas来提高我们练习的性能。
- en: 7.3.3 Cython and pandas
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 Cython和pandas
- en: 'We are now going to re-implement the tip analysis code using Cython and profile
    the potential performance benefits. We will keep this subsection deliberately
    short because there is really no such thing as a *direct* relationship between
    Cython and pandas. In this case, our ability to use Cython is completely based
    on NumPy, as depicted in figure 7.1\. So, if you have read the Cython chapter,
    you should be mostly comfortable with the following code. Here we will recreate
    our ongoing example with Cython: we will be using Cython best practices from the
    get-go.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 Cython 重新实现提示分析代码，并分析潜在的性能优势。我们将故意使这个子节简短，因为没有真正的 Cython 和 pandas 之间的
    *直接* 关系。在这种情况下，我们使用 Cython 的能力完全基于 NumPy，如图 7.1 所示。因此，如果你已经阅读了 Cython 章节的话，你应该对下面的代码感到很熟悉。在这里，我们将使用
    Cython 重新创建我们的示例：我们将从一开始就使用 Cython 的最佳实践。
- en: '![](../Images/CH07_F01_Antao.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F01_Antao.png)'
- en: Figure 7.1 pandas using Cython via NumPy
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 使用 Cython 通过 NumPy 的 pandas
- en: 'The Pure Python calling code is (see `07-pandas/sec3-numpy-numpexpr-cython/
    traversing_cython_top.py`):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 纯 Python 调用代码是（见 `07-pandas/sec3-numpy-numpexpr-cython/traversing_cython_top.py`）：
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ① We use the pyximport system for expediency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们为了方便使用 pyximport 系统。
- en: ② We need to include NumPy C include files.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们需要包含 NumPy C 包含文件。
- en: ③ We need to access the NumPy representation of the series.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们需要访问序列的 NumPy 表示。
- en: ④ We call the Cython implementation of the function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们调用函数的 Cython 实现。
- en: 'The Cython code is (see `07-pandas/sec3-numpy-numpexpr-cython/traversing_ cython_impl.pyx`):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 代码是（见 `07-pandas/sec3-numpy-numpexpr-cython/traversing_cython_impl.pyx`）：
- en: '[PRE38]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Accesses Cython support functions
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ① 访问 Cython 支持函数
- en: ② Imports access to C-level NumPy functions
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入对 C 级 NumPy 函数的访问
- en: ③ Deactivates all Python-bound code for array bounds checking, None checking,
    and Wrap indexing
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 禁用所有与 Python 绑定的代码，包括数组边界检查、None 检查和包装索引
- en: ④ We also use C-division without any checks. This is the only new concept introduced
    here.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们还使用了不带任何检查的 C-除法。这是这里引入的唯一新概念。
- en: ⑤ We use a C-only function (cdef) and type the return (cnp.float64_t). This
    allows Cython to avoid Python overhead in the signature.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 我们使用 C-only 函数（cdef）并指定返回类型（cnp.float64_t）。这允许 Cython 在签名中避免 Python 开销。
- en: ⑥ We type the inputs as memoryviews of 64-bit floats, which are substantially
    faster than general Python objects or even NumPy arrays.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 我们将输入类型定义为 64 位浮点数的内存视图，这比一般的 Python 对象甚至 NumPy 数组要快得多。
- en: ⑦ As we have cleaned up the function completely of Python interactions, the
    GIL can be released when called.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 由于我们已经完全清理了函数与 Python 的交互，当调用时可以释放 GIL。
- en: ⑧ We use Cython type for all the variables.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 我们为所有变量使用 Cython 类型。
- en: ⑨ We only divide at the end, which is substantially more efficient.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 我们只在最后进行除法，这要高效得多。
- en: ⑩ We have a “bridge” function that is callable from Python. This will implicitly
    convert the NumPy array to memoryviews.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 我们有一个可以从 Python 调用的“桥梁”函数。这将隐式地将 NumPy 数组转换为内存视图。
- en: For completeness, I annotated all the code with the practices that we studied
    in the Cython chapter (chapter 4). The only new feature here is the usage of the
    `cdivision` annotation, which will not raise Python errors with a denominator
    of 0 and be more efficient. The code will crash if there is a division by 0\.
    Remember, we carefully removed all rows with a `total_amount` of 0 in the native
    Python code, so for our case, we will be fine.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我使用我们在 Cython 章节中学到的实践对所有的代码进行了注释。这里唯一的新特性是使用 `cdivision` 注解，它不会因为分母为
    0 而引发 Python 错误，并且更高效。如果发生除以 0 的情况，代码将会崩溃。记住，我们在原生 Python 代码中已经仔细移除了所有 `total_amount`
    为 0 的行，所以对于我们的情况，我们会没事的。
- en: If you change the code, remember to use `cython -a` to generate an HTML report
    of potential interactions with the Python interpreter (i.e., potential performance
    bottlenecks). Performance? 8.51 ms. The best of the pack!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你修改了代码，请记得使用 `cython -a` 生成与 Python 解释器潜在交互的 HTML 报告（即潜在的性能瓶颈）。性能？8.51 毫秒。这是最好的！
- en: The takeaway
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Since pandas is built on top of NumPy, using NumPy is implicit when we use pandas.
    Still, if we explicitly require NumPy data structures, we can further improve
    performance speed. Cython can also be used with the NumPy data structures in pandas
    to improve performance speed. When working with very large data frames and complex
    algorithms, NumExpr is often your best bet for speed. Because of the many variables,
    including hardware, software, data, and both long-term and short-term goals, it
    is impossible to rank these solutions against each other. What works best for
    my situation and my needs may not be the best solution for yours. Understanding
    and experimenting with these different approaches should give you the insights
    you need to select an appropriate strategy for specific projects.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 pandas 是建立在 NumPy 之上的，因此当我们使用 pandas 时，使用 NumPy 是隐含的。尽管如此，如果我们明确要求 NumPy
    数据结构，我们还可以进一步提高性能速度。Cython 也可以与 pandas 中的 NumPy 数据结构一起使用来提高性能速度。当处理非常大的数据框和复杂的算法时，NumExpr
    往往是速度的最佳选择。由于涉及许多变量，包括硬件、软件、数据和长期及短期目标，因此不可能将这些解决方案相互比较。最适合我的情况和需求的方法可能不是最适合你的最佳解决方案。理解和实验这些不同的方法应该会给你提供选择适当策略所需的见解，以用于特定的项目。
- en: Next, we will consider another approach to optimize pandas. We will use Apache
    Arrow to increase the performance of some common operations, such as reading data
    from the disk.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑另一种优化 pandas 的方法。我们将使用 Apache Arrow 来提高一些常见操作的性能，例如从磁盘读取数据。
- en: 7.4 Reading data into pandas with Arrow
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用 Arrow 将数据读入 pandas
- en: In this section, we will accelerate data loading into a pandas data frame using
    Apache Arrow to do the loading. But before we get started on that, let’s take
    a step back and understand the somewhat confusing relationship between pandas
    and Arrow.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Apache Arrow 加速数据加载到 pandas 数据框中。但在我们开始之前，让我们退一步，了解 pandas 和 Arrow
    之间有些令人困惑的关系。
- en: Note This section is intended to be a primer on how pandas and Arrow work together,
    not on Arrow per se, although we will indulge in some minor analytics at the end
    of the section. For more information on Arrow itself, see [https://arrow.apache.org/](https://arrow.apache.org/).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节旨在介绍 pandas 和 Arrow 如何协同工作，而不是专门介绍 Arrow，尽管我们将在本节末尾进行一些简单的分析。有关 Arrow 的更多信息，请参阅
    [https://arrow.apache.org/](https://arrow.apache.org/)。
- en: 7.4.1 The relationship between pandas and Apache Arrow
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 pandas 和 Apache Arrow 之间的关系
- en: Apache Arrow is essentially a language-independent memory format for columnar
    data. It is language-independent, which means it is not tied to Python/pandas
    or any other language. It is, at its core, a set of libraries to perform basic
    operations in very fast low-level languages like C, Rust, or Go, although sometimes
    implementations exist in higher-level languages like JavaScript. For slower languages,
    the faster implementations are wrapped around in a layer. For example, the Python
    “implementation” of Arrow is actually a wrapper for the C++ implementation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow 实质上是一种针对列式数据的语言无关内存格式。它是语言无关的，这意味着它与 Python/pandas 或任何其他语言无关。在核心上，它是一组库，用于在非常快速的底层语言（如
    C、Rust 或 Go）中执行基本操作，尽管有时也存在在高级语言（如 JavaScript）中的实现。对于较慢的语言，较快的实现被封装在一层中。例如，Arrow
    的 Python “实现”实际上是对 C++ 实现的封装。
- en: 'Here we will consider Arrow as a subsidiary to pandas: we want Arrow to accelerate
    certain parts of pandas data analysis, not replace pandas wholesale. Maybe in
    the future, as its analytics implementation grows, Arrow can work as a complete
    pandas replacement, but that is not yet the case.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将把 Arrow 视为 pandas 的辅助工具：我们希望 Arrow 加速 pandas 数据分析的部分功能，而不是完全取代 pandas。也许在未来，随着其分析实现的增长，Arrow
    可以作为完整的 pandas 替代品，但目前还不是这种情况。
- en: In this section, we will replace pandas persistence mechanism (i.e., the reading
    of CSV files with Arrow) and briefly touch on Arrow analytics. In the next section,
    we will discuss Arrow’s efficient Interprocess Communication (IPC) mechanism using
    the supplied IPC server, Plasma. In both cases, our objective is to determine
    whether we can increase speed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将用 Arrow 替换 pandas 的持久化机制（即使用 Arrow 读取 CSV 文件）并简要介绍 Arrow 分析。在下一节中，我们将讨论使用提供的
    IPC 服务器 Plasma 的 Arrow 高效进程间通信（IPC）机制。在这两种情况下，我们的目标是确定我们是否可以提高速度。
- en: Arrow has more functionality available. We will study the persistence part in
    more detail in the next chapter, especially in terms of file formats and their
    effects on efficiency. Arrow is also able to deal with several different persistent
    backends. In addition, there is functionality for remote procedure calls (RPC)
    to send data across different computers. A general overview of the current Arrow
    architecture is depicted in figure 7.2.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow 有更多可用的功能。我们将在下一章更详细地研究持久化部分，特别是文件格式及其对效率的影响。Arrow 还能够处理几个不同的持久化后端。此外，还有远程过程调用（RPC）的功能，可以在不同的计算机之间发送数据。当前
    Arrow 架构的概述如图 7.2 所示。
- en: '![](../Images/CH07_F02_Antao.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F02_Antao.png)'
- en: Figure 7.2 The internal architecture of PyArrow
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 PyArrow 的内部架构
- en: On top of Python is a Python wrapper for the C Arrow implementation. The C part
    is composed of several components—chief among them, a fledgling analysis engine
    that can make use of GPU computing and a persistence layer that can deal with
    several backends like the file system, Amazon S3, and Hadoop, and with several
    file formats such as CSV, Parquet, and JSON. Finally, there is interprocess and
    intermachine functionality to allow efficient communication with other processes,
    potentially running on other machines. Most important, the underlying Arrow data
    format is transversal to many programming languages and hardware architectures.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 之上是一个 C Arrow 实现的 Python 包装器。C 部分由几个组件组成——其中最重要的是一个初露锋芒的分析引擎，它可以利用
    GPU 计算，以及一个可以处理文件系统、Amazon S3 和 Hadoop 等多个后端以及 CSV、Parquet 和 JSON 等多个文件格式的持久化层。最后，还有进程间和机器间功能，允许与其他进程（可能运行在其他机器上）进行高效通信。最重要的是，底层的
    Arrow 数据格式适用于许多编程语言和硬件架构。
- en: Now let’s see how efficient Arrow is at data reading compared to pandas.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 Arrow 在数据读取方面的效率与 pandas 相比如何。
- en: 7.4.2 Reading a CSV file
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 读取 CSV 文件
- en: In the first section of this chapter, we used pandas to read a CSV file for
    information about taxi rides in New York City. One of the problems that Apache
    Arrow modernizes, compared to pandas, is file reading. It has a multithreaded
    reader and is substantially more intelligent at inferring column types. As stated
    in the beginning of this section, features of both pandas and Arrow will probably
    evolve after this book is published, and the relationship may change. At the time
    of this writing, Apache Arrow has a more modern architecture and it’s difficult
    to see how pandas, with its need to support a long, successful legacy of applications,
    can catch up.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们使用 pandas 读取纽约市出租车行程的信息 CSV 文件。Apache Arrow 相比于 pandas，现代化改进的一个问题就是文件读取。它有一个多线程的读取器，并且在推断列类型方面更加智能。正如本节开头所述，pandas
    和 Arrow 的特性在本书出版后可能会演变，它们之间的关系可能会发生变化。在撰写本文时，Apache Arrow 拥有更现代的架构，而 pandas 需要支持长期成功的应用遗产，因此很难赶上。
- en: 'Let’s load the same CSV file and study memory occupation of the loaded data
    (see `07-pandas/sec4-arrow-intro/read_csv.py`):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载相同的 CSV 文件并研究加载数据的内存占用（见 `07-pandas/sec4-arrow-intro/read_csv.py`）：
- en: '[PRE39]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① We import the CSV processor from PyArrow.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们从 PyArrow 导入 CSV 处理器。
- en: ② We traverse all columns to get type and allocation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们遍历所有列以获取类型和分配。
- en: 'The operation goes from 12 s to 2 s on my computer, a six-fold decrease. Here
    is an abridged version of the output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的电脑上，操作时间从 12 秒减少到 2 秒，减少了六倍。以下是输出摘要：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Without any help, Arrow’s memory consumption is 865 MB, compared with 2 GB from
    pandas. If we help pandas, then we get down to 250 MB, although using the naive
    Arrow to help pandas is not really fair. That being said, how does Arrow fare?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何帮助的情况下，Arrow 的内存消耗为 865 MB，而 pandas 为 2 GB。如果我们帮助 pandas，那么我们可以降低到 250 MB，尽管使用原始的
    Arrow 来帮助 pandas 并不公平。话虽如此，Arrow 的表现如何？
- en: 'Arrow fares pretty well: without domain knowledge, an automated system could
    potentially do better, but not much. `VendorID` has only three values (`1`, `2`,
    and `null`) and could be reduced to `int8`, which is valid for most integers.
    But in regard to using doubles for floats, Arrow could not know that we would
    be OK with a smaller representation.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow 表现相当不错：没有领域知识，自动化的系统可能做得更好，但不会太多。`VendorID` 只有三个值（`1`、`2` 和 `null`）并且可以减少到
    `int8`，这对于大多数整数是有效的。但在使用双精度浮点数方面，Arrow 无法知道我们是否可以接受更小的表示。
- en: Note Arrow types are different from Python and NumPy/pandas, although conversion
    is easy from one to the other. While it is simple from a programmatic perspective,
    there are important differences in the way types are represented.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Arrow类型与Python和NumPy/pandas不同，尽管从一种类型到另一种类型的转换很容易。虽然从程序的角度来看很简单，但在类型表示的方式上存在重要差异。
- en: 'You may have noticed that `VendorID`, which includes `null`, is coded as an
    integer. This would be impossible in pandas/NumPy, unless we recode the NA. Arrow
    implements missing values in a completely different way from NumPy/pandas: there
    is an *extra* bit array with one entry per row indicating whether the value is
    missing. This means that, at the expense of a modest memory cost for most types,
    missing values do not require any representation on the type per se. Hence, an
    integer can be represented by an integer, without any recoding for missing values.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，包括`null`的`VendorID`被编码为整数。在pandas/NumPy中，除非我们重新编码NA，否则这是不可能的。Arrow对缺失值的实现与NumPy/pandas完全不同：有一个额外的位数组，每行有一个条目，指示值是否缺失。这意味着，在大多数类型的适度内存成本下，缺失值不需要在类型本身上进行任何表示。因此，整数可以用整数表示，无需对缺失值进行重新编码。
- en: 'With Arrow’s representation of missing values, we can substantially reduce
    the memory requirements of many columns, but we still have to inform Arrow in
    a few cases. For that, PyArrow provides the `ConvertOptions` class:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Arrow对缺失值的表示，我们可以显著减少许多列的内存需求，但在某些情况下我们仍然需要通知Arrow。为此，PyArrow提供了`ConvertOptions`类：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ① Arrow can infer the type of store_and_fwd_flag as a boolean from true_values
    and false_values but, given that VendorID is numeric, Arrow needs to explicitly
    state its type.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ① Arrow可以从true_values和false_values推断出store_and_fwd_flag的类型为布尔型，但鉴于VendorID是数字型，Arrow需要明确声明其类型。
- en: ② We inform Arrow that Y and 1 are to be converted to true. This will only be
    used in columns that are boolean. We perform a similar operation for false values.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们通知Arrow将Y和1转换为true。这仅用于布尔类型的列。对于false值，我们执行类似的操作。
- en: ③ We pass the conversion options to the CSV reader.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们将转换选项传递给CSV读取器。
- en: '`VendorID` is technically not a `boolean`, but as it has only two possible
    values, we recode it as such to save memory.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上来说，`VendorID`不是一个`boolean`，但由于它只有两个可能的值，我们将其重新编码为布尔型以节省内存。
- en: Because missing values are handled separately, the memory cost of columns with
    a small number of possible states but that include missing values is much more
    efficient, with `boolean` being the most extreme case.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺失值是单独处理的，具有少量可能状态但包含缺失值的列的内存成本要高效得多，其中`boolean`是最极端的情况。
- en: For `store_and_fwd_flag`, we display the three values, along with memory occupation.
    Because it is below 1 MB, we also print the value as 790 KB.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`store_and_fwd_flag`，我们显示三个值，以及内存占用。因为它低于1 MB，所以我们还将值打印为790 KB。
- en: 'Unfortunately, we will have to convert from Arrow to pandas for analysis, as
    the internal formats are different. As such, we pay a memory and time price:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们将不得不将Arrow转换为pandas进行分析，因为内部格式不同。因此，我们付出了内存和时间上的代价：
- en: '[PRE42]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As expected, the pandas version requires more memory. For example, `store_and_
    fwd_flag` is now an object. Even if you convert it to a more efficient type, it
    will be less compact than what is possible with Arrow’s representation of missing
    values.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，pandas版本需要更多的内存。例如，`store_and_fwd_flag`现在是一个对象。即使你将其转换为更有效的类型，它也比Arrow对缺失值的表示方式要紧凑。
- en: 'At this stage, it appears that many of the gains that we made from using Arrow
    are lost, but that is not so. There are two points to consider: the time cost
    of converting the data and the memory occupation. Both can be addressed.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，看起来我们使用Arrow读取文件所获得的许多收益都丢失了，但事实并非如此。有两个要点需要考虑：转换数据的时间成本和内存占用。这两者都可以解决。
- en: 'The time cost of the conversion is a fraction of the gain from reading the
    file in Arrow: reading in pandas is around 12 s. Reading in Arrow is around 2
    s to which the conversion time is added. That time is 23 ms on my computer, thus
    negligible when compared with the read gains.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 转换的时间成本是读取文件时收益的一小部分：在pandas中读取大约需要12秒。在Arrow中读取大约需要2秒，然后加上转换时间。在我的电脑上，这个时间是23毫秒，与读取收益相比可以忽略不计。
- en: 'The memory problem is real, and with the current approach, at one point, we
    need twice as much memory. Fortunately, Arrow provides a solution. You can ask
    Arrow to self-destruct the Arrow structure during conversion. This will not increase
    memory consumption at any point in time, at the expense of destroying the Arrow
    version:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 内存问题确实是存在的，并且按照当前的方法，在某个时刻，我们需要两倍多的内存。幸运的是，Arrow 提供了一个解决方案。你可以要求 Arrow 在转换过程中自我销毁
    Arrow 结构。这不会在任何时刻增加内存消耗，但会以销毁 Arrow 版本为代价：
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The takeaway
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要点
- en: As this example should make clear, Arrow provides modern functionality with
    improved efficiency in both time and space when compared to pandas. As of now,
    Arrow is very far from offering the same analytics capabilities as pandas, so
    it is best used integrated with pandas. But let’s take a few minutes to see what
    data analysis with Arrow looks like and get a taste of what is possible, and probable,
    with future versions of Arrow.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子应该清楚地表明的那样，与 pandas 相比，Arrow 在时间和空间效率方面都提供了现代功能，并且改进了效率。到目前为止，Arrow 在提供与
    pandas 相同的分析能力方面还相去甚远，因此它最好与 pandas 集成使用。但让我们花几分钟时间看看使用 Arrow 进行数据分析的样子，并尝尝未来版本
    Arrow 可能实现的功能。
- en: 7.4.3 Analyzing with Arrow
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 使用 Arrow 进行分析
- en: Since this book is about what you can do right now to increase efficiency, what
    interests us about Arrow is how it works in service of pandas. But the fact is,
    Arrow can do data analysis on its own. Although it is seriously lacking in features
    for analysis, especially compared with pandas, as time goes by, its analytical
    capabilities will only improve.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书是关于你现在可以做什么来提高效率，我们对 Arrow 感兴趣的是它是如何服务于 pandas 的。但事实是，Arrow 可以独立进行数据分析。尽管与
    pandas 相比，它在分析功能方面严重缺乏，但随着时间的推移，其分析能力只会不断提高。
- en: 'To see how Arrow does data analysis, we will return, once again, to our analysis
    of tips for taxi drivers:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到 Arrow 如何进行数据分析，我们再次回到我们对出租车司机小费的分析：
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The details of the code and, by extension, the design philosophy differ significantly
    from pandas. First, at this stage, the interface is lower level: if you try to
    perform the `not_equal` operation using an integer (0) and not a float (0.0),
    the code will fail, as the types are different. Second, the interface is substantially
    more functional than object-oriented; note that we call functions with array parameters
    and not methods on top of arrays. Finally, error reporting is based on error codes,
    not throwing exceptions. Irrespective of API preferences, there are potential
    benefits to being minimalist by mapping the underlying C Arrow library with the
    least Python idiom on top, one advantage being speed.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的细节以及由此延伸的设计理念与 pandas 有很大差异。首先，在这个阶段，接口是低级别的：如果你尝试使用整数（0）而不是浮点数（0.0）来执行 `not_equal`
    操作，代码将会失败，因为类型不同。其次，接口在功能上比面向对象的方式更为实质；注意我们使用数组参数调用函数，而不是在数组上调用方法。最后，错误报告基于错误代码，而不是抛出异常。无论
    API 偏好如何，通过将底层的 C Arrow 库与最少的 Python 习惯用法映射在一起，都可以获得潜在的好处，其中之一就是速度。
- en: The time cost of the computation of the tip fraction is ~15 ms on my computer.
    This time is about half of the equivalent pandas version (`get_tip_mean_vector`).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的电脑上，计算小费比例的时间成本大约是 15 毫秒。这个时间大约是等效 pandas 版本（`get_tip_mean_vector`）的一半。
- en: We’ve seen how Arrow can efficiently load data into pandas. But there is a second
    way we can make use of Arrow and pandas together, and we turn to that next. Let’s
    see how Arrow can help with interoperation between different languages; remember,
    Arrow provides a standard memory format across implementations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了 Arrow 如何高效地将数据加载到 pandas 中。但是，我们还可以利用 Arrow 和 pandas 的第二种方式，接下来我们就转向这一点。让我们看看
    Arrow 如何帮助不同语言之间的互操作性；记住，Arrow 在不同实现之间提供了一个标准的内存格式。
- en: 7.5 Using Arrow interop to delegate work to more efficient languages and systems
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 使用 Arrow 互操作将工作委托给更高效的语言和系统
- en: One of the advantages of Arrow is its standard in-memory format, which allows
    data structure representation to be shared across implementations over many different
    languages. It does this sharing with zero-copy, or at the very least, it transfers
    data structure representations efficiently. In this section, we will explore why
    the Arrow architecture is more efficient than the alternatives. We will also implement
    an example using Arrow’s Plasma server for interprocess communication.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow的一个优点是其标准的内存格式，它允许数据结构表示在许多不同语言之间的实现中共享。它是通过零拷贝来共享的，或者至少，它以高效的方式传输数据结构表示。在本节中，我们将探讨为什么Arrow架构比其他替代方案更高效。我们还将实现一个使用Arrow的Plasma服务器进行进程间通信的示例。
- en: The main objective of this section is to illustrate how Arrow can make interprocess
    communication more efficient. Given that Plasma is under heavy development and
    that you can actually implement memory sharing explicitly yourself across processes,
    take the content here as more of an illustration of a design pattern. That being
    said, the code is, of course, fully functional and usable.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的主要目标是说明Arrow如何使进程间通信更加高效。鉴于Plasma正在积极开发中，并且你实际上可以在进程间显式地实现内存共享，这里的内容更多的是作为一个设计模式的说明。当然，代码是完全功能性和可用的。
- en: 7.5.1 Implications of Arrow’s language interop architecture
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 Arrow语言互操作架构的影响
- en: 'Imagine a scenario where you are doing most of your processing in Python, but
    you need to use a piece of R code to do some analysis. There are many ways to
    conduct interprocess and interlanguage communication, but consider two typical
    non-Arrow scenarios and two typical Arrow scenarios, as shown in figure 7.3 and
    described as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，你大部分的处理工作在Python中完成，但你需要使用一段R代码来进行一些分析。有几种进行进程间和语言间通信的方法，但考虑两种典型的非Arrow场景和两种典型的Arrow场景，如图7.3所示，如下所述：
- en: The first approach would be to write our data in a file format in Python (e.g.,
    CSV) and then read it in R. This could be very memory-efficient, but the time
    cost would be abysmal due to disk usage.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法是将我们的数据以Python（例如，CSV）的文件格式写入，然后在R中读取。这可能非常节省内存，但由于磁盘使用，时间成本将非常低。
- en: The second approach would use rpy2,[³](#pgfId-1017497) which would require converting
    from pandas to R data frames, which are roughly the equivalent of pandas in the
    R world. This would entail at least doubling the cost of conversion, both in time
    and memory. It is actually worse than this as we will see in the following discussion.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法将使用rpy2[³](#pgfId-1017497)，这将需要将pandas转换为R数据框，这在R世界中大致相当于pandas。这将至少使转换成本加倍，无论是时间还是内存。实际上，正如我们将在下面的讨论中看到的那样，情况比这还要糟糕。
- en: With Arrow, a third approach would be to take a pandas data frame, convert it
    to Arrow, pass it to R, convert it from Arrow, and process it. This requires time
    for two conversions (pandas to Arrow and Arrow to R). Memory-wise, the outcome
    will depend on whether you can be destructive, which adds no increase in consumption;
    if not, you will need to roughly double your memory during conversion.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Arrow，第三种方法是将pandas数据框转换为Arrow，然后传递给R，再从Arrow转换，并处理它。这需要两个转换的时间（pandas到Arrow和Arrow到R）。在内存方面，结果将取决于你是否可以进行破坏性操作，这不会增加任何消耗；如果不能，你将需要在转换期间大约加倍你的内存。
- en: Finally, the best-case scenario with Arrow, if you are processing in *both*
    Python and R, is based on Arrow only (e.g., no pandas), then it is just a question
    of passing a memory pointer, which adds zero cost in terms of processing and of
    memory.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用Arrow的最佳情况是，如果你在Python和R中同时处理数据，仅基于Arrow（例如，不使用pandas），那么这只是一个传递内存指针的问题，这在处理和内存方面都不会增加任何成本。
- en: '![](../Images/CH07_F03_Antao.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F03_Antao.png)'
- en: Figure 7.3 Some alternatives for interoperability between Python and R
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 Python和R之间互操作的一些替代方案
- en: 'It might seem at first glance that solution 2, going from pandas’ native format
    to R’s, is more efficient than using solution 3, going from pandas to Arrow and
    from Arrow to R. However, that is not the case for a few reasons:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，方案2（从pandas的本地格式转换为R的格式）似乎比方案3（从pandas转换为Arrow，然后从Arrow转换为R）更高效。然而，这并不是事实，原因有几个：
- en: The in-memory Arrow format is shared by all Arrow implementations, irrespective
    of language. This means that sharing an Arrow data structure is in essence just
    sharing a memory pointer.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存中的Arrow格式被所有Arrow实现共享，无论语言如何。这意味着共享一个Arrow数据结构本质上就是共享一个内存指针。
- en: When converting from pandas to R, the converter has to occur on one side, either
    Python or R, which means that it will be nonnative and very inefficient in one
    of the formats (i.e., the one that is nonnative).
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从pandas转换为R时，转换必须在一边进行，要么是Python要么是R，这意味着它将是非本地的，并且在其中一个格式（即非本地的）中非常低效。
- en: Arrow converters are done in C/C++, multithreaded, and designed from scratch
    precisely to be as efficient as possible. You have seen the effect of that philosophy
    in the previous section about the performance of using CSV readers.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arrow转换器是用C/C++编写的，多线程的，从头开始设计，以确保尽可能高效。您在前一节关于使用CSV读取器性能的讨论中已经看到了这种哲学的效果。
- en: 'There is also another important reason: in a complex system, you will need
    2n converters. For example, if you use pandas, Java, R, and Rust, you will need
    pandas/Java, pandas/R, Java/R, Java/Rust, pandas/Rust, and R/Rust converters.
    But if you use Arrow as an intermediate format, then you will need only four:
    pandas/Arrow, Java/Arrow, R/Arrow, and Rust/Arrow. If you use more systems, the
    combinations explode.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的原因是：在复杂系统中，您将需要2n个转换器。例如，如果您使用pandas、Java、R和Rust，您将需要pandas/Java、pandas/R、Java/R、Java/Rust、pandas/Rust和R/Rust转换器。但如果您使用Arrow作为中间格式，那么您只需要四个：pandas/Arrow、Java/Arrow、R/Arrow和Rust/Arrow。如果您使用更多的系统，组合会爆炸式增长。
- en: It is beyond the scope of this book to delve into the use of languages other
    than Python, but feel free to experiment with other language options to get a
    measure of the performance effect of each one.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涉及Python之外的语言的使用，但您可以自由地尝试其他语言选项，以衡量每种语言的性能影响。
- en: What we will do next, totally in Python, is demonstrate how to efficiently interprocess
    communication of data. In many real-world scenarios, one or more of your components
    would be implemented in a different language.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将要做的，完全使用Python，是展示如何高效地进行数据进程间通信。在许多实际场景中，您的一个或多个组件可能会用不同的语言实现。
- en: 7.5.2 Zero-copy operations on data with Arrow’s Plasma server
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 使用Arrow的Plasma服务器进行零拷贝数据操作
- en: 'Let’s go back to our old, familiar NYC taxi dataset and compute a set of statistics
    over the data. We will divide this into three processes: one reads the data and
    submits it for processing, another does the analysis, and a third just shows the
    results. There are two main reasons to consider this architecture: (1) There might
    be an implementation of an algorithm that we need that is available in a separate
    process that cannot be linked directly to Python, and (2) we might prefer to separate
    the more expensive processing code from the analysis code.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们熟悉的老NYC出租车数据集，并计算一组数据统计。我们将将其分为三个进程：一个读取数据并将其提交处理，另一个进行分析，第三个仅显示结果。考虑这种架构有两个主要原因：（1）可能有一个算法的实现可用，它位于一个单独的进程中，无法直接链接到Python，并且（2）我们可能更喜欢将更昂贵的处理代码与分析代码分开。
- en: 'Arrow provides a server, called Plasma, that manages shared memory: it allows
    you to register, read, and write objects, along with all operations to consult
    the catalog of existing objects. This facilitates the way processes are able to
    find each other in a standard fashion. This server is local—that is, not accessible
    on the network but via a local socket. It mostly exists to facilitate sharing
    of memory, easily find existing objects, and allow for the sharing of memory across
    processes that do not overlap in the lifetime (i.e., a consumer process is started
    after the producer has died).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow提供了一个名为Plasma的服务器，用于管理共享内存：它允许您注册、读取和写入对象，以及所有查询现有对象目录的操作。这简化了进程以标准方式找到彼此的方式。这个服务器是本地的——也就是说，不能通过网络访问，但可以通过本地套接字访问。它主要存在是为了促进内存共享，轻松找到现有对象，并允许在进程生命周期不重叠的情况下共享内存（即，消费者进程在生产者死亡后启动）。
- en: 'The first thing that we need to do is to start the Plasma server:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是启动Plasma服务器：
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This will use a UNIX socket, `/tmp/fast_python`, which is a form of interprocess
    communication, to allow processes to communicate. One gigabyte will be allocated
    as a shared space.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用UNIX套接字`/tmp/fast_python`，这是一种进程间通信的形式，以允许进程通信。将分配一个千兆字节的共享空间。
- en: 'Our first process is responsible for loading a CSV file and putting it in Plasma:
    we connect to the Plasma socket, read a file with Arrow, and deposit it with Plasma
    (see `07-pandas/sec5-arrow-plasma/load_csv.py`):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个进程负责加载CSV文件并将其放入Plasma：我们连接到Plasma套接字，使用Arrow读取文件，并将其存入Plasma（见`07-pandas/sec5-arrow-plasma/load_csv.py`）：
- en: '[PRE46]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① We connect to Plasma via the socket.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们通过套接字连接到Plasma。
- en: ② We are assuming that the CSV is in the NYC taxi format.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们假设CSV是纽约出租车格式。
- en: ③ We create an ID for our table.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们为我们的表格创建一个ID。
- en: ④ We put the object in Plasma.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们将对象放入等离子体中。
- en: When we put an object in Plasma, we have to give it an ID (i.e. name it). We
    will use a name that is `csv-`, followed by the PID (process ID) of our process.
    This name is good enough for our intentions here, but you might need something
    with less clash potential with other name strings as a more general solution.
    Plasma requires a 20-byte ID, so we pad the name with spaces to get to a size
    of 20 and then encode our string using the US-ASCII codec, which will return a
    list of bytes. You can use any codec you’d like as long as it will convert one
    character to 1 byte, or you will end up with a byte array that is too long.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将对象放入等离子体时，我们必须给它一个ID（即命名它）。我们将使用以`csv-`开头的名称，后跟我们的进程的PID（进程ID）。这个名称对我们的目的来说已经足够好了，但您可能需要一个与其他名称字符串冲突可能性更小的名称作为更通用的解决方案。等离子体需要一个20字节的ID，因此我们用空格填充名称以达到20个字节的大小，然后使用US-ASCII编解码器对字符串进行编码，这将返回一个字节数组。只要它将一个字符转换为1字节，您可以使用任何编解码器，否则您将得到一个太长的字节数组。
- en: We use the object ID to denote and find our tables. There are other potentially
    more sophisticated techniques, such as using object metadata, but as long as we
    have a way to find our objects of interest, an object ID is enough for illustrative
    purposes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用对象ID来表示和查找我们的表格。还有其他可能更复杂的技巧，例如使用对象元数据，但只要我们有找到我们感兴趣对象的方法，对象ID就足以用于说明目的。
- en: Warning If you do not have enough memory available, Plasma will evict older
    objects.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果您没有足够的可用内存，等离子体将驱逐较旧的对象。
- en: 'Before we implement the two other processes, let’s create a support script
    to list all CSVs currently available in Plasma, which allows us to monitor what
    is in Plasma. We will also monitor for results, which we will name with the prefix
    `result-` (see `07-pandas/ sec5-arrow-plasma/list_csvs.py`):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现其他两个过程之前，让我们创建一个支持脚本以列出当前在等离子体中可用的所有CSV文件，这允许我们监控等离子体中的内容。我们还将监控结果，我们将使用前缀`result-`命名（见`07-pandas/sec5-arrow-plasma/list_csvs.py`）：
- en: '[PRE47]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ① We list all the objects.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们列出所有对象。
- en: ② As decoding of IDs might not yield valid strings, we need to catch exceptions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ② 由于ID的解码可能不会产生有效的字符串，我们需要捕获异常。
- en: After we get the list of all objects, which is returned as a dictionary, we
    look for IDs starting with either `csv-` or `result-`. Because not all IDs may
    actually convert to a string (i.e., other stuff may be being shared in the Plasma
    server), we are careful to catch all exceptions that cannot be decoded, and then
    we ignore them as they are not really errors.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获取所有对象的列表后，该列表作为字典返回，我们寻找以`csv-`或`result-`开头的ID。因为并非所有ID都可以实际转换为字符串（即，其他东西可能在等离子体服务器中共享），我们小心地捕获所有无法解码的异常，然后忽略它们，因为它们并不是真正的错误。
- en: 'For all relevant cases, we print the decoded ID, the original one and some
    metadata associated. Here is an example:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有相关情况，我们打印解码后的ID、原始ID和一些相关元数据。以下是一个示例：
- en: '[PRE48]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let’s implement our computation server. It will be in an eternal loop,
    looking for objects whose name starts with `csv-`. If one is found and a result
    doesn’t exist yet, then perform the computation and put the result in Plasma according
    to the object ID naming conversion starting with `result-` (see `07-pandas/sec5-arrow-plasma/compute_
    stats.py`):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现我们的计算服务器。它将处于永恒循环中，寻找以`csv-`开头的对象。如果找到一个对象并且结果尚不存在，那么执行计算并将结果根据以`result-`开头的对象ID命名转换放入等离子体中（见`07-pandas/sec5-arrow-plasma/compute_stats.py`）：
- en: '[PRE49]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ① We check whether a result already exists.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们检查结果是否已存在。
- en: ② We get the table from Plasma.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们从等离子体获取表格。
- en: ③ We put the result on Plasma.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们将结果放入等离子体。
- en: Most of our code has been presented in the current and previous sections. The
    only conceptual novelties are the use of the `contains` function to see whether
    a result already exists and the `get` function to get the table.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大部分代码已在当前和上一节中展示。唯一的概念创新是使用`contains`函数来查看结果是否已存在，以及使用`get`函数来获取表格。
- en: 'Finally, let’s see the results (see `07-pandas/sec5-arrow-plasma/show_results.py`):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看结果（见`07-pandas/sec5-arrow-plasma/show_results.py`）：
- en: '[PRE50]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The is really nothing new in this code, but note that we get the object in the
    last line without blocking by specifying a timeout of 0\. Plasma has blocking
    semantics by default, but you can specify a timeout if desired.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实际上并没有什么新东西，但请注意，我们在最后一行指定超时为0时不阻塞地获取对象。等离子体默认具有阻塞语义，但您可以根据需要指定超时。
- en: There is more to be said about Plasma (e.g., using a lower-level API to get
    and put objects or how to efficiently transfer pandas objects). However, because
    the development of the Arrow/Plasma architecture is still evolving, it is probably
    more important to get a grasp of the concepts for the IPC involved as we did here.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Plasma（例如，使用更底层的 API 获取和放置对象或如何有效地传输 pandas 对象）还有很多可以说的。然而，由于 Arrow/Plasma
    架构的开发仍在演进中，因此了解我们在这里所涉及到的 IPC 概念可能更为重要。
- en: We will revisit Arrow in the next chapter when we discuss data persistence.
    Arrow has a lot to offer on the storage front.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章讨论数据持久性时重新审视 Arrow。Arrow 在存储方面有很多可提供的内容。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: pandas is the most widely used data analysis library in the Python world, but
    it is not designed with efficiency, neither in terms of computational nor in-memory
    storage, as the first concern.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 是 Python 世界中最广泛使用的数据分析库，但它并不是在设计时优先考虑效率，无论是在计算效率还是内存存储效率方面。
- en: Very simple techniques for loading data can dramatically decrease the memory
    footprint of pandas data frames by, for example, ignoring some columns that are
    not needed for computation or informing pandas about the type of each column in
    advance.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据的非常简单技术可以通过例如忽略一些不需要用于计算的列或提前通知 pandas 每个列的类型来显著减少 pandas 数据框的内存占用。
- en: Judicious use of indexing can decrease processing times, although the flexibility
    of pandas indexing is somewhat limited.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聪明地使用索引可以减少处理时间，尽管 pandas 索引的灵活性有所限制。
- en: Different row iteration strategies can have differences of more than two orders
    of magnitude in performance. Avoid explicit loops whenever possible and use vectorization
    operations.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的行迭代策略在性能上可能会有两个数量级以上的差异。尽可能避免显式循环，并使用向量化操作。
- en: While pandas is built on top of NumPy, sometimes explicitly requiring that NumPy
    data structures are extracted from pandas can further increase performance speed.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 pandas 是建立在 NumPy 之上的，但有时明确要求从 pandas 中提取 NumPy 数据结构可以进一步提高性能速度。
- en: Cython can be used with pandas, albeit indirectly via NumPy data structures,
    and can substantially increase speed.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cython 可以与 pandas 一起使用，尽管是通过 NumPy 数据结构间接使用，并且可以显著提高速度。
- en: For very large data frames *and* complex formulas, NumExpr may be an efficient
    strategy to perform data analysis.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常大的数据框和复杂的公式，NumExpr 可能是进行数据分析的有效策略。
- en: Apache Arrow can serve many tasks. In this chapter, we focus on how it can complement
    pandas, especially as a fast reader of data. But be sure to check other functionalities
    of the project.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Arrow 可以执行许多任务。在本章中，我们重点关注它如何补充 pandas，特别是作为数据快速读取器。但请确保检查项目的其他功能。
- en: Arrow’s architecture, through its Plasma server, can be used for efficient data
    transfer across processes on the same machine. This can be quite useful to process
    data with different languages and frameworks, as the format is shared by all languages
    where it is implemented.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arrow 的架构，通过其 Plasma 服务器，可以在同一台机器上的进程之间进行高效的数据传输。这对于使用不同语言和框架处理数据非常有用，因为该格式在所有实现的语言中都是共享的。
- en: '* * *'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '¹  If you plan on running this example, consider using the subsampled data
    frames `df_10` and `df_100`: you will still get a feeling for the time required
    without waiting so long.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ¹  如果你打算运行这个示例，考虑使用子采样数据框 `df_10` 和 `df_100`：你仍然可以感受到所需时间，而无需等待那么久。
- en: '²  Of course, from a Python perspective, the difference is only conceptual:
    if you passed a series to the function, it would work as well, but with pandas
    objects. We are making an implementation point here, even if the language is more
    flexible.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ²  当然，从 Python 的角度来看，这种差异只是概念上的：如果你向函数传递了一个序列，它也会正常工作，但使用 pandas 对象。我们在这里做一个实现点，即使语言更加灵活。
- en: ³  If you also use R—its pairing with Python is quite common in the data science
    world—consider integrating both by using the rpy2 package, which will embed an
    R process in Python and provide elegant primitives for Python/R communication.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ³  如果你同时使用 R（在数据科学领域，Python 与 R 的搭配相当常见）——考虑通过使用 rpy2 包来整合两者，该包将在 Python 中嵌入
    R 进程，并为 Python/R 通信提供优雅的原语。
