- en: 6 Common design building blocks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6个常见的设计构建模块
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Adding new activation functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新的激活函数
- en: Inserting new layers to improve training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入新层以改善训练
- en: Skipping layers as a useful design pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过层作为有用的设计模式
- en: Combining new activations, layers, and skips into new approaches more powerful
    than the sum of their parts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新的激活、层和跳过组合成比它们各自总和更强大的新方法
- en: 'At this point, we have learned about the three most common and fundamental
    types of neural networks: fully connected, convolutional, and recurrent. We have
    improved all of these architectures by changing the optimizer and learning rate
    schedule, which alter how we update the parameters (weights) of our models, giving
    us more accurate models almost for free. All of the things we have learned thus
    far also have a long shelf life and have taught us about problems that have existed
    for decades (and continue). They give you a good foundation to speak the language
    of deep learning and some very fundamental building blocks that larger algorithms
    are made from.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了三种最常见和最基础的神经网络类型：全连接、卷积和循环。我们通过改变优化器和学习率计划来改进了所有这些架构，这改变了我们更新模型（权重）参数的方式，几乎免费地为我们提供了更精确的模型。我们迄今为止所学的一切也都有很长的保质期，并教会了我们关于几十年来（以及继续）存在的问题。它们为您提供了良好的基础，让您能够说一口流利的深度学习语言，以及一些构成更大算法的基本构建模块。
- en: Now that we have these better tools for training our models, we can learn about
    newer approaches to designing neural networks. Many of these newer methods *would
    not work* without the improvements to learning like `Adam` (discussed in the last
    chapter), which is why we are only talking about them now! Most of the ideas in
    this chapter are less than 10 years old and have a lot of utility—but are also
    still evolving. In another four years, some may be supplanted by newer options.
    But now that we have some shared vernacular to talk about the mechanics of deep
    learning, we can examine why the techniques in this chapter help build better
    models. Anything that works better will likely be tackling the same underlying
    problems, so the lessons should remain timeless.[¹](#fn12)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了更好的工具来训练我们的模型，我们可以学习关于设计神经网络的新方法。许多这些新方法*不会工作*，如果没有像`Adam`（在上一章中讨论过）这样的学习改进，这就是为什么我们现在才讨论它们！本章中的大多数想法都不到10年历史，但非常有用——但它们仍在不断发展。再过四年，一些可能被更新的选项所取代。但现在，既然我们有一些共享的术语来讨论深度学习的机制，我们可以检查为什么本章中的技术有助于构建更好的模型。任何表现更好的东西都可能解决相同的基本问题，因此这些经验教训应该是永恒的。[¹](#fn12)
- en: This chapter gives you some of the most widely used building blocks for model
    training in production today. For this reason, we spend time talking about *why*
    these new techniques work so that you can learn to recognize the logic and reasoning
    behind them and start to develop your intuition about how you might make your
    own changes. It is also important to understand these techniques because many
    new methods that are developed are variants of the ones we learn about in this
    chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了当前生产中用于模型训练的一些最广泛使用的构建模块。因此，我们花费时间讨论了*为什么*这些新技术能够有效，这样您可以学会识别它们背后的逻辑和推理，并开始培养自己对如何进行自我调整的直觉。了解这些技术同样重要，因为许多新开发的方法都是本章中我们所学内容的变体。
- en: We discuss five new methods in this chapter that work for feed-forward models
    and one new improvement to RNNs. We introduce the first five methods roughly in
    the order that they were invented, as each tends to use the preceding techniques
    in their design. Individually, they improve accuracy and speed up training; but
    combined, they are greater than the sum of their parts. The first method is a
    new activation function called a *rectified linear unit* (ReLU). Then we sandwich
    a new type of normalization layer between our linear/convolution layers and our
    nonlinear activation layers. After that, we learn about two types of design choices,
    *skip connections* and *1 × 1 convolution*, that reuse the layers we already know
    about. The same way vinegar is terrible on its own but amazing in a larger recipe,
    skip connections and 1 × 1 convolution are combined to create the fifth method,
    a *residual layer*. Residual layers provide one of the largest improvements in
    accuracy for CNNs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论了五种适用于前馈模型的新方法以及一种对RNN的改进。我们大致按照它们被发明的顺序介绍前五种方法，因为每个方法在设计时都倾向于使用先前的技术。单独来看，它们可以提高准确性和加快训练速度；但结合在一起，它们的整体效果大于各部分之和。第一种方法是一种新的激活函数，称为*线性整流单元*（ReLU）。然后我们在线性/卷积层和我们的非线性激活层之间夹入一种新的归一化层。之后，我们了解两种设计选择，*跳跃连接*和*1
    × 1卷积*，它们重复使用我们已知的层。就像醋本身很糟糕，但在更大的食谱中却很神奇一样，跳跃连接和1 × 1卷积结合在一起创造了第五种方法，即*残差层*。残差层为CNN提供了最大的准确度提升之一。
- en: We end by introducing the *long short-term memory* (LSTM) layer, which is a
    type of RNN. The original RNN we talked about is not widely used anymore because
    of how difficult it is to get working, but it is much easier to explain how RNNs
    work in general. LSTMs have been successfully used for over 20 years and are a
    great vehicle to explain how many of the lessons in this chapter can be recombined
    into a new kind of solution. We’ll also briefly mention a “diet” version of the
    LSTM that is a little less memory hungry, and we’ll use it to keep our models
    in an envelope that can safely run in Colab’s lower-end free GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了*长短期记忆*（LSTM）层，这是一种RNN。我们之前讨论的原始RNN现在不再广泛使用，因为它很难实现，但解释RNN的一般工作原理要容易得多。LSTMs已经成功使用了20多年，是解释本章中许多课程如何重新组合成一种新解决方案的绝佳途径。我们还将简要提及一种“节食”版本的LSTM，它对内存的需求略低，我们将使用它来确保我们的模型可以在Colab的低端免费GPU上安全运行。
- en: Getting set up with a baseline
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 设置基线
- en: 'The following code loads the Fashion-MNIST dataset again, since using it is
    hard enough for us to see improvements but easy enough that we don’t have to wait
    a long time for the results. We use this dataset to create a baseline—a model
    showing what we can accomplish with our current methods—that we can compare against
    to see the impact our new techniques have on accuracy:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码再次加载Fashion-MNIST数据集，因为我们使用它既足够困难以使我们能够看到改进，又足够简单，以至于我们不需要等待很长时间才能看到结果。我们使用这个数据集来创建一个基线——一个展示我们当前方法可以完成什么工作的模型——我们可以将其与我们的新技术对准确度的影响进行比较：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, we train fully connected and convolutional networks for almost
    every example, because the code for some of the techniques looks a little different
    for each type of network. You should compare how the code changes between the
    fully connected and CNN models to understand what parts are related to the fundamental
    idea and which are artifacts of the type of network we are implementing. This
    will help you to apply these techniques to other models in the future!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为几乎每个示例训练全连接和卷积网络，因为某些技术的代码对于每种类型的网络看起来都略有不同。你应该比较全连接和CNN模型之间的代码变化，以了解哪些部分与基本思想相关，哪些是我们在实现网络类型时的副作用。这将帮助你将来将这些技术应用到其他模型中！
- en: 'Let’s define some basic hyperparameters and details that we reuse throughout
    the chapter and the book. Here we have the code specifying our features, the number
    of hidden neurons for the fully connected layers, the number of channels and filters
    for the convolutional network, and the total number of classes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一些基本的超参数和细节，这些将在本章和整本书中重复使用。这里我们有指定我们特征的代码，全连接层的隐藏神经元数量，卷积网络的通道和滤波器数量，以及总的类别数：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ What are the width and height of our images?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们图像的宽度和高度是多少？
- en: '❷ How many values are in the input? We use this to help determine the size
    of subsequent layers: 28 * 28 images.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入中有多少个值？我们使用这个来确定后续层的大小：28 * 28图像。
- en: ❸ Hidden layer size
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 隐藏层大小
- en: ❹ How many channels are in the input?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 输入中有多少个通道？
- en: ❺ How many channels are in the input?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 输入中有多少个通道？
- en: ❻ How many classes are there?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 有多少个类别？
- en: A fully connected and convolutional baseline
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全连接和卷积基线
- en: 'Finally, we can define our models. We add a few more layers for each fully
    connected and CNN compared to what we used in previous chapters. As we introduce
    new approaches to building a network in this chapter, we’ll compare to these simpler
    starting models. First we define the fully connected network here. Thanks to the
    simplicity of fully connected networks, it is easy to use list unpacking with
    the `*` operator to make the same code work for almost any number of hidden layers.
    We can unpack a list comprehension `[define_block for _ in range(n)]` to create
    `n` layers of `define_block` layers:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以定义我们的模型。与之前章节相比，每个全连接和CNN模型我们都添加了几个额外的层。在本章中，我们将介绍构建网络的新方法，并将与这些更简单的起始模型进行比较。首先，我们在这里定义全连接网络。由于全连接网络的简单性，我们可以使用`*`运算符进行列表解包，使相同的代码适用于几乎任何数量的隐藏层。我们可以解包一个列表推导式`[define_block
    for _ in range(n)]`来创建`n`层的`define_block`层：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ First hidden layer
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个隐藏层
- en: ❷ Now that each remaining layer has the same input/output sizes, we can make
    them with a list unpacking.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在由于每个剩余层都有相同的输入/输出大小，我们可以使用列表解包来构建它们。
- en: 'We define our CNN layer next. We could write nicer code if we were making the
    model deeper, but the approaches we have learned so far aren’t good enough to
    learn a deep CNN. So we kept the CNN fairly shallow to make it easier to compare
    against. But once you learn about the techniques in this chapter, you’ll be able
    to make deeper networks that reliably converge to more accurate solutions! Here’s
    the code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的CNN层。如果我们正在构建一个更深的模型，我们可以写出更优雅的代码，但到目前为止我们学到的方法还不够好，无法学习到深层的CNN。因此，我们保持了CNN的相对简单，以便更容易进行比较。但一旦你了解了本章中的技术，你将能够构建更深层的网络，这些网络能够可靠地收敛到更精确的解！以下是代码：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use our new `train_network` function to train all of the models from this
    point forward. Remember that we modified this method to use the `AdamW` optimizer
    by default, so we don’t have to specify that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用新的`train_network`函数从这一点开始训练所有模型。记住，我们修改了此方法以默认使用`AdamW`优化器，因此我们不需要指定：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, a small change: we’ll use the `del` command to delete some of our
    neural networks once we are done with them in this chapter. If you are unlucky
    and get one of the lower-end GPUs from Colab, you might run out of memory running
    these examples. Let’s be explicit and tell Python we are finished so we can get
    back the GPU memory and avoid such annoyances:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个小改动：在本章完成我们对神经网络的处理后，我们将使用`del`命令删除一些神经网络。如果你不幸地从Colab获取了低端GPU，运行这些示例可能会耗尽内存。让我们明确地告诉Python我们已经完成，这样我们就可以回收GPU内存并避免这种烦恼：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now let’s look at the initial results, plotted with seaborn (this should be
    very familiar by this point). We are comparing the performance of our two initial
    models and the future enhancements we will add. That way, we can see that these
    improvements apply to more than just one type of network. Unsurprisingly, the
    CNN performs much better than the fully connected network. We expect this because
    we are working with images, and as we learned in chapter 3, convolutional layers
    are a powerful way of encoding the “structural prior” about pixels and their relationships
    into our network:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看初始结果，使用seaborn进行绘图（到这一点应该非常熟悉了）。我们正在比较我们两个初始模型和未来将要添加的改进。这样，我们可以看到这些改进不仅适用于一种类型的网络。不出所料，CNN的性能远优于全连接网络。我们预期这一点，因为我们正在处理图像，正如我们在第3章中学到的，卷积层是将关于像素及其关系的“结构先验”编码到我们网络中的强大方式：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/CH06_UN01_Raff.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN01_Raff.png)'
- en: With these results in hand, we can dive into the rest of this chapter! We spend
    each section talking about a new kind of `Module` you can use in your neural networks
    and the intuition or reason for using it, and then apply it to our baseline networks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些结果在手，我们可以深入本章的其余部分！我们每个部分都讨论你可以在神经网络中使用的新`Module`类型及其使用它的直觉或原因，然后将其应用于基线网络。
- en: 6.1 Better activation functions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 更好的激活函数
- en: We have been relying very heavily on the tanh (⋅) activation function and, to
    a smaller degree, the sigmoid function *σ*(⋅), throughout this book. They are
    two of the original activation functions used for neural networks, but they are
    not the only options. Currently, we as a community don’t know definitively what
    makes one activation function better than another, and there isn’t any one option
    you should always use. But we have learned about some things that are usually
    undesirable in an activation function. Both tanh (⋅) and *σ*(⋅) can lead to a
    problem called *vanishing gradients*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直非常依赖tanh(⋅)激活函数，以及在一定程度上依赖sigmoid函数σ(⋅)，贯穿整本书。它们是神经网络使用的原始激活函数中的两个，但并非只有这两种选择。目前，我们作为社区还没有确定性地知道什么使得一个激活函数比另一个更好，也没有一个选项你应该总是使用。但我们已经了解了一些通常在激活函数中不希望看到的事情。tanh(⋅)和σ(⋅)都可能导致一个称为“梯度消失”的问题。
- en: 6.1.1  Vanishing gradients
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 梯度消失
- en: Remember that every architecture we define learns by treating the network as
    one giant function *f*[Θ](**x**), where we need to use the gradient (∇) with respect
    to the parameters (Θ) of f to adjust its weights according to a loss function
    ℓ(⋅,⋅). So, we perform
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们定义的每一个架构都是通过将网络视为一个巨大的函数f[Θ](**x**)来学习的，其中我们需要使用相对于f的参数(Θ)的梯度(∇)来根据损失函数ℓ(⋅,⋅)调整其权重。所以我们执行
- en: '![](../Images/ch6-eqs-to-illustrator_A01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ch6-eqs-to-illustrator_A01.png)'
- en: 'But what if ∇[Θ]*f*[Θ](**x**) is very small? If that happens, there will be
    almost no change in the value of Θ, and thus no learning:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果∇[Θ]*f*[Θ](**x**)非常小怎么办？如果发生这种情况，Θ的值几乎不会改变，因此没有学习：
- en: '![](../Images/ch6-eqs-to-illustrator_A02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ch6-eqs-to-illustrator_A02.png)'
- en: '![](../Images/ch6-eqs-to-illustrator_A03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ch6-eqs-to-illustrator_A03.png)'
- en: While momentum (discussed in chapter 5) can help with this problem, it would
    be better if the gradients never vanished in the first place. This is because
    as the math shows us, if we get too close to zero, there is nothing to do.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然（在第5章中讨论的）动量可以帮助解决这个问题，但如果梯度一开始就没有消失那就更好了。这是因为正如数学所显示的，如果我们接近零，就无事可做了。
- en: 'How do the tanh and sigmoid activations result in this vanishing gradient problem?
    Let’s plot both of these functions again:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: tanh和sigmoid激活是如何导致这个梯度消失问题的？让我们再次绘制这两个函数：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/CH06_UN02_Raff.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH06_UN02_Raff.png)'
- en: Both activations have a property called *saturation*, which is when the activation
    stops changing as the input keeps changing. For both tanh (⋅) and *σ*(⋅), if the
    input x keeps getting larger, both activations saturate at the value 1.0. If the
    input to the activation is 100 and you double the input value, you still get a
    value of (almost) 1.0 from the output. That’s what saturation is. Both of these
    activations also saturate on the left-hand side of the plot, so when the input
    x becomes very small, tanh (⋅) will saturate at − 1.0 and *σ*(⋅) will saturate
    at 0.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个激活都有一个称为“饱和”的特性，即当输入继续变化时，激活停止变化。对于tanh(⋅)和σ(⋅)，如果输入x继续增大，这两个激活都会饱和在值1.0。如果激活函数的输入是100，你将输入值加倍，你仍然会从输出得到一个值（几乎是）1.0。这就是饱和的含义。这两个激活函数在图表的左侧也饱和，所以当输入x变得非常小的时候，tanh(⋅)将饱和在-1.0，而σ(⋅)将饱和在0。
- en: 'Let’s plot the derivatives of these functions—we see that saturation has an
    undesirable result:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些函数的导数——我们看到饱和度会产生不理想的结果：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/CH06_UN03_Raff.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH06_UN03_Raff.png)'
- en: Do you see the problem in the plot? As the activation *begins to saturate, its
    gradient begins to vanish*. This happens with *any* activation function that saturates.
    Since our weight changes based on the value of the gradient ∇, our network will
    stop learning if too many neurons begin saturating.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你在图中看到问题了吗？随着激活函数开始饱和，其梯度开始消失。这发生在任何饱和的激活函数上。由于我们的权重变化基于梯度∇的值，如果太多神经元开始饱和，我们的网络将停止学习。
- en: This does not mean that you should never use tanh (⋅) and *σ*(⋅); there are
    some cases where you want saturation (we’ll see an example at the end of this
    chapter with the LSTM). If you don’t have a specific reason why you *want* saturation,
    I recommend avoiding activation functions that saturate—which is the next thing
    we learn to do.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着你永远不应该使用tanh(⋅)和σ(⋅)；有些情况下你想要饱和（我们将在本章末尾看到一个LSTM的例子）。如果你没有特定的理由想要饱和，我建议避免使用饱和的激活函数——这是我们接下来要学习做的事情。
- en: Note Saturating activations are not the only cause of vanishing gradients. You
    can check whether your gradients are vanishing by looking at a histogram of them
    (using the `.grad` member variable). If you are using an activation function that
    can saturate, you can also plot a histogram of the activation functions to check
    whether that is the cause of vanishing gradients. For example, if you are using
    *σ*(⋅) as your activation function, and a histogram says 50% of your activations
    are within 0.01 of 1.0 or 0.0, you know saturation is the source of the problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意饱和激活不是梯度消失的唯一原因。你可以通过查看其直方图（使用 `.grad` 成员变量）来检查你的梯度是否消失。如果你使用可以饱和的激活函数，你也可以绘制激活函数的直方图来检查这是否是梯度消失的原因。例如，如果你使用
    *σ*(⋅) 作为你的激活函数，并且直方图显示 50% 的激活值在 0.01 与 1.0 或 0.0 之间，你就知道饱和是问题的根源。
- en: '6.1.2  Rectified linear units (ReLUs): Avoiding vanishing gradients'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2  修正线性单元 (ReLUs)：避免梯度消失
- en: 'Now we know that an activation function that saturates, by default, is probably
    not a great activation function to use. What should we use instead? The most common
    approach to fix this is to use an activation function known as the *rectified
    linear unit* (ReLU),[²](#fn13) which has a very simple definition:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，默认情况下饱和的激活函数可能不是一个很好的激活函数来使用。我们应该使用什么替代方案呢？最常见的方法是使用一种称为**ReLU（修正线性单元**）的激活函数，[²](#fn13)，它有一个非常简单的定义：
- en: '![](../Images/CH06_F00_EQ01.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F00_EQ01.png)'
- en: 'That’s all the ReLU does. If the input is positive, the return is unaltered.
    If the input is negative, the return is zero, instead. This may seem surprising,
    as we have harped on the importance of having a nonlinearity. But it turns out
    almost *any* nonlinearity is sufficient to learn from. Choosing a simple activation
    function like this also leads to a simple derivative:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 就做这么多。如果输入是正的，返回值不变。如果输入是负的，返回值是零。这可能会让人感到惊讶，因为我们一直强调非线性的重要性。但事实证明，几乎**任何**非线性都是足够的来学习。选择这样一个简单的激活函数也导致了一个简单的导数：
- en: '![](../Images/CH06_F00_EQ02.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F00_EQ02.png)'
- en: And that’s all it takes. For half of all possible inputs, the ReLU has a constant
    value as its derivative. For most use cases, simply replacing tanh (⋅) or *σ*(⋅)
    with the ReLU activation will allow your model to converge to a more accurate
    solution in fewer epochs. However, the ReLU often performs worse for very small
    networks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部。对于所有可能输入的一半，ReLU 的导数是一个常数。对于大多数用例，只需将 tanh (⋅) 或 *σ*(⋅) 替换为 ReLU 激活函数，就可以使你的模型在更少的迭代次数中收敛到更精确的解。然而，ReLU
    对于非常小的网络通常表现较差。
- en: 'Why? Instead of having a *vanishing* gradient, the ReLU has *no* gradient for
    *x* < = 0. If you have a lot of neurons, it is OK if some of them “die” and stop
    activating; but if you don’t have enough extra neurons, it becomes a serious problem.
    This can again be solved with a simple modification: instead of returning 0 for
    negative inputs, let’s return something else. This leads us to what is called
    the *leaky ReLU*.[³](#fn14) The leaky ReLU takes a “leaking” factor α, which is
    supposed to be small. Values in the range *α* ∈ [0.01,0.3] are often used, and
    the specific value has relatively little impact in most cases.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？ReLU 相比于有**消失**梯度的激活函数，对于 *x* <= 0 的输入没有梯度。如果你有很多神经元，一些神经元“死亡”并停止激活是可以接受的；但如果你没有足够的额外神经元，这就会成为一个严重的问题。这个问题可以通过一个简单的修改来解决：对于负输入，而不是返回
    0，让我们返回其他值。这引出了我们所说的**Leaky ReLU**。[³](#fn14) Leaky ReLU 取一个“泄漏”因子 α，它应该是小的。在
    *α* ∈ [0.01,0.3] 范围内的值通常被使用，并且在这种情况下，具体值的影响相对较小。
- en: 'What is the mathematical definition of the leaky ReLU? Again, it is a simple
    change that reduces negative values by a factor of α. This new activation and
    derivative can be succinctly defined as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 的数学定义是什么？同样，它是一个简单的变化，通过一个因子 α 减少了负值。这个新的激活和导数可以简洁地定义为以下：
- en: '![](../Images/CH06_F00_EQ03.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F00_EQ03.png)'
- en: Saying the same thing in code, we have
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用代码表达同样的意思，我们有
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Activation function converted to code
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将激活函数转换为代码
- en: ❷ Derivative of the activation function, where x is the original input to which
    the activation was applied
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 激活函数的导数，其中 x 是应用激活的原输入
- en: 'The intuition to an improved ReLU is that there is a hard “floor” when *x*
    < = 0. Because there is no change at this floor’s level, there is no gradient.
    Instead, we would like the floor to “leak” so that it changes—but slowly. If it
    changes even just a little, we can get a gradient. Let’s plot all of these activations
    and see what they look like:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 改进ReLU的直觉是，当*x* <= 0时，存在一个硬“地板”。因为在这个地板的水平上没有变化，所以没有梯度。相反，我们希望地板“泄漏”以便它发生变化——但变化缓慢。即使它只改变一点点，我们也可以得到一个梯度。让我们绘制所有这些激活函数，看看它们看起来像什么：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/CH06_UN04_Raff.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN04_Raff.png)'
- en: 'We can see that to the right, as the input gets larger, ReLU and LeakyReLU
    behave linearly, just increasing with the input. To the left, as the input gets
    smaller, both are *still* linear, but the ReLU is stuck at zero and the LeakyReLU
    is decreasing. The nonlinearity for both is simply changing the slope of the line,
    which is enough. Now let’s plot gradients with a little more code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当输入增大时，ReLU和LeakyReLU的行为呈线性，只是随着输入的增加而增加。当输入变小时，两者仍然是线性的，但ReLU被固定在零，而LeakyReLU在减小。两者的非线性仅仅是改变线的斜率，这已经足够了。现在让我们用更多的代码来绘制梯度：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/CH06_UN05_Raff.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN05_Raff.png)'
- en: So the LeakyReLU has gradient values that will *never* vanish on their own,
    but the interaction between layers could still cause an exploding or vanishing
    gradient. We’ll tackle that problem later in this chapter with residual layers
    and LSTMs, but the LeakyReLU at least does not contribute to the problem like
    the tanh (⋅) and *σ*(⋅) activations do.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LeakyReLU的梯度值永远不会自行消失，但层之间的交互仍然可能导致梯度爆炸或消失。我们将在本章的后续部分使用残差层和LSTMs来解决这个问题，但至少LeakyReLU不会像tanh(⋅)和*σ*(⋅)激活那样导致问题。
- en: 6.1.3  Training with LeakyReLU activations
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 使用LeakyReLU激活进行训练
- en: 'Now that we understand why we want to use the LeakyReLU, let’s check how good
    it is by testing training with it and see if accuracy improves. We’ll train new
    versions of our models with the LeakyReLU, which we usually find equals or outperforms
    the standard ReLU due to its slightly better behavior (lack of hard zeros in activation
    and gradient). First we define the leak rate to use. PyTorch uses a default of
    *α* = 0.01, a fairly conservative value that works just well enough to avoid a
    zero gradient for a normal ReLU. We use *α* = 0.1, which is my preferred default
    value, but this is not a critical choice:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了为什么想要使用LeakyReLU，让我们通过测试使用它进行训练并查看准确性是否有所提高来检查它的效果。我们将使用LeakyReLU训练我们模型的新版本，我们通常发现它等于或优于标准的ReLU，因为它的行为略好（激活和梯度中没有硬零）。首先，我们定义要使用的泄漏率。PyTorch使用默认值*α*
    = 0.01，这是一个相当保守的值，足以避免正常ReLU的零梯度。我们使用*α* = 0.1，这是我首选的默认值，但这不是一个关键的选择：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ How much we want LeakyReLU to leak. Anything in [0.01, 0.3] would be fine.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们希望LeakyReLU泄漏多少。在[0.01, 0.3]范围内的任何值都是可以的。
- en: 'Next we define a new version of both architectures, only changing the `nn.Tanh()`
    function to `nn.LeakyReLU`. First is the fully connected model, still just five
    lines of code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两种架构的新版本，只将`nn.Tanh()`函数更改为`nn.LeakyReLU`。首先是全连接模型，仍然只有五行代码：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The CNN model can be done the same way, but the function name is getting long
    to type and read. Let’s see another way to organize code. We define a `cnnLayer`
    function that takes in the input and output size of each layer and returns the
    combination of `Conv2d` and the activation function that makes a complete layer.
    This way, when we experiment with new ideas, we can just change this function,
    and the rest of our code will change with it; we don’t have to make as many edits.
    We can also add niceties like computing the padding size automatically, using
    common defaults like the kernel size, and keeping the output the same size as
    the input:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CNN模型也可以用同样的方式实现，但函数名太长，难以输入和阅读。让我们看看另一种组织代码的方法。我们定义一个`cnnLayer`函数，它接受每一层的输入和输出大小，并返回由`Conv2d`和激活函数组成的完整层。这样，当我们尝试新想法时，只需更改这个函数，其余的代码也会随之改变；我们不需要进行太多编辑。我们还可以添加一些便利的功能，比如自动计算填充大小，使用常见的默认值，如核大小，并保持输出大小与输入相同：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ This is a common pattern, so let’s automate it as a default if not asked.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个常见的模式，所以如果不需要，我们可以将其自动化为默认设置。
- en: ❷ Padding to stay the same size
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 填充以保持相同大小
- en: ❸ Combines the layer and activation into a single unit
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将层和激活合并为一个单一单元
- en: 'Now our CNN’s code is more concise and easier to read. The `cnnLayer` function
    also makes it easier to use list unpacking as we did with the fully connected
    model. The following is our generic CNN code block. Ignoring the object name,
    this code block can be re-purposed for many different styles of CNN hidden layers
    by changing the definition of the cnnLayer function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们CNN的代码更加简洁，更容易阅读。`cnnLayer`函数也使得像全连接模型那样使用列表解包变得更加容易。以下是我们通用的CNN代码块。忽略对象名称，这个代码块可以通过改变`cnnLayer`函数的定义，被重新用于许多不同风格的CNN隐藏层：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are ready to train both models. As always, PyTorch’s modular design means
    we don’t have to alter anything else:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好训练这两个模型。像往常一样，PyTorch模块化的设计意味着我们不需要更改其他任何东西：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s compare our new `relu_results` and our original `fc_results` and `cnn_results`.
    You should see that the LeakyReLU easily outperforms its tanh counterpart for
    both the CNN and fully connected networks. Not only is it more accurate, but it
    is also numerically nicer and easier to implement. The exp () function that is
    required to compute tanh requires a decent amount of compute, but ReLUs only have
    simple multiplication and max () operations, which are faster. Here’s the code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下我们的新`relu_results`和原始的`fc_results`以及`cnn_results`。你应该会发现LeakyReLU在CNN和全连接网络中都比其tanh对应物表现更好。它不仅更准确，而且在数值上更优美，实现起来也更简单。计算tanh所需的exp()函数需要相当的计算量，但ReLU只有简单的乘法和max()操作，这要快得多。下面是代码：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH06_UN06_Raff.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN06_Raff.png)'
- en: ReLU variants
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU变体
- en: So the ReLU family gives better accuracy, is a little faster, and is less code
    when implemented from scratch. For these reasons, the ReLU has quickly become
    the default favorite among many in the community; it is a good practical choice
    since it has been used successfully in most modern neural networks for several
    years now.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ReLU家族提供了更好的准确度，稍微快一点，而且从头开始实现时代码更少。出于这些原因，ReLU已经迅速成为社区中许多人的默认首选；由于它已经在大多数现代神经网络中成功使用了几年，因此它是一个很好的实用选择。
- en: There are *many* other flavors of the ReLU activation function, several of which
    are built into PyTorch. There is the `PReLU`, which attempts to learn what α should
    be for the LeakyReLU, removing it as a hyperparameter. `ReLU6` introduces an intentional
    saturation for the odd scenario where you want that kind of behavior. There are
    also “smooth” extensions to the ReLU like `CELU`, `GELU`, and `ELU`, which have
    been derived to have certain properties. Those are just the ones already in PyTorch;
    you can find even more variants and alternatives to the ReLU online ([http://mng.bz/VBeX](http://mng.bz/VBeX)).
    We don’t have time or space to go into all of these, but the improvement from
    tanh (⋅) to ReLUs is not as big as the difference from ReLU and its leaky variant
    to these other new flavors. If you want to learn more about these other activation
    functions, they are worth trying, as we have seen it can make a big difference.
    But you’ll generally be safe and in good company using any ReLU variants as your
    default choice.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数还有许多其他版本，其中一些已经内置到PyTorch中。有`PReLU`，它试图学习LeakyReLU中α应该是什么值，从而将其作为一个超参数去除。`ReLU6`引入了在你想有那种行为时的有意饱和。还有ReLU的“平滑”扩展，如`CELU`、`GELU`和`ELU`，它们被推导出来具有某些特性。这些只是PyTorch中已有的；你可以在网上找到更多ReLU的变体和替代方案([http://mng.bz/VBeX](http://mng.bz/VBeX))。我们没有时间或空间去探讨所有这些，但从tanh到ReLU的改进并不像ReLU及其泄漏变体到这些新风味之间的差异那么大。如果你想了解更多关于这些其他激活函数的信息，它们值得一试，因为我们已经看到它们可以带来很大的差异。但通常，使用任何ReLU变体作为默认选择都是安全且明智的。
- en: '6.2 Normalization layers: Magically better convergence'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 正则化层：神奇地更好的收敛
- en: 'To explain a *normalization layer* and how it works, let’s talk about how you
    might handle normalizing a normal dataset **X** = {**x**[1], **x**[2], …, **x**[n]}
    that has n rows and d features. Before you start feeding the matrix X into your
    favorite ML algorithm, you usually *normalize* or standardize the features in
    some way. This could be making sure the values are all within the range of [0,1],
    or subtracting the mean μ and dividing by the standard deviation σ.[⁴](#fn15)
    Standardizing by removing the mean and dividing by the standard deviation is something
    you have probably done before, given its ubiquity, but let’s write out the three
    steps (where ϵ is a tiny value like 10^(−15) to avoid division by zero if everything
    has the same value):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释**归一化层**及其工作原理，让我们谈谈如何处理一个具有n行和d个特征的正常数据集**X** = {**x**[1], **x**[2], …,
    **x**[n]}的归一化。在你开始将矩阵X输入你最喜欢的机器学习算法之前，你通常以某种方式对特征进行**归一化**或标准化。这可能确保所有值都在[0,1]的范围内，或者减去均值μ并除以标准差σ。[⁴](#fn15)
    通过减去均值并除以标准差进行标准化可能是你以前做过的事情，鉴于其普遍性，但让我们写出三个步骤（其中ϵ是一个很小的值，如10^(−15)，以避免所有值都相同时的除零错误）：
- en: '![](../Images/CH06_F00_EQ04.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F00_EQ04.png)'
- en: This results in the data X̂ having a mean of zero and a standard deviation of
    1\. We do this because most algorithms are sensitive to the *scale* of the input
    data. This scale sensitivity means if you multiply every feature in your dataset
    by 1,000, it changes what your models end up learning. By performing normalization
    or standardization, we ensure that our data is in a reasonable numeric range (–1
    to 1 is a good place to be), making it easier for our optimization algorithms
    to run.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得数据X̂具有均值为零和标准差为1。我们这样做是因为大多数算法对输入数据的**尺度**很敏感。这种尺度敏感性意味着如果你将你的数据集中的每个特征乘以1,000，它将改变你的模型最终学习到的内容。通过执行归一化或标准化，我们确保我们的数据在一个合理的数值范围内（-1到1是一个不错的选择），这使得我们的优化算法更容易运行。
- en: 6.2.1  Where do normalization layers go?
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 归一化层放在哪里？
- en: Before we train our neural networks, we again usually do normalization or standardization
    before passing the data into the first layer of our network. But what if we applied
    this normalization process before *every* layer of a neural network? Would this
    allow the network to learn even faster? If we include a few extra details, it
    turns out that the answer is yes! The organization of our new network with normalization
    layers is shown in figure 6.1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练神经网络之前，我们通常在将数据传递到网络的第一个层之前再次进行归一化或标准化。但如果我们把这种归一化过程应用到神经网络的每一层呢？这会让网络学习得更快吗？如果我们包含一些额外的细节，结果证明答案是肯定的！具有归一化层的新网络的组织结构如图6.1所示。
- en: '![](../Images/CH06_F01_Raff.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F01_Raff.png)'
- en: 'Figure 6.1 Three versions of a network with three hidden layers and one output
    layer: (a) the normal approach we have learned thus far; (b) and (c) two different
    ways to add a normalization layer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 三种具有三个隐藏层和一个输出层的网络版本：(a) 我们迄今为止所学的常规方法；(b) 和 (c) 添加归一化层的两种不同方式
- en: 'Let’s use **x**[l] to denote the input to the lth layer and, similarly, **μ**[l]
    and **σ**[l] for the mean and standard deviation of the lth layer’s inputs. Normalization
    layers are applied at every layer with one extra trick: let’s let the network
    *learn* how to scale the data instead of assuming that a mean of 0 and standard
    deviation of 1 are the best choices. This takes the form'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用**x**[l]表示第l层的输入，同样，用**μ**[l]和**σ**[l]表示第l层输入的均值和标准差。归一化层在每一层都应用了一个额外的技巧：让我们让网络**学习**如何缩放数据，而不是假设均值为0和标准差为1是最好的选择。这表现为
- en: '![](../Images/CH06_UN07_Raff.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN07_Raff.png)'
- en: 'The first term is the same kind we had before: the mean is removed from the
    data and divided by the standard deviation. The crucial additions here are γ,
    which lets the network change the scale of the data, and β, which lets the network
    shift the data left/right. Since the network controls γ and β, they are learned
    parameters (so γ and β are included in the set of all parameters Θ). It is common
    to initialize **γ** = ![](../Images/vec_1.png) and **β** = ![](../Images/vec_0.png)
    so that at the start, each layer is doing a simple standardization. As training
    progresses, gradient descent allows us to scale (alter γ) or shift (alter β) the
    result as desired.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项与我们之前所拥有的相同类型：从数据中移除均值并除以标准差。这里的关键补充是γ，它允许网络改变数据的尺度，以及β，它允许网络将数据左移/右移。由于网络控制γ和β，它们是学习参数（因此γ和β包含在所有参数Θ的集合中）。通常初始化**γ**
    = ![](../Images/vec_1.png) 和 **β** = ![](../Images/vec_0.png)，以便在开始时，每个层都在执行简单的标准化。随着训练的进行，梯度下降允许我们按需缩放（改变γ）或平移（改变β）结果。
- en: 'Normalization layers have been extraordinarily successful, and I often describe
    them as “magic pixie dust”: you sprinkle some normalization layers into your network,
    and suddenly it starts converging faster to more accurate solutions. Even networks
    that would not train at all suddenly start working. We’ll talk about two normalization
    layers that are the most widely used: *batch* and *layer*.[⁵](#fn16) Once we have
    discussed each type, we can explain when to pick one over the other (but you should
    almost always use some form of normalization layer). The only difference between
    the two approaches is what goes into computing the mean μ and standard deviation
    σ at each layer; they both use the same equations and follow the diagram in figure
    6.1.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化层取得了非凡的成功，我经常把它们描述为“魔法小精灵的粉末”：你把一些归一化层撒到你的网络中，突然它开始更快地收敛到更准确的解。甚至那些根本无法训练的网络突然开始工作。我们将讨论两种最广泛使用的归一化层：*批*和*层*。[⁵](#fn16)
    讨论完每种类型后，我们可以解释何时选择其中一种而不是另一种（但你几乎总是应该使用某种形式的归一化层）。这两种方法之间的唯一区别是计算每个层的均值μ和标准差σ的内容；它们都使用相同的方程式，并遵循图6.1中的图示。
- en: 6.2.2  Batch normalization
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 批归一化
- en: The first and most popular type of normalization layer is *batch normalization*
    (BN). BN is applied differently depending on the structure of the input data.
    If we are working with fully connected layers (PyTorch dimension (*B*,*D*)), we
    take the average and standard deviation of the feature values D over the B items
    in the batch. Hence, we *normalize* over the data features in a given *batch*.
    This means μ, σ, γ, and β have a shape of (*D*), and each item in the batch is
    normalized by the mean and standard deviation *of just that batch of data*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种也是最流行的归一化层类型是**批归一化**（BN）。根据输入数据的结构，BN的应用方式不同。如果我们正在处理全连接层（PyTorch维度(*B*,
    *D*)），我们将批中B个项目的特征值D的平均值和标准差取出来。因此，我们在给定的*批*中的数据特征上进行*归一化*。这意味着μ, σ, γ, 和 β的形状为(*D*)，批中的每个项目都通过该批数据的平均值和标准差进行归一化。
- en: 'To make this clear, let’s look at some hypothetical Python code that computes
    μ and σ based on a tensor with shape (*B*,*D*). We make the `for` loops explicit
    to make it clear. If you implement this for real use, you should try to use functions
    like `torch.sum` and `torch.mean` to make this run quickly:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更清晰，让我们看看一些基于形状为(*B*, *D*)的张量的假设Python代码，该代码计算μ和σ。我们明确地使用`for`循环来使其更清晰。如果你要真正实现这个功能，你应该尝试使用像`torch.sum`和`torch.mean`这样的函数来使这个过程运行得更快：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '❶ This BN example uses explicit loops: you wouldn’t write torch code like this
    for real! X is shape (B, D) for this example.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个BN示例使用了显式的循环：你不会为真实情况编写这样的torch代码！在这个例子中，X的形状是(B, D)。
- en: ❷ Goes over every item in the batch
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历批中的每个项目
- en: ❸ Averages the features
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 平均特征值
- en: ❹ Handles standard deviation the same way
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 以相同的方式处理标准差
- en: Because it averages over the batch dimension of the tensor, BN is sensitive
    to the batch size during training and is impossible to use with a batch size of
    1\. It also takes some cleverness to use at inference/prediction time because
    you don’t want your predictions to depend on the other data being available! To
    resolve this, most implementations keep a running estimate of the mean and standard
    deviation across all previously seen batches and use that single estimate for
    all predictions once training is done. PyTorch has already handled this for you.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BN在训练过程中对批次大小敏感，并且无法使用批次大小为1的情况，因此它需要一些巧妙的方法在推理/预测时间使用，因为你不希望你的预测依赖于其他数据的存在！为了解决这个问题，大多数实现都会保留所有先前看到的批次中均值和标准差的运行估计，并在训练完成后使用这个单一估计对所有预测进行操作。PyTorch已经为你处理了这个问题。
- en: 'What if we have one-dimensional data of shape (*B*,*C*,*D*)? In this case,
    we normalize the *channels* over the batch. This means μ, σ, γ, and β each have
    a shape of (*C*). This is because we want to treat each of the D values in a channel
    as having the same nature and structure, so the average is over the *B* × *D*
    values in the channel across all B batches. We then apply the same scale γ and
    shift β to all the values in each channel. What if we have 2D data of shape (*B*,*C*,*W*,*H*)?
    Similar to the 1D case, μ, σ, γ, and β have a shape of (*C*). For any z-dimensional
    structured data where we have channels, we always use BN over the channels. The
    following table summarizes which PyTorch module you should be looking for depending
    on the tensor shape:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一维数据，形状为(*B*,*C*,*D*)呢？在这种情况下，我们在批次上归一化*通道*。这意味着μ、σ、γ和β每个都具有(*C*)的形状。这是因为我们希望将每个通道中的D个值视为具有相同的性质和结构，因此平均是在所有B批次中通道的*B*
    × *D*个值上进行的。然后我们对每个通道中的所有值应用相同的缩放γ和偏移β。如果我们有形状为(*B*,*C*,*W*,*H*)的二维数据呢？与一维情况类似，μ、σ、γ和β的形状也是(*C*)。对于任何具有通道的z维结构化数据，我们始终在通道上使用BN。以下表格总结了根据张量形状，你应该寻找哪个PyTorch模块：
- en: '| Tensor shape | PyTorch module |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 张量形状 | PyTorch模块 |'
- en: '| (*B*,*D*) | `torch.nn.BatchNorm1d(D)` |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (*B*,*D*) | `torch.nn.BatchNorm1d(D)` |'
- en: '| (*B*,*C*,*D*) | `torch.nn.BatchNorm1d(C)` |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (*B*,*C*,*D*) | `torch.nn.BatchNorm1d(C)` |'
- en: '| (*B*,*C*,*W*,*H*) | `torch.nn.BatchNorm2d(C)` |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (*B*,*C*,*W*,*H*) | `torch.nn.BatchNorm2d(C)` |'
- en: '| (*B*,*C*,*W*,*H*,*D*) | `torch.nn.BatchNorm3d(C)` |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (*B*,*C*,*W*,*H*,*D*) | `torch.nn.BatchNorm3d(C)` |'
- en: 'If we were to apply BN to an input tensor X at inference time, it might look
    something like this in code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在推理时间对输入张量X应用BN，代码可能看起来像这样：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 6.2.3  Training with batch normalization
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 使用批归一化进行训练
- en: When we created our data loaders for Fashion-MNIST, we used a batch size of
    128, so we should have no issue applying BN to our architectures. Following the
    previous table, we add `BatchNorm1d` after each `nn.Linear` layer of our fully
    connected network—that is the only change we need to make! Let’s see what that
    looks like in the next code snippet. I put BN after the linear layer instead of
    before to match what most people do, so that when you read this in other code,
    it looks familiar:[⁶](#fn17)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为Fashion-MNIST创建数据加载器时，我们使用了128的批次大小，因此我们应该没有问题将BN应用于我们的架构。根据前面的表格，我们在全连接网络的每个`nn.Linear`层后添加`BatchNorm1d`——这就是我们需要做的唯一更改！让我们看看下一个代码片段中的样子。我将BN放在线性层之后而不是之前，以匹配大多数人做的，这样当你阅读其他代码时，它看起来会熟悉一些：[⁶](#fn17)
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Thanks to how we organized our CNNs, we can redefine the `cnnLayer` function
    in the next code block to change its behavior. All we have to do is add `nn.BatchNorm2d`
    after each `nn.Conv2d` layer of our CNN. Then we run the exact same code that
    defined `cnn_relu_model` but rename it as our `cnn_bn_model` this time:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们如何组织我们的CNN，我们可以在下一个代码块中重新定义`cnnLayer`函数以改变其行为。我们所有要做的就是在我们CNN的每个`nn.Conv2d`层之后添加`nn.BatchNorm2d`。然后我们运行定义`cnn_relu_model`的相同代码，但这次将其重命名为`cnn_bn_model`：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ This is a common pattern, so let’s automate it as a default if not asked.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个常见的模式，所以如果不特别要求，我们可以将其自动化为默认设置。
- en: ❶ Padding to stay the same size
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 填充以保持相同大小
- en: ❶ Combines the layer and activation into a single unit
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将层和激活合并为一个单元
- en: '❶ The only change: adding BatchNorm2d after our convolution!'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 唯一的变化：在卷积后添加BatchNorm2d！
- en: 'Next is our familiar code block to train this new batch norm based fully connected
    and CNN models, respectively:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们熟悉的代码块，用于分别训练基于批归一化的全连接和CNN模型：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The results of our new network are shown next, plotted against our previous
    best result from adding the ReLU activation. Again we see an improvement in accuracy
    across the board, *especially* for our CNN:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新网络的输出结果如下，与添加ReLU激活函数之前的最佳结果进行比较。我们再次看到在所有方面都提高了准确性，*特别是*对于我们的CNN：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH06_UN08_Raff.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN08_Raff.png)'
- en: Note Looking at the difference in improvement between the fully connected and
    CNN-based models, we might infer that the fully connected architecture is starting
    to reach the best it can do without being made larger or deeper. That’s not important
    right now, but I mention it as an example of using multiple models to infer hypotheses
    about your data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：观察完全连接模型和基于CNN的模型之间的改进差异，我们可能会推断出完全连接架构开始达到其最佳状态，而无需变得更大或更深。现在这并不重要，但我提到它作为使用多个模型来推断关于你的数据假设的例子。
- en: 'Why does BN work so well? The best intuition I can give you is the logic we
    walked through earlier: normalizing helps to ensure that our literal numeric values
    after each layer are in a generically “good” range; and through γ and β, the network
    can *decide* precisely where that range is. But getting to the root of why this
    works is an active research area in deep learning! Unfortunately, no one has a
    truly definitive answer yet.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么BN效果如此之好？我能给出的最佳直觉是我们之前走过的逻辑：归一化有助于确保我们每层之后的实际数值在一个普遍的“良好”范围内；通过γ和β，网络可以*决定*这个范围的确切位置。但找到这个工作原理的根本原因是深度学习中的一个活跃研究领域！不幸的是，还没有人有一个真正确定的答案。
- en: 6.2.4  Layer normalization
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 层归一化
- en: 'Another popular type of normalization approach is (confusingly) called *layer
    normalization* (LN), where we look at the average activation *over features* instead
    of over batches. This means every example in a batch gets its own μ and σ values
    but shares a γ and a β that are learned. Again, we can look at an example with
    explicit code to make this clear:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的归一化方法是（令人困惑地）称为*层归一化*（LN），其中我们查看特征的平均激活而不是批次的平均。这意味着批处理中的每个示例都有自己的μ和σ值，但共享一个学习到的γ和β。再次，我们可以通过一个带有明确代码的例子来查看这一点：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ So X is of shape (B, D) for this example.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所以在这个例子中，X的形状是(B, D)。
- en: ❷ Notice this has changed from X[i,:] before!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注意这已经从X[i,:]之前进行了更改！
- en: ❸ Again, changed from X[i,:]!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 再次，从X[i,:]进行了更改！
- en: The only difference between LN and BN is thus *what* we are averaging over!
    With LN, it does not matter how many examples are present in a batch B, so we
    can use LN when batches are smaller.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: LN和BN之间的唯一区别就是*我们平均的是什么*！在LN中，批处理B中存在的示例数量并不重要，因此我们可以使用LN时批处理更小。
- en: With that, we can dive into our repeated example of applying this new method
    to our same network architecture. Unlike BN, which has a different class in PyTorch
    for each kind of tensor shape, LN has one class for *all* architectures. There
    are some nuanced reasons that relate to certain kinds of problems where LN is
    preferred over BN and that require extra flexibility.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以深入到我们重复的例子，将这种方法应用到我们的相同网络架构中。与BN不同，BN在PyTorch中为每种张量形状有不同的类，而LN有一个类适用于*所有*架构。有一些细微的原因与某些问题相关，在这些问题中，LN比BN更受欢迎，并且需要额外的灵活性。
- en: 6.2.5  Training with layer normalization
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.5 使用层归一化进行训练
- en: 'The `nn.LayerNorm` class takes a single argument that is a *list* of integers.
    If you are working on fully connected layers with tensors of shape (*B*,*D*),
    you use `[D]` as the list, giving you `nn.LayerNorm([D])` as the layer construction.
    Here you can see the code for our fully connected network that uses LN. For fully
    connected layers, simply replace the BN with LN:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.LayerNorm`类接受一个单一参数，它是一个*整数列表*。如果你正在处理形状为(*B*,*D*)的完全连接层，你使用`[D]`作为列表，得到`nn.LayerNorm([D])`作为层构造。在这里，你可以看到我们使用LN的完全连接网络的代码。对于完全连接层，只需将BN替换为LN：'
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Why does LN take a list of integers? This list tells LN, from right to left,
    which values to average over. So if we have a 2D problem with tensors of shape
    (*B*,*C*,*W*,*H*), we give LN the last three dimensions as a list `[C, W, H]`.
    That covers *all the features*, which is what we want LN to normalize over. This
    makes LN a little trickier for CNNs because we also need to pay attention to how
    large the width and height are, and they change every time we apply max pooling.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么LN需要一个整数列表？这个列表告诉LN，从右到左，哪些值需要平均。所以如果我们有一个形状为(*B*,*C*,*W*,*H*)的张量2D问题，我们给LN最后一个三个维度作为列表`[C,
    W, H]`。这涵盖了*所有特征*，这是我们希望LN进行归一化的。这使得LN对于CNN来说稍微复杂一些，因为我们还需要注意宽度和高度的大小，并且每次应用最大池化时都会发生变化。
- en: Following is the new `cnnLayer` function that works around this. We add a `pool_factor`
    argument that keeps track of how many times pooling has been applied. After, is
    an LN object with a list `[out_filters, W//(2**pool_factor), H//(2**pool_factor)]`
    where we shrink with width and height based on how many times pooling has been
    applied.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是绕过这个问题的新的 `cnnLayer` 函数。我们添加了一个 `pool_factor` 参数，用于跟踪池化应用了多少次。之后是一个 LN 对象，其列表
    `[out_filters, W//(2**pool_factor), H//(2**pool_factor)]` 根据池化应用的次数调整宽度和高度。
- en: Note This is also *another* reason to use padding with our convolutional layers.
    By padding the convolution so that the output has the same width and height as
    the input, we simplify the things we need to keep track of. Right now, we need
    to divide by 2 for each round of pooling. The code would be *far* more complicated
    if we also had to keep track of how many times we did convolutions. That would
    also make it much harder to make changes to our network’s definition.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这也是使用填充与我们的卷积层一起使用的另一个原因。通过填充卷积，使得输出具有与输入相同的宽度和高度，我们简化了需要跟踪的事情。目前，我们需要在每一轮池化时除以2。如果我们还必须跟踪卷积进行了多少次，代码将会更加复杂。这也会使得对网络定义的更改变得更加困难。
- en: 'Here’s the code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ This is a common pattern, so let’s automate it as a default if not asked.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个常见的模式，所以如果不需要询问，我们就将其自动化为默认设置。
- en: ❷ Padding to stay the same size
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 填充以保持相同大小
- en: ❸ Combines the layer and activation into a single unit
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将层和激活合并为一个单一单元
- en: '❹ The only change: switching to LayerNorm after our convolution!'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 唯一的改变：在卷积后切换到 LayerNorm！
- en: 'Now that we have our new `cnnLayer` function, we can create a `cnn_ln_model`
    that uses LN. The following code shows its creation, since we have to add the
    `pool_factor` argument after we’ve performed pooling:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了新的 `cnnLayer` 函数，我们可以创建一个使用 LN 的 `cnn_ln_model`。以下代码展示了它的创建，因为我们必须在执行池化后添加
    `pool_factor` 参数：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ We’ve done one round of pooling, so pool_factor=1.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们已经进行了一轮池化，所以池化因子=1。
- en: ❷ Now we’ve done two rounds of pooling, so pool_factor=2.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在我们已经进行了两轮池化，所以池化因子=2。
- en: 'It’s a little more work, but nothing too painful. We can now train these two
    new models:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要做一点额外的工作，但并不痛苦。我们现在可以训练这两个新的模型：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s plot the results with LN, BN, and the ReLU-based models that have no
    normalization layers. The magic pixie dust powers don’t appear to be as strong
    in LN. For the CNN, LN is an improvement over *no* normalization but worse than
    BN. For the fully connected layers, LN appears to be more in line with the non-normalized
    variant:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 LN、BN 和没有归一化层的基于 ReLU 的模型来绘制结果。LN 的神奇魔力似乎并不那么强烈。对于 CNN，LN 相比没有归一化的模型是一个改进，但不如
    BN。对于全连接层，LN 似乎与非归一化的变体更一致：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/CH06_UN09_Raff.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN09_Raff.png)'
- en: At first blush, LN does not look as helpful. Its code is clunkier to include,
    and it does not perform quite as well in terms of the model’s accuracy. The clunkier
    code does have a purpose, though. BN is really only useful for fully connected
    layers and convolutional models, so PyTorch can easily hardcode it to the two
    tasks. LN can be helpful for almost any architecture (e.g., RNNs), and the list
    of integers telling LN explicitly what to normalize over allows us to use the
    same `Module` for these varieties of use cases.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，LN 并不显得那么有帮助。它的代码更难以包含，并且在模型准确度方面表现并不完全出色。不过，更复杂的代码确实有其目的。BN 对于全连接层和卷积模型非常有用，因此
    PyTorch 可以轻松地将它硬编码到这两个任务中。LN 几乎对任何架构（例如 RNN）都有帮助，而明确告诉 LN 要归一化哪些整数的列表允许我们使用相同的
    `Module` 来处理这些不同用例。
- en: 6.2.6  Which normalization layer to use?
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.6 使用哪个归一化层？
- en: While normalization layers have been around for a few years, fully understanding
    them and when to use them is an active research problem. For non-recurrent networks,
    using BN is a good idea *provided* you can train on batches *B* ≥ 64. If your
    batch isn’t big enough, you won’t get a good estimate of μ and σ, and your results
    could suffer. If your problem fits this situation of larger batches, BN will *usually*
    improve your results, but there are instances where it does not. So if you have
    difficulty getting your model to train, it’s worth testing a version of your model
    without BN to see if you are in the odd situation where BN *hurts* instead of
    helping. Fortunately these situations are rare, so I still default to including
    BN if I have CNNs and large batches.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管归一化层已经存在了几年，但完全理解它们以及何时使用它们是一个活跃的研究问题。对于非循环网络，如果你可以在批处理大小 *B* ≥ 64 上进行训练，使用
    BN 是一个好主意。如果你的批处理大小不够大，你将无法得到 μ 和 σ 的良好估计，你的结果可能会受到影响。如果你的问题适合这种大批量的情况，BN 通常会*通常*提高你的结果，但也有一些情况它不起作用。所以如果你发现你的模型难以训练，尝试测试没有
    BN 的模型版本，看看你是否处于 BN 反而会*伤害*而不是帮助的奇怪情况。幸运的是，这些情况很少见，所以如果我有 CNN 和大型批处理，我仍然默认包括 BN。
- en: 'LN has been particularly popular for recurrent architectures and is worth adding
    to your RNN. LN should be your first pick whenever you reuse a subnetwork with
    weight sharing: BN’s statistics assume *one* distribution, and when you do weight
    sharing, you get *multiple* distributions, which can cause problems. LN can also
    be effective when you have small batch sizes *B* ≤ 32, although it does not usually
    provide the same level of improvement as BN for CNNs and fully connected networks.
    This can be a factor if it looks like enlarging your network will improve performance
    but you can’t make it any larger without having to shrink the batch size due to
    a lack of RAM.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: LN 对于循环架构特别受欢迎，值得添加到你的 RNN 中。每次你重用具有权重共享的子网络时，LN 应该是你的首选：BN 的统计假设*一个*分布，而在你进行权重共享时，你会得到*多个*分布，这可能会引起问题。LN
    在批处理大小 *B* ≤ 32 的情况下也可能会有效，尽管它通常不会像 BN 那样为 CNN 和全连接网络提供相同级别的改进。这可能是一个因素，如果你认为扩大你的网络将提高性能，但你不能不缩小批处理大小（由于内存不足）来扩大网络。
- en: While BN and LN are the two most popular and widely used normalization layers,
    they are far from the only ones being developed and used. It’s worth keeping your
    eye out for new approaches to normalization layers; using them is often an easy
    change to make to your code.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 BN（批量归一化）和 LN（层归一化）是最流行和最广泛使用的归一化层，但它们远非唯一正在开发和使用的。值得留意新的归一化层方法；使用它们通常只需对代码进行简单的修改。
- en: 6.2.7  A peculiarity of layer normalization
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.7 层归一化的一个特性
- en: Given how important LN has become, I’d like to share a deeper insight about
    what makes normalization layers particularly unusual and sometimes confusing to
    think about. Let’s talk about the *strength* or, more technically, the *capacity*
    of a network.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LN（层归一化）已经变得非常重要，我想分享一些关于使归一化层特别不同寻常且有时令人困惑的深入见解。让我们来谈谈网络的*强度*，或者更技术性地，*容量*。
- en: I’m using these words a little loosely to refer to how complex a problem or
    neural network is.[⁷](#fn18) A good mental model is to think of complexity as
    random-looking or erratic; some examples are shown in figure 6.2\. The more complex
    a function is, the more complex or powerful our neural network must be to be able
    to approximate it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里稍微宽松地使用这些词来指代一个问题的复杂度或神经网络复杂度。[⁷](#fn18) 一个好的心理模型是将复杂度想象成看起来随机或无规律的；一些例子在图6.2中展示。一个函数越复杂，我们的神经网络必须越复杂或越强大才能近似它。
- en: '![](../Images/CH06_F02_Raff.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F02_Raff.png)'
- en: Figure 6.2 We can think of the complexity of a function as how squiggly and
    erratic it looks. This diagram shows a very low-complexity function on the left
    (linear); complexity increases as you move to the right.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 我们可以将函数的复杂度想象成它看起来有多扭曲和无规律。这张图显示了左侧一个非常低复杂度的函数（线性）；随着向右移动，复杂度增加。
- en: 'Things get interesting when we talk about the model, because two different
    factors are interacting:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论模型时，事情变得有趣，因为有两个不同的因素在相互作用：
- en: What does the model technically have the capacity to represent (i.e., if an
    oracle could tell you the perfect parameters for a given model)?
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在技术上能够表示什么（即，如果有一个先知能告诉你给定模型的完美参数）？
- en: What can our optimization process *actually learn*?
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的最优化过程*实际上*能学到什么？
- en: We saw in chapter 5 that our optimization approach is not perfect, so it is
    always the case that what a model can learn must be less than or equal to what
    it can represent. Whenever we add an additional layer or increase the number of
    neurons in a layer, we increase the network’s capacity.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们看到了我们的优化方法并不完美，所以一个模型能够学习的内容必须小于或等于它能够表示的内容。每次我们添加一个额外的层或在层中增加神经元的数量，我们都会增加网络的容量。
- en: 'We can describe underfitting the data as having a model with less capacity
    than the complexity of the problem we are trying to solve. It’s like trying to
    use a microwave to cater a wedding: you don’t have the capacity to meet the needs
    of the situation. This, along with two more important situations, is shown in
    figure 6.3\. The first situation could cause overfitting, but that does mean we
    can solve the problem. This is nice because we have tools to tackle it (e.g.,
    making the model smaller or adding regularization). We can easily check if we
    are in the second situation by giving all the data points random labels: if the
    training loss goes down, the model has enough capacity to memorize the entire
    dataset.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将欠拟合数据描述为具有比我们试图解决的问题的复杂性更少的模型。这就像试图用微波炉举办婚礼：你无法满足这种情况的需求。这，加上另外两种重要的情况，如图6.3所示。第一种情况可能导致过拟合，但这并不意味着我们可以解决这个问题。这是好事，因为我们有工具来处理它（例如，使模型更小或添加正则化）。我们可以通过给所有数据点随机标签来轻松检查我们是否处于第二种情况：如果训练损失下降，则模型有足够的容量来记住整个数据集。
- en: '![](../Images/CH06_F03_Raff.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_Raff.png)'
- en: Figure 6.3 Three common types of situations that occur when specifying a neural
    network. Focus on the relative comparisons of complexity, not the exact type shown.
    The first situation is common when our model is too small. The second and third
    are both common when we make our model larger, but the third is much harder to
    reason about. Normalization layers help move us from the bad third to the better
    second situation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3展示了在指定神经网络时可能出现的三种常见情况。关注复杂性的相对比较，而不是显示的确切类型。第一种情况在我们模型太小的时候很常见。第二种和第三种情况在我们使模型更大时都常见，但第三种情况更难以推理。归一化层帮助我们从糟糕的第三种情况转移到更好的第二种情况。
- en: The third situation in figure 6.3 is the most nefarious, and we do not have
    any good ways to check if that’s the problem we are experiencing. Usually it’s
    discovered only when something better is invented (e.g., going from SGD to SGD
    with momentum). Normalization layers are unique in that they *do not increase
    capacity*. Said another way, their representational capacity does not change,
    but what they can learn improves.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3中的第三种情况是最为恶劣的，我们没有任何好的方法来检查这是否是我们遇到的问题。通常只有在发明了更好的方法（例如，从SGD到带有动量的SGD）时才会被发现。归一化层是独一无二的，因为它们**不会增加容量**。换句话说，它们的表示能力不会改变，但它们可以学习的内容却得到了提升。
- en: Normalization layers are fairly unique in this regard, and we as a community
    are still trying to figure out *why* they work so well. Because they do not increase
    capacity, many practitioners and researchers are vexed by this behavior. It would
    be nicer if we did not have to include these normalization layers since they do
    not impact capacity, but currently, they are too valuable to live without.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，归一化层相当独特，我们作为社区仍在努力弄清楚**为什么**它们工作得如此之好。因为它们不会增加容量，许多实践者和研究人员都对此行为感到困扰。如果我们可以不包含这些归一化层，那会更好，因为它们不会影响容量，但到目前为止，它们的价值太高，无法放弃。
- en: Proof that batch norm does not add representational power
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化不会增加表示能力的证明
- en: It may seem odd that I say that normalization layers do not add any representational
    power to the network. We’ve added a new kind of layer, and adding more layers
    generally makes a network capable of representing more complex functions. So why
    are normalization layers different?
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我说归一化层不会给网络增加任何表示能力，这听起来可能有些奇怪。我们添加了一种新的层，而增加更多的层通常会使网络能够表示更复杂的函数。那么，归一化层为什么不同呢？
- en: We can answer that with a little algebra! Remember that we said normalization
    layers take the form of
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一点代数来回答这个问题！记住，我们说过归一化层的格式是
- en: '![](../Images/CH06_F03_EQ01.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ01.png)'
- en: but that x value is the result of a linear layer. So we can rewrite this as
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，那个x值是线性层的输出。因此，我们可以将这个表达式重写为
- en: '![](../Images/CH06_F03_EQ02.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ02.png)'
- en: 'Using algebra, we can take the following steps to simplify this. First, the
    order of operations in the numerator doesn’t change with or without the parentheses,
    so let’s remove them:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代数，我们可以采取以下步骤来简化这一点。首先，分子中操作顺序的顺序在有或没有括号的情况下都不会改变，所以让我们去掉它们：
- en: '![](../Images/CH06_F03_EQ03.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ03.png)'
- en: 'Now let’s move γ to the left and apply it to the terms in the numerator. We
    group the two shifts of the bias term b and the mean μ:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 γ 移到左边，并应用于分子中的项。我们将偏置项 b 和均值 μ 的两个平移分组：
- en: '![](../Images/CH06_F03_EQ04.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ04.png)'
- en: 'Next we apply the division by σ to each term independently:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将 σ 的除法独立应用于每个项：
- en: '![](../Images/CH06_F03_EQ05.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ05.png)'
- en: 'The left-most term involves the vector matrix product of **x**^⊤*W*, so we
    can move all the element-wise operations with γ and σ onto just W and make x something
    that happens afterward (the result is the same):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最左侧的项涉及向量矩阵乘积 **x**^⊤*W*，因此我们可以将所有与 γ 和 σ 相关的逐元素操作移动到 W 上，并将 x 视为之后发生的事情（结果相同）：
- en: '![](../Images/CH06_F03_EQ06.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ06.png)'
- en: 'Do you see the answer in sight? We have the same situation we talked about
    in chapters 2 and 3\. Normalization is a *linear operation*, and *any consecutive
    sequence of linear operations is equivalent to one linear operation*! The sequence
    of linear layers followed by BN is *equivalent* to a different, single `nn.Linear`
    layer with weights W̃ + and biases *b̃*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到答案了吗？我们遇到了与第 2 章和第 3 章中讨论的相同情况。归一化是一个 *线性操作*，*任何连续的线性操作序列都等价于一个线性操作*！跟随 BN
    的线性层序列 *等价* 于一个不同的、单一的 `nn.Linear` 层，其权重为 W̃ + 和偏置 *b̃*：
- en: '![](../Images/CH06_F03_EQ07.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_EQ07.png)'
- en: If you use convolutional layers, you get the same kind of result. This has caused
    some deep learning researchers and even practitioners consternation because BN
    is so effective, but its utility is so enormous it’s hard to turn down.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用卷积层，你会得到相同的结果。这导致一些深度学习研究人员甚至实践者感到困惑，因为批量归一化（BN）非常有效，但其效用巨大，难以拒绝。
- en: '6.3 Skip connections: A network design pattern'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 跳跃连接：一种网络设计模式
- en: Now that we have learned about some new `Module`s that improve our networks,
    let’s turn to learning about new *designs* that we can incorporate into our networks.
    The first is called a *skip connection*. With a normal feed-forward network, an
    output from one layer goes directly to the next layer. With skip connections,
    this is still true, but we also “skip” the next layer and connect to a preceding
    layer as well. There are *many* ways to do this, and figure 6.4 shows a few options.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些新的 `Module`，它们可以改进我们的网络，让我们转向了解可以整合到我们网络中的新 *设计*。第一个被称为 *跳跃连接*。在正常的正向传播网络中，一个层的输出直接传递到下一层。有了跳跃连接，这一点仍然成立，但我们还会“跳过”下一层，并将连接到前一层。有
    *许多* 种方法可以做到这一点，图 6.4 展示了几个选项。
- en: '![](../Images/CH06_F04_Raff.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Raff.png)'
- en: Figure 6.4 The left-most diagram shows a normal feed-forward design. The right
    two diagrams show two different ways to implement skip connections. The black
    dots on the connections indicate concatenation of the outputs from all of the
    inputs. Only linear layers are shown, for simplicity, but the networks would also
    contain normalization and activation layers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 最左侧的图显示了一个正常的正向传播设计。右侧的两个图显示了实现跳跃连接的两种不同方法。连接上的黑色点表示所有输入输出结果的连接。为了简单起见，只显示了线性层，但网络还会包含归一化和激活层。
- en: 'The left image is a normal network like we have been using. The second shows
    one strategy of having every other layer “skip” to the next one. When this happens,
    the number of inputs to every second layer increases based on the size of the
    outputs from the two preceding layers’ inputs. The idea is that every solid dot
    in the diagram indicates concatenation of outputs. So if x and h connect in the
    diagram, they input to the next layer: [**x**,**h**]. In code, this would be something
    like `torch.cat([x, h], dim=1)`. That way, the two inputs x and h have shapes
    (*B*,*D*) and (*B*,*H*). We want to stack the features so the result will have
    shape (*B*,*D*+*H*). The third example in the figure shows multiple inputs skipping
    to one specific layer, giving it three times as many inputs. This causes the final
    layer’s input size to triple based on the three layers coming into it, all having
    an output size of h.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的图像是我们一直在使用的正常网络。第二幅图显示了每个其他层“跳过”到下一层的策略。当这种情况发生时，每第二个层的输入数量会根据前两层输入的输出大小增加。这个想法是，图中的每个实心点都表示输出的连接。所以如果x和h在图中连接，它们将输入到下一层：[**x**，**h**]。在代码中，这将是类似于
    `torch.cat([x, h], dim=1)` 的东西。这样，两个输入x和h的形状是(*B*，*D*)和(*B*，*H*)。我们想要堆叠特征，以便结果将具有(*B*，*D*+*H*)的形状。图中的第三个示例显示了多个输入跳过到特定层，给它提供了三倍多的输入。这导致最终层的输入大小根据进入它的三个层的三倍增加，所有层的输出大小都是h。
- en: Why use skip connections? Part of the intuition is that skip connections can
    make the optimization process easier. Said another way, skip connections can shrink
    the gap between a network’s capacity (what it could represent) and what it can
    learn (what it learns to represent). Figure 6.5 highlights this fact. Consider
    the normal network example on the left. The gradient contains the information
    we need to learn and adjust every parameter. On the left side, the first hidden
    layer requires waiting for three other layers to process and pass along a gradient
    before obtaining any information. But every step is also an opportunity for noise,
    and if we had too many layers, learning would become ineffective. The network
    on the right will, for very deep networks, reduce the path of the gradient by
    half.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用跳过连接？部分直觉是跳过连接可以使优化过程更容易。换句话说，跳过连接可以缩小网络容量（它能表示什么）和它能学习的内容（它学习表示的内容）之间的差距。图6.5突出了这一点。考虑左边的正常网络示例。梯度包含了我们需要学习和调整每个参数所需的信息。在左边，第一隐藏层需要等待三个其他层处理并传递梯度，才能获得任何信息。但每一步也是噪声的机会，如果我们有太多的层，学习就会变得无效。右边的网络对于非常深的网络，将梯度的路径减少了一半。
- en: '![](../Images/CH06_F05_Raff.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F05_Raff.png)'
- en: Figure 6.5 Every operation done makes the network more complex, but also makes
    the gradient more complex, creating a tradeoff between depth (capacity) and learnability
    (optimization). Skip connections create a short path with fewer operations, and
    can make the gradient easier to learn. Only linear layers are shown for simplicity,
    but would also contain normalization and activation layers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 每个操作都会使网络更复杂，但也会使梯度更复杂，在深度（容量）和可学习性（优化）之间产生权衡。跳过连接创建了一条操作更少的短路径，可以使梯度更容易学习。为了简单起见，只显示了线性层，但也会包含归一化和激活层。
- en: 'A more extreme option is to connect every hidden layer to the output layer
    with a skip connection. This is shown in figure 6.6\. Every hidden layer gets
    some amount of information about the gradient directly from the output layer,
    giving more direct access to the gradient, and the gradient is also processed
    from the longer path going layer by layer. This more direct feedback can make
    learning easier. It can also have benefits for certain applications that require
    high- and low-level details. Imagine you are trying to tell the difference between
    mammals: high-level details like shape can easily tell apart a whale and dog,
    but low-level details like the style of fur are important for differentiating
    between different dog breeds.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更极端的选项是将每个隐藏层通过跳过连接直接连接到输出层。这如图6.6所示。每个隐藏层都能直接从输出层获得一些关于梯度的信息，从而提供了对梯度的更直接访问，同时梯度也是从更长路径逐层处理。这种更直接的反馈可以使学习更容易。它还可以为需要高、低级细节的某些应用带来好处。想象一下，你正在尝试区分哺乳动物：高级细节，如形状，可以很容易地区分鲸鱼和狗，但低级细节，如毛发的风格，对于区分不同的狗品种很重要。
- en: '![](../Images/CH06_F06_Raff.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F06_Raff.png)'
- en: Figure 6.6 Creating dense skip connections where everything connects to the
    output layer significantly reduces the length of the path from the output gradient
    back to each layer. It can also be helpful when different layers learn different
    types of features. Only linear layers are shown, for simplicity, but a real network
    would also contain normalization and activation layers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 创建密集跳转连接，其中所有内容都连接到输出层，这显著缩短了从输出梯度回到每一层的路径长度。当不同层学习不同类型的特征时，这也可能很有帮助。为了简单起见，只显示了线性层，但实际网络还会包含归一化和激活层。
- en: A noisy definition of noise
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声的噪声定义
- en: Here, and many times throughout the book, we use a very broad definition of
    *noise*. Noise could be literal noise added to the network, or it could be vanishing
    or exploding gradients, but often it is just means *hard to use*. The gradient
    is not magic; it is a greedy process looking for the best current choice.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以及本书中的许多地方，我们使用一个非常宽泛的噪声定义。噪声可以是添加到网络中的实际噪声，也可以是消失或爆炸的梯度，但通常它只是意味着*难以使用*。梯度不是魔法；它是一个寻找最佳当前选择的贪婪过程。
- en: Imagine playing a game of telephone, trying to communicate a message from one
    person to the next. Each person in the telephone progression greedily listens
    to and passes along the message to the next person. But when the message makes
    its way back to the start of the telephone chain, it is often a garbled version
    of what was originally said. The more people in the telephone line, the more likely
    that errors happen in the message. If each person was a hidden layer in the network,
    we would have the same issue with our training! The gradient is the message, and
    each hidden layer alters the message as it tries to pass it back. If it is too
    deep, the message becomes too distorted by the telephone game to be useful.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下玩一个电话游戏，试图将一条信息从一个人传递到另一个人。在电话传递过程中，每个人贪婪地倾听并传递信息给下一个人。但是当信息回到电话链的起点时，它通常是被严重扭曲的原话版本。电话线中的人越多，信息中发生错误的可能性就越大。如果每个人都是网络中的一个隐藏层，那么我们在训练中就会遇到同样的问题！梯度是信息，每个隐藏层在尝试将其传递回去时都会改变信息。如果太深，信息就会被电话游戏扭曲得太厉害，以至于没有用了。
- en: However, having every layer connect directly to the output can be overkill and
    eventually make the learning problem more difficult. Imagine if you had 100 hidden
    layers, all connected directly to the output layer—that would be a *huge* input
    for the output layer to deal with. That said, this approach has been used successfully[⁸](#fn19)
    by organizing “blocks” of dense skip connections.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让每一层都直接连接到输出可能会过度设计，并最终使学习问题更加困难。想象一下，如果你有100个隐藏层，它们都直接连接到输出层——这将是一个*巨大*的输入，输出层需要处理。尽管如此，这种方法已经通过组织“块”密集跳转连接而成功使用[⁸](#fn19)。
- en: 6.3.1  Implementing fully connected skips
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 实现全连接跳过
- en: Now that we’ve talked about skip connections, let’s implement one for a fully
    connected network. This example shows how to create the second style of skip connection
    from figure 6.6, where a large number of layers skip to one final layer. Then
    we can reuse this layer multiple times to create a hybrid strategy of shortcuts
    every other layer to implement the first style from figure 6.5.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了跳转连接，那么让我们为全连接网络实现一个。这个例子展示了如何创建图6.6中所示的第二种跳转连接风格，其中大量层跳转到最终一层。然后我们可以多次重用这个层，以创建一种混合策略，每隔一层就实现图6.5中的第一种风格。
- en: Figure 6.7 shows show this works. Each set of skip connections makes one block
    defined by a `SkipFC` `Module`. By stacking multiple blocks, we re-create the
    style of network used in figure 6.6\. Figure 6.5 could be re-created using a single
    `SkipFC(6,D, N)`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7展示了这是如何工作的。每一组跳转连接定义了一个由`SkipFC` `Module`定义的块。通过堆叠多个块，我们重新创建了图6.6中使用的网络风格。图6.5可以使用单个`SkipFC(6,D,N)`重新创建。
- en: '![](../Images/CH06_F07_Raff.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F07_Raff.png)'
- en: Figure 6.7 The architecture we define for our fully connected network with skip
    connections. Each dashed block shows a block of dense connections and is created
    using one `SkipFC` object that we define.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 我们为具有跳转连接的全连接网络定义的架构。每个虚线块表示一个密集连接块，并使用我们定义的一个`SkipFC`对象创建。
- en: To do this, we need to store a list of hidden layers. In PyTorch, this should
    be done using the `ModuleList` class. Computer scientists are not very creative
    when they name things, so as the name implies, it is a `list` that stores only
    objects of the `Module` class type. This is important so PyTorch knows that it
    should search through the `ModuleList` for more `Module` objects. That way, it
    still works to use automatic differentiation and grab all the parameters using
    a single `.parameters()` function.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要存储一个隐藏层的列表。在 PyTorch 中，这应该使用 `ModuleList` 类来完成。计算机科学家在命名事物时并不非常富有创造力，所以正如其名所暗示的，它是一个只存储
    `Module` 类型的对象的 `list`。这是很重要的，这样 PyTorch 就知道它应该搜索 `ModuleList` 以找到更多的 `Module`
    对象。这样，它仍然可以使用自动微分，并使用单个 `.parameters()` 函数获取所有参数。
- en: 'The following PyTorch module defines a class for creating skip connections.
    It creates one larger block of multiple layers in a dense-style skip connection
    containing `n_layers` total. Used on its own, it can create dense networks; or,
    used sequentially, it can create staggered skip connections:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 PyTorch 模块定义了一个用于创建跳过连接的类。它创建了一个包含 `n_layers` 个总层的多个层的较大块，以密集式跳过连接的形式。单独使用时，它可以创建密集网络；或者，连续使用时，它可以创建交错跳过连接：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ The final layer will be treated differently, so let’s grab its index to use
    in the next two lines.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最后一层将被不同处理，所以让我们获取它的索引，以便在接下来的两行中使用。
- en: ❷ The linear and batch norm layers are stored in layers and bns, respectively.
    A list comprehension creates all the layers in one line each. “if i == l” allows
    us to single out the last layer, which needs to use out_size instead of in_size.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 线性和批归一化层分别存储在 layers 和 bns 中。一个列表推导式在一行中创建了所有层。“if i == l”允许我们单独选择最后一层，它需要使用
    out_size 而不是 in_size。
- en: ❸ Since we are writing our own forward function instead of using nn.Sequential,
    we can use one activation object multiple times.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于我们正在编写自己的前向函数而不是使用 nn.Sequential，我们可以多次使用一个激活对象。
- en: ❹ First, we need a location to store the activations from every layer (except
    the last one) in this block. All the activations will be combined as the input
    to the last layer, which is what makes the skips!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 首先，我们需要一个位置来存储这个块中每个层（除了最后一层）的激活值。所有激活值都将组合成最后一层的输入，这就是跳过连接的原因！
- en: ❺ Zips the linear and normalization layers into paired tuples, using [:-1] to
    select all but the last item in each list.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将线性和归一化层压缩成成对的元组，使用 [:-1] 来选择每个列表中除了最后一个项目之外的所有项目。
- en: ❻ Concatenates the activations together, making the input for the last layer
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将激活值连接在一起，作为最后一层的输入
- en: ❼ We manually use the last linear and batch-norm layer on this concatenated
    input, giving us the result.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 我们手动使用最后一个线性层和批归一化层来处理这个连接的输入，从而得到结果。
- en: 'With the `SkipFC` class, we can easily create networks that contain skip connections.
    [Figure 6.7](#fig-chp6_skip_connection_code) showed how to use three of these
    objects followed by a linear layer to define a network, which we do in the next
    block of code. Notice that we are still using the `nn.Sequential` object to organize
    all this code, which now proceeds in a feed-forward manner; and the non-feed-forward
    skips are encapsulated by the `SkipFC` object. This helps keep our code shorter
    and easier to read and organize. This approach of encapsulating the non-feed-forward
    parts into a custom `Module` is my preferred approach to organize custom networks:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `SkipFC` 类，我们可以轻松地创建包含跳过连接的网络。[图 6.7](#fig-chp6_skip_connection_code) 展示了如何使用三个这样的对象，然后是一个线性层来定义一个网络，我们在下一块代码中这样做。请注意，我们仍然使用
    `nn.Sequential` 对象来组织所有这些代码，现在它以前馈方式执行；非前馈跳过被 `SkipFC` 对象封装。这有助于使我们的代码更短、更易于阅读和组织。将非前馈部分封装到自定义
    `Module` 中的这种方法是我组织自定义网络的首选方法：
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With that done, we can look at the results of this new network. The following
    code calls seaborn to plot only the fully connected networks we have trained so
    far. The results are middling at best, with skip connections not clearly better
    or worse than a network without BN:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们可以查看这个新网络的结果。以下代码调用 seaborn 来绘制我们迄今为止训练的完整连接网络。结果中等，跳过连接并没有明显优于没有 BN
    的网络：
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/CH06_UN10_Raff.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN10_Raff.png)'
- en: Everything I explained about skip connections helping with learning and gradient
    flow is still true, but it’s not the whole story. Part of the issue is that skip
    connections were more effective on their own before normalization layers were
    invented to help tackle the same problem. Since our networks both have BN, the
    skip connections are a little redundant. Part of the problem is that our network
    isn’t that deep, so the difference isn’t that big.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前解释的关于跳跃连接帮助学习和梯度流的内容仍然有效，但这并不是全部。部分问题是，在发明归一化层来帮助解决相同问题之前，跳跃连接本身更有效。由于我们的网络都包含
    BN，因此跳跃连接有点多余。部分问题是我们的网络并不深，所以差异并不大。
- en: 6.3.2  Implementing convolutional skips
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 实现卷积跳跃
- en: Because skip connections are so widely used, despite what we’ve seen thus far,
    let’s repeat this exercise for the convolutional network. This will be a quick
    excursion as the code is almost identical. I’m repeating this because of how important
    and widely used skip connections are as a component in larger solutions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于跳跃连接被广泛使用，尽管我们迄今为止已经看到了，让我们重复这个练习来针对卷积网络。这将是一次快速的远足，因为代码几乎相同。我重复这个练习是因为跳跃连接作为更大解决方案组件的重要性及其广泛使用。
- en: 'We replace our `SkipFC` class with a new `SkipConv2d` class in the following
    code. The `forward` function is identical. The only difference is that we define
    a few helper variables for things like the kernel size and padding, which don’t
    exist for fully connected layers, and we replace the contents of `lns` and `bns`
    with `Conv2d` and `BatchNorm2d`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们用新的 `SkipConv2d` 类替换了我们的 `SkipFC` 类。`forward` 函数是相同的。唯一的区别是我们定义了一些辅助变量，如核大小和填充，这些在全连接层中不存在，并用
    `Conv2d` 和 `BatchNorm2d` 替换了 `lns` 和 `bns` 的内容：
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ The last convolution will have a different number of inputs and output channels,
    so we still need that index.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最后一次卷积将具有不同的输入和输出通道数，所以我们仍然需要那个索引。
- en: ❷ Simple helper values
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 简单的辅助值
- en: ❸ Defines the layers used, altering the construction of the last layer using
    the same “if i == l” list comprehension. We will combine convolutions via their
    channels, so the in and out channels change for the last layer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义了使用的层，使用相同的“if i == l”列表推导式改变最后层的构建。我们将通过它们的通道组合卷积，因此最后层的输入和输出通道会改变。
- en: ❹ This code is identical to the SkipFC class, but it’s worth highlighting the
    most important line that could change . . .
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这段代码与 SkipFC 类相同，但值得强调的是可能改变其功能的最重要的一行……
- en: ❺... which is the concatenation of all the activations. Our tensors are organized
    as (B, C, W, H), which is the default in PyTorch. But you can change that, and
    sometimes people use (B, W, H, C). In that situation, the C channel would be at
    index 3 instead of 1, so you’d use cat=3\. This is also how you can adapt this
    code to work with RNNs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❺... 这是对所有激活的连接。我们的张量组织为 (B, C, W, H)，这是 PyTorch 的默认设置。但你可以改变它，有时人们使用 (B, W,
    H, C)。在这种情况下，C 通道将位于索引 3 而不是 1，所以你会使用 cat=3。这也是你可以如何将此代码适配到 RNN 中的方法。
- en: 'Now we can define a CNN `Module` that uses skip connections. But there is a
    significant problem: our input has only three channels. That’s too few for the
    network to learn something useful. To fix that, we insert a single `Conv2d` layer
    at the start with *no activation function*. This makes it mathematically redundant,
    but our code is easier to organize because `SkipConv2d` starts constructing a
    larger set of filters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一个使用跳跃连接的 CNN `Module`。但是有一个重大问题：我们的输入只有三个通道。这对网络来说太少了，无法学习到有用的东西。为了解决这个问题，我们在开始处插入一个带有
    *无激活函数* 的单个 `Conv2d` 层。这在数学上是多余的，但我们的代码组织起来更容易，因为 `SkipConv2d` 开始构建一个更大的滤波器集：
- en: '[PRE34]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Also notice that we can’t do skips across the `MaxPool2d` layer. Pooling changes
    the width and height of the image, and we would get an error if we tried to concatenate
    two tensors of shape (*B*,*C*,*W*,*H*) and (*B*,*C*,*W*/2,*H*/2), because the
    axes have different sizes. The only tensor axis that can have a different size
    when we concatenate is the axis on which we are concatenating! So we can do (*B*,*C*,*W*,*H*)
    and (*B*,*C*/2,*W*,*H*) if we are concatenating on the C axis (`dim=1`).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不能在 `MaxPool2d` 层之间进行跳跃。池化会改变图像的宽度和高度，如果我们尝试连接形状为 (*B*,*C*,*W*,*H*) 和 (*B*,*C*,*W*/2,*H*/2)
    的两个张量，将会出错，因为轴的大小不同。当我们连接时，唯一可以有不同的轴是我们要连接的轴！所以如果我们是在 C 轴（`dim=1`）上连接，我们可以做 (*B*,*C*,*W*,*H*)
    和 (*B*,*C*/2,*W*,*H*)。
- en: 'The next chunk of code plots the results for the skip CNN and the previous
    ones and results in the same story we just saw. But it was important to do this
    exercise to make sure you are comfortable with skip connections and prime you
    with the two issues we just saw—needing to adjust the number of channels for skip
    connections, and the difficulty skipping past pooling layers (we address the first
    of these items in the next two sections, and the second pooling issue is good
    to understand):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码绘制了跳跃CNN和之前的结果，结果与刚才看到的故事相同。但进行这项练习很重要，以确保你对跳跃连接感到舒适，并让你准备好我们刚才看到的两个问题——需要调整跳跃连接的通道数，以及跳过池化层的困难（我们将在下一两个部分中解决第一个问题，第二个池化问题也是很好的理解）：
- en: '[PRE35]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/CH06_UN11_Raff.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN11_Raff.png)'
- en: Again, the results are fairly inconclusive. In practical experience, skip connections
    can sometimes make a big difference on their own, but it’s very problem-dependent.
    So why are we learning about them? Combined with one more trick, skip connections
    form one of the basic building blocks of a more powerful and routinely successful
    technique called a *residual layer* that is reliably more effective. But we need
    to build our way up to this more sophisticated approach to understand the foundation
    it is built on. Let’s learn about one more ingredient we can combine with skip
    connections to create this fabled “residual” layer that I keep hyping.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，结果相当不确定。在实际经验中，跳跃连接有时可以单独带来很大的差异，但这非常依赖于具体问题。那么我们为什么要学习它们呢？结合另一个技巧，跳跃连接形成了一个更强大且通常更有效的技术的基本构建块之一，即所谓的*残差层*，它确实更有效。但我们需要逐步构建这种更复杂的方法，以了解其基础。让我们了解另一个我们可以与跳跃连接结合使用的成分，以创建我不断吹嘘的这个传说中的“残差”层。
- en: '6.4 1 × 1 Convolutions: Sharing and reshaping information in channels'
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 1 × 1 卷积：在通道中共享和重塑信息
- en: Everything we have done with convolutions thus far has been to capture *spatial*
    information, and we have said that the point of convolutions is to capture this
    spatial *prior* that values near each other (spatially) are related to each other.
    As such, our convolutions have a kernel with some size k so we can capture information
    about our ⌊*k*/2⌋ neighbors.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止使用卷积所做的一切都是为了捕捉*空间*信息，我们说过卷积的目的是捕捉这种空间*先验*，即彼此（在空间上）靠近的值是相互关联的。因此，我们的卷积有一个大小为k的核，这样我们就可以捕捉到⌊*k*/2⌋个邻居的信息。
- en: But what if we set *k* = 1? This gives us *no* information about our neighbors
    and thus captures *no spatial information*. At first glance, this seems to make
    a convolution a useless operation. However, there can be valuable reasons to use
    a convolution with this kind of neighbor blindness. One particular application
    is as a computationally cheap operation to change the number of channels at a
    given layer. This would have been a better choice for the first layer of our CNN-based
    skip connection. We didn’t want a fully hidden layer; we just wanted the number
    of channels C to be a more convenient value. Using *k* = 1 is often called a 1
    × 1 convolution, and doing so is ≈ 9× faster (3²/1 = 9, ignoring overheads) than
    the normal 3 × 3 layer we used and requires less memory.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们设*k* = 1呢？这给我们关于邻居的*没有任何信息*，因此捕捉到*没有空间信息*。乍一看，这似乎使卷积成为一个无用的操作。然而，使用这种邻居盲目的卷积可能有有价值的原因。一个特定的应用是作为一种计算成本低的操作来改变给定层的通道数。这将是基于我们的CNN跳跃连接的第一层的一个更好的选择。我们不想有一个完全隐藏的层；我们只想让通道数C成为一个更方便的值。使用*k*
    = 1通常被称为1 × 1卷积，这样做比我们使用的正常3 × 3层快≈9倍（3²/1 = 9，忽略开销），并且需要的内存更少。
- en: This is possible because we have *C*[in] input channels and *C*[out] output
    channels when we perform a convolution. So a convolution with *k* = 1 is looking
    not at spatial neighbors but at spatial *channels* by grabbing a stack of *C*[in]
    values and processing them all at once. You can see this in figure 6.8, which
    shows applying a 1 × 1 convolution to an image with three channels.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们在执行卷积时，有*C*[in]个输入通道和*C*[out]个输出通道。因此，当*k* = 1时，卷积不是看空间邻居，而是通过抓取一个*C*[in]值的堆栈来查看空间*通道*，并一次性处理它们。您可以在图6.8中看到这一点，该图展示了将1
    × 1卷积应用于具有三个通道的图像。
- en: '![](../Images/CH06_F08_Raff.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F08_Raff.png)'
- en: Figure 6.8 Example of a 1 × 1 convolution applied to an image. The filter in
    the middle looks like a stick because it has a width and height of 1\. It is applied
    to every pixel location alone, without looking at any neighboring pixels.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 1 × 1卷积应用于图像的示例。中间的滤波器看起来像一根棍子，因为它宽度和高度都是1。它应用于每个像素位置，而不考虑任何相邻像素。
- en: 'In essence, we are giving the network a new prior: that it should try to share
    information across channels, rather than looking at neighboring locations. Another
    way to think about this is that if each channel has learned to look for a different
    kind of pattern, we are telling the network to *focus on the patterns found at
    this location* instead of having it try to build new spatial patterns.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们正在给网络提供一个新的先验：它应该尝试在通道之间共享信息，而不是查看相邻位置。另一种思考方式是，如果每个通道都学会了寻找不同类型的模式，我们正在告诉网络*关注这个位置找到的模式*，而不是让它尝试构建新的空间模式。
- en: For example, say we are processing an image, and one channel has learned to
    identify horizontal edges, another vertical edges, another edges at a 45-degree
    angle, and so on. If we want a channel to learn to identify *any* edge, we can
    accomplish that by looking just at the channel values (i.e., have *any* of these
    angle-dependent edge-detecting filters fired?) without considering the neighboring
    pixels. If such identifications are useful, *k* = 1 convolutions can help improve
    learning and reduce computational cost!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在处理一个图像，一个通道已经学会了识别水平边缘，另一个通道识别垂直边缘，另一个通道识别45度角的边缘，等等。如果我们想让一个通道学会识别*任何*边缘，我们可以通过仅查看通道值（即，是否*任何*这些角度相关的边缘检测滤波器被触发？）来实现，而不考虑相邻像素。如果这样的识别是有用的，*k*
    = 1卷积可以帮助提高学习效率和降低计算成本！
- en: 6.4.1  Training with 1 × 1 convolutions
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 使用1 × 1卷积进行训练
- en: 'Such convolutions are easy to implement: we add a second helper function named
    `infoShareBlock` in conjunction with the `cnnLayer` function we used for batch
    normalization. This new function takes the number of input filters and applies
    a 1 × 1 convolution to maintain the size of the output and hopefully perform useful
    learning along the way:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的卷积很容易实现：我们在用于批量归一化的`cnnLayer`函数的基础上，添加了一个名为`infoShareBlock`的第二个辅助函数。这个新函数接受输入滤波器的数量，并应用一个1
    × 1卷积以保持输出大小，并希望在这个过程中进行有用的学习：
- en: '[PRE36]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following code implements the new approach by adding the `infoShareBlock`
    once per block of hidden layers. I’ve chosen to do it after two rounds of normal
    hidden layers. The `infoShareBlock`s are fairly cheap, so it’s OK to sprinkle
    them around. I sometimes add one per region of the network (e.g., once per pooling);
    others add them more regularly. You can experiment with it for your problem to
    see if it’s useful and find what works:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过在每个隐藏层块中添加一次`infoShareBlock`来实现新的方法。我选择在两轮正常隐藏层之后进行。`infoShareBlock`相当便宜，所以可以在周围随意添加。我有时在每个网络区域（例如，每个池化一次）添加一个；其他人更规律地添加它们。你可以对你的问题进行实验，看看它是否有用，并找出什么有效：
- en: '[PRE37]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ First info block after 2x cnnLayers
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在2x cnnLayers之后的第一个信息块
- en: ❷ Trains up this model
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练此模型
- en: 'When we plot the results, we see that we did not improve our accuracy much.
    Why is that? Well, the example we gave about information sharing *can be done
    by larger filters anyway*. So this process has not necessarily allowed us to learn
    something we could not learn before:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制结果时，我们发现我们的准确率并没有提高多少。为什么？好吧，我们给出的关于信息共享的例子*可以通过更大的滤波器来完成*。所以这个过程并不一定允许我们学习到我们之前无法学习到的东西：
- en: '[PRE38]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](../Images/CH06_UN12_Raff.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH06_UN12_Raff.png)'
- en: While 1 × 1 convolutions are cheaper to run, we can’t just always use them and
    expect better results. They are a more strategic tool, and it takes a mix of experience
    and intuition to learn when they are worth adding to your model. As mentioned
    at the start of this section, one of their most common uses is as a fast and easy
    way to change the number of channels at a given layer. This is the critical function
    they provide for implementing residual layers in the next section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然1 × 1卷积运行起来更便宜，但我们不能总是使用它们并期望得到更好的结果。它们是一个更策略性的工具，需要经验和直觉的混合来学习何时值得将其添加到模型中。正如本节开头提到的，它们最常用的一个用途是作为快速简单地更改给定层中通道数的一种方式。这是它们在下一节中实现残差层所提供的关键功能。
- en: 6.5 Residual connections
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 残差连接
- en: 'We have learned about two approaches that, on their own, don’t seem to be all
    that useful. Sometimes they do better, sometimes worse. But if we combine skip
    connections and 1 × 1 convolutions in just the right way, we get an approach called
    a *residual connection* that converges faster to more accurate solutions. The
    specific design pattern called a residual connection is exceptionally popular,
    and you should almost always use it as your default approach to specifying a model
    architecture when working with fully connected or convolutional layers. The strategy,
    ideas, and intuition behind the residual connection reoccur in this chapter and
    multiple times in this book, and the residual connection has become a widely used
    design component. It is broken into two types of con-nections: the standard block
    and the bottleneck variant.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了两种方法，单独来看，它们似乎并不那么有用。有时它们表现得更好，有时则更差。但如果我们以正确的方式将跳过连接和1 × 1卷积结合起来，我们得到一种称为*残差连接*的方法，它更快地收敛到更精确的解。称为残差连接的具体设计模式非常受欢迎，当与全连接或卷积层一起工作时，您几乎总是应该将其作为指定模型架构的默认方法。残差连接背后的策略、思想和直觉在本章和本书的多次出现中反复出现，残差连接已成为广泛使用的设计组件。它分为两种类型的连接：标准块和瓶颈变体。
- en: 6.5.1  Residual blocks
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 残差块
- en: 'The first connection type is a *residual block*, shown in figure 6.9\. The
    block is a kind of skip connection where two layers combine at the end, creating
    long and short paths. However, in a residual block, the short path has *no operations*.
    We simply leave the input unaltered! Once the long path has computed its result
    h, we add it to the input x to get the final result **x** **+** **h**. We often
    denote the long path as a subnetwork *F*(⋅) = **h**, and we can describe all residual
    connections as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种连接类型是*残差块*，如图6.9所示。该块是一种跳过连接，其中两个层在末尾结合，创建长路径和短路径。然而，在残差块中，短路径*没有操作*。我们只是简单地保留输入不变！一旦长路径计算出其结果h，我们就将其加到输入x上以得到最终结果**x**
    **+** **h**。我们通常将长路径表示为子网络*F*(⋅) = **h**，我们可以将所有残差连接描述如下：
- en: '![](../Images/CH06_UN13_Raff.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN13_Raff.png)'
- en: '![](../Images/CH06_F09_Raff.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F09_Raff.png)'
- en: Figure 6.9 Example of a residual block architecture. The left side of the block
    is the short path, which performs no operations or alterations to the input. The
    right side of the block is a residual connection and performs a skip connection
    with two rounds of BN/linear activation to produce an intermediate result h. The
    output is the sum of the input x and h.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 残差块架构的示例。块的左侧是短路径，它不对输入进行任何操作或更改。块的右侧是残差连接，通过两轮BN/线性激活执行跳过连接，产生中间结果h。输出是输入x和h的总和。
- en: When we start combining multiple residual blocks one after another, we create
    an architecture with a very interesting design. You can see that in figure 6.10,
    where we end up with a *long* path and a *short* path through the network. The
    short path makes it easier to learn deep architectures by having as few operations
    as possible. Fewer operations means less chance of noise in the gradient, making
    it easy to propagate a useful gradient back farther than would otherwise be possible.
    The long path then performs the actual work, learning units of complexity that
    are added back in via the skip connections (using addition instead of concatenation).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始将多个残差块一个接一个地组合起来时，我们创建了一个设计非常有趣的架构。您可以在图6.10中看到这一点，其中我们通过网络得到一条*长路径*和一条*短路径*。短路径通过尽可能少地进行操作，使得学习深度架构变得更容易。操作越少，梯度中的噪声机会就越少，这使得传播一个有用的梯度比其他情况下更远变得容易。长路径随后执行实际工作，通过跳过连接（使用加法而不是拼接）学习复杂性的单元。
- en: '![](../Images/CH06_F10_Raff.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F10_Raff.png)'
- en: Figure 6.10 An architecture with multiple layers of residual blocks. This creates
    a long path and a short path through the network. The short path on top makes
    it easy to get the gradient to propagate back to many layers, allowing for more
    depth by avoiding as much work as possible. The long path does the actual work,
    letting the network learn complex functions one piece at a time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 具有多层残差块的架构。这通过网络创建了一条长路径和一条短路径。顶部的短路径使得将梯度传播回许多层变得容易，通过尽可能避免工作，允许更深的层次。长路径执行实际工作，让网络一次学习一个复杂函数。
- en: This kind of residual block can easily be converted to a fully connected counterpart.
    But when we are working with images, we like to do a few rounds of max pooling
    to help build some translation invariance and then double the number of channels
    after each round of pooling to do a consistent amount of computation and work
    at each layer of the network. But the residual block *requires* that the input
    and output have the *exact same shape* because we are using addition instead of
    concatenation. This is where the bottleneck layer helps, as we see shortly.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的残差块可以很容易地转换为全连接的对应版本。但是，当我们处理图像时，我们喜欢进行几轮最大池化以帮助建立一些平移不变性，并在每轮池化后加倍通道数，以在每个网络层中进行一致的计算和工作。但是，残差块
    *需要* 输入和输出具有 *完全相同的形状*，因为我们使用的是加法而不是拼接。这就是瓶颈层发挥作用的地方，正如我们很快就会看到的。
- en: 6.5.2  Implementing residual blocks
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2  实现残差块
- en: 'The residual block we have described is called *Type E* and is one of the favored
    residual setups. As you might have guessed from the name, people have tried tons
    of different reorderings on the normalization, convolution, and number of layers
    in each residual block. They all tend to work well, but we’ll stick with Type
    E for simplicity. Notice how we can use `nn.Sequential` to organize our code the
    same way our math reads, helping us keep the definition simple and easy to check
    for correctness:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所描述的残差块被称为 *类型 E*，并且是受欢迎的残差配置之一。正如你可能从名称中猜到的，人们在标准化、卷积以及每个残差块中的层数上尝试了大量的不同重排。它们都倾向于工作得很好，但为了简单起见，我们将坚持使用类型
    E。注意我们如何可以使用 `nn.Sequential` 来组织我们的代码，使其与我们阅读数学的方式相同，这有助于我们保持定义简单且易于检查正确性：
- en: '[PRE39]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ How much padding will our convolutional layers need to maintain the input
    shape?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的卷积层需要多少填充来保持输入形状？
- en: '❷ Defines the conv and BN layers we use in a subnetwork: just two hidden layers
    of conv/BN/activation'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在子网络中定义我们使用的卷积和 BN 层：仅两个隐藏层的卷积/BN/激活
- en: '❸ F() has all the work for the long path: we just add it to the input.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ F() 包含了长路径的所有工作：我们只需将其添加到输入中。
- en: 6.5.3  Residual bottlenecks
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3  剩余瓶颈
- en: The residual layer is a simple extension of the skip connection idea that works
    by making the short path do as little work as possible to help with the gradient
    flow and minimize noise. But we need a way to handle different numbers of channels
    after we do pooling. The solution is 1 × 1 convolutions. We can use the 1 × 1
    layer to do the minimum amount of work to simply change the number of channels
    in the input, increasing or decreasing the number of channels as we see fit. The
    preferred approach is to create a *residual bottleneck*, as shown in figure 6.11\.
    The short path is still short and has no activation function but simply performs
    a 1 × 1 convolution followed by BN to change the original number of channels C
    into the desired number *C*′.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 残差层是跳过连接思想的简单扩展，通过使短路径尽可能少地工作来帮助梯度流动并最小化噪声。但是，我们在进行池化后需要一种处理不同通道数的方法。解决方案是 1
    × 1 卷积。我们可以使用 1 × 1 层来完成最小的工作量，仅改变输入中的通道数，根据需要增加或减少通道数。首选的方法是创建一个 *残差瓶颈*，如图 6.11
    所示。短路径仍然很短，没有激活函数，只是执行一个 1 × 1 卷积然后是 BN，将原始通道数 C 转换为所需的通道数 *C*′。
- en: '![](../Images/CH06_F11_Raff.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F11_Raff.png)'
- en: Figure 6.11 Example of a bottleneck connection where the input has (*B*,*C*,*W*,*H*)
    as its input shape and the goal is to have an output shape of (*B*,*C*′,*W*,*H*).
    The short path has to change shape, so a 1 × 1 convolution is used to do the minimum
    work needed to change the number of channels. The long path is supposed to be
    a bottleneck that encourages compression, so it starts with a 1 × 1 convolution
    to shrink the number of channels, followed by a normal convolution and ending
    with another 1 × 1 convolution to expand the number of channels back up to the
    desired size *C*′.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 瓶颈连接的示例，其中输入的形状为 (*B*,*C*,*W*,*H*)，目标是输出形状为 (*B*,*C*′,*W*,*H*)。短路径需要改变形状，因此使用
    1 × 1 卷积来完成所需的最小工作量以改变通道数。长路径应该是一个鼓励压缩的瓶颈，因此它从 1 × 1 卷积开始以减少通道数，然后是正常卷积，最后以另一个
    1 × 1 卷积结束，将通道数扩展回所需的尺寸 *C*′。
- en: The approach is called a bottleneck because of the long path *F*(⋅) on the right
    that has three hidden layers. The first hidden layer uses another 1 × 1 convolution
    to shrink the number of channels C before doing a normal hidden layer in the middle,
    followed by a final 1 × 1 convolution to expand the number of channels back up
    to the original count. There are two reasons and interpretations behind the bottleneck.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为瓶颈，是因为右侧的长路径*F*(⋅)有三个隐藏层。第一个隐藏层使用另一个1 × 1卷积来减少通道数C，然后再进行中间的正常隐藏层，最后通过一个最终的1
    × 1卷积将通道数恢复到原始数量。瓶颈背后有两个原因和解释。
- en: The first is a design choice of the original authors of the residual network.
    They wanted to make their networks deeper as a way to increase their capacity.
    Making the bottlenecks shrink and then expand keeps the number of parameters down,
    saving valuable GPU memory for adding more layers! The authors used this to train
    a network with a total of 152 layers. At the time, this was an obscenely deep
    network and set a new record for results on the seminal ImageNet dataset (widely
    used as a benchmark by researchers).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是残差网络原始作者的的设计选择。他们希望通过使网络更深来增加其容量。使瓶颈收缩再扩张可以减少参数数量，为添加更多层节省宝贵的GPU内存！作者们用这个方法训练了一个总共有152层的网络。在当时，这是一个极其深的网络，并在原始的ImageNet数据集（被研究人员广泛用作基准）上取得了新的记录。
- en: 'The second reason draws on the concept of compression: making things smaller.
    An entire area of machine learning studies compression as a tool to make models
    learn interesting things. The idea is that if you force a model to go from a large
    number of parameters to a small number, you force the model to create more meaningful
    and compact representations. As a loose analogy, think about how you can simply
    say “cat” to communicate a specific species of animal with a particular shape,
    dietary habits, and more with just three letters. If you say “the orange house
    cat,” you quickly narrow down the mental image of what you are talking about with
    very little information given. That is the idea behind compression: being able
    to compress implies some level of intelligence.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是借鉴了压缩的概念：使事物变得更小。机器学习的一个研究领域将压缩作为一种工具，使模型学习有趣的东西。想法是，如果你迫使模型从一个大量参数的状态转换到一个小参数状态，你迫使模型创建更多有意义的紧凑表示。作为一个粗略的类比，想想你如何只用三个字母“猫”来传达一个具有特定形状、饮食习惯等的特定物种动物。如果你说“the
    orange house cat”，你就可以用很少的信息快速缩小你正在谈论的图像。这就是压缩背后的想法：能够压缩意味着某种程度的智能。
- en: 6.5.4  Implementing residual bottlenecks
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.4 实现残差瓶颈
- en: 'Let’s convert the idea of a residual bottleneck into code. It looks very similar
    to `ResidualBlockE`. The main difference is that we have two subnetworks: first
    is the long path, encoded in `self.F`; the short path gets a member variable `self.shortcut`.
    We do this with a little trick: if the bottleneck is *not* changing the number
    of channels, we use the `Identity` function to implement it. This function simply
    returns the input as the output. If the number of channels *does* change, we override
    the definition with a small `nn.Sequential` that does the 1 × 1 followed by BN:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将残差瓶颈的想法转换为代码。它看起来非常类似于`ResidualBlockE`。主要区别在于我们有两个子网络：第一个是长路径，编码在`self.F`中；短路径有一个成员变量`self.shortcut`。我们用一个小技巧来实现它：如果瓶颈*不*改变通道数，我们使用`Identity`函数来实现。这个函数简单地返回输入作为输出。如果通道数*确实*改变，我们用一个小`nn.Sequential`覆盖定义，执行1
    × 1卷积后跟BN：
- en: '[PRE40]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ How much padding will our convolutional layers need to maintain the input
    shape?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的反卷积层需要多少填充才能保持输入形状？
- en: ❷ The bottleneck should be smaller, so output /4 or input. You could also try
    changing max to min; it’s not a major issue.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 瓶颈应该更小，所以输出/4或输入。你也可以尝试将最大值改为最小值；这不是一个大问题。
- en: ❸ Defines the three sets of BN and convolution layers we need. Notice that for
    the 1 convs, we use padding=0, because 1 will not change shape!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义了我们需要的三组BN和卷积层。注意，对于1个卷积，我们使用padding=0，因为1不会改变形状！
- en: ❹ Compresses down
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 压缩
- en: ❺ Normal layer doing a full conv
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 正常层进行完整卷积
- en: ❻ Expands back up
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 扩展回原状
- en: ❼ By default, our shortcut is the identity function, which simply returns the
    input as the output.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 默认情况下，我们的快捷方式是恒等函数，它简单地返回输入作为输出。
- en: ❽ If we need to change the shape, let’s turn the shortcut into a small layer
    with 1 conv and BM.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 如果我们需要改变形状，让我们将快捷方式转换为一个包含1个卷积和BM的小层。
- en: ❾ “shortcut(x)” plays the role of “x"; we do as little work as possible to keep
    the tensor shapes the same.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ “shortcut(x)”在功能上等同于“x”；我们尽可能少做工作以保持张量形状不变。
- en: 'Now we can define a residual network! It is difficult to make this exactly
    the same as our original network because each residual layer includes two or three
    rounds of layers as well, but the following definition gets us to something reasonably
    close. Since our network is on the small side, we added a `LeakyReLU` after rounds
    of residual blocks. You do not have to include this, because the long path through
    the residual layers has activation functions in it. For very deep networks (30+
    blocks), I recommend not including the activation between blocks, to help get
    information through all those layers. This is not a critical detail, though, and
    the approach will work well either way. You can always try it and see which way
    works best for your problem and network size. Also notice how defining our own
    blocks as a `Module` lets us specify this very complex network in relatively few
    lines of code:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一个残差网络了！要使其与我们的原始网络完全相同是很困难的，因为每个残差层还包括两到三个轮次的层，但以下定义使我们接近这个目标。由于我们的网络规模较小，我们在残差块轮次之后添加了一个`LeakyReLU`。你不必包含这个，因为通过残差层的长路径中包含激活函数。对于非常深的网络（30+块），我建议不要在块之间包含激活函数，以帮助信息通过所有这些层。但这不是一个关键细节，两种方法都能很好地工作。你总是可以尝试一下，看看哪种方法最适合你的问题和网络规模。同时注意，将我们自己的块定义为`Module`允许我们在相对较少的代码行中指定这个非常复杂的网络：
- en: '[PRE41]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ BottleNeck to start because we need more channels. It’s also common to start
    with just one normal hidden layer before beginning residual blocks.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以BottleNeck开始，因为我们需要更多的通道。在开始残差块之前，先开始一个正常的隐藏层也是很常见的。
- en: ❷ Inserts an activation after each residual. This is optional.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在每个残差后插入一个激活函数。这是可选的。
- en: 'Now, if we plot the results, we should *finally* see consistent improvement!
    While the difference is not big for this dataset, it will be more dramatic for
    larger and more challenging problems. Fashion-MNIST just does not have much room
    for improvement. We are hitting over 98% accuracy, a significant improvement from
    the 93% we started with:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们绘制结果，我们应该*最终*看到一致性的改进！虽然对于这个数据集，差异不是很大，但对于更大和更具挑战性的问题，差异将会更加显著。Fashion-MNIST几乎没有改进的空间。我们已经达到了超过98%的准确率，这比我们开始的93%有显著提高：
- en: '[PRE42]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](../Images/CH06_UN14_Raff.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN14_Raff.png)'
- en: With that said, it is hard to overstate how large an impact residual connections
    have had on modern deep learning, and you should almost always default to implementing
    a residual-style network. You will often see people refer to *ResNet-X*, where
    the *X* stands for the total number of layers in a specific residual network architecture.
    This is because thousands of researchers and practitioners like to start with
    ResNet as their baseline and add or remove a few parts to customize it to their
    problem. In many cases, using ResNet with no change except to the output layer
    gives a strong result on an image classification problem. As such, it should become
    one of your go-to tools when tackling real-life problems.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，很难过分强调残差连接对现代深度学习产生的影响有多大，你应该几乎总是默认实现残差风格的网络。你经常会看到人们提到*ResNet-X*，其中*X*代表特定残差网络架构中的总层数。这是因为成千上万的研究人员和从业者喜欢以ResNet作为他们的基准，并添加或删除一些部分来定制它以适应他们的问题。在许多情况下，除了输出层外没有其他变化地使用ResNet在图像分类问题上可以得到一个强大的结果。因此，当解决现实问题时，它应该成为你的首选工具之一。
- en: 6.6 Long short-term memory RNNs
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 长短期记忆RNNs
- en: The RNN we described in chapter 4 is rarely used in practice. It is notoriously
    difficult to train and get working on complex problems. Many different variants
    of RNNs have been published, but the tried-and-true option that always works well
    is called a *long short-term memory* (LSTM) *network*. LSTMs are a type of RNN
    architecture originally developed in 1997\. Despite their age (and a small tune-up
    in 2006), LSTMs have continued to be one of the best options available for a recurrent
    architecture.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第4章中描述的RNN在实践中的应用很少。它在训练和解决复杂问题上是臭名昭著的困难。已经发表了多种RNN的不同变体，但始终有效且表现良好的选项被称为*长短期记忆*（LSTM）*网络*。LSTMs是一种在1997年最初开发的RNN架构。尽管它们已经存在了一段时间（以及2006年的一次小调整），但LSTMs仍然是可用的循环架构中最好的选择之一。
- en: '6.6.1  RNNs: A fast review'
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 RNNs：快速回顾
- en: 'Let’s quickly recall what a simple RNN looks like with figure 6.12 and equation
    6.1\. We can describe it succinctly with its inputs **x**[t] and the previous
    hidden state **h**[t] being concatenated together into one larger input. They
    are processed by a `nn.Linear` layer and go through the tanh nonlinearity, and
    that becomes the output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下简单的RNN在图6.12和方程6.1中的样子。我们可以用它的输入 **x**[t] 和前一个隐藏状态 **h**[t] 连接成一个更大的输入来简洁地描述它。它们通过一个
    `nn.Linear` 层进行处理，并经过tanh非线性变换，这成为输出：
- en: '![](../Images/ch6-eqs-to-illustrator3x.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch6-eqs-to-illustrator3x.png)'
- en: '![](../Images/CH06_F12_Raff.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F12_Raff.png)'
- en: Figure 6.12 A simple RNN that we learned about in chapter 4\. Information over
    time is captured in the hidden activations **h**[t] that are fed into the next
    RNN layer and used as the output to any following layers. Small black dots denote
    concatenation.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12展示了我们在第4章中学到的简单RNN。随时间变化的信息被捕获在隐藏激活 **h**[t] 中，这些激活被输入到下一个RNN层，并用作任何后续层的输出。小黑点表示连接。
- en: 'One of the challenges with learning an RNN is that, when unrolled through time,
    many operations are performed on the same tensor. It is hard for the RNN to learn
    how to use this finite space to extract information it needs from the fixed-sized
    representation of a variable number of timesteps, and then to also add information
    to that representation. This problem is similar to what residual layers tackle:
    it’s easy for the gradient to send information back when there are fewer operations
    on it. If we have a sequence with 50 steps, that’s like trying to learn a network
    with 50 layers with 50 more opportunities for gradient noise to get in the way.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 学习RNN的一个挑战是，当通过时间展开时，许多操作都在同一个张量上执行。RNN很难学习如何使用这个有限的空间从变量时间步长的固定大小表示中提取所需的信息，并添加信息到该表示中。这个问题与残差层解决的问题类似：当操作较少时，梯度很容易发送信息回传。如果我们有一个包含50个步骤的序列，这就如同尝试学习一个有50层且梯度噪声有50次机会干扰的网络。
- en: 6.6.2  LSTMs and the gating mechanism
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 LSTMs和门控机制
- en: 'The difficulty of getting a gradient to carry a signal back many time steps
    is one of the primary issues an LSTM tries to solve. To do this, the LSTM creates
    two sets of states: a hidden state **h**[t] and a context state **C**[t]. **h**[t]
    does the work and tries to learn complex functions, and the context **C**[t] tries
    to simply hold valuable information for use later. You can think of **C**[t] as
    focusing on *long-term* information and **h**[t] as working on *short-term* information;
    hence, we get the name *long short-term memory*.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 获取梯度在多个时间步长中传递信号的难度是LSTM试图解决的主要问题之一。为了做到这一点，LSTM创建了两组状态：一个隐藏状态 **h**[t] 和一个上下文状态
    **C**[t]。**h**[t] 执行工作并尝试学习复杂函数，而上下文 **C**[t] 则试图简单地保存有用的信息以供以后使用。你可以将 **C**[t]
    视为专注于 *长期* 信息，而 **h**[t] 则专注于 *短期* 信息；因此，我们得到了 *长短期记忆* 这个名字。
- en: 'LSTMs use a strategy called *gating* to make this happen. A gating mechanism
    produces a value in the range of [0,1]. That way, if you multiply by the result
    of a gate, you can remove everything (the gate returned 0 and anything time 0
    is 0) or allow everything to pass through (the gate returned 1). LSTMs are designed
    with three gates:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM使用一种称为 *门控* 的策略来实现这一点。门控机制产生一个在[0,1]范围内的值。这样，如果你乘以门控的结果，你可以移除所有内容（门控返回0，任何时间0的值都是0）或者允许所有内容通过（门控返回1）。LSTM被设计为具有三个门：
- en: The *forget gate* allows us to forget what is in our context **C**[t].
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门* 允许我们忘记上下文 **C**[t] 中的内容。'
- en: The *input gate* controls what we want to add or input into **C**[t].
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入门* 控制我们想要添加或输入到 **C**[t] 中的内容。'
- en: The *output gate* is how much of the context **C**[t] we want to include in
    the final output **h**[t].
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出门* 决定了我们希望在最终输出 **h**[t] 中包含多少上下文 **C**[t]。'
- en: Figure 6.13 shows what this looks like at a high level.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13展示了这一过程在高级别上的样子。
- en: '![](../Images/CH06_F13_Raff.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F13_Raff.png)'
- en: Figure 6.13 The strategy of an LSTM involves three gates that operate sequentially
    and allow interaction between the short term **h**[t] and the long term **C**[t].
    The forget gate is red, the input gate is blue, and the output gate is green.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 LSTM的策略涉及三个依次操作的门，允许短期 **h**[t] 和长期 **C**[t] 之间的交互。遗忘门是红色，输入门是蓝色，输出门是绿色。
- en: This gating is accomplished using the sigmoid *σ*(⋅) and tanh (⋅) activation
    functions. Implementing gating mechanisms is a situation where you *want saturation*
    because you need your outputs to be in a very specific range of values so you
    can create a prior that *some inputs need to be allowed, and some need to be stopped*.
    The sigmoid activation function *σ*(⋅) produces a value in the range of [0,1].
    If we take another value α and multiply it with the result of the sigmoid, we
    get **z** = **α** ⋅ *σ*(⋅). If *σ*(⋅) = 0, then we have essentially closed the
    gate on z containing any information about α. If *σ*(⋅) = 1, we have **z** = **α**
    and have essentially let all the information flow through the gate. For values
    in between, we end up regulating how much information/content from α gets to flow
    through the network. We can see how an LSTM uses this gating approach in figure
    6.14.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这种门控是通过使用sigmoid *σ*(⋅) 和 tanh (⋅) 激活函数来实现的。实现门控机制是一种你*希望饱和*的情况，因为你需要你的输出值在一个非常特定的范围内，以便你可以创建一个先验，*一些输入需要被允许，而一些则需要被阻止*。sigmoid激活函数
    *σ*(⋅) 产生一个在[0,1]范围内的值。如果我们取另一个值α并将其与sigmoid的结果相乘，我们得到 **z** = **α** ⋅ *σ*(⋅)。如果
    *σ*(⋅) = 0，那么我们实际上已经关闭了包含任何关于α信息的z门。如果 *σ*(⋅) = 1，那么我们得到 **z** = **α**，并且实际上让所有信息都通过门。对于中间的值，我们最终调节了从α流经网络的信息/内容量。我们可以在图6.14中看到LSTM如何使用这种门控方法。
- en: '![](../Images/CH06_F14_Raff.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F14_Raff.png)'
- en: Figure 6.14 Diagram showing the detailed operations of an LSTM and how each
    gate is implemented and connected. The × indicates two values being multiplied
    together, and the + shows values being added together. The same color coding applies
    (red for the forget gate, blue for the input gate, and green for the output gate).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14展示了LSTM的详细操作以及每个门是如何实现和连接的。×表示两个值相乘，+表示值相加。相同的颜色编码适用（红色代表遗忘门，蓝色代表输入门，绿色代表输出门）。
- en: 'The context vector is on the top half and the short-term hidden states on the
    bottom. The setup has some properties reminiscent of the residual network we just
    learned about. The context **C**[t] acts kind of like a short path: few (and simple)
    operations are performed on it, making it easier for the gradient to flow back
    over very long sequences. The bottom half of the LSTM does the heavy lifting (like
    the residual path) with linear layers and nonlinear activation functions and attempts
    to learn more complex functions that are necessary but also make it harder for
    the gradient to propagate through to the beginning.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文向量位于上半部分，短期隐藏状态位于下半部分。这种设置有一些与我们所学的残差网络相似的性质。上下文 **C**[t] 类似于一条短路径：对其执行的操作很少（且简单），这使得梯度能够更容易地流回非常长的序列。LSTM的下半部分执行繁重的任务（如残差路径），使用线性层和非线性激活函数，并试图学习更复杂的函数，这些函数是必要的，但同时也使得梯度传播到开始部分变得更加困难。
- en: This idea of gating and having short and long paths is the main secret to making
    LSTMs work well and better than the simple RNN we learned about. Just like all
    RNNs, the LSTM also uses weight sharing over time. So we have looked at the LSTM
    cell for one input in time, and the same weights are reused for every item over
    time.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这种门控和存在短路径与长路径的想法是LSTM工作良好并且比我们所学到的简单RNN更好的主要秘密。就像所有RNN一样，LSTM也使用时间上的权重共享。因此，我们已经研究了时间上单个输入的LSTM单元，并且相同的权重在时间上的每个项目上都被重复使用。
- en: A very thorough description and explanation of LSTMs by Chris Olah is available
    at [https://colah.github.io/posts/2015-08-Understanding-LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs);
    he also describes some of the deeper details about *peephole connections*, which
    are a standard improvement in modern LSTMs. The idea behind the peephole is to
    also concatenate the old context **C**[*t* − 1] into the `nn.Linear` layers that
    decide the forget fate and input gate, the idea being that you really should know
    what you are about to forget before deciding to forget it. Similarly, adding it
    to the input gate helps you avoid adding redundant information you already have.
    Following this train of logic, one more peephole connection adds **C**[t] to the
    last `nn.Linear` layer which gates the output, the idea being you should know
    what you are looking at outputting before deciding if you should output it.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Chris Olah 对 LSTM 的非常详尽描述和解释可在 [https://colah.github.io/posts/2015-08-Understanding-LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs)
    找到；他还描述了关于 *窥视孔连接* 的更深层细节，这是现代 LSTM 中的一个标准改进。窥视孔背后的想法是将旧上下文 **C**[*t* − 1] 连接到决定遗忘命运和输入门的
    `nn.Linear` 层，其想法是在决定忘记之前，你真的应该知道你即将忘记什么。同样，将其添加到输入门可以帮助你避免添加你已经拥有的冗余信息。遵循这一逻辑，另一个窥视孔连接将
    **C**[t] 添加到最后一个 `nn.Linear` 层，该层控制输出，其想法是在决定是否输出之前，你应该知道你正在查看什么。
- en: There are *tons* of variants on the LSTM, including another popular RNN called
    the *Gated Recurrent Unit* (GRU). The GRU has the same inspiration as the LSTM
    but tries to get the hidden state **h**[t] to do double-duty as the short- and
    long-term memory. This makes the GRU faster, use less memory, and is easier to
    write code with, which is why we use it. The downside is that the GRU is not always
    as accurate as the LSTM. By contrast, the LSTM with peephole connections has been
    a tried-and-true method that is hard to beat and is what people are usually referring
    to when they say an “LSTM.”
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 有许多变体，包括另一个流行的 RNN，称为 *门控循环单元* (GRU)。GRU 与 LSTM 有相同的灵感，但试图让隐藏状态 **h**[t]
    执行双重任务，作为短期和长期记忆。这使得 GRU 更快，使用更少的内存，并且更容易编写代码，这就是我们使用它的原因。缺点是 GRU 不总是像 LSTM 那样准确。相比之下，带有窥视孔连接的
    LSTM 是一个经过验证且难以超越的方法，这就是人们通常所说的“LSTM”。
- en: 6.6.3  Training an LSTM
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 训练 LSTM
- en: 'Now that we have talked about what an LSTM is, let’s implement one and try
    it. We’ll reuse the dataset and problem from chapter 4, where we tried to predict
    the original language a name might have come from. The first thing we do is set
    up that problem again using the same code and our `LanguageNameDataset` class.
    Briefly, here’s a review of the block creating the data loader objects:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了 LSTM 是什么，那么让我们来实现一个并尝试它。我们将重用第 4 章中的数据集和问题，其中我们试图预测一个名字可能来自的原语言。我们首先要做的是使用相同的代码和我们的
    `LanguageNameDataset` 类再次设置这个问题。简要来说，以下是创建数据加载器对象的块回顾：
- en: '[PRE43]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Reuses our code from chapter 4
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重新使用第 4 章中的代码
- en: 'Let’s set up a new RNN as our baseline. I’m using a three-layer one, but I’m
    not making it bidirectional. While bidirectional layers help with the problem
    of getting information across time, I want to make that problem *worse* so that
    we can better see the benefit of an LSTM:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个新的 RNN 作为我们的基线。我使用的是一个三层结构，但我没有使其双向。虽然双向层有助于解决跨时间获取信息的问题，但我希望使这个问题变得更糟，这样我们就能更好地看到
    LSTM 的好处：
- en: '[PRE44]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Simple old-style RNN
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 简单的老式循环神经网络 (RNN)
- en: ❷ (B, T) -> (B, T, D)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T) -> (B, T, D)
- en: ❸ (B, T, D) -> ( (B,T,D) , (S, B, D) )
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T, D) -> ( (B,T,D) , (S, B, D) )
- en: ❹ We need to take the RNN output and reduce it to one item, (B, D).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们需要将 RNN 输出缩减为一个项目，(B, D)。
- en: ❺ (B, D) -> (B, classes)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, D) -> (B, classes)
- en: ❻ Applies gradient clipping to maximize its performance
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 应用梯度裁剪以最大化其性能
- en: 'Next we implement an LSTM layer. Since the LSTM is built directly into PyTorch,
    it’s very easy to incorporate. We can just replace every `nn.RNN` layer with an
    `nn.LSTM`! We are also reusing the `LastTimeStep` layer from chapter 4: if you
    go back and look at the code, you now know why we had this comment: “Result is
    either a tuple `(out, h_t)` or a tuple `(out, (h_t, c_t))`.” It’s because when
    we use an LSTM, we get the hidden states `h_t` and the context states `c_t`! That
    addition in chapter 4 made our code a little more future-proof for the LSTM we
    are now using. The LSTM version of the network is as follows, and we had to change
    only one line to make it happen:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们实现一个 LSTM 层。由于 LSTM 直接集成在 PyTorch 中，因此很容易将其结合。我们只需将每个 `nn.RNN` 层替换为 `nn.LSTM`！我们还在重用第
    4 章中的 `LastTimeStep` 层：如果你回顾一下代码，你现在知道为什么我们有了这个注释：“结果是 `(out, h_t)` 或 `(out, (h_t,
    c_t))` 的元组。”这是因为当我们使用 LSTM 时，我们得到隐藏状态 `h_t` 和上下文状态 `c_t`！第 4 章中的这个添加使我们的代码对现在使用的
    LSTM 有了更多的未来保障。网络的 LSTM 版本如下，我们只改变了一行来使其发生：
- en: '[PRE45]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ (B, T) -> (B, T, D)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T) -> (B, T, D)
- en: ❷ nn.RNN became nn.LSTM, and now we are upgraded to LSTMs w/(B, T, D) -> ( (B,T,D)
    , (S, B, D) ).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ nn.RNN 变成了 nn.LSTM，现在我们升级到了 LSTMs w/(B, T, D) -> ( (B,T,D) , (S, B, D) )。
- en: ❸ We need to take the RNN output and reduce it to one item, (B, D).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们需要将 RNN 输出减少到一个项目，即 (B, D)。
- en: ❹ (B, D) -> (B, classes)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, D) -> (B, classes)
- en: ❺ We still want to use gradient clipping with every kind of RNN, including LSTMs.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们仍然希望对每种 RNN，包括 LSTM，使用梯度裁剪。
- en: 'If we plot the results, we can see that the LSTM helps improve them over the
    RNN. We also have some evidence of the LSTM starting to overfit after about four
    epochs. If we were trying to train for real, we might want to reduce the number
    of neurons to prevent overfitting or use a validation step to help us learn that
    we need to stop after four epochs:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制结果，我们可以看到 LSTM 有助于改进它们相对于 RNN。我们还有一些证据表明，在约四个周期后，LSTM 开始出现过度拟合。如果我们真的在尝试训练，我们可能想要减少神经元的数量以防止过度拟合，或者使用验证步骤帮助我们学习在四个周期后停止：
- en: '[PRE46]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](../Images/CH06_UN15_Raff.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_UN15_Raff.png)'
- en: 'The LSTM is one of the few tried-and-true deep learning methods that has lasted
    for *decades* without much change. If you need an RNN, the LSTM is a good default
    choice. The gated recurrent unit (GRU) we mentioned is also a good choice, particularly
    if you need something that requires less computational resources. We won’t go
    into the details of the GRU, but it’s an intentional simplification of the LSTM
    design: it is simpler in that it does not need the context state *C*[t] and instead
    tries to get the hidden state **h**[t] to do double duty as the hidden and context.
    This explaining the details a bit more involved, but I like the example at [https://blog.floydhub.com/gru-with-pytorch](https://blog.floydhub.com/gru-with-pytorch)
    if you want to learn more. If you have shorter and simpler data or if compute
    limitations are a concern, the GRU is worth checking out because it requires less
    memory. I like using the GRU because its code is a little simpler, since there
    is only the hidden state, but there are definitely applications where the LSTM
    performs better. You can use either a GRU or LSTM and get good results, but the
    best will probably come from tuning (e.g., with Optuna) an LSTM.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是少数经过验证且经久不衰的深度学习方法之一，它在过去几十年中几乎没有变化。如果你需要一个 RNN，LSTM 是一个很好的默认选择。我们提到的门控循环单元（GRU）也是一个不错的选择，尤其是当你需要更少的计算资源时。我们不会深入探讨
    GRU 的细节，但它是对 LSTM 设计的有意简化：它更简单，因为它不需要上下文状态 *C*[t]，而是试图让隐藏状态 **h**[t] 执行双重职责，即作为隐藏状态和上下文状态。更详细地解释这一点可能有些复杂，但如果你想要了解更多，我喜欢这个例子
    [https://blog.floydhub.com/gru-with-pytorch](https://blog.floydhub.com/gru-with-pytorch)。如果你有更短、更简单的数据，或者如果计算限制是一个问题，GRU
    值得检查，因为它需要的内存更少。我喜欢使用 GRU，因为它的代码稍微简单一些，因为只有一个隐藏状态，但确实有一些应用中 LSTM 的表现更好。你可以使用 GRU
    或 LSTM 并获得良好的结果，但最好的结果可能来自调整（例如，使用 Optuna）一个 LSTM。
- en: Exercises
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Inside Deep Learning 练习的 Manning 在线平台上分享和讨论你的解决方案 ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: Try replacing the LeakyReLU with the `nn.PReLU` activation function in the various
    CNNs we have trained. Does it perform better or worse?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在我们的各种CNN中将LeakyReLU激活函数替换为`nn.PReLU`激活函数。它的表现是更好还是更差？
- en: Write a `for` loop to train CNN models with 1 to 20 sets of hidden layers, once
    with BN layers and one without. How does BN impact the ability to learn deeper
    models?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`for`循环来训练具有1到20层隐藏层的CNN模型，一次使用BN层，一次不使用。BN如何影响学习更深层次模型的能力？
- en: '*MATH*: We used algebra to show that a linear layer followed by BN is equivalent
    to one linear layer. Try doing the same kind of math to show that BN followed
    by a linear layer *also* is equivalent to one different linear layer.'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MATH*: 我们使用代数证明了线性层后跟BN与一个线性层等价。尝试用同样的数学方法来证明BN后跟线性层*也*等价于另一个不同的线性层。'
- en: Re-implement the `ResidualBlockE` for fully connected layers instead of convolutional
    layers. Does making the fully connected model use residual connections still improve
    performance?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ResidualBlockE`重新实现为全连接层而不是卷积层。使用残差连接的全连接模型是否仍然能提高性能？
- en: Write a `for` loop to train a residual model with progressively more combinations
    of `ResidualBlockE` layers. Do the residual blocks allow you to train even deeper
    models?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`for`循环来训练一个使用更多组合的`ResidualBlockE`层的残差模型。残差块是否允许你训练更深层次的模型？
- en: Try creating a bidirectional LSTM layer. Do you get better or worse results?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试创建一个双向LSTM层。你得到更好的还是更差的结果？
- en: Try training various LSTM and GRU networks with varying numbers of layers and
    neurons, and compare them in terms of the time to reach a desired accuracy level.
    Do you see any relative pros or cons of each?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用不同层数和神经元的各种LSTM和GRU网络进行训练，并比较它们达到期望精度水平所需的时间。你看到任何相对的优缺点吗？
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A family of activation functions called rectified linear units requires fewer
    epochs to reach better accuracies.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组称为ReLU线性单元的激活函数需要更少的epoch就能达到更好的准确性。
- en: Inserting a normalization layer between every linear layer and activation function
    provides another boost.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个线性层和激活函数之间插入一个归一化层提供了另一个提升。
- en: Batch normalization works well for most problems, but with weight sharing (like
    recurrent architectures) and small batch sizes.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化对于大多数问题都工作得很好，但在权重共享（如循环架构）和小批量大小的情况下。
- en: Layer normalization is not always as accurate but is almost always a safe addition
    to any network design.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层归一化并不总是那么准确，但几乎总是任何网络设计中的一个安全添加。
- en: Skip connections provide a different strategy for combining layers that can
    create long and short paths through a network.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳跃连接提供了一种不同的策略来组合层，可以在网络中创建长路径和短路径。
- en: A 1 × 1 convolution can be used to adjust a convolutional layer’s number of
    channels.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1卷积可以用来调整卷积层的通道数。
- en: A design pattern called residual layers allows us to build deeper networks that
    improve accuracy by mixing skip connections and 1 × 1 convolutions.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种称为残差层的模式设计允许我们构建更深层次的网络，通过混合跳跃连接和1 × 1卷积来提高准确性。
- en: The tanh (⋅) and *σ*(⋅) activation functions are useful for creating gating
    mechanisms, which encode a new kind of prior.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tanh (⋅) 和 *σ*(⋅) 激活函数对于创建门控机制很有用，这些机制编码了一种新的先验。
- en: Gating is a strategy of forcing a neuron’s activation to be between 0 and 1
    and then multiplying another layer’s output by this value.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控是一种策略，强制神经元的激活介于0和1之间，然后将另一个层的输出乘以这个值。
- en: RNNs can be improved by using an approach called long short-term memory (LSTM),
    which is a careful application of gating.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用称为长短期记忆（LSTM）的方法可以改进RNN，这是一种谨慎的门控应用。
- en: '* * *'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ You can learn a lot by reading papers on neural networks from the 1990s. Even
    if they are not used today, the creativity of past solutions may give you good
    lessons and intuition for solving modern problems. As I’m writing this book, I’m
    working on research enabled by some mostly forgotten work from 1995.[↩](#fnref12)
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 通过阅读20世纪90年代的神经网络论文，你可以学到很多东西。即使它们今天不再使用，过去解决方案的创造性也可能为你解决现代问题提供良好的教训和直觉。在我撰写这本书的时候，我正在研究一些主要被遗忘的1995年的工作所启发的科研。[↩](#fnref12)
- en: ² V. Nair and G. E. Hinton, “Rectified linear units improve restricted Boltzmann
    machines,” *Proceedings of the 27th International Conference on Machine Learning*,
    pp. 807–814, 2010.[↩](#fnref13)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ² V. Nair 和 G. E. Hinton，“Rectified linear units improve restricted Boltzmann
    machines”，*第27届国际机器学习会议论文集*，第807-814页，2010。[↩](#fnref13)
- en: ³ A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectifier nonlinearities improve
    neural network acoustic models,” *Proceedings of the 30th International Conference
    on Machine Learning*, vol. 28, p. 6, 2013.[↩](#fnref14)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ³ A. L. Maas, A. Y. Hannun, 和 A. Y. Ng，"Rectifier nonlinearities improve neural
    network acoustic models"，《第 30 届国际机器学习会议论文集》，第 28 卷，第 6 页，2013。[↩](#fnref14)
- en: ⁴ Annoyingly, σ is used for denoting both standard deviation and the sigmoid
    activation function. You’ll have to use context to make sure you differentiate
    them.[↩](#fnref15)
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 让人烦恼的是，σ 用来表示标准差和 sigmoid 激活函数。你将不得不使用上下文来确保你能区分它们。[↩](#fnref15)
- en: ⁵ Yes, the names are confusing. I don’t like them either.[↩](#fnref16)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 是的，这些名字很令人困惑。我也不喜欢它们。[↩](#fnref16)
- en: ⁶ I prefer to put BN before the linear layer, as I think it tracks better with
    the intuition we’ve laid out. There are also niche cases with thousands of activations
    where BN before linear layers can perform better in my experience. But these are
    nitpicky details. I’d rather show you what is common than take a stand on obtuse
    particulars.[↩](#fnref17)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶ 我更喜欢将 BN 放在线性层之前，因为我认为它与我们已阐述的直觉更吻合。在我的经验中，也有成千上万的激活的特定案例，在这些案例中，线性层之前的 BN
    可以表现得更好。但这些只是些小细节。我更愿意展示常见的内容，而不是对那些晦涩的细节发表意见。[↩](#fnref17)
- en: ⁷ Defining what *complexity* or *capacity* means is possible, but it gets very
    technical very quickly. It is a rabbit hole too deep for this book.[↩](#fnref18)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷ 定义“复杂度”或“容量”的含义是可能的，但它很快就会变得非常技术性。这也像是一个对这个书来说太深奥的兔子洞。[↩](#fnref18)
- en: ⁸ The most prominent example is a network called DenseNet ([https://github.com/liuzhuang13/DenseNet](https://github.com/liuzhuang13/DenseNet)).[↩](#fnref19)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 最突出的例子是一个名为 DenseNet 的网络 ([https://github.com/liuzhuang13/DenseNet](https://github.com/liuzhuang13/DenseNet))[↩](#fnref19)
