- en: Part 1\.
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分
- en: '[Part 1](#part01) explores the map and reduce style of computing. We’ll introduce
    map and reduce, as well as the helper and convenience functions that you’ll need
    to get the most out of this style. In this section, we’ll also cover the basics
    of parallel computing. The tools and techniques in this part are useful for large
    data in categories 1 and 2: tasks that are both storable and computable locally,
    and tasks that are not storable locally but are still computable locally.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一部分](#part01) 探讨了 map 和 reduce 计算风格。我们将介绍 map 和 reduce，以及您需要充分利用这种风格的辅助和便利函数。在本节中，我们还将涵盖并行计算的基础知识。本部分中的工具和技术对于第
    1 类和第 2 类的大数据很有用：既可本地存储又可本地计算的任务，以及不可本地存储但仍然可本地计算的任务。'
- en: Chapter 1\. Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章. 简介
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Introducing the map and reduce style of programming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 map 和 reduce 编程风格
- en: Understanding the benefits of parallel programming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并行编程的好处
- en: Extending parallel programming to a distributed environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行编程扩展到分布式环境
- en: Parallel programming in the cloud
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云端的并行编程
- en: 'This book teaches a set of programming techniques, tools, and frameworks for
    mastering large datasets. Throughout this book, I’ll refer to the style of programming
    you’re learning as a *map and reduce* style. The map and reduce style of programming
    is one in which we can easily write parallel programs—programs that can do multiple
    things at the same time—by organizing our code around two functions: `map` and
    `reduce`. To get a better sense of why we’ll want to use a map and reduce style,
    consider this scenario:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍了一系列编程技术、工具和框架，用于掌握大数据集。在这本书中，我将把您正在学习的编程风格称为 *map 和 reduce* 风格。map 和 reduce
    编程风格是一种我们可以通过围绕两个函数 `map` 和 `reduce` 组织我们的代码来轻松编写并行程序——可以同时做很多事情的程序。为了更好地理解为什么我们会想使用
    map 和 reduce 风格，请考虑以下场景：
- en: '|  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: 'Two young programmers have come up with an idea for how to rank pages on the
    internet. They want to rank pages based on the importance of the other sites on
    the internet that link to them. They think the internet should be just like high
    school: the more the cool kids talk about you, the more important you are. The
    two young programmers love the idea, but how can they possibly analyze the entire
    internet?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 两位年轻的程序员提出了一种如何对互联网上的页面进行排名的想法。他们希望根据链接到他们的其他互联网站点的相关性来排名页面。他们认为互联网应该就像高中一样：越是有酷的孩子谈论你，你就越重要。这两位年轻的程序员喜欢这个想法，但他们如何可能分析整个互联网呢？
- en: '|  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: A reader well versed in Silicon Valley history will recognize this scenario
    as the Google.com origin story. In its early years, Google popularized a way of
    programming called *MapReduce* as a way to effectively process and rank the entire
    internet. This style was a natural fit for Google because
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对硅谷历史了如指掌的读者会认出这个场景是 Google.com 的起源故事。在其早期，谷歌通过推广一种称为 *MapReduce* 的编程方式，作为有效处理和排名整个互联网的方法。这种风格对谷歌来说是一种自然的选择，因为
- en: Both of Google’s founders were math geeks, and MapReduce has its roots in math.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谷歌的两位创始人都是数学爱好者，MapReduce 的根源在于数学。
- en: Map and reduce-centric programming results in simple parallelization when compared
    with a more traditional style of programming.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与更传统的编程风格相比，以 map 和 reduce 为中心的编程导致简单的并行化。
- en: '|  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**`map` and `reduce` vs. MapReduce**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**`map` 和 `reduce` 与 MapReduce**'
- en: I’m going to refer to a map and reduce style of programming a lot in this book.
    Indeed, this style is the primary means through which I’ll be teaching you how
    to scale up your programs beyond your laptop. Though this style is similar in
    name and functionality to MapReduce, it is distinct from and more general than
    MapReduce. MapReduce is a framework for parallel and distributed computing. The
    map and reduce style is a style of programming that allows programmers to run
    their work in parallel with minimal rewriting and extend this work to distributed
    workflows, possibly using MapReduce, or possibly using other means.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我将多次提到 map 和 reduce 编程风格。实际上，这种风格是我将用来教您如何将程序扩展到笔记本电脑之外的主要手段。尽管这种风格在名称和功能上与
    MapReduce 相似，但它与 MapReduce 不同，并且更通用。MapReduce 是并行和分布式计算的一个框架。map 和 reduce 风格是一种编程风格，允许程序员以最小的重写并行运行他们的工作，并将这项工作扩展到分布式工作流程，可能使用
    MapReduce，也可能使用其他方法。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In this book, we’ll tackle the same issues Google tackled in their early stages.
    We’ll look at a style of programming that makes it easy to take a good idea and
    scale it up. We’ll look at a way of programming that makes it easy to go from
    doing work as an individual to doing work on a team, or from doing work on your
    laptop to doing work in a distributed parallel environment. In other words, we’ll
    look at how to master large datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将解决谷歌早期阶段所面临的问题。我们将研究一种使将好主意扩展变得容易的编程风格。我们将研究一种使从个人工作到团队合作，或从笔记本电脑工作到分布式并行环境工作变得容易的编程方式。换句话说，我们将研究如何掌握大型数据集。
- en: 1.1\. What you’ll learn in this book
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 你将在本书中学到什么
- en: In this book, you’ll learn a style of programming that makes parallelization
    easy. You’ll learn how to write scalable, parallel code that will work just as
    well on one machine as it will on thousands. You’ll learn how to
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你将学习一种使并行化变得容易的编程风格。你将学习如何编写可扩展的、并行化的代码，它在一台机器上运行的效果和在成千上万台机器上运行的效果一样好。你将学习如何
- en: chunk large problems into small pieces
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大型问题分解成小块
- en: use the `map` and `reduce` functions
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 和 `reduce` 函数
- en: run programs in parallel on your personal computer
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的个人电脑上并行运行程序
- en: run programs in parallel in distributed cloud environments
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式云环境中并行运行程序
- en: 'and you’ll learn two popular frameworks for working with large datasets: Apache
    Hadoop and Apache Spark.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将学习两个用于处理大型数据集的流行框架：Apache Hadoop 和 Apache Spark。
- en: This book is for the programmer who can write working data-transformation programs
    already and now needs to scale those programs up. They need to be able to work
    with more data and to do it faster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为那些已经能够编写工作数据转换程序，现在需要扩展这些程序的程序员而写的。他们需要能够处理更多的数据，并且更快地完成这些工作。
- en: 1.2\. Why large datasets?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 为什么需要大型数据集？
- en: You’ve probably heard conversations about an amorphous set of problems in modern
    computing that revolve around the notion of *big data*. Big data tends to mean
    different things to different people. I find that most people use that phrase
    to mean that the data “feels” big—it’s uncomfortable to work with or unwieldy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经听说过关于现代计算中一系列无定形问题的对话，这些问题围绕着 *大数据* 的概念。大数据对不同的人意味着不同的事情。我发现大多数人使用这个短语来表示数据“感觉”很大——它难以处理或难以驾驭。
- en: 'Because one of the goals of this book is to get you comfortable with any size
    dataset, we’ll work with *large datasets*. As I think of it, large dataset problems
    come in three sizes:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为本书的一个目标是要让你对任何大小的数据集都感到舒适，我们将使用 *大型数据集* 进行工作。在我看来，大型数据集问题可以分为三种规模：
- en: The data can both fit on and be processed on a personal computer.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据可以适应并处理在个人电脑上。
- en: The solution for the problem can be executed from a personal computer, but the
    data can’t be stored on a personal computer.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决这个问题的方案可以从个人电脑上执行，但数据不能存储在个人电脑上。
- en: The solution for the problem can’t be executed on a personal computer, and the
    data can’t be stored on one either.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决这个问题的方案不能在个人电脑上执行，数据也不能存储在个人电脑上。
- en: You likely already know how to solve problems that fall in the first category.
    Most problems—especially those that are used to teach programming—fall into this
    first category. The second group of problems is a bit harder. They require a technique
    called *parallel computing* that allows us to get the most out of our hardware.
    Lastly, we have the third group of problems. These problems are expensive, requiring
    either more money or more time to solve. To solve them, we’ll want to use a technique
    called *distributed computing*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道如何解决第一类问题。大多数问题——尤其是那些用来教授编程的问题——都属于第一类。第二组问题稍微难一些。它们需要一种称为 *并行计算* 的技术，使我们能够充分利用我们的硬件。最后，我们有第三组问题。这些问题成本高昂，需要更多的资金或更多的时间来解决。为了解决这些问题，我们希望使用一种称为
    *分布式计算* 的技术。
- en: '|  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Dask—A different type of distributed computing**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dask—一种不同类型的分布式计算**'
- en: The map and reduce style of programming puts data at the forefront and is excellent
    for working with data, from small data transformations up to large distributed
    data stores.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Map 和 Reduce 编程风格将数据置于首位，非常适合处理数据，从小型数据转换到大型分布式数据存储。
- en: If you aren’t interested in learning a style of programming that will make your
    Python code easier to read and easier to scale, but you still want to be able
    to manage large datasets, one tool out there for you is Dask. Dask is a Python
    framework for distributed data frames with a NumPy and pandas look-alike API.
    If that sounds like something you’re interested in, I recommend *Data Science
    with Python and Dask*, by Jesse Daniel (Manning, 2019; [http://mng.bz/ANxg](http://mng.bz/ANxg)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想学习一种会使你的Python代码更容易阅读和扩展的编程风格，但仍然想能够管理大型数据集，那么市面上有一款适合你的工具，那就是Dask。Dask是一个具有类似NumPy和pandas
    API的Python分布式数据框架。如果你对此感兴趣，我推荐Jesse Daniel的《Python和Dask数据科学》（Manning，2019；[http://mng.bz/ANxg](http://mng.bz/ANxg)）。
- en: '|  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Through this book, you’ll learn a style of programming that allows you to write
    code in the same way for problems of all three sizes. You’ll also learn about
    parallel computing and two distributed computing frameworks (Hadoop and Spark),
    and we’ll explore how to use those frameworks in a distributed cloud environment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，你将学会一种编程风格，它允许你以相同的方式为所有三种规模的问题编写代码。你还将了解并行计算以及两个分布式计算框架（Hadoop和Spark），我们将探讨如何在分布式云环境中使用这些框架。
- en: 1.3\. What is parallel computing?
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3. 什么是并行计算？
- en: Parallel computing, which I’ll also refer to as *parallel programming* and *parallelization*,
    is a way to get your computer to do multiple things at once. For example, referring
    to the scenario you saw earlier, our young programmers are going to need to process
    more than one web page at a time; otherwise, they might never finish—there are
    a lot of web pages. Even processing one page per half second wouldn’t bring them
    to 200,000 pages a day. To scrape and analyze the entire internet, they’re going
    to need to be able to scale up their processing. Parallel computing will allow
    them to do just that.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算，我还会将其称为*并行编程*和*并行化*，是一种让你的计算机同时做很多事情的方法。例如，参考你之前看到的场景，我们的年轻程序员将需要一次处理多个网页；否则，他们可能永远无法完成——有太多的网页。即使每半秒处理一个网页，他们每天也无法达到20万个页面。为了抓取和分析整个互联网，他们需要能够扩展他们的处理能力。并行计算将使他们能够做到这一点。
- en: 1.3.1\. Understanding parallel computing
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1. 理解并行计算
- en: 'To understand parallel programming, let’s first talk about what happens in
    standard procedural programming. The standard procedural programming workflow
    typically looks like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解并行编程，让我们首先谈谈标准过程式编程中会发生什么。标准过程式编程的工作流程通常看起来像这样：
- en: A program starts to run.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序开始运行。
- en: The program issues an instruction.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序发出一条指令。
- en: That instruction is executed.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那条指令被执行了。
- en: Steps 2 and 3 are repeated.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2和步骤3被重复执行。
- en: The program finishes running.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序运行结束。
- en: This is a straightforward way of programming; however, it limits us to executing
    one instruction at a time ([figure 1.1](#ch01fig01)). Steps 2 and 3 need to resolve
    before we can move on to Step 4\. And Step 4 routes us back to Steps 2 and 3,
    leaving us in the same pickle.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种直接的编程方式；然而，它限制了我们一次只能执行一条指令（[图1.1](#ch01fig01)）。步骤2和步骤3需要解决，我们才能继续到步骤4。而步骤4将我们带回到步骤2和步骤3，使我们陷入同样的困境。
- en: Figure 1.1\. The procedural computing process involves issuing instructions
    and resolving them in sequence.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1. 过程式计算过程涉及发出指令并按顺序解决它们。
- en: '![](01fig01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig01.jpg)'
- en: In a standard linear program, if the instructions in Step 2 take a long time
    to execute, then we won’t be able to move on to the next section of the problem.
    Imagine what this looks like for our young programmers trying to scrape the entire
    internet. How many of their instructions are going to be “scrape web page abc.com/xyz”?
    Probably a lot. What’s more, we know that the scraping of one web page (like the
    Amazon homepage, for instance) is in no way going to alter the content of other
    web pages (such as the *New York Times* homepage).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准线性程序中，如果步骤2中的指令执行时间很长，那么我们就无法继续进行问题的下一个部分。想象一下我们的年轻程序员试图抓取整个互联网时这会是什么样子。他们有多少条指令会是“抓取网页abc.com/xyz”？可能很多。更重要的是，我们知道抓取一个网页（比如亚马逊首页）根本不会以任何方式改变其他网页的内容（比如《纽约时报》首页）。
- en: 'Parallel programming allows us to execute all of these similar and independent
    steps simultaneously. In parallel programming, our workflow is going to look more
    like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程允许我们同时执行所有这些相似且独立的步骤。在并行编程中，我们的工作流程将看起来更像是这样：
- en: A program starts to run.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序开始运行。
- en: The program divides up the work into chunks of instructions and data.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序将工作划分为指令和数据块。
- en: Each chunk of work is executed independently.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个工作块都是独立执行的。
- en: The chunks of work are reassembled.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作块被重新组装。
- en: The program finishes running.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序运行结束。
- en: By programming this way, we free ourselves from the instruction-execution loop
    we were trapped in before ([figure 1.2](#ch01fig02)). Now we can split our work
    up into as many chunks as we’d like, as long as we have a way of processing them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式编程，我们摆脱了之前陷入的指令执行循环([图1.2](#ch01fig02))。现在我们可以将我们的工作分成我们想要的任何多块，只要我们有处理它们的方法。
- en: Figure 1.2\. The parallel computing process divides work into chunks that can
    be processed separately from one another and recombined when finished.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2。并行计算过程将工作划分为可以单独处理并完成后重新组合的块。
- en: '![](01fig02.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](01fig02.jpg)'
- en: This process would be much better for the young programmers wishing to scrape
    the entire internet. They still need to find a way to get enough computing resources
    to process all of the chunks, but every time they acquire a new machine, they
    make their process that much faster. And indeed, even early-stage Google was running
    on a cluster of thousands of computers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望抓取整个互联网的年轻程序员来说，这个过程会更好。他们仍然需要找到一种方法来获取足够的计算资源来处理所有这些块，但每次他们获得一台新机器，他们的过程就会变得更快。实际上，即使是早期的谷歌也是在成千上万的计算机集群上运行的。
- en: 1.3.2\. Scalable computing with the map and reduce style
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2. 使用map和reduce风格的可扩展计算
- en: When we think about the map and reduce style of computing, it’s important to
    do so in the context of both the size of our data and the capacity of the compute
    resources available to us ([figure 1.3](#ch01fig03)). With normal-sized data—which
    allows us to use personal computer-scale resources to work on data we can store
    on a personal computer—we can rely on the fundamentals of the map and reduce style
    and standard Python code. In this area, we won’t see much difference from other
    styles of programming.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考map和reduce风格的计算时，重要的是要在我们数据的大小和我们可用的计算资源容量的背景下进行思考([图1.3](#ch01fig03))。对于正常大小的数据——这允许我们使用个人电脑规模的资源来处理我们可以存储在个人电脑上的数据——我们可以依赖map和reduce风格的基本原理和标准Python代码。在这个领域，我们不会看到与其他编程风格太大的区别。
- en: 'Figure 1.3\. We can think of the map and reduce style of programming as a construction
    project: from blueprints, which help us organize our work; to the transformation
    of raw material; to the assembly of parts into a final product.'
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3。我们可以将map和reduce风格的编程视为一个建设项目：从蓝图，帮助我们组织工作；到原材料的转换；到将部件组装成最终产品。
- en: '![](01fig03_alt.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](01fig03_alt.jpg)'
- en: Moving up in size of data, we arrive at a place where we can use our personal
    computer hardware to process the data, but we’re having trouble storing the data
    on a personal computer. At this point, we could, if we wanted to, work on our
    job in a cluster, but it’s not a necessity. Here, the benefits of the map and
    reduce style start to become apparent. We can use a slightly modified version
    of our code from the smaller sized data to work on data in this size category.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据规模的增加，我们到达了一个可以使用个人电脑硬件处理数据的地方，但我们却在个人电脑上存储数据时遇到了麻烦。在这种情况下，如果我们愿意，我们可以在集群中工作我们的工作，但这不是必需的。在这里，map和reduce风格的好处开始变得明显。我们可以使用稍微修改过的代码来处理这个规模的数据。
- en: And finally, we arrive at the final and largest category of data. This is data
    that we need to both process and store in a distributed environment. Here, we
    can use distributed computing frameworks such as Hadoop and Spark. And although
    we can’t use the exact same code, we can use the principles and patterns from
    the smaller sizes. We’ll often also need to use a cloud computing service, such
    as AWS.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了数据的最最终和最大的类别。这是我们需要在分布式环境中处理和存储的数据。在这里，我们可以使用像Hadoop和Spark这样的分布式计算框架。尽管我们不能使用完全相同的代码，但我们可以使用从小规模数据中提取的原则和模式。我们通常还需要使用云计算服务，如AWS。
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Other large data technologies: Splunk, Elasticsearch, Pig, and Hive**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他大型数据技术：Splunk、Elasticsearch、Pig和Hive**'
- en: Because this book focuses on scalable workflows, I intentionally omitted big
    data tools that only make sense to operate once you’re already in a high-volume
    environment, including Splunk, Elasticsearch, Apache Pig, and Apache Hive. The
    latter two, built on the Hadoop stack, are natural bedfellows with Hadoop and
    Spark. If you’re operating with a large volume of data, investigating these tools
    is well worth your while.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这本书专注于可扩展的工作流程，所以我故意省略了那些只有在高容量环境中才有意义的大数据工具，包括Splunk、Elasticsearch、Apache
    Pig和Apache Hive。后两者基于Hadoop堆栈，与Hadoop和Spark是天然的伙伴。如果你正在处理大量数据，调查这些工具是非常值得的。
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We can see this at play in [figure 1.3](#ch01fig03). [Figure 1.3](#ch01fig03)
    shows how the techniques taught in this book match up against the various sizes
    of data and the compute resources available. We begin by covering techniques that
    you can use on your laptop or personal computer: the built-in map and reduce and
    parallel computing abilities of Python. In the final two sections (from [chapter
    7](kindle_split_017.html#ch07) and on), we cover distributed computing frameworks
    such as Hadoop and Spark, as well as how to deploy these services on the cloud
    using Amazon Web Services EMR.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图1.3](#ch01fig03)中看到这一点。图1.3显示了本书中教授的技术如何与各种数据大小和可用的计算资源相匹配。我们首先介绍你可以在笔记本电脑或个人计算机上使用的技巧：Python内置的map和reduce以及并行计算能力。在最后两个部分（从[第7章](kindle_split_017.html#ch07)开始），我们将介绍分布式计算框架，如Hadoop和Spark，以及如何使用Amazon
    Web Services EMR在云中部署这些服务。
- en: 1.3.3\. When to program in a map and reduce style
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3\. 何时使用map和reduce风格编程
- en: The map and reduce style of programming is applicable everywhere, but its specific
    strengths are in areas where you may need to scale. Scaling means starting with
    a small application, such as a little game you might build on your laptop in an
    evening as a pet project, and applying it to a much larger use case, such as a
    viral game that *everyone* is playing on their cell phones.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: map和reduce风格的编程适用于任何地方，但它的具体优势在于你可能需要扩展的领域。扩展意味着从一个小的应用程序开始，比如你晚上在笔记本电脑上作为一个宠物项目构建的小游戏，并将其应用到更大的用例中，比如每个人都在他们的手机上玩的热门游戏。
- en: 'Consider one small step in our hypothetical game: improving the AI. Say we
    have an AI opponent against which all the players compete, and we want the AI
    to improve every 1,000 matches. At first, we’ll be able to update our AI on a
    single machine. After all, we only have 1,000 matches, and it’s trivial to process
    them. Even as the number of players picks up, we’ll only have to run this improvement
    every few hours. Eventually, however, if our game gets popular enough, we’ll have
    to dedicate several machines to this task—the amount of information they’ll need
    to process will be larger (there will be more matches in the match history), and
    they’ll need to process the information faster (because the rate of plays will
    be faster). This would be an excellent application for a map and reduce style
    because we could easily modify our code to be parallel, allowing us to scale our
    AI improvements up to any number of users.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们假设的游戏中的一小步：改进AI。比如说，我们有一个所有玩家都在竞争的AI对手，我们希望AI每1000场比赛后都能改进。一开始，我们可以在单台机器上更新我们的AI。毕竟，我们只有1000场比赛，处理它们是微不足道的。即使玩家数量增加，我们也只需要每隔几个小时运行这个改进。然而，最终，如果我们的游戏足够受欢迎，我们就需要为这个任务分配几台机器——他们需要处理的信息量会更大（比赛历史中会有更多比赛），他们需要更快地处理信息（因为游戏速度会更快）。这将是一个非常适合map和reduce风格的绝佳应用，因为我们可以轻松修改我们的代码以实现并行，这样我们就可以将我们的AI改进扩展到任何数量的用户。
- en: 1.4\. The map and reduce style
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. map和reduce风格
- en: 'The parallel programming workflow has three parts that distinguish it from
    the standard linear workflow:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程工作流程有三个部分，使其与标准线性工作流程区分开来：
- en: Divide the work into chunks.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将工作分成块。
- en: Work on those chunks separately.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别处理这些块。
- en: Reassemble the work.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新组装工作。
- en: In this book, we’ll let the functions `map` and `reduce` handle these three
    parts for us.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将让函数`map`和`reduce`为我们处理这三个部分。
- en: 1.4.1\. The map function for transforming data
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.1\. 用于转换数据的map函数
- en: '`map` is a function we’ll use to transform sequences of data from one type
    to another ([figure 1.4](#ch01fig04)). The function gets its name from mathematics,
    where some mathematicians think of functions as rules for taking an input and
    returning the single corresponding output. Considering again our young and ambitious
    programmers, they may want to `map` a sequence of web pages (or the sequence of
    all web pages) into the URLs that those pages contain. They could then use those
    URLs to see which pages were linked to most often and by whom.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`是我们将用于将数据序列从一种类型转换为另一种类型的函数（[图1.4](#ch01fig04)）。这个函数的名字来源于数学，在那里一些数学家认为函数是获取输入并返回相应输出的规则。再次考虑我们年轻而有抱负的程序员，他们可能想要将网页序列（或所有网页的序列）`map`到包含在这些页面中的URL上。然后，他们可以使用这些URL来查看哪些页面被链接得最频繁以及由谁链接。'
- en: Figure 1.4\. We can use the `map` function to transform a sequence of data from
    one type to another, such as transforming web page URLs into lists of links found
    on those pages.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4\. 我们可以使用`map`函数将数据序列从一种类型转换为另一种类型，例如将网页URL转换为包含在这些页面上的链接列表。
- en: '![](01fig04_alt.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig04_alt.jpg)'
- en: A key thing to remember about `map` is that it always retains the same number
    of objects in the output as were provided in the input. For example, if we wanted
    to get the outbound links on 100,000 websites with `map`, then the resulting data
    structure would be 100,000 lists of links.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`map`的一个关键点是，它总是保留与输入中提供的相同数量的对象。例如，如果我们想用`map`获取10万个网站的出站链接，那么结果数据结构将是10万个链接列表。
- en: '|  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: '`map` and `reduce` have their roots in a style of programming called declarative
    programming. Declarative programming focuses on explaining the logic of our code
    and not on specifying low-level details. That’s why scaling our code is natural
    in the map and reduce style: the logic stays the same, even if the size of the
    problem changes.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`和`reduce`的根源在于一种称为声明式编程的编程风格。声明式编程侧重于解释代码的逻辑，而不是指定低级细节。这就是为什么在map和reduce风格中扩展我们的代码是自然的：逻辑保持不变，即使问题的大小发生变化。'
- en: '|  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'It’s worth taking a look at a small example of `map` in action now because
    of how fundamental it is to what we’ll be doing throughout this book. Let’s imagine
    we want to add seven to a sequence of four numbers: –1, 0, 1, and 2\. To do this,
    we write a small function called `add_seven` that takes a number `n` and returns
    `n+7`. To do this for our sequence of numbers, we’d simply call `map` on `add_seven`
    and our sequence ([figure 1.5](#ch01fig05)).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`map`函数在我们整本书中的操作中起着基础性作用，现在看看一个`map`函数的简单示例是值得的。让我们设想我们想要将7加到四个数字的序列中：-1,
    0, 1, 和 2。为此，我们编写了一个名为`add_seven`的小函数，它接受一个数字`n`并返回`n+7`。为了对我们数字序列进行此操作，我们只需在`add_seven`和我们的序列上调用`map`（[图1.5](#ch01fig05)）。
- en: Figure 1.5\. A basic use of `map` would be to increment a sequence of numbers,
    such as changing –1, 0, 1, and 2 into 6, 7, 8, and 9.
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5\. `map`的基本用法之一是增加数字序列，例如将-1, 0, 1, 和 2转换为6, 7, 8, 和 9。
- en: '![](01fig05_alt.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig05_alt.jpg)'
- en: 'You’ll note that, like we touched on previously, we have the same number of
    inputs (4) as outputs (4). Also, these inputs and outputs have a direct 1 to 1
    relationship: a particular output corresponds to each and every input.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，就像我们之前提到的，我们有与输出（4）相同的输入（4）。此外，这些输入和输出有直接的1对1关系：特定的输出对应于每个输入。
- en: 1.4.2\. The reduce function for advanced transformations
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.2\. 高级变换的reduce函数
- en: 'If we want to take that sequence and turn it into something of a different
    length, we’ll need our other critical function: `reduce`. `reduce` allows us to
    take a sequence of data and transform it into a data structure of any shape or
    size ([figure 1.6](#ch01fig06)). For example, if our programmers wanted to take
    those links and turn them into frequency counts—finding which pages are linked
    to the most—they would need to use `reduce`, because it is possible that the number
    of pages linked to is different from the number of pages crawled. We can easily
    imagine that 100 web pages might link to anywhere between 0 and 1 million external
    pages, depending on what the web pages in question are.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将那个序列转换成不同长度的东西，我们需要我们的另一个关键函数：`reduce`。`reduce`允许我们将数据序列转换成任何形状或大小的数据结构（[图1.6](#ch01fig06)）。例如，如果我们的程序员想要将那些链接转换成频率计数——找出哪些页面被链接得最多——他们就需要使用`reduce`，因为链接的页数可能与爬取的页数不同。我们可以很容易地想象，100个网页可能链接到0到100万个外部页面，这取决于相关的网页内容。
- en: 'Figure 1.6\. We can use the `reduce` function to turn a sequence of data of
    one type into something else: another sequence or even a primitive.'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6\. 我们可以使用`reduce`函数将一种类型的数据序列转换为另一种类型：另一个序列或甚至是一个原始数据类型。
- en: '![](01fig06_alt.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig06_alt.jpg)'
- en: We can even use `reduce` to turn a sequence of data into a primitive data type
    if we’d like, such as an integer or a string. For example, we could use `reduce`
    to find the number of outbound links on 100 web pages (an integer) or we could
    use it to find the longest word in a long text document, such as a book (a string).
    In this way, `reduce` is a lot more flexible than `map`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，甚至可以使用`reduce`将数据序列转换为原始数据类型，例如整数或字符串。例如，我们可以使用`reduce`来找出100个网页上的出站链接数量（一个整数），或者我们可以用它来找出长文本文档中最长的单词，例如一本书（一个字符串）。这样，`reduce`比`map`更加灵活。
- en: 1.4.3\. Map and reduce for data transformation pipelines
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.3\. 用于数据转换管道的Map和Reduce
- en: Often, we’ll want to use `map` and `reduce` together, one right after another.
    This pattern gives rise to the *MapReduce* programming pattern. The MapReduce
    programming pattern relies on the `map` function to transform some data into another
    type of data and then uses the `reduce` function to combine that data. A mathematical
    example might be taking the sum of the greatest prime factor of a sequence of
    numbers. We can use `map` to transform each number into its greatest prime factor
    and then use `reduce` to take their sum. A more practical example may be finding
    the longest word on a sequence of web pages, when all we have is the URLs. We
    can use `map` to turn the URLs into text and `reduce` to find the longest word
    ([figure 1.7](#ch01fig07)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望将`map`和`reduce`一起使用，一个紧接着另一个。这种模式产生了*MapReduce*编程模式。MapReduce编程模式依赖于`map`函数将某些数据转换为另一种类型的数据，然后使用`reduce`函数来合并这些数据。一个数学示例可能是计算一系列数字的最大质因数之和。我们可以使用`map`将每个数字转换为它的最大质因数，然后使用`reduce`来计算它们的和。一个更实际的例子可能是找出一系列网页中最长的单词，当我们只有URL时。我们可以使用`map`将URL转换为文本，然后使用`reduce`来找出最长的单词（[图1.7](#ch01fig07)）。
- en: Figure 1.7\. The functions `map` and `reduce` are often used together to perform
    complex transformations of large amounts of data quickly.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7\. 函数`map`和`reduce`通常一起使用，以快速执行大量数据的复杂转换。
- en: '![](01fig07_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig07_alt.jpg)'
- en: 1.5\. Distributed computing for speed and scale
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 分布式计算以实现速度和规模
- en: To get the most out of parallel programming, we need to be working in a distributed
    environment, that is, an environment where it’s possible to spread the workload
    out across several machines. Consider the following scenario.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用并行编程，我们需要在一个分布式环境中工作，也就是说，一个可以在多台机器之间分配工作负载的环境。考虑以下场景。
- en: '|  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A financial trading firm has come up with a way of forecasting the next day’s
    market activity based on the overnight taxi and rideshare traffic in New York
    City, combined with the morning’s fish prices. The firm’s simulation is perfect,
    but it takes five hours to run. The traffic results are considered final at 3:00
    a.m., and the markets don’t open until 9:00 a.m. That would give it plenty of
    time, except the fish prices aren’t available until 6:00 a.m. on some days. How
    can the trading firm get its model to run in time?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一家金融交易公司提出了一种基于纽约市夜间出租车和拼车交通以及早晨的鱼价来预测第二天市场活动的方法。公司的模拟非常完美，但需要五小时才能运行。交通结果在凌晨3:00被视为最终结果，而市场直到上午9:00才开放。这本来会留出足够的时间，但有些日子鱼价直到早上6:00才有。交易公司如何让它的模型及时运行？
- en: '|  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the above scenario, our traders are out of luck if they are hoping to input
    the actual fish price data for that day. Lucky for them, it’s possible to distribute
    this problem over a network of computers and have them each compute a separate
    scenario. That way, no matter what the fish price data says, they’ll already have
    the results on hand.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述场景中，如果我们的交易员希望输入当天的实际鱼价数据，他们将不会有什么运气。幸运的是，可以将这个问题分布到计算机网络中，让它们各自计算一个不同的场景。这样，无论鱼价数据如何，他们都已经有了结果在手。
- en: Distributed computing is an extension of parallel computing in which the compute
    resource we are dedicating to work on each chunk of a given task is its own machine.
    This can get complex. All of these machines have to communicate with the machine
    that splits the tasks up and combines the results. The benefit is that many, many
    complex tasks—like financial simulations—can be performed simultaneously and have
    their results brought together ([figure 1.8](#ch01fig08)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算是并行计算的一种扩展，其中我们为每个任务块分配的计算资源是其自己的机器。这可能会变得复杂。所有这些机器都必须与分割任务并合并结果的机器进行通信。好处是，许多复杂的任务——如金融模拟——可以同时执行，并将结果汇总（[图1.8](#ch01fig08)）。
- en: Figure 1.8\. We can use distributed computing to run sophisticated scenarios
    simultaneously and return the results to a single location.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8\. 我们可以使用分布式计算同时运行复杂的场景，并将结果返回到单个位置。
- en: '![](01fig08_alt.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig08_alt.jpg)'
- en: Importantly, for problems that we can execute in a distributed manner, we can
    often speed them up simply by distributing the work over more and more machines
    or by improving the capability of the machines that the tasks are being distributed
    across. Which, if either, solution is going to result in faster code depends on
    the problem. The good news for our financial trading firm, though, is that they
    probably have the money for either one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，对于我们可以以分布式方式执行的问题，我们通常可以通过将工作分配到越来越多的机器或提高分配给任务的机器的能力来加快它们的速度。哪种解决方案会导致代码运行更快取决于问题。然而，对我们金融交易公司来说的好消息是，他们可能有钱实现其中任何一个方案。
- en: '1.6\. Hadoop: A distributed framework for map and reduce'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6\. Hadoop：用于映射和归约的分布式框架
- en: To learn more about distributed computing, we’ll first look at a specific form
    of distributed computing called *Apache Hadoop*, or simply *Hadoop*. Hadoop was
    designed as an open source implementation of Google’s original MapReduce framework
    and has evolved into distributed computing software that is used widely by companies
    processing large amounts of data. Examples of such companies include Spotify,
    Yelp, and Netflix.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于分布式计算的信息，我们首先将查看一种称为*Apache Hadoop*的特定分布式计算形式，或简单地称为*Hadoop*。Hadoop被设计为谷歌原始MapReduce框架的开源实现，并已发展成为被处理大量数据的公司广泛使用的分布式计算软件。此类公司的例子包括Spotify、Yelp和Netflix。
- en: '|  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Scenario
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: 'Spotify is a cloud music provider that has two signature offerings: free music
    over the internet and customized, curated playlists that help you discover new
    music. These custom playlists work by comparing songs you like and listen to with
    what other users listen to, then suggesting songs that you may have missed. The
    challenge is that Spotify has hundreds of millions of users. How can Spotify compare
    the musical taste of all these users?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Spotify是一家云音乐提供商，提供两项标志性服务：通过互联网提供免费音乐和定制、精选的播放列表，帮助您发现新音乐。这些定制播放列表通过比较您喜欢和收听的歌曲与其他用户收听的内容，然后建议您可能错过的歌曲来实现。挑战在于Spotify有数亿用户。Spotify如何比较所有这些用户的音乐品味？
- en: '|  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: To create their music recommendations, Spotify uses Hadoop. Hadoop allows Spotify
    to store its listening logs (petabytes of information) on a distributed filesystem
    and then regularly analyze that information. The volume of data is the reason
    Hadoop is so valuable.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建他们的音乐推荐，Spotify使用Hadoop。Hadoop允许Spotify在其分布式文件系统上存储其收听日志（PB级信息），然后定期分析这些信息。数据量是Hadoop如此有价值的原因。
- en: If Spotify had a smaller amount of information, it could use a relational database.
    With many petabytes of data, though, that becomes infeasible. For comparison,
    since we’re talking about music, a 10 PB playlist of MP3s would take about 20,000
    years to play. If someone started playing it before humans domesticated livestock,
    you could finish the playlist in your lifetime.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spotify拥有较少的信息，它可以使用关系数据库。然而，由于我们谈论的是音乐，一个包含10PB MP3的10PB播放列表需要大约20,000年才能播放。如果有人在大约人类驯化家畜之前开始播放，您可以在一生中完成这个播放列表。
- en: Using Hadoop means that the data storage and the processing both can be distributed,
    so Spotify doesn’t have to pay attention, necessarily, to how much data it has.
    As long as it can pay for new machines to store the data on, it can pull them
    together with Hadoop ([figure 1.9](#ch01fig09)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop意味着数据存储和处理都可以分布式进行，因此Spotify不必 necessarily关注它有多少数据。只要它能支付新机器来存储数据，它就可以使用Hadoop将它们聚集在一起（[图1.9](#ch01fig09)）。
- en: Figure 1.9\. Hadoop allows us to store data on a distributed file system of
    nodes and analyze the data with a highly parallel MapReduce process.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9\. Hadoop允许我们在分布式文件系统的节点上存储数据，并使用高度并行的MapReduce过程分析数据。
- en: '![](01fig09_alt.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig09_alt.jpg)'
- en: 1.7\. Spark for high-powered map, reduce, and more
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7\. Spark用于强大的映射、减少以及其他功能
- en: We’ll also touch on *Apache Spark* (or simply *Spark*) as a distributed computing
    framework. Spark is something of a successor to the Apache Hadoop framework that
    does more of its work in memory instead of by writing to files. The memory referenced
    in this case is not the memory of a single machine but, rather, the memories of
    a cluster of machines.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将简要介绍 *Apache Spark*（或简称 *Spark*）作为一种分布式计算框架。Spark某种程度上是Apache Hadoop框架的继承者，它更多地通过内存操作来完成工作，而不是通过写入文件。这里的内存指的是一个集群中多台机器的内存，而不是单台机器的内存。
- en: The result is that Apache Spark can be much faster than Apache Hadoop. By Apache’s
    own estimations, Spark can run more than 100 times faster than Hadoop, though
    both will significantly increase your speed when compared to a linear process
    on a single machine. Spark also has some nice libraries for machine learning that
    we’ll take a look at.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是Apache Spark可以比Apache Hadoop快得多。根据Apache自己的估计，Spark可以比Hadoop快100多倍，尽管与单台机器上的线性过程相比，两者都会显著提高速度。Spark还有一些用于机器学习的优秀库，我们将对其进行探讨。
- en: Ultimately, whether you decide you want to use Spark or Hadoop for your work
    will be up to you. Spark, like Hadoop, is being used by a lot of large organizations,
    such as Amazon, eBay, and even NASA. Both are excellent choices.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你决定是否想在工作中使用Spark或Hadoop将取决于你。Spark，就像Hadoop一样，被许多大型组织使用，例如亚马逊、eBay，甚至是NASA。两者都是非常好的选择。
- en: 1.8\. AWS Elastic MapReduce—Large datasets in the cloud
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8\. AWS弹性MapReduce—云中的大数据集
- en: 'One of the most popular ways to implement Hadoop and Spark today is through
    Amazon’s Elastic MapReduce. Elastic MapReduce (EMR) joins the MapReduce framework
    we’ve been talking about with Amazon’s “elastic” series of cloud computing APIs,
    such as Elastic Cloud Compute (EC2). These tools have a very relevant purpose:
    allowing software developers to focus on writing code and not on the procurement
    and maintenance of hardware.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 目前实现Hadoop和Spark最受欢迎的方式之一是通过亚马逊的弹性MapReduce。弹性MapReduce（EMR）将我们一直在讨论的MapReduce框架与亚马逊的“弹性”系列云计算API相结合，例如弹性云计算（EC2）。这些工具具有非常相关的目的：让软件开发者能够专注于编写代码，而不是关注硬件的采购和维护。
- en: In traditional distributed computing, an individual—or more often a company—has
    to own all the machines. They then have to unite those machines into a cluster,
    ensure those machines stay up to date with all the latest software, and otherwise
    ensure that all of the machines stay running. With EMR, all we need to dabble
    in distributed computing is some spare change, and Amazon handles the rest.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的分布式计算中，个人或更常见的是公司必须拥有所有机器。然后他们必须将这些机器联合成一个集群，确保这些机器与所有最新的软件保持同步，并确保所有机器都在运行。使用EMR，我们只需要一些零钱就可以涉足分布式计算，亚马逊处理其余的事情。
- en: Because EMR allows us to run distributed jobs on demand, without having to own
    our own cluster, we can expand the scope of problems we want to solve with parallel
    programming. EMR allows us to tackle small problems with parallel programming
    because it makes it cost effective. We don’t have to make an up-front investment
    in servers to prototype new ideas. EMR also allows us to tackle large problems
    with parallel programming because we can procure as many resources as we need,
    whether that’s tens or thousands of machines ([figure 1.10](#ch01fig10)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于EMR允许我们按需运行分布式作业，而无需拥有自己的集群，我们可以通过并行编程扩大我们想要解决的问题的范围。EMR允许我们通过并行编程解决小型问题，因为它使其具有成本效益。我们不需要在服务器上进行前期投资来原型化新想法。EMR还允许我们通过并行编程解决大型问题，因为我们可以根据需要获取所需的所有资源，无论是几十台还是几千台机器（[图1.10](#ch01fig10)）。
- en: Figure 1.10\. EMR allows us to run small parallel jobs more cheaply, while also
    allowing us to expand when we need to run large jobs.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10\. EMR允许我们以更低的成本运行小型并行作业，同时在我们需要运行大型作业时进行扩展。
- en: '![](01fig10_alt.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig10_alt.jpg)'
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: We can use the map and reduce style of programming to solve problems on our
    local machine or in a distributed cloud environment.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用映射和减少风格的编程来解决本地机器或分布式云环境中的问题。
- en: Parallel programming helps us speed up our programs by running many operations
    at the same time on different processors or on different machines.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行编程通过在不同的处理器或不同的机器上同时运行许多操作来帮助我们加快程序的速度。
- en: The `map` function performs one-to-one transformations and is a great way to
    transform data so it is more suitable for use.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 函数执行一对一的转换，是转换数据使其更适合使用的绝佳方式。'
- en: The `reduce` function performs one-to-any transformations and is a great way
    to assemble data into a final result.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce` 函数执行一对一到多一的转换，是组装数据到最终结果的绝佳方式。'
- en: Distributed computing allows us to solve problems rapidly if we have enough
    computers.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式计算允许我们在拥有足够计算机的情况下快速解决问题。
- en: We can do distributed computing a number of ways, including using the Apache
    Hadoop and Apache Spark libraries.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式执行分布式计算，包括使用 Apache Hadoop 和 Apache Spark 库。
- en: AWS is a cloud computing environment that makes it easy and cost-effective to
    do massive parallel work.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 是一个云计算环境，它使得进行大规模并行工作变得既容易又经济高效。
- en: 'Chapter 2\. Accelerating large dataset work: Map and parallel computing'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章\. 加速大数据集工作：`map` 和并行计算
- en: '*This chapter covers*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using `map` to transform lots of data
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 转换大量数据
- en: Using parallel programming to transform lots of data
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用并行编程转换大量数据
- en: Scraping data from the web in parallel with `map`
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 并行从网络抓取数据
- en: 'In this chapter, we’ll look at `map` and how to use it for parallel programming,
    and we’ll apply those concepts to complete two web scraping exercises. With `map`,
    we’ll focus on three primary capabilities:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 `map` 及其如何用于并行编程，并将这些概念应用于完成两个网络抓取练习。使用 `map`，我们将关注三个主要功能：
- en: We can use it to replace `for` loops.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用它来替换 `for` 循环。
- en: We can use it to transform data.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用它来转换数据。
- en: Map evaluates only when necessary, not when called.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`map` 只在必要时进行评估，而不是在调用时。'
- en: These core ideas about `map` are also why it’s so useful for us in parallel
    programming. In parallel programming, we’re using multiple processing units to
    do partial work on a task and combining that work later. Transforming lots of
    data from one type to another is an easy task to break into pieces, and the instructions
    for doing so are generally easy to transfer. Making code parallel with `map` can
    be as easy as adding four lines of code to a program.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关于 `map` 的核心思想也是它在并行编程中对我们如此有用的原因。在并行编程中，我们使用多个处理单元对任务进行部分工作，并在稍后结合这些工作。将大量数据从一种类型转换为另一种类型是一项容易分解成片段的任务，并且执行这些操作的指令通常很容易转移。使用
    `map` 使代码并行化可以简单到在程序中添加四行代码。
- en: 2.1\. An introduction to map
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. `map` 简介
- en: 'In [chapter 1](kindle_split_010.html#ch01), we talked a little bit about `map`,
    which is a function for transforming sequences of data. Specifically, we looked
    at the example of applying the mathematical function `n+7` to a list of integers:
    [–1,0,1,2]. And we looked at the graphic in [figure 2.1](#ch02fig01), which shows
    a series of numbers being mapped to their outputs.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第1章](kindle_split_010.html#ch01) 中，我们简要地讨论了 `map`，这是一个用于转换数据序列的函数。具体来说，我们查看了一个将数学函数
    `n+7` 应用到整数列表 [–1,0,1,2] 上的例子。我们还查看了一个图形 [图2.1](#ch02fig01)，它显示了一系列数字被映射到它们的输出。
- en: 'Figure 2.1\. The `map` function applies another function to all the values
    in a sequence and returns a sequence of their outputs: transforming [–1, 0, 1,
    2] into [6, 7, 8, 9].'
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1\. `map` 函数将另一个函数应用于序列中的所有值，并返回它们输出的序列：将 [–1, 0, 1, 2] 转换为 [6, 7, 8, 9]。
- en: '![](02fig01_alt.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig01_alt.jpg)'
- en: This figure shows the essence of `map`. We have an input of some length, in
    this case four, and an output of that same length. And each input gets transformed
    by the same function as all the other inputs. These transformed inputs are then
    returned as our output.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了 `map` 的本质。我们有一个长度为一些的输入，在这种情况下是四个，以及相同长度的输出。每个输入都通过与其他所有输入相同的函数进行转换。这些转换后的输入随后作为我们的输出返回。
- en: '|  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Some Python knowledge required**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**需要一些Python知识**'
- en: We will cover some advanced topics in this book as we work up to dealing with
    large datasets. That said, in the first section of this book ([chapters 1](kindle_split_010.html#ch01)
    through [6](kindle_split_015.html#ch06)), one of my goals is to provide all my
    readers with background knowledge that may be missing from their programming education.
    Depending on your experience, you may already be familiar with some of the concepts,
    such as regular expressions, classes and methods, higher order functions, and
    anonymous functions. If not, you will be by the end of [chapter 6](kindle_split_015.html#ch06).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们处理大数据集的过程中，这本书将涵盖一些高级主题。话虽如此，在这本书的第一部分（[第1章](kindle_split_010.html#ch01)到[第6章](kindle_split_015.html#ch06)），我的一个目标是为所有读者提供可能缺失的编程教育背景知识。根据你的经验，你可能已经熟悉了一些概念，比如正则表达式、类和方法、高阶函数以及匿名函数。如果不熟悉，到第6章结束时你将熟悉它们。
- en: By the end of this first section, my goal is to have you ready to learn about
    distributed computing frameworks and processing large datasets. If at any point
    you feel like you need more background knowledge on Python, I recommend Naomi
    Ceder’s *The Quick Python Book* (Manning, 2018; [http://mng.bz/vl11](http://mng.bz/vl11)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一部分结束时，我的目标是让你准备好学习关于分布式计算框架和大数据集处理。如果你在任何时候觉得你需要更多关于Python的背景知识，我推荐Naomi
    Ceder的《快速Python书》（Manning，2018；[http://mng.bz/vl11](http://mng.bz/vl11)）。
- en: '|  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: That’s all fine and good, but most of us aren’t concerned with middle-school
    math problems such as applying simple algebraic transformations. Let’s take a
    look at a few ways that `map` can be used in practice so we can really begin to
    see its power.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来不错，但对我们大多数人来说，并不关心中学数学问题，比如应用简单的代数变换。让我们看看`map`在实际应用中有哪些用法，这样我们才能真正看到它的力量。
- en: '|  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Scenario
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 情景
- en: You want to generate a call list for your sales team, but the original developers
    for your customer sign-up form forgot to build data validation checks into the
    form. As a result, all the phone numbers are formatted differently. For example,
    some will be formatted nicely—(123) 456-7890; some are just numbers—1234567890;
    some use dots as separators—123.456.7890; and others, trying to be helpful, include
    a country code—+1 123 456-7890.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你想为你的销售团队生成一个电话列表，但你的客户注册表的原开发人员忘记在表单中构建数据验证检查。结果，所有的电话号码格式都不一样。例如，有些格式很好看——(123)
    456-7890；有些只是数字——1234567890；有些使用点作为分隔符——123.456.7890；还有一些，试图提供帮助，包括国家代码——+1 123
    456-7890。
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'First, let’s tackle this problem in a way that you’re probably familiar with
    already: `for` looping. We’ll do that in [listing 2.1](#ch02ex01). Here, we first
    create a regular expression that matches all numbers and compile that. Then, we
    go through each phone number and get the digits out of that number with the regular
    expression’s `.findall` method. From there, we count off the digits from the right.
    We assign the first four from the right as the last four, the next three as the
    first three, and the next three as an area code. We assume any other digits would
    just be a country code (+1 for the United States). We store all of these in variables,
    and then we use Python’s string formatting to append them to a list to store our
    results: `new_numbers`.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用你可能已经熟悉的方式解决这个问题：`for`循环。我们将在[列表2.1](#ch02ex01)中这样做。在这里，我们首先创建一个匹配所有数字的正则表达式，并编译它。然后，我们遍历每个电话号码，并使用正则表达式的`.findall`方法从这个号码中提取数字。从那里，我们从右向左数数字。我们将最右边的四个数字作为最后四个，接下来的三个数字作为第一个三个，再接下来的三个数字作为区号。我们假设任何其他数字都将是国家代码（美国为+1）。我们将所有这些存储在变量中，然后我们使用Python的字符串格式化将它们附加到一个列表中，以存储我们的结果：`new_numbers`。
- en: Listing 2.1\. Formatting phone numbers with a `for` loop
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1\. 使用`for`循环格式化电话号码
- en: '[PRE0]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* Compiles our regular expression**'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 编译我们的正则表达式**'
- en: '***2* Loops through all the phone numbers**'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历所有电话号码**'
- en: '***3* Gathers the numbers into variables**'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将数字收集到变量中**'
- en: '***4* Appends the numbers in the right format**'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 以正确的格式附加数字**'
- en: 'How do we tackle this with `map`? Similarly, but with `map`, we have to separate
    this problem into two parts. Let’s separate it like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用`map`来解决这个问题？同样，但使用`map`，我们必须将这个问题分成两部分。让我们这样分开：
- en: Resolving the formatting of a phone number
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决电话号码的格式问题
- en: Applying that solution to all the phone numbers we have
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该解决方案应用于我们所有的电话号码
- en: First up, we’ll tackle formatting the phone numbers. To do that, let’s create
    a small class with a method that finds the last 10 numbers of a string and returns
    them in our pretty format. That class will compile a regular expression to find
    all the numbers. We can then use the last seven numbers to print a phone number
    in the format we desire. If there are more than seven, we’ll ignore the country
    code.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将处理电话号码的格式化。为了做到这一点，让我们创建一个包含一个方法的小类，该方法可以找到字符串的最后 10 个数字，并以美观的格式返回它们。这个类将编译一个正则表达式来查找所有的数字。然后我们可以使用最后七个数字来以我们期望的格式打印电话号码。如果有超过七个，我们将忽略国家代码。
- en: '|  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We want to use a *class* (instead of a function) here because it will allow
    us to compile the regular expression once but use it many times. Over the long
    run, this will save our computer the effort of repeatedly compiling the regular
    expression.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用类（而不是函数）的原因是因为它将允许我们一次性编译正则表达式，但多次使用它。从长远来看，这将节省我们的计算机重复编译正则表达式的努力。
- en: '|  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We’ll create a `.pretty_format` method that expects a misformatted phone number
    (a string) and uses the compiled regular expression to find all of the numbers.
    Then, just as we did in the previous example, we’ll take matches at positions
    –10, –9, and –8, using e slice syntax, and assign them to a variable named `area
    code`. These numbers should be our area code. We’ll take the matches at positions
    –7, –6, and –5 and assign them to be the first three numbers of the phone number.
    And we’ll take the last four numbers to be the last four of the phone numbers.
    Again, any numbers that occur before –10 will be ignored. These will be country
    codes. Lastly, we’ll use Python’s string formatting to print the numbers in our
    desired format. The class would look something like the following listing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个 `.pretty_format` 方法，该方法期望一个格式错误的电话号码（一个字符串），并使用编译后的正则表达式来查找所有的数字。然后，就像我们在前面的例子中所做的那样，我们将匹配位置
    –10、–9 和 –8，使用切片语法，并将它们分配给一个名为 `area code` 的变量。这些数字应该是我们的区号。我们将匹配位置 –7、–6 和 –5
    并将它们分配为电话号码的前三位数字。然后我们将电话号码的最后四位数字取出来。同样，任何在 –10 之前出现的数字将被忽略。这些将是国家代码。最后，我们将使用
    Python 的字符串格式化来以我们期望的格式打印数字。这个类可能看起来像以下列表所示。
- en: Listing 2.2\. A class for reformatting phone numbers with `map`
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.2\. 使用 `map` 重新格式化电话号码的类
- en: '[PRE1]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Creates a class to hold our compiled regular expression**'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个类来保存我们的编译后的正则表达式**'
- en: '***2* Creates an initialization method to compile the regular expression**'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个初始化方法来编译正则表达式**'
- en: '***3* Creates a format method to do the formatting**'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个格式化方法来进行格式化**'
- en: '***4* Gathers the numbers from the phone number string**'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从电话号码字符串中收集数字**'
- en: '***5* Returns the numbers in the desired “pretty” format**'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 返回所需的“美观”格式的数字**'
- en: Now that we’re able to turn phone numbers of any format into phone numbers in
    a pretty format, we can combine our class with `map` to apply it to a list of
    phone numbers of any length. To combine the two, we’ll instantiate our class and
    pass the method as the function that `map` will apply to all the elements of a
    sequence. We can do that as shown in the following listing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够将任何格式的电话号码转换成美观的格式，我们可以将我们的类与 `map` 结合起来，将其应用于任意长度的电话号码列表。为了结合这两个功能，我们将实例化我们的类，并将方法作为
    `map` 将其应用于序列中所有元素的功能传递。我们可以像以下列表所示那样做。
- en: Listing 2.3\. Applying the `.pretty_format` method to phone numbers
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.3\. 将 `.pretty_format` 方法应用于电话号码
- en: '[PRE2]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Initializes test data to validate our function**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 初始化测试数据以验证我们的函数**'
- en: '***2* Initializes our class so we can use its method**'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 初始化我们的类，以便我们可以使用其方法**'
- en: '***3* Maps the .pretty_format method across the phone numbers and prints the
    results**'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将 .pretty_format 方法映射到电话号码并打印结果**'
- en: You’ll notice at the very bottom that we convert our `map` results to a `list`
    before we print them. If we were going to use them in our code, we would not need
    to do this; however, because `map`s are lazy if we print them without converting
    them to a `list`, we’ll just see a generic `map` object as output. This isn’t
    as satisfying as the nicely formatted phone numbers that we expected.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在最底部，我们在打印之前将 `map` 的结果转换为 `list`。如果我们打算在代码中使用它们，我们就不需要这样做；然而，因为 `map`
    是惰性的，如果我们不将它们转换为 `list` 就打印它们，我们只会看到一个通用的 `map` 对象作为输出。这并不像我们期望的格式良好的电话号码那样令人满意。
- en: 'Another thing you’ll notice about this example is that we were set up perfectly
    to take advantage of `map` because we were doing a 1-to-1 transformation. That
    is, we were transforming each element of a sequence. In essence, we’ve turned
    this problem into our middle-school algebra example: applying `n+7` to a list
    of numbers.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到这个例子中的另一件事是我们完美地设置了利用`map`的优势，因为我们正在进行1对1的转换。也就是说，我们正在转换序列中的每个元素。本质上，我们已经把这个问题转化成了我们中学的代数例子：将`n+7`应用到数字列表上。
- en: 'In [figure 2.2](#ch02fig02), we can see the similarities between the two problems.
    For each problem, we’re doing three things: taking a sequence of data, transforming
    it with some function, and getting the outputs. The only difference between the
    two is the data type (integers versus phone number strings) and the transformation
    (simple arithmetic versus regular expression pattern matching and pretty printing).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2.2](#ch02fig02)中，我们可以看到两个问题的相似之处。对于每个问题，我们都在做三件事：获取一系列数据，用某个函数对其进行转换，并获取输出。这两个问题之间的唯一区别是数据类型（整数与电话号码字符串）和转换（简单的算术与正则表达式模式匹配和格式化打印）。
- en: Figure 2.2\. We can use `map` to clean text strings into a common format by
    applying a cleaning function to all of them.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2。我们可以使用`map`通过应用清洗函数到所有这些来清理文本字符串到一个通用格式。
- en: '![](02fig02_alt.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig02_alt.jpg)'
- en: 'The key with `map` is recognizing situations where we can apply this three-step
    pattern. Once we start looking for it, we’ll start to see it everywhere. Let’s
    take a look at another, and more complex, version of this pattern: web scraping.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`的关键在于识别可以应用这种三步模式的情况。一旦我们开始寻找，我们就会到处看到它。让我们看看这个模式的另一个版本，一个更复杂版本：网络爬虫。'
- en: '|  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Scenario
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: In the early 2000s, your company’s archrival may have posted some information
    about their top-secret formula on their blog. You can access all their blog posts
    through a URL that includes the date the post was made (e.g., [https://arch-rival-business.com/blog/01-01-2001](https://arch-rival-business.com/blog/01-01-2001)).
    Design a script that can retrieve the content of every web page posted between
    January 1, 2001, and December 31, 2010.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代初，你公司的竞争对手可能在他们的博客上发布了一些关于他们顶级机密公式的信息。你可以通过包含发布日期的URL访问他们所有的博客文章（例如，[https://arch-rival-business.com/blog/01-01-2001](https://arch-rival-business.com/blog/01-01-2001)）。设计一个脚本，可以检索2001年1月1日至2010年12月31日之间发布的每个网页的内容。
- en: '|  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s think about how we’re going to get the data from our archrival’s blog.
    We’ll be retrieving data from URLs. These URLs, then, can be our input data. And
    the transformation will take these URLs and turn them into web page content. Thinking
    about the problem like this, we can see that it’s similar to the others we’ve
    used `map` for in this chapter.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下我们如何从我们的竞争对手的博客中获取数据。我们将从URL中检索数据。这些URL，然后，可以成为我们的输入数据。转换将把这些URL转换成网页内容。这样思考问题，我们可以看到它与我们在本章中使用`map`的其他问题非常相似。
- en: '[Figure 2.3](#ch02fig03) shows the problem posed in the same format as the
    previous problems we’ve solved with `map`. On the top, we can see the input data.
    Here, however, instead of phone numbers or integers, we’ll have URLs. On the bottom,
    again, we have our output data. This is where we’ll eventually have our HTML.
    In the middle, we have a function that will take each URL and return HTML.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.3](#ch02fig03)展示了与之前我们用`map`解决的相同格式的难题。在顶部，我们可以看到输入数据。然而，这里我们不会使用电话号码或整数，而是使用URL。在底部，我们再次看到我们的输出数据。这就是我们最终会得到HTML的地方。在中间，我们有一个函数，它将接受每个URL并返回HTML。'
- en: Figure 2.3\. We also can use `map` to retrieve the HTML corresponding to a sequence
    of URLs, once we write a function that can do that for a single URL.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3。我们还可以使用`map`来检索与一系列URL对应的HTML，一旦我们编写了一个可以针对单个URL执行此操作的函数。
- en: '![](02fig03_alt.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig03_alt.jpg)'
- en: 2.1.1\. Retrieving URLs with map
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 使用`map`检索URL
- en: 'With the problem posed like this, we know we can solve it with `map`. The question,
    then, becomes: How can we get a list of all these URLs? Python has a handy datetime
    library for solving problems like this. Here, we create a generator function that
    takes start and end date `tuple`s in `(YYYY,MM,DD)` format and produces a list
    of dates between them. We use a generator instead of a normal loop because this
    prevents us from storing all the numbers in memory in advance. The keyword `yield`
    in the following listing distinguishes this as a generator, instead of a traditional
    function that uses `return`.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个问题，我们知道我们可以用 `map` 解决它。那么问题来了：我们如何获取所有这些 URL 的列表？Python 有一个方便的 datetime
    库来解决这个问题。在这里，我们创建了一个生成器函数，它接受 `(YYYY,MM,DD)` 格式的开始和结束日期 `tuple`，并生成它们之间的日期列表。我们使用生成器而不是常规循环，因为这可以防止我们提前在内存中存储所有数字。以下列表中的
    `yield` 关键字将此区分为一个生成器，而不是使用 `return` 的传统函数。
- en: Listing 2.4\. A date range generating function
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.4\. 生成日期范围的函数
- en: '[PRE3]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Imports the datetime library’s date class**'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导入 datetime 库的 date 类**'
- en: '***2* Creates our generator function**'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建我们的生成器函数**'
- en: '***3* Unpacks the date start and stop tuples to store them as dates**'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 解包日期开始和结束元组以将它们存储为日期**'
- en: '***4* Loops through all the dates until we’ve reached our stop date**'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 遍历所有日期，直到达到我们的截止日期**'
- en: '***5* Returns the date as a path**'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 返回日期作为路径**'
- en: '***6* Increments the date by one day**'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将日期增加一天**'
- en: Taking advantage of datetime
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 利用 datetime
- en: 'The majority of the work this function does comes from Python’s datetime library’s
    `date` class. The datetime `date` class represents a date and contains knowledge
    about the Gregorian calendar and some convenience methods for working with dates.
    You’ll notice that we import the `date` class directly as `date`. In our function,
    we instantiate two of these classes: one for our start date and one for our stop
    date. Then, we let our function generate new dates until we hit our stop date.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数所做的绝大部分工作都来自于 Python 的 datetime 库的 `date` 类。datetime `date` 类表示一个日期，并包含关于格里高利历的知识以及一些用于处理日期的便利方法。你会注意到我们直接将
    `date` 类导入为 `date`。在我们的函数中，我们创建了两个这样的类：一个用于我们的开始日期，一个用于我们的结束日期。然后，我们让我们的函数生成新的日期，直到我们达到截止日期。
- en: The last line of our function uses the ordinal date representation, which is
    the date as the number of days since January 1, year 1\. By incrementing this
    value and turning it into a `date` class, we can increase our date by one. Because
    our `date` class is calendar aware, it will automatically progress through the
    weeks, months, and years. It will even account for leap years.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们函数的最后一行使用了序数日期表示法，即从公元 1 年 1 月 1 日起的天数。通过增加这个值并将其转换为 `date` 类，我们可以增加我们的日期一天。因为我们的
    `date` 类是日历感知的，它会自动通过周、月和年。它甚至还会考虑闰年。
- en: Lastly, it’s worth looking at the line our `yield` statement is on. This is
    where we output URLs. We take the base URL of the website—[http://jtwolohan.com/arch-rival-blog/](http://jtwolohan.com/arch-rival-blog/)—and
    append the date formatted as a MM-DD-YYYY string to the end, just like our problem
    specified. The `.strftime` method from the `date` class allows us to use a date
    formatting language to turn dates into strings formatted however we want.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得看看我们的 `yield` 语句所在的行。这是我们输出 URL 的地方。我们取网站的基准 URL——[http://jtwolohan.com/arch-rival-blog/](http://jtwolohan.com/arch-rival-blog/)——并将日期格式化为
    MM-DD-YYYY 字符串附加到末尾，就像我们的问题所指定的那样。`date` 类的 `.strftime` 方法允许我们使用日期格式化语言将日期转换为所需的字符串格式。
- en: Turning input into output
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将输入转换为输出
- en: 'Once we’ve got our input data, the next step is coming up with a function to
    turn our input data into the output data. Our output data here is going to be
    the web content of the URL. Lucky for us again, Python provides some useful tools
    for that in its urllib.request library. Taking advantage of that, a function like
    the following may work for us:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了输入数据，下一步就是编写一个函数，将我们的输入数据转换为输出数据。这里的输出数据将是 URL 的网页内容。幸运的是，Python 在其 urllib.request
    库中提供了一些有用的工具。利用这一点，以下这样的函数可能对我们适用：
- en: '[PRE4]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function takes a URL and returns the HTML found at that URL. We rely on
    Python’s request library’s `urlopen` function to retrieve the data at the URL.
    This data is returned to us as an `HTTPResponse` object, but we can use its `.read`
    method to return the HTML as a string. It’s worth trying this function out in
    your REPL environment on a URL for a website you visit often (like [www.manning.com](http://www.manning.com))
    to see the function in action.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个URL，并返回该URL上找到的HTML。我们依赖于Python的request库的`urlopen`函数来检索URL上的数据。这些数据以`HTTPResponse`对象的形式返回给我们，但我们可以使用它的`.read`方法将HTML作为字符串返回。值得尝试在你的REPL环境中使用你经常访问的网站（如[www.manning.com](http://www.manning.com)）的URL来查看这个函数的实际效果。
- en: 'Then, like in previous scenarios, we can apply this function to all the data
    in our sequence, using `map` like this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，就像在先前的场景中一样，我们可以使用`map`将这个函数应用于我们序列中的所有数据，如下所示：
- en: '[PRE5]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This single line of code takes our `get_url` function and applies it to each
    and every URL generated by our `days_between` function. Passing the start and
    end dates ((2000,1,1) and (2011,1,1)) to our `days_between` function results in
    a generator of days between January 1, 2000, and January 1, 2011: every day of
    the first decade of the 21st century. The values that this function returns are
    stored in the variable `blog_posts`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行代码将我们的`get_url`函数应用于由我们的`days_between`函数生成的每个URL。将起始日期和结束日期（（2000年1月1日）和（2011年1月1日））传递给`days_between`函数，结果生成2000年1月1日和2011年1月1日之间的日期生成器：21世纪第一个十年的每一天。该函数返回的值存储在变量`blog_posts`中。
- en: If you run this on your local machine, the program should finish almost instantly.
    How is that possible? Certainly we can’t scrape 10 years of web pages that quickly,
    can we? Well, no. But with our generator function and with `map`, we don’t actually
    try to.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地的机器上运行这个程序，程序应该几乎立即完成。这是怎么可能的？当然，我们不可能那么快地抓取10年的网页，对吧？嗯，不。但是，有了我们的生成器函数和`map`，我们实际上并没有尝试这样做。
- en: 2.1.2\. The power of lazy functions (like map) for large datasets
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2. 惰性函数（如map）对大数据集的强大功能
- en: '`map` is what we call a lazy function. That means it doesn’t actually evaluate
    when we call it. Instead, when we call `map`, Python stores the instructions for
    evaluating the function and runs them at the exact moment we ask for the value.
    That’s why when we’ve wanted to see the values of our `map` statements previously,
    we’ve explicitly converted the `map`s to lists; lists in Python require the actual
    objects, not the instructions for generating those objects.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`是我们所说的惰性函数。这意味着当我们调用它时，它实际上并不进行评估。相反，当我们调用`map`时，Python会存储评估函数的指令，并在我们要求值的确切时刻运行它们。这就是为什么当我们之前想要查看`map`语句的值时，我们明确地将`map`转换为列表；Python中的列表需要实际的物体，而不是生成这些物体的指令。'
- en: 'If we think back to our first example of `map`—mapping `n+7` across a list
    of numbers: [–1,0,1,2]—we used [figure 2.4](#ch02fig04) to describe `map`.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下`map`的第一个例子——将`n+7`映射到数字列表：[–1,0,1,2]——我们使用了[图2.4](#ch02fig04)来描述`map`。
- en: Figure 2.4\. We initially thought about `map` as something that transforms a
    sequence of inputs into a sequence of outputs.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4。我们最初认为`map`是一种将输入序列转换为输出序列的东西。
- en: '![](02fig04_alt.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4](02fig04_alt.jpg)'
- en: It is, however, a little more accurate to think about `map` like in [figure
    2.5](#ch02fig05).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将`map`想象成[图2.5](#ch02fig05)中的样子会更准确一些。
- en: Figure 2.5\. In Python, the base `map` turns a sequence of inputs into instructions
    for computing a sequence of outputs—not the sequence itself.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5。在Python中，基本的`map`将输入序列转换为计算输出序列的指令——而不是序列本身。
- en: '![](02fig05_alt.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5](02fig05_alt.jpg)'
- en: 'In [figure 2.5](#ch02fig05), we have the same input values on the top and the
    same function we’re applying to all of those values; however, our outputs have
    changed. Where before we had 6, 7, 8, and 9, now we have instructions. If we had
    the computer evaluate these instructions, the results would be 6, 7, 8, and 9\.
    Often in our programs, we will act like these two outputs are equal. However,
    as programmers, we’ll need to remember that there’s a slight difference: the default
    `map` in Python doesn’t evaluate when called, it creates instructions for later
    evaluation.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2.5](#ch02fig05)中，我们有相同的输入值在顶部，以及我们应用于所有这些值的相同函数；然而，我们的输出已经改变。之前我们有6、7、8和9，现在我们有指令。如果我们让计算机评估这些指令，结果将是6、7、8和9。在我们的程序中，我们通常会认为这两个输出是相等的。然而，作为程序员，我们需要记住，这里有一个细微的差别：Python中的默认`map`在调用时并不进行评估，它为后续评估创建指令。
- en: 'As a Python programmer, you’ve probably already seen lazy data floating around.
    A common place to find lazy objects in Python is the `range` function. When moving
    from Python2 to Python3, the Python folks decided to make `range` lazy so that
    Python programmers (you and me) can create huge ranges without doing two things:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Python程序员，你可能已经见过懒数据。在Python中找到懒对象的一个常见地方是`range`函数。当从Python2迁移到Python3时，Python团队决定使`range`变为懒加载，这样Python程序员（你和我）就可以创建巨大的范围，而无需做两件事：
- en: Taking the time to generate a massive list of numbers
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 花时间生成大量数字列表
- en: Storing all those values in memory when we may only need a few
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们可能只需要少数几个值时，将这些值全部存储在内存中
- en: These benefits are the same for `map`. We like a lazy `map` because it allows
    us to transform a lot of data without an unnecessarily large amount of memory
    or spending the time to generate it. That’s exactly how we want it to work.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处对`map`也是相同的。我们喜欢懒`map`，因为它允许我们在不必要大量内存或花费时间生成它的情况下转换大量数据。这正是我们想要的方式。
- en: 2.2\. Parallel processing
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 并行处理
- en: Great, so now we have a way to get all of our data from the internet using `map`.
    But using `map` to get that data offline one page at a time is going to be very
    slow. If it takes us 1 second to scrape a single webpage, and we need to scrape
    3,652 web pages, then it will take us a little more than an hour to download all
    the data (3,652 pages × 1 second per page/60 seconds per minute = 61 minutes).
    This is not an incredibly long time to wait, but it’s long enough that we want
    to avoid it if we can. And we can.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在我们有了使用`map`从互联网获取所有数据的方法。但是，使用`map`逐页离线获取数据将会非常慢。如果我们爬取一个网页需要1秒钟，而我们需要爬取3,652个网页，那么下载所有数据（3,652页
    × 每页1秒/每分钟60秒 = 61分钟）将需要超过一个小时。这并不是一个特别长的等待时间，但足够长，以至于我们想要避免它，如果我们能的话。我们可以做到。
- en: How can we avoid this wait? Well, what if instead of downloading a single page
    at a time, we downloaded multiple pages at once? Using parallel programming, we
    can do just that.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何避免这种等待？好吧，如果我们一次下载多个页面而不是一次下载一个页面会怎样？使用并行编程，我们就可以做到这一点。
- en: Parallel programming means to program in such a way that we divide our problem
    into chunks that can then be processed separately and simultaneously. Typically,
    the work we’ll want to do on each of these chunks is going to be the same. For
    example, in our case, we want to process each URL (a separate piece of data, unrelated
    to any other URL) and retrieve a website at that URL (a common process).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程意味着以这种方式编程，我们将问题分解成可以分别和同时处理的块。通常，我们想要对每个块执行的工作将是相同的。例如，在我们的案例中，我们想要处理每个URL（一个独立的数据块，与其他任何URL无关）并检索该URL上的网站（一个常见的过程）。
- en: '[Figure 2.6](#ch02fig06) shows the difference between downloading URLs with
    standard linear processing and with parallel processing.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.6](#ch02fig06)显示了使用标准线性处理和并行处理下载URL之间的差异。'
- en: Figure 2.6\. Reading one web page at a time is slow; we can speed this up with
    parallel programming.
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6：一次读取一个网页很慢；我们可以通过并行编程来加快这个速度。
- en: '![](02fig06_alt.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig06_alt.jpg)'
- en: Using linear processing, we’d be processing URLs and turning them into web pages
    one at a time. We’d work on one URL, get the data, then work on the next URL.
    Parallel programming allows us to split this task up and process it faster. When
    we write parallel code, we assign a number of “workers” (typically CPUs) to the
    task. Each of these workers then takes a chunk of our data and processes it.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性处理，我们会逐个处理URL并将它们转换为网页。我们会处理一个URL，获取数据，然后处理下一个URL。并行编程允许我们将这个任务分割并更快地处理。当我们编写并行代码时，我们会为任务分配一定数量的“工作者”（通常是CPU）。然后，每个工作者都会处理我们数据的一部分。
- en: In [figure 2.6](#ch02fig06), the data is the same and the data transformation
    is the same. The only change in our setup is the number of tasks we’re performing
    at once. Where before we were doing one task at a time, now we’re doing four.
    This will make our work go four times more quickly.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2.6](#ch02fig06)中，数据是相同的，数据转换也是相同的。我们设置中唯一的变化是我们一次执行的任务数量。之前我们一次只做一项任务，现在我们一次做四项。这将使我们的工作速度提高四倍。
- en: 2.2.1\. Processors and processing
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1. 处理器和处理
- en: If four is better, why not eight? Why not 10? Why not 1,000? Well, that’s a
    really good question. Something most people don’t think about when they’re working
    on computers, something that most programmers don’t even think about, is the effect
    computer hardware has on how the computer behaves. Most people will know, for
    example, whether they have a Mac or a PC; however, unless they have an Intel sticker
    on their computer somewhere, most people probably couldn’t say what type of processor
    they have.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果四个更好，为什么不试试八个？为什么不是十个？为什么不是一千个？嗯，这是一个非常好的问题。当人们在电脑上工作时，大多数人不会考虑的一个问题，甚至大多数程序员也不会考虑的一个问题，就是计算机硬件对计算机行为的影响。例如，大多数人会知道他们有Mac还是PC；然而，除非他们的电脑上贴有Intel的标签，否则大多数人可能无法说出他们有什么类型的处理器。
- en: 'In parallel programming, though, these processors are our heroes. Processors
    are little circuit boards that are capable of executing instructions, that is,
    actually doing work. Often, we think of our computer’s memory as the limiting
    factor to what we can do, and it certainly can be. But our CPU can be just as
    important. Having a lot of memory with a weak processor is like being in the buffet
    line with only one plate: sure, there’s a lot of food, but most of it won’t ever
    get eaten. If our CPU has multiple cores, it’s like getting extra plates: every
    time we go to the buffet, we’ll be able to bring that much more food back to the
    table.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在并行编程中，这些处理器是我们的英雄。处理器是能够执行指令的小电路板，也就是说，实际上在做工作。我们常常认为我们电脑的内存是我们能做的事情的限制因素，这确实可以。但我们的CPU同样重要。有很多内存但处理器弱就像在自助餐队伍中只有一个盘子：当然，有很多食物，但大部分食物永远不会被吃掉。如果我们的CPU有多个核心，就像得到额外的盘子：每次我们去自助餐，我们就能带更多的食物回到桌子上。
- en: 'With CPUs, like with plates, more is better. The more we have, the more we
    can assign to tasks, and the more work we can do. You can check how many CPUs
    you have on your machine by running the following Python command in your Python
    REPL:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 就像盘子一样，CPU越多越好。我们拥有的越多，我们就能分配给任务越多，我们就能完成更多的工作。你可以通过在你的Python REPL中运行以下Python命令来检查你的机器上有多少个CPU：
- en: '[PRE6]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Alternatively, you can run the following command from the terminal:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以从终端运行以下命令：
- en: '[PRE7]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Both of these commands do the same thing. The first bit imports the `os` module
    from the Python standard library, and the second bit checks how many CPUs you
    have. The `os` module, if you’re not familiar with it, is stocked full of tools
    for interacting with your operating system. Depending on which operating system
    you’re using, the exact details of some of these functions will change. It’s worthwhile
    to familiarize yourself with the details before using too much of this module.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个命令做的是同一件事。第一个部分从Python标准库中导入`os`模块，第二个部分检查你有多少个CPU。如果你不熟悉`os`模块，它充满了与操作系统交互的工具。根据你使用的操作系统，一些函数的确切细节会有所不同。在使用这个模块之前熟悉这些细节是值得的。
- en: These commands are useful because they tell you how much of a speed increase
    you’ll get from your standard parallel programming implementation. When we implement
    code in parallel in Python, by default Python will use all of our CPUs. If we
    don’t want it to, we’ll have to specify that we want it to use fewer.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令很有用，因为它们会告诉你从标准并行编程实现中你能获得多少速度提升。当我们用Python并行实现代码时，默认情况下Python会使用我们所有的CPU。如果我们不希望它这样做，我们就必须指定我们希望它使用更少的CPU。
- en: But that’s jumping a little ahead. What does it even look like to implement
    parallel code in Python? Let’s return to our URL downloading example. We want
    to scrape web pages in parallel. How much do we have to modify our code? The following
    listing will give you an idea.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但这有点超前了。在Python中实现并行代码是什么样子呢？让我们回到我们的URL下载示例。我们想要并行抓取网页。我们需要修改我们的代码多少？下面的列表将给你一个想法。
- en: Listing 2.5\. Web scraping in parallel
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5\. 并行网页抓取
- en: '[PRE8]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Imports the multiprocessing library**'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导入multiprocessing库**'
- en: '***2* Gathers our processors with Pool()**'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用Pool()收集我们的处理器**'
- en: '***3* Performs our map in parallel**'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 并行执行我们的map操作**'
- en: As we can see in [listing 2.5](#ch02ex05), the code doesn’t have to change very
    much at all. Because we organized our code using `map` in the first place, making
    our code parallel for this problem required only two new lines of code and adding
    two characters to a third line. If you have four CPUs on your machine, this program
    should run about four times faster than the nonparallel version. That would cut
    our hypothetical one-hour run time down to about 15 minutes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[代码列表2.5](#ch02ex05)中看到的那样，代码根本不需要做太多改变。因为我们一开始就使用`map`组织了我们的代码，所以使我们的代码并行化只需要两行新代码，并在第三行添加两个字符。如果你的机器上有四个CPU，这个程序应该比非并行版本快大约四倍。这将把我们的假设运行时间从一小时缩短到大约15分钟。
- en: That was pretty easy, and it should be. This type of task falls under the umbrella
    of tasks that are dismissively referred to as *embarrassingly parallel*. In other
    words, the solution to speeding up these tasks is embarrassingly easy. That said,
    some problems can pop up when doing parallel programming.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单，应该是这样的。这类任务通常被轻蔑地称为*令人尴尬的并行*。换句话说，加快这些任务的解决方案简单得令人尴尬。尽管如此，在并行编程过程中可能会出现一些问题。
- en: Some of the problems we may encounter when working with parallelization in Python
    are
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Python进行并行化工作时，我们可能会遇到一些问题
- en: The inability to pickle data or functions, causing our programs to not run
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法pickle数据或函数，导致我们的程序无法运行
- en: Order-sensitive operations returning inconsistent results
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序操作返回不一致的结果
- en: State-dependent operations returning inconsistent results
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态相关操作返回不一致的结果
- en: 2.2.2\. Parallelization and pickling
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 并行化和pickle
- en: When we write code in parallel—for example, when we called our parallel `map`
    function previously—Python does a lot of work behind the scenes. When our parallelizations
    don’t work, it’s usually because we aren’t fully thinking through the work that
    Python is hiding from us. One of the things that Python hides from us is *pickling*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在并行编写代码时——例如，当我们之前调用我们的并行`map`函数时——Python在幕后做了很多工作。当我们的并行化不起作用时，通常是因为我们没有完全思考Python隐藏给我们的工作。Python隐藏给我们的东西之一就是*pickle*。
- en: Pickling is Python’s version of object serialization or marshalling, with object
    serialization being the storing of objects from our code in an efficient binary
    format on the disk that can be read back by our program at a later time. The term
    *pickling* comes from Python’s `pickle` module, which provides functions for pickling
    data and reading pickled data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle是Python的对象序列化或打包的版本，对象序列化是将我们的代码中的对象以高效的二进制格式存储在磁盘上，以便我们的程序在以后的时间读取。术语*pickle*来自Python的`pickle`模块，该模块提供了pickle数据和读取pickle数据的函数。
- en: The pickling and unpickling process looks something like [figure 2.7](#ch02fig07).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle和unpickle的过程看起来就像[图2.7](#ch02fig07)所示。
- en: Figure 2.7\. Pickling allows us to save data and instructions in a machine-readable
    state so Python can reuse it later.
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7\. Pickle允许我们将数据和指令以机器可读的状态保存，这样Python可以在以后重用它。
- en: '![](02fig07_alt.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig07_alt.jpg)'
- en: On the left, we begin the process with our original programming environment
    and our original code objects. Nothing special is going on at this point; we’re
    just programming in Python as usual. Next, we pickle our code objects. Now our
    code objects are saved in a binary file on a disk. Next, we read the pickled file
    from a new programming environment, and our original code objects become accessible
    to us in the new environment. Everything that we pickled in the first environment
    is now accessible to us in the new one just as it was previously.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们以原始编程环境和原始代码对象开始这个过程。此时并没有什么特别的事情发生；我们只是在Python中像往常一样编程。接下来，我们将代码对象pickle。现在，我们的代码对象已保存在磁盘上的二进制文件中。接下来，我们从新的编程环境读取pickle文件，我们的原始代码对象在新环境中对我们变得可访问。在第一个环境中pickle的所有内容现在在新环境中以同样的方式对我们可用。
- en: '|  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Our code can sit in the pickled format for as long as we’d like. In parallel
    programming, usually we read the file back into a Python environment quickly,
    but there’s no reason we couldn’t leave the pickled objects on the disk for a
    longer amount of time. However, pickling data for long-term storage is not a good
    idea, because if you upgrade your Python version, the data may become unreadable.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码可以以我们想要的任何格式保存。在并行编程中，我们通常会将文件快速读回到Python环境中，但也没有理由我们不能将pickle对象保留在磁盘上更长时间。然而，将数据pickle以进行长期存储并不是一个好主意，因为如果你升级了Python版本，数据可能变得不可读。
- en: '|  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why do we use pickling in parallel programming? Remember how we talked about
    parallel programming allowing our program to do multiple things at the same time?
    Python pickles objects—functions and data—to transfer the work to each of the
    processors that will be working on our problem. That process looks something like
    [figure 2.8](#ch02fig08).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在并行编程中使用pickle？记得我们讨论过并行编程允许我们的程序同时做很多事情吗？Python通过pickle对象（函数和数据）将工作传输到将处理我们的问题的每个处理器。这个过程看起来像[图2.8](#ch02fig08)。
- en: Figure 2.8\. Pickling allows us to share data across processors or even across
    machines, saving the instructions and data and then executing them elsewhere.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8。pickle允许我们在处理器之间或甚至在不同机器之间共享数据，保存指令和数据，然后在其他地方执行它们。
- en: '![](02fig08_alt.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig08_alt.jpg)'
- en: We start with our code operating on only one processor; this is the standard
    way of coding. To work our code in parallel, Python then divides our problem into
    parts that can each be tackled by an individual processing unit. The master work
    stream then pickles these parts. This pickling ensures that the processor will
    know how to perform the work we need it to do. When the processing unit is ready
    to do the work, it reads the pickled file from the disk and does the work. Then,
    finally, the worker pickles the result and returns it to the master.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从只在单个处理器上运行的代码开始；这是编码的标准方式。为了使我们的代码并行工作，Python然后将我们的问题分解成可以由单个处理单元解决的各个部分。主工作流将这些部分pickle。这种pickle确保处理器将知道如何执行我们需要它执行的工作。当处理单元准备好执行工作的时候，它从磁盘读取pickle文件并执行工作。然后，最后，工作进程pickle结果并将其返回给主进程。
- en: 'Most of the time, this approach works flawlessly; however, only some types
    of Python objects can be pickled. If we try to use parallel methods on objects
    that can’t be pickled, Python will throw an error. Luckily for us, most standard
    Python objects are pickleable and, therefore, usable in parallel Python code.
    Python can naturally pickle the following types:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这种方法工作得很好；然而，只有某些类型的Python对象可以被pickle。如果我们尝试在无法pickle的对象上使用并行方法，Python将抛出错误。幸运的是，大多数标准Python对象都是pickleable的，因此可以在并行Python代码中使用。Python可以自然pickle以下类型：
- en: None, True, and False
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: None、True和False
- en: Integers, floating-point numbers, complex numbers
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整数、浮点数、复数
- en: Strings, bytes, bytearrays
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串、字节、字节数组
- en: Tuples, lists, sets, and dictionaries containing only pickleable objects
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只包含可pickle对象的元组、列表、集合和字典
- en: Functions defined at the top level of a module
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模块顶层定义的函数
- en: Built-in functions defined at the top level of a module
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模块顶层定义的内建函数
- en: Classes that are defined at the top level of a module
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模块顶层定义的类
- en: 'We can’t pickle the following types of objects:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能pickle以下类型的对象：
- en: Lambda functions
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda函数
- en: Nested functions
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌套函数
- en: Nested classes
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌套类
- en: The easiest way to avoid problems with the unpickleable types is to avoid using
    them when working with Python’s built-in multiprocessing module. For situations
    where we absolutely must use them, a community library called *pathos* solves
    many of these problems with a module called `dill` (Get it? Dill pickles?). The
    `dill` module takes a different approach to pickling that allows us to pickle
    just about anything we’d like, including the three object types we weren’t able
    to pickle before.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 避免与Python内置的multiprocessing模块一起使用不可pickle的类型的最简单方法是避免使用它们。对于我们绝对必须使用它们的情况，一个名为*pathos*的社区库通过一个名为`dill`的模块解决了许多这些问题（明白了吗？Dill
    pickles？）。`dill`模块采用不同的pickle方法，允许我们pickle几乎所有我们想要的东西，包括我们之前无法pickle的三个对象类型。
- en: Using pathos and `dill` is not much different from using the multiprocessing
    module. The first thing we have to do is install the library. From the command
    line, run
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pathos和`dill`与使用multiprocessing模块没有太大区别。我们首先要做的是安装库。从命令行运行
- en: '[PRE9]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In addition to installing pathos, Python also will install some of the libraries
    pathos depends on, including `dill`. With pathos installed, we can now call on
    it, and it will use `dill` behind the scenes. If you remember back to our multiprocessing
    example, it looked something like this:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 除了安装pathos，Python还会安装pathos依赖的一些库，包括`dill`。安装了pathos后，我们现在可以调用它，它将在幕后使用`dill`。如果你还记得我们的multiprocessing示例，它看起来像这样：
- en: '[PRE10]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To convert this to pathos, we just have to make a few changes. Our new code
    will look like this:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转换为pathos，我们只需做一些更改。我们的新代码将看起来像这样：
- en: '[PRE11]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Moving from multiprocessing to pathos requires only two real changes. First,
    we have to import from pathos instead of from multiprocessing. Also, in pathos
    the pool we want is called `ProcessPool` instead of just `Pool`. Just like `Pool`,
    `ProcessPool` is the function that will recruit worker processor units for us.
    We need to call `ProcessPool` in place of `Pool`. As with `Pool`, we only need
    to specify the number of nodes if we want to use fewer than the maximum number
    of nodes. We’re specifying it here for demonstration purposes. With our `ProcessPool`
    now accessible as `P`, we can call it just like we called our `multiprocessing.Pool`
    object: `P.map`.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 从多进程迁移到 pathos 只需要两个真正的变化。首先，我们必须从 pathos 而不是从 multiprocessing 导入。此外，在 pathos
    中，我们想要的池被称作 `ProcessPool` 而不是仅仅 `Pool`。就像 `Pool` 一样，`ProcessPool` 是为我们招募工作处理单元的函数。我们需要用
    `ProcessPool` 代替 `Pool`。与 `Pool` 一样，我们只需要指定节点数，如果我们想使用少于最大节点数。我们在这里指定它是为了演示目的。现在我们的
    `ProcessPool` 可以作为 `P` 访问，我们可以像调用我们的 `multiprocessing.Pool` 对象一样调用它：`P.map`。
- en: 2.2.3\. Order and parallelization
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 排序和并行化
- en: Another condition that can cause us problems when we’re working in parallel
    is order sensitivity. When we work in parallel, we’re not guaranteed that tasks
    will be finished in the same order they’re input. This means that if we’re doing
    work that needs to be processed in a linear order, we probably shouldn’t do it
    in parallel.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在我们并行工作时可能引起我们问题的条件是顺序敏感性。当我们并行工作时，我们不能保证任务将以它们输入的相同顺序完成。这意味着，如果我们正在做需要按线性顺序处理的工作，我们可能不应该并行执行。
- en: 'To test this for yourself, try running this command in Python:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 要亲自测试这一点，请尝试在 Python 中运行此命令：
- en: '[PRE12]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If we do this with a `for` loop, we expect to get a nice ordered list of every
    number between 0 and 99 printed to our screen. With our `map` construction, though,
    we don’t get this. With `map`, we get a somewhat ordered, somewhat mismatched
    sequence printed to our screen and a list of `None`s. What’s going on?
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用 `for` 循环来做这件事，我们期望在屏幕上打印出从 0 到 99 的每个数字的有序列表。然而，使用 `map` 构造时，我们并没有得到这个结果。使用
    `map`，我们在屏幕上得到的是一个部分有序、部分不匹配的序列，以及一个 `None` 的列表。这是怎么回事？
- en: When Python parallelizes our code, it chunks our problem up for our processing
    units to work on. Our processing units, for their part, grab the first available
    chunk every time they have capacity to work on the problem. They then work this
    problem until it’s complete, then they grab the next available chunk to work.
    When chunks are available out of order, they will be completed out of order. We
    can visualize this process as shown in [figure 2.9](#ch02fig09).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Python 并行化我们的代码时，它会将问题分成几块，以便我们的处理单元进行处理。从它们的角度来看，每次它们有处理问题的能力时，都会抓取第一个可用的块。然后它们处理这个问题，直到完成，然后抓取下一个可用的块来处理。当块按顺序不可用时，它们将按顺序完成。我们可以将这个过程可视化，如图
    2.9 所示。
- en: Figure 2.9\. Parallel processing doesn’t necessarily finish tasks in order,
    so we have to know if that is acceptable before we use parallel techniques.
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.9\. 并行处理不一定按顺序完成任务，所以在使用并行技术之前，我们必须知道这是否可以接受。
- en: '![](02fig09_alt.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![02fig09_alt.jpg](02fig09_alt.jpg)'
- en: In [figure 2.9](#ch02fig09), our problem starts at the top. We’ve chunked the
    problem into 10 pieces and put them in a queue. As the processors become available,
    they’ll pull a task from the queue, work it, and send the results to the completed
    tasks area at the bottom. The processors then grab the next available tasks and
    process them until all of the tasks are finished. But the time it takes for these
    operations to finish varies. For example, in the completed tasks area we can see
    that tasks 1, 2, 3, and 5 have been completed, and tasks 4, 6, 7, and 8 are currently
    being worked. Tasks 9 and 10 are still queued up, unassigned to a processor. A
    situation like this can easily occur if tasks 1 (or 2 or 3) and 5 are short, but
    task 4 is long, such that two tasks can be completed in the time it takes the
    single task 4 to finish.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 2.9 中，我们的问题从顶部开始。我们将问题分成 10 块，并将它们放入队列中。当处理器可用时，它们将从队列中拉取一个任务，处理它，并将结果发送到底部的已完成任务区域。然后处理器抓取下一个可用的任务并处理它们，直到所有任务都完成。但是这些操作完成所需的时间是不同的。例如，在已完成任务区域，我们可以看到任务
    1、2、3 和 5 已经完成，而任务 4、6、7 和 8 正在处理中。任务 9 和 10 仍然在队列中，尚未分配给处理器。如果任务 1（或 2 或 3）和
    5 很短，但任务 4 很长，以至于两个任务可以在单个任务 4 完成的时间内完成，这种情况很容易发生。
- en: 'All that said, even though Python may not complete the problems in order, it
    still remembers the order in which it was supposed to do them. Indeed, our `map`
    returns in the exact order we would expect, even if it doesn’t process in that
    order. To demonstrate that, we can run the following code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Python可能不会按顺序完成问题，但它仍然记得它应该按什么顺序完成。确实，我们的`map`返回的顺序正是我们预期的，即使它不是按那个顺序处理的。为了证明这一点，我们可以运行以下代码：
- en: '[PRE13]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The printed output won’t be ordered, but the list that’s returned will be. The
    printed output shows the order in which the chunks were worked; the list output
    shows the data structure that was returned. We can see that even though Python
    works the problem in the “wrong” order, it still orders the results properly.
    When is this going to cause problems for us? Well, if we rely on state for one.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的输出将不会排序，但返回的列表将会排序。打印的输出显示了处理块的工作顺序；列表输出显示了返回的数据结构。我们可以看到，尽管Python以“错误”的顺序处理问题，但它仍然正确地排序了结果。什么时候这会给我们带来问题呢？嗯，如果我们依赖于状态的话。
- en: 2.2.4\. State and parallelization
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 状态和并行化
- en: In object-oriented programming, we’ll often write methods that rely on the state
    of the class. Consider the fizz/buzz problem. The fizz/buzz problem is a problem
    that’s often used to introduce programming language syntax. It involves looping
    through numbers and returning *fizz* if a number is not evenly divisible by three
    (or five, or some other number), and returning *buzz* if it is. The expected output
    is a sequence of fizzes and buzzes at the appropriate intervals.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在面向对象的编程中，我们经常编写依赖于类状态的函数。考虑“砰”/“蜂鸣”问题。这是一个常用于介绍编程语言语法的例子。它涉及遍历数字，如果数字不能被三（或五，或其他数字）整除，则返回“砰”，如果可以，则返回“蜂鸣”。预期的输出是在适当间隔的“砰”和“蜂鸣”序列。
- en: In Python, we could solve the fizz/buzz problem with a class, as shown in the
    following listing.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以使用类来解决“砰”/“蜂鸣”问题，如下面的列表所示。
- en: Listing 2.6\. Classic fizz/buzz problem with a `for` loop
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6\. 使用`for`循环的经典“砰”/“蜂鸣”问题
- en: '[PRE14]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* The counter starts at 0.**'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计数器从0开始。**'
- en: '***2* foo function that decides on fizzes and buzzes**'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 决定“砰”声和“蜂鸣声”的foo函数**'
- en: '***3* Increments the counter each time the function is run**'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 每次函数运行时增加计数器**'
- en: '***4* If the counter is divisible by three, buzz.**'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果计数器能被三整除，则发出蜂鸣声。**'
- en: '***5* If it’s not, fizz.**'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 如果不是，则发出“砰”声。**'
- en: '***6* Both prints the statement and returns it**'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 两者都打印语句并返回它**'
- en: '***7* Tests that the class is working**'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 测试类是否正常工作**'
- en: 'The class pays attention to how many times we’ve called its `.foo` method,
    and every third time it will print and return buzz instead of fizz. We use it
    in a loop to demonstrate that it’s working properly. If you run this on your local
    machine, you’ll see that this works just like we expect: we print out fizzes,
    with a buzz interjected in every third spot. However, something strange happens
    when we try to do the same thing using a parallel `map`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 该类关注我们调用其`.foo`方法多少次，并且每第三次它会打印并返回“蜂鸣”而不是“砰”，而不是“砰”。我们使用它在一个循环中来演示它是否正常工作。如果你在自己的机器上运行这个程序，你会看到这就像我们预期的那样工作：我们打印出“砰”声，每隔第三个位置插入一个“蜂鸣”声。然而，当我们尝试使用并行的`map`做同样的事情时，会发生一些奇怪的事情：
- en: '[PRE15]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What’s going on here? Why do we only get fizz and no buzz? Let’s return to something
    we talked about earlier when we were discussing `map`. Remember how we said that
    `map` doesn’t actually do the calculations, it simply stores the instructions
    for the calculations? That’s why we call it lazy. Well, in this case, the instructions
    to do the calculations for `FB.foo` include the state of `FB` at the time we ask
    for it. So since `FB.n` is 0 at the time we ask for the instructions, `map` uses
    `FB.n` = 0 for all of the operations, even if `FB.n` changes by the time we use
    it. And since `FB.n` = 0 will always produce a fizz, all we get is fizz.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？为什么我们只听到了“砰”声而没有蜂鸣声？让我们回到之前讨论`map`时提到的事情。记得我们说过`map`实际上并不执行计算，它只是存储计算指令吗？这就是我们称之为“惰性”的原因。在这个例子中，执行`FB.foo`计算指令包括我们在请求它时`FB`的状态。所以，由于在请求指令时`FB.n`为0，`map`对所有操作都使用`FB.n`
    = 0，即使在我们使用它时`FB.n`发生了变化。由于`FB.n` = 0总是产生“砰”声，所以我们只听到了“砰”声。
- en: 'We can test this by changing `FB.n` to 2, which should always produce a buzz,
    and running the same command. That would look something like this:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`FB.n`改为2来测试它，这将始终产生“蜂鸣”，并运行相同的命令。这看起来可能像这样：
- en: '[PRE16]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, like we expect, we store the instructions for `FB.foo` when `FB.n = 2`,
    and the result is that we get all buzz and no fizz.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，正如我们所期望的，当`FB.n = 2`时，我们存储了`FB.foo`的指令，结果是得到了所有的buzz而没有fizz。
- en: 'What can we do instead? Often, situations like this simply require us to rethink
    the problem. A common solution is to take internal state and make it an external
    variable. For example, we could use the numbers generated by `range` instead of
    the internal values stored by `FB`. We could then also replace the class with
    a simple function, like this:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做什么呢？通常，类似这种情况需要我们重新思考问题。一个常见的解决方案是将内部状态变成外部变量。例如，我们可以使用`range`生成的数字而不是`FB`存储的内部值。然后我们也可以将类替换为一个简单的函数，如下所示：
- en: '[PRE17]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function does exactly what the `.foo` method does, but it relies on the
    value of an external variable `n` instead of an internal state `self.n`. We can
    then apply this to the numbers generated by range with a parallel `map` and get
    our results back, just like we expect.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数完全做了`.foo`方法所做的事情，但它依赖于外部变量`n`的值，而不是内部状态`self.n`。然后我们可以将这个函数应用到由`range`生成的数字上，并通过并行`map`得到我们预期的结果。
- en: '[PRE18]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When you run this, note that the printed values don’t return in the correct
    order. That’s because, as we noted in the previous section, the processors are
    grabbing the first available job off the stack and completing it as fast as they
    can. Sometimes, a fizz job will go slower than a buzz, and two buzzes will be
    printed in a row. Other times, a buzz will take longer, and we’ll see three or
    more fizzes in a row. The resulting data, though, will be in the proper order:
    fizz, fizz, buzz . . . fizz, fizz, buzz . . . fizz, fizz, buzz.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个程序时，请注意打印的值不会按正确的顺序返回。这是因为，正如我们在上一节中提到的，处理器正在从堆栈中抓取第一个可用的任务并尽可能快地完成它。有时，fizz任务会比buzz慢，并且会连续打印两个buzz。有时，buzz会花费更长的时间，我们会看到三或更多的fizz连续出现。然而，最终的数据将会按正确的顺序排列：fizz,
    fizz, buzz ... fizz, fizz, buzz ... fizz, fizz, buzz。
- en: It’s worth taking a second to look at how this would be depicted visually. We’ll
    look first at what happens when we attempt to do parallelization with state, then
    at what happens without it. To start, let’s recall [figure 2.8](#ch02fig08) (as
    duplicated in [figure 2.10](#ch02fig10)).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花点时间看看这会如何在视觉上呈现。我们首先看看当我们尝试使用状态进行并行化时会发生什么，然后看看没有它会发生什么。首先，让我们回顾一下[图2.8](#ch02fig08)（如[图2.10](#ch02fig10)中重复所示）。
- en: Figure 2.10\. When we pickle work and distribute it with a parallel `map`, we’re
    pickling state information as well. This allows us to execute the work in parallel
    but may produce unexpected results.
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10。当我们使用并行`map`来序列化和分发工作时，我们也会序列化状态信息。这允许我们并行执行工作，但可能会产生意外的结果。
- en: '![](02fig10_alt.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![02fig10_alt.jpg](02fig10_alt.jpg)'
- en: 'This graphic demonstrates what’s happening when we’re performing parallel calculations:
    first we chop our task up into chunks—in this case, four—then we save those chunks
    to the disk in a pickled format. Our processors then grab them and work them until
    they’re all complete. With respect to state, it’s this second step—pickling the
    data—that we need to be most aware of.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了我们在进行并行计算时发生的情况：首先，我们将任务分割成块——在这个例子中是四块——然后将这些块以序列化的格式保存到磁盘上。然后我们的处理器获取它们并处理，直到全部完成。关于状态，我们需要最关注的是这个第二步——序列化数据。
- en: At the first step, `map` provides instructions for each part of the problem.
    This step is akin to our parallelization step, where we chunk the problem. Remember,
    `map` doesn’t do the work of the problem immediately, it writes the instructions
    and does them later—it’s lazy. Second, we save the instructions to a disk. `map`
    already captured these instructions, so it’s easy to do. However, note that we’re
    saving the instructions for the problem. That means that any state we needed is
    going to be stored as well, such as when we store `FB.n`. Then, the last step,
    our processors read the instructions and execute them.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，`map`为问题的每个部分提供指令。这一步类似于我们的并行化步骤，我们将问题分割成块。记住，`map`不会立即完成问题的任务，它会写入指令并在稍后执行——它是懒惰的。第二步，我们将指令保存到磁盘上。`map`已经捕获了这些指令，所以很容易做到。然而，请注意，我们正在保存问题的指令。这意味着我们需要的任何状态都将被存储，例如当我们存储`FB.n`时。最后一步，我们的处理器读取指令并执行它们。
- en: '2.3\. Putting it all together: Scraping a Wikipedia network'
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3。整合一切：抓取维基百科网络
- en: We’ve covered a lot of powerful stuff in this chapter. To wrap it all up, let’s
    consider one final scenario involving creating a network graph, such as the one
    in [Figure 2.11](#ch02fig11).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了很多强大的内容。为了总结所有内容，让我们考虑一个最终的场景，即创建一个网络图，例如[图2.11](#ch02fig11)中的那种。
- en: Figure 2.11\. A network graph is a series of nodes connected by edges that is
    often used to display relationships between objects, such as friendship between
    people, communication between systems, or roads between cities.
  id: totrans-357
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11。网络图是由边连接的节点序列，常用于显示对象之间的关系，例如人与人之间的友谊、系统之间的通信或城市之间的道路。
- en: '![](02fig11_alt.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig11_alt.jpg)'
- en: '|  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: 'We want to create a topic network graph from Wikipedia. That is, we want to
    be able to enter a term (for example: *parallel computing*) and find all the Wikipedia
    pages in that page’s immediate *network*—pages that link to that page or that
    that page links to. The result will be a graph of all the pages in our network.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望从维基百科创建一个主题网络图。也就是说，我们希望能够输入一个术语（例如：*并行计算*）并找到该页面直接的网络中的所有维基百科页面——链接到该页面或该页面链接到的页面。结果将是我们网络中所有页面的图。
- en: '|  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s begin by thinking about the problem at hand and sketching out a solution.
    Wikipedia has a nice API for getting data about Wikipedia pages, so we’ll want
    to use that. And we know we’re going to start from a single page, so we’ll want
    to use that page as a starting point for our network. From that page, we’ll want
    to get all the inbound and outbound links, which will be other nodes in our graph.
    Then, for each of the other nodes, we’ll want to get the nodes to which those
    are related.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先思考当前的问题，并绘制出一个解决方案。维基百科有一个很好的API用于获取维基百科页面的数据，因此我们想要使用它。我们知道我们将从一个单独的页面开始，因此我们想要将这个页面作为我们网络的起点。从这个页面开始，我们想要获取所有入链和出链，这些将成为我们图中的其他节点。然后，对于每个其他节点，我们想要获取与这些节点相关联的节点。
- en: 'We can break that down further into a to-do list:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个任务进一步分解成一个待办事项列表：
- en: Write a function that gets the inbound and outbound links of a Wikipedia page.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数来获取维基百科页面的入链和出链。
- en: Get the inbound and outbound links from our initial page.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的初始页面获取入链和出链。
- en: Gather those pages in one long list.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些页面整理成一个长长的列表。
- en: Get the inbound and outbound links from all of those pages.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有这些页面获取入链和出链。
- en: We’ll do this in parallel, to speed things up.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将并行进行，以加快速度。
- en: Represent all our links as edges between pages.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有链接表示为页面之间的边。
- en: 'Bonus: Use a graphing library (like networkx) to display the graph.'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励：使用图形库（如networkx）来显示图形。
- en: 'Let’s first write a function to get the inbound and outbound links of a Wikipedia
    page based on its title. We’ll start by importing the `JSON` module and the `requests`
    class from the `urllib` module. You’ll remember `urllib` from before: this module
    helps us with getting data from the internet, which is exactly what we’ll want
    to be doing with our Wikipedia pages. The `JSON` module is a module for parsing
    data in JSON format. It reads JSON into native Python types. We’ll use this to
    convert the data Wikipedia provides us into an easily manageable format.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个函数来获取基于维基百科页面标题的入链和出链。我们将首先从`urllib`模块导入`JSON`模块和`requests`类。你可能会记得`urllib`：这个模块帮助我们从互联网获取数据，这正是我们想要在维基百科页面中做的事情。`JSON`模块是一个用于解析JSON格式数据的模块。它将JSON数据读取为原生Python类型。我们将使用它将维基百科提供的数据转换为易于管理的格式。
- en: Next, we’ll create a little function to help us turn Wikipedia page links into
    just the titles of those pages. Wikipedia naturally packages these links as JSON
    objects—we only want the title string.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个小的函数来帮助我们将维基百科页面链接转换为这些页面的标题。维基百科自然将这些链接打包成JSON对象——我们只需要标题字符串。
- en: Then, finally, we get to creating our actual function for getting information
    from Wikipedia. Our function, `get_wiki_links` expects a page title and turns
    that into a `dict` of inbound and outbound links. This `dict` will allow us easy
    access to those links later on.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达了创建从维基百科获取信息的实际函数。我们的函数`get_wiki_links`期望一个页面标题，并将其转换为包含入链和出链的`dict`。这个`dict`将允许我们稍后轻松访问这些链接。
- en: The first thing we do in this function is create the URL for our query. If you’re
    curious about where the URL comes from, Wikipedia has a well-documented API online;
    however, I’ll explain the pertinent parts here. The `/w/api.php` tells Wikipedia
    that we want to use its API and not request a standard web page. The `action=query`
    tells Wikipedia that we’ll be doing a query action. Query is one of the many actions
    Wikipedia makes available. It’s tailored for getting metadata about pages, such
    as which pages link to and are linked from a given page.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们首先创建查询的URL。如果你对URL的来源感兴趣，维基百科在网上有很好的API文档；然而，我将在下面解释相关部分。`/w/api.php`告诉维基百科我们想要使用其API而不是请求标准网页。`action=query`告诉维基百科我们将执行查询操作。查询是维基百科提供的许多操作之一。它专门用于获取有关页面的元数据，例如哪些页面链接到给定页面以及哪些页面被链接到。
- en: The `prop=links|linkshere` tells the Wikipedia API that the properties we’re
    interested in are the page’s links and which pages link to the page. `pllimt`
    and `lhlimit` tell the API that we want to get at most 500 results. This is the
    maximum number of results we can get without registering ourselves as a bot. The
    `title` parameter is where we put the title of the page we want, and the `format`
    parameters define how the data returned to us should be formatted. We’ll choose
    JSON for convenience’s sake.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`prop=links|linkshere`告诉维基百科API我们感兴趣的属性是页面的链接以及哪些页面链接到该页面。`pllimit`和`lhlimit`告诉API我们希望获取最多500个结果。这是在不注册为机器人的情况下我们可以获取的最大结果数。`title`参数是我们放置我们想要页面的标题的地方，`format`参数定义了返回给我们的数据应该如何格式化。为了方便起见，我们将选择JSON格式。'
- en: Next, with our URL set, we can pass it `request.urlopen`, which opens the URL
    with a `get` request. Wikipedia takes our request to its API and passes us back
    the information we requested. We can read this information into memory with the
    `.read` method, and we do. Since we asked Wikipedia to return this information
    as JSON, we can then read the JSON string with `json.reads`, which turns JSON
    strings into Python objects. The resulting object `j` is a `dict` that represents
    the JSON object that Wikipedia returns.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置好我们的URL后，我们可以将其传递给`request.urlopen`，它使用`get`请求打开URL。维基百科将我们的请求发送到其API，并返回我们请求的信息。我们可以使用`.read`方法将此信息读入内存，我们确实这样做了。由于我们要求维基百科以JSON格式返回此信息，因此我们可以使用`json.reads`读取JSON字符串，它将JSON字符串转换为Python对象。生成的对象`j`是一个`dict`，它表示维基百科返回的JSON对象。
- en: Now we can wade through those objects and pull out the links, which will be
    four levels deep at `page`[`'query']['pages']`[`0]["links"]` and `page`[`'query']['pages']`
    [`0]["linkshere"]`. The former object contains the pages to which our current
    page links, and the latter contains the pages that link to our current page. The
    Wikipedia API defines this structure, which is how we know where to find the data
    we need. These objects—the `links` and `linkshere`—as we noted before, are not
    the page titles but JSON objects, with the title as an element. To get just the
    title, we’ll use our `link_to_title` function. Because we’ll have more than one
    link and these links will be in a list, we’ll use `map` to transform all of the
    objects to just their titles.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以遍历这些对象，并从中提取链接，这些链接将在`page`[`'query']['pages']`[`0"]["links"]`和`page`[`'query']['pages']`[`0"]["linkshere"]`中达到四个层级。前一个对象包含指向我们当前页面的页面，后一个对象包含链接到我们当前页面的页面。维基百科API定义了这种结构，这是我们了解如何找到所需数据的方式。正如我们之前所提到的，这些对象——`links`和`linkshere`——不是页面标题，而是JSON对象，其中标题作为一个元素。为了只获取标题，我们将使用我们的`link_to_title`函数。由于我们将有多个链接，并且这些链接将在一个列表中，我们将使用`map`将所有对象转换为它们的标题。
- en: Finally, we’ll return these objects as a `dict`. Altogether, that will look
    like the following listing.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这些对象作为`dict`返回。整体来看，如下所示。
- en: Listing 2.7\. A function for retrieving a Wikipedia page’s network from its
    title
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7\. 从标题检索维基百科页面网络的函数
- en: '[PRE19]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* Imports the libraries we’ll need**'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导入我们需要的库**'
- en: '***2* Creates a helper function for getting the title from a link result**'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个辅助函数，用于从链接结果中获取标题**'
- en: '***3* Creates a helper function that gets titles for the links found, if they
    exist**'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个辅助函数，用于获取找到的链接的标题（如果存在的话**）'
- en: '***4* Defines our get_ wiki_links function**'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 定义我们的get_wiki_links函数**'
- en: '***5* Quotes the title to ensure it’s URL-safe**'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 引用标题以确保它是URL安全的**'
- en: '***6* Sends an HTTP request to the URL and reads the response**'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 向URL发送HTTP请求并读取响应**'
- en: '***7* Parses the response as JSON**'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 将响应解析为JSON**'
- en: '***8* Cleans the inbound and outbound links if they exist**'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 如果存在，清理传入和传出的链接**'
- en: '***9* Returns the page’s title and its inbound and outbound links**'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 返回页面的标题及其入链和出链**'
- en: At this point, we’ve tackled item 1 on our to-do list and put ourselves in a
    good position to tackle 2 and 3\. We can do that now by writing a small function
    and creating an *only on execute* section of our script. Let’s do that now.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了待办事项列表中的第1项，并为我们处理第2项和第3项奠定了良好的基础。现在我们可以通过编写一个小函数并创建脚本中的“仅在执行时”部分来实现这一点。让我们现在就做吧。
- en: 'Next, here’s a simple function that will flatten the page’s inbound and outbound
    links into one big list:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这是一个简单的函数，它将页面的入链和出链展平成一个大的列表：
- en: '[PRE20]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And here’s the part of our code that will run if and only if we call this script
    with Python3:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们代码的一部分，它将在我们用Python3调用此脚本时运行：
- en: '[PRE21]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `if __name__ == "__main__"` tells Python only to use this code if it’s called
    directly as a script. The line after that says to get all the links from the parallel
    computing page on Wikipedia using our function. And the last line stores the network
    in a variable.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '`if __name__ == "__main__"` 告诉Python只有当它直接作为脚本调用时才使用此代码。之后的行表示使用我们的函数从维基百科的并行计算页面获取所有链接。最后一行将网络存储在一个变量中。'
- en: 'Next, let’s use this list, and the function we just wrote, to get all of the
    Wikipedia pages in the network of the parallel computing page. We’ll do this in
    parallel to speed things up. To do that, we’re going to want to extend the *run
    only when executed* section of our code from before. We’ll add a few lines so
    it looks like this:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用这个列表以及我们刚刚编写的函数，来获取并行计算页面网络中的所有维基百科页面。我们将并行执行以加快速度。为此，我们需要扩展之前代码中的“仅在执行时运行”部分。我们将添加几行，使其看起来像这样：
- en: '[PRE22]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We’ve called `Pool` again to round up some processors to use in parallel programming.
    We then use those processors to get the Wikipedia page info for each page that
    either linked to or was linked to by our root page: parallel computing. Assuming
    we have four processors, we’re completing this task in one-quarter the time it
    would take if we got the info for the pages one by one.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次调用 `Pool` 来召集一些处理器用于并行编程。然后我们使用这些处理器获取每个页面（无论是链接到还是被我们的根页面“并行计算”链接）的维基百科页面信息。假设我们有四个处理器，我们将在这个任务上节省四分之一的时间，如果我们逐个获取页面的信息的话。
- en: 'Now we want to represent each of these page objects as the edges between pages.
    What is this going to look like? A good representation of this will be a `tuple`,
    with the object in first position representing the page doing the linking and
    the object in second position representing the page being linked to. If `Parallel_computing`
    links to Python, we’ll want a `tuple` like this: `("Parallel_computing", "Python")`.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要将每个页面对象表示为页面之间的边。这会是什么样子呢？这个表示将是一个 `tuple`，其中第一个位置的元素代表执行链接的页面，第二个位置的元素代表被链接到的页面。如果
    `Parallel_computing` 链接到 Python，我们想要的 `tuple` 将是这样的：`("Parallel_computing", "Python")`。
- en: To create these, we’ll need another function. This function will turn each page
    `dict` into a list of these edge `tuple`s.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这些，我们需要另一个函数。这个函数将每个页面 `dict` 转换为这些边 `tuple` 的列表。
- en: '[PRE23]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function loops through each page in a page’s network, creating a list of
    `tuple`s for all the pages in the out-links, of the form `(page,out-link)`, and
    for all the pages in the in-links, of the form `(in-link,page)`. We’ll then add
    these two lists together and return them.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数遍历一个页面的网络中的每个页面，为所有出链页面创建一个 `tuple` 列表，形式为 `(page,out-link)`，以及所有入链页面，形式为
    `(in-link,page)`。然后我们将这两个列表合并并返回。
- en: 'We’ll also need to update the script portion of our code. That section now
    looks like this:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更新代码中的脚本部分。这部分现在看起来像这样：
- en: '[PRE24]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We’ve added a line that applies this `page_to_edges` function to all the pages
    we gathered with our previous function. Because we still have all those processors
    handy, let’s just use them again to get this task done faster too.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一行，将 `page_to_edges` 函数应用于我们之前函数收集的所有页面。因为我们仍然有所有这些处理器可用，所以我们可以再次使用它们来更快地完成这项任务。
- en: The last thing we’ll want to do is flatten this list of edges into one big list.
    The best way to do so is to use Python’s itertools `chain` function. The `chain`
    function takes an iterable of iterables and chains them together so they can all
    be accessed one after another. For example, it allows us to treat [[1,2,3],[1,2],[1,2,3]]
    as if it was [1,2,3,1,2,1,2,3].
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后想要做的是将这个边列表展平成一个大的列表。这样做最好的方式是使用Python的itertools `chain` 函数。`chain` 函数接受一个可迭代的可迭代对象，并将它们连接起来，以便可以逐个访问。例如，它允许我们将
    [[1,2,3],[1,2],[1,2,3]] 视为 [1,2,3,1,2,1,2,3]。
- en: We’ll use this chain function on our `edges` object. At this point, we’re done
    needing our processors for parallelization, so we’ll out-dent, moving out of this
    block of code, and let our processors go.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的`edges`对象上使用这个链式函数。到这一点，我们不再需要我们的处理器来进行并行化，所以我们将缩进，退出这个代码块，并让我们的处理器释放。
- en: '[PRE25]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `chain` function is lazy by default, so we’ll need to wrap it in a `list`
    call, just like `map`, if we want to print it to the screen. If you do decide
    to print it to the screen, don’t expect to see much. You’ll be looking at 1,000,000
    string-string `tuple`s (1,000 `tuple`s for each of the 1,000 pages in our network).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`chain`函数默认是惰性的，所以如果我们想将其打印到屏幕上，就像`map`一样，我们需要将其包裹在一个`list`调用中。如果你决定将其打印到屏幕上，不要期望看到太多。你将看到1,000,000个字符串-字符串`tuple`s（网络中的每1000页有1000个`tuple`）。'
- en: '|  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'We just wrote about 50 lines of code, piece by piece. When we code like this,
    sometimes we can miss little things that cause our code to break. If you ever
    have trouble getting your code to run, remember that you can find the source code
    for this book online. Please refer to it if you ever spend more than a few minutes
    debugging: [www.manning.com/downloads/1961](http://www.manning.com/downloads/1961).'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚写了大约50行代码，一点一点地。当我们这样编码时，有时我们可能会错过一些导致代码出错的小细节。如果你在运行代码时遇到麻烦，请记住，你可以在网上找到这本书的源代码。如果你在调试时花费了超过几分钟的时间，请参考它：[www.manning.com/downloads/1961](http://www.manning.com/downloads/1961)。
- en: '|  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 2.3.1\. Visualizing our graph
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 可视化我们的图
- en: 'The best way to visualize our graph is to take it out of Python and import
    it into Gephi, a dedicated piece of graph visualization software. Gephi is well
    known in the social sciences for being an excellent network and graph visualization
    tool. It can work with data in many formats but prefers a custom format called
    .gefx. We’ll use a Python library called *networkx* to export our graph to this
    format. That whole process will look something like this:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化我们的图的最佳方式是将它从Python中移出，并导入到Gephi中，这是一款专门的图形可视化软件。Gephi在社会科学中因作为出色的网络和图形可视化工具而闻名。它可以处理多种格式的数据，但更喜欢一种自定义格式，即.gefx。我们将使用一个名为*networkx*的Python库将我们的图导出为此格式。整个过程看起来可能像这样：
- en: '[PRE26]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What we’re doing here is creating a directed graph (`nx.DiGraph`) object and
    adding edges to it by iterating through our chained edges. The graph object has
    a method, `.add_edge`, that allows us to construct a graph by declaring its edges
    one by one. Once this is done, all that’s left to do is export the graph in the
    Gephi format, .gefx. The networkx library has a convenience function for that
    called `write_gefx`. We’ll use that on our graph object and provide a path name.
    The graph is then saved in .gefx format at that path. On my machine, the output
    file is just under 36 MB.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是创建一个有向图（`nx.DiGraph`）对象，并通过遍历我们的链式边来向其中添加边。图对象有一个`.add_edge`方法，它允许我们通过逐个声明其边来构建一个图。一旦完成，剩下的就是将图导出为Gephi格式，即.gefx。networkx库有一个方便的函数叫做`write_gefx`，我们将使用它来处理我们的图对象并提供一个路径名。然后，图将以.gefx格式保存在该路径。在我的机器上，输出文件的大小略低于36
    MB。
- en: '|  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-420
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Gephi is excellent graph visualization software; however, this is not a book
    on visualizing graphs. If you don’t think you’ll find it satisfying to visualize
    your web scraping, or if you get frustrated using Gephi, feel free to skip ahead.
    We won’t use Gephi again in this book.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: Gephi是一款出色的图形可视化软件；然而，这本书并不是关于图形可视化的。如果你认为你不会从可视化你的网络爬取中找到满足感，或者如果你在使用Gephi时感到沮丧，请随意跳过。在这本书中我们不会再使用Gephi。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: From here, we can load up Gephi, import our .gefx file, and view our graph.
    If you don’t have Gephi installed, you can find it at [https://gephi.org](https://gephi.org).
    Gephi is free software, distributed under an open source license, and runs on
    Windows, MacOS, and Linux.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以打开Gephi，导入我们的.gefx文件，并查看我们的图。如果你没有安装Gephi，你可以在[https://gephi.org](https://gephi.org)找到它。Gephi是免费软件，在开源许可下分发，并在Windows、MacOS和Linux上运行。
- en: When you open Gephi, you may have to play around with the settings a bit with
    the graph to get it to show something pretty. I’ll leave the graph visualization
    up to you and your creativity because I’m far from an expert in this area.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开Gephi时，你可能需要调整一些设置，以便让图显示得更好看。我将把图形可视化留给你和你的创造力，因为我远非这个领域的专家。
- en: 'If you’re short on patience for learning how to visualize a graph with more
    than 100,000 nodes, change the settings on our query to retrieve a smaller number
    of pages. I’ll also leave it up to you to look back through the code and figure
    out how to do that. (Hint: It’s in our request to the Wikipedia API.)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你学习如何可视化超过10万个节点的图没有耐心，请更改我们的查询设置以检索更少的页面数量。我还会让你自己回顾代码，找出如何做到这一点。（提示：在我们的Wikipedia
    API请求中。）
- en: When I request only 50 neighbors from each page, I end up with a network of
    about 1,300 nodes that looks something like [figure 2.12](#ch02fig12) by default
    in Gephi.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 当我只请求每个页面的50个邻居时，我最终得到大约1,300个节点的网络，默认情况下在Gephi中看起来像[图2.12](#ch02fig12)。
- en: Figure 2.12\. Network of Wikipedia pages surrounding *parallel computing*
  id: totrans-427
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12\. 围绕*并行计算*的Wikipedia页面网络
- en: '![](02fig12_alt.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig12_alt.jpg)'
- en: 2.3.2\. Returning to map
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 返回到映射
- en: Before we wrap up the chapter, it’s worth looking at how what we’ve done fits
    into the `map` diagrams we’ve been using. Coming back to the `map` data transformation
    diagrams is useful because it allows us to contextualize a complex task—web scraping
    and creating an entity network—in a simple way.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，看看我们所做的工作如何适合我们一直在使用的`map`图是有意义的。回到`map`数据转换图是有用的，因为它以简单的方式使我们能够将复杂任务——网络抓取和创建实体网络——进行上下文化。
- en: 'First, let’s start with a diagram of the entire process ([figure 2.13](#ch02fig13)).
    On the left, we start with our seed document. We apply our `get_wiki_links` function
    to this document to get all the pages in our network: the inbound and outbound
    linking pages. From there, and secondly, we map the `get_wiki_links` function
    across all of these pages. This returns the extended network, that is, the pages
    that link to and are linked from the pages that link to and are linked from our
    seed page. Third, we convert all of these links into edges. This transforms the
    data from a more implicit data structure to a more explicit definition of a graph.
    And finally, we use each of these edges to construct the graph.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从整个过程的流程图([图2.13](#ch02fig13))开始。在左侧，我们从种子文档开始。我们将`get_wiki_links`函数应用于此文档以获取我们网络中的所有页面：入链和出链的页面。从那里，其次，我们将`get_wiki_links`函数映射到所有这些页面上。这返回了扩展网络，即链接到并从我们的种子页面链接的页面。第三，我们将所有这些链接转换为边。这将数据从更隐式的数据结构转换为更明确的图定义。最后，我们使用这些边中的每一个来构建图。
- en: Figure 2.13\. We’ll turn a single seed page into a network of pages in four
    steps.
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13\. 我们将单个种子页面转换为页面网络，分为四个步骤。
- en: '![](02fig13_alt.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig13_alt.jpg)'
- en: 'In this process, we used two map statements: one to turn our initial network
    into an extended network, and one to turn our extended network into edges. In
    the first instance, shown in [figure 2.13](#ch02fig13), we take all of the links
    that we retrieved from our seed scrape, we scrape these, and we return the network
    of each link. The result is that where before we had a list of pages (or, if you
    remember what the data looked like, a `dict` with the page title, the inbound
    links, and the outbound links), we now have a list of lists of pages (or again:
    a list of these “page” `dict`s). Though there is a lot happening in between—we
    ping the Wikipedia API, the Wikipedia API fetches the page and returns the result,
    we parse that result into JSON, we sort through the JSON to find the values we
    want, we store them in a `dict` and return the `dict`—we can represent all of
    this as a data transformation from one object to the next.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们使用了两个映射语句：一个将我们的初始网络扩展为一个扩展网络，另一个将我们的扩展网络转换为边。在第一种情况下，如[图2.13](#ch02fig13)所示，我们取自种子抓取的所有链接，抓取这些链接，并返回每个链接的网络。结果是，之前我们有一个页面列表（或者，如果你还记得数据的样子，一个包含页面标题、入链和出链的`dict`），我们现在有一个页面列表的列表（或者再次：一个包含这些“页面”`dict`的列表）。尽管中间发生了很多事情——我们ping
    Wikipedia API，Wikipedia API获取页面并返回结果，我们将结果解析为JSON，我们通过JSON找到我们想要的值，我们将它们存储在`dict`中并返回`dict`——我们可以将所有这些表示为从一个对象到下一个对象的数据转换。
- en: 'Next, we complete the third step of taking the networks retrieved in our second
    step and turning them into a list of edges that we can use to define a directed
    graph. We wrote a `path_to_edges` function to use for this purpose. What we’re
    doing is not that complicated: we’re taking two lists of strings and turning them
    into a single list of `tuple`s; however, abstracting that away with the `path_to_edges`
    function allows us to visualize the entire transformation at a higher level. This
    higher-level understanding corresponds directly with our overall process and highlights
    what’s going on: our link networks are being transformed into edges.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们完成第三步，将我们在第二步中检索到的网络转换为我们可以用来定义有向图的边的列表。我们编写了一个 `path_to_edges` 函数来用于这个目的。我们所做的不那么复杂：我们正在将两个字符串列表转换为单个
    `tuple` 列表；然而，通过 `path_to_edges` 函数将这一点抽象出来，允许我们在更高层次上可视化整个转换。这种高层次的理解直接对应于我们的整体过程，并突出了正在发生的事情：我们的链接网络正在被转换为边。
- en: Looking back at the Wikipedia scraping, network creation program we just wrote,
    we can see that using `map` is quite natural for a lot of tasks. Indeed, anytime
    we’re converting a sequence of some type into a sequence of another type, what
    we’re doing can be expressed as a map. I like to refer to these situations as
    N-to-N transformations because we’re converting some number of data elements,
    N, into that same number of data elements but in a different format.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们刚刚编写的 Wikipedia 抓取和网络创建程序，我们可以看到使用 `map` 对于许多任务来说是非常自然的。事实上，每次我们将某种类型的序列转换为另一种类型的序列时，我们正在做的事情都可以表达为
    `map`。我喜欢将这些情况称为 N-to-N 转换，因为我们正在将一些数据元素 N 转换为相同数量的数据元素，但格式不同。
- en: In just this last example, we encountered two of these N-to-N situations. We
    first turned N links into N networks of links. Then we turned N networks of links
    into N edges. In each of these situations, we used a map, as we just diagrammed.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这个最后的例子中，我们遇到了两种这些 N-to-N 情况。我们首先将 N 个链接转换为 N 个链接网络。然后我们将 N 个链接网络转换为 N 个边。在这些每种情况下，我们都使用了
    `map`，就像我们刚才所绘制的。
- en: 'We also used parallel programming in each of these situations to complete the
    task more quickly. They were excellent candidates for parallel programming because
    we had time-consuming, repetitive tasks that we could express neatly in self-contained
    instructions. We used a parallel `map` to accomplish this. The parallel `map`
    allows us to express our desire for parallelization and use syntax similar to
    what we’d use if we were doing a nonparallel `map`. All in all, the amount of
    effort it takes to make this problem parallel only adds up to four lines of code:
    one import; wrangling our processors with `Pool();` and modifying our `map` statements
    to use `Pool`’s `.map` method.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也在这些情况下使用了并行编程来更快地完成任务。它们是并行编程的绝佳候选者，因为我们有耗时且重复的任务，我们可以用自包含的指令清晰地表达出来。我们使用并行
    `map` 来完成这个任务。并行 `map` 允许我们表达我们的并行化愿望，并使用类似于我们进行非并行 `map` 时使用的语法。总的来说，使这个问题并行化所需的工作量仅相当于四行代码：一个导入；使用
    `Pool()` 处理我们的处理器，并修改我们的 `map` 语句以使用 `Pool` 的 `.map` 方法。
- en: 2.4\. Exercises
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 练习
- en: 2.4.1\. Problems of parallelization
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 并行化的问题
- en: Parallelization is an effective way to speed up our programs but may come with
    a few problems. Earlier in this chapter, I named three. How many can you remember,
    and what are they?
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化是一种有效加快我们程序速度的方法，但可能会带来一些问题。在本章的早期，我提到了三个。你能记住多少，它们是什么？
- en: 2.4.2\. Map function
  id: totrans-442
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. `map` 函数
- en: The `map` function is a key piece of how we’ll approach large datasets in this
    book. Which sentence best describes the map function?
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '`map` 函数是我们在本书中处理大数据集的关键部分。哪句话最能描述 `map` 函数？'
- en: '`map` transforms a sequence of data into a different, same-sized sequence.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 将数据序列转换为不同大小相同的序列。'
- en: '`map` allows us to process data conditionally, replacing `if-else` statements.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 允许我们条件性地处理数据，替换 `if-else` 语句。'
- en: '`map` replaces conditional `while` loops with optimized bytecode.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 用优化的字节码替换条件 `while` 循环。'
- en: 2.4.3\. Parallelization and speed
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 并行化和速度
- en: Parallelization is useful because it allows us to process large datasets more
    quickly. Which of the following explains how parallelization works?
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化是有用的，因为它允许我们更快地处理大数据集。以下哪个解释说明了并行化是如何工作的？
- en: Parallelization optimizes our code during compilation.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化在编译时优化我们的代码。
- en: Parallelization computes similar tasks on several compute resources.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化在多个计算资源上计算类似任务。
- en: Parallelization removes duplication from our code and reduces the number of
    expensive operations.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化从我们的代码中移除重复，并减少昂贵的操作数量。
- en: 2.4.4\. Pickling storage
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4. 序列化存储
- en: Which of the following is *not* a good use for pickling?
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个不是使用序列化的好用途？
- en: Short-term, single-machine storage
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短期、单机存储
- en: Sharing data between compute tasks on a cluster
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上的计算任务之间共享数据
- en: Long-term storage where data integrity is key
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期存储，其中数据完整性至关重要
- en: 2.4.5\. Web scraping data
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.5. 网络抓取数据
- en: 'In web scraping, one of the most common things we’ll have to do is transform
    `dict`s into something else. Use `map` to transform a list of `dict`s into only
    the page text, with this as your input data:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络抓取中，我们最常见的事情之一是将 `dict`s 转换为其他东西。使用 `map` 将 `dict` 列表转换为仅包含页面文本的内容，以下为您的输入数据：
- en: '[PRE27]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Your resulting list should be `["Hello world!","No page found","Yet another
    web page."]`.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果列表应该是 `["Hello world!","No page found","Yet another web page."]`。
- en: 2.4.6\. Heterogenous map transformations
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.6. 异构映射转换
- en: So far, we’ve only looked at using `map` to transform homogenous lists, which
    contain data of all the same type. There’s no reason, though, why we couldn’t
    use `map` to transform a list of heterogenous data. Write a function that turns
    `[1, "A", False] into [2,"B",True]`.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了如何使用 `map` 来转换同质列表，这些列表包含相同类型的数据。然而，没有理由我们不能使用 `map` 来转换异构数据的列表。编写一个函数，将
    `[1, "A", False]` 转换为 `[2,"B",True]`。
- en: Summary
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: '`map` statements are an excellent way to transform a sequence of data (such
    as data in a list or a tuple) into a sequence of data of some other type.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 语句是转换数据序列（如列表或元组中的数据）到其他类型数据序列的绝佳方式。'
- en: Whenever we encounter a `for` loop, we should look for the opportunity to replace
    that loop with a `map`.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当我们遇到一个 `for` 循环时，我们应该寻找将其替换为 `map` 的机会。
- en: Because `map` defines rules for transformations, instead of performing the actual
    transformations, it is readily paired with parallel techniques that can allow
    us to speed up our code.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为 `map` 定义了转换规则，而不是执行实际的转换，所以它可以很容易地与并行技术配对，这可以让我们加快代码的执行速度。
- en: We can use `map` to scrape data from Wikipedia, or anywhere on the web, if we
    know a sequence of URLs that we want to scrape or APIs we want to call.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们知道要抓取的URL序列或要调用的API，我们可以使用 `map` 从维基百科或任何网页上抓取数据。
- en: Because `map` creates instructions and doesn’t immediately evaluate them, it
    doesn’t always play nicely with stateful objects, especially when applied in parallel.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为 `map` 创建指令但不立即评估它们，所以它并不总是与有状态的对象很好地配合，尤其是在并行应用时。
- en: Chapter 3\. Function pipelines for mapping complex transformations
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章. 用于映射复杂转换的函数管道
- en: '*This chapter covers*'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using `map` to do complex data transformations
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 进行复杂的数据转换
- en: Chaining together small functions into pipelines
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将小型函数链接成管道
- en: Applying these pipelines in parallel on large datasets
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型数据集上并行应用这些管道
- en: 'In the last chapter, we saw how you can use `map` to replace `for` loops and
    how using `map` makes parallel computing straightforward: a small modification
    to `map,` and Python will take care of the rest. But so far with `map`, we’ve
    been working with simple functions. Even in the Wikipedia scraping example from
    [chapter 2](kindle_split_011.html#ch02), our hardest working function only pulled
    text off the internet. If we want to make parallel programming *really* useful,
    we’ll want to use `map` in more complex ways. This chapter introduces how to do
    complex things with `map`. Specifically, we’re going to introduce two new concepts:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何使用 `map` 来替换 `for` 循环，以及使用 `map` 如何使并行计算变得简单：对 `map` 进行小的修改，Python
    将会处理其余部分。但到目前为止，我们使用 `map` 时一直在处理简单的函数。即使在第二章的维基百科抓取示例中，我们最费力的函数也只是在互联网上提取文本。如果我们想使并行编程真正有用，我们就会想以更复杂的方式使用
    `map`。本章介绍了如何使用 `map` 来做复杂的事情。具体来说，我们将介绍两个新概念：
- en: Helper functions
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 辅助函数
- en: Function chains (also known as pipelines)
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数链（也称为管道）
- en: 'We’ll tackle those topics by looking at two very different examples. In the
    first, we’ll decode the secret messages of a malicious group of hackers. In the
    second, we’ll help our company do demographic profiling on its social media followers.
    Ultimately, though, we’ll solve both of these problems the same way: by creating
    function chains out of small helper functions.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看两个非常不同的例子来处理这些主题。在第一个例子中，我们将解码一个恶意黑客小组的秘密信息。在第二个例子中，我们将帮助我们的公司在社交媒体粉丝中进行人口统计分析。最终，尽管如此，我们将以相同的方式解决这两个问题：通过将小的辅助函数组合成函数链。
- en: 3.1\. Helper functions and function chains
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 辅助函数和函数链
- en: '*Helper functions* are small, simple functions that we rely on to do complex
    things. If you’ve heard the (rather gross) saying, “The best way to eat an elephant
    is one bite at a time,” then you’re already familiar with the idea of helper functions.
    With helper functions, we can break down large problems into small pieces that
    we can code quickly. In fact, let’s put forth this as a possible adage for programmers:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '*辅助函数* 是一些小而简单的函数，我们依赖它们来完成复杂的事情。如果你听说过（相当粗俗的）说法，“吃大象最好的办法是一口一口吃”，那么你已经熟悉辅助函数的概念了。有了辅助函数，我们可以将大问题分解成小块，我们可以快速编码。事实上，让我们提出一个可能的格言供程序员参考：'
- en: '*The best way to solve a complex problem is one helper function at a time.*'
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*解决复杂问题的最佳方式是一次使用一个辅助函数*。'
- en: ''
  id: totrans-481
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*J.T. Wolohan*'
  id: totrans-482
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*J.T. Wolohan*'
- en: '*Function chains* or *pipelines* are the way we put helper functions to work.
    (The two terms mean the same thing, and different people favor one or the other;
    I’ll use both terms interchangeably to keep from overusing either one.) For example,
    if we were baking a cake (a complex task for the baking challenged among us),
    we’d want to break that process up into lots of small steps:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '*函数链* 或 *管道* 是我们将辅助函数投入使用的途径。（这两个术语意思相同，不同的人可能更喜欢其中一个；我将交替使用这两个术语，以避免过度使用任何一个。）例如，如果我们正在烘焙蛋糕（对我们中烘焙挑战者来说是一个复杂的任务），我们希望将这个过程分解成许多小步骤：'
- en: Add flour.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入面粉。
- en: Add sugar.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入糖。
- en: Add shortening.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入黄油。
- en: Mix the ingredients.
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合原料。
- en: Put the cake in the oven.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将蛋糕放入烤箱。
- en: Take the cake from the oven.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从烤箱中取出蛋糕。
- en: Let the cake set.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让蛋糕凝固。
- en: Frost the cake.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给蛋糕抹上奶油。
- en: Each of these steps is small and easily understood. These would be our helper
    functions. None of these helper functions by themselves can take us from having
    raw ingredients to having a cake. We need to chain these actions (functions) together
    to bake the cake. Another way of saying that would be that we need to pass the
    ingredients through our cake making pipeline, along which they will be transformed
    into a cake. To put this another way, let’s take a look at our simple `map` statement
    again, this time in [figure 3.1](#ch03fig01).
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤都很小，容易理解。这些将是我们的辅助函数。这些辅助函数中的任何一个单独都不能使我们从原材料变成蛋糕。我们需要将这些动作（函数）链接起来以烘焙蛋糕。另一种说法是，我们需要将原料通过我们的蛋糕制作管道传递，在这个过程中它们将被转换成蛋糕。换一种说法，让我们再次看看我们的简单
    `map` 语句，这次是在 [图3.1](#ch03fig01) 中。
- en: Figure 3.1\. The standard map statement shows how we can apply a single function
    to several values to return a sequence of values transformed by the function.
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1. 标准的 `map` 语句显示了我们可以如何将一个函数应用于多个值，以返回一个由该函数转换的值序列。
- en: '![](03fig01_alt.jpg)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01_alt.jpg)'
- en: As we’ve seen several times, we have our input values on the top, a function
    that we’re passing these values through in the middle, and on the bottom, we have
    our output values. In this case, `n+7` is our helper function. The `n+7` function
    does the work in this situation, not `map`. `map` applies the helper function
    to all of our input values and provides us with output values, but on its own,
    it doesn’t do us too much good. We need a specific output, and for that we need
    `n+7`.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们多次看到的，我们在顶部有输入值，中间有一个函数，我们将这些值传递给这个函数，底部是我们的输出值。在这种情况下，`n+7` 是我们的辅助函数。`n+7`
    函数在这种情况下做工作，而不是 `map`。`map` 将辅助函数应用于所有输入值，并为我们提供输出值，但单独来看，它对我们帮助不大。我们需要一个特定的输出，而为了达到这个目的，我们需要
    `n+7`。
- en: It’s also worth taking a look at function chains, sequences of (relatively)
    small functions that we apply one after another. They also have their basis in
    math. We get them from a rule that mathematicians call *function composition*.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，看看函数链，也就是一系列（相对）小的函数，我们一个接一个地应用。它们也有数学的基础。我们从数学家称为 *函数组合* 的规则中得到它们。
- en: 'Function composition says that a complex function like j(x) = ((x+7)²–2)*5
    is the same as smaller functions chained together that each do one piece of the
    complex function. For example, we might have these four functions:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 函数组合表明，像 j(x) = ((x+7)²–2)*5 这样复杂的函数与一系列较小的函数相同，这些函数连在一起，每个函数都执行复杂函数的一部分。例如，我们可能有以下四个函数：
- en: f(x) = x+7
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: f(x) = x+7
- en: g(x) = x²
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: g(x) = x²
- en: h(x) = x – 2
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: h(x) = x – 2
- en: i(x) = x * 5
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: i(x) = x * 5
- en: We could chain them together as i(h(g(f(x)))) and have that equal j(x). We can
    see that play out in [figure 3.2](#ch03fig02).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它们连在一起，形成 i(h(g(f(x))))，并使其等于 j(x)。我们可以在[图3.2](#ch03fig02)中看到这一过程。
- en: Figure 3.2\. Function composition says that if we apply a series of functions
    in sequence, then it’s the same as if we applied them all together as a single
    function.
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2。函数组合表明，如果我们按顺序应用一系列函数，那么这就像我们应用它们作为一个单一函数一样。
- en: '![](03fig02_alt.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig02_alt.jpg)'
- en: 'As we move through the pipeline in [figure 3.2](#ch03fig02), we can see our
    four helper functions: f, g, h, and i. We can see what happens as we input 3 for
    x into this chain of functions. First, we apply f to x and get 10 (3+7). Then
    we apply g to 10 and get 100 (10²). Then we apply h to 100 and get 98 (100–2).
    Then, lastly, we apply i to 98 and get 490 (98*5). The resulting value is the
    same as if we had input 3 into our original function j.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过[图3.2](#ch03fig02)中的管道时，我们可以看到我们的四个辅助函数：f、g、h和i。我们可以看到，当我们将x的值3输入到这个函数链中时会发生什么。首先，我们对x应用f，得到10（3+7）。然后我们对10应用g，得到100（10²）。然后我们对100应用h，得到98（100–2）。最后，我们最后对98应用i，得到490（98*5）。得到的值与我们将3输入原始函数j时得到的值相同。
- en: 'With these two simple ideas—helper functions and pipelines—we can achieve complex
    results. In this chapter, you’ll learn how to implement these two ideas in Python.
    As I mentioned in the chapter introduction, we’ll explore the power of these ideas
    in two scenarios:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这两个简单想法——辅助函数和管道——我们可以实现复杂的结果。在本章中，你将学习如何在Python中实现这两个想法。正如我在章节介绍中提到的，我们将探索这两个想法在两个场景中的力量：
- en: Cracking a secret code
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 破解秘密代码
- en: Predicting the demographics of social media followers
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测社交媒体粉丝的人口统计
- en: 3.2\. Unmasking hacker communications
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 揭示黑客通信
- en: Now that we’re familiar with the concept of function pipelines, let’s explore
    their power with a scenario. Here, we’ll conquer a complex task by breaking it
    up into many smaller tasks.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了函数管道的概念，让我们通过一个场景来探索它们的威力。在这里，我们将通过分解成许多更小的任务来征服一个复杂的任务。
- en: '|  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A malicious group of hackers has started using numbers in place of common characters
    and Chinese characters to separate words to foil automated attempts to spy on
    them. To read their communications—and find out what they’re saying—we need to
    write some code that will undo their trickery. Let’s write a script that turns
    their hacker speak into a list of English words.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 一群恶意的黑客开始使用数字代替常见字符和汉字来分隔单词，以阻止自动尝试监视他们。为了阅读他们的通信——并找出他们在说什么——我们需要编写一些代码来撤销他们的诡计。让我们编写一个脚本，将他们的黑客语言转换为英语单词列表。
- en: '|  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We’ll solve this problem like we’ve solved the previous problems in the book:
    by starting with `map`. Specifically, we’ll use the idea of `map` to set up the
    big picture data transformation that we’re doing. For that, we’ll visualize the
    problem in [figure 3.3](#ch03fig03).'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像在书中解决前一个问题一样解决这个问题：从`map`开始。具体来说，我们将使用`map`的想法来设置我们正在执行的大图数据转换。为此，我们将使用[图3.3](#ch03fig03)来可视化这个问题。
- en: Figure 3.3\. We can express our hacker problem as a `map` transformation in
    which we start with hard-to-read hacker messages as input. Then, after we clean
    them with our `hacker_translate` function, they become plain English text.
  id: totrans-516
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3。我们可以将我们的黑客问题表达为一个`map`转换，其中我们以难以阅读的黑客消息作为输入。然后，在我们用`hacker_translate`函数清理它们之后，它们变成了普通英语文本。
- en: '![](03fig03_alt.jpg)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig03_alt.jpg)'
- en: 'On the top, we have our input values. We can see that they’re some pretty hard-to-read
    hacker communications, and at first glance they don’t make a lot of sense. In
    the middle, we have our `map` statement and our `hacker_translate` function. This
    will be our heavy lifter function. It will do the work of cleaning the texts.
    And finally, on the bottom, we have our outputs: plain English.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们有我们的输入值。我们可以看到，它们是一些很难阅读的黑客通信，乍一看，它们并没有太多意义。在中间，我们有我们的`map`语句和`hacker_translate`函数。这将是我们的重载函数。它将完成清理文本的工作。最后，在底部，我们有我们的输出：普通英语。
- en: 'Now this problem is not a simple problem; it’s more like baking a cake. To
    accomplish it, let’s split it up into several smaller problems that we can solve
    easily. For example, for any given hacker string, we’ll want to do the following:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个问题不是一个简单的问题；它更像是烘焙蛋糕。为了完成它，让我们将其分解为几个更小的、我们可以轻松解决的问题。例如，对于任何给定的黑客字符串，我们希望执行以下操作：
- en: Replace all the 7s with t’s.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有的7替换为t。
- en: Replace all the 3s with e’s.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有的3替换为e。
- en: Replace all the 4s with a’s.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有的4替换为a。
- en: Replace all the 6s with g’s.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有的6替换为g。
- en: Replace all the Chinese characters with spaces.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有中文字符替换为空格。
- en: If we can do these five things for each string of hacker text, we’ll have our
    desired result of plain English text. Before we write any code, let’s take a look
    at how these functions will transform our text. First, we’ll start with replacing
    the 7s with t’s in [figure 3.4](#ch03fig04).
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以为每行黑客文本做这五件事情，我们将得到我们想要的纯英文文本。在我们编写任何代码之前，让我们看看这些函数将如何转换我们的文本。首先，我们将从[图3.4](#ch03fig04)中将7替换为t开始。
- en: Figure 3.4\. Part of our hacker translate pipeline will involve replacing 7s
    with t’s. We’ll accomplish that by mapping a function that performs that replacement
    on all of our inputs.
  id: totrans-526
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4. 我们的黑客翻译管道的一部分将涉及将7替换为t。我们将通过映射一个在所有输入上执行该替换的函数来完成。
- en: '![](03fig04_alt.jpg)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4辅助](03fig04_alt.jpg)'
- en: 'At the top of [figure 3.4](#ch03fig04), we see our unchanged input texts: garbled
    unreadable hacker communications. In the middle, we have our function `replace_7t`,
    which will replace all the 7s with t’s. And on the bottom, we have no 7s in our
    text anywhere. This makes our texts a little more readable.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.4](#ch03fig04)的顶部，我们看到我们的未更改的输入文本：混乱的、难以阅读的黑客通信。在中间，我们有我们的函数`replace_7t`，它将替换所有的7为t。在底部，我们的文本中没有任何7。这使得我们的文本稍微易于阅读。
- en: Moving on, we’ll replace all the 3s in all the hacker communications with e’s.
    We can see that happening in [figure 3.5](#ch03fig05).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将将所有黑客通信中的3替换为e。我们可以在[图3.5](#ch03fig05)中看到这一点。
- en: Figure 3.5\. The second step in our hacker translate pipeline will involve replacing
    3s with e’s. We’ll accomplish that by mapping a function that performs that replacement
    on all of our inputs.
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5. 我们黑客翻译管道的第二步将涉及将3替换为e。我们将通过映射一个在所有输入上执行该替换的函数来完成。
- en: '![](03fig05_alt.jpg)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5辅助](03fig05_alt.jpg)'
- en: At the top of [figure 3.5](#ch03fig05), we see our slightly cleaned hacker texts;
    we’ve already replaced the 7s with t’s. In the middle, we have our `replace_3e`
    function, which works to replace the 3s with e’s. And on the bottom, we have our
    now more readable text. All the 3s are gone, and we have some e’s in there.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.5](#ch03fig05)的顶部，我们看到我们的黑客文本经过轻微清理；我们已经将7替换为t。在中间，我们有我们的`replace_3e`函数，它旨在将3替换为e。在底部，我们有现在更易读的文本。所有的3都消失了，并且其中有一些e。
- en: Continuing on, we’ll do the same thing with 4s and a’s and 6s and g’s, until
    we’ve removed all our numbers. We’ll skip discussing those functions for the sake
    of avoiding repetition. Once we’ve completed those steps, we’re ready to tackle
    those Chinese characters. We can see that in [figure 3.6](#ch03fig06).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行，我们将用4和a以及6和g做同样的事情，直到我们移除了所有的数字。我们将跳过讨论这些函数，以避免重复。一旦我们完成了这些步骤，我们就准备好处理那些中文字符了。我们可以在[图3.6](#ch03fig06)中看到这一点。
- en: Figure 3.6\. Subbing on Chinese characters is going to be the last step in our
    `hacker_translate` function chain, and we can tackle it with a `map` statement.
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6. 在中文字符上进行替换将是我们的`hacker_translate`函数链中的最后一步，我们可以用`map`语句来处理它。
- en: '![](03fig06_alt.jpg)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6辅助](03fig06_alt.jpg)'
- en: 'In [figure 3.6](#ch03fig06), we see at the top we have mostly English sentences
    with Chinese characters smooshing the words together. In the middle, we have our
    splitting function: `sub_chinese`. And on the bottom, finally, we have our fully
    cleaned sentences.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.6](#ch03fig06)中，我们看到顶部主要是带有中文字符的英文句子，这些字符将单词粘在一起。在中间，我们有我们的分割函数：`sub_chinese`。在底部，最终，我们有完全清理后的句子。
- en: 3.2.1\. Creating helper functions
  id: totrans-537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1. 创建辅助函数
- en: Now that we’ve got our solution sketched out, let’s start writing some code.
    First, we’ll write all our replacement helper functions.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了解决方案，让我们开始编写一些代码。首先，我们将编写所有替换辅助函数。
- en: 'We’ll write all of these functions at once because they all follow a similar
    pattern: we take a string, find all of some character (a number) and replace it
    with some other character (a letter). For example, in `replace_7t`, we find all
    of the 7s and replace them with t’s. We do this with the built-in Python string
    method `.replace`. The `.replace` method allows us to specify as parameters which
    characters we want to remove and the characters with which we want to replace
    them, as shown in the following listing.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一次性编写所有这些函数，因为它们都遵循类似的模式：我们取一个字符串，找到所有某个字符（一个数字）并将其替换为另一个字符（一个字母）。例如，在 `replace_7t`
    中，我们找到所有的7并将其替换为t。我们使用内置的Python字符串方法 `.replace` 来做这件事。`.replace` 方法允许我们指定要移除的字符以及要替换的字符，如下面的列表所示。
- en: Listing 3.1\. Replacement helper functions
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 替换辅助函数
- en: '[PRE28]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* Replaces all the 7s with t’s**'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将所有7替换为t**'
- en: '***2* Replaces all the 3s with e’s**'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将所有3替换为e**'
- en: '***3* Replaces all the 6s with g’s**'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将所有6替换为g**'
- en: '***4* Replaces all the 4s with a’s**'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将所有4替换为a**'
- en: That takes care of the first handful of steps. Now we want to split where the
    Chinese text occurs. This task is a little more involved. Because the hackers
    are using different Chinese characters to represent spaces, not just the same
    one again and again, we can’t use `replace` here. We have to use a regular expression.
    Because we’re using a regular expression, we’re going to want to create a small
    class that can compile it for us ahead of time. In this case, our `sub_chinese`
    function is actually going to be a class method. We’ll see that play out in the
    following listing.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就完成了前几个步骤。现在我们想要在中文文本出现的地方进行分割。这个任务稍微复杂一些。因为黑客使用不同的中文字符来表示空格，而不仅仅是重复使用同一个字符，所以我们不能在这里使用
    `replace`。我们必须使用正则表达式。因为我们使用正则表达式，我们想要创建一个小的类，它可以提前为我们编译它。在这种情况下，我们的 `sub_chinese`
    函数实际上将是一个类方法。我们将在下面的列表中看到这一点。
- en: Listing 3.2\. Split on Chinese characters function
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 在中文字符上进行分割的函数
- en: '[PRE29]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '***1* We compile our regular expression on initialization of the class.**'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们在类初始化时编译我们的正则表达式。**'
- en: '***2* In this case, we want to match one or more Chinese characters. Those
    characters can be found in the Unicode range from 4e00 to 9fff.**'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在这个例子中，我们想要匹配一个或多个中文字符。这些字符可以在Unicode范围从4e00到9fff中找到。**'
- en: '***3* Now we can use this compiled regular expression in a method that uses
    the expression pattern’s split method.**'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 现在我们可以使用这个编译好的正则表达式在一个使用表达式模式分割方法的方法中。**'
- en: The first thing we do here is create a class called `chinese_matcher`. Upon
    initialization, that class is going to compile a regular expression that matches
    all the Chinese characters. That regular expression is going to be a range regular
    expression that looks up the Unicode characters between `\u4e00` (the first Chinese
    character in the Unicode standard) and `\u9fff` (the last Chinese character in
    the Unicode standard). If you’ve used regular expressions before, you should already
    be familiar with this concept for matching capital letters with regular expressions
    like `[A-Z]+`, which matches one or more uppercase English characters. We’re using
    the same concept here, except instead of matching uppercase characters, we’re
    matching Chinese characters. And instead of typing in the characters directly,
    we’re typing in their Unicode numbers.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的第一件事是创建一个名为 `chinese_matcher` 的类。在初始化时，该类将编译一个匹配所有中文字符的正则表达式。这个正则表达式将是一个范围正则表达式，查找Unicode字符从
    `\u4e00`（Unicode标准中的第一个中文字符）到 `\u9fff`（Unicode标准中的最后一个中文字符）。如果你之前使用过正则表达式，你应该已经熟悉这个概念，比如使用正则表达式
    `[A-Z]+` 匹配一个或多个大写英文字符。我们在这里使用的是同样的概念，只是不是匹配大写字符，而是匹配中文字符。而且不是直接输入字符，而是输入它们的Unicode编号。
- en: Having set up that regular expression, we can use it in a method. In this case,
    we’ll use it in a method called `.sub_chinese`. This method will apply the regular
    expression method .`split` to an arbitrary string and return the results. Because
    we know our regular expression matches one or more Chinese characters, the result
    will be that every time a Chinese character appears in the string, we’ll change
    that character to a space.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好正则表达式后，我们可以在一个方法中使用它。在这种情况下，我们将使用一个名为 `.sub_chinese` 的方法。这个方法将应用正则表达式方法 `.split`
    到任意字符串上，并返回结果。因为我们知道我们的正则表达式匹配一个或多个中文字符，所以结果将是每次字符串中出现中文字符时，我们将该字符替换为空格。
- en: 3.2.2\. Creating a pipeline
  id: totrans-554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 创建一个管道
- en: 'Now we have all of our helper functions ready and we’re ready to bake our hacker-foiling
    cake. The next thing to do is to chain these helper functions together. Let’s
    take a look at three ways to do this:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有的辅助函数，我们准备烘焙我们的黑客阻挠蛋糕。接下来要做的就是将这些辅助函数连接起来。让我们看看三种实现方式：
- en: Using a sequence of maps
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一系列映射
- en: Chaining functions together with `compose`
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compose`链式连接函数
- en: Creating a function pipeline with `pipe`
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pipe`创建函数管道
- en: A sequence of maps
  id: totrans-559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一系列地图
- en: For this method, we take all of our functions and map them across the results
    of one another.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种方法，我们取所有我们的函数并将它们映射到彼此的结果上。
- en: We map `replace_7t` across our sample messages.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将`replace_7t`映射到我们的样本消息上。
- en: Then we map `replace_3e` across the results of that.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将`replace_3e`映射到那个结果上。
- en: Then we map `replace_6g` across the results of that.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将`replace_6g`映射到那个结果上。
- en: Then we map `replace_4a` across the results of that.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将`replace_4a`映射到那个结果上。
- en: Finally, we map `C.sub_chinese`.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将`C.sub_chinese`映射。
- en: The solution shown in [listing 3.3](#ch03ex03) isn’t pretty, but it works. If
    you print the results, you’ll see all of our garbled sample sentences translated
    into easily readable English, with the words split apart from one another—exactly
    what we wanted. Remember, you need to evaluate `map` before you can print it!
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.3](#ch03ex03)中显示的解决方案并不美观，但它有效。如果你打印结果，你会看到所有的混乱样本句子都被翻译成了易于阅读的英语，单词之间被分开——这正是我们想要的。记住，你需要评估`map`之后才能打印它！'
- en: Listing 3.3\. Chaining functions by sequencing `map`s
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3\. 通过序列`map`链式连接函数
- en: '[PRE30]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Constructing a pipeline with compose
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用`compose`构建管道
- en: 'Although we certainly can chain our functions together this way, there are
    better ways. We’ll take a look at two functions that can help us do this:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们当然可以用这种方式将函数连接起来，但还有更好的方法。我们将查看两个可以帮助我们做到这一点的函数：
- en: '`compose`'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`compose`'
- en: '`pipe`'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pipe`'
- en: 'Each of these functions is in the toolz package, which you can install with
    `pip` like you would most python packages: `pip install toolz`.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数都在toolz包中，你可以像安装大多数Python包一样使用`pip`安装：`pip install toolz`。
- en: First, let’s look at `compose`. The `compose` function takes our helper functions
    in the reverse order that we would like them applied and returns a function that
    applies them in the desired order. For example, `compose(foo, bar, bizz)` would
    apply `bizz`, then `bar`, then `foo`. In the specific context of our problem,
    that would look like [listing 3.4](#ch03ex04).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看`compose`。`compose`函数以我们希望它们应用的相反顺序接收我们的辅助函数，并返回一个应用它们的函数。例如，`compose(foo,
    bar, bizz)`将应用`bizz`，然后是`bar`，然后是`foo`。在我们问题的具体上下文中，这看起来就像[列表3.4](#ch03ex04)。
- en: In [listing 3.4](#ch03ex04), you can see that we call the `compose` function
    and pass it all the functions we want to include in our pipeline. We pass them
    in reverse order because `compose` is going to apply them backwards. We store
    the output of our `compose` function, which is itself a function, to a variable.
    And then we can call that variable or pass it along to `map`, which applies it
    to all the sample messages.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表3.4](#ch03ex04)中，你可以看到我们调用了`compose`函数，并传递了所有我们想要包含在管道中的函数。我们以相反的顺序传递它们，因为`compose`将反向应用它们。我们将`compose`函数的输出（它本身也是一个函数）存储到一个变量中。然后我们可以调用那个变量或将它传递给`map`，它将对所有样本消息应用它。
- en: Listing 3.4\. Using `compose` to create a function pipeline
  id: totrans-576
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4\. 使用`compose`创建函数管道
- en: '[PRE31]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If you print this, you’ll notice that the results are the same as when we chained
    our functions together with a sequence of `map` statements. The major difference
    is that we’ve cleaned up our code quite a bit, and here we only have one `map`
    statement.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印出来，你会注意到结果与我们将函数通过一系列`map`语句连接起来的结果相同。主要的不同之处在于我们清理了大量的代码，这里我们只有一个`map`语句。
- en: Pipelines with pipe
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用`pipe`的管道
- en: Next, let’s look at `pipe`. The `pipe` function will pass a value through a
    pipeline. It expects the value to pass and the functions to apply to it. Unlike
    `compose`, `pipe` expects the functions to be in the order we want to apply them.
    So `pipe(x, foo, bar, bizz)` applies `foo` to x, then `bar` to that value, and
    finally `bizz` to that value. Another important difference between `compose` and
    `pipe` is that `pipe` evaluates each of the functions and returns a result, so
    if we want to pass it to `map`, we actually have to wrap it in a function definition.
    Again, turning to our specific example, that will look something like the following
    listing.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看`pipe`。`pipe`函数将通过管道传递一个值。它期望值通过管道传递，并应用函数。与`compose`不同，`pipe`期望函数按照我们想要应用它们的顺序。所以`pipe(x,
    foo, bar, bizz)`将`foo`应用于x，然后是`bar`应用于那个值，最后是`bizz`应用于那个值。`compose`和`pipe`之间的另一个重要区别是，`pipe`评估每个函数并返回一个结果，因此如果我们想将其传递给`map`，我们实际上必须将其包裹在一个函数定义中。再次回到我们的具体例子，它看起来可能如下所示。
- en: Listing 3.5\. Using `pipe` to create a function pipeline
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5. 使用`pipe`创建函数管道
- en: '[PRE32]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we create a function that takes our input and returns that value after
    it has been piped through a sequence of functions that we pass to `pipe` as parameters.
    In this case, we’re starting with `replace_7t`, then applying `replace_3e`, `replace_6g`,
    `replace_4a`, and lastly `C.sub_chinese`, in that order. The result, as with `compose`,
    is the same as when we chained the functions together using a sequence of `map`s—you’re
    free to print out the results and prove this to yourself—but the way we get there
    is a lot cleaner.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个函数，它接受我们的输入，并在通过我们传递给`pipe`作为参数的函数序列管道后返回该值。在这种情况下，我们开始于`replace_7t`，然后应用`replace_3e`、`replace_6g`、`replace_4a`，最后是`C.sub_chinese`，按照这个顺序。结果，与`compose`一样，与使用一系列`map`将函数链接在一起时相同——你可以自由地打印出结果并证明给自己看——但我们到达那里的方式要干净得多。
- en: Creating pipelines of helper functions provides two major advantages. The code
    becomes
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 创建辅助函数的管道提供了两个主要优势。代码变得更加
- en: Very readable and clear
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常可读且清晰
- en: Modular and easy to edit
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化和易于编辑
- en: 'The former advantage, increasing readability, is especially true when we have
    to do complex data transformations or when we want to perform a sequence of possibly
    related, or possibly unrelated, actions. For example, having just been introduced
    to the notion of `compose`, I’m pretty confident you could make a guess at what
    this pipeline does:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个优势，提高可读性，特别是在我们必须进行复杂的数据转换或当我们想要执行一系列可能相关或可能无关的操作时，这一点尤其正确。例如，刚刚接触到`compose`的概念，我相当确信你可以猜测这个管道做了什么：
- en: '[PRE33]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The latter advantage, making code modular and easy to edit, is a major perk
    when we’re dealing with dynamic situations. For example, let’s say our hacker
    adversaries change their ruse so they’re now replacing even more letters! We could
    simply add new functions into our pipeline to adjust. If we find that the hackers
    stop replacing a letter, we can remove that function from the pipeline.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个优势，使代码模块化和易于编辑，在处理动态情况时是一个主要的好处。例如，假设我们的黑客对手改变了他们的诡计，现在他们正在替换更多的字母！我们只需简单地将新函数添加到我们的管道中即可进行调整。如果我们发现黑客停止替换一个字母，我们可以从管道中删除该函数。
- en: A hacker translate pipeline
  id: totrans-590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一个黑客翻译管道
- en: Lastly, let’s return to our `map` example of this problem. At the beginning,
    we’d hoped to have one function, `hacker_translate`, that took us from garbled
    hacker secrets to plain English. We can see what we really did in [figure 3.7](#ch03fig07).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们回到这个问题的`map`示例。一开始，我们希望有一个函数`hacker_translate`，它将我们从混乱的黑客秘密翻译成普通的英语。我们可以从[图3.7](#ch03fig07)中看到我们实际上做了什么。
- en: Figure 3.7\. We can solve the hacker translation problem by constructing a chain
    of functions that each solve one part of the problem.
  id: totrans-592
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7. 我们可以通过构建一个解决问题各个部分的函数链来解决黑客翻译问题。
- en: '![](03fig07_alt.jpg)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![03fig07_alt.jpg](03fig07_alt.jpg)'
- en: '[Figure 3.7](#ch03fig07) shows our input values up top and our output values
    on the bottom, and through the middle we see how our five helper functions change
    our inputs. Breaking our complicated problem up into several small problems made
    coding the solution to this problem rather straightforward, and with `map`, we
    can easily apply the pipeline to any number of inputs that we need.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.7](#ch03fig07)显示了我们的输入值在上部，输出值在底部，中间我们看到我们的五个辅助函数如何改变输入。将复杂问题分解成几个小问题使得编写这个问题的解决方案变得相当直接，并且使用`map`，我们可以轻松地将管道应用于所需输入的任意数量。'
- en: 3.3\. Twitter demographic projections
  id: totrans-595
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 推测的Twitter人口统计
- en: In the previous section, we looked at how to foil a group of hackers by chaining
    small functions together and applying them across all the hackers’ messages. In
    this section, we’ll dive even deeper into what we can do using small, simple helper
    functions chained together.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了如何通过链式连接小型函数并将它们应用于所有黑客的消息来挫败一群黑客。在本节中，我们将更深入地探讨我们可以使用链式连接的小型、简单辅助函数能做什么。
- en: '|  |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Scenario
  id: totrans-598
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: The head of marketing has a theory that male customers are more likely to engage
    with our product on social media than female customers and has asked us to write
    an algorithm to predict the gender of Twitter users mentioning our product based
    on the text of their posts. The marketing head has provided us with lists of Tweet
    IDs for each customer. We have to write a script that turns these lists of IDs
    into both a score representing how strongly we believe them to be of a given gender
    and a prediction about their gender.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 市场部门负责人有一个理论，认为男性客户比女性客户更有可能在我们产品的社交媒体上互动，并要求我们编写一个算法，根据用户帖子中的文本预测提及我们产品的Twitter用户的性别。市场部门负责人为我们提供了每个客户的推文ID列表。我们必须编写一个脚本，将这些ID列表转换为表示我们对其性别信念强度的得分以及关于他们性别的预测。
- en: '|  |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: To tackle this problem, again, we’re going to start with a big picture `map`
    diagram. We can see that in [figure 3.8](#ch03fig08).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们再次从一个大图`map`开始。我们可以看到在[图3.8](#ch03fig08)中。
- en: 'Figure 3.8\. The map diagram for our `gender_prediction_pipeline` demonstrates
    the beginning and end of the problem: we’ll take a list of Tweet IDs and convert
    them into predictions about a user.'
  id: totrans-602
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8\. 我们的`gender_prediction_pipeline`的`map`图展示了问题的开始和结束：我们将从推文ID列表中获取，并将它们转换为关于用户的预测。
- en: '![](03fig08_alt.jpg)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig08_alt.jpg)'
- en: The `map` diagram in [figure 3.8](#ch03fig08) allows us to see our input data
    on the top and our output data on the bottom, which will help us think about how
    to solve the problem. On the top, we can see that we have a sequence of lists
    of numbers, each representing a Tweet ID. That will be our input format. And on
    the bottom, we see that we have a sequence of `dict`s, each with a key for `"score"`
    and `"gender"`. This gives us a sense of what we’ll have to do with our function
    `gender_prediction_pipeline`.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.8](#ch03fig08)中的`map`图让我们能够看到我们的输入数据在顶部，我们的输出数据在底部，这将帮助我们思考如何解决问题。在顶部，我们可以看到我们有一系列代表推文ID的数字列表，这将是我们的输入格式。在底部，我们看到我们有一系列包含`"score"`和`"gender"`键的字典，这让我们对我们的`gender_prediction_pipeline`函数需要做什么有了概念。'
- en: 'Now, predicting the gender of a Twitter user from several Tweet IDs is not
    one task; it’s actually several tasks. To accomplish this, we’re going to have
    to do the following:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从多个推文ID中预测Twitter用户的性别不是一个任务；实际上，这是一系列任务。为了完成这个任务，我们必须要做以下几步：
- en: Retrieve the tweets represented by those IDs
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取那些ID表示的推文
- en: Extract the tweet text from those tweets
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从那些推文中提取推文文本
- en: Tokenize the extracted text
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对提取的文本进行分词
- en: Score the tokens
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分分词
- en: Score users based on their tweet scores
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据推文得分对用户进行评分
- en: Categorize the users based on their score
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户的得分对用户进行分类
- en: 'Looking at the list of tasks, we can actually break down our process into two
    transformations: those that are happening at the user level and those that are
    happening at the tweet level. The user-level transformations include things like
    scoring the user and categorizing the user. The tweet-level transformations include
    things like retrieving the tweet, extracting the text, tokenizing the text, and
    scoring the text. If we were still working with `for` loops, this type of situation
    would mean that we would need a nested `for` loop. Since we’re working with `map`,
    we’ll have to have a `map` inside our `map`.'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 观察任务列表，我们实际上可以将我们的过程分解为两个转换：在用户级别发生的转换和在推文级别发生的转换。用户级别的转换包括评分用户和分类用户等。推文级别的转换包括检索推文、提取文本、分词文本和评分文本等。如果我们仍在使用`for`循环，这种情况意味着我们需要一个嵌套的`for`循环。由于我们正在使用`map`，我们将在我们的`map`内部使用`map`。
- en: 3.3.1\. Tweet-level pipeline
  id: totrans-613
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 推文级别管道
- en: Let’s look at our tweet-level transformation first. At the tweet level, we’ll
    convert a Tweet ID into a single score for that tweet, representing the gender
    score of that tweet. We’ll score the tweets by giving them points based on the
    words they use. Some words will make the tweet more of a “man’s tweet,” and some
    will make the tweet more of a “woman’s tweet.” We can see this process playing
    out in [figure 3.9](#ch03fig09).
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们的推文级别转换。在推文级别，我们将把一个推文ID转换成该推文的单个分数，代表该推文的性别分数。我们将根据他们使用的单词给推文打分。一些单词会使推文更像“男性的推文”，而另一些单词会使推文更像“女性的推文”。我们可以在[图3.9](#ch03fig09)中看到这个过程是如何进行的。
- en: Figure 3.9\. We can chain four functions together into a pipeline that will
    accomplish each of the subparts of our problem.
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9。我们可以将四个函数链接在一起形成一个管道，以完成我们问题的各个子部分。
- en: '![](03fig09_alt.jpg)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig09_alt.jpg)'
- en: '|  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Text classification**'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本分类**'
- en: Classifying a tweet by assigning scores to words it uses may seem simplistic,
    but it’s actually not too far from how both academia and industry approach the
    situation. Lexicon-based methods of classification, which assign words points
    and then roll those points up into an overall score, achieve remarkable performance
    given their simplicity. And because they are transparent, they offer the benefit
    of interpretability to practitioners.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对推文中使用的单词分配分数来对推文进行分类可能看起来很简单，但实际上它并不太远离学术界和工业界处理这种情况的方法。基于词汇表的分类方法，通过给单词分配分数然后将这些分数汇总为总分，以它们的简单性实现了显著的性能。而且因为它们是透明的，它们为从业者提供了可解释性的好处。
- en: 'In this chapter, we only approximate the real thing, but you can find a state-of-the
    art classifier on my GitHub page: [https://github.com/jtwool/TwitterGenderPredictor](https://github.com/jtwool/TwitterGenderPredictor).'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只对实际情况进行了近似，但您可以在我的GitHub页面上找到一个最先进的分类器：[https://github.com/jtwool/TwitterGenderPredictor](https://github.com/jtwool/TwitterGenderPredictor)。
- en: '|  |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[Figure 3.9](#ch03fig09) shows the several transformations that our tweets
    will undertake as we transform them from ID to score. Starting at the top left,
    we see that we start with Tweet IDs as an input, then we pass them through a `get_tweet_from_id`
    function and get tweet objects back. Next, we pass those tweet objects through
    a `tweet_to_text` function, which turns the tweet objects into the text of those
    tweets. Then, we tokenize the tweets by applying our `tokenize_text` function.
    After that, we score the tweets with our `score_text` function.'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.9](#ch03fig09)展示了我们的推文在从ID转换为分数的过程中将经历的几个转换。从左上角开始，我们看到我们以推文ID作为输入，然后通过`get_tweet_from_id`函数传递它们，并返回推文对象。接下来，我们将这些推文对象通过`tweet_to_text`函数传递，该函数将推文对象转换为这些推文的文本。然后，我们通过应用`tokenize_text`函数对推文进行分词。之后，我们使用`score_text`函数对推文进行评分。'
- en: 'Turning our attention to user-level transformations, the process here is a
    little simpler:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注用户级别的转换，这里的流程要简单一些：
- en: We apply the tweet-level process to each of the user’s tweets.
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将推文级别的流程应用于用户的所有推文。
- en: We take the average of the resulting tweet scores to get our user-level score.
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取推文分数的平均值以获得用户级别的分数。
- en: We categorize the user as either `"male"` or `"female"`.
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将用户分类为“男性”或“女性”。
- en: '[Figure 3.10](#ch03fig10) shows the user-level process playing out.'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.10](#ch03fig10)展示了用户级别流程的执行过程。'
- en: Figure 3.10\. We can chain small functions together to turn lists of users’
    Tweet IDs into scores, then into averages, and, finally, into predictions about
    their demographics.
  id: totrans-628
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10。我们可以将小函数链接起来，将用户推文ID的列表转换为分数，然后转换为平均值，最后转换为对他们的人口统计特征的预测。
- en: '![](03fig10_alt.jpg)'
  id: totrans-629
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig10_alt.jpg)'
- en: We can see that each user starts as a list of Tweet IDs. Applying our `score_user`
    function, across all of these lists of Tweet IDs, we get back a single score for
    each user. Then, we can use our `categorize_user` function to turn this score
    into a `dict` that includes both the score and the predicted gender of the user,
    just like we wanted at the outset.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个用户最初都是一个推文ID的列表。通过在我们的`score_user`函数上应用所有这些推文ID列表，我们为每个用户返回一个单一分数。然后，我们可以使用我们的`categorize_user`函数将这个分数转换成一个包含分数和用户预测性别的`dict`，就像我们最初想要的那样。
- en: 'These `map` diagrams give us a roadmap for writing our code. They help us see
    what data transformations need to take place and where we’re able to construct
    pipelines. For example, we now know that we need two function chains: one for
    the tweets and one for the users. With that in mind, let’s start tackling the
    tweet pipeline.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`map`图为我们编写代码提供了路线图。它们帮助我们了解需要发生哪些数据转换，以及我们可以在哪里构建管道。例如，我们现在知道我们需要两个函数链：一个用于推文，一个用于用户。考虑到这一点，让我们开始解决推文管道。
- en: 'Our tweet pipeline will consist of four functions. Let’s tackle them in this
    order:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的推文管道将包括四个函数。让我们按以下顺序解决它们：
- en: '`get_tweet_from_id`'
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_tweet_from_id`'
- en: '`tweet_to_text`'
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tweet_to_text`'
- en: '`tokenize_text`'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenize_text`'
- en: '`score_text`'
  id: totrans-636
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`score_text`'
- en: 'Our `get_tweet_from_id` function is responsible for taking a Tweet ID as input,
    looking up that Tweet ID on Twitter, and returning a tweet object that we can
    use. The easiest way to scrape Twitter data will be to use the `python-twitter`
    package. You can install `python-twitter` easily with `pip`:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`get_tweet_from_id`函数负责接收一个推文ID作为输入，在Twitter上查找该推文ID，并返回我们可以使用的推文对象。抓取Twitter数据最简单的方法是使用`python-twitter`包。你可以用`pip`轻松安装`python-twitter`：
- en: '[PRE34]'
  id: totrans-638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once you have `python-twitter` set up, you’ll need to set up a developer account
    with Twitter. (See the “[Twitter developer accounts](#ch03sb02)” sidebar.) You
    can do that at [https://developer.twitter.com/](https://developer.twitter.com/).
    If you have a Twitter account already, there’s no need to create another account;
    you can sign in with the account you already have. With your account set up, you’re
    ready to apply for what Twitter calls an *app*. You’ll need to fill out an application
    form, and if you tell Twitter that you’re using this book to learn parallel programming,
    they’ll be happy to give you an account. When you’re prompted to describe your
    use case, I suggest entering the following:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了`python-twitter`，你还需要在Twitter上设置一个开发者账户。（见“[Twitter开发者账户](#ch03sb02)”侧边栏。）你可以在[https://developer.twitter.com/](https://developer.twitter.com/)完成这项操作。如果你已经有了Twitter账户，就没有必要创建另一个账户；你可以用已有的账户登录。账户设置好后，你就可以申请Twitter所说的“应用”了。你需要填写一个申请表，如果你告诉Twitter你正在用这本书学习并行编程，他们会很高兴为你提供一个账户。当你被要求描述你的用例时，我建议输入以下内容：
- en: The core purpose of my app is to learn parallel programming techniques. I am
    following along with a scenario provided in [chapter 3](#ch03) of *Mastering Large
    Datasets with Python*, by JT Wolohan, published by Manning Publications.
  id: totrans-640
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我的应用程序的核心目的是学习并行编程技术。我正在跟随由Manning Publications出版的由JT Wolohan撰写的《Python精通大数据集》第3章中提供的场景。
- en: ''
  id: totrans-641
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I intend to do a lexical analysis of fewer than 1,000 Tweets.
  id: totrans-642
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我打算对少于1,000条推文进行词汇分析。
- en: ''
  id: totrans-643
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I do not plan on using my app to Tweet, Retweet, or “like” content.
  id: totrans-644
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不打算使用我的应用程序来发推文、转发或“点赞”内容。
- en: ''
  id: totrans-645
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I will not display any Tweets anywhere online.
  id: totrans-646
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不会在任何在线地方显示任何推文。
- en: '|  |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Twitter developer accounts**'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '**Twitter开发者账户**'
- en: Because this scenario involves Twitter scraping, the automated collection of
    Twitter data, I would like to offer you the opportunity to do real Twitter scraping.
    Doing so requires you to request a Twitter developer account. These developer
    accounts used to be much easier to get. Twitter is beginning to restrict who can
    develop on its platform because it wants to crack down on bots. If you don’t want
    to sign up for Twitter, you don’t want to sign up for a developer account, or
    you don’t want to wait, you can proceed without signing up for a developer account.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个场景涉及到Twitter抓取，即Twitter数据的自动收集，我想给你一个机会进行真实的Twitter抓取。这样做需要你请求一个Twitter开发者账户。这些开发者账户过去更容易获得。Twitter开始限制谁可以在其平台上开发，因为它想要打击机器人。如果你不想注册Twitter，不想注册开发者账户，或者不想等待，你可以不注册开发者账户继续操作。
- en: In the repository for this book, I include text that can stand in for the tweets,
    and you can omit the first two functions (`get_tweet_from_id` and `tweet_to_text`)
    from your tweet-level pipeline.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的存储库中，我包括了可以替代推文的文本，你可以从你的推文级管道中省略前两个功能（`get_tweet_from_id` 和 `tweet_to_text`）。
- en: '|  |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Once you have your Twitter developer account set up and confirmed by Twitter
    (this may take an hour or two), you’ll navigate to your app and find your consumer
    key, your consumer secret, your access token key, and your access token secret
    ([figure 3.11](#ch03fig11)). These are the credentials for your app. They tell
    Twitter to associate your requests with your app.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的Twitter开发者账户设置完成并被Twitter确认（这可能需要一两个小时），您将导航到您的应用程序并找到您的消费者密钥、消费者密钥、访问令牌密钥和访问令牌密钥（[图3.11](#ch03fig11)）。这些是您的应用程序的凭证。它们告诉Twitter将您的请求与您的应用程序关联起来。
- en: Figure 3.11\. The “Keys and Tokens” tab in your Twitter developer account provides
    you with API keys, access tokens, and access secrets for your project.
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11。您的Twitter开发者账户中的“密钥和令牌”选项卡为您提供了项目的API密钥、访问令牌和访问密钥。
- en: '![](03fig11_alt.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11](03fig11_alt.jpg)'
- en: With your developer account set up and `python-twitter` installed, we’re finally
    ready to start coding our tweet-level pipeline. The first thing we do is import
    the python-twitter library. This is the library we just installed. It provides
    a whole host of convenient functions for working with the Twitter API. Before
    we can use any of those nice functions, however, we need to authenticate our app.
    We do so by initiating an `Api` class from the library. The class takes our application
    credentials, which we get from the Twitter developers website, and uses them when
    it makes calls to the Twitter API.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的开发者账户设置完成并且安装了`python-twitter`之后，我们终于可以开始编写我们的推文级管道代码了。我们首先要做的是导入`python-twitter`库。这正是我们刚刚安装的库。它提供了一系列方便的函数，用于与Twitter
    API一起工作。然而，在我们能够使用这些美好的函数之前，我们需要对我们的应用程序进行认证。我们通过从库中初始化一个`Api`类来完成认证。这个类接受我们的应用程序凭证，这些凭证是从Twitter开发者网站上获取的，并在它调用Twitter
    API时使用这些凭证。
- en: With this class ready to go, we can then create a function to return tweets
    from Twitter IDs. We’ll need to pass our API object to this function so we can
    use it to make the requests to Twitter. Once we do that, we can use the API object’s
    `.GetStatus` method to retrieve Tweets by their ID. Tweets retrieved in this way
    come back as Python objects, perfect for using in our script.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类准备就绪之后，我们可以创建一个函数来返回来自Twitter ID的推文。我们需要将我们的API对象传递给这个函数，以便我们可以使用它来向Twitter发出请求。一旦我们这样做，我们就可以使用API对象的`.GetStatus`方法通过ID检索推文。以这种方式检索到的推文会以Python对象的形式返回，非常适合在我们的脚本中使用。
- en: We’ll use that fact in our next function, `tweet_to_text`, which takes the tweet
    object and returns its text. This function is very short. It calls the text property
    of our tweet object and returns that value. The text property of tweet objects
    that `python-twitter` returns contains, as we would expect, the text of the tweets.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一个函数`tweet_to_text`中使用这个事实，该函数接受推文对象并返回其文本。这个函数非常简短。它调用推文对象的文本属性并返回该值。`python-twitter`返回的推文对象的文本属性，正如我们所期望的，包含了推文的文本。
- en: 'With the tweet text ready, we can tokenize it. Tokenization is a process in
    which we break text up into smaller units that we can analyze. In some cases,
    this can be pretty complicated, but for our purpose, we’ll split text wherever
    white space occurs to separate words from one another. For a sentence like `"This
    is a tweet"`, we would get a list containing each word: `["This", "is", "a", "tweet"]`.
    We’ll use the built-in string `.split` method to do that.'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 在推文文本准备好之后，我们可以对其进行分词。分词是一个将文本分解成更小单元以便分析的过程。在某些情况下，这可能相当复杂，但就我们的目的而言，我们将根据空白字符将文本分割开来，以分隔单词。对于像“这是条推文”这样的句子，我们会得到一个包含每个单词的列表：`["这是",
    "条", "推文"]`。我们将使用内置的字符串`.split`方法来完成这个操作。
- en: Once we have our tokens, we need to score them. For that, we’ll use our `score
    _text` function. This function will look up each token in a lexicon, retrieve
    its score, and then add all of those scores together to get an overall score for
    the tweet. To do that, we need a lexicon, a list of words and their associated
    scores. We’ll use a `dict` to accomplish that here. To look up the scores for
    each word, we can map the `dict`’s `.get` method across the list of words.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了令牌，我们需要对它们进行评分。为此，我们将使用我们的`score_text`函数。这个函数将查找每个令牌在词典中的位置，检索其评分，然后将所有这些评分加在一起以获得推文的总体评分。为了做到这一点，我们需要一个词典，一个包含单词及其相关评分的列表。在这里，我们将使用`dict`来实现这一点。为了查找每个单词的评分，我们可以将`dict`的`.get`方法映射到单词列表上。
- en: The `dict .get` method allows us to look up a key and provide a default value
    in case we don’t find it. This is useful in our case because we want words that
    we don’t find in our lexicon to have a neutral value of zero.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict.get` 方法允许我们在找不到键的情况下查找键并提供默认值。在我们的情况下，这很有用，因为我们希望我们词典中没有找到的单词具有零的中性值。'
- en: 'To turn this method into a function, we use what’s called a *lambda function*.
    The `lambda` keyword allows us to specify variables and how we want to transform
    them. For example, `lambda x: x+2` defines a function that adds two to whatever
    value is passed to it. The code `lambda x: lexicon.get(x, 0)` looks up whatever
    it is passed in our lexicon and returns either the value or 0 (if it doesn’t find
    anything). We’ll often use it for short functions.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将此方法转换为函数，我们使用所谓的 *lambda 函数*。`lambda` 关键字允许我们指定变量以及我们想要如何转换它们。例如，`lambda
    x: x+2` 定义了一个函数，它将两个加到传递给它的任何值上。代码 `lambda x: lexicon.get(x, 0)` 在我们的词典中查找传递给它的任何内容，并返回该值或
    0（如果找不到）。我们经常将其用于短函数。'
- en: Finally, with all of those helper functions written, we can construct our `score_
    tweet` pipeline. This pipeline will take a Tweet ID, pass it through all of these
    helper functions, and return the result. For this process, we’ll use the `pipe`
    function from the toolz library. This pipeline represents the entirety of what
    we want to do at the tweet level. We can see all of the code needed in the following
    listing.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在编写了所有这些辅助函数之后，我们可以构建我们的 `score_tweet` 管道。这个管道将接受一个推文 ID，将其传递到所有这些辅助函数中，并返回结果。为此过程，我们将使用
    toolz 库中的 `pipe` 函数。这个管道代表了我们在推文级别想要做的全部内容。我们可以在以下列表中看到所有需要的代码。
- en: Listing 3.6\. Tweet-level pipeline
  id: totrans-663
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6\. 推文级别管道
- en: '[PRE35]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '***1* Imports the python-twitter library**'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导入 python-twitter 库**'
- en: '***2* Authenticates our app**'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 验证我们的应用程序**'
- en: '***3* Uses our app to look up tweets by their ID**'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用我们的应用程序通过 ID 查找推文**'
- en: '***4* Gets the text from a tweet object**'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从推文对象中获取文本**'
- en: '***5* Splits text on white space so we can analyze words**'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在空白处拆分文本，以便我们可以分析单词**'
- en: '***6* Creates our score_text function**'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 创建我们的 `score_text` 函数**'
- en: '***7* Creates a mini sample lexicon for scoring words**'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 创建一个用于评分单词的迷你样本词典**'
- en: '***8* Replaces each word with its point value**'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将每个单词替换为其点值**'
- en: '***9* Pipes a tweet through our pipeline**'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 将推文通过我们的管道**'
- en: 3.3.2\. User-level pipeline
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 用户级别管道
- en: 'Having constructed our tweet-level pipeline, we’re ready to construct our user-level
    pipeline. As we laid out previously, we’ll need to do three things for our user-level
    pipeline:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了我们的推文级别管道之后，我们准备构建我们的用户级别管道。正如我们之前所阐述的，我们需要为我们的用户级别管道做三件事：
- en: Apply the tweet pipeline to all of the user’s tweets
  id: totrans-676
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将推文管道应用于用户的所有推文
- en: Take the average of the score of those tweets
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取这些推文得分的平均值
- en: Categorize the user based on that average
  id: totrans-678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据这个平均值对用户进行分类
- en: For conciseness, we’ll collapse the first two actions into one function, and
    we’ll let the third action be a function all its own. When all is said and done,
    our user-level helper functions will look like the following listing.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们将前两个操作合并为一个函数，并将第三个操作作为一个独立的函数。当一切完成时，我们的用户级别辅助函数将如下所示。
- en: Listing 3.7\. User-level helper functions
  id: totrans-680
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.7\. 用户级别辅助函数
- en: '[PRE36]'
  id: totrans-681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '***1* Averages the scores of all of a user’s tweets**'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计算用户所有推文的平均得分**'
- en: '***2* Finds the number of tweets**'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 计算推文数量**'
- en: '***3* Finds the sum total of all of a user’s individual tweet scores**'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 计算用户所有单独推文的得分总和**'
- en: '***4* Returns the sum total divided by the number of tweets**'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 返回总和除以推文数量**'
- en: '***5* Takes the score and returns a predicted gender as well**'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取得分并返回预测的性别**'
- en: '***6* If the user_score is greater than 0, we’ll say that the user is male.**'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 如果用户得分大于 0，我们将说该用户是男性。**'
- en: '***7* Otherwise, we’ll say the user is female.**'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 否则，我们将说该用户是女性。**'
- en: '***8* Composes these helper functions into a pipeline function**'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将这些辅助函数组合成一个管道函数**'
- en: 'In our first user-level helper function, we need to accomplish two things:
    score all of the user’s tweets, then find the average score. We already know how
    to score their tweets—we just built a pipeline for that exact purpose! To score
    the tweets, we’ll map that pipeline across all the tweets. However, we don’t need
    the scores themselves, we need the average score.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个用户级别辅助函数中，我们需要完成两件事：对所有用户的推文进行评分，然后找到平均得分。我们已经知道如何评分他们的推文——我们只是为这个确切目的构建了一个管道！为了评分推文，我们将该管道映射到所有推文上。然而，我们不需要得分本身，我们需要平均得分。
- en: To find a simple average, we want to take the sum of the values and divide it
    by the number of values that we’re summing. To find the sum, we can use Python’s
    built-in `sum` function on the tweets. To find the number of tweets, we can find
    the length of the list with the `len` function. With those two values ready, we
    can calculate the average by dividing the sum by the length.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到简单的平均值，我们需要将值的总和除以我们正在求和的值的数量。为了找到总和，我们可以在推文中使用Python的内置`sum`函数。为了找到推文数量，我们可以使用`len`函数找到列表的长度。有了这两个值，我们可以通过除以长度来计算平均值。
- en: 'This will give us an average tweet score for each user. With that, we can categorize
    the user as being either `"Male"` or `"Female"`. To make that categorization,
    we’ll create another small helper function: `categorize_user`. This function will
    check to see if the user’s average score is greater than zero. If it is, it will
    return a `dict` with the score and a gender prediction of `"Male"`. If their average
    score is zero or less, it will return a `dict` with the score and a gender prediction
    of `"Female"`.'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们每个用户的平均推文评分。有了这个，我们可以将用户分类为“男性”或“女性”。为了进行这种分类，我们将创建另一个小的辅助函数：`categorize_user`。这个函数将检查用户的平均评分是否大于零。如果是，它将返回一个包含评分和性别预测为“男性”的`dict`。如果他们的平均评分是零或更少，它将返回一个包含评分和性别预测为“女性”的`dict`。
- en: These two quick helper functions are all we’ll need for our user-level pipeline.
    Now we can compose them, remembering to supply them in reverse order from how
    we want to apply them. That means we put our categorization function first, because
    we’re using it last, and our scoring function last, because we’re using it first.
    The result is a new function—`gender_prediction_pipeline`—that we can use to make
    gender predictions about a user.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个快速辅助函数就足够我们用户级管道使用了。现在我们可以将它们组合起来，记得按照我们想要应用它们的相反顺序提供它们。这意味着我们首先放置分类函数，因为我们最后使用它，然后是评分函数，因为我们首先使用它。结果是一个新的函数——`gender_prediction_pipeline`，我们可以用它来对用户的性别进行预测。
- en: 3.3.3\. Applying the pipeline
  id: totrans-694
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 应用管道
- en: Now that we have both our user-level and tweet-level function chains ready,
    all that’s left to do is apply the functions to our data. To do so, we can either
    use Tweet IDs with our full tweet-level function chain, or—if you decided not
    to sign up for a Twitter developer account—we can use just the text of the tweets.
    If you’ll be using just the tweet text, make sure to create a tweet-level function
    chain (`score_tweet`) that omits the `get_tweet_from_id` and `tweet_to_text` functions.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了用户级和推文级函数链，我们剩下的只是将函数应用于我们的数据。为此，我们可以使用推文ID和我们的完整推文级函数链，或者——如果你决定不注册Twitter开发者账户——我们可以只使用推文文本。如果你将只使用推文文本，请确保创建一个省略`get_tweet_from_id`和`tweet_to_text`函数的推文级函数链（`score_tweet`）。
- en: Applying the pipeline to Tweet IDs
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将管道应用于推文ID
- en: Applying our pipelines in the first instance might look something like [listing
    3.8](#ch03ex08). There, we start by initializing our data. The data we’re starting
    with is four lists of five Tweet IDs. Each of the four lists represents a user.
    The Tweet IDs don’t actually come from the same user; however, they are real tweets,
    randomly sampled from the internet.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次应用我们的管道时，它可能看起来像[列表3.8](#ch03ex08)。在那里，我们首先初始化我们的数据。我们开始的数据是四个包含五个推文ID的列表。这四个列表代表一个用户。推文ID实际上并不来自同一个用户；然而，它们是来自互联网的真实的推文，随机抽取的。
- en: Listing 3.8\. Applying the gender prediction pipeline to Tweet IDs
  id: totrans-698
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8\. 将性别预测管道应用于推文ID
- en: '[PRE37]'
  id: totrans-699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '***1* First, we need to initialize our data. Here, we’re using four sets of
    Tweet IDs.**'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先，我们需要初始化我们的数据。在这里，我们使用四组推文ID。**'
- en: '***2* Then we can apply our pipeline to our data with map. Here, we’re using
    a parallel map.**'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 然后，我们可以使用map将我们的管道应用于我们的数据。在这里，我们使用并行map。**'
- en: 'With our data initialized, we can now apply our `gender_prediction_pipeline`.
    We’ll do that in a way we introduced last chapter: with a parallel `map`. We first
    call `Pool` to gather up some processors, then we use the `.map` method of that
    `Pool` to apply our prediction function in parallel.'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化了数据后，现在可以应用我们的`gender_prediction_pipeline`。我们将按照上一章介绍的方法来做：使用并行`map`。我们首先调用`Pool`来收集一些处理器，然后我们使用该`Pool`的`.map`方法来并行应用我们的预测函数。
- en: 'If we were doing this in an industry setting, this would be an excellent opportunity
    to use a parallel map for two reasons:'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在行业环境中做这件事，这将是一个使用并行map的绝佳机会，原因有两个：
- en: We’re doing what amounts to the same task for each user.
  id: totrans-704
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对每个用户都执行相同的工作。
- en: Both retrieving the data from the web and finding the scores of all those tweets
    are relatively time- and memory-consuming operations.
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络中检索数据以及找到所有这些推文的评分都是相对耗时和耗内存的操作。
- en: To the first point, whenever we find ourselves doing the same thing over and
    over again, we should think about using parallelization to speed up our work.
    This is especially true if we’re working on a dedicated machine (like our personal
    laptop or a dedicated compute cluster) and don’t need to concern ourselves with
    hoarding processing resources other people or applications may need.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第一个问题，每当我们发现自己反复做同样的事情时，我们应该考虑使用并行化来加快我们的工作速度。这在我们使用专用机器（如我们的个人笔记本电脑或专用计算集群）并且不需要担心抢占其他人或应用程序可能需要的处理资源时尤其如此。
- en: To the second point, we’re best off using parallel techniques in situations
    in which the calculations are at least somewhat difficult or time-consuming. If
    the work we’re trying to do in parallel is too easy, we may spend more time dividing
    the work and reassembling the results than we would just doing it in a standard
    linear fashion.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第二个问题，我们最好在计算至少有些困难或耗时的情况下使用并行技术。如果我们试图并行执行的工作太简单，我们可能花费更多的时间来分割工作和重新组装结果，这比以标准线性方式执行它要花费更多的时间。
- en: Applying the pipeline to tweet text
  id: totrans-708
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将管道应用于推文文本
- en: Applying the pipeline to tweet text directly will look very similar to applying
    the pipeline to Tweet IDs, as shown in the following listing.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 将管道直接应用于推文文本将非常类似于将管道应用于推文 ID，如下面的列表所示。
- en: Listing 3.9\. Applying the gender prediction pipeline to tweet text
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9\. 将性别预测管道应用于推文文本
- en: '[PRE38]'
  id: totrans-711
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* First, we need to initialize our data. Here, we’re using four sets of
    Tweet IDs.**'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先，我们需要初始化我们的数据。这里，我们使用四组推文 ID。**'
- en: '***2* Then we can apply our pipeline to our data with map. Here, we’re using
    a parallel map.**'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 然后，我们可以使用map将我们的管道应用到我们的数据上。这里，我们使用的是并行map。**'
- en: The only change in [listing 3.9](#ch03ex09) versus [listing 3.8](#ch03ex08)
    is our input data. Instead of having tweet IDs that we want to find on Twitter,
    retrieve, and score, we can score the tweet text directly. Because our `score_tweet`
    function chain removes the `get_tweet_from_id` and `tweet_to_text` helper functions,
    the `gender_prediction_pipeline` will work exactly as we want.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 与[列表 3.8](#ch03ex08)相比，[列表 3.9](#ch03ex09)的唯一变化是我们的输入数据。我们不再需要从推特上找到我们想要查找、检索和评分的推文
    ID，而是可以直接评分推文文本。因为我们的`score_tweet`函数链去除了`get_tweet_from_id`和`tweet_to_text`辅助函数，所以`gender_prediction_pipeline`将完全按照我们的预期工作。
- en: That it is so easy to modify our pipelines is one of the major reasons why we
    want to assemble them in the first place. When conditions change, as they often
    do, we can quickly and easily modify our code to respond to them. We could even
    create two function chains if we envisioned having to handle both situations.
    One function chain could be `score_tweet_from_text` and would work on tweets provided
    in text form. Another function chain could be `score_tweet_from_id` and would
    categorize tweets provided in Tweet ID form.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如此容易地修改我们的管道是我们最初想要组装它们的主要原因之一。当条件变化时，就像它们经常发生的那样，我们可以快速轻松地修改我们的代码来应对它们。如果我们预见到需要处理两种情况，我们甚至可以创建两个函数链。一个函数链可以是`score_tweet_from_text`，它将处理以文本形式提供的推文。另一个函数链可以是`score_tweet_from_id`，它将分类以推文
    ID 形式提供的推文。
- en: Looking back throughout this example, we created six helper functions and two
    pipelines. For those pipelines, we used both the `pipe` function and the `compose`
    function from the toolz package. We also used these functions with a parallel
    `map` to pull down tweets from the internet in parallel. Using helper functions
    and function chains makes our code easy to understand and modify and plays nicely
    with our parallel `map`, which wants to apply the same function over and over
    again.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾整个示例，我们创建了六个辅助函数和两个管道。对于这些管道，我们使用了来自toolz包的`pipe`函数和`compose`函数。我们还使用这些函数与并行`map`一起从互联网上并行下载推文。使用辅助函数和函数链使我们的代码易于理解和修改，并且与想要反复应用相同函数的并行`map`很好地配合。
- en: 3.4\. Exercises
  id: totrans-717
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 练习
- en: 3.4.1\. Helper functions and function pipelines
  id: totrans-718
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 辅助函数和函数管道
- en: In this chapter, you’ve learned about the interrelated ideas of helper functions
    and function pipelines. In your own words, define both of those terms, then describe
    how they are related.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了辅助函数和函数管道相互关联的概念。用你自己的话定义这两个术语，然后描述它们是如何相关的。
- en: 3.4.2\. Math teacher trick
  id: totrans-720
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 数学老师的小技巧
- en: A classic math teacher trick has students perform a series of arithmetic operations
    on an “unknown” number, and at the end, the teacher guesses the number the students
    are thinking of. The trick is that the final number is always a constant the teacher
    knows in advance. One such example is doubling a number, adding 10, halving it,
    and subtracting the original number. Using a series of small helper functions
    chained together, map this process across all numbers between 1 and 100\. How
    does the teacher always know what number you’re thinking of?
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的数学老师技巧是让学生对一个“未知”数字进行一系列算术运算，最后老师猜测学生正在想的数字。这个技巧是最终数字总是老师事先知道的常数。一个这样的例子是将一个数字翻倍，加10，然后除以2，再减去原始数字。使用一系列小型辅助函数链接起来，将这个过程映射到1到100之间的所有数字上。老师是如何总是知道你在想什么数字的？
- en: Example
  id: totrans-722
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE39]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 3.4.3\. Caesar’s cipher
  id: totrans-724
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3. 凯撒密码
- en: 'Caesar’s cipher is an old way of constructing secret codes in which one shifts
    the position of a letter by 13 places, so A becomes N, B becomes O, C becomes
    P, and so on. Chain three functions together to create this cypher: one to convert
    a letter to an integer, one to add 3 to a number, and one to convert a number
    to a letter. Apply this cypher to a word by mapping the chained functions of a
    string. Create one new function and a new pipeline to reverse your cypher.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 凯撒密码是一种古老的方法，通过将字母的位置移动13个位置来构建秘密代码，所以A变成N，B变成O，C变成P，以此类推。将三个函数链接起来创建这个密码：一个将字母转换为整数，一个将数字加3，一个将数字转换为字母。通过映射字符串的链接函数将这个密码应用于一个单词。创建一个新函数和一个新管道来反转你的密码。
- en: Example
  id: totrans-726
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE40]'
  id: totrans-727
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Summary
  id: totrans-728
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Designing programs with small helper functions makes hard problems easy to solve
    by breaking them up into bite-sized pieces.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小型辅助函数设计程序，通过将问题分解成小块来使难题容易解决。
- en: When we pass a function through a function pipeline `pipe`, it expects the input
    data as its first argument and the functions in the order we want to apply them
    as the remaining arguments.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将一个函数通过函数管道 `pipe` 传递时，它期望输入数据作为其第一个参数，我们想要应用的函数作为剩余参数的顺序。
- en: When we create a function chain with `compose`, we pass the functions in our
    function chain as arguments in reverse order, and the resulting function applies
    that chain.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们使用 `compose` 创建一个函数链时，我们以相反的顺序将函数链中的函数作为参数传递，并且生成的函数应用这个链。
- en: Constructing function chains and pipelines is useful because they’re modular,
    they play very nicely with `map`, and we can readily move them into parallel workflows,
    such as by using the `Pool()` technique we learned in [chapter 2](kindle_split_011.html#ch02).
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建函数链和管道是有用的，因为它们是模块化的，它们与 `map` 的配合非常好，并且我们可以轻松地将它们移动到并行工作流程中，例如使用我们在[第2章](kindle_split_011.html#ch02)中学到的
    `Pool()` 技术。
- en: We can simplify working with nested data structures by using nested function
    pipelines, which we can apply with `map.`
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使用嵌套函数管道来简化处理嵌套数据结构，我们可以用 `map.` 来应用它。
- en: Chapter 4\. Processing large datasets with lazy workflows
  id: totrans-734
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章. 使用惰性工作流程处理大型数据集
- en: '*This chapter covers*'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Writing lazy workflows for processing large datasets locally
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地编写处理大型数据集的惰性工作流程
- en: Understanding the lazy behavior of `map`
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 `map` 的惰性行为
- en: Writing classes with generators for lazy simulations
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成器编写用于惰性模拟的类
- en: 'In [chapter 2](kindle_split_011.html#ch02) ([section 2.1.2](kindle_split_011.html#ch02lev2sec2),
    to be exact), I introduced the idea that our beloved `map` function is *lazy*
    by default; that is, it only evaluates when the value is needed downstream. In
    this chapter, we’ll look at a few of the benefits of laziness, including how we
    can use laziness to process big data on our laptop. We’ll focus on the benefits
    of laziness in two contexts:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_011.html#ch02) ([第2.1.2节](kindle_split_011.html#ch02lev2sec2)，更确切地说)，我介绍了我们喜爱的
    `map` 函数默认是*惰性*的；也就是说，它只有在需要时才会进行评估。在本章中，我们将探讨惰性的几个好处，包括我们如何利用惰性在笔记本电脑上处理大数据。我们将关注两个方面的惰性好处：
- en: File processing
  id: totrans-740
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件处理
- en: Simulations
  id: totrans-741
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟
- en: With file processing, we’ll see that laziness allows us to process much more
    data than could fit in memory without laziness. With simulations, we’ll see how
    we can use laziness to run “infinite” simulations. Indeed, lazy functions allow
    us to work with an infinite amount of data just as easily as we could if we were
    working with a limited amount of data.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件处理中，我们将看到惰性使我们能够处理比没有惰性时能放入内存中多得多的数据。在模拟中，我们将看到我们如何使用惰性来运行“无限”模拟。实际上，惰性函数使我们能够像处理有限数量的数据一样轻松地处理无限数量的数据。
- en: 4.1\. What is laziness?
  id: totrans-743
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 什么是懒加载？
- en: Laziness, or *lazy evaluation*, is a strategy that programming languages use
    when deciding when to perform computations. Under lazy evaluation, the Python
    interpreter executes lazy Python code only when the program needs the results
    of that code.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 懒加载，或 *懒加载评估*，是编程语言在决定何时执行计算时使用的一种策略。在懒加载下，Python 解释器仅在程序需要该代码的结果时才执行懒加载 Python
    代码。
- en: 'For example, consider the `range` function in Python, which generates a sequence
    of numbers lazily. That is, we can call `range(10000)` and we won’t get a list
    of 10,000 numbers back; we’ll get an iterator that knows how to generate 10,000
    numbers. This means we can make absurdly large `range` calls without being concerned
    that we’ll use up all our memory storing integers. For example, `range(10000000000)`
    has the same size as `range(10)`. You can check this yourself with only two lines
    of code:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑 Python 中的 `range` 函数，它以懒加载的方式生成数字序列。也就是说，我们可以调用 `range(10000)`，而不会返回一个包含
    10,000 个数字的列表；我们会得到一个知道如何生成 10,000 个数字的迭代器。这意味着我们可以进行荒谬地大的 `range` 调用，而不用担心我们会用完所有内存来存储整数。例如，`range(10000000000)`
    的大小与 `range(10)` 相同。你可以用两行代码自己检查这一点：
- en: '[PRE41]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Lazy evaluation like this is the opposite of *eager* evaluation, where everything
    is evaluated when it’s called. This is probably how you’re used to thinking about
    programming. You write a piece of code, then when the computer gets to that point,
    it computes whatever it is you told it to compute. By contrast, with lazy evaluation,
    the computer takes in your instructions and files them away until it needs to
    use them. If you never ask the computer for a final result, it will never perform
    any of the intermediate steps.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 这种懒加载与 *急切* 评估相反，其中所有内容都是在调用时进行评估。这可能是你习惯于思考编程的方式。你写一段代码，然后当计算机到达那个点时，它会计算你告诉它计算的内容。相比之下，在懒加载中，计算机接收你的指令并将它们存档，直到需要使用它们。如果你从未要求计算机提供最终结果，它将永远不会执行任何中间步骤。
- en: 'In that way, lazy evaluation is a lot like a high school student with an assignment
    due far in the future. The teacher can tell the student how to write the assignment
    at the beginning of the year. They can even warn the student that the assignment
    will be coming due in a few weeks. But it’s not until right before the deadline
    that the student actually begins to work on their assignment. The major difference:
    the computer will always complete the work.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，懒加载就像一个即将在遥远的未来提交作业的高中生。老师可以在年初就告诉学生如何写作业。他们甚至可以警告学生，作业将在几周后到期。但直到截止日期前，学生才会真正开始做作业。主要区别是，计算机总是会完成工作。
- en: Furthermore, just like the student is putting off doing their assignment so
    they can do other things—like work on other assignments due sooner or just hang
    out with their friends—our lazily evaluated program is doing other things too.
    Because our program lazily evaluates our instructions, it has more memory (time)
    to do the other things we ask of it (other assignments) or even run other processes
    altogether (hanging out with its friends maybe?).
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，就像学生推迟做作业以便做其他事情——比如处理即将到期的其他作业或只是和朋友闲逛——我们的懒加载程序也在做其他事情。因为我们的程序懒加载我们的指令，它有更多的内存（时间）来做我们要求的其他事情（其他作业）或甚至运行其他进程（也许是与朋友闲逛？）。
- en: 4.2\. Some lazy functions to know
  id: totrans-750
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 一些需要了解的懒加载函数
- en: 'We’ve already discussed how two functions you’re familiar with—`map` and `range`—are
    lazy. In this section, we’ll focus on three other lazy functions you should know
    about:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了两个你熟悉的函数——`map` 和 `range`——是如何懒加载的。在本节中，我们将关注你应该了解的另外三个懒加载函数：
- en: '**`filter`—** A function for pruning sequences'
  id: totrans-752
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`filter`—** 用于修剪序列的函数'
- en: '**`zip`—** A function for merging sequences'
  id: totrans-753
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`zip`—** 用于合并序列的函数'
- en: '**`iglob`—** A function for lazily reading from the filesystem'
  id: totrans-754
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`iglob`—** 用于从文件系统中懒加载的函数'
- en: The `filter` function takes a sequence and restricts it to only the elements
    that meet a given condition. The `zip` function takes two sequences and returns
    a single sequence of `tuple`s, each of which contains an element from each of
    the original sequences. And the `iglob` function is a lazy way of querying our
    filesystem.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 函数接受一个序列，并将其限制为仅包含满足给定条件的元素。`zip` 函数接受两个序列，并返回一个包含元组的单一序列，每个元组包含原始序列中的每个元素。而
    `iglob` 函数是一种查询文件系统的懒加载方式。'
- en: 4.2.1\. Shrinking sequences with the filter function
  id: totrans-756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 使用 `filter` 函数缩小序列
- en: 'The `filter` function does exactly what you expect it to: it acts as a filter.
    Specifically, it takes a conditional function and a sequence and returns a lazy
    iterable with all the elements of that sequence that satisfy that condition ([figure
    4.1](#ch04fig01)). For example, in the following listing, we see how `filter`
    can take a function that checks if a number is even and return an iterable of
    only even numbers.'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 函数确实如您所期望的那样工作：它作为一个过滤器。具体来说，它接受一个条件函数和一个序列，并返回一个包含满足该条件的序列所有元素的惰性可迭代对象（[图
    4.1](#ch04fig01)）。例如，在下面的列表中，我们看到 `filter` 如何接受一个检查数字是否为偶数的函数，并返回一个只包含偶数的可迭代对象。'
- en: Figure 4.1\. The `filter` function produces a new sequence that contains only
    elements that make the qualifier function return `True`.
  id: totrans-758
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1\. `filter` 函数产生一个新的序列，只包含使限定函数返回 `True` 的元素。
- en: '![](04fig01_alt.jpg)'
  id: totrans-759
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01_alt.jpg)'
- en: Listing 4.1\. Retrieving even numbers from a sequence
  id: totrans-760
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1\. 从序列中检索偶数
- en: '[PRE42]'
  id: totrans-761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In [listing 4.1](#ch04ex01), we call `list` on our filter to get it to print
    nicely, just like we did with `map`. We have to call `list` in both cases because
    `filter` and `map` are both lazy and won’t evaluate until we’re interested in
    specific values. Because lists are not lazy, converting our lazy objects to a
    `list` lets us see the individual values.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.1](#ch04ex01) 中，我们对过滤器调用 `list` 以便它能够以良好的方式打印出来，就像我们对 `map` 所做的那样。在这两种情况下，我们必须调用
    `list`，因为 `filter` 和 `map` 都是惰性的，直到我们对特定值感兴趣时才会进行评估。因为列表不是惰性的，将我们的惰性对象转换为 `list`
    让我们能够看到单个值。
- en: 'The `filter` function is a valuable tool because we can use it to concisely
    define a common operation. Four related functions are also helpful to know about,
    all of which perform the same basic operation, with a twist:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 函数是一个非常有价值的工具，因为它可以帮助我们简洁地定义一个常见的操作。还有四个相关的函数也值得了解，它们都执行相同的基本操作，但略有不同：'
- en: '`filterfalse`'
  id: totrans-764
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filterfalse`'
- en: '`keyfilter`'
  id: totrans-765
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`keyfilter`'
- en: '`valfilter`'
  id: totrans-766
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`valfilter`'
- en: '`itemfilter`'
  id: totrans-767
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`itemfilter`'
- en: Just like filter, all of these functions just do what we expect them to do.
    We can use the `filterfalse` function when we want to get all the results that
    make a qualifier function return `False`. We can use the `keyfilter` function
    when we want to filter on the keys of a `dict`. We can use the `valfilter` function
    when we want to filter on the values of a `dict`. And we can use `itemfilter`
    when we want to filter on both the keys and the values of a `dict`. We can see
    examples of all of these in action in [listing 4.2](#ch04ex02).
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `filter` 一样，所有这些函数只是做我们期望它们做的事情。当我们想要获取使限定函数返回 `False` 的所有结果时，我们可以使用 `filterfalse`
    函数。当我们想要根据 `dict` 的键进行过滤时，我们可以使用 `keyfilter` 函数。当我们想要根据 `dict` 的值进行过滤时，我们可以使用
    `valfilter` 函数。而且，当我们想要根据 `dict` 的键和值进行过滤时，我们可以使用 `itemfilter`。我们可以在 [列表 4.2](#ch04ex02)
    中看到所有这些函数的示例。
- en: In [listing 4.2](#ch04ex02), we use all four of these functions. The first,
    `filterfalse`, is from the `itertools` module that ships with Python. When we
    combine `iterfalse` with `is_even` from before, we get all the not-even (odd)
    numbers. For `keyfilter`, `valfilter`, and `itemfilter`, we need to input a `dict`.
    When we combine `keyfilter` with `is_even`, we get back all the items from the
    `dict` that have even keys. When we combine `valfilter` with `is_even`, we get
    back all the items from the `dict` that have even values. For `itemfilter`, we
    can evaluate both the keys and the values of the `dict`. In [listing 4.2](#ch04ex02),
    we create a small function, `both_are_even`, that tests if both the key and the
    value of an item are even. As the listing shows, we do get back the items for
    which both the key and the value are even.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.2](#ch04ex02) 中，我们使用了这四个函数。第一个，`filterfalse`，来自 Python 中的 `itertools`
    模块。当我们结合 `iterfalse` 和之前的 `is_even` 函数时，我们得到所有非偶数（奇数）。对于 `keyfilter`、`valfilter`
    和 `itemfilter`，我们需要输入一个 `dict`。当我们结合 `keyfilter` 和 `is_even` 时，我们得到 `dict` 中所有具有偶数键的项。当我们结合
    `valfilter` 和 `is_even` 时，我们得到 `dict` 中所有具有偶数值的项。对于 `itemfilter`，我们可以评估 `dict`
    的键和值。在 [列表 4.2](#ch04ex02) 中，我们创建了一个小函数 `both_are_even`，用于测试一个项的键和值是否都是偶数。正如列表所示，我们确实得到了键和值都是偶数的项。
- en: Listing 4.2\. Testing variations of the `filter` function
  id: totrans-770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2\. 测试 `filter` 函数的变体
- en: '[PRE43]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 4.2.2\. Combining sequences with zip
  id: totrans-772
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 使用 zip 组合序列
- en: '`zip` is another lazy function. We use `zip` when we have two iterables that
    we want to join together so that the items in the first position are together,
    the items in the second position are together, and so on. Naturally, it makes
    sense to think about the `zip` function like a zipper. When the zipper passes
    over each pair of teeth, it pulls them together into a pair ([figure 4.2](#ch04fig02)).'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '`zip` 是另一个懒加载函数。当我们有两个可迭代对象想要合并在一起，使得第一个位置的项目在一起，第二个位置的项目在一起，以此类推时，我们会使用 `zip`。自然地，我们可以将
    `zip` 函数想象成一个拉链。当拉链穿过每一对齿时，它会将它们拉在一起形成一个对([图 4.2](#ch04fig02))。'
- en: Figure 4.2\. The `zip` function behaves like a zipper, but instead of interlocking
    metal teeth, it interlocks the values of Python iterables.
  id: totrans-774
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2\. `zip` 函数的行为就像一个拉链，但它不是通过金属齿的互锁，而是通过 Python 可迭代对象的值进行互锁。
- en: '![](04fig02_alt.jpg)'
  id: totrans-775
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig02_alt.jpg)'
- en: We use the `zip` function when we have related sequences that we want to bring
    together. For instance, if an ice cream vendor knows how many ice cream cones
    they’ve sold in the last two weeks, they may be interested in zipping that together
    with the temperature to analyze if there are any trends, as shown in the following
    listing.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有两个相关的序列想要将它们放在一起时，我们会使用 `zip` 函数。例如，如果一个冰淇淋摊贩知道他们在过去两周内卖出了多少冰淇淋筒，他们可能会对将这个数据与温度一起拉链起来感兴趣，以分析是否存在任何趋势，如下面的列表所示。
- en: Listing 4.3\. Ice cream data and the `zip` function
  id: totrans-777
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. 冰淇淋数据和 `zip` 函数
- en: '[PRE44]'
  id: totrans-778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Having data paired in `tuple`s like that is helpful because `tuple`s can easily
    be passed to functions and unpacked. Because `zip` is lazy, the resulting iterator
    takes up hardly any memory. That means we can collect and move around massive
    amounts of data on our machine without holding it in memory.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式将数据配对在 `tuple` 中是有帮助的，因为 `tuple` 可以很容易地传递给函数并解包。由于 `zip` 是懒加载的，因此生成的迭代器几乎不占用内存。这意味着我们可以在机器上收集和移动大量数据，而无需将其存储在内存中。
- en: The resulting single sequence is also the perfect target for mapping a function
    across because `map` takes a function and a sequence to which you want to apply
    that function. Because `map` is lazy as well, we can calculate all of these sales
    figures without much memory overhead. Indeed, all of our lazy functions, like
    `map`, `filter`, and `zip`, play nicely with one another. And because they all
    take sequences as inputs in one way or another, they can all be chained together
    and maintain their nice low-memory overhead laziness.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的单个序列也是映射函数的完美目标，因为 `map` 函数接受一个函数和一个序列，你想要应用该函数。由于 `map` 也是懒加载的，我们可以计算所有这些销售额，而无需太多的内存开销。实际上，我们所有的懒加载函数，如
    `map`、`filter` 和 `zip`，都很好地协同工作。并且由于它们都以某种方式接受序列作为输入，因此它们可以串联在一起，并保持它们低内存开销的懒加载特性。
- en: 4.2.3\. Lazy file searching with iglob
  id: totrans-781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 使用 iglob 进行懒加载文件搜索
- en: The last function we’ll look at here is `iglob`. We can use the `iglob` function
    to find a sequence of files on our filesystem that match a given pattern. Specifically,
    the files match based on the standard Unix rules. For situations where filesystem-based
    storage is used, like in a lot of prototypes, this can be extremely helpful.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将要查看的最后一个函数是 `iglob`。我们可以使用 `iglob` 函数来找到文件系统上与给定模式匹配的文件序列。具体来说，文件是根据标准的
    Unix 规则匹配的。对于在许多原型中使用基于文件系统的存储的情况，这可以非常有帮助。
- en: 'For example, if we have blog posts stored as JSON objects in files, we may
    be able to select all of the blog posts from June 2015 with a single line of code
    (the second line):'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将博客文章存储为文件中的 JSON 对象，我们可能能够用一行代码（第二行）选择 2015 年 6 月的所有博客文章。
- en: '[PRE45]'
  id: totrans-784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This type of statement would find all the JSON files in the 06 directory inside
    the 2015 directory inside the directory where we’re storing all our blog posts.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的语句将找到存储在我们存储所有博客文章的目录中的 2015 目录内的 06 目录内的所有 JSON 文件。
- en: For a single month, getting all the blog posts lazily may not be that big of
    a deal. But if we have several posts a day and several years of posts, then our
    list will be several thousand items long. Or if we’ve done some web scraping and
    stored each page as a .JSON object with metadata about when it was collected,
    we may have millions of these files. Holding that all in memory would be a burden
    on whatever processing we want to do.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一月份，懒加载获取所有博客文章可能不是什么大问题。但如果我们每天有几篇文章，并且有几年的文章，那么我们的列表将会有几千个项目长。或者如果我们进行了一些网页抓取，并将每个页面存储为包含收集时元数据的
    .JSON 对象，我们可能会有数百万这样的文件。将所有这些存储在内存中会对我们想要进行的任何处理造成负担。
- en: In just a minute, we’ll see an example where this lazy file processing can be
    useful, but first, let’s take a second to talk about the nitty gritty details
    of sequence data types in Python.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 在一分钟内，我们将看到一个例子，说明这种惰性文件处理可能很有用，但首先，让我们花点时间来谈谈Python中序列数据类型的细节。
- en: '4.3\. Understanding iterators: The magic behind lazy Python'
  id: totrans-788
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 理解迭代器：懒惰Python背后的魔法
- en: So far, we’ve talked about the benefits of laziness and about a couple functions
    that can take advantage of them. In this section, we’ll dig into the details of
    iterators—objects that we can move through in sequence—and talk about generators—special
    functions for creating sequences. We touched on generators briefly in [chapter
    2](kindle_split_011.html#ch02), but this time we’ll dive even deeper, including
    a look at small generator expressions.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了懒惰的好处以及一些可以利用这些好处的函数。在本节中，我们将深入了解迭代器的细节——我们可以按顺序移动的对象——并讨论生成器——用于创建序列的特殊函数。我们在[第2章](kindle_split_011.html#ch02)中简要提到了生成器，但这次我们将更深入地探讨，包括对小生成器表达式的分析。
- en: It’s important that we understand how iterators work because they are fundamental
    to our ability to process big data on our laptop or desktop computer. We use iterators
    to replace data with instructions about where to find data and to replace transformations
    with instructions for how to execute those transformations. This substitution
    means that the computer only has to concern itself with the data it is processing
    *right now*, as opposed to the data it just processed or has to process in the
    future.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 理解迭代器的工作方式非常重要，因为它们是我们能够在笔记本电脑或台式计算机上处理大数据的基础。我们使用迭代器用查找数据的指令替换数据，用执行这些转换的指令替换转换。这种替换意味着计算机只需关注它现在正在处理的数据，而不是它刚刚处理过的数据或将来需要处理的数据。
- en: '4.3.1\. The backbone of lazy Python: Iterators'
  id: totrans-791
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1. 懒惰Python的骨架：迭代器
- en: Iterators are the base class of all the Python data types that can be iterated
    over. That is, we can loop over the items of an iterator, or we can map a function
    across one, like we learned how to do in [chapter 2](kindle_split_011.html#ch02).
    The iteration process is defined by a special method called `.__iter__()`. If
    a class has this method and returns an object with a `.__next__()` method, then
    we can iterate over it.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器是所有可以迭代的Python数据类型的基类。也就是说，我们可以遍历迭代器的项目，或者我们可以像在第2章中学习的那样，将函数映射到它上面。迭代过程由一个特殊的方法`.__iter__()`定义。如果一个类有这个方法并且返回一个具有`.__next__()`方法的对象，那么我们可以遍历它。
- en: 'Thank you, __next__(): The one-way nature of iterators'
  id: totrans-793
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 感谢`__next__()`：迭代器的一向性
- en: The `.__next__()` method tells Python what the next object in the sequence is.
    We can call it directly with the `next()` function. For example, if we’ve filtered
    a list of words down to only the words that have the letter m in them, we can
    retrieve the next m word with `next()`.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '`.__next__()`方法告诉Python序列中的下一个对象是什么。我们可以直接使用`next()`函数来调用它。例如，如果我们已经过滤了一个单词列表，只包含有字母m的单词，我们可以使用`next()`来检索下一个包含m的单词。'
- en: '[Listing 4.4](#ch04ex04) demonstrates calling the `next` function on a lazy
    object. We create a small function to check if the string has an m in it. We then
    use that function with `filter` to winnow down our words to only the words containing
    m. Then, because the result of our filter is an iterable, we can call the next
    function on it to get an m word.'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.4](#ch04ex04)演示了在惰性对象上调用`next`函数。我们创建了一个小函数来检查字符串中是否有字母m。然后我们使用这个函数与`filter`一起筛选出只包含字母m的单词。然后，因为我们的筛选结果是可迭代的，我们可以调用`next`函数来获取一个包含m的单词。'
- en: Listing 4.4\. Retrieving m words with `next`
  id: totrans-796
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4. 使用`next`检索m单词
- en: '[PRE46]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: If you run this in the console, you’ll be unsurprised to find that the `next()`
    function gets you the next item every time you call it. Something you might be
    surprised by, however, is that `filter` (and `map`, and all our lazy friends)
    are one-way streets; once we call `next`, the item returned to us is removed from
    the sequence. We can never back up and retrieve that item again. We can verify
    this by calling `list()` on the iterable after calling `next`. (See [figure 4.3](#ch04fig03).)
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个控制台中运行它，你会发现每次调用`next()`函数时都会得到下一个项目，这不会让你感到惊讶。然而，你可能惊讶的是，`filter`（以及`map`和所有我们的懒惰朋友）是单向的；一旦我们调用`next`，返回给我们的项目就会从序列中移除。我们永远无法回退并再次检索那个项目。我们可以通过在调用`next`之后对可迭代对象调用`list()`来验证这一点。（参见[图4.3](#ch04fig03)。）
- en: Figure 4.3\. When we call the `.__next__()` method or the `next()` function,
    we get the next item in the iterable.
  id: totrans-799
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. 当我们调用 `.__next__()` 方法或 `next()` 函数时，我们得到可迭代中的下一个项目。
- en: '![](04fig03_alt.jpg)'
  id: totrans-800
  prefs: []
  type: TYPE_IMG
  zh: '![04fig03_alt.jpg](04fig03_alt.jpg)'
- en: Iterators work like this because they’re optimized for bigger data, but they
    can cause us problems if we want to explore them element-by-element. They’re not
    meant for by-hand inspection; they’re meant for processing big data. Losing access
    to elements we’ve already seen can make iterators a little clumsier than lists
    if we’re still tinkering with our code. However, when we’re confident our code
    is working like expected, iterators use less memory and offer better performance.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器之所以这样工作，是因为它们针对大数据进行了优化，但如果我们想逐个元素地探索它们，它们可能会给我们带来问题。它们不是用来手工检查的；它们是用来处理大数据的。如果我们还在调试代码，丢失已经看到的元素可能会使迭代器比列表稍微笨拙一些。然而，当我们确信代码按预期工作，迭代器使用更少的内存并提供更好的性能。
- en: '4.3.2\. Generators: Functions for creating data'
  id: totrans-802
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 生成器：创建数据的函数
- en: 'Generators are a class of functions in Python that lazily produce values in
    a sequence. They’re a simple way of implementing an iterator. In [chapter 2](kindle_split_011.html#ch02),
    we used a generator function to produce URLs in a sequence. The benefit of that
    was we didn’t have to spend memory on holding the list in place. Indeed, that’s
    the primary advantage of generators and lazy functions: avoiding storing more
    in memory than we need to.'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是 Python 中一类懒加载产生值的函数。它们是实现迭代器的一种简单方式。在[第 2 章](kindle_split_011.html#ch02)中，我们使用生成器函数按顺序生成
    URL。这样做的优点是我们不需要在内存中保留列表。实际上，这正是生成器和懒加载函数的主要优势：避免在内存中存储我们不需要的更多数据。
- en: As we saw in [chapter 2](kindle_split_011.html#ch02), one way of designing a
    generator is by defining a function that uses the `yield` statement. For example,
    if we wanted a function that would produce the first n even numbers, we could
    do that with a generator. That function would take a number, n, and yield the
    value of i*2 for every i between 1 and n, as demonstrated in [listing 4.5](#ch04ex05).
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 2 章](kindle_split_011.html#ch02)中看到的，设计生成器的一种方法是通过定义一个使用 `yield` 语句的函数。例如，如果我们想要一个生成前
    n 个偶数的函数，我们可以使用生成器来实现。这个函数将接受一个数字 n，并为 1 到 n 之间的每个 i 产生 i*2 的值，如[列表 4.5](#ch04ex05)所示。
- en: 'Generator expressions: Infinite amounts of data in a single line of code'
  id: totrans-805
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成器表达式：单行代码中的无限数据量
- en: 'If we’re planning on doing that kind of generation multiple times, the `yield`
    statement is great. However, if we’re only planning on using those numbers once,
    we can code this even more concisely with a *generator expression*. Generator
    expressions look like *list comprehensions*—short declarations of how to manipulate
    data into a new list—but instead of generating the list up front, they create
    a lazy iterator. This has the same advantage all our other lazy approaches have
    had: we can work with more data without incurring memory overhead.'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计划多次进行这种生成，`yield` 语句非常出色。然而，如果我们只计划使用这些数字一次，我们可以用*生成器表达式*来更简洁地编写代码。生成器表达式看起来像*列表解析*——简短的声明如何将数据转换成新列表——但它们不是预先生成列表，而是创建一个懒加载的迭代器。这具有我们所有其他懒加载方法所具有的相同优势：我们可以处理更多数据而不会产生内存开销。
- en: A generator expression for the first 100 even numbers is shown at the end of
    [listing 4.5](#ch04ex05). You’ll note that the brackets around the expression
    are round instead of square. This is the syntactic distinction between a generator
    expression and a list comprehension.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 4.5](#ch04ex05)的末尾展示了前 100 个偶数的生成器表达式。你会注意到表达式周围的括号是圆括号而不是方括号。这是生成器表达式和列表解析之间的语法区别。
- en: Listing 4.5\. Even numbers generator function
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5\. 偶数生成函数
- en: '[PRE47]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To get an intuitive understanding of the difference between generator expressions
    and list comprehensions, let’s open up a Python console and run nearly identical
    commands: one with a generator expression and one with a list comprehension. For
    these statements, we’ll use a function from the `itertools` module called `count`.
    The `count` function produces a lazy sequence of numbers, similar to `range`,
    but it’s open-ended; the `count` function won’t stop.'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解生成器表达式和列表解析之间的区别，让我们打开 Python 控制台并运行几乎相同的命令：一个使用生成器表达式，另一个使用列表解析。对于这些语句，我们将使用
    `itertools` 模块中的一个函数 `count`。`count` 函数产生一个懒加载的数字序列，类似于 `range`，但它没有结束；`count`
    函数不会停止。
- en: 'If we want an infinite string of even numbers, we can run a single command
    (after we’ve imported `count` from `itertools`):'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要一个无限长的偶数序列，我们可以运行一个单独的命令（在我们从`itertools`导入`count`之后）：
- en: '[PRE48]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You’ll notice that this command runs instantly. If we call `next()` on the
    `evens` object we just created, we’ll get an even number. We also can take chunks
    from this sequence with the `islice` function from the `itertools` module (pronounced
    “i” “slice,” not “is” “lice”):'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这个命令立即运行。如果我们对刚刚创建的`evens`对象调用`next()`，我们将得到一个偶数。我们还可以使用`itertools`模块中的`islice`函数从这个序列中取出块（发音为“i”
    “slice”，不是“is” “lice”）：
- en: '[PRE49]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Compare this with the same from a list comprehension.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与列表解析的相同结果进行比较。
- en: '|  |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-817
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The following code is not going to finish running, so you may be better off
    running it in a web-based shell like [https://repl.it](https://repl.it).
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将无法完成运行，因此你最好在像[https://repl.it](https://repl.it)这样的基于网络的壳中运行它。
- en: '|  |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Here’s the list comprehension version:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 这是列表解析版本：
- en: '[PRE50]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Our generator expression runs quickly and easily, but our list comprehension
    never finishes. That’s because the list comprehension is attempting to generate
    all these numbers at once and store them in a list. That’s nice if we want access
    to a specific element, or if we’ll need repeated access to the sequence. Generators
    lose numbers that have been used, we can only access them once. But if we have
    to work with a large amount of data, our list comprehension is going to take quite
    a bit more time.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的生成器表达式运行得又快又简单，但我们的列表解析永远不会完成。这是因为列表解析试图一次性生成所有这些数字并将它们存储在一个列表中。如果我们想要访问特定的元素，或者如果我们需要重复访问序列，这是很好的。生成器会丢失已经使用过的数字，我们只能访问一次。但如果我们必须处理大量数据，我们的列表解析将花费更多的时间。
- en: '4.4\. The poetry puzzle: Lazily processing a large dataset'
  id: totrans-823
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 诗歌谜题：懒加载处理大量数据集
- en: Now that we’ve taken some time to review the ins and outs of iterators and lazy
    functions, let’s take a look at two practical scenarios where we’d want to use
    these tools.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经花了一些时间来回顾迭代器和懒加载函数的来龙去脉，让我们看看两个实际场景，在这些场景中我们会想要使用这些工具。
- en: '|  |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-826
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A new poem has taken global culture by storm, but nobody can definitively identify
    the mysterious author. Two poets are claiming the poem and have provided you with
    terabytes of their unpublished poems so you can validate which poet is more likely
    to be the true author of the poem.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 一首新的诗歌在全球文化中掀起了一场风暴，但没有人能够确切地识别这位神秘的作者。两位诗人声称这首诗是他们的作品，并提供了他们未发表的诗歌的数TB数据，以便你验证哪位诗人更有可能是这首诗的真实作者。
- en: '|  |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'In this scenario, we need to process a large amount of data from two authors
    to confirm which one of them authored the popular mystery poem. We’ll use a simple
    but powerful technique: comparing the frequencies of *function words*. Function
    words are words that have little content value but help sentences do things. Among
    other words, function words include the articles *a* and *the*. We’ll use the
    ratio of those two words, *a* and *the*, to detect our true author ([figure 4.4](#ch04fig04)).'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要处理两位作者的大量数据以确认哪位作者创作了流行的神秘诗歌。我们将使用一种简单但强大的技术：比较*功能词*的频率。功能词是那些内容价值很小但有助于句子执行任务的词。在其他词中，功能词包括冠词*a*和*the*。我们将使用这两个词*a*和*the*的比率来检测我们的真实作者（[图4.4](#ch04fig04)）。
- en: Figure 4.4\. Counting function words can give us an idea of the true author
    of a document.
  id: totrans-830
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4\. 计算功能词可以让我们了解文档的真实作者。
- en: '![](04fig04_alt.jpg)'
  id: totrans-831
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig04_alt.jpg)'
- en: 4.4.1\. Generating data for this example
  id: totrans-832
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 为本例生成数据
- en: 'Because this scenario calls for a large dataset, large here being more than
    however much memory you have on the computer you’re following along on, I’ve opted
    to provide a data generation script in the book’s repository ([https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)).
    You can use that function to generate as much data as you want for this scenario.
    I suggest generating at least 100 MB if you can and then deleting it all after
    you’ve finished this section. That said, if you have a petabyte hard drive laying
    around, feel free to fill it up. The code in this section will be able to process
    it just fine—though it may take some time. Lazy functions are great at processing
    data, but hardware still limits how quickly we can work through it. Another great
    option: generate a tiny bit of data, finish the chapter, then generate more data
    and let the code process it overnight.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个场景需要一个大型数据集，这里的“大”是指超过你在跟随的电脑上的内存量，我已选择在本书的仓库中提供一个数据生成脚本（[https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)）。你可以使用该函数生成你想要的任何数据量。如果你能生成至少100
    MB，我建议你完成这一部分后将其全部删除。话虽如此，如果你有一块PB级的硬盘，请随意填满它。本节中的代码将能够处理它——尽管可能需要一些时间。懒函数在处理数据方面非常出色，但硬件仍然限制了我们可以多快地处理它。另一个好选择：生成一小部分数据，完成本章，然后生成更多数据，让代码在夜间处理。
- en: 'Unfortunately, because each author has provided us with so much information,
    we’ll never be able to process it all in memory at once. So we’ll have to use
    lazy functions to process it bit by bit. We’ll also use some of the techniques
    we learned in [chapter 3](kindle_split_012.html#ch03): breaking our large problem
    down into pieces we can solve with small, helper functions.'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于每位作者都向我们提供了大量信息，我们永远无法一次性在内存中处理所有信息。因此，我们不得不使用懒函数来逐步处理。我们还将使用我们在[第3章](kindle_split_012.html#ch03)中学到的一些技术：将我们的大型问题分解成我们可以用小型辅助函数解决的问题。
- en: First, let’s take a look at what we’ll need to do.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们需要做什么。
- en: We want to eventually compare the ratio of *a* and *the* for each author.
  id: totrans-836
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终想要比较每位作者中*a*和*the*的比例。
- en: To do that, we need to read in each file for each author.
  id: totrans-837
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要为每位作者读取每个文件。
- en: We’ll also need a way to get word counts for *a* and *the*.
  id: totrans-838
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要一种方法来获取*a*和*the*的词频。
- en: And to do that, we’ll need to break the poems into words.
  id: totrans-839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要将诗篇分解成单词。
- en: Ultimately, our process is going to look like [figure 4.5](#ch04fig05). First,
    we’ll read the files in. Then, we’ll clean them so they’re nice workable lists
    of words instead of unstructured poems. Then, we’ll filter them down to just the
    words in which we’re interested. Finally, we’ll get counts and calculate a ratio.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们的过程将类似于[图4.5](#ch04fig05)。首先，我们将读取文件。然后，我们将清理它们，使它们成为整洁的工作列表，而不是无结构的诗歌。然后，我们将过滤掉我们感兴趣的单词。最后，我们将获取计数并计算比例。
- en: Figure 4.5\. Lots of small steps will add up to help us determine the author
    of the mystery poem.
  id: totrans-841
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5。许多小步骤将帮助我们确定神秘诗的作者。
- en: '![](04fig05_alt.jpg)'
  id: totrans-842
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig05_alt.jpg)'
- en: 4.4.2\. Reading poems in with iglob
  id: totrans-843
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2。使用iglob读取诗篇
- en: In [section 4.2.3](#ch04lev2sec3), we looked at `iglob`, a function for searching
    for files on a filesystem and returning a list of matching paths as an iterable.
    Because our poets were generous enough to provide us with reams of their unpublished
    works, we’ll want to use this function to limit the overhead we need to spend
    storing these paths.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4.2.3节](#ch04lev2sec3)中，我们探讨了`iglob`，这是一个在文件系统上搜索文件并返回匹配路径列表作为可迭代对象的函数。由于我们的诗人慷慨地提供了他们未发表的大量作品，我们将想要使用这个函数来限制我们需要花费在存储这些路径上的开销。
- en: 'A straightforward step like this is also something that’s good to get out of
    the way first. To read in the poems by each author, let’s define two iterables
    using `iglob`: one for each author. This is a quick two-liner, as we can see in
    the following listing.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的直接步骤也是我们首先应该解决的问题。为了读取每位作者的诗篇，让我们使用`iglob`定义两个可迭代对象：一个用于每位作者。这只是一个简单的两行代码，如下所示。
- en: Listing 4.6\. Listing the authors’ poems using `iglob`
  id: totrans-846
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.6。使用`iglob`列出作者的诗篇
- en: '[PRE51]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 4.4.3\. A poem-cleaning regular expression class
  id: totrans-848
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3。一个诗篇清理正则表达式类
- en: Now that we have the poems, we’ll need a way to munge them into a workable data
    format. As we’ve done before with text data, we’ll ultimately want to convert
    the long string of text data we get when we open and read the poem files into
    a list of words. Before that, however, we’ll want to remove all the punctuation
    using a regular expression. For poems, this would be especially important because
    poets are known for distinctive use of punctuation.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了诗歌，我们需要一种方法将它们转换成可工作的数据格式。就像我们之前处理文本数据一样，我们最终希望将打开和读取诗歌文件时得到的文本数据的长时间字符串转换成单词列表。在此之前，我们还想使用正则表达式移除所有标点符号。对于诗歌来说，这尤为重要，因为诗人以独特的标点符号使用而闻名。
- en: Because we’ll be using a regular expression, we’ll want to create a class so
    that we can compile that regular expression once and use it as many times as we’d
    like. We’ll give that class an attribute with a compiled regular expression that
    matches all the punctuation we want to remove and a method that uses that regular
    expression to remove the punctuation. Since we’re using that method to make our
    text data easy to work with, it also makes sense to add lowercasing in there to
    normalize our text, and to split our words on whitespace.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用正则表达式，我们希望创建一个类，这样我们就可以编译那个正则表达式一次，然后根据需要多次使用它。我们将给这个类添加一个属性，包含一个编译后的正则表达式，该表达式匹配我们想要移除的所有标点符号，以及一个使用该正则表达式来移除标点的函数。由于我们使用该函数使我们的文本数据易于处理，因此在那里添加小写化以规范化文本，以及根据空白分割单词也是合理的。
- en: We can see how that would play out in [figure 4.6](#ch04fig06), where we transform
    a poem into our desired data structure. We start with the raw poem text, as the
    poet intended it, but after it’s cleaned, the text is ready for us to analyze.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [图 4.6](#ch04fig06) 中看到这是如何实现的，其中我们将一首诗转换成我们期望的数据结构。我们开始于诗人原本意图的原始诗歌文本，但在清理后，文本就准备好供我们分析了。
- en: Figure 4.6\. We can use a class containing a regular expression to transform
    a poem into a list of words.
  id: totrans-852
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6\. 我们可以使用包含正则表达式的类将一首诗转换成单词列表。
- en: '![](04fig06_alt.jpg)'
  id: totrans-853
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig06_alt.jpg)'
- en: Ultimately, we should end up with a class that looks like the following listing.
    In the listing, I’ve chosen to remove all periods, commas, semicolons, colons,
    exclamation points, question marks, and hyphens with the regular expression.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们应该得到一个看起来像以下列表的类。在列表中，我选择使用正则表达式移除所有句号、逗号、分号、冒号、感叹号、问号和连字符。
- en: Listing 4.7\. A poem cleaner class
  id: totrans-855
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7\. 一个诗歌清理类
- en: '[PRE52]'
  id: totrans-856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '***1* Compiles the regular expression to match all punctuation**'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 编译正则表达式以匹配所有标点**'
- en: '***2* Removes punctuation from the poem**'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从诗歌中移除标点**'
- en: '***3* Returns the no-punctuation poem lowercased and split into a list of tokens**'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 返回没有标点的诗歌小写并分割成标记列表**'
- en: 4.4.4\. Calculating the ratio of articles
  id: totrans-860
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4\. 计算冠词的比例
- en: 'The next step we’ll work on, getting a ratio of articles, we’ll solve with
    two custom functions: the `filter` function we’ve already looked at in this chapter
    and the `itertools.chain` function we looked at in [chapter 3](kindle_split_012.html#ch03),
    plus a new function from the toolz library: `frequencies`. And all of this is
    going to be inside a wrapper function that we can use to pass in our `PoemCleaner`
    class ([figure 4.7](#ch04fig07)).'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将要处理的一步，获取冠词的比例，我们将使用两个自定义函数来解决：我们已经在本章中查看过的 `filter` 函数和我们在 [第 3 章](kindle_split_012.html#ch03)
    中查看过的 `itertools.chain` 函数，以及来自 toolz 库的新函数：`frequencies`。所有这些都将在一个包装函数内部进行，我们可以使用这个包装函数来传递我们的
    `PoemCleaner` 类 ([图 4.7](#ch04fig07))。
- en: Figure 4.7\. A large function will wrap all our smaller functions so we can
    readily apply our poem analysis pipeline.
  id: totrans-862
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7\. 一个大函数将包含所有我们的较小函数，这样我们就可以方便地应用我们的诗歌分析流程。
- en: '![](04fig07_alt.jpg)'
  id: totrans-863
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig07_alt.jpg)'
- en: The first custom function we’ll need is a function to determine if a word should
    be kept. We don’t want to spend time or memory counting all the words, because
    we’ll only use *a* and *the* to determine authorship. For this, we’ll use a filter
    in conjunction with a helper function to narrow a lazy sequence of all the words
    down to just *a*’s and *the*’s. That helper function has to return `True` if a
    word is *a* or *the*, and `False` otherwise. That helper function will look like
    [listing 4.8](#ch04ex08).
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要的自定义函数是确定一个单词是否应该被保留的函数。我们不想花费时间或内存去计数所有单词，因为我们只会用 *a* 和 *the* 来确定作者身份。为此，我们将使用一个过滤器与一个辅助函数结合，将所有单词的懒序列缩小到仅包含
    *a* 和 *the*。这个辅助函数必须返回 `True` 如果一个单词是 *a* 或 *the*，否则返回 `False`。这个辅助函数看起来像 [列表
    4.8](#ch04ex08)。
- en: '|  |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'The function word method for detecting the true author of a text may seem overly
    simplistic, but it’s not far removed from a technique that was used in the most
    popular authorship analysis of all time: discovering the identity of the unattributed
    *Federalist Papers*. In that instance, a list of 30 function words was used to
    identify James Madison as the sole or primary author of the 12 disputed essays.'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检测文本真实作者的函数词方法可能看起来过于简单，但它并不远离历史上最受欢迎的作者身份分析技术：发现未署名的《联邦党人文集》的作者身份。在那个例子中，使用了一个包含
    30 个函数词的列表来识别詹姆斯·麦迪逊是 12 篇争议文章的唯一或主要作者。
- en: '|  |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 4.8\. Function to test if a word is *a* or *the*
  id: totrans-869
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.8\. 测试一个单词是否为 *a* 或 *the* 的函数
- en: '[PRE53]'
  id: totrans-870
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '***1* We check if w is in the list containing “a” and “the” and return the
    result.**'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们检查 w 是否在包含“a”和“the”的列表中，并返回结果。**'
- en: Once we’ve got that function built, we can use it as the condition part of our
    `filter` function. The input sequence for that filter is going to be our `.clean_poem`
    method mapped across the sequence of poem paths. We’ll apply the `itertools.chain`
    function to the resulting sequences of words so we can treat them as one big sequence.
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了这个函数，我们就可以将其用作 `filter` 函数的条件部分。该过滤器的输入序列将是我们的 `.clean_poem` 方法映射到诗歌路径序列。我们将应用
    `itertools.chain` 函数到结果序列的单词序列，这样我们就可以将它们视为一个大的序列。
- en: 'At this point, we’ve got a way to get a sequence for each author’s uses of
    *a* and *the*. Now we need to count them and find a ratio between them. For the
    counting, the toolz library has a function `frequencies` that can do just that.
    It takes a sequence in and returns a `dict` of items that occurred in the sequence
    as keys with corresponding values equal to the number of times they occurred ([figure
    4.8](#ch04fig08)). In other words: it provides the frequencies of items in our
    sequence.'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一种获取每个作者使用 *a* 和 *the* 的序列的方法。现在我们需要计数并找到它们之间的比率。对于计数，toolz 库有一个名为
    `frequencies` 的函数可以做到这一点。它接受一个序列作为输入，并返回一个 `dict`，其中包含在序列中出现的项目作为键，相应的值为它们出现的次数（[图
    4.8](#ch04fig08)）。换句话说：它提供了我们序列中项目的频率。
- en: Figure 4.8\. The `frequencies` function takes a sequence and turns it into a
    `dict` of items from the original sequence and the number of times they occur.
  id: totrans-874
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. `frequencies` 函数接受一个序列并将其转换为包含原始序列中项目及其出现次数的 `dict`。
- en: '![](04fig08_alt.jpg)'
  id: totrans-875
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig08_alt.jpg)'
- en: From those counts, we can write another small function to calculate the ratios.
    That function needs to take a `dict` and return the value of the `"a"` key divided
    by the value of the `"the"` key. Because we’re doing division, it’s prudent to
    use the `.get` method of our `dict` with an ever-so-slightly larger than zero
    value so we don’t risk dividing by zero. That helper function and the combined
    poem analysis functions should look like the following listing.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些计数中，我们可以编写另一个小函数来计算比率。该函数需要接受一个 `dict` 并返回 `"a"` 键的值除以 `"the"` 键的值。因为我们正在进行除法，所以使用我们的
    `dict` 的 `.get` 方法并使用一个略大于零的值是谨慎的，这样我们就不冒除以零的风险。这个辅助函数和组合诗歌分析函数应该看起来像以下列表。
- en: Listing 4.9\. Poem analysis function
  id: totrans-877
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.9\. 诗歌分析函数
- en: '[PRE54]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: To tie all of this together, we’ll need to create an instance of our `PoemCleaner`
    class and apply our `analyze_poems` function to the iterables for each of our
    authors. Altogether, we’ll have the code in the following listing. At the very
    end of the listing, I’ve added a `print` statement that’ll show the authors’ different
    tendencies, as well as the value found in the original poem. Running this script
    will tell you who the true author is!
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些内容串联起来，我们需要创建我们的 `PoemCleaner` 类的实例，并将我们的 `analyze_poems` 函数应用于每个作者的迭代器。总的来说，我们将在以下列表中看到代码。在列表的末尾，我添加了一个
    `print` 语句，将显示作者的不同倾向，以及原始诗歌中找到的值。运行此脚本将告诉你真正的作者是谁！
- en: Listing 4.10\. Poem puzzle final script
  id: totrans-880
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.10\. 诗歌谜题最终脚本
- en: '[PRE55]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: With this script, we can parse a larger amount of data than we could handle
    in memory. Being able to do this is a key milestone in transitioning from a developer
    who can only work with small data to a developer who can work with big(ish) data.
    As we saw in this example with `iglob` and `filter`, laziness helps us a lot in
    this respect. Next, we’ll see how laziness can help us in generating data.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个脚本，我们可以解析比我们能够处理内存中的数据更大的数据量。能够做到这一点是从只能处理小数据的开发者过渡到可以处理大数据（ish）的开发商的关键里程碑。正如我们在
    `iglob` 和 `filter` 的例子中所看到的，懒惰在这方面帮了我们很多。接下来，我们将看到懒惰如何帮助我们生成数据。
- en: '4.5\. Lazy simulations: Simulating fishing villages'
  id: totrans-883
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 懒惰模拟：模拟渔村
- en: The poetry puzzle covered in [section 4.4](#ch04lev1sec4) showed us how we can
    work with big data on our local machine; however, we also can use the tools in
    this chapter for producing big data.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4.4节](#ch04lev1sec4)中介绍的诗歌谜题展示了我们如何在本地机器上处理大数据；然而，我们也可以使用本章中的工具来生成大数据。
- en: '|  |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-886
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: An environmental conservation group has commissioned you to design a simulation
    that illustrates the problem of overfishing. They have outlined a scenario involving
    four small villages and a lake. The people in each of those villages have agreed
    to take only one fish per person per year; however, some years a village will
    cheat and take twice as many fish as they’re allowed. Each village has its own
    propensity for cheating, but if two villages get caught cheating in the same year,
    each village increases its propensity for cheating. The villages will also grow
    each year. How many years can these villages survive?
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 一个环境保护组织委托你设计一个模拟，以说明过度捕鱼的问题。他们概述了一个涉及四个小村庄和一个湖泊的场景。这些村庄的每个人每年都同意只捕一条鱼；然而，有些年份，一个村庄会作弊，捕捞的鱼是允许的两倍。每个村庄都有其自身的作弊倾向，但如果两个村庄在同一年被发现作弊，每个村庄的作弊倾向都会增加。这些村庄每年也会增长。这些村庄能生存多少年？
- en: '|  |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'For simulation problems like this, it’s often useful to program in a slightly
    different way than we’ve been programming up to now. For simulations, we get a
    lot of value from writing classes, which we haven’t talked about much outside
    of a way to compile regular expressions. Classes are great because they allow
    us to consolidate the data about each piece of the simulation ([figure 4.9](#ch04fig09)).
    In this specific simulation, we have two actors that need special attention and
    deserve their own class:'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种模拟问题，我们通常以与之前编程略有不同的方式编程。对于模拟，我们通过编写类获得很多价值，我们之前很少以编译正则表达式之外的方式讨论过类。类很棒，因为它们允许我们整合关于模拟每个部分的资料（[图4.9](#ch04fig09)）。在这个特定的模拟中，有两个演员需要特别注意，并值得拥有自己的类：
- en: The simulation as a whole
  id: totrans-890
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟的整体
- en: The villages
  id: totrans-891
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 村庄
- en: Figure 4.9\. We can use classes to represent actors in a complex simulation
    scenario, such as villages fishing a lake over time.
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9。我们可以使用类来表示复杂模拟场景中的演员，例如随着时间的推移，村庄在湖中捕鱼。
- en: '![](04fig09_alt.jpg)'
  id: totrans-893
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig09_alt.jpg)'
- en: Considering the simulation as a class will give us a place to keep track of
    what year we’re in, how many fish are remaining, and which villages are associated
    with the simulation. It will give us an easy way to run lots of simulations in
    parallel, as we’ll see later on.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 将模拟视为一个类将给我们一个地方来跟踪我们处于哪一年，剩下多少鱼，以及哪些村庄与模拟相关联。它将给我们一个简单的方式来并行运行许多模拟，正如我们稍后将看到的。
- en: Considering the villages as a class is useful for many of the same reasons.
    The villages are all going to have their own unique bits of data, like a unique
    population and a unique inclination toward cheating. The villages also will need
    to do certain things, like increase population (and maybe increase their rate
    of cheating) each year.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 将村庄视为一个类对于许多相同的原因是有用的。这些村庄都将拥有自己独特的数据，如独特的人口和独特的作弊倾向。村庄还需要做一些事情，比如每年增加人口（也许会增加他们的作弊率）。
- en: You may notice that breaking the problem up into two classes is similar to how
    we were breaking large problems into chains of small helper functions in [chapter
    3](kindle_split_012.html#ch03). Indeed, we’ll further break up the large simulation
    inside those two classes. The `Lake_Simulation` class will get a method for handling
    the simulation itself, and the fishing villages will get methods for fishing and
    updating.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，将问题分解为两个类与我们如何在[第3章](kindle_split_012.html#ch03)中将大问题分解为一系列小辅助函数的方式相似。确实，我们将在这两个类内部进一步分解大型模拟。`Lake_Simulation`类将获得处理模拟本身的方法，而渔村将获得捕鱼和更新的方法。
- en: 4.5.1\. Creating a village class
  id: totrans-897
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1. 创建村庄类
- en: Between the villages and the simulation, the village is a smaller chunk of work,
    so let’s start there. For the villages, we’ll create a class that has an attribute
    to store its population and an attribute to store its cheating rate ([figure 4.10](#ch04fig10)).
    Each of these two attributes will be unique to each village, and we don’t want
    to have to set them ourselves for each simulation, so we’ll use a random variable
    in their place. I’m going to keep the villages small—between 1,000 and 5,000—and
    the amount of cheating relatively low—between .05 and .15.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 在村庄和模拟之间，村庄是工作量较小的一部分，所以让我们从这里开始。对于村庄，我们将创建一个具有存储其人口和作弊率的属性的类（[图 4.10](#ch04fig10)）。这两个属性中的每一个都将对每个村庄是唯一的，我们不希望为每个模拟自己设置它们，所以我们将使用随机变量来代替。我将保持村庄规模较小——在
    1,000 到 5,000 之间——并且作弊的量相对较低——在 0.05 到 0.15 之间。
- en: Figure 4.10\. The `Village` class represents everything the village is and can
    do, including going fishing and growing.
  id: totrans-899
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10\. `Village` 类代表了村庄能做的一切，包括去捕鱼和增长。
- en: '![](04fig10_alt.jpg)'
  id: totrans-900
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig10_alt.jpg)'
- en: To generate random numbers for the population and cheat rate, we’ll need to
    import Python’s `random` module and use its `uniform` function, which selects
    a value between two points with uniform likelihood. In other words, every number
    in that range has the same likelihood of occurring. We can call the `uniform`
    function for both population and cheat rate, as we see in the following listing.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成人口和作弊率的随机数，我们需要导入 Python 的 `random` 模块并使用它的 `uniform` 函数，该函数以均匀的可能性选择两个点之间的值。换句话说，该范围内的每个数字都有相同的出现可能性。我们可以为人口和作弊率调用
    `uniform` 函数，如下面的列表所示。
- en: Listing 4.11\. The beginnings of a `Village` class
  id: totrans-902
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.11\. `Village` 类的起点
- en: '[PRE56]'
  id: totrans-903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '***1* Defines a Village class**'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义一个 `Village` 类**'
- en: '***2* Customizes what happens when the class is initialized**'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在类初始化时自定义发生的事情**'
- en: '***3* Gives the class a population value uniformly selected between 1,000 and
    5,000**'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为类提供一个在 1,000 到 5,000 之间的均匀选择的人口值**'
- en: '***4* Gives the class a cheating rate between 5% and 15%**'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 为类提供一个介于 5% 和 15% 之间的作弊率**'
- en: 'This lays out the core of the `Village` class, and we can move on to some of
    the stuff the village will do: like going fishing and updating itself every round.
    Let’s take a look at going fishing first.'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 这就概述了 `Village` 类的核心，我们可以继续看村庄将要做的一些事情：比如去捕鱼和每轮更新自己。让我们首先看看去捕鱼。
- en: 'Gone Fishing: A first method for our simulation object'
  id: totrans-909
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 去捕鱼：为我们的模拟对象实现第一个方法
- en: Every year, when a village goes fishing, it has the option to cheat, and all
    villages cheat at a different rate. To account for that, we’ll generate a uniform
    random variable between 0 and 1\. If that number is below the cheat rate, the
    village will cheat; otherwise, it’ll play by the rules. When a village cheats,
    it’ll take two fish per person. When it doesn’t, it’ll take only one fish. And
    then lastly, because our simulation will need to know if our village cheated and
    how many fish it took, we’ll return the amount of fish taken and if the village
    cheated, as shown in the following listing.
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，当一个村庄去捕鱼时，它都有作弊的选择，并且所有村庄的作弊率都不同。为了解决这个问题，我们将生成一个介于 0 和 1 之间的均匀随机变量。如果这个数字低于作弊率，那么这个村庄将会作弊；否则，它将按照规则行事。当一个村庄作弊时，每人将拿走两条鱼。当它不作弊时，每人只拿走一条鱼。最后，因为我们的模拟需要知道我们的村庄是否作弊以及它拿走了多少鱼，我们将返回拿走的鱼的数量以及村庄是否作弊，如下面的列表所示。
- en: Listing 4.12\. A method for going fishing
  id: totrans-911
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.12\. 去捕鱼的方法
- en: '[PRE57]'
  id: totrans-912
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '***1* Checks if the village will cheat; if it does, apply the cheat rules.**'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 检查村庄是否会作弊；如果会，应用作弊规则。**'
- en: '***2* If the village doesn’t cheat, apply the standard rules.**'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果村庄不作弊，应用标准规则。**'
- en: '***3* At the end, return the fish the village took and if they cheated or not.**'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 最后，返回村庄拿走的鱼以及它们是否作弊。**'
- en: A yearly update function for the village class
  id: totrans-916
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 村庄类的年度更新函数
- en: After going fishing, each village also will change a little each year. Every
    year, the population will grow, and, depending how many villages cheated that
    year, a given village may increase the rate at which it decides to cheat. To keep
    things simple, our villages will all grow at a rate of 2.5% each year.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕鱼之后，每个村庄每年也会有所变化。每年，人口都会增长，并且，根据当年有多少村庄作弊，一个特定的村庄可能会增加它决定作弊的比率。为了简化问题，我们的村庄每年将以
    2.5% 的比率增长。
- en: To decide whether or not we increase the cheat rate, we’ll need to know how
    many cheaters there were this year of the simulation. Because that information
    is contained inside the simulation class, we’ll need to pass the simulation to
    the `.update` method, as shown in the following listing.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定是否增加作弊率，我们需要知道模拟中今年有多少作弊者。因为这个信息包含在模拟类中，所以我们需要将模拟传递给 `.update` 方法，如下面的列表所示。
- en: Listing 4.13\. Updating our villages
  id: totrans-919
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.13\. 更新我们的村庄
- en: '[PRE58]'
  id: totrans-920
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '***1* If we find more than two cheaters, increase the cheat rate.**'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果我们发现超过两个作弊者，增加作弊率。**'
- en: '***2* Increase the population no matter what.**'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 不论如何增加人口。**'
- en: 4.5.2\. Designing the simulation class for our fishing simulation
  id: totrans-923
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 为我们的捕鱼模拟设计模拟类
- en: Those two methods, `.go_fishing` and `.update`, round out our `Village` class.
    We’ll use that class to represent villages in our simulation ([figure 4.11](#ch04fig11)).
    Additionally, as mentioned earlier, we’ll need a class for the simulation itself.
    This class will keep track of the simulation-level variables, such as what year
    it is and how many fish remain. Additionally, our simulation class will have a
    rather large method for running the simulation itself.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方法，`.go_fishing` 和 `.update`，完善了我们的 `Village` 类。我们将使用这个类来表示模拟中的村庄（[图 4.11](#ch04fig11)）。此外，如前所述，我们还需要一个表示模拟本身的类。这个类将跟踪模拟级别的变量，例如现在是哪一年以及还剩下多少鱼。此外，我们的模拟类将有一个相当大的方法来运行模拟本身。
- en: Figure 4.11\. Our simulation is a cyclical process in which we go fishing, check
    if we need to stop the simulation, update the simulation if we’ll keep going,
    and then go fishing again.
  id: totrans-925
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.11\. 我们的模拟是一个循环过程，其中我们捕鱼，检查是否需要停止模拟，如果我们将继续，则更新模拟，然后再次捕鱼。
- en: '![](04fig11_alt.jpg)'
  id: totrans-926
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig11_alt.jpg)'
- en: Setting up the simulation with the .__init__ method
  id: totrans-927
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 .__init__ 方法设置模拟
- en: 'The start of our simulation is its `.__init__` method, which will set up the
    simulation ([listing 4.14](#ch04ex14)). To set up the simulation, we only need
    four things:'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模拟的开始是它的 `.__init__` 方法，该方法将设置模拟（[列表 4.14](#ch04ex14)）。为了设置模拟，我们只需要四件事：
- en: '***The villages—*** Represented by a list of village objects, in our case 4'
  id: totrans-929
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***村庄—*** 由一个村庄对象列表表示，在我们的例子中是 4'
- en: '***The fish—*** In this case, just the number of fish'
  id: totrans-930
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***鱼—*** 在这种情况下，只是鱼的数量'
- en: '***A start year—*** Also a number, in our case 1'
  id: totrans-931
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***起始年份—*** 也是一个数字，在我们的例子中是 1'
- en: '***The number of cheaters—*** Again, an integer indicating the number of cheating
    villages'
  id: totrans-932
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***作弊者的数量—*** 再次，一个整数，表示作弊村庄的数量'
- en: We’ll assign each of these variables to the simulation class itself, so as our
    simulation changes, the variables will carry with it.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些变量分配给模拟类本身，因此随着模拟的变化，这些变量也会随之变化。
- en: Listing 4.14\. Setting up the simulation
  id: totrans-934
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.14\. 设置模拟
- en: '[PRE59]'
  id: totrans-935
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Writing simulation logic in our .simulate method
  id: totrans-936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在我们的 .simulate 方法中编写模拟逻辑
- en: The simulation logic will go in the simulation class’s `.simulate` method. That
    means this method will be responsible for
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟逻辑将放在模拟类的 `.simulate` 方法中。这意味着这个方法将负责
- en: finding the results of a year of fishing
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出一年捕鱼的结果
- en: updating the simulation after each year
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每年更新模拟
- en: ending the simulation if we run out of fish
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们用完鱼，就结束模拟
- en: ending the simulation if we survive “long enough”
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们“存活”足够长，就结束模拟
- en: 'Because our simulation can go on forever if the lake never gets overfished,
    we’ll start our `.simulate` method with an infinite loop; in this case, we’ll
    use an infinite `for` loop:'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模拟可以无限进行，如果湖里永远不会过度捕捞，我们将以一个无限循环开始我们的 `.simulate` 方法；在这种情况下，我们将使用一个无限 `for`
    循环：
- en: '[PRE60]'
  id: totrans-943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `itertools.count()` function returns a generator that produces an infinite
    sequence of increasing numbers (1, 2, 3, 4, . . . 1000, 1001, 1002, . . . infinity).
    By using “`_`” we tell Python to ignore the value returned by the `for` loop,
    since we won’t be needing it.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: '`itertools.count()` 函数返回一个生成器，它产生一个无限增长的数字序列（1, 2, 3, 4, . . . 1000, 1001,
    1002, . . . 无穷大）。通过使用“`_`”，我们告诉 Python 忽略 `for` 循环返回的值，因为我们不需要它。'
- en: 'Villages going fishing: Introducing methodcaller for map and classes'
  id: totrans-945
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 村庄捕鱼：介绍 map 和类的方法 caller
- en: With our loop set up, we can start finding the results of our year of fishing.
    For our simulation, each of our villages goes fishing individually. That’s why
    we set up the village classes with a `.go_fishing` method. To have all the villages
    go fishing, we can map their `.go_fishing` method across the list of classes in
    our simulation’s `.villages` attribute.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好循环之后，我们可以开始寻找我们捕鱼年份的结果。对于我们的模拟，我们每个村庄都会单独去捕鱼。这就是为什么我们设置了带有`.go_fishing`方法的村庄类。为了使所有村庄都去捕鱼，我们可以将它们的`.go_fishing`方法映射到模拟的`.villages`属性中的类列表上。
- en: To do this, we’ll need the `operator.methodcaller` function. `methodcaller`
    takes a string and returns a function that calls the method with the name of that
    string on any object passed to it. Because the map and reduce style of programming
    we’re looking at in this book is so function-oriented, being able to call class
    methods using a function is extremely helpful This capability allows us to use
    functions like `map` and `filter` on them.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要`operator.methodcaller`函数。`methodcaller`接受一个字符串并返回一个函数，该函数在传递给它的任何对象上调用具有该字符串名称的方法。由于我们在本书中查看的map和reduce编程风格非常面向函数，能够使用函数调用类方法是非常有帮助的。这种能力使我们能够使用`map`和`filter`等函数对它们进行操作。
- en: From there, because our `.go_fishing` method returns a `tuple` of fish caught
    and a number indicating if that village cheated or not, our output from mapping
    this function across a list of villages will look as if we used the `zip` function
    on a sequence of fish caught and a sequence of cheating indicators. Knowing this,
    we can unzip the sequence of `tuple`s and take the sums of the individual sequences,
    which will give us the total number of fish caught and the total number of cheaters.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，因为我们的`.go_fishing`方法返回一个捕获的鱼和表示该村庄是否作弊的数字的`tuple`，我们将这个函数映射到村庄列表上的输出看起来就像我们在捕获的鱼和作弊指示符的序列上使用了`zip`函数一样。了解这一点后，我们可以解压缩`tuple`序列并取各个序列的总和，这将给出捕获的总鱼数和作弊者的总数。
- en: 'Unzipping is the opposite of zipping. Whereas zipping takes two sequences and
    returns a list of `tuple`s, unzipping takes a single sequence and returns two.
    We can call unzip by putting a star in front of the list when we call the `zip`
    function: `zip(*my_ sequence)`. We can see unzip and the rest of the first phase
    of our simulate step in the following listing.'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 解压缩是压缩的反义词。而压缩接受两个序列并返回一个`tuple`列表，解压缩则接受一个单一序列并返回两个。当我们调用`zip`函数时，可以通过在列表前加一个星号来调用`unzip`：`zip(*my_sequence)`。我们可以在下面的列表中看到`unzip`和我们的模拟步骤的第一阶段的其余部分。
- en: Listing 4.15\. All the villages go fishing
  id: totrans-950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.15\. 所有村庄都去捕鱼
- en: '[PRE61]'
  id: totrans-951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '***1* Unzips the yearly_results into two lists: fishes and cheats**'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将yearly_results解压缩成两个列表：fishes和cheats**'
- en: '***2* The fishes list contains the number of fish fished by each village, its
    sum being the total fish fished.**'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 鱼列表包含每个村庄捕捞的鱼的数量，其总和是捕捞的总鱼数。**'
- en: '***3* The cheats list contains a 1 for each village that cheated, its sum being
    the number of cheaters.**'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 欺骗列表包含每个作弊村庄的1，其总和是作弊者的数量。**'
- en: After we figure out how many cheaters there were and how many fish were caught,
    we’ll check if the simulation should end or if we should keep going. For this,
    we’ll use two `if` checks, each of which will break our infinite `for` loop.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们弄清楚有多少作弊者和捕捞了多少鱼之后，我们将检查模拟是否应该结束，或者我们应该继续进行。为此，我们将使用两个`if`检查，每个检查都将中断我们的无限`for`循环。
- en: The first `if` check will check if we’ve made it through 1,000 simulated years.
  id: totrans-956
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个`if`检查将检查我们是否已经通过了1,000个模拟年份。
- en: The second `if` check will check if all the fish have been fished.
  id: totrans-957
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个`if`检查将检查是否所有的鱼都被捕捞了。
- en: If each of these conditions is triggered, we’ll print a message to the screen
    explaining what happened. If we wanted to store the results of our simulation,
    this would be a good place to write our simulation results to a file. The following
    listing shows what this short bit of our code looks like.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些条件中的任何一个被触发，我们将在屏幕上打印一条消息来解释发生了什么。如果我们想存储我们的模拟结果，这将是一个将我们的模拟结果写入文件的好地方。下面的列表显示了这段代码的短小片段看起来是什么样子。
- en: Listing 4.16\. Checking if the simulation should be over
  id: totrans-959
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.16\. 检查模拟是否应该结束
- en: '[PRE62]'
  id: totrans-960
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Final calculations: Resolving the year'
  id: totrans-961
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最终计算：解决年份
- en: If we make it past the year-end checks, we can update our simulation for the
    year. Updating the simulation involves removing the fished fish from the remaining
    fish, repopulating the fish some amount (fish do make more fish, after all), and
    updating all the villages. If you’d like, we also may want to add a `print` statement
    here so we can see what happens year over year.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过了年末检查，我们可以更新我们的模拟以反映该年。更新模拟涉及从剩余的鱼中移除捕获的鱼，以一定数量重新繁殖鱼（毕竟，鱼会繁殖），并更新所有村庄。如果你愿意，我们也可以在这里添加一个
    `print` 语句，这样我们就可以看到每年发生了什么。
- en: To update the amount of fish remaining, we’ll subtract `total_fished` from `self.fish`
    and then increase `self.fish` by 15%. To update all the villages, we’ll again
    map `methodcaller` across all our villages. This time, however, we’ll call for
    the `.update` method instead of `.go_fishing`. Lastly, for a `print` statement,
    I recommend including at least the year and the number of fish remaining. See
    the one shown in the following listing.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新剩余鱼的数量，我们将从 `self.fish` 中减去 `total_fished`，然后增加 `self.fish` 的 15%。为了更新所有村庄，我们再次在所有村庄上映射
    `methodcaller`。然而，这次我们将调用 `.update` 方法而不是 `.go_fishing`。最后，对于 `print` 语句，我建议至少包括年份和剩余鱼的数量。请参见以下列表中所示的内容。
- en: Listing 4.17\. The `.simulate` method
  id: totrans-964
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.17\. `.simulate` 方法
- en: '[PRE63]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '***1* Updates the fish remaining**'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 更新剩余的鱼**'
- en: '***2* Updates the villages by mapping the “update” method across them**'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通过映射“update”方法更新村庄**'
- en: And with that, we’ve completed our fishing simulation! Initialize a `Lake_Simulation`
    class and call the `.simulate` method a few times to see what happens. You should
    get a different number of years survived each time you run the simulation. The
    following listing shows the full simulation code.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就完成了我们的捕鱼模拟！初始化一个 `Lake_Simulation` 类，并多次调用 `.simulate` 方法以查看发生了什么。每次运行模拟时，你应该得到不同的生存年数。以下列表显示了完整的模拟代码。
- en: Listing 4.18\. Full fishing simulation
  id: totrans-969
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.18\. 完整的捕鱼模拟
- en: '[PRE64]'
  id: totrans-970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We can run this simulation a few times (commenting out or changing the random
    seed each time) to see different results. The output of our simulation will be
    years, printed to the terminal, with the amount of fish remaining in that year.
    Usually, we’ll see our simulation end after around 10 years, as shown in [listing
    4.19](#ch04ex19). Sometimes, though, it will go on for thousands of runs.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行这个模拟几次（每次注释掉或更改随机种子），以查看不同的结果。我们的模拟输出将是年份，打印到终端，显示该年剩余的鱼的数量。通常，我们的模拟会在大约
    10 年后结束，如 [列表 4.19](#ch04ex19) 所示。有时，它将持续数千次运行。
- en: '|  |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Simulations and random seeds**'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: '**模拟和随机种子**'
- en: When we run simulations, we’ll use lots of random number generators. Randomness
    in our code can cause confusion when we share it with others and they’re expecting
    to get the same results we got. One way we can get around this is by using a *random
    seed*. By setting a seed, we can ensure that we’ll get effectively random numbers,
    but that those numbers will be in the same sequence every time. Any other user
    running the same code with the same random seed will get the same results we do.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行模拟时，我们会使用大量的随机数生成器。我们的代码中的随机性可能会在与其他人共享时造成困惑，因为他们期望得到与我们相同的结果。我们可以通过使用
    *随机种子* 来解决这个问题。通过设置种子，我们可以确保我们将得到有效随机的数字，但这些数字将每次都在相同的序列中。任何其他使用相同代码和相同随机种子的用户都将得到与我们相同的结果。
- en: '|  |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 4.19\. Fishing scenario output
  id: totrans-976
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.19\. 捕鱼场景输出
- en: '[PRE65]'
  id: totrans-977
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '***1* In most runs of the scenario, the lake will be overfished in a dozen
    or so years.**'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在大多数场景的运行中，湖泊将在大约十年左右被过度捕捞。**'
- en: These long runs represent the scenarios in which the villages avoid cheating
    during the early stages of the simulation. You can play around with the cheat
    rate, number of fish taken, and fish population growth rate to see how the simulation
    behaves under different assumptions.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 这些长时间运行代表的是村庄在模拟早期阶段避免作弊的场景。你可以调整作弊率、捕获的鱼的数量和鱼群增长率，以查看模拟在不同假设下的行为。
- en: 4.6\. Exercises
  id: totrans-980
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 练习
- en: 4.6.1\. Lazy functions
  id: totrans-981
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1\. 懒惰函数
- en: Lazy functions are common when we use a map and reduce style in Python. Which
    of the following functions are lazy?
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用 Python 中的 map 和 reduce 风格时，懒惰函数很常见。以下哪些函数是懒惰的？
- en: '`map`'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`'
- en: '`reduce`'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce`'
- en: '`filter`'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`list`'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`list`'
- en: '`zip`'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zip`'
- en: '`sum`'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sum`'
- en: '`range`'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`range`'
- en: '`len`'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len`'
- en: 4.6.2\. Fizz buzz generator
  id: totrans-991
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2\. Fizz buzz 生成器
- en: 'A classic toy programming problem is the fizz buzz problem, where we want to
    replace any number divisible by 3 with fizz and any number divisible by 5 with
    buzz. If a number is divisible by both fizz and buzz, it should be replaced with
    fizz buzz. We implemented a version of this using classes in [chapter 2](kindle_split_011.html#ch02).
    Create a generator that solves this problem. Hint: Remember, you can use the modulo
    operator (%) to check if division produces a remainder.'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的玩具编程问题就是fizz buzz问题，其中我们想要将能被3整除的数字替换为fizz，将能被5整除的数字替换为buzz。如果一个数字既能被fizz也能被buzz整除，则应将其替换为fizz
    buzz。我们在[第2章](kindle_split_011.html#ch02)中实现了一个使用类的版本。创建一个生成器来解决这个问题。提示：记住，你可以使用取模运算符（%）来检查除法是否产生余数。
- en: 4.6.3\. Repeat access
  id: totrans-993
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.3\. 重复访问
- en: When we use a built-in generator function such as `range`, we can only iterate
    through it once. Why is that so?
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用内置的生成器函数，如`range`时，我们只能迭代一次。为什么是这样？
- en: 4.6.4\. Parallel simulations
  id: totrans-995
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.4\. 并行模拟
- en: There are many ways to run several simulations in parallel. One way is to map
    the `.simulate` method from our `Lake_Simulation` class over a sequence using
    the `with Pool() as P:` construction we introduced in [chapter 2](kindle_split_011.html#ch02).
    Modify the code from [listing 4.18](#ch04ex18) so you can run simulations in parallel.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以并行运行多个模拟。一种方法是将我们的`Lake_Simulation`类的`.simulate`方法映射到序列上，使用我们在[第2章](kindle_split_011.html#ch02)中引入的`with
    Pool() as P:`构造。修改[列表4.18](#ch04ex18)中的代码，以便可以并行运行模拟。
- en: 4.6.5\. Scrabble words
  id: totrans-997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.5\. Scrabble单词
- en: 'The popular game Scrabble involves spelling words by placing tiles on a board.
    Spelling long words and words with more rare letters in them earns you more points.
    In a simplified version, Z is worth 10 points; F, H, V, and W are worth 5; B,
    C, M, and P are worth three; and all other letters are worth one point. Using
    the functions `map` and `filter`, reduce this list of words to only the ones that
    are worth more than eight points: zebra, fever, charm, mouse, hair, brill, thorn.'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 流行游戏Scrabble涉及在棋盘上放置拼字来拼写单词。拼写长单词和包含更多罕见字母的单词可以获得更多分数。在一个简化的版本中，Z值10分；F、H、V和W值5分；B、C、M和P值3分；其他所有字母值1分。使用`map`和`filter`函数，将单词列表减少到只包含价值超过八分的单词：zebra、fever、charm、mouse、hair、brill、thorn。
- en: Summary
  id: totrans-999
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Lazy functions are those that evaluate only when we need the values they return—no
    sooner and no later. We can use lazy functions like `map`, `filter`, `zip`, and
    `iglob` to work with massive amounts of data on our laptops.
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惰性函数是那些只有在需要它们返回的值时才进行评估的函数——不会早也不会晚。我们可以使用像`map`、`filter`、`zip`和`iglob`这样的惰性函数来处理我们笔记本电脑上的大量数据。
- en: Python implements laziness through iterators, which we can create ourselves,
    receive from functions, or build with convenient generator functions and statements.
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python通过迭代器实现惰性，我们可以自己创建迭代器，从函数中接收迭代器，或使用方便的生成器函数和语句构建迭代器。
- en: We can create generators with functions using `yield` statements or through
    concise and powerful list comprehension-like generator expressions.
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`yield`语句或通过简洁而强大的类似列表解析的生成器表达式来创建使用函数的生成器。
- en: We can only go through iterators one way; once we’ve seen an element from an
    iterator, we never have access to that same element again.
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只能以一个方向遍历迭代器；一旦我们从一个迭代器中看到了一个元素，我们就永远无法再次访问该元素。
- en: 'We can use the `filter` function to conveniently gather a subset of a list.
    There’s a whole family of functions just like the filter function: `filterfalse`,
    `valfilter`, `keyfilter`, and `itemfilter`.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`filter`函数方便地收集列表的一个子集。与`filter`函数类似，有一系列函数：`filterfalse`、`valfilter`、`keyfilter`和`itemfilter`。
- en: We can use `zip` to combine two lists into a single sequence of `tuple`s—a handy
    trick when combined with `map`.
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`zip`将两个列表组合成一个单一的`tuple`序列——当与`map`结合使用时，这是一个实用的技巧。
- en: We can use `frequencies` from the toolz library to get counts of the unique
    elements of a sequence.
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用来自toolz库的`frequencies`来获取序列中唯一元素的数量。
- en: We can apply lazy functions and generators toward solving data-intensive problems,
    such as text analysis and simulations.
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将惰性函数和生成器应用于解决数据密集型问题，例如文本分析和模拟。
- en: We can use `methodcaller` to map an object’s method over a sequence of that
    object.
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`methodcaller`将对象的方法映射到该对象的序列上。
- en: Chapter 5\. Accumulation operations with reduce
  id: totrans-1009
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 使用reduce进行累积操作
- en: '*This chapter covers*'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Recognizing the `reduce` pattern for N-to-X data transformations
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别N-to-X数据转换的`reduce`模式
- en: Writing helper functions for reductions
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写用于减少的辅助函数
- en: Writing lambda functions for simple reductions
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写用于简单归约的lambda函数
- en: Using `reduce` to summarize data
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`reduce`总结数据
- en: 'In [chapter 2](kindle_split_011.html#ch02), we learned about the first part
    of the map and reduce style of programming: `map`. In this chapter, we introduce
    the second part: `reduce`. As we noted in [chapter 2](kindle_split_011.html#ch02),
    `map` performs N-to-N transformations. That is, if we have a situation where we
    want to take a sequence and get a same-sized sequence back, `map` is our go-to
    function. Among the examples of this that we’ve reviewed are file processing (we
    have a list of files and we want to do something to all of them; discussed in
    [chapter 4](kindle_split_013.html#ch04)) and web scraping (we have a list of websites
    and we want to get the content for each of them; discussed in [chapter 2](kindle_split_011.html#ch02)).'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_011.html#ch02)中，我们学习了map和reduce编程风格的第一个部分：`map`。在本章中，我们将介绍第二个部分：`reduce`。正如我们在[第2章](kindle_split_011.html#ch02)中提到的，`map`执行N到N的转换。也就是说，如果我们有一个想要将一个序列转换成相同大小序列的情况，`map`就是我们的首选函数。在我们回顾的例子中，这包括文件处理（我们有一个文件列表，我们想要对它们中的每一个都做些什么；在第4章[讨论](kindle_split_013.html#ch04)）和网页抓取（我们有一个网站列表，我们想要获取每个网站的内容；在第2章[讨论](kindle_split_011.html#ch02)）。
- en: In this chapter, we’ll focus on `reduce` for N-to-X transformations; that is,
    situations where we have some sequence but we want to get something back besides
    another same-sized sequence, usually a sequence of a different size or possibly
    not even a sequence at all. Note, however, that situations do exist where we’ll
    actually want to use `reduce` to get back a sequence of the same size. We’ll look
    at all of these situations, learning about `reduce` and using it in some common
    transformations that you’re already familiar with. Learning `reduce` will give
    us a handy tool to use in situations where `map` isn’t appropriate, but where
    we still want to benefit from using a common programming pattern.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于`reduce`进行N到X的转换；也就是说，我们有一些序列，但我们想要得到除了另一个相同大小的序列之外的东西，通常是不同大小的序列，或者甚至根本不是序列。然而，确实存在我们实际上想要使用`reduce`来得到相同大小序列的情况。我们将探讨所有这些情况，了解`reduce`并在一些你已熟悉的常见转换中使用它。学习`reduce`将为我们提供一个方便的工具，在`map`不适用但仍然想要从使用通用编程模式中受益的情况下使用。
- en: 5.1\. N-to-X with reduce
  id: totrans-1017
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 使用`reduce`进行N到X转换
- en: 'When we say that `reduce` is a function for N-to-X transformations, we mean
    that whenever we have a sequence and want to transform it into something that
    we can’t use `map` for, we can happily use `reduce`. This is one of the reasons
    why `map` and `reduce` pair so neatly together: `map` can take care of most of
    the transformations in a very concise manner, whereas `reduce` can take care of
    the very final transformation, albeit in a somewhat less elegant fashion.'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说`reduce`是一个N到X的转换函数时，我们的意思是，无论何时我们有一个序列并且想要将其转换成我们不能使用`map`的东西，我们都可以愉快地使用`reduce`。这是`map`和`reduce`如此完美搭配的原因之一：`map`可以以非常简洁的方式处理大多数转换，而`reduce`则可以处理最后的转换，尽管它的方式可能不那么优雅。
- en: An example of an N-to-X transformation that you’re already familiar with, and
    that we’ll take a look at in more detail in [section 5.2](#ch05lev1sec2), is the
    summation function. In math, this is typically represented with a Σ. In Python,
    we have access to the `sum` function from the base library. The summation function
    takes a sequence of numbers (integers, floats, imaginary numbers) and returns
    a single number that is the total of all the numbers in the sequence added together
    ([figure 5.1](#ch05fig01)).
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 一个你已经熟悉的N到X转换的例子，我们将在[第5.2节](#ch05lev1sec2)中更详细地探讨，就是求和函数。在数学中，这通常用Σ表示。在Python中，我们可以从基础库中访问`sum`函数。求和函数接受一个数字序列（整数、浮点数、虚数）并返回一个单一的数字，它是序列中所有数字的总和（[图5.1](#ch05fig01)）。
- en: Figure 5.1\. The sum function is a common example of the reduce pattern that
    most people already know.
  id: totrans-1020
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1. 求和函数是大多数人已经知道的reduce模式的一个常见例子。
- en: '![](05fig01_alt.jpg)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig01_alt.jpg)'
- en: 'For example, if we had a sequence with the numbers 10, 5, 1, 19, 11, and 203,
    we could sum them up and get a single number back. This would take us from our
    six original numbers down to only one resulting number. We would have transformed
    our data from size N (6) down to X (1). This is the essence of the reduce pattern:
    taking a sequence and transforming it into something else.'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个包含数字 10、5、1、19、11 和 203 的序列，我们可以将它们相加并得到一个单一的数字。这样，我们就将我们的六个原始数字缩减到只有一个结果数字。我们将数据的大小从
    N（6）缩减到 X（1）。这就是 reduce 模式的本质：将一个序列转换成其他东西。
- en: 5.2\. The three parts of reduce
  id: totrans-1023
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. Reduce 的三个部分
- en: 'Summing a sequence of numbers with `reduce` is simple, but it will still require
    all three parts of a `reduce` function ([figure 5.2](#ch05fig02)):'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `reduce` 对数字序列进行求和很简单，但它仍然需要 `reduce` 函数的三个部分（[图 5.2](#ch05fig02)）：
- en: An accumulator function
  id: totrans-1025
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个累加器函数
- en: A sequence
  id: totrans-1026
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个序列
- en: An initializer
  id: totrans-1027
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个初始化器
- en: 'Figure 5.2\. A `reduce` function has three parts: an accumulator, which specifies
    `reduce`’s behavior, a sequence, which we reduce over, and an initial value, which
    we use to start our `reduce` operation.'
  id: totrans-1028
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2\. `reduce` 函数有三个部分：累加器，它指定 `reduce` 的行为；一个序列，我们对其执行 `reduce` 操作；以及一个初始值，我们用它来开始我们的
    `reduce` 操作。
- en: '![](05fig02_alt.jpg)'
  id: totrans-1029
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig02_alt.jpg)'
- en: The accumulator function does the heavy lifting for `reduce`. It’s a special
    type of helper function, like the ones we were using for `map` in [chapters 2](kindle_split_011.html#ch02),
    [3](kindle_split_012.html#ch03), and [4](kindle_split_013.html#ch04). A sequence
    is an object that we can iterate through, such as lists, strings, and generators.
    And our initializer is the initial value to be passed to our accumulator. In most
    implementations of `reduce`, this parameter is optional.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器函数为 `reduce` 执行繁重的工作。它是一种特殊的辅助函数，就像我们在第 2 章、第 3 章、第 4 章中使用 `map` 时所用的那些函数一样。序列是一个我们可以遍历的对象，例如列表、字符串和生成器。我们的初始化器是要传递给累加器的初始值。在大多数
    `reduce` 的实现中，此参数是可选的。
- en: If we were to sum up a sequence of numbers, we would want
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要对数字序列进行求和，我们希望
- en: to have our accumulator function be an addition function
  id: totrans-1032
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使我们的累加器函数成为一个加法函数
- en: our sequence to be the sequence of numbers we’d like to sum
  id: totrans-1033
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们要处理的序列是我们想要求和的数字序列
- en: our initial value to be 0 to start counting at zero
  id: totrans-1034
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的初始值设为 0 以从零开始计数
- en: In Python, that may look something like the following listing. To run this code,
    you’ll need to define an addition function—`my_add`. We’ll do that in the next
    subsection on accumulation functions.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，这可能会看起来像以下列表。要运行此代码，您需要定义一个加法函数——`my_add`。我们将在下一节关于累积函数的子节中这样做。
- en: Listing 5.1\. The three parts of `reduce`
  id: totrans-1036
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.1\. `reduce` 的三个部分
- en: '[PRE66]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '***1* First, we need to import reduce from the functools library.**'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先，我们需要从 `functools` 库中导入 `reduce`。**'
- en: '***2* Then, we can set up our data to sum up.**'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 然后，我们可以设置我们的数据以进行求和。**'
- en: '***3* When we call reduce, notice how the accumulator comes first, then the
    sequence, then the initial value.**'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 当我们调用 reduce 时，请注意累加器先出现，然后是序列，最后是初始值。**'
- en: '[Listing 5.1](#ch05ex01) provides an example of how summation with `reduce`
    may look. Two things are worth noting about this short bit of code. First, we
    need to `import reduce` from the functools library. The `reduce` function is not
    a default import like `map`, though it is available with any distribution of Python.
    In deprecated versions of Python (2.7 and below), `reduce` was available by default.'
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.1](#ch05ex01) 提供了使用 `reduce` 进行求和的示例。关于这段简短代码有两点值得注意。首先，我们需要从 `functools`
    库中导入 `reduce`。`reduce` 函数不是像 `map` 那样默认导入，尽管它在任何 Python 发行版中都是可用的。在 Python 的旧版本（2.7
    及以下）中，`reduce` 是默认可用的。'
- en: '|  |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Removing the reduce function from base Python**'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '**从基础 Python 中移除 reduce 函数**'
- en: In 2002, the creator of Python, Guido van Rossum, referred to including many
    of the approaches in this book as a mistake. He had the view that these approaches
    harmed readability and that the reduce method in particular was hard for most
    people to understand. I disagree. Reduce simply is not widely taught. Additionally,
    the rise of parallel and distributed computing makes these tools extremely valuable.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2002 年，Python 的创造者 Guido van Rossum 将本书中包含的许多方法称为一个错误。他认为这些方法损害了可读性，特别是 reduce
    方法对大多数人来说很难理解。我不同意。Reduce 只是未被广泛教授。此外，并行和分布式计算的出现使得这些工具变得极其有价值。
- en: In this chapter, you’ll learn about a powerful, versatile tool that the Python
    language maintainers don’t want you to know about.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解一个强大、多功能的工具，Python语言的维护者不希望你了解这个工具。
- en: '|  |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Second, the order in which we place our parameters for `reduce` is specific.
    Like `map`, the accumulator or helper function comes first, then the sequence,
    and then our initializer comes last. The initializer comes last because it is
    an optional parameter.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们为`reduce`放置参数的顺序是特定的。像`map`一样，累加器或辅助函数首先出现，然后是序列，最后是我们的初始化器。初始化器放在最后，因为它是一个可选参数。
- en: 5.2.1\. Accumulation functions in reduce
  id: totrans-1048
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. `reduce`中的累加函数
- en: Accumulator functions are all of a common prototype. They take an accumulated
    value and the next element in the sequence and return another object, typically
    of the same type as the accumulated value. For example, in our sum function, we’re
    going to want to take in the sum up to that point as our accumulated value, and
    the next element in the sequence as our next value, and add them together. The
    code for that will look like the following listing.
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: 累加函数都具有一个共同的原型。它们接收一个累加值和序列中的下一个元素，并返回另一个对象，通常是累加值的同一类型。例如，在我们的求和函数中，我们希望接收到目前为止的总和作为累加值，序列中的下一个元素作为下一个值，并将它们相加。相应的代码如下所示。
- en: Listing 5.2\. An accumulator function for summation
  id: totrans-1050
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.2\. 求和的累加函数
- en: '[PRE67]'
  id: totrans-1051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '***1* Our my_add function takes in an accumulated value (acc) and the next
    element (nxt).**'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的my_add函数接收一个累加值（acc）和下一个元素（nxt）。**'
- en: '***2* It returns those two values added together, which will be another number,
    just like acc.**'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 它返回这两个值相加的结果，这将是一个新的数字，就像acc一样。**'
- en: The one thing you’ll want to note about this one-line function is the variable
    names. My preferred convention labeling the variables to a `reduce` accumulator
    function is to use `acc` for the accumulated value and `nxt` for the next value;
    however, there are others. Some more concise teams like to use `a` to represent
    the accumulator and `b` to represent the next value. You also may see `left` used
    to represent the accumulator and `right` used to represent the next value. To
    understand why the accumulator function needs to take in an accumulated value
    and the next element, it helps to understand how `reduce` does its transformations.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一行函数，你需要注意的一点是变量名。我偏好的命名约定是将变量标记为`reduce`累加函数，使用`acc`表示累加值，`nxt`表示下一个值；然而，还有其他方式。一些更简洁的团队喜欢使用`a`来表示累加器，`b`来表示下一个值。你也可能会看到使用`left`来表示累加器，`right`来表示下一个值。为了理解为什么累加函数需要接收一个累加值和下一个元素，了解`reduce`如何进行转换是有帮助的。
- en: How reduce works
  id: totrans-1055
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何进行递减
- en: In its simplest implementations, `reduce` loops over a sequence, processing
    each element in conjunction with an accumulated value. This accumulated value
    starts as either the initializer value, if we provide one, or the first element
    of the sequence if we do not. For example, when `reduce` is summing up 10, 5,
    1, 19, 11, and 203, it’s adding 10 to 0 to get 10, then adding 5 to the current
    total (10) to get 15, then adding 1 to that to get 16, then adding 19 to that
    to get 35, and so on until all the numbers are processed ([figure 5.3](#ch05fig03)).
    The total value is the accumulator value. The next value in the sequence is the
    next value.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的实现中，`reduce`遍历序列，与累加值一起处理每个元素。这个累加值开始时要么是我们提供的初始化值，要么是没有提供时序列的第一个元素。例如，当`reduce`对10、5、1、19、11和203进行求和时，它是将10加到0得到10，然后将5加到当前总和中（10）得到15，然后将1加到那个数上得到16，然后将19加到那个数上得到35，依此类推，直到所有数字都被处理（[图
    5.3](#ch05fig03)）。总值是累加值。序列中的下一个值是下一个值。
- en: '|  |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Reducing from left to right**'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '**从左到右递减**'
- en: Because `reduce` loops over a sequence from left to right, some teams, as we
    just mentioned, will call the accumulated value in their accumulator helper functions
    `left` and the next value passed to those functions `right`. Versions of the `reduce`
    function in other programming languages can reduce from right to left instead.
    In these situations, teams can easily tell which functions were written for left-to-right
    reductions and which were written for right-to-left reductions.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`reduce`是从左到右遍历序列的，正如我们刚才提到的，一些团队会在他们的累加辅助函数中称累加值为`left`，将这些函数传递的下一个值为`right`。在其他编程语言中的`reduce`函数版本可以从右到左进行递减。在这些情况下，团队可以很容易地判断哪些函数是为左到右递减编写的，哪些是为右到左递减编写的。
- en: '|  |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 5.3\. The `reduce` function works by processing each element of a sequence
    and joining it with an accumulator value.
  id: totrans-1061
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3\. `reduce` 函数通过处理序列中的每个元素并将其与累加器值连接来工作。
- en: '![](05fig03_alt.jpg)'
  id: totrans-1062
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig03_alt.jpg)'
- en: When these data structures and functions are simple, `reduce` can seem unnecessary;
    however, as the data structures, and the transformations we want to make of them,
    become more complex, we can use `reduce` to make our transformations more transparent.
    More sophisticated implementations of `reduce`, like we’ll see in [chapter 6](kindle_split_015.html#ch06),
    also allow for parallel reductions, which provide the same performance improvements
    we saw with parallel `map`, with little to no rewriting of our code.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些数据结构和函数很简单时，`reduce` 可能看起来是不必要的；然而，当数据结构和我们想要对其进行的转换变得更加复杂时，我们可以使用 `reduce`
    来使我们的转换更加透明。更复杂的 `reduce` 实现，如我们将在 [第 6 章](kindle_split_015.html#ch06) 中看到的，还允许进行并行归约，这提供了与并行
    `map` 相同的性能改进，而无需对我们的代码进行大量重写。
- en: Testing our summation function
  id: totrans-1064
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 测试我们的求和函数
- en: At this point, we have a working summation reduction. Feel free to run the combined
    code from [listings 5.1](#ch05ex01) and [5.2](#ch05ex02). If you wrap the `reduce`
    call in a `print` function, you should see an integer printed to your screen.
    Unlike `map` and the lazy data types we looked at in [chapter 4](kindle_split_013.html#ch04),
    `reduce` evaluates when it’s called.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个工作的求和归约。您可以自由运行 [列表 5.1](#ch05ex01) 和 [5.2](#ch05ex02) 中的组合代码。如果您将
    `reduce` 调用包裹在 `print` 函数中，您应该在屏幕上看到一个整数被打印出来。与 [第 4 章](kindle_split_013.html#ch04)
    中我们查看的 `map` 和惰性数据类型不同，`reduce` 在被调用时进行评估。
- en: 5.2.2\. Concise accumulations using lambda functions
  id: totrans-1066
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 使用 lambda 函数进行简洁的累积
- en: As you may have been thinking while typing up the code for [listing 5.2](#ch05ex02),
    sometimes it seems silly to create a whole function for a one-line statement like
    adding together two numbers. In cases like this, it’s common to use a lambda function
    instead of defining a function.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在编写 [列表 5.2](#ch05ex02) 的代码时，您可能已经在思考，有时为像将两个数字相加这样的单行语句创建整个函数似乎很愚蠢。在这种情况下，通常使用
    lambda 函数而不是定义一个函数。
- en: Lambda functions are also known as anonymous functions because we don’t save
    them to the name space. Although we’re perfectly free to call our `my_add` function
    whenever and wherever we want, the anonymous function only exists inside of the
    `reduce` call and will not be available beyond the scope of that single command.
    For small operations, this is really nice. We don’t even have to worry about naming
    these functions. For larger operations, we’d rather have a callable function.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数也被称为匿名函数，因为我们没有将它们保存到命名空间中。尽管我们完全可以随时随地在任何地方调用我们的 `my_add` 函数，但匿名函数仅存在于
    `reduce` 调用中，并且在该单个命令的作用域之外不可用。对于小型操作，这真的很好。我们甚至不必担心这些函数的命名。对于大型操作，我们更愿意有一个可调用的函数。
- en: 'Lambda functions in Python are defined in three parts ([figure 5.4](#ch05fig04)):'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中的 lambda 函数由三部分定义 ([图 5.4](#ch05fig04))：
- en: The lambda keyword
  id: totrans-1070
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: lambda 关键字
- en: The parameters the function will take
  id: totrans-1071
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数将接受的参数
- en: A colon and the statement that the function will execute
  id: totrans-1072
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个冒号和函数将要执行的语句
- en: Figure 5.4\. We can use lambda functions in place of standard functions in `map`
    or `reduce` operations when we won’t need to use the function’s behavior again.
  id: totrans-1073
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4\. 当我们不需要再次使用函数的行为时，我们可以在 `map` 或 `reduce` 操作中使用 lambda 函数代替标准函数。
- en: '![](05fig04_alt.jpg)'
  id: totrans-1074
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig04_alt.jpg)'
- en: For example, our `my_add` function could be a simple lambda function.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的 `my_add` 函数可以是一个简单的 lambda 函数。
- en: 'You’ll notice the `lambda` keyword is the first thing in our statement, followed
    by our two parameters: `acc` and `nxt`. The two parameters are separated by a
    comma, just like they would be in a normal function declaration. Unlike a function
    declaration, however, we won’t find any name for the function. Additionally, we
    declare the function’s behavior immediately after the parameters on the same line,
    only separated by a colon and a space. Lastly, you’ll notice that this lambda
    statement returns a function. We could assign this to a variable and use it like
    a normal function if we wanted to; however, usually we’ll just want to be done
    with our anonymous, throwaway lambda function.'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 `lambda` 关键字是我们语句中的第一件事，后面跟着我们的两个参数：`acc` 和 `nxt`。这两个参数由逗号分隔，就像它们在正常函数声明中一样。然而，与函数声明不同的是，我们不会找到任何函数名。此外，我们在同一行上参数之后立即声明函数的行为，只由冒号和空格分隔。最后，你会注意到这个
    lambda 语句返回一个函数。如果我们想的话，我们可以将其分配给一个变量并像正常函数一样使用它；然而，通常我们只想完成我们的匿名、一次性 lambda 函数。
- en: The best use for a lambda function is to declare it right inside our `reduce`
    call. To do that, we just write the lambda statement in the first position of
    our `reduce` function where the accumulator function goes, leaving the latter
    two positions for our sequence and our initializer. For example, we could simplify
    the code from [listings 5.1](#ch05ex01) and [5.2](#ch05ex02) down to the code
    in the following listing using a lambda function.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: lambda 函数的最佳用途是在我们的 `reduce` 调用中直接声明它。要做到这一点，我们只需在 `reduce` 函数的第一个位置（即累积函数的位置）写下
    lambda 语句，留下后两个位置用于我们的序列和初始化器。例如，我们可以使用 lambda 函数将 [列表 5.1](#ch05ex01) 和 [5.2](#ch05ex02)
    中的代码简化到以下列表中的代码。
- en: Listing 5.3\. Lambda function inside `reduce` for summation
  id: totrans-1078
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. `reduce` 中的 lambda 函数用于求和
- en: '[PRE68]'
  id: totrans-1079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '***1* Our lambda addition function goes in the first position of our reduce
    statement.**'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的 lambda 加法函数位于 reduce 语句的第一个位置。**'
- en: 'This code achieves the same end as our previous code. This time, though, we
    don’t need to save space for our addition function. In this specific case, using
    the lambda function works great because our task at hand is small: we’re adding
    two numbers. Other useful cases for using lambdas are when we want to expose class
    methods or attributes.'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实现了我们之前代码相同的结果。不过，这次我们不需要为我们的加法函数留出空间。在这种情况下，使用 lambda 函数效果很好，因为我们的任务很小：我们正在添加两个数字。其他使用
    lambda 的情况包括当我们想要公开类方法或属性时。
- en: For example, we can use a lambda function to expose the `.get` method from the
    `dict` class and sum the price of several products. We can see this play out in
    the following listing.
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用 lambda 函数来公开 `dict` 类的 `.get` 方法并计算几个产品的价格总和。我们可以在下面的列表中看到这一点。
- en: Listing 5.4\. Lambda functions can be used to expose class methods
  id: totrans-1083
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. Lambda 函数可以用来公开类方法
- en: '[PRE69]'
  id: totrans-1084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '***1* Our product data is stored in dicts, each containing a price and a serial
    number (sn).**'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的产品数据存储在字典中，每个字典包含一个价格和一个序列号（sn）。**'
- en: '***2* We can call the .get method of each dict in the lambda function to add
    the prices.**'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 我们可以在 lambda 函数中调用每个字典的 .get 方法来添加价格。**'
- en: '[Listing 5.4](#ch05ex04) shows a classic lambda function: we need to do something
    that’s a bit nuanced, like getting the value of the price key of a `dict` and
    adding it to another value, but not something that we’ll want to necessarily ever
    do again. Our lambda function is still very readable, and it would feel silly
    to create a whole function to get a value from a `dict` and add it to another
    value.'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.4](#ch05ex04) 展示了一个经典的 lambda 函数：我们需要做一些稍微复杂的事情，比如获取 `dict` 中价格键的值并将其添加到另一个值中，但不是我们一定会再次想要做的事情。我们的
    lambda 函数仍然非常易于阅读，而且创建一个完整的函数来从 `dict` 中获取值并添加到另一个值中会显得很愚蠢。'
- en: 5.2.3\. Initializers for complex start behavior in reduce
  id: totrans-1088
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. `reduce` 中复杂起始行为的初始化器
- en: The last piece of the `reduce` puzzle is the initializer parameter. The initializer
    is the value that our `reduce` operation will use as the very first accumulated
    value. We can think of it as inserting that value at the head of our sequence
    and shifting all the other values to the right by 1 ([figure 5.5](#ch05fig05)).
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce` 问题的最后一部分是初始化参数。初始化参数是 `reduce` 操作将用作第一个累积值的值。我们可以将其视为在序列的开头插入该值，并将所有其他值向右移动
    1 位（[图 5.5](#ch05fig05)）。'
- en: Figure 5.5\. An initializer value shifts all the values to the right by 1, changing
    the start value of the `reduce` operation.
  id: totrans-1090
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.5\. 初始化值将所有值向右移动 1 位，改变了 `reduce` 操作的起始值。
- en: '![](05fig05_alt.jpg)'
  id: totrans-1091
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig05_alt.jpg)'
- en: For our summation reduction, adding an initializer value of 10 would increase
    our entire `reduce` by 10\. Instead of starting with the first value (the default)
    or with zero, we would start adding to 10\. This might be useful if we wanted
    to add a $10 handling fee to all of the orders, for example.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的求和简化，添加一个初始值10将会使整个`reduce`增加10。而不是从第一个值（默认）或从零开始，我们将从10开始累加。如果我们想给所有订单添加一个10的处理费，例如，这可能会很有用。
- en: Most often though, we’ll want to use an initializer not when we want to change
    the *value* of our data but when we want to change the *type* of the data. By
    seeding our `reduce` with a value of a different type, our accumulator function
    can expect two different type parameters, even when we have a list in which all
    the values are of the same type. We can see this play out if we change the integer
    0 in our summation reduction to a float 0, 0.0, as shown in the following listing.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数情况下，我们想要使用初始值，并不是因为我们想要改变数据的*值*，而是因为我们想要改变数据的*类型*。通过用不同类型的值初始化我们的`reduce`，我们的累加函数可以期望有两个不同的类型参数，即使我们有一个所有值都是同一类型的列表。如果我们改变求和简化中的整数0为浮点数0，0.0，就像以下列表所示，我们可以看到这一点是如何发挥作用的。
- en: Listing 5.5\. Seeding a summation function with a float
  id: totrans-1094
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5：使用浮点数初始化求和函数
- en: '[PRE70]'
  id: totrans-1095
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '***1* Changing the last parameter from an integer (0) to a float (0.0) changes
    the output**'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将最后一个参数从整数(0)更改为浮点数(0.0)会改变输出**'
- en: Inserting a float into our summation reduction, as seen in [listing 5.5](#ch05ex05),
    changes the eventual output type of our summation to a float. This happens because
    a float plus an integer always returns a float in Python. This effect cascades
    across our reduction because the accumulator function always has a float for its
    accumulator parameter ([figure 5.6](#ch05fig06)).
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表5.5](#ch05ex05)所示，将浮点数插入到我们的求和简化中，会改变我们求和的最终输出类型为浮点数。这是因为浮点数加整数在Python中总是返回浮点数。由于累加函数总是有一个浮点数作为其累加参数([图5.6](#ch05fig06))，这种效果会跨简化级联。
- en: Figure 5.6\. The type of the initializer often determines the behavior of our
    accumulator function.
  id: totrans-1098
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6：初始器的类型通常决定了我们累加函数的行为。
- en: '![](05fig06_alt.jpg)'
  id: totrans-1099
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig06_alt.jpg)'
- en: This pattern, where we use the initializer to alter the type of our sequence,
    is going to be a common occurrence. We’ll often want our accumulator to take and
    return a type that is different from the type of elements that are in our list.
    This represents a wider variety of transformations than we could achieve with
    just a single data type. We’ll look at an example of that pattern shortly in [section
    5.3.2](#ch05lev2sec5) and again later in this chapter.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式，即我们使用初始值来改变我们序列的类型，将会是一个常见的现象。我们经常希望我们的累加器能够接受并返回与列表中元素类型不同的类型。这比我们仅使用单一数据类型所能实现的转换种类更广泛。我们将在[5.3.2节](#ch05lev2sec5)中稍后看到一个该模式的示例，并在本章稍后再次看到。
- en: 5.3\. Reductions you’re familiar with
  id: totrans-1101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3：你熟悉的简化
- en: 'Having looked at the basics of `reduce` with the summation function, let’s
    look at two more reductions that you’ve already seen in this book:'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看使用求和函数的基本`reduce`之后，让我们看看这本书中你已经看到的另外两个简化：
- en: '`filter`'
  id: totrans-1103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`frequencies`'
  id: totrans-1104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`frequencies`'
- en: We explored both functions in [chapter 4](kindle_split_013.html#ch04). The `filter`
    function returns a list of items that evaluate `True` for a given condition. The
    `frequencies` function returns a `dict` whose keys are the unique elements of
    a list and whose values are the counts of those items in the list.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](kindle_split_013.html#ch04)中探讨了这两个函数。`filter`函数返回一个列表，其中包含对给定条件评估为`True`的项目。`frequencies`函数返回一个`dict`，其键是列表的唯一元素，其值是列表中这些项的计数。
- en: 5.3.1\. Creating a filter with reduce
  id: totrans-1106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1. 使用`reduce`创建过滤器
- en: For our `filter` reduction, let’s perform a `filter` operation that returns
    only even numbers. That way, we can compare this code to some of the examples
    we worked on in [chapter 4](kindle_split_013.html#ch04). Before diving straight
    into the reduction, however, we should think about what this reduction is going
    to look like ([figure 5.7](#ch05fig07)).
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`filter`简化，让我们执行一个只返回偶数的`filter`操作。这样，我们可以将此代码与我们之前在[第4章](kindle_split_013.html#ch04)中工作的一些示例进行比较。然而，在直接进入简化之前，我们应该思考这种简化将看起来是什么样子([图5.7](#ch05fig07))。
- en: Figure 5.7\. The filter reduction is an N-to-X transformation from a list of
    some size to a list of some smaller or equal size.
  id: totrans-1108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7：`filter`简化是一个从某个大小的列表到某个更小或相等大小的列表的N-to-X转换。
- en: '![](05fig07_alt.jpg)'
  id: totrans-1109
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig07_alt.jpg)'
- en: The `filter` function starts with a sequence of some sort, so we know that it’s
    a good candidate for reduction on that ground. Our output data in this instance
    is going to be a list of a length equal to or less than that of our previous sequence.
    For example, if we have a sequence where all the numbers are 2 (an even number),
    then our reduction should return the same sequence as a list. In contrast, if
    all the numbers are odd, our reduction should return an empty list.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter`函数从一个某种类型的序列开始，因此我们知道它是一个很好的归约候选。在这个实例中，我们的输出数据将是一个长度等于或小于我们之前序列长度的列表。例如，如果我们有一个所有数字都是2（一个偶数）的序列，那么我们的归约应该返回相同的序列作为一个列表。相反，如果所有数字都是奇数，我们的归约应该返回一个空列表。'
- en: Thinking this through gives us some sense of how our reduction needs to behave
    and how we need to set it up. We know we’re going to need to be able to return
    an empty list in some cases, so it makes sense to initialize our reduction with
    an empty list. The rest of our reduction is going to depend on the accumulator
    function we design. Because we’re attempting to filter down to just the even numbers,
    I’m going to call this function `keep_if_even`.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过思考这个问题，我们可以对我们的归约行为和如何设置它有一个大致的了解。我们知道在某些情况下，我们需要能够返回一个空列表，因此用空列表初始化我们的归约是有意义的。我们归约的其余部分将取决于我们设计的累积函数。因为我们试图筛选出仅包含偶数的值，所以我将把这个函数命名为`keep_if_even`。
- en: 'The `keep_if_even` function is going to need to take in two things:'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: '`keep_if_even`函数将需要接收两个东西：'
- en: An accumulated value (a list of even numbers)
  id: totrans-1113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 累积值（一个偶数列表）
- en: The next value in our sequence
  id: totrans-1114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们序列中的下一个值
- en: The function will also need to return either the original accumulated value,
    if the next value is not even, or the accumulated value plus the new value, if
    the next value is even. This function is implemented in the following listing.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 函数还需要返回原始累积值，如果下一个值不是偶数，或者如果下一个值是偶数，则返回累积值加上新值。这个函数在下面的列表中实现。
- en: Listing 5.6\. An is it even? filter reduction
  id: totrans-1116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6\. 一个“它是偶数吗？”过滤器归约
- en: '[PRE71]'
  id: totrans-1117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '***1* Our accumulator function expects an accumulated value (a list) and the
    next item (a number).**'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的累积函数期望一个累积值（一个列表）和下一个项目（一个数字）。**'
- en: '***2* Checks if the next item is even**'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 检查下一个项目是否为偶数**'
- en: '***3* If it is, we add it to our accumulator and return a new list.**'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果它是，我们将其添加到我们的累积中并返回一个新的列表。**'
- en: '***4* If it is not, we return the original accumulator.**'
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果它不是，我们返回原始累积。**'
- en: Much of the code in [listing 5.6](#ch05ex06) will be similar to code you’ve
    seen before, in either this chapter or [chapter 4](kindle_split_013.html#ch04).
    One important thing to point out, however, is that we use the construction `acc
    + [nxt]` instead of `acc.append(nxt)`. We first used the `acc` and `nxt` parameter
    names in [listing 5.2](#ch05ex02) of this chapter, with `acc` representing the
    accumulated value and `nxt` representing the next element in our sequence.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.6](#ch05ex06)中的大部分代码将与我们之前在这个章节或[第4章](kindle_split_013.html#ch04)中看到的代码相似。然而，有一点需要指出的是，我们使用构造`acc
    + [nxt]`而不是`acc.append(nxt)`。我们第一次在这个章节的[列表5.2](#ch05ex02)中使用`acc`和`nxt`参数名，其中`acc`代表累积值，而`nxt`代表我们序列中的下一个元素。'
- en: We don’t use the `.append` method here because although `.append` is the preferred
    way of adding values to a list, our accumulator function will always need to return
    a value. By design, the `.append` method modifies the list in place and returns
    `None`. This forces us to use `acc + [nxt]`, which returns a new list.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不使用`.append`方法，因为尽管`.append`是向列表中添加值的推荐方式，但我们的累积函数始终需要返回一个值。按照设计，`.append`方法会就地修改列表并返回`None`。这迫使我们使用`acc
    + [nxt]`，它返回一个新的列表。
- en: You’ll also note that this filter works as desired on the edge cases identified
    a few paragraphs ago. If we pass in a list of all 2s, we’ll get the same-sized
    list back. If we pass in a list of all odd numbers (say, 3s), we’ll get an empty
    list back.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 你也会注意到，这个过滤器在前面几段中确定的边缘情况下按预期工作。如果我们传入一个包含所有2的列表，我们将得到同样大小的列表。如果我们传入一个包含所有奇数（比如，3）的列表，我们将得到一个空列表。
- en: 5.3.2\. Creating frequencies with reduce
  id: totrans-1125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 使用reduce创建频率
- en: The next type of reduction that we’ll tackle is the frequency reduction. Frequency,
    which we saw in [chapter 4](kindle_split_013.html#ch04) as `frequencies`, is a
    way of counting the elements of a sequence. Again, let’s stop to think about the
    N-to-X transformation that’s going on in this function. We’ll start with a sequence
    (N), and we want to end up with a `dict` with some number of keys, each corresponding
    to a unique element in the sequence, and a value totaling their count within the
    sequence ([figure 5.8](#ch05fig08)).
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要解决的下一类归约是频率归约。频率，我们在第 4 章（`frequencies`）中看到，是计数序列元素的一种方式。再次，让我们停下来思考这个函数中正在进行的
    N-to-X 转换。我们将从一个序列（N）开始，并希望最终得到一个 `dict`，其中包含一些键，每个键对应于序列中的一个唯一元素，以及一个值总计它们在序列中的计数（[图
    5.8](#ch05fig08)）。
- en: Figure 5.8\. Our frequency reduction transforms a list into a `dict` with keys
    for each unique element and values totaling their counts. We need to initialize
    with a `dict` because the accumulation function takes two parameters of different
    types.
  id: totrans-1127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8\. 我们的频率归约将列表转换为一个 `dict`，其中键对应于每个唯一元素，值总计它们的计数。我们需要初始化为一个 `dict`，因为累加函数接受两种不同类型的参数。
- en: '![](05fig08_alt.jpg)'
  id: totrans-1128
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig08_alt.jpg)'
- en: The accumulator function for our frequency reduction will take a `dict` as an
    accumulated value and a miscellaneous element as our next value. It will have
    to return a `dict` as well so that, as we move through our sequence, we can ensure
    we always have a `dict` as our accumulated value. It also will have to count the
    element. To do this, we’ll increment the value of that element as a key by 1\.
    Also, this time, let’s wrap our `reduce` operation in a function so we can reuse
    it.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们频率归约的累加函数将接受一个 `dict` 作为累加值和一个杂项元素作为我们的下一个值。它必须返回一个 `dict`，这样在我们遍历序列时，我们可以确保我们始终有一个
    `dict` 作为我们的累加值。它还必须计数元素。为此，我们将该元素的值作为键增加 1。此外，这次，让我们将我们的 `reduce` 操作包装在一个函数中，这样我们就可以重用它。
- en: '[Listing 5.7](#ch05ex07) provides the code for the accumulator and the reduction,
    along with test data and some print statements that demonstrate our function is
    working as desired. In those statements, we can see that our `frequencies` function
    can be used to count up sequences of all different types. We’re able to do this
    because `reduce` doesn’t care what type of objects we’re iterating over and because
    our accumulation function doesn’t rely on the objects in a sequence being of a
    specific type. We also see the importance of initializing our reduction with an
    empty `dict` so we can use the `.get` method from the start.'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.7](#ch05ex07) 提供了累加函数和归约的代码，以及一些测试数据和打印语句，这些语句展示了我们的函数按预期工作。在这些语句中，我们可以看到我们的
    `frequencies` 函数可以用来计数所有不同类型的序列。我们能够做到这一点，因为 `reduce` 不关心我们正在迭代的对象类型，以及我们的累加函数不依赖于序列中的对象是特定类型。我们还看到了初始化我们的归约为一个空
    `dict` 的重要性，这样我们就可以从开始就使用 `.get` 方法。'
- en: Listing 5.7\. Finding frequencies using a reduction
  id: totrans-1131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7\. 使用归约查找频率
- en: '[PRE72]'
  id: totrans-1132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '***1* Our make_counts function has the standard accumulator function parameters:
    acc and nxt.**'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的 make_counts 函数具有标准的累加函数参数：acc 和 nxt。**'
- en: '***2* For each element we come across, we increment the number of times we’ve
    seen that element by 1.**'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对于我们遇到的每个元素，我们将该元素出现的次数增加 1。**'
- en: '***3* Returns the accumulated value at the end of the function**'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在函数结束时返回累加值**'
- en: '***4* Our frequency reduction function will only need to take a sequence of
    some kind.**'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 我们的频率归约函数只需要接受某种类型的序列。**'
- en: '***5* Our reduce statement uses the make_counts function we just made, as well
    as an empty dict as an initializer object.**'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 我们的 reduce 语句使用了我们刚刚创建的 make_counts 函数，以及一个空的字典作为初始化对象。**'
- en: 5.4\. Using map and reduce together
  id: totrans-1138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 结合使用 map 和 reduce
- en: At this point, we’ve covered the basics of `reduce`. If you can decompose a
    problem into an N-to-X transformation, all that stands between you and a reduction
    that solves that problem is a well-crafted accumulation function. That said, I’d
    be remiss if we wrapped up our discussion of `reduce` without discussing how we
    can use it in conjunction with `map` in the eponymous map-reduce pattern.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了 `reduce` 的基础知识。如果你可以将一个问题分解为 N-to-X 转换，那么解决该问题的归约与一个精心设计的累加函数之间的所有东西就只是你了。话虽如此，如果我们不讨论如何在
    map-reduce 模式中与 `map` 结合使用 `reduce`，那就有些疏忽了。
- en: So far, we’ve focused on situations where at least some of the data we want
    to end up with comes directly from our sequence.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注的是我们想要得到的数据中至少有一部分直接来自我们的序列。
- en: In the sum reduction ([listing 5.3](#ch05ex03)), we needed the values in the
    list.
  id: totrans-1141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在求和归约（[列表 5.3](#ch05ex03)）中，我们需要列表中的值。
- en: In the filter reduction ([listing 5.6](#ch05ex06)), we wanted to end up with
    the values that met a given condition.
  id: totrans-1142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在过滤归约（[列表 5.6](#ch05ex06)）中，我们希望得到满足给定条件的值。
- en: In the frequency reduction ([listing 5.7](#ch05ex07)), we used the sequence
    elements as keys for our `dict`.
  id: totrans-1143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在频率归约（[列表 5.7](#ch05ex07)）中，我们使用了序列元素作为我们的 `dict` 的键。
- en: This is not always the case. Sometimes we don’t want to work with the data in
    our sequence, only data that is somewhat related to our sequence. The classic
    example is that we have a sequence of file paths and want to open those files
    and do something with them. We saw that in [chapter 4](kindle_split_013.html#ch04)
    in the poetry puzzle example. In that example, we had a bunch of files; however,
    it was the content of those files that was interesting to us, not the files themselves.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总是如此。有时我们不想处理序列中的数据，只想处理与序列有一定关联的数据。一个经典的例子是我们有一个文件路径序列，想要打开这些文件并对它们进行一些操作。我们在第
    4 章的诗歌谜题示例中看到了这一点。在那个例子中，我们有一堆文件；然而，对我们来说，这些文件的内容才是有趣的，而不是文件本身。
- en: 'Another version of this problem could be a twist on the Scrabble exercise at
    the end of [chapter 4](kindle_split_013.html#ch04). What if instead of filtering
    our list down to the words that met some point threshold, we summed all the points
    represented by the words in a list? In that example, the list may contain the
    words we’ve scored to date, and their sum would equal our total score. To find
    our total score, we want to convert the words to their scores (an N-to-N transformation)
    and then reduce those scores into a total score (an N-to-X transformation) ([figure
    5.9](#ch05fig09)). Because this process represents both an N-to-N transformation
    and an N-to-X transformation, we can use both `map` and `reduce`: `map` to transform
    the words to scores and `reduce` to sum them up.'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的另一个版本可能是对第 4 章末的 Scrabble 练习的一种变体。如果我们不是过滤我们的列表，只保留达到某些得分阈值的单词，而是将列表中单词所代表的分数全部相加会怎样？在那个例子中，列表可能包含我们迄今为止评分的单词，它们的总和就是我们的总分。为了找到我们的总分，我们希望将单词转换为它们的分数（一个
    N 到 N 的转换），然后将这些分数归约成一个总分（一个 N 到 X 的转换）[图 5.9](#ch05fig09)。因为这个过程代表了 N 到 N 的转换和
    N 到 X 的转换，我们可以同时使用 `map` 和 `reduce`：`map` 用于将单词转换为分数，`reduce` 用于将它们求和。
- en: Figure 5.9\. We can use the map and reduce pattern to transform words into scores
    and then calculate a sum of those scores.
  id: totrans-1146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9\. 我们可以使用 map 和 reduce 模式将单词转换为分数，然后计算这些分数的总和。
- en: '![](05fig09_alt.jpg)'
  id: totrans-1147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig09_alt.jpg)'
- en: 'To do this, we’ll need to concoct two helper functions: one for `map` and one
    for `reduce`. If you completed exercise 4.6.5, you already have both of them on
    hand. (If you don’t have them, you can either complete the exercise now or find
    the code in this book’s source code repository at [https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets).)
    The helper function for `map` will need to take in a word and return a score.
    Just like in exercise 4.6.5, we’ll use the simplified scoring scheme: Z is worth
    10 points; F, H, V, and W are worth 5; B, C, M, and P are worth 3; and all other
    letters are worth 1 point. The helper function for `reduce` will be either the
    helper function from [listing 5.2](#ch05ex02) or the lambda expression from [listing
    5.3](#ch05ex03)—either will work.'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要构建两个辅助函数：一个用于 `map`，一个用于 `reduce`。如果你完成了练习 4.6.5，你已经有这两个函数了。（如果你没有，你现在可以完成这个练习，或者在这个书的源代码仓库[https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)中找到代码。）`map`
    的辅助函数需要接受一个单词并返回一个分数。就像在练习 4.6.5 中一样，我们将使用简化的评分方案：Z 值为 10 分；F、H、V 和 W 值为 5 分；B、C、M
    和 P 值为 3 分；所有其他字母值均为 1 分。`reduce` 的辅助函数将是 [列表 5.2](#ch05ex02) 中的辅助函数或 [列表 5.3](#ch05ex03)
    中的 lambda 表达式——两者都可以使用。
- en: With those two helper functions in place, to find our total score we `map` the
    scoring function across our words and `reduce` over the results of that `map`.
    We can see this entire process in the following listing.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个辅助函数就绪的情况下，为了找到我们的总分，我们需要将评分函数映射到我们的单词上，并对映射的结果进行`reduce`操作。我们可以在下面的列表中看到整个流程。
- en: Listing 5.8\. Scoring words with `map` and `reduce`
  id: totrans-1150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8\. 使用 `map` 和 `reduce` 评分单词
- en: '[PRE73]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '***1* This reduction is identical to the summation reduction we used at the
    beginning of the chapter, except instead of passing reduce a sequence of numbers,
    we pass it the result of our map operation.**'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这种归约与我们在本章开头使用的求和归约相同，只是我们没有向`reduce`传递一系列数字，而是传递了我们的`map`操作的结果。**'
- en: The power of map and reduce is in the simplicity of its execution. When we actually
    go to execute our `reduce` and `map` statements, we do so in a single line of
    code, though this one line implements complex behavior through the invoked helper
    functions. We can use the map and reduce pattern to decouple the transformation
    logic—the things we want to do to our data—from the actual transformation itself.
    This permits simplicity and leads to highly reusable code. When working with large
    datasets, keeping our functions simple becomes paramount because we may have to
    wait a long time to discover we made a small error.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`和`reduce`的力量在于其执行的简单性。当我们实际执行`reduce`和`map`语句时，我们只使用一行代码，尽管这一行代码通过调用的辅助函数实现了复杂的行为。我们可以使用`map`和`reduce`模式来解耦转换逻辑——我们想要对我们数据进行的事情——与实际的转换本身。这允许我们保持简单，并导致高度可重用的代码。当处理大型数据集时，保持我们的函数简单变得至关重要，因为我们可能需要等待很长时间才能发现我们犯了一个小错误。'
- en: 5.5\. Analyzing car trends with reduce
  id: totrans-1154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 使用reduce分析汽车趋势
- en: Before we move on from [chapter 5](#ch05) and start looking at `reduce` in parallel,
    let’s try our hand at a more complex reduction scenario.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开[第5章](#ch05)并开始并行查看`reduce`之前，让我们尝试一个更复杂的归约场景。
- en: '|  |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-1157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: Your customer is a used car dealer. They have data on cars that they’ve bought
    and sold in the last six months and are hoping you can help them find what type
    of used cars they make the most profit on. One salesman believes that highly fuel-efficient
    cars (those that get more than 35 miles per gallon (mpg)) make the most money,
    while another believes that medium-mileage cars (with odometers at 60,000 to 100,000
    miles) result in the highest average profit on resale. Given a CSV file with a
    variety of attributes about some used cars, write a script to find the average
    profit on cars of low (<18 mpg), medium (18–35 mpg), and high (>35 mpg) fuel efficiency,
    as well as low (<60,000 miles), medium (60,000–100,000 miles), and high mileage
    (>100,000 miles), to settle the debate.
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 您的客户是一位二手车经销商。他们有过去六个月内购买和销售汽车的数据，希望您能帮助他们找到哪种类型的二手车能带来最大的利润。一位销售员认为高燃油效率的汽车（每加仑超过35英里（mpg）的汽车）能带来最多的收入，而另一位销售员认为中等里程的汽车（里程表在60,000到100,000英里之间）在转售时能带来最高的平均利润。给定一个包含一些二手车各种属性的CSV文件，编写一个脚本来找出低（<18
    mpg）、中（18–35 mpg）和高（>35 mpg）燃油效率的汽车的平均利润，以及低（<60,000英里）、中（60,000–100,000英里）和高（>100,000英里）里程的汽车的平均利润，以解决这场辩论。
- en: '|  |'
  id: totrans-1159
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Before we dig into the details of the problem, let’s take a look at its fundamentals:
    the data transformations. We’ll start with a series of `dict`s, each of which
    represents a vehicle. By default, these `dict`s will have a lot of information
    we’re not interested in and won’t have some of the information we do want, so
    it’ll be a good idea to transform the data into a better format for analysis.
    We’ll tackle that with a `map` because we want to clean up each `dict`. From there,
    we want to roll that data up into a `dict` that can help us understand the profit
    that each type of car produces. This will require a reduction.'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨问题的细节之前，让我们看看它的基础：数据转换。我们将从一个代表车辆的`dict`系列开始。默认情况下，这些`dict`将包含我们不感兴趣的大量信息，并且可能缺少一些我们想要的信息，因此将数据转换成更适合分析的好格式是个好主意。我们将使用`map`来处理这个问题，因为我们想要清理每个`dict`。从那里，我们希望将数据汇总到一个`dict`中，这个`dict`可以帮助我们了解每种类型的汽车能产生多少利润。这需要归约。
- en: Overall, the whole problem will look something like [figure 5.10](#ch05fig10).
    On the left, we start with the data our customer hands us. We’ll concoct a function
    to clean up each record and `map` that across our data. Then, we’ll pass that
    into `reduce`, which itself has an accumulator function we’ve designed to collect
    the necessary information. For this, we’ll want to gather both sum and count by
    group—the two figures necessary to calculate an average.
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，整个问题看起来会像[图5.10](#ch05fig10)那样。在左侧，我们从客户给出的数据开始。我们将构造一个函数来清理每条记录，并将该函数映射到我们的数据上。然后，我们将它传递给`reduce`函数，该函数本身有一个我们设计的累加器函数，用于收集必要的信息。为此，我们需要按组收集总和和计数——这两个数字是计算平均数所必需的。
- en: Figure 5.10\. We can solve our car data analysis task using a `map` step that
    cleans up car data and a `reduce` step that accumulates the data into one data
    structure that answers our question.
  id: totrans-1162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. 我们可以使用一个`map`步骤来清理汽车数据，以及一个`reduce`步骤来将数据累积到一个回答我们问题的单一数据结构中，来解决这个问题。
- en: '![](05fig10_alt.jpg)'
  id: totrans-1163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig10_alt.jpg)'
- en: 5.5.1\. Using map to clean our car data
  id: totrans-1164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1\. 使用`map`清理汽车数据
- en: To design our cleaning helper function, let’s first take a closer look at the
    individual elements with which we’ll be working. Each car in our dataset is going
    to look something like [figure 5.11](#ch05fig11).
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计我们的清理辅助函数，让我们首先更仔细地看看我们将要工作的单个元素。我们的数据集中的每辆车将看起来像[图 5.11](#ch05fig11)。
- en: 'Figure 5.11\. Each car in our dataset will have many attributes, only four
    of which we really care about: price-buy, price-sell, mpg, and odo. We’ll use
    `map` plus a helper function to transform those numerical variables into categorical
    variables for easier comparison.'
  id: totrans-1166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11\. 我们的数据集中的每辆车都将有许多属性，其中只有四个是我们真正关心的：购买价格、销售价格、mpg 和 odo。我们将使用`map`和一个辅助函数将这些数值变量转换为分类变量，以便更容易比较。
- en: '![](05fig11_alt.jpg)'
  id: totrans-1167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig11_alt.jpg)'
- en: 'For each entry, we’ll have a `dict` with lots of attributes we’re not particularly
    interested in, along with the four that we are interested in: `price-buy`, `price-sell`,
    `mpg`, and `odo`. These four keys in our `dict` represent the price the car was
    bought at, the price the car was sold at, the manufacturer-listed miles per gallon
    of the vehicle, and the number of miles on the car. However, we’re not actually
    interested in the values of any of these variables directly. Rather, we’re interested
    in values that we can calculate from them.'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个条目，我们将有一个包含许多属性（我们不太感兴趣）的`dict`，以及四个我们感兴趣的属性：`price-buy`、`price-sell`、`mpg`和`odo`。在我们的`dict`中的这四个键代表汽车购买时的价格、销售时的价格、车辆列出的每加仑里程数以及汽车上的里程数。然而，我们实际上并不直接对任何这些变量的值感兴趣。相反，我们感兴趣的是可以从它们计算出的值。
- en: Instead of price bought and sold, we’re interested in total profit.
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对购买和销售价格不感兴趣，我们感兴趣的是总利润。
- en: Instead of absolute miles per gallon, we’re interested in low, medium, and high
    mpg.
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对每加仑绝对里程数不感兴趣，我们感兴趣的是低、中、高 mpg。
- en: Instead of absolute number of miles, we’re interested in low, medium, and high
    mileage.
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对绝对里程数不感兴趣，我们感兴趣的是低、中、高里程。
- en: 'To that end, to clean each data entry, we’ll want to do three things:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，为了清理每个数据条目，我们想要做三件事：
- en: Calculate profit on the vehicle from price bought and sold
  id: totrans-1173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从购买和销售价格计算车辆利润
- en: Sort the vehicle into low, medium, and high mpg
  id: totrans-1174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将车辆分为低、中、高 mpg
- en: Sort the vehicle into low, medium, and high mileage
  id: totrans-1175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将车辆分为低、中、高里程
- en: To do this, we’ll create three separate functions that each handle a piece of
    the problem and wrap them in a single function we can map across all our data.
    Let’s design each of these three helper functions now, starting with calculating
    profit.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将创建三个单独的函数，每个函数处理问题的一部分，并将它们包装在一个我们可以映射到所有数据的单一函数中。现在让我们设计这三个辅助函数，从计算利润开始。
- en: 'The profit calculation function is only a small change from a basic operation:
    arithmetic. In other conditions, this might be a good case for a lambda function;
    however, because we’re planning on using this function inside another function,
    we’ll want to give it a name. Our `get_profit` function will find the difference
    between the price the car was sold at and the price the car was bought at. We
    can see it in the following listing.'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 利润计算函数仅是对基本操作（算术）的微小改动。在其他情况下，这可能是一个使用 lambda 函数的好案例；然而，因为我们计划在另一个函数内部使用这个函数，所以我们将给它一个名字。我们的`get_profit`函数将找出汽车销售价格和购买价格之间的差异。我们可以在下面的列表中看到它。
- en: Listing 5.9\. Lambda function for calculating price differences
  id: totrans-1178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.9\. 计算价格差异的 Lambda 函数
- en: '[PRE74]'
  id: totrans-1179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: One thing to note about [listing 5.9](#ch05ex09) is that we use the `.get` method
    of the `dict` instead of the `[<key>]` syntax because with `get` we can provide
    a default value. We do this to preempt the errors that a missing value would throw
    (though there are no missing values in the data you’ve been provided).
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[列表 5.9](#ch05ex09)的一个需要注意的事项是，我们使用`dict`的`.get`方法而不是`[<key>]`语法，因为使用`get`我们可以提供一个默认值。我们这样做是为了防止缺失值抛出的错误（尽管你提供的数据中没有缺失值）。
- en: 'Next up, we have two helper functions that provide similar functionality: one
    that buckets mpg into three categories—low, medium, and high—and one that buckets
    mileage into three categories—low, medium, and high. Because these functions are
    so similar, let’s work on them at the same time.'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有两个提供类似功能的有用函数：一个将mpg分为三个类别——低、中、高，另一个将里程分为三个类别——低、中、高。因为这些函数非常相似，让我们同时处理它们。
- en: 'Both of these functions share a common behavior: comparing a value to a series
    of break points and then assigning them to either low, medium, or high. We can
    write a general function that takes a `dict`, a key, and two break points and
    returns `low` when the value of the `dict` at the key specified is below the first
    break point, `medium` when it’s below the second, and `high` when it’s above both.
    That function will look like the code in the following listing.'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数共享一个共同的行为：将一个值与一系列分界点进行比较，然后将其分配给低、中或高。我们可以编写一个通用函数，该函数接受一个`dict`、一个键和两个分界点，当`dict`在指定键的值低于第一个分界点时返回`low`，当它低于第二个分界点时返回`medium`，当它高于两个分界点时返回`high`。该函数将类似于以下列表中的代码。
- en: Listing 5.10\. A generic low-medium-high function
  id: totrans-1183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10\. 一个通用的低-中-高函数
- en: '[PRE75]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '***1* If the value of the dict at the key of interest is below our first break,
    we return low.**'
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果感兴趣键的dict值低于我们的第一个分界点，我们返回低。**'
- en: '***2* If that value is below the second break, we return medium.**'
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果该值低于第二个分界点，我们返回中等。**'
- en: '***3* If it’s not lower than either break, we return high.**'
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果它不低于任何一个分界点，我们返回高。**'
- en: 'With this function written, we can start to assemble all of the pieces together.
    We’ll want to do three things:'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写了这个函数之后，我们可以开始组装所有这些部件。我们想要做三件事：
- en: Take in a `dict`
  id: totrans-1189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收一个`dict`
- en: Clean the `dict` with our `select_keys` function
  id: totrans-1190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的`select_keys`函数清理`dict`
- en: Return a `dict` that has three keys
  id: totrans-1191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个包含三个键的`dict`
- en: A profit key indicating the profit made on the vehicle
  id: totrans-1192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个表示车辆利润的利润键
- en: An mpg key indicating the vehicle’s mpg category
  id: totrans-1193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个表示车辆mpg类别的mpg键
- en: An odo key indicating the vehicle’s mileage
  id: totrans-1194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个表示车辆里程的odo键
- en: A wrapper function for that process may look like the following listing.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的包装函数可能看起来如下所示。
- en: Listing 5.11\. Wrapping our car helpers into a single function
  id: totrans-1196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.11\. 将我们的汽车辅助函数包装成一个单一函数
- en: '[PRE76]'
  id: totrans-1197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '***1* Initializes a new dict for our output data**'
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 初始化一个用于输出数据的新`dict`**'
- en: '***2* Uses our profit function to get the profit**'
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用我们的利润函数来获取利润**'
- en: '***3* Uses the low-medium-high function twice to get our mpg and odo categories**'
  id: totrans-1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用低-中-高函数两次来获取我们的mpg和odo类别**'
- en: '***4* Each use takes different parameters corresponding to the specifics of
    those variables.**'
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 每次使用都对应于那些变量的具体参数。**'
- en: 5.5.2\. Using reduce for sums and counts
  id: totrans-1202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2\. 使用reduce进行求和和计数
- en: 'With our `map` wrapper function written, it’s time to move on to our reduction
    ([figure 5.12](#ch05fig12)). Knowing what our `map` will begin returning, we can
    use `reduce` to convert those items into our desired output data. What we want
    is a `dict` with six keys: one each for high, medium, and low mpg and one each
    for high, medium, and low mileage. The values of each of these keys should contain
    the average profit on vehicles of that type. Because we’ll need the total profit
    and the total number of cars sold to calculate average profit, we’ll keep track
    of those values as well. For readability, it makes sense to throw those values
    into a `dict` too. This will leave us with a `dict` with six keys—one for each
    of the categories, each of which points to another `dict` with three keys: one
    for average profit and two for the values necessary to calculate the average profit.'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写了我们的`map`包装函数之后，是时候继续我们的减少（[图5.12](#ch05fig12)）。知道我们的`map`将开始返回什么，我们可以使用`reduce`将这些项转换为所需的输出数据。我们想要的是一个包含六个键的`dict`：每个高、中、低mpg一个，每个高、中、低里程一个。每个这些键的值应该包含该类型车辆的平均利润。由于我们需要总利润和总售车数来计算平均利润，我们将跟踪这些值。为了可读性，将这些值放入`dict`中是有意义的。这将给我们留下一个包含六个键的`dict`——每个类别一个，每个类别都指向另一个包含三个键的`dict`：一个用于平均利润，两个用于计算平均利润所需的值。
- en: Figure 5.12\. We’ll `reduce` over the profit and vehicle category data to produce
    a single `dict` that contains the `total`, `count`, and `average` for each category.
  id: totrans-1204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12\. 我们将遍历利润和车辆类别数据以生成一个包含每个类别的`total`、`count`和`average`的单一`dict`。
- en: '![](05fig12_alt.jpg)'
  id: totrans-1205
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig12_alt.jpg)'
- en: 'To do this, our accumulator function will roll the profit of each observation
    of our dataset into keys of our accumulated value: one based on its mileage category
    and one based on its mpg category. Because calculating the total profit, count,
    and average is a little involved—more than we can accomplish with a single expression—let’s
    wrap this behavior in a helper function. That helper function will take the accumulated
    total, count, and average of the category of car and mix in the profit for the
    new car, while also incrementing the count and calculating a new average. We can
    see these two functions together in the following listing.'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们的累加函数将把数据集中每个观察到的利润滚入累积值的键中：一个基于其里程类别，另一个基于其mpg类别。因为计算总利润、计数和平均值稍微复杂一些——超出了我们用一个表达式就能完成的能力——让我们把这个行为封装在一个辅助函数中。这个辅助函数将接受该类别汽车的累积总和、计数和平均值，并混合新汽车的利润，同时增加计数并计算新的平均值。我们可以在下面的列表中一起看到这两个函数。
- en: Listing 5.12\. Profit average accumulator and helper function
  id: totrans-1207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.12\. 利润平均值累加器和辅助函数
- en: '[PRE77]'
  id: totrans-1208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '***1* Defines a helper function that calculates averages**'
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义一个计算平均值的辅助函数**'
- en: '***2* Uses the .get method in case we find an empty dict**'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用.get方法，以防我们找到一个空字典**'
- en: '***3* Our average value will be the profit divided by the count.**'
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 我们的平均值将是利润除以计数。**'
- en: '***4* Again, our accumulator function will take an acc and a nxt.**'
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 再次，我们的累加函数将接受一个acc和一个nxt。**'
- en: '***5* Because we’ll use profit twice, we’ll store it in a variable for easy
    access.**'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 因为我们将使用利润两次，所以我们将它存储在一个变量中以方便访问。**'
- en: '***6* We’ll modify the accumulated value for each of the two categories in
    which the car belongs.**'
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 我们将修改汽车所属的两个类别中每个类别的累积值。**'
- en: Again, in [listing 5.12](#ch05ex12), as occurred several times previously in
    this chapter, we’re using the `dict .get` method to access the key of a `dict`
    and provide a default value. In each of these cases, we want to have a default
    value that provides the expected type of data to the function using the resulting
    data. In our `acc_average` function, we use `get` because our addition operation
    needs a number. In this case, we specify the integer 0 if we don’t have the key
    in question. In our `sort_and_add` accumulator function, we specify an empty `dict`
    because our `acc_average` function expects a `dict` in its first position. Because
    we use the `.get` method in both places, we can go from having no data to having
    a fully populated data structure without making any assumptions about what categories
    are in the underlying data. This is the same trick we used in our `frequencies`
    reduction example, just on a bigger scale.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，在[列表5.12](#ch05ex12)中，正如在本章中多次发生的那样，我们使用`dict.get`方法来访问`dict`的键并提供默认值。在这些情况下，我们希望有一个默认值，它向使用结果数据的函数提供预期的数据类型。在我们的`acc_average`函数中，我们使用`get`是因为我们的加法操作需要一个数字。在这种情况下，如果我们没有找到相关的键，我们指定整数0。在我们的`sort_and_add`累加函数中，我们指定一个空`dict`，因为我们的`acc_average`函数期望在第一个位置有一个`dict`。因为我们在这两个地方都使用了`.get`方法，所以我们可以在没有任何关于底层数据中类别假设的情况下，从没有数据到拥有完整的数据结构。这是我们`frequencies`累加示例中使用的相同技巧，只是规模更大。
- en: 5.5.3\. Applying the map and reduce pattern to cars data
  id: totrans-1216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. 将map和reduce模式应用于汽车数据
- en: 'With all of our helper functions written, including the data transformation
    for `map` and the accumulator for `reduce`, we’re ready to process our data. One
    of the great things about using a map and reduce style is that this takes only
    a single line of code:'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写了所有的辅助函数，包括`map`的数据转换和`reduce`的累加器之后，我们就可以处理我们的数据了。使用map和reduce风格的一个好处是，这只需要一行代码：
- en: '[PRE78]'
  id: totrans-1218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We use `map` to apply the `clean_entry` function to each entry in our cars
    data, resulting in a cleaned sequence of data that is ready for us to reduce through.
    Then we call `reduce` with its three parameters: the accumulator function, the
    data, and an optional initializer. For the accumulator function, we use the accumulator
    we designed: `sort_and_add`. For the data, we use the results from our `map` operation.
    For the initializer, we use an empty `dict`.'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`map`将`clean_entry`函数应用于汽车数据中的每个条目，从而得到一个清理后的数据序列，我们就可以通过它进行累减。然后我们调用`reduce`，并传递其三个参数：累加函数、数据和可选的初始化器。对于累加函数，我们使用我们设计的累加器：`sort_and_add`。对于数据，我们使用`map`操作的结果。对于初始化器，我们使用一个空`dict`。
- en: 'Altogether, our code will look like the following listing. Run the code and
    settle the debate between the two car salesmen: Which car category makes the most
    profit?'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们的代码将如下所示。运行代码，解决两位汽车销售员之间的争论：哪个汽车类别能带来最大的利润？
- en: Listing 5.13\. Map and reduce to find average used car profit
  id: totrans-1221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.13. 使用 map 和 reduce 查找平均二手车利润
- en: '[PRE79]'
  id: totrans-1222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 5.6\. Speeding up map and reduce
  id: totrans-1223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6. 加速 map 和 reduce
- en: Looking back on the exercise from [section 5.5](#ch05lev1sec5), we can see that
    we didn’t do anything to make our map and reduce operation any faster. From the
    techniques we’ve covered so far in this book, we might think about using a parallel
    map from [chapter 2](kindle_split_011.html#ch02) to speed up this process. Unfortunately,
    using a parallel map will counterintuitively make our work slower—not faster.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾 [第 5.5 节](#ch05lev1sec5) 的练习，我们可以看到我们没有做任何事来使我们的 map 和 reduce 操作更快。从本书中到目前为止我们介绍的技术来看，我们可能会考虑使用
    [第 2 章](kindle_split_011.html#ch02) 中的并行 map 来加速这个过程。不幸的是，使用并行 map 反而会使我们的工作变慢，而不是变快。
- en: 'A parallel `map` will slow down our map and reduce workflow because it will
    force us to iterate over the dataset twice, incurring the associated costs of
    storing and retrieving data from memory. This happens because `map`, as we’ve
    mentioned before, is naturally lazy. It stores instructions; it doesn’t evaluate.
    That means that we don’t evaluate our lazy `map` until we’re in the `reduce` loop.
    Our parallel `map`, on the other hand, is eager: it evaluates immediately. This
    means that by the time we’re reducing, we’ve already looped through our data once
    ([figure 5.13](#ch05fig13)).'
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 并行 `map` 会减慢我们的 map 和 reduce 工作流程，因为它会迫使我们两次遍历数据集，从而产生存储和从内存中检索数据的关联成本。这是因为，正如我们之前提到的，`map`
    是天生的懒加载。它存储指令；它不进行评估。这意味着我们不会在 `reduce` 循环中评估我们的懒 `map`。另一方面，我们的并行 `map` 是急切的：它立即进行评估。这意味着当我们开始进行
    reduce 时，我们已经在数据上循环了一次 ([图 5.13](#ch05fig13))。
- en: Figure 5.13\. Using a parallel `map` can counterintuitively be slower than using
    a lazy `map` in map and reduce scenarios—we’ll want to choose the right combination
    of map and reduce for the best performance.
  id: totrans-1226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.13. 在 map 和 reduce 场景中，使用并行 `map` 可能会比使用懒 `map` 反而慢——我们希望选择 map 和 reduce
    的最佳组合以获得最佳性能。
- en: '![](05fig13_alt.jpg)'
  id: totrans-1227
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig13_alt.jpg)'
- en: That we’re prevented from using parallelization here is a pretty undesirable
    side effect. After all, one of the big reasons we’re exploring these techniques
    is that they’re supposed to be good for big datasets. If we can’t use parallelization,
    we can’t scale our processing with our data and we’ll ultimately be limited in
    the size of data we can use. Fortunately for us, we can always use parallelization
    at the `reduce` level instead of at the `map` level. We’ll take a look at that
    in the next chapter, on parallel `reduce`.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法使用并行化是一个相当不希望出现的副作用。毕竟，我们探索这些技术的一个重要原因就是它们应该对大数据集有益。如果我们不能使用并行化，我们就不能随着数据扩展我们的处理能力，我们最终将受到数据大小的限制。幸运的是，我们可以始终在
    `reduce` 层而不是在 `map` 层使用并行化。我们将在下一章，关于并行 `reduce` 的章节中探讨这一点。
- en: 5.7\. Exercises
  id: totrans-1229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7. 练习
- en: These exercises test your knowledge of `reduce` and accumulator functions and
    reinforce the material in this chapter.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习测试你对 `reduce` 和累加函数的了解，并加强本章的内容。
- en: 5.7.1\. Situations to use reduce
  id: totrans-1231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1. 使用 reduce 的情况
- en: The `reduce` function is a powerful and flexible tool. In which of the following
    situations would you use `reduce`, and in which should you use another tool we’ve
    covered in this book?
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce` 函数是一个强大且灵活的工具。在以下哪些情况下你会使用 `reduce`，而在哪些情况下你应该使用本书中介绍的其他工具？'
- en: You have a long sequence of words and you return only a sequence containing
    the letter *A*.
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一长串单词，你只返回包含字母 *A* 的序列。
- en: You have a sequence of users and you want to transform them into just their
    User ID number.
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一系列用户，你想要将他们转换成仅包含他们的用户 ID 号的序列。
- en: You have a series of users and you want to find the five who have purchased
    the most from you.
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一系列用户，你想要找到购买量最大的五个用户。
- en: You have a sequence of purchase orders and you want to find the average price
    of a purchase.
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一系列采购订单，你想要找到采购的平均价格。
- en: 5.7.2\. Lambda functions
  id: totrans-1237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2. Lambda 函数
- en: We can use lambda functions for simple functions that we are only planning on
    using once; however, there is no difference at bytecode level between these functions
    and normal Python functions. Replicate the following functions with lambda functions.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 lambda 函数来编写简单的函数，我们只计划使用一次；然而，在字节码级别上，这些函数与普通 Python 函数之间没有区别。用 lambda
    函数复制以下函数。
- en: '[PRE80]'
  id: totrans-1239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 5.7.3\. Largest numbers
  id: totrans-1240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.3\. 最大的数字
- en: In Python, we can use the `max` function to find the maximum value in a sequence
    and the `min` function to find the minimum value in a sequence. However, sometimes
    we don’t want just the largest or smallest value, we want the largest or smallest
    several values. Use `reduce` to write a function that gets the five largest (or
    smallest) values from a sequence.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以使用 `max` 函数在序列中找到最大值，以及 `min` 函数在序列中找到最小值。然而，有时我们不仅想要最大或最小的值，我们想要最大或最小的几个值。使用
    `reduce` 编写一个函数，从序列中获取五个最大（或最小）的值。
- en: Once you have it written, try extending the function to collect the largest
    (or smallest) N values.
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦编写完成，尝试扩展函数以收集最大（或最小）的 N 个值。
- en: Example
  id: totrans-1243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE81]'
  id: totrans-1244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 5.7.4\. Group words by length
  id: totrans-1245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.4\. 按长度分组单词
- en: '*Group by* is a useful reduction where we take the elements of a sequence and
    group them based on the results of some function applied to them. Use `reduce`
    to write a version of this function that can group words based on their length.'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: '*按组分组* 是一种有用的归约，其中我们根据对序列元素应用的一些函数的结果将它们分组。使用 `reduce` 编写此函数的版本，可以根据单词的长度对单词进行分组。'
- en: Example
  id: totrans-1247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE82]'
  id: totrans-1248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Summary
  id: totrans-1249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The `reduce` function accumulates a sequence of data (N) into something else
    (X), with the help of an accumulator function and an initializer.
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce` 函数通过累加器函数和初始化器将一系列数据（N）累积到其他东西（X）中。'
- en: 'Accumulator functions take two variables: one for the accumulated data (often
    designated as `acc`, `left`, or `a`), and one for the next element in the sequence
    (designated `nxt`, `right`, or `b`).'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累加器函数接受两个变量：一个用于累积数据（通常指定为 `acc`、`left` 或 `a`），另一个用于序列中的下一个元素（指定为 `nxt`、`right`
    或 `b`）。
- en: '`reduce` is useful in situations where you have a sequence of data and want
    something other than a sequence back.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce` 在你有数据序列并想要返回序列之外的内容的情况下非常有用。'
- en: '`reduce`’s behavior is heavily customizable based on the accumulator function
    we pass to it.'
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce` 的行为可以根据我们传递给它的累加器函数进行高度定制。'
- en: Anonymous lambda functions can be useful when our accumulation function is concise,
    clear, and unlikely to be reused.
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匿名 lambda 函数在累加器函数简洁、清晰且不太可能被重用时非常有用。
- en: We can use `map` and `reduce` together to break complex transformations up into
    small contingent parts.
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将 `map` 和 `reduce` 结合起来，将复杂的转换分解成小的依赖部分。
- en: '`map`, counterintuitively, provides better performance than parallel `map`
    when we’re using both `map` and `reduce`.'
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`，出人意料地，在同时使用 `map` 和 `reduce` 时提供了比并行 `map` 更好的性能。'
- en: Chapter 6\. Speeding up map and reduce with advanced parallelization
  id: totrans-1257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 6 章\. 通过高级并行化加速 map 和 reduce
- en: '*This chapter covers*'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Advanced parallelization with `map` and `starmap`
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 和 `starmap` 的高级并行化
- en: Writing parallel `reduce` and `map reduce` patterns
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写并行 `reduce` 和 `map reduce` 模式
- en: Accumulation and combination functions
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累加和组合函数
- en: 'We ended [chapter 5](kindle_split_014.html#ch05) with a paradoxical situation:
    using a parallel method and more compute resources was slower than a linear approach
    with fewer compute resources. Intuitively, we know this is wrong. If we’re using
    more resources, we should at the very least be as fast as our low-resource effort—hopefully
    we’re faster. We never want to be slower.'
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 5 章](kindle_split_014.html#ch05) 结束时遇到了一个矛盾的情况：使用并行方法和更多的计算资源比使用较少计算资源的线性方法要慢。直观上，我们知道这是错误的。如果我们使用更多的资源，我们至少应该和我们的低资源工作一样快——希望我们更快。我们永远不希望更慢。
- en: 'In this chapter, we’ll take a look at how to get the most out of parallelization
    in two ways:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种方法来最大限度地发挥并行化的优势：
- en: By optimizing our use of parallel `map`
  id: totrans-1264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过优化我们的并行 `map` 使用
- en: By using a parallel `reduce`
  id: totrans-1265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用并行 `reduce`
- en: Parallel `map`, which I introduced in [section 2.2](kindle_split_011.html#ch02lev1sec2),
    is a great technique for transforming a large amount of data quickly. However,
    we did gloss over some nuances when we were learning the basics. We’ll dig into
    those nuances in this chapter. Parallel `reduce` is parallelization that occurs
    at the `reduce` step of our map and reduce pattern. That is, we’ve already called
    `map`, and now we’re ready to accumulate the results of all those transformations.
    With parallel `reduce`, we use parallelization in the accumulation process instead
    of the transformation process.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 并行 `map`，我在[第2.2节](kindle_split_011.html#ch02lev1sec2)中介绍过，是一种快速转换大量数据的优秀技术。然而，当我们学习基础知识时，我们确实忽略了一些细微差别。我们将在本章深入探讨这些细微差别。并行
    `reduce` 是在 `map` 和 `reduce` 模式的 `reduce` 步骤中发生的并行化。也就是说，我们已经调用了 `map`，现在我们准备累积所有这些转换的结果。使用并行
    `reduce`，我们在累积过程中使用并行化，而不是在转换过程中。
- en: 6.1\. Getting the most out of parallel map
  id: totrans-1267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 充分利用并行 map
- en: 'Back in [chapter 2](kindle_split_011.html#ch02), when we introduced parallel
    `map`, we covered a few of its shortfalls:'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第2章](kindle_split_011.html#ch02)，当我们介绍并行 `map` 时，我们讨论了它的一些不足：
- en: Python’s parallel `map` uses pickling, a method of saving Python objects to
    the disk, to share work; this causes problems when working with some data types.
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的并行 `map` 使用序列化，一种将 Python 对象保存到磁盘的方法，来共享工作；这在使用某些数据类型时会导致问题。
- en: Parallel `map` sometimes can result in unintended consequences when we’re working
    with stateful objects, such as classes.
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们处理具有状态的对象，如类时，并行 `map` 有时会导致意想不到的后果。
- en: The results of a parallel `map` operation are not always evaluated in the order
    that we would expect.
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行 `map` 操作的结果并不总是按照我们预期的顺序进行评估。
- en: Ultimately, however, we concluded that there were more situations in which we
    could live with those constraints than those in which we couldn’t. Indeed, up
    until [chapter 5](kindle_split_014.html#ch05), we hadn’t seen a scenario where
    we needed to worry about parallel `map`. And then we came across the first of
    two situations where parallel `map` is slower than the lazy `map`. Parallel `map`
    will be slower than lazy `map` when
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最终我们得出结论，我们可以忍受这些约束的情况比不能忍受的情况要多。确实，直到[第5章](kindle_split_014.html#ch05)，我们还没有看到需要担心并行
    `map` 的场景。然后我们遇到了第一个情况，其中并行 `map` 比懒 `map` 慢。当
- en: we’re going to iterate through the sequence a second time later in our workflow
  id: totrans-1273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在工作流程的稍后阶段再次遍历序列
- en: the size of the work done in each parallel instance is small compared to the
    overhead that parallelization imposes
  id: totrans-1274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个并行实例完成的工作量与并行化带来的开销相比很小
- en: In the first situation, when we’re going to iterate through the sequence a second
    time—that is, we’re going to `map` over a sequence and then do something later
    with all of its elements—using lazy `map` allows us to sidestep the first iteration.
    Instead of iterating through our sequence to transform all the elements, with
    lazy `map` we can perform the transformations in what would have been the second
    iteration. We visualized this in [figure 5.13](kindle_split_014.html#ch05fig13),
    shown again in [figure 6.1](#ch06fig01).
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，当我们打算第二次遍历序列时——也就是说，我们将对序列进行 `map` 操作，然后稍后处理其所有元素——使用懒 `map` 允许我们跳过第一次迭代。使用懒
    `map`，我们可以在原本的第二次迭代中执行转换，而不是遍历我们的序列以转换所有元素。我们在[图5.13](kindle_split_014.html#ch05fig13)中可视化了这一点，再次在[图6.1](#ch06fig01)中展示。
- en: '[Figure 6.1](#ch06fig01) shows how the lazy `map` outputs a lazy `map` object,
    no iteration involved, whereas the parallel `map` iterates through the entire
    sequence. We’ll look at solving this problem using parallel `reduce` in [section
    6.2](#ch06lev1sec2).'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.1](#ch06fig01) 展示了懒 `map` 输出懒 `map` 对象，没有迭代过程，而并行 `map` 遍历整个序列。我们将在[第6.2节](#ch06lev1sec2)中探讨使用并行
    `reduce` 解决这个问题。 '
- en: Figure 6.1\. Lazy `map` can be faster than parallel `map` when we’ll follow
    up our `map` statement by iterating over the results.
  id: totrans-1277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1\. 当我们打算在 `map` 语句之后遍历结果时，懒 `map` 可能比并行 `map` 更快。
- en: '![](06fig01_alt.jpg)'
  id: totrans-1278
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig01_alt.jpg)'
- en: 6.1.1\. Chunk sizes and getting the most out of parallel map
  id: totrans-1279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 数据块大小和充分利用并行 map
- en: The second situation—when the sequence is split into a large number of chunks
    whose overhead is large compared to the amount of work being done on those chunks—is
    one we haven’t encountered yet. In these instances, parallel `map` will be slower
    than lazy because we’re adding overhead to the task.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况——当序列被分成大量大块，其开销与在这些块上执行的工作量相比很大时——是我们尚未遇到的情况。在这些情况下，并行`map`将比懒加载慢，因为我们正在向任务添加开销。
- en: If we imagine our programs as a software project, we can imagine parallelization
    as the contractor. The contractor wants to get the job done with as few workers
    as possible because every new worker added requires the contractor to explain
    the task to them (which costs time) and pay them (which costs money). Around the
    margins, this might not matter. But if the contractor has workers sitting around
    not doing work but getting paid, or they’re spending so much time explaining the
    project to new workers that they can’t oversee it, the contractor would be better
    off with a smaller team.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将我们的程序想象成一个软件项目，我们可以将并行化想象成承包商。承包商希望用尽可能少的工人来完成工作，因为每个新工人加入都需要承包商向他们解释任务（这需要时间）并支付他们工资（这需要金钱）。在边缘上，这可能无关紧要。但如果有工人闲着却领工资，或者他们花了太多时间向新工人解释项目以至于无法监督他们，承包商可能更愿意拥有一个更小的团队。
- en: The same is true for our parallel processing. For example, imagine we have 100
    seconds of work to do, and each time we add a new parallel worker, we need to
    spend 1 second communicating with that worker. If we have
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于我们的并行处理。例如，假设我们有100秒的工作要做，每次我们添加一个新并行工人，我们都需要花费1秒与该工人通信。如果我们有
- en: 2 workers working 50 seconds each, we can get the job done in 52 seconds
  id: totrans-1283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2个工人每人工作50秒，我们可以在52秒内完成工作
- en: 4 workers working 25 seconds each, we can get the job done in 29 seconds
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4个工人每人工作25秒，我们可以在29秒内完成工作
- en: 25 workers working 4 seconds each, we’ll complete the task in 29 seconds
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 25个工人每人工作4秒，我们将在29秒内完成任务
- en: 100 workers working 1 second each, we’ll take 101 seconds
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个工人每人工作1秒，我们将花费101秒
- en: After a point, the amount of work being done is too small to justify the cost
    of communicating it. We need to ensure that when we assign work to our parallel
    jobs, we’re assigning enough work that the processors spend a large enough amount
    of time doing the work to justify taking the time to communicate it to them. The
    way we do that is by specifying a *chunk size*.
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个点上，进行的工作量太小，不足以证明传递它的成本。我们需要确保当我们分配工作给我们的并行作业时，我们分配的工作量足够大，使得处理器有足够的时间进行工作，从而证明将它们传递给它们的时间是合理的。我们这样做是通过指定一个*块大小*。
- en: Chunk size refers to the size of the different pieces into which we break our
    tasks for parallel processing. Larger chunk size tasks will require the processors
    to spend more time working on them, whereas smaller chunk size tasks will be finished
    more quickly.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小指的是我们将任务分解成并行处理的不同块的大小。较大的块大小任务将需要处理器花费更多时间来处理它们，而较小的块大小任务将更快完成。
- en: '|  |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'It’s ideal to pick a chunk size that’s large—we’ll learn how to pick the right
    size later in this chapter—but that still allows all the processors to finish
    their final task at approximately the same time. If we choose a chunk size that’s
    too small, we end up in the situation described at the beginning of the chapter:
    communicating the instructions takes longer than processing our jobs. If we choose
    a chunk size that’s too large, we’ll end up in a position where only one processor
    is working the final chunk, while the others are waiting.'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个大的块大小是理想的——我们将在本章后面学习如何选择正确的大小——但仍然允许所有处理器在大约相同的时间内完成最终任务。如果我们选择一个过小的块大小，我们就会陷入本章开头描述的情况：传递指令的时间比处理我们的工作的时间更长。如果我们选择一个过大的块大小，我们最终会处于一个位置，只有一个处理器在处理最终的块，而其他处理器在等待。
- en: '|  |'
  id: totrans-1292
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We can intuitively understand these limit behaviors by thinking about their
    extremes. If we ask each of our processors to handle only a single element at
    a time, we then have to
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过思考它们的极限情况来直观地理解这些极限行为。如果我们要求我们的每个处理器一次只处理一个元素，那么我们就需要
- en: transfer that element and the instructions for processing it,
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将该元素及其处理指令
- en: process it,
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理它，
- en: and transfer that element back.
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并将该元素传回。
- en: Then we have to repeat those steps for every single element. Assuming a reasonable-
    sized task, this is certainly more work than just processing each element one-by-one.
    In linear processing, we don’t have the added communication steps we have in parallel
    processing.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们必须为每个单独的元素重复这些步骤。假设一个合理大小的任务，这肯定比逐个处理每个元素要费时得多。在线性处理中，我们没有并行处理中存在的额外通信步骤。
- en: For the large chunk size problem, it helps to first think about an infinitely
    large chunk size. Well, that’s the same as using just a single processor, because
    we’ll only have one chunk. If our chunk size is half the size of our sequence,
    we’ll only be using two processors. If it’s a third of our sequence, we’ll only
    use three. It may seem like this might not be a problem, especially if we have
    a computer with only a few processors, but think about what happens when our second
    processor gets all the easy work and the first processor gets all the hard work.
    Our first processor will continue to work long after the second processor has
    stopped.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大块大小问题，首先考虑无限大的块大小是有帮助的。嗯，这相当于只使用单个处理器，因为我们只有一个块。如果我们的块大小是序列大小的一半，我们只会使用两个处理器。如果它是序列大小的三分之一，我们只会使用三个。这看起来可能不是问题，特别是如果我们只有几个处理器的计算机，但想想当第二个处理器完成所有简单的工作而第一个处理器完成所有困难的工作时会发生什么。第一个处理器将在第二个处理器停止工作后继续工作。
- en: The optimal chunk size is somewhere in between these two extremes. Unfortunately,
    beyond this general notion that chunking too small and chunking too large are
    bad, giving advice about specific chunk sizes is hard. The very reason why Python
    makes `chunksize` available as an option is because we’ll want to vary it according
    to the task at hand. I recommend starting with the default value, then increasing
    your chunk size until you see runtime start to decrease.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳块大小位于这两个极端之间。不幸的是，除了这个普遍的观念，即块太小和块太大都不好，给出具体的块大小建议是困难的。正是出于这个原因，Python将`chunksize`作为选项提供，因为我们希望根据任务的不同而调整它。我建议从默认值开始，然后逐渐增加块大小，直到你看到运行时间开始下降。
- en: 6.1.2\. Parallel map runtime with variable sequence and chunk size
  id: totrans-1300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 变量序列和块大小的并行映射运行时间
- en: Now that we know more about chunk size and differences in the behavior of parallel
    `map` and lazy `map`, let’s look at some code. We’ll start by seeing how lazy
    and parallel `map` behave over different-sized sequences, and how, for simple
    operations on small data, there’s really no benefit to parallelization. Then we’ll
    test out parallel `map` with a few different chunk sizes and see how that impacts
    our performance.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对块大小和并行`map`与惰性`map`的行为差异有了更多的了解，让我们看看一些代码。我们将从观察惰性和并行`map`在不同大小的序列上的行为开始，以及对于简单的操作和少量数据，实际上并行化并没有带来任何好处。然后我们将测试不同块大小的并行`map`，看看这对我们的性能有何影响。
- en: Sequence size and parallel map runtime
  id: totrans-1302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 序列大小和并行映射运行时间
- en: What’s the optimal size at which we should start thinking about parallelization?
    Well, a lot of that depends on how complex our task is.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在多大的时候开始考虑并行化？嗯，这很大程度上取决于我们的任务有多复杂。
- en: '|  |'
  id: totrans-1304
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: When our tasks are complex, we benefit quickly from parallelization. When our
    tasks are simple, we benefit only when there’s a large amount of data.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的任务复杂时，我们很快就能从并行化中受益。当我们的任务简单时，只有在有大量数据的情况下我们才能从中受益。
- en: '|  |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Consider the example at the end of [chapter 2](kindle_split_011.html#ch02) when
    we were scraping data from the web and there was web-related latency with every
    request. In these situations, parallelization is almost always going to make sense.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到[第2章](kindle_split_011.html#ch02)结尾的例子，当我们从网络抓取数据时，每次请求都会有与网络相关的延迟。在这些情况下，并行化几乎总是有意义的。
- en: But what about when our tasks are small, such as doing arithmetic or calling
    methods of Python data types? Here, the situation is murky and depends on the
    size of the sequence. We can prove this to ourselves if we run a lazy `map` and
    a parallel `map`. The following listing shows how this can be done, using a `times_two`
    function as a simple operation and comparing parallel `map` and lazy `map` on
    sequences with between 1 and 1 million elements.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们的任务很小，比如进行算术运算或调用Python数据类型的方法时，情况又如何呢？在这种情况下，情况并不明朗，这取决于序列的大小。如果我们运行一个惰性`map`和一个并行`map`，我们就可以证明这一点。以下列表显示了如何进行这种比较，使用`times_two`函数作为简单的操作，并在包含1到100万元素之间的序列上比较并行`map`和惰性`map`。
- en: Listing 6.1\. Comparing parallel `map` and lazy `map` on different-sized sequences
  id: totrans-1310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1\. 比较不同大小的序列上的并行`map`和惰性`map`
- en: '[PRE83]'
  id: totrans-1311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: In the output of that code, we can see a pattern appear.
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 在该代码的输出中，我们可以看到一个模式的出现。
- en: '[PRE84]'
  id: totrans-1313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: For small sequence sizes or processes that complete quickly, not only is it
    not beneficial to use parallel `map`, it’s counterproductive. Lazy `map` is actually
    faster. However, when we start to notice that our code is taking a while to run—when
    we start facing delays of seconds or minutes—using parallel map is faster.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小的序列大小或快速完成的进程，不仅使用并行`map`没有好处，反而会适得其反。懒惰`map`实际上更快。然而，当我们开始注意到我们的代码运行缓慢——当我们开始面临秒或分钟的延迟时——使用并行映射会更快。
- en: Chunk size and parallel map runtime
  id: totrans-1315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 块大小和并行映射运行时间
- en: We can run the same experiment with chunk size as well. For this experiment,
    instead of varying the size of the sequence, we’ll hold the sequence constant
    and only vary the size of the chunks our parallelization approach uses. We’ll
    have to use a large enough sequence that we’ll see some variation, but not so
    long that we’ll be waiting forever for our results. Based on our previous experiment,
    about 10 million will do. The code for this experiment appears in the following
    listing.
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用块大小进行相同的实验。对于这个实验，我们不会改变序列的大小，而是保持序列不变，只改变并行化方法使用的块的大小。我们必须使用足够大的序列，以便我们看到一些变化，但不会太长，以至于我们永远等不到结果。根据我们之前的实验，大约1000万就足够了。这个实验的代码如下所示。
- en: Listing 6.2\. Comparing the effect of chunk size on parallel `map` runtime
  id: totrans-1317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2\. 比较块大小对并行`map`运行时间的影响
- en: '[PRE85]'
  id: totrans-1318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The results of this code appear in the following output snippet. We can see
    that for small chunk sizes, our runtime is high. This is because the amount of
    time spent on communicating between all the workers is high, relative to the performance
    gained. By splitting the problem up into too many pieces, we make it inefficient.
    With too large of a chunk size, though, we get the reverse problem: we’re not
    using enough workers to solve the problem efficiently. Most of the sizes in the
    middle, however, give us reasonably good performance when compared to the two
    extremes.'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的结果出现在以下输出片段中。我们可以看到，对于小的块大小，我们的运行时间很高。这是因为所有工作者之间通信所花费的时间相对于获得性能来说很高。通过将问题分成太多的部分，我们使其变得低效。然而，如果块大小太大，我们会遇到相反的问题：我们没有使用足够的工作者来有效地解决问题。然而，中间的大部分大小与两个极端相比，给出了相当合理的性能。
- en: '[PRE86]'
  id: totrans-1320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '6.1.3\. More parallel maps: .imap and starmap'
  id: totrans-1321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 更多并行映射：.imap和starmap
- en: 'We should be familiar with two more types of parallel maps in Python:'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该熟悉Python中另外两种类型的并行映射：
- en: '`.imap` for lazy(ish) parallel mapping'
  id: totrans-1323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.imap`用于懒惰的并行映射'
- en: '`starmap` for parallel mapping over sequences of `tuple`s'
  id: totrans-1324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`starmap`用于并行映射`tuple`序列'
- en: We can use the `.imap` method to work in parallel on very large sequences efficiently
    and `starmap` to work with complex iterables, especially those we’re likely to
    create using the `zip` function.
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`.imap`方法有效地在非常大的序列上并行工作，并使用`starmap`处理复杂的可迭代对象，特别是那些我们可能使用`zip`函数创建的对象。
- en: Using .imap and .imap_unordered for large sequences
  id: totrans-1326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用.imap和.imap_unordered处理大型序列
- en: We discussed the benefits to laziness in [chapter 4](kindle_split_013.html#ch04),
    and when working in parallel there’s no reason we have to give them up. If we
    want to be lazy and parallel, we can use the `.imap` and `.imap_unordered` methods
    of `Pool()`. These methods both return iterators instead of lists, as shown in
    the following listing. Other than that, `.imap` behaves just like parallel `map`.
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](kindle_split_013.html#ch04)讨论了懒惰的好处，并且当并行工作时，我们没有理由放弃它们。如果我们想既懒惰又并行，我们可以使用`Pool()`的`.imap`和`.imap_unordered`方法。这两个方法都返回迭代器而不是列表，如下面的列表所示。除此之外，`.imap`的行为就像并行`map`。
- en: Listing 6.3\. Variations of parallel `map`
  id: totrans-1328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3\. 并行`map`的变体
- en: '[PRE87]'
  id: totrans-1329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***1* Our standard parallel map returns a list.**'
  id: totrans-1330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的常规并行映射返回一个列表。**'
- en: '***2* Both lazy parallel maps return iterator objects.**'
  id: totrans-1331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 懒惰的并行映射都返回迭代器对象。**'
- en: '`.imap_unordered` behaves the same, except it doesn’t necessarily put the sequence
    in the right order for our iterator. That’s why it’s called unordered: the values
    are placed in the iterator in the exact order our processor processes them. When
    we’re dealing with big datasets, the laziness of these two methods can mean a
    big decrease in runtime for our programs.'
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: '`.imap_unordered`的行为相同，但它不一定将序列放入我们的迭代器的正确顺序。这就是为什么它被称为无序：值被放置在迭代器中的确切顺序与我们的处理器处理它们的顺序相同。当我们处理大型数据集时，这两种方法的懒惰性可以意味着我们的程序运行时间的显著减少。'
- en: Using starmap for working with zip in parallel
  id: totrans-1333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用starmap并行处理zip
- en: 'We’ve seen how useful `map` can be for transforming data and how we can use
    it in parallel to speed up operations on large datasets; however, `map` has a
    disappointing shortcoming: it can only be used on functions that take a single
    parameter. Sometimes, this isn’t enough. We’ll want to use functions that take
    two or more parameters. We can use `starmap` in those situations to get the same
    benefits.'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到 `map` 在转换数据方面是多么有用，以及我们如何可以使用它并行地加速大数据集上的操作；然而，`map` 有一个令人失望的缺点：它只能用于接受单个参数的函数。有时，这还不够。我们希望使用接受两个或更多参数的函数。在这些情况下，我们可以使用
    `starmap` 来获得相同的好处。
- en: The `starmap` function unpacks `tuple`s as positional parameters to the function
    with which we’re mapping, and we can use it as a lazy function (from `itertools.starmap`)
    or a parallel function (as a method of a `Pool()` object, typically `P.starmap`).
    If we `zip` two sequences together, as we learned how to do in [chapter 4](kindle_split_013.html#ch04),
    then we’ve got an iterable primed and ready to go for use with `starmap`.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: '`starmap` 函数将 `tuple` 解包为映射函数的位置参数，我们可以将其用作一个惰性函数（来自 `itertools.starmap`）或一个并行函数（作为
    `Pool()` 对象的方法，通常为 `P.starmap`）。如果我们像在[第4章](kindle_split_013.html#ch04)中学到的那样将两个序列一起
    `zip`，那么我们就得到了一个准备就绪的可迭代对象，可以与 `starmap` 一起使用。'
- en: For example, we might want to find the largest element at each position in two
    sequences. Instead of looping through the sequences and comparing them, we could
    `zip` the sequences together and `map` over them. [Listing 6.4](#ch06ex04) shows
    a comparison between these two methods. In the first, we use a list comprehension
    and an enumerate to compare the elements in the same places. In the second, with
    `starmap`, we `zip` together our parameters and then `map` the relevant function
    across them.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能希望在每个位置找到两个序列中的最大元素。而不是遍历序列并比较它们，我们可以将序列 `zip` 在一起并对它们进行 `map`。[列表6.4](#ch06ex04)展示了这两种方法的比较。在第一种方法中，我们使用列表推导式和
    `enumerate` 来比较相同位置上的元素。在第二种方法中，使用 `starmap`，我们将参数 `zip` 在一起，然后对它们上的相关函数进行 `map`。
- en: Listing 6.4\. Using `starmap` to use `map` with multiple variables
  id: totrans-1337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4\. 使用 `starmap` 来使用带有多个变量的 `map`
- en: '[PRE88]'
  id: totrans-1338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '***1* To use starmap, we need to import it from itertools.**'
  id: totrans-1339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 要使用 `starmap`，我们需要从 `itertools` 中导入它。**'
- en: '***2* First, let’s create some testing data.**'
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 首先，让我们创建一些测试数据。**'
- en: '***3* A list comprehension to show how this could be done without map**'
  id: totrans-1341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* 一个列表推导式来展示如何在不使用 `map` 的情况下完成这个操作**'
- en: '***4* Uses starmap and zip to achieve the same effect**'
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4* 使用 `starmap` 和 `zip` 实现相同的效果**'
- en: In addition to simplifying the code and bringing it into a pattern we’re familiar
    with by now, `starmap` brings along all the benefits we’ve grown to expect from
    `map`. Both `zip` and `starmap` are lazy, so we can work with big datasets with
    greater piece of mind that we’re only holding the data we need in memory. We can
    also quickly convert our `starmap` to work in parallel by making it a method call
    to a `Pool()` object.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简化代码并将其带入我们现在已经熟悉的模式之外，`starmap` 还带来了我们从 `map` 中期待的所有好处。`zip` 和 `starmap`
    都是惰性的，因此我们可以更加放心地处理大数据集，因为我们只保留内存中需要的数据。我们还可以通过将其作为 `Pool()` 对象的方法调用来快速将 `starmap`
    转换为并行工作。
- en: 6.2\. Solving the parallel map and reduce paradox
  id: totrans-1344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 解决并行映射和归约悖论
- en: 'At the end of [chapter 5](kindle_split_014.html#ch05), we noticed a problem—our
    parallel `map` and `reduce` was slower than our lazy `map` and `reduce`. Then
    in [section 6.1](#ch06lev1sec1), we explored the behavior of parallel `map` in
    more depth. Although that helps us understand the problem better, it doesn’t necessarily
    help us solve it. To solve the problem, we’ll have to do something different:
    use a parallel `reduce`. In this section, we’ll take a look at implementing parallel
    `reduce` to speed up our reduction operations.'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_014.html#ch05)的结尾，我们注意到一个问题——我们的并行 `map` 和 `reduce` 比我们的惰性
    `map` 和 `reduce` 慢。然后在[第6.1节](#ch06lev1sec1)中，我们更深入地探讨了并行 `map` 的行为。尽管这有助于我们更好地理解问题，但它并不一定有助于我们解决问题。为了解决问题，我们不得不做些不同的事情：使用并行
    `reduce`。在本节中，我们将探讨实现并行 `reduce` 以加快我们的归约操作。
- en: 6.2.1\. Parallel reduce for faster reductions
  id: totrans-1346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 并行归约以实现更快的归约
- en: The easiest way to think of parallel `reduce` is as a cross between our parallel
    `map` and our linear `reduce`. Parallel `reduce` will share the costs and benefits
    of parallel `map`, while having the signature of linear `reduce`. Just like with
    parallel `map`, parallel `reduce` will
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 将并行 `reduce` 思考为我们的并行 `map` 和线性 `reduce` 之间的交叉。并行 `reduce` 将共享并行 `map` 的成本和好处，同时具有线性
    `reduce` 的签名。就像并行 `map` 一样，并行 `reduce` 将
- en: break a problem up into chunks
  id: totrans-1348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将问题分解成块
- en: make no guarantees about order
  id: totrans-1349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不保证顺序
- en: need to pickle data
  id: totrans-1350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要序列化数据
- en: be finicky about stateful objects
  id: totrans-1351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对有状态的对象要挑剔
- en: run slower than its linear counterpart on small datasets
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在小数据集上比其线性对应物运行得更慢
- en: run faster than its linear counterpart on big datasets
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大数据集上比其线性对应物运行得更快
- en: Like linear `reduce`, parallel `reduce` will
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性`reduce`一样，并行`reduce`将
- en: require an accumulator function, some data, and an initial value
  id: totrans-1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个累积函数、一些数据和初始值
- en: perform N-to-X transformations
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行N到X的转换
- en: All things considered, we can use parallel `reduce` to solve the problem we
    faced at the end of [chapter 5](kindle_split_014.html#ch05). We can perform transformations
    and accumulate the results in a time-friendly way.
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，我们可以使用并行`reduce`来解决我们在[第5章](kindle_split_014.html#ch05)结尾遇到的问题。我们可以以时间友好的方式执行转换并累积结果。
- en: Breaking down the parallel reduce parameters
  id: totrans-1358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 拆分并行reduce参数
- en: 'When we first looked at `reduce` in [chapter 5](kindle_split_014.html#ch05),
    one of the graphics we looked at showed the parts of our `reduce` function. We
    saw that `reduce` had three parts:'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第5章](kindle_split_014.html#ch05)中首次查看`reduce`时，我们查看的其中一个图形显示了我们的`reduce`函数的各个部分。我们看到`reduce`有三个部分：
- en: An accumulation function
  id: totrans-1360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个累积函数
- en: A sequence
  id: totrans-1361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个序列
- en: An initializer value
  id: totrans-1362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个初始化值
- en: That figure, shown here again in [figure 6.2](#ch06fig02), lays out what we
    need to be able to use `reduce`. In comparison to `map`, `reduce` is a little
    more complex—`map` has two parts, whereas `reduce` has three—but not overly so.
    Parallel `reduce` ups the ante.
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 那个图表，如图6.2所示，概述了我们能够使用`reduce`所需的内容。与`map`相比，`reduce`稍微复杂一些——`map`有两个部分，而`reduce`有三个，但并不过分复杂。并行`reduce`增加了难度。
- en: 'Figure 6.2\. The `reduce` function takes three parameters: an accumulator,
    a sequence, and an initial value.'
  id: totrans-1364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2。`reduce`函数有三个参数：一个累积器、一个序列和一个初始值。
- en: '![](06fig02_alt.jpg)'
  id: totrans-1365
  prefs: []
  type: TYPE_IMG
  zh: '![06fig02_alt.jpg](06fig02_alt.jpg)'
- en: 'The implementation of parallel `reduce` we’ll be looking at has six parts:'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的并行`reduce`实现有六个部分：
- en: An accumulation function
  id: totrans-1367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个累积函数
- en: A sequence
  id: totrans-1368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个序列
- en: An initializer value
  id: totrans-1369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个初始化值
- en: A `map`
  id: totrans-1370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`map`
- en: A `chunksize`
  id: totrans-1371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`chunksize`
- en: A combination function
  id: totrans-1372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个组合函数
- en: You should recognize most of these six parts, which are diagrammed in [figure
    6.3](#ch06fig03). The first three—the accumulation function, sequence, and initializer
    value—come directly from `reduce`. We just finished talking about `chunksize`
    in [section 6.1.2](#ch06lev2sec2). That leaves us with two new parameters, and
    even these two are only new-ish.
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能认出这六个部分中的大部分，它们在图6.3中有所展示。前三个——累积函数、序列和初始化值——直接来自`reduce`。我们刚刚在[第6.1.2节](#ch06lev2sec2)中谈到了`chunksize`。这留下了两个新参数，而且这两个参数也只是新近出现的。
- en: 'Figure 6.3\. Parallel `reduce` has six parameters: an accumulation function,
    a sequence, an initializer value, a map, a `chunksize`, and a combination function—three
    more than the standard `reduce` function.'
  id: totrans-1374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3。并行`reduce`有六个参数：一个累积函数、一个序列、一个初始化值、一个`map`、一个`chunksize`和一个组合函数——比标准`reduce`函数多三个。
- en: '![](06fig03_alt.jpg)'
  id: totrans-1375
  prefs: []
  type: TYPE_IMG
  zh: '![06fig03_alt.jpg](06fig03_alt.jpg)'
- en: 'The `map` parameter to parallel `reduce` is exactly what we would expect it
    to be, given its name: it’s a `map` function. The parallel `reduce` implementation
    we’ll use piggy-backs off the parallelism we implemented in our parallel `map`.
    That’s why our parallel `reduce` will share all of its benefits and drawbacks—a
    lot of the behavior is directly inherited.'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 并行`reduce`的`map`参数正是我们根据其名称所期望的：它是一个`map`函数。我们将使用的并行`reduce`实现将利用我们在并行`map`中实现的并行性。这就是为什么我们的并行`reduce`将共享其所有优点和缺点——大部分行为是直接继承的。
- en: That being said, we don’t have to pass our parallel `reduce` a parallel `map`.
    We are free to pass it a lazy `map`. For example, we could pass it the lazy `map`
    that comes standard with Python. If we do this, we won’t have a parallel `reduce`,
    we’ll have a lazy `reduce`. This is much less useful than a lazy `map`, however,
    because `reduce` only results in a single accumulated value—even if that value
    is a complex data structure—and we have to operate on the entire sequence to know
    what it is.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们不必将并行`reduce`传递一个并行`map`。我们可以自由地传递一个惰性`map`。例如，我们可以传递Python标准库中的惰性`map`。如果我们这样做，我们不会有一个并行的`reduce`，而是一个惰性`reduce`。然而，这比惰性`map`要少用得多，因为`reduce`只产生一个累积值——即使这个值是一个复杂的数据结构——而且我们必须在整个序列上操作才能知道它是什么。
- en: The last parameter is a combination function. The combination function is like
    an accumulation function, except for the parts of our parallel reduction problem.
    To understand how combination functions work, let’s take a look at the parallel
    `reduce` workflow in greater depth.
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个参数是一个组合函数。组合函数类似于累加函数，但针对我们的并行归约问题。为了理解组合函数是如何工作的，让我们更深入地看看并行`reduce`的工作流程。
- en: 6.2.2\. Combination functions and the parallel reduce workflow
  id: totrans-1379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 组合函数和并行归约工作流程
- en: Because parallel `reduce` is based on parallel `map`, the parallel `reduce`
    workflow has the same primary parts that our parallel `map` workflow does ([figure
    6.4](#ch06fig04)). We will
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: 因为并行`reduce`基于并行`map`，所以并行`reduce`工作流程具有与我们的并行`map`工作流程相同的主体部分([图6.4](#ch06fig04))。我们将
- en: break our problem into pieces
  id: totrans-1381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的问题分解成块
- en: do some work
  id: totrans-1382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一些工作
- en: combine the work
  id: totrans-1383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并工作
- en: return a result
  id: totrans-1384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个结果
- en: Figure 6.4\. Parallel `reduce` workflows involve doing one operation in parallel
    on chunks of our original sequence with an aggregation function and another operation
    on the data that results from the aggregation (combination).
  id: totrans-1385
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4\. 并行`reduce`工作流程涉及在原始序列的块上并行执行一个操作，使用聚合函数，并在聚合（组合）结果的数据上执行另一个操作。
- en: '![](06fig04_alt.jpg)'
  id: totrans-1386
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig04_alt.jpg)'
- en: 'For parallel `map`, we need to understand all of these steps, but most of our
    code writing effort will go into the second step: doing the work of transforming
    our data. In some situations—when we’re specifying the chunk size—we’ll be concerning
    ourselves with the first step as well: breaking the problem into pieces. With
    parallel `reduce`, we also need to consider the third step: combining the work.
    This is where our combination function comes into play.'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: 对于并行`map`，我们需要理解所有这些步骤，但大部分的代码编写工作将投入到第二步：执行将我们的数据转换的工作。在某些情况下——当我们指定块大小时——我们也会关注第一步：将问题分解成块。对于并行`reduce`，我们还需要考虑第三步：合并工作。这正是我们的组合函数发挥作用的地方。
- en: The implicit combination function in parallel map
  id: totrans-1388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 并行`map`中的隐式组合函数
- en: In parallel `map`, we don’t need to call a combination function because the
    data is always joined in the same way. As a result, the combination function is
    hardcoded into the parallel `map` operation itself. Because `map` is performing
    an N-to-N transformation of data—a concept introduced in [chapter 2](kindle_split_011.html#ch02),
    which describes how `map` transforms sequences into sequences of the same size
    with different elements—we know that our combination function will always be some
    form of adding two sequences together.
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行`map`中，我们不需要调用组合函数，因为数据总是以相同的方式连接。因此，组合函数被硬编码到并行`map`操作本身中。因为`map`执行的是N到N的数据转换——这是一个在第二章中引入的概念，它描述了`map`如何将序列转换成具有不同元素的相同大小的序列——我们知道我们的组合函数将始终是某种将两个序列相加的形式。
- en: For any two pieces of work that our parallel `map` function completes, the master
    can reassemble those pieces by combining them in the right order. The piece that
    corresponds to the earlier elements of the sequence goes first, and the piece
    that corresponds to the later elements of the sequence goes next. We can imagine
    this function as both the image in [figure 6.5](#ch06fig05) and the code in the
    following listing.
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的并行`map`函数完成的任何两个工作块，主节点可以通过以正确的顺序合并这些块来重新组装这些块。对应于序列中较早元素的块先出现，对应于序列中较晚元素的块随后出现。我们可以将这个函数想象成[图6.5](#ch06fig05)中的图像和以下列表中的代码。
- en: Listing 6.5\. The implicit combination function in parallel `map`
  id: totrans-1391
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5\. 并行`map`中的隐式组合函数
- en: '[PRE89]'
  id: totrans-1392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '***1* Notice how the signature of the function looks like the signature of
    our accumulators—it takes a left and a right object and returns an object of the
    same type as the left.**'
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 注意函数的签名看起来像我们的累加器的签名——它接受一个左对象和一个右对象，并返回与左对象相同类型的对象。**'
- en: 'In [listing 6.5](#ch06ex05), we can see what a `map` combination function would
    look like if we had to write it ourselves. We can imagine that two sequences—in
    this case `xs` and `ys`—are the parts returned by our parallel `map` operation,
    and we can use the `map_combination` function to combine them. We also see that
    the `map_combination` function is similar to an accumulation function. We’re even
    using two of the variant parameter names for accumulation functions: left and
    right.'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表6.5](#ch06ex05)中，我们可以看到如果我们自己编写它，一个`map`组合函数会是什么样子。我们可以想象，两个序列——在这种情况下是`xs`和`ys`——是我们并行`map`操作返回的部分，我们可以使用`map_combination`函数来组合它们。我们还看到，`map_combination`函数类似于一个累积函数。我们甚至使用了累积函数的两个变体参数名称：左和右。
- en: Figure 6.5\. A parallel `reduce` summation workflow is a simple case where we
    have the same function for the accumulation step and combination step.
  id: totrans-1395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5\. 并行`reduce`求和工作流程是一个简单的例子，其中我们在累积步骤和组合步骤中使用了相同的函数。
- en: '![](06fig05_alt.jpg)'
  id: totrans-1396
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig05_alt.jpg)'
- en: Custom combination functions for parallel reduce
  id: totrans-1397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 并行`reduce`的自定义组合函数
- en: 'With parallel `reduce`, however, we trade the simplicity of always having the
    same combination function for the flexibility of more possible transformations.
    Let’s consider three cases and see how we would handle the combination function
    in each case:'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用并行`reduce`，我们以更多可能变换的灵活性为代价，换取了总是有相同组合函数的简单性。让我们考虑三种情况，看看我们如何在每种情况下处理组合函数：
- en: Summation
  id: totrans-1399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求和
- en: '`filter`'
  id: totrans-1400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`frequencies`'
  id: totrans-1401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`频率`'
- en: We implemented summation with `reduce` in [section 5.2](kindle_split_014.html#ch05lev1sec2)—the
    purpose of this function is to add a sequence of numbers. When we use `reduce`
    for summation, we accumulate a partial sum and continuously add new values to
    this partial sum until there are no more elements in our sequence. Combining this
    with our parallel workflow, we get a process that looks like [figure 6.5](#ch06fig05).
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5.2节](kindle_split_014.html#ch05lev1sec2)中实现了使用`reduce`进行求和——这个函数的目的是将一系列数字相加。当我们使用`reduce`进行求和时，我们累积一个部分和，并持续将新值添加到这个部分和中，直到我们的序列中没有更多元素。结合我们的并行工作流程，我们得到一个类似于[图6.5](#ch06fig05)的过程。
- en: 'The process follows the basic parallel workflow steps we outlined at the beginning
    of [section 6.2.2](#ch06lev2sec5). We first break the problem into pieces, turning
    our sequence into several smaller sequences, then do some work:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程遵循我们在[第6.2.2节](#ch06lev2sec5)开头概述的基本并行工作流程步骤。我们首先将问题分解成几部分，将我们的序列转换成几个较小的序列，然后进行一些工作：
- en: First, we sum each of the smaller sequences.
  id: totrans-1404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们求出每个较小序列的总和。
- en: Then, we combine our results. Combining the partial sums requires us to take
    the sum of sums.
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们组合我们的结果。组合部分和需要我们取和的和。
- en: Finally, we can return this value as our result.
  id: totrans-1406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们可以将这个值作为我们的结果返回。
- en: 'In summation, we get lucky because the combination function is the same as
    the accumulation function. The accumulation function takes two values—both of
    which are numerical—adds them together, and returns their result to get an intermediary
    sum. Combining our subsequence sums is the same process: we add together pairs
    of the sums, each of which is a numerical value. The following listing approximates
    this process and shows how we can use the accumulation function to our `reduce`
    again to combine our partial results.'
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: 在求和过程中，我们很幸运，因为组合函数与累积函数相同。累积函数接受两个值——都是数值——将它们相加，并返回它们的和以得到一个中间和。组合我们的子序列和是一个相同的过程：我们将和的成对数值相加。以下列表近似这个过程，并展示了我们如何使用累积函数再次对`reduce`进行组合以合并我们的部分结果。
- en: Listing 6.6\. Approximation of parallel `reduce` summation
  id: totrans-1408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6\. 并行`reduce`求和的近似
- en: '[PRE90]'
  id: totrans-1409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '***1* Our accumulation function is simple addition.**'
  id: totrans-1410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的累积函数是简单的加法。**'
- en: '***2* We break our long sequence into three parts.**'
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 我们将长序列分成三部分。**'
- en: '***3* We work each of those parts independently.**'
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 我们独立地处理这些部分。**'
- en: '***4* Then, finally, we combine those parts—notice how we use my_add in both
    of the final two steps.**'
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 最后，我们将这些部分组合起来——注意我们在最后两个步骤中都使用了my_add。**'
- en: We’re not always lucky enough that we get to use the same function, however.
    Next, we’ll explore the parallel `reduce` workflow for the `filter` function.
    We first saw `filter` in [chapter 4](kindle_split_013.html#ch04), and we implemented
    a `reduce`-based version of it in [section 5.3.1](kindle_split_014.html#ch05lev2sec4).
    The idea behind `filter` is that we have a large sequence and we want to create
    a subsequence that contains only the elements of that sequence that cause a function
    to return `True`.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不总是幸运到可以使用同一个函数。接下来，我们将探索`filter`函数的并行`reduce`工作流程。我们第一次在[第4章](kindle_split_013.html#ch04)中看到`filter`，并在[第5.3.1节](kindle_split_014.html#ch05lev2sec4)中实现了基于`reduce`的版本。`filter`背后的想法是我们有一个大序列，我们想要创建一个子序列，它只包含导致函数返回`True`的该序列的元素。
- en: Our standard `filter` workflow is to start with an empty sequence and move through
    our sequence element by element, adding only the elements that make our condition
    function return `True` to our accumulated sequence. To make this parallel, we
    will
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们标准的`filter`工作流程是从一个空序列开始，逐个元素地通过我们的序列，只将使我们的条件函数返回`True`的元素添加到累积序列中。为了使其并行，我们将
- en: break our sequence into smaller sequences
  id: totrans-1416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的序列拆分成更小的序列
- en: accumulate the elements of those small sequences that make our condition return
    `True` in a new sequence
  id: totrans-1417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个新序列中累积那些使我们的条件返回`True`的小序列的元素
- en: join those new sequences together
  id: totrans-1418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些新序列连接起来
- en: return the composite sequence
  id: totrans-1419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回组合序列
- en: We can see this entire process in [figure 6.6](#ch06fig06). Notice that the
    function for step 2, which takes a sequence and produces a subsequence, is different
    from our function for step 3, which joins the sequences together. The function
    for taking sequences and returning subsequences is our accumulation function for
    our `filter` reduction from [chapter 5](kindle_split_014.html#ch05). The function
    for joining the sequences is actually the implicit combination function from `map`.
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图6.6](#ch06fig06)中看到整个流程。注意，步骤2的函数，它接受一个序列并生成一个子序列，与步骤3的函数不同，该函数将序列连接起来。获取序列并返回子序列的函数是我们在[第5章](kindle_split_014.html#ch05)中`filter`归约的累积函数。连接序列的函数实际上是`map`的隐式组合函数。
- en: Figure 6.6\. In our workflow for the parallel `filter`, we need to use a different
    function for our accumulation step than for the combination step. This makes the
    operation more complex than our parallel summation.
  id: totrans-1421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 在我们的并行`filter`工作流程中，我们需要在累积步骤和组合步骤中使用不同的函数。这使得操作比我们的并行求和更复杂。
- en: '![](06fig06_alt.jpg)'
  id: totrans-1422
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig06_alt.jpg)'
- en: We can modify our example approximating parallel summation from [listing 6.6](#ch06ex06)
    to approximate a parallel `filter` to see this in action. First, we’ll have to
    create a new accumulation function. Here we’ll use the `keep_if_even` function
    we wrote in [section 5.3.1](kindle_split_014.html#ch05lev2sec4). We’ll also need
    to add a combination function. Because we already identified this function to
    be the same function from parallel `map`’s implicit combination step, let’s use
    the function we wrote in [listing 6.5](#ch06ex05). We can see the combination
    of the two, approximating a `filter` function using parallel `reduce`, in the
    following listing.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将[列表6.6](#ch06ex06)中的示例近似并行求和修改为近似并行`filter`以观察其作用。首先，我们必须创建一个新的累积函数。在这里，我们将使用我们在[第5.3.1节](kindle_split_014.html#ch05lev2sec4)中编写的`keep_if_even`函数。我们还需要添加一个组合函数。因为我们已经确定这个函数与并行`map`的隐式组合步骤中的相同函数，所以让我们使用我们在[列表6.5](#ch06ex05)中编写的函数。我们可以在以下列表中看到这两个函数的组合，使用并行`reduce`近似`filter`函数。
- en: Listing 6.7\. Parallel `filter` using different accumulation and combination
    functions
  id: totrans-1424
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7\. 使用不同的累积和组合函数的并行`filter`
- en: '[PRE91]'
  id: totrans-1425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '***1* Creates our combination function**'
  id: totrans-1426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建我们的组合函数**'
- en: '***2* Creates our accumulation function from filter**'
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从`filter`创建我们的累积函数**'
- en: '***3* Assigns our accumulation and combination functions to differentiate them**'
  id: totrans-1428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 分配我们的累积和组合函数以区分它们**'
- en: '***4* Uses the accumulation function on our broken-up sequences, returning
    intermediate results**'
  id: totrans-1429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用我们拆分的序列上的累积函数，返回中间结果**'
- en: '***5* Uses our combination function on those results, returning a final result**'
  id: totrans-1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用我们的组合函数处理这些结果，返回最终结果**'
- en: In [listing 6.7](#ch06ex07), we can see that our accumulation function (represented
    by `f_acc`) and our combination function (represented by `f_com`) are different.
    Like we mentioned earlier, the accumulation function is `keep_if_even`, from [chapter
    5](kindle_split_014.html#ch05), and the combination function is `map_combination`
    from [listing 6.5](#ch06ex05). We need both of these functions to take our broken-up
    work and achieve the desired result.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.7](#ch06ex07) 中，我们可以看到我们的累积函数（用 `f_acc` 表示）和我们的组合函数（用 `f_com` 表示）是不同的。正如我们之前提到的，累积函数是来自
    [第 5 章](kindle_split_014.html#ch05) 的 `keep_if_even`，组合函数是来自 [列表 6.5](#ch06ex05)
    的 `map_combination`。我们需要这两个函数来处理我们拆分的工作并达到预期的结果。
- en: It’s important to notice that these functions expect different types of parameters.
    The `keep_if_even` function takes a list in first position and a numerical value
    in second position. The `map_combination` function expects lists in both positions.
    In our case with `filter`, we know that the accumulation step always results in
    a list, so our combination function takes two lists.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这些函数期望不同的参数类型。`keep_if_even` 函数接受一个列表作为第一个位置和一个数值作为第二个位置。`map_combination`
    函数期望两个位置都是列表。在我们的 `filter` 情况中，我们知道累积步骤总是产生一个列表，因此我们的组合函数接受两个列表。
- en: '|  |'
  id: totrans-1433
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: Combination functions always take two parameters of the same type because each
    parameter is the result of the same process.
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 组合函数总是接受两个相同类型的参数，因为每个参数都是同一过程的结果。
- en: '|  |'
  id: totrans-1436
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We can see this rule in our `frequencies` example as well. We first implemented
    the `frequencies` function, which returns a `dict` of elements and their counts
    when provided with a sequence, in [section 5.3.2](kindle_split_014.html#ch05lev2sec5).
    In its linear form, we went through each element of the sequence and incremented
    the count of each element by one every time we saw it. In parallel, we’re going
    to need to do four things:'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在我们的 `frequencies` 示例中看到这个规则。我们首先在 [第 5.3.2 节](kindle_split_014.html#ch05lev2sec5)
    实现了 `frequencies` 函数，该函数在提供序列时返回一个包含元素及其计数的 `dict`。在其线性形式中，我们遍历序列中的每个元素，每次遇到它时，将每个元素的计数增加一。在并行情况下，我们需要做四件事情：
- en: Break up our sequence into smaller sequences
  id: totrans-1438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的序列拆分成更小的序列
- en: Obtain counts from those smaller sequences
  id: totrans-1439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些较小的序列中获取计数
- en: Combine the counts together
  id: totrans-1440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计数合并在一起
- en: Return our combined counts
  id: totrans-1441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回我们的合并计数
- en: '[Figure 6.7](#ch06fig07) shows that, like `filter`, the `frequencies` process
    will use different functions for the accumulation and combination steps. For the
    accumulation step, we’ll use the `make_counts` function from [listing 5.7](kindle_split_014.html#ch05ex07).
    For the combination step, we’ll have to write an entirely new function. This function
    will have to go through the unique keys of our two `dict`s and add the values
    of those keys together in a new `dict`. We can see that even though our `frequencies`
    process can take iterables with any number of types of elements, we’ll always
    be passing `dict`s to our combination function because that’s the type that our
    `make_counts` accumulation function returns.'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.7](#ch06fig07) 显示，与 `filter` 类似，`frequencies` 处理过程将使用不同的函数进行累积和组合步骤。对于累积步骤，我们将使用来自
    [列表 5.7](kindle_split_014.html#ch05ex07) 的 `make_counts` 函数。对于组合步骤，我们必须编写一个全新的函数。这个函数必须遍历我们两个
    `dict` 的唯一键，并将这些键的值相加到一个新的 `dict` 中。我们可以看到，尽管我们的 `frequencies` 处理过程可以接受任何类型的可迭代对象，但我们总是将
    `dict` 传递给我们的组合函数，因为这是 `make_counts` 累积函数返回的类型。'
- en: Figure 6.7\. The parallel `frequencies` reduction workflow can take a number
    of types as its input, but it will always pass `dict`s into its combination step
    and return `dict`s as a result.
  id: totrans-1443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.7\. 并行 `frequencies` 减少工作流程可以接受多种类型的输入，但它将始终将 `dict` 传递到其组合步骤，并以 `dict`
    作为结果返回。
- en: '![](06fig07_alt.jpg)'
  id: totrans-1444
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig07_alt.jpg)'
- en: '[Listing 6.8](#ch06ex08) shows an approximation of the parallel `reduce` version
    of `filter`. We can see the original `make_counts` accumulation function and our
    new combination function, in the same general pattern we saw with both our summation
    example and our `filter` example. Again, we see one of the major benefits of adopting
    a map and reduce style: we can use the same patterns of programming to solve a
    diverse set of problems.'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.8](#ch06ex08) 展示了 `filter` 的并行 `reduce` 版本的近似实现。我们可以看到原始的 `make_counts`
    累积函数和我们的新组合函数，这与我们在求和示例和 `filter` 示例中看到的一般模式相同。再次强调，我们看到了采用 map 和 reduce 风格的一个主要好处：我们可以使用相同的编程模式来解决各种各样的问题。'
- en: Listing 6.8\. Approximating a parallel `reduce frequencies`
  id: totrans-1446
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.8\. 平行 `reduce frequencies` 的近似
- en: '[PRE92]'
  id: totrans-1447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '***1* Creates a unique sequence of keys by finding a set that represents the
    union of both sets of keys**'
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过找到一个表示两个键集合并集的集合来创建一个唯一的键序列**'
- en: '***2* Because dict keys are of the keys type, we’ll have to use explicit set
    conversion.**'
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 因为字典键是键类型，我们将不得不使用显式的集合转换。**'
- en: '***3* Loops through the keys and returns a dict mapping keys to the sum of
    its value in each dict**'
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历键并返回一个字典，将键映射到每个字典中值的总和**'
- en: '***4* The make_counts function is our old accumulator from [chapter 5](kindle_split_014.html#ch05).**'
  id: totrans-1451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* make_counts 函数是我们在[第 5 章](kindle_split_014.html#ch05)中的旧累加器。**'
- en: '***5* Assigns make_counts as the accumulation function and combine_counts as
    the combination function**'
  id: totrans-1452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将 make_counts 作为累加函数，combine_counts 作为组合函数**'
- en: '***6* Works on the split-up sequences using our accumulation functions**'
  id: totrans-1453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 使用我们的累加函数处理分割后的序列**'
- en: '***7* Combines the intermediate results using our combination function**'
  id: totrans-1454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 使用我们的组合函数合并中间结果**'
- en: We can see this reusable pattern in how similar [listings 6.7](#ch06ex07) and
    [6.8](#ch06ex08) are. Having abstracted the combination and accumulation into
    `f_acc` and `f_com`, all we needed to change to get from one to the other was
    how those functions resolve. Now that we’ve seen how summation, `filter`, and
    `frequencies` will work in parallel, let’s take a look at how we can actually
    implement these three functions with parallel `reduce`.
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在类似 [列表 6.7](#ch06ex07) 和 [6.8](#ch06ex08) 的相似性中看到这个可重用的模式。通过将组合和累加抽象为 `f_acc`
    和 `f_com`，我们需要的所有变化只是这些函数如何解决。现在我们已经看到求和、`filter` 和 `frequencies` 将如何在并行中工作，让我们看看我们如何实际上使用并行
    `reduce` 来实现这三个函数。
- en: 6.2.3\. Implementing parallel summation, filter, and frequencies with fold
  id: totrans-1456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 使用 fold 实现并行求和、过滤和频率
- en: So far in this chapter, we’ve looked at implementation nuances of parallelism.
    Specifically, we’ve looked at when we should use parallel workflows and how the
    parallel `reduce` workflow differs from the parallel `map` workflow with which
    we were already familiar. Now that we’ve got that down, we can finally solve the
    problem we noticed at the end of [chapter 5](kindle_split_014.html#ch05) of `reduce`
    working more slowly in parallel. We’re finally ready to use parallel `reduce`.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们探讨了并行化的实现细节。具体来说，我们探讨了何时应该使用并行工作流程，以及并行 `reduce` 工作流程与我们已经熟悉的并行
    `map` 工作流程有何不同。现在我们已经掌握了这些，我们最终可以解决我们在[第 5 章](kindle_split_014.html#ch05)末尾注意到的问题，即
    `reduce` 在并行中工作得更慢。我们终于准备好使用并行 `reduce`。
- en: 'Like our standard `map` and our parallel `map`, the moving from standard `reduce`
    to parallel `reduce` is a little anticlimactic. Assuming that we have our accumulation
    and combination functions in place, implementing parallel `reduce` requires only
    three steps:'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的标准 `map` 和并行 `map` 一样，从标准 `reduce` 转向并行 `reduce` 有点令人失望。假设我们已经有了我们的累加和组合函数，实现并行
    `reduce` 只需要三个步骤：
- en: Importing the proper classes and functions
  id: totrans-1459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入适当的类和函数
- en: Rounding up some processors
  id: totrans-1460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rounding up some processors
- en: Passing our `reduce` function the right helper functions and variables
  id: totrans-1461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将正确的辅助函数和变量传递给我们的 `reduce` 函数
- en: For the first of these three steps, we have to move beyond what base Python
    gives us. Python doesn’t natively support parallel `reduce`. One of the libraries
    we’ll need for this is the pathos library, which we discussed in [chapter 2](kindle_split_011.html#ch02)
    when we first introduced parallelism and discussed some problems related to pickling.
    We can use pathos to get around Python’s weaknesses in pickling and chunk up our
    problem up for parallel `reduce`.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这三个步骤中的第一个，我们必须超越 Python 基础提供的功能。Python 并没有原生支持并行 `reduce`。我们将需要的库之一是 pathos
    库，我们在介绍并行性和讨论与序列化相关的一些问题时讨论了它[第 2 章](kindle_split_011.html#ch02)。我们可以使用 pathos
    来克服 Python 在序列化方面的弱点，并将问题分割成块以进行并行 `reduce`。
- en: 'We’ll also need to reach into the toolz library for an implementation of parallel
    `reduce`. We used the toolz library before in [chapters 2](kindle_split_011.html#ch02)
    and [4](kindle_split_013.html#ch04) when we borrowed handy functions that fit
    the map and reduce style of programming. The parallel `reduce` implementation
    in the toolz library is called `fold`. `fold` is an alternative name for `reduce`,
    which is useful as a metaphor for `reduce`: folding each element into the accumulator,
    one at a time, until only the accumulator is left.'
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从 toolz 库中获取并行 `reduce` 的实现。我们在第 2 章 [chapters 2](kindle_split_011.html#ch02)
    和第 4 章 [chapters 4](kindle_split_013.html#ch04) 中使用过 toolz 库，当时我们借用了适合 map 和 reduce
    编程风格的便捷函数。toolz 库中的并行 `reduce` 实现称为 `fold`。`fold` 是 `reduce` 的另一个名称，它作为 `reduce`
    的隐喻很有用：逐个将每个元素折叠到累加器中，直到只剩下累加器。
- en: '|  |'
  id: totrans-1464
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The toolz library**'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: '**toolz 库**'
- en: The toolz library is intended to be the functional utility library that Python
    never came with. Many functional programming languages—Scala, Clojure, Haskell,
    and OCaml—come with handy utilities for common sequence transformation patterns.
    Python does not, and toolz fills in those convenience functions. A high-performance
    version of the library is available as CyToolz. You can install CyToolz with `pip
    install cytoolz`.
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: toolz 库旨在成为 Python 从未附带的功能性实用库。许多功能性编程语言——Scala、Clojure、Haskell 和 OCaml——都附带了一些方便的通用序列转换模式工具。Python
    没有，而 toolz 补充了这些便利函数。该库的高性能版本称为 CyToolz。您可以使用 `pip install cytoolz` 安装 CyToolz。
- en: '|  |'
  id: totrans-1467
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Once we have these imports, all we need to do is call `Pool` to round up some
    processors and call our parallel `reduce` with all the right parameters. With
    summation, for example, we’ll need to make our imports, call `Pool`, and pass
    our parallel `reduce` (`fold`) our addition function. We can see this all in action
    in the following listing.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这些导入，我们只需要调用 `Pool` 来汇总一些处理器，并使用所有正确的参数调用我们的并行 `reduce`（`fold`）。例如，对于求和，我们需要进行导入，调用
    `Pool`，并将我们的加法函数传递给并行 `reduce`（`fold`）。我们可以在以下列表中看到所有这些操作。
- en: Listing 6.9\. Summation in parallel with `reduce`
  id: totrans-1469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.9\. 使用 `reduce` 进行并行求和
- en: '[PRE93]'
  id: totrans-1470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '***1* We’ll need features of the dill, pathos, and toolz libraries to perform
    a parallel reduce.**'
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们将需要 dill、pathos 和 toolz 库的功能来执行并行 reduce。**'
- en: '***2* Creates our accumulation and combination function, which are the same
    for summation**'
  id: totrans-1472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建我们的累加和组合函数，这些函数对于求和是相同的**'
- en: '***3* Rounds up the processors we want to use**'
  id: totrans-1473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将我们想要使用的处理器数量汇总**'
- en: '***4* Passes the parameters to our parallel reduce function: fold**'
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将参数传递给我们的并行 reduce 函数：fold**'
- en: '***5* Includes a linear reduce for comparison**'
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 包含一个线性 reduce 以进行比较**'
- en: '[Listing 6.9](#ch06ex09) shows that, just like calling `map` in parallel versus
    calling our regular lazy `map`, calling `reduce` in parallel requires almost no
    modification to our base code. We need to import some capabilities that are not
    included in base Python, sure, but there are no substantial changes to the workflow.
    Importantly, we use exactly the same accumulation function in each case.'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.9](#ch06ex09) 显示，就像在并行调用 `map` 与调用我们常规的懒 `map` 相比，并行调用 `reduce` 几乎不需要修改我们的基础代码。当然，我们需要导入一些基础
    Python 中未包含的功能，但工作流程没有实质性的变化。重要的是，我们在每种情况下都使用完全相同的累加函数。'
- en: '|  |'
  id: totrans-1477
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: '[Listing 6.9](#ch06ex09) also shows how we call parallel `map` as a parameter
    to parallel `reduce`. This is because the parallel `reduce` implementation in
    the toolz library does not actually implement parallelism. This function has to
    sit on top of a parallel `map` to do its parallel magic. If we wanted to, we could
    pass our normal lazy `map` function to the `fold` function and we would get a
    linear `reduce` back. This can be useful if we’re testing our code on a small
    subset of a larger dataset because we can use the `fold` function without parallelism
    and then add the parallelism later when we’re working with a big dataset.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.9](#ch06ex09) 还显示了如何将并行 `map` 作为参数传递给并行 `reduce`。这是因为 toolz 库中的并行 `reduce`
    实现实际上并没有实现并行性。这个函数必须位于并行 `map` 之上才能执行其并行魔法。如果我们愿意，我们可以将我们的常规懒 `map` 函数传递给 `fold`
    函数，我们就会得到一个线性 `reduce`。如果我们只是在测试较大数据集的一个小子集时，这可能很有用，因为我们可以在处理大数据集时添加并行性，而无需并行性即可使用
    `fold` 函数。'
- en: '|  |'
  id: totrans-1480
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: For a parallel `filter`, we see that the process is mostly the same, except
    that now we need to add our combination function and an initializer. We can see
    this process in the following listing.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 对于并行 `filter`，我们看到过程基本上是相同的，但现在我们需要添加我们的组合函数和一个初始化器。我们可以在以下列表中看到这个过程。
- en: Listing 6.10\. `filter` in parallel with `reduce`
  id: totrans-1482
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.10\. 使用`reduce`并行实现`filter`
- en: '[PRE94]'
  id: totrans-1483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '***1* Our parallel reduce implementation requires the same imports as before.**'
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们并行`reduce`的实现需要与之前相同的导入。**'
- en: '***2* As in [listing 6.7](#ch06ex07), map_combination is our combination function.**'
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如[列表6.7](#ch06ex07)所示，map_combination是我们的组合函数。**'
- en: '***3* keep_if_even, from [chapter 5](kindle_split_014.html#ch05), is our accumulation
    function.**'
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* keep_if_even，来自[第5章](kindle_split_014.html#ch05)，是我们的累加函数。**'
- en: '***4* Notice the empty list being used as an initializer and the combination
    function map_combination.**'
  id: totrans-1487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 注意到用作初始化器的空列表和组合函数map_combination。**'
- en: '***5* Our standard reduce workflow for comparison**'
  id: totrans-1488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 我们的标准`reduce`工作流程进行比较**'
- en: '[Listing 6.10](#ch06ex10) shows how the parallel `filter` workflow incorporates
    the combination function and the initializer. Just like our linear `filter`, we
    put the initializer—an empty list—in third position. Again, we use an empty list
    for `filter` because we want to return a list. Also, we can see how the combination
    function is passed to our parallel `reduce` function in final position as a named
    parameter. This combination function and the parallel `map` parameter are the
    only things that distinguish our linear `reduce` from our parallel `reduce`.'
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.10](#ch06ex10)展示了并行`filter`工作流程如何结合组合函数和初始化器。就像我们的线性`filter`一样，我们将初始化器——一个空列表——放在第三个位置。再次，我们使用空列表作为`filter`，因为我们想返回一个列表。我们还可以看到组合函数是如何作为命名参数传递给我们的并行`reduce`函数的最终位置的。这个组合函数和并行`map`参数是区分我们的线性`reduce`和并行`reduce`的唯一因素。'
- en: We can see the same limited changes between linear and parallel `frequencies`,
    as shown in the following listing. Again, what’s important is that we pass the
    combination function and the parallel `map`.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到线性`frequencies`和并行`frequencies`之间相同的有限变化，如下面的列表所示。再次，重要的是我们传递了组合函数和并行`map`。
- en: Listing 6.11\. Implementing `frequencies` in parallel with parallel `reduce`
  id: totrans-1491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11\. 使用并行`reduce`并行实现`frequencies`
- en: '[PRE95]'
  id: totrans-1492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '***1* Uses the same three imports: dill, ProcessingPool, and fold**'
  id: totrans-1493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用相同的三个导入：dill、ProcessingPool和fold**'
- en: '***2* Implements choice to generate some example data**'
  id: totrans-1494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 实现了选择以生成一些示例数据**'
- en: '***3* combine_counts will be our combination function.**'
  id: totrans-1495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* combine_counts将作为我们的组合函数。**'
- en: '***4* make_counts will be our accumulation function.**'
  id: totrans-1496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* make_counts将作为我们的累加函数。**'
- en: '***5* Uses a generator expression to create a lot of dummy data**'
  id: totrans-1497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用生成器表达式创建大量虚拟数据**'
- en: '***6* Calls our parallel reduce on this data, passing our accumulation function,
    the data, a dict as an initializer, our parallel map, and the combination function**'
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 在此数据上调用我们的并行`reduce`，传递我们的累加函数、数据、作为初始化器的字典、我们的并行`map`和组合函数**'
- en: '***7* Includes a linear reduce for comparison**'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 包含一个线性`reduce`进行比较**'
- en: 'We can see again how similar the parallel `reduce` and the linear `reduce`
    are. With the key exception of the combination function and the parallel `map`,
    they both use the same parameters: the same accumulation function, the same data
    inputs, and the same initializer. Across all three examples—parallel `reduce`
    summation, parallel `reduce filter`, and parallel `reduce frequencies`—we have
    seen our combination functions increase in complexity. Getting this combination
    function right is the key to successfully using parallel `reduce`.'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次看到并行`reduce`和线性`reduce`是多么相似。除了组合函数和并行`map`之外，它们都使用相同的参数：相同的累加函数、相同的数据输入和相同的初始化器。在所有三个示例——并行`reduce`求和、并行`reduce`过滤和并行`reduce`频率——中，我们都看到了我们的组合函数复杂性增加。正确实现组合函数是成功使用并行`reduce`的关键。
- en: Summary
  id: totrans-1501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Sometimes, parallel `map` can be slower than a lazy `map`, especially when the
    amount of data is small or the work to be done is easy.
  id: totrans-1502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，并行`map`可能比懒`map`慢，尤其是在数据量小或要完成的工作简单时。
- en: There are several variations of `map`, such as `starmap` and `.imap`, that can
    be useful in the right situation.
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有几种`map`的变体，如`starmap`和`.imap`，在适当的情况下可能很有用。
- en: We can use parallel `reduce` in conjunction with lazy `map`s for a fast map
    and reduce workflow.
  id: totrans-1504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将并行`reduce`与懒`map`结合使用，以实现快速的map和reduce工作流程。
- en: 'Parallel `reduce` takes five parameters: an accumulator function, a sequence,
    an initializer, a parallel `map` function, and an optional combiner.'
  id: totrans-1505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行`reduce`需要五个参数：一个累加函数、一个序列、一个初始化器、一个并行`map`函数和一个可选的合并函数。
- en: The parallel `map` function tells parallel `reduce` how to split up the workload.
  id: totrans-1506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行`map`函数告诉并行`reduce`如何分配工作负载。
- en: The optional combiner tells `reduce` how to join chunks of work completed in
    parallel whose data type may be different from that of items in the sequence.
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选的合并器告诉 `reduce` 如何将并行完成的工作块连接起来，这些工作块的数据类型可能与序列中项的数据类型不同。
- en: To use parallel `reduce`, we need to design a combine function that can combine
    the different accumulated chunks.
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用并行 `reduce`，我们需要设计一个合并函数，该函数可以将不同的累积块合并在一起。
