- en: 10 Combining building blocks to gain more power: Neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 结合构建块以获得更多力量：神经网络
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: what is a neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是神经网络
- en: 'the architecture of a neural network: nodes, layers, depth, and activation
    functions'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的架构：节点、层、深度和激活函数
- en: training neural networks using backpropagation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播训练神经网络
- en: potential problems in training neural networks, such as the vanishing gradient
    problem and overfitting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络训练中可能遇到的问题，例如梯度消失问题和过拟合
- en: techniques to improve neural network training, such as regularization and dropout
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高神经网络训练的技术，例如正则化和dropout
- en: using Keras to train neural networks for sentiment analysis and image classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 训练用于情感分析和图像分类的神经网络
- en: using neural networks as regression models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络用作回归模型
- en: '![](../Images/10-unnumb-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-unnumb-1.png)'
- en: 'In this chapter, we learn *neural networks*, also called *multilayer perceptrons*.
    Neural networks are one of the most popular (if not the most popular) machine
    learning models out there. They are so useful that the field has its own name:
    *deep learning*. Deep learning has numerous applications in the most cutting-edge
    areas of machine learning, including image recognition, natural language processing,
    medicine, and self-driving cars. Neural networks are meant to, in a broad sense
    of the word, mimic how the human brain operates. They can be very complex, as
    figure 10.1 shows.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章，我们学习**神经网络**，也称为**多层感知器**。神经网络是（如果不是最受欢迎的）最流行的机器学习模型之一。它们非常有用，以至于该领域有自己独特的名称：**深度学习**。深度学习在机器学习的最前沿领域有众多应用，包括图像识别、自然语言处理、医学和自动驾驶汽车。从广义上讲，神经网络旨在模仿人脑的工作方式。它们可能非常复杂，如图
    10.1 所示。
- en: '![](../Images/10-1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-1.png)'
- en: Figure 10.1 A neural network. It may look complicated, but in the next few pages,
    we will demystify this image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 一个神经网络。它可能看起来很复杂，但在接下来的几页中，我们将揭开这个图像的神秘面纱。
- en: The neural network in figure 10.1 may look scary with lots of nodes, edges,
    and so on. However, we can understand neural networks in much simpler ways. One
    way to see them is as a collection of perceptrons (which we learned in chapters
    5 and 6). I like to see neural networks as compositions of linear classifiers
    that give rise to nonlinear classifiers. In low dimensions, the linear classifiers
    would look like lines or planes, and the nonlinear classifiers would look like
    complicated curves or surfaces. In this chapter, we discuss the intuition behind
    neural networks and the details about how they work, and we also code neural networks
    and use them for several applications such as image recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 中的神经网络可能因为有很多节点、边等看起来令人畏惧。然而，我们可以用更简单的方式理解神经网络。一种看待它们的方式是将它们视为感知器（我们在第
    5 章和第 6 章中学过）的集合。我喜欢将神经网络看作是线性分类器的组合，这些分类器产生了非线性分类器。在低维度中，线性分类器看起来像是线或平面，而非线性分类器看起来像是复杂的曲线或表面。在本章中，我们讨论神经网络背后的直觉以及它们如何工作的细节，我们还编写神经网络代码并使用它们进行图像识别等几个应用。
- en: 'Neural networks are useful for classification and regression. In this chapter,
    we focus mostly on classification neural networks, but we also learn the small
    changes needed to make them work for regression. First, a bit of terminology.
    Recall that in chapter 5, we learned the perceptron, and in chapter 6, we learned
    the logistic classifier. We also learned that they are called the discrete and
    continuous perceptrons. To refresh your memory, the output of the discrete perceptron
    is either 0 or 1, and the output of a continuous perceptron is any number in the
    interval (0,1). To calculate this output, the discrete perceptron uses the step
    function (the section “The step function and activation functions” in chapter
    5), and the continuous perceptron uses the sigmoid function (the section “A probability
    approach to classification: The sigmoid function” in chapter 6). In this chapter,
    we refer to both classifiers as perceptrons, and when needed, we specify whether
    we are talking about a discrete or a continuous perceptron.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在分类和回归中很有用。在本章中，我们主要关注分类神经网络，但也学习了使它们适用于回归所需的小改动。首先，一些术语。回想一下，在第5章中，我们学习了感知器，在第6章中，我们学习了逻辑分类器。我们还了解到它们被称为离散和连续感知器。为了刷新你的记忆，离散感知器的输出是0或1，而连续感知器的输出是(0,1)区间内的任何数字。为了计算这个输出，离散感知器使用步函数（第5章中的“步函数和激活函数”部分），而连续感知器使用sigmoid函数（第6章中的“一种概率方法进行分类：sigmoid函数”部分）。在本章中，我们将这两个分类器都称为感知器，并在需要时，我们会指明我们是在谈论离散感知器还是连续感知器。
- en: 'The code for this chapter is available in this GitHub repository: [https://github.com/luisguiserrano/manning/tree/master/Chapter_10_Neural_Networks](https://github.com/luisguiserrano/manning/tree/master/Chapter_10_Neural_Networks).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在以下GitHub仓库中找到：[https://github.com/luisguiserrano/manning/tree/master/Chapter_10_Neural_Networks](https://github.com/luisguiserrano/manning/tree/master/Chapter_10_Neural_Networks)。
- en: 'Neural networks with an example: A more complicated alien planet'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络示例：一个更复杂的外星世界
- en: 'In this section, we learn what a neural network is using a familiar sentiment
    analysis example from chapters 5 and 6\. The scenario is the following: we find
    ourselves on a distant planet populated by aliens. They seem to speak a language
    formed by two words, *aack* and *beep*, and we want to build a machine learning
    model that helps us determine whether an alien is happy or sad based on the words
    they say. This is called sentiment analysis, because we need to build a model
    to analyze the sentiment of the aliens. We record some aliens talking and manage
    to identify by other means whether they are happy or sad, and we come up with
    the dataset shown in in table 10.1.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过第5章和第6章中熟悉的情感分析示例来学习神经网络。场景如下：我们发现自己在一个由外星人居住的遥远星球上。他们似乎说一种由两个单词组成的语言，*aack*和*beep*，我们想要构建一个机器学习模型，帮助我们根据他们所说的单词来判断外星人是否快乐或悲伤。这被称为情感分析，因为我们需要构建一个模型来分析外星人的情绪。我们记录了一些外星人的谈话，并设法通过其他方式确定他们是否快乐或悲伤，并据此构建了表10.1所示的数据集。
- en: Table 10.1 Our dataset, in which each row represents an alien. The first column
    represents the sentence they uttered. The second and third columns represent the
    number of appearances of each of the words in the sentence. The fourth column
    represents the alien’s mood.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1我们的数据集，其中每一行代表一个外星人。第一列代表他们所说的句子。第二列和第三列代表句子中每个单词出现的次数。第四列代表外星人的情绪。
- en: '| Sentence | Aack | Beep | Mood |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | Aack | Beep | 情绪 |'
- en: '| “*Aack*” | 1 | 0 | Sad |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| “*Aack*” | 1 | 0 | 悲伤 |'
- en: '| “*Aack aack*” | 2 | 0 | Sad |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| “*Aack aack*” | 2 | 0 | 悲伤 |'
- en: '| “*Beep*” | 0 | 1 | Sad |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| “*Beep*” | 0 | 1 | 悲伤 |'
- en: '| “*Beep beep*” | 0 | 2 | Sad |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| “*Beep beep*” | 0 | 2 | 悲伤 |'
- en: '| “*Aack beep*” | 1 | 1 | Happy |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| “*Aack beep*” | 1 | 1 | 快乐 |'
- en: '| “*Aack aack beep*” | 2 | 1 | Happy |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| “*Aack aack beep*” | 2 | 1 | 快乐 |'
- en: '| “*Beep aack beep*” | 1 | 2 | Happy |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| “*Beep aack beep*” | 1 | 2 | 快乐 |'
- en: '| “*Beep aack beep aack*” | 2 | 2 | Happy |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| “*Beep aack beep aack*” | 2 | 2 | 快乐 |'
- en: This looks like a nice enough dataset, and we should be able to fit a classifier
    to this data. Let’s plot it first, as shown in figure 10.2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集看起来相当不错，我们应该能够将分类器拟合到这些数据上。让我们先绘制它，如图10.2所示。
- en: '![](../Images/10-2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-2.png)'
- en: Figure 10.2 The plot of the dataset in table 10.1\. The horizontal axis corresponds
    to the number of appearances of the word *aack*, and the vertical axis to the
    number of appearances of the word *beep*. The happy faces correspond to the happy
    aliens, and the sad faces to the sad aliens.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 表10.1中数据集的绘图。水平轴对应于单词*aack*出现的次数，垂直轴对应于单词*beep*出现的次数。快乐的面孔对应于快乐的异形，悲伤的面孔对应于悲伤的异形。
- en: 'From figure 10.2, it looks like we won’t be able to fit a linear classifier
    to this data. In other words, drawing a line that splits the happy and the sad
    faces apart would be impossible. What can we do? We’ve learned other classifiers
    that can do the job, such as the naive Bayes classifier (chapter 8) or decision
    trees (chapter 9). But in this chapter, we stick with perceptrons. If our goal
    is to separate the points in figure 10.2, and one line won’t do it, what then
    is better than one line? What about the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从图10.2来看，我们似乎无法将线性分类器拟合到这些数据上。换句话说，画一条将快乐和悲伤的面孔分开的线是不可能的。我们能做什么？我们已经学习了其他可以完成这项工作的分类器，例如朴素贝叶斯分类器（第8章）或决策树（第9章）。但在这个章节中，我们坚持使用感知器。如果我们的目标是分离图10.2中的点，并且一条直线做不到，那么什么比一条直线更好？以下是什么？
- en: Two lines
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两条直线
- en: A curve
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一条曲线
- en: These are examples of neural networks. Let’s begin by seeing why the first one,
    a classifier using two lines, is a neural network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是神经网络的例子。让我们首先看看为什么第一个，使用两条直线的分类器，是一个神经网络。
- en: 'Solution: If one line is not enough, use two lines to classify your dataset'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：如果一条直线不够，可以使用两条直线来分类你的数据集
- en: In this section, we explore a classifier that uses two lines to split the dataset.
    We have many ways to draw two lines to split this dataset, and one of these ways
    is illustrated in figure 10.3\. Let’s call them line 1 and line 2.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨一个使用两条直线来分割数据集的分类器。我们有多种方法可以画两条直线来分割这个数据集，其中一种方法如图10.3所示。让我们称它们为直线1和直线2。
- en: '![](../Images/10-3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![不等式图](../Images/10-3.png)'
- en: Figure 10.3 The happy and the sad points in our dataset cannot be divided by
    one line. However, drawing two lines separates them well—the points above both
    lines can be classified as happy, and the remaining points as sad. Combining linear
    classifiers this way is the basis for neural networks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 在我们的数据集中，快乐和悲伤的点不能由一条直线分开。然而，画两条直线可以很好地将它们分开——位于两条直线之上的点可以分类为快乐，其余的点为悲伤。以这种方式结合线性分类器是神经网络的基础。
- en: 'We can define our classifier as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义我们的分类器如下：
- en: Sentiment analysis classifier
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析分类器
- en: A sentence is classified as happy if its corresponding point is above the two
    lines shown in figure 10.3\. If it is below at least one of the lines, it is classified
    as sad.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个句子对应点位于图10.3中所示的两条直线之上，则将其分类为快乐。如果它至少低于一条直线，则将其分类为悲伤。
- en: Now, let’s throw in some math. Can we think of two equations for these lines?
    Many equations would work, but let’s use the following two (where *x*[a] is the
    number of times the word *aack* appears in the sentence, and *x*[b] is the number
    of times *beep* appears).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加入一些数学。我们能想到这两条直线的两个方程吗？许多方程都可以工作，但让我们使用以下两个（其中*x*[a]是句子中单词*aack*出现的次数，*x*[b]是单词*beep*出现的次数）。
- en: 'Line 1: 6*x*[a] + 10*x*[b] – 15 = 0'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1条直线：6*x*[a] + 10*x*[b] – 15 = 0
- en: 'Line 2: 10*x*[a] + 6*x*[b] – 15 = 0'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2条直线：10*x*[a] + 6*x*[b] – 15 = 0
- en: 'aside: How did WE find these equations? Notice that line 1 passes through the
    points (0, 1.5) and (2.5, 0). Therefore, the slope, defined as the change in the
    horizontal axis divided by the change in the vertical axis, is precisely ![](../Images/10_03_E01.png).
    The *y*-intercept—namely, the height at which the line crosses the vertical axis—is
    1.5\. Therefore, the equation of this line is ![](../Images/10_03_E02.png). By
    manipulating this equation, we get 6*x*[a] + 10*x*[b] – 15 = 0\. We can take a
    similar approach to find the equation for line 2.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'aside: 我们是如何找到这些方程的？注意，第1行通过点(0, 1.5)和(2.5, 0)。因此，斜率，定义为水平轴的变化除以垂直轴的变化，恰好是![方程图](../Images/10_03_E01.png)。*y*轴截距——即直线与垂直轴相交的高度是1.5。因此，这条直线的方程是![方程图](../Images/10_03_E02.png)。通过操作这个方程，我们得到6*x*[a]
    + 10*x*[b] – 15 = 0。我们可以用类似的方法找到第2条直线的方程。'
- en: 'Therefore, our classifier becomes the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的分类器变成了以下形式：
- en: Sentiment analysis classifier
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析分类器
- en: 'A sentence is classified as happy if both of the following two inequalities
    hold:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下两个不等式都成立，则句子被分类为快乐：
- en: '**Inequality 1**: 6*x*[a] + 10*x*[b] – 15 ≥ 0'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不等式1**：6*x*[a] + 10*x*[b] – 15 ≥ 0'
- en: '**Inequality 2**: 10*x*[a] + 6*x*[b] **–** 15 ≥ 0'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不等式2**：10*x*[a] + 6*x*[b] **–** 15 ≥ 0'
- en: If at least one of them fails, then the sentence is classified as sad.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其中至少有一个失败，那么句子就被分类为悲伤。
- en: As a consistency check, table 10.2 contains the values of each of the two equations.
    At the right of each equation, we check whether the equation’s value is larger
    than or equal to 0\. The right-most column checks whether both values are larger
    than or equal to 0.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一致性检查，表10.2包含了两个方程式各自的值。在每个方程式的右侧，我们检查方程式的值是否大于或等于0。最右侧的列检查两个值是否都大于或等于0。
- en: Table 10.2 The same dataset as in table 10.1, but with some new columns. The
    fourth and the sixth columns correspond to our two lines. The fifth and seventh
    column check if the equation of each of the lines at each of the data points gives
    a non-negative value. The last column checks if the two values obtained are both
    non-negative.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.2 与表10.1相同的dataset，但有一些新的列。第四列和第六列对应于我们的两条线。第五列和第七列检查每条线在每个数据点上的方程式是否给出非负值。最后一列检查得到的两个值是否都是非负的。
- en: '| Sentence | Aack | Beep | Equation 1 | Equation 1 ≥ 0? | Equation 2 | Equation
    2 ≥ 0? | Both equations ≥ 0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | 阿克 | 哔 | 方程式1 | 方程式1 ≥ 0? | 方程式2 | 方程式2 ≥ 0? | 两个方程式 ≥ 0 |'
- en: '| “*Aack*” | 1 | 0 | –9 | No | –5 | No | No |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| “*阿克*” | 1 | 0 | –9 | 否 | –5 | 否 | 否 |'
- en: '| “*Aack aack*” | 2 | 0 | –3 | No | 5 | Yes | No |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| “*阿克 阿克*” | 2 | 0 | –3 | 否 | 5 | 是 | 否 |'
- en: '| “*Beep*” | 0 | 1 | –5 | No | –9 | No | No |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| “*哔*” | 0 | 1 | –5 | 否 | –9 | 否 | 否 |'
- en: '| “*Beep beep*” | 0 | 2 | 5 | Yes | 3 | No | No |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| “*哔哔*” | 0 | 2 | 5 | 是 | 3 | 否 | 否 |'
- en: '| “*Aack beep*” | 1 | 1 | 1 | Yes | 1 | Yes | Yes |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| “*阿克 哔*” | 1 | 1 | 1 | 是 | 1 | 是 | 是 |'
- en: '| “*Aack aack beep*” | 1 | 2 | 11 | Yes | 7 | Yes | Yes |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| “*阿克 阿克 哔*” | 1 | 2 | 11 | 是 | 7 | 是 | 是 |'
- en: '| “*Beep aack beep*” | 2 | 1 | 7 | Yes | 11 | Yes | Yes |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| “*哔哔 阿克 哔*” | 2 | 1 | 7 | 是 | 11 | 是 | 是 |'
- en: '| “*Beep aack beep aack*” | 2 | 2 | 17 | Yes | 17 | Yes | Yes |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| “*哔哔哔*” | 2 | 2 | 17 | 是 | 17 | 是 | 是 |'
- en: Note that the right-most column in table 10.2 (yes/no) coincides with the right-most
    column in table 10.1 (happy/sad). This means the classifier managed to classify
    all the data correctly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，表10.2（是/否）最右侧的列与表10.1（快乐/悲伤）最右侧的列相匹配。这意味着分类器成功地将所有数据正确分类。
- en: Why two lines? Is happiness not linear?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是两条线？幸福不是线性的吗？
- en: In chapters 5 and 6, we managed to infer things about the language based on
    the equations of the classifiers. For example, if the weight of the word *aack*
    was positive, we concluded that it was likely a happy word. What about now? Could
    we infer anything about the language in this classifier that contains two equations?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章和第6章中，我们根据分类器的方程式推断出了关于语言的一些信息。例如，如果单词*aack*的权重是正的，我们得出结论，它很可能是一个快乐的词。那么现在呢？我们能否从这个包含两个方程式的分类器中推断出关于语言的信息？
- en: 'The way we could think of two equations is that maybe on the alien planet,
    happiness is not a simple linear thing but is instead based on two things. In
    real life, happiness can be based on many things: it can be based on having a
    fulfilling career combined with a happy family life and food on the table. It
    could be based on having coffee and a doughnut. In this case, let’s say that the
    two aspects of happiness are career and family. For an alien to be happy, it needs
    to have *both*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样考虑两个方程式，也许在外星人的星球上，幸福不是一个简单的线性事物，而是基于两件事。在现实生活中，幸福可以基于许多事情：它可以是基于拥有充实的事业和快乐的家庭生活以及桌上的食物。它也可以是基于拥有咖啡和甜甜圈。在这种情况下，让我们假设幸福的两个方面是事业和家庭。对于一个外星人来说，要快乐，他需要拥有*两者*。
- en: It turns out that in this case, both career happiness and family happiness are
    simple linear classifiers, and each is described by one of the two lines. Let’s
    say that line 1 corresponds to career happiness and line 2 to family happiness.
    Thus, we can think of alien happiness as the diagram in figure 10.4\. In this
    diagram, career happiness and family happiness are joined by an AND operator,
    which checks whether both are true. If they are, then the alien is happy. If any
    of them fails, the alien is unhappy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在这种情况下，职业幸福和家庭幸福都是简单的线性分类器，每个分类器都由两条线中的一条描述。让我们假设第1行对应职业幸福，第2行对应家庭幸福。因此，我们可以将外星人的幸福视为图10.4中的图表。在这个图表中，职业幸福和家庭幸福通过AND运算符连接，该运算符检查两者是否都为真。如果它们都是真的，那么外星人就是快乐的。如果其中任何一个失败，外星人就不快乐。
- en: '![](../Images/10-4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-4.png)'
- en: Figure 10.4 The happiness classifier is formed by the career happiness classifier,
    the family happiness classifier, and an AND operator. If both the career and family
    happiness classifiers output a Yes, then so does the happiness classifier. If
    any of them outputs a No, then the happiness classifier also outputs a No.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 幸福分类器由职业幸福分类器、家庭幸福分类器和AND运算符组成。如果职业和家庭幸福分类器都输出Yes，那么幸福分类器也输出Yes。如果其中任何一个输出No，那么幸福分类器也输出No。
- en: The family and career happiness classifiers are both perceptrons, because they
    are given by the equation of a line. Can we turn this AND operator into another
    perceptron? The answer is yes, and we’ll see how in the next subsection.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 家庭和职业幸福分类器都是感知器，因为它们由一条线的方程给出。我们能将这个AND运算符转换成另一个感知器吗？答案是肯定的，我们将在下一小节中看到如何做到这一点。
- en: Figure 10.4 is starting to look like a neural network. Just a few more steps
    and a little bit more math, and we’ll get to something looking much more like
    figure 10.1 at the beginning of the chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4开始看起来像神经网络。只需再走几步，再加上一点数学，我们就能得到本章开头图10.1中的样子。
- en: Combining the outputs of perceptrons into another perceptron
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将感知器的输出组合到另一个感知器中
- en: Figure 10.4 hints at a combination of perceptrons, in which we plug in the outputs
    of two perceptrons as inputs into a third perceptron. This is how neural networks
    are built, and in this section, we see the math behind it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4暗示了一种感知器的组合，其中我们将两个感知器的输出作为第三个感知器的输入。这就是神经网络是如何构建的，在本节中，我们将看到其背后的数学原理。
- en: 'In the section “The step function and activation functions” in chapter 5, we
    defined the step function, which returns 0 if the input is negative and 1 if the
    input is positive or zero. Notice that because we are using the step function,
    these are discrete perceptrons. Using this function, we can define the family
    and career happiness classifiers as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章的“阶跃函数和激活函数”部分，我们定义了阶跃函数，当输入为负时返回0，当输入为正或零时返回1。请注意，因为我们使用了阶跃函数，所以这些是离散感知器。使用这个函数，我们可以将家庭和职业幸福分类器定义为以下：
- en: Career happiness classifier
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 职业幸福分类器
- en: 'Weights:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: 6'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嘟嘟声*：6'
- en: '*Beep*: 10'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔哔声*：10'
- en: '**Bias**: –15'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：-15'
- en: '**Score of a sentence**: 6*x*[a] + 10*x*[b] – 15'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子得分**：6*x*[a] + 10*x*[b] – 15'
- en: '**Prediction**: *F* = *step*(6*x*[a] + 10*x*[b] – 15)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*F* = *step*(6*x*[a] + 10*x*[b] – 15)'
- en: Family happiness classifier
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 家庭幸福分类器
- en: 'Weights:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: 10'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嘟嘟声*：10'
- en: '*Beep*: 6'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔哔声*：6'
- en: '**Bias**: –15'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：-15'
- en: '**Score of a sentence**: 10*x*[a]+6*x*[b] – 15'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子得分**：10*x*[a]+6*x*[b] – 15'
- en: '**Prediction**: *C* = *step*(10*x*[a]+6*x*[b] – 15)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*C* = *step*(10*x*[a]+6*x*[b] – 15)'
- en: The next step is to plug the outputs of the career and family happiness classifiers
    into a new happiness classifier. Try verifying that the following classifier works.
    Figure 10.5 contains two tables with the outputs of the career and family classifiers,
    and a third table in which the first two columns are the inputs and the outputs
    of the career and family classifier, and the last column is the output of the
    family classifier. Each of the tables in figure 10.5 corresponds to a perceptron.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将职业和家庭幸福分类器的输出连接到一个新的幸福分类器。尝试验证以下分类器是否工作。图10.5包含两个表格，分别展示了职业和家庭分类器的输出，以及一个包含前两列是职业和家庭分类器的输入和输出，最后一列是家庭分类器输出的第三个表格。图10.5中的每个表格对应一个感知器。
- en: '![](../Images/10-5.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-5.png)'
- en: Figure 10.5 Three perceptron classifiers, one for career happiness, one for
    family happiness, and one for happiness, which combines the two previous ones.
    The outputs of the career and family perceptrons are inputs into the happiness
    perceptron.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 包含三个感知器分类器，一个用于职业幸福，一个用于家庭幸福，还有一个将前两个结合起来的幸福分类器。职业和家庭感知器的输出是幸福感知器的输入。
- en: Happiness classifier
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 幸福分类器
- en: 'Weights:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: 'Career: 1'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 职业：1
- en: 'Family: 1'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭：1
- en: '**Bias**: –1.5'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：-1.5'
- en: '**Score of a sentence**: 1 · *C* + 1 · *F* – 1.5'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子得分**：1 · *C* + 1 · *F* – 1.5'
- en: '**Prediction**: *ŷ* = *step*(1 · *C* + 1 · *F* – 1.5)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *step*(1 · *C* + 1 · *F* – 1.5)'
- en: This combination of classifiers is a neural network. Next, we see how to make
    this look like the image in figure 10.1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类器的组合构成了一个神经网络。接下来，我们将看到如何使其看起来像图10.1中的图像。
- en: A graphical representation of perceptrons
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的图形表示
- en: In this section, I show you how to represent perceptrons graphically, which
    gives rise to the graphical representation of neural networks. We call them neural
    networks because their basic unit, the perceptron, vaguely resembles a neuron.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何以图形方式表示感知器，这导致了神经网络图形表示的产生。我们称它们为神经网络，因为它们的基本单元，即感知器，大致类似于神经元。
- en: 'A neuron comprises three main parts: the soma, the dendrites, and the axon.
    In broad terms, the neuron receives signals coming from other neurons through
    the dendrites, processes them in the soma, and sends a signal through the axon
    to be received by other neurons. Compare this to a perceptron, which receives
    numbers as inputs, applies a mathematical'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元由三个主要部分组成：细胞体、树突和轴突。从广义上讲，神经元通过树突接收来自其他神经元的信号，在细胞体中处理它们，并通过轴突发送信号以被其他神经元接收。这与感知器形成对比，感知器接收数字作为输入，并应用数学
- en: operation to them (normally consisting of a sum composed with an activation
    function), and outputs a new number. This process is illustrated in figure 10.6.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对它们进行的操作（通常由一个与激活函数组合的和组成），并输出一个新的数字。这个过程如图 10.6 所示。
- en: '![](../Images/10-6.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-6.png)'
- en: 'Figure 10.6 A perceptron is loosely based on a neuron. Left: A neuron with
    its main components: the dendrites, the soma, and the axon. Signals come in through
    the dendrites, get processed in the soma, and get sent to other neurons through
    the axon. Right: A perceptron. The nodes in the left correspond to numerical inputs,
    the node in the middle performs a mathematical operation and outputs a number.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 一个感知器松散地基于神经元。左图：一个神经元及其主要组成部分：树突、细胞体和轴突。信号通过树突进入，在细胞体中处理，并通过轴突发送到其他神经元。右图：感知器。左边的节点对应于数值输入，中间的节点执行数学运算并输出一个数字。
- en: 'More formally, recall the definition of a perceptron from chapters 5 and 6,
    in which we had the following entities:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，回想一下第 5 章和第 6 章中感知器的定义，其中我们有了以下实体：
- en: '**Inputs**: *x*[1], *x*[2], …, *x*[n]'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：*x*[1]，*x*[2]，…，*x*[n]'
- en: '**Weights**: *w*[1], *w*[2], …, *w*[n]'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：*w*[1]，*w*[2]，…，*w*[n]'
- en: '**Bias**: *b*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏置**：*b*'
- en: '**An activation function**: Either the step function (for discrete perceptrons)
    or the sigmoid function (for continuous perceptrons). (Later in this chapter we
    learn other new activation functions.)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：对于离散感知器是阶跃函数，对于连续感知器是 Sigmoid 函数。（在本章的后面我们将学习其他新的激活函数。）'
- en: '**A prediction**: Defined by the formula *ŷ* = *f*(*w*[1] *x*[1] + *w*[2] *x*[2]
    + … + *w*[n] *x*[n] + *b*), where *f* is the corresponding activation function'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：由公式 *ŷ* = *f*(*w*[1] *x*[1] + *w*[2] *x*[2] + … + *w*[n] *x*[n] + *b*)
    定义，其中 *f* 是相应的激活函数'
- en: The way these are located in the diagram is illustrated in figure 10.7\. On
    the left, we have the input nodes, and on the right, we have the output node.
    The input variables go on the input nodes. The final input node doesn’t contain
    a variable, but it contains a value of 1\. The weights are located on the edges
    connecting the input nodes with the output node. The weight corresponding to the
    final input node is the bias. The mathematical operations for calculating the
    prediction happen inside the output node, and this node outputs the prediction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在图中的位置如图 10.7 所示。在左侧，我们有输入节点，在右侧，我们有输出节点。输入变量放在输入节点上。最后的输入节点不包含变量，但它包含值为 1。权重位于连接输入节点和输出节点的边路上。对应于最终输入节点的权重是偏置。计算预测的数学运算发生在输出节点内部，并且该节点输出预测。
- en: 'For example, the perceptron defined by the equation *ŷ* = *σ*(3*x*[1] – 2*x*[2]
    + 4*x*[3] + 2) is illustrated in figure 10.7\. Notice that in this perceptron,
    the following steps are performed:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由方程 *ŷ* = *σ*(3*x*[1] – 2*x*[2] + 4*x*[3] + 2) 定义的感知器如图 10.7 所示。注意，在这个感知器中，以下步骤被执行：
- en: The inputs are multiplied with their corresponding weights and added to obtain
    3*x*[1] – 2*x*[2] + 4*x*[3].
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入与相应的权重相乘并相加，得到 3*x*[1] – 2*x*[2] + 4*x*[3]。
- en: The bias is added to the previous equation, to obtain 3*x*[1] – 2*x*[2] + 4*x*[3]
    + 2.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将偏置添加到前面的方程中，得到 3*x*[1] – 2*x*[2] + 4*x*[3] + 2。
- en: The sigmoid activation function is applied to obtain the output *ŷ* = *σ*(3*x*[1]
    – 2*x*[2] + 4*x*[3] + 2).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Sigmoid 激活函数应用于得到输出 *ŷ* = *σ*(3*x*[1] – 2*x*[2] + 4*x*[3] + 2)。
- en: '![](../Images/10-7.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-7.png)'
- en: Figure 10.7 A visual representation of a perceptron. The inputs (features and
    bias) appear as nodes on the left, and the weights and bias are on the edges connecting
    the input nodes to the main node in the middle. The node in the middle takes the
    linear combination of the weights and the inputs, adds the bias, and applies the
    activation function, which in this case is the sigmoid function. The output is
    the prediction given by the formula *ŷ* = *σ*(3*x*[1] – 2*x*[2] + 4*x*[3] + 2).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 感知器的视觉表示。输入（特征和偏置）作为左边的节点出现，权重和偏置位于连接输入节点到中间主节点的边上。中间的节点对权重和输入进行线性组合，加上偏置，并应用激活函数，在这种情况下是Sigmoid函数。输出是公式
    *ŷ* = *σ*(3*x*[1] – 2*x*[2] + 4*x*[3] + 2) 给出的预测。
- en: For example, if the input to this perceptron is the point (*x*[1], *x*[2], *x*[3])
    = (1, 3, 1), then the output is *σ*(3 · 1 – 2 · 3 + 4 · 1 + 2) = *σ*(3) = 0.953.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果这个感知器的输入是点 (*x*[1], *x*[2], *x*[3]) = (1, 3, 1)，那么输出是 *σ*(3 · 1 – 2 · 3
    + 4 · 1 + 2) = *σ*(3) = 0.953。
- en: If this perceptron was defined using the step function instead of the sigmoid
    function, the output would be *step*(3 · 1 – 2 · 3 + 4 · 1 + 2) = *step*(3) =
    1.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个感知器是用阶跃函数而不是Sigmoid函数定义的，输出将是 *step*(3 · 1 – 2 · 3 + 4 · 1 + 2) = *step*(3)
    = 1。
- en: This graphical representation makes perceptrons easy to concatenate, as we see
    in the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种图形表示使得感知器容易串联，正如我们在下一节中看到的。
- en: A graphical representation of neural networks
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的图形表示
- en: As we saw in the previous section, a neural network is a concatenation of perceptrons.
    This structure is meant to loosely emulate the human brain, in which the output
    of several neurons becomes the input to another neuron. In the same way, in a
    neural network, the output of several perceptrons becomes the input of another
    perceptron, as illustrated in figure 10.8.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，神经网络是感知器的串联。这种结构旨在松散地模拟人脑，其中几个神经元的输出成为另一个神经元的输入。同样，在神经网络中，几个感知器的输出成为另一个感知器的输入，如图10.8所示。
- en: '![](../Images/10-8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-8.png)'
- en: 'Figure 10.8 Neural networks are meant to (loosely) emulate the structure of
    the brain. Left: The neurons are connected inside the brain in a way that the
    output of a neuron becomes the input to another neuron. Right: The perceptrons
    are connected in a way that the output of a perceptron becomes the input to another
    perceptron.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 神经网络旨在（松散地）模拟大脑的结构。左：神经元在大脑内部以某种方式连接，使得一个神经元的输出成为另一个神经元的输入。右：感知器以某种方式连接，使得一个感知器的输出成为另一个感知器的输入。
- en: The neural network we built in the previous section, in which we concatenate
    the career perceptron and the family perceptron with the happiness perceptron,
    is illustrated in figure 10.9.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中构建的神经网络，其中我们将职业感知器和家庭感知器与幸福感知器串联起来，如图10.9所示。
- en: Notice that in the diagram in figure 10.9, the inputs to the career and family
    perceptrons are repeated. A cleaner way to write this, in which these inputs don’t
    get repeated, is illustrated in figure 10.10.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到图10.9中的图中，职业和家庭感知器的输入被重复了。一种更清晰的方式来表达，其中这些输入不会重复，如图10.10所示。
- en: Notice that these three perceptrons use the step function. We did this only
    for educational purposes, because in real life, neural networks never use the
    step function as an activation function, because it makes it impossible for us
    to use gradient descent (more on this in the section “Training neural networks”).
    The sigmoid function, however, is widely used in neural networks, and in the section
    “Different activation functions,” we learn some other useful activation functions
    used in practice.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这三个感知器使用了阶跃函数。我们这样做只是为了教育目的，因为在现实生活中，神经网络永远不会使用阶跃函数作为激活函数，因为这会使我们无法使用梯度下降（更多内容请参阅“训练神经网络”部分）。然而，Sigmoid函数在神经网络中却被广泛使用，在“不同的激活函数”部分，我们将学习一些实际中使用的其他有用的激活函数。
- en: '![](../Images/10-9.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-9.png)'
- en: Figure 10.9 When we connect the outputs of the career and family perceptrons
    into the happiness perceptron, we get a neural network. This neural network uses
    the step function as an activation function.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 当我们将职业和家庭感知器的输出连接到幸福感知器时，我们得到一个神经网络。这个神经网络使用阶跃函数作为激活函数。
- en: '![](../Images/10-10.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-10.png)'
- en: Figure 10.10 A cleaned-up version of the diagram in figure 10.9\. In this diagram,
    the features *x*[a] and *x*[b], and the bias are not repeated. Instead, each of
    them connects to both of the nodes at the right, nicely combining the three perceptrons
    into the same diagram.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 图10.9中图表的清理版本。在这个图表中，特征 *x*[a] 和 *x*[b]，以及偏置没有重复。相反，它们各自连接到右侧的两个节点，很好地将三个感知器组合到同一个图表中。
- en: The boundary of a neural network
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的边界
- en: In chapters 5 and 6, we studied the boundaries of perceptrons, which are given
    by lines. In this section, we see what the boundaries of neural networks look
    like.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章和第6章中，我们研究了感知器的边界，这些边界由线给出。在本节中，我们看看神经网络边界是什么样的。
- en: Recall from chapters 5 and 6 that both the discrete perceptron and the continuous
    perceptron (logistic classifier) have a linear boundary given by the linear equation
    defining them. The discrete perceptron assigns predictions of 0 and 1 to the points
    according to what side of the line they are. The continuous perceptron assigns
    a prediction between 0 and 1 to every point in the plane. The points over the
    line get a prediction of 0.5, the points on one side of the line get predictions
    higher than 0.5, and the points on the other side get predictions lower than 0.5\.
    Figure 10.11 illustrates the discrete and continuous perceptrons corresponding
    to the equation 10*x*[a] + 6*x*[b] – 15 = 0.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从第5章和第6章回顾，离散感知器和连续感知器（逻辑分类器）都有一个由定义它们的线性方程给出的线性边界。离散感知器根据点位于线的哪一侧将预测0和1分配给点。连续感知器将0到1之间的预测分配给平面上的每个点。线上的点得到0.5的预测，线一侧的点得到高于0.5的预测，而线另一侧的点得到低于0.5的预测。图10.11说明了对应于方程10*x*[a]
    + 6*x*[b] – 15 = 0的离散和连续感知器。
- en: '![](../Images/10-111.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-111.png)'
- en: 'Figure 10.11 The boundary of a perceptron is a line. Left: For a discrete perceptron,
    the points on one side of the line are given a prediction of 0, and the points
    on the other side a prediction of 1\. Right: For a continuous perceptron, the
    points are all given a prediction in the interval (0,1). In this example, the
    points at the very left get predictions close to 0, those at the very right get
    predictions close to 1, and those over the line get predictions of 0.5.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 感知器的边界是一条线。左：对于离散感知器，线一侧的点被预测为0，另一侧的点被预测为1。右：对于连续感知器，所有点都被预测在区间(0,1)内。在这个例子中，最左边的点得到的预测接近0，最右边的点得到的预测接近1，而在线上的点得到的预测为0.5。
- en: 'We can also visualize the output of a neural network in a similar way. Recall
    that the output of the neural network with the step activation function is the
    following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以以类似的方式可视化神经网络的输出。回想一下，具有步进激活函数的神经网络的输出如下：
- en: If 6*x*[a] + 10*x*[b] – 15 ≥ 0 and 10*x*[a] + 6*x*[b] – 15 ≥ 0, then the output
    is 1.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 6*x*[a] + 10*x*[b] – 15 ≥ 0 且 10*x*[a] + 6*x*[b] – 15 ≥ 0，则输出为1。
- en: Otherwise, the output is 0.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，输出为0。
- en: This boundary is illustrated in the left side of figure 10.12 using two lines.
    Notice that it’s expressed as a combination of the boundaries of the two input
    perceptrons and the bias node. The boundary obtained with the step activation
    function is made by broken lines, whereas the one obtained with the sigmoid activation
    function is a curve.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在图10.12的左侧，使用两条线来表示这个边界。请注意，它是由两个输入感知器边界和偏置节点的组合表示的。使用步进激活函数得到的边界用虚线表示，而使用sigmoid激活函数得到的边界是一条曲线。
- en: 'To study these boundaries more carefully, check the following notebook: [https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Plotting_Boundaries.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Plotting_Boundaries.ipynb).
    In this notebook, the boundaries of the two lines and the two neural networks
    are plotted with the step and sigmoid activation functions, as illustrated in
    figure 10.13.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要更仔细地研究这些边界，请查看以下笔记本：[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Plotting_Boundaries.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Plotting_Boundaries.ipynb)。在这个笔记本中，使用步进和sigmoid激活函数绘制了两条线和两个神经网络的边界，如图10.13所示。
- en: '![](../Images/10-12.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-12.png)'
- en: Figure 10.12 To build a neural network, we use the outputs of two perceptrons
    and a bias node (represented by a classifier that always outputs a value of 1)
    to a third perceptron. The boundary of the resulting classifier is a combination
    of the boundaries of the input classifiers. On the left, we see the boundary obtained
    using the step function, which is a broken line. On the right, we see the boundary
    obtained using the sigmoid function, which is a curve.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 构建神经网络时，我们使用两个感知器和一个偏置节点（用一个总是输出值为1的分类器表示）的输出连接到第三个感知器。结果分类器的边界是输入分类器边界的组合。在左侧，我们看到使用阶跃函数得到的边界，这是一条折线。在右侧，我们看到使用sigmoid函数得到的边界，这是一条曲线。
- en: '![](../Images/10-13.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-13.png)'
- en: 'Figure 10.13 The plots of the boundaries of the classifiers. Top: The two linear
    classifiers, the career (left) and family (right) classifiers. Bottom: The two
    neural networks, using the step function (left) and the sigmoid function (right).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 分类器边界的图示。顶部：两个线性分类器，职业（左侧）和家庭（右侧）分类器。底部：两个神经网络，使用阶跃函数（左侧）和sigmoid函数（右侧）。
- en: Note that the neural network with the sigmoid activation function actually doesn’t
    fit the entire dataset well, because it misclassifies the point (1,1), as shown
    in the bottom right of figure 10.13\. Try changing the weights in a way that it
    fits this point well. (See exercise 10.3 at the end of the chapter).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，具有sigmoid激活函数的神经网络实际上并不很好地拟合整个数据集，因为它错误地将点（1,1）分类了，如图10.13的右下角所示。尝试以某种方式改变权重，使其很好地拟合这个点。（参见本章末尾的练习10.3）。
- en: The general architecture of a fully connected neural network
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接神经网络的通用架构
- en: 'In the previous sections, we saw an example of a small neural network, but
    in real life, neural networks are much larger. The nodes are arranged in layers,
    as illustrated in figure 10.14\. The first layer is the input layer, the final
    layer is the output layer, and all the layers in between are called the hidden
    layers. The arrangement of nodes and layers is called the *architecture* of the
    neural network. The number of layers (excluding the input layer) is called the
    depth of the neural network. The neural network in figure 10.14 has a depth of
    3, and the following architecture:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了一个小型神经网络的例子，但在现实生活中，神经网络要大得多。节点按层排列，如图10.14所示。第一层是输入层，最后一层是输出层，所有介于两者之间的层都称为隐藏层。节点和层的排列称为神经网络的*架构*。层的数量（不包括输入层）称为神经网络的深度。图10.14中的神经网络深度为3，其架构如下：
- en: An input layer of size 4
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为4的输入层
- en: A hidden layer of size 5
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为5的隐藏层
- en: A hidden layer of size 3
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为3的隐藏层
- en: An output layer of size 1
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为1的输出层
- en: '![](../Images/10-14.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-14.png)'
- en: Figure 10.14 The general architecture of a neural network. The nodes are divided
    into layers, where the leftmost layer is the input layer, the rightmost layer
    is the output layer, and all the layers in between are hidden layers. All the
    nodes in a layer are connected to all the (non-bias) nodes in the next layer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 神经网络的一般架构。节点被划分为层，其中最左侧的层是输入层，最右侧的层是输出层，所有介于两者之间的层都是隐藏层。同一层中的所有节点都连接到下一层中的所有（非偏置）节点。
- en: Neural networks are often drawn without the bias nodes, but it is assumed they
    are part of the architecture. However, we don’t count bias nodes in the architecture.
    In other words, the size of a layer is the number of non-bias nodes in that layer.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常绘制时不包含偏置节点，但假设它们是架构的一部分。然而，我们不计入架构中的偏置节点。换句话说，层的尺寸是该层中非偏置节点的数量。
- en: Notice that in the neural network in figure 10.14, every node in a layer is
    connected to every (non-bias) node in the next layer. Furthermore, no connections
    happen between nonconsecutive layers. This architecture is called *fully connected*.
    For some applications, we use different architectures where not all the connections
    are there, or where some nodes are connected between nonconsecutive layers—see
    the section “Other architectures for more complex dialects” to read about some
    of them. However, in this chapter, all the neural networks we build are fully
    connected.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在图10.14中的神经网络中，同一层中的每个节点都连接到下一层中的每个（非偏置）节点。此外，非连续层之间没有连接。这种架构称为*全连接*。对于某些应用，我们使用不同的架构，其中并非所有连接都存在，或者某些节点在非连续层之间连接——请参阅“更复杂方言的其他架构”部分，了解其中的一些内容。然而，在本章中，我们构建的所有神经网络都是全连接的。
- en: Picture the boundary of a neural network like the one shown in figure 10.15\.
    In this diagram, you can see the classifier corresponding to each node. Notice
    that the first hidden layer is formed by linear classifiers, and the classifiers
    in each successive layer are slightly more complex than those in the previous
    ones.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络边界想象成图10.15中所示的那样。在这个图中，你可以看到每个节点对应的分类器。注意，第一隐藏层由线性分类器组成，每个后续层的分类器比前一个层的稍微复杂一些。
- en: '![](../Images/10-15.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-15.png)'
- en: Figure 10.15 The way I like to visualize neural networks. Each of the nodes
    corresponds to a classifier, and this classifier has a well-defined boundary.
    The nodes in the first hidden layer all correspond to linear classifiers (perceptrons),
    so they are drawn as lines. The boundaries of the nodes in each layer are formed
    by combining the boundaries from the previous layer. Therefore, the boundaries
    get more and more complex in each hidden layer. In this diagram, we have removed
    the bias nodes.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 我喜欢这样可视化神经网络。每个节点都对应一个分类器，这个分类器有一个明确的边界。第一隐藏层中的节点都对应线性分类器（感知器），因此它们被绘制成线条。每个层的节点边界是由前一层边界组合而成的。因此，每个隐藏层的边界变得越来越复杂。在这个图中，我们移除了偏差节点。
- en: A great tool to play with to understand neural networks is TensorFlow Playground,
    which can be found at [https://playground.tensorflow.org](https://playground.tensorflow.org).
    Several graphical datasets are available there, and it is possible to train neural
    networks with different architectures and hyperparameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的工具来玩以理解神经网络的是TensorFlow Playground，可以在[https://playground.tensorflow.org](https://playground.tensorflow.org)找到。那里有几个图形数据集可用，你可以用不同的架构和超参数训练神经网络。
- en: Training neural networks
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: In this chapter, we’ve seen how neural networks look in general and that they’re
    not as mysterious as they sound. How do we train one of these monsters? In theory,
    the process is not complicated, although it can be computationally expensive.
    We have several tricks and heuristics we can use to speed it up. In this section,
    we learn this training process. Training a neural network is not that different
    from training other models, such as the perceptron or the logistic classifier.
    We begin by initializing all the weights and biases at random. Next, we define
    an error function to measure the performance of the neural network. Finally, we
    repeatedly use the error function to tune in the weights and biases of the model
    to reduce the error function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了神经网络的一般外观，以及它们并不像听起来那么神秘。我们如何训练这样的怪物呢？从理论上讲，这个过程并不复杂，尽管它可能计算成本很高。我们有几个技巧和启发式方法可以使用来加速这个过程。在本节中，我们学习这个训练过程。训练神经网络与其他模型（如感知器或逻辑分类器）的训练并没有太大区别。我们首先随机初始化所有权重和偏差。接下来，我们定义一个错误函数来衡量神经网络的性能。最后，我们反复使用错误函数来调整模型的权重和偏差，以减少错误函数。
- en: 'Error function: A way to measure how the neural network is performing'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数：衡量神经网络性能的一种方式
- en: In this section, we learn about the error function used to train neural networks.
    Luckily, we’ve seen this function before—the log-loss function from the section
    “Logistic classifiers” in chapter 6\. Recall that the formula for the log loss
    is
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习用于训练神经网络的错误函数。幸运的是，我们之前已经见过这个函数——第6章“逻辑分类器”部分中的对数损失函数。回忆一下，对数损失的公式是
- en: '*log loss* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*),'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*对数损失* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*)，'
- en: where *y* is the label and *ŷ* the prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y* 是标签，*ŷ* 是预测。
- en: As a refresher, a good reason for using log loss for classification problems
    is that it returns a small value when the prediction and the label are close and
    a large value when they are far.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为复习，使用对数损失函数进行分类问题的一个很好的原因是，当预测和标签接近时，它返回一个较小的值，而当它们相距较远时，它返回一个较大的值。
- en: 'Backpropagation: The key step in training the neural network'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播：训练神经网络的关键步骤
- en: 'In this section, we learn the most important step in the process of training
    a neural network. Recall that in chapters 3, 5, and 6 (linear regression, perceptron
    algorithm, and logistic regression), we used gradient descent to train our models.
    This is also the case for neural networks. The training algorithm is called the
    *backpropagation algorithm*, and its pseudocode follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习训练神经网络过程中的最重要步骤。回忆一下，在第3章、第5章和第6章（线性回归、感知器算法和逻辑回归）中，我们使用梯度下降来训练我们的模型。对于神经网络也是如此。训练算法被称为*反向传播算法*，其伪代码如下：
- en: Pseudocode for the backpropagation algorithm
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法的伪代码
- en: Initialize the neural network with random weights and biases.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机权重和偏置初始化神经网络。
- en: 'Repeat many times:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Calculate the loss function and its gradient (namely, the derivatives with respect
    to each one of the weights and biases).
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失函数及其梯度（即相对于每个权重和偏置的导数）。
- en: Take a small step in the direction opposite to the gradient to decrease the
    loss function by a small amount.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着梯度相反的方向迈一小步，以减少损失函数的小量。
- en: The weights you obtain correspond to a neural network that (likely) fits the
    data well.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你获得的权重对应于一个（很可能）很好地拟合数据的神经网络。
- en: The loss function of a neural network is complicated, because it involves the
    logarithm of the prediction, and the prediction itself is a complicated function.
    Furthermore, we need to calculate the derivative with respect to many variables,
    corresponding to each of the weights and biases of the neural network. In appendix
    B, “Using gradient descent to train neural networks,” we go over the mathematical
    details of the backpropagation algorithm for a neural network with one hidden
    layer or arbitrary size. See some recommended resources in appendix C to go deep
    into the math of backpropagation for deeper neural networks. In practice, great
    packages, such as Keras, TensorFlow, and PyTorch, have implemented this algorithm
    with great speed and performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的损失函数很复杂，因为它涉及到预测的对数，而预测本身是一个复杂的函数。此外，我们需要计算相对于许多变量的导数，对应于神经网络的每个权重和偏置。在附录B“使用梯度下降训练神经网络”中，我们回顾了具有一个隐藏层或任意大小的神经网络的反向传播算法的数学细节。在附录C中，可以看到一些推荐资源，以深入了解深层神经网络的反向传播数学。在实践中，像Keras、TensorFlow和PyTorch这样的优秀包已经以极高的速度和性能实现了此算法。
- en: Recall that when we learned linear regression models (chapter 3), discrete perceptrons
    (chapter 5), and continuous perceptrons (chapter 6), the process always had a
    step where we moved a line in the way we needed to model our data well. This type
    of geometry is harder to visualize for neural networks, because it happens in
    much higher dimensions. However, we can still form a mental picture of backpropagation,
    and for this, we need to focus on only one of the nodes of the neural network
    and one data point. Imagine a classifier like the one on the right in figure 10.16\.
    This classifier is obtained from the three classifiers on the left (the bottom
    one corresponds to the bias, which we represent by a classifier that always returns
    a prediction of 1). The resulting classifier misclassifies the point, as is shown.
    From the three input classifiers, the first one classifies the point well, but
    the other two don’t. Thus, the backpropagation step will increase the weight on
    the edge corresponding to the top classifier and decrease those corresponding
    to the two classifiers at the bottom. This ensures the resulting classifier will
    look more like the top one, and thus, its classification for the point will improve.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们学习线性回归模型（第3章）、离散感知器（第5章）和连续感知器（第6章）时，这个过程总是有一个步骤，我们需要移动一条线以更好地模拟我们的数据。这种类型的几何图形对于神经网络来说更难可视化，因为它发生在更高的维度中。然而，我们仍然可以形成反向传播的心理图景，为此，我们需要专注于神经网络的一个节点和一个数据点。想象一下图10.16右边的分类器。这个分类器是从左边的三个分类器得到的（底部的一个对应于偏置，我们用总是返回预测为1的分类器来表示）。结果分类器错误地分类了点，如图所示。从三个输入分类器中，第一个很好地分类了点，但其他两个没有。因此，反向传播步骤将增加对应于顶部分类器的边的权重，并减少对应于底部两个分类器的权重。这确保了结果分类器将更像顶部的一个，因此，它对点的分类将得到改善。
- en: '![](../Images/10-16.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-16.png)'
- en: Figure 10.16 A mental picture of backpropagation. At each step of the training
    process, the weights of the edges are updated. If a classifier is good, its weight
    gets increased by a small amount, and if it is bad, its weight gets decreased.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16 反向传播的心理图景。在训练过程的每一步，边的权重都会更新。如果一个分类器很好，它的权重会增加一小部分，如果它不好，它的权重会减少。
- en: 'Potential problems: From overfitting to vanishing gradients'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的问题：从过拟合到梯度消失
- en: In practice, neural networks work very well. But, due to their complexity, many
    problems arise with their training. Luckily, we can have a solution for the most
    pressing ones. One problem
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，神经网络工作得非常好。但是，由于它们的复杂性，许多问题出现在它们的训练过程中。幸运的是，我们可以为最紧迫的问题提供一个解决方案。一个问题
- en: that neural networks have is overfitting—really big architectures can potentially
    memorize our data without generalizing it well. In the next section, we see some
    techniques to reduce overfitting when training neural networks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络存在的问题是过拟合——非常大的架构可能会潜在地记住我们的数据，而没有很好地泛化。在下一节中，我们将看到一些在训练神经网络时减少过拟合的技术。
- en: Another serious problem that neural networks can have is vanishing gradients.
    Notice that the sigmoid function is very flat on the ends, which signifies that
    the derivatives (tangents to the curve) are too flat (see figure 10.17). This
    means their slopes are very close to zero.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能还会遇到另一个严重的问题，那就是梯度消失。请注意，sigmoid函数在两端非常平坦，这表明其导数（曲线的切线）过于平坦（见图10.17）。这意味着它们的斜率非常接近于零。
- en: '![](../Images/10-17.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-17.png)'
- en: Figure 10.17 The sigmoid function is flat at the ends, which means that for
    large positive and negative values, its derivative is very small, hampering the
    training.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17 sigmoid函数在两端平坦，这意味着对于大的正负值，其导数非常小，这阻碍了训练。
- en: During the backpropagation process, we compose many of these sigmoid functions
    (which means we plug in the output of a sigmoid function as the input to another
    sigmoid function repeatedly). As expected, this composition results in derivatives
    that are very close to zero, which means the steps taken during backpropagation
    are tiny. If this is the case, it may take us a very long time to get to a good
    classifier, which is a problem.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，我们组合了许多sigmoid函数（这意味着我们反复将sigmoid函数的输出作为另一个sigmoid函数的输入）。正如预期的那样，这种组合导致导数非常接近于零，这意味着反向传播中采取的步骤非常小。如果这种情况发生，我们可能需要很长时间才能得到一个好的分类器，这是一个问题。
- en: We have several solutions to the vanishing gradient problem, and so far one
    of the most effective ones is to change the activation function. In the section
    “Different activation functions,” we learn some new activation functions to help
    us deal with the vanishing gradient problem.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个解决梯度消失问题的方案，到目前为止，其中最有效的一个是改变激活函数。在“不同的激活函数”这一节中，我们学习了一些新的激活函数，以帮助我们处理梯度消失问题。
- en: 'Techniques for training neural networks: Regularization and dropout'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的技巧：正则化和dropout
- en: As mentioned in the previous section, neural networks are prone to overfitting.
    In this section, we discuss some techniques to decrease the amount of overfitting
    during the training of neural networks.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络容易过拟合。在本节中，我们讨论了一些在神经网络训练过程中减少过拟合量的技术。
- en: How do we pick the correct architecture? This is a difficult question, with
    no concrete answer. The rule of thumb is to err on the side of picking a larger
    architecture than we may need and then apply techniques to reduce the amount of
    overfitting that your network may have. In some way it is like picking a pair
    of pants, where the only choices you have are too small or too big. If we pick
    pants that are too small, there is not much we can do. On the other hand, if we
    pick pants that are too big, we can wear a belt to make them fit better. It’s
    not ideal, but it’s all we have for now. Picking the correct architecture based
    on the dataset is a complicated problem, and a lot of research is currently being
    done in this direction. To learn more about this, check out the resources in appendix
    C.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择正确的架构？这是一个困难的问题，没有具体的答案。通常的做法是选择一个比我们可能需要的更大的架构，然后应用技术来减少网络可能有的过拟合量。在某种程度上，这就像挑选一条裤子，你唯一的选择要么太小要么太大。如果我们选择太小的裤子，我们几乎无能为力。另一方面，如果我们选择太大的裤子，我们可以用皮带调整使其更合身。这并不理想，但这是我们目前唯一的选择。根据数据集选择正确的架构是一个复杂的问题，目前在这个方向上正在进行大量研究。想了解更多，请参阅附录C中的资源。
- en: 'Regularization: A way to reduce overfitting by punishing higher weights'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化：通过惩罚更高的权重来减少过拟合的方法
- en: As we learned in chapter 4, we can use L1 and L2 regularization to decrease
    overfitting in regression and classification models, and neural networks are no
    exception. The way one applies regularization in neural networks is the same as
    one would apply it in linear regression—by adding a regularization term to the
    error function. If we are doing L1 regularization, the regularization term is
    equal to the regularization parameter (λ) times the sum of the absolute values
    of all the weights of our model (not including the biases). If we are doing L2
    regularization, then we take the sum of squares instead of absolute values. As
    an example, the L2 regularization error of the neural network in the example in
    the section “Neural networks with an example” is
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第4章所学，我们可以使用L1和L2正则化来减少回归和分类模型中的过拟合，神经网络也不例外。在神经网络中应用正则化的方式与在线性回归中应用的方式相同——通过向误差函数中添加一个正则化项。如果我们进行L1正则化，正则化项等于正则化参数（λ）乘以我们模型中所有权重（不包括偏置）的绝对值之和。如果我们进行L2正则化，那么我们取平方和而不是绝对值。例如，在“具有示例的神经网络”部分中的示例神经网络，其L2正则化误差为
- en: '*log loss* + λ *·* (6² + 10² + 10² + 6² + 1² + 1²) = *log loss* + 274λ.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*对数损失* + λ *·* (6² + 10² + 10² + 6² + 1² + 1²) = *对数损失* + 274λ。'
- en: 'Dropout: Making sure a few strong nodes are not dominating the training'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout：确保少数强节点不会主导训练
- en: 'Dropout is an interesting technique used to reduce overfitting in neural networks,
    and to understand it, let’s consider the following analogy: imagine that we are
    right-handed, and we like to go to the gym. After some time, we start noticing
    that our right bicep is growing a lot, but our left one is not. We then start
    paying more attention to our training and realize that because we are right-handed,
    we tend to always pick up the weights with the right arm, and we’re not allowing
    the left arm to do much exercise. We decide that enough is enough, so we take
    a drastic measure. Some days we decide to tie our right hand to our back and force
    ourselves to do the entire routine without using the right arm. After this, we
    start seeing that the left arm starts to grow, as desired. Now, to get both arms
    to work, we do the following: every day before heading to the gym, we flip two
    coins, one for each arm. If the left coin falls on heads, we tie the left arm
    to our back, and if the right arm falls on heads, we tie the right arm to our
    back. Some days we’ll work with both arms, some days with only one, and some days
    with none (those are leg days, perhaps). The randomness of the coins will make
    sure that, on average, we are working both arms almost equally.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种用于减少神经网络过拟合的有趣技术，为了理解它，让我们考虑以下类比：想象我们习惯用右手，喜欢去健身房。过了一段时间，我们开始注意到我们的右二头肌长得很多，但左边的却没有。然后我们开始更加关注我们的训练，并意识到因为我们习惯用右手，所以我们总是用右手拿起杠铃，没有让左手得到足够的锻炼。我们决定不能再这样下去了，于是采取了一个激进的措施。有些日子我们决定把右手绑在背后，强迫自己整个训练过程不用右手。这样之后，我们开始看到左臂开始按照预期增长。现在，为了让两只手臂都得到锻炼，我们这样做：每天去健身房之前，我们抛两个硬币，一个代表每只手臂。如果左边的硬币正面朝上，我们就把左臂绑在背后，如果右臂的硬币正面朝上，我们就把右臂绑在背后。有些日子我们会用两只手臂工作，有些日子只用一只，有些日子则不用（可能是腿部训练日）。硬币的随机性将确保，平均来看，我们几乎同样地锻炼两只手臂。
- en: Dropout uses this logic, except instead of arms, we are training the weights
    in the neural network. When a neural network has too many nodes, some of the nodes
    pick up patterns in the data that are useful for making good predictions, whereas
    other nodes pick up patterns that are noisy or irrelevant. The dropout process
    removes some of the nodes randomly at every epoch and performs one gradient descent
    step on the remaining ones. By dropping some of the nodes at each epoch, it is
    likely that sometimes we may drop the ones that have picked up the useful patterns,
    thus forcing the other nodes to pick up the slack.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout使用这个逻辑，除了使用臂之外，我们在神经网络中训练权重。当一个神经网络有太多节点时，一些节点会捕捉到数据中的模式，这些模式对于做出良好的预测是有用的，而其他节点则会捕捉到嘈杂或不相关的模式。Dropout过程在每个epoch随机移除一些节点，并对剩余的节点执行一次梯度下降步骤。通过在每个epoch移除一些节点，我们可能会移除那些已经捕捉到有用模式的节点，从而迫使其他节点承担更多的责任。
- en: To be more specific, the dropout process attaches a small probability *p* to
    each of the neurons. In each epoch of the training process, each neuron is removed
    with probability *p*, and the neural network is trained only with the remaining
    ones. Dropout is used only on the hidden
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，dropout过程为每个神经元附加一个小的概率*p*。在训练过程的每个epoch中，每个神经元以概率*p*被移除，神经网络只训练剩余的神经元。Dropout仅在隐藏层中使用，而不是在输入或输出层中。dropout过程在图10.18中说明，其中在每个训练的四个epoch中移除了一些神经元。
- en: layers, not on the input or output layers. The dropout process is illustrated
    in figure 10.18, where some neurons are removed in each of four epochs of training.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout过程如图10.18所示。在不同的epoch中，我们随机选择节点从训练中移除，以给所有节点一个更新其权重的机会，避免少数单个节点主导训练。
- en: Dropout has had great success in the practice, and I encourage you to use it
    every time you train a neural network. The packages that we use for training neural
    networks make it easy to use, as we’ll see later in this chapter.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout在实践中有很大的成功，我鼓励你在每次训练神经网络时都使用它。我们用于训练神经网络的包使其易于使用，正如我们将在本章后面看到的那样。
- en: '![](../Images/10-18.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-18.png)'
- en: Figure 10.18 The dropout process. At different epochs, we pick random nodes
    to remove from the training to give all the nodes an opportunity to update their
    weights and not have a few single nodes dominating the training.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18 dropout过程。在不同的epoch中，我们随机选择节点从训练中移除，以给所有节点一个更新其权重的机会，避免少数单个节点主导训练。
- en: 'Different activation functions: Hyperbolic tangent (tanh) and the rectified
    linear unit (ReLU)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的激活函数：双曲正切（tanh）和修正线性单元（ReLU）
- en: 'As we saw in the section “Potential problems,” the sigmoid function is a bit
    too flat, which causes problems with vanishing gradients. A solution to this problem
    is to use different activation functions. In this section, we cover two different
    activation functions that are crucial to improve our training process: the hyperbolic
    tangent (tanh) and the rectified linear unit (ReLU)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在“潜在问题”部分看到的那样，sigmoid函数有点太平坦，这会导致梯度消失问题。解决这个问题的一个方法是使用不同的激活函数。在本节中，我们介绍了两个对改进我们的训练过程至关重要的不同激活函数：双曲正切（tanh）和修正线性单元（ReLU）
- en: Hyperbolic tangent (tanh)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切（tanh）
- en: 'The *hyperbolic tangent* function tends to work better than the sigmoid function
    in practice, due to its shape, and is given by the following formula:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其形状，双曲正切函数在实践中往往比sigmoid函数表现更好，其公式如下：
- en: '![](../Images/10_18_E01.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10_18_E01.png)'
- en: Tanh is a bit less flat than sigmoid, but it still has a similar shape, as is
    shown in figure 10.19\. It provides an improvement over sigmoid, but it still
    suffers from the vanishing gradient problem.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh比sigmoid稍微平坦一些，但形状仍然相似，如图10.19所示。它对sigmoid有所改进，但仍然存在梯度消失问题。
- en: '![](../Images/10-19.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-19.png)'
- en: 'Figure 10.19 Three different activation functions used in neural networks.
    Left: The sigmoid function, represented by the Greek letter *sigma*. Middle: The
    hyperbolic tangent, or tanh. Right: The rectified linear unit, or ReLU.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19神经网络中使用的三种不同的激活函数。左：sigmoid函数，用希腊字母*sigma*表示。中：双曲正切，或tanh。右：修正线性单元，或ReLU。
- en: Rectified linear unit (ReLU)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: 'A much more popular activation that is commonly used in neural networks is
    the *rectified linear unit*, or ReLU. This one is simple: if the input is negative,
    the output is zero; otherwise, the output is equal to the input. In other words,
    it leaves nonnegative numbers alone and turns all the negative numbers into zero.
    For *x* ≥ 0, *ReLU*(*x*) = *x*, and for *x* < 0, *ReLU*(*x*) = 0\. ReLU is a good
    solution to the vanishing gradient problem, because its derivative is 1 when the
    input is positive, and thus, it is widely used in large neural networks.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中常用的一种更受欢迎的激活函数是修正线性单元，或ReLU。这个很简单：如果输入是负数，输出是零；否则，输出等于输入。换句话说，它对非负数保持不变，并将所有负数转换为零。对于*x*
    ≥ 0，*ReLU*(*x*) = *x*，对于*x* < 0，*ReLU*(*x*) = 0。ReLU是解决梯度消失问题的良好解决方案，因为当输入为正时，其导数为1，因此，它在大型神经网络中得到了广泛的应用。
- en: The great thing about these activation functions is that we can combine different
    ones in the same neural network. In one of the most common architectures, every
    node uses the ReLU activation function except for the last one, which uses the
    sigmoid. The reason for this sigmoid at the end is that if our problem is a classification
    problem, the output of the neural network must be between 0 and 1.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数的伟大之处在于我们可以在同一个神经网络中组合不同的激活函数。在最常见的架构之一中，每个节点都使用ReLU激活函数，除了最后一个节点，它使用sigmoid。在最后使用sigmoid的原因是，如果我们的问题是分类问题，神经网络的输出必须在0到1之间。
- en: 'Neural networks with more than one output: The softmax function'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个输出的神经网络：softmax函数
- en: 'So far, the neural networks we’ve worked with have had only one output. However,
    it is not hard to build a neural network that produces several outputs using the
    softmax function that we learned in the section “Classifying into multiple classes:
    The softmax function” in chapter 6\. The softmax function is a multivariate extension
    of the sigmoid, and we can use it to turn scores into probabilities.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的神经网络只有一个输出。然而，使用我们在第6章“将数据分类到多个类别：softmax函数”部分学到的softmax函数构建一个产生多个输出的神经网络并不困难。softmax函数是sigmoid的多变量扩展，我们可以用它将分数转换为概率。
- en: 'The best way to illustrate the softmax function is with an example. Imagine
    that we have a neural network whose job is to determine whether an image contains
    an aardvark, a bird, a cat, or a dog. In the final layer, we have four nodes,
    one corresponding to each animal. Instead of applying the sigmoid function to
    the scores coming from the previous layer, we apply the softmax function to all
    of them. For example, if the scores are 0, 3, 1, and 1, softmax returns the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方式是通过一个例子来说明softmax函数。想象一下，我们有一个神经网络，其任务是确定图像中是否包含河马、鸟、猫或狗。在最后一层，我们有四个节点，每个节点对应一种动物。我们不是将sigmoid函数应用于来自前一层的分数，而是将softmax函数应用于所有这些分数。例如，如果分数是0、3、1和1，softmax返回以下结果：
- en: '![](../Images/10_18_E02.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10_18_E02.png)'
- en: These results indicate that the neural network strongly believes that the image
    corresponds to a bird.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明神经网络强烈相信该图像对应于一只鸟。
- en: Hyperparameters
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Like most machine learning algorithms, neural networks use many hyperparameters
    that we can fine-tune to get them to work better. These hyperparameters determine
    how we do our training, namely, how long we want the process to go, at what speed,
    and how we choose to enter our data into the model. Some of the most important
    hyperparameters in neural networks follow:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数机器学习算法一样，神经网络使用许多超参数，我们可以微调这些超参数以使它们工作得更好。这些超参数决定了我们的训练方式，即我们希望这个过程持续多长时间，以什么速度进行，以及我们如何选择将数据输入到模型中。神经网络中最重要的几个超参数如下：
- en: '**Learning rate *η***:: the size of the step that we use during our training'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率 *η***：我们在训练过程中使用的步长大小'
- en: '**Number of epochs**: the number of steps we use for our training'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**：我们用于训练的步数'
- en: '**Batch vs. mini-batch vs. stochastic gradient descent**: how many points at
    a time enter the training process—namely, do we enter the points one by one, in
    batches, or all at the same time?'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量 vs. 小批量 vs. 随机梯度下降**：一次进入训练过程的数据点数量——也就是说，我们是逐个输入点，批量输入，还是同时全部输入？'
- en: '**Architecture**:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：'
- en: The number of layers in the neural network
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的层数
- en: The number of nodes per layer
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的节点数
- en: The activation functions used in each node
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点使用的激活函数
- en: '**Regularization parameters****:**'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化参数****：'
- en: L1 or L2 regularization
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1或L2正则化
- en: The regularization term λ
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化项 λ
- en: '**Dropout probability *p***'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout概率 *p***'
- en: We tune these hyperparameters in the same way we tune them for other algorithms,
    using methods such as grid search. In chapter 13, we elaborate on these methods
    more with a real-life example.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与其他算法相同的方式调整这些超参数，使用诸如网格搜索等方法。在第13章中，我们通过一个实际例子更详细地阐述了这些方法。
- en: Coding neural networks in Keras
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Keras中编码神经网络
- en: Now that we learned the theory behind neural networks, it’s time to put them
    in practice! Many great packages have been written for neural networks, such as
    Keras, TensorFlow, and PyTorch. These three are powerful, and in this chapter,
    we’ll use Keras due to its simplicity. We’ll build two neural networks for two
    different datasets. The first dataset contains points with two features and labels
    of 0 and 1\. The dataset is two-dimensional, so we’ll be able to look at the nonlinear
    boundary created by the model. The second dataset is a common dataset used in
    image recognition called the MNIST (Modified National Institute of Standards and
    Technology) dataset. The MNIST dataset contains handwritten digits that we can
    classify using a neural network.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了神经网络背后的理论，是时候将它们付诸实践了！已经编写了许多用于神经网络的优秀软件包，例如Keras、TensorFlow和PyTorch。这三个都非常强大，在本章中，我们将使用Keras，因为它简单易用。我们将为两个不同的数据集构建两个神经网络。第一个数据集包含具有两个特征和标签0和1的点。该数据集是二维的，因此我们将能够查看模型创建的非线性边界。第二个数据集是图像识别中常用的数据集，称为MNIST（修改后的国家标准与技术研究院）数据集。MNIST数据集包含我们可以使用神经网络进行分类的手写数字。
- en: A graphical example in two dimensions
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 二维空间中的图形示例
- en: 'In this section, we’ll train a neural network in Keras on the dataset shown
    in figure 10.20\. The dataset contains two labels, 0 and 1\. The points with label
    0 are drawn as squares, and those with label 1 are drawn as triangles. Notice
    that the points with label 1 are located mostly at the center, whereas the points
    with label 0 are located on the sides. For this type of dataset, we need a classifier
    with a nonlinear boundary, which makes it a good example for a neural network.
    The code for this section follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在图10.20所示的数据集上使用Keras训练一个神经网络。该数据集包含两个标签，0和1。标签为0的点被绘制为正方形，而标签为1的点被绘制为三角形。请注意，标签为1的点主要位于中心，而标签为0的点位于边缘。对于这种类型的数据集，我们需要一个具有非线性边界的分类器，这使得它成为神经网络的一个很好的例子。本节的代码如下：
- en: '**Notebook**: Graphical_example.ipynb'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**: Graphical_example.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Graphical_example.ipynb)'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Graphical_example.ipynb)'
- en: '**Dataset**: one_circle.csv'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**: one_circle.csv'
- en: '![](../Images/10-20.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-20.png)'
- en: Figure 10.20 Neural networks are great for nonlinearly separable sets. To test
    this, we’ll train a neural network on this circular dataset.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20 神经网络非常适合非线性可分集。为了测试这一点，我们将在这个圆形数据集上训练一个神经网络。
- en: Before we train the model, let’s look at some random rows in our data. The input
    will be called x, with features x_1 and x_2, and the output will be called y.
    Table 10.3 has some sample data points. The dataset has 110 rows.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之前，让我们查看我们数据中的某些随机行。输入将被称为x，具有特征x_1和x_2，输出将被称为y。表10.3包含一些样本数据点。该数据集有110行。
- en: Table 10.3 The dataset with 110 rows, two features, and labels of 0 and 1
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.3 包含110行、两个特征和标签0和1的数据集
- en: '| x_1 | x_2 | *y*   |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| x_1 | x_2 | *y*   |'
- en: '| –0.759416 | 2.753240 | 0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| –0.759416 | 2.753240 | 0 |'
- en: '| –1.885278 | 1.629527 | 0 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| –1.885278 | 1.629527 | 0 |'
- en: '| ... | ... | ... |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... |'
- en: '| 0.729767 | –2.479655 | 1 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 0.729767 | –2.479655 | 1 |'
- en: '| –1.715920 | –0.393404 | 1 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| –1.715920 | –0.393404 | 1 |'
- en: Before we build and train the neural networks, we must do some data preprocessing.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建和训练神经网络之前，我们必须进行一些数据预处理。
- en: 'Categorizing our data: Turning nonbinary features into binary ones'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们的数据进行分类：将非二进制特征转换为二进制
- en: 'In this dataset, the output is a number between 0 and 1, but it represents
    two classes. In Keras, it is recommended to categorize this type of output. This
    simply means that points with label 0 will now have a label [1,0], and points
    with label 1 will now have a label [0,1]. We do this using the `to_categorical`
    function as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，输出是一个介于0到1之间的数字，但它代表两个类别。在Keras中，建议对这种类型的输出进行分类。这仅仅意味着标签为0的点现在将具有标签[1,0]，而标签为1的点现在将具有标签[0,1]。我们使用以下
    `to_categorical` 函数来完成这项操作：
- en: '[PRE0]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The new labels are called `categorized_y`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 新的标签称为 `categorized_y`。
- en: The architecture of the neural network
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构
- en: 'In this section, we build the architecture of the neural network for this dataset.
    Deciding which architecture to use is not an exact science, but it is normally
    recommended to go a little bigger rather than a little smaller. For this dataset,
    we’ll use the following architecture with two hidden layers (figure 10.21):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们构建了此数据集的神经网络架构。决定使用哪种架构并不是一门精确的科学，但通常建议比实际的小一点更好。对于这个数据集，我们将使用以下架构，包含两个隐藏层（图10.21）：
- en: Input layer
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: 'Size: 2'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：2
- en: First hidden layer
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个隐藏层
- en: Size:128
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：128
- en: 'Activation function: ReLU'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：ReLU
- en: Second hidden layer
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个隐藏层
- en: 'Size: 64'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：64
- en: 'Activation function: ReLU'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：ReLU
- en: Output layer
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: 'Size: 2'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：2
- en: 'Activation function: softmax'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：softmax
- en: '![](../Images/10-211.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-211.png)'
- en: 'Figure 10.21 The architecture that we will use to classify our dataset. It
    contains two hidden layers: one of 128 and one of 64 nodes. The activation function
    between them is a ReLU, and the final activation function is a softmax.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21 我们将用于分类数据集的架构。它包含两个隐藏层：一个有128个节点，另一个有64个节点。它们之间的激活函数是ReLU，最终的激活函数是softmax。
- en: Furthermore, we’ll add dropout layers between our hidden layers with a dropout
    probability of 0.2, to prevent overfitting.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将在隐藏层之间添加dropout层，dropout概率为0.2，以防止过拟合。
- en: Building the model in Keras
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中构建模型
- en: 'Building the neural network takes only a few lines of code in Keras. First
    we import the necessary packages and functions as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中构建神经网络只需要几行代码。首先，我们按照以下方式导入必要的包和函数：
- en: '[PRE1]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, on to define the model with the architecture that we have defined in the
    previous subsection. First, we define the model with the following line:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义模型，其架构与我们之前小节中定义的架构相同。首先，我们使用以下行定义模型：
- en: '[PRE2]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defines the model
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义模型
- en: ❷ Adds the first hidden layer with a ReLU activation function
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加第一个隐藏层，并使用ReLU激活函数
- en: ❸ Adds a dropout with a probability of 0.2
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个概率为0.2的dropout
- en: ❹ Adds the second hidden layer with a ReLU activation function
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加第二个隐藏层，并使用ReLU激活函数
- en: ❺ Adds the output layer with a softmax activation function
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加输出层，并使用softmax激活函数
- en: 'Once the model is defined, we can compile it, as shown here:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了模型，我们就可以编译它，如下所示：
- en: '[PRE3]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The parameters in the `compile` function follow:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`compile`函数中的参数如下：'
- en: '`loss = ''categorical_crossentropy''`: this is the loss function, which we
    have defined as the log loss. Because our labels have more than one column, we
    need to use the multivariate version for the log loss function, called categorical
    cross-entropy.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss = ''categorical_crossentropy''`：这是损失函数，我们将其定义为对数损失。因为我们的标签有多列，我们需要使用对数损失函数的多变量版本，称为分类交叉熵。'
- en: '`optimizer = ''adam''`: packages like Keras have many built-in tricks that
    help us train a model in an optimal way. It’s always a good idea to add an optimizer
    to our training. Some of the best ones are Adam, SGD, RMSProp, and AdaGrad. Try
    this same training with other optimizers, and see how they do.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer = ''adam''`：像Keras这样的包有很多内置技巧可以帮助我们以最佳方式训练模型。添加优化器到我们的训练中总是一个好主意。其中一些最好的是Adam、SGD、RMSProp和AdaGrad。尝试使用其他优化器进行相同的训练，看看它们的性能如何。'
- en: '`metrics = [''accuracy'']`: As the training goes, we get reports on how the
    model is doing at each epoch. This flag allows us to define what metrics we want
    to see during the training, and for this example, we’ve picked the accuracy.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics = [''accuracy'']`：随着训练的进行，我们会在每个epoch得到模型表现的报告。此标志允许我们定义在训练期间想要看到的指标，在这个例子中，我们选择了准确率。'
- en: 'When we run the code, we get a summary of the architecture and number of parameters,
    as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们会得到架构和参数数量的摘要，如下所示：
- en: '[PRE4]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each row in the previous output is a layer (dropout layers are treated as separate
    layers for description purposes). The columns correspond to the type of the layer,
    the shape (number of nodes), and the number of parameters, which is precisely
    the number of weights plus the number of biases. This model has a total of 8,770
    trainable parameters.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个输出中的每一行代表一个层（为了描述目的，dropout层被视为单独的层）。列对应于层的类型、形状（节点数）和参数数量，这正好是权重数加上偏置数。此模型总共有8,770个可训练参数。
- en: Training the model
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'For training, one simple line of code suffices, shown next:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，只需要一行简单的代码，如下所示：
- en: '[PRE5]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s examine each of the inputs to this fit function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查fit函数的每个输入。
- en: '`x and categorized_y`: the features and labels, respectively.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x and categorized_y`：分别是特征和标签。'
- en: '`epochs`: the number of times we run backpropagation on our whole dataset.
    Here we do it 100 times.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：我们在整个数据集上运行反向传播的次数。在这里，我们做了100次。'
- en: '`batch_size`: the length of the batches that we use to train our model. Here
    we are introducing our data to the model in batches of 10\. For a small dataset
    like this one, we don’t need to input it in batches, but in this example, we are
    doing it for exposure.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：我们用于训练模型的批次的长度。在这里，我们以10个批次向模型介绍我们的数据。对于如此小的数据集，我们不需要分批输入，但在这个例子中，我们这样做是为了展示。'
- en: 'As the model trains, it outputs some information at each epoch, namely, the
    loss (error function) and the accuracy. For contrast, notice next how the first
    epoch has a high loss and a low accuracy, whereas the last epoch has much better
    results in both metrics:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型的训练，它会在每个时代输出一些信息，即损失（误差函数）和准确率。为了对比，注意第一个时代损失高、准确率低，而最后一个时代在这两个指标上都有更好的结果：
- en: '[PRE6]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The final accuracy of the model on the training is 0.9\. This is good, although
    remember that accuracy must be calculated in the testing set instead. I won’t
    do it here, but try splitting the dataset into a training and a testing set and
    retraining this neural network to see what testing accuracy you obtain. Figure
    10.22 shows the plot of the boundary of the neural network.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练集上的最终准确率为0.9。这很好，尽管请记住，准确率必须在测试集中计算。这里我不会做，但尝试将数据集分成训练集和测试集，并重新训练这个神经网络，看看你获得什么测试准确率。图10.22显示了神经网络边界的图。
- en: '![](../Images/10-22.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-22.png)'
- en: Figure 10.22 The boundary of the neural network classifier we trained. Notice
    that it correctly classifies most of the points, with a few exceptions.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.22 我们训练的神经网络分类器的边界。注意它正确地分类了大多数点，但有少数例外。
- en: Note that the model managed to classify the data pretty well, encircling the
    triangles and leaving the squares outside. It made some mistakes, due to noisy
    data, but this is OK. The rigged boundary hints to small levels of overfitting,
    but in general it seems like a good model.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型成功地将数据分类得相当好，将三角形包围起来，将正方形留在外面。由于数据噪声，它犯了一些错误，但这没关系。有缺陷的边界暗示了轻微的过拟合，但总的来说，这似乎是一个不错的模型。
- en: Training a neural network for image recognition
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 训练用于图像识别的神经网络
- en: 'In this section, we learn how to train a neural network for image recognition.
    The dataset we use is MNIST, a popular dataset for image recognition, which contains
    70,000 handwritten digits from 0 to 9\. The label of each image is the corresponding
    digit. Each grayscale image comes as a 28-by-28 matrix of numbers between 0 and
    255, where 0 represents white, 255 represents black, and any number in between
    represents a shade of gray. The code for this section follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何训练用于图像识别的神经网络。我们使用的数据集是MNIST，这是一个流行的图像识别数据集，包含从0到9的70000个手写数字。每张图像的标签是对应的数字。每个灰度图像都是一个28x28的数字矩阵，数值介于0到255之间，其中0代表白色，255代表黑色，介于两者之间的任何数值代表灰色。本节的代码如下：
- en: '**Notebook**: Image_recognition.ipynb'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：Image_recognition.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Image_recognition.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Image_recognition.ipynb)'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Image_recognition.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/Image_recognition.ipynb)'
- en: '**Dataset**: MNIST (comes preloaded with Keras)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：MNIST（与Keras预加载）'
- en: Loading the data
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'This dataset comes preloaded in Keras, so it is easy to load it into NumPy
    arrays. In fact, it has already been separated into training and testing sets
    of sizes 60,000 and 10,000, respectively. The following lines of code will load
    them into NumPy arrays:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集在Keras中预先加载，因此很容易将其加载到NumPy数组中。实际上，它已经被分成了大小为60,000和10,000的训练集和测试集。以下代码行将它们加载到NumPy数组中：
- en: '[PRE7]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In figure 10.23, you can see the first five images in the dataset with their
    labels.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在图10.23中，你可以看到数据集中的前五张图像及其标签。
- en: '![](../Images/10-23.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-23.png)'
- en: Figure 10.23 Some examples of handwritten digits in MNIST with their labels
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.23 MNIST中一些手写数字的示例及其标签
- en: Preprocessing the data
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Neural networks receive vectors as input instead of matrices, so we must turn
    each 28-by-28 image into a long vector of length 28² = 784\. We can use the `reshape`
    function for this, as shown next:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络接收向量作为输入而不是矩阵，因此我们必须将每个28x28的图像转换成长度为28²=784的长向量。我们可以使用`reshape`函数来完成此操作，如下所示：
- en: '[PRE8]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As with the previous example, we must also categorize the labels. Because the
    label is a number between 0 and 9, we must turn that into a vector of length 10,
    in which the entry corresponding to the label is a 1 and the rest are 0\. We can
    do this with the following lines of code:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子一样，我们也必须对标签进行分类。因为标签是一个介于0到9之间的数字，我们必须将其转换成一个长度为10的向量，其中对应标签的项是一个1，其余都是0。我们可以通过以下代码行来完成这个操作：
- en: '[PRE9]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This process is illustrated in figure 10.24.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在图10.24中得到了说明。
- en: '![](../Images/10-24.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-24.png)'
- en: Figure 10.24 Before training the neural network, we preprocess the images and
    the labels in the following way. We shape the rectangular image into a long vector
    by concatenating the rows. We then convert each label into a vector of length
    10 with only one non-zero entry in the position of the corresponding label.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24 在训练神经网络之前，我们以以下方式预处理图像和标签。我们将矩形图像通过连接行塑造成一个长向量。然后我们将每个标签转换成一个长度为10的向量，其中对应标签的位置只有一个非零项。
- en: Building and training the model
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: 'We can use the same architecture that we used in the previous model, with a
    small change, because the input is now of size 784\. In the next lines of code,
    we define the model and its architecture:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与前面模型相同的架构，只是稍作修改，因为现在的输入大小是784。在下面的代码行中，我们定义了模型及其架构：
- en: '[PRE10]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we compile and train the model for 10 epochs with a batch size of 10, as
    shown here. This model has 109,386 trainable parameters, so training for 10 epochs
    may take a few minutes on your computer.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们以10个批次的规模编译并训练模型10个周期，如图所示。这个模型有109,386个可训练参数，所以在你的电脑上训练10个周期可能需要几分钟。
- en: '[PRE11]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Looking at the output, we can see that the model has a training accuracy of
    0.9164, which is good, but let’s evaluate the testing accuracy to make sure the
    model is not overfitting.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 观察输出，我们可以看到模型的训练准确率为0.9164，这很好，但让我们评估测试准确率以确保模型没有过拟合。
- en: Evaluating the model
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We can evaluate the accuracy in the testing set by making predictions in the
    testing dataset and comparing them with the labels. The neural network outputs
    vectors of length 10 with the probabilities it assigns to each of the labels,
    so we can obtain the predictions by looking at the entry of maximum value in this
    vector, as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在测试数据集上进行预测并将它们与标签进行比较来评估测试集的准确率。神经网络输出长度为10的向量，其中包含它分配给每个标签的概率，因此我们可以通过查看这个向量中最大值的项来获得预测，如下所示：
- en: '[PRE12]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When we compare these to the labels, we get a testing accuracy of 0.942, which
    is quite good. We can do better than this with more complicated architectures,
    such as convolutional neural networks (see more of this in the next section),
    but it’s good to know that with a small, fully connected neural network, we can
    do quite well in an image recognition problem.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些与标签进行比较时，我们得到了0.942的测试准确率，这相当不错。我们可以通过更复杂的架构，如卷积神经网络（更多内容将在下一节中介绍），做得更好，但知道即使是一个小型全连接神经网络，我们也能在图像识别问题上做得相当好。
- en: Let’s now look at some predictions. In figure 10.25, we can see a correct one
    (left) and an incorrect one (right). Notice that the incorrect one is a poorly
    written image of a number 3, which also looks a bit like an 8.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一些预测结果。在图10.25中，我们可以看到一个正确的预测（左）和一个错误的预测（右）。注意，错误的预测是一个写得很差的数字3的图像，它也有一点像数字8。
- en: '![](../Images/10-25.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-25.png)'
- en: 'Figure 10.25 Left: An image of a 4 that has been correctly classified by the
    neural network. Right: An image of a 3 that has been incorrectly classified as
    an 8.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25 左：一个被神经网络正确分类的4的图像。右：一个被错误分类为8的3的图像。
- en: With this exercise, we can see that the process of training such a large neural
    network is simple with a few lines of code in Keras! Of course, there is much
    more one can do here. Play with the notebook, add more layers to the neural network,
    change the hyperparameters, and see how high you can improve the testing accuracy
    for this model!
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，我们可以看到，使用Keras的几行代码训练这样一个大型神经网络的过程非常简单！当然，这里还有很多其他可以做的事情。玩转这个笔记本，向神经网络添加更多层，改变超参数，看看你能将这个模型的测试准确率提高到多高！
- en: Neural networks for regression
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于回归的神经网络
- en: Throughout this chapter, we’ve seen how to use neural networks as a classification
    model, but neural networks are just as useful as regression models. Luckily, we
    have only two small tweaks to apply to a classification neural network to obtain
    a regression neural network. The first tweak is to remove the final sigmoid function
    from the neural network. The role of this
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何使用神经网络作为分类模型，但神经网络作为回归模型同样有用。幸运的是，我们只需要对分类神经网络进行两项小的调整，就可以得到一个回归神经网络。第一项调整是从神经网络中移除最终的sigmoid函数。这个函数的作用是将输入转换为0到1之间的数字，因此如果我们移除它，神经网络将能够返回任何数字。第二项调整是将误差函数更改为绝对误差或均方误差，因为这些是回归相关的误差函数。其他所有内容都将保持不变，包括训练过程。
- en: function is to turn the input into a number between 0 and 1, so if we remove
    it, the neural network will be able to return any number. The second tweak is
    to change the error function to the absolute error or the mean square error, because
    these are the error functions associated with regression. Everything else will
    remain the same, including the training process.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的作用是将输入转换为0到1之间的数字，因此如果我们移除它，神经网络将能够返回任何数字。第二项调整是将误差函数更改为绝对误差或均方误差，因为这些是回归相关的误差函数。其他所有内容都将保持不变，包括训练过程。
- en: As an example, let’s look at the perceptron in figure 10.7 in the section “A
    graphical representation of perceptrons.” This perceptron makes the prediction
    *ŷ* = *σ*(3*x*[1] – 2*x*[2] + 4*x*[3] + 2). If we remove the sigmoid activation
    function, the new perceptron makes the prediction *ŷ* = 3*x*[1] – 2*x*[2] + 4*x*[3]
    + 2\. This perceptron is illustrated in figure 10.26\. Notice that this perceptron
    represents a linear regression model.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看第10.7节“感知器的图形表示”中的图10.7中的感知器。这个感知器做出预测 *ŷ* = *σ*(3*x*[1] – 2*x*[2] +
    4*x*[3] + 2)。如果我们移除sigmoid激活函数，新的感知器将做出预测 *ŷ* = 3*x*[1] – 2*x*[2] + 4*x*[3] +
    2。这个感知器在图10.26中展示。注意，这个感知器代表了一个线性回归模型。
- en: '![](../Images/10-26.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-26.png)'
- en: Figure 10.26 If we remove the activation function from a perceptron, we turn
    a classification model into a linear regression model. The linear regression model
    predicts any numerical value, not just one between 0 and 1.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.26 如果我们从感知器中移除激活函数，我们将分类模型转换为线性回归模型。线性回归模型可以预测任何数值，而不仅仅是0到1之间的一个数值。
- en: 'To illustrate this process, we train a neural network in Keras on a familiar
    dataset: the dataset of housing prices in Hyderabad. Recall that in the section
    “Real-life application: Using Turi Create to predict housing prices in India,”
    in chapter 3, we trained a linear regression model to fit this dataset. The code
    for this section follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，我们在Keras上使用一个熟悉的数据集：海得拉巴的房价数据集进行神经网络训练。回想一下，在第3章的“实际应用：使用Turi Create预测印度房价”部分，我们训练了一个线性回归模型来拟合这个数据集。这一部分的代码如下：
- en: '**Notebook**: House_price_predictions_neural_network.ipynb'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：House_price_predictions_neural_network.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/House_price_predictions_neural_network.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/House_price_predictions_neural_network.ipynb)'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/House_price_predictions_neural_network.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_10_Neural_Networks/House_price_predictions_neural_network.ipynb)'
- en: '**Dataset**: Hyderabad.csv'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：Hyderabad.csv'
- en: 'The details for loading the dataset and splitting the dataset into features
    and labels can be found in the notebook. The architecture of the neural network
    that we’ll use follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集和将数据集拆分为特征和标签的详细信息可以在笔记本中找到。我们将使用的神经网络架构如下：
- en: An input layer of size 38 (the number of columns in the dataset)
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层大小为38（数据集的列数）
- en: A hidden layer of size 128 with a ReLU activation function and a dropout parameter
    of 0.2
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为128的隐藏层，带有ReLU激活函数和0.2的dropout参数
- en: A hidden layer of size 64 with a ReLU activation function and a dropout parameter
    of 0.2
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为64的隐藏层，带有ReLU激活函数和0.2的dropout参数
- en: An output layer of size 1 with no activation function
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层大小为1，没有激活函数
- en: '[PRE13]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To train the neural network, we use the mean square error function and the
    Adam optimizer. We’ll train for 10 epochs using a batch size of 10, as shown here:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经网络，我们使用均方误差函数和Adam优化器。我们将使用10个批次的10个epoch进行训练，如下所示：
- en: '[PRE14]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This neural network reports a root mean square error of 5,535,425 in the training
    dataset. Study this model further by adding a testing set, and play with the architecture,
    and see how much you can improve it!
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络在训练数据集上报告了553,542.5的均方根误差。通过添加测试集并调整架构来进一步研究这个模型，看看你能将其改进多少！
- en: Other architectures for more complex datasets
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更复杂数据集的其他架构
- en: Neural networks are useful in many applications, perhaps more so than any other
    machine learning algorithm currently. One of the most important qualities of neural
    networks is their versatility. We can modify the architectures in very interesting
    ways to better fit our data and solve our problem. To find out more about these
    architectures, check out *Grokking Deep Learning* by Andrew Trask (Manning, 2019)
    and a set of videos available in appendix C or at [https://serrano.academy/neural-networks/](https://serrano.academy/neural-networks/).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在许多应用中都非常有用，可能比目前任何其他机器学习算法都要有用。神经网络最重要的特性之一是其多功能性。我们可以以非常有趣的方式修改架构，以更好地适应我们的数据和解决问题。要了解更多关于这些架构的信息，请参阅安德鲁·特拉斯特（Andrew
    Trask）的《Grokking Deep Learning》（Manning, 2019）以及附录C中或[https://serrano.academy/neural-networks/](https://serrano.academy/neural-networks/)提供的视频集。
- en: 'How neural networks see: Convolutional neural networks (CNN)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络如何“看”：卷积神经网络（CNN）
- en: 'As we learned in this chapter, neural networks are great with images, and we
    can use them in many applications, such as the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中学到的，神经网络在图像处理方面非常出色，我们可以在许多应用中使用它们，如下所示：
- en: '**Image recognition**: the input is an image, and the output is the label on
    the image. Some famous datasets used for image recognition follow:'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：输入是一个图像，输出是图像上的标签。以下是一些用于图像识别的著名数据集：'
- en: 'MNIST: handwritten digits in 28-by-28 gray scale images'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST：28-by-28灰度图像中的手写数字
- en: 'CIFAR-10: color images, with 10 labels such as airplane, automobile, and so
    on, in 32-by-32 images'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-10：彩色图像，有10个标签，如飞机、汽车等，在32-by-32的图像中
- en: 'CIFAR-100: similar to CIFAR-10, but with 100 labels such as aquatic mammals,
    flowers, and so on'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-100：类似于CIFAR-10，但有100个标签，如水生哺乳动物、花卉等
- en: '**Semantic segmentation**: the input is an image, and the output is not only
    the labels of the things found in the image but also their location inside the
    image. Normally, the neural network outputs this location as a bounded rectangle
    in the image.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**：输入是一个图像，输出不仅包括图像中找到的物体的标签，还包括它们在图像中的位置。通常，神经网络将这个位置输出为图像中的一个边界矩形。'
- en: In the section “Training a neural network for image recognition,” we built a
    small, fully connected neural network that classified the MNIST dataset quite
    well. However, for more complicated images, such as pictures and faces, a neural
    network like this one won’t do well because turning the image into a long vector
    loses a lot of information. For these complicated images, we need different architectures,
    and this is where convolutional neural networks come to help us.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在“训练用于图像识别的神经网络”这一节中，我们构建了一个小型、全连接的神经网络，它对MNIST数据集的分类效果相当不错。然而，对于更复杂的图像，如图片和面部，这样的神经网络表现不会很好，因为将图像转换为长向量会丢失很多信息。对于这些复杂的图像，我们需要不同的架构，这就是卷积神经网络帮助我们的地方。
- en: For the details on neural networks, review the resources in appendix C, but
    here is a rough outline of how they work. Imagine that we have a large image that
    we want to process. We take a smaller window, say 5-by-5, or 7-by-7 pixels, and
    swipe it through the large image. Every time we pass it through, we apply a formula
    called a convolution. Thus, we end with a slightly smaller filtered image, which
    in some way summarizes the previous one—a convolutional layer. A convolutional
    neural network consists of several of these convolutional layers, followed by
    some fully connected layers.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的详细信息，请参阅附录C中的资源，但以下是他们工作的大致概述。想象一下，我们有一个大图像需要处理。我们取一个较小的窗口，比如5-by-5或7-by-7像素，然后将其在大型图像上滑动。每次通过时，我们应用一个称为卷积的公式。因此，我们最终得到一个稍微小一点的过滤图像，它在某种程度上总结了上一个图像——这是一个卷积层。卷积神经网络由几个这样的卷积层和一些全连接层组成。
- en: When it comes to complicated images, we normally wouldn’t go about training
    a neural network from scratch. A useful technique called *transfer learning* consists
    of starting with a pretrained network and using our data to tweak some of its
    parameters (usually the last layer). This technique tends to work well and at
    a low computational cost. Networks such as InceptionV3, ImageNet, ResNet, and
    VGG have been trained by companies and research groups with large computational
    power, so it’s highly recommended for us to use them.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到复杂的图像时，我们通常不会从头开始训练神经网络。一种有用的技术称为**迁移学习**，它包括从一个预训练的网络开始，并使用我们的数据来调整其一些参数（通常是最后一层）。这种技术通常效果很好，且计算成本较低。InceptionV3、ImageNet、ResNet和VGG等网络都是由拥有强大计算能力的公司和研究机构训练的，因此我们强烈推荐使用它们。
- en: 'How neural networks talk: Recurrent neural networks (RNN), gated recurrent
    units (GRU), and long short-term memory networks (LSTM)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络如何说话：循环神经网络（RNN）、门控循环单元（GRU）和长短期记忆网络（LSTM）
- en: 'One of the most fascinating applications of neural networks is when we can
    get them to talk to us or understand what we say. This involves listening to what
    we say or reading what we write, analyzing it, and being able to respond or act.
    The ability for computers to understand and process language is called natural
    language processing. Neural networks have had a lot of success in natural language
    processing. The sentiment analysis example at the beginning of this chapter is
    part of natural language processing, because it entails understanding sentences
    and determining whether they have positive or negative sentiment. As you can imagine,
    many more cutting-edge applications exist, such as the following:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最迷人的应用之一是当我们能让它们与我们交谈或理解我们所说的话时。这涉及到倾听我们说的话或阅读我们写的内容，分析它，并能够做出回应或行动。计算机理解和处理语言的能力被称为自然语言处理。神经网络在自然语言处理方面取得了许多成功。本章开头提到的情感分析示例就是自然语言处理的一部分，因为它涉及到理解句子并确定它们是否具有积极或消极的情感。正如你可以想象的那样，还存在许多更前沿的应用，例如以下内容：
- en: '**Machine translation**: translating sentences from various languages into
    others.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：将句子从一种语言翻译成另一种语言。'
- en: '**Speech recognition**: decoding human voice and turning it into text.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：解码人类语音并将其转换为文本。'
- en: '**Text summarization**: summarizing large texts into a few paragraphs.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**：将大量文本总结成几段。'
- en: '**Chatbots**: a system that can talk to humans and answer questions. These
    are not yet perfected, but useful chatbots operate in specific topics, such as
    customer support.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人**：一种可以与人类交谈并回答问题的系统。这些系统尚未完善，但有用的聊天机器人通常在特定主题下运行，例如客户支持。'
- en: The most useful architectures that work well for processing texts are *recurrent
    neural networks*, and some more advanced versions of them called *long short-term
    memory networks* (LSTM) and *gated recurrent units* (GRU). To get an idea of what
    they are, imagine a neural network where the output is plugged back into the network
    as part of the inputs. In this way, neural networks have a memory, and when trained
    properly, this memory can help them make sense of the topic in the text.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处理文本来说，最有效的架构是*循环神经网络*，以及它们的更高级版本，称为*长短期记忆网络*（LSTM）和*门控循环单元*（GRU）。为了理解它们是什么，想象一个神经网络，其中输出被作为输入的一部分重新连接到网络中。这样，神经网络就有了记忆，当训练得当，这种记忆可以帮助它们理解文本中的主题。
- en: 'How neural networks paint paintings: Generative adversarial networks (GAN)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络如何绘制画作：生成对抗网络（GAN）
- en: One of the most fascinating applications of neural networks is generation. So
    far, neural networks (and most other ML models in this book) have worked well
    in predictive machine learning, namely, being able to answer questions such as
    “How much is that?” or “Is this a cat or a dog?” However, in recent years, many
    advances have occurred in a fascinating area called *generative machine learning*.
    Generative machine learning is the area of machine learning that teaches the computer
    how to create things, rather than simply answer questions. Actions such as painting
    a painting, composing a song, or writing a story represent a much higher level
    of understanding of the world.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最迷人的应用之一是生成。到目前为止，神经网络（以及本书中大多数其他机器学习模型）在预测机器学习方面表现良好，即能够回答诸如“那是什么价格？”或“这是猫还是狗？”等问题。然而，近年来，在被称为*生成机器学习*的迷人领域取得了许多进展。生成机器学习是机器学习的一个领域，它教会计算机如何创造事物，而不仅仅是回答问题。绘画、作曲或写故事等行为代表了理解世界的一个更高层次。
- en: Without a doubt, one of the most important advances in the last few years has
    been the development of *generative adversarial networks*, or GANs. Generative
    adversarial networks have shown fascinating results when it comes to image generation.
    GANs consist of two competing networks, the generator and the discriminator. The
    generator attempts to generate real-looking images, whereas the discriminator
    tries to tell the real images and the fake images apart. During the training process,
    we feed real images to the discriminator, as well as fake images generated by
    the generator. When applied to a dataset of human faces, this process results
    in a generator that can generate some very real-looking faces. In fact, they look
    so real that humans often have a hard time telling them apart. Test yourself against
    a GAN—[https://www.whichfaceisreal.com](https://www.whichfaceisreal.com).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，在过去的几年里，最重要的进步之一是**生成对抗网络**（GANs）的发展。在图像生成方面，生成对抗网络展现出了令人着迷的结果。GANs由两个相互竞争的网络组成，即生成器和判别器。生成器试图生成看起来真实的图像，而判别器则试图区分真实图像和伪造图像。在训练过程中，我们向判别器提供真实图像以及生成器生成的伪造图像。当应用于人脸数据集时，这个过程产生了一个能够生成一些非常逼真的人脸的生成器。事实上，它们看起来如此逼真，以至于人类往往很难区分它们。你可以通过一个GAN来测试自己——[https://www.whichfaceisreal.com](https://www.whichfaceisreal.com)。
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Neural networks are a powerful model used for classification and regression.
    A neural network consists of a set of perceptrons organized in layers, where the
    output of one layer serves as input to the next layer. Their complexity allows
    them to achieve great success in applications that are difficult for other machine
    learning models.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是一种用于分类和回归的强大模型。神经网络由一组按层组织的感觉器组成，其中一层输出的结果作为下一层的输入。它们的复杂性使它们能够在其他机器学习模型难以应用的领域中取得巨大成功。
- en: Neural networks have cutting-edge applications in many areas, including image
    recognition and text processing.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在许多领域都有前沿的应用，包括图像识别和文本处理。
- en: The basic building block of a neural network is the perceptron. A perceptron
    receives several values as inputs, and outputs one value by multiplying the inputs
    by weights, adding a bias, and applying an activation function.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的基本构建块是感知器。感知器接收几个值作为输入，通过将输入乘以权重、添加偏差并应用激活函数来输出一个值。
- en: Popular activation functions include sigmoid, hyperbolic tangent, softmax, and
    the rectified linear unit (ReLU). They are used between layers in a neural network
    to break linearity and help us build more complex boundaries.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的激活函数包括sigmoid、双曲正切、softmax和修正线性单元（ReLU）。它们用于神经网络中的层之间，以打破线性并帮助我们构建更复杂的边界。
- en: The sigmoid function is a simple function that sends any real number to the
    interval between 0 and 1\. The hyperbolic tangent is similar, except the output
    is the interval between –1 and 1\. Their goal is to squish our input into a small
    interval so that our answers can be interpreted as a category. They are mostly
    used for the final (output) layer in a neural network. Due to the flatness of
    their derivatives, they may cause problems with vanishing gradients.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个简单的函数，它将任何实数映射到0和1之间的区间。双曲正切函数类似，但输出是-1和1之间的区间。它们的目标是将我们的输入挤压到一个小区间内，以便我们的答案可以被解释为类别。它们主要用于神经网络中的最终（输出）层。由于它们的导数平坦，它们可能会导致梯度消失问题。
- en: The ReLU function is a function that sends negative numbers to 0, and non-negative
    numbers to themselves. It showed great success in reducing the vanishing gradient
    problem, and thus it is used more in training neural networks than the sigmoid
    function or the hyperbolic tangent function.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU函数是一个将负数映射到0，非负数映射到自身的函数。它在减少梯度消失问题方面取得了巨大成功，因此比sigmoid函数或双曲正切函数更多地用于神经网络训练。
- en: Neural networks have a very complex structure, which makes them hard to train.
    The process we use to train them, called backpropagation, has shown great success.
    Backpropagation consists of taking the derivative of the loss function and finding
    all the partial derivatives with respect to all the weights of the model. We then
    use these derivatives to update the weights of the model iteratively to improve
    its performance.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络具有非常复杂的结构，这使得它们难以训练。我们用来训练它们的称为反向传播的过程已经取得了巨大成功。反向传播包括对损失函数求导，并找到所有关于模型所有权重的偏导数。然后我们使用这些导数迭代地更新模型的权重，以提高其性能。
- en: Neural networks are prone to overfitting and other problems such as vanishing
    gradients, but we can use techniques such as regularization and dropout to help
    reduce these problems.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络容易过拟合和其他问题，如梯度消失，但我们可以使用正则化和dropout等技术来帮助减少这些问题。
- en: We have some useful packages to train neural networks, such as Keras, TensorFlow,
    and PyTorch. These packages make it very easy for us to train neural networks,
    because we have to define only the architecture of the model and the error functions,
    and they take care of the training. Furthermore, they have many built-in cutting-edge
    optimizers that we can take advantage of.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一些有用的包来训练神经网络，例如Keras、TensorFlow和PyTorch。这些包使我们能够非常容易地训练神经网络，因为我们只需要定义模型的架构和误差函数，它们会负责训练。此外，它们有许多内置的先进优化器，我们可以利用这些优化器。
- en: Exercises
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 10.1
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 10.1
- en: The following image shows a neural network in which all the activations are
    sigmoid functions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了一个所有激活都是sigmoid函数的神经网络。
- en: '![](../Images/10-unnumb-2.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-unnumb-2.png)'
- en: What would this neural network predict for the input (1,1)?
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络会对输入(1,1)做出什么预测？
- en: Exercise 10.2
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 10.2
- en: 'As we learned in exercise 5.3, it is impossible to build a perceptron that
    mimics the XOR gate. In other words, it is impossible to fit the following dataset
    with a perceptron and obtain 100% accuracy:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在练习5.3中学到的，不可能构建一个模拟XOR门的感知器。换句话说，不可能使用感知器拟合以下数据集并获得100%的准确率：
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0 | 0 | 0 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: This is because the dataset is not linearly separable. Using a neural network
    of depth 2, build a perceptron that mimics the XOR gate shown previously. As the
    activation functions, use the step function instead of the sigmoid function to
    get discrete outputs.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为数据集不是线性可分的。使用深度为2的神经网络，构建一个模拟之前显示的XOR门的感知器。作为激活函数，使用步函数而不是sigmoid函数以获得离散输出。
- en: hint This will be hard to do using a training method; instead, try eyeballing
    the weights. Try (or search online how) to build an XOR gate using AND, OR, and
    NOT gates, and use the results of exercise 5.3 to help you.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：使用训练方法来做这个可能会很困难；相反，尝试凭直觉调整权重。尝试（或在网上搜索如何）使用AND、OR和NOT门构建一个XOR门，并使用练习 5.3
    的结果来帮助你。
- en: Exercise 10.3
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 10.3
- en: At the end of the section “A graphical representation of neural networks,” we
    saw that the neural network in figure 10.13 with the activation function doesn’t
    fit the dataset in table 10.1 because the point (1,1) is misclassified.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在“神经网络图形表示”部分的结尾，我们看到了图10.13中的神经网络，其激活函数与表10.1中的数据集不匹配，因为点(1,1)被错误分类。
- en: Verify that this is the case.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证这是否是这种情况。
- en: Change the weights so that the neural network classifies every point correctly.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变权重，使神经网络能够正确分类每个点。
