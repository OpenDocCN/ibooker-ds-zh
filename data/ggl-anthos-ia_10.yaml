- en: 10 Networking environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 网络环境
- en: Ameer Abbas
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Ameer Abbas
- en: This chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Anthos cloud networking and hybrid connectivity between multiple cloud environments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos云网络和多个云环境之间的混合连接
- en: Anthos Kubernetes and GKE networking, including Dataplane v2
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos Kubernetes和GKE网络，包括Dataplane v2
- en: Anthos multicluster networking, including service discovery and routing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos多集群网络，包括服务发现和路由
- en: Service-to-service and client-to-service connectivity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务间和客户端到服务连接
- en: 'Anthos networking can be divided into four sections. Each section provides
    a layer of connectivity between entities such as environments (e.g., public cloud
    and on-prem), Anthos GKE clusters, and service-to-service communications. The
    four layers, shown in figure 10.1, follow:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos网络可以分为四个部分。每个部分为环境（例如，公共云和本地）、Anthos GKE集群以及服务间通信等实体提供一层连接。如图10.1所示，这四层包括：
- en: '*Cloud networking and hybrid connectivity*—Addresses the lowest layer of networking
    and covers how different infrastructure environments can be interconnected.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云网络和混合连接*——涉及网络的最底层，涵盖不同基础设施环境如何相互连接。'
- en: '*Anthos GKE networking*—Anthos GKE clusters come in a variety of implementations,
    depending on the infrastructure in which they are deployed. This section covers
    Anthos GKE cluster networking, including how ingress works in various environments.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos GKE网络*——Anthos GKE集群的实施方式多种多样，取决于它们部署的基础设施。本节涵盖Anthos GKE集群网络，包括在各种环境中如何进行入口操作。'
- en: '*Multicluster networking*—Addresses how various Anthos GKE clusters connect
    to each other. Anthos GKE clusters may be deployed in a single infrastructure
    environment (e.g., in GCP), or they can be deployed across multiple infrastructure
    environments (e.g., GCP and in an on-prem data center).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多集群网络*——讨论各种Anthos GKE集群如何相互连接。Anthos GKE集群可以部署在单一基础设施环境中（例如，在GCP中），也可以部署在多个基础设施环境中（例如，GCP和本地数据中心）。'
- en: '*Service and client connectivity*—Addresses how applications running on Anthos
    connect to each other. This section also addresses how clients and services running
    outside of Anthos can connect to services running inside the Anthos platform.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务和客户端连接*——讨论在Anthos上运行的应用程序如何相互连接。本节还讨论运行在Anthos平台外部的客户端和服务如何连接到Anthos平台内部运行的服务。'
- en: '![10-01](../../OEBPS/Images/10-01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![10-01](../../OEBPS/Images/10-01.png)'
- en: Figure 10.1 Four layers of Anthos networking
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 Anthos网络的四层
- en: 10.1 Cloud networking and hybrid connectivity
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 云网络和混合连接
- en: 'This section addresses various aspects of network connectivity at the infrastructure
    environment level. Anthos is a multicloud platform and can run in one or more
    public and private cloud environments. At the infrastructure layer, you can deploy
    Anthos in the following ways:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论基础设施环境层面的网络连接的各个方面。Anthos是一个多云平台，可以在一个或多个公共和私有云环境中运行。在基础设施层，您可以以下方式部署Anthos：
- en: '*In a single cloud environment*—For example, GCP or on-prem data center or
    even in another public cloud'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在单一云环境中*——例如，GCP或本地数据中心，甚至在其他公共云中'
- en: '*In a multi-/hybrid cloud environment*—For example, a platform deployed in
    GCP and in one or more on-prem data centers'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在多/混合云环境中*——例如，在GCP和一家或多家本地数据中心部署的平台'
- en: 10.1.1 Single-cloud deployment
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 单云部署
- en: The Anthos platform can be deployed in a single-cloud environment. The single-cloud
    environment can be on GCP, another public or private cloud, or on-prem data centers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos平台可以部署在单云环境中。单云环境可以是GCP、其他公共或私有云，或本地数据中心。
- en: Anthos on GCP
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos在GCP上
- en: Anthos on GCP uses resources that are placed within a virtual private cloud
    (VPC; [https://cloud.google.com/vpc](https://cloud.google.com/vpc)). You can configure
    VPCs in GCP in multiple ways. Depending on the needs of the company, a single
    VPC (in a single GCP project) might suffice. In a more complex design, shared
    VPC, peered VPC, or even multiple disparate VPCs are required. The Anthos platform
    can work with a variety of VPC designs. Choosing the right VPC architecture up
    front is important because it may pose scalability and operational consequences
    later. We discuss various VPC design and decision criteria next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 在 GCP 中使用放置在虚拟专用云（VPC；[https://cloud.google.com/vpc](https://cloud.google.com/vpc)）内的资源。您可以在
    GCP 中以多种方式配置 VPC。根据公司的需求，一个单一 VPC（在单个 GCP 项目中）可能就足够了。在更复杂的设计中，可能需要共享 VPC、对等 VPC，甚至多个不同的
    VPC。Anthos 平台可以与各种 VPC 设计协同工作。在选择正确的 VPC 架构时，事先考虑是很重要的，因为这可能会在以后带来可扩展性和运营方面的后果。我们将在下一节讨论各种
    VPC 设计和决策标准。
- en: Single VPC
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 单一 VPC
- en: Single VPC is the simplest design. For small environments, where everything
    is contained in a single GCP project, you may choose a single VPC. A single VPC
    results in a flat network, meaning all resources using the VPC are on the same
    network. You can control connectivity between resources via security features
    at various layers in the Anthos platform. For example, you can use a VPC firewall
    ([https://cloud.google.com/vpc/docs/firewalls](https://cloud.google.com/vpc/docs/firewalls))
    at the network layer, Kubernetes NetworkPolicies ([http://mng.bz/ZoWj](http://mng.bz/ZoWj)*)*
    inside Kubernetes Dataplane, and Anthos Service Mesh ([http://mng.bz/RlOn](http://mng.bz/RlOn))
    authentication and authorization policies at the service mesh layer. With this
    approach, multiple teams use resources in the same GCP project and same VPC. Single
    VPC design, shown in figure 10.2, also simplifies network administration. All
    resources, whether inside or outside of the Anthos platform, reside on the same
    flat network and can communicate easily as allowed via security rules. No additional
    configuration is required to connect resources together.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单一 VPC 是最简单的设计。对于所有内容都包含在单个 GCP 项目中的小型环境，您可以选择单一 VPC。单一 VPC 导致网络扁平化，这意味着使用 VPC
    的所有资源都在同一网络中。您可以通过 Anthos 平台各个层次的安全功能来控制资源之间的连接性。例如，您可以在网络层使用 VPC 防火墙（[https://cloud.google.com/vpc/docs/firewalls](https://cloud.google.com/vpc/docs/firewalls)），在
    Kubernetes Dataplane 内使用 Kubernetes NetworkPolicies ([http://mng.bz/ZoWj](http://mng.bz/ZoWj)*)*，以及在服务网格层使用
    Anthos Service Mesh ([http://mng.bz/RlOn](http://mng.bz/RlOn)) 认证和授权策略。采用这种方法，多个团队可以在同一
    GCP 项目和同一 VPC 中使用资源。如图 10.2 所示的单一 VPC 设计也简化了网络管理。所有资源，无论在 Anthos 平台内部还是外部，都位于同一扁平网络中，并且可以按照安全规则允许的方式轻松通信。无需额外的配置即可连接资源。
- en: '![10-02](../../OEBPS/Images/10-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![10-02](../../OEBPS/Images/10-02.png)'
- en: Figure 10.2 Single VPC architecture in GCP
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 GCP 中的单一 VPC 架构
- en: The primary challenge with a single VPC design is scale. Although a single VPC
    design might be sufficient for small- to medium-sized implementations, large implementations
    may not be possible because you will start to hit VPC limits ([https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota)).
    As the organization grows, separate projects may need to be created for separate
    products, teams, or environments. A single VPC design does not support multiproject
    environments. Depending on the industry, regulations might exist that prohibit
    hosting all resources in a single VPC and require some level of network or project
    separation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 单一 VPC 设计的主要挑战是可扩展性。尽管单一 VPC 设计可能适用于中小型实施，但对于大型实施来说，可能无法实现，因为你将开始遇到 VPC 限制（[https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota)）。随着组织的增长，可能需要为不同的产品、团队或环境创建单独的项目。单一
    VPC 设计不支持多项目环境。根据行业，可能存在禁止在单一 VPC 中托管所有资源的法规，并要求进行某种程度的网络或项目分离。
- en: When designing your network structure for Anthos, you must understand and account
    for the longevity of the platform up front. For example, in two to four years,
    how much will the platform scale, and will any other restrictions arise that need
    to be considered, quota- or regulation-wise?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当为 Anthos 设计您的网络结构时，您必须事先了解并考虑平台的长期性。例如，在两到四年后，平台将扩展多少，是否会出现需要考虑的任何其他限制，无论是配额还是法规方面的？
- en: Shared VPC
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 共享 VPC
- en: Using a shared VPC ([https://cloud.google.com/vpc/docs/shared-vpc](https://cloud.google.com/vpc/docs/shared-vpc))
    is the recommended way to provision a network on GCP for the Anthos platform.
    You can use a shared VPC design, shown in figure 10.3, for both simple and complex
    (large-scale and multitenant) Anthos environments. A shared VPC allows a single
    VPC to be shared across multiple projects, which results in a single flat network
    space shared by multiple tenants, each within their own project. Separating GCP
    projects by products/tenants allows for granular IAM permissioning at the project
    level. At the same time, resources in multiple projects can still connect to each
    other (if allowed) as if they were on a single network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享VPC（[https://cloud.google.com/vpc/docs/shared-vpc](https://cloud.google.com/vpc/docs/shared-vpc)）是GCP上为Anthos平台配置网络的推荐方式。您可以使用图10.3中显示的共享VPC设计，适用于简单和复杂（大规模和多租户）的Anthos环境。共享VPC允许单个VPC在多个项目中共享，从而在多个租户之间共享单个扁平网络空间，每个租户都在自己的项目中。通过产品/租户分离GCP项目，可以在项目级别实现细粒度的IAM权限控制。同时，多个项目中的资源仍然可以相互连接（如果允许），就像它们在单个网络中一样。
- en: '![10-03](../../OEBPS/Images/10-03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![10-03](../../OEBPS/Images/10-03.png)'
- en: Figure 10.3 Shared VPC architecture in GCP
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 GCP中的共享VPC架构
- en: A network host project contains a centralized “shared” VPC. All network resources
    are located in this network host project, including subnets, firewall rules, and
    network permissions. A centralized networking team owns and controls the network
    host project. This arrangement ensures that the organization’s network best practices
    are enforced by a single qualified team of networking experts. Multiple service
    projects can then use network resources from the host project. Subnets are shared
    from the network host project to multiple service projects and are used by resources
    inside each service project.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 网络主机项目包含一个集中的“共享”VPC。所有网络资源都位于此网络主机项目中，包括子网、防火墙规则和网络权限。一个集中的网络团队拥有并控制网络主机项目。这种安排确保由一个合格的网络专家团队强制执行组织的网络最佳实践。然后，多个服务项目可以使用主机项目的网络资源。子网从网络主机项目共享到多个服务项目，并由每个服务项目内的资源使用。
- en: For Anthos on GCP, it is recommended to create a service project and name it
    similar to platform_admins. The Anthos platform (all GKE clusters) resides inside
    the platform_admins project. The platform_admins project (as the name suggests)
    is owned by the platform administrator team who manages and maintains the life
    cycle of the Anthos platform. Platform administrators are one of many tenants
    of the network host project. Similarly, products and environments get their own
    service projects. Anthos is a shared multitenant platform where each tenant gets
    a “landing zone” in which to run their services on Anthos. A landing zone is a
    set of resources required to run a service on the Anthos platform and is typically
    one (or more) namespaces in one (or more) Anthos GKE clusters and a set of policies
    required to run that service. All non-Anthos resources (belonging to a service)
    are provisioned and managed in the individual service’s GCP project. This way,
    multiple tenants can have their own projects for non-Anthos resources, and they
    can all share a single Anthos GKE on GCP clusters. Using a shared VPC allows Anthos
    and non-Anthos resources to connect to each other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GCP上的Anthos，建议创建一个服务项目，并将其命名为类似platform_admins。Anthos平台（所有GKE集群）位于platform_admins项目中。platform_admins项目（正如其名称所示）由平台管理员团队拥有，该团队负责管理和维护Anthos平台的生命周期。平台管理员是网络主机项目的众多租户之一。同样，产品和环境也有它们自己的服务项目。Anthos是一个共享的多租户平台，每个租户都获得一个“着陆区”，在其中运行他们的服务。着陆区是一组在Anthos平台上运行服务所需的资源，通常是（一个或多个）Anthos
    GKE集群中的一个（或多个）命名空间以及运行该服务所需的一组策略。所有非Anthos资源（属于一个服务）都在单个服务项目的GCP项目中配置和管理。这样，多个租户可以拥有自己的非Anthos资源项目，并且他们都可以在GCP集群上共享单个Anthos
    GKE。使用共享VPC允许Anthos和非Anthos资源相互连接。
- en: Multiple VPC
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多个VPC
- en: The two previous VPC implementations result in a flat network where all resources
    are provisioned in a single logical VPC. In some cases, security or regulatory
    restrictions may require the separation of resources into multiple VPCs. The company
    or organization may also want each team or product to manage their own network.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个VPC实现导致网络扁平化，所有资源都在单个逻辑VPC中配置。在某些情况下，安全或监管限制可能需要将资源分离到多个VPC中。公司或组织还可能希望每个团队或产品管理自己的网络。
- en: '![10-04](../../OEBPS/Images/10-04.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![10-04](../../OEBPS/Images/10-04.png)'
- en: Figure 10.4 Multiple VPC architecture in GCP
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 GCP中的多个VPC架构
- en: 'The Anthos GKE on GCP platform can be installed in one VPC. Multitenancy allows
    you to share the Anthos GKE on GCP with multiple tenants, as shown in figure 10.4\.
    You might have to connect services running on the Anthos platform to services
    outside of the platform. In this design, these services run in different VPCs.
    For example, services running on Anthos GKE in GCP may be running on a VPC called
    anthos_vpc and non-Anthos resources may be running on a VPC called product_1_
    vpc. You can connect these services in the following ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE在GCP平台可以安装在一个VPC中。多租户允许您与多个租户共享Anthos GKE on GCP，如图10.4所示。您可能需要将运行在Anthos平台上的服务连接到平台外的服务。在这个设计中，这些服务运行在不同的VPC中。例如，运行在GCP中Anthos
    GKE上的服务可能运行在名为anthos_vpc的VPC中，而非Anthos资源可能运行在名为product_1_vpc的VPC中。您可以通过以下方式连接这些服务：
- en: '*IPsec VPN* ([http://mng.bz/2a4N](http://mng.bz/2a4N))—You can create an IPsec
    VPN tunnel between two VPCs. IPsec VPN traffic flows over the public internet
    in a secure manner. Traffic traveling between the two networks is encrypted by
    one VPN gateway and then decrypted by the other VPN gateway to protect your data
    as it travels over the internet. Flowing through the public internet may result
    in performance degradation, however. IPsec VPN can be helpful for large-scale
    environments.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IPsec VPN* ([http://mng.bz/2a4N](http://mng.bz/2a4N))—您可以在两个VPC之间创建一个IPsec
    VPN隧道。IPsec VPN流量以安全的方式在公共互联网上传输。两个网络之间的流量由一个VPN网关加密，然后由另一个VPN网关解密，以保护数据在互联网上的传输。然而，通过公共互联网传输可能会导致性能下降。IPsec
    VPN对于大规模环境可能很有帮助。'
- en: '*VPC Network Peering* ([http://mng.bz/1MvZ](http://mng.bz/1MvZ))—You can peer
    multiple VPCs to allow VPC interconnectivity without having to connect VPCs via
    IPsec VPN. VPC peering offers the same data plane and performance characteristics
    as a single VPC but with boundaries for administration (security and configurations),
    resulting in better security and performance for VPC-to-VPC traffic. VPC Network
    Peering requires coordination between the network admins of the two VPCs. It does
    not allow for overlapping IP addresses. Also, both VPC owners maintain separate
    firewalls with desired rules allowing traffic between subnets belonging to the
    two VPCs.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VPC网络对等连接* ([http://mng.bz/1MvZ](http://mng.bz/1MvZ))—您可以对多个VPC进行对等连接，以实现VPC互连，而无需通过IPsec
    VPN连接VPC。VPC对等连接提供了与单个VPC相同的数据平面和性能特性，但具有管理（安全和配置）边界，从而为VPC到VPC流量提供更好的安全性和性能。VPC网络对等连接需要两个VPC的网络管理员之间的协调。它不允许重叠的IP地址。此外，两个VPC的所有者都维护有单独的防火墙，并设置所需的规则以允许两个VPC的子网之间的流量。'
- en: '*Public internet and secure ingress*—If VPC peering or VPN is not an option,
    services can communicate over the public internet. In this case, higher-level
    functionality like Anthos Service Mesh can be used to encrypt traffic between
    networks using TLS or mTLS (mutual TLS). This method works well if only a small
    number of services require connectivity across VPCs because this method requires
    per-service (destination service) configuration as opposed to the previous two
    methods, which connect two networks at the TCP/IP layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共互联网和安全的入口*—如果VPC对等连接或VPN不是可行的选项，服务可以通过公共互联网进行通信。在这种情况下，可以使用高级功能，如Anthos
    Service Mesh，使用TLS或mTLS（相互TLS）加密网络之间的流量。如果只有少数服务需要跨VPC的连接，这种方法效果很好，因为这种方法需要为每个服务（目标服务）进行配置，而前两种方法是在TCP/IP层连接两个网络。'
- en: Anthos on a single non-GCP environment
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个非GCP环境中部署Anthos
- en: 'You can deploy Anthos in a variety of non-GCP environments, including on-prem
    data centers, public clouds, and private clouds. At the infrastructure layer,
    you should consider two primary network designs: single and multiple.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在各种非GCP环境中部署Anthos，包括本地数据中心、公共云和私有云。在基础设施层，您应该考虑两种主要的网络设计：单网络和多网络。
- en: Single flat network
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 单一扁平网络
- en: As the name suggests, flat networks are composed of a single logical network
    space where both Anthos and non-Anthos resources (e.g., VMs running outside of
    the Anthos platform) reside on the same network. A flat network is a set of subnets
    connected by routers and switches where each IP endpoint can switch or route to
    another endpoint given the correct routing (and firewall) rules. A single GCP
    VPC is an example of a flat network where you may have multiple subnets and routing/firewall
    rules to allow routing between any two endpoints.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，扁平网络由一个单一的逻辑网络空间组成，其中既包含 Anthos 资源，也包含非 Anthos 资源（例如，在 Anthos 平台外运行的虚拟机）。扁平网络是一组通过路由器和交换机连接的子网，每个
    IP 端点在正确的路由（和防火墙）规则下可以切换或路由到另一个端点。单个 GCP VPC 是扁平网络的一个例子，您可能拥有多个子网和路由/防火墙规则，以允许任何两个端点之间的路由。
- en: Flat networks are easier to manage compared to multiple disparate networks,
    but they require more rigor when it comes to security because all entities are
    on the same logical network space. Firewall rules, network policies, and other
    functionality can ensure only the allowed entities have network access. Flat networks
    may also run into scalability problems. Typically, these networks use RFC1918
    address space ([https://datatracker.ietf.org/doc/html/rfc1918](https://datatracker.ietf.org/doc/html/rfc1918)),
    which provides a finite number of IP addresses (just under 18 million addresses).
    Typically, a flat logical network requires all resources use the same RFC1918
    space. Exceptions to this general rule arise where large organizations may use
    their own public IP address space for internal addressing. Regardless of the IP
    address usage, it is important to note that, in a flat network, no two endpoints
    may have the same IP address.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与多个不同的网络相比，扁平网络更容易管理，但在安全性方面需要更加严谨，因为所有实体都在同一个逻辑网络空间中。防火墙规则、网络策略和其他功能可以确保只有允许的实体才能访问网络。扁平网络也可能遇到可扩展性问题。通常，这些网络使用
    RFC1918 地址空间（[https://datatracker.ietf.org/doc/html/rfc1918](https://datatracker.ietf.org/doc/html/rfc1918)），它提供有限数量的
    IP 地址（接近 1800 万个地址）。通常，扁平逻辑网络要求所有资源使用相同的 RFC1918 空间。在某些情况下，大型组织可能会使用自己的公共 IP 地址空间进行内部寻址，这违反了这一普遍规则。无论
    IP 地址的使用情况如何，重要的是要注意，在扁平网络中，两个端点不能有相同的 IP 地址。
- en: Multiple networks
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 多个网络
- en: 'Anthos can also be deployed in a multinetwork environment. Anthos GKE clusters
    can be deployed on single or multiple networks as required. Typically, it is easier
    to manage network connectivity for applications running on an Anthos platform
    if Anthos is deployed in the same network. You can deploy the Anthos platform
    across multiple disconnected networks, though, in some cases, it may be required
    to connect these multiple networks. You have the following ways to connect applications
    running on an Anthos platform across multiple networks:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 还可以部署在多网络环境中。根据需要，Anthos GKE 集群可以部署在单个或多个网络上。通常，如果 Anthos 在同一网络中部署，则更容易管理在
    Anthos 平台上运行的应用的网络连接。尽管如此，在某些情况下，可能需要连接这些多个网络。您有以下几种方式将运行在 Anthos 平台上的应用连接到多个网络：
- en: '*VPN/ISP*—You can connect multiple networks together via a VPN, or the chosen
    ISP may provide this connectivity. These are the typical choices for connecting
    multiple on-prem data centers.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VPN/ISP*—您可以通过 VPN 将多个网络连接在一起，或者选择的 ISP 可能提供这种连接性。这些是连接多个本地数据中心时的典型选择。'
- en: '*VPC peering*—You can use VPC peering if Anthos is deployed on a public cloud
    that offers VPC peering functionality.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VPC 对接*—如果 Anthos 部署在提供 VPC 对接功能的公共云上，您可以使用 VPC 对接。'
- en: '*Gateways or mTLS*—Services may be connected securely over the public internet
    using TLS, mTLS, or a secured API gateway. This functionality exists through service
    meshes like Anthos Service Mesh (ASM; [https://cloud.google.com/service-mesh/docs/overview](https://cloud.google.com/service-mesh/docs/overview))
    or API gateways like Apigee ([https://cloud.google.com/apigee](https://cloud.google.com/apigee)*).*
    This is done on a per-service level, whereas the first two options are configured
    at the network layer.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网关或 mTLS*—服务可以通过 TLS、mTLS 或安全的 API 网关在公共互联网上安全连接。这种功能通过服务网格（如 Anthos 服务网格（ASM；[https://cloud.google.com/service-mesh/docs/overview](https://cloud.google.com/service-mesh/docs/overview)））或
    API 网关（如 Apigee [https://cloud.google.com/apigee](https://cloud.google.com/apigee)）来实现。*
    这是在每个服务级别上完成的，而前两种选项是在网络层配置的。'
- en: 10.1.2 Multi/hybrid cloud deployment
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 多/混合云部署
- en: 'Anthos is a multicloud platform and can be deployed to multiple environments,
    for example, public/private clouds and on-prem data centers. Managing networking
    across multiple environments is challenging because each environment is unique,
    and managing resources differs depending on the provider. For instance, the way
    a GCP VPC is provisioned is different from an AWS VPC or a data center network.
    Anthos provides a common interface across multiple environments. You can deploy
    the Anthos platform to multiple environments in the following three ways:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 是一个多云平台，可以部署到多个环境中，例如，公有/私有云和本地数据中心。在多个环境中管理网络具有挑战性，因为每个环境都是独特的，而且根据提供商的不同，管理资源的方式也不同。例如，GCP
    VPC 的配置方式与 AWS VPC 或数据中心网络不同。Anthos 在多个环境中提供了一个统一的接口。您可以通过以下三种方式将 Anthos 平台部署到多个环境中：
- en: '*Multicloud deployment*—You can deploy the Anthos platform to multiple public
    cloud environments, for example, GCP and one or more public clouds like AWS and
    Azure.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多云部署*—您可以将 Anthos 平台部署到多个公有云环境中，例如，GCP 和一个或多个公有云，如 AWS 和 Azure。'
- en: '*Hybrid cloud deployment*—You can deploy the Anthos platform to GCP and one
    or more on-prem data centers.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合云部署*—您可以将 Anthos 平台部署到 GCP 和一个或多个本地数据中心。'
- en: '*Multi and hybrid cloud deployment*—This deployment is a combination of the
    two deployments previously mentioned. For example, you can deploy the Anthos platform
    to GCP, to one or more on-prem data centers, and to one or more non-GCP public
    clouds.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多和混合云部署*—这种部署是之前提到的两种部署的组合。例如，您可以将 Anthos 平台部署到 GCP、一个或多个本地数据中心以及一个或多个非 GCP
    的公有云。'
- en: Multi/hybrid networking
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多/混合网络
- en: 'When you deploy Anthos across multiple infrastructure environments, these environments
    must have network connectivity to GCP. Three network connectivity options are
    available to connect multiple infrastructure environments: Cloud Interconnect,
    Cloud VPN, and public internet.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在多个基础设施环境中部署 Anthos 时，这些环境必须与 GCP 具有网络连接。有三种网络连接选项可用于连接多个基础设施环境：云互连、云 VPN
    和公共互联网。
- en: Cloud Interconnect
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 云互连
- en: Cloud Interconnect ([http://mng.bz/Pxe2](http://mng.bz/Pxe2)) extends an on-prem
    network to Google’s network through a highly available, low-latency connection.
    You can use Dedicated Interconnect to connect directly to Google or use Partner
    Interconnect to connect to Google through a supported service provider. Dedicated
    Interconnect provides direct physical connections between your on-prem network
    and Google’s network. Dedicated Interconnect enables you to transfer large amounts
    of data between networks, which can be more cost effective than purchasing additional
    bandwidth over the public internet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 云互连 ([http://mng.bz/Pxe2](http://mng.bz/Pxe2)) 通过一个高可用、低延迟的连接将本地网络扩展到 Google
    的网络。您可以使用专用互连直接连接到 Google，或者使用合作伙伴互连通过支持的服务提供商连接到 Google。专用互连在您的本地网络和 Google 网络之间提供直接的物理连接。专用互连使您能够在网络之间传输大量数据，这可能比在公共互联网上购买额外的带宽更经济高效。
- en: For Dedicated Interconnect, you provision a Dedicated Interconnect connection
    between the Google network and your own router in a common location (see [http://mng.bz/JlQp](http://mng.bz/JlQp)).
    Figure 10.5 shows a single Dedicated Interconnect connection between a VPC network
    and your on-prem network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于专用互连，您在公共位置（见 [http://mng.bz/JlQp](http://mng.bz/JlQp)）配置 Google 网络和您自己的路由器之间的专用互连连接。图
    10.5 显示了 VPC 网络和您的本地网络之间的单个专用互连连接。
- en: '![10-05](../../OEBPS/Images/10-05.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![10-05](../../OEBPS/Images/10-05.png)'
- en: Figure 10.5 Dedicated Interconnect between GCP and an on-prem data center
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 GCP 和本地数据中心之间的专用互连
- en: For this basic setup, a Dedicated Interconnect connection is provisioned between
    the Google network and the on-prem router in a common colocation facility.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种基本设置，在公共托管设施中，Google 网络和本地路由器之间配置了一个专用互连连接。
- en: When you create a VLAN attachment ([http://mng.bz/wPR7](http://mng.bz/wPR7)),
    you associate it with a Cloud Router ([http://mng.bz/qdlK](http://mng.bz/qdlK)).
    This Cloud Router creates a BPG session for the VLAN attachment and its corresponding
    on-prem peer router. The Cloud Router receives the routes that your on-prem router
    advertises. These routes are added as custom dynamic routes in your VPC network.
    The Cloud Router also advertises routes for Google Cloud resources to the on-prem
    peer router.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建VLAN附件（[http://mng.bz/wPR7](http://mng.bz/wPR7)）时，您将其与云路由器（[http://mng.bz/qdlK](http://mng.bz/qdlK)）关联。此云路由器为VLAN附件及其对应的本地对等路由器创建BPG会话。云路由器接收您的本地路由器广播的路由。这些路由作为自定义动态路由添加到您的VPC网络中。云路由器还向本地对等路由器广播谷歌云资源的路由。
- en: 'Depending on your availability needs, you can configure Dedicated Interconnect
    to support mission-critical services or applications that can tolerate some downtime.
    To achieve a specific level of reliability, Google offers the following two prescriptive
    configurations:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的可用性需求，您可以将专用互连配置为支持关键任务服务或可以容忍一些停机时间的应用程序。为了达到特定的可靠性水平，谷歌提供了以下两种规定配置：
- en: Achieve 99.99% (52.60 minutes per year) availability for Dedicated Interconnect
    ([http://mng.bz/71ax](http://mng.bz/71ax))(recommended)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现专用互连的99.99%（每年52.60分钟）可用性（[http://mng.bz/71ax](http://mng.bz/71ax)）（推荐）
- en: Achieve 99.9% availability for Dedicated Interconnect ([http://mng.bz/516q](http://mng.bz/516q))
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现专用互连的99.9%可用性（[http://mng.bz/516q](http://mng.bz/516q)）
- en: Cloud Interconnect is the most robust and secure option to connect GCP and non-GCP
    environments and is the recommended option to connect GCP and one or more on-prem
    data centers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 云互连是将GCP和非GCP环境连接起来的最稳健和最安全的选项，也是连接GCP和一个或多个本地数据中心的推荐选项。
- en: Cloud VPN
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 云VPN
- en: Cloud VPN ([http://mng.bz/mJPn](http://mng.bz/mJPn)*)* securely connects your
    peer network to your VPC network through an IPSsec *VPN* connection. Traffic traveling
    between the two networks is encrypted by one VPN gateway, and then decrypted by
    the other VPN gateway, which protects your data as it travels over the internet.
    Google Cloud offers high-availability VPN, which provides higher uptime and throughput
    with an additional/ redundant VPN connection at a higher cost.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 云VPN（[http://mng.bz/mJPn](http://mng.bz/mJPn)）*通过IPSec VPN连接安全地将您的对等网络连接到您的VPC网络。在两个网络之间传输的流量由一个VPN网关加密，然后由另一个VPN网关解密，这保护了您的数据在互联网上的传输。谷歌云提供高可用性VPN，通过额外的/冗余VPN连接以更高的成本提供更高的正常运行时间和吞吐量。
- en: Each Cloud VPN tunnel can support up to 3 gigabits per second total for ingress
    and egress. You can use multiple Cloud VPN tunnels to increase your ingress and
    egress bandwidth.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个云VPN隧道可以支持总吞吐量高达每秒3千兆比特的入站和出站。您可以使用多个云VPN隧道来增加您的入站和出站带宽。
- en: You can use Cloud VPN between GCP and on-prem data centers as well as between
    GCP and other public cloud vendors. This is the easiest option to set up, and
    you can be running without any delays. Cloud VPN can also be used in conjunction
    with Cloud Interconnect as a secondary connectivity option.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用云VPN在GCP和本地数据中心之间，以及GCP和其他公共云供应商之间进行连接。这是设置最简单的选项，您可以在没有任何延迟的情况下运行。云VPN还可以与云互连结合使用，作为次要连接选项。
- en: Public internet
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 公共互联网
- en: Applications running on the Anthos platform on multiple environments can be
    connected over the public internet without using Cloud Interconnect or VPN. Applications
    running on the platform connect over the public internet using TLS/mTLS.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个环境中运行的Anthos平台上的应用程序可以在不使用云互连或VPN的情况下通过公共互联网连接。平台上的应用程序通过TLS/mTLS在公共互联网上连接。
- en: Anthos Service Mesh (ASM) is part of the Anthos platform. ASM uses client-side
    proxies injected into each Pod to connect services. One of the security features
    of these proxies is to secure connectivity using mTLS. Using a common root certificate
    authority on multiple environments, the sidecar proxies can connect using a secure
    mTLS connection via gateways (e.g., ingress or east-west gateways) across the
    public internet. For details on Anthos Service Mesh, please refer to chapter 4.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos服务网格（ASM）是Anthos平台的一部分。ASM使用注入到每个Pod中的客户端代理来连接服务。这些代理的一个安全特性是使用mTLS来确保连接的安全性。通过在多个环境中使用共同的根证书颁发机构，边车代理可以通过网关（例如，入口或东西向网关）在公共互联网上使用安全的mTLS连接。有关Anthos服务网格的详细信息，请参阅第4章。
- en: If many services require connectivity between environments, then this option
    might not be operationally scalable. In such a case, it is recommended you use
    one of the network connectivity options mentioned earlier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果许多服务需要在环境之间进行连接，那么此选项可能无法在操作上扩展。在这种情况下，建议您使用前面提到的网络连接选项之一。
- en: Disconnected environments
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 断开连接的环境
- en: In some situations, you may be required to have environments that are completely
    disconnected from each other. Anthos platform supports disconnected environments.
    The disconnected environments must have network connectivity to GCP so that the
    platform (i.e., Anthos clusters) can be registered to a GCP project. This is required
    for control plane traffic only. For certain Anthos functionalities, registering
    a cluster is required. For example, to use multicluster ingress on Anthos clusters,
    all participating clusters must be registered to GCP. The services across disconnected
    environments will not be able to communicate to each other.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能需要具有完全相互断开的环境。Anthos平台支持断开连接的环境。断开连接的环境必须与GCP有网络连接，以便平台（即Anthos集群）可以注册到GCP项目。这仅适用于控制平面流量。对于某些Anthos功能，需要注册集群。例如，要在Anthos集群上使用多集群入口，所有参与的集群都必须注册到GCP。断开连接环境中的服务将无法相互通信。
- en: 10.2 Anthos GKE networking
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 Anthos GKE网络
- en: 'Anthos GKE clusters can be deployed to a variety of environments, for example,
    on GCP, on VMware in an on-prem data center, on bare metal servers, and on AWS.
    In addition to the supported Anthos clusters, you can also register any conformant
    Kubernetes cluster to the Anthos platform. For example, you can register EKS clusters
    running in AWS and AKS clusters running in Azure to the Anthos platform. Currently,
    the following six types of Anthos clusters are available:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE集群可以部署到各种环境中，例如，在GCP上、在本地数据中心VMware上、在裸金属服务器上，以及在AWS上。除了支持的Anthos集群外，您还可以将任何符合规范的Kubernetes集群注册到Anthos平台。例如，您可以将运行在AWS上的EKS集群和运行在Azure上的AKS集群注册到Anthos平台。目前，以下六种类型的Anthos集群可供使用：
- en: Anthos clusters on GCP (GKE)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP上的Anthos集群（GKE）
- en: Anthos clusters on VMware (GKE on-prem)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware上的Anthos集群（本地数据中心的GKE）
- en: Anthos clusters on bare metal
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于裸金属的Anthos集群
- en: Anthos clusters on AWS (GKE on AWS)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的Anthos集群（AWS上的GKE）
- en: Anthos clusters on Azure (GKE on Azure)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure上的Anthos集群（Azure上的GKE）
- en: Anthos attached clusters (conformant Kubernetes clusters)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos附加集群（符合Kubernetes规范的集群）
- en: 10.2.1 Anthos cluster networking
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 Anthos集群网络
- en: Cluster IP addressing
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 集群IP寻址
- en: 'All Anthos GKE clusters require the following three IP subnets:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Anthos GKE集群都需要以下三个IP子网：
- en: Node and API server IP addresses
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点和API服务器IP地址
- en: Pod IP addresses
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod IP地址
- en: Services or ClusterIP addresses
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务或ClusterIP地址
- en: '*Node and API server IP addresses* are LAN (for on-prem data centers) or VPC
    (for public clouds) IP addresses. Each node and API server gets a single IP address.
    Depending on the number of nodes/API servers required, ensure you have the required
    number of IP addresses.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*节点和API服务器IP地址*是局域网（对于本地数据中心）或VPC（对于公共云）IP地址。每个节点和API服务器都分配一个单独的IP地址。根据所需的节点/API服务器的数量，确保您有足够的IP地址。'
- en: '*Pod IP addresses* are assigned to every Pod in an Anthos GKE cluster. Each
    node in an Anthos cluster is assigned a unique IP address range, which is used
    to assign Pod IP addresses (running inside that node). If the Pod moves from one
    node to another, its IP address changes based on the IP address range of the new
    node. The API server takes a large IP range, often called the Pod CIDR IP range,
    for example a /14 or a /16 (you can learn about IP subnets at [http://mng.bz/610G](http://mng.bz/610G)).
    The server then equally divides this range into smaller IP ranges and assigns
    a unique range to each node. You define the desired number of Pods per node, which
    is used by the API server to slice the large subnet into smaller subnets per node.
    For example, if you want 30 Pods per node, each node requires a minimum of a /27\.
    Your Pod IP range must be large enough to account for *N* subnets with 32 addresses
    each, where *N* is the maximum number of nodes in the cluster.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pod IP地址*分配给Anthos GKE集群中的每个Pod。Anthos集群中的每个节点都分配了一个唯一的IP地址范围，该范围用于分配Pod IP地址（在该节点内部运行）。如果Pod从一个节点移动到另一个节点，其IP地址将根据新节点的IP地址范围进行更改。API服务器使用一个大的IP范围，通常称为Pod
    CIDR IP范围，例如/14或/16（你可以在[http://mng.bz/610G](http://mng.bz/610G)上了解IP子网）。然后服务器将这个范围平均分成更小的IP范围，并为每个节点分配一个唯一的范围。你定义每个节点期望的Pod数量，API服务器将使用这个数量将大子网切割成每个节点的小子网。例如，如果你希望每个节点有30个Pod，每个节点至少需要一个/27。你的Pod
    IP范围必须足够大，以容纳*N*个子网，每个子网有32个地址，其中*N*是集群中节点的最大数量。'
- en: Pod IP addresses are routable within the cluster. They may or may not be routable
    from outside of the cluster, depending on the type and implementation of the cluster.
    This is discussed in detail in the next section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Pod IP地址在集群内部是可路由的。它们可能或可能不可从集群外部路由，这取决于集群的类型和实现方式。这一点将在下一节中详细讨论。
- en: '*Service or ClusterIP addresses* are assigned to every Kubernetes Service.
    Unlike Pod IP addresses, which may change as Pods move between nodes, ClusterIP
    addresses remain static and act as a load-balancing virtual IP address (VIP) to
    multiple Pods representing a single Kubernetes Service. As the name suggests,
    service IPs or ClusterIPs are locally significant to the cluster and cannot be
    accessed from outside of the cluster. Services inside the cluster can access services
    using ClusterIPs.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务或ClusterIP地址*分配给每个Kubernetes服务。与Pod IP地址不同，Pod IP地址可能会随着Pod在节点之间移动而改变，而ClusterIP地址保持静态，并作为多个表示单个Kubernetes服务的Pod的负载均衡虚拟IP地址（VIP）。正如其名所示，服务IP或ClusterIP对集群是本地相关的，并且不能从集群外部访问。集群内部的服务可以使用ClusterIP访问服务。'
- en: Cluster networking data plane
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 集群网络数据平面
- en: Anthos GKE clusters provide two options for networking data planes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE集群提供了两种网络数据平面的选项。
- en: 'GKE Dataplane v1: kube-proxy and Calico'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GKE数据平面v1：kube-proxy和Calico
- en: Kubernetes manages connectivity among Pods and Services using the kube-proxy
    component. This is deployed as a static Pod on each node by default. Any GKE cluster
    running version 1.16 or later has a kube-proxy deployed as a DaemonSet.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用kube-proxy组件管理Pod和Service之间的连接。默认情况下，它作为每个节点上的静态Pod部署。任何运行1.16或更高版本的GKE集群都部署了一个作为DaemonSet的kube-proxy。
- en: kube-proxy is not an in-line proxy but an egress-based load-balancing controller.
    It watches the Kubernetes API server and continually maps the ClusterIP to healthy
    Pods by adding and removing destination NAT rules to the node’s iptables subsystem.
    When a container running in a Pod sends traffic to a Service’s ClusterIP, the
    node selects a Pod at random and routes the traffic to that Pod.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy不是一个内联代理，而是一个基于出口的负载均衡控制器。它监视Kubernetes API服务器，并通过向节点的iptables子系统添加和删除目标NAT规则，持续地将ClusterIP映射到健康的Pod。当一个在Pod中运行的容器向服务的ClusterIP发送流量时，节点随机选择一个Pod并将流量路由到该Pod。
- en: When you configure a Service, you can optionally remap its listening port by
    defining values for port and targetPort. The port is where clients reach the application.
    The targetPort is the port where the application is listening for traffic within
    the Pod. kube-proxy manages this port remapping by adding and removing iptables
    rules on the node.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当你配置服务时，你可以通过定义端口和targetPort的值来可选地重映射其监听端口。端口是客户端到达应用程序的地方。targetPort是应用程序在Pod内部监听流量的端口。kube-proxy通过在节点上添加和删除iptables规则来管理这个端口重映射。
- en: In GKE Dataplane v1, Kubernetes NetworkPolicies are implemented using the Calico
    component. Calico is an open source networking and network security solution for
    containers, virtual machines, and native host-based workloads. This implementation
    uses components that rely heavily on iptables functionality in the Linux kernel.
    Dataplane v2 addresses and resolves some of these problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GKE Dataplane v1 中，Kubernetes NetworkPolicies 使用 Calico 组件实现。Calico 是一个开源的容器、虚拟机和基于主机的工作负载的网络和网络安全解决方案。此实现使用依赖于
    Linux 内核中 iptables 功能的组件。Dataplane v2 解决并解决了一些这些问题。
- en: 'GKE Dataplane v2: eBPF and Cilium'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: GKE Dataplane v2：eBPF 和 Cilium
- en: GKE Dataplane v2, shown in figure 10.6, is an opinionated data plane that harnesses
    the power of extended Berkeley Packet Filter (eBPF) and Cilium, an open source
    project that makes the Linux kernel Kubernetes aware using eBPF.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 10.6 所示的 GKE Dataplane v2 是一个具有观点的数据平面，它利用了扩展伯克利包过滤器 (eBPF) 和 Cilium 的力量，Cilium
    是一个开源项目，它使用 eBPF 使 Linux 内核对 Kubernetes 有所了解。
- en: '![10-06](../../OEBPS/Images/10-06.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![10-06](../../OEBPS/Images/10-06.png)'
- en: Figure 10.6 GKE Dataplane v2 architecture
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 GKE Dataplane v2 架构
- en: Dataplane V2 addresses the observability, scalability, and functional requirements
    by providing a programmable data path. eBPF, a new Linux networking paradigm,
    exposes programmable hooks to the network stack inside the Linux kernel. The ability
    to enrich the kernel with user-space information—without jumping back and forth
    between user and kernel spaces—enables context-aware operations on network packets
    at high speeds.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Dataplane V2 通过提供可编程数据路径来解决可观察性、可伸缩性和功能需求。eBPF，一种新的 Linux 网络范式，向 Linux 内核内部的网络堆栈公开可编程钩子。通过在用户空间和内核空间之间跳转，丰富内核的用户空间信息的能力，使得可以在高速下对网络数据包进行上下文感知的操作。
- en: 'The new data plane adds two new cluster components: the cilium-agent DaemonSet
    that programs the eBPF data path and the cilium-operator Deployment that manages
    the Cilium-internal CRDs and helps the cilium-agent avoid watching every Pod.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据平面增加了两个新的集群组件：用于编程 eBPF 数据路径的 cilium-agent DaemonSet 和管理 Cilium-内部 CRDs 并帮助
    cilium-agent 避免监视每个 Pod 的 cilium-operator Deployment。
- en: The data plane also eliminates both Calico cluster components—the calico-node
    DaemonSet and the calico-typha Deployment. These components provide NetworkPolicy
    enforcement, which is provided by the cilium-agent DaemonSet.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面还消除了 Calico 集群组件——calico-node DaemonSet 和 calico-typha Deployment。这些组件提供
    NetworkPolicy 执行功能，该功能由 cilium-agent DaemonSet 提供。
- en: The data plane also removes the kube-proxy static Pod from the nodes. kube-proxy
    provides service resolution functionality to the cluster, which is also provided
    by the cilium-agent.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面还从节点中移除了 kube-proxy 静态 Pod。kube-proxy 为集群提供服务解析功能，该功能也由 cilium-agent 提供。
- en: Dataplane V2 offers networking programmability and scalability to Anthos clusters,
    as shown in figure 10.7\. Enterprises use Kubernetes NetworkPolicies to declare
    how Pods can communicate with one another. However, there previously was no scalable
    way to troubleshoot and audit the behavior of these policies. With eBPF in GKE,
    you can now enforce real-time policies as well as correlate policy actions (allow/deny)
    to Pod, namespace, and policy names at a line rate with minimal impact on the
    node’s CPU and memory resources. As packets come into the VM, specialized eBPF
    programs can be installed in the kernel to decide how to route the packet. Unlike
    iptables, eBPF programs have access to Kubernetes-specific metadata, including
    network policy information. This way, they can not only allow or deny the packet,
    they can also report annotated actions back to the user space. These events make
    it possible for you to generate network policy logs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Dataplane V2 为 Anthos 集群提供网络可编程性和可伸缩性，如图 10.7 所示。企业使用 Kubernetes NetworkPolicies
    来声明 Pod 之间如何相互通信。然而，之前没有可伸缩的方式来调试和审计这些策略的行为。在 GKE 中使用 eBPF 后，你现在可以执行实时策略，以及以最小影响节点
    CPU 和内存资源的方式，以行速率关联策略操作（允许/拒绝）到 Pod、命名空间和政策名称。当数据包进入虚拟机时，可以在内核中安装专门的 eBPF 程序来决定如何路由数据包。与
    iptables 不同，eBPF 程序可以访问 Kubernetes 特定的元数据，包括网络策略信息。这样，它们不仅可以允许或拒绝数据包，还可以将带注释的操作报告回用户空间。这些事件使您能够生成网络策略日志。
- en: '![10-07](../../OEBPS/Images/10-07.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![10-07](../../OEBPS/Images/10-07.png)'
- en: 'Figure 10.7 GKE Dataplane v2: Network policy flow logging'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 GKE Dataplane v2：网络策略流日志
- en: Table 10.1 shows a comparison of networking features between GKE Dataplane v1
    and v2.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1 显示了 GKE Dataplane v1 和 v2 之间网络功能的比较。
- en: Table 10.1
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1
- en: '| Network feature | Existing | New Dataplane |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 网络功能 | 现有 | 新 Dataplane |'
- en: '| ClusterIP service resolution | kube-proxy using iptables | cilium-agent using
    eBPF on sockets |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 集群 IP 服务解析 | kube-proxy 使用 iptables | cilium-agent 在套接字上使用 eBPF |'
- en: '| NodePort service resolution | kube-proxy using iptables | cilium-agent using
    eBPF on eth0 TC hooks |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 节点端口服务解析 | kube-proxy 使用 iptables | cilium-agent 在 eth0 TC 钩子上使用 eBPF |'
- en: '| Load balancer service resolution | kube-proxy using iptables redirecting
    to service chain | cilium-agent using eBPF on eth0 TC hooks (same hook as previously)
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 负载均衡器服务解析 | kube-proxy 使用 iptables 重定向到服务链 | cilium-agent 在 eth0 TC 钩子上使用
    eBPF（与之前相同的钩子）|'
- en: '| Network policy enforcement | Calico using iptables | cilium-agent using eBPF
    on socket as well as eth0 TC hooks |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 网络策略执行 | Calico 使用 iptables | cilium-agent 在套接字以及 eth0 TC 钩子上使用 eBPF |'
- en: Depending on the type and implementation of the Anthos cluster, networking design
    and requirements vary. In the next section, we look at each type of Anthos cluster
    in terms of networking requirements and best practices.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Anthos集群的类型和实现，网络设计和需求会有所不同。在下一节中，我们将从网络需求和最佳实践的角度分析每种类型的Anthos集群。
- en: Anthos GKE on GCP
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 上的 Anthos GKE
- en: 'An Anthos GKE cluster runs on GCP and uses GCP VPC functionality for Kubernetes
    networking. Two types of implementations of Anthos GKE on GCP are available: VPC-native
    clusters and routes-based clusters.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE 集群在 GCP 上运行，并使用 GCP VPC 功能进行 Kubernetes 网络功能。GCP 上 Anthos GKE 的两种实现方式可用：VPC
    原生集群和基于路由的集群。
- en: VPC-native clusters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 原生集群
- en: 'This is the default and recommended implementation for Anthos GKE on GCP clusters.
    A cluster that uses alias IP address ranges is called a VPC-native cluster. VPC-native
    clusters use real VPC IP addresses for Pod IP ranges. This option allows Pod-to-Pod
    communication within a single cluster as well as across multiple (VPC-native)
    clusters in the same VPC. It also allows direct Pod connectivity to any routable
    VPC entity, for example, GCE instances. VPC-native clusters use secondary IP address
    ranges for Pod IP and Service IP ranges. VPC-native clusters offer the following
    benefits:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Anthos GKE 在 GCP 集群中的默认和推荐实现。使用别名 IP 地址范围的集群称为 VPC 原生集群。VPC 原生集群使用真实的 VPC
    IP 地址作为 Pod IP 范围。此选项允许单个集群内的 Pod 到 Pod 通信，以及在同一 VPC 中多个（VPC 原生）集群之间的通信。它还允许 Pod
    直接连接到任何可路由的 VPC 实体，例如 GCE 实例。VPC 原生集群使用次要 IP 地址范围用于 Pod IP 和服务 IP 范围。VPC 原生集群提供以下优势：
- en: Pod IP addresses are natively routable within the cluster’s VPC network and
    other VPC networks connected to it by VPC Network Peering.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod IP 地址在集群的 VPC 网络内以及通过 VPC 网络对等连接的 VPC 网络中是原生可路由的。
- en: Pod IP addresses are reserved in the VPC network before the Pods are created
    in your cluster. This prevents conflict with other resources in the VPC network
    and allows you to better plan IP address allocations.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的集群中创建 Pod 之前，Pod IP 地址已在 VPC 网络中预留。这可以防止与 VPC 网络中的其他资源发生冲突，并允许您更好地规划 IP 地址分配。
- en: Pod IP address ranges do not depend on custom static routes. They do not consume
    the system-generated and custom static routes quota. Instead, automatically generated
    subnet routes handle routing for VPC-native clusters.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod IP 地址范围不依赖于自定义静态路由。它们不会消耗系统生成的和自定义静态路由配额。相反，自动生成的子网路由处理 VPC 原生集群的路由。
- en: You can create firewall rules that apply to just Pod IP address ranges instead
    of any IP address on the cluster’s nodes.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以创建仅适用于 Pod IP 地址范围的防火墙规则，而不是集群节点上的任何 IP 地址。
- en: Pod IP address ranges, and subnet secondary IP address ranges in general, are
    accessible from on-prem networks connected with Cloud VPN or Cloud Interconnect
    using Cloud Routers.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod IP 地址范围，以及通常的子网次要 IP 地址范围，可以通过连接到 Cloud VPN 或 Cloud Interconnect 的本地网络使用
    Cloud Router 访问。
- en: Routes-based clusters
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 基于路由的集群
- en: A cluster that uses Google Cloud Routes is called a routes-based cluster. Google
    Cloud routes define the paths that network traffic takes from a VM instance to
    other destinations. The Pod IP address ranges in a routes-based cluster are not
    VPC IP addresses and, therefore, are not natively routable inside the VPC. Cloud
    Routes are created for each Pod IP address range so that Pods within a cluster
    can communicate with other Pods running on different nodes. Routes-based clusters
    do not provide Pod-to-Pod intercluster connectivity for multiple Anthos GKE clusters.
    To create a routes-based cluster, you must explicitly turn off the VPC-native
    option.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Google Cloud Routes的集群称为基于路由的集群。Google Cloud routes定义了网络流量从VM实例到其他目的地的路径。基于路由的集群中的Pod
    IP地址范围不是VPC IP地址，因此不能在VPC内部原生路由。为每个Pod IP地址范围创建Cloud Routes，以便集群内的Pods可以与其他节点上运行的Pods通信。基于路由的集群不提供多个Anthos
    GKE集群之间的Pod到Pod跨集群连接。要创建基于路由的集群，您必须明确关闭VPC原生选项。
- en: In a routes-based cluster, each node is allocated a /24 range of IP addresses
    for Pods. With a /24 range, you have 256 addresses, but the maximum number of
    Pods per node is 110\. By having approximately twice as many available IP addresses
    as possible Pods, Kubernetes can mitigate IP address reuse as Pods are added to
    and removed from a node.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于路由的集群中，每个节点为Pods分配了一个/24范围的IP地址。使用/24范围，您有256个地址，但每个节点的最大Pod数量是110。通过提供大约是可能Pod数量两倍的可用IP地址，Kubernetes可以在Pod被添加到节点和从节点移除时减轻IP地址重用的风险。
- en: A routes-based cluster has a range of IP addresses that are used for Pods and
    Services. Even though the range is used for both Pods and Services, it is called
    the *Pod address range*. The last /20 of the Pod address range is used for Services.
    A /20 range has 4,096 addresses that are used for Services as well as Pods.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于路由的集群有一系列IP地址，用于Pods和Services。尽管这个范围用于Pods和Services，但它被称为*Pod地址范围*。Pod地址范围的最后/20用于Services。一个/20范围有4,096个地址，这些地址既用于Services也用于Pods。
- en: 'In command output, the Pod address range is called clusterIpv4Cidr, and the
    range of addresses used for Services is called servicesIpv4Cidr. For example,
    the output of gcloud container clusters describe includes output like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令输出中，Pod地址范围被称为clusterIpv4Cidr，用于Services的地址范围被称为servicesIpv4Cidr。例如，gcloud
    container clusters describe的输出可能包含如下内容：
- en: '[PRE0]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For GKE version 1.7 and later, the Pod address range can be from any RFC1918
    block: 10.0.0.0/8, 172.16.0.0/12, or 192.168.0.0/16\. For earlier versions, the
    Pod address range must be from 10.0.0.0/8.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GKE 1.7版本及以后的版本，Pod地址范围可以从任何RFC1918块：10.0.0.0/8、172.16.0.0/12或192.168.0.0/16。对于早期版本，Pod地址范围必须来自10.0.0.0/8。
- en: The maximum number of nodes, Pods, and Services for a given GKE cluster is determined
    by the size of the cluster subnet and the size of the Pod address range. You cannot
    change the Pod address range size after you create a cluster. When you create
    a cluster, ensure that you choose a Pod address range large enough to accommodate
    the cluster’s anticipated growth.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 给定GKE集群的节点、Pods和Services的最大数量由集群子网的大小和Pod地址范围的大小决定。创建集群后，您不能更改Pod地址范围的大小。在创建集群时，请确保您选择的Pod地址范围足够大，以容纳集群预期的增长。
- en: Anthos GKE cluster IP allocation
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE集群IP分配
- en: 'Kubernetes uses the following IP ranges to assign IP addresses to nodes, Pods,
    and Services:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用以下IP范围来为节点、Pods和Service分配IP地址：
- en: '*Node IP*—In Anthos GKE clusters, a node is a GCE instance. Each node has an
    IP address assigned from the cluster’s VPC network. This node IP provides connectivity
    from control components like kube-proxy and the kubelet to the Kubernetes API
    server. This IP is the node’s connection to the rest of the cluster.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点IP*—在Anthos GKE集群中，节点是一个GCE实例。每个节点都从集群的VPC网络中分配了一个IP地址。这个节点IP提供了从控制组件（如kube-proxy和kubelet）到Kubernetes
    API服务器的连接。这个IP是节点连接到集群其余部分的方式。'
- en: '*Pod IP CIDR*—Each node has a pool of IP addresses that GKE assigns to Pods
    running on that node (a /24 CIDR block by default). You can optionally specify
    the range of IPs when you create the cluster. The Flexible Pod CIDR range feature
    allows you to reduce the size of the range for Pod IPs for nodes in a given node
    pool. Each Pod has a single IP address assigned from the Pod CIDR range of its
    node. This IP address is shared by all containers running within the Pod and connects
    them to other Pods running in the cluster. The maximum number of Pods you can
    run on a node is equal to half of the Pod IP CIDR range. For example, you can
    run a maximum of 110 Pods on a node with a /24 range—not 256 as you might expect.
    This number of Pods provides a buffer so that Pods don’t become unschedulable
    due to a transient lack of IP addresses in the Pod IP range for a given node.
    For ranges smaller than /24, half as many Pods can be scheduled as IP addresses
    in the range.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pod IP CIDR*—每个节点都有一个IP地址池，GKE将这些地址分配给在该节点上运行的Pods（默认情况下为/24 CIDR块）。在创建集群时，您可以可选地指定IP地址的范围。灵活的Pod
    CIDR范围功能允许您减小给定节点池中节点Pod IP地址范围的尺寸。每个Pod从其节点的Pod CIDR范围内分配一个单独的IP地址。这个IP地址由Pod内所有运行的容器共享，并将它们连接到集群中运行的其他Pod。您可以在节点上运行的Pod的最大数量等于Pod
    IP CIDR范围的一半。例如，您可以在/24范围内运行最多110个Pod，而不是您可能预期的256个。这个Pod数量提供了一个缓冲区，以便Pod不会因为特定节点Pod
    IP地址范围内的IP地址暂时不足而变得不可调度。对于小于/24的范围，可以安排的Pod数量是范围内IP地址数量的一半。'
- en: '*Service IP*—Each Service has an IP address, called the ClusterIP, assigned
    from the cluster’s VPC network. You can optionally customize the VPC network when
    you create the cluster. In Kubernetes, you can assign arbitrary key-value pairs
    called *labels* to any Kubernetes resource. Kubernetes uses labels to group multiple
    related Pods into a logical unit called a Service. A Service has a stable IP address
    and ports and provides load balancing among the set of Pods whose labels match
    all the labels you define in the label selector when you create the Service.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务IP*—每个服务都有一个IP地址，称为ClusterIP，从集群的VPC网络中分配。在创建集群时，您可以可选地自定义VPC网络。在Kubernetes中，您可以将任意键值对称为*labels*分配给任何Kubernetes资源。Kubernetes使用标签将多个相关的Pod组合成一个逻辑单元，称为服务。服务有一个稳定的IP地址和端口，并为标签选择器中定义的所有标签匹配的Pod集提供负载均衡。'
- en: Egress traffic and controls
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 出流量和控制
- en: For VPC-native clusters, traffic egressing a Pod is routed using normal VPC
    routing functionality. Pod IP addresses are preserved in the TCP header as the
    source IP address. You must create the appropriate firewall rules to allow traffic
    between Pods and other VPC resources. You can also use NetworkPolicy to further
    control the flow of traffic between Pods within a cluster, as well as traffic
    egressing Pods. These policies are enforced by the GKE Dataplane implementation
    explained in the previous section. At the Service layer, you can use egress policy
    through ASM to control what traffic exits the clusters. In this case, an Envoy
    proxy called the istio-egressgateway exists at the perimeter of the service mesh
    through which all egress traffic flows. For routes-based clusters, all Pod egress
    traffic goes through NAT via the node IP address.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于VPC原生集群，Pod的出流量使用正常的VPC路由功能进行路由。Pod IP地址作为源IP地址保留在TCP头部。您必须创建适当的防火墙规则以允许Pod与其他VPC资源之间的流量。您还可以使用NetworkPolicy进一步控制集群内Pod之间的流量以及Pod的出流量。这些策略由上一节中解释的GKE
    Dataplane实现强制执行。在服务层，您可以通过ASM使用出口策略来控制什么流量离开集群。在这种情况下，一个名为istio-egressgateway的Envoy代理存在于服务网格的边缘，所有出流量都通过它流过。对于基于路由的集群，所有Pod的出流量都通过节点IP地址进行NAT。
- en: Load balancers and ingress
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器和入口
- en: 'GKE provides the following three types of load balancers to control access
    and to spread incoming traffic across your cluster as evenly as possible. You
    can configure one Service to use multiple types of load balancers simultaneously:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GKE提供以下三种类型的负载均衡器来控制访问并尽可能均匀地分散集群中的入流量。您可以为一个服务同时配置多种类型的负载均衡器：
- en: External load balancers manage traffic coming from outside the cluster and outside
    your Google Cloud VPC network. They use forwarding rules associated with the Google
    Cloud network to route traffic to a Kubernetes node.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部负载均衡器管理来自集群外部和您的Google Cloud VPC网络外部的流量。它们使用与Google Cloud网络关联的转发规则将流量路由到Kubernetes节点。
- en: Internal load balancers manage traffic coming from within the same VPC network.
    Like external load balancers, they use forwarding rules associated with the Google
    Cloud network to route traffic to a Kubernetes node.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部负载均衡器管理来自同一VPC网络内部的流量。与外部负载均衡器一样，它们使用与Google Cloud网络关联的转发规则将流量路由到Kubernetes节点。
- en: HTTP(S) load balancers are specialized external load balancers used for HTTP(S)
    traffic. They use an Ingress resource rather than a forwarding rule to route traffic
    to a Kubernetes node.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP(S)负载均衡器是专门用于HTTP(S)流量的外部负载均衡器。它们使用Ingress资源而不是转发规则来将流量路由到Kubernetes节点。
- en: The external and internal load balancers described here are TCP/L4 load balancers.
    If your Service needs to be reachable from outside the cluster and outside your
    VPC network, you can configure your Service as a load balancer by setting the
    Service’s type field to Loadbalancer. GKE then provisions a network load balancer
    in front of the Service. The network load balancer is aware of all nodes in your
    cluster and configures your VPC network’s firewall rules to allow connections
    to the Service from outside the VPC network, using the Service’s external IP address.
    You can assign a static external IP address to the Service.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的外部和内部负载均衡器是TCP/L4负载均衡器。如果您的服务需要从集群外部和您的VPC网络外部可达，您可以通过将服务类型字段设置为Loadbalancer来配置您的服务作为负载均衡器。然后GKE在服务前面部署一个网络负载均衡器。网络负载均衡器了解您集群中的所有节点，并配置您的VPC网络防火墙规则以允许从VPC网络外部连接到服务，使用服务的公共IP地址。您可以将静态公共IP地址分配给服务。
- en: 'For traffic that needs to reach your cluster from within the same VPC network,
    you can configure your Service to provision an internal load balancer. The internal
    load balancer chooses an IP address from your cluster’s VPC subnet instead of
    an external IP address. Applications or services within the VPC network can use
    this IP address to communicate with Services inside the cluster. An example of
    a Service manifest that creates an internal load balancer follows. You can configure
    an external load balancer in the same way by removing the annotation (which creates
    an internal load balancer):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要从同一VPC网络内部到达您的集群的流量，您可以配置您的服务以部署一个内部负载均衡器。内部负载均衡器从您的集群VPC子网中选择一个IP地址，而不是外部IP地址。VPC网络内的应用程序或服务可以使用此IP地址与集群内的服务进行通信。以下是一个创建内部负载均衡器的服务描述示例。您可以通过删除注释（这会创建一个内部负载均衡器）以相同的方式配置外部负载均衡器：
- en: '[PRE1]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The annotation creates an internal Google load balancer.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该注释创建了一个内部Google负载均衡器。
- en: ❷ Creates a Google load balancer
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个Google负载均衡器
- en: 'Many applications, such as RESTful web service APIs, communicate using HTTP(S).
    You can allow clients external to your VPC network to access this type of application
    using a Kubernetes Ingress resource. An Ingress resource allows you to map hostnames
    and URL paths to Services within the cluster. An Ingress resource is associated
    with one or more Service objects, each of which is associated with a set of Pods.
    When you create an Ingress resource, the GKE Ingress controller creates a Google
    Cloud HTTP(S) load balancer and configures it according to the information in
    the Ingress and its associated Services. To use Ingress, you must have the HTTP
    load balancing add-on enabled. GKE clusters have HTTP load balancing enabled by
    default. GKE Ingress resources come in the following two types:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序，如RESTful Web服务API，使用HTTP(S)进行通信。您可以通过使用Kubernetes Ingress资源允许VPC网络外部的客户端访问此类应用程序。Ingress资源允许您将主机名和URL路径映射到集群内的服务。Ingress资源与一个或多个服务对象相关联，每个对象都与一组Pod相关联。当您创建Ingress资源时，GKE
    Ingress控制器创建一个Google Cloud HTTP(S)负载均衡器，并根据Ingress及其相关服务的信息进行配置。要使用Ingress，您必须启用HTTP负载均衡附加组件。GKE集群默认启用HTTP负载均衡。GKE
    Ingress资源有以下两种类型：
- en: Ingress for external HTTP(S) load balancer deploys the Google Cloud external
    HTTP(S) load balancer. This internet-facing load balancer is deployed globally
    across Google’s edge network as a managed and scalable pool of load-balancing
    resources.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部HTTP(S)负载均衡器的Ingress部署了Google Cloud外部HTTP(S)负载均衡器。这个面向互联网的负载均衡器作为管理可扩展的负载均衡资源池在全球Google边缘网络上部署。
- en: Ingress for Internal HTTP(S) load balancing deploys the Google Cloud internal
    HTTP(S) load balancer. This internal HTTP(S) load balancer is powered by Envoy
    proxy systems outside of your GKE cluster, but within your VPC network.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部HTTP(S)负载均衡的Ingress部署了Google Cloud内部HTTP(S)负载均衡器。这个内部HTTP(S)负载均衡器由位于您的GKE集群外部但位于您的VPC网络之外的Envoy代理系统提供支持。
- en: 'HTTP(S) load balancing, configured by Ingress, includes the following features:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP(S)负载均衡，由入口配置，包括以下功能：
- en: Flexible configuration for Services. An Ingress defines how traffic reaches
    your Services and how the traffic is routed to your application. In addition,
    an Ingress can provide a single IP address for multiple Services in your cluster.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务灵活配置。入口定义了流量如何到达您的服务以及如何将流量路由到您的应用程序。此外，入口可以为您的集群中的多个服务提供一个单一的IP地址。
- en: Integration with Google Cloud network services.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Google Cloud网络服务的集成。
- en: Support for multiple TLS certificates. An Ingress can specify the use of multiple
    TLS certificates for request termination.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多个TLS证书。入口可以指定使用多个TLS证书进行请求终止。
- en: When you create the Ingress resource, GKE provisions an HTTP(S) load balancer
    in the Google Cloud project according to the rules in the manifest and the associated
    Service manifests. The load balancer sends a request to a node’s IP address at
    the NodePort. After the request reaches the node, the chosen GKE Dataplane routes
    the traffic to the appropriate Pod (for the desired Service). For Dataplane v1,
    the node uses its iptables NAT table to choose a Pod. kube-proxy manages the iptables
    rules on the node. For Dataplane v2, GKE provides this functionality using eBPF
    and Cilium agents.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建入口资源时，GKE根据清单中的规则和相关服务清单在Google Cloud项目中配置HTTP(S)负载均衡器。负载均衡器向节点的NodePort发送请求。在请求到达节点后，选择的GKE
    Dataplane将流量路由到适当的Pod（对于所需的服务）。对于Dataplane v1，节点使用其iptables NAT表来选择Pod。kube-proxy管理节点上的iptables规则。对于Dataplane
    v2，GKE使用eBPF和Cilium代理提供此功能。
- en: 'In the following example, the Ingress definition routes traffic for demo.example.com
    to a Service named frontend on port 80, and demo-backend.example.com to a Service
    named users on port 8080:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，入口定义将demo.example.com的流量路由到名为frontend的服务，端口为80，将demo-backend.example.com的流量路由到名为users的服务，端口为8080：
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Requests to host demo.example.com are forwarded to the Service frontend on
    port 80.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请求demo.example.com的托管服务被转发到端口80上的服务frontend。
- en: ❷ Requests to host demo-backend.example.com are forwarded to the Service users
    on port 8080.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 请求demo-backend.example.com的托管服务被转发到端口8080上的服务用户。
- en: Container-native load balancing
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 容器原生负载均衡
- en: Container-native load balancing is the practice of load balancing directly to
    Pod endpoints in GKE using network endpoint groups (NEGs). With Ingress, Service-bound
    traffic is sent from the HTTP load balancer to any of the node IPs on the node
    port. After the request reaches the node, the GKE Dataplane routes the traffic
    to the desired Pod, a process that results in extra hops. In some cases, the Pod
    may not even be running on the node, and thus, the node sends the request to the
    node where the desired Pod is running. Additional hops add latency and make the
    traffic path more complex.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 容器原生负载均衡是在GKE中使用网络端点组（NEGs）直接对Pod端点进行负载均衡的实践。使用入口，服务绑定的流量从HTTP负载均衡器发送到节点上的任何节点IP。在请求到达节点后，GKE
    Dataplane将流量路由到所需的Pod，这个过程会产生额外的跳数。在某些情况下，Pod甚至可能不在节点上运行，因此节点将请求发送到运行所需Pod的节点。额外的跳数增加了延迟，并使流量路径更加复杂。
- en: With NEGs, traffic is load balanced from the load balancer directly to the Pod
    IP, as opposed to traversing the nodes. In addition, Pod readiness gates are implemented
    to determine the health of Pods from the perspective of the load balancer and
    not just the Kubernetes in-cluster health probes. This improves overall traffic
    stability by making the load balancer infrastructure aware of life cycle events
    such as Pod startup, Pod loss, or VM loss. These capabilities resolve the previously
    described limitations and result in more performant and stable networking.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NEGs时，流量直接从负载均衡器负载均衡到Pod IP，而不是穿越节点。此外，Pod就绪网关被实施，以从负载均衡器的角度确定Pod的健康状况，而不仅仅是Kubernetes集群内的健康探测。这使得负载均衡器基础设施能够意识到生命周期事件，如Pod启动、Pod丢失或VM丢失，从而提高了整体流量的稳定性。这些功能解决了之前描述的限制，并导致网络性能更高且更稳定。
- en: 'Container-native load balancing is enabled by default for Services when all
    the following conditions are true:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当以下所有条件都为真时，默认为服务启用容器原生负载均衡：
- en: For Services created in GKE clusters 1.17.6-gke.7 and up
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于在GKE集群1.17.6-gke.7及更高版本中创建的服务
- en: Using VPC-native clusters
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VPC原生集群
- en: Not using a shared VPC
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用共享VPC
- en: Not using GKE network policy
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用GKE网络策略
- en: 'For clusters where NEGs are not the default, it is still strongly recommended
    to use container-native load balancing, but it must be enabled explicitly on a
    per-Service basis. The annotation should be applied to Services in the following
    manner:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NEGs不是默认设置的集群，仍然强烈建议使用容器本机负载均衡，但必须在每个服务的基础上显式启用。该注解应按以下方式应用于服务：
- en: '[PRE3]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The annotation creates a network endpoint group for Pods in the service.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该注解为服务中的Pod创建一个网络端点组。
- en: 'In the Service manifest, you must use type: NodePort unless you’re using container-native
    load balancing. If you’re using container-native load balancing, use type: ClusterIP.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '在服务清单中，您必须使用type: NodePort，除非您正在使用容器本机负载均衡。如果您正在使用容器本机负载均衡，请使用type: ClusterIP。'
- en: Shared VPC considerations and best practices
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 共享VPC的考虑事项和最佳实践
- en: 'The GKE Ingress controllers use a Google Cloud service account to deploy and
    manage Google Cloud resources. When a GKE cluster resides in a service project
    of a shared VPC, this service account may not have the rights to manage network
    resources owned by the host project. The Ingress controller actively manages firewall
    rules to provide access between load balancers and Pods as well as between centralized
    health checkers and Pods. You can manage this in the following ways:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: GKE入口控制器使用Google Cloud服务帐户来部署和管理Google Cloud资源。当GKE集群位于共享VPC的服务项目中时，此服务帐户可能没有管理主机项目拥有的网络资源的权利。入口控制器积极管理防火墙规则，以提供负载均衡器和Pod之间以及集中式健康检查器和Pod之间的访问。您可以通过以下方式管理：
- en: '*Manual firewall rule provisioning*—If your security policies allow firewall
    management only from the host project, you can provision these firewall rules
    manually. When deploying Ingress in a shared VPC, the Ingress resource event provides
    the specific firewall rule you need to provide access. To manually provision a
    firewall rule, view the Ingress resource using the describe command:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*手动防火墙规则提供*—如果您的安全策略仅允许从主机项目进行防火墙管理，您可以手动提供这些防火墙规则。当在共享VPC中部署入口时，入口资源事件提供了您需要提供访问的具体防火墙规则。要手动提供防火墙规则，请使用describe命令查看入口资源：'
- en: '[PRE4]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of this command, shown next, should have the required firewall rule
    that can be implemented in the host network project:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令的输出，如下所示，应包含可以在主机网络项目中实施的所需防火墙规则：
- en: '[PRE5]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Automatic firewall rule provisioning*—An automated approach is to provide
    the GKE Ingress controller service account the permissions to update firewall
    rules. You do this by creating a custom IAM role, providing the ability to manage
    firewall rules in the host network project and then granting this role to the
    GKE Ingress service account.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动防火墙规则提供*—一种自动方法是授予GKE入口控制器服务帐户更新防火墙规则的权限。您通过创建自定义IAM角色，提供在主机网络项目中管理防火墙规则的能力，然后将此角色授予GKE入口服务帐户。'
- en: 'First, create a custom IAM role with the required permissions:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，创建一个具有所需权限的自定义IAM角色：
- en: '[PRE6]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, grant the custom role to the GKE Ingress controller service account:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将自定义角色授予GKE入口控制器服务帐户：
- en: '[PRE7]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Multicluster Ingress
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群入口
- en: 'In some cases, you have to run the same service on multiple GKE clusters. Many
    factors drive multicluster topologies, including close user proximity for apps,
    cluster and regional high availability, security and organizational separation,
    cluster migration, and data locality. *Multicluster* Ingress (MCI), shown in figure
    10.8, is a cloud-hosted multicluster Ingress controller for Anthos GKE clusters.
    It’s a Google-hosted service that supports deploying shared load-balancing resources
    across clusters and across regions. Multicluster Ingress is designed to meet the
    load-balancing needs of multicluster, multiregional environments. It’s a controller
    for the external HTTP(S) load balancer to provide Ingress for traffic coming from
    the internet across one or more clusters. Multicluster Ingress’s multicluster
    support satisfies many use cases, including the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您必须在多个GKE集群上运行相同的服务。多集群拓扑结构由许多因素驱动，包括应用的用户邻近性、集群和区域高可用性、安全性和组织分离、集群迁移以及数据本地性。*多集群*
    入口（MCI），如图10.8所示，是用于Anthos GKE集群的云托管多集群入口控制器。它是一个由Google托管的云服务，支持在集群和区域之间部署共享的负载均衡资源。多集群入口旨在满足多集群、多区域环境的负载均衡需求。它是外部HTTP(S)负载均衡器的控制器，用于提供来自互联网的流量在单个或多个集群之间的入口。多集群入口的多集群支持满足许多用例，包括以下内容：
- en: A single, consistent VIP for an app, independent of where the app is deployed
    globally
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单一、一致的VIP，与应用部署的全球位置无关
- en: Multiregional, multicluster availability through health checking and traffic
    failover
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过健康检查和流量故障转移实现的多区域、多集群可用性
- en: Proximity-based routing through public Anycast VIPs for low client latency
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过公共Anycast VIP进行基于邻近性的路由，以降低客户端延迟
- en: Transparent cluster migration for upgrades or cluster rebuilds
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明集群迁移以进行升级或集群重建
- en: '![10-08](../../OEBPS/Images/10-08.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![10-08](../../OEBPS/Images/10-08.png)'
- en: Figure 10.8 Multicluster Ingress to multiple GKE clusters in GCP
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 多集群入口到GCP中的多个GKE集群
- en: Multicluster Ingress is an Ingress controller that programs the external HTTP(S)
    load balancer using NEGs. When you create a MultiClusterIngress resource, GKE
    deploys the Compute Engine load balancer resources and configures the appropriate
    Pods across clusters as backends. The NEGs are used to track Pod endpoints dynamically,
    so the Google load balancer has the right set of healthy backends.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群入口是一个Ingress控制器，它使用NEGs（网络端点组）来编程外部HTTP(S)负载均衡器。当你创建一个MultiClusterIngress资源时，GKE会部署计算引擎负载均衡器资源，并在集群中配置适当的Pod作为后端。NEGs用于动态跟踪Pod端点，因此Google负载均衡器拥有正确的健康后端集合。
- en: 'Multicluster Ingress uses a centralized Kubernetes API server to deploy Ingress
    across multiple clusters. This centralized API server is called the *config cluster*.
    Any GKE cluster can act as the config cluster. The config cluster uses two custom
    resource types: MultiClusterIngress and MultiClusterService. By deploying these
    resources on the config cluster, the Anthos Ingress controller deploys load balancers
    across multiple clusters. The following concepts and components make up Multicluster
    Ingress:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群入口使用集中的Kubernetes API服务器在多个集群中部署入口。这个集中的API服务器被称为*配置集群*。任何GKE集群都可以充当配置集群。配置集群使用两种自定义资源类型：MultiClusterIngress和MultiClusterService。通过在配置集群上部署这些资源，Anthos入口控制器在多个集群中部署负载均衡器。以下概念和组件构成了多集群入口：
- en: '*Anthos Ingress controller*—A globally distributed control plane that runs
    as a service outside of your clusters. This allows the life cycle and operations
    of the controller to be independent of GKE clusters.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos入口控制器*—一个全球分布的控制平面，作为集群外部的服务运行。这允许控制器的生命周期和操作独立于GKE集群。'
- en: '*Config cluster*—A chosen GKE cluster running on Google Cloud where the MultiClusterIngress
    and MultiClusterService resources are deployed. This is a centralized point of
    control for these multicluster resources, which exist in and are accessible from
    a single logical API to retain consistency across all clusters. The Ingress controller
    watches the config cluster and reconciles the load-balancing infrastructure.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*配置集群*—一个选择的运行在Google Cloud上的GKE集群，其中部署了MultiClusterIngress和MultiClusterService资源。这是这些多集群资源的集中控制点，这些资源存在于单个逻辑API中，并且可以从单个逻辑API访问，以保持所有集群的一致性。入口控制器监视配置集群并协调负载均衡基础设施。'
- en: '*Fleet*—A concept that groups clusters and infrastructure, manages resources,
    and keeps a consistent policy across them (for more details about fleets, see
    chapter 2). MCI uses the concept of fleets for how Ingress is applied across different
    clusters. Clusters that you register to a fleet become visible to MCI, so they
    can be used as backends for Ingress. Fleets possess a characteristic known as
    *namespace sameness*, which assumes that resources with identical names and the
    same namespace across clusters are instances of the same resource. In effect,
    this means that Pods in the ns1 namespace with the label app: foo across different
    clusters are all considered part of the same pool of application backends from
    the perspective of Multicluster Ingress. Figure 10.9 shows an example of two services,
    foo and bar, running on two clusters being load balanced by MCI.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*舰队*—一个将集群和基础设施分组的概念，管理资源，并在它们之间保持一致的政策（有关舰队的更多详细信息，请参阅第2章）。MCI使用舰队的概念来应用不同集群中的入口。你注册到舰队的集群对MCI可见，因此可以用作入口的后端。舰队具有一种称为*命名空间一致性*的特性，它假设跨集群中具有相同名称和相同命名空间的资源是同一资源的实例。实际上，这意味着不同集群中ns1命名空间具有标签app:
    foo的Pod都被视为多集群入口视角下的同一应用后端池的一部分。图10.9显示了两个服务foo和bar在两个集群上运行，并由MCI进行负载均衡的示例。'
- en: '![10-09](../../OEBPS/Images/10-09.png)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![10-09](../../OEBPS/Images/10-09.png)'
- en: 'Figure 10.9 Multicluster Ingress: Fleet and namespace sameness'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.9 多集群入口：舰队和命名空间一致性
- en: '*Member cluster*—Clusters registered to a fleet are called member clusters.
    Member clusters in the fleet comprise the full scope of backends that MCI is aware
    of.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成员集群*—注册到舰队中的集群被称为成员集群。舰队中的成员集群构成了MCI所了解的后端的全范围。'
- en: 'After the config cluster has been configured, you create the two custom resources,
    MultiClusterIngress and MultiClusterService, for your multicluster Service. Note
    that the resource names are comparatively similar to Service and Ingress, required
    for Ingress in a single cluster. Examples of these resources deployed to the config
    cluster follow:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置集群配置完成后，您为您的多集群 Service 创建了两个自定义资源，MultiClusterIngress 和 MultiClusterService。请注意，资源名称与
    Service 和 Ingress 相比较相似，这是在单个集群中需要 Ingress 的。以下是在配置集群上部署的这些资源的示例：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The MultiClusterService spec looks similar to the Service spec, with a clusters
    section added to define Service in multiple clusters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ MultiClusterService 规范看起来与 Service 规范类似，增加了一个集群部分来定义在多个集群中的 Service。
- en: ❷ The GKE Cluster URI links for Service running in multiple clusters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 多个集群中运行的 Service 的 GKE 集群 URI 链接
- en: ❸ The MultiClusterIngress spec is similar to the Ingress spec, except that it
    points to a MultiClusterService (instead of a Service).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ MultiClusterIngress 规范与 Ingress 规范类似，但指向的是 MultiClusterService（而不是 Service）。
- en: 'Note that MulticlusterService includes a cluster selector stanza at the bottom.
    Removing this sends Ingress traffic to all member clusters. Adding a cluster selector
    may be useful if you want to remove MCI traffic from a specific cluster (or clusters)—for
    example, if you are performing upgrades or maintenance to a cluster. If the clusters
    stanza is present in the MulticlusterService resource, Ingress traffic is sent
    to only the clusters available in the list. Clusters are explicitly referenced
    by <region | zone>/<name>. Member clusters within the same fleet and region should
    have unique names so that there are no naming collisions. Like a Service, MulticlusterService
    is a selector for Pods, but it is also capable of selecting labels and clusters.
    The pool of clusters that it selects across are called member clusters, and these
    are all the clusters registered to the fleet. This MulticlusterService deploys
    a derived Service in all member clusters with the selector app: foo. If app: foo
    Pods exist in that cluster, then those Pod IPs will be added as backends for the
    MulticlusterService.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，MulticlusterService 在底部包含一个集群选择段。移除此段会将 Ingress 流量发送到所有成员集群。如果您想从特定集群（或多个集群）中移除
    MCI 流量（例如，如果您正在对集群进行升级或维护），则添加集群选择器可能很有用。如果 MulticlusterService 资源中存在集群段，则 Ingress
    流量只会发送到列表中可用的集群。集群通过 <region | zone>/<name> 明确引用。同一舰队和区域内的成员集群应具有唯一名称，以避免命名冲突。像
    Service 一样，MulticlusterService 是 Pod 的选择器，但它还可以选择标签和集群。它所选择的集群池称为成员集群，这些是注册到舰队的所有集群。此
    MulticlusterService 在所有成员集群中部署了一个派生 Service，选择器为 app: foo。如果该集群中存在 app: foo Pods，则那些
    Pod IP 将被添加为 MulticlusterService 的后端。'
- en: Anthos on-prem (on VMware)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on-prem（在 VMware 上）
- en: Anthos on-prem clusters on VMware automatically create an island mode configuration
    in which Pods can directly talk to each other within a cluster but cannot be reached
    from outside the cluster. This configuration forms an “island” within the network
    that is not connected to the external network. Clusters form a full node-to-node
    mesh across the cluster nodes, allowing a Pod to reach other Pods within the cluster
    directly.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VMware 上的 Anthos on-prem 集群自动创建一个岛屿模式配置，其中 Pods 可以在集群内部直接相互通信，但不能从集群外部访问。这种配置在网络上形成一个“岛屿”，该岛屿未连接到外部网络。集群在集群节点之间形成一个完整的节点到节点的网状结构，允许
    Pod 直接访问集群内的其他 Pod。
- en: Networking requirements
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 网络要求
- en: 'Anthos on-prem clusters are installed using an admin workstation VM, which
    contains all the tools and configurations required to deploy Anthos on-prem clusters.
    The admin workstation VM deploys an admin cluster. The admin cluster deploys one
    or more user clusters. Your applications run on user clusters. The admin cluster
    manages the deployment and life cycle of multiple user clusters. You do not run
    your applications on the admin cluster. In your initial installation of Anthos
    on-prem, you create the following virtual machines (VMs):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on-prem 集群是通过一个管理员工作站虚拟机安装的，该虚拟机包含部署 Anthos on-prem 集群所需的所有工具和配置。管理员工作站虚拟机部署一个管理员集群。管理员集群部署一个或多个用户集群。您的应用程序运行在用户集群上。管理员集群管理多个用户集群的部署和生命周期。您不会在管理员集群上运行应用程序。在您的
    Anthos on-prem 初始安装中，您创建了以下虚拟机（VM）：
- en: One VM for an admin workstation
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于管理员工作站的虚拟机
- en: Four VMs for an admin cluster
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个用于管理员集群的虚拟机
- en: Three VMs for a user cluster (you can create additional user clusters as well
    as larger user clusters if needed)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个用于用户集群的虚拟机（如果需要，您还可以创建额外的用户集群或更大的用户集群）
- en: In your vSphere environment, you must have a network that can support the creation
    of these VMs. Your network must also be able to support a vCenter Server and a
    load balancer. Your network needs to support outbound traffic to the internet
    so that your admin workstation and your cluster nodes can fetch GKE on-prem components
    and call certain Google services. If you want external clients to call services
    in your GKE on-prem clusters, your network must support inbound traffic from the
    internet. IP address architecture and allocation is discussed in the next section.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的vSphere环境中，您必须有一个能够支持创建这些虚拟机的网络。您的网络还必须能够支持vCenter Server和负载均衡器。您的网络需要支持向互联网的出站流量，以便您的管理工作站和集群节点可以获取GKE本地组件并调用某些Google服务。如果您想外部客户端调用您的GKE本地集群中的服务，您的网络必须支持来自互联网的入站流量。IP地址架构和分配将在下一节讨论。
- en: Anthos on-prem cluster IP allocation
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos本地集群IP分配
- en: 'The following IP addresses are required for Anthos on-prem on a VMware cluster:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在VMware集群上运行Anthos本地所需的以下IP地址：
- en: '*Node IP*—Dynamic Host Configuration Protocol (DHCP) or statically assigned
    IP addresses for the nodes (virtual machines or VMs). Must be routable within
    the data center. You can manually assign static IPs. Node IP addressing depends
    on the implementation of a load balancer in the Anthos on-prem cluster. If the
    cluster is configured with integrated mode load balancing or bundled mode load
    balancing, you can use DHCP or statically assigned IP addresses for the nodes.
    If the cluster is configured with manual mode load balancing, you must use static
    IP addresses for nodes. In this case, ensure enough IP addresses are set aside
    to account for cluster growth. Load-balancing modes are discussed in detail in
    the next section.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点IP*—节点（虚拟机或VM）的动态主机配置协议（DHCP）或静态分配的IP地址。必须在数据中心内可路由。您可以手动分配静态IP。节点IP地址取决于Anthos本地集群中负载均衡器的实现。如果集群配置为集成模式负载均衡或捆绑模式负载均衡，您可以为节点使用DHCP或静态分配的IP地址。如果集群配置为手动模式负载均衡，您必须为节点使用静态IP地址。在这种情况下，请确保预留足够的IP地址以应对集群增长。负载均衡模式将在下一节详细讨论。'
- en: '*Pod IP CIDR*—Non-routable CIDR block for all Pods in the cluster. From this
    range, smaller /24 ranges are assigned per node. If you need an *N* node cluster,
    ensure this block is large enough to support *N* x /24 blocks.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pod IP CIDR*—集群中所有Pod的非路由CIDR块。从这个范围中，每个节点分配较小的/24范围。如果您需要一个*N*节点的集群，请确保此块足够大，以支持*N*
    x /24块。'
- en: '*Services IP*—In island mode, like Pod CIDR block, so this is used only within
    the cluster and is any private CIDR block that does not overlap with the nodes,
    VIPs, or Pod CIDR block. You can share the same block among multiple clusters.
    The size of the block determines the number of services. In addition to your Services,
    a block of Service IP addresses is used for cluster control plane Services. One
    Service IP is needed for the Ingress service, and 10 or more IPs for Kubernetes
    services like cluster DNS.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务IP*—在孤岛模式下，类似于Pod CIDR块，因此这仅在集群内部使用，是任何不与节点、VIP或Pod CIDR块重叠的私有CIDR块。您可以在多个集群之间共享相同的块。块的大小决定了服务的数量。除了您的服务外，一个服务IP地址块还用于集群控制平面服务。入口服务需要一个服务IP，Kubernetes服务（如集群DNS）需要10个或更多的IP地址。'
- en: Egress traffic and controls
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 出站流量和控制
- en: All egress traffic from the Pod to targets outside the cluster is run through
    NAT by the node IP. You can use NetworkPolicy to further control the flow of traffic
    between Pods within a cluster as well as traffic egressing Pods. These policies
    are enforced by Calico running inside each cluster. At the Service layer, you
    can use EgressPolicy through ASM to control what traffic exits the clusters. In
    this case, an Envoy proxy called the istio-egressgateway exists at the perimeter
    of the service mesh through which all egress traffic flows.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从Pod到集群外部的所有出站流量都通过节点IP进行NAT。您可以使用NetworkPolicy进一步控制集群内Pod之间的流量以及Pod的出站流量。这些策略由每个集群内运行的Calico强制执行。在服务层，您可以通过ASM使用EgressPolicy来控制什么流量离开集群。在这种情况下，一个名为istio-egressgateway的Envoy代理存在于服务网格的边缘，所有出站流量都通过它流过。
- en: Load balancers
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: 'Anthos on-prem clusters provide two ways to access Services from outside of
    the cluster: load balancers and Ingress. This section addresses load balancers
    and different modes of implementations.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos本地集群提供两种从集群外部访问服务的方法：负载均衡器和入口。本节讨论负载均衡器和不同的实现模式。
- en: 'Anthos on-prem clusters can run in one of three load-balancing modes: integrated,
    manual, or bundled:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on-prem 集群可以运行在三种负载均衡模式之一：集成、手动或捆绑：
- en: '*Integrated mode*—With integrated mode, Anthos on-prem uses an F5 BIG-IP load
    balancer. The customer provides the F5 BIG-IP load balancer with the appropriate
    licensing. You need to have a user role with sufficient permissions to set up
    and manage the F5 load balancer. Either the administrator role or the resource
    administrator role is sufficient. For more information, see [http://mng.bz/oJnN](http://mng.bz/oJnN).
    You set aside multiple VIP addresses to be used for Services, which are configured
    to be type Loadbalancer. Each Service requires one VIP. The number of VIPs required
    depends on the number of Services of type Loadbalancer. You specify these VIPs
    in your cluster configuration file, and Anthos on-prem automatically configures
    the F5 BIG-IP load balancer to use the VIPs.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成模式*—使用集成模式时，Anthos on-prem 使用 F5 BIG-IP 负载均衡器。客户需要提供带有适当许可的 F5 BIG-IP 负载均衡器。您需要拥有足够的权限来设置和管理
    F5 负载均衡器的用户角色。管理员角色或资源管理员角色就足够了。有关更多信息，请参阅 [http://mng.bz/oJnN](http://mng.bz/oJnN)。您需要预留多个
    VIP 地址用于服务，这些服务被配置为负载均衡器类型。每个服务需要一个 VIP。所需的 VIP 数量取决于负载均衡器类型服务的数量。您需要在集群配置文件中指定这些
    VIP，并且 Anthos on-prem 会自动配置 F5 BIG-IP 负载均衡器以使用这些 VIP。'
- en: The advantages of integrated mode are you get to use an enterprise-grade load
    balancer and its configuration is mostly automated. This mode is also opinionated
    in that it requires an F5 load balancer, which may incur additional licensing
    and support cost.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成模式的优势在于您可以使用企业级负载均衡器，并且其配置主要是自动化的。此模式也是具有偏见的，因为它要求使用 F5 负载均衡器，这可能会产生额外的许可和支持成本。
- en: '*Manual mode*—With manual mode, Anthos on-prem uses a load balancer of your
    choice. Manual load-balancing mode requires additional configuration compared
    to integrated mode. You need to manually configure the VIPs to be used for Services.
    With manual load balancing, you cannot run Services of type Loadbalancer. Instead,
    you can create Services of type NodePort and manually configure your load balancer
    to use them as backends. You must specify the NodePort values to be used for these
    Services. You can choose values in the 30000-32767 range. For more information,
    see [http://mng.bz/nJov](http://mng.bz/nJov).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*手动模式*—使用手动模式时，Anthos on-prem 使用您选择的负载均衡器。与集成模式相比，手动负载均衡模式需要额外的配置。您需要手动配置用于服务的
    VIP。在手动负载均衡的情况下，您不能运行负载均衡器类型的服务。相反，您可以创建节点端口类型的服务，并手动配置负载均衡器以使用它们作为后端。您必须指定用于这些服务的节点端口值。您可以选择
    30000-32767 范围内的值。有关更多信息，请参阅 [http://mng.bz/nJov](http://mng.bz/nJov)。'
- en: The advantage of manual mode is you get absolute freedom in terms of what load
    balancer you choose. On the other hand, the configuration is manual, which may
    result in increased operational overhead.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手动模式的优势在于您在选择负载均衡器方面拥有绝对自由。另一方面，配置是手动的，这可能会导致运营成本增加。
- en: '*Bundled mode*—In bundled load-balancing mode, Anthos on-prem provides and
    manages the load balancer. Unlike integrated mode, no license is required for
    a load balancer, and the amount of setup that you must do is minimal. The bundled
    load balancer that GKE on-prem provides is the *Seesaw load balancer* ([https://github.com/google/seesaw](https://github.com/google/seesaw)).
    Seesaw load balancers run as VMs inside VMware. We recommend that you use vSphere
    6.7+ and Virtual Distributed Switch 6.6+ for bundled load-balancing mode. You
    can run Seesaw load balancer in high availability (HA) and non-HA mode. In HA
    mode, two Seesaw VMs are configured. In non-HA mode, a single Seesaw VM is configured.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*捆绑模式*—在捆绑负载均衡模式下，Anthos on-prem 提供并管理负载均衡器。与集成模式不同，不需要为负载均衡器购买许可证，并且您必须进行的设置工作非常少。GKE
    on-prem 提供的捆绑负载均衡器是 *Seesaw 负载均衡器* ([https://github.com/google/seesaw](https://github.com/google/seesaw))。Seesaw
    负载均衡器在 VMware 内作为虚拟机运行。我们建议您在捆绑负载均衡模式下使用 vSphere 6.7+ 和 Virtual Distributed Switch
    6.6+。您可以在高可用性 (HA) 和非 HA 模式下运行 Seesaw 负载均衡器。在 HA 模式下，配置了两个 Seesaw 虚拟机。在非 HA 模式下，配置了一个
    Seesaw 虚拟机。'
- en: The advantage of bundled mode is a single team can oversee both cluster creation
    and load balancer configuration. For example, a cluster administration team would
    not have to rely on a separate networking team to acquire, run, and configure
    the load balancer ahead of time. Another advantage is that the configuration is
    completely automated. Anthos on-prem configures the Service VIPs automatically
    on the load balancer.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 捆绑模式的优点是一个团队可以监督集群创建和负载均衡器配置。例如，集群管理团队不需要依赖单独的网络团队提前获取、运行和配置负载均衡器。另一个优点是配置完全自动化。Anthos
    on-prem 会自动在负载均衡器上配置服务 VIP。
- en: Anthos on VMware clusters can run Services of type Loadbalancer as long as a
    loadBalancerIP field is configured in the Service’s specification. In the loadBalancerIP
    field, you need to provide the VIP that you want to use. This will be configured
    on F5, pointing to the NodePorts of the Service.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VMware 集群中，只要在服务的规范中配置了 loadBalancerIP 字段，Anthos on VMware 集群就可以运行类型为 Loadbalancer
    的服务。在 loadBalancerIP 字段中，您需要提供您想要使用的 VIP。这将配置在 F5 上，指向服务的 NodePort。
- en: 'An example of a Service manifest follows. You can access a Service running
    inside an Anthos on-prem cluster called frontend via the SERVICE VIP:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个服务清单的示例。您可以通过 SERVICE VIP 访问名为 frontend 的 Anthos on-prem 集群内部运行的服务：
- en: '[PRE9]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The load balancer IP address is defined in the Service spec.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 负载均衡器 IP 地址在服务规范中定义。
- en: In addition to Service VIPs, a control plane VIP for the Kubernetes API server
    is required by the load balancer. And last, an Ingress controller runs inside
    each Anthos on-prem cluster. The Ingress controller Service also has a VIP called
    the Ingress VIP. Services exposed via Ingress use the Ingress VIP to access Kubernetes
    Services.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 除了服务 VIP 之外，负载均衡器还需要 Kubernetes API 服务器的控制平面 VIP。最后，每个 Anthos on-prem 集群内部运行一个
    Ingress 控制器。Ingress 控制器服务也有一个称为 Ingress VIP 的 VIP。通过 Ingress 暴露的服务使用 Ingress VIP
    访问 Kubernetes 服务。
- en: The Anthos on-prem high-level load-balancing architecture is shown in figure
    10.10.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on-prem 的高级负载均衡架构如图 10.10 所示。
- en: '![10-10](../../OEBPS/Images/10-10.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![10-10](../../OEBPS/Images/10-10.png)'
- en: 'Figure 10.10 Anthos on-prem: load balancer network architecture'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 Anthos on-prem：负载均衡器网络架构
- en: Table 10.2 summarizes what you must do to prepare for load balancing in various
    modes.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 总结了在各种模式下准备负载均衡必须执行的操作。
- en: Table 10.2 How to prepare for load balancing
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 如何为负载均衡做准备
- en: '|  | Integrated/bundled mode | Manual mode |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 集成/捆绑模式 | 手动模式 |'
- en: '| Choose VIPs before you create your clusters. | Yes | Yes |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 在创建集群之前选择 VIP | 是 | 是 |'
- en: '| Choose node IP addresses before you create your clusters. | No, if using
    DHCPYes, if using static IP addresses | Yes |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 在创建集群之前选择节点 IP 地址 | 如果使用 DHCP 否，如果使用静态 IP 地址 是 | 是 |'
- en: '| Choose nodePort values before you create your clusters. | No | Yes |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 在创建集群之前选择 nodePort 值 | 否 | 是 |'
- en: '| Manually configure your load balancer | No | Yes |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 手动配置您的负载均衡器 | 否 | 是 |'
- en: Ingress
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress
- en: Anthos on-prem includes an L7 load balancer with an Envoy-based Ingress controller
    that handles Ingress object rules for ClusterIP Services deployed within the cluster.
    The Ingress controller itself is exposed as a NodePort Service in the cluster.
    The Ingress NodePort Service can be reached through a L3/L4 F5 load balancer.
    The installation configures a VIP address (Ingress VIP) (with port 80 and 443)
    on the load balancer. The VIP points to the ports in the NodePort Service for
    the Ingress controller. This is how external clients can access services in the
    cluster.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on-prem 包含一个基于 Envoy 的 Ingress 控制器的 L7 负载均衡器，该控制器处理集群内部部署的 ClusterIP
    服务中的 Ingress 对象规则。Ingress 控制器本身在集群中作为 NodePort 服务暴露。Ingress NodePort 服务可以通过 L3/L4
    F5 负载均衡器访问。安装配置了负载均衡器上的 VIP 地址（Ingress VIP）（端口 80 和 443）。VIP 指向 Ingress 控制器的 NodePort
    服务端口。这就是外部客户端如何访问集群中的服务。
- en: 'To expose a Service via Ingress, you must create a DNS A record to point the
    DNS name to the Ingress VIP. Then you can create a Service and an Ingress resource
    for the Service. For example, let’s say you want to expose a frontend Service
    of a sample guestbook application. First, create a DNS A record for the guestbook
    application pointing to the Ingress VIP as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 Ingress 暴露服务，您必须创建一个 DNS A 记录，将 DNS 名称指向 Ingress VIP。然后您可以创建一个服务和一个 Ingress
    资源。例如，假设您想暴露一个示例 guestbook 应用程序的前端服务。首先，为 guestbook 应用程序创建一个指向 Ingress VIP 的 DNS
    A 记录，如下所示：
- en: '[PRE10]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, create a Service for the frontend Deployment. Note that the Service is
    of type ClusterIP:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为前端部署创建一个服务。请注意，服务类型为 ClusterIP：
- en: '[PRE11]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ For Ingress, the Service type is ClusterIP (instead of Loadbalancer).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于入口，服务类型是 ClusterIP（而不是 Loadbalancer）。
- en: 'Finally, create the Ingress rule:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建入口规则：
- en: '[PRE12]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The Ingress rule points to the Service name and port.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 入口规则指向服务名称和端口。
- en: Anthos on bare metal
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 在裸金属上
- en: Anthos on bare metal is a GCP-supported Anthos GKE implementation deployed on
    bare metal servers. Anthos on bare metal eliminates the need for a virtualization
    layer or a hypervisor. All cluster nodes and API servers run directly on bare
    metal servers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 在裸金属上是 GCP 支持的 Anthos GKE 实现，部署在裸金属服务器上。Anthos 在裸金属上消除了对虚拟化层或虚拟机管理程序的必要性。所有集群节点和
    API 服务器都直接在裸金属服务器上运行。
- en: Anthos on bare metal deployment models and networking architecture are described
    in detail in chapter 17.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 在裸金属部署模型和网络架构的详细描述见第 17 章。
- en: Anthos on AWS
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on AWS
- en: 'Anthos clusters on AWS (GKE on AWS) is hybrid cloud software that extends Google
    Kubernetes Engine to Amazon Web Services. Anthos on AWS uses AWS resources such
    as Elastic Compute Cloud (EC2), Elastic Block Storage (EBS), and Elastic Load
    Balancer (ELB). Anthos clusters on AWS have the following two components:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 上的 Anthos 集群（GKE on AWS）是一种混合云软件，它将 Google Kubernetes Engine 扩展到 Amazon
    Web Services。Anthos on AWS 使用 AWS 资源，如弹性计算云（EC2）、弹性块存储（EBS）和弹性负载均衡器（ELB）。AWS 上的
    Anthos 集群具有以下两个组件：
- en: '*Management service*—An environment that can install and update your user clusters,
    uses the AWS API to provision resources'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理服务*—一个可以安装和更新您的用户集群的环境，使用 AWS API 来配置资源'
- en: '*User clusters*—Anthos on AWS clusters where you run your containerized applications'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户集群*—运行您的容器化应用程序的 Anthos on AWS 集群'
- en: Networking requirements
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 网络需求
- en: Both the management service and the user clusters are deployed inside an AWS
    VPC on EC2 instances. You can create your management service in a *dedicated AWS
    VPC* ([http://mng.bz/v1ax](http://mng.bz/v1ax)) or an *existing AWS VPC* ([http://mng.bz/41GB](http://mng.bz/41GB)).
    You need a management service in every AWS VPC where you run Anthos on AWS user
    clusters. The management service is installed in one AWS Availability Zone. You
    only need one management service per VPC; a management service can manage multiple
    user clusters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 管理服务和用户集群都部署在 EC2 实例上的 AWS VPC 内。您可以在一个 *专用 AWS VPC* ([http://mng.bz/v1ax](http://mng.bz/v1ax))
    或一个 *现有 AWS VPC* ([http://mng.bz/41GB](http://mng.bz/41GB)) 中创建您的管理服务。在您运行 Anthos
    on AWS 用户集群的每个 AWS VPC 中都需要一个管理服务。管理服务安装在单个 AWS 可用区中。每个 VPC 只需要一个管理服务；一个管理服务可以管理多个用户集群。
- en: 'A user cluster consists of two components: a control plane or the Kubernetes
    API server and node pools where your applications run. The management service’s
    primary component is a cluster operator. The cluster operator is a Kubernetes
    operator that creates and manages your AWSClusters and AWSNodePools. The cluster
    operator stores configuration in an etcd database with storage persisted on an
    AWS EBS volume. An AWSClusters resource creates and manages the user clusters’
    control plane, and an AWSNodePools resource creates and manages the user clusters’
    node pools.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群由两个组件组成：控制平面或 Kubernetes API 服务器以及运行应用程序的节点池。管理服务的主要组件是集群操作员。集群操作员是一个 Kubernetes
    操作员，它创建和管理您的 AWSClusters 和 AWSNodePools。集群操作员将配置存储在具有存储在 AWS EBS 卷上的持久存储的 etcd
    数据库中。AWSClusters 资源创建和管理用户集群的控制平面，AWSNodePools 资源创建和管理用户集群的节点池。
- en: When you install a management cluster into a dedicated VPC, Anthos on AWS creates
    control plane replicas in every zone you specify in dedicatedVPC.availabilityZones.
    When you install a management cluster into existing infrastructure, Anthos on
    AWS creates an AWSCluster with three control plane replicas in the same Availability
    Zones. Each replica belongs to its own AWS Auto Scaling group, which restarts
    instances when they are terminated. The management service places the control
    planes in a private subnet behind an AWS Network Load Balancer (NLB). The management
    service interacts with the control plane using NLB.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将管理集群安装到专用 VPC 中时，Anthos on AWS 在您在 dedicatedVPC.availabilityZones 中指定的每个区域创建控制平面副本。当您将管理集群安装到现有基础设施中时，Anthos
    on AWS 在相同的可用区创建一个具有三个控制平面副本的 AWSCluster。每个副本属于其自己的 AWS Auto Scaling 组，当实例被终止时，它会重新启动实例。管理服务将控制平面放置在
    AWS 网络负载均衡器（NLB）后面的私有子网中。管理服务使用 NLB 与控制平面交互。
- en: As shown in figure 10.11, each control plane stores configuration in a local
    etcd database. These databases are replicated and set up in a stacked, high-availability
    topology ([http://mng.bz/wPKW](http://mng.bz/wPKW)). One control plane manages
    one or more AWSNodePools.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 10.11 所示，每个控制平面将配置存储在本地 etcd 数据库中。这些数据库被复制并设置在堆叠的高可用拓扑中 ([http://mng.bz/wPKW](http://mng.bz/wPKW))。一个控制平面管理一个或多个
    AWSNodePools。
- en: '![10-11](../../OEBPS/Images/10-11.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![10-11](../../OEBPS/Images/10-11.png)'
- en: Figure 10.11 Anthos on AWS architecture
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 Anthos on AWS 架构
- en: 'The following VPC resources are required when creating Anthos on AWS clusters
    in a dedicated VPC:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在专用 VPC 中创建 Anthos on AWS 集群时，需要以下 VPC 资源：
- en: '*VPC CIDR range*—The total CIDR range of IP addresses for the AWS VPC that
    anthos-gke creates, for example, 10.0.0.0/16.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VPC CIDR 范围*—anthos-gke 创建的 AWS VPC 的 IP 地址总 CIDR 范围，例如，10.0.0.0/16。'
- en: '*Availability Zones*—The AWS EC2 Availability Zones where you want to create
    nodes and control planes.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可用区*—您想要在其中创建节点和控制平面的 AWS EC2 可用区。'
- en: '*Private CIDR block*—The CIDR block for your private subnet. Anthos on AWS
    components, such as the management service, run in the private subnet. This subnet
    must be within the VPC’s CIDR range specified in vpcCIDRBlock. You need one subnet
    for each Availability Zone.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*私有 CIDR 块*—您的私有子网的 CIDR 块。Anthos on AWS 组件（如管理服务）在私有子网中运行。此子网必须在 vpcCIDRBlock
    中指定的 VPC 的 CIDR 范围内。您需要为每个可用区分配一个子网。'
- en: '*Public CIDR block*—The CIDR blocks for your public subnet. You need one subnet
    for each Availability Zone. The public subnet exposes cluster services such as
    load balancers to the security groups and address ranges specified in AWS network
    ACLs and security groups.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共 CIDR 块*—您的公共子网的 CIDR 块。您需要为每个可用区分配一个子网。公共子网将集群服务（如负载均衡器）暴露给 AWS 网络访问控制列表和安全组中指定的安全组和地址范围。'
- en: '*SSH CIDR block*—The CIDR block that allows inbound SSH to your bastion host.
    You can use IP ranges, for example, 203.0.113.0/24\. If you want to allow SSH
    from any IP address, use 0.0.0.0/0\. When you create a management service using
    the default settings, the control plane has a private IP address. This IP address
    isn’t accessible from outside the AWS VPC. You can access the management service
    with a *bastion host* or using another connection to the AWS VPC such as a VPN
    or *AWS Direct Connect* ([https://aws.amazon.com/directconnect/](https://aws.amazon.com/directconnect/)).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SSH CIDR 块*—允许对您的堡垒主机进行 SSH 入站的 CIDR 块。例如，您可以使用 IP 范围，如 203.0.113.0/24。如果您想允许从任何
    IP 地址进行 SSH，请使用 0.0.0.0/0。当您使用默认设置创建管理服务时，控制平面有一个私有 IP 地址。此 IP 地址无法从 AWS VPC 外部访问。您可以使用
    *堡垒主机* 或使用其他连接到 AWS VPC 的方式（如 VPN 或 *AWS Direct Connect* ([https://aws.amazon.com/directconnect/](https://aws.amazon.com/directconnect/)))
    访问管理服务。'
- en: 'The following VPC resources are required when creating Anthos on AWS clusters
    in an existing VPC:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有 VPC 中创建 Anthos on AWS 集群时，需要以下 VPC 资源：
- en: At least one public subnet.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个公共子网。
- en: At least one private subnet.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个私有子网。
- en: An internet gateway with a route to the public subnet.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有指向公共子网路由的互联网网关。
- en: A NAT gateway with a route to the private subnet.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有指向私有子网路由的 NAT 网关。
- en: DNS hostnames enabled.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用 DNS 主机名。
- en: No custom value for domain-name in your DHCP options sets. Anthos on AWS does
    not support values other than the default EC2 domain names.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的 DHCP 选项集中没有为域名指定自定义值。Anthos on AWS 不支持除默认 EC2 域名以外的值。
- en: Choose or create an AWS security group that allows SSH (port 22) inbound from
    the security groups or IP ranges where you will be managing your Anthos clusters
    on AWS installation.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择或创建一个 AWS 安全组，允许从您将管理 AWS 安装上的 Anthos 集群的安全组或 IP 范围（端口 22）进行 SSH 入站。
- en: Anthos on AWS cluster IP allocation
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on AWS 集群 IP 分配
- en: 'The management service creates user clusters and uses a cluster operator with
    the resources AWSClusters and AWSNodePools to create the user clusters’ control
    planes and node pools, respectively. The IP address per user cluster is defined
    in the AWSCluster resource. An example of an AWSCluster resource follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 管理服务创建用户集群，并使用具有资源的集群操作员 AWSClusters 和 AWSNodePools 分别创建用户集群的控制平面和节点池。每个用户集群的
    IP 地址在 AWSCluster 资源中定义。以下是一个 AWSCluster 资源的示例：
- en: '[PRE13]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Anthos on AWS cluster networking values are defined.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了 Anthos on AWS 集群的网络值。
- en: ❷ Anthos on AWS cluster control plane values are defined.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义了 Anthos on AWS 集群的控制平面值。
- en: You define the required IP addresses in the networking section.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您在网络部分定义所需的 IP 地址。
- en: 'An Anthos on AWS cluster requires the following IP addresses:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on AWS 集群需要以下 IP 地址：
- en: '*Node IP*—Node IPs are assigned to the EC2 instances as they are created. Each
    EC2 instance is assigned a single IP from the private subnet in its availability
    zone. These addresses are defined in the management service spec.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点IP*—节点IP在创建EC2实例时分配。每个EC2实例在其可用区中分配一个私有子网的单个IP。这些地址在管理服务规范中定义。'
- en: '*Pod IP CIDR*—The CIDR range of IPv4 addresses used by the cluster’s Pods.
    The range must be within your VPC CIDR address range but not part of a subnet.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pod IP CIDR*—集群Pod使用的IPv4地址的CIDR范围。该范围必须在您的VPC CIDR地址范围内，但不能是子网的一部分。'
- en: '*Services IP*—The range of IPv4 addresses used by the cluster’s Services. The
    range must be within your VPC CIDR address range but not part of a subnet.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务IP*—集群服务使用的IPv4地址范围。该范围必须在您的VPC CIDR地址范围内，但不能是子网的一部分。'
- en: Egress traffic and controls
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 出站流量和控制
- en: All egress traffic from the Pod to targets outside the cluster is run through
    NAT by the node IP. You can use NetworkPolicy to further control the flow of traffic
    between Pods within a cluster as well as traffic egressing Pods. These policies
    are enforced by Calico running inside each cluster. At the Service layer, you
    can use EgressPolicy through ASM to control what traffic exits the clusters. In
    this case, an Envoy proxy called the istio-egressgateway exists at the perimeter
    of the service mesh through which all egress traffic flows.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 从Pod到集群外部的所有出站流量都通过节点IP进行NAT。您可以使用NetworkPolicy进一步控制集群内Pod之间的流量以及Pod的出站流量。这些策略由每个集群内运行的Calico强制执行。在服务层，您可以通过ASM使用EgressPolicy来控制离开集群的流量。在这种情况下，一个名为istio-egressgateway的Envoy代理存在于服务网格的边缘，所有出站流量都通过它流动。
- en: In addition, you can control the traffic flow at the AWSNodePools security group
    layer. With security groups, you can further allow or deny traffic for both Ingress
    and egress.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以在AWSNodePools安全组层控制流量。使用安全组，您可以进一步允许或拒绝进出流量的访问。
- en: Load balancers
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: When you create a Service of type Loadbalancer, a Anthos on AWS controller configures
    a classic or network ELB on AWS. Anthos on AWS requires tags on subnets that contain
    load balancer endpoints. Anthos on AWS automatically tags all subnets specified
    in the spec.Networking.ServiceLoadBalancerSubnetIDs field ([http://mng.bz/X5mY](http://mng.bz/X5mY))
    of the AWSCluster resource.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个类型为Loadbalancer的服务时，Anthos on AWS控制器会在AWS上配置一个经典或网络ELB。Anthos on AWS需要在包含负载均衡器端点的子网上使用标签。Anthos
    on AWS会自动为AWSCluster资源中Networking.ServiceLoadBalancerSubnetIDs字段指定的所有子网添加标签（[http://mng.bz/X5mY](http://mng.bz/X5mY)）。
- en: 'To create the tag, get the subnet ID of the load balancer subnets. Use the
    aws command-line utility to create the tag on the subnets as follows. For multiple
    subnets, make sure the subnet IDs are separated by spaces:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建标签，获取负载均衡器子网的子网ID。使用aws命令行工具在子网上创建标签，如下所示。对于多个子网，请确保子网ID之间用空格分隔：
- en: '[PRE14]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You need a tag for every user cluster on the subnet.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在子网上为每个用户集群添加一个标签。
- en: You can create internal and external load balancers. Internal load balancers
    are created on the private subnets whereas external load balancers are created
    on the public subnets. You can create either type of load balancer using either
    a classic or a network load balancer. For more information on the differences
    between load balancer types, see the AWS documentation ([http://mng.bz/ydpJ](http://mng.bz/ydpJ)).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建内部和外部负载均衡器。内部负载均衡器在私有子网上创建，而外部负载均衡器在公共子网上创建。您可以使用经典或网络负载均衡器创建任何类型的负载均衡器。有关负载均衡器类型之间的差异的更多信息，请参阅AWS文档（[http://mng.bz/ydpJ](http://mng.bz/ydpJ)）。
- en: 'Different types of load balancers are created using annotations. Consider the
    following Service spec:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 使用注解创建不同类型的负载均衡器。考虑以下服务规范：
- en: '[PRE15]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This resource creates a classic public load balancer for the Service. To create
    a public network load balancer, add the following annotation to the previous spec:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 此资源为服务创建一个经典公共负载均衡器。要创建公共网络负载均衡器，请将以下注解添加到前面的规范中：
- en: '[PRE16]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The annotation creates a classic public load balancer in AWS, exposing a Service.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该注解在AWS中创建一个经典公共负载均衡器，并暴露一个服务。
- en: 'To create a private classic load balancer, add the following annotation to
    the Service spec:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个私有经典负载均衡器，请将以下注解添加到服务规范中：
- en: '[PRE17]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The annotation creates an internal load balancer in AWS, exposing a Service.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该注解在AWS中创建一个内部负载均衡器，并暴露一个服务。
- en: 'Finally, to create a private network load balancer, add both annotations to
    the Service spec:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要创建一个私有网络负载均衡器，请将这两个注解添加到服务规范中：
- en: '[PRE18]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Both annotations together create an internal network load balancer in AWS.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这两个注解共同在 AWS 中创建了一个内部网络负载均衡器。
- en: Ingress
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress
- en: 'You can use Ingress on Anthos on AWS clusters in the following two ways:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 AWS 集群上的 Anthos 中以下两种方式使用 Ingress：
- en: '*Application Load Balancer*—Application Load Balancer (ALB) ([http://mng.bz/Mlr2](http://mng.bz/Mlr2))
    is an AWS-managed L7 HTTP load balancer. After the load balancer receives a request,
    it evaluates the listener rules in priority order to determine which rule to apply
    and then selects a target from the target group for the rule action. This method
    uses an alb-ingress-controller installed in the Anthos on AWS cluster with proper
    permissions to create ALBs for Ingress.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用程序负载均衡器*—应用程序负载均衡器 (ALB) ([http://mng.bz/Mlr2](http://mng.bz/Mlr2)) 是一个
    AWS 管理的 L7 HTTP 负载均衡器。负载均衡器接收到请求后，将按优先级顺序评估监听器规则，以确定要应用哪个规则，然后从目标组中选择一个目标进行规则操作。此方法使用安装在
    Anthos on AWS 集群中的 alb-ingress-controller，并具有适当的权限来为 Ingress 创建 ALB。'
- en: '*ASM Ingress*—You can install Anthos Service Mesh on an Anthos on AWS cluster
    and use ASM Ingress. ASM Ingress, a Service called istio-ingressgateway, in an
    L7 Envoy proxy that lives at the perimeter of the service mesh. The istio-ingressgateway
    Service itself is exposed using ELB, as described in the previous section. All
    L7 load balancing and routing is handled by the istio-ingressgateway.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ASM Ingress*—您可以在 Anthos on AWS 集群上安装 Anthos 服务网格并使用 ASM Ingress。ASM Ingress
    是一个名为 istio-ingressgateway 的服务，它位于服务网格边缘的 L7 Envoy 代理中。istio-ingressgateway 服务本身使用
    ELB 暴露，如前所述。所有 L7 负载均衡和路由都由 istio-ingressgateway 处理。'
- en: Exposing Services using Ingress
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ingress 暴露服务
- en: To use the ALB method, follow the instructions at [http://mng.bz/aMpJ](http://mng.bz/aMpJ)
    and deploy the alb-ingress-controller to the Anthos on AWS cluster. The alb-ingress-controller
    is a Deployment that runs on the Anthos on AWS cluster with proper AWS credentials
    and Kubernetes RBAC permission to create the rules and resources required to create
    an ALB for Ingress.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ALB 方法，请按照[http://mng.bz/aMpJ](http://mng.bz/aMpJ)中的说明操作，并将 alb-ingress-controller
    部署到 Anthos on AWS 集群。alb-ingress-controller 是一个在 Anthos on AWS 集群上运行的 Deployment，具有适当的
    AWS 凭证和 Kubernetes RBAC 权限，可以创建创建 ALB 所需的规则和资源。
- en: 'You can now create an Ingress resource with proper annotations to create an
    ALB and the required resources for your Service. An example of a Service spec
    follows. Note that the type of the Service must be NodePort:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以创建一个带有适当注解的 Ingress 资源，以创建 ALB 和您服务所需的相关资源。以下是一个服务规范的示例。请注意，服务类型必须是 NodePort：
- en: '[PRE19]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And the Ingress resource to expose this Service using an ALB is shown next.
    Note the two annotations that configures an internet-facing ALB:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个展示了使用 ALB 暴露此服务的 Ingress 资源。注意配置面向互联网 ALB 的两个注解：
- en: '[PRE20]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ These annotations create an internet-facing Application Load Balancer in AWS.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这些注解在 AWS 中创建了一个面向互联网的应用程序负载均衡器。
- en: You can also use ASM Ingress to expose your Services. To use ASM, follow the
    steps at [http://mng.bz/gJKR](http://mng.bz/gJKR) to install ASM on your Anthos
    on AWS cluster. Once ASM is installed, you should see the istio-ingressgateway
    Deployment and Service in the istio-system namespace.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 ASM Ingress 来暴露您的服务。要使用 ASM，请按照[http://mng.bz/gJKR](http://mng.bz/gJKR)中的步骤在您的
    Anthos on AWS 集群上安装 ASM。一旦 ASM 安装完成，您应该在 istio-system 命名空间中看到 istio-ingressgateway
    部署和服务。
- en: 'An example of the Service spec looks like the following. Note that the Service
    type is ClusterIP instead of NodePort, used in the ALB method. The reason is that
    in the case of ASM, the L7 proxy runs inside the cluster, whereas the ALB is a
    managed HTTP load balancer that runs outside of the cluster:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 服务规范的示例如下。请注意，服务类型是 ClusterIP，而不是在 ALB 方法中使用的 NodePort。原因是，在 ASM 的情况下，L7 代理在集群内部运行，而
    ALB 是一个在集群外部运行的托管 HTTP 负载均衡器：
- en: '[PRE21]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And the Ingress resource looks like the following. Note the annotation that
    uses ASM for Ingress:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 资源如下所示。注意使用 ASM 的注解：
- en: '[PRE22]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The annotation uses the Istio Ingress controller.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该注解使用 Istio Ingress 控制器。
- en: Anthos attached clusters
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 附加集群
- en: The final type of Anthos clusters is Anthos attached clusters. Attaching clusters
    lets you view your existing Kubernetes clusters in the Google Cloud console along
    with your Anthos clusters and enable a subset of Anthos features on them, including
    configuration with Anthos Config Management. You can attach any conformant Kubernetes
    cluster to Anthos and view it in the Cloud console with your Anthos clusters.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 集群的最后一种类型是附加集群。附加集群允许你在 Google Cloud 控制台中查看你的现有 Kubernetes 集群以及你的 Anthos
    集群，并在它们上启用 Anthos 的一些功能子集，包括与 Anthos Config Management 的配置。你可以将任何符合标准的 Kubernetes
    集群附加到 Anthos，并在 Cloud 控制台中与你的 Anthos 集群一起查看。
- en: Regardless of where your clusters are, you need to register any clusters that
    you want to use with Anthos with your project’s fleet by using Connect. A fleet
    provides a unified way to view and manage multiple clusters and their workloads
    as part of Anthos. We previously discussed fleets in regard to Anthos GKE on GCP,
    but on-prem clusters can join a fleet as well. Any Anthos cluster can be part
    of any one fleet, though not all features are available, based on where the cluster
    is located.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的集群在哪里，你都需要使用 Connect 将你想要与 Anthos 一起使用的任何集群注册到你的项目舰队中。舰队提供了一种统一的方式来查看和管理多个集群及其工作负载，作为
    Anthos 的一部分。我们之前讨论了与 GCP 上的 Anthos GKE 相关的舰队，但本地集群也可以加入舰队。任何 Anthos 集群都可以成为任何单一舰队的成员，尽管并非所有功能都可用，这取决于集群的位置。
- en: Networking requirements
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 网络要求
- en: 'To successfully register your cluster, you need to ensure that the following
    domains are reachable from your Kubernetes cluster:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功注册你的集群，你需要确保以下域名可以从你的 Kubernetes 集群中访问：
- en: cloudresourcemanager.googleapis.com—Resolves metadata regarding the Google Cloud
    project the connecting cluster
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cloudresourcemanager.googleapis.com—解决有关连接集群的 Google Cloud 项目的元数据
- en: oauth2.googleapis.com—To obtain short-lived OAuth tokens for agent operations
    against gkeconnect.googleapis.com
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: oauth2.googleapis.com—为了获取针对 gkeconnect.googleapis.com 的代理操作的短期 OAuth 令牌
- en: gkeconnect.googleapis.com—To establish the channel used to receive requests
    from Google Cloud and issue responses
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkeconnect.googleapis.com—用于接收来自 Google Cloud 的请求并发出响应的通道
- en: gkehub.googleapis.com—To create Google Cloud-side hub membership resources that
    correspond to the cluster you’re connecting with Google Cloud
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkehub.googleapis.com—用于创建与 Google Cloud 连接的集群相对应的 Google Cloud 端的枢纽成员资源
- en: www.googleapis.com—To authenticate service tokens from incoming Google Cloud
    service requests
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: www.googleapis.com—用于验证来自传入 Google Cloud 服务请求的服务令牌
- en: gcr.io—To pull a GKE Connect Agent image
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gcr.io—用于拉取 GKE Connect 代理镜像
- en: If you’re using a proxy for Connect, you must also update the proxy’s allow
    list with these domains. If you use gcloud to register your Kubernetes cluster,
    these domains also need to be reachable in the fleet where you run the gcloud
    commands.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用代理进行 Connect，你还必须更新代理的允许列表以包含这些域。如果你使用 gcloud 注册你的 Kubernetes 集群，这些域也必须在运行
    gcloud 命令的舰队中可访问。
- en: You only need outbound connectivity on port 443 to these domains. No inbound
    connections are required to register Anthos attached clusters. You can also use
    VPC Service Controls for additional TE security.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要对这些域的 443 端口进行出站连接。注册附加集群不需要入站连接。你也可以使用 VPC 服务控制来提供额外的 TE 安全性。
- en: Using VPC Service Controls
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VPC 服务控制
- en: 'If you use *VPC Service Controls* ([http://mng.bz/51z1](http://mng.bz/51z1))
    for additional data security in your application, ensure that the following services
    are in your service perimeter:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在应用程序中使用 *VPC 服务控制* ([http://mng.bz/51z1](http://mng.bz/51z1)) 以提供额外的数据安全，请确保以下服务位于你的服务边界内：
- en: Resource Manager API (cloudresourcemanager.googleapis.com)
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 资源管理器 API (cloudresourcemanager.googleapis.com)
- en: GKE Connect API (gkeconnect.googleapis.com)
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GKE Connect API (gkeconnect.googleapis.com)
- en: GKE Hub API (gkehub.googleapis.com)
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GKE Hub API (gkehub.googleapis.com)
- en: You also need to set up private connectivity for access to the relevant APIs.
    You can find out how to do this at [http://mng.bz/610D](http://mng.bz/610D).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要设置私有连接以访问相关的 API。你可以在 [http://mng.bz/610D](http://mng.bz/610D) 上找到如何做到这一点的方法。
- en: 10.2.2 Anthos GKE IP address management
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 Anthos GKE IP 地址管理
- en: Except for Anthos GKE on GCP, all other Anthos clusters operate in an island
    mode configuration in which Pods can directly talk to each other within a cluster
    but cannot be reached from outside the cluster. This configuration forms an “island”
    within the network that is not connected to the external network. This allows
    you to create multiple Anthos clusters using the same IP addressing.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Anthos GKE on GCP之外，所有其他Anthos集群都在岛模式配置下运行，其中Pod可以在集群内部直接相互通信，但不能从集群外部访问。这种配置在网络上形成一个“岛屿”，该岛屿不连接到外部网络。这允许您使用相同的IP寻址创建多个Anthos集群。
- en: For Anthos clusters in island mode, IP address management and IP exhaustion
    is not an problem. You can standardize on an IP schema and use the same schema
    for all clusters.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 对于岛模式下的Anthos集群，IP地址管理和IP耗尽不是问题。您可以采用标准化的IP方案，并为所有集群使用相同的方案。
- en: 'GCP recommends running Anthos GKE on GCP clusters in VPC-native mode. In VPC-native
    mode, any IP address used by any cluster is a real VPC IP address. This means
    with VPC-native clusters, you cannot use overlapping IP addresses, and you must
    use unique subnets for every cluster. Recall that each Anthos GKE on GCP cluster
    requires the following three IP ranges:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: GCP建议在GCP集群上运行Anthos GKE，以VPC原生模式运行。在VPC原生模式下，任何集群使用的IP地址都是真实的VPC IP地址。这意味着在VPC原生集群中，您不能使用重叠的IP地址，并且必须为每个集群使用唯一的子网。回想一下，每个Anthos
    GKE on GCP集群都需要以下三个IP范围：
- en: '*Node IP*—Assigned to GCE instances or nodes belonging to clusters. One IP
    is required per node. These IP addresses are automatically assigned using the
    primary subnet.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点IP*—分配给GCE实例或属于集群的节点。每个节点需要一个IP地址。这些IP地址使用主子网自动分配。'
- en: '*Pod IP CIDR*—Assigned to every Pod that runs inside the cluster. A large subnet
    is assigned to the cluster. The cluster control plane divides this large subnet
    into smaller subnets, and each subnet (of equal size) is assigned to every node.
    For example, you can have a Pod IP CIDR of 10.0.0.0/16, and the cluster control
    plane assigns subnets of size /24 (from the Pod IP CIDR block) to each node, starting
    with 10.0.0.0/24 for the first node, 10.0.1.0/24 for the second node, and so on.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pod IP CIDR*—分配给集群内部运行的每个Pod。集群分配一个大子网。集群控制平面将这个大子网划分为更小的子网，每个子网（大小相等）分配给每个节点。例如，您可以有一个Pod
    IP CIDR为10.0.0.0/16，集群控制平面将大小为/24的子网（从Pod IP CIDR块中）分配给每个节点，第一个节点为10.0.0.0/24，第二个节点为10.0.1.0/24，依此类推。'
- en: '*Service IP CIDR*—Assigned to Services running inside the cluster. Every Service
    of type ClusterIP requires one IP address.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务IP CIDR*—分配给集群内部运行的服务。每个类型为ClusterIP的服务都需要一个IP地址。'
- en: Let’s address these one at a time in a bit more detail.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一更详细地讨论这些问题。
- en: Node IP
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 节点IP
- en: 'To determine the size of the node IP pool, you must know the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定节点IP池的大小，您必须知道以下信息：
- en: Number of clusters in a GCP region
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP区域中的集群数量
- en: Number of maximum nodes per cluster
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个集群的最大节点数
- en: 'If you have equal-sized clusters, you can simply multiply the two numbers to
    get the total number of maximum nodes required to run in that region:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有大小相等的集群，您可以直接将这两个数字相乘，以得到在该区域运行所需的最大节点总数：
- en: '[PRE23]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You can then determine the host bits you need for the node IP subnet from table
    10.3.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从表10.3中确定节点IP子网所需的主机位。
- en: Table 10.3 Determining the required host bits
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.3 确定所需的主机位
- en: '| Nodes required | Host bits for nodes |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 节点所需数量 | 节点主机位 |'
- en: '| 1-4 | 3 (or /29) |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 1-4 | 3 (或 /29) |'
- en: '| 5-12 | 4 (or /28) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 5-12 | 4 (或 /28) |'
- en: '| 13-28 | 5 (or /27) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 13-28 | 5 (或 /27) |'
- en: '| 29-60 | 6 (or /26) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 29-60 | 6 (或 /26) |'
- en: '| 61-124 | 7 (or /25) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 61-124 | 7 (或 /25) |'
- en: '| 125-252 | 8 (or /24) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 125-252 | 8 (或 /24) |'
- en: '| 253-508 | 9 (or /23) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 253-508 | 9 (或 /23) |'
- en: '| 509-1020 | 10 (or /22) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 509-1020 | 10 (或 /22) |'
- en: '| 1021-2044 | 11 (or /21) |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 1021-2044 | 11 (或 /21) |'
- en: '| 2045-4092 | 12 (or /20) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 2045-4092 | 12 (或 /20) |'
- en: '| 4093-8188 | 13 (or /19) |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 4093-8188 | 13 (或 /19) |'
- en: You can use a single subnet for multiple GKE clusters.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用单个子网为多个GKE集群提供服务。
- en: Pod IP CIDR
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: Pod IP CIDR
- en: To determine the Pod IP CIDR, determine the maximum number of Pods per node
    you need in your cluster over its lifetime. If you cannot determine the maximum
    number you need, use the quota limit of 110 Pods per node as the maximum. Use
    table 10.4 to determine the host bits needed for the required number of Pods.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定Pod IP CIDR，确定您在集群生命周期内需要的每个节点上的Pod的最大数量。如果您无法确定所需的最高数量，请使用每个节点110个Pod的配额限制作为最大值。使用表10.4确定所需Pod数量的主机位。
- en: Table 10.4 Determining the host bits required for the Pods
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.4 确定Pod所需的主机位
- en: '| Pods-per-node count | Host bits for Pods |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 每节点Pod数量 | Pod主机位 |'
- en: '| 1-8 | 4 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 1-8 | 4 |'
- en: '| 9-16 | 5 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 9-16 | 5 |'
- en: '| 17-32 | 6 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 17-32 | 6 |'
- en: '| 33-64 | 7 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 33-64 | 7 |'
- en: '| 65-110 | 8 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 65-110 | 8 |'
- en: 'To calculate the Pod IP CIDR block, you need the host bits for nodes and pods,
    and use the following formula:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 Pod IP CIDR 块，你需要节点和 Pod 的主机位，并使用以下公式：
- en: '[PRE24]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For example, let’s assume that you need 110 pods per node, and the total number
    of nodes across all GKE clusters in the region is 5,000\. First, determine the
    host bits for nodes using table 10.3; this would be 13\. Then, determine the host
    bits for Pods using table 10.4; this would be 8\. Then, using the formula, your
    Pod IP CIDR block netmask needs to be the following:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你每个节点需要 110 个 Pod，该区域所有 GKE 集群中的节点总数为 5,000。首先，使用表 10.3 确定节点的位数；这将是一个 13。然后，使用表
    10.4 确定 Pod 的位数；这将是一个 8。然后，使用公式，你的 Pod IP CIDR 块的子网掩码需要如下所示：
- en: '[PRE25]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You would need a subnet with a mask of /11.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个掩码为 /11 的子网。
- en: Service IP CIDR
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 服务 IP CIDR
- en: To calculate the Service IP CIDR, determine the maximum number of cluster IP
    addresses you need in your cluster over its lifetime. Every Service requires one
    cluster IP. You cannot share Service IP subnets between clusters. This means you
    need a different Service IP subnet per cluster.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算服务 IP CIDR，确定你在集群生命周期内需要的最大集群 IP 地址数量。每个服务需要一个集群 IP。你无法在集群之间共享服务 IP 子网。这意味着你需要为每个集群使用不同的服务
    IP 子网。
- en: Once you know the maximum number of Services in a cluster, you can use table
    10.5 to get the subnet mask you need.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道集群中服务的最大数量，你就可以使用表 10.5 来获取所需的子网掩码。
- en: Table 10.5 Determining the required subnet mask
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.5 确定所需的子网掩码
- en: '| Number of cluster IP addresses | Netmask |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 集群 IP 地址数量 | 子网掩码 |'
- en: '| 1-32 | /27 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 1-32 | /27 |'
- en: '| 33-64 | /26 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 33-64 | /26 |'
- en: '| 65-128 | /25 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 65-128 | /25 |'
- en: '| 129-256 | /24 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 129-256 | /24 |'
- en: '| 257-512 | /23 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 257-512 | /23 |'
- en: '| 513-1,024 | /22 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 513-1,024 | /22 |'
- en: '| 1,025-2,048 | /21 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 1,025-2,048 | /21 |'
- en: '| 2,049-4,096 | /20 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 2,049-4,096 | /20 |'
- en: '| 4,097-8,192 | /19 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 4,097-8,192 | /19 |'
- en: '| 8,193-16,384 | /18 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 8,193-16,384 | /18 |'
- en: '| 16,385-32,768 | /17 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 16,385-32,768 | /17 |'
- en: '| 32,769-65,536 | /16 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 32,769-65,536 | /16 |'
- en: Configuring privately used public IPs for Anthos GKE
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Anthos GKE 的私有使用公共 IP
- en: From the previous section, you can see that in very large GKE environments,
    you may run into IP exhaustion. The biggest source of IP exhaustion in large GKE
    environments is the Pod IP CIDR block. GCP VPCs use the RFC1918 address space
    for networking resources. In large environments, the RFC1918 space might not be
    sufficient to configure Anthos. This is especially a concern for managed service
    providers that deliver their managed services to many tenants on Anthos.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中，你可以看到在非常大的 GKE 环境中，你可能会遇到 IP 耗尽的问题。在大型 GKE 环境中，IP 耗尽的最大来源是 Pod IP CIDR
    块。GCP VPC 使用 RFC1918 地址空间作为网络资源。在大型环境中，RFC1918 空间可能不足以配置 Anthos。这对于向 Anthos 的许多租户提供托管服务的托管服务提供商来说尤其是一个问题。
- en: One way to mitigate address exhaustion is to use privately used public IP (PUPI)
    addresses for the GKE Pod CIDR block. PUPIs are any public IP addresses not owned
    by Google that a customer can use privately on Google Cloud. The customer doesn’t
    necessarily own these addresses.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解地址耗尽的一种方法是为 GKE Pod CIDR 块使用私有使用公共 IP（PUPI）地址。PUPI 是任何客户可以在 Google Cloud 上私用的、不属于
    Google 的公共 IP 地址。客户不一定拥有这些地址。
- en: Figure 10.12 shows a company (producer) that offers a managed service to a customer
    (consumer).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 展示了一家（生产者）向客户（消费者）提供托管服务公司的示例。
- en: '![10-12](../../OEBPS/Images/10-12.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![10-12](../../OEBPS/Images/10-12.png)'
- en: Figure 10.12 Anthos GKE using privately used public IP (PUPI) addressing
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 使用私有使用公共 IP（PUPI）寻址的 Anthos GKE
- en: 'This setup involves the following considerations:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置涉及以下考虑因素：
- en: '*Primary CIDR block*—A non-PUPI CIDR block used for nodes and internal load
    balancing (ILB) and must be nonoverlapping across VPCs'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主 CIDR 块*—用于节点和内部负载均衡（ILB）的非 PUPI CIDR 块，必须在 VPC 之间不重叠'
- en: '*Producer secondary CIDR block*—A PUPI CIDR block used for Pods (e.g., 45.45.0.0/16)'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产者次要 CIDR 块*—用于 Pod 的 PUPI CIDR 块（例如，45.45.0.0/16）'
- en: '*Consumer secondary CIDR block*—Any other PUPI CIDR block on the customer side
    (e.g., 5.5/16)'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消费者次要 CIDR 块*—客户侧的任何其他 PUPI CIDR 块（例如，5.5/16）'
- en: The company’s managed service is in the producer VPC (vpc-producer) and is built
    on an Anthos GKE Deployment. The company’s GKE cluster uses the PUPI 45.0.0.0/8
    CIDR block for Pod addresses. The customer’s applications are in the consumer
    VPC (vpc-consumer). The customer also has an Anthos GKE installation. The GKE
    cluster in the consumer VPC uses the PUPI 5.0.0.0/8 CIDR block for Pod addresses.
    The two VPCs are peered with one another. Both VPCs use the RFC1918 address space
    for node, service, and load balancing addresses.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司的托管服务位于生产者VPC（vpc-producer）中，并基于Anthos GKE部署构建。公司的GKE集群使用PUPI 45.0.0.0/8
    CIDR块为Pod地址。客户的程序位于消费者VPC（vpc-consumer）中。客户还拥有一个Anthos GKE安装。消费者VPC中的GKE集群使用PUPI
    5.0.0.0/8 CIDR块为Pod地址。两个VPC相互对等连接。两个VPC都使用RFC1918地址空间作为节点、服务和负载均衡地址。
- en: By default, the consumer VPC (vpc-consumer) exports all RFC1918 address spaces
    to the producer VPC (vpc-producer). Unlike RFC1918 private addresses and extended
    private addresses (CGN, Class E), PUPIs aren’t automatically advertised to VPC
    peers by default. If the vpc-consumer Pods must communicate with vpc-producer,
    the consumer must enable the VPC peering connection to export PUPI addresses.
    Likewise, the producer must configure the producer VPC to import PUPI routes over
    the VPC peering connection.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消费者VPC（vpc-consumer）将所有RFC1918地址空间导出到生产者VPC（vpc-producer）。与RFC1918私有地址和扩展私有地址（CGN，E类）不同，PUPIs默认情况下不会自动向VPC对等方广播。如果vpc-consumer
    Pods必须与vpc-producer中的资源通信，消费者必须启用VPC对等连接以导出PUPI地址。同样，生产者必须配置生产者VPC以通过VPC对等连接导入PUPI路由。
- en: The vpc-consumer address space that is exported into vpc-producer must not overlap
    with any RFC1918 or PUPI address used in vpc-producer. The producer must inform
    the consumer which PUPI CIDR blocks the managed service uses and ensure that the
    consumer isn’t using these blocks. The producer and consumer must also agree and
    assign nonoverlapping address space for ILB and node addresses in vpc-producer.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 导入到vpc-producer的vpc-consumer地址空间不得与vpc-producer中使用的任何RFC1918或PUPI地址重叠。生产者必须通知消费者托管服务使用的PUPI
    CIDR块，并确保消费者没有使用这些块。生产者和消费者还必须同意并为vpc-producer中的ILB和节点地址分配非重叠地址空间。
- en: PUPIs don’t support service networking. In most cases, resources in vpc-consumer
    communicate with services in vpc-producer through ILB addresses in the producer
    cluster. If the producer Pods are required to initiate communication directly
    with resources in vpc-consumer, and PUPI addressing doesn’t overlap, then the
    producer must configure the producer VPC to export the PUPI routes over the VPC
    peering connection. Likewise, the consumer must configure the VPC peering connection
    to import routes into vpc-consumer. If the consumer VPC already uses the PUPI
    address, then the producer should instead configure the IP masquerade feature
    and hide the Pod IP addresses behind the producer node IP addresses.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: PUPIs不支持服务网络。在大多数情况下，vpc-consumer中的资源通过生产者集群中的ILB地址与vpc-producer中的服务通信。如果生产者Pods需要直接与vpc-consumer中的资源通信，并且PUPI寻址不重叠，那么生产者必须配置生产者VPC以通过VPC对等连接导出PUPI路由。同样，消费者必须配置VPC对等连接以将路由导入vpc-consumer。如果消费者VPC已经使用PUPI地址，那么生产者应改为配置IP伪装功能，并将Pod
    IP地址隐藏在生产者节点IP地址之后。
- en: The previous example shows a more complex producer/consumer model. You can simply
    use this in a single project model. This would free up RFC1918 space that may
    otherwise be used for Pod IP CIDR.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例显示了一个更复杂的生产者/消费者模型。您可以在单个项目模型中简单地使用它。这将释放可能用于Pod IP CIDR的RFC1918空间。
- en: 10.3 Anthos multicluster networking
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 Anthos多集群网络
- en: 'This section addresses mechanisms for connecting Services running across multiple
    clusters. Every hybrid and multicloud Anthos architecture, by definition, has
    more than one cluster. For example, you have Anthos GKE clusters running in GCP
    and Anthos GKE on-prem clusters running in on-prem data centers. The Services
    running on Anthos clusters often require network connectivity to Services running
    in other Anthos clusters. For multicluster Service networking, let’s look at the
    following scenarios:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了连接跨多个集群运行的服务机制。每个混合和多云Anthos架构，按定义，都有多个集群。例如，您有在GCP上运行的Anthos GKE集群和在本地数据中心运行的Anthos
    GKE本地集群。在Anthos集群上运行的服务通常需要与其他Anthos集群中运行的服务进行网络连接。对于多集群服务网络，让我们看看以下场景：
- en: '*Multicluster networking on GCP*—In this architecture, all Services run on
    multiple Anthos GKE clusters in GCP.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GCP上的多集群网络*—在这个架构中，所有服务都在GCP上的多个Anthos GKE集群上运行。'
- en: '*Multicluster networking in hybrid and multicloud environments*—In this architecture,
    Services run on multiple Anthos GKE clusters in hybrid and multicloud environments.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合和多云环境中的多集群网络*——在这个架构中，服务在混合和多云环境中的多个Anthos GKE集群上运行。'
- en: 10.3.1 Multicluster networking on GCP
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 GCP上的多集群网络
- en: Cloud native enterprises can run the Anthos platform on GCP. This can be on
    a single cluster in a single region. Often, an Anthos platform consists of multiple
    clusters in multiple regions for resiliency.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生企业可以在GCP上运行Anthos平台。这可以是在单个区域的单个集群中。通常，Anthos平台由多个区域中的多个集群组成，以提供弹性。
- en: In GCP, Google recommends using a shared VPC model with multiple Service projects.
    One of these Service projects belongs to the platform_admins group and contains
    all the Anthos GKE clusters that form the Anthos platform. Resources on these
    clusters are shared by multiple tenants. We also recommend using VPC-native clusters.
    VPC-native clusters use VPC IP addresses for Pod IPs, which allow direct Pod-to-Pod
    connectivity across multiple clusters. A typical Anthos platform architecture
    on GCP looks like the one shown in figure 10.13.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中，谷歌建议使用具有多个服务项目的共享VPC模型。其中这些服务项目之一属于platform_admins组，并包含构成Anthos平台的全部Anthos
    GKE集群。这些集群上的资源由多个租户共享。我们还建议使用VPC原生集群。VPC原生集群使用VPC IP地址作为Pod IP，这允许跨多个集群实现Pod到Pod的直接连接。典型的GCP上Anthos平台架构如图10.13所示。
- en: '![10-13](../../OEBPS/Images/10-13.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![10-13](../../OEBPS/Images/10-13.png)'
- en: 'Figure 10.13 Anthos architecture: single environment'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 Anthos架构：单一环境
- en: This architecture represents a single environment, for example, production in
    this case. A single network host project called project-0-nethost-prod manages
    the shared VPC. Two service projects exist, one for platform admins called project-1-platform_admins-prod,
    where the Anthos platform is deployed and managed by the platform administrator,
    and one for a product called project-2-product1-prod, where resources pertaining
    to product1 reside. In this example, the Anthos platform is deployed across two
    GCP regions to provide regional redundancy. You can create the same architecture
    with more than two regions or even a single region. Inside each region is a single
    subnet with secondary ranges. Two zonal Anthos GKE clusters exist per region.
    Multiple clusters per region provide cluster- and zone-level resiliency. You can
    use the same design for more than two clusters per region. All clusters are VPC-native
    clusters allowing Pod-to-Pod connectivity between clusters. ASM is installed on
    all clusters, forming a multicluster service mesh. ASM control planes discover
    Services and endpoints running in all clusters and configure the Envoy sidecar
    proxy running inside each Pod with routing information pertaining to all Services
    running inside the mesh.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构代表单一环境，例如，在这种情况下为生产环境。一个名为project-0-nethost-prod的单个网络主机项目管理共享VPC。存在两个服务项目，一个是为平台管理员名为project-1-platform_admins-prod的项目，其中Anthos平台由平台管理员部署和管理，另一个是为产品名为project-2-product1-prod的项目，其中包含与product1相关的资源。在此示例中，Anthos平台部署在两个GCP区域中，以提供区域冗余。您可以使用两个以上区域或甚至单个区域创建相同的架构。每个区域内部有一个单独的子网和次要范围。每个区域存在两个区域级别的Anthos
    GKE集群。每个区域多个集群提供集群和区域级别的弹性。您可以为每个区域超过两个集群使用相同的设计。所有集群都是VPC原生集群，允许集群之间的Pod到Pod连接。ASM安装在每个集群上，形成一个多集群服务网格。ASM控制平面发现所有集群中运行的服务和端点，并配置每个Pod内运行的Envoy边车代理的与网格内所有服务相关的路由信息。
- en: Every tenant or product gets a landing zone in the form of a Kubernetes namespace
    (in all clusters inside the mesh) and a set of policies. Tenants can deploy their
    Services inside their own namespaces only. You can run the same Service in multiple
    Anthos GKE clusters for resiliency. These Services are called *distributed services*.
    Distributed services act as a single logical Service from the perspective of all
    other entities. In figure 10.13, product1-service is a distributed service with
    four endpoints, where each endpoint runs in a different cluster. As shown in figure
    10.14, ASM takes care of Service discovery, and VPC-native clusters allow for
    L3/L4 Pod-to-Pod connectivity.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 每个租户或产品都会以Kubernetes命名空间（在网格内所有集群中）和一组策略的形式获得一个着陆区。租户只能在它们自己的命名空间内部署他们的服务。您可以在多个Anthos
    GKE集群中运行相同的服务以提供弹性。这些服务被称为*分布式服务*。分布式服务从所有其他实体的角度来看充当单个逻辑服务。如图10.13所示，product1-service是一个具有四个端点的分布式服务，每个端点在不同的集群中运行。如图10.14所示，ASM负责服务发现，VPC原生集群允许L3/L4
    Pod到Pod的连接。
- en: '![10-14](../../OEBPS/Images/10-14.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![10-14](../../OEBPS/Images/10-14.png)'
- en: Figure 10.14 Anthos GKE multicluster networking
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 Anthos GKE多集群网络
- en: 10.3.2 Multicluster networking in hybrid and multicloud environments
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 混合和多云环境中的多集群网络
- en: Apart from Anthos GKE on GCP, all other Anthos GKE clusters run in island mode.
    This means that the IP addressing used for Pods and Services inside the cluster
    is not routable outside of the cluster. In this scenario, you can still connect
    Services running on multiple clusters either in the same environment—for example,
    multiple Anthos GKE clusters running in an on-prem data center—or across multiple
    infrastructure environments—for example, Services running in Anthos GKE in GCP
    and in on-prem data center environments.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在GCP上的Anthos GKE之外，所有其他Anthos GKE集群都以孤岛模式运行。这意味着集群内部Pod和服务使用的IP地址在集群外部是不可路由的。在这种情况下，您仍然可以在同一环境中（例如，在本地数据中心运行的多个Anthos
    GKE集群）或跨多个基础设施环境（例如，在GCP的Anthos GKE和在本地数据中心环境中运行的服务）连接运行在多个集群上的服务。
- en: 'You should consider the following three aspects when connecting multiple Anthos
    GKE clusters in hybrid or multicloud environments:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合或多云环境中连接多个Anthos GKE集群时，应考虑以下三个方面：
- en: Network connectivity between Pods running on multiple clusters
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个集群上运行的Pod之间的网络连接
- en: Service discovery across multiple clusters
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多集群间的服务发现
- en: In the case of hybrid and multicloud architectures, connectivity between infrastructure
    environments
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混合和多云架构的情况下，基础设施环境之间的连接
- en: Network connectivity
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 网络连接
- en: Every Anthos GKE cluster has a load balancer. The load balancer either comes
    bundled during installation or can be configured manually. These load balancers
    allow Services to be exposed via NodePort to resources running outside the cluster.
    Each Service gets a VIP address (Service VIP), which is routable and reachable
    inside the network. The load balancer routes the traffic to the node IP on the
    Service NodePort, which gets forwarded to the Pod IP on the desired port.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Anthos GKE集群都有一个负载均衡器。负载均衡器在安装期间捆绑提供，或者可以手动配置。这些负载均衡器允许服务通过NodePort暴露给集群外部的资源。每个服务都获得一个VIP地址（服务VIP），该地址在网络上可路由且可访问。负载均衡器将流量路由到服务NodePort上的节点IP，该IP被转发到所需端口的Pod
    IP。
- en: Services (and Pods) running in one cluster can access the Service VIP for a
    Service running in another cluster, which gets routed to the desired Pod via the
    load balancer, as shown in figure 10.15.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个集群中运行的服务（和Pod）可以访问另一个集群中运行的服务VIP，该服务通过负载均衡器路由到所需的Pod，如图10.15所示。
- en: '![10-15](../../OEBPS/Images/10-15.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![10-15](../../OEBPS/Images/10-15.png)'
- en: Figure 10.15 Anthos GKE hybrid multicluster networking
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 Anthos GKE混合多集群网络
- en: Anthos clusters can also be configured with Ingress controllers. Ingress controllers
    are L7/HTTP load balancers that typically reside inside the cluster. The Ingress
    controllers themselves are exposed via an L3/L4 load balancer. This way you can
    use one VIP (the Ingress VIP) for multiple Services running on the same cluster,
    as shown in figure 10.16\. Ingress controllers act upon Ingress rules, which dictate
    how the traffic is to be routed inside the cluster.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos集群还可以配置Ingress控制器。Ingress控制器是位于集群内部的L7/HTTP负载均衡器。Ingress控制器本身通过L3/L4负载均衡器暴露。这样，您可以使用一个VIP（入口VIP）为同一集群上运行的多个服务提供服务，如图10.16所示。Ingress控制器根据入口规则操作，这些规则规定了如何在集群内部路由流量。
- en: '![10-16](../../OEBPS/Images/10-16.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![10-16](../../OEBPS/Images/10-16.png)'
- en: 'Figure 10.16 Anthos GKE hybrid multicluster networking: Ingress'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16 Anthos GKE混合多集群网络：入口
- en: Multicluster Service discovery
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群服务发现
- en: Anthos Service Mesh is used for multicluster Service discovery, as illustrated
    in figure 10.17\. An ASM control plane is installed in each cluster. The ASM control
    plane discovers Services and endpoints from all clusters. This is also known as
    the Service Mesh. The ASM control plane must have network access to the Kubernetes
    API server of all Anthos clusters inside the Service Mesh. ASM creates its own
    service registry, which is a list of Services and their associated endpoints (or
    Pods).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos Service Mesh用于多集群服务发现，如图10.17所示。每个集群都安装了一个ASM控制平面。ASM控制平面从所有集群发现服务和端点。这也被称为服务网格。ASM控制平面必须能够访问服务网格内所有Anthos集群的Kubernetes
    API服务器。ASM创建了自己的服务注册表，这是一个包含服务和它们相关联的端点（或Pod）的列表。
- en: '![10-17](../../OEBPS/Images/10-17.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![10-17](../../OEBPS/Images/10-17.png)'
- en: 'Figure 10.17 Anthos GKE hybrid multicluster networking: Anthos Service Mesh'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 Anthos GKE 混合多集群网络：Anthos 服务网格
- en: In Anthos GKE on GCP, the endpoints are the actual Pod IP addresses if using
    VPC-native clusters. Traffic flows from Pod to Pod using VPC routing. In non-GCP
    Anthos clusters, traffic between clusters flows through an L7 Envoy proxy. This
    proxy runs as a Service in every Anthos cluster called the istio-ingressgateway.
    Traffic bound for all Services inside a cluster flows through the istio-ingressgateway,
    which is configured to inspect the host header and route the traffic to the appropriate
    Service inside the cluster.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCP 上的 Anthos GKE 中，如果使用 VPC 原生集群，端点是实际的 Pod IP 地址。流量通过 VPC 路由从 Pod 流向 Pod。在非
    GCP Anthos 集群中，集群间的流量通过一个 L7 Envoy 代理流过。此代理在每个 Anthos 集群中以服务形式运行，称为 istio-ingressgateway。流向集群内所有服务的流量都通过
    istio-ingressgateway，该服务配置为检查主机头并将流量路由到集群内的适当服务。
- en: For distributed Services, we recommend using ASM, which provides Service discovery
    and traffic routing functionality across multiple clusters.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式服务，我们建议使用 ASM，它提供跨多个集群的服务发现和流量路由功能。
- en: Hybrid and multicloud connectivity
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云和多云连接
- en: 'You can connect Services running in multiple Anthos clusters across multiple
    infrastructure environments as long as you can reach the Kubernetes API server
    and the external public load balancer of the target Anthos cluster. You can connect
    infrastructure environments in the following three ways:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你能访问目标 Anthos 集群的 Kubernetes API 服务器和外部公共负载均衡器，你就可以连接运行在多个 Anthos 集群中的多个基础设施环境中的服务。你可以以下三种方式连接基础设施环境：
- en: '*Cloud Interconnect*—Cloud Interconnect extends the on-prem network to Google’s
    network through a highly available, low-latency connection. You can use Dedicated
    Interconnect to connect directly to Google or use Partner Interconnect to connect
    to Google through a supported service provider. Dedicated Interconnect provides
    direct physical connections between your on-prem network and Google’s network.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云互连*—通过高度可用、低延迟的连接将本地网络扩展到 Google 的网络。你可以使用专用互连直接连接到 Google，或者使用合作伙伴互连通过支持的服务提供商连接到
    Google。专用互连在你的本地网络和 Google 的网络之间提供直接的物理连接。'
- en: '*Cloud VPN*—Cloud VPN securely connects your peer network to your VPC network
    through an IPsec VPN connection. Traffic traveling between the two networks is
    encrypted by one VPN gateway and then decrypted by the other VPN gateway.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云 VPN*—通过 IPsec VPN 连接安全地将你的对等网络连接到你的 VPC 网络。两个网络之间的流量由一个 VPN 网关加密，然后由另一个
    VPN 网关解密。'
- en: '*Public internet*—An Anthos platform on multiple environments can be connected
    over the public internet without using Cloud Interconnect or VPN. Services running
    on the platform connect over the public internet using TLS/mTLS. This type of
    connectivity is done at a per-service level using ASM and not at the network level.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共互联网*—多个环境上的 Anthos 平台可以通过公共互联网连接，而无需使用云互连或 VPN。平台上的服务通过 TLS/mTLS 在公共互联网上连接。此类连接是在每个服务级别使用
    ASM 完成的，而不是在网络级别。'
- en: These connectivity models are explained in detail in section 10.1.2, “Multi/hybrid
    cloud deployment.”
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接模型在 10.1.2 节“多/混合云部署”中进行了详细解释。
- en: 10.4 Services and client connectivity
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 服务和客户端连接
- en: 'This section addresses services and client connectivity in the Anthos platform,
    which can be divided into the following three categories:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论 Anthos 平台中的服务和客户端连接，可以分为以下三个类别：
- en: '*Client-to-Service connectivity*—This is also sometimes referred to as north-south
    traffic, suggesting traffic originates from outside of the platform (north) and
    travels into the platform (south).'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客户端到服务连接*—这也被称为南北流量，表示流量起源于平台外部（北）并进入平台（南）。'
- en: '*Service-to-Service connectivity*—This is also sometimes referred to as east-west
    traffic, suggesting traffic traverses the platform laterally (hence east-west).
    All traffic originates and terminates inside the Anthos platform.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务到服务连接*—这也被称为东西流量，表示流量在平台横向穿越（因此是东西）。所有流量都在 Anthos 平台内部发起和终止。'
- en: '*Service-to-external service connectivity*—This is traffic egressing the platform.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务到外部服务连接*—这是离开平台的流量。'
- en: 10.4.1 Client-to-Service connectivity
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 客户端到服务连接
- en: 'In this context, a client refers to an entity that resides outside of the Anthos
    platform and a Service that runs inside the Anthos platform. You can access Services
    inside the Anthos platform in the following two ways:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，客户端指的是位于Anthos平台外部的一个实体以及运行在Anthos平台内部的一个服务。您可以通过以下两种方式访问Anthos平台内部的服务：
- en: '*With ASM*—With ASM, you can use ASM Ingress for HTTP(S) and TCP traffic. ASM
    provides additional L7 functionality, for example, the ability to perform authentication
    and authorization at Ingress. ASM is the recommended way of accessing web-based
    Services in the Anthos platform. ASM can also be used for TCP-based Services.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用ASM*—使用ASM，您可以使用ASM Ingress来处理HTTP(S)和TCP流量。ASM提供了额外的L7功能，例如，在Ingress处执行身份验证和授权的能力。ASM是Anthos平台上访问基于Web服务的推荐方式。ASM也可以用于基于TCP的服务。'
- en: '*Without ASM*—All Anthos clusters support the option to configure a load balancer.
    The load balancer either comes integrated/bundled when you deploy the Anthos cluster
    or it can be configured manually. Any TCP-based service can be exposed using a
    service of type Loadbalancer*,* which creates a Service VIP that can be accessed
    by the client. In addition, all Anthos clusters can be configured with Ingress.
    Ingress controllers typically run inside the cluster as L7 proxies (with the exception
    of Anthos GKE on GCP and Anthos on AWS using ALB). The Ingress controllers themselves
    are exposed using the L3/L4 load balancer. Ingress is the recommended way to expose
    web-based Services. Ingress rules are implemented as part of the Service deploy
    pipeline, and Ingress controllers enforce these rules, which include listening
    and routing traffic to the appropriate service.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不使用ASM*—所有Anthos集群都支持配置负载均衡器的选项。负载均衡器在您部署Anthos集群时可能集成/捆绑，或者可以手动配置。任何基于TCP的服务都可以使用类型为Loadbalancer*的服务来暴露，这会创建一个客户端可以访问的服务VIP。此外，所有Anthos集群都可以配置Ingress。Ingress控制器通常在集群内部作为L7代理运行（除了在GCP上的Anthos
    GKE和AWS上的Anthos使用ALB）。Ingress控制器本身是通过L3/L4负载均衡器暴露的。Ingress是暴露基于Web服务的推荐方式。Ingress规则作为服务部署管道的一部分实现，Ingress控制器强制执行这些规则，包括监听和路由流量到适当的服务。'
- en: 10.4.2 Service-to-Service connectivity
  id: totrans-481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 Service到Service连接性
- en: 'Services that run inside the cluster require network connectivity. This is
    accomplished in two ways:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 集群内部运行的服务需要网络连接。这可以通过两种方式实现：
- en: '*With ASM*—We recommend using ASM, especially in a multicluster Anthos platform.
    ASM provides Service discovery as well as routing logic between Services. ASM
    can also handle authentication and authorization between Services. For example,
    you can enable mTLS at the service mesh level, encrypting all Service-to-Service
    traffic. You can also configure security policies at an individual Service layer.
    Besides Service discovery and networking, ASM provides additional features such
    as telemetry, quotas, rate limiting, and circuit breaker. For more on the features
    and benefits of ASM, please see chapter 4.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用ASM*—我们推荐使用ASM，尤其是在多集群的Anthos平台上。ASM提供服务发现以及Service之间的路由逻辑。ASM还可以处理Service之间的身份验证和授权。例如，您可以在服务网格级别启用mTLS，加密所有Service到Service的流量。您还可以在单个Service层配置安全策略。除了服务发现和网络功能外，ASM还提供其他功能，如遥测、配额、速率限制和断路器。有关ASM的功能和好处，请参阅第4章。'
- en: '*Without ASM*—If you choose to not use ASM, you can still configure Service-to-Service
    connectivity. From a networking standpoint, you can use either the load balancer
    or the Ingress pattern to access Services running inside clusters. You would have
    to configure Service discovery yourself. You can use DNS to provide this functionality.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不使用ASM*—如果您选择不使用ASM，您仍然可以配置Service到Service连接性。从网络角度来看，您可以使用负载均衡器或Ingress模式来访问集群内部运行的服务。您将不得不自行配置服务发现。您可以使用DNS来提供此功能。'
- en: In either case, you can also use NetworkPolicy inside the cluster to control/limit
    the traffic between Pods and Services.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，您也可以在集群内部使用NetworkPolicy来控制/限制Pod和Service之间的流量。
- en: 10.4.3 Service-to-external Services connectivity
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 Service到外部服务连接性
- en: 'You can control egress traffic from any Anthos cluster in the following two
    ways:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下两种方式控制任何Anthos集群的出口流量：
- en: '*With ASM*—ASM provides both an Ingress and an egress gateway. We have previously
    discussed how Ingress works with ASM. Similarly, you can configure a second proxy
    at the perimeter of the Service Mesh called the istio-egressgateway. You can then
    configure ServiceEntries for only the Services that are allowed to be accessed
    from inside the cluster. You can set the outboundTrafficPolicy mode to REGISTRY_ONLY.
    This blocks all outbound traffic that is not destined for a Service inside the
    mesh. You can then create individual ServiceEntries for access to Services running
    outside the platform. An example of ServiceEntry may look like the following:'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用ASM*—ASM提供入站和出站网关。我们之前讨论了Ingress如何与ASM协同工作。同样，您可以在服务网格外围配置第二个代理，称为istio-egressgateway。然后，您可以为仅允许从集群内部访问的服务配置ServiceEntries。您可以将outboundTrafficPolicy模式设置为REGISTRY_ONLY。这将阻止所有不是目标为网格内服务的出站流量。然后，您可以创建单独的ServiceEntries以访问平台外运行的服务。一个ServiceEntry的示例可能如下所示：'
- en: '[PRE26]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ The location MESH_EXTERNAL signifies that the Service is external to the Service
    Mesh, and a DNS entry is manually added to the mesh registry.
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 位置MESH_EXTERNAL表示服务位于服务网格之外，并且手动将DNS条目添加到网格注册表中。
- en: This rule allows traffic destined for *httpbin.org* on port 80\. Note the location
    of the Service is MESH_EXTERNAL, signifying this service is outside of the Service
    Mesh and the Anthos platform.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此规则允许目标为端口80的*httpbin.org*的流量。注意服务位置为MESH_EXTERNAL，表示此服务位于服务网格和Anthos平台之外。
- en: '*Without ASM*—You can use NetworkPolicies inside the cluster to control Ingress
    and egress traffic to Pods based on label selectors. Because all Pod egress traffic
    exits via the node IP, you can further control egress traffic through firewall
    rules by limiting what destinations the node IP subnets can access.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不使用ASM*—您可以在集群内部使用NetworkPolicies根据标签选择器控制Pod的入站和出站流量。由于所有Pod出站流量都通过节点IP退出，您可以通过限制节点IP子网可以访问的目的地来进一步通过防火墙规则控制出站流量。'
- en: Summary
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Anthos networking can be divided into the following four layers:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos网络可以分为以下四个层次：
- en: '*Cloud networking and hybrid connectivity*—The lowest layer of Anthos networking.
    This layer describes how to set up networking within each cloud environment as
    well as options to securely connect multiple cloud environments together. Inside
    GCP, you can set up a single network (or VPC), a shared VPC, or multiple VPCs,
    depending on the organizational and functional requirements. In non-GCP environments,
    all Anthos clusters are treated as isolated networks (or in “island mode”). Multiple
    hybrid connectivity options follow:'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云网络和混合连接*—Anthos网络的最低层。本层描述了如何在每个云环境中设置网络以及将多个云环境安全连接起来的选项。在GCP内部，您可以根据组织结构和功能需求设置单个网络（或VPC）、共享VPC或多个VPC。在非GCP环境中，所有Anthos集群都被视为孤立网络（或“岛屿模式”）。以下是一些混合连接选项：'
- en: '*Dedicated Interconnect*—Provides direct physical connections between your
    on-prem network and Google’s network. Dedicated Interconnect enables you to transfer
    large amounts of data between networks, which can be more cost effective than
    purchasing additional bandwidth over the public internet.'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*专用互联*—在您的本地网络和谷歌网络之间提供直接的物理连接。专用互联使您能够在网络之间传输大量数据，这可能比在公共互联网上购买额外的带宽更经济高效。'
- en: '*Cloud VPN*—Securely extends your peer network to Google’s network through
    an IPsec VPN tunnel. Traffic is encrypted and travels between the two networks
    over the public internet. Cloud VPN is useful for low-volume data connections.'
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云VPN*—通过IPsec VPN隧道安全地将您的对等网络扩展到谷歌网络。流量被加密，并通过公共互联网在两个网络之间传输。云VPN适用于低流量数据连接。'
- en: '*Public internet*—Does not require any special software or hardware to connect
    disparate networks together. Instead, TLS/mTLS connection is used to secure Service-to-Service
    connections.'
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共互联网*—连接不同的网络不需要任何特殊的软件或硬件。相反，使用TLS/mTLS连接来保护服务到服务的连接。'
- en: '*Anthos GKE networking*—The Kubernetes networking layer. Anthos GKE clusters
    can be deployed to a variety of environments, for example, GCP, on VMware in an
    on-prem data center, on bare metal servers, and on AWS. In addition to the supported
    Anthos clusters, you can also register *any conformant Kubernetes* cluster to
    the Anthos platform. For example, you can *register EKS* clusters running in AWS
    *and AKS* clusters running in Azure to the Anthos platform. The following six
    types of Anthos clusters are currently available:'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos GKE网络*—Kubernetes网络层。Anthos GKE集群可以部署到各种环境中，例如GCP、在本地数据中心VMware上、在裸金属服务器上，以及AWS上。除了支持的Anthos集群外，您还可以将任何符合Kubernetes规范的集群注册到Anthos平台。例如，您可以将运行在AWS上的EKS集群和运行在Azure上的AKS集群注册到Anthos平台。目前有六种类型的Anthos集群可供使用：'
- en: Anthos clusters on GCP(GKE)
  id: totrans-500
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP上的Anthos集群（GKE）
- en: Anthos clusters on VMware (GKE on-prem)
  id: totrans-501
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware上的Anthos集群（本地GKE）
- en: Anthos clusters on bare metal
  id: totrans-502
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于裸金属的Anthos集群
- en: Anthos clusters on AWS (GKE on AWS)
  id: totrans-503
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的Anthos集群（AWS上的GKE）
- en: Anthos clusters on Azure (GKE on Azure)
  id: totrans-504
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure上的Anthos集群（Azure上的GKE）
- en: Anthos attached clusters (conformant Kubernetes clusters)
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos附加集群（符合Kubernetes规范的集群）
- en: '*Anthos multicluster networking*—Deals with environments with multiple clusters
    where services need to communicate across cluster boundaries. This section can
    be divided into the following two subsections:'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos多集群网络*—处理需要跨集群边界通信的多个集群的环境。本节可以分为以下两个子节：'
- en: '*Anthos multicluster networking with GKE on GCP*—In GKE on GCP, you can either
    have a flat network architecture (using a single or shared VPC) or a multiple-network
    (multiple-VPC) model. In a flat network architecture using VPC-native GKE clusters,
    VPC networking automatically allows for Pod-to-Pod connectivity between multiple
    clusters. Clusters can be in any region. No additional configuration is required
    for Pod-to-Pod connectivity between clusters. In a multiple VPC architecture,
    you need additional configuration to connect Pods and Services between multiple
    clusters. For example, you can use special gateways or Ingress model to communicate
    between clusters.'
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在GCP上的GKE与Anthos多集群网络*—在GKE on GCP中，您可以选择平面网络架构（使用单个或共享VPC）或多个网络（多个VPC）模型。在平面网络架构中使用VPC原生GKE集群时，VPC网络自动允许多个集群之间的Pod到Pod连接。集群可以位于任何区域。集群之间Pod到Pod连接不需要额外配置。在多个VPC架构中，您需要额外的配置来连接多个集群之间的Pod和服务。例如，您可以使用特殊的网关或入口模型在集群之间进行通信。'
- en: '*Anthos multicluster networking in non-GCP environments*—In all non-GCP clusters,
    a cluster and its address range are isolated from other clusters. This means that
    no direct connectivity exists between Pods in multiple clusters. To connect multiple
    clusters, you must use special gateways or Ingress. Anthos Service Mesh can be
    used to deploy such gateways. Often called “east-west gateways,” these are deployed
    in all clusters participating in a multicluster mesh. In addition, ASM also provides
    multicluster Service discovery.'
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos多集群网络在非GCP环境中*—在所有非GCP集群中，集群及其地址范围与其他集群隔离。这意味着多个集群之间的Pod之间不存在直接连接。要连接多个集群，您必须使用特殊的网关或入口。Anthos
    Service Mesh可以用来部署此类网关。通常称为“东西向网关”，这些网关被部署在参与多集群网络的各个集群中。此外，ASM还提供多集群服务发现。'
- en: '*Service layer networking*—The top layer of Anthos networking is Service layer
    networking. This layer addresses how Services discover and communicate with one
    another. In the previous section, we mentioned Anthos Service Mesh, which can
    enable you to do the following tasks:'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务层网络*—Anthos网络的顶层是服务层网络。这一层解决服务如何发现和相互通信的问题。在上一节中，我们提到了Anthos Service Mesh，它可以让您执行以下任务：'
- en: ASM allows you to create a Service Mesh atop multiple Anthos clusters running
    in multicloud environments. This layer abstracts the complexity of lower-layer
    networking and allows you to focus on the Service layer.
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASM允许您在运行在多云环境中的多个Anthos集群上创建服务网格。这一层抽象了底层网络的复杂性，让您可以专注于服务层。
- en: ASM uses sidecar-per-workload and specialized gateways to connect multiple clusters
    across multiple environments and networks.
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASM使用每个工作负载的边车和专用网关连接多个环境中的多个集群。
- en: By using ASM, you can focus on Service layer functions—for example, authentication,
    encryption, and authorization—instead of managing individual workloads at the
    cluster level. This allows operators and administrators to operate at scale where
    there may be multiple clusters, in multiple environments, in multiple networks
    running numerous services.
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用ASM，您可以专注于服务层功能——例如，身份验证、加密和授权——而不是在集群级别管理单个工作负载。这使得操作员和管理员能够在可能存在多个集群、多个环境、多个网络运行众多服务的情况下进行规模化操作。
