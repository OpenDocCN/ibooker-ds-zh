- en: 3 Computing environment built on Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 Kubernetes 构建的计算环境
- en: Scott Surovich
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Scott Surovich
- en: This chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding Kubernetes management, architecture, components, and resources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 管理、架构、组件和资源
- en: Declarative application management
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明式应用程序管理
- en: Understanding Kubernetes resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 资源
- en: Controlling Pod scheduling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制 Pod 调度
- en: Examples and case study
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例子和案例研究
- en: Like many new technologies, Kubernetes can be difficult to learn and implement.
    Creating a cluster manually requires an extensive skill set that includes public
    key infrastructure, Kubernetes, Linux, and networking. Many vendors recognized
    this problem and have automated cluster creation, allowing you to create Kubernetes
    clusters with little to no Kubernetes background. Although automation allows anyone
    to create a cluster, it also eliminates a lot of Kubernetes knowledge that can
    help you troubleshoot problems that you may encounter as a cluster administrator,
    or a developer, consuming the platform.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多新技术一样，Kubernetes 可能难以学习和实施。手动创建集群需要包括公钥基础设施、Kubernetes、Linux 和网络在内的广泛技能集。许多供应商认识到这个问题，并自动化了集群创建，允许你创建具有很少或没有
    Kubernetes 背景的 Kubernetes 集群。虽然自动化允许任何人创建集群，但它也消除了很多可以帮助你解决作为集群管理员或平台开发者可能遇到的问题的
    Kubernetes 知识。
- en: The question that comes up frequently is, “Do you really need to know Kubernetes?”
    The answer differs, depending on the role you will play in the cluster, but no
    matter what role you will have, you will need to have some understanding of how
    Kubernetes functions. For example, if you are a cluster admin, you should understand
    how all the cluster components interact. This understanding will help you troubleshoot
    cluster and workload deployment problems. As a developer, you should understand
    basic Kubernetes operations and the various Kubernetes resources, also referred
    to as Kubernetes *objects*, which can be used to deploy your workloads. It’s also
    important to understand how to force your deployment to a node or a set of nodes
    by using options like selectors, tolerations, and affinity/anti-affinity rules.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 经常出现的问题是，“你真的需要了解 Kubernetes 吗？”答案因你在集群中扮演的角色而异，但无论你将扮演什么角色，你都需要对 Kubernetes
    的功能有一定的了解。例如，如果你是集群管理员，你应该了解所有集群组件如何交互。这种理解将帮助你排查集群和工作负载部署问题。作为开发者，你应该了解基本的 Kubernetes
    操作和各种 Kubernetes 资源，也称为 Kubernetes *对象*，这些资源可以用来部署你的工作负载。了解如何通过使用选择器、容忍度和亲和/反亲和规则等选项将部署强制推送到节点或一组节点也很重要。
- en: In this chapter, you will learn how each component in a Kubernetes cluster interacts
    with the others. Once you understand the basic interaction, you will learn about
    the most used Kubernetes resources. Finally, to end the chapter, you will learn
    the details of how Kubernetes schedules workloads and how to constrain the scheduler
    to place workloads based on labels, selectors, and affinity/anti-affinity rules.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习 Kubernetes 集群中每个组件如何与其他组件交互。一旦你理解了基本交互，你将了解最常用的 Kubernetes 资源。最后，为了结束本章，你将学习
    Kubernetes 如何调度工作负载的细节，以及如何根据标签、选择器和亲和/反亲和规则来约束调度器放置工作负载。
- en: 3.1 Why do you need to understand Kubernetes?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 为什么你需要了解 Kubernetes？
- en: At the heart of Anthos is Kubernetes, which provides the compute engine for
    applications running in a cluster. Kubernetes is an open source project created
    by Google that has been around for years. At the time of this writing, the Cloud
    Native Computing Foundation has certified 90 Kubernetes offerings. Among the certified
    offerings are distributions from IBM, Canonical, SUSE, Mirantis, VMware, Rancher,
    Amazon, Microsoft, and, of course, Google.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 的核心是 Kubernetes，它为在集群中运行的应用程序提供计算引擎。Kubernetes 是由 Google 创建的一个开源项目，已经存在多年。在撰写本文时，云原生计算基金会已经认证了
    90 个 Kubernetes 产品。认证产品包括来自 IBM、Canonical、SUSE、Mirantis、VMware、Rancher、Amazon、Microsoft
    以及当然，Google 的发行版。
- en: 'Hearing the common complaint that deploying Kubernetes was “too difficult,”
    most vendor solutions made it easier. Although making the installation easier
    is a necessary step for most enterprises and frees up time to focus on more important
    activities, it does lead to a problem: not understanding the basic components
    and resources included in a cluster.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 听到常见的抱怨说部署 Kubernetes “太难了”，大多数供应商解决方案使其变得更容易。虽然使安装更容易对于大多数企业来说是必要的步骤，并且可以腾出时间专注于更重要的事情，但它也导致了一个问题：不了解集群中包含的基本组件和资源。
- en: Using a different service example, assume you have an application that requires
    a new database. You may not have any idea how to create a new database schema
    or SQL queries, but you know that Google offers MySQL, and you create a new instance
    for the application. The MySQL instance will be created automatically, and once
    it has been deployed, you can create a database using the GCP console.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的服务示例，假设您有一个需要新数据库的应用程序。您可能不知道如何创建新的数据库模式或 SQL 查询，但您知道 Google 提供了 MySQL，并为应用程序创建了一个新实例。MySQL
    实例将自动创建，一旦部署，您就可以使用 GCP 控制台创建数据库。
- en: Because you may not have a strong SQL background, you may stumble through and
    create a single table in the database with multiple fields that will work with
    the application. The database may perform well for a few days or weeks, but as
    it gets larger, the performance will start to slow down. A single-table database,
    though easy to implement, is not an optimized solution. If you had a SQL background,
    you would have created a database with multiple tables and relationships, making
    the database more efficient and scalable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您可能没有强大的 SQL 背景，您可能会在数据库中创建一个具有多个字段的单个表，这些字段将与应用程序一起工作。数据库可能在几天或几周内表现良好，但随着其变大，性能将开始变慢。虽然单个表数据库易于实现，但并不是一个优化的解决方案。如果您有
    SQL 背景，您会创建一个具有多个表和关系的数据库，使数据库更高效和可扩展。
- en: This scenario is like understanding how Kubernetes works and the features provided
    by the system. To use Kubernetes to its full potential, you should understand
    the underlying architecture and the role of each component. Knowing how components
    integrate with one another and what resources can be used will help you make good
    architectural decisions when deploying a cluster or deploying an application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景类似于理解 Kubernetes 的工作原理以及系统提供的功能。为了充分发挥 Kubernetes 的潜力，您应该了解其底层架构和每个组件的作用。了解组件如何相互集成以及可以使用哪些资源将帮助您在部署集群或部署应用程序时做出良好的架构决策。
- en: The details to cover each cluster component and the more than 60 resource types
    included with Kubernetes could fill a series of books. Because many of the topics
    in this chapter reference resources including Pods and DaemonSets, it will begin
    with a Kubernetes resource pocket guide, providing a brief definition of the most
    used API resources.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖每个集群组件和 Kubernetes 包含的超过 60 种资源类型的详细信息可以填满一系列书籍。由于本章中的许多主题引用了包括 Pods 和 DaemonSets
    在内的资源，因此它将从一个 Kubernetes 资源口袋指南开始，提供最常用 API 资源的简要定义。
- en: In this chapter, we will provide a background of Kubernetes components, resources,
    and commonly used add-on components, which provide the compute power that powers
    Anthos. If you are newer to Kubernetes, many books on the market today explain
    how to build a cluster and how to use kubectl and devote entire chapters to each
    Kubernetes resource. This chapter should be viewed as an introduction to resources,
    with an in-depth focus on how to control the placement of deployments in a cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供 Kubernetes 组件、资源和常用附加组件的背景信息，这些组件为 Anthos 提供计算能力。如果您对 Kubernetes
    比较陌生，市场上许多书籍都解释了如何构建集群以及如何使用 kubectl，并专门用整章内容介绍每个 Kubernetes 资源。本章应被视为资源介绍，深入关注如何在集群中控制部署的位置。
- en: 3.1.1 Technical requirements
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 技术要求
- en: 'The hands-on portion of this chapter will require you to have access to a Kubernetes
    cluster running in GCP with the following deployment pattern:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实践部分将要求您能够访问在 GCP 上运行的 Kubernetes 集群，并采用以下部署模式：
- en: The cluster must be deployed across at least two different zones in the same
    region. The examples shown in this chapter will be based on us-east4 zones, across
    us-east4-a, us-east4-b, and us-east4-c, but you can use different zones for your
    cluster.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群必须在同一区域的至少两个不同的区域中部署。本章中展示的示例将基于 us-east4 区域的 us-east4-a、us-east4-b 和 us-east4-c
    区域，但您可以使用不同的区域来部署您的集群。
- en: Each zone must contain at least one Kubernetes node.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个区域必须至少包含一个 Kubernetes 节点。
- en: This chapter is not specific to Kubernetes on GCP; the resources and constructs
    used in the exercises are applicable to any Kubernetes cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容不仅限于 GCP 上的 Kubernetes；练习中所使用的资源和结构适用于任何 Kubernetes 集群。
- en: 3.1.2 History and overview
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 历史和概述
- en: Because the audience for this book includes readers who may be newer to Kubernetes
    and readers who are seasoned Kubernetes administrators, we have added information
    covering some history and progression from physical servers to containers in the
    online appendix A.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的读者可能对Kubernetes较新或是有经验的Kubernetes管理员，我们在在线附录A中添加了一些关于从物理服务器到容器的历史和进化的信息。
- en: 3.1.3 Managing Kubernetes clusters
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 管理Kubernetes集群
- en: 'When a company decides to run a Kubernetes cluster in the cloud, they will
    often use the cloud provider’s native offering, such as the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当一家公司决定在云中运行Kubernetes集群时，他们通常会使用云提供商的本地产品，例如以下这些：
- en: 'Google Kubernetes Engine (GKE): [https://cloud.google.com/kubernetes-engine/](https://cloud.google.com/kubernetes-engine/)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '谷歌Kubernetes引擎（GKE）: [https://cloud.google.com/kubernetes-engine/](https://cloud.google.com/kubernetes-engine/)'
- en: 'Amazon Elastic Kubernetes Service (EKS): [https://aws.amazon.com/eks/](https://aws.amazon.com/eks/)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '亚马逊弹性Kubernetes服务（EKS）: [https://aws.amazon.com/eks/](https://aws.amazon.com/eks/)'
- en: 'Azure Kubernetes Service (AKS): [http://mng.bz/GR7V](http://mng.bz/GR7V)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '微软Azure Kubernetes服务（AKS）: [http://mng.bz/GR7V](http://mng.bz/GR7V)'
- en: Using the native offering offers the quickest and easiest way to get a new cluster
    up and running, because the providers have automated the installation. To get
    from ground zero to a running cluster, you need to provide only a few pieces of
    information, like the number and size of the nodes, zones, and regions. With this
    information and a click or API call, you can have a cluster in a few minutes,
    ready to deploy your applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本地产品是获取新集群并快速运行的最快和最简单的方法，因为提供商已经自动化了安装。要从零开始到运行集群，您只需要提供一些信息，如节点数量和大小、区域和地区。有了这些信息和点击或API调用，您可以在几分钟内拥有一个集群，准备好部署您的应用程序。
- en: 'Google was the first cloud service provider to offer their Kubernetes solution
    across both the cloud and on-prem, without requiring any specialized hardware
    solution. Before Google did this, other offerings required organizations to deploy
    a different solution for each cloud provider and their on-prem clusters. Using
    a different solution for multiple installations often leads to a variety of different
    problems, including these:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Google是第一个提供其Kubernetes解决方案的云服务提供商，该解决方案既适用于云环境也适用于本地环境，而不需要任何专门的硬件解决方案。在Google这样做之前，其他提供的产品要求组织为每个云提供商及其本地集群部署不同的解决方案。为多个安装使用不同的解决方案通常会导致各种不同的问题，包括以下这些：
- en: Increased staff to support each deployment
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加人员以支持每个部署
- en: Differences in the deployment of an application for on-prem and off-prem
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地和远程部署应用程序的差异
- en: Different identity management solutions
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的身份管理解决方案
- en: Different Kubernetes versions
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的Kubernetes版本
- en: Different security models
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的安全模型
- en: Difficulty in standardizing cluster operations
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化集群操作的困难
- en: No single view for all clusters
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有所有集群的单个视图
- en: Each of these differences makes the job of running Kubernetes more difficult
    and, ultimately, more costly for an organization.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异使得运行Kubernetes的工作更加困难，最终对组织来说成本更高。
- en: Google recognized these problems and created Anthos, which addresses the on-prem
    and off-prem challenges by providing a Kubernetes installation and management
    solution that not only works on GCP and on-prem clusters but also in other cloud
    providers like AWS and Azure running Anthos.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Google认识到这些问题，并创建了Anthos，它通过提供一种Kubernetes安装和管理解决方案来解决本地和远程挑战，该解决方案不仅适用于GCP和本地集群，还适用于运行Anthos的其他云提供商，如AWS和Azure。
- en: 'Using Anthos provides a common environment no matter where you deploy it. Imagine
    having a single support path and a common set of tools for all your clusters in
    GCP, AWS, Azure, and on-prem. Anthos provides an organization with many advantages,
    including the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Anthos无论部署在哪里都提供统一的平台。想象一下，您有一个单一的支持路径和一套通用的工具，用于GCP、AWS、Azure和本地环境中的所有集群。Anthos为组织提供了许多优势，包括以下这些：
- en: A consolidated view of clusters inside the Anthos console
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Anthos控制台中查看集群的统一视图
- en: A common service mesh offering
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通用的服务网格提供
- en: Configuration management using ACM
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ACM进行配置管理
- en: 'All options supported by Google: a single point of contact for all cluster
    components'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google支持的所有选项：所有集群组件的单一点联系
- en: Best of all, Anthos is based on the upstream Kubernetes, so you get all the
    standard features but with the added tools and components that Anthos provides,
    making multiple cloud cluster management easier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的是，Anthos基于上游Kubernetes，因此您获得所有标准功能，但还增加了Anthos提供的工具和组件，这使得多云集群管理更加容易。
- en: Next, we will jump into the architecture that makes up a Kubernetes cluster
    and how the components communicate with each other.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入了解构成 Kubernetes 集群的架构以及组件之间是如何相互通信的。
- en: 3.2 Kubernetes architecture
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 Kubernetes 架构
- en: 'Like any infrastructure, Kubernetes consists of multiple components that communicate
    to create a cluster. The components are grouped into two layers: the control plane
    and the worker nodes. The control plane keeps the cluster state, accepting incoming
    requests, scheduling workloads, and running controllers, whereas the worker nodes
    communicate with the control plane to report available resources, run container
    workloads, and maintain node network rules.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何基础设施一样，Kubernetes 由多个组件组成，这些组件通过通信来创建集群。这些组件被分为两层：控制平面和工作节点。控制平面负责保持集群状态，接受传入请求，调度工作负载，并运行控制器，而工作节点则与控制平面通信，以报告可用资源，运行容器工作负载，并维护节点网络规则。
- en: If you are running Anthos on GCP, you may not be familiar with the components
    of the control plane or the worker nodes, because you do not interact with them
    like you would with an on-prem installation. As this section will explain, Kubernetes
    clusters have a layer called the control plane that contains the components required
    to run Kubernetes. When a cluster is running in GCP, the control plane is created
    in a Google-managed project, which limits you from interacting with the admin
    nodes and the Kubernetes components.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在 GCP 上运行 Anthos，你可能不熟悉控制平面或工作节点的组件，因为你不会像在本地安装中那样与之交互。本节将解释，Kubernetes
    集群有一个名为控制平面的层，其中包含运行 Kubernetes 所需的组件。当集群在 GCP 上运行时，控制平面是在一个 Google 管理的项目中创建的，这限制了你对管理节点和
    Kubernetes 组件的交互。
- en: All GKE clusters can be viewed in your GCP console, located under the Kubernetes
    Engine section. For each cluster, you can view the details of the nodes by clicking
    on the cluster in the details pane, then selecting Nodes. The node details will
    be displayed, as shown in figure 3.1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 GKE 集群都可以在 GCP 控制台中查看，位于 Kubernetes 引擎部分。对于每个集群，你可以通过在详情面板中点击集群，然后选择节点来查看节点的详细信息。节点详细信息将显示，如图
    3.1 所示。
- en: '![03-01](../../OEBPS/Images/03-01.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![03-01](../../OEBPS/Images/03-01.png)'
- en: Figure 3.1 GKE node details
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 GKE 节点详细信息
- en: Unlike GKE on GCP, an on-prem installation of GKE provides access to the control
    plane nodes and Kubernetes resources for the clusters. Of course, Google still
    supports the on-prem control plane, but you may be asked to look at components
    to troubleshoot any problems or configuration changes to a cluster. If you have
    only deployed GKE on GCP, you may not know all the components of the control plane
    and how they interact. Understanding this interaction is vital to troubleshooting
    and finding root causes to any problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GCP 上的 GKE 不同，本地安装的 GKE 提供了对控制平面节点和集群 Kubernetes 资源的访问。当然，Google 仍然支持本地控制平面，但你可能需要检查组件以排查任何问题或对集群进行配置更改。如果你只在
    GCP 上部署了 GKE，你可能不了解控制平面的所有组件以及它们是如何交互的。理解这种交互对于故障排除和找到任何问题的根本原因至关重要。
- en: Note When you deploy a GKE on-prem cluster, three Kubernetes config files are
    created. One will be named using the user cluster’s name with a suffix of -kubeconfig,
    one is called kubeconfig, and the last one is called internal-cluster-kubeconfig-debug.
    The kubeconfig file is configured to target the load-balanced address of the admin
    cluster, whereas internal-cluster-kubeconfig-debug is configured to target the
    admin cluster’s API server directly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当你在本地部署 GKE 集群时，会创建三个 Kubernetes 配置文件。其中一个将以用户集群的名称加上 -kubeconfig 后缀命名，一个是
    kubeconfig，最后一个叫做 internal-cluster-kubeconfig-debug。kubeconfig 文件配置为针对管理集群的负载均衡地址，而
    internal-cluster-kubeconfig-debug 则配置为直接针对管理集群的 API 服务器。
- en: To view the multiple configuration files, see figure 3.2.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看多个配置文件，请参阅图 3.2。
- en: '![03-02](../../OEBPS/Images/03-02.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![03-02](../../OEBPS/Images/03-02.png)'
- en: Figure 3.2 Admin cluster and user cluster configuration files
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 管理集群和用户集群配置文件
- en: With the importance of understanding the system, let’s move on to each layer
    in a cluster, starting with the control plane.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解系统的重要性之后，让我们继续了解集群中的每一层，从控制平面开始。
- en: 3.2.1 Understanding the cluster layers
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 理解集群层
- en: 'The first layer, the control plane, contains five or six components (in reality,
    the two controllers actually contain multiple components). The control plane includes
    the components that provide cluster management, cluster state, and scheduling
    features. We will detail each component in the next section, but for now, we just
    want to introduce the control plane components, shown here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层，控制平面，包含五个或六个组件（实际上，两个控制器实际上包含多个组件）。控制平面包括提供集群管理、集群状态和调度功能的组件。我们将在下一节中详细说明每个组件，但现在，我们只想介绍这里所示的控制平面组件：
- en: ETCD
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ETCD
- en: The Kubernetes API server
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes API 服务器
- en: The Kubernetes scheduler
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 调度器
- en: The Kubernetes controller manager, which contains multiple controllers
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含多个控制器的 Kubernetes 控制器管理器
- en: Node controller
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点控制器
- en: Endpoint controller
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端点控制器
- en: Replication controller
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本控制器
- en: Service account/token controller
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务帐户/令牌控制器
- en: The cloud controller manager, which contains multiple controllers
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含多个控制器的云控制器管理器
- en: Route controller
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由控制器
- en: Service controller
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务控制器
- en: Node controller
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点控制器
- en: To view a graphical representation of the control plane, see figure 3.3\. At
    the end of this section, we will provide a complete component diagram, including
    how each component communicates.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看控制平面的图形表示，请参阅图 3.3。在本节结束时，我们将提供一个完整的组件图，包括每个组件的通信方式。
- en: '![03-03](../../OEBPS/Images/03-03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![03-03](../../OEBPS/Images/03-03.png)'
- en: Figure 3.3 Control plane components
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 控制平面组件
- en: The second layer in the cluster is the collection of worker nodes, which are
    responsible for running the cluster workloads. Each worker node has three components
    that work together to run applications, as shown in figure 3.4.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的第二层是由工作节点组成的集合，它们负责运行集群工作负载。每个工作节点有三个组件协同工作以运行应用程序，如图 3.4 所示。
- en: '![03-04](../../OEBPS/Images/03-04.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![03-04](../../OEBPS/Images/03-04.png)'
- en: Figure 3.4 Worker node components
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 工作节点组件
- en: Up to this point, we haven’t explained how each component interacts with the
    others. Before we show a full diagram of cluster interactions, we need to understand
    each component in the cluster. In the next section, we will explain each cluster
    component, and, to close out the section, we will combine the two diagrams to
    show the connectivity between all components.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有解释每个组件如何与其他组件交互。在我们展示集群交互的完整图之前，我们需要了解集群中的每个组件。在下一节中，我们将解释每个集群组件，并在本节结束时，我们将结合两个图来展示所有组件之间的连接性。
- en: 3.2.2 The control plane components
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 控制平面组件
- en: As mentioned earlier, the control plane includes up to six components. Each
    of the components works together to provide cluster services. Understanding each
    component is key to delivering a robust, stable cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，控制平面包括多达六个组件。每个组件协同工作以提供集群服务。理解每个组件是提供强大、稳定的集群的关键。
- en: etcd
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: etcd
- en: Every resource in the cluster and its state are maintained in the etcd key-value
    database. The entire cluster state is stored inside this database, making etcd
    the most important component in a cluster. Without a functioning etcd database,
    you do not have a functioning cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的每个资源和其状态都维护在 etcd 键值数据库中。整个集群状态都存储在这个数据库中，使得 etcd 成为集群中最重要的组件。如果没有一个正常工作的
    etcd 数据库，你就没有了一个正常工作的集群。
- en: Because etcd is so important, you should always have at least three replicas
    running in a cluster. Depending on the size of the cluster, you may want to have
    more than three, but no matter how many you decide to run, always run an odd number
    of replicas. Running an odd number of etcd nodes allows the cluster to elect a
    majority leader, minimizing the chance of the etcd cluster going into a split-brain
    state. If a cluster goes into a split-brain state, more than one node claims to
    be the majority leader, which leads to data inconsistencies and corruption. If
    you find yourself in a split-brain state, you will need to recreate the etcd cluster
    from an etcd backup.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 etcd 非常重要，你应该在集群中始终至少运行三个副本。根据集群的大小，你可能想要运行超过三个，但无论你决定运行多少，总是运行奇数个副本。运行奇数个
    etcd 节点允许集群选举一个多数领导者，最小化 etcd 集群进入脑裂状态的机会。如果一个集群进入脑裂状态，多个节点声称自己是多数领导者，这会导致数据不一致和损坏。如果你发现自己处于脑裂状态，你需要从
    etcd 备份中重新创建 etcd 集群。
- en: Although running multiple copies will make etcd highly available, you also need
    to create a regular backup of your database and store it outside of the cluster
    in a safe location. If you lose your entire cluster or your etcd database gets
    corrupted, you will be able to restore your backup to restore a node or the entire
    cluster. We will explain the process to back up etcd later in this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然运行多个副本会使 etcd 高可用，但您还需要创建数据库的常规备份，并将其存储在集群外的一个安全位置。如果您丢失整个集群或 etcd 数据库损坏，您将能够恢复备份以恢复节点或整个集群。我们将在本章后面解释备份
    etcd 的过程。
- en: The last consideration for etcd after making it highly available and creating
    regular backups is security. The etcd database contains every Kubernetes resource,
    so it will contain sensitive data like secrets, which may contain data like passwords.
    If someone gets a copy of your etcd database, they can easily pull any of the
    resources out because, by default, they are stored as clear text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使 etcd 高可用并创建常规备份之后，对 etcd 的最后一个考虑因素是安全性。etcd 数据库包含每个 Kubernetes 资源，因此它将包含敏感数据，如机密，这些机密可能包含密码等数据。如果有人获得了您的
    etcd 数据库副本，他们可以轻松地拉取任何资源，因为默认情况下，它们以明文形式存储。
- en: Covering etcd could require an entire chapter. For more information on etcd,
    head over to the main etcd site at [https://etcd.io/docs/](https://etcd.io/docs/).
    Google also provides the steps and a script to back up GKE on-prem clusters. You
    can find the documentation and the script at [http://mng.bz/zm1r](http://mng.bz/zm1r).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖 etcd 可能需要整整一章。有关 etcd 的更多信息，请访问主 etcd 网站 [https://etcd.io/docs/](https://etcd.io/docs/)。Google
    还提供了备份 GKE 本地集群的步骤和脚本。您可以在 [http://mng.bz/zm1r](http://mng.bz/zm1r) 找到文档和脚本。
- en: The Kubernetes API server
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API 服务器
- en: The API server is the front door to a cluster. All requests that come into the
    cluster enter through the API server, which will interact with the other component
    to fulfill requests. These requests come from users and services from the kubectl
    CLI, Kubernetes Dashboard, or direct JSON API calls.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器是集群的大门。所有进入集群的请求都通过 API 服务器进入，它将与其他组件交互以完成请求。这些请求来自 kubectl CLI、Kubernetes
    Dashboard 或直接 JSON API 调用的用户和服务。
- en: It’s really an event-driven hub-and-spoke model. The API server encapsulates
    etcd. All other components communicate with the API server. The API server doesn’t
    communicate with controllers directly in response to requests. Instead, the controllers
    watch for relevant change events.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个事件驱动的中心辐射模型。API 服务器封装了 etcd。所有其他组件都与 API 服务器通信。API 服务器不会直接响应请求与控制器通信。相反，控制器会监视相关的更改事件。
- en: The Kubernetes scheduler
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 调度器
- en: If the API server receives a request to create a Pod, it will communicate with
    the Kubernetes scheduler, which decides which worker node will run the workload.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 API 服务器收到创建 Pod 的请求，它将与 Kubernetes 调度器通信，调度器将决定哪个工作节点将运行工作负载。
- en: When a workload attempts to request a resource that cannot be met, or has constraints
    that cannot be matched, it will fail to schedule and the Pod will not start. If
    this happens, you will need to find out why the scheduling failed and either change
    your deployment code or add resources to your nodes to fulfill the request.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作负载尝试请求无法满足的资源或具有无法匹配的约束时，它将无法调度，Pod 也不会启动。如果发生这种情况，您需要找出调度失败的原因，或者更改您的部署代码，或者向您的节点添加资源以满足请求。
- en: The Kubernetes controller manager
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制器管理器
- en: The controller manager is often referred to as a *control loop*. To allow Kubernetes
    to keep all resources in a requested, desired state, the state of each resource
    must be compared to its requested state. The process that makes this happen is
    known as a *control loop*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器管理器通常被称为*控制循环*。为了使 Kubernetes 保持所有资源处于请求的、期望的状态，必须将每个资源的状态与其请求状态进行比较。实现这一过程的过程被称为*控制循环*。
- en: The Kubernetes controller manager consists of a single binary that runs separate
    threads for each “logical” controller. The bundled controllers and their roles
    are shown in table 3.1.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制器管理器由一个二进制文件组成，该文件为每个“逻辑”控制器运行单独的线程。包含的控制器及其角色在表 3.1 中显示。
- en: Table 3.1 Bundled controllers and their roles
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 包含的控制器及其角色
- en: '| Controller | Description |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 控制器 | 描述 |'
- en: '| Node | Maintains the status of all nodes |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 维护所有节点的状态 |'
- en: '| Replication | Maintains the number of pods for replication controllers |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 复制 | 维护复制控制器的 Pod 数量 |'
- en: '| Endpoint | Maintains the mapping of pods to services, creating endpoints
    for services |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 端点 | 维护 Pod 到服务的映射，为服务创建端点 |'
- en: '| Service accounts/token | Creates the initial default account and API tokens
    for namespaces |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 服务账户/令牌 | 为命名空间创建初始默认账户和 API 令牌 |'
- en: The main concept to take away from the table is that by using a control loop,
    the manager constantly checks the resource(s) that it controls to keep them in
    the declared state.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以得出的主要概念是，通过使用控制循环，管理器不断检查其控制的资源（资源），以保持它们处于声明状态。
- en: The Kubernetes controller manager deals with internal Kubernetes resource states.
    If you are using a cloud provider, your cluster will need a controller to maintain
    certain resources, which is the role of the cloud controller manager.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制器管理器处理内部 Kubernetes 资源状态。如果你使用云服务提供商，你的集群将需要一个控制器来维护某些资源，这就是云控制器管理器的角色。
- en: The cloud controller manager
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 云控制器管理器
- en: Note You may not see this controller on every cluster you interact with. A cluster
    will run a cloud controller only if it has been configured to interface with a
    cloud provider.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能在每个你交互的集群中都不会看到这个控制器。一个集群只有在配置了与云服务提供商接口的情况下才会运行云控制器。
- en: To allow cloud providers flexibility, the cloud controller manager is separate
    from the standard Kubernetes controller manager. By decoupling the two controllers,
    each cloud provider can add features to their offering that may differ from other
    providers or base Kubernetes components.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给云服务提供商提供灵活性，云控制器管理器与标准的 Kubernetes 控制器管理器是分开的。通过解耦这两个控制器，每个云服务提供商都可以向其服务中添加可能与其他提供商或基础
    Kubernetes 组件不同的功能。
- en: Like the Kubernetes controller manager, the cloud controller manager uses a
    control loop to maintain the desired state of resources. It is also a single binary
    that runs multiple controllers and their processes, as shown in table 3.2.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kubernetes 控制器管理器类似，云控制器管理器使用控制循环来维护资源的期望状态。它也是一个运行多个控制器及其进程的单个二进制文件，如表 3.2
    所示。
- en: Table 3.2 Controllers run by the cloud controller manager
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2 云控制器管理器运行的控制器
- en: '| Controller | Description |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 控制器 | 描述 |'
- en: '| Node | Creates node resources and maintains the status of the nodes located
    in the cloud provider |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 创建节点资源并维护位于云服务提供商中的节点状态 |'
- en: '| Route | Maintains network routes to provide node communication |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 路由 | 维护网络路由以提供节点通信 |'
- en: '| Service | Maintains cloud provider components like load balancers, network
    filtering, and IP addresses |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 维护云服务提供商组件，如负载均衡器、网络过滤和 IP 地址 |'
- en: 'Finally, when we say *cloud provider*, we do not mean you are limited to only
    public cloud service providers. At the time of this writing, Kubernetes includes
    support for the following cloud providers:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们说“云服务提供商”时，我们并不是指你仅限于使用公共云服务提供商。在撰写本文时，Kubernetes 支持以下云服务提供商：
- en: Amazon AWS
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon AWS
- en: Microsoft Azure
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Azure
- en: Google Cloud Platform (GCP)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud Platform (GCP)
- en: OpenStack
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack
- en: Huawei
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 华为
- en: vSphere
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere
- en: Now that the control plane has been explained, let’s move on to the worker node
    components.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经解释了控制平面，让我们继续讨论工作节点组件。
- en: 3.2.3 Worker node components
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 工作节点组件
- en: From a high level, you should have a basic understanding of the components in
    the control plane. It’s the layer responsible for cluster interaction and workload
    deployments. Alone, the control plane can’t do very much—it needs to have a target
    that can run the actual workload once it’s scheduled, and that’s where the worker
    node comes in.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，你应该对控制平面中的组件有一个基本的了解。它是负责集群交互和工作负载部署的层。单独的控制平面不能做很多事情——它需要一个目标，一旦调度，就可以运行实际的工作负载，这就是工作节点的作用所在。
- en: The kubelet
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet
- en: The kubelet is the component responsible for running a Pod and for reporting
    the node’s status to the Kubernetes scheduler. When the scheduler decides which
    node will run a workload, the kubelet retrieves it from the API server, and the
    Pod is created based on the specs that were pulled.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 是负责运行 Pod 并向 Kubernetes 调度器报告节点状态的组件。当调度器决定哪个节点将运行工作负载时，kubelet 从 API
    服务器检索它，并根据拉取的规范创建 Pod。
- en: kube-proxy
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy
- en: We will mention this in more detail when we discuss services in the next section,
    but for now you only need to understand a basic overview of kube-proxy. kube-proxy
    is responsible for creating and deleting network rules, which allow network connectivity
    to a Pod. If the host operating system offers a packet filter, kube-proxy will
    use it, but if no packet filter is offered, the traffic will be managed by kube-proxy
    itself.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节讨论服务时详细介绍这一点，但到目前为止，你只需要了解 kube-proxy 的基本概述。kube-proxy 负责创建和删除网络规则，这些规则允许
    Pod 进行网络连接。如果主机操作系统提供了数据包过滤器，kube-proxy 将使用它；如果没有提供，流量将由 kube-proxy 本身管理。
- en: Depending on the network provider you decide to use for a cluster, you may have
    the option to run your cluster in a kube-proxyless mode. A Container Network Interface
    (CNI) like Cilium uses eBPF to provide the same functionality that kube-proxy
    provides but without requiring additional components outside of the base CNI deployment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你为集群选择的网络提供商，你可能可以选择在无 kube-proxy 模式下运行你的集群。像 Cilium 这样的容器网络接口（CNI）使用 eBPF
    提供与 kube-proxy 相同的功能，但无需在基本 CNI 部署之外添加额外的组件。
- en: Container runtime
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时
- en: The container runtime is the component responsible for running the actual container
    on the host. It has become common for people to refer to the container runtime
    as simply Docker. This is understandable because Docker did bring containers to
    the masses, but over the years, other alternatives have been developed. Two of
    the most popular alternatives are CRI-O and containerd.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时是负责在主机上运行实际容器的组件。人们通常将容器运行时简称为 Docker。这是可以理解的，因为 Docker 确实将容器普及开来，但多年来，其他替代方案也被开发出来。其中两种最受欢迎的替代方案是
    CRI-O 和 containerd。
- en: 'At one time, the container runtime was integrated into the kubelet, which made
    adding a new runtime difficult. As Kubernetes matured, the team developed the
    Container Runtime Interface (CRI), which provides the ability to simply “plug
    in” a container runtime. No matter which runtime is in use, its responsibility
    is the same: to run the actual container on the node.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经，容器运行时被集成到 kubelet 中，这使得添加新的运行时变得困难。随着 Kubernetes 的成熟，团队开发了容器运行时接口（CRI），它提供了简单“插入”容器运行时的能力。无论使用哪种运行时，其责任都是相同的：在节点上运行实际的容器。
- en: Now that we have reviewed each layer and their components, let’s show the connectivity
    between the two layers and how the components interact, as illustrated in figure
    3.5.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了每一层及其组件，让我们展示这两层之间的连接以及组件如何交互，如图 3.5 所示。
- en: '![03-05](../../OEBPS/Images/03-05.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![03-05](../../OEBPS/Images/03-05.png)'
- en: Figure 3.5 Cluster component communications
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 集群组件通信
- en: This concludes the section on Kubernetes cluster components. Knowing how the
    components interact will help you to diagnose problems and understand how the
    cluster interacts as a system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了关于 Kubernetes 集群组件的部分。了解组件如何交互将帮助你诊断问题，并理解集群作为一个系统如何交互。
- en: Depending on your role, understanding the cluster components and how they interact
    may be less important than understanding cluster resources. Kubernetes resources
    are used by every user that interacts with a cluster, and users should understand,
    at the very least, the most used resources. For reference, you can read about
    Kubernetes resources on the Kubernetes website at [http://mng.bz/0yRm](http://mng.bz/0yRm).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的角色，了解集群组件及其交互可能不如了解集群资源那么重要。Kubernetes 资源被每个与集群交互的用户使用，用户至少应该了解最常用的资源。作为参考，你可以在
    Kubernetes 网站上阅读有关 Kubernetes 资源的信息，网址为 [http://mng.bz/0yRm](http://mng.bz/0yRm)。
- en: To effectively deploy an application on Kubernetes, you need to understand the
    features of the infrastructure, starting with Kubernetes objects. Next, we will
    move on to DevOps paradigms and Kubernetes cluster components.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 上有效地部署应用程序，你需要了解基础设施的功能，从 Kubernetes 对象开始。接下来，我们将继续探讨 DevOps 范式和
    Kubernetes 集群组件。
- en: 3.2.4 Understanding declarative and imperative
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 理解声明性和命令性
- en: In DevOps, an automation framework can use two different implementation methods,
    referred to as *DevOps paradigms*. They include the declarative model and the
    imperative model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DevOps 中，自动化框架可以使用两种不同的实现方法，被称为 *DevOps 范式*。它们包括声明性模型和命令性模型。
- en: Each of the paradigms will be explained in this chapter, but before diving into
    the differences between them, you should understand the concept of a control loop.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将解释每个范式，但在深入探讨它们之间的差异之前，你应该了解控制循环的概念。
- en: Understanding control loops
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 理解控制循环
- en: To maintain your desired state, Kubernetes implements a set of control loops.
    A control loop is an endless loop that is always checking that the declared state
    of a resource is the same as its current state.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了维持您期望的状态，Kubernetes实现了一套控制循环。控制循环是一个无限循环，它始终检查资源的声明状态是否与其当前状态相同。
- en: If you declare that a deployment should have three replicas of a Pod, and one
    Pod is deleted accidentally, Kubernetes will create a new Pod to keep the states
    in sync. Figure 3.6 shows a graphical representation of the ReplicaSet control
    loop and how it maintains the desired replica count.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您声明部署应该有三个Pod副本，并且意外删除了一个Pod，Kubernetes将创建一个新的Pod以保持状态同步。图3.6显示了ReplicaSet控制循环的图形表示以及它如何维持期望的副本计数。
- en: '![03-06](../../OEBPS/Images/03-06.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![03-06](../../OEBPS/Images/03-06.png)'
- en: Figure 3.6 Control loop example
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 控制循环示例
- en: As you can see, a control loop doesn’t need to be complex to maintain a desired
    state. The replication controller simply keeps looping through all the ReplicaSet
    resources in the cluster, comparing the currently available number of Pods to
    the desired number of Pods that is declared. Kubernetes will either add or delete
    a Pod to make the current replica count equal to the count that has been set on
    the deployment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，控制循环不需要复杂就能维持期望状态。副本控制器简单地循环遍历集群中所有的ReplicaSet资源，比较当前可用的Pod数量与声明的期望Pod数量。Kubernetes将添加或删除Pod，以使当前副本计数等于在部署上设置的计数。
- en: Understanding the features of Anthos and how Kubernetes maintains the declared
    state of a deployment is important for any user of Kubernetes, but it’s only the
    beginning. Because deploying a cluster has been made so simple by many vendors,
    developers and administrators often overlook the advantages of understanding the
    entire system. As mentioned earlier, to design an effective cluster or application,
    you should understand the basic functionality of the cluster components. In the
    next section, the Kubernetes architecture will be covered, including the components
    of the control plane and worker nodes and how they interact with each other.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Anthos的功能以及Kubernetes如何维护部署的声明状态对于任何Kubernetes用户来说都很重要，但这只是开始。因为许多供应商已经使得部署集群变得非常简单，开发人员和管理员往往忽略了理解整个系统的优势。如前所述，为了设计一个有效的集群或应用程序，您应该了解集群组件的基本功能。在下一节中，将介绍Kubernetes架构，包括控制平面和工作节点组件以及它们之间的交互方式。
- en: One of the first concepts to understand is the difference between the declarative
    and imperative models. Table 3.3 provides a brief description of each model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的第一批概念之一是声明性模型和命令性模型之间的区别。表3.3提供了每个模型的简要描述。
- en: Table 3.3 Declarative and imperative models
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3 声明性和命令性模型
- en: '| Model | Description |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述 |'
- en: '| Declarative | Developers declare what they would like the system to do; there
    is no need to tell the system *how* to do it.The declarative model uses Kubernetes
    manifests to declare the application’s desired state. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 声明性 | 开发者声明他们希望系统执行的操作；无需告诉系统*如何*执行。声明性模型使用Kubernetes清单来声明应用程序的期望状态。 |'
- en: '| Imperative | Developers are responsible for creating each step required for
    the desired end state. The steps to create the deployment are completely defined
    by the developer.The imperative model uses kubectl commands like create, run,
    and delete to tell the API server what resources to manage. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 命令性 | 开发者负责创建实现所需最终状态所需的每个步骤。创建部署的步骤完全由开发者定义。命令性模型使用kubectl命令（如create、run和delete）告诉API服务器要管理哪些资源。
    |'
- en: In a declarative model, you can manage several resources in a single file. For
    example, if we wanted to deploy an NGINX web server that included a new namespace,
    the deployment, and a service, we would create a single YAML file with all the
    resources. The manifest would then be deployed using the kubectl apply command,
    which will create each resource and add an annotation that includes the last applied
    configuration. Because Kubernetes tracks the resources and you have all the resources
    in a single file, it is easier to manage and track changes to the deployment and
    resources.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明性模型中，您可以在单个文件中管理多个资源。例如，如果我们想部署一个包含新命名空间、部署和服务的新NGINX网络服务器，我们将创建一个包含所有资源的单个YAML文件。然后，使用kubectl
    apply命令部署该清单，该命令将创建每个资源并添加一个包含最后应用的配置的注释。因为Kubernetes跟踪资源，并且您在单个文件中拥有所有资源，所以管理部署和资源的变化更容易。
- en: 'In an imperative model, you must run multiple commands to create your final
    deployment. Using the previous example where you want to deploy an NGINX server,
    a service, and an Ingress rule, you would need to execute the following three
    kubectl commands:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 imperative 模型中，你必须运行多个命令来创建你的最终部署。使用之前的示例，其中你想要部署一个 NGINX 服务器、一个服务和一个 Ingress
    规则，你需要执行以下三个 kubectl 命令：
- en: '[PRE0]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although this would accomplish the same deployment as our declarative example,
    it has some limitations that are not immediately noticeable using our simple example.
    One limitation is that the kubectl command does not allow you to configure every
    option available for each resource. In the example, we deploy a Pod with a single
    container running NGINX. If we needed to add a second container to perform a specialized
    task, like logging, we wouldn’t be able to add it imperatively because the kubectl
    command does not have the option to launch two containers in a Pod.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这会完成与我们的声明式示例相同的部署，但它有一些限制，在简单的示例中并不立即明显。一个限制是 kubectl 命令不允许你为每个资源配置所有可用的选项。在示例中，我们部署了一个运行
    NGINX 的 Pod。如果我们需要添加第二个容器来执行专门的任务，如日志记录，我们就无法通过 imperative 方式添加它，因为 kubectl 命令没有在
    Pod 中启动两个容器的选项。
- en: It is a good practice to avoid using imperative deployments unless you are attempting
    to resolve a problem quickly. If you find yourself using imperative commands for
    any reason, you should keep track of your changes so that you can alter your declarative
    manifests to keep them in sync with any changes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用 imperative 部署是一个好习惯，除非你试图快速解决问题。如果你出于任何原因使用了 imperative 命令，你应该跟踪你的更改，以便你可以修改你的声明式清单，以保持它们与任何更改同步。
- en: To understand how Kubernetes uses the declarative model, you need to understand
    how the system maintains the declared state with the currently running state for
    a deployment by using control loops.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 Kubernetes 如何使用声明式模型，你需要了解系统如何通过使用控制循环来维护声明状态与当前运行状态之间的同步，以对部署进行管理。
- en: 3.2.5 Understanding Kubernetes resources
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 理解 Kubernetes 资源
- en: Throughout this book, you will see references to multiple Kubernetes resources.
    As mentioned earlier in the chapter, there are more than 60 resource types included
    with a new cluster, not including any custom resources that may be added through
    CRDs (custom resource definitions). Multiple Kubernetes books are available, so
    this chapter will provide only an introduction to each resource to provide a base
    knowledge that will be used in most of the chapters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，你会看到对多个 Kubernetes 资源的引用。如本章前面所述，一个新集群包含60多种资源类型，不包括可能通过 CRDs（自定义资源定义）添加的任何自定义资源。有多个
    Kubernetes 书籍可供选择，因此本章将只提供每个资源的简介，以提供在大多数章节中都将使用的基礎知识。
- en: 'It’s challenging to remember all the base resources, and you may not always
    have a pocket guide available to you. Luckily, you can use a few commands to look
    up resources and the options that are available for each. The first command, shown
    next, lists all the API resources available on a cluster:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 记住所有基本资源是有挑战性的，你可能并不总是有口袋指南可用。幸运的是，你可以使用一些命令来查找资源以及每个资源可用的选项。第一个命令，如下所示，列出了集群上可用的所有
    API 资源：
- en: '[PRE1]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output provides the name of the resource—any short name, if it can be used
    at a namespace level—and the kind of resource. This is helpful if you know what
    each one does, but you forgot the name or whether it can be set at a namespace
    level. If you need additional information for any resource, Kubernetes provides
    the next command, which provides the details for each one:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了资源的名称——如果可以在命名空间级别使用，任何简短名称——以及资源的类型。如果你知道每个资源的作用，但忘记了名称或是否可以在命名空间级别设置，这将很有帮助。如果你需要任何资源的额外信息，Kubernetes
    提供了下一个命令，它为每个资源提供详细信息：
- en: '[PRE2]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The explain command provides a short description of the resource and all the
    fields that can be used in a manifest. For example, next you see a brief description
    of what a Pod is and some of the fields that can be used when creating the resource:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: explain 命令提供了资源及其在清单中可用的所有字段的简要描述。例如，接下来你会看到一个关于 Pod 的简要描述以及创建资源时可以使用的一些字段：
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from the output, each field has a detailed explanation and a
    link to provide additional detailed information, when applicable.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个字段都有详细的解释和链接，当适用时提供额外的详细信息。
- en: You may not have access to a system with kubectl installed all the time, so
    table 3.4 provides a short description of most of the common resources you will
    use in a cluster.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能无法始终访问安装了kubectl的系统，因此表3.4提供了大多数您将在集群中使用的常见资源的简要描述。
- en: Table 3.4 Resources used in a cluster
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.4 集群中使用的资源
- en: '| Kubernetes resource | Description |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Kubernetes资源 | 描述 |'
- en: '| ConfigMaps | Hold configuration data for Pods. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ConfigMaps | 存储Pod的配置数据。|'
- en: '| EndpointSlice | A collection of Pods that are used as targets by services.
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| EndpointSlice | 一组用作服务目标的Pod。|'
- en: '| Namespace | Used to divide clusters between multiple developers or applications.
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Namespace | 用于在多个开发人员或应用程序之间划分集群。|'
- en: '| Node | Provides the compute power to a Kubernetes cluster. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Node | 为Kubernetes集群提供计算能力。|'
- en: '| PersistentVolumeClaim | Allows an application to claim a persistent volume.
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| PersistentVolumeClaim | 允许应用程序声明持久卷。|'
- en: '| PersistentVolume | A storage resource provisioned at the cluster layer. Claims
    to PersistentVolume are provided by a PersistentVolumeClaim. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| PersistentVolume | 在集群层配置的存储资源。PersistentVolumeClaim请求由PersistentVolume提供。|'
- en: '| Pod | A container or a collection of containers. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Pod | 一个容器或一组容器。|'
- en: '| ResourceQuota | Sets quota restrictions, enforced per namespace. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ResourceQuota | 设置配额限制，按命名空间强制执行。|'
- en: '| Secret | Holds secret data of a certain type. The total bytes of the values
    in the data field must be less than the MaxSecretSize bytes configuration value.
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Secret | 存储特定类型的秘密数据。数据字段中值的总字节数必须小于MaxSecretSize配置值。|'
- en: '| ServiceAccount | Provides an identity that can be authenticated and authorized
    to resources in a cluster. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ServiceAccount | 提供一个身份，可以对其进行身份验证和授权以访问集群中的资源。|'
- en: '| Service | Provides a named abstraction of software service consisting of
    a local port that the proxy listens on and the selector that determines which
    Pods will answer requests sent through the proxy. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Service | 提供一个名为抽象的软件服务，由代理监听的本地端口和确定哪些Pod将响应用户通过代理发送的请求的选择器组成。|'
- en: '| CustomResourceDefinition | Represents a resource that should be exposed on
    the API server. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CustomResourceDefinition | 表示应该在API服务器上公开的资源。|'
- en: '| DaemonSet | Used to deploy a container to all nodes, or a subset of nodes,
    in the cluster. This includes any new nodes that may be added after the initial
    deployment. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DaemonSet | 用于将容器部署到集群中的所有节点或节点子集。这包括在初始部署之后可能添加的任何新节点。|'
- en: '| Deployment | Enables declarative updates for Pods and ReplicaSets. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Deployment | 允许对Pod和ReplicaSet进行声明性更新。|'
- en: '| ReplicaSet | Ensures that a specified number of Pod replicas are running
    at any given time. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ReplicaSet | 确保在任何给定时间都有指定数量的Pod副本正在运行。|'
- en: '| StatefulSet | StatefulSet represents a set of Pods with consistent identities
    and controlled Pod starting and stopping. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| StatefulSet | StatefulSet代表一组具有一致标识和控制Pod启动和停止的Pod。|'
- en: '| Ingress | A collection of rules that direct inbound connections to reach
    the Pod endpoints. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Ingress | 一组规则，用于将入站连接定向到Pod端点。|'
- en: '| NetworkPolicy | Defines what network traffic is allowed for a set of Pods.
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| NetworkPolicy | 定义了一组Pod允许的网络流量。|'
- en: '| PodSecurityPolicy | Controls the ability to make requests that affect the
    security context that will be applied to a Pod and container. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| PodSecurityPolicy | 控制对影响Pod和容器将应用的安全上下文产生影响的能力的请求。|'
- en: '| ClusterRole | A cluster-level, logical grouping of PolicyRules that can be
    referenced as a unit by a RoleBinding or ClusterRoleBinding. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| ClusterRole | 集群级别的PolicyRules逻辑分组，可以作为一个单元由RoleBinding或ClusterRoleBinding引用。|'
- en: '| ClusterRoleBinding | Assigns the permissions defined in a ClusterRole to
    a user, group, or service account. The scope of a ClusterRoleBinding is cluster
    wide. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| ClusterRoleBinding | 将ClusterRole中定义的权限分配给用户、组或服务账户。ClusterRoleBinding的作用域是集群范围的。|'
- en: '| Role | A namespaced, logical grouping of PolicyRules that can be referenced
    as a unit by a RoleBinding. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Role | PolicyRules的逻辑分组，可以作为一个单元由RoleBinding引用。|'
- en: '| RoleBinding | Assigns the permissions defined in a Role to a user, group,
    or service account. It can reference a Role in the same namespace or a ClusterRole
    in the global namespace.The scope of a RoleBinding is only to the namespace it
    is defined in. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| RoleBinding | 将Role中定义的权限分配给用户、组或服务账户。它可以引用同一命名空间中的Role或全局命名空间中的ClusterRole。RoleBinding的作用域仅限于其定义的命名空间。|'
- en: '| StorageClass | Describes the parameters for a class of storage for which
    PersistentVolumes can be dynamically provisioned. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| StorageClass | 描述可以动态预配 PersistentVolumes 的存储类别的参数。|'
- en: Understanding the resources available is one of the keys to creating the best
    application deployments and to helping troubleshoot cluster or deployment problems.
    Without an understanding of these resources, you may not know what to look at
    if an Ingress rule isn’t working as expected. Using the resources in the table,
    you can find three resources that are required for an Ingress rule. The first
    is the Ingress itself, the second is the Service, and the last is the Endpoints/EndpointSlices.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 了解可用的资源是创建最佳应用程序部署和帮助解决集群或部署问题的关键之一。如果没有对这些资源的了解，如果 Ingress 规则没有按预期工作，你可能不知道该查看什么。使用表中的资源，你可以找到三个对于
    Ingress 规则所必需的资源。第一个是 Ingress 本身，第二个是服务，最后一个是端点/端点切片。
- en: Looking at the flow between resources for Ingress, an incoming request is evaluated
    by the Ingress controller, and a matching Ingress resource is found. Ingress rules
    route traffic based on the Service name defined in the Ingress rule, and, finally,
    the request is sent to a Pod from the Endpoints created by the Service.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 Ingress 资源之间的流程，一个进入的请求会被 Ingress 控制器评估，并找到匹配的 Ingress 资源。Ingress 规则根据 Ingress
    规则中定义的服务名称路由流量，最后请求被发送到由服务创建的端点对应的 Pod。
- en: 3.2.6 Kubernetes resources in depth
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.6 Kubernetes 资源深入探讨
- en: A brief overview of resources and what they are used for is a great refresher,
    if you already have experience with resources. We realize that not every reader
    will have years of experience interacting with Kubernetes resources, so in this
    section, you will find additional details on some of the most commonly used cluster
    resources.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对资源已经有了一定的了解，对资源和它们用途的简要概述是一个很好的复习。我们意识到并非每位读者都有与 Kubernetes 资源互动多年的经验，因此在本节中，你将找到一些最常用集群资源的额外细节。
- en: One thing that all GKE Kubernetes clusters have in common, on-prem or off-prem,
    is that they are built on the upstream Kubernetes code, and they all contain the
    base set of Kubernetes resources. Interacting with these base types is something
    you are likely to do daily, and having a strong understanding of each component,
    its function, and use case examples is important.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 GKE Kubernetes 集群共有的一个特点是，无论是在本地还是远程，它们都是基于上游 Kubernetes 代码构建的，并且它们都包含 Kubernetes
    资源的基本集合。与这些基本类型交互是你可能每天都会做的事情，对每个组件、其功能和用例示例有深入理解是很重要的。
- en: Namespaces
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间
- en: Namespaces provide a scope for names. Names of resources need to be unique within
    a namespace, but not across namespaces.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间为名称提供范围。资源名称需要在命名空间内是唯一的，但不能跨命名空间。
- en: Namespaces create a logical separation between tenants in the cluster, providing
    a cluster with multitenancy. As defined by Gartner, “Multitenancy is a reference
    to the mode of operation of software where multiple independent instances of one
    or multiple applications operate in a shared environment. The instances (tenants)
    are logically isolated, but physically integrated” ([http://mng.bz/Kl74](http://mng.bz/Kl74)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间在集群中的租户之间创建逻辑分离，为集群提供多租户功能。根据 Gartner 的定义，“多租户是指软件操作的模式，其中一个或多个应用程序的多个独立实例在共享环境中运行。实例（租户）在逻辑上是隔离的，但在物理上是集成的”
    ([http://mng.bz/Kl74](http://mng.bz/Kl74))。
- en: Kubernetes resources that are created at a namespace level are referred to as
    being *namespaced*. If you read that a resource is namespaced, it means the resource
    is managed at a namespace level, rather than at a cluster level.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在命名空间级别创建的 Kubernetes 资源被称为 *namespaced*。如果你看到某个资源是 namespaced 的，这意味着该资源是在命名空间级别而不是集群级别进行管理的。
- en: 'In a namespace, you can create resources that will provide security and resource
    limits. To provide a safe multitenant cluster, you can use the following categories
    of Kubernetes resources:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在命名空间中，你可以创建提供安全和资源限制的资源。为了提供一个安全的多租户集群，你可以使用以下类别的 Kubernetes 资源：
- en: RBAC
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBAC
- en: Resource quotas
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源配额
- en: Network policies
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略
- en: Namespace security resources (previously Pod security policies)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间安全资源（以前称为 Pod 安全策略）
- en: We will discuss each of the resources in more detail in this section, but for
    now, you need to understand only that a namespace is a logical partition of a
    cluster.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中更详细地讨论每个资源，但就目前而言，你需要理解的是，命名空间是集群的逻辑分区。
- en: 'Namespaces are also used when you create a service, which we will cover in
    the services section. The service is assigned a DNS name that includes the service
    name and the namespace. For example, if you created two services called myweb1
    and myweb2 in a namespace called sales, in a cluster named cluster.local, the
    assigned DNS names would be as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建服务时，也会使用命名空间，我们将在服务部分中介绍。服务被分配一个DNS名称，包括服务名称和命名空间。例如，如果你在名为sales的命名空间中创建名为myweb1和myweb2的两个服务，在一个名为cluster.local的集群中，分配的DNS名称如下：
- en: myweb1.sales.svc.cluster.local
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: myweb1.sales.svc.cluster.local
- en: myweb2.sales.svc.cluster.local
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: myweb2.sales.svc.cluster.local
- en: Pods
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Pods
- en: A Pod is the smallest deployable unit that Kubernetes can manage and may contain
    one or more containers. If a Pod has multiple containers running, they all share
    a common networking stack, allowing each container to communicate with the other
    containers in the Pod using localhost or 127.0.0.1\. They also share any volumes
    that are mounted to the Pod, allowing each container access to a shared file location.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Pod是Kubernetes可以管理的最小部署单元，可能包含一个或多个容器。如果一个Pod运行多个容器，它们都共享一个公共的网络堆栈，允许每个容器使用localhost或127.0.0.1与Pod中的其他容器通信。它们还共享挂载到Pod的任何卷，允许每个容器访问共享的文件位置。
- en: When a pod is created, it is assigned an IP address, and the assigned address
    should be considered ephemeral. You should never target the IP address of a Pod
    because it will likely change at some point when the Pod is replaced. To target
    an application that is running in a Pod, you should target a service name, which
    will use endpoints to direct traffic to the correct Pod where the application
    is running. We will discuss endpoints and services in their respective topics
    in this section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod创建时，它会分配一个IP地址，分配的地址应被视为短暂的。你不应该针对Pod的IP地址，因为它在Pod被替换时可能会在某个时刻发生变化。要针对在Pod中运行的应用程序，你应该针对服务名称，这将使用端点将流量导向运行应用程序的正确Pod。我们将在本节的相关主题中讨论端点和服务。
- en: Although no standard exists for how many containers should be in a single Pod,
    the best practice is to add containers that should be scheduled and managed together.
    Actions such as scaling and Pod restarts should be considered when deciding to
    add multiple containers to a Pod. Events like these are handled at a Pod level,
    not at a container level, so these actions will affect all containers in the Pod.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有关于单个Pod中应该有多少容器的标准，但最佳实践是添加应该一起调度和管理的容器。在决定将多个容器添加到Pod时，应考虑扩展和Pod重启等操作。这些事件在Pod级别处理，而不是在容器级别，因此这些操作将影响Pod中的所有容器。
- en: Example
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: You create a Pod with a web server and a database. You decide that you need
    to scale the web server to handle the current traffic load. When you scale the
    Pod, it will scale *both* the web server and the database server.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建一个包含Web服务器和数据库的Pod。你决定你需要扩展Web服务器以处理当前的流量负载。当你扩展Pod时，它将扩展*两者*：Web服务器和数据库服务器。
- en: To scale only the web server, you should deploy a Pod with the web server and
    a second Pod with the database server, which will allow you to scale each application
    independently.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只想扩展Web服务器，你应该部署一个包含Web服务器的Pod和一个包含数据库服务器的第二个Pod，这将允许你独立扩展每个应用程序。
- en: 'Many design patterns use multiple containers in a Pod. A common use case for
    multiple containers in a Pod is referred to as a *sidecar*. A sidecar is a container
    that runs with the main container in your Pod, usually to add some functionality
    to the main container without requiring any changes to it. Some common examples
    that use sidecars to handle tasks follow:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 许多设计模式在Pod中使用多个容器。Pod中多个容器的常见用例被称为*边车*。边车是与Pod中的主容器一起运行的容器，通常用于在不修改主容器的情况下添加一些功能。以下是一些使用边车处理任务的常见示例：
- en: Logging
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录
- en: Monitoring
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控
- en: Istio sidecar
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio边车
- en: Backup sidecar (i.e., Veritas NetBackup)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份边车（即Veritas NetBackup）
- en: You can look at other examples on the Kubernetes site at [http://mng.bz/91Ja](http://mng.bz/91Ja).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kubernetes网站上查看其他示例，网址为[http://mng.bz/91Ja](http://mng.bz/91Ja)。
- en: Understanding Pods is a key point to understanding Kubernetes deployments. They
    will be the most common resource that you will interact with.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Pod是理解Kubernetes部署的关键点。它们将成为你将与之交互的最常见资源。
- en: Labels and selectors
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 标签和选择器
- en: Kubernetes uses labels to identify, organize, and link resources, allowing you
    to identify attributes. When you create a resource in Kubernetes, you can supply
    one or more key-value pair labels like app:frontend-webserver or lob=sales.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用标签来识别、组织和链接资源，允许你识别属性。当你创建Kubernetes中的资源时，你可以提供一个或多个键值对标签，如app:frontend-webserver或lob=sales。
- en: Selectors are used to reference a set of resources, allowing you to select the
    resource(s) you want to link, or select, using the assigned labels. You can think
    of selectors as a dynamic grouping mechanism—any label that matches the selector
    will be added as a target. This will be shown in the next section covering the
    services resource, which uses selectors to link the service to the Pods running
    the application.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器用于引用一组资源，允许你选择你想要链接或选择的资源（资源集），使用分配的标签。你可以将选择器视为一种动态分组机制——任何匹配选择器的标签都将被添加为目标。这将在下一节中展示，该节将介绍使用选择器将服务链接到运行应用程序的Pod的服务资源。
- en: Services
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 服务
- en: We can use many of the previous resources to provide a full picture of how they
    connect to create an application. The last piece of the puzzle is the Service
    resource, which exposes an application to allow it to accept requests using a
    defined DNS name.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多之前提到的资源来全面了解它们如何连接以创建一个应用程序。拼图中的最后一部分是服务资源，它将应用程序暴露出来，允许它使用定义的DNS名称接受请求。
- en: Remember that when you create a Pod with your application, it is assigned an
    IP address. This IP address will change when the Pod is replaced, which is why
    you never want to configure a connection to the Pods using an IP address.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当你创建一个包含你的应用程序的Pod时，它会被分配一个IP地址。当Pod被替换时，这个IP地址将会改变，这就是为什么你永远不希望使用IP地址来配置连接到Pod的原因。
- en: Unlike Pods, which are ephemeral by nature, a Service is stable once created
    and is rarely deleted and recreated, providing a stable IP address and DNS name.
    Even if a Service is deleted and recreated, the DNS name will remain the same,
    providing a stable name that you can target to access the application. You can
    create a few Service types in Kubernetes, as shown in table 3.5.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与本质上短暂的Pod不同，一旦创建，服务就是稳定的，很少被删除和重新创建，提供稳定的IP地址和DNS名称。即使服务被删除和重新创建，DNS名称也将保持不变，提供一个稳定的名称，你可以将其作为目标来访问应用程序。在Kubernetes中，你可以创建几种服务类型，如表3.5所示。
- en: Table 3.5 Services in Kubernetes
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.5 Kubernetes中的服务
- en: '| Service name | Description | Network scope |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 服务名称 | 描述 | 网络范围 |'
- en: '| ClusterIP | Exposes the service internally to the cluster. | InternalExternal
    by using an Ingress rule |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 集群IP | 在集群内部暴露服务。 | 通过使用入口规则内部和外部暴露 |'
- en: '| NodePort | Exposes the service internally to the cluster.Exposes the service
    to external clients using the assigned NodePort. Using the NodePort with any worker
    node DNS/IP address will provide a connection to the Pod(s). | Internal and external
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 节点端口 | 在集群内部暴露服务。使用分配的节点端口向外部客户端暴露服务。使用任何工作节点DNS/IP地址与节点端口结合将提供对Pod（s）的连接。
    | 内部和外部 |'
- en: '| LoadBalancer | Exposes the service internally to the cluster.Exposes the
    service externally to the cluster using an external load-balancer service. | Internal
    and external |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 负载均衡器 | 在集群内部暴露服务。使用外部负载均衡器服务在集群外部暴露服务。 | 内部和外部 |'
- en: 'Now let’s use an example to explain how Kubernetes uses services to expose
    an application in a namespace called sales in a cluster using the name cluster.local:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用一个例子来解释Kubernetes如何使用服务在一个名为sales的命名空间和一个名为cluster.local的集群中暴露一个应用程序：
- en: A deployment is created for an NGINX server.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为NGINX服务器创建了一个部署。
- en: The deployment name is nginx-frontend.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的名称是nginx-frontend。
- en: 'The deployment has been labeled with app: frontend-web.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '部署已被标记为app: frontend-web。'
- en: Three replicas have been created.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已创建了三个副本。
- en: The three running Pods have been assigned the IP addresses 192.10.1.105, 192.10.3.107,
    and 192.10.4.108.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三个运行中的Pod已经被分配了IP地址192.10.1.105、192.10.3.107和192.10.4.108。
- en: 'To provide access to the server, a new service is deployed called frontend-web.
    In the manifest to create the service, a *label* *selector* is used to select
    any Pods that match app: frontend-web.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '为了提供对服务器的访问，部署了一个名为frontend-web的新服务。在创建服务的清单中，使用了一个*标签* *选择器*来选择任何匹配app: frontend-web的Pod。'
- en: Kubernetes will use the service request and the selector to create matching
    endpoints.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes将使用服务请求和选择器来创建匹配的端点。
- en: 'Because the selector matches the label that was used in the deployment for
    the NGINX server, Kubernetes will create an endpoint that links to the three Pod
    IPs: 192.10.1.105, 192.10.3.107, and 192.10.4.108.'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为选择器匹配了用于 NGINX 服务器的部署中使用的标签，Kubernetes 将创建一个链接到三个 Pod IP（192.10.1.105、192.10.3.107
    和 192.10.4.108）的端点。
- en: The service will receive an IP address from the cluster’s Service IP pool, and
    a DNS name that is created using the <service name>.<namespace>.svc.<cluster domain>.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务将从集群的 Service IP 池中接收一个 IP 地址，以及使用 <service name>.<namespace>.svc.<cluster
    domain> 创建的 DNS 名称。
- en: Because the application name is nginx-frontend, the DNS name will be nginx-frontend.sales.svc.cluster.local.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为应用名称是 nginx-frontend，DNS 名称将是 nginx-frontend.sales.svc.cluster.local。
- en: If any of the Pod IPs change due to a restart, the endpoints will be updated
    by the kube-controller-manager, providing you a stable endpoint to the Pods, even
    when a Pod IP address changes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何 Pod IP 由于重启而更改，kube-controller-manager 将更新端点，为您提供稳定的端点访问 Pods，即使 Pod IP
    地址发生变化。
- en: EndpointSlices
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: EndpointSlices
- en: EndpointSlices map Kubernetes services to Pod(s) that are running the application,
    linked by matching labels between the service selector and the Pod(s) with a matching
    label. A graphical representation is shown in figure 3.7.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: EndpointSlices 将 Kubernetes 服务映射到运行应用程序的 Pod(s)，通过服务选择器和具有匹配标签的 Pod(s)之间的匹配标签进行链接。图
    3.7 展示了其图形表示。
- en: '![03-07](../../OEBPS/Images/03-07.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![03-07](../../OEBPS/Images/03-07.png)'
- en: Figure 3.7 Kubernetes endpoints
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 Kubernetes 端点
- en: In this figure, a service named nginx-service has been created in a namespace.
    The service is using a selector for the key app, equal to the value nginx-frontend.
    Using the selector, Kubernetes will look for any matching labels in the namespace
    equal to app=nginx-frontend. The namespace has three running Pods, and two of
    the Pods have been labeled with app=nginx-frontend. Because the selector matches,
    all matching Pod IP addresses are added to the EndpointSlices.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，在命名空间中创建了一个名为 nginx-service 的服务。该服务使用 key app 的选择器，其值等于 nginx-frontend。使用选择器，Kubernetes
    将在命名空间中查找任何匹配的标签，标签等于 app=nginx-frontend。该命名空间有三个正在运行的 Pod，其中两个 Pod 已标记为 app=nginx-frontend。因为选择器匹配，所有匹配的
    Pod IP 地址都被添加到 EndpointSlices。
- en: Annotations
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Annotations
- en: Annotations may look similar to selectors at a first glance. They are key-value
    pairs, just like labels are, but unlike labels, they are not used by selectors
    to create a collection of services.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Annotations 在第一眼看起来可能类似于选择器。它们是键值对，就像标签一样，但与标签不同，它们不是由选择器用来创建服务集合的。
- en: You can use annotations to create records in a resource, like Git branch information,
    image hashes, support contacts, and more.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 annotations 在资源中创建记录，如 Git 分支信息、图像散列、支持联系人等。
- en: ConfigMaps
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMaps
- en: ConfigMaps are used to store application information that is not confidential,
    separate from the container image. Although you could store a configuration directly
    in your container image, it would make your deployment too rigid—any configuration
    change would require you to create your new image. This would lead to maintaining
    multiple images, one for each configuration.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMaps 用于存储非机密的应用信息，与容器镜像分离。虽然您可以直接在容器镜像中存储配置，但这会使您的部署变得过于僵化——任何配置更改都需要您创建新的镜像。这将导致维护多个镜像，每个配置一个。
- en: A better method would be to store the configuration in a ConfigMap that is read
    in by your Pod when it is started. ConfigMaps can be mounted as a file in the
    container or as environment variables, depending on the application requirements.
    Deploying the image with a different configuration requires only a different ConfigMap,
    rather than an entire image build.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是将配置存储在 ConfigMap 中，当 Pod 启动时读取。根据应用需求，ConfigMaps 可以作为文件挂载到容器中，或作为环境变量。使用不同配置部署镜像只需要不同的
    ConfigMap，而不是整个镜像构建。
- en: For example, imagine you have a web server image that requires a different configuration
    based on deployment location. You want to use the same image across your entire
    organization, regardless of the location where the container will run. To accomplish
    this, you create a web container image that is configured to use a ConfigMap for
    the web server configuration. By using an external configuration, you are making
    your image portable by allowing a configuration outside of the container itself.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个需要根据部署位置不同配置不同的Web服务器镜像。你希望在容器运行的位置无关的情况下在整个组织中使用相同的镜像。为了实现这一点，你创建了一个配置为使用ConfigMap进行Web服务器配置的Web容器镜像。通过使用外部配置，你通过允许容器之外进行配置，使你的镜像具有可移植性。
- en: Secrets
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 机密
- en: Secrets are like ConfigMaps because they contain external information that will
    be used by Pods. Unlike ConfigMaps, secrets are not stored in cleartext; they
    are stored using Base64 encoding.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 机密就像ConfigMap一样，因为它们包含Pod将使用的外部信息。与ConfigMap不同，机密不是以明文形式存储的；它们是使用Base64编码存储的。
- en: If you have worked with Base64 encoding before, you are probably thinking that
    it’s not very different from, or more secure than, cleartext—and you would be
    right. Secrets in Kubernetes do not use Base64 encoding to hide the secret; they
    are Base64 encoded to allow secrets to store binary information. If a person has
    access to view the secret, it is trivial to decode the information. Because of
    this, it is suggested you encrypt your secrets using an external secret manager
    like Vault or Google Secret Manager.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过Base64编码，你可能认为它与明文没有太大区别，或者没有更安全——你是对的。Kubernetes中的机密不使用Base64编码来隐藏机密；它们是Base64编码的，以便机密可以存储二进制信息。如果有人有权查看机密，解码信息是微不足道的。因此，建议你使用像Vault或Google
    Secret Manager这样的外部机密管理器来加密你的机密。
- en: Note
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can also encrypt secrets when they are stored in etcd, but this encrypts
    the value only in the database, not in Kubernetes. If you enable this feature,
    you are protecting the secrets in the etcd database only. To secure your secrets,
    you should use both encryption methods because this will protect your secrets
    both in the cluster and in etcd.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在存储在etcd中的机密时对其进行加密，但这仅加密数据库中的值，而不是Kubernetes。如果你启用此功能，你只保护了etcd数据库中的机密。为了保护你的机密，你应该使用两种加密方法，因为这将保护你在集群和etcd中的机密。
- en: etcd was discussed in section 3.2.2, “The control plane components.”
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: etcd在第3.2.2节“控制平面组件”中进行了讨论。
- en: ResourceQuotas
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额
- en: Remember that namespaces are used to provide a logical separation for applications
    or teams. Because a cluster may be shared with multiple applications, we need
    to have a way to control any effect a single namespace may have on the other cluster
    resources. Luckily, Kubernetes includes ResourceQuotas to provide resource controls.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，命名空间用于为应用程序或团队提供逻辑分离。因为集群可能被多个应用程序共享，我们需要有一种方法来控制单个命名空间可能对其他集群资源产生的影响。幸运的是，Kubernetes包括资源配额来提供资源控制。
- en: 'A quota can be set on any standard Kubernetes resource that is namespaced;
    therefore, ResourceQuotas are set at a namespace level and control the resources
    that the namespace can consume, including the following:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在任何命名空间的标准Kubernetes资源上设置配额；因此，资源配额是在命名空间级别设置的，并控制命名空间可以消耗的资源，包括以下内容：
- en: CPU
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: Memory
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存
- en: Storage
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储
- en: Pods
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: Services
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: Quotas allow you to control the resources that a namespace can consume, allowing
    you to share a cluster with multiple namespaces while providing a “guarantee”
    to cluster resources.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 配额允许你控制命名空间可以消耗的资源，允许你与多个命名空间共享集群，同时为集群资源提供“保证”。
- en: RBAC
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC
- en: Role-based access control (RBAC), is used to control what users can do within
    a cluster. Roles are created and assigned permissions, which are then assigned
    to users or groups, providing permissions to the cluster.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基于角色的访问控制（RBAC）用于控制用户在集群内可以执行的操作。角色被创建并分配权限，然后这些权限被分配给用户或组，从而为集群提供权限。
- en: To provide RBAC, Kubernetes uses roles and binding resources. Roles are used
    to create a set of permissions to a resource or resources, whereas bindings are
    used to assign the permission set to a user or service.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供基于角色的访问控制（RBAC），Kubernetes使用角色和绑定资源。角色用于为资源或资源集创建一组权限，而绑定用于将权限集分配给用户或服务。
- en: Roles and ClusterRoles
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 角色和集群角色
- en: A Role creates a set of permissions for a resource or resources. Two different
    types of Roles in Kubernetes are used to define the scope of the permissions,
    as shown in table 3.6.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 角色为资源或资源集创建一组权限。Kubernetes中使用了两种不同类型的角色来定义权限的范围，如表3.6所示。
- en: Table 3.6 Roles used in Kubernetes
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.6 Kubernetes中使用的角色
- en: '| Role type | Scope | Description |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 角色类型 | 范围 | 描述 |'
- en: '| Role | Namespace | Permissions in a Role can be used only in the namespace
    in which it was created. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 角色 | 命名空间 | 角色中的权限只能在创建它的命名空间中使用。 |'
- en: '| ClusterRole | Cluster | Permissions in a ClusterRole can be used cluster
    wide. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 集群角色 | 集群 | 集群角色中的权限可以在集群范围内使用。 |'
- en: The scope of Roles can be confusing for people who are new to Kubernetes. The
    Role resource is more straightforward than the ClusterRole resource. When you
    create a Role, it must contain a namespace value, which creates the role in the
    assigned namespace. Because a Role exists only in the namespace, it can be used
    to assign permissions only in the namespace itself—it cannot be used anywhere
    else in the cluster.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes新手来说，角色的范围可能会令人困惑。Role资源比ClusterRole资源更直接。当您创建一个Role时，它必须包含一个命名空间值，这将在指定的命名空间中创建角色。因为角色仅存在于命名空间中，所以它只能用于在自身命名空间中分配权限——它不能用于集群中的其他任何地方。
- en: A ClusterRole is created at the cluster level and can be used anywhere in the
    cluster to assign permissions. When assigned to the cluster level, the permissions
    that are granted in the ClusterRole will be assigned to all defined resources
    in the cluster. However, if you use a ClusterRole at a namespace level, the permissions
    will be available only in the assigned namespace.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterRole在集群级别创建，可以在集群的任何地方用于分配权限。当分配到集群级别时，ClusterRole中授予的权限将被分配给集群中所有定义的资源。然而，如果您在命名空间级别使用ClusterRole，权限将仅在指定的命名空间中可用。
- en: Two of the most common ClusterRoles are the built-in admin and view. By themselves,
    Roles and ClusterRoles do not assign a set of permissions to any user. To assign
    a Role to a user, you need to bind the Role using a RoleBinding or a ClusterRoleBinding.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最常见的ClusterRole是内置的admin和view。Role和ClusterRole本身并不将一组权限分配给任何用户。要将Role分配给用户，您需要使用RoleBinding或ClusterRoleBinding绑定Role。
- en: RoleBinding and ClusterRoleBinding
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 角色绑定和集群角色绑定
- en: Roles simply define the set of permissions that will be allowed for resources;
    they do not assign the granted permissions to any user or service. To grant the
    permissions defined in a role, you need to create a binding.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 角色仅定义了允许资源使用的权限集；它们不会将授予的权限分配给任何用户或服务。要授予角色中定义的权限，您需要创建一个绑定。
- en: Similar to Roles and ClusterRoles, bindings have two scopes, as described in
    table 3.7.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 与角色和ClusterRole类似，绑定有两个范围，如表3.7中所述。
- en: Table 3.7 Binding types and their scopes
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.7 绑定类型及其范围
- en: '| Binding type | Scope | Description |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 绑定类型 | 范围 | 描述 |'
- en: '| RoleBinding | Namespace | Can be used to assign permissions only in the namespace
    in which it was created |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 角色绑定 | 命名空间 | 可以用于仅在其创建的命名空间中分配权限 |'
- en: '| ClusterRoleBinding | Cluster | Can be used to assign permissions cluster
    wide |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 集群角色绑定 | 集群 | 可以用于在集群范围内分配权限 |'
- en: Now that we have discussed Kubernetes resources, let’s move on to how you can
    control where a Pod will be scheduled to run.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了Kubernetes资源，让我们继续讨论如何控制Pod将被调度到何处运行。
- en: 3.2.7 Controlling Pod scheduling
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.7 控制Pod调度
- en: In this section, we will explain how you can control where a workload is placed
    using features like node labels, affinity/anti-affinity rules, selectors, taints,
    and tolerations.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何使用节点标签、亲和/反亲和规则、选择器、污点和容忍等特性来控制工作负载放置的位置。
- en: 'As Kubernetes has gained popularity, the use cases have grown and become more
    complex. You may run into deployments that requires special scheduling, such as
    the following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Kubernetes的流行，用例已经增长并变得更加复杂。您可能会遇到需要特殊调度的部署，如下所示：
- en: A Pod that requires a GPU or other specialized hardware
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要GPU或其他专用硬件的Pod
- en: Forcing Pods to run on the same node
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制Pod在同一个节点上运行
- en: Forcing Pods to run on different nodes
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制Pod在不同的节点上运行
- en: Specific local storage requirements
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定的本地存储需求
- en: Using locally installed NVMe drives
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本地安装的NVMe驱动器
- en: If you simply deploy a manifest to your cluster, the scheduler does not take
    any “special” considerations into account when selecting a node. If you deployed
    a Pod that required CUDA and the Pod was scheduled to run on a node that did not
    have a GPU, the application would fail to start because the required hardware
    would not be available to the application.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是将清单部署到你的集群中，调度器在选择节点时不会考虑任何“特殊”的考虑因素。如果你部署了一个需要CUDA的Pod，并且Pod被调度到一个没有GPU的节点上，应用程序将无法启动，因为所需的硬件将不可用给应用程序。
- en: Kubernetes provides the ability to force a Pod to run on a particular node,
    or set of nodes, using advanced scheduling options that are set at the node level
    and in your deployment. At the node level, we use node labels and node taints
    to group nodes, and at the deployment level, we use node selectors, affinity/anti-affinity
    rules, and taints/tolerations to decide on Pod placement.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了使用在节点级别和你的部署中设置的先进调度选项来强制Pod在特定节点或节点集上运行的能力。在节点级别，我们使用节点标签和节点污点来分组节点，在部署级别，我们使用节点选择器、亲和力/反亲和力规则以及污点/容忍度来决定Pod的放置。
- en: Using node labels and taints
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点标签和污点
- en: At the node level, you can use two methods to control the Pods that will be
    scheduled on the node or nodes. The first method is by labeling the node, and
    the second is by tainting the node. Although both methods allow you to control
    whether a Pod will be scheduled on the node, they have different use cases—either
    attracting the Pods or repelling them.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点级别，你可以使用两种方法来控制将在节点或节点上调度的Pod。第一种方法是标记节点，第二种是污点节点。尽管这两种方法都允许你控制Pod是否将在节点上调度，但它们有不同的使用场景——要么吸引Pod，要么排斥Pod。
- en: Attracting versus repelling
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 吸引与排斥
- en: You can use labels to group a set of nodes to target in your deployment, forcing
    the Pod(s) to run on that particular set of nodes. When you label a node, you
    are not rejecting any workloads. A label is an optional value that can be used
    by a deployment, if a value is set in the deployment to use a label. In this way,
    you are setting an attraction for Pods that may have a requirement that a label
    will provide, for example, gpu=true.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用标签来分组你部署中要针对的一组节点，强制Pod在这些特定的节点上运行。当你标记一个节点时，你并没有拒绝任何工作负载。标签是一个可选值，如果部署中设置了使用标签的值，则可以由部署使用。这样，你为可能需要标签提供的某些要求的Pod设置了一个吸引力，例如，gpu=true。
- en: 'To label a node using kubectl, you use the following label option:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用kubectl标记节点，你使用以下标签选项：
- en: '[PRE4]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If a deployment requires a GPU, it uses a selector that tells the scheduler
    that it needs to be scheduled on a node with a label gpu=true. The scheduler will
    look for nodes with a matching label and then schedule the Pod to run on one of
    the nodes with a matching label. If a matching label cannot be found, the Pod
    will fail to be scheduled and will not start.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个部署需要GPU，它将使用一个选择器告诉调度器它需要在具有标签gpu=true的节点上调度。调度器将寻找具有匹配标签的节点，然后将Pod调度到具有匹配标签的节点之一上。如果找不到匹配的标签，Pod将无法被调度，并且不会启动。
- en: Using a label is completely optional. Using the previous example label, if you
    create a deployment that does not select the gpu=true label, your Pod will not
    be excluded from nodes that contain the label.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签是完全可选的。使用之前的示例标签，如果你创建的部署没有选择gpu=true标签，你的Pod不会被排除在包含该标签的节点之外。
- en: 'Taints work differently: rather than creating a key value that invites the
    Pods to be run on it, you use a taint to repel any scheduling request that cannot
    tolerate the value set by the taint. To create a taint, you need to supply a key
    value and an effect, which controls whether a Pod is scheduled. For example, if
    you wanted to control the nodes that have a GPU, you could set a taint on a node
    using kubectl like this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 污点（Taints）的工作方式不同：而不是创建一个邀请Pod在其上运行的键值，你使用污点来排斥任何无法容忍污点设置的值的调度请求。要创建一个污点，你需要提供一个键值和一个效果，这控制着Pod是否被调度。例如，如果你想控制具有GPU的节点，你可以使用kubectl在节点上设置一个污点，如下所示：
- en: '[PRE5]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This would taint node1 with the key-value of gpu=true and the effect of NoSchedule,
    which tells the scheduler to repel all scheduling requests that do not contain
    a toleration of gpu=true. Unlike a label, which would allow Pods that do not specify
    a label to be scheduled, a taint setting with the effect NoSchedule will deny
    any Pod that does not “tolerate” gpu=true to be scheduled.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用键值对 gpu=true 和效果 NoSchedule 污点节点1，这告诉调度器排斥所有不包含对 gpu=true 的容忍的调度请求。与标签不同，标签会允许未指定标签的
    Pods 被调度，而具有 NoSchedule 效果的污点设置将拒绝任何不“容忍”gpu=true的 Pod 被调度。
- en: 'Taints have three effects that can be set: NoSchedule, PreferNoSchedule, and
    NoExecute. Each one sets a control on how the taint will be applied:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 污点（Taints）有三个可以设置的效果：NoSchedule、PreferNoSchedule 和 NoExecute。每个效果都设置了对污点如何应用的控制：
- en: NoSchedule—This is a “hard” setting that will deny any scheduling request that
    does not tolerate the taint.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSchedule—这是一个“硬”设置，将拒绝任何不耐受污点的调度请求。
- en: PreferNoSchedule—This is a “soft” setting that will attempt to avoid scheduling
    a Pod that does not tolerate the taint.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PreferNoSchedule—这是一个“软”设置，将尝试避免调度一个不耐受污点的 Pod。
- en: NoExecute—This affects already running Pods on a node; it is not used for scheduling
    a Pod.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoExecute—这影响节点上已经运行的 Pods；它不用于调度 Pod。
- en: Now that we have explained how to create labels and taints on nodes, we need
    to understand how a deployment is configured to control Pod placement.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了如何在节点上创建标签和污点，我们需要了解部署是如何配置来控制 Pod 放置的。
- en: Using node nodeSelectors
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点选择器
- en: When a Pod is created, you can add a nodeSelector to your manifest to control
    the node on which the Pod will be scheduled. By using any label that is assigned
    to a node, you can force the scheduler to schedule the Pod on a node or a set
    of nodes.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建 Pod 时，你可以在你的清单中添加节点选择器来控制 Pod 将被调度到的节点。通过使用分配给节点的任何标签，你可以强制调度器在某个节点或一组节点上调度
    Pod。
- en: 'You may not know all the labels available on a cluster. If you have access,
    you can use kubectl to get a list of the nodes and all labels by using the get
    nodes command with the —show-labels option as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不知道集群上所有可用的标签。如果你有权限，可以使用 kubectl 通过带有 —show-labels 选项的 get nodes 命令来获取节点和所有标签的列表，如下所示：
- en: '[PRE6]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will list each node and the labels that have been assigned, as shown here:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出每个节点及其分配的标签，如下所示：
- en: '![03-07-unnumb](../../OEBPS/Images/03-07-unnumb.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![03-07-unnumb](../../OEBPS/Images/03-07-unnumb.png)'
- en: You can also see the node labels in the GCP console, as shown in figure 3.8,
    by clicking Details for a node.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在 GCP 控制台中看到节点标签，如图 3.8 所示，通过点击节点的详细信息。
- en: '![03-08](../../OEBPS/Images/03-08.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![03-08](../../OEBPS/Images/03-08.png)'
- en: Figure 3.8 GCP node console view
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 GCP 节点控制台视图
- en: 'Using a label from the cluster in the images, we can create a manifest that
    will deploy NGINX on the third node by using a nodeSelector, as shown next:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在镜像中使用集群中的标签，我们可以创建一个清单，通过使用节点选择器在第三个节点上部署 NGINX，如下所示：
- en: '[PRE7]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the value kubernetes.io/hostname:gke-cluster-1-default-pool-ead436da-8j7k
    in a nodeSelector, we forced the Pod to run the third node in the cluster. To
    verify the Pod did schedule on the correct node, we can use kubectl to get the
    Pods using the -o wide option, like this and as shown in figure 3.9:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点选择器中的值 kubernetes.io/hostname:gke-cluster-1-default-pool-ead436da-8j7k，我们强制
    Pod 在集群的第三个节点上运行。为了验证 Pod 是否在正确的节点上调度，我们可以使用 kubectl 使用 -o wide 选项获取 Pods，如下所示，如图
    3.9 所示：
- en: '[PRE8]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![03-09](../../OEBPS/Images/03-09.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![03-09](../../OEBPS/Images/03-09.png)'
- en: Figure 3.9 Getting Pods with wide output
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 获取具有宽输出的 Pods
- en: The nodeSelector option allows you to use any label to control what nodes will
    be used to schedule your Pods. If the nodeSelector value does not match any nodes,
    the Pod will fail to schedule and will remain in a pending state until it is deleted
    or a label is updated on a node that matches the selector. In the next example,
    shown in figure 3.10, we tried to force a deployment to a nodeSelector that had
    a value of a host that does not exist in the cluster. First, we can look at all
    the Pods to check the status using kubectl get pods.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 节点选择器选项允许你使用任何标签来控制将用于调度你的 Pods 的节点。如果节点选择器值与任何节点不匹配，Pod 将无法调度，并将保持挂起状态，直到它被删除或在节点上更新匹配选择器的标签。在下一个示例中，如图
    3.10 所示，我们尝试将部署强制到具有集群中不存在的主机值的节点选择器。首先，我们可以使用 kubectl get pods 查看所有 Pods 的状态来检查状态。
- en: '![03-10](../../OEBPS/Images/03-10.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![03-10](../../OEBPS/Images/03-10.png)'
- en: Figure 3.10 get pods output
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 get pods 输出
- en: 'Notice that the nginx-test2 Pod is in a pending state. The next step in checking
    why the Pod fails to start is to describe the Pod:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，nginx-test2 Pod 处于挂起状态。检查 Pod 为什么无法启动的下一步是描述 Pod：
- en: '[PRE9]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A description of the pod will be displayed, as illustrated in figure 3.11, including
    the current status at the bottom of the output.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 将显示 Pod 的描述，如图 3.11 所示，包括输出底部的当前状态。
- en: '![03-11](../../OEBPS/Images/03-11.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![03-11](../../OEBPS/Images/03-11.png)'
- en: Figure 3.11 kubectl describe output
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 kubectl describe 输出
- en: 'In the message area, the status shows 0/3 nodes are available: 3 node(s) didn’t
    match node selector. Because our nodeSelector did not match any existing label,
    the Pod failed to start. To resolve this, you should verify that the nodeSelector
    is correct and, if it is, verify that a node has the same label set.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息区域，状态显示 0/3 个节点可用：3 个节点未匹配节点选择器。因为我们的节点选择器没有匹配任何现有标签，Pod 无法启动。要解决这个问题，您应验证节点选择器是否正确，如果正确，请验证是否有节点具有相同的标签集。
- en: Using affinity rules
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用亲和规则
- en: 'Node affinity is another way to control which node your Pods will run on. Unlike
    a nodeSelector, an affinity rule can do the following:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性是控制 Pod 在哪个节点上运行的另一种方式。与节点选择器不同，亲和规则可以执行以下操作：
- en: Contain additional syntax beyond a simple matching label.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含比简单的匹配标签更复杂的语法。
- en: Schedule based on an affinity rule match, but if a match is not found, the Pod
    will schedule on any node.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据亲和规则匹配进行调度，但如果找不到匹配项，Pod 将在任何节点上调度。
- en: Unlike a nodeSelector, which has a single value, a node affinity rule can contain
    operators, allowing for more complex selections. Table 3.8 contains a list of
    operators and a description of how they are evaluated.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 与只有一个值的节点选择器不同，节点亲和规则可以包含操作符，允许进行更复杂的选择。表 3.8 包含操作符列表及其评估描述。
- en: Table 3.8 Operators and their descriptions
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.8 操作符及其描述
- en: '| Operator | Description |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 操作符 | 描述 |'
- en: '| In | Checks the label against a list. If any value is in the list, it is
    considered a match. |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| In | 检查标签是否在列表中。如果列表中有任何值，则视为匹配。|'
- en: '| NotIn | Checks the label against a list, and if the value is not in the list,
    it is considered a match. |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| NotIn | 检查标签是否在列表中，如果值不在列表中，则视为匹配。|'
- en: '| Exists | Checks whether the label exists; if it does, it is considered a
    match.Note: The value of the label does not matter and is not evaluated in the
    match. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Exists | 检查标签是否存在；如果存在，则视为匹配。注意：标签的值无关紧要，且在匹配中不进行评估。|'
- en: '| DoesNotExist | Checks whether the label exists; if the label does not match
    any in the list, it is considered a match.Note: The value of the label does not
    matter and is not evaluated in the match. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| DoesNotExist | 检查标签是否存在；如果标签与列表中的任何标签不匹配，则视为匹配。注意：标签的值无关紧要，且在匹配中不进行评估。|'
- en: '| Gt | Used to compare numeric values in a label; if a value is greater than
    (Gt) the label, it is considered a match.Note: This operator works only with a
    single number. |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Gt | 用于比较标签中的数值；如果值大于（Gt）标签，则视为匹配。注意：此操作符仅与单个数字一起使用。|'
- en: '| Lt | Used to compare numeric values in a label; if a value is less than (Lt)
    the label, it is considered a match.Note: This operator works only with a single
    number. |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Lt | 用于比较标签中的数值；如果值小于（Lt）标签，则视为匹配。注意：此操作符仅与单个数字一起使用。|'
- en: 'Using a node affinity rule, you can choose a soft or hard affinity based on
    your requirements. You can create affinity rules using two preferences: RequiredDuringSchedulingIgnoredDuringExecution,
    also known as a hard affinity, and preferredDuringSchedulingIgnoredDuringExecution,
    also known as a soft affinity. If you use a hard affinity, the affinity must match
    or the Pod will fail to schedule. However, if you use a soft affinity, the affinity
    rule will be used if it matches. If a match is not found, the Pod will schedule
    on any node in the cluster.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点亲和规则，您可以根据需求选择软亲和性或硬亲和性。您可以使用两个偏好创建亲和规则：RequiredDuringSchedulingIgnoredDuringExecution，也称为硬亲和性，和preferredDuringSchedulingIgnoredDuringExecution，也称为软亲和性。如果您使用硬亲和性，亲和性必须匹配，否则
    Pod 将无法调度。但是，如果您使用软亲和性，如果匹配，则将使用亲和规则。如果没有找到匹配项，Pod 将在集群中的任何节点上调度。
- en: Creating node affinity rules
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 创建节点亲和规则
- en: Node affinity is set in a manifest in the PodSpec, under the Affinity field,
    as nodeAffinity. To better explain how to use a node affinity rule, let’s use
    an example cluster to create a manifest that uses an affinity rule.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性在 PodSpec 中的 manifest 中设置，在亲和性字段下，作为 nodeAffinity。为了更好地解释如何使用节点亲和规则，让我们使用一个示例集群创建一个使用亲和规则的
    manifest。
- en: The cluster has three nodes, described in table 3.9\. The labels we will use
    in the rule are in bold.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 该集群有三个节点，如表3.9所示。在规则中我们将使用的标签将以粗体显示。
- en: Table 3.9 Nodes in a cluster
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.9 集群中的节点
- en: '| Node | Node labels |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 节点标签 |'
- en: '| Node 1 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-a**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-77fd9484-7fd6,kubernetes.io/os=linux
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 节点1 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-a**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-77fd9484-7fd6,kubernetes.io/os=linux
    |'
- en: '| Node 2 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-b**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-ca0442ad-hqk5,kubernetes.io/os=linux
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 节点2 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-b**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-ca0442ad-hqk5,kubernetes.io/os=linux
    |'
- en: '| Node 3 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-c**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-ead436da-8j7k,kubernetes.io/os=linux
    |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 节点3 | beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=e2-medium,beta.kubernetes.io/os=linux,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/gke-preemptible=true,failure-domain.beta.kubernetes.io/region=us-central1,**failure-domain.beta.kubernetes.io/zone=us-central1-c**,kubernetes.io/arch=amd64,kubernetes.io/hostname=gke-cluster-1-default-pool-ead436da-8j7k,kubernetes.io/os=linux
    |'
- en: 'We want to create a deployment that will create an NGINX server in either the
    us-central1-a or us-central1-c zones. Using the following manifest, we can create
    a Pod in either of the zones using an affinity rule based on the failure-domain.beta
    .kubernetes.io/zone key:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望创建一个部署，在us-central1-a或us-central1-c区域中创建一个NGINX服务器。使用以下清单，我们可以使用基于failure-domain.beta.kubernetes.io/zone键的亲和力规则在任一区域创建Pod：
- en: '[PRE10]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By using the key failure-domain.beta.kubernetes.io/zone in the matchExpressions,
    we set the affinity to evaluate to true if the node label matches either us-central1-a
    or us-central1-c. Because the second node in the cluster has a label value of
    us-central2-b, it will evaluate as false and will not be selected to run the Pod.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在matchExpressions中使用键failure-domain.beta.kubernetes.io/zone，我们设置亲和力评估为true，如果节点标签匹配us-central1-a或us-central1-c。因为集群中的第二个节点具有标签值us-central2-b，它将评估为false，并且不会被选中来运行Pod。
- en: Using Pod affinity and anti-affinity rules
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pod亲和力和反亲和力规则
- en: A Pod affinity rule will ensure that deployed Pods are running on the same set
    of nodes as a matching label, and an anti-affinity rule is used to ensure that
    Pods will *not* run on the same nodes as a matching label. Pod affinity rules
    are used for different use cases than node affinity rules. Whereas node affinity
    allows you to select a node based on the cluster node labels, Pod affinity and
    anti-affinity rules use the labels of Pods that are already running in the cluster.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Pod亲和力规则将确保已部署的Pod在具有匹配标签的同一组节点上运行，而反亲和力规则用于确保Pod不会在具有匹配标签的同一节点上运行。Pod亲和力规则用于与节点亲和力规则不同的用例。节点亲和力允许您根据集群节点标签选择节点，而Pod亲和力和反亲和力规则使用已在集群中运行的Pod的标签。
- en: Creating Pod affinity rules
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Pod亲和力规则
- en: When you create an affinity rule, you are telling the scheduler to place your
    Pod on a node that has an existing Pod that matches the selected value in the
    affinity rule. Pod affinity rules, like node affinity rules, can be created as
    soft or hard affinity rules. They also use operators like node affinity rules,
    including In*,* NotIn*,* Exists*, and* DoesNotExist, but they *do not* support
    the Gt or Lt operators.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个亲和规则时，你是在告诉调度器将你的Pod放置在具有与亲和规则中选定的值匹配的现有Pod的节点上。Pod亲和规则，就像节点亲和规则一样，可以创建为软亲和规则或硬亲和规则。它们也使用与节点亲和规则相同的运算符，包括In*、NotIn*、Exists*和DoesNotExist，但它们*不支持*Gt或Lt运算符。
- en: Pod affinity rules are specified in the PodSpec, under the affinity and podAffinity
    fields. They require an additional parameter that node affinity rules do not use—the
    topologyKey. The topologyKey is used by Kubernetes to create a list of nodes that
    will be checked against the affinity rule. Using a topologyKey, you can decide
    to look for matches based on different filters like zones or nodes.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Pod亲和规则在PodSpec中的亲和性和podAffinity字段中指定。它们需要一个节点亲和规则不使用的额外参数——topologyKey。topologyKey被Kubernetes用来创建一个将用于检查亲和规则的节点列表。使用topologyKey，你可以决定根据不同的过滤器（如区域或节点）来查找匹配项。
- en: 'For example, imagine you have a software package that is licensed per node,
    and each time a Pod that runs a portion of the software runs on another node,
    you need to purchase an additional license. To lower costs, you decide to create
    an affinity rule that will force the Pods to run where an existing licensed Pod
    is running. The existing Pod runs using a label called license with a value of
    widgets. An example manifest follows that creates a Pod on a node with an existing
    Pod with a label license=widgets. Because we need to be on the same node to maintain
    licensing, we will use a topologyKey that will filter by kubernetes.io/hostname:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个按节点许可的软件包，每次运行软件一部分的Pod运行在另一个节点上时，你需要购买额外的许可证。为了降低成本，你决定创建一个亲和规则，强制Pod在现有已许可的Pod运行的地方运行。现有的Pod使用名为license的标签，其值为widgets。以下是一个示例清单，它创建了一个在具有标签license=widgets的现有Pod的节点上的Pod。由于我们需要在同一节点上运行以维护许可，我们将使用一个按kubernetes.io/hostname过滤的topologyKey：
- en: '[PRE11]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The manifest tells Kubernetes to create the Pod nginx-widgets, running an image
    called nginx-widgets on a host that already has a Pod running using the label
    license with the value of widgets.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 清单告诉Kubernetes创建名为nginx-widgets的Pod，在已经运行使用标签license且值为widgets的Pod的主机上运行名为nginx-widgets的镜像。
- en: Creating Pod anti-affinity rules
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Pod反亲和规则
- en: Anti-affinity rules do the opposite of affinity rules. Whereas affinity rules
    group Pods based on a set of rules, anti-affinity rules are used to run Pods on
    different nodes. When you use an anti-affinity rule, you are telling Kubernetes
    that you *do not* want the Pod to run on another node that has an existing Pod
    with the values declared in the rule. Some common use cases for using anti-affinity
    rules include forcing Pods to avoid other running Pods or spreading Pods across
    availability zones.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 反亲和规则与亲和规则相反。亲和规则根据一组规则对Pod进行分组，而反亲和规则用于在不同的节点上运行Pod。当你使用反亲和规则时，你是在告诉Kubernetes你*不希望*Pod运行在另一个节点上，该节点上已经运行了一个具有规则中声明的值的现有Pod。使用反亲和规则的一些常见用例包括强制Pod避免其他正在运行的Pod或跨可用区域分散Pod。
- en: Pod anti-affinity rules are specified in the PodSpec, under the affinity and
    podAntiAffinity fields. They also require the topologyKey parameter to filter
    the list of nodes that will be used to compare the affinity rules.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Pod反亲和规则在PodSpec中的亲和性和podAntiAffinity字段中指定。它们也需要topologyKey参数来过滤将用于比较亲和规则的节点列表。
- en: In our affinity example, we used a topologyKey that used the hostname of the
    node. If we used the same key for the deployment, zones wouldn’t be considered;
    it would only avoid placing the Pod on the same node as another running Pod. Although
    the Pods would spread across nodes, the selected nodes could all be in the same
    zone, which would fail to spread the Pods across zones.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的亲和示例中，我们使用了一个使用节点主机名的topologyKey。如果我们为部署使用相同的键，则不会考虑区域；它只会避免将Pod放置在另一个正在运行的Pod所在的同一节点上。尽管Pod会分散到各个节点，但选定的节点可能都在同一区域，这将无法跨区域分散Pod。
- en: 'To spread the Pods across zones, we will use the label failure-domain.beta
    .kubernetes.io/zone, and we will use the operator In to compare the label app
    for the value of nginx-frontend, as shown in the next code snippet. We will also
    use a soft anti-affinity rule, rather than a hard rule, allowing Kubernetes to
    use the same zone, if there is no other choice:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将Pod分散到不同的区域，我们将使用标签failure-domain.beta.kubernetes.io/zone，并且我们将使用操作符In来比较标签app的值为nginx-frontend，如以下代码片段所示。我们还将使用软反亲和规则，而不是硬规则，允许Kubernetes在没有其他选择的情况下使用相同的区域：
- en: '[PRE12]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By using failure-domain.beta.kubernetes.io/zone as the topologyKey, we are telling
    Kubernetes that we want to avoid placing any Pod that has a label of app=nginx-frontend
    in the same zone.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用failure-domain.beta.kubernetes.io/zone作为拓扑键，我们告诉Kubernetes我们希望避免将任何具有app=nginx-frontend标签的Pod放置在同一个区域。
- en: Using taints and tolerations
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 使用污染和容忍度
- en: Although you are more likely to require scheduling a Pod to a specific node,
    some use cases exist where you will want to reserve specific nodes to only certain
    workloads, essentially disabling default scheduling. Unlike nodeSelector and affinity
    rules, taints are used to automatically repel, rather than attract, a Pod to a
    node. This is useful when you want a node to reject any scheduling attempts by
    default, unless a deployment specifically states the correct “tolerations” to
    be scheduled on the node. For example, imagine you have a cluster that has a few
    hundred nodes and a few nodes have GPUs available. Because GPUs are expensive,
    we want to restrict Pods on these nodes to only applications that require a GPU,
    rejecting any standard scheduling requests.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能更可能需要将Pod调度到特定的节点，但存在一些用例，你可能希望只为特定的工作负载保留特定的节点，从而基本上禁用默认调度。与nodeSelector和亲和规则不同，污染用于自动排斥，而不是吸引，Pod到节点。当你想要节点默认拒绝任何调度尝试，除非部署明确指定了要在节点上调度的正确“容忍度”时，这很有用。例如，想象你有一个包含数百个节点和几个具有GPU可用性的节点的集群。因为GPU很昂贵，我们希望限制这些节点上的Pod只用于需要GPU的应用程序，拒绝任何标准调度请求。
- en: Using controls like nodeSelector or affinity rules will not tell the Kubernetes
    scheduler to avoid using a node. These provide a developer the ability to control
    how Pods will be deployed, and if they don’t provide either of these, the scheduler
    will attempt to use any node in the cluster. Because GPUs are expensive, we want
    to reject any scheduling attempt to run a Pod on a node with a GPU that doesn’t
    require using a GPU.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如nodeSelector或亲和规则之类的控制项不会告诉Kubernetes调度器避免使用某个节点。这些提供了开发者控制Pod部署方式的能力，如果它们不提供这些中的任何一个，调度器将尝试使用集群中的任何节点。因为GPU很昂贵，我们希望拒绝任何在不需要使用GPU的节点上运行Pod的调度尝试。
- en: Creating a node taint
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 创建节点污染
- en: 'To stop the scheduler from scheduling Pods on a node, you need to “taint” the
    node with a value. To create a taint, use kubectl with the taint command and the
    node you want to taint, the key-value, and the effect. The key-value can be any
    value that you want to assign, and the effect can be one of three values: NoSchedule,
    PreferNoSchedule, or NoExecute*, described in table 3.10.*'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止调度器在节点上调度Pod，你需要使用带有taint命令的kubectl和要污染的节点、键值以及效果。键值可以是任何你想要分配的值，效果可以是以下三个值之一：NoSchedule、PreferNoSchedule或NoExecute*，如表3.10*中所述。
- en: Table 3.10 Taint effects
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.10 污染效果
- en: '| Effect | Description |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Effect | 描述 |'
- en: '| NoSchedule | If a Pod does not specify a toleration that matches the node
    taint, it will not be scheduled to run on the node. |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| NoSchedule | 如果Pod没有指定与节点污染匹配的容忍度，它将不会被调度到该节点上。|'
- en: '| PreferNoSchedule | If a Pod does not specify a toleration that matches the
    node taint, the scheduler will attempt to avoid scheduling the Pod on the node.
    |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| PreferNoSchedule | 如果Pod没有指定与节点污染匹配的容忍度，调度器将尝试避免在节点上调度Pod。|'
- en: '| NoExecute | If a Pod is already running on a node and a taint is added, if
    the Pod does not match the taint, it will be evicted from the node. |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| NoExecute | 如果Pod已经在节点上运行并且添加了污染，如果Pod不匹配污染，它将被从节点上驱逐。|'
- en: 'For example, if we had a GPU in a node named node1, we would taint the node
    using the following command:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个名为node1的节点上有一个GPU，我们可以使用以下命令来污染该节点：
- en: '[PRE13]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The key in the taint command tells the scheduler what taint must be matched
    to allow a Pod to schedule on the node. If the taint is not matched by a Pod request
    using a toleration, the scheduler will not schedule a Pod on the node, based on
    the effect NoSchedule.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 污点命令中的键告诉调度器必须匹配什么污点才能允许Pod在节点上调度。如果Pod请求没有使用容忍匹配污点，则基于NoSchedule的效果，调度器不会在节点上调度Pod。
- en: Creating Pods with tolerations
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 创建具有容忍的Pod
- en: By default, once a node has a taint set, the scheduler will not attempt to run
    any Pod on the tainted node. By design, you set a taint to tell the scheduler
    to avoid using the node in any scheduling, unless the deployment specifically
    requests running on the node. To allow a Pod to run on a node that has been tainted,
    you need to supply a *toleration* in the deployment. A toleration is used to tell
    the scheduler that the Pod can “tolerate” the taint on the node, which will allow
    the scheduler to use a node that matches the toleration with an assigned taint.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，一旦节点设置了污点，调度器将不会尝试在该受污节点上运行任何Pod。按设计，您设置污点是为了告诉调度器避免在任何调度中使用该节点，除非部署明确请求在该节点上运行。要允许Pod在已设置污点的节点上运行，您需要在部署中提供一个*容忍*。容忍用于告诉调度器Pod可以“容忍”节点上的污点，这将允许调度器使用与容忍匹配并分配了污点的节点。
- en: Note Taints will not attract a Pod request—they only reject any Pod that does
    not have a toleration set. As such, to direct a Pod to run on a node with a taint,
    you need to set a toleration and a node selection, or a node affinity. The selector
    will tell the scheduler to use a node with a matching label, and then the toleration
    tells the scheduler that the Pod can tolerate the taint set on the node. Because
    tolerations tell the scheduler to “prefer” a node with a matching taint, if one
    cannot be found, the scheduler will use any node in the cluster with a matching
    label.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 注意污点不会吸引Pod请求——它们只会拒绝任何没有设置容忍的Pod。因此，要将Pod引导到具有污点的节点上运行，您需要设置一个容忍和一个节点选择，或者节点亲和性。选择器将告诉调度器使用具有匹配标签的节点，然后容忍告诉调度器Pod可以容忍节点上设置的污点。因为容忍告诉调度器“偏好”具有匹配污点的节点，如果找不到这样的节点，调度器将使用集群中具有匹配标签的任何节点。
- en: Key takeaway Tolerations and node selectors/affinity rules work together to
    select the node that the Pod will run on.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点容忍和节点选择器/亲和规则协同工作，以选择Pod将要运行的节点。
- en: Tolerations are created in the pod.spec section of your manifest by assigning
    one or more tolerations that include the key to match, an operator, an optional
    value, and the taint effect.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在您的清单的pod.spec部分分配一个或多个包含要匹配的键、操作符、可选值和污点效果的容忍来创建容忍。
- en: The key must be assigned to the key that matches the node on which you want
    to schedule the Pod. The operator value tells the scheduler to simply look for
    the key (Exists) or to match a key value (Equals). If you use the Equals operator,
    your toleration must contain a value field. Finally, the effect needs to be matched
    for the Pod to be scheduled on the node.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 该键必须分配给与您想要在节点上调度Pod的节点匹配的键。操作值告诉调度器简单地查找键（存在）或匹配键值（等于）。如果您使用等于操作符，您的容忍必须包含一个值字段。最后，必须匹配效果，Pod才能在节点上调度。
- en: 'To schedule a Pod that can tolerate the GPU taint for node1, you would add
    the following to your PodSpec:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 要调度一个可以容忍node1的GPU污点的Pod，您需要在PodSpec中添加以下内容：
- en: '[PRE14]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Adding the toleration tells the scheduler that the Pod should be assigned to
    a node that has a taint key of gpu with an effect of NoSchedule.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 添加容忍告诉调度器Pod应该被分配到具有gpu污点键和NoSchedule效果的节点。
- en: Controlling where Pods will be scheduled is a key point to ensure that your
    application deployments can meet your assigned SLA/SLO objectives.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 控制Pod的调度位置是确保您的应用程序部署能够满足分配的SLA/SLO目标的关键点。
- en: 3.3 Advanced topics
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 进阶主题
- en: This section contains a few advanced topics that we wanted to include in this
    chapter. We think these are important topics, but they aren’t required to understand
    the main topics in the chapter.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了一些我们希望在章节中包含的进阶主题。我们认为这些主题很重要，但它们不是理解章节主要主题所必需的。
- en: 3.3.1 Aggregate ClusterRoles
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 聚合ClusterRoles
- en: When a new component is added to the cluster, a new ClusterRole is often created
    and can be assigned to users to manage the service. Sometimes a role may be created,
    and you may notice that a user assigned the ClusterRole of admin has permissions
    to the new components by default. Other times, you may notice that a newly added
    component, like Istio, does not allow the built-in admin role to use any Istio
    resources.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 当向集群添加新组件时，通常会创建一个新的 ClusterRole，并将其分配给用户以管理该服务。有时可能会创建一个角色，并且你可能注意到被分配了 ClusterRole
    的 admin 用户默认具有对新组件的权限。在其他时候，你可能注意到新添加的组件，如 Istio，不允许内置的 admin 角色使用任何 Istio 资源。
- en: 'It may sound odd that a role like admin would not have permissions to every
    resource by default. Kubernetes includes two ClusterRoles that provide some form
    of admin access: the admin ClusterRole and the cluster-admin ClusterRole. They
    may sound similar, but how permissions are assigned to them is very different.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像 admin 这样的角色默认没有权限访问每个资源可能听起来有些奇怪。Kubernetes 包含两个 ClusterRoles，它们提供某种形式的
    admin 访问权限：admin ClusterRole 和 cluster-admin ClusterRole。它们可能听起来很相似，但分配给它们的权限方式非常不同。
- en: The cluster-admin role is straightforward—it is assigned wildcards for all permissions,
    providing access to every resource, including new resources. The admin role is
    not assigned wildcard permissions. Each permission assigned to the admin role
    is usually explicitly assigned. Because the role does not use wildcards, any new
    permissions need to be assigned for new resources.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: cluster-admin 角色很简单——它为所有权限分配了通配符，提供对每个资源的访问权限，包括新资源。admin 角色没有分配通配符权限。分配给 admin
    角色的每个权限通常都是明确分配的。因为该角色不使用通配符，所以任何新的权限都需要为新资源分配。
- en: 'To make this process easier, Kubernetes has a concept called aggregated ClusterRoles.
    When a new ClusterRole is created, it can be aggregated to any other ClusterRole
    by assigning an aggregationRule. An example to help explain how aggregation works
    follows. The default admin ClusterRole looks similar to the next example:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程更容易，Kubernetes 有一个称为聚合 ClusterRoles 的概念。当创建一个新的 ClusterRole 时，可以通过分配一个
    aggregationRule 来将其聚合到任何其他 ClusterRole。以下是一个帮助解释聚合如何工作的示例。默认的 admin ClusterRole
    看起来与下一个示例类似：
- en: '[PRE15]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this code snippet, you can see that the admin ClusterRole has an aggregationRule
    that contains rbac.authorization.k8s.io/aggregate-to-admin: ''true''. When a new
    ClusterRole is created, it can be automatically aggregated with the built-in admin
    ClusterRole if it uses the same aggregationRule. For example, a new CRD has been
    deployed to the cluster that creates a new ClusterRole. Because the permissions
    for the new ClusterRole should be assigned to admins, it has been created with
    an aggregationRule that matches rbac.authorization.k8s.io/aggregate-to-admin:
    "true", as shown next:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个代码片段中，你可以看到 admin ClusterRole 有一个包含 rbac.authorization.k8s.io/aggregate-to-admin:
    ''true'' 的 aggregationRule。当创建一个新的 ClusterRole 时，如果它使用相同的聚合规则，它可以自动与内置的 admin
    ClusterRole 聚合。例如，已经部署到集群中的新 CRD 创建了一个新的 ClusterRole。因为新 ClusterRole 的权限应该分配给管理员，所以它已经创建了一个与
    rbac.authorization.k8s.io/aggregate-to-admin: "true" 匹配的聚合规则，如下所示：'
- en: '[PRE16]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will create a new ClusterRole named aggregated-example-admin that assigns
    the actions get, list, watch, create, patch, and delete to the resource newresource
    in the newapi apiGroup. This new ClusterRole can be bound to any user that you
    want to assign permissions to, but because the permission is required by admins,
    it also has a label assigned of rbac.authorization.k8s.io/aggregate-to-admin:
    "true", which matches the aggregationRule that is assigned in the admin ClusterRole.
    The labels match, so a controller on the API server will notice the matching labels
    and “merge” the permissions from the new ClusterRole with the admin ClusterRole.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '这将创建一个名为 aggregated-example-admin 的新 ClusterRole，将 get、list、watch、create、patch
    和 delete 等操作分配给 newapi apiGroup 中的资源 newresource。这个新的 ClusterRole 可以绑定到任何你想分配权限的用户，但由于权限是管理员所需的，它还分配了一个标签
    rbac.authorization.k8s.io/aggregate-to-admin: "true"，这与在 admin ClusterRole 中分配的聚合规则相匹配。标签匹配，因此
    API 服务器上的控制器会注意到匹配的标签，并将新 ClusterRole 的权限与 admin ClusterRole 合并。'
- en: 3.3.2 Custom schedulers
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 自定义调度器
- en: One of the most misunderstood concepts in Kubernetes is how the cluster schedules
    workloads. You will often hear that applications deployed on a Kubernetes cluster
    are highly available, and they are, when deployed correctly. To deploy a highly
    available application, it’s beneficial to understand how the kube-scheduler makes
    decisions and the options available to your deployments to help influence the
    decisions that it will make.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，最被误解的概念之一是集群如何调度工作负载。你经常会听到在Kubernetes集群上部署的应用程序具有高可用性，当正确部署时，它们确实是。为了部署高可用性应用程序，了解kube-scheduler如何做出决策以及你的部署可以影响其决策的选项是有益的。
- en: The default Kubernetes scheduler, kube-scheduler, has the job of scheduling
    Pods to worker nodes based on a set of criteria that include node affinity, taints
    and tolerations, and node selectors. Although Kubernetes includes the base scheduler,
    you are not stuck using only a single scheduler for all deployments. If you need
    special scheduling considerations that the base scheduler does not include, you
    can create custom schedulers by specifying a scheduler in the manifest, and if
    one is not provided, the default scheduler will be used. Creating a custom scheduler
    is beyond the scope of this book, but you can read more about custom schedulers
    on the Kubernetes site at [http://mng.bz/jm9y](http://mng.bz/jm9y).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Kubernetes调度器，kube-scheduler，负责根据一系列标准将Pod调度到工作节点，包括节点亲和性、污点和容忍度以及节点选择器。尽管Kubernetes包含了基础调度器，但你并不局限于只使用单个调度器来处理所有部署。如果你需要基础调度器不包括的特殊调度考虑因素，你可以通过在清单中指定调度器来创建自定义调度器，如果没有提供，则将使用默认调度器。创建自定义调度器超出了本书的范围，但你可以在Kubernetes网站上了解更多关于自定义调度器的信息：[http://mng.bz/jm9y](http://mng.bz/jm9y)。
- en: 'An example of a Pod that sets the scheduler to a custom scheduler named custom-scheduler1
    follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个将调度器设置为名为custom-scheduler1的自定义调度器的Pod的例子：
- en: '[PRE17]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The Kubernetes scheduler watches the API server for Pods that require scheduling.
    Once it determines that a Pod needs to be scheduled, it will determine the most
    appropriate node by going through a multistage decision process that will filter
    out nodes and then assign a score to nodes that have not been filtered out.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes调度器监视API服务器以查找需要调度的Pod。一旦确定一个Pod需要被调度，它将通过一个多阶段决策过程来确定最合适的节点，这个过程将过滤掉节点，然后对未被过滤掉的节点进行评分。
- en: 3.4 Examples and case studies
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 例子和案例研究
- en: Using the knowledge from the chapter, address each of the requirements in the
    case study found next. Remember, if you deployed your GKE cluster across different
    regions, replace the example regions in the exercises with your regions. To save
    on any potential cost, the examples require only a single node in each region.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章的知识，解决接下来案例研究中提出的每个要求。记住，如果你在不同地区部署了你的GKE集群，请将练习中的示例区域替换为你的区域。为了节省任何潜在的成本，示例只需要每个区域一个节点。
- en: 3.4.1 FooWidgets Industries
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 FooWidgets Industries
- en: You have been asked to assist FooWidgets Industries with a new GKE cluster that
    they have deployed. They quickly discovered that they did not have the internal
    skills to complete their deployment, and, therefore, the current state of the
    cluster is a simple, new cluster across three GCP zones.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求协助FooWidgets Industries处理他们新部署的GKE集群。他们很快发现他们没有内部技能来完成部署，因此集群的当前状态是在三个GCP区域中的简单、新的集群。
- en: Cluster overview and requirements
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 集群概述和需求
- en: 'FooWidgets Industries has a GKE cluster that has been deployed across three
    zones: us-east4-a, use-east4-b, and us-east4-c. The company has various requirements
    for Pod placement based on internal standards and specialized hardware use. They
    have included a breakdown of the desired placement of workloads and the labels
    that should be assigned to nodes, outlined in table 3.11.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: FooWidgets Industries有一个部署在三个区域（us-east4-a、us-east4-b和us-east4-c）的GKE集群。公司根据内部标准和专用硬件使用有不同的Pod放置要求。他们包括了期望的工作负载放置和应分配给节点的标签的分解，如表3.11所示。
- en: Table 3.11 Placement of workloads
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.11 工作负载放置
- en: '| Zone | Desired workloads | Label/taint | Node name |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 所需工作负载 | 标签/污点 | 节点名称 |'
- en: '| us-east4-a | Any workloadFast disk access | disk=fast | gke-cls1-pool1-1d704097-4361
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| us-east4-a | 任何工作负载 | disk=fast | gke-cls1-pool1-1d704097-4361 |'
- en: '| us-east4-b | Only workloads that require a GPU | workload=gpu | gke-cls1-pool1-52bcec35-tf0q
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| us-east4-b | 仅需要GPU的工作负载 | workload=gpu | gke-cls1-pool1-52bcec35-tf0q |'
- en: '| us-east4-c | Any workload |  | gke-cls1-pool1-e327d1de-6ts3 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| us-east4-c | 任何工作负载 |  | gke-cls1-pool1-e327d1de-6ts3 |'
- en: 'The statement of work requires you to provide the requirements in the table.
    The cluster has not been configured past the initial deployment stage and will
    require you to complete the following configuration:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 工作说明书要求你在表中提供需求。集群尚未配置到初始部署阶段，需要你完成以下配置：
- en: Create any node labels or taints that are required to achieve workload placement
    based on the supported workloads documented in the table.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建任何节点标签或污点，以根据表中记录的支持工作负载实现工作负载放置。
- en: Create an example deployment using an NGINX image to demonstrate successful
    placement of workloads based on the requirements provided by FooWidgets Industries.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个使用 NGINX 镜像的示例部署，以演示根据 FooWidgets Industries 提供的需求成功放置工作负载。
- en: The next section contains the solution to address FooWidgets’ requirements.
    You can follow along with the solution or, if you are comfortable, configure your
    cluster to address the requirements and use the solution to verify your results.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分包含解决 FooWidgets 需求的方案。你可以跟随方案进行操作，或者如果你感到舒适，配置你的集群以满足需求并使用方案来验证你的结果。
- en: 'FooWidgets Industries solution: Labels and taints'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: FooWidgets Industries 解决方案：标签和污点
- en: The first requirement is to create any labels or taints that may be required.
    Using the requirements table, we can tell that we need to label the nodes in us-east4-a
    with disk=fast. This label will allow a deployment to force scheduling on a node
    that has the required fast disks for the application. The second requirement is
    to limit any running workloads in the us-east4-b zone to only applications that
    require a GPU. For this requirement, we have decided to taint all nodes in the
    us-east4-b zone with workload=gpu.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项要求是创建可能需要的任何标签或污点。使用需求表，我们可以得知我们需要在 us-east4-a 中的节点上添加 disk=fast 的标签。这个标签将允许部署强制在具有所需快速磁盘的应用程序节点上进行调度。第二项要求是将
    us-east4-b 区域中运行的任何工作负载限制为仅需要 GPU 的应用程序。对于这个要求，我们决定将 us-east4-b 区域中的所有节点污点化为 workload=gpu。
- en: 'Why is a label used for one solution and a taint for the other? You may recall
    that labels and taints are used to accomplish different scheduling requirements:
    we use labels to attract workloads, whereas we use taints to repel them. In the
    requirements, FooWidgets clearly states that us-east4-a and us-east4-c can run
    any type of workload, but us-east4-b must run only workloads that require a GPU.
    If a deployment is created that does not specify a label on a node, the scheduler
    will still consider that node as a potential node for scheduling. Labels are used
    to force a deployment to a particular node, but they do not reject workloads that
    do not contain a label request. This behavior is far different from a node that
    has been assigned a taint. When a node is tainted, it will repel any workloads
    that do not contain a toleration for the assigned node taint. If a deployment
    is created without any tolerations for the node taint, the scheduler will automatically
    exclude the tainted nodes from scheduling the workload.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一个解决方案使用标签，而另一个使用污点？你可能还记得，标签和污点用于完成不同的调度需求：我们使用标签来吸引工作负载，而使用污点来排斥它们。在需求中，FooWidgets
    明确指出 us-east4-a 和 us-east4-c 可以运行任何类型的工作负载，但 us-east4-b 必须仅运行需要 GPU 的工作负载。如果创建了一个没有在节点上指定标签的部署，调度器仍然会考虑该节点作为潜在的调度节点。标签用于强制部署到特定的节点，但它们不会拒绝不包含标签请求的工作负载。这种行为与被分配了污点的节点大不相同。当一个节点被污点化时，它会排斥任何不包含分配节点污点容忍的工作负载。如果一个部署没有为节点污点指定任何容忍，调度器将自动排除污点节点进行工作负载调度。
- en: Creating the labels and taints
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标签和污点
- en: 'We need to label the node in us-east4-a with disk=fast. To label the node,
    we use the kubectl label command, supplying the node and the label:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 us-east4-a 中的节点上添加 disk=fast 的标签。为了添加标签，我们使用 kubectl label 命令，提供节点和标签：
- en: '[PRE18]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we need to add a taint to the nodes in the us-east4-b zone with workload=gpu.
    Remember that a taint will repel any request that does not tolerate the assigned
    node taint, but it doesn’t attract a workload. This means that you also need to
    add a label to direct the GPU Pods to the correct node. To taint the node, we
    use the kubectl taint command, supplying the node name and the taint:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在 us-east4-b 区域的节点上添加 workload=gpu 的污点。记住，污点会排斥任何不能容忍分配节点污点的请求，但它不会吸引工作负载。这意味着你还需要添加一个标签来引导
    GPU Pods 到正确的节点。为了污点节点，我们使用 kubectl taint 命令，提供节点名称和污点：
- en: '[PRE19]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, label the node to attract the GPU Pods:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为节点添加标签以吸引 GPU Pods：
- en: '[PRE20]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that we did not add a label or a taint to the node in us-east4-c because
    that zone can run any workload.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有在 us-east4-c 区域的节点上添加标签或污点，因为该区域可以运行任何工作负载。
- en: Now that the nodes are labeled, you need to create example deployments to verify
    that workload placement matches the requirements from the table.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 现在节点已标记，您需要创建示例部署以验证工作负载放置是否符合表格中的要求。
- en: Creating a deployment that requires fast disk access
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 创建需要快速磁盘访问的部署
- en: 'To force a deployment that requires fast disk access to the us-east4-a zone,
    you need to add a nodeSelector to the deployment. The following code snippet creates
    an NGINX server that contains a nodeSelector using the label disk=fast, forcing
    the workload to run on a node in the us-east4-a zone:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 要将需要快速磁盘访问的部署强制到 us-east4-a 区域，您需要在部署中添加一个 nodeSelector。以下代码片段创建了一个包含使用标签 disk=fast
    的 nodeSelector 的 NGINX 服务器，强制工作负载在 us-east4-a 区域的节点上运行：
- en: '[PRE21]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When you create and execute the manifest, the nodeSelector tells the scheduler
    to use a node with the label disk:fast. To verify the selector is working correctly,
    we can list the Pods with -o wide to list the node that the Pod is running on.
    In us-east4-a we have a single node, gke-cls1-pool1-1d704097-436. The abbreviated
    output from kubectl get pods confirms that the Pod was scheduled correctly:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建并执行清单时，nodeSelector 会告诉调度器使用带有标签 disk:fast 的节点。为了验证选择器是否正常工作，我们可以使用 -o wide
    列出 Pods，以列出 Pod 运行的节点。在 us-east4-a 中，我们有一个单独的节点，gke-cls1-pool1-1d704097-436。kubectl
    get pods 的缩略输出确认 Pod 已正确调度：
- en: '[PRE22]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that you have confirmed that Pods requiring fast disk access can be scheduled
    correctly, you need to create a deployment to test workloads that require GPUs.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已确认需要快速磁盘访问的 Pod 可以正确调度，您需要创建一个部署来测试需要 GPU 的工作负载。
- en: Creating a deployment that requires a GPU
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 创建需要 GPU 的部署
- en: 'Any workload that requires a GPU needs to be scheduled on a node in us-east4-b.
    We already tainted the node in that zone, and to confirm that a workload requiring
    a GPU will be scheduled correctly, we need to create a test deployment with a
    toleration using the code that follows:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 任何需要 GPU 的工作负载都需要在 us-east4-b 区域的节点上调度。我们已经在该区域的节点上添加了污点，为了确认需要 GPU 的工作负载可以正确调度，我们需要创建一个使用以下代码的容忍度测试部署：
- en: '[PRE23]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When this code snippet is applied, you can verify that the Pod is running on
    the correct node in us-east4-b using kubectl get pods -o wide:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用此代码片段时，您可以使用 kubectl get pods -o wide 验证 Pod 是否正在 us-east4-b 区域的正确节点上运行：
- en: '[PRE24]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Comparing the output with the table that lists the nodes in each zone verifies
    that the Pod has been scheduled on a node in the us-east4-b zone.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 将输出与列出每个区域节点的表格进行比较，可以验证 Pod 已在 us-east4-b 区域的节点上调度。
- en: Congratulations! You have successfully addressed the workload requirements and
    have proven that you understand how to schedule workloads based on node labels
    and taints.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功解决工作负载需求，并证明了您理解如何根据节点标签和污点调度工作负载。
- en: Summary
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The control plan receives and stores objects and schedules workloads, whereas
    worker nodes are where the actual containers will execute once scheduled by the
    Kubernetes scheduler.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制计划接收并存储对象并调度工作负载，而工作节点是实际容器在 Kubernetes 调度器调度后执行的地方。
- en: 'Two different deployment models are available: declarative and imperative.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用两种不同的部署模型：声明式和命令式。
- en: You gained an understanding of Kubernetes resources and their functions.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经了解了 Kubernetes 资源及其功能。
- en: You can use selectors, taints, tolerations, and anti-affinity and affinity rules
    to control what nodes will be used for specific workloads.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用选择器、污点、容忍度和亲和力以及反亲和力规则来控制哪些节点将用于特定工作负载。
