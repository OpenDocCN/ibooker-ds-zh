- en: 17 Using predictions to make more predictions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 使用预测来做出更多预测
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Examining the autoregressive LSTM (ARLSTM) architecture
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查自回归LSTM（ARLSTM）架构
- en: Discovering the caveat of the ARLSTM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现ARLSTM的局限性
- en: Implementing an ARLSTM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现ARLSTM
- en: In the last chapter, we examined and built a convolutional neural network (CNN).
    We even combined it with the LSTM architecture to test whether we could outperform
    the LSTM models. The results were mixed, as the CNN models performed worse as
    single-step models, performed best as multi-step models, and performed equally
    well as multi-output models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们检查并构建了一个卷积神经网络（CNN）。我们甚至将其与LSTM架构结合，以测试我们是否能够超越LSTM模型。结果混合，因为CNN模型作为单步模型表现较差，作为多步模型表现最佳，作为多输出模型表现相等。
- en: Now we’ll focus entirely on the multi-step models, as all of them output the
    entire sequence of predictions in a single shot. We’re going to modify that behavior
    and gradually output the prediction sequence, using past predictions to make new
    predictions. That way, the model will create rolling forecasts, but using its
    own predictions to inform the output.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将完全专注于多步模型，因为它们都一次性输出整个预测序列。我们将修改这种行为，并逐步输出预测序列，使用过去的预测来做出新的预测。这样，模型将创建滚动预测，但使用自己的预测来指导输出。
- en: This architecture is commonly used with LSTM and is called *autoregressive LSTM*
    (ARLSTM). In this chapter, we’ll first explore the general architecture of the
    ARLSTM model, and then we’ll build it in Keras to see if we can build a new top-performing
    multi-step model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构通常与LSTM一起使用，称为*自回归LSTM*（ARLSTM）。在本章中，我们将首先探讨ARLSTM模型的一般架构，然后我们将使用Keras构建它，以查看我们是否可以构建一个新的顶级多步模型。
- en: 17.1 Examining the ARLSTM architecture
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 检查ARLSTM架构
- en: We have built many multi-step models that all output predictions for traffic
    volume in the next 24 hours. Each model has generated the entire prediction sequence
    in a single shot, meaning that we get 24 values from the model right away.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了许多多步模型，所有这些模型都输出未来24小时的交通量预测。每个模型都在一次操作中生成整个预测序列，这意味着我们可以立即从模型中获得24个值。
- en: For illustration purposes, let’s consider a simple model with only an LSTM layer.
    Figure 17.1 shows the general architecture of the multi-step models we have built
    so far. Each of them had inputs coming in, passing through a layer, whether it
    is `LSTM`, `Dense`, or `Conv1D`, and resulting in a sequence of 24 values. This
    type of architecture forces an output of 24 values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，让我们考虑一个只有LSTM层的简单模型。图17.1展示了我们迄今为止构建的多步模型的一般架构。每个模型都有输入进来，通过一个层，无论是`LSTM`、`Dense`还是`Conv1D`，最终产生一个24个值的序列。这种架构强制输出24个值。
- en: '![](../../OEBPS/Images/17-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/17-01.png)'
- en: Figure 17.1 Illustrating a single-shot multi-step model with an LSTM layer.
    All multi-step models that we have built have had this general architecture. The
    LSTM layer can easily be replaced by a CNN layer or a dense layer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 展示了具有LSTM层的单次多步模型。我们构建的所有多步模型都具有这种通用架构。LSTM层可以很容易地被CNN层或密集层替换。
- en: But what if we want a longer sequence? Or a shorter sequence? What if we wish
    to forecast the next 8 hours only, or forecast the next 48 hours? In that case,
    we must redo our data windows and retrain the models, which might represent quite
    a bit of work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想要更长的序列或更短的序列呢？如果我们只想预测接下来的8小时，或者预测接下来的48小时呢？在这种情况下，我们必须重新调整数据窗口并重新训练模型，这可能会是一项相当多的工作。
- en: Instead, we can opt for an autoregressive deep learning model. As you can see
    in figure 17.2, each prediction is sent back into the model, allowing it to generate
    the next prediction. This process is repeated until we obtain a sequence of the
    desired length.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以选择一个自回归深度学习模型。如图17.2所示，每个预测都会被送回模型，允许它生成下一个预测。这个过程会重复进行，直到我们获得所需长度的序列。
- en: '![](../../OEBPS/Images/17-02.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/17-02.png)'
- en: Figure 17.2 An autoregressive LSTM model. This model returns a first prediction
    at *t*[24], and it is sent back into the model to generate the prediction at *t*[25].
    This process is repeated until the desired output length is obtained. Again, an
    LSTM layer is shown, but it could be a CNN or a dense layer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2 自回归LSTM模型。此模型在*t*[24]处返回第一个预测，并将其送回模型以生成*t*[25]处的预测。这个过程会重复进行，直到获得所需的输出长度。再次强调，这里展示了一个LSTM层，但它可以是CNN层或密集层。
- en: You can see how easy it becomes to generate any sequence length using an autoregressive
    deep learning architecture. This approach has the added advantage of allowing
    us to forecast time series with different scales, such as hours, days, or months,
    while avoiding having to retrain a new model. This is the type of architecture
    built by Google DeepMind to create WaveNet ([https://deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)),
    a model that generates raw audio sequences. In the context of time series, DeepAR
    ([http://mng.bz/GEoV](http://mng.bz/GEoV)) is a methodology that also uses an
    autoregressive recurrent neural network to achieve state-of-the-art results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用自回归深度学习架构生成任何序列长度是多么容易。这种方法还有一个额外的优势，即允许我们预测不同时间尺度的时序，如小时、天或月，同时避免需要重新训练新模型。这是
    Google DeepMind 构建WaveNet ([https://deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio))
    所采用的架构，该模型生成原始音频序列。在时序的背景下，DeepAR ([http://mng.bz/GEoV](http://mng.bz/GEoV)) 是一种使用自回归循环神经网络实现最先进结果的方法。
- en: Nevertheless, autoregressive deep learning models come with a major caveat,
    which is the accumulation of error. We have forecast many time series, and we
    know that there is always some discrepancy between our predictions and the actual
    values. That error accumulates as it is fed back into the model, meaning that
    later predictions will have a larger error than earlier predictions. Thus, while
    the autoregressive deep learning architecture seems powerful, it might not be
    the best solution for a particular problem. Hence the importance of using a rigorous
    testing protocol, which is really what we have developed since chapter 13.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自回归深度学习模型存在一个主要的缺点，即误差的累积。我们已经预测了许多时间序列，并且我们知道我们的预测值和实际值之间总是存在一些差异。这种误差随着它被反馈回模型而累积，这意味着后续的预测将比早期的预测有更大的误差。因此，尽管自回归深度学习架构看起来很强大，但它可能不是特定问题的最佳解决方案。因此，使用严格的测试协议非常重要，这正是我们从第
    13 章开始就一直在开发的。
- en: Still, it is good to have this model in your toolbox of time series forecasting
    methods. In the next section, we’ll code an autoregressive LSTM model to produce
    forecasts for the next 24 hours. We’ll compare its performance to that of our
    previous multi-step models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将这个模型添加到你的时间序列预测方法工具箱中是很好的。在下一节中，我们将编写一个自回归 LSTM 模型来预测接下来的 24 小时。我们将将其性能与之前的多步模型进行比较。
- en: 17.2 Building an autoregressive LSTM model
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 构建自回归 LSTM 模型
- en: We are now ready to code our own autoregressive deep learning model in Keras.
    Specifically, we’ll code an ARLSTM model, since our experiments have shown that
    the LSTM model achieves the best performance of the multi-step models. Thus we’ll
    try to further improve this model by making it autoregressive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好在 Keras 中编写我们自己的自回归深度学习模型。具体来说，我们将编写一个 ARLSTM 模型，因为我们的实验表明 LSTM 模型在多步模型中实现了最佳性能。因此，我们将尝试通过使其自回归来进一步改进这个模型。
- en: As always, make sure that you have the `DataWindow` class and the `compile_and_fit`
    function accessible in your notebook or Python script. They are the same versions
    that we developed in chapter 13.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，请确保你在笔记本或 Python 脚本中可以访问 `DataWindow` 类和 `compile_and_fit` 函数。它们与第 13
    章中开发的版本相同。
- en: 'Note At any time, feel free to consult the source code for this chapter on
    GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在任何时候，都可以自由查阅 GitHub 上本章的源代码：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17)。
- en: The first step is to read the training, validation, and test sets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是阅读训练集、验证集和测试集。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we’ll define our window of data. In this case, we’ll reuse the window
    of data we used for the LSTM model. The input and label sequences will each have
    24 timesteps. We’ll specify a `shift` of 24 so that the model outputs 24 predictions.
    Our target remains the traffic volume.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的数据窗口。在这种情况下，我们将重用之前用于 LSTM 模型的数据窗口。输入和标签序列将各有 24 个时间步长。我们将指定一个 `shift`
    为 24，以便模型输出 24 个预测。我们的目标仍然是交通量。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we’ll wrap our model in a class called `AutoRegressive`, which inherits
    from the `Model` class in Keras. This is what allows us to access inputs and outputs.
    That way, we’ll be able to specify that the output should become an input at each
    prediction step.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将用名为 `AutoRegressive` 的类包装我们的模型，该类继承自 Keras 中的 `Model` 类。这使得我们可以访问输入和输出。这样，我们将能够指定输出应在每个预测步骤成为输入。
- en: 'We’ll start by defining the `__init__` function in our `AutoRegressive` class.
    This function takes three parameters:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在我们的 `AutoRegressive` 类中定义 `__init__` 函数。此函数接受三个参数：
- en: '`self`—References the instance of the `AutoRegressive` class.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self` — 指向 `AutoRegressive` 类的实例。'
- en: '`units`—Represents the number of neurons in a layer.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units` — 表示层中的神经元数量。'
- en: '`out_steps`—Represents the length of the prediction sequence. In this case,
    it is 24.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_steps` — 表示预测序列的长度。在这种情况下，它是 24。'
- en: 'Then we’ll make use of three different Keras layers: the `Dense` layer, the
    `RNN` layer, and the `LSTMCell` layer. The `LSTMCell` layer is a lower-level layer
    than the `LSTM` layer. It allows us to access more granular information, such
    as state and predictions, which we can then manipulate to feed an output back
    into the model as an input. As for the `RNN` layer, this is used to train the
    `LSTMCell` layer on the input data. Its output is then passed through the `Dense`
    layer to generate a prediction. This is the complete `__init__` function:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用三个不同的 Keras 层：`Dense` 层、`RNN` 层和 `LSTMCell` 层。`LSTMCell` 层是比 `LSTM`
    层更底层的层。它允许我们访问更细粒度的信息，例如状态和预测，然后我们可以操作这些信息，将输出作为输入反馈到模型中。至于 `RNN` 层，它用于在输入数据上训练
    `LSTMCell` 层。其输出随后通过 Dense 层生成预测。这是完整的 `__init__` 函数：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The number of neurons in a layer is defined by units, and the length of the
    prediction sequence is defined by out_steps.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 层中的神经元数量由 units 定义，预测序列的长度由 out_steps 定义。
- en: ❷ The LSTMCell layer is a lower-level class that allows us to access more granular
    information, such as state and outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ LSTMCell 层是一个低级类，它允许我们访问更细粒度的信息，例如状态和输出。
- en: ❸ The RNN layer wraps the LSTMCell layer so it is easier to train the LSTM on
    the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ RNN 层包裹了 LSTMCell 层，这使得在数据上训练 LSTM 更加容易。
- en: ❹ The prediction comes from this Dense layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 预测来自这个 Dense 层。
- en: With the initialization done, the next step is to define a function that outputs
    the very first prediction. Since this is an autoregressive model, that prediction
    is then fed back into the model as an input to generate the next prediction. We
    must therefore have a method to capture that very first forecast before entering
    the autoregressive loop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化完成后，下一步是定义一个输出第一个预测的函数。由于这是一个自回归模型，该预测随后被反馈到模型中作为输入以生成下一个预测。因此，我们必须有一种方法在进入自回归循环之前捕获那个非常第一个预测。
- en: Thus, we’ll define the `warmup` function, which replicates a single-step LSTM
    model. We’ll simply pass the inputs into the `lstm_rnn` layer, get the prediction
    from the `Dense` layer, and return both the prediction and the state.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将定义 `warmup` 函数，该函数复制单步 LSTM 模型。我们将简单地将输入传递到 `lstm_rnn` 层，从 Dense 层获取预测，并返回预测和状态。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Pass the inputs through the LSTM layer. The output is sent to the Dense layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入通过 LSTM 层。输出被发送到 Dense 层。
- en: ❷ Get a prediction from the Dense layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 Dense 层获取预测。
- en: Now that we have a way to capture the first prediction, we can define the `call`
    function, which will run a loop to generate the sequence of predictions with a
    length of `out_steps`. Note that the function must be named `call` because it
    is called implicitly by Keras; naming it differently would result in an error.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种捕获第一次预测的方法，我们可以定义 `call` 函数，该函数将运行一个循环以生成长度为 `out_steps` 的预测序列。请注意，函数必须命名为
    `call`，因为它是被 Keras 隐式调用的；如果命名不同，将导致错误。
- en: Since we are using the `LSTMCell` class, which is a low-level class, we must
    manually pass in the previous state. Once the loop is finished, we stack our predictions
    and make sure they have the right output shape using the `transpose` method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是低级类 `LSTMCell`，我们必须手动传入前一个状态。一旦循环完成，我们将我们的预测堆叠起来，并确保它们具有正确的输出形状，使用 `transpose`
    方法。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Initialize an empty list to collect all the predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化一个空列表以收集所有预测。
- en: ❷ The first prediction is obtained from the warmup function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 首次预测是通过 warmup 函数获得的。
- en: ❸ Place the first prediction in the list of predictions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将第一个预测放入预测列表中。
- en: ❹ The prediction becomes an input for the next one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 预测成为下一个的输入。
- en: ❺ Generate a new prediction using the previous one as an input.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用前一个预测作为输入生成一个新的预测。
- en: ❻ Stack all the predictions. At this point, we have a shape (time, batch, features).
    It must be changed to (batch, time, features).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将所有预测值堆叠起来。到目前为止，我们有一个形状（time, batch, features）。它必须改为（batch, time, features）。
- en: ❼ Use transpose to get the needed shape of (batch, time, features).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用转置来获取所需的形状（batch, time, features）。
- en: The complete class is shown in the following listing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的类定义如下所示。
- en: Listing 17.1 Defining a class to implement an ARLSTM model
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1 定义一个类来实现ARLSTM模型
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have now defined our `AutoRegressive` class, which implements an autoregressive
    LSTM model. We can use it and train a model on our data. We’ll initialize it with
    32 units and an output sequence length of 24 timesteps, since the objective of
    the multi-step model is to forecast the next 24 hours.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了我们的`AutoRegressive`类，它实现了一个自回归LSTM模型。我们可以使用它并在我们的数据上训练一个模型。我们将使用32个单元和一个输出序列长度为24个时间步长来初始化它，因为多步模型的目的是预测接下来的24小时。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we’ll compile the model, train it, and store its performance metrics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译模型，训练它，并存储其性能指标。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can visualize the predictions of our model against the actual values by using
    the `plot` method from our `DataWindow` class.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`DataWindow`类的`plot`方法来可视化模型的预测值与实际值。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In figure 17.3 many predictions are very close to the actual values, sometimes
    even overlapping them. This indicates that we have a fairly accurate model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在图17.3中，许多预测值非常接近实际值，有时甚至与之重叠。这表明我们有一个相当准确的模型。
- en: '![](../../OEBPS/Images/17-03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/17-03.png)'
- en: Figure 17.3 Forecasting traffic volume for the next 24 hours using an ARLSTM
    model. Many predictions (shown as crosses) overlap the actual values (shown as
    squares), which means that we have a fairly accurate model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3 使用ARLSTM模型预测未来24小时的交通量。许多预测值（以交叉表示）与实际值（以正方形表示）重叠，这意味着我们有一个相当准确的模型。
- en: This visual inspection is not sufficient to determine whether we have a new
    top-performing model, so we’ll display its MAE against that of all previous multi-step
    models. The result is shown in figure 17.4, which shows that our autoregressive
    LSTM model achieves an MAE of 0.063 on the validation set and 0.049 on the test
    set. This is a better score than the CNN, and the CNN + LSTM models, as well as
    the simple LSTM model. Thus, the ARLSTM model becomes the top-performing multi-step
    model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种视觉检查不足以确定我们是否有一个新的顶级模型，因此我们将显示其与所有先前多步模型的MAE。结果如图17.4所示，显示我们的自回归LSTM模型在验证集上实现了0.063的MAE，在测试集上实现了0.049的MAE。这个分数比CNN、CNN
    + LSTM模型以及简单的LSTM模型都要好。因此，ARLSTM模型成为了顶级的多步模型。
- en: '![](../../OEBPS/Images/17-04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/17-04.png)'
- en: Figure 17.4 The MAE of all our multi-step models on the validation and test
    sets. The ARLSTM model achieves a lower MAE than the CNN and the CNN + LSTM models
    and the simple LSTM model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4 所有我们的多步模型在验证集和测试集上的MAE。ARLSTM模型的MAE低于CNN、CNN + LSTM模型以及简单的LSTM模型。
- en: Always keep in mind that the performance of each model depends on the problem
    at stake. The takeaway here is not that the ARLSTM is always the best model, but
    that it is the best-performing model for this situation. For another problem,
    you might find another champion model. If you have been completing the exercises
    since chapter 13, you can already see this happening. Keep in mind that each model
    we have built since chapter 13 is meant to be another tool in your toolbox to
    help you maximize the chances of solving a time series forecasting problem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总是记住，每个模型的性能都取决于所涉及的问题。这里的要点不是ARLSTM总是最好的模型，而是它是这种情况下的最佳性能模型。对于另一个问题，你可能会找到另一个冠军模型。如果你从第13章开始就一直在完成练习，你现在已经看到了这种情况的发生。记住，自第13章以来我们构建的每个模型都是为了成为你工具箱中的另一个工具，帮助你最大化解决时间序列预测问题的机会。
- en: 17.3 Next steps
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 下一步
- en: This is a rather short chapter, as it builds on concepts that we have already
    covered, such as the LSTM architecture and data windowing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这章相当简短，因为它建立在我们已经覆盖的概念之上，例如LSTM架构和数据窗口。
- en: The autoregressive LSTM model outperformed the simple LSTM multi-step model
    in our example, and it performed better than a CNN model. Again, this does not
    mean that an ARLSTM model will always outperform a CNN model or a simple LSTM
    model. Each problem is unique, and a different architecture might result in the
    best performance for a different problem. The important thing is that you now
    have a wide array of models you can test and adapt to each problem in order to
    find the best solution possible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，自回归LSTM模型优于简单的LSTM多步模型，并且比CNN模型表现更好。再次强调，这并不意味着ARLSTM模型总是会优于CNN模型或简单的LSTM模型。每个问题都是独特的，不同的架构可能对不同的问题产生最佳性能。重要的是，你现在有一系列可以测试和适应每个问题的模型，以找到可能的最佳解决方案。
- en: This brings the deep learning part of the book almost to a conclusion. In the
    next chapter, we’ll apply our knowledge of deep learning methods for time series
    forecasting in a capstone project. As before, a problem and dataset will be provided,
    and we must produce a forecasting model to solve the problem.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎完成了本书的深度学习部分。在下一章中，我们将应用我们对时间序列预测深度学习方法的了解，在一个综合项目中。和以前一样，将提供一个问题和数据集，我们必须生成一个预测模型来解决问题。
- en: 17.4 Exercises
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 练习
- en: 'In the exercises since chapter 13, we have built many models to forecast the
    air quality in Beijing using all three types of models (single-step, multi-step,
    and multi-output). Now we’ll build one last multi-step model using an ARLSTM model.
    The solution can be found on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从第13章开始的练习中，我们已经构建了许多模型来预测北京的空气质量，使用了三种类型的模型（单步、多步和多输出）。现在我们将使用一个ARLSTM模型构建最后一个多步模型。解决方案可以在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17)。
- en: 'For the multi-step model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多步模型：
- en: Build an ARLSTM model.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个ARLSTM模型。
- en: Plot its predictions.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测图。
- en: Evaluate the model using the mean absolute error (MAE) and store the MAE for
    comparison.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平均绝对误差（MAE）评估模型，并存储MAE以进行比较。
- en: Is the ARLSTM model the champion model?
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ARLSTM模型是冠军模型吗？
- en: Of course, feel free to experiment further. For example, you can vary the number
    of units to see how it impacts the model’s performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以自由地进一步实验。例如，你可以改变单元的数量，看看它如何影响模型的表现。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The autoregressive architecture in deep learning has given birth to state-of-the-art
    models, such as WaveNet and DeepAR.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的自回归架构催生了最先进的模型，如WaveNet和DeepAR。
- en: An autoregressive deep learning model generates a sequence of predictions, but
    each prediction is fed back into the model as an input.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自回归深度学习模型生成一系列预测，但每个预测都会作为输入反馈到模型中。
- en: A caveat regarding autoregressive deep learning models is that errors accumulate
    as the length of the sequence increases. Therefore, an early bad prediction can
    have a large effect on a late prediction.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于自回归深度学习模型的一个注意事项是，随着序列长度的增加，错误会累积。因此，一个早期的错误预测可能会对晚期的预测产生重大影响。
