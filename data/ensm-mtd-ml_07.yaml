- en: '5 Sequential ensembles: Gradient boosting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 顺序集成：梯度提升
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using gradient descent to optimize loss functions for training models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降优化训练模型的损失函数
- en: Implementing gradient boosting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现梯度提升
- en: Training histogram gradient-boosting models efficiently
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效训练直方图梯度提升模型
- en: Gradient boosting with the LightGBM framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 LightGBM 框架中使用梯度提升
- en: Avoiding overfitting with LightGBM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LightGBM 避免过拟合
- en: Using custom loss function with LightGBM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LightGBM 自定义损失函数
- en: The previous chapter introduced boosting, where we train weak learners sequentially
    and “boost” them into a strong ensemble model. An important sequential ensemble
    method introduced in chapter 4 is adaptive boosting (AdaBoost).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章介绍了提升，其中我们按顺序训练弱学习器并将它们“提升”为强大的集成模型。在第4章中介绍的一个重要顺序集成方法是自适应提升（AdaBoost）。
- en: AdaBoost is a foundational boosting model that trains a new weak learner to
    fix the misclassifications of the previous weak learner. It does this by maintaining
    and adaptively updating weights on training examples. These weights reflect the
    extent of misclassification and indicate priority training examples to the base-learning
    algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是一种基础的提升模型，它通过训练一个新的弱学习器来纠正前一个弱学习器的误分类。它是通过维护和自适应更新训练样本上的权重来做到这一点的。这些权重反映了误分类的程度，并指示给基础学习算法优先训练的样本。
- en: 'In this chapter, we look at an alternative to weights on training examples
    to convey misclassification information to a base-learning algorithm for boosting:
    loss function gradients. Recall that we use loss functions to measure how well
    a model fits each training example in the data set. The gradient of the loss function
    for a single example is called the *residual* and, as we’ll see shortly, captures
    the deviation between true and predicted labels. This error, or residual, of course,
    measures the amount of misclassification.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一种在训练样本上使用权重作为向基础学习算法传递误分类信息的替代方案：损失函数梯度。回想一下，我们使用损失函数来衡量模型在数据集中每个训练样本上的拟合程度。单个示例的损失函数梯度被称为
    *残差*，正如我们将很快看到的，它捕捉了真实标签和预测标签之间的偏差。这个错误，或残差，当然衡量了误分类的程度。
- en: In contrast to AdaBoost, which uses weights as a surrogate for residuals, gradient
    boosting uses these residuals directly! Thus, gradient boosting is another sequential
    ensemble method that aims to train weak learners over residuals (i.e., gradients).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用权重作为残差的代理的 AdaBoost 相比，梯度提升直接使用这些残差！因此，梯度提升是另一种旨在在残差（即梯度）上训练弱学习器的顺序集成方法。
- en: The framework of gradient boosting can be applied to any loss function, which
    means that any classification, regression, or ranking problem can be “boosted”
    using weak learners. This flexibility has been a key reason for the emergence
    and ubiquity of gradient boosting as a state-of-the-art ensemble approach. Several
    powerful packages and implementations of gradient boosting are available (LightGBM,
    CatBoost, XGBoost) and provide the ability to train models on big data efficiently
    via parallel computing and GPUs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的框架可以应用于任何损失函数，这意味着任何分类、回归或排序问题都可以使用弱学习器进行“提升”。这种灵活性是梯度提升作为最先进的集成方法出现和普及的关键原因。有几个强大的梯度提升包和实现（LightGBM、CatBoost、XGBoost）可用，并能够通过并行计算和GPU高效地在大数据上训练模型。
- en: This chapter is organized as follows. To gain a deeper understanding of gradient
    boosting, we need a deeper understanding of gradient descent. So, we kick off
    the chapter with an example of gradient descent that can be used to train a machine-learning
    model (section 5.1).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的组织如下。为了更深入地理解梯度提升，我们需要更深入地理解梯度下降。因此，我们以一个可以用来训练机器学习模型的梯度下降示例（第5.1节）开始本章。
- en: Section 5.2 aims to provide intuition for learning with residuals, which is
    at the heart of gradient boosting. Then, we implement our own version of gradient
    boosting and walk through it to understand how it combines gradient descent and
    boosting at every step to train a sequential ensemble. This section also introduces
    histogram-based gradient boosting, which essentially bins the training data to
    significantly accelerate tree learning and allows for scaling to larger data sets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第5.2节旨在提供使用残差进行学习的直观理解，这是梯度提升的核心。然后，我们实现自己的梯度提升版本，并逐步了解它是如何结合梯度下降和提升的每一步来训练顺序集成的。本节还介绍了基于直方图的梯度提升，它本质上将训练数据分箱，从而显著加速树学习，并允许扩展到更大的数据集。
- en: Section 5.3 introduces LightGBM, a free and open source gradient-boosting package
    and important tool for building and deploying real-world machine learning applications.
    In section 5.4, we see how to avoid overfitting with strategies such as early
    stopping and adapting the learning rate to train effective models with LightGBM
    and how to extend LightGBM to custom loss functions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第5.3节介绍了LightGBM，这是一个免费且开源的梯度提升包，也是构建和部署现实世界机器学习应用的重要工具。在第5.4节中，我们将看到如何通过早期停止和调整学习率等策略来避免过拟合，以及如何将LightGBM扩展到自定义损失函数。
- en: 'All of this leads us to a demonstration of how to use gradient boosting in
    a real-world task: document retrieval, which will be the focus of our chapter-concluding
    case study (section 5.5). Document retrieval, which is a form of information retrieval,
    is a key task in many applications and one we’ve all used at some time or another
    (e.g., web search engines).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都引导我们演示如何在现实世界任务中使用梯度提升：文档检索，这将是本章案例研究的重点（第5.5节）。文档检索作为一种信息检索形式，在许多应用中都是一个关键任务，我们都在某个时候使用过（例如，网络搜索引擎）。
- en: To understand gradient boosting, we first have to understand gradient descent,
    a simple yet effective approach that is widely used for training many machine-learning
    algorithms. This will help us contextualize the role gradient descent plays inside
    gradient boosting, both conceptually and algorithmically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解梯度提升，我们首先必须理解梯度下降，这是一种简单而有效的方法，广泛用于训练许多机器学习算法。这将帮助我们理解梯度下降在梯度提升中扮演的角色，无论是从概念上还是从算法上。
- en: 5.1 Gradient descent for minimization
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 最小化梯度下降
- en: We now delve into gradient descent, an optimization approach at the heart of
    many training algorithms. Understanding gradient descent will allow us to understand
    how the gradient-boosting framework ingeniously combines this optimization procedure
    with ensemble learning. Optimization, or the search for the “best,” is at the
    heart of many applications. Indeed, the search for the best model is at the heart
    of all machine learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在深入探讨梯度下降，这是许多训练算法核心的优化方法。理解梯度下降将使我们能够理解梯度提升框架如何巧妙地将这种优化过程与集成学习相结合。优化，或寻找“最佳”，是许多应用的核心。确实，寻找最佳模型是所有机器学习的核心。
- en: NOTE Learning problems are often cast as optimization problems. For example,
    training is essentially finding the best-fitting model given the data. If the
    notion of “best” is characterized by a loss function, then training is cast as
    a minimization problem because the best model corresponds to the lowest loss.
    Alternately, if the notion of “best” is characterized by a likelihood function,
    then training is cast as a maximization problem because the best model corresponds
    to the highest likelihood (or probability). Unless specified, we’ll characterize
    model quality or fit using loss functions, which will require us to perform minimization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：学习问题通常被表述为优化问题。例如，训练本质上是在给定数据的情况下找到最佳拟合模型。如果“最佳”的概念由损失函数来表征，那么训练就被表述为最小化问题，因为最佳模型对应于最低的损失。或者，如果“最佳”的概念由似然函数来表征，那么训练就被表述为最大化问题，因为最佳模型对应于最高的似然（或概率）。除非指定，我们将使用损失函数来表征模型质量或拟合度，这将要求我们进行最小化。
- en: Loss functions explicitly measure the fit of a model on a data set. Most often,
    we measure loss with respect to the true labels, by quantifying the error between
    the predicted and true labels. Thus, the best model will have the lowest error,
    or loss.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数明确衡量模型在数据集上的拟合度。通常，我们通过量化预测标签和真实标签之间的误差来衡量损失。因此，最佳模型将具有最低的误差或损失。
- en: You may be familiar with loss functions such as cross entropy (for classification)
    or mean squared error (for regression). We’ll revisit cross entropy in section
    5.4.3 and mean squared error in chapter 7\. Given a loss function, training is
    the search for the optimal model that minimizes the loss, as illustrated in figure
    5.1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉诸如交叉熵（用于分类）或均方误差（用于回归）之类的损失函数。我们将在第5.4.3节中回顾交叉熵，在第7章中回顾均方误差。给定一个损失函数，训练是寻找最小化损失的最优模型的过程，如图5.1所示。
- en: '![CH05_F01_Kunapuli](../Images/CH05_F01_Kunapuli.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Kunapuli](../Images/CH05_F01_Kunapuli.png)'
- en: Figure 5.1 An optimization procedure for finding the best model. Machine-learning
    algorithms search for the best model among all possible candidate models. The
    notion of “best” is quantified by the loss function, which evaluates the quality
    of a selected candidate using the labels and the data. Thus, machine-learning
    algorithms are essentially optimization procedures. Here, the optimization procedure
    sequentially identifies increasingly better models *f*[1], *f*[2], and the final
    model, *f*[3].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 寻找最佳模型的最优化过程。机器学习算法在所有可能的候选模型中寻找最佳模型。最佳的概念通过损失函数来量化，该函数使用标签和数据评估所选候选者的质量。因此，机器学习算法本质上是最优化过程。在这里，最优化过程依次识别越来越好的模型
    *f*[1]，*f*[2]，以及最终模型 *f*[3]。
- en: 'One example of such a search you may be familiar with is a grid search for
    parameter selection during training of, say, decision trees. With grid search,
    we choose among many modeling choices: number of leaves, maximum tree depth, and
    so on systematically and exhaustively over a grid of parameters.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉的一个这样的搜索例子是在训练决策树时进行参数选择的网格搜索。在网格搜索中，我们系统地、详尽地在参数网格上选择许多建模选择：叶子数、最大树深度等。
- en: Another, more effective optimization technique is gradient descent, which uses
    first derivative information, or gradients, to guide our search. In this section,
    we look at two examples of gradient descent. The first is a simple illustrative
    example to understand and visualize the basics of how gradient descent works.
    The second example demonstrates how gradient descent can be used on an actual
    loss function with data to train a machine-learning model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更有效的优化技术是梯度下降，它使用一阶导数信息，即梯度，来引导我们的搜索。在本节中，我们查看两个梯度下降的例子。第一个是一个简单的说明性例子，用于理解和可视化梯度下降的基本工作原理。第二个例子演示了如何使用实际损失函数和数据来训练机器学习模型。
- en: 5.1.1 Gradient descent with an illustrative example
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 带有示例的梯度下降
- en: We’ll use the Branin function, a commonly used example function, to illustrate
    how gradient descent works, before moving on to a more concrete case grounded
    in machine learning (section 5.1.2). The Branin function is a function of two
    variables (*w*[1] and *w*[2]), defined as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Branin函数，这是一个常用的示例函数，来展示梯度下降的工作原理，然后再转向一个更具体的基于机器学习的案例（第5.1.2节）。Branin函数是两个变量（*w*[1]和*w*[2]）的函数，定义为
- en: '![CH05_F01_Kunapuli-ch5-eqs-0x](../Images/CH05_F01_Kunapuli-ch5-eqs-0x.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Kunapuli-ch5-eqs-0x](../Images/CH05_F01_Kunapuli-ch5-eqs-0x.png)'
- en: where *a* = 1, *b* = 5.1/4*π*², *c* = 5/*π*, *r* = 6, *s* = 10, and *t* = 1/8*π*
    are fixed constants, which we won’t worry about. We can visualize this function
    by plotting a 3D plot of *w*[1] versus *w*[2] versus *f*(*w*[1],*w*[2]). Figure
    5.2 illustrates the 3D surface plot as well as the contour plot (i.e., the surface
    plot viewed from above).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a* = 1, *b* = 5.1/4*π*², *c* = 5/*π*, *r* = 6, *s* = 10, 和 *t* = 1/8*π*
    是固定的常数，我们不必担心。我们可以通过绘制 *w*[1] 与 *w*[2] 相对于 *f*(*w*[1],*w*[2]) 的3D图来可视化这个函数。图5.2展示了3D表面图以及等高线图（即从上方观看的表面图）。
- en: '![CH05_F02_Kunapuli](../Images/CH05_F02_Kunapuli.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Kunapuli](../Images/CH05_F02_Kunapuli.png)'
- en: Figure 5.2 The surface plot (left) and contour plot (right) of the Branin function.
    We can visually verify that this function has four minima, which are the centers
    of the elliptical regions in the contour plot.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 Branin函数的表面图（左）和等高线图（右）。我们可以直观地验证这个函数有四个最小值，这些最小值是等高线图中等椭圆区域的中心。
- en: 'Visualization of the Branin function shows us that it takes the smallest values
    at four different locations, which are called local minimizers, or minima. So
    how can we identify these local minima? There’s always the brute-force approach:
    we can make a grid over the variables *w*[1] and *w*[2] and evaluate *f*(*w*[1],*w*[2])
    at every possible combination exhaustively. However, there are several problems
    with this. First, how coarse or fine should our grid be? If our grid is too coarse,
    we may miss the minimizer in our search. If our grid is too fine, then we’ll have
    a very large number of grid points to search over, making our optimization procedure
    very slow.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Branin函数的可视化显示它在四个不同的位置取最小值，这些位置被称为局部最小值或最小值。那么我们如何识别这些局部最小值呢？总是有暴力方法：我们可以在变量
    *w*[1] 和 *w*[2] 上建立一个网格，并穷尽地评估每个可能组合的 *f*(*w*[1],*w*[2])。然而，这种方法有几个问题。首先，我们的网格应该有多粗或多细？如果我们的网格太粗，我们可能会错过搜索中的最小值。如果我们的网格太细，那么我们将有大量的网格点要搜索，这将使我们的最优化过程非常缓慢。
- en: Second, and more worrying, this approach ignores all the extra information inherent
    in the function itself, which could be quite helpful in guiding our search. For
    instance, the first derivatives, or the rates of change of *f*(*w*[1],*w*[2])
    with respect to *w*[1] and *w*[2], can be very helpful.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，并且更令人担忧的是，这种方法忽略了函数本身固有的所有额外信息，这些信息可能对我们的搜索非常有帮助。例如，一阶导数，即 *f*(*w*[1],*w*[2])
    关于 *w*[1] 和 *w*[2] 的变化率，可能非常有帮助。
- en: Understanding and implementing gradient descent
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和实现梯度下降
- en: The first derivative information is known as the gradient of *f*(*w*[1],*w*[2])
    and is a measure of the (local) slope of the function surface. More importantly,
    the gradient points in the direction of steepest ascent; that is, moving in the
    direction of steepest ascent will lead us to bigger values of *f*(*w*[1],*w*[2]).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶导数信息被称为 *f*(*w*[1],*w*[2]) 的梯度，它是函数表面（局部）斜率的度量。更重要的是，梯度指向最陡上升的方向；也就是说，沿着最陡上升方向移动将导致
    *f*(*w*[1],*w*[2]) 的更大值。
- en: 'If we want to use gradient information to find the minimizers, then we have
    to travel in the opposite direction of the gradient! This is precisely the simple,
    yet highly effective principle behind gradient descent: keep going in the direction
    of the negative gradient, and you’ll end up at a (local) minimizer.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用梯度信息来找到最小值，那么我们必须沿着梯度的反方向前进！这正是梯度下降简单而高效的原则：继续沿着负梯度方向前进，最终你会到达一个（局部）最小值。
- en: 'We can formalize this intuition in the following pseudocode, which describes
    the steps of gradient descent. As shown, gradient descent is an iterative procedure
    that steadily moves toward a local minimizer by moving in the direction of steepest
    descent, that is, the negative gradient:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下伪代码形式化这种直觉，它描述了梯度下降的步骤。如图所示，梯度下降是一个迭代过程，通过沿着最陡下降方向（即负梯度）移动，稳步向局部最小值移动：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The gradient descent procedure is fairly straightforward. First, we initialize
    our solution (and call it **w**[old]); this can be a random initialization or
    perhaps a more sophisticated guess. Starting from this initial guess, we compute
    the negative gradient, which tells us which direction we want to go.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降过程相当直接。首先，我们初始化我们的解（并称之为 **w**[old]）；这可以是随机初始化，或者可能是一个更复杂的猜测。从这个初始猜测开始，我们计算负梯度，这告诉我们想要前进的方向。
- en: Next, we compute a step length, which tells us the distance or how far we want
    to go in the direction of the negative gradient. Computing the step length is
    important, as it ensures that we don’t overshoot our solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算步长，这告诉我们沿着负梯度方向移动的距离或距离有多远。计算步长很重要，因为它确保我们不会超过我们的解。
- en: The step length computation is another optimization problem, where we want to
    identify a scalar *α* > 0 such that traveling along the gradient g for a distance
    of *α* produces the biggest decrease in the loss function. Formally, this is known
    as a *line search problem* and is often used to efficiently select step lengths
    during optimization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 步长计算是另一个优化问题，我们的目标是找到一个正标量 *α* > 0，使得沿着梯度 g 移动距离 *α* 可以使损失函数的最大减少。形式上，这被称为 *线搜索问题*，通常用于优化过程中高效地选择步长。
- en: NOTE Many optimization packages and tools (e.g., scipy.optimize used in this
    chapter) provide exact and approximate line search functions that can be used
    to identify step lengths. Alternately, step length can also be set according to
    some predetermined strategy, often for efficiency. In machine learning, the step
    length is often called the *learning rate* and is represented by the Greek letter
    eta (*η*).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：许多优化包和工具（例如，本章中使用的 scipy.optimize）提供了精确和近似的线搜索函数，可用于识别步长。或者，步长也可以根据某些预定的策略设置，通常是为了效率。在机器学习中，步长通常被称为
    *学习率*，用希腊字母 η (*η*) 表示。
- en: With a direction and distance, we can take this step and update our solution
    guess to **w**[new]. Once we get there, we check for convergence. There are several
    tests for convergence; here, we assume convergence if the solution doesn’t change
    much between consecutive iterations. If converged, then we’ve found a local minimizer.
    If not, then we iterate again from **w**[new]. The following listing shows how
    to perform gradient descent.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个方向和距离，我们可以采取这一步，并将我们的解猜测更新为 **w**[new]。一旦到达那里，我们检查收敛性。有几种收敛性测试；这里，我们假设在连续迭代之间解变化不大时收敛。如果收敛，那么我们就找到了一个局部最小值。如果没有，那么我们从
    **w**[new] 再次迭代。以下列表显示了如何执行梯度下降。
- en: Listing 5.1 Gradient descent
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 梯度下降
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Gradient descent requires a function f and its gradient g.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 梯度下降需要一个函数 f 和其梯度 g。
- en: ❷ Initializes gradient descent to “not converged”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将梯度下降初始化为“未收敛”
- en: ❸ Computes the negative gradient
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算负梯度
- en: ❹ Normalizes gradient to unit length
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将梯度归一化到单位长度
- en: ❺ Computes step length using line search
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用线搜索计算步长
- en: ❻ If the line search fails, make it 1.0.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 如果线搜索失败，则将其设置为 1.0。
- en: ❼ Computes the update
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算更新
- en: ❽ Computes the change from the previous iteration
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算与前一次迭代的改变
- en: ❾ Converges if change is small or maximum iterations are reached
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 当变化很小或达到最大迭代次数时收敛
- en: ❿ Gets ready forthe next iteration
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 准备进行下一次迭代
- en: We can test drive this gradient descent procedure on the Branin function. To
    do this, in addition to the function itself, we’ll also need its gradient. We
    can compute the gradient explicitly by dredging up the basics of calculus (if
    not the memories of it).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Branin 函数上测试这个梯度下降过程。为此，除了函数本身之外，我们还需要其梯度。我们可以通过挖掘微积分的基础（如果记忆中还有的话）来显式地计算梯度。
- en: 'The gradient is a vector with two components: the gradient of *f* with respect
    to *w*[1] and *w*[2], respectively. With this gradient, we can compute the direction
    of steepest increase everywhere:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是一个有两个分量的向量：*f* 对 *w*[1] 和 *w*[2] 的梯度，分别。有了这个梯度，我们可以计算在每处的最大增加方向：
- en: '![CH05_F02_Kunapuli-eqs-4x](../Images/CH05_F02_Kunapuli-eqs-4x.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Kunapuli-eqs-4x](../Images/CH05_F02_Kunapuli-eqs-4x.png)'
- en: 'We can implement the Branin function and its gradient as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像下面这样实现 Branin 函数及其梯度：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the function and the gradient, listing 5.1 also requires an
    initial guess x_init. Here, we’ll initialize gradient descent with w_ini=[-4,-5]''
    (transposed because these are column vectors, mathematically speaking). Now, we
    can call the gradient descent procedure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了函数和梯度之外，列表 5.1 还需要一个初始猜测 x_init。在这里，我们将使用 w_ini=[-4,-5]' (转置，因为这些是列向量，从数学的角度讲)
    来初始化梯度下降。现在，我们可以调用梯度下降过程：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Gradient descent returns an optimal solution w_optimal=[3.14, 2.27] and the
    optimization path w_path, which is the sequence of intermediate solutions that
    the procedure iterated through on its way to the optimal solution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降返回一个最优解 w_optimal=[3.14, 2.27] 和优化路径 w_path，这是在达到最优解的过程中，程序迭代通过的中间解的序列。
- en: And voila! In figure 5.3, we see that gradient descent is able to reach one
    of the four local minimizers of the Branin function. There are several important
    things to note about gradient descent, as we’ll discuss next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！在图 5.3 中，我们看到梯度下降能够达到 Branin 函数的四个局部最小值之一。关于梯度下降，有几个重要的事情需要注意，我们将在下面讨论。
- en: '![CH05_F03_Kunapuli](../Images/CH05_F03_Kunapuli.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_Kunapuli](../Images/CH05_F03_Kunapuli.png)'
- en: Figure 5.3 The figure on the left shows the full descent path of gradient descent,
    starting from [-4,-5]' (square) and converging to one of the local minima (circle).
    The figure on the right shows the zoomed-in version of the same descent path as
    gradient descent approaches the solution. Note that the gradient steps become
    smaller, and the descent algorithm tends to zigzag as it approaches the solution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 左图显示了梯度下降的完整下降路径，从 [-4,-5]' (正方形) 开始，收敛到局部最小值之一 (圆形)。右图显示了当梯度下降接近解时，相同下降路径的放大版本。请注意，梯度步骤变得越小，下降算法在接近解时倾向于曲折。
- en: Properties of gradient descent
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的性质
- en: First, observe that the gradient steps become smaller and smaller as we approach
    one of the minimizers. This is because gradients vanish at minimizers. More importantly,
    gradient descent exhibits zigzagging behavior because the gradient doesn’t point
    at the local minimizer itself; rather, it points in the direction of steepest
    ascent (or descent, if negative).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，观察当我们接近一个最小值时，梯度步骤变得越来越小。这是因为梯度在最小值处消失。更重要的是，梯度下降表现出曲折行为，因为梯度并不指向局部最小值本身；相反，它指向最陡上升（或下降，如果为负）的方向。
- en: The gradient at a point essentially captures local information, that is, the
    nature of the function close to that point. Gradient descent chains several such
    gradient steps to get to a minimizer. When the gradient descent has to pass through
    steep valleys, it’s tendency to use local information causes it to bounce around
    the valley walls as it moves toward the minimum.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在某一点的梯度本质上捕捉了局部信息，即该点附近函数的性质。梯度下降通过连续几个这样的梯度步骤来达到最小值。当梯度下降必须穿过陡峭的山谷时，它倾向于使用局部信息，导致它在移动向最小值的过程中在山谷两侧弹跳。
- en: Second, gradient descent converged to one of the four local minimizers of the
    Branin function. You can get it to converge to a different minimizer by changing
    the initialization. Figure 5.4 illustrates various gradient descent paths for
    different initializations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，梯度下降收敛到了Branin函数的四个局部最小值之一。通过改变初始化，你可以让它收敛到不同的最小值。图5.4展示了不同初始化下的各种梯度下降路径。
- en: 'The sensitivity of gradient descent to initialization is illustrated in figure
    5.4, where different random initializations cause gradient descent to converge
    to different local minimizers. This behavior may be familiar to those of you who
    have used k-means clustering: different initializations will often produce different
    clusterings, each of which is a different local solution.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4展示了梯度下降对初始化的敏感性，其中不同的随机初始化导致梯度下降收敛到不同的局部最小值。这种行为对于那些使用过k-means聚类的人来说可能很熟悉：不同的初始化通常会产生不同的聚类，每个聚类都是一个不同的局部解。
- en: '![CH05_F04_Kunapuli](../Images/CH05_F04_Kunapuli.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04_Kunapuli](../Images/CH05_F04_Kunapuli.png)'
- en: Figure 5.4 Different initializations will cause gradient descent to reach different
    local minima.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4不同的初始化会导致梯度下降达到不同的局部最小值。
- en: An interesting challenge with gradient descent is in identifying the appropriate
    initialization as different initializations lead gradient descent to different
    local minimizers. From an optimization perspective, it’s not always easy to identify
    the correct initialization beforehand.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的一个有趣挑战在于确定适当的初始化，因为不同的初始化会导致梯度下降收敛到不同的局部最小值。从优化的角度来看，事先确定正确的初始化并不总是容易的。
- en: However, from a machine-learning perspective, the different local solutions
    may demonstrate the same generalization behavior. That is, the locally optimal
    learned models all have similar predictive performance. This situation is commonly
    encountered with neural networks and deep learning, which is why training procedures
    for many deep models are initialized from pretrained solutions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从机器学习的角度来看，不同的局部解可能表现出相同的一般化行为。也就是说，局部最优的学习模型都具有相似的预测性能。这种情况在神经网络和深度学习中很常见，这也是为什么许多深度模型的训练过程都是从预训练的解决方案开始的。
- en: TIP The sensitivity of gradient descent to initialization depends on the type
    of function being optimized. If the function is convex or cup-shaped everywhere,
    then any local minimizer that gradient descent identifies will always be a global
    minimizer too! This is the case with models learned by support vector machine
    (SVM) optimizers. However, a good initial guess is still important as it may cause
    the algorithm to converge faster. Many real-world problems are typically non-convex
    and have several local minima. Gradient descent will converge to one of them,
    depending on the initialization and shape of the function in the locality of the
    initial guess. The objective function of k-means clustering is non-convex, which
    is why different initializations produce different clusterings. See *Algorithms
    for Optimization* by Mykel Kochenderfer and Tim Wheeler (MIT Press, 2019) for
    a solid and hands-on introduction to optimization.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：梯度下降对初始化的敏感性取决于被优化的函数类型。如果函数在所有地方都是凸的或杯状的，那么梯度下降识别的任何局部最小值也将总是全局最小值！这是支持向量机（SVM）优化器学习模型的情况。然而，一个好的初始猜测仍然很重要，因为它可能会使算法更快地收敛。许多现实世界的问题通常是非凸的，并且有几个局部最小值。梯度下降将收敛到其中之一，这取决于初始化和初始猜测局部函数的形状。k-means聚类的目标函数是非凸的，这就是为什么不同的初始化会产生不同的聚类。参见Mykel
    Kochenderfer和Tim Wheeler所著的《优化算法》（MIT Press，2019），这是一本关于优化的扎实且实用的入门书籍。
- en: 5.1.2 Gradient descent over loss functions for training
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 梯度下降在训练损失函数上的应用
- en: 'Now that we understand the basics of how gradient descent works on a simple
    example (the Branin function), let’s build a classification task from scratch
    using a loss function of our own. Then, we’ll use gradient descent to train a
    model. First, we create a 2D classification problem as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了梯度下降在简单示例（Branin函数）上的基本工作原理，让我们从头开始构建一个分类任务，并使用我们自己的损失函数。然后，我们将使用梯度下降来训练模型。首先，我们创建一个如下所示的2D分类问题：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This synthetic classification data set is visualized in figure 5.5.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个合成分类数据集在图5.5中进行了可视化。
- en: '![CH05_F05_Kunapuli](../Images/CH05_F05_Kunapuli.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Kunapuli](../Images/CH05_F05_Kunapuli.png)'
- en: Figure 5.5 A (nearly) linearly separable two-class data set over which we’ll
    train a classifier. The positive examples have labels y = 1, and the negative
    examples have labels y = 0.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 一个（几乎）线性可分的两类数据集，我们将在此数据集上训练一个分类器。正例的标签为 y = 1，而负例的标签为 y = 0。
- en: We specifically create a linearly separable data set (with some noise, of course)
    so that we can train a linear separator or classification function. This will
    keep our loss function formulation simple and make our gradients easy to calculate.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别创建了一个线性可分的数据集（当然，其中包含一些噪声），这样我们就可以训练一个线性分离器或分类函数。这将使我们的损失函数公式简单，并使我们的梯度易于计算。
- en: 'The classifier we want to train, *h*[w](*x*), takes 2D data points *x* = [*x*[1],*x*[2]]''
    and returns a prediction using a linear function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要训练的分类器 *h*[w](*x*) 接受 2D 数据点 *x* = [*x*[1],*x*[2]]' 并使用线性函数返回一个预测：
- en: '![CH05_F05_Kunapuli-eqs-5x](../Images/CH05_F05_Kunapuli-eqs-5x.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Kunapuli-eqs-5x](../Images/CH05_F05_Kunapuli-eqs-5x.png)'
- en: 'The classifier is parameterized by *w* *=* [*w*[1]*, w*[2]]'', which we have
    to learn using the training examples. To learn, we’ll need a loss function over
    the true label and predicted label. We’ll use the familiar squared loss (or squared
    error) that measures the cost for an individual labeled training example (*x*,*y*):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器由 *w* *=* [*w*[1]*, w*[2]]' 参数化，我们必须使用训练示例来学习它。为了学习，我们需要一个关于真实标签和预测标签的损失函数。我们将使用熟悉的平方损失（或平方误差），它衡量单个标记训练示例
    (*x*,*y*) 的成本：
- en: '![CH05_F05_Kunapuli-eqs-6x](../Images/CH05_F05_Kunapuli-eqs-6x.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Kunapuli-eqs-6x](../Images/CH05_F05_Kunapuli-eqs-6x.png)'
- en: 'The squared loss function computes the loss between the prediction of the current
    candidate model (*h*[w]) on a single training example (*x*) and its true label
    (*y*). For the *n* training examples in the data set, the overall loss can be
    written as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失函数计算当前候选模型 (*h*[w]) 在单个训练示例 (*x*) 上的预测与其真实标签 (*y*) 之间的损失。对于数据集中的 *n* 个训练示例，整体损失可以表示如下：
- en: '![CH05_F05_Kunapuli-eqs-7x](../Images/CH05_F05_Kunapuli-eqs-7x.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Kunapuli-eqs-7x](../Images/CH05_F05_Kunapuli-eqs-7x.png)'
- en: The expression for the overall loss is just the sum of the individual losses
    of the *n* training examples in the data set.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 整体损失的表达式只是数据集中 *n* 个训练示例的个别损失的求和。
- en: The expression 1/2(*y* – *Xw*)’(*y* – *Xw*) is simply the *vectorized* version
    of the overall loss, which uses dot products instead of loops. In the vectorized
    version, the boldface *y* is an *n* × 1 vector of true labels; x is an *n* × 2
    data matrix, where each row is a 2D training example; and *w* is a 2 × 1 model
    vector that we want to learn.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式 1/2(*y* – *Xw*)’(*y* – *Xw*) 是整体损失的 *向量化* 版本，它使用点积而不是循环。在向量化版本中，粗体的 *y*
    是一个 *n* × 1 的真实标签向量；x 是一个 *n* × 2 的数据矩阵，其中每一行是一个 2D 训练示例；而 *w* 是一个我们想要学习的 2 ×
    1 模型向量。
- en: 'As before, we’ll need the gradient of the loss function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要损失函数的梯度：
- en: '![CH05_F05_Kunapuli-eqs-9x](../Images/CH05_F05_Kunapuli-eqs-9x.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Kunapuli-eqs-9x](../Images/CH05_F05_Kunapuli-eqs-9x.png)'
- en: 'We implement the vectorized versions because they are more compact and more
    efficient as they avoid explicit loops for summation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现向量化版本，因为它们更紧凑、更高效，避免了显式的循环求和：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TIP If you’re alarmed at the prospect of hand-computing gradients, despair not;
    alternatives are available that can numerically approximate the gradients and
    are used for training many machine-learning models, including deep learning and
    gradient boosting. These alternatives rely on finite difference approximations
    or autodifferentiation (which is based on the first principles of numerical calculus
    and linear algebra) to compute gradients efficiently. An easy-to-use tool is the
    function scipy.optimize.approx_fprime available in the scipy scientific package.
    A far more powerful tool is JAX ([https://github.com/google/jax](https://github.com/google/jax)),
    which is free and open source. JAX is intended for computing gradients of complex
    functions representing deep neural networks with many layers. JAX can differentiate
    through loops, branches, and even recursion, and it has GPU support for large-scale
    gradient computations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你对手动计算梯度感到担忧，不要绝望；有其他方法可以数值近似梯度，并且被用于训练许多机器学习模型，包括深度学习和梯度提升。这些替代方案依赖于有限差分近似或自动微分（它基于数值计算和线性代数的基本原理）来有效地计算梯度。一个易于使用的工具是
    scipy 科学包中可用的函数 scipy.optimize.approx_fprime。一个更强大的工具是 JAX ([https://github.com/google/jax](https://github.com/google/jax))，它是免费且开源的。JAX
    旨在计算表示具有许多层的深度神经网络的复杂函数的梯度。JAX 可以通过循环、分支甚至递归进行微分，并且它支持大规模梯度计算的 GPU。
- en: What does our loss function look like? We can visualize it as before, as shown
    in figure 5.6\. This loss function is bowl-shaped and convex, and has one global
    minimum, which is our optimal classifier, *w*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数看起来是什么样子？我们可以像之前一样可视化它，如图 5.6 所示。这个损失函数是碗形的，且是凸的，它有一个全局最小值，这就是我们的最优分类器，*w*。
- en: '![CH05_F06_Kunapuli](../Images/CH05_F06_Kunapuli.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F06_Kunapuli](../Images/CH05_F06_Kunapuli.png)'
- en: Figure 5.6 The overall squared loss over the entire training set, visualized
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 整个训练集上的整体平方损失可视化
- en: 'As before, we perform gradient descent, this time initializing at *w* = [0.0,-0.99]''
    using the following code snippet, with the gradient descent path shown in figure
    5.7:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们执行梯度下降，这次初始化为 *w* = [0.0,-0.99]'，使用以下代码片段，梯度下降路径如图 5.7 所示：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Gradient descent has learned a final learned model: *w*^* = [0.174,0.119]''.
    The linear classifier learned by our gradient descent procedure is visualized
    in figure 5.7 (right). In addition to visually confirming that the gradient descent
    procedure learned a useful model, we can also compute training accuracy.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降已经学习了一个最终的学习模型：*w*^* = [0.174,0.119]'. 通过我们的梯度下降过程学习到的线性分类器在图 5.7（右）中进行了可视化。除了通过视觉确认梯度下降过程学习到了有用的模型外，我们还可以计算训练准确率。
- en: 'Recall that linear classifier *h*[w](*x*) = *w*[1]*x*[1] + *w*[2]*x*[2] returns
    real-valued predictions, which we need to convert to 0 or 1\. This is straightforward:
    we simply assign all positive predictions (examples above the line, geometrically)
    to the class *y*[pred] = 1 and assign negative predictions (examples below the
    line, geometrically) to the class *y*[pred] = 0:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，线性分类器 *h*[w](*x*) = *w*[1]*x*[1] + *w*[2]*x*[2] 返回的是实数值预测，我们需要将其转换为 0 或
    1。这是直截了当的：我们只需将所有正预测（几何上位于线上的示例）分配给类别 *y*[pred] = 1，并将负预测（几何上位于线下的示例）分配给类别 *y*[pred]
    = 0：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Success! The training accuracy learned by our implementation of gradient descent
    is 99.5%.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们实现的梯度下降学习到的训练准确率为 99.5%。
- en: '![CH05_F07_Kunapuli](../Images/CH05_F07_Kunapuli.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Kunapuli](../Images/CH05_F07_Kunapuli.png)'
- en: 'Figure 5.7 Left: Gradient descent over our squared loss function starting at
    w_init (square) and converging at the optimal solution (circle). Right: The learned
    model *w*^* = [0.174,0.119]'' is a linear classifier that fits the training data
    quite well as it separates both the classes.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 左：从 w_init（正方形）开始，在最优解（圆形）处收敛的梯度下降过程。右：学习到的模型 *w*^* = [0.174,0.119]' 是一个线性分类器，它很好地拟合了训练数据，因为它将两个类别分开。
- en: Now that we understand how gradient descent uses gradient information sequentially
    to minimize a loss function during training, let’s see how we can extend it with
    boosting to train a sequential ensemble.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了梯度下降如何在训练过程中使用梯度信息依次最小化损失函数，让我们看看我们如何通过提升（boosting）来扩展它以训练一个序列集成。
- en: '5.2 Gradient boosting: Gradient descent + boosting'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 梯度提升：梯度下降 + 提升法
- en: In gradient boosting, we aim to train a sequence of weak learners that approximate
    the gradient at each iteration. Gradient boosting and its successor, Newton boosting,
    are currently considered state-of-the-art ensemble methods and are widely implemented
    and deployed for several tasks in diverse application areas.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升中，我们的目标是训练一系列弱学习器，在每个迭代中逼近梯度。梯度提升及其继任者牛顿提升目前被认为是最先进的集成方法，并且在多个应用领域的多个任务中得到了广泛实现和部署。
- en: 'We’ll first look at the intuition of gradient boosting and contrast it with
    another familiar boosting method: AdaBoost. Armed with this intuition, as before,
    we’ll implement our own version of gradient boosting to visualize what is really
    going on under the hood.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将探讨梯度提升的直观理解，并将其与另一种熟悉的提升方法：AdaBoost进行比较。有了这种直观理解，就像之前一样，我们将实现我们自己的梯度提升版本，以可视化底层真正发生的事情。
- en: 'Then, we’ll look at two gradient-boosting approaches available in scikit-learn:
    the GradientBoostingClassifier, and its more scalable counterpart, HistogramGradientBoostingClassifer.
    This will set us up nicely for LightGBM, a power- ful and flexible implementation
    of gradient boosting widely used for practical applications.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨scikit-learn中可用的两种梯度提升方法：GradientBoostingClassifier及其更可扩展的对应版本HistogramGradientBoostingClassifer。这将为我们学习LightGBM打下良好的基础，LightGBM是一种强大且灵活的梯度提升实现，广泛用于实际应用。
- en: '5.2.1 Intuition: Learning with residuals'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 直观理解：使用残差进行学习
- en: The key component of sequential ensemble methods, such as AdaBoost and gradient
    boosting, is that they aim to train a new weak estimator at each iteration to
    fix the errors made by the weak estimator at the previous iteration. However,
    AdaBoost and gradient boosting train new weak estimators on poorly classified
    examples in rather different ways.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 序列集成方法，如AdaBoost和梯度提升的关键组成部分是，它们旨在每个迭代中训练一个新的弱估计器来纠正前一个迭代中弱估计器所犯的错误。然而，AdaBoost和梯度提升在训练新的弱估计器上对分类不良的例子有相当不同的方式。
- en: AdaBoost vs. gradient boosting
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost与梯度提升的比较
- en: AdaBoost identifies high-priority training examples by weighting them such that
    misclassified examples have higher weights than correctly classified ones. In
    this way, AdaBoost can tell the base-learning algorithm which training examples
    it should focus on in the current iteration. In contrast, gradient boosting uses
    residuals or errors (between the true and predicted labels) to tell the base-learning
    algorithm which training examples it should focus on in the next iteration.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost通过给误分类的例子赋予比正确分类的例子更高的权重来识别高优先级的训练例子。这样，AdaBoost可以告诉基础学习算法在当前迭代中应该关注哪些训练例子。相比之下，梯度提升使用残差或误差（真实标签和预测标签之间的差异）来告诉基础学习算法在下一个迭代中应该关注哪些训练例子。
- en: 'What exactly is a residual? For a training example, it’s simply the error between
    the true label and the corresponding prediction. Intuitively, a correctly classified
    example must have a small residual, and a misclassified example must have a large
    residual. More concretely, if a classifier *h* makes a prediction *h*(*x*) on
    a training example *x*, a naïve way of computing the residual would be to directly
    measure the difference between them:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 残差究竟是什么？对于一个训练例子，它仅仅是真实标签和相应预测之间的误差。直观上，一个正确分类的例子必须有一个小的残差，而一个误分类的例子必须有一个大的残差。更具体地说，如果一个分类器*h*对一个训练例子*x*做出预测*h*(*x*)，计算残差的一个简单方法就是直接测量它们之间的差异：
- en: '![CH05_F07_Kunapuli-eqs-10x](../Images/CH05_F07_Kunapuli-eqs-10x.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Kunapuli-eqs-10x](../Images/CH05_F07_Kunapuli-eqs-10x.png)'
- en: 'Recall the squared loss function we were using previously: *f*[loss](*y*, *x*)
    = ½(*y* – *h*(*x*))². The gradient of this loss *f* with respect to our model
    *h* is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们之前使用的平方损失函数：*f*[损失](*y*, *x*) = ½(*y* – *h*(*x*))²。这个损失函数*f*相对于我们的模型*h*的梯度如下：
- en: '![CH05_F07_Kunapuli-eqs-12x](../Images/CH05_F07_Kunapuli-eqs-12x.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Kunapuli-eqs-12x](../Images/CH05_F07_Kunapuli-eqs-12x.png)'
- en: The negative gradient of the squared loss is exactly the same as our residual!
    This means that the gradient of the loss function is a measure of the misclassification
    and is the residual.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失的负梯度正好与我们的残差相同！这意味着损失函数的梯度是误分类的度量，也是残差。
- en: Training examples that are badly misclassified will have large gradients (residuals)
    as the gap between the true and predicted labels will be large. Training examples
    that are correctly classified will have small gradients.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 严重误分类的训练例子将会有大的梯度（残差），因为真实标签和预测标签之间的差距会很大。正确分类的训练例子将会有小的梯度。
- en: This is evident in figure 5.8, where the magnitude and sign of the residuals
    indicate the training examples that require the most attention. Thus, analogous
    to AdaBoost, we have a measure of how badly each training example is misclassified.
    How can we use this information to train a weak learner?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图5.8中很明显，其中残差的幅度和符号指示了需要最多关注的训练示例。因此，类似于AdaBoost，我们有衡量每个训练示例错误分类程度的一个指标。我们如何利用这个信息来训练一个弱学习器？
- en: '![CH05_F08_Kunapuli](../Images/CH05_F08_Kunapuli.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F08_Kunapuli](../Images/CH05_F08_Kunapuli.png)'
- en: Figure 5.8 Comparing AdaBoost (left) to gradient boosting (right). Both approaches
    train weak estimators that improve classification performance on misclassified
    examples. AdaBoost uses weights, with misclassified examples being assigned higher
    weights. Gradient boosting uses residuals, with misclassified examples having
    higher residuals. The residuals are nothing but negative loss gradients.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 比较AdaBoost（左）与梯度提升（右）。两种方法都训练弱估计器，以改善对错误分类示例的分类性能。AdaBoost使用权重，错误分类的示例被分配更高的权重。梯度提升使用残差，错误分类的示例具有更高的残差。残差不过是负损失梯度。
- en: Using weak learners to approximate gradients
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用弱学习器来近似梯度
- en: Continuing our analogy with AdaBoost, recall that once it assigns weights to
    all the training examples, we have a weight-augmented data set (**x**[i],*y*[i],*D*[i])
    with i = 1, ..., *n*, of weighted examples. Thus, training a weak learner in AdaBoost
    is an instance of a weighted classification problem. With an appropriate base
    classification algorithm, AdaBoost trains a weak classifier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的AdaBoost类比，回想一下，一旦它为所有训练示例分配了权重，我们就得到了一个带有加权示例的权重增强数据集（**x**[i], *y*[i],
    *D*[i]），其中i = 1, ..., *n*。因此，在AdaBoost中训练弱学习器是加权分类问题的一个实例。使用适当的基础分类算法，AdaBoost训练一个弱分类器。
- en: In gradient boosting, we no longer have weights *D*[i]. Instead, we have residuals
    (or negative loss gradients), *r*[i], and a residual-augmented data set (*x*[i,]
    *r*[i]). Instead of classification labels (*y*[i] = 0 or 1) and example weights
    (*D*[i]), each training example now has an associated residual, which can be viewed
    as a real-valued label.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升中，我们不再有权重 *D*[i]。相反，我们有残差（或负损失梯度）*r*[i]和一个残差增强数据集(*x*[i,] *r*[i])。而不是分类标签(*y*[i]
    = 0或1)和示例权重(*D*[i])，每个训练示例现在都有一个相关的残差，这可以被视为一个实值标签。
- en: Thus, training a weak learner in gradient boosting is an instance of a regression
    problem, which requires a base-learning algorithm such as decision-tree regression.
    When trained, weak estimators in gradient boosting can be viewed as approximate
    gradients.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在梯度提升中训练弱学习器是回归问题的一个实例，这需要一个如决策树回归的基础学习算法。当训练后，梯度提升中的弱估计器可以被视为近似梯度。
- en: Figure 5.9 illustrates how gradient descent differs from gradient boosting and
    how gradient boosting is conceptually similar to gradient descent. The key difference
    between the two is that gradient descent directly uses the negative gradient,
    while gradient boosting trains a weak regressor to approximate the negative gradient.
    We now have all the ingredients to formalize the algorithmic steps of gradient
    boosting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9说明了梯度下降与梯度提升的不同之处，以及梯度提升在概念上与梯度下降的相似之处。这两种方法的关键区别在于，梯度下降直接使用负梯度，而梯度提升通过训练一个弱回归器来近似负梯度。我们现在拥有了形式化梯度提升算法步骤的所有要素。
- en: '![CH05_F09_Kunapuli](../Images/CH05_F09_Kunapuli.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F09_Kunapuli](../Images/CH05_F09_Kunapuli.png)'
- en: Figure 5.9 Comparing gradient descent (left) to gradient boosting (right). At
    iteration *t*, gradient descent updates the model using the negative gradient,
    -*g*[t]. At iteration *t*, gradient boosting approximates the negative gradient
    by training a weak regressor, *h*[t], on the negative residuals -*r*^t[i]. The
    step length *α*[t] in gradient descent is equivalent to the hypothesis weight
    of each base estimator in a sequential ensemble.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 比较梯度下降（左）与梯度提升（右）。在迭代*t*时，梯度下降使用负梯度-*g*[t]更新模型。在迭代*t*时，梯度提升通过在负残差-*r*^t[i]上训练弱回归器*h*[t]来近似负梯度。梯度下降中的步长*α*[t]相当于序列集成中每个基础估计器的假设权重。
- en: NOTE Gradient boosting aims to fit a weak estimator to residuals, which are
    real-valued. Thus, gradient boosting will *always* need to use a regression algorithm
    as a base-learning algorithm and learn regressors as weak estimators. This will
    be the case even when the loss function corresponds to binary or multiclass classification,
    regression, or ranking.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：梯度提升旨在将弱估计器拟合到残差，这些残差是实值。因此，梯度提升将*始终*需要使用回归算法作为基础学习算法，并学习回归器作为弱估计器。即使损失函数对应于二元或多元分类、回归或排序，也是如此。
- en: Gradient boosting is gradient descent + boosting
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是梯度下降 + 提升树
- en: 'To summarize, gradient boosting combines gradient descent and boosting:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，梯度提升结合了梯度下降和提升：
- en: Like AdaBoost, gradient boosting trains a weak learner to fix the mistakes made
    by the previous weak learner. AdaBoost uses example weights to focus learning
    on misclassified examples, while gradient boosting uses example residuals to do
    the same.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AdaBoost类似，梯度提升训练一个弱学习器来纠正前一个弱学习器犯的错误。AdaBoost使用示例权重来关注错误分类的示例，而梯度提升使用示例残差来完成同样的任务。
- en: Like gradient descent, gradient boosting updates the current model with gradient
    information. Gradient descent uses the negative gradient directly, while gradient
    boosting trains a weak regressor over the negative residuals to approximate the
    gradient.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与梯度下降类似，梯度提升使用梯度信息更新当前模型。梯度下降直接使用负梯度，而梯度提升在负残差上训练一个弱回归器来近似梯度。
- en: Finally, both gradient descent and gradient boosting are additive algorithms;
    that is, they generate sequences of intermediate terms that are additively combined
    to produce the final model. This is apparent in figure 5.10.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，梯度下降和梯度提升都是加性算法；也就是说，它们生成一系列中间项，这些中间项通过加性组合来产生最终模型。这在图5.10中很明显。
- en: '![CH05_F10_Kunapuli](../Images/CH05_F10_Kunapuli.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Kunapuli](../Images/CH05_F10_Kunapuli.png)'
- en: Figure 5.10 Both gradient descent (left) and gradient boosting (right) produce
    a sequence of updates. In gradient descent, each iteration additively updates
    the current model with the new negative gradient (-*g*[t]). In gradient boosting,
    each iteration additively updates the current model with the new approximate weak
    gradient estimate (the regression tree, *h*[t]).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 梯度下降（左）和梯度提升（右）都产生一系列更新。在梯度下降中，每次迭代通过添加新的负梯度（-*g*[t]）来更新当前模型。在梯度提升中，每次迭代通过添加新的近似弱梯度估计（回归树，*h*[t]）来更新当前模型。
- en: 'At each iteration, AdaBoost, gradient descent, and gradient boosting all update
    the current model using an additive expression of the following form:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，AdaBoost、梯度下降和梯度提升都使用以下形式的加性表达式来更新当前模型：
- en: '![CH05_F10_Kunapuli-eqs-13xa](../Images/CH05_F10_Kunapuli-eqs-13xa.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Kunapuli-eqs-13xa](../Images/CH05_F10_Kunapuli-eqs-13xa.png)'
- en: 'More formally, this appears as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，这表现为以下形式：
- en: '![CH05_F10_Kunapuli-eqs-13xa](../Images/CH05_F10_Kunapuli-eqs-13x.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Kunapuli-eqs-13xa](../Images/CH05_F10_Kunapuli-eqs-13x.png)'
- en: 'We can unravel this expression for iterations *t*, *t* - 1, *t* - 2, ..., 0
    to obtain the overall update sequence AdaBoost, gradient descent, and gradient
    boosting produce:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个表达式展开到迭代 *t*、*t* - 1、*t* - 2、...、0，以获得AdaBoost、梯度下降和梯度提升产生的整体更新序列：
- en: '![CH05_F10_Kunapuli-eqs-14x](../Images/CH05_F10_Kunapuli-eqs-14x.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Kunapuli-eqs-14x](../Images/CH05_F10_Kunapuli-eqs-14x.png)'
- en: The key differences between the three algorithms are in how we compute the updates
    *h*[t] and the hypothesis weights (also known as step lengths) *α*[t]. We can
    summarize the update steps of all three algorithms in table 5.1.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 三种算法之间的关键区别在于我们如何计算更新 *h*[t] 和假设权重（也称为步长）*α*[t]。我们可以在表5.1中总结所有三种算法的更新步骤。
- en: Table 5.1 Comparing AdaBoost, gradient descent, and gradient boosting
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 比较AdaBoost、梯度下降和梯度提升
- en: '| Algorithm | Loss function | Base-learning algorithm | Update direction *h*[t](*x*)
    | Step length *α*[t] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 损失函数 | 基础学习算法 | 更新方向 *h*[t](*x*) | 步长 *α*[t] |'
- en: '| AdaBoost for classification | Exponential | Classification with weighted
    examples | Weak classifier | Computed in closed form |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost分类 | 指数 | 加权示例分类 | 弱分类器 | 闭式计算 |'
- en: '| Gradient descent | User-specified | None | Gradient vector | Line search
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 梯度下降 | 用户指定 | 无 | 梯度向量 | 线搜索 |'
- en: '| Gradient boosting | User-specified | Regression with examples and residuals
    | Weak regressor | Line search |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升 | 用户指定 | 带有示例和残差的回归 | 弱回归器 | 线搜索 |'
- en: 'The reason gradient boosting = gradient descent + boosting is because it *generalizes*
    the boosting procedure from the exponential loss function used by AdaBoost to
    any user-specified loss function. For gradient boosting to flexibly adapt to a
    wide variety of loss functions, it adopts two general procedures: (1) approximate
    gradients using weak regressors, and (2) compute the hypothesis weights (or step
    lengths) using line search.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升 = 梯度下降 + 提升的原因是它*泛化*了从AdaBoost使用的指数损失函数到任何用户指定的损失函数的提升过程。为了使梯度提升能够灵活地适应各种损失函数，它采用了两个通用程序：(1)
    使用弱回归器近似梯度，(2) 使用线搜索计算假设权重（或步长）。
- en: 5.2.2 Implementing gradient boosting
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 实现梯度提升
- en: 'As before, we’ll put our intuition to practice by implementing our own version
    of gradient boosting. The basic algorithm can be outlined with the following pseudocode:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将通过实现我们自己的梯度提升版本来将我们的直觉付诸实践。基本算法可以用以下伪代码概述：
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This training procedure is almost the same as that of gradient descent except
    for a couple of differences: (1) instead of using the negative gradient, we use
    an approximate gradient trained on the negative residuals, and (2) instead of
    checking for convergence, the algorithm terminates after a finite, maximum number
    of iterations *T*. The following listing implements this pseudocode specifically
    for the squared loss. It uses a type of line search called *golden section search*
    to find the best step length.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练过程几乎与梯度下降相同，只是有几个不同之处：(1) 我们不是使用负梯度，而是使用在负残差上训练的近似梯度，(2) 我们不是检查收敛性，而是在有限的最大迭代次数
    *T* 后算法终止。以下列表具体实现了这个伪代码，它使用一种称为*黄金分割搜索*的线搜索类型来找到最佳步长。
- en: Listing 5.2 Gradient boosting for the squared loss
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.2 平方损失梯度提升
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Gets dimensions of the data set
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取数据集的维度
- en: ❷ Initializes an empty ensemble
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化一个空的集成
- en: ❸ Predicts the ensemble on the training set
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练集上预测集成
- en: ❹ Computes residuals as negative gradients of the squared loss
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算平方损失的负梯度作为残差
- en: ❺ Fits weak regression tree (h[t]) to the examples and residuals
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将弱回归树（h[t]）拟合到示例和残差
- en: ❻ Gets predictions of the weak learner, h[t]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取弱学习器的预测，h[t]
- en: ❼ Sets up the line search problem
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 设置线搜索问题
- en: ❽ Finds the best step length using the golden section search
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用黄金分割搜索找到最佳步长
- en: ❾ Updates the ensemble predictions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 更新集成预测
- en: ❿ Updates the ensemble
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 更新集成
- en: Once the model is trained, we can make predictions (see the following listing)
    as with the AdaBoost ensemble. Note that, just like our AdaBoost implementation
    previously, this model returns predictions of -1/1 rather than 0/1.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们可以像AdaBoost集成一样进行预测（见以下列表）。请注意，就像我们之前的AdaBoost实现一样，此模型返回-1/1的预测而不是0/1。
- en: Listing 5.3 Predictions using gradient-boosted model
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 使用梯度提升模型进行预测
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Initializes all the predictions to 0
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有预测初始化为0
- en: ❷ Aggregates individual predictions from each regressor
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 聚合每个回归器的单个预测
- en: ❸ Converts weighted predictions to -1/1 labels
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将加权预测转换为-1/1标签
- en: 'We can test drive this implementation on a simple two-moons classification
    example. Note that we convert the training labels from 0/1 to -1/1 to ensure that
    we learn and predict correctly:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在一个简单的两月亮分类示例上测试这个实现。请注意，我们将训练标签从0/1转换为-1/1，以确保我们正确学习和预测：
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Converts training labels to -1/1
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将训练标签转换为-1/1
- en: ❷ Splits into train and test sets
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据集分为训练集和测试集
- en: ❸ Trains and gets the test error
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练并获取测试错误
- en: The error rate of this model is 6%, which is pretty good.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的错误率为6%，相当不错。
- en: Visualizing gradient-boosting iterations
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化梯度提升迭代
- en: Finally, to comprehensively nail down our understanding of gradient boosting,
    let’s step through the first few iterations to see how gradient boosting uses
    residuals to boost classification. In our implementation, we initialize our predictions
    to be *F*(*x*[i]) = 0\. This means that in the first iteration, the residuals
    for examples in Class 1 will be *r*[i] = 1 - 0 = 1, and the residuals for the
    examples in Class 0 will be *r*[i] = -1 - 0 = -1\. This is evident in figure 5.11.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了全面巩固我们对梯度提升的理解，让我们逐步分析前几次迭代，看看梯度提升是如何使用残差来提升分类的。在我们的实现中，我们初始化预测为 *F*(*x*[i])
    = 0。这意味着在第一次迭代中，类别1的示例的残差将为 *r*[i] = 1 - 0 = 1，而类别0的示例的残差将为 *r*[i] = -1 - 0 =
    -1。这如图5.11所示。
- en: In the first iteration, all the training examples have high residuals (either
    +1 or -1), and the base-learning algorithm (decision-tree regression) has to train
    a weak regressor taking all these residuals into account. The trained regression
    tree (*h*[1]) is shown in figure 5.11 (right).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，所有训练样本都有较高的残差（要么是+1，要么是-1），基学习算法（决策树回归）必须训练一个考虑所有这些残差的弱回归器。训练的回归树(*h*[1])如图5.11（右）所示。
- en: '![CH05_F11_Kunapuli](../Images/CH05_F11_Kunapuli.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F11_Kunapuli](../Images/CH05_F11_Kunapuli.png)'
- en: 'Figure 5.11 Iteration 1: residuals (left) and the weak regressor trained over
    the residuals (right)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 第1次迭代：残差（左）和基于残差训练的弱回归器（右）
- en: 'The current ensemble consists of only one regression tree: *F* = *α*[1]*h*[1].
    We can also visualize the classification predictions of *h*[1] and the ensemble
    *F*. The resulting classifications achieve an overall error rate of 16%, as shown
    in figure 5.12.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当前集成仅包含一个回归树：*F* = *α*[1]*h*[1]。我们还可以可视化分类预测的*h*[1]和集成*F*。如图5.12所示，这些分类的整体错误率为16%。
- en: '![CH05_F12_Kunapuli](../Images/CH05_F12_Kunapuli.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F12_Kunapuli](../Images/CH05_F12_Kunapuli.png)'
- en: 'Figure 5.12 Iteration 1: Predictions of the weak learner (*h*[1]) and the whole
    ensemble (*F*). Since this is the first iteration, the ensemble consists of only
    one weak regressor.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 第1次迭代：弱学习器(*h*[1])和整个集成(*F*)的预测。由于这是第一次迭代，集成仅包含一个弱回归器。
- en: In iteration 2, we compute the residuals again. Now, the residuals begin to
    show more separation, which reflects how well they are classified by the current
    ensemble. The decision-tree regressor attempts to fit the residuals again (figure
    5.13, right), though this time, it focuses on examples that have been misclassified
    previously.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2次迭代中，我们再次计算残差。现在，残差开始显示出更多的分离，这反映了它们被当前集成分类得有多好。决策树回归器试图再次拟合残差（见图5.13，右），尽管这次它专注于之前被错误分类的示例。
- en: '![CH05_F13_Kunapuli](../Images/CH05_F13_Kunapuli.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F13_Kunapuli](../Images/CH05_F13_Kunapuli.png)'
- en: 'Figure 5.13 Iteration 2: residuals (left) and the weak regressor trained over
    the residuals (right)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 第2次迭代：残差（左）和基于残差训练的弱回归器（右）
- en: 'The ensemble now consists of two regression trees: *F* = *α*[1]*h*[1] + *α*[2]*h*[2].
    We can now visualize the classification predictions of the newly trained regressor
    *h*[2] and the overall ensemble *F* (see figure 5.14).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集成包含两个回归树：*F* = *α*[1]*h*[1] + *α*[2]*h*[2]。我们现在可以可视化新训练的回归器*h*[2]和整体集成*F*（见图5.14）。
- en: '![CH05_F14_Kunapuli](../Images/CH05_F14_Kunapuli.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F14_Kunapuli](../Images/CH05_F14_Kunapuli.png)'
- en: 'Figure 5.14 Iteration 2: predictions of the weak learner (*h*[2]) and the overall
    ensemble (*F*)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 第2次迭代：弱学习器(*h*[2])和整体集成(*F*)的预测
- en: The weak learner trained in iteration 2 has an overall error rate of 39.5%.
    Yet the first two weak learners have already boosted ensemble performance up to
    91% accuracy, that is, 9% error. This process continues in iteration 3, as shown
    in figure 5.15.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2次迭代中训练的弱学习器整体错误率为39.5%。然而，前两个弱学习器已经将集成性能提升至91%的准确率，即9%的错误率。这个过程在第3次迭代中继续，如图5.15所示。
- en: '![CH05_F15_Kunapuli](../Images/CH05_F15_Kunapuli.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F15_Kunapuli](../Images/CH05_F15_Kunapuli.png)'
- en: 'Figure 5.15 Iteration 3: residuals (left) and the weak regressor trained over
    the residuals (right)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 第3次迭代：残差（左）和基于残差训练的弱回归器（右）
- en: In this manner, gradient boosting continues to sequentially train and add base
    regressors to the ensemble. Figure 5.16 shows the model trained after 10 iterations;
    the ensemble consists of 10 weak regressor estimators and has boosted overall
    training accuracy to 97.5%!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，梯度提升法继续按顺序训练并添加基回归器到集成中。图5.16显示了经过10次迭代后训练的模型；集成包含10个弱回归估计器，并将整体训练准确率提升至97.5%！
- en: '![CH05_F16_Kunapuli](../Images/CH05_F16_Kunapuli.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F16_Kunapuli](../Images/CH05_F16_Kunapuli.png)'
- en: Figure 5.16 Final gradient boosting ensemble after 10 iterations
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 经过10次迭代后的最终梯度提升集成
- en: 'There are several publicly available and efficient implementations of gradient
    boosting that you can use for your machine-learning tasks. For the rest of this
    section, we’ll focus on the most familiar: scikit-learn.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种公开且高效的梯度提升实现可供您在机器学习任务中使用。在本节的其余部分，我们将重点关注最熟悉的：scikit-learn。
- en: 5.2.3 Gradient boosting with scikit-learn
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 使用scikit-learn进行梯度提升
- en: 'We’ll now look at how to use two scikit-learn classes: GradientBoostingClassifier
    and a new version called HistogramGradientBoostingClassifier. The latter trades
    exactness for speed to train models significantly faster than GradientBoostingClassifier,
    making it ideally suited for larger data sets.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何使用两个 scikit-learn 类：GradientBoostingClassifier 和一个新版本，称为 HistogramGradientBoostingClassifier。后者以精确度为代价换取速度，可以比
    GradientBoostingClassifier 快得多地训练模型，使其非常适合大型数据集。
- en: 'scikit-learn’s GradientBoostingClassifier essentially implements the same gradient-boosting
    algorithm that we have ourselves implemented in this section. Its usage is similar
    to other scikit-learn classifiers such as AdaBoostClassifier. There are two key
    differences from AdaBoostClassifier, however:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 GradientBoostingClassifier 实质上实现了我们在本节中自己实现的相同的梯度提升算法。它的使用方式与其他
    scikit-learn 分类器（如 AdaBoostClassifier）类似。然而，与 AdaBoostClassifier 有两个关键的区别：
- en: Unlike AdaBoostClassifier, which supports several different types of base estimators,
    GradientBoostingClassifier only supports tree-based ensembles. Therefore, it will
    always use decision trees as base estimators, and there is no mechanism to specify
    other types of base-learning algorithms.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与支持多种不同类型基估计器的 AdaBoostClassifier 不同，GradientBoostingClassifier 只支持基于树的集成。因此，它始终使用决策树作为基估计器，并且没有机制来指定其他类型的基学习算法。
- en: AdaBoostClassifier optimizes the exponential loss (by design). GradientBoostingClassifier
    allows the user to select either the logistic or exponential loss functions. The
    logistic loss (also known as cross entropy) is a commonly used loss function for
    binary classification (and also has a multiclass variant).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoostClassifier 通过设计优化指数损失。GradientBoostingClassifier 允许用户选择逻辑回归或指数损失函数。逻辑损失（也称为交叉熵）是二分类中常用的损失函数（也有多类变体）。
- en: Note Training a GradientBoostingClassifier with the exponential loss very is
    similar to (but not exactly the same as) training an AdaBoostClassifier.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用指数损失函数训练 GradientBoostingClassifier 与训练 AdaBoostClassifier 非常相似（但并不完全相同）。
- en: 'In addition to selecting the loss function, we can also set additional learning
    parameters. These parameters are often selected by cross validation (CV), much
    like any other machine-learning algorithm (see section 4.3 for parameter selection
    in AdaBoostClassifier):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择损失函数外，我们还可以设置其他学习参数。这些参数通常通过交叉验证（CV）选择，就像任何其他机器学习算法一样（参见 AdaBoostClassifier
    中的 4.3 节以了解参数选择）：
- en: We can control the complexity of the base tree estimators directly with max
    _depth and max_leaf_nodes. Higher values mean that the base tree learning algorithm
    has greater flexibility in training more complex trees. The caveat here, of course,
    is that deeper trees, or trees with more leaf nodes, tend to overfit the training
    data.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以直接通过 max_depth 和 max_leaf_nodes 控制基树估计器的复杂性。更高的值意味着基树学习算法在训练更复杂的树时具有更大的灵活性。当然，这里的警告是，更深层次的树或具有更多叶节点的树往往会对训练数据进行过拟合。
- en: n_estimators caps the number of weak learners that will be trained sequentially
    by GradientBoostingClassifier and is essentially the number of algorithm iterations.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_estimators 限制了 GradientBoostingClassifier 将按顺序训练的弱学习器的数量，本质上就是算法迭代的次数。
- en: 'Like AdaBoost, gradient boosting also trains weak learners (*h*[t] in iteration
    *t*) sequentially and constructs an ensemble incrementally and additively: *F*[t](*x*)
    = *F*[t-1](*x*) + *η* ⋅ *α*[t] ⋅ *h*[t](*x*). Here, *α*[t] is the weight of weak
    learner *h*[t] (or the step length), and *η* is the learning rate. The learning
    rate is a user-defined learning parameter that lies in the range 0 < *η* ≤ 1\.
    Recall that a slower learning rate means that it will often take more iterations
    to train an ensemble. It may be necessary to opt for slower learning rates to
    make successive weak learners more robust to outliers and noise. Learning rate
    is controlled by the learning_rate parameter.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AdaBoost 类似，梯度提升也按顺序训练弱学习器（迭代 t 中的 *h*[t]）并逐步和递增地构建集成：*F*[t](*x*) = *F*[t-1](*x*)
    + *η* ⋅ *α*[t] ⋅ *h*[t](*x*)。在这里，*α*[t] 是弱学习器 *h*[t] 的权重（或步长），而 *η* 是学习率。学习率是一个用户定义的学习参数，其范围在
    0 < *η* ≤ 1 之间。回想一下，较慢的学习率意味着训练集成通常需要更多的迭代。可能需要选择较慢的学习率，以便使后续的弱学习器对异常值和噪声更加鲁棒。学习率由
    learning_rate 参数控制。
- en: 'Let’s look at an example of gradient boosting in action on the breast cancer
    data set. We train and evaluate a GradientBoostingClassifier model using this
    data set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在乳腺癌数据集上梯度提升的实例。我们使用这个数据集来训练和评估一个 GradientBoostingClassifier 模型：
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Loads the data set and splits it into training and test sets
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集并将其分为训练集和测试集
- en: ❷ Trains a gradient boosting model with these learning parameters
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用这些学习参数训练梯度提升模型
- en: 'And how well did this model do? This gradient boosting classifier achieves
    4.9% test error, which is pretty good:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型表现如何？这个梯度提升分类器达到了4.9%的测试误差，这相当不错：
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A key limitation of GradientBoostingClassifier, however, is its speed; while
    effective, it does tend to be rather slow on large data sets. The efficiency bottleneck,
    as it turns out, is in tree learning. Recall that gradient boosting has to learn
    a regression tree at each iteration as a base estimator. For large data sets,
    the number of splits a tree learner has to consider becomes prohibitively large.
    This has led to the emergence of histogram-based gradient boosting, which aims
    to speed up base estimator tree learning, allowing gradient boosting to scale
    up to larger data sets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GradientBoostingClassifier的一个关键限制是其速度；虽然有效，但在大数据集上它确实会相对较慢。实际上，效率瓶颈在于树学习。回想一下，梯度提升在每个迭代中都必须学习一个回归树作为基估计器。对于大数据集，树学习器必须考虑的分割数量变得过于庞大。这导致了基于直方图的梯度提升的出现，其目的是加快基估计器树学习，使梯度提升能够扩展到更大的数据集。
- en: 5.2.4 Histogram-based gradient boosting
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 基于直方图的梯度提升
- en: To understand the need for histogram-based tree learning, we have to revisit
    how a decision-tree algorithm learns a regression tree. In tree learning, we learn
    a tree in a top-down fashion, one decision node at a time. The standard way to
    do this is by presorting the feature values, enumerating all possible splits,
    and then evaluating all of them to find the best split. Let’s say we have 1 million
    (10⁶) training examples, each of dimension 100\. Standard tree learning will enumerate
    and evaluate (on the order of) 100 million splits (10⁶ *×* 100 *=* 10⁸) to identify
    a decision node! This is clearly untenable.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解基于直方图的树学习的必要性，我们必须回顾决策树算法是如何学习回归树的。在树学习中，我们自上而下地学习一棵树，一次一个决策节点。标准的方法是通过预排序特征值，枚举所有可能的分割，然后评估所有这些分割以找到最佳的分割。假设我们有1,000,000（10⁶）个训练示例，每个示例的维度为100。标准树学习将枚举并评估（大约）10亿个分割（10⁶
    *×* 100 = 10⁸）以识别决策节点！这显然是不可行的。
- en: One alternative is to reorganize the feature values into a small number of bins.
    In this hypothetical example, suppose we binned each feature column into 100 bins.
    Now, to find the best split, we have to only search over 10,000 splits (100 ×
    100 = 10⁴), which can speed up training rather dramatically!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方案是将特征值重新组织成少数几个桶。在这个假设的例子中，假设我们将每个特征列分桶到100个桶中。现在，为了找到最佳的分割，我们只需要在10,000个分割中进行搜索（100
    × 100 = 10⁴），这可以显著加快训练速度！
- en: Of course, this means that we’re trading off exactness for speed. However, there
    is usually a large amount of redundancy or repeated information in many (big)
    data sets, which we compress by binning the data into smaller buckets. Figure
    5.17 illustrates this tradeoff.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这意味着我们是在精确度和速度之间进行权衡。然而，在许多（大数据）集中通常存在大量冗余或重复信息，我们通过将数据分桶到更小的桶中来进行压缩。图5.17说明了这种权衡。
- en: '![CH05_F17_Kunapuli](../Images/CH05_F17_Kunapuli.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F17_Kunapuli](../Images/CH05_F17_Kunapuli.png)'
- en: 'Figure 5.17 Left: A simple 1D regression problem with 50 data points. Center:
    Standard tree learning evaluates every possible split, which is illustrated by
    a line between each pair of data points. The best split is the one with the lowest
    split criterion (here, squared loss). Right: Histogram-based binning first puts
    the data into five buckets, and then evaluates the splits between each pair of
    data buckets. Again, the best split is the one with the lowest split criterion
    (also squared loss).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 左：一个简单的1D回归问题，有50个数据点。中：标准树学习评估每个可能的分割，这通过每对数据点之间的线表示。最佳的分割是具有最低分割标准（此处为平方损失）的分割。右：基于直方图的分桶首先将数据放入五个桶中，然后评估每个数据桶之间的分割。同样，最佳的分割是具有最低分割标准（也是平方损失）的分割。
- en: In figure 5.17, we contrast the behaviors of standard tree learning and histogram-based
    tree learning. In standard tree learning, each split considered is between two
    successive data points (figure 5.17, center); for 50 data points, we have to evaluate
    49 splits.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.17中，我们对比了标准树学习和基于直方图的树学习的行为。在标准树学习中，每个考虑的分割都是在两个连续的数据点之间（图5.17，中心）；对于50个数据点，我们必须评估49个分割。
- en: In histogram-based splitting, we first bin the data (figure 5.17, right) into
    five bins. Now, each split considered is between two successive data buckets;
    for five bins, we only have to evaluate four splits! Now imagine how this would
    scale to millions of data points.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于直方图的分割中，我们首先将数据（图5.17，右）分为五个桶。现在，每个考虑的分割都是在两个连续的数据桶之间；对于五个桶，我们只需要评估四个分割！现在想象一下，如果数据点达到数百万，这将如何扩展。
- en: 'scikit-learn 0.21 introduced a version of gradient boosting called HistogramGradientBoostingClassifier
    that implements histogram-based gradient boosting such that its training time
    is significantly improved. The following snippet shows how to train and evaluate
    HistogramGradientBoostingClassifier on the breast cancer data set:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 0.21引入了一种名为HistogramGradientBoostingClassifier的梯度提升版本，它实现了基于直方图的梯度提升，从而显著提高了训练时间。以下代码片段展示了如何在乳腺癌数据集上训练和评估HistogramGradientBoostingClassifier：
- en: '[PRE14]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Initializes a histogram-based gradient-boosting classifier
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化基于直方图的梯度提升分类器
- en: ❷ Trains the ensemble
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练集成
- en: 'On the breast cancer data set, HistGradientBoostingClassifier achieved a test
    error of 4.2%. scikit-learn’s histogram-based boosting implementation itself is
    inspired by another popular gradient-boosting package: LightGBM.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在乳腺癌数据集上，HistGradientBoostingClassifier实现了4.2%的测试错误率。scikit-learn的基于直方图的提升实现本身是受到另一个流行的梯度提升包LightGBM的启发。
- en: '5.3 LightGBM: A framework for gradient boosting'
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 LightGBM：梯度提升框架
- en: 'Light Gradient Boosted Machines (LightGBM)[¹](#pgfId-1171188) is an open source
    gradient boosting framework that was originally developed and released by Microsoft.
    At its core, LightGBM is essentially a histogram-based gradient-boosting approach.
    However, it also has several modeling and algorithmic features that enable it
    to handle large-scale data. In particular, LightGBM offers the following advantages:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 光梯度提升机器（LightGBM）[¹](#pgfId-1171188)是一个开源的梯度提升框架，最初由微软开发和发布。在其核心，LightGBM本质上是一种基于直方图的梯度提升方法。然而，它还具有几个建模和算法特性，使其能够处理大规模数据。特别是，LightGBM提供了以下优势：
- en: Algorithmic speedups such as gradient-based one-sided sampling and exclusive
    feature bundling that result in faster training and lower memory usage (these
    are described in more detail in section 5.3.1)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法加速，如基于梯度的单侧采样和独家特征捆绑，这些可以导致更快的训练和更低的内存使用（这些在5.3.1节中描述得更详细）
- en: Support for a large number of loss functions for classification, regression,
    and ranking, as well as application-specific custom loss functions (see section
    5.3.2)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持大量用于分类、回归和排序的损失函数，以及特定应用的自定义损失函数（参见第5.3.2节）
- en: Support for parallel and GPU learning, which enables LightGBM to handle large-scale
    data sets (parallel/GPU-based machine learning is out-of-scope for this book)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持并行和GPU学习，这使得LightGBM能够处理大规模数据集（本书不涉及基于并行/GPU的机器学习）
- en: We’ll also delve into how to apply LightGBM to some practical learning situations
    to avoid overfitting (section 5.4.1), and ultimately a case study on a real-world
    data set (section 5.5). It’s impossible to detail all the features available in
    LightGBM in this limited space, of course. Instead, this section and the next
    introduce LightGBM and illustrate its usage and applications in practical settings.
    This should enable you to springboard further into advanced use cases of LightGBM
    for your applications through its documentation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将深入探讨如何将LightGBM应用于一些实际学习场景以避免过拟合（第5.4.1节），以及最终在一个真实世界数据集上的案例研究（第5.5节）。当然，在这个有限的空间内不可能详细描述LightGBM的所有功能。相反，本节和下一节介绍了LightGBM，并展示了其在实际环境中的应用和用法。这应该能够帮助你通过其文档进一步深入到LightGBM的高级用例。
- en: 5.3.1 What makes LightGBM “light”?
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 LightGBM“轻量”的原因是什么？
- en: Recall from our earlier discussion that the biggest computational bottleneck
    in scaling gradient boosting to large (with many training examples) or high-dimensional
    (with many features) data sets is tree learning, specifically, identifying optimal
    splits in the regression tree base estimators. As we saw in the previous section,
    histogram-based gradient boosting attempts to address this computational bottleneck.
    This works reasonably well for medium-sized data sets. However, histogram-bin
    construction can itself be slow if we have a very large number of data points,
    a large number of features, or both.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们之前的讨论，将梯度提升扩展到大型（具有许多训练示例）或高维（具有许多特征）数据集的最大计算瓶颈是树学习，特别是识别回归树基估计器的最佳分割。正如我们在上一节中看到的，基于直方图的梯度提升试图解决这个问题。这对于中等大小的数据集来说效果相当好。然而，如果我们有大量数据点、大量特征或两者兼而有之，直方图桶的构建本身可能很慢。
- en: In this section, we’ll look at two key conceptual improvements LightGBM implements
    that often lead to significant speedups in training times in practice. The first,
    Gradient-based One-Side Sampling (GOSS), aims to reduce the number of *training
    examples*, while the second, Exclusive Feature Bundling (EFB), aims to reduce
    the number of *features*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨LightGBM实现的两个关键概念改进，这些改进在实践中通常会导致训练时间的显著加快。第一个是梯度基于单侧采样（GOSS），旨在减少*训练示例*的数量，而第二个是独占特征捆绑（EFB），旨在减少*特征*的数量。
- en: Gradient-based one-side sampling
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的单侧采样
- en: A well-known approach to dealing with a very large number of training examples
    is to downsample the data set, that is, randomly sample a smaller subset of the
    data set. We’ve already seen examples of this in other ensemble approaches such
    as pasting (which is bagging without replacement; see chapter 2, section 2.4.1).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量训练示例的一个众所周知的方法是下采样数据集，即随机采样数据集的一个较小的子集。我们已经在其他集成方法中看到了这种方法的例子，例如粘贴（这是不重复的bagging；见第2章，第2.4.1节）。
- en: There are two problems with randomly downsampling the data set. First, not all
    examples are equally important; as in AdaBoost, some training examples are more
    important than others depending on the extent of their misclassification. Thus,
    it’s imperative that downsampling not throw away high-importance training examples.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 随机下采样数据集有两个问题。首先，并非所有示例都同等重要；就像AdaBoost一样，一些训练示例的重要性高于其他示例，这取决于它们错误分类的程度。因此，下采样不应丢弃高重要性的训练示例。
- en: Second, sampling should also ensure that some fraction of correctly classified
    examples is also included. This is important in order to not overwhelm the base-learning
    algorithm with misclassified examples, which will inevitably lead to overfitting.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，采样还应确保包含一定比例的正确分类示例。这对于防止基本学习算法因错误分类示例过多而过度拟合至关重要。
- en: 'This is addressed by downsampling the data smartly using the Gradient-based
    One-Side Sampling (GOSS) procedure. Briefly, GOSS performs the following steps:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用基于梯度的单侧采样（GOSS）程序智能地下采样数据来解决。简要来说，GOSS执行以下步骤：
- en: 'Use the gradient magnitude, similar to AdaBoost, which uses sample weights.
    Remember that the gradient indicates how much more the prediction can be improved:
    well-trained examples have small gradients, while under-trained (typically, misclassified
    or confusing) examples have large gradients.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度幅度，类似于AdaBoost，它使用样本权重。记住，梯度表示预测可以改进多少：训练良好的示例具有小的梯度，而训练不足（通常是错误分类或混淆）的示例具有大的梯度。
- en: Select the top *a%* of examples with the largest gradients; call this subset
    top.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大梯度的前*a%*个示例；称这个子集为top。
- en: Randomly sample *b%* of the remaining examples; call this subset rand.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机采样剩余示例的*b%*；称这个子集为rand。
- en: 'Assign weights to examples in both sets: *w*[top] = 1, *w*[rand] = (100 – *a*)/*b*'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个集合中的示例分配权重：*w*[top] = 1，*w*[rand] = (100 – *a*)/*b*
- en: Train a base regressor over this sampled data (examples, residuals, weights).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此采样数据（示例、残差、权重）上训练一个基本回归器。
- en: The weights computed in step 4 ensure that there is a good balance between under-trained
    and well-trained samples. Overall, such sampling also fosters ensemble diversity,
    which ultimately leads to better ensembles.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步计算出的权重确保了训练不足和训练良好的样本之间有一个良好的平衡。总体而言，这种采样还促进了集成多样性，这最终导致更好的集成。
- en: Exclusive feature bundling
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 独占特征捆绑
- en: Aside from a large number of training examples, big data also often provides
    the challenge of very high dimensionality, which can adversely affect histogram
    construction and slow down the overall training process. Similar to downsampling
    training examples, if we’re able to downsample the features as well, it’s possible
    to gain (sometimes very big) improvements in training speed. This is especially
    so when feature space is sparse, and features are mutually exclusive.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大量的训练示例外，大数据还常常带来非常高的维度挑战，这可能会对直方图构建产生不利影响并减慢整体训练过程。类似于对训练示例进行下采样，如果我们能够对特征也进行下采样，就有可能获得（有时是非常大的）训练速度的提升。这在特征空间稀疏且特征相互排斥时尤其如此。
- en: One common example of such a feature space is when we apply one-hot vectorization
    to categorical variables. For instance, consider a categorical variable that takes
    10 unique values. When one-hot vectorized, this variable is expanded to 10 binary
    variables of which only one is nonzero, and all others are zero. This makes the
    10 columns corresponding to this feature highly sparse.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特征空间的一个常见例子是我们对分类变量应用one-hot向量化。例如，考虑一个具有10个唯一值的分类变量。当进行one-hot向量化时，这个变量扩展为10个二进制变量，其中只有一个是非零的，其余都是零。这使得对应于这个特征的10列非常稀疏。
- en: 'Exclusive Feature Bundling (EFB) works in reverse, exploits this sparsity,
    and aims to compress mutually exclusive columns into one column to reduce the
    number of effective features. At a high level, EFB performs two steps:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 独立特征捆绑（EFB）的工作方式相反，利用这种稀疏性，旨在将相互排斥的列压缩为一列以减少有效特征的数量。从高层次来看，EFB执行两个步骤：
- en: Identify features that can be bundled together by measuring conflicts or the
    number of times both features are *nonzero simultaneously.* The intuition here
    is that if two features are often mutually exclusive, they are low conflict and
    can be bundled together.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过测量冲突或两个特征同时非零的次数来识别可以捆绑在一起的特征。这里的直觉是，如果两个特征经常相互排斥，它们是低冲突的，可以捆绑在一起。
- en: Merge the identified low-conflict features into a feature bundle. The idea here
    is to preserve information carefully when merging nonzero values, which is typically
    done by adding offsets to feature values to prevent overlaps.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将识别出的低冲突特征合并成一个特征包。这里的想法是在合并非零值时仔细保留信息，这通常是通过向特征值添加偏移量来防止重叠来实现的。
- en: 'Intuitively, this is like having two features: pass and fail. Since one can’t
    pass and fail an exam at the same time, we can merge them both into one feature
    (i.e., collapse two columns in the data set into one).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 直观来说，这就像有两个特征：通过和失败。由于一个人不能同时通过和失败考试，我们可以将它们合并成一个特征（即，将数据集中的两列合并为一列）。
- en: pass and fail, of course, are zero-conflict features and will never overlap.
    More often, two or more features might not be perfectly zero-conflict, but low
    conflict with some small number of overlaps. In such cases, EFB will still bundle
    these features together, which compresses several data columns into one column!
    By merging features in this manner, EFB effectively reduces the overall number
    of features, which often makes training much faster.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过和失败，当然是零冲突特征，永远不会重叠。更常见的是，两个或更多特征可能不是完全零冲突，但与一些小的重叠具有低冲突。在这种情况下，EFB仍然会将这些特征捆绑在一起，这会将几个数据列压缩为一列！通过这种方式合并特征，EFB有效地减少了整体特征数量，这通常会使训练速度更快。
- en: 5.3.2 Gradient boosting with LightGBM
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 使用LightGBM进行梯度提升
- en: LightGBM is available for various platforms, including Windows, Linux, and macOS,
    and it can either be built from scratch or installed using tools such as pip.
    Its usage syntax is quite similar to that of scikit-learn.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM适用于各种平台，包括Windows、Linux和macOS，它可以从头开始构建或使用pip等工具安装。它的使用语法与scikit-learn非常相似。
- en: 'Continuing with the breast cancer data set from section 5.2.3, we can train
    a gradient boosting model using LightGBM as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用第5.2.3节中的乳腺癌数据集，我们可以使用LightGBM训练一个梯度提升模型，如下所示：
- en: '[PRE15]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we instantiate an instance of LGBMClassifier and set it to train an ensemble
    of 20 regression stumps (i.e., the base estimators will be regression trees of
    depth 1). The other important specification here is boosting_type. LightGBM can
    be trained in four modes:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实例化一个LGBMClassifier实例，并将其设置为训练一个包含20个回归树（即，基估计器将是深度为1的回归树）的集成。这里的一个重要规范是boosting_type。LightGBM可以在四种模式下进行训练：
- en: boosting_type='rf'—Trains traditional random forest ensembles (see chapter 2,
    section 2.3)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: boosting_type='rf'—训练传统的随机森林集成（参见第2章，第2.3节）
- en: boosting_type='gbdt'—Trains an ensemble using traditional gradient boosting
    (refer to section 5.2)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: boosting_type='gbdt'—使用传统的梯度提升（参考第5.2节）训练集成模型
- en: boosting_type='goss'—Trains an ensemble using GOSS (refer to section 5.3.1)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: boosting_type='goss'—使用GOSS（参考第5.3.1节）训练集成模型
- en: boosting_type='dart'—Trains an ensemble using Dropouts meet Multiple Additive
    Regression Trees (DART; see section 5.5)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: boosting_type='dart'—使用Dropouts meet Multiple Additive Regression Trees (DART；参考第5.5节)训练集成模型
- en: 'The last three gradient-boosting modes essentially trade off between training
    speed and predictive performance, and we’ll explore this in our case study. For
    now, check out how well the model we just trained using boosting_type=''gbdt''
    turns out:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后三个梯度提升模式基本上是在训练速度和预测性能之间进行权衡，我们将在案例研究中探讨这一点。现在，让我们看看我们刚刚使用boosting_type='gbdt'训练的模型表现如何：
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our first LightGBM classifier achieves 94.7% accuracy on the test set held out
    from the breast cancer data set. Now that we’ve familiarized ourselves with the
    basic functionality of LightGBM, let’s look at how we can train models for real-world
    use cases with LightGBM.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个LightGBM分类器在从乳腺癌数据集中保留的测试集上达到了94.7%的准确率。现在我们已经熟悉了LightGBM的基本功能，让我们看看如何使用LightGBM为实际用例训练模型。
- en: 5.4 LightGBM in practice
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 LightGBM的实际应用
- en: 'In this section, we describe how to train models in practice using LightGBM.
    As always, this means ensuring that LightGBM models generalize well and don’t
    overfit. As with AdaBoost, we look to set the learning rate (section 5.4.1) or
    employ early stopping (section 5.4.2) as a means to control overfitting:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了如何使用LightGBM在实际中训练模型。一如既往，这意味着确保LightGBM模型具有良好的泛化能力，并且不会过拟合。与AdaBoost类似，我们寻求设置学习率（第5.4.1节）或采用提前停止（第5.4.2节）作为控制过拟合的手段：
- en: '*Learning rate*—By selecting an effective learning rate, we try to control
    the rate at which the model learns so that it doesn’t rapidly fit, and then overfit
    the training data. We can think of this as a proactive modeling approach, where
    we try to identify a good training strategy so that it leads to a good model.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习率*—通过选择一个有效的学习率，我们试图控制模型学习的速率，使其不会快速拟合，然后过拟合训练数据。我们可以将这视为一种主动建模方法，其中我们试图确定一个好的训练策略，以便它能够导致一个好的模型。'
- en: '*Early stopping*—By enforcing early stopping, we try to stop training as soon
    as we observe that the model is starting to overfit. We can think of this as a
    reactive modeling approach, where we contemplate terminating training as soon
    as we think we have a good model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提前停止*—通过实施提前停止，我们试图在观察到模型开始过拟合时立即停止训练。我们可以将这视为一种反应式建模方法，其中我们考虑在认为我们有一个好模型时立即终止训练。'
- en: 'Finally, we also explore one of the most powerful functionalities of LightGBM:
    its support for custom loss functions. Recall that one of the major benefits of
    gradient boosting is that it’s a general procedure, widely applicable to many
    loss functions.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还探索了LightGBM最强大的功能之一：它对自定义损失函数的支持。回想一下，梯度提升的主要好处之一是它是一个通用过程，广泛适用于许多损失函数。
- en: While LightGBM provides support for many standard loss functions for classification,
    regression, and ranking, sometimes it may be necessary to train with application-specific
    loss functions. In section 5.4.3, we’ll see precisely how we can do this with
    LightGBM.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LightGBM支持许多标准损失函数，用于分类、回归和排序，但有时可能需要使用特定应用的损失函数进行训练。在第5.4.3节中，我们将看到如何使用LightGBM精确地做到这一点。
- en: 5.4.1 Learning rate
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 学习率
- en: When using gradient boosting, as with other machine-learning algorithms, it’s
    possible to overfit on the training data. This means that, while we achieve very
    good training set performance, this doesn’t result in a similar test set performance.
    That is, the model we’ve trained fails to generalize well. LightGBM, like scikit-learn,
    provides us with the means to control model complexity before overfitting.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用梯度提升时，与其他机器学习算法一样，我们可能在训练数据上过拟合。这意味着，虽然我们实现了非常好的训练集性能，但这并没有导致类似的测试集性能。也就是说，我们训练的模型泛化能力不佳。LightGBM，就像scikit-learn一样，为我们提供了在过拟合之前控制模型复杂性的手段。
- en: Learning rate via cross validation
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交叉验证学习率
- en: LightGBM allows us to control the learning rate through the learning_rate training
    parameter (a positive number that has a default value of 0.1). This parameter
    also has a couple of aliases, shrinkage_rate and eta, which are other terms for
    the learning rate commonly used in machine-learning literature. Though all of
    these parameters have the same effect, care must be taken to set only one of them.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 允许我们通过 learning_rate 训练参数（一个默认值为 0.1 的正数）来控制学习率。此参数还有几个别名，shrinkage_rate
    和 eta，它们是机器学习文献中常用的学习率的术语。尽管所有这些参数都有相同的效果，但必须注意只设置其中一个。
- en: How can we figure out an effective learning rate for our problem? As with any
    other learning parameter, we can use CV. Recall that we also used CV to select
    the learning rate for AdaBoost in the previous chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定我们问题的有效学习率？与其他任何学习参数一样，我们可以使用交叉验证。回想一下，在前一章中，我们也使用了交叉验证来选择 AdaBoost 的学习率。
- en: 'LightGBM plays nicely with scikit-learn, and we can combine the relevant functionalities
    from both packages to perform model learning. In listing 5.4, we combine scikit-learn’s
    StratifiedKFold class to split the training data into 10 folds of training and
    validation sets. StratifiedKFold ensures that we preserve class distributions,
    that is, the fractions of different classes across the folds. Once the CV folds
    are set up, we can train and validate models on these 10 folds for different choices
    of learning rates: 0.1, 0.2, ..., 1.0.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 与 scikit-learn 有着良好的兼容性，我们可以结合这两个包的相关功能来执行模型学习。在列表 5.4 中，我们结合了 scikit-learn
    的 StratifiedKFold 类来将训练数据分割成 10 个训练和验证集。StratifiedKFold 确保我们保留了类别分布，即不同类别在各个折中的比例。一旦设置好
    CV 折，我们就可以在这些 10 个折上针对不同选择的学习率（0.1、0.2、...、1.0）进行模型训练和验证。
- en: Listing 5.4 Cross validation with LightGBM and scikit-learn
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 LightGBM 和 scikit-learn 的交叉验证
- en: '[PRE17]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Initializes learning rates and number of cross validation folds
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化学习率和交叉验证折数
- en: ❷ Splits data into training and validation folds
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据分割成训练和验证折
- en: ❸ Saves training and validation errors
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 保存训练和验证误差
- en: ❹ Trains a LightGBM classifier for each fold with different learning rates
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对每个折使用不同学习率训练 LightGBM 分类器
- en: ❺ Saves training and validation errors
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 保存训练和验证误差
- en: ❻ Averages training and validation errors across folds
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在各个折之间平均训练和验证误差
- en: We can visualize the training and validation errors for different learning rates
    in figure 5.18.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图 5.18 中可视化不同学习率下的训练和验证误差。
- en: '![CH05_F18_Kunapuli](../Images/CH05_F18_Kunapuli.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F18_Kunapuli](../Images/CH05_F18_Kunapuli.png)'
- en: Figure 5.18 Averaged training and validation errors of LightGBM across 10 folds
    of the breast cancer data set
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 LightGBM 在乳腺癌数据集的 10 折中的平均训练和验证误差
- en: Unsurprisingly, as the learning rate increases, the training error continues
    to decrease, suggesting that the model first fits and then begins to overfit the
    training data. The validation error doesn’t show the same trend. It decreases
    initially, and then increases; a learning rate of 0.4 produces the lowest validation
    error. This, then, is the best choice of learning rate.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，随着学习率的增加，训练误差持续下降，这表明模型首先拟合，然后开始过度拟合训练数据。验证误差没有显示出相同的趋势。它最初下降，然后上升；学习率为
    0.4 产生了最低的验证误差。这就是最佳的学习率选择。
- en: Cross validation with LightGBM
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 的交叉验证
- en: LightGBM provides its own functionality to perform CV with given parameter choices
    through a function called cv, as shown in the following listing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 提供了通过名为 cv 的函数执行具有给定参数选择的交叉验证的功能，如下面的列表所示。
- en: Listing 5.5 Cross validation with LightGBM
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 LightGBM 的交叉验证
- en: '[PRE18]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Puts data into a LightGBM “Dataset” object
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据放入 LightGBM 的“Dataset”对象中
- en: ❷ Specifies learning parameters
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定学习参数
- en: ❸ Performs 5-fold CV, each with 100 estimators
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行 5 折交叉验证，每个折包含 100 个估计器
- en: In listing 5.5, we perform 5-fold CV over 100 boosting rounds (thus eventually
    training 100 base estimators). Setting stratified=True ensures that we preserve
    class distributions, that is, the fractions of different classes across the folds.
    Setting shuffle=True randomly shuffles the training data before splitting the
    data into folds.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 5.5 中，我们对 100 个提升轮次进行了 5 折交叉验证（因此最终训练了 100 个基础估计器）。设置 stratified=True 确保我们保留了类别分布，即不同类别在各个折中的比例。设置
    shuffle=True 在将数据分割成折之前随机打乱训练数据。
- en: 'We can visualize the training objective as training progresses. In listing
    5.5, we train our classification model by optimizing cross entropy, set via ''objective'':
    ''cross_entropy''. As shown in figure 5.19, as we add more base estimators to
    our sequential ensemble, the average 5-fold cross-entropy objective decreases.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以随着训练的进行来可视化训练目标。在列表5.5中，我们通过设置''objective'': ''cross_entropy''来优化交叉熵，以训练我们的分类模型。如图5.19所示，随着我们将更多的基础估计器添加到我们的顺序集成中，平均5折交叉熵目标降低。'
- en: '![CH05_F19_Kunapuli](../Images/CH05_F19_Kunapuli.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F19_Kunapuli](../Images/CH05_F19_Kunapuli.png)'
- en: Figure 5.19 The average cross entropy across the folds decreases with increasing
    iterations, as we add more base estimators to the ensemble.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 随着迭代次数的增加，跨折平均交叉熵降低，因为我们向集成中添加更多的基础估计器。
- en: 5.4.2 Early stopping
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 提前停止
- en: 'Another way of reining in overfitting behavior is through early stopping. As
    we’ve seen with AdaBoost, the idea of early stopping is pretty straightforward.
    As we train sequential ensembles, we train one base estimator at each iteration.
    This process continues until we reach the user-specified ensemble size (in LightGBM,
    there are several aliases to specify this: n_estimators, num_trees, num_rounds).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种控制过拟合行为的方法是通过提前停止。正如我们在AdaBoost中看到的，提前停止的想法非常简单。当我们训练顺序集成时，我们在每次迭代中训练一个基础估计器。这个过程一直持续到我们达到用户指定的集成大小（在LightGBM中，有几个别名可以指定这一点：n_estimators、num_trees、num_rounds）。
- en: As the number of base estimators in the ensemble increases, the complexity of
    the ensemble also increases, which eventually leads to overfitting. To avoid this,
    we employ early stopping; that is, instead of training the model, we stop before
    we reach the limit of the ensemble size. We keep track of overfitting behavior
    by means of a validation set. Then, we train until we see no improvement in validation
    performance for a certain prespecified number of iterations.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 随着集成中基础估计器数量的增加，集成的复杂性也增加，这最终会导致过拟合。为了避免这种情况，我们采用提前停止；也就是说，我们不在达到集成大小极限之前停止训练模型。我们通过验证集来跟踪过拟合行为。然后，我们训练，直到我们观察到验证性能在预指定的迭代次数内没有改善。
- en: 'For example, let’s say that we’ve started training an ensemble of 500 base
    estimators and set early stopping iterations to 5\. This is how early stopping
    works: when training, we keep a close eye on the validation error as we grow our
    ensemble, and when the validation error doesn’t improve over a window of five
    iterations or early stopping rounds, we terminate training.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经开始训练一个包含500个基础估计器的集成，并将提前停止迭代次数设置为5。这是提前停止的工作原理：在训练过程中，我们密切关注随着集成增长而增长的验证错误，当验证错误在五个迭代窗口或提前停止轮次内没有改善时，我们终止训练。
- en: In LightGBM, we can incorporate early stopping if we specify a value for the
    parameter early_stopping_rounds. As long as the overall validation score (say,
    accuracy) improves over the last early_stopping_rounds, LightGBM will continue
    to train. However, if the score hasn’t improved after early_stopping_rounds, LightGBM
    terminates.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在LightGBM中，如果我们为参数early_stopping_rounds指定一个值，我们可以实现提前停止。只要整体验证分数（例如，准确率）在最后的early_stopping_rounds内有所改善，LightGBM将继续训练。然而，如果在early_stopping_rounds之后分数没有改善，LightGBM将终止。
- en: As with AdaBoost, LightGBM also needs us to explicitly specify a validation
    set as well as a scoring metric for early stopping. In listing 5.6, we use the
    area under the receiver-operator curve (AUC) as the scoring metric to determine
    early stopping.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与AdaBoost一样，LightGBM也需要我们明确指定验证集以及提前停止的评分指标。在列表5.6中，我们使用接收器操作特征曲线下的面积（AUC）作为评分指标来确定提前停止。
- en: The AUC is an important evaluation metric for classification problems and can
    be interpreted as the probability that the model will rank a randomly chosen positive
    example higher than a randomly chosen negative example. Thus, high values of AUC
    are preferred as it means that the model is more discriminative.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: AUC是分类问题的重要评估指标，可以解释为模型将随机选择的正例排名高于随机选择的负例的概率。因此，较高的AUC值更受欢迎，因为这表明模型更具区分性。
- en: Listing 5.6 Early stopping with LightGBM
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6 LightGBM中的提前停止
- en: '[PRE19]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Splits data into train and validation sets
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据分为训练集和验证集
- en: ❷ Performs early stopping if there’s no change in the validation score after
    five rounds
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在经过五轮后，如果验证分数没有变化则执行提前停止
- en: ❸ Uses AUC as the validation scoring metric for early stopping
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用AUC作为提前停止的验证评分指标
- en: 'Let’s look at the output produced by LightGBM. In listing 5.6, we set n_estimators=50,
    which means training will add one base estimator per iteration:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看LightGBM产生的输出。在列表5.6中，我们设置了n_estimators=50，这意味着每次迭代都会添加一个基础估计器：
- en: '[PRE20]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, observe that training terminates after 32 iterations, meaning that LightGBM
    did indeed terminate before going all the way to training a full set of 50 base
    estimators. Next, note that the best iteration was 27, which had a score (in this
    case, AUC) of 0.996069.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，观察训练在32次迭代后终止，这意味着LightGBM确实在训练完整的50个基础估计器之前就终止了。接下来，注意最佳迭代是第27次，得分为0.996069（在这种情况下，是AUC）。
- en: Over the next 5 (early_stopping_rounds) iterations, from 28 to 32, LightGBM
    observed that adding additional estimators didn’t improve the validation score
    significantly. This triggers the early stopping criterion, causing LightGBM to
    terminate and return an ensemble with 32 base estimators.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的5（early_stopping_rounds）次迭代中，从28到32，LightGBM观察到添加额外的估计器并没有显著提高验证分数。这触发了提前停止标准，导致LightGBM终止，并返回一个包含32个基础估计器的集成。
- en: 'NOTE In its output, LightGBM reports two metrics: AUC, which we specified as
    the evaluation metric, and binary logistic loss, which is its default evaluation
    metric. Since we specified early stopping with respect to AUC, the algorithm terminates
    even though the binary logistic loss keeps decreasing. Put another way, if we’d
    used binary logistic loss as our evaluation metric, early stopping would not have
    terminated this early and would’ve kept going. In practical situations, such metrics
    are often task dependent and should be chosen carefully with the downstream application
    in mind.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在LightGBM的输出中，它报告了两个指标：AUC，我们指定为评估指标，以及二进制逻辑损失，这是其默认评估指标。由于我们针对AUC指定了提前停止，即使二进制逻辑损失持续下降，算法也会终止。换句话说，如果我们使用二进制逻辑损失作为评估指标，提前停止不会这么早终止，而会继续进行。在实际情况下，此类指标通常取决于任务，并且应仔细选择，考虑到下游应用。
- en: We also visualize the training and validation errors as well as the ensemble
    size for different choices of early_stopping_rounds.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可视化了不同early_stopping_rounds选择下的训练和验证误差以及集成大小。
- en: Small values of early_stopping_rounds make LightGBM very “impatient” and aggressive
    in that it doesn’t wait too long to see if there is any improvement before stopping
    learning early. This leads to underfitting; in figure 5.20, for instance, setting
    early_stopping_rounds to 1 leads to an ensemble of just five base estimators,
    hardly enough to even fit the training data properly!
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: early_stopping_rounds的值较小会使LightGBM非常“不耐烦”和“激进”，因为它不会等待太久，看看在停止学习之前是否有任何改进。这会导致欠拟合；例如，在图5.20中，将early_stopping_rounds设置为1会导致只有五个基础估计器的集成，几乎不足以正确地拟合训练数据！
- en: '![CH05_F20_Kunapuli](../Images/CH05_F20_Kunapuli.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F20_Kunapuli](../Images/CH05_F20_Kunapuli.png)'
- en: 'Figure 5.20 Left: Training and validation errors for different values of early_stopping_rounds.
    Right: Ensemble sizes for different values of early_stopping_rounds.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 左：不同early_stopping_rounds值下的训练和验证误差。右：不同early_stopping_rounds值下的集成大小。
- en: 'Large values of early_stopping_rounds make LightGBM too passive in that it
    waits for longer periods to see if there is any improvement. The choice of early_
    stopping_rounds ultimately depends on your problem: how big it is, what your performance
    metric is, and the complexity of the models you’re willing to tolerate.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: early_stopping_rounds的值较大会使LightGBM过于“被动”，它会等待更长的时间，看看是否有任何改进。early_stopping_rounds的选择最终取决于你的问题：它有多大，你的性能指标是什么，以及你愿意容忍的模型复杂性。
- en: 5.4.3 Custom loss functions
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 自定义损失函数
- en: Recall that one of the most powerful features of gradient boosting is that it’s
    applicable to a wide variety of loss functions. This means that it’s also possible
    for us to design our own, problem-specific loss functions to handle specific properties
    of our data set and task. Perhaps our data set is imbalanced, meaning that different
    classes have different amounts of data. In such situations, rather than high accuracy,
    we might require high recall (fewer false negatives, e.g., in medical diagnoses)
    or high precision (fewer false positives, e.g., in spam detection). In many such
    scenarios, it’s often necessary to design our own problem-specific loss functions.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，梯度提升法最强大的特性之一是它适用于广泛的损失函数。这意味着我们也可以设计自己的、针对特定问题的损失函数来处理我们数据集和任务的特定属性。也许我们的数据集是不平衡的，这意味着不同的类别有不同的数据量。在这种情况下，我们可能需要高召回率（例如，在医疗诊断中更少的误判）或高精确度（例如，在垃圾邮件检测中更少的误判）。在许多这样的场景中，通常需要设计我们自己的针对特定问题的损失函数。
- en: NOTE For more details on evaluation metrics such as precision and recall, as
    well metrics for other machine-learning tasks such as regression and ranking,
    see *Evaluating Machine Learning Models* by Alice Zheng (O’Reilly, 2015).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关精确度、召回率等评估指标以及回归和排名等其他机器学习任务的指标等更多详细信息，请参阅Alice Zheng所著的《评估机器学习模型》（O’Reilly，2015年）。
- en: With gradient boosting generally, and LightGBM specifically, once we have a
    loss function, we can rapidly train and evaluate models that are targeted toward
    our problem. In this section, we’ll explore how to use LightGBM for a custom loss
    function called the *focal loss*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升法中，特别是LightGBM中，一旦我们有一个损失函数，我们就可以快速训练和评估针对我们问题的模型。在本节中，我们将探讨如何使用LightGBM来实现一个名为
    *焦点损失* 的自定义损失函数。
- en: The focal loss
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失
- en: The focal loss was introduced for dense object detection, or the problem of
    object detection in a large number of densely packed windows in an image. Ultimately,
    such object-detection tasks come down to a foreground versus background classification
    problem, which is highly imbalanced as there are often many more windows with
    background than foreground objects of interest.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失是为了密集目标检测或图像中大量密集排列的窗口中的目标检测问题而引入的。最终，这样的目标检测任务归结为前景与背景的分类问题，由于通常背景窗口比感兴趣的前景对象多得多，因此这个问题高度不平衡。
- en: The focal loss, in general, was designed for and is well suited for classification
    problems with such class imbalances. It’s a modification of the classic cross-entropy
    loss that puts more focus on harder-to-classify examples, while ignoring the easier
    examples.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失通常是为具有此类类别不平衡的分类问题设计的，并且非常适合这些问题。它是对经典交叉熵损失的修改，它更关注难以分类的示例，而忽略容易分类的示例。
- en: More formally, recall that the standard cross-entropy loss between a true label
    and a predicted label can be computed as
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，记住，真实标签和预测标签之间的标准交叉熵损失可以计算如下：
- en: '![CH05_F20_Kunapuli-eqs-20x](../Images/CH05_F20_Kunapuli-eqs-20x.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F20_Kunapuli-eqs-20x](../Images/CH05_F20_Kunapuli-eqs-20x.png)'
- en: where *p*[pred] is the probability of predicting class 1, that is, prob(*y*[pred]
    = 1) = *p*[pred]. Note that, for a binary classification problem, since the only
    other label is 0, the probability of negative prediction will be prob(*y*[pred]
    = 0) = 1 - *p*[pred].
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[pred] 是预测类别 1 的概率，即 prob(*y*[pred] = 1) = *p*[pred]。请注意，对于二元分类问题，由于唯一的其他标签是
    0，负预测的概率将是 prob(*y*[pred] = 0) = 1 - *p*[pred]。
- en: 'The focal loss introduces a *modulating factor* to each term in the cross-entropy
    loss:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失在交叉熵损失的每一项中引入了一个 *调节因子*：
- en: '![CH05_F20_Kunapuli-eqs-23x](../Images/CH05_F20_Kunapuli-eqs-23x.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F20_Kunapuli-eqs-23x](../Images/CH05_F20_Kunapuli-eqs-23x.png)'
- en: 'The modulating factor suppresses the contribution of well-classified examples,
    forcing a learning algorithm to focus on poorly classified examples. The extent
    of this focus is determined by a user-controllable parameter, *γ* > 0\. To see
    how modulation works, let’s compare the cross-entropy loss with the focal loss
    with *γ* = 2:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 调节因子会抑制分类良好的示例的贡献，迫使学习算法专注于分类不良的示例。这种关注的程度由一个用户可控制的参数 *γ* > 0 决定。为了了解调节是如何工作的，让我们比较
    *γ* = 2 时的交叉熵损失与焦点损失：
- en: '*Well-classified example*—Let’s say the true label is *y*[true] = 1 with high
    predicted label probability *p*[pred] = 0.95\. The cross-entropy loss is *L*[ce]
    = -1 ⋅ log0.95 - 0 ⋅ log0.05 = 0.0513, while the focal loss is *L*[fo] = -1 ⋅
    log0.95 ⋅ 0.05² - 0 ⋅ log0.05 ⋅ 0.95² = 0.0001\. The modulating factor in the
    focal loss, thus, down-weights the loss if an example is classified with high
    confidence.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类良好的示例*——假设真实标签是 *y*[true] = 1，具有高预测标签概率 *p*[pred] = 0.95。交叉熵损失是 *L*[ce]
    = -1 ⋅ log0.95 - 0 ⋅ log0.05 = 0.0513，而焦点损失是 *L*[fo] = -1 ⋅ log0.95 ⋅ 0.05² -
    0 ⋅ log0.05 ⋅ 0.95² = 0.0001。因此，焦点损失中的调制因子会降低高置信度分类的损失。'
- en: '*Poorly classified example*—Let’s say the true label is *y*[true] =1 with low
    predicted label probability *p*[pred] = 0.05\. The cross-entropy loss is *L*[ce]
    = -1 ⋅ log0.05 - 0 ⋅ log0.95 = 2.9957, while the focal loss is *L*[fo] = -1 ⋅
    log0.05 ⋅ 0.95² - 0 ⋅ log0.95 ⋅ 0.05² = 2.7036\. The modulating factor affects
    the loss for this example far less because it’s classified with low confidence.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类不良的示例*——假设真实标签是 *y*[true] = 1，具有低预测标签概率 *p*[pred] = 0.05。交叉熵损失是 *L*[ce]
    = -1 ⋅ log0.05 - 0 ⋅ log0.95 = 2.9957，而焦点损失是 *L*[fo] = -1 ⋅ log0.05 ⋅ 0.95² -
    0 ⋅ log0.95 ⋅ 0.05² = 2.7036。调制因子对这一示例的影响远小于低置信度分类。'
- en: This effect can be seen in figure 5.21, where the focal loss is plotted for
    different values of γ. For bigger values of γ, well-classified examples (with
    high probability of *y* = 1) have lower losses, while poorly classified examples
    have higher losses.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效果可以在图 5.21 中看到，其中绘制了不同 γ 值的焦点损失。对于更大的 γ 值，分类良好的示例（*y* = 1 的概率高）具有更低的损失，而分类不良的示例具有更高的损失。
- en: '![CH05_F21_Kunapuli](../Images/CH05_F21_Kunapuli.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F21_Kunapuli](../Images/CH05_F21_Kunapuli.png)'
- en: Figure 5.21 The focal loss visualized for various values of *γ*. When *γ* =
    0, the original cross-entropy loss is recovered. As *γ* increases, the part of
    the curve corresponding to well-classified examples becomes longer, reflecting
    the loss function’s focus on poor classification.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 对于不同的 *γ* 值，可视化焦点损失。当 *γ* = 0 时，恢复原始交叉熵损失。随着 *γ* 的增加，对应于分类良好的示例的曲线部分变得更长，反映了损失函数对不良分类的关注。
- en: Gradient boosting with the focal loss
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 带焦点损失的梯度提升
- en: 'To use the focal loss to train gradient boosted decision trees (GBDT), we have
    to provide LightGBM with two functions:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用焦点损失来训练梯度提升决策树（GBDT），我们必须向 LightGBM 提供两个函数：
- en: The actual loss function itself, which will be used for function evaluations
    and scoring during learning
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际的损失函数本身，它将在学习过程中的函数评估和评分中使用
- en: The first derivative (gradient) and second derivative (Hessian) of the loss
    function, which will be used for learning the constituent base-estimator trees
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数的第一导数（梯度）和第二导数（海森矩阵），这些将被用于学习构成基础估计树
- en: LightGBM uses the Hessian information for learning at leaf nodes. For the moment,
    we can put this small detail aside, as we’ll revisit it in the next chapter.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 使用海森矩阵信息在叶节点进行学习。目前，我们可以暂时忽略这个细节，因为下一章我们将重新讨论它。
- en: Listing 5.7 shows how we can define custom loss functions. The focal_loss function
    is the loss itself, implemented exactly as defined at the start of this subsection.
    The focal_loss_metric function turns focal_loss into a scoring metric for use
    with LightGBM.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 展示了如何定义自定义损失函数。focal_loss 函数是损失本身，实现方式与本节开头定义的完全一致。focal_loss_metric
    函数将 focal_loss 转换为 LightGBM 使用的评分指标。
- en: The focal_loss_objective function returns the gradient and the Hessian of the
    loss function for LightGBM to use in tree learning. This function is rather unintuitively
    suffixed with “objective” to be consistent with LightGBM’s usage, as will become
    apparent shortly.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: focal_loss_objective 函数返回损失函数的梯度和海森矩阵，供 LightGBM 在树学习中使用。这个函数很不直观地带有“objective”后缀，以与
    LightGBM 的用法保持一致，这一点很快就会变得明显。
- en: Listing 5.7 Defining custom loss functions
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 定义自定义损失函数
- en: '[PRE21]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Defines the focal loss function
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义焦点损失函数
- en: ❷ Wrapper function that returns a LightGBM-compatible scoring metric
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回 LightGBM 兼容评分指标的包装函数
- en: ❸ Autodifferentiation computes gradient and Hessian derivatives
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 自动微分计算梯度和海森矩阵导数
- en: 'Care must be taken to ensure that the loss function, metric, and objective
    are all vector-compatible; that is, they can take array-like objects ytrue and
    ypred as inputs. In listing 5.7, we’ve used scipy’s derivative functionality to
    approximate the first and second derivatives. It’s also possible to analytically
    derive and implement the first and second derivatives for some loss functions.
    Once we’ve defined our custom loss function, it’s straightforward to use it with
    LightGBM:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意确保损失函数、指标和目标都是向量兼容的；也就是说，它们可以接受类似数组的对象 ytrue 和 ypred 作为输入。在列表 5.7 中，我们使用了
    scipy 的导数功能来近似一阶和二阶导数。对于某些损失函数，也有可能通过解析推导并实现一阶和二阶导数。一旦我们定义了我们的自定义损失函数，就可以直接与 LightGBM
    一起使用：
- en: '[PRE22]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Sets objective to ensure that LightGBM uses the gradients of the focal loss
    for learning
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置目标以确保 LightGBM 使用焦点损失的梯度进行学习
- en: ❷ Sets metric to ensure that LightGBM uses the focal loss for evaluation
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置指标以确保 LightGBM 使用焦点损失进行评估
- en: ❸ Imports the sigmoid function from “scipy”
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从“scipy”导入 sigmoid 函数
- en: ❹ Gets raw scores and then computes the probability of class=1 using the sigmoid
    function
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取原始分数，然后使用 sigmoid 函数计算类别=1 的概率
- en: ❺ Converts to a 0/1 label, where the prediction is class=1 if probability >
    0.5, and class=0 otherwise
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将预测转换为 0/1 标签，其中如果概率 > 0.5，则预测类别=1，否则类别=0
- en: GBDT with focal loss achieves a validation score of 96.5% on the breast cancer
    data set.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 带有焦点损失的 GBDT 在乳腺癌数据集上实现了 96.5% 的验证分数。
- en: '5.5 Case study: Document retrieval'
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 案例研究：文档检索
- en: Document retrieval is the task of retrieving documents from a database to match
    a user’s query. For example, a paralegal at a law firm might need to search for
    information about previous cases from legal archives to establish precedent and
    research case law. Or perhaps a graduate student might need to search for articles
    from a journal’s database during the course of a literature survey of work in
    a specific area. You may also have seen a feature called “related articles” on
    many websites that lists articles that may be related to the article you’re currently
    reading. There are many such use cases for document retrieval in a wide range
    of domains, where a user searches for specific terms, and the system must return
    a list of documents relevant to the search.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 文档检索是从数据库中检索文档以匹配用户查询的任务。例如，一家律师事务所的律师助理可能需要从法律档案中搜索有关先前案例的信息，以建立先例和研究案例法。或者，也许研究生在特定领域的文献综述过程中需要从期刊数据库中搜索文章。你可能也见过许多网站上有一个名为“相关文章”的功能，列出可能与当前阅读的文章相关的文章。在广泛的领域中，有许多这样的文档检索用例，其中用户搜索特定术语，系统必须返回与搜索相关的文档列表。
- en: 'This challenging problem has two key components: first, finding the documents
    that match the user’s query, and second, ranking the documents according to some
    notion of relevance to the user. In this case study, the problem is set up as
    a three-class classification problem of identifying the relevance rank/class (least
    relevant, moderately relevant, or highly relevant) given a query-document pair.
    We’ll explore the performance of different LightGBM classifiers for this task.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具有挑战性的问题有两个关键组成部分：首先，找到与用户查询匹配的文档，其次，根据对用户的某种相关性概念对文档进行排序。在本案例研究中，问题被设定为一个三分类问题，即根据查询-文档对识别相关性排名/类别（最不相关、中等相关或高度相关）。我们将探讨不同
    LightGBM 分类器在此任务上的性能。
- en: 5.5.1 The LETOR data set
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 LETOR 数据集
- en: The data set we’ll use for this case study is called LEarning TO Rank (LETOR)
    v4.0, which was itself created from a large corpus of web pages called GOV2\.
    The GOV2 data set ([http://mng.bz/41aD](http://mng.bz/41aD)) is a collection of
    about 25 million web pages extracted from the .gov domain.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于本案例研究的数据集称为 LEarning TO Rank (LETOR) v4.0，它本身是由一个名为 GOV2 的大型网页语料库创建的。GOV2
    数据集 ([http://mng.bz/41aD](http://mng.bz/41aD)) 是从 .gov 域提取的大约 2500 万个网页的集合。
- en: The LETOR 4.0 data collection ([http://mng.bz/Q8DR](http://mng.bz/Q8DR)) is
    derived from the GOV2 corpus and is made freely available by Microsoft Research.
    The collection contains several data sets, and we’ll use the data set that was
    originally developed for the Million Query track of the 2008 Text Retrieval Conference
    (TREC), specifically, MQ2008.rar.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: LETOR 4.0 数据集 ([http://mng.bz/Q8DR](http://mng.bz/Q8DR)) 是从 GOV2 语料库派生出来的，并由微软研究院免费提供。该集合包含几个数据集，我们将使用最初为
    2008 年文本检索会议 (TREC) 的百万查询赛道开发的数据集，具体为 MQ2008.rar。
- en: 'Each training example in the MQ2008 data set corresponds to a query-document
    *pair*. The data itself is in LIBSVM format, and several examples are shown in
    this section. Each row in the data set is a labeled training example in the format:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: MQ2008数据集中的每个训练示例对应一个查询-文档*对*。数据本身是LIBSVM格式，本节展示了几个示例。数据集中的每一行都是一个标记的训练示例，格式如下：
- en: '[PRE23]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Every example has 46 features extracted from a query-document pair, and a relevance
    label. The features include the following:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 每个示例都有46个特征，这些特征是从查询-文档对中提取的，还有一个相关性标签。特征包括以下内容：
- en: Low-level content features extracted from the body, anchor, title, and URL.
    These include features commonly used in text mining such as term frequency, inverse
    document frequency, document length, and various combinations.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从正文、锚点、标题和URL中提取的低级内容特征。这些包括在文本挖掘中常用的特征，如词频、逆文档频率、文档长度以及各种组合。
- en: 'High-level content features extracted from the body, anchor, and title. These
    features are extracted using two well-known retrieval systems: Okapi BM25 and
    language-model approaches for information retrieval (LMIR).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从正文、锚点和标题中提取的高级内容特征。这些特征使用两个著名的检索系统提取：Okapi BM25和用于信息检索的语言模型方法（LMIR）。
- en: Hyperlink features extracted from hyperlinks using several tools such as Google
    PageRank and variations.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从超链接中提取的超链接特征，使用了几种工具，如Google PageRank及其变体。
- en: Hybrid features containing both content and hyperlink information.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含内容和超链接信息的混合特征。
- en: 'The label for each query-document example is a relevance rank that takes three
    unique values: 0 (least relevant), 1 (moderately relevant), and 2 (highly relevant).
    In our case study, these are treated as class labels, making this an instance
    of a three-class classification problem. Following are some examples of the data:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 每个查询-文档示例的标签是一个相关性排名，有三个独特的值：0（最不相关）、1（中等相关）和2（高度相关）。在我们的案例研究中，这些被视为类别标签，这使得这是一个三分类问题的实例。以下是一些数据示例：
- en: '[PRE24]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Much more detail can be found in the documentation and references provided
    with the LETOR 4.0 data collection. A part of this data set that we’ll use for
    the case study is available in the companion GitHub repository. We first load
    this data set and split into training and test sets:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息可以在LETOR 4.0数据收集提供的文档和参考中找到。我们将用于案例研究的数据集的一部分可在配套的GitHub存储库中找到。我们首先加载数据集并将其分为训练集和测试集：
- en: '[PRE25]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We now have a training set with 12,000 examples and a test set with 3,000 examples.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个包含12,000个示例的训练集和一个包含3,000个示例的测试集。
- en: 5.5.2 Document retrieval with LightGBM
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 使用LightGBM进行文档检索
- en: 'We’ll learn four models using LightGBM. Each of these models represents a tradeoff
    between speed and accuracy:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LightGBM学习四个模型。这些模型代表了速度和准确度之间的权衡：
- en: '*Random forest*—Our now-familiar parallel homogeneous ensemble of randomized
    decision trees. This method will serve as a baseline approach.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机森林*——我们熟悉的并行同质集成随机决策树。这种方法将作为基线方法。'
- en: '*Gradient boosted decision trees* *(GBDT)*—This is the standard approach to
    gradient boosting and represents a balance between models with good generalization
    performance and training speed.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升决策树（GBDT）*——这是梯度提升的标准方法，代表了具有良好泛化性能和训练速度的模型之间的平衡。'
- en: '*Gradient-based One-Side Sampling (GOSS)*—This variant of gradient boosting
    downsamples the training data and is ideally suited for large data sets; due to
    downsampling, it may lose out on generalization, but is typically very fast to
    train.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于梯度的单侧采样（GOSS）*——这种梯度提升的变体对训练数据进行下采样，非常适合大型数据集；由于下采样，它可能会失去泛化能力，但通常训练速度非常快。'
- en: '*Dropouts meet Multiple Additive Regression Trees* *(DART)*—This variant incorporates
    the notion of dropout from deep learning, where neural units are randomly and
    temporarily dropped during backpropagation iterations to mitigate overfitting.
    Similarly, DART randomly and temporarily drops base estimators from the overall
    ensemble during gradient-fitting iterations to mitigate overfitting. DART is often
    the slowest of all the gradient-boosting options available in LightGBM.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropouts meets Multiple Additive Regression Trees* *(DART)*——这种变体结合了深度学习中的dropout概念，在反向传播迭代过程中随机且临时地丢弃神经单元以减轻过拟合。同样，DART在梯度拟合迭代过程中随机且临时地丢弃整体集成中的基础估计器以减轻过拟合。DART通常是LightGBM中所有梯度提升选项中最慢的。'
- en: 'We’ll train a model using each of these four approaches with the following
    learning hyperparameters. Specifically, observe that all the models are trained
    using the multiclass logistic loss, a generalization of the binary logistic loss
    function that is used in logistic regression. The number of early_stopping_rounds
    is set to 25:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下学习超参数使用这四种方法中的每一种来训练模型。具体来说，注意所有模型都是使用多类逻辑损失进行训练的，这是逻辑回归中使用的二进制逻辑损失函数的推广。early_stopping_rounds的数量设置为25：
- en: '[PRE26]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Beyond these parameters that are common to all models, we’ll also need to identify
    other model-wise hyperparameters such as learning rate (to control the rate of
    learning) or the number of leaf nodes (to control the complexity of the base estimator
    trees). These hyperparameters are selected using scikit-learn’s randomized CV
    module: RandomizedSearchCV. Specifically, we perform 5-fold CV over a grid of
    various parameter choices; however, instead of exhaustively evaluating all possible
    learning-parameter combinations the way GridSearchCV does, RandomizedSearchCV
    samples a smaller number of model combinations for faster parameter selection:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些所有模型都共有的参数之外，我们还需要确定其他模型特有的超参数，例如学习率（用于控制学习速率）或叶节点数量（用于控制基估计器树的复杂度）。这些超参数使用scikit-learn的随机交叉验证模块RandomizedSearchCV进行选择。具体来说，我们在各种参数选择的网格上执行5折交叉验证；然而，与GridSearchCV完全评估所有可能的参数组合不同，RandomizedSearchCV采样更少的模型组合以加快参数选择：
- en: '[PRE27]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following snippet is used to train random forests using LightGBM:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段用于使用LightGBM训练随机森林：
- en: '[PRE28]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Similarly, LightGBM is also trained with boosting=''gbdt'', boosting=''goss'',
    and boosting=''dart'' with code similar to the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，LightGBM也使用boosting='gbdt'、boosting='goss'和boosting='dart'进行训练，代码与以下类似：
- en: '[PRE29]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The CV-based learning-parameter selection procedure explores several different
    values for the following parameters:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 基于交叉验证的学习参数选择过程探索以下参数的几个不同值：
- en: num_leaves, which limits the number of leaf nodes and hence base-estimator complexity
    to control overfitting
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_leaves，限制叶节点的数量，从而限制基估计器复杂度以控制过拟合
- en: min_child_samples and min_child_weight, which limit each leaf node either by
    size or by the sum of Hessian values to control overfitting
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: min_child_samples和min_child_weight，通过大小或Hessian值的和限制每个叶节点，以控制过拟合
- en: subsample and colsample_bytree, which specify the fractions of training examples
    and features to sample from the training data, respectively, to accelerate training
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: subsample和colsample_bytree，分别指定从训练数据中采样训练示例和特征的分数，以加速训练
- en: reg_alpha and reg_lambda, which specify the amount of regularization of the
    leaf node values, to control overfitting as well
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reg_alpha和reg_lambda，指定叶节点值的正则化程度，以控制过拟合
- en: top_rate and other_rate, the sampling rate for GOSS (specifically)
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: top_rate和其他_rate，GOSS（特别是）的采样率
- en: drop_rate, the dropout rate for DART (specifically)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: drop_rate，DART（特别是）的dropout率
- en: '![CH05_F22_Kunapuli](../Images/CH05_F22_Kunapuli.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F22_Kunapuli](../Images/CH05_F22_Kunapuli.png)'
- en: 'Figure 5.22 All algorithms trained using LightGBM. Left: Comparing test set
    accuracy of random forest, GBDT, GOSS, and DART; Right: Comparing the overall
    training times of random forest, GBDT, GOSS, and DART. GBDT is the fastest at
    19.71 seconds and the other methods are slower as indicated, compared to GBDT.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 使用LightGBM训练的所有算法。左：比较随机森林、GBDT、GOSS和DART的测试集准确率；右：比较随机森林、GBDT、GOSS和DART的整体训练时间。GBDT以19.71秒的速度最快，其他方法如所示，比GBDT慢。
- en: 'For each of these approaches, we’re interested in looking at two performance
    measures: the test set accuracy and overall model development time, which includes
    parameter selection and training time. These are shown in figure 5.22\. The key
    takeaways are as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些方法中的每一个，我们感兴趣的是查看两个性能指标：测试集准确率和整体模型开发时间，这包括参数选择和训练时间。这些在图5.22中显示。关键要点如下：
- en: GOSS and GBDT perform similarly, including overall modeling times. This difference
    will become much more pronounced for increasingly larger data sets, especially
    those with hundreds of thousands of training examples.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GOSS和GBDT的表现相似，包括整体建模时间。这种差异将在数据集越来越大时变得更加明显，特别是那些有数十万个训练示例的数据集。
- en: DART achieves the best performance. However, this comes at the cost of significantly
    increased training time. Here, for instance, DART has a running time of close
    to 20 minutes, compared to 3 minutes for random forest and under 30 seconds for
    GBDT and GOSS.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DART 实现了最佳性能。然而，这代价是显著增加的训练时间。例如，DART 的运行时间接近 20 分钟，而随机森林为 3 分钟，GBDT 和 GOSS
    不到 30 秒。
- en: Note that LightGBM supports both multi-CPU as well as GPU processing, which
    may be able to significantly improve running times.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，LightGBM 支持多 CPU 以及 GPU 处理，这可能会显著提高运行时间。
- en: Summary
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Gradient descent is often used to minimize a loss function to train a machine-learning
    model.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降通常用于最小化损失函数以训练机器学习模型。
- en: Residuals, or errors between the true labels and model predictions, can be used
    to characterize correctly classified and poorly classified training examples.
    This is analogous to how AdaBoost uses weights.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差，即真实标签与模型预测之间的误差，可以用来描述正确分类和错误分类的训练示例。这与 AdaBoost 使用权重的方式类似。
- en: Gradient boosting combines gradient descent and boosting to learn a sequential
    ensemble of weak learners.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升结合了梯度下降和提升来学习一个弱学习者的顺序集成。
- en: Weak learners in gradient boosting are regression trees that are trained over
    the residuals of the training examples and approximate the gradient.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升中的弱学习器是训练在训练示例残差上的回归树，并近似梯度。
- en: Gradient boosting can be applied to a wide variety of loss functions arising
    from classification, regression, or ranking tasks.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升可以应用于分类、回归或排序任务中产生的各种损失函数。
- en: Histogram-based tree learning trades off exactness for efficiency, allowing
    us to train gradient-boosting models very rapidly and scale up to larger data
    sets.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于直方图的树学习在精确性和效率之间进行权衡，使我们能够非常快速地训练梯度提升模型，并扩展到更大的数据集。
- en: Learning can be sped up even further by smartly sampling training examples (Gradient-based
    One-Side Sampling, GOSS) or smartly bundling features (Exclusive Feature Bundling,
    EFB).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过智能地采样训练示例（基于梯度的单侧采样，GOSS）或智能地捆绑特征（独家特征捆绑，EFB），可以进一步加快学习速度。
- en: LightGBM is a powerful, publicly available framework for gradient boosting that
    incorporates both GOSS and EFB.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM 是一个强大的、公开可用的梯度提升框架，它结合了 GOSS 和 EFB。
- en: As with AdaBoost, we can avoid overfitting in gradient boosting by choosing
    an effective learning rate or via early stopping. LightGBM provides support for
    both.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AdaBoost 一样，我们可以通过选择有效的学习率或通过提前停止来避免梯度提升中的过拟合。LightGBM 提供了对这两种方法的支持。
- en: In addition to a wide variety of loss functions for classification, regression,
    and ranking, LightGBM also provides support for incorporation of our own custom,
    problem-specific loss functions for training.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了分类、回归和排序的广泛损失函数之外，LightGBM 还提供了支持，以便在训练中结合我们自己的定制、特定问题的损失函数。
- en: '* * *'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) LightGBM is available for Python, R, and many other platforms. See the
    LightGBM installation guide for detailed instructions on installation at [http://mng.bz/v1K1](http://mng.bz/v1K1).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ^ (1.) LightGBM 可用于 Python、R 以及许多其他平台。请参阅 LightGBM 安装指南，以获取有关安装的详细说明，请访问 [http://mng.bz/v1K1](http://mng.bz/v1K1)。
