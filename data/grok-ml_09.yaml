- en: 9 Splitting data by asking questions: Decision trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 通过提问来分割数据：决策树
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: what is a decision tree
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是什么
- en: using decision trees for classification and regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行分类和回归
- en: building an app-recommendation system using users’ information
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用用户信息构建应用推荐系统
- en: accuracy, Gini index, and entropy, and their role in building decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率、基尼指数和熵，以及它们在构建决策树中的作用
- en: using Scikit-Learn to train a decision tree on a university admissions dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn在大学录取数据集上训练决策树
- en: '![](../Images/9-unnumb-1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-unnumb-1.png)'
- en: In this chapter, we cover decision trees. Decision trees are powerful classification
    and regression models, which also give us a great deal of information about our
    dataset. Just like the previous models we’ve learned in this book, decision trees
    are trained with labeled data, where the labels that we want to predict can be
    classes (for classification) or values (for regression). For most of this chapter,
    we focus on decision trees for classification, but near the end of the chapter,
    we describe decision trees for regression. However, the structure and training
    process of both types of tree is similar. In this chapter, we develop several
    use cases, including an app-recommendation system and a model for predicting admissions
    at a university.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍决策树。决策树是强大的分类和回归模型，同时也为我们提供了大量关于数据集的信息。就像我们在本书中学到的先前模型一样，决策树使用标记数据进行训练，我们想要预测的标签可以是类别（用于分类）或值（用于回归）。在本章的大部分内容中，我们专注于分类决策树，但在本章的末尾，我们描述了回归决策树。然而，这两种类型树的结构和训练过程是相似的。在本章中，我们开发了几个用例，包括一个应用推荐系统和预测大学录取的模型。
- en: 'Decision trees follow an intuitive process to make predictions—one that very
    much resembles human reasoning. Consider the following scenario: we want to decide
    whether we should wear a jacket today. What does the decision process look like?
    We may look outside and check if it’s raining. If it’s raining, then we definitely
    wear a jacket. If it’s not, then maybe we check the temperature. If it is hot,
    then we don’t wear a jacket, but if it is cold, then we wear a jacket. In figure
    9.1, we can see a graph of this decision process, where the decisions are made
    by traversing the tree from top to bottom.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树遵循直观的过程进行预测——这与人类的推理非常相似。考虑以下场景：我们想决定今天是否应该穿夹克。决策过程看起来像什么？我们可能会向外看并检查是否在下雨。如果下雨，那么我们肯定会穿夹克。如果不下雨，那么我们可能会检查温度。如果天气热，那么我们就不穿夹克，但如果天气冷，那么我们就穿夹克。在图9.1中，我们可以看到这个决策过程的图形，其中决策是通过从上到下遍历树来做出的。
- en: '![](../Images/9-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-1.png)'
- en: Figure 9.1 A decision tree used to decide whether we want to wear a jacket or
    not on a given day. We make the decision by traversing the tree down and taking
    the branch corresponding to each correct answer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 用于决定在给定的一天是否需要穿夹克的决策树。我们通过遍历树向下并选择与每个正确答案对应的分支来做出决定。
- en: Our decision process looks like a tree, except it is upside down. The tree is
    formed of vertices, called *nodes*, and edges. On the very top, we can see the
    *root node*, from which two branches emanate. Each of the nodes has either two
    or zero branches (edges) emanating from them, and for this reason, we call it
    a *binary tree*. The nodes that have two branches emanating from them are called
    *decision nodes*, and the nodes with no branches emanating from them are called
    *leaf nodes*, or *leaves*. This arrangement of nodes, leaves, and edges is what
    we call a decision tree. Trees are natural objects in computer science, because
    computers break every process into a sequence of binary operations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的决策过程看起来像一棵树，但它是倒置的。树由顶点组成，称为**节点**和边。在最顶部，我们可以看到**根节点**，从中伸出两个分支。每个节点要么有两个或零个分支（边）从中伸出，因此我们称它为**二叉树**。有两个分支从中伸出的节点称为**决策节点**，没有分支从中伸出的节点称为**叶子节点**或**叶子**。这种节点、叶子和边的排列就是我们所说的决策树。树是计算机科学中的自然对象，因为计算机将每个过程分解成一系列二进制操作。
- en: The simplest possible decision tree, called a *decision stump*, is formed by
    a single decision node (the root node) and two leaves. This represents a single
    yes-or-no question, based on which we immediately make a decision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的决策树，称为**决策桩**，由一个决策节点（根节点）和两个叶子组成。这代表了一个简单的是或否问题，基于此我们立即做出决定。
- en: The depth of a decision tree is the number of levels underneath the root node.
    Another way to measure it is by the length of the longest path from the root node
    to a leaf, where a path is measured by the number of edges it contains. The tree
    in figure 9.1 has a depth of 2\. A decision stump has a depth of 1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的深度是根节点下方的层数。另一种衡量方法是根节点到叶子节点的最长路径的长度，其中路径的长度由其包含的边数来衡量。图9.1中的树深度为2。决策树桩的深度为1。
- en: 'Here is a summary of the definitions we’ve learned so far:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是到目前为止我们所学的定义总结：
- en: decision tree A machine learning model based on yes-or-no questions and represented
    by a binary tree. The tree has a root node, decision nodes, leaf nodes, and branches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树 基于是或否问题并由二叉树表示的机器学习模型。该树有一个根节点、决策节点、叶子节点和分支。
- en: root node The topmost node of the tree. It contains the first yes-or-no question.
    For convenience, we refer to it as the *root*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点 树的最顶层节点。它包含第一个是或否问题。为了方便起见，我们称它为*根*。
- en: decision node Each yes-or-no question in our model is represented by a decision
    node, with two branches emanating from it (one for the “yes” answer, and one for
    the “no” answer).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 决策节点 我们模型中的每个是或否问题都由一个决策节点表示，从这个节点延伸出两个分支（一个用于“是”答案，另一个用于“否”答案）。
- en: leaf node A node that has no branches emanating from it. These represent the
    decisions we make after traversing the tree. For convenience, we refer to them
    as *leaves*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 叶子节点 没有分支从其延伸出的节点。这些代表我们在遍历树后所做的决策。为了方便起见，我们称它们为*叶子*。
- en: branch The two edges emanating from each decision node, corresponding to the
    “yes” and “no” answers to the question in the node. In this chapter, by convention,
    the branch to the left corresponds to “yes” and the branch to the right to “no.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分支 从每个决策节点延伸出的两个边，对应于节点中问题的“是”和“否”答案。在本章中，按照惯例，左边的分支对应“是”，右边的分支对应“否”。
- en: depth The number of levels in the decision tree. Alternatively, it is the number
    of branches on the longest path from the root node to a leaf node.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 决策树中的层数。或者，它也是从根节点到叶子节点的最长路径上的分支数。
- en: Throughout this chapter, nodes are drawn as rectangles with rounded edges, the
    answers in the branches as diamonds, and leaves as ovals. Figure 9.2 shows how
    a decision tree looks in general.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，节点被绘制为边缘圆润的矩形，分支中的答案以菱形表示，叶子以椭圆形表示。图9.2展示了决策树的一般外观。
- en: '![](../Images/9-2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-2.png)'
- en: Figure 9.2 A regular decision tree with a root node, decision nodes, branches,
    and leaves. Note that each decision node contains a yes-or-no question. From each
    possible answer, one branch emanates, which can lead to another decision node
    or a leaf. This tree has a depth of 2, because the longest path from a leaf to
    the root goes through two branches.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 一个带有根节点、决策节点、分支和叶子的常规决策树。请注意，每个决策节点都包含一个是或否问题。从每个可能的答案中，延伸出一个分支，可以引导到另一个决策节点或叶子。这棵树深度为2，因为从叶子到根的最长路径穿过两个分支。
- en: 'How did we build this tree? Why were those the questions we asked? We could
    have also checked if it was Monday, if we saw a red car outside, or if we were
    hungry, and built the following decision tree:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何构建这棵树的？为什么我们问的是这些问题？我们还可以检查是否是星期一，是否看到外面的红色汽车，或者我们是否饿了，然后构建以下决策树：
- en: '![](../Images/9-3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-3.png)'
- en: Figure 9.3 A second (maybe not as good) decision tree we could use to decide
    whether we want to wear a jacket on a given day
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 我们可以用第二个（可能不是那么好）的决策树来决定在给定的一天是否要穿夹克
- en: 'Which tree do we think is better when it comes to deciding whether or not to
    wear a jacket: tree 1 (figure 9.1) or tree 2 (figure 9.3)? Well, as humans, we
    have enough experience to figure out that tree 1 is much better than tree 2 for
    this decision. How would a computer know? Computers don’t have experience per
    se, but they have something similar, which is data. If we wanted to think like
    a computer, we could just go over all possible trees, try each one of them for
    some time—say, one year—and compare how well they did by counting how many times
    we made the right decision using each tree. We’d imagine that if we use tree 1,
    we were correct most days, whereas if we used tree 2, we may have ended up freezing
    on a cold day without a jacket or wearing a jacket on an extremely hot day. All
    a computer has to do is go over all trees, collect data, and find which one is
    the best one, right?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到决定是否穿夹克时，我们认为哪棵树更好：树1（图9.1）还是树2（图9.3）？嗯，作为人类，我们有足够的经验来判断树1对于这个决定比树2好得多。计算机会如何知道呢？计算机本身没有经验，但它们有类似的东西，那就是数据。如果我们想像计算机一样思考，我们可以只是遍历所有可能的树，尝试每一棵树一段时间——比如说，一年——然后通过计算我们使用每一棵树做出正确决定的次数来比较它们的性能。我们可以想象，如果我们使用树1，我们大多数日子都是正确的，而如果我们使用树2，我们可能会在寒冷的一天没有穿夹克，或者在极其炎热的一天穿上了夹克。计算机所需要做的就是遍历所有树，收集数据，并找到最好的那一棵，对吧？
- en: 'Almost! Unfortunately, even for a computer, searching over all the possible
    trees to find the most effective one would take a really long time. But luckily,
    we have algorithms that make this search much faster, and thus, we can use decision
    trees for many wonderful applications, including spam detection, sentiment analysis,
    and medical diagnosis. In this chapter, we’ll go over an algorithm for constructing
    good decision trees quickly. In a nutshell, we build the tree one node at a time,
    starting from the top. To pick the right question corresponding to each node,
    we go over all the possible questions we can ask and pick the one that is right
    the highest number of times. The process goes as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎是！不幸的是，即使是对于计算机来说，搜索所有可能的树以找到最有效的一棵树也会花费非常长的时间。但是幸运的是，我们有使这种搜索变得更快的方法，因此，我们可以使用决策树进行许多奇妙的应用，包括垃圾邮件检测、情感分析和医疗诊断。在本章中，我们将介绍一种快速构建良好决策树的方法。简而言之，我们一次构建一个节点，从顶部开始。为了选择与每个节点相对应的正确问题，我们考虑所有可能的问题，并选择那些出现次数最多的一个问题。这个过程如下所示：
- en: Picking a good first question
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个好的第一个问题
- en: 'We need to pick a good first question for the root of our tree. What would
    be a good question that helps us decide whether to wear a jacket on a given day?
    Initially, it can be anything. Let’s say we come up with five candidates for our
    first question:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为树的根选择一个好的第一个问题。什么是一个好的问题，可以帮助我们决定在给定的一天是否要穿夹克？最初，它可以是任何问题。让我们假设我们为第一个问题想出了五个候选问题：
- en: Is it raining?
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天气在下雨吗？
- en: Is it cold outside?
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外面冷吗？
- en: Am I hungry?
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我饿吗？
- en: Is there a red car outside?
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外面有红色的车吗？
- en: Is it Monday?
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是星期一吗？
- en: Out of these five questions, which one seems like the best one to help us decide
    whether we should wear a jacket? Our intuition says that the last three questions
    are useless to help us decide. Let’s say that from experience, we’ve noticed that
    among the first two, the first one is more useful. We use that question to start
    building our tree. So far, we have a simple decision tree, or a decision stump,
    consisting of that single question, as illustrated in Figure 9.4.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这五个问题中，哪一个看起来最适合帮助我们决定是否应该穿夹克？我们的直觉告诉我们，最后三个问题对我们做出决定没有帮助。让我们假设，根据经验，我们发现前两个问题中，第一个更有用。我们使用这个问题开始构建我们的树。到目前为止，我们有一个简单的决策树，或者称为决策桩，由那个单一的问题组成，如图9.4所示。
- en: '![](../Images/9-4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-4.png)'
- en: Figure 9.4 A simple decision tree (decision stump) that consists of only the
    question, “Is it raining?” If the answer is yes, the decision we make is to wear
    a jacket.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 一个简单的决策树（决策桩），它只包含一个问题：“天气在下雨吗？”如果答案是肯定的，我们做出的决定是穿上夹克。
- en: 'Can we do better? Imagine that we start noticing that when it rains, wearing
    a jacket is always the correct decision. However, there are days on which it doesn’t
    rain, and not wearing a jacket is not the correct decision. This is where question
    2 comes to our rescue. We use that question to help us in the following way: after
    we check that it is not raining, *then* we check the'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否做得更好？想象一下，当我们开始注意到下雨时，穿夹克总是正确的决定。然而，有些天不下雨，不穿夹克就不是正确的决定。这就是问题2出现的时候。我们用这个问题来帮助我们，以下是这样做的：在我们确认没有下雨之后，*然后*我们检查
- en: temperature, and if it is cold, we decide to wear a jacket. This turns the left
    leaf of the tree into a node, with two leaves emanating from it, as shown in figure
    9.5.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 温度，如果天气冷，我们就决定穿夹克。这使得树的左叶子变成一个节点，从这个节点延伸出两个叶子，如图9.5所示。
- en: '![](../Images/9-5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-5.png)'
- en: Figure 9.5 A slightly more complicated decision tree than the one in figure
    9.4, where we have picked one leaf and split it into two further leaves. This
    is the same tree as in figure 9.1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：比图9.4中的决策树稍微复杂一些，我们在其中选择了一片叶子并将其分割成两个更进一步的叶子。这与图9.1中的树是相同的。
- en: Now we have our decision tree. Can we do better? Maybe we can if we add more
    nodes and leaves to our tree. But for now, this one works very well. In this example,
    we made our decisions using our intuition and our experience. In this chapter,
    we learn an algorithm that builds these trees solely based on data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的决策树。我们能做得更好吗？也许我们可以通过向我们的树中添加更多的节点和叶子来实现。但到目前为止，这个树已经非常有效了。在这个例子中，我们使用我们的直觉和经验来做决定。在本章中，我们将学习一个仅基于数据构建这些树的算法。
- en: 'Many questions may arise in your head, such as the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在脑海中涌现出许多问题，例如以下这些问题：
- en: How exactly do you decide which is the best possible question to ask?
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你究竟是如何决定提出哪个最佳问题的？
- en: Does the process of always picking the best possible question actually get us
    to build *the* best decision tree?
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总是选择最佳可能问题的过程实际上能让我们构建出*最佳*决策树吗？
- en: Why don’t we instead build all the possible decision trees and pick the best
    one from there?
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不构建所有可能的决策树，然后从中选择最好的一个呢？
- en: Will we code this algorithm?
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们会编写这个算法吗？
- en: Where can we find decision trees in real life?
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在现实生活中可以在哪里找到决策树？
- en: We can see how decision trees work for classification, but how do they work
    for regression?
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到决策树是如何用于分类的，但它们又是如何用于回归的呢？
- en: 'This chapter answers all of these questions, but here are some quick answers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章回答了所有这些问题，但这里有一些快速的答案：
- en: '**How exactly do you decide which is the best possible question to ask?**'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**你究竟是如何决定提出哪个最佳问题的？**'
- en: 'We have several ways to do this. The simplest one is using accuracy, which
    means: which question helps me be correct more often? However, in this chapter,
    we also learn other methods, such as Gini index or entropy.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们有几种方法可以做到这一点。最简单的一种是使用准确度，这意味着：哪个问题能让我更频繁地做出正确的决定？然而，在本章中，我们还学习了其他方法，例如基尼指数或熵。
- en: '**Does the process of always picking the best possible question actually get
    us to build** *the* **best decision tree?**'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**总是选择最佳可能问题的过程实际上能让我们构建出** *最佳* **决策树吗？**'
- en: 'Actually, this process does not guarantee that we get the best possible tree.
    This is what we call a *greedy algorithm*. Greedy algorithms work as follows:
    at every point, the algorithm makes the best possible available move. They tend
    to work well, but it’s not always the case that making the best possible move
    at each timestep gets you to the best overall outcome. There may be times in which
    asking a weaker question groups our data in a way that we end up with a better
    tree at the end of the day. However, the algorithms for building decision trees
    tend to work very well and very quickly, so we’ll live with this. Look at the
    algorithms that we see in this chapter, and try to figure out ways to improve
    them by removing the greedy property!'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际上，这个过程并不能保证我们得到最佳可能的树。这就是我们所说的*贪婪算法*。贪婪算法是这样工作的：在每一个点上，算法做出最佳可能的选择。它们通常工作得很好，但并不是在每一个时间步都做出最佳可能的选择就能得到最佳的整体结果。有时候，提出一个较弱的问题可能会以某种方式对我们的数据进行分组，最终我们得到一个更好的树。然而，构建决策树的算法通常工作得非常好且非常快，所以我们就这样接受了。看看本章中我们看到的所有算法，并尝试找出去除贪婪属性的方法来改进它们！
- en: '**Why don’t we instead build all the possible decision trees and pick the best
    one from there?**'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么我们不构建所有可能的决策树，然后从中选择最好的一个呢？**'
- en: The number of possible decision trees is very large, especially if our dataset
    has many features. Going through all of them would be very slow. Here, finding
    each node requires only a linear search across the features and not across all
    the possible trees, which makes it much faster.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的决策树数量非常大，尤其是如果我们的数据集有很多特征。遍历所有这些树会非常慢。在这里，找到每个节点只需要在特征上做线性搜索，而不是在所有可能的树上搜索，这使得它更快。
- en: '**Will we code this algorithm?**'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们会编码这个算法吗？**'
- en: This algorithm can be coded by hand. However, we’ll see that because it is recursive,
    the coding can get a bit tedious. Thus, we’ll use a useful package called Scikit-Learn
    to build decision trees with real data.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个算法可以手动编码。然而，我们会看到，由于它是递归的，编码可能会有些繁琐。因此，我们将使用一个有用的包Scikit-Learn，用真实数据构建决策树。
- en: '**Where can we find decision trees in real life?**'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们可以在现实生活中找到决策树吗？**'
- en: In many places! They are used extensively in machine learning, not only because
    they work very well but also because they give us a lot of information on our
    data. Some places in which decision trees are used are in recommendation systems
    (to recommend videos, movies, apps, products to buy, etc)., in spam classification
    (to decide whether or not an email is spam), in sentiment analysis (to decide
    whether a sentence is happy or sad), and in biology (to decide whether or not
    a patient is sick or to help identify certain hierarchies in species or in types
    of genomes).
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在许多地方！它们在机器学习中被广泛使用，不仅因为它们工作得非常好，而且因为它们为我们提供了大量关于数据的信息。决策树被使用的某些地方包括在推荐系统中（推荐视频、电影、app、购买产品等），在垃圾邮件分类中（决定一封邮件是否为垃圾邮件），在情感分析中（决定一句话是快乐还是悲伤），以及在生物学中（决定患者是否生病或帮助识别物种或基因组类型中的某些层次结构）。
- en: '**We can see how decision trees work for classification, but how do they work
    for regression?**'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们可以看到决策树是如何进行分类的，但它们又是如何进行回归的呢？**'
- en: A regression decision tree looks exactly like a classification decision tree,
    except for the leaves. In a classification decision tree, the leaves have classes,
    such as yes and no. In a regression decision tree, the leaves have values, such
    as 4, 8.2, or –199\. The prediction our model makes is given by the leaf at which
    we arrived when traversing the tree in a downward fashion.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回归决策树看起来与分类决策树完全一样，只是叶子节点不同。在分类决策树中，叶子节点有类别，如是或否。在回归决策树中，叶子节点有值，如4，8.2，或-199。我们的模型做出的预测是在向下遍历树时到达的叶子节点。
- en: 'The first use case that we’ll study in this chapter is a popular application
    in machine learning, and one of my favorites: recommendation systems.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将研究的第一个用例是机器学习中的一个流行应用，也是我最喜欢的之一：推荐系统。
- en: 'The code for this chapter is available in this GitHub repository: [https://github.com/luisguiserrano/manning/tree/master/Chapter_9_Decision_Trees](https://github.com/luisguiserrano/manning/tree/master/Chapter_9_Decision_Trees).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在以下GitHub仓库中找到：[https://github.com/luisguiserrano/manning/tree/master/Chapter_9_Decision_Trees](https://github.com/luisguiserrano/manning/tree/master/Chapter_9_Decision_Trees)。
- en: 'The problem: We need to recommend apps to users according to what they are
    likely to download'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：我们需要根据用户可能下载的内容向他们推荐app
- en: 'Recommendation systems are one of the most common and exciting applications
    in machine learning. Ever wonder how Netflix recommends movies, YouTube guesses
    which videos you may watch, or Amazon shows you products you might be interested
    in buying? These are all examples of recommendation systems. One simple and interesting
    way to see recommendation problems is to consider them classification problems.
    Let’s start with an easy example: our very own app-recommendation system using
    decision trees.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是机器学习中最为常见和激动人心的应用之一。你是否好奇Netflix是如何推荐电影的，YouTube是如何猜测你可能观看的视频，或者Amazon是如何展示你可能感兴趣购买的产品？这些都是推荐系统的例子。一个简单且有趣的方式来观察推荐问题，就是将它们视为分类问题。让我们从一个简单的例子开始：我们自己的使用决策树的app推荐系统。
- en: 'Let’s say we want to build a system that recommends to users which app to download
    among the following options. We have the following three apps in our store (figure
    9.6):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要构建一个系统，向用户推荐他们可以在以下选项中下载的app。我们商店中有以下三个app（图9.6）：
- en: '**Atom Count**: an app that counts the number of atoms in your body'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原子计数器**：一个用于计算你身体中原子数量的app'
- en: '**Beehive Finder**: an app that maps your location and finds the closest beehives'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蜂巢寻找者**：一个将你的位置映射并找到最近蜂巢的app'
- en: '**Check Mate Mate**: an app for finding Australian chess players'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查棋局**：一个用于寻找澳大利亚棋手的app'
- en: '![](../Images/9-6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-6.png)'
- en: 'Figure 9.6 The three apps we are recommending: Atom Count, an app for counting
    the number of atoms in your body; Beehive Finder, an app for locating the nearest
    beehives to your location; and Check Mate Mate, an app for finding Australian
    chess players in your area'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 我们推荐的三款应用：Atom Count，一款用于计算你体内原子数量的应用；Beehive Finder，一款用于定位你位置附近蜂箱的应用；以及
    Check Mate Mate，一款用于寻找你所在地区澳大利亚棋手的应用
- en: The training data is a table with the platform used by the user (iPhone or Android),
    their age, and the app they have downloaded (in real life there are many more
    platforms, but for simplicity we’ll assume that these are the only two options).
    Our table contains six people, as shown in table 9.1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是一个表格，包含用户使用的平台（iPhone 或 Android）、他们的年龄以及他们下载的应用（在现实生活中有更多平台，但为了简单起见，我们假设只有这两种选项）。我们的表格包含六个人，如表
    9.1 所示。
- en: Table 9.1 A dataset with users of an app store. For each customer, we record
    their platform, age, and the app they downloaded.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 一个应用商店的用户数据集。对于每个客户，我们记录他们的平台、年龄和他们下载的应用。
- en: '| Platform | Age | App |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | 年龄 | 应用 |'
- en: '| iPhone | 15 | Atom Count |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 15 | Atom Count |'
- en: '| iPhone | 25 | Check Mate Mate |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 25 | Check Mate Mate |'
- en: '| Android | 32 | Beehive Finder |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Android | 32 | Beehive Finder |'
- en: '| iPhone | 35 | Check Mate Mate |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 35 | Check Mate Mate |'
- en: '| Android | 12 | Atom Count |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Android | 12 | Atom Count |'
- en: '| Android | 14 | Atom Count |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Android | 14 | Atom Count |'
- en: Given this table, which app would you recommend to each of the following three
    customers?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个表格，你会向以下三位客户推荐哪个应用？
- en: '**Customer 1**: a 13-year-old iPhone user'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户 1**：一位 13 岁的 iPhone 用户'
- en: '**Customer 2**: a 28-year-old iPhone user'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户 2**：一位 28 岁的 iPhone 用户'
- en: '**Customer 3**: a 34-year-old Android user'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户 3**：一位 34 岁的 Android 用户'
- en: 'What we should do follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该采取以下措施：
- en: '**Customer 1:** a 13-year-old iPhone user. To this customer, we should recommend
    Atom Count, because it seems (looking at the three customers in their teens) that
    young people tend to download Atom Count.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户 1**：一位 13 岁的 iPhone 用户。对于这位客户，我们应该推荐 Atom Count，因为从这些青少年客户（三个）来看，年轻人似乎更倾向于下载
    Atom Count。'
- en: '**Customer 2:** a 28-year-old iPhone user. To this customer, we should recommend
    Check Mate Mate, because looking at the two iPhone users in the dataset (aged
    25 and 35), they both downloaded Check Mate Mate.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户 2**：一位 28 岁的 iPhone 用户。对于这位客户，我们应该推荐 Check Mate Mate，因为从数据集中两位 iPhone
    用户（年龄分别为 25 岁和 35 岁）来看，他们都下载了 Check Mate Mate。'
- en: '**Customer 3:** a 34-year-old Android user. To this customer, we should recommend
    Beehive Finder, because there is one Android user in the dataset who is 32 years
    old, and they downloaded Beehive Finder.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户 3**：一位 34 岁的 Android 用户。对于这位客户，我们应该推荐 Beehive Finder，因为在数据集中有一位 32 岁的
    Android 用户下载了 Beehive Finder。'
- en: However, going customer by customer seems like a tedious job. Next, we’ll build
    a decision tree to take care of all customers at once.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，逐个客户进行操作似乎是一项繁琐的工作。接下来，我们将构建一个决策树来一次性处理所有客户。
- en: 'The solution: Building an app-recommendation system'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案：构建一个应用推荐系统
- en: 'In this section, we see how to build an app-recommendation system using decision
    trees. In a nutshell, the algorithm to build a decision tree follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用决策树构建一个应用推荐系统。简而言之，构建决策树的算法遵循以下步骤：
- en: Figure out which of the data is the most useful to decide which app to recommend.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定哪些数据对于决定推荐哪个应用最有用。
- en: This feature splits the data into two smaller datasets.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个特征将数据分割成两个更小的数据集。
- en: Repeat processes 1 and 2 for each of the two smaller datasets.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个较小的数据集重复步骤 1 和 2。
- en: In other words, what we do is decide which of the two features (platform or
    age) is more successful at determining which app the users will download and pick
    this one as our root of the decision tree. Then, we iterate over the branches,
    always picking the most determining feature for the data in that branch, thus
    building our decision tree.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们所做的是决定两个特征（平台或年龄）中哪一个更成功地决定了用户会下载哪个应用，并选择这个作为我们的决策树的根。然后，我们遍历分支，始终选择该分支数据中最具决定性的特征，从而构建我们的决策树。
- en: 'First step to build the model: Asking the best question'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的第一步：提出最佳问题
- en: 'The first step to build our model is to figure out the most useful feature:
    in other words, the most useful question to ask. First, let’s simplify our data
    a little bit. Let’s call everyone under 20 years old “Young” and everyone 20 or
    older “Adult” (don’t worry—we’ll go back to the original dataset soon, in the
    section “Splitting the data using continuous features, such as age”). Our modified
    dataset is shown in table 9.2.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 构建我们的模型的第一步是找出最有用的特征：换句话说，最有用的提问。首先，让我们稍微简化我们的数据。让我们称20岁以下的人为“青年”，20岁及以上的人为“成人”（别担心——我们很快就会回到原始数据集，在“使用连续特征（如年龄）划分数据”部分）。我们的修改后的数据集如表9.2所示。
- en: Table 9.2 A simplified version of the dataset in table 9.1, where the age column
    has been simplified to two categories, “young” and “adult”
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.2 表9.1中数据集的简化版本，其中年龄列已简化为两个类别，“青年”和“成人”
- en: '| Platform | Age | App |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | 年龄 | 应用 |'
- en: '| iPhone | Young | Atom Count |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 青年 | Atom Count |'
- en: '| iPhone | Adult | Check Mate Mate |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 成人 | Check Mate Mate |'
- en: '| Android | Adult | Beehive Finder |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Android | 成人 | Beehive Finder |'
- en: '| iPhone | Adult | Check Mate Mate |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 成人 | Check Mate Mate |'
- en: '| Android | Young | Atom Count |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Android | 青年 | Atom Count |'
- en: '| Android | Young | Atom Count |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Android | 青年 | Atom Count |'
- en: The building blocks of decision trees are questions of the form “Does the user
    use an iPhone?” or “Is the user young?” We need one of these to use as our root
    of the tree. Which one should we pick? We should pick the one that best determines
    the app they downloaded. To decide which question is better at this, let’s compare
    them.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的基本构建块是形式为“用户是否使用iPhone？”或“用户是否是青年？”的问题。我们需要其中之一作为树的根。我们应该选择哪一个？我们应该选择最能确定他们下载的应用的问题。为了决定哪个问题在这方面更好，让我们比较它们。
- en: 'First question: Does the user use an iPhone or Android?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题：用户使用的是iPhone还是Android？
- en: 'This question splits the users into two groups, the iPhone users and Android
    users. Each group has three users in it. But we need to keep track of which app
    each user downloaded. A quick look at table 9.2 helps us notice the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题将用户分为两组，iPhone用户和Android用户。每组都有三个用户。但我们需要跟踪每个用户下载了哪个应用。快速查看表9.2可以帮助我们注意到以下内容：
- en: Of the iPhone users, one downloaded Atom Count and two downloaded Check Mate
    Mate.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在iPhone用户中，有一个人下载了Atom Count，两个人下载了Check Mate Mate。
- en: Of the Android users, two downloaded Atom Count and one downloaded Beehive Finder.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Android用户中，有两个人下载了Atom Count，一个人下载了Beehive Finder。
- en: The resulting decision stump is shown in figure 9.7.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果决策树桩如图9.7所示。
- en: '![](../Images/9-7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-7.png)'
- en: 'Figure 9.7 If we split our users by platform, we get this split: the iPhone
    users are on the left, and the Android users on the right. Of the iPhone users,
    one downloaded Atom Count and two downloaded Check Mate Mate. Of the Android users,
    two downloaded Atom Count and one downloaded Beehive Finder.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 如果我们按平台划分我们的用户，我们得到以下划分：iPhone用户在左侧，Android用户在右侧。在iPhone用户中，有一个人下载了Atom
    Count，两个人下载了Check Mate Mate。在Android用户中，有两个人下载了Atom Count，一个人下载了Beehive Finder。
- en: Now let’s see what happens if we split them by age.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如果我们按年龄划分会发生什么。
- en: 'Second question: Is the user young or adult?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题：用户是青年还是成人？
- en: 'This question splits the users into two groups, the young and the adult. Again,
    each group has three users in it. A quick look at table 9.2 helps us notice what
    each user downloaded, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题将用户分为两组，青年和成人。再次，每组都有三个用户。快速查看表9.2可以帮助我们注意到每个用户下载了什么，如下所示：
- en: The young users all downloaded Atom Count.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 青年用户都下载了Atom Count。
- en: Of the adult users, two downloaded Atom Count and one downloaded Beehive Finder.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在成人用户中，有两个人下载了Atom Count，一个人下载了Beehive Finder。
- en: The resulting decision stump is shown in figure 9.8.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果决策树桩如图9.8所示。
- en: '![](../Images/9-8.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-8.png)'
- en: 'Figure 9.8 If we split our users by age, we get this split: the young are on
    the left, and the adults on the right. Out of the young users, all three downloaded
    Atom Count. Out of the adult users, one downloaded Beehive Finder and two downloaded
    Check Mate Mate.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 如果我们按年龄划分我们的用户，我们得到以下划分：青年在左侧，成人在右侧。在青年用户中，所有三个都下载了Atom Count。在成人用户中，一个人下载了Beehive
    Finder，两个人下载了Check Mate Mate。
- en: 'From looking at figures 9.7 and 9.8, which one looks like a better split? It
    seems that the second one (based on age) is better, because it has picked up on
    the fact that all three young people downloaded Atom Count. But we need the computer
    to figure out that age is a better feature, so we’ll give it some numbers to compare.
    In this section, we learn three ways to compare these two splits: accuracy, Gini
    impurity, and entropy. Let’s start with the first one: accuracy.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从图9.7和9.8中看，哪一个看起来是更好的分割？似乎第二个（基于年龄）更好，因为它注意到了所有三个年轻人下载了Atom Count的事实。但我们需要计算机来确定年龄是一个更好的特征，所以我们会给它一些数字来比较。在本节中，我们学习三种比较这两种分割的方法：准确率、吉尼不纯度和熵。让我们从第一个开始：准确率。
- en: 'Accuracy: How often is our model correct?'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：我们的模型有多正确？
- en: We learned about accuracy in chapter 7, but here is a small recap. Accuracy
    is the fraction of correctly classified data points over the total number of data
    points.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第七章学习了准确率，但这里有一个简短的回顾。准确率是正确分类的数据点数占总数据点数的比例。
- en: 'Suppose that we are allowed only one question, and with that one question,
    we must determine which app to recommend to our users. We have the following two
    classifiers:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们只被允许问一个问题，并且必须通过这个问题来确定向用户推荐哪个应用。我们有以下两个分类器：
- en: '**Classifier** 1: asks the question “What platform do you use?” and from there,
    determines what app to recommend'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器** 1：询问“你使用什么平台？”然后根据这个信息确定推荐哪个应用'
- en: '**Classifier 2**: asks the question “What is your age?” and from there, determines
    what app to recommend'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器2**：询问“你多大年龄？”然后根据这个信息确定推荐哪个应用'
- en: 'Let’s look more carefully at the classifiers. The key observation follows:
    if we must recommend an app by asking only one question, our best bet is to look
    at all the people who answered with the same answer and recommend the most common
    app among them.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看分类器。关键观察结果是：如果我们必须通过只问一个问题来推荐一个应用，我们最好的选择是查看所有给出相同答案的人，并推荐他们中最常见的应用。
- en: '**Classifier 1**: What platform do you use?'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类器1**：你使用什么平台？'
- en: If the answer is “iPhone,” then we notice that of the iPhone users, the majority
    downloaded Check Mate Mate. Therefore, we recommend Check Mate Mate to all the
    iPhone users. We are correct **two times out of three**
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果答案是“iPhone”，那么我们会注意到在所有iPhone用户中，大多数下载了Check Mate Mate。因此，我们向所有iPhone用户推荐Check
    Mate Mate。我们正确率是**三分之二**。
- en: If the answer is “Android,” then we notice that of the Android users, the majority
    downloaded Atom Count, so that is the one we recommend to all the Android users.
    We are correct **two times out of three**.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果答案是“Android”，那么我们会注意到在所有Android用户中，大多数下载了Atom Count，所以我们向所有Android用户推荐这个应用。我们正确率是**三分之二**。
- en: '**Classifier 2**: What is your age?'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类器2**：你多大年龄？'
- en: If the answer is “young,” then we notice that all the young people downloaded
    Atom Count, so that is the recommendation we make. We are correct **three times
    out of three**.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果答案是“年轻”，那么我们会注意到所有年轻人下载了Atom Count，所以我们做出这个推荐。我们正确率是**三次三**。
- en: If the answer is “adult,” then we notice that of the adults, the majority downloaded
    Check Mate Mate, so we recommend that one. We are correct **two times out of three**.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果答案是“成人”，那么我们会注意到在所有成人中，大多数下载了Check Mate Mate，所以我们推荐这个应用。我们正确率是**三分之二**。
- en: Notice that classifier 1 is correct **four times out of six**, and classifier
    2 is correct **five times out of six.** Therefore, for this dataset, classifier
    2 is better. In figure 9.9, you can see the two classifiers with their accuracy.
    Notice that the questions are reworded so that they have yes-or-no answers, which
    doesn’t change the classifiers or the outcome.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到分类器1有**四次六次正确**，分类器2有**五次六次正确**。因此，对于这个数据集，分类器2更好。在图9.9中，你可以看到这两个分类器及其准确率。注意，问题被重新措辞，以便它们有是或否的答案，这不会改变分类器或结果。
- en: '![](../Images/9-9.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-9.png)'
- en: Figure 9.9 Classifier 1 uses platform, and classifier 2 uses age. To make the
    prediction at each leaf, each classifier picks the most common label among the
    samples in that leaf. Classifier 1 is correct four out of six times, and classifier
    2 is correct five out of six times. Therefore, based on accuracy, classifier 2
    is better.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9中，分类器1使用平台，分类器2使用年龄。为了在每个叶节点上进行预测，每个分类器都会选择该叶节点样本中最常见的标签。分类器1有四次是正确的，分类器2有五次是正确的。因此，基于准确率，分类器2表现更好。
- en: 'Gini impurity index: How diverse is my dataset?'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 吉尼不纯度指数：我的数据集有多多样？
- en: 'The *Gini impurity index,* or *Gini index*, is another way we can compare the
    platform and age splits. The Gini index is a measure of diversity in a dataset.
    In other words, if we have a set in which all the elements are similar, this set
    has a low Gini index, and if all the elements are different, it has a large Gini
    index. For clarity, consider the following two sets of 10 colored balls (where
    any two balls of the same color are indistinguishable):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*基尼不纯度指数*，或*基尼指数*，是我们比较平台和年龄分割的另一种方法。基尼指数是数据集中多样性的度量。换句话说，如果我们有一个所有元素都相似的集合，这个集合的基尼指数就低，如果所有元素都不同，它就有大的基尼指数。为了清晰起见，考虑以下两组10个彩色球（其中任何两个相同颜色的球都是不可区分的）：'
- en: '**Set 1**: eight red balls, two blue balls'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合1**：八个红色球，两个蓝色球'
- en: '**Set 2**: four red balls, three blue balls, two yellow balls, one green ball'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合2**：四个红色球，三个蓝色球，两个黄色球，一个绿色球'
- en: 'Set 1 looks more pure than set 2, because set 1 contains mostly red balls and
    a couple of blue ones, whereas set 2 has many different colors. Next, we devise
    a measure of impurity that assigns a low value to set 1 and a high value to set
    2\. This measure of impurity relies on probability. Consider the following question:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 集合1看起来比集合2更纯净，因为集合1主要包含红色球和一些蓝色球，而集合2有许多不同的颜色。接下来，我们制定一个不纯度的度量标准，将低值分配给集合1，将高值分配给集合2。这个不纯度度量标准依赖于概率。考虑以下问题：
- en: If we pick two random elements of the set, what is the probability that they
    have a different color? The two elements don’t need to be distinct; we are allowed
    to pick the same element twice.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从集合中随机抽取两个元素，它们颜色不同的概率是多少？这两个元素不需要是不同的；我们允许重复抽取同一个元素。
- en: For set 1, this probability is low, because the balls in the set have similar
    colors. For set 2, this probability is high, because the set is diverse, and if
    we pick two balls, they’re likely to be of
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合1，这个概率较低，因为集合中的球颜色相似。对于集合2，这个概率较高，因为集合多样化，如果我们抽取两个球，它们很可能是不同颜色的。
- en: 'different colors. Let’s calculate these probabilities. First, notice that by
    the law of complementary probabilities (see the section “What the math just happened?”
    in chapter 8), the probability that we pick two balls of different colors is 1
    minus the probability that we pick two balls of the same color:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 不同颜色的。让我们计算这些概率。首先，注意根据互补概率定律（见第8章“数学刚刚发生了什么？”部分），我们抽取两个不同颜色球的概率是1减去我们抽取两个相同颜色球的概率：
- en: P(picking two balls of different color) = 1 – P(picking two balls of the same
    color)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: P(抽取两个不同颜色的球) = 1 – P(抽取两个相同颜色的球)
- en: 'Now let’s calculate the probability that we pick two balls of the same color.
    Consider a general set, where the balls have *n* colors. Let’s call them color
    1, color 2, all the way up to color *n*. Because the two balls must be of one
    of the *n* colors, the probability of picking two balls of the same color is the
    sum of probabilities of picking two balls of each of the *n* colors:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算抽取两个相同颜色球的概率。考虑一个一般的集合，其中球有 *n* 种颜色。让我们称它们为颜色1，颜色2，一直到颜色 *n*。因为两个球必须属于这
    *n* 种颜色之一，所以抽取两个相同颜色球的概率是抽取每种颜色两个球的概率之和：
- en: P(picking two balls of the same color) = P(both balls are color 1) + P(both
    balls are color 2) + … + P(both balls are color *n*)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: P(抽取两个相同颜色的球) = P(两个球都是颜色1) + P(两个球都是颜色2) + … + P(两个球都是颜色 *n*)
- en: 'What we used here is the sum rule for disjoint probabilities, that states the
    following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的是不相交概率的求和规则，该规则如下：
- en: sum rule for disjoint probabilities If two events *E* and *F* are disjoint,
    namely, they never occur at the same time, then the probability of either one
    of them happening (the union of the events) is the sum of the probabilities of
    each of the events. In other words,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不相交概率的求和规则 如果两个事件 *E* 和 *F* 是不相交的，即它们永远不会同时发生，那么其中一个发生（事件的并集）的概率是每个事件概率的和。换句话说，
- en: '*P*(*E* ∪ *F*) = *P*(*E*) + *P*(*F*)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*E* ∪ *F*) = *P*(*E*) + *P*(*F*)'
- en: Now, let’s calculate the probability that two balls have the same color, for
    each of the colors. Notice that we’re picking each ball completely independently
    from the others. Therefore, by the product rule for independent probabilities
    (section “What the math just happened?” in chapter 8), the probability that both
    balls have color 1 is the square of the probability that we pick one ball and
    it is of color 1\. In general, if *p*[i] is the probability that we pick a random
    ball and it is of color i, then
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算两个球颜色相同的概率，对于每种颜色。注意，我们完全独立地抽取每个球。因此，根据独立概率的乘法法则（第 8 章的“数学发生了什么？”部分），两个球都是颜色
    1 的概率是抽取一个球且它为颜色 1 的概率的平方。一般来说，如果 *p*[i] 是我们随机抽取一个球且它为颜色 i 的概率，那么
- en: P(both balls are color i) = *p*[i]².
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: P(两个球都是颜色 i) = *p*[i]²。
- en: Putting all these formulas together (figure 9.10), we get that
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些公式放在一起（图 9.10），我们得到
- en: P(picking two balls of different colors) = 1 – *p*[1]² – *p*[2]² – … – *p*[n]².
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: P(抽取两个不同颜色的球) = 1 – *p*[1]² – *p*[2]² – … – *p*[n]²。
- en: This last formula is the Gini index of the set.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的公式是集合的基尼指数。
- en: '![](../Images/9-10.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-10.png)'
- en: Figure 9.10 Summary of the calculation of the Gini impurity index
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 基尼不纯度指数的计算总结
- en: Finally, the probability that we pick a random ball of color *i* is the number
    of balls of color *i* divided by the total number of balls. This leads to the
    formal definition of the Gini index.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随机抽取一个颜色为 *i* 的球的概率是颜色为 *i* 的球的数量除以球的总数。这导致了基尼指数的正式定义。
- en: gini impurity index In a set with *m* elements and *n* classes, with *a*[i]
    elements belonging to the *i*-th class, the Gini impurity index is
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度指数 在一个包含 *m* 个元素和 *n* 个类别的集合中，其中 *a*[i] 个元素属于第 *i* 类，基尼不纯度指数是
- en: '*Gini* = 1 – *p*[1]² – *p*[2]² – … – *p*[n]²,'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gini* = 1 – *p*[1]² – *p*[2]² – … – *p*[n]²，'
- en: where *p*[i] = *a*[i] / *m*. This can be interpreted as the probability that
    if we pick two random elements out of the set, they belong to different classes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[i] = *a*[i] / *m*。这可以解释为如果我们从集合中随机抽取两个元素，它们属于不同类别的概率。
- en: Now we can calculate the Gini index for both of our sets. For clarity, the calculation
    of the Gini index for set 1 is illustrated in figure 9.11 (with red and blue replaced
    by black and white).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算我们两个集合的基尼指数。为了清晰起见，集合 1 的基尼指数计算在图 9.11 中展示（红色和蓝色被黑色和白色替代）。
- en: '**Set 1**: {red, red, red, red, red, red, red, red, blue, blue} (eight red
    balls, two blue balls)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合 1**：{红色，红色，红色，红色，红色，红色，红色，红色，蓝色，蓝色}（八个红色球，两个蓝色球）'
- en: '![](../Images/09_10_E01.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_10_E01.png)'
- en: '**Set 2**: {red, red, red, red, blue, blue, blue, yellow, yellow, green}'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合 2**：{红色，红色，红色，红色，蓝色，蓝色，蓝色，黄色，黄色，绿色}'
- en: '![](../Images/09_10_E02.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_10_E02.png)'
- en: Notice that, indeed, the Gini index of set 1 is larger than that of set 2.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实际上集合 1 的基尼指数大于集合 2 的基尼指数。
- en: '![](../Images/9-111.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-111.png)'
- en: Figure 9.11 The calculation of the Gini index for the set with eight black balls
    and two white balls. Note that if the total area of the square is 1, the probability
    of picking two black balls is 0.8², and the probability of picking two white balls
    is 0.2² (these two are represented by the shaded squares). Thus, the probability
    of picking two balls of a different color is the remaining area, which is 1 –
    0.8² – 0.2² =0.32\. That is the Gini index.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 具有八个黑色球和两个白色球的集合的基尼指数计算。注意，如果正方形的总面积为 1，则抽取两个黑色球的概率是 0.8²，抽取两个白色球的概率是
    0.2²（这两个用阴影部分表示）。因此，抽取两个不同颜色球的概率是剩余的面积，即 1 – 0.8² – 0.2² = 0.32。这就是基尼指数。
- en: 'How do we use the Gini index to decide which of the two ways to split the data
    (age or platform) is better? Clearly, if we can split the data into two purer
    datasets, we have performed a better split. Thus, let’s calculate the Gini index
    of the set of labels of each of the leaves. Looking at figure 9.12, here are the
    labels of the leaves (where we abbreviate each app by the first letter in its
    name):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用基尼指数来决定两种数据分割方式（年龄或平台）哪种更好？显然，如果我们能将数据分割成两个更纯净的数据集，我们就进行了更好的分割。因此，让我们计算每个叶子的标签集合的基尼指数。查看图
    9.12，以下是叶子的标签（我们用每个应用的名称的第一个字母来缩写每个应用）：
- en: 'Classifier 1 (by platform):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 1（按平台分类）：
- en: 'Left leaf (iPhone): {A, C, C}'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧叶子（iPhone）：{A，C，C}
- en: 'Right leaf (Android): {A, A, B}'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧叶子（Android）：{A，A，B}
- en: 'Classifier 2 (by age):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 2（按年龄分类）：
- en: 'Left leaf (young): {A, A, A}'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧叶子（年轻人）：{A，A，A}
- en: 'Right leaf (adult): {B, C, C}'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧叶子（成年人）：{B，C，C}
- en: 'The Gini indices of the sets {A, C, C}, {A, A, B}, and {B, C, C} are all the
    same: ![](../Images/09_11_E01.png). The Gini index of the set {A, A, A} is ![](../Images/09_11_E02.png).
    In general, the Gini index of a pure set is always 0\. To measure the purity of
    the split, we average the Gini indices of the two leaves. Therefore, we have the
    following calculations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, C, C}, {A, A, B}, 和 {B, C, C} 的基尼指数都是相同的：![](../Images/09_11_E01.png)。集合
    {A, A, A} 的基尼指数为 ![](../Images/09_11_E02.png)。一般来说，纯集合的基尼指数总是 0。为了衡量分割的纯度，我们平均两个叶子的基尼指数。因此，我们得到以下计算：
- en: 'Classifier 1 (by platform):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 1（按平台划分）：
- en: Average Gini index = 1/2(0.444+0.444) = 0.444
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 平均基尼指数 = 1/2(0.444+0.444) = 0.444
- en: 'Classifier 2 (by age):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 2（按年龄划分）：
- en: Average Gini index = 1/2(0.444+0) = 0.222
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 平均基尼指数 = 1/2(0.444+0) = 0.222
- en: '![](../Images/9-12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-12.png)'
- en: Figure 9.12 The two ways to split the dataset, by platform and age, and their
    Gini index calculations. Notice that splitting the dataset by age gives us two
    smaller datasets with a lower average Gini index. Therefore, we choose to split
    the dataset by age.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 按平台和年龄划分数据集的两种分割方式及其基尼指数计算。注意，按年龄划分数据集给我们两个更小的数据集，它们的平均基尼指数更低。因此，我们选择按年龄划分数据集。
- en: We conclude that the second split is better, because it has a lower average
    Gini index.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出结论，第二次分割更好，因为它具有更低的平均基尼指数。
- en: aside The Gini impurity index should not be confused with the Gini coefficient.
    The Gini coefficient is used in statistics to calculate the income or wealth inequality
    in countries. In this book, whenever we talk about the Gini index, we are referring
    to the Gini impurity index.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白：基尼不纯度指数不应与基尼系数混淆。基尼系数在统计学中用于计算国家的收入或财富不平等。在这本书中，每当提到基尼指数时，我们指的是基尼不纯度指数。
- en: 'Entropy: Another measure of diversity with strong applications in information
    theory'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 熵：另一种衡量多样性的度量，在信息理论中有着广泛的应用
- en: In this section, we learn another measure of homogeneity in a set—its entropy—which
    is based on the physical concept of entropy and is highly important in probability
    and information theory. To understand entropy, we look at a slightly strange probability
    question. Consider the same two sets of colored balls as in the previous section,
    but think of the colors as an ordered set.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习集合中另一种同质性的度量——其熵，它基于熵的物理概念，在概率和信息理论中非常重要。为了理解熵，我们来看一个稍微奇怪的概率问题。考虑与上一节相同的两组彩色球，但将颜色视为一个有序集合。
- en: '**Set 1**: {red, red, red, red, red, red, red, red, blue, blue} (eight red
    balls, two blue balls)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合 1**：{红色，红色，红色，红色，红色，红色，红色，红色，蓝色，蓝色}（八个红色球，两个蓝色球）'
- en: '**Set 2**: {red, red, red, red, blue, blue, blue, yellow, yellow, green} (four
    red balls, three blue balls, two yellow balls, one green ball)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合 2**：{红色，红色，红色，红色，蓝色，蓝色，蓝色，黄色，黄色，绿色}（四个红色球，三个蓝色球，两个黄色球，一个绿色球）'
- en: 'Now, consider the following scenario: we have set 1 inside a bag, and we start
    picking balls out of this bag and immediately return each ball we just picked
    back to the bag. We record the colors of the balls we picked. If we do this 10
    times, imagine that we get the following sequence:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑以下场景：我们在一个袋子里有集合 1，我们开始从这个袋子中取出球，并且立即将我们刚刚取出的每个球放回袋子中。我们记录我们取出的球的颜色。如果我们这样做
    10 次，想象一下我们得到以下序列：
- en: Red, red, red, blue, red, blue, blue, red, red, red
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红色，红色，红色，蓝色，红色，蓝色，蓝色，红色，红色，红色
- en: 'Here is the main question that defines entropy:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是定义熵的主要问题：
- en: What is the probability that, by following the procedure described in the previous
    paragraph, we get the exact sequence that defines set 1, which is {red, red, red,
    red, red, red, red, red, blue, blue}?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前一段描述的程序进行操作，我们得到定义集合 1 的确切序列 {红色，红色，红色，红色，红色，红色，红色，红色，蓝色，蓝色} 的概率是多少？
- en: This probability is not very large, because we must be really lucky to get this
    sequence. Let’s calculate it. We have eight red balls and two blue balls, so the
    probability that we get a red ball is ![](../Images/09_12_Ea_frac8-10.png) and
    the probability that we get a blue ball is ![](../Images/09_12_Ea_frac2-10.png).
    Because all the draws are independent, the probability that we get the desired
    sequence is
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率并不很大，因为我们必须非常幸运才能得到这个序列。让我们来计算它。我们有八个红色球和两个蓝色球，所以得到红色球的概率是 ![](../Images/09_12_Ea_frac8-10.png)
    和得到蓝色球的概率是 ![](../Images/09_12_Ea_frac2-10.png)。因为所有的抽取都是独立的，得到所需序列的概率是
- en: '![](../Images/09_12_E01.png).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/09_12_E01.png)。'
- en: 'This is tiny, but can you imagine the corresponding probability for set 2?
    For set 2, we are picking balls out of a bag with four red balls, three blue balls,
    two yellow balls, and one green ball and hoping to obtain the following sequence:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常小，但你能想象集合 2 对应的概率吗？对于集合 2，我们是从一个包含四个红色球、三个蓝色球、两个黄色球和一个绿色球的袋子中取球，并希望获得以下序列：
- en: Red, red, red, red, blue, blue, blue, yellow, yellow, green.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红色，红色，红色，红色，蓝色，蓝色，蓝色，黄色，黄色，绿色。
- en: This is nearly impossible, because we have many colors and not many balls of
    each color. This probability, which is calculated in a similar way, is
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是不可能的，因为我们有很多颜色，每种颜色的球并不多。这个概率，以类似的方式计算，是
- en: '![](../Images/09_12_E02.png).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/09_12_E02.png).'
- en: The more diverse the set, the more unlikely we’ll be able to get the original
    sequence by picking one ball at a time. In contrast, the most pure set, in which
    all balls are of the same color, is easy to obtain this way. For example, if our
    original set has 10 red balls, each time we pick a random ball, the ball is red.
    Thus, the probability of getting the sequence {red, red, red, red, red, red, red,
    red, red, red} is 1.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 集合越多样化，我们通过一次取一个球的方式获取原始序列的可能性就越小。相比之下，最纯的集合，其中所有球的颜色都相同，这种方式很容易获得。例如，如果我们的原始集合有
    10 个红色球，每次我们随机取一个球，球就是红色的。因此，获取序列 {红色，红色，红色，红色，红色，红色，红色，红色，红色，红色} 的概率是 1。
- en: These numbers are very small for most cases—and this is with only 10 elements.
    Imagine if our dataset had one million elements. We would be dealing with tremendously
    small numbers. When we have to deal with really small numbers, using logarithms
    is the best method, because they provide a convenient way to write small numbers.
    For instance, 0.000000000000001 is equal to 10^(–15), so its logarithm in base
    10 is –15, which is a much nicer number to work with.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这些数字都非常小——而且这里只有 10 个元素。想象一下，如果我们的数据集有一百万个元素，我们将处理极其小的数字。当我们必须处理非常小的数字时，使用对数是最好的方法，因为它们提供了一种方便的方式来表示小数。例如，0.000000000000001
    等于 10^(-15)，所以它在 10 为底的对数是 -15，这是一个更容易处理的好数字。
- en: 'The entropy is defined as follows: we start with the probability that we recover
    the initial sequence by picking elements in our set, one at a time, with repetition.
    Then we take the logarithm, and divide by the total number of elements in the
    set. Because decision trees deal with binary decisions, we’ll be using logarithms
    in base 2\. The reason we took the negative of the logarithm is because logarithms
    of very small numbers are all negative, so we multiply by –1 to turn it into a
    positive number. Because we took a negative, the more diverse the set, the higher
    the entropy.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的定义如下：我们从通过一次取一个元素的方式从我们的集合中恢复初始序列的概率开始，这些元素可以重复取。然后我们取对数，并除以集合中元素的总数。因为决策树处理二元决策，我们将使用以
    2 为底的对数。我们取对数的负值是因为非常小的数字的对数都是负的，所以我们乘以 -1 来将其转换为正数。因为我们取了负值，所以集合越多样化，熵就越高。
- en: 'Now we can calculate the entropies of both sets and expand them using the following
    two identities:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算两个集合的熵，并使用以下两个恒等式来展开它们：
- en: '*log*(*ab*) = *log*(*a*) + *log*(*b*)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*log*(*ab*) = *log*(*a*) + *log*(*b*)'
- en: '*log*(*a*^c) = *c* *log*(*a*)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*log*(*a*^c) = *c* *log*(*a*)'
- en: '**Set 1**: {red, red, red, red, red, red, red, red, blue, blue} (eight red
    balls, two blue balls)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合 1**: {红色，红色，红色，红色，红色，红色，红色，红色，蓝色，蓝色}（八个红色球，两个蓝色球）'
- en: '![](../Images/09_12_E03.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_12_E03.png)'
- en: '**Set 2**: {red, red, red, red, blue, blue, blue, yellow, yellow, green}'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合 2**: {红色，红色，红色，红色，蓝色，蓝色，蓝色，黄色，黄色，绿色}'
- en: '![](../Images/09_12_E04.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_12_E04.png)'
- en: 'Notice that the entropy of set 2 is larger than the entropy of set 1, which
    implies that set 2 is more diverse than set 1\. The following is the formal definition
    of entropy:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到集合 2 的熵大于集合 1 的熵，这意味着集合 2 比集合 1 更具多样性。以下是对熵的正式定义：
- en: entropy In a set with *m* elements and *n* classes, with *a*[i] elements belonging
    to the *i*-th class, the entropy is
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个包含 *m* 个元素和 *n* 个类别的集合中，其中 *a*[i] 个元素属于第 *i*- 个类别，熵是
- en: '![](../Images/09_12_E05.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_12_E05.png)'
- en: 'We can use entropy to decide which of the two ways to split the data (platform
    or age) is better in the same way as we did with the Gini index. The rule of thumb
    is that if we can split the data into two datasets with less combined entropy,
    we have performed a better split. Thus, let’s calculate the entropy of the set
    of labels of each of the leaves. Again, looking at figure 9.12, here are the labels
    of the leaves (where we abbreviate each app by the first letter in its name):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用熵来决定在两种分割数据的方式（平台或年龄）中哪一种更好，就像我们使用 Gini 指数时做的那样。经验法则是，如果我们能够将数据分割成两个具有更少组合熵的数据集，我们就已经进行了更好的分割。因此，让我们计算每个叶子标签集合的熵。再次查看图
    9.12，以下是叶子的标签（我们用每个应用程序名称的第一个字母来缩写每个应用程序）：
- en: 'Classifier 1 (by platform):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 1（按平台）：
- en: 'Left leaf: {A, C, C}'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 左叶子：{A, C, C}
- en: 'Right leaf: {A, A, B}'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 右叶子：{A, A, B}
- en: 'Classifier 2 (by age):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 2（按年龄）：
- en: 'Left leaf: {A, A, A}'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 左叶子：{A, A, A}
- en: 'Right leaf: {B, C, C}'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 右叶子：{B, C, C}
- en: 'The entropies of the sets {A, C, C}, {A, A, B}, and {B, C, C} are all the same:
    ![](../Images/09_12_E06.png). The entropy of the set {A, A, A} is ![](../Images/09_12_E07.png).
    In general, the entropy of a set in which all elements are the same is always
    0\. To measure the purity of the split, we average the entropy of the sets of
    labels of the two leaves, as follows (illustrated in figure 9.13):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, C, C}, {A, A, B}, 和 {B, C, C} 的熵都相同：![](../Images/09_12_E06.png)。集合 {A,
    A, A} 的熵为 ![](../Images/09_12_E07.png)。一般来说，所有元素都相同的集合的熵总是 0。为了测量分割的纯度，我们平均两个叶子标签集合的熵，如下所示（如图
    9.13 所示）：
- en: 'Classifier 1 (by platform):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 1（按平台）：
- en: Average entropy = 1/2(0.918 + 0.918) = 0.918
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 平均熵 = 1/2(0.918 + 0.918) = 0.918
- en: 'Classifier 2 (by age):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 2（按年龄）：
- en: Average entropy = 1/2(0.918+0) = 0.459
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 平均熵 = 1/2(0.918+0) = 0.459
- en: '![](../Images/9-131.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-131.png)'
- en: Figure 9.13 The two ways to split the dataset, by platform and age, and their
    entropy calculations. Notice that splitting the dataset by age gives us two smaller
    datasets with a lower average entropy. Therefore, we again choose to split the
    dataset by age.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 通过平台和年龄分割数据集的两种方式，以及它们的熵计算。注意，通过年龄分割数据集给我们带来了两个平均熵更低的小数据集。因此，我们再次选择通过年龄分割数据集。
- en: Thus, again we conclude that the second split is better, because it has a lower
    average entropy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们再次得出结论，第二个分割更好，因为它具有更低的平均熵。
- en: Entropy is a tremendously important concept in probability and statistics, because
    it has strong connections with information theory, mostly thanks to the work of
    Claude Shannon. In fact, an important concept called *information gain* is precisely
    the change in entropy. To learn more on the topic, please see appendix C for a
    video and a blog post which covers this topic in much more detail.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是概率论和统计学中的一个极其重要的概念，因为它与信息论有很强的联系，这主要归功于克劳德·香农的工作。事实上，一个称为 *信息增益* 的重要概念正是熵的变化。要了解更多关于这个主题的信息，请参阅附录
    C 中的视频和博客文章，它们更详细地介绍了这个主题。
- en: 'Classes of different sizes? No problem: We can take weighted averages'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 不同大小的类别？没问题：我们可以取加权平均
- en: In the previous sections we learned how to perform the best possible split by
    minimizing average Gini impurity index or entropy. However, imagine that you have
    a dataset with eight data points (which when training the decision tree, we also
    refer to as samples), and you split it into two datasets of sizes six and two.
    As you may imagine, the larger dataset should count for more in the calculations
    of Gini impurity index or entropy. Therefore, instead of considering the average,
    we consider the weighted average, where at each leaf, we assign the proportion
    of points corresponding to that leaf. Thus, in this case, we would weigh the first
    Gini impurity index (or entropy) by 6/8, and the second one by 2/8\. Figure 9.14
    shows an example of a weighted average Gini impurity index and a weighted average
    entropy for a sample split.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何通过最小化平均 Gini 不纯度指数或熵来实现最佳分割。然而，想象一下，你有一个包含八个数据点的数据集（在训练决策树时，我们也将这些数据点称为样本），并将其分割成两个大小分别为六和二的数据集。正如你可能想象的那样，较大的数据集在
    Gini 不纯度指数或熵的计算中应该占更大的比重。因此，我们考虑加权平均，在每一个叶子节点，我们分配对应叶子的点数比例。因此，在这种情况下，我们会将第一个
    Gini 不纯度指数（或熵）的权重设为 6/8，第二个的权重为 2/8。图 9.14 展示了一个加权平均 Gini 不纯度指数和加权平均熵的示例。
- en: '![](../Images/9-141.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-141.png)'
- en: Figure 9.14 A split of a dataset of size eight into two datasets of sizes six
    and two. To calculate the average Gini index and the average entropy, we weight
    the index of the left dataset by 6/8 and that of the right dataset by 2/8\. This
    results in a weighted Gini index of 0.333 and a weighted entropy of 0.689.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 将大小为八的数据集分割成两个大小分别为六和二的数据集。为了计算平均Gini指数和平均熵，我们将左侧数据集的指数按6/8加权，右侧数据集的指数按2/8加权。这导致加权Gini指数为0.333，加权熵为0.689。
- en: Now that we’ve learned three ways (accuracy, Gini index, and entropy) to pick
    the best split, all we need to do is iterate this process many times to build
    the decision tree! This is detailed in the next section.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了三种方法（准确率、Gini指数和熵）来选择最佳分割，我们所需做的就是多次迭代此过程以构建决策树！这将在下一节中详细介绍。
- en: 'Second step to build the model: Iterating'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 建模的第二步：迭代
- en: In the previous section, we learned how to split the data in the best possible
    way using one of the features. That is the bulk of the training process of a decision
    tree. All that is left to finish building our decision tree is to iterate on this
    step many times. In this section we learn how to do this.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用一个特征以最佳方式分割数据。这是决策树训练过程中的主要内容。要完成构建我们的决策树，我们只剩下多次迭代这一步骤。在本节中，我们将学习如何做到这一点。
- en: Using the three methods, accuracy, Gini index, and entropy, we decided that
    the best split was made using the “age” feature. Once we make this split, our
    dataset is divided into two datasets. The split into these two datasets, with
    their accuracy, Gini index, and entropy, is illustrated in figure 9.15.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三种方法，准确率、Gini指数和熵，我们决定使用“年龄”特征进行最佳分割。一旦我们进行这个分割，我们的数据集就被分割成两个数据集。这两个数据集的分割，包括它们的准确率、Gini指数和熵，如图9.15所示。
- en: '![](../Images/9-151.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-151.png)'
- en: Figure 9.15 When we split our dataset by age, we get two datasets. The one on
    the left has three users who downloaded Atom Count, and the one on the right has
    one user who downloaded Beehive Count and two who downloaded Check Mate Mate.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 当我们按年龄分割我们的数据集时，我们得到两个数据集。左侧的数据集有三个用户下载了原子计数，右侧的数据集有一个用户下载了蜂巢计数，两个用户下载了检查者计数。
- en: Notice that the dataset on the left is pure—all the labels are the same, its
    accuracy is 100%, and its Gini index and entropy are both 0\. There’s nothing
    more we can do to split this dataset or to improve the classifications. Thus,
    this node becomes a leaf node, and when we get to that leaf, we return the prediction
    “Atom Count.”
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意左侧的数据集是纯的——所有标签都相同，其准确率为100%，其Gini指数和熵都为0。我们无法再对此数据集进行分割或改进分类。因此，此节点成为叶子节点，当我们到达该叶子时，我们返回预测“原子计数”。
- en: 'The dataset on the right can still be divided, because it has two labels: “Beehive
    Count” and “Check Mate Mate.” We’ve used the age feature already, so let’s try
    using the platform feature. It turns out that we’re in luck, because the Android
    user downloaded Beehive Count, and the two iPhone users downloaded Check Mate
    Mate. Therefore, we can split this leaf using the platform feature and obtain
    the decision node shown in figure 9.16.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的数据集仍然可以分割，因为它有两个标签：“蜂巢计数”和“检查者计数”。我们已经使用了年龄特征，所以让我们尝试使用平台特征。结果我们发现我们很幸运，因为安卓用户下载了蜂巢计数，而两个iPhone用户下载了检查者计数。因此，我们可以使用平台特征来分割这个叶子，并获得图9.16中所示的决策节点。
- en: '![](../Images/9-161.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-161.png)'
- en: Figure 9.16 We can split the right leaf of the tree in figure 9.15 using platform
    and obtain two pure datasets. Each one of them has an accuracy of 100% and a Gini
    index and entropy of 0.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 我们可以使用平台来分割图9.15中的右叶子，从而获得两个纯数据集。每个数据集的准确率都是100%，Gini指数和熵都是0。
- en: After this split, we are done, because we can’t improve our splits any further.
    Thus, we obtain the tree in figure 9.17.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分割后，我们就完成了，因为我们无法进一步改进我们的分割。因此，我们获得了图9.17中的树。
- en: '![](../Images/9-17.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-17.png)'
- en: Figure 9.17 The resulting decision tree has two nodes and three leaves. This
    tree predicts every point in the original dataset correctly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 结果决策树有两个节点和三个叶子。此树正确预测了原始数据集中的每个点。
- en: This is the end of our process, and we have built a decision tree that classifies
    our entire dataset. We almost have all the pseudocode for the algorithm, except
    for some final details which we see in the next section.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们过程的结束，我们已经构建了一个能够对整个数据集进行分类的决策树。我们几乎有了算法的所有伪代码，除了下一节中我们将看到的某些最终细节。
- en: 'Last step: When to stop building the tree and other hyperparameters'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步：何时停止构建树以及其他超参数
- en: 'In the previous section, we built a decision tree by recursively splitting
    our dataset. Each split was performed by picking the best feature to split. This
    feature was found using any of the following metrics: accuracy, Gini index, or
    entropy. We finish when the portion of the dataset corresponding to each of the
    leaf nodes is pure—in other words, when all the samples on it have the same label.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过递归分割我们的数据集来构建决策树。每次分割都是通过选择最佳特征来进行的。这个特征是通过以下任何一种指标找到的：准确率、基尼指数或熵。我们完成分割，当数据集对应于每个叶节点的部分是纯的——换句话说，当它上面的所有样本都有相同的标签时。
- en: 'Many problems can arise in this process. For instance, if we continue splitting
    our data for too long, we may end up with an extreme situation in which every
    leaf contains very few samples, which can lead to serious overfitting. The way
    to prevent this is to introduce a stopping condition. This condition can be any
    of the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中可能会出现许多问题。例如，如果我们继续分割我们的数据时间过长，我们可能会陷入一个极端情况，即每个叶节点包含非常少的样本，这可能导致严重的过拟合。防止这种情况的方法是引入一个停止条件。这个条件可以是以下任何一种：
- en: Don’t split a node if the change in accuracy, Gini index, or entropy is below
    some threshold.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果准确率、基尼指数或熵的变化低于某个阈值，则不要分割节点。
- en: Don’t split a node if it has less than a certain number of samples.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个节点少于一定数量的样本，则不要分割该节点。
- en: Split a node only if both of the resulting leaves contain at least a certain
    number of samples.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有当两个结果叶节点都至少包含一定数量的样本时，才分割一个节点。
- en: Stop building the tree after you reach a certain depth.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在达到一定深度后停止构建树。
- en: 'All of these stopping conditions require a hyperparameter. More specifically,
    these are the hyperparameters corresponding to the previous four conditions:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些停止条件都需要一个超参数。更具体地说，这些是前四个条件的超参数：
- en: The minimum amount of change in accuracy (or Gini index, or entropy)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确率（或基尼指数、熵）的最小变化量
- en: The minimum number of samples that a node must have to split it
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个节点必须具有的最小样本数才能进行分割
- en: The minimum number of samples allowed in a leaf node
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 叶节点中允许的最小样本数
- en: The maximum depth of the tree
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树的最大深度
- en: 'The way we pick these hyperparameters is either by experience or by running
    an exhaustive search where we look for different combinations of hyperparameters
    and choose the one that performs best in our validation set. This process is called
    *grid search*, and we’ll study it in more detail in the section "Tuning the hyperparameters
    to find the best model: Grid search" in chapter 13.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这些超参数的方式要么是经验，要么是通过运行穷举搜索，在搜索中我们寻找不同超参数的组合，并选择在验证集上表现最好的一个。这个过程被称为*网格搜索*，我们将在第13章的“调整超参数以找到最佳模型：网格搜索”部分更详细地研究它。
- en: 'The decision tree algorithm: How to build a decision tree and make predictions
    with it'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法：如何构建决策树并使用它进行预测
- en: Now we are finally ready to state the pseudocode for the decision tree algorithm,
    which allows us to train a decision tree to fit a dataset.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于准备好陈述决策树算法的伪代码，这允许我们训练一个决策树来拟合数据集。
- en: Pseudocode for the decision tree algorithm
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的伪代码
- en: 'Inputs:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A training dataset of samples with their associated labels
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含其相关标签的样本训练数据集
- en: A metric to split the data (accuracy, Gini index, or entropy)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分割数据的指标（准确率、基尼指数或熵）
- en: One (or more) stopping condition
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个（或多个）停止条件
- en: 'Output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A decision tree that fits the dataset
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个适合数据集的决策树
- en: 'Procedure:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Add a root node, and associate it with the entire dataset. This node has level
    0\. Call it a leaf node.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个根节点，并将其与整个数据集关联。这个节点具有级别0。称它为叶节点。
- en: 'Repeat until the stopping conditions are met at every leaf node:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复直到每个叶节点都满足停止条件：
- en: Pick one of the leaf nodes at the highest level.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最高层的一个叶节点。
- en: Go through all the features, and select the one that splits the samples corresponding
    to that node in an optimal way, according to the selected metric. Associate that
    feature to the node.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历所有特征，并选择一个根据所选指标以最佳方式分割该节点对应样本的特征。将该特征与节点关联。
- en: This feature splits the dataset into two branches. Create two new leaf nodes,
    one for each branch, and associate the corresponding samples to each of the nodes.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个特征将数据集分割成两个分支。为每个分支创建两个新的叶节点，并将相应的样本与每个节点关联。
- en: If the stopping conditions allow a split, turn the node into a decision node,
    and add two new leaf nodes underneath it. If the level of the node is *i*, the
    two new leaf nodes are at level *i* + 1.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果停止条件允许分裂，则将节点转换为决策节点，并在其下方添加两个新的叶子节点。如果节点的级别是*i*，则两个新的叶子节点位于*i* + 1级别。
- en: If the stopping conditions don’t allow a split, the node becomes a leaf node.
    To this leaf node, associate the most common label among its samples. That label
    is the prediction at the leaf.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果停止条件不允许分裂，则该节点变为叶子节点。为此叶子节点，关联其样本中最常见的标签。该标签就是叶子节点的预测。
- en: 'Return:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: The decision tree obtained.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得了决策树。
- en: 'To make predictions using this tree, we simply traverse down it, using the
    following rules:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此树进行预测，我们只需按照以下规则遍历它：
- en: Traverse the tree downward. At every node, continue in the direction that is
    indicated by the feature.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向下遍历树。在每个节点，继续沿着由特征指示的方向前进。
- en: When arriving at a leaf, the prediction is the label associated with the leaf
    (the most common among the samples associated with that leaf in the training process).
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当到达叶子节点时，预测就是与该叶子节点关联的标签（在训练过程中与该叶子节点关联的样本中最常见的标签）。
- en: 'This is how we make predictions using the app-recommendation decision tree
    we built previously. When a new user comes, we check their age and their platform,
    and take the following actions:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用之前构建的应用推荐决策树进行预测的方式。当新用户到来时，我们检查他们的年龄和平台，并采取以下行动：
- en: If the user is young, then we recommend them Atom Count.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户是年轻人，那么我们推荐他们使用Atom Count。
- en: If the user is an adult, then we check their platform.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户是成年人，那么我们检查他们的平台。
- en: If the platform is Android, then we recommend Beehive Count.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果平台是Android，那么我们推荐使用Beehive Count。
- en: If the platform is iPhone, then we recommend Check Mate Mate.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果平台是iPhone，那么我们推荐Check Mate Mate。
- en: aside The literature contains terms like Gini gain and information gain when
    training decision trees. The Gini gain is the difference between the weighted
    Gini impurity index of the leaves and the Gini impurity index (entropy) of the
    decision node we are splitting. In a similar way, the information gain is the
    difference between the weighted entropy of the leaves and the entropy of the root.
    The more common way to train decision trees is by maximizing the Gini gain or
    the information gain. However, in this chapter, we train decision trees by, instead,
    minimizing the weighted Gini index or the weighted entropy. The training process
    is exactly the same, because the Gini impurity index (entropy) of the decision
    node is constant throughout the process of splitting that particular decision
    node.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在训练决策树时，文献中包含诸如Gini增益和信息增益等术语。Gini增益是指叶子节点的加权Gini不纯度指数与我们要分裂的决策节点的Gini不纯度指数（熵）之间的差异。类似地，信息增益是指叶子节点的加权熵与根节点的熵之间的差异。训练决策树最常见的做法是通过最大化Gini增益或信息增益。然而，在本章中，我们通过最小化加权Gini指数或加权熵来训练决策树。训练过程完全相同，因为决策节点的Gini不纯度指数（熵）在整个分裂该决策节点的过程中是恒定的。
- en: Beyond questions like yes/no
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越了是/否这类问题
- en: 'In the section “The solution: Building an app-recommendation system,” we learned
    how to build a decision tree for a very specific case in which every feature was
    categorical and binary (meaning that it has only two classes, such as the platform
    of the user). However, almost the same algorithm works to build a decision tree
    with categorical features with more classes (such as dog/cat/bird) and even with
    numerical features (such as age or average income). The main step to modify is
    the step in which we split the dataset, and in this section, we show you how.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在“解决方案：构建应用推荐系统”这一节中，我们学习了如何构建一个决策树，该决策树适用于一个非常具体的案例，其中每个特征都是分类的二进制特征（意味着它只有两个类别，例如用户的平台）。然而，几乎相同的算法可以用来构建具有更多类别的分类特征决策树（例如狗/猫/鸟）以及具有数值特征（例如年龄或平均收入）的决策树。需要修改的主要步骤是我们分裂数据集的步骤，在本节中，我们将向您展示如何进行。
- en: Splitting the data using non-binary categorical features, such as dog/cat/bird
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非二进制分类特征（如狗/猫/鸟）分割数据。
- en: 'Recall that when we want to split a dataset based on a binary feature, we simply
    ask one yes-or-no question of the form, “Is the feature X?” For example, when
    the feature is the platform, a question to ask is “Is the user an iPhone user?”
    If we have a feature with more than two classes, we just ask several questions.
    For example, if the input is an animal that could be a dog, a cat, or a bird,
    then we ask the following questions:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们想要根据二进制特征分割数据集时，我们只需提出一个形式为“特征X吗？”的简单是或否问题。例如，当特征是平台时，一个问题可以是“用户是iPhone用户吗？”如果我们有一个具有超过两个类别的特征，我们只需提出几个问题。例如，如果输入是一个可能是狗、猫或鸟的动物，那么我们提出以下问题：
- en: Is the animal a dog?
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这动物是狗吗？
- en: Is the animal a cat?
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这动物是猫吗？
- en: Is the animal a bird?
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这动物是鸟吗？
- en: No matter how many classes a feature has, we can split it into several binary
    questions (figure 9.18).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 无论特征有多少个类别，我们都可以将其分割成几个二进制问题（图9.18）。
- en: '![](../Images/9-18.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-18.png)'
- en: Figure 9.18 When we have a nonbinary feature, for example, one with three or
    more possible categories, we instead turn it into several binary (yes-or-no) features,
    one for each category. For example, if the feature is a dog, the answers to the
    three questions “Is it a dog?,” “Is it a cat?,” and “Is it a bird?” are “yes,”
    “no,” and “no.”
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 当我们有一个非二进制特征时，例如有三个或更多可能类别的特征，我们将其转换为几个二进制（是或否）特征，每个类别一个。例如，如果特征是狗，对三个问题“它是狗吗？”、“它是猫吗？”和“它是鸟吗？”的回答分别是“是”、“否”和“否”。
- en: 'Each of the questions splits the data in a different way. To figure out which
    of the three questions gives us the best split, we use the same methods as in
    the section “First step to build the model”: accuracy, Gini index, or entropy.
    This process of turning a nonbinary categorical feature into several binary features
    is called *one-hot encoding*. In the section “Turning categorical data into numerical
    data” in chapter 13, we see it used in a real dataset.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题都以不同的方式分割数据。为了确定三个问题中哪一个给我们提供了最佳的分割，我们使用与“建立模型的第一步”部分中相同的方法：准确率、基尼指数或熵。将非二进制分类特征转换为几个二进制特征的过程称为*独热编码*。在第13章的“将分类数据转换为数值数据”部分中，我们看到它在真实数据集中被使用。
- en: Splitting the data using continuous features, such as age
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用连续特征（如年龄）分割数据
- en: Recall that before we simplified our dataset, the “age” feature contained numbers.
    Let’s get back to our original table and build a decision tree there (table 9.3).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在我们简化数据集之前，“年龄”特征包含数字。让我们回到原始表格，并在那里构建一个决策树（表9.3）。
- en: Table 9.3 Our original app recommendation dataset with the platform and (numerical)
    age of the users. This is the same as table 9.1.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.3 我们的原始应用推荐数据集，包含平台和用户的（数值）年龄。这与表9.1相同。
- en: '| Platform | Age | App |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | 年龄 | 应用 |'
- en: '| iPhone | 15 | Atom Count |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 15 | 原子计数 |'
- en: '| iPhone | 25 | Check Mate Mate |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 25 | 检查者 |'
- en: '| Android | 32 | Beehive Finder |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 安卓 | 32 | 蜂巢寻找者 |'
- en: '| iPhone | 35 | Check Mate Mate |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| iPhone | 35 | 检查者 |'
- en: '| Android | 12 | Atom Count |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 安卓 | 12 | 原子计数 |'
- en: '| Android | 14 | Atom Count |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 安卓 | 14 | 原子计数 |'
- en: The idea is to turn the Age column into several questions of the form, “Is the
    user younger than X?” or “Is the user older than X?” It seems like we have infinitely
    many questions to ask, because there are infinitely many numbers, but notice that
    many of these questions split the data in the same way. For example, asking, “Is
    the user younger than 20?” and “Is the user younger than 21,” gives us the same
    split. In fact, only seven splits are possible, as illustrated in figure 9.19.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是将年龄列转换为几个形式为“用户是否小于X？”或“用户是否大于X？”的问题。看起来我们似乎有无限多的问题要问，因为数字是无限的，但请注意，许多这些问题以相同的方式分割数据。例如，询问“用户是否小于20？”和“用户是否小于21？”会给我们相同的分割。实际上，只有七种可能的分割方式，如图9.19所示。
- en: '![](../Images/9-19.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-19.png)'
- en: Figure 9.19 A graphic of the seven possible ways to split the users by age.
    Note that it doesn’t matter where we put the cutoffs, as long as they lie between
    consecutive ages (except for the first and last cutoff).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 按年龄分割用户的七种可能方式的图形。注意，只要截止点位于连续年龄之间（除了第一个和最后一个截止点），放置截止点的位置无关紧要。
- en: As a convention, we’ll pick the midpoints between consecutive ages to be the
    age for splitting. For the endpoints, we can pick any random value that is out
    of the interval. Thus, we have seven possible questions that split the data into
    two sets, as shown in table 9.4\. In this table, we have also calculated the accuracy,
    the Gini impurity index, and the entropy of each of the splits.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，我们将选择连续年龄之间的中点作为分割的年龄。对于端点，我们可以选择任何随机值，只要它不在区间内。因此，我们有七个可能的问题将数据分割成两个集合，如表9.4所示。在此表中，我们还计算了每个分割的准确率、基尼不纯度指数和熵。
- en: Notice that the fourth question (“Is the user younger than 20?”) gives the highest
    accuracy, the lowest weighted Gini index, and the lowest weighted entropy and,
    therefore, is the best split that can be made using the “age” feature.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第四个问题（“用户是否小于20岁？”）给出了最高的准确率，最低的加权基尼指数，以及最低的加权熵，因此这是使用“年龄”特征可以做出的最佳分割。
- en: Table 9.4 The seven possible questions we can pick, each with the corresponding
    splitting. In the first set, we put the users who are younger than the cutoff,
    and in the second set, those who are older than the cutoff.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.4：我们可以选择的七个可能的问题，每个问题都有相应的分割。在第一个集合中，我们放入小于截止年龄的用户，在第二个集合中，放入大于截止年龄的用户。
- en: '| Question | First set (yes) | Second set (no) | Labels | Weighted accuracy
    | Weighted Gini impurity index | Weighted entropy |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 第一组（是） | 第二组（否） | 标签 | 加权准确率 | 加权基尼不纯度指数 | 加权熵 |'
- en: '| Is the user younger than 7? | empty | 12, 14, 15, 25, 32, 35 | {},{A,A,A,C,B,C}
    | 3/6 | 0.611 | 1.459 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于7岁？ | empty | 12, 14, 15, 25, 32, 35 | {},{A,A,A,C,B,C} | 3/6 | 0.611
    | 1.459 |'
- en: '| Is the user younger than 13? | 12 | 14, 15, 25, 32, 35 | {A},{A,A,C,B,C}
    | 3/6 | 0.533 | 1.268 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于13岁？ | 12 | 14, 15, 25, 32, 35 | {A},{A,A,C,B,C} | 3/6 | 0.533 | 1.268
    |'
- en: '| Is the user younger than 14.5? | 12, 14 | 15, 25, 32, 35 | {A,A}{A,C,B,C}
    | 4/6 | 0.417 | 1.0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于14.5岁？ | 12, 14 | 15, 25, 32, 35 | {A,A}{A,C,B,C} | 4/6 | 0.417 | 1.0
    |'
- en: '| Is the user younger than 20? | 12, 14, 15 | 25, 32, 35 | {A,A,A},{C,B,C}
    | 5/6 | 0.222 | 0.459 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于20岁？ | 12, 14, 15 | 25, 32, 35 | {A,A,A},{C,B,C} | 5/6 | 0.222 | 0.459
    |'
- en: '| Is the user younger than 28.5? | 12, 14, 15, 25 | 32, 35 | {A,A,A,C},{B,C}
    | 4/6 | 0.416 | 0.874 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于28.5岁？ | 12, 14, 15, 25 | 32, 35 | {A,A,A,C},{B,C} | 4/6 | 0.416 |
    0.874 |'
- en: '| Is the user younger than 33.5? | 12, 14, 15, 25, 32 | 35 | {A,A,A,C,B},{C}
    | 4/6 | 0.467 | 1.145 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于33.5岁？ | 12, 14, 15, 25, 32 | 35 | {A,A,A,C,B},{C} | 4/6 | 0.467 |
    1.145 |'
- en: '| Is the user younger than 100? | 12, 14, 15, 25, 32, 35 | empty | {A,A,A,C,B,C},{}
    | 3/6 | 0.611 | 1/459 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 用户是否小于100岁？ | 12, 14, 15, 25, 32, 35 | empty | {A,A,A,C,B,C},{} | 3/6 | 0.611
    | 1/459 |'
- en: 'Carry out the calculations in the table, and verify that you get the same answers.
    The entire calculation of these Gini indices is in the following notebook: [https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Gini_entropy_calculations.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Gini_entropy_calculations.ipynb).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在表中进行计算，并验证你是否得到相同的答案。这些基尼指数的整个计算过程在以下笔记本中：[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Gini_entropy_calculations.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Gini_entropy_calculations.ipynb)。
- en: 'For clarity, let’s carry out the calculations of accuracy, weighted Gini impurity
    index, and weighted entropy for the third question. Notice that this question
    splits the data into the following two sets:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，让我们对第三个问题的准确率、加权基尼不纯度指数和加权熵进行计算。注意，这个问题将数据分割成以下两个集合：
- en: '**Set 1** (younger than 14.5)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合1**（小于14.5岁）'
- en: 'Ages: 12, 14'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄：12, 14
- en: 'Labels: {A, A}'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：{A, A}
- en: '**Set 2** (14.5 and older):'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合2**（14.5岁及以上）：'
- en: 'Ages: 15, 25, 32, 25'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄：15, 25, 32, 25
- en: 'Labels: {A, C, B, C}'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：{A, C, B, C}
- en: Accuracy calculation
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率计算
- en: The most common label in set 1 is “A” and in set 2 is “C,” so these are the
    predictions we’ll make for each of the corresponding leaves. In set 1, every element
    is predicted correctly, and in set 2, only two elements are predicted correctly.
    Therefore, this decision stump is correct in four out of the six data points,
    for an accuracy of 4/6 = 0.667.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 集合1中最常见的标签是“A”，集合2中是“C”，因此我们将为每个相应的叶子节点做出这些预测。在集合1中，每个元素都被正确预测，而在集合2中，只有两个元素被正确预测。因此，这个决策树桩在六个数据点中的四个是正确的，准确率为4/6
    = 0.667。
- en: 'For the next two calculations, notice the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个计算中，请注意以下内容：
- en: Set 1 is pure (all its labels are the same), so its Gini impurity index and
    entropy are both 0.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合1是纯的（其所有标签都相同），因此其基尼不纯度指数和熵都是0。
- en: In set 2, the proportions of elements with labels “A,” “B,” and “C” are 1/4,
    1/4, and 2/4 =1/2, respectively.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集合2中，标签为“A”、“B”和“C”的元素比例分别为 1/4、1/4和 2/4 = 1/2。
- en: Weighted Gini impurity index calculation
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 加权Gini不纯度指数计算
- en: The Gini impurity index of the set {A, A} is 0.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, A} 的Gini不纯度指数为 0。
- en: The Gini impurity index of the set {A, C, B, C} is ![](../Images/09_19_E01.png)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, C, B, C} 的Gini不纯度指数为 ![](../Images/09_19_E01.png)
- en: The weighted average of the two Gini impurity indices is ![](../Images/09_19_E02.png)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 两个Gini不纯度指数的加权平均为 ![](../Images/09_19_E02.png)
- en: Accuracy calculation
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度计算
- en: The entropy of the set {A, A} is 0.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, A} 的熵为 0。
- en: The entropy of the set {A, C, B, C} is ![](../Images/09_19_E03.png)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 {A, C, B, C} 的熵为 ![](../Images/09_19_E03.png)
- en: The weighted average of the two entropies is ![](../Images/09_19_E04.png)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 两个熵的加权平均为 ![](../Images/09_19_E04.png)
- en: A numerical feature becomes a series of yes-or-no questions, which can be measured
    and compared with the other yes-or-no questions coming from other features, to
    pick the best one for that decision node.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数值特征变成一系列是或否的问题，这些可以测量并与来自其他特征的另一些是或否的问题进行比较，以选择那个决策节点最佳的问题。
- en: 'aside This app-recommendation model is very small, so we could do it all by
    hand. However, to see it in code, please check this notebook: [https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/App_recommendations.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/App_recommendations.ipynb).
    The notebook uses the Scikit-Learn package, which we introduce in more detail
    in the section “Using Scikit-Learn to build a decision tree.”'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 旁注：这个应用推荐模型非常小，所以我们完全可以手动完成。但是，要查看代码，请查看这个笔记本：[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/App_recommendations.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/App_recommendations.ipynb)。该笔记本使用Scikit-Learn包，我们将在“使用Scikit-Learn构建决策树”一节中详细介绍。
- en: The graphical boundary of decision trees
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的图形边界
- en: 'In this section, I show you two things: how to build a decision tree geometrically
    (in two dimensions) and how to code a decision tree in the popular machine learning
    package Scikit-Learn.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示两件事：如何从几何角度（在二维空间中）构建决策树，以及如何在流行的机器学习包Scikit-Learn中编码决策树。
- en: Recall that in classification models, such as the perceptron (chapter 5) or
    the logistic classifier (chapter 6), we plotted the boundary of the model that
    separated the points with labels 0 and 1, and it turned out to be a straight line.
    The boundary of a decision tree is also nice, and when the data is two-dimensional,
    it is formed by a combination of vertical and horizontal lines. In this section,
    we illustrate this with an example. Consider the dataset in figure 9.20, where
    the points with label 1 are triangles, and the points with label 0 are squares.
    The horizontal and vertical axes are called *x*[0] and *x*[1], respectively.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在分类模型中，例如感知器（第5章）或逻辑分类器（第6章），我们绘制了将标签为0和1的点分开的模型边界，结果是一条直线。决策树的边界也很理想，当数据是二维的时，它是由垂直线和水平线的组合形成的。在本节中，我们通过一个例子来说明这一点。考虑图9.20中的数据集，其中标签为1的点为三角形，标签为0的点为正方形。水平和垂直轴分别称为
    *x*[0] 和 *x*[1]。
- en: '![](../Images/9-201.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-201.png)'
- en: Figure 9.20 A dataset with two features (*x*[0] and *x*[1]) and two labels (triangle
    and square) in which we will train a decision tree
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 一个包含两个特征 (*x*[0] 和 *x*[1]) 和两个标签（三角形和正方形）的数据集，我们将在这个数据集上训练决策树
- en: If you had to split this dataset using only one horizontal or vertical line,
    what line would you pick? There could be different lines, according to the criteria
    you would use to measure the effectiveness of a solution. Let’s go ahead and select
    a vertical line at *x*[0] = 5\. This leaves mostly triangles to the right of it
    and mostly squares to the left of it, with the exception of two misclassified
    points, one square and one triangle (figure 9.21). Try checking all the other
    possible vertical and horizontal lines, compare them using your favorite metric
    (accuracy, Gini index, and entropy), and verify that this is the line that best
    divides the points.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您必须仅使用一条水平线或垂直线来分割这个数据集，您会选择哪条线？根据您用来衡量解决方案有效性的标准，可能会有不同的线。让我们先选择一条垂直线 *x*[0]
    = 5。这将使它右侧主要是三角形，左侧主要是正方形，除了两个错误分类的点，一个是正方形，一个是三角形（图9.21）。尝试检查所有其他可能的垂直线和水平线，使用您最喜欢的指标（准确度、Gini指数和熵）进行比较，并验证这是最佳分割点的线。
- en: Now let’s look at each half separately. This time, it’s easy to see that two
    horizontal lines at *x*[1] = 8 and *x*[1] = 2.5 will do the job on the left and
    the right side, respectively. These lines completely divide the dataset into squares
    and triangles. Figure 9.22 illustrates the result.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们分别看看每一半。这次，很容易看出在 *x*[1] = 8 和 *x*[1] = 2.5 的两条水平线将分别在左右两侧完成任务。这些线完全将数据集划分为正方形和三角形。图
    9.22 展示了结果。
- en: '![](../Images/9-212.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-212.png)'
- en: Figure 9.21 If we have to use only one vertical or horizontal line to classify
    this dataset in the best possible way, which one would we use? Based on accuracy,
    the best classifier is the vertical line at *x*[0] = 5, where we classify everything
    to the right of it as a triangle, and everything to the left of it as a square.
    This simple classifier classifies 8 out of the 10 points correctly, for an accuracy
    of 0.8.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 如果我们只能使用一条垂直线或水平线以最佳方式对数据进行分类，我们会使用哪一条？基于准确率，最佳分类器是 *x*[0] = 5 的垂直线，我们将它右侧的所有点分类为三角形，左侧的所有点分类为正方形。这个简单的分类器正确分类了
    8 个点中的 10 个，准确率为 0.8。
- en: '![](../Images/9-221.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-221.png)'
- en: Figure 9.22 The classifier in figure 9.21 leaves us with two datasets, one at
    each side of the vertical line. If we had to classify each one of them, again
    using one vertical or horizontal line, which one would we choose? The best choices
    are horizontal lines at *x*[1] = 8 and *x*[1] = 2.5, as the figure shows.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 图 9.21 中的分类器将我们留在了两个数据集，每个数据集位于垂直线的两侧。如果我们必须使用一条垂直线或水平线来对每个数据集进行分类，我们会选择哪一条？如图所示，最佳选择是在
    *x*[1] = 8 和 *x*[1] = 2.5 的水平线。
- en: What we did here was build a decision tree. At every stage, we picked from each
    of the two features (*x*[0] and *x*[1]) and selected the threshold that best splits
    our data. In fact, in the next subsection, we use Scikit-Learn to build the same
    decision tree on this dataset.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是构建一个决策树。在每一个阶段，我们从两个特征（*x*[0] 和 *x*[1]）中选取，并选择最佳分割数据的阈值。实际上，在下一小节中，我们使用
    Scikit-Learn 在这个数据集上构建相同的决策树。
- en: Using Scikit-Learn to build a decision tree
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn 构建决策树
- en: 'In this section, we learn how to use a popular machine learning package called
    Scikit-Learn (abbreviated sklearn) to build a decision tree. The code for this
    section follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何使用一个流行的机器学习包 Scikit-Learn（简称 sklearn）来构建决策树。本节的代码如下：
- en: '**Notebook**: Graphical_example.ipynb'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**：Graphical_example.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Graphical_example.ipynb)'
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Graphical_example.ipynb)'
- en: 'We begin by loading the dataset as a Pandas DataFrame called `dataset` (introduced
    in chapter 8), with the following lines of code:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据集作为名为 `dataset` 的 Pandas DataFrame 加载，如以下代码行所示：
- en: '[PRE0]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we separate the features from the labels as shown here:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将特征与标签分开，如下所示：
- en: '[PRE1]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To build the decision tree, we create a `DecisionTreeClassifier` object and
    use the `fit` function, as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建决策树，我们创建一个 `DecisionTreeClassifier` 对象，并使用 `fit` 函数，如下所示：
- en: '[PRE2]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We obtained the plot of the tree, shown in figure 9.23, using the `display_tree`
    function in the utils.py file.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 utils.py 文件中的 `display_tree` 函数获得了树的图，如图 9.23 所示。
- en: '![](../Images/9-23.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-23.png)'
- en: Figure 9.23 The resulting decision tree of depth 2 that corresponds to the boundary
    in figure 9.22\. It has three nodes and four leaves.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 深度为 2 的决策树，对应图 9.22 中的边界。它有三个节点和四个叶子。
- en: 'Notice that the tree in figure 9.23 corresponds precisely to the boundary in
    figure 9.22\. The root node corresponds to the first vertical line at *x*[0] =
    5, with the points at each side of the line corresponding to the two branches.
    The two horizontal lines at *x*[1] = 8.0 and *x*[1] = 2.5 on the left and right
    halves of the plot correspond to the two branches. Furthermore, at each node we
    have the following information:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图 9.23 中的树与图 9.22 中的边界完全对应。根节点对应于第一个垂直线 *x*[0] = 5，线两侧的点对应两个分支。图左侧和右侧的 *x*[1]
    = 8.0 和 *x*[1] = 2.5 的两条水平线对应两个分支。此外，在每个节点我们都有以下信息：
- en: '**Gini**: the Gini impurity index of the labels at that node'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基尼指数**：该节点标签的基尼不纯度指数'
- en: '**Samples**: the number of data points (samples) corresponding to that node'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本**：对应该节点的数据点（样本）数量'
- en: '**Value**: the number of data points of each of the two labels at that node'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**：该节点两个标签数据点的数量'
- en: 'As you can see, this tree has been trained using the Gini index, which is the
    default in Scikit-Learn. To train it using entropy, we can specify it when building
    the `DecisionTree` object, as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个树是使用基尼指数训练的，这是Scikit-Learn的默认值。要使用熵来训练它，我们可以在构建`DecisionTree`对象时指定它，如下所示：
- en: '[PRE3]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can specify more hyperparameters when training the tree, which we see in
    the next section with a much bigger example.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练树时，我们可以指定更多的超参数，我们将在下一节中通过一个更大的示例看到。
- en: 'Real-life application: Modeling student admissions with Scikit-Learn'
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实应用：使用Scikit-Learn建模学生录取
- en: 'In this section, we use decision trees to build a model that predicts admission
    to graduate schools. The dataset can be found in Kaggle (see appendix C for the
    link). As in the section “The graphical boundary of decision trees,” we’ll use
    Scikit-Learn to train the decision tree and Pandas to handle the dataset. The
    code for this section follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用决策树来构建一个预测研究生院录取的模型。数据集可以在Kaggle上找到（参见附录C中的链接）。与“决策树的图形边界”部分一样，我们将使用Scikit-Learn来训练决策树，并使用Pandas来处理数据集。本节的代码如下：
- en: '**Notebook**: University_admissions.ipynb'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：University_admissions.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/University_Admissions.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/University_Admissions.ipynb)'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/University_Admissions.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/University_Admissions.ipynb)'
- en: '**Dataset**: Admission_Predict.csv'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：Admission_Predict.csv'
- en: 'The dataset has the following features:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集具有以下特征：
- en: '**GRE score**: a number out of 340'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE分数**：一个340分以下的数字'
- en: '**TOEFL score**: a number out of 120'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TOEFL分数**：一个120分以下的数字'
- en: '**University rating**: a number from 1 to 5'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大学排名**：一个从1到5的数字'
- en: '**Statement of purpose strength (SOP)**: a number from 1 to 5'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的陈述强度（SOP）**：一个从1到5的数字'
- en: '**Undergraduate grade point average (CGPA)**: a number from 1 to 10'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本科平均成绩点（CGPA）**：一个从1到10的数字'
- en: '**Letter of recommendation strength (LOR)**: a number from 1 to 5'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐信强度（LOR）**：一个从1到5的数字'
- en: '**Research experience**: Boolean variable (0 or 1)'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究经验**：布尔变量（0或1）'
- en: The labels on the dataset are the chance of admission, which is a number between
    0 and 1\. To have binary labels, we’ll consider every student with a chance of
    0.75 or higher as “admitted,” and any other student as “not admitted.”
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的标签是录取概率，这是一个介于0和1之间的数字。为了得到二元标签，我们将录取概率为0.75或更高的每个学生视为“录取”，其他学生视为“未录取”。
- en: 'The code for loading the dataset into a Pandas DataFrame and performing this
    preprocessing step is shown next:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集到Pandas DataFrame并执行此预处理步骤的代码如下：
- en: '[PRE4]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first few rows of the resulting dataset are shown in table 9.5.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据集的前几行如表9.5所示。
- en: Table 9.5 A dataset with 400 students and their scores in standardized tests,
    grades, university ratings, letters of recommendations, statements of purpose,
    and information about their chances of being admitted to graduate school
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.5 一个包含400名学生及其标准化考试分数、成绩、大学排名、推荐信、目的陈述和其被录取到研究生院机会的信息的数据集
- en: '| GRE score | TOEFL score | University rating | SOP | LOR | CGPA | Research
    | Admitted |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| GRE分数 | TOEFL分数 | 大学排名 | SOP | LOR | CGPA | 研究 | 录取 |'
- en: '| 337 | 118 | 4 | 4.5 | 4.5 | 9.65 | 1 | True |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 337 | 118 | 4 | 4.5 | 4.5 | 9.65 | 1 | True |'
- en: '| 324 | 107 | 4 | 4.0 | 4.5 | 8.87 | 1 | True |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 324 | 107 | 4 | 4.0 | 4.5 | 8.87 | 1 | True |'
- en: '| 316 | 104 | 3 | 3.0 | 3.5 | 8.00 | 1 | False |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 316 | 104 | 3 | 3.0 | 3.5 | 8.00 | 1 | False |'
- en: '| 322 | 110 | 3 | 3.5 | 2.5 | 8.67 | 1 | True |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 322 | 110 | 3 | 3.5 | 2.5 | 8.67 | 1 | True |'
- en: '| 314 | 103 | 2 | 2.0 | 3.0 | 8.21 | 0 | False |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 314 | 103 | 2 | 2.0 | 3.0 | 8.21 | 0 | False |'
- en: 'As we saw in the section “The graphical boundary of decision trees,” Scikit-Learn
    requires that we enter the features and the labels separately. We’ll build a Pandas
    DataFrame called `features` containing all the columns except the Admitted column,
    and a Pandas Series called `labels` containing only the Admitted column. The code
    follows:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在“决策树的图形边界”部分所看到的，Scikit-Learn要求我们分别输入特征和标签。我们将构建一个名为`features`的Pandas DataFrame，包含除了“录取”列之外的所有列，以及一个名为`labels`的Pandas
    Series，只包含“录取”列。代码如下：
- en: '[PRE5]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we create a `DecisionTreeClassifier` object (which we call `dt`) and use
    the `fit` method. We’ll train it using the Gini index, as shown next, so there
    is no need to specify the `criterion` hyperparameter, but go ahead and train it
    with entropy and compare the results with those that we get here:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个`DecisionTreeClassifier`对象（我们称之为`dt`）并使用`fit`方法。我们将使用Gini指数进行训练，如以下所示，因此不需要指定`criterion`超参数，但请继续使用熵进行训练，并将结果与这里得到的结果进行比较：
- en: '[PRE6]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To make predictions, we can use the `predict` function. For example, here is
    how we make predictions for the first five students:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，我们可以使用`predict`函数。例如，以下是我们对前五个学生进行预测的方法：
- en: '[PRE7]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, the decision tree we just trained massively overfits. One way to see
    this is by using the `score` function and realizing that it scores 100% in the
    training set. In this chapter, we won’t test the model, but will try building
    a testing set and verifying that this model overfits. Another way to see the overfitting
    is to plot the tree and notice that its depth is 10 (see the notebook). In the
    next section, we learn about some hyperparameters that help us prevent overfitting.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们刚刚训练的决策树过度拟合了。一种观察这个现象的方法是使用`score`函数，并意识到它在训练集上的得分是100%。在本章中，我们不会测试模型，而是尝试构建一个测试集并验证这个模型是否过度拟合。另一种观察过度拟合的方法是绘制树形图，并注意其深度为10（见笔记本）。在下一节中，我们将了解一些有助于我们防止过度拟合的超参数。
- en: Setting hyperparameters in Scikit-Learn
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中设置超参数
- en: 'To prevent overfitting, we can use some of the hyperparameters that we learned
    in the section “Last step: When to stop building the tree and other hyperparameters,”
    such as the following:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过度拟合，我们可以使用我们在“最后一步：何时停止构建树和其他超参数”部分中学到的某些超参数，例如以下内容：
- en: '`max_depth`: the maximum allowed depth.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：允许的最大深度。'
- en: '`max_features`: the maximum number of features considered at each split (useful
    for when there are too many features, and the training process takes too long).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：在每次分割时考虑的最大特征数（当特征太多且训练过程太长时很有用）。'
- en: '`min_impurity_decrease`: the decrease in impurity must be higher than this
    threshold to split a node.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_decrease`：节点的不纯度必须高于此阈值才能分割节点。'
- en: '`min_impurity_split`: when the impurity at a node is lower than this threshold,
    the node becomes a leaf.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_split`：当节点的不纯度低于此阈值时，节点变为叶节点。'
- en: '`min_samples_leaf`: the minimum number of samples required for a leaf node.
    If a split leaves a leaf with less than this number of samples, the split is not
    performed.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`：叶节点所需的最小样本数。如果一个分割留下的叶节点样本数少于这个数量，则不执行分割。'
- en: '`min_samples_split`: the minimum number of samples required to split a node.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：分割节点所需的最小样本数。'
- en: 'Play around with these parameters to find a good model. We’ll use the following:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试调整这些参数以找到一个好的模型。我们将使用以下参数：
- en: '`max_depth = 3`'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth = 3`'
- en: '`min_samples_leaf = 10`'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf = 10`'
- en: '`min_samples_split = 10`'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split = 10`'
- en: '[PRE8]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resulting tree is illustrated in figure 9.24\. Note that in this tree, all
    the edges to the right correspond to “False” and to the left to “True.”
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 结果树如图9.24所示。请注意，在这棵树中，所有指向右边的边对应于“False”，而指向左边的边对应于“True”。
- en: '![](../Images/9-24.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-24.png)'
- en: Figure 9.24 A decision tree of depth 3 trained in the student admissions dataset
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24 学生录取数据集中训练的深度为3的决策树
- en: The prediction given at each of the leaves is the label corresponding to the
    majority of the nodes in that leaf. In the notebook, each node has a color assigned
    to it, ranging from orange to blue. The orange nodes are those with more points
    with label 0, and the blue nodes are those with label 1\. Notice that the white
    leaf, in which there are the same number of points with labels 0 and 1\. For this
    leaf, any prediction has the same performance. In this case, Scikit-Learn defaults
    to the first class in the list, which in this case is false.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个叶节点给出的预测是该叶节点中大多数节点的标签。在笔记本中，每个节点都分配了一个颜色，从橙色到蓝色。橙色节点是具有更多标签0的节点，蓝色节点是具有标签1的节点。请注意，白色叶节点中标签0和1的点数相同。对于这个叶节点，任何预测的性能都相同。在这种情况下，Scikit-Learn默认选择列表中的第一个类别，在这种情况下是false。
- en: 'To make a prediction, we use the `predict` function. For example, let’s predict
    the admission for a student with the following numbers:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，我们使用`predict`函数。例如，让我们预测以下数字的学生的录取情况：
- en: 'GRE score: 320'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRE分数：320
- en: 'TOEFL score: 110'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TOEFL分数：110
- en: 'University rating: 3'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大学评级：3
- en: 'SOP: 4.0'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SOP：4.0
- en: 'LOR: 3.5'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LOR：3.5
- en: 'CGPA: 8.9'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGPA：8.9
- en: 'Research: 0 (no research)'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究：0（无研究）
- en: '[PRE9]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The tree predicts that the student will be admitted.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 树预测该学生会获得录取。
- en: 'From this tree, we can infer the following things about our dataset:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个树中，我们可以推断出关于我们数据集的以下信息：
- en: The most important feature is the sixth column (*X*[5]), corresponding to the
    CGPA, or the grades. The cutoff grade is 8.735 out of 10\. In fact, most of the
    predictions to the right of the root node are “admit” and to the left are “not
    admit,” which implies that CGPA is a very strong feature.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的特征是第六列(*X*[5])，对应于CGPA，即成绩。截止分数是10分中的8.735分。事实上，根节点右侧的大多数预测都是“录取”，左侧的是“不录取”，这表明CGPA是一个非常强的特征。
- en: After this feature, the two most important ones are GRE score (*X*[0]) and TOEFL
    score (*X*[1]), both standardized tests. In fact, among the students who got good
    grades, most of them are likely to be admitted, unless they did poorly on the
    GRE, as accounted for by the sixth leaf from the left in the tree in figure 9.24.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此特征之后，最重要的两个特征是GRE分数(*X*[0])和托福分数(*X*[1])，都是标准化测试。事实上，在取得好成绩的学生中，大多数人很可能被录取，除非他们在GRE上表现不佳，如图9.24中从左数第六片叶子所说明的那样。
- en: Aside from grades and standardized tests, the only other feature appearing in
    the tree is SOP, or the strength of the statement of purpose. This is located
    down in the tree, and it didn’t change the predictions much.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了成绩和标准化测试之外，树中出现的唯一其他特征是SOP，即目的陈述的强度。它位于树的下方，并且没有太多地改变预测。
- en: Recall, however, that the construction of the tree is greedy in nature, namely,
    at each point it selects the top feature. This doesn’t guarantee that the choice
    of features is the best, however. For example, there could be a combination of
    features that is very strong, yet none of them is strong individually, and the
    tree may not be able to pick this up. Thus, even though we got some information
    about the dataset, we should not yet throw away the features that are not present
    in the tree. A good feature selection algorithm, such as L1 regularization, would
    come in handy when selecting features in this dataset.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，树的构建本质上是贪婪的，即在每一个点上它都选择最顶端的特征。但这并不保证特征的选择是最佳的。例如，可能存在一个特征组合非常强大，但没有任何一个单独强大，树可能无法捕捉到这一点。因此，尽管我们得到了一些关于数据集的信息，但我们不应立即丢弃树中未出现的特征。在特征选择时，一个好的特征选择算法，如L1正则化，将非常有用。
- en: Decision trees for regression
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归决策树
- en: 'In most of this chapter, we’ve used decision trees for classification, but
    as was mentioned earlier, decision trees are good regression models as well. In
    this section, we see how to build a decision tree regression model. The code for
    this section follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分内容中，我们使用了决策树进行分类，但如前所述，决策树也是很好的回归模型。在本节中，我们将看到如何构建决策树回归模型。本节的代码如下：
- en: '**Notebook**: Regression_decision_tree.ipynb'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：Regression_decision_tree.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Regression_decision_tree.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Regression_decision_tree.ipynb)'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Regression_decision_tree.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_9_Decision_Trees/Regression_decision_tree.ipynb)'
- en: 'Consider the following problem: we have an app, and we want to predict the
    level of engagement of the users in terms of how many days per week they used
    it. The only feature we have is the user’s age. The dataset is shown in table
    9.6, and its plot is in figure 9.25.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下问题：我们有一个应用，我们想要预测用户每周使用应用的天数来衡量他们的互动水平。我们拥有的唯一特征是用户的年龄。数据集显示在表9.6中，其图表在图9.25中。
- en: Table 9.6 A small dataset with eight users, their age, and their engagement
    with our app. The engagement is measured in the number of days when they opened
    the app in one week.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.6 一个包含八个用户、他们的年龄以及他们与我们应用互动的小数据集。互动是通过他们在一周内打开应用的天数来衡量的。
- en: '| Age | Engagement |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 | 互动 |'
- en: '| 10 | 7 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 |'
- en: '| 20 | 5 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 5 |'
- en: '| 30 | 7 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 7 |'
- en: '| 40 | 1 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 1 |'
- en: '| 50 | 2 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 2 |'
- en: '| 60 | 1 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 1 |'
- en: '| 70 | 5 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 70 | 5 |'
- en: '| 80 | 4 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 4 |'
- en: 'From this dataset, it seems that we have three clusters of users. The young
    users (ages 10, 20, 30) use the app a lot, the middle-aged users (ages 40, 50,
    60) don’t use it very much, and the older users (ages 70, 80) use it sometimes.
    Thus, a prediction like this one would make sense:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个数据集中，似乎我们有三个用户群体。年轻用户（10岁、20岁、30岁）经常使用应用，中年用户（40岁、50岁、60岁）不太使用，而老年用户（70岁、80岁）有时使用。因此，这样的预测是有意义的：
- en: If the user is 34 years old or younger, the engagement is 6 days per week.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户年龄为34岁或以下，每周的参与度为6天。
- en: If the user is between 35 and 64, the engagement is 1 day per week.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户年龄在35岁到64岁之间，每周的参与度为1天。
- en: If the user is 65 or older, the engagement is 3.5 days per week.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户年龄为65岁或以上，每周的参与度为3.5天。
- en: '![](../Images/9-25.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![图9.25](../Images/9-25.png)'
- en: Figure 9.25 The plot of the dataset in table 9.6, where the horizontal axis
    corresponds to the age of the user and the vertical axis to the number of days
    per week that they engaged with the app
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 表9.6中数据集的图表，其中水平轴对应用户的年龄，垂直轴对应他们每周与该应用的互动天数
- en: The predictions of a regression decision tree look similar to this, because
    the decision tree splits our users into groups and predicts a fixed value for
    each of the groups. The way to split the users is by using the features, exactly
    like we did for classification problems.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 回归决策树的预测看起来类似于这个，因为决策树将我们的用户分成组，并为每个组预测一个固定值。分割用户的方式是通过使用特征，这与我们在分类问题中所做的方式完全一样。
- en: Lucky for us, the algorithm used for training a regression decision tree is
    very similar to the one we used for training a classification decision tree. The
    only difference is that for classification trees, we used accuracy, Gini index,
    or entropy, and for regression trees, we use the mean square error (MSE). The
    mean square error may sound familiar—we used it to train linear regression models
    in the section “How do we measure our results? The error function” in chapter
    3.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，用于训练回归决策树的算法与我们用于训练分类决策树的算法非常相似。唯一的区别是，对于分类树，我们使用了准确率、基尼指数或熵，而对于回归树，我们使用平均平方误差（MSE）。平均平方误差可能听起来很熟悉——我们在第3章的“我们如何衡量我们的结果？误差函数”部分使用了它来训练线性回归模型。
- en: Before we get into the algorithm, let’s think about it conceptually. Imagine
    that you have to fit a line as close as possible to the dataset in figure 9.25\.
    But there is a catch—the line must be horizontal. Where should we fit this horizontal
    line? It makes sense to fit it in the “middle” of the dataset—in other words,
    at a height equal to the average of the labels, which is 4\. That is a very simple
    classification model, which assigns to every point the same prediction of 4.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入算法之前，让我们从概念上思考一下。想象一下，你必须将一条线尽可能紧密地拟合到图9.25中的数据集。但是有一个限制——这条线必须是水平的。我们应该在哪里拟合这条水平线呢？将其拟合在数据集的“中间”是有意义的——换句话说，在标签的平均高度，即4。这是一个非常简单的分类模型，它将相同的预测值4分配给每个点。
- en: Now, let’s go a bit further. If we had to use two horizontal segments, how should
    we fit them as close as possible to the data? We might have several guesses, with
    one being to put a high bar for the points to the left of 35 and a low bar to
    the right of 35\. That represents a decision stump that asks the question, “Are
    you younger than 35?” and assigns predictions based on how the user answered that
    question.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更进一步。如果我们必须使用两个水平段，我们应该如何将它们尽可能紧密地拟合到数据上？我们可能会有几个猜测，其中一个是将35岁左边的点设置一个较高的栏，而将35岁右边的点设置一个较低的栏。这代表了一个决策树桩，它提出了问题：“你小于35岁吗？”并根据用户如何回答这个问题来分配预测。
- en: What if we could split each of these two horizontal segments into two more—where
    should we locate them? We can continue following this process until we have broken
    down the users
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够将这两个水平段各自再分成两个——我们应该将它们定位在哪里呢？我们可以继续遵循这个过程，直到我们将用户分解成几个标签非常相似的组。然后，我们预测该组中所有用户的平均标签。
- en: into several groups in which their labels are very similar. We then predict
    the average label for all the users in that group.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用户分解成几个标签非常相似的组。然后，我们预测该组中所有用户的平均标签。
- en: 'The process we just followed is the process of training a regression decision
    tree. Now let’s get more formal. Recall that when a feature is numerical, we consider
    all the possible ways to split it. Thus, the possible ways to split the age feature
    are using, for example, the following cutoffs: 15, 25, 35, 45, 55, 65, and 75\.
    Each of these cutoffs gives us two smaller datasets, which we call the left dataset
    and the right dataset. Now we carry out the following steps:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才遵循的过程是训练回归决策树的过程。现在让我们更正式一些。回想一下，当一个特征是数值时，我们考虑所有可能的分割方式。因此，分割年龄特征的可能是使用以下截止值：15，25，35，45，55，65和75。每个这些截止值都给我们两个较小的数据集，我们称之为左数据集和右数据集。现在我们执行以下步骤：
- en: For each of the smaller datasets, we predict the average value of the labels.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个较小的数据集，我们预测标签的平均值。
- en: We calculate the mean square error of the prediction.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算预测的平均平方误差。
- en: We select the cutoff that gives us the smallest square error.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择给出最小平方误差的截止点。
- en: 'For example, if our cutoff is 65, then the two datasets are the following:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的截止点是65岁，那么两个数据集如下所示：
- en: '**Left dataset**: users younger than 65\. The labels are {7, 5, 7, 1, 2, 1}.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**左侧数据集**：65岁以下的用户。标签为{7, 5, 7, 1, 2, 1}。'
- en: '**Right dataset**: users 65 or older. The labels are {5,4}.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**右侧数据集**：65岁或以上的用户。标签为{5,4}。'
- en: 'For each dataset, we predict the average of the labels, which is 3.833 for
    the left one and 4.5 for the right one. Thus, the prediction for the first six
    users is 3.833, and for the last two is 4.5\. Now, we calculate the MSE as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据集，我们预测标签的平均值，左侧为3.833，右侧为4.5。因此，前六个用户的预测值为3.833，最后两个为4.5。现在，我们按照以下方式计算MSE：
- en: '![](../Images/09_25_E01.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09_25_E01.png)'
- en: In table 9.7, we can see the values obtained for each of the possible cutoffs.
    The full calculations are at the end of the notebook for this section.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在表9.7中，我们可以看到每个可能的截止点所得到的值。完整的计算在笔记本本节的末尾。
- en: Table 9.7 The nine possible ways to split the dataset by age using a cutoff.
    Each cutoff splits the dataset into two smaller datasets, and for each of these
    two, the prediction is given by the average of the labels. The mean square error
    (MSE) is calculated as the average of the squares of the differences between the
    labels and the prediction. Notice that the splitting with the smallest MSE is
    obtained with a cutoff of 35\. This gives us the root node in our decision tree.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.7 使用截止点按年龄分割数据集的九种可能方式。每个截止点将数据集分割成两个更小的数据集，对于这两个数据集，预测值由标签的平均值给出。均方误差（MSE）是通过计算标签和预测值之间差异的平方的平均值来计算的。请注意，具有最小MSE的分割是在35岁的截止点获得的。这为我们决策树中的根节点提供了信息。
- en: '| Cutoff | Labels left set | Labels right set | Prediction left set | Prediction
    right set | MSE |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 截止点 | 左侧标签集 | 右侧标签集 | 左侧预测集 | 右侧预测集 | MSE |'
- en: '| 0 | {} | {7,5,7,1,2,1,5,4} | None | 4.0 | 5.25 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 0 | {} | {7,5,7,1,2,1,5,4} | None | 4.0 | 5.25 |'
- en: '| 15 | {7} | {5,7,1,2,1,5,4} | 7.0 | 3.571 | 3.964 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| 15 | {7} | {5,7,1,2,1,5,4} | 7.0 | 3.571 | 3.964 |'
- en: '| 25 | {7,5} | {7,1,2,1,5,4} | 6.0 | 3.333 | 3.917 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 25 | {7,5} | {7,1,2,1,5,4} | 6.0 | 3.333 | 3.917 |'
- en: '| 35 | {7,5,7} | {1,2,1,5,4} | 6.333 | 2.6 | 1.983 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 35 | {7,5,7} | {1,2,1,5,4} | 6.333 | 2.6 | 1.983 |'
- en: '| 45 | {7,5,7,1} | {2,1,5,4} | 5.0 | 3.0 | 4.25 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| 45 | {7,5,7,1} | {2,1,5,4} | 5.0 | 3.0 | 4.25 |'
- en: '| 55 | {7,5,7,1,2} | {1,5,4} | 4.4 | 3.333 | 4.983 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 55 | {7,5,7,1,2} | {1,5,4} | 4.4 | 3.333 | 4.983 |'
- en: '| 65 | {7,5,7,1,2,1} | {5,4} | 3.833 | 4.5 | 5.167 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 65 | {7,5,7,1,2,1} | {5,4} | 3.833 | 4.5 | 5.167 |'
- en: '| 75 | {7,5,7,1,2,1,5} | {4} | 4.0 | 4.0 | 5.25 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 75 | {7,5,7,1,2,1,5} | {4} | 4.0 | 4.0 | 5.25 |'
- en: '| 100 | {7,5,7,1,2,1,5,4} | {} | 4.0 | none | 5.25 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 100 | {7,5,7,1,2,1,5,4} | {} | 4.0 | none | 5.25 |'
- en: The best cutoff is at 35 years old, because it gave us the prediction with the
    least mean square error. Thus, we’ve built the first decision node in our regression
    decision tree. The next steps are to continue splitting the left and right datasets
    recursively in the same fashion. Instead of doing it by hand, we’ll use Scikit-Learn
    as before.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳截止点为35岁，因为它给出了最小的均方误差预测。因此，我们在回归决策树中建立了第一个决策节点。接下来的步骤是以相同的方式递归地继续分割左侧和右侧数据集。我们不会手动进行，而是像之前一样使用Scikit-Learn。
- en: 'First, we define our features and labels. We can use arrays for this, as shown
    next:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的特征和标签。我们可以使用数组来完成这项工作，如下所示：
- en: '[PRE10]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we build a regression decision tree of maximum depth 2 using the `DecisionTreeRegressor`
    object as follows:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用`DecisionTreeRegressor`对象构建一个最大深度为2的回归决策树，如下所示：
- en: '[PRE11]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The resulting decision tree is shown in figure 9.26\. The first cutoff is at
    35, as we had already figured out. The next two cutoffs are at 15 and 65\. At
    the right of figure 9.26, we can also see the predictions for each of these four
    resulting subsets of the data.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的决策树如图9.26所示。第一个截止点为35岁，正如我们之前已经计算出的。接下来的两个截止点为15岁和65岁。在图9.26的右侧，我们还可以看到这些四个数据子集的预测值。
- en: '![](../Images/9-26.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-26.png)'
- en: 'Figure 9.26 Left: The resulting decision tree obtained in Scikit-Learn. This
    tree has three decision nodes and four leaves. Right: The plot of the predictions
    made by this decision tree. Note that the cutoffs are at ages 35, 15, and 65,
    corresponding to the decision nodes in the tree. The predictions are 7, 6, 1.33,
    and 4.5, corresponding to the leaves in the tree.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 左侧：Scikit-Learn中得到的决策树。此树有三个决策节点和四个叶子节点。右侧：此决策树做出的预测图。注意，截止年龄为35岁、15岁和65岁，对应树中的决策节点。预测值为7、6、1.33和4.5，对应树中的叶子节点。
- en: Applications
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: 'Decision trees have many useful applications in real life. One special feature
    of decision trees is that, aside from predicting, they give us a lot of information
    about our data, because they organize it in a hierarchical structure. Many times,
    this information is of as much or even more value as the capacity of making predictions.
    In this section, we see some examples of decision trees used in real life in the
    following fields:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在现实生活中有许多有用的应用。决策树的一个特殊之处在于，除了预测之外，它们还为我们提供了大量关于数据的信息，因为它们以分层结构组织数据。很多时候，这些信息的价值与预测能力相当，甚至更高。在本节中，我们将看到决策树在以下领域的实际应用示例：
- en: Health care
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健
- en: Recommendation systems
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Decision trees are widely used in health care
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在医疗保健领域被广泛使用
- en: Decision trees are widely used in medicine, not only to make predictions but
    also to identify features that are determinant in the prediction. You can imagine
    that in medicine, a black box saying “the patient is sick” or “the patient is
    healthy” is not good enough. However, a decision tree comes with a great deal
    of information about why the prediction was made. The patient could be sick based
    on their symptoms, family medical history, habits, or many other factors.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在医学领域被广泛使用，不仅用于预测，还用于识别预测中的决定性特征。你可以想象，在医学领域，一个只说“患者生病”或“患者健康”的黑盒是不够的。然而，决策树提供了大量关于预测原因的信息。患者可能因为症状、家族病史、习惯或许多其他因素而生病。
- en: Decision trees are useful in recommendation systems
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在推荐系统中非常有用
- en: In recommendation systems, decision trees are also useful. One of the most famous
    recommendation systems problems, the Netflix prize, was won with the help of decision
    trees. In 2006, Netflix held a competition that involved building the best possible
    recommendation system to predict user ratings of their movies. In 2009, they awarded
    $1,000,000 USD to the winner, who improved the Netflix algorithm by over 10%.
    The way they did this was using gradient-boosted decision trees to combine more
    than 500 different models. Other recommendation engines use decision trees to
    study the engagement of their users and figure out the demographic features to
    best determine engagement.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中，决策树同样非常有用。最著名的推荐系统问题之一，Netflix 奖项，就是借助决策树获得的。2006年，Netflix举办了一场竞赛，旨在构建最佳推荐系统以预测用户对电影的评分。2009年，他们向获奖者颁发了100万美元美元奖金，获奖者通过改进Netflix算法使其提升了超过10%。他们这样做的方式是使用梯度提升决策树来结合超过500个不同的模型。其他推荐引擎也使用决策树来研究用户的参与度，并找出最佳确定参与度的人口统计特征。
- en: In chapter 12, we will learn more about gradient-boosted decision trees and
    random forests. For now, the best way to imagine them is as a collection of many
    decision trees working together to make the best predictions.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章中，我们将学习更多关于梯度提升决策树和随机森林的知识。目前，想象它们最好的方式是作为许多决策树共同工作以做出最佳预测的集合。
- en: Summary
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Decision trees are important machine learning models, used for classification
    and regression.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是重要的机器学习模型，用于分类和回归。
- en: The way decision trees work is by asking binary questions about our data and
    making a prediction based on the answers to those questions.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的工作方式是通过询问关于数据的数据的二进制问题，并根据这些问题的答案做出预测。
- en: The algorithm for building decision trees for classification consists of finding
    the feature in our data that best determines the label and iterating over this
    step.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建分类决策树的算法包括找到最佳决定标签的特征，并迭代这一步骤。
- en: We have several ways to tell if a feature determines the label best. The three
    that we learned in this chapter are accuracy, Gini impurity index, and entropy.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有几种方法可以判断一个特征是否最佳地决定了标签。在本章中我们学习到的三种方法是准确率、基尼不纯度指数和熵。
- en: The Gini impurity index measures the purity of a set. In that way, a set in
    which every element has the same label has a Gini impurity index of 0\. A set
    in which every element has a different label has a Gini impurity label close to
    1.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尼不纯度指数衡量集合的纯度。因此，每个元素都有相同标签的集合具有基尼不纯度指数为0。每个元素都有不同标签的集合具有接近1的基尼不纯度指数。
- en: Entropy is another measure for the purity of a set. A set in which every element
    has the same label has an entropy of 0\. A set in which half of the elements have
    one label and the other half has another label has an entropy of 1\. When building
    a decision tree, the difference in entropy before and after a split is called
    information gain.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵是衡量集合纯度的另一种度量。所有元素都具有相同标签的集合熵为0。一半元素具有一个标签，另一半具有另一个标签的集合熵为1。在构建决策树时，分割前后的熵差称为信息增益。
- en: The algorithm for building a decision tree for regression is similar to the
    one used for classification. The only difference is that we use the mean square
    error to select the best feature to split the data.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建回归决策树的算法与用于分类的算法类似。唯一的区别是我们使用均方误差来选择最佳特征以分割数据。
- en: In two dimensions, regression tree plots look like the union of several horizontal
    lines, where each horizontal line is the prediction for the elements in a particular
    leaf.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二维空间中，回归树图看起来像是几条水平线的并集，其中每条水平线是对特定叶节点中元素的预测。
- en: Applications of decision trees range very widely, from recommendation algorithms
    to applications in medicine and biology.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的应用范围非常广泛，从推荐算法到医学和生物学的应用。
- en: Exercises
  id: totrans-520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 9.1
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.1
- en: In the following spam-detection decision tree model, determine whether an email
    from your mom with the subject line, “Please go to the store, there’s a sale,”
    will be classified as spam.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下垃圾邮件检测决策树模型中，确定来自您母亲的电子邮件，主题为“请去商店，有促销活动”，是否会被分类为垃圾邮件。
- en: '![](../Images/9-unnumb-2.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-unnumb-2.png)'
- en: Exercise 9.2
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.2
- en: 'Our goal is to build a decision tree model to determine whether credit card
    transactions are fraudulent. We use the dataset of credit card transactions below,
    with the following features:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个决策树模型，以确定信用卡交易是否为欺诈。我们使用以下信用卡交易数据集，具有以下特征：
- en: '**Value**: value of the transaction.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**：交易的值。'
- en: '**Approved vendor**: the credit card company has a list of approved vendors.
    This variable indicates whether the vendor is in this list.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已批准供应商**：信用卡公司有一份已批准供应商的名单。此变量表示供应商是否在该名单中。'
- en: '|  | Value | Approved vendor | Fraudulent |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | 值 | 已批准供应商 | 欺诈 |'
- en: '| Transaction 1 | $100 | Not approved | Yes |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 1 | $100 | 未批准 | 是 |'
- en: '| Transaction 2 | $100 | Approved | No |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 2 | $100 | 已批准 | 否 |'
- en: '| Transaction 3 | $10,000 | Approved | No |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 3 | $10,000 | 已批准 | 否 |'
- en: '| Transaction 4 | $10,000 | Not approved | Yes |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 4 | $10,000 | 未批准 | 是 |'
- en: '| Transaction 5 | $5,000 | Approved | Yes |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 5 | $5,000 | 已批准 | 是 |'
- en: '| Transaction 6 | $100 | Approved | No |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 交易编号 6 | $100 | 已批准 | 否 |'
- en: 'Build the first node of the decision tree with the following specifications:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下规格构建决策树的第一节点：
- en: Using the Gini impurity index
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基尼不纯度指数
- en: Using entropy
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用熵
- en: Exercise 9.3
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.3
- en: A dataset of patients who have tested positive or negative for COVID-19 follows.
    Their symptoms are cough (C), fever (F), difficulty breathing (B), and tiredness
    (T).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个COVID-19检测结果为阳性或阴性的患者数据集。他们的症状是咳嗽 (C)、发烧 (F)、呼吸困难 (B) 和疲劳 (T)。
- en: '|  | Cough (C) | Fever (F) | Difficulty breathing (B) | Tiredness (T) | Diagnosis
    |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|  | 咳嗽 (C) | 发烧 (F) | 呼吸困难 (B) | 疲劳 (T) | 诊断 |'
- en: '| Patient 1 |  | X | X | X | Sick |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 1 |  | X | X | X | 病人 |'
- en: '| Patient 2 | X | X |  | X | Sick |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 2 | X | X |  | X | 病人 |'
- en: '| Patient 3 | X |  | X | X | Sick |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 3 | X |  | X | X | 病人 |'
- en: '| Patient 4 | X | X | X |  | Sick |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 4 | X | X | X |  | 病人 |'
- en: '| Patient 5 | X |  |  | X | Healthy |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 5 | X |  |  | X | 健康 |'
- en: '| Patient 6 |  | X | X |  | Healthy |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 6 |  | X | X |  | 健康 |'
- en: '| Patient 7 |  | X |  |  | Healthy |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 7 |  | X |  |  | 健康 |'
- en: '| Patient 8 |  |  |  | X | Healthy |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 8 |  |  |  | X | 健康 |'
- en: Using accuracy, build a decision tree of height 1 (a decision stump) that classifies
    this data. What is the accuracy of this classifier on the dataset?
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 使用准确率，构建一个高度为1的决策树（决策树桩），对数据进行分类。这个分类器在数据集上的准确率是多少？
