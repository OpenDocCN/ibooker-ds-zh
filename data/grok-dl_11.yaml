- en: 'Chapter 12\. Neural networks that write like Shakespeare: recurrent layers
    for variable-length data'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章：像莎士比亚一样写作的神经网络：用于可变长度数据的循环层
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: The challenge of arbitrary length
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意长度挑战
- en: The surprising power of averaged word vectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均词向量的惊人力量
- en: The limitations of bag-of-words vectors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋向量局限性
- en: Using identity vectors to sum word embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单位向量求和词嵌入
- en: Learning the transition matrices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过渡矩阵
- en: Learning to create useful sentence vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习创建有用的句子向量
- en: Forward propagation in Python
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中的正向传播
- en: Forward propagation and backpropagation with arbitrary length
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有任意长度的正向传播和反向传播
- en: Weight update with arbitrary length
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有任意长度的权重更新
- en: “There’s something magical about Recurrent Neural Networks.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “循环神经网络有一种神奇的力量。”
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Andrej Karpathy, “The Unreasonable Effectiveness of Recurrent Neural Networks,”
    [http://mng.bz/VPW](http://mng.bz/VPW)*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*安德烈·卡帕蒂，“循环神经网络的不合理有效性”，[http://mng.bz/VPW](http://mng.bz/VPW)*'
- en: The challenge of arbitrary length
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任意长度挑战
- en: Let’s model arbitrarily long sequences of data with neural networks!
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们用神经网络来模拟任意长度的数据序列！
- en: This chapter and [chapter 11](kindle_split_019.xhtml#ch11) are intertwined,
    and I encourage you to ensure that you’ve mastered the concepts and techniques
    from [chapter 11](kindle_split_019.xhtml#ch11) before you dive into this one.
    In [chapter 11](kindle_split_019.xhtml#ch11), you learned about natural language
    processing (NLP). This included how to modify a loss function to learn a specific
    pattern of information within the weights of a neural network. You also developed
    an intuition for what a word embedding is and how it can represent shades of similarity
    with other word embeddings. In this chapter, we’ll expand on this intuition of
    an embedding conveying the meaning of a single word by creating embeddings that
    convey the meaning of variable-length phrases and sentences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和[第11章](kindle_split_019.xhtml#ch11)相互交织，我鼓励你在深入研究这一章之前，确保你已经掌握了第11章中的概念和技术。[第11章](kindle_split_019.xhtml#ch11)中，你学习了关于自然语言处理（NLP）的内容。这包括如何修改损失函数来学习神经网络权重中的特定信息模式。你还培养了对词嵌入的理解，以及它如何与其他词嵌入表示相似度的细微差别。在本章中，我们将通过创建能够传达可变长度短语和句子意义的嵌入来扩展这种对嵌入传达单个词语意义的直觉。
- en: Let’s first consider this challenge. If you wanted to create a vector that held
    an entire sequence of symbols within its contents in the same way a word embedding
    stores information about a word, how would you accomplish this? We’ll start with
    the simplest option. In theory, if you concatenated or stacked the word embeddings,
    you’d have a vector of sorts that held an entire sequence of symbols.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑这个挑战。如果你想要创建一个包含整个符号序列的向量，就像词嵌入存储关于一个词的信息一样，你会如何实现？我们将从最简单的方法开始。从理论上讲，如果你连接或堆叠词嵌入，你将得到一种类型的向量，它包含整个符号序列。
- en: '![](Images/f0210-01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0210-01.jpg)'
- en: 'But this approach leaves something to be desired, because different sentences
    will have different-length vectors. This makes comparing two vectors together
    tricky, because one vector will stick out the side. Consider the following second
    sentence:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法仍有不足之处，因为不同的句子会有不同长度的向量。这使得比较两个向量变得困难，因为其中一个向量会突出出来。考虑以下第二句话：
- en: '![](Images/f0210-02_alt.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0210-02_alt.jpg)'
- en: In theory, these two sentences should be very similar, and comparing their vectors
    should indicate a high degree of similarity. But because “the cat sat” is a shorter
    vector, you have to choose which part of “the cat sat still” vector to compare
    to. If you align left, the vectors will appear to be identical (ignoring the fact
    that “the cat sat still” is, in fact, a different sentence). But if you align
    right, then the vectors will appear to be extraordinarily different, despite the
    fact that three-quarters of the words are the same, in the same order. Although
    this naive approach shows some promise, it’s far from ideal in terms of representing
    the meaning of a sentence in a useful way (a way that can be compared with other
    vectors).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这两个句子应该非常相似，比较它们的向量应该显示出高度的相似性。但是因为“the cat sat”是一个较短的向量，你必须选择“the cat sat
    still”向量中的哪一部分进行比较。如果你从左边对齐，向量看起来将完全相同（忽略“the cat sat still”实际上是一个不同的句子这一事实）。但是如果你从右边对齐，那么向量看起来将非常不同，尽管四分之三的单词是相同的，并且顺序相同。尽管这种朴素的方法显示出一些希望，但在以有用方式（可以与其他向量进行比较的方式）表示句子意义方面还远远不够理想。
- en: Do comparisons really matter?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较真的重要吗？
- en: Why should you care about whether you can compare two sentence vectors?
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么你应该关心你是否可以比较两个句子向量？
- en: The act of comparing two vectors is useful because it gives an approximation
    of what the neural network sees. Even though you can’t read two vectors, you can
    tell when they’re similar or different (using the function from [chapter 11](kindle_split_019.xhtml#ch11)).
    If the method for generating sentence vectors doesn’t reflect the similarity you
    observe between two sentences, then the network will also have difficulty recognizing
    when two sentences are similar. All it has to work with are the vectors!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个向量的行为很有用，因为它可以近似地反映出神经网络所看到的内容。即使你不能直接读取两个向量，你也能判断它们是相似还是不同（使用第11章中的函数[chapter
    11](kindle_split_019.xhtml#ch11)）。如果生成句子向量的方法没有反映出你观察到的两个句子之间的相似性，那么网络在识别两个句子相似时也会遇到困难。它所需要处理的只有向量！
- en: As we continue to iterate and evaluate various methods for computing sentence
    vectors, I want you to remember why we’re doing this. We’re trying to take the
    perspective of a neural network. We’re asking, “Will the correlation summarization
    find correlation between sentence vectors similar to this one and a desirable
    label, or will two nearly identical sentences instead generate wildly different
    vectors such that there is very little correlation between sentence vectors and
    the corresponding labels you’re trying to predict?” We want to create sentence
    vectors that are useful for predicting things about the sentence, which, at a
    minimum, means similar sentences need to create similar vectors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们继续迭代并评估计算句子向量的各种方法时，我想让你记住我们为什么要这样做。我们试图从神经网络的视角出发。我们问，“相关性总结是否会找到与这个句子向量类似的句子和期望标签之间的相关性，或者两个几乎相同的句子会产生截然不同的向量，使得句子向量和相应的标签之间的相关性非常小？”我们希望创建对预测句子中的事物有用的句子向量，这至少意味着相似的句子需要创建相似的向量。
- en: The previous way of creating the sentence vectors (concatenation) had issues
    because of the rather arbitrary way of aligning them, so let’s explore the next-simplest
    approach. What if you take the vector for each word in a sentence, and average
    them? Well, right off the bat, you don’t have to worry about alignment because
    each sentence vector is of the same length!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 之前创建句子向量的方法（连接）存在问题，因为它们对齐的方式相当任意，所以让我们探索下一个最简单的方法。如果你取句子中每个单词的向量并取平均值会怎样？嗯，一开始，你不必担心对齐问题，因为每个句子向量长度相同！
- en: '![](Images/f0211-01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0211-01.jpg)'
- en: Furthermore, the sentences “the cat sat” and “the cat sat still” will have similar
    sentence vectors because the words going into them are similar. Even better, it’s
    likely that “a dog walked” will be similar to “the cat sat,” even though no words
    overlap, because the words used are also similar.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，“the cat sat”和“the cat sat still”这两个句子将会有相似的句子向量，因为进入它们的单词是相似的。更好的是，“a dog
    walked”可能和“the cat sat”相似，即使没有单词重叠，因为使用的单词也是相似的。
- en: As it turns out, averaging word embeddings is a surprisingly effective way to
    create word embeddings. It’s not perfect (as you’ll see), but it does a strong
    job of capturing what you might perceive to be complex relationships between words.
    Before moving on, I think it will be extremely beneficial to take the word embeddings
    from [chapter 11](kindle_split_019.xhtml#ch11) and play around with the average
    strategy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，平均词嵌入是一种出奇有效的创建词嵌入的方法。它并不完美（正如你将看到的），但它很好地捕捉了你可能感知到的词语之间复杂关系的各个方面。在继续之前，我认为从[第11章](kindle_split_019.xhtml#ch11)中提取词嵌入并尝试平均策略将非常有益。
- en: The surprising power of averaged word vectors
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均词向量的惊人力量
- en: It’s the amazingly powerful go-to tool for neural prediction
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是神经预测中强大而常用的工具
- en: In the previous section, I proposed the second method for creating vectors that
    convey the meaning of a sequence of words. This method takes the average of the
    vectors corresponding to the words in a sentence, and intuitively we expect these
    new average sentence vectors to behave in several desirable ways.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我提出了创建表示一系列词语意义的向量的第二种方法。这种方法取句子中对应词语的向量的平均值，直观地，我们期望这些新的平均句子向量以几种期望的方式表现。
- en: In this section, let’s play with sentence vectors generated using the embeddings
    from the previous chapter. Break out the code from [chapter 11](kindle_split_019.xhtml#ch11),
    train the embeddings on the IMDB corpus as you did before, and let’s experiment
    with average sentence embeddings.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们使用上一章生成的嵌入来玩句子向量。将[第11章](kindle_split_019.xhtml#ch11)中的代码提取出来，像之前一样在IMDB语料库上训练嵌入，然后让我们实验平均句子嵌入。
- en: At right is the same normalization performed when comparing word embeddings
    before. But this time, let’s prenormalize all the word embeddings into a matrix
    called `normed_weights`. Then, create a function called `make_sent_vect` and use
    it to convert each review (list of words) into embeddings using the average approach.
    This is stored in the matrix `reviews2vectors`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧是之前比较词嵌入时执行的同一次归一化。但这次，让我们将所有词嵌入预先归一化到一个称为`normed_weights`的矩阵中。然后，创建一个名为`make_sent_vect`的函数，并使用它通过平均方法将每个评论（单词列表）转换为嵌入。这存储在矩阵`reviews2vectors`中。
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* Tokenized reviews**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 分词评论**'
- en: After this, you create a function that queries for the most similar reviews
    given an input review, by performing a dot product between the input review’s
    vector and the vector of every other review in the corpus. This dot product similarity
    metric is the same one we briefly discussed in [chapter 4](kindle_split_012.xhtml#ch04)
    when you were learning to predict with multiple inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你将创建一个函数，通过在输入评论的向量与语料库中每个其他评论的向量之间执行点积，来查询给定输入评论的最相似评论。这种点积相似度指标与我们之前在[第4章](kindle_split_012.xhtml#ch04)中简要讨论的相同，当时你正在学习使用多个输入进行预测。
- en: Perhaps surprisingly, when you query for the most similar reviews to the average
    vector between the two words “boring” and “awful,” you receive back three very
    negative reviews. There appears to be interesting statistical information within
    these vectors, such that negative and positive embeddings cluster together.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可能令人惊讶的是，当你查询两个词“无聊”和“糟糕”之间的平均向量最相似的评论时，你收到了三个非常负面的评论。似乎在这些向量中存在有趣的统计信息，使得负面和正面的嵌入聚集在一起。
- en: How is information stored in these embeddings?
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这些嵌入中信息是如何存储的？
- en: When you average word embeddings, average shapes remain
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当你平均词嵌入时，平均形状保持不变
- en: 'Considering what’s going on here requires a little abstract thought. I recommend
    digesting this kind of information over a period of time, because it’s probably
    a different kind of lesson than you’re used to. For a moment, I’d like you to
    consider that a word vector can be visually observed as a *squiggly line* like
    this one:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这里发生的事情需要一点抽象思维。我建议你花一段时间消化这类信息，因为它可能与你习惯的教训不同。暂时，我想让你考虑一个词向量可以像这样被可视化为一条*波浪线*：
- en: '![](Images/f0213-01_alt.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0213-01_alt.jpg)'
- en: 'Instead of thinking of a vector as a list of numbers, think about it as a line
    with high and low points corresponding to high and low values at different places
    in the vector. If you selected several words from the corpus, they might look
    like this figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将向量视为数字列表，而要将其视为一条有高点和低点的线，这些高点低点对应于向量中不同位置的高值和低值。如果你从语料库中选择了几个词，它们可能看起来像这样：
- en: Consider the similarities between the various words. Notice that each vector’s
    corresponding shape is unique. But “terrible” and “boring” have a certain similarity
    in their shape. “beautiful” and “wonderful” also have a similarity to their shape,
    but it’s different from that of the other words. If we were to cluster these little
    squiggles, words with similar meaning would cluster together. More important,
    parts of these squiggles have true meaning in and of themselves.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑各种单词之间的相似性。注意，每个向量的对应形状是唯一的。但“糟糕”和“无聊”在形状上具有一定的相似性。“美丽”和“奇妙”也与它们的形状相似，但与其它单词不同。如果我们对这些小波浪线进行聚类，具有相似意义的单词会聚在一起。更重要的是，这些波浪线的一部分本身就有真正的意义。
- en: '![](Images/f0213-02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0213-02.jpg)'
- en: For example, for the negative words, there’s a downward and then upward spike
    about 40% from the left. If I were to continue drawing lines corresponding to
    words, this spike would continue to be distinctive. There’s nothing magical about
    that spike that means “negativity,” and if I retrained the network, it would likely
    show up somewhere else. The spike indicates negativity only because all the negative
    words have it!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于负面词汇，从左侧大约40%的位置有一个向下然后向上的尖峰。如果我要继续绘制与单词对应的线条，这个尖峰将继续保持独特。那个尖峰并没有什么神奇之处意味着“负面”，如果我重新训练网络，它可能会出现在其他地方。这个尖峰仅表明负面，因为所有负面词汇都有这个特征！
- en: Thus, during the course of training, these shapes are molded such that different
    curves in different positions convey meaning (as discussed in [chapter 11](kindle_split_019.xhtml#ch11)).
    When you take an average curve over the words in a sentence, the most dominant
    meanings of the sentence hold true, and the noise created by any particular word
    gets averaged away.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练过程中，这些形状被塑造成不同的曲线在不同位置传达意义（如第11章所述）。当你对一个句子中的单词取平均曲线时，句子的最主导意义是真实的，而任何特定单词产生的噪声则被平均掉。
- en: How does a neural network use embeddings?
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络如何使用嵌入？
- en: Neural networks detect the curves that have correlation with a target label
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络检测与目标标签相关的曲线
- en: 'You’ve learned about a new way to view word embeddings as a squiggly line with
    distinctive properties (curves). You’ve also learned that these curves are developed
    throughout the course of training to accomplish the target objective. Words with
    similar meaning in one way or another will often share a distinctive bend in the
    curve: a combination of high-low pattern among the weights. In this section, we’ll
    consider how the correlation summarization processes these curves as input. What
    does it mean for a layer to consume these curves as input?'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到一种将词嵌入视为具有独特性质（曲线）的波浪线的新方法。你还了解到，这些曲线是在训练过程中逐步发展以实现目标目标的。在某种意义上相似意义的单词通常会共享曲线上的一个独特弯曲：权重中的高低模式组合。在本节中，我们将考虑相关性总结过程如何将这些曲线作为输入进行处理。对于一层来说，将这些曲线作为输入意味着什么呢？
- en: Truth be told, a neural network consumes embeddings just as it consumed the
    streetlight dataset in the book’s early chapters. It looks for correlation between
    the various bumps and curves in the hidden layer and the target label it’s trying
    to predict. This is why words with one particular aspect of similarity share similar
    bumps and curves. At some point during training, a neural network starts developing
    unique characteristics between the shapes of different words so it can tell them
    apart, and grouping them (giving them similar bumps/curves) to help make accurate
    predictions. But this is another way of summarizing the lessons from the end of
    [chapter 11](kindle_split_019.xhtml#ch11). We want to press further.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，神经网络消费嵌入的方式就像它在本书早期章节中消费街灯数据集一样。它寻找隐藏层中各种凹凸和曲线与它试图预测的目标标签之间的相关性。这就是为什么具有特定相似方面的单词会共享相似的凹凸和曲线。在训练过程中某个时刻，神经网络开始发展不同单词形状之间的独特特征，以便将其区分开来，并将它们分组（给予它们相似的凹凸/曲线），以帮助做出准确的预测。但这又是总结第11章末尾教训的另一种方式。我们希望进一步探讨。
- en: 'In this chapter, we’ll consider what it means to sum these embeddings into
    a sentence embedding. What kinds of classifications would this summed vector be
    useful for? We’ve identified that taking an average across the word embeddings
    of a sentence results in a vector with an average of the characteristics of the
    words therein. If there are many positive words, the final embedding will look
    somewhat positive (with other noise from the words generally cancelling out).
    But note that this approach is a bit mushy: given enough words, these different
    wavy lines should all average together to generally be a straight line.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨将这些嵌入求和成一个句子嵌入的含义。这种求和向量对哪些类型的分类会有用？我们已经确定，对句子中所有单词嵌入取平均会得到一个具有句子中所有单词特征平均值的向量。如果有很多积极词汇，最终的嵌入将看起来有些积极（其他单词的噪声通常相互抵消）。但请注意，这种方法有点模糊：给定足够多的单词，这些不同的波浪线都应该平均在一起，通常变成一条直线。
- en: 'This brings us to the first weakness of this approach: when attempting to store
    arbitrarily long sequences (a sentence) of information into a fixed-length vector,
    if you try to store too much, eventually the sentence vector (being an average
    of a multitude of word vectors) will average out to a straight line (a vector
    of near-0s).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了这种方法的第一大弱点：当试图将任意长度的信息序列（一个句子）存储到固定长度的向量中时，如果你试图存储太多，最终句子向量（作为众多单词向量的平均值）将平均成一个直线（接近0的向量）。
- en: In short, this process of storing the information of a sentence doesn’t decay
    nicely. If you try to store too many words into a single vector, you end up storing
    almost nothing. That being said, a sentence is often not that many words; and
    if a sentence has repeating patterns, these sentence vectors can be useful, because
    the sentence vector will retain the most dominant patterns present across the
    word vectors being summed (such as the negative spike in the previous section).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个过程存储句子信息的方式并不优雅。如果你试图将太多单词存储到单个向量中，最终你几乎什么都没有存储。话虽如此，一个句子通常不会有很多单词；如果一个句子有重复的模式，这些句子向量可能是有用的，因为句子向量将保留被求和的单词向量中最占主导地位的图案（例如，前一部分中的负峰值）。
- en: The limitations of bag-of-words vectors
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋向量的局限性
- en: Order becomes irrelevant when you average word embeddings
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当你对单词嵌入取平均时，顺序变得无关紧要
- en: The biggest issue with average embeddings is that they have no concept of order.
    For example, consider the two sentences “Yankees defeat Red Sox” and “Red Sox
    defeat Yankees.” Generating sentence vectors for these two sentences using the
    average approach will yield identical vectors, but the sentences are conveying
    the exact opposite information! Furthermore, this approach ignores grammar and
    syntax, so “Sox Red Yankees defeat” will also yield an identical sentence embedding.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 平均嵌入的最大问题是它们没有顺序的概念。例如，考虑两个句子“Yankees defeat Red Sox”和“Red Sox defeat Yankees”。使用平均方法为这两个句子生成句子向量将得到相同的向量，但这两个句子传达的信息正好相反！此外，这种方法忽略了语法和句法，所以“Red
    Sox Yankees defeat Sox”也会得到相同的句子嵌入。
- en: 'This approach of summing or averaging word embeddings to form the embedding
    for a phrase or sentence is classically known as a *bag-of-words* approach because,
    much like throwing a bunch of words into a bag, order isn’t preserved. The key
    limitation is that you can take any sentence, scramble all the words around, and
    generate a sentence vector, and no matter how you scramble the words, the vector
    will be the same (because addition is associative: *a* + *b* == *b* + *a*).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词嵌入求和或平均以形成短语或句子嵌入的方法在经典上被称为“词袋”方法，因为这与把一堆单词扔进一个袋子类似，顺序没有被保留。关键限制是你可以取任何句子，打乱所有单词的顺序，并生成一个句子向量，无论你如何打乱单词，向量都会相同（因为加法是结合的：*a*
    + *b* == *b* + *a*）。
- en: The real topic of this chapter is generating sentence vectors in a way where
    order *does* matter. We want to create vectors such that scrambling them around
    changes the resulting vector. More important, the *way in which order matters*
    (otherwise known as *the way in which order changes the vector*) should be *learned*.
    In this way, the neural network’s representation of order can be based around
    trying to solve a task in language and, by extension, hopefully capture the essence
    of order in language. I’m using language as an example here, but you can generalize
    these statements to any sequence. Language is just a particularly challenging,
    yet universally known, domain.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的真正主题是以一种顺序**确实**重要的方式生成句子向量。我们希望创建的向量是，打乱它们的顺序会改变结果向量。更重要的是，**顺序重要性的方式**（也称为**顺序改变向量的方式**）应该**被学习**。这样，神经网络对顺序的表示就可以围绕尝试解决语言任务来构建，并且通过扩展，希望捕捉到语言中顺序的本质。我在这里使用语言作为例子，但你可以将这些陈述推广到任何序列。语言只是一个特别具有挑战性但普遍为人所知的领域。
- en: One of the most famous and successful ways of generating vectors for sequences
    (such as a sentence) is a *recurrent neural network* (RNN). In order to show you
    how it works, we’ll start by coming up with a new, and seemingly wasteful, way
    of doing the average word embeddings using something called an *identity matrix*.
    An identity matrix is just an arbitrarily large, square matrix (num rows == num
    columns) of 0s with 1s stretching from the top-left corner to the bottom-right
    corner as in the examples shown here.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 生成序列（如句子）向量的最著名和最成功的方法之一是**循环神经网络**（RNN）。为了向您展示它是如何工作的，我们将首先提出一种新的、看似浪费的方法来使用所谓的**单位矩阵**进行平均词嵌入。单位矩阵只是一个任意大的正方形矩阵（行数等于列数），其中0填充，从左上角到右下角有1。
- en: 'All three of these matrices are identity matrices, and they have one purpose:
    performing vector-matrix multiplication with *any* vector will return the original
    vector. If I multiply the vector `[3,5]` by the top identity matrix, the result
    will be `[3,5]`.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个矩阵都是单位矩阵，它们有一个目的：与**任何**向量进行向量-矩阵乘法都会返回原始向量。如果我将向量 `[3,5]` 乘以顶部的单位矩阵，结果将是
    `[3,5]`。
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using identity vectors to sum word embeddings
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用单位向量求和词嵌入
- en: Let’s implement the same logic using a different approach
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们使用不同的方法实现相同的逻辑
- en: You may think identity matrices are useless. What’s the purpose of a matrix
    that takes a vector and outputs that same vector? In this case, we’ll use it as
    a teaching tool to show how to set up a more complicated way of summing the word
    embeddings so the neural network can take order into account when generating the
    final sentence embedding. Let’s explore another way of summing embeddings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为单位矩阵没有用。一个将向量输入并输出相同向量的矩阵有什么用？在这种情况下，我们将将其用作教学工具，展示如何设置一种更复杂的方法来求和词嵌入，以便神经网络在生成最终的句子嵌入时可以考虑到顺序。让我们探索另一种求和嵌入的方法。
- en: '![](Images/f0216-01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0216-01.jpg)'
- en: 'This is the standard technique for summing multiple word embeddings together
    to form a sentence embedding (dividing by the number of words gives the average
    sentence embedding). The example on the right adds a step *between* each sum:
    vector-matrix multiplication by an identity matrix.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将多个词嵌入相加形成句子嵌入（除以单词数量得到平均句子嵌入）的标准技术。右侧的示例在每次求和之间增加了一步：通过单位矩阵进行向量-矩阵乘法。
- en: The vector for “Red” is multiplied by an identity matrix, and then the output
    is summed with the vector for “Sox,” which is then vector-matrix multiplied by
    the identity matrix and added to the vector for “defeat,” and so on throughout
    the sentence. Note that because the vector-matrix multiplication by the identity
    matrix returns the same vector that goes into it, the process on the right yields
    *exactly the same sentence embedding* as the process at top left.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: “红色”的向量乘以单位矩阵，然后将输出与“索克斯”的向量相加，接着将“索克斯”的向量通过单位矩阵进行向量-矩阵乘法并加到“击败”的向量上，以此类推，直到整个句子。注意，因为通过单位矩阵进行向量-矩阵乘法返回的是相同的向量，所以右侧的过程与左上角的过程产生**完全相同的句子嵌入**。
- en: '![](Images/f0216-02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0216-02.jpg)'
- en: Yes, this is wasteful computation, but that’s about to change. The main thing
    to consider here is that if the matrices used were any matrix other than the identity
    matrix, changing the order of the words would change the resulting embedding.
    Let’s see this in Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这确实是浪费计算，但这种情况即将改变。这里要考虑的主要问题是，如果使用的矩阵不是单位矩阵，那么改变单词的顺序将改变生成的嵌入。让我们用Python来看看这个例子。
- en: Matrices that change absolutely nothing
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全不改变任何东西的矩阵
- en: Let’s create sentence embeddings using identity matrices in Python
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们用Python使用单位矩阵创建句子嵌入
- en: In this section, we’ll demonstrate how to play with identity matrices in Python
    and ultimately implement the new sentence vector technique from the previous section
    (proving that it produces identical sentence embeddings).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何在Python中玩转单位矩阵，并最终实现上一节中提到的新句子向量技术（证明它产生相同的句子嵌入）。
- en: At right, we first initialize four vectors (`a`, `b`, `c`, and `d`) of length
    3 as well as an identity matrix with three rows and three columns (identity matrices
    are always square). Notice that the identity matrix has the characteristic set
    of 1s running diagonally from top-left to bottom-right (which, by the way, is
    called the *diagonal* in linear algebra). Any square matrix with 1s along the
    diagonal and 0s everywhere else is an identity matrix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，我们首先初始化长度为3的四个向量（`a`、`b`、`c`和`d`），以及一个三行三列的单位矩阵（单位矩阵总是方阵）。请注意，单位矩阵具有从左上角到右下角的对角线上的1（顺便说一下，这在线性代数中被称为**对角线**）。任何对角线上有1而其他地方都是0的方阵都是单位矩阵。
- en: '![](Images/f0217-01.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0217-01.jpg)'
- en: We then proceed to perform vector-matrix multiplication with each of the vectors
    and the identity matrix (using NumPy’s dot function). As you can see, the output
    of this process is a new vector identical to the input vector.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后继续对每个向量与单位矩阵进行向量矩阵乘法（使用NumPy的点函数）。如您所见，此过程的输出是一个与输入向量相同的新向量。
- en: 'Because vector-matrix multiplication by an identity matrix returns the same
    vector we started with, incorporating this process into the sentence embedding
    should seem trivial, and it is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单位矩阵与向量的矩阵乘法返回相同的向量，将此过程纳入句子嵌入应该看起来很简单，确实如此：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Both ways of creating sentence embeddings generate the same vector. This is
    only because the identity matrix is a very special kind of matrix. But what would
    happen if we didn’t use the identity matrix? What if, instead, we used a different
    matrix? In fact, the identity matrix is the *only* matrix guaranteed to return
    the same vector that it’s vector-matrix multiplied with. No other matrix has this
    guarantee.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 两种创建句子嵌入的方法都会生成相同的向量。这仅仅是因为单位矩阵是一种非常特殊的矩阵。但如果我们不使用单位矩阵会怎样呢？如果我们使用不同的矩阵会怎样呢？实际上，单位矩阵是**唯一**保证返回与它进行向量矩阵乘法相同的向量的矩阵。没有其他矩阵有这样的保证。
- en: Learning the transition matrices
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习转换矩阵
- en: What if you allowed the identity matrices to change to minimize the loss?
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果允许单位矩阵改变以最小化损失会怎样？
- en: 'Before we begin, let’s remember the goal: generating sentence embeddings that
    cluster according to the meaning of the sentence, such that given a sentence,
    we can use the vector to find sentences with a similar meaning. More specifically,
    these sentence embeddings should care about the order of words.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们记住目标：生成根据句子意义聚类的句子嵌入，这样给定一个句子，我们可以使用向量找到具有相似意义的句子。更具体地说，这些句子嵌入应该关注单词的顺序。
- en: Previously, we tried summing word embeddings. But this meant “Red Sox defeat
    Yankees” had an identical vector to the sentence “Yankees defeat Red Sox,” despite
    the fact that these two sentences have opposite meanings. Instead, we want to
    form sentence embeddings where these two sentences generate *different* embeddings
    (yet still cluster in a meaningful way). The theory is that if we use the identity-matrix
    way of creating sentence embeddings, but used any other matrix other than the
    identity matrix, the sentence embeddings would be different depending on the order.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们尝试过将词嵌入相加。但这意味着“红袜队击败洋基队”与句子“洋基队击败红袜队”具有相同的向量，尽管这两个句子具有相反的意义。相反，我们希望形成句子嵌入，使得这两个句子生成**不同**的嵌入（但仍以有意义的方式进行聚类）。理论上是这样的，如果我们使用单位矩阵创建句子嵌入的方式，但使用任何除单位矩阵之外的矩阵，句子嵌入将根据顺序的不同而不同。
- en: 'Now the obvious question: what matrix to use instead of the identity matrix.
    There are an infinite number of choices. But in deep learning, the standard answer
    to this kind of question is, “You’ll learn the matrix just like you learn any
    other matrix in a neural network!” OK, so you’ll just learn this matrix. How?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在显然的问题是：用哪个矩阵代替单位矩阵。有无限多的选择。但在深度学习中，这类问题的标准答案是，“你将像学习神经网络中的任何其他矩阵一样学习这个矩阵！”好吧，所以你将只学习这个矩阵。怎么学？
- en: Whenever you want to train a neural network to learn something, you always need
    a task for it to learn. In this case, that task should require it to generate
    interesting sentence embeddings by learning both useful word vectors and useful
    modifications to the identity matrices. What task should you use?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你想要训练一个神经网络学习某样东西时，你总是需要给它一个学习任务。在这种情况下，这个任务应该要求它通过学习有用的词向量和有用的单位矩阵修改来生成有趣的句子嵌入。你应该使用什么任务？
- en: '![](Images/f0218-01.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0218-01.jpg)'
- en: 'The goal was similar when you wanted to generate useful word embeddings (fill
    in the blank). Let’s try to accomplish a very similar task: training a neural
    network to take a list of words and attempt to predict the next word.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要生成有用的词嵌入（填空）时，目标相似。让我们尝试完成一个非常类似的任务：训练一个神经网络，使其能够接受一系列单词并尝试预测下一个单词。
- en: '![](Images/f0218-02_alt.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0218-02_alt.jpg)'
- en: Learning to create useful sentence vectors
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习创建有用的句子向量
- en: Create the sentence vector, make a prediction, and modify the sen- ntence vector
    via its parts
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建句子向量，进行预测，并通过其部分修改句子向量
- en: In this next experiment, I don’t want you to think about the network like previous
    neural networks. Instead, think about creating a sentence embedding, using it
    to predict the next word, and then modifying the respective parts that formed
    the sentence embedding to make this prediction more accurate. Because you’re predicting
    the next word, the sentence embedding will be made from the parts of the sentence
    you’ve seen so far. The neural network will look something like the figure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个下一个实验中，我不想让你像以前那样思考网络。相反，考虑创建一个句子嵌入，使用它来预测下一个单词，然后修改形成句子嵌入的相关部分，以使这个预测更准确。因为你正在预测下一个单词，所以句子嵌入将由你迄今为止看到的句子部分组成。神经网络将类似于图中的样子。
- en: 'It’s composed of two steps: create the sentence embedding, and then use that
    embedding to predict which word comes next. The input to this network is the text
    “Red Sox defeat,” and the word to be predicted is “Yankees.”'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 它由两个步骤组成：创建句子嵌入，然后使用该嵌入来预测下一个单词。这个网络的输入是文本“Red Sox defeat”，要预测的单词是“Yankees”。
- en: '![](Images/f0219-01.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0219-01.jpg)'
- en: I’ve written *Identity matrix* in the boxes between the word vectors. This matrix
    will only *start* as an identity matrix. During training, you’ll backpropagate
    gradients into these matrices and update them to help the network make better
    predictions (just as for the rest of the weights in the network).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我在词向量之间的框中写下了**单位矩阵**。这个矩阵最初将只是一个单位矩阵。在训练过程中，你将反向传播梯度到这些矩阵中，并更新它们以帮助网络做出更好的预测（就像网络中其余的权重一样）。
- en: This way, the network will *learn how to incorporate more information than just
    a sum of word embeddings*. By allowing the (initially, identity) matrices to change
    (and become *not* identity matrices), you let the neural network learn how to
    create embeddings where the order in which the words are presented changes the
    sentence embedding. But this change isn’t arbitrary. The network will learn how
    to incorporate the order of words in a way that’s *useful for the task of predicting
    the next word*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，网络将学会如何整合比词嵌入总和更多的信息。通过允许（最初是单位矩阵的）矩阵发生变化（并成为**不是**单位矩阵），你让神经网络学会如何创建嵌入，其中单词呈现的顺序会改变句子嵌入。但这种变化不是任意的。网络将学会以对预测下一个单词的任务**有用**的方式整合单词的顺序。
- en: You’ll also constrain the *transition matrices* (the matrices that are originally
    identity matrices) to all be the same matrix. In other words, the matrix from
    “Red” -> “Sox” will be reused to transition from “Sox” -> “defeat.” Whatever logic
    the network learns in one transition will be reused in the next, and only logic
    that’s useful at every predictive step will be allowed to be learned in the network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将**转换矩阵**（最初是单位矩阵的矩阵）约束为都是同一个矩阵。换句话说，从“Red”到“Sox”的矩阵将被重新用于从“Sox”到“defeat”的转换。网络在一个转换中学到的任何逻辑都将被用于下一个转换，并且只有对每个预测步骤有用的逻辑才允许在网络中学习。
- en: Forward propagation in Python
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python中的正向传播
- en: Let’s take this idea and see how to perform a simple forward propagation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们看看如何执行简单的正向传播
- en: 'Now that you have the conceptual idea of what you’re trying to build, let’s
    check out a toy version in Python. First, let’s set up the weights (I’m using
    a limited vocab of nine words):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对想要构建的概念有了概念上的理解，让我们来看看Python中的玩具版本。首先，让我们设置权重（我使用了一个包含九个单词的有限词汇量）：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* Word embeddings**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 词嵌入**'
- en: '***2* Sentence embedding to output classification weights**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 句子嵌入以输出分类权重**'
- en: '***3* Transition weights**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 转换权重**'
- en: 'This code creates three sets of weights. It creates a Python dictionary of
    word embeddings, the identity matrix (transition matrix), and a classification
    layer. This classification layer `sent2output` is a weight matrix to predict the
    next word given a sentence vector of length 3\. With these tools, forward propagation
    is trivial. Here’s how forward propagation works with the sentence “red sox defeat”
    -> “yankees”:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了三组权重。它创建了一个包含词嵌入、单位矩阵（转换矩阵）和分类层的Python字典。这个分类层`sent2output`是一个权重矩阵，用于根据长度为3的句子向量预测下一个单词。有了这些工具，正向传播变得非常简单。这是如何使用句子“red
    sox defeat” -> “yankees”进行正向传播的：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1* Creates a sentence embedding**'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建句子嵌入**'
- en: '***2* Predicts over all vocabulary**'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在整个词汇表上预测**'
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How do you backpropagate into this?
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你如何进行反向传播？
- en: It might seem trickier, but they’re the same steps you already learned
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这可能看起来有点复杂，但它们是你已经学过的相同步骤
- en: 'You just saw how to perform forward prediction for this network. At first,
    it might not be clear how backpropagation can be performed. But it’s simple. Perhaps
    this is what you see:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚看到了如何进行这个网络的正向预测。一开始，可能不清楚如何进行反向传播。但它很简单。也许你会看到：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1* Normal neural network ([chapters 1](kindle_split_009.xhtml#ch01)–[5](kindle_split_013.xhtml#ch05))**'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 正常神经网络 ([第1章](kindle_split_009.xhtml#ch01)–[第5章](kindle_split_013.xhtml#ch05))**'
- en: '***2* Some sort of strange additional piece**'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 一些奇怪的附加部分**'
- en: '***3* Normal neural network again ([chapter 9](kindle_split_017.xhtml#ch09)
    stuff)**'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 再次是正常神经网络 ([第9章](kindle_split_017.xhtml#ch09)的内容)**'
- en: Based on previous chapters, you should feel comfortable with computing a loss
    and backpropagating until you get to the gradients at `layer_2`, called `layer_2_delta`.
    At this point, you might be wondering, “Which direction do I backprop in?” Gradients
    could go back to `layer_1` by going backward through the `identity` matrix multiplication,
    or they could go into `word_vects['defeat']`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的章节，你应该对计算损失和反向传播直到得到`layer_2`的梯度，即`layer_2_delta`感到舒适。在这个时候，你可能想知道，“我应该向哪个方向反向传播？”梯度可以通过通过`identity`矩阵乘法向后传播回到`layer_1`，或者它们可以进入`word_vects['defeat']`。
- en: 'When you add two vectors together during forward propagation, you backpropagate
    the same gradient into *both* sides of the addition. When you generate `layer_2_delta`,
    you’ll backpropagate it twice: once across the identity matrix to create `layer_1_delta`,
    and again to `word_vects[''defeat'']`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在正向传播过程中将两个向量相加时，你将相同的梯度反向传播到加法运算的**两边**。当你生成`layer_2_delta`时，你将反向传播两次：一次通过单位矩阵创建`layer_1_delta`，然后再次传播到`word_vects['defeat']`：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Targets the one-hot vector for “yankees”**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 针对的是“yankees”的单热向量**'
- en: '***2* Can ignore the “1” as in [chapter 11](kindle_split_019.xhtml#ch11)**'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 可以忽略[第11章](kindle_split_019.xhtml#ch11)中的“1”**'
- en: '***3* Again, can ignore the “1”**'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 再次，可以忽略“1”**'
- en: Let’s train it!
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们开始训练它！
- en: You have all the tools; let’s train the network on a toy corpus
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你已经拥有了所有工具；让我们在玩具语料库上训练网络
- en: 'So that you can get an intuition for what’s going on, let’s first train the
    new network on a toy task called the Babi dataset. This dataset is a synthetically
    generated question-answer corpus to teach machines how to answer simple questions
    about an environment. You aren’t using it for QA (yet), but the simplicity of
    the task will help you better see the impact made by learning the identity matrix.
    First, download the Babi dataset. Here are the bash commands:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你对正在发生的事情有一个直观的了解，让我们首先在一个称为Babi数据集的玩具任务上训练新的网络。这个数据集是一个合成的问答语料库，用于教机器如何回答关于环境的简单问题。你现在还没有用它来进行问答（还没有），但任务的简单性将帮助你更好地看到学习单位矩阵带来的影响。首先，下载Babi数据集。以下是bash命令：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With some simple Python, you can open and clean a small dataset to train the
    network:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些简单的Python，你可以打开并清理一个小数据集来训练网络：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, this dataset contains a variety of simple statements and questions
    (with punctuation removed). Each question is followed by the correct answer. When
    used in the context of QA, a neural network reads the statements in order and
    answers questions (either correctly or incorrectly) based on information in the
    recently read statements.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个数据集包含各种简单的陈述和问题（已移除标点符号）。每个问题后面都跟着正确的答案。在问答（QA）的上下文中，神经网络按顺序读取陈述并基于最近读取的陈述中的信息回答问题（要么正确要么错误）。
- en: For now, you’ll train the network to attempt to finish each sentence when given
    one or more starting words. Along the way, you’ll see the importance of allowing
    the recurrent matrix (previously the identity matrix) to learn.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你将训练网络尝试在给定一个或多个起始单词的情况下完成每个句子。在这个过程中，你会看到允许循环矩阵（之前是单位矩阵）学习的重要性。
- en: Setting things up
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置环境
- en: Before you can create matrices, you need to learn how many parameters you have
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在你能够创建矩阵之前，你需要了解你有多少个参数
- en: 'As with the word embedding neural network, you first need to create a few useful
    counts, lists, and utility functions to use during the predict, compare, learn
    process. These utility functions and objects are shown here and should look familiar:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与词嵌入神经网络一样，你首先需要创建一些有用的计数、列表和实用函数，以便在预测、比较、学习过程中使用。这些实用函数和对象在此处显示，应该看起来很熟悉：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At left, you create a simple list of the vocabulary words as well as a lookup
    dictionary allowing you to go back and forth between a word’s text and its index.
    You’ll use its index in the vocabulary list to pick which row and column of the
    embedding and prediction matrices correspond to which word. At right is a utility
    function for converting a list of words to a list of indices, as well as the function
    for `softmax`, which you’ll use to predict the next word.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，你创建了一个简单的词汇表单词列表以及一个查找字典，允许你在单词的文本和其索引之间来回转换。你将使用词汇表列表中的索引来选择嵌入和预测矩阵的哪一行和哪一列对应于哪个单词。在右侧是一个将单词列表转换为索引列表的实用函数，以及用于`softmax`的函数，你将使用它来预测下一个单词。
- en: 'The following code initializes the random seed (to get consistent results)
    and then sets the embedding size to 10\. You create a matrix of word embeddings,
    recurrent embeddings, and an initial `start` embedding. This is the embedding
    modeling an empty phrase, which is key to the network modeling how sentences tend
    to start. Finally, there’s a `decoder` weight matrix (just like from embeddings)
    and a `one_hot` utility matrix:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码初始化了随机种子（以获得一致的结果），然后将嵌入大小设置为10。你创建了一个词嵌入矩阵、循环嵌入矩阵以及一个初始的`start`嵌入。这是表示空短语的嵌入，这对于网络模拟句子倾向于如何开始至关重要。最后，还有一个`decoder`权重矩阵（就像嵌入一样）和一个`one_hot`实用矩阵：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* Word embeddings**'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* Word embeddings**'
- en: '***2* Embedding -> embedding (initially the identity matrix)**'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* Embedding -> embedding (initially the identity matrix)**'
- en: '***3* Sentence embedding for an empty sentence**'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* Sentence embedding for an empty sentence**'
- en: '***4* Embedding -> output weights**'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* Embedding -> output weights**'
- en: '***5* One-hot lookups (for the loss function)**'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* One-hot lookups (for the loss function)**'
- en: Forward propagation with arbitrary length
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播任意长度
- en: You’ll forward propagate using the same logic described earlier
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你将使用前面描述的相同逻辑进行前向传播
- en: The following code contains the logic to forward propagate and predict the next
    word. Note that although the construction might feel unfamiliar, it follows the
    same procedure as before for summing embeddings while using the identity matrix.
    Here, the identity matrix is replaced with a matrix called `recurrent`, which
    is initialized to be all 0s (and will be learned through training).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码包含前向传播和预测下一个单词的逻辑。请注意，尽管构建可能感觉不熟悉，但它遵循与之前相同的程序，在求和嵌入的同时使用单位矩阵。在这里，单位矩阵被替换为一个名为`recurrent`的矩阵，它被初始化为全0（并且将通过训练来学习）。
- en: Furthermore, instead of predicting only at the last word, you make a prediction
    (`layer['pred']`) at every timestep, based on the embedding generated by the previous
    words. This is more efficient than doing a new forward propagation from the beginning
    of the phrase each time you want to predict a new term.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你不仅预测最后一个单词，而且在每个时间步长都基于前一个单词生成的嵌入进行预测（`layer['pred']`）。这比每次想要预测新词时都从短语的开头进行新的前向传播更有效。
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* Forward propagates**'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* Forward propagates**'
- en: '***2* Tries to predict the next term**'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* Tries to predict the next term**'
- en: '***3* Generates the next hidden state**'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* Generates the next hidden state**'
- en: There’s nothing particularly new about this bit of code relative to what you’ve
    learned in the past, but there’s a particular piece I want to make sure you’re
    familiar with before we move forward. The list called `layers` is a new way to
    forward propagate.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与你过去学到的内容相比，这段代码并没有什么特别之处，但有一个特定的部分我想确保你在我们继续前进之前熟悉。名为 `layers` 的列表是一种新的正向传播方式。
- en: Notice that you end up doing more forward propagations if the length of `sent`
    is larger. As a result, you can’t use static layer variables as before. This time,
    you need to keep appending new layers to the list based on the required number.
    Be sure you’re comfortable with what’s going on in each part of this list, because
    if it’s unfamiliar to you in the forward propagation pass, it will be very difficult
    to know what’s going on during the backpropagation and weight update steps.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果 `sent` 的长度较大，你最终会进行更多的正向传播。因此，你不能再像以前那样使用静态层变量。这次，你需要根据所需数量不断向列表中添加新层。确保你对这个列表的每一部分都感到舒适，因为如果在正向传播过程中你不熟悉它，那么在反向传播和权重更新步骤中了解情况将会非常困难。
- en: Backpropagation with arbitrary length
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机长度反向传播
- en: You’ll backpropagate using the same logic described earlier
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你将使用前面描述的相同逻辑进行反向传播
- en: As described with the “Red Sox defeat Yankees” example, let’s implement backpropagation
    over arbitrary-length sequences, assuming you have access to the forward propagation
    objects returned from the function in the previous section. The most important
    object is the `layers` list, which has two vectors (`layer['state']` and `layer['previous->hidden']`).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如“红袜队击败洋基队”示例中所述，让我们实现任意长度序列的反向传播，假设你有权访问上一节中函数返回的前向传播对象。最重要的对象是 `layers` 列表，它包含两个向量（`layer['state']`
    和 `layer['previous->hidden']`）。
- en: In order to backpropagate, you’ll take the output gradient and add a new object
    to each list called `layer['state_delta']`, which will represent the gradient
    at that layer. This corresponds to variables like `sox_delta`, `layer_0_delta`,
    and `defeat_delta` from the “Red Sox defeat Yankees” example. You’re building
    the same logic in a way that it can consume the variable-length sequences from
    the forward propagation logic.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行反向传播，你需要将输出梯度添加到每个列表中一个新的对象，称为 `layer['state_delta']`，它将代表该层的梯度。这对应于“红袜队击败洋基队”示例中的
    `sox_delta`、`layer_0_delta` 和 `defeat_delta` 等变量。你正在以这种方式构建相同的逻辑，使其能够消费正向传播逻辑中的变长序列。
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* Forward**'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 正向**'
- en: '***2* Backpropagates**'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 反向传播**'
- en: '***3* If not the first layer**'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果不是第一层**'
- en: '***4* If the last layer, don’t pull from a later one, because it doesn’t exist**'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果是最后一层，不要从后面拉取，因为它不存在**'
- en: Before moving on to the next section, be sure you can read this code and explain
    it to a friend (or at least to yourself). There are no new concepts in this code,
    but its construction can make it seem a bit foreign at first. Spend some time
    linking what’s written in this code back to each line of the “Red Sox defeat Yankees”
    example, and you should be ready for the next section and updating the weights
    using the gradients you backpropagated.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，确保你能阅读这段代码并向朋友（或者至少是自己）解释它。这段代码中没有新的概念，但它的构建可能一开始会让你觉得有些陌生。花点时间将这段代码中写的内容与“红袜队击败洋基队”示例中的每一行联系起来，你应该为下一节和更新使用反向传播得到的梯度权重做好准备。
- en: Weight update with arbitrary length
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机长度权重更新
- en: You’ll update weights using the same logic described earlier
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你将使用前面描述的相同逻辑来更新权重
- en: As with the forward and backprop logic, this weight update logic isn’t new.
    But I’m presenting it after having explained it so you can focus on the engineering
    complexity, having (hopefully) already grokked (ha!) the theory complexity.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与正向和反向传播逻辑一样，这个权重更新逻辑并不新颖。但我在解释了它之后才提出它，这样你可以专注于工程复杂性，希望你已经（可能）已经理解了理论复杂性。
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Forward**'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 正向**'
- en: '***2* Backpropagates**'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 反向传播**'
- en: '***3* If the last layer, don’t pull from a later one, because it doesn’t exist**'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果是最后一层，不要从后面拉取，因为它不存在**'
- en: '***4* Updates weights**'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 更新权重**'
- en: Execution and output analysis
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行和输出分析
- en: You’ll update weights using the same logic described earlier
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你将使用前面描述的相同逻辑来更新权重
- en: 'Now the moment of truth: what happens when you run it? Well, when I run this
    code, I get a relatively steady downtrend in a metric called *perplexity*. Technically,
    perplexity is the probability of the correct label (word), passed through a log
    function, negated, and exponentiated (*e*^*x*).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是真相大白的时候：当你运行它时会发生什么？嗯，当我运行这段代码时，我得到了一个被称为 *困惑度* 的指标的相对稳定的下降趋势。技术上讲，困惑度是通过一个对数函数传递的正确标签（单词）的概率，取反，然后指数化（*e*^*x*）。
- en: But what it represents theoretically is the difference between two probability
    distributions. In this case, the perfect probability distribution would be 100%
    probability allocated to the correct term and 0% everywhere else.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 但从理论上讲，它代表了两个概率分布之间的差异。在这种情况下，完美的概率分布将是100%的概率分配给正确的术语，而其他地方都是0%。
- en: Perplexity is high when two probability distributions don’t match, and it’s
    low (approaching 1) when they do match. Thus, a decreasing perplexity, like all
    loss functions used with stochastic gradient descent, is a good thing! It means
    the network is learning to predict probabilities that match the data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个概率分布不匹配时，困惑度会很高，而当它们匹配时，困惑度会很低（接近1）。因此，像所有与随机梯度下降一起使用的损失函数一样，困惑度的下降是一个好现象！这意味着网络正在学习预测与数据匹配的概率。
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'But this hardly tells you what’s going on in the weights. Perplexity has faced
    some criticism over the years (particularly in the language-modeling community)
    for being overused as a metric. Let’s look a little more closely at the predictions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 但这几乎不能告诉你权重中发生了什么。困惑度多年来一直受到一些批评（尤其是在语言建模社区中），因为它被过度用作一个指标。让我们更仔细地看看预测：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code takes a sentence and predicts the word the model thinks is most likely.
    This is useful because it gives a sense for the kinds of characteristics the model
    takes on. What kinds of things does it get right? What kinds of mistakes does
    it make? You’ll see in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码接受一个句子，并预测模型认为最可能的单词。这很有用，因为它可以让你对模型所具有的特征有一个感觉。它做对了哪些事情？它犯了哪些错误？你将在下一节中看到。
- en: Looking at predictions can help you understand what’s going on
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查看预测可以帮助你了解正在发生的事情
- en: 'You can look at the output predictions of the neural network as it trains to
    learn not only what kinds of patterns it picks up, but also the order in which
    it learns them. After 100 training steps, the output looks like this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看神经网络在训练过程中学习时的输出预测，这不仅可以帮助你了解它所选择的模式类型，还可以了解它学习这些模式的顺序。经过100个训练步骤后，输出看起来是这样的：
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Neural networks tend to start off random. In this case, the neural network
    is likely only biased toward whatever words it started with in its first random
    state. Let’s keep training:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络往往是从随机开始的。在这种情况下，神经网络可能只偏向于它在第一次随机状态中开始的任何单词。让我们继续训练：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After 10,000 training steps, the neural network picks out the most common word
    (“the”) and predicts it at every timestep. This is an extremely common error in
    recurrent neural networks. It takes lots of training to learn finer-grained detail
    in a highly skewed dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10,000个训练步骤后，神经网络挑选出最常见的单词（“the”）并在每个时间步预测它。这是循环神经网络中一个非常常见的错误。在高度倾斜的数据集中学习更精细的细节需要大量的训练。
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These mistakes are really interesting. After seeing only the word “sandra,”
    the network predicts “is,” which, although not exactly the same as “moved,” isn’t
    a bad guess. It picked the wrong verb. Next, notice that the words “to” and “the”
    were correct, which isn’t as surprising because these are some of the more common
    words in the dataset, and presumably the network has been trained to predict the
    phrase “to the” after the verb “moved” many times. The final mistake is also compelling,
    mistaking “bedroom” for the word “garden.”
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误真的很有趣。在只看到单词“sandra”之后，网络预测了“is”，虽然并不完全等同于“moved”，但也不是一个糟糕的猜测。它选择了错误的动词。接下来，注意“to”和“the”这两个词是正确的，这并不令人惊讶，因为这些是数据集中的一些更常见的单词，并且据推测，网络已经被训练了许多次来预测在动词“moved”之后的短语“to
    the”。最后的错误也很引人注目，将“bedroom”误认为是单词“garden”。
- en: It’s important to note that there’s almost no way this neural network could
    learn this task perfectly. After all, if I gave you the words “sandra moved to
    the,” could you tell me the correct next word? More context is needed to solve
    this task, but the fact that it’s unsolvable, in my opinion, creates educational
    analysis for the ways in which it fails.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，几乎没有任何方法可以让这个神经网络完美地学习这个任务。毕竟，如果我只给你单词“sandra moved to the”，你能告诉我正确的下一个单词吗？需要更多的上下文来解决这个任务，但在我看来，这个任务无法解决，这实际上为分析它失败的方式提供了教育意义。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Recurrent neural networks predict over arbitrary-length sequences
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环神经网络预测任意长度的序列
- en: In this chapter, you learned how to create vector representations for arbitrary-length
    sequences. The last exercise trained a linear recurrent neural network to predict
    the next term given a previous phrase of terms. To do this, it needed to learn
    how to create embeddings that accurately represented variable-length strings of
    terms into a fixed-size vector.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何为任意长度的序列创建向量表示。最后一个练习训练了一个线性循环神经网络，根据之前的一串术语预测下一个术语。为此，它需要学习如何创建能够将可变长度的术语字符串准确表示为固定大小向量的嵌入。
- en: 'This last sentence should drive home a question: how does a neural network
    fit a variable amount of information into a fixed-size box? The truth is, sentence
    vectors don’t encode everything in the sentence. The name of the game in recurrent
    neural networks is not just what these vectors remember, but also what they forget.
    In the case of predicting the next word, most RNNs learn that only the last couple
    of words are really necessary,^([[*](#ch12fn01)]) and they learn to forget (aka,
    not make unique patterns in their vectors for) words further back in the history.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一句话应该引发一个问题：神经网络如何将可变数量的信息拟合到固定大小的盒子中？事实是，句子向量并没有编码句子中的所有内容。循环神经网络的游戏规则不仅仅是这些向量记得什么，还包括它们忘记了什么。在预测下一个单词的情况下，大多数RNN学习到只有最后几个单词真正必要，^([[*](#ch12fn01)])并且它们学会了忘记（即在它们的向量中不形成独特模式）历史中更早的单词。
- en: ^*
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^*
- en: ''
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See, for example, “Frustratingly Short Attention Spans in Neural Language Modeling”
    by Michał Daniluk et al. (paper presented at ICLR 2017), [https://arxiv.org/abs/1702.04521](https://arxiv.org/abs/1702.04521).
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，参见Michał Daniluk等人撰写的“Frustratingly Short Attention Spans in Neural Language
    Modeling”（在2017年ICLR会议上发表的论文），[https://arxiv.org/abs/1702.04521](https://arxiv.org/abs/1702.04521)。
- en: But note that there are no nonlinearities in the generation of these representations.
    What kinds of limitations do you think that could create? In the next chapter,
    we’ll explore this question and more using nonlinearities and gates to form a
    neural network called a *long short-term memory network* (LSTM). But first, make
    sure you can sit down and (from memory) code a working linear RNN that converges.
    The dynamics and control flow of these networks can be a bit daunting, and the
    complexity is about to jump by quite a bit. Before moving on, become comfortable
    with what you’ve learned in this chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，这些表示的生成过程中没有非线性。你认为这会带来什么样的局限性？在下一章中，我们将使用非线性性和门来形成一个称为**长短期记忆网络**（LSTM）的神经网络，来探讨这个问题以及其他问题。但首先，确保你能坐下来（从记忆中）编写一个能够收敛的工作线性RNN。这些网络的动力和控制流可能有点令人畏惧，复杂性即将大幅增加。在继续之前，熟悉一下本章所学的内容。
- en: And with that, let’s dive into LSTMs!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，让我们深入探讨LSTMs！
