- en: 2 TensorFlow essentials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 TensorFlow 的基本要素
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the TensorFlow workflow
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 的工作流程
- en: Creating interactive notebooks with Jupyter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jupyter 创建交互式笔记本
- en: Visualizing algorithms by using TensorBoard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 可视化算法
- en: Before implementing machine-learning algorithms, let’s get familiar with how
    to use TensorFlow. You’re going to get your hands dirty writing simple code right
    away! This chapter covers some essential advantages of TensorFlow to convince
    you that it’s the machine-learning library of choice. Before you continue, follow
    the procedures in the appendix for step-by-step installation instructions, then
    return here.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现机器学习算法之前，让我们熟悉如何使用 TensorFlow。你将立即动手编写简单的代码！本章将介绍 TensorFlow 的一些基本优势，以说服你它是最受欢迎的机器学习库。在你继续之前，请按照附录中的步骤进行安装说明，然后返回此处。
- en: As a thought experiment, let’s see what happens when we use Python code without
    a handy computing library. It’ll be like using a new smartphone without installing
    any additional apps. The functionality will be there, but you’d be much more productive
    if you had the right tools.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种思想实验，让我们看看当我们不使用便捷的计算库时使用 Python 代码会发生什么。这就像使用一部没有安装任何额外应用程序的新智能手机一样。功能是有的，但如果你有了正确的工具，你会更加高效。
- en: Suppose you’re a private business owner tracking the flow of sales for your
    products. Your inventory consists of 100 items, and you represent each item’s
    price in a vector called `prices`. Another 100-dimensional vector called `amounts`
    represents the inventory count of each item. You can write the chunk of Python
    code shown in listing 2.1 to calculate the revenue of selling all products. Keep
    in mind that this code doesn’t import any libraries.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一位私人企业主，正在跟踪你产品的销售流向。你的库存由 100 件商品组成，你将每件商品的价格表示为一个名为 `prices` 的向量。另一个 100
    维向量 `amounts` 表示每种商品的库存数量。你可以编写如列表 2.1 所示的 Python 代码块来计算所有产品的收入。请注意，此代码没有导入任何库。
- en: Listing 2.1 Computing the inner product of two vectors without using a library
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 不使用库计算两个向量的内积
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That’s too much code to calculate the inner product of two vectors (also known
    as the *dot product*). Imagine how much code would be required for something more
    complicated, such as solving linear equations or computing the distance between
    two vectors, if you still lacked TensorFlow and its friends, like the Numerical
    Python (NumPy) library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '计算两个向量的内积（也称为点积）的代码已经很多了。想象一下，如果你仍然缺乏 TensorFlow 及其朋友（如数值 Python（NumPy）库），解决线性方程或计算两个向量之间距离的代码将需要多少。 '
- en: When installing the TensorFlow library, you also install a well-known, robust
    Python library called NumPy, which facilitates mathematical manipulation in Python.
    Using Python without its libraries (NumPy and TensorFlow) is like using a camera
    without an autofocus mode; sure, you gain more flexibility, but you can easily
    make careless mistakes. (For the record, we have nothing against photographers
    who micromanage aperture, shutter, and ISO—the so-called “manual” knobs used to
    prepare your camera to take an image.) It’s easy to make mistakes in machine learning,
    so let’s keep our camera on autofocus and use TensorFlow to help automate tedious
    software development.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 TensorFlow 库时，你还会安装一个知名且稳健的 Python 库，名为 NumPy，它简化了 Python 中的数学操作。在不使用其库（NumPy
    和 TensorFlow）的情况下使用 Python，就像使用没有自动对焦模式的相机一样；当然，你获得了更多的灵活性，但你很容易犯粗心大意的错误。（记录在案，我们并不反对那些精心管理光圈、快门和
    ISO（用于准备相机拍摄图像的所谓“手动”旋钮）的摄影师。）在机器学习中很容易犯错误，所以让我们保持相机在自动对焦模式下，并使用 TensorFlow 来帮助自动化繁琐的软件开发。
- en: 'The following code snippet shows how to write the same inner product concisely
    using NumPy:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何使用 NumPy 简洁地编写相同的内积：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Python is a succinct language. Fortunately for you, this book doesn’t have pages
    and pages of cryptic code. On the other hand, the brevity of the Python language
    also implies that a lot is happening behind each line of code, which you should
    study carefully as you follow along in this chapter. You will find that this is
    a core theme for TensorFlow, something that it balances elegantly as an add-on
    library to Python. TensorFlow hides enough of the complexity (like autofocus)
    but also allows you to turn those magical configurable knobs when you want to
    get your hands dirty.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Python是一种简洁的语言。幸运的是，这本书没有充斥着难以理解的代码。另一方面，Python语言的简洁性也意味着每一行代码背后都发生了很多事情，你在跟随本章学习时应该仔细研究。你会发现，这将是TensorFlow的核心主题，它是作为Python的附加库优雅地平衡这一点的。TensorFlow隐藏了足够的复杂性（比如自动对焦），但同时也允许你在想要深入时调整那些神奇的配置旋钮。
- en: Machine-learning algorithms require many mathematical operations. Often, an
    algorithm boils down to a composition of simple functions iterated until convergence.
    Sure, you may use any standard programming language to perform these computations,
    but the secret to both manageable and high-performing code is the use of a well-written
    library, such as TensorFlow (which officially supports Python, C++, JavaScript,
    Go, and Swift).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法需要许多数学运算。通常，一个算法可以归结为一系列简单函数的迭代，直到收敛。当然，你可以使用任何标准的编程语言来执行这些计算，但既可管理又高性能的代码的秘密在于使用一个编写良好的库，比如TensorFlow（它官方支持Python、C++、JavaScript、Go和Swift）。
- en: TIP Detailed documentation about various functions for the APIs is available
    at [https://www.tensorflow.org/api_docs](https://www.tensorflow.org/api_docs/).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 关于API的各种函数的详细文档可在[https://www.tensorflow.org/api_docs](https://www.tensorflow.org/api_docs/)找到。
- en: The skills you’ll learn in this chapter are geared toward using TensorFlow for
    computations, because machine learning relies on mathematical formulations. After
    going through the examples and code listings, you’ll be able to use TensorFlow
    for arbitrary tasks, such as computing statistics on big data, and to use TensorFlow
    friends like NumPy and Matplotlib (for visualization) to understand why your machine-learning
    algorithms are making those decisions and provide explainable results. The focus
    here is entirely on how to use TensorFlow as opposed to general machine learning,
    which we will get into in later chapters. That sounds like a gentle start, right?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你将学习的技能是针对使用TensorFlow进行计算，因为机器学习依赖于数学公式。在浏览了示例和代码列表之后，你将能够使用TensorFlow执行任意任务，例如在大数据上计算统计数据，并使用TensorFlow的朋友如NumPy和Matplotlib（用于可视化）来理解为什么你的机器学习算法会做出那些决策，并提供可解释的结果。这里的重点完全在于如何使用TensorFlow，而不是通用的机器学习，我们将在后面的章节中涉及。这听起来像是一个温和的开始，对吧？
- en: Later in this chapter, you’ll use flagship TensorFlow features that are essential
    for machine learning. These features include representation of computation as
    a dataflow graph, separation of design and execution, partial subgraph computation,
    and autodifferentiation. Without further ado, let’s write our first TensorFlow
    code!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，你将使用对机器学习至关重要的TensorFlow旗舰功能。这些功能包括将计算表示为数据流图、设计执行分离、部分子图计算和自动微分。无需多言，让我们编写我们的第一个TensorFlow代码！
- en: 2.1 Ensuring that TensorFlow works
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 确保TensorFlow正常运行
- en: First, you should ensure that everything is working correctly. Check the oil
    level in your car, repair the blown fuse in your basement, and ensure that your
    credit balance is zero. I’m kidding; we’re talking about TensorFlow.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你应该确保一切运行正常。检查你的汽车油位，修复地下室烧毁的保险丝，并确保你的信用余额为零。我开玩笑的；我们说的是TensorFlow。
- en: 'Create a new file called test.py for your first piece of code. Import TensorFlow
    by running the following script:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的第一段代码创建一个名为test.py的新文件。通过运行以下脚本导入TensorFlow：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Having technical difficulty?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到技术难题？
- en: An error commonly occurs at this step if you installed the GPU version and the
    library fails to search for CUDA drivers. Remember that if you compiled the library
    with CUDA, you need to update your environment variables with the path to CUDA.
    Check the CUDA instructions on TensorFlow. (See [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)
    for further information.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装了GPU版本且库未能搜索CUDA驱动程序，在这个步骤中通常会出错。记住，如果你用CUDA编译了库，你需要更新你的环境变量以包含CUDA的路径。查看TensorFlow上的CUDA说明。（有关更多信息，请参阅[https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)。）
- en: This single import prepares TensorFlow to do your bidding. If the Python interpreter
    doesn’t complain, you’re ready to start using TensorFlow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单独的导入使TensorFlow准备好为你服务。如果Python解释器没有抱怨，你就准备好开始使用TensorFlow了。
- en: Sticking with TensorFlow conventions
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循TensorFlow约定
- en: The TensorFlow library is usually imported with the `tf` alias. Generally, qualifying
    TensorFlow with `tf` is a good idea because it keeps you consistent with other
    developers and open source TensorFlow projects. You may use another alias (or
    no alias), of course, but then successfully reusing other people’s snippets of
    TensorFlow code in your own projects will be an involved process. The same is
    true of NumPy as `np`, and Matplotlib as `plt`, which you will see used as conventions
    throughout the book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow库通常使用`tf`别名导入。通常，使用`tf`来限定TensorFlow是一个好主意，因为它能让你与其他开发者和开源TensorFlow项目保持一致性。当然，你也可以使用另一个别名（或没有别名），但这样在项目中成功重用其他人的TensorFlow代码片段将是一个复杂的过程。同样，NumPy作为`np`，Matplotlib作为`plt`，你将在整本书中看到它们作为惯例的使用。
- en: 2.2 Representing tensors
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 张量的表示
- en: Now that you know how to import TensorFlow into a Python source file, let’s
    start using it! As discussed in chapter 1, a convenient way to describe an object
    in the real world is to list its properties or features. You can describe a car,
    for example, by its color, model, engine type, mileage, and so on. An ordered
    list of features is called a *feature vector*, and that’s exactly what you’ll
    represent in `TensorFlow` code.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何将TensorFlow导入Python源文件，让我们开始使用它！如第1章所述，描述现实世界中的对象的一个方便方法是列出其属性或特征。例如，你可以通过颜色、型号、引擎类型、里程数等来描述一辆车。特征的一个有序列表称为特征向量，这正是你将在`TensorFlow`代码中表示的内容。
- en: Feature vectors are among the most useful devices in machine learning because
    of their simplicity; they’re lists of numbers. Each data item typically consists
    of a feature vector, and a good dataset has hundreds, if not thousands, of feature
    vectors. No doubt you’ll often deal with more than one vector at a time. A matrix
    concisely represents a list of vectors, in which each column of a matrix is a
    feature vector.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是机器学习中最有用的工具之一，因为它们的简单性；它们是数字的列表。每个数据项通常由一个特征向量组成，一个好的数据集有数百个，甚至数千个特征向量。毫无疑问，你经常会同时处理多个向量。一个矩阵简洁地表示了一组向量，其中矩阵的每一列都是一个特征向量。
- en: The syntax to represent matrices in TensorFlow is a vector of vectors, all of
    the same length. Figure 2.1 is an example of a matrix with two rows and three
    columns, such as [[1, 2, 3], [4, 5, 6]]. Notice that this vector contains two
    elements and that each element corresponds to a row of the matrix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中表示矩阵的语法是一个向量，由长度相同的向量组成。图2.1是一个具有两行三列的矩阵示例，例如[[1, 2, 3], [4, 5,
    6]]。请注意，这个向量包含两个元素，每个元素对应矩阵的一行。
- en: '![CH02_F01_Mattmann2](../Images/CH02_F01_Mattmann2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F01_Mattmann2](../Images/CH02_F01_Mattmann2.png)'
- en: Figure 2.1 The matrix in the bottom half of the diagram is a visualization from
    its compact code notation in the top half of the diagram. This form of notation
    is a common paradigm in scientific computing libraries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 图表下半部分的矩阵是其上半部分紧凑代码表示的视觉化。这种表示法是科学计算库中的一种常见范式。
- en: We access an element in a matrix by specifying its row and column indices. The
    first row and first column indicate the first top-left element, for example. Sometimes,
    it’s convenient to use more than two indices, such as when referencing a pixel
    in a color image not only by its row and column, but also by its red/green/ blue
    channel. A *tensor* is a generalization of a matrix that specifies an element
    by an arbitrary number of indices.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过指定矩阵的行和列索引来访问矩阵中的一个元素。第一行和第一列表示左上角的第一元素，例如。有时，使用超过两个索引会更方便，例如在引用彩色图像中的一个像素时，不仅通过其行和列，还通过其红/绿/蓝通道。张量是矩阵的推广，它通过任意数量的索引来指定一个元素。
- en: Example of a tensor
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 张量示例
- en: Suppose that an elementary school enforces assigned seating for all its students.
    You’re the principal, and you’re terrible with names. Luckily, each classroom
    has a grid of seats, and you can easily nickname a student by their row and column
    index.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一所小学为所有学生安排了固定的座位。你是校长，你记不住名字。幸运的是，每个教室都有一个座位网格，你可以通过学生的行和列索引轻松地给他们起绰号。
- en: 'The school has multiple classrooms, so you can’t simply say, “Good morning
    4,10! Keep up the good work.” You need to also specify the classroom: “Hi, 4,10
    from classroom 2.” Unlike a matrix, which needs only two indices to specify an
    element, the students in this school need three numbers. They’re all part of a
    rank-3 tensor.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 学校有多个教室，所以你不能简单地说，“早上好4,10！继续保持！”你还需要指定教室：“嗨，2号教室的4,10。”与只需要两个索引来指定元素的矩阵不同，这个学校的学生需要三个数字。他们都是三秩张量的一部分。
- en: The syntax for tensors is even more nested vectors. As shown in figure 2.2,
    a 2 × 3 × 2 tensor is [[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]],
    which can be thought of as two matrices, each of size 3 × 2\. Consequently, we
    say that this tensor has a *rank* of 3\. In general, the rank of a tensor is the
    number of indices required to specify an element. Machine-learning algorithms
    in TensorFlow act on tensors, so it’s important to understand how to use them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的语法甚至更嵌套的向量。如图2.2所示，一个2×3×2的张量是 [[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10],
    [11, 12]]]，这可以想象成两个大小为3×2的矩阵。因此，我们说这个张量的*秩*为3。一般来说，张量的秩是指定一个元素所需的索引数。TensorFlow中的机器学习算法作用于张量，因此理解如何使用它们是很重要的。
- en: '![CH02_F02_Mattmann2](../Images/CH02_F02_Mattmann2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F02_Mattmann2](../Images/CH02_F02_Mattmann2.png)'
- en: Figure 2.2 You can think of this tensor as being multiple matrices stacked on
    top of one another. To specify an element, you must indicate the row and column,
    as well as which matrix is being accessed. Therefore, the rank of this tensor
    is 3.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 你可以将这个张量想象成多个矩阵叠加在一起。要指定一个元素，你必须指明行和列，以及访问的是哪个矩阵。因此，这个张量的秩为3。
- en: It’s easy to get lost in the many ways to represent a tensor. Intuitively, three
    lines of code in listing 2.2 are trying to represent the same 2 × 2 matrix. This
    matrix represents two feature vectors of two dimensions each. It could, for example,
    represent two people’s ratings of two movies. Each person, indexed by the row
    of the matrix, assigns a number to describe their review of the movie, indexed
    by the column. Run the code to see how to generate a matrix in TensorFlow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多表示张量的方式中很容易迷失方向。直观地看，列表2.2中的三行代码试图表示同一个2×2矩阵。这个矩阵代表两个各二维的特征向量。例如，它可以代表两个人对两部电影的评价。每个人，通过矩阵的行进行索引，分配一个数字来描述他们对电影的评论，通过列进行索引。运行代码以查看如何在TensorFlow中生成矩阵。
- en: Listing 2.2 Different ways to represent tensors
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 表示张量的不同方式
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ You’ll use NumPy matrices in TensorFlow.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你将在TensorFlow中使用NumPy矩阵。
- en: ❷ Defines a 2 × 2 matrix in three ways
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以三种方式定义一个2×2矩阵
- en: ❸ Prints the type for each matrix
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印每个矩阵的类型
- en: ❹ Creates tensor objects from the various types
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从各种类型创建张量对象
- en: ❺ Notice that the types will be the same now.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 注意现在类型将是相同的。
- en: 'The first variable (`m1`) is a list, the second variable (`m2`) is an `ndarray`
    from the NumPy library, and the last variable (`m3`) is TensorFlow’s constant
    `Tensor` object, which you initialize by using `tf.constant`. None of the three
    ways to specify a matrix is necessarily better than any another, but each way
    does give you a raw set of list values (`m1`), a typed NumPy object (`m2`), or
    an initialized data flow operation: a tensor (`m3`).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变量（`m1`）是一个列表，第二个变量（`m2`）是来自NumPy库的`ndarray`，最后一个变量（`m3`）是TensorFlow的常量`Tensor`对象，你通过使用`tf.constant`来初始化它。指定矩阵的三种方式中，没有一种方式必然比另一种更好，但每种方式都给你一组原始的列表值（`m1`）、一个类型化的NumPy对象（`m2`）或一个初始化的数据流操作：张量（`m3`）。
- en: 'All operators in TensorFlow, such as `negative`, are designed to operate on
    tensor objects. A convenient function you can sprinkle anywhere to make sure that
    you’re dealing with tensors as opposed to the other types is `tf.convert_to_tensor``(...)`.
    Most functions in the TensorFlow library already perform this function (redundantly),
    even if you forget to do so. Using `tf.convert_to_tensor(...)` is optional, but
    we show it here because it helps demystify the implicit type system being handled
    across the library and overall as part of the Python programming language. Listing
    2.3 outputs the following three times:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的所有操作符，如`negative`，都是设计来对张量对象进行操作的。一个方便的函数，你可以在任何地方使用它来确保你正在处理张量而不是其他类型，是`tf.convert_to_tensor``(...)`。TensorFlow库中的大多数函数已经执行了这个功能（冗余），即使你忘记了这样做。使用`tf.convert_to_tensor(...)`是可选的，但我们在这里展示它，因为它有助于阐明库和整个Python编程语言中正在处理的隐式类型系统。列表2.3输出以下三次：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'TIP To make copying and pasting easier, you can find the code listings on the
    book’s GitHub site: [https://github.com/chrismattmann/MLwithTensorFlow2ed](https://github.com/chrismattmann/MLwithTensorFlow2ed/).
    You will also find a fully functional Docker image that you can use with all the
    data, and code and libraries to run the examples in the book. Install it, using
    `docker` `pull` `chrismattmann/mltf2`, and see the appendix for more details.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：为了使复制和粘贴更容易，你可以在本书的 GitHub 网站上找到代码列表：[https://github.com/chrismattmann/MLwithTensorFlow2ed](https://github.com/chrismattmann/MLwithTensorFlow2ed/)。你还可以找到一个完全可用的
    Docker 镜像，你可以使用所有数据、代码和库来运行本书中的示例。使用 `docker pull chrismattmann/mltf2` 安装它，并查看附录以获取更多详细信息。
- en: Let’s take another look at defining tensors in code. After importing the TensorFlow
    library, you can use the `tf.constant` operator as follows. Listing 2.3 shows
    a couple of tensors of various dimensions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看如何在代码中定义张量。在导入 TensorFlow 库之后，你可以使用 `tf.constant` 操作符，如下所示。列表 2.3 展示了一些不同维度的张量。
- en: Listing 2.3 Creating tensors
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 创建张量
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Defines a 2 × 1 matrix of rank 2
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个 2 × 1 的 2 阶矩阵
- en: ❷ Defines a 1 × 2 matrix of rank 2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个 1 × 2 的 2 阶矩阵
- en: ❸ Defines a rank-3 tensor
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个 3 阶张量
- en: ❹ Try printing the tensors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 尝试打印张量。
- en: 'Running listing 2.3 produces the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表 2.3 生成以下输出：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see from the output, each tensor is represented by the aptly named
    `Tensor` object. Each `Tensor` object has a unique label (`name`), a dimension
    (`shape`) to define its structure, and a data type (`dtype`) to specify the kind
    of values you’ll manipulate. Because you didn’t explicitly provide a name, the
    library automatically generated the names: `Const:0`, `Const_1:0`, and `Const_2:0`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个张量都由一个恰如其分的 `Tensor` 对象表示。每个 `Tensor` 对象都有一个唯一的标签（`name`），一个定义其结构的维度（`shape`），以及一个数据类型（`dtype`），用于指定你将操作的价值类型。因为你没有明确提供名称，库自动生成了名称：`Const:0`、`Const_1:0`
    和 `Const_2:0`。
- en: Tensor types
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 张量类型
- en: Notice that each element of `m1` ends with a decimal point. The decimal point
    tells Python that the data type of the elements isn’t an integer, but a float.
    You can pass in explicit `dtype` values. Much like NumPy arrays, tensors take
    on a data type that specifies the kind of values you’ll manipulate in that tensor.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 `m1` 的每个元素都以小数点结尾。小数点告诉 Python，元素的类型不是整数，而是浮点数。你可以传递显式的 `dtype` 值。与 NumPy
    数组类似，张量采用一个数据类型，该数据类型指定了你将在该张量中操作的价值类型。
- en: TensorFlow also comes with a few convenient constructors for some simple tensors.
    The constructor `tf.zeros(shape``)`, for example, creates a tensor with all values
    initialized at `0` of a specific shape, such as `[2, 3]` or `[1, 2]`. Similarly,
    `tf.ones(shape``)` creates a tensor of a specific shape with all values initialized
    to `1` at the same time. The `shape` argument is a one-dimensional (1D) tensor
    of type `int32` (a list of integers) describing the dimensions of the tensor.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 还提供了一些方便的构造函数来创建一些简单的张量。例如，构造函数 `tf.zeros(shape)` 会创建一个所有值初始化为 `0`
    的特定形状的张量，例如 `[2, 3]` 或 `[1, 2]`。同样，`tf.ones(shape)` 会创建一个所有值初始化为 `1` 的特定形状的张量。`shape`
    参数是一个一维（1D）的 `int32` 类型（整数列表）张量，描述了张量的维度。
- en: Exercise 2.1
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2.1
- en: How would you initialize a 500 × 500 tensor with all elements equaling 0.5?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何初始化一个所有元素都等于 0.5 的 500 × 500 张量？
- en: '**Answer**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 2.3 Creating operators
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 创建操作符
- en: Now that you have a few starting tensors ready to be used, you can apply more
    interesting operators, such as addition and multiplication. Consider each row
    of a matrix representing the transaction of money to (positive value) and from
    (negative value) another person. Negating the matrix is a way to represent the
    transaction history of the other person’s flow of money. Let’s start simple and
    run a negation op (short for *operation*) on the `m1` tensor from listing 2.3\.
    Negating a matrix turns the positive numbers into negative numbers of the same
    magnitude, and vice versa.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好了一些起始张量，可以应用更多有趣的操作符，例如加法和乘法。考虑矩阵的每一行代表向另一个人（正数）或从另一个人（负数）转账的交易。取反矩阵是表示另一个人资金流动交易历史的一种方式。让我们从简单的操作开始，对列表
    2.3 中的 `m1` 张量执行取反操作（简称 *operation*）。取反矩阵会将正数转换为相同大小的负数，反之亦然。
- en: Negation is one of the simplest operations. As shown in listing 2.4, negation
    takes only one tensor as input and produces a tensor with every element negated.
    Try running the code. If you master defining negation, you can generalize that
    skill for use in all other TensorFlow operations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 取反是最简单的操作之一。如列表 2.4 所示，取反只接受一个张量作为输入，并产生每个元素都被取反的张量。尝试运行代码。如果你掌握了定义取反的技能，你可以将这项技能推广到所有其他
    TensorFlow 操作中。
- en: note *Defining* an operation, such as negation, is different from *running*
    it. So far, you’ve *defined* how operations should behave. In section 2.4, you’ll
    *evaluate* (or *run*) them to compute their values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: note 定义操作，例如取反，与运行它是不同的。到目前为止，你已经定义了操作应该如何表现。在第 2.4 节中，你将评估（或运行）它们以计算它们的值。
- en: Listing 2.4 Using the negation operator
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 使用取反操作符
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Defines an arbitrary tensor
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个任意张量
- en: ❷ Negates the tensor
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 取反张量
- en: ❸ Prints the object
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印对象
- en: 'Listing 2.4 generates the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 生成以下输出：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that the output isn’t `[[-1,` `-2]]` because you’re printing the definition
    of the negation op, not the actual evaluation of the op. The printed output shows
    that the negation op is a `Tensor` class with a name, shape, and data type. The
    name was assigned automatically, but you could’ve provided it explicitly as well
    when using the `tf.negative` op in listing 2.4\. Similarly, the shape and data
    type were inferred from the `[[1,` `2]]` that you passed in.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出不是 `[[-1, -2]]`，因为你打印的是取反操作的定义，而不是操作的实际评估。打印的输出显示取反操作是一个具有名称、形状和数据类型的 `Tensor`
    类。名称是自动分配的，但你在使用列表 2.4 中的 `tf.negative` 操作时也可以明确提供它。同样，形状和数据类型是从你传递的 `[[1, 2]]`
    推断出来的。
- en: Useful TensorFlow operators
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的 TensorFlow 操作符
- en: 'The official documentation at [https://github.com/tensorflow/docs/tree/r1.15/site/
    en/api_docs/python/tf/math](https://github.com/tensorflow/docs/tree/r1.15/site/en/api_docs/python/tf/math)
    carefully lays out all available math ops. Specific examples of commonly used
    operators include the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://github.com/tensorflow/docs/tree/r1.15/site/en/api_docs/python/tf/math](https://github.com/tensorflow/docs/tree/r1.15/site/en/api_docs/python/tf/math)
    的官方文档中，仔细列出了所有可用的数学操作符。常用操作符的特定示例包括以下内容：
- en: '`tf.add(x,` `y`)—Adds two tensors of the same type, x + y'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.add(x, y)`—将相同类型的两个张量相加，x + y'
- en: '`tf.subtract(x,` `y)`—Subtracts tensors of the same type, x - y'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.subtract(x, y)`—从相同类型的张量中减去，x - y'
- en: '`tf.multiply(x,` `y)`—Multiplies two tensors elementwise'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.multiply(x, y)`—逐元素乘以两个张量'
- en: '`tf.pow(x,` `y)`—Takes the elementwise x to the power of ytf.exp(x)`—Equivalent
    to pow(e, x), where e is Euler’s number (2.718 ...)`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.pow(x, y)`—取元素 x 的 y 次幂`tf.exp(x)`—相当于 pow(e, x)，其中 e 是欧拉数（2.718 ...）`'
- en: '`tf.sqrt(x)`—Equivalent to pow(x, 0.5)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.sqrt(x)`—相当于 pow(x, 0.5)'
- en: '`tf.div(x,` `y)`—Takes the elementwise division of x and y'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.div(x, y)`—取 x 和 y 的逐元素除法'
- en: '`tf.truediv(x,` `y)`—Same as `tf.div`, but casts the arguments as a float'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.truediv(x, y)`—与 `tf.div` 相同，但将参数转换为浮点数'
- en: '`tf.floordiv(x,` `y)`—Same as `truediv`, but rounds down the final answer into
    an integer'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.floordiv(x, y)`—与 `truediv` 相同，但将最终答案向下舍入为整数'
- en: '`tf.mod(x,` `y)`—Takes the elementwise remainder from division'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.mod(x, y)`—取除法的逐元素余数'
- en: Exercise 2.2
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2.2
- en: 'Use the TensorFlow operators you’ve learned so far to produce the Gaussian
    distribution (also known as the normal distribution). See figure 2.3 for a hint.
    For reference, you can find the probability density of the normal distribution
    online: [https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你迄今为止学到的 TensorFlow 操作符来生成高斯分布（也称为正态分布）。参见图 2.3 以获取提示。为了参考，你可以在网上找到正态分布的概率密度：[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)。
- en: '**Answer**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: 'Most mathematical expressions—such as ×, -, +, and so on—are shortcuts for
    their TensorFlow equivalents, used for brevity. The Gaussian function includes
    many operations, so it’s cleaner to use shorthand notations as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数学表达式——如 ×、-、+ 等等——都是其 TensorFlow 等价的快捷方式，用于简洁。高斯函数包含许多操作，因此使用以下简写符号更干净：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 2.4 Executing operators within sessions
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 在会话中执行操作符
- en: A *session* is an environment of a software system that describes how the lines
    of code should run. In TensorFlow, a session sets up how the hardware devices
    (such as CPU and GPU) talk to one another. That way, you can design your machine-learning
    algorithm without worrying about micromanaging the hardware on which it runs.
    Later, you can configure the session to change its behavior without changing a
    line of the machine-learning code.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*会话*是软件系统的一个环境，它描述了代码应该如何运行。在 TensorFlow 中，会话设置硬件设备（如 CPU 和 GPU）之间如何通信。这样，您可以在不担心微管理运行其上的硬件的情况下设计您的机器学习算法。稍后，您可以配置会话以更改其行为，而无需更改机器学习代码中的一行。'
- en: To execute an operation and retrieve its calculated value, TensorFlow requires
    a session. Only a registered session may fill the values of a `Tensor` object.
    To do so, you must create a session class by using `tf.Session``()` and tell it
    to run an operator, as shown in listing 2.5\. The result will be a value you can
    use for further computations later.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行一个操作并检索其计算值，TensorFlow 需要一个会话。只有注册的会话才能填充 `Tensor` 对象的值。为此，您必须使用 `tf.Session()`
    创建会话类，并告诉它运行一个操作，如列表 2.5 所示。结果将是一个您可以用作后续计算的值。
- en: Listing 2.5 Using a session
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 使用会话
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Defines an arbitrary matrix
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个任意矩阵
- en: ❷ Runs the negation operator on it
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在其上运行否定操作
- en: ❸ Starts a session to be able to run operations
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 启动会话以便能够运行操作
- en: ❹ Tells the session to evaluate negMatrix
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 告诉会话评估 negMatrix
- en: ❺ Prints the resulting matrix
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印出结果矩阵
- en: Congratulations! You’ve written your first full TensorFlow code. Although all
    that this code does is negate a matrix to produce `[[-1,` `-2]]`, the core overhead
    and framework are the same as everything else in TensorFlow. A session not only
    configures *where* your code will be computed on your machine, but also crafts
    *how* the computation will be laid out to parallelize computation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经编写了您的第一个完整的 TensorFlow 代码。尽管这段代码所做的只是将矩阵取反以产生 `[[−1,` `−2]]`，但其核心开销和框架与
    TensorFlow 中的其他所有内容相同。会话不仅配置了代码将在您的机器上计算的位置，而且还构建了如何将计算布局以并行化计算。
- en: Code performance seems a bit slow
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 代码性能似乎有点慢
- en: You may have noticed that running your code took a few seconds more than you
    expected. It may appear to be unnatural that TensorFlow takes seconds to negate
    a small matrix. But substantial preprocessing occurs to optimize the library for
    larger, more complicated computations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到运行代码比您预期的多花了几秒钟。TensorFlow 对一个小矩阵取反需要几秒钟看起来可能不太自然。但为了优化库以适应更大、更复杂的计算，实际上进行了大量的预处理。
- en: Every `Tensor` object has an `eval()` function to evaluate the mathematical
    operations that define its value. But the `eval()` function requires defining
    a session object for the library to understand how to best use the underlying
    hardware. In listing 2.5, we used `sess.run``(...)`, which is equivalent to invoking
    the `Tensor`’s `eval()` function in the context of the session.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `Tensor` 对象都有一个 `eval()` 函数来评估定义其值的数学操作。但 `eval()` 函数需要定义一个会话对象，以便库了解如何最好地使用底层硬件。在列表
    2.5 中，我们使用了 `sess.run(...)`，这在会话的上下文中相当于调用 `Tensor` 的 `eval()` 函数。
- en: When you’re running TensorFlow code through an interactive environment (for
    debugging or presentation purposes or when using Jupyter, as described later in
    the chapter), it’s often easier to create the session in interactive mode, in
    which the session is implicitly part of any call to `eval()`. That way, the session
    variable doesn’t need to be passed around throughout the code, making it easier
    to focus on the relevant parts of the algorithm, as shown in listing 2.6.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当您通过交互式环境（用于调试或演示目的或使用 Jupyter，如本章后面所述）运行 TensorFlow 代码时，在交互模式下创建会话通常更容易，在这种情况下，会话隐式地成为对
    `eval()` 的任何调用的部分。这样，会话变量就不需要在代码中传递，这使得更容易关注算法的相关部分，如列表 2.6 所示。
- en: Listing 2.6 Using the interactive session mode
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 使用交互式会话模式
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Starts an interactive session so the sess variable no longer needs to be passed
    around
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启动一个交互式会话，这样 sess 变量就不再需要传递
- en: ❷ Defines an arbitrary matrix and negates it
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个任意矩阵并对其取反
- en: ❸ You can evaluate negMatrix now without explicitly specifying a session.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 现在您可以在不明确指定会话的情况下评估 negMatrix。
- en: ❹ Prints the negated matrix
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出否定后的矩阵
- en: ❺ Remember to close the session to free resources.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 记得关闭会话以释放资源。
- en: 2.5 Understanding code as a graph
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 理解代码作为图
- en: Consider a doctor who predicts the expected weight of a newborn to be 7.5 pounds.
    You’d like to figure out how that prediction differs from the actual measured
    weight. Being an overly analytical engineer, you design a function to describe
    the likelihood of all possible weights of the newborn. A weight of 8 pounds is
    more likely than 10 pounds, for example.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一位医生预测新生儿的预期体重为7.5磅。你可能想弄清楚这个预测与实际测量体重的差异。作为一个过于分析的工程师，你设计了一个函数来描述新生儿所有可能体重的可能性。例如，8磅的体重比10磅更有可能。
- en: You can choose to use the Gaussian (otherwise known as normal) probability distribution
    function. This function takes a number as input and outputs a non-negative number
    describing the probability of observing the input. This function shows up all
    the time in machine learning and is easy to define in TensorFlow. It uses multiplication,
    division, negation, and a couple of other fundamental operators.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择使用高斯（也称为正态）概率分布函数。该函数接受一个数字作为输入，并输出一个非负数，描述观察输入的概率。这个函数在机器学习中经常出现，并且在TensorFlow中很容易定义。它使用乘法、除法、否定以及其他一些基本算子。
- en: Think of every operator as being a node in a graph. Whenever you see a plus
    symbol (+) or any mathematical concept, picture it as one of many nodes. The edges
    between these nodes represent the composition of mathematical functions. Specifically,
    the `negative` operator we’ve been studying is a node, and the incoming/outgoing
    edges of this node are how the `Tensor` transforms. A tensor flows through the
    graph, which is why this library is called TensorFlow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个算子想象成图中的一个节点。每当看到加号（+）或任何数学概念时，想象它是众多节点之一。这些节点之间的边代表数学函数的组合。具体来说，我们一直在研究的`negative`（否定）算子是一个节点，该节点的输入/输出边表示`Tensor`的转换。张量在图中流动，这就是为什么这个库被称为TensorFlow。
- en: 'Here’s a thought: every operator is a strongly typed function that takes input
    tensors of a dimension and produces output of the same dimension. Figure 2.3 is
    an example of how the Gaussian function can be designed with TensorFlow. The function
    is represented as a graph in which operators are nodes and edges represent interactions
    between nodes. This graph as a whole represents a complicated mathematical function
    (specifically, the Gaussian function). Small segments of the graph represent simple
    mathematical concepts, such as negation and doubling.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个想法：每个算子都是一个强类型函数，它接受一个具有特定维度的输入张量，并产生相同维度的输出。图2.3展示了如何使用TensorFlow设计高斯函数。该函数以图的形式表示，其中算子是节点，边表示节点之间的交互。整个图代表一个复杂的数学函数（具体来说，是高斯函数）。图的小部分代表简单的数学概念，例如否定和加倍。
- en: '![CH02_F03_Mattmann2](../Images/CH02_F03_Mattmann2.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_Mattmann2](../Images/CH02_F03_Mattmann2.png)'
- en: Figure 2.3 The graph represents the operations needed to produce a Gaussian
    distribution. The links between the nodes represent how data flows from one operation
    to the next. The operations themselves are simple; the complexity arises from
    the way they intertwine.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 该图表示生成高斯分布所需的操作。节点之间的链接表示数据如何从一个操作流向下一个操作。操作本身很简单；复杂性来自于它们交织的方式。
- en: TensorFlow algorithms are easy to visualize. They can be described simply by
    flowcharts. The technical (and more correct) term for such a flowchart is a *dataflow
    graph*. Every arrow in a dataflow graph is called an *edge*. In addition, every
    state of the dataflow graph is called a *node*. The purpose of the session is
    to interpret your Python code into a dataflow graph and then associate the computation
    of each node of the graph to the CPU or GPU.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow算法易于可视化。它们可以通过流程图简单地描述。这种流程图的术语是*数据流图*。数据流图中的每条箭头称为*边*。此外，数据流图中的每个状态都称为*节点*。会话的目的是将你的Python代码解释为数据流图，并将图中每个节点的计算与CPU或GPU关联起来。
- en: 2.5.1 Setting session configurations
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 设置会话配置
- en: You can also pass options to `tf.Session`. TensorFlow automatically determines
    the best way to assign a GPU or CPU device to an operation, for example, depending
    on what’s available. You can pass an additional option, `log_device_placement=True`,
    when creating a session. Listing 2.7 shows you exactly where on your hardware
    the computations are evoked.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以向`tf.Session`传递选项。TensorFlow会自动确定将GPU或CPU设备分配给操作的最佳方式，例如，根据可用性。在创建会话时，你可以传递一个额外的选项，`log_device_placement=True`。列表2.7显示了计算在硬件上的确切位置。
- en: Listing 2.7 Logging a session
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 记录会话
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Defines a matrix and negates it
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个矩阵并取其相反数
- en: ❷ Starts the session with a special config passed into the constructor to enable
    logging
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用特殊配置启动会话，该配置通过构造函数传入以启用日志记录
- en: ❸ Evaluates negMatrix
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 评估negMatrix
- en: ❹ Prints the resulting value
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出结果值
- en: ❺ Prints the resulting graph
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印出结果图
- en: 'This code outputs info about which CPU/GPU devices are used in the session
    for each operation. Running listing 2.7 results in traces of output like the following
    to show which device was used to run the negation op:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码输出有关每个操作在会话中使用的CPU/GPU设备的信息。运行列表2.7会产生如下输出跟踪，以显示用于运行否定操作的设备：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Sessions are essential in TensorFlow code. You need to call a session to “run”
    the math. Figure 2.4 maps out how the components on TensorFlow interact with the
    machine-learning pipeline. A session not only runs a graph operation, but also
    can take placeholders, variables, and constants as input. We’ve used constants
    so far, but in later sections, we’ll start using variables and placeholders. Here’s
    a quick overview of these three types of values:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow代码中，会话是必不可少的。你需要调用会话来“运行”数学运算。图2.4概述了TensorFlow组件如何与机器学习流程交互。会话不仅运行图操作，还可以接受占位符、变量和常量作为输入。到目前为止，我们已使用常量，但在后面的章节中，我们将开始使用变量和占位符。以下是这三种类型值的快速概述：
- en: '*Placeholder* —A value that’s unassigned but will be initialized by the session
    wherever it’s run. Typically, placeholders are the input and output of your model.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*占位符* —一个未分配但将在会话中初始化的值。通常，占位符是模型的输入和输出。'
- en: '*Variable* —A value that can change, such as parameters of a machine-learning
    model. Variables must be initialized by the session before they’re used.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变量* —一个可以改变的值，例如机器学习模型的参数。变量在使用之前必须由会话初始化。'
- en: '*Constant* —A value that doesn’t change, such as a hyperparameter or setting.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*常量* —一个不会改变的值，例如超参数或设置。'
- en: The entire pipeline for machine learning with TensorFlow follows the flow of
    figure 2.4\. Most of the code in TensorFlow consists of setting up the graph and
    session. After you design a graph and hook up the session to execute it, your
    code is ready to use.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中机器学习的整个流程遵循图2.4的流程。TensorFlow中的大部分代码都是设置图和会话。在你设计好图并将会话连接以执行它之后，你的代码就可以使用了。
- en: '![CH02_F04_Mattmann2](../Images/CH02_F04_Mattmann2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F04_Mattmann2](../Images/CH02_F04_Mattmann2.png)'
- en: Figure 2.4 The session dictates how the hardware will be used to process the
    graph most efficiently. When the session starts, it assigns the CPU and GPU devices
    to each of the nodes. After processing, the session outputs data in a usable format,
    such as a NumPy array. A session optionally may be fed placeholders, variables,
    and constants.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 会话决定了如何最有效地使用硬件来处理图。当会话开始时，它会为每个节点分配CPU和GPU设备。处理完毕后，会话以可用的格式输出数据，例如NumPy数组。会话可以选择性地提供占位符、变量和常量。
- en: 2.6 Writing code in Jupyter
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 在Jupyter中编写代码
- en: Because TensorFlow is primarily a Python library, you should make full use of
    Python’s interpreter. *Jupyter* is a mature environment for exercising the interactive
    nature of the language. It’s a web application that displays computation elegantly
    so that you can share annotated interactive algorithms with others to teach a
    technique or demonstrate code. Jupyter also easily integrates with visualization
    libraries like Python’s Matplotlib and can be used to share elegant data stories
    about your algorithm, to evaluate its accuracy, and to present results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow主要是一个Python库，你应该充分利用Python的解释器。*Jupyter*是一个成熟的交互式环境。它是一个网络应用程序，可以优雅地显示计算结果，这样你就可以与他人分享带有注释的交互式算法，以教授一种技术或演示代码。Jupyter还可以轻松地与可视化库如Python的Matplotlib集成，并可用于分享关于你的算法的优雅数据故事，评估其准确性，并展示结果。
- en: You can share your Jupyter notebooks with other people to exchange ideas, and
    you can download their notebooks to learn about their code. See the appendix to
    get started installing the Jupyter Notebook application.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将你的Jupyter笔记本与他人分享以交换想法，你也可以下载他们的笔记本来了解他们的代码。请参阅附录以开始安装Jupyter Notebook应用程序。
- en: 'From a new terminal, change the directory to the location where you want to
    practice TensorFlow code, and start a notebook server:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个新的终端开始，将目录更改为你想要练习TensorFlow代码的位置，并启动笔记本服务器：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Running this command should launch a new browser window with the Jupyter notebook’s
    dashboard. If no window opens automatically, you can navigate to http://localhost:8888
    from any browser. You’ll see a web page similar to the one in figure 2.5.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令应该会打开一个新浏览器窗口，显示 Jupyter 笔记本的仪表板。如果没有窗口自动打开，你可以从任何浏览器导航到 http://localhost:8888。你将看到一个类似于图
    2.5 的网页。
- en: '![CH02_F05_Mattmann2](../Images/CH02_F05_Mattmann2.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F05_Mattmann2](../Images/CH02_F05_Mattmann2.png)'
- en: Figure 2.5 Running the Jupyter notebook will launch an interactive notebook
    on http://localhost:8888.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 运行 Jupyter 笔记本将在 http://localhost:8888 上启动一个交互式笔记本。
- en: TIP The `jupyter notebook` command didn’t work? Make sure that your `PYTHONPATH`
    environment variable includes the path to the `jupyter` script created when you
    installed the library. Also, this book uses both Python 3.7 (recommended) and
    Python 2.7 examples (due to the BregmanToolkit, which you’ll read about in chapter
    7). For this reason, you will want to install Jupyter with Python kernels enabled.
    For more information, see [https://ipython .readthedocs.io/en/stable/install/kernel_install.html](https://ipython.readthedocs.io/en/stable/install/kernel_install.html).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果 `jupyter notebook` 命令没有工作？请确保你的 `PYTHONPATH` 环境变量包含了安装库时创建的 `jupyter`
    脚本的路径。此外，本书使用了 Python 3.7（推荐）和 Python 2.7 的示例（由于 BregmanToolkit，你将在第 7 章中了解到）。因此，你将需要安装带有
    Python 内核的 Jupyter。更多信息，请参阅 [https://ipython.readthedocs.io/en/stable/install/kernel_install.html](https://ipython.readthedocs.io/en/stable/install/kernel_install.html)。
- en: Create a new notebook by clicking the New drop-down menu at top right; then
    choose Notebooks > Python 3\. The new Python3 kernel is enabled and is the only
    option by default because Python 2 was deprecated as of January 1, 2020\. This
    command creates a new file called Untitled.ipynb, which you can start editing
    immediately through the browser interface. You can change the name of the notebook
    by clicking the current Untitled name and typing something more memorable, such
    as TensorFlow Example Notebook. The convention you use when you look at the listing
    code is simply titling the notebook Listing <Chapter Number>.<Number>.ipynb (for
    example, Listing 2.8.ipynb), but you can pick whatever you want. Organization—who’d
    have thought that was useful?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击右上角的“新建”下拉菜单创建一个新的笔记本；然后选择 Notebooks > Python 3。新的 Python3 内核被启用，并且默认情况下是唯一选项，因为
    Python 2 自 2020 年 1 月 1 日起已弃用。此命令创建了一个名为 Untitled.ipynb 的新文件，你可以通过浏览器界面立即开始编辑。你可以通过点击当前
    Untitled 名称并输入一些更有记忆性的名称来更改笔记本的名称，例如 TensorFlow 示例笔记本。当你查看代码列表时使用的约定是简单地给笔记本命名
    Listing <Chapter Number>.<Number>.ipynb（例如，Listing 2.8.ipynb），但你可以选择任何你想要的名称。组织——谁能想到这会有用呢？
- en: 'Everything in the Jupyter notebook is an independent chunk of code or text
    called a *cell*. Cells help divide a long block of code into manageable pieces
    of code snippets and documentation. You can run cells individually or choose to
    run everything at the same time, in order. There are three common ways to evaluate
    cells:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 笔记本中的每一项都是一个独立的代码块或文本块，称为 *cell*。单元格有助于将长块代码分成可管理的代码片段和文档。你可以单独运行单元格，或者选择按顺序一次性运行所有单元格。有三种常见的评估单元格的方法：
- en: Pressing Shift-Enter in a cell executes the cell and highlights the cell below
    it.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单元格中按下 Shift-Enter 将执行单元格并突出显示其下方的单元格。
- en: Pressing Ctrl-Enter maintains the cursor in the current cell after executing
    it.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按下 Ctrl-Enter 将光标保持在当前单元格执行后。
- en: Pressing Alt-Enter executes the cell and inserts a new empty cell directly below
    it.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按下 Alt-Enter 将执行单元格并在其下方插入一个新空单元格。
- en: You can change the cell type by clicking the drop-down menu in the toolbar,
    as shown in figure 2.6\. Alternatively, you can press Esc to leave edit mode,
    use the arrow keys to highlight a cell, and press Y (for code mode) or M (for
    markdown mode).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击工具栏中的下拉菜单更改单元格类型，如图 2.6 所示。或者，你可以按 Esc 离开编辑模式，使用箭头键突出显示一个单元格，然后按 Y（用于代码模式）或
    M（用于 Markdown 模式）。
- en: '![CH02_F06_Mattmann2](../Images/CH02_F06_Mattmann2.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F06_Mattmann2](../Images/CH02_F06_Mattmann2.png)'
- en: Figure 2.6 The drop-down menu changes the type of cell in the notebook. The
    Code cell is for Python code, whereas the Markdown code is for text descriptions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 下拉菜单更改笔记本中的单元格类型。代码单元格用于 Python 代码，而 Markdown 代码用于文本描述。
- en: Finally, you can create a Jupyter notebook that elegantly demonstrates TensorFlow
    code by interlacing code and text cells as shown in figure 2.7.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以创建一个 Jupyter 笔记本，优雅地展示 TensorFlow 代码，如图 2.7 所示，通过交织代码和文本单元格。
- en: Exercise 2.3
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2.3
- en: If you look closely at figure 2.7, you’ll notice that it uses `tf.neg` instead
    of `tf .negative`. That’s strange. Could you explain why we might have done that?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察图 2.7，你会注意到它使用 `tf.neg` 而不是 `tf .negative`。这很奇怪。你能解释一下我们为什么可能那样做吗？
- en: '**Answer**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: You should be aware that the TensorFlow library changed naming conventions,
    and you may run into these artifacts when following old TensorFlow tutorials online.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该意识到 TensorFlow 库改变了命名约定，你可能会在遵循在线旧 TensorFlow 教程时遇到这些遗留下来的问题。
- en: '![CH02_F07_Mattmann2](../Images/CH02_F07_Mattmann2.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F07_Mattmann2](../Images/CH02_F07_Mattmann2.png)'
- en: Figure 2.7 An interactive Python notebook presents both code and comments grouped
    for readability.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 一个交互式 Python 笔记本展示了为了可读性而分组排列的代码和注释。
- en: One mistake people often make when using Jupyter, though, is relying too much
    on it for some of the more complex machine learning that you can do with TensorFlow.
    Jupyter makes interacting with Python code and TensorFlow a pleasure, but it is
    no substitute for long-running training that you need to code up and then “fire
    and forget” for hours, days, or even weeks. In those situations, we recommend
    taking your notebook; using Jupyter’s `save as Python file` functionality (available
    from the File menu); and then running the saved Python file from the command line,
    using the Python interpreter, in a `tmux` or `screen`. These command-line utilities
    let your current interactive session keep running while you log off and allow
    you to come back and check the status of your command later, placing you back
    in the session as though you never left. These tools are UNIX tools, but through
    Cygwin and virtual machines, they also work in Windows. As you will learn in later
    chapters, especially when performing distributed, multi-GPU training elegantly
    with TensorFlow’s session API, you will be stuck if your code exists only in a
    Jupyter notebook. The notebook environment binds you to a particular run time,
    may be shut down unintentionally (especially on a supercomputer), or may freeze
    or lock up after days, because Jupyter can eat up a bunch of memory if you let
    it run for a while.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人们在使用 Jupyter 时常常犯的一个错误是过度依赖它来进行一些可以使用 TensorFlow 完成的更复杂的机器学习任务。Jupyter 使得与
    Python 代码和 TensorFlow 的交互变得愉快，但它不能替代你需要编写代码并长时间运行（数小时、数天甚至数周）“点火后忘记”的训练。在这些情况下，我们建议将你的笔记本保存；使用
    Jupyter 的“另存为 Python 文件”功能（可在文件菜单中找到）；然后从命令行运行保存的 Python 文件，使用 Python 解释器，在 `tmux`
    或 `screen` 中运行。这些命令行实用程序允许你的当前交互会话在您注销后继续运行，并允许您稍后回来检查命令的状态，仿佛您从未离开过会话。这些工具是 UNIX
    工具，但通过 Cygwin 和虚拟机，它们在 Windows 上也能工作。正如你将在后面的章节中学到的那样，特别是当使用 TensorFlow 的会话 API
    优雅地执行分布式、多 GPU 训练时，如果你的代码仅存在于 Jupyter 笔记本中，你会陷入困境。笔记本环境将你绑定到特定的运行时间，可能无意中关闭（尤其是在超级计算机上），或者可能在使用数天后冻结或锁定，因为如果让
    Jupyter 运行一段时间，它可能会消耗大量内存。
- en: TIP Visit the Juptyer home screen periodically, look for notebooks that are
    running (green) that no longer need to be, select those notebooks, and then click
    the Shutdown button near the top to free memory. Your email, web browsing, and
    other activities will thank you!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 定期访问 Jupyter 的主屏幕，寻找那些正在运行（绿色）但不再需要的笔记本，选择这些笔记本，然后点击顶部附近的关机按钮以释放内存。你的电子邮件、网页浏览和其他活动都会感谢你！
- en: 2.7 Using variables
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 使用变量
- en: Using TensorFlow constants is a good start, but most interesting applications
    require data to change. A neuroscientist may be interested in detecting neural
    activity from sensor measurements, for example. A spike in neural activity could
    be a Boolean variable that changes over time. To capture this activity in TensorFlow,
    you can use the `Variable` class to represent a node whose value changes over
    time.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 常量是一个好的开始，但大多数有趣的应用都需要数据发生变化。例如，神经科学家可能对从传感器测量中检测神经活动感兴趣。神经活动的峰值可能是一个随时间变化的布尔变量。为了在
    TensorFlow 中捕捉这种活动，你可以使用 `Variable` 类来表示一个随时间变化的节点值。
- en: Example of using a variable object in machine learning
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中变量对象的使用示例
- en: Finding the equation of a line that best fits many points is a classic machine-learning
    problem that’s discussed in greater detail in chapter 3\. The algorithm starts
    with an initial guess, which is an equation characterized by a few numbers (such
    as the slope or y-intercept). Over time, the algorithm generates increasingly
    better guesses for these numbers, which are also called parameters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最佳拟合多个点的直线方程是机器学习中的一个经典问题，在第三章中进行了更详细的讨论。算法从一个初始猜测开始，这个猜测是一个由几个数字（如斜率或y截距）特征化的方程。随着时间的推移，算法会生成越来越好的这些数字的猜测，这些数字也被称为参数。
- en: So far, we’ve been manipulating only constants. Programs with only constants
    aren’t that interesting for real-world applications, though, so TensorFlow allows
    richer tools such as variables, which are containers for values that may change
    over time. A machine-learning algorithm updates the parameters of a model until
    it finds the optimal value for each variable. In the world of machine learning,
    it’s common for parameters to fluctuate until eventually settling down, making
    variables an excellent data structure for them.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只操作了常量。只有常量的程序对于现实世界的应用并不那么有趣，因此TensorFlow允许更丰富的工具，如变量，它们是可能随时间变化的值的容器。机器学习算法更新模型的参数，直到找到每个变量的最优值。在机器学习的世界里，参数通常会波动，直到最终稳定下来，这使得变量成为它们的优秀数据结构。
- en: The code in listing 2.8 is a simple TensorFlow program that demonstrates how
    to use variables. It updates a variable whenever sequential data abruptly increases
    in value. Think about recording measurements of a neuron’s activity over time.
    This piece of code can detect when the neuron’s activity suddenly spikes. The
    algorithm is an oversimplification for didactic purposes, of course.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8中的代码是一个简单的TensorFlow程序，演示了如何使用变量。它会在序列数据突然增加时更新变量。想象一下记录神经元活动随时间的变化。这段代码可以检测神经元活动突然增加的情况。当然，这个算法是为了教学目的而简化的。
- en: Start with importing TensorFlow. TensorFlow allows you to declare a session
    by using `tf.InteractiveSession``()`. When you’ve declared an interactive session,
    TensorFlow functions don’t require the session attribute that they would otherwise,
    which makes coding in Jupyter notebooks easier.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入TensorFlow。TensorFlow允许你使用`tf.InteractiveSession()`声明一个会话。当你声明了一个交互式会话后，TensorFlow函数不需要它们通常需要的会话属性，这使得在Jupyter笔记本中编码更加容易。
- en: Listing 2.8 Using a variable
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 使用变量
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Starts the session in interactive mode so you won’t need to pass around sess
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以交互模式启动会话，这样你就不需要传递sess
- en: ❷ Let’s say you have some raw data like this.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 假设你有一些原始数据如下。
- en: ❸ Creates a Boolean variable called spike to detect a sudden increase in a series
    of numbers
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个名为spike的布尔变量来检测一系列数字的突然增加
- en: ❹ Because all variables must be initialized, initialize the variable by calling
    run() on its initializer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 因为所有变量都必须初始化，所以通过在其初始化器上调用run()来初始化变量。
- en: ❺ Loops through the data (skipping the first element) and updates the spike
    variable when a significant increase occurs
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历数据（跳过第一个元素），并在发生显著增加时更新spike变量
- en: ❻ To update a variable, assign it a new value, using tf.assign(<var name>, <new
    value>). Evaluate it to see the change.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 要更新一个变量，给它分配一个新的值，使用tf.assign(<var name>, <new value>)。评估它以查看变化。
- en: ❼ Remember to close the session after it’ll no longer be used.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 记得在不再使用会话后关闭它。
- en: 'The expected output of listing 2.8 is a list of spike values over time:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8的预期输出是随时间变化的spike值列表：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 2.8 Saving and loading variables
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 保存和加载变量
- en: Imagine writing a monolithic block of code, of which you’d like to individually
    test a tiny segment. In complicated machine-learning situations, saving and loading
    data at known checkpoints makes debugging code much easier. TensorFlow provides
    an elegant interface to save and load variable values to disk; let’s see how to
    use it for that purpose.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下编写一个庞大的代码块，你希望单独测试其中的一小段。在复杂的机器学习情况下，在已知检查点保存和加载数据可以使调试代码变得更加容易。TensorFlow提供了一个优雅的接口来保存和加载变量值到磁盘；让我们看看如何使用它来达到这个目的。
- en: You’ll revamp the code that you created in listing 2.8 to save the spike data
    to disk so that you can load it elsewhere. You’ll change the spike variable from
    a simple Boolean to a vector of Booleans that captures the history of spikes (listing
    2.9). Notice that you’ll explicitly name the variables so that they can be loaded
    later with the same names. Naming a variable is optional but highly encouraged
    for organizing your code. Later in the book, particularly in chapters 14 and 15,
    you will also use the `tf.identity` function to name the variable so that you
    can reference it when restoring a saved model graph.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你将修改在列表 2.8 中创建的代码，以便将尖峰数据保存到磁盘上，这样你就可以在其他地方加载它。你将把 spike 变量从简单的布尔值改为布尔向量，以捕获尖峰的历史（列表
    2.9）。请注意，你将明确命名变量，以便以后可以用相同的名称加载它们。命名变量是可选的，但强烈建议这样做以组织代码。在本书的后面部分，特别是在第 14 章和第
    15 章中，你还将使用 `tf.identity` 函数来命名变量，以便在恢复保存的模型图时可以引用它。
- en: Try running this code to see the results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行此代码以查看结果。
- en: Listing 2.9 Saving variables
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 保存变量
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Imports TensorFlow and enables interactive sessions
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 TensorFlow 并启用交互式会话
- en: ❷ Let’s say you have a series of data like this.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 假设你有一系列这样的数据。
- en: ❸ Defines a Boolean vector called spikes to locate a sudden spike in raw data
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个名为 spikes 的布尔向量，用于定位原始数据中的突然尖峰
- en: ❹ Don’t forget to initialize the variable.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 不要忘记初始化变量。
- en: ❺ The saver op will enable saving and restoring variables. If no dictionary
    is passed into the constructor, it saves all variables in the current program.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 保存器 op 将启用保存和恢复变量。如果构造函数中没有传递字典，则保存当前程序中的所有变量。
- en: ❻ Loop through the data and update the spikes variable when a significant increase
    occurs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 遍历数据，并在发生显著增加时更新 spikes 变量。
- en: ❼ Updates the value of spikes by using the tf.assign function
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 tf.assign 函数更新 spikes 的值
- en: ❽ Don’t forget to evaluate the updater; otherwise, spikes won’t be updated.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 不要忘记评估更新器；否则，spikes 不会更新。
- en: ❾ Saves the variable to disk
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将变量保存到磁盘
- en: ❿ Prints the relative file path of the saved variables
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 打印保存变量的相对文件路径
- en: You’ll notice that a couple of files are generated—one of them being spikes.ckpt—in
    the same directory as your source code. This file is a compactly stored binary
    file, so you can’t easily modify it with a text editor. To retrieve this data,
    you can use the `restore` function from the `saver` op, as demonstrated in listing
    2.10.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在源代码相同的目录中生成了几个文件——其中一个是 spikes.ckpt——这是一个紧凑存储的二进制文件，因此你无法用文本编辑器轻松修改它。要检索这些数据，你可以使用
    `saver` op 中的 `restore` 函数，如列表 2.10 所示。
- en: Listing 2.10 Loading variables
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 加载变量
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Creates a variable of the same size and name as the saved data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个与保存数据大小和名称相同的变量
- en: ❷ You no longer need to initialize this variable because it’ll be loaded directly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 你不再需要初始化这个变量，因为它将直接加载。
- en: ❸ Creates the saver op to restore saved data
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建保存器 op 以恢复保存的数据
- en: ❹ Restores data from the spikes.ckpt file
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从 spikes.ckpt 文件中恢复数据
- en: ❺ Prints the loaded data
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印加载的数据
- en: 'The expected output of listing 2.10 is a Python list of the spikes in your
    data that looks like the following. The first message is simply TensorFlow telling
    you that it is loading the model graph and associated parameters (later in the
    book, we will refer to these as weights) from the checkpoint file spikes.ckpt:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 的预期输出是包含你数据中尖峰的 Python 列表，如下所示。第一条消息只是 TensorFlow 告诉你它正在从检查点文件 spikes.ckpt
    加载模型图和相关参数（在本书的后面，我们将这些称为权重）：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 2.9 Visualizing data using TensorBoard
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 使用 TensorBoard 可视化数据
- en: In machine learning, the most time-consuming part isn’t programming; it’s waiting
    for code to finish running unless you use early stopping or see overfitting and
    want to terminate your model training process. A famous dataset called ImageNet,
    for example, contains more than 14 million images prepared to be used in a machine-learning
    context. Sometimes, it can take days or weeks to finish training an algorithm
    using a large dataset. TensorFlow’s handy dashboard, TensorBoard, affords you
    a quick peek at the way values are changing in each node of the graph, giving
    you some idea of how your code is performing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最耗时的部分不是编程；而是等待代码运行完成，除非你使用提前停止或看到过拟合并希望终止你的模型训练过程。例如，一个著名的数据集 ImageNet
    包含了超过 1400 万张图像，这些图像准备用于机器学习环境。有时，使用大型数据集训练一个算法可能需要几天或几周的时间。TensorFlow 的便捷仪表板
    TensorBoard 允许你快速查看图中每个节点值的改变情况，从而让你对代码的执行情况有一个大致的了解。
- en: Let’s see how to visualize variable trends over time in a real-world example.
    In this section, you’ll implement a moving-average algorithm in TensorFlow; then
    you’ll carefully track the variables you care about for visualization in TensorBoard.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在现实世界的例子中可视化变量随时间的变化趋势。在本节中，你将在TensorFlow中实现移动平均算法；然后你将仔细跟踪你关心的变量，以便在TensorBoard中进行可视化。
- en: Wondering why there is a second edition of this book?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你想知道这本书为什么会有第二版吗？
- en: 'One key reason why this book exists is the preceding italicized text (Sometimes,
    it can take days or weeks to finish training an algorithm using a large dataset).
    Prepare for that experience in this book. Later in the book, I will be re-creating
    some famous (read: big) models, including the VGG -Face model for facial identification,
    and generating a sentiment analysis model using all the Netflix review data in
    natural-language processing. Prepare in this case for TensorFlow to run overnight
    on your laptop or for it to take days on the supercomputer that you have access
    to. Don’t worry; I’m here to guide you along the way and be your emotional support!'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书存在的一个关键原因是前面的斜体文本（有时，使用大数据集训练算法可能需要几天或几周的时间）。在这本书中为这种体验做好准备。本书后面，我将重新创建一些著名的（读：大型）模型，包括用于面部识别的VGG-Face模型，以及使用自然语言处理中的所有Netflix评论数据生成情感分析模型。在这种情况下，为TensorFlow在你的笔记本电脑上运行一整夜或在你可访问的超级计算机上运行几天做好准备。别担心；我在这里指导你，并成为你的情感支持者！
- en: 2.9.1 Implementing a moving average
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.1 实现移动平均
- en: 'In this section, you’ll use TensorBoard to visualize how data changes. Suppose
    that you’re interested in calculating the average stock price of a company. Typically,
    computing the average is a matter of adding all the values and dividing by the
    total: mean = (*x*[1] + *x*[2] + ... + *x*[n]) / *n*. When the total number of
    values is unknown, you can use a technique called *exponential averaging* to estimate
    the average value of an unknown number of data points. The exponential average
    algorithm calculates the current estimated average as a function of the previous
    estimated average and the current value.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用TensorBoard来可视化数据的变化。假设你对计算一家公司的平均股价感兴趣。通常，计算平均值是添加所有值然后除以总数的问题：mean
    = (*x*[1] + *x*[2] + ... + *x*[n]) / *n*。当值的总数未知时，你可以使用称为 *指数平均* 的技术来估计未知数量数据点的平均值。指数平均算法将当前估计的平均值计算为先前估计的平均值和当前值的一个函数。
- en: More succinctly, *Avg*[t] = *f* (*Avg*[t - 1], *x*[t]) = (1 - α) *Avg*[t - 1]
    + α *x*[t]. Alpha (α) is a parameter that will be tuned, representing how strongly
    recent values should be biased in the calculation of the average. The higher the
    value of α, the more dramatically the calculated average will differ from the
    previously estimated average. Figure 2.8 (shown after listing 2.15 later in this
    chapter) shows how TensorBoard visualizes the values and corresponding running
    average over time.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 更简洁地说，*Avg*[t] = *f* (*Avg*[t - 1], *x*[t]) = (1 - α) *Avg*[t - 1] + α *x*[t]。Alpha
    (α) 是一个将被调整的参数，表示在平均计算中最近值应该有多大的偏差。α的值越高，计算的平均值与先前估计的平均值之间的差异就越大。图2.8（在本书后面的列表2.15之后显示）展示了TensorBoard如何随时间可视化值和相应的运行平均值。
- en: When you code this moving average, it’s a good idea to think about the main
    piece of computation that takes place in each iteration. In this case, each iteration
    will compute *Avg*[t] = (1 - α) *Avg*[t - 1] + α *x*[t]. As a result, you can
    design a TensorFlow operator (listing 2.11) that does exactly as the formula says.
    To run this code, you’ll eventually have to define `alpha`, `curr_value`, and
    `prev_avg`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写这个移动平均的代码时，考虑每个迭代中发生的主要计算部分是个好主意。在这种情况下，每个迭代将计算 *Avg*[t] = (1 - α) *Avg*[t
    - 1] + α *x*[t]。因此，你可以设计一个TensorFlow操作符（列表2.11），它正好按照公式执行。要运行此代码，你最终必须定义 `alpha`、`curr_value`
    和 `prev_avg`。
- en: Listing 2.11 Defining the average update operator
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 定义平均更新操作符
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ alpha is a tf.constant, curr_value is a placeholder, and prev_avg is a variable.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ alpha 是一个 tf.constant，curr_value 是一个占位符，prev_avg 是一个变量。
- en: You’ll define the undefined variables later. The reason you’re writing code
    in such a backward way is that defining the interface first forces you to implement
    the peripheral setup code to satisfy the interface. Skipping ahead, let’s jump
    right to the session part to see how your algorithm should behave. Listing 2.12
    sets up the primary loop and calls the `update_avg` operator on each iteration.
    Running the `update_avg` operator depends on the `curr_value`, which is fed via
    the `feed_dict` argument.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在稍后定义未定义的变量。你之所以以这种方式编写代码，是因为首先定义接口迫使你必须实现外围设置代码以满足接口。跳过前面的部分，让我们直接跳到会话部分，看看你的算法应该如何表现。列表
    2.12 设置了主要循环并在每次迭代中调用 `update_avg` 操作符。运行 `update_avg` 操作符取决于 `curr_value`，它通过
    `feed_dict` 参数提供。
- en: Listing 2.12 Running iterations of the exponential average algorithm
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12 运行指数平均算法的迭代
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Great. The general picture is clear, and all that’s left to do is write out
    the undefined variables. Let’s fill in the gaps and implement a working piece
    of TensorFlow code. Copy listing 2.13 so that you can run it.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。整体图景很清晰，剩下要做的就是写出未定义的变量。让我们填补空白并实现一段可工作的 TensorFlow 代码。复制列表 2.13 以便你可以运行它。
- en: Listing 2.13 Filling in missing code to complete the exponential average algorithm
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.13 填充缺失的代码以完成指数平均算法
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Creates a vector of 100 numbers with a mean of 10 and standard deviation of
    1
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个具有 100 个数字的向量，均值为 10，标准差为 1
- en: ❷ Defines alpha as a constant
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 alpha 定义为一个常数
- en: ❸ A placeholder is like a variable, but the value is injected from the session.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 占位符就像一个变量，但其值是从会话中注入的。
- en: ❹ Initializes the previous average to zero
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将前一个平均值初始化为零
- en: ❺ Loops through the data one by one to update the average
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 逐个遍历数据以更新平均值
- en: 2.9.2 Visualizing the moving average
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.2 可视化移动平均
- en: 'Now that you have a working implementation of a moving-average algorithm, let’s
    visualize the results by using TensorBoard. Visualization with TensorBoard is
    usually a two-step process:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经实现了一个移动平均算法的工作版本，让我们使用 TensorBoard 来可视化结果。使用 TensorBoard 的可视化通常是一个两步过程：
- en: Pick out which nodes you care about measuring by annotating them with a *summary
    op*.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 *总结操作* 注释你关心的节点来挑选出你想要测量的节点。
- en: Call `add_summary` on them to queue up data to be written to disk.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在它们上调用 `add_summary` 以将数据排队到磁盘上。
- en: Suppose that you have an `img` placeholder and a `cost` op, as shown in listing
    2.14\. You can annotate them (by giving each a name such as `img` or `cost`) so
    that they’re capable of being visualized in TensorBoard. You’ll do something similar
    with your moving-average example.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个 `img` 占位符和一个 `cost` 操作符，如列表 2.14 所示。你可以注释它们（通过给每个一个名称，如 `img` 或 `cost`），这样它们就可以在
    TensorBoard 中进行可视化。你将在你的移动平均示例中做类似的事情。
- en: Listing 2.14 Annotating with a summary op
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.14 使用总结操作进行注释
- en: '[PRE24]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: More generally, to communicate with TensorBoard, you must use a summary op,
    which produces serialized strings used by a `SummaryWriter` to save updates to
    a directory. Every time you call the `add_summary` method from `SummaryWriter`,
    TensorFlow saves data to disk for TensorBoard to use.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地，为了与 TensorBoard 通信，你必须使用总结操作，它产生由 `SummaryWriter` 用来保存更新到目录的序列化字符串。每次你从
    `SummaryWriter` 调用 `add_summary` 方法时，TensorFlow 都会将数据保存到磁盘上供 TensorBoard 使用。
- en: Warning Be careful not to call the `add_summary` function too often! Although
    doing so will produce higher-resolution visualizations of your variables, it’ll
    be at the cost of more computation and slightly slower learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 请注意不要频繁调用 `add_summary` 函数！虽然这样做会产生更高分辨率的变量可视化，但代价是更多的计算和略微缓慢的学习。
- en: 'Run the following command to make a directory called logs in the same folder
    as this source code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令在与此源代码相同的文件夹中创建一个名为 logs 的目录：
- en: '[PRE25]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run TensorBoard with the location of the logs directory passed in as an argument:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传递给参数的日志目录位置运行 TensorBoard：
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Open a browser, and navigate to http://localhost:6006, which is the default
    URL for TensorBoard. Listing 2.15 shows how to hook up the `SummaryWriter` to
    your code. Run it, and refresh TensorBoard to see the visualizations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 打开浏览器，导航到 http://localhost:6006，这是 TensorBoard 的默认 URL。列表 2.15 展示了如何将 `SummaryWriter`
    连接到你的代码。运行它，并刷新 TensorBoard 以查看可视化。
- en: Listing 2.15 Writing summaries to view in TensorBoard
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.15 将总结写入以在 TensorBoard 中查看
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Creates a summary node for the averages
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为平均值创建一个总结节点
- en: ❷ Creates a summary node for the values
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为值创建一个总结节点
- en: ❸ Merges the summaries to make them easier to run at the same time
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 合并总结以使它们更容易同时运行
- en: ❹ Passes in the logs directory’s location to the writer
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将日志目录的位置传递给writer
- en: ❺ Optional, but allows you to visualize the computation graph in TensorBoard
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 可选，但允许您在TensorBoard中可视化计算图
- en: ❻ Runs the merged op and the update_avg op at the same time
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 同时运行合并操作和更新平均操作
- en: ❼ Adds the summary to the writer
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将摘要添加到writer中
- en: TIP You may need to ensure that the TensorFlow session has ended before starting
    TensorBoard. If you rerun listing 2.15, you’ll need to remember to clear the logs
    directory.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在启动TensorBoard之前，您可能需要确保TensorFlow会话已经结束。如果您重新运行列表2.15，您需要记得清除日志目录。
- en: '![CH02_F08_Mattmann2](../Images/CH02_F08_Mattmann2.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F08_Mattmann2](../Images/CH02_F08_Mattmann2.png)'
- en: Figure 2.8 The summary display in TensorBoard created in listing 2.15\. TensorBoard
    provides a user-friendly interface to visualize data produced in TensorFlow.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 列表2.15中创建的TensorBoard的摘要显示。TensorBoard为可视化TensorFlow生成数据提供了一个用户友好的界面。
- en: '2.10 Putting it all together: The TensorFlow system architecture and API'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 将所有内容整合：TensorFlow系统架构和API
- en: I have not delved into everything that TensorFlow can do—the rest of the book
    is for those topics—but I have illustrated its core components and the interfaces
    between them. The collection of these components and interfaces makes up the TensorFlow
    architecture, shown in figure 2.9.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我并没有深入探讨TensorFlow能做的一切——本书的其余部分将涉及这些主题——但我已经展示了其核心组件及其之间的接口。这些组件和接口的集合构成了TensorFlow架构，如图2.9所示。
- en: The TensorFlow 2 version of the listings incorporates new features, including
    always eager execution and updated package names for the optimizers and training.
    The new listings work well in Python 3; I welcome your feedback on them if you
    give them a try. You can find the TensorFlow 2 listing code at [https://github.com/
    chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2](http://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2/).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2版本的列表包含了新特性，包括始终急切执行和优化器及训练的更新包名。新的列表在Python 3中运行良好；如果您尝试了它们，欢迎您对这些列表提供反馈。您可以在[https://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2](http://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2/)找到TensorFlow
    2的列表代码。
- en: '![CH02_F09_Mattmann2](../Images/CH02_F09_Mattmann2.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F09_Mattmann2](../Images/CH02_F09_Mattmann2.png)'
- en: Figure 2.9 The TensorFlow system architecture and APi. We’ve spent a lot of
    time on sessions, instrumentations, model representation, and training thus far.
    The later portions of the book progressively explore math, complex models for
    facial recognition called convolution neural networks (CNNs), optimization, and
    the rest of the system.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 TensorFlow系统架构和API。迄今为止，我们已经花费了大量时间在会话、仪器、模型表示和训练上。本书的后续部分将逐步探讨数学、用于人脸识别的复杂模型——卷积神经网络（CNNs）、优化以及系统的其他部分。
- en: 'Keeping up with the versions: TF2 and beyond'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随版本：TF2及以后
- en: This book is standardized on two versions of TensorFlow from the 1.x series.
    Version 1.15, which is the latest release in the 1.x series, works well with Python
    3\. In chapters 7 and 19, you’ll read about a few examples that require Python
    2; for that reason, TensorFlow 1.14 is required.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本书以TensorFlow 1.x系列的两个版本为标准。1.x系列中的最新版本1.15与Python 3兼容良好。在第7章和第19章中，您将阅读到一些需要Python
    2的示例；因此，需要TensorFlow 1.14。
- en: Also, a complete port of the listings and code that address TensorFlow 2 was
    released while this book was under development. (See the appendix for all the
    details.) You’ll notice that 85-90% of the code for the listings that work in
    TensorFlow 2 is the same. The main reason is that the data cleaning, gathering,
    preparation, and evaluation code is fully reusable because it uses accompanying
    ML libraries like Scikit and Matplotlib.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本书开发期间，还发布了针对TensorFlow 2的完整列表和代码的移植。（详见附录中的所有详细信息。）您会注意到，在TensorFlow 2中运行的列表代码中有85-90%是相同的。主要原因在于数据清洗、收集、准备和评估代码可以完全重用，因为它使用了伴随的ML库，如Scikit和Matplotlib。
- en: One reasons why it’s so hard to pick a version of the framework to depend on
    and go with it is that software changes so quickly. The author of this book found
    that out when trying to run some listings from the first edition. Though the concepts
    and architecture and system itself remained the same and are close to what would
    actually run, even in two years, TensorFlow changed a great deal, with more than
    20 versions released since TensorFlow 1.0 and the current 1.15.2 version—notably,
    the last 1.x release. Parts of these changes had to do with breaking changes in
    the 1.x version of the system, but other parts had to do with a more fundamental
    architectural understanding gained by performing some of the suggested examples
    at the end of each chapter, stumbling, and then realizing that TensorFlow has
    code and interfaces to tackle the problem. As Scott Penberthy, head of applied
    AI at Google and TensorFlow guru, states in the foreword, chasing TensorFlow versions
    isn’t the point; the details, architecture, cleaning steps, processing, and evaluation
    techniques will withstand the test of time while the great software engineers
    improve the scaffolding around tensors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个框架版本并坚持使用它之所以如此困难，一个原因就是软件变化如此之快。本书的作者在尝试运行第一版中的某些列表时发现了这一点。尽管概念、架构和系统本身保持不变，并且接近实际运行的版本，即使在两年后，TensorFlow
    也发生了很大变化，自 TensorFlow 1.0 以来已发布了 20 多个版本，目前是 1.15.2 版本——值得注意的是，最后的 1.x 版本。这些变化的部分原因与系统
    1.x 版本的破坏性变化有关，但其他部分则与通过执行每章末尾建议的示例获得的一些更根本的架构理解有关，摸索前行，然后意识到 TensorFlow 有代码和接口来处理问题。正如谷歌应用人工智能负责人、TensorFlow
    大师 Scott Penberthy 在前言中所言，追逐 TensorFlow 版本并不是重点；细节、架构、清理步骤、处理和评估技术将经受时间的考验，而伟大的软件工程师将不断改进围绕张量的框架。
- en: Today, TensorFlow 2.0 is attracting a lot of attention, but rather than chase
    the latest version, which has some fundamental (breaking) changes from the 1.x
    version, I want to deliver a core understanding that will last beyond (breaking)
    changes and enshrine the fundamentals of machine learning and concepts that make
    TensorFlow so special, independent of the version. This edition of the book exists
    largely due to many sleepless nights stumbling through machine-learning concepts
    (such as model building, estimation, and tensor math) that are core elements of
    the TensorFlow architecture.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，TensorFlow 2.0 正在吸引众多关注，但与其追逐最新的版本，这个版本与 1.x 版本相比有一些根本性的（破坏性）变化，我更希望传达一种超越（破坏性）变化的核心理解，并确立机器学习的基础以及使
    TensorFlow 如此特别的概念，不受版本的影响。这本书的版本在很大程度上是由于许多不眠之夜，在机器学习概念（如模型构建、估计和张量数学）中摸索前行，这些是
    TensorFlow 架构的核心元素。
- en: Some of those concepts are shown in figure 2.9\. We’ve spent a lot of time in
    this chapter discussing the power of sessions, how to represent tensors and graphs,
    and how to train a model, save it to disk, and restore it. But I haven’t covered
    how to perform a prediction, how to build even more complex graphs (such as regression,
    classification, CNNs, and RNNs), and how to run TensorFlow on gigabytes and terabytes
    of data. Those tasks are all possible with TensorFlow because Google and the developers
    spent so much time thinking through all the challenges you will face doing machine
    learning at scale.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些概念在图 2.9 中展示。我们在本章中花费了大量时间讨论会话的力量，如何表示张量和图，以及如何训练模型、将其保存到磁盘并恢复。但我还没有涵盖如何进行预测，如何构建更复杂的图（如回归、分类、CNN
    和 RNN），以及如何在千兆和太字节的数据上运行 TensorFlow。所有这些任务都是可能的，因为谷歌和开发者花费了大量时间思考你在进行大规模机器学习时可能遇到的挑战。
- en: Each of those challenges is a gray box in the TensorFlow architecture, meaning
    that it is a component with easy-to-use programming language AP*Is and has* plenty
    of documentation and support. When something doesn’t make sense, that’s what I’m
    here for. All the gray boxes will be addressed in the rest of the book. How’s
    that for customer service?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战中的每一个都是 TensorFlow 架构中的灰色盒子，这意味着它是一个具有易于使用的编程语言 AP*Is 和*大量文档和支持的组件。当某些事情不清楚时，这就是我在这里的原因。本书的其余部分将解决所有这些灰色盒子。这难道不是客户服务吗？
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You should start thinking of mathematical algorithms in terms of a flowchart
    of computation. When you consider nodes to be operations and edges to be data
    flow, writing TensorFlow code becomes trivial. After you define your graph, you
    evaluate it under a session, and you have your result.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该用计算流程图的方式来思考数学算法。当你把节点看作操作，把边看作数据流时，编写TensorFlow代码就变得非常简单。定义好你的图之后，在会话中评估它，你就能得到结果。
- en: No doubt there’s more to TensorFlow than representing computations as a graph.
    As you’ll see in the coming chapters, some of the built-in functions are tailored
    to the field of machine learning. In fact, TensorFlow has some of the best support
    for CNNs, a popular type of model for processing images (with promising results
    in audio and text as well).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毫无疑问，TensorFlow不仅仅能将计算表示为图。正如你将在接下来的章节中看到的，一些内置函数是为机器学习领域量身定制的。实际上，TensorFlow在CNNs方面提供了最好的支持，CNNs是一种流行的图像处理模型（在音频和文本处理方面也取得了有希望的结果）。
- en: TensorBoard provides an easy way to visualize the way data changes in TensorFlow
    code, as well as troubleshoot bugs by inspecting trends in data.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard提供了一种简单的方式来可视化TensorFlow代码中数据的变化方式，以及通过检查数据趋势来调试错误。
- en: TensorFlow works wonderfully with the Jupyter Notebook application, which is
    an elegant interactive medium for sharing and documenting Python code.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow与Jupyter Notebook应用配合得非常好，这是一个优雅的交互式媒介，用于分享和记录Python代码。
- en: The book is standardized on TensorFlow 1.15 and 1.14 for Python 3 and 2, respectively,
    and I’m committed to showing you the power of the architecture by discussing all
    the components and interfaces in the system. There’s an effort to port the code
    examples in the book to TensorFlow 2 in a branch of the book’s GitHub code repo.
    Where possible, the book maps concepts in a way that is resilient to API-level
    changes. See the appendix for details.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书以TensorFlow 1.15和1.14为标准，分别针对Python 3和2，我致力于通过讨论系统中的所有组件和接口来展示架构的强大功能。书中还努力将代码示例移植到TensorFlow
    2，在本书的GitHub代码仓库的一个分支中。尽可能的情况下，本书以能够抵御API级别变化的方式映射概念。详情请见附录。
