- en: 1 Getting to know asyncio
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 了解asyncio
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What asyncio is and the benefits it provides
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是asyncio及其提供的优势
- en: Concurrency, parallelism, threads, and processes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发、并行、线程和进程
- en: The global interpreter lock and the challenges it poses to concurrency
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局解释器锁及其对并发的挑战
- en: How non-blocking sockets can achieve concurrency with only one thread
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非阻塞套接字如何仅用一个线程实现并发
- en: The basics of how event-loop-based concurrency works
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于事件循环的并发工作原理
- en: Many applications, especially in today’s world of web applications, rely heavily
    on I/O (input/output) operations. These types of operations include downloading
    the contents of a web page from the internet, communicating over a network with
    a group of microservices, or running several queries together against a database
    such as MySQL or Postgres. A web request or communication with a microservice
    may take hundreds of milliseconds, or even seconds if the network is slow. A database
    query could be time intensive, especially if that database is under high load
    or the query is complex. A web server may need to handle hundreds or thousands
    of requests at the same time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序，尤其是在当今的Web应用程序世界中，严重依赖于I/O（输入/输出）操作。这些类型的操作包括从互联网下载网页内容、通过网络与一组微服务进行通信，或者对数据库（如MySQL或Postgres）运行多个查询。Web请求或与微服务的通信可能需要数百毫秒，如果网络慢，甚至可能需要几秒。数据库查询可能耗时较长，尤其是如果数据库负载高或查询复杂。Web服务器可能需要同时处理数百或数千个请求。
- en: Making many of these I/O requests at once can lead to substantial performance
    issues. If we run these requests one after another as we would in a sequentially
    run application, we’ll see a compounding performance impact. As an example, if
    we’re writing an application that needs to download 100 web pages or run 100 queries,
    each of which takes 1 second to execute, our application will take at least 100
    seconds to run. However, if we were to exploit concurrency and start the downloads
    and wait simultaneously, in theory, we could complete these operations in as little
    as 1 second.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 同时进行许多I/O请求可能会导致显著的性能问题。如果我们像在顺序运行的应用程序中那样一个接一个地运行这些请求，我们将看到累积的性能影响。例如，如果我们正在编写一个需要下载100个网页或运行100个查询的应用程序，每个查询执行需要1秒，那么我们的应用程序至少需要100秒才能运行。然而，如果我们利用并发性，同时开始下载并等待，理论上，我们可以将这些操作完成在1秒内。
- en: asyncio was first introduced in Python 3.4 as an additional way to handle these
    highly concurrent workloads outside of multithreading and multiprocessing. Properly
    utilizing this library can lead to drastic performance and resource utilization
    improvements for applications that use I/O operations, as it allows us to start
    many of these long-running tasks together.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: asyncio首次在Python 3.4中引入，作为处理这些高度并发工作负载的额外方式，而不仅仅是多线程和多进程。正确利用这个库可以为使用I/O操作的应用程序带来显著的性能和资源利用率改进，因为它允许我们同时启动许多这些长时间运行的任务。
- en: In this chapter, we’ll introduce the basics of concurrency to better understand
    how we can achieve it with Python and the asyncio library. We’ll explore the differences
    between CPU-bound work and I/O-bound work to know which concurrency model best
    suits our specific needs. We’ll also learn about the basics of processes and threads
    and the unique challenges to concurrency in Python caused by its global interpreter
    lock (GIL). Finally, we’ll get an understanding of how we can utilize a concept
    called *non-blocking I/O* with an event loop to achieve concurrency using only
    one Python process and thread. This is the primary concurrency model of asyncio.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍并发的基础知识，以便更好地理解我们如何使用Python和asyncio库实现它。我们将探讨CPU密集型工作与I/O密集型工作的区别，以了解哪种并发模型最适合我们的特定需求。我们还将学习进程和线程的基础知识以及Python的全局解释器锁（GIL）对并发带来的独特挑战。最后，我们将了解如何利用名为*非阻塞I/O*的概念以及事件循环，仅使用一个Python进程和线程来实现并发。这是asyncio的主要并发模型。
- en: 1.1 What is asyncio?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 什么是asyncio？
- en: In a synchronous application, code runs sequentially. The next line of code
    runs as soon as the previous one has finished, and only one thing is happening
    at once. This model works fine for many, if not most, applications. However, what
    if one line of code is especially slow? In that case, all other code after our
    slow line will be stuck waiting for that line to complete. These potentially slow
    lines can block the application from running any other code. Many of us have seen
    this before in buggy user interfaces, where we happily click around until the
    application freezes, leaving us with a spinner or an unresponsive user interface.
    This is an example of an application being blocked leading to a poor user experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在同步应用程序中，代码按顺序执行。下一行代码将在前一行代码完成后立即运行，并且一次只发生一件事。这种模型对于许多，如果不是大多数应用程序来说都很好。然而，如果有一行代码特别慢怎么办？在这种情况下，我们慢行之后的所有其他代码都将陷入等待该行完成。这些可能缓慢的行可能会阻止应用程序运行任何其他代码。我们中的许多人以前都见过这种情况，在有缺陷的用户界面中，我们愉快地点击，直到应用程序冻结，留下我们一个旋转器或无响应的用户界面。这是应用程序被阻塞导致用户体验不佳的一个例子。
- en: While any operation can block an application if it takes long enough, many applications
    will block waiting on I/O. I/O refers to a computer’s input and output devices
    such as a keyboard, hard drive, and, most commonly, a network card. These operations
    wait for user input or retrieve the contents from a web-based API. In a synchronous
    application, we’ll be stuck waiting for those operations to complete before we
    can run anything else. This can cause performance and responsiveness issues, as
    we can only have one long operation running at any given time, and that operation
    will stop our application from doing anything else.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然任何操作如果耗时足够长都可能阻塞应用程序，但许多应用程序会因等待I/O而阻塞。I/O指的是计算机的输入和输出设备，例如键盘、硬盘驱动器，以及最常见的网络卡。这些操作等待用户输入或从基于Web的API检索内容。在同步应用程序中，我们将陷入等待这些操作完成，然后才能运行其他任何操作。这可能导致性能和响应性问题，因为我们只能在任何给定时间运行一个长时间的操作，而这个操作将阻止我们的应用程序执行其他任何操作。
- en: One solution to this issue is to introduce concurrency. In the simplest terms,
    *concurrency* means allowing more than one task being handled at the same time.
    In the case of concurrent I/O, examples include allowing multiple web requests
    to be made at the same time or allowing simultaneous connections to a web server.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一是引入并发。用最简单的话来说，**并发**意味着允许同时处理多个任务。在并发I/O的情况下，例子包括允许同时发起多个网络请求或允许同时连接到Web服务器。
- en: There are several ways to achieve this concurrency in Python. One of the most
    recent additions to the Python ecosystem is the asyncio library. *asyncio* is
    short for *asynchronous I/O*. It is a Python library that allows us to run code
    using an asynchronous programming model. This lets us handle multiple I/O operations
    at once, while still allowing our application to remain responsive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中实现这种并发有几种方法。Python生态系统中最新的添加之一是asyncio库。*asyncio*是*异步I/O*的缩写。它是一个Python库，允许我们使用异步编程模型运行代码。这让我们能够同时处理多个I/O操作，同时仍然允许我们的应用程序保持响应。
- en: So what is asynchronous programming? It means that a particular long-running
    task can be run in the background separate from the main application. Instead
    of blocking all other application code waiting for that long-running task to be
    completed, the system is free to do other work that is not dependent on that task.
    Then, once the long-running task is completed, we’ll be notified that it is done
    so we can process the result.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，异步编程是什么意思呢？这意味着可以将在后台独立于主应用程序运行的长运行任务。而不是阻塞所有其他应用程序代码等待那个长运行任务完成，系统可以自由地执行不依赖于该任务的其他工作。然后，一旦长运行任务完成，我们将被通知它已完成，这样我们就可以处理结果。
- en: In Python version 3.4, asyncio was first introduced with decorators alongside
    generator `yield` `from` syntax to define coroutines. A coroutine is a method
    that can be paused when we have a potentially long-running task and then resumed
    when that task is finished. In Python version 3.5, the language implemented first-class
    support for coroutines and asynchronous programming when the keywords `async`
    and `await` were explicitly added to the language. This syntax, common in other
    programming languages such as C# and JavaScript, allows us to make asynchronous
    code look like it is run synchronously. This makes asynchronous code easy to read
    and understand, as it looks like the sequential flow most software engineers are
    familiar with. asyncio is a library to execute these coroutines in an asynchronous
    fashion using a concurrency model known as a *single-threaded event loop*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python 3.4版本中，asyncio库首次引入，与装饰器和生成器`yield` `from`语法一起使用来定义协程。协程是一种方法，当执行一个可能长时间运行的任务时可以被暂停，并在该任务完成后继续执行。在Python
    3.5版本中，语言实现了对协程和异步编程的一级支持，通过显式地将`async`和`await`关键字添加到语言中。这种语法，在C#和JavaScript等其他编程语言中很常见，允许我们将异步代码看起来像同步运行。这使得异步代码易于阅读和理解，因为它看起来像大多数软件工程师熟悉的顺序流程。asyncio是一个库，它使用称为*单线程事件循环*的并发模型以异步方式执行这些协程。
- en: While the name of asyncio may make us think that this library is only good for
    I/O operations, it has functionality to handle other types of operations as well
    by interoperating with multithreading and multiprocessing. With this interoperability,
    we can use `async` and `await` syntax with threads and processes making these
    workflows easier to understand. This means this library not only is good for I/O
    based concurrency but can also be used with code that is CPU intensive. To better
    understand what type of workloads asyncio can help us with and which concurrency
    model is best for each type of concurrency, let’s explore the differences between
    I/O and CPU-bound operations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然asyncio的名字可能让我们认为这个库只适用于I/O操作，但它通过与多线程和多进程的互操作，也有处理其他类型操作的功能。有了这种互操作性，我们可以使用`async`和`await`语法与线程和进程一起使用，使这些工作流程更容易理解。这意味着这个库不仅适用于基于I/O的并发，还可以与CPU密集型代码一起使用。为了更好地理解asyncio可以帮助我们处理哪种类型的工作负载以及哪种并发模型最适合每种类型的并发，让我们来探讨I/O和CPU密集型操作之间的区别。
- en: 1.2 What is I/O-bound and what is CPU-bound?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 什么是I/O密集型和CPU密集型？
- en: When we refer to an operation as I/O-bound or CPU-bound we are referring to
    the limiting factor that prevents that operation from running faster. This means
    that if we increased the performance of what the operation was bound on, that
    operation would complete in less time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到一个操作是I/O密集型或CPU密集型时，我们是指阻止该操作运行更快的限制因素。这意味着如果我们提高了该操作所依赖的性能，该操作将更快完成。
- en: In the case of a CPU-bound operation, it would complete faster if our CPU was
    more powerful, for instance by increasing its clock speed from 2 GHz to 3 GHz.
    In the case of an I/O-bound operation, it would get faster if our I/O devices
    could handle more data in less time. This could be achieved by increasing our
    network bandwidth through our ISP or upgrading to a faster network card.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU密集型操作的情况下，如果我们的CPU更强大，例如通过将时钟速度从2 GHz增加到3 GHz，它将更快完成。在I/O密集型操作的情况下，如果我们的I/O设备能够在更短的时间内处理更多数据，它将更快。这可以通过通过我们的ISP增加网络带宽或升级到更快的网络卡来实现。
- en: CPU-bound operations are typically computations and processing code in the Python
    world. An example of this is computing the digits of pi or looping over the contents
    of a dictionary, applying business logic. In an I/O-bound operation we spend most
    of our time waiting on a network or other I/O device. An example of an I/O-bound
    operation would be making a request to a web server or reading a file from our
    machine’s hard drive.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python世界中，CPU密集型操作通常是计算和处理代码。这方面的例子包括计算π的数字或遍历字典的内容，应用业务逻辑。在I/O密集型操作中，我们大部分时间都在等待网络或其他I/O设备。一个I/O密集型操作的例子是对Web服务器发出请求或从我们的机器硬盘上读取文件。
- en: Listing 1.1 I/O-bound and CPU-bound operations
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.1 I/O密集型和CPU密集型操作
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ I/O-bound web request
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ I/O密集型网络请求
- en: ❷ CPU-bound response processing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ CPU密集型响应处理
- en: ❸ CPU-bound string concatenation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ CPU密集型字符串连接
- en: ❹ I/O-bound write to disk
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ I/O密集型写入磁盘
- en: I/O-bound and CPU-bound operations usually live side by side one another. We
    first make an I/O-bound request to download the contents of https:/ /www.example.com.
    Once we have the response, we perform a CPU-bound loop to format the headers of
    the response and turn them into a string separated by newlines. We then open a
    file and write the string to that file, both I/O-bound operations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: I/O密集型和CPU密集型操作通常并排存在。我们首先发起一个I/O密集型请求来下载https://www.example.com的内容。一旦我们得到响应，我们执行一个CPU密集型循环来格式化响应的头部，并将它们转换成由换行符分隔的字符串。然后我们打开一个文件并将字符串写入该文件，这两个都是I/O密集型操作。
- en: Asynchronous I/O allows us to pause execution of a particular method when we
    have an I/O operation; we can run other code while waiting for our initial I/O
    to complete in the background. This allows us to execute many I/O operations concurrently,
    potentially speeding up our application.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 异步I/O允许我们在有I/O操作时暂停特定方法的执行；我们可以在等待初始I/O在后台完成的同时运行其他代码。这允许我们并发执行许多I/O操作，可能加快我们的应用程序。
- en: 1.3 Understanding concurrency, parallelism, and multitasking
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 理解并发、并行和多任务
- en: To better understand how concurrency can help our applications perform better,
    it is first important to learn and fully understand the terminology of concurrent
    programming. We’ll learn more about what concurrency means and how asyncio uses
    a concept called multitasking to achieve it. Concurrency and parallelism are two
    concepts that help us understand how programming schedules and carries out various
    tasks, methods, and routines that drive action.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解并发如何帮助我们的应用程序性能更好，首先重要的是学习和完全理解并发编程的术语。我们将了解更多关于并发意味着什么以及asyncio如何使用一个称为多任务的概念来实现它。并发和并行是两个帮助我们理解编程如何调度和执行各种任务、方法和例程的概念，这些任务、方法和例程驱动着行动。
- en: 1.3.1 Concurrency
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 并发
- en: When we say two tasks are happening *concurrently*, we mean those tasks are
    happening at the same time. Take, for instance, a baker baking two different cakes.
    To bake these cakes, we need to preheat our oven. Preheating can take tens of
    minutes depending on the oven and the baking temperature, but we don’t need to
    wait for our oven to preheat before starting other tasks, such as mixing the flour
    and sugar together with eggs. We can do other work until the oven beeps, letting
    us know it is preheated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说两个任务正在*并发*发生时，我们的意思是这些任务是在同一时间发生的。以一个面包师同时烤制两个不同的蛋糕为例。为了烤制这些蛋糕，我们需要预热烤箱。预热可能需要数十分钟，这取决于烤箱和烘焙温度，但我们不必在开始其他任务，如将面粉和糖与鸡蛋混合之前等待烤箱预热。我们可以在烤箱哔哔声响起，通知我们烤箱已预热之前做其他工作。
- en: We also don’t need to limit ourselves from starting work on the second cake
    before finishing the first. We can start one cake batter, put it in a stand mixer,
    and start preparing the second batter while the first batter finishes mixing.
    In this model, we’re switching between different tasks concurrently. This switching
    between tasks (doing something else while the oven heats, switching between two
    different cakes) is *concurrent* behavior.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也不必限制自己在完成第一个蛋糕之前就开始制作第二个蛋糕。我们可以开始制作一个蛋糕糊，放入立式搅拌机中，同时在第一个蛋糕糊完成搅拌的过程中开始准备第二个蛋糕糊。在这个模型中，我们是在不同任务之间进行切换。这种在任务之间的切换（在烤箱加热时做其他事情，在两个不同的蛋糕之间切换）是*并发*行为。
- en: 1.3.2 Parallelism
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 并行
- en: While concurrency implies that multiple tasks are in process simultaneously,
    it does not imply that they are running together in parallel. When we say something
    is running *in parallel*, we mean not only are there two or more tasks happening
    concurrently, but they are also executing at the same time. Going back to our
    cake baking example, imagine we have the help of a second baker. In this scenario,
    we can work on the first cake while the second baker works on the second. Two
    people making batter at once is parallel because we have two distinct tasks running
    concurrently (figure 1.1).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并发意味着多个任务同时进行，但它并不意味着它们是并行运行的。当我们说某事正在*并行*运行时，我们的意思不仅是有两个或更多任务正在并发发生，而且它们也是同时执行的。回到我们的蛋糕烘焙例子，想象我们有第二个面包师的帮助。在这种情况下，我们可以在第二个面包师制作第二个蛋糕的同时制作第一个蛋糕。两个人同时制作蛋糕糊是并行的，因为我们有两个不同的任务正在并发运行（图1.1）。
- en: '![01-01](Images/01-01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![01-01](Images/01-01.png)'
- en: Figure 1.1 With *concurrency*, we have multiple tasks happening at the same
    time, but only one we’re actively doing at a given point in time. With *parallelism*,
    we have multiple tasks happening and are actively doing more than one simultaneously.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 在 *并发* 的情况下，我们同时进行多个任务，但在某一特定时刻，我们只积极进行一个任务。在 *并行* 的情况下，我们同时进行多个任务，并且积极同时进行多个任务。
- en: Putting this into terms of applications run by our operating system, let’s imagine
    it has two applications running. In a system that is only concurrent, we can switch
    between running these applications, running one application for a short while
    before letting the other one run. If we do this fast enough, it gives the appearance
    of two things happening at once. In a system that is parallel, two applications
    are running simultaneously, and we’re actively running two things concurrently.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将这应用到我们操作系统运行的应用程序上，让我们想象它正在运行两个应用程序。在一个只有并发性的系统中，我们可以在这两个应用程序之间切换，先运行一个应用程序一段时间，然后再让另一个运行。如果我们这样做得足够快，就会给人一种同时发生两件事的印象。在一个并行系统中，两个应用程序是同时运行的，我们积极同时运行两个并发任务。
- en: The concepts of concurrency and parallelism are similar (figure 1.2) and slightly
    confusing to differentiate, but it is important to understand what makes them
    distinct from one another.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并发和并行的概念相似（见图 1.2），但区分它们有些令人困惑，但了解它们之间的区别很重要。
- en: '![01-02](Images/01-02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![01-02](Images/01-02.png)'
- en: Figure 1.2 With concurrency, we switch between running two applications. With
    parallelism, we actively run two applications simultaneously.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 在并发的情况下，我们在运行两个应用程序之间切换。在并行的情况下，我们积极同时运行两个应用程序。
- en: 1.3.3 The difference between concurrency and parallelism
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 并发与并行的区别
- en: Concurrency is about multiple tasks that can happen independently from one another.
    We can have concurrency on a CPU with only one core, as the operation will employ
    *preemptive multitasking* (defined in the next section) to switch between tasks.
    Parallelism, however, means that we must be executing two or more tasks at the
    same time. On a machine with one core, this is not possible. To make this possible,
    we need a CPU with multiple cores that can run two tasks together.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 并发是指多个任务可以独立发生。我们可以在只有一个核心的 CPU 上实现并发，因为操作将使用 *预先多任务处理*（在下一节中定义）在任务之间切换。然而，并行意味着我们必须同时执行两个或更多任务。在一个只有一个核心的机器上，这是不可能的。为了实现这一点，我们需要一个具有多个核心的
    CPU，可以同时运行两个任务。
- en: While parallelism implies concurrency, concurrency does not always imply parallelism.
    A multithreaded application running on a multiple-core machine is both concurrent
    and parallel. In this setup, we have multiple tasks running at the same time,
    and there are two cores independently executing the code associated with those
    tasks. However, with multitasking we can have multiple tasks happening concurrently,
    yet only one of them is executing at a given time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并行性暗示了并发性，但并发性并不总是意味着并行性。在多核心机器上运行的多线程应用程序既是并发的也是并行的。在这个设置中，我们有多个任务同时运行，有两个核心独立执行与这些任务相关的代码。然而，在多任务处理的情况下，我们可以有多个任务并发发生，但在某一特定时刻只有一个任务在执行。
- en: 1.3.4 What is multitasking?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 什么是多任务处理？
- en: 'Multitasking is everywhere in today’s world. We multitask while making breakfast
    by taking a call or answering a text while we wait for water to boil to make tea.
    We even multitask while commuting to work, by reading a book while the train takes
    us to our stop. Two main kinds of multitasking are discussed in this section:
    *preemptive multitasking* and *cooperative multitasking*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，多任务处理无处不在。我们在做早餐时进行多任务处理，一边接电话或回短信，一边等待水烧开泡茶。我们甚至在通勤去工作时进行多任务处理，一边读书，一边乘坐火车到我们的车站。本节讨论了两种主要的多任务处理类型：*预先多任务处理*
    和 *协作多任务处理*。
- en: Preemptive multitasking
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 预先多任务处理
- en: In this model, we let the operating system decide how to switch between which
    work is currently being executed via a process called *time slicing*. When the
    operating system switches between work, we call it *preempting*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们让操作系统通过一个称为 *时间切片* 的过程来决定如何切换当前正在执行的工作。当操作系统在任务之间切换时，我们称之为 *抢占*。
- en: How this mechanism works under the hood is up to the operating system itself.
    It is primarily achieved through using either multiple threads or multiple processes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制在底层是如何工作的，取决于操作系统本身。它主要通过使用多个线程或多个进程来实现。
- en: Cooperative multitasking
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 协作多任务处理
- en: In this model, instead of relying on the operating system to decide when to
    switch between which work is currently being executed, we explicitly code points
    in our application where we can let other tasks run. The tasks in our application
    operate in a model where they *cooperate*, explicitly saying, “I’m pausing my
    task for a while; go ahead and run other tasks.”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们不是依赖于操作系统来决定何时在哪些工作之间切换，而是在我们的应用程序中明确编码可以允许其他任务运行的位置。我们应用程序中的任务在一个*协作*的模型中运行，明确表示，“我将在一段时间内暂停我的任务；请继续运行其他任务。”
- en: 1.3.5 The benefits of cooperative multitasking
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 协作多任务的优势
- en: asyncio uses cooperative multitasking to achieve concurrency. When our application
    reaches a point where it could wait a while for a result to come back, we explicitly
    mark this in code. This allows other code to run while we wait for the result
    to come back in the background. Once the task we marked has completed, we in effect
    “wake up” and resume executing the task. This gives us a form of concurrency because
    we can have multiple tasks started at the same time but, importantly, not in parallel
    because they aren’t executing code simultaneously.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: asyncio 使用协作多任务来实现并发。当我们的应用程序达到一个可能需要等待一段时间以获取结果的状态时，我们会在代码中明确标记这一点。这允许其他代码在我们等待结果返回的背景中运行。一旦我们标记的任务完成，我们实际上“唤醒”并继续执行该任务。这给我们带来了一种并发形式，因为我们可以在同一时间启动多个任务，但重要的是，它们不是并行执行的，因为它们不是同时执行代码。
- en: Cooperative multitasking has benefits over preemptive multitasking. First, cooperative
    multitasking is less resource intensive. When an operating system needs to switch
    between running a thread or process, it involves a *context switch*. Context switches
    are intensive operations because the operating system must save information about
    the running process or thread to be able to reload it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 协作多任务相较于抢占式多任务有优势。首先，协作多任务资源消耗更少。当操作系统需要在运行线程或进程之间切换时，它涉及到一个*上下文切换*。上下文切换是密集型操作，因为操作系统必须保存有关正在运行的过程或线程的信息，以便能够重新加载它。
- en: A second benefit is *granularity*. An operating system knows that a thread or
    task should be paused based on whichever scheduling algorithm it uses, but that
    might not be the best time to pause. With cooperative multitasking, we explicitly
    mark the areas that are the best for pausing our tasks. This gives us some efficiency
    gains in that we are only switching tasks when we explicitly know it is the right
    time to do so. Now that we understand concurrency, parallelism, and multitasking,
    we’ll use these concepts to understand how to implement them in Python with threads
    and processes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个优势是*粒度*。操作系统知道根据它使用的哪个调度算法，线程或任务应该被暂停，但这可能不是暂停的最佳时机。在协作多任务中，我们明确标记了最适合暂停我们的任务的区域。这使我们获得了一些效率提升，因为我们只在明确知道这是正确的时间时切换任务。现在我们理解了并发、并行和多任务，我们将使用这些概念来了解如何在
    Python 中使用线程和进程来实现它们。
- en: 1.4 Understanding processes, threads, multithreading, and multiprocessing
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 理解进程、线程、多线程和多进程
- en: To better set us up to understand how concurrency works in the Python world,
    we’ll first need to understand the basics about how threads and processes work.
    We’ll then examine how to use them for multithreading and multiprocessing to do
    work concurrently. Let’s start with some definitions around processes and threads.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地让我们理解 Python 世界中的并发是如何工作的，我们首先需要了解线程和进程的基本工作原理。然后我们将检查如何使用它们进行多线程和多进程以并发地执行工作。让我们从关于进程和线程的一些定义开始。
- en: 1.4.1 Process
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 进程
- en: A *process* is an application run that has a memory space that other applications
    cannot access. An example of creating a Python process would be running a simple
    “hello world” application or typing `python` at the command line to start up the
    REPL (read eval print loop).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*进程*是一个具有其他应用程序无法访问的内存空间的运行中的应用程序。创建 Python 进程的一个例子是运行一个简单的“hello world”应用程序，或者在命令行中键入`python`以启动
    REPL（读取评估打印循环）。'
- en: Multiple processes can run on a single machine. If we are on a machine that
    has a CPU with multiple cores, we can execute multiple processes at the same time.
    If we are on a CPU with only one core, we can still have multiple applications
    running simultaneously, through time slicing. When an operating system uses time
    slicing, it will switch between which process is running automatically after some
    amount of time. The algorithms that determine when this switching occurs are different,
    depending on the operating system.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 单个机器上可以运行多个进程。如果我们在一台具有多个核心的CPU的机器上，我们可以同时执行多个进程。如果我们在一核CPU上，我们仍然可以通过时间切片同时运行多个应用程序。当操作系统使用时间切片时，它将在一段时间后自动在运行的进程之间切换。确定何时发生这种切换的算法因操作系统而异。
- en: 1.4.2 Thread
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 线程
- en: Threads can be thought of as lighter-weight processes. In addition, they are
    the smallest construct that can be managed by an operating system. They do not
    have their own memory as does a process; instead, they share the memory of the
    process that created them. Threads are associated with the process that created
    them. A process will always have at least one thread associated with it, usually
    known as the *main thread*. A process can also create other threads, which are
    more commonly known as *worker* or *background* threads. These threads can perform
    other work concurrently alongside the main thread. Threads, much like processes,
    can run alongside one another on a multi-core CPU, and the operating system can
    also switch between them via time slicing. When we run a normal Python application,
    we create a process as well as a main thread that will be responsible for running
    our Python application.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 线程可以被视为比进程更轻量级的实体。此外，它们是操作系统可以管理的最小结构。它们没有自己的内存，就像进程一样；相反，它们共享创建它们的进程的内存。线程与创建它们的进程相关联。一个进程将始终至少有一个与之关联的线程，通常被称为*主线程*。进程还可以创建其他线程，这些线程更常见地被称为*工作线程*或*后台线程*。这些线程可以在主线程旁边并行执行其他工作。线程，就像进程一样，可以在多核CPU上并行运行，操作系统也可以通过时间切片在它们之间切换。当我们运行一个普通的Python应用程序时，我们创建了一个进程以及一个主线程，该线程将负责运行我们的Python应用程序。
- en: Listing 1.2 Processes and threads in a simple Python application
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.2 简单Python应用程序中的进程和线程
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![01-03](Images/01-03.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![01-03](Images/01-03.png)'
- en: Figure 1.3 A process with one main thread reading from memory
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 一个只有一个主线程的进程从内存中读取
- en: 'In figure 1.3, we sketch out the process for listing 1.2\. We create a simple
    application to show us the basics of the main thread. We first grab the process
    ID (a unique identifier for a process) and print it to prove that we indeed have
    a dedicated process running. We then get the active count of threads running as
    well as the current thread’s name to show that we are running one thread—the main
    thread. While the process ID will be different each time this code is run, running
    listing 1.2 will give output similar to the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.3中，我们概述了列表1.2的过程。我们创建了一个简单的应用程序来展示主线程的基本知识。我们首先获取进程ID（进程的唯一标识符）并将其打印出来，以证明我们确实有一个专用的进程在运行。然后我们获取正在运行的线程的活跃计数以及当前线程的名称，以显示我们正在运行一个线程——主线程。虽然每次运行此代码时进程ID都会不同，但运行列表1.2将给出类似于以下内容的输出：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Processes can also create other threads that share the memory of the main process.
    These threads can do other work concurrently for us via what is known as *multithreading*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 进程还可以创建其他线程，这些线程共享主进程的内存。这些线程可以通过所谓的*多线程*为我们执行其他并行工作。
- en: Listing 1.3 Creating a multithreaded Python application
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.3 创建一个多线程Python应用程序
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![01-04](Images/01-04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![01-04](Images/01-04.png)'
- en: Figure 1.4 A multithreaded program with two worker threads and one main thread,
    each sharing the process’s memory
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 具有两个工作线程和一个主线程的多线程程序，每个线程共享进程的内存
- en: 'In figure 1.4, we sketch out the process and threads for listing 1.3\. We create
    a method to print out the name of the current thread and then create a thread
    to run that method. We then call the `start` method of the thread to start running
    it. Finally, we call the `join` method. `join` will cause the program to pause
    until the thread we started completed. If we run the previous code, we’ll see
    output similar to the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.4中，我们概述了列表1.3的过程和线程。我们创建了一个方法来打印当前线程的名称，然后创建一个线程来运行该方法。然后我们调用线程的`start`方法来启动它。最后，我们调用`join`方法。`join`将导致程序暂停，直到我们启动的线程完成。如果我们运行前面的代码，我们将看到类似于以下内容的输出：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that when running this you may see the *hello from thread* and *python
    is currently running 2 thread(s)* messages print on the same line. This is a *race
    condition*; we’ll explore a bit about this in the next section and in chapters
    6 and 7.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当运行此程序时，你可能会在同一行看到“hello from thread”和“python is currently running 2 thread(s)”的消息。这是一个*竞态条件*；我们将在下一节以及第6章和第7章中探讨这一点。
- en: Multithreaded applications are a common way to achieve concurrency in many programming
    languages. There are a few challenges in utilizing concurrency with threads in
    Python, however. Multithreading is only useful for I/O-bound work because we are
    limited by the global interpreter lock, which is discussed in section 1.5.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多编程语言中，多线程应用是实现并发的一种常见方式。然而，在Python中利用线程实现并发有几个挑战。多线程仅适用于I/O密集型工作，因为我们受到全局解释器锁的限制，这在第1.5节中讨论过。
- en: Multithreading is not the only way we can achieve concurrency; we can also create
    multiple processes to do work concurrently for us. This is known as *multiprocessing*.
    In multiprocessing, a parent process creates one or more child processes that
    it manages. It can then distribute work to the child processes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程不是我们实现并发的唯一方式；我们还可以创建多个进程来为我们并发地执行工作。这被称为*多进程*。在多进程中，父进程创建一个或多个子进程，它管理这些子进程。然后，它可以向子进程分配工作。
- en: Python gives us the multiprocessing module to handle this. The API is similar
    to that of the threading module. We first create a process with a `target` function.
    Then, we call its `start` method to execute it and finally its `join` method to
    wait for it to complete running.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Python为我们提供了多进程模块来处理这个问题。API与线程模块类似。我们首先使用一个`target`函数创建一个进程。然后，我们调用其`start`方法来执行它，最后调用其`join`方法来等待它完成运行。
- en: Listing 1.4 Creating multiple processes
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.4 创建多个进程
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![01-05](Images/01-05.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![01-05](Images/01-05.png)'
- en: Figure 1.5 An application utilizing multiprocessing with one parent process
    and two child processes
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 使用一个父进程和两个子进程的应用程序利用多进程
- en: In figure 1.5, we sketch out the process and threads for listing 1.4\. We create
    one child process that prints its process ID, and we also print out the parent
    process ID to prove that we are running different processes. Multiprocessing is
    typically best when we have CPU-intensive work.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.5中，我们概述了列表1.4中的过程和线程。我们创建了一个子进程，该进程打印其进程ID，我们还打印出父进程ID以证明我们在运行不同的进程。在处理CPU密集型工作方面，多进程通常是最好的选择。
- en: Multithreading and multiprocessing may seem like magic bullets to enable concurrency
    with Python. However, the power of these concurrency models is hindered by an
    implementation detail of Python—the global interpreter lock.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程和多进程可能看起来像是使用Python实现并发的神奇子弹。然而，这些并发模型的力量受到Python实现细节的限制——全局解释器锁。
- en: 1.5 Understanding the global interpreter lock
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 理解全局解释器锁
- en: The *global interpreter lock*, abbreviated GIL and pronounced *gill*, is a controversial
    topic in the Python community. Briefly, the GIL prevents one Python process from
    executing more than one Python bytecode instruction at any given time. This means
    that even if we have multiple threads on a machine with multiple cores, a Python
    process can have only one thread running Python code at a time. In a world where
    we have CPUs with multiple cores, this can pose a significant challenge for Python
    developers looking to take advantage of multithreading to improve the performance
    of their application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*全局解释器锁*，简称GIL，发音为*gill*，在Python社区中是一个有争议的话题。简而言之，GIL防止一个Python进程在任何给定时间执行多个Python字节码指令。这意味着即使我们在具有多个核心的机器上有多个线程，Python进程一次也只能运行一个线程。在一个拥有多核心CPU的世界里，这可能会对希望利用多线程来提高其应用程序性能的Python开发者构成重大挑战。'
- en: Note Multiprocessing can run multiple bytecode instructions concurrently because
    each Python process has its own GIL.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：多进程可以并发运行多个字节码指令，因为每个Python进程都有自己的GIL。
- en: So why does the GIL exist? The answer lies in how memory is managed in CPython.
    In CPython, memory is managed primarily by a process known as *reference counting*.
    Reference counting works by keeping track of who currently needs access to a particular
    Python object, such as an integer, dictionary, or list. A reference count is an
    integer keeping track of how many places reference that particular object. When
    someone no longer needs that referenced object, the reference count is decremented,
    and when someone else needs it, it is incremented. When the reference count reaches
    zero, no one is referencing the object, and it can be deleted from memory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么GIL存在呢？答案在于CPython中内存是如何管理的。在CPython中，内存主要是由一个称为*引用计数*的过程来管理的。引用计数通过跟踪谁目前需要访问特定的Python对象，例如整数、字典或列表。引用计数是一个整数，用于跟踪有多少地方引用了该特定对象。当某人不再需要该引用对象时，引用计数会递减，当其他人需要它时，它会递增。当引用计数达到零时，没有人引用该对象，它可以从内存中删除。
- en: What is CPython?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是CPython？
- en: CPython is the reference implementation of Python. By *reference implementation*
    we mean it is the standard implementation of the language and is used as the *reference*
    for proper behavior of the language. There are other implementations of Python
    such as Jython, which is designed to run on the Java Virtual Machine, and IronPython,
    which is designed for the .NET framework.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CPython是Python的参考实现。通过*参考实现*，我们指的是它是语言的标准实现，并被用作语言的*参考*以实现正确的语言行为。Python还有其他实现，例如Jython，它设计用于在Java虚拟机上运行，以及IronPython，它设计用于.NET框架。
- en: The conflict with threads arises in that the implementation in CPython is not
    thread safe. When we say CPython is not *thread safe*, we mean that if two or
    more threads modify a shared variable, that variable may end in an unexpected
    state. This unexpected state depends on the order in which the threads access
    the variable, commonly known as a *race condition*. Race conditions can arise
    when two threads need to reference a Python object at the same time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与线程的冲突在于CPython的实现不是线程安全的。当我们说CPython不是*线程安全*时，我们的意思是如果两个或多个线程修改一个共享变量，该变量可能会最终处于一个意外的状态。这种意外的状态取决于线程访问变量的顺序，通常称为*竞争条件*。当两个线程需要同时引用一个Python对象时，可能会出现竞争条件。
- en: As shown in figure 1.6, if two threads increment the reference count at one
    time, we could face a situation where one thread causes the reference count to
    be zero when the object is still in use by the other thread. The likely result
    of this would be an application crash when we try to read the potentially deleted
    memory.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如图1.6所示，如果两个线程同时增加引用计数，我们可能会遇到一种情况，其中一个线程在另一个线程仍在使用该对象时将其引用计数设置为零。这种情况下，当我们尝试读取可能已被删除的内存时，很可能会发生应用程序崩溃。
- en: '![01-06](Images/01-06.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![01-06](Images/01-06.png)'
- en: Figure 1.6 A race condition where two threads try to increment a reference count
    simultaneously. Instead of an expected count of two, we get one.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 两个线程试图同时增加引用计数的竞争条件。我们得到的不是预期的两个计数，而是一个。
- en: To demonstrate the effect of the GIL on multithreaded programming, let’s examine
    the CPU-intensive task of computing the *n*th number in the Fibonacci sequence.
    We’ll use a fairly slow implementation of the algorithm to demonstrate a time-intensive
    operation. A proper solution would utilize memoization or mathematical techniques
    to improve performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示GIL对多线程编程的影响，让我们检查计算斐波那契序列中的第*n*个数的CPU密集型任务。我们将使用一个相当慢的算法实现来演示耗时操作。一个合适的解决方案将利用记忆化或数学技术来提高性能。
- en: Listing 1.5 Generating and timing the Fibonacci sequence
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.5 生成和计时斐波那契序列
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This implementation uses recursion and is overall a relatively slow algorithm,
    requiring exponential *O*(*2^N*) time to complete. If we are in a situation where
    we need to print two Fibonacci numbers, it is easy enough to synchronously call
    them and time the result, as we have done in the preceding listing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现使用递归，整体上是一个相对较慢的算法，需要指数时间 *O*(2^N) 来完成。如果我们需要打印两个斐波那契数，同步调用它们并计时是足够的简单，就像我们在前面的列表中做的那样。
- en: 'Depending on the speed of the CPU we run on, we will see different timings,
    but running the code in listing 1.5 will yield output similar to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们运行的CPU的速度，我们会看到不同的计时结果，但运行列表1.5中的代码将产生类似于以下内容的输出：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a fairly long computation, but our function calls to `print_fibs` are
    independent from one another. This means that they can be put in multiple threads
    that our CPU can, in theory, run concurrently on multiple cores, thus, speeding
    up our application.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当长的计算，但我们对`print_fibs`的函数调用是相互独立的。这意味着它们可以被放入多个线程中，我们的CPU理论上可以在多个核心上并发运行，从而加快我们的应用程序。
- en: Listing 1.6 Multithreading the Fibonacci sequence
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.6：多线程计算斐波那契数列
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding listing, we create two threads, one to compute `fib(40)` and
    one to compute `fib(41)` and start them concurrently by calling `start()` on each
    thread. Then we make a call to `join()`, which will cause our main program to
    wait until the threads finish. Given that we start our computation of `fib(40)`
    and `fib(41)` simultaneously and run them concurrently, you would think we could
    see a reasonable speedup; however, we will see an output like the following even
    on a multi-core machine.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们创建了两个线程，一个用于计算`fib(40)`，另一个用于计算`fib(41)`，并通过在每个线程上调用`start()`来并发启动它们。然后我们调用`join()`，这将导致我们的主程序等待直到线程完成。鉴于我们同时启动了`fib(40)`和`fib(41)`的计算，并且并发运行它们，你可能会认为我们可以看到合理的加速；然而，即使在多核机器上，我们也会看到如下所示的输出。
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our threaded version took almost the same amount of time. In fact, it was even
    a little slower! This is almost entirely due to the GIL and the overhead of creating
    and managing threads. While it is true the threads run concurrently, only one
    of them is allowed to run Python code at a time due to the lock. This leaves the
    other thread in a waiting state until the first one completes, which completely
    negates the value of multiple threads.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的多线程版本花费的时间几乎相同。实际上，它甚至稍微慢了一点！这几乎完全归因于GIL以及创建和管理线程的开销。虽然线程是并发运行的，但由于锁的存在，一次只能允许一个线程运行Python代码。这导致其他线程处于等待状态，直到第一个线程完成，这完全抵消了多线程的价值。
- en: 1.5.1 Is the GIL ever released?
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 GIL是否会被释放？
- en: Based on the previous example, you may be wondering if concurrency in Python
    can ever happen with threads, given that the GIL prevents running two lines of
    Python concurrently. The GIL, however, is not held forever such that we can’t
    use multiple threads to our advantage.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的例子，你可能想知道，鉴于GIL阻止了Python中两条代码的并发执行，Python中的线程能否实现并发。然而，GIL并不是永远持有的，这样我们就可以利用多个线程的优势。
- en: The global interpreter lock is released when I/O operations happen. This lets
    us employ threads to do concurrent work when it comes to I/O, but not for CPU-bound
    Python code itself (there are some notable exceptions that release the GIL for
    CPU-bound work in certain circumstances, and we’ll look at these in a later chapter).
    To illustrate this, let’s use an example of reading the status code of a web page.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生I/O操作时，全局解释器锁（GIL）会被释放。这使我们能够在进行I/O操作时使用线程来执行并发工作，但并不适用于CPU密集型的Python代码本身（在某些情况下，有一些显著的例外，这些例外会在特定情况下释放GIL以进行CPU密集型工作，我们将在后面的章节中探讨这些内容）。为了说明这一点，让我们用一个读取网页状态码的例子来说明。
- en: Listing 1.7 Synchronously reading status codes
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.7：同步读取状态码
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding listing, we retrieve the contents of example.com and print
    the status code twice. Depending on our network connection speed and our location,
    we’ll see output similar to the following when running this code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们检索了example.com的内容，并打印了状态码两次。根据我们的网络连接速度和位置，当我们运行此代码时，我们会看到如下所示的输出：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have a baseline for what a synchronous version looks like, we can
    write a multithreaded version to compare to. In our multithreaded version, in
    an attempt to run them concurrently, we’ll create one thread for each request
    to example.com.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个同步版本的基线，我们可以编写一个多线程版本来与之比较。在我们的多线程版本中，为了尝试并发运行它们，我们将为每个example.com的请求创建一个线程。
- en: Listing 1.8 Multithreaded status code reading
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.8：多线程读取状态码
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When we execute the preceding listing, we will see output like the following,
    depending again on our network connection and location:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行前面的列表时，我们会看到如下所示的输出，这取决于我们的网络连接和位置：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is roughly two times faster than our original version that did not use
    threads, since we’ve run the two requests at roughly the same time! Of course,
    depending on your internet connection and machine specs, you will see different
    results, but the numbers should be directionally similar.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们没有使用线程的原始版本快大约两倍，因为我们几乎同时运行了这两个请求！当然，根据你的互联网连接和机器规格，你将看到不同的结果，但数字应该方向上相似。
- en: So how is it that we can release the GIL for I/O but not for CPU-bound operations?
    The answer lies in the system calls that are made in the background. In the case
    of I/O, the low-level system calls are outside of the Python runtime. This allows
    the GIL to be released because it is not interacting with Python objects directly.
    In this case, the GIL is only reacquired when the data received is translated
    back into a Python object. Then, at the operating-system level, the I/O operations
    execute concurrently. This model gives us concurrency but not parallelism. In
    other languages, such as Java or C++, we would get true parallelism on multi-core
    machines because we don’t have the GIL and can execute simultaneously. However,
    in Python, because of the GIL, the best we can do is concurrency of our I/O operations,
    and only one piece of Python code is executing at a given time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们是如何释放 GIL 以实现 I/O 并发，而不对 CPU 密集型操作进行释放的呢？答案在于在后台进行的系统调用。在 I/O 的情况下，低级系统调用是在
    Python 运行时之外进行的。这允许 GIL 被释放，因为它并没有直接与 Python 对象交互。在这种情况下，GIL 只在数据被转换回 Python 对象时重新获取。然后，在操作系统级别，I/O
    操作可以并发执行。这种模型给我们带来了并发性，但没有并行性。在其他语言中，例如 Java 或 C++，我们可以在多核机器上获得真正的并行性，因为我们没有 GIL
    并且可以同时执行。然而，在 Python 中，由于 GIL，我们最好的选择是 I/O 操作的并发性，并且一次只有一个 Python 代码块在执行。
- en: 1.5.2 asyncio and the GIL
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.2 asyncio 和 GIL
- en: asyncio exploits the fact that I/O operations release the GIL to give us concurrency,
    even with only one thread. When we utilize asyncio we create objects called *coroutines*.
    A coroutine can be thought of as executing a lightweight thread. Much like we
    can have multiple threads running at the same time, each with their own concurrent
    I/O operation, we can have many coroutines running alongside one another. While
    we are waiting for our I/O-bound coroutines to finish, we can still execute other
    Python code, thus, giving us concurrency. It is important to note that asyncio
    does not circumvent the GIL, and we are still subject to it. If we have a CPU-bound
    task, we still need to use multiple processes to execute it concurrently (which
    can be done with asyncio itself); otherwise, we will cause performance issues
    in our application. Now that we know it is possible to achieve concurrency for
    I/O with only a single thread, let’s dive into the specifics of how this works
    with non-blocking sockets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: asyncio 利用 I/O 操作释放 GIL 的特性，即使在只有一个线程的情况下也能给我们带来并发性。当我们使用 asyncio 时，我们创建称为 *协程*
    的对象。协程可以被视为执行轻量级线程。就像我们可以同时运行多个线程，每个线程都有自己的并发 I/O 操作一样，我们也可以同时运行许多协程。当我们等待 I/O
    密集型协程完成时，我们仍然可以执行其他 Python 代码，从而给我们带来并发性。重要的是要注意，asyncio 并没有绕过 GIL，我们仍然受其影响。如果我们有一个
    CPU 密集型任务，我们仍然需要使用多个进程来并发执行它（这可以通过 asyncio 本身完成）；否则，我们将在应用程序中引起性能问题。现在我们知道，仅使用单个线程就可以实现
    I/O 的并发性，让我们深入了解这是如何与非阻塞套接字一起工作的具体细节。
- en: 1.6 How single-threaded concurrency works
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 单线程并发的工作原理
- en: In the previous section, we introduced multiple threads as a mechanism for achieving
    concurrency for I/O operations. However, we don’t need multiple threads to achieve
    this kind of concurrency. We can do it all within the confines of one process
    and one thread. We do this by exploiting the fact that, at the system level, I/O
    operations can be completed concurrently. To better understand this, we’ll need
    to dive into how sockets work and, in particular, how non-blocking sockets work.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了多线程作为实现I/O操作并发的机制。然而，我们并不需要多个线程来实现这种并发。我们可以在一个进程和一个线程的范围内完成所有这些操作。我们通过利用这样一个事实来实现：在系统级别，I/O操作可以并发完成。为了更好地理解这一点，我们需要深入了解套接字的工作原理，特别是非阻塞套接字的工作原理。
- en: 1.6.1 What is a socket?
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6.1 什么是套接字？
- en: 'A *socket* is a low-level abstraction for sending and receiving data over a
    network. It is the basis for how data is transferred to and from servers. Sockets
    support two main operations: sending bytes and receiving bytes. We write bytes
    to a socket, which will then get sent to a remote address, typically some type
    of server. Once we’ve sent those bytes, we wait for the server to write its response
    back to our socket. Once these bytes have been sent back to our socket, we can
    then read the result.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*套接字* 是在网络中发送和接收数据的一个低级抽象。它是数据从服务器到服务器以及从服务器到客户端传输的基础。套接字支持两种主要操作：发送字节和接收字节。我们将字节写入套接字，然后这些字节将被发送到远程地址，通常是某种服务器。一旦我们发送了这些字节，我们就等待服务器将其响应写回到我们的套接字。一旦这些字节被发送回我们的套接字，我们就可以读取结果。'
- en: Sockets are a low-level concept and are fairly easy to understand if you think
    of them as mailboxes. You can put a letter in your mailbox that your letter carrier
    then picks up and delivers to the recipient’s mailbox. The recipient opens their
    mailbox and your letter. Depending on the contents, the recipient may send you
    a letter back. In this analogy, you may think of the letter as the data or bytes
    we want to send. Consider that the act of putting a letter into the mailbox is
    writing the bytes to a socket, and opening the mailbox to read the letter is reading
    bytes from a socket. The letter carrier can be thought of as the transfer mechanism
    over the internet, routing the data to the correct address.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字是一个低级概念，如果你把它们想象成邮箱，就相对容易理解。你可以把一封信放进你的邮箱，然后你的信使会取走并把它送到收件人的邮箱。收件人打开他们的邮箱和你的信。根据内容，收件人可能会给你回一封信。在这个类比中，你可以把信想象成我们想要发送的数据或字节。考虑一下，把信放进邮箱的行为是将字节写入套接字，而打开邮箱读取信的行为是从套接字读取字节。信使可以被看作是互联网上的传输机制，将数据路由到正确的地址。
- en: 'In the case of getting the contents from example.com as we saw earlier, we
    open a socket that connects to example.com’s server. We then write a request to
    get the contents to that socket and wait for the server to reply with the result:
    in this case, the HTML of the web page. We can visualize the flow of bytes to
    and from the server in figure 1.7.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前看到的从example.com获取内容的情况，我们打开一个连接到example.com服务器的套接字。然后我们向该套接字写入一个请求以获取内容，并等待服务器回复结果：在这种情况下，网页的HTML。我们可以在图1.7中可视化字节从服务器到服务器的流动。
- en: '![01-07](Images/01-07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![01-07](Images/01-07.png)'
- en: Figure 1.7 Writing bytes to a socket and reading bytes from a socket
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 向套接字写入字节和从套接字读取字节
- en: Sockets are *blocking* by default. Simply put, this means that when we are waiting
    for a server to reply with data, we halt our application or *block* it until we
    get data to read. Thus, our application stops running any other tasks until we
    get data from the server, an error happens, or there is a timeout.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字默认是*阻塞*的。简单来说，这意味着当我们等待服务器回复数据时，我们会暂停我们的应用程序或*阻塞*它，直到我们得到可以读取的数据。因此，我们的应用程序停止运行任何其他任务，直到我们从服务器获取数据、发生错误或超时。
- en: At the operating system level, we don’t need to do this blocking. Sockets can
    operate in *non-blocking* mode. In non-blocking mode, when we write bytes to a
    socket, we can just fire and forget the write or read, and our application can
    go on to perform other tasks. Later, we can have the operating system tell us
    that we received bytes and deal with it at that time. This lets the application
    do any number of things while we wait for bytes to come back to us. Instead of
    blocking and waiting for data to come to us, we become more reactive, letting
    the operating system inform us when there is data for us to act on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作系统级别，我们不需要这样做阻塞。套接字可以以*非阻塞*模式运行。在非阻塞模式下，当我们向套接字写入字节时，我们只需触发写入或读取操作，然后我们的应用程序可以继续执行其他任务。稍后，操作系统可以告诉我们我们已经收到了字节，我们可以在那时处理它。这使得应用程序在等待字节返回时可以做任何数量的事情。而不是阻塞并等待数据到来，我们变得更加反应性，让操作系统在有数据供我们操作时通知我们。
- en: 'In the background, this is performed by a few different event notification
    systems, depending on which operating system we’re running. asyncio is abstracted
    enough that it switches between the different notification systems, depending
    on which one our operating system supports. The following are the event notification
    systems used by specific operating systems:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，这由几个不同的事件通知系统执行，具体取决于我们运行的操作系统。asyncio足够抽象，可以在不同的通知系统之间切换，取决于我们的操作系统支持哪个。以下是由特定操作系统使用的通知系统：
- en: '*kqueue*—FreeBSD and MacOS'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*kqueue*—FreeBSD和MacOS'
- en: '*epoll*—Linux'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*epoll*—Linux'
- en: '*IOCP (I/O completion port)*—Windows'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IOCP (I/O completion port)*—Windows'
- en: These systems keep track of our non-blocking sockets and notify us when they
    are ready for us to do something with them. This notification system is the basis
    of how asyncio can achieve concurrency. In asyncio’s model of concurrency, we
    have only one thread executing Python at any given time. When we hit an I/O operation,
    we hand it over to our operating system’s event notification system to keep track
    of it for us. Once we have done this handoff, our Python thread is free to keep
    running other Python code or add more non-blocking sockets for the OS to keep
    track of for us. When our I/O operation finishes, we “wake up” the task that was
    waiting for the result and then proceed to run any other Python code that came
    after that I/O operation. We can visualize this flow in figure 1.8 with a few
    separate operations that each rely on a socket.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统跟踪我们的非阻塞套接字，并在它们准备好让我们对它们进行操作时通知我们。这个通知系统是asyncio实现并发的基础。在asyncio的并发模型中，我们任何时候只有一个线程在执行Python。当我们遇到I/O操作时，我们将其交给操作系统的事件通知系统来跟踪。一旦我们完成这个移交，我们的Python线程就可以自由地运行其他Python代码或为操作系统添加更多非阻塞套接字，以便它为我们跟踪。当我们的I/O操作完成时，我们“唤醒”等待结果的任务，然后继续运行在该I/O操作之后到来的任何其他Python代码。我们可以用图1.8中的几个独立操作来可视化这个流程，每个操作都依赖于套接字。
- en: '![01-08](Images/01-08.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![01-08](Images/01-08.png)'
- en: Figure 1.8 Making a non-blocking I/O request returns immediately and tells the
    O/S to watch sockets for data. This allows execute_other_code() to run right away
    instead of waiting for the I/O requests to finish. Later, we can be alerted when
    I/O is complete and process the response.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 制作非阻塞I/O请求立即返回并告诉操作系统监视套接字以获取数据。这允许execute_other_code()立即运行，而不是等待I/O请求完成。稍后，当I/O完成时，我们可以被通知并处理响应。
- en: But how do we keep track of which tasks are waiting for I/O as opposed to ones
    that can just run because they are regular Python code? The answer lies in a construct
    called an event loop.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何跟踪哪些任务是等待I/O操作，而不是那些可以因为它们是常规Python代码而直接运行的？答案在于一个称为事件循环的结构。
- en: 1.7 How an event loop works
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 事件循环的工作原理
- en: An event loop is at the heart of every asyncio application. *Event loops* are
    a fairly common design pattern in many systems and have existed for quite some
    time. If you’ve ever used JavaScript in a browser to make an asynchronous web
    request, you’ve created a task on an event loop. Windows GUI applications use
    what are called message loops behind the scenes as a primary mechanism for handling
    events such as keyboard input, while still allowing the UI to draw.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 事件循环是每个asyncio应用程序的核心。*事件循环*是许多系统中相当常见的设计模式，并且已经存在了一段时间。如果你曾经使用浏览器中的JavaScript进行异步Web请求，你就在事件循环上创建了一个任务。Windows
    GUI应用程序在幕后使用所谓的消息循环作为处理事件（如键盘输入）的主要机制，同时仍然允许UI进行绘制。
- en: 'The most basic event loop is extremely simple. We create a queue that holds
    a list of events or messages. We then loop forever, processing messages one at
    a time as they come into the queue. In Python, a basic event loop might look something
    like this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的事件循环非常简单。我们创建一个队列，其中包含事件或消息的列表。然后我们永远循环，逐个处理队列中进入的消息。在Python中，一个基本的事件循环可能看起来像这样：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In asyncio, the event loop keeps a queue of tasks instead of messages. Tasks
    are wrappers around a coroutine. A coroutine can pause execution when it hits
    an I/O-bound operation and will let the event loop run other tasks that are not
    waiting for I/O operations to complete.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在asyncio中，事件循环保留一个任务队列而不是消息队列。任务是对协程的包装。当协程遇到I/O操作时，它可以暂停执行，并让事件循环运行其他不等待I/O操作完成的任务。
- en: 'When we create an event loop, we create an empty queue of tasks. We can then
    add tasks into the queue to be run. Each iteration of the event loop checks for
    tasks that need to be run and will run them one at a time until a task hits an
    I/O operation. At that time the task will be “paused,” and we instruct our operating
    system to watch any sockets for I/O to complete. We then look for the next task
    to be run. On every iteration of the event loop, we’ll check to see if any of
    our I/O has completed; if it has, we’ll “wake up” any tasks that were paused and
    let them finish running. We can visualize this as follows in figure 1.9: the main
    thread submits tasks to the event loop, which can then run them.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个事件循环时，我们创建一个空的任务队列。然后我们可以将任务添加到队列中以便运行。事件循环的每次迭代都会检查需要运行的任务，并且会逐个运行它们，直到一个任务遇到I/O操作。那时任务将被“暂停”，我们指示我们的操作系统监视任何套接字以完成I/O。然后我们寻找下一个要运行的任务。在事件循环的每次迭代中，我们会检查是否有我们的I/O已经完成；如果已经完成，我们将“唤醒”任何被暂停的任务，并让它们完成运行。我们可以如下在图1.9中可视化这一点：主线程将任务提交给事件循环，然后它可以运行它们。
- en: '![01-09](Images/01-09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![01-09](Images/01-09.png)'
- en: Figure 1.9 An example of a thread submitting tasks to the event loop
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 一个线程将任务提交给事件循环的示例
- en: 'To illustrate this, let’s imagine we have three tasks that each make an asynchronous
    web request. Imagine these tasks have a bit of code to do setup, which is CPU-bound,
    then they make a web request, and they follow with some CPU-bound postprocessing
    code. Now, let’s submit these tasks to the event loop simultaneously. In pseudocode,
    we would write something like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们想象我们有三个任务，每个任务都进行异步网络请求。想象这些任务有一段用于设置的代码，这是CPU密集型的，然后它们发起网络请求，接着执行一些CPU密集型的后处理代码。现在，让我们将这些任务同时提交给事件循环。在伪代码中，我们会写一些像这样的事情：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: All three tasks start with CPU-bound work and we are single-threaded, so only
    the first task starts executing code, and the other two are left waiting to run.
    Once the CPU-bound setup work is finished in Task 1, it hits an I/O-bound operation
    and will pause itself to say, “I’m waiting for I/O; any other tasks waiting to
    run can run.”
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个任务都以CPU密集型工作开始，我们都是单线程的，所以只有第一个任务开始执行代码，其他两个则等待运行。一旦任务1中的CPU密集型设置工作完成，它遇到I/O密集型操作并将自己暂停，表示“我在等待I/O；任何其他等待运行的任务可以运行。”
- en: Once this happens, Task 2 can begin executing. Task 2 starts its CPU-bound code
    and then pauses, waiting for I/O. At this time both Task 1 and Task 2 are waiting
    concurrently for their network request to complete. Since Tasks 1 and 2 are both
    paused waiting for I/O, we start running Task 3.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发生这种情况，任务2就可以开始执行。任务2开始执行其CPU密集型代码，然后暂停，等待I/O。在这个时候，任务1和任务2都在并发地等待它们的网络请求完成。由于任务1和任务2都暂停等待I/O，我们开始运行任务3。
- en: Now imagine once Task 3 pauses to wait for its I/O to complete, the web request
    for Task 1 has finished. We’re now alerted by our operating system’s event notification
    system that this I/O has finished. We can now resume executing Task 1 while both
    Task 2 and Task 3 are waiting for their I/O to finish.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一旦任务3暂停等待其I/O完成，任务1的网络请求已经完成。我们现在被我们的操作系统的事件通知系统提醒，这个I/O已经完成。我们现在可以继续执行任务1，同时任务2和任务3都在等待它们的I/O完成。
- en: In figure 1.10, we show the execution flow of the pseudocode we just described.
    If we look at any vertical slice of this diagram, we can see that only one CPU-bound
    piece of work is running at any given time; however, we have up to two I/O-bound
    operations happening concurrently. This overlapping of waiting for I/O per each
    task is where the real time savings of asyncio comes in.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.10中，我们展示了我们刚刚描述的伪代码的执行流程。如果我们观察这个图中的任何垂直切片，我们可以看到在任何给定时间点，只有一个CPU密集型的工作正在运行；然而，我们同时最多有两个I/O密集型操作在进行。这种每个任务等待I/O重叠的地方正是asyncio节省时间的真正所在。
- en: '![01-10](Images/01-10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![01-10](Images/01-10.png)'
- en: Figure 1.10 Executing multiple tasks concurrently with I/O operations
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 使用I/O操作并发执行多个任务
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CPU-bound work is work that primarily utilizes our computer’s processor whereas
    I/O-bound work primarily utilizes our network or other input/output devices. asyncio
    primarily helps us make I/O-bound work concurrent, but it exposes APIs for making
    CPU-bound work concurrent as well.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU密集型工作主要是利用我们的计算机处理器的工作，而I/O密集型工作主要是利用我们的网络或其他输入/输出设备。asyncio主要帮助我们使I/O密集型工作并发，但它也提供了用于使CPU密集型工作并发的API。
- en: Processes and threads are the basic most units of concurrency at the operating
    system level. Processes can be used for I/O and CPU-bound workloads and threads
    can (usually) only be used to manage I/O-bound work effectively in Python due
    to the GIL preventing code from executing in parallel.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程和线程是操作系统级别的最基本并发单元。进程可以用于 I/O 和计算密集型工作负载，而线程（通常）只能用于在 Python 中有效地管理 I/O 密集型工作，因为全局解释器锁（GIL）阻止代码并行执行。
- en: We’ve seen how, with non-blocking sockets, instead of stopping our application
    while we wait for data to come in, we can instruct the operating system to tell
    us when data has come in. Exploiting this is part of what allows asyncio to achieve
    concurrency with only a single thread.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用非阻塞套接字，在我们等待数据到来时，不是停止我们的应用程序，而是可以指示操作系统在数据到来时通知我们。利用这一点是 asyncio
    能够仅使用单个线程实现并发的一部分。
- en: We’ve introduced the event loop, which is the core of asyncio applications.
    The event loop loops forever, looking for tasks with CPU-bound work to run while
    also pausing tasks that are waiting for I/O.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经介绍了事件循环，它是 asyncio 应用程序的核心。事件循环永远循环，寻找需要运行的计算密集型任务的，同时暂停等待 I/O 的任务。
