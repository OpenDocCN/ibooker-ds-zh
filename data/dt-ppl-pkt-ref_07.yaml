- en: Chapter 7\. Orchestrating Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。管道编排
- en: Previous chapters have described the building blocks of data pipelines, including
    data ingestion, data transformation, and the steps in a machine learning pipeline.
    This chapter covers how to “orchestrate,” or tie together, those blocks or steps.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章已经描述了数据管道的构建块，包括数据摄取、数据转换以及机器学习管道中的步骤。本章涵盖了如何“编排”或连接这些块或步骤。
- en: Orchestration ensures that the steps in a pipeline are run in the correct order
    and that dependencies between steps are managed properly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 编排确保管道中的步骤按正确顺序运行，并正确管理步骤之间的依赖关系。
- en: When I introduced the challenge of orchestrating pipelines in [Chapter 2](ch02.xhtml#ch02),
    I also introduced the concept of *workflow orchestration platforms* (also referred
    to as *workflow management systems* (WMSs), *orchestration platforms*, or *orchestration
    frameworks*). In this chapter, I will highlight Apache Airflow, which is one of
    the most popular such frameworks. Though the bulk of the chapter is dedicated
    to examples in Airflow, the concepts are transferable to other frameworks as well.
    In fact, I note some alternatives to Airflow later in the chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在[第2章](ch02.xhtml#ch02)介绍管道编排的挑战时，我还介绍了*工作流编排平台*（也称为*工作流管理系统*（WMSs）、*编排平台*或*编排框架*）的概念。本章将重点介绍Apache
    Airflow，这是最流行的此类框架之一。尽管本章大部分内容都专注于Airflow中的示例，但这些概念也适用于其他框架。实际上，我稍后在本章中也提到了一些Airflow的替代方案。
- en: Finally, the later sections of this chapter discuss some more advanced concepts
    in pipeline orchestration, including coordinating multiple pipelines on your data
    infrastructure.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章的后续部分讨论了管道编排中一些更高级的概念，包括在数据基础架构上协调多个管道。
- en: Directed Acyclic Graphs
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有向无环图
- en: Though I introduced DAGs in [Chapter 2](ch02.xhtml#ch02), it’s worth repeating
    what they are. This chapter talks about how they are designed and implemented
    in Apache Airflow to orchestrate tasks in a data pipeline.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在[第2章](ch02.xhtml#ch02)介绍了DAGs，但值得重复一下它们是什么。本章讨论了它们在Apache Airflow中的设计和实现，用于编排数据管道中的任务。
- en: Pipeline steps (tasks) are always *directed*, meaning they start with a task
    or multiple tasks and end with a specific task or tasks. This is required to guarantee
    a path of execution. In other words, it ensures that tasks do not run before all
    their dependent tasks are completed successfully.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 管道步骤（任务）始终是*有向*的，这意味着它们从一个或多个任务开始，并以特定的任务或任务结束。这是为了保证执行路径。换句话说，它确保任务在其所有依赖任务成功完成之前不会运行。
- en: Pipeline graphs must also be *acyclic*, meaning that a task cannot point back
    to a previously completed task. In other words, it cannot cycle back. If it could,
    then a pipeline could run endlessly!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管道图必须也是*无环*的，这意味着一个任务不能指向先前已完成的任务。换句话说，它不能循环。如果可以的话，管道将无休止地运行！
- en: You’ll recall the following example of a DAG from [Chapter 2](ch02.xhtml#ch02),
    which is illustrated in [Figure 7-1](#fig_0701). This is a DAG that was defined
    in Apache Airflow.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得来自[第2章](ch02.xhtml#ch02)的DAG的以下示例，它在[图7-1](#fig_0701)中有所说明。这是在Apache Airflow中定义的一个DAG。
- en: '![dppr 0203](Images/dppr_0203.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0203](Images/dppr_0203.png)'
- en: Figure 7-1\. A DAG with four tasks. After Task A completes, Task B and Task
    C run. When they both complete, Task D runs.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。一个具有四个任务的DAG。在任务A完成后，任务B和任务C运行。当它们都完成时，任务D运行。
- en: Tasks in Airflow can represent anything from the execution of a SQL statement
    to the execution of a Python script. As you will see in the following sections,
    Airflow allows you to define, schedule, and execute the tasks in a data pipeline
    and ensure that they are run in the proper order.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 中的任务可以代表从执行 SQL 语句到执行 Python 脚本的任何内容。正如您将在接下来的章节中看到的，Airflow 允许您定义、调度和执行数据管道中的任务，并确保它们按正确的顺序运行。
- en: Apache Airflow Setup and Overview
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Airflow 设置和概述
- en: 'Airflow is an open source project started by Maxime Beauchemin at Airbnb in
    2014\. It joined the Apache Software Foundation’s Incubator program in March 2016\.
    Airflow was built to solve a common challenge faced by data engineering teams:
    how to build, manage, and monitor workflows (data pipelines in particular) that
    involve multiple tasks with mutual dependencies.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 是一个由Maxime Beauchemin于2014年在Airbnb启动的开源项目。它于2016年3月加入了Apache软件基金会的孵化器计划。Airflow的建立旨在解决数据工程团队面临的常见挑战：如何构建、管理和监控涉及多个具有相互依赖关系的任务的工作流程（特别是数据管道）。
- en: In the six years since it was first released, Airflow has become one of the
    most popular workflow management platforms among data teams. Its easy-to-use web
    interface, advanced command-line utilities, built-in scheduler, and high level
    of customizability mean that it’s a good fit with just about any data infrastructure.
    Though built in Python, it can execute tasks running on any language or platform.
    In fact, though most commonly used in managing data pipelines, it’s truly a generalized
    platform for orchestrating any sort of dependent tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自首次发布以来的六年里，Airflow已成为数据团队中最受欢迎的工作流管理平台之一。其易于使用的Web界面、高级命令行实用程序、内置调度程序以及高度可定制化的特性意味着它几乎适用于任何数据基础设施。尽管是用Python构建的，但它可以执行任何语言或平台上运行的任务。事实上，尽管最常用于管理数据管道，但它实际上是一种用于编排任何类型依赖任务的通用平台。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The code samples and overview in this chapter reference Airflow version 1.x.
    Airflow 2.0 is on the horizon and promises some major enhancements such as a shiny
    new Web UI, a new and improved scheduler, a fully featured REST API, and more.
    Although the specifics of this chapter refer to Airflow 1.x, the concepts will
    remain true in Airflow 2.0\. In addition, the code provided here is intended to
    work with Airflow 2.0 with little or no modification.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码示例和概述参考的是Airflow版本1.x。Airflow 2.0正在接近，承诺带来一些重大增强，如全新的Web UI、改进的调度程序、全功能的REST
    API等。尽管本章具体内容涉及Airflow 1.x，但这些概念在Airflow 2.0中仍然适用。此外，此处提供的代码旨在与Airflow 2.0兼容，几乎无需修改。
- en: Installing and Configuring
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和配置
- en: Installing Airflow is thankfully quite simple. You’ll need to make use of `pip`,
    which was introduced in [“Setting Up Your Python Environment”](ch04.xhtml#setup-python-enviro).
    As you install and fire up Airflow for the first time, you’ll be introduced to
    some of its components, such as the Airflow database, web server, and scheduler.
    I define what each of these are and how they can be further configured in the
    following sections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Airflow非常简单。您需要使用`pip`，这在[“设置Python环境”](ch04.xhtml#setup-python-enviro)中已介绍过。当您首次安装和启动Airflow时，会介绍其一些组件，如Airflow数据库、Web服务器和调度程序。在接下来的章节中，我将定义每个组件及其如何进一步配置。
- en: You can follow the installation instructions from the official [Airflow Quick
    Start Guide](https://oreil.ly/_fGy8). This typically takes less than five minutes!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照官方[Airflow快速入门指南](https://oreil.ly/_fGy8)的安装说明进行操作。这通常不到五分钟！
- en: Once you have Airflow installed and the web server running, you can visit [*http://localhost:8080*](http://localhost:8080)
    in your browser to view the Airflow web UI. If you’d like to learn more about
    the various components of Airflow and how they can be configured, the remainder
    of this section goes into detail on each. If you’re ready to build your first
    Airflow DAG, you can skip ahead to [“Building Airflow DAGs”](#build-airflow-dags).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完Airflow并且Web服务器运行后，您可以在浏览器中访问[*http://localhost:8080*](http://localhost:8080)来查看Airflow
    Web界面。如果您想了解更多关于Airflow各组件及其配置的信息，本节其余部分将详细介绍每个组件。如果您准备构建您的第一个Airflow DAG，可以直接跳转到[“构建Airflow
    DAGs”](#build-airflow-dags)。
- en: For more advanced deployments of Airflow, I suggest taking a look at the official
    [Airflow documentation](https://oreil.ly/_VAXS).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高级的Airflow部署，建议查阅官方[Airflow文档](https://oreil.ly/_VAXS)。
- en: Airflow Database
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow数据库
- en: Airflow uses a database to store all the metadata related to the execution history
    of each task and DAG as well as your Airflow configuration. By default, Airflow
    uses a SQLite database. When you ran the `airflow initdb` command during the installation,
    Airflow created a SQLite database for you. For learning Airflow or even a small-scale
    project, that’s just fine. However, for larger scale needs I suggest using a MySQL
    or Postgres database. Thankfully, Airflow uses the highly regarded `SqlAlchemy`
    library behind the scenes and can easily be reconfigured to use such a database
    instead of SQLite.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow使用数据库来存储与每个任务和DAG执行历史相关的所有元数据，以及您的Airflow配置。默认情况下，Airflow使用SQLite数据库。在安装过程中运行`airflow
    initdb`命令时，Airflow会为您创建一个SQLite数据库。对于学习Airflow或者小规模项目来说，这是可以接受的。然而，对于更大规模的需求，我建议使用MySQL或Postgres数据库。幸运的是，Airflow在幕后使用备受推崇的`SqlAlchemy`库，并且可以轻松重新配置以使用这些数据库，而不是SQLite。
- en: 'To change which database Airflow uses, you’ll need to open the *airflow.cfg*
    file, which is located in the path you used for `AIRFLOW_HOME` during installation.
    In the installation example, that was `~/airflow`. In the file, you’ll see a line
    for the `sql_alchemy_conn` configuration. It will looks something like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改Airflow使用的数据库，请打开*airflow.cfg*文件，该文件位于安装期间用于`AIRFLOW_HOME`的路径中。在安装示例中，那是`~/airflow`。在文件中，您将看到一个用于`sql_alchemy_conn`配置的行。它看起来像这样：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By default the value is set to a connection string for a local SQLite database.
    In the following example, I’ll create and configure a Postgres database and user
    for Airflow and then configure Airflow to use the new database instead of the
    default SQLite database.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，该值设置为本地SQLite数据库的连接字符串。在以下示例中，我将创建和配置一个Postgres数据库和用户供Airflow使用，然后配置Airflow以使用新的数据库而不是默认的SQLite数据库。
- en: Note that I assume that you have a Postgres server running and access to run
    `psql` (the Postgres interactive terminal) and permission to create databases
    and users in `psql`. Any Postgres database will do, but it must be accessible
    from the machine where Airflow is running. To learn more about installing and
    configuring a Postgres server, see the [official site](https://www.postgresql.org).
    You may also be using a managed Postgres instance on a platform like AWS. That’s
    just fine as long as the machine where Airflow is installed can access it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我假设您已经运行了一个Postgres服务器，并且有权限在`psql`中运行（Postgres交互式终端）和创建数据库和用户。任何Postgres数据库都可以，但必须能够从安装Airflow的机器访问它。要了解有关安装和配置Postgres服务器的更多信息，请参阅[官方网站](https://www.postgresql.org)。您也可以在像AWS这样的平台上使用托管的Postgres实例。只要安装Airflow的机器能够访问它即可。
- en: First, launch `psql` on the command line or otherwise open a SQL editor connected
    to your Postgres server.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在命令行上启动`psql`或以其他方式打开连接到您的Postgres服务器的SQL编辑器。
- en: 'Now, create a user for Airflow to use. For simplicity, name it `airflow`. In
    addition, set a password for the user:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个用户供Airflow使用。为了简单起见，命名为`airflow`。此外，为用户设置密码：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, create a database for Airflow. I’ll call it `airflowdb`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为Airflow创建一个数据库。我将其命名为`airflowdb`：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, grant the new user all privileges on the new database. Airflow will
    need to both read and write to the database:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，授予新用户在新数据库上的所有权限。Airflow需要读取和写入数据库：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can go back and modify the connection string in the *airflow.cfg* file.
    I’ll assume that your Postgres server is running on the same machine as Airflow,
    but if not, you’ll need to modify the following by replacing `localhost` with
    the full path to the host where Postgres is running. Save *airflow.cfg* when you’re
    done:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以返回并修改*airflow.cfg*文件中的连接字符串。我假设您的Postgres服务器正在与Airflow运行在同一台机器上，但如果不是，则需要通过将`localhost`替换为Postgres运行的主机的完整路径来修改以下内容。完成后保存*airflow.cfg*：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since Airflow will need to connect to a Postgres database via Python, you’ll
    also need to install the `psycopg2` library:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Airflow将通过Python连接到Postgres数据库，您还需要安装`psycopg2`库：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, go back to the command line to reinitialize the Airflow database in
    Postgres:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回命令行，在Postgres中重新初始化Airflow数据库：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Going forward, you can find all of the Airflow metadata in the `airflowdb` database
    on the Postgres server. There’s a wealth of information there, including task
    history, that can be queried. You can query it directly from the Postrgres database
    or right in the Airflow web UI, as described in the next section. Having the data
    queryable via SQL opens up a world of reporting and analysis opportunities. There’s
    no better way to analyze the performance of your pipelines, and you can do it
    with the data that Airflow collections by default! In [Chapter 10](ch10.xhtml#ch10)
    of this book, I discuss using this and other data to measure and monitor the performance
    of your data pipelines.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，您可以在Postgres服务器的`airflowdb`数据库中找到所有Airflow元数据。那里有大量信息，包括任务历史记录，可以查询。您可以直接从Postrgres数据库或Airflow
    Web UI中进行查询，如下一节所述。通过SQL查询数据使得分析和报告的机会无限。没有比使用默认情况下Airflow收集的数据更好的方法来分析您的流水线的性能！在本书的[第十章](ch10.xhtml#ch10)中，我将讨论使用这些数据和其他数据来衡量和监控数据流水线的性能。
- en: Web Server and UI
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Web服务器和UI
- en: When you started the web server after installation by running the `airflow webs
    erver -p 8080` command, you may have taken a sneak peek at what it had in store.
    If not, open a web browser and navigate to *http://localhost:8080*. If you’re
    working with a fresh install of Airflow, you’ll see something like [Figure 7-2](#fig_0702).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装后通过运行 `airflow webs erver -p 8080` 命令启动 Web 服务器后，您可能已经偷偷看过其内容。如果没有，请打开 Web
    浏览器并导航至 *http://localhost:8080*。如果您使用的是全新安装的 Airflow，您将看到类似于 [图 7-2](#fig_0702)
    的页面。
- en: '![dppr 0702](Images/dppr_0702.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0702](Images/dppr_0702.png)'
- en: Figure 7-2\. The Airflow web UI.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. Airflow Web UI。
- en: The home page of the Web UI shows a list of DAGs. As you can see, Airflow comes
    with some sample DAGs included. They’re a great place to get started if you’re
    new to Airflow. As you create your own DAGs, they’ll show up there as well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Web UI 的首页显示了 DAG 列表。正如您所见，Airflow 包含一些示例 DAG。如果您对 Airflow 还不熟悉，这些示例是一个很好的起点。当您创建自己的
    DAG 时，它们也会显示在这里。
- en: 'There are a number of links and information for each DAG on the page:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上每个 DAG 都有许多链接和信息：
- en: A link to open the properties of the DAG including the path where the source
    file resides, tags, the description, and so on.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个链接，用于打开 DAG 属性，包括源文件路径、标签、描述等信息。
- en: A toggle to enable and pause the DAG. When enabled, the schedule defined in
    the fourth column dictates when it runs. When paused, the schedule is ignored,
    and the DAG can only be run by manual execution.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换按钮，用于启用和暂停 DAG。启用时，第四列中定义的计划决定运行时间。暂停时，计划被忽略，只能通过手动执行运行 DAG。
- en: The name of the DAG, which, when clicked, brings you to the DAG detail page,
    as shown in [Figure 7-3](#fig_0703).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAG 的名称，点击后会带您到 DAG 详细页面，如 [图 7-3](#fig_0703) 所示。
- en: The schedule that the DAG runs on when not paused. It’s shown in [crontab format](https://oreil.ly/btt0G)
    and defined in the DAG source file.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAG 在未暂停时运行的计划。以 [crontab 格式](https://oreil.ly/btt0G) 显示，并在 DAG 源文件中定义。
- en: The owner of the DAG. Usually this is `airflow` but in more complex deployments
    you may have multiple owners to choose from.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAG 的所有者。通常情况下是 `airflow`，但在复杂的部署中，可能有多个所有者可供选择。
- en: Recent Tasks, which is a summary of the latest DAG run.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的任务，这是最新 DAG 运行的摘要。
- en: A timestamp of the last run of the DAG.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一次 DAG 运行的时间戳。
- en: A summary of previous DAG runs.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前 DAG 运行的摘要。
- en: A set of links to various DAG configuration and information. You’ll also see
    these links if you click the name of the DAG.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组链接，指向各种 DAG 的配置和信息。如果点击 DAG 的名称，也会看到这些链接。
- en: When you click the name of a DAG, you’ll be taken to the tree view of the DAG
    on the DAG detail page, as shown in [Figure 7-3](#fig_0703). This is the `example_python_operator`
    DAG that ships with Airflow. The DAG has five tasks that are all `PythonOperators`
    (you’ll learn about operators later in this section). After the `print_the_context`
    task completes successfully, five tasks kick off. When they are done, the DAG
    run is completed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击 DAG 的名称时，会跳转到 DAG 详细页面的树形视图，在 [图 7-3](#fig_0703) 中展示。这是 Airflow 提供的示例 `example_python_operator`
    DAG。该 DAG 包含五个任务，全部是 `PythonOperators`（您将在本节后面学习有关运算符的内容）。在 `print_the_context`
    任务成功完成后，会启动五个任务。任务完成后，DAG 运行完成。
- en: '![dppr 0703](Images/dppr_0703.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0703](Images/dppr_0703.png)'
- en: Figure 7-3\. A tree view of a Airflow DAG.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. Airflow DAG 的树形视图。
- en: You can also click the Graph View button on the top of the page to see what
    the DAG looks like as a graph. I find this view to be the most useful. You can
    see what this particular DAG looks like as a graph in [Figure 7-4](#fig_0704).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以点击页面顶部的图形视图按钮，查看 DAG 的图形视图。我发现这个视图非常有用。您可以在 [图 7-4](#fig_0704) 中看到这个特定 DAG
    的图形表示。
- en: In more complex DAGs with lots of tasks, the graph view can get a little difficult
    to see on the screen. However, note that you can zoom in and out and scroll around
    the graph using your mouse.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含大量任务的复杂 DAG 中，图形视图可能在屏幕上显示时会有些难以看清。但请注意，您可以使用鼠标放大、缩小和滚动图形。
- en: '![dppr 0704](Images/dppr_0704.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0704](Images/dppr_0704.png)'
- en: Figure 7-4\. A graph view of an Airflow DAG.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. Airflow DAG 的图形视图。
- en: 'There are a number of other options on the screen, many which are self-explanatory.
    However, I’d like to focus on two more: Code and Trigger DAG.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕上还有许多其他选项，其中许多是不言自明的。但我想重点介绍另外两个选项：代码和触发 DAG。
- en: When you click Code, you’ll of course see the code behind the DAG. The first
    thing you’ll notice is that the DAG is defined in a Python script. In this case,
    the file is called *example_python_operator.py*. You’ll learn more about the structure
    of a DAG source file later in this chapter. For now, it’s important to know that
    it holds the configuration of the DAG, including its schedule, a definition of
    each task, and the dependencies between each task.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击Code时，您当然会看到DAG背后的代码。您将注意到的第一件事是，DAG是在Python脚本中定义的。在本例中，文件名为*example_python_operator.py*。稍后，您将了解有关DAG源文件结构的更多信息。目前，重要的是知道它保存了DAG的配置，包括其调度、每个任务的定义以及每个任务之间的依赖关系。
- en: The Trigger DAG button allows you to execute the DAG on-demand. Though Airflow
    is built to run DAGs on a schedule, during development, during testing, and for
    off-schedule needs in production, this is the easiest way to run a DAG right away.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 触发DAG按钮允许您按需执行DAG。尽管Airflow是为了按计划运行DAG而构建的，在开发期间，在测试期间以及在生产环境中的非计划需求中，这是立即运行DAG的最简单方式。
- en: Besides managing DAGs, there are a number of other features of the web UI that
    will come in handy. On the top navigation bar, if you click Data Profiling, you’ll
    see options for Ad Hoc Query, Charts, and Known Events. Here you can query information
    from the Airflow database if you’d rather not connect to it directly from another
    tool.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了管理DAGs之外，Web UI的许多其他功能也会很有用。在顶部导航栏中，如果您点击数据分析，您将看到Ad Hoc查询、图表和已知事件的选项。在这里，您可以查询Airflow数据库中的信息，如果您不愿直接从其他工具连接到它。
- en: Under Browse, you can find the run history of DAGs and other log files, and
    under Admin you can find various configuration settings. You can learn more about
    advanced configuration options in the [official Airflow documentation](https://oreil.ly/OuUS_).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览下，您可以找到DAG的运行历史记录和其他日志文件，在管理员中，您可以找到各种配置设置。您可以在[官方Airflow文档](https://oreil.ly/OuUS_)中了解更多关于高级配置选项的信息。
- en: Scheduler
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度器
- en: The Airflow Scheduler is a service that you kicked off when you ran the `airflow
    scheduler` command earlier in this chapter. When running, the scheduler is constantly
    monitoring DAGs and tasks and running any that have been scheduled to run or have
    had their dependencies met (in the case of tasks in a DAG).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow调度器是您在本章前面运行`airflow scheduler`命令时启动的服务。运行时，调度器不断监视DAG和任务，并运行已计划运行或已满足依赖关系的任务（对于DAG中的任务）。
- en: The scheduler uses the executor that is defined in the `[core]` section of the
    *airflow.cfg* file to run tasks. You can learn about executors in the following
    section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器使用在*airflow.cfg*文件的`[core]`部分中定义的执行器来运行任务。您可以在以下部分了解更多关于执行器的信息。
- en: Executors
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行器
- en: '*Executors* are what Airflow uses to run tasks that the Scheduler determines
    are ready to run. There are number of different types of executors that Airflow
    supports. By default, the `SequentialExecutor` is used. You can change the type
    of executor in the *airflow.cfg* file. Under the `core` section of the file, you’ll
    see a `executor` variable that can be set to any of the executor types listed
    in this section and in the Airflow documentation. As you can see, the `SequentialExecutor`
    is set when Airflow is first installed:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*执行器*是Airflow用来运行调度器确定已准备运行的任务的工具。Airflow支持多种类型的执行器。默认情况下，使用`SequentialExecutor`。您可以在*airflow.cfg*文件中更改执行器的类型。在文件的`core`部分下，您将看到一个`executor`变量，可以设置为此部分和Airflow文档中列出的任何执行器类型。正如您所看到的，当首次安装Airflow时设置了`SequentialExecutor`：'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Though the default, the `SequentialExecutor` is not meant for production use
    cases as it can run only one task at a time. It’s fine for testing simple DAGs,
    but that’s about it. However, it’s the only executor that is compatible with a
    SQLite database, so if you haven’t configured another database with Airflow, the
    `SequentialExecutor` is your only option.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管默认情况下，`SequentialExecutor`并不适合生产环境，因为它只能同时运行一个任务。对于测试简单的DAGs是可以接受的，但仅限于此。然而，它是与SQLite数据库兼容的唯一执行器，因此如果您还没有配置Airflow与其他数据库，那么`SequentialExecutor`就是您唯一的选择。
- en: If you plan to use Airflow at any sort of scale, I suggest using another executor
    such as the `CeleryExecutor`, `DaskExecutor`, or `KubernetesExecutor`. Your choice
    in part should depend on what infrastructure you’re most comfortable with. For
    example, to use the `CeleryExecutor`, you’ll need to set up a Celery broker using
    RabbitMQ, Amazon SQL, or Redis.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划在任何规模上使用Airflow，我建议你使用其他执行器，如`CeleryExecutor`、`DaskExecutor`或`KubernetesExecutor`。你的选择应该部分取决于你最熟悉的基础设施。例如，要使用`CeleryExecutor`，你需要设置一个Celery代理，使用RabbitMQ、Amazon
    SQL或Redis。
- en: Configuring the infrastructure required by each executor is out of the scope
    of this book, but the samples in this section will run even on the `SequentialExecutor`.
    You can learn more about Airflow executors in [their documentation](https://oreil.ly/YOplY).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 配置每个执行器所需的基础设施超出了本书的范围，但本节中的示例即使在`SequentialExecutor`上也可以运行。你可以在[它们的文档](https://oreil.ly/YOplY)中了解更多关于Airflow执行器的信息。
- en: Operators
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运算符
- en: 'Recall that each of the nodes in a DAG is a task. In Airflow, each task implements
    an *operator*. Operators are what actually execute scripts, commands, and other
    operations. There a number of operators. Here are the most common:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，DAG中的每个节点都是一个任务。在Airflow中，每个任务都实现了一个*运算符*。运算符实际上执行脚本、命令和其他操作。这里有一些常见的运算符：
- en: '`BashOperator`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BashOperator`'
- en: '`PythonOperator`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PythonOperator`'
- en: '`SimpleHttpOperator`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SimpleHttpOperator`'
- en: '`EmailOperator`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmailOperator`'
- en: '`SlackAPIOperator`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SlackAPIOperator`'
- en: '`MySqlOperator`, `PostgresOperator`, and other database-specific operators
    for executing SQL commands'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MySqlOperator`，`PostgresOperator`，以及其他特定于数据库的运算符，用于执行SQL命令。'
- en: '`Sensor`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sensor`'
- en: As you’ll learn in the following section, operators are instantiated and assigned
    to each task in a DAG.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在接下来的章节中学到的那样，运算符被实例化并分配给DAG中的每个任务。
- en: Building Airflow DAGs
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Airflow的DAG。
- en: Now that you know how Airflow works, it’s time to build a DAG! Though Airflow
    comes with a collection of sample DAGs, I’m going to follow some samples from
    earlier in this book and build a DAG that performs the steps of a sample ELT process.
    Specifically, it will extract data from a database, load it into a data warehouse,
    and then transform the data into a data model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了Airflow的工作原理，是时候构建一个DAG了！虽然Airflow附带了一些示例DAG，但我将按照本书之前的一些示例构建一个DAG，执行一个示例ELT过程的步骤。具体来说，它将从数据库中提取数据，将其加载到数据仓库中，然后将数据转换为数据模型。
- en: A Simple DAG
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的DAG
- en: Before I build the sample ELT DAG, it’s important to understand how DAGs are
    defined in Airflow. A DAG is defined in a Python script, where its structure and
    task dependencies are written in Python code. [Example 7-1](#ex_0701) is the definition
    of a simple DAG with three tasks. It’s referred to as a *DAG definition file*.
    Each task is defined as a `BashOperator`, with the first and third printing out
    some text and the second sleeping for three seconds. Though it doesn’t do anything
    particularly useful, it’s fully functional and representative of the DAG definitions
    you’ll write later.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建示例ELT DAG之前，了解如何在Airflow中定义DAG是很重要的。DAG在Python脚本中定义，其中结构和任务依赖关系都是用Python代码编写的。[示例
    7-1](#ex_0701) 是一个简单DAG的定义，包含三个任务。它被称为*DAG定义文件*。每个任务都定义为一个`BashOperator`，第一个和第三个任务打印一些文本，第二个任务休眠三秒钟。虽然它没有做任何特别有用的事情，但它是完全功能的，代表了稍后你将编写的DAG定义。
- en: Example 7-1\. simple_dag.py
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1\. simple_dag.py
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Before you move on and run the DAG, I’d like to point out the key features of
    the DAG definition file. First, like any Python script, necessary modules are
    imported. Next, the DAG itself is defined and assigned some properties such as
    a name (`simple_dag`), a schedule, a start date, and more. In fact, there are
    many more properties that I don’t define in this simple example that you may need
    to utilize and can find later in the chapter or in the official Airflow documentation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在你继续并运行DAG之前，我想指出DAG定义文件的关键特性。首先，像任何Python脚本一样，需要导入必要的模块。接下来，定义DAG本身并分配一些属性，如名称（`simple_dag`）、调度时间、开始日期等等。实际上，在这个简单示例中我没有定义的属性还有很多，你可能需要在本章节或官方Airflow文档中找到它们。
- en: Next, I define the three tasks in the DAG. All are of type `BashOperator`, meaning
    when executed, they’ll run a bash command. Each task is also assigned several
    properties, including an alphanumeric identifier called `task_id`, as well as
    the bash command that runs when the task is executed. As you’ll see later, each
    operator type has its own custom properties just as the `BashOperator` has `bash_command`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我定义了DAG中的三个任务。所有任务都是`BashOperator`类型，这意味着当执行时，它们将运行一个bash命令。每个任务还分配了几个属性，包括一个称为`task_id`的字母数字标识符，以及任务执行时运行的bash命令。正如稍后您将看到的那样，每种运算符类型都有其自定义属性，就像`BashOperator`有`bash_command`一样。
- en: The last two lines of the DAG definition define the dependencies between the
    tasks. The way to read it is that when the task `t1` completes, `t2` runs. When
    `t2` completes, `t3` runs. When you view the DAG in the Airflow web UI, you’ll
    see this reflected in both the tree and graph view.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DAG定义的最后两行定义了任务之间的依赖关系。阅读方法是，当任务`t1`完成时，`t2`运行。当`t2`完成时，`t3`运行。当您在Airflow Web
    UI中查看DAG时，您会在树形和图形视图中看到这一点。
- en: 'To run the DAG, you’ll need to save its definition file in the location where
    Airflow is looking for DAGs. You can find this location (or modify it) in the
    *airflow.cfg* file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行DAG，您需要将其定义文件保存在Airflow查找DAG的位置。您可以在*airflow.cfg*文件中找到这个位置（或修改它）：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Save the DAG definition in a file named *simple_dag.py* and place it in the
    *dags_folder* location. If you already have the Airflow web UI and Scheduler running,
    refresh the Airflow web UI, and you should see a DAG named `simple_dag` in the
    listing. If not, wait a few seconds and try again, or stop and restart the web
    service.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将DAG定义保存在名为*simple_dag.py*的文件中，并将其放置在*dags_folder*位置。如果您已经运行Airflow Web UI和调度器，请刷新Airflow
    Web UI，您应该在列表中看到名为`simple_dag`的DAG。如果没有，请等待几秒钟然后重试，或者停止并重新启动Web服务。
- en: Next, click the name of the DAG to view it in more detail. You’ll be able to
    see the graph and tree view of the DAG as well as the code that you just wrote.
    Ready to give it a try? Either on this screen or back on the home page, flip the
    toggle so that the DAG is set to On, as shown in [Figure 7-5](#fig_0705).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击DAG名称以查看更详细的信息。您将能够查看DAG的图形和树形视图，以及您刚刚编写的代码。准备好尝试了吗？在这个屏幕上或回到主页上，将切换按钮翻转，使DAG设置为On，如[图7-5](#fig_0705)所示。
- en: '![dppr 0705](Images/dppr_0705.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0705](Images/dppr_0705.png)'
- en: Figure 7-5\. An enabled DAG.
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5。一个启用的DAG。
- en: Recall that in the code, the `schedule_interval` property of the DAG is set
    to `timedelta(days=1)`. That means the DAG is set to run once a day at midnight
    UTC. You’ll see that schedule reflected on both the Airflow home page next to
    the DAG and on the DAG detail page. Also note that the `start_date` property of
    the DAG is set to `days_ago(1)`. That means the first run of the DAG is set one
    day prior to the current day. When the DAG is set to On, the first scheduled run
    is 0:00:00 UTC on the day prior the current day and thus will execute as soon
    as the executor has availability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆一下，在代码中，DAG的`schedule_interval`属性设置为`timedelta(days=1)`。这意味着DAG被设置为每天在UTC时间午夜运行一次。你会在Airflow首页和DAG详细页面上看到这个调度的反映。同时请注意，DAG的`start_date`属性设置为`days_ago(1)`。这意味着DAG的第一次运行被设置为当前日期的前一天。当DAG被设置为启动时，第一次调度运行是在UTC时间前一天的0:00:00，因此将在执行器有空闲时立即执行。
- en: You can check on the status of a DAG run on the DAG detail page or by navigating
    to Browse → DAG Runs on the top menu. From there you should see a visual status
    of the DAG run, as well as each task in the DAG. [Figure 7-6](#fig_0706) shows
    a run of the `simple_dag` example where all tasks succeeded. The final status
    of the DAG is marked as “success” near the top left of the screen.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在DAG详细页面上检查DAG运行的状态，或者通过导航到顶部菜单中的Browse → DAG Runs来查看。从那里，您应该能够看到DAG运行的可视状态，以及DAG中每个任务的状态。[图7-6](#fig_0706)显示了一个`simple_dag`示例的运行，其中所有任务都成功完成。DAG的最终状态标记为屏幕左上角的“success”。
- en: If you want to run the DAG on-demand, click the Trigger DAG button on the DAG
    detail page.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想按需运行DAG，请在DAG详细页面点击触发DAG按钮。
- en: '![dppr 0706](Images/dppr_0706.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0706](Images/dppr_0706.png)'
- en: Figure 7-6\. A graph view of an Airflow DAG.
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6。一个Airflow DAG的图形视图。
- en: An ELT Pipeline DAG
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个ELT管道的DAG
- en: Now that you know how to create a simple DAG, you can build a functional DAG
    for the extract, load, and transform steps of a data pipeline. This DAG consists
    of five tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何创建一个简单的DAG后，你可以构建一个用于数据管道的抽取、加载和转换步骤的功能性DAG。这个DAG包含五个任务。
- en: The first two tasks use `BashOperators` to execute two different Python scripts
    that each extract data from a Postgres database table and send the results as
    a CSV file to an S3 bucket. Though I won’t re-create the logic for the scripts
    here, you can find it in [“Extracting Data from a PostgreSQL Database”](ch04.xhtml#extract-data-postgressql).
    In fact, you can use any of the extraction examples from that chapter if you want
    to extract from a MySQL database, REST API, or MongoDB database.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个任务使用`BashOperator`来执行两个不同的Python脚本，每个脚本从Postgres数据库表中提取数据，并将结果作为CSV文件发送到S3存储桶。虽然我不会在这里重新创建脚本的逻辑，但你可以在[“从PostgreSQL数据库提取数据”](ch04.xhtml#extract-data-postgressql)中找到它。事实上，如果你想从MySQL数据库、REST
    API或MongoDB数据库中提取数据，你可以使用该章节中的任何提取示例。
- en: When each of those tasks completes, a corresponding task to load the data from
    the S3 bucket into a data warehouse is executed. Once again, each task uses a
    `BashOperator` to execute a Python script that contains the logic to load the
    CSV. You can find the sample code for that in [“Loading Data into a Snowflake
    Data Warehouse”](ch05.xhtml#load-data-snowflake) or [“Loading Data into a Redshift
    Warehouse”](ch05.xhtml#load-data-redshift), depending on which platform you use.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 每当这些任务完成时，将执行相应的任务，从S3存储桶中加载数据到数据仓库中。再次，每个任务都使用`BashOperator`来执行包含加载CSV逻辑的Python脚本。你可以在[“将数据加载到Snowflake数据仓库”](ch05.xhtml#load-data-snowflake)或者[“将数据加载到Redshift数据仓库”](ch05.xhtml#load-data-redshift)中找到相关示例代码，具体取决于你使用的平台。
- en: The final task in the DAG uses a `PostgresOperator` to execute a SQL script
    (stored in a *.sql* file) on the data warehouse to create a data model. You’ll
    recall this logic from [Chapter 6](ch06.xhtml#ch06). Together, these five tasks
    make up a simple pipeline following the ELT pattern first introduced in [Chapter 3](ch03.xhtml#ch03).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DAG中的最后一个任务使用`PostgresOperator`来执行一个SQL脚本（存储在*.sql*文件中），该脚本用于在数据仓库中创建数据模型。你可能还记得这一逻辑来自[第6章](ch06.xhtml#ch06)。这五个任务组成了一个简单的流水线，遵循了首次在[第3章](ch03.xhtml#ch03)中介绍的ELT模式。
- en: '[Figure 7-7](#fig_0707) shows a graph view of the DAG.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-7](#fig_0707)显示了DAG的图形视图。'
- en: '![dppr 0707](Images/dppr_0707.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0707](Images/dppr_0707.png)'
- en: Figure 7-7\. Graph view of the sample ELT DAG.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7。ELT DAG示例的图形视图。
- en: '[Example 7-2](#ex_0702) shows the definition of the DAG. Take a moment to read
    it, even though I’ll walk through it in detail as well. You can save it to the
    Airflow *dags* folder, but don’t enable it just yet.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 7-2](#ex_0702)展示了DAG的定义。花点时间阅读它，即使我也会详细讲解。你可以将其保存到Airflow的*dags*文件夹中，但暂时不要启用它。'
- en: Example 7-2\. elt_pipeline_sample.py
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2。elt_pipeline_sample.py
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From [Example 7-1](#ex_0701), you’ll recall importing some necessary Python
    packages and creating a `DAG` object. This time around, there’s one more package
    to import to make use of the `PostgresOperator` in the final task of the DAG.
    This DAG, like the previous sample, is scheduled to run once a day at midnight,
    starting the previous day.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从[示例 7-1](#ex_0701)中，你可能还记得导入了一些必要的Python包并创建了一个`DAG`对象。这一次，为了在DAG的最后一个任务中使用`PostgresOperator`，还需要导入一个额外的包。
- en: The final task utilizes a `PostgresOperator` to execute a SQL script stored
    in a directory on the same machine as Airflow on the data warehouse. The contents
    of the SQL script will look something like the data model transforms from [Chapter 6](ch06.xhtml#ch06).
    For example, given the DAG is extracting and loading an `Orders` table and a `Customers`
    table, I’ll use the following sample from [Chapter 6](ch06.xhtml#ch06). You can
    of course use any SQL query to match the data you’re working with.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个任务利用`PostgresOperator`执行存储在与Airflow同一台机器上的数据仓库上的目录中的SQL脚本。SQL脚本的内容看起来类似于[第6章](ch06.xhtml#ch06)中的数据模型转换。例如，考虑到DAG正在提取和加载`Orders`表和`Customers`表，我将使用来自[第6章](ch06.xhtml#ch06)的以下示例。当然，你可以使用任何SQL查询来匹配你正在处理的数据。
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Before you enable the DAG, there’s one more step. That is to set the connection
    to use for the `PostgresOperator`. As you can see in the DAG definition, there
    is a parameter called `postgres_conn_id` with a value of `redshift_dw`. You’ll
    need to define the `redshift_dw` connection in the Airflow web UI so that the
    `PostgresOperator` can execute the script.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用DAG之前，还有一个步骤。那就是设置用于`PostgresOperator`的连接。正如在DAG定义中所见，有一个名为`postgres_conn_id`的参数，其值为`redshift_dw`。你需要在Airflow的Web
    UI中定义`redshift_dw`连接，以便`PostgresOperator`可以执行该脚本。
- en: 'To do so, follow these steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，请按照以下步骤操作：
- en: Open the Airflow web UI and select Admin → Connections from the top navigation
    bar.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Airflow web UI，从顶部导航栏选择 Admin → Connections。
- en: Click the Create tab.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击创建标签页。
- en: Set Conn ID to *redshift_dw* (or whatever ID you want to use in your DAG definition
    file).
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Conn ID 设置为 *redshift_dw*（或者你在 DAG 定义文件中想要使用的任何 ID）。
- en: Select Postgres for Conn Type.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 Conn Type 为 Postgres。
- en: Set the connection information for your database.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置数据库的连接信息。
- en: Click Save.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击保存。
- en: Note that Amazon Redshift is compatible with Postgres connections, which I why
    I chose that Conn Type. You’ll find connections for Snowflake and dozens of other
    databases and platforms such as Spark.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Amazon Redshift 兼容 Postgres 连接，这就是我选择该 Conn Type 的原因。你会在 Snowflake 和数十种其他数据库和平台（如
    Spark）中找到连接。
- en: Now, you’re ready to enable the DAG. You can go back to the home page or view
    the DAG detail page and click the toggle to set the DAG to On. Because the schedule
    of the DAG is daily at midnight starting the previous day, a run will be scheduled
    immediately, and the DAG will execute. You can check on the status of a DAG run
    on the DAG detail page, or by navigating to Browse → DAG Runs on the top menu.
    As always, you can trigger a one-time run of the DAG using the Trigger DAG button
    on the DAG detail page.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以准备启用 DAG。你可以返回主页或查看 DAG 详细页面，然后点击切换按钮将 DAG 设为启用状态。因为 DAG 的调度是每天午夜开始的前一天，所以会立即安排一个运行并执行
    DAG。你可以在 DAG 详细页面查看 DAG 运行的状态，或者通过顶部菜单中的 Browse → DAG Runs 导航到 DAG 运行页面。如常，你也可以通过
    DAG 详细页面上的 Trigger DAG 按钮触发 DAG 的单次运行。
- en: Though this example is a bit simplified, it pieces together the steps of an
    ELT pipeline. In a more complex pipeline, you will find many more tasks. In addition
    to more data extracts and loads, there will likely be many data models, some of
    which are dependent on each other. Airflow makes it easy to ensure they are executed
    in the proper order. On most production deployments of Airflow you’ll find many
    DAGs for pipelines that may have some dependency on each other, or some external
    system or process. See [“Advanced Orchestration Configurations”](#advcd-orchestration)
    for some tips on managing such challenges.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子有些简化，但它整合了 ELT 管道的步骤。在更复杂的管道中，你会发现许多更多的任务。除了更多的数据抽取和加载之外，可能还会有许多数据模型，其中一些依赖于彼此。Airflow
    可以轻松确保它们按正确的顺序执行。在大多数 Airflow 的生产部署中，你会发现许多 DAG 用于可能彼此有依赖关系的管道，或者一些外部系统或流程。查看
    [“高级编排配置”](#advcd-orchestration) 获取有关管理此类挑战的一些提示。
- en: Additional Pipeline Tasks
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他管道任务
- en: In addition to the functional tasks in the sample ELT pipeline in the previous
    section, production-quality pipelines require other tasks, such as sending notifications
    to a Slack channel when a pipeline completes or fails, running data validation
    checks at various points in a pipeline, and more. Thankfully, all of these tasks
    can be handled by an Airflow DAG.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前一节样本 ELT 管道中的功能任务外，生产质量的管道还需要其他任务，例如在管道完成或失败时向 Slack 频道发送通知，以及在管道的各个点运行数据验证检查等。幸运的是，所有这些任务都可以由
    Airflow DAG 处理。
- en: Alerts and Notifications
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报和通知
- en: 'Although the Airflow web UI is a great place to view the status of DAG runs,
    it’s often better to receive an email when DAG fails (or even when it succeeds).
    There are a number of options for sending notifications. For example, if you want
    to get an email when a DAG fails, you can add the following parameters when you
    instantiate the `DAG` object in the definition file. You can also add these to
    tasks instead of the DAG if you only want to be notified for particular tasks:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Airflow web UI 是查看 DAG 运行状态的好地方，但当 DAG 失败（甚至成功）时，最好是收到电子邮件通知。有多种选项可用于发送通知。例如，如果你希望在
    DAG 失败时收到电子邮件，可以在定义文件中实例化 `DAG` 对象时添加以下参数。如果你只想对特定任务收到通知，也可以将这些参数添加到任务而不是 DAG：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before Airflow can send you email, you’ll need to provide the details of your
    SMTP server in the `[smtp]` section of *airflow.cfg*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Airflow 可以发送邮件给你之前，你需要在 *airflow.cfg* 文件的 `[smtp]` 部分提供你的 SMTP 服务器的详细信息。
- en: 'You can also use the `EmailOperator` in a task to send an email at any point
    in a DAG:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在 DAG 中的任务中使用 `EmailOperator` 发送电子邮件：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In addition to the `EmailOperator`, there are both official and community-supported
    operators for sending messages to Slack, Microsoft Teams, and other platforms.
    Of course, you can always create your own Python script to send a message to the
    platform of your choice and execute it using a `BashOperator`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`EmailOperator`之外，还有官方和社区支持的操作器，用于向Slack、Microsoft Teams和其他平台发送消息。当然，您也可以始终创建自己的Python脚本，将消息发送到您选择的平台，并使用`BashOperator`执行它。
- en: Data Validation Checks
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据验证检查
- en: '[Chapter 8](ch08.xhtml#ch08) discusses data validation and testing pipelines
    in more detail, but adding task to your Airflow DAGs to run validation on data
    is a good practice. As you’ll learn in that chapter, data validation may be implemented
    in a SQL or Python script or by calling some other external application. By now
    you know that Airflow can handle them all!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.xhtml#ch08)详细讨论了数据验证和测试管道，但在您的Airflow DAG中添加任务以验证数据是一个良好的实践。正如您将在该章中了解到的那样，数据验证可以通过SQL或Python脚本实现，或者通过调用其他外部应用程序来实现。到目前为止，您已经知道Airflow可以处理它们全部！'
- en: Advanced Orchestration Configurations
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级编排配置
- en: The previous section introduced a simple DAG that runs a full, end-to-end data
    pipeline that follows the ELT pattern. This section introduces a few challenges
    you may face when building more complex pipelines or find the need to coordinate
    multiple pipelines with shared dependencies or different schedules.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了一个简单的DAG，它运行了一个完整的端到端数据管道，遵循了ELT模式。本节介绍了在构建更复杂的管道或需要协调具有共享依赖或不同调度的多个管道时可能遇到的一些挑战。
- en: Coupled Versus Uncoupled Pipeline Tasks
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 耦合与非耦合管道任务
- en: Though the examples so far may make it seem that all steps (tasks) in a data
    pipeline are linked together cleanly, that is not always the case. Take a streaming
    data ingestion. For example, say Kafka is used to stream data to an S3 bucket
    where it is continuously loaded into a Snowflake data warehouse using Snowpipe
    (see Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迄今为止的示例可能会让人觉得数据管道中的所有步骤（任务）都能够清晰地链接在一起，但情况并非总是如此。以流数据摄入为例。例如，假设使用Kafka将数据流式传输到S3存储桶，然后使用Snowpipe将数据连续加载到Snowflake数据仓库中（请参阅第[4章](ch04.xhtml#ch04)和第[5章](ch05.xhtml#ch05)）。
- en: In this case, data is continuously flowing into the data warehouse, but the
    step to transform the data will still be scheduled to run at a set interval such
    as every 30 minutes. Unlike the DAG in [Example 7-2](#ex_0702), specific runs
    of the data ingestions are not direct dependencies of the task to transform the
    data into a data model. In such a situation, the tasks are said to be *uncoupled*
    as opposed to the *coupled* tasks in a DAG.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据持续流入数据仓库，但将数据转换的步骤仍然计划按照固定间隔运行，例如每30分钟一次。与示例[7-2](#ex_0702)中的DAG不同，数据摄入的特定运行并不是将数据转换为数据模型任务的直接依赖项。在这种情况下，任务被称为*非耦合*，而不是DAG中的*耦合*任务。
- en: Given this reality, data engineers must be thoughtful in how they orchestrate
    pipelines. Though there are no hard rules, it’s necessary to make consistent and
    resilient decisions throughout pipelines in order to manage decoupled tasks. In
    the example of streaming data ingestions and a scheduled transform step, the transform
    logic must take into account that data from two different sources (say the `Orders`
    and `Customers` tables) might be in slightly different states of refresh. The
    transform logic must take into account cases where there is an `Order` record
    without a corresponding `Customer` record, for example.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一现实，数据工程师必须在管道的编排方式上进行深思熟虑。虽然没有硬性规定，但在整个管道中进行一致和弹性的决策是管理解耦任务的必要条件。例如，在流数据导入和定期转换步骤的示例中，转换逻辑必须考虑来自两个不同来源（例如`Orders`和`Customers`表）的数据可能处于稍微不同的刷新状态。转换逻辑必须考虑存在仅具有订单记录而没有相应客户记录的情况，例如。
- en: When to Split Up DAGs
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时拆分DAG
- en: A key decision point in designing pipelines is determining what tasks belong
    together in a DAG. Though it’s possible to create a DAG with all the extract,
    load, transform, validation, and alerting tasks on your data infrastructure, it’s
    going to get overly complex pretty quickly.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计管道时的一个关键决策点是确定哪些任务应该组成一个DAG。虽然可能创建一个包含数据基础设施中所有的抽取、加载、转换、验证和警报任务的DAG，但这将很快变得非常复杂。
- en: 'Three factors go into determining when tasks should be broken out into multiple
    DAGs and when they should remain in a single DAG:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 决定何时将任务拆分成多个DAG以及何时将其保留在单个DAG中，有三个因素需要考虑：
- en: When the tasks need to run on different schedules, break into multiple DAGS
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务需要在不同的计划下运行时，拆分成多个DAG
- en: If you have some that only need to run daily, and some that run every 30 minutes,
    you should likely split them into two DAGs. Otherwise, you’ll waste time and resources
    to run some tasks 47 extra times per day! In a world where compute costs are frequently
    based on actual usage, that’s a big deal.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些任务只需每天运行一次，而其他任务每30分钟运行一次，你可能应该将它们拆分为两个DAG。否则，你将浪费时间和资源来额外运行某些任务47次！在计算成本经常基于实际使用的情况下，这是一个大问题。
- en: When a pipeline is truly independent, keep it separate
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个流水线是真正独立的时候，保持它们分开
- en: If the tasks in the pipeline only relate to each other, then keep them in a
    single DAG. Going back to [Example 7-2](#ex_0702), if the `Orders` and `Customer`
    table ingestions are only used by the data model in that DAG and no other tasks
    rely on the data model, then it makes sense for the DAG to remain on its own.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果管道中的任务仅相互关联，则保持它们在一个单独的DAG中。回顾[示例 7-2](#ex_0702)，如果`Orders`和`Customer`表的摄取仅由该DAG中的数据模型使用，并且没有其他任务依赖于数据模型，则保持DAG独立是有意义的。
- en: When a DAG becomes too complex, determine whether you can break it out logically
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个DAG变得过于复杂时，确定是否可以逻辑上分解它
- en: Though this is a bit subjective, if you find yourself looking at a graph view
    of a DAG with hundreds of tasks and a spider web of dependency arrows, it’s time
    to consider how to break up the DAG. Otherwise, you may find it hard to maintain
    in the future.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这有些主观，但如果你发现自己看着一个具有数百个任务和依赖箭头的DAG图形视图，那么现在是考虑如何拆分DAG的时候了。否则，将来可能会难以维护。
- en: Though it may seem like a headache to deal with multiple DAGs that may share
    dependencies (for example, a data ingestion), it’s often necessary. In the next
    section, I discuss how to implement cross-DAG dependencies in Airflow.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管处理可能共享依赖的多个DAG（例如数据摄取）可能看起来很头疼，但通常是必要的。在接下来的部分，我将讨论如何在Airflow中实现跨DAG的依赖关系。
- en: Coordinating Multiple DAGs with Sensors
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Sensor协调多个DAG
- en: Given the need for shared dependencies between DAGs, Airflow tasks can implement
    a special type of operator called a `Sensor`. An Airflow `Sensor` is designed
    to check the status of some external task or process and then continue execution
    of downstream dependencies in its DAG when the check criteria has been met.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 给定需要在DAG之间共享依赖的需求，Airflow任务可以实现一种称为`Sensor`的特殊操作器。Airflow的`Sensor`设计用于检查某些外部任务或进程的状态，一旦满足检查条件，就会继续执行其DAG中下游的依赖任务。
- en: If you find the need to coordinate two different Airflow DAGs, you can use a
    `ExternalTaskSensor` to check the status of a task in another DAG or the status
    of another DAG in full. [Example 7-3](#ex_0703) defines a DAG with two tasks.
    The first uses an `ExternalTaskSensor` to check the status of the `elt_pipeline_sample`
    DAG from an earlier section of this chapter. When that DAG completes, then the
    `Sensor` is marked as “success” and the second task (“task1”) is executed.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要协调两个不同的Airflow DAG，可以使用`ExternalTaskSensor`来检查另一个DAG中任务的状态或整个另一个DAG的状态。[示例 7-3](#ex_0703)定义了一个具有两个任务的DAG。第一个任务使用`ExternalTaskSensor`来检查本章前面部分中`elt_pipeline_sample`
    DAG的状态。当该DAG完成时，`Sensor`标记为“成功”，然后执行第二个任务（“task1”）。
- en: Example 7-3\. sensor_test.py
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-3\. sensor_test.py
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Figure 7-8](#fig_0708) shows the graph view of the DAG.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-8](#fig_0708)显示了DAG的图形视图。'
- en: '![dppr 0708](Images/dppr_0708.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0708](Images/dppr_0708.png)'
- en: Figure 7-8\. Graph view of the sample ELT DAG.
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 示例ELT DAG的图形视图。
- en: 'When enabled, this DAG will first kick off the `dag_sensor` task. Note its
    properties:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 启用时，此DAG将首先启动`dag_sensor`任务。请注意其属性：
- en: The `external_dag_id` is set to the ID of the DAG that the `Sensor` will monitor.
    In this case, it’s the `elt_pipeline_sample` DAG.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`external_dag_id`设置为`Sensor`将监视的DAG的ID。在本例中，它是`elt_pipeline_sample` DAG。'
- en: The `external_task_id` property is set to `None` in this case, which means that
    the `Sensor` is waiting on the entire `elt_pipeline_sample` DAG to complete successfully.
    If you were to instead set this to a particular `task_id` in the `elt_pipeline_sample`
    DAG, as soon as that `task_id` completed successfully, `sensor1` would complete
    and kick off `dummy_task`.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，`external_task_id`属性设置为`None`，这意味着`Sensor`正在等待整个`elt_pipeline_sample`
    DAG成功完成。如果你将其设置为`elt_pipeline_sample` DAG中的特定`task_id`，那么一旦该`task_id`成功完成，`sensor1`将完成并启动`dummy_task`。
- en: The `mode` property is set to `reschedule`. By default, sensors run with the
    `poke` mode. In that mode, the sensor blocks a worker slot while “poking” to check
    on the external task. Depending on what kind of executor you’re using, and how
    many tasks are being run, this is not ideal. In `reschedule` mode, the worker
    slot is released by rescheduling the task and thus opening up a worker slot until
    it is set to run again.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode` 属性设置为 `reschedule`。默认情况下，传感器以 `poke` 模式运行。在该模式下，传感器会阻塞一个工作槽位，同时“poke”以检查外部任务。根据您使用的执行器类型以及运行的任务数量，这并不理想。在
    `reschedule` 模式下，通过重新调度任务释放工作槽位，从而使工作槽位空闲，直到再次设置为运行。'
- en: The `timeout` parameter is set to the number of seconds the `ExternalTaskSensor`
    will continue to check its external dependency before it times out. It’s good
    practice to set a reasonable timeout here; otherwise, the DAG will continue to
    run in perpetuity.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` 参数设置为 `ExternalTaskSensor` 在超时之前继续检查其外部依赖的秒数。在这里设置一个合理的超时时间是一个良好的实践；否则，DAG
    将无限期地继续运行。'
- en: One thing to keep in mind is that DAGs run on a specific schedule, and thus
    the `Sensor` needs to check for a specific DAG run. By default, the `ExternalTaskSensor`
    will check for the run of the `external_dag_id` with the current schedule of the
    DAG it belongs to. Because both the `elt_pipeline_sample` and `sensor_test` DAGs
    run once per day at midnight, it’s fine to go with the default. However, if the
    two DAGs run on different schedules, then it’s best to specify which run of the
    `elt_pipeline_sample` the `Sensor` should check on. You can do this using either
    the `execution_delta` or `execution_date_fn` parameter of the `ExternalTaskSensor`.
    The `execution_date_fn` parameter defines a specific datetime of a DAG run, and
    I find it to be less useful than `execution_delta`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是 DAG 按特定的时间表运行，因此 `Sensor` 需要检查特定的 DAG 运行。默认情况下，`ExternalTaskSensor`
    将检查其所属 DAG 的当前时间表下的 `external_dag_id` 的运行。由于 `elt_pipeline_sample` 和 `sensor_test`
    DAG 每天午夜运行一次，使用默认设置就可以了。但是，如果两个 DAG 按不同的时间表运行，则最好指定 `Sensor` 应该检查哪个 `elt_pipeline_sample`
    的运行。您可以使用 `execution_delta` 或 `execution_date_fn` 参数来实现这一点。`execution_date_fn`
    参数定义了 DAG 运行的特定日期时间，但我发现它比 `execution_delta` 不太有用。
- en: 'The `execution_delta` parameter can be used to look back at a specific run
    of a DAG. For example, to look at the most recent run of a DAG that is scheduled
    for every 30 minutes, you would create a task that is defined like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`execution_delta` 参数可用于查看 DAG 的特定运行。例如，要查看每 30 分钟调度一次的 DAG 的最近运行，您可以创建一个任务，定义如下：'
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Managed Airflow Options
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 托管 Airflow 选项
- en: Though installing a simple Airflow instance is pretty straightforward, it becomes
    much more of challenge at production scale. Dealing with more complex executors
    to handle greater parallelization of tasks, keeping your instance up-to-date,
    and scaling underlying resources are jobs that not every data engineer has the
    time to take on.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然安装一个简单的 Airflow 实例相当简单，但在生产规模上就变得更具挑战性。处理更复杂的执行器以处理任务更大的并行性，保持您的实例最新，并扩展基础资源是不是每位数据工程师都有时间承担的工作。
- en: Like many other open source tools, there are several fully managed solutions
    for Airflow. Two of the most well known are [Cloud Composer](https://oreil.ly/ratu0)
    on Google Cloud and [Astronomer](https://oreil.ly/yM7d8). Though you’ll incur
    a monthly fee that will far exceed running Airflow on a server of your own, the
    administration aspects of Airflow are taken care of.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他开源工具类似，Airflow 有几种完全托管的解决方案。其中两个最著名的是 [Cloud Composer](https://oreil.ly/ratu0)
    在 Google Cloud 上和 [Astronomer](https://oreil.ly/yM7d8)。尽管您将支付月费，远远超过在自己的服务器上运行
    Airflow 的成本，但 Airflow 的管理方面将得到照顾。
- en: 'Similar to some of the build versus buy decisions throughout this book, hosting
    Airflow on your own versus choosing a managed solution depends on your particular
    situation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于本书中许多建设与购买的决策，根据您的具体情况，选择自托管 Airflow 还是选择托管解决方案。
- en: Do you have a systems operations team that can help you self-host?
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否有系统运维团队可以帮助您进行自托管？
- en: Do you have the budget to spend on a managed service?
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的预算是否足以支持托管服务？
- en: How many DAGs and tasks make up your pipelines? Are you running at a high enough
    scale to require more complex Airflow executors?
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的管道中有多少 DAG 和任务？您的运行规模是否足够高，需要更复杂的 Airflow 执行器？
- en: What are your security and privacy requirements? Are you comfortable allowing
    an external service to connect to your internal data and systems?
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的安全和隐私需求是什么？您是否愿意允许外部服务连接到您的内部数据和系统？
- en: Other Orchestration Frameworks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他编排框架
- en: Though this chapter is focused on Airflow, it’s by no means the only game in
    town. There are some other great orchestration frameworks such as [Luigi](https://oreil.ly/QU2FZ)
    and [Dagster](https://docs.dagster.io). [Kubeflow Pipelines](https://www.kubeflow.org),
    which is geared toward machine learning pipeline orchestration, is also well supported
    and popular in the ML community.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章重点讨论 Airflow，但这并不是唯一的选择。还有一些其他出色的编排框架，例如[Luigi](https://oreil.ly/QU2FZ)和[Dagster](https://docs.dagster.io)。而针对机器学习管道编排的[Kubeflow
    Pipelines](https://www.kubeflow.org)也受到了广泛支持，在机器学习社区中非常流行。
- en: When it comes to orchestration of the transform step for data models, [dbt](https://www.getdbt.com)
    by Fishtown Analytics is an excellent option. Like Airflow, it’s an open source
    product built in Python, so you can run it on your own at no cost or choose to
    pay for a managed version, called *dbt Cloud*. Some organizations choose to use
    Airflow or another general orchestrator for their data ingestions and to run things
    like Spark jobs, but then use dbt for transforming their data models. In such
    a case, dbt job runs are triggered by a task in an Airflow DAG, with dbt handling
    the dependencies between data models on its own. Some examples of using dbt are
    included in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据模型转换步骤的编排方面，由 Fishtown Analytics 提供的[dbt](https://www.getdbt.com)是一个出色的选择。像
    Airflow 一样，它是一个用 Python 构建的开源产品，因此您可以免费在自己的环境中运行，或选择付费的托管版本，称为*dbt Cloud*。一些组织选择使用
    Airflow 或其他通用编排工具进行数据摄取，并运行诸如 Spark 作业之类的任务，但然后使用 dbt 来转换其数据模型。在这种情况下，dbt 作业运行由
    Airflow DAG 中的任务触发，dbt 可以自行处理数据模型之间的依赖关系。有关使用 dbt 的示例，可以参考[第 9 章](ch09.xhtml#ch09)。
