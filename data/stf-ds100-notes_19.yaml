- en: 18Â  Estimators, Bias, and Variance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18  ä¼°è®¡å™¨ã€åå·®å’Œæ–¹å·®
- en: åŸæ–‡ï¼š[https://ds100.org/course-notes/probability_2/probability_2.html](https://ds100.org/course-notes/probability_2/probability_2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://ds100.org/course-notes/probability_2/probability_2.html](https://ds100.org/course-notes/probability_2/probability_2.html)'
- en: '*Learning Outcomes* ***   Explore commonly seen random variables like Bernoulli
    and Binomial distributions'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*å­¦ä¹ æˆæœ* ***   æ¢ç´¢å¸¸è§çš„éšæœºå˜é‡ï¼Œå¦‚ä¼¯åŠªåˆ©å’ŒäºŒé¡¹å¼åˆ†å¸ƒ'
- en: Apply the Central Limit Theorem to approximate parameters of a population
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨ä¸­å¿ƒæé™å®šç†æ¥è¿‘ä¼¼æ€»ä½“å‚æ•°
- en: Use sampled data to model an estimation of and infer the true underlying distribution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æŠ½æ ·æ•°æ®å¯¹çœŸå®çš„æ½œåœ¨åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ä¼°è®¡
- en: 'Estimate the true population distribution from a sample using the bootstrapping
    technique**  **Last time, we introduced the idea of random variables: numerical
    functions of a sample. Most of our work in the last lecture was done to build
    a background in probability and statistics. Now that weâ€™ve established some key
    ideas, weâ€™re in a good place to apply what weâ€™ve learned to our original goal
    â€“ understanding how the randomness of a sample impacts the model design process.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªåŠ©æ³•æŠ€æœ¯ä»æ ·æœ¬ä¸­ä¼°è®¡çœŸå®æ€»ä½“åˆ†å¸ƒ**  **ä¸Šæ¬¡ï¼Œæˆ‘ä»¬ä»‹ç»äº†éšæœºå˜é‡çš„æ¦‚å¿µï¼šæ ·æœ¬çš„æ•°å€¼å‡½æ•°ã€‚åœ¨ä¸Šä¸€è®²ä¸­ï¼Œæˆ‘ä»¬çš„å¤§éƒ¨åˆ†å·¥ä½œæ˜¯å»ºç«‹æ¦‚ç‡å’Œç»Ÿè®¡å­¦çš„èƒŒæ™¯ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»å»ºç«‹äº†ä¸€äº›å…³é”®çš„æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°æˆ‘ä»¬æœ€åˆçš„ç›®æ ‡ä¸Š
    - ç†è§£æ ·æœ¬çš„éšæœºæ€§å¦‚ä½•å½±å“æ¨¡å‹è®¾è®¡è¿‡ç¨‹ã€‚
- en: In this lecture, we will delve more deeply into the idea of fitting a model
    to a sample. Weâ€™ll explore how to re-express our modeling process in terms of
    random variables and use this new understanding to steer model complexity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨å°†æ¨¡å‹æ‹Ÿåˆåˆ°æ ·æœ¬çš„æƒ³æ³•ã€‚æˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ç”¨éšæœºå˜é‡é‡æ–°è¡¨è¾¾æˆ‘ä»¬çš„å»ºæ¨¡è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨è¿™ç§æ–°çš„ç†è§£æ¥å¼•å¯¼æ¨¡å‹çš„å¤æ‚æ€§ã€‚
- en: 18.1 Common Random Variables
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.1 å¸¸è§éšæœºå˜é‡
- en: 'There are several cases of random variables that appear often and have useful
    properties. Below are the ones we will explore further in this course. The numbers
    in parentheses are the parameters of a random variable, which are constants. Parameters
    define a random variableâ€™s shape (i.e., distribution) and its values. For this
    lecture, weâ€™ll focus more heavily on the bolded random variables and their special
    properties, but you should familiarize yourself with all the ones listed below:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§ç»å¸¸å‡ºç°å¹¶ä¸”å…·æœ‰æœ‰ç”¨ç‰¹æ€§çš„éšæœºå˜é‡æƒ…å†µã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬è¯¾ç¨‹ä¸­è¿›ä¸€æ­¥æ¢è®¨çš„æƒ…å†µã€‚æ‹¬å·ä¸­çš„æ•°å­—æ˜¯éšæœºå˜é‡çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°æ˜¯å¸¸æ•°ã€‚å‚æ•°å®šä¹‰äº†éšæœºå˜é‡çš„å½¢çŠ¶ï¼ˆå³åˆ†å¸ƒï¼‰å’Œå…¶å€¼ã€‚åœ¨æœ¬è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†æ›´åŠ é‡ç‚¹å…³æ³¨åŠ ç²—çš„éšæœºå˜é‡åŠå…¶ç‰¹æ®Šæ€§è´¨ï¼Œä½†ä½ åº”è¯¥ç†Ÿæ‚‰ä¸‹é¢åˆ—å‡ºçš„æ‰€æœ‰éšæœºå˜é‡ï¼š
- en: '**Bernoulli(p)**'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼¯åŠªåˆ©(p)**'
- en: Takes on value 1 with probability p, and 0 with probability 1 - p.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡på–å€¼1ï¼Œä»¥æ¦‚ç‡1-på–å€¼0ã€‚
- en: AKA the â€œindicatorâ€ random variable.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆç§°â€œæŒ‡ç¤ºâ€éšæœºå˜é‡ã€‚
- en: Let X be a Bernoulli(p) random variable
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾Xæ˜¯ä¸€ä¸ªä¼¯åŠªåˆ©(p)éšæœºå˜é‡
- en: \(\mathbb{E}[X] = 1 * p + 0 * (1-p) = p\)
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{E}[X] = 1 * p + 0 * (1-p) = p\)
- en: \(\mathbb{E}[X^2] = 1^2 * p + 0 * (1-p) = p\)
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{E}[X^2] = 1^2 * p + 0 * (1-p) = p\)
- en: \(\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = p - p^2 = p(1-p)\)
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = p - p^2 = p(1-p)\)
- en: '**Binomial(n, p)**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äºŒé¡¹å¼(n, p)**'
- en: Number of 1s in \(n\) independent Bernoulli(p) trials.
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(n\) ç‹¬ç«‹ä¼¯åŠªåˆ©(p)è¯•éªŒä¸­çš„1çš„æ•°é‡ã€‚
- en: Let \(Y\) be a Binomial(n, p) random variable.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾\(Y\)æ˜¯ä¸€ä¸ªäºŒé¡¹å¼(n, p)éšæœºå˜é‡ã€‚
- en: 'The distribution of \(Y\) is given by the binomial formula, and we can write
    \(Y = \sum_{i=1}^n X_i\) where:'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(Y\)çš„åˆ†å¸ƒç”±äºŒé¡¹å¼å…¬å¼ç»™å‡ºï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆ\(Y = \sum_{i=1}^n X_i\)ï¼Œå…¶ä¸­ï¼š
- en: \(X_i\) s the indicator of success on trial i. \(X_i = 1\) if trial i is a success,
    else 0.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(X_i\)æ˜¯ç¬¬\(i\)æ¬¡è¯•éªŒæˆåŠŸçš„æŒ‡ç¤ºã€‚å¦‚æœç¬¬\(i\)æ¬¡è¯•éªŒæˆåŠŸï¼Œåˆ™\(X_i = 1\)ï¼Œå¦åˆ™ä¸º0ã€‚
- en: All \(X_i\) are i.i.d. and Bernoulli(p).
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„\(X_i\)éƒ½æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¼¯åŠªåˆ©(p)ã€‚
- en: \(\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\)
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\)
- en: \(\text{Var}(X) = \sum_{i=1}^n \text{Var}(X_i) = np(1-p)\)
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\text{Var}(X) = \sum_{i=1}^n \text{Var}(X_i) = np(1-p)\)
- en: \(X_i\)â€™s are independent, so \(\text{Cov}(X_i, X_j) = 0\) for all i, j.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(X_i\)æ˜¯ç‹¬ç«‹çš„ï¼Œæ‰€ä»¥å¯¹äºæ‰€æœ‰çš„i, jï¼Œ\(\text{Cov}(X_i, X_j) = 0\)ã€‚
- en: Uniform on a finite set of values
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰é™å€¼é›†ä¸Šå‡åŒ€åˆ†å¸ƒ
- en: Probability of each value is 1 / (number of possible values).
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå€¼çš„æ¦‚ç‡æ˜¯1 / (å¯èƒ½çš„å€¼çš„æ•°é‡)ã€‚
- en: For example, a standard/fair die.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸€ä¸ªæ ‡å‡†/å…¬å¹³çš„éª°å­ã€‚
- en: Uniform on the unit interval (0, 1)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä½åŒºé—´(0, 1)ä¸Šå‡åŒ€åˆ†å¸ƒ
- en: Density is flat at 1 on (0, 1) and 0 elsewhere.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯†åº¦åœ¨(0, 1)ä¸Šä¸º1ï¼Œåœ¨å…¶ä»–åœ°æ–¹ä¸º0ã€‚
- en: Normal(\(\mu, \sigma^2\))
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£æ€(\(\mu, \sigma^2\))
- en: \(f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right)\)
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right)\)
- en: 18.1.1 Example
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.1.1 ä¾‹å­
- en: Suppose you win cash based on the number of heads you get in a series of 20
    coin flips. Let \(X_i = 1\) if the \(i\)-th coin is heads, 0 otherwise. Which
    payout strategy would you choose?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æ ¹æ®20æ¬¡æŠ›ç¡¬å¸ä¸­å¾—åˆ°çš„æ­£é¢æ•°é‡èµ¢å¾—ç°é‡‘ã€‚å¦‚æœç¬¬\(i\)æ¬¡æŠ›ç¡¬å¸å¾—åˆ°æ­£é¢ï¼Œåˆ™ä»¤\(X_i = 1\)ï¼Œå¦åˆ™ä¸º0ã€‚ä½ ä¼šé€‰æ‹©å“ªç§æ”¯ä»˜ç­–ç•¥ï¼Ÿ
- en: A. \(Y_A = 10 * X_1 + 10 * X_2\)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: A. \(Y_A = 10 * X_1 + 10 * X_2\)
- en: B. \(Y_B = \sum_{i=1}^{20} X_i\)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: B. \(Y_B = \sum_{i=1}^{20} X_i\)
- en: C. \(Y_C = 20 * X_1\)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: C. \(Y_C = 20 * X_1\)
- en: '*Solution* **Let \(X_1, X_2, ... X_{20}\) be 20 i.i.d Bernoulli(0.5) random
    variables. Since the \(X_i\)â€™s are independent, \(\text{Cov}(X_i, X_j) = 0\) for
    all pairs \(i, j\). Additionally, Since \(X_i\) is Bernoulli(0.5), we know that
    \(\mathbb{E}[X] = p = 0.5\) and \(\text{Var}(X) = p(1-p) = 0.25\). We can calculate
    the following for each scenario:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*è§£å†³æ–¹æ¡ˆ* **è®¾\(X_1, X_2, ... X_{20}\)æ˜¯20ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¼¯åŠªåˆ©(0.5)éšæœºå˜é‡ã€‚ç”±äº\(X_i\)æ˜¯ç‹¬ç«‹çš„ï¼Œå¯¹äºæ‰€æœ‰çš„\(i,
    j\)å¯¹ï¼Œ\(\text{Cov}(X_i, X_j) = 0\)ã€‚å¦å¤–ï¼Œç”±äº\(X_i\)æ˜¯ä¼¯åŠªåˆ©(0.5)ï¼Œæˆ‘ä»¬çŸ¥é“\(\mathbb{E}[X] =
    p = 0.5\)å’Œ\(\text{Var}(X) = p(1-p) = 0.25\)ã€‚æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ç§æƒ…å†µçš„å¦‚ä¸‹å†…å®¹ï¼š'
- en: '|  | A. \(Y_A = 10 * X_1 + 10 * X_2\) | B. \(Y_B = \sum_{i=1}^{20} X_i\) |
    C. \(Y_C = 20 * X_1\) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | A. \(Y_A = 10 * X_1 + 10 * X_2\) | B. \(Y_B = \sum_{i=1}^{20} X_i\) |
    C. \(Y_C = 20 * X_1\) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Expectation | \(\mathbb{E}[Y_A] = 10 (0.5) + 10(0.5) = 10\) | \(\mathbb{E}[Y_B]
    = 0.5 + ... + 0.5 = 10\) | \(\mathbb{E}[Y_C] = 20(0.5) = 10\) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| æœŸæœ› | \(\mathbb{E}[Y_A] = 10 (0.5) + 10(0.5) = 10\) | \(\mathbb{E}[Y_B] =
    0.5 + ... + 0.5 = 10\) | \(\mathbb{E}[Y_C] = 20(0.5) = 10\) |'
- en: '| Variance | \(\text{Var}(Y_A) = 10^2 (0.25) + 10^2 (0.25) = 50\) | \(\text{Var}(Y_B)
    = 0.25 + ... + 0.25 = 5\) | \(\text{Var}(Y_C) = 20^2 (0.25) = 100\) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹å·® | \(\text{Var}(Y_A) = 10^2 (0.25) + 10^2 (0.25) = 50\) | \(\text{Var}(Y_B)
    = 0.25 + ... + 0.25 = 5\) | \(\text{Var}(Y_C) = 20^2 (0.25) = 100\) |'
- en: '| Standard Deviation | \(\text{SD}(Y_A) \approx 7.07\) | \(\text{SD}(Y_B) \approx
    2.24\) | \(\text{SD}(Y_C) = 10\) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| æ ‡å‡†å·® | \(\text{SD}(Y_A) \approx 7.07\) | \(\text{SD}(Y_B) \approx 2.24\) |
    \(\text{SD}(Y_C) = 10\) |'
- en: As we can see, all the scenarios have the same expected value but different
    variances. The higher the variance, the greater the risk and uncertainty, so the
    â€œrightâ€ strategy depends on your personal preference. Would you choose the â€œsafestâ€
    option B, the most â€œriskyâ€ option C, or somewhere in the middle (option A)?**  **##
    18.2 Sample Statistics
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ‰€æœ‰çš„æƒ…æ™¯éƒ½æœ‰ç›¸åŒçš„æœŸæœ›å€¼ï¼Œä½†æ–¹å·®ä¸åŒã€‚æ–¹å·®è¶Šå¤§ï¼Œé£é™©å’Œä¸ç¡®å®šæ€§å°±è¶Šå¤§ï¼Œå› æ­¤â€œæ­£ç¡®â€çš„ç­–ç•¥å–å†³äºä¸ªäººçš„åå¥½ã€‚ä½ ä¼šé€‰æ‹©â€œæœ€å®‰å…¨â€çš„é€‰é¡¹Bï¼Œæœ€â€œå†’é™©â€çš„é€‰é¡¹Cï¼Œè¿˜æ˜¯ä»‹äºä¸¤è€…ä¹‹é—´çš„é€‰é¡¹Aï¼Ÿ**  **##
    18.2 æ ·æœ¬ç»Ÿè®¡
- en: 'Today, weâ€™ve talked extensively about populations; if we know the distribution
    of a random variable, we can reliably compute expectation, variance, functions
    of the random variable, etc. Note that:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©ï¼Œæˆ‘ä»¬å·²ç»å¹¿æ³›è®¨è®ºäº†æ€»ä½“ï¼›å¦‚æœæˆ‘ä»¬çŸ¥é“éšæœºå˜é‡çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥å¯é åœ°è®¡ç®—æœŸæœ›ã€æ–¹å·®ã€éšæœºå˜é‡çš„å‡½æ•°ç­‰ã€‚è¯·æ³¨æ„ï¼š
- en: The distribution of a *population* describes how a random variable behaves across
    *all* individuals of interest.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ€»ä½“*çš„åˆ†å¸ƒæè¿°äº†éšæœºå˜é‡åœ¨*æ‰€æœ‰*æ„Ÿå…´è¶£çš„ä¸ªä½“ä¸­çš„è¡Œä¸ºã€‚'
- en: The distribution of a *sample* describes how a random variable behaves in a
    *specific sample* from the population.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ ·æœ¬*çš„åˆ†å¸ƒæè¿°äº†éšæœºå˜é‡åœ¨æ¥è‡ªæ€»ä½“çš„*ç‰¹å®šæ ·æœ¬*ä¸­çš„è¡Œä¸ºã€‚'
- en: In Data Science, however, we often do not have access to the whole population,
    so we donâ€™t know its distribution. As such, we need to collect a sample and use
    its distribution to estimate or infer properties of the population. In cases like
    these, we can take several samples of size \(n\) from the population (an easy
    way to do this is using `df.sample(n, replace=True)`), and compute the mean of
    each *sample*. When sampling, we make the (big) assumption that we sample uniformly
    at random with replacement from the population; each observation in our sample
    is a random variable drawn i.i.d from our population distribution. Remember that
    our sample mean is a random variable since it depends on our randomly drawn sample!
    On the other hand, our population mean is simply a number (a fixed value).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸æ— æ³•æ¥è§¦åˆ°æ•´ä¸ªæ€»ä½“ï¼Œå› æ­¤æˆ‘ä»¬ä¸çŸ¥é“å®ƒçš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†ä¸€ä¸ªæ ·æœ¬ï¼Œå¹¶ä½¿ç”¨å®ƒçš„åˆ†å¸ƒæ¥ä¼°è®¡æˆ–æ¨æ–­æ€»ä½“çš„å±æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ€»ä½“ä¸­å–å‡ ä¸ªå¤§å°ä¸º\(n\)çš„æ ·æœ¬ï¼ˆä¸€ä¸ªç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨`df.sample(n,
    replace=True)`ï¼‰ï¼Œå¹¶è®¡ç®—æ¯ä¸ª*æ ·æœ¬*çš„å‡å€¼ã€‚åœ¨æŠ½æ ·æ—¶ï¼Œæˆ‘ä»¬åšå‡ºï¼ˆå¾ˆå¤§çš„ï¼‰å‡è®¾ï¼Œå³æˆ‘ä»¬ä»æ€»ä½“ä¸­å‡åŒ€éšæœºåœ°è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·ï¼›æˆ‘ä»¬æ ·æœ¬ä¸­çš„æ¯ä¸ªè§‚å¯Ÿéƒ½æ˜¯ä»æˆ‘ä»¬çš„æ€»ä½“åˆ†å¸ƒä¸­ç‹¬ç«‹åŒåˆ†å¸ƒåœ°éšæœºæŠ½å–çš„éšæœºå˜é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬çš„æ ·æœ¬å‡å€¼æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå› ä¸ºå®ƒå–å†³äºæˆ‘ä»¬éšæœºæŠ½å–çš„æ ·æœ¬ï¼å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ€»ä½“å‡å€¼åªæ˜¯ä¸€ä¸ªæ•°å­—ï¼ˆä¸€ä¸ªå›ºå®šçš„å€¼ï¼‰ã€‚
- en: 18.2.1 Sample Mean
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.1 æ ·æœ¬å‡å€¼
- en: Consider an i.i.d. sample \(X_1, X_2, ..., X_n\) drawn from a population with
    mean ğœ‡ and SD ğœ. We define the sample mean as \[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n
    X_i\]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªä»å…·æœ‰å‡å€¼ğœ‡å’Œæ ‡å‡†å·®ğœçš„æ€»ä½“ä¸­æŠ½å–çš„i.i.d.æ ·æœ¬\(X_1, X_2, ..., X_n\)ã€‚æˆ‘ä»¬å®šä¹‰æ ·æœ¬å‡å€¼ä¸º\[\bar{X}_n =
    \frac{1}{n} \sum_{i=1}^n X_i\]
- en: 'The expectation of the sample mean is given by: \[\begin{align} \mathbb{E}[\bar{X}_n]
    &= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] \\ &= \frac{1}{n} (n \mu) \\ &= \mu
    \end{align}\]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬å‡å€¼çš„æœŸæœ›å€¼ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š\[\begin{align} \mathbb{E}[\bar{X}_n] &= \frac{1}{n} \sum_{i=1}^n
    \mathbb{E}[X_i] \\ &= \frac{1}{n} (n \mu) \\ &= \mu \end{align}\]
- en: 'The variance is given by: \[\begin{align} \text{Var}(\bar{X}_n) &= \frac{1}{n^2}
    \text{Var}( \sum_{i=1}^n X_i) \\ &= \frac{1}{n^2} \left( \sum_{i=1}^n \text{Var}(X_i)
    \right) \\ &= \frac{1}{n^2} (n \sigma^2) = \frac{\sigma^2}{n} \end{align}\]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹å·®ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š\[\begin{align} \text{Var}(\bar{X}_n) &= \frac{1}{n^2} \text{Var}(
    \sum_{i=1}^n X_i) \\ &= \frac{1}{n^2} \left( \sum_{i=1}^n \text{Var}(X_i) \right)
    \\ &= \frac{1}{n^2} (n \sigma^2) = \frac{\sigma^2}{n} \end{align}\]
- en: \(\bar{X}_n\) is normally distributed by the Central Limit Theorem (CLT).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bar{X}_n\)æ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼ˆCLTï¼‰å‘ˆæ­£æ€åˆ†å¸ƒã€‚
- en: 18.2.2 Central Limit Theorem
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.2 ä¸­å¿ƒæé™å®šç†
- en: In [Data 8](https://inferentialthinking.com/chapters/14/4/Central_Limit_Theorem.html?)
    and in the previous lecture, you encountered the **Central Limit Theorem (CLT)**.
    This is a powerful theorem for estimating the distribution of a population with
    mean \(\mu\) and standard deviation \(\sigma\) from a collection of smaller samples.
    The CLT tells us that if an i.i.d sample of size \(n\) is large, then the probability
    distribution of the **sample mean** is **roughly normal** with mean \(\mu\) and
    SD of \(\frac{\sigma}{\sqrt{n}}\). More generally, any theorem that provides the
    rough distribution of a statistic and **doesnâ€™t need the distribution of the population**
    is valuable to data scientists! This is because we rarely know a lot about the
    population.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Data 8](https://inferentialthinking.com/chapters/14/4/Central_Limit_Theorem.html?)å’Œä¹‹å‰çš„è®²åº§ä¸­ï¼Œä½ é‡åˆ°äº†**ä¸­å¿ƒæé™å®šç†ï¼ˆCLTï¼‰**ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å®šç†ï¼Œç”¨äºä»ä¸€ç³»åˆ—è¾ƒå°çš„æ ·æœ¬ä¸­ä¼°è®¡å…·æœ‰å‡å€¼\(\mu\)å’Œæ ‡å‡†å·®\(\sigma\)çš„æ€»ä½“çš„åˆ†å¸ƒã€‚ä¸­å¿ƒæé™å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æœä¸€ä¸ªå¤§å°ä¸º\(n\)çš„i.i.dæ ·æœ¬å¾ˆå¤§ï¼Œé‚£ä¹ˆ**æ ·æœ¬å‡å€¼**çš„æ¦‚ç‡åˆ†å¸ƒ**å¤§è‡´æ­£æ€**ï¼Œå‡å€¼ä¸º\(\mu\)ï¼Œæ ‡å‡†å·®ä¸º\(\frac{\sigma}{\sqrt{n}}\)ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œä»»ä½•æä¾›ç»Ÿè®¡é‡ç²—ç•¥åˆ†å¸ƒå¹¶ä¸”**ä¸éœ€è¦æ€»ä½“åˆ†å¸ƒ**çš„å®šç†å¯¹äºæ•°æ®ç§‘å­¦å®¶æ¥è¯´éƒ½æ˜¯æœ‰ä»·å€¼çš„ï¼è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å¾ˆå°‘å¯¹æ€»ä½“äº†è§£å¾ˆå¤šã€‚
- en: '![clt](../Images/727e911a7179e45a5d63f8ada1128247.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![clt](../Images/727e911a7179e45a5d63f8ada1128247.png)'
- en: Importantly, the CLT assumes that each observation in our samples is drawn i.i.d
    from the distribution of the population. In addition, the CLT is accurate only
    when \(n\) is â€œlargeâ€, but what counts as a â€œlargeâ€ sample size depends on the
    specific distribution. If a population is highly symmetric and unimodal, we could
    need as few as \(n=20\); if a population is very skewed, we need a larger \(n\).
    If in doubt, you can bootstrap the sample mean and see if the bootstrapped distribution
    is bell-shaped. Classes like Data 140 investigate this idea in great detail.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯ï¼Œä¸­å¿ƒæé™å®šç†å‡è®¾æˆ‘ä»¬æ ·æœ¬ä¸­çš„æ¯ä¸ªè§‚å¯Ÿéƒ½æ˜¯ä»æ€»ä½“çš„åˆ†å¸ƒä¸­æŠ½å–çš„i.i.dã€‚æ­¤å¤–ï¼Œä¸­å¿ƒæé™å®šç†ä»…åœ¨\(n\)â€œå¤§â€æ—¶æ‰å‡†ç¡®ï¼Œä½†ä»€ä¹ˆæ ·çš„â€œå¤§â€æ ·æœ¬é‡å–å†³äºç‰¹å®šçš„åˆ†å¸ƒã€‚å¦‚æœä¸€ä¸ªæ€»ä½“é«˜åº¦å¯¹ç§°å’Œå•å³°ï¼Œæˆ‘ä»¬å¯èƒ½åªéœ€è¦\(n=20\)ï¼›å¦‚æœä¸€ä¸ªæ€»ä½“éå¸¸å€¾æ–œï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤§çš„\(n\)ã€‚å¦‚æœæœ‰ç–‘é—®ï¼Œå¯ä»¥å¯¹æ ·æœ¬å‡å€¼è¿›è¡Œè‡ªä¸¾ï¼Œå¹¶æŸ¥çœ‹è‡ªä¸¾åˆ†å¸ƒæ˜¯å¦å‘ˆé’Ÿå½¢ã€‚åƒData
    140è¿™æ ·çš„è¯¾ç¨‹ä¼šå¯¹è¿™ä¸ªæƒ³æ³•è¿›è¡Œè¯¦ç»†çš„æ¢è®¨ã€‚
- en: For a more in-depth demo, check out [onlinestatbook](https://onlinestatbook.com/stat_sim/sampling_dist/).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´è¯¦ç»†çš„æ¼”ç¤ºï¼Œè¯·æŸ¥çœ‹[onlinestatbook](https://onlinestatbook.com/stat_sim/sampling_dist/)ã€‚
- en: 18.2.3 Using the Sample Mean to Estimate the Population Mean
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.3 ä½¿ç”¨æ ·æœ¬å‡å€¼ä¼°è®¡æ€»ä½“å‡å€¼
- en: Now letâ€™s say we want to use the sample mean to **estimate** the population
    mean, for example, the average height of Cal undergraduates. We can typically
    collect a **single sample**, which has just one average. However, what if we happened,
    by random chance, to draw a sample with a different mean or spread than that of
    the population? We might get a skewed view of how the population behaves (consider
    the extreme case where we happen to sample the exact same value \(n\) times!).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨æ ·æœ¬å‡å€¼æ¥**ä¼°è®¡**æ€»ä½“å‡å€¼ï¼Œä¾‹å¦‚ï¼ŒåŠ å·å¤§å­¦æœ¬ç§‘ç”Ÿçš„å¹³å‡èº«é«˜ã€‚é€šå¸¸æˆ‘ä»¬å¯ä»¥æ”¶é›†ä¸€ä¸ª**å•ä¸€æ ·æœ¬**ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªå¹³å‡å€¼ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç¢°å·§ä»¥éšæœºæ–¹å¼æŠ½å–äº†ä¸€ä¸ªå…·æœ‰ä¸åŒå‡å€¼æˆ–æ‰©å±•æ€§çš„æ ·æœ¬ï¼Œä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿæˆ‘ä»¬å¯èƒ½ä¼šå¯¹æ€»ä½“è¡Œä¸ºæœ‰ä¸€ä¸ªåæ–œçš„çœ‹æ³•ï¼ˆè€ƒè™‘æç«¯æƒ…å†µï¼Œæˆ‘ä»¬ç¢°å·§æŠ½å–äº†ç›¸åŒçš„å€¼
    \(n\) æ¬¡ï¼ï¼‰ã€‚
- en: '![clt](../Images/25c1dbe1b23509eeff51ea2073a35f3f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![clt](../Images/25c1dbe1b23509eeff51ea2073a35f3f.png)'
- en: For example, notice the difference in variation between these two distributions
    that are different in sample size. The distribution with a bigger sample size
    (\(n=800\)) is tighter around the mean than the distribution with a smaller sample
    size (\(n=200\)). Try plugging in these values into the standard deviation equation
    for the normal distribution to make sense of this!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ³¨æ„è¿™ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å˜åŒ–å·®å¼‚ï¼Œè¿™ä¸¤ä¸ªåˆ†å¸ƒåœ¨æ ·æœ¬å¤§å°ä¸Šæ˜¯ä¸åŒçš„ã€‚æ ·æœ¬é‡æ›´å¤§çš„åˆ†å¸ƒï¼ˆ\(n=800\)ï¼‰æ¯”æ ·æœ¬é‡è¾ƒå°çš„åˆ†å¸ƒï¼ˆ\(n=200\)ï¼‰æ›´ç´§å¯†åœ°å›´ç»•å‡å€¼ã€‚å°è¯•å°†è¿™äº›å€¼ä»£å…¥æ­£æ€åˆ†å¸ƒçš„æ ‡å‡†åå·®æ–¹ç¨‹ä¸­ï¼Œä»¥ç†è§£è¿™ä¸€ç‚¹ï¼
- en: Applying the CLT allows us to make sense of all of this and resolve this issue.
    By drawing many samples, we can consider how the sample distribution varies across
    multiple subsets of the data. This allows us to approximate the properties of
    the population without the need to survey every single member.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ä¸­å¿ƒæé™å®šç†ä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è§£æ‰€æœ‰è¿™äº›å¹¶è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡æŠ½å–è®¸å¤šæ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ ·æœ¬åˆ†å¸ƒåœ¨æ•°æ®çš„å¤šä¸ªå­é›†ä¸­çš„å˜åŒ–ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿‘ä¼¼æ€»ä½“çš„å±æ€§ï¼Œè€Œæ— éœ€è°ƒæŸ¥æ¯ä¸ªæˆå‘˜ã€‚
- en: 'Given this potential variance, it is also important that we consider the **average
    value and spread** of all possible sample means, and what this means for how big
    \(n\) should be. For every sample size, the expected value of the sample mean
    is the population mean: \[\mathbb{E}[\bar{X}_n] = \mu\]. We call the sample mean
    an **unbiased estimator** of the population mean and will explore this idea more
    in the next lecture.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè¿™ç§æ½œåœ¨çš„å·®å¼‚ï¼Œæˆ‘ä»¬è¿˜è¦è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„æ ·æœ¬å‡å€¼çš„**å¹³å‡å€¼å’Œæ‰©å±•æ€§**ï¼Œä»¥åŠè¿™å¯¹ \(n\) åº”è¯¥æœ‰å¤šå¤§çš„å½±å“ã€‚å¯¹äºæ¯ä¸ªæ ·æœ¬é‡ï¼Œæ ·æœ¬å‡å€¼çš„æœŸæœ›å€¼æ˜¯æ€»ä½“å‡å€¼ï¼š\[\mathbb{E}[\bar{X}_n]
    = \mu\]ã€‚æˆ‘ä»¬ç§°æ ·æœ¬å‡å€¼æ˜¯æ€»ä½“å‡å€¼çš„**æ— åä¼°è®¡é‡**ï¼Œå¹¶å°†åœ¨ä¸‹ä¸€è®²ä¸­æ›´å¤šåœ°æ¢è®¨è¿™ä¸ªæƒ³æ³•ã€‚
- en: '*Data 8 Recap: Square Root Law* **The square root law ([Data 8](https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html#the-square-root-law))
    states that if you increase the sample size by a factor, the SD decreases by the
    square root of the factor. \[\text{SD}(\bar{X_n}) = \frac{\sigma}{\sqrt{n}}\]
    The sample mean is more likely to be close to the population mean if we have a
    larger sample size.**  **## 18.3 Prediction and Inference'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*Data 8 Recap: å¹³æ–¹æ ¹å®šå¾‹* **å¹³æ–¹æ ¹å®šå¾‹ï¼ˆ[Data 8](https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html#the-square-root-law)ï¼‰æŒ‡å‡ºï¼Œå¦‚æœå°†æ ·æœ¬é‡å¢åŠ ä¸€ä¸ªå› å­ï¼Œæ ‡å‡†åå·®å°†å‡å°‘è¯¥å› å­çš„å¹³æ–¹æ ¹ã€‚
    \[\text{SD}(\bar{X_n}) = \frac{\sigma}{\sqrt{n}}\] å¦‚æœæˆ‘ä»¬æœ‰æ›´å¤§çš„æ ·æœ¬é‡ï¼Œæ ·æœ¬å‡å€¼æ›´æœ‰å¯èƒ½æ¥è¿‘æ€»ä½“å‡å€¼ã€‚**  **##
    18.3 é¢„æµ‹å’Œæ¨æ–­'
- en: 'At this point in the course, weâ€™ve spent a great deal of time working with
    models. When we first introduced the idea of modeling a few weeks ago, we did
    so in the context of **prediction**: using models to make *accurate predictions*
    about unseen data. Another reason we might build models is to better understand
    complex phenomena in the world around us. **Inference** is the task of using a
    model to infer the true underlying relationships between the feature and response
    variables. For example, if we are working with a set of housing data, *prediction*
    might ask: given the attributes of a house, how much is it worth? *Inference*
    might ask: how much does having a local park impact the value of a house?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹çš„è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬èŠ±äº†å¤§é‡æ—¶é—´ç ”ç©¶æ¨¡å‹ã€‚å‡ å‘¨å‰æˆ‘ä»¬é¦–æ¬¡ä»‹ç»äº†å»ºæ¨¡çš„æ¦‚å¿µæ—¶ï¼Œæ˜¯åœ¨**é¢„æµ‹**çš„èƒŒæ™¯ä¸‹ï¼šä½¿ç”¨æ¨¡å‹å¯¹æœªçŸ¥æ•°æ®è¿›è¡Œ*å‡†ç¡®é¢„æµ‹*ã€‚æˆ‘ä»¬æ„å»ºæ¨¡å‹çš„å¦ä¸€ä¸ªåŸå› æ˜¯æ›´å¥½åœ°ç†è§£æˆ‘ä»¬å‘¨å›´å¤æ‚çš„ç°è±¡ã€‚**æ¨æ–­**æ˜¯ä½¿ç”¨æ¨¡å‹æ¨æ–­ç‰¹å¾å’Œå“åº”å˜é‡ä¹‹é—´çœŸå®çš„åŸºæœ¬å…³ç³»çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨å¤„ç†ä¸€ç»„æˆ¿å±‹æ•°æ®ï¼Œ*é¢„æµ‹*å¯èƒ½ä¼šé—®ï¼šæ ¹æ®æˆ¿å±‹çš„å±æ€§ï¼Œå®ƒå€¼å¤šå°‘é’±ï¼Ÿ*æ¨æ–­*å¯èƒ½ä¼šé—®ï¼šå½“åœ°å…¬å›­å¯¹æˆ¿å±‹ä»·å€¼æœ‰å¤šå¤§å½±å“ï¼Ÿ
- en: A major goal of inference is to draw conclusions about the full population of
    data given only a random sample. To do this, we aim to estimate the value of a
    **parameter**, which is a numerical function of the *population* (for example,
    the population mean \(\mu\)). We use a collected sample to construct a **statistic**,
    which is a numerical function of the random *sample* (for example, the sample
    mean \(\bar{X}_n\)). Itâ€™s helpful to think â€œpâ€ for â€œparameterâ€ and â€œpopulation,â€
    and â€œsâ€ for â€œsampleâ€ and â€œstatistic.â€
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨æ–­çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯ä»…å‡­éšæœºæ ·æœ¬å¯¹å®Œæ•´æ•°æ®æ€»ä½“è¿›è¡Œæ¨æ–­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¼°è®¡*å‚æ•°*çš„å€¼ï¼Œè¿™æ˜¯*æ€»ä½“*çš„æ•°å€¼å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œæ€»ä½“å‡å€¼ \(\mu\)ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨æ”¶é›†çš„æ ·æœ¬æ¥æ„å»º**ç»Ÿè®¡é‡**ï¼Œè¿™æ˜¯éšæœº*æ ·æœ¬*çš„æ•°å€¼å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œæ ·æœ¬å‡å€¼
    \(\bar{X}_n\)ï¼‰ã€‚å°†â€œpâ€è§†ä¸ºâ€œå‚æ•°â€å’Œâ€œæ€»ä½“â€ï¼Œå°†â€œsâ€è§†ä¸ºâ€œæ ·æœ¬â€å’Œâ€œç»Ÿè®¡é‡â€æ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: Since the sample represents a *random* subset of the population, any statistic
    we generate will likely deviate from the true population parameter, and it *could
    have been different*. We say that the sample statistic is an **estimator** of
    the true population parameter. Notationally, the population parameter is typically
    called \(\theta\), while its estimator is denoted by \(\hat{\theta}\).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ ·æœ¬ä»£è¡¨æ€»ä½“çš„*éšæœº*å­é›†ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„ä»»ä½•ç»Ÿè®¡é‡å¯èƒ½ä¼šåç¦»çœŸå®çš„æ€»ä½“å‚æ•°ï¼Œå¹¶ä¸”*å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ*ã€‚æˆ‘ä»¬è¯´æ ·æœ¬ç»Ÿè®¡é‡æ˜¯çœŸå®æ€»ä½“å‚æ•°çš„**ä¼°è®¡é‡**ã€‚åœ¨ç¬¦å·ä¸Šï¼Œæ€»ä½“å‚æ•°é€šå¸¸ç§°ä¸º
    \(\theta\)ï¼Œè€Œå…¶ä¼°è®¡é‡ç”¨ \(\hat{\theta}\) è¡¨ç¤ºã€‚
- en: 'To address our inference question, we aim to construct estimators that closely
    estimate the value of the population parameter. We evaluate how â€œgoodâ€ an estimator
    is by answering three questions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”æˆ‘ä»¬çš„æ¨æ–­é—®é¢˜ï¼Œæˆ‘ä»¬æ—¨åœ¨æ„å»ºèƒ½å¤Ÿç´§å¯†ä¼°è®¡æ€»ä½“å‚æ•°å€¼çš„ä¼°è®¡é‡ã€‚æˆ‘ä»¬é€šè¿‡å›ç­”ä¸‰ä¸ªé—®é¢˜æ¥è¯„ä¼°ä¼°è®¡é‡çš„â€œå¥½åâ€ï¼š
- en: Do we get the right answer for the parameter, on average?
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¹³å‡å¾—åˆ°å‚æ•°çš„æ­£ç¡®ç­”æ¡ˆå—ï¼Ÿ
- en: How variable is the answer?
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆæœ‰å¤šå¤§çš„å˜åŒ–ï¼Ÿ
- en: How close is our answer to the parameter?
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç­”æ¡ˆä¸å‚æ•°æœ‰å¤šæ¥è¿‘ï¼Ÿ
- en: 18.3.1 Modeling as Estimation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.3.1 å»ºæ¨¡ä½œä¸ºä¼°è®¡
- en: Now that weâ€™ve established the idea of an estimator, letâ€™s see how we can apply
    this learning to the modeling process. To do so, weâ€™ll take a moment to formalize
    our data collection and models in the language of random variables.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å»ºç«‹äº†ä¼°è®¡é‡çš„æ¦‚å¿µï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†è¿™ç§å­¦ä¹ åº”ç”¨åˆ°å»ºæ¨¡è¿‡ç¨‹ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†èŠ±ä¸€ç‚¹æ—¶é—´ç”¨éšæœºå˜é‡çš„è¯­è¨€æ¥å½¢å¼åŒ–æˆ‘ä»¬çš„æ•°æ®æ”¶é›†å’Œæ¨¡å‹ã€‚
- en: Say we are working with an input variable, \(x\), and a response variable, \(Y\).
    We assume that \(Y\) and \(x\) are linked by some relationship \(g\); in other
    words, \(Y = g(x)\). \(g\) represents some â€œuniversal truthâ€ or â€œlaw of natureâ€
    that defines the underlying relationship between \(x\) and \(Y\). In the image
    below, \(g\) is denoted by the red line.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æ­£åœ¨å¤„ç†ä¸€ä¸ªè¾“å…¥å˜é‡\(x\)å’Œä¸€ä¸ªå“åº”å˜é‡\(Y\)ã€‚æˆ‘ä»¬å‡è®¾\(Y\)å’Œ\(x\)é€šè¿‡æŸç§å…³ç³»\(g\)ç›¸å…³è”ï¼›æ¢å¥è¯è¯´ï¼Œ\(Y = g(x)\)ã€‚\(g\)ä»£è¡¨ä¸€äº›å®šä¹‰\(x\)å’Œ\(Y\)ä¹‹é—´åŸºç¡€å…³ç³»çš„â€œæ™®éçœŸç†â€æˆ–â€œè‡ªç„¶æ³•åˆ™â€ã€‚åœ¨ä¸‹é¢çš„å›¾åƒä¸­ï¼Œ\(g\)ç”±çº¢çº¿è¡¨ç¤ºã€‚
- en: As data scientists, however, we have no way of directly â€œseeingâ€ the underlying
    relationship \(g\). The best we can do is collect observed data out in the real
    world to try to understand this relationship. Unfortunately, the data collection
    process will always have some inherent error (think of the randomness you might
    encounter when taking measurements in a scientific experiment). We say that each
    observation comes with some random error or **noise** term, \(\epsilon\). This
    error is assumed to be a random variable with expectation \(\mathbb{E}(\epsilon)=0\),
    variance \(\text{Var}(\epsilon) = \sigma^2\), and be i.i.d. across each observation.
    The existence of this random noise means that our observations, \(Y(x)\), are
    *random variables*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥â€œçœ‹åˆ°â€åŸºç¡€å…³ç³»\(g\)ã€‚æˆ‘ä»¬èƒ½åšçš„æœ€å¥½çš„äº‹æƒ…å°±æ˜¯æ”¶é›†åœ¨ç°å®ä¸–ç•Œä¸­è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼Œä»¥å°è¯•ç†è§£è¿™ç§å…³ç³»ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ•°æ®æ”¶é›†è¿‡ç¨‹æ€»ä¼šå­˜åœ¨ä¸€äº›å›ºæœ‰çš„è¯¯å·®ï¼ˆæƒ³è±¡ä¸€ä¸‹åœ¨ç§‘å­¦å®éªŒä¸­è¿›è¡Œæµ‹é‡æ—¶å¯èƒ½é‡åˆ°çš„éšæœºæ€§ï¼‰ã€‚æˆ‘ä»¬è¯´æ¯ä¸ªè§‚å¯Ÿéƒ½ä¼´éšç€ä¸€äº›éšæœºè¯¯å·®æˆ–**å™ªå£°**é¡¹\(\epsilon\)ã€‚å‡å®šè¿™ä¸ªè¯¯å·®æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼ŒæœŸæœ›ä¸º\(\mathbb{E}(\epsilon)=0\)ï¼Œæ–¹å·®ä¸º\(\text{Var}(\epsilon)
    = \sigma^2\)ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªè§‚å¯Ÿä¸­éƒ½æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚è¿™ç§éšæœºå™ªå£°çš„å­˜åœ¨æ„å‘³ç€æˆ‘ä»¬çš„è§‚å¯Ÿ\(Y(x)\)æ˜¯*éšæœºå˜é‡*ã€‚
- en: '![data](../Images/5eaf9077fe8626eeeaef0e0b08df0c17.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![data](../Images/5eaf9077fe8626eeeaef0e0b08df0c17.png)'
- en: We can only observe our random sample of data, represented by the blue points
    above. From this sample, we want to estimate the true relationship \(g\). We do
    this by constructing the model \(\hat{Y}(x)\) to estimate \(g\).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªèƒ½è§‚å¯Ÿåˆ°æˆ‘ä»¬çš„éšæœºæ•°æ®æ ·æœ¬ï¼Œç”¨è“è‰²ç‚¹è¡¨ç¤ºã€‚ä»è¿™ä¸ªæ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦ä¼°è®¡çœŸå®å…³ç³»\(g\)ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºæ¨¡å‹\(\hat{Y}(x)\)æ¥ä¼°è®¡\(g\)ã€‚
- en: '\[\text{True relationship: } g(x)\]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\text{çœŸå®å…³ç³»: } g(x)\]'
- en: '\[\text{Observed relationship: }Y = g(x) + \epsilon\]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\text{è§‚å¯Ÿåˆ°çš„å…³ç³»: }Y = g(x) + \epsilon\]'
- en: '\[\text{Prediction: }\hat{Y}(x)\]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\text{é¢„æµ‹: }\hat{Y}(x)\]'
- en: '![y_hat](../Images/b824b58c6845393488f502df80032368.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![y_hat](../Images/b824b58c6845393488f502df80032368.png)'
- en: 18.3.1.1 Estimating a Linear Relationship
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18.3.1.1 ä¼°è®¡çº¿æ€§å…³ç³»
- en: If we assume that the true relationship \(g\) is linear, we can express the
    response as \(Y = f_{\theta}(x)\), where our true relationship is modeled by \[Y
    = g(x) + \epsilon\] \[ f_{\theta}(x) = Y = \theta_0 + \sum_{j=1}^p \theta_j x_j
    + \epsilon\]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å‡è®¾çœŸå®å…³ç³»\(g\)æ˜¯çº¿æ€§çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†å“åº”è¡¨ç¤ºä¸º\(Y = f_{\theta}(x)\)ï¼Œå…¶ä¸­æˆ‘ä»¬çš„çœŸå®å…³ç³»ç”±\[Y = g(x) + \epsilon\]
    \[ f_{\theta}(x) = Y = \theta_0 + \sum_{j=1}^p \theta_j x_j + \epsilon\]æ¥å»ºæ¨¡ã€‚
- en: '*Which Expressions are random?* *In our two equations above, the true relationship
    \(g(x) = \theta_0 + \sum_{j=1}^p \theta_j x_j\) is not random, but \(\epsilon\)
    is random. Hence, \(Y = f_{\theta}(x)\) is also random.*  *This true relationship
    has true, unobservable parameters \(\theta\), and it has random noise \(\epsilon\),
    so we can never observe the true relationship. Instead, the next best thing we
    can do is obtain a sample \(\Bbb{X}\), \(\Bbb{Y}\) of \(n\) observed relationships,
    \((x, Y)\) and use it to train a model and obtain an estimate of \(\hat{\theta}\)
    \[\hat{Y}(x) = f_{\hat{\theta}}(x) = \hat{\theta_0} + \sum_{j=1}^p \hat{\theta_j}
    x_j\]'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*å“ªäº›è¡¨è¾¾å¼æ˜¯éšæœºçš„ï¼Ÿ* *åœ¨æˆ‘ä»¬ä¸Šé¢çš„ä¸¤ä¸ªæ–¹ç¨‹ä¸­ï¼ŒçœŸå®å…³ç³»\(g(x) = \theta_0 + \sum_{j=1}^p \theta_j x_j\)ä¸æ˜¯éšæœºçš„ï¼Œä½†\(\epsilon\)æ˜¯éšæœºçš„ã€‚å› æ­¤ï¼Œ\(Y
    = f_{\theta}(x)\)ä¹Ÿæ˜¯éšæœºçš„ã€‚* *è¿™ä¸ªçœŸå®å…³ç³»æœ‰çœŸå®çš„ã€ä¸å¯è§‚æµ‹çš„å‚æ•°\(\theta\)ï¼Œå¹¶ä¸”å®ƒæœ‰éšæœºå™ªå£°\(\epsilon\)ï¼Œæ‰€ä»¥æˆ‘ä»¬æ°¸è¿œæ— æ³•è§‚å¯Ÿåˆ°çœŸå®å…³ç³»ã€‚ç›¸åï¼Œæˆ‘ä»¬èƒ½åšçš„ä¸‹ä¸€ä¸ªæœ€å¥½çš„äº‹æƒ…å°±æ˜¯è·å¾—ä¸€ä¸ªæ ·æœ¬\(\Bbb{X}\)ï¼Œ\(\Bbb{Y}\)çš„\(n\)ä¸ªè§‚å¯Ÿå…³ç³»\((x,
    Y)\)ï¼Œå¹¶ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹å¹¶è·å¾—\(\hat{\theta}\)çš„ä¼°è®¡\[\hat{Y}(x) = f_{\hat{\theta}}(x) = \hat{\theta_0}
    + \sum_{j=1}^p \hat{\theta_j} x_j\]'
- en: '*Which Expressions are random?* *In our estimating equation above, our sample
    \(\Bbb{X}\), \(\Bbb{Y}\) are random. Hence, the estimates we calculate from our
    samples \(\hat{\theta}\) are also random, so our predictor \(\hat{Y}(x)\) is also
    random.*  *Now taking a look at our original equations, we can see that they both
    have differing sources of randomness. For our observed relationship, \(Y = g(x)
    + \epsilon\), \(\epsilon\) represents measurement errors and reflects randomness
    from the future. For the estimation model, the data we have is a random sample
    collected from the population, so the randomness from the past.**  **## 18.4 Bootstrap
    Resampling (Review)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*å“ªäº›è¡¨è¾¾å¼æ˜¯éšæœºçš„ï¼Ÿ* *åœ¨æˆ‘ä»¬ä¸Šé¢çš„ä¼°è®¡æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ ·æœ¬\(\Bbb{X}\)ï¼Œ\(\Bbb{Y}\)æ˜¯éšæœºçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»æ ·æœ¬ä¸­è®¡ç®—çš„ä¼°è®¡\(\hat{\theta}\)ä¹Ÿæ˜¯éšæœºçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„é¢„æµ‹\(\hat{Y}(x)\)ä¹Ÿæ˜¯éšæœºçš„ã€‚*
    *ç°åœ¨çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„åŸå§‹æ–¹ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬éƒ½æœ‰ä¸åŒçš„éšæœºæ¥æºã€‚å¯¹äºæˆ‘ä»¬è§‚å¯Ÿåˆ°çš„å…³ç³»ï¼Œ\(Y = g(x) + \epsilon\)ï¼Œ\(\epsilon\)ä»£è¡¨æµ‹é‡è¯¯å·®å¹¶åæ˜ æœªæ¥çš„éšæœºæ€§ã€‚å¯¹äºä¼°è®¡æ¨¡å‹ï¼Œæˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®æ˜¯ä»æ€»ä½“ä¸­æ”¶é›†çš„éšæœºæ ·æœ¬ï¼Œå› æ­¤æ˜¯è¿‡å»çš„éšæœºæ€§ã€‚**  **##
    18.4 è‡ªåŠ©æ³•é‡é‡‡æ ·ï¼ˆå¤ä¹ ï¼‰'
- en: To determine properties of the sampling distribution of an estimator like variance,
    weâ€™d need to have access to the population so that we can consider all possible
    samples and compute an estimate for each sample.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®šä¼°è®¡é‡çš„æŠ½æ ·åˆ†å¸ƒçš„å±æ€§ï¼Œæ¯”å¦‚æ–¹å·®ï¼Œæˆ‘ä»¬éœ€è¦è®¿é—®æ€»ä½“ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„æ ·æœ¬å¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¼°è®¡ã€‚
- en: '![y_hat](../Images/8f88835b8fbe9546a2e01246ba83fe92.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![y_hat](../Images/8f88835b8fbe9546a2e01246ba83fe92.png)'
- en: However, we donâ€™t have access to the population; we only have *one* random sample
    from the population. How can we consider all possible samples if we only have
    one?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬æ— æ³•è®¿é—®æ€»ä½“ï¼›æˆ‘ä»¬åªæœ‰æ¥è‡ªæ€»ä½“çš„*ä¸€ä¸ª*éšæœºæ ·æœ¬ã€‚å¦‚æœæˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬å¦‚ä½•è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„æ ·æœ¬å‘¢ï¼Ÿ
- en: The idea of bootstrapping is to treat our random sample as a â€œpopulationâ€ and
    resample from it *with replacement*. Intuitively, a random sample resembles the
    population, so a random *resample* also resamples a random sample.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ©æ³•çš„æƒ³æ³•æ˜¯å°†æˆ‘ä»¬çš„éšæœºæ ·æœ¬è§†ä¸ºâ€œæ€»ä½“â€ï¼Œå¹¶ä»ä¸­è¿›è¡Œ*æœ‰æ”¾å›*çš„é‡æ–°é‡‡æ ·ã€‚ç›´è§‚åœ°è¯´ï¼Œéšæœºæ ·æœ¬ç±»ä¼¼äºæ€»ä½“ï¼Œå› æ­¤éšæœº*é‡æ–°é‡‡æ ·*ä¹Ÿé‡æ–°å¯¹éšæœºæ ·æœ¬è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚
- en: '![y_hat](../Images/47acea8264d72e12383558934a21e252.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![y_hat](../Images/47acea8264d72e12383558934a21e252.png)'
- en: 'Bootstrap resampling is a technique for estimating the sampling distribution
    of an estimator. To execute it, we can follow the pseudocode below:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Bootstrapé‡é‡‡æ ·æ˜¯ä¸€ç§ä¼°è®¡ä¼°è®¡é‡æŠ½æ ·åˆ†å¸ƒçš„æŠ€æœ¯ã€‚è¦æ‰§è¡Œå®ƒï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„ä¼ªä»£ç è¿›è¡Œï¼š
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Why must we resample *with replacement*?* **Given an original sample of size
    \(n\), we want a resample that has the same size \(n\) as the original. Sampling
    *without* replacement will give us the original sample with shuffled rows. Hence,
    when we calculate summary statistics like the average, our sample *without* replacement
    will always have the same average as the original sample, defeating the purpose
    of a bootstrap.**  **How well does bootstrapping actually represent our population?
    The bootstrapped sampling distribution of an estimator does not exactly match
    the sampling distribution of that estimator, but it is often close. Similarly,
    the variance of the bootstrapped distribution is often close to the true variance
    of the estimator. The example below displays the results from different bootstraps
    from a *known* population using a sample size of \(n=50\).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»è¿›è¡Œ*æœ‰æ”¾å›*çš„é‡æ–°é‡‡æ ·ï¼Ÿ* **ç»™å®šå¤§å°ä¸º\(n\)çš„åŸå§‹æ ·æœ¬ï¼Œæˆ‘ä»¬å¸Œæœ›å¾—åˆ°ä¸åŸå§‹æ ·æœ¬ç›¸åŒå¤§å°\(n\)çš„é‡æ–°é‡‡æ ·ã€‚*ä¸*è¿›è¡Œæ›¿æ¢çš„æŠ½æ ·å°†ç»™æˆ‘ä»¬æ´—ç‰Œåçš„åŸå§‹æ ·æœ¬ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬è®¡ç®—åƒå¹³å‡å€¼è¿™æ ·çš„æ‘˜è¦ç»Ÿè®¡æ—¶ï¼Œæˆ‘ä»¬*ä¸*è¿›è¡Œæ›¿æ¢çš„æ ·æœ¬å°†å§‹ç»ˆå…·æœ‰ä¸åŸå§‹æ ·æœ¬ç›¸åŒçš„å¹³å‡å€¼ï¼Œä»è€Œç ´åäº†è‡ªåŠ©æ³•çš„ç›®çš„ã€‚**
    **è‡ªåŠ©æ³•å®é™…ä¸Šå¦‚ä½•ä»£è¡¨æˆ‘ä»¬çš„æ€»ä½“ï¼Ÿä¼°è®¡é‡çš„è‡ªåŠ©æ³•æŠ½æ ·åˆ†å¸ƒå¹¶ä¸å®Œå…¨åŒ¹é…è¯¥ä¼°è®¡é‡çš„æŠ½æ ·åˆ†å¸ƒï¼Œä½†é€šå¸¸æ˜¯æ¥è¿‘çš„ã€‚åŒæ ·ï¼Œè‡ªåŠ©æ³•åˆ†å¸ƒçš„æ–¹å·®é€šå¸¸æ¥è¿‘äºä¼°è®¡é‡çš„çœŸå®æ–¹å·®ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†ä½¿ç”¨æ ·æœ¬å¤§å°\(n=50\)ä»*å·²çŸ¥*æ€»ä½“è¿›è¡Œä¸åŒè‡ªåŠ©æ³•çš„ç»“æœã€‚**'
- en: '![y_hat](../Images/5317e1d1c656f7443e965bfcc4ce9b05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![y_hat](../Images/5317e1d1c656f7443e965bfcc4ce9b05.png)'
- en: In the real world, we donâ€™t know the population distribution. The center of
    the boostrapped distribution is the estimator applied to our original sample,
    so we have no way of recovering the estimatorâ€™s true expected value. The quality
    of our bootstrapped distribution depends on the quality of our original sample;
    if our original sample was not representative of the population, bootstrap is
    next to useless.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“æ€»ä½“åˆ†å¸ƒã€‚è‡ªåŠ©æ³•åˆ†å¸ƒçš„ä¸­å¿ƒæ˜¯åº”ç”¨äºæˆ‘ä»¬åŸå§‹æ ·æœ¬çš„ä¼°è®¡é‡ï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•æ¢å¤ä¼°è®¡é‡çš„çœŸå®æœŸæœ›å€¼ã€‚æˆ‘ä»¬çš„è‡ªåŠ©æ³•åˆ†å¸ƒçš„è´¨é‡å–å†³äºæˆ‘ä»¬åŸå§‹æ ·æœ¬çš„è´¨é‡ï¼›å¦‚æœæˆ‘ä»¬çš„åŸå§‹æ ·æœ¬ä¸ä»£è¡¨æ€»ä½“ï¼Œè‡ªåŠ©æ³•å‡ ä¹æ²¡æœ‰ç”¨å¤„ã€‚
- en: One thing to note is that the bootstrap often does not work well for some statistics,
    like the median or other quantile-based statistics, that depend heavily on a small
    number of observations out of a larger sample. **Bootstrapping does not overcome
    the weakness of small samples as a basis for inference**. Indeed, for the very
    smallest samples, it may be better to make additional assumptions such as a parametric
    family.**********
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œè‡ªåŠ©æ³•é€šå¸¸å¯¹æŸäº›ç»Ÿè®¡é‡ï¼ˆå¦‚ä¸­ä½æ•°æˆ–å…¶ä»–åŸºäºåˆ†ä½æ•°çš„ç»Ÿè®¡é‡ï¼‰æ•ˆæœä¸ä½³ï¼Œè¿™äº›ç»Ÿè®¡é‡ä¸¥é‡ä¾èµ–äºè¾ƒå¤§æ ·æœ¬ä¸­çš„å°‘æ•°è§‚å¯Ÿç»“æœã€‚**è‡ªåŠ©æ³•æ— æ³•å…‹æœå°æ ·æœ¬ä½œä¸ºæ¨æ–­ä¾æ®çš„å¼±ç‚¹**ã€‚äº‹å®ä¸Šï¼Œå¯¹äºéå¸¸å°çš„æ ·æœ¬ï¼Œæœ€å¥½æ˜¯åšå‡ºé¢å¤–çš„å‡è®¾ï¼Œæ¯”å¦‚å‚æ•°æ—ã€‚
