- en: 21 Training linear classifiers with logistic regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21 使用逻辑回归训练线性分类器
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Separating data classes with simple linear cuts
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单的线性切割分离数据类别
- en: What is logistic regression?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归是什么？
- en: Training linear classifiers using scikit-learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练线性分类器
- en: Interpreting the relationship between class prediction and trained classifier
    parameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释类别预测与训练分类器参数之间的关系
- en: 'Data classification, much like clustering, can be treated as a geometry problem.
    Similarly, labeled classes cluster together in an abstract space. By measuring
    the distance between points, we can identify which data points belong to the same
    cluster or class. However, as we learned in the last section, computing that distance
    can be costly. Fortunately, it’s possible to find related classes without measuring
    the distance between all points. This is something we have done before: in section
    14, we examined the customers of a clothing store. Each customer was represented
    by two features: height and weight. Plotting these features revealed a cigar-shaped
    plot. We flipped the cigar on its side and sliced it vertically into three segments
    representing three classes of customers: small, medium, and large.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分类，就像聚类一样，可以被视为一个几何问题。同样，标记的类别在抽象空间中聚集在一起。通过测量点之间的距离，我们可以识别哪些数据点属于同一个簇或类别。然而，正如我们在上一节中学到的，计算这个距离可能是昂贵的。幸运的是，我们可以找到相关的类别，而无需测量所有点之间的距离。这是我们之前做过的事情：在第14节中，我们检查了一家服装店的顾客。每个顾客由两个特征表示：身高和体重。绘制这些特征揭示了一个雪茄形状的图表。我们将雪茄翻转并垂直切割成三个部分，代表三种顾客类别：小型、中型和大型。
- en: It’s possible to separate distinct classes of data by carving out those classes
    as though with a knife. The carving can be carried out with simple linear cuts.
    Previously, we limited ourselves to vertical downward cuts. In this section, we
    learn how to cut the data at an angle to maximize class separation. Through directed
    linear cuts, we can classify our data without relying on distance calculations.
    In the process, we learn how to train and interpret linear classifiers. Let’s
    get started by revisiting the problem of separating customers by size.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将数据类别切割出来，就像用刀切割一样来分离不同的数据类别。切割可以通过简单的线性切割来完成。之前，我们限制了自己只进行垂直向下切割。在本节中，我们将学习如何以角度切割数据以最大化类别分离。通过有方向的线性切割，我们可以对数据进行分类，而不依赖于距离计算。在这个过程中，我们将学习如何训练和解释线性分类器。让我们通过重新审视按大小分离顾客的问题来开始。
- en: 21.1 Linearly separating customers by size
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1 通过大小线性分离顾客
- en: In section 14, we simulated customer heights (in inches) and weights (in pounds).
    Customers with larger inch/pound combinations fell into the Large customer class.
    We’ll now rerun that simulation. Our heights and weights are stored in a feature
    matrix `X`, and the customer classes are stored in the class-label array of `y`.
    For the purpose of this exercise, we focus on the two classes Large and Not Large.
    We assume that customers in the Large class are taller than 72 inches and heavier
    than 160 lb. After we simulate this data, we make a scatter plot of `X` in which
    the plotted points are colored based on the class labels in `y` (figure 21.1).
    This visual representation will help us look for the spatial separation between
    the different customer types.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14节中，我们模拟了顾客的身高（以英寸为单位）和体重（以磅为单位）。身高和体重组合较大的顾客被归入大型顾客类别。现在我们将重新运行这个模拟。我们的身高和体重存储在特征矩阵`X`中，顾客类别存储在标签数组`y`中。为了这个练习的目的，我们关注大型和非大型两个类别。我们假设大型类别的顾客身高超过72英寸，体重超过160磅。在模拟完这些数据后，我们制作了一个散点图，图中`X`的绘制点根据`y`中的类别标签着色（图21.1）。这种视觉表示将帮助我们寻找不同顾客类型之间的空间分离。
- en: '![](../Images/21-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-01.png)'
- en: 'Figure 21.1 A plot of customer measurements: inches vs. lbs. Large and Not
    Large customers are colored differently based on their class.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.1 顾客测量值的图表：英寸与磅。大型和非大型顾客根据其类别以不同的颜色着色。
- en: Listing 21.1 Simulating categorized customer measurements
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.1 模拟分类顾客测量值
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Plots customer measurements while coloring the customers based on class. Customer
    heights and weights are treated as two different features in the feature matrix
    X. Customer class type is stored with the label array y.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在根据类别着色顾客的同时绘制顾客测量值。顾客身高和体重在特征矩阵X中被视为两个不同的特征。顾客类别类型存储在标签数组y中。
- en: ❷ Customers fall into two classes, Large and Not Large.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 顾客分为两大类，大型和非大型。
- en: ❸ Follows the linear formula from section 14 to model weight as a function of
    height
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 按照第 14 节中的线性公式将体重建模为高度函数
- en: ❹ Customers are considered Large if their height is greater than 72 inches and
    their weight is greater than 160 lb.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果客户的高度大于 72 英寸且体重大于 160 磅，则被视为大型客户。
- en: Our plot resembles a cigar with different shades of colors at both ends. We
    can imagine a knife slicing through the cigar to separate the colors. The knife
    acts like a boundary that separates the two customer classes. We can represent
    this boundary using a line with a slope of –3.5 and a y-intercept of 415\. The
    formula for the line is `lbs = -3.5 * inches + 415`. Let’s add this linear boundary
    to the plot (figure 21.2).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图表类似于两端颜色不同的雪茄。我们可以想象一把刀切过雪茄来分离颜色。这把刀就像一个边界，将两个客户类别分开。我们可以使用斜率为 –3.5 且 y
    截距为 415 的直线来表示这个边界。这条线的公式是 `lbs = -3.5 * inches + 415`。让我们将这个线性边界添加到图表中（图 21.2）。
- en: '![](../Images/21-02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-02.png)'
- en: 'Figure 21.2 A plot of customer measurements: `inches` vs. `lbs`. A linear boundary
    separates the Large and Not Large customers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.2 客户测量值的图表：`英寸` 对 `磅`。一条线性边界将大型客户和非大型客户分开。
- en: Note We learn how to automatically compute the linear boundary later in this
    section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们将在本节后面学习如何自动计算线性边界。
- en: Listing 21.2 Plotting a boundary to separate the two customer classes
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.2 绘制用于分隔两个客户类别的边界
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The plotted line is called a *linear decision boundary* because it can be utilized
    to accurately choose a customer’s class. Most of the customers in the Large class
    are located above that line. Given a customer with a measurement of `(inches,
    lbs)`, we predict the customer’s class by checking whether `lbs > -3.5 * inches
    + 415`. If the inequality is true, then the customer is Large. Let’s use the inequality
    to predict the customer classes. We store our predictions in a `y_pred` array
    and evaluate our predictions by printing the f-measure.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的线被称为 *线性决策边界*，因为它可以用来准确地选择客户的类别。大多数大型客户都位于这条线以上。给定一个测量值为 `(英寸, 磅)` 的客户，我们通过检查
    `lbs > -3.5 * inches + 415` 来预测客户的类别。如果这个不等式成立，则该客户是大型客户。让我们使用这个不等式来预测客户类别。我们将我们的预测存储在
    `y_pred` 数组中，并通过打印 f-measure 来评估我们的预测。
- en: Note As we discussed in section 20, the f-measure is our preferred way of evaluating
    class prediction quality. As a reminder, the f-measure equals the harmonic mean
    of a classifier’s precision and recall.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 如我们在第 20 节中讨论的，f-measure 是我们评估类别预测质量的首选方式。作为提醒，f-measure 等于分类器的精确率和召回率的调和平均值。
- en: Listing 21.3 Predicting classes using a linear boundary
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.3 使用线性边界预测类别
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ If b is a Python Boolean, int(b) returns 1 if the Boolean is True and 0 otherwise.
    Hence, we can return the class label for measurements (inches, lbs) by running
    int(lbs > –3.5 * inches + 415).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果 b 是 Python 布尔值，则 int(b) 返回 1（如果布尔值为 True）或 0（否则）。因此，我们可以通过运行 int(lbs >
    –3.5 * inches + 415) 来返回测量值（英寸、磅）的类别标签。
- en: 'As expected, the f-measure is high. Given the inequality `lbs > -3.5 * inches
    + 415`, we can accurately classify our data. Furthermore, we can run the classification
    more concisely using vector dot products. Consider the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，f-measure 很高。给定不等式 `lbs > -3.5 * inches + 415`，我们可以准确地分类我们的数据。此外，我们可以使用向量点积更简洁地运行分类。考虑以下内容：
- en: Our inequality rearranges to `3.5 * inches + lbs - 415 > 0`.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的不等式重新排列为 `3.5 * inches + lbs - 415 > 0`。
- en: The dot product of two vectors `[x, y, z]` and `[a, b, c]` is equal to `a *
    x + b * y + c * z`.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个向量 `[x, y, z]` 和 `[a, b, c]` 的点积等于 `a * x + b * y + c * z`。
- en: If we take the dot product of vectors `[inches, lbs, 1]` and `[3.5, 1, -415]`,
    the result equals `3.5 * inches + lbs - 415`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将向量 `[inches, lbs, 1]` 和 `[3.5, 1, -415]` 的点积，结果等于 `3.5 * inches + lbs -
    415`。
- en: Thus our inequality reduces to `w @ v > 0`, where `w` and `v` are both vectors,
    and `@` is the dot-product operator, as show in figure 21.3.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们的不等式简化为 `w @ v > 0`，其中 `w` 和 `v` 都是向量，`@` 是点积运算符，如图 21.3 所示。
- en: Note that only one of the vectors is dependent on values of `lbs` and `inches`.
    The second vector, `[3.5, 1, -415]`, does not vary with customer measurements.
    Data scientists refer to this invariant vector as the *weight vector* or simply
    *weights*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有一个向量依赖于 `lbs` 和 `inches` 的值。第二个向量 `[3.5, 1, -415]` 不随客户测量值变化。数据科学家将这个不变的向量称为
    *权重向量* 或简称为 *权重*。
- en: Note This name is unrelated to our measured customer weights in pounds.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这个名称与我们测量的客户体重（磅）无关。
- en: '![](../Images/21-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-03.png)'
- en: Figure 21.3 We can visualize the dot product between `weights` and `[inches,`
    `lbs,` `1]` as a directed graph. In the graph, the leftmost nodes represent the
    measurements `[inches,` `lbs,` `1]`, and the edge weights represent the weights
    `[3.5,` `1,` `-415]`. We multiply each node by its corresponding edge weight and
    sum the results. That sum equals the product between our two vectors `v` and `w`.
    Customer classification is determined by whether `w` `@` `v` `>` `0`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.3 我们可以将 `weights` 和 `[英寸, 磅, 1]` 之间的点积可视化为一个有向图。在图中，最左边的节点代表测量值 `[英寸, 磅,
    1]`，边权重代表权重 `[3.5, 1, -415]`。我们乘以每个节点对应的边权重并求和。这个和等于我们两个向量 `v` 和 `w` 的乘积。客户分类由
    `w @ v > 0` 来确定。
- en: 'Using vector dot products, we’ll re-create the contents of `y_pred` in two
    lines of code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量点积，我们将在两行代码中重新创建 `y_pred` 的内容：
- en: Assign a `weights` vector to equal `[3.5, 1, -415]`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `weights` 向量赋值为 `[3.5, 1, -415]`。
- en: Classify each `(inches, lbs)` customer sample in `X` using the dot product of
    `weights` and `[inches, lbs, 1]`.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用点积将每个 `(英寸, 磅)` 客户样本在 `X` 中分类，点积是 `weights` 和 `[英寸, 磅, 1]` 的点积。
- en: Listing 21.4 Predicting classes using vector dot products
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.4 使用向量点积预测类别
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can further consolidate our code if we use matrix multiplication.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用矩阵乘法，我们可以进一步简化我们的代码。
- en: 'Consider the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: Currently, we must iterate over each `[inches, lbs]` row in matrix `X` and append
    a `1` to get vector `[inches, lbs, 1]`.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们必须遍历矩阵 `X` 中的每个 `[英寸, 磅]` 行，并附加一个 `1` 以获得向量 `[英寸, 磅, 1]`。
- en: Instead, we can concatenate a column of ones to matrix `X` and obtain a three-column
    matrix `M`. Each matrix row equals `[inches, lbs, 1]`. We refer to `M` as the
    *padded feature matrix*.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，我们可以将一列 1 连接到矩阵 `X` 上，从而获得一个三列矩阵 `M`。每个矩阵行等于 `[英寸, 磅, 1]`。我们将 `M` 称为 *填充特征矩阵*。
- en: Running `[weights @ v for v in M]` returns the dot product between `weights`
    and every row in matrix `M`. Of course, this operation is equivalent to the matrix
    product between `M` and `weights`.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `[weights @ v for v in M]` 返回 `weights` 和矩阵 `M` 中每一行的点积。当然，这个操作等同于 `M` 和
    `weights` 之间的矩阵乘积。
- en: We can concisely compute the matrix product by running `M @ weights`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过执行 `M @ weights` 来简洁地计算矩阵乘积。
- en: Running `M @ weights > 0` returns a Boolean array. Each element is true only
    if `3.5 * inches + lbs - 415 > 0` for the corresponding customer measurements.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `M @ weights > 0` 返回一个布尔数组。只有当对应客户的测量值满足 `3.5 * 英寸 + 磅 - 415 > 0` 时，每个元素才为真。
- en: Essentially, `M @ weights > 0` returns a Boolean vector whose *i*th value is
    true if `y_pred[i] == 1` and false otherwise. We can transform the Booleans into
    numeric labels using NumPy’s `astype` method. Consequently, we can generate our
    predictions just by running `(M @ weights > 0).astype(int)`. Let’s confirm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`M @ weights > 0` 返回一个布尔向量，其 *i* 个值如果 `y_pred[i] == 1` 则为真，否则为假。我们可以使用 NumPy
    的 `astype` 方法将布尔值转换为数值标签。因此，我们可以通过执行 `(M @ weights > 0).astype(int)` 来生成我们的预测。让我们来验证一下。
- en: Listing 21.5 Predicting classes using matrix multiplication
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.5 使用矩阵乘法预测类别
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Concatenates a column of ones to feature matrix X to create the three-column
    matrix M
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将一列 1 连接到特征矩阵 X 上，以创建三列矩阵 M
- en: ❷ Checks to ensure that our predictions remain the same. Note that the matrix
    product returns a NumPy array, which must be converted to a list for this comparison.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查以确保我们的预测保持不变。请注意，矩阵乘积返回一个 NumPy 数组，必须将其转换为列表才能进行此比较。
- en: We’ve boiled down customer classification to a simple matrix-vector product.
    This matrix product classifier is called a *linear classifier*. A weight vector
    is all that is required for a linear classifier to categorize input features.
    Here, we define a `linear_classifier` function that takes as input a feature matrix
    `X` and weight vector `weights`. It returns an array of class predictions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将客户分类简化为一个简单的矩阵-向量乘积。这种矩阵乘积分类器被称为 *线性分类器*。线性分类器只需要一个权重向量来对输入特征进行分类。在这里，我们定义了一个
    `linear_classifier` 函数，它接受特征矩阵 `X` 和权重向量 `weights` 作为输入，并返回一个类别预测数组。
- en: Listing 21.6 Defining a linear classifier function
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.6 定义线性分类器函数
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Linear classifiers check whether weighted features and a constant add up to
    a value greater than zero. The constant value, which is stored in `weights[-1]`,
    is referred to as the *bias*. The remaining weights are called the *coefficients*.
    During classification, every coefficient is multiplied against its corresponding
    feature. In our case, the `inches` coefficient in `weights[0]` is multiplied against
    `inches`, and the `lbs` coefficient in `weights[1]` is multiplied against `lbs`.
    Thus, `weights` contains two coefficients and one bias, taking the form `[inches_coef,
    lbs_coef, bias]`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器检查加权特征和一个常数相加是否大于零。存储在`weights[-1]`中的常数值被称为*偏差*。其余的权重被称为*系数*。在分类过程中，每个系数都会乘以其对应特征。在我们的情况下，`weights[0]`中的`inches`系数会乘以`inches`，`weights[1]`中的`lbs`系数会乘以`lbs`。因此，`weights`包含两个系数和一个偏差，形式为`[inches_coef,
    lbs_coef, bias]`。
- en: We’ve derived our `weights` vector using a known decision boundary, but `weights`
    can also be computed directly from our training set `(X, y)`. In the following
    subsection, we discuss how to train a linear classifier. Training consists of
    finding coefficients and a bias that linearly separate our customer classes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用已知的决策边界推导出了`weights`向量，但`weights`也可以直接从我们的训练集`(X, y)`中计算得出。在下面的子节中，我们将讨论如何训练一个线性分类器。训练包括找到系数和偏差，这些系数和偏差可以线性地分离我们的客户类别。
- en: Note The trained results are not equal to `weights` because an infinite number
    of `weights` vectors satisfy the inequality `M @ weights > 0`. We can prove this
    by multiplying both sides by a positive constant `k`. Of course, `0 * k` equals
    `0`. Meanwhile, `weights * k` produces a new vector `w2`. Hence, `M @ w2` is greater
    than `0` whenever `M @ weights > 0` (and vice versa). There are infinite numbers
    of `k` constants and hence an infinite number of `w2` vectors, but these vectors
    point in the same direction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：训练结果不等于`weights`，因为无限多个`weights`向量满足不等式`M @ weights > 0`。我们可以通过将两边乘以一个正常数`k`来证明这一点。当然，`0
    * k`等于`0`。同时，`weights * k`产生一个新的向量`w2`。因此，当`M @ weights > 0`时（反之亦然），`M @ w2`大于`0`。存在无限多个`k`常数，因此存在无限多个`w2`向量，但这些向量指向同一方向。
- en: 21.2 Training a linear classifier
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2 训练线性分类器
- en: We want to find a weight vector that optimizes class prediction on `X`. Let’s
    start by setting `weights` to equal three random values. Then we compute the f-measure
    associated with this random vector. We expect the f-measure to be very low.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找到一个权重向量，以优化`X`上的类别预测。让我们首先将`weights`设置为三个随机值。然后我们计算与这个随机向量相关的f度量。我们预计f度量会非常低。
- en: Listing 21.7 Classification using random weights
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.7 使用随机权重进行分类
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As expected, our f-measure is terrible! We can gain insight into why by printing
    `y_pred`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，我们的f度量非常糟糕！我们可以通过打印`y_pred`来深入了解原因。
- en: Listing 21.8 Outputting the predicted classes
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.8 输出预测类别
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All our data points are assigned a class label of 1! The product of our weights
    and each feature vector is always greater than zero, so our weights must be too
    high. Lowering the weights will yield more Class 0 predictions. For instance,
    if we set the weights to `[0, 0, 0]`, all our class predictions equal 0.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的数据点都被分配了一个类别标签1！我们权重和每个特征向量的乘积总是大于零，所以我们的权重必须太高。降低权重将产生更多的类别0预测。例如，如果我们把权重设置为`[0,
    0, 0]`，我们所有的类别预测都等于0。
- en: Listing 21.9 Shifting the class predictions by lowering the weights
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.9 通过降低权重来调整类别预测
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Lowering the weights yields more Class 0 predictions. Raising them yields more
    Class 1 predictions. Thus, we can intelligently raise and lower the weights until
    our predictions align with the actual class labels. Let’s devise a strategy for
    adjusting the weights to match the labels. We start by adjusting the bias at `weights[-1]`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 降低权重会产生更多的类别0预测。提高它们会产生更多的类别1预测。因此，我们可以智能地提高和降低权重，直到我们的预测与实际类别标签一致。让我们制定一个调整权重以匹配标签的策略。我们首先调整`weights[-1]`处的偏差。
- en: Note Adjusting the coefficients requires a bit more nuance, so for now, we’ll
    focus on the bias.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：调整系数需要更多的细微差别，所以现在我们将专注于偏差。
- en: 'Our goal is to minimize the difference between the predictions in `y_pred`
    and the actual labels in `y`. How do we do this? One simple strategy entails comparing
    each *predicted*/*actual* class-label pair. Based on each comparison, we can tweak
    the bias like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最小化`y_pred`中的预测与`y`中的实际标签之间的差异。我们如何做到这一点？一种简单的策略是比较每个*预测*/*实际*类别标签对。基于每个比较，我们可以调整偏差如下：
- en: If the prediction equals the actual class, then the prediction is correct. Hence,
    we will not modify the bias.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测值等于实际类别，则预测是正确的。因此，我们不会修改偏差。
- en: If the prediction is 1 and the actual class is 0, then the weight is too high.
    Hence, we’ll lower the bias by one unit.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测值为 1 而实际类别为 0，则权重过高。因此，我们将偏差降低一个单位。
- en: If the prediction is 0 and the actual class is 1, the weight is too low. Hence,
    we’ll increase the bias by one unit.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测值为 0 而实际类别为 1，则权重过低。因此，我们将偏差增加一个单位。
- en: Let’s define a function to compute this bias shift based on predicted and actual
    labels.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，根据预测和实际标签来计算这个偏差偏移。
- en: Note Keep in mind that per existing conventions, the bias shift is subtracted
    from the weights. So, our `get_bias_shift` function returns a positive value when
    the weights are intended to decrease.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 请记住，根据现有惯例，偏差偏移是从权重中减去的。因此，当权重旨在降低时，我们的 `get_bias_shift` 函数返回一个正值。
- en: Listing 21.10 Computing the bias shift based on prediction quality
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.10 基于预测质量计算偏差偏移
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Mathematically, we can show that our `get_bias_shift` function is equivalent
    to `predicted - actual`. The following code definitively proves this for all four
    combinations of predicted and actual class labels.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以证明我们的 `get_bias_shift` 函数等价于 `predicted - actual`。以下代码明确证明了这一点，适用于预测和实际类别标签的所有四种组合。
- en: Listing 21.11 Computing the bias shift using arithmetic
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.11 使用算术计算偏差偏移
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It’s worth noting that our single unit shift is an arbitrary value. Rather than
    shifting the bias by a single unit, we can shift it one-tenth of a unit, or 10
    units, or 100 units. The value of the shift can be controlled by a parameter called
    the *learning rate*. The learning rate is multiplied against `predicted - actual`
    to adjust the shift size. So if we want to lower the shift to 0.1, we can easily
    do so by running `learning_rate * (predicted - actual)`, where `learning_rate`
    is equal to 0.1\. This adjustment can influence the quality of training. We’ll
    therefore redefine our `get_bias_shift` function with a `learning_rate` parameter
    that is preset to 0.1.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的单个单位偏移是一个任意值。我们不仅可以偏移一个单位，还可以偏移十分之一单位、10 个单位或 100 个单位。偏移的值可以通过一个称为
    *学习率* 的参数来控制。学习率与 `predicted - actual` 相乘以调整偏移大小。因此，如果我们想将偏移降低到 0.1，我们可以通过运行 `learning_rate
    * (predicted - actual)` 来轻松实现，其中 `learning_rate` 等于 0.1。这种调整可以影响训练质量。因此，我们将重新定义
    `get_bias_shift` 函数，并添加一个预设为 0.1 的 `learning_rate` 参数。
- en: Listing 21.12 Computing the bias shift with a learning rate
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.12 使用学习率计算偏差偏移
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we are ready to adjust our bias. Listing 21.13 iterates over each `[inches,
    lbs, 1]` vector in `M`. For every *i*th vector, we predict the class label and
    compare it to the actual class in `y[i]`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备调整偏差。列表 21.13 遍历 `M` 中的每个 `[inches, lbs, 1]` 向量。对于每个 *i* 个向量，我们预测类别标签并将其与
    `y[i]` 中的实际类别进行比较。
- en: Note As a reminder, the class prediction for every vector `v` is equal to `int(v
    @ weights > 0)`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 作为提醒，每个向量 `v` 的类别预测等于 `int(v @ weights > 0)`。
- en: Using each prediction, we compute the bias shift and subtract it from the bias
    stored in `weights[-1]`. When all the iterations are complete, we print the adjusted
    bias and compare it to its original value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每个预测，我们计算偏差偏移并将其从存储在 `weights[-1]` 中的偏差中减去。当所有迭代完成后，我们打印调整后的偏差并将其与原始值进行比较。
- en: Listing 21.13 Iteratively shifting the bias
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.13 逐次调整偏差
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Predicts the class label for vector v associated with a row in matrix M
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预测与矩阵 M 中的行关联的向量 v 的类别标签
- en: The bias has drastically decreased. This makes sense, given that our weights
    were way too large. Let’s check whether the shift improved our f-measure.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差大幅下降。考虑到我们的权重过大，这是有道理的。让我们检查偏移是否提高了我们的 f-measure。
- en: Listing 21.14 Checking performance after the bias shift
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.14 偏移后检查性能
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our f-measure remains the same. Simply adjusting the bias is insufficient.
    We need to adjust the coefficients as well, but how? Naively, we could subtract
    the bias shift from every coefficient. We could just iterate over each training
    example and run `weights -= bias_shift`. Unfortunately, this naive approach is
    flawed: it always adjusts the coefficients, but it is dangerous to adjust the
    coefficients when their associated features are equal to zero. We’ll illustrate
    why with a simple example.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的f度量保持不变。仅仅调整偏差是不够的。我们还需要调整系数，但如何调整呢？天真地，我们可以从每个系数中减去偏差偏移。我们可以遍历每个训练示例并运行`weights
    -= bias_shift`。不幸的是，这种天真方法是有缺陷的：它总是调整系数，但在相关特征等于零时调整系数是危险的。我们将用一个简单的例子来说明原因。
- en: Imagine that a blank entry in our customer dataset is erroneously recorded as
    `(0, 0)`. Our model treats this data point as a customer who weighs nothing and
    is 0 inches tall. Of course, such a customer is not physically possible, but that’s
    beside the point. This theoretical customer is definitely Not Large, so their
    correct class label should be 0\. When our linear model classifies the customer,
    it takes the dot product of `[0, 0, 1]` and `[inches_coef, lbs_coef, bias]`. Of
    course, the coefficients are multiplied by zero and cancel out, so the final dot
    product is equal to just `bias` (figure 21.4). If `bias > 0`, the classifier incorrectly
    assigns a Class 1 label. Here, we’d need to decrease the bias using `bias_shift`.
    Would we also adjust the coefficients? No! Our coefficients did not impact the
    prediction. Thus, we can’t evaluate the coefficient quality. For all we know,
    the coefficients are set to their optimal values. If so, then subtracting the
    bias shift would make the model worse.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在我们的客户数据集中，一个空白条目错误地记录为`(0, 0)`。我们的模型将这个数据点视为一个体重为零、身高为零英寸的客户。当然，这样的客户在物理上是不可能的，但这不是重点。这个理论上的客户肯定不是大客户，因此他们的正确类别标签应该是0。当我们的线性模型对客户进行分类时，它将`[0,
    0, 1]`与`[inches_coef, lbs_coef, bias]`进行点积。当然，系数乘以零并相互抵消，所以最终的点积仅等于`bias`（图21.4）。如果`bias
    > 0`，分类器会错误地分配类别1标签。在这里，我们需要使用`bias_shift`减小偏差。我们会调整系数吗？不！我们的系数没有影响预测。因此，我们无法评估系数质量。据我们所知，系数已经设置为最优值。如果是这样，那么减去偏差偏移会使模型变得更差。
- en: '![](../Images/21-04.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图21.4](../Images/21-04.png)'
- en: Figure 21.4 We can visualize the dot product between `weights` and `[0,` `0,`
    `1]` as a directed graph. In the graph, the leftmost nodes represent the zero-value
    features, and the edge weights represent the coefficients and the bias. We multiply
    each node by its corresponding edge weight and sum the results. That sum equals
    `bias`. Customer classification is determined by whether `bias` `>` `0`. The coefficients
    don’t impact the prediction and therefore should not be altered in any way.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4 我们可以将`weights`和`[0, 0, 1]`之间的点积可视化为一个有向图。在图中，最左边的节点代表零值特征，边权重代表系数和偏差。我们乘以每个节点对应的边权重并求和。这个和等于`bias`。客户分类取决于`bias`是否大于`0`。系数不影响预测，因此不应以任何方式更改。
- en: We should never shift `lbs_coef` if the `lbs` features equal zero. However,
    for nonzero inputs, subtracting `bias_shift` from `lbs_coef` remains appropriate.
    We could ensure this by setting the `lbs_coef` shift to equal `bias_shift if lbs
    else 0`. Alternatively, we can set the shift to equal `bias_shift * lbs`. This
    product is zero when `lbs` is zero. Otherwise, the product shifts `lbs_coef` in
    the same direction as the bias. Similarly, we can shift `inches_coef` by `bias_shift
    * inches` units. In other words, we’ll shift each coefficient by the product of
    its feature and `bias_shift`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`lbs`特征等于零，我们永远不应该移动`lbs_coef`。然而，对于非零输入，从`lbs_coef`中减去`bias_shift`仍然是合适的。我们可以通过将`lbs_coef`的移动设置为等于`bias_shift
    if lbs else 0`来确保这一点。或者，我们可以将移动设置为等于`bias_shift * lbs`。当`lbs`为零时，这个乘积为零。否则，这个乘积将按与偏差相同的方向移动`lbs_coef`。同样，我们可以通过`bias_shift
    * inches`单位移动`inches_coef`。换句话说，我们将每个系数移动为其特征和`bias_shift`的乘积。
- en: NumPy allows us to compute our weight shifts all at once by running `bias_shift
    * [inches, lbs, 1]`. Of course, the `[inches, lbs, 1]` vector corresponds to a
    row in the padded feature matrix `M`. Thus, we can adjust the weights based on
    each *i*th prediction by running `weights -= bias_shift * M[i]`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy允许我们通过运行`bias_shift * [inches, lbs, 1]`一次计算我们的权重偏移。当然，`[inches, lbs, 1]`向量对应于填充特征矩阵`M`中的一行。因此，我们可以通过运行`weights
    -= bias_shift * M[i]`根据每个第*i*个预测调整权重。
- en: With this in mind, let’s iterate over each actual label in `y` and adjust the
    weights based on the predicted values. Then we check whether the f-measure has
    improved.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们迭代`y`中的每个实际标签，并根据预测值调整权重。然后我们检查f度量是否有所提高。
- en: Listing 21.15 Computing all weight shifts in one line of code
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.15：一行代码计算所有权重移动
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: During the iteration, `inches_coef` has decreased by 6.39 units (from 1.76 to
    –4.63), and the bias has decreased by just 0.1 units (from –12.02 to –12.12).
    This discrepancy makes sense because the coefficient shift is proportional to
    height. Customers are on average 64 inches tall, so the coefficient shift is 64-fold
    greater than the bias. As we’ll soon discover, large differences in weight shifts
    can lead to problems. Later, we eliminate these problems through a process called
    *standardization*; but first let’s turn to our f-measure.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代过程中，`inches_coef`减少了6.39个单位（从1.76变为-4.63），而偏差仅减少了0.1个单位（从-12.02变为-12.12）。这种差异是有道理的，因为系数的移动与高度成正比。顾客的平均身高为64英寸，因此系数的移动是偏差的64倍。正如我们很快会发现的那样，重量移动的大差异可能会导致问题。稍后，我们通过一个称为*标准化*的过程来消除这些问题；但首先让我们转向我们的f度量。
- en: Our f-measure has risen from 0.43 to 0.78\. The weight-shift strategy is working!
    What happens if we repeat the iteration 1,000 times? Let’s find out. Listing 21.16
    monitors the changes in f-measure over 1,000 weight-shift iterations. Then we
    plot each *i*th f-measure relative to the *i* th iteration (figure 21.5). We utilize
    the plot to monitor how the classifier’s performance improves over time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的f度量从0.43上升到0.78。权重移动策略正在起作用！如果我们重复迭代1,000次会怎样？让我们来看看。列表21.16监控了1,000次权重移动迭代中f度量的变化。然后我们绘制每个*i*次f度量相对于*i*次迭代的相对值（图21.5）。我们利用这个图表来监控分类器性能随时间如何提高。
- en: Note For the purpose of this exercise, we set weights to their original seeded
    random values. This allows us to monitor how performance improves relative to
    our starting f-measure of 0.43.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了进行这项练习，我们将权重设置为它们的原始随机种子值。这使我们能够监控性能相对于我们起始的f度量0.43如何提高。
- en: Listing 21.16 Tweaking the weights over multiple iterations
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.16：在多次迭代中调整权重
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Sets the starting weights to random values
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将起始权重设置为随机值
- en: ❷ Repeats the weight-shift logic across 1,000 iterations
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在1,000次迭代中重复权重移动逻辑
- en: ❸ Tracks performance for the weights at each iteration
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 跟踪每次迭代的权重性能
- en: ❹ Shifts the weights by iterating over each predicted/actual class-label pair
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过迭代每个预测/实际类标签对来调整权重
- en: '![](../Images/21-05.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-05.png)'
- en: Figure 21.5 Plotted iterations vs. model f-measure. The model weights are tweaked
    at each iteration. The f-measure oscillates widely between low and reasonable
    values. These oscillations need to be eliminated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.5：绘制迭代与模型f度量。在每次迭代中调整模型权重。f度量在低和合理值之间广泛波动。这些波动需要消除。
- en: 'The final f-measure is 0.68\. Our classifier is very poorly trained. What happened?
    Well, according to our plot, the classifier performance oscillates wildly throughout
    the iterations. Sometimes the f-measure goes as high as 0.80; other times, it
    drops to approximately 0.60\. After about 400 iterations, the classifier fluctuates
    nonstop between these two values. The rapid fluctuations are caused by a weight
    shift that is consistently too high. This is analogous to an airplane that is
    flying much too fast. Imagine an airplane flying 600 miles per hour after takeoff.
    The airplane maintains this rapid speed, allowing it to cover 1,500 miles in under
    three hours. However, as the airplane approaches its destination, the pilot refuses
    to slow down, so the plane overshoots its target airport and is forced to turn
    around. If the pilot doesn’t lower the velocity, the plane will miss the landing
    again. This will lead to a sequence of never-ending aerial U-turns, similar to
    the oscillations in our plot. For the pilot, the solution is simple: reduce speed
    over the course of the flight.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的f度量是0.68。我们的分类器训练得非常差。发生了什么？根据我们的图表，分类器在整个迭代过程中性能波动非常剧烈。有时f度量高达0.80；有时，它下降到大约0.60。大约400次迭代后，分类器在这两个值之间不断波动。这种快速波动是由一个始终过高的权重移动引起的。这类似于一架飞得太快的飞机。想象一下飞机起飞后以每小时600英里的速度飞行。飞机保持这种快速速度，使其在不到三小时内覆盖1,500英里。然而，当飞机接近目的地时，飞行员拒绝减速，所以飞机错过了目标机场，被迫掉头。如果飞行员不降低速度，飞机将再次错过着陆。这将导致一系列永无止境的空中U形转弯，类似于我们图表中的波动。对于飞行员来说，解决方案很简单：在整个飞行过程中减速。
- en: 'We face an analogous solution: we should slowly lower the weight shift over
    each additional iteration. How do we lower the weight shift? One approach is to
    divide the shift by `k` for each *k*th iteration. Let’s execute this strategy.
    We reset our weights to random values and iterate over `k` values ranging from
    1 to 1,001\. In each iteration, we set the weight shift equal to `bias_shift *
    M[i] / k`. Then we regenerate our performance plot (figure 21.6).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临一个类似的问题：我们应该在每次额外的迭代中逐渐降低权重变化。我们如何降低权重变化？一种方法是在每次*k*次迭代中将变化除以*k*。让我们执行这个策略。我们将权重重置为随机值，并遍历从1到1,001的*k*值。在每次迭代中，我们将权重变化设置为`bias_shift
    * M[i] / k`。然后我们重新生成我们的性能图（图21.6）。
- en: '![](../Images/21-06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-06.png)'
- en: Figure 21.6 Plotted iterations vs. model f-measure. The model weights are tweaked
    at each *k*th iteration, in proportion to `1/k`. Dividing the weight shifts by
    `k` limits oscillation. Thus, the f-measure converges to a reasonable value.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.6绘制了迭代次数与模型f度量之间的关系。在每个*k*次迭代中，模型权重按`1/k`的比例进行调整。通过将权重变化除以*k*来限制振荡。因此，f度量收敛到一个合理的值。
- en: Listing 21.17 Reducing weight shifts over multiple iterations
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.17在多次迭代中减少权重变化
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Trains a linear model from features X and labels y. The function is reused
    elsewhere in this section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从特征X和标签y训练线性模型。该函数在本节的其他地方被重用。
- en: ❷ The predict function drives the weight shift by allowing us to compare predicted
    and actual class outputs. Later in this section, we modify predict to add nuance
    to the weight shifts.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测函数通过允许我们比较预测和实际类别输出来驱动权重变化。在本节的后面部分，我们将修改预测以增加权重变化的细微差别。
- en: ❸ A model with N features has N + 1 total weights representing N coefficients
    and one bias.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 具有N个特征的模型有N + 1个总权重，代表N个系数和一个偏差。
- en: ❹ At each k th iteration, we dampen the weight shift by dividing by k. This
    reduces the weight-shift oscillations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在每个*k*次迭代中，我们通过除以*k*来减弱权重变化，这减少了权重变化的振荡。
- en: ❺ Returns the optimized weights along with tracked performance across the 1,000
    iterations
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回优化后的权重以及1,000次迭代中的跟踪性能
- en: 'Our gradual weight-shift reduction was successful. The f-measure converges
    to a steady value of 0.82\. We achieved convergence using a *perceptron training
    algorithm*. A *perceptron* is a simple linear classifier that was invented in
    the 1950s. Perceptrons are very easy to train. We just need to apply the following
    steps to training set `(X, y)`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逐步权重变化减少是成功的。f度量收敛到一个稳定的0.82值。我们使用**感知机训练算法**实现了收敛。**感知机**是一种简单的线性分类器，它在20世纪50年代被发明。感知机很容易训练。我们只需要将以下步骤应用于训练集`(X,
    y)`：
- en: Append a column of ones to feature matrix `X` to create a padded matrix `M`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一列1添加到特征矩阵`X`中，以创建一个填充矩阵`M`。
- en: Create a `weights` vector containing `M.shape[1]` random values.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含`M.shape[1]`个随机值的`weights`向量。
- en: Iterate over every *i*th row in `M`, and predict the *i*th class by running
    `M[i] @ weights > 0`.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历`M`中的每一行*i*，并通过运行`M[i] @ weights > 0`来预测第*i*个类别。
- en: Compare the *i*th prediction to the actual class label in `y[i]`. Then, compute
    the bias shift by running `(predicted - actual) * lr`, where `lr` is the learning
    rate.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第*i*个预测与实际类别标签`y[i]`进行比较。然后，通过运行`(predicted - actual) * lr`来计算偏差变化，其中`lr`是学习率。
- en: Adjust the weights by running `weights -= bias_shift * M[i] / k`. Initially,
    the constant `k` is set to `1`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`weights -= bias_shift * M[i] / k`来调整权重。最初，常数`k`被设置为`1`。
- en: Repeat steps 3 through 5 over multiple iterations. At each iteration, increment
    `k` by `1` to limit oscillation.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多次迭代中重复步骤3到5。在每次迭代中，将`k`增加`1`以限制振荡。
- en: Through repetition, the perceptron training algorithm eventually converges to
    a steady f-measure. However, that f-measure is not necessarily optimal. For instance,
    our perceptron converged to an f-measure of 0.82\. This level of performance is
    acceptable, but it doesn’t match our initial performance of 0.97\. Our trained
    decision boundary doesn’t separate the data as well as our initial decision boundary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复，感知机训练算法最终收敛到一个稳定的f度量。然而，这个f度量并不一定是最佳的。例如，我们的感知机收敛到一个f度量0.82。这个性能水平是可以接受的，但它并不符合我们最初的0.97的性能。我们的训练决策边界并没有像初始决策边界那样很好地分离数据。
- en: How do the two boundaries compare visually? We can easily find out. Using algebraic
    manipulation, we can transform a weight vector `[inches_coef, lbs_coef, bias]`
    into a linear decision boundary equal to `lbs = -(inches_coef * inches + bias)
    / lbs_coef`. With this in mind, we’ll plot both our new and old decision boundaries,
    together with our customer data (figure 21.7).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 两个边界在视觉上如何比较？我们可以轻松地找出答案。通过代数操作，我们可以将权重向量 `[inches_coef, lbs_coef, bias]` 转换为等于
    `lbs = -(inches_coef * inches + bias) / lbs_coef` 的线性决策边界。考虑到这一点，我们将绘制新旧决策边界以及我们的客户数据（图
    21.7）。
- en: '![](../Images/21-07.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21-07.png)'
- en: 'Figure 21.7 A plot of customer measurements: `inches` vs. `lbs`. Two linear
    boundaries separate the Large and Not Large customers. The trained boundary’s
    separation is worse relative to the baseline boundary.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.7 客户测量的图表：`英寸` 对 `磅`。两个线性边界将大型和非大型客户分开。训练得到的边界的分离效果相对于基线边界更差。
- en: Listing 21.18 Comparing new and old decision boundaries
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.18 比较新旧决策边界
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our trained linear boundary is inferior to our initial linear boundary, but
    this is not the fault of the perceptron algorithm. Instead, the training is hindered
    by large, fluctuating features in matrix `X`. In the next subsection, we discuss
    why large `X` values impede performance. We’ll limit that impediment through a
    process called *standardization*, in which `X` is adjusted to equal `(X - X.mean(axis=0))
    / X.std(axis=0)`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练得到的线性边界劣于我们最初的线性边界，但这并不是感知机算法的过错。相反，训练过程受到了矩阵 `X` 中大范围、波动性特征的限制。在下一小节中，我们将讨论为什么大的
    `X` 值会阻碍性能。我们将通过一个称为 *标准化* 的过程来限制这种阻碍，在这个过程中，`X` 被调整为等于 `(X - X.mean(axis=0))
    / X.std(axis=0)`。
- en: 21.2.1 Improving perceptron performance through standardization
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.2.1 通过标准化提高感知机性能
- en: 'Perceptron training is impeded by large feature values in `X`. This is due
    to the discrepancy between the coefficient shifts and bias shifts. As we discussed,
    the coefficient shift is proportional to the associated feature value. Furthermore,
    these values can be quite high. For instance, the average customer height is greater
    than 60 inches: the `inches_coef` shift is more than 60-fold higher than the bias
    shift, so we can’t tweak the bias by a little without tweaking the coefficients
    by a lot. Thus, by tuning the bias, we are liable to significantly shift `inches_coef`
    toward a less-than-optimal value.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机训练受到 `X` 中大特征值的影响。这是由于系数移动和偏差移动之间的差异。正如我们讨论的那样，系数移动与相关的特征值成比例。此外，这些值可能相当高。例如，平均客户身高超过
    60 英寸：`inches_coef` 移动幅度比偏差移动幅度高 60 倍以上，因此我们不可能只稍微调整偏差而不大幅调整系数。因此，通过调整偏差，我们可能会将
    `inches_coef` 严重移向一个非最佳值。
- en: Our training lacks all nuance because the coefficient shifts are much too high.
    However, we can lower these shifts by reducing column means in matrix `X`. Additionally,
    we need to lower the dispersion in the matrix. Otherwise, unusually large customer
    measurements could cause overly large coefficient shifts. We therefore need to
    decrease the column means and standard deviations. To start, let’s print the current
    values of `X.mean(axis=0)` and `X.std(axis=0)`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练缺乏所有细微差别，因为系数的移动幅度过大。然而，我们可以通过减少矩阵 `X` 中的列均值来降低这些移动幅度。此外，我们还需要降低矩阵的分散度。否则，异常大的客户测量值可能导致系数移动幅度过大。因此，我们需要降低列均值和标准差。首先，让我们打印
    `X.mean(axis=0)` 和 `X.std(axis=0)` 的当前值。
- en: Listing 21.19 Printing feature means and standard deviations
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.19 打印特征均值和标准差
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The feature means and standard deviations are relatively high. How do we make
    them smaller? Well, as we learned in section 14, it is trivial to shift a dataset’s
    mean toward zero: we simply need to subtract `means` from `X`. Adjusting the standard
    deviations is less straightforward, but mathematically we can show that `(X -
    means) / stds` returns a matrix whose column dispersions all equal 1.0.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 特征均值和标准差相对较高。我们如何使它们变小？嗯，正如我们在第 14 节中学到的，将数据集的均值移向零是微不足道的：我们只需要从 `X` 中减去 `means`。调整标准差不太直接，但从数学上我们可以证明
    `(X - means) / stds` 返回一个矩阵，其列分散度都等于 1.0。
- en: Note Here is the proof. Running `X - means` returns a matrix whose every column
    `v` has a mean of 0.0\. Hence, the variance of each `v` equals `[e * e for e in
    v] / N`, where `N` is the number of column elements. Of course, this operation
    can be expressed as a simple dot product, `v @ v / N`. The standard deviation
    `std` equals the square root of the variance, so `std = sqrt(v @ v) / sqrt(N)`.
    Note that `sqrt(v @ v)` is equal to the magnitude of `v`, which we can express
    as `norm(v)`. Thus, `std = norm(v) / sqrt(N)`. Suppose we divide `v` by `std`
    to generate a new vector `v2`. Since `v2 = v / std`, we expect the magnitude of
    `v2` to equal `norm(v) / std`. The standard deviation of `v2` is equal to `norm(v2)
    / sqrt(N)`. By substituting out `norm(v2)`, we get `norm(v) / (sqrt(N) * std)`.
    However, `norm(v) / sqrt(N) = std`. So the standard deviation of `v2` reduces
    to `std / std`, which equals 1.0.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这里是证明。运行`X - means`返回一个矩阵，其每一列`v`的均值为0.0。因此，每个`v`的方差等于`[e * e for e in v]
    / N`，其中`N`是列元素的数量。当然，这个操作可以表示为一个简单的点积，`v @ v / N`。标准差`std`等于方差的平方根，所以`std = sqrt(v
    @ v) / sqrt(N)`。注意`sqrt(v @ v)`等于`v`的模，我们可以表示为`norm(v)`。因此，`std = norm(v) / sqrt(N)`。假设我们将`v`除以`std`来生成一个新的向量`v2`。由于`v2
    = v / std`，我们期望`v2`的模等于`norm(v) / std`。`v2`的标准差等于`norm(v2) / sqrt(N)`。通过代入`norm(v2)`，我们得到`norm(v)
    / (sqrt(N) * std)`。然而，`norm(v) / sqrt(N) = std`。所以`v2`的标准差减少到`std / std`，等于1.0。
- en: This simple process is called *standardization*. Let’s standardize our feature
    matrix by running `(X - means) / stds`. The resulting matrix has column means
    of 0 and column standard deviations of 1.0.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的过程被称为*标准化*。让我们通过运行`(X - means) / stds`来标准化我们的特征矩阵。得到的矩阵具有列均值为0和列标准差为1.0。
- en: Listing 21.20 Standardizing the feature matrix
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.20 标准化特征矩阵
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Standardizes measurements derived from the customer distribution. We reuse
    the function elsewhere in this section.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标准化来自客户分布的测量。我们在本节的其它地方重用了这个函数。
- en: We now check whether training on the standardized feature matrix improves our
    results. We also plot the trained decision boundary relative to the standardized
    data (figure 21.8).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在检查在标准化的特征矩阵上训练是否提高了我们的结果。我们还绘制了相对于标准化数据的训练好的决策边界（图21.8）。
- en: '![](../Images/21-08.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-08.png)'
- en: Figure 21.8 A plot of standardized customer measurements. A trained decision
    boundary separates Large and Not Large customers. The trained boundary’s separation
    is on par with the baseline decision boundary in figure 21.2.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.8 标准化客户测量的图表。训练好的决策边界将大型客户和非大型客户分开。训练边界的分离效果与图21.2中的基线决策边界相当。
- en: Listing 21.21 Training on the standardized feature matrix
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.21 在标准化的特征矩阵上训练
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Plots the linear decision boundary derived from weights, together with the
    standardized data
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绘制由权重导出的线性决策边界，以及标准化后的数据
- en: ❷ Transforms the weights into a linear function
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将权重转换为线性函数
- en: Success! Our new f-measure equals 0.98\. This f-measure is higher than our baseline
    value of 0.97\. Furthermore, the angle of our new decision boundary closely resembles
    the baseline boundary in figure 21.2\. We achieved improvement in performance
    through standardization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们新的f度量等于0.98。这个f度量高于我们的基线值0.97。此外，我们新的决策边界角度与图21.2中的基线边界非常相似。我们通过标准化实现了性能的提升。
- en: Note Standardization is similar to normalization. Both techniques lower the
    values in inputted data and eliminate unit differences (such as inches versus
    centimeters). For some tasks, such as PCA analysis, the two techniques can be
    used interchangeably. However, when we’re training linear classifiers, standardization
    achieves superior results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 标准化类似于归一化。这两种技术都会降低输入数据中的值并消除单位差异（例如英寸与厘米）。对于某些任务，如PCA分析，这两种技术可以互换使用。然而，当我们训练线性分类器时，标准化可以达到更好的效果。
- en: We should note that our trained classifier now requires all input data to be
    standardized before classification. Hence, given any new data `d`, we need to
    classify that data by running `linear_classifier(standardize(d), weights)`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，我们的训练好的分类器现在需要所有输入数据在分类之前进行标准化。因此，对于任何新的数据`d`，我们需要通过运行`linear_classifier(standardize(d),
    weights)`来对该数据进行分类。
- en: Listing 21.22 Standardizing new classifier inputs
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.22 标准化新的分类器输入
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’ve standardized our data and achieved a high level of performance. Unfortunately,
    this optimal f-measure is still not guaranteed by the training algorithm. Perceptron
    training quality can fluctuate, even if the algorithm is run repeatedly on the
    same training set. This is due to the random weights assigned in the initial training
    step: certain starting weights converge to a worse decision boundary. Let’s illustrate
    the model’s inconsistency by training a perceptron five times. After each training
    run, we check whether the resulting f-measure falls below our initial baseline
    of 0.97.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经标准化了我们的数据并实现了高水平的性能。不幸的是，这个最优的f度量仍然不能由训练算法保证。感知机训练质量可能会波动，即使算法在相同的训练集上重复运行。这是由于初始训练步骤中分配的随机权重：某些起始权重收敛到一个更差的决策边界。让我们通过五次训练感知机来展示模型的不一致性。每次训练运行后，我们检查得到的f度量是否低于我们最初的基线0.97。
- en: Listing 21.23 Checking a perceptron’s training consistency
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.23 检查感知机的训练一致性
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In 80% of instances, the trained model performance falls below the baseline.
    Our basic perceptron model is clearly flawed. We discuss its flaws in the subsequent
    subsection. In the process, we derive one of the most popular linear models in
    data science: logistic regression.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在80%的情况下，训练模型的性能低于基线。我们基本感知机模型显然是有缺陷的。我们将在后续小节中讨论其缺陷。在这个过程中，我们推导出数据科学中最受欢迎的线性模型之一：逻辑回归。
- en: 21.3 Improving linear classification with logistic regression
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.3 使用逻辑回归改进线性分类
- en: 'During class prediction, our linear boundary makes a simple binary decision.
    However, as we learned in section 20, not all predictions should be treated equally.
    Sometimes we are more confident in some predictions than others. For instance,
    if all neighbors in a KNN model vote unanimously for Class 1, we are 100% confident
    in that prediction. But if just six of nine neighbors vote for Class 1, we are
    66% confident in that prediction. This measure of confidence is lacking in our
    perceptron model. The model has just two outputs: 0 and 1, based on whether the
    data lies above or below the decision boundary.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在类别预测过程中，我们的线性边界做出简单的二元决策。然而，正如我们在第20节中学到的，不是所有的预测都应该同等对待。有时我们对某些预测比其他预测更有信心。例如，如果一个KNN模型的所有邻居都一致投票给类别1，我们对那个预测的信心是100%。但如果只有九个邻居中的六个投票给类别1，我们对那个预测的信心是66%。这种信心度量在我们的感知机模型中是缺失的。该模型只有两个输出：0和1，基于数据是否位于决策边界之上或之下。
- en: What about a data point that lies exactly on the decision boundary? Currently,
    our logic will assign Class 0 to that point.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果一个数据点正好位于决策边界上呢？目前，我们的逻辑将类别0分配给那个点。
- en: Note If measurements in `v` lie on the decision boundary, then `weights @ v
    == 0`. Hence `int(weights @ v > 0)` returns `0`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果`v`中的测量值位于决策边界上，那么`weights @ v == 0`。因此`int(weights @ v > 0)`返回`0`。
- en: However, that assignment is arbitrary. If the point is not positioned above
    or below the decision boundary, we cannot decide on either class! Thus, our confidence
    in either class should equal 50%. What if we shift the point 0.0001 units above
    the boundary? Our confidence in Class 1 should go up, but not by much. We can
    assume that the Class 1 likelihood increases to 50.001% while the Class 0 likelihood
    decreases to 49.999%. Only if the point is positioned far from the boundary should
    our confidence rise sharply, as illustrated in figure 21.9\. For instance, if
    the point is 100 units above the boundary, then our confidence in Class 1 should
    reach 100% and our confidence in Class 0 should drop to 0%.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种分配是任意的。如果点不在决策边界之上或之下，我们无法决定哪个类别！因此，我们对两个类别的信心应该相等，都是50%。如果我们把点向上移动0.0001个单位？我们对类别1的信心应该会增加，但不会太多。我们可以假设类别1的可能性增加到50.001%，而类别0的可能性减少到49.999%。只有当点远离边界时，我们的信心才会急剧上升，如图21.9所示。例如，如果点在边界上方100个单位，那么我们对类别1的信心应该达到100%，而对类别0的信心应该下降到0%。
- en: '![](../Images/21-09.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-09.png)'
- en: 'Figure 21.9 A plot of customer measurements: `inches` vs. `lbs`. A linear boundary
    separates our two customer classes. Only customers who are either close to or
    far from the boundary are displayed. Customers who are too close to the boundary
    are harder to classify. We are much more confident in the class label of those
    customers who lie far from the decision boundary.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.9 客户测量值的图表：`英寸`对`磅`。一条线性边界分隔了我们的两个客户类别。只有接近或远离边界的客户被显示出来。太靠近边界的客户更难分类。我们对远离决策边界的那些客户的类别标签更有信心。
- en: Class confidence is determined by distance from the boundary and position relative
    to the boundary. If a point lies 100 units below the decision boundary, its Class
    1 and 0 likelihoods should be flipped. We can capture both distance and position
    with *directed distance*. Unlike regular distance, directed distance can be negative.
    We’ll assign each point a negative distance if it falls below the decision boundary.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 类别置信度由边界距离和相对于边界的位置决定。如果一个点位于决策边界下方100个单位，其类别1和0的可能性应该颠倒。我们可以用*指向距离*来捕捉距离和位置。与常规距离不同，指向距离可以是负数。如果一个点低于决策边界，我们将为其分配一个负距离。
- en: Note Hence, if a point is 100 units below the boundary, its directed distance
    to the boundary equals –100.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 因此，如果一个点位于边界下方100个单位，其到边界的指向距离等于-100。
- en: 'Let’s select a function to compute the Class 1 confidence based on directed
    distance from the boundary. The function should rise to 1.0 as the directed distance
    rises to infinity. Conversely, it should drop to 0.0 as the directed distance
    drops to negative infinity. Finally, the function should equal 0.5 when the directed
    distance equals zero. In this book, we have encountered a function that fits these
    criteria: in section 7, we introduced the cumulative distribution function of
    the normal curve. This S-shaped curve equals the probability of randomly drawing
    a value from a normal distribution that’s less than or equal to some `z`. The
    function starts at 0.0 and increases to 1.0\. It’s also equal to 0.5 when `z ==
    0`. As a reminder, the cumulative distribution can be computed by running `scipy.stats.norm.cdf(z)`.
    Here, we plot the CDF for z-values ranging from –10 to 10 (figure 21.10).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个函数来计算基于边界指向距离的类别1置信度。当指向距离增加到无穷大时，该函数应增加到1.0。相反，当指向距离减少到负无穷大时，它应减少到0.0。最后，当指向距离等于零时，该函数应等于0.5。在这本书中，我们遇到了一个符合这些标准的函数：在第7节中，我们介绍了正态曲线的累积分布函数。这个S形曲线等于从正态分布中随机抽取一个小于或等于某个`z`值的概率。该函数从0.0开始增加至1.0。当`z
    == 0`时，它也等于0.5。作为提醒，累积分布可以通过运行`scipy.stats.norm.cdf(z)`来计算。在这里，我们绘制了从-10到10的z值的累积分布函数（图21.10）。
- en: '![](../Images/21-10.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-10.png)'
- en: Figure 21.10 A cumulative distribution function of a normal distribution. The
    S-shaped curve starts at 0.0 and rises toward 1.0\. It equals 0.5 when the input
    is 0.0\. This plot fits our criteria for capturing uncertainty based on the directed
    distance from the decision boundary.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.10 正态分布的累积分布函数。S形曲线从0.0开始，向1.0增长。当输入为0.0时，它等于0.5。这个图符合我们基于决策边界指向距离捕捉不确定性的标准。
- en: Listing 21.24 Measuring uncertainty using `stats.norm.cdf`
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.24 使用`stats.norm.cdf`测量不确定性
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Confirms the curve has equal confidence in both classes when z lies directly
    on the 0.0 threshold
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当z直接位于0.0阈值时，确认曲线对两个类别的置信度相同
- en: 'The S-shaped cumulative normal distribution curve fits our stated confidence
    criteria. It’s an adequate function for computing classifier uncertainty. But
    in recent decades, this curve’s usage has fallen out of favor. There are several
    reasons. One of the most pressing concerns is that no exact formula exists for
    calculating `stats.norm.cdf`: instead, the area under the normal distribution
    is computed by approximation. Consequently, data scientists have turned to a different
    S-shaped curve, whose straightforward formula is easy to remember: the *logistic*
    curve. The logistic function of `z` is `1 / (1 - e ** z)` where `e` is a constant
    equal to approximately 2.72\. Much like the cumulative normal distribution, the
    logistic function ranges from 0 to 1 and is equal to 0.5 when `z == 0`. Let’s
    plot the logistic curve, together with `stats.norm.cdf` (figure 21.11).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: S形累积正态分布曲线符合我们声明的置信度标准。它是一个计算分类器不确定性的充分函数。但在最近几十年里，这种曲线的使用已经不再受欢迎。有几个原因。最紧迫的问题是，没有精确的公式来计算`stats.norm.cdf`：相反，正态分布下的面积是通过近似计算的。因此，数据科学家转向了另一种S形曲线，其公式简单易记：对数逻辑曲线。`z`的对数逻辑函数是`1
    / (1 - e ** z)`，其中`e`是一个约等于2.72的常数。与累积正态分布类似，对数逻辑函数的范围从0到1，当`z == 0`时等于0.5。让我们绘制对数逻辑曲线，以及`stats.norm.cdf`（图21.11）。
- en: '![](../Images/21-11.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-11.png)'
- en: Figure 21.11 A cumulative distribution function of a normal distribution, plotted
    together with the logistic curve. Both S-shaped curves start at 0.0 and rise toward
    1.0\. They equal 0.5 when the input is 0.0\. Both curves fit our criteria for
    capturing uncertainty based on the directed distance from the decision boundary.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.11 正态分布的累积分布函数与逻辑曲线一起绘制。两条 S 形曲线都从 0.0 开始，向 1.0 上升。当输入为 0.0 时，它们等于 0.5。两条曲线都符合我们基于决策边界有向距离捕捉不确定性的标准。
- en: Listing 21.25 Measuring uncertainty using the logistic curve
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.25 使用逻辑曲线测量不确定性
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The two curves don’t precisely overlap, but they both
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这两条曲线并不完全重叠，但它们都
- en: Equal approximately `1` when `z > 5`
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `z > 5` 时，约等于 `1`
- en: Equal approximately `0` when `-z > 5`
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `-z > 5` 时，约等于 `0`
- en: Equal an ambiguous value between `0` and `1` when `-5 < z < 5`
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `-5 < z < 5` 时，等于 `0` 和 `1` 之间的一个模糊值
- en: Equal `0.5` when `z == 0`
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `z == 0` 时，等于 `0.5`
- en: 'Hence, we can use the logistic curve as our measure of uncertainty. Let’s utilize
    the curve to assign Class 1 label likelihoods for all our customers. This requires
    us to compute the directed distance between each customer’s measurements and the
    boundary. Computing these distances is surprisingly simple: we just need to execute
    `M @ weights`, where `M` is the padded feature matrix. Essentially, we were computing
    these distances all along—we just weren’t fully utilizing them until now!'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用逻辑曲线作为我们不确定性的度量。让我们利用这条曲线为所有客户分配类别 1 的标签可能性。这需要我们计算每个客户测量值与边界的有向距离。计算这些距离非常简单：我们只需要执行
    `M @ weights`，其中 `M` 是填充的特征矩阵。实际上，我们一直在计算这些距离——我们只是直到现在还没有充分利用它们！
- en: Note Let’s quickly prove that `M @ weights` returns the distances to the decision
    boundary. For clarity’s sake, we’ll use our initial weights of `[3.5, 1, -415]`,
    representing the decision boundary `lbs = -3.5 * inches - 415`. Thus, we’re taking
    the distance between measurements `(inches, lbs)` and the decision boundary point
    `(inches, -3.5 * inches + 415)`. Of course, the x-axis coordinates both equal
    `inches`, so we’re taking the distance along the y-axis. This distance equals
    `lbs - (-3.5* inches + 415)`. The formula rearranges to `3.5 * inches + lbs -
    415`. This equals the dot product of `[3.5, 1, -415]` and `[inches, lbs, 1]`.
    The first vector equals `weights`, and the second vector represents a row in `M`.
    Therefore, `M @ weights` returns an array of directed distances.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：让我们快速证明 `M @ weights` 返回到决策边界的距离。为了清晰起见，我们将使用我们的初始权重 `[3.5, 1, -415]`，它代表决策边界
    `lbs = -3.5 * inches - 415`。因此，我们正在计算测量值 `(inches, lbs)` 与决策边界点 `(inches, -3.5
    * inches + 415)` 之间的距离。当然，x 轴坐标都等于 `inches`，所以我们正在计算 y 轴上的距离。这个距离等于 `lbs - (-3.5
    * inches + 415)`。公式重新排列为 `3.5 * inches + lbs - 415`。这等于 `[3.5, 1, -415]` 和 `[inches,
    lbs, 1]` 的点积。第一个向量等于 `weights`，第二个向量代表 `M` 中的行。因此，`M @ weights` 返回一个有向距离数组。
- en: 'If `M @ weights` returns the directed distances, then `1 / (1 + e ** -(M @
    weights))` returns the Class 1 likelihoods. Listing 21.26 plots distance versus
    likelihood. We also add our binary perceptron predictions to the plot: these correspond
    to `M @ weights > 0` (figure 21.12).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `M @ weights` 返回有向距离，那么 `1 / (1 + e ** -(M @ weights))` 返回类别 1 的可能性。列表 21.26
    绘制了距离与可能性之间的关系。我们还添加了我们的二元感知机预测到图中：这些对应于 `M @ weights > 0`（图 21.12）。
- en: '![](../Images/21-12.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21-12.png)'
- en: Figure 21.12 Class 1 likelihoods from the logistic curve plotted together with
    the perceptron predictions. The likelihoods show nuance, while the perceptron
    predictions are limited to either 0 or 1.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.12 将逻辑曲线的类别 1 可能性与感知机预测一起绘制。可能性显示了细微差别，而感知机预测仅限于 0 或 1。
- en: Note As a reminder, we computed `weights` by training on the standardized features
    in `X_s`. Hence, we must append a column of ones to `X_s` to pad the feature matrix.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：作为提醒，我们通过在 `X_s` 中的标准化特征上训练来计算 `weights`。因此，我们必须向 `X_s` 添加一列 1 以填充特征矩阵。
- en: Listing 21.26 Comparing logistic uncertainty to the perceptron’s predictions
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.26 比较逻辑不确定性与感知机的预测
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Directed distances to the boundary equals the product of the padded feature
    matrix and weights.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 到边界的有向距离等于填充特征矩阵和权重的乘积。
- en: ❷ Perceptron predictions are determined by distances > 0\. Note that Python
    automatically converts Booleans True and False to integers 1 and 0, so we can
    plug distances > 0 directly into plt.scatter without an integer conversion
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 感知机预测由大于 0 的距离决定。请注意，Python 自动将布尔值 True 和 False 转换为整数 1 和 0，因此我们可以直接将大于 0
    的距离插入到 plt.scatter 中，而无需进行整数转换。
- en: 'The plotted logistic likelihoods continuously increase with directed distance.
    In contrast, the perceptron predictions are brutishly simple: the perceptron has
    either 100% confidence in a Class 1 label or 0% confidence. Interestingly, both
    the logistic curve and the perceptron are 0% confident when the directed distance
    is very negative. However, as the directed distance rises, the plots begin to
    diverge. The logistic plot is more conservative: its confidence increases slowly
    and mostly falls below 85%. Meanwhile, the perceptron model’s confidence jumps
    to 100% when `distances > 0`. This jump is unwarranted. The model is overconfident,
    like an inexperienced teenager—it is bound to make mistakes! Fortunately, we can
    teach the model caution by incorporating the uncertainty captured by the logistic
    curve.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的逻辑似然随着有向距离的增加而持续增加。相比之下，感知器的预测非常简单：感知器对类别1标签有100%的信心或0%的信心。有趣的是，当有向距离非常负时，逻辑曲线和感知器都是0%的信心。然而，随着有向距离的增加，图表开始发散。逻辑图表更为保守：其信心缓慢增加，并且大部分低于85%。同时，当`distances
    > 0`时，感知器模型的信心跳到100%。这种跳跃是没有根据的。模型过于自信，就像一个没有经验的青少年——它注定会犯错误！幸运的是，我们可以通过结合逻辑曲线捕获的不确定性来教导模型谨慎行事。
- en: We can incorporate uncertainty by updating our weight-shift computation. Currently,
    the weight shift is proportional to `predicted - actual`, where the variables
    represent predicted and actual class labels. Instead, we can make the shift proportional
    to `confidence(predicted) - actual`, where `confidence(predicted)` captures our
    confidence in the predicted class. In a perceptron model, `confidence(predicted)`
    always equals 0 or 1\. By contrast, in the nuanced logistic model, the weight
    shift takes on a more granular range of values.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过更新我们的权重偏移计算来包含不确定性。目前，权重偏移与`predicted - actual`成比例，其中变量代表预测和实际类别标签。相反，我们可以使偏移与`confidence(predicted)
    - actual`成比例，其中`confidence(predicted)`捕捉我们对预测类别的信心。在感知器模型中，`confidence(predicted)`始终等于0或1。相比之下，在细致的逻辑模型中，权重偏移采用更细粒度的值范围。
- en: Consider, for example, a data point that has a class label of 1 and lies directly
    on the decision boundary. The perceptron computes a 0 weight shift when presented
    with this data during training, so the perceptron will not adjust its weights.
    It learns absolutely nothing from the observation. By contrast, a logistic model
    returns a weight shift that’s proportional to 0.5 – 1 == –0.5\. The model will
    tweak its appraisal of the class label’s uncertainty and adjust the weights accordingly.
    Unlike the perceptron, the logistic model has a flexible capacity to learn.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个具有类别标签1且直接位于决策边界上的数据点。在训练期间，当感知器遇到这个数据时，它会计算0权重偏移，因此感知器不会调整其权重。它从观察中学习到的东西绝对为零。相比之下，逻辑模型返回的权重偏移与0.5
    – 1等于-0.5成比例。模型将调整其对类别标签不确定性的评估并相应地调整权重。与感知器不同，逻辑模型具有灵活的学习能力。
- en: Let’s update our model training code to incorporate logistic uncertainty. We
    simply need to swap our `predict` function output from `int(weights @ v > 0)`
    to `1 / (1 + e ** -(weights @ v))`. Here, we make the swap using two lines of
    code. Then we train our improved model to generate a new vector of weights and
    plot the new decision boundary to validate our result (figure 21.13).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的模型训练代码，以包含逻辑不确定性。我们只需将我们的`predict`函数输出从`int(weights @ v > 0)`更改为`1 /
    (1 + e ** -(weights @ v))`。在这里，我们使用两行代码进行替换。然后我们训练改进的模型以生成新的权重向量，并绘制新的决策边界以验证我们的结果（图21.13）。
- en: '![](../Images/21-13.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图像21-13](../Images/21-13.png)'
- en: Figure 21.13 A plot of standardized customer measurements. A logistically trained
    decision boundary separates Large and Not Large customers. The trained boundary’s
    separation is on par with the baseline decision boundary in figure 21.2.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.13 标准化客户测量的图表。逻辑训练的决策边界将大型客户和非大型客户分开。训练边界的分离与图21.2中的基线决策边界相当。
- en: Listing 21.27 Incorporating uncertainty into training
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.27 将不确定性纳入训练
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Our train function takes an optional row-level class predictor called predict.
    This predictor is preset to return int(weights @ v > 0). Here, we swap it out
    for the more nuanced logistic_predict function.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的训练函数接受一个可选的行级类预测器，称为predict。这个预测器默认返回`int(weights @ v > 0)`。在这里，我们将其替换为更细致的逻辑预测函数。
- en: 'The learned decision boundary is nearly identical to that of the perceptron
    output. However, our `train_logistic` function is subtly different: it produces
    more consistent results than the perceptron. Previously, we showed that the trained
    perceptron model performs below our baseline in four out of five training runs.
    Is this the case for `train_logistic`? Let’s find out.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的决策边界几乎与感知器输出相同。然而，我们的 `train_logistic` 函数有细微的差别：它产生的结果比感知器更一致。之前，我们展示了训练好的感知器模型在五次训练中有四次的表现低于我们的基线。这是否也适用于
    `train_logistic`？让我们来看看。
- en: Listing 21.28 Checking the logistic model’s training consistency
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.28 检查逻辑回归模型的训练一致性
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The trained model does not fall below the baseline in any of the runs, so it
    is superior to the perceptron. This superior model is called a *logistic regression
    classifier*. The model’s training algorithm is also commonly called *logistic
    regression*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型在所有运行中都没有低于基线，因此它优于感知器。这个优越的模型被称为 *逻辑回归分类器*。该模型的训练算法也通常被称为 *逻辑回归*。
- en: Note Arguably, this name is not semantically correct. Classifiers predict categorical
    variables, while regression models predict numeric values. Technically speaking,
    the logistic regression classifier uses logistic regression to predict the numeric
    uncertainty, but it is not a regression model. But the term *logistic regression*
    has become ubiquitous with the term *logistic regression classifier* in the machine
    learning community.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这个名称在语义上可能不正确。分类器预测分类变量，而回归模型预测数值。从技术上讲，逻辑回归分类器使用逻辑回归来预测数值不确定性，但它不是一个回归模型。但术语
    *逻辑回归* 在机器学习社区中与 *逻辑回归分类器* 术语变得普遍。
- en: A logistic regression classifier is trained just like a perceptron, but with
    one small difference. The weight shift is not proportional to `int(distance -
    y[i] > 0)`, where `distance = M[i] @ weights`. Instead, it is proportional to
    `1 / (1 + e ** -distance) - y[i]`. This difference leads to much more stable performance
    over random training runs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归分类器就像感知器一样进行训练，但有一个小的不同。权重偏移量不是与 `int(distance - y[i] > 0)` 成正比，其中 `distance
    = M[i] @ weights`。相反，它是与 `1 / (1 + e ** -distance) - y[i]` 成正比。这种差异导致在随机训练运行中具有更稳定的性能。
- en: Note What happens if the weight shift is directly proportional to `distance
    - y[i]`? Well, the trained model learns to minimize the distance between a line
    and the values in `y`. For classification purposes, this is not really useful;
    but for regression, it is invaluable. For instance, if we set `y` to equal `lbs`
    and `X` to equal `inches`, we could train a line to predict customer weight using
    customer height. With two lines of code, we utilize `train` to implement this
    type of linear regression algorithm. Can you figure out how?
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果权重偏移量直接与 `distance - y[i]` 成正比会发生什么？嗯，训练好的模型会学习最小化一条线与 `y` 中值的距离。对于分类目的来说，这并不是很有用；但对于回归来说，它非常有价值。例如，如果我们把
    `y` 设置为等于 `lbs`，把 `X` 设置为等于 `inches`，我们就可以训练一条线来预测顾客的体重，使用顾客的身高。通过两行代码，我们利用 `train`
    实现这种类型的线性回归算法。你能想出来吗？
- en: 21.3.1 Running logistic regression on more than two features
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1 在超过两个特征上运行逻辑回归
- en: 'We’ve trained our logistic regression model on two customer measurements: height
    (`inches`) and weight (`lbs`). However, our `train_logistic` function can process
    any number of input features. We’ll prove this by adding a third feature: customer
    waist circumference. On average, waist circumference is equal to 45% of an individual’s
    height. We’ll use this fact to simulate the customer waist measurements. Then
    we’ll input all three measurements into `train_logistic` and evaluate the trained
    model’s performance.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在两个客户测量值上训练了我们的逻辑回归模型：身高（`inches`）和体重（`lbs`）。然而，我们的 `train_logistic` 函数可以处理任意数量的输入特征。我们将通过添加第三个特征：客户腰围来证明这一点。平均而言，腰围等于个人身高的
    45%。我们将利用这一事实来模拟客户的腰围测量值。然后我们将所有三个测量值输入到 `train_logistic` 中，并评估训练好的模型的表现。
- en: Listing 21.29 Training a three-feature logistic regression model
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.29 训练一个具有三个特征的逻辑回归模型
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Each waist measurement equals 45% of a customer’s height, with a random fluctuation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个腰围测量值等于顾客身高的 45%，并带有随机波动。
- en: ❷ We need to standardize waists before appending that array to other standardized
    customer measurements.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在将数组附加到其他标准化客户测量值之前，我们需要标准化腰围。
- en: The trained three-feature model continues to perform exceptionally, with an
    f-measure of 0.97\. The main difference is that the model now contains four weights.
    The first three weights are coefficients corresponding to the three customer measurements,
    and the final weight is the bias. Geometrically, the four weights represent a
    higher-dimensional linear boundary that takes the form of a three-dimensional
    line called a *plane*. The plane separates our two customer classes in 3D space,
    as illustrated in figure 21.14.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的三特征模型继续表现出色，f-measure 为 0.97。主要区别在于模型现在包含四个权重。前三个权重是对应于三个客户测量的系数，最后一个权重是偏差。从几何上看，四个权重代表一个更高维度的线性边界，其形式为三维线，称为
    *平面*。该平面在 3D 空间中分隔我们的两个客户类别，如图 21.14 所示。
- en: '![](../Images/21-14.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-14.png)'
- en: Figure 21.14 Linear classification in 3D space. A linear plane slices through
    the data like a cleaver and separates that data into two distinct classes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.14 3D 空间中的线性分类。一个线性平面像刀一样穿过数据，将数据分成两个不同的类别。
- en: Similarly, we can optimize for linear separation in any arbitrary number of
    dimensions. The resulting weights represent a multidimensional linear decision
    boundary. Shortly, we will run logistic regression on a dataset with 13 features.
    Scikit-learn’s implementation of the logistic regression classifier will prove
    useful for this purpose.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在任意数量的维度上优化线性分离。结果权重代表一个多维线性决策边界。不久，我们将在一个具有 13 个特征的数据集上运行逻辑回归。Scikit-learn
    的逻辑回归分类器实现将证明对此目的很有用。
- en: 21.4 Training linear classifiers using scikit-learn
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.4 使用 scikit-learn 训练线性分类器
- en: Scikit-learn has a built-in class for logistic regression classification. We
    start by importing this `logisticRegression` class.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 有一个用于逻辑回归分类的内建类。我们首先导入这个 `logisticRegression` 类。
- en: Note Scikit-learn also includes a `perceptron` class, which can be imported
    from `sklearn.linear_model`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Scikit-learn 还包括一个 `perceptron` 类，可以从 `sklearn.linear_model` 中导入。
- en: Listing 21.30 Importing scikit-learn’s `LogisticRegression` class
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.30 导入 scikit-learn 的 `LogisticRegression` 类
- en: '[PRE29]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, we initialize the classifier object `clf`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化分类器对象 `clf`。
- en: Listing 21.31 Initializing scikit-learn’s `LogisticRegression` classifier
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.31 初始化 scikit-learn 的 `LogisticRegression` 分类器
- en: '[PRE30]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As discussed in section 20, we can train any `clf` by running `clf.fit(X, y)`.
    Let’s train our logistic classifier using the two-feature standardized matrix
    `X_s`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 20 节所述，我们可以通过运行 `clf.fit(X, y)` 来训练任何 `clf`。让我们使用两个特征的标准化矩阵 `X_s` 来训练我们的逻辑分类器。
- en: Listing 21.32 Training scikit-learn’s `LogisticRegression` classifier
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.32 训练 scikit-learn 的 `LogisticRegression` 分类器
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The classifier has learned the weight vector `[inches_coef, lbs_coef, bias]`.
    The vector’s coefficients are stored in the `clf.coef_` attribute. Meanwhile,
    the bias must be accessed separately using the `clf.intercept_` attribute. Combining
    these attributes gives us the full vector, which can be visualized as a decision
    boundary (figure 21.15).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器已学习到权重向量 `[inches_coef, lbs_coef, bias]`。向量的系数存储在 `clf.coef_` 属性中。同时，偏差必须通过
    `clf.intercept_` 属性单独访问。结合这些属性，我们得到完整的向量，这可以可视化为一个决策边界（图 21.15）。
- en: '![](../Images/21-15.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-15.png)'
- en: Figure 21.15 A plot of standardized customer measurements. A logistically trained
    decision boundary derived with scikit-learn separates Large and Not Large customers.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.15 标准化客户测量的图表。使用 scikit-learn 得到的逻辑训练决策边界将大型客户和非大型客户分开。
- en: Listing 21.33 Accessing the trained decision boundary
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.33 访问训练好的决策边界
- en: '[PRE32]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can make predictions on new data by executing `clf.predict`. As a reminder,
    the inputted data must be standardized for our predictions to make sense.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行 `clf.predict` 来对新数据进行预测。提醒一下，输入的数据必须标准化，以便我们的预测有意义。
- en: Listing 21.34 Predicting classes with the linear classifier
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.34 使用线性分类器预测类别
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Additionally, we can output the class-label probabilities by running `clf.predict_proba`.
    These probabilities represent the class-label uncertainties generated by the logistic
    curve.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过运行 `clf.predict_proba` 来输出类标签概率。这些概率代表由逻辑曲线生成的类标签不确定性。
- en: Listing 21.35 Outputting the uncertainty associated with each class
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.35 输出每个类别的相关不确定性
- en: '[PRE34]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the previous two code listings, we’ve relied on a custom `standardize` function
    to standardize our input data. Scikit-learn includes its own standardization class
    called `StandardScaler`. Here, we import and initialize that class.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个代码列表中，我们依赖于自定义的 `standardize` 函数来标准化我们的输入数据。Scikit-learn 包含自己的标准化类，称为 `StandardScaler`。在这里，我们导入并初始化该类。
- en: Listing 21.36 Initializing scikit-learn’s standardization class
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.36 初始化 scikit-learn 的标准化类
- en: '[PRE35]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Running `standard_scaler.fit_transform(X)` returns a standardized matrix. The
    means of the matrix columns equal 0, and the standard deviations equal 1\. Of
    course, the matrix is identical to our existing standardized matrix, `X_s`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `standard_scaler.fit_transform(X)` 返回一个标准化矩阵。矩阵列的均值等于 0，标准差等于 1。当然，矩阵与我们的现有标准化矩阵
    `X_s` 相同。
- en: Listing 21.37 Standardizing training data using scikit-learn
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.37 使用 scikit-learn 标准化训练数据
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `standard_scaler` object has learned the means and standard deviations associated
    with our feature matrix, so it can now standardize data based on these statistics.
    Listing 21.38 standardizes our `new_data` matrix by running `standard_scaler.transform(new_data)`.
    We pass the standardized data into our classifier. The predicted output should
    equal our previously seen `predictions` array.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`standard_scaler` 对象已经学会了与我们的特征矩阵相关的均值和标准差，因此现在可以根据这些统计数据标准化数据。列表 21.38 通过运行
    `standard_scaler.transform(new_data)` 标准化我们的 `new_data` 矩阵。我们将标准化后的数据传递给我们的分类器。预测输出应该等于我们之前看到的
    `predictions` 数组。'
- en: Listing 21.38 Standardizing new data using scikit-learn
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.38 使用 scikit-learn 标准化新数据
- en: '[PRE37]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: By combining the `LogisticRegression` and `StandardScaler` classes, we can train
    logistic models on complex inputs. In the next subsection, we train a model that
    can process more than two features and predict more than two class labels.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合 `LogisticRegression` 和 `StandardScaler` 类，我们可以在复杂输入上训练逻辑模型。在下一小节中，我们将训练一个可以处理超过两个特征并预测超过两个类别标签的模型。
- en: Relevant scikit-learn linear classifier methods
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的 scikit-learn 线性分类器方法
- en: '`clf = LogisticRegression()`—Initializes a logistic regression classifier'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = LogisticRegression()`—初始化逻辑回归分类器'
- en: '`scaler = StandardScaler()`—Initializes a standard scaler'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaler = StandardScaler()`—初始化一个标准缩放器'
- en: '`clf.fit(scalar.fit_transform(X))`—Trains the classifier on standardized data'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.fit(scalar.fit_transform(X))`—在标准化数据上训练分类器'
- en: '`clf.predict(scalar.transform(new_data))`—Predicts classes from the standardized
    data'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.predict(scalar.transform(new_data))`—从标准化数据预测类别'
- en: '`clf.predict_proba(scalar.transform(new_data))`—Predicts class probabilities
    from the standardized data'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.predict_proba(scalar.transform(new_data))`—从标准化数据预测类别概率'
- en: 21.4.1 Training multiclass linear models
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.4.1 训练多类别线性模型
- en: We’ve shown how linear classifiers can find decision boundaries that separate
    two classes of data. However, many problems require us to differentiate between
    more than two classes. Consider, for example, the centuries-old practice of wine
    tasting. Some experts are renowned for being able to distinguish between many
    classes of wine using sensory input. Suppose we try to build a wine-tasting machine.
    Using sensors, the machine will detect chemical patterns in a glass of wine. These
    measurements will be fed into a linear classifier as features. The classifier
    will then identify the wine (impressing us with its refinement and sophistication).
    To train the linear classifier, we need a training set. Fortunately, such a dataset
    is provided via scikit-learn. Let’s load this dataset by importing and running
    the `load_wine` function and then print the feature names and class labels from
    the data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了线性分类器如何找到将两类数据分开的决策边界。然而，许多问题需要我们区分超过两个类别。例如，考虑一下历史悠久的品酒实践。一些专家因其能够通过感官输入区分许多葡萄酒类别而闻名。假设我们尝试构建一个品酒机器。使用传感器，机器将检测一杯葡萄酒中的化学模式。这些测量值将被作为特征输入到线性分类器中。分类器将随后识别葡萄酒（以其精致和复杂度给我们留下深刻印象）。为了训练线性分类器，我们需要一个训练集。幸运的是，这样的数据集通过
    scikit-learn 提供。让我们通过导入并运行 `load_wine` 函数来加载数据集，然后打印数据中的特征名称和类别标签。
- en: Listing 21.39 Importing scikit-learn’s wine dataset
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.39 导入 scikit-learn 的葡萄酒数据集
- en: '[PRE38]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ The dataset has “flavonoids” misspelled as “flavanoids.”
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据集中“flavonoids”被误拼为“flavanoids。”
- en: The dataset contains 13 measured features, including alcohol content (Feature
    0), magnesium level (Feature 4), and hue (Feature 10). It also contains three
    classes of wine.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 13 个测量特征，包括酒精含量（特征 0）、镁含量（特征 4）和色调（特征 10）。它还包含三种葡萄酒类别。
- en: Note The actual identities of these wines are lost to time, although they probably
    correspond to different types of red wines such as Cabernet, Merlot, and Pinot
    Noir.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些酒的真正身份已经随着时间的流逝而丢失，尽管它们可能对应于不同的红葡萄酒类型，如赤霞珠、梅洛和黑皮诺。
- en: How do we train a logistic regression model to distinguish between the three
    wine types? Well, we could initially train a simple binary classifier to check
    whether a wine belongs to Class 0\. Alternatively, we could train a different
    classifier that predicts whether a wine belongs to Class 1\. Finally, a third
    classifier would determine whether the wine is a Class 2 wine. This is essentially
    scikit-learn’s built-in logic for multiclass linear classification. Given three
    class categories, scikit-learn learns three decision boundaries, one for each
    class. Then the model computes three different predictions on inputted data and
    chooses the prediction with the highest confidence level.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练逻辑回归模型来区分三种葡萄酒类型？嗯，我们最初可以训练一个简单的二分类器来检查葡萄酒是否属于类别0。或者，我们可以训练一个不同的分类器来预测葡萄酒是否属于类别1。最后，第三个分类器将确定葡萄酒是否属于类别2。这实际上是scikit-learn内置的多类线性分类逻辑。给定三个类别类别，scikit-learn学习三个决策边界，每个类别一个。然后模型对输入数据计算三个不同的预测，并选择置信度最高的预测。
- en: Note This is another reason computed confidence is critical to carrying out
    linear classification.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这是计算置信度对执行线性分类至关重要的另一个原因。
- en: If we train our logistic regression pipeline on the three-class wine data, we’ll
    obtain three decision boundaries corresponding to Classes 0, 1, and 2\. Each decision
    boundary will have its own weight vector. Every weight vector will have a bias,
    so the trained model will have three biases. These three biases will be stored
    in a three-element `clf.intercept_` array. Accessing `clf.intercept_[i]` will
    provide us with the bias for Class *i*. Let’s train the wine model and print the
    resulting three biases.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在三种类别的葡萄酒数据上训练我们的逻辑回归管道，我们将获得对应于类别0、1和2的三个决策边界。每个决策边界将有自己的权重向量。每个权重向量都将有一个偏差，因此训练好的模型将有三个偏差。这三个偏差将存储在一个三个元素的`clf.intercept_`数组中。访问`clf.intercept_[i]`将为我们提供类别*i*的偏差。让我们训练葡萄酒模型并打印出结果中的三个偏差。
- en: Listing 21.40 Training a multiclass wine predictor
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.40 训练多类葡萄酒预测器
- en: '[PRE39]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Along with the bias, each decision boundary must have coefficients. The coefficients
    are used to weigh the inputted features during classification, so there is a one-to-one
    correspondence between coefficients and features. Our dataset contains 13 features
    representing various properties of wine, so each decision boundary must have 13
    corresponding coefficients. The coefficients for the three different boundaries
    can be stored in a 3-by-13 matrix. In scikit-learn, that matrix is contained in
    `clf.coef_`. Each *i*th row of the matrix corresponds to the boundary of Class
    *i*, and each *j*th column corresponds to the *j*th feature coefficient. For example,
    we know that Feature 0 equals the alcohol content of a wine, so `clf_coeff_[2][0]`
    equals the Class 2 boundary’s alcohol coefficient.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 除了偏差之外，每个决策边界还必须有系数。系数用于分类时权衡输入的特征，因此系数与特征之间存在一一对应的关系。我们的数据集包含13个特征，代表葡萄酒的各种属性，因此每个决策边界必须有13个相应的系数。三个不同边界的系数可以存储在一个3x13的矩阵中。在scikit-learn中，该矩阵包含在`clf.coef_`中。矩阵的第*i*行对应于类别*i*的边界，第*j*列对应于第*j*个特征系数。例如，我们知道特征0等于葡萄酒的酒精含量，所以`clf_coeff_[2][0]`等于类别2边界的酒精系数。
- en: Let’s visualize the coefficient matrix as a heatmap (figure 21.16). This will
    allow us to display the feature names and class labels corresponding to the rows
    and columns. Note that the lengthy feature names are easier to read if we display
    a transpose of the matrix. Thus, we input `clf.coeff_.T` into `sns.heatmap`..
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将系数矩阵可视化为热图（图21.16）。这将使我们能够显示与行和列对应的特征名称和类别标签。注意，如果显示矩阵的转置，较长的特征名称更容易阅读。因此，我们将`clf.coeff_.T`输入到`sns.heatmap`中。
- en: '![](../Images/21-16.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-16.png)'
- en: Figure 21.16 A heatmap representing 13 feature coefficients across the three
    decision boundaries
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.16 代表三个决策边界的13个特征系数的热图
- en: Listing 21.41 Displaying a transpose of the coefficient matrix
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表21.41 显示系数矩阵的转置
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Adjusts the width and height of the plotted heatmap to 20 inches and 10 inches,
    respectively
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调整绘制的热图宽度和高度分别为20英寸和10英寸
- en: ❷ Transposes the coefficient matrix for easier display of the coefficient names
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 转置系数矩阵以便更容易显示系数名称
- en: ❸ Adjusts the label font for readability
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调整标签字体以提高可读性
- en: In the heatmap, the coefficients vary from boundary to boundary. For example,
    the alcohol coefficients equal –0.81, –1, and 0.2 for class boundaries 0, 1, and
    2, respectively. Such differences in coefficients can be very useful; they allow
    us to better understand how the inputted features drive prediction.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在热图中，系数在边界之间变化。例如，对于类别边界0、1和2，酒精系数分别为-0.81、-1和0.2。这种系数的差异非常有用；它们使我们能够更好地理解输入特征如何驱动预测。
- en: Relevant scikit-learn linear classifier attributes
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 相关scikit-learn线性分类器属性
- en: '`clf.coef_`—Accesses the coefficient matrix of a trained linear classifier'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.coef_`—访问训练好的线性分类器的系数矩阵'
- en: '`clf.intercept_`—Accesses all bias values in a trained linear classifier'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.intercept_`—访问训练好的线性分类器中的所有偏差值'
- en: 21.5 Measuring feature importance with coefficients
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.5 使用系数测量特征重要性
- en: In section 20, we discussed how the KNN classifier is not interpretable. Using
    KNN, we can predict the class associated with the inputted features, but we cannot
    comprehend why these features belong to that class. Fortunately, the logistic
    regression classifier is easier to interpret. We can gain insights into how the
    model’s features drive the prediction by examining their corresponding coefficients.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在第20节中，我们讨论了KNN分类器不可解释的问题。使用KNN，我们可以预测与输入特征相关的类别，但我们无法理解为什么这些特征属于该类别。幸运的是，逻辑回归分类器更容易解释。通过检查它们对应的系数，我们可以深入了解模型特征如何驱动预测。
- en: Linear classification is driven by the weighted sum of the features and the
    coefficients. So if a model takes three features A, B, and C and relies on three
    coefficients [1, 0, 0.25], then the prediction is partially determined by the
    value A + 0.25 * C. Note that in this example, feature B is zeroed out. Multiplying
    a zero coefficient by a feature always produces a zero value, so that feature
    never impacts the model’s predicted output.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类是由特征和系数的加权求和驱动的。因此，如果一个模型采用三个特征A、B和C并依赖于三个系数[1, 0, 0.25]，那么预测部分由A + 0.25
    * C的值决定。注意，在这个例子中，特征B被置零。将零系数乘以一个特征总是产生零值，因此该特征永远不会影响模型的预测输出。
- en: Now, let’s consider a feature whose coefficient is very close to zero. The feature
    influences predictions, but its impact is minimal. Alternatively, if a coefficient
    is far from zero, the associated feature will impact the model’s prediction much
    more heavily. Basically, coefficients with higher absolute values have more impact
    on the model, so their associated features are more important when assessing model
    performance. For instance, in our example, feature A is the most impactful because
    its coefficient is furthest from zero (figure 21.17).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个系数非常接近零的特征。该特征影响预测，但影响最小。或者，如果一个系数远离零，相关的特征将对模型预测产生更大的影响。基本上，绝对值更高的系数对模型的影响更大，因此它们相关的特征在评估模型性能时更为重要。例如，在我们的例子中，特征A是最有影响的，因为它的系数离零最远（图21.17）。
- en: '![](../Images/21-17.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-17.png)'
- en: Figure 21.17 We can visualize the weighted sum of features [A, B, C] and coefficients
    [1, 0, 0.25] as a directed graph. In the graph, the leftmost nodes represent the
    features, and the edge weights represent the coefficients. We multiply each node
    by its corresponding edge weight and sum the results. That sum equals A + C /
    4, so A is four times more impactful than C. Meanwhile, B is zeroed out and has
    no impact on the final results.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.17 我们可以将特征[A, B, C]和系数[1, 0, 0.25]的加权求和可视化为一个有向图。在图中，最左边的节点代表特征，边权重代表系数。我们乘以每个节点对应的边权重并求和。这个和等于A
    + C / 4，所以A比C影响大四倍。同时，B被置零，对最终结果没有影响。
- en: 'Features can be rated by their coefficients to assess their *feature importance*
    : a score that ranks the usefulness of features during classification. Different
    classifier models yield different feature importance scores. In linear classifiers,
    the absolute values of the coefficients serve as crude measures of importance.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过系数对特征进行评分，以评估其*特征重要性*：一个在分类过程中对特征有用性的评分。不同的分类器模型会产生不同的特征重要性评分。在线性分类器中，系数的绝对值作为粗略的重要性度量。
- en: Note The models presented in section 22 have more nuanced feature importance
    scores.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：第22节中展示的模型具有更细致的特征重要性评分。
- en: What feature is most useful for correctly detecting a Class 0 wine? We can check
    by sorting the features based on the absolute values of the Class 0 coefficients
    in `clf.coef_[0]`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个特征对于正确检测 0 类葡萄酒最有用？我们可以通过根据 `clf.coef_[0]` 中 0 类系数的绝对值对特征进行排序来检查。
- en: Listing 21.42 Ranking Class 0 features by importance
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.42 按重要性排序 0 类特征
- en: '[PRE41]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Ranks the features based on the absolute value of the coefficients in clf.coef_[class_label]
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据 clf.coef_[class_label] 中系数的绝对值对特征进行排名
- en: ❷ Computes the absolute values
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算绝对值
- en: ❸ Sorts feature indices by absolute values in descending order
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 按绝对值降序排列特征索引
- en: Proline appears at the top of the ranked list; it is a chemical commonly found
    in wine whose concentration is dependent on grape type. Proline concentration
    is the most important feature for identifying Class 0 wines. Now, let’s check
    which feature drives Class 1 wine identification.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 脯氨酸在排名列表中位居首位；它是一种在葡萄酒中常见的化学物质，其浓度取决于葡萄类型。脯氨酸浓度是识别 0 类葡萄酒最重要的特征。现在，让我们检查哪个特征驱动
    1 类葡萄酒的识别。
- en: Listing 21.43 Ranking Class 1 features by importance
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.43 按重要性排序 1 类特征
- en: '[PRE42]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Proline concentration is the most important feature for both Class 0 and Class
    1 wines. However, that feature influences the two class predictions in different
    ways: the Class 0 proline coefficient is positive (1.08), while the Class 1 coefficient
    is negative (–1.14). The coefficient signs are very important. Positive coefficients
    increase the weighted sum of linear values, and negative values decrease that
    sum. Therefore, proline decreases the weighted sum during Class 1 classification.
    That decrease leads to a negative directed distance from the decision boundary,
    so the Class 1 likelihood drops to zero. Meanwhile, the positive Class 0 coefficient
    has a completely opposite effect. Thus a high proline concentration implies the
    following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 脯氨酸浓度是 0 类和 1 类葡萄酒最重要的特征。然而，该特征以不同的方式影响两个类别的预测：0 类的脯氨酸系数为正（1.08），而 1 类的系数为负（-1.14）。系数的符号非常重要。正系数会增加线性值的加权总和，而负值会减少这个总和。因此，脯氨酸在
    1 类分类期间会减少加权总和。这种减少导致从决策边界到负的定向距离，因此 1 类的可能性降至零。同时，正的 0 类系数有完全相反的效果。因此，高脯氨酸浓度意味着以下：
- en: A wine is less likely to be a Class 1 wine.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 葡萄酒不太可能是 1 类葡萄酒。
- en: A wine is more likely to be a Class 0 wine.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 葡萄酒更有可能是 0 类葡萄酒。
- en: We can check our hypothesis by plotting histograms of proline concentration
    for the two classes of wine (figure 21.18).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制两种葡萄酒类别的脯氨酸浓度直方图来检验我们的假设（图 21.18）。
- en: '![](../Images/21-18.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/21-18.png)'
- en: Figure 21.18 A histogram of proline concentrations across Class 0 and Class
    1 wines. The Class 0 concentrations are noticeably greater than those of Class
    1\. Our classifier has picked up on this signal by making proline the top-ranking
    coefficient for both Class 0 and Class 1.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.18 0 类和 1 类葡萄酒中脯氨酸浓度的直方图。0 类的浓度明显大于 1 类。我们的分类器通过将脯氨酸作为 0 类和 1 类的最高排名系数来捕捉到这个信号。
- en: Listing 21.44 Plotting proline histograms across Classes 0 and 1 wines
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.44 绘制 0 类和 1 类葡萄酒的脯氨酸直方图
- en: '[PRE43]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: On average, proline concentration is higher in Class 0 than in Class 1\. This
    difference serves as a signal for distinguishing between the two wines. Our classifier
    has successfully learned this signal. By probing the classifier’s coefficients,
    we have also learned something about the chemical makeup of different wines.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，0 类的脯氨酸浓度高于 1 类。这种差异是区分两种葡萄酒的信号。我们的分类器成功地学习了这个信号。通过探测分类器的系数，我们还了解到了不同葡萄酒的化学成分。
- en: Unlike KNN models, logistic regression classifiers are interpretable. They’re
    also easy to train and fast to run, so linear classifiers are an improvement over
    KNN models. Unfortunately, linear classifiers still suffer from some very serious
    flaws that limit their practical use in certain circumstances.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与 KNN 模型不同，逻辑回归分类器是可解释的。它们也易于训练和快速运行，因此线性分类器比 KNN 模型有所改进。不幸的是，线性分类器仍然存在一些非常严重的缺陷，限制了它们在某些情况下的实际应用。
- en: 21.6 Linear classifier limitations
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.6 线性分类器局限性
- en: 'Linear classifiers work poorly on raw data. As we’ve observed, standardization
    is required to achieve the best results. Similarly, linear models cannot handle
    categorical features without data preprocessing. Suppose we’re building a model
    to predict whether a pet will be adopted from a shelter. Our model can predict
    on three pet categories: cat, dog, and bunny. The simplest way to represent these
    categories is with numbers: 0 for cat, 1 for dog, and 2 for bunny. However, this
    representation will cause a linear model to fail. The model gives bunnies twice
    the attention that it gives dogs, and it entirely ignores cats. For the model
    to treat each pet with equal attention, we must transform the categories into
    a three-element binary vector `v`. If a pet belongs to category `i`, `v[i]` is
    set to `1`. Otherwise, `v[i]` equals `0`. Thus, we represent a cat as `v = [1,
    0, 0]`, a dog as `v = [0, 1, 0]`, and a bunny as `v = [0, 0, 1]`. This vectorization
    is similar to the text vectorization seen in section 13\. We can carry it using
    scikit-learn. Still, such transformations can be cumbersome. The models covered
    in the subsequent section can analyze raw data without additional preprocessing.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器在原始数据上表现不佳。正如我们所观察到的，标准化是获得最佳结果所必需的。同样，线性模型无法处理未经数据预处理的分类特征。假设我们正在构建一个模型来预测宠物是否会从收容所被领养。我们的模型可以在三个宠物类别上进行预测：猫、狗和兔子。表示这些类别的最简单方法是用数字：猫为
    0，狗为 1，兔子为 2。然而，这种表示会导致线性模型失败。模型给兔子的关注是狗的两倍，并且完全忽略了猫。为了使模型对每种宠物给予同等关注，我们必须将类别转换为三个元素的二进制向量
    `v`。如果一个宠物属于类别 `i`，则 `v[i]` 被设置为 `1`。否则，`v[i]` 等于 `0`。因此，我们用 `v = [1, 0, 0]` 表示猫，用
    `v = [0, 1, 0]` 表示狗，用 `v = [0, 0, 1]` 表示兔子。这种向量化类似于第 13 节中看到的文本向量化。我们可以使用 scikit-learn
    来执行这种转换。然而，这种转换可能很繁琐。下一节中介绍的模式可以在不进行额外预处理的情况下分析原始数据。
- en: Note Categorical variable vectorization is often called *one-hot encoding*.
    Scikit-learn includes a `OneHotEncoder` transformer, which can be imported from
    `sklearn.preprocessing`. The `OneHotEncoder` class can automatically detect and
    vectorize all categorical features in your training set.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 注意分类变量向量化通常被称为*独热编码*。Scikit-learn 包含一个 `OneHotEncoder` 转换器，可以从 `sklearn.preprocessing`
    中导入。`OneHotEncoder` 类可以自动检测并向量化训练集中所有的分类特征。
- en: 'The most serious limitation of linear classifiers is right there in the name:
    linear classifiers learn *linear* decision boundaries. More precisely, a line
    (or plane in higher dimensions) is required to separate the classes of data. However,
    there are countless classification problems that are not linearly separable. Consider,
    for example, the problem of classifying urban and non-urban households. Let’s
    assume prediction is driven by the distance to the city center. All households
    less than two distance units from the center are classified as urban; all other
    households are considered suburban. The following code simulates these households
    with a 2D normal distribution. We also train a logistic regression classifier
    to distinguish between household classes. Finally, we visualize the model’s linear
    boundary and the actual households in 2D space (figure 21.19).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器的最严重限制就在其名称中：线性分类器学习*线性*决策边界。更准确地说，需要一条线（或在更高维度的平面）来分离数据的类别。然而，有无数分类问题不是线性可分的。例如，考虑将城市家庭和非城市家庭进行分类的问题。假设预测是由距离城市中心的距离驱动的。所有距离中心小于两个距离单位的家庭都被归类为城市家庭；所有其他家庭被认为是郊外家庭。以下代码使用二维正态分布模拟这些家庭。我们还训练了一个逻辑回归分类器来区分家庭类别。最后，我们在二维空间中可视化模型的线性边界和实际的家庭（图
    21.19）。
- en: '![](../Images/21-19.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图 21.19](../Images/21-19.png)'
- en: Figure 21.19 Simulated households plotted relative to the city center at `(0,`
    `0`). Households that are closer to the center are considered urban. No linear
    separation exists between the urban and suburban households, so the trained linear
    boundary is unable to distinguish between them.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.19 模拟的家庭相对于城市中心 `(0, 0)` 的位置。靠近中心的家庭被认为是城市家庭。城市家庭和郊外家庭之间不存在线性分离，因此训练的线性边界无法将它们区分开来。
- en: Listing 21.45 Simulating a nonlinearly separable scenario
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 21.45 模拟非线性可分场景
- en: '[PRE44]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ The x and y coordinates of each household are drawn from two standard normal
    distributions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个家庭的 x 和 y 坐标来自两个标准正态分布。
- en: ❷ The city center is located at a coordinate (0, 0). A household’s spatial distance
    from the center is therefore equal to its norm. Households within two units of
    the center are labeled urban.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 城市中心位于坐标（0，0）。因此，一个家庭与中心的距离等于其范数。距离中心两个单位以内的家庭被标记为城市。
- en: ❸ Our data was drawn from a distribution with a mean of 0 and an std of 1\.
    Standardization is therefore not required to train the linear model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们的数据来自均值为0，标准差为1的分布。因此，对线性模型进行标准化不是必需的。
- en: ❹ Plots the trained decision boundary alongside the household coordinates
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制训练好的决策边界与家庭坐标一起
- en: The linear boundary fails to separate the classes. The dataset’s geometry does
    not allow for such a separation. In data science terms, the data is *not linearly
    separable*. Hence, a linear classifier cannot be adequately trained. We need to
    run a nonlinear approach. In the subsequent section, we learn about decision tree
    techniques that can overcome this limitation.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 线性边界无法分离类别。数据集的几何形状不允许进行这种分离。在数据科学术语中，数据是**非线性可分的**。因此，线性分类器无法得到充分的训练。我们需要运行非线性方法。在下一节中，我们将学习可以克服这一局限性的决策树技术。
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In certain instances, we can separate data classes using *linear decision boundaries*.
    All data points below the linear boundary are classified as belonging to Class
    0, and all data points above the linear boundary are classified as belonging to
    Class 1\. Effectively, the linear boundary checks whether the weighted features
    and a constant add to a value greater than zero. The constant value is called
    the *bias*, and the remaining weights are called the *coefficients*.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可以使用**线性决策边界**来分离数据类别。所有位于线性边界以下的数据点被分类为属于类别0，所有位于线性边界以上的数据点被分类为属于类别1。实际上，线性边界检查加权特征和一个常数相加是否大于零。这个常数值被称为**偏置**，其余的权重被称为**系数**。
- en: Through algebraic manipulation, we can transform linear classification into
    the matrix product inequality defined by `M @ weights > 0`. Such multiplication-driven
    classification defines a *linear classifier*. The matrix `M` is a *padded feature
    matrix* with an appended column of ones, `weights` is a vector, and the final
    vector element is the bias. The remaining weights are coefficients.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过代数操作，我们可以将线性分类转化为由 `M @ weights > 0` 定义的矩阵乘积不等式。这种由乘法驱动的分类定义了一个**线性分类器**。矩阵
    `M` 是一个带有附加一列的**填充特征矩阵**，`weights` 是一个向量，最终向量的元素是偏置。其余的权重是系数。
- en: 'To obtain a good decision boundary, we start by randomly initializing `weights`.
    We then iteratively adjust the weights based on the difference between predicted
    and actual classes. In the simplest possible linear classifier, this weight shift
    is proportional to the difference between the predicted and actual classes. Hence,
    the weight shift is proportional to one of three values: –1, 0, or 1.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获得一个好的决策边界，我们首先随机初始化 `weights`。然后，我们根据预测类别和实际类别之间的差异迭代调整权重。在最简单的线性分类器中，这种权重偏移与预测类别和实际类别之间的差异成正比。因此，权重偏移与三个值之一成比例：-1、0或1。
- en: We should never tweak a coefficient if the associated feature equals zero. We
    can ensure this constraint if we multiply the weight shift by the corresponding
    feature value in matrix `M`.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果相关的特征等于零，我们绝不应该调整系数。我们可以通过将权重偏移乘以矩阵 `M` 中相应的特征值来确保这个约束。
- en: Iteratively tweaking the weights can cause the classifier to fluctuate between
    good and subpar performance. To limit oscillation, we need to lower the weight
    shift with every subsequent iteration. This can be done by dividing the weight
    shift by `k` over each *k* th iteration.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代调整权重可能导致分类器在良好和较差的性能之间波动。为了限制波动，我们需要在每次后续迭代中降低权重偏移。这可以通过在每 *k* 次迭代中将权重偏移除以
    `k` 来实现。
- en: Iterative weight adjustment can converge on a decent decision boundary, but
    it is not guaranteed to locate the optimal decision boundary. We can improve the
    boundary by decreasing the data’s mean and standard deviation. Such *standardization*
    can be achieved if we subtract the means and then divide by the standard deviations.
    The resulting dataset has a mean of 0 and a standard deviation of 1.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代权重调整可以收敛到一个不错的决策边界，但并不能保证找到最优的决策边界。我们可以通过减小数据的均值和标准差来改善边界。这种**标准化**可以通过减去均值然后除以标准差来实现。结果数据集的均值为0，标准差为1。
- en: The simplest linear classifier is known as a *perceptron*. Perceptrons perform
    well, but their results can be inconsistent. The failure of the perceptrons are
    partially caused by a lack of nuance. Points closer to the decision boundary are
    more ambiguous with regard to their classification. We can capture this uncertainty
    using an S-shaped curve that ranges between values of 0 and 1\. The cumulative
    normal distribution function serves as a decent measure of uncertainty, but the
    simpler *logistic curve* is easier to compute. The logistic curve is equal to
    `1 / (1 + e ** -z)`.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最简单的线性分类器被称为 **感知器**。感知器表现良好，但它们的输出可能不一致。感知器失败的部分原因是缺乏细微差别。靠近决策边界的点在分类上更加模糊。我们可以使用介于
    0 和 1 之间的 S 形曲线来捕捉这种不确定性。累积正态分布函数可以作为不确定性的一个不错的度量，但更简单的 **逻辑曲线**更容易计算。逻辑曲线等于 `1
    / (1 + e ** -z)`。
- en: We can incorporate uncertainty into model training by setting the weight shift
    proportionally to `actual - 1 / (1 + e ** -distance)`. Here, `distance` represents
    the directed distance to the decision boundary. We can compute all directed distances
    by running `M @ weights`.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过将权重偏移量按比例设置为 `实际 - 1 / (1 + e ** -distance)` 来将不确定性纳入模型训练。在这里，`distance`
    代表到决策边界的有向距离。我们可以通过运行 `M @ weights` 来计算所有有向距离。
- en: A classifier trained using logistic uncertainty is called a *logistic regression
    classifier*. This classifier yields more consistent results than a simple perceptron.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑不确定性训练的分类器被称为 **逻辑回归分类器**。这种分类器比简单的感知器产生更一致的结果。
- en: Linear classifiers can be extended to *N* classes by training *N* different
    linear decision boundaries.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过训练 **N** 个不同的线性决策边界，可以将线性分类器扩展到 **N** 个类别。
- en: The coefficients in a linear classifier serve as a measure of *feature importance*.
    Coefficients with the largest absolute values map to features that have a significant
    impact on a model’s predictions. The coefficient’s sign determines whether the
    presence of a feature indicates the presence or absence of a class.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性分类器中的系数作为 **特征重要性** 的度量。具有最大绝对值的系数映射到对模型预测有显著影响的特征。系数的符号决定了特征的存在与否表示类别的存在或不存在。
- en: Linear classification models fail whenever the data is not *linearly separable*
    and a good linear decision boundary does not exist.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据不是 **线性可分** 且不存在良好的线性决策边界时，线性分类模型会失败。
