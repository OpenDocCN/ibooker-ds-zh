- en: Chapter 3\. Introduction to Big Data and Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。大数据与数据科学简介
- en: 'The popular use of big data can be traced to a single research paper published
    in 2004: [“MapReduce: Simplified Data Processing on Large Clusters”](https://ai.google/research/pubs/pub62),
    by Jeffrey Dean and Sanjay Ghemawat. In this 13-page paper (including source code),
    two engineers at Google explained how the company had found a way to bring its
    gigantic indexing needs down to reasonable processing requirements through a radically
    new type of algorithm running on massively parallel clusters. The basic idea of
    MapReduce is to break work into *mappers* that can run in parallel and *reducers*
    that take the output of mappers and process it. The first operation is called
    “mapping” because it takes each element of input data and “maps” a function onto
    it, leaving the output for the reducer to handle.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '大数据的流行可以追溯到2004年发表的一篇研究论文：[《MapReduce: Simplified Data Processing on Large
    Clusters》](https://ai.google/research/pubs/pub62)，由 Jeffrey Dean 和 Sanjay Ghemawat
    撰写。在这篇13页的论文（包括源代码）中，Google 的两位工程师解释了公司如何通过一种全新的基于大规模并行集群的算法，将其庞大的索引需求降低到合理的处理要求。MapReduce
    的基本思想是将工作分解为可以并行运行的“映射器”和处理映射器输出的“减少器”。第一个操作称为“映射”，因为它将输入数据的每个元素“映射”到一个函数上，将输出留给减少器处理。'
- en: For example, to count words in all the documents on all the nodes in a cluster,
    assuming each document is stored on a single node, we can have thousands of mappers,
    running in parallel, produce a list of documents and the word count of each, and
    send that list to the reducer. The reducer will then create a master list of all
    documents with their word counts and calculate the total word count by adding
    all the counts for all the documents together ([Figure 3-1](#the_basic_architecture_behind_mapreduce)).
    Assuming that the disk is much slower than the network and that the mapper reading
    documents is much slower than sending a total to the reducer, this program would
    scale very nicely to a large cluster without any perceptible performance degradation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要统计集群中所有节点上所有文档中的单词数，假设每个文档存储在单个节点上，我们可以有成千上万个并行运行的映射器，生成每个文档及其单词计数的列表，并将该列表发送给减少器。然后，减少器将创建包含所有文档及其单词计数的主列表，并通过将所有文档的所有计数相加来计算总单词计数（[图 3-1](#the_basic_architecture_behind_mapreduce)）。假设磁盘比网络慢得多，映射器读取文档比发送总数给减少器慢得多，这个程序将在大型集群中非常好地扩展，而且性能几乎没有明显的下降。
- en: '![The basic architecture behind MapReduce](Images/ebdl_0301.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce 的基本架构](Images/ebdl_0301.png)'
- en: Figure 3-1\. The basic architecture behind MapReduce
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1。MapReduce 的基本架构
- en: Hadoop Leads the Historic Shift to Big Data
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 领导历史性的大数据转型
- en: Although Google did not release its internal MapReduce tools, developers inspired
    by the paper created a free, open source implementation called Hadoop that quickly
    became central to the processing of big data by organizations everywhere.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Google 没有发布其内部的 MapReduce 工具，但受论文启发的开发者创建了一个名为 Hadoop 的免费开源实现，它很快成为各个组织处理大数据的核心工具。
- en: The Hadoop File System
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop 文件系统
- en: A special filesystem is needed to provide data efficiently to MapReduce, and
    the most popular one is the Hadoop File System (HDFS). It is a massively parallel,
    highly available, self-healing filesystem. However, it makes no attempt to implement
    the relational model (although SQL-like interfaces were later built on top of
    it). Instead, like many of the other NoSQL databases that were growing up at the
    time, HDFS is a sophisticated kind of key/value store.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 需要一个特殊的文件系统来高效提供数据，而最流行的就是 Hadoop 文件系统（HDFS）。它是一个大规模并行、高可用、自我修复的文件系统。然而，它并未试图实现关系模型（尽管后来在其之上构建了类似
    SQL 的接口）。相反，与当时正在成长的许多其他 NoSQL 数据库一样，HDFS 是一种复杂的键值存储系统。
- en: It makes multiple copies of each block (by default, three copies) and stores
    these copies on different nodes. This way, if one node dies, two other copies
    are still available and the block will be copied to a third node once the failure
    is detected without affecting availability. The multiple copies also facilitate
    load balancing, because we can choose to send the work to the least busy node
    that contains the data. For example, in [Figure 3-2](#example_of_distributed_storage_in_hdfs),
    file 11 is stored across different nodes. It has two blocks. Block 1 is stored
    on nodes 1, 2, and 3, while block 2 is stored on nodes 1, 3, and 4\. When the
    file is being processed, work for block 1 can be performed on any of nodes where
    it is stored—whichever is less busy. Similarly, work on block 2 can be performed
    on any of the three nodes where it is stored. If one of the nodes—say, node 3—were
    to get corrupted or otherwise become unavailable, HDFS would still have access
    to the two other copies of each block stored on node 3 and would make sure to
    create replacement copies for the blocks originally stored on node 3 on other
    nodes that are still up and running.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它会为每个块创建多个副本（默认情况下为三个副本），并将这些副本存储在不同的节点上。这样，如果一个节点失效，仍然可以使用另外两个副本，并且在检测到故障后，会将该块复制到第三个节点，而不会影响可用性。多个副本还有助于负载均衡，因为我们可以选择将工作发送到包含数据的最不繁忙的节点。例如，在[图 3-2](#example_of_distributed_storage_in_hdfs)中，文件11存储在不同的节点上。它有两个块。块1存储在节点1、2和3上，而块2存储在节点1、3和4上。在处理文件时，可以在存储块1的任何节点上执行块1的工作，选择最不繁忙的节点。类似地，可以在存储块2的三个节点中的任何一个上执行块2的工作。如果其中一个节点（例如节点3）损坏或变得不可用，HDFS仍然可以访问存储在节点3上的每个块的另外两个副本，并确保在其他仍然运行的节点上为最初存储在节点3上的块创建替代副本。
- en: '![Example of distributed storage in HDFS](Images/ebdl_0302.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![HDFS中分布式存储的示例](Images/ebdl_0302.png)'
- en: Figure 3-2\. Example of distributed storage in HDFS
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. HDFS中分布式存储的示例
- en: How Processing and Storage Interact in a MapReduce Job
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何处理和存储在MapReduce作业中的交互
- en: In our previous example of counting words, a job will be created that contains
    a list of all the blocks in all the files and the job manager will send the work
    for each block to the least-loaded node that contains it, evening the load across
    the cluster. Of course, to create a file list, we now have to reassemble the files
    from the blocks. Assuming that’s too much for a single node, we can have multiple
    reducers processing the work. In order to make sure that the same reducer gets
    all the blocks for a single file, we will leverage what’s called the *shuffle*
    step of MapReduce, where the output of the mappers contains a key and a value
    and a shuffle function is applied to the key to send all work with the same key
    to the same reducer. For example, we can use a hash function that takes a filename
    and returns 0 or 1, perhaps by adding all the ASCII values for the letters in
    the filename and dividing by 2\. All output for files whose names hash to 0 goes
    to reducer 1, whereas output for files whose names hash to 1 goes to reducer 2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的计算单词示例中，将创建一个作业，其中包含所有文件中所有块的列表，并且作业管理器将将每个块的工作发送到包含它的最少负载节点，从而平衡集群中的负载。当然，为了创建文件列表，我们现在必须从块中重新组装文件。假设这对于单个节点来说太多了，我们可以有多个减少者来处理工作。为了确保同一个减少者获得单个文件的所有块，我们将利用MapReduce的*shuffle*步骤，其中映射器的输出包含键和值，并且对键应用洗牌函数，以便将所有具有相同键的工作发送到同一个减少者。例如，我们可以使用一个哈希函数，该函数获取文件名并返回0或1，例如通过添加文件名中所有字母的ASCII值并除以2来执行此操作。所有名称哈希为0的文件的输出都会发送到减少者1，而名称哈希为1的文件的输出则会发送到减少者2。
- en: When there are multiple reducers, in order to create a single file we will need
    to channel all of the reducers to a single reducer that will assemble the final
    output into one file, adding complexity and processing time. Instead, to optimize
    for multiple reducers working in parallel, most MapReduce jobs generate multiple
    files, typically in the same directory. To enable this, most Hadoop components
    work on directories rather than files. For example, Pig scripts, Hive, and other
    projects expect a directory as input rather than a file, and treat all files in
    that directory as a single “logical” file. In [Figure 3-3](#mapperscomma_reducerscomma_and_file_stor),
    a folder called *WordCount* is created, and files with unique names contain the
    output of the two reducers. The names of the files are usually generated automatically,
    so they aren’t meaningful and are never used directly except by the internal program
    code that needs to read them. All work is done on the *WordCount* folder, treating
    it as a single logical file. This is equivalent to concatenating all the files
    in the folder.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个Reducer时，为了创建单个文件，我们需要将所有Reducer引导到一个单独的Reducer，将最终输出组合成一个文件，增加了复杂性和处理时间。相反，为了优化多个并行工作的Reducer，大多数MapReduce作业生成多个文件，通常在同一个目录中。为了实现这一点，大多数Hadoop组件处理目录而不是文件。例如，Pig脚本、Hive和其他项目期望目录作为输入而不是文件，并将该目录中的所有文件视为单个“逻辑”文件。在[图3-3](#mapperscomma_reducerscomma_and_file_stor)中，创建了一个名为*WordCount*的文件夹，包含两个Reducer输出的文件。文件的名称通常是自动生成的，因此它们没有实际意义，也从不直接使用，除非内部程序代码需要读取它们。所有工作都在*WordCount*文件夹上进行，将其视为单个逻辑文件。这等同于连接文件夹中的所有文件。
- en: '![Mappers, reducers, and file storage in Hadoop](Images/ebdl_0303.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop中的Mapper、Reducer和文件存储](Images/ebdl_0303.png)'
- en: Figure 3-3\. Mappers, reducers, and file storage in Hadoop
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. Hadoop中的Mapper、Reducer和文件存储
- en: Because so much work is done on the block level, Hadoop usually has a large
    block size, defaulting to 64 or 128 MB. Since only one file can be stored in a
    block, this makes Hadoop not very efficient for storing small files—a 1 KB file
    would still take a whole block of 64 or 128 MB. To optimize storage, *sequence
    files* were introduced. A sequence file is a collection of key/value pairs and
    is often used to store lots of small files in one large file by using the smaller
    files’ names as the keys and the contents as the values.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因为大部分工作在块级别进行，Hadoop通常具有较大的块大小，默认为64或128 MB。由于一个块只能存储一个文件，这使得Hadoop不适合存储小文件
    —— 一个1 KB的文件仍然需要整个64或128 MB的块。为了优化存储，*序列文件* 被引入。序列文件是键/值对的集合，通常用于将大量小文件存储在一个大文件中，使用较小文件的名称作为键和内容作为值。
- en: While very efficient and elegant, MapReduce requires developers to think about
    their logic carefully and divide the work correctly. If the work is not distributed
    properly, the job can suffer from significant performance degradation. For example,
    if we have 1,000 mappers running on a 1,000-node cluster and 999 of the mappers
    take 5 minutes to run but the last mapper takes 5 hours, the entire job will still
    require 5 hours to run.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非常高效和优雅，MapReduce要求开发人员仔细思考他们的逻辑并正确划分工作。如果工作没有正确分布，作业可能会遭受显著的性能下降。例如，如果我们有1,000个mapper运行在一个1,000节点的集群上，其中999个mapper花费5分钟运行，但最后一个mapper花费5小时，整个作业仍然需要5小时才能完成。
- en: Schema on Read
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Schema on Read
- en: In relational databases, the schema (a list of columns, their names, and their
    types) of a table is defined when the table is created. When data is inserted
    into the table, it has to conform to this predefined structure. This requires
    a very formal and careful approach to schema management because if the data does
    not match the table schema it will not be inserted or stored in a data warehouse.
    In fact, there won’t be any place it can be inserted, since inserting it will
    require the definition of a schema that matches the data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系数据库中，表的模式（列的列表、它们的名称和它们的类型）在创建表时被定义。当数据插入表中时，它必须符合这个预定义的结构。这要求对模式管理采取非常正式和谨慎的方法，因为如果数据不符合表的模式，它将无法被插入或存储在数据仓库中。事实上，没有任何地方可以插入它，因为插入它将需要定义一个与数据匹配的模式。
- en: Since HDFS is a filesystem (it basically looks like a Linux filesystem to the
    user), it can store all sorts of data. Of course, in order to do any processing,
    the data has to be given a schema or structure. To achieve this, Hadoop takes
    a “schema on read” approach—it applies the schema to the data when it reads it.
    For example, a user can define an external Hive table for a file in HDFS. When
    that table is queried, Hive will attempt to map the data in the file to the table
    definition, giving it a schema. If the data does not match the schema definition,
    the query will fail. However, unlike with a relational database, the data is still
    stored in HDFS and preserved. It just cannot be used until the correct schema
    is applied. Because of this approach, data can be added to HDFS with minimal effort,
    without any checks and without any schema being defined.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HDFS是一个文件系统（对用户来说基本上看起来像是Linux文件系统），它可以存储各种类型的数据。当然，为了进行任何处理，数据必须具有模式或结构。为此，Hadoop采用“读时模式”方法——它在读取数据时应用模式。例如，用户可以为HDFS中的文件定义一个外部Hive表。当查询该表时，Hive将尝试将文件中的数据映射到表定义中，赋予其一个模式。如果数据与模式定义不匹配，则查询将失败。然而，与关系数据库不同的是，数据仍然存储在HDFS中并得以保存。只是在应用正确的模式之前无法使用。由于这种方法，可以轻松地向HDFS添加数据，无需任何检查和无需定义任何模式。
- en: Hadoop Projects
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop项目
- en: Hadoop has spawned a rich ecosystem of projects to do everything from ingestion
    to administration to management. Tables [3-1](#popular_on-premises_tools_related_to_had)
    and [3-2](#popular_cloud-based_tools_related_to_had) cover some of the most popular
    projects traditionally included with Hadoop distributions. Most are open source,
    although some components of Cloudera and MapR require a commercial subscription
    (indicated with a * in [Table 3-1](#popular_on-premises_tools_related_to_had)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop已经衍生出一个丰富的项目生态系统，涵盖从数据摄入到管理和管理的所有内容。表 [3-1](#popular_on-premises_tools_related_to_had)
    和 [3-2](#popular_cloud-based_tools_related_to_had) 涵盖了一些传统上包含在Hadoop发行版中最受欢迎的项目。大多数是开源的，尽管Cloudera和MapR的某些组件需要商业订阅（在
    [Table 3-1](#popular_on-premises_tools_related_to_had) 中用 * 标示）。
- en: Table 3-1\. *Popular on-premises tools related to Hadoop and HDFS*
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Table 3-1\. *与Hadoop和HDFS相关的热门本地工具*
- en: '|   | **Apache Hadoop** | **Cloudera** | **Hortonworks (including IBM, Microsoft
    Azure, and Pivotal)** | **MapR** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|   | **Apache Hadoop** | **Cloudera** | **Hortonworks（包括IBM、Microsoft Azure和Pivotal）**
    | **MapR** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Ingestion** | Sqoop, Flume |   | NiFi |   |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **摄入** | Sqoop, Flume |   | NiFi |   |'
- en: '| **Relational interface/DB** | Hive | Hive, Impala* | Hive | Hive, Drill |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **关系接口/数据库** | Hive | Hive, Impala* | Hive | Hive, Drill |'
- en: '| **NoSQL** | HBase | HBase, Kudu | HBase | MapRDB (variant of HBase) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **NoSQL** | HBase | HBase, Kudu | HBase | MapRDB（HBase的变种） |'
- en: '| **Security** | Ranger | Sentry* | Ranger |   |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **安全** | Ranger | Sentry* | Ranger |   |'
- en: '| **Governance** | Atlas | Navigator* | Atlas | Resells Waterline* |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **治理** | Atlas | Navigator* | Atlas | Resells Waterline* |'
- en: '| **Filesystem** | HDFS | HDFS | HDFS | MapR-FS |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **文件系统** | HDFS | HDFS | HDFS | MapR-FS |'
- en: Table 3-2\. *Popular cloud-based tools related to Hadoop and HDFS*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Table 3-2\. *与Hadoop和HDFS相关的热门云工具*
- en: '|   | **AWS** | **Azure** | **Google Cloud Platform** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|   | **AWS** | **Azure** | **Google Cloud Platform** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Ingestion** | Kinesis | Event Hub | Cloud Pub/Sub |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **摄入** | Kinesis | 事件中心 | Cloud Pub/Sub |'
- en: '| **Integration** | Glue | ADF | Cloud Dataflow |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **集成** | Glue | ADF | Cloud Dataflow |'
- en: '| **Relational interface/DB** | Hive, Presto, RedShift, Aurora | Hive | Cloud
    Spanner |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **关系接口/数据库** | Hive, Presto, RedShift, Aurora | Hive | Cloud Spanner |'
- en: '| **NoSQL** | DynamoDB | AzureNoSQL | Bigtable |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **NoSQL** | DynamoDB | AzureNoSQL | Bigtable |'
- en: '| **Security** |   | Security Center |   |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **安全** |   | 安全中心 |   |'
- en: '| **Governance** | Glue | Azure Governance |   |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **治理** | Glue | Azure治理 |   |'
- en: '| **Filesystem** | EBS, EFS | ADLS | ECFS |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **文件系统** | EBS, EFS | ADLS | ECFS |'
- en: '| **Object store** | S3 | Blob Storage | GCS |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **对象存储** | S3 | Blob存储 | GCS |'
- en: One of the most important developments in the Hadoop ecosystem was *Spark*,
    an extension of the concept that offers both more speed and more flexibility than
    MapReduce. Spark originated as network speeds improved and reduced the need to
    tightly couple compute and storage. Spark was started at UC Berkeley’s AMPLab
    in 2009 and is now a top-level Apache project. It is commercially supported by
    Databricks and is included in every Hadoop distribution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop生态系统中最重要的发展之一是*Spark*，这是一个概念的扩展，比MapReduce提供了更快的速度和更大的灵活性。随着网络速度的提高，Spark起源于UC
    Berkeley的AMPLab于2009年，并成为Apache顶级项目。它得到了Databricks的商业支持，并包含在每个Hadoop发行版中。
- en: The core idea of Spark is to create a large in-memory data set across a cluster
    of computers. If HDFS strove to create a single persistent filesystem across a
    cluster, Spark effectively creates one large memory space across the cluster.
    At the core of Spark is the Resilient Distributed Dataset (RDD), which appears
    as a single data set to the programs using Spark.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的核心思想是在计算机集群上创建一个大的内存数据集。如果HDFS努力在集群中创建一个单一的持久性文件系统，那么Spark有效地在集群中创建一个大的内存空间。Spark的核心是弹性分布式数据集（RDD），对于使用Spark的程序来说，它看起来像一个单一的数据集。
- en: Another improvement in Spark is the generalization of the old MapReduce model.
    Instead of a single pipeline consisting of mapper, shuffler, and reducer (with
    multiple instances potentially running in parallel), complex pipelines can pass
    data through multiple different instances of each stage. For instance, Spark can
    easily pass the output of one reducer to another reducer, which in Hadoop would
    require cumbersome manual coding and slow writes and reads to and from the disk.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个Spark的改进是对旧的MapReduce模型的泛化。不再是由单一的包含映射器、分组器和减少器的管道（可能并行运行多个实例）组成，而是复杂的管道可以通过多个不同阶段的多个实例传递数据。例如，Spark可以轻松地将一个减少器的输出传递给另一个减少器，在Hadoop中则需要繁琐的手动编码，并且读写磁盘时速度缓慢。
- en: Although Spark is written in Scala, it can be used through interfaces in Java,
    Python, R, and other languages as well as SparkSQL, a SQL interface to RDDs based
    on a layer of abstraction called a DataFrame.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark是用Scala编写的，但也可以通过Java、Python、R和其他语言的接口使用，以及SparkSQL，它是基于称为DataFrame的抽象层的RDD的SQL接口。
- en: Data Science
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: 'A lot of analytics are *descriptive*: they look back at what has happened and
    rely on human experts to examine the history and make decisions about the future.
     Sometimes humans do a good job of this, but sometimes they don’t. Often they
    rely on their intuition and personal experience and have very little opportunity
    to validate their decisions. Imagine if, instead, you could *predict* what was
    going to happen before it happened and validate it against the historical data?
    Or if you could actually test things on a small subset of users before rolling
    them out on a large scale?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 很多分析是*描述性*的：它们回顾过去发生的事情，并依赖人类专家来审查历史并做出关于未来的决策。有时人类在这方面做得很好，但有时却不尽如人意。他们常常依赖直觉和个人经验，并很少有机会验证他们的决定。想象一下，如果你能在事情发生之前就*预测*会发生什么，并根据历史数据验证它？或者如果你能在大规模推广之前在小部分用户中实际测试这些事物呢？
- en: 'The idea of data science is to make recommendations about actions to take based
    on factual information represented as data. Even the name of the discipline is
    indicative of the underlying principle. When I asked DJ Patil, who coined the
    phrase, why he called it “data science,” he told me how he’d started a new group
    at LinkedIn focused on using data and advanced analytics to answer all sorts of
    questions about LinkedIn’s user experience and business. To decide what to call
    this group, they placed three different wanted ads describing the person they
    were looking for. All three ads were for the same position with the same job description
    and preferred experience, placed on the same job sites. The only difference was
    the name: “Data Scientist,” “Data Analyst,” and “Data Engineer.” The ad that drew
    the most applicants was “Data Scientist,” so they named the group “Data Science.”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的理念是根据以数据形式表示的事实信息来进行行动建议。甚至这个学科的名称也显示了其基本原则。当我询问起“数据科学”这一术语的创始人DJ Patil时，为什么要这样称呼，他告诉我他如何在LinkedIn开展了一个新的团队，专注于使用数据和高级分析来回答关于LinkedIn用户体验和业务的各种问题。为了决定如何称呼这个团队，他们发布了三个不同的招聘广告，描述他们正在寻找的人才。这三个广告都是同一个职位和同一个工作描述，首选经验要求相同，发布在同一个招聘网站上。唯一的区别在于名称：“数据科学家”，“数据分析师”和“数据工程师”。吸引最多申请者的广告是“数据科学家”，因此他们将团队命名为“数据科学”。
- en: 'This is a good example of what’s commonly referred to as *A/B* or *split testing*:
    you try A and B on different groups of people and rigorously measure which one
    is better, before making a decision. Most data-driven companies, like LinkedIn
    and Google, do not allow any code to be released without instrumentation that
    allows them to measure its effectiveness. They also traditionally test new features
    in several different markets before rolling these out to the broader user base.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，通常被称为*A/B*或*分割测试*：你在不同的人群中尝试A和B，并严格测量哪个更好，然后再做出决定。像 LinkedIn 和 Google
    这样的数据驱动型公司，在发布任何代码之前，都要加入仪器，以便测量其效果。他们还会在几个不同的市场上测试新功能，然后再将其推广给更广泛的用户群体。
- en: At the core of data science is a combination of math (specifically statistics),
    computer science (especially data handling and machine learning), and domain or
    business knowledge. The domain knowledge is crucial for the data scientist to
    understand what problems need to be solved, what data is relevant, and how to
    interpret the results. Many books and articles have been written on the technical
    aspects of data science, but in this book we will focus on data science as it
    can be practiced by large enterprises. To introduce the central concepts, I’ll
    reprint here the following essay by Veijko Krunic, who consults with large enterprises
    about how to start practicing data science.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的核心是数学（特别是统计学）、计算机科学（尤其是数据处理和机器学习）以及领域或业务知识的结合。领域知识对于数据科学家来说至关重要，因为它帮助他们理解需要解决的问题、哪些数据是相关的，以及如何解释结果。有很多书籍和文章专注于数据科学的技术方面，但在本书中，我们将关注大企业实践的数据科学。为了介绍核心概念，我将重印一下
    Veijko Krunic 的下面这篇文章，他向大企业咨询如何开始实践数据科学。
- en: What Should Your Analytics Organization Focus On?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您的分析组织应该关注哪些问题？
- en: '![](Images/ebdl_03in01.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ebdl_03in01.png)'
- en: '***Veljko Krunic** is an independent consultant and trainer helping his clients
    to get the best business results from data science and big data. He has worked
    with organizations ranging from the Fortune 10 to early-stage startups, guiding
    them through the complete lifecycle of big data and analytical solutions, from
    early proof-of-concept efforts to the improvement of mission-critical systems.
    His previous employers include Hortonworks, the SpringSource division of VMware,
    and the JBoss division of Red Hat. He has a PhD in computer science and an MS
    in engineering management from the University of Colorado at Boulder, with a focus
    on strategic planning and applied statistics in quality sciences. He is also a
    Six Sigma Master Black Belt.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Veljko Krunic** 是一名独立顾问和培训师，帮助客户从数据科学和大数据中获得最佳业务结果。他曾与从财富前十到初创企业的组织合作，指导它们完成大数据和分析解决方案的整个生命周期，从早期概念验证到改进关键任务系统。他曾在Hortonworks、VMware的SpringSource部门以及Red
    Hat的JBoss部门工作过。他在科罗拉多大学博尔德分校获得了计算机科学博士学位和工程管理硕士学位，专注于战略规划和质量科学中的应用统计学。他还是一名Six
    Sigma Master Black Belt。'
- en: Many companies are making investments in big data and data science, and are
    rightfully expecting significant business benefits. At the same time, the resulting
    big data systems and data science methods may be among the most complex technologies
    to enter the enterprise market in recent memory. The size of the effort makes
    it hard to match the tools and techniques you adopt to the organization’s goals.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司正在对大数据和数据科学进行投资，并且理所当然地期望获得显著的业务利益。同时，由此产生的大数据系统和数据科学方法可能是近年来进入企业市场的最复杂技术之一。努力的规模使得很难将您采用的工具和技术与组织的目标相匹配。
- en: As an executive, you will be exposed to a huge variety of new technologies and
    concepts. You may have been bombarded with such terms as deep learning, HMMs,
    Bayesian networks, GLMs, SVMs, and more. On the big data infrastructure side,
    you may have heard terms like Spark, HDFS, MapReduce, HBase, Cassandra, Hadoop,
    Impala, Storm, Hive, Flink, and many others. The Hortonworks Hadoop distribution
    alone packages 26 Apache projects as of the end of 2018, and the Apache Software
    Foundation has over 300 active projects, many of which are data-related. There
    are also hundreds of commercial products fighting for a place in the ecosystem.
    It’s easy to get lost in all the noise. Even your technically trained staff can
    be overwhelmed. It’s rare to find a data scientist or architect who possesses
    excellent (or even strong practitioner-level) skills in *all* those areas. It
    is much more likely for knowledge about various parts of the system to be fragmented
    among different members of the team. It may even happen that this is the first
    time the team is working on a project similar to what you are doing now.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名高管，您将接触到大量新技术和概念。您可能已经被诸如深度学习、HMM、贝叶斯网络、GLM、SVM等术语轰炸过。在大数据基础设施方面，您可能听说过Spark、HDFS、MapReduce、HBase、Cassandra、Hadoop、Impala、Storm、Hive、Flink等术语，以及许多其他技术。截至2018年底，Hortonworks
    Hadoop发行版单独包含26个Apache项目，而Apache软件基金会有超过300个活跃项目，其中许多与数据相关。还有数百种商业产品在生态系统中争夺一席之地。在这些噪音中很容易迷失方向，即使是您技术训练有素的员工也可能感到不知所措。很难找到一位数据科学家或架构师在所有这些领域都具有优秀（甚至是强大的实践者级别）的技能。更可能的情况是系统的各个部分的知识分散在团队不同成员之间。甚至可能是团队第一次进行类似您现在正在做的项目。
- en: This essay is meant to keep you focused on the important questions you need
    to answer as an executive. How do you know that you are directing your project
    down the road to optimal business success, as opposed to simply following the
    direction determined by the preexisting knowledge your team happens to possess?
    When no single person has expertise in all the areas covered by a project, how
    do you know that you are investing in the areas that will give you the best payoff?
    How can you avoid playing the unfortunate “knowledge poker” game, in which various
    team members are assumed to hold cards that the project needs for success, but
    no one is actually sure what you collectively know as a team and whether the full
    scope of the project is covered?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章旨在让您集中精力回答作为一名高管需要解决的重要问题。您如何知道自己正在引导项目走向最佳的商业成功之路，而不仅仅是按照团队已有知识决定的方向前进？当没有任何一个人精通项目涵盖的所有领域时，您如何确保投资于将带来最佳回报的领域？您如何避免进行不幸的“知识扑克”游戏，其中假设各个团队成员持有项目成功所需的牌，但实际上没有人确切知道团队共同知道什么，以及项目的完整范围是否被覆盖？
- en: The previously mentioned technologies are important to many projects, to be
    sure. However, if these terms are the only ones that you are hearing when you
    are talking with your data science and big data teams, you should ask yourself
    whether the right focus is being put on the system as a whole, or whether your
    team is overfocusing on certain components of the system at the expense of the
    system as a whole. In particular, *is the focus on things you need as the executive,
    or on things your team knows (or hopes to learn)*? Thinking about the relationship
    of engineering to your business, with a focus on final results, is the discipline
    of *whole system engineering*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的技术对许多项目至关重要。然而，如果在与数据科学和大数据团队交流时，您听到的只有这些术语，您应该问自己是否正确关注了整个系统，或者您的团队是否过于专注于系统的某些组件而忽视了整体系统。特别是，焦点是在您作为高管需要的事务上，还是在您的团队了解（或希望学习）的事务上？考虑工程与您的业务之间的关系，并侧重于最终结果，这就是“整体系统工程”的学科。
- en: It is not your team’s fault if it is not focused on the system as a whole. It
    is fair to say that as of now, industry in general suffers from the same absence
    of focus on systems. Most presentations, meetups, and marketing materials focus
    on only a few of the technologies that are in turn only a *part* of the successful
    analytical stack. Much less time is devoted to system engineering. For that matter,
    how much do we even talk in our community about the simple but fundamental notion
    that big data systems are engineering *systems* that have to be fit for *business*
    purposes?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的团队没有专注于整体系统，这并不是他们的错。可以公正地说，目前整个行业普遍缺乏对系统的关注。大多数演示文稿、聚会和营销材料只关注成功分析堆栈的一小部分技术。在系统工程方面投入的时间远远不足。至于这一点，我们在社区中甚至有多少讨论大数据系统是为*业务*目的而设计的*系统*的简单但基本的概念？
- en: As an example of the current tendency to focus on various parts of the system,
    with the goal of making the right choice between alternative methods, a lot of
    ink is spilled discussing the relative strengths and weaknesses of individual
    machine learning methods. These are important tactical decisions, but your job
    is to avoid getting trapped in technical decisions before you know your strategy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为关注系统各部分的当前倾向的例子，目标是在备选方法之间做出正确选择，许多墨水被浪费在讨论单个机器学习方法的相对优势和劣势上。这些是重要的战术决策，但你的工作是在你了解你的战略之前避免被技术决策困住。
- en: 'Let’s take as an example the MNIST data set, which presents a classification
    problem: handwritten digits that the computer should classify as digits from 0
    to 9\. It is probably the most widely used data set in computer vision today,
    used to test computer vision algorithms developed in settings that range from
    classroom projects to major internet corporations. The period between 1998 and
    2016 saw improvement in the accuracy of classification from an error rate of 2.4%
    (achieved with a relatively simple *k*-nearest neighbors algorithm) to 0.21% (achieved
    with an ensemble of deep neural networks).^([1](ch03.xhtml#ch03fn1)) For 18 years,
    some of the brightest minds in machine learning worked hard to improve the error
    rate by 2.19%. That is a significant improvement, allowing, for instance, a computer
    scanner to read most of the addresses written on mail envelopes automatically,
    versus a human having to look at almost every envelope.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子来说，MNIST数据集呈现了一个分类问题：计算机应该将手写数字分类为0到9的数字。这可能是当今计算机视觉中使用最广泛的数据集，用于测试在从课堂项目到主要互联网公司的设置中开发的计算机视觉算法。从1998年到2016年，分类准确率从使用相对简单的*k*-最近邻算法达到的错误率2.4%改进到0.21%（使用深度神经网络集成）。^([1](ch03.xhtml#ch03fn1))
    18年来，一些机器学习领域最聪明的人努力将错误率提高了2.19%。这是一个显著的改进，例如，允许计算机扫描仪自动读取大多数信封上写的地址，而不是人类几乎需要查看每个信封。
- en: 'However, when you are running a business project, you are not interested in
    the behavior of a single classification algorithm. The underlying question you
    need to ask is: “Does that difference of *x*% significantly contribute to the
    success of the project?” Sometimes it will, sometimes it won’t. Sometimes you
    don’t need the best classification method known to humankind; you may benefit
    more from simply collecting additional data sources that might produce better
    overall results.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你在运行一个商业项目时，你不会对单一分类算法的行为感兴趣。你需要问的根本问题是：“这个*x*%的差异是否显著促进了项目的成功？”有时会，有时不会。有时你不需要人类已知的最佳分类方法；你可能更多受益于简单地收集可能产生更好总体结果的额外数据源。
- en: The reality of system engineering is that it is much more important to avoid
    big pitfalls from which you cannot recover than it is to make the best possible
    choice between two close competitors, whether they are methods or products. Certainly,
    if you don’t make the best decision about a particular part of the system, you
    might be giving a significant advantage to your competitors, and such an error
    might even kill you down the road. But before “down the road” can even become
    an issue, you must first have the ability to develop a viable product that could
    start moving down that road.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 系统工程的现实是，避免你无法恢复的大坑比在两个竞争对手（无论是方法还是产品）之间做出最佳选择更为重要得多。当然，如果你没有对系统的特定部分做出最佳决策，你可能会给你的竞争对手带来显著优势，这样的错误甚至可能在未来毁了你。但在“未来”成为问题之前，你必须首先有能力开发一个可行的产品，可以开始在那条道路上前行。
- en: 'And while big data and data science do bring important new elements to the
    table, they don’t significantly change the best practices of system engineering.
    As an executive, you are on a good course to developing a viable product if, after
    meeting with your data science team, you are able to clearly and concisely answer
    the following questions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大数据和数据科学确实为企业带来了重要的新元素，但它们并没有显著改变系统工程的最佳实践。作为一名高管，在与你的数据科学团队会面后，如果你能够清晰而简洁地回答以下问题，那么你正在朝着开发一个可行产品的良好方向前进：
- en: How are the data science concepts they are talking about related to your business?
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们所谈论的数据科学概念与你的业务有何关联？
- en: Is your organization prepared to act on the results of the analysis? (Business
    results don’t magically materialize just because you’ve completed an analysis—they
    materialize when you take *appropriate business actions* based on the results
    of the analysis. Both at the start and throughout the project, you need a clear
    understanding of the business actions available to your organization.)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的组织是否准备根据分析结果采取行动？（仅仅因为你完成了分析并不意味着商业结果会神奇地出现——只有在根据分析结果采取适当的商业行动时，商业结果才会出现。在项目开始和整个过程中，你需要清楚地了解你的组织可以采取哪些商业行动。）
- en: What part of your machine learning system should you invest in to get best “bang
    for the buck”?
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的机器学习系统的哪一部分应该投资，以获取最大的“回报率”？
- en: If the team needs to do more research to answer the previous questions, what
    exactly is that research, what is the range of possible answers they expect to
    get, and what are the things they need to complete that research (time to try
    methods, additional data, etc.)?
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果团队需要进行更多研究来回答之前的问题，那么这些研究究竟是什么，他们预期获得的可能答案范围是什么，以及完成这些研究所需的事项（尝试方法的时间、额外数据等）？
- en: Key to your success is distinguishing whether you are running a system engineering
    effort that is able to produce predictable results (or a spectrum of possible
    results), or a *research project* whose results might be great, but are not exactly
    predictable or marketable. While there is a place for both in business and industry,
    you certainly should not mistake the two types of project. And you should also
    guide your team so that the members focus on executing the right type of project.
    An inability to clearly answer the previous questions is a major red flag, and
    until you can “ace” them, better methods will not help you much.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的关键在于区分你是否正在进行能够产生可预测结果（或一系列可能结果）的系统工程努力，还是一项研究项目，其结果可能很好，但并非完全可预测或市场化。虽然在商业和行业中两者都有存在的空间，但你绝对不应该混淆这两种类型的项目。而且你还应该引导你的团队，使其专注于执行正确类型的项目。无法清晰回答前述问题是一个主要的警示信号，直到你能够“完美”回答这些问题之前，更好的方法并不会对你有太大帮助。
- en: The process of whole system engineering is beyond the scope of this book, but
    there is one critical component that you will get right if you apply the techniques
    advised here. That critical component is an appreciation of the need to understand
    your data, and that getting to know your data is a non-trivial challenge.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 整个系统工程的过程超出了本书的范围，但有一个关键组成部分，如果你应用了这里建议的技术，你将能够做到正确。那个关键组成部分是意识到需要理解你的数据，并且了解你的数据是一个非平凡的挑战。
- en: Some teams make the error of assuming that by the simple virtue of having recorded
    their data in some database or data lake, they are automatically experts on the
    data. Starting with that as a premise, it may appear logical to devote all attention
    to “other, more important issues.” The reality is that cataloging and interpreting
    modern enterprise data is a complex problem, and organizations must make substantial
    investments in data governance to carry out these tasks. Essentially, if you have
    the wrong data, improving classification by 2.19% will not necessarily help. If
    you’re working with the wrong data or have misinterpreted your data, you must
    change course as soon as possible. Luckily, you may be able to reuse the investments
    you’ve made in tools.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队犯了一个错误，即认为仅仅因为将数据记录在某个数据库或数据湖中，他们就自动成为了数据专家。从这个前提出发，似乎将所有注意力都集中在“其他更重要的问题”上是合理的。然而，现实是，编目和解释现代企业数据是一个复杂的问题，组织必须在数据治理上进行重大投资才能完成这些任务。实际上，如果你有错误的数据，将分类提高2.19%并不一定有帮助。如果你使用的是错误的数据或者误解了你的数据，你必须尽快改变方向。幸运的是，你可能能够重复利用你在工具上的投资。
- en: Finding and implementing the right approach to your data is a key aspect of
    system engineering, and the approach should be evaluated near the beginning of
    your project to make sure you are on the right track.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 找到并实施适合您数据的正确方法是系统工程的关键方面，应在项目开始阶段评估该方法，以确保您走在正确的道路上。
- en: Machine Learning
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: '*Machine learning* refers to the process of training a computer program to
    build a statistical model based on data. It is a very broad and deep topic, and
    we are not going to do it justice here; this section aims simply to give you a
    flavor of what machine learning is about.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习*指的是根据数据训练计算机程序构建统计模型的过程。这是一个非常广泛和深刻的主题，我们在这里无法充分涵盖；本节仅旨在简要介绍机器学习的基本概念。'
- en: Machine learning can be supervised or unsupervised. *Supervised* machine learning
    involves feeding training data to create a model. For example, if we want to predict
    prices of homes in a specific area, we could feed historical sales data to the
    model and create a formula that should accurately predict values for other similar
    homes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以是有监督的或者是无监督的。*有监督*机器学习涉及将训练数据输入以创建模型。例如，如果我们想预测特定区域房屋的价格，我们可以将历史销售数据输入模型，并创建一个能够准确预测其他类似房屋价值的公式。
- en: Various machine learning algorithms have been around for years and are well
    understood and trusted. While there are thousands of algorithms and they can always
    be improved, the most common algorithms, such as linear regression, are available
    even in common tools such as Microsoft Excel. The difficult part of machine learning
    is usually not the model, but the data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已有各种机器学习算法，这些算法被深入理解和信任。虽然有成千上万的算法并且它们总是可以改进，但最常见的算法，如线性回归，甚至在常见工具如Microsoft
    Excel中也是可以找到的。机器学习的难点通常不在于模型，而在于数据。
- en: Without the right data, the model is going to be *unstable*. A model is said
    to be unstable if it seems to work well on test data but does not accurately predict
    results on real data out in the field. A common technique is to break historical
    data into two random data sets, train the model on one (the training data set),
    and then apply the model to the other (the test data set) to see whether it accurately
    predicts the results. This may catch some of the problems, but if the entire data
    set is biased, the model will still be unstable when used against real-world data.
    Furthermore, the conditions that the model was trained on may change and the model
    may no longer apply. This is called “model drift.” For instance, in the housing
    example a new road or a new business might dramatically affect the prices, and
    we would need to retrain the model to incorporate this new information.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 没有正确的数据，模型会变得*不稳定*。如果模型在测试数据上表现良好，但在实际场景中不能准确预测结果，就称其为不稳定。一个常见的技术是将历史数据随机分成两组，一组用于训练模型（训练数据集），另一组用于应用模型（测试数据集），以查看其是否能准确预测结果。这可能会捕捉到一些问题，但如果整个数据集有偏差，当模型应用于真实世界数据时仍然会是不稳定的。此外，模型训练时的条件可能会发生变化，模型可能不再适用。这称为“模型漂移”。例如，在房屋价格预测中，新的道路或新的商业可能会显著影响价格，因此我们需要重新训练模型以融入这些新信息。
- en: In general, the key to creating good models is having the right features—the
    inputs to the model that determine the outcome. Imagine if, in our house pricing
    example, the quality of schools was not considered. Two identical houses on the
    same street but on opposite sides of the school district boundary might have significantly
    different prices if one school is dramatically better than the other. No matter
    how much data we get to train our model, if this variable is missing, we will
    not be able to accurately predict the prices. And even if we have the right features,
    we have to have representative data. For example, if all our data is from districts
    with similar-performing schools, the model will be trained to ignore school scores
    because they will not produce any variations in home prices. To accurately train
    the model, we would need data from a representative mix of high- and low-performing
    school districts. Feature engineering is one of the most critical tasks every
    data scientist has to perform.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，创建良好模型的关键在于拥有正确的特征——决定结果的模型输入。想象一下，在我们的房屋定价示例中，如果没有考虑学校质量，同一条街上两个相同的房子但在学区边界的对立面可能会有显著不同的价格，如果一个学校比另一个显著好。无论我们有多少数据来训练我们的模型，如果缺少这个变量，我们将无法准确预测房价。即使我们拥有正确的特征，我们也必须拥有具有代表性的数据。例如，如果我们所有的数据都来自表现类似的学区，模型将被训练忽略学校成绩，因为它们不会在房价中产生任何变化。为了准确训练模型，我们需要来自高、低表现学区的代表性混合数据。特征工程是每个数据科学家必须执行的最关键任务之一。
- en: Not only is it important to have the right data, but it should be of high quality.
    Unlike in regular analytics, where data problems often jump out as results that
    do not make sense, it can be very difficult to spot bad data in machine learning
    unless it makes the models unstable. For instance, in our case if the school district
    information was corrupted or wrong, we might be able to build a stable model as
    long as it was corrupted consistently for both the training and the testing data
    sets, but the model would likely be useless on real live data where this information
    is not corrupted.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅拥有正确的数据很重要，而且数据质量也应该很高。与常规分析不同，常规分析中数据问题通常会导致不合理的结果，但在机器学习中，除非数据使模型不稳定，否则很难发现糟糕的数据。例如，在我们的情况中，如果学区信息损坏或错误，只要训练数据集和测试数据集都一致损坏，我们可能能够建立一个稳定的模型，但是在真实数据中，这些信息没有损坏，模型可能就毫无用处了。
- en: '*Unsupervised* learning refers to machine learning that is not trained. For
    example, customer segmentation is often done using unsupervised machine learning.
    The program is given a set of customers and huge amounts of different demographic
    information, and then breaks the data into buckets of “similar” customers. Just
    as with supervised learning, if segmentation is performed on bad data, it may
    produce unreliable results and it may be very difficult to figure out that, say,
    the 7 segments you produce of 100,000 customers each are somehow wrong. Because
    models tend to be so complex that they are impenetrable to human understanding,
    *explainability* has become a major topic in machine learning.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*非监督学习*是指未经过训练的机器学习。例如，客户分群通常使用非监督机器学习完成。程序会提供一组客户和大量不同的人口统计信息，然后将数据分成“相似”的客户组。与监督学习一样，如果在糟糕的数据上进行分群，可能会产生不可靠的结果，而且很难发现例如你产生了10万名客户中的7个分群都有问题。由于模型往往非常复杂，以至于人类难以理解，*可解释性*已成为机器学习中的一个重要主题。'
- en: Explainability
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: I had an interesting discussion once with a data scientist who told me how he
    was hired to do customer segmentation. When he presented the results of his model
    to his client, the VP of marketing picked two customer records out of the same
    segment and asked why those two were grouped together. The data scientist couldn’t
    explain the results beyond going over how he had trained the model, and the VP
    announced he wasn’t going to invest hundreds of thousands of dollars into a campaign
    based on this segmentation if no one could explain it to him.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经我与一位数据科学家进行过一次有趣的讨论，他告诉我他被聘请进行客户分群。当他向客户展示了模型的结果时，市场副总裁从同一个分组中挑选出两个客户记录，并问为什么这两个客户被归为一组。数据科学家无法解释结果，只能重述他训练模型的过程，然后副总裁宣布如果没有人能向他解释，他不会投资数十万美元用于基于此分群的市场活动。
- en: Explainability is not just a matter of curiosity or an aid to debugging. It
    is a fundamental question of trust. Users of analytics are often asked to prove
    that the models they use are not making inappropriate or illegal decisions. Discrimination
    is one of those tricky areas. Imagine a town where one ethnic population has on
    average about a quarter of the income of another ethnic population. Should the
    model be allowed to consider ethnicity as a variable? Most people (and anti-discrimination
    laws) would say no. But what if we make decisions based on income? People with
    more income might get higher credit limits at local banks or stores; this seems
    to be fair and common sense. However, what if we don’t know people’s true income
    levels when they apply for credit? Most store credit applications do not require
    a copy of an income tax return or paycheck. In addition, some people may be part
    of the gig economy and not earn formal income even if their household income is
    quite high, while other people might not have a credit history or score to use
    for verification purposes. What to do?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性不仅仅是好奇心或调试的帮助。这是信任的一个根本问题。分析用户经常被要求证明他们使用的模型不会做出不适当或非法的决定。歧视是其中一个棘手的领域。想象一下一个镇，其中一个民族平均收入大约是另一个民族的四分之一。模型是否应被允许将种族作为一个变量来考虑？大多数人（以及反歧视法律）可能会说不。但是，如果我们基于收入做出决定呢？收入更高的人可能会在当地银行或商店获得更高的信用额度；这似乎是公平和常识的体现。然而，如果我们不知道申请信用时人们的真实收入水平怎么办？大多数商店信用申请不要求提供所得税单或工资单的复印件。此外，一些人可能参与零工经济，即使他们的家庭收入非常高，也没有正式收入，而其他人可能没有信用历史或分数可用于验证目的。该怎么办？
- en: A clever data scientist might be tempted to infer ethnicity from the first and
    last names and cross-reference average income for that ethnicity in that town
    against the income reported by the applicant. If the inferred income and reported
    income are far apart, the application process would require additional credit
    checks. Is that legal? Are we now discriminating by ethnicity? How would a credit
    officer even know why “the computer” is requiring additional credit checks? What
    if the data scientist was not consciously checking ethnicity, but the algorithm
    uncovered correlations between names and income level blindly?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个聪明的数据科学家可能会被诱惑从申请人的姓和名来推断种族，并将该镇该种族的平均收入与申请人报告的收入进行交叉参考。如果推断的收入和报告的收入相差甚远，申请过程将需要额外的信用检查。这是否合法？我们现在是在通过种族歧视吗？信贷官员又如何知道“计算机”为何要求额外的信用检查？如果数据科学家并非有意检查种族，而是算法盲目地发现了姓名与收入水平之间的相关性呢？
- en: Such problems require explainability. Which variables caused the decision? What
    were the values of those variables? This is a difficult, but promising, area of
    machine learning. A lot of work is being done in academia as well as at machine
    learning companies such as FICO.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题需要解释性。是哪些变量导致了这个决定？这些变量的值是多少？这是一个困难但有前景的机器学习领域。学术界以及像FICO这样的机器学习公司正在进行大量工作。
- en: Change Management
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变更管理
- en: Since the real world rarely stays the same, models that represented it at one
    point in time may well lose their predictive abilities. This is known as *model
    drift*. And since the outputs of the models are predictive, it is difficult to
    tell how good the predictions are and, therefore, whether the model is still relevant
    until the actual events happen. Thus, constant monitoring is critical to keep
    the model performing accurately. If drift is detected, the model needs to be retrained
    on new data. Even the data may drift. For example, IoT sensors in harsh conditions
    such as oil fields might go bad. Some of them may start spewing incorrect data,
    which must not be allowed to affect the outcome of the model. This data drift
    must be checked for outliers, so incorrect data does not corrupt the model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界很少保持不变，一度代表其的模型可能会失去其预测能力。这被称为*模型漂移*。由于模型的输出是预测性的，因此很难判断预测有多准确，因此模型是否仍然相关直到实际事件发生。因此，持续监控是保持模型准确运行的关键。如果检测到漂移，模型需要重新基于新数据进行训练。甚至数据本身也可能会漂移。例如，在像油田这样的恶劣条件下的物联网传感器可能会出问题。其中一些可能会开始喷出错误数据，这些数据不能影响模型的结果。必须检查这些数据漂移中的异常值，以确保错误数据不会破坏模型。
- en: Unfortunately, rerunning the existing model on the new data may not produce
    a stable model because the model won’t reflect any new variables that may be introduced
    into the mix to affect the outcomes. For example, a model that once accurately
    predicted house sale prices in a neighborhood may not consider the fact that a
    new highway is being constructed through the neighborhood, and that proximity
    to this highway significantly affects the prices. Until this new variable (proximity
    to the new highway) is added to the model, the model may not be able to predict
    prices accurately. In short, one must regularly construct new models or retrain
    existing ones using the most appropriate current machine learning algorithms.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，将现有模型重新运行在新数据上可能不会产生稳定的模型，因为模型不会反映可能影响结果的任何新变量。例如，曾经在一个社区准确预测房价的模型可能并未考虑到新的高速公路正在该社区建设中，而且这个高速公路的接近会显著影响房价。在这种情况下，除非将这个新变量（新高速公路的接近度）添加到模型中，否则模型可能无法准确预测房价。简言之，人们必须定期建立新模型或使用最合适的当前机器学习算法重新训练现有模型。
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Advanced analytics, machine learning, predictive analytics, recommendation engines—the
    list is long and there are still many challenges to overcome, but this technology
    is very promising. It is already changing our lives, with applications ranging
    from self-driving cars, to much improved voice recognition and visual recognition
    technology, to looking for disease signals in genetic codes, reading IoT signals
    to provide predictive maintenance, or predicting what our houses are worth. All
    of this runs on data—and what better place to get that data than an enterprise
    data lake?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 高级分析、机器学习、预测分析、推荐引擎——这些技术应用广泛，但还有许多挑战需要克服，这些技术非常有前景。它已经改变了我们的生活，应用从自动驾驶汽车，到大大改进的语音识别和视觉识别技术，再到寻找基因密码中的疾病信号，读取物联网信号以提供预测性维护，或者预测我们的房屋价值。所有这些都依赖于数据——而获取数据的更好地方又何处比企业数据湖更好呢？
- en: ^([1](ch03.xhtml#ch03fn1-marker)) See [*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/)
    and [*http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html*](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#ch03fn1-marker)) 请参阅[*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/)和[*http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html*](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。
