- en: 5 Probabilistic deep learning models with TensorFlow Probability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 TensorFlow Probability中的概率深度学习模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Introduction to probabilistic deep learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率深度学习模型简介
- en: The negative log-likelihood on new data as a proper performance measure
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据上的负对数似然作为适当的性能指标
- en: Fitting probabilistic deep learning models for continuous and count data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为连续和计数数据拟合概率深度学习模型
- en: Creating custom probability distributions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义概率分布
- en: '![](../Images/5-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-unnumb.png)'
- en: 'In chapters 3 and 4, you encountered a kind of uncertainty that’s inherent
    to the data. For example, in chapter 3, you saw in the blood pressure example
    that two women with the same age can have quite different blood pressures. Even
    the blood pressure of the same woman can be different when measured at two different
    times within the same week. To capture this data-inherent variability, we used
    a conditional probability distribution (CPD): *P*(*y*|*x*). With this distribution,
    you captured the outcome variability of *y* by a model. To refer to this inherent
    variability in the DL community, the term aleatoric uncertainty is used. The term
    aleatoric stems from the Latin word alea, which means dice, as in Alea iacta est
    (“the die is cast”).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章和第4章中，你遇到了一种固有的数据不确定性。例如，在第3章中，你在血压的例子中看到，年龄相同的两位女性可以有相当不同的血压。即使是同一位女性，在同一个星期内不同时间测量的血压也可能不同。为了捕捉这种数据固有的变异性，我们使用了条件概率分布（CPD）：P(y|x)。通过这个分布，你通过模型捕捉了y的输出变异性。在深度学习社区中，为了指代这种固有的变异性，使用了术语“随机不确定性”。这个术语“随机”来源于拉丁语单词alea，意为骰子，正如“Alea
    iacta est”（骰子已掷出）。
- en: In this chapter, we focus further on developing and evaluating probabilistic
    models to quantify the aleatoric uncertainty. Why should you care about uncertainties?
    It’s not only about theoretical baublery; it has real practical importance for
    making critical or costly decisions based on the predictions. Think, for example,
    about a situation where a New York ta*x**[i]* driver transports an art dealer
    to a great art auction that starts in 25 minutes. The art dealer promises a generous
    tip ($500) if she arrives there in time. That matters to the ta*x**[i]* driver!
    He, fortunately, has the newest gadgets, and his travel-time prediction tool is
    based on a probabilistic model yielding probabilistic proposals for the travel-time
    prediction.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进一步关注开发和评估概率模型以量化随机不确定性。为什么你应该关注不确定性？这不仅仅关乎理论上的繁琐；它对于基于预测做出关键或昂贵的决策具有实际的重要性。想想看，比如一个纽约出租车司机在25分钟内将艺术品经销商送到一个即将开始的盛大艺术品拍卖会的情况。如果她准时到达，艺术品经销商承诺给予丰厚的小费（500美元）。这对出租车司机来说很重要！幸运的是，他拥有最新的设备，他的旅行时间预测工具基于一个概率模型，该模型为旅行时间预测提供概率建议。
- en: The tool proposes two routes leading to the auction. Route 1 has a predicted
    average travel time of *μ*[1] = 19 min but a high uncertainty (standard deviation
    of *σ*[1] = 12 min), and route 2 has an average travel time of *μ*[2] = 22 min
    but a small uncertainty (standard deviation of *σ*[2] = 2 min). The taxi driver
    decides to take the second route, which is a really smart decision. The probability
    of getting the tip is about 93%[1](#pgfId-1081613) when taking route 2 even if
    the mean travel time of route 2 is substantially longer than that of route 1\.
    The chance of getting the tip when choosing the first route with 19 minutes mean
    travel time is about 69%.[2](#pgfId-1081648) But such information can only be
    derived from a probabilistic model that predicts not a single value, but a reliable
    probability distribution over all possible travel times.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工具提出了两条通往拍卖会的路线。路线1预测的平均旅行时间为μ[1] = 19分钟，但不确定性很高（标准差σ[1] = 12分钟），而路线2的平均旅行时间为μ[2]
    = 22分钟，但不确定性很小（标准差σ[2] = 2分钟）。即使路线2的平均旅行时间比路线1长得多，选择路线2获得小费的概率约为93%[1](#pgfId-1081613)。选择平均旅行时间为19分钟的路线1获得小费的概率约为69%[2](#pgfId-1081648)。但这样的信息只能从一个预测所有可能旅行时间的可靠概率分布的概率模型中得出。
- en: 'From chapter 4, you know how to fit a probabilistic model. You use a neural
    network (NN) to determine the parameters of the predicted CPD for the outcome.
    In principle, developing a probabilistic DL model is easy:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从第4章，你知道如何拟合概率模型。你使用神经网络（NN）来确定预测CPD（条件概率分布）的参数。原则上，开发概率深度学习模型是容易的：
- en: You pick an appropriate distribution model for the outcome.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择一个合适的分布模型来预测结果。
- en: You set up an NN that has as many output nodes as the model has parameters.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你设置一个NN，其输出节点数量与模型参数数量相同。
- en: You derive from the picked distribution the negative-log-likelihood (NLL) function
    and use that function as the loss function to train the model.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你从选定的分布中推导出负对数似然（NLL）函数，并使用该函数作为损失函数来训练模型。
- en: Note To get our terminology straight, when we talk about output, we mean a node
    in the last layer of the NN; when we talk about an outcome, we mean the target
    variable *y*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了使我们的术语一致，当我们谈论输出时，我们指的是NN最后一层的节点；当我们谈论结果时，我们指的是目标变量 *y*。
- en: Chapter 4 focused on the maximum likelihood (MaxLike) approach for fitting a
    model. This approach leads to the NLL as the loss function. You can determine
    the NLL for a chosen probability distribution manually. We did this in chapter
    4 in section 4.2 for classification problems and in section 4.3 for standard regression
    problems. But as you saw in chapter 4, we quickly needed some calculus and programming
    to derive the NLL as the loss function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章重点介绍了用于拟合模型的极大似然（MaxLike）方法。这种方法导致NLL作为损失函数。你可以手动确定所选概率分布的NLL。我们在第4章的第4.2节中针对分类问题，在第4.3节中针对标准回归问题进行了这种操作。但正如你在第4章中看到的，我们很快就需要一些微积分和编程来推导出NLL作为损失函数。
- en: 'In this chapter, you’ll get to know TensorFlow Probability (TFP), an extension
    of TensorFlow that makes it easy to fit a probabilistic DL model without requiring
    you to manually define the corresponding loss function. You’ll see how to use
    TFP for different applications, and you’ll gain an intuitive understanding of
    what’s going on behind the scenes. Fitting probabilistic models allows you to
    easily incorporate your field knowledge: you simply pick an appropriate outcome
    distribution (in figure 5.1, it’s sketched as a distribution plate in the DL machine)
    and so model the randomness of real-world data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解TensorFlow Probability（TFP），这是TensorFlow的一个扩展，它使得在不要求你手动定义相应的损失函数的情况下轻松拟合概率深度学习模型变得容易。你将看到如何使用TFP进行不同的应用，并且你将获得对幕后发生的事情的直观理解。拟合概率模型允许你轻松地结合你的领域知识：你只需选择一个合适的结果分布（在图5.1中，它被描绘为深度学习机器中的分布板）并因此模拟现实世界数据的随机性。
- en: In this chapter, you’ll also develop highly performant probabilistic DL models
    for different tasks. Probabilistic models can be optimized in two ways. First,
    we choose an appropriate architecture, the network shelf in figure 5.1, for example.
    (We covered this aspect in chapter 2.) Second, and this is the focus of this chapter,
    we enhance the model by choosing the right distribution for the outcome. But how
    does one determine the performance of a probabilistic model in the first place?
    You’ll see that the criterion for selecting the best performing probabilistic
    model is easy! The model with the lowest NLL on new data is the best performing
    probabilistic prediction model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你还将为不同的任务开发高性能的概率深度学习模型。概率模型可以通过两种方式优化。首先，我们选择一个合适的架构，例如图5.1中的网络架。 （我们在第2章中讨论了这一点。）其次，这也是本章的重点，我们通过选择合适的结果分布来增强模型。但一个人最初如何确定概率模型的性能呢？你会发现选择最佳性能的概率模型的准则非常简单！在新数据上具有最低NLL的模型是最佳性能的概率预测模型。
- en: 5.1 Evaluating and comparing different probabilistic prediction models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 评估和比较不同的概率预测模型
- en: The goal of a probabilistic prediction model is to yield accurate probabilistic
    predictions on new data. This means that the predicted CPD at a given *x* should
    match the observed distribution as well as possible. The correct measure is simple.
    You already saw it in chapter 4; it’s the NLL that’s minimized on the training
    data. But now, it’s evaluated on new data (test data) not used in the training.
    The lower the NLL on this test data, the better the model performance is expected
    for new data in general. It can even be proved mathematically that the test NLL
    is optimal when assessing the prediction performance of a model.[3](#pgfId-1071960)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 概率预测模型的目标是在新数据上产生准确的概率预测。这意味着在给定的 *x* 处预测的CPD应该尽可能匹配观察到的分布。正确的衡量标准很简单。你已经在第4章中看到了它；它是训练数据上最小化的NLL。但现在，它是在未用于训练的新数据（测试数据）上评估的。在测试数据上的NLL越低，模型对新数据的预期性能越好。甚至可以证明，在评估模型的预测性能时，测试NLL是最优的。[3](#pgfId-1071960)
- en: '![](../Images/5-1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-1.png)'
- en: Figure 5.1 Principle idea of probabilistic modeling in deep learning (*DL*).
    The network determines the parameters of a probability distribution. We fit the
    model using the mighty MaxLike principle. In the figure, the outcome is modeled
    by a Normal distribution, where the NN is used to control one parameter (see the
    chosen last plate with one output node), usually the mean value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 深度学习（*DL*）中概率建模的基本思想。网络确定概率分布的参数。我们使用强大的最大似然原理来拟合模型。在图中，结果由正态分布建模，其中神经网络用于控制一个参数（参见选定的最后一个带有单个输出节点的板），通常是平均值。
- en: 'In the process of model development, you usually tune your models. In that
    process, you repeatedly fit several models to your train data and evaluate the
    performance on the validation data. In the end, you select the model with the
    highest prediction performance on the validation data. But when you always use
    the same validation data to check the performance of the model, it’s possible
    that you overfit on the validation data. It’s, therefore, good practice in DL,
    as well as in machine learning (ML), to work with three data sets:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发的过程中，你通常会调整你的模型。在这个过程中，你反复将几个模型拟合到你的训练数据上，并在验证数据上评估其性能。最后，你选择在验证数据上预测性能最高的模型。但是，当你总是使用相同的验证数据来检查模型的性能时，你可能会在验证数据上过度拟合。因此，在深度学习（DL）以及机器学习（ML）中，使用三个数据集是一个好的实践：
- en: A training data set to fit the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于拟合模型的训练数据集
- en: A validation data set to check the prediction performance of the model
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于检查模型预测性能的验证数据集
- en: A test data set that’s not touched at any point in the model selection procedure
    and that’s only used to evaluate the prediction performance of the final model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在模型选择过程中任何一点都没有被触及的测试数据集，并且只用于评估最终模型的预测性能
- en: Sometimes, it’s not possible to get all three data sets. In statistics, it’s
    quite common to work only with train and test data, where the test data takes
    the role of the validation and test data. Another approach commonly used in ML
    and statistics is cross-validation. In that technique, you split the training
    repeatedly into two parts and use one for training and the other for validation.
    Because this cross-validation procedure requires us to repeat a time-costly training
    process several times, deep learners usually don’t apply this technique.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，无法获得所有三个数据集。在统计学中，只使用训练数据和测试数据是很常见的，其中测试数据扮演了验证和测试数据的角色。在机器学习和统计学中，常用的另一种方法是交叉验证。在该技术中，你反复将训练数据分成两部分，一部分用于训练，另一部分用于验证。因为这种交叉验证过程需要我们多次重复耗时较大的训练过程，所以深度学习者通常不采用这种技术。
- en: Warning In statistics, sometimes we only use a single data set. To still be
    able to evaluate the performance of the developed prediction model on the same
    data, sophisticated methods have developed over a long period of time and are
    still in use in some parts of the statistics community. These methods account
    for the fact that the model saw the data during fitting and applied corrections
    to account for that. These methods include, for example, the Akaike Information
    Criterion(AIC) or the Bayesian Information Criterion(BIC). Don’t get confused.
    If you have a validation set, you don’t need these methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 在统计学中，有时我们只使用一个数据集。为了仍然能够评估在相同数据上开发的预测模型的表现，经过长时间的发展，一些复杂的方法仍然在一些统计学术界中使用。这些方法考虑了模型在拟合过程中看到了数据这一事实，并应用了校正来考虑这一点。这些方法包括，例如，赤池信息准则(AIC)或贝叶斯信息准则(BIC)。不要混淆。如果你有一个验证集，你不需要这些方法。
- en: 5.2 Introducing TensorFlow Probability (TFP)
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 介绍TensorFlow Probability (TFP)
- en: In this section, you’ll learn about a convenient way to fit probabilistic models
    to your data. For this process, we introduce TFP that’s built on top of TensorFlow
    and tailored for probabilistic DL models. Using TFP allows you to think in terms
    of distribution models for your outcome. It frees you from handcrafting appropriate
    loss functions for your NN output. TFP provides special layers where you can plug
    in a distribution. It lets you compute the likelihood of the observed data without
    setting up any formulas or functions. In the last chapter, you set up probabilistic
    models without TFP, which is possible but sometimes cumbersome. Remember the procedure
    in chapter 4?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解一种方便的方法来将概率模型拟合到你的数据上。为此，我们引入了建立在TensorFlow之上并针对概率深度学习模型定制的TFP。使用TFP允许你以结果分布模型的方式思考。它让你免于为NN输出手工制作合适的损失函数。TFP提供了特殊的层，你可以插入一个分布。它让你能够在不设置任何公式或函数的情况下计算观测数据的似然性。在上一个章节中，你设置了没有TFP的概率模型，这是可能的，但有时比较繁琐。记得第4章中的程序吗？
- en: You pick an appropriate distribution for the outcome.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你为结果选择一个合适的分布。
- en: You set up an NN with as many outputs as you have parameters in your selected
    outcome distribution.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你设置一个具有与所选结果分布中参数数量相同的输出的神经网络（NN）。
- en: You define a loss function that yields the NLL.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你定义一个损失函数，该函数产生NLL。
- en: 'In the case of linear regression, you pick a Gaussian distribution as a model
    for the CPD(see figure 5.2 that shows the blood pressure example). In this case,
    your CPD is *N*(*μ**[x]*, *σ**[x]*), which has only two parameters: the mean (*μ**[x]*)
    and the standard deviation (σx). For a Gaussian distribution, you can define a
    95% prediction interval that gives you a range that covers 95% of the occurring
    outcomes. The borders of the 95% prediction interval *q*^(2.5%) , *q*^(97.5%)
    , are usually the 0.025 and 0.975 quantiles. These quantities are also called
    the 2.5% and 97.5% percentile. The 0.975 quantile (*q*^(97.5%)) is that value
    in the outcome distribution for which 97.5% of all occurring outcomes are smaller
    or equal to this value. In a Normal distribution, *N*(*μ**[x]*, *σ**[x]*), the
    97.5% quantile is given by #'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归的情况下，你选择高斯分布作为CPD（条件概率分布）的模型（参见图5.2，展示了血压的例子）。在这种情况下，你的CPD是 *N*(*μ**[x]*,
    *σ**[x]*)，它只有两个参数：均值 (*μ**[x]*) 和标准差 (σx)。对于高斯分布，你可以定义一个95%的预测区间，这个区间覆盖了95%的观测结果。95%预测区间的边界
    *q*^(2.5%) , *q*^(97.5%) ，通常是0.025和0.975分位数。这些量也被称为2.5%和97.5%的分位数。0.975分位数 (*q*^(97.5%))
    是结果分布中这样一个值，即97.5%的所有观测结果都小于或等于这个值。在正态分布 *N*(*μ**[x]*, *σ**[x]*) 中，97.5%的分位数由以下公式给出：
- en: '*q*^(97.5%) = *μ**[x]* + 1.96 ⋅ *σ**[x]* ≈ *μ**[x]* + 2 ⋅ *σ**[x]* .'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*q*^(97.5%) = *μ**[x]* + 1.96 ⋅ *σ**[x]* ≈ *μ**[x]* + 2 ⋅ *σ**[x]* .'
- en: '![](../Images/5-2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-2.png)'
- en: Figure 5.2 Scatter plot and regression model for the blood pressure example.
    The bell-shaped curves are the conditional probability distributions(CPDs) of
    the outcome SBP, which is conditioned on the observed value *x*(age). The length
    of the solid horizontal bar indicates the likelihood of an observed SBP of 131
    for a 22-year-old woman.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2展示了血压例子的散点图和回归模型。钟形曲线是结果SBP（收缩压）的条件概率分布（CPD），它是基于观测值 *x*（年龄）的。实线水平条的长度表示22岁女性观测到SBP为131的似然性。
- en: 'If you assume a constant standard deviation, then the derivation of the loss
    gets quite easy. That’s because when minimizing the NLL, you can ignore in the
    likelihood all parts that depend on the standard deviation. After some derivation,
    it turns out that minimizing the mean NLL is the same as minimizing the mean squared
    error (MSE) (see the sidebar titled “MaxLike-based derivation of the MSE loss
    in linear regression” in chapter 4):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你假设标准差是常数，那么损失函数的推导就变得相当简单。这是因为当最小化负对数似然（NLL）时，你可以忽略所有依赖于标准差的部分。经过一些推导后，发现最小化平均NLL与最小化平均均方误差（MSE）是相同的（参见第4章侧边栏“基于MaxLike的线性回归中MSE损失的推导”）：
- en: '![](../Images/equation_5-4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/equation_5-4.png)'
- en: where *x* is the age in our example. After the slope and intercept is optimized,
    you can derive the constant standard deviation from the residuals. You need to
    know this standard deviation to get a probabilistic model with known CPD *P*(*Y*|*X*
    = *x*) = *N*(*y* ; *μ**[x]* , *σ*), allowing you to determine the likelihood of
    an observed outcome.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是我们例子中的年龄。在斜率和截距优化后，你可以从残差中推导出常数标准差。你需要知道这个标准差来得到一个具有已知CPD *P*(*Y*|*X*
    = *x*) = *N*(*y* ; *μ**[x]* , *σ*) 的概率模型，这允许你确定观测结果的似然性。
- en: But if you want to allow the standard deviation to depend on *x*, the derivation
    of the loss gets more complicated. The terms in the likelihood that depend on
    the variance can no longer be ignored when minimizing the NLL. The resulting loss
    isn’t the MSE anymore but a more complicated expression (for a derivation, see
    the sidebar titled “MaxLike-based derivation of the MSE loss in linear regression”
    in chapter 4):[4](#pgfId-1072012)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想让标准差依赖于*x*，损失函数的推导会变得更加复杂。在最小化NLL时，依赖于方差的似然中的项不能再被忽略。结果损失不再是MSE，而是一个更复杂的表达式（推导过程见第4章侧边栏“基于MaxLike的线性回归中MSE损失的推导”）：[4](#pgfId-1072012)
- en: '![](../Images/equation_5-6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_5-6.png)'
- en: This isn’t a standard loss function in Keras; therefore, you needed to define
    a custom loss function that then compiles into the model. In chapter 4, you saw
    how to derive the NLL and how to work with customized loss functions. Doing this
    manually gives you a good understanding and full control over the whole fitting
    procedure. But it also is sometimes error-prone and not really convenient. Therefore,
    you’ll now learn about TFP, which can facilitate your work considerably.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Keras中不是一个标准的损失函数；因此，你需要定义一个自定义损失函数，然后将其编译到模型中。在第4章中，你看到了如何推导NLL以及如何使用自定义损失函数。手动做这件事可以让你对整个拟合过程有一个很好的理解和完全的控制。但这也可能是容易出错的，并且并不真正方便。因此，你现在将了解TFP，它可以大大促进你的工作。
- en: TFP allows you to concentrate on the model part. When thinking in probabilistic
    models, you try to find a prediction model that predicts for new data CPDs that
    assign high likelihoods to the observed outcomes. To measure the performance of
    a probabilistic model, you, therefore, use the joint likelihood of the observed
    data. Accordingly, you use the negative log-likelihood as a measure for the “badness”
    of probabilistic models, which is the reason why you use the NLL as the loss function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TFP允许你专注于模型部分。在概率模型中思考时，你试图找到一个预测模型，该模型对新数据预测CPD，并赋予观察到的结果高似然。因此，为了衡量概率模型的性能，你使用观察数据的联合似然。因此，你使用负对数似然作为概率模型“坏度”的度量，这也是为什么你使用NLL作为损失函数的原因。
- en: See figure 5.2 for an example where the CPD is a Normal *N*(*μ**[x]*, *σ*) distribution
    with a constant variance *σ*. The parameter is modeled by *μ**[x]* *= a · x +
    b* , the linear regression in its standard form.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅图5.2，其中CPD是一个具有常数方差σ的正态*N*(*μ**[x]*, *σ*)分布的例子。参数由*μ**[x]* *= a · x + b*
    模型，这是其标准形式的线性回归。
- en: You’ll see later in this chapter that for other tasks like modeling discrete
    count data with low average counts, the Normal distribution isn’t the best choice.
    You might want to pick another distribution family for the CPD; for example, a
    Poisson distribution. Within the TFP framework, this isn’t a big deal. You can
    simply change the distribution. You’ll see how to do this in several examples
    in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本章后面看到，对于其他任务，如使用低平均计数建模离散计数数据，正态分布并不是最佳选择。你可能想为CPD选择另一个分布族；例如，泊松分布。在TFP框架内，这并不是什么大问题。你可以简单地更改分布。你将在本章的几个示例中看到如何做到这一点。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/zjNw](http://mng.bz/zjNw)
    . Step through the notebook while reading. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/zjNw](http://mng.bz/zjNw)
    。在阅读时逐步查看笔记本。|'
- en: Because probability distributions are the main tool for capturing uncertainties
    in probabilistic models, let’s check out how to work with a TFP distribution.
    TFP provides a quickly growing collection of distributions. The Normal distribution
    is the distribution family that you’re probably most familiar with, so let’s start
    with defining a Normal distribution from TFP (see listing 5.1). We’ll sample from
    it and then determine the likelihood of the sampled values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概率分布是捕捉概率模型中不确定性的主要工具，让我们来看看如何使用TFP分布。TFP提供了一系列快速增长的分布。正态分布是你可能最熟悉的分布族，所以让我们从定义TFP中的正态分布开始（参见列表5.1）。我们将从中采样，然后确定采样值的似然。
- en: Listing 5.1 Working with a TFP Normal distribution
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 使用TFP正态分布
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Creates a 1D Normal distribution with a mean of 3 and a standard deviation
    of 1.5
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个均值为3，标准差为1.5的一维正态分布
- en: ❷ Samples two realizations from the Normal distribution
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从正态分布中采样两个实现
- en: ❸ Computes the likelihood for each of the two sampled values in the defined
    Normal distribution
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算定义的正态分布中每个采样值的似然
- en: You can use TFP distributions for different purposes. See table 5.1 and the
    notebook for some important methods that you can apply to TFP distributions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用TFP分布用于不同的目的。请参阅表5.1和笔记本，了解您可以应用于TFP分布的一些重要方法。
- en: Table 5.1 Important methods for TFP distributionsa
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 TFP分布的重要方法a
- en: '| Methods for TensorFlow distributions | Description | Numerical result when
    calling the method on`dist = tfd.Normal(loc=1.0, scale=0.1)` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow分布的方法 | 描述 | 在`dist = tfd.Normal(loc=1.0, scale=0.1)`上调用方法时的数值结果
    |'
- en: '| `sample(n)` | Samples n numbers from the distribution | `dist.sample(3).numpy()``array([1.0985107,
    1.0344477, 0.9714464], dtype = float32)`Note that these are random numbers. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `sample(n)` | 从分布中采样n个数字 | `dist.sample(3).numpy()``array([1.0985107, 1.0344477,
    0.9714464], dtype = float32)`注意，这些是随机数。 |'
- en: '| `prob(value)` | Returns the likelihood (probability density in the case of
    models for continuous outcomes) or probability (in the case of models for discrete
    outcomes) for the values (tensors) | `dist.prob((0,1,2)).numpy()``array([7.694609e-22,
    3.989423e+00, 7.694609e-22], dtype = float32)` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `prob(value)` | 返回值的似然（对于连续结果的模型的情况是对数概率密度）或概率（对于离散结果的模型的情况） | `dist.prob((0,1,2)).numpy()``array([7.694609e-22,
    3.989423e+00, 7.694609e-22], dtype = float32)` |'
- en: '| `log_prob(value)` | Returns the log-likelihood or log probability for the
    values (tensors) | `dist.log_prob((0,1,2)).numpy()``array([-48.616352, 1.3836466,
    -48.616352], dtype = float32)` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `log_prob(value)` | 返回值的对数似然或对数概率 | `dist.log_prob((0,1,2)).numpy()``array([-48.616352,
    1.3836466, -48.616352], dtype = float32)` |'
- en: '| `cdf(value)` | Returns the cumulative distribution function (CDF) that this
    is a sum or the integral of, up to the given values (tensors) | `dist.cdf((0,1,2)).numpy()``array([7.619854e-24,
    5.000000e-01, 1.000000e+00], dtype = float32)` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `cdf(value)` | 返回累积分布函数（CDF），这是给定值（张量）的总和或积分 | `dist.cdf((0,1,2)).numpy()``array([7.619854e-24,
    5.000000e-01, 1.000000e+00], dtype = float32)` |'
- en: '| `mean()` | Returns the mean of the distribution | `dist.mean().numpy()`1.0
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `mean()` | 返回分布的均值 | `dist.mean().numpy()`1.0 |'
- en: '| `stddev()` | Returns the standard deviation of the distribution | `dist.stddev().numpy()`0.1
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `stddev()` | 返回分布的标准差 | `dist.stddev().numpy()`0.1 |'
- en: '| a For more methods, see [http://mng.bz/048p](http://mng.bz/048p) . |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| a 更多方法，请参阅 [http://mng.bz/048p](http://mng.bz/048p) 。 |'
- en: 5.3 Modeling continuous data with TFP
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 使用TFP建模连续数据
- en: In this section, you’ll use TFP for a linear prediction model. You’ll adapt
    the linear model to optimally fit data that shows a quite complicated variability.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用TFP进行线性预测模型。您将调整线性模型以最佳地拟合显示相当复杂变异性的数据。
- en: 'When doing a first model selection experiment, it’s best if you start with
    some simulated data that gives you full control over the structure of the data.
    You can get train, validation, and test data by randomly splitting the simulated
    data into three parts, then lock the test data. Visualizing the synthetic training
    and validation data shows that it looks a little bit like a fish (see figure 5.3).
    The test data shouldn’t be touched; you shouldn’t even have a look at it! Figure
    5.3 doesn’t show the test. If you get this data and manually try to draw a smooth
    curve through the data, you’d probably end up with a straight line. But there
    are more things to notice: the data variability isn’t a constant but changes over
    the *x* range. In this and the following chapter, you’ll learn how you can set
    up probabilistic models that account for these properties.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行第一次模型选择实验时，最好从一些可以完全控制数据结构的模拟数据开始。您可以通过将模拟数据随机分成三部分来获取训练、验证和测试数据，然后锁定测试数据。可视化合成训练和验证数据表明它看起来有点像鱼（见图5.3）。测试数据不应被触及；您甚至不应查看它！图5.3没有显示测试数据。如果您得到这些数据并手动尝试在数据上绘制平滑曲线，您可能会得到一条直线。但还有更多需要注意的事情：数据的可变性不是一个常数，而是在*x*范围内变化的。在本章和下一章中，您将学习如何设置考虑这些特性的概率模型。
- en: '![](../Images/5-3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-3.png)'
- en: Figure 5.3 Visualization of the train (left) and test (right) data of the synthetic
    data set
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3合成数据集的训练（左）和测试（右）数据的可视化
- en: 5.3.1 Fitting and evaluating a linear regression model with constant variance
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 使用恒定方差拟合和评估线性回归模型
- en: Let’s assume you want to develop a probabilistic prediction model for the data
    shown in figure 5.3\. After inspecting the data, the first idea is to go for a
    linear model. Let’s use the TFP framework to set up a probabilistic linear regression
    model (see listing 5.2). For this purpose, you pick a Normal distribution *N*(*μ**[x]*,
    *σ*[2]) as the CPD.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要为图5.3所示的数据开发一个概率预测模型。在检查数据后，第一个想法是采用线性模型。让我们使用TFP框架来设置一个概率线性回归模型（见列表5.2）。为此，你选择一个正态分布
    *N*(*μ**[x]*, *σ*[2]) 作为CPD。
- en: Let’s assume the standard linear regression setting where only the parameter
    *μ* *x* depends on the input *x* and the standard deviation is a constant. How
    do you handle a constant standard deviation in TFP? You saw in chapter 4 (see
    the sidebar titled “MaxLike-based derivation of the MSE loss in linear regression”)
    that the value of the constant variance doesn’t impact the estimation of the linear
    model. Therefore, you can freely pick any value for the constant variance; for
    example, 1 when using `tfd.Normal``()`(see listing 5.2). Accordingly, your NN
    needs only to estimate one output for the parameter *μ* *x*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们考虑标准的线性回归设置，其中只有参数 *μ* *x* 依赖于输入 *x*，并且标准差是一个常数。在TFP中如何处理常数标准差？你在第4章中看到（见标题为“基于MaxLike的线性回归MSE损失的推导”的侧边栏）恒定方差值不会影响线性模型的估计。因此，你可以自由选择任何恒定方差值；例如，当使用
    `tfd.Normal``()` 时选择1（见列表5.2）。相应地，你的神经网络只需要估计参数 *μ* *x* 的一个输出。
- en: 'TFP lets you mix Keras layers with TFP distributions. You can connect the output
    of the NN with a distribution via the `tfp.layers.DistributionLambda` layer. This
    layer takes as input two things: a distribution and the values for its parameters.
    Technically speaking, a distribution like `tf.distributions.Normal` is a realization
    of `tf.distributions.Distribution` . You can, therefore, call the methods shown
    in table 5.1\. The distribution parameters (such as loc (*μ*) and scale (*σ*)
    for normal) are given by the previous layer.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TFP允许你混合Keras层与TFP分布。你可以通过 `tfp.layers.DistributionLambda` 层将神经网络的输出与一个分布连接起来。这个层接受两个东西：一个分布及其参数的值。从技术上讲，像
    `tf.distributions.Normal` 这样的分布是 `tf.distributions.Distribution` 的一个实现。因此，你可以调用表5.1中显示的方法。分布参数（如正态分布的loc
    (*μ*) 和 scale (*σ*)）由前一层的值给出。
- en: 'In listing 5.2, you see how to use a `tfp.layers.DistributionLambda` layer
    to construct the CPD *P*(*y*|*x*, *w*). Here we choose a Normal distribution *N*(*μ**[x]*,
    *σ*[2]) with a location parameter *μ* *x*, depending on *x* and a fixed-scale
    parameter *σ**[x]* = 1\. The corresponding NLL is directly given by the likelihood
    that the CPD has assigned to the observed outcome y: −log(*P*(*y*|*x*, *w*)).
    Because the result `distr` of `tfp.layers.DistributionLambda` is of type `tfd.Distribution`
    , this translates to -distr.log_prob(*y*) (see the second line in table 5.1).
    Thus, TFP frees you from deriving and programming the appropriate loss function
    for your model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.2中，你看到如何使用 `tfp.layers.DistributionLambda` 层来构建CPD *P*(*y*|*x*, *w*)。在这里，我们选择一个具有位置参数
    *μ* *x* 的正态分布 *N*(*μ**[x]*, *σ*[2])，该参数依赖于 *x*，以及一个固定尺度参数 *σ**[x]* = 1。相应的NLL直接由CPD分配给观察到的结果y的概率给出：-log(*P*(*y*|*x*,
    *w*)）。因为 `tfp.layers.DistributionLambda` 的结果 `distr` 是 `tfd.Distribution` 类型，这转化为
    -distr.log_prob(*y*)（见表5.1的第二行）。因此，TFP让你免于推导和编程适合你模型的适当损失函数。
- en: Listing 5.2 Using TFP for linear regression with a constant variance
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.2 使用TFP进行具有常数方差的线性回归
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Computes the NLL of an observed *y* under the fitted distribution distr
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在拟合分布distr下计算观察到的 *y* 的NLL
- en: ❷ Uses the output of the last layer (params) as the parameter(s) of a distribution
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用最后一层的输出（params）作为分布的参数
- en: ❸ Sets up the NN with one output node
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置具有一个输出节点的神经网络
- en: ❹ Calls a distributional layer to take the function my_dist with the argument
    params
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 调用一个分布层以使用参数params调用函数my_dist
- en: ❺ Connects the output of the NN with a distribution
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将神经网络的输出与一个分布连接起来
- en: ❻ Compiles the model with NLL as a loss function
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用NLL作为损失函数编译模型
- en: With the TFP code in listing 5.2, you’ve fitted a linear regression model. But
    is it a probabilistic model? A probabilistic model needs to provide a whole CPD
    for the output for each input *x*. In the case of a Gaussian CPD, this not only
    requires an estimated mean *μ* *x* but also a standard deviation *σ*. In the standard
    linear regression case, a constant variance is chosen independently of the *x*
    position. In this case, we can estimate *σ*[2] by the variance of the residuals.
    This means you first need to fit the linear model before you can determine the
    variance that is used for all CPDs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.2中的TFP代码中，你已经拟合了一个线性回归模型。但它是概率模型吗？概率模型需要为每个输入*x*提供整个CPD。在高斯CPD的情况下，这不仅需要估计均值*μ*
    *x*，还需要标准差*σ*。在标准的线性回归情况下，方差是独立于*x*位置选择的常数。在这种情况下，我们可以通过残差的方差来估计*σ*[2]。这意味着你首先需要拟合线性模型，然后才能确定用于所有CPD的方差。
- en: Now you’re ready to use the trained model to do some probabilistic predictions
    on your validation data. For each test point, you’ll predict a Gaussian CPD. To
    visualize how the model performs on the validation data, you can draw the validation
    data along with the predicted mean of the CPD, *μ* *x*(see the solid line in figure
    5.4), and the mean plus/minus two times the standard deviation of the CPD, *μ**[x[i]]*
    *± 2* ⋅ *σ**[x]* corresponding to the 0.025 and 0.975 quantiles (see the dashed
    lines in figure 5.4).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好使用训练好的模型对你的验证数据进行一些概率预测。对于每个测试点，你将预测一个高斯CPD。为了可视化模型在验证数据上的表现，你可以绘制验证数据以及CPD预测的均值，*μ*
    *x*（见图5.4中的实线），以及CPD均值加减两倍标准差的位置，*μ**[x[i]]* *± 2* ⋅ *σ**[x]*，对应于0.025和0.975分位数（见图5.4中的虚线）。
- en: '![](../Images/5-4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![5-4.png](../Images/5-4.png)'
- en: Figure 5.4 The synthetic validation data along with the predicted probabilistic
    model for linear regression, which is the same as an NN without a hidden layer.
    The mean μx of the CPD is modeled by the NN, and the standard deviation is assumed
    to be a constant. The black solid line indicates the positions of μxi, and the
    dashed lines show the positions of the 0.025 and 0.975 quantiles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 线性回归的合成验证数据以及预测的概率模型，这与没有隐藏层的NN相同。CPD的均值μx由NN建模，标准差假设为常数。黑色实线表示μxi的位置，虚线表示0.025和0.975分位数的位置。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/zjNw](http://mng.bz/zjNw)
    . Step through the code to solve exercise 1 and follow the code while reading.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/zjNw](http://mng.bz/zjNw)
    。逐步执行代码以解决练习1，并在阅读时跟随代码。'
- en: Use TFP to fit a linear model with a constant standard deviation.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TFP来拟合具有常数标准差的线性模型。
- en: Which constant standard deviation yields the lowest validation NLL?
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个常数标准差会产生最低的验证NLL？
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/5-5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![5-5.png](../Images/5-5.png)'
- en: Figure 5.5 Validation data along with the predicted Gaussian CPDs. The model
    used is the linear model with constant data variability (see listing 5.2). The
    black solid line indicates the position of the mean, and the dashed lines are
    the positions of the 0.025 and 0.975 quantiles. For the four picked data points
    (filled dots), the predicted CPD is shown. The corresponding likelihoods are indicated
    by the solid horizontal line segments connecting the dots with the curves.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 验证数据以及预测的高斯CPDs。使用的模型是具有常数数据变异性的线性模型（见列表5.2）。黑色实线表示均值的定位，虚线表示0.025和0.975分位数的定位。对于四个选定的数据点（实心点），显示了预测的CPD。相应的似然性由连接点与曲线的实线段表示。
- en: When checking your fit visually (see figure 5.4), you might be satisfied with
    the positions of the mean (black solid line). But the varying spread of the outcome
    isn’t captured at all. Does it matter? Yes, for sure. To better see this, let’s
    check out how well the CPD matches the data distribution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当你视觉上检查拟合（见图5.4）时，你可能会对均值的定位（黑色实线）感到满意。但结果分布的变化范围并没有被捕捉到。这重要吗？当然重要。为了更好地看到这一点，让我们检查CPD与数据分布的匹配程度。
- en: 'A matching CPD can assign high likelihoods to the observed outcome. For visualization
    purposes, let’s do this for four randomly chosen test points: (-0.81, -6.14),
    (1.03, 3.42), (4.59, 6.68), and (5.75, 17.06) (see the filled circles in figure
    5.5). The *x* positions of the selected points are indicated by the four dotted
    vertical lines in figure 5.5\. A trained NN predicts the CPD from the *x* values.
    You get four Gaussian distributions with four different means but always the same
    standard deviation. Each CPD gives the probability distribution for all possible
    outcomes *y* for a given input of the test point *x**[test]* : *P*(*y*|*μ**[x[test]]*
    , *σ*) . The likelihood that the CPD assigns to the actually observed *y* test
    is indicated by the horizontal line connecting the filled dots with the curved
    lines: *P*(*y**[test]*|*μ**[x[test]]* , *σ*).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个匹配的CPD可以为观察到的结果分配高似然。为了可视化，让我们为四个随机选择的测试点这样做：(-0.81, -6.14), (1.03, 3.42),
    (4.59, 6.68), 和 (5.75, 17.06)（见图5.5中的填充圆圈）。图5.5中选定点*x*的位置由四条虚线垂直线指示。训练好的NN从*x*值预测CPD。你得到四个具有不同均值但相同标准差的高斯分布。每个CPD为给定测试点*x**[test]*的输入的所有可能结果*y*提供概率分布：*P*(*y*|*μ**[x[test]]*
    , *σ*)。CPD分配给实际观察到的*y**[test]*的似然由连接填充点与曲线的横线表示：*P*(*y**[test]*|*μ**[x[test]]*
    , *σ*)。
- en: On first viewing figure 5.5, the model seems to assign reasonable likelihoods
    to the true outcomes. To quantify this, you can determine for each test point
    the assigned likelihood of the true observation and summarize this by the mean
    NLL over all points in the validation set, which is in this case:[5](#pgfId-1072215)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次查看图5.5时，模型似乎为真实结果分配了合理的似然。为了量化这一点，你可以确定每个测试点分配给真实观察的似然，并通过验证集中所有点的平均NLL来总结，在这种情况下：[5](#pgfId-1072215)
- en: NLL(constant sigma model) = 3.53
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: NLL(恒定标准差模型) = 3.53
- en: How could you get a model that predicts better Gaussian CPDs? Remember, the
    better the model, the lower the NLL for the validation set. For a given standard
    deviation, a point gets the optimal likelihood if it’s in the center of the Gaussian
    CPD. OK, the four picked points (see figure 5.5) aren’t really in the center of
    their CPDs. If that were the case, these would lie on the bold line, but they
    are quite close. A second tuning parameter for the Gaussian is the standard deviation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何得到一个能更好地预测高斯CPD的模型？记住，模型越好，验证集的NLL（负对数似然）就越低。对于给定的标准差，一个点如果位于高斯CPD的中心，就会得到最优似然。好的，四个选定的点（见图5.5）实际上并不位于它们CPD的中心。如果那样的话，它们会位于粗体线上，但它们相当接近。高斯的一个第二个调整参数是标准差。
- en: 5.3.2  Fitting and evaluating a linear regression model with a nonconstant standard
    deviation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 使用非恒定标准差的线性回归模型进行拟合和评估
- en: Let’s try to get a better prediction model for the synthetic data that takes
    into account the varying spread of the data (see figure 5.3). How can you adapt
    your model to allow for a Gaussian CPD with a nonconstant standard deviation?
    No big deal! Just allow the model to learn the data variability too! You still
    assume a Normal distribution as CPD, but this time you want to learn both parameters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试为考虑数据变化分布的合成数据获得更好的预测模型（见图5.3）。你如何调整你的模型以允许具有非恒定标准差的高斯CPD？没问题！只需允许模型学习数据变化！你仍然假设正态分布作为CPD，但这次你想要学习两个参数。
- en: The output of the NN can, in principle, take any value from minus infinity to
    plus infinity. One way to ensure that the standard deviation is always positive
    is to feed the output of the NN through an exponential function. We have done
    this before. A popular alternative is to use the `softplus` function. In figure
    5.6, you see the `softplus` function beside the exponential (`exp`) function.
    In contrast to the exponential function, the `softplus` function increases linearly
    for large *x* values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: NN的输出原则上可以取从负无穷大到正无穷大的任何值。确保标准差始终为正的一种方法是将NN的输出通过指数函数。我们之前已经这样做过了。一种流行的替代方法是使用`softplus`函数。在图5.6中，你可以看到`softplus`函数在指数(`exp`)函数旁边。与指数函数不同，`softplus`函数在大的*x*值上线性增加。
- en: '![](../Images/5-6.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-6.png)'
- en: Figure 5.6 The `softplus` function compared with the exponential (`exp`) function.
    Both functions map arbitrary values to positive values.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 `softplus`函数与指数(`exp`)函数的比较。这两个函数将任意值映射到正值。
- en: This only requires a small change in our TFP code. You now have two output nodes
    (see line 5 in listing 5.3), and you tell the distributional layer the location
    and scale parameters derived by the NN.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这只需要对我们TFP代码进行微小改动。现在你有两个输出节点（见列表5.3中的第5行），并且你告诉分布层由NN推导出的位置和尺度参数。
- en: Listing 5.3 A shallow NN for linear regression with nonconstant variance
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 用于线性回归的非常数方差浅层NN
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Computes the NLL of the model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算模型的NLL
- en: ❷ The first output node defines the mean (loc).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个输出节点定义了均值（loc）。
- en: ❸ The second output defines the standard deviation (scale) via the softplus
    function. To ensure a non-negative scale, a small constant is added.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第二个输出通过软加函数定义了标准差（尺度）。为了确保尺度非负，添加了一个小的常数。
- en: ❹ Sets up the NN with two output nodes
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置具有两个输出节点的NN
- en: Let’s display the fitted model along with the data (see figure 5.7). But this
    isn’t what you hoped for! True, the variance isn’t modeled to be a constant across
    the whole *x* range. But the dashed lines, indicating *μ**[x[i]]* *± 2* ⋅ *σ**[x]*
    , only roughly follow the underlying data variability.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示拟合的模型以及数据（见图5.7）。但这并不是你期望的结果！确实，方差在整个*x*范围内并没有被建模为常数。但虚线，表示*μ**[x[i]]*
    *± 2* ⋅ *σ**[x]*，只是大致遵循潜在的数据变异性。
- en: '![](../Images/5-7.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-7.png)'
- en: Figure 5.7 The synthetic validation data along with the predicted probabilistic
    model where the mean and the standard deviation of the CPD is modeled by an NN
    without a hidden layer. The black solid line indicates the position of the mean,
    and the dashed lines designate the positions of the 0.025 and 0.975 quantiles.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 合成验证数据以及预测的概率模型，其中CPD的均值和标准差由没有隐藏层的NN建模。黑色实线表示均值的位臵，虚线表示0.025和0.975分位数的位置。
- en: 'Can you guess what happened? Let’s look at the code in listing 5.3 and think
    about the used NN architecture, which has no hidden layer. The two nodes in the
    output layer depend linearly on the input: out[1] = *a* ⋅ *x* + *b* and out[2]
    = *c* ⋅ *x* + *b* . You can see in listing 5.3 that the first output gives the
    value of the `loc` argument, and the second output is put through a `softplus`
    function to determine the `scale` argument. Thus, the standard deviation only
    depends monotonically on the input, meaning it’s impossible for the fitted standard
    deviation to follow the non-monotonic variance structure of the data. You can
    quantify the prediction performance by the mean validation NLL (see the notebook
    titled “Result: Monotonic sigma”):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜到发生了什么吗？让我们看看列表5.3中的代码，并思考所使用的没有隐藏层的NN架构。输出层的两个节点线性依赖于输入：out[1] = *a* ⋅ *x*
    + *b* 和 out[2] = *c* ⋅ *x* + *b*。在列表5.3中可以看到，第一个输出给出了`loc`参数的值，第二个输出通过`softplus`函数来确定`scale`参数。因此，标准差仅单调地依赖于输入，这意味着拟合的标准差不可能遵循数据的非单调方差结构。你可以通过平均验证NLL（见标题为“结果：单调方差”的笔记本）来量化预测性能：
- en: NLL(monotonic sigma) = 3.55
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: NLL(单调方差) = 3.55
- en: This is comparable with the value 3.53 we got using a constant variance. You
    would expect a better, or at least an equal, performance for the “monotonic sigma”
    model because this model is more flexible and is also able to learn a constant
    variance (the “constant sigma” model). The small performance decrease, therefore,
    indicates a little overfitting, meaning the training set shows a slightly increasing
    variance by chance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这与使用常数方差得到的价值3.53相当。你可能会期望“单调方差”模型有更好的表现，或者至少是相等的性能，因为这个模型更灵活，并且也能够学习到常数方差（即“常数方差”模型）。因此，性能的小幅下降表明略微过拟合，意味着训练集偶然显示出略微增加的方差。
- en: 'How can you improve the model? You need to allow the standard deviation to
    depend on *x* in a more flexible manner. You saw in the last chapters the simple
    recipe of DL to achieve flexibility: stack more layers. If you just introduce
    a hidden layer between the input and the two output nodes, you’d also allow the
    mean to depend in a non-linear manner on *x*. You don’t really want to do this
    because then you wouldn’t get a linear regression model anymore. But you’re free
    to choose an architecture that directly connects the input with the first output
    controlling the mean (out1) and to put one or several hidden layer(s) between
    the input and the second output controlling the standard deviation (out2) (see
    figure 5.8).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何改进模型？你需要允许标准差以更灵活的方式依赖于 *x*。在上一个章节中，你看到了深度学习实现灵活性的简单配方：堆叠更多层。如果你只是在输入和两个输出节点之间引入一个隐藏层，你也会允许均值以非线性的方式依赖于
    *x*。你实际上并不想这样做，因为那样你就不会得到线性回归模型了。但你可以选择一个直接将输入与控制均值的第一个输出（out1）连接的架构，并在输入和第二个输出（控制标准差）之间放置一个或多个隐藏层（见图
    5.8）。
- en: '![](../Images/5-8.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-8.png)'
- en: Figure 5.8 An NN architecture (left) for the two parameters μx, *σ*x of a Gaussian
    CPD. The first output (out1) determines the mean μx of the CPD that linearly depends
    on *x*. The second output (out2) controls the standard deviation *σ*x of the CPD
    that depends on *x* in a flexible manner. In the notebook for listing 5.4, there
    are three hidden layers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 一个神经网络架构（左）用于高斯 CPD 的两个参数 μx, *σ*x。第一个输出（out1）确定 CPD 的均值 μx，该均值线性依赖于 *x*。第二个输出（out2）控制
    CPD 的标准差 *σ*x，该标准差以灵活的方式依赖于 *x*。在列表 5.4 的笔记本中，有三个隐藏层。
- en: 'To code the architecture in figure 5.8, you can use TFP and Keras. The functional
    API of Keras gives you greater flexibility in coding complex architectures where
    different output nodes can be computed in a very different manner from the input.
    But this requires that you save the result of each layer in a tensor variable
    and define (for each layer) which tensor it gets as input (see listing 5.4). The
    architecture shown in figure 5.8 translates to the code shown in listing 5.4,
    where you choose three hidden layers: one with 30 and two with 20 nodes between
    the input and the second output.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要在图 5.8 中编码架构，你可以使用 TFP 和 Keras。Keras 的功能性 API 在编码复杂架构时提供了更大的灵活性，其中不同的输出节点可以以非常不同的方式从输入中计算。但这要求你将每个层的输出保存到张量变量中，并定义（对于每个层）它接收哪个张量作为输入（参见列表
    5.4）。图 5.8 中所示的架构对应于列表 5.4 中所示的代码，其中你选择了三个隐藏层：一个有 30 个节点，两个有 20 个节点，位于输入和第二个输出之间。
- en: Listing 5.4 Using an NN with a hidden layer for linear regression with nonconstant
    variance
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 使用具有隐藏层的神经网络进行具有非恒定方差的线性回归
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The first output models the mean; no hidden layers are used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个输出模型化均值；没有使用隐藏层。
- en: ❷ The second output models the spread of the distribution; three hidden layers
    are used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个输出模型化分布的扩散；使用了三个隐藏层。
- en: ❸ Combining the outputs for the mean and the spread
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 结合均值和扩散的输出
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/zjNw](http://mng.bz/zjNw)
    . Step through the code to solve exercises 2 and 3, and follow the code while
    reading.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/zjNw](http://mng.bz/zjNw)
    。逐步执行代码以解决练习 2 和 3，并在阅读时跟随代码。'
- en: Fit linear regression models with nonconstant standard deviation.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合具有非恒定标准差的线性回归模型。
- en: How do you pick the best model?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何选择最佳模型？
- en: How does the predicted CPD look outside the range of the training data?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测 CPD 在训练数据范围之外看起来如何？
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The resulting probabilistic fit is displayed in figure 5.9\. Now everything
    looks very nice! On the average, you have a linear relationship between input
    and outcome, but the variability of the outcome is different for different inputs.
    This is reflected by the distance of the two dashed lines in figure 5.9, between
    which ~95% of all data should fall, which this model seems to fulfill. Also, the
    predicted CPDs *P*(*y*|*μ**[x[test]]*, *σ*) and the resulting likelihoods *P*(*y**[test]*|*μ**[x[test]]*,
    *σ*) for the four test points (-0.81, -6.14), (1.03, 3.42), (4.59, 6.68), (5.75,
    17.06) really look good now! Just enjoy for a moment how much an appropriate standard
    deviation of the Gaussian CPD can improve the likelihood with which the probabilistic
    model expects the true outcome (compare figures 5.5, 5.7, and 5.9).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的概率拟合显示在图5.9中。现在一切看起来都非常完美！平均而言，输入和结果之间存在线性关系，但不同输入的结果变异性不同。这通过图5.9中两条虚线的距离反映出来，其中大约95%的所有数据应该落在其中，而这个模型似乎做到了。此外，对于四个测试点（-0.81,
    -6.14）、（1.03, 3.42）、（4.59, 6.68）、（5.75, 17.06）预测的CPD *P*(*y*|*μ**[x[test]]*, *σ*)
    和由此产生的似然 *P*(*y**[test]*|*μ**[x[test]]*, *σ*) 看起来现在非常好！就此刻享受一下，适当的高斯CPD标准差如何能提高概率模型预测真实结果的似然（比较图5.5、5.7和5.9）。
- en: '![](../Images/5-9.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-9.png)'
- en: Figure 5.9 Validation data along with the predicted Gaussian CPDs from the linear
    model that allows for flexible data variability. The black solid line indicates
    the position of the mean, and the dashed lines represent the positions of the
    0.025 and 0.975 quantiles. For four picked data points (the filled dots), the
    CPD is shown. The corresponding likelihoods are indicated by the solid horizontal
    line segments. An animated version comparing this approach with figure 5.5 can
    be found at [http://mng.bz/K2JP](http://mng.bz/K2JP) .
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9展示了验证数据以及允许灵活数据变异性的线性模型的预测高斯CPD。黑色实线表示均值的位置，虚线表示0.025和0.975分位数的位置。对于四个选定的数据点（实心点），显示了CPD。相应的似然由实线水平线段表示。与图5.5比较的动画版本可以在[http://mng.bz/K2JP](http://mng.bz/K2JP)找到。
- en: 'To quantify the prediction performance, you can again determine the mean validation
    NLL(see the topic “Result: Flexible sigma” in the notebook):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化预测性能，你可以再次确定验证数据的平均负对数似然（参见笔记本中的“结果：灵活标准差”主题）：
- en: Validation mean NLL(flexible sigma) = 3.11
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 验证平均负对数似然（灵活标准差）= 3.11
- en: According to the mean NLL on the validation data, you’d pick the last model
    with a flexible standard deviation of the Gaussian CPD as the winner. Let’s do
    a clean finish and make sure that you avoid the overfitting pitfall when reporting
    the mean test NLL. Use your winning model to determine again the mean test NLL,
    which is
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据验证数据的平均负对数似然，你会选择具有灵活高斯CPD标准差的最后一个模型作为获胜者。让我们干净利落地完成这项工作，并确保你在报告平均测试负对数似然时避免过拟合陷阱。使用你的获胜模型再次确定平均测试负对数似然，它是
- en: Test mean NLL(flexible sigma) = 3.15
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 测试平均负对数似然（灵活标准差）= 3.15
- en: Let’s look back and recap what you learned. If you want to develop a probabilistic
    prediction model, your goal is to predict the correct CPD on new data. To evaluate
    if the predicted CPD indeed describes the distribution of the outcome in new data,
    you computed the mean NLL on the validation data. But what about the well-established
    MSE or the mean absolute error(MAE)? These two performance measures quantify how
    much the predicted mean deviates (on average) from the actual observed value.
    But these completely ignore the variance of the CPD, meaning they don’t evaluate
    the correctness of the full CPD. For this reason, it’s not appropriate to report
    the MSE or MAE alone if you want to assess the quality of a probabilistic model.
    We recommend that you always report the performance of a probabilistic regression
    model by the validation NLL and give only additional measures like validation
    MSE and MAE (or accuracy in case classification).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾并总结一下你学到了什么。如果你想开发一个概率预测模型，你的目标是预测新数据上的正确CPD。为了评估预测的CPD是否确实描述了新数据中结果的分布，你计算了验证数据的平均负对数似然。但MSE或平均绝对误差（MAE）这些已建立的性能指标如何？这两个性能指标量化了预测均值（平均）偏离实际观察值的程度。但它们完全忽略了CPD的方差，这意味着它们没有评估整个CPD的正确性。因此，如果你想评估概率模型的质量，单独报告MSE或MAE是不合适的。我们建议你始终通过验证负对数似然来报告概率回归模型的性能，并仅提供额外的指标，如验证MSE和MAE（或分类情况下的准确率）。
- en: 5.4 Modeling count data with TensorFlow Probability
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 使用 TensorFlow Probability 模型建模计数数据
- en: 'When setting up a probabilistic model, the most challenging task is to pick
    the right distribution model for the outcome. The data type of the outcome plays
    an important role in this game. In the blood pressure example, the outcome was
    continuous. Given a certain age of a woman, you modeled the possible blood pressure
    values by a continuous Normal distribution, *N*(μ , *σ* ), which has two parameters:
    the mean, *μ* , and standard deviation, *σ*. In the MNIST example, the outcome
    was categorical, and your task was to predict from a raw image the probability
    that the image shows the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9 (see chapter 2).
    The predicted probabilities for the 10 possible digits make up the categorical
    outcome distribution, which is called multinomial distribution and has 10 parameters
    p0, p1, . . . , p9 that add up to 1.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置概率模型时，最具挑战性的任务是选择正确的分布模型来表示结果。结果的数据类型在这个游戏中起着重要作用。在血压的例子中，结果是连续的。给定一个女性的特定年龄，你通过连续正态分布
    *N*(μ , *σ* ) 来建模可能的血压值，该分布有两个参数：均值 *μ* 和标准差 *σ*。在 MNIST 例子中，结果是分类的，你的任务是预测从原始图像中显示数字
    0、1、2、3、4、5、6、7、8 或 9 的概率（见第 2 章）。对 10 个可能的数字的预测概率构成了分类结果分布，这被称为多项式分布，具有 10 个参数
    p0, p1, ..., p9，它们的总和为 1。
- en: In this section, you’ll learn how to model outcomes that are counts. There are
    many use cases where you want to model count data; for example, counting the number
    of people *y**[i]* that are on a given image *x**[i]* , or the number of comments
    that a blog post receives in the upcoming 24 hours, or the number of deer killed
    in a road accident during a certain hour based on some features *x**[i]*(like
    the time of day, date, and so on). You’ll later have the opportunity to do a case
    study predicting the number of roadkills. We recommend that you step through the
    following notebook while working through this section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何建模计数结果。有许多用例需要建模计数数据；例如，计算给定图像 *x**[i]* 上的 *y**[i]* 人数，或者预测一篇博客文章在未来
    24 小时内收到的评论数量，或者根据某些特征 *x**[i]*（如一天中的时间、日期等）在某个小时内发生的交通事故中杀死的鹿的数量。你将有机会进行一项预测交通事故数量的案例研究。我们建议你在学习本节内容时逐步浏览以下笔记本。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/90xx](http://mng.bz/90xx)
    . Step through the notebook and try to understand the code. What’s the mean NLL
    on the test data absolute error and the RMSE for linear regression and for Poisson
    regression? |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/90xx](http://mng.bz/90xx)
    。逐步浏览笔记本，并尝试理解代码。测试数据上的平均 NLL 绝对误差和线性回归以及泊松回归的 RMSE 是多少？ |'
- en: 'Let’s start with a classical example in count data analysis, taken from [https://stats.idre
    .ucla.edu/r/dae/zip/](https://stats.idre.ucla.edu/r/dae/zip/) . Your task here
    is to predict the number of fish (*y*) caught by a fishing party in a state park.
    We have a small data set of 250 groups that we call camper data. The groups of
    individuals visited a state park and provided the following information:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从计数数据分析的经典示例开始，该示例来自 [https://stats.idre.ucla.edu/r/dae/zip/](https://stats.idre.ucla.edu/r/dae/zip/)。你的任务是预测在州立公园钓鱼的团体捕获的鱼的数量（*y*）。我们有一个包含
    250 个团体的小型数据集，我们称之为露营者数据。这些个人团体参观了州立公园，并提供了以下信息：
- en: How many people are in the group?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组里有多少人？
- en: How many children in the group?
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组里有多少孩子？
- en: Did the group come with a camper to the park?
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个团体是否带着露营车来公园？
- en: 'To set up a probabilistic model for count data, you’ll learn about two new
    distributions that can only output whole numbers and, thus, are suitable for count
    data: the Poisson distribution and the zero-inflated Poisson (*z*IP) distribution.
    The Poisson distribution has one parameter; the ZIP distribution has two parameters.
    Shortly, you’ll learn more about their meaning. To use one of these distribution
    models for predicting a count outcome with a probabilistic DL model, you follow
    the standard TFP procedure: use an NN with an appropriate architecture and capacity
    to transform the input into a predicted CPD, and in the TFP, pick the distribution
    that corresponds to your chosen outcome prediction (see figure 5.10).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置计数数据的概率模型，你将了解两种只能输出整数的新分布，因此适合计数数据：泊松分布和零膨胀泊松 (*z*IP) 分布。泊松分布有一个参数；ZIP
    分布有两个参数。简而言之，你将很快了解它们的含义。要使用这些分布模型之一，通过概率深度学习模型预测计数结果，你遵循标准的 TFP 程序：使用具有适当架构和容量的神经网络将输入转换为预测的
    CPD，然后在 TFP 中选择与所选结果预测相对应的分布（见图 5.10）。
- en: '![](../Images/5-10.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10](../Images/5-10.png)'
- en: Figure 5.10 Count data modeling with probabilistic DL. The network determines
    the parameters of a probability distribution. Fit the model using the MaxLike
    principle. In the example shown, the outcome is count data. Here it’s modeled
    by a Poisson distribution, where NN is used to control its rate parameter *λ*
    (see the chosen last plate with one output node), which gives the mean and the
    variance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 使用概率深度学习进行计数数据建模。网络确定概率分布的参数。使用最大似然原理拟合模型。在示例中，结果是计数数据。这里它通过泊松分布建模，其中NN用于控制其速率参数*λ*（参见选定的最后一个带有单个输出节点的板），这给出了平均值和方差。
- en: 'Before working through this chapter, your first impulse might be to fit the
    camper data with a standard NN and a linear activation function at the output
    node and then train the NN using the MSE loss. Using a linear regression model
    is, in fact, what many people do in this situation, but it’s not the best solution!
    You can try this naive approach using the code provided in the notebook: [http://mng.bz/jgMz](http://mng.bz/jgMz)
    .'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习本章之前，你可能会首先想用标准NN和输出节点上的线性激活函数来拟合露营车数据，然后使用MSE损失训练NN。实际上，在这种情况下，许多人使用线性回归模型，但这并不是最佳解决方案！你可以尝试使用笔记本中提供的代码进行这种天真方法：[http://mng.bz/jgMz](http://mng.bz/jgMz)。
- en: 'To check the performance of the model, you can compare the predicted outcome
    distributions with the actual outcomes. Let’s pick the predicted outcome distribution
    for the test observations 31 and 33 (see figure 5.11). Observation 31 is a group
    that caught five fish and had the following features: used live bait, had a camper,
    and were four persons with one child. Observation 33 is a group that caught zero
    fish and had the following features: used live bait, no camper, four persons,
    two children. Looking at the predicted CPDs for observations 31 and 33 shows that
    the likelihood of the observed outcome (five fish and zero fish) is quite good
    for both observations (the dotted line in figure 5.11). But according to these
    CPDs, negative numbers of fish caught would also have quite high likelihoods.
    Common sense tells you that catching -2 fish is rather unlikely. Another problem
    of the linear regression model for count data is that the predicted outcome distributions
    are continuous, but the possible number of fish caught are integer numbers--you
    can’t catch half a fish.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查模型的性能，你可以将预测的结果分布与实际结果进行比较。让我们选取测试观察值31和33的预测结果分布（见图5.11）。观察值31是一个捕获了五条鱼并具有以下特征的群体：使用了活饵，有露营车，四个人，一个孩子。观察值33是一个捕获了零条鱼并具有以下特征的群体：使用了活饵，没有露营车，四个人，两个孩子。观察31和33的预测CPD显示，观察到的结果（五条鱼和零条鱼）在这两个观察中都有相当好的可能性（图5.11中的虚线）。但是，根据这些CPD，捕获负数条鱼也有相当高的可能性。常识告诉你，捕获-2条鱼是非常不可能的。计数数据的线性回归模型的另一个问题是，预测的结果分布是连续的，但可能捕获的鱼的数量是整数——你不能捕获半条鱼。
- en: '![](../Images/5-11.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11](../Images/5-11.png)'
- en: 'Figure 5.11 The predicted Normal distribution for the test observations 31
    (left) and 33 (right) for the camper data. The dashed lines indicate the positions
    of the predicted means. The thick dotted lines indicate the likelihoods of the
    observed outcome of observations: 31 (five fish caught) and 33 (*z*ero fish caught).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 露营车数据中测试观察值31（左）和33（右）的预测正态分布。虚线表示预测均值的位臵。粗虚线表示观察到的结果的似然性：31（捕获五条鱼）和33（*z*ero条鱼捕获）。
- en: To compare the observed outcomes with the predicted CPDs simultaneously for
    all test points in one plot, you need to draw a CPD at each test point. Such a
    plot would look rather crowded. Instead of the whole CPD, figure 5.12 shows only
    the mean (solid line), the 2.5% (lower dashed line), and 97.5% percentile (upper
    dashed line) to characterize it. Because of multiple features, it’s no longer
    possible to put the feature on the horizontal axis (as in the simple linear regression).
    Instead, the predicted mean of the CPD is plotted on the horizontal axis. Observations
    31 and 33 shown in figure 5.11 are highlighted. Again, the problem of predicted
    negative number of fish caught is visible. The lower dashed line is below zero
    and even the solid line, representing the predicted mean of the number of fish
    caught is in some regions below zero. Further, in case of a good fit, the solid
    line should run through the mean of the observed data points, which is obviously
    not the case. The plot shows only the observed data and not its mean, but still,
    you can see that the predicted mean (up to 8) is larger than the data average.
    Besides this, the 95% prediction interval indicated by the dashed lines seems
    to be unreasonably large for the small numbers of fish predicted.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一个图表中同时比较观察结果与所有测试点的预测CPD，你需要在每个测试点绘制一个CPD。这样的图表看起来会相当拥挤。而不是整个CPD，图5.12只显示了均值（实线）、2.5%（下虚线）和97.5%分位数（上虚线）来表征它。由于多个特征，不再可能将特征放在横轴上（如简单线性回归中）。相反，预测的CPD均值被绘制在横轴上。图5.11中显示的31号和33号观测值被突出显示。再次，预测捕获的鱼数量为负的问题显而易见。下虚线低于零，甚至代表预测捕获的鱼数量均值的实线在某些区域也低于零。此外，在拟合良好的情况下，实线应该穿过观察数据点的均值，这显然不是情况。该图表只显示了观察数据，而不是其均值，但仍然可以看出预测的均值（高达8）大于数据平均值。此外，由虚线表示的95%预测区间对于预测的少量鱼来说似乎不合理地大。
- en: '![](../Images/5-12.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/5-12.png)'
- en: Figure 5.12 Linear regression showing a comparison of the predicted CPDs with
    the observed data. The observed fish caught in the test camper data is plotted
    versus the predicted mean number of fish caught. The solid line depicts the mean
    of the predicted CPD. The dashed lines represent the 0.025 and 0.975 quantiles,
    yielding the borders of a 95% prediction interval. The highlighted points correspond
    to observation 33 with zero fish caught (on the left) and observation 31 with
    five fish caught (on the right).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 线性回归展示了预测CPD与观察数据的比较。测试露营者数据中观察到的捕获的鱼数与预测的捕获鱼数均值进行比较。实线描述了预测CPD的均值。虚线代表0.025和0.975分位数，形成95%预测区间的边界。突出显示的点对应于捕获零条鱼的观测值33（左侧）和捕获五条鱼的观测值31（右侧）。
- en: Linear regression assuming a Gaussian CPD has obviously some flaws. But which
    distribution is more appropriate? That’s the question! Even after realizing that
    you’re dealing with count data, there are different options for count data. In
    the simplest case, count data can be described by a Poisson distribution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设高斯CPD进行线性回归显然存在一些缺陷。但哪种分布更合适呢？这正是问题所在！即使意识到你正在处理计数数据，也有不同的计数数据选项。在最简单的情况下，计数数据可以用泊松分布来描述。
- en: 5.4.1 The Poisson distribution for count data
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 计数数据的泊松分布
- en: Back in the pre-DL era, around the year 1900, Ladislaus von Bortkiewicz wanted
    to model the number of soldiers kicked to death by horses each year in each of
    the 14 cavalry corps of the Prussian army. As training data, they had data from
    20 years. In his first statistics book, Das Gesetz der kleinen Zahlen (The Law
    of Small Numbers), he modeled the number of soldiers kicked to death by horses
    by the Poisson distribution, a distribution named after Siméon Denis Poisson.
    To keep our discussion less bloody, the number of raindrops in a bucket per minute
    can also be modeled by a Poisson distribution. Or, in our case, the number of
    fish caught per state park camper group. In all these examples, you count the
    number of events per unit; these units are often units of time, but they can also
    be other units like the number of homicides per 100,000 inhabitants.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习之前的时代，大约在 1900 年，拉迪斯拉夫·冯·博特基维茨（Ladislaus von Bortkiewicz）想要模拟普鲁士军队 14 个骑兵军团每年被马踢死的士兵数量。作为训练数据，他们有
    20 年的数据。在他的第一本统计学著作《小数定律》（Das Gesetz der kleinen Zahlen）中，他使用泊松分布来模拟每年被马踢死的士兵数量，这种分布是以西莫恩·德尼·泊松（Siméon
    Denis Poisson）的名字命名的。为了使我们的讨论不那么血腥，每分钟桶中的雨滴数量也可以用泊松分布来模拟。或者，在我们的情况下，每个州立公园露营者小组捕获的鱼的数量。在所有这些例子中，你计算每单位的事件数量；这些单位通常是时间单位，但也可以是其他单位，如每
    10 万居民中的谋杀案数量。
- en: 'Often in real life, you have to deal with randomness, and you don’t always
    observe the same number of events each time (unit). But let’s assume that, on
    average, there are two events per unit (two dead soldiers per year, two raindrops
    in the bucket per minute, or two fish caught per stay). Figure 5.13 shows the
    resulting probability distribution of possible observed outcomes. The average
    number of events is the only needed information to fix the distribution (see equation
    5.1). This distribution assigns a probability to each possible outcome: a probability
    to observe zero events per unit or to observe one event per unit, two events per
    unit, and so on. Because it’s a probability distribution, all probabilities add
    up to one.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，我们经常不得不处理随机性，并且每次（单位）观察到的事件数量并不总是相同的。但让我们假设平均每单位发生两个事件（每年每支部队两个阵亡士兵、每分钟桶中的两个雨滴或每次停留捕获的两个鱼）。图
    5.13 展示了可能观察到的结果的概率分布。事件数量的平均值是确定分布的唯一所需信息（见方程 5.1）。这种分布为每个可能的结果分配一个概率：观察到一个单位零事件、一个单位一个事件、两个单位的事件，等等。因为它是一个概率分布，所有概率的总和为
    1。
- en: '![](../Images/5-13.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图像 5-13](../Images/5-13.png)'
- en: Figure 5.13 Poisson distribution for the case that there are, on average, two
    events per unit (two dead soldiers, two raindrops in the bucket, or two fish caught).
    It’s quite likely to observe either one or two events. This probability distribution
    is from the notebook [nb_ch05_02.ipynb](http://nb_ch05_02.ipynb) .
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 表示，在平均每单位发生两个事件（两个阵亡士兵、两个桶中的雨滴或两次捕获的鱼）的情况下，观察到一或两个事件的可能性相当高。这种概率分布来自笔记本
    [nb_ch05_02.ipynb](http://nb_ch05_02.ipynb)。
- en: Figure 5.13 shows that two events per (time) unit occur with a quite high probability
    of above 0.25, but other possible outcomes also have some probability. The average
    number of events per unit plays an important role because it defines the only
    parameter of the Poisson distribution, which is often called rate and is usually
    indicated in formulas with the symbol *λ* . In our case, *λ* = 2 , but because
    *λ* is the average number of events per unit, *λ* isn’t always an integer. Just
    in case you’re curious, there’s a formula for the Poisson distribution that defines
    the probability for each possible value k of the counted events. The probability
    with which it’s observed is
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 显示，每（时间）单位发生两个事件的可能性相当高，超过 0.25，但其他可能的结果也有一定的概率。事件数量的平均值起着重要作用，因为它定义了泊松分布的唯一参数，通常称为速率，通常用符号
    *λ* 表示。在我们的情况下，*λ* = 2，但 *λ* 是每单位事件数量的平均值，因此 *λ* 不总是整数。如果你好奇，泊松分布有一个公式，定义了计数事件的可能值
    k 的每个值的概率。观察到的概率是
- en: '![](../Images/equation_5-17.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![方程 5-17](../Images/equation_5-17.png)'
- en: where k! indicates the factorial of k (*k* ! = 1 ⋅ 2 ⋅ 3⋅ ... ⋅ *k* ) . Note
    that now, in contrast to the Gaussian *P*(*y* = *k*), we have a real probability
    and not just a density (as the Gaussian). To stress this, it is sometimes also
    called probability mass function. A remarkable property of Poisson distributions
    is that *λ* not only defines the center of the distribution (the expected value),
    but also the variance. The average number of events *λ* is, therefore, all you
    need to fix the distribution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 k! 表示 k 的阶乘（*k* ! = 1 ⋅ 2 ⋅ 3⋅ ... ⋅ *k*）。请注意，与高斯 *P*(*y* = *k*) 相比，我们现在有一个实际的概率，而不仅仅是密度（如高斯）。为了强调这一点，有时它也被称为概率质量函数。泊松分布的一个显著特性是
    *λ* 不仅定义了分布的中心（期望值），还定义了方差。因此，事件的平均数 *λ* 是固定分布所需的所有内容。
- en: In this listing, you can see how the Poisson distribution can be modeled with
    TFP.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，你可以看到如何使用 TFP 来建模泊松分布。
- en: Listing 5.5 The Poisson distribution in TFP
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 TFP 中的泊松分布
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Poisson distribution with parameter rate = 2
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 速率参数为 2 的泊松分布
- en: ❷ Integer values from 0 to 10 for the x-axis in figure 5.13
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 图 5.13 中 x 轴的整数值从 0 到 10
- en: ❸ Computes the probability for the values
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算值的概率
- en: ❹ The mean value, yielding 2.0
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 均值，得到 2.0
- en: ❺ The standard deviation, yielding sqrt(2.0) = 1.41 . . .
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 标准差，得到 sqrt(2.0) = 1.41 ...
- en: Now you have a proper distribution for count data. We estimate the parameter
    `rate` of the distribution using an NN. We use a very simple network, without
    a hidden layer, and its task is to predict a Poisson CPD that only requires you
    to determine the `rate` parameter. The parameter `rate` in the Poisson distribution
    is zero or a positive real value. Because the output of an NN with linear activation
    could be negative, we take the exponential function as the activation function
    before using it as the rate. (you could also choose to use a softplus activation,
    if you would like to do so.) This network is shown in the following listing.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有一个适合计数数据的适当分布。我们使用神经网络估计分布的参数 `rate`。我们使用一个非常简单的网络，没有隐藏层，其任务是预测一个泊松 CPD，只需要确定
    `rate` 参数。泊松分布中的参数 `rate` 是零或一个正实数。因为具有线性激活的神经网络的输出可能是负数，所以我们使用指数函数作为激活函数，在将其用作速率之前。（你也可以选择使用
    softplus 激活，如果你愿意的话。）这个网络在下面的列表中显示。
- en: Listing 5.6 Simple Poisson regression for the number of fish caught
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 简单的泊松回归用于捕获的鱼的数量
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ We use the exponential of the output to model the rate.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用输出的指数来建模速率。
- en: ❷ Defines a single layer with one output
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个具有一个输出的单层
- en: ❸ Glues the NN and the output layer together. Note that output p_y is a tf.distribution.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将神经网络和输出层粘合在一起。注意，输出 p_y 是一个 tf.distribution。
- en: ❹ The second argument is the output of the model and thus a TFP distribution.
    It's as simple as calling log_prob to calculate the log probability of the observation
    that’s needed to calculate the NLL.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第二个参数是模型的输出，因此是一个 TFP 分布。它就像调用 log_prob 来计算所需计算 NLL 的观察值的对数概率一样简单。
- en: If you stepped through the code in the notebook, you noticed that the solution
    with the Poisson distribution is better than using linear regression. The root
    mean squared error(RMSE) is approximately 7.2 for the Poisson regression, lower
    than 8.6, which is the RMSE for the linear regression. But more importantly, the
    NLL is 2.7 for the Poisson regression compared to 3.6 for the linear regression
    and, thus, is much lower.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经逐步通过了笔记本中的代码，你会注意到使用泊松分布的解决方案比使用线性回归要好。泊松回归的均方根误差（RMSE）大约为 7.2，低于线性回归的
    8.6。但更重要的是，泊松回归的 NLL 为 2.7，而线性回归为 3.6，因此要低得多。
- en: Let’s have a closer look at the results from the Poisson prediction and use
    the fitted Poisson model to predict the CPDs for the test observations 31 and
    33 (see figure 5.14). The predicted outcome distributions for observations 31
    and 33 are Poisson distributions with rate[31] = 5.56 and rate[33] = 0.55 , respectively.
    The likelihood of the observed outcomes (five fish and zero fish) is quite good
    for both observations (the dotted line in figure 5.11).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看泊松预测的结果，并使用拟合的泊松模型来预测测试观察值 31 和 33 的 CPD（见图 5.14）。观察值 31 和 33 的预测结果分布分别是速率[31]
    = 5.56 和速率[33] = 0.55 的泊松分布。对于两个观察值（五条鱼和零条鱼）的观测结果，似然度相当好（图 5.11 中的虚线）。
- en: '![](../Images/5-14.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-14.png)'
- en: Figure 5.14 The predicted Poisson distribution for the test observations 31
    (left) and 33 (right). The thick dotted lines indicate the likelihood of the observed
    outcome for the observations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 测试观测值31（左）和33（右）的预测泊松分布。粗虚线表示观测结果的似然性。
- en: 'To compare the observed outcomes with the predicted CPDs simultaneously for
    all the test points in one plot, you can again plot the observed number of fish
    versus the predicted mean number of fish (see figure 5.15) and indicate the mean
    of the predicted CPD as a solid line and the 2.5% and 97.5% percentiles of the
    predicted CPD as dashed lines. You might wonder why the quantile curves aren’t
    smooth but take whole numbers at the position of the predicted CPD (see the right
    panel in figure 5.15). The answer lies in the nature of the Poisson distribution
    and the definition of the percentiles: a Poisson model can only assign probabilities
    to integer values. The 97.5% percentile, for example, is defined as the value
    in the distribution that can only be an integer for which 97.5% of the values
    are smaller than or equal to this value.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要在一个图表中同时比较所有测试点的观测结果与预测的CPD，你可以再次绘制观测到的鱼的数量与预测的鱼的平均数量的对比图（见图5.15），并将预测CPD的平均值用实线表示，预测CPD的2.5%和97.5%分位数用虚线表示。你可能想知道为什么分位数曲线不光滑，但在预测CPD的位置取整数值（见图5.15的右侧面板）。答案在于泊松分布的本质和分位数的定义：泊松模型只能为整数值分配概率。例如，97.5%分位数定义为分布中只能为整数的值，对于这个值，97.5%的值都小于或等于这个值。
- en: How to read diagnostic plots for probabilistic models
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如何阅读概率模型的诊断图
- en: Here’s some hints for reading the diagnostic graphs used to evaluate the performance
    of probabilistic models. Note that in figures 5.12, 5.15, and 5.17, the observed
    outcome is plotted versus the mean of the predicted CPD (not versus an input feature
    as in the simple linear regression with one variable). Because in the camper data
    we have four features, it’s not possible to use a single input feature for the
    horizontal axis, and the graphical representation gets harder. Also note that
    different combinations of input features can yield not only the same predicted
    mean for the CPD, but can potentially have different quantiles. In these cases,
    you have several CPDs at the same mean positions but with different quantiles,
    resulting in more than one quantile at the same *x* position. The more data you
    have, the higher the probability to observe such cases. In chapter 6, you’ll see
    such cases. Also, the jumps in the quantiles (Dashed lines in the figures) aren’t
    artifact. Because the x-axis doesn’t represent a smooth change of an input variable
    anymore, such jumps are possible. For the same reason, the quantiles don’t need
    to change smoothly (see figure 5.17). The fact that quantiles do not need to show
    a monotonic behavior was already visible in the simple linear regression example
    (see figure 5.9). The curve of the CPD’s mean (solid line in figure 5.15) is always
    the main diagonal because it plots the mean versus the mean.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些阅读用于评估概率模型性能的诊断图的提示。注意，在图5.12、5.15和5.17中，观测结果是以预测CPD的平均值进行对比（而不是像只有一个变量的简单线性回归那样与输入特征对比）。因为在露营者数据中我们有四个特征，所以不可能使用单个输入特征作为横坐标，图形表示变得更加复杂。还请注意，不同的输入特征组合不仅可以产生相同的CPD预测平均值，而且可能具有不同的分位数。在这些情况下，你会在相同的平均值位置上有几个不同的CPD，但分位数不同，导致在相同的*x*位置上有多个分位数。数据越多，观察到这种情况的概率就越高。在第6章中，你会看到这样的例子。此外，分位数（图中的虚线）的跳跃不是人为的。因为x轴不再代表输入变量的平滑变化，这样的跳跃是可能的。同样地，分位数不需要平滑变化（见图5.17）。分位数不需要显示单调行为的事实已经在简单的线性回归示例中可见（见图5.9）。CPD平均值的曲线（图5.15中的实线）始终是主对角线，因为它绘制的是平均值与平均值的关系。
- en: '![](../Images/5-15.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-15.png)'
- en: Figure 5.15 Poisson regression prediction results for the camper example. The
    observed fish caught in the test sample is plotted versus the predicted mean number
    of fish caught. To indicate the predicted CPD, the solid lines depict the CPD’s
    mean. The dashed lines represent the 0.025 and 0.975 quantiles, yielding the borders
    of a 95% prediction interval.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 针对露营者示例的泊松回归预测结果。测试样本中捕获的鱼的数量与预测的捕获鱼的平均数量进行对比。为了表示预测的CPD，实线描绘了CPD的平均值。虚线代表0.025和0.975分位数，从而得到95%预测区间的边界。
- en: 'Note that in contrast to the linear regression model, the Poisson model predicts
    outcome distributions that only assign probabilities to values that can actually
    be observed: the non-negative and whole numbers of fish caught. In case of an
    ideal probabilistic prediction model, the mean of the observed values should correspond
    to the solid line, and 95% of all points should be between the two dashed lines.
    According to these criteria, the Poisson models seem to be reasonably good, at
    least for the majority of data. But still, there might be room for improvement.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与线性回归模型不同，泊松模型预测的结果分布只分配概率给可以实际观察到的值：捕到的非负整数鱼的数量。在理想概率预测模型的情况下，观察值的平均值应该对应于实线，95%
    的所有点应该位于两条虚线之间。根据这些标准，泊松模型似乎相当合理，至少对于大多数数据来说是这样。但仍然，可能还有改进的空间。
- en: 'It seems that there are a lot of groups that didn’t catch any fish at all.
    How can it be that there are so many unlucky fishermen? We can only speculate,
    but it might also be that camping parties just brought their fishing gear as an
    excuse to drink vast amounts of beer and not to go fishing at all. Therefore,
    there might be two reasons to leave the state park with zero fish: bad luck or
    no fishing at all. A model explicitly taking those lazy fishermen into account
    is the zero-inflated model. We discuss that model next.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有很多组完全没有捕到鱼。怎么会有这么多不幸的渔民呢？我们只能猜测，但也许露营团体只是以钓鱼装备为借口，喝大量的啤酒，根本不去钓鱼。因此，可能有两种原因导致带着零鱼离开州立公园：运气不好或根本不去钓鱼。一个明确考虑那些懒惰渔民的模型是零膨胀模型。我们将在下一节讨论该模型。
- en: 5.4.2 Extending the Poisson distribution to a zero-inflated Poisson (*z*IP)
    distribution
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 将泊松分布扩展到零膨胀泊松 (*z*IP) 分布
- en: 'The zero-inflated Poisson (*z*IP) distribution takes into account the fact
    that there are many zeros, more than are compatible with the number of expected
    zeros in a Poisson distribution. In our examples, these are the lazy camping parties
    not fishing at all. In the ZIP distribution, you can model the excess of zeros
    by the introduction of a zero-generating process as follows: you toss a coin.
    The coin has a probability p to show heads. If this is the case, you have a lazy
    camping party who yields zero fish. If not, you have a regular fishing group,
    and you can use a Poisson distribution to predict the number of fishes caught.
    To fit your count data with a ZIP outcome model in a TFP manner, you set up an
    NN with a ZIP distributional.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 零膨胀泊松 (*z*IP) 分布考虑了存在许多零的事实，这些零比泊松分布中预期的零的数量要多。在我们的例子中，这些是根本不去钓鱼的懒惰露营团体。在 ZIP
    分布中，你可以通过引入零生成过程来模拟零的过剩，如下所示：你掷一枚硬币。硬币有概率 p 显示正面。如果是这样，你有一个懒惰的露营团体，他们捕到的鱼为零。如果不是，你有一个常规的钓鱼团体，你可以使用泊松分布来预测捕到的鱼的数量。为了以
    TFP 方式将你的计数数据与 ZIP 结果模型拟合，你设置了一个具有 ZIP 分布的神经网络。
- en: 'Unfortunately, TFP doesn’t yet provide a ZIP distribution. But it’s quite easy
    to define a custom function for a new distribution based on existing TFP distributions
    (see listing 5.7). The ZIP needs two parameters:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，TFP 还没有提供 ZIP 分布。但是，根据现有的 TFP 分布定义一个新的自定义函数相当简单（参见列表 5.7）。ZIP 需要两个参数：
- en: The probability p with which additional zeros are produced
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产生额外零的概率 p
- en: The rate of a Poisson distribution
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泊松分布的速率
- en: 'The ZIP function takes as input the two output nodes from an NN: one for the
    rate and one for p. As shown in listing 5.7, we use the exponential transformation
    on the first component `out[:,0:1]` of the output `out` to get a positive value
    for the rate, and the sigmoid transformation on the output `out_2` of the NN to
    get a value between 0 and 1 for p.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ZIP 函数接受来自神经网络的两个输出节点：一个用于速率，一个用于 p。如列表 5.7 所示，我们对输出 `out` 的第一个组件 `out[:,0:1]`
    应用指数变换以获得正的速率值，并对神经网络输出 `out_2` 应用 sigmoid 变换以获得 p 的值在 0 到 1 之间。
- en: Listing 5.7 Custom distribution for a ZIP distribution
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 ZIP 分布的自定义分布
- en: '[PRE6]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The first component codes the rate. We used exponential to guarantee values
    > 0 and the squeeze function to flatten the tensor.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个组件编码速率。我们使用指数函数来保证值大于 0，并使用挤压函数来平坦化张量。
- en: ❷ The second component codes zero inflation; using the sigmoid squeezes the
    value between 0 and 1.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个组件编码零膨胀；使用 sigmoid 函数将值挤压在 0 和 1 之间。
- en: ❸ The two probabilities for 0’s or Poissonian distribution
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 0 或泊松分布的两个概率
- en: ❹ tfd.Categorical allows creating a mixture of two components.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ tfd.Categorical 允许创建两个组件的混合。
- en: ❺ Zero as a deterministic value
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 零作为一个确定性的值
- en: ❻ Value drawn from a Poissonian distribution
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 从泊松分布中抽取的值
- en: The network is then simply a network without a hidden layer and two output nodes.
    The following listing shows the code to setup the network.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 网络随后就变成了一个没有隐藏层和两个输出节点的简单网络。以下列表显示了设置网络的代码。
- en: Listing 5.8 An NN in front of a ZIP distribution
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8ZIP分布前的NN
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ A dense layer without activation. The transformation is done in the zero_inf
    function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个没有激活的密集层。转换在zero_inf函数中完成。
- en: You can now use the fitted ZIP model to predict the probability distribution
    for observations in the test data set. Let’s use the fitted ZIP model to predict
    the CPDs for the test observations 31 and 33 (see figure 5.16).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用拟合的ZIP模型来预测测试数据集中的观测值的概率分布。让我们使用拟合的ZIP模型来预测测试观测值31和33的CPD（见图5.16）。
- en: '![](../Images/5-16.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-16.png)'
- en: Figure 5.16 The predicted ZIP distribution for the test observations 31 (left)
    and 33 (right). The thick dotted lines indicate the likelihood of the observed
    outcome of the observations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16测试观测值31（左）和33（右）的预测ZIP分布。粗虚线表示观测结果的似然性。
- en: The most striking feature of the predicted ZIP CPDs is the large peak at 0 (see
    figure 5.16). This is due to the zero-inflated process modeling a higher number
    of zeros compared to a Poisson process. The likelihood of the observed outcomes
    (five fish and zero fish) is quite good for both observations (see the dotted
    lines in figure 5.16).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 预测ZIP CPD最显著的特征是在0处有一个大的峰值（见图5.16）。这是由于零膨胀过程相对于泊松过程模拟了更多的零。观察到的结果（五条鱼和零条鱼）的似然度对于两个观测值都相当好（见图5.16中的虚线）。
- en: To compare the observed outcomes with the predicted CPDs simultaneously for
    all test points in one plot, you can again plot the observed number of fish versus
    the predicted mean number of fish (see figure 5.17). To indicate the shape of
    the predicted CPD, we plot the mean of the predicted CPD (see the solid lines
    in figure 5.17) and the 2.5% and 97.5% percentiles of the predicted CPD (see the
    dashed lines in figure 5.17). In the ZIP model, the 2.5% percentile stays at zero
    over the whole range. This means that for all groups, the predicted ZIP CPDs assign
    a probability higher than 2.5% to the outcome zero, which nicely corresponds to
    the high number of observed zeros.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要在一个图中同时比较所有测试点的观察结果与预测的CPD，你可以再次绘制观察到的鱼的数量与预测的捕鱼平均数量（见图5.17）。为了表示预测CPD的形状，我们绘制预测CPD的平均值（见图5.17中的实线）和预测CPD的2.5%和97.5%分位数（见图5.17中的虚线）。在ZIP模型中，2.5%分位数在整个范围内保持在零。这意味着对于所有组，预测的ZIP
    CPD将高于2.5%的概率分配给零这个结果，这与观察到的零的数量很高很好地对应。
- en: '![](../Images/5-17.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-17.png)'
- en: Figure 5.17 Regression prediction results from the ZIP model for the camper
    example. The observed fish caught in the test sample is plotted versus the predicted
    mean number of fish caught. To indicate the predicted CPD, the solid lines depict
    the CPD’s mean. The dashed lines represent the 0.025 and 0.975 quantiles, yielding
    the borders of a 95% prediction interval.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 ZIP模型对露营示例的回归预测结果。测试样本中观察到的捕鱼数量与预测的捕鱼平均数量进行对比。为了表示预测的CPD，实线表示CPD的平均值。虚线代表0.025和0.975分位数，从而得到95%预测区间的边界。
- en: In figure 5.17, it seems that the mean of the observed values is close to the
    solid lines and that 95% of all points are in between the two dashed lines. When
    quantifying the performance of the ZIP model by its test NLL, it turns out that
    the zero-inflated model outperforms both the linear regression and the Poisson
    model (see table 5.2).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.17中，观察到的值的平均值似乎接近实线，而95%的点都位于两条虚线之间。通过测试NLL来量化ZIP模型的表现，结果表明零膨胀模型优于线性回归和泊松模型（见表5.2）。
- en: Table 5.2 Comparison of the prediction performance of different models on validation
    data. If you run the notebook, you might get slightly different values. There’s
    some randomness involved, and there’s not a lot of data. But the overall behavior
    should be more or less the same.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2不同模型在验证数据上的预测性能比较。如果你运行笔记本，可能会得到略微不同的值。这里涉及一些随机性，而且数据量不是很多。但整体行为应该大致相同。
- en: '|  | Linear Regression | Poisson | Zero-Inflated |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 线性回归 | 泊松 | 零膨胀 |'
- en: '| RMSE | 8.6 | 7.2 | 7.3 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | 8.6 | 7.2 | 7.3 |'
- en: '| MAE | 4.7 | 3.1 | 3.2 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 4.7 | 3.1 | 3.2 |'
- en: '| NLL | 3.6 | 2.7 | 2.2 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| NLL | 3.6 | 2.7 | 2.2 |'
- en: For fitting the data of the fishing party, there is no clear consensus to which
    model is the overall best. Table 5.2 reveals that Poisson is best in terms of
    RMSE and MAE. The ZIP model is best in terms of NLL. But as discussed in section
    5.2, to measure the probabilistic prediction performance, you should use the mean
    test NLL. The ZIP model is, therefore, the best probabilistic model of all three
    models’ tests. Note that strictly speaking, the NLL should be only compared among
    discrete models (Poisson or ZIP as CPD) but not between a continuous and a discrete
    model (Gaussian or Poisson as CPD). Is there a better model? There might be a
    model with a lower NLL, but you can’t tell. In this application, there’s no theoretical
    lower bound of the NLL.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于拟合渔猎团体的数据，没有明确的共识表明哪个模型是整体上最好的。表5.2显示，泊松分布就RMSE和MAE而言是最好的。ZIP模型在NLL方面表现最佳。但如第5.2节所述，为了衡量概率预测性能，您应该使用平均测试NLL。因此，ZIP模型是三个模型测试中最好的概率模型。请注意，严格来说，NLL只应在离散模型（泊松或ZIP作为CPD）之间进行比较，而不应在连续和离散模型（高斯或泊松作为CPD）之间进行比较。是否有更好的模型？可能存在一个NLL更低的模型，但您无法确定。在这个应用中，NLL没有理论上的下限。
- en: In the end, just for completeness, we’d like to note that there’s a third way
    to deal with count data, and that’s using the so-called negative binomial distribution.
    Like the ZIP distribution, it’s a distribution with two parameters that allows
    not only the mean of the counts, but also the standard deviation of the counts
    to depend on the input.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了完整性，我们想指出，处理计数数据还有第三种方法，那就是使用所谓的负二项分布。像ZIP分布一样，它是一个具有两个参数的分布，不仅允许计数均值依赖于输入，还允许计数的标准差依赖于输入。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A probabilistic model predicts for each input a whole conditional probability
    distribution (CPD).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率模型为每个输入预测一个完整的条件概率分布（CPD）。
- en: The predicted CPD assigns for each possible outcome *y*, a probability with
    which it’s expected.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的CPD为每个可能的输出*y*分配一个预期的概率。
- en: The negative log-likelihood (NLL) measures how well the CPD matches the actual
    distribution of the outcomes.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负对数似然（NLL）衡量CPD与实际结果分布的匹配程度。
- en: You use the NLL as a loss function when training a probabilistic model.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练概率模型时，您使用NLL作为损失函数。
- en: You use the NLL on new data to measure and to compare the prediction performance
    of different probabilistic models.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您使用NLL对新数据进行测量和比较不同概率模型的预测性能。
- en: Using a proper choice for the CPD enhances the performance of your models.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用合适的CPD选择可以增强您模型的性能。
- en: For continuous data, a common first choice is a Normal distribution.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于连续数据，一个常见的选择是正态分布。
- en: For count data, common choices for distribution are Poisson, negative binomial,
    or zero-inflated Poisson(*z*IP).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于计数数据，常见的分布选择是泊松分布、负二项分布或零膨胀泊松分布（*z*IP）。
- en: '1.This probability can be computed as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 1.这个概率可以按以下方式计算：
- en: '`   import tensorflow_probability as tfp`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`   import tensorflow_probability as tfp`'
- en: '`   dist = tfp.distributions.Normal(loc=22, scale=2)`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`   dist = tfp.distributions.Normal(loc=22, scale=2)`'
- en: '`   dist.cdf(25) #0.933`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`   dist.cdf(25) #0.933`'
- en: '2.The probability can be computed as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 2.概率可以按以下方式计算：
- en: '`   import tensorflow_probability as tfp`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`   import tensorflow_probability as tfp`'
- en: '`   dist = tfp.distributions.Normal(loc=19, scale=12)`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`   dist = tfp.distributions.Normal(loc=19, scale=12)`'
- en: '`   dist.cdf(25) #0.691`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`   dist.cdf(25) #0.691`'
- en: 3.Just to give you an idea for the proof, in this proof, it’s assumed that the
    data is generated from a true distribution. In practice, the true distribution
    is never known. It then can be shown that the NLL is a so-called proper score
    that reaches only its minimal value if the predicted distribution is equal to
    the true distribution.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 3.仅为了给您一个证明的思路，在这个证明中，假设数据是从一个真实分布生成的。在实践中，真实分布是未知的。然后可以证明NLL是一个所谓的正确得分，只有当预测分布等于真实分布时，它才达到其最小值。
- en: 4.In contrast to this sidebar mentioned here from chapter 4 (section 4.3), we
    divided by n. This is OK because it does not change the position of the minimum.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 4.与这里提到的第4章（第4.3节）中的侧边栏相反，我们除以n。这是可以的，因为它不会改变最小值的位置。
- en: '5.See the notebook titled “Result: Constant sigma.”'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 5.参见标题为“结果：常量sigma”的笔记本。
