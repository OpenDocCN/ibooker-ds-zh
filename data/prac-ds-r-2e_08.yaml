- en: Chapter 6\. Choosing and evaluating models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：选择和评估模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Mapping business problems to machine learning tasks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将商业问题映射到机器学习任务
- en: Evaluating model quality
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型质量
- en: Explaining model predictions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释模型预测
- en: In this chapter, we will discuss the modeling process ([figure 6.1](../Text/06.xhtml#ch06fig01)).
    We discuss this process before getting into the details of specific machine learning
    approaches, because the topics in this chapter apply generally to any kind of
    model. First, let’s discuss choosing an appropriate model approach.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论建模过程（[图6.1](../Text/06.xhtml#ch06fig01)）。在深入探讨特定机器学习方法的细节之前，我们先讨论这个过程，因为本章讨论的主题适用于任何类型的模型。首先，让我们讨论选择合适的模型方法。
- en: Figure 6.1\. Mental model
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1\. 心理模型
- en: '![](Images/06fig01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig01.jpg)'
- en: 6.1\. Mapping problems to machine learning tasks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1\. 将问题映射到机器学习任务
- en: 'As a data scientist, your task is to map a business problem to a good machine
    learning method. Let’s look at a real-world situation. Suppose that you’re a data
    scientist at an online retail company. There are a number of business problems
    that your team might be called on to address:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，您的任务是将商业问题映射到一个好的机器学习方法。让我们看看一个现实世界的情况。假设您是一家在线零售公司的数据科学家。您的团队可能会被要求解决以下一些商业问题：
- en: Predicting what customers might buy, based on past transactions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据历史交易预测客户可能会购买的商品
- en: Identifying fraudulent transactions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别欺诈交易
- en: Determining price elasticity (the rate at which a price increase will decrease
    sales, and vice versa) of various products or product classes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定各种产品或产品类别的价格弹性（价格上升导致销售下降的比率，反之亦然）
- en: Determining the best way to present product listings when a customer searches
    for an item
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定当客户搜索商品时展示产品列表的最佳方式
- en: 'Customer segmentation: grouping customers with similar purchasing behavior'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分：将具有相似购买行为的客户分组
- en: 'AdWord valuation: how much the company should spend to buy certain AdWords
    on search engines'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdWord估值：公司在搜索引擎上购买特定AdWords应花费的金额
- en: Evaluation of marketing campaigns
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估营销活动
- en: Organizing new products into a product catalog
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新产品组织到产品目录中
- en: 'Your intended uses of the model have a big influence on what methods you should
    use. If you want to know how small variations in input variables affect outcome,
    then you likely want to use a regression method. If you want to know what single
    variable drives most of a categorization, then decision trees might be a good
    choice. Also, each business problem suggests a statistical approach to try. For
    the purposes of this discussion, we will group the different kinds of problems
    that a data scientist typically solves into these categories:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您对模型的预期用途将对您应使用的方法有很大影响。如果您想知道输入变量的微小变化如何影响结果，那么您可能想使用回归方法。如果您想知道哪个单一变量驱动了分类的大部分，那么决策树可能是一个不错的选择。此外，每个商业问题都建议尝试一种统计方法。为了讨论的目的，我们将数据科学家通常解决的问题的不同类型分组到以下类别中：
- en: '***Classification—*** Assigning labels to datums'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分类—*** 将标签分配给数据'
- en: '***Scoring—*** Assigning numerical values to datums'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***评分—*** 将数值分配给数据'
- en: '***Grouping—*** Discovering patterns and commonalities in data'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分组—*** 在数据中发现模式和共性'
- en: In this section, we’ll describe these problem classes and list some typical
    approaches to each.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述这些问题类别，并列出针对每个类别的典型方法。
- en: 6.1.1\. Classification problems
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1\. 分类问题
- en: Let’s try the following example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试以下示例。
- en: '* * *'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose your task is to automate the assignment of new products to your company’s
    product categories, as shown in [figure 6.2](../Text/06.xhtml#ch06fig02).*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设您的任务是自动化将新产品分配到您公司的产品类别中，如图6.2所示。*'
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Figure 6.2\. Assigning products to product categories
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2\. 将产品分配到产品类别
- en: '![](Images/06fig02_alt.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig02_alt.jpg)'
- en: This can be more complicated than it sounds. Products that come from different
    sources may have their own product classification that doesn’t coincide with the
    one that you use on your retail site, or they may come without any classification
    at all. Many large online retailers use teams of human taggers to hand categorize
    their products. This is not only labor intensive, but inconsistent and error prone.
    Automation is an attractive option; it’s labor saving, and can improve the quality
    of the retail site.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能比听起来更复杂。来自不同来源的产品可能有它们自己的产品分类，这与你在零售网站上使用的分类不一致，或者它们可能没有任何分类。许多大型在线零售商使用由人类标签员组成的团队手动分类他们的产品。这不仅劳动密集，而且不一致且容易出错。自动化是一个吸引人的选择；它节省劳动力，并且可以提高零售网站的质量。
- en: 'Product categorization based on product attributes and/or text descriptions
    of the product is an example of *classification*: deciding how to assign (known)
    labels to an object. Classification itself is an example of what is called *supervised
    learning*: in order to learn how to classify objects, you need a dataset of objects
    that have already been classified (called the *training set* ). Building training
    data is the major expense for most classification tasks, especially text-related
    ones.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于产品属性和/或产品文本描述的产品分类是*分类*的一个例子：决定如何将（已知的）标签分配给一个对象。分类本身是所谓*监督学习*的一个例子：为了学习如何分类对象，你需要一个已经分类的对象数据集（称为*训练集*）。构建训练数据是大多数分类任务的主要开销，尤其是与文本相关的任务。
- en: '* * *'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Multicategory vs. two-category classification**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**多类别与二类别分类**'
- en: Product classification is an example of *multicategory* or *multinomial* classification.
    Most classification problems and most classification algorithms are specialized
    for two-category, or binomial, classification. There are tricks to using binary
    classifiers to solve multicategory problems (for example, building one classifier
    for each category, called a *one-versus-rest* classifier). But in most cases it’s
    worth the effort to find a suitable multiple-category implementation, as they
    tend to work better than multiple binary classifiers (for example, using the package
    `mlogit` instead of the base method `glm()` for logistic regression).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 产品分类是*多类别*或*多项式*分类的一个例子。大多数分类问题和大多数分类算法都是针对二类别，或二项式，分类专门化的。有一些技巧可以使用二进制分类器来解决多类别问题（例如，为每个类别构建一个分类器，称为*一对余*分类器）。但在大多数情况下，找到合适的多类别实现是值得努力的，因为它们往往比多个二进制分类器工作得更好（例如，使用`mlogit`包而不是基础方法`glm()`进行逻辑回归）。
- en: '* * *'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Common classification methods that we will cover in this book include logistic
    regression (with a threshold) and decision tree ensembles.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将涵盖的常见分类方法包括逻辑回归（带有阈值）和决策树集成。
- en: 6.1.2\. Scoring problems
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2\. 评分问题
- en: Scoring can be explained as follows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评分可以解释如下。
- en: '* * *'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose that your task is to help evaluate how different marketing campaigns
    can increase valuable traffic to the website. The goal is not only to bring more
    people to the site, but to bring more people who buy.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你的任务是帮助评估不同的营销活动如何增加对网站有价值的流量。目标是不仅吸引更多的人到网站上，还要吸引更多购买的人。*'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In this situation, you may want to consider a number of different factors:
    the communication channel (ads on websites, YouTube videos, print media, email,
    and so on); the traffic source (Facebook, Google, radio stations, and so on);
    the demographic targeted; the time of year, and so on. You want to measure if
    these factors increase sales, and by how much.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可能需要考虑许多不同的因素：沟通渠道（网站上的广告、YouTube 视频、印刷媒体、电子邮件等）；流量来源（Facebook、Google、电台等）；目标受众；一年中的时间，等等。你想要衡量这些因素是否增加了销售额，以及增加了多少。
- en: 'Predicting the increase in sales from a particular marketing campaign based
    on factors such as these is an example of *regression*, or *scoring*. In this
    case, a regression model would map the different factors being measured into a
    numerical value: sales, or the increase in sales from some baseline.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些因素预测特定营销活动带来的销售额增加是*回归*或*评分*的一个例子。在这种情况下，回归模型将测量的不同因素映射到一个数值：销售额，或从某个基线开始的销售额增加。
- en: 'Predicting the probability of an event (like belonging to a given class) can
    also be considered scoring. For example, you might think of fraud detection as
    classification: is this event fraud or not? However, if you are trying to estimate
    the probability that an event is fraud, this can be considered scoring. This is
    shown in [figure 6.3](../Text/06.xhtml#ch06fig03). Scoring is also an instance
    of supervised learning.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 预测事件（如属于某个给定类别）的概率也可以被认为是评分。例如，您可能会将欺诈检测视为分类：这个事件是欺诈还是不是？然而，如果您试图估计事件是欺诈的概率，这可以被认为是评分。这如图
    [图 6.3](../Text/06.xhtml#ch06fig03) 所示。评分也是监督学习的一个实例。
- en: Figure 6.3\. Notional example of determining the probability that a transaction
    is fraudulent
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3\. 确定交易欺诈概率的概念示例
- en: '![](Images/06fig03_alt.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig03_alt.jpg)'
- en: '6.1.3\. Grouping: working without known targets'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3\. 分组：在没有已知目标的情况下工作
- en: The preceding methods require that you have a training dataset of situations
    with known outcomes. In some situations, there’s not (yet) a specific outcome
    that you want to predict. Instead, you may be looking for patterns and relationships
    in the data that will help you understand your customers or your business better.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法要求您有一个已知结果的训练数据集。在某些情况下，您可能还没有（或尚未）想要预测的具体结果。相反，您可能正在寻找数据中的模式和关系，这将帮助您更好地了解您的客户或您的业务。
- en: 'These situations correspond to a class of approaches called *unsupervised learning*:
    rather than predicting outputs based on inputs, the objective of unsupervised
    learning is to discover similarities and relationships in the data. Some common
    unsupervised tasks include these:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情况对应于一类称为 *无监督学习* 的方法：无监督学习的目标不是基于输入预测输出，而是发现数据中的相似性和关系。一些常见的无监督任务包括以下这些：
- en: '***Clustering—*** Grouping similar objects together'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***聚类——*** 将相似对象分组在一起'
- en: '***Association rules—*** Discovering common behavior patterns, for example,
    items that are always bought together, or library books that are always checked
    out together'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***关联规则——*** 发现常见的购买模式，例如，总是一起购买的商品，或者总是一起借出的图书馆书籍'
- en: Let’s expand on these two types of unsupervised methods.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨这两种无监督方法。
- en: When to use basic clustering
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用基本聚类
- en: A good clustering example is the following.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的聚类示例如下。
- en: '* * *'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to segment your customers into general categories of people
    with similar buying patterns. You might not know in advance what these groups
    should be.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设您想将客户分成具有相似购买模式的通用人群类别。您可能事先不知道这些群体应该是什么。*'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This problem is a good candidate for *k-means clustering*. K-means clustering
    is one way to sort the data into groups such that members of a cluster are more
    similar to each other than they are to members of other clusters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题非常适合使用 *k-means 聚类*。k-means 聚类是一种将数据分组的方法，使得同一组内的成员彼此之间比与其他组成员更相似。
- en: Suppose that you find (as in [figure 6.4](../Text/06.xhtml#ch06fig04)) that
    your customers cluster into those with young children, who make more family-oriented
    purchases, and those with no children or with adult children, who make more leisure-
    and social-activity-related purchases. Once you have assigned a customer into
    one of those clusters, you can make general statements about their behavior. For
    example, a customer in the with-young-children cluster is likely to respond more
    favorably to a promotion on attractive but durable glassware than to a promotion
    on fine crystal wine glasses.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您发现（如图 [图 6.4](../Text/06.xhtml#ch06fig04) 所示），您的客户分为有小孩的群体，他们购买更多家庭导向的产品，以及没有小孩或成年子女的群体，他们购买更多休闲和社会活动相关的产品。一旦您将客户分配到这些群体之一，您就可以对他们的一般行为做出概括性陈述。例如，有小孩的客户群体可能对促销耐用的吸引人玻璃器皿的反应更为积极，而不是对精美水晶酒杯的促销。
- en: Figure 6.4\. Notional example of clustering your customers by purchase pattern
    and purchase amount
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4\. 根据购买模式和购买金额对客户进行聚类的概念示例
- en: '![](Images/06fig04_alt.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig04_alt.jpg)'
- en: We will cover k-means and other clustering approaches in more detail in [section
    9.1](../Text/09.xhtml#ch09lev1sec1).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [第 9.1 节](../Text/09.xhtml#ch09lev1sec1) 中更详细地介绍 k-means 和其他聚类方法。
- en: When to use association rules
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用关联规则
- en: You might be interested in directly determining which products tend to be purchased
    together. For example, you might find that bathing suits and sunglasses are frequently
    purchased at the same time, or that people who purchase certain cult movies, like
    *Repo Man*, will often buy the movie soundtrack at the same time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对直接确定哪些产品倾向于一起购买感兴趣。例如，你可能发现泳装和太阳镜经常一起购买，或者购买某些文化电影（如《Repo Man》）的人通常会同时购买电影原声带。
- en: 'This is a good application for association rules (or even recommendation systems).
    You can mine useful product recommendations: whenever you observe that someone
    has put a bathing suit into their shopping cart, you can recommend suntan lotion,
    as well. This is shown in [figure 6.5](../Text/06.xhtml#ch06fig05). We’ll cover
    the Apriori algorithm for discovering association rules in [section 9.2](../Text/09.xhtml#ch09lev1sec2).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对关联规则（甚至推荐系统）的良好应用。你可以挖掘有用的产品推荐：每当观察到有人将泳装放入购物车时，你还可以推荐防晒霜。这如图6.5所示。我们将在[第9.2节](../Text/09.xhtml#ch09lev1sec2)中介绍用于发现关联规则的Apriori算法。
- en: Figure 6.5\. Notional example of finding purchase patterns in your data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5。在数据中寻找购买模式的概念示例
- en: '![](Images/06fig05_alt.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig05_alt.jpg)'
- en: 6.1.4\. Problem-to-method mapping
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4. 问题到方法的映射
- en: To summarize the preceding, [table 6.1](../Text/06.xhtml#ch06table01) maps some
    typical business problems to their corresponding machine learning tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总结前面的内容，[表6.1](../Text/06.xhtml#ch06table01) 将一些典型的商业问题映射到相应的机器学习任务。
- en: Table 6.1\. From problem to approach
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1。从问题到方法
- en: '| Example tasks | Machine learning terminology |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 示例任务 | 机器学习术语 |'
- en: '| --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Identifying spam email Sorting products in a product catalog Identifying
    loans that are about to default Assigning customers to preexisting customer clusters
    | *Classification*—Assigning known labels to objects. Classification is a supervised
    method, so you need preclassified data in order to train a model. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 识别垃圾邮件 在产品目录中排序产品 识别即将违约的贷款 将客户分配到现有的客户群组 | *分类*—将已知的标签分配给对象。分类是一种监督方法，因此你需要预分类的数据来训练模型。
    |'
- en: '| Predicting the value of AdWords Estimating the probability that a loan will
    default Predicting how much a marketing campaign will increase traffic or sales
    Predicting the final price of an auction item based on the final prices of similar
    products that have been auctioned in the past | *Regression*—Predicting or forecasting
    numerical values. Regression is also a supervised method, so you need data where
    the output is known, in order to train a model. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 预测AdWords的价值 估计贷款违约的概率 预测营销活动将增加多少流量或销售额 基于过去拍卖的类似产品的最终价格预测拍卖物品的最终价格 | *回归*—预测或预测数值。回归也是一种监督方法，因此你需要已知输出的数据来训练模型。
    |'
- en: '| Finding products that are purchased together Identifying web pages that are
    often visited in the same session Identifying successful (often-clicked) combinations
    of web pages and AdWords | *Association rules*—Finding objects that tend to appear
    in the data together. Association rules are an unsupervised method; you do not
    need data where you already know the relationships, but are trying to discover
    the relationships within your data. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 寻找一起购买的产品 识别在同一会话中经常访问的网页 识别成功的（经常点击的）网页和AdWords的组合 | *关联规则*—寻找在数据中倾向于一起出现的对象。关联规则是一种无监督方法；你不需要已知关系的现有数据，而是试图发现数据中的关系。
    |'
- en: '| Identifying groups of customers with the same buying patterns Identifying
    groups of products that are popular in the same regions or with the same customer
    clusters Identifying news items that are all discussing similar events | *Clustering*—Finding
    groups of objects that are more similar to each other than to objects in other
    groups. Clustering is also an unsupervised method; you do not need pregrouped
    data, but are trying to discover the groupings within your data. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 识别具有相同购买模式的客户群体 识别在相同地区或具有相同客户群组的流行产品群体 识别讨论类似事件的新闻条目 | *聚类*—找到彼此之间比其他组中的对象更相似的物体组。聚类也是一种无监督方法；你不需要预分组的数据，而是试图发现数据中的分组。
    |'
- en: '* * *'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Prediction vs. forecasting**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测与预测**'
- en: 'In everyday language, we tend to use the terms *prediction* and *forecasting*
    interchangeably. Technically, to predict is to pick an outcome, such as “It will
    rain tomorrow,” and to forecast is to assign a probability: “There’s an 80% chance
    it will rain tomorrow.” For unbalanced class applications (such as predicting
    credit default), the difference is important. Consider the case of modeling loan
    defaults, and assume the overall default rate is 5%. Identifying a group that
    has a 30% default rate is an inaccurate prediction (you don’t know who in the
    group will default, and most people in the group won’t default), but potentially
    a very useful forecast (this group defaults at six times the overall rate).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常用语中，我们倾向于将*预测*和*预测*这两个词互换使用。技术上，预测是选择一个结果，例如“明天会下雨”，而预测是分配一个概率：“有80%的可能性明天会下雨。”对于不平衡类别应用（如预测信用违约），这种区别很重要。考虑建模贷款违约的情况，并假设整体违约率为5%。识别一个违约率为30%的群体是不准确的预测（你不知道群体中谁会违约，群体中的大多数人不会违约），但可能是一个非常有用的预测（这个群体的违约率是整体率的六倍）。
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 6.2\. Evaluating models
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2. 评估模型
- en: When building a model, you must be able to estimate model quality in order to
    ensure that your model will perform well in the real world. To attempt to estimate
    future model performance, we often split our data into training data and test
    data, as illustrated in [figure 6.6](../Text/06.xhtml#ch06fig06). *Test data*
    is data not used during training, and is intended to give us some experience with
    how the model will perform on new data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，你必须能够估计模型质量，以确保你的模型在现实世界中表现良好。为了尝试估计未来的模型性能，我们通常将数据分为训练数据和测试数据，如图6.6所示。[测试数据]是在训练期间未使用的数据，目的是让我们对模型在新数据上的表现有一些经验。
- en: Figure 6.6\. Schematic of model construction and evaluation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6. 模型构建和评估示意图
- en: '![](Images/06fig06_alt.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig06_alt.jpg)'
- en: 'One of the things the test set can help you identify is *overfitting*: building
    a model that memorizes the training data, and does not generalize well to new
    data. A lot of modeling problems are related to overfitting, and looking for signs
    of overfit is a good first step in diagnosing models.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集可以帮助你识别的一件事是*过度拟合*：构建一个记住训练数据的模型，并且对新数据泛化不好。许多建模问题都与过度拟合有关，寻找过度拟合的迹象是诊断模型的好第一步。
- en: 6.2.1\. Overfitting
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1. 过度拟合
- en: An overfit model looks great on the training data and then performs poorly on
    new data. A model’s prediction error on the data that it trained from is called
    *training error*. A model’s prediction error on new data is called *generalization
    error*. Usually, training error will be smaller than generalization error (no
    big surprise). Ideally, though, the two error rates should be close. If generalization
    error is large, and your model’s test performance is poor, then your model has
    probably *overfit*—it’s memorized the training data instead of discovering generalizable
    rules or patterns. You want to avoid overfitting by preferring (as long as possible)
    simpler models which do in fact tend to generalize better.^([[1](../Text/06.xhtml#ch06fn1)])
    [Figure 6.7](../Text/06.xhtml#ch06fig07) shows the typical appearance of a reasonable
    model and an overfit model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 过度拟合的模型在训练数据上看起来很棒，但在新数据上的表现却很差。模型在它训练的数据上的预测误差被称为*训练误差*。模型在新数据上的预测误差被称为*泛化误差*。通常，训练误差会小于泛化误差（这并不令人惊讶）。然而，理想情况下，这两个误差率应该很接近。如果泛化误差很大，并且你的模型测试性能不佳，那么你的模型可能已经*过度拟合*——它记住了训练数据而不是发现可泛化的规则或模式。你希望通过（尽可能）选择更简单的模型来避免过度拟合，因为这些模型实际上往往能更好地泛化。[^1](../Text/06.xhtml#ch06fn1)
    [图6.7](../Text/06.xhtml#ch06fig07)展示了合理模型和过度拟合模型的典型外观。
- en: ¹
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Other techniques to prevent overfitting include regularization (preferring small
    effects from model variables) and bagging (averaging different models to reduce
    variance).
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 防止过度拟合的其他技术包括正则化（偏好模型变量的小效应）和袋装法（平均不同的模型以减少方差）。
- en: Figure 6.7\. A notional illustration of overfitting
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7. 过度拟合的概念性插图
- en: '![](Images/06fig07_alt.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig07_alt.jpg)'
- en: An overly complicated and overfit model is bad for at least two reasons. First,
    an overfit model may be much more complicated than anything useful. For example,
    the extra wiggles in the overfit part of [figure 6.7](../Text/06.xhtml#ch06fig07)
    could make optimizing with respect to *x* needlessly difficult. Also, as we mentioned,
    overfit models tend to be less accurate in production than during training, which
    is embarrassing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个过于复杂和过度拟合的模型至少有两个缺点。首先，过度拟合的模型可能比任何有用的模型都要复杂得多。例如，[图6.7](../Text/06.xhtml#ch06fig07)中过度拟合部分的额外波动可能会使相对于*x*的优化变得毫无必要地困难。此外，正如我们提到的，过度拟合的模型在生产中的准确性通常低于训练期间，这很尴尬。
- en: Testing on held-out data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在保留数据上进行测试
- en: In [section 4.3.1](../Text/04.xhtml#ch04lev2sec7) we introduced the idea of
    splitting your data into test-train or test-train-calibration sets, as shown in
    [figure 6.8](../Text/06.xhtml#ch06fig08). Here we’ll go into more detail about
    why you want to split your data this way.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4.3.1节](../Text/04.xhtml#ch04lev2sec7)中，我们介绍了将数据分为测试-训练或测试-训练-校准集的想法，如图6.8所示。在这里，我们将更详细地讨论为什么您想要以这种方式分割数据。
- en: Figure 6.8\. Splitting data into training and test (or training, calibration,
    and test) sets
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8. 将数据分割为训练和测试（或训练、校准和测试）集
- en: '![](Images/06fig08_alt.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig08_alt.jpg)'
- en: '* * *'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you are building models to predict used car prices, based on various
    features of the car. You fit both a linear regression model and a random forest
    model, and you wish to compare the two.*^([[2](../Text/06.xhtml#ch06fn2)])'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设您正在构建基于汽车各种特征的模型来预测二手车价格。您拟合了一个线性回归模型和一个随机森林模型，并且您希望比较这两个模型。*^([[2](../Text/06.xhtml#ch06fn2)])'
- en: ²
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Both these modeling techniques will be covered in later chapters of the book.
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这两种建模技术将在本书的后续章节中介绍。
- en: '* * *'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you do not split your data, but instead use all available data to both train
    and evaluate each model, then you might think that you will pick the better model,
    because the model evaluation has seen more data. However, the data used to build
    a model is not the best data for evaluating the model’s performance. This is because
    there’s an optimistic *measurement bias* in this data, because this data was seen
    during model construction. Model construction is optimizing your performance measure
    (or at least something related to your performance measure), so you tend to get
    exaggerated estimates of performance on your training data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有分割数据，而是使用所有可用数据来训练和评估每个模型，那么您可能会认为您将选择更好的模型，因为模型评估已经看到了更多的数据。然而，用于构建模型的数据并不是评估模型性能的最佳数据。这是因为在这组数据中存在乐观的*测量偏差*，因为在这组数据中看到了模型构建过程。模型构建是优化您的性能度量（或者至少与您的性能度量相关的东西），因此您往往会得到训练数据上性能的夸张估计。
- en: In addition, data scientists naturally tend to tune their models to get the
    best possible performance out of them. This also leads to exaggerated measures
    of performance. This is often called *multiple comparison bias*. And since this
    tuning might sometimes take advantage of quirks in the training data, it can potentially
    lead to overfit.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据科学家自然倾向于调整他们的模型以获得最佳性能。这也导致了性能的过度评估。这通常被称为*多重比较偏差*。而且，由于这种调整可能有时会利用训练数据中的怪癖，它可能导致过度拟合。
- en: A recommended precaution for this optimistic bias is to split your available
    data into test and training. Perform all of your clever work on the training data
    alone, and delay measuring your performance with respect to your test data until
    as late as possible in your project (as all choices you make after seeing your
    test or holdout performance introduce a modeling bias). The desire to keep the
    test data secret for as long as possible is why we often actually split data into
    training, calibration, and test sets (as we’ll demonstrate in [section 8.2.1](../Text/08.xhtml#ch08lev2sec1)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种乐观偏差的一个推荐预防措施是将您可用的数据分为测试和训练集。仅在训练数据上执行所有您聪明的工作，并尽可能晚地测量您相对于测试数据的性能（因为您在看到测试或保留性能后所做的所有选择都会引入建模偏差）。我们希望尽可能长时间地保持测试数据保密，这就是我们通常实际上将数据分为训练、校准和测试集的原因（我们将在[第8.2.1节](../Text/08.xhtml#ch08lev2sec1)中演示）。
- en: When partitioning your data, you want to balance the trade-off between keeping
    enough data to fit a good model, and holding out enough data to make good estimates
    of the model’s performance. Some common splits are 70% training to 30% test, or
    80% training to 20% test. For large datasets, you may even sometimes see a 50–50
    split.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对数据进行分区时，你希望平衡保持足够的数据以拟合良好模型和保留足够的数据以对模型性能进行良好估计之间的权衡。一些常见的分割比例是70%用于训练，30%用于测试，或者80%用于训练，20%用于测试。对于大型数据集，有时甚至可以看到50-50的分割。
- en: K-fold cross-validation
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: 'Testing on holdout data, while useful, uses each example only once: either
    as part of the model construction or as part of the held-out model evaluation
    set. This is not *statistically efficient*,^([[3](../Text/06.xhtml#ch06fn3)])
    because the test set is often much smaller than our whole dataset. This means
    we are losing some precision in our estimate of model performance by partitioning
    our data so simply. In our example scenario, suppose you were not able to collect
    a very large dataset of historical used car prices. Then you might feel that you
    do not have enough data to split into training and test sets that are large enough
    to both build good models *and* evaluate them properly. In this situation, you
    might choose to use a more thorough partitioning scheme called *k-fold cross-validation*.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在保留数据上进行测试，虽然有用，但每个示例只使用一次：要么作为模型构建的一部分，要么作为保留的模型评估集的一部分。这并不是**统计效率高的**，^([[3](../Text/06.xhtml#ch06fn3)])
    因为测试集通常比我们的整个数据集小得多。这意味着通过如此简单地对数据进行分区，我们在对模型性能的估计中损失了一些精度。在我们的示例场景中，假设你无法收集到大量历史二手车价格数据集。那么你可能觉得你没有足够的数据来分割成足够大的训练集和测试集，以便既能构建良好的模型，又能正确评估它们。在这种情况下，你可能会选择使用更彻底的分区方案，称为**k折交叉验证**。
- en: ³
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An estimator is called statistically efficient when it has minimal variance
    for a given dataset size.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一个估计量对于一个给定的数据集大小具有最小方差时，我们称其为统计效率高的估计量。
- en: The idea behind k-fold cross-validation is to repeat the construction of a model
    on different subsets of the available training data and then evaluate that model
    only on data not seen during construction. This allows us to use each and every
    example in both training and evaluating models (just never the same example in
    both roles at the same time). The idea is shown in [figure 6.9](../Text/06.xhtml#ch06fig09)
    for *k* = 3.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: k折交叉验证背后的思想是在不同的可用训练数据子集上重复构建模型，然后在构建过程中未看到的数据上评估该模型。这使我们能够在训练和评估模型时使用每个示例（只是永远不会同时在两个角色中使用相同的示例）。这种思想在[图6.9](../Text/06.xhtml#ch06fig09)中对于**k**
    = 3进行了展示。
- en: Figure 6.9\. Partitioning data for 3-fold cross-validation
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9\. 3折交叉验证的数据分区
- en: '![](Images/06fig09_alt.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig09_alt.jpg)'
- en: In the figure, the data is split into three non-overlapping partitions, and
    the three partitions are arranged to form three test-train splits. For each split,
    a model is trained on the training set and then applied to the corresponding test
    set. The entire set of predictions is then evaluated, using the appropriate evaluation
    scores that we will discuss later in the chapter. This simulates training a model
    and then evaluating it on a holdout set that is the same size as the entire dataset.
    Estimating the model’s performance on all the data gives us a more precise estimate
    of how a model of a given type would perform on new data. Assuming that this performance
    estimate is satisfactory, then you would go back and train a final model, using
    *all* the training data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，数据被分割成三个非重叠的部分，这三个部分被排列成三个测试-训练分割。对于每个分割，在训练集上训练一个模型，然后将其应用于相应的测试集。然后使用我们在本章后面将要讨论的适当评估分数来评估整个预测集。这模拟了在一个与整个数据集大小相同的保留集上训练模型并对其进行评估。对所有数据进行模型性能的估计，给我们提供了一个更精确的估计，即给定类型的模型在新数据上会如何表现。假设这个性能估计是令人满意的，那么你就可以回去使用**所有**训练数据来训练一个最终模型。
- en: For big data, a test-train split tends to be good enough and is much quicker
    to implement. In data science applications, cross-validation is generally used
    for tuning modeling parameters, which is basically trying many models in succession.
    Cross-validation is also used when nesting models (using one model as input to
    another model). This is an issue that can arise when transforming data for analysis,
    and is discussed in [chapter 7](../Text/07.xhtml#ch07).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大数据，测试-训练分割通常足够好，并且实施起来更快。在数据科学应用中，交叉验证通常用于调整建模参数，这基本上是连续尝试许多模型。交叉验证也用于嵌套模型（使用一个模型作为另一个模型的输入）。这是在转换数据进行分析时可能出现的问题，并在第7章中讨论。
- en: 6.2.2\. Measures of model performance
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2. 模型性能度量
- en: 'In this section, we’ll introduce some quantitative measures of model performance.
    From an evaluation point of view, we group model types this way:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些模型性能的定量度量。从评估的角度来看，我们这样分组模型类型：
- en: Classification
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Scoring
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分
- en: Probability estimation
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率估计
- en: Clustering
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: For most model evaluations, we just want to compute one or two summary scores
    that tell us if the model is effective. To decide if a given score is high or
    low, we generally compare our model’s performance to a few baseline models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数模型评估，我们只想计算一个或两个总结分数，告诉我们模型是否有效。为了决定给定的分数是高还是低，我们通常将我们的模型性能与几个基线模型进行比较。
- en: The null model
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 零模型
- en: The null model is the best version of a very simple model you’re trying to outperform.
    The most typical null model is a model that returns the same answer for all situations
    (a constant model). We use null models as a lower bound on desired performance.
    For example, in a categorical problem, the null model would always return the
    most popular category, as this is the easy guess that is least often wrong. For
    a score model, the null model is often the average of all the outcomes, as this
    has the least square deviation from all the outcomes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 零模型是你试图超越的非常简单模型的最佳版本。最典型的零模型是对于所有情况都返回相同答案的模型（一个常量模型）。我们将零模型用作期望性能的下限。例如，在分类问题中，零模型将始终返回最流行的类别，因为这是最容易猜测且最不常出错的选择。对于评分模型，零模型通常是所有结果的平均值，因为这与所有结果的最小平方偏差最小。
- en: The idea is that if you’re not outperforming the null model, you’re not delivering
    value. Note that it can be hard to do as good as the best null model, because
    even though the null model is simple, it’s privileged to know the overall distribution
    of the items it will be quizzed on. We always assume the null model we’re comparing
    to is the best of all possible null models
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，如果你没有超越零模型，你就没有提供价值。请注意，要做得和最好的零模型一样好可能很困难，因为尽管零模型很简单，但它有特权知道将要被测验的项目的大致分布。我们总是假设我们比较的零模型是所有可能的零模型中最好的。
- en: Single-variable models
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量模型
- en: We also suggest comparing any complicated model against the best single-variable
    model you have available (please see [chapter 8](../Text/08.xhtml#ch08) for how
    to convert single variables into single-variable models ). A complicated model
    can’t be justified if it doesn’t outperform the best single-variable model available
    from your training data. Also, business analysts have many tools for building
    effective single-variable models (such as pivot tables), so if your client is
    an analyst, they’re likely looking for performance above this level.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还建议将任何复杂的模型与您可用的最佳单变量模型进行比较（请参阅第8章了解如何将单变量转换为单变量模型）。如果一个复杂的模型不能超越从您的训练数据中可用的最佳单变量模型，那么它就没有理由。此外，商业分析师有许多构建有效单变量模型（如交叉表）的工具，所以如果您的客户是分析师，他们很可能在寻找高于这个水平的性能。
- en: We’ll present the standard measures of model quality, which are useful in model
    construction. In all cases, we suggest that in addition to the standard model
    quality assessments, you try to design your own custom business-oriented metrics
    with your project sponsor or client. Usually this is as simple as assigning a
    notional dollar value to each outcome and then seeing how your model performs
    under that criterion. Let’s start with how to evaluate classification models and
    then continue from there.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍模型质量的标准化度量，这些度量在模型构建中很有用。在所有情况下，我们建议除了标准模型质量评估之外，您还应该尝试与您的项目赞助商或客户设计自己的以业务为导向的指标。通常这就像为每个结果分配一个名义美元价值，然后看看您的模型在这个标准下的表现。让我们从如何评估分类模型开始，然后继续下去。
- en: 6.2.3\. Evaluating classification models
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3. 评估分类模型
- en: A classification model places examples into one of two or more categories. For
    measuring classifier performance, we’ll first introduce the incredibly useful
    tool called the *confusion matrix* and show how it can be used to calculate many
    important evaluation scores. The first score we’ll discuss is accuracy.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类模型将示例放入两个或更多类别之一。为了衡量分类器的性能，我们首先介绍一个极其有用的工具，称为*混淆矩阵*，并展示它是如何被用来计算许多重要评估分数的。我们将讨论的第一个分数是准确率。
- en: '* * *'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose we want to classify email into spam (email we in no way want) and
    non-spam (email we want).*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设我们想要将电子邮件分类为垃圾邮件（我们根本不想要的电子邮件）和非垃圾邮件（我们想要的电子邮件）。*'
- en: '* * *'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A ready-to-go example (with a good description) is the “Spambase Data Set” ([http://mng.bz/e8Rh](http://mng.bz/e8Rh)).
    Each row of this dataset is a set of features measured for a specific email and
    an additional column telling whether the mail was spam (unwanted) or non-spam
    (wanted). We’ll quickly build a spam classification model using logistic regression
    so we have results to evaluate. We will discuss logistic regression in [section
    7.2](../Text/07.xhtml#ch07lev1sec2), but for right now you can just download the
    file Spambase/spamD.tsv from the book’s GitHub site ([https://github.com/WinVector/PDSwR2/tree/master/Spambase](https://github.com/WinVector/PDSwR2/tree/master/Spambase))
    and then perform the steps shown in the following listing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现成的示例（附带良好描述）是“Spambase数据集”（[http://mng.bz/e8Rh](http://mng.bz/e8Rh)）。该数据集的每一行都是针对特定电子邮件测量的特征集，以及一个额外的列，说明邮件是否是垃圾邮件（不受欢迎）或非垃圾邮件（受欢迎）。我们将快速构建一个垃圾邮件分类模型，以便我们有结果来评估。我们将在[第7.2节](../Text/07.xhtml#ch07lev1sec2)中讨论逻辑回归，但就目前而言，您可以从本书的GitHub网站（[https://github.com/WinVector/PDSwR2/tree/master/Spambase](https://github.com/WinVector/PDSwR2/tree/master/Spambase)）下载Spambase/spamD.tsv文件，然后执行以下列表中显示的步骤。
- en: Listing 6.1\. Building and applying a logistic regression spam model
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1\. 构建和应用逻辑回归垃圾邮件模型
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Reads in the data
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取数据
- en: ❷ Splits the data into training and test sets
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据分为训练集和测试集
- en: ❸ Creates a formula that describes the model
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个描述模型的公式
- en: ❹ Fits the logistic regression model
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 拟合逻辑回归模型
- en: ❺ Makes predictions on the training and test sets
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在训练集和测试集上做出预测
- en: The spam model predicts the probability that a given email is spam. A sample
    of the results of our simple spam classifier is shown in the next listing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件模型预测给定电子邮件是垃圾邮件的概率。我们简单垃圾邮件分类器的结果样本在下一列表中展示。
- en: Listing 6.2\. Spam classifications
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2\. 垃圾邮件分类
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The first column gives the actual class label (spam or non-spam). The second
    column gives the predicted probability that an email is spam. If the probability
    > 0.5, the email is labeled “spam;” otherwise, it is “non-spam.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一列给出了实际的类别标签（垃圾邮件或非垃圾邮件）。第二列给出了电子邮件是垃圾邮件的预测概率。如果概率大于0.5，则电子邮件被标记为“垃圾邮件”；否则，它被标记为“非垃圾邮件”。
- en: The confusion matrix
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: The absolute most interesting summary of classifier performance is the confusion
    matrix. This matrix is just a table that summarizes the classifier’s predictions
    against the actual known data categories.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器性能最有趣的总结是混淆矩阵。这个矩阵仅仅是一个表格，总结了分类器对实际已知数据类别的预测。
- en: The confusion matrix is a table counting how often each combination of known
    outcomes (the truth) occurred in combination with each prediction type. For our
    email spam example, the confusion matrix is calculated by the R command in the
    following listing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个表格，统计了已知结果（真相）与每种预测类型组合发生的频率。对于我们的电子邮件垃圾邮件示例，混淆矩阵是通过以下列表中的R命令计算的。
- en: Listing 6.3\. Spam confusion matrix
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3\. 垃圾邮件混淆矩阵
- en: '[PRE2]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The rows of the table (labeled *truth*) correspond to the actual labels of
    the datums: whether they are really spam or not. The columns of the table (labeled
    *prediction*) correspond to the predictions that the model makes. So the first
    cell of the table (*truth = non-spam* and *prediction = non-spam*) corresponds
    to the 264 emails in the test set that are not spam, and that the model (correctly)
    predicts are not spam. These correct negative predictions are called *true negatives*.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的行（标记为*真相*）对应于数据点的实际标签：它们是否真的是垃圾邮件。表格的列（标记为*预测*）对应于模型做出的预测。因此，表格的第一个单元格（*真相
    = 非垃圾邮件*和*预测 = 非垃圾邮件*）对应于测试集中不是垃圾邮件的264封电子邮件，并且模型（正确地）预测它们不是垃圾邮件。这些正确的负预测被称为*真正负例*。
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Confusion matrix conventions
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵惯例
- en: A number of tools, as well as Wikipedia, draw confusion matrices with the actual
    truth values controlling the x-axis in the figure. This is likely due to the math
    convention that the first coordinate in matrices and tables names the row (vertical
    offset), and not the column (horizontal offset). It is our feeling that direct
    labels, such as “pred” and “actual,” are much clearer than *any* convention. Also
    note that in residual graphs the prediction is always the x-axis, and being visually
    consistent with this important convention is a benefit. So in this book, we will
    plot predictions on the x-axis (regardless how that is named).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具以及维基百科都会绘制混淆矩阵，实际的真实值控制图中的x轴。这可能是由于矩阵和表格中数学惯例，即矩阵和表格的第一个坐标命名行（垂直偏移），而不是列（水平偏移）。我们觉得直接标签，如“pred”和“actual”，比*任何*惯例都要清晰。此外，请注意，在残差图中，预测总是x轴，与这个重要惯例保持视觉一致性是一个好处。因此，在这本书中，我们将预测绘制在x轴上（无论其名称如何）。
- en: '* * *'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: It is standard terminology to refer to datums that are in the class of interest
    as *positive* instances, and those not in the class of interest as *negative*
    instances. In our scenario, spam emails are positive instances, and non-spam emails
    are negative instances.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将属于感兴趣类别的数据称为*正实例*，而不属于感兴趣类别的数据称为*负实例*是标准术语。在我们的场景中，垃圾邮件是正实例，非垃圾邮件是负实例。
- en: In a two-by-two confusion matrix, every cell has a special name, as illustrated
    in [table 6.2](../Text/06.xhtml#ch06table02).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个二乘二的混淆矩阵中，每个单元格都有一个特殊名称，如[表6.2](../Text/06.xhtml#ch06table02)所示。
- en: Table 6.2\. Two-by-two confusion matrix
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2. 二乘二混淆矩阵
- en: '|   | Prediction=NEGATIVE (predicted as non-spam) | Prediction=POSITIVE (predicted
    as spam) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测=NEGATIVE (预测为非垃圾邮件) | 预测=POSITIVE (预测为垃圾邮件) |'
- en: '| --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Truth mark=NEGATIVE (non-spam)** | True negatives (TN) confmat_spam[1,1]=264
    | False positives (FP) confmat_spam[1,2]=14 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **真实标记=NEGATIVE (非垃圾邮件**) | 真阴性 (TN) confmat_spam[1,1]=264 | 假阳性 (FP) confmat_spam[1,2]=14
    |'
- en: '| **Truth mark=POSITIVE (spam** **)** | False negatives (FN ) confmat_spam[2,1]=22
    | True positives (TP) confmat_spam[2,2]=158 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **真实标记=POSITIVE (垃圾邮件**) | 假阴性 (FN) confmat_spam[2,1]=22 | 真阳性 (TP) confmat_spam[2,2]=158
    |'
- en: Using this summary, we can now start to calculate various performance metrics
    of our spam filter.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个摘要，我们现在可以开始计算垃圾邮件过滤器的各种性能指标。
- en: '* * *'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Changing a score to a classification
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将分数转换为分类
- en: Note that we converted the numerical prediction score into a decision by checking
    if the score was above or below 0.5\. This means that if the model returned a
    probability higher than 50% that an email is spam, we classify it as spam. For
    some scoring models (like logistic regression) the 0.5 score is likely a threshold
    that gives a classifier with reasonably good accuracy. However, accuracy isn’t
    always the end goal, and for unbalanced training data, the 0.5 threshold won’t
    be good. Picking thresholds other than 0.5 can allow the data scientist to trade
    *precision* for *recall* (two terms that we’ll define later in this chapter).
    You can start at 0.5, but consider trying other thresholds and looking at the
    ROC curve (see [section 6.2.5](../Text/06.xhtml#ch06lev2sec9)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将数值预测分数转换为决策，通过检查分数是否高于或低于0.5。这意味着如果模型返回的电子邮件是垃圾邮件的概率高于50%，我们就将其分类为垃圾邮件。对于某些评分模型（如逻辑回归），0.5的分数可能是一个提供合理准确性的分类器阈值。然而，准确率并不总是最终目标，对于不平衡的训练数据，0.5的阈值可能不是好的选择。选择除0.5以外的阈值可以让数据科学家在*精确度*和*召回率*（这两个术语我们将在本章后面定义）之间进行权衡。你可以从0.5开始，但考虑尝试其他阈值并查看ROC曲线（见[第6.2.5节](../Text/06.xhtml#ch06lev2sec9)）。
- en: '* * *'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Accuracy
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率
- en: Accuracy answers the question, “When the spam filter says this email is or is
    not spam, what’s the probability that it’s correct?” For a classifier, accuracy
    is defined as the number of items categorized correctly divided by the total number
    of items. It’s simply what fraction of classifications the classifier makes is
    correct. This is shown in [figure 6.10](../Text/06.xhtml#ch06fig10).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率回答了这样的问题：“当垃圾邮件过滤器说这封邮件是或不是垃圾邮件时，它正确的概率是多少？”对于一个分类器，准确率定义为正确分类的项目数除以总项目数。这很简单，就是分类器做出的分类中正确比例。这如图[6.10图](../Text/06.xhtml#ch06fig10)所示。
- en: Figure 6.10\. Accuracy
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10. 准确率
- en: '![](Images/06fig10.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig10.jpg)'
- en: 'At the very least, you want a classifier to be accurate. Let’s calculate the
    accuracy of the spam filter:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，你希望分类器是准确的。让我们计算垃圾邮件过滤器的准确率：
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The error of around 8% is unacceptably high for a spam filter, but is good for
    illustrating different sorts of model evaluation criteria.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于垃圾邮件过滤器来说，大约8%的错误率是不可接受的，但对于说明不同类型的模型评估标准来说却是好的。
- en: Before we move on, we’d like to share the confusion matrix of a good spam filter.
    In the next listing, we create the confusion matrix for the Akismet comment spam
    filter from the Win-Vector blog.^([[4](../Text/06.xhtml#ch06fn4)])
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们想分享一个良好的垃圾邮件过滤器的混淆矩阵。在下一个列表中，我们创建来自Win-Vector博客的Akismet评论垃圾邮件过滤器的混淆矩阵。[^([4](../Text/06.xhtml#ch06fn4))]
- en: ⁴
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [http://www.win-vector.com/blog/](http://www.win-vector.com/blog/).
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 见[http://www.win-vector.com/blog/](http://www.win-vector.com/blog/)。
- en: Listing 6.4\. Entering the Akismet confusion matrix by hand
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4\. 手动输入Akismet混淆矩阵
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because the Akismet filter uses link destination clues and determination from
    other websites (in addition to text features), it achieves a more acceptable accuracy:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Akismet过滤器使用链接目的地线索和来自其他网站的判断（除了文本特征），它达到了更可接受的精确度：
- en: '[PRE5]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: More importantly, Akismet seems to have suppressed fewer good comments. Our
    next section on precision and recall will help quantify this distinction.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，Akismet似乎抑制了更少的良好评论。我们将在下一节中讨论精确度和召回率，以量化这种区别。
- en: '* * *'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '** * * ***'
- en: Accuracy is an inappropriate measure for unbalanced classes
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不平衡的类别，准确度是一个不合适的衡量标准
- en: Suppose we have a situation where we have a rare event (say, severe complications
    during childbirth). If the event we’re trying to predict is rare (say, around
    1% of the population), the null model that says the rare event never happens is
    *very* (99%) accurate. The null model is in fact more accurate than a useful (but
    not perfect model) that identifies 5% of the population as being “at risk” and
    captures all of the bad events in the 5%. This is not any sort of paradox. It’s
    just that accuracy is not a good measure for events that have unbalanced distribution
    or unbalanced costs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个罕见事件的情况（比如，分娩期间的严重并发症）。如果我们试图预测的事件是罕见的（比如，大约1%的人口），那么说罕见事件永远不会发生的零模型是**非常**（99%）准确的。实际上，零模型比一个有用（但不完美）的模型更准确，后者将5%的人口识别为“有风险”，并捕捉到5%中的所有不良事件。这并不是任何形式的悖论。这只是因为准确性不是衡量不平衡分布或不平衡成本事件的好指标。
- en: '* * *'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '** * * ***'
- en: Precision and recall
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: Another evaluation measure used by machine learning researchers is a pair of
    numbers called precision and recall. These terms come from the field of information
    retrieval and are defined as follows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究人员使用的另一个评估指标是一对称为精确度和召回率的数字。这些术语来自信息检索领域，其定义如下。
- en: '*Precision* answers the question, “If the spam filter says this email is spam,
    what’s the probability that it’s really spam?” Precision is defined as the ratio
    of true positives to predicted positives. This is shown in [figure 6.11](../Text/06.xhtml#ch06fig11).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**回答了这样的问题：“如果垃圾邮件过滤器说这封邮件是垃圾邮件，那么它真的是垃圾邮件的概率是多少？”精确度定义为真实阳性与预测阳性的比率。这如图6.11所示。'
- en: Figure 6.11\. Precision
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11\. 精确度
- en: '![](Images/06fig11.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig11.jpg)'
- en: 'We can calculate the precision of our spam filter as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式计算垃圾邮件过滤器的精确度：
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It is only a coincidence that the precision is so close to the accuracy number
    we reported earlier. Again, precision is how often a positive indication turns
    out to be correct. It’s important to remember that precision is a function of
    the combination of the classifier and the dataset. It doesn’t make sense to ask
    how precise a classifier is in isolation; it’s only sensible to ask how precise
    a classifier is for a given dataset. The hope is that the classifier will be similarly
    precise on the overall population that the dataset is drawn from—a population
    with the same distribution of positives instances as the dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度如此接近我们之前报告的准确度数字只是一个巧合。再次强调，精确度是指阳性指示最终被证明是正确的情况的频率。重要的是要记住，精确度是分类器和数据集组合的函数。单独询问分类器的精确度是没有意义的；只有针对给定的数据集询问分类器的精确度才有意义。希望分类器在数据集抽取的整体人口中也会有类似的精确度——一个具有与数据集相同的阳性实例分布的人口。
- en: In our email spam example, 92% precision means 8% of what was flagged as spam
    was in fact not spam. This is an unacceptable rate for losing possibly important
    messages. Akismet, on the other hand, had a precision of over 99.99%, so it throws
    out very little non-spam email.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的电子邮件垃圾邮件示例中，92%的精确度意味着被标记为垃圾邮件的8%实际上并不是垃圾邮件。这对于丢失可能重要的信息来说是不可接受的比率。另一方面，Akismet的精确度超过99.99%，因此它丢弃的非垃圾邮件非常少。
- en: '[PRE7]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The companion score to precision is *recall*. Recall answers the question, “Of
    all the spam in the email set, what fraction did the spam filter detect?” Recall
    is the ratio of true positives over all actual positives, as shown in [figure
    6.12](../Text/06.xhtml#ch06fig12).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度的伴随分数是 *召回率*。召回率回答了这样的问题：“在电子邮件集中的所有垃圾邮件中，有多少比例被垃圾邮件过滤器检测到了？” 召回率是真实阳性与所有实际阳性的比率，如
    [图 6.12](../Text/06.xhtml#ch06fig12) 所示。
- en: Figure 6.12\. Recall
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12\. 召回率
- en: '![](Images/06fig12_fig12.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig12_fig12.jpg)'
- en: Let’s compare the recall of the two spam filters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较两个垃圾邮件过滤器的召回率。
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For our email spam filter, this is 88%, which means about 12% of the spam email
    we receive will still make it into our inbox. Akismet has a recall of 99.88%.
    In both cases, most spam is in fact tagged (we have high recall) and precision
    is emphasized over recall. This is appropriate for a spam filter, because it’s
    more important to not lose non-spam email than it is to filter every single piece
    of spam out of our inbox.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的电子邮件垃圾邮件过滤器，这个值是 88%，这意味着我们收到的垃圾邮件中仍有大约 12% 会进入我们的收件箱。Akismet 的召回率为 99.88%。在这两种情况下，大多数垃圾邮件实际上都被标记了（我们有高召回率），并且精确度比召回率更重要。这对于垃圾邮件过滤器来说是合适的，因为不丢失非垃圾邮件比从我们的收件箱中过滤掉每一件垃圾邮件更重要。
- en: 'It’s important to remember this: precision is a measure of confirmation (when
    the classifier indicates positive, how often it is in fact correct), and recall
    is a measure of utility (how much the classifier finds of what there actually
    is to find). Precision and recall tend to be relevant to business needs and are
    good measures to discuss with your project sponsor and client.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住这一点：精确度是确认度的度量（当分类器指示为正时，实际上它是正确的频率），召回率是实用性的度量（分类器找到了多少实际存在的可找内容）。精确度和召回率通常与商业需求相关，并且是与你的项目赞助商和客户讨论的好指标。
- en: F1
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: F1
- en: '* * *'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose that you had multiple spam filters to choose from, each with different
    values of precision and recall. How do you pick which spam filter to use?*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你有多个垃圾邮件过滤器可供选择，每个过滤器都有不同的精确度和召回率。你该如何选择使用哪个垃圾邮件过滤器呢？*'
- en: '* * *'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In situations like this, some people prefer to have just one number to compare
    all the different choices by. One such score is the *F1 score*. The F1 score measures
    a trade-off between precision and recall. It is defined as the harmonic mean of
    the precision and recall. This is most easily shown with an explicit calculation:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有些人更喜欢有一个数字来比较所有不同的选择。这样一个评分就是 *F1 分数*。F1 分数衡量了精确度和召回率之间的权衡。它被定义为精确度和召回率的调和平均值。这可以通过一个明确的计算来最容易地展示：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our spam filter with 0.93 precision and 0.88 recall has an F1 score of 0.90\.
    F1 is 1.00 when a classifier has perfect precision and recall, and goes to 0.00
    for classifiers that have either very low precision or recall (or both). Suppose
    you think that your spam filter is losing too much real email, and you want to
    make it “pickier” about marking email as spam; that is, you want to increase its
    precision. Quite often, increasing the precision of a classifier will also lower
    its recall: in this case, a pickier spam filter may also mark fewer real spam
    emails as spam, and allow it into your inbox. If the filter’s recall falls too
    low as its precision increases, this will result in a lower F1\. This possibly
    means that you have traded too much recall for better precision.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们具有 0.93 精确度和 0.88 召回率的垃圾邮件过滤器有一个 0.90 的 F1 分数。当分类器具有完美的精确度和召回率时，F1 为 1.00，而对于具有非常低精确度或召回率（或两者都有）的分类器，F1
    会降到 0.00。假设你认为你的垃圾邮件过滤器丢失了太多的真实邮件，并且你想要让它“更挑剔”地标记邮件为垃圾邮件；也就是说，你想要提高它的精确度。通常情况下，提高分类器的精确度也会降低其召回率：在这种情况下，一个更挑剔的垃圾邮件过滤器可能会标记更少的真实垃圾邮件为垃圾邮件，并允许它进入你的收件箱。如果随着精确度的提高，过滤器的召回率变得太低，这将导致
    F1 分数降低。这可能意味着你为了更好的精确度而牺牲了太多的召回率。
- en: Sensitivity and specificity
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感度和特异性
- en: '* * *'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose that you have successfully trained a spam filter with acceptable precision
    and recall, using your work email as training data. Now you want to use that same
    spam filter on a personal email account that you use primarily for your photography
    hobby. Will the filter work as well?*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你已经使用你的工作电子邮件作为训练数据，成功训练了一个具有可接受精确度和召回率的垃圾邮件过滤器。现在你想要将同一个垃圾邮件过滤器应用于你主要用于摄影爱好者的个人电子邮件账户。这个过滤器会同样有效吗？*'
- en: '* * *'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: It’s possible the filter will work just fine on your personal email as is, since
    the nature of spam (the length of the email, the words used, the number of links,
    and so on) probably doesn’t change much between the two email accounts. However,
    the *proportion* of spam you get on the personal email account may be different
    than it is on your work email. This can change the performance of the spam filter
    on your personal email.^([[5](../Text/06.xhtml#ch06fn5)])
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器可能在你个人的电子邮件上工作得很好，因为垃圾邮件的性质（电子邮件的长度、使用的单词、链接的数量等）在这两个电子邮件账户之间可能变化不大。然而，你个人电子邮件账户上收到的垃圾邮件的比例可能与工作电子邮件上的不同。这可能会改变你个人电子邮件上的垃圾邮件过滤器的性能。[^5](../Text/06.xhtml#ch06fn5)
- en: ⁵
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The spam filter performance can also change because the nature of the non-spam
    will be different, too: the words commonly used will be different; the number
    of links or images in a legitimate email may be different; the email domains of
    people you correspond with may be different. For this discussion, we will assume
    that the proportion of spam email is the main reason that a spam filter’s performance
    will be different.'
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤器的性能也可能发生变化，因为非垃圾邮件的性质也会不同：常用的单词可能不同；合法电子邮件中的链接或图像数量可能不同；你与之通信的人的电子邮件域名可能不同。对于这次讨论，我们将假设垃圾邮件的比例是垃圾邮件过滤器性能差异的主要原因。
- en: Let’s see how changes in the proportion of spam can change the performance metrics
    of the spam filter. Here we simulate having email sets with both higher and lower
    proportions of email than the data that we trained the filter on.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看垃圾邮件比例的变化如何改变垃圾邮件过滤器的性能指标。在这里，我们模拟了具有比我们训练过滤器时更多的和更少的电子邮件集。
- en: Listing 6.5\. Seeing filter performance change when spam proportions change
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5. 观察垃圾邮件比例变化时过滤器性能的变化
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Pulls 100 emails out of the test set at random
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从测试集中随机抽取100封电子邮件
- en: ❷ A convenience function to print out the confusion matrix, precision, and recall
    of the filter on a test set.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是一个方便的函数，用于在测试集上打印出过滤器的混淆矩阵、精确度和召回率。
- en: ❸ Looks at performance on a test set with the same proportion of spam as the
    training data
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查与训练数据中相同比例的垃圾邮件的测试集上的性能
- en: ❹ Adds back only additional spam, so the test set has a higher proportion of
    spam than the training set
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 仅将额外的垃圾邮件添加回来，因此测试集中垃圾邮件的比例高于训练集
- en: ❺ Adds back only non-spam, so the test set has a lower proportion of spam than
    the training set
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 仅将非垃圾邮件添加回来，因此测试集中垃圾邮件的比例低于训练集
- en: 'Note that the recall of the filter is the same in all three cases: about 88%.
    When the data has more spam than the filter was trained on, the filter has higher
    precision, which means it throws a lower proportion of non-spam email out. This
    is good! However, when the data has less spam than the filter was trained on,
    the precision is lower, meaning the filter will throw out a higher fraction of
    non-spam email. This is undesirable.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，过滤器的召回率在这三种情况下都是相同的：大约88%。当数据中的垃圾邮件数量多于过滤器训练的数据时，过滤器的精确度更高，这意味着它抛出的非垃圾邮件比例更低。这是好事！然而，当数据中的垃圾邮件数量少于过滤器训练的数据时，精确度较低，这意味着过滤器会抛出更高比例的非垃圾邮件。这是不希望的。
- en: Because there are situations where a classifier or filter may be used on populations
    where the prevalence of the positive class (in this example, spam) varies, it’s
    useful to have performance metrics that are independent of the class prevalence.
    One such pair of metrics is *sensitivity* and *specificity*. This pair of metrics
    is common in medical research, because tests for diseases and other conditions
    will be used on different populations, with different prevalence of a given disease
    or condition.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在一些情况，分类器或过滤器可能被用于正类（在本例中为垃圾邮件）的普遍性不同的群体中，因此拥有独立于类别普遍性的性能指标是有用的。这样一对指标是*灵敏度*和*特异性*。这对指标在医学研究中很常见，因为疾病和其他条件的测试将用于不同的群体，这些群体中特定疾病或条件的普遍性不同。
- en: '*Sensitivity* is also called the *true positive rate* and is exactly equal
    to recall. *Specificity* is also called the *true negative rate*: it is the ratio
    of true negatives to all negatives. This is shown in [figure 6.13](../Text/06.xhtml#ch06fig13).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*灵敏度*也称为*真正阳性率*，与召回率完全相同。*特异性*也称为*真正阴性率*：它是真正阴性数与所有阴性数的比率。这如图6.13所示。'
- en: Figure 6.13\. Specificity
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13. 特异性
- en: '![](Images/06fig13.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig13.jpg)'
- en: Sensitivity and recall answer the question, “What fraction of spam does the
    spam filter find?” Specificity answers the question, “What fraction of non-spam
    does the spam filter find?”
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性和召回率回答了“垃圾邮件过滤器找到了多少比例的垃圾邮件？”的问题。特异性回答了“垃圾邮件过滤器找到了多少比例的非垃圾邮件？”的问题。
- en: 'We can calculate specificity for our spam filter:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算我们的垃圾邮件过滤器的特异性：
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: One minus the specificity is also called the *false positive rate*. False positive
    rate answers the question, “What fraction of non-spam will the model classify
    as spam?” You want the false positive rate to be low (or the specificity to be
    high), and the sensitivity to also be high. Our spam filter has a specificity
    of about 0.95, which means that it will mark about 5% of non-spam email as spam.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性减一也称为*假阳性率*。假阳性率回答了“模型将多少比例的非垃圾邮件分类为垃圾邮件？”的问题。您希望假阳性率低（或特异性高），同时敏感性也要高。我们的垃圾邮件过滤器的特异性约为0.95，这意味着它将大约5%的非垃圾邮件标记为垃圾邮件。
- en: 'An important property of sensitivity and specificity is this: if you flip your
    labels (switch from *spam* being the class you’re trying to identify to *non-spam*
    being the class you’re trying to identify), you just switch sensitivity and specificity.
    Also, a trivial classifier that always says positive or always says negative will
    always return a zero score on either sensitivity or specificity. So useless classifiers
    always score poorly on at least one of these measures.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性和特异性的一个重要特性是：如果您翻转标签（从试图识别的类别*垃圾邮件*切换到试图识别的类别*非垃圾邮件*），您只是交换了敏感性和特异性。此外，一个总是说正面或总是说负面的简单分类器在敏感性和特异性上都会得到零分。因此，无用的分类器在这至少一个指标上得分总是很差。
- en: Why have both precision/recall and sensitivity/specificity? Historically, these
    measures come from different fields, but each has advantages. Sensitivity/specificity
    is good for fields, like medicine, where it’s important to have an idea how well
    a classifier, test, or filter separates positive from negative instances independently
    of the distribution of the different classes in the population. But precision/recall
    gives you an idea how well a classifier or filter will work on a specific population.
    If you want to know the probability that an email identified as spam is really
    spam, you have to know how common spam is in that person’s email box, and the
    appropriate measure is precision.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么需要精确率/召回率和敏感度/特异性？从历史上看，这些指标来自不同的领域，但每个都有其优点。敏感度/特异性适用于像医学这样的领域，在这些领域中，了解分类器、测试或过滤器如何独立于人口中不同类别的分布将正例与负例分开是很重要的。但精确率/召回率可以给您一个关于分类器或过滤器在特定人群上如何工作的概念。如果您想知道被识别为垃圾邮件的电子邮件实际上是垃圾邮件的概率，您必须知道垃圾邮件在该人的电子邮件收件箱中有多常见，适当的指标是精确率。
- en: 'Summary: Using common classification performance measures'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：使用常见的分类性能指标
- en: You should use these standard scores while working with your client and sponsor
    to see which measure most models their business needs. For each score, you should
    ask them if they need that score to be high, and then run a quick thought experiment
    with them to confirm you’ve gotten their business need. You should then be able
    to write a project goal in terms of a minimum bound on a pair of these measures.
    [Table 6.3](../Text/06.xhtml#ch06table03) shows a typical business need and an
    example follow-up question for each measure.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在与客户和赞助商合作时，您应使用这些标准分数，以查看哪些指标最符合他们的业务需求。对于每个分数，您应询问他们是否需要该分数较高，然后与他们进行快速的思想实验，以确认您已经理解了他们的业务需求。然后您应该能够用这些指标中的一对的最小界限来撰写项目目标。[表6.3](../Text/06.xhtml#ch06table03)展示了典型的业务需求和每个指标的示例后续问题。
- en: Table 6.3\. Classifier performance measures business stories.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3\. 分类器性能度量业务故事。
- en: '| Measure | Typical business need | Follow-up question |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 典型的业务需求 | 后续问题 |'
- en: '| --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | “We need most of our decisions to be correct.” | “Can we tolerate
    being wrong 5% of the time? And do users see mistakes like spam marked as non-spam
    or non-spam marked as spam as being equivalent?” |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | “我们需要大多数决策都是正确的。” | “我们能否容忍5%的错误率？并且用户是否将垃圾邮件被标记为非垃圾邮件或非垃圾邮件被标记为垃圾邮件视为等效？”
    |'
- en: '| Precision | “Most of what we marked as spam had darn well better be spam.”
    | “That would guarantee that most of what is in the spam folder is in fact spam,
    but it isn’t the best way to measure what fraction of the user’s legitimate email
    is lost. We could cheat on this goal by sending all our users a bunch of easy-to-identify
    spam that we correctly identify. Maybe we really want good specificity.” |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | “我们标记为垃圾邮件的大部分内容确实应该是垃圾邮件。” | “这将保证大多数在垃圾邮件文件夹中的内容确实是垃圾邮件，但这并不是衡量用户丢失多少合法邮件的最佳方式。我们可以通过向所有用户发送大量易于识别的垃圾邮件来正确识别，从而在这个目标上作弊。也许我们真正想要的是良好的特异性。”
    |'
- en: '| Recall | “We want to cut down on the amount of spam a user sees by a factor
    of 10 (eliminate 90% of the spam).” | “If 10% of the spam gets through, will the
    user see mostly non-spam mail or mostly spam? Will this result in a good user
    experience?” |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | “我们希望将用户看到的垃圾邮件数量减少10倍（消除90%的垃圾邮件）。” | “如果10%的垃圾邮件得以通过，用户将看到主要是非垃圾邮件还是主要是垃圾邮件？这将导致良好的用户体验吗？”
    |'
- en: '| Sensitivity | “We have to cut a lot of spam; otherwise, the user won’t see
    a benefit.” | “If we cut spam down to 1% of what it is now, would that be a good
    user experience?” |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 敏感性 | “我们必须削减大量的垃圾邮件；否则，用户将看不到任何好处。” | “如果我们把垃圾邮件削减到现在的1%，这将是一个好的用户体验吗？”
    |'
- en: '| Specificity | “We must be at least *three nines* on legitimate email; the
    user must see at least 99.9% of their non-spam email.” | “Will the user tolerate
    missing 0.1% of their legitimate email, and should we keep a spam folder the user
    can look at?” |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 特异性 | “我们必须至少达到*三个九*的合法电子邮件；用户必须至少看到99.9%的非垃圾邮件。” | “用户能否容忍错过0.1%的合法邮件，我们应该保留一个用户可以查看的垃圾邮件文件夹吗？”
    |'
- en: One conclusion for this dialogue process on spam classification could be to
    recommend writing the business goals as maximizing sensitivity while maintaining
    a specificity of at least 0.999.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于垃圾邮件分类的对话过程，一个结论可能是建议将业务目标写为最大化敏感性，同时保持至少0.999的特异性。
- en: 6.2.4\. Evaluating scoring models
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4\. 评估评分模型
- en: Let’s demonstrate evaluation on a simple example.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个简单的例子中演示评估。
- en: '* * *'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you’ve read that the rate at which crickets chirp is proportional
    to the temperature, so you have gathered some data and fit a model that predicts
    temperature (in Fahrenheit) from the chirp rate (chirps/sec) of a striped ground
    cricket. Now you want to evaluate this model.*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你已经读到，蟋蟀鸣叫的速率与温度成正比，因此你已经收集了一些数据并拟合了一个模型，该模型可以从条纹地面蟋蟀的鸣叫速率（每秒鸣叫次数）预测温度（华氏度）。现在你想要评估这个模型。*'
- en: '* * *'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: You can fit a linear regression model to this data, and then make predictions,
    using the following listing. We will discuss linear regression in detail in [chapter
    8](../Text/08.xhtml#ch08). Make sure you have the dataset crickets.csv in your
    working directory.^([[6](../Text/06.xhtml#ch06fn6)])
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下列表拟合一个线性回归模型，并进行预测。我们将在第8章详细讨论线性回归。确保你的工作目录中有crickets.csv数据集。[^6](../Text/06.xhtml#ch06fn6)]
- en: ⁶
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'George W. Pierce, *The Song of Insects*, Harvard University Press, 1948\. You
    can find the dataset here: [https://github.com/WinVector/PDSwR2/tree/master/cricketchirps](https://github.com/WinVector/PDSwR2/tree/master/cricketchirps)'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 乔治·W·皮尔斯，《昆虫之歌》，哈佛大学出版社，1948年。你可以在这里找到数据集：[https://github.com/WinVector/PDSwR2/tree/master/cricketchirps](https://github.com/WinVector/PDSwR2/tree/master/cricketchirps)
- en: Listing 6.6\. Fitting the cricket model and making predictions
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6\. 拟合蟋蟀模型并进行预测
- en: '[PRE12]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 6.14](../Text/06.xhtml#ch06fig14) compares the actual data (points)
    to the model’s predictions (the line). The differences between the predictions
    of `temperatureF` and `temp_pred` are called the *residuals* or *error* of the
    model on the data. We will use the residuals to calculate some common performance
    metrics for scoring models.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.14](../Text/06.xhtml#ch06fig14)比较了实际数据（点）与模型的预测（线）。预测的`temperatureF`和`temp_pred`之间的差异被称为模型的*残差*或*误差*。我们将使用残差来计算评分模型的常见性能指标。'
- en: Figure 6.14\. Scoring residuals
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14\. 评分残差
- en: '![](Images/06fig14_alt.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig14_alt.jpg)'
- en: Root mean square error
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根均方误差
- en: The most common goodness-of-fit measure is called *root mean square error (RMSE)*.
    The RMSE is the square root of the average squared residuals (also called the
    mean squared error). RMSE answers the question, “How much is the predicted temperature
    typically off?” We calculate the RMSE as shown in the following listing.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的拟合优度度量称为*均方根误差（RMSE）*。RMSE是平均平方残差的平方根（也称为均方误差）。RMSE回答了“预测温度通常偏离多少？”这个问题。我们按照以下列表所示计算RMSE。
- en: Listing 6.7\. Calculating RMSE
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7\. 计算RMSE
- en: '[PRE13]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The RMSE is in the same units as the outcome: since the outcome (temperature)
    is in degrees Fahrenheit, the RMSE is also in degrees Fahrenheit. Here the RMSE
    tells you that the model’s predictions will typically (that is, on average) be
    about 3.6 degrees off from the actual temperature. Suppose that you consider a
    model that typically predicts the temperature to within 5 degrees to be “good.”
    Then, congratulations! You have fit a model that meets your goals.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE与结果单位相同：由于结果（温度）是以华氏度为单位，因此RMSE也是以华氏度为单位。在这里，RMSE告诉你模型的预测通常（即平均）会偏离实际温度大约3.6度。假设你认为一个通常能将温度预测在5度以内的模型是“好的”，那么恭喜你！你已经拟合了一个符合你目标的模型。
- en: RMSE is a good measure, because it is often what the fitting algorithms you’re
    using are explicitly trying to minimize. In a business setting, a good RMSE-related
    goal would be “We want the RMSE on account valuation to be under $1,000 per account.”
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE是一个好的度量，因为它通常是你在使用的拟合算法明确试图最小化的。在商业环境中，一个好的RMSE相关目标可能是“我们希望账户估值的RMSE低于每账户1000美元。”
- en: The quantity `mean(error_sq)` is called the *mean squared error*. We will call
    the quantity `sum(error_sq)` the *sum squared error*, and also refer to it as
    the model’s *variance*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 量`mean(error_sq)`被称为*均方误差*。我们将量`sum(error_sq)`称为*均方误差之和*，也将其称为模型的*方差*。
- en: R-squared
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: R平方
- en: Another important measure of fit is called *R-squared* (or *R2*, or the *coefficient
    of determination*). We can motivate the definition of R-squared as follows.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的拟合度度量称为*R平方*（或R2，或确定系数）。我们可以如下推导R平方的定义。
- en: For the data that you’ve collected, the simplest baseline prediction of the
    temperature is simply the average temperature in the dataset. This is the *null
    model*; it’s not a very good model, but you have to perform at least better than
    it does. The data’s *total variance* is the sum squared error of the null model.
    You want the sum squared error of your actual model to be much smaller than the
    data’s variance—that is, you want the ratio of your model’s sum squared error
    to the total variance to be near zero. R-squared is defined as one minus this
    ratio, so we want R-squared to be close to one. This leads to the following calculation
    for R-squared.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你收集的数据，温度的最简单基线预测只是数据集中的平均温度。这是一个*零模型*；它不是一个很好的模型，但你必须至少做得比它好。数据的*总方差*是零模型的均方误差之和。你希望你的实际模型的均方误差之和远小于数据的方差——也就是说，你希望你的模型均方误差与总方差之比接近零。R平方定义为这个比率的倒数，因此我们希望R平方接近1。这导致了以下R平方的计算。
- en: Listing 6.8\. Calculating R-squared
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8\. 计算R平方
- en: '[PRE14]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Calculates the squared error terms
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算平方误差项
- en: ❷ Sums them to get the model’s sum squared error, or variance
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将它们相加得到模型的均方误差，或方差
- en: ❸ Calculates the squared error terms from the null model
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从零模型计算平方误差项
- en: ❹ Calculates the data’s total variance
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算数据的总方差
- en: ❺ Calculates R-squared
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算R平方
- en: As R-squared is formed from a ratio comparing your model’s variance to the total
    variance, you can think of R-squared as a measure of how much variance your model
    “explains.” R-squared is also sometimes referred to as a measure of how well the
    model “fits” the data, or its “goodness of fit.”
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于R平方是由一个比率构成的，该比率比较了你的模型方差与总方差，因此你可以将R平方视为衡量你的模型“解释”了多少方差的一个指标。R平方有时也被称为衡量模型“拟合”数据程度或其“拟合优度”的指标。
- en: The best possible R-squared is 1.0, with near-zero or negative R-squareds being
    horrible. Some other models (such as logistic regression) use deviance to report
    an analogous quantity called *pseudo R-squared*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳可能的R平方是1.0，接近零或负的R平方是糟糕的。一些其他模型（如逻辑回归）使用偏差来报告一个类似数量的指标，称为*伪R平方*。
- en: Under certain circumstances, R-squared is equal to the square of another measure
    called the *correlation* (see [http://mng.bz/ndYf](http://mng.bz/ndYf)). A good
    statement of a R-squared business goal would be “We want the model to explain
    at least 70% of variation in account value.”
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，R-squared 等于另一个称为 *相关系数* 的度量（见[http://mng.bz/ndYf](http://mng.bz/ndYf)）。一个关于
    R-squared 商业目标的良好陈述将是：“我们希望模型至少解释账户价值变化的70%。”
- en: 6.2.5\. Evaluating probability models
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.5\. 评估概率模型
- en: Probability models are models that both decide if an item is in a given class
    and return an estimated probability (or confidence) of the item being in the class.
    The modeling techniques of logistic regression and decision trees are fairly famous
    for being able to return good probability estimates. Such models can be evaluated
    on their final decisions, as we’ve already shown in [section 6.2.3](../Text/06.xhtml#ch06lev2sec7),
    but they can also be evaluated in terms of their estimated probabilities.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 概率模型是既能判断一个项目是否属于给定类别，又能返回该项目属于该类别的估计概率（或置信度）的模型。逻辑回归和决策树建模技术在返回良好的概率估计方面相当著名。这些模型可以根据它们的最终决策进行评估，正如我们在[第6.2.3节](../Text/06.xhtml#ch06lev2sec7)中已经展示的那样，但它们也可以根据它们的估计概率进行评估。
- en: In our opinion, most of the measures for probability models are very technical
    and very good at comparing the qualities of different models on the same dataset.
    It’s important to know them, because data scientists generally use these criteria
    among themselves. But these criteria aren’t easy to precisely translate into businesses
    needs. So we recommend tracking them, but not using them with your project sponsor
    or client.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看来，大多数概率模型的度量都非常技术化，并且非常擅长比较同一数据集上不同模型的品质。了解它们很重要，因为数据科学家通常在这些标准之间进行交流。但将这些标准精确地转化为业务需求并不容易。因此，我们建议跟踪它们，但不要在与你的项目赞助商或客户交流时使用它们。
- en: To motivate the use of the different metrics for probability models, we’ll continue
    the spam filter example from [section 6.2.3](../Text/06.xhtml#ch06lev2sec7).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激励使用概率模型的不同指标，我们将继续从[第6.2.3节](../Text/06.xhtml#ch06lev2sec7)中的垃圾邮件过滤器示例。
- en: '* * *'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose that, while building your spam filter, you try several different algorithms
    and modeling approaches and come up with several models, all of which return the
    probability that a given email is spam. You want to compare these different models
    quickly and identify the one that will make the best spam filter.*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设在构建你的垃圾邮件过滤器时，你尝试了多种不同的算法和建模方法，并提出了几个模型，所有这些模型都返回了给定电子邮件是垃圾邮件的概率。你想要快速比较这些不同的模型，并确定哪一个将制作出最佳的垃圾邮件过滤器。*'
- en: '* * *'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In order to turn a probability model into a classifier, you need to select
    a threshold: items that score higher than that threshold will be classified as
    spam; otherwise, they are classified as non-spam. The easiest (and probably the
    most common) threshold for a probability model is 0.5, but the “best possible”
    classifier for a given probability model may require a different threshold. This
    optimal threshold can vary from model to model. The metrics in this section compare
    probability models directly, without having turned them into classifiers. If you
    make the reasonable assumption that the best probability model will make the best
    classifier, then you can use these metrics to quickly select the most appropriate
    probability model, and then spend some time tuning the threshold to build the
    best classifier for your needs.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将概率模型转换为分类器，你需要选择一个阈值：得分高于该阈值的项将被分类为垃圾邮件；否则，它们将被分类为非垃圾邮件。对于概率模型来说，最简单（也可能是最常见的）的阈值是0.5，但针对特定概率模型的“最佳”分类器可能需要不同的阈值。这个最佳阈值可能因模型而异。本节中的指标直接比较概率模型，而没有将它们转换为分类器。如果你合理地假设最佳的概率模型将产生最佳的分类器，那么你可以使用这些指标来快速选择最合适的概率模型，然后花一些时间调整阈值，以构建满足你需求的最佳分类器。
- en: The double density plot
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 双密度图
- en: When thinking about probability models, it’s useful to construct a double density
    plot (illustrated in [figure 6.15](../Text/06.xhtml#ch06fig15)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑概率模型时，构建双密度图（如图6.15所示）是有用的。
- en: Listing 6.9\. Making a double density plot
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9\. 制作双密度图
- en: '[PRE15]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Figure 6.15\. Distribution of scores broken up by known classes
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15\. 按已知类别拆分的分数分布
- en: '![](Images/06fig15_alt.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig15_alt.jpg)'
- en: 'The x-axis in the figure corresponds to the prediction scores returned by the
    spam filter. [Figure 6.15](../Text/06.xhtml#ch06fig15) illustrates what we’re
    going to try to check when evaluating estimated probability models: examples in
    the class should mostly have high scores, and examples not in the class should
    mostly have low scores.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的x轴对应于垃圾邮件过滤器返回的预测分数。[图6.15](../Text/06.xhtml#ch06fig15)展示了我们在评估估计概率模型时试图检查的内容：属于该类别的示例应该主要具有高分数，而不属于该类别的示例应该主要具有低分数。
- en: Double density plots can be useful when picking classifier thresholds, or the
    threshold score where the classifier switches from labeling an email as non-spam
    to spam. As we mentioned earlier, the standard classifier threshold is 0.5, meaning
    that if the probability that an email is spam is greater than one-half, then we
    label the email as spam. This is the threshold that you used in [section 6.2.3](../Text/06.xhtml#ch06lev2sec7).
    However, in some circumstances you may choose to use a different threshold. For
    instance, using a threshold of 0.75 for the spam filter will produce a classifier
    with higher precision (but lower recall), because a higher fraction of emails
    that scored higher than 0.75 are actually spam.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 双密度图在选择分类器阈值时可能很有用，或者分类器从将电子邮件标记为非垃圾邮件切换到垃圾邮件的阈值分数。如我们之前提到的，标准分类器阈值是0.5，这意味着如果电子邮件是垃圾邮件的概率大于一半，我们就将其标记为垃圾邮件。这是你在[第6.2.3节](../Text/06.xhtml#ch06lev2sec7)中使用的阈值。然而，在某些情况下，你可能选择使用不同的阈值。例如，使用垃圾邮件过滤器的0.75阈值将产生一个具有更高精确度（但召回率较低）的分类器，因为分数高于0.75的电子邮件中有更高的比例实际上是垃圾邮件。
- en: The receiver operating characteristic curve and the AUC
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线和AUC
- en: The *receiver operating characteristic curve* (or *ROC curve*) is a popular
    alternative to the double density plot. For each different classifier we’d get
    by picking a different score threshold between spam and not-spam, we plot both
    the true positive (TP) rate and the false positive (FP) rate. The resulting curve
    represents every possible trade-off between true positive rate and false positive
    rate that is available for classifiers derived from this model. [Figure 6.16](../Text/06.xhtml#ch06fig16)
    shows the ROC curve for our spam filter, as produced in the next listing. In the
    last line of the listing, we compute the *AUC* or *area under the curve*, which
    is another measure of the quality of the model.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*接收者操作特征曲线*（或*ROC曲线*）是双密度图的一个流行替代方案。对于通过在垃圾邮件和非垃圾邮件之间选择不同的分数阈值得到的每个不同的分类器，我们绘制了真正的阳性（TP）率和错误的阳性（FP）率。得到的曲线代表了从该模型派生出的分类器中可用的每个可能的真正阳性率和错误阳性率之间的权衡。[图6.16](../Text/06.xhtml#ch06fig16)显示了我们的垃圾邮件过滤器的ROC曲线，如下一列表所示。在列表的最后一行，我们计算了*AUC*或*曲线下的面积*，这是对模型质量的一种衡量。'
- en: Figure 6.16\. ROC curve for the email spam example
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16\. 电子邮件垃圾邮件示例的ROC曲线
- en: '![](Images/06fig16_alt.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig16_alt.jpg)'
- en: Listing 6.10\. Plotting the receiver operating characteristic curve
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10\. 绘制接收者操作特征曲线
- en: '[PRE16]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Plots the receiver operating characteristic (ROC) curve
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绘制接收者操作特征（ROC）曲线
- en: ❷ Calculates the area under the ROC curve explicitly
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显式计算ROC曲线下的面积
- en: The reasoning behind the AUC
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: AUC背后的推理
- en: 'At one end of the spectrum of models is the ideal perfect model that would
    return a score of 1 for spam emails and a score of 0 for non-spam. This ideal
    model would form an ROC with three points:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型光谱的一端是理想的完美模型，该模型将为垃圾邮件返回1分，为非垃圾邮件返回0分。这个理想模型将形成一个有三个点的ROC曲线：
- en: '(0,0)—Corresponding to a classifier defined by the threshold `p = 1`: nothing
    gets classified as spam, so this classifier has a zero false positive rate and
    a zero true positive rate.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (0,0)—对应于由阈值`p = 1`定义的分类器：没有任何内容被分类为垃圾邮件，因此这个分类器的错误阳性率和真正阳性率都为0。
- en: '(1,1)—Corresponding to a classifier defined by the threshold `p = 0`: everything
    gets classified as spam, so this classifier has a false positive rate of 1 and
    a true positive rate of 1.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1,1)—对应于由阈值`p = 0`定义的分类器：所有内容都被分类为垃圾邮件，因此这个分类器的错误阳性率和真正阳性率都为1。
- en: '(0,1)—Corresponding to any classifier defined by a threshold between 0 and
    1: everything is classified correctly, so this classifier has a false positive
    rate of 0 and a true positive rate of 1.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (0,1)—对应于由0到1之间的阈值定义的任何分类器：所有内容都被正确分类，因此这个分类器的错误阳性率为0，真正阳性率为1。
- en: 'The shape of the ROC for the ideal model is shown in [figure 6.17](../Text/06.xhtml#ch06fig17).
    The area under the curve for this model is 1\. A model that returns random scores
    would have an ROC that is the diagonal line from the origin to the point (1,0):
    the true positive rate is proportional to the threshold. The area under the curve
    for the random model is 0.5\. So you want a model whose AUC is close to 1, and
    greater than 0.5.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 理想模型的ROC曲线形状如图6.17[所示](../Text/06.xhtml#ch06fig17)。此模型的曲线下面积为1。返回随机分数的模型将有一个从原点到点(1,0)的对角线ROC：真正的阳性率与阈值成正比。随机模型的曲线下面积为0.5。因此，你希望模型具有接近1的AUC，并且大于0.5。
- en: Figure 6.17\. ROC curve for an ideal model that classifies perfectly
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17\. 完美分类的理想模型的ROC曲线
- en: '![](Images/06fig17_alt.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17_替代](Images/06fig17_alt.jpg)'
- en: When comparing multiple probability models, you generally want to prefer models
    that have a higher AUC. However, you also want to examine the shape of the ROC
    to explore possible project goal trade-offs. Each point on the curve shows the
    trade-off between achievable true positive and false positive rates with this
    model. If you share the information from the ROC curve with your client, they
    may have an opinion about the acceptable trade-offs between the two.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较多个概率模型时，你通常希望优先考虑具有更高AUC的模型。然而，你还需要检查ROC曲线的形状，以探索可能的项目目标权衡。曲线上的每个点都显示了使用此模型可实现的真正阳性率和假阳性率之间的权衡。如果你与客户分享ROC曲线的信息，他们可能会对两种可接受的权衡有意见。
- en: Log likelihood
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然
- en: '*Log likelihood* is a measure of how well the model’s predictions “match” the
    true class labels. It is a non-positive number, where a log likelihood of 0 means
    a perfect match: the model scores all the spam as being spam with a probability
    of 1, and all the non-spam as having a probability 0 of being spam. The larger
    the magnitude of the log likelihood, the worse the match.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*对数似然*是衡量模型预测“匹配”真实类别标签好坏的一个指标。它是一个非正数，其中对数似然为0表示完美匹配：模型将所有垃圾邮件评分的概率为1，将所有非垃圾邮件评分的概率为0。对数似然的绝对值越大，匹配越差。'
- en: The log likelihood of a model’s prediction on a specific instance is the logarithm
    of the probability that the model assigns to the instance’s actual class. As shown
    in [figure 6.18](../Text/06.xhtml#ch06fig18), for a spam email with an estimated
    probability of *p* of being spam, the log likelihood is `log(p)`; for a non-spam
    email, the same score of *p* gives a log likelihood of `log(1 - p)`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对特定实例预测的对数似然是该模型分配给实例实际类别的概率的对数。如图6.18[所示](../Text/06.xhtml#ch06fig18)，对于一个估计概率为*p*的垃圾邮件，其对数似然是`log(p)`；对于非垃圾邮件，相同的*p*分数给出对数似然为`log(1
    - p)`。
- en: Figure 6.18\. Log likelihood of a spam filter prediction
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18\. 垃圾邮件过滤器预测的对数似然
- en: '![](Images/06fig18.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18](Images/06fig18.jpg)'
- en: 'The log likelihood of a model’s predictions on an entire dataset is the sum
    of the individual log likelihoods:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对整个数据集预测的对数似然是各个单独对数似然的和：
- en: '[PRE17]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here `y` is the true class label (0 for non-spam and 1 for spam) and `py` is
    the probability that an instance is of class 1 (spam). We are using multiplication
    to select the correct logarithm. We also use the convention that `0 * log(0) =
    0` (though for simplicity, this isn’t shown in the code).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`y`是真实的类别标签（非垃圾邮件为0，垃圾邮件为1），`py`是实例属于类别1（垃圾邮件）的概率。我们使用乘法来选择正确的对数。我们还使用约定`0
    * log(0) = 0`（尽管为了简单起见，这在代码中没有显示）。
- en: '[Figure 6.19](../Text/06.xhtml#ch06fig19) shows how log likelihood rewards
    matches and penalizes mismatches between the actual label of an email and the
    score assigned by the model. For positive instances (spam), the model should predict
    a value close to 1, and for negative instances (non-spam), the model should predict
    a value close to 0\. When the prediction and the class label match, the contribution
    to the log likelihood is a small negative number. When they don’t match, the contribution
    to the log likelihood is a larger negative number. The closer to 0 the log likelihood
    is, the better the prediction.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.19](../Text/06.xhtml#ch06fig19)展示了对数似然如何奖励匹配并惩罚实际邮件标签与模型分配的分数之间的不匹配。对于正实例（垃圾邮件），模型应该预测一个接近1的值，而对于负实例（非垃圾邮件），模型应该预测一个接近0的值。当预测与类别标签匹配时，对数似然的贡献是一个小的负数。当它们不匹配时，对数似然的贡献是一个较大的负数。对数似然越接近0，预测越好。'
- en: Figure 6.19\. Log likelihood penalizes mismatches between the prediction and
    the true class label.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19\. 对数似然惩罚预测与真实类别标签之间的不匹配。
- en: '![](Images/06fig19_alt.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig19_alt.jpg)'
- en: The next listing shows one way to calculate the log likelihood of the spam filter’s
    predictions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表展示了计算垃圾邮件过滤器预测的似然对数的一种方法。
- en: Listing 6.11\. Calculating log likelihood
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.11\. 计算似然对数
- en: '[PRE18]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ A function to calculate y * log(py), with the convention that 0 * log(0) =
    0
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个计算 y * log(py) 的函数，约定 0 * log(0) = 0
- en: ❷ Gets the class labels of the test set as TRUE/FALSE, which R treats as 1/0
    in arithmetic operations
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取测试集的类别标签为 TRUE/FALSE，R 在算术运算中将它们视为 1/0
- en: ❸ Calculates the log likelihood of the model’s predictions on the test set
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算模型在测试集上的预测的似然对数
- en: The log likelihood is useful for comparing multiple probability models *on the
    same test dataset*—because the log likelihood is an unnormalized sum, its magnitude
    implicitly depends on the size of the dataset, so you can’t directly compare log
    likelihoods that were computed on different datasets. When comparing multiple
    models, you generally want to prefer models with a larger (that is, smaller magnitude)
    log likelihood.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 似然对数对于比较同一测试数据集上的多个概率模型很有用——因为似然对数是一个非归一化的总和，其大小隐式地依赖于数据集的大小，因此你不能直接比较在不同数据集上计算的似然对数。在比较多个模型时，你通常希望偏好具有较大（即较小幅度）似然对数的模型。
- en: At the very least, you want to compare the model’s performance to the null model
    of predicting the same probability for every example. The best observable single
    estimate of the probability of being spam is the observed rate of spam on the
    training set.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，你想要将模型的性能与预测每个示例相同概率的空模型进行比较。最佳可观察的单个估计值是训练集中垃圾邮件的观察率。
- en: Listing 6.12\. Computing the null model’s log likelihood
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.12\. 计算空模型的似然对数
- en: '[PRE19]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The spam model assigns a log likelihood of `-134.9478` to the test set, which
    is much better than the null model’s `-306.8964`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将 `-134.9478` 的似然对数分配给测试集，这比空模型的 `-306.8964` 要好得多。
- en: Deviance
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差
- en: Another common measure when fitting probability models is the *deviance*. The
    deviance is defined as `-2*(logLikelihood-S)`, where `S` is a technical constant
    called “the log likelihood of the saturated model.” In most cases, the saturated
    model is a perfect model that returns probability 1 for items in the class and
    probability 0 for items not in the class (so `S=0`). The lower the deviance, the
    better the model.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合概率模型时，另一个常见的度量是 *偏差*。偏差定义为 `-2*(logLikelihood-S)`，其中 `S` 是称为“饱和模型的似然对数”的技术常数。在大多数情况下，饱和模型是一个完美的模型，对于属于类的项目返回概率
    1，对于不属于类的项目返回概率 0（因此 `S=0`）。偏差越低，模型越好。
- en: We’re most concerned with ratios of deviance, such as the ratio between the
    null deviance and the model deviance. These deviances can be used to calculate
    a pseudo R-squared (see [http://mng.bz/j338](http://mng.bz/j338)). Think of the
    null deviance as how much variation there is to explain, and the model deviance
    as how much was left unexplained by the model. You want a pseudo R-squared that
    is close to 1.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最关心的是偏差率的比率，例如空偏差与模型偏差之间的比率。这些偏差可以用来计算伪 R 平方（见 [http://mng.bz/j338](http://mng.bz/j338)）。将空偏差视为需要解释的变异量，将模型偏差视为模型未解释的变异量。你希望伪
    R 平方接近 1。
- en: In the next listing, we show a quick calculation of deviance and pseudo R-squared
    using the `sigr` package.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们展示了使用 `sigr` 包快速计算偏差和伪 R 平方的方法。
- en: Listing 6.13\. Computing the deviance and pseudo R-squared
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.13\. 计算偏差和伪 R 平方
- en: '[PRE20]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Like the log likelihood, deviance is unnormalized, so you should only compare
    deviances that are computed over the same dataset. When comparing multiple models,
    you will generally prefer models with smaller deviance. The pseudo R-squared is
    normalized (it’s a function of a ratio of deviances), so in principle you can
    compare pseudo R-squareds even when they were computed over different test sets.
    When comparing multiple models, you will generally prefer models with larger pseudo
    R-squareds.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 与似然对数一样，偏差是非归一化的，因此你应该只比较在相同数据集上计算的偏差。在比较多个模型时，你通常会偏好偏差较小的模型。伪 R 平方是归一化的（它是偏差比率的函数），所以原则上你可以比较在不同测试集上计算的伪
    R 平方。在比较多个模型时，你通常会偏好伪 R 平方较大的模型。
- en: AIC
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: AIC
- en: An important variant of deviance is the *Akaike information criterion* (*AIC*).
    This is equivalent to `deviance + 2*numberOfParameters` used in the model. The
    more parameters in the model, the more complex the model is; the more complex
    a model is, the more likely it is to overfit. Thus, AIC is deviance penalized
    for model complexity. When comparing models (on the same test set), you will generally
    prefer the model with the smaller AIC. The AIC is useful for comparing models
    with different measures of complexity and modeling variables with differing numbers
    of levels. However, adjusting for model complexity is often more reliably achieved
    using the holdout and cross-validation methods discussed in [section 6.2.1](../Text/06.xhtml#ch06lev2sec5).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差的一个重要变体是**赤池信息量准则**（**AIC**）。这相当于模型中使用的`deviance + 2*numberOfParameters`。模型中的参数越多，模型越复杂；模型越复杂，越有可能过拟合。因此，AIC是对模型复杂度进行惩罚的偏差。当比较模型（在同一测试集上）时，你通常会倾向于选择AIC较小的模型。AIC在比较具有不同复杂度度量以及具有不同水平数量的建模变量时很有用。然而，调整模型复杂度通常更可靠地通过使用在[第6.2.1节](../Text/06.xhtml#ch06lev2sec5)中讨论的保留法和交叉验证法来实现。
- en: 'So far, we have evaluated models on how well they perform *in general*: the
    overall rates at which a model returns correct or incorrect predictions on test
    data. In the next section, we look at one method for evaluating a model on *specific*
    examples, or *explaining* why a model returns a specific prediction on a given
    example.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经评估了模型在**总体**上的表现：模型在测试数据上返回正确或错误预测的整体比率。在下一节中，我们将探讨一种评估模型在**特定**示例上的方法，或者解释为什么模型在给定示例上返回特定的预测。
- en: 6.3\. Local interpretable model-agnostic explanations (LIME) for explai- ining
    model predictions
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3\. 本地可解释模型无关解释（LIME）用于解释模型预测
- en: In many people’s opinion, the improved prediction performance of modern machine
    learning methods like deep learning or gradient boosted trees comes at the cost
    of decreased explanation. As you saw in [chapter 1](../Text/01.xhtml#ch01), a
    human domain expert can review the if-then structure of a decision tree and compare
    it to their own decision-making processes to decide if the decision tree will
    make reasonable decisions. Linear models also have an easily explainable structure,
    as you will see in [chapter 8](../Text/08.xhtml#ch08). However, other methods
    have far more complex structures that are difficult for a human to evaluate. Examples
    include the multiple individual trees of a random forest (as in [figure 6.20](../Text/06.xhtml#ch06fig20)),
    or the highly connected topology of a neural net.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多人看来，现代机器学习方法（如深度学习或梯度提升树）的预测性能提升是以降低可解释性为代价的。正如你在[第1章](../Text/01.xhtml#ch01)中看到的，一个人类领域专家可以审查决策树的if-then结构，并将其与自己的决策过程进行比较，以决定决策树是否会做出合理的决策。线性模型也有一个易于解释的结构，正如你将在[第8章](../Text/08.xhtml#ch08)中看到的。然而，其他方法的结构远更复杂，难以被人类评估。例如，随机森林中的多个单独的树（如图6.20所示[../Text/06.xhtml#ch06fig20]），或者神经网络的高度连接拓扑结构。
- en: Figure 6.20\. Some kinds of models are easier to manually inspect than others.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20\. 一些模型比其他模型更容易手动检查。
- en: '![](Images/06fig20.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig20.jpg)'
- en: If a model evaluates well on holdout data, that is an indication that the model
    will perform well in the wild—but it’s not foolproof. One potential issue is that
    the holdout set generally comes from the same source as the training data, and
    has all the same quirks and idiosyncrasies of the training data. How do you know
    whether your model is learning the actual concept of interest, or simply the quirks
    in the data? Or, putting it another way, will the model work on similar data from
    a different source?
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在保留数据上表现良好，这表明模型将在野外表现良好——但这并不是万无一失的。一个潜在的问题是，保留集通常来自与训练数据相同的来源，并且具有与训练数据相同的怪癖和独特性。你如何知道你的模型是否在学习实际感兴趣的概念，或者只是学习数据中的怪癖？或者，换一种说法，模型是否会在来自不同来源的类似数据上工作？
- en: '* * *'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to train a classifier to distinguish documents about Christianity
    from documents about atheism.*'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想要训练一个分类器来区分关于基督教的文档和关于无神论的文档。*'
- en: '* * *'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: One such model was trained using a corpus of postings from the 20 Newsgroups
    Dataset, a dataset frequently used for research in machine learning on text. The
    resulting random forest model was 92% accurate on holdout.^([[7](../Text/06.xhtml#ch06fn7)])
    On the surface, this seems pretty good.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个模型是使用20 Newsgroups数据集的帖子语料库进行训练的，这是一个常用于机器学习文本研究的数据集。该随机森林模型在保留数据上的准确率为92%。^([[7](../Text/06.xhtml#ch06fn7)])
    表面上看，这似乎相当不错。
- en: ⁷
  id: totrans-370
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-371
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The experiment is described in Ribeiro, Singh, and Guestrin, “‘Why Should I
    Trust You?’ Explaining the Predictions of Any Classifier,” [https://arxiv.org/pdf/1602.04938v1.pdf](https://arxiv.org/pdf/1602.04938v1.pdf).
  id: totrans-372
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该实验在Ribeiro, Singh和Guestrin的论文“‘Why Should I Trust You?’ Explaining the Predictions
    of Any Classifier”中有描述，[https://arxiv.org/pdf/1602.04938v1.pdf](https://arxiv.org/pdf/1602.04938v1.pdf)。
- en: However, delving deeper into the model showed that it was exploiting idiosyncrasies
    in the data, using the distribution of words like “There” or “Posting” or “edu”
    to decide whether a post was about Christianity or about atheism. In other words,
    the model was looking at the wrong features in the data. An example of a classification
    by this model is shown in [figure 6.21](../Text/06.xhtml#ch06fig21).^([[8](../Text/06.xhtml#ch06fn8)])
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深入研究模型发现，它正在利用数据中的特殊性，使用“那里”、“帖子”或“edu”等词语的分布来决定帖子是关于基督教还是关于无神论。换句话说，该模型正在查看数据中的错误特征。该模型的一个分类示例在[图6.21](../Text/06.xhtml#ch06fig21).^([[8](../Text/06.xhtml#ch06fn8)])中展示。
- en: ⁸
  id: totrans-374
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-375
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Source: [https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)'
  id: totrans-376
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)
- en: Figure 6.21\. Example of a document and the words that most strongly contributed
    to its classification as “atheist” by the model
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21。一个文档及其被模型分类为“无神论者”的最强贡献词语示例
- en: '![](Images/06fig21_alt.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig21_alt.jpg)'
- en: In addition, since the documents in the corpus seem to have included the names
    of specific posters, this model could also potentially be learning whether a *person*
    who posts frequently in the training corpus is a Christian or an atheist, which
    is not the same as learning if a *text* is Christian or atheist, especially when
    trying to apply the model to a document from a different corpus, with different
    authors.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于语料库中的文档似乎包含了特定发帖者的名字，这个模型也可能潜在地学习到在训练语料库中频繁发帖的“人”是基督徒还是无神论者，这不同于学习“文本”是基督徒还是无神论者，尤其是在尝试将模型应用于来自不同语料库、不同作者的文档时。
- en: Another real-world example is Amazon’s recent attempt to automate resume reviews,
    using the resumes of people hired by Amazon over a 10-year period as training
    data.^([[9](../Text/06.xhtml#ch06fn9)]) As Reuters reported, the company discovered
    that their model was discriminating against women. It penalized resumes that included
    words like “women’s,” and downvoted applicants who had graduated from two particular
    all-women’s colleges. Researchers also discovered that the algorithm ignored common
    terms that referred to specific skills (such as the names of computer programming
    languages), and favored words like *executed* or *captured* that were disproportionately
    used by male applicants.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个现实世界的例子是亚马逊最近尝试自动化简历审查，使用亚马逊在过去10年内雇佣的人的简历作为训练数据.^([[9](../Text/06.xhtml#ch06fn9)])
    如路透社报道，公司发现他们的模型对女性存在歧视。它惩罚包含“女性”等词语的简历，并对毕业于两所特定女子大学的申请者进行降级。研究人员还发现，该算法忽略了指代特定技能的常见术语（例如计算机编程语言的名称），并偏好像“执行”或“捕获”这样的词语，这些词语在男性申请者中被不成比例地使用。
- en: ⁹
  id: totrans-381
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-382
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeffrey Dastin, “Amazon scraps secret AI recruiting tool that showed bias against
    women,” Reuters, October 9, 2018, [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G).
  id: totrans-383
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jeffrey Dastin，“亚马逊取消对女性存在歧视的秘密AI招聘工具，”路透社，2018年10月9日，[https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)。
- en: In this case, the flaw was not in the machine learning algorithm, but in the
    training data, which had apparently captured existing biases in Amazon’s hiring
    practices—which the model then codified. Prediction explanation techniques like
    LIME can potentially discover such issues.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，问题不在于机器学习算法，而在于训练数据，该数据显然捕捉到了亚马逊招聘实践中的现有偏见——模型随后将这些偏见编码。LIME等预测解释技术可能发现此类问题。
- en: '6.3.1\. LIME: Automated sanity checking'
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1\. LIME：自动性检查
- en: In order to detect whether a model is really learning the concept, and not just
    data quirks, it’s not uncommon for domain experts to manually sanity-check a model
    by running some example cases through and looking at the answers. Generally, you
    would want to try a few typical cases, and a few extreme cases, just to see what
    happens. You can think of LIME as one form of automated sanity checking.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测模型是否真正学习到概念，而不是仅仅学习数据中的异常，领域专家通常会手动通过运行一些示例案例来检查模型，并查看答案。通常，你想要尝试一些典型案例和一些极端案例，以看看会发生什么。你可以将LIME视为一种自动性检查的形式。
- en: LIME produces an “explanation” of a model’s prediction on a specific datum.
    That is, LIME tries to determine which features of that datum contributed the
    most to the model’s decision about it. This helps data scientists attempt to understand
    the behavior of black-box machine learning models.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: LIME可以产生一个关于模型对特定数据预测的解释。也就是说，LIME试图确定哪些特征对该数据产生了最大的影响，从而帮助数据科学家尝试理解黑盒机器学习模型的行为。
- en: 'To make this concrete, we will demonstrate LIME on two tasks: classifying iris
    species, and classifying movie reviews.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明，我们将演示LIME在两个任务上的应用：鸢尾花物种分类和电影评论分类。
- en: '6.3.2\. Walking through LIME: A small example'
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2\. 漫步LIME：一个小例子
- en: The first example is iris classification.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子是鸢尾花分类。
- en: '* * *'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you have a dataset of petal and sepal measurements for three varieties
    of iris. The object is to predict whether a given iris is a setosa based on its
    petal and sepal dimensions.*'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你有一个包含三种鸢尾花花瓣和萼片尺寸的数据集。目标是根据花瓣和萼片的尺寸预测给定的鸢尾花是否为setosa。*'
- en: '* * *'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Let’s get the data and split it into test and training.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取数据并将其分为测试集和训练集。
- en: Listing 6.14\. Loading the iris dataset
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.14\. 加载鸢尾花数据集
- en: '[PRE21]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Setosa is the positive class.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Setosa是正类。
- en: ❷ Uses 75% of the data for training, the remainder as holdout (test data)
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用75%的数据进行训练，其余作为保留（测试数据）
- en: The variables are the length and width of the sepals and petals. The outcome
    you want to predict is `class`, which is 1 when the iris is *setosa*, and 0 otherwise.
    You will fit a gradient boosting model (from the package `xgboost`) to predict
    `class`.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是萼片和花瓣的长度和宽度。你想要预测的结果是`class`，当鸢尾花是*setosa*时为1，否则为0。你将使用来自`xgboost`包的梯度提升模型来预测`class`。
- en: You will learn about gradient boosting models in detail in [chapter 10](../Text/10.xhtml#ch10);
    for now, we have wrapped the fitting procedure into the function `fit_iris_example()`
    that takes as input a matrix of inputs and a vector of class labels, and returns
    a model that predicts `class`.^([[10](../Text/06.xhtml#ch06fn10)]) The source
    code for `fit_iris_example()` is in [https://github.com/WinVector/PDSwR2/tree/master/LIME_iris/lime_iris_example.R](https://github.com/WinVector/PDSwR2/tree/master/LIME_iris/lime_iris_example.R);
    in [chapter 10](../Text/10.xhtml#ch10), we will unpack how the function works
    in detail.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第[10章](../Text/10.xhtml#ch10)中详细了解梯度提升模型；目前，我们已经将拟合过程封装到函数`fit_iris_example()`中，该函数接受输入矩阵和类别标签向量作为输入，并返回一个预测`class`的模型。^([[10](../Text/06.xhtml#ch06fn10)])
    `fit_iris_example()`的源代码位于[https://github.com/WinVector/PDSwR2/tree/master/LIME_iris/lime_iris_example.R](https://github.com/WinVector/PDSwR2/tree/master/LIME_iris/lime_iris_example.R)；在第[10章](../Text/10.xhtml#ch10)中，我们将详细解释该函数的工作原理。
- en: ^(10)
  id: totrans-402
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-403
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The `xgboost` package requires that the input be a numeric matrix, and the class
    labels be a numeric vector.
  id: totrans-404
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`xgboost`包要求输入是一个数值矩阵，类别标签是一个数值向量。'
- en: To get started, convert the training data to a matrix and fit the model. Make
    sure that `lime_iris_example.R` is in your working directory.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，将训练数据转换为矩阵并拟合模型。确保`lime_iris_example.R`位于你的工作目录中。
- en: Listing 6.15\. Fitting a model to the iris training data
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.15\. 将模型拟合到鸢尾花训练数据
- en: '[PRE22]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Loads the convenience function
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载便利函数
- en: ❷ The input to the model is the first four columns of the training data, converted
    to a matrix.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型的输入是训练数据的前四列，转换为一个矩阵。
- en: After you fit the model, you can evaluate the model on the test data. The model’s
    predictions are the probability that a given iris is *setosa*.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在您拟合模型后，您可以在测试数据上评估模型。模型的预测是给定留兰香是 *setosa* 的概率。
- en: Listing 6.16\. Evaluating the iris model
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.16\. 评估留兰香模型
- en: '[PRE23]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Makes predictions on the test data. The predictions are the probability that
    an iris is a setosa.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在测试数据上做出预测。预测是留兰香是 *setosa* 的概率。
- en: ❷ A data frame of predictions and actual outcome
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测和实际结果的数据框
- en: ❸ Examines the confusion matrix
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查混淆矩阵
- en: 'Note that all the datums in the test set fall into the diagonals of the confusion
    matrix: the model correctly labels all *setosa* examples as “*setosa*” and all
    the others as “not *setosa*.” This model predicts perfectly on the test set! However,
    you might still want to know which features of an iris are most important when
    classifying it with your model. Let’s take a specific example from the `test`
    dataset and explain it, using the `lime` package.^([[11](../Text/06.xhtml#ch06fn11)])'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，测试集中的所有数据点都落在混淆矩阵的对角线上：该模型正确地将所有 *setosa* 示例标记为 “*setosa*”，并将所有其他标记为 “not
    *setosa*”。该模型在测试集上的预测完美无缺！然而，您可能仍然想知道在用您的模型对留兰香进行分类时，哪些特征是最重要的。让我们从 `test` 数据集中的一个特定示例开始，并使用
    `lime` 包对其进行解释.^([[11](../Text/06.xhtml#ch06fn11)])
- en: ^(11)
  id: totrans-417
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-418
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The `lime` package does not support every type of model out of the box. See
    `help(model_support)` for the list of model classes that it does support (`xgboost`
    is one), and how to add support for other types of models. See also LIME’s README
    ([https://cran.r-project.org/web/packages/lime/README.html](https://cran.r-project.org/web/packages/lime/README.html))
    for other examples.
  id: totrans-419
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`lime` 包并不支持所有类型的模型。请参阅 `help(model_support)` 获取它支持的模型类列表（`xgboost` 是其中之一），以及如何添加对其他类型模型的支持。有关其他示例，请参阅
    LIME 的 README 文件 ([https://cran.r-project.org/web/packages/lime/README.html](https://cran.r-project.org/web/packages/lime/README.html))。'
- en: 'First, use the training set and the model to build an *explainer*: a function
    that you will use to explain the model’s predictions.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用训练集和模型构建一个 *解释器*：一个您将用于解释模型预测的函数。
- en: Listing 6.17\. Building a LIME explainer from the model and training data
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.17\. 从模型和训练数据构建 LIME 解释器
- en: '[PRE24]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Builds the explainer from the training data
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从训练数据构建解释器
- en: ❷ Bins the continuous variables when making explanations
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在解释时对连续变量进行分箱
- en: ❸ Uses 10 bins
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 10 个分箱
- en: Now pick a specific example from the test set.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从测试集中选择一个特定示例。
- en: Listing 6.18\. An example iris datum
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.18\. 留兰香数据示例
- en: '[PRE25]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ A single row data frame
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 单行数据框
- en: ❷ This example is a setosa.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这个示例是 *setosa*。
- en: ❸ And the model predicts that it is a setosa.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 并且模型预测它是 *setosa*。
- en: 'Now explain the model’s prediction on `example`. Note that the `dplyr` package
    also has a function called `explain()`, so if you have `dplyr` in your namespace,
    you may get a conflict trying to call `lime`’s `explain()` function. To prevent
    this ambiguity, specify the function using namespace notation: `lime::explain(...)`.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解释 `example` 上的模型预测。请注意，`dplyr` 包也有一个名为 `explain()` 的函数，所以如果您在命名空间中有 `dplyr`，您可能在调用
    `lime` 的 `explain()` 函数时遇到冲突。为了防止这种歧义，请使用命名空间符号指定函数：`lime::explain(...)`.
- en: Listing 6.19\. Explaining the iris example
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.19\. 解释留兰香示例
- en: '[PRE26]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ The number of labels to explain; use 1 for binary classification.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要解释的标签数量；对于二分类，使用 1。
- en: ❷ The number of features to use when fitting the explanation
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在拟合解释时使用的特征数量
- en: You can visualize the explanation using `plot_features()`, as shown in [figure
    6.22](../Text/06.xhtml#ch06fig22).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `plot_features()` 函数可视化解释，如图 6.22 所示 [图 6.22](../Text/06.xhtml#ch06fig22)。
- en: '[PRE27]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Figure 6.22\. Visualize the explanation of the model’s prediction.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22\. 可视化模型预测的解释
- en: '![](Images/06fig22_alt.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig22_alt.jpg)'
- en: The explainer expects the model will predict that this example is a *setosa*
    (Label = 1), and that the example’s value of `Petal.Length` is strong evidence
    supporting this prediction.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器期望模型预测此示例是 *setosa*（标签 = 1），并且该示例的 `Petal.Length` 值是支持此预测的强烈证据。
- en: How LIME works
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 的工作原理
- en: 'In order to better understand LIME’s explanations, and to diagnose when the
    explanations are trustworthy or not, it helps to understand how LIME works at
    a high level. [Figure 6.23](../Text/06.xhtml#ch06fig23) sketches out the LIME
    procedure for a classifier at a high level. The figure shows these points:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 LIME 的解释，并诊断解释是否可信，了解 LIME 的高层次工作原理很有帮助。[图 6.23](../Text/06.xhtml#ch06fig23)
    高层次地概述了 LIME 对分类器的处理过程。该图显示了以下要点：
- en: '*The model’s decision surface*. A classifier’s *decision surface* is the surface
    in variable space that separates where the model classifies datums as positive
    (in our example, as “*setosa*”) from where it classifies them as negative (in
    our example, as “not *setosa*”).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型的决策表面*。分类器的*决策表面*是在变量空间中分隔模型将数据点分类为正例（在我们的例子中，作为“*setosa*”）和将其分类为负例（在我们的例子中，作为“not
    *setosa*”）的表面。'
- en: '*The datum we want to explain* marked as the circled plus in the figure. In
    the figure, the datum is a positive example. In the explanation that follows,
    we’ll call this point “the original example,” or `example`.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中标记为圆圈加号的*我们想要解释的数据点*。在图中，该数据点是一个正例。在接下来的解释中，我们将称这个点为“原始示例”或`example`。
- en: '*Synthetic data points* that the algorithm creates and gives to the model to
    evaluate. We’ll detail how the synthetic examples come about.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法创建并给模型评估的*合成数据点*。我们将详细说明合成示例是如何产生的。
- en: '*LIME’s estimate of the decision surface* near the example we are trying to
    explain. We’ll detail how LIME comes up with this estimate.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LIME对我们要解释的示例附近的决策表面的估计*。我们将详细说明LIME是如何得出这个估计的。'
- en: Figure 6.23\. Notional sketch of how LIME works
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23。LIME工作原理的概念草图
- en: '![](Images/06fig23_alt.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23的替代文本](Images/06fig23_alt.jpg)'
- en: 'The procedure is as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 程序如下：
- en: “Jitter” the original example to generate synthetic examples that are similar
    to it. You can think of each jittered point as the original example with the value
    of each variable changed slightly. For example, if the original example is
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “抖动”原始示例以生成与它相似的合成示例。你可以将每个抖动点视为原始示例，其中每个变量的值略有变化。例如，如果原始示例是
- en: '[PRE28]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: then a jittered point might be
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那么一个抖动点可能为
- en: '[PRE29]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To make sure that the synthetic examples are plausible, LIME uses the distributions
    of the data in the training set to generate the jittered data. For our discussion,
    we’ll call the set of synthetic examples {`s_i`}. [Figure 6.23](../Text/06.xhtml#ch06fig23)
    shows the synthetic data as the additional pluses and minuses. Note that the jittering
    is randomized. This means that running `explain()` on the same example multiple
    times will produce different results each time. If LIME’s explanation is strong,
    the results should not be too different, so that the explanations remain quantitatively
    similar. In our case, it’s likely that `Petal.Length` will always show up as the
    variable with the most weight; it’s just the exact value of `Petal.Length`’s weight
    and its relationship to the other variables that will vary.
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了确保合成示例是合理的，LIME使用训练集中数据的分布来生成抖动数据。在我们的讨论中，我们将称合成示例的集合为 `{s_i}`。[图6.23](../Text/06.xhtml#ch06fig23)显示了作为额外正号和负号的合成数据。请注意，抖动是随机的。这意味着在同一个示例上多次运行
    `explain()` 将每次产生不同的结果。如果LIME的解释是强有力的，结果应该不会太不同，这样解释在数量上仍然相似。在我们的案例中，`Petal.Length`很可能总是作为权重最大的变量出现；只是`Petal.Length`的权重及其与其他变量的关系会有所不同。
- en: Use the model to make predictions {`y_i`} on all the synthetic examples. In
    [figure 6.23](../Text/06.xhtml#ch06fig23), the pluses indicate synthetic examples
    that the model classified as positive, and the minuses indicate synthetic examples
    that the model classified as negative. LIME will use the values of {`y_i`} to
    get an idea of what the decision surface of the model looks like near the original
    example. In [figure 6.23](../Text/06.xhtml#ch06fig23), the decision surface is
    the large curvy structure that separates the regions where the model classifies
    datums as positive from the regions where it classifies datums as negative.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型对所有合成示例进行预测 `{y_i}`。在[图6.23](../Text/06.xhtml#ch06fig23)中，正号表示模型被分类为正例的合成示例，负号表示模型被分类为负例的合成示例。LIME将使用
    `{y_i}` 的值来了解模型在原始示例附近的决策表面看起来像什么。在[图6.23](../Text/06.xhtml#ch06fig23)中，决策表面是分隔模型将数据点分类为正例的区域和将其分类为负例的区域的大型曲线结构。
- en: Fit an *m*-dimensional linear model for {`y_i`} as a function of {`s_i`}. The
    linear model is LIME’s estimate of the original model’s decision surface near
    `example`, shown as a dashed line in [figure 6.23](../Text/06.xhtml#ch06fig23).
    Using a linear model means that LIME assumes that the model’s decision surface
    is locally linear (flat) in a small neighborhood around `example`. You can think
    of LIME’s estimate as the flat surface (in the figure, it’s a line) that separates
    the positive synthetic examples from the negative synthetic examples most accurately.
    The R² of the linear model (reported as the “explanation fit” in [figure 6.22](../Text/06.xhtml#ch06fig22))
    indicates how well this assumption is met. If the explanation fit is close to
    0, then there isn’t a flat surface that separates the positive examples from the
    negative examples well, and LIME’s explanation is probably not reliable. You specify
    the value of *m* with the `n_features` parameter in the function `explain().`
    In our case, we are using four features (all of them) to fit the linear model.
    When there is a large number of features (as in text processing), LIME tries to
    pick the best *m* features to fit the model. The coefficients of the linear model
    give us the weights of the features in the explanation. For classification, a
    large positive weight means that the corresponding feature is strong evidence
    in favor of the model’s prediction, and a large negative weight means that the
    corresponding feature is strong evidence against it.
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 {`y_i`} 作为 {`s_i`} 的函数拟合一个 *m*- 维线性模型。线性模型是 LIME 对原始模型决策表面的估计，如图 6.23 [图](../Text/06.xhtml#ch06fig23)
    中的虚线所示。使用线性模型意味着 LIME 假设模型的决策表面在 `example` 附近的局部区域是线性的（平坦的）。你可以将 LIME 的估计看作是最准确地分离正合成示例和负合成示例的平坦表面（在图中，它是一条线）。线性模型的
    R²（在 [图 6.22](../Text/06.xhtml#ch06fig22) 中报告为“解释拟合”）表明这一假设得到了多好的满足。如果解释拟合接近 0，那么就没有一个平坦的表面能够很好地将正例和负例分开，LIME
    的解释可能不可靠。你通过函数 `explain()` 中的 `n_features` 参数指定 *m* 的值。在我们的例子中，我们使用四个特征（所有特征）来拟合线性模型。当有大量特征（如文本处理中）时，LIME
    会尝试选择最佳的 *m* 个特征来拟合模型。线性模型的系数给出了解释中特征的权重。对于分类，一个大的正权重意味着相应的特征是模型预测的有力证据，而一个大的负权重意味着相应的特征是反对模型预测的有力证据。
- en: Taking the steps as a whole
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 将步骤作为一个整体来考虑
- en: 'This may seem like a lot of steps, but they are all supplied in a convenient
    wrapper by the `lime` package. Altogether, the steps are implementing a solution
    to a simple counter-factual question: how would a given example score differently
    if it had different attributes? The summaries emphasize what are the most important
    plausible variations.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有很多步骤，但它们都由 `lime` 包提供了一个方便的包装器。总的来说，这些步骤是在实现一个简单的反事实问题的解决方案：如果给定的示例有不同的属性，它的评分会有何不同？总结强调了最重要的合理变化。
- en: Back to the iris example
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 回到鸢尾花示例
- en: Let’s pick a couple more examples and explain the model’s predictions on them.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再挑选几个示例，并解释模型对这些示例的预测。
- en: Listing 6.20\. More iris examples
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.20\. 更多鸢尾花示例
- en: '[PRE30]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Both examples are negative (not setosa ).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这两个示例都是负的（不是 setosa）。
- en: ❷ The model predicts that both examples are negative.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型预测这两个示例都是负的。
- en: 'The explainer expects that the model will predict that both these examples
    are not *setosa* (Label = 0). For case 110 (the second row of `example` and the
    right side plot of [figure 6.24](../Text/06.xhtml#ch06fig24)), this is again because
    of `Petal.Length`. Case 58 (the left side plot of [figure 6.24](../Text/06.xhtml#ch06fig24))
    seems strange: most of the evidence seems to contradict the expected classification!
    Note that the explanation fit for case 58 is quite small: it’s an order of magnitude
    less than the fit for case 110\. This tells you that you may not want to trust
    this explanation.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器期望模型会预测这两个示例都不是 *setosa*（标签 = 0）。对于案例 110（`example` 的第二行和 [图 6.24](../Text/06.xhtml#ch06fig24)
    的右侧图），这又是由于 `Petal.Length`。案例 58（[图 6.24](../Text/06.xhtml#ch06fig24) 的左侧图）看起来很奇怪：大部分证据似乎都与预期的分类相矛盾！请注意，案例
    58 的解释拟合相当小：它比案例 110 的拟合小一个数量级。这告诉你，你可能不想相信这个解释。
- en: Figure 6.24\. Explanations of the two iris examples
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24\. 两个鸢尾花示例的解释
- en: '![](Images/06fig24_alt.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig24_alt.jpg)'
- en: Let’s look at how these three examples compare to the rest of the iris data.
    [Figure 6.25](../Text/06.xhtml#ch06fig25) shows the distribution of petal and
    sepal dimensions in the data, with the three sample cases marked.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这三个例子与鸢尾花数据的其余部分如何比较。[图6.25](../Text/06.xhtml#ch06fig25)显示了数据中花瓣和萼片尺寸的分布，其中三个样本案例被标记出来。
- en: Figure 6.25\. Distributions of petal and sepal dimensions by species
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25\. 不同物种花瓣和萼片尺寸的分布
- en: '![](Images/06fig25_alt.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig25_alt.jpg)'
- en: It’s clear from [figure 6.25](../Text/06.xhtml#ch06fig25) that petal length
    strongly differentiates *setosa* from the other species of iris. With respect
    to petal length, case 30 is obviously *setosa*, and case 110 is obviously not.
    Case 58 appears to be not *setosa* due to petal length, but as noted earlier,
    the entire explanation of case 58 is quite poor, probably because case 58 sits
    at some sort of kink on the model’s decision surface.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图6.25](../Text/06.xhtml#ch06fig25)中可以看出，花瓣长度强烈区分了*setosa*与其他的鸢尾花物种。就花瓣长度而言，案例30显然是*setosa*，而案例110显然不是。案例58似乎不是*setosa*，因为花瓣长度，但如前所述，案例58的整个解释相当差，可能是因为案例58位于模型决策曲面上的一种类型的拐点上。
- en: Now let’s try LIME on a larger example.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试在一个更大的例子上使用LIME。
- en: 6.3.3\. LIME for text classification
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3\. LIME用于文本分类
- en: '* * *'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*For this example, you will classify movie reviews from the Internet Movie
    Database (IMDB). The task is to identify positive reviews.*'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这个例子中，你将使用互联网电影数据库（IMDB）中的电影评论进行分类。任务是识别正面评论。*'
- en: '* * *'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'For convenience, we’ve converted the data from the original archive^([[12](../Text/06.xhtml#ch06fn12)])
    into two RDS files, `IMDBtrain.RDS` and `IMDBtest.RDS`, found at [https://github.com/WinVector/PDSwR2/tree/master/IMDB](https://github.com/WinVector/PDSwR2/tree/master/IMDB).
    Each RDS object is a list with two elements: a character vector representing 25,000
    reviews, and a vector of numeric labels where 1 means a positive review and 0
    a negative review.^([[13](../Text/06.xhtml#ch06fn13)]) You will again fit an `xgboost`
    model to classify the reviews.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们将原始存档中的数据转换为两个RDS文件，`IMDBtrain.RDS`和`IMDBtest.RDS`，它们可以在[https://github.com/WinVector/PDSwR2/tree/master/IMDB](https://github.com/WinVector/PDSwR2/tree/master/IMDB)找到。每个RDS对象包含两个元素：一个表示25,000条评论的字符向量，以及一个表示数值标签的向量，其中1表示正面评论，0表示负面评论.^([[13](../Text/06.xhtml#ch06fn13)])
    你将再次拟合一个`xgboost`模型来对评论进行分类。
- en: ^(12)
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^{(12)}
- en: ''
  id: totrans-481
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The original data can be found at [http://s3.amazonaws.com/text-datasets/aclImdb.zip](http://s3.amazonaws.com/text-datasets/aclImdb.zip).
  id: totrans-482
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始数据可以在[http://s3.amazonaws.com/text-datasets/aclImdb.zip](http://s3.amazonaws.com/text-datasets/aclImdb.zip)找到。
- en: ^(13)
  id: totrans-483
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^{(13)}
- en: ''
  id: totrans-484
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The extraction/conversion script we used to create the RDS files can be found
    at [https://github.com/WinVector/PDSwR2/tree/master/IMDB/getIMDB.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/getIMDB.R).
  id: totrans-485
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建RDS文件所使用的提取/转换脚本可以在[https://github.com/WinVector/PDSwR2/tree/master/IMDB/getIMDB.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/getIMDB.R)找到。
- en: You might wonder how LIME jitters a text datum. It does so by randomly removing
    words from the document, and then converting the resulting new text into the appropriate
    representation for the model. If removing a word tends to change the classification
    of a document, then that word is probably important to the model.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道LIME是如何抖动文本数据的。它是通过随机从文档中删除单词来做到这一点的，然后将生成的新文本转换为模型适当的表示。如果删除一个单词倾向于改变文档的分类，那么这个单词可能对模型很重要。
- en: First, load the training set. Make sure you have downloaded the RDS files into
    your working directory.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载训练集。确保你已经将RDS文件下载到你的工作目录中。
- en: Listing 6.21\. Loading the IMDB training data
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.21\. 加载IMDB训练数据
- en: '[PRE31]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Loads the zeallot library. Calls install.packages(“zeallot”) if this fails.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载zeallot库。如果失败，则调用install.packages(“zeallot”)。
- en: '❷ The command read(IMDBtrain.RDS) returns a list object. The zeallot assignment
    arrow %<-% unpacks the list into two elements: texts is a character vector of
    reviews, and labels is a 0/1 vector of class labels. The label 1 designates a
    positive review.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 命令read(IMDBtrain.RDS)返回一个列表对象。使用zeallot赋值箭头%<-%将列表解包成两个元素：texts是一个包含评论的字符向量，labels是一个0/1的类别标签向量。标签1表示正面评论。
- en: 'You can examine the reviews and their corresponding labels. Here’s a positive
    review:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以检查评论及其相应的标签。这里有一个正面评论：
- en: '[PRE32]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here’s a negative review:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个负面评论：
- en: '[PRE33]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Representing documents for modeling
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 为建模表示文档
- en: For our text model, the features are the individual words, and there are a lot
    of them. To use `xgboost` to fit a model on texts, we have to build a finite feature
    set, or the *vocabulary*. The words in the vocabulary are the only features that
    the model will consider.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的文本模型，特征是单个词，有很多这样的词。为了使用 `xgboost` 在文本上拟合模型，我们必须构建一个有限的特征集，或称为 *词汇表*。词汇表中的词是模型唯一考虑的特征。
- en: We don’t want to use words that are too common, because common words that show
    up in both positive reviews and negative reviews won’t be informative. We also
    don’t want to use words that are too rare, because a word that rarely shows up
    in a review is not that useful. For this task, let’s define “too common” as words
    that show up in more than half the training documents, and “too rare” as words
    that show up in fewer than 0.1% of the documents.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想使用太常见的词，因为出现在正面评论和负面评论中的常见词不会提供信息。我们也不想使用太罕见的词，因为很少出现在评论中的词并不那么有用。对于这个任务，让我们将“太常见”定义为出现在超过一半的训练文档中的词，将“太罕见”定义为出现在不到0.1%的文档中的词。
- en: We’ll build a vocabulary of 10,000 words that are not too common or too rare,
    using the package `text2vec`. For brevity, we’ve wrapped the procedure in the
    function `create_pruned_vocabulary()`, which takes a vector of documents as input
    and returns a vocabulary object. The source code for `create_pruned_vocabulary()`
    is in [https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R).
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `text2vec` 包构建一个包含10,000个词的词汇表，这些词不太常见也不太罕见。为了简洁，我们将该过程封装在函数 `create_pruned_vocabulary()`
    中，该函数接受文档向量作为输入并返回一个词汇表对象。`create_pruned_vocabulary()` 函数的源代码位于 [https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R)。
- en: Once we have the vocabulary, we have to convert the texts (again using `text2vec`)
    into a numeric representation that `xgboost` can use. This representation is called
    a *document-term matrix*, where the rows represent each document in the corpus,
    and each column represents a word in the vocabulary. For a document-term matrix
    `dtm`, the entry `dtm[i, j]` is the number of times that the vocabulary word `w[j]`
    appeared in document `texts[i]`. See [figure 6.26](../Text/06.xhtml#ch06fig26).
    Note that this representation loses the order of the words in the documents.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了词汇表，我们必须将文本（再次使用 `text2vec`）转换为 `xgboost` 可以使用的数值表示。这种表示称为 *文档-词矩阵*，其中行表示语料库中的每个文档，每列表示词汇表中的一个词。对于文档-词矩阵
    `dtm`，`dtm[i, j]` 是词汇表词 `w[j]` 在文档 `texts[i]` 中出现的次数。参见 [图6.26](../Text/06.xhtml#ch06fig26)。请注意，这种表示会丢失文档中词的顺序。
- en: Figure 6.26\. Creating a document-term matrix
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26\. 创建文档-词矩阵
- en: '![](Images/06fig26_alt.jpg)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig26_alt.jpg)'
- en: 'The document-term matrix will be quite large: 25,000 rows by 10,000 columns.
    Luckily, most words in the vocabulary won’t show up in a given document, so each
    row will be mostly zeros. This means that we can use a special representation
    called a *sparse matrix*, of class `dgCMatrix`, that represents large, mostly
    zero matrices in a space-efficient way.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 文档-词矩阵将会相当大：25,000行乘以10,000列。幸运的是，词汇表中的大多数词不会出现在给定的文档中，所以每一行将主要是零。这意味着我们可以使用一种特殊表示，称为
    *稀疏矩阵*，该矩阵以空间高效的方式表示大型、主要为零的矩阵。
- en: We’ve wrapped this conversion in the function `make_matrix()` that takes as
    input a vector of texts and a vocabulary, and returns a sparse matrix. As in the
    iris example, we’ve also wrapped the model fitting into a function `fit_imdb_model()`
    that takes as input a document term matrix and the numeric document labels, and
    returns an `xgboost` model. The source code for these functions is also in [https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个转换封装在函数 `make_matrix()` 中，该函数接受文本向量和词汇表作为输入，并返回一个稀疏矩阵。与鸢尾花示例一样，我们还把模型拟合封装在函数
    `fit_imdb_model()` 中，该函数接受文档词矩阵和数值文档标签作为输入，并返回一个 `xgboost` 模型。这些函数的源代码也位于 [https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R)。
- en: 6.3.4\. Training the text classifier
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4\. 训练文本分类器
- en: After you download `lime_imdb_example.R` into your working directory, you can
    create the vocabulary and a document-term matrix from the training data, and fit
    the model. This may take a while.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 `lime_imdb_example.R` 下载到您的当前工作目录后，您可以从训练数据中创建词汇表和文档-词矩阵，并拟合模型。这可能需要一些时间。
- en: Listing 6.22\. Converting the texts and fitting the model
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.22\. 转换文本并拟合模型
- en: '[PRE34]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Creates the vocabulary from the training data
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从训练数据创建词汇表
- en: ❷ Creates the document-term matrix of the training corpus
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建训练语料库的文档-词矩阵
- en: ❸ Trains the model
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练模型
- en: Now load the test corpus and evaluate the model.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加载测试语料库并评估模型。
- en: Listing 6.23\. Evaluate the review classifier
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.23\. 评估评论分类器
- en: '[PRE35]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Reads in the test corpus
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取测试语料库
- en: ❷ Converts the corpus to a document-term matrix
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将语料库转换为文档-词矩阵
- en: ❸ Makes predictions (probabilities) on the test corpus
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试语料库上做出预测(概率)
- en: ❹ Creates a frame with true and predicted labels
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建包含真实标签和预测标签的框架
- en: ❺ Computes the confusion matrix
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算混淆矩阵
- en: ❻ Computes the accuracy
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算准确率
- en: ❻ Plots the distribution of predictions
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 绘制预测分布
- en: Based on its performance on the test set, the model does a good, but not perfect,
    job at classifying reviews. The distribution of test prediction scores ([figure
    6.27](../Text/06.xhtml#ch06fig27)) shows that most negative (class 0) reviews
    have low scores, and most positive (class 1) reviews have high scores. However,
    there are some positive reviews that get scores near 0, and some negative reviews
    get scores near 1\. And some reviews have scores near 0.5, meaning the model isn’t
    sure about them at all. You would like to improve the classifier to do a better
    job on these seemingly ambiguous reviews.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其在测试集上的表现，模型在分类评论方面做得很好，但并不完美。测试预测分数的分布([图6.27](../Text/06.xhtml#ch06fig27))显示，大多数负评(类别0)得分低，大多数正评(类别1)得分高。然而，也有一些正评得分接近0，一些负评得分接近1。还有一些评论得分接近0.5，这意味着模型对这些看似模糊的评论完全不确定。你希望改进分类器，在这些看似模糊的评论上做得更好。
- en: Figure 6.27\. Distribution of test prediction scores
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27\. 测试预测分数的分布
- en: '![](Images/06fig27_alt.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27的替代文本](Images/06fig27_alt.jpg)'
- en: 6.3.5\. Explaining the classifier’s predictions
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5\. 解释分类器的预测
- en: Try explaining the predictions for a few example reviews to get some insight
    into the model. First, build the explainer from the training data and the model.
    For text models, the `lime()` function takes a preprocessor function that converts
    the training texts and the synthetic examples to a document-term matrix for the
    model.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试解释几个示例评论的预测，以了解模型。首先，从训练数据和模型中构建解释器。对于文本模型，`lime()`函数需要一个预处理函数，该函数将训练文本和合成示例转换为模型所需的文档-词矩阵。
- en: Listing 6.24\. Building an explainer for a text classifier
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.24\. 为文本分类器构建解释器
- en: '[PRE36]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now take a short sample text from the test corpus. This review is positive,
    and the model predicts that it is positive.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从测试语料库中取一个简短的样本文本。这条评论是积极的，模型预测它是积极的。
- en: Listing 6.25\. Explaining the model’s prediction on a review
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.25\. 解释模型对评论的预测
- en: '[PRE37]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now explain the model’s classification in terms of the five most evidential
    words. The words that affect the prediction the most are shown in [figure 6.28](../Text/06.xhtml#ch06fig28).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用五个最有力的证据词来解释模型的分类。影响预测最多的单词在[图6.28](../Text/06.xhtml#ch06fig28)中显示。
- en: Figure 6.28\. Explanation of the prediction on the sample review
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28\. 样本评论的预测解释
- en: '![](Images/06fig28_alt.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图6.28的替代文本](Images/06fig28_alt.jpg)'
- en: Listing 6.26\. Explaining the model’s prediction
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.26\. 解释模型的预测
- en: '[PRE38]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In [listing 6.26](../Text/06.xhtml#ch06ex26), you used `plot_features()` to
    visualize the explanation, as you did in the `iris` example, but `lime` also has
    a special visualization for text, `plot_text_ explanations()`.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表6.26](../Text/06.xhtml#ch06ex26)中，你使用了`plot_features()`来可视化解释，就像在`iris`示例中做的那样，但`lime`还有一个针对文本的特殊可视化，即`plot_text_explanations()`。
- en: As shown in [figure 6.29](../Text/06.xhtml#ch06fig29), `plot_text_explanations()`
    highlights the key words within the text, green for supporting evidence, and red
    for contradictory. The stronger the evidence, the darker the color. Here, the
    explainer expects that the model will predict that this review is positive, based
    on the words *delightful*, *great*, and *beautiful*, and in spite of the word
    *bad*.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.29](../Text/06.xhtml#ch06fig29)所示，`plot_text_explanations()`突出了文本中的关键词，绿色代表支持证据，红色代表矛盾。证据越强，颜色越深。在这里，解释器期望模型会根据单词*delightful*、*great*和*beautiful*预测这条评论是积极的，尽管有单词*bad*。
- en: '[PRE39]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Figure 6.29\. Text explanation of the prediction in [listing 6.26](../Text/06.xhtml#ch06ex26)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29\. [列表6.26](../Text/06.xhtml#ch06ex26)中预测的文本解释
- en: '![](Images/06fig29_alt.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![图6.29的替代文本](Images/06fig29_alt.jpg)'
- en: Let’s look at a couple more reviews, including one that the model misclassified.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看几篇评论，包括一篇模型分类错误的评论。
- en: Listing 6.27\. Examining two more reviews
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.27. 检查两篇更多的评论
- en: '[PRE40]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Both these reviews are negative.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这两篇评论都是负面的。
- en: ❷ The model misclassified the second review.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型错误地将第二篇评论分类。
- en: As shown in [figure 6.30](../Text/06.xhtml#ch06fig30), the explainer expects
    that the model will classify the first review as negative, based mostly on the
    words *pointless* and *boring*. It expects that the model will classify the second
    review as positive, based on the words *8*, *sensitive*, and *seen*, and in spite
    of the words *bad* and (somewhat surprisingly) *better*.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.30](../Text/06.xhtml#ch06fig30)所示，解释器预计模型将主要基于单词*pointless*和*boring*将第一篇评论分类为负面。它预计模型将基于单词*8*、*sensitive*和*seen*将第二篇评论分类为正面，尽管有单词*bad*和（有些令人惊讶地）*better*。
- en: Figure 6.30\. Explanation visualizations for the two sample reviews in [listing
    6.27](../Text/06.xhtml#ch06ex27)
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30. [列表6.27](../Text/06.xhtml#ch06ex27)中的两个样本评论的解释可视化
- en: '![](Images/06fig30_alt.jpg)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig30_alt.jpg)'
- en: 'Note that according to [figure 6.30](../Text/06.xhtml#ch06fig30), the probability
    of the classification of the second review appears to be 0.51—in other words,
    the explainer expects that the model won’t be sure of its prediction at all. Let’s
    compare this to what the model predicted in reality:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，根据[图6.30](../Text/06.xhtml#ch06fig30)，第二次评论的分类概率似乎为0.51——换句话说，解释器预计模型对其预测将完全没有把握。让我们将其与模型在现实中的预测进行比较：
- en: '[PRE41]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The model actually predicts the label 1 with probability 0.6: not a confident
    prediction, but slightly more confident than the explainer estimated (though still
    wrong). The discrepancy is because the label and probability that the explainer
    returns are from the predictions of the linear approximation to the model, *not*
    from the model itself. You may occasionally even see cases where the explainer
    and the model return different labels for the same example. This will usually
    happen when the explanation fit is poor, so you don’t want to trust those explanations,
    anyway.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 模型实际上以0.6的概率预测标签1：这不是一个自信的预测，但比解释器估计的要稍微自信一些（尽管仍然是错误的）。差异在于解释器返回的标签和概率来自模型线性近似的预测，*而不是*来自模型本身。你偶尔甚至可能会看到解释器和模型对同一示例返回不同的标签的情况。这通常发生在解释器拟合不佳时，因此你根本不应该相信那些解释。
- en: As the data scientist responsible for classifying reviews, you may wonder about
    the seemingly high importance of the number *8*. On reflection, you might remember
    that some movie reviews include the ratings “8 out of 10,” or “8/10.” This may
    lead you to consider extracting apparent ratings out of the reviews before passing
    them to the text processor, and adding them to the model as an additional special
    feature. You may also not like using words like *seen* or *idea* as features.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 作为负责分类评论的数据科学家，你可能会对数字*8*看似很高的重要性感到好奇。经过反思，你可能记得一些电影评论中包含评分“8分/10分”，或者“8/10”。这可能会让你考虑在将评论传递给文本处理器之前提取明显的评分，并将它们作为额外的特殊特征添加到模型中。你可能也不喜欢使用像*seen*或*idea*这样的词作为特征。
- en: As a simple experiment, you can try removing the numbers 1 through 10 from the
    vocabulary,^([[14](../Text/06.xhtml#ch06fn14)]) and then refitting the model.
    The new model correctly classifies `test_10294` and returns a more reasonable
    explanation, as shown in [figure 6.31](../Text/06.xhtml#ch06fig31).
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项简单的实验，你可以尝试从词汇表中移除数字1到10，然后重新调整模型。新的模型正确地分类了`test_10294`，并返回了一个更合理的解释，如图6.31所示。
- en: ^(14)
  id: totrans-555
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([14](../Text/06.xhtml#ch06fn14))
- en: ''
  id: totrans-556
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This involves adding the numbers 1 through 10 as strings to the stopword list
    in the function `create_pruned_vocabulary()` in the file lime_imdb_example.R.
    We leave recreating the vocabulary and document-term matrices, and refitting the
    review classifier, as an exercise for the reader.
  id: totrans-557
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这涉及到将数字1到10作为字符串添加到文件lime_imdb_example.R中`create_pruned_vocabulary()`函数的停用词列表中。我们将重新创建词汇表和文档-词矩阵，以及重新调整评论分类器，作为读者的练习。
- en: Figure 6.31\. Explanation visualizations for `test_10294`
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31. `test_10294`的解释可视化
- en: '![](Images/06fig31_alt.jpg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig31_alt.jpg)'
- en: Looking at the explanations of other reviews that the model misclassifies can
    lead you to improved feature engineering or data preprocessing that can potentially
    improve your model. You may decide that sequences of words (*good idea*, rather
    than just *idea*) make better features. Or you may decide that you want a text
    representation and model that looks at the order of the words in the document
    rather than just word frequencies. In any case, looking at explanations of a model’s
    predictions on corner cases can give you insight into your model, and help you
    decide how to better achieve your modeling goals.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 查看模型对其他被错误分类的评论的解释可以引导你改进特征工程或数据预处理，这可能会提高你的模型。你可能会决定，单词序列（*好主意*，而不是仅仅*主意*）是更好的特征。或者你可能会决定，你想要一个文本表示和模型，它查看文档中单词的顺序，而不仅仅是单词频率。无论如何，查看模型对边缘情况的预测解释可以让你对你的模型有更深入的了解，并帮助你决定如何更好地实现你的建模目标。
- en: Summary
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You now have some solid ideas on how to choose among modeling techniques. You
    also know how to evaluate the quality of data science work, be it your own or
    that of others. The remaining chapters of [part 2](../Text/p2.xhtml#part02) of
    the book will go into more detail on how to build, test, and deliver effective
    predictive models. In the next chapter, we’ll actually start building predictive
    models.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对如何选择建模技术有了些稳固的想法。你也知道了如何评估数据科学工作的质量，无论是自己的还是他人的。本书第二部分[第2部分](../Text/p2.xhtml#part02)的剩余章节将更详细地介绍如何构建、测试和交付有效的预测模型。在下一章，我们将实际开始构建预测模型。
- en: In this chapter you have learned
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了
- en: How to match the problem you want to solve to appropriate modeling approaches.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将你想要解决的问题与适当的建模方法相匹配。
- en: How to partition your data for effective model evaluation.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何划分你的数据以进行有效的模型评估。
- en: How to calculate various measures for evaluating classification models.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算用于评估分类模型的各项指标。
- en: How to calculate various measures for evaluating scoring (regression) models.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算用于评估评分（回归）模型的各项指标。
- en: How to calculate various measures for evaluating probability models.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算用于评估概率模型的各项指标。
- en: How to use the `lime` package to explain individual predictions from a model.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用`lime`包解释模型中的单个预测。
