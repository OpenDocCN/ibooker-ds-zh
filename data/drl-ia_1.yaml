- en: Part 2\. Above and beyond
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2部分。超越
- en: After mastering the basics of deep reinforcement learning in [part 1](kindle_split_009.html#part01),
    [part 2](#part02) delves into a variety of more advanced techniques and tackles
    more sophisticated environments. The chapters in this part can more or less be
    approached in any order as they do not rely on each other. However, each chapter
    tends to be more complex than the previous one, so it still may be better to go
    in sequence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握深度强化学习基础知识[第1部分](kindle_split_009.html#part01)之后，[第2部分](#part02)深入探讨各种更高级的技术，并应对更复杂的环境。本部分中的章节可以大致按任何顺序进行，因为它们之间并不相互依赖。然而，每个章节通常比前一个章节更复杂，因此按顺序进行可能仍然更好。
- en: In [chapter 6](kindle_split_016.html#ch06) we’ll introduce an alternative framework
    for training neural networks using ideas borrowed from the biological sciences.
    In particular, we’ll adapt Charles Darwin’s theory of evolution by natural selection
    for machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_016.html#ch06)中，我们将介绍一个使用来自生物科学思想的替代框架来训练神经网络的方案。特别是，我们将将查尔斯·达尔文的自然选择进化理论应用于机器学习。
- en: In [chapter 7](kindle_split_017.html#ch07) we’ll show that most approaches to
    reinforcement learning are impoverished in how they represent states of the environment,
    and we’ll fix that by modeling a full probability distribution. In [chapter 8](kindle_split_018.html#ch08)
    we’ll show you how to imbue reinforcement learning agents with a sense of human-like
    curiosity. In [chapter 9](kindle_split_019.html#ch09) we’ll extend what we’ve
    learned training individual reinforcement learning agents to scenarios with dozens
    or hundreds of agents all interacting together.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](kindle_split_017.html#ch07)中，我们将展示大多数强化学习方法的不足之处在于它们如何表示环境的状态，我们将通过建模完整的概率分布来解决这个问题。在[第8章](kindle_split_018.html#ch08)中，我们将向您展示如何赋予强化学习代理类似人类的求知欲。在[第9章](kindle_split_019.html#ch09)中，我们将扩展我们训练单个强化学习代理所学的知识，以应对数十或数百个代理相互作用的场景。
- en: In [chapter 10](kindle_split_020.html#ch10) we’ll tackle one last major project
    to implement a machine learning model with a crude form of symbolic reasoning.
    This will allow us to inspect the internal behavior of the neural network and
    make the model more interpretable. Finally, [chapter 11](kindle_split_021.html#ch11)
    briefly reviews the core concepts in the book and provides a roadmap for further
    study.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](kindle_split_020.html#ch10)中，我们将实施一个最后的主要项目，以实现一个具有粗糙形式符号推理的机器学习模型。这将使我们能够检查神经网络的内部行为，并使模型更具可解释性。最后，[第11章](kindle_split_021.html#ch11)简要回顾了本书的核心概念，并为进一步学习提供了路线图。
- en: 'Chapter 6\. Alternative optimization methods: Evolutionary algorithms'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章。替代优化方法：进化算法
- en: '*This chapter covers*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Evolution algorithms for solving optimization problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于解决优化问题的进化算法
- en: Pros and cons of evolutionary approaches versus previous algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与先前算法相比，进化方法的优缺点
- en: Solving the CartPole game without backpropagation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用反向传播解决CartPole游戏
- en: Why evolutionary strategies can scale better than other algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么进化策略可以比其他算法扩展得更好
- en: Neural networks were loosely inspired by real biological brains, and convolutional
    neural networks were also inspired by the biological mechanism of vision. There
    is a long tradition of advances in technology and engineering being motivated
    by biological organisms. Nature, through the process of evolution by natural selection,
    has solved many problems elegantly and efficiently. Naturally, people wondered
    whether evolution itself could be borrowed and implemented on a computer to generate
    solutions to problems. As you will see, we can indeed harness evolution to solve
    problems, and it works surprisingly well and is relatively easy to implement.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络松散地受到真实生物大脑的启发，卷积神经网络也受到视觉生物机制的启发。技术进步和工程发展的长期传统是由生物体激发的。自然通过自然选择的过程，以优雅和高效的方式解决了许多问题。自然地，人们想知道进化本身是否可以被借鉴并在计算机上实现，以生成问题的解决方案。正如您将看到的，我们确实可以利用进化来解决问题，并且它出奇地有效，并且相对容易实现。
- en: In natural evolution, biological traits change and new traits are generated
    simply by the fact that some traits confer a survival and reproduction advantage
    that results in those organisms being able to seed more copies of their genes
    in the next generation. The survival advantage of a gene depends entirely on the
    environment, which is often unpredictable and dynamic. Our use cases for simulated
    evolution are much simpler, since we generally want to maximize or minimize a
    single number, such as the loss when training a neural network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然进化中，生物特性通过某些特性赋予生存和繁殖优势而改变，这些优势使得这些生物能够在下一代中产生更多基因副本。一个基因的生存优势完全取决于环境，环境通常是不可预测和动态的。我们用于模拟进化的用例要简单得多，因为我们通常希望最大化或最小化一个单一的数字，例如在训练神经网络时的损失。
- en: In this chapter you will learn how to use simulated evolutionary algorithms
    to train neural networks for use in reinforcement learning without using backpropagation
    and gradient descent.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用模拟进化算法来训练神经网络，用于强化学习，而不使用反向传播和梯度下降。
- en: 6.1\. A different approach to reinforcement learning
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1。强化学习的一种不同方法
- en: Why would we even think about abandoning backpropagation? Well, with both DQN
    and policy gradient approaches we created one agent whose policy depended on a
    neural network to approximate the Q function or policy function. As shown in [figure
    6.1](#ch06fig01), the agent interacts with the environment, collects experiences,
    and then uses backpropagation to improve the accuracy of its neural network and,
    hence, its policy. We needed to carefully tune several hyperparameters ranging
    from selecting the right optimizer function, mini-batch size, and learning rate
    so that the training would be stable and successful. Since the training of both
    DQN and policy gradient algorithms relies on stochastic gradient descent, which
    as the name suggests relies on noisy gradients, there is no guarantee that these
    models will successfully learn (i.e., converge on a good local or global optimum).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么会考虑放弃反向传播呢？嗯，在DQN和策略梯度方法中，我们创建了一个代理，其策略依赖于一个神经网络来近似Q函数或策略函数。如图6.1所示，代理与环境交互，收集经验，然后使用反向传播来提高其神经网络的准确性，从而提高其策略的准确性。我们需要仔细调整多个超参数，从选择正确的优化器函数、批量大小和学习率，以确保训练稳定且成功。由于DQN和策略梯度算法的训练都依赖于随机梯度下降，正如其名称所暗示的，它依赖于噪声梯度，因此无法保证这些模型能够成功学习（即收敛到一个好的局部或全局最优）。
- en: Figure 6.1\. For the past algorithms that we covered, our agent interacted with
    environment, collected experiences, and then learned from those experiences. We
    repeated the same process over and over for each epoch until the agent stopped
    learning.
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1。对于之前我们讨论的算法，我们的代理与环境交互，收集经验，然后从这些经验中学习。我们重复进行相同的过程，直到代理停止学习。
- en: '![](06fig01_alt.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![06fig01_alt.jpg](06fig01_alt.jpg)'
- en: Depending on the environment and the complexity of the network, creating an
    agent with the right hyperparameters may be incredibly difficult. Moreover, in
    order to use gradient descent and backpropagation, we need a model that is differentiable.
    There are certainly interesting and useful models you could construct that might
    be impossible to train with gradient descent due to the lack of differentiability.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 根据环境和网络的复杂性，创建具有正确超参数的代理可能非常困难。此外，为了使用梯度下降和反向传播，我们需要一个可微分的模型。当然，你可以构建一些有趣和有用的模型，但由于缺乏可微分性，可能无法使用梯度下降进行训练。
- en: Instead of creating one agent and improving it, we can instead learn from Charles
    Darwin and use evolution by (un)natural selection. We could spawn multiple different
    agents with different parameters (weights), observe which ones did the best, and
    “breed” the best agents such that the descendants could inherit their parents’
    desirable traits—just like in natural selection. We could emulate biological evolution
    using algorithms. We wouldn’t need to struggle to tune hyperparameters and wait
    for multiple epochs to see if an agent is learning “correctly.” We could just
    pick the agents that are already performing better ([figure 6.2](#ch06fig02)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是创建一个代理并改进它，我们可以从查尔斯·达尔文那里学习，并使用（非）自然选择进行进化。我们可以生成具有不同参数（权重）的多个不同代理，观察哪个表现最好，然后“繁殖”最佳代理，以便后代能够继承父母的优良特性——就像在自然选择中一样。我们可以使用算法来模拟生物进化。我们不需要努力调整超参数并等待多个epoch来查看代理是否“正确”地学习。我们只需选择已经表现更好的代理（如图6.2所示）。
- en: Figure 6.2\. Evolutionary algorithms are different from gradient descent-based
    optimization techniques. With evolutionary strategies, we generate agents and
    pass the most favorable weights down to the subsequent agents.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2\. 进化算法与基于梯度的优化技术不同。在进化策略中，我们生成代理，并将最有利的权重传递给后续代理。
- en: '![](06fig02_alt.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig02_alt.jpg)'
- en: This class of algorithms does not require an individual agent to learn. It does
    not rely on gradient descent and is aptly called a *gradient-free algorithm*.
    But just because individual agents are not being nudged toward some objective
    directly does not mean that we are relying on pure chance. The renowned evolutionary
    biologist Richard Dawkins once said, “Natural selection is anything but random.”
    Similarly, in our quest to build, or more accurately *discover*, the best agent,
    we will not be relying on pure chance. We will be selecting for the fittest amongst
    a population with a variance in traits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这类算法不需要单个代理进行学习。它不依赖于梯度下降，因此被称为*无梯度算法*。但仅仅因为单个代理没有被直接推向某个目标，并不意味着我们依赖于纯粹的偶然。著名的进化生物学家理查德·道金斯曾经说过：“自然选择绝非随机。”同样，在我们构建或更准确地说*发现*最佳代理的过程中，我们不会依赖于纯粹的偶然。我们将从具有性状差异的种群中选择最适应的个体。
- en: 6.2\. Reinforcement learning with evolution strategies
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 使用进化策略的强化学习
- en: In this section we’ll talk about how fitness plays into evolution strategies,
    and we’ll briefly cover the task of selecting the fittest agents. Next, we’ll
    work on how to recombine those agents into new agents and show what happens when
    we introduce mutations. This evolution is a multiple-generation process, so we’ll
    discuss that and recap the full training loop.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论适应度如何影响进化策略，并简要介绍选择最适应代理的任务。接下来，我们将研究如何将这些代理重新组合成新的代理，并展示当我们引入突变时会发生什么。这种进化是一个多代过程，因此我们将讨论这一点，并回顾完整的训练循环。
- en: 6.2.1\. Evolution in theory
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 理论上的进化
- en: If you remember from your high school biology class, natural selection selects
    for the “most fit” individuals from each generation. In biology this represents
    the individuals that had the greatest reproductive success, and hence passed on
    their genetic information to subsequent generations. Birds with beak shapes more
    adept at procuring seeds from trees would have more food and thus be more likely
    to survive to pass that beak shape gene to their children and grandchildren. But
    remember, “most fit” is relative to an environment. A polar bear is well adapted
    to the polar ice caps but would be very unfit in the Amazonian rainforests. You
    can think of the environment as determining an objective or fitness function that
    assigns individuals a fitness score based on their performance within that environment;
    their performance is determined solely by their genetic information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得你高中生物课的内容，自然选择会从每一代中选出“最适应”的个体。在生物学中，这代表着那些具有最大繁殖成功的个体，因此将他们的遗传信息传递给了下一代。那些能够从树上获取种子的喙形鸟类会有更多的食物，因此更有可能生存下来，并将这种喙形基因传递给他们的子女和孙子女。但记住，“最适应”是相对于环境而言的。北极熊适应了北极冰盖，但在亚马逊雨林中就会非常不适应。你可以把环境看作是确定一个目标或适应度函数，该函数根据个体在该环境中的表现为其分配适应度分数；他们的表现完全由他们的遗传信息决定。
- en: In biology, each mutation very subtly changes the characteristics of the organism,
    such that it may be difficult to discern one generation from another. However,
    allowing these mutations and variations to accumulate over multiple generations
    allows for perceptible changes. In the evolution of birds’ beaks, for example,
    a population of birds would initially have had roughly the same beak shape. But
    as time progressed, random mutations were introduced into the population. Most
    of these mutations probably did not impact the birds at all or even had a deleterious
    effect, but with a large enough population and enough generations, random mutations
    occurred that affected beak shapes favorably. Birds with better suited beaks would
    have an advantage getting food over the other birds, and therefore they’d have
    a higher likelihood of passing down their genes. Therefore, the next generation
    would have an increased frequency of the favorably shaped beak gene.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物学中，每一次突变都会非常微妙地改变生物体的特征，以至于可能难以区分一代与另一代。然而，允许这些突变和变异在多代中积累，可以产生可感知的变化。例如，在鸟类喙的进化中，一个鸟群的喙形状最初可能大致相同。但随着时间的推移，种群中引入了随机突变。这些突变中的大多数可能对鸟类没有任何影响，甚至可能产生有害效果，但有了足够大的种群和足够多的代数，随机突变发生了，这些突变对喙形状产生了有利影响。拥有更适合喙的鸟类在获取食物方面会优于其他鸟类，因此它们有更高的可能性将基因传给下一代。因此，下一代会有更多有利形状的喙基因频率。
- en: In *evolutionary reinforcement learning*, we are selecting for traits that give
    our agents the highest reward in a given environment, and by *traits* we mean
    model parameters (e.g., the weights of a neural network) or entire model structures.
    An RL agent’s fitness can be determined by the expected reward it would receive
    if it were to perform in the environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在*进化强化学习*中，我们正在选择那些能使我们的智能体在特定环境中获得最高奖励的特质，而当我们提到*特质*时，我们指的是模型参数（例如，神经网络的权重）或整个模型结构。一个强化学习智能体的适应性可以通过它在环境中表现时预期获得的奖励来决定。
- en: Let’s say agent A played the Atari game Breakout and was able to achieve an
    average score of 500 while agent B was only able to obtain 300 points. We would
    say that agent A is more fit than agent B and that we want our optimal agent to
    be more similar to agent A than B. Remember, the only reason why agent A would
    be more fit than agent B is because its model parameters were slightly more optimized
    to the environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设智能体A玩Atari游戏Breakout并能够达到平均分数500分，而智能体B只能获得300分。我们会说智能体A比智能体B更适应，我们希望我们的最优智能体与A更相似而不是B。记住，智能体A比智能体B更适应的唯一原因是它的模型参数对环境的优化略好。
- en: The objective in evolutionary reinforcement learning is exactly the same as
    in backpropagation and gradient descent-based training. The only difference is
    that we use this evolutionary process, which is often referred to as a *genetic
    algorithm*, to optimize the parameters of a model such as a neural network ([figure
    6.3](#ch06fig03)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 进化强化学习的目标与基于反向传播和梯度下降的训练目标完全相同。唯一的区别在于我们使用这种进化过程，通常被称为*遗传算法*，来优化模型参数，例如神经网络（[图6.3](#ch06fig03)）。
- en: Figure 6.3\. In an evolutionary algorithm approach to reinforcement learning,
    agents compete in an environment, and the agents that are more fit (those that
    generate more rewards) are preferentially copied to produce offspring. After many
    iterations of this process, only the most fit agents are left.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3。在进化算法方法中，强化学习的智能体在环境中竞争，适应性更强的智能体（产生更多奖励的智能体）被优先复制以产生后代。经过多次迭代这个过程，只剩下最适应的智能体。
- en: '![](06fig03_alt.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3](06fig03_alt.jpg)'
- en: The process is quite simple, but let’s run through the steps of a genetic algorithm
    in more detail. Let’s say we have a neural network that we want to use as an agent
    to play Gridworld, and we want to train it using a genetic algorithm. Remember,
    *training* a neural network just means iteratively updating its parameters such
    that its performance improves. Also recall that given a fixed neural network architecture,
    the parameters completely determine its behavior, so to copy a neural network
    we just need to copy its parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程相当简单，但让我们更详细地回顾一下遗传算法的步骤。假设我们有一个神经网络，我们想将其用作智能体来玩Gridworld，并想使用遗传算法来训练它。记住，*训练*一个神经网络只是指迭代更新其参数，使其性能提高。还请记住，给定一个固定的神经网络架构，参数完全决定了其行为，因此要复制一个神经网络，我们只需要复制其参数。
- en: 'Here’s how we would train such a neural network using a genetic algorithm (graphically
    depicted in [figure 6.4](#ch06fig04)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用遗传算法（如图6.4所示）训练此类神经网络的方法：
- en: We generate an initial population of random parameter vectors. We refer to each
    parameter vector in the population as an *individual*. Let’s say this initial
    population has 100 individuals.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成一个由随机参数向量组成的初始种群。我们将种群中的每个参数向量称为一个“个体”。假设这个初始种群有100个个体。
- en: We iterate through this population and assess the fitness of each individual
    by running the model in Gridworld with that parameter vector and recording the
    rewards. Each individual is assigned a fitness score based on the rewards it earns.
    Since the initial population is random, they will all likely perform very poorly,
    but there will be a few, just by chance, that will perform better than others.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过这个种群，并通过对每个个体使用该参数向量在Gridworld中运行模型并记录奖励来评估每个个体的适应度。每个个体根据其获得的奖励分配一个适应度得分。由于初始种群是随机的，它们的表现可能都非常差，但仅凭偶然，会有一些个体的表现优于其他个体。
- en: We randomly sample a pair of individuals (“parents”) from the population, weighted
    according to their relative fitness score (individuals with higher fitness have
    a higher probability of being selected) to create a “breeding population.”
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从种群中随机抽取一对个体（“父母”），根据它们的相对适应度得分进行加权（适应度更高的个体有更高的被选中概率）以创建一个“繁殖种群”。
- en: '|  |'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: There are many different methods of selecting “parents” for the next generation.
    One way is to simply map a probability of selection onto each individual based
    on their relative fitness score, and then sample from this distribution. In this
    way, the most fit will be selected most often, but there will still be a small
    chance of poor performers being selected. This may help maintain population diversity.
    Another way is to simply rank all the individuals and take the top *N* individuals,
    and use those to mate to fill the next generation. Just about any method that
    preferentially selects the top performers to mate will work, but some are better
    than others. There’s a tradeoff between selecting the best performers and reducing
    population diversity—this is very similar to the exploration versus exploitation
    tradeoff in reinforcement learning.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择下一代“父母”的方法有很多种。一种方法是将选择概率简单地映射到每个个体，基于它们的相对适应度得分，然后从这个分布中进行抽样。这样，最适应的个体将被最频繁地选中，但仍有小概率选中表现不佳的个体。这有助于维持种群多样性。另一种方法是将所有个体进行排名，并选择前*N*个个体，然后使用这些个体进行交配以填充下一代。几乎任何优先选择表现最好的个体进行交配的方法都能工作，但有些方法比其他方法更好。在选择最佳表现者和减少种群多样性之间有一个权衡——这与强化学习中的探索与利用权衡非常相似。
- en: '|  |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: Figure 6.4\. A genetic algorithm optimization of neural networks for reinforcement
    learning. A population of initial neural networks (the RL agents) is tested in
    the environment, earning rewards. Each individual agent is labeled by how fit
    it is, which is based on the rewards earned. Individuals are selected for the
    next generation based on their fitness; more fit individuals are more likely to
    be included in the next generation. The selected individuals “mate” and are “mutated”
    to increase genetic diversity.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4\. 用于强化学习的神经网络遗传算法优化。一组初始神经网络（强化学习代理）在环境中进行测试，获得奖励。每个个体代理根据其获得的奖励进行标记，这基于其适应度。个体根据其适应度被选中用于下一代；适应度更高的个体更有可能被包含在下一代中。选中的个体“交配”并“变异”以增加遗传多样性。
- en: '![](06fig04_alt.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](06fig04_alt.jpg)'
- en: 'The individuals in the breeding population will then “mate” to produce “offspring”
    that will form a new, full population of 100 individuals. If the individuals are
    simply parameter vectors of real numbers, mating vector 1 with vector 2 involves
    taking a subset from vector 1 and combining it with a complementary subset of
    vector 2 to make a new offspring vector of the same dimensions. For example, suppose
    you have vector 1: [1 2 3] and vector 2: [4 5 6]. Vector 1 mates with vector 2
    to produce [1 5 6] and [4 2 3]. We simply randomly pair up individuals from the
    breeding populations and recombine them to produce two new offspring until we
    fill up a new population. This creates new “genetic” diversity with the best performers.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '培育种群中的个体将“交配”以产生“后代”，从而形成一个由100个个体组成的新完整种群。如果个体仅仅是实数参数向量，那么向量1与向量2的交配涉及从向量1中取一个子集，并将其与向量2的一个互补子集结合，以形成一个相同维度的新的后代向量。例如，假设你有一个向量1:
    [1 2 3] 和向量2: [4 5 6]。向量1与向量2交配产生 [1 5 6] 和 [4 2 3]。我们只是随机配对培育种群中的个体，并将它们重新组合以产生两个新的后代，直到我们填满一个新的种群。这创造了新的“遗传”多样性，并保留了最佳表现者。'
- en: We now have a new population with the top solutions from the last generation,
    along with new offspring solutions. At this point, we will iterate over our solutions
    and randomly mutate some of them to make sure we introduce new genetic diversity
    into every generation to prevent premature convergence on a local optimum. Mutation
    simply means adding a little random noise to the parameter vectors. If these were
    binary vectors, mutation would mean randomly flipping a few bits; otherwise we
    might add some Gaussian noise. The mutation rate needs to be fairly low, or we’ll
    risk ruining the already present good solutions.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有一个新的种群，其中包含上一代的最优解，以及新的后代解。在这个时候，我们将迭代我们的解，并随机突变其中的一些，以确保我们向每一代引入新的遗传多样性，以防止过早收敛到局部最优。突变只是意味着向参数向量添加一点随机噪声。如果这些是二进制向量，那么突变意味着随机翻转几个比特；否则我们可能会添加一些高斯噪声。突变率需要相当低，否则我们可能会破坏已经存在的良好解。
- en: We now have a new population of mutated offspring from the previous generation.
    We repeat this process with the new population for *N* number of generations or
    until we reach *convergence* (which is when the average population’s fitness has
    stopped improving significantly).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在有一代新的突变后代。我们将用新的种群重复这个过程，进行*N*代或直到我们达到*收敛*（即平均种群适应度不再显著提高）。
- en: 6.2.2\. Evolution in practice
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2. 实践中的进化
- en: Before we dive into the reinforcement learning application, we’ll run a super
    simple genetic algorithm on an example problem for illustrative purposes. We will
    create a population of random strings and try to evolve them toward a target string
    of our choosing, such as “Hello World!”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入强化学习应用之前，我们将为了说明目的在一个示例问题上运行一个超级简单的遗传算法。我们将创建一个随机的字符串种群，并尝试将它们进化成我们选择的特定目标字符串，例如“Hello
    World！”
- en: Our initial population of random strings will look like “gMIgSkybXZyP” and “adlBOM
    XIrBH.” We’ll use a function that can tell us how similar these strings are to
    the target string to give us the fitness scores. We’ll then sample pairs of parents
    from the population weighted by their relative fitness scores, such that individuals
    with higher fitness scores are more likely to be chosen to become parents. Next
    we’ll mate these parents (also called *crossing* or *recombining*) to produce
    two offspring strings and add them to the next generation. We’ll also mutate the
    offspring by randomly flipping a few characters in the string. We’ll iterate this
    process and expect that the population will become enriched with strings very
    close to our target; probably at least one will hit our target exactly (at which
    point we’ll stop the algorithm). This evolutionary process for strings is depicted
    in [figure 6.5](#ch06fig05).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始的随机字符串种群将看起来像“gMIgSkybXZyP”和“adlBOM XIrBH。”我们将使用一个函数来告诉我们这些字符串与目标字符串的相似度，以给出适应度分数。然后我们将根据它们的相对适应度分数从种群中采样父母对，使得适应度分数较高的个体更有可能被选中成为父母。接下来我们将这些父母（也称为*交叉*或*重组*）交配以产生两个后代字符串，并将它们添加到下一代。我们还将通过在字符串中随机翻转一些字符来突变后代。我们将迭代这个过程，并期望种群将富含接近我们目标的字符串；可能至少有一个会完全命中我们的目标（此时我们将停止算法）。字符串的这种进化过程在[图6.5](#ch06fig05)中展示。
- en: Figure 6.5\. A string diagram outlining the major steps in a genetic algorithm
    for evolving a set of random strings toward a target string. We start with a population
    of random strings, compare each to the target string, and assign a fitness score
    to each string based on how similar it is to the target string. We then select
    high-fitness parents to “mate” (or recombine) to produce children, and then we
    mutate the children to introduce new genetic variance. We repeat the process of
    selecting parents and producing children until the next generation is full (when
    it’s the same size as the starting population).
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.5\. 一个字符串图，概述了将一组随机字符串进化到目标字符串的遗传算法的主要步骤。我们从一个随机字符串种群开始，将每个字符串与目标字符串进行比较，并根据每个字符串与目标字符串的相似度分配一个适应度分数。然后我们选择高适应度父母进行“交配”（或重新组合）以产生后代，然后我们突变后代以引入新的遗传变异。我们重复选择父母和产生后代的过程，直到下一代满员（其大小与起始种群相同）。
- en: '![](06fig05_alt.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig05_alt.jpg)'
- en: This is perhaps a silly example, but it’s one of the simplest demonstrations
    of a genetic algorithm, and the concepts will directly transfer to our reinforcement
    learning tasks. [Listings 6.1](#ch06ex01) through [6.4](#ch06ex04) show the code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个愚蠢的例子，但它是遗传算法最简单的演示之一，而且这些概念将直接转移到我们的强化学习任务中。[列表 6.1](#ch06ex01) 到 [6.4](#ch06ex04)
    展示了代码。
- en: In [listing 6.1](#ch06ex01) we begin by setting up the functions that will instantiate
    an initial population of random strings and also define a function that can compute
    a similarity score between two strings, which we will ultimately use as our fitness
    function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.1](#ch06ex01) 中，我们首先设置函数以实例化一个初始的随机字符串种群，并定义一个可以计算两个字符串之间相似度分数的函数，我们最终将使用它作为我们的适应度函数。
- en: 'Listing 6.1\. Evolving strings: set up random strings'
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1\. 进化字符串：设置随机字符串
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1*** The list of characters we sample from to produce random strings'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们从中采样以生成随机字符串的字符列表'
- en: '***2*** The string we’re trying to evolve from a random population'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 我们试图从随机种群中进化的字符串'
- en: '***3*** Sets up a simple class to store information about each member of the
    population'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置一个简单的类来存储关于种群每个成员的信息'
- en: '***4*** Computes a similarity metric between two strings, giving us a fitness
    score'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 计算两个字符串之间的相似度指标，给出适应度分数'
- en: '***5*** Produces an initial random population of strings'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 生成一个初始的随机字符串种群'
- en: The preceding code creates an initial population of individuals which are class
    objects composed of a string field and a fitness score field. Then it creates
    the random strings by sampling from a list of alphabetic characters. Once we have
    a population, we need to evaluate the fitness of each individual. For strings,
    we can compute a similarity metric using a built-in Python module called `SequenceMatcher`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个初始个体种群，这些个体是包含字符串字段和适应度分数字段的类对象。然后它通过从字母字符列表中采样来创建随机字符串。一旦我们有了种群，就需要评估每个个体的适应度。对于字符串，我们可以使用一个名为
    `SequenceMatcher` 的内置 Python 模块来计算相似度指标。
- en: In [listing 6.2](#ch06ex02), we define two functions, `recombine` and `mutate`.
    As their names suggest, the former will take two strings and recombine them to
    create two new strings, and the latter will randomly flip characters in a string
    to mutate them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.2](#ch06ex02) 中，我们定义了两个函数，`recombine` 和 `mutate`。正如它们的名称所暗示的，前者将接受两个字符串并将它们重新组合成两个新的字符串，而后者将通过随机翻转字符串中的字符来突变它们。
- en: 'Listing 6.2\. Evolving strings: recombine and mutate'
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. 进化字符串：重新组合和突变
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** Recombines two parent strings into two new offspring'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将两个父字符串重新组合成两个新的后代'
- en: '***2*** Mutates a string by randomly flipping characters'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 通过随机翻转字符来突变字符串'
- en: The preceding recombination function takes two parent strings like “hello there”
    and “fog world” and randomly recombines them by generating a random integer up
    to the length of the strings and taking the first piece of parent 1 and the second
    piece of parent 2 to create an offspring, such as “fog there” and “hello world”
    if the split happened in the middle. If we have evolved a string that contains
    part of what we want, like “hello” and another string that contains another part
    of what we want like “world,” then the recombination process might give us all
    of what we want.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述重新组合函数接受两个父字符串，如“hello there”和“fog world”，并通过生成一个随机整数（长度不超过字符串长度）来随机重新组合它们，取父
    1 的第一部分和父 2 的第二部分来创建一个后代，例如，如果分割发生在中间，则生成“fog there”和“hello world”。如果我们进化了一个包含我们想要的部分的字符串，如“hello”，另一个字符串包含我们想要的其他部分，如“world”，那么重新组合过程可能会给我们所有我们想要的东西。
- en: The mutation process takes a string like “hellb” and, with some small probability
    (the mutation rate), will replace a character in the string with a random one.
    For example, if the mutation rate was 20% (0.2), it is probable that at least
    one of the 5 characters in “hellb” will be mutated to a random character. Hopefully
    it will be mutated into “hello” if that is the target. The purpose of mutation
    is to introduce new information (variance) into the population. If all we did
    was recombine, it is likely that all the individuals in the population would become
    too similar too quickly, and we wouldn’t find the solution we wanted, because
    information gets lost each generation if there is no mutation. Note that the mutation
    rate is critical. If it’s too high, the fittest individuals will lose their fitness
    by mutation, and if it’s too low, we won’t have enough variance to find the optimal
    individual. Unfortunately, you have to find the right mutation rate empirically.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 突变过程会从一个像“hellb”这样的字符串开始，并且以一定的概率（突变率）将字符串中的一个字符替换为随机字符。例如，如果突变率为20%（0.2），那么在“hellb”中的5个字符中至少有一个可能会突变成随机字符。如果目标是“hello”，那么它可能会突变成“hello”。突变的目的在于将新的信息（变异性）引入种群。如果我们只进行重组，那么种群中的个体可能会迅速变得过于相似，我们也就找不到我们想要的解决方案，因为如果没有突变，信息会在每一代中丢失。请注意，突变率是至关重要的。如果太高，最适应的个体可能会因为突变而失去其适应性；如果太低，我们可能不会有足够的变异性来找到最优个体。不幸的是，你必须通过经验来找到合适的突变率。
- en: In [listing 6.3](#ch06ex03) we define a function that will loop through each
    individual in a population of strings, compute its fitness score, and associate
    it with that individual. We also define a function that will create the subsequent
    generation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6.3](#ch06ex03)中，我们定义了一个函数，该函数将遍历字符串种群中的每个个体，计算其适应度分数，并将其与该个体关联起来。我们还定义了一个函数，该函数将创建下一代。
- en: 'Listing 6.3\. Evolving strings: evaluate individuals and create new generation'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. 进化字符串：评估个体并创建新的一代
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Assigns a fitness score to each individual in the population'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 为种群中的每个个体分配适应度分数'
- en: '***2*** Generates a new generation by recombination and mutation'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 通过重组和突变生成新的一代'
- en: These are the last two functions we need to complete the evolutionary process.
    We have a function that evaluates each individual in the population and assigns
    a fitness score, which just indicates how similar the individual’s string is to
    the target string. The fitness score will vary depending on what the objective
    is for a given problem. Lastly, we have a function that generates a new population
    by sampling the most fit individuals in the current population, recombining them
    to produce offspring, and mutating them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们需要完成的最后两个函数，以完成进化过程。我们有一个函数可以评估种群中的每个个体，并为其分配一个适应度分数，这仅仅表示个体的字符串与目标字符串的相似程度。适应度分数将根据给定问题的目标而变化。最后，我们有一个函数可以通过采样当前种群中最适应的个体，将它们重新组合以产生后代，并对它们进行突变来生成新的种群。
- en: In [listing 6.4](#ch06ex04) we put everything together and iterate the previous
    steps to some maximum number of generations. That is, we start with an initial
    population, go through the process of fitness-scoring individuals and creating
    a new offspring population, and then repeat this sequence a number of times. After
    a sufficient number of generations, we expect the final population to be enriched
    with strings very close to our target string.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6.4](#ch06ex04)中，我们将所有内容组合在一起，并对前几步进行迭代，直到达到最大代数。也就是说，我们从一个初始种群开始，通过评估个体适应度并创建新的后代种群的过程，然后重复这一序列多次。经过足够多的代数后，我们期望最终种群中会包含与目标字符串非常接近的字符串。
- en: 'Listing 6.4\. Evolving strings: putting it all together'
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4\. 进化字符串：将所有内容组合在一起
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** Sets the mutation rate to 0.001%'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将突变率设置为0.001%'
- en: '***2*** Creates the initial random population'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建初始随机种群'
- en: '***3*** Records population average fitness over training time'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 记录训练时间内的种群平均适应度'
- en: 'If you run the algorithm, it should take a few minutes on a modern CPU. You
    can find the highest ranked individual in the population as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行该算法，它应该在现代CPU上花费几分钟。你可以按照以下方式找到种群中排名最高的个体：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It worked! You can also see the average fitness level of the population increasing
    each generation in [figure 6.6](#ch06fig06). This is actually a more difficult
    problem to optimize using an evolutionary algorithm because the space of strings
    is not continuous; it is hard to take small, incremental steps in the right direction
    since the smallest step is flipping a character. Hence, if you try making a longer
    target string, it will take much more time and resources to evolve.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 成功了！你还可以在[图6.6](#ch06fig06)中看到每一代种群的平均适应度水平都在增加。实际上，使用进化算法优化这个问题更困难，因为字符串空间不是连续的；由于最小的步骤是翻转一个字符，因此很难在正确的方向上采取小的增量步骤。因此，如果你尝试制作一个更长的目标字符串，进化将需要更多的时间和资源。
- en: Figure 6.6\. This is a plot of average population fitness over the generations.
    The average population fitness increases fairly monotonically and then plateaus,
    which looks promising. If the plot was very jagged, the mutation rate might be
    too high or the population size too low. If the plot converged too quickly, the
    mutation rate might be too low.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6。这是平均种群适应度随代数变化的图。平均种群适应度相当单调地增加，然后达到平台期，这看起来很有希望。如果图非常锯齿状，变异率可能太高或种群规模太小。如果图收敛得太快，变异率可能太低。
- en: '![](06fig06_alt.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig06_alt.jpg)'
- en: When we’re optimizing real-valued parameters in a model, even a small increase
    in value might improve the fitness, and we can exploit that, which makes optimization
    faster. But although discrete-valued individuals are harder to optimize in an
    evolutionary algorithm, they are *impossible* to optimize using vanilla gradient
    descent and backpropagation, because they are not differentiable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在模型中优化实值参数时，即使是很小的值增加也可能提高适应度，我们可以利用这一点，这使得优化更快。但是，尽管离散值个体在进化算法中更难优化，但它们使用传统的梯度下降和反向传播方法是*不可能*进行优化的，因为它们是不可微分的。
- en: 6.3\. A genetic algorithm for CartPole
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. CartPole的遗传算法
- en: Let’s see how this evolution strategy works in a simple reinforcement learning
    example. We’re going to use an evolutionary process to optimize an agent to play
    CartPole, the environment we introduced in [chapter 4](kindle_split_013.html#ch04)
    where the agent is rewarded for keeping the pole upright ([figure 6.7](#ch06fig07)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种进化策略在一个简单的强化学习示例中的工作方式。我们将使用进化过程来优化一个智能体来玩CartPole，这是我们[第4章](kindle_split_013.html#ch04)中介绍的环境，其中智能体会因为保持杆竖直而获得奖励（[图6.7](#ch06fig07)）。
- en: Figure 6.7\. We will use the CartPole environment to test our agent. The agent
    is rewarded by keeping the pole upright, and it can move the cart left or right.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7。我们将使用CartPole环境来测试我们的智能体。智能体通过保持杆竖直获得奖励，并且它可以左右移动小车。
- en: '![](06fig07.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig07.jpg)'
- en: We can represent an agent as a neural network that approximates the policy function—it
    accepts a state and outputs an action, or more typically a probability distribution
    over actions. The following listing shows an example of a three-layer network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个智能体表示为一个近似策略函数的神经网络——它接受一个状态并输出一个动作，或者更典型的是输出动作的概率分布。以下列表展示了三层网络的一个示例。
- en: Listing 6.5\. Defining an agent
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5。定义一个智能体
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1*** Unpacks the parameter vector into individual layer matrices'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将参数向量解包成单个层矩阵'
- en: '***2*** A simple linear layer with bias'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 一个简单的带偏置的线性层'
- en: '***3*** A rectified linear unit activation function'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 一个修正线性单元激活函数'
- en: '***4*** The last layer will output log probabilities over actions.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 最后一层将输出动作的对数概率。'
- en: The function in [listing 6.5](#ch06ex05) defines a 3-layer neural network. The
    first two layers use rectified linear unit activation functions, and the last
    layer uses a log-softmax activation function so that we get log probabilities
    over actions as the final output. Notice that this function expects an input state,
    `x`, and `unpacked_params`, which is a tuple of individual parameter matrices
    that are used in each layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.5](#ch06ex05)中的函数定义了一个三层神经网络。前两层使用修正线性单元激活函数，而最后一层使用log-softmax激活函数，以便我们得到作为最终输出的动作的对数概率。请注意，这个函数期望一个输入状态`x`和一个`unpacked_params`，它是一个元组，包含每个层中使用的单个参数矩阵。'
- en: To make the recombination and mutation process easier, we will create a population
    of parameter vectors (1-tensors) that we must then “unpack” or decompose into
    individual parameter matrices for use in each layer of the neural network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使重组和变异过程更容易，我们将创建一个参数向量（1-张量）的种群，然后我们必须将其“解包”或分解成单个参数矩阵，以便在神经网络的每一层中使用。
- en: Listing 6.6\. Unpacking a parameter vector
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6。解包参数向量
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1*** The layers parameter specifies the shape of each layer matrix.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 层参数指定了每个层矩阵的形状。'
- en: '***2*** Stores each individual layer tensor'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 存储每个单独的层张量'
- en: '***3*** Iterates through each layer'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 遍历每一层'
- en: '***4*** Unpacks the individual layer into matrix form'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将单个层解包成矩阵形式'
- en: '***5*** Adds the unpacked tensor to the list'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将解包的张量添加到列表中'
- en: The preceding function takes a flat parameter vector as the `params` input and
    a specification of the layers that it contains as the `layers` input, which is
    a list of tuples; it unpacks the parameter vector into a set of individual layer
    matrices and bias vectors stored in a list. The default set for `layers` specifies
    a 3-layer neural network, which therefore consists of 3 weight matrices with dimensions
    25 × 4, 10 × 25, and 2 × 10, and 3 bias vectors of dimensions 1 × 25, 1 × 10,
    and 1 × 2 for a total of 4 * 25 + 25 + 10 * 25 + 10 + 2 * 10 + 2 = 407 parameters
    in the flattened parameter vector.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数接受一个平面参数向量作为`params`输入，以及它包含的层的规范作为`layers`输入，这是一个元组的列表；它将参数向量解包成一系列单个层矩阵和偏置向量，这些向量存储在一个列表中。`layers`的默认设置指定了一个3层神经网络，因此它由3个维度为25
    × 4、10 × 25和2 × 10的权重矩阵以及维度为1 × 25、1 × 10和1 × 2的3个偏置向量组成，总共在平面参数向量中有407个参数。
- en: The only reason we’re adding this complexity of using flattened parameter vectors
    and unpacking them for use is that we want to be able to mutate over and recombine
    the entire set of parameters, which ends up being simpler overall and matches
    what we did with strings. An alternative approach would be to think of each layer’s
    neural network as an individual chromosome (if you remember the biology)—only
    matched chromosomes will recombine. Using this approach, you would only recombine
    parameters from the same layer. This would prevent information from later layers
    corrupting the earlier layers. We encourage you to try to implement it using this
    “chromosomal” approach as a challenge once you’re comfortable with the way we
    do it here. You’ll need to iterate over each layer, recombine, and mutate them
    separately.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加使用扁平参数向量和解包它们的复杂性的唯一原因是我们想要能够突变和重新组合整个参数集，这最终使得整个过程更简单，并且与我们对字符串的处理方式相匹配。另一种方法是，将每个层的神经网络视为一个单独的染色体（如果你记得生物学中的概念）——只有匹配的染色体才会重新组合。使用这种方法，你将只重新组合来自同一层的参数。这将防止后续层的信息破坏早期层。我们鼓励你在熟悉我们这里的方法后，尝试使用这种“染色体”方法作为挑战。你需要遍历每一层，分别重新组合和突变它们。
- en: Next let’s add a function to create a population of agents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加一个函数来创建一个代理种群。
- en: Listing 6.7\. Spawning a population
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7\. 生成种群
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1*** N is the number of individuals in the population; size is the length
    of the parameter vectors.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** N是种群中个体的数量；大小是参数向量的长度。'
- en: '***2*** Creates a randomly initialized parameter vector'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建一个随机初始化的参数向量'
- en: '***3*** Creates a dictionary to store the parameter vector and its associated
    fitness score'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建一个字典来存储参数向量和其关联的适应度分数'
- en: Each agent will be a simple Python dictionary that stores the parameter vector
    for that agent and the fitness score for that agent.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理将是一个简单的Python字典，存储该代理的参数向量和该代理的适应度分数。
- en: Next we implement the function that will recombine two parent agents to produce
    two new child agents.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个函数，该函数将重新组合两个父代代理以产生两个新的子代代理。
- en: Listing 6.8\. Genetic recombination
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.8\. 遗传重组
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1*** x1 and x2 are agents, which are dictionaries.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** x1和x2是代理，它们是字典。'
- en: '***2*** Extracts just the parameter vector'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 仅提取参数向量'
- en: '***3*** Randomly produces a split or crossover point'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 随机产生一个分割或交叉点'
- en: '***4*** The first child is produced by taking the first segment of parent 1
    and the second segment of parent 2.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 第一个子代是通过取父代1的第一个片段和父代2的第二个片段产生的。'
- en: '***5*** Creates new child agents by packaging the new parameter vectors into
    dictionaries'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 通过将新的参数向量打包成字典来创建新的子代代理'
- en: This function takes two agents who serve as parents and produces two children
    or offspring. It does so by taking a random split or crossover point, and then
    taking the first piece of parent 1 and combining it with the second piece of parent
    2, and likewise combines the second piece of parent 1 and the first piece of parent
    2\. This is exactly the same mechanism we used to recombine strings before.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受两个作为父母的智能体，并产生两个子女或后代。它是通过取一个随机的分割或交叉点，然后取父母 1 的第一部分与父母 2 的第二部分结合，同样结合父母
    1 的第二部分和父母 2 的第一部分来完成的。这正是我们之前用于重组字符串的相同机制。
- en: That was the first stage for populating the next generation; the second stage
    is to mutate the individuals with some fairly low probability. Mutation is the
    only source of new genetic information in each generation—recombination only shuffles
    around information that already exists.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 那是填充下一代的第一阶段；第二阶段是以相当低的概率变异个体。变异是每一代中新的遗传信息的唯一来源——重组只是重新排列已存在的信息。
- en: Listing 6.9\. Mutating the parameter vectors
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.9\. 修改参数向量
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1*** rate is the mutation rate, where 0.01 is a 1% mutation rate.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 速率是变异率，其中 0.01 是 1% 的变异率。'
- en: '***2*** Uses the mutation rate to decide how many elements in the parameter
    vector to mutate'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用变异率来决定要变异的参数向量中的元素数量'
- en: '***3*** Randomly resets the selected elements in the parameter vector'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 随机重置参数向量中选定的元素'
- en: We follow basically the same procedure as we did for strings; we randomly change
    a few elements of the parameter vector. The mutation rate parameter controls the
    number of elements that we change. We need to control the mutation rate carefully
    to balance the creation of new information that can be used to improve existing
    solutions and the destruction of old information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上遵循与字符串相同的程序；我们随机更改参数向量的一些元素。变异率参数控制我们更改的元素数量。我们需要仔细控制变异率，以平衡创建可用于改进现有解决方案的新信息与破坏旧信息。
- en: Next we need to assess the fitness of each agent by actually testing them on
    the environment (CartPole in our case).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过在实际环境中测试每个智能体（在我们的案例中是 CartPole）来评估每个智能体的适应性。
- en: Listing 6.10\. Testing each agent in the environment
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10\. 测试环境中的每个智能体
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1*** While game is not lost'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 游戏未失败时'
- en: '***2*** Gets the action probabilities from the model using the agent''s parameter
    vector'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用智能体的参数向量从模型中获取动作概率'
- en: '***3*** Probabilistically selects an action by sampling from a categorical
    distribution'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 通过从分类分布中进行采样以概率选择动作'
- en: '***4*** Keeps track of the number of time steps the game is not lost as the
    score'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将游戏未失败的时间步数作为得分跟踪'
- en: The `test_model` function takes an *agent* (a dictionary of a parameter vector
    and its fitness value) and runs it in the CartPole environment until it loses
    the game and returns the number of time steps it lasted as its score. We want
    to breed agents that can last longer and longer in CartPole (therefore achieving
    a high score).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_model` 函数接受一个 *智能体*（一个参数向量和其适应性值的字典）并在 CartPole 环境中运行它，直到游戏失败并返回其持续的时间步数作为其得分。我们希望培育出在
    CartPole 中可以持续更长时间（因此获得高分）的智能体。'
- en: We need to do this for all the agents in the population.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为种群中的所有智能体执行此操作。
- en: Listing 6.11\. Evaluate all the agents in the population
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.11\. 评估种群中的所有智能体
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1*** Total fitness for this population; used to later calculate the average
    fitness of the population'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 该种群的总适应性；用于稍后计算种群的平均适应性'
- en: '***2*** Iterates through each agent in the population'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 遍历种群中的每个智能体'
- en: '***3*** Runs the agent in the environment to assess its fitness'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 在环境中运行智能体以评估其适应性'
- en: '***4*** Stores the fitness value'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 存储适应性值'
- en: The `evaluate_population` function iterates through each agent in the population
    and runs `test_model` on them to assess their fitness.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_population` 函数遍历种群中的每个智能体，并在它们上运行 `test_model` 以评估其适应性。'
- en: The final main function we need is the `next_generation` function in [listing
    6.12](#ch06ex12). Unlike our string genetic algorithm from earlier, where we probabilistically
    selected parents based on their fitness score, here we employ a different selection
    mechanism. The *probabilistic selection mechanism* is similar to how we choose
    actions in a policy gradient method, and it works well there, but for choosing
    parents in a genetic algorithm, it often ends up leading to too rapid convergence.
    Genetic algorithms require more exploration than gradient-descent–based methods.
    In this case we’ll use a selection mechanism called *tournament-style selection*
    ([figure 6.8](#ch06fig08)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要的主要函数是[列表6.12](#ch06ex12)中的`next_generation`函数。与之前我们的基于字符串的遗传算法不同，在之前的算法中，我们根据适应性分数以概率选择父母，而在这里，我们采用不同的选择机制。*概率选择机制*类似于我们在策略梯度方法中选择动作的方式，并且在那里效果很好，但选择遗传算法中的父母时，它往往会导致过快的收敛。遗传算法比基于梯度下降的方法需要更多的探索。在这种情况下，我们将使用一种称为*锦标赛式选择*（[图6.8](#ch06fig08)）的选择机制。
- en: Figure 6.8\. In tournament selection we evaluate the fitness of all the individuals
    in the population as usual, and then we choose a random subset of the full population
    (in this figure just 2 of 4), and then choose the top individuals (usually 2)
    in this subset, mate them to produce offspring and mutate them. We repeat this
    selection process until we fill up the next generation.
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8。在锦标赛选择中，我们像往常一样评估种群中所有个体的适应性，然后随机选择整个种群的一个子集（在此图中为4个中的2个），然后选择该子集中的顶级个体（通常是2个），将它们配对产生后代并对其进行变异。我们重复此选择过程，直到填满下一代。
- en: '![](06fig08_alt.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig08_alt.jpg)'
- en: In tournament-style selection we select a random subset from the whole population
    and then choose the top two individuals in this subset as the parents. This ensures
    we don’t always select the same top two parents, but we do end up selecting the
    better-performing agents more often.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在锦标赛式选择中，我们从整个种群中随机选择一个子集，然后选择该子集的前两名个体作为父母。这确保我们不会总是选择相同的两个顶级父母，但最终我们会更频繁地选择表现更好的代理。
- en: We can change the *tournament size* (the size of the random subset) to control
    the degree to which we favor choosing the best agents in the current generation,
    at the risk of losing genetic diversity. In the extreme case, we could set the
    tournament size to be equal to the size of the population, in which case we would
    only select the top two individuals in the population. At the other extreme, we
    could make the tournament size 2, so that we are randomly selecting parents.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过改变*锦标赛大小*（随机子集的大小）来控制我们偏好在当前一代中选择最佳代理的程度，这可能会失去遗传多样性。在极端情况下，我们可以将锦标赛大小设置为与种群大小相等，在这种情况下，我们只会选择种群中的前两个个体。在另一个极端情况下，我们可以将锦标赛大小设置为2，这样我们就是随机选择父母。
- en: In this example we set the tournament size as a percentage of the size of the
    population. Empirically, tournament sizes of about 20% seem to work fairly well.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将锦标赛大小设置为种群大小的百分比。经验表明，大约20%的锦标赛大小似乎效果相当好。
- en: Listing 6.12\. Creating the next generation
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.12。创建下一代
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1*** While the new population is not full'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 当新种群尚未满员时'
- en: '***2*** Selects a percentage of the full population as a subset'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从整个种群中选择一定百分比的个体作为子集'
- en: '***3*** Subsets the population to get a batch of agents and matches each one
    with their index value in the original population'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将种群划分为一批代理，并将每个代理与原始种群中的索引值相匹配'
- en: '***4*** Sorts this batch in increasing order of score'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 按分数递增顺序对这一批次进行排序'
- en: '***5*** The last agents in the sorted batch are the agents with the highest
    scores; selects the top 2 as parents.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 在排序批次中，最后几个代理具有最高的分数；选择前两个作为父母。'
- en: '***6*** Recombines the parents to get offspring'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将父母重新组合以获得后代'
- en: '***7*** Mutates the children before putting them into the next generation'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 在将子代放入下一代之前对其进行变异'
- en: The `next_generation` function creates a list of random indices to index the
    population list and create a subset for a tournament batch. We use the `enumerate`
    function to keep track of the index positions of each agent in the subset so we
    can refer back to them in the main population. Then we sort the batch of fitness
    scores in ascending order and take the last two elements in the list as the top
    two individuals in that batch. We look up their indices and select the whole agent
    from the original population list.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`next_generation` 函数创建一个随机索引列表，用于索引种群列表并创建一个用于锦标赛批次的子集。我们使用 `enumerate` 函数来跟踪子集中每个代理的索引位置，以便我们可以在主种群中回溯它们。然后我们按升序排序批次的适应度分数，并取列表中的最后两个元素作为该批次的前两名个体。我们查找它们的索引，并从原始种群列表中选取整个代理。'
- en: Putting it all together, we can train a population of agents to play CartPole
    in just a handful of generations. You should experiment with the hyperparameters
    of mutation rate, population size, and number of generations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们只需几代就能训练一个种群来玩 CartPole。你应该尝试突变率、种群大小和代数等超参数。
- en: Listing 6.13\. Training the models
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.13\. 训练模型
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1*** The number of generations to evolve'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 进化的代数数量'
- en: '***2*** The number of individuals in each generation'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 每一代中的个体数量'
- en: '***3*** Initializes a population'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 初始化种群'
- en: '***4*** Evaluates the fitness of each agent in the population'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 评估种群中每个代理的适应度'
- en: '***5*** Populates the next generation'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 填充下一代'
- en: The first generation begins with a population of random parameter vectors, but
    by chance some of these will be better than the others, and we preferentially
    select these to mate and produce offspring for the next generation. To maintain
    genetic diversity, we allow each individual to be mutated slightly. This process
    repeats until we have individuals who are exceptionally good at playing CartPole.
    You can see in [figure 6.9](#ch06fig09) that the score steadily increases each
    generation of evolution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代开始于随机参数向量的种群，但出于偶然，其中一些会比其他的好，我们优先选择这些进行交配并产生下一代的后代。为了保持遗传多样性，我们允许每个个体发生轻微的变异。这个过程一直重复，直到我们拥有在玩
    CartPole 方面特别出色的个体。你可以在[图 6.9](#ch06fig09)中看到，每一代进化的分数都在稳步增加。
- en: Figure 6.9\. The average score of the population over generations in a genetic
    algorithm used to train agents to play CartPole.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.9\. 用于训练代理玩 CartPole 的遗传算法中，每一代种群的平均分数。
- en: '![](06fig09_alt.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig09_alt.jpg)'
- en: 6.4\. Pros and cons of evolutionary algorithms
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 进化算法的优缺点
- en: The algorithm we implemented in this chapter is a bit different from the previous
    approaches we’ve used in this book. There are circumstances where an evolutionary
    approach works better, such as with problems that would benefit more from exploration;
    other circumstances make it impractical, such as problems where it is expensive
    to gather data. In this section we’ll discuss the advantages and disadvantages
    of evolutionary algorithms and where you might benefit from using them over gradient
    descent.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本章实现的算法与我们在这本书中使用的先前方法略有不同。在某些情况下，进化方法效果更好，例如在那些从探索中获益更多的问题中；在其他情况下，它可能不切实际，例如在收集数据成本高昂的问题中。在本节中，我们将讨论进化算法的优点和缺点，以及你可能在哪些情况下从使用它们而不是梯度下降中受益。
- en: 6.4.1\. Evolutionary algorithms explore more
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 进化算法探索更多
- en: 'One advantage of gradient-free approaches is that they tend to explore more
    than their gradient-based counterparts. Both DQN and policy gradients followed
    a similar strategy: collect experiences and nudge the agent to take actions that
    led to greater rewards. As we discussed, this tends to cause agents to abandon
    exploring new states if they prefer an action already. We addressed this with
    DQN by incorporating an epsilon-greedy strategy, meaning there’s a small chance
    the agent will take a random action even if it has a preferred action. With the
    stochastic policy gradient we relied on drawing a variety of actions from the
    action probability vector output by our model.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 无梯度方法的优点是它们倾向于比基于梯度的方法探索更多。DQN 和策略梯度都遵循了类似的策略：收集经验和引导代理采取导致更大奖励的行动。正如我们讨论的，这往往会使得代理在偏好某个动作时放弃探索新状态。我们通过在
    DQN 中结合 epsilon-greedy 策略来解决此问题，这意味着代理即使有首选动作，也有一定几率采取随机动作。在随机策略梯度中，我们依赖于从我们的模型输出的动作概率向量中抽取各种动作。
- en: The agents in the genetic algorithm, on the other hand, are not nudged in any
    direction. We produce a lot of agents in each generation, and with so much random
    variation between them, most of them will have different policies than each other.
    There still is an exploration versus exploitation problem in evolutionary strategies
    because too little mutation can lead to premature convergence where the whole
    population becomes filled with nearly identical individuals, but it’s generally
    easier to ensure adequate exploration with genetic algorithms than with gradient
    descent-based ones.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，遗传算法中的代理（agents）不会被推向任何方向。我们在每一代中产生大量的代理，由于它们之间有如此多的随机变异，大多数代理将具有与彼此不同的策略。在进化策略中仍然存在探索与利用的问题，因为过少的变异可能导致过早收敛，整个种群被几乎完全相同的个体填满，但与基于梯度的算法相比，通常更容易确保遗传算法有足够的探索。
- en: 6.4.2\. Evolutionary algorithms are incredibly sample intensive
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 进化算法对样本的需求极其密集
- en: As you could probably see from the code in this chapter, we needed to run each
    agent in a population of 500 through the environment to determine their fitness.
    That means we needed to perform 500 major computations before we could make an
    update to the population. Evolutionary algorithms tend to be more sample hungry
    than gradient-based methods, since we aren’t strategically adjusting the weights
    of our agents; we are just creating lots of agents and hoping that the random
    mutations and recombinations we introduce are beneficial. We will say that evolutionary
    algorithms are less *data-efficient* than DQN or PG methods.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能从本章的代码中看到的那样，我们需要将种群中的每个代理500个通过环境运行以确定它们的适应性。这意味着在我们能够对种群进行更新之前，我们需要执行500次主要计算。进化算法通常比基于梯度的方法更“样本密集”，因为我们不是战略性地调整代理的权重；我们只是创建大量的代理，并希望我们引入的随机变异和重组是有益的。我们将说进化算法比DQN或PG方法“数据效率”更低。
- en: Suppose we want to decrease the size of the population to make the algorithm
    run faster. If we decrease the population size, there are fewer agents to select
    from when we are picking the two parents. This will make it likely that less fit
    individuals will make it into the next generation. We rely on a large number of
    agents being produced in hopes of finding a combination that leads to better fitness.
    Additionally, as in biology, mutations usually have a negative impact and lead
    to worse fitness. Having a larger population increases the probability that at
    least a few mutations will be beneficial.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要减小种群的大小以使算法运行得更快。如果我们减小种群大小，在挑选两个父母时可供选择的代理就少了。这将使得适应性较低的个体更有可能进入下一代。我们依赖于产生大量的代理，希望找到一种组合，从而带来更好的适应性。此外，正如在生物学中一样，变异通常具有负面影响并导致适应性变差。拥有更大的种群增加了至少有少数变异是有益的概率。
- en: Being data-inefficient is a problem if collecting data is expensive, such as
    in robotics or with autonomous vehicles. Having a robot collect one episode of
    data usually takes a couple of minutes, and we know from our past algorithms that
    training a simple agent takes hundreds if not thousands of episodes. Imagine how
    many episodes an autonomous vehicle would need to sufficiently explore its state
    space (the world). In addition to taking considerably more time, training with
    physical agents is much more expensive, since you need to purchase the robot and
    account for any maintenance. It would be ideal if we could train such agents without
    having to give them physical bodies.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果收集数据成本高昂，例如在机器人技术或自动驾驶车辆中，数据效率低下是一个问题。让机器人收集一次数据通常需要几分钟，而且我们知道从我们过去的算法中，训练一个简单的代理需要数百次甚至数千次数据。想象一下自动驾驶车辆需要探索其状态空间（世界）需要多少次数据。除了需要更多时间外，使用物理代理进行训练的成本也更高，因为你需要购买机器人并考虑任何维护费用。如果能不给他们提供物理身体就能训练这样的代理那就太理想了。
- en: 6.4.3\. Simulators
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3\. 模拟器
- en: Simulators address the preceding concerns. Instead of using an expensive robot
    or building a car with the necessary sensors, we could instead use computer software
    to emulate the experiences the environment would provide. For example, when training
    agents to drive autonomous cars, instead of equipping cars with the necessary
    sensors and deploying the model on physical cars, we could just train the agents
    inside software environments, such as the driving game Grand Theft Auto. The agent
    would receive as input the images of its surroundings, and it would be trained
    to output driving actions that would get the vehicle to the programmed destination
    as safely as possible.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器解决了上述问题。我们不必使用昂贵的机器人或构建带有必要传感器的汽车，而可以使用计算机软件来模拟环境提供的体验。例如，在训练智能体驾驶自动驾驶汽车时，我们不必在汽车上配备必要的传感器并在物理汽车上部署模型，而只需在软件环境中训练智能体，例如驾驶游戏《侠盗猎车手》。智能体将接收其周围环境的图像作为输入，并训练输出驾驶动作，以尽可能安全地将车辆开到预定目的地。
- en: Not only are simulators significantly cheaper to train agents with, but agents
    are able to train much more quickly since they can interact with the simulated
    environment much faster than in real life. If you need to watch and understand
    a two-hour movie, it will require two hours of your time. If you focus more intensely,
    you could probably increase the playback speed by two or three, dropping the amount
    of time needed to an hour or a bit less. A computer, on the other hand, could
    be finished before you’ve viewed the first act. For example, an 8 GPU computer
    (which could be rented from a cloud service) running ResNet-50, an established
    deep learning model for image classification, can process over 700 images per
    second. In a two-hour movie running at 24 frames per second (standard in Hollywood),
    there are 172,800 frames that need to be processed. This would require four minutes
    to finish. We could also effectively increase the playback speed for our deep
    learning model by dropping every few frames, which will drop our processing time
    to under two minutes. We could also throw more computers at the problem to increase
    processing power. For a more recent reinforcement learning example, the OpenAI
    Five bots were able to play 180 years of Dota 2 games each day. You get the picture—computers
    can process faster than we can, and that’s why simulators are valuable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅模拟器在训练智能体时成本显著更低，而且由于智能体可以比现实生活中更快地与模拟环境交互，它们能够更快地进行训练。如果你需要观看并理解一部两小时的电影，这将需要你两小时的时间。如果你更加专注，可能可以将播放速度提高两到三倍，将所需时间缩短到一小时或更少。另一方面，电脑可以在你观看第一幕之前就完成。例如，一台8
    GPU的电脑（可以从云服务中租用）运行ResNet-50，这是一个用于图像分类的成熟深度学习模型，每秒可以处理超过700张图像。在一部每秒24帧（好莱坞标准）的两小时电影中，需要处理172,800帧。这将需要四分钟来完成。我们还可以通过跳过每几个帧来有效地提高我们深度学习模型的播放速度，这将使我们的处理时间缩短到两分钟以下。我们还可以投入更多的电脑来增加处理能力。作为一个更近期的强化学习例子，OpenAI
    Five机器人每天能够玩180年的Dota 2游戏。你明白了——电脑的处理速度比我们快，这就是为什么模拟器有价值。
- en: 6.5\. Evolutionary algorithms as a scalable alternative
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5. 进化算法作为可扩展的替代方案
- en: If a simulator is available, the time and financial costs of collecting samples
    with evolutionary algorithms is less of an issue. In fact, producing a viable
    agent with evolutionary algorithms can sometimes be faster than gradient-based
    approaches because we do not have to compute the gradients via backpropagation.
    Depending on the complexity of the network, this will cut down the computation
    time by roughly 2–3 times. But there is another advantage of evolutionary algorithms
    that can allow them to train faster than their gradient counterparts—evolutionary
    algorithms can scale incredibly well when parallelized. We will discuss this in
    some detail in this section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有模拟器可用，使用进化算法收集样本的时间和财务成本就不再是问题。事实上，使用进化算法产生一个可行的智能体有时可能比基于梯度的方法更快，因为我们不需要通过反向传播来计算梯度。根据网络的复杂度，这可以将计算时间减少大约2-3倍。但进化算法的另一个优势是它们可以非常有效地进行并行化，这允许它们比梯度算法训练得更快。我们将在本节中详细讨论这一点。
- en: 6.5.1\. Scaling evolutionary algorithms
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1. 扩展进化算法
- en: OpenAI released a paper called “Evolutionary Strategies as a Scalable Alternative
    to Reinforcement Learning” by Tim Salimans et al. (2017), in which they described
    training agents incredibly quickly and efficiently by adding more machines. On
    a single machine with 18 CPU cores, they were able to make a 3D humanoid learn
    to walk in 11 hours. But with 80 machines (1,440 CPU cores) they were able to
    produce an agent in under 10 minutes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI发布了一篇由Tim Salimans等人撰写的论文，名为“Evolutionary Strategies as a Scalable Alternative
    to Reinforcement Learning”（2017年），其中他们描述了通过增加更多机器来快速且高效地训练代理。在一台拥有18个CPU核心的单台机器上，他们能够在11小时内让一个3D类人生物学会走路。但使用80台机器（1,440个CPU核心）的情况下，他们能够在10分钟内产生一个代理。
- en: You may be thinking that’s obvious—they just threw more machines and money at
    the problem. But this is actually trickier than it sounds, and other gradient-based
    approaches struggle to scale to that many machines.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为这是显而易见的——他们只是投入了更多的机器和金钱来解决这个问题。但实际上，这比听起来要复杂得多，其他基于梯度的方法在扩展到那么多机器时都遇到了困难。
- en: Let’s first look at how their algorithm differs from what we did earlier. *Evolutionary
    algorithm* is an umbrella term for a wide variety of algorithms that take inspiration
    from biological evolution and rely on iteratively selecting slightly better solutions
    from a large population to optimize a solution. The approach we implemented to
    play CartPole is more specifically called a *genetic algorithm*, because it more
    closely resembles the way biological genes get “updated” from generation to generation
    through recombination and mutation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看他们的算法与之前我们所做的是如何不同的。“进化算法”是一个总称，涵盖了从生物进化中汲取灵感的广泛算法，这些算法通过从大量种群中迭代选择略微更好的解决方案来优化解决方案。我们用来玩CartPole的算法更具体地被称为“遗传算法”，因为它更接近生物基因通过重组和突变从一代到下一代“更新”的方式。
- en: There’s another class of evolutionary algorithms confusingly termed *evolutionary
    strategies* (ES), which employ a less biologically accurate form of evolution,
    as illustrated in [figure 6.10](#ch06fig10).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一类被称为“进化策略”（ES）的进化算法，其名称令人困惑，它采用了一种不太准确的生物进化形式，如[图6.10](#ch06fig10)所示。
- en: Figure 6.10\. In an evolutionary strategy we create a population of individuals
    by repeatedly adding a small amount of random noise to a parent individual to
    generate multiple variants of the parent. We then assign fitness scores to each
    variant by testing them in the environment, and then we get a new parent by taking
    a weighted sum of all the variants.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10。在进化策略中，我们通过反复向一个父个体添加少量随机噪声来生成多个父个体的变体，从而创建一个个体种群。然后我们通过在环境中测试每个变体来为每个变体分配适应度分数，然后通过取所有变体的加权平均来获得一个新的父个体。
- en: '![](06fig10_alt.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig10_alt.jpg)'
- en: If we’re training a neural network with an ES algorithm, we start with a single
    parameter vector *θ**[t]*, sample a bunch of noise vectors of equal size (usually
    from a Gaussian distribution), such as *e[i]* ~ *N*(*μ*,*σ*), where *N* is a Gaussian
    distribution with mean vector *μ* and standard deviation *σ*. We then create a
    population of parameter vectors that are mutated versions of *θ**[t]* by taking
    ![](pg162.jpg). We test each of these mutated parameter vectors in the environment
    and assign them fitness scores based on their performance in the environment.
    Lastly, we get an updated parameter vector by taking a weighted sum of each of
    the mutated vectors, where the weights are proportional to their fitness scores
    ([figure 6.11](#ch06fig11)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用ES算法来训练神经网络，我们从一个单一的参数向量 *θ**[t]* 开始，采样一系列大小相等的噪声向量（通常来自高斯分布），例如 *e[i]*
    ~ *N*(*μ*,*σ*), 其中 *N* 是具有均值向量 *μ* 和标准差 *σ* 的高斯分布。然后我们创建一个参数向量种群，这些参数向量是 *θ**[t]*
    的突变版本，通过 ![](pg162.jpg) 来实现。我们在环境中测试每个突变参数向量，并根据它们在环境中的表现分配给它们适应度分数。最后，我们通过取每个突变向量的加权平均来获得一个更新的参数向量，其中权重与它们的适应度分数成正比
    ([图6.11](#ch06fig11))。
- en: Figure 6.11\. In an evolutionary strategy, at each time step we get an updated
    parameter vector by taking the old parameter vector and adding it to a weighted
    sum of the noise vectors, where the weights are proportional to the fitness scores.
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11。在进化策略中，在每一个时间步，我们通过将旧参数向量与噪声向量的加权平均相加来获得一个更新的参数向量，其中权重与适应度分数成正比。
- en: '![](06fig11.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig11.jpg)'
- en: This evolutionary strategy algorithm is significantly simpler than the genetic
    algorithm we implemented earlier since there is no mating step. We only perform
    mutation, and the recombination step does not involve swapping pieces from different
    parents but is just a simple weighted summation which is very easy to implement
    and computationally fast. As we’ll see, this approach is also easier to parallelize.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种进化策略算法比我们之前实现的遗传算法简单得多，因为没有配对步骤。我们只执行变异，重组步骤不涉及从不同父母交换部分，而只是一个简单的加权求和，这很容易实现且计算速度快。正如我们将看到的，这种方法也更容易并行化。
- en: 6.5.2\. Parallel vs. serial processing
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 并行与串行处理
- en: When we used a genetic algorithm to train agents to play CartPole, we had to
    sequentially iterate over each agent and let each agent play CartPole until it
    lost, in order to determine the fittest agent in each generation before we started
    the next run. If the agent takes 30 seconds to run through the environment, and
    we are determining the fitness for 10 agents, this will take 5 minutes. This is
    known as running a program *serially* ([figure 6.12](#ch06fig12)).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用遗传算法来训练代理玩CartPole时，我们必须按顺序迭代每个代理，并让每个代理玩CartPole直到它失败，以便在开始下一次运行之前确定每一代的最佳代理。如果代理运行环境需要30秒，而我们正在为10个代理确定适应度，这将需要5分钟。这被称为*串行*运行程序（[图6.12](#ch06fig12)）。
- en: Figure 6.12\. Determining the fitness of an agent is often the slowest step
    in a training loop and requires that we run the agent through the environment
    (possibly many times). If we are doing this on a single computer, we will be doing
    this in serial—we have to wait for one to finish running through the environment
    before we can start determining the fitness of the second agent. The time it takes
    to run this algorithm is a function of the number of agents and the time it takes
    to run through the environment for a single agent.
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12\. 确定代理的适应度通常是训练循环中最慢的一步，需要我们让代理在环境中运行（可能多次）。如果我们在一个计算机上做这件事，我们将按顺序进行——我们必须等待一个代理完成环境中的运行，然后我们才能开始确定第二个代理的适应度。运行此算法所需的时间是代理数量和单个代理运行环境所需时间的函数。
- en: '![](06fig12_alt.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig12_alt.jpg)'
- en: Determining each agent’s fitness will generally be the longest-running task
    in an evolutionary algorithm, but each agent can evaluate its own fitness independent
    of each other. But there’s no reason we need to wait for agent 1 to finish playing
    in the environment before we start evaluating agent 2\. We could instead run each
    agent in the generation on multiple computers at the same time. Each of the 10
    agents would go on 10 machines, and we can determine their fitness simultaneously.
    This means that completing one generation will take ~30 seconds on 10 machines
    as opposed to 5 minutes on one machine, a 10x speedup. This is known as running
    the process in *parallel* ([figure 6.13](#ch06fig13)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 确定每个代理的适应度通常将是进化算法中运行时间最长的任务，但每个代理可以独立于其他代理评估自己的适应度。但没有任何理由我们需要等待代理1完成环境中的游戏，然后才开始评估代理2。我们可以在多个计算机上同时运行这一代中的每个代理。每个10个代理将在10台机器上运行，我们可以同时确定它们的适应度。这意味着在10台机器上完成一代将需要大约30秒，而在一台机器上则需要5分钟，速度提升了10倍。这被称为在*并行*中运行过程（[图6.13](#ch06fig13)）。
- en: Figure 6.13\. If we have multiple machines at our disposal, we can determine
    the fitness of each agent on its own machine in parallel with each other. We do
    not have to wait for one agent to finish running through the environment before
    starting the next one. This will provide a huge speed up if we are training agents
    with a long episode length. You can see now that this algorithm is only a function
    of the time it takes to assess the fitness of a single agent, and not the number
    of agents we are assessing.
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13\. 如果我们有多个机器可供使用，我们可以并行地确定每个代理在其自己的机器上的适应度。我们不必等待一个代理完成环境中的运行，然后再开始下一个代理。如果我们正在训练具有长事件长度的代理，这将提供巨大的速度提升。现在你可以看到，这个算法只取决于评估单个代理适应度所需的时间，而不是我们正在评估的代理数量。
- en: '![](06fig13_alt.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig13_alt.jpg)'
- en: 6.5.3\. Scaling efficiency
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.3\. 缩放效率
- en: 'Now we can throw more machines and money at the problem, and we won’t have
    to wait nearly as long. In the previous hypothetical example where we added 10
    machines and got a 10x speedup—a scaling efficient of 1.0\. *Scaling efficiency*
    is a term used to describe how a particular approach improves as more resources
    are thrown at it and can be calculated as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以向问题投入更多的机器和资金，而且我们不必等待那么长时间。在之前的假设例子中，我们增加了10台机器并获得了10倍的速度提升——扩展效率为1.0。*扩展效率*是一个术语，用来描述随着更多资源投入而提高的特定方法，可以按照以下方式计算：
- en: '![](pg164_alt.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg164_alt.jpg)'
- en: In the real world, processes never have a scaling efficiency of 1\. There is
    always some additional cost to adding more machines that decreases efficiency.
    More realistically, adding 10 more machines will only give us a 9x speedup. Using
    the previous scaling efficiency equation we can calculate the scaling efficiency
    as 0.9 (which is pretty good in the real world).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，流程的扩展效率永远不会是1。增加更多机器总会有一些额外的成本，这会降低效率。更现实的情况是，增加10台机器只会给我们带来9倍的速度提升。使用之前的扩展效率方程，我们可以计算出扩展效率为0.9（这在现实世界中已经相当不错了）。
- en: Ultimately we need to combine the results from assessing the fitness of each
    agent in parallel so that we can recombine and mutate them. Thus, we need to use
    true parallel processing followed by a period of sequential processing. This is
    more generally referred to as *distributed computing* ([figure 6.14](#ch06fig14)),
    since we start with a single processor (often called the *master node*) and distribute
    tasks to multiple processors to run in parallel, and then collect the results
    back onto the master node.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们需要结合评估每个代理的适应度结果，以便我们可以重新组合和变异它们。因此，我们需要使用真正的并行处理，然后进行一段时间的顺序处理。这更一般地被称为*分布式计算*（[图6.14](#ch06fig14)），因为我们从一个处理器（通常称为*主节点*）开始，将任务分配给多个处理器并行运行，然后将结果收集回主节点。
- en: Figure 6.14\. A general schematic for how distributed computing works. A master
    node assigns tasks to worker nodes; the worker nodes perform those tasks and then
    send their results back to the master node (not shown).
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.14. 分布式计算的一般示意图。主节点将任务分配给工作节点；工作节点执行这些任务，然后将结果发送回主节点（未显示）。
- en: '![](06fig14.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig14.jpg)'
- en: Every step takes a little bit of network time to communicate between machines,
    which is something we would not encounter if we were running everything on a single
    machine. Additionally, if just one machine is slower than the others, the other
    workers will need to wait. To get the maximal scaling efficiency, we want to reduce
    the amount of communication between nodes as much as possible, both in terms of
    the number of times nodes need to send data as well as the amount of data that
    they send.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都需要一点网络时间来在机器之间进行通信，这是如果我们把所有事情都在单个机器上运行时不会遇到的情况。此外，如果有一台机器比其他机器慢，其他工作节点将需要等待。为了获得最大的扩展效率，我们希望尽可能减少节点之间的通信量，无论是节点需要发送数据的次数，还是它们发送的数据量。
- en: 6.5.4\. Communicating between nodes
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.4. 节点间的通信
- en: The researchers at OpenAI developed a neat strategy for distributed computing
    where each node sends only one number (not a whole vector) to each other node,
    eliminating the need for a separate master node. The idea is that each worker
    is first initialized with the same parent parameter vector. Then each worker adds
    a noise vector to its parent to create a slightly different child vector ([figure
    6.15](#ch06fig15)). Each worker then runs the child vector through the environment
    to get its fitness score. The fitness score from each worker is sent to all other
    workers, which just involves sending a single number. Since each worker has the
    same set of random seeds, each worker can recreate the noise vectors used by all
    the other workers. Lastly, each worker creates the same new parent vector and
    the process repeats.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的研究人员为分布式计算开发了一种巧妙策略，其中每个节点只向其他节点发送一个数字（而不是整个向量），从而消除了需要单独主节点的需求。这个想法是每个工作节点首先被初始化为相同的父参数向量。然后每个工作节点向其父节点添加一个噪声向量，以创建一个略微不同的子向量（[图6.15](#ch06fig15)）。然后每个工作节点将子向量通过环境运行以获取其适应度分数。每个工作节点的适应度分数被发送给所有其他工作节点，这仅仅涉及发送一个数字。由于每个工作节点都有相同的随机种子集，每个工作节点都可以重新创建所有其他工作节点使用的噪声向量。最后，每个工作节点创建相同的新父向量，然后过程重复。
- en: Figure 6.15\. The architecture derived from OpenAI’s distributed ES paper. Each
    worker creates a child parameter vector from a parent by adding noise to the parent.
    Then it evaluates the child’s fitness and sends the fitness score to all other
    agents. Using shared random seeds, each agent can reconstruct the noise vectors
    used to create the other vectors from the other workers without each having to
    send an entire vector. Lastly, new parent vectors are created by performing a
    weighted sum of the child vectors, weighted according to their fitness scores.
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.15\. 从 OpenAI 分布式 ES 论文中推导出的架构。每个工作节点通过向父节点添加噪声来从父节点创建一个子参数向量。然后它评估子节点的适应度并将适应度分数发送给所有其他智能体。使用共享随机种子，每个智能体可以重建用于从其他工作节点创建其他向量的噪声向量，而无需发送整个向量。最后，通过执行子向量的加权求和来创建新的父向量，加权系数根据其适应度分数确定。
- en: '![](06fig15_alt.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig15_alt.jpg)'
- en: Setting the random seed allows us to consistently generate the same random numbers
    every time, even on different machines. If you run the code in [listing 6.14](#ch06ex14),
    you will get the output shown, even though these numbers should be generated “randomly.”
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 设置随机种子允许我们每次都能一致地生成相同的随机数，即使在不同的机器上也是如此。如果您运行 [列表 6.14](#ch06ex14) 中的代码，您将得到显示的输出，尽管这些数字应该是“随机”生成的。
- en: Listing 6.14\. Setting the random seed
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.14\. 设置随机种子
- en: '[PRE14]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Seeding is important; it allows experiments involving random numbers to be reproduced
    by other researchers. If you do not supply an explicit seed, the system time or
    some other sort of variable number is used. If we came up with a novel RL algorithm,
    we would want others to be able to verify our work on their own machines. We would
    want the agent that another lab generated to be identical, to eliminate any source
    of error (and therefore doubt). That’s why it’s important we provide as much detail
    about our algorithm as possible—the architecture, the hyperparameters used, and
    sometimes the random seed we used. However, we hope we’ve developed an algorithm
    that is robust and that the particular set of random numbers generated doesn’t
    matter to the performance of the algorithm.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 种子设置很重要；它允许涉及随机数的实验可以被其他研究人员重现。如果您不提供明确的种子，系统时间或某种其他类型的变量数将被使用。如果我们提出了一种新的强化学习算法，我们希望其他人能够在自己的机器上验证我们的工作。我们希望另一个实验室生成的智能体与我们的智能体完全相同，以消除任何错误来源（以及因此产生的怀疑）。这就是为什么我们提供尽可能多的关于我们算法的详细信息很重要的原因——架构、使用的超参数，有时甚至是使用的随机种子。然而，我们希望我们已经开发出了一种鲁棒的算法，并且生成的特定随机数集对算法的性能没有影响。
- en: 6.5.5\. Scaling linearly
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.5\. 线性缩放
- en: Because the OpenAI researchers reduced the volume of data sent between the nodes,
    adding nodes did not affect the network significantly. They were able to scale
    to over a thousand workers linearly.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 OpenAI 研究人员减少了节点间传输的数据量，增加节点并没有对网络产生显著影响。他们能够线性扩展到超过一千个工作节点。
- en: '*Scaling linearly* means that for every machine added, we receive roughly the
    same performance boost as we did by adding the previous machine. This is denoted
    by a straight line on a graph of performance over resources, as seen in [figure
    6.16](#ch06fig16).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性缩放*意味着对于每增加一台机器，我们获得的性能提升与增加前一台机器时获得的提升大致相同。这在性能与资源的关系图上表示为一条直线，如图 [图 6.16](#ch06fig16)
    所示。'
- en: Figure 6.16\. Figure recreated from the OpenAI “Evolutionary Strategies as a
    Scalable Alternative to Reinforcement Learning” paper. The figure demonstrates
    that as more computing resources were added, the time improvement remained constant.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.16\. 从 OpenAI “进化策略作为可扩展强化学习替代方案”论文中重新创建的图。该图表明，随着更多计算资源的增加，时间改进保持恒定。
- en: '![](06fig16.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig16.jpg)'
- en: 6.5.6\. Scaling gradient-based approaches
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.6\. 缩放基于梯度的方法
- en: Gradient-based approaches can be trained on multiple machines as well. However,
    they do not scale nearly as well as ES. Currently, most distributed training of
    gradient-based approaches involves training the agent on each worker and then
    passing the gradients back to a central machine to be aggregated. All the gradients
    must be passed for each epoch or update cycle, which requires a lot of network
    bandwidth and strain on the central machine. Eventually the network gets saturated,
    and adding more workers does not improve training speed as well ([figure 6.17](#ch06fig17)).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的方法也可以在多台机器上训练。然而，它们的扩展性远不如ES。目前，大多数基于梯度的分布式训练涉及在每个工作器上训练代理，然后将梯度发送回中央机器进行汇总。每个epoch或更新周期都必须传递所有梯度，这需要大量的网络带宽并对中央机器造成压力。最终，网络会饱和，增加更多的工作器也不会提高训练速度（[图6.17](#ch06fig17)）。
- en: Figure 6.17\. The performance of current gradient-based approaches looks like
    this. In the beginning, there is a seemingly linear trend because the network
    has not been saturated. But eventually, as more resources are added, we get less
    and less of a performance boost.
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.17。当前基于梯度的方法的表现如下。一开始，由于网络尚未饱和，因此呈现出一种看似线性的趋势。但最终，随着资源的增加，性能提升越来越少。
- en: '![](06fig17_alt.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig17_alt.jpg)'
- en: Evolutionary approaches, on the other hand, do not require backpropagation,
    so they do not need to send gradient updates to a central server. And with smart
    techniques like the ones that OpenAI developed, they may only need to send a single
    number.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，进化方法不需要反向传播，因此它们不需要向中央服务器发送梯度更新。并且，通过像OpenAI开发的那种智能技术，它们可能只需要发送一个数字。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Evolutionary algorithms provide us with more powerful tools for our toolkit.
    Based on biological evolution, we
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法为我们提供了更强大的工具。基于生物进化，我们
- en: Produce individuals
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产生个体
- en: Select the best from the current generation
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从当前一代中选择最佳者
- en: Shuffle the genes around
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱基因
- en: Mutate them to introduce some variation
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过变异引入一些变化
- en: Mate them to create new generations for the next population
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将它们配对以创建下一代的种群
- en: Evolutionary algorithms tend to be more data hungry and less data-efficient
    than gradient-based approaches; in some circumstances this may be fine, notably
    if you have a simulator.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法通常比基于梯度的方法更“数据饥渴”且数据效率更低；在某些情况下这可能没问题，特别是如果你有一个模拟器。
- en: Evolutionary algorithms can optimize over nondifferentiable or even discrete
    functions, which gradient-based methods cannot do.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法可以优化非可微或甚至离散的函数，而基于梯度的方法无法做到。
- en: Evolutionary strategies (ES) are a subclass of evolutionary algorithms that
    do not involve biological-like mating and recombination, but instead use copying
    with noise and weighted sums to create new individuals from a population.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化策略（ES）是进化算法的一个子类，它不涉及类似生物的交配和重组，而是使用带噪声的复制和加权求和来从种群中创建新的个体。
- en: 'Chapter 7\. Distributional DQN: Getting the full story'
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第七章。分布式DQN：全面了解
- en: '*This chapter covers*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Why a full probability distribution is better than a single number
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么完整的概率分布比单个数字更好
- en: Extending ordinary deep Q-networks to output full probability distributions
    over Q values
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将普通深度Q网络扩展到输出Q值的完整概率分布
- en: Implementing a distributional variant of DQN to play Atari Freeway
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现DQN的分布变体以玩Atari Freeway
- en: Understanding the ordinary Bellman equation and its distributional variant
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解普通贝尔曼方程及其分布变体
- en: Prioritizing experience replay to improve training speed
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑经验回放以提高训练速度
- en: We introduced Q-learning in [chapter 3](kindle_split_012.html#ch03) as a way
    to determine the value of taking each possible action in a given state; the values
    were called action values or Q values. This allowed us to apply a policy to these
    action values and to choose actions associated with the highest action values.
    In this chapter we will extend Q-learning to not just determine a point estimate
    for the action values, but an entire distribution of action values for each action;
    this is called *distributional Q-learning*. Distributional Q-learning has been
    shown to result in dramatically better performance on standard benchmarks, and
    it also allows for more nuanced decision-making, as you will see. Distributional
    Q-learning algorithms, combined with some other techniques covered in this book,
    are currently considered a state-of-the-art advance in reinforcement learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](kindle_split_012.html#ch03)中介绍了Q学习，作为一种确定给定状态下每个可能动作的价值的方法；这些值被称为动作值或Q值。这使得我们可以将这些动作值应用于策略，并选择与最高动作值相关的动作。在本章中，我们将扩展Q学习，不仅确定动作值的点估计，而且确定每个动作的整个动作值分布；这被称为“分布式Q学习”。分布式Q学习已被证明在标准基准测试中表现出显著更好的性能，并且它还允许进行更细致的决策，正如您将看到的。分布式Q学习算法，结合本书中介绍的一些其他技术，目前被认为是强化学习中的一个最先进的进展。
- en: Most environments we wish to apply reinforcement learning to involve some amount
    of randomness or unpredictability, where the rewards we observe for a given state-action
    pair have some variance. In ordinary Q-learning, which we might call *expected-value
    Q-learning*, we only learn the average of the noisy set of observed rewards. But
    by taking the average, we throw away valuable information about the dynamics of
    the environment. In some cases, the rewards observed may have a more complex pattern
    than just being clustered around a single value. There may be two or more clusters
    of different reward values for a given state-action; for example, sometimes the
    same state-action will result in a large positive reward and sometimes in a large
    negative reward. If we just take the average, we will get something close to 0,
    which is never an observed reward in this case.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望应用强化学习的大多数环境都涉及一些随机性或不可预测性，其中观察到的给定状态-动作对的奖励存在一些变异性。在普通的Q学习中，我们可能称之为“期望值Q学习”，我们只学习观察到的噪声奖励集的平均值。但通过取平均值，我们丢弃了关于环境动态的有价值信息。在某些情况下，观察到的奖励可能比围绕单个值聚集的更复杂的模式。对于给定的状态-动作，可能有两个或更多不同奖励值的奖励簇；例如，有时相同的状态-动作会导致大的正奖励，有时会导致大的负奖励。如果我们只取平均值，我们将会得到接近0的值，而这在这个情况下永远不会是观察到的奖励。
- en: Distributional Q-learning seeks to get a more accurate picture of the distribution
    of observed rewards. One way to do this would be to keep a record of all the rewards
    observed for a given state-action pair. Of course, this would require a lot of
    memory, and for state spaces of high dimensionality it would be computationally
    impractical. This is why we must make some approximations. But first, let’s delve
    deeper into what expected-value Q-learning is missing, and what distributional
    Q-learning offers.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式Q学习试图获得观察到的奖励分布的更准确图景。实现这一目标的一种方法是对给定状态-动作对观察到的所有奖励进行记录。当然，这需要大量的内存，对于高维度的状态空间来说，这在计算上是不切实际的。这就是为什么我们必须做出一些近似。但首先，让我们更深入地探讨一下期望值Q学习缺少了什么，以及分布式Q学习提供了什么。
- en: 7.1\. What’s wrong with Q-learning?
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1. Q-learning有什么问题？
- en: The expected-value type of Q-learning we’re familiar with is flawed, and to
    illustrate this we’ll consider a real-world medical example. Imagine we are a
    medical company, and we want to build an algorithm to predict how a patient with
    high blood pressure (hypertension) will respond to 4-week course of a new anti-hypertensive
    drug called Drug X. This will help us decide whether or not to prescribe this
    drug to an individual patient.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所熟悉的期望值类型的Q学习是有缺陷的，为了说明这一点，我们将考虑一个现实世界的医学例子。想象一下，我们是一家医疗公司，我们想要构建一个算法来预测患有高血压（高血压）的患者对名为Drug
    X的新抗高血压药物4周疗程的反应。这将帮助我们决定是否给个别患者开这种药。
- en: We gather a bunch of clinical data by running a randomized clinical trial in
    which we take a population of patients with hypertension and randomly assign them
    to a treatment group (those who will get the real drug) and a control group (those
    who will get a placebo, an inactive drug). We then record blood pressure over
    time while the patients in each group are taking their respective drugs. At the
    end we can see which patients responded to the drug and how much better they did
    compared to the placebo ([figure 7.1](#ch07fig01)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过进行一项随机临床试验来收集大量临床数据，在这个试验中，我们选取了一群患有高血压的患者，并将他们随机分配到治疗组（将接受真实药物的患者）和对照组（将接受安慰剂，一种非活性药物）。然后，我们在每个组的患者服用各自药物的过程中记录血压随时间的变化。最后，我们可以看到哪些患者对药物有反应，以及与安慰剂相比，他们的改善程度如何（[图7.1](#ch07fig01)）。
- en: 'Figure 7.1\. In a randomized control trial of a drug, we study the outcome
    of some treatment compared to a placebo (a nonactive substance). We want to isolate
    the effect we are trying to treat, so we take a population with some condition
    and randomly sort them into two groups: a treatment group and a control group.
    The treatment group gets the experimental drug we are testing, and the control
    group gets the placebo. After some time, we can measure the outcome for both groups
    of patients and see if the treatment group, on average, had a better response
    than the placebo group.'
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1. 在一项药物的随机对照试验中，我们研究某种治疗与安慰剂（一种非活性物质）相比的结果。我们希望隔离我们试图治疗的效应，所以我们选取了一个具有某种条件的人群，并将他们随机分为两组：治疗组和对照组。治疗组接受我们正在测试的实验药物，对照组接受安慰剂。一段时间后，我们可以测量两组患者的结果，并看到治疗组的平均反应是否比安慰剂组更好。
- en: '![](07fig01_alt.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![07fig01_alt.jpg](07fig01_alt.jpg)'
- en: Once we’ve collected our data, we can plot a histogram of the change in blood
    pressure after four weeks on the drug for the treatment and control groups. We
    might see something like the results in [figure 7.2](#ch07fig02).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收集了数据，我们就可以绘制治疗组和对照组在服用药物四周后血压变化的直方图。我们可能会看到[图7.2](#ch07fig02)中的结果。
- en: 'Figure 7.2\. Histogram of the measured blood pressure change for the control
    and treatment groups in a simulated randomized control trial. The x-axis is the
    change in blood pressure from the start (before treatment) to after treatment.
    We want blood pressure to decrease, so negative numbers are good. We count the
    number of patients who have each value of blood pressure change, so the peak at
    –3 for the control group means that most of those patients had a blood pressure
    drop of 3 mmHg. You can see that there are two subgroups of patients in the treatment
    group: one group had a significant reduction in blood pressure, and another group
    had minimal to no effect. We call this a bimodal distribution, where mode is another
    word for “peak” in the distribution.'
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2. 模拟随机对照试验中对照组和治疗组的测量血压变化直方图。x轴是从开始（治疗前）到治疗后的血压变化。我们希望血压下降，所以负数是好的。我们计算具有每个血压变化值的患者的数量，因此对照组中-3的峰值意味着大多数患者血压下降了3毫米汞柱。你可以看到治疗组中有两个患者亚组：一个组血压显著下降，另一个组几乎没有效果。我们称这种分布为双峰分布，其中“峰”是分布中“峰值”的另一种说法。
- en: '![](07fig02_alt.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![07fig02_alt.jpg](07fig02_alt.jpg)'
- en: If you first look at the control group histogram in [figure 7.2](#ch07fig02),
    it appears to be a normal-like distribution centered around –3.0 mmHg (a unit
    of pressure), which is a fairly insignificant reduction in blood pressure, as
    you would expect from a placebo. Our algorithm would be correct to predict that
    for any patient given a placebo, their expected blood pressure change would be
    –3.0 mmHg on average, even though individual patients had greater or lesser changes
    than that average value.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你首先查看[图7.2](#ch07fig02)中的对照组直方图，它看起来像是一个以-3.0毫米汞柱（压力的单位）为中心的正常分布，这是一个相当不显著的血压降低，正如你从安慰剂中预期的。我们的算法正确地预测，对于任何接受安慰剂的患者，他们的平均血压变化将是-3.0毫米汞柱，尽管个别患者的血压变化可能大于或小于这个平均值。
- en: Now look at the treatment group histogram. The distribution of blood pressure
    change is bimodal, meaning there are two peaks, as if we had combined two separate
    normal distributions. The right-most mode is centered at –2.5 mmHg, much like
    the control group, suggesting that this subgroup within the treatment group did
    not benefit from the drug compared to the placebo. However, the left-most mode
    is centered at –22.3 mmHg, which is a very significant reduction in blood pressure.
    In fact, it’s greater than any currently existing anti-hypertensive drug. This
    again indicates that there is a subgroup within the treatment group, but this
    subgroup strongly benefits from the drug.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看治疗组的直方图。血压变化的分布是双峰的，这意味着有两个峰值，就像我们结合了两个单独的正态分布一样。最右侧的峰值位于-2.5 mmHg，与对照组非常相似，这表明这个治疗组内的子组与安慰剂相比没有从药物中获益。然而，最左侧的峰值位于-22.3
    mmHg，这是血压的显著降低。事实上，它比目前任何现有的抗高血压药物都要好。这再次表明，治疗组内存在一个子组，而这个子组强烈地从药物中获益。
- en: If you’re a physician, and a patient with hypertension walks into your office,
    all else being equal, should you prescribe them this new drug? If you take the
    expected value (the average) of the treatment group distribution, you’d only get
    about –13 mmHg change in blood pressure, which is between the two modes in the
    distribution. This is still significant compared to the placebo, but it’s worse
    than many existing anti-hypertensives on the market. By that standard, the new
    drug does not appear to be very effective, despite the fact that a decent number
    of patients get tremendous benefit from it. Moreover, the expected value of –13
    mmHg is very poorly representative of the distribution, since very few patients
    actually had that level of blood pressure reduction. Patients either had almost
    no response to the drug or a very robust response; there were very few moderate
    responders.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名医生，并且一位患有高血压的患者走进你的办公室，在其他条件相同的情况下，你应该给他们开这种新药吗？如果你取治疗组分布的期望值（平均值），你只会得到大约-13
    mmHg的血压变化，这位于分布的两个峰值之间。与安慰剂相比，这仍然是有意义的，但比市场上许多现有的抗高血压药物都要差。按照这个标准，新药看起来并不非常有效，尽管相当数量的患者从中获得了巨大的益处。此外，-13
    mmHg的期望值并不能很好地代表分布，因为实际上很少有患者的血压降低到那个水平。患者要么对药物几乎没有反应，要么有非常强烈的反应；很少有患者有适度的反应。
- en: '[Figure 7.3](#ch07fig03) illustrates the limitations of expected values compared
    to seeing the full distribution. If you use the expected values of blood pressure
    changes for each drug, and just pick the drug with the lowest expected value in
    terms of blood pressure change (ignoring patient-specific complexities, such as
    side effects), you will be acting optimally at the population level, but not necessarily
    at the individual level.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#ch07fig03)说明了期望值与观察完整分布相比的局限性。如果你使用每种药物血压变化的期望值，并且只选择血压变化期望值最低的药物（忽略患者特定的复杂性，如副作用），你将在群体层面上做出最优决策，但在个体层面上则不一定。'
- en: Figure 7.3\. Here we compare simulated Drug A to Drug X to see which lowers
    blood pressure the most. Drug A has a lower average (expected) value of –15.5
    mmHg and a lower standard deviation, but Drug X is bimodal with one mode centered
    at –22.5 mmHg. Notice that for Drug X virtually no patients had a blood pressure
    change that fell near the average value.
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3。在这里，我们比较模拟的药物A与药物X，看看哪个能降低血压最多。药物A的平均（期望）值为-15.5 mmHg，标准差更低，但药物X是双峰分布，一个峰值位于-22.5
    mmHg。请注意，对于药物X，几乎没有患者的血压变化接近平均值。
- en: '![](07fig03_alt.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig03_alt.jpg)'
- en: So what does this have to do with deep reinforcement learning? Well, Q-learning,
    as you’ve learned, gives us the expected (average, time-discounted) state-action
    values. As you might imagine, this can lead to the same limitations we’ve been
    discussing in the case of drugs, with multimodal distributions. Learning a full
    probability distribution of state-action values would give us a lot more power
    than just learning the expected value, as in ordinary Q-learning. With the full
    distribution, we could see if there is multimodality in the state-action values
    and how much variance there is in the distribution. [Figure 7.4](#ch07fig04) models
    the action-value distributions for three different actions, and you can see that
    some actions have more variance than others. With this additional information,
    we can employ risk-sensitive policies—policies that aim not merely to maximize
    expected rewards but also to control the amount of risk we take in doing so.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这与深度强化学习有什么关系呢？嗯，正如你所学的，Q-learning给我们提供了预期的（平均、时间折扣）状态-动作值。正如你可能想象的那样，这可能导致我们在药物案例中讨论过的相同限制，即多模态分布。学习状态-动作值的完整概率分布将比仅仅学习期望值（如普通Q-learning中那样）给我们带来更多的能力。有了完整的分布，我们可以看到状态-动作值中是否存在多模态，以及分布中有多少方差。[图7.4](#ch07fig04)
    模型了三种不同动作的动作值分布，你可以看到有些动作的方差比其他动作大。有了这些额外信息，我们可以采用风险敏感策略——这些策略不仅旨在最大化预期奖励，而且还要控制我们这样做所承担的风险量。
- en: 'Figure 7.4\. Top: An ordinary Q function takes a state-action pair and computes
    the associated Q value. Middle: A distributional Q function takes a state-action
    pair and computes a probability distribution over all possible Q values. Probabilities
    are bounded in the interval [0,1], so it returns a vector with all elements in
    [0,1] and their sum is 1\. Bottom: An example Q value distribution produced by
    the distributional Q function for three different actions for some state. Action
    A is likely to lead to an average reward of –5, whereas action B is likely to
    lead to an average reward of +4.'
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4\. 顶部：一个普通的Q函数接受一个状态-动作对并计算相关的Q值。中部：一个分布Q函数接受一个状态-动作对并计算所有可能的Q值的概率分布。概率被限制在区间[0,1]内，因此它返回一个所有元素都在[0,1]区间内且它们的和为1的向量。底部：对于某个状态，分布Q函数为三个不同的动作产生的示例Q值分布。动作A可能导致平均奖励为-5，而动作B可能导致平均奖励为+4。
- en: '![](07fig04_alt.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4](07fig04_alt.jpg)'
- en: 'Most convincingly, an empirical study was done that evaluated several popular
    variants and improvements to the original DQN algorithm, including a distributional
    variant of DQN, to see which were most effective alone and which were most important
    in combination (“Rainbow: Combining Improvements in Deep Reinforcement Learning”
    by Hessel et al., 2017). It turns out that distributional Q-learning was the best-performing
    algorithm overall, among all the individual improvements to DQN that they tested.
    They combined all the techniques together in a “Rainbow” DQN, which was shown
    to be far more effective than any individual technique. They then tested to see
    which components were most crucial to the success of Rainbow, and the results
    were that distributional Q-learning, multistep Q-learning (covered in [chapter
    5](kindle_split_014.html#ch05)), and prioritized replay (which will be briefly
    covered in [section 7.7](#ch07lev1sec7)) were the most important to the Rainbow
    algorithm’s performance.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '最有说服力的是，进行了一项实证研究，评估了原始DQN算法的几个流行变体和改进，包括DQN的分布变体，以查看哪些单独最有效，哪些在组合中最为重要（“Rainbow:
    Combining Improvements in Deep Reinforcement Learning” by Hessel et al., 2017）。结果发现，在所有他们测试的DQN的个别改进中，分布Q-learning是整体表现最好的算法。他们将所有技术结合在一起，形成了一个“Rainbow”
    DQN，这被证明比任何单一技术都更有效。然后他们测试了哪些组件对Rainbow的成功最为关键，结果是分布Q-learning、多步Q-learning（在第5章中介绍）和优先级重放（将在第7.7节中简要介绍）对Rainbow算法的性能最为重要。'
- en: In this chapter you will learn how to implement a distributional deep Q-network
    (Dist-DQN) that outputs a probability distribution over state-action values for
    each possible action given a state. We saw some probability concepts in [chapter
    4](kindle_split_013.html#ch04), where we employed a deep neural network as a policy
    function that directly output a probability distribution over actions, but we
    will review these concepts and go into even more depth here, as these concepts
    are important to understand in order to implement Dist-DQN. Our discussion of
    probability and statistics may seem a bit too academic at first, but it will become
    clear why we need these concepts for a practical implementation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将学习如何实现一个分布式深度Q网络（Dist-DQN），它为给定状态下的每个可能动作输出一个状态-动作值的概率分布。我们在[第4章](kindle_split_013.html#ch04)中看到了一些概率概念，在那里我们使用深度神经网络作为策略函数，直接输出动作的概率分布，但我们将回顾这些概念，并在这里进行更深入的探讨，因为这些概念对于实现Dist-DQN至关重要。我们关于概率和统计的讨论可能一开始看起来有点过于学术，但很快就会清楚为什么我们需要这些概念来进行实际的应用。
- en: This chapter is the most conceptually difficult chapter in the whole book as
    it contains a lot of probability concepts that are difficult to grasp at first.
    There is also more math here than in any other chapter. Getting through this chapter
    is a big accomplishment; you will learn or review a lot of fundamental topics
    in machine learning and reinforcement learning that will give you a greater grasp
    of these fields.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这章是整本书中最概念上最难的一章，因为它包含了许多在最初难以掌握的概率概念。这里的数学内容也比其他任何章节都要多。通过这一章是一个巨大的成就；你将学习或复习许多机器学习和强化学习的基本主题，这将使你对这些领域有更深入的理解。
- en: 7.2\. Probability and statistics revisited
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2. 概率和统计回顾
- en: While the mathematics behind probability theory is consistent and uncontroversial,
    the interpretation of what it means to say something as trivial as “the probability
    of a fair coin turning up heads is 0.5” is actually somewhat contentious. The
    two major camps are called *frequentists* and *Bayesians*.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然概率理论的数学基础是一致的且无争议的，但关于说“公平硬币出现正面的概率是0.5”这样的简单陈述的含义，实际上是有争议的。两个主要阵营被称为*频率主义者*和*贝叶斯主义者*。
- en: A frequentist says the probability of a coin turning up heads is whatever proportion
    of heads are observed if one could flip the coin an infinite number of times.
    A short sequence of coin flips might yield a proportion of heads as high as 0.8,
    but as you keep flipping, it will tend toward 0.5 exactly, in the infinite limit.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 频率主义者认为，硬币出现正面的概率是如果可以无限次抛硬币，观察到的正面比例。一系列的硬币抛掷可能会得到高达0.8的正面比例，但随着你继续抛掷，它将趋向于0.5，在无限极限中。
- en: Hence, probabilities are just frequencies of events. In this case, there are
    two possible outcomes, heads or tails, and each outcome’s probability is its frequency
    after an infinite number of trials (coin flips). This is, of course, why probabilities
    are values between 0 (impossible) and 1 (certain), and the probabilities for all
    possible outcomes must sum to 1.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率只是事件发生的频率。在这种情况下，有两种可能的结果，正面或反面，每种结果发生的概率是在无限次试验（抛硬币）后该结果的频率。这就是为什么概率的值在0（不可能）和1（必然）之间，并且所有可能结果的概率之和必须等于1。
- en: This is a simple and straightforward approach to probability, but it has significant
    limitations. In the frequentist setting, it is difficult or perhaps impossible
    to make sense of a question like “what is the probability that Jane Doe will be
    elected to city council?” since it is impossible in practice and theory for such
    an election to happen an infinite number of times. Frequentist probability doesn’t
    make much sense for these kinds of one-off events. We need a more powerful framework
    to handle these situations, and that is what Bayesian probability gives us.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单直接的概率方法，但它有显著的局限性。在频率主义设定中，对于像“简·多伊被选为市议会成员的概率是多少？”这样的问题，在实践中和理论上都很难理解，因为这样的选举不可能发生无限次。频率主义概率对于这类一次性事件并没有太多意义。我们需要一个更强大的框架来处理这些情况，这正是贝叶斯概率所提供的。
- en: In the Bayesian framework, probabilities represent degrees of belief about various
    possible outcomes. You can certainly have a belief about something that can only
    happen once, like an election, and your belief about what is likely to happen
    can vary depending on how much information you have about a particular situation,
    and new information will cause you to update your beliefs (see [table 7.1](#ch07table01)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯框架中，概率代表了对各种可能结果的信念程度。你当然可以相信某件只能发生一次的事情，比如选举，你的信念关于可能发生的事情可以取决于你对特定情况了解多少，新信息将导致你更新你的信念（见[表7.1](#ch07table01)）。
- en: Table 7.1\. Frequentist versus Bayesian probabilities
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1\. 经验概率与贝叶斯概率
- en: '| Frequentist | Bayesian |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 经验主义 | 贝叶斯 |'
- en: '| --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Probabilities are frequencies of individual outcomes | Probabilities are
    degrees of belief |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 概率是单个结果的频率 | 概率是信念程度 |'
- en: '| Computes the probability of the data given a model | Computes the probability
    of a model given the data |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 计算给定模型的数据概率 | 计算给定数据模型的概率 |'
- en: '| Uses hypothesis testing | Uses parameter estimation or model comparison |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 使用假设检验 | 使用参数估计或模型比较 |'
- en: '| Is computationally easy | Is (usually) computationally difficult |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 计算简单 | 计算通常困难 |'
- en: 'The basic mathematical framework for probability consists of a *sample space*,
    Ω, which is the set of all possible outcomes for a particular question. In the
    case of an election, for example, the sample space is the set of all candidates
    eligible to be elected. There is a probability distribution (or measure) function,
    *P*: Ω → [0,1], where *P* is a function from the sample space to real numbers
    in the interval from 0 to 1\. You could plug in *P*(*candidate A*) and it will
    spit out a number between 0 and 1 indicating the probability of candidate A winning
    the election.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 概率论的基本数学框架由一个**样本空间**Ω组成，它是特定问题的所有可能结果的集合。例如，在选举的情况下，样本空间是所有有资格被选为候选人的集合。存在一个概率分布（或测度）函数，*P*：Ω
    → [0,1]，其中*P*是从样本空间到0到1之间的实数的函数。你可以插入*P*(*候选人A*)，它将输出一个介于0和1之间的数字，表示候选人A赢得选举的概率。
- en: '|  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Probability theory is more complicated than what we’ve articulated here and
    involves a branch of mathematics called *measure theory*. For our purposes, we
    do not need to delve any deeper into probability theory than we already have.
    We will stick with an informal and mathematically nonrigorous introduction to
    the probability concepts we need.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 概率论比我们在这里阐述的要复杂得多，它涉及到一个称为**测度理论**的数学分支。就我们的目的而言，我们不需要比我们已经有的更深入地研究概率论。我们将坚持一个非正式且数学上非严格的概率概念介绍。
- en: '|  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The *support* of a probability distribution is another term we will use. The
    support is just the subset of outcomes that are assigned nonzero probabilities.
    For example, temperatures can’t be less than 0 Kelvin, so negative temperatures
    would be assigned probability 0; the support of the probability distribution over
    temperatures would be from 0 to positive infinity. Since we generally don’t care
    about outcomes that are impossible, you’ll often see “support” and “sample space”
    used interchangeably, even though they may not be the same.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布的**支撑集**是我们将使用的另一个术语。支撑集仅仅是分配了非零概率的子集。例如，温度不能低于0开尔文，所以负温度会被分配概率0；温度概率分布的支撑集将从0到正无穷。由于我们通常不关心不可能发生的结果，你经常会看到“支撑集”和“样本空间”被互换使用，尽管它们可能并不相同。
- en: 7.2.1\. Priors and posteriors
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 先验和后验
- en: If we were to ask you “what is the probability of each candidate in a 4-way
    race winning?” without specifying who the candidates were or what the election
    was about, you might refuse to answer, citing insufficient information. If we
    really pressed you, you might say that since you know nothing else, each candidate
    has a ¼ chance of winning. With that answer, you’ve established a *prior probability
    distribution* that is uniform (each possible outcome has the same probability)
    over the candidates.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不指定候选人是谁或选举的内容，而问你“在四选一的比赛中有多少候选人获胜的概率？”你可能会拒绝回答，理由是信息不足。如果我们真的坚持要你回答，你可能会说，由于你一无所知，每个候选人都有1/4的获胜机会。有了这个答案，你就建立了一个**先验概率分布**，它在候选人中是均匀的（每个可能的结果都有相同的概率）。
- en: In the Bayesian framework, probabilities represent beliefs, and beliefs are
    always tentative in situations when new information can become available, so a
    prior probability distribution is just the distribution you start with before
    receiving some new information. After you receive new information, such as some
    biographical information about the candidates, you might update your prior distribution
    based on that new information—this updated distribution is now called your *posterior
    probability distribution*. The distinction between prior and posterior distribution
    is contextual, since your posterior distribution will become a new prior distribution
    right before you receive another set of new information. Your beliefs are continually
    updated as a succession of prior distributions to posterior distributions (see
    [figure 7.5](#ch07fig05)), and this process is generically called *Bayesian inference*.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯框架中，概率代表信念，而在新信息可能成为可用的情况中，信念总是试探性的，因此先验概率分布就是在接收到一些新信息之前开始的分布。在你接收到新信息，例如关于候选人的某些传记信息后，你可能会根据这些新信息更新你的先验分布——这个更新后的分布现在被称为你的*后验概率分布*。先验分布和后验分布之间的区别是情境性的，因为你的后验分布将在你接收到另一组新信息之前立即成为新的先验分布。你的信念会不断更新，从一系列先验分布到后验分布（参见[图7.5](#ch07fig05)），这个过程通常被称为*贝叶斯推断*。
- en: Figure 7.5\. Bayesian inference is the process of starting with a prior distribution,
    receiving some new information, and using that to update the prior into a new,
    more informed distribution called the posterior distribution.
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5\. 贝叶斯推断是从先验分布开始，接收一些新信息，并使用这些信息将先验分布更新为一个新的、更信息的分布，称为后验分布。
- en: '![](07fig05_alt.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig05_alt.jpg)'
- en: 7.2.2\. Expectation and variance
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 期望和方差
- en: There are a number of questions we can ask a probability distribution. We can
    ask what the single “most likely” outcome is, which we commonly think of as the
    *mean* or *average* of the distribution. You’re probably familiar with the calculation
    of the mean as taking the sum of all the results and dividing by the number of
    results. For example, the mean of the 5-day temperature forecast of [18, 21, 17,
    17, 21]°C is [18 + 21 + 17 + 17 + 21]/5 = 94/5 = 18.8°C. This is the average predicted
    temperature over a sample of 5 days in Chicago, Illinois, USA.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向概率分布提出许多问题。我们可以询问单个“最可能”的结果是什么，这通常被认为是分布的*均值*或*平均值*。你可能熟悉均值的计算，即取所有结果的和，然后除以结果的数量。例如，[18,
    21, 17, 17, 21]°C的5天温度预报的均值是[18 + 21 + 17 + 17 + 21]/5 = 94/5 = 18.8°C。这是美国伊利诺伊州芝加哥5天样本的平均预测温度。
- en: 'Consider if instead we asked five people to give us their prediction for tomorrow’s
    temperature in Chicago and they happened to give us the same numbers, [18, 21,
    17, 17, 21]°C. If we wanted the average temperature for tomorrow, we would follow
    the same procedure, adding the numbers up and dividing by the number of samples
    (five) to get the average predicted temperature for tomorrow. But what if person
    1 was a meteorologist, and we had a lot more confidence in their prediction compared
    to the other four people that we randomly polled on the street? We would probably
    want to weight the meteorologist’s prediction higher than the others. Let’s say
    we think that their prediction is 60% likely to be true, and the other four are
    merely 10% likely to be true (notice 0.6 + 4 * 0.10 = 1.0), this is a *weighted
    average*; it’s computed by multiplying each sample by its weight. In this case,
    that works out as follows: [(0.6 * 18) + 0.1 * (21 + 17 + 17 + 21)] = 18.4°C.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下，如果我们请了五个人预测明天芝加哥的温度，并且他们恰好给出了相同的数字，[18, 21, 17, 17, 21]°C。如果我们想得到明天的平均温度，我们会遵循相同的程序，将数字相加，然后除以样本数量（五个）来得到预测的平均温度。但如果第一个人是一位气象学家，而我们对其预测的信心比在街上随机调查的其他四个人要高得多呢？我们可能会希望将气象学家的预测权重高于其他人。假设我们认为他们的预测有60%的可能性是正确的，而其他四个人只有10%的可能性是正确的（注意0.6
    + 4 * 0.10 = 1.0），这是一个*加权平均*；它是通过将每个样本乘以其权重来计算的。在这种情况下，计算结果如下：[(0.6 * 18) + 0.1
    * (21 + 17 + 17 + 21)] = 18.4°C。
- en: Each temperature is a possible outcome for tomorrow, but not all outcomes are
    equally likely in this case, so we multiply each possible outcome by its probability
    (weight) and then sum. If all the weights are equal and sum to 1, we get an ordinary
    average calculation, but many times it is not. When the weights are not all the
    same, we get a weighted average called the *expectation value* of a distribution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 每个温度都是明天的可能结果，但在这个情况下，并非所有结果的可能性都相同，因此我们乘以每个可能结果的概率（权重），然后求和。如果所有权重都相等且总和为1，我们得到一个普通的平均计算，但很多时候并不如此。当权重不相等时，我们得到一个加权平均，称为分布的*期望值*。
- en: The expected value of a probability distribution is its “center of mass,” the
    value that is most likely on average. Given a probability distribution, *P*(*x*),
    where *x* is the sample space, the expected value for discrete distributions is
    calculated as follows.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布的期望值是其“质心”，即平均情况下最可能出现的值。给定一个概率分布*P*(*x*)，其中*x*是样本空间，离散分布的期望值计算如下。
- en: Table 7.2\. Computing an expected value from a probability distribution
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2\. 从概率分布计算期望值
- en: '| Math | Python |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | Python |'
- en: '| --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg179a.jpg) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| ![](pg179a.jpg) |'
- en: '[PRE15]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The expected value operator (where *operator* is another term for *function*)
    is denoted ![](ed.jpg), and it’s a function that takes in a probability distribution
    and returns its expected value. It works by taking a value, *x*, multiplying by
    its associated probability, *P*(*x*), and summing for all possible values of *x*.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值算子（其中*算子*是*函数*的另一种说法）表示为![](ed.jpg)，它是一个接收概率分布并返回其期望值的函数。它通过取一个值*x*，乘以其关联的概率*P*(*x*)，并对所有可能的*x*值求和来实现。
- en: In Python, if *P*(*x*) is represented as a numpy array of probabilities, `probs`,
    and another numpy array of `outcomes` (the sample space), the expected value is
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，如果将概率密度函数*P*(*x*)表示为一个包含概率的numpy数组`probs`，以及另一个包含`outcomes`（样本空间）的numpy数组，期望值可以这样计算：
- en: '[PRE16]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Alternatively, the expected value can be computed as the inner (dot) product
    between the `probs` array and the `outcomes` array, since the inner product does
    the same thing—it multiplies each corresponding element in the two arrays and
    sums them all.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，期望值可以通过`probs`数组与`outcomes`数组之间的内积（点积）来计算，因为内积做的是同样的事情——它将两个数组中对应元素相乘并求和。
- en: '[PRE17]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A discrete probability distribution means that its sample space is a finite
    set, or in other words, only a finite number of possible outcomes can occur. A
    coin toss, for example, can only have one of two outcomes.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 离散概率分布意味着其样本空间是一个有限集，换句话说，只有有限数量的可能结果可以发生。例如，抛硬币的结果只有两种可能。
- en: 'However, tomorrow’s temperature could be any real number (or if measured in
    Kelvin, it could be any real number from 0 to infinity), and the real numbers
    or any subset of the real numbers is infinite since we can continually divide
    them: 1.5 is a real number, and so is 1.500001, and so forth. When the sample
    space is infinite, this is a *continuous probability distribution*.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，明天的温度可以是任何实数（或者如果以开尔文为单位测量，它可以是0到无穷大的任何实数），因为实数或任何实数的子集是无限的，因为我们可以不断地将它们除以：1.5是一个实数，1.500001也是一个实数，以此类推。当样本空间是无限的时候，这就是一个*连续概率分布*。
- en: In a continuous probability distribution, the distribution does not tell you
    the probability of a particular outcome, because with an infinite number of possible
    outcomes, each individual outcome must have an infinitely small probability in
    order for the sum to be 1\. Thus, a continuous probability distribution tells
    you the *probability density* around a particular possible outcome. The probability
    density is the sum of probabilities around a small interval of some value—it’s
    the probability that the outcome will fall within some small interval. The difference
    between discrete and continuous distributions is depicted in [figure 7.6](#ch07fig06).
    That’s all we’ll say about continuous distributions for now because in this book
    we’ll really only deal with discrete probability distributions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续概率分布中，分布并不告诉你特定结果的概率，因为可能的结果是无限的，每个单独的结果必须具有无限小的概率，以便总和为1。因此，连续概率分布告诉你特定可能结果周围的*概率密度*。概率密度是围绕某个值的小区间内的概率之和——它是结果落在某个小区间内的概率。离散分布和连续分布之间的区别在[图7.6](#ch07fig06)中展示。现在我们就说这么多关于连续分布的内容，因为在这本书中，我们实际上只处理离散概率分布。
- en: 'Figure 7.6\. Left: A discrete distribution is like a numpy array of probabilities
    associated with another numpy array of outcome values. There is a finite set of
    probabilities and outcomes. Right: A continuous distribution represents an infinite
    number of possible outcomes, and the y axis is the probability density (which
    is the probability that the outcome takes on a value within a small interval).'
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6。左：离散分布类似于与另一个结果值数组关联的概率数组。存在有限个概率和结果。右：连续分布代表无限多个可能的结果，y轴是概率密度（即结果在微小区间内取值的概率）。
- en: '![](07fig06_alt.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig06_alt.jpg)'
- en: Another question we can ask of a probability distribution is its spread or *variance*.
    Our beliefs about something can be more or less confident, so a probability distribution
    can be narrow or wide respectively. The calculation of variance uses the expectation
    operator and is defined as ![](pg177-01a.jpg)], but don’t worry about remembering
    this equation—we will use built-in numpy functions to compute variance. Variance
    is either denoted *Var*(*X*) or *σ*² (sigma squared) where ![](pg180a.jpg) is
    the standard deviation, so the variance is the standard deviation squared. The
    *μ* in this equation is the standard symbol for mean, which again is ![](in178-01.jpg),
    where *X* is a *random variable* of interest.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向概率分布提出另一个问题，即其范围或*方差*。我们对某事的信念可能更加或更少自信，因此概率分布可以是狭窄的或宽泛的。方差的计算使用期望运算符，定义为
    ![](pg177-01a.jpg)]，但不必担心记住这个方程——我们将使用内置的 numpy 函数来计算方差。方差可以用 *Var*(*X*) 或 *σ*²（sigma
    squared）表示，其中 ![](pg180a.jpg) 是标准差，因此方差是标准差的平方。此方程中的 *μ* 是均值的常用符号，它再次是 ![](in178-01.jpg)，其中
    *X* 是感兴趣的*随机变量*。
- en: A random variable is just another way of using a probability distribution. A
    random variable is associated with a probability distribution, and a probability
    distribution can yield random variables. We can create a random variable, *T*,
    for tomorrow’s temperature. It is a random variable since it is an unknown value
    but it can only take on specific values that are valid with respect to its underlying
    probability distribution. We can use random variables wherever a normal deterministic
    variable might be used, but if we add a random variable with a deterministic variable,
    we will get a new random variable.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量只是使用概率分布的另一种方式。随机变量与概率分布相关联，而概率分布可以产生随机变量。我们可以为明天的温度创建一个随机变量 *T*。由于它是一个未知值，因此它是一个随机变量，但它只能取与它所依赖的概率分布有效的特定值。我们可以在可能使用正常确定性变量的任何地方使用随机变量，但如果我们将随机变量与确定性变量相加，我们将得到一个新的随机变量。
- en: For example, if we think tomorrow’s temperature is just going to be today’s
    temperature plus some random noise, we can model this as *T* = *t*[0] + *e*, where
    *e* is a random variable of noise. The noise might have *normal (Gaussian) distribution*
    centered around 0 with a variance of 1\. Thus *T* will be a new normal distribution
    with mean *t*[0] (today’s temperature), but it will still have a variance of 1\.
    A normal distribution is the familiar bell-shaped curve.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们认为明天的温度只是今天的温度加上一些随机噪声，我们可以将此建模为 *T* = *t*[0] + *e*，其中 *e* 是噪声的随机变量。噪声可能具有以0为中心、方差为1的*正态（高斯）分布*。因此
    *T* 将是一个新的正态分布，均值为 *t*[0]（今天的温度），但它仍然具有方差为1。正态分布是熟悉的钟形曲线。
- en: '[Table 7.3](#ch07table03) shows a few common distributions. The normal distribution
    gets wider or narrower depending on the variance parameter, but otherwise it looks
    the same for any set of parameters. In contrast, the beta and gamma distributions
    can look quite different depending on their parameters—two different versions
    of each of these are shown.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.3](#ch07table03) 展示了一些常见的分布。正态分布的宽度或窄度取决于方差参数，但就任何一组参数而言，其外观都是相同的。相比之下，贝塔分布和伽马分布的外观可能会因参数的不同而大相径庭——每种分布都展示了两种不同的版本。'
- en: Table 7.3\. Common probability distributions
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.3。常见概率分布
- en: '| Normal distribution | ![](pg179-01.jpg) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 正态分布 | ![](pg179-01.jpg) |'
- en: '| Beta distribution | ![](pg179-02.jpg) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 贝塔分布 | ![](pg179-02.jpg) |'
- en: '| Gamma distribution | ![](pg179-03.jpg) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 伽马分布 | ![](pg179-03.jpg) |'
- en: 'Random variables are typically denoted with a capital letter like *X*. In Python,
    we might set up a random variable using numpy’s `random` module:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量通常用大写字母表示，如 *X*。在 Python 中，我们可能使用 numpy 的 `random` 模块设置一个随机变量：
- en: '[PRE18]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here we made *T* an anonymous function that accepts no arguments and just adds
    a small random number to 18.4 every time it is called. The variance of *T* is
    1, which means that most of the values that *T* returns will be within 1 degree
    of 18.4\. If the variance was 10, the spread of likely temperatures would be greater.
    Generally we start with a prior distribution that has high variance, and as we
    get more information the variance decreases. However, it is possible for new information
    to increase the variance of the posterior if the information we get is very unexpected
    and makes us less certain.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将*T*设为一个无参数的匿名函数，每次调用时只将一个小的随机数加到18.4上。*T*的方差为1，这意味着*T*返回的大多数值将在18.4度以内。如果方差是10，可能的温度范围就会更广。通常我们从一个具有高方差的先验分布开始，随着我们获得更多信息，方差会减小。然而，如果得到的信息非常意外并且让我们变得不太确定，新信息可能会增加后验的方差。
- en: 7.3\. The Bellman equation
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 贝尔曼方程
- en: We mentioned Richard Bellman in [chapter 1](kindle_split_010.html#ch01), but
    here we will discuss the Bellman equation, which underpins much of reinforcement
    learning. The Bellman equation shows up everywhere in the reinforcement learning
    literature, but if all you want to do is write Python, you can do that without
    understanding the Bellman equation. This section is optional; it’s for those interested
    in a bit more mathematical background.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](kindle_split_010.html#ch01)中提到了理查德·贝尔曼，但在这里我们将讨论贝尔曼方程，它是强化学习的基础。贝尔曼方程在强化学习的文献中无处不在，但如果你只想用Python编写代码，你可以在不了解贝尔曼方程的情况下做到这一点。本节是可选的；它是为那些对更多数学背景感兴趣的人准备的。
- en: As you’ll recall, the Q function tells us the value of a state-action pair,
    and value is defined as the expected sum of time-discounted rewards. In the Gridworld
    game, for example, *Q**[π]*(*s*,*a*) tells us the average rewards we will get
    if we take action *a* in state *s* and follow policy *π* from then forward. The
    optimal Q function is denoted *Q*^* and is the Q function that is perfectly accurate.
    When we first start playing a game with a randomly initialized Q function, it
    is going to give us very inaccurate Q value predictions, but the goal is to iteratively
    update the Q function until it gets close to the optimal *Q*^*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆，Q函数告诉我们状态-动作对的价值，价值被定义为时间折扣奖励的期望总和。例如，在Gridworld游戏中，*Q**[π]*(*s*,*a*)告诉我们如果我们采取动作*a*处于状态*s*并从那时起遵循策略*π*，我们将获得的平均奖励。最优Q函数表示为*Q*^*，是完美准确的Q函数。当我们第一次用随机初始化的Q函数开始玩游戏时，它将给出非常不准确的Q值预测，但目标是迭代更新Q函数，直到它接近最优的*Q*^*。
- en: The Bellman equation tells us how to update the Q function when rewards are
    observed,
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程告诉我们如何在观察到奖励时更新Q函数，
- en: '| *Q**[π]*(*s[t]*,*a[t]*) ← *r[t]* + *γ* × *V**[π]*(*s[t]*[+1]), |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| *Q**[π]*(*s[t]*,*a[t]*) ← *r[t]* + *γ* × *V**[π]*(*s[t]*[+1]), |'
- en: where *V**[π]*(*s[t]*[+1]) = *max*[*Q**[π]*(*s[t]*[+1],*a*)]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *V**[π]*(*s[t]*[+1]) = *max*[*Q**[π]*(*s[t]*[+1],*a*)]
- en: So the Q value of the current state, *Q**[π]*(*s[t]*,*a*), should be updated
    to be the observed reward *r[t]* plus the value of the next state *V**[π]*(*s[t]*[+1])
    multiplied by the discount factor *γ* (the left-facing arrow in the equation means
    “assign the value on the right side to the variable on the left side”). The value
    of the next state is simply whatever the highest Q value is for the next state
    (since we get a different Q value for each possible action).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当前状态的Q值，*Q**[π]*(*s[t]*,*a*), 应该更新为观察到的奖励*r[t]*加上下一个状态的价值*V**[π]*(*s[t]*[+1])乘以折扣因子*γ*（方程中的左向箭头表示“将右侧的值赋给左侧的变量”）。下一个状态的价值仅仅是下一个状态的最高Q值（因为我们对每个可能的动作都会得到不同的Q值）。
- en: If we use neural networks to approximate the Q function, we try to minimize
    the error between the predicted *Q**[π]*(*s[t]*,*a[t]*) on the left side of the
    Bellman equation and the quantity on the right side by updating the neural network’s
    parameters.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用神经网络来近似Q函数，我们尝试通过更新神经网络的参数来最小化贝尔曼方程左侧预测的*Q**[π]*(*s[t]*,*a[t]*)与右侧数量的误差。
- en: 7.3.1\. The distributional Bellman equation
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1. 分布式贝尔曼方程
- en: The Bellman equation implicitly assumes that the environment is deterministic
    and thus that observed rewards are deterministic (i.e., the observed reward will
    be always the same if you take the same action in the same state). In some cases
    this is true, but in other cases it is not. All the games we have used and will
    use (except for Gridworld) involve at least some amount of randomness. For example,
    when we downsample the frames of a game, two originally different states will
    get mapped into the same downsampled state, leading to some unpredictability in
    observed rewards.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程隐含地假设环境是确定性的，因此观察到的奖励也是确定性的（即，如果你在相同的状态下采取相同的行动，观察到的奖励将始终相同）。在某些情况下这是真的，但在其他情况下则不是。我们使用和将要使用的所有游戏（除了网格世界）都至少涉及一些随机性。例如，当我们对游戏的帧进行下采样时，原本不同的两个状态将被映射到相同的下采样状态，导致观察到的奖励存在一些不可预测性。
- en: In this case, we can make the deterministic variable *r[t]* into a random variable
    *R*(*s[t]*,*a*) that has some underlying probability distribution. If there is
    randomness in how states evolve into new states, the Q function must be a random
    variable as well. The original Bellman equation can now be represented as
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以将确定性的变量 *r[t]* 转换为具有某些潜在概率分布的随机变量 *R*(*s[t]*,*a*)。如果状态如何演变到新状态存在随机性，那么Q函数也必须是随机变量。原始的贝尔曼方程现在可以表示为
- en: '| ![](pg181-1.jpg) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| ![](pg181-1.jpg) |'
- en: Again, the Q function is a random variable because we interpret the environment
    as having stochastic transitions. Taking an action may not lead to the same next
    state, so we get a probability distribution over next states and actions. The
    expected Q value of the next state-action pair is the most likely Q value given
    the most likely next state-action pair.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，Q函数是一个随机变量，因为我们把环境解释为具有随机转换。采取一个行动可能不会导致相同的下一个状态，因此我们得到下一个状态和动作的概率分布。下一个状态-动作对的期望Q值是最可能的Q值，给定最可能的下一个状态-动作对。
- en: 'If we get rid of the expectation operator, we get a full distributional Bellman
    equation:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去掉期望算子，我们得到一个完整的分布贝尔曼方程：
- en: '| *Z*(*s[t]*,*a[t]*) ← *R*(*s[t]*,*a[t]*) + *γ* · *Z*(*S[t]*[+1],*A[t]*[+1])
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| *Z*(*s[t]*,*a[t]*) ← *R*(*s[t]*,*a[t]*) + *γ* · *Z*(*S[t]*[+1],*A[t]*[+1])
    |'
- en: Here we use *Z* to denote the distributional Q value function (which we will
    also refer to as the *value distribution*). When we do Q-learning with the original
    Bellman equation, our Q function will learn the expected value of the value distribution
    because that is the best it can do, but in this chapter we will use a slightly
    more sophisticated neural network that will return a value distribution and thus
    can learn the distribution of observed rewards rather than just the expected value.
    This is useful for the reasons we described in the first section—by learning a
    distribution we have a way to utilize risk-sensitive policies that take into consideration
    the variance and possible multimodality of the distribution.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们用 *Z* 表示分布Q值函数（我们也将称之为 *值分布*）。当我们使用原始贝尔曼方程进行Q学习时，我们的Q函数将学习值分布的期望值，因为这是它能做的最好的，但在这章中，我们将使用一个稍微复杂一点的神经网络，它将返回一个值分布，因此可以学习观察到的奖励的分布，而不仅仅是期望值。这很有用，正如我们在第一部分所描述的原因——通过学习分布，我们有一种方法来利用风险敏感策略，这些策略考虑了分布的方差和可能的多个模态。
- en: 7.4\. Distributional Q-learning
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4. 分布式Q学习
- en: We’ve now covered all the preliminaries necessary to implement a distributional
    deep Q-network (Dist-DQN). If you didn’t completely understand all of the material
    in the previous sections, don’t worry; it will become more clear when we start
    writing the code.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了实现分布深度Q网络（Dist-DQN）所需的所有预备知识。如果你没有完全理解前几节的所有材料，不要担心；当我们开始编写代码时，它将变得更加清晰。
- en: In this chapter we are going to use one of the simplest Atari games in the OpenAI
    Gym, Freeway ([figure 7.7](#ch07fig07)), so that we can train the algorithm on
    a laptop CPU. Unlike other chapters, we’re also going to use the RAM version of
    the game. If you look at the available game environments at [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari),
    you will see that each game has two versions, with one being labeled with “RAM.”
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用OpenAI Gym中最简单的Atari游戏之一，Freeway ([图7.7](#ch07fig07))，这样我们就可以在笔记本电脑CPU上训练算法。与本章的其他章节不同，我们还将使用游戏的RAM版本。如果你查看[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)上可用的游戏环境，你会看到每个游戏都有两个版本，其中一个被标记为“RAM”。
- en: Figure 7.7\. Screenshot from the Atari game Freeway. The objective is to move
    the chicken across the freeway, avoiding oncoming traffic.
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7\. 来自Atari游戏Freeway的屏幕截图。目标是移动小鸡穿过高速公路，避免迎面而来的交通。
- en: '![](07fig07.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig07.jpg)'
- en: Freeway is a game where you control a chicken with actions of UP, DOWN, or NO-OP
    (“no-operation” or do nothing). The objective is to move the chicken across the
    freeway, avoiding oncoming traffic, to get to the other side, where you get a
    reward of +1\. If you don’t get all three chickens across the road in a limited
    amount of time, you lose the game and get a negative reward.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Freeway是一款游戏，你可以通过UP、DOWN或NO-OP（“无操作”或什么都不做）等动作来控制小鸡。目标是移动小鸡穿过高速公路，避免迎面而来的交通，到达另一边，在那里你会得到+1的奖励。如果你在有限的时间内没有把所有三只小鸡都带到路上，你就输了游戏，并得到一个负奖励。
- en: In most cases in this book, we train our DRL agents using the raw pixel representation
    of the game and thus use convolutional layers in our neural network. In this case,
    though, we’re introducing new complexity by making a distributional DQN, so we’ll
    avoid convolutional layers to keep the focus on the topic at hand and keep the
    training efficient.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大多数情况下，我们使用游戏的原始像素表示来训练我们的DRL智能体，因此在我们的神经网络中使用卷积层。然而，在这种情况下，我们通过引入分布式DQN来引入新的复杂性，因此我们将避免使用卷积层，以保持对当前主题的关注并保持训练效率。
- en: The RAM version of each game is essentially a compressed representation of the
    game in the form of a 128-element vector (the positions and velocities of each
    game character, etc.). A 128-element vector is small enough to process through
    a few fully connected (dense) layers. Once you are comfortable with the simple
    implementation we’ll use here, you can use the pixel version of the game and upgrade
    the Dist-DQN to use convolutional layers.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 每个游戏的RAM版本实际上是游戏的一种压缩表示形式，形式为一个128个元素的向量（每个游戏角色的位置和速度等）。128个元素的向量足够小，可以通过几个全连接（密集）层进行处理。一旦你熟悉了我们在这里使用的简单实现，你就可以使用游戏的像素版本，并将Dist-DQN升级以使用卷积层。
- en: 7.4.1\. Representing a probability distribution in Python
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1\. 在Python中表示概率分布
- en: If you didn’t read the optional [section 7.3](#ch07lev1sec3), the only important
    thing you missed is that instead of using a neural network to represent a Q function,
    *Q**[π]*(*s*,*a*), that returns a single Q value, we can instead denote a value
    distribution, *Z**[π]*(*s*,*a*), that represents a random variable of Q values
    given a state-action pair. This probabilistic formalism subsumes the deterministic
    algorithms we’ve been using in prior chapters, since a deterministic outcome can
    always be represented by a *degenerate* probability distribution ([figure 7.8](#ch07fig08)),
    where all the probability is assigned to a single outcome.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有阅读可选的[第7.3节](#ch07lev1sec3)，你错过的重要信息是，我们不是使用神经网络来表示Q函数，即*Q**[π]*(*s*,*a*)，它返回一个单一的Q值，而是可以表示一个值分布，*Z**[π]*(*s*,*a*)，它表示给定一个状态-动作对的Q值随机变量。这种概率形式包含了我们在前几章中使用过的确定性算法，因为确定性结果总可以通过一个*退化的*概率分布来表示([图7.8](#ch07fig08))，其中所有概率都分配给单个结果。
- en: Figure 7.8\. This is a degenerate distribution, since all the possible values
    are assigned a probability of 0 except for one value. The outcome values that
    are not assigned 0 probability are called the probability distribution’s support.
    The degenerate distribution has a support of 1 element (in this case, the value
    0).
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8\. 这是一个退化的分布，因为除了一个值外，所有可能的值都被分配了0概率。未被分配0概率的输出值被称为概率分布的支持。退化的分布有一个只有一个元素的支持（在这种情况下，值0）。
- en: '![](07fig08_alt.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig08_alt.jpg)'
- en: Let’s first start with how we’re going to represent and work with value distributions.
    As we did in the section on probability theory, we will represent a discrete probability
    distribution over rewards using two numpy arrays. One numpy array will be the
    possible outcomes (i.e., the *support* of the distribution), and the other will
    be an equal-sized array storing the probabilities for each associated outcome.
    Recall, if we take the inner product between the support array and the probability
    array, we get the expected reward of the distribution.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从如何表示和操作值分布开始。正如我们在概率理论章节中所做的那样，我们将使用两个numpy数组来表示奖励上的离散概率分布。一个numpy数组将表示可能的结果（即分布的支持），另一个将是一个大小相等的数组，存储每个相关结果的概率。回想一下，如果我们取支持数组和概率数组的内积，我们得到分布的期望奖励。
- en: One problem with the way we’re representing the value distribution, *Z*(*s*,*a*),
    is that since our array is a finite size, we can only represent a finite number
    of outcomes. In some cases, the rewards are usually restricted within some fixed,
    finite range, but in the stock market, for example, the amount of money you can
    make or lose is theoretically unlimited. With our method, we have to choose a
    minimum and maximum value that we can represent. This limitation has been solved
    in a follow-up paper by Dabney et al., “Distributional Reinforcement Learning
    with Quantile Regression” (2017). We will briefly discuss their approach at the
    end of the chapter.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表示值分布的方式存在一个问题，即*Z*(*s*,*a*)，因为我们的数组大小是有限的，所以我们只能表示有限数量的结果。在某些情况下，奖励通常被限制在某个固定的有限范围内，但在股市等例子中，你可以赚或亏的钱在理论上是没有限制的。使用我们的方法，我们必须选择一个可以表示的最小值和最大值。这个问题在Dabney等人后续的论文中得到了解决，“基于分位数回归的分布强化学习”（2017年）。我们将在本章末尾简要讨论他们的方法。
- en: For Freeway, we restrict the support to be between –10 and +10\. All time steps
    that are *nonterminal* (i.e., those that don’t result in a winning or losing state)
    give a reward of –1 to penalize taking too much time crossing the road. We reward
    +10 if the chicken successfully crosses the road and –10 if the game is lost (if
    the chicken doesn’t cross the road before the timer runs out). When the chicken
    gets hit by a car, the game isn’t necessarily lost; the chicken just gets pushed
    down away from the goal.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Freeway，我们将支持范围限制在-10到+10之间。所有非终端时间步（即不导致赢或输的状态）都会给予-1的奖励，以惩罚花费太多时间穿越道路。如果小鸡成功穿越道路，则奖励+10；如果游戏失败（如果小鸡在计时器耗尽之前没有穿越道路），则奖励-10。当小鸡被车撞到时，游戏不一定失败；小鸡只是被推离目标。
- en: Our Dist-DQN will take a state, which is a 128-element vector, and will return
    3 separate but equal-sized tensors representing the probability distribution over
    the support for each of the 3 possible actions (UP, DOWN, NO-OP) given the input
    state. We will use a 51-element support, so the support and probability tensors
    will be 51 elements.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Dist-DQN将接受一个状态，这是一个128个元素的向量，并将返回3个大小相同但独立的张量，表示在给定输入状态的情况下，每个3个可能动作（UP，DOWN，NO-OP）的支持概率分布。我们将使用51个元素的支持，因此支持和概率张量将是51个元素。
- en: If our agent begins the game with a randomly initialized Dist-DQN, takes action
    UP, and receives a reward of –1, how do we update our Dist-DQN? What is the target
    distribution and how do we compute a loss function between two distributions?
    Well, we use whatever distribution the Dist-DQN returns for the subsequent state,
    *s[t]*[+1], as a prior distribution, and we update the prior distribution with
    the single observed reward, *r[t]*, such that a little bit of the distribution
    gets redistributed around the observed *r[t]*.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的智能体以随机初始化的Dist-DQN开始游戏，执行向上（UP）动作，并获得奖励-1，我们如何更新我们的Dist-DQN？目标分布是什么，我们如何计算两个分布之间的损失函数？嗯，我们使用Dist-DQN返回的后续状态*s[t]*[+1]的分布作为先验分布，并使用观察到的单个奖励*r[t]*更新先验分布，使得分布的一部分在观察到的*r[t]*周围重新分配。
- en: If we start with a uniform distribution and observe *r[t]* = –1, the posterior
    distribution should no longer be uniform, but it should still be pretty close
    ([figure 7.9](#ch07fig09)). Only if we repeatedly observe *r[t]* = –1 for the
    same state should the distribution start to strongly peak around –1\. In normal
    Q-learning, the discount rate, γ (gamma), controlled how much the expected future
    rewards contribute to the value of the current state. In distributional Q-learning,
    the γ parameter controls how much we update the prior toward the observed reward,
    which achieves a similar function ([figure 7.10](#ch07fig10)).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从一个均匀分布开始，并观察到*r[t]* = –1，后验分布不再应该是均匀的，但它仍然应该非常接近（[图7.9](#ch07fig09)）。只有当我们反复观察到*r[t]*
    = –1对于相同的状态时，分布才开始在-1周围强烈峰值。在正常的Q学习中，折现率γ（伽马）控制了预期未来奖励对当前状态价值的影响程度。在分布Q学习中，γ参数控制了我们更新先验分布以接近观察到的奖励的程度，从而实现了类似的功能（[图7.10](#ch07fig10)）。
- en: Figure 7.9\. We’ve created a function that takes a discrete distribution and
    updates it based on observed rewards. This function is performing a kind of approximate
    Bayesian inference by updating a prior distribution into a posterior distribution.
    Starting from a uniform distribution (on top, we observe some rewards and we get
    a peaked distribution at 0 (shown in the middle), and then we observe even more
    rewards (all zeros), and the distribution becomes a narrow, normal-like distribution
    (as shown on the bottom).
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9。我们创建了一个函数，该函数接受一个离散分布并根据观察到的奖励更新它。这个函数通过将先验分布更新为后验分布来执行一种近似贝叶斯推理。从一个均匀分布（顶部）开始，我们观察到一些奖励，我们得到一个峰值分布在0（中间所示），然后我们观察到更多的奖励（所有都是零），分布变成一个狭窄的、类似正态的分布（如底部所示）。
- en: '![](07fig09.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig09.jpg)'
- en: Figure 7.10\. This figure shows how a uniform distribution changes with lower
    or higher values for gamma (the discount factor).
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10。此图展示了当gamma（折扣因子）的值较低或较高时，均匀分布如何变化。
- en: '![](07fig10_alt.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig10_alt.jpg)'
- en: If we discount the future a lot, the posterior will be strongly centered around
    the recently observed reward. If we weakly discount the future, the observed reward
    will only mildly update the prior distribution, *Z*(*S[t]*[+1],*A[t]*[+1]). Since
    Freeway has sparse positive rewards in the beginning (because we need to take
    many actions before we observe our first win), we will set gamma so we only make
    small updates to the prior distribution.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们大量折现未来，后验分布将强烈地集中在最近观察到的奖励周围。如果我们弱化折现未来，观察到的奖励只会轻微地更新先验分布，*Z*(*S[t]*[+1],*A[t]*[+1])。由于Freeway在开始时具有稀疏的正奖励（因为我们需要采取许多行动才能观察到我们的第一次胜利），我们将设置gamma，以便我们只对先验分布进行小更新。
- en: In [listing 7.1](#ch07ex01) we set up an initial uniform discrete probability
    distribution and show how to plot it.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.1](#ch07ex01)中，我们设置了一个初始的均匀离散概率分布，并展示了如何绘制它。
- en: Listing 7.1\. Setting up a discrete probability distribution in numpy
  id: totrans-369
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1。在numpy中设置离散概率分布
- en: '[PRE19]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1*** Sets the minimum and maximum values of the support of the distribution'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置分布支持的最小值和最大值'
- en: '***2*** Sets the number of elements of the support'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置支持元素的数量'
- en: '***3*** Creates the support tensor, a tensor of evenly spaced values from –10
    to +10'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建支持张量，一个从–10到+10的等间距值的张量'
- en: '***4*** Plots the distribution as a bar plot'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将分布绘制为条形图'
- en: 'We have defined a uniform probability distribution; now let’s see how we update
    the distribution. We want a function, `update_dist(z, reward)`, that takes a prior
    distribution and an observed reward and returns a posterior distribution. We represent
    the support of the distribution as a vector from –10 to 10:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了一个均匀概率分布；现在让我们看看我们如何更新分布。我们想要一个函数，`update_dist(z, reward)`，它接受一个先验分布和一个观察到的奖励，并返回一个后验分布。我们将分布的支持表示为一个从–10到10的向量：
- en: '[PRE20]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We need to be able to find the closest support element in the support vector
    to an observed reward. For example, if we observe *r[t]* = –1, we’ll want to map
    that to either –1.2 or –0.8 since those are the closest (equally close) support
    elements. More importantly, we want the indices of these support elements so that
    we can get their corresponding probabilities in the probability vector. The support
    vector is static—we never update it. We only update the corresponding probabilities.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够找到与观察到的奖励最接近的支持向量元素。例如，如果我们观察到*r[t]* = –1，我们希望将其映射到–1.2或–0.8，因为那些是最接近的（同样接近）的支持向量元素。更重要的是，我们想要这些支持向量元素的索引，以便我们可以在概率向量中获取它们对应的概率。支持向量是静态的——我们从不更新它。我们只更新相应的概率。
- en: You can see that each support element is 0.4 away from its nearest neighbors.
    The numpy `linspace` function creates a sequence of evenly spaced elements, and
    the spacing is given by ![](pg186-1.jpg), where *N* is the number of support elements.
    If you plug 10, –10, and *N* = 51 into that formula, you get 0.4\. We call this
    value *dz* (for delta Z), and we use it to find the closest support element index
    value by the equation ![](pg186-2.jpg), where *b[j]* is the index value. Since
    *b[j]* may be a fractional number, and indices need to be non-negative integers,
    we simply round the value to the nearest whole number with `np.round(...)`. We
    also need to clip any values outside the minimum and maximum support range. For
    example, if the observed *r[t]* = –2 then ![](pg187.jpg). You can see that the
    support element with index 20 is –2, which in this case exactly corresponds to
    the observed reward (no rounding needed). We can then find the corresponding probability
    for the –2 support element using the index.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个支持元素距离其最近邻居有0.4的距离。numpy的`linspace`函数创建了一个等间距元素序列，间距由![](pg186-1.jpg)给出，其中*N*是支持元素的数量。如果你将10、-10和*N*
    = 51代入该公式，你会得到0.4。我们称这个值为*dz*（delta Z），我们使用它通过方程![](pg186-2.jpg)来找到最接近的支持元素索引值，其中*b[j]*是索引值。由于*b[j]*可能是一个分数数，而索引需要是非负整数，我们只需将值四舍五入到最接近的整数，使用`np.round(...)`。我们还需要剪辑任何超出最小和最大支持范围的值。例如，如果观察到的*r[t]*
    = -2，那么![](pg187.jpg)。你可以看到索引为20的支持元素是-2，在这种情况下，它正好对应于观察到的奖励（不需要四舍五入）。然后我们可以使用索引找到-2支持元素的对应概率。
- en: Once we find the index value of the support element corresponding to the observed
    reward, we want to redistribute some of the probability mass to that support and
    the nearby support elements. We have to take care that the final probability distribution
    is a real distribution and sums to 1\. We will simply take some of the probability
    mass from the neighbors on the left and right and add it to the element that corresponds
    to the observed reward. Then those nearest neighbors will steal some probability
    mass from their nearest neighbor, and so on, as shown in [figure 7.11](#ch07fig11).
    The amount of probability mass stolen will get exponentially smaller the farther
    we go from the observed reward.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到对应于观察奖励的支持元素的索引值，我们希望将一部分概率质量重新分配到该支持和附近的支撑元素。我们必须注意，最终的概率分布是一个真实分布，并且总和为1。我们将简单地从左右邻居那里取一些概率质量，并将其添加到对应于观察奖励的元素。然后这些最近的邻居将从他们的最近邻居那里窃取一些概率质量，依此类推，如图[图
    7.11](#ch07fig11)所示。窃取的概率质量量将随着我们远离观察奖励而指数级减小。
- en: Figure 7.11\. The `update_dist` function redistributes probability from neighbors
    toward the observed reward value.
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.11。`update_dist`函数将概率从邻居重新分配到观察奖励值。
- en: '![](07fig11_alt.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![图片 7.11](07fig11_alt.jpg)'
- en: In [listing 7.2](#ch07ex02) we implement the function that takes a set of supports,
    the associated probabilities, and an observation, and returns an updated probability
    distribution by redistributing the probability mass toward the observed value.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 7.2](#ch07ex02)中，我们实现了这样一个函数，它接受一组支持集、相关的概率和一个观察值，并通过将概率质量重新分配到观察值来返回一个更新的概率分布。
- en: Listing 7.2\. Updating a probability distribution
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.2。更新概率分布
- en: '[PRE21]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1*** Calculates the support spacing value'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 计算支持间距值'
- en: '***2*** Calculates the index value of the observed reward in the support'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 计算观察奖励在支持中的索引值'
- en: '***3*** Rounds and clips the value to make sure it is a valid index value for
    the support'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 通过四舍五入值以确保它是一个有效的支持索引值'
- en: '***4*** Starting from the immediate left neighbor, steals part of its probability'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 从紧邻的左侧开始，窃取其部分概率'
- en: '***5*** Starting from the immediate right neighbor, steals part of its probability'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 从紧邻的右侧开始，窃取其部分概率'
- en: '***6*** Divides by the sum to make sure it sums to 1'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 通过除以总和来确保总和为1'
- en: 'Let’s walk through the mechanics of this to see how it works. We start with
    a uniform prior distribution:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过其工作原理来了解它是如何工作的。我们从一个均匀先验分布开始：
- en: '[PRE22]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can see that each support has a probability of about 0.02\. We observe
    *r[t]* = –1, and we calculate *b[j]* ≈ 22\. We then find the nearest left and
    right neighbors, denoted *m[l]* and *m[r]*, to be indices 21 and 23, respectively.
    We multiply *m[l]* by γ*^j*, where *j* is a value that we increment by 1 starting
    at 1, so we get a sequence of exponentially decreasing gammas: γ¹,γ², . . . γ*^j*.
    Remember, gamma must be a value between 0 and 1, so the sequence of gammas will
    be 0.5, 0.25, 0.125, 0.0625 if γ = 0.5. So at first we take 0.5 * 0.02 = 0.01
    from the left and right neighbors and add it to the existing probability at *b[j]*
    = 22, which is also 0.02\. So the probability at *b[j]* = 22 will become 0.01
    + 0.01 + 0.02 = 0.04.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个支持的概率大约是 0.02。我们观察到 *r[t]* = -1，并计算出 *b[j]* ≈ 22。然后我们找到最近的左右邻居，分别表示为索引
    21 和 23。我们将 *m[l]* 乘以 γ*^j*，其中 *j* 是我们从 1 开始递增的值，所以我们得到一系列指数递减的 γ：γ¹,γ², . . .
    γ*^j*。记住，γ 必须是一个介于 0 和 1 之间的值，所以如果 γ = 0.5，则 γ 的序列将是 0.5, 0.25, 0.125, 0.0625。因此，最初我们从左右邻居那里取出
    0.5 * 0.02 = 0.01 并将其添加到现有的概率 *b[j]* = 22，该概率也是 0.02。所以 *b[j]* = 22 的概率将变为 0.01
    + 0.01 + 0.02 = 0.04。
- en: Now the left neighbor, *m[l]*, steals probability mass from its own left neighbor
    at index 20, but it steals less because we multiply by γ². The right neighbor,
    *m[r]*, does the same by stealing from its neighbor on the right. Each element
    in turn steals from either its left or right neighbor until we get to the end
    of the array. If gamma is close to 1, like 0.99, a lot of probability mass will
    be redistributed to the support close to *r[t]*.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，左邻居 *m[l]* 从其自己的索引 20 的左侧邻居那里窃取概率质量，但由于我们乘以 γ²，所以窃取的量较少。右邻居 *m[r]* 通过从其右侧邻居那里窃取来做同样的事情。每个元素依次从其左侧或右侧邻居那里窃取，直到我们到达数组的末尾。如果
    γ 接近 1，例如 0.99，则大量概率质量将被重新分配到接近 *r[t]* 的支持区域。
- en: Let’s test our distribution update function. We’ll give it an observed reward
    of –1 starting from a uniform distribution.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试我们的分布更新函数。我们将从一个均匀分布开始，给它一个观察到的奖励 -1。
- en: Listing 7.3\. Redistributing probability mass after a single observation
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3\. 单次观察后重新分配概率质量
- en: '[PRE23]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You can see in [figure 7.12](#ch07fig12) that the distribution is still fairly
    uniform, but now there is a distinct “bump” centered at –1\. We can control how
    big this bump is with the discount factor γ. On your own, try changing gamma to
    see how it changes the update.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 7.12](#ch07fig12)中看到，分布仍然相当均匀，但现在有一个以 -1 为中心的明显“隆起”。我们可以通过折现因子 γ 来控制这个隆起的大小。自己尝试改变
    γ，看看它是如何改变更新的。
- en: Figure 7.12\. This is the result of updating an initially uniform probability
    distribution after observing a single reward. Some probability mass is redistributed
    toward the support element corresponding to the observed reward.
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.12\. 这是观察单个奖励后更新初始均匀概率分布的结果。一些概率质量被重新分配到对应于观察到的奖励的支持元素。
- en: '![](07fig12_alt.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig12_alt.jpg)'
- en: Now let’s see how the distribution changes when we observe a sequence of varying
    rewards. (We have just made up this sequence of rewards; they do not come from
    the Freeway game.) We should be able to observe multimodality.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当我们观察一个变化奖励的序列时，分布是如何变化的。（我们刚刚编造了这个奖励序列；它们并不来自高速公路游戏。）我们应该能够观察到多模态性。
- en: Listing 7.4\. Redistributing probability mass with a sequence of observations
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4\. 使用观察序列重新分配概率质量
- en: '[PRE24]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can see in [figure 7.13](#ch07fig13) that there are now four peaks of varying
    heights corresponding to the four different kinds of rewards observed, namely
    10, 0, 1, and –10\. The highest peak (mode of the distribution) corresponds to
    10, since that was the most frequently observed reward.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 7.13](#ch07fig13)中看到，现在有四个不同高度的峰值，对应于观察到的四种不同类型的奖励，即 10, 0, 1 和 -10。最高的峰值（分布的众数）对应于
    10，因为这是观察到的最频繁的奖励。
- en: Figure 7.13\. This is the result of updating an initially uniform probability
    distribution after observing a sequence of different rewards. Each “peak” in the
    distribution corresponds to an observed reward.
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.13\. 这是观察一系列不同奖励后更新初始均匀概率分布的结果。分布中的每个“峰值”对应于一个观察到的奖励。
- en: '![](07fig13_alt.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig13_alt.jpg)'
- en: Now let’s see how the variance decreases if we observe the same reward multiple
    times, starting from a uniform prior.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如果我们多次观察相同的奖励，从均匀先验开始，方差会如何降低。
- en: Listing 7.5\. Decreased variance with sequence of same reward
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.5\. 相同奖励序列下的方差降低
- en: '[PRE25]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can see in [figure 7.14](#ch07fig14) that the uniform distribution transforms
    into a normal-like distribution centered at 5 with much lower variance. We will
    use this function to generate the target distribution that we want the Dist-DQN
    to learn to approximate. Let’s build the Dist-DQN now.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图7.14](#ch07fig14)中看到，均匀分布转换成一个以5为中心、方差较低的类似正态分布。我们将使用这个函数来生成我们希望Dist-DQN学习逼近的目标分布。现在让我们构建Dist-DQN。
- en: Figure 7.14\. The result of updating an initially uniform probability distribution
    after observing the same reward multiple times. The uniform distribution converges
    toward a normal-like distribution.
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14\. 观察相同奖励多次后更新初始均匀概率分布的结果。均匀分布收敛到一个类似正态的分布。
- en: '![](07fig14_alt.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig14_alt.jpg)'
- en: 7.4.2\. Implementing the Dist-DQN
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2\. 实现Dist-DQN
- en: As we briefly discussed earlier, the Dist-DQN will take a 128-element state
    vector, pass it through a couple of dense feedforward layers, and then it will
    use a `for` loop to multiply the last layer by 3 separate matrices to get 3 separate
    distribution vectors. We will lastly apply the softmax function to ensure it is
    a valid probability distribution. The result is a neural network with 3 different
    output “heads.” We collect these 3 output distributions into a single 3 × 51 matrix
    and return that as the final output of the Dist-DQN. Thus, we can get the individual
    action-value distributions for a particular action by indexing a particular row
    of the output matrix. [Figure 7.15](#ch07fig15) shows the overall architecture
    and tensor transformations. In [listing 7.6](#ch07ex06) we define the function
    that implements the Dist-DQN.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前简要讨论的，Dist-DQN将接受一个128个元素的向量状态，通过几个密集的前馈层传递，然后它将使用一个`for`循环将最后一层乘以3个不同的矩阵，以获得3个独立的分布向量。最后，我们将应用softmax函数以确保它是一个有效的概率分布。结果是具有3个不同输出“头部”的神经网络。我们将这3个输出分布收集到一个3
    × 51的矩阵中，并将其作为Dist-DQN的最终输出。因此，我们可以通过索引输出矩阵的特定行来获取特定动作的个别动作值分布。[图7.15](#ch07fig15)显示了整体架构和张量转换。在[列表7.6](#ch07ex06)中，我们定义了实现Dist-DQN的函数。
- en: Figure 7.15\. The Dist-DQN accepts a 128-element state vector and produces 3
    separate 51-element probability distribution vectors, which then get stacked into
    a single 3 × 51 matrix.
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.15\. Dist-DQN接受一个128个元素的向量状态，并产生3个独立的51个元素的概率分布向量，然后这些向量被堆叠成一个3 × 51的矩阵。
- en: '![](07fig15_alt.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig15_alt.jpg)'
- en: Listing 7.6\. The Dist-DQN
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6\. Dist-DQN
- en: '[PRE26]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1*** x is the 128-element vector state, theta is the parameter vector, and
    aspace is the size of the action space.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** x是128个元素的向量状态，theta是参数向量，aspace是动作空间的大小。'
- en: '***2*** Defines the layer dimensions so we can unpack theta into appropriately
    sized matrices'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 定义层维度，以便我们可以将theta解包成适当大小的矩阵'
- en: '***3*** Unpacks the first portion of theta into the first layer matrix'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将theta的第一个部分解包到第一个层矩阵中'
- en: '***4*** The dimensions of this computation are B × 128 × 128 × 100 = B × 100,
    where B is batch size.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 这个计算的维度是 B × 128 × 128 × 100 = B × 100，其中B是批量大小。'
- en: '***5*** The dimensions of this computation are B × 100 × 100 × 25 = B × 25.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 这个计算的维度是 B × 100 × 100 × 25 = B × 25。'
- en: '***6*** Loops through each action to generate each action-value distribution'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 遍历每个动作以生成每个动作值分布'
- en: '***7*** The dimensions of this computation are B × 25 × 25 × 51 = B × 51.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 这个计算的维度是 B × 25 × 25 × 51 = B × 51。'
- en: '***8*** The dimensions of the last layer are B × 3 × 51.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 最后层的维度是 B × 3 × 51。'
- en: In this chapter we will do gradient descent manually, and to make this easier
    we have our Dist-DQN accept a single parameter vector called `theta` that we will
    unpack and reshape into multiple separate layer matrices of the appropriate sizes.
    This is easier since we can just do gradient descent on a single vector rather
    than on multiple separate entities. We also will use a separate target network
    as we did in [chapter 3](kindle_split_012.html#ch03), so all we need to do is
    keep a copy of `theta` and pass that into the same `dist_dqn` function.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将手动进行梯度下降，为了使这个过程更容易，我们让Dist-DQN接受一个名为`theta`的单个参数向量，我们将将其解包并重塑成多个适当大小的单独层矩阵。这样做更容易，因为我们只需对单个向量而不是多个单独实体进行梯度下降。我们还将使用一个单独的目标网络，就像我们在第3章中做的那样，所以我们只需要保留`theta`的一个副本，并将其传递给相同的`dist_dqn`函数。
- en: The other novelty here is the multiple output heads. We’re used to a neural
    network returning a single output vector, but in this case we want it to return
    a matrix. To do that, we set up a loop where we multiply `l2` by each of three
    separate layer matrices, resulting in three different output vectors that we stack
    into a matrix. Other than that, it is a very simple neural network with a total
    of five dense layers.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的另一个新特性是多个输出头。我们习惯于神经网络返回一个单一的输出向量，但在这个案例中，我们希望它返回一个矩阵。为了做到这一点，我们设置了一个循环，将
    `l2` 乘以三个单独的层矩阵中的每一个，从而得到三个不同的输出向量，我们将它们堆叠成一个矩阵。除此之外，它是一个非常简单的神经网络，总共有五个密集层。
- en: Now we need a function that will take the output of our Dist-DQN, a reward,
    and an action, and generate the target distribution we want our neural network
    to get closer to. This function will use the `update_dist` function we used earlier,
    but it only wants to update the distribution associated with the action that was
    actually taken. Also, as you learned in [chapter 3](kindle_split_012.html#ch03),
    we also need a different target when we’ve reached a terminal state. At the terminal
    state, the expected reward is the observed reward, since there are no future rewards
    by definition. That means the Bellman update reduces to *Z*(*s[t]*,*a[t]*) ← *R*(*S[t]*,*A[t]*).
    Since we only observe a single reward, and there is no prior distribution to update,
    the target becomes what is called a *degenerate distribution*. That’s just a fancy
    term for a distribution where all the probability mass is concentrated at a single
    value.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个函数，它将接收我们 Dist-DQN 的输出、一个奖励和一个行动，并生成我们希望我们的神经网络接近的目标分布。这个函数将使用我们之前使用的
    `update_dist` 函数，但它只想更新与实际采取的行动相关的分布。此外，正如你在[第 3 章](kindle_split_012.html#ch03)中学到的，当我们达到终端状态时，我们还需要一个不同的目标。在终端状态，期望奖励是观察到的奖励，因为根据定义没有未来的奖励。这意味着贝尔曼更新简化为
    *Z*(*s[t]*,*a[t]*) ← *R*(*S[t]*,*A[t]*)。由于我们只观察到单个奖励，并且没有先验分布需要更新，目标就变成了所谓的 *退化分布*。这只是一个术语，指的是所有概率质量都集中在单个值上的分布。
- en: Listing 7.7\. Computing the target distribution
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.7. 计算目标分布
- en: '[PRE27]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1*** Loops through the batch dimension'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 遍历批次维度'
- en: '***2*** If the reward is not –1, it is a terminal state and the target is a
    degenerate distribution at the reward value.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 如果奖励不是 –1，则表示终端状态，目标是一个在奖励值上的退化分布。'
- en: '***3*** If the state is nonterminal, the target distribution is a Bayesian
    update of the prior given the reward.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 如果状态是非终端的，目标分布是先验的贝叶斯更新，基于奖励。'
- en: '***4*** Only changes the distribution for the action that was taken'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 只更改采取的行动的分布'
- en: The `get_target_dist` function takes a batch of data of shape *B* × 3 × 51 where
    *B* is the batch dimension, and it returns an equal-sized tensor. For example,
    if we only have one example in our batch, 1 × 3 × 51, and the agent took action
    1 and observed a reward of –1, this function would return a 1 × 3 × 51 tensor,
    except that the 1 × 51 distribution associated with index 1 (of dimension 1) will
    be changed according to the `update_dist` function using the observed reward of
    –1\. If the observed reward was instead 10, the 1 × 51 distribution associated
    with action 1 would be updated to be a degenerate distribution where all elements
    have 0 probability except the one associated with the reward of 10 (index 50).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_target_dist` 函数接收一个形状为 *B* × 3 × 51 的数据批次，其中 *B* 是批次维度，并返回一个大小相等的张量。例如，如果我们批次中只有一个示例，即
    1 × 3 × 51，并且智能体采取了行动 1 并观察到奖励为 –1，那么这个函数将返回一个 1 × 3 × 51 的张量，除了与索引 1（维度 1）相关联的
    1 × 51 分布将根据 `update_dist` 函数使用观察到的奖励 –1 进行更改。如果观察到的奖励是 10，那么与行动 1 相关的 1 × 51
    分布将被更新为一个退化的分布，其中除了与奖励 10（索引 50）相关联的元素外，所有元素的概率均为 0。'
- en: 7.5\. Comparing probability distributions
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5. 比较概率分布
- en: Now that we have a Dist-DQN and a way to generate target distributions, we need
    a loss function that will calculate how different the predicted action-value distribution
    is from the target distribution; then we can backpropagate and do gradient descent
    as usual, to update the Dist-DQN parameters to be more accurate next time. We
    often use the mean squared error (MSE) loss function when trying to minimize the
    distance between two batches of scalars or vectors, but this is not an appropriate
    loss function between two probability distributions. But there are many possible
    choices for a loss function between probability distributions. We want a function
    that will measure how different or distant two probability distributions are and
    that will minimize that distance.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了Dist-DQN和生成目标分布的方法，我们需要一个损失函数来计算预测的动作值分布与目标分布之间的差异；然后我们可以像往常一样进行反向传播和梯度下降，以更新Dist-DQN参数，使其在下一次更加准确。当我们试图最小化两个标量或向量批次之间的距离时，我们通常使用均方误差（MSE）损失函数，但这不是两个概率分布之间的适当损失函数。但是，在概率分布之间有许多可能的损失函数选择。我们想要一个函数来衡量两个概率分布之间的差异或距离，并最小化那个距离。
- en: In machine learning, we are usually trying to train a parametric model (e.g.,
    a neural network) to predict or produce data that closely matches empirical data
    from some data set. Thinking probabilistically, we can conceive of a neural network
    as generating synthetic data and trying to train the neural network to produce
    more and more realistic data—data that closely resembles some empirical data set.
    This is how we train *generative* models (models that generate data); we update
    their parameters so that the data they generate looks very close to some training
    (empirical) data set.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常试图训练一个参数模型（例如，神经网络）来预测或产生与某些数据集的经验数据密切匹配的数据。从概率的角度思考，我们可以将神经网络视为生成合成数据并尝试训练神经网络产生越来越真实的数据——类似于某些经验数据集的数据。这就是我们训练*生成*模型（生成数据的模型）的方式；我们更新它们的参数，使得它们生成的数据看起来非常接近某些训练（经验）数据集。
- en: For example, let’s say we want to build a generative model that produces images
    of celebrities’ faces ([figure 7.16](#ch07fig16)). In order to do this, we need
    some training data, so we use the freely available CelebA data set that contains
    hundreds of thousands of high quality photographs of various celebrities such
    as Will Smith and Britney Spears. Let’s call our generative model *P* and this
    empirical data set *Q*.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要构建一个生成模型，该模型可以生成名人脸部的图像（[图7.16](#ch07fig16)）。为了做到这一点，我们需要一些训练数据，因此我们使用了包含数十万张各种名人（如威尔·史密斯和布兰妮·斯皮尔斯）高质量照片的免费CelebA数据集。让我们称我们的生成模型为*P*，这个经验数据集为*Q*。
- en: Figure 7.16\. A generative model can be a probabilistic model that trains by
    maximizing the probability that it generates samples that are similar to some
    empirical data set. Training happens in an iterative loop where the empirical
    data is supplied to the generative model, which tries to maximize the probability
    of the empirical data. Before training, the generative model will assign low probability
    to examples taken from the training data set, and the objective is for the generative
    model to assign high probability to examples drawn from the data set. After a
    sufficient number of iterations, the generative model will have assigned high
    probability to the empirical data, and we can then sample from this distribution
    to generate new, synthetic data.
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.16\. 生成模型可以是一个概率模型，通过最大化生成与某些经验数据集相似的样本的概率来进行训练。训练发生在迭代循环中，其中经验数据被提供给生成模型，该模型试图最大化经验数据的概率。在训练之前，生成模型将给从训练数据集中取出的示例分配低概率，目标是为生成模型分配高概率给从数据集中抽取的示例。经过足够多的迭代后，生成模型将给经验数据分配高概率，然后我们可以从这个分布中采样以生成新的、合成的数据。
- en: '![](07fig16_alt.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16](07fig16_alt.jpg)'
- en: The images in data set *Q* were sampled from the real world, but they are just
    a small sample of the infinite number of photographs that already exist but are
    not in the data set and that could have been taken but were not. For example,
    there may just be one headshot photo of Will Smith in the data set, but another
    photo of Will Smith taken at a different angle could just as easily have been
    part of the data set. A photo of Will Smith with a baby elephant on top of his
    head, while not impossible, would be less likely to be included in the data set
    because it is less likely to exist (who would put a baby elephant on their head?).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 *Q* 中的图像是从现实世界中抽取的，但它们只是无限多张照片中的一部分，这些照片已经存在但不在数据集中，或者可能被拍摄但未被拍摄。例如，数据集中可能只有一张威尔·史密斯的头像照片，但另一张从不同角度拍摄的威尔·史密斯的照片也可能同样容易成为数据集的一部分。一张威尔·史密斯头顶上有一只小象的照片，虽然不是不可能，但不太可能包含在数据集中，因为它不太可能存在（谁会在头上放一只小象呢？）。
- en: There are naturally more and less likely photos of celebrities, so the real
    world has a probability distribution over images of celebrities. We can denote
    this true probability distribution of celebrity photos as *Q*(*x*), where *x*
    is some arbitrary image, and *Q*(*x*) tells us the probability of that image existing
    in the world. If *x* is a specific image in data set *Q*, then *Q*(*x*) = 1.0,
    since that image definitely exists in the real world. However, if we plug in an
    image that’s not in the data set but likely exists in the real world outside of
    our small sample, then *Q*(*x*) might equal 0.9.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 明星的照片自然有更多或更少的可能性，所以现实世界对名人照片有一个概率分布。我们可以用 *Q*(*x*) 表示这个真实概率分布，其中 *x* 是某个任意图像，*Q*(*x*)
    告诉我们该图像在世界上存在的概率。如果 *x* 是数据集 *Q* 中的一个特定图像，那么 *Q*(*x*) = 1.0，因为该图像肯定存在于现实世界中。然而，如果我们插入一个不在数据集中但可能存在于现实世界中的图像，那么
    *Q*(*x*) 可能等于 0.9。
- en: When we randomly initialize our generative model *P*, it will output random-looking
    images that look like white noise. We can think of our generative model as a random
    variable, and every random variable has an associated probability distribution
    that we denote *P*(*x*), so we can also ask our generative model what the probability
    of a specific image is given its current set of parameters. When we first initialize
    it, it will think all images are more or less equally probable, and all will be
    assigned a fairly low probability. So if we ask *P*(“Will Smith photo”) it will
    return some tiny probability, but if we ask *Q*(“Will Smith photo”), we’ll get
    1.0.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们随机初始化我们的生成模型 *P* 时，它将输出看起来像白色噪声的随机图像。我们可以将我们的生成模型视为一个随机变量，每个随机变量都有一个相关的概率分布，我们用
    *P*(*x*) 表示，因此我们也可以询问我们的生成模型在当前参数集下特定图像的概率。当我们第一次初始化它时，它会认为所有图像的可能性大致相同，并且都会被分配一个相当低的概率。所以如果我们询问
    *P*(“威尔·史密斯照片”)，它将返回一个非常小的概率，但如果我们询问 *Q*(“威尔·史密斯照片”)，我们将得到 1.0。
- en: 'In order to train our generative model *P* to generate realistic celebrity
    photos using data set *Q*, we need to ensure the generative model assigns high
    probability to the data in *Q* and also to data not in *Q* but that plausibly
    could be. Mathematically, we want to maximize this ratio:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的生成模型 *P*，使其能够使用数据集 *Q* 生成逼真的名人照片，我们需要确保生成模型对 *Q* 中的数据以及不在 *Q* 中但可能存在的数据进行高概率分配。从数学上讲，我们希望最大化这个比率：
- en: '![](pg195-1.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](pg195-1.jpg)'
- en: We call this the *likelihood ratio* (LR) between *P*(*x*) and *Q*(*x*). *Likelihood*
    in this context is just another word for *probability*.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这为 *P*(*x*) 和 *Q*(*x*) 之间的 *似然比* (LR)。在这个上下文中，“似然”只是“概率”的另一个说法。
- en: If we take the ratio for an example image of Will Smith that exists in *Q* using
    an untrained *P*, we might get
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 以威尔·史密斯存在于 *Q* 中的一个示例图像为例，如果我们使用未经训练的 *P* 来计算其比率，我们可能会得到
- en: '![](pg195-2.jpg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![](pg195-2.jpg)'
- en: This is a tiny ratio. We want to backpropagate into our generative model and
    do gradient descent to update its parameters so that this ratio is maximized.
    This likelihood ratio is the objective function we want to maximize (or minimize
    its negative).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很小的比率。我们希望将反向传播输入到我们的生成模型中，并进行梯度下降来更新其参数，以使这个比率最大化。这个似然比是我们想要最大化的目标函数（或者最小化其负值）。
- en: But we don’t want to do this just for a single image; we want the generative
    model to maximize the total probability of all the images in data set *Q*. We
    can find this total probability by taking the product of all the individual examples
    (because the probability of *A and B* is the probability of *A times* the probability
    of *B* when *A* and *B* are independent and come from the same distribution).
    So our new objective function is the product of the likelihood ratios for each
    piece of data in the data set. We have several math equations coming up but we’re
    just using them to explain the underlying probability concepts; don’t spend any
    time trying to remember them.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不想只为单一图像做这件事；我们希望生成模型最大化数据集 *Q* 中所有图像的总概率。我们可以通过取所有单个示例的乘积来找到这个总概率（因为当 *A*
    和 *B* 独立且来自同一分布时，*A and B* 的概率是 *A* 的概率乘以 *B* 的概率）。因此，我们的新目标函数是数据集中每个数据点的似然比的乘积。我们将出现几个数学方程，但我们只是用它们来解释潜在的概率概念；不要花时间试图记住它们。
- en: Table 7.4\. The likelihood ratio in math and Python
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.4. 数学与Python中的似然比
- en: '| Math | Python |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | Python |'
- en: '| --- | --- |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg196-1.jpg) |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| ![pg196-1.jpg](pg196-1.jpg) |'
- en: '[PRE28]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: One problem with this objective function is that computers have a hard time
    multiplying a bunch of probabilities, since they are tiny floating-point numbers
    that when multiplied together create even smaller floating-point numbers. This
    results in numerical inaccuracies and ultimately numerical underflow, since computers
    have a finite range of numbers they can represent. To improve this situation,
    we generally use log probabilities (equivalently, log likelihoods) because the
    logarithm function turns tiny probabilities into large numbers ranging from negative
    infinity (when the probability approaches 0) up to a maximum of 0 (when the probability
    is 1).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标函数的一个问题是，计算机很难乘以许多概率，因为它们是微小的浮点数，当它们相乘时会产生更小的浮点数。这导致数值不准确，并最终导致数值下溢，因为计算机可以表示的数字范围是有限的。为了改善这种情况，我们通常使用对数概率（等价于对数似然），因为对数函数将微小的概率转换为从负无穷大到最大0（当概率为1）的较大数字。
- en: 'Logarithms also have the nice property that log(*a* × *b*) = log(*a*) + log(*b*),
    so we can turn multiplication into addition, and computers can handle that a lot
    better without risking numerical instability or overflows. We can transform the
    previous product log-likelihood ratio equation into this:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 对数还有一个很好的性质，即 log(*a* × *b*) = log(*a*) + log(*b*)，因此我们可以将乘法转换为加法，而计算机可以更好地处理这一点，而不会冒着数值不稳定性或溢出的风险。我们可以将先前的乘积对数似然比方程转换为这个：
- en: Table 7.5\. The log-likelihood ratio in math and Python
  id: totrans-461
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.5. 数学与Python中的对数似然比
- en: '| Math | Python |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | Python |'
- en: '| --- | --- |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg196-2.jpg) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| ![pg196-2.jpg](pg196-2.jpg) |'
- en: '[PRE29]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: This log-probability version of the equation is simpler and better for computation,
    but another problem is that we want to weight individual samples differently.
    For example, if we sample an image of Will Smith from the data set, it should
    have a higher probability than an image of some less famous celebrity, since the
    less famous celebrity probably has fewer photos taken of them. We want our model
    to put more weight on learning images that are more probable out in the real world
    or, in other words, with respect to the empirical distribution *Q*(*x*). We will
    weight each log-likelihood ratio by its *Q*(*x*) probability.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的对数概率版本更简单，更适合计算，但另一个问题是，我们希望对不同的样本进行不同的加权。例如，如果我们从数据集中采样威尔·史密斯的图像，它应该比一些不太出名的名人的图像有更高的概率，因为不太出名的名人可能被拍摄的照片更少。我们希望我们的模型更加重视学习在现实世界中更可能出现的图像，换句话说，与经验分布
    *Q*(*x*) 相比。我们将根据 *Q*(*x*) 的概率对每个对数似然比进行加权。
- en: Table 7.6\. The weighted log-likelihood ratio in math and Python
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.6. 数学与Python中的加权对数似然比
- en: '| Math | Python |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | Python |'
- en: '| --- | --- |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg197-1.jpg) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| ![pg197-1.jpg](pg197-1.jpg) |'
- en: '[PRE30]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '|'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: We now have an objective function that measures how likely a sample from the
    generative model is, compared to the real-world distribution of data, weighted
    by how likely the sample is in the real world.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个目标函数，它衡量从生成模型中采样的样本相对于现实世界数据分布的可能性，并按样本在现实世界中的可能性进行加权。
- en: There’s one last minor problem. This objective function must be maximized because
    we want the log-likelihood ratio to be high, but by convenience and convention
    we prefer to have objective functions that are error or loss functions to be minimized.
    We can remedy this by adding a negative sign, so a high likelihood ratio becomes
    a small error or loss.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 最后还有一个小问题。这个目标函数必须最大化，因为我们希望对数似然比很高，但为了方便和惯例，我们更喜欢将目标函数作为误差或损失函数来最小化。我们可以通过添加负号来解决这个问题，这样高的似然比就变成了小的误差或损失。
- en: Table 7.7\. The Kullback-Leibler divergence
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 7.7\. Kullback-Leibler 散度
- en: '| Math | Python |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | Python |'
- en: '| --- | --- |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![](pg197-2.jpg) |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| ![](pg197-2.jpg) |'
- en: '[PRE31]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '|'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'You may notice we switched out *LR* for some strange symbols: *D[KL]*(*Q* ‖*P*).
    It turns out the objective function we just created is a very important one in
    all of machine learning; it’s called the *Kullback-Leibler divergence* (KL divergence
    for short). The KL divergence is a kind of error function between probability
    distributions; it tells you how different two probability distributions are.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到我们用一些奇怪的符号替换了 *LR*：*D[KL]*(*Q* ‖*P*)。实际上，我们刚才创建的目标函数在所有机器学习中都非常重要；它被称为
    *Kullback-Leibler 散度*（简称KL散度）。KL散度是概率分布之间的一种误差函数；它告诉你两个概率分布有多不同。
- en: Often we are trying to minimize the distance between a model-generated probability
    distribution and some empirical distribution from real data, so we want to minimize
    the KL divergence. As you just saw, minimizing the KL divergence is equivalent
    to maximizing the joint log-likelihood ratio of the generated data compared to
    the empirical data. One important thing to note is that the KL divergence is not
    symmetric, i.e., *D[KL]*(*Q* ‖*P*) ≠ *D[KL]*(*P* ‖*Q*), and this should be clear
    from its mathematical definition. The KL divergence contains a ratio, and no ratio
    can equal its inverse unless both are 1, i.e., ![](pg197-3.jpg) unless *a* = *b*.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们试图最小化模型生成的概率分布与真实数据中的某些经验分布之间的距离，因此我们希望最小化KL散度。正如你刚才看到的，最小化KL散度等价于最大化生成数据与经验数据联合对数似然比。需要注意的是，KL散度不是对称的，即
    *D[KL]*(*Q* ‖*P*) ≠ *D[KL]*(*P* ‖*Q*)，这应该从其数学定义中清楚看出。KL散度包含一个比率，没有任何比率可以等于其倒数，除非两者都是1，即
    ![](pg197-3.jpg) 除非 *a* = *b*。
- en: Although the KL divergence makes a perfect objective function, we can simplify
    it just a bit for our purposes. Recall that log(*a*/*b*) = log(*a*) – log(*b*)
    in general. So we can rewrite the KL divergence as
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然KL散度可以构成一个完美的目标函数，但为了我们的目的，我们可以稍微简化一下。回想一下，在一般情况下，log(*a*/*b*) = log(*a*)
    – log(*b*)。因此，我们可以将KL散度重写为
- en: '| *D[KL]*(*Q* ‖*P*) = *–*Σ*[i]**Q*(*x*) × log(*P*(*x[i]*)) – log(*Q*(*x[i]*))
    |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| *D[KL]*(*Q* ‖*P*) = *–*Σ*[i]**Q*(*x*) × log(*P*(*x[i]*)) – log(*Q*(*x[i]*))
    |'
- en: 'Note that in machine learning, we only want to optimize the model (update the
    parameters of the model to reduce the error); we cannot change the empirical distribution
    *Q*(*x*). Therefore, we really only care about the weighted log probability on
    the left side:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在机器学习中，我们只想优化模型（更新模型的参数以减少误差）；我们不能改变经验分布 *Q*(*x*)。因此，我们真正关心的是左侧的加权对数概率：
- en: '| *H*(*Q*,*P*) = *–*Σ*[i]**Q*(*x*) × log(*P*(*x[i]*)) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| *H*(*Q*,*P*) = *–*Σ*[i]**Q*(*x*) × log(*P*(*x[i]*)) |'
- en: This simplified version is called the cross-entropy loss and is denoted *H*(*Q*,*P*).
    This is the actual loss function that we will use in this chapter to get the error
    between our predicted action-value distribution and a target (empirical) distribution.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化的版本被称为交叉熵损失，表示为 *H*(*Q*,*P*)。这是我们将在本章中使用以获取预测动作值分布与目标（经验）分布之间误差的实际损失函数。
- en: In [listing 7.8](#ch07ex08) we implement the cross-entropy loss as a function
    that takes a batch of action-value distributions and computes the loss between
    that and a target distribution.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 7.8](#ch07ex08) 中，我们将交叉熵损失实现为一个函数，该函数接受一批动作值分布并计算该分布与目标分布之间的损失。
- en: Listing 7.8\. The cross-entropy loss function
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.8\. 交叉熵损失函数
- en: '[PRE32]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1*** Loss between prediction distribution x and target distribution y'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 预测分布 x 和目标分布 y 之间的损失'
- en: '***2*** Loops through batch dimension'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 在批量维度上循环'
- en: '***3*** Flattens along action dimension to get a concatenated sequence of the
    distributions'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 沿动作维度展开以获得分布的连接序列'
- en: The `lossfn` function takes a prediction distribution, `x`, of dimensions *B*
    × 3 × 51 and a target distribution, `y`, of the same dimensions, and then it flattens
    the distribution over the action dimension to get a *B* × 153 matrix. Then it
    loops through each 1 × 153 row in the matrix and computes the cross entropy between
    the 1 × 153 prediction distribution and the 1 × 153 target distribution. Rather
    than explicitly summing over the product of `x` and `y`, we can combine these
    two operations and get the result in one shot by using the inner product operator,
    `@`.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '`lossfn`函数接受一个维度为*B* × 3 × 51的预测分布`x`和一个维度相同的目标分布`y`，然后它将分布展平到动作维度以获得一个*B*
    × 153的矩阵。然后它遍历矩阵中的每个1 × 153行，并计算1 × 153预测分布和1 × 153目标分布之间的交叉熵。我们不需要显式地对`x`和`y`的乘积求和，我们可以通过使用内积运算符`@`将这两个操作结合起来，并在一次操作中获取结果。'
- en: We could choose to just compute the loss between the specific action-value distribution
    for the action that was taken, but we compute the loss for all three action-value
    distributions so that the Dist-DQN learns to keep the other two actions not taken
    unchanged; it only updates the action-value distribution that was taken.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择仅计算所采取动作的具体动作值分布之间的损失，但我们计算了所有三个动作值分布的损失，这样Dist-DQN就能学会保持未采取的两个动作不变；它只更新所采取的动作值分布。
- en: 7.6\. Dist-DQN on simulated data
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6\. 在模拟数据上使用Dist-DQN
- en: Let’s test all the parts so far with a simulated target distribution to see
    if our Dist-DQN can successfully learn to match the target distribution. In [listing
    7.9](#ch07ex09) we take an initially uniform distribution, run it through our
    Dist-DQN, and update it using a synthetic vector of two reward observations.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用模拟的目标分布来测试到目前为止的所有部分，看看我们的Dist-DQN是否能够成功学会匹配目标分布。在[列表7.9](#ch07ex09)中，我们取一个初始均匀分布，通过我们的Dist-DQN运行它，并使用两个奖励观察的合成向量来更新它。
- en: Listing 7.9\. Testing with simulated data
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9\. 使用模拟数据测试
- en: '[PRE33]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '***1*** Defines the action space to be of size 3'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 定义动作空间的大小为3'
- en: '***2*** Defines the total number of Dist-DQN parameters based on layer sizes'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 根据层大小定义Dist-DQN的总参数数'
- en: '***3*** Randomly initializes a parameter vector for Dist-DQN'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 随机初始化Dist-DQN的参数向量'
- en: '***4*** Clones theta to use as a target network'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 复制theta以用作目标网络'
- en: '***5*** Synchronizes the main and target Dist-DQN parameters every 75 steps'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 每75步同步主网络和目标Dist-DQN参数'
- en: '***6*** Randomly initializes two states for testing'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 随机初始化两个测试状态'
- en: '***7*** Creates synthetic action data'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 创建合成动作数据'
- en: '***8*** Creates synthetic reward data'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 创建合成奖励数据'
- en: '***9*** Initializes a prediction batch'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 初始化一个预测批次'
- en: '***10*** Initializes a target batch'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 初始化一个目标批次'
- en: The purpose of the preceding code is to test the Dist-DQN’s ability to learn
    the distribution for two samples of synthetic data. In our synthetic data, action
    0 is associated with a reward of 0, and action 2 is associated with a reward of
    10\. We expect the Dist-DQN to learn that state 1 is associated with action 1
    and state 2 with action 2 and learn the distributions. You can see, in [figure
    7.17](#ch07fig17), with the randomly initialized parameter vector, that the prediction
    distribution for all three actions (remember, we flattened it along the action
    dimension) is pretty much a uniform distribution, whereas the target distribution
    has a peak within action 0 (since we plotted only the first sample). After training,
    the prediction and target distributions should match fairly well.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的目的是测试Dist-DQN学习两个合成数据样本分布的能力。在我们的合成数据中，动作0与奖励0相关联，动作2与奖励10相关联。我们期望Dist-DQN学会状态1与动作1相关联，状态2与动作2相关联，并学习分布。您可以在[图7.17](#ch07fig17)中看到，使用随机初始化的参数向量，所有三个动作的预测分布（记住，我们在动作维度上进行了展平）几乎是一个均匀分布，而目标分布则在动作0内有一个峰值（因为我们只绘制了第一个样本）。经过训练后，预测分布和目标分布应该相当匹配。
- en: Figure 7.17\. This shows the predicted action-value distributions produced by
    an untrained Dist-DQN and the target distribution after observing a reward. There
    are three separate action-value distributions of length 51 elements, but here
    they’ve been concatenated into one long vector to illustrate the overall fit between
    the prediction and target. The first 51 elements correspond to the action-value
    distribution of the NO-OP operation, the second 51 elements correspond to the
    action-value distribution of the UP action, and the last 51 elements correspond
    to the DOWN distribution. You can see the prediction is a completely flat (uniform)
    distribution for all three actions, whereas the target distribution has a mode
    (a peak) for action 0 and some noisy peaks for the other two actions. The goal
    is to get the prediction to match the target distribution.
  id: totrans-512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.17。这显示了未训练的 Dist-DQN 生成的预测动作值分布和观察奖励后的目标分布。有三个独立的动作值分布，长度为 51 个元素，但在这里它们被连接成一个长向量，以说明预测和目标之间的整体拟合。前
    51 个元素对应于 NO-OP 操作的动作值分布，接下来的 51 个元素对应于 UP 动作的动作值分布，最后的 51 个元素对应于 DOWN 分布。你可以看到预测对于所有三个动作都是完全平坦（均匀）的分布，而目标分布对于动作
    0 有一个模态（一个峰值），对于其他两个动作有一些噪声峰值。目标是使预测与目标分布相匹配。
- en: '![](07fig17_alt.jpg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig17_alt.jpg)'
- en: The reason why a target network is so important is very clear with Dist-DQN.
    Remember, a target network is just a copy of the main model that we update after
    some lag time. We use the target network’s prediction to create the target for
    learning, but we only use the main model parameters to do gradient descent. This
    stabilizes the training because without a target network the target distribution
    will change after each parameter update from gradient descent.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dist-DQN 中，为什么目标网络如此重要是非常清晰的。记住，目标网络只是我们在一段时间滞后后更新的主模型的一个副本。我们使用目标网络的预测来创建学习目标，但我们只使用主模型参数来进行梯度下降。这稳定了训练过程，因为没有目标网络，目标分布会在每次参数更新后从梯度下降中改变。
- en: Yet gradient descent is trying to move the parameters toward better matching
    the target distribution, so there is a circularity (hence instability) that can
    lead to the target distribution dramatically changing as a result of this dance
    between the Dist-DQN’s predictions and the target distribution. By using a lagged
    copy of the Dist-DQN prediction (via a lagged copy of the parameters, which is
    the target network), the target distribution does not change every iteration and
    is not immediately affected by the continual updates from the main Dist-DQN model.
    This significantly stabilizes the training. If you reduce the `update_rate` to
    1 and try training, you will see that the target evolves into something completely
    wrong. Let’s now look at how to train the Dist-DQN.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，梯度下降试图将参数移动到更好地匹配目标分布的方向，因此存在一个循环（因此不稳定），这可能导致目标分布由于 Dist-DQN 的预测和目标分布之间的这种舞蹈而剧烈变化。通过使用
    Dist-DQN 预测的滞后副本（通过参数的滞后副本，即目标网络），目标分布不会在每个迭代中改变，并且不会立即受到主 Dist-DQN 模型持续更新的影响。这显著稳定了训练。如果你将
    `update_rate` 减少到 1 并尝试训练，你会看到目标演变成为完全错误的东西。现在让我们看看如何训练 Dist-DQN。
- en: Listing 7.10\. Dist-DQN training on synthetic data
  id: totrans-516
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.10。在合成数据上训练 Dist-DQN
- en: '[PRE34]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1*** Adds some random noise to the rewards to mitigate overfitting'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 向奖励中添加一些随机噪声以减轻过拟合'
- en: '***2*** Uses the main model Dist-DQN to make distribution predictions'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用主模型 Dist-DQN 进行分布预测'
- en: '***3*** Uses the target network Dist-DQN to make distribution predictions (using
    lagged parameters)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用目标网络 Dist-DQN 进行分布预测（使用滞后参数）'
- en: '***4*** Uses the target network’s distributions to create the target distribution
    for learning'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 使用目标网络的分布来创建学习目标分布'
- en: '***5*** Uses the main model’s distribution prediction in the loss function'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 在损失函数中使用主模型的分布预测'
- en: '***6*** Synchronizes the target network parameters with the main model parameters'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将目标网络参数与主模型参数同步'
- en: The top graphic in [figure 7.18](#ch07fig18) shows that the target and prediction
    from Dist-DQN now match almost exactly after training (you may not even be able
    to see that there are two overlapping distributions anymore). It works! The loss
    plot on the bottom of [figure 7.18](#ch07fig18) has those spikes from each time
    the target network is synchronized to the main model and the target distribution
    suddenly changes, leading to a higher than normal loss at that time step. We can
    also look at the learned distributions for each action for each sample in the
    batch. The following listing shows how to do this.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.18](#ch07fig18)顶部的图形显示，经过训练后，Dist-DQN的目标和预测现在几乎完全匹配（你可能甚至看不到两个重叠的分布 anymore）。它成功了！[图7.18](#ch07fig18)底部的损失图显示了每次目标网络同步到主模型和目标分布突然变化时的峰值，导致该时间步的损失高于正常水平。我们还可以查看批量中每个样本的每个动作的学习分布。以下列表显示了如何做到这一点。'
- en: 'Figure 7.18\. Top: The concatenated action-value distributions for all three
    actions after training. Bottom: Loss plot over training time. The baseline loss
    is decreasing, but we see ever-increasing spikes.'
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.18\. 顶部：训练后所有三个动作的连接动作值分布。底部：训练时间上的损失图。基线损失正在下降，但我们看到持续增加的峰值。
- en: '![](07fig18_alt.jpg)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig18_alt.jpg)'
- en: Listing 7.11\. Visualizing the learned action-value distributions
  id: totrans-527
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.11\. 可视化学习到的动作值分布
- en: '[PRE35]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '***1*** Loops through experiences in batch'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 通过批量遍历经验'
- en: '***2*** Loops through each action'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 遍历每个动作'
- en: In [figure 7.19](#ch07fig19) you can see that in the first sample, the distribution
    on the left associated with action 0 has collapsed into a degenerate distribution
    at 0, just like the simulated data. Yet the other two actions remain fairly uniform
    with no clear peaks. Similarly, in the second sample in the batch, the action
    2 (DOWN) distribution is a degenerate distribution at 10, as the data was also
    degenerate (a sequence of identical samples), and the other two actions remain
    fairly uniform.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7.19](#ch07fig19)中，你可以看到在第一个样本中，与动作0相关的左侧分布已经塌缩到0的退化解分布，就像模拟数据一样。然而，其他两个动作的分布仍然相当均匀，没有明显的峰值。同样，在批量中的第二个样本中，动作2（向下）的分布是一个退化解分布，值为10，因为数据也是退化的（一系列相同的样本），其他两个动作的分布仍然相当均匀。
- en: Figure 7.19\. Each row contains the action-value distributions for an individual
    state, and each column in a row is the distribution for actions 0, 1, and 2 respectively.
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.19\. 每一行包含单个状态的动作值分布，每行中的每一列分别是动作0、1和2的分布。
- en: '![](07fig19_alt.jpg)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig19_alt.jpg)'
- en: This Dist-DQN test has almost everything we will use in a real experiment with
    Atari Freeway. There are just two functions we need before we get to playing Freeway.
    One will preprocess the states returned from the OpenAI Gym environment. We will
    get a 128-element numpy array with elements ranging from 0 to 255, and we’ll need
    to convert it to a PyTorch tensor and normalize the values to be between 0 and
    1 to moderate the size of the gradients.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Dist-DQN测试几乎包含了我们在Atari Freeway真实实验中将要使用的一切。在我们开始玩Freeway之前，我们只需要两个函数。一个将预处理来自OpenAI
    Gym环境的返回状态。我们将得到一个包含0到255之间元素的128元素numpy数组，我们需要将其转换为PyTorch张量，并将值归一化到0到1之间，以调节梯度的规模。
- en: We also need a policy function that decides which actions to take, given the
    predicted action-value distributions. With access to a full probability distribution
    over action values, we can utilize more sophisticated risk-sensitive policies.
    In this chapter, we will use a simple policy of choosing actions based on their
    expected value, in order to keep complexity to a minimum. Although we are learning
    a full probability distribution, we will choose actions based on their expected
    value, just like in ordinary Q-learning.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个策略函数，它根据预测的动作值分布来决定采取哪些动作。有了动作值的完整概率分布的访问权限，我们可以利用更复杂的风险敏感策略。在本章中，我们将使用一个简单的策略，根据动作的期望值来选择动作，以将复杂性保持在最低。尽管我们正在学习完整的概率分布，但我们仍将根据期望值来选择动作，就像在普通的Q-learning中一样。
- en: Listing 7.12\. Preprocessing states and selecting actions
  id: totrans-536
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.12\. 预处理状态和选择动作
- en: '[PRE36]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '***1*** Normalizes state values to be between 0 and 1'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将状态值归一化到0到1之间'
- en: '***2*** Loops through batch dimension of distribution'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 遍历分布的批量维度'
- en: '***3*** Computes the expectation values for each action-value distribution'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 计算每个动作值分布的期望值'
- en: '***4*** Computes the action associated with the highest expectation value'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 计算与最高期望值相关的动作'
- en: Recall, we can compute the expected (or expectation) value of a discrete distribution
    by simply taking the inner product of the support tensor with the probability
    tensor. We do this for all three actions and select the one that has the highest
    expected value. Once you get comfortable with the code here, you can try coming
    up with a more sophisticated policy, perhaps one that takes into consideration
    the variance (i.e., the confidence) of each action-value distribution.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们可以通过简单地计算支持张量与概率张量的内积来计算离散分布的期望值（或期望）。我们对所有三个动作都这样做，并选择具有最高期望值的动作。一旦你熟悉了这里的代码，你可以尝试提出一个更复杂的策略，也许是一个考虑每个动作值分布方差的策略。
- en: 7.7\. Using distributional Q-learning to play Freeway
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7\. 使用分布式Q学习来玩Freeway
- en: 'We’re finally ready to use the Dist-DQN algorithm to play the Atari game Freeway.
    We don’t need any other major functionality besides what we’ve already described.
    We will have a main Dist-DQN model and a copy—the target network to stabilize
    training. We will use an epsilon-greedy strategy with a decreasing epsilon value
    over epochs: with probability epsilon the action selection will be random, otherwise
    the action will be selected by the `get_action` function, which chooses based
    on the highest expected value. We will also use an experience replay mechanism,
    just like with an ordinary DQN.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好使用Dist-DQN算法来玩Atari游戏Freeway了。除了我们已经描述的功能外，我们不需要其他任何主要功能。我们将有一个主要的Dist-DQN模型和一个副本——目标网络，用于稳定训练。我们将使用一个随着epoch减少的epsilon值进行epsilon-greedy策略：以概率epsilon，动作选择将是随机的，否则动作将由`get_action`函数选择，该函数基于最高的期望值进行选择。我们还将使用与普通DQN相同的经验回放机制。
- en: We will also introduce a very basic form of *prioritized replay*. With normal
    experience replay, we store all the experiences the agent has in a fixed-size
    memory buffer, and new experiences displace old ones at random; then we randomly
    sample a batch from this memory buffer for training. In a game like Freeway, though,
    where almost all actions result in a –1 reward and we rarely get a +10 or –10
    reward, the experience replay memory is going to be heavily dominated by data
    that all says basically the same thing. It’s not very informative to the agent,
    and the truly significant experiences such as winning or losing the game get strongly
    diluted, significantly slowing learning.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍一种非常基本的**优先回放**形式。在正常经验回放中，我们将智能体所经历的所有经验存储在一个固定大小的内存缓冲区中，新的经验会随机替换旧的经验；然后我们从这个内存缓冲区中随机抽取一批数据进行训练。然而，在像Freeway这样的游戏中，几乎所有的动作都会导致-1的奖励，我们很少得到+10或-10的奖励，经验回放内存将主要由说同样事情的数据所主导。这对智能体来说并不很有信息量，真正重要的经验，比如赢得或输掉游戏，会被严重稀释，从而显著减缓学习速度。
- en: To alleviate this problem, whenever we take an action that leads to a winning
    or losing state of the game (i.e., when we get a reward of –10 or +10), we add
    multiple copies of this experience to the replay buffer to prevent it from being
    diluted by all the –1 reward experiences. Hence, we *prioritize* certain highly
    informative experiences over other less informative experiences because we really
    want our agent to learn which actions lead to success or failure rather than just
    game continuation.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，每当我们的动作导致游戏赢得或输掉的状态（即当我们得到-10或+10的奖励时），我们将这个经验的多个副本添加到回放缓冲区中，以防止它被所有-1奖励经验稀释。因此，我们**优先**某些高度信息性的经验，而不是其他信息量较少的经验，因为我们真的希望我们的智能体学习哪些动作会导致成功或失败，而不仅仅是游戏的持续。
- en: If you access the code for this chapter on this book’s GitHub at [http://mng.bz/JzKp](http://mng.bz/JzKp),
    you will find the code we used to record frames of the live game play during training.
    We also recorded the real-time changes in the action-value distributions so you
    can see how the game play affects the predicted distributions and vice versa.
    We do not include that code here in the book, as it would take too much space.
    In [listing 7.13](#ch07ex13) we initialize the hyperparameters and variables we’ll
    need for our Dist-DQN algorithm.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个书的GitHub上访问本章的代码[http://mng.bz/JzKp](http://mng.bz/JzKp)，你会找到我们用来记录训练期间实时游戏画面的代码。我们还记录了动作值分布的实时变化，这样你可以看到游戏如何影响预测分布，反之亦然。我们在这里的书中不包括那段代码，因为它会占用太多空间。在[列表7.13](#ch07ex13)中，我们初始化了Dist-DQN算法所需的超参数和变量。
- en: Listing 7.13\. Dist-DQN plays Freeway, preliminaries
  id: totrans-548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.13\. Dist-DQN玩Freeway，初步
- en: '[PRE37]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '***1*** Experience replay buffer using the deque data structure'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用deque数据结构的经验回放缓冲区'
- en: '***2*** Learning rate'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 学习率'
- en: '***3*** Discount factor'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 折扣因子'
- en: '***4*** Starting epsilon for epsilon-greedy policy'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** epsilon-greedy策略的起始epsilon'
- en: '***5*** Ending/minimum epsilon'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 结束/最小epsilon'
- en: '***6*** Prioritized-replay; duplicates highly informative experiences in the
    replay this many times'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 优先回放；在回放中重复这么多次数高度信息丰富的经验'
- en: '***7*** Updates the target network every 25 steps'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 每25步更新一次目标网络'
- en: '***8*** The total number of parameters for Dist-DQN'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** Dist-DQN的总参数数量'
- en: '***9*** Randomly initializes parameters for Dist-DQN'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 随机初始化Dist-DQN的参数'
- en: '***10*** Initializes parameters for target network'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 初始化目标网络的参数'
- en: '***11*** Stores each win (successful freeway crossing) as a 1 in this list'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 将每次胜利（成功的公路穿越）作为列表中的1存储'
- en: These are all the settings and starting objects we need before we get to the
    main training loop. All of it is roughly the same as what we did for the simulation
    test, except we have a prioritized replay setting that controls how many copies
    of a highly informative experience (such as a win) we should add to the replay.
    We also use an epsilon-greedy strategy, and we will start with an initially high
    epsilon value and decrease it during training to a minimum value to maintain a
    minimal amount of exploration.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入主训练循环之前，这些都是我们需要的所有设置和起始对象。所有这些都大致与我们为模拟测试所做的相同，只是我们有一个优先回放设置，该设置控制我们应该添加多少个高度信息丰富的经验（如胜利）的副本到回放中。我们还使用epsilon-greedy策略，我们将从初始的高epsilon值开始，在训练过程中将其降低到最小值，以保持最小的探索量。
- en: Listing 7.14\. The main training loop
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.14。主训练循环
- en: '[PRE38]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1*** Epsilon-greedy action selection'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** epsilon-greedy动作选择'
- en: '***2*** Takes selected action in the environment'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 在环境中采取选定的动作'
- en: '***3*** Changes reward to +10 if environment produced reward of 1 (successful
    freeway crossing)'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 如果环境产生了1的奖励（成功的公路穿越），则将奖励更改为+10'
- en: '***4*** Changes reward to –10 if game is over (no crossings after a long time)'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 如果游戏结束（长时间后没有穿越），则将奖励更改为-10'
- en: '***5*** Changes reward to –1 if original reward was 0 (game is just continuing)
    to penalize doing nothing'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 如果原始奖励是0（游戏仍在继续），则将奖励更改为-1，以惩罚无所作为'
- en: '***6*** Prepares experience as a tuple of the starting state, the observed
    reward, the action taken, and the subsequent state'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将经验作为起始状态、观察到的奖励、采取的动作和后续状态的元组准备'
- en: '***7*** Adds experience to replay memory'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将经验添加到回放内存中'
- en: '***8*** If reward is 10, that indicates a successful crossing, and we want
    to amplify this experience.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 如果奖励是10，这表示成功穿越，我们希望放大这种体验。'
- en: '***9*** Once replay buffer is full, begins training'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 一旦回放缓冲区已满，开始训练'
- en: '***10*** Gradient descent'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 梯度下降'
- en: '***11*** Synchronizes the target network parameters to the main model parameters'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 将目标网络参数同步到主模型参数'
- en: '***12*** Decrements epsilon as a function of the epoch number'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 将epsilon作为epoch数量的函数递减'
- en: '***13*** Resets the environment if the game is over'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13*** 如果游戏结束，则重置环境'
- en: Almost all of this is the same kind of code we used for the ordinary DQN a few
    chapters ago. The only changes are that we’re dealing with Q distributions rather
    than single Q values and that we use prioritized replay. If you plot the losses,
    you should get something like [figure 7.20](#ch07fig20).
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有这些都是我们之前几章中使用的普通DQN的相同类型的代码。唯一的区别是我们处理的是Q分布而不是单个Q值，并且我们使用优先回放。如果你绘制损失图，你应该得到类似于[图7.20](#ch07fig20)的东西。
- en: Figure 7.20\. The loss plot for training Dist-DQN on the Atari game Freeway.
    The loss gradually declines but has significant “spikiness” due to the periodic
    target network updates.
  id: totrans-578
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.20。在Atari游戏Freeway上训练Dist-DQN的损失图。损失逐渐下降，但由于周期性目标网络更新，具有显著的“尖峰”。
- en: '![](07fig20_alt.jpg)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![07fig20_alt.jpg](07fig20_alt.jpg)'
- en: The loss plot in [figure 7.20](#ch07fig20) generally goes down but has “spikiness”
    due to the updates of the target network, just like we saw with the simulated
    example. If you investigate the `cum_rewards` list, you should get a list of ones
    [1, 1, 1, 1, 1, 1] indicating how many successful chicken crossings occurred.
    If you’re getting four or more, that indicates a successfully trained agent.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.20](#ch07fig20)中的损失图通常呈下降趋势，但由于目标网络的更新，具有“尖峰”，就像我们在模拟示例中看到的那样。如果你调查`cum_rewards`列表，你应该得到一个包含1的列表[1,
    1, 1, 1, 1, 1]，表示成功穿越的次数。如果你得到四个或更多，这表明代理已经成功训练。'
- en: '[Figure 7.21](#ch07fig21) shows a mid-training game screenshot alongside the
    corresponding predicted action-value distributions (again, refer to the GitHub
    code to see how to do this).'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.21](#ch07fig21)展示了训练过程中的游戏截图以及相应的预测动作-价值分布（再次，参考GitHub代码了解如何实现这一点）。'
- en: 'Figure 7.21\. Left: Screenshot of live gameplay in Atari Freeway. Right: The
    corresponding action-value distributions of each of the each actions overlaid.
    The spike on the right corresponds to the UP action and the spike on the left
    corresponds mostly to the NO-OP action. Since the right spike is larger, the agent
    is more likely to take the UP action, which seems like the right thing to do in
    this case. It is difficult to see, but the UP action also has a spike on top of
    the NO-OP spike on the left, so the UP action-value distribution is bimodal, suggesting
    that taking the UP action might lead to either a –1 reward or a +10 reward, but
    the +10 reward is more likely since that spike is taller.'
  id: totrans-582
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.21. 左：Atari Freeway实时游戏截图。右：每个动作对应的动作-价值分布叠加。右边的尖峰对应向上动作，左边的尖峰主要对应无操作动作。由于右边的尖峰更大，因此智能体更有可能采取向上动作，这在这种情况下似乎是正确的。难以看到的是，向上动作在左边的无操作尖峰上方也有一个尖峰，因此向上动作的价值分布是双峰的，这表明采取向上动作可能会导致-1奖励或+10奖励，但由于尖峰更高，+10奖励的可能性更大。
- en: '![](07fig21_alt.jpg)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig21_alt.jpg)'
- en: 'In [figure 7.21](#ch07fig21) you can see that the action-value distribution
    for the UP action has two modes (peaks): one at –1 and the other at +10\. The
    expectation value of this distribution is much higher than the other actions,
    so this action will be selected.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7.21](#ch07fig21)中，您可以看到向上动作的动作-价值分布有两个模式（峰值）：一个在-1，另一个在+10。这个分布的期望值远高于其他动作，因此这个动作将被选中。
- en: '[Figure 7.22](#ch07fig22) shows a few of the learned distributions in the experience
    replay buffer, to give you a better view of the distributions. Each row is a sample
    from the replay buffer associated with a single state. Each figure in a row is
    the action-value distribution for the NO-OP, UP, and DOWN actions respectively.
    Above each figure is the expected value of that distribution. You can see that
    in all the samples, the UP action has the highest expected value, and it has two
    clear peaks: one at –1 and another at +10\. The distributions for the other two
    actions have a lot more variance, because once the agent learns that going up
    is the best way to win, there are fewer and fewer experiences using the other
    two actions, so they remain relatively uniform. If we continued training for longer,
    they would eventually converge to a peak at –1 and possibly a smaller peak at
    –10, since with epsilon greedy we will still be taking a few random actions.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.22](#ch07fig22) 展示了经验回放缓冲区中的一些学习到的分布，以便您更好地了解分布情况。每一行是从与单个状态相关的回放缓冲区中抽取的样本。每一行中的每个图分别表示无操作、向上和向下动作的动作-价值分布。每个图的上方是该分布的期望值。您可以看到，在所有样本中，向上动作的期望值最高，并且有两个清晰的峰值：一个在-1，另一个在+10。其他两个动作的分布有更多的变异性，因为一旦智能体学会向上是获胜的最佳方式，使用其他两个动作的经验就会越来越少，因此它们保持相对均匀。如果我们继续训练更长的时间，它们最终会收敛到一个峰值在-1，可能还有一个较小的峰值在-10，因为epsilon贪婪策略我们仍然会采取一些随机动作。'
- en: Figure 7.22\. Each column has the action-value distributions for a particular
    action for a given state (row). The number above each plot is the expectation
    value for that distribution, which is the weighted average value for that distribution.
    The distributions look fairly similar by eye, but the expected values are distinct
    enough to result in significantly different action selections.
  id: totrans-586
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.22. 每一列显示了给定状态（行）特定动作的动作-价值分布。每个图上方的数字是该分布的期望值，即该分布的加权平均值。从直观上看，分布看起来相当相似，但期望值足够不同，以至于导致显著不同的动作选择。
- en: '![](07fig22_alt.jpg)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig22_alt.jpg)'
- en: Distributional Q-learning is one of the biggest improvements to Q-learning in
    the past few years, and it’s still being actively researched. If you compare Dist-DQN
    to ordinary DQN, you should find you get better overall performance with Dist-DQN.
    It is not well understood why Dist-DQN performs so much better, especially given
    that we are only choosing actions based on expected values, but a few reasons
    are likely. One is that training a neural network to predict multiple things at
    the same time has been shown to improve generalization and overall performance.
    In this chapter, our Dist-DQN learned to predict three full probability distributions
    rather than a single action value, so these auxiliary tasks force the algorithm
    to learn more robust abstractions.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式Q学习是过去几年中Q学习最大的改进之一，并且仍在积极研究中。如果你将Dist-DQN与普通DQN进行比较，你应该会发现你使用Dist-DQN可以获得更好的整体性能。Dist-DQN表现如此之好的原因尚不清楚，尤其是考虑到我们只是根据期望值来选择动作，但可能有几个原因。其中一个原因是，同时训练神经网络来预测多个事物已被证明可以提高泛化能力和整体性能。在本章中，我们的Dist-DQN学会了预测三个完整的概率分布，而不是单个行为值，因此这些辅助任务迫使算法学习更鲁棒的模式。
- en: We also discussed a significant limitation in the way we’ve implemented Dist-DQN,
    namely that we’re using discrete probability distributions with finite support,
    so we can only represent action values within a very small range, from –10 to
    10\. We could make this range wider at the cost of more computational processing,
    but we can never represent an arbitrarily small or large value with this approach.
    The way we’ve implemented it is to use a fixed set of supports but learn the set
    of associated probabilities.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了我们在实现Dist-DQN时存在的一个重大限制，即我们使用具有有限支持点的离散概率分布，因此我们只能表示从-10到10的非常小的范围内的行为值。我们可以通过增加更多的计算处理来使这个范围更广，但使用这种方法我们永远无法表示任意小或大的值。我们实现它的方式是使用一组固定的支持点，但学习与之相关联的概率集。
- en: One fix to this problem is to instead use a fixed set of probabilities over
    a variable (learned) set of supports. For example, we can fix our probability
    tensor to range from 0.1 to 0.9, e.g., `array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
    0.8, 0.9])`, and we instead have our Dist-DQN predict the set of associated supports
    for these fixed probabilities. That is, we’re asking our Dist-DQN to learn what
    support value has a probability of 0.1, and 0.2, and so on. This is called *quantile
    regression* because these fixed probabilities end up representing quantiles of
    the distribution ([figure 7.23](#ch07fig23)). We learn the supports at and below
    the 50th percentile (probability 0.5), the 60th percentile, and so on.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法是改用一组固定的概率覆盖一个可变（学习得到的）支持集。例如，我们可以将我们的概率张量固定在0.1到0.9的范围内，例如`array([0.1,
    0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])`，然后我们让Dist-DQN预测这些固定概率的关联支持集。也就是说，我们要求Dist-DQN学习支持值具有0.1、0.2等概率。这被称为*分位数回归*，因为这些固定的概率最终代表了分布的分位数（[图7.23](#ch07fig23)）。我们学习位于并低于第50个百分位（概率0.5）、第60个百分位等处的支持点。
- en: Figure 7.23\. In quantile regression, rather than learning what probabilities
    are assigned to a fixed set of supports, we learn a set of supports that correspond
    to a fixed set of probabilities (quantiles). Here you can see that the median
    value is 1 since it is at the 50th percentile.
  id: totrans-591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.23。在分位数回归中，我们不是学习分配给一组固定支持的概率，而是学习一组与一组固定概率（分位数）相对应的支持。在这里，你可以看到中位数是1，因为它位于第50个百分位。
- en: '![](07fig23_alt.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![图7.23](07fig23_alt.jpg)'
- en: With this approach, we still have a discrete probability distribution, but we
    can now represent any possible action value—it can be arbitrarily small or large
    and we have no fixed range.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们仍然有一个离散的概率分布，但现在我们可以表示任何可能的行为值——它可以任意小或大，我们没有固定的范围。
- en: Summary
  id: totrans-594
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The advantages of distributional Q-learning include improved performance and
    a way to utilize risk-sensitive policies.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式Q学习的优势包括改进的性能和一种利用风险敏感策略的方法。
- en: Prioritized replay can speed learning by increasing the proportion of highly
    informative experiences in the experience replay buffer.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先回放可以通过增加经验回放缓冲区中高度信息性经验的比例来加速学习。
- en: The Bellman equation gives us a precise way to update a Q function.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman方程为我们提供了一种精确的方式来更新Q函数。
- en: The OpenAI Gym includes alternative environments that produce RAM states, rather
    than raw video frames. The RAM states are easier to learn since they are usually
    of much lower dimensionality.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym 包含了产生 RAM 状态的替代环境，而不是原始视频帧。由于 RAM 状态通常具有远低的维度，因此它们更容易学习。
- en: Random variables are variables that can take on a set of outcomes weighted by
    an underlying probability distribution.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机变量是可能根据潜在的概率分布以一定权重取一组结果的变量。
- en: The entropy of a probability distribution describes how much information it
    contains.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分布的熵描述了它包含多少信息。
- en: The KL divergence and cross-entropy can be used to measure the loss between
    two probability distributions.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KL 散度和交叉熵可以用来衡量两个概率分布之间的损失。
- en: The support of a probability distribution is the set of values that have nonzero
    probability.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分布的支持集是具有非零概率的值的集合。
- en: Quantile regression is a way to learn a highly flexible discrete distribution
    by learning the set of supports rather than the set of probabilities.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数回归是一种通过学习支持集而不是概率集来学习高度灵活的离散分布的方法。
- en: Chapter 8\. Curiosity-driven exploration
  id: totrans-604
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 章\. 好奇心驱动的探索
- en: '*This chapter covers*'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding the sparse reward problem
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解稀疏奖励问题
- en: Understanding how curiosity can serve as an intrinsic reward
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解好奇心如何作为内在奖励
- en: Playing Super Mario Bros. from OpenAI Gym
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 OpenAI Gym 中玩超级马里奥兄弟
- en: Implementing an intrinsic curiosity module in PyTorch
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现内在好奇心模块
- en: Training a deep Q-network agent to successfully play Super Mario Bros. without
    using rewards
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个深度 Q 网络智能体，在不使用奖励的情况下成功玩超级马里奥兄弟
- en: The fundamental reinforcement learning algorithms we have studied so far, such
    as deep Q-learning and policy gradient methods are very powerful techniques in
    a lot of situations, but they fail dramatically in other environments. Google’s
    DeepMind pioneered the field of deep reinforcement learning back in 2013 when
    they used deep Q-learning to train an agent to play multiple Atari games at superhuman
    performance levels. But the performance of the agent was highly variable across
    different types of games. At one extreme, their DQN agent played the Atari game
    Breakout vastly better than a human, but at the other extreme the DQN was much
    worse than a human at playing Montezuma’s Revenge ([figure 8.1](#ch08fig01)),
    where it could not even pass the first level.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止研究的基本强化学习算法，如深度 Q 学习和策略梯度方法，在许多情况下是非常强大的技术，但在其他环境中却表现不佳。2013 年，Google
    的 DeepMind 在使用深度 Q 学习训练智能体以在超级人类性能水平上玩多个 Atari 游戏时，开创了深度强化学习的领域。但智能体的性能在不同类型的游戏中高度可变。在一种极端情况下，他们的
    DQN 智能体在玩 Atari 游戏 Breakout 时远远优于人类，但在另一种极端情况下，DQN 在玩 Montezuma 的复仇（[图 8.1](#ch08fig01)）时比人类差得多，在那里它甚至无法通过第一关。
- en: Figure 8.1\. Screenshot from the Montezuma’s Revenge Atari game. The player
    must navigate through obstacles to get a key before any rewards are received.
  id: totrans-612
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1\. Montezuma 的复仇 Atari 游戏的截图。玩家必须在获得任何奖励之前通过障碍物来获取钥匙。
- en: '![](08fig01_alt.jpg)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig01_alt.jpg)'
- en: '|  |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The paper that brought great attention to the field of deep reinforcement learning
    was “Human-level control through deep reinforcement learning” by Volodymyr Mnih
    and collaborators at Google DeepMind in 2015\. The paper is fairly readable and
    contains the details you’d need to replicate their results.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年，Volodymyr Mnih 和 Google DeepMind 的同事们发表的论文“通过深度强化学习实现人类水平控制”引起了深度强化学习领域的广泛关注。这篇论文相当易读，并包含了复制他们结果所需的详细信息。
- en: '|  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: What’s the difference between the environments that explains these disparities
    in performance? The games that DQN was successful at all gave relatively frequent
    rewards during game play and did not require significant long-term planning. Montezuma’s
    Revenge, on the other hand, only gives a reward after the player finds a key in
    the room, which also contains numerous obstacles and enemies. With a vanilla DQN,
    the agent starts exploring essentially at random. It will take random actions
    and wait to observe rewards, and those rewards reinforce which actions are best
    to take given the environment. But in the case of Montezuma’s Revenge, it is extremely
    unlikely that the agent will find the key and get a reward with this random exploration
    policy, so it will never observe a reward and will never learn.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 什么差异导致了这些性能差异的环境？DQN在所有成功的游戏中都提供了相对频繁的奖励，并且不需要显著的长期规划。另一方面，Montezuma’s Revenge只有在玩家在房间里找到钥匙后才会给予奖励，这个房间还包含了许多障碍和敌人。使用普通的DQN，智能体基本上是随机探索的。它会采取随机行动并等待观察奖励，这些奖励会强化在给定环境中采取的最佳行动。但在Montezuma’s
    Revenge的情况下，智能体通过这种随机探索策略找到钥匙并获得奖励的可能性极低，因此它将永远不会观察到奖励，也永远不会学习。
- en: This problem is called the *sparse reward problem*, since the rewards in the
    environment are sparsely distributed ([figure 8.2](#ch08fig02)). If the agent
    doesn’t observe enough reward signals to reinforce its actions, it can’t learn.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为*稀疏奖励问题*，因为环境中的奖励分布得非常稀疏（[图8.2](#ch08fig02)）。如果智能体没有观察到足够的奖励信号来强化其行动，它就无法学习。
- en: Figure 8.2\. In environments with dense rewards, the rewards are received fairly
    frequently during the training time, making it easy to reinforce actions. In sparse
    reward environments, rewards may only be received after many sub-goals are completed,
    making it difficult or impossible for an agent to learn based on reward signals
    alone.
  id: totrans-620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2\. 在密集奖励的环境中，奖励在训练时间内相对频繁地获得，这使得强化行动变得容易。在稀疏奖励环境中，奖励可能只有在完成许多子目标之后才会获得，这使得智能体仅基于奖励信号学习变得困难或不可能。
- en: '![](08fig02_alt.jpg)'
  id: totrans-621
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig02_alt.jpg)'
- en: Animal and human learning offer us the only natural examples of intelligent
    systems, and we can turn to them for inspiration. Indeed, researchers trying to
    tackle this sparse reward problem noticed that humans not only maximize extrinsic
    rewards (those from the environment), like food and sex, but they also demonstrate
    an intrinsic curiosity, a motivation to explore just for the sake of understanding
    how things work and to reduce their uncertainty about their environment.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 动物和人类的学习为我们提供了唯一自然存在的智能系统例子，我们可以从中获得灵感。确实，试图解决这个稀疏奖励问题的研究人员注意到，人类不仅最大化外在奖励（来自环境的奖励），如食物和性，而且表现出内在的好奇心，一种仅仅为了理解事物如何运作以及减少对环境不确定性的探索动机。
- en: In this chapter you will we learn about methods for successfully training reinforcement
    learning agents in sparse reward environments by using principles from human intelligence,
    specifically our innate curiosity. You will see how curiosity can drive the development
    of basic skills that the agent can use to accomplish sub-goals and find the sparse
    rewards. In particular, you will see how a curiosity-powered agent can play the
    Atari game Super Mario Bros. and learn how to navigate the dynamic terrain just
    by curiosity alone.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习使用人类智能的原则，特别是我们天生的好奇心，来成功训练稀疏奖励环境中的强化学习智能体的方法。您将看到好奇心如何推动智能体发展基本技能，这些技能可以帮助智能体完成子目标并找到稀疏奖励。特别是，您将看到好奇心驱动的智能体如何玩Atari游戏超级马里奥兄弟，并仅通过好奇心学习如何导航动态地形。
- en: '|  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-625
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: 'The code for this chapter is in this book’s GitHub repository in the [chapter
    8](#ch08) folder: [http://mng.bz/JzKp](http://mng.bz/JzKp).'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于本书GitHub仓库的[第8章](#ch08)文件夹中：[http://mng.bz/JzKp](http://mng.bz/JzKp)。
- en: '|  |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 8.1\. Tackling sparse rewards with predictive coding
  id: totrans-628
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 使用预测编码解决稀疏奖励问题
- en: In the world of neuroscience, and particularly computational neuroscience, there
    is a framework for understanding neural systems at a high level called the *predictive
    coding model*. In this model, the theory says that all neural systems from individual
    neurons up to large-scale neural networks are running an algorithm that attempts
    to predict inputs, and hence tries to minimize the *prediction error* between
    what it expects to experience and what it actually experiences. So at a high level,
    as you’re going about your day, your brain is taking in a bunch of sensory information
    from the environment, and it’s training to predict how the sensory information
    will evolve. It’s trying to stay one step ahead of the actual raw data coming
    in.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经科学的世界里，尤其是在计算神经科学中，有一个用于理解神经系统的框架，称为**预测编码模型**。在这个模型中，理论认为，从单个神经元到大型神经网络的所有神经网络都在运行一个试图预测输入的算法，因此试图最小化它期望体验和实际体验之间的**预测误差**。所以从高层次来看，当你忙于日常事务时，你的大脑正在从环境中吸收大量的感官信息，并且正在训练预测这些感官信息将如何演变。它试图领先于实际传入的原始数据。
- en: If something surprising (i.e., unexpected) happens, your brain experiences a
    large prediction error and then presumably does some parameter updating to prevent
    that from happening again. For example, you might be talking to someone you just
    met, and your brain is constantly trying to predict the next word that person
    will say before they say it. Since this is someone you don’t know, your brain
    will probably have a relatively high average prediction error, but if you become
    best friends, you’ll probably be quite good at finishing their sentences. This
    is not something you try to do; whether you want to or not, your brain is trying
    to reduce its prediction error.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生一些令人惊讶（即意外）的事情，你的大脑会经历一个大的预测误差，然后可能进行一些参数更新以防止再次发生。例如，你可能在和刚认识的人交谈，你的大脑会不断尝试预测那个人在说出下一句话之前会说什么。由于你并不了解这个人，你的大脑可能有一个相对较高的平均预测误差，但如果你成为最好的朋友，你可能会非常擅长完成他们的句子。这不是你试图做的事情；无论你愿不愿意，你的大脑都在试图减少其预测误差。
- en: Curiosity can be thought of as a kind of desire to reduce the uncertainty in
    your environment (and hence reduce prediction errors). If you were a software
    engineer and you saw some online posts about this interesting field called machine
    learning, your curiosity to read a book like this would be based on the goal of
    reducing your uncertainty about machine learning.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 好奇心可以被视为一种减少你环境中不确定性（从而减少预测误差）的欲望。如果你是一名软件工程师，并且看到了一些关于这个有趣领域机器学习的在线帖子，你想要阅读这样一本书的好奇心将基于减少你对机器学习不确定性的目标。
- en: One of the first attempts to imbue reinforcement learning agents with a sense
    of curiosity involved using a prediction error mechanism. The idea was that in
    addition to trying to maximize extrinsic (i.e., environment-provided) rewards,
    the agent would also try to predict the next state of the environment given its
    action, and it would try to reduce its prediction error. In very familiar areas
    of an environment, the agent would learn how it works and would have a low prediction
    error. By using this prediction error as another kind of reward signal, the agent
    would be incentivized to visit areas of the environment that were novel and unknown.
    That is, the higher the prediction error is, the more surprising a state is, and
    therefore the agent should be incentivized to visit these high prediction error
    states. [Figure 8.3](#ch08fig03) shows the basic framework for this approach.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 试图赋予强化学习智能体好奇心感的第一种尝试涉及使用预测误差机制。想法是，除了试图最大化外部（即环境提供的）奖励外，智能体还会尝试根据其动作预测环境的下一个状态，并且它会尝试减少其预测误差。在环境非常熟悉的部分，智能体会学习它的工作方式，并且会有一个低的预测误差。通过使用这种预测误差作为另一种奖励信号，智能体将受到激励去访问环境中的新颖和未知区域。也就是说，预测误差越高，状态越令人惊讶，因此智能体应该被激励去访问这些高预测误差状态。[图8.3](#ch08fig03)展示了这种方法的框架。
- en: Figure 8.3\. Prediction error is summed with the extrinsic environment reward
    for use by the agent.
  id: totrans-633
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3\. 预测误差与外部环境奖励相加，供智能体使用。
- en: '![](08fig03_alt.jpg)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig03_alt.jpg)'
- en: The idea is to sum the prediction error (which we will call the *intrinsic reward*)
    with the extrinsic reward and use that total as the new reward signal for the
    environment. Now the agent is incentivized to not only figure out how to maximize
    the environment reward but also to be curious about the environment. The prediction
    error is calculated as shown in [figure 8.4](#ch08fig04).
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是将预测误差（我们将称之为*内在奖励*）与外在奖励相加，并使用这个总和作为环境的新奖励信号。现在代理不仅被激励去找出如何最大化环境奖励，而且对环境保持好奇心。预测误差的计算方式如[图8.4](#ch08fig04)所示。
- en: Figure 8.4\. The prediction module takes in a state, *S[t]*, (and action *a[t]*,
    not shown) and produces a prediction for the subsequent state, *Š[t+1]* (pronounced
    “S hat t+1”, where the hat symbol suggests an approximation). This prediction,
    along with the true next state, are passed to a mean-squared error function (or
    some other error function), which produces the prediction error.
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4. 预测模块接收一个状态，*S[t]*（以及未显示的动作 *a[t]*）并产生对后续状态 *Š[t+1]*（发音为“S hat t+1”，其中帽子符号表示近似）的预测。这个预测，连同真实下一个状态，被传递到一个均方误差函数（或某些其他误差函数），它产生预测误差。
- en: '![](08fig04_alt.jpg)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig04_alt.jpg)'
- en: The intrinsic reward is based on the prediction error of states in the environment.
    This works fairly well on the first pass, but people eventually realized that
    it runs into another problem, often called the “noisy TV problem” ([figure 8.5](#ch08fig05)).
    It turns out that if you train these agents in an environment that has a constant
    source of randomness, such as a TV screen playing random noise, the agent will
    have a constantly high prediction error and will be unable to reduce it. It just
    stares at the noisy TV indefinitely, since it is highly unpredictable and thus
    provides a constant source of intrinsic rewards. This is more than just an academic
    problem, since many real-world environments have similar sources of randomness
    (e.g., a tree’s leaves rustling in the wind).
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 内在奖励是基于环境中状态的预测误差。这在第一次尝试中效果相当好，但人们最终意识到它遇到了另一个问题，通常被称为“噪声电视问题”([图8.5](#ch08fig05))。结果发现，如果你在一个有恒定随机源的环境中训练这些代理，比如播放随机噪声的电视屏幕，代理将会有一个持续的高预测误差，并且无法降低它。它只是无限期地盯着噪声电视看，因为它高度不可预测，因此提供了持续的内在奖励来源。这不仅仅是一个学术问题，因为许多现实世界环境有类似的随机源（例如，风中树叶的沙沙声）。
- en: Figure 8.5\. The noisy TV problem is a theoretical and practical problem where
    a reinforcement learning agent with a naive sense of curiosity will become entranced
    by a noisy TV, forever staring at it. This is because it is intrinsically rewarded
    by unpredictability, and white noise is very unpredictable.
  id: totrans-639
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5. 噪声电视问题是一个理论和实践问题，其中具有天真好奇感的强化学习代理会着迷于噪声电视，永远盯着它看。这是因为它本质上被不可预测性所奖励，而白噪声是非常不可预测的。
- en: '![](08fig05_alt.jpg)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig05_alt.jpg)'
- en: At this point, it seems like prediction error has a lot of potential, but the
    noisy TV problem is a big flaw. Perhaps we shouldn’t pay attention to the absolute
    prediction error but instead to the rate of change of prediction error. When the
    agent transitions to an unpredictable state, it will experience a transient surge
    of prediction error, but then it goes away. Similarly, if the agent encounters
    a noisy TV, at first it is highly unpredictable and therefore has a high prediction
    error, but the high prediction error is maintained, so the rate of change is zero.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，预测误差似乎有很大的潜力，但噪声电视问题是一个很大的缺陷。也许我们不应该关注绝对预测误差，而应该关注预测误差的变化率。当代理过渡到一个不可预测的状态时，它会经历一个短暂的预测误差激增，然后它就会消失。同样，如果代理遇到噪声电视，最初它是非常不可预测的，因此具有很高的预测误差，但高预测误差被维持，所以变化率为零。
- en: This formulation is better, but it still has some potential issues. Imagine
    that an agent is outside and sees a tree with leaves blowing in the wind. The
    leaves are blowing around randomly, so this is a high prediction error. The wind
    stops blowing, and the prediction error goes down, since the leaves are not moving
    anymore. Then the wind starts blowing again, and prediction error goes up. In
    this case, even if we’re using a prediction error rate, the rate will be fluctuating
    along with the wind. We need something more robust.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式更好，但仍然存在一些潜在问题。想象一下，一个智能体在外面看到一棵树叶在风中飘动的树。树叶随机地飘动，因此这是一个高预测误差。当风停止吹动时，预测误差下降，因为树叶不再移动。然后风又开始吹动，预测误差上升。在这种情况下，即使我们使用预测误差率，该比率也会随着风的波动而波动。我们需要更稳健的东西。
- en: We want to use this prediction error idea, but we don’t want it to be vulnerable
    to trivial randomness or unpredictability in the environment that doesn’t matter.
    How do we add in the “doesn’t matter” constraint to the prediction error module?
    Well, when we say that something doesn’t matter, we mean that it does not affect
    us or is perhaps uncontrollable. If leaves are randomly blowing in the wind, the
    agent’s actions don’t affect the leaves, and the leaves don’t affect the actions
    of the agent. It turns out we can implement this idea as a separate module, in
    addition to the state prediction module—that’s the subject of this chapter. This
    chapter is based on elucidating and implementing the idea from a paper by Deepak
    Pathak et al. titled “Curiosity-driven Exploration by Self-supervised Prediction”
    (2017), which successfully resolves the issues we’ve been discussing.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要使用这个预测误差的想法，但不想让它对环境中无关紧要的随机性或不可预测性过于敏感。我们如何将“无关紧要”的约束添加到预测误差模块中？好吧，当我们说某件事无关紧要时，我们的意思是它不会影响我们，或者可能无法控制。如果树叶在风中随机飘动，智能体的动作不会影响树叶，树叶也不会影响智能体的动作。结果我们发现，我们可以将这个想法作为一个独立的模块来实现，除了状态预测模块——这就是本章的主题。本章基于阐明和实现Deepak
    Pathak等人撰写的论文中的想法，该论文题为“通过自监督预测驱动的探索”（2017年），该论文成功地解决了我们一直在讨论的问题。
- en: We will follow this paper pretty closely because it was one of the biggest contributions
    to solving the sparse reward problem, and this paper led to a flurry of related
    research. It also turns out to describe one of the easiest algorithms to implement,
    among the many others in this area. In addition, one of the goals of this book
    is to not only teach you the foundational knowledge and skills of reinforcement
    learning, but to give you a solid-enough mathematics background to be able to
    read and understand reinforcement learning papers and implement them on your own.
    Of course, some papers require advanced mathematics, and they are outside the
    scope of this book, but many of the biggest papers in the field require only some
    basic calculus, algebra, and linear algebra—things that you probably know if you
    have made it this far. The only real barrier is getting past the mathematical
    notation, which we hope to make easier here. We want to teach you how to fish
    rather than just giving you fish, as the saying goes.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将非常关注这篇论文，因为它是对解决稀疏奖励问题的一大贡献，这篇论文也引发了一系列相关研究。此外，这篇论文还描述了在这个领域众多算法中，最容易实现的一个算法。此外，本书的一个目标不仅是教授你强化学习的基础知识和技能，而且要给你一个足够的数学背景，以便能够阅读和理解强化学习论文，并自己实现它们。当然，有些论文需要高级数学，这些内容超出了本书的范围，但该领域的许多最大论文只需要一些基本的微积分、代数和线性代数——如果你已经走到这一步，你很可能已经知道这些。唯一的真正障碍是克服数学符号，我们希望在这里使它更容易理解。正如俗话所说，我们想要教会你如何钓鱼，而不是仅仅给你鱼。
- en: 8.2\. Inverse dynamics prediction
  id: totrans-645
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 逆动力学预测
- en: We’ve described how we could use the prediction error as a curiosity signal.
    The prediction error module from the last section is implemented as a function,
    *f*:(*S[t]*,*a[t]*) → *Ŝ**[t]*[+1], that takes a state and the action taken and
    returns the predicted next state ([figure 8.6](#ch08fig06)). It is predicting
    the future (forward) state of the environment, so we call it the *forward-prediction
    model*.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述了如何将预测误差用作好奇心信号。上一节的预测误差模块实现为一个函数，*f*:(*S[t]*,*a[t]*) → *Ŝ**[t]*[+1]，它接受一个状态和采取的动作，并返回预测的下一个状态（[图8.6](#ch08fig06)）。它正在预测环境的未来（正向）状态，所以我们称它为*正向预测模型*。
- en: Figure 8.6\. Diagram of the forward-prediction module function, *f:(S[t],a[t])
    → Š[t+1]*, which maps a current state and action to a predicted next state.
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6\. 前向预测模块函数 *f:(S[t],a[t]) → Š[t+1]* 的示意图，它将当前状态和动作映射到一个预测的下一个状态。
- en: '![](08fig06.jpg)'
  id: totrans-648
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig06.jpg)'
- en: Remember, we want to only predict aspects of the state that actually matter,
    not parts that are trivial or noise. The way we build in the “doesn’t matter”
    constraint to the prediction model is to add another model called an *inverse
    model*, *g*:(*S[t]*,*S[t]*[+1]) → *â**[t]*. This is a function, *g*, that takes
    a state and the next state, and then returns a prediction for which action was
    taken that led to the transition from *s[t]* to *s[t]*[+1], as shown in [figure
    8.7](#ch08fig07).
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们只想预测状态中真正重要的方面，而不是琐碎或噪声的部分。我们将“不重要”的约束添加到预测模型中的方法是通过添加另一个称为*逆模型*的模型，*g*:(*S[t]*,*S[t]*[+1])
    → *â**[t]*。这是一个函数，*g*，它接受一个状态和下一个状态，然后返回一个预测，即导致从 *s[t]* 到 *s[t]*[+1] 的转换所采取的动作，如图8.7所示。
- en: Figure 8.7\. The inverse model takes two consecutive states and tries to predict
    which action was taken.
  id: totrans-650
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7\. 逆模型接受两个连续的状态并试图预测执行了哪个动作。
- en: '![](08fig07.jpg)'
  id: totrans-651
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig07.jpg)'
- en: On its own, this inverse model is not really useful; there’s an additional model
    that is tightly coupled to the inverse model called the *encoder model*, denoted
    φ. The encoder function, ![](pg221-1.jpg), takes a state and returns an encoded
    state ![](common1.jpg) such that the dimensionality of ![](common1.jpg) is significantly
    lower than the raw state *S[t]* ([figure 8.8](#ch08fig08)). The raw state might
    be an RGB video frame with height, width, and channel dimensions, and φ will encode
    that state into a low-dimensional vector. For example, a frame might be 100 pixels
    by 100 pixels by 3 color channels for a total of 30,000 elements. Many of those
    pixels will be redundant and not useful, so we want our encoder to encode this
    state into say a 200-element vector with high-level non-redundant features.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，这个逆模型并不真正有用；还有一个与逆模型紧密耦合的附加模型，称为*编码器模型*，表示为φ。编码器函数，![](pg221-1.jpg)，接受一个状态并返回一个编码后的状态![](common1.jpg)，使得![](common1.jpg)的维度显著低于原始状态
    *S[t]* ([图8.8](#ch08fig08))。原始状态可能是一个具有高度、宽度和通道维度的RGB视频帧，φ将这个状态编码成一个低维向量。例如，一个帧可能是100像素×100像素×3个颜色通道，总共30,000个元素。其中许多像素可能是冗余的，没有用，因此我们希望我们的编码器将这个状态编码成一个200个元素的向量，包含高级非冗余特征。
- en: Figure 8.8\. The encoder model takes a high-dimensional state representation
    such as an RGB array and encodes it as a low-dimensional vector.
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8\. 编码器模型接受一个高维状态表示，例如RGB数组，并将其编码为一个低维向量。
- en: '![](08fig08.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig08.jpg)'
- en: '|  |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-656
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: A variable with the tilde symbol over it, such as ![](common1.jpg), denotes
    some sort of transformed version of the underlying variable, which may have different
    dimensionality. A variable with the hat symbol over it, such as ![](common1.jpg),
    denotes an approximation (or prediction) of the underlying state and has the same
    dimensionality.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在变量上方带有波浪线符号，例如![](common1.jpg)，表示底层变量的某种转换版本，它可能具有不同的维度。在变量上方带有帽子符号，例如![](common1.jpg)，表示底层状态的近似（或预测），并且具有相同的维度。
- en: '|  |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The encoder model is trained via the inverse model because we actually use the
    encoded states as inputs to the forward and inverse models *f* and *g* rather
    than the raw states. That is, the forward model becomes a function, ![](pg217-1.jpg),
    where ![](pg217-2.jpg) refers to a prediction of the encoded state, and the inverse
    model becomes a function, ![](pg217-3.jpg) ([figure 8.9](#ch08fig09)). The notation
    *P*:*a* × *b* → *c* means that we define some function *P* that takes a pair (*a*,*b*)
    and transforms it into a new object *c*.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器模型是通过逆模型进行训练的，因为我们实际上使用的是编码后的状态作为前向和逆模型 *f* 和 *g* 的输入，而不是原始状态。也就是说，前向模型变成了一个函数，![](pg217-1.jpg)，其中![](pg217-2.jpg)指的是对编码状态的预测，而逆模型变成了一个函数，![](pg217-3.jpg)
    ([图8.9](#ch08fig09))。符号 *P*:*a* × *b* → *c* 表示我们定义了一个函数 *P*，它接受一对 (*a*,*b*) 并将其转换成一个新的对象
    *c*。
- en: Figure 8.9\. The forward-prediction module actually uses encoded states, not
    the raw states. Encoded states are denoted φ(*S[t]*) or ![](common1.jpg).
  id: totrans-660
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9\. 前向预测模块实际上使用的是编码后的状态，而不是原始状态。编码后的状态表示为φ(*S[t*)*) 或 ![](common1.jpg)。
- en: '![](08fig09.jpg)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig09.jpg)'
- en: The encoder model isn’t trained directly—it is *not* an autoencoder. It is only
    trained through the inverse model. The inverse model is trying to predict the
    action that was taken to transition from one state to the next using the encoded
    states as inputs, and in order to minimize its own prediction error, its error
    will backpropagate through to the encoder model as well as itself. The encoder
    model will then learn to encode states in a way that is useful for the task of
    the inverse model. Importantly, although the forward model also uses the encoded
    states as inputs, we do *not* backpropagate from the forward model to the encoder
    model. If we did, the forward model would coerce the encoder model into mapping
    all states into a single fixed output, since that would be the easiest to predict.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 编码模型不是直接训练的——它**不是**一个自动编码器。它仅通过逆模型进行训练。逆模型试图使用编码状态作为输入来预测从一种状态转换到下一个状态所采取的动作，并且为了最小化其自身的预测错误，其错误将反向传播到编码模型以及自身。然后，编码模型将学会以对逆模型任务有用的方式编码状态。重要的是，尽管前向模型也使用编码状态作为输入，但我们**不**从前向模型反向传播到编码模型。如果我们这样做，前向模型会迫使编码模型将所有状态映射到单个固定的输出，因为那将是最容易预测的。
- en: '[Figure 8.10](#ch08fig10) shows the overall graph structure: the forward pass
    of the components and also the backward (backpropagation) pass to update the model
    parameters. It is worth repeating that the inverse model backpropagates back through
    to the encoder model, and the encoder model is only trained together with the
    inverse model. We must use PyTorch’s `detach()` method to detach the forward model
    from the encoder so it won’t backpropagate into the encoder. The purpose of the
    encoder is not to give us a low-dimensional input for improved performance but
    to learn to encode the state using a representation that only contains information
    relevant for predicting actions. This means that aspects of the state that are
    randomly fluctuating and have no impact on the agent’s actions will be stripped
    from this encoded representation. This, in theory, should avoid the noisy TV problem.'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.10](#ch08fig10) 展示了整体的图结构：组件的前向传递以及反向（反向传播）传递以更新模型参数。值得重复的是，逆模型反向传播回编码模型，并且编码模型仅与逆模型一起训练。我们必须使用PyTorch的
    `detach()` 方法将前向模型从编码器中分离出来，这样它就不会反向传播到编码器。编码器的目的不是为了给我们一个低维输入以改善性能，而是学会使用仅包含与预测动作相关的信息的表示来编码状态。这意味着那些随机波动且对代理动作没有影响的状态方面将从这种编码表示中去除。理论上，这应该可以避免噪声电视问题。'
- en: Figure 8.10\. The curiosity module. First the encoder will encode states *S[t]*
    and *S[t+1]* into low-dimensional vectors, φ(*S[t]*) and φ(*S[t+1]*) respectively.
    These encoded states are passed to the forward and inverse models. Notice that
    the inverse model backpropagates to the encoded model, thereby training it through
    its own error. The forward model is trained by backpropagating from its own error
    function, but it does not backpropagate through to the encoder like the inverse
    model does. This ensures that the encoder learns to produce state representations
    that are only useful for predicting which action was taken. The black circle indicates
    a copy operation that copies the output from the encoder and passes the copies
    to the forward and inverse models.
  id: totrans-664
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10. 好奇心模块。首先，编码器将状态 *S[t]* 和 *S[t+1]* 编码成低维向量，分别记为 φ(*S[t]*) 和 φ(*S[t+1]*)。这些编码后的状态被传递到前向和逆模型。请注意，逆模型反向传播到编码模型，从而通过其自身的错误来训练它。前向模型通过其自身的错误函数进行反向传播来训练，但它不像逆模型那样反向传播到编码器。这确保了编码器学会产生仅对预测采取的动作有用的状态表示。黑色圆圈表示一个复制操作，它复制编码器的输出并将副本传递到前向和逆模型。
- en: '![](08fig10_alt.jpg)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10的替代图片](08fig10_alt.jpg)'
- en: Notice that for both the forward and inverse models we need access to the data
    for a full transition, i.e., we need (*S[t]*,*a[t]*,*S[t]*[+1]). This is not an
    issue when we use an experience replay memory, as we did in [chapter 3](kindle_split_012.html#ch03)
    about deep Q-learning, since the memory will store a bunch of these kinds of tuples.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于前向和逆模型，我们需要访问完整转换的数据，即我们需要 (*S[t]*, *a[t]*, *S[t+1]*)。当我们使用经验回放记忆时，这不是一个问题，正如我们在第3章（关于深度Q学习的[kindle_split_012.html#ch03]）中所述，因为记忆将存储大量这类元组。
- en: 8.3\. Setting up Super Mario Bros.
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3. 设置超级马里奥兄弟。
- en: Together, the forward, inverse, and encoder models form the *intrinsic curiosity
    module* (ICM), which we will discuss in detail later in this chapter. The components
    of the ICM function together for the sole purpose of generating an intrinsic reward
    that drives curiosity in the agent. The ICM generates a new intrinsic reward signal
    based on information from the environment, so it is independent of how the agent
    model is implemented. The ICM can be used for any type of environment, but it
    will be most useful in sparse reward environments.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: Together, the forward, inverse, and encoder models form the *intrinsic curiosity
    module* (ICM), which we will discuss in detail later in this chapter. The components
    of the ICM function together for the sole purpose of generating an intrinsic reward
    that drives curiosity in the agent. The ICM generates a new intrinsic reward signal
    based on information from the environment, so it is independent of how the agent
    model is implemented. The ICM can be used for any type of environment, but it
    will be most useful in sparse reward environments.
- en: We could use whatever agent model implementation we want, such as a distributed
    actor-critic model (covered in [chapter 5](kindle_split_014.html#ch05)). In this
    chapter we will use a Q-learning model to keep things simple and focus on implementing
    the ICM. We will use Super Mario Bros. as our testbed.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: We could use whatever agent model implementation we want, such as a distributed
    actor-critic model (covered in [chapter 5](kindle_split_014.html#ch05)). In this
    chapter we will use a Q-learning model to keep things simple and focus on implementing
    the ICM. We will use Super Mario Bros. as our testbed.
- en: Super Mario Bros. does not really suffer from the sparse reward problem. The
    particular environment implementation we will use provides a reward in part based
    on forward progress through the game, so positive rewards are almost continuously
    provided. However, Super Mario Bros. is still a great choice to test the ICM because
    we can choose to “turn off” the extrinsic (environment-provided) reward signal;
    we can see how well the agent explores the environment just based on curiosity,
    and we can see how well correlated the extrinsic and intrinsic rewards are.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: Super Mario Bros. does not really suffer from the sparse reward problem. The
    particular environment implementation we will use provides a reward in part based
    on forward progress through the game, so positive rewards are almost continuously
    provided. However, Super Mario Bros. is still a great choice to test the ICM because
    we can choose to “turn off” the extrinsic (environment-provided) reward signal;
    we can see how well the agent explores the environment just based on curiosity,
    and we can see how well correlated the extrinsic and intrinsic rewards are.
- en: The implementation of Super Mario Bros. we will use has 12 discrete actions
    that can be taken at each time step, including a NO-OP (no-operation) action.
    [Table 8.1](#ch08table01) lists all the actions.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: The implementation of Super Mario Bros. we will use has 12 discrete actions
    that can be taken at each time step, including a NO-OP (no-operation) action.
    [Table 8.1](#ch08table01) lists all the actions.
- en: Table 8.1\. Actions in Super Mario Bros.
  id: totrans-672
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Table 8.1\. Super Mario Bros.中的动作
- en: '| Index | Action |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| Index | Action |'
- en: '| --- | --- |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| --- | ---'
- en: '| 0 | NO-OP / Do nothing |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| 0 | NO-OP / 不做任何操作 |'
- en: '| 1 | Right |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 右 |'
- en: '| 2 | Right + Jump |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 右 + 跳跃 |'
- en: '| 3 | Right + Run |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 右 + 跑 |'
- en: '| 4 | Right + Jump + Run |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 右 + 跳跃 + 跑 |'
- en: '| 5 | Jump |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 跳跃 |'
- en: '| 6 | Left |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 左 |'
- en: '| 7 | Left + Run |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 左 + 跑 |'
- en: '| 8 | Left + Jump |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 左 + 跳跃 |'
- en: '| 9 | Left + Jump + Run |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 左 + 跳跃 + 跑 |'
- en: '| 10 | Down |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 下 |'
- en: '| 11 | Up |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 上 |'
- en: 'You can install Super Mario Bros. by itself with `pip`:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 'You can install Super Mario Bros. by itself with `pip`:'
- en: '[PRE39]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: After it is installed, you can test the environment (e.g., try running this
    code in a Jupyter Notebook) by playing a random agent and taking random actions.
    To review how to use the OpenAI Gym, please refer back to [chapter 4](kindle_split_013.html#ch04).
    In the following listing we instantiate the Super Mario Bros. environment and
    test it by taking random actions.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: After it is installed, you can test the environment (e.g., try running this
    code in a Jupyter Notebook) by playing a random agent and taking random actions.
    To review how to use the OpenAI Gym, please refer back to [chapter 4](kindle_split_013.html#ch04).
    In the following listing we instantiate the Super Mario Bros. environment and
    test it by taking random actions.
- en: Listing 8.1\. Setting up the Super Mario Bros. environment
  id: totrans-690
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Listing 8.1\. 设置Super Mario Bros.环境
- en: '[PRE40]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '***1*** This wrapper module will make the action-space smaller by combining
    actions together.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 此封装模块将通过组合动作来减小动作空间。'
- en: '***2*** There are two sets of action-spaces we can import: one with 5 actions
    (simple) and one with 12 (complex).'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 我们可以导入两组动作空间：一组包含5个动作（简单）和一组包含12个动作（复杂）。'
- en: '***3*** Wraps the environment’s action space to be 12 discrete actions'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将环境动作空间封装为12个离散动作'
- en: '***4*** Tests the environment by taking random actions'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 通过随机动作测试环境'
- en: If everything went well, a little window should pop up displaying Super Mario
    Bros., but it will be taking random actions and not making any forward progress
    through the level. By the end of this chapter you will have trained an agent that
    makes consistent forward progress and has learned to avoid or jump on enemies
    and to jump over obstacles. This, only using the intrinsic curiosity-based reward.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，应该会弹出一个显示超级马里奥兄弟的小窗口，但它将随机行动，不会在关卡中取得任何进展。到本章结束时，你将训练出一个能够持续向前推进并学会避开或跳过敌人的智能体。这仅使用基于内在好奇心的奖励。
- en: In the OpenAI Gym interface, the environment is instantiated as a class object
    called `env` and the main method you need to use is its `step(...)` method. The
    `step` method takes an integer representing the action to be taken. As with all
    OpenAI Gym environments, this one returns `state`, `reward`, `done`, and `info`
    data after each action is taken. The `state` is a numpy array with dimensions
    (240, 256, 3) representing an RGB video frame. The `reward` is bounded between
    –15 and 15 and is based on the amount of forward progress. The `done` variable
    is a Boolean that indicates whether or not the game is over (e.g., whether Mario
    dies). The `info` variable is a Python dictionary with the metadata listed in
    [table 8.2](#ch08table02).
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenAI Gym 接口中，环境作为一个名为 `env` 的类对象实例化，你需要使用的主要方法是它的 `step(...)` 方法。`step`
    方法接受一个表示要采取的动作的整数。与所有 OpenAI Gym 环境一样，在每次动作后，它返回 `state`、`reward`、`done` 和 `info`
    数据。`state` 是一个维度为 (240, 256, 3) 的 numpy 数组，表示一个 RGB 视频帧。`reward` 介于 –15 和 15 之间，基于前进的距离。`done`
    变量是一个布尔值，表示游戏是否结束（例如，马里奥是否死亡）。`info` 变量是一个包含 [表 8.2](#ch08table02) 中列出的元数据的 Python
    字典。
- en: 'Table 8.2\. The metadata returned after each action in the `info` variable
    (source: [https://github.com/Kautenja/gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros))'
  id: totrans-698
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 8.2\. 在 `info` 变量中每次动作后返回的元数据（来源：[https://github.com/Kautenja/gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros))
- en: '| Key | Type | Description |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| Key | Type | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| coins | int | The number of collected coins |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| coins | int | 收集到的金币数量 |'
- en: '| flag_get | bool | True if Mario reached a flag or ax |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| flag_get | bool | 如果马里奥到达了旗帜或斧头，则为 True |'
- en: '| life | int | The number of lives left, i.e., {3, 2, 1} |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| life | int | 剩余生命值，即 {3, 2, 1} |'
- en: '| score | int | The cumulative in-game score |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| score | int | 游戏中的累积得分 |'
- en: '| stage | int | The current stage, i.e., {1, ..., 4} |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| stage | int | 当前阶段，即 {1, ..., 4} |'
- en: '| status | str | Mario''s status, i.e., {''small'', ''tall'', ''fireball''}
    |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| status | str | 马里奥的状态，即 {''small'', ''tall'', ''fireball''} |'
- en: '| time | int | The time left on the clock |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| time | int | 剩余时间 |'
- en: '| world | int | The current world, i.e., {1, ..., 8} |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| world | int | 当前世界，即 {1, ..., 8} |'
- en: '| x_pos | int | Mario''s *x* position in the stage |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| x_pos | int | 马里奥在舞台中的 *x* 位置 |'
- en: We will only need to use the `x_pos` key. In addition to getting the state after
    calling the `step` method, you can also retrieve the state at any point by calling
    `env.render ("rgb_array")`. That’s basically all you need to know about the environment
    in order to train an agent to play it.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要使用 `x_pos` 键。除了获取调用 `step` 方法后的状态外，还可以通过调用 `env.render ("rgb_array")` 在任何时刻检索状态。这基本上就是你需要了解的环境信息，以便训练一个能够玩游戏的智能体。
- en: 8.4\. Preprocessing and the Q-network
  id: totrans-711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4\. 预处理和 Q 网络
- en: The raw state is an RGB video frame with dimensions (240, 256, 3), which is
    unnecessarily high-dimensional and would be computationally costly for no advantage.
    We will convert these RGB states into grayscale and resize them to 42 × 42 to
    allow our model to train much faster.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 原始状态是一个维度为 (240, 256, 3) 的 RGB 视频帧，这是不必要的维数高，并且对于没有优势的情况来说计算成本很高。我们将这些 RGB 状态转换为灰度，并调整大小为
    42 × 42，以便我们的模型能够更快地训练。
- en: Listing 8.2\. Downsample state and convert to grayscale
  id: totrans-713
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 下采样状态并转换为灰度
- en: '[PRE41]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '***1*** The scikit-image library has an image-resizing function built in.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** scikit-image 库内置了一个图像缩放功能。'
- en: '***2*** To convert to grayscale, we simply take the maximum values across the
    channel dimension for good contrast.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 要转换为灰度，我们只需取通道维度的最大值以获得良好的对比度。'
- en: The `downscale_obs` function accepts the state array (`obs`), a tuple indicating
    the new size in height and width, and a Boolean for whether to convert to grayscale
    or not. We set it to `True` by default since that is what we want. We use the
    scikit-image library’s `resize` function, so you may need to install it if you
    don’t have it already (go to the download page at [https://scikit-image.org/](https://scikit-image.org/)).
    It’s a very useful library for working with image data in the form of multidimensional
    arrays.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '`downscale_obs`函数接受状态数组（`obs`），一个表示新高度和宽度的元组，以及一个布尔值，表示是否转换为灰度。我们默认将其设置为`True`，因为这正是我们想要的。我们使用scikit-image库的`resize`函数，所以如果您还没有安装它，可能需要安装（请访问[https://scikit-image.org/](https://scikit-image.org/)的下载页面）。这是一个非常有用的库，用于处理以多维数组形式存在的图像数据。'
- en: 'You can use matplotlib to visualize a frame of the state:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用matplotlib可视化状态的一帧：
- en: '[PRE42]'
  id: totrans-719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The downsampled image will look pretty blurry, but it still contains enough
    visual information to play the game.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样后的图像看起来会很模糊，但它仍然包含足够的信息来玩游戏。
- en: We need to build a few other data processing functions to transform these raw
    states into a useful form. We will not just pass a single 42 × 42 frame to our
    models; we will instead pass the last three frames of the game (in essence, adding
    a channel dimension) so the states will be a 3 × 42 × 42 tensor ([figure 8.11](#ch08fig11)).
    Using the last three frames gives our model access to velocity information (i.e.,
    how fast and in which direction objects are moving) rather than just positional
    information.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要构建一些其他数据处理函数，将这些原始状态转换为有用的形式。我们不仅将单个42 × 42帧传递给我们的模型；我们还将游戏的最后三个帧（本质上是在添加一个通道维度）传递，因此状态将是一个3
    × 42 × 42张量（[图8.11](#ch08fig11)）。使用最后三个帧使我们的模型能够访问速度信息（即物体移动的速度和方向），而不仅仅是位置信息。
- en: Figure 8.11\. Each state given to the agent is a concatenation of the three
    most recent (grayscale) frames in the game. This is necessary so that the model
    can have access to not just the position of objects, but also their direction
    of movement.
  id: totrans-722
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.11。代理接收到的每个状态都是游戏中最近三个（灰度）帧的拼接。这是必要的，以便模型不仅可以访问物体的位置，还可以访问它们的运动方向。
- en: '![](08fig11_alt.jpg)'
  id: totrans-723
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11的替代图](08fig11_alt.jpg)'
- en: When the game first starts, we only have access to the first frame, so we prepare
    the initial state by concatenating the same state three times to get the 3 × 42
    × 42 initial state. After this initial state, we can replace the last frame in
    the state with the most recent frame from the environment, replace the second
    frame with the old last one, and replace the first frame with the old second.
    Basically, we have a fixed length first-in-first-out data structure where we append
    to the right, and the left automatically pops off. Python has a built-in data
    structure called `deque` in the `collections` library that can implement this
    behavior when the `maxlen` attribute is set to `3`.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 当游戏一开始，我们只能访问第一帧，因此通过将相同状态拼接三次来准备初始状态，以获得3 × 42 × 42的初始状态。在此初始状态之后，我们可以用环境中最新的帧替换状态中的最后一帧，用旧的最后一帧替换第二帧，用旧的第二帧替换第一帧。基本上，我们有一个固定长度的先进先出数据结构，向右追加，而左侧自动弹出。Python在`collections`库中有一个内置的数据结构`deque`，当`maxlen`属性设置为`3`时，它可以实现这种行为。
- en: We will use three functions to prepare the raw states in a form that our agent
    and encoder models will use. The `prepare_state` function resizes the image, converts
    to grayscale, converts from numpy to PyTorch tensor, and adds a batch dimension
    using the `.unsqueeze(dim=)` method. The `prepare_multi_state` function takes
    a tensor of dimensions `Batch x Channel x Height x Width` and updates the channel
    dimension with new frames. This function will only be used during the testing
    of the trained model; during training we will use a `deque` data structure to
    continuously append and pop frames. Lastly the `prepare_initial_state` function
    prepares the state when we first start the game and don’t have a history of two
    prior frames. This function will copy the one frame three times to create a `Batch
    x 3 x Height x Width` tensor.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三个函数来准备原始状态，使其符合我们的代理和编码器模型所需的形式。`prepare_state`函数调整图像大小，转换为灰度，从numpy转换为PyTorch张量，并使用`.unsqueeze(dim=)`方法添加批量维度。`prepare_multi_state`函数接收维度为`Batch
    x Channel x Height x Width`的张量，并更新通道维度以包含新帧。此函数仅在训练模型的测试期间使用；在训练过程中，我们将使用`deque`数据结构来持续追加和弹出帧。最后，`prepare_initial_state`函数在游戏开始时准备状态，此时我们还没有两个先前帧的历史记录。此函数将一个帧复制三次以创建一个`Batch
    x 3 x Height x Width`张量。
- en: Listing 8.3\. Preparing the states
  id: totrans-726
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3. 准备状态
- en: '[PRE43]'
  id: totrans-727
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1*** Downscales state and converts to grayscale, converts to a PyTorch tensor,
    and finally adds a batch dimension'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将状态降维并转换为灰度，转换为PyTorch张量，并最终添加一个批处理维度'
- en: '***2*** Given an existing 3-frame state1 and a new single frame 2, adds the
    latest frame to the queue'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 给定现有的3帧状态1和一个新的单个帧2，将最新帧添加到队列中'
- en: '***3*** Creates a state with three copies of the same frame and adds a batch
    dimension'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建一个包含三个相同帧的状态并添加一个批处理维度'
- en: 8.5\. Setting up the Q-network and policy function
  id: totrans-731
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5. 设置Q网络和策略函数
- en: As we mentioned, we will use a deep Q-network (DQN) for the agent. Recall that
    a DQN takes a state and produces action values, i.e., predictions for the expected
    rewards for taking each possible action. We use these action values to determine
    a policy for action selection. For this particular game there are 12 discrete
    actions, so the output layer of our DQN will produce a vector of length 12 where
    the first element is the predicted value of taking action 0, and so on.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，我们将使用深度Q网络（DQN）作为智能体。回想一下，DQN接受一个状态并产生动作值，即对采取每个可能动作的预期奖励的预测。我们使用这些动作值来确定动作选择策略。对于这个特定的游戏，有12个离散动作，所以我们的DQN输出层将产生一个长度为12的向量，其中第一个元素是采取动作0的预测值，以此类推。
- en: Remember that action values are (in general) unbounded in either direction;
    they can be positive or negative if the rewards can be positive or negative (which
    they can be in this game), so we do not apply any activation function on the last
    layer. The input to the DQN is a tensor of shape `Batch x 3 x 42 x 42`, where,
    remember, the channel dimension (`3`) is for the most recent three frames of game
    play.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，动作值（在一般情况下）在任一方向上都是无界的；如果奖励可以是正的或负的（在这个游戏中确实如此），它们可以是正的或负的，所以我们不在最后一层应用任何激活函数。DQN的输入是一个形状为`Batch
    x 3 x 42 x 42`的张量，记住，通道维度（`3`）是最近三个游戏帧。
- en: For the DQN, we use an architecture consisting of four convolutional layers
    and two linear layers. The *exponential linear unit* (ELU) activation function
    is used after each convolutional layer and the first linear layer (but there’s
    no activation function after the last linear layer). The architecture is diagrammed
    in [figure 8.12](#ch08fig12). As an exercise you can add a *long short-term memory*
    (LSTM) or *gated recurrent unit* (GRU) layer that can allow the agent to learn
    from long-term temporal patterns.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DQN，我们使用由四个卷积层和两个线性层组成的架构。在每个卷积层和第一个线性层之后使用指数线性单元（ELU）激活函数，但在最后一个线性层之后没有激活函数。架构在[图8.12](#ch08fig12)中进行了说明。作为练习，你可以添加一个*长短期记忆*（LSTM）或*门控循环单元*（GRU）层，这可以使智能体能够从长期时间模式中学习。
- en: Figure 8.12\. The DQN architecture we will use. The state tensor is the input,
    and it is passed through four convolutional layers and then two linear layers.
    The ELU activation function is applied after the first five layers but not the
    output layer because the output needs to be able to produce arbitrarily scaled
    Q values.
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.12. 我们将使用的DQN架构。状态张量是输入，它通过四个卷积层，然后是两个线性层。在第一层到第五层之后应用了ELU激活函数，但在输出层没有应用，因为输出需要能够产生任意缩放的Q值。
- en: '![](08fig12_alt.jpg)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig12_alt.jpg)'
- en: Our DQN will learn to predict the expected rewards for each possible action
    given the state (i.e., action values or Q values), and we use these action values
    to decide which action to take. Naively we should just take the action associated
    with the highest value, but our DQN will not produce accurate action values in
    the beginning, so we need to have a policy that allows for some exploration so
    the DQN can learn better action-value estimates.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DQN将学习预测给定状态（即动作值或Q值）的每个可能动作的预期奖励，我们使用这些动作值来决定采取哪个动作。直观地，我们应该只采取与最高值相关的动作，但我们的DQN在开始时不会产生准确的动作值，因此我们需要有一个允许某些探索的策略，以便DQN可以学习更好的动作值估计。
- en: Earlier we discussed using the epsilon-greedy policy, where we take a random
    action with probability ε and take the action with the highest value with probability
    (1 – ε). We usually set ε to be some reasonably small probability like 0.1, and
    often we’ll slowly decrease ε during training so that it becomes more and more
    likely to choose the highest value action.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了使用ε-贪婪策略，其中我们以概率ε随机采取一个动作，以概率（1 – ε）采取值最高的动作。我们通常将ε设置为一个合理的较小概率，如0.1，并且我们通常会在训练过程中逐渐减小ε，使其越来越有可能选择值最高的动作。
- en: We also discussed sampling from a softmax function as our policy. The softmax
    function essentially takes a vector input with arbitrary real numbers and outputs
    a same-sized vector where each element is a probability, so all elements sum to
    1\. It therefore creates a discrete probability distribution. If the input vector
    is a set of action values, the softmax function will return a discrete probability
    distribution over the actions based on their action values, such that the action
    with the highest action value will be assigned the highest probability. If we
    sample from this distribution, the actions with the highest values will be chosen
    more often, but other actions will also be chosen. The problem with this approach
    is that if the best action (according to the action values) is only slightly better
    than other options, the worse actions will still be chosen with a fairly high
    frequency. For example, in the following example we take an action-value tensor
    for five actions and apply the softmax function from PyTorch’s functional module.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了将 softmax 函数作为我们的策略进行采样。softmax 函数本质上接受一个包含任意实数的向量输入，并输出一个相同大小的向量，其中每个元素都是一个概率，因此所有元素的总和为
    1。因此，它创建了一个离散概率分布。如果输入向量是一组动作值，softmax 函数将根据它们的动作值返回一个动作的离散概率分布，使得具有最高动作值的动作将被分配最高的概率。如果我们从这个分布中进行采样，具有最高值的动作将被更频繁地选择，但其他动作也会被选择。这种方法的缺点是，如果最佳动作（根据动作值）仅略优于其他选项，则较差的动作仍然会以相当高的频率被选择。例如，在以下示例中，我们取五个动作的动作值张量，并应用
    PyTorch 功能模块中的 softmax 函数。
- en: '[PRE44]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see, the best action (index 1) is only slightly better than the others,
    so all the actions have pretty high probability, and this policy is not that much
    different from a uniformly random policy. We will use a policy that begins with
    a softmax policy to encourage exploration, and after a fixed number of game steps
    we will switch to an epsilon-greedy strategy, which will continue to give us some
    exploration capacity but mostly just takes the best action.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，最佳动作（索引 1）仅略优于其他动作，因此所有动作都有相当高的概率，并且这种策略与均匀随机策略没有太大区别。我们将使用一个以 softmax
    策略开始的策略来鼓励探索，并在固定数量的游戏步骤后切换到 epsilon-greedy 策略，这将继续给我们一些探索能力，但主要只是采取最佳动作。
- en: Listing 8.4\. The policy function
  id: totrans-742
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.4\. 策略函数
- en: '[PRE45]'
  id: totrans-743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '***1*** The policy function takes a vector of action values and an epsilon
    (eps) parameter.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 策略函数接受一个动作值向量和 epsilon（eps）参数。'
- en: '***2*** If eps is not provided, uses a softmax policy. We sample from the softmax
    using the multinomial function.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 如果未提供 eps，则使用 softmax 策略。我们使用多项式函数从 softmax 中进行采样。'
- en: The other big component we need for the DQN is an *experience replay memory*.
    Gradient-based optimization does not work well if you only pass one sample of
    data at a time because the gradients are too noisy. In order to average over the
    noisy gradients, we need to take sufficiently large samples (called batches or
    mini-batches) and average or sum the gradients over all the samples. Since we
    only see one sample of data at a time when playing a game, we instead store the
    experiences in a “memory” store and then sample mini-batches from the memory for
    training.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DQN，我们还需要另一个重要组件，即 *经验回放内存*。基于梯度的优化在每次只传递一个数据样本时效果不佳，因为梯度太嘈杂。为了平均这些嘈杂的梯度，我们需要取足够大的样本（称为批或小批）并平均或对所有样本的梯度进行求和。由于我们在玩游戏时每次只能看到一个数据样本，因此我们将在“内存”存储中存储经验，然后从内存中采样小批进行训练。
- en: We will build an experience replay class that contains a list to store tuples
    of experiences, where each tuple is of the form (*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1]).
    The class will also have methods to add a memory and sample a mini-batch.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个经验回放类，该类包含一个用于存储经验元组的列表，其中每个元组的形式为 (*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1])。该类还将具有添加内存和采样小批的方法。
- en: Listing 8.5\. Experience replay
  id: totrans-748
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.5\. 经验回放
- en: '[PRE46]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1*** N is the maximum size of the memory list.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** N 是内存列表的最大大小。'
- en: '***2*** batch_size is the number of samples to generate from the memory with
    the get_batch(...) method.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** batch_size 是使用 get_batch(...) 方法从内存中生成的样本数量。'
- en: '***3*** Every 500 iterations of adding a memory, shuffles the memory list to
    promote a more random sample'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 每添加 500 次内存后，将内存列表进行洗牌，以促进更随机的采样'
- en: '***4*** If the memory is not full, adds to the list; otherwise replaces a random
    memory with the new one'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 如果内存未满，则添加到列表中；否则用新的随机内存替换'
- en: '***5*** Uses Python’s built-in shuffle function to shuffle the memory list'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用Python的内置shuffle函数对记忆列表进行洗牌'
- en: '***6*** Randomly samples a mini-batch from the memory list'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 从记忆列表中随机抽取一个迷你批次'
- en: '***7*** Creates an array of random integers representing indices'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 创建一个表示索引的随机整数数组'
- en: The experience replay class essentially wraps a list with extra functionality.
    We want to be able to add tuples to the list, but only up to a maximum number,
    and we want to be able to sample from the list. When we sample with the `get_batch(...)`
    method, we create an array of random integers representing indices in the memory
    list. We index into the memory list with these indices, retrieving a random sample
    of memories. Since each sample is a tuple, (*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1]),
    we want to separate out the different components and stack them together into
    a *S[t]* tensor, *a[t]* tensor, and so on, where the first dimension of the array
    is the batch size. For example, the *S[t]* tensor we want to return should be
    of dimension batch_size × 3 (channels) × 42 (height) × 42 (width). PyTorch’s `stack(...)`
    function will concatenate a list of individual tensors into a single tensor. We
    also make use of the `squeeze(...)` and `unsqueeze(...)` methods to remove and
    add dimensions of size `1`.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放类本质上是一个具有额外功能的列表包装器。我们希望能够将元组添加到列表中，但数量有限，并且我们希望能够从列表中采样。当我们使用`get_batch(...)`方法采样时，我们创建一个表示记忆列表中索引的随机整数数组。我们使用这些索引索引记忆列表，检索一个随机的记忆样本。由于每个样本都是一个元组，(*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1])，我们希望分离出不同的组件并将它们堆叠在一起形成一个
    *S[t]* 张量，*a[t]* 张量，等等，其中数组的第一个维度是批大小。例如，我们希望返回的 *S[t]* 张量应该是维度为 batch_size ×
    3（通道）× 42（高度）× 42（宽度）。PyTorch的`stack(...)`函数将单个张量列表连接成一个单一的张量。我们还使用了`squeeze(...)`和`unsqueeze(...)`方法来移除和添加尺寸为`1`的维度。
- en: With all of that set up, we have just about everything we need to train a vanilla
    DQN besides the training loop itself. In the next section we will implement the
    intrinsic curiosity module.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些设置完成后，除了训练循环本身之外，我们几乎拥有了训练一个原始DQN（vanilla DQN）所需的一切。在下一节中，我们将实现内在好奇心模块（intrinsic
    curiosity module）。
- en: 8.6\. Intrinsic curiosity module
  id: totrans-759
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6\. 内在好奇心模块
- en: 'As we described earlier, the intrinsic curiosity module (ICM) is composed of
    three independent neural network models: the forward model, inverse model, and
    the encoder ([figure 8.13](#ch08fig13)). The forward model is trained to predict
    the next (encoded) state, given the current (encoded) state and an action. The
    inverse model is trained to predict the action that was taken, given two successive
    (encoded) states, *φ*(*S[t]*) and *φ*(*S[t]*[+1]). The encoder simply transforms
    a raw three-channel state into a single low-dimensional vector. The inverse model
    acts indirectly to train the encoder to encode states in a way that only preserves
    information relevant to predicting the action.'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所描述的，内在好奇心模块（ICM）由三个独立的神经网络模型组成：前向模型（forward model）、逆向模型（inverse model）和编码器（encoder）（[图8.13](#ch08fig13)）。前向模型被训练来预测给定当前（编码）状态和动作的下一个（编码）状态。逆向模型被训练来预测在两个连续（编码）状态，*φ*(*S[t]*)
    和 *φ*(*S[t]*[+1]) 下所采取的动作。编码器简单地将原始的三通道状态转换成一个单维低维向量。逆向模型间接地训练编码器以编码状态，只保留与预测动作相关的信息。
- en: Figure 8.13\. A high-level overview of the intrinsic curiosity module (ICM).
    The ICM has three components that are each separate neural networks. The encoder
    model encodes states into a low-dimensional vector, and it is trained indirectly
    through the inverse model, which tries to predict the action that was taken given
    two consecutive states. The forward model predicts the next encoded state, and
    its error is the prediction error that is used as the intrinsic reward.
  id: totrans-761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13\. 内在好奇心模块（ICM）的高级概述。ICM有三个组成部分，每个都是独立的神经网络。编码器模型将状态编码成低维向量，并通过逆向模型间接训练，逆向模型试图根据两个连续状态预测所采取的动作。前向模型预测下一个编码状态，其误差用作内在奖励。
- en: '![](08fig13_alt.jpg)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig13_alt.jpg)'
- en: 'The input and output types of each component of the ICM are shown in [figure
    8.14](#ch08fig14). The forward model is a simple two-layer neural network with
    linear layers. The input to the forward model is constructed by concatenating
    the state *φ*(*S[t]*) with the action *a[t]*. The encoded state *φ*(*S[t]*) is
    a tensor *B* × 288 and the action *a[t]* : *B* × 1 is a batch of integers indicating
    the action index, so we make a one-hot encoded vector by creating a vector of
    size 12 and setting the respective *a[t]* index to 1\. Then we concatenate these
    two tensors to create a batch × 288 + 12 = batch × 300 dimensional tensor. We
    use the rectified linear unit (ReLU) activation unit after the first layer, but
    we do not use an activation function after the output layer. The output layer
    produces a *B* × 288 tensor.'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 'ICM每个组件的输入和输出类型在[图8.14](#ch08fig14)中展示。前向模型是一个简单的两层神经网络，具有线性层。前向模型的输入是通过将状态*φ*(*S[t]*)与动作*a[t]*连接而成的。编码后的状态*φ*(*S[t]*)是一个维度为*B*
    × 288的张量，动作*a[t]* : *B* × 1是一个表示动作索引的整数批，因此我们创建一个大小为12的向量，并将相应的*a[t]*索引设置为1。然后我们将这两个张量连接起来，创建一个维度为批
    × 288 + 12 = 批 × 300的张量。我们在第一层后使用修正线性单元（ReLU）激活单元，但在输出层后不使用激活函数。输出层产生一个*B* × 288的张量。'
- en: Figure 8.14\. This figure shows the type and dimensionality of the inputs and
    outputs of each component of the ICM.
  id: totrans-764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.14。此图显示了ICM每个组件的输入和输出的类型和维度。
- en: '![](08fig14.jpg)'
  id: totrans-765
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig14.jpg)'
- en: The inverse model is also a simple two-layer neural network with linear layers.
    The input is two encoded states, *S[t]* and *S[t]*[+1], concatenated to form a
    tensor of dimension batch × 288 + 288 = batch × 576\. We use a ReLU activation
    function after the first layer. The output layer produces a tensor of dimension
    batch × 12 with a softmax function applied, resulting in a discrete probability
    distribution over actions. When we train the inverse model, we compute the error
    between this discrete distribution over actions and a one-hot encoded vector of
    the true action taken.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 逆模型也是一个简单的两层神经网络，具有线性层。输入是两个编码后的状态，*S[t]*和*S[t]*[+1]，连接形成一个维度为批 × 288 + 288
    = 批 × 576的张量。我们在第一层后使用ReLU激活函数。输出层通过应用softmax函数产生一个维度为批 × 12的张量，从而得到一个关于动作的离散概率分布。当我们训练逆模型时，我们计算这个关于动作的离散分布与真实动作的one-hot编码向量之间的误差。
- en: The encoder is a neural network composed of four convolutional layers (with
    an identical architecture to the DQN), with an ELU activation function after each
    layer. The final output is then flattened to get a flat 288-dimensional vector
    output.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是一个由四个卷积层（与DQN具有相同的架构）组成的神经网络，每个层后都跟一个ELU激活函数。最终输出被展平，得到一个288维度的向量输出。
- en: The whole point of the ICM is to produce a single quantity, the forward-model
    prediction error ([figure 8.15](#ch08fig15)). We literally take the error produced
    by the loss function and use that as the intrinsic reward signal for our DQN.
    We can add this intrinsic reward to the extrinsic reward to get the final reward
    signal, *r[t]* = *r[i]* + *r[e]*. We can scale the intrinsic or extrinsic rewards
    to control the proportions of the total reward.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: ICM（集成控制模型）的整个目的就是产生一个单一的量，即前向模型预测误差（[图8.15](#ch08fig15)）。我们实际上是将损失函数产生的误差用作我们DQN的内生奖励信号。我们可以将这个内生奖励添加到外在奖励中，以获得最终的奖励信号，*r[t]*
    = *r[i]* + *r[e]*。我们可以调整内生或外在奖励的规模，以控制总奖励的比例。
- en: Figure 8.15\. The DQN and the ICM contribute to a single overall loss function
    that is given to the optimizer to minimize with respect to the DQN and ICM parameters.
    The DQN’s Q-value predictions are compared to the observed rewards. The observed
    rewards, however, are summed together with the ICM’s prediction error to get a
    new reward value.
  id: totrans-769
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.15。DQN和ICM共同贡献于一个单一的总体损失函数，该函数被提供给优化器，以最小化DQN和ICM参数。DQN的Q值预测与观察到的奖励进行比较。然而，观察到的奖励与ICM的预测误差相加，以得到新的奖励值。
- en: '![](08fig15_alt.jpg)'
  id: totrans-770
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig15_alt.jpg)'
- en: '[Figure 8.16](#ch08fig16) shows the ICM in more detail, including the agent
    model (DQN). Let’s look at the code for the components of the ICM.'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.16](#ch08fig16) 更详细地展示了ICM，包括代理模型（DQN）。让我们看看ICM组件的代码。'
- en: Figure 8.16\. A complete view of the overall algorithm, including the ICM. First
    we generate B samples from the experience replay memory and use these for the
    ICM and DQN. We run the ICM forward to generate a prediction error, which is then
    provided to the DQN’s error function. The DQN learns to predict action values
    that reflect not only extrinsic (environment provided) rewards but also an intrinsic
    (prediction error-based) reward.
  id: totrans-772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.16\. 包含 ICM 的整体算法的完整视图。我们首先从经验回放内存中生成 B 个样本，并使用这些样本进行 ICM 和 DQN。我们运行 ICM
    前向生成预测误差，然后将其提供给 DQN 的误差函数。DQN 学习预测反映不仅外部（环境提供）奖励，而且还反映内在（基于预测误差）奖励的动作值。
- en: '![](08fig16_alt.jpg)'
  id: totrans-773
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig16_alt.jpg)'
- en: Listing 8.6\. ICM components
  id: totrans-774
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.6\. ICM 组件
- en: '[PRE47]'
  id: totrans-775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '***1*** Phi is the encoder network.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** Phi 是编码器网络。'
- en: '***2*** Gnet is the inverse model.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** Gnet 是逆模型。'
- en: '***3*** Fnet is the forward model.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** Fnet 是前向模型。'
- en: '***4*** The actions are stored as integers in the replay memory, so we convert
    to a one-hot encoded vector.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 动作以整数形式存储在回放内存中，因此我们将其转换为独热编码向量。'
- en: None of these components have complicated architectures. They’re fairly mundane,
    but together they form a powerful system. Now we need to include our DQN model,
    which is a simple set of a few convolutional layers.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件中没有任何一个具有复杂的架构。它们相当普通，但组合在一起就形成了一个强大的系统。现在我们需要包含我们的 DQN 模型，它是一组简单的卷积层。
- en: Listing 8.7\. Deep Q-network
  id: totrans-781
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.7\. 深度 Q 网络
- en: '[PRE48]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '***1*** The output is of shape N x 12.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 输出的形状为 N x 12。'
- en: We’ve covered the ICM components; now let’s put them together. We’re going to
    define a function that accepts (*S[t]*,*a[t]*,*S[t]*[+1]) and returns the forward-model
    prediction error and the inverse-model error. The forward-model error will be
    used not only to backpropagate and train the forward model but also as the intrinsic
    reward for the DQN. The inverse-model error is only used to backpropagate and
    train the inverse and encoder models. First we’ll look at the hyperparameter setup
    and the instantiation of the models.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 ICM 组件；现在让我们将它们组合起来。我们将定义一个函数，该函数接受 (*S[t]*,*a[t]*,*S[t]*[+1]) 并返回前向模型预测误差和逆模型误差。前向模型误差不仅用于反向传播和训练前向模型，还作为
    DQN 的内在奖励。逆模型误差仅用于反向传播和训练逆模型和编码器模型。首先，我们将查看超参数设置和模型的实例化。
- en: Listing 8.8\. Hyperparameters and model instantiation
  id: totrans-785
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.8\. 超参数和模型实例化
- en: '[PRE49]'
  id: totrans-786
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***1*** We can add the parameters from each model into a single list and pass
    that into a single optimizer.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们可以将每个模型的参数添加到一个单独的列表中，并将其传递给单个优化器。'
- en: Some of the parameters in the `params` dictionary will look familiar, such as
    `batch _size`, but the others probably don’t. We’ll go over them, but first let’s
    take a look at the overall loss function.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '`params` 字典中的一些参数看起来可能很熟悉，例如 `batch _size`，但其他参数可能不熟悉。我们将介绍它们，但首先让我们看一下整体损失函数。'
- en: 'Here’s the formula for the overall loss for all four models (including the
    DQN):'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有四个模型（包括 DQN）的整体损失公式：
- en: '| *minimize*[λ × *Q[loss]* + (1 – β)*F[loss]* + β × *G[loss]*] |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '| *minimize*[λ × *Q[loss]* + (1 – β)*F[loss]* + β × *G[loss]*] |'
- en: This formula adds the DQN loss to the forward and inverse model losses, each
    scaled by a coefficient. The DQN loss has a free-scaling parameter, λ, whereas
    the forward and inverse model losses share a scaling parameter, β, so that they’re
    inversely related. This is the only loss function we backpropagate through, so
    at each training step we backpropagate through all four models starting from this
    single loss function.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 此公式将 DQN 损失添加到前向和逆模型损失中，每个损失都按系数缩放。DQN 损失有一个自由缩放参数 λ，而前向和逆模型损失共享一个缩放参数 β，这样它们就成反比。这是我们唯一反向传播的损失函数，因此在每个训练步骤中，我们从单个损失函数开始反向传播所有四个模型。
- en: The `max_episode_len` and `min_progress` parameters are used to set the minimum
    amount of forward progress Mario must make or we’ll reset the environment. Sometimes
    Mario will get stuck behind an obstacle and will just keep taking the same action
    forever, so if Mario doesn’t move forward enough in a reasonable amount of time,
    we just assume he’s stuck.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_episode_len` 和 `min_progress` 参数用于设置马里奥必须做出的最小前进量，否则我们将重置环境。有时马里奥会卡在障碍物后面，并且会一直执行相同的动作，所以如果马里奥在合理的时间内没有足够的前进，我们就假设他卡住了。'
- en: During training, if the policy function says to take action 3 (for example),
    we will repeat that action six times (set according to the `action_repeats` parameter)
    instead of just once. This helps the DQN learn the value of actions more quickly.
    During testing (i.e., inference), we only take the action once. The gamma parameter
    is the same gamma parameter from the DQN chapter. When training the DQN, the target
    value is not just the current reward *r[t]* but the highest predicted action value
    for the next state, so the full target is *r[t]* + γ × max(*Q*(*S[t]*[+1])). Lastly,
    the `frames_per_state` parameter is set to `3` since each state is the last three
    frames of the game play.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，如果策略函数指示采取行动3（例如），我们将重复该动作六次（根据`action_repeats`参数设置），而不是仅仅一次。这有助于DQN更快地学习动作的价值。在测试期间（即推理），我们只采取一次动作。gamma参数与DQN章节中的相同gamma参数。在训练DQN时，目标值不仅仅是当前奖励*r[t]*，而是下一个状态的最高预测动作值，因此完整的目标是*r[t]*
    + γ × max(*Q*(*S[t]*[+1])). 最后，`frames_per_state`参数设置为`3`，因为每个状态是游戏播放的最后三个帧。
- en: Listing 8.9\. The loss function and reset environment
  id: totrans-794
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9。损失函数和重置环境
- en: '[PRE50]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Finally, we get to the actual ICM function.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达实际的ICM函数。
- en: Listing 8.10\. The ICM prediction error calculation
  id: totrans-797
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10。ICM预测误差计算
- en: '[PRE51]'
  id: totrans-798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '***1*** Encodes state1 and state2 using the encoder model'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用编码器模型对state1和state2进行编码'
- en: '***2*** Runs the forward model using the encoded states, but we detach them
    from the graph'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用编码的状态运行前向模型，但我们将其从图中分离'
- en: '***3*** The inverse model returns a softmax probability distribution over actions.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 逆模型返回一个关于动作的softmax概率分布。'
- en: It must be repeated how important it is to properly detach nodes from the graph
    when running the ICM. Recall that PyTorch (and pretty much all other machine learning
    libraries) builds a computational graph where nodes are *operations* (computations),
    and *connections* (also called edges) between nodes are the tensors that flow
    in and out of individual operations. By calling the `.detach()` method, we disconnect
    the tensor from the computational graph and treat it just like raw data; this
    prevents PyTorch from backpropagating through that edge. If we don’t detach the
    `state1_hat` and `state2_hat` tensors when we run the forward model and its loss,
    the forward model will backpropagate into the encoder and will corrupt the encoder
    model.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 重复强调在运行ICM时正确分离图节点的重要性。回想一下，PyTorch（以及几乎所有其他机器学习库）构建了一个计算图，其中节点是*操作*（计算），而节点之间的*连接*（也称为边）是流向和流出单个操作的张量。通过调用`.detach()`方法，我们断开张量与计算图之间的连接，并像处理原始数据一样对待它；这防止PyTorch通过该边进行反向传播。如果我们不在运行前向模型及其损失时分离`state1_hat`和`state2_hat`张量，前向模型将反向传播到编码器，并损坏编码器模型。
- en: We’ve now approached the main training loop. Remember, since we’re using experience
    replay, training only happens when we sample from the replay buffer. We’ll set
    up a function that samples from the replay buffer and computes the individual
    model errors.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经接近主要的训练循环。记住，由于我们使用经验回放，只有在从回放缓冲区采样时才会进行训练。我们将设置一个从回放缓冲区采样并计算单个模型误差的函数。
- en: Listing 8.11\. Mini-batch training using experience replay
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.11。使用经验回放进行的小批量训练
- en: '[PRE52]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '***1*** We reshape these tensors to add a single dimension to be compatible
    with the models.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将这些张量重塑以添加一个维度，使其与模型兼容。'
- en: '***2*** Runs the ICM'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 运行ICM'
- en: '***3*** Scales the forward-prediction error using the eta parameter'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用eta参数缩放前向预测误差'
- en: '***4*** Starts totaling up the reward; makes sure to detach the i_reward tensor'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 开始累计奖励；确保分离i_reward张量'
- en: '***5*** The use_explicit Boolean variable lets us decide whether or not to
    use explicit rewards in addition to the intrinsic reward.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** use_explicit布尔变量让我们决定是否在内在奖励之外使用显式奖励。'
- en: '***6*** Computes the action values for the next state'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 计算下一个状态的动作值'
- en: '***7*** Since the action_batch is a tensor of integers of action indices, we
    convert this to a tensor of one-hot encoded vectors.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 由于action_batch是一个动作索引的整数张量，我们将其转换为one-hot编码向量的张量。'
- en: Now let’s tackle the main training loop, shown in [listing 8.12](#ch08ex12).
    We initialize the first state using the `prepare_initial_state(...)` function
    we defined earlier, which just takes the first frame and repeats it three times
    along the channel dimension. We also set up a `deque` instance, to which we will
    append each frame as we observe them. The `deque` is set to a `maxlen` of `3`,
    so only the most recent three frames are stored. We convert the `deque` first
    to a list and then to a PyTorch tensor of dimensions 1 × 3 × 42 × 42 before passing
    it to the Q-network.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们处理主要的训练循环，如[列表8.12](#ch08ex12)所示。我们使用之前定义的`prepare_initial_state(...)`函数初始化第一个状态，该函数只是将第一帧重复三次沿通道维度。我们还设置了一个`deque`实例，我们将逐帧将其附加到该实例。`deque`设置为`maxlen`为`3`，因此只存储最近的三个帧。我们在将其传递到Q网络之前，将`deque`首先转换为列表，然后转换为维度为1
    × 3 × 42 × 42的PyTorch张量。
- en: Listing 8.12\. The training loop
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.12。训练循环
- en: '[PRE53]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '***1*** We need to keep track of the last x position in order to reset if there’s
    no forward progress.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们需要跟踪最后x位置，以便在没有前进进度的情况下重置。'
- en: '***2*** Runs the DQN forward to get action-value predictions'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 运行DQN以获取动作值预测'
- en: '***3*** After the first 1,000 epochs, switches to the epsilon-greedy policy'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 在前1,000个epoch之后，切换到epsilon-greedy策略'
- en: '***4*** Repeats whatever action the policy says 6 times, to speed up learning'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 重复执行策略指定的任何动作6次，以加快学习速度'
- en: '***5*** Converts the deque object into a tensor'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将deque对象转换为张量'
- en: '***6*** Adds the single experience to the replay buffer'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将单个经验添加到重放缓冲区'
- en: '***7*** If Mario is not making sufficient forward progress, restarts the game
    and tries again'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 如果马里奥没有取得足够的向前进度，重新开始游戏并再次尝试'
- en: '***8*** Gets the errors for one mini-batch of data from the replay buffer'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 从重放缓冲区获取一个数据小批次的错误'
- en: '***9*** Computes the overall loss'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 计算总体损失'
- en: While it’s a bit lengthy, this training loop is pretty simple. All we do is
    prepare a state, input to the DQN, get action values (Q values), input to the
    policy, get an action to take, and then call the `env.step(action)` method to
    perform the action. We then get the next state and some other metadata. We add
    this full experience as a tuple, (*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1]), to the experience
    replay memory. Most of the action is happening in the mini-batch training function
    we already covered.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个训练循环有点长，但它相当简单。我们只是准备一个状态，输入到DQN中，获取动作值（Q值），输入到策略中，获取要采取的动作，然后调用`env.step(action)`方法执行该动作。然后我们获取下一个状态和一些其他元数据。我们将整个经验作为一个元组(*S[t]*,*a[t]*,*r[t]*,*S[t]*[+1])添加到经验重放内存中。大部分动作发生在我们之前已经覆盖的小批量训练函数中。
- en: That is the main code you need to build an end-to-end DQN and ICM to train on
    Super Mario Bros. Let’s test it out by training for 5,000 epochs, which takes
    about 30 minutes or so running on a MacBook Air (with no GPU). We will train with
    `use_ extrinsic=False` in the mini-batch function, so it is learning only from
    the intrinsic reward. You can plot the individual losses for each of the ICM components
    and the DQN with the following code. We will log-transform the loss data to keep
    them on a similar scale.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是构建用于在超级马里奥兄弟上训练的端到端DQN和ICM所需的主要代码。让我们通过训练5,000个epoch来测试它，这大约需要30分钟的时间，在运行没有GPU的MacBook
    Air上。我们在小批量函数中使用`use_extrinsic=False`进行训练，因此它只从内在奖励中学习。你可以使用以下代码绘制每个ICM组件和DQN的单独损失。我们将对损失数据进行对数变换，以保持它们在相似的尺度上。
- en: '[PRE54]'
  id: totrans-827
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: As shown in [figure 8.17](#ch08fig17), the DQN loss initially drops and then
    slowly increases and plateaus. The forward loss seems to slowly decrease but is
    pretty noisy. The inverse model looks sort of flatlined, but if you were to zoom
    in, it does seem to very slowly decrease over time. The loss plots look a lot
    nicer if you set `use_extrinsic=True` and use the extrinsic rewards. But don’t
    feel let down by the loss plots. If we test the trained DQN, you will see that
    it does a lot better than the loss plots suggest. This is because the ICM and
    DQN are behaving like an adversarial dynamic system since the forward model is
    trying to lower its prediction error, but the DQN is trying to maximize the prediction
    error by steering the agent toward unpredictable states of the environment ([figure
    8.18](#ch08fig18)).
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8.17](#ch08fig17)所示，DQN损失最初下降，然后缓慢增加并趋于平稳。前向损失似乎缓慢下降，但相当嘈杂。逆模型看起来有点平坦，但如果你放大查看，它似乎确实随着时间的推移而非常缓慢地下降。如果你设置`use_extrinsic=True`并使用外部奖励，损失图看起来会更好。但不要因为损失图而感到失望。如果我们测试训练好的DQN，你会看到它的表现比损失图所暗示的要好得多。这是因为ICM和DQN像对抗性动态系统一样表现，因为前向模型试图降低其预测误差，而DQN试图通过将智能体引导到环境中的不可预测状态来最大化预测误差（[图8.18](#ch08fig18)）。
- en: Figure 8.17\. These are the losses for the individual components of the ICM
    and the DQN. The losses do not smoothly decrease like we’re used to with a single
    supervised neural network because the DQN and ICM are trained adversarially.
  id: totrans-829
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.17。这是ICM和DQN的各个组件的损失。损失并没有像我们习惯的单个监督神经网络那样平滑地下降，因为DQN和ICM是进行对抗性训练的。
- en: '![](08fig17_alt.jpg)'
  id: totrans-830
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17](08fig17_alt.jpg)'
- en: Figure 8.18\. The DQN agent and forward model are trying to optimize antagonistic
    objectives and hence form an adversarial pair.
  id: totrans-831
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.18。DQN智能体和前向模型试图优化对抗性目标，因此形成了一个对抗性对。
- en: '![](08fig18_alt.jpg)'
  id: totrans-832
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18](08fig18_alt.jpg)'
- en: If you look at the loss plot for a *generative adversarial network* (GAN), the
    generator and discriminator loss look somewhat similar to our DQN and forward
    model loss with `use_extrinsic=False`. The losses do not smoothly decrease like
    you’re used to when you train a single machine learning model.
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看*生成对抗网络*（GAN）的损失图，生成器和判别器的损失看起来与我们的DQN和前向模型损失`use_extrinsic=False`时有些相似。损失并没有像你训练单个机器学习模型时那样平滑地下降。
- en: A better assessment of how well the overall training is going is to track the
    episode length over time. The episode length should be increasing if the agent
    is learning how to progress through the environment more effectively. In our training
    loop, whenever the episode finishes (i.e., when the `done` variable becomes `True`
    because the agent dies or doesn’t make sufficient forward progress), we save the
    current `info['x_pos']` to the `ep_lengths` list. We expect that the maximum episode
    lengths will get longer and longer over training time.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 对整体训练进展的更好评估是跟踪随时间变化的场景长度。如果智能体正在学习如何更有效地通过环境，场景长度应该会增加。在我们的训练循环中，每当场景结束时（即，当`done`变量变为`True`，因为智能体死亡或没有足够的向前进展时），我们将当前的`info['x_pos']`保存到`ep_lengths`列表中。我们预计最大场景长度会随着训练时间的推移而越来越长。
- en: '[PRE55]'
  id: totrans-835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In [figure 8.19](#ch08fig19) we see that early on the biggest spike is getting
    to the 150 mark (i.e., the x position in the game), but over training time the
    farthest distance the agent is able to reach (represented by the height of the
    spikes) steadily increases, although there is some randomness.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8.19](#ch08fig19)中，我们可以看到，一开始最大的峰值达到150（即游戏中的x位置），但随着训练时间的推移，智能体能够达到的最远距离（由峰值的高度表示）稳步增加，尽管有一些随机性。
- en: Figure 8.19\. Training time is on the x axis and episode length is on the y
    axis. We see bigger and bigger spikes over training time, which is what we expect.
  id: totrans-837
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.19。训练时间在x轴上，场景长度在y轴上。我们看到随着训练时间的推移，峰值越来越大，这正是我们所期望的。
- en: '![](08fig19_alt.jpg)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19](08fig19_alt.jpg)'
- en: The episode length plot looks promising, but let’s render a video of our trained
    agent playing Super Mario Bros. If you’re running this on your own computer, the
    OpenAI Gym provides a render function that will open a new window with live game
    play. Unfortunately, this won’t work if you’re using a remote machine or cloud
    virtual machine. In those cases, the easiest alternative is to run a loop of the
    game, saving each observation frame to a list, and once the loop terminates, convert
    it to a numpy array. You can then save this numpy array of video frames as a video
    and play it in a Jupyter Notebook.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 集剧长度图看起来很有希望，但让我们渲染一个我们训练的智能体玩超级马里奥兄弟的视频。如果您在自己的计算机上运行此代码，OpenAI Gym 提供了一个渲染函数，该函数将打开一个新窗口，显示实时游戏。不幸的是，如果您使用远程机器或云虚拟机，这将不起作用。在这种情况下，最简单的替代方案是运行一个游戏循环，将每个观察帧保存到列表中，一旦循环终止，就将其转换为
    numpy 数组。然后您可以将这个视频帧的 numpy 数组保存为视频，并在 Jupyter Notebook 中播放。
- en: '[PRE56]'
  id: totrans-840
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In [listing 8.13](#ch08ex13) we use the built-in OpenAI Gym render method to
    view the game in real-time.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 8.13](#ch08ex13)中，我们使用内置的 OpenAI Gym 渲染方法来实时查看游戏。
- en: Listing 8.13\. Testing the trained agent
  id: totrans-842
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.13\. 测试训练好的智能体
- en: '[PRE57]'
  id: totrans-843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: There’s not much to explain here if you followed the training loop; we’re just
    extracting the part that runs the network forward and takes an action. Notice
    that we still use an epsilon-greedy policy with epsilon set to 0.1\. Even during
    inference, the agent needs a little bit of randomness to keep it from getting
    stuck. One difference to notice is that in test (or inference) mode, we only enact
    the action once and not six times like we did in training. Assuming you get the
    same results as us, your trained agent should make fairly consistent forward progress
    and should be able to jump over obstacles ([figure 8.20](#ch08fig20)). Congratulations!
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遵循了训练循环，这里就没有太多需要解释的；我们只是提取了运行网络前向传播并采取行动的部分。请注意，我们仍然使用 epsilon-greedy 策略，其中
    epsilon 设置为 0.1。即使在推理过程中，智能体也需要一点随机性以避免陷入停滞。一个值得注意的差异是，在测试（或推理）模式下，我们只执行一次动作，而不是像训练中那样执行六次。假设您得到的结果与我们相同，您的训练智能体应该会有相当一致的前进进度，并且能够跳过障碍物（[图
    8.20](#ch08fig20)）。恭喜！
- en: Figure 8.20\. The Mario agent trained only from intrinsic rewards successfully
    jumping over a chasm. This demonstrates it has learned basic skills without any
    explicit rewards to do so. With a random policy, the agent would not even be able
    to move forward, let alone learn to jump over obstacles.
  id: totrans-845
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.20\. 仅从内在奖励训练的马里奥智能体成功跳过了一个深渊。这表明它已经学会了基本的技能，而无需任何明确的奖励来做这件事。使用随机策略，智能体甚至无法向前移动，更不用说学会跳过障碍物了。
- en: '![](08fig20.jpg)'
  id: totrans-846
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig20.jpg)'
- en: If you’re not getting the same results, try changing the hyperparameters, particularly
    the learning rate, mini-batch size, maximum episode length, and minimum forward
    progress. Training for 5,000 epochs with intrinsic rewards works, but in our experience
    it’s sensitive to these hyperparameters. Of course, 5,000 epochs is not very long,
    so training for longer will result in more interesting behavior.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有得到相同的结果，尝试更改超参数，特别是学习率、小批量大小、最大剧集长度和最小前进进度。使用内在奖励进行 5,000 个时期的训练是有效的，但根据我们的经验，它对这些超参数很敏感。当然，5,000
    个时期并不长，所以进行更长时间的训练将导致更有趣的行为。
- en: '|  |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**How will this work in other environments?**'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: '**在其他环境中这将如何工作？**'
- en: We trained our DQN agent with an ICM-based reward on a single environment, Super
    Mario Bros, but the paper “Large-Scale Study of Curiosity-Driven Learning” by
    Yuri Burda et al. (2018) demonstrated how effective intrinsic rewards alone can
    be. They ran a number of experiments using curiosity-based rewards across multiple
    games, finding that a curious agent could progress through 11 levels in Super
    Mario Bros. and could learn to play Pong, among other games. They used essentially
    the same ICM we just built, except they used a more sophisticated actor-critic
    model called *proximal policy optimization* (PPO) rather than DQN.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个环境中使用基于 ICM 的奖励训练了我们的 DQN 智能体，这个环境是超级马里奥兄弟，但 Yuri Burda 等人于 2018 年发表的论文“Curiosity-Driven
    Learning 的大规模研究”展示了仅使用内在奖励本身可以多么有效。他们使用基于好奇心的奖励在多个游戏中进行了一系列实验，发现一个好奇的智能体可以在超级马里奥兄弟中通过
    11 个关卡，并且可以学会玩乒乓球等游戏。他们基本上使用了我们刚刚构建的相同 ICM，但他们使用了一个更复杂的演员-评论家模型，称为**近端策略优化**（PPO），而不是
    DQN。
- en: An experiment you can try is to replace the encoder network with a *random projection*.
    A random projection just means multiplying the input data by a randomly initialized
    matrix (e.g., a randomly initialized neural network that is fixed and not trained).
    The Burda et al. 2018 paper demonstrated that a random projection works almost
    as well as the trained encoder.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试的一个实验是将编码器网络替换为**随机投影**。随机投影仅仅意味着将输入数据乘以一个随机初始化的矩阵（例如，一个固定且未训练的随机初始化神经网络）。Burda等人2018年的论文证明了随机投影的效果几乎与训练过的编码器一样好。
- en: '|  |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 8.7\. Alternative intrinsic reward mechanisms
  id: totrans-853
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7. 替代内在奖励机制
- en: In this chapter we described the serious problem faced by RL agents in environments
    with sparse rewards. We considered the solution to be imbuing agents with a sense
    of curiosity, and we implemented an approach from the Pathak et al. 2017 paper,
    one of the most widely cited papers in reinforcement learning research in recent
    years. We chose to demonstrate this approach not just because it is popular, but
    because it builds on what we’ve learned in previous chapters without introducing
    too many new notions. Curiosity-based learning (which goes by many names) is a
    very active area of research, and there are many alternative approaches, some
    of which we think are better than the ICM.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了RL代理在稀疏奖励环境中面临的严重问题。我们认为解决方案是赋予代理一种好奇心，并实施了Pathak等人2017年论文中的方法，这是近年来强化学习研究中最广泛引用的论文之一。我们选择展示这种方法，不仅因为它很受欢迎，而且因为它建立在前面章节学到的内容之上，而没有引入太多新概念。基于好奇心的学习（有多个名称）是一个非常活跃的研究领域，有许多替代方法，其中一些我们认为比ICM更好。
- en: Many of the other exciting methods use Bayesian inference and information theory
    to come up with novel mechanisms to drive curiosity. The prediction error (PE)
    approach we used in this chapter is just one implementation under a broader PE
    umbrella. The basic idea, as you now know, is that the agent wants to reduce its
    PE (or in other words, its uncertainty about the environment), but it must do
    so by actively seeking out novelty lest it be surprised by something unexpected.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他令人兴奋的方法使用贝叶斯推理和信息论来提出新的机制来驱动好奇心。我们在本章中使用的预测误差（PE）方法只是更广泛的PE伞下的一种实现。正如你现在所知，基本思想是代理想要减少其PE（或者说，其对环境的不确定性），但它必须通过积极寻求新异来做到这一点，以免被意外之事所惊吓。
- en: Another umbrella is that of agent *empowerment.* Rather than seeking to minimize
    prediction error and make the environment more predictable, empowerment strategies
    optimize the agent to maximize its control over the environment ([figure 8.21](#ch08fig21)).
    One paper in this area is “Variational Information Maximisation for Intrinsically
    Motivated Reinforcement Learning” by Shakir Mohamed and Danilo Jimenez Rezende
    (2015). We can make the informal statement about maximizing control over the environment
    into a precise mathematical statement (which we will only approximate here).
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个概念伞是**代理赋能**。与其寻求最小化预测误差并使环境更加可预测，赋能策略优化代理以最大化其对环境的控制（[图8.21](#ch08fig21)）。这一领域的一篇论文是Shakir
    Mohamed和Danilo Jimenez Rezende于2015年发表的“Variational Information Maximisation for
    Intrinsically Motivated Reinforcement Learning”。我们可以将关于最大化对环境控制的非正式陈述转化为一个精确的数学陈述（我们在这里只做近似）。
- en: Figure 8.21\. The two main approaches for solving the sparse reward problem
    with curiosity-like methods are prediction error methods, like the one we used
    in this chapter, and empowerment methods. Rather than trying to maximize the prediction
    error between a given state and the next predicted state, empowerment methods
    aim to maximize the mutual information (MI) between the agent’s actions and the
    next states. If the MI between the agent’s action and the next state is high,
    that means the agent has a high level of control (or power) over the resulting
    next states (i.e., if you know which action the agent took, you can predict the
    next state well). This incentivizes the agent to learn how to maximally control
    the environment.
  id: totrans-857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.21. 使用类似好奇心的方法解决稀疏奖励问题的两种主要方法是预测误差方法，如我们本章所使用的，和赋能方法。赋能方法的目标不是试图最大化给定状态和下一个预测状态之间的预测误差，而是试图最大化代理的动作和下一个状态之间的互信息（MI）。如果代理的动作和下一个状态之间的MI很高，这意味着代理对产生的下一个状态有很高的控制水平（或力量）（即如果你知道代理采取了哪个动作，你可以很好地预测下一个状态）。这激励代理学习如何最大限度地控制环境。
- en: '![](08fig21_alt.jpg)'
  id: totrans-858
  prefs: []
  type: TYPE_IMG
  zh: '![图8.21](08fig21_alt.jpg)'
- en: The premise relies on the quantity called *mutual information* (MI). We will
    not define it mathematically here, but informally, MI measures how much information
    is shared between two sources of data called *random variables* (because usually
    we deal with data that has some amount of randomness or uncertainty). Another
    less tautological definition is that MI measures how much your uncertainty about
    one quantity, *x*, is reduced given another quantity, *y*.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 前提依赖于称为 *互信息*（MI）的量。我们在这里不对其进行数学定义，但非正式地说，MI衡量两个称为 *随机变量* 的数据源之间共享了多少信息（因为我们通常处理具有一些随机性或不确定性的数据）。另一种不那么冗长的定义是，MI衡量在给定另一个量
    *y* 的情况下，你对一个量 *x* 的不确定性减少了多少。
- en: Information theory was first developed with real-world communication problems
    in mind, where one problem is how to best encode messages across a possibly noisy
    communication channel so that the received message is the least corrupted ([figure
    8.22](#ch08fig22)). Suppose we have an original message *x* that we want to send
    across a noisy communication line (e.g., using radio waves), and we want to maximize
    the mutual information between *x* and the received message *y*. We do this by
    developing some way of encoding *x*, which might be a textual document, into a
    pattern of radio waves that minimizes the probability of the data being corrupted
    by noise. Once someone else receives the decoded message, *y*, they can be assured
    that their received message is very close to the original message.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论最初是为了解决现实世界的通信问题而开发的，其中一个问题是如何在可能存在噪声的通信信道上最佳地编码消息，以便接收到的消息是最少被破坏的（[图8.22](#ch08fig22)）。假设我们有一个原始消息
    *x*，我们希望将其通过一个有噪声的通信线路（例如，使用无线电波）发送出去，并且我们希望最大化 *x* 和接收到的消息 *y* 之间的互信息。我们通过开发一种方法来编码
    *x*，这可能是一篇文本文档，将其编码成一种无线电波模式，以最小化数据被噪声破坏的概率。一旦有人接收并解码了消息 *y*，他们可以确信他们接收到的消息非常接近原始消息。
- en: Figure 8.22\. Claude Shannon developed communication theory, which was born
    from the need to encode messages efficiently and robustly across noisy communication
    channels as depicted here. The goal is to encode the message such that the mutual
    information between the received message and the sent message is maximal.
  id: totrans-861
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.22。克劳德·香农开发了通信理论，这一理论源于在噪声通信信道上高效且鲁棒地编码消息的需求，如图所示。目标是编码消息，使得接收到的消息与发送的消息之间的互信息最大化。
- en: '![](08fig22_alt.jpg)'
  id: totrans-862
  prefs: []
  type: TYPE_IMG
  zh: '![08fig22_alt.jpg](08fig22_alt.jpg)'
- en: 'In our example, *x* and *y* were both some sort of written message, but *x*
    and *y* need not be the same type of quantities. For example, we can ask what
    the mutual information is between the one-year stock price history of a company
    and its annual revenue: If we start with a very uncertain estimate about the annual
    revenue of a company, and then we learn the one-year stock price history, how
    much is our uncertainty reduced? If it’s reduced a lot, the MI is high.'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，*x* 和 *y* 都是某种形式的书面消息，但 *x* 和 *y* 不一定是相同类型的量。例如，我们可以询问一家公司一年股价历史和其年度收入之间的互信息是多少：如果我们对一家公司的年度收入有一个非常不确定的估计，然后我们学习了该公司的股价历史一年，我们的不确定性减少了多少？如果减少了很多，那么互信息就很高。
- en: That example involved different quantities, but both used the units of dollars—that
    need not be the case either. We could ask what the MI is between the daily temperature
    and the sales of ice cream shops.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 那个例子涉及不同的量，但两者都使用了美元单位——这也不一定需要。我们可以询问每日温度和冰淇淋店销售额之间的互信息是多少。
- en: In the case of agent empowerment in reinforcement learning, the objective is
    to maximize the mutual information between an action (or sequence of actions)
    and the resulting future state (or states). Maximizing this objective means that
    if you know what action the agent took, you will have a high confidence about
    what the resulting state was. This means the agent has a high degree of control
    over the environment, since it can reliably reach states given its actions. Hence,
    a maximally empowered agent has maximal degrees of freedom.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，代理赋能的目标是最大化动作（或动作序列）与结果未来状态（或状态）之间的互信息。最大化这一目标意味着如果你知道代理执行了什么动作，你将高度确信结果状态是什么。这意味着代理对环境有高度的控制力，因为它可以可靠地通过其动作达到状态。因此，一个完全赋能的代理具有最大的自由度。
- en: This is different than the prediction-error approach because minimizing PE directly
    encourages exploration, whereas maximizing empowerment may induce exploratory
    behavior as a means to learn empowering skills, but only indirectly. Consider
    a young woman, Sarah, who decides to travel the world and explore as much as possible.
    She is reducing her uncertainty about the world. Compare her to Bill Gates, who
    by being extraordinarily rich, has a high degree of power. He may not be interested
    in traveling as much as Sarah, but he can if he wants, and no matter where he
    is at any time, he can go where he wants to go.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 这与预测误差方法不同，因为最小化PE直接鼓励探索，而最大化权能可能通过学习赋予技能作为探索行为的手段，但只是间接的。考虑一个年轻女性，莎拉，她决定环游世界并尽可能多地探索。她正在减少对世界的不确定性。将她与比尔·盖茨进行比较，他通过非凡的财富，拥有很高的权力。他可能不像莎拉那样对旅行感兴趣，但如果有需要，他可以，而且无论何时何地，他都可以去他想去的地方。
- en: Both empowerment and curiosity objectives have their use cases. Empowerment-based
    objectives have been shown to be useful for training agents to acquire complex
    skills without any extrinsic reward (e.g., robotic tasks or sports games), whereas
    curiosity-based objectives tend to be more useful for exploration (e.g., games
    like Super Mario Bros. where the goal is to progress through levels). In any case,
    these two metrics are more similar than they are different.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 权能和好奇心目标都有其用例。基于权能的目标已被证明在训练智能体获取复杂技能而无需任何外在奖励（例如，机器人任务或体育游戏）时很有用，而基于好奇心的目标则更适用于探索（例如，超级马里奥兄弟等游戏，其目标是通过关卡）。无论如何，这两个指标比它们的不同之处更为相似。
- en: Summary
  id: totrans-868
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The sparse reward problem is when an environment rarely produces a useful reward
    signal, which severely challenges the way ordinary DRL attempts to learn.
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏奖励问题是指环境很少产生有用的奖励信号，这严重挑战了普通深度强化学习尝试学习的方式。
- en: The sparse reward problem can be solved by creating synthetic reward signals
    that we call curiosity rewards.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建我们称之为好奇心奖励的合成奖励信号可以解决稀疏奖励问题。
- en: A curiosity module creates synthetic rewards based on how unpredictable the
    next state of the environment is, encouraging the agent to explore more unpredictable
    parts of the environment.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好奇心模块根据环境下一状态的不确定性创建合成奖励，鼓励智能体探索环境中的更多不可预测的部分。
- en: 'The intrinsic curiosity module (ICM) consists of three independent neural networks:
    a forward-prediction model, an inverse model, and an encoder.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内在好奇心模块（ICM）由三个独立的神经网络组成：一个前向预测模型、一个逆模型和一个编码器。
- en: The encoder encodes high-dimensional states into a low-dimensional vector with
    high-level features (which removes noise and trivial features).
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器将高维状态编码成具有高级特征的低维向量（这消除了噪声和琐碎的特征）。
- en: The forward-prediction model predicts the next encoded state, and its error
    provides the curiosity signal.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向预测模型预测下一个编码状态，其误差提供好奇心信号。
- en: The inverse model trains the encoder by taking two successive encoded states
    and predicting the action that was taken.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆模型通过取两个连续编码状态并预测所采取的动作来训练编码器。
- en: Empowerment is a closely related but alternative approach to curiosity-based
    learning. In empowerment, the agent is incentivized to learn how to maximize the
    amount of control it has over the environment.
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权能是与基于好奇心学习密切相关但替代的方法。在权能中，智能体被激励去学习如何最大化其对环境的控制量。
- en: Chapter 9\. Multi-agent reinforcement learning
  id: totrans-877
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章\. 多智能体强化学习
- en: '*This chapter covers*'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Why ordinary Q-learning can fail in the multi-agent setting
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么普通Q学习在多智能体设置中会失败
- en: How to deal with the “curse of dimensionality” with multiple agents
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理多个智能体时的“维度诅咒”
- en: How to implement multi-agent Q-learning models that can perceive other agents
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现能够感知其他智能体的多智能体Q学习模型
- en: How to scale multi-agent Q-learning by using the mean field approximation
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用平均场近似来扩展多智能体Q学习
- en: How to use DQNs to control dozens of agents in a multi-agent physics simulation
    and game
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用DQNs在多智能体物理模拟和游戏中控制数十个智能体
- en: So far, the reinforcement learning algorithms we have covered—Q-learning, policy
    gradients, and actor-critic algorithms—have all been applied to control a single
    agent in an environment. But what about situations where we want to control multiple
    agents that can interact with each other? The simplest example of this would be
    a two-player game where each player is implemented as a reinforcement learning
    agent. But there are other situations in which we might want to model hundreds
    or thousands of individual agents all interacting with each other, such as a traffic
    simulation. In this chapter you will learn how to adapt what you’ve learned so
    far into this multi-agent scenario by implementing an algorithm called *mean field
    Q-learning* (MF-Q), first described in a paper titled “Mean Field Multi-Agent
    Reinforcement Learning” by Yaodong Yang et al. (2018).
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍的强化学习算法——Q学习、策略梯度法和演员-评论家算法——都应用于控制环境中的单个代理。但当我们想要控制可以相互交互的多个代理时怎么办呢？这个最简单的例子就是两人游戏，其中每个玩家都是一个强化学习代理。但还有其他情况，我们可能想要模拟数百或数千个相互作用的个体代理，例如交通模拟。在本章中，你将学习如何通过实现一个名为*平均场Q学习*（MF-Q）的算法来将你迄今为止学到的知识应用到这种多代理场景中，该算法最早在Yaodong
    Yang等人于2018年发表的论文《平均场多代理强化学习》中描述。
- en: 9.1\. From one to many agents
  id: totrans-885
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1. 从单一代理到多个代理
- en: In the case of games, the environment might contain other agents that we do
    not control, often called *non-player characters* (NPCs). For example, in [chapter
    8](kindle_split_018.html#ch08) we trained an agent to play Super Mario Bros.,
    which has many NPCs. These NPCs are controlled by some other unseen game logic,
    but they can and often do interact with the main player. From the perspective
    of our deep Q-network (DQN) agent, these NPCs are nothing more than patterns in
    the state of the environment that change over time. Our DQN is not directly aware
    of the actions of the other players. This is not an issue because these NPCs do
    not learn; they have fixed policies. As you’ll see in this chapter, sometimes
    we want to go beyond mere NPCs and actually model the behavior of many interacting
    agents that learn ([figure 9.1](#ch09fig01)), and this requires a bit of a reformulation
    of the basic reinforcement learning framework you’ve learned about so far in this
    book.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏的情况下，环境可能包含我们无法控制的其它代理，通常被称为*非玩家角色*（NPCs）。例如，在[第8章](kindle_split_018.html#ch08)中，我们训练了一个代理来玩超级马里奥兄弟，其中有许多NPC。这些NPC由一些看不见的游戏逻辑控制，但它们可以并且经常与主要玩家互动。从我们的深度Q网络（DQN）代理的角度来看，这些NPC不过是环境中随时间变化的模式。我们的DQN并不直接意识到其他玩家的行为。这不是问题，因为这些NPC不会学习；它们有固定的策略。正如你将在本章中看到的，有时我们想要超越仅仅NPC，实际上模拟许多相互学习的代理的行为（[图9.1](#ch09fig01)），这需要对你在本书中迄今为止学到的基本强化学习框架进行一些重新表述。
- en: Figure 9.1\. In the multi-agent setting, each agent’s actions not only affect
    the evolution of the environment, but also the policies of other agents, leading
    to highly dynamic agent interactions. The environment will produce a state and
    reward, which each agent 1 through j use to take actions using their own policies.
    However, each agent’s policy will affect all the other agents’ policies.
  id: totrans-887
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1。在多代理设置中，每个代理的行为不仅影响环境的演变，还影响其他代理的策略，导致高度动态的代理交互。环境将产生状态和奖励，每个代理1到j使用它们自己的策略来采取行动。然而，每个代理的策略将影响所有其他代理的策略。
- en: '![](09fig01_alt.jpg)'
  id: totrans-888
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](09fig01_alt.jpg)'
- en: For example, imagine that we directly want to control the actions of many interacting
    agents in some environment using a deep reinforcement learning algorithm. For
    example, there are games with multiple players grouped into teams, and we may
    want to develop an algorithm that can play a bunch of players on a team against
    another team. Or we may want to control the actions of hundreds of simulated cars
    to model traffic patterns. Or maybe we’re economists and we want to model the
    behavior of thousands of agents in a model of an economy. This is a different
    situation than having NPCs because, unlike NPCs, these other agents all learn,
    and their learning is affected by each other.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，我们直接想要使用深度强化学习算法来控制某些环境中许多相互作用的代理的行为。例如，有一些游戏有多名玩家组成团队，我们可能想要开发一个算法，可以玩一个团队对抗另一个团队。或者我们可能想要控制数百个模拟汽车的行为来模拟交通模式。或者也许我们是经济学家，我们想要在一个经济模型中模拟数千个代理的行为。这与NPC的情况不同，因为，与NPC不同，这些其他代理都在学习，并且他们的学习相互影响。
- en: The most straightforward way to extend what we know already into a multi-agent
    setting is to instantiate multiple DQNs (or some other similar algorithm) for
    the various agents, and each agent sees the environment as it is and takes actions.
    If the agents we are trying to control all use the same policy, which is a reasonable
    assumption in some cases (e.g., in a multi-player game where each player is identical),
    then we could even re-use a single DQN (i.e., a single set of parameters) to model
    multiple agents.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们已知的内容扩展到多智能体设置的最直接方法是为各种智能体实例化多个DQN（或某些其他类似算法），并且每个智能体都按照其所见的环境采取行动。如果我们试图控制的智能体都使用相同的策略，这在某些情况下是一个合理的假设（例如，在多玩家游戏中，每个玩家都是相同的），那么我们甚至可以重用一个DQN（即一组参数）来模拟多个智能体。
- en: This approach is called *independent Q-learning* (IL-Q), and it works reasonably
    well, but it misses the fact that interactions between agents affect the decision-making
    of each. With an IL-Q algorithm, each agent is completely unaware of what other
    agents are doing and how other agents’ actions might affect itself. Each agent
    only gets a state representation of the environment, which includes the current
    state of each other agent, but it essentially treats the activity of other agents
    in the environment as noise since the behavior of other agents is, at most, only
    partially predictable ([figure 9.2](#ch09fig02)).
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为*独立Q学习*（IL-Q），它工作得相当好，但它忽略了智能体之间的交互会影响每个智能体的决策这一事实。在使用IL-Q算法的情况下，每个智能体完全不知道其他智能体在做什么以及其他智能体的动作可能如何影响自己。每个智能体只得到环境的状态表示，其中包括每个其他智能体的当前状态，但它本质上将环境中其他智能体的活动视为噪声，因为其他智能体的行为最多只能部分预测（[图9.2](#ch09fig02)）。
- en: Figure 9.2\. In independent Q-learning, an agent does not directly perceive
    the actions of other agents but rather pretends they are part of the environment.
    This is an approximation that loses the convergence guarantees that Q-learning
    has in the single-agent setting, since the other agents make the environment nonstationary.
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2. 在独立Q学习中，智能体不会直接感知其他智能体的动作，而是假装它们是环境的一部分。这是一个近似，因为它失去了在单智能体设置中Q学习所具有的收敛保证，因为其他智能体使得环境变得非平稳。
- en: '![](09fig02.jpg)'
  id: totrans-893
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2](09fig02.jpg)'
- en: In the ordinary Q-learning we’ve done so far, where there’s only a single agent
    in the environment, we know the Q function will converge to the optimal value,
    so we will converge on an optimal policy (it is mathematically guaranteed to converge
    in the long run). This is because in the single-agent setting, the environment
    is *stationary*, meaning the distribution of rewards for a given action in a given
    state is always the same ([figure 9.3](#ch09fig03)). This stationary feature is
    violated in the multi-agent setting since the rewards an individual agent receives
    will vary not only based on its own actions but on the actions of other agents.
    This is because all agents are reinforcement learning agents that learn through
    experience; their policies are constantly changing in response to changes in the
    environment. If we use IL-Q in this nonstationary environment, we lose the convergence
    guarantee, and this can impair the performance of independent Q-learning significantly.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止所进行的普通Q学习中，环境中只有一个智能体，我们知道Q函数将收敛到最优值，因此我们将收敛到最优策略（从数学上保证在长期内收敛）。这是因为单智能体设置中，环境是*平稳的*，意味着在给定状态下给定动作的奖励分布始终相同（[图9.3](#ch09fig03)）。在多智能体设置中，这种平稳特性被违反，因为单个智能体收到的奖励不仅取决于其自身的动作，还取决于其他智能体的动作。这是因为所有智能体都是通过经验学习的强化学习智能体；他们的策略会随着环境的变化而不断变化。如果我们在这个非平稳环境中使用IL-Q，我们将失去收敛保证，这可能会严重影响独立Q学习的性能。
- en: Figure 9.3\. In a stationary environment, the expected (i.e., average) value
    over time for a given state will remain constant (stationary). Any particular
    state transition may have a stochastic component, hence the noisy-looking time
    series, but the mean of the time series is constant. In a nonstationary environment,
    the expected value for a given state transition will change over time, which is
    depicted in this time series as a changing mean or baseline over time. The Q function
    is trying to learn the expected value for state-actions, and it can only converge
    if the state-action values are stationary, but in the multi-agent setting, the
    expected state-action values can change over time due to the evolving policies
    of other agents.
  id: totrans-895
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3。在稳定环境中，给定状态随时间变化的期望（即平均）值将保持恒定（稳定）。任何特定的状态转移可能有一个随机成分，因此时间序列看起来有噪声，但时间序列的均值是恒定的。在非稳定环境中，给定状态转移的期望值会随时间变化，这在时间序列中表现为随时间变化的均值或基线。Q函数试图学习状态-动作的期望值，并且只有当状态-动作值是稳定的时候，它才能收敛。但在多智能体设置中，由于其他智能体策略的演变，期望状态-动作值可能会随时间变化。
- en: '![](09fig03_alt.jpg)'
  id: totrans-896
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig03_alt.jpg)'
- en: 'A normal Q function is a function *Q*(*s*,*a*): *S* × *A* → *R* ([figure 9.4](#ch09fig04));
    it’s a function from a state-action pair to a reward (some real number). We can
    remedy the problems with IL-Q by making a slightly more sophisticated Q function
    that incorporates knowledge of the actions of other agents, *Q[j]*(*s*,*a[j],a[–j]*):
    *S* × *A[j]* × *A[–j]* → *R*. This is a Q function for the agent indexed by *j*
    that takes a tuple of the state, agent *j*’s action, and all the other agents’
    actions (denoted –*j*, pronounced “not *j*”) to the predicted reward for this
    tuple (again, just a real number). It is known that a Q function of this sort
    regains the convergence guarantee that it will eventually learn the optimal value
    and policy functions, and thus this modified Q function is able to perform much
    better.'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '正常的Q函数是一个函数 *Q*(*s*,*a*): *S* × *A* → *R* ([图9.4](#ch09fig04))；它是一个从状态-动作对到奖励（某个实数）的函数。我们可以通过创建一个稍微复杂一些的Q函数来修复IL-Q的问题，该函数结合了其他智能体动作的知识，*Q[j]*(*s*,*a[j],a[–j]*):
    *S* × *A[j]* × *A[–j]* → *R*。这是一个针对索引为 *j* 的智能体的Q函数，它接受一个状态、智能体 *j* 的动作以及所有其他智能体的动作（表示为
    –*j*，发音为“非 *j*”）的元组，并预测这个元组的奖励（再次，只是一个实数）。已知这种类型的Q函数能够恢复收敛保证，即它最终会学习到最优值和策略函数，因此这个修改后的Q函数能够表现得更好。'
- en: Figure 9.4\. The Q function takes a state and produces state-action values (Q
    values), which are then used by the policy function to produce an action. Alternatively,
    we can directly train a policy function that operates on a state and returns a
    probability distribution over actions.
  id: totrans-898
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4。Q函数接受一个状态并产生状态-动作值（Q值），然后这些值被策略函数用来产生一个动作。或者，我们也可以直接训练一个策略函数，该函数在状态上操作并返回一个动作的概率分布。
- en: '![](09fig04_alt.jpg)'
  id: totrans-899
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig04_alt.jpg)'
- en: Unfortunately, this new Q function is intractable when the number of agents
    is large because the joint action-space *a[–j]* is extremely large and grows exponentially
    with the number of agents. Remember how we encode an action? We use a vector with
    length equal to the number of actions. If we want to encode a single action, we
    make this a *one-hot vector* where all elements are 0 except at the position corresponding
    to the action, which is set to 1\. For example, in the Gridworld environment the
    agent has four actions (up, down, left, right), so we encode actions as a length
    4 vector, where [1,0,0,0] could be encoded as “up” and [0,1,0,0] could be “down”
    and so forth.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当智能体数量很大时，这个新的Q函数是不可处理的，因为联合动作空间 *a[–j]* 非常大，并且随着智能体数量的增加而呈指数增长。还记得我们如何编码动作吗？我们使用一个长度等于动作数量的向量。如果我们想要编码一个单独的动作，我们将其编码为一个
    *one-hot 向量*，其中所有元素都是0，除了对应于动作的位置，该位置被设置为1。例如，在Gridworld环境中，智能体有四个动作（上、下、左、右），因此我们将动作编码为一个长度为4的向量，其中
    [1,0,0,0] 可以编码为“上”，[0,1,0,0] 可以编码为“下”，依此类推。
- en: 'Remember, the policy *π*(*s*):*S* → *A* is a function that takes a state and
    returns an action. If it is a deterministic policy, it will have to return one
    of these one-hot vectors; if it is a stochastic policy, it returns a probability
    distribution over the actions, e.g., [0.25,0.25,0.2,0.3]. The exponential growth
    is due to the fact that if we want to unambiguously encode a joint action—for
    example, the joint action of two agents with four actions each in Gridworld—then
    we have to use a 4² = 16 length one-hot vector instead of just a 4 length vector.
    This is because there are 16 different possible combinations of actions between
    two agents with 4 actions each: [Agent 1: Action 1, Agent 2: Action 4], [Agent
    1: Action 3, Agent 2: Action 3] and so on (see [figure 9.5](#ch09fig05)).'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，策略*π*(*s*):*S* → *A*是一个函数，它接受一个状态并返回一个行动。如果它是一个确定性策略，它必须返回这些one-hot向量中的一个；如果它是一个随机策略，它返回一个关于行动的概率分布，例如，[0.25,0.25,0.2,0.3]。指数增长是由于如果我们想要明确编码一个联合行动——例如，Gridworld中两个代理的联合行动，每个代理有四个动作——那么我们必须使用一个4²
    = 16长度的一热向量，而不是一个4长度的向量。这是因为两个具有4个动作的代理之间有16种不同的行动组合可能：[代理1：行动1，代理2：行动4]，[代理1：行动3，代理2：行动3]等等（参见[图9.5](#ch09fig05))。
- en: Figure 9.5\. If each agent has an action space of size 4 (i.e., it is represented
    by a 4 element one-hot vector), the joint action space of two agents is 42 = 16,
    or 4N where N is the number of agents. This means the growth of the joint action
    space is exponential in the number of agents. The figure on the right shows the
    joint action space size for agents with individual action spaces of size 2\. Even
    with just 25 agents, the joint action space becomes a 33,554,432 element one-hot
    vector, which is computationally impractical to work with.
  id: totrans-902
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5. 如果每个代理的动作空间大小为4（即由一个4元素的一热向量表示），两个代理的联合行动空间是4² = 16，或者4*N*，其中*N*是代理的数量。这意味着联合行动空间的增长是随着代理数量的指数增长的。右边的图显示了具有单个动作空间大小为2的代理的联合行动空间大小。即使只有25个代理，联合行动空间也变成了一个包含33,554,432个元素的one-hot向量，这在计算上是不切实际的。
- en: '![](09fig05_alt.jpg)'
  id: totrans-903
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5](09fig05_alt.jpg)'
- en: If we want to model the joint action of 3 agents, we have to use a 4³ = 64 length
    vector. So, in general for Gridworld, we have to use a 4*^N* length vector, where
    *N* is the number of agents. For any environment, the size of the joint action
    vector will be |*A*|*^N* where |*A*| refers to the size of the action space (i.e.,
    the number of discrete actions). That is an exponentially growing vector in the
    number of agents, and this is impractical and intractable for any significant
    number of agents. Exponential growth is always a bad thing, since it means your
    algorithm can’t scale. This exponentially large joint action space is the main
    new complication that *multi-agent reinforcement learning* (MARL) brings, and
    it is the problem we’ll spend this chapter solving.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要模拟3个代理的联合行动，我们必须使用一个4³ = 64长度的向量。所以，对于Gridworld来说，我们通常需要使用一个4*N*长度的向量，其中*N*是代理的数量。对于任何环境，联合行动向量的大小将是|*A*|*^N*，其中|*A*|指的是动作空间的大小（即离散动作的数量）。这是一个随着代理数量的指数增长的向量，对于任何大量的代理来说，这是不切实际且难以处理的。指数增长总是件坏事，因为它意味着你的算法无法扩展。这个指数级大的联合行动空间是*多代理强化学习*（MARL）带来的主要新复杂性，也是我们将在这章中解决的问题。
- en: 9.2\. Neighborhood Q-learning
  id: totrans-905
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 邻域Q学习
- en: You might be wondering if there is a more efficient and compact way of representing
    actions and joint actions that might get around this issue of an impractically
    large joint-action space, but unfortunately there is no unambiguous way to represent
    an action using a more compact encoding. Try thinking of how you could communicate,
    unambiguously, which actions a group of agents took using a single number, and
    you’ll realize you can’t do it better than with an exponentially growing number.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道是否有更高效、更紧凑的方式来表示行动和联合行动，从而可能绕过这个不切实际的大的联合行动空间问题，但不幸的是，没有一种明确的方式来使用更紧凑的编码表示一个行动。试着想想你如何能够用一个单一的数字无歧义地传达一组代理所采取的行动，你就会意识到你无法做得比指数级增长的数字更好。
- en: At this point, MARL doesn’t seem practical, but we can change that by making
    some approximations to this idealized joint-action Q function. One option is to
    recognize that in most environments, only agents in close proximity to each other
    will have any significant effect on each other. We don’t necessarily need to model
    the joint actions of *all* the agents in the environment; we can approximate this
    by only modeling the joint actions of agents within the same *neighborhood*. In
    a sense, we divide the full joint-action space into a set of overlapping subspaces
    and only compute Q values for these much smaller subspaces. We might call this
    method *neighborhood Q-learning* or *subspace Q-learning* ([figure 9.6](#ch09fig06)).
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，多智能体强化学习（MARL）似乎不太实用，但我们可以通过对此理想化的联合行动Q函数进行一些近似来改变这一点。一个选择是认识到，在大多数环境中，只有彼此靠近的智能体之间才会产生任何显著的影响。我们不一定需要模拟环境中所有智能体的联合行动；我们可以通过仅模拟同一*邻域*内智能体的联合行动来近似这一点。从某种意义上说，我们将完整的联合行动空间划分为一组重叠的子空间，并且只为这些小得多的子空间计算Q值。我们可能将这种方法称为*邻域Q学习*或*子空间Q学习*([图9.6](#ch09fig06))。
- en: Figure 9.6\. In neighborhood MARL, each agent has a field of view (FOV) or neighborhood,
    and it can only see the actions of the other agents within this neighborhood.
    However, it may still get the full state information about the environment.
  id: totrans-908
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6。在邻域MARL中，每个智能体都有一个视野（FOV）或邻域，并且它只能看到此邻域内其他智能体的行动。然而，它可能仍然会获得关于环境的完整状态信息。
- en: '![](09fig06.jpg)'
  id: totrans-909
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig06.jpg)'
- en: By constraining the size of the neighborhood, we stop the exponential growth
    of the joint-action space to the fixed size we set for the neighborhood. If we
    have a multi-agent Gridworld with 4 actions for each agent and 100 agents total,
    the full joint-action space is 4^100, which is an intractable size; no computer
    could possibly compute with (or even store) such a large vector. However, if we
    use subspaces of the joint-action space and set the size of each subspace (neighborhood)
    to 3 (so the size of each subspace is 4³ = 64), this is a much bigger vector than
    with a single agent, but it’s definitely something we can compute with. In this
    case, if we’re computing the Q values for agent 1, we find the 3 agents closest
    in distance to agent 1 and build a joint-action one-hot vector of length 64 for
    these 3 agents. That’s what we give to the Q function ([figure 9.7](#ch09fig07)).
    So for each of the 100 agents, we would build these subspace joint-action vectors,
    and use them to compute Q values for each agent. Then we would use those Q values
    to take actions as usual.
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 通过限制邻域的大小，我们阻止联合行动空间以我们为邻域设置的固定大小进行指数增长。如果我们有一个包含4个行动的每个智能体和总共100个智能体的多智能体网格世界，完整的联合行动空间是4^100，这是一个无法处理的大小；没有任何计算机能够计算（甚至存储）如此大的向量。然而，如果我们使用联合行动空间的子空间，并将每个子空间（邻域）的大小设置为3（因此每个子空间的大小是4³
    = 64），这将比单个智能体大得多，但我们确实可以计算它。在这种情况下，如果我们正在计算智能体1的Q值，我们会找到距离智能体1最近的3个智能体，并为这3个智能体构建一个长度为64的联合行动one-hot向量。这就是我们提供给Q函数的内容([图9.7](#ch09fig07))。因此，对于这100个智能体中的每一个，我们都会构建这些子空间联合行动向量，并使用它们来计算每个智能体的Q值。然后，我们会使用这些Q值来像往常一样采取行动。
- en: Figure 9.7\. The neighborhood Q function for agent j accepts the current state
    and the joint-action vector for the other agents within its neighborhood (or field
    of view), denoted *a[–j]*. It produces Q values that get passed to the policy
    function that chooses the action to take.
  id: totrans-911
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7。智能体j的邻域Q函数接受当前状态和其邻域内其他智能体的联合行动向量（或视野），表示为*a[–j]*。它产生传递给选择采取行动的策略函数的Q值。
- en: '![](09fig07_alt.jpg)'
  id: totrans-912
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig07_alt.jpg)'
- en: Let’s write some pseudocode for how this works.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一些伪代码来说明它是如何工作的。
- en: Listing 9.1\. Pseudocode for neighborhood Q-learning, [part 1](kindle_split_009.html#part01)
  id: totrans-914
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.1。邻域Q学习的伪代码，[第一部分](kindle_split_009.html#part01)
- en: '[PRE58]'
  id: totrans-915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '***1*** Iterates through all the agents in the environment, stored in a list'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 遍历环境中存储在列表中的所有智能体'
- en: '***2*** Retrieves the current environment state'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取当前环境状态'
- en: '***3*** This function will find the closest 3 agents to agent j.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 此函数将找到距离智能体j最近的3个智能体。'
- en: '***4*** This function will return the joint action of agent j’s neighbors.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 此函数将返回智能体j邻居的联合行动。'
- en: '***5*** Gets the Q values for each action of agent j, given the state and the
    joint action of its neighbors'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 根据状态和智能体j邻居的联合行动获取每个行动的Q值'
- en: '***6*** This function will return a discrete action using the Q values.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 此函数将使用Q值返回一个离散行动。'
- en: 'The pseudocode in [listing 9.1](#ch09ex01) shows that we need a function that
    takes the current agent *j* and finds its nearest three neighbors, and then we
    need another function that will build the joint action using these three nearest
    neighbors. At this point, we have another problem: how do we build the joint action
    without already knowing the actions of the other agents? In order to compute the
    Q values for agent *j* (and thus take an action), we need to know the actions
    that agents –*j* are taking (we use –*j* to denote the agents that are *not* agent
    *j*, but in this case only the nearest neighbors). In order to figure out the
    actions of agents –*j*, however, we would need to compute all of their Q values,
    and then it seems like we get into an infinite loop and never get anywhere.'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.1](#ch09ex01)中的伪代码显示，我们需要一个函数，它接受当前代理*j*并找到其最近的三个邻居，然后我们需要另一个函数，它将使用这三个最近的邻居构建联合动作。在这个阶段，我们遇到了另一个问题：我们如何在不知道其他代理动作的情况下构建联合动作？为了计算代理*j*的Q值（从而采取行动），我们需要知道代理-*j*正在采取的动作（我们使用-*j*来表示不是代理*j*的代理，但在这个情况下只有最近的邻居）。然而，为了弄清楚代理-*j*的动作，我们需要计算它们的所有Q值，然后似乎我们陷入了一个无限循环，永远无法前进。'
- en: To avoid this problem, we start by initializing all the actions for the agents
    randomly, and then we can compute the joint actions using these random actions.
    But if that’s all we did, using joint actions wouldn’t be much help, since they’re
    random. In the pseudocode in [listing 9.2](#ch09ex02) we address the problem by
    rerunning this process a few times (that’s the `for m in range(M)` part, where
    `M` is some small number like 5). The first time we run this, the joint action
    will be random, but then all the agents will have taken an action based on their
    Q functions, so the second time it will be slightly less random, and if we keep
    doing this a few more times, the initial randomness will be sufficiently diluted
    and we can take the actions at the end of this iteration in the real environment.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，我们首先随机初始化所有代理的动作，然后我们可以使用这些随机动作来计算联合动作。但如果我们只做这些，使用联合动作并不会有多大帮助，因为它们是随机的。在[列表
    9.2](#ch09ex02)中的伪代码中，我们通过重复执行这个过程几次来解决该问题（这就是`for m in range(M)`部分，其中`M`是像5这样的小数字）。第一次运行时，联合动作将是随机的，但随后所有代理都将根据它们的Q函数采取行动，因此第二次将稍微不那么随机，如果我们再这样做几次，初始的随机性将足够稀释，我们就可以在真实环境中采取这个迭代末尾的动作。
- en: Listing 9.2\. Pseudocode for neighborhood Q-learning, [part 2](kindle_split_015.html#part02)
  id: totrans-924
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2\. 邻域Q学习的伪代码，[第二部分](kindle_split_015.html#part02)
- en: '[PRE59]'
  id: totrans-925
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '***1*** Iterates through the process of computing joint actions and Q values
    a few times to dilute the initial randomness'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 通过迭代计算联合动作和Q值几次来稀释初始随机性'
- en: '***2*** Needs to loop through agents again to take the final actions that were
    computed in the previous loop'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 需要再次遍历代理以采取前一个循环中计算的最终动作'
- en: '[Listings 9.1](#ch09ex01) and [9.2](#ch09ex02) show the basic structure of
    how we will implement neighborhood Q-learning, but one detail we’ve left out is
    exactly how to construct the joint-action space for the neighboring agents. We
    build a joint action from a set of individual actions by using the *outer product*
    operation from linear algebra. The simplest way to express this is to “promote”
    an ordinary vector to a matrix. For example, we have a length 4 vector and we
    could promote it to a 4 × 1 matrix. In PyTorch and numpy we can do this using
    the `reshape` method on a tensor, e.g., `torch.Tensor([1,0,0,0]).reshape(1,4)`.
    The result we get when multiplying two matrices depends on their dimensions and
    the order in which we multiply them. If we take an *A*: 1 × 4 matrix and multiply
    it by another matrix *B*: 4 × 1, then we get a 1 × 1 result, which is a *scalar*
    (a single number). This would be the *inner product* of two vectors (promoted
    to matrices), since the largest dimensions are sandwiched in between the two singlet
    dimensions. The outer product is just the reverse of this, where the two large
    dimensions are on the outside and the two singlet dimensions are on the inside,
    resulting in a 4 × 1 ⊗ 1 × 4 = 4 × 4 matrix.'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表9.1](#ch09ex01) 和 [9.2](#ch09ex02) 展示了我们将如何实现邻域Q学习的基本结构，但我们遗漏的一个细节是确切地如何构建邻近智能体的联合动作空间。我们通过使用线性代数中的外积运算从一组个别动作构建联合动作。表达这个的最简单方法是将一个普通向量“提升”为一个矩阵。例如，我们有一个长度为4的向量，我们可以将其提升为一个4
    × 1的矩阵。在PyTorch和numpy中，我们可以通过在张量上使用`reshape`方法来实现这一点，例如，`torch.Tensor([1,0,0,0]).reshape(1,4)`。当我们相乘两个矩阵时得到的结果取决于它们的维度以及我们乘法的顺序。如果我们取一个*A*:
    1 × 4的矩阵并将其与另一个矩阵*B*: 4 × 1相乘，那么我们得到一个1 × 1的结果，这是一个*标量*（一个单独的数字）。这将是两个向量（提升为矩阵）的*内积*，因为最大的维度被夹在两个单维维度之间。外积只是这个过程的逆过程，其中两个大维度在外部，两个单维维度在内部，结果是一个4
    × 1 ⊗ 1 × 4 = 4 × 4的矩阵。'
- en: 'If we have two agents in Gridworld with individual actions [0,0,0,1] (“right”)
    and [0,0,1,0] (“left”), their joint action can be computed by taking the outer
    product of these vectors. Here’s how we do it in numpy:'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Gridworld中有两个具有个别动作[0,0,0,1]（“向右”）和[0,0,1,0]（“向左”）的智能体，它们的联合动作可以通过这些向量的外积来计算。以下是在numpy中如何操作的示例：
- en: '[PRE60]'
  id: totrans-930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The result is a 4 × 4 matrix, with a total of 16 elements as we would expect
    from our discussion in the previous section. The dimension of the result of the
    outer product between two matrices is dim(*A*) * dim(*B*), where *A* and *B* are
    vectors and “dim” refers to the size (dimension) of the vector. The outer product
    is the reason why the joint-action space grows exponentially. Generally, we need
    our neural network Q function to operate on inputs that are vectors, so since
    the outer product gives us a matrix result, we simply flatten it into a vector:'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个4 × 4的矩阵，总共有16个元素，正如我们在上一节讨论中所期望的那样。两个矩阵外积的结果维度是dim(*A*) * dim(*B*)，其中*A*和*B*是向量，“dim”指的是向量的大小（维度）。外积是联合动作空间指数增长的原因。通常，我们需要我们的神经网络Q函数在输入为向量的输入上操作，因此由于外积给我们一个矩阵结果，我们只需将其展平为一个向量：
- en: '[PRE61]'
  id: totrans-932
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Hopefully, you can appreciate that the neighborhood Q-learning approach is not
    much more complicated than ordinary Q-learning. We just need to give it an additional
    input, which is the joint-action vector of each agent’s nearest neighbors. Let’s
    figure out the details by tackling a real problem.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能理解，邻域Q学习的方法并不比普通Q学习复杂多少。我们只需要给它一个额外的输入，即每个智能体最近邻的联合动作向量。让我们通过解决一个实际问题来弄清楚细节。
- en: 9.3\. The 1D Ising model
  id: totrans-934
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. 一维伊辛模型
- en: In this section we’re going to apply MARL to solve a real physics problem that
    was first described in the early 1920s by physicist Wilhelm Lenz and his student
    Ernst Ising. But first, a brief physics lesson. Physicists were trying to understand
    the behavior of magnetic materials such as iron by mathematical models. A piece
    of iron that you can hold in your hand is a collection of iron atoms that are
    grouped together by metallic bonding. An atom is composed of a nucleus of protons
    (positively charged), neutrons (no charge), and an outer “shell” of electrons
    (negatively charged). Electrons, like other elementary particles, have a property
    called *spin*, which is quantized such that an electron can only have a spin-up
    or spin-down at any time ([figure 9.8](#ch09fig08)).
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用多智能体强化学习（MARL）来解决一个真实物理问题，这个问题最初在20世纪20年代初由物理学家维尔纳·伦茨和他的学生恩斯特·伊辛描述。但首先，让我们简要地回顾一下物理学。物理学家试图通过数学模型来理解像铁这样的磁性材料的性质。你可以用手拿的一块铁是由通过金属键结合在一起的铁原子组成的。一个原子由带正电的质子（正电荷）、不带电的中子以及带负电的外层“壳”电子组成。电子，像其他基本粒子一样，有一个叫做自旋的属性，它是量子化的，这意味着电子在任何时候只能向上自旋或向下自旋（[图9.8](#ch09fig08)）。
- en: Figure 9.8\. Electrons are negatively charged elementary particles that surround
    the nucleus of every atom. They have a property called spin, and it can either
    be spin-up or spin-down. Since they are charged particles, they generate a magnetic
    field, and the direction they spin determines the orientation of the poles (north
    or south) of the magnetic field.
  id: totrans-936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8。电子是围绕每个原子核的带负电的基本粒子。它们有一个叫做自旋的属性，可以是向上自旋或向下自旋。由于它们是带电粒子，它们会产生磁场，它们旋转的方向决定了磁场的极（北或南）的方向。
- en: '![](09fig08.jpg)'
  id: totrans-937
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig08.jpg)'
- en: The spin property can be thought of as the electron rotating either clockwise
    or counter-clockwise; this is not literally true, but it suffices for our purposes.
    When a charged object rotates, it creates a magnetic field, so if you took a rubber
    balloon, gave it a static charge by rubbing it on the carpet, and then spun it
    around, you would have yourself a balloon magnet (albeit an extremely weak magnet).
    Electrons likewise create a magnetic field by virtue of their spin and electric
    charge, so electrons really are very tiny magnets, and since all iron atoms have
    electrons, the entire piece of iron can become a big magnet if all of its electrons
    are aligned in the same direction (i.e., all spin-up or all spin-down).
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋属性可以想象成电子要么顺时针旋转，要么逆时针旋转；这虽然不是字面上的真实情况，但对于我们的目的来说已经足够了。当一个带电物体旋转时，它会创造一个磁场，所以如果你拿一个气球，通过在地毯上摩擦给它一个静电电荷，然后让它旋转，你就有了一个气球磁铁（尽管是一个非常微弱的磁铁）。电子同样通过它们的自旋和电荷创造磁场，所以电子实际上是非常微小的磁铁，因为所有铁原子都有电子，如果所有电子都沿着相同方向排列（即所有向上或所有向下），整个铁块就可以变成一个大磁铁。
- en: Physicists were trying to study how the electrons “decide” to align themselves,
    and how the temperature of the iron affects this process. If you heat up a magnet,
    at some point the aligned electrons will start randomly alternating their spins
    so that the material loses its net magnetic field. Physicists knew that an individual
    electron creates a magnetic field, and that a tiny magnetic field will affect
    a nearby electron. If you’ve ever played with two bar magnets, you’ve noticed
    that they will naturally line up in one direction or repel in the opposite direction.
    The electrons do the same thing. It makes sense that electrons would also try
    to align themselves to be the same spin ([figure 9.9](#ch09fig09)).
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 物理学家试图研究电子是如何“决定”对齐自己的，以及铁的温度如何影响这个过程。如果你加热一个磁铁，在某个时刻，对齐的电子将开始随机交替它们的自旋，这样材料就会失去其净磁场。物理学家知道单个电子会产生磁场，而微小的磁场会影响附近的电子。如果你曾经玩过两个条形磁铁，你会注意到它们会自然地沿着一个方向排列或相反方向排斥。电子也是这样做的。电子试图对齐以使自旋方向相同是有道理的（[图9.9](#ch09fig09)）。
- en: Figure 9.9\. When electrons are packed together, they prefer to have their spins
    aligned in the same direction because it is a lower energy configuration than
    when their spins are anti-aligned, and all physical systems tend toward lower
    energy (all else being equal).
  id: totrans-940
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.9。当电子聚集在一起时，它们倾向于使它们的自旋方向一致，因为这比它们的自旋反方向时能量更低，而所有物理系统都倾向于向低能量状态发展（其他条件相同）。
- en: '![](09fig09.jpg)'
  id: totrans-941
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig09.jpg)'
- en: There’s one added complexity though. Although individual electrons have a tendency
    to align themselves, a sufficiently large group of aligned electrons actually
    becomes unstable. This is because as the number of aligned electrons grows larger,
    the magnetic field grows and creates some internal strain on the material. So
    what really happens is that electrons will form clusters, called *domains*, in
    which all the electrons are aligned (either spin up or down), but other domains
    also form. For example, there might be a domain of 100 electrons aligned spin-up
    next to another domain of 100 electrons all aligned spin-down. So at the very
    local level, electrons minimize their energy by being aligned, but when too many
    are aligned and the magnetic field becomes too strong, the overall energy of the
    system grows, causing the electrons to align only into relatively small domains.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，还存在一个额外的复杂性。尽管单个电子有自行对齐的倾向，但足够大的一组对齐电子实际上变得不稳定。这是因为随着对齐电子数量的增加，磁场增强并在材料上产生一些内部应力。所以真正发生的事情是，电子将形成称为“区域”的簇，其中所有电子都是对齐的（要么自旋向上，要么向下），但也会形成其他区域。例如，可能有一个由100个电子组成的自旋向上的区域紧挨着一个由100个电子组成的自旋向下的区域。所以在非常局部层面上，电子通过对齐来最小化它们的能量，但当对齐的电子太多且磁场太强时，系统的整体能量增加，导致电子只对齐到相对较小的区域。
- en: Presumably the interactions between trillions of electrons in the bulk material
    result in the complex organization of electrons into domains, but it is very difficult
    to model that many interactions. So physicists made a simplifying assumption that
    a given electron is only affected by its nearest neighbors, which is exactly the
    same assumption we’ve made with neighborhood Q-learning ([figure 9.10](#ch09fig10)).
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 据推测，大量材料中万亿个电子之间的相互作用会导致电子复杂地组织成区域，但很难模拟这么多相互作用。因此，物理学家做出了一个简化的假设，即一个给定的电子只受其最近邻的影响，这与我们在邻里Q学习（[图9.10](#ch09fig10)）中做出的假设完全相同。
- en: Figure 9.10\. This is a high-resolution Ising model where each pixel represents
    an electron. The lighter pixels are spin-up, and black is spin-down. You can see
    that the electrons organize into domains where all the electrons within a domain
    are aligned, but nearby electrons in an adjacent domain are anti-aligned with
    respect to the first domain. This organization reduces the energy of the system.
  id: totrans-944
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.10。这是一个高分辨率的伊辛模型，其中每个像素代表一个电子。较亮的像素表示自旋向上，黑色表示自旋向下。你可以看到电子组织成区域，其中区域内的所有电子都是对齐的，但相邻区域中的电子与第一个区域是反对齐的。这种组织降低了系统的能量。
- en: '![](09fig10.jpg)'
  id: totrans-945
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10](09fig10.jpg)'
- en: Remarkably, we can model the behavior of many electrons and observe the large-scale
    emergent organization with multi-agent reinforcement learning. All we need to
    do is interpret the energy of an electron as its “reward.” If an electron changes
    its spin to align with its neighbor, we will give it a positive reward; if it
    decides to anti-align, we give it a negative reward. When all the electrons are
    trying to maximize their rewards, this is the same as trying to minimize their
    energy, and we will get the same result the physicists get when they use energy-based
    models.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们可以使用多智能体强化学习来模拟许多电子的行为，并观察大规模的涌现组织。我们所需做的只是将电子的能量解释为其“奖励”。如果一个电子改变其自旋以与其邻居对齐，我们将给予它正奖励；如果它决定反对齐，我们给予它负奖励。当所有电子都试图最大化它们的奖励时，这等同于尝试最小化它们的能量，我们将得到物理学家在使用基于能量的模型时得到的结果。
- en: You might wonder why these modeled electrons wouldn’t just all align in the
    same direction rather than form domains like a real magnet if the electrons get
    positive rewards for being aligned. Our model is not completely realistic, but
    it does end up forming domains because with a sufficiently large number of electrons,
    it becomes increasingly improbable for all of them to align in the same direction,
    given that there is some randomness to the process ([figure 9.11](#ch09fig11)).
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如果电子因为对齐而获得正奖励，为什么这些模拟的电子不会全部朝同一方向对齐，而不是像真实磁铁那样形成区域。我们的模型并不完全现实，但它最终形成了区域，因为随着电子数量的足够大，考虑到过程中存在一些随机性，所有电子都朝同一方向对齐变得越来越不可能（[图9.11](#ch09fig11)）。
- en: Figure 9.11\. This is a depiction of a 2D Ising model of electron spins where
    + is spin-up and – is spin-down. There is a domain of electrons that are all spin-down
    (highlighted in black), and these are surrounded by a shell of spin-up electrons.
  id: totrans-948
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.11。这是电子自旋的二维伊辛模型的描述，其中+表示自旋向上，-表示自旋向下。有一个所有电子自旋向下的电子域（用黑色突出显示），并且这些电子被自旋向上的电子壳层包围。
- en: '![](09fig11_alt.jpg)'
  id: totrans-949
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig11_alt.jpg)'
- en: As you’ll see, we can also model the temperature of the system by changing the
    amount of exploration and exploitation. Remember, exploration involves randomly
    choosing actions, and a high temperature involves random changes as well. They’re
    quite analogous.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们还可以通过改变探索和利用的量来模拟系统的温度。记住，探索涉及随机选择动作，高温也涉及随机变化。它们非常相似。
- en: Modeling the behavior of electron spins may seem unimportant, but the same basic
    modeling technique used for electrons can be used to solve problems in genetics,
    finance, economics, botany, and sociology, among others. It also happens to be
    one of the simplest ways to test out MARL, so that’s our main motivation here.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟电子自旋的行为可能看起来并不重要，但用于电子的相同基本建模技术可以用于解决遗传学、金融、经济学、植物学和社会学等领域的问题。这也恰好是测试多智能体强化学习（MARL）的最简单方法之一，因此这是我们在这里的主要动机。
- en: The only thing we need to do to create an Ising model is to create a grid of
    binary digits where 0 represents spin-down and 1 represents spin-up. This grid
    could be of any dimensions. We could have a one-dimensional grid (a vector), a
    two-dimensional grid (a matrix), or some high-order tensor.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 创建伊辛模型所需做的唯一事情是创建一个二进制数字的网格，其中0表示自旋向下，1表示自旋向上。这个网格可以是任何维度。我们可以有一个一维网格（向量）、二维网格（矩阵）或某些高阶张量。
- en: Over the next few code listings we will first solve the 1D Ising model, since
    it is so easy that we don’t need to use any fancy mechanisms like experience replay
    or distributed algorithms. We won’t even use PyTorch’s built-in optimizers—we
    will write out the gradient descent manually in just a few lines of code. In [listing
    9.3](#ch09ex03) we’ll define some functions to create the electron grid.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个代码列表中，我们首先解决一维伊辛模型，因为它如此简单，以至于我们不需要使用任何像经验回放或分布式算法这样的复杂机制。我们甚至不会使用PyTorch的内置优化器——我们将在几行代码中手动编写梯度下降。在[列表9.3](#ch09ex03)中，我们将定义一些函数来创建电子网格。
- en: 'Listing 9.3\. 1D Ising model: Create the grid and produce rewards'
  id: totrans-954
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.3。一维伊辛模型：创建网格并产生奖励
- en: '[PRE62]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '***1*** Converts the floating-point numbers into a byte object to make it binary'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将浮点数转换为字节对象以使其二进制化'
- en: '***2*** This function takes neighbors in s and compares them to agent a; if
    they match, the reward is higher.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 此函数将s中的邻居与代理a进行比较；如果它们匹配，则奖励更高。'
- en: 'We have two functions in [listing 9.3](#ch09ex03); the first creates a randomly
    initialized 1D grid (a vector) by first creating a grid of numbers drawn from
    a standard normal distribution. Then we set all the negative numbers to be 0 and
    all the positive numbers to be 1, and we will get approximately the same number
    of 1s and 0s in the grid. We can visualize the grid using matplotlib:'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表9.3](#ch09ex03)中，我们有两个函数；第一个通过首先创建一个从标准正态分布中抽取数字的网格来创建一个随机初始化的一维网格（向量）。然后我们将所有负数设置为0，所有正数设置为1，这样在网格中我们将得到大约相同数量的1和0。我们可以使用matplotlib可视化网格：
- en: '[PRE63]'
  id: totrans-959
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As you can see in [figure 9.12](#ch09fig12), the 1s are lightly shaded and the
    0s are dark. We have to use the `np.expand_dims(...)` function to make the vector
    into a matrix by adding a singlet dimension, since `plt.imshow` only works on
    matrices or 3-tensors.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图9.12](#ch09fig12)所示，1被轻微着色，0被深色着色。我们必须使用`np.expand_dims(...)`函数将向量转换为矩阵，通过添加单维来做到这一点，因为`plt.imshow`只适用于矩阵或3张量。
- en: Figure 9.12\. This is a 1D Ising model representing the electron spins for electrons
    arranged in a single row.
  id: totrans-961
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.12。这是一个表示单行排列的电子自旋的一维伊辛模型。
- en: '![](09fig12_alt.jpg)'
  id: totrans-962
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig12_alt.jpg)'
- en: The second function in [listing 9.3](#ch09ex03) is our reward function. It accepts
    a list, `s`, of binary digits, and a single binary digit, `a`, and then compares
    how many values in `s` match `a`. If all of the values match, the reward is maximal,
    and if none of them match, the reward is negative. The input `s` will be the list
    of neighbors. In this case, we will use the two nearest neighbors, so for a given
    agent its neighbors will be the agents to its left and right on the grid. If an
    agent is at the end of the grid, its right neighbor will be the first element
    in the grid, so we wrap around to the beginning. This makes the grid into a circular
    grid.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表9.3](#ch09ex03)中的第二个函数是我们的奖励函数。它接受一个二进制数字列表`s`和一个单独的二进制数字`a`，然后比较`s`中有多少个值与`a`匹配。如果所有值都匹配，则奖励最大，如果没有一个值匹配，则奖励为负。输入`s`将是邻居列表。在这种情况下，我们将使用最近的两个邻居，因此对于给定的代理，其邻居将是网格上其左右两侧的代理。如果一个代理位于网格的末端，其右侧邻居将是网格的第一个元素，因此我们将循环回到开始。这使得网格成为一个环形网格。'
- en: Each element in the grid (either 1 or 0) represents an electron being spin-up
    or spin-down. In reinforcement learning jargon, the electrons are individual *agents*
    in the environment. Agents need to have value functions and policies, so they
    cannot merely be a binary number. The binary number on the grid represents the
    action of the agent, choosing to be either spin-up or spin-down. Hence, we need
    to model our agents using a neural network. We will use a Q-learning approach
    rather than the policy gradient method. In [listing 9.4](#ch09ex04) we define
    a function that will create parameter vectors to be used in a neural network.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 网格中的每个元素（要么是1要么是0）代表一个自旋向上或自旋向下的电子。在强化学习术语中，电子是环境中的个体*代理*。代理需要具有价值函数和政策，因此它们不能仅仅是一个二进制数。网格上的二进制数代表代理的动作，选择自旋向上或自旋向下。因此，我们需要使用神经网络来模拟我们的代理。我们将使用Q学习方法而不是策略梯度方法。在[列表9.4](#ch09ex04)中，我们定义了一个函数，该函数将创建用于神经网络的参数向量。
- en: 'Listing 9.4\. The 1D Ising model: Generate neural network parameters'
  id: totrans-965
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.4\. 一维伊辛模型：生成神经网络参数
- en: '[PRE64]'
  id: totrans-966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '***1*** This function generates a list of parameter vectors for a neural network.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 此函数生成用于神经网络的参数向量列表。'
- en: Since we will be using a neural network to model the Q function, we need to
    generate the parameters for it. In our case, we will use a separate neural network
    for each agent, although this is unnecessary; each agent has the same policy,
    so we could re-use the same neural network. We’ll do this just to show how it
    works; for the later examples we will use a shared Q function for agents with
    identical policies.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用神经网络来模拟Q函数，我们需要为其生成参数。在我们的情况下，我们将为每个代理使用一个单独的神经网络，尽管这并非必需；每个代理都有相同的策略，因此我们可以重用相同的神经网络。我们将这样做只是为了展示它是如何工作的；对于后面的示例，我们将为具有相同策略的代理使用共享的Q函数。
- en: Since the 1D Ising model is so simple, we will write the neural network manually
    by specifying all the matrix multiplications rather than using PyTorch’s built-in
    layers. We need to make a Q function that accepts a state vector and a parameter
    vector, and in the function body we unpack the parameter vector into multiple
    matrices that form each layer of the network.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一维伊辛模型非常简单，我们将手动编写神经网络，通过指定所有矩阵乘法而不是使用PyTorch的内置层。我们需要创建一个Q函数，该函数接受状态向量和参数向量，并在函数体中将参数向量展开成多个矩阵，这些矩阵形成网络的每一层。
- en: 'Listing 9.5\. The 1D Ising model: Defining the Q function'
  id: totrans-970
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.5\. 一维伊辛模型：定义Q函数
- en: '[PRE65]'
  id: totrans-971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '***1*** Takes the first tuple in layers and multiplies those numbers to get
    the subset of the theta vector to use as the first layer'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从layers中的第一个元组中取出这些数字，将它们相乘以获取用作第一个层的theta向量子集'
- en: '***2*** Reshapes the theta vector subset into a matrix for use as the first
    layer of the neural network'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将theta向量子集重塑为矩阵，用作神经网络的第一个层'
- en: '***3*** This is the first layer computation. The s input is a joint-action
    vector of dimensions (4,1).'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 这是第一层计算。输入`s`是一个维度为(4,1)的联合动作向量。'
- en: '***4*** We can also input an activation function to use for the last layer;
    the default is tanh since our reward ranges [-1,1].'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 我们还可以输入用于最后一层的激活函数；默认为tanh，因为我们的奖励范围是[-1,1]。'
- en: This is the Q function implemented as simple 2-layer neural network ([figure
    9.13](#ch09fig13)). It expects a state vector, `s`, that is the binary vector
    of neighbors states, and a parameter vector, theta. It also needs the keyword
    parameter, `layers`, which is a list of the form `[(s1,s2),(s3,s4)...]` that indicates
    the shape of the parameter matrix for each layer. All Q functions return Q values
    for each possible action; in this case they are for down or up (two actions).
    For example, it might return the vector `[-1,1]`, indicating the expected reward
    for changing the spin to down is –1 and the expected reward for changing the spin
    to up is +1.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由简单两层神经网络实现的 Q 函数（[图 9.13](#ch09fig13)）。它期望一个状态向量 `s`，这是一个表示邻居状态的二进制向量，以及一个参数向量
    theta。它还需要一个关键字参数 `layers`，它是一个形式为 `[(s1,s2),(s3,s4)...]` 的列表，表示每层的参数矩阵的形状。所有
    Q 函数都为每个可能的行为返回 Q 值；在这种情况下，它们是向下或向上（两种行为）。例如，它可能返回向量 `[-1,1]`，表示将自旋改变为向下时的预期奖励为
    -1，将自旋改变为向上时的预期奖励为 +1。
- en: Figure 9.13\. The *Q* function for agent *j* accepts a parameter vector and
    a onehot encoded joint-action vector for the neighbors of agent *j*.
  id: totrans-977
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.13\. 代理 *j* 的 *Q* 函数接受一个参数向量和代理 *j* 邻居的 one-hot 编码联合动作向量。
- en: '![](09fig13.jpg)'
  id: totrans-978
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig13.jpg)'
- en: The advantage of using a single parameter vector is that it is easy to store
    all the parameters for multiple neural networks as a list of vectors. We just
    let the neural network unpack the vector into layer matrices. We use the `tanh`
    activation function because its output is in the interval [-1,1], and our reward
    is in the interval [-2,2], so a +2 reward will strongly push the Q value output
    toward +1\. However, we want to be able to re-use this Q function for our later
    projects, so we provide the activation function as an optional keyword parameter,
    `afn`. In [listing 9.6](#ch09ex06) we define some helper functions to produce
    state information from the environment (which is the grid).
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个参数向量的优点是，它很容易将多个神经网络的参数存储为向量的列表。我们只需让神经网络将向量解包为层矩阵。我们使用 `tanh` 激活函数，因为其输出在区间
    [-1,1] 内，而我们的奖励在区间 [-2,2] 内，所以 +2 的奖励将强烈推动 Q 值输出向 +1。然而，我们希望能够将这个 Q 函数用于我们后面的项目，因此我们提供了激活函数作为可选的关键字参数
    `afn`。在 [列表 9.6](#ch09ex06) 中，我们定义了一些辅助函数，用于从环境（即网格）生成状态信息。
- en: 'Listing 9.6\. The 1D Ising model: Get the state of the environment'
  id: totrans-980
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6\. 1D 伊辛模型：获取环境的状态
- en: '[PRE66]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '***1*** Takes a single binary number and turns it into a one-hot encoded action
    vector like [0,1].'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将单个二进制数转换为类似 [0,1] 的 one-hot 编码动作向量。'
- en: '***2*** If the input is 0 (down), the action vector is [1,0]; otherwise it
    is [0,1].'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 如果输入是 0（向下），动作向量是 [1,0]；否则它是 [0,1]。'
- en: '***3*** s is a vector with 2 elements where s[0] = left neighbor, s[1] = right
    neighbor.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** s 是一个包含 2 个元素的向量，其中 s[0] = 左邻居，s[1] = 右邻居。'
- en: '***4*** Gets the action vectors for each element in s'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 获取 s 中每个元素的动作向量'
- en: '***5*** Creates the joint-action space using the outer-product, then flattens
    into a vector'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用外积创建联合动作空间，然后将其展平为向量'
- en: The functions in [listing 9.6](#ch09ex06) are two auxiliary functions we need
    to prepare the state information for the Q function. The `get_substate` function
    takes a single binary number (`0` for spin-down and `1` for spin-up) and turns
    it into a one-hot encoded action vector, where `0` becomes `[1,0]` and `1` becomes
    `[0,1]` for an action space of [down, up]. The grid only contains a series of
    binary digits representing the spin of each agent, but we need to turn those binary
    digits into action vectors and then take the outer product to get a joint-action
    vector for the Q function. In [listing 9.7](#ch09ex07) we put some of the pieces
    we’ve made together to create a new grid and a set of parameter vectors that,
    in effect, comprises the set of agents on the grid.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.6](#ch09ex06) 中的函数是我们为 Q 函数准备状态信息所需的两个辅助函数。`get_substate` 函数接受一个单独的二进制数（`0`
    表示自旋向下，`1` 表示自旋向上）并将其转换为 one-hot 编码的动作向量，其中 `0` 变为 `[1,0]`，`1` 变为 `[0,1]`，动作空间为
    [向下，向上]。网格只包含一系列表示每个代理自旋的二进制数字，但我们需要将这些二进制数字转换为动作向量，然后进行外积以获得 Q 函数的联合动作向量。在 [列表
    9.7](#ch09ex07) 中，我们将我们制作的一些部分组合在一起，创建了一个新的网格和一组参数向量，实际上构成了网格上代理的集合。'
- en: 'Listing 9.7\. The 1D Ising model: Initialize the grid'
  id: totrans-988
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.7\. 1D 伊辛模型：初始化网格
- en: '[PRE67]'
  id: totrans-989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '***1*** Sets the total size of the grid to be a 20-length vector'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将网格的总大小设置为 20 长度的向量'
- en: '***2*** Sets the size of the hidden layer. Our Q function is just a 2-layer
    neural network, so there’s only one hidden layer.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置隐藏层的大小。我们的Q函数只是一个两层神经网络，所以只有一个隐藏层。'
- en: '***3*** Generates a list of parameter vectors that will parameterize the Q
    function'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 生成一组参数向量，这些向量将参数化Q函数'
- en: '***4*** Makes a clone of the grid (for reasons that will become clear in the
    main training loop)'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 创建网格的克隆（原因将在主训练循环中变得清晰）。'
- en: If you run the [listing 9.7](#ch09ex07) code, you should get something like
    [figure 9.14](#ch09fig14), but yours will look different since it is randomly
    initialized.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行[列表9.7](#ch09ex07)代码，你应该得到类似于[图9.14](#ch09fig14)的结果，但你的看起来会不同，因为它是随机初始化的。
- en: '[PRE68]'
  id: totrans-995
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Figure 9.14\. A 1D Ising model of electrons arranged in a single row.
  id: totrans-996
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.14\. 一个由单行电子排列的1D伊辛模型。
- en: '![](09fig14_alt.jpg)'
  id: totrans-997
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig14_alt.jpg)'
- en: You will notice that the spins are pretty randomly distributed between up (1)
    and down (0). When we train our Q function, we expect to get the spins to align
    themselves in the same direction. They may not *all* align in the same direction,
    but they should at least cluster into domains that are all aligned. Let’s get
    into the main training loop now that we have all of the necessary functions defined.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到自旋在向上（1）和向下（0）之间分布得相当随机。当我们训练我们的Q函数时，我们期望自旋会自行对齐到同一方向。它们可能不会*全部*对齐到同一方向，但它们至少应该聚集成所有对齐的域。现在我们已经定义了所有必要的函数，让我们进入主要的训练循环。
- en: 'Listing 9.8\. The 1D Ising model: The training loop'
  id: totrans-999
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.8\. 1D伊辛模型：训练循环
- en: '[PRE69]'
  id: totrans-1000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '***1*** Learning rate'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 学习率'
- en: '***2*** Since we’re dealing with multiple agents, each controlled by a separate
    Q function, we have to keep track of multiple losses.'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 由于我们处理多个智能体，每个智能体由一个单独的Q函数控制，我们必须跟踪多个损失。'
- en: '***3*** Iterates through each agent'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 遍历每个智能体'
- en: '***4*** Gets the left neighbor; if at the beginning, loops to the end'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 获取左侧邻居；如果到达开始，则循环到末尾'
- en: '***5*** Gets the right neighbor; if at the end, loops to the beginning'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 获取右侧邻居；如果到达末尾，则循环到开始'
- en: '***6*** state_ is the two binary digits representing the spins of the left
    and right neighbors.'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** state_ 是表示左右邻居自旋的两个二进制位。'
- en: '***7*** state is a vector of two binary digits representing actions of two
    agents; turn this into'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** state 是一个由两个二进制位组成的向量，表示两个智能体的动作；将其转换为'
- en: '***8*** The policy is to take the action associated with the highest Q value.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 策略是采取与最高Q值相关的动作。'
- en: '***9*** We take the action in our temporary copy of the grid, grid_, and only
    once all agents have taken actions do we copy them into the main grid.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 我们在我们的临时网格副本中采取动作，grid_，并且只有当所有智能体都采取动作后，我们才将它们复制到主网格中。'
- en: '***10*** The target value is the Q value vector with the Q value associated
    with the action taken replaced with the reward observed.'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 目标值是将采取的动作的Q值替换为观察到的奖励的Q值向量。'
- en: '***11*** Manual gradient descent'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 手动梯度下降'
- en: '***12*** Copies the contents of the temporary grid_ into the main grid vector.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 将临时网格_的内容复制到主网格向量中。'
- en: In the main training loop, we iterate through all 20 agents (which are representing
    electrons), and for each one we find its left and right neighbors, get their joint-action
    vector, and use that to compute Q values for the two possible actions of spin-down
    and spin-up. The 1D Ising model, as we’ve set it up, isn’t just a line of grid
    cells but rather a circular chain of grid cells such that all agents have a left
    and right neighbor ([figure 9.15](#ch09fig15)).
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 在主训练循环中，我们遍历所有20个智能体（代表电子），对于每个智能体，我们找到它的左右邻居，获取它们的联合动作向量，并使用该向量计算两个可能的动作（自旋向下和自旋向上）的Q值。我们设置的1D伊辛模型不仅仅是一行网格单元，而是一个环形网格单元链，这样所有智能体都有一个左右邻居（[图9.15](#ch09fig15)）。
- en: Figure 9.15\. We are representing the 1D Ising model with a single binary vector,
    but it is actually a circular grid because we treat the leftmost electron as being
    immediately next to the rightmost electron.
  id: totrans-1014
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.15\. 我们用一个单二进制向量表示1D伊辛模型，但实际上它是一个环形网格，因为我们把最左边的电子视为紧挨着最右边的电子。
- en: '![](09fig15.jpg)'
  id: totrans-1015
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig15.jpg)'
- en: Each agent has its own associated parameter vector that we use to parameterize
    the Q function, so each agent is controlled by a separate deep Q-network (although
    it is only a 2-layer neural network, so not really “deep”). Again, since each
    agent has the same optimal policy, which is to align the same way as its neighbors,
    we could have used a single DQN to control them all. We will use this approach
    in our subsequent projects, but we thought it was useful to show how straightforward
    it is to model each agent separately. In other environments, where agents may
    have differing optimal policies, you would need to use separate DQNs for each
    one.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 每个智能体都有一个与其关联的参数向量，我们用它来参数化Q函数，因此每个智能体由一个独立的深度Q网络（尽管它只是一个2层神经网络，所以并不真正“深”）控制。再次强调，由于每个智能体都有相同的最佳策略，即以与其邻居相同的方式对齐，我们本可以使用单个DQN来控制它们。我们将在后续项目中使用这种方法，但我们认为展示如何单独建模每个智能体是很有用的。在其他环境中，如果智能体可能有不同的最佳策略，你需要为每个智能体使用单独的DQN。
- en: We’ve simplified this main training function a bit to avoid distractions ([figure
    9.16](#ch09fig16)). First, notice that the policy we use is a greedy policy. The
    agent takes the action that has the highest Q value every time; there’s no epsilon-greedy
    policy where we sometimes take a random action. In general, some sort of exploration
    strategy is necessary, but this is such a simple problem that it still works.
    In the next section, we will solve a 2D Ising model on a square grid, and in that
    case we will use a softmax policy where the temperature parameter will model the
    actual physical temperature of the system of electrons we are trying to model.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简化了这个主要训练函数，以避免干扰（[图9.16](#ch09fig16)）。首先，请注意我们使用的策略是贪婪策略。智能体每次都采取具有最高Q值的动作；没有epsilon-greedy策略，其中我们有时会采取随机动作。一般来说，某种探索策略是必要的，但这个问题如此简单，它仍然有效。在下一节中，我们将在一个正方形网格上解决2D伊辛模型，在这种情况下，我们将使用softmax策略，其中温度参数将模拟我们试图模拟的电子系统的实际物理温度。
- en: Figure 9.16\. This is a string diagram for the main training loop. For each
    agent, j, the corresponding Q function accepts a parameter vector and the joint-action
    vector for agent j, denoted a–j. The Q function outputs a 2-element Q value vector
    that is input to the policy function, and it chooses an action (a binary digit)
    that then gets stored in a mirror (clone) of the grid environment. After all agents
    have chosen actions, the mirrored grid synchronizes with the main grid. The rewards
    are generated for each agent and are passed to the loss function, which computes
    a loss and backpropagates the loss into the Q function, and ultimately into the
    parameter vector for updating.
  id: totrans-1018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.16。这是主训练循环的字符串图。对于每个智能体，j，相应的Q函数接受一个参数向量和智能体j的联合动作向量，表示为a–j。Q函数输出一个2个元素的Q值向量，该向量输入到策略函数中，并选择一个动作（一个二进制数字），然后存储在网格环境的镜像（克隆）中。在所有智能体都选择动作后，镜像网格与主网格同步。为每个智能体生成奖励，并将其传递给损失函数，该函数计算损失并将损失反向传播到Q函数，最终更新参数向量。
- en: '![](09fig16_alt.jpg)'
  id: totrans-1019
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16的替代图](09fig16_alt.jpg)'
- en: The other simplification we made is that the target Q value is set to be *r[t]*[+1]
    (the reward after taking the action). Normally it would be *r[t]*[+1] + γ * *V*(*S[t]*[+1]),
    where the last term is the discount factor gamma times the value of the state
    after taking the action. The *V*(*S[t]*[+1]) is calculated by just taking the
    maximum Q value of the subsequent state *S[t]*[+1]. This is the bootstrapping
    term we learned about in the DQN chapter. We will include this term in the 2D
    Ising model later in this chapter.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出的另一个简化是，目标Q值被设置为*r[t]*[+1]（采取动作后的奖励）。通常它会是*r[t]*[+1] + γ * *V*(*S[t]*[+1])，其中最后一项是折扣因子gamma乘以采取动作后的状态值。*V*(*S[t]*[+1])是通过简单地计算后续状态*S[t]*[+1]的最大Q值来计算的。这是我们DQN章节中学到的自举项。我们将在本章后面的2D伊辛模型中包含这个项。
- en: 'If you run the training loop and plot the grid again, you should see something
    like this:'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行训练循环并再次绘制网格，你应该会看到类似这样的：
- en: '[PRE70]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The first plot in [figure 9.17](#ch09fig17) is a scatter plot of the losses
    over each epoch for each agent (each color is a different agent). You can see
    that the losses all fall and plateau around 30 epochs. The bottom plot is our
    Ising model grid, of course, and you can see that it has organized into two domains
    that are all completely aligned with each other. The lighter part in the middle
    is a group of agents that are aligned in the up (`1`) direction, and the rest
    are aligned in the down (`0`) direction. This is much better than the random distribution
    we started off with, so our MARL algorithm definitely worked in solving this 1D
    Ising model.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.17](#ch09fig17)中的第一个图是每个代理（每种颜色代表不同的代理）在每个时代损失值的散点图。你可以看到所有损失值都在大约30个时代时下降并达到平台期。底部的图是我们的伊辛模型网格，当然，你可以看到它已经组织成两个完全相互对齐的域。中间较亮的部分是一组向上（`1`）方向对齐的代理，其余的则向下（`0`）方向对齐。这比我们最初随机分布的要好得多，所以我们的多智能体强化学习算法在解决这个一维伊辛模型方面肯定有效。'
- en: 'Figure 9.17\. Top: The losses for each agent over the training epochs. You
    can see that they all decrease and at a minimum at around 30 epochs or so. Bottom:
    The 1D Ising model after maximizing rewards (minimizing energy). You can see that
    all the electrons are clustered together into domains where they are all oriented
    the same way.'
  id: totrans-1024
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.17。顶部：每个代理在训练时代中的损失。你可以看到它们都在大约30个时代时下降并达到最小值。底部：最大化奖励（最小化能量）后的1D伊辛模型。你可以看到所有电子都聚集在一起，形成所有电子方向相同的域。
- en: '![](09fig17_alt.jpg)'
  id: totrans-1025
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17的替代图](09fig17_alt.jpg)'
- en: We have successfully “solved” the 1D Ising model. Let’s add a bit more complexity
    by moving on to a 2D Ising model. In addition to addressing some of the simplifications
    we’ve made, we’ll introduce a new approach to neighborhood Q-learning called *mean
    field Q-**learning*.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地“解决了”一维伊辛模型。让我们通过转向二维伊辛模型来增加一些复杂性。除了解决我们已做的简化之外，我们还将引入一种新的邻域Q学习方法，称为*平均场Q-学习*。
- en: 9.4\. Mean field Q-learning and the 2D Ising model
  id: totrans-1027
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4. 平均场Q学习和二维伊辛模型
- en: You just saw how a neighborhood Q-learning approach is able to solve the 1D
    Ising model fairly rapidly. This is because, rather than using the full joint-action
    space that would have been a 2^(20) = 1,048,576 element joint-action vector, which
    is intractable, we just used each agent’s left and right neighbors. That reduced
    the size down to a 2² = 4 element joint-action vector, which is very manageable.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚看到了邻域Q学习方法是如何能够快速解决一维伊辛模型的。这是因为，我们并没有使用完整的联合动作空间，这将是一个2^(20) = 1,048,576个元素的联合动作向量，这是难以处理的，我们只是使用了每个代理的左右邻居。这把大小减少到了一个2²
    = 4个元素的联合动作向量，这是非常容易管理的。
- en: In a 2D grid, if we want to do the same thing and just get the joint-action
    space of an agent’s immediate neighbors, there are 8 neighbors, so the joint-action
    space is a 2⁸ = 256 element vector. Computing with a 256 element vector is definitely
    doable, but doing it for say 400 agents in a 20 × 20 grid will start to get costly.
    If we wanted to use a 3D Ising model, the number of immediate neighbors would
    be 26 and the joint-action space is 2^(26) = 67,108,864; now we’re into intractable
    territory again.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个二维网格中，如果我们想做同样的事情，只是获取一个代理的直接邻居的联合动作空间，有8个邻居，所以联合动作空间是一个2⁸ = 256个元素的向量。用256个元素的向量进行计算是可行的，但如果在一个20
    × 20的网格中使用400个代理，这将会变得成本高昂。如果我们想使用三维伊辛模型，直接邻居的数量将是2⁶，联合动作空间是2^(26) = 67,108,864；现在我们又回到了难以处理的地带。
- en: As you can see, the neighborhood approach is much better than using the full
    joint-action space, but with more complex environments, even the joint-action
    space of immediate neighbors is too large when the number of neighbors is large.
    We need to make an even bigger simplifying approximation. Remember, the reason
    why the neighborhood approach works in the Ising model is because an electron’s
    spin is most affected by the magnetic field of its nearest neighbors. The magnetic
    field strength decreases proportionally to the square of the distance from the
    field source, so it is reasonable to ignore distant electrons.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，邻域方法比使用完整的联合动作空间要好得多，但面对更复杂的环境，即使当邻居数量较多时，直接邻居的联合动作空间也太大。我们需要做出更大的简化近似。记住，邻域方法在伊辛模型中之所以有效，是因为电子的磁矩最受其最近邻磁场的影响。磁场强度与磁场源距离的平方成比例减少，因此忽略远距离电子是合理的。
- en: We can make another approximation by noting that when two magnets are brought
    together, the resulting field is a kind of sum of these two magnets ([figure 9.18](#ch09fig18)).
    We can replace the knowledge of there being two separate magnets with an approximation
    of there being one magnet with a magnetic field that is the sum of the two components.
    It is not the individual magnetic fields of the nearest electrons that matter
    so much as their sum, so rather than giving our Q function the spin information
    about each neighboring electron, we can instead give it the sum of their spins.
    For example, in the 1D grid, if the left neighbor has an action vector of [1,0]
    (down) and the right neighbor has an action vector of [0,1] (up), the sum would
    be [1,0] + [0,1] = [1,1].
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过注意到当两个磁铁放在一起时，产生的磁场是这两种磁铁的一种总和（[图9.18](#ch09fig18)）。我们可以用有一个磁场是两个分量之和的磁铁的近似来代替存在两个单独磁铁的知识。不是最近的电子的个别磁场那么重要，而是它们的总和，所以，而不是给我们的Q函数每个邻近电子的旋转信息，我们可以给出它们的旋转总和。例如，在1D网格中，如果左边的邻居有一个动作向量为[1,0]（向下）而右边的邻居有一个动作向量为[0,1]（向上），总和将是[1,0]
    + [0,1] = [1,1]。
- en: 'Figure 9.18\. Left: A single bar magnet and its magnetic field lines. Recall
    that a magnet has two magnetic poles, often called North (N) and South (S). Right:
    Put two bar magnets close together, and their combined magnetic field is a bit
    more complicated. When we’re modeling how the electron spins behave in a 2D or
    3D grid, we care about the overall magnetic field generated by the contributations
    of all the electrons in a neighborhood; we don’t need to know what the magnetic
    field is for each individual electron.'
  id: totrans-1032
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.18\. 左：一根条形磁铁及其磁场线。回想一下，磁铁有两个磁极，通常称为北（N）和南（S）。右：将两个条形磁铁放在一起，它们的组合磁场要复杂一些。当我们模拟电子在2D或3D网格中的自旋行为时，我们关心的是由一个区域内所有电子的贡献产生的整体磁场；我们不需要知道每个单个电子的磁场是什么。
- en: '![](09fig18_alt.jpg)'
  id: totrans-1033
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig18_alt.jpg)'
- en: Machine learning algorithms perform better when data is normalized within a
    fixed range like [0,1], partly due to the fact that the activation functions only
    output data within a limited output range (the *codomain*), and they can be “saturated”
    by inputs that are too large or too small. For example, the `tanh` function has
    a codomain (the range of values that it can possibly output) in the interval [–1,+1],
    so if you give it two really large but non-equal numbers, it will output numbers
    very close to 1\. Since computers have limited precision, the output values both
    might end up rounding to 1 despite being based on different inputs. If we had
    normalized these inputs to be within [–1,1], for example, `tanh` might return
    0.5 for one input and 0.6 for the other, a meaningful difference.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在[0,1]这样的固定范围内归一化时，机器学习算法的表现更好，部分原因是激活函数只输出有限输出范围（*陪域*）内的数据，并且它们可以被过大或过小的输入“饱和”。例如，`tanh`函数的陪域（它可以输出的值的范围）在区间[–1,+1]内，所以如果你给它两个非常大但非相等的数字，它将输出非常接近1的数字。由于计算机的精度有限，输出值可能最终都四舍五入到1，尽管它们基于不同的输入。如果我们把这些输入归一化到[–1,1]内，例如，`tanh`可能对一个输入返回0.5，对另一个输入返回0.6，这是一个有意义的差异。
- en: So rather than just giving the sum of the individual action vectors to our Q
    function, we will give it the sum divided by the total value of all the elements,
    which will normalize the elements in the resulting vector to be between [0,1].
    For example, we will compute [1,0] + [0,1] = [1,1]/2 = [0.5,0.5]. This normalized
    vector will sum to 1, and each element will be between [0,1], so what does that
    remind you of? A probability distribution. We will, in essence, compute a probability
    distribution over the actions of the nearest neighbors, and give that vector to
    our Q function.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不会只是把个别动作向量的总和给我们的Q函数，而是给它总和除以所有元素的总值，这将使结果向量中的元素归一化到[0,1]之间。例如，我们将计算[1,0]
    + [0,1] = [1,1]/2 = [0.5,0.5]。这个归一化向量将总和为1，每个元素将在[0,1]之间，这让你想起了什么？一个概率分布。本质上，我们将计算最近邻动作的概率分布，并将这个向量给我们的Q函数。
- en: '|  |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Computing the mean field action vector**'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算平均场动作向量**'
- en: In general, we compute the mean field action vector with this formula,
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们用这个公式来计算平均场动作向量，
- en: '![](pg264.jpg)'
  id: totrans-1039
  prefs: []
  type: TYPE_IMG
  zh: '![](pg264.jpg)'
- en: where *a[–j]* is just a notation for the mean field of the neighboring agents
    around agent *j*, and *a[i]* refers to the action vector for agent *i*, which
    is one of agent *j*’s neighbors. So we sum all the action vectors in the neighborhood
    of size *N* for agent *j*, and then we divide by the size of the neighborhood
    to normalize the results. If the math doesn’t suit you, you will see how this
    works in Python soon.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a[–j]* 是对智能体 *j* 周围邻居的平均场的表示，而 *a[i]* 指的是智能体 *i* 的动作向量，它是智能体 *j* 的邻居之一。因此，我们计算智能体
    *j* 邻居大小为 *N* 的所有动作向量之和，然后除以邻居的大小以归一化结果。如果你对数学不熟悉，你很快就会在Python中看到它是如何工作的。
- en: '|  |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: This approach is called a *mean field approximation*, or in our case, *mean
    field Q-learning* (MF-Q). The idea is that we compute a kind of average magnetic
    field around each electron rather than supplying the individual magnetic fields
    of each neighbor ([figure 9.19](#ch09fig19)). The great thing about this approach
    is that the mean field vector is only as long as an individual action vector,
    no matter how big our neighborhood size is or how many total agents we have.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为*平均场近似*，或者在我们的情况下，*平均场Q学习*（MF-Q）。其想法是，我们计算围绕每个电子的一种平均磁场，而不是提供每个邻居的单独磁场（[图9.19](#ch09fig19)）。这种方法的好处是，平均场向量与单个动作向量一样长，无论我们的邻居大小是多少，或者我们有多少个总智能体。
- en: Figure 9.19\. The joint action for a pair of electron spins is the outer product
    between their individual action vectors, which is a 4-element one-hot vector.
    Rather than using this exact joint action, we can approximate it by taking the
    average of these two action vectors, resulting in what’s called the mean field
    approximation. For two electrons together, with one spin-up and the other spin-down,
    the mean field approximation results in reducing this two electron system to a
    single “virtual” electron with an indeterminate spin of [0.5,0.5].
  id: totrans-1043
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.19。一对电子自旋的联合动作是它们各自动作向量的外积，这是一个4个元素的独热向量。我们不是使用这个确切的联合动作，而是通过取这两个动作向量的平均值来近似它，这被称为平均场近似。对于两个电子，一个自旋向上，另一个自旋向下，平均场近似将这个双电子系统简化为一个具有不确定自旋[0.5,0.5]的“虚拟”电子。
- en: '![](09fig19.jpg)'
  id: totrans-1044
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19](09fig19.jpg)'
- en: This means that our mean field vector for each agent will only be a 2-element
    vector for the 1D Ising model and also for the 2D and higher dimensional Ising
    models. Our environment can be arbitrarily complex and high-dimensional, and it
    will still be computationally easy.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们每个智能体的平均场向量将只是一个2个元素的向量，对于一维伊辛模型，以及二维和更高维度的伊辛模型。我们的环境可以是任意复杂和高度复杂的，并且计算上仍然容易。
- en: Let’s see how mean field Q-learning (MF-Q) works on the 2D Ising model. The
    2D Ising model is exactly the same as the 1D version, except now it’s a 2D grid
    (i.e., a matrix). The agent in the top-left corner will have its left neighbor
    be the agent in the top-right corner, and its neighbor above will be the agent
    in the bottom-left corner, so the grid is actually wrapped around the surface
    of a sphere ([figure 9.20](#ch09fig20)).
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看平均场Q学习（MF-Q）在二维伊辛模型上的工作原理。二维伊辛模型与一维版本完全相同，只是现在它是一个二维网格（即矩阵）。左上角的智能体将把右上角的智能体作为其左侧邻居，而其上方的邻居将是左下角的智能体，因此网格实际上是围绕球面展开的（[图9.20](#ch09fig20)）。
- en: Figure 9.20\. We represent the 2D Ising model as a 2D square grid (i.e., a matrix),
    but we design it so that there are no boundaries, and the agents that appear to
    be on a boundary are actually immediately adjacent to the agents on the opposite
    side of the grid. Thus the 2D grid is really a 2D grid wrapped around the surface
    of a sphere.
  id: totrans-1047
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.20。我们将二维伊辛模型表示为一个二维正方形网格（即矩阵），但我们设计它使得没有边界，那些看似在边界上的智能体实际上是与网格另一侧的智能体立即相邻的。因此，二维网格实际上是围绕球面展开的二维网格。
- en: '![](09fig20.jpg)'
  id: totrans-1048
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20](09fig20.jpg)'
- en: 'Listing 9.9\. Mean field Q-learning: The policy function'
  id: totrans-1049
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.9。平均场Q学习：策略函数
- en: '[PRE71]'
  id: totrans-1050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '***1*** We will use the deque data structure as an experience replay storage
    list, since it can be set to have a maximum size.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们将使用双端队列（deque）数据结构作为经验回放存储列表，因为它可以被设置为具有最大容量。'
- en: '***2*** We will use the shuffle function to shuffle the experience replay buffer.'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 我们将使用洗牌函数来洗牌经验回放缓冲区。'
- en: '***3*** This policy function takes in a Q value vector and returns an action,
    either 0 (down) or 1 (up).'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 这个策略函数接受一个Q值向量并返回一个动作，要么是0（向下）要么是1（向上）。'
- en: '***4*** This is the softmax function definition.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 这是softmax函数的定义。'
- en: '***5*** The softmax function converts the Q values into a probability distribution
    over the actions. We use the multinomial function to randomly select an action
    weighted by the probabilities.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** Softmax 函数将 Q 值转换为动作的概率分布。我们使用多项式函数根据概率随机选择一个动作。'
- en: The first new function we’re going to use for the 2D Ising model is the softmax
    function. You saw this before in [chapter 2](kindle_split_011.html#ch02) when
    we introduced the idea of a policy function. A policy function is a function,
    *π*:*S* → *A*, from the space of states to the space of actions. In other words,
    you give it a state vector and it returns an action to take. In [chapter 4](kindle_split_013.html#ch04)
    we used a neural network as a policy function and directly trained it to output
    the best actions. In Q-learning, we have the intermediate step of first computing
    action values (Q values) for a given state, and then we use those action values
    to decide which action to take. So in Q-learning, the policy function takes in
    Q values and returns an action.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要为 2D Ising 模型使用的第一个新函数是 softmax 函数。你之前在[第 2 章](kindle_split_011.html#ch02)中见过这个函数，当时我们介绍了策略函数的概念。策略函数是一个函数，*π*:*S*
    → *A*，从状态空间映射到动作空间。换句话说，你给它一个状态向量，它就会返回一个要采取的动作。在[第 4 章](kindle_split_013.html#ch04)中，我们使用神经网络作为策略函数，并直接训练它输出最佳动作。在
    Q-learning 中，我们有一个中间步骤，首先计算给定状态的行动值（Q 值），然后使用这些行动值来决定采取哪个动作。因此，在 Q-learning 中，策略函数接收
    Q 值并返回一个动作。
- en: '|  |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: definition
  id: totrans-1058
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: The softmax function is defined mathematically as
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数的数学定义如下
- en: '![](pg266.jpg)'
  id: totrans-1060
  prefs: []
  type: TYPE_IMG
  zh: '![pg266.jpg](pg266.jpg)'
- en: where *P[t]*(*a*) is the probability distribution over actions, *q[t]*(*a*)
    is a Q-value vector, and *τ* is the temperature parameter.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P[t]*(*a*) 是动作的概率分布，*q[t]*(*a*) 是一个 Q 值向量，*τ* 是温度参数。
- en: '|  |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As a reminder, the softmax function takes in a vector with arbitrary numbers
    and then “normalizes” this vector to be a probability distribution, so that all
    the elements are positive and sum to 1, and each element after the transformation
    is proportional to the element before the transformation (i.e., if an element
    was the largest in the vector, it will be assigned the largest probability). The
    softmax function has one additional input, the temperature parameter, denoted
    with the Greek symbol tau, τ.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，softmax 函数接收一个包含任意数字的向量，然后“归一化”这个向量成为一个概率分布，使得所有元素都是正数，并且总和为 1，并且变换后的每个元素与变换前的元素成比例（即如果一个元素在向量中是最大的，它将被分配最大的概率）。softmax
    函数还有一个额外的输入，即温度参数，用希腊字母 tau，τ 表示。
- en: If the temperature parameter is large, it will minimize the difference in probabilities
    between the elements, and if the temperature is small, differences in the input
    will be magnified. For example, the vector `softmax([10,5,90], temp=100) = [0.2394,
    0.2277, 0.5328]` and `softmax([10,5,90], temp=0.1) = [0.0616, 0.0521, 0.8863]`.
    With a high temperature, even though the last element, 90, is 9 times larger than
    the second-largest element, 10, the resulting probability distribution assigns
    it a probability of 0.53, which is only about twice as big as the second-largest
    probability. When the temperature approaches infinity, the probability distribution
    will be uniform (i.e., all probabilities are equal). When the temperature approaches
    0, the probability distribution will become a *degenerate distribution* where
    all the probability mass is at a single point. By using this as a policy function,
    when τ → ∞, the actions will be selected completely randomly, and when τ → 0,
    the policy becomes the `argmax` function (which we used in the previous section
    with the 1D Ising model).
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 如果温度参数较大，它将最小化元素之间的概率差异，如果温度较小，输入之间的差异将被放大。例如，向量 `softmax([10,5,90], temp=100)
    = [0.2394, 0.2277, 0.5328]` 和 `softmax([10,5,90], temp=0.1) = [0.0616, 0.0521,
    0.8863]`。在高温下，尽管最后一个元素，90，是第二大元素，10 的 9 倍，但结果概率分布只分配给它 0.53 的概率，这仅是第二大概率的两倍左右。当温度趋近于无穷大时，概率分布将是均匀的（即所有概率都相等）。当温度趋近于
    0 时，概率分布将变成一个 *退化的分布*，其中所有概率质量都集中在单个点上。通过将其用作策略函数，当 τ → ∞ 时，动作将被完全随机选择，当 τ → 0
    时，策略变为 `argmax` 函数（我们在上一节中使用 1D Ising 模型时使用过）。
- en: The reason this parameter is called “temperature” is because the softmax function
    is also used in physics to model physical systems like the spins of a system of
    electrons, where the temperature changes the behavior of the system. There’s a
    lot of cross-pollination between physics and machine learning. In physics it’s
    called the *Boltzmann distribution*, where it “gives the probability that a system
    will be in a certain state as a function of that state’s energy and the temperature
    of the system” (Wikipedia). In some reinforcement learning academic papers you
    might see the softmax policy referred to as the Boltzmann policy, but now you
    know it’s the same thing.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数被称为“温度”的原因是因为 softmax 函数在物理学中也被用来模拟物理系统，例如电子系统的自旋，其中温度会改变系统的行为。物理学和机器学习之间有很多交叉融合。在物理学中，它被称为
    *玻尔兹曼分布*，其中它“给出了系统处于某种状态的概率，这取决于该状态的能量和系统的温度”（维基百科）。在一些强化学习学术论文中，你可能会看到 softmax
    政策被称为玻尔兹曼政策，但现在你知道它们是同一件事。
- en: We are using a reinforcement learning algorithm to solve a physics problem,
    so the temperature parameter of the softmax function actually corresponds to the
    temperature of the electron system we are modeling. If we set the temperature
    of the system to be very high, the electrons will spin randomly and their tendency
    to align to neighbors will be overcome by the high temperature. If we set the
    temperature too low, the electrons will be stuck and won’t be able to change much.
    In [listing 9.10](#ch09ex10) we introduce a function to find the coordinates of
    agents and another function to generate the rewards in the new 2D environment.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用强化学习算法来解决一个物理学问题，因此 softmax 函数的温度参数实际上对应于我们正在模拟的电子系统的温度。如果我们把系统的温度设置得非常高，电子将随机自旋，它们对齐邻居的倾向将被高温所克服。如果我们把温度设置得太低，电子将陷入停滞，无法进行太多变化。在
    [列表 9.10](#ch09ex10) 中，我们介绍了一个函数来查找代理的坐标，另一个函数来生成新 2D 环境中的奖励。
- en: 'Listing 9.10\. Mean field Q-learning: Coordinate and reward functions'
  id: totrans-1067
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.10\. 均场 Q 学习：坐标和奖励函数
- en: '[PRE72]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '***1*** Takes a single index value from the flattened grid and converts it
    back into [x,y] coordinates'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从展平的网格中取一个单一索引值并将其转换回 [x,y] 坐标'
- en: '***2*** Finds x coordinate'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 查找 x 坐标'
- en: '***3*** Finds y coordinate'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 查找 y 坐标'
- en: '***4*** This is the reward function for the 2D grid.'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 这是 2D 网格的奖励函数。'
- en: '***5*** The reward is based on how different the action is from the mean field
    action.'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 奖励基于动作与平均场动作的差异程度。'
- en: '***6*** Scales the reward to be between [–1,+1] using the tanh function'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用 tanh 函数将奖励缩放到 [–1,+1] 范围内'
- en: 'It is inconvenient to work with [x,y] coordinates to refer to agents in the
    2D grid. We generally refer to agents using a single index value based on flattening
    the 2D grid into a vector, but we need to be able to convert this flat index into
    [x,y] coordinates, and that is what the `get_coords` function does. The `get_reward_2d`
    function is our new reward function for the 2D grid. It computes the difference
    between an action vector and a mean field vector. For example, if the mean field
    vector is [0.25,0.75] and the action vector is [1,0], the reward should be lower
    than if the action vector were [0,1]:'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [x,y] 坐标来引用 2D 网格中的代理是不方便的。我们通常使用基于将 2D 网格展平为向量的单一索引值来引用代理，但我们需要能够将这个扁平索引转换为
    [x,y] 坐标，这正是 `get_coords` 函数所做的事情。`get_reward_2d` 函数是我们为 2D 网格的新奖励函数。它计算动作向量与平均场向量的差异。例如，如果平均场向量是
    [0.25,0.75]，动作向量是 [1,0]，则奖励应该低于如果动作向量是 [0,1] 的情况：
- en: '[PRE73]'
  id: totrans-1076
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now we need to create a function that will find an agent’s nearest neighbors
    and then compute the mean field vector for these neighbors.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建一个函数，该函数将找到代理的最近邻居，然后计算这些邻居的平均场向量。
- en: 'Listing 9.11\. Mean field Q-learning: Calculate the mean action vector'
  id: totrans-1078
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.11\. 均场 Q 学习：计算平均动作向量
- en: '[PRE74]'
  id: totrans-1079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '***1*** Converts vectorized index j into grid coordinates [x,y], where [0,0]
    is top left'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将向量化的索引 j 转换为网格坐标 [x,y]，其中 [0,0] 是左上角'
- en: '***2*** This will be the action mean vector that we will add to.'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这将是我们将要添加的动作均值向量。'
- en: '***3*** Two for loops allow us to find each of the 8 nearest neighbors of agent
    j.'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 两个循环允许我们找到代理 j 的 8 个最近邻居。'
- en: '***4*** Converts each neighbor’s binary spin into an action vector'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将每个邻居的二进制自旋转换为动作向量'
- en: '***5*** Normalizes the action vector to be a probability distribution'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将动作向量归一化为概率分布'
- en: This function accepts an agent index, j (a single integer, the index based on
    the flattened grid) and returns that agent’s eight nearest (surrounding) neighbors’
    mean action on the grid. We find the eight nearest neighbors by getting the agent’s
    coordinates, such as [5,5], and then we just add every combination of [x,y] where
    *x*,*y* ∈ {0,1}. So we’ll do [5,5] + [1,0] = [6,5] and [5,5] + [-1,1] = [4,6],
    etc.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一个智能体索引 j（一个整数，基于展平网格的索引）并返回该智能体在网格上的八个最近（周围）邻居的平均动作。我们通过获取智能体的坐标，例如 [5,5]，然后只是添加所有
    [x,y] 的组合，其中 *x*, *y* ∈ {0,1}。所以我们将做 [5,5] + [1,0] = [6,5] 和 [5,5] + [-1,1] =
    [4,6]，等等。
- en: These are all the additional functions we need for the 2D case. We’ll re-use
    the `init_grid` function and `gen_params` functions from earlier. Let’s initialize
    the grid and parameters.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是我们需要用于 2D 情况的附加函数。我们将重新使用之前提到的 `init_grid` 函数和 `gen_params` 函数。让我们初始化网格和参数。
- en: '[PRE75]'
  id: totrans-1087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We’re starting with a 10 × 10 grid to make it run faster, but you should try
    playing with larger grid sizes. You can see in [figure 9.21](#ch09fig21) that
    the spins are randomly distributed on the initial grid, so we hope that after
    we run our MARL algorithm it will look a lot more organized—we hope to see clusters
    of aligned electrons. We’ve reduced the hidden layer size to 10, to further reduce
    the computational cost. Notice that we’re only generating a single parameter vector;
    we’re going to be using a single DQN to control all of the 100 agents, since they
    have the same optimal policy. We’re creating two copies of the main grid for reasons
    that will be clear once we get to the training loop.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个 10 × 10 的网格开始，以便使其运行更快，但你应该尝试使用更大的网格尺寸。你可以在[图 9.21](#ch09fig21)中看到，自旋在初始网格上是随机分布的，所以我们希望运行我们的多智能体强化学习算法后，它将看起来更有组织——我们希望看到对齐的电子簇。我们将隐藏层的大小减少到
    10，以进一步降低计算成本。请注意，我们只生成一个参数向量；我们将使用单个 DQN 来控制所有 100 个智能体，因为它们具有相同的最佳策略。我们正在创建主网格的两个副本，原因将在训练循环中变得清晰。
- en: Figure 9.21\. This is a randomly initialized 2D Ising model. Each grid square
    represents an electron. The light-colored grid squares represent electrons oriented
    with spin-up and the dark squares are spin-down.
  id: totrans-1089
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.21\. 这是一个随机初始化的 2D 伊辛模型。每个网格方块代表一个电子。浅色网格方块代表自旋向上的电子，深色方块是自旋向下的。
- en: '![](09fig21.jpg)'
  id: totrans-1090
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig21.jpg)'
- en: 'For this example, we are going to add some of the complexities we left out
    of the 1D case, since this is a harder problem. We will be using an experience
    replay mechanism to store experiences and train on mini-batches of these experiences.
    This reduces the variance in the gradients and stabilizes the training. We will
    also use the proper target Q values, *r[t]*[+1] + γ * *V*(*S[t]*[+1]), so we need
    to calculate Q values twice per iteration: once to decide which action to take,
    and then again to get *V*(*S[t]*[+1]). In [listing 9.12](#ch09ex12) we jump into
    the main training loop of the 2D Ising model.'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将添加一些我们在 1D 情况中省略的复杂性，因为这是一个更难的问题。我们将使用经验回放机制来存储经验，并在这些经验的迷你批次上进行训练。这减少了梯度的方差并稳定了训练。我们还将使用正确的目标
    Q 值，*r[t]*[+1] + γ * *V*(*S[t]*[+1])，因此我们需要在每个迭代中计算 Q 值两次：一次决定采取哪个动作，然后再次获取 *V*(*S[t]*[+1])。在[列表
    9.12](#ch09ex12)中，我们跳入了 2D 伊辛模型的主要训练循环。
- en: 'Listing 9.12\. Mean field Q-learning: The main training loop'
  id: totrans-1092
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.12\. 平均场 Q 学习：主要训练循环
- en: '[PRE76]'
  id: totrans-1093
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '***1*** num_iter controls how many times we iterate to get rid of the initial
    randomness from the mean field actions.'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** num_iter 控制我们迭代多少次以消除平均场动作的初始随机性。'
- en: '***2*** Makes a list of lists to store the losses for each agent'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建一个列表的列表来存储每个智能体的损失'
- en: '***3*** replay_size controls the total number of experiences we store in the
    experience replay list.'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** replay_size 控制我们在经验回放列表中存储的总经验数。'
- en: '***4*** The experience replay is a deque collection, which is basically a list
    with a maximum size.'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 经验回放是一个双端队列集合，基本上是一个具有最大大小的列表。'
- en: '***5*** Sets the batch size to 10, so we get a random subset of 10 experiences
    from the replay and train with that'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将批大小设置为 10，因此我们从回放中获取一个随机的 10 个经验样本并使用它进行训练'
- en: '***6*** The discount factor'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 折扣因子'
- en: '***7*** Stores the mean field actions for all the agents'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 存储所有智能体的平均场动作'
- en: '***8*** Stores the Q values for the next state after taking an action'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 存储采取动作后的下一个状态的 Q 值'
- en: '***9*** Since mean fields are initialized randomly, we iterate a few times
    to dilute the initial randomness.'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 由于平均场是随机初始化的，我们迭代几次以稀释初始随机性。'
- en: '***10*** Iterates through all agents in the grid'
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 遍历网格中的所有代理'
- en: '***11*** Collects an experience and adds to the experience replay buffer'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 收集经验并将其添加到经验重放缓冲区'
- en: '***12*** Once the experience replay buffer has more experiences than the batch
    size parameter, starts training'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12*** 当经验重放缓冲区中的经验比批大小参数多时，开始训练'
- en: '***13*** Generates a list of random indices to subset the replay buffer'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13*** 生成一个随机索引列表以子集重放缓冲区'
- en: That’s a lot of code, but it’s only a little more complicated than what we had
    for the 1D Ising model. The first thing to point out is that since the mean field
    of each agent depends on its neighbors, and the neighbors’ spins are randomly
    initialized, all the mean fields will be random to begin with too. To help convergence,
    we first allow each agent to select an action based on these random mean fields,
    and we store the action in the temporary grid copy, `grid__`, so that the main
    grid doesn’t change until all agents have made a final decision about which action
    to take. After each agent has made a tentative action in `grid__`, we update the
    second temporary grid copy, `grid_` which is what we’re using to calculate the
    mean fields. In the next iteration, the mean fields will change, and we allow
    the agents to update their tentative actions. We do this a few times (controlled
    by the `num_iter` parameter) to allow the actions to stabilize around a near optimal
    value based on the current version of the Q function. Then we update the main
    grid and collect all the actions, rewards, mean fields, and `q_next` values (*V*(*S[t]*[+1])
    and add them to the experience replay buffer.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很多，但比我们之前用于1D伊辛模型的代码复杂不了多少。首先要注意的是，由于每个代理的均值场依赖于其邻居，而邻居的磁矩是随机初始化的，因此所有均值场一开始也将是随机的。为了帮助收敛，我们首先允许每个代理根据这些随机均值场选择一个动作，并将这个动作存储在临时的网格副本`grid__`中，这样主网格就不会改变，直到所有代理都做出了关于要采取哪个动作的最终决定。在每个代理在`grid__`中做出试探性动作之后，我们更新第二个临时的网格副本`grid_`，这是我们用来计算均值场的。在下一个迭代中，均值场将发生变化，我们允许代理更新他们的试探性动作。我们这样做几次（由`num_iter`参数控制），以允许动作根据当前版本的Q函数稳定在接近最优值。然后我们更新主网格并收集所有的动作、奖励、均值场和`q_next`值（*V*(*S[t]*[+1]）并将它们添加到经验重放缓冲区。
- en: Once the replay buffer has more experiences than the batch size parameter, we
    can begin training on mini-batches of experiences. We generate a list of random
    index values and use these to subset some random experiences in the replay buffer.
    Then we run one step of gradient descent as usual. Let’s run the training loop
    and see what we get.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦重放缓冲区中的经验比批大小参数多，我们就可以开始对经验的小批量进行训练。我们生成一个随机索引值的列表，并使用这些索引值从重放缓冲区中选取一些随机经验。然后我们像往常一样运行一步梯度下降。让我们运行训练循环，看看我们得到什么。
- en: '[PRE77]'
  id: totrans-1109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: It worked! You can see from [figure 9.22](#ch09fig22) that all but three of
    the electrons (agents) have their spins aligned in the same direction, which minimizes
    the energy of the system (and maximizes the reward). The loss plot looks chaotic
    partly because we’re using a single DQN to model each agent, so the DQN is sort
    of in a battle against itself when one agent is trying to align to its neighbor
    but its neighbor is trying to align to another agent. Some instability can happen.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 这成功了！你可以从[图9.22](#ch09fig22)中看到，除了三个电子（代理）之外的所有电子（代理）的磁矩都朝同一方向对齐，这最小化了系统的能量（并最大化了奖励）。损失图看起来很混乱，部分原因是我们使用单个DQN来模拟每个代理，所以当某个代理试图与其邻居对齐时，而其邻居却试图与另一个代理对齐，DQN似乎是在与自己战斗。可能会发生一些不稳定。
- en: Figure 9.22\. The top graph is the loss plot for the DQN. The loss doesn’t look
    like it is converging, but we can see that it does indeed learn to minimize the
    energy of the system (maximize reward) as a whole in the bottom panel.
  id: totrans-1111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.22。顶部图是DQN的损失图。损失看起来并不像是在收敛，但我们确实可以看到它确实学会了在整个底部面板中整体最小化系统的能量（最大化奖励）。
- en: '![](09fig22_alt.jpg)'
  id: totrans-1112
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig22_alt.jpg)'
- en: In the next section we will push our multi-agent reinforcement learning skills
    to the next level by tackling a harder problem with two teams of agents battling
    against each other in a game.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过解决一个更难的问题，即两个团队在游戏中相互对抗，来将我们的多智能体强化学习技能提升到下一个水平。
- en: 9.5\. Mixed cooperative-competitive games
  id: totrans-1114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5. 混合合作-竞争游戏
- en: If you think of the Ising model as a multiplayer game, it would be considered
    a pure cooperative multiplayer game, since all the agents have the same objective
    and their reward is maximized when they work together to all align in the same
    direction. In contrast, chess would be a pure competitive game, because when one
    player is winning the other player is losing; it is zero-sum. Team-based games,
    like basketball or football, are called *mixed cooperative-competitive games,*
    since the agents on the same team need to cooperate in order to maximize their
    rewards, but when one team as a whole is winning, the other team must be losing,
    so at the team-to-team level it is a competitive game.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将伊辛模型想象成一个多人游戏，那么它将被视为一个纯粹的协作多人游戏，因为所有代理都有相同的目标，并且当它们一起协作并朝相同方向对齐时，它们的奖励最大化。相比之下，象棋将是一个纯粹的竞争游戏，因为当一个玩家获胜时，另一个玩家就会失败；这是一个零和游戏。基于团队的游戏，如篮球或足球，被称为*混合协作-竞争游戏*，因为同一团队的代理需要协作以最大化他们的奖励，但当整个团队获胜时，另一队必须失败，因此在团队对团队层面上它是一个竞争游戏。
- en: In this section we are going to use an open source Gridworld-based game that
    is specially designed for testing multi-agent reinforcement learning algorithms
    in cooperative, competitive, or mixed cooperative-competitive scenarios ([figure
    9.23](#ch09fig23)). In our case, we will set up a mixed cooperative-competitive
    scenario with two teams of Gridworld agents that can move around in the grid and
    can also attack other agents on the opposing team. Each agent starts with 1 “health
    point” (HP), and when they’re attacked the HP decreases little by little until
    it gets to 0, at which point the agent dies and is cleared off the grid. Agents
    get rewards for attacking and killing agents on the opposing team.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个开源的基于网格世界的游戏，该游戏专门设计用于测试在协作、竞争或混合协作-竞争场景中的多智能体强化学习算法（[图9.23](#ch09fig23)）。在我们的案例中，我们将设置一个混合协作-竞争场景，其中有两个可以移动并在网格中移动的Gridworld代理团队，并且它们还可以攻击对方团队的代理。每个代理开始时都有1个“健康点”（HP），当它们被攻击时，HP会逐渐减少，直到达到0，此时代理死亡并被从网格中清除。代理通过攻击和杀死对方团队的代理获得奖励。
- en: Figure 9.23\. Screenshot from the MAgent multiplayer Gridworld game with two
    opposing teams of Gridworld agents. The objective is for each team to kill the
    other.
  id: totrans-1117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.23。MAgent多人网格世界游戏的截图，其中有两个对立的Gridworld代理团队。目标是每个团队杀死对方。
- en: '![](09fig23.jpg)'
  id: totrans-1118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig23.jpg)'
- en: Since all the agents on one team share the same objective and hence the optimal
    policy, we can use a single DQN to control all the agents on one team, and a different
    DQN to control the agents on the other team. It’s basically a battle between two
    DQNs, so this would be a perfect opportunity to try out different kinds of neural
    networks and see which is better. To keep things simple, though, we’ll use the
    same DQN for each team.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个团队上的所有代理都有相同的目标和最优策略，我们可以使用单个DQN来控制一个团队上的所有代理，并使用不同的DQN来控制另一队的代理。这基本上是两个DQN之间的战斗，所以这将是一个尝试不同类型的神经网络并查看哪个更好的完美机会。然而，为了保持简单，我们将为每个团队使用相同的DQN。
- en: You’ll need to install the MAgent library from [https://github.com/geek-ai/MAgent](https://github.com/geek-ai/MAgent)
    by following the instructions on the readme page. From this point on, we’ll assume
    you have it installed and that you can successfully run `import magent` in your
    Python environment.
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要按照readme页面上的说明安装MAgent库[https://github.com/geek-ai/MAgent](https://github.com/geek-ai/MAgent)。从这一点开始，我们将假设您已经安装了它，并且可以在您的Python环境中成功运行`import
    magent`。
- en: Listing 9.13\. Creating the MAgent environment
  id: totrans-1121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.13。创建MAgent环境
- en: '[PRE78]'
  id: totrans-1122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '***1*** Imports the cityblock distance function from scipy to compute distances
    between agents on the grid'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从scipy导入cityblock距离函数来计算网格上代理之间的距离'
- en: '***2*** Sets up the environment in “battle” mode with a 30 x 30 grid'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 在“战斗”模式下设置一个30 x 30网格的环境'
- en: '***3*** Sets up our ability to view the game after training'
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置我们在训练后查看游戏的能力'
- en: '***4*** Initializes the two team objects'
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 初始化两个团队对象'
- en: MAgent is highly customizable, but we will be using the built-in configuration
    called “battle” to set up a two-team battle scenario. MAgent has an API similar
    to OpenAI Gym but there are some important differences. First, we have to set
    up “handles” for each of the two teams. These are objects, `team1` and `team2`,
    that have methods and attributes relevant to each team. We generally pass these
    handles to a method of the environment object, `env`. For example, to get a list
    of the coordinates of each agent on team 1, we use `env.get_pos(team1)`.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: MAgent非常可定制，但我们将使用内置配置“战斗”来设置一个双团队战斗场景。MAgent有一个类似于OpenAI Gym的API，但有一些重要的区别。首先，我们必须为两个团队中的每一个设置“句柄”。这些是具有与每个团队相关的方法和属性的对象，`team1`和`team2`。我们通常将这些句柄传递给环境对象的方法`env`。例如，要获取团队1中每个代理的坐标列表，我们使用`env.get_pos(team1)`。
- en: We’re going to use the same technique to solve this environment as we did for
    the 2D Ising model, but with two DQNs. We’ll use a softmax policy and experience
    replay buffer. Things will get a bit complicated because the number of agents
    changes over training, since agents can die and be removed from the grid.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前解决二维伊辛模型相同的技术来解决这个环境，但这次使用两个深度Q网络（DQN）。我们将使用softmax策略和经验回放缓冲区。由于代理在训练过程中可能会死亡并被从网格中移除，因此代理的数量会随训练而变化，所以事情会变得有些复杂。
- en: 'With the Ising model, the state of the environment was the joint actions; there
    was no additional state information. In MAgent we additionally have the positions
    and health points of agents as state information. The Q function will be *Q[j]*(*s[t]*,*a[–j]*)
    where *a[–j]* is the mean field for the agents within agent *j*’s *field of view*
    (FOV) or neighborhood. By default, each agent has a FOV of the 13 × 13 grid around
    itself. Thus, each agent will have a state of this binary 13 × 13 FOV grid that
    shows a `1` where there are other agents. However, MAgent separates the FOV matrix
    by teams, so each agent has two 13 × 13 FOV grids: one for its own team and one
    for the other team. We will need to combine these into a single state vector by
    flattening and concatenating them together. MAgent also provides the health points
    of the agents in the FOV, but for simplicity we will not use these.'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 在伊辛模型中，环境的状况是联合动作；没有额外的状态信息。在MAgent中，我们还有代理的位置和健康点作为状态信息。Q函数将是*Q[j]*(*s[t]*,*a[–j]*)，其中*a[–j]*是代理*j*的视野（FOV）或邻域内的平均场。默认情况下，每个代理都有一个13
    × 13网格的视野，围绕自身。因此，每个代理将有一个显示其他代理位置的13 × 13二进制FOV网格的状态。然而，MAgent将FOV矩阵按团队分开，所以每个代理有两个13
    × 13的FOV网格：一个用于自己的团队，一个用于其他团队。我们需要通过展平和连接它们来将这些合并成一个单一的状态向量。MAgent还提供了FOV中代理的健康点，但为了简单起见，我们不会使用这些。
- en: We’ve initialized the environment, but we haven’t initialized the agents on
    the grid. We now have to decide how many agents and where to place them on the
    grid for each team.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经初始化了环境，但还没有初始化网格上的代理。现在我们必须决定每个团队有多少代理，以及它们在网格上的位置。
- en: Listing 9.14\. Adding the agents
  id: totrans-1131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.14\. 添加代理
- en: '[PRE79]'
  id: totrans-1132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '***1*** Generates two parameter vectors to parameterize two DQNs'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 生成两个参数向量来参数化两个DQN'
- en: '***2*** Sets the number of agents for each team to 16'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将每个团队的代理数量设置为16'
- en: '***3*** Sets the initial gap distance between each team’s agents'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 设置每个团队代理之间的初始间隙距离'
- en: '***4*** Loops to position agents on team 1 on the left side of the grid'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 循环将团队1的代理定位在网格的左侧'
- en: '***5*** Loops to position agents on team 2 on the right side of the grid'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 循环将团队2的代理定位在网格的右侧'
- en: '***6*** Adds the agents to the grid for team 1 using the position lists we
    just created'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用我们刚刚创建的位置列表将代理添加到团队1的网格中'
- en: Here we’ve set up our basic parameters. We’re creating a 30 × 30 grid with 16
    agents for each team to keep the computational cost low, but if you have a GPU,
    feel free to make a bigger grid with more agents. We initialize two parameter
    vectors, one for each team. Again we’re only using a simple two-layer neural network
    as the DQN. We can now visualize the grid.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经设置了基本参数。我们创建了一个30 × 30的网格，每个团队有16个代理以保持计算成本低，但如果你有GPU，你可以自由地创建一个更大的网格，并添加更多的代理。我们初始化了两个参数向量，每个团队一个。我们再次只使用简单的两层神经网络作为DQN。现在我们可以可视化网格了。
- en: '[PRE80]'
  id: totrans-1140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Team 2 is on the left and team 1 on the right ([figure 9.24](#ch09fig24)). All
    the agents are initialized in a square pattern, and the teams are separated by
    just one grid square. Each agent’s action space is a 21-length vector, depicted
    in [figure 9.25](#ch09fig25). In [listing 9.15](#ch09ex15) we introduce a function
    to find the neighboring agents of a particular agent.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 团队2位于左侧，团队1位于右侧（[图9.24](#ch09fig24)）。所有代理都以正方形模式初始化，团队之间仅隔一个网格方块。每个代理的动作空间是一个21个长度的向量，如图[9.25](#ch09fig25)所示。在[列表9.15](#ch09ex15)中，我们介绍了一个函数来找到特定代理的邻近代理。
- en: Figure 9.24\. The starting positions for the two teams of agents in the MAgent
    environment. The light squares are the individual agents.
  id: totrans-1142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.24\. MAgent环境中两个代理团队的起始位置。浅色方块是单个代理。
- en: '![](09fig24.jpg)'
  id: totrans-1143
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig24.jpg)'
- en: Figure 9.25\. This depicts the action space for agents in the MAgent library.
    Each agent can move in 13 different directions or attack in 8 directions immediately
    around it. The turn actions are disabled by default, so the action space is 13
    + 8 = 21.
  id: totrans-1144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.25\. 这描绘了MAgent库中代理的动作空间。每个代理可以朝13个不同的方向移动或在其周围8个方向立即攻击。默认情况下，转向动作被禁用，因此动作空间为13
    + 8 = 21。
- en: '![](09fig25.jpg)'
  id: totrans-1145
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig25.jpg)'
- en: Listing 9.15\. Finding the neighbors
  id: totrans-1146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.15\. 寻找邻居
- en: '[PRE81]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '***1*** Given [x,y] positions of all agents in pos_list, returns indices of
    agents that are within the radius of agent j'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 给定pos_list中所有代理的[x,y]位置，返回在代理j半径范围内的代理索引'
- en: We need this function to find the neighbors in the FOV of each agent to be able
    to calculate the mean action vector. We can use `env.get_pos(team1)` to get a
    list of coordinates for each agent on team 1, and then we can pass this into the
    `get_neighbors` function along with an index, `j`, to find the neighbors of agent
    `j`.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这个函数来找到每个代理视野中的邻居，以便能够计算平均动作向量。我们可以使用`env.get_pos(team1)`来获取团队1中每个代理的坐标列表，然后我们可以将此传递给`get_neighbors`函数，并附带一个索引`j`，以找到代理`j`的邻居。
- en: '[PRE82]'
  id: totrans-1150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: So agent `5` has 10 other agents on team 1 within its 13 × 13 FOV.
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代理`5`在其13 × 13视野范围内有10个团队1的代理。
- en: We now need to create a few other helper functions. The actions that the environment
    accepts and returns are integers 0 to 20, so we need to be able to convert this
    to a one-hot action vector and back to integer form. We also need a function that
    will get the mean field vector for the neighbors around an agent.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一些其他辅助函数。环境接受的和返回的动作是0到20的整数，因此我们需要能够将此转换为one-hot动作向量和返回到整数形式。我们还需要一个函数来获取代理周围邻居的平均场向量。
- en: Listing 9.16\. Calculating the mean field action
  id: totrans-1153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.16\. 计算平均场动作
- en: '[PRE83]'
  id: totrans-1154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '***1*** Converts integer representation of action into one-hot vector representation'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将动作的整数表示转换为one-hot向量表示'
- en: '***2*** Converts one-hot vector action into integer representation'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将one-hot向量动作转换为整数表示'
- en: '***3*** Gets the mean field action of agent j; pos_list is what is returned
    by env.get_pos(team1), and l is action space dimension'
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 获取代理j的平均场动作；pos_list是env.get_pos(team1)返回的，l是动作空间维度'
- en: '***4*** Finds all the neighbors of the agents using pos_list'
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 使用pos_list找到代理的所有邻居'
- en: '***5*** Makes sure we don’t divide by zero'
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 确保我们不会除以零'
- en: The `get_mean_field` function first calls the `get_neighbors` function to get
    the coordinates of all the agents for agent j. The `get_mean_field` function then
    uses these coordinates to get the agents’ action vectors, add them, and divide
    by the total number of agents to normalize. The `get_mean_field` function expects
    the corresponding action vector `act_list` (a list of integer-based actions) where
    indices in `pos_list` and `act_list` match to the same agent. The parameter `r`
    refers to the radius in grid squares around agent j that we want to include as
    neighbors, and `l` is the size of the action space, which is 21.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_mean_field`函数首先调用`get_neighbors`函数来获取代理j的所有代理的坐标。然后`get_mean_field`函数使用这些坐标来获取代理的动作向量，将它们相加，并除以代理总数以归一化。`get_mean_field`函数期望相应的动作向量`act_list`（基于整数的动作列表），其中`pos_list`和`act_list`中的索引与同一代理匹配。参数`r`指的是我们想要包括为邻居的代理j周围的网格方块半径，而`l`是动作空间的大小，即21。'
- en: Unlike the Ising model examples, we’re going to create separate functions to
    select actions for each agent and to do training, since this is a more complex
    environment and we want to modularize a bit more. After each step in the environment,
    we get an observation tensor for all the agents simultaneously.
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 与伊辛模型示例不同，我们将为每个智能体创建单独的函数来选择动作并进行训练，因为这是一个更复杂的环境，我们希望进行更多的模块化。在环境中的每一步之后，我们同时为所有智能体获得一个观察张量。
- en: The observation returned by `env.get_observation(team1)` is actually a tuple
    with two tensors. The first tensor is shown in the top portion of [figure 9.26](#ch09fig26).
    It is a complex high-order tensor, whereas the second tensor in the tuple has
    some additional information that we will ignore. From now on, when we say *observation*
    or *state*, we mean the first tensor as depicted in [figure 9.26](#ch09fig26).
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.get_observation(team1)` 返回的观察实际上是一个包含两个张量的元组。第一个张量显示在 [图 9.26](#ch09fig26)
    的顶部部分。它是一个复杂的高阶张量，而元组中的第二个张量包含一些我们将忽略的额外信息。从现在开始，当我们说 *观察* 或 *状态* 时，我们指的是 [图 9.26](#ch09fig26)
    中所示的第一个张量。'
- en: Figure 9.26\. The structure of the observation tensor. It is an *N* x 13 x 13
    x 7 tensor where *N* is the number of agents on the team.
  id: totrans-1163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.26\. 观察张量的结构。它是一个 *N* x 13 x 13 x 7 的张量，其中 *N* 是团队中的智能体数量。
- en: '![](09fig26.jpg)'
  id: totrans-1164
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig26.jpg)'
- en: '[Figure 9.26](#ch09fig26) shows that this observation tensor is arranged in
    slices. The observation is an *N* × 13 × 13 × 7 tensor where *N* is the number
    of agents (in our case 16). Each 13 × 13 slice of the tensor for a single agent
    shows the FOV with the location of the wall (slice 0), team 1 agents (slice 1),
    team 1 agents’ HPs (slice 2), and so forth. We will only be using slices 1 and
    4 for the locations of the agents on team 1 and team 2 within the FOV. So, a single
    agent’s observation tensor will be 13 × 13 × 2, and we will flatten this into
    a vector to get a 338-length state vector. We’ll then concatenate this state vector
    with the mean field vector, which is length 21, to get a 338 + 21 = 359 length
    vector that will be given to the Q function. It would be ideal to use a two-headed
    neural network like we did in [chapter 7](kindle_split_017.html#ch07). That way
    one head could process the state vector and the other could process the mean field
    action vector, and we could then recombine the processed information in a later
    layer. We did not do that here for simplicity, but is a good exercise for you
    to try. In listing 9.27 we define a function to choose the action for an agent,
    given its observation (the mean field of its neighboring agents).'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.26](#ch09fig26) 显示，这个观察张量是按切片排列的。观察是一个 *N* × 13 × 13 × 7 的张量，其中 *N* 是智能体的数量（在我们的案例中是
    16）。单个智能体的张量切片（13 × 13）显示了视野中的墙壁位置（切片 0）、团队 1 的智能体（切片 1）、团队 1 智能体的生命值（切片 2）等等。我们只将使用切片
    1 和 4 来确定团队 1 和团队 2 在视野中的智能体位置。因此，单个智能体的观察张量将是 13 × 13 × 2，我们将将其展平成一个 338 长度的状态向量。然后我们将这个状态向量与长度为
    21 的平均场向量连接起来，得到一个 338 + 21 = 359 长度的向量，这个向量将被提供给 Q 函数。使用像我们在 [第 7 章](kindle_split_017.html#ch07)
    中所做的那样具有两个头部的神经网络将是理想的。这样，一个头部可以处理状态向量，另一个可以处理平均场动作向量，然后我们可以在后续的层中重新组合处理过的信息。我们没有在这里这样做是为了简单起见，但这是一个很好的练习，你可以尝试。在列表
    9.27 中，我们定义了一个函数来选择智能体的动作，给定它的观察（其邻近智能体的平均场）。'
- en: Listing 9.17\. Choosing actions
  id: totrans-1166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.17\. 选择动作
- en: '[PRE84]'
  id: totrans-1167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '***1*** Gets the number of agents'
  id: totrans-1168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 获取智能体的数量'
- en: '***2*** Clones the action vector to avoid changing in place'
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 复制动作向量以避免原地更改'
- en: '***3*** Alternates a few times to converge on action'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 几次交替以收敛到动作'
- en: '***4*** Loops through the agents and computes their neighborhood mean field
    action vectors'
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 遍历智能体并计算它们的邻域平均场动作向量'
- en: '***5*** Uses the mean field actions and state to compute Q values and select
    actions using a softmax policy'
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用平均场动作和状态来计算 Q 值并使用 softmax 策略选择动作'
- en: '***6*** Randomly initializes the mean field vectors'
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 随机初始化平均场向量'
- en: 'This is the function we will use to choose all the actions for each agent after
    we get an observation. It utilizes a mean field Q function parameterized by `param`
    and `layers` to sample actions for all agents using the softmax policy. The `infer_acts`
    function takes the following parameters (vector dimensions in parentheses for
    each):'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在获得观察后用于为每个智能体选择所有动作的函数。它利用由 `param` 和 `layers` 参数化的平均场 Q 函数，使用 softmax
    策略为所有智能体采样动作。`infer_acts` 函数接受以下参数（每个向量的维度用括号括起来）：
- en: '`obs` is the observation tensor, *N* × 13 × 13 × 2.'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`obs` 是观察张量，*N* × 13 × 13 × 2。'
- en: '`mean_fields` is tensor containing all the mean field actions for each agent,
    *N* × 21.'
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean_fields` 是包含每个智能体的所有均值场动作的张量，*N* × 21。'
- en: '`pos_list` is list of positions for each agent returned by `env.get_pos(...).`'
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos_list` 是每个智能体返回的 `env.get_pos(...)` 的位置列表。'
- en: '`acts` is a vector of integer-represented actions of each agent (N,).'
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts` 是每个智能体的整数表示的动作向量（N,）。'
- en: '`num_iter` is the number of times to alternative between action sampling and
    policy updates.'
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iter` 是在动作采样和政策更新之间交替的次数。'
- en: '`temp` is the softmax policy temperature, to control the exploration rate.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temp` 是 softmax 策略温度，用于控制探索率。'
- en: 'The function returns a tuple:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 函数返回一个元组：
- en: '`acts_` is a vector of integer actions sampled from policy (N,).'
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts_` 是从策略中采样得到的整数动作向量（N,）。'
- en: '`mean_fields_` is a tensor of mean field vectors for each agent (N,21).'
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean_fields_` 是每个智能体的均值场向量张量（N,21）。'
- en: '`qvals` is a tensor of Q values for each action for each agent (N,21).'
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qvals` 是每个智能体每个动作的 Q 值张量（N,21）。'
- en: Lastly, we need the function that does the training. We will give this function
    our parameter vector and experience replay buffer and let it do the mini-batch
    stochastic gradient descent.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个执行训练的函数。我们将向这个函数提供我们的参数向量和经验回放缓冲区，并让它执行迷你批次的随机梯度下降。
- en: Listing 9.18\. The training function
  id: totrans-1186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.18\. 训练函数
- en: '[PRE85]'
  id: totrans-1187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '***1*** Generates a random list of indices to subset the experience replay'
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 生成一个随机索引列表来划分经验回放'
- en: '***2*** Subsets the experience replay buffer to get a mini-batch of data'
  id: totrans-1189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将经验回放缓冲区划分为迷你批次的数据子集'
- en: '***3*** Collects all states from the mini-batch into single tensor'
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将迷你批次的全部状态收集到一个单独的张量中'
- en: '***4*** Collects all actions from the mini-batch into a single tensor'
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将迷你批次的全部动作收集到一个单独的张量中'
- en: '***5*** Collects all rewards from the mini-batch into a single tensor'
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将迷你批次的全部奖励收集到一个单独的张量中'
- en: '***6*** Collects all mean field actions from the mini-batch into a single tensor'
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将迷你批次的全部均值场动作收集到一个单独的张量中'
- en: '***7*** Collects all state values from the mini-batch into a single tensor'
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将迷你批次的全部状态值收集到一个单独的张量中'
- en: '***8*** Loops through each experience in the mini-batch'
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 遍历迷你批次中的每个经验'
- en: '***9*** Computes Q values for each experience in the replay'
  id: totrans-1196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 计算回放中每个经验的经验值'
- en: '***10*** Computes the target Q values'
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 计算目标 Q 值'
- en: '***11*** Stochastic gradient descent'
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 随机梯度下降'
- en: This function works pretty much the same way we did experience replay with the
    2D Ising model in [listing 9.12](#ch09ex12), but the state information is more
    complicated.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数基本上与我们在 [列表 9.12](#ch09ex12) 中的 2D Ising 模型中执行的经验回放的方式相同，但状态信息更复杂。
- en: 'The `train` function trains a single neural network using stored experiences
    in an experience replay memory buffer. It has the following inputs and outputs:'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 函数使用经验回放缓冲区中存储的经验来训练单个神经网络。它有以下输入和输出：'
- en: 'Inputs:'
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：
- en: '`batch_size` (`int`)'
  id: totrans-1202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`)'
- en: '`replay`, list of tuples `(obs_1_small, acts_1,rewards1,act_means1,qnext1)`'
  id: totrans-1203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replay`, 包含元组的列表 `(obs_1_small, acts_1, rewards1, act_means1, qnext1)`'
- en: '`param` (`vector`) neural network parameter vector'
  id: totrans-1204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`param` (`vector`) 神经网络参数向量'
- en: '`layers` (`list`) contains shape of neural network layers'
  id: totrans-1205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers` (`list`) 包含神经网络层的形状'
- en: '`J` (`int`) number of agents on this team'
  id: totrans-1206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`J` (`int`) 该团队中的智能体数量'
- en: '`gamma` (`float` in `[0,1]`) discount factor'
  id: totrans-1207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma` (`float` in `[0,1]`) 折扣因子'
- en: '`lr` (`float`) learning rate for SGD'
  id: totrans-1208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr` (`float`) 用于随机梯度下降的学习率'
- en: 'Returns:'
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回：
- en: '`loss` (`float`)'
  id: totrans-1210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`float`)'
- en: We’ve now set up the environment, set up the agents for the two teams, and defined
    several functions to let us train the two DQNs we’re using for mean field Q-learning.
    Now we get into the main loop of game play. Be warned, there is a lot of code
    in the next few listings, but most of it is just boilerplate and isn’t critical
    for understanding the overall algorithm.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经设置了环境，为两个团队设置了智能体，并定义了几个函数，让我们能够训练我们用于均值场 Q 学习的两个 DQN。现在我们进入游戏的主循环。警告，接下来的几个列表中有大量的代码，但其中大部分只是样板代码，对于理解整体算法并不关键。
- en: Let’s first set up our preliminary data structures, like the replay buffers.
    We will need separate replay buffers for team 1 and team 2\. In fact, we will
    need almost everything separate for team 1 and team 2.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置我们的初步数据结构，比如回放缓冲区。我们需要为团队 1 和团队 2 分别设置回放缓冲区。实际上，我们需要为团队 1 和团队 2 几乎所有东西都是分开的。
- en: Listing 9.19\. Initializing the actions
  id: totrans-1213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.19\. 初始化动作
- en: '[PRE86]'
  id: totrans-1214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '***1*** Stores the number of agents on each team'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 存储每个团队中的智能体数量'
- en: '***2*** Initializes the actions for all the agents'
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 初始化所有智能体的动作'
- en: '***3*** Creates replay buffer using a deque data structure'
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用deque数据结构创建重放缓冲区'
- en: '***4*** Creates tensors to store the Q(s’) values, where s’ is the next state'
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 创建张量以存储Q(s’）值，其中s’是下一个状态'
- en: '***5*** Initializes the mean fields for each agent'
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 为每个代理初始化均值场'
- en: '***6*** Creates tensors to store the rewards for each agent'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 创建张量以存储每个代理的奖励'
- en: The variables in [listing 9.19](#ch09ex19) allow us to keep track of the actions
    (integers), mean field action vectors, rewards, and next state Q values for each
    agent so that we can package these into experiences and add them into the experience
    replay system. In [listing 9.20](#ch09ex20) we define a function to take actions
    on behalf of a particular team of agents and another function to store experiences
    in the replay buffer.
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表9.19](#ch09ex19)中的变量使我们能够跟踪每个代理的动作（整数）、均值场动作向量、奖励和下一个状态的Q值，以便我们可以将这些打包成经验并添加到经验重放系统中。在[列表9.20](#ch09ex20)中，我们定义了一个函数来代表特定团队的代理执行动作，以及另一个函数来将经验存储在重放缓冲区中。'
- en: Listing 9.20\. Taking a team step and adding to the replay
  id: totrans-1222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.20\. 执行团队步骤并添加到重放
- en: '[PRE87]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***1*** Gets observation tensor from team 1, which is a 16 x 13 x 13 x 7 tensor'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从团队1获取观察张量，这是一个16 x 13 x 13 x 7的张量'
- en: '***2*** Gets the list of indices for the agents that are still alive'
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取仍然存活的代理的索引列表'
- en: '***3*** Subsets the observation tensor to only get the positions of the agents'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将观察张量子集化，仅获取代理的位置'
- en: '***4*** Gets the list of coordinates for each agent on a team'
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 获取团队中每个代理的坐标列表'
- en: '***5*** Decides which actions to take using the DQN for each agent'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用DQN为每个代理决定采取哪些动作'
- en: '***6*** Adds each individual agent’s experience separately to the replay buffer'
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 分别将每个代理的经验添加到重放缓冲区中'
- en: '***7*** Loops through each agent'
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 遍历每个代理'
- en: The `team_step` function is the workhouse of the main loop. We use it to collect
    all the data from the environment and to run the DQN to decide which actions to
    take. The `add_to_replay` function takes the observation tensor, action tensor,
    reward tensor, action mean field tensor, and the next state Q value tensor and
    adds each individual agent experience to the replay buffer separately.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: '`team_step` 函数是主循环的工作核心。我们使用它来收集环境中的所有数据并运行DQN以决定采取哪些动作。`add_to_replay` 函数接受观察张量、动作张量、奖励张量、动作均值场张量和下一个状态Q值张量，并将每个代理的单独经验分别添加到重放缓冲区中。'
- en: The rest of the code is all within a giant `while` loop, so we will break it
    into parts, but just remember that it’s all part of the same loop. Also remember
    that all this code is together in Jupyter Notebooks on this book’s GitHub page
    at [http://mng.bz/JzKp](http://mng.bz/JzKp). It contains all of the code we used
    to create the visualizations, and more comments. We finally get to the main training
    loop of the algorithm in [listing 9.21](#ch09ex21).
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分都在一个巨大的 `while` 循环中，因此我们将将其分解成几个部分，但请记住，它们都是同一个循环的一部分。同时，请记住，所有这些代码都集中在这个书的GitHub页面上的Jupyter
    Notebooks中，网址为[http://mng.bz/JzKp](http://mng.bz/JzKp)。它包含了我们用来创建可视化效果的代码以及更多的注释。我们最终到达算法的主训练循环，见[列表9.21](#ch09ex21)。
- en: Listing 9.21\. Training loop
  id: totrans-1233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.21\. 训练循环
- en: '[PRE88]'
  id: totrans-1234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '***1*** While the game is not over'
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 当游戏未结束时'
- en: '***2*** Uses the team_step method to collect environment data and choose actions
    for the agents using the DQN'
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用团队步骤方法收集环境数据并使用DQN为代理选择动作'
- en: '***3*** Instantiates the chosen actions in the environment'
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 在环境中实例化选定的动作'
- en: '***4*** Takes a step in the environment, which will generate a new observation
    and rewards'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在环境中执行一步，这将生成新的观察结果和奖励'
- en: '***5*** Reruns team_step to get the Q values for the next state in the environment'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将团队步骤应用于获取环境中下一状态的质量值'
- en: '***6*** Renders the environment for viewing later'
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 将环境渲染出来以便稍后查看'
- en: '***7*** Collects the rewards into a tensor for each agent'
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将每个代理的奖励收集到一个张量中'
- en: The `while` loop runs as long as the game is not over; the game ends when all
    the agents on one team die. Within the `team_step` function, we first get the
    observation tensor and subset the part we want as we described before, resulting
    in a 13 × 13 × 2 tensor. We also get `ids_1`, which are the indices for the agents
    that are still alive on team 1\. We also need to get the coordinate positions
    of each agent on each team. Then we use our `infer_acts` function to choose actions
    for each agent and instantiate them in the environment, and finally take an environment
    step, which will generate new observations and rewards. Let’s continue in the
    while loop.
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环会一直运行，直到游戏没有结束；当一队所有智能体死亡时，游戏结束。在`team_step`函数中，我们首先获取观察张量，并像之前描述的那样子集我们想要的
    部分，结果得到一个13 × 13 × 2的张量。我们还得到`ids_1`，这是团队1中仍然存活的智能体的索引。我们还需要获取每个团队中每个智能体的坐标位置。然后我们使用我们的`infer_acts`函数为每个智能体选择动作并在环境中实例化它们，最后进行环境步骤，这将生成新的观察和奖励。让我们继续在`while`循环中。'
- en: Listing 9.22\. Adding to the replay (still in `while loop` from [listing 9.21](#ch09ex21))
  id: totrans-1243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.22\. 在回放中添加（仍在`while loop`中，来自[列表9.21](#ch09ex21)）
- en: '[PRE89]'
  id: totrans-1244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '***1*** Adds to experience replay'
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 添加到经验回放'
- en: '***2*** Shuffles the replay buffer'
  id: totrans-1246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 打乱回放缓冲区'
- en: '***3*** Builds a zipped list of IDs to keep track of which agents died and
    will be cleared from the grid'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 构建一个ID的压缩列表，以跟踪哪些智能体死亡并将从网格中清除'
- en: '***4*** Clears the dead agents off the grid'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 清除网格上的死亡智能体'
- en: '***5*** Now that the dead agents are cleared, gets the new list of agent IDs'
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 现在已清除死亡的智能体，获取新的智能体ID列表'
- en: '***6*** Subsets the old list of IDs based on which agents are still alive'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 根据哪些智能体仍然存活，将旧ID列表划分为子集'
- en: '***7*** Subsets the action list based on the agents that are still alive'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 根据仍然存活的智能体，将动作列表划分为子集'
- en: '***8*** If the replay buffers are sufficiently full, starts training'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 如果回放缓冲区足够满，开始训练'
- en: In this last part of the code, all we do is collect all the data into a tuple
    and append it to the experience replay buffers for training. The one complexity
    of MAgent is that the number of agents decreases over time as they die, so we
    need to do some housekeeping with our arrays to make sure we’re keeping the data
    matched up with the right agents over time.
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的最后一部分，我们只是将所有数据收集到一个元组中，并将其追加到经验回放缓冲区以进行训练。MAgent的一个复杂性是，随着智能体的死亡，智能体的数量会随时间减少，因此我们需要对我们的数组进行一些维护，以确保我们随着时间的推移保持数据与正确的智能体匹配。
- en: 'If you run the training loop for just a handful of epochs, the agents will
    start demonstrating some skill in battle, since we made the grid very small and
    only have 16 agents on each team. You can view a video of the recorded game by
    following the instructions here: [http://mng.bz/aRdz](http://mng.bz/aRdz). You
    should see the agents charge at each other and a few get killed before the video
    ends. [Figure 9.27](#ch09fig27) is a screenshot toward the end of our video showing
    one of the teams clearly beating the other team by attacking them in a corner.'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只运行几个epoch的训练循环，由于我们使网格非常小，并且每个队伍只有16个智能体，智能体将开始在战斗中展示一些技能。你可以通过以下说明观看记录的游戏视频：[http://mng.bz/aRdz](http://mng.bz/aRdz)。你应该会看到智能体相互冲撞，并在视频结束时有一些被杀死。[图9.27](#ch09fig27)是我们视频末尾的截图，显示了其中一个队伍通过在角落攻击对方队伍明显击败了另一个队伍。
- en: Figure 9.27\. Screenshot of the MAgent battle game after training with mean
    field Q-learning. The dark team has forced the light team into the corner and
    it’s attacking them.
  id: totrans-1255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.27\. 使用平均场Q学习训练后的MAgent战斗游戏的截图。暗色队伍迫使浅色队伍进入角落并对其进行攻击。
- en: '![](09fig27.jpg)'
  id: totrans-1256
  prefs: []
  type: TYPE_IMG
  zh: '![图9.27](09fig27.jpg)'
- en: Summary
  id: totrans-1257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Ordinary Q-learning does not work well in the multi-agent setting because the
    environment becomes nonstationary as agents learn new policies.
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多智能体设置中，普通Q学习效果不佳，因为随着智能体学习新策略，环境变得非平稳。
- en: A nonstationary environment means that the expected value of rewards changes
    over time.
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非平稳环境意味着奖励的期望值会随时间变化。
- en: In order to handle this nonstationarity, the Q function needs to have access
    to the joint-action space of other agents, but this joint-action space scales
    exponentially with the number of agents, which becomes intractable for most practical
    problems.
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了处理这种非平稳性，Q函数需要访问其他智能体的联合动作空间，但这个联合动作空间随着智能体数量的指数级增长，对于大多数实际问题来说变得难以处理。
- en: Neighborhood Q-learning can mitigate the exponential scaling by only computing
    over the joint-action space of the immediate neighbors of a given agent, but even
    this can be too large if the number of neighbors is large.
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻域Q学习可以通过仅计算给定代理的即时邻居的联合动作空间来减轻指数级扩展，但如果邻居数量很大，这仍然可能太大。
- en: Mean field Q-learning (MF-Q) scales linearly with the number of agents because
    we only compute a mean action rather than a full joint-action space.
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均场Q学习（MF-Q）与代理数量成线性关系，因为我们只计算一个平均动作而不是完整的联合动作空间。
- en: 'Chapter 10\. Interpretable reinforcement learning: Attention and relational
    models'
  id: totrans-1263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章\. 可解释的强化学习：注意力和关系模型
- en: '*This chapter covers*'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Implementing a relational reinforcement algorithm using the popular self-attention
    model
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流行的自注意力模型实现关系强化算法
- en: Visualizing attention maps to better interpret the reasoning of an RL agent
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过可视化注意力图来更好地解释RL代理的推理
- en: Reasoning about model invariance and equivariance
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于模型不变性和等价性的推理
- en: Incorporating double Q-learning to improve the stability of training
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将双Q学习融入以提升训练的稳定性
- en: Hopefully by this point you have come to appreciate just how powerful the combination
    of deep learning and reinforcement learning is for solving tasks previously thought
    to be the exclusive domain of humans. Deep learning is a class of powerful learning
    algorithms that can comprehend and reason through complex patterns and data, and
    reinforcement learning is the framework we use to solve control problems.
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到这一点，你已经开始欣赏深度学习和强化学习结合在一起解决以前被认为是人类专属领域任务的强大力量。深度学习是一类强大的学习算法，能够理解和推理复杂模式和数据，而强化学习是我们用来解决控制问题的框架。
- en: Throughout this book we’ve used games as a laboratory for experimenting with
    reinforcement learning algorithms as they allow us to assess the ability of these
    algorithms in a very controlled setting. When we build an RL agent that learns
    to play a game well, we are generally satisfied our algorithm is working. Of course,
    reinforcement learning has many more applications outside of playing games; in
    some of these other domains, the raw performance of the algorithm using some metric
    (e.g., the accuracy percentage on some task) is not useful without knowing *how*
    the algorithm is making its decision.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们一直使用游戏作为实验强化学习算法的实验室，因为它们允许我们在一个非常受控的环境中评估这些算法的能力。当我们构建一个RL代理，使其学会玩好游戏时，我们通常对我们的算法是有效的感到满意。当然，强化学习在游戏之外还有许多应用；在这些其他领域，如果不知道算法是如何做出决策的，那么使用某些指标（例如，某些任务上的准确率百分比）的原始性能是没有用的。
- en: For example, machine learning algorithms employed in healthcare decisions need
    to be explainable, since patients have a right to know *why* they are being diagnosed
    with a particular disease or why they are being recommended a particular treatment.
    Although conventional deep neural networks can be trained to achieve remarkable
    feats, it is often unclear what process is driving their decision-making.
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在医疗决策中使用的机器学习算法需要是可解释的，因为患者有权知道他们为什么被诊断出患有特定疾病，或者为什么被推荐特定的治疗方案。尽管传统的深度神经网络可以被训练以实现非凡的成就，但通常不清楚是什么过程在驱动它们的决策。
- en: In this chapter we will introduce a new deep learning architecture that goes
    some way to unraveling this problem. Moreover, it not only offers interpretability
    gains but also performance gains in many cases. This new class of models is called
    *attention models* because they learn how to *attend to* (or focus on) only the
    salient aspects of an input. More specifically for our case, we will be developing
    a *self-attention model*, which is a model that allows each feature within an
    input to learn to attend to various other features in the input. This form of
    attention is closely related to the class of neural networks termed *graph neural
    networks*, which are neural networks explicitly designed to operate on graph structured
    data.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种新的深度学习架构，它有助于解决这一问题。此外，它不仅提供了可解释性的提升，而且在许多情况下也带来了性能的提升。这一类模型被称为*注意力模型*，因为它们学习如何*关注*（或聚焦于）输入中的显著方面。更具体地说，对于我们的案例，我们将开发一个*自注意力模型*，这是一种允许输入中的每个特征学习关注输入中其他各种特征的模型。这种注意力形式与被称为*图神经网络*的神经网络类别密切相关，这些神经网络是专门设计用于在图结构数据上操作的。
- en: 10.1\. Machine learning interpretability with attention and relational biases
  id: totrans-1273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 基于注意力和关系偏好的机器学习可解释性
- en: 'A *graph* (also called a network) is a data structure that is composed of a
    set of *nodes* and *edges* (connections) between nodes ([figure 10.1](#ch10fig01)).
    The nodes could represent anything: people in a social network, publications in
    a publication citation network, cities connected by highways, or even images where
    each pixel is a node and adjacent pixels are connected by edges. A graph is a
    very generic structure for representing data with relational structure, which
    is almost all the data we see in practice. Whereas *convolutional neural networks*
    are designed to process grid-like data, such as images, and *recurrent neural
    networks* are well-poised for sequential data, graph neural networks are more
    generic in that they can handle any data that can be represented as a graph. Graph
    neural networks have opened up a whole new set of possibilities for machine learning
    and they are an active area of research ([figure 10.2](#ch10fig02)).'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: '*图*（也称为网络）是由一组*节点*和节点之间的*边*（连接）组成的**数据结构**（[图10.1](#ch10fig01)）。节点可以代表任何事物：社交网络中的人，出版物引用网络中的出版物，由高速公路连接的城市，甚至图像，其中每个像素是一个节点，相邻像素通过边连接。图是一种非常通用的结构，用于表示具有关系结构的数据，这在实践中我们看到的所有数据几乎都是这样的。而**卷积神经网络**旨在处理网格状数据，如图像，**循环神经网络**则非常适合处理序列数据，图神经网络在这一点上更为通用，因为它们可以处理任何可以表示为图的数据。图神经网络为机器学习开辟了一整套新的可能性，它们是研究的热点领域（[图10.2](#ch10fig02)）。'
- en: Figure 10.1\. A simple graph. Graphs are composed of nodes (the numberlabeled
    circles) and edges (the lines) between nodes that represent relationships between
    nodes. Some data is naturally represented with this kind of graph structure, and
    traditional neural network architectures are unable to process this kind of data.
    Graph neural networks (GNNs), on the other hand, can directly operate on graph-structured
    data.
  id: totrans-1275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1. 一个简单的图。图由节点（编号的圆圈）和节点之间的边（线条）组成，这些边代表节点之间的关系。某些数据自然可以用这种类型的图结构表示，而传统的神经网络架构无法处理这种数据。另一方面，图神经网络（GNNs）可以直接在图结构数据上操作。
- en: '![](10fig01.jpg)'
  id: totrans-1276
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1](10fig01.jpg)'
- en: Figure 10.2\. A graph neural network can directly operate on a graph, compute
    over the nodes and edges, and return an updated graph. In this example, the graph
    neural network decides to remove the edge connecting the bottom two nodes. This
    is an abstract example, but the nodes could represent real world variables and
    the arrows represent causal direction, so the algorithm would be learning to infer
    causal pathways between variables.
  id: totrans-1277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2. 图神经网络可以直接在图上操作，对节点和边进行计算，并返回一个更新后的图。在这个例子中，图神经网络决定移除连接底部两个节点的边。这是一个抽象的例子，但节点可以代表现实世界的变量，而箭头代表因果关系，因此算法将学习推断变量之间的因果关系。
- en: '![](10fig02_alt.jpg)'
  id: totrans-1278
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2](10fig02_alt.jpg)'
- en: '*Self-attention models* (SAMs) can be used to construct graph neural networks,
    but our goal is not to operate on explicitly graph-structured data; we will instead
    be working with image data, as usual, but we will use a self-attention model to
    learn a graph representation of the features within the image. In a sense, we
    hope the SAM will convert a raw image into a graph structure, and that the graph
    structure it constructs will be somewhat interpretable. If we train a SAM on a
    bunch of images of people playing basketball, for example, we might hope it learns
    to associate the people with the ball, and the ball with the basket. That is,
    we want it to learn that the ball is a node, the basket is a node, and the players
    are nodes and to learn the appropriate edges between the nodes. Such a representation
    would give us much more insight into the mechanics of our machine learning model
    than would a conventional convolutional neural network or the like.'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注意力模型*（SAMs）可以用来构建图神经网络，但我们的目标不是在显式图结构数据上操作；我们将像往常一样处理图像数据，但我们将使用自注意力模型来学习图像内部特征的一个图表示。从某种意义上说，我们希望SAM将原始图像转换为图结构，并且它构建的图结构将具有一定的可解释性。例如，如果我们在一组打篮球的人的图像上训练一个SAM，我们可能希望它学会将人关联到球，将球关联到篮筐。也就是说，我们希望它学会球是一个节点，篮筐是一个节点，球员是节点，并学习节点之间适当的边。这种表示将比传统的卷积神经网络或类似的结构给我们提供更多关于我们机器学习模型机制的了解。'
- en: Different neural network architectures such as convolutional, recurrent, or
    attention have different *inductive biases* that can improve learning when those
    biases are accurate. *Inductive reasoning* is when you observe some data and infer
    a more general pattern or rule from it. *Deductive reasoning* is what we do in
    mathematics when we start with some premises and by following logical rules assumed
    to be true, we can make a conclusion with certainty.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的神经网络架构，如卷积、循环或注意力，具有不同的**归纳偏见**，当这些偏见准确时可以改善学习。**归纳推理**是你观察一些数据并从中推断出更普遍的模式或规则的时候。**演绎推理**是我们做数学时从一些前提开始，通过遵循被认为是真实的逻辑规则，我们可以做出确定性结论的时候。
- en: For example, the syllogism “All planets are round. Earth is a planet. Therefore,
    the Earth is round” is a form of deductive reasoning. There is no uncertainty
    about the conclusion if we assume the premises to be true.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，三段论“所有行星都是圆的。地球是一颗行星。因此，地球是圆的”是一种演绎推理的形式。如果我们假设前提是真的，那么结论就没有不确定性。
- en: Inductive reasoning, on the other hand, can only lead to probabilistic conclusions.
    Inductive reasoning is what you do when you play a game like chess. You cannot
    deduce what the other player is going to do; you have to rely on the available
    evidence and make an inference. Biases are essentially your expectations before
    you have seen any data. If you always expected your chess opponent, no matter
    who it was, to make a particular opening move, that would be a strong (inductive)
    bias.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，归纳推理只能导致概率性的结论。当你玩象棋这样的游戏时，你无法推断出对手将要做什么；你必须依靠可用的证据并做出推断。偏见本质上是你看到任何数据之前的期望。如果你总是期望你的对手，无论他是谁，都会做出特定的开局走法，那么这将是一个强烈的（归纳）偏见。
- en: Biases are often talked about in the pejorative sense, but in machine learning
    architectural biases are essential. It is the inductive bias of compositionality,
    i.e., that complex data can be decomposed into simpler and simpler components
    in a hierarchical fashion, that makes deep learning so powerful in the first place.
    If we know the data is images in a grid-like structure, we can make our models
    biased toward learning local features as convolutional neural networks do. If
    we know our data is relational, a neural network with a relational inductive bias
    will improve performance.
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见通常被带有贬义地谈论，但在机器学习中，架构偏见是至关重要的。正是组合性的归纳偏见，即复杂数据可以以分层的方式分解成越来越简单的组件，使得深度学习一开始就如此强大。如果我们知道数据是网格状结构的图像，我们可以使我们的模型偏向于学习局部特征，就像卷积神经网络所做的那样。如果我们知道我们的数据是关系性的，具有关系归纳偏见的神经网络将提高性能。
- en: 10.1.1\. Invariance and equivariance
  id: totrans-1284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1\. 不变性和等变性
- en: 'Biases are the prior knowledge we have about the structure of the data we wish
    to learn, and they make learning much faster. But there’s more to it than just
    biases. With a convolutional neural network (CNN), the bias is toward learning
    local features, but CNNs also have the property of translation *invariance*. A
    function is said to be invariant to a particular transformation of its input when
    such a transformation does not change the output. For example, the addition function
    is invariant to the order of its inputs *add*(*x*,*y*) = *add*(*y*,*x*), whereas
    the subtraction operator does not share this order invariance (this particular
    invariant property has its own special name: *commutativity*). In general, a function,
    *f*(*x*), is invariant with respect to some transformation, *g*(*x*), to its input,
    *x*, when *f*(*g*(*x*)) = *f*(*x*). CNNs are functions in which a translation
    (movement up, down, left, or right) of an object in an image will not impact the
    behavior of the CNN classifier; it is invariant to translation (top panel of [figure
    10.3](#ch10fig03)).'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见是我们对希望学习的数据结构的先验知识，这使得学习过程变得更快。但偏见不仅仅是偏见。使用卷积神经网络（CNN）时，偏见倾向于学习局部特征，但CNNs也具有**平移不变性**的特性。当一个函数对它输入的特定变换不改变输出时，我们说这个函数对该变换是不变的。例如，加法函数对输入顺序是不变的，即`add(x,
    y) = add(y, x)`，而减法运算符则不具备这种顺序不变性（这个特定的不变性质有自己的特殊名称：**交换律**）。一般来说，一个函数$f(x)$相对于其输入$x$的某个变换$g(x)$是不变的，当$f(g(x))
    = f(x)$。CNNs是那种即使图像中的对象发生平移（向上、向下、向左或向右移动）也不会影响CNN分类器行为的函数；它对平移是不变的（[图10.3](#ch10fig03)的上面板）。
- en: 'Figure 10.3\. Invariance: Rotational invariance is a property of the function
    such that a rotation transformation of the input does not change the output of
    the function. Equivariance: Translational equivariance for a function is when
    applying the translation to the input results in the same output as when you apply
    the translation after the function has already been performed on the unaltered
    input.'
  id: totrans-1286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3. 不变性：旋转不变性是函数的一个属性，即输入的旋转变换不会改变函数的输出。等变性：对于一个函数，当将平移应用于输入时，结果与在函数已经对未改变输入执行平移之后应用平移时相同，这被称为平移等变性。
- en: '![](10fig03_alt.jpg)'
  id: totrans-1287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig03_alt.jpg)'
- en: If we use a CNN to detect the location of an object in an image, it is no longer
    invariant to translation but rather *equivariant* (bottom panel of [figure 10.3](#ch10fig03)).
    *Equivariance* is when *f*(*g*(*x*)) = *g*(*f*(*x*)), for some transformation
    function *g*. This equation says that if we take an image with a face in the center,
    apply a translation so the face is now in the top-left corner, and then run it
    through a CNN face detector, the result is the same as if we had just run the
    original centered image through the face detector and then translated the output
    to the top-left corner. The distinction is subtle, and often invariance and equivariance
    are used interchangeably since they are related.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用CNN来检测图像中物体的位置，它就不再对平移保持不变，而是**等变**（图10.3的底部面板）。**等变性**是指对于某个变换函数**g**，有**f**(g(*x*))
    = **g**(f(*x*)）。这个方程表示，如果我们取一个中心有脸的图像，应用一个平移使脸现在在左上角，然后通过CNN人脸检测器运行，结果与如果我们只是将原始居中的图像通过人脸检测器运行然后将输出平移到左上角是相同的。这种区别很微妙，通常不变性和等变性被互换使用，因为它们是相关的。
- en: Ideally, we want our neural network architectures to be invariant to many kinds
    of transformations our input data might suffer. In the case of images, we generally
    want our machine learning model to be invariant to translations, rotations, smooth
    deformations (e.g., stretching or squeezing), and to noise. CNNs are only invariant
    or equivariant to translations but are not necessarily robust against rotations
    or smooth deformations.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望我们的神经网络架构对输入数据可能遭受的许多种类的变换保持不变。在图像的情况下，我们通常希望我们的机器学习模型对平移、旋转、平滑变形（例如，拉伸或挤压）和噪声保持不变。CNNs只对平移保持不变或等变，但并不一定对旋转或平滑变形具有鲁棒性。
- en: In order to get the kind of invariance we want, we need a *relational model*—a
    model that is capable of identifying objects and relating them to one another.
    If we have an image of a cup on top of a table, and we train a CNN to identify
    the cup, it will perform well. But if we were to rotate the image 90 degrees,
    it would likely fail because it is not rotation-invariant, and our training data
    did not include rotated images. However, a (purely) relational model should, in
    principle, have no problem with this because it can learn to do relational reasoning.
    It can learn that “cups are on tables,” and this relational description does not
    depend on a particular viewing angle. Hence, machine learning models with relational
    reasoning abilities can model powerful and generic relations between objects.
    *Attention models* are one way of achieving this and the topic of this chapter.
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得我们想要的这种不变性，我们需要一个**关系模型**——一个能够识别对象并将它们相互关联的模型。如果我们有一张桌子上放着一杯咖啡的图片，并且我们训练一个CNN来识别杯子，它将表现得很好。但是如果我们把图片旋转90度，它很可能会失败，因为它不是旋转不变的，而且我们的训练数据中没有包含旋转的图片。然而，一个（纯粹地）关系模型在原则上应该没有问题，因为它可以学习进行关系推理。它可以学习到“杯子在桌子上”，这种关系描述不依赖于特定的视角。因此，具有关系推理能力的机器学习模型可以建模对象之间强大而通用的关系。**注意力模型**是实现这一目标的一种方法，也是本章的主题。
- en: 10.2\. Relational reasoning with attention
  id: totrans-1291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 带有注意力的关系推理
- en: 'There are many possible ways of implementing a relational model. We know what
    we want: a model that can learn how objects in input data are related to one another.
    We also want the model to learn higher-level features over such objects, just
    like a CNN does. We also want to maintain the composability of ordinary deep learning
    models so we can stack together multiple layers (such as CNN layers) to learn
    more and more abstract features. And perhaps most important of all, we need this
    to be computationally efficient so we can train this relational model on large
    amounts of data.'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 实现关系模型有许多可能的方法。我们知道我们想要什么：一个可以学习输入数据中对象之间关系的模型。我们还想让模型学习这些对象的高级特征，就像CNN一样。我们还想保持普通深度学习模型的组合性，这样我们就可以堆叠多个层（如CNN层）来学习更多更抽象的特征。也许最重要的是，我们需要这个模型在计算上高效，这样我们就可以在大量数据上训练这个关系模型。
- en: A generic model called *self-attention* meets all of these requirements, although
    it is less scalable than the other models we’ve looked at so far. Self-attention,
    as the name suggests, involves an attention mechanism in which the model can learn
    to attend to a subset of the input data. But before we get to self-attention,
    let’s first talk about ordinary attention.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为 *自注意力* 的一般模型满足了所有这些要求，尽管它比我们之前看到的其他模型的可扩展性要低。正如其名所示，自注意力涉及一种注意力机制，模型可以学习关注输入数据的一个子集。但在我们讨论自注意力之前，让我们先谈谈普通的注意力。
- en: 10.2.1\. Attention models
  id: totrans-1294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1\. 注意力模型
- en: Attention models are loosely inspired by human and animal forms of attention.
    With human vision, we cannot see or focus on the entire field of view in front
    of us; our eyes make saccadic (rapid, jerky) movements to scan across the field
    of view, and we can consciously decide to focus on a particularly salient area
    within our view. This allows us to focus on processing the relevant aspects of
    a scene, which is an efficient use of resources. Moreover, when we’re engaged
    in thought and reasoning, we can only attend to a few things at once. We also
    naturally tend to employ relational reasoning when we say things like “he is older
    than her” or “the door closed behind me;” we are relating the properties or behavior
    of certain objects in the world to others. Indeed, words in human language generally
    only convey meaning when related to other words. In many cases, there is no absolute
    frame of reference; we can only describe things as they relate to other things
    that we know.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模型在某种程度上受到了人类和动物注意力形式的影响。在人类视觉中，我们无法看到或关注我们面前整个视野；我们的眼睛会进行快速、跳跃的运动来扫描视野，并且我们可以有意识地决定关注视野中的某个特别显著的区域。这使我们能够关注处理场景的相关方面，这是一种有效利用资源的方式。此外，当我们进行思考和推理时，我们一次只能关注几件事情。我们还说“他比她年纪大”或“门在我身后关上了”之类的话时，我们自然会使用关系推理；我们将世界上某些对象的属性或行为与另一些对象联系起来。的确，人类语言中的词语通常只有在与其他词语相关时才有意义。在许多情况下，没有绝对的参考框架；我们只能描述事物与我们所知的其他事物之间的关系。
- en: '*Absolute* (nonrelational) attention models are designed to function like our
    eyes in that they try to learn how to extract only the relevant parts of the input
    data for efficiency and interpretability (you can see what the model is learning
    to attend to when making a decision), whereas the self-attention model we will
    build here is a way of introducing relational reasoning into the model; the goal
    is not necessarily to distill the data.'
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: '*绝对*（非关系性）的注意力模型旨在像我们的眼睛一样工作，即它们试图学习如何为了效率和可解释性（你可以在模型做出决策时看到模型正在学习关注哪些内容）仅提取输入数据的相关部分，而我们在这里将要构建的自注意力模型是一种将关系推理引入模型的方法；目标不一定是从数据中提取精华。'
- en: The simplest form of absolute attention for an image classifier would be a model
    that actively crops the image, selecting subregions from the image and only processing
    those ([figure 10.4](#ch10fig04)). The model would have to learn what to focus
    on, but this would tell us what parts of the image it is using to make its classification.
    This is difficult to implement because cropping is nondifferentiable. In order
    to crop a 28 × 28 pixel image, we would need our model to produce integer-valued
    coordinates that form the rectangular subregion to subset, but integer-valued
    functions are noncontinuous and thus nondifferentiable, meaning we can’t apply
    gradient descent-based training algorithms.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类器来说，最简单的绝对注意力形式是一个主动裁剪图像的模型，从图像中选择子区域并只处理这些子区域（[图10.4](#ch10fig04)）。模型必须学习关注什么，但这将告诉我们它使用图像的哪些部分来进行分类。这很难实现，因为裁剪是不可微分的。为了裁剪一个28
    × 28像素的图像，我们需要我们的模型产生整数值坐标，这些坐标形成一个矩形子区域以进行子集，但整数值函数是非连续的，因此不可微分，这意味着我们不能应用基于梯度的训练算法。
- en: Figure 10.4\. An example of absolute attention where a function might simply
    look at subregions of an image and only process those one at a time. This can
    significantly reduce the computational burden, since the dimensionality of each
    segment is much smaller than the whole image.
  id: totrans-1298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4. 绝对注意力的一种示例，其中函数可能只是查看图像的子区域，并且一次只处理这些子区域。这可以显著减少计算负担，因为每个段落的维度远小于整个图像。
- en: '![](10fig04.jpg)'
  id: totrans-1299
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig04.jpg)'
- en: We could train such a model using a genetic algorithm, as you learned in [chapter
    6](kindle_split_016.html#ch06), or we could use reinforcement learning. In the
    reinforcement learning case, the model would produce an integer set of coordinates,
    crop the image based on those coordinates, process the subregion, and make a classification
    decision. If it classifies correctly, it would get a positive reward, or it would
    get a negative reward for an incorrect classification. In this way, we could employ
    the REINFORCE algorithm you learned earlier to train the model to perform an otherwise
    nondifferentiable function. This procedure is described in the paper “Recurrent
    Models of Visual Attention” by Volodymyr Mnih et al. (2014). This form of attention
    is termed *hard* attention because it is nondifferentiable.
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用遗传算法来训练这样的模型，正如你在[第6章](kindle_split_016.html#ch06)中学到的，或者我们可以使用强化学习。在强化学习的情况下，模型将生成一组整数坐标，根据这些坐标裁剪图像，处理子区域，并做出分类决策。如果它分类正确，它将获得正奖励，或者如果分类错误，它将获得负奖励。这样，我们可以使用你之前学到的REINFORCE算法来训练模型执行其他情况下不可微分的函数。这一过程在Volodymyr
    Mnih等人撰写的论文“Recurrent Models of Visual Attention”（2014年）中有所描述。这种形式的注意力被称为*硬*注意力，因为它不可微分。
- en: There is also *soft* attention, which is a differentiable form of attention
    that simply applies a filter to minimize or maintain certain pixels in the image
    by multiplying each pixel in the image by a soft attention value between 0 and
    1\. The attention model can then learn to set certain pixels to 0 or maintain
    certain relevant pixels ([figure 10.5](#ch10fig05)). Since the attention values
    are real numbers and not integers, this form of attention is differentiable, but
    it loses the efficiency of a hard attention model, since it still needs to process
    the entire image rather than just a portion of it.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有*软*注意力，这是一种可微分的注意力形式，它通过将图像中的每个像素乘以介于0和1之间的软注意力值来简单地应用过滤器，以最小化或保持图像中的某些像素。注意力模型可以学习将某些像素设置为0或保持某些相关像素（[图10.5](#ch10fig05)）。由于注意力值是实数而不是整数，这种形式的注意力是可微分的，但它失去了硬注意力模型的效率，因为它仍然需要处理整个图像而不是图像的一部分。
- en: Figure 10.5\. An example of soft attention where a model would learn which pixels
    to keep and which pixels to ignore (i.e., set to 0). Unlike the hard-attention
    model, the soft-attention model needs to process the entire image at once, which
    can be computationally demanding.
  id: totrans-1302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5. 软注意力的一种示例，其中模型会学习保留哪些像素以及忽略哪些像素（即设置为0）。与硬注意力模型不同，软注意力模型需要一次性处理整个图像，这可能在计算上要求较高。
- en: '![](10fig05.jpg)'
  id: totrans-1303
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig05.jpg)'
- en: In a self-attention model (SAM), the process is quite different and more complicated.
    Remember, the output of a SAM is essentially a graph, except that each node is
    constrained to only be connected with a few other nodes (hence the “attention”
    aspect).
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力模型（SAM）中，过程相当不同且更复杂。记住，SAM的输出本质上是一个图，除了每个节点仅限于与其他少数节点连接（因此有“注意力”这一方面）。
- en: 10.2.2\. Relational reasoning
  id: totrans-1305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2. 关系推理
- en: Before we get into the details of self-attention, let’s first sketch out how
    a general *relational reasoning module* ought to work. Any machine learning model
    is typically fed some raw data in the form of a vector or higher-order tensor,
    or perhaps a sequence of such tensors, as in language models. Let’s use an example
    from language modeling, or *natural language processing* (NLP), because it is
    a bit easier to grasp than processing raw images. Let’s consider the task of translating
    a simple sentence from English into Chinese.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨自注意力机制的细节之前，让我们首先概述一下一个通用的*关系推理模块*应该如何工作。任何机器学习模型通常以向量的形式提供一些原始数据，或者更高阶的张量，或者可能是这些张量的序列，如语言模型。让我们用一个来自语言建模的例子，或者说是*自然语言处理*（NLP），因为它比处理原始图像更容易理解。让我们考虑将一个简单的英文句子翻译成中文的任务。
- en: '| English | Chinese |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '| 英语 | 中文 |'
- en: '| --- | --- |'
  id: totrans-1308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| I ate food. | ![](pg290-1.jpg) |'
  id: totrans-1309
  prefs: []
  type: TYPE_TB
  zh: '| 我吃了食物。 | ![图片](pg290-1.jpg) |'
- en: Each word, *w[i]*, in English is encoded as a fixed-length one-hot vector, ![](in290-01.jpg),
    with dimensionality *n*. The dimensionality determines the maximal vocabulary
    size. For example, if *n* = 10, the model can only handle a vocabulary of 10 words
    in total, so usually it is much larger, such as *n* ≈ 40000\. Likewise, each word
    in Chinese is encoded as a fixed-length vector. We want to build a translation
    model that can translate each word of English into Chinese.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 英语中的每个单词，*w[i]*，都被编码为一个固定长度的独热向量，![图片](in290-01.jpg)，其维度为*n*。维度决定了最大词汇量。例如，如果*n*
    = 10，则模型总共只能处理10个单词的词汇量，所以通常它要大得多，例如*n* ≈ 40000。同样，中文中的每个单词也被编码为固定长度的向量。我们希望构建一个可以将每个英语单词翻译成中文的翻译模型。
- en: The first approaches to this problem were based on recurrent neural networks,
    which are inherently sequential models, as they are capable of storing data from
    each input. A recurrent neural network, at a high level, is a function that maintains
    an internal state that is updated with each input that it sees ([figure 10.6](#ch10fig06)).
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最初方法是基于循环神经网络，它们本质上是有序模型，因为它们能够存储每个输入的数据。在高级别上，循环神经网络是一个函数，它维护一个内部状态，该状态会随着它看到的每个输入而更新（[图10.6](#ch10fig06)）。
- en: Figure 10.6\. A recurrent neural network (RNN) is capable of maintaining an
    internal state that is updated with each new input it receives. This allows RNNs
    to model sequential data such as time series or language.
  id: totrans-1312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6。循环神经网络（RNN）能够维护一个内部状态，该状态会随着每个新输入的接收而更新。这使得RNN能够模拟序列数据，如时间序列或语言。
- en: '![](10fig06.jpg)'
  id: totrans-1313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig06.jpg)'
- en: Most RNN language models work by first having an encoder model that consumes
    a single English word at a time, and once done gives its internal state vector
    to a different decoder RNN that outputs individual Chinese words one at a time.
    The problem with RNNs is that they are not easily parallelized because you must
    maintain an internal state, which depends on the sequence length ([figure 10.7](#ch10fig07)).
    If sequence lengths vary across inputs and outputs, you have to synchronize all
    the sequences until they’re done processing.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RNN语言模型通过首先使用一个编码器模型来处理单个英语单词，一旦完成，就将其内部状态向量发送给不同的解码器RNN，解码器RNN逐个输出目标句子中的单词，直到它停止。RNN的问题在于它们不容易并行化，因为你必须维护一个内部状态，这取决于序列长度（[图10.7](#ch10fig07)）。如果输入和输出的序列长度不同，你必须同步所有序列，直到它们完成处理。
- en: Figure 10.7\. Schematic of an RNN language model. Two separate RNNs are used,
    an encoder and a decoder. The encoder takes an input sentence word by word and,
    once complete, sends its internal state to the decoder RNN, which produces each
    word in the target sentence until it halts.
  id: totrans-1315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7。RNN语言模型的示意图。使用两个独立的RNN，一个编码器和一个解码器。编码器逐个处理输入句子中的单词，一旦完成，就将它的内部状态发送给解码器RNN，解码器RNN产生目标句子中的每个单词，直到它停止。
- en: '![](10fig07_alt.jpg)'
  id: totrans-1316
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig07_alt.jpg)'
- en: While many thought that language models needed recurrence to work well, given
    the natural sequential nature of language, researchers found that a relatively
    simple attention model with no recurrence at all could perform even better and
    is trivially parallelizable, making it easier to train faster and with more data.
    These are the so-called *transformer models*, which rely on self-attention. We
    will not get into their details—we’ll just sketch out the basic mechanism here.
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多人认为语言模型需要递归才能很好地工作，因为语言具有自然的序列性质，但研究人员发现，一个相对简单的没有递归的注意力模型甚至可以表现得更好，并且可以轻易地并行化，这使得训练更快，并且可以使用更多的数据。这些就是所谓的*转换器模型*，它们依赖于自注意力。我们不会深入探讨它们的细节——我们在这里只是概述基本机制。
- en: The idea is that a Chinese word, *c[i]*, can be translated as a function of
    the weighted combination of a context of English words, *e[i]*. The context is
    simply a fixed-length collection of words that are in proximity to a given English
    word. Given the sentence “My dog Max chased a squirrel up the tree and barked
    at it,” a context of three words for the word “squirrel” would be the subsentence
    “Max chased a squirrel up the tree” (i.e., we include the three words on either
    side of the target word).
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是，一个中文词，*c[i]*，可以翻译为英文词的加权组合的上下文，*e[i]*的函数。上下文简单地是一个固定长度的单词集合，这些单词与给定的英文词相邻。给定句子“我的狗Max追逐了一只松鼠上树，并对它吠叫”，对于单词“松鼠”的上下文将是子句“Max追逐了一只松鼠上树”（即，我们包括目标词两侧的三个词）。
- en: 'For the English phrase “I ate food” in [figure 10.7](#ch10fig07), we would
    use all three words. The first Chinese word would be produced by taking a weighted
    sum of all the English words in the sentence: *c[i]* = *f*(Σ*a[i]* · *[e[i]]*),
    where *a[i]* is the (attention) weight, which is a number between 0 and 1 such
    that Σ*a[i]* = 1\. The function *f* would be a neural network, such as a simple
    feedforward neural network. The function as a whole would need to learn the neural
    network weights in *f* as well as the attention weights, *a[i]*. The attention
    weights would be produced by some other neural network function.'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[图10.7](#ch10fig07)中的英文短语“我吃了食物”，我们会使用所有三个词。第一个中文词将通过取句子中所有英文词的加权总和来生成：*c[i]*
    = *f*(Σ*a[i]* · *[e[i]]*), 其中 *a[i]* 是（注意力）权重，它是一个介于0和1之间的数字，使得 Σ*a[i]* = 1。函数
    *f* 将是一个神经网络，例如一个简单的前馈神经网络。整个函数需要学习 *f* 中的神经网络权重以及注意力权重，*a[i]*。注意力权重将由某个其他神经网络函数产生。
- en: After successful training, we can inspect these attention weights and see which
    English words are attended to when translating to a given Chinese word. For example,
    when producing the Chinese word ![](pg290-2.jpg), the English word “I” would have
    a high attention weight associated with it, whereas the other words would be mostly
    ignored.
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功训练后，我们可以检查这些注意力权重，并查看在翻译为给定的中文词时，哪些英文词被关注。例如，当生成中文词 ![](pg290-2.jpg) 时，英文词“我”将与一个高注意力权重相关联，而其他词则大部分被忽略。
- en: 'This general procedure is called *kernel regression*. To take an even simpler
    example, let’s say we have a data set that looks like [figure 10.8](#ch10fig08),
    and we want to make a machine learning model that can take an unseen *x* and predict
    an appropriate *y*, given this training data. There are two broad classes of how
    to do this: *nonparametric* and *parametric* methods.'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个一般过程被称为 *核回归*。为了举一个更简单的例子，假设我们有一个看起来像[图10.8](#ch10fig08)的数据集，我们想要构建一个机器学习模型，该模型可以接受未见的
    *x* 并根据这个训练数据预测适当的 *y*。有两种广泛的方法来做这件事：*非参数* 和 *参数* 方法。
- en: Figure 10.8\. Scatter plot of a nonlinear data set on which we might want to
    train a regression algorithm.
  id: totrans-1322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8. 我们可能想要在上面训练回归算法的非线性数据集的散点图。
- en: '![](10fig08_alt.jpg)'
  id: totrans-1323
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig08_alt.jpg)'
- en: Neural networks are *parametric models* because they have a fixed set of adjustable
    parameters. A simple polynomial function like *f*(*x*) = *ax*³ + *bx*² + *c* is
    a parametric model because we have three parameters (*a*,*b*,*c*) that we can
    train to fit this function to some data.
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是 *参数模型*，因为它们有一个固定的可调整参数集。一个简单的多项式函数，如 *f*(*x*) = *ax*³ + *bx*² + *c*，是一个参数模型，因为我们有三个参数
    (*a*,*b*,*c*) 可以训练以适应这个函数。
- en: A *nonparametric model* is a model that either has no trainable parameters or
    has the ability to dynamically adjust the number of parameters it has, based on
    the training data. Kernel regression is an example of a nonparametric model for
    prediction; the simplest version of kernel regression is to simply find the nearest
    *x[i]* points in the training data, *X*, to some new input, *x*, and then return
    the corresponding *y* ∈ *Y* in the training data that is the average ([figure
    10.9](#ch10fig09)).
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *非参数模型* 是一个要么没有可训练参数，要么能够根据训练数据动态调整其参数数量的模型。核回归是预测的非参数模型的一个例子；核回归的最简单版本是简单地找到训练数据中，*X*
    中与某个新输入，*x* 最接近的 *x[i]* 点，然后返回训练数据中相应的 *y* ∈ *Y*，它是平均值 ([图10.9](#ch10fig09))。
- en: Figure 10.9\. One way to perform nonparametric kernel regression to predict
    the y component of a new x value is to find the most similar (i.e., the closest)
    x’s in the training data and then take the average of their respective y components.
  id: totrans-1326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9. 预测新x值的y分量的一种非参数核回归方法是找到训练数据中最相似的（即最接近的）x值，然后取它们各自的y分量的平均值。
- en: '![](10fig09_alt.jpg)'
  id: totrans-1327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig09_alt.jpg)'
- en: 'In this case, however, we have to choose how many points qualify as being the
    nearest neighbors to the input *x*, and it is problematic since all of these nearest
    neighbors contribute equally to the outcome. Ideally, we could weight (or attend
    to) all the points in the data set according to how similar they are to the input,
    and then take the weighted sum of their corresponding *y[i]* to make a prediction.
    We’d need some function, *f*:*X* → *A*: a function that takes an input *x* ∈ *X*
    and returns a set of attention weights *a* ∈ *A* that we could use to perform
    this weighted sum. This procedure is essentially exactly what we’ll do in attention
    models, except that the difficulty lies in deciding how to efficiently compute
    the attention weights.'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们必须选择有多少个点可以被认为是输入 *x* 的最近邻，这是一个问题，因为所有这些最近邻对结果贡献是相等的。理想情况下，我们可以根据它们与输入的相似程度对数据集中的所有点进行加权（或关注），然后取它们相应的
    *y[i]* 的加权总和来进行预测。我们需要一个函数，*f*:*X* → *A*：一个函数，它接受一个输入 *x* ∈ *X* 并返回一个我们可以用来执行此加权总和的注意力权重集
    *a* ∈ *A*。这个程序本质上就是我们将在注意力模型中做的事情，只是困难在于决定如何有效地计算注意力权重。
- en: In general, a *self-attention model* seeks to take a collection of objects and
    learn how each of those objects is related to the other objects via attention
    weights. In graph theory, a graph is a data structure, *G* = (*N*,*E*), i.e.,
    a collection of nodes, *N*, and edges (connections or relations) between nodes,
    *E*. The collection, *N*, might just be a set of node labels such as {0,1,2,3,...},
    or each node might contain data, and thus each node might be represented by some
    feature vector. In the latter case, we can store our collection of nodes as a
    matrix *N*:![](icon-r.jpg)*^n*^×*^f*, where *f* is the feature dimension, such
    that each row is a feature vector for that node.
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个 *自注意力模型*试图通过注意力权重学习一组对象之间的关系。在图论中，一个图是一个数据结构，*G* = (*N*,*E*)，即节点集合，*N*，以及节点之间的边（连接或关系），*E*。集合，*N*，可能只是一个节点标签的集合，如{0,1,2,3,...}，或者每个节点可能包含数据，因此每个节点可能由某个特征向量表示。在后一种情况下，我们可以将我们的节点集合存储为一个矩阵
    *N*：![](icon-r.jpg)*^n*^×*^f*，其中 *f* 是特征维度，这样每一行都是该节点的特征向量。
- en: The collection of edges, *E*, can be represented by an *adjacency matrix, E*:![](icon-r.jpg)*^n*^×*^n*,
    where each row and column are nodes, such that a particular value in row 2, column
    3 represents the strength of the relationship between node 2 and node 3 (right
    panel of [figure 10.10](#ch10fig10)). This is the very basic setup for a graph,
    but graphs can get more complicated where even the edges have feature vectors
    associated with them. We will not attempt that here.
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 边的集合，*E*，可以用一个邻接矩阵，E*：![](icon-r.jpg)*^n*^×*^n*，来表示，其中每一行和每一列都是节点，这样特定的行2，列3中的值就代表了节点2和节点3之间关系的强度（[图10.10](#ch10fig10)的右侧面板）。这是图的基本设置，但图可以变得更加复杂，甚至边都有与它们关联的特征向量。我们在这里不会尝试那样做。
- en: Figure 10.10\. The graph structure on the left can be represented quantitatively
    with a node feature matrix that encodes the individual node features and an adjacency
    matrix that encodes the edges (i.e., connections or arrows) between nodes. A 1
    in the a row in the b column indicates that node a has an edge from a to b. The
    node features could be something like an RGBA value if the nodes represented pixels.
  id: totrans-1331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.10。左边的图结构可以用一个节点特征矩阵来定量表示，该矩阵编码了单个节点特征，以及一个邻接矩阵来编码节点之间的边（即连接或箭头）。在b列的a行中的1表示节点a有一个从a到b的边。节点特征可能是RGBA值，如果节点代表像素的话。
- en: '![](10fig10_alt.jpg)'
  id: totrans-1332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig10_alt.jpg)'
- en: A self-attention model works by starting with a set of nodes, *N*:![](icon-r.jpg)*^n*^×*^f*,
    and then computes the attention weights between all pairs of nodes. In effect,
    it creates an edge matrix *E*:![](icon-r.jpg)*^n*^×*^n*. After creating the edge
    matrix, it will update the node features such that each node sort of gets blended
    together with the other nodes that it attends to. In a sense, each node sends
    a message to the other nodes to which it most strongly attends, and when nodes
    receive messages from other nodes, they update themselves. We call this one-step
    process a *relational module*, after which we get an updated node matrix, *N*:![](icon-r.jpg)*^n*^×*^f*,
    that we can pass on to another relational module that will do the same thing ([figure
    10.11](#ch10fig11)). By inspecting the edge matrix, we can see which nodes are
    attending to which other nodes, and it gives us an idea of the reasoning of the
    neural network.
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模型通过从一个节点集合开始，*N*:![](icon-r.jpg)*^n*^×*^f*，然后计算所有节点对之间的注意力权重。实际上，它创建了一个边矩阵
    *E*:![](icon-r.jpg)*^n*^×*^n*。在创建边矩阵后，它将更新节点特征，使得每个节点似乎与其他它关注的节点混合在一起。从某种意义上说，每个节点向它最关注的其他节点发送消息，当节点从其他节点接收消息时，它们会更新自己。我们将这一步过程称为*关系模块*，之后我们得到一个更新的节点矩阵，*N*:![](icon-r.jpg)*^n*^×*^f*，我们可以将其传递给另一个关系模块，该模块将执行相同的事情（[图10.11](#ch10fig11)）。通过检查边矩阵，我们可以看到哪些节点在关注哪些其他节点，这给我们提供了神经网络推理的线索。
- en: Figure 10.11\. A relational module, at the highest level, processes a node matrix,
    ![](pg302-1.jpg), and outputs a new, updated node matrix, ![](pg302-2.jpg), where
    the dimensionality of the node feature may be different.
  id: totrans-1334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11. 在最高级别上，关系模块处理节点矩阵，![](pg302-1.jpg)，并输出一个新的、更新的节点矩阵，![](pg302-2.jpg)，其中节点特征的维度可能不同。
- en: '![](10fig11.jpg)'
  id: totrans-1335
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig11.jpg)'
- en: In a *self-attention language model*, each word from one language is attending
    to all the words in context of the other language, but the attention weights (or
    edges) represent to what degree each word is attending to (i.e., related to) each
    other word. Hence, a self-attention language model can reveal the meaning of a
    translated Chinese word with respect to the words in an English sentence. For
    example, the Chinese word ![](pg290-3.jpg) means “eat,” so this Chinese word would
    have a large attention weight to “eat” but would only weakly be attending to the
    other words.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个*自注意力语言模型*中，一种语言中的每个词都在关注另一种语言上下文中的所有词，但注意力权重（或边）表示每个词关注（即与）其他词的程度。因此，自注意力语言模型可以揭示翻译的中文词相对于英语句子中词的意义。例如，中文词![](pg290-3.jpg)的意思是“吃”，所以这个中文词会对“吃”有较大的注意力权重，但只会对其他词有微弱的关注。
- en: Self-attention makes more intuitive sense when used in language models, but
    in this book we’ve mostly dealt with machine learning models that operate on visual
    data, such as pixels from a video frame. Visual data, however, is not naturally
    structured as a collection of objects or nodes that we can directly pass into
    a relational module. We need a way of turning a bunch of pixels into a set of
    objects. One way to do it would be to simply call each individual pixel an object.
    To make things more computationally efficient, and to be able to process the image
    into more meaningful objects, we can first pass the raw image through a few convolutional
    layers that will return a tensor with dimensions (*C*,*H*,*W*) for channels, height,
    and width. In this way, we can define the objects in the convolved image as vectors
    across the channel dimension, i.e., each object is a vector of dimension *C*,
    and there will be *N* = *H* * *W* number of objects ([figure 10.12](#ch10fig12)).
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力在语言模型中使用时更有直观意义，但在这本书中，我们主要处理的是在视觉数据上操作的机器学习模型，例如视频帧中的像素。然而，视觉数据并不是自然地结构化为我们可以直接传递到关系模块中的对象或节点集合。我们需要一种方法将大量像素转换为一组对象。一种方法是将每个单独的像素称为一个对象。为了提高计算效率，并能够将图像处理成更有意义的对象，我们首先可以将原始图像通过几个卷积层，这将返回一个具有(*C*,*H*,*W*)维度的张量，用于通道、高度和宽度。这样，我们可以将卷积图像中的对象定义为跨通道维度的向量，即每个对象是一个维度为
    *C* 的向量，将有 *N* = *H* * *W* 个对象（[图10.12](#ch10fig12)）。
- en: Figure 10.12\. A convolutional layer returns a series of convolutional “filters”
    stored in a 3-tensor with shape channels (i.e., the number of filters) by height
    by width. We can turn this into a set of nodes by taking slices along the channel
    dimension where each node is then a channel-length vector, for a total of height
    times width number of nodes. We package these into a new matrix of dimension N
    × C, where N is the number of nodes and C is the channel dimension.
  id: totrans-1338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.12. 卷积层返回一系列存储在形状为通道（即，滤波器的数量）× 高度 × 宽度的3张量中的卷积“滤波器”。我们可以通过沿通道维度切片来将其转换为节点集，其中每个节点是一个通道长度向量，总共有高度乘以宽度的节点数。我们将这些节点打包到一个新的N
    × C维度的矩阵中，其中N是节点数，C是通道维度。
- en: '![](10fig12_alt.jpg)'
  id: totrans-1339
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig12_alt.jpg)'
- en: After a raw image has been processed through a few trained CNN layers, we would
    expect that each position in the feature maps corresponds to particular salient
    features in the underlying image. For example, we hope the CNNs might learn to
    detect objects in the image that we can then pass into our relational module to
    process relations between objects. Each convolutional filter learns a particular
    feature for each spatial position, so taking all these learned features for a
    particular (*x*,*y*) grid position in an image yields a single vector for that
    position that encodes all the learned features. We can do this for all the grid
    positions to collect a set of putative objects in the image, which we can represent
    as nodes in a graph, except that we do not know the connectivity between the nodes
    yet. That is what our relational reasoning module will attempt to do.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始图像经过几个训练好的CNN层处理后，我们预计特征图中的每个位置都对应于底层图像中的特定显著特征。例如，我们希望CNN能够学会检测图像中的对象，然后我们可以将其传递到我们的关系模块中处理对象之间的关系。每个卷积滤波器学习每个空间位置的一个特定特征，因此对于图像中特定(*x*,
    *y*)网格位置的所有这些学习到的特征，我们得到一个编码了所有学习特征的单一向量。我们可以对所有网格位置执行此操作，收集图像中一组假定的对象，我们可以将这些对象表示为图中的节点，但我们还不知道节点之间的连接性。这正是我们的关系推理模块将尝试做的事情。
- en: 10.2.3\. Self-attention models
  id: totrans-1341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.3. 自注意力模型
- en: There are many possible ways to build a relational module, but as we’ve discussed,
    we will implement one based on a self-attention mechanism. We have described the
    idea at a high level, but it is time we got into the details of implementation.
    The model we’ll build is based on the one described in the paper “Deep reinforcement
    learning with relational inductive biases” by Vinicius Zambaldi et al. (2019)
    from DeepMind.
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: 建立关系模块有许多可能的方法，但正如我们讨论的，我们将基于自注意力机制实现一个。我们已经从高层次描述了这个想法，但现在是我们深入了解实现细节的时候了。我们将构建的模型基于DeepMind的Vinicius
    Zambaldi等人（2019年）在论文“具有关系归纳偏好的深度强化学习”中描述的模型。
- en: We already discussed the basic framework of a node matrix *N*:![](icon-r.jpg)*^n*^×*^f*
    and an edge matrix *E*:![](icon-r.jpg)*^n*^×*^n*, and we discussed the need to
    process a raw image into a node matrix. Just like with kernel regression, we need
    some way of computing the distance (or inversely, the similarity) between two
    nodes. There is no single option for this, but one common approach is to simply
    take the *inner product* (also called *dot product*) between the two nodes’ feature
    vectors as their similarity.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了节点矩阵 *N*:![图标-r](icon-r.jpg)*^n*^×*^f* 和边矩阵 *E*:![图标-r](icon-r.jpg)*^n*^×*^n*
    的基本框架，并讨论了将原始图像处理成节点矩阵的需要。就像核回归一样，我们需要一种方法来计算两个节点之间的距离（或相反，相似度）。没有单一的选择，但一种常见的方法是简单地取两个节点特征向量的**内积**（也称为**点积**）作为它们的相似度。
- en: The dot product between two equal-length vectors is computed by multiplying
    corresponding elements in each vector and then summing the result. For example,
    the inner product between vectors *a* = (1,–2,3) and *b* = (–1,5,–2) is denoted
    <*a*,*b*> and is calculated as <*a*,*b*> = S*a[i]b[i]*, which in this case is
    1 × –1 + –2 × 5 + 3 × –2 = –1 – 10 – 6 = –17\. The sign of each element in *a*
    and *b* are opposite, so the resulting inner product is a negative number indicating
    strong disagreement between the vectors. In contrast, if *a* = (1,–2,3), *b* =
    (2,–3,2) then <*a*,*b*> = 14, which is a big positive number, since the two vectors
    are more similar element by element. Hence, the dot product gives us an easy way
    to compute the similarity between a pair of vectors, such as nodes in our node
    matrix. This approach leads to what is called (scaled) *dot product attention*;
    the scaled part will come into play later.
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 两个等长向量的点积是通过将每个向量中对应元素相乘，然后将结果相加来计算的。例如，向量 *a* = (1,–2,3) 和 *b* = (–1,5,–2)
    的内积表示为 <*a*,*b*>，并计算为 <*a*,*b*> = S*a[i]b[i]*，在这个例子中是 1 × –1 + –2 × 5 + 3 × –2
    = –1 – 10 – 6 = –17。*a* 和 *b* 中每个元素的符号是相反的，所以结果的内积是一个负数，表示这两个向量之间存在强烈的分歧。相比之下，如果
    *a* = (1,–2,3)，*b* = (2,–3,2)，那么 <*a*,*b*> = 14，这是一个很大的正数，因为这两个向量在元素上更相似。因此，点积为我们提供了一种简单的方法来计算一对向量之间的相似度，例如我们节点矩阵中的节点。这种方法导致所谓的（缩放）*点积注意力*；缩放部分将在稍后发挥作用。
- en: Once we have our initial set of nodes in the node matrix *N*, we will project
    this matrix into three new separate node matrices that are referred to as *keys,
    queries*, and *values*. With the kernel regression example, the query is the new
    *x* for which we want to predict the corresponding *y*, which is the value. The
    query is *x*, the *y* is the value. In order to find the value, we must locate
    the nearest *x[i]* in the training data, which is the key. We measure the similarity
    between the query and the keys, find the keys that are most similar to the query,
    and then return the average value for that set of keys.
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在节点矩阵 *N* 中有了初始节点集，我们将把这个矩阵投影到三个新的单独的节点矩阵中，这些矩阵被称为 *keys, queries* 和 *values*。在核回归的例子中，查询是我们想要预测对应
    *y* 值的新 *x*，这个值就是我们要找的。查询是 *x*，*y* 是值。为了找到这个值，我们必须在训练数据中找到最近的 *x[i]*，这就是关键。我们测量查询和关键之间的相似度，找到与查询最相似的关键，然后返回这些关键的平均值。
- en: This is exactly what we will do in self-attention, except that the queries,
    keys, and values will all come from the same origin. We multiply the original
    node matrix by three separate projection matrices to produce a query matrix, a
    key matrix, and a value matrix. The projection matrices will be learned during
    training just like any other parameters in the model. During training, the projection
    matrices will learn how to produce queries, keys, and values that will lead to
    optimal attention weights ([figure 10.13](#ch10fig13)).
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在自注意力中将要做的，只不过查询、关键和值都将来自同一个来源。我们将原始节点矩阵乘以三个不同的投影矩阵，以产生查询矩阵、关键矩阵和值矩阵。投影矩阵将在训练过程中学习，就像模型中的任何其他参数一样。在训练过程中，投影矩阵将学习如何产生查询、关键和值，这将导致最优的注意力权重（[图10.13](#ch10fig13)）。
- en: 'Figure 10.13\. A high-level view of a self-attention-based relational module.
    The input to a relational module is a node matrix N: ..n×f with n nodes each with
    an f-dimensional feature vector. The relational module then copies this matrix
    for a total of three copies, and projects each one into a new matrix via a simple
    linear layer without an activation function, creating separate query, key, and
    value matrices. The query and key matrices are input to a compatibility function,
    which is any function that computes how compatible (similar in some way) each
    node is to each other node, resulting in a set of unnormalized attention weights,
    ![](pg296.jpg). This matrix is then normalized via the softmax function across
    the rows, such that each row’s values will sum to 1\. The value matrix and normalized
    attention matrix are then multiplied, ![](pg296a.jpg). The output of the relational
    module is then usually passed through one or more linear layers (not depicted).'
  id: totrans-1347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.13. 基于自注意力关系模块的高级视图。关系模块的输入是一个节点矩阵N：..n×f，其中n个节点每个都有一个f维特征向量。关系模块随后将这个矩阵复制三次，并通过一个没有激活函数的简单线性层将每个矩阵投影到一个新的矩阵中，从而创建独立的查询、键和值矩阵。查询和键矩阵输入到一个兼容性函数中，该函数是任何计算每个节点与其他节点兼容性（以某种方式相似）的函数，从而得到一组未归一化的注意力权重，![](pg296.jpg)。然后通过softmax函数对矩阵的行进行归一化，使得每一行的值之和为1。然后，值矩阵和归一化注意力矩阵相乘，![](pg296a.jpg)。关系模块的输出通常通过一个或多个线性层（未展示）。
- en: '![](10fig13_alt.jpg)'
  id: totrans-1348
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig13_alt.jpg)'
- en: Let’s take a single pair of nodes to make this concrete. Say we have a node
    (which is a feature vector), *a*:![](icon-r.jpg)^(10), and another node, *b*:![](icon-r.jpg)^(10).
    To calculate the self-attention of these two nodes, we first will project these
    nodes into a new space by multiplying by some projection matrices, i.e., *a[Q]
    = a^TQ*, *a[K] = a^TK*, *a[V] = a^TV*, where the superscript *T* indicates transposition,
    such that the node vector is now a column vector, e.g., *a^T*:![](icon-r.jpg)¹^×^(10),
    and the corresponding matrix is *Q*:![](icon-r.jpg)^(10)^×*^d*, such that *a[Q]=
    a^TQ*:![](icon-r.jpg)*^d*. We now have three new versions of *a* that may be of
    some different dimensionality from the input, e.g., *a[Q]*,*a[K]*,*a[V]*:![](icon-r.jpg)^(20).
    We do the same for the node *b*. We can calculate how related *a* is to itself
    first by multiplying (via the inner product) its query and key together. Remember,
    we compute *all* pairwise interactions between nodes, including self-interactions.
    Unsurprisingly, objects are likely to be related to themselves, although not necessarily,
    since the corresponding queries and keys (after projection) may be different.
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一对节点为例来具体说明。假设我们有一个节点（它是一个特征向量），*a*:![](icon-r.jpg)^(10)，以及另一个节点，*b*:![](icon-r.jpg)^(10)。为了计算这两个节点的自注意力，我们首先通过一些投影矩阵将这些节点投影到一个新的空间中，即*a[Q]
    = a^TQ*，*a[K] = a^TK*，*a[V] = a^TV*，其中上标*T*表示转置，使得节点向量现在是一个列向量，例如*a^T*:![](icon-r.jpg)¹^×^(10)，相应的矩阵是*Q*:![](icon-r.jpg)^(10)^×*^d*，使得*a[Q]=
    a^TQ*:![](icon-r.jpg)*^d*。我们现在有了三个新的*a*版本，它们可能具有与输入不同的维度，例如*a[Q]*，*a[K]*，*a[V]*:![](icon-r.jpg)^(20)。我们对节点*b*也做同样的处理。我们可以通过（通过内积）将查询和键相乘来首先计算*a*与自身的相关性。记住，我们计算节点之间的所有成对交互，包括自交互。不出所料，对象很可能与自身相关，尽管不一定，因为相应的查询和键（在投影后）可能不同。
- en: After we multiply the query and key together for object *a*, we get an unnormalized
    attention weight, *w[a,a]* = <*a[Q]*,*a[K]*>, which is a single scalar value for
    the self-attention between *a* and *a* (itself). We then do the same for the pairwise
    interaction between *a* and *b*, and *b* and *a*, and *b* and *b*, so we get a
    total of four attention weights. These could be arbitrarily small or large numbers,
    so we normalize all the attention weights using the softmax function, which as
    you may recall, takes a bunch of numbers (or a vector) and normalizes all the
    values to be in the interval [0,1] and forces them to sum to 1 so that they form
    a proper discrete probability distribution. This normalization forces the attention
    mechanism to only attend to what is absolutely necessary for the task. Without
    this normalization, the model could easily attend to everything, and it would
    remain un-interpretable.
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将查询和键值相乘以获取对象 *a* 之后，我们得到一个未归一化的注意力权重，*w[a,a]* = <*a[Q]*,*a[K]*>，这是一个表示 *a*
    与 *a*（自身）之间的自注意力的单个标量值。然后我们对 *a* 和 *b*、*b* 和 *a*、*b* 和 *b* 之间的成对交互做同样的处理，因此我们得到总共四个注意力权重。这些可能是不规则的小或大数字，因此我们使用
    softmax 函数对所有注意力权重进行归一化，你可能还记得，这个函数接受一组数字（或向量）并将所有值归一化到 [0,1] 区间内，并使它们之和为 1，从而形成一个合适的离散概率分布。这种归一化迫使注意力机制只关注任务绝对必要的部分。没有这种归一化，模型可能会轻易地关注一切，这将使其难以解释。
- en: Once we have the normalized attention weights, we can collect them into an attention
    weight matrix. In our simple example with two objects, *a* and *b*, this would
    be a 2 × 2 matrix. We can then multiply the attention matrix by each value vector,
    which will increase or decrease the elements in each vector value according to
    the attention weights. This will give us a set of new and updated node vectors.
    Each node has been updated based on the strength of its relationships to other
    nodes.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了归一化的注意力权重，我们可以将它们收集到一个注意力权重矩阵中。在我们的简单示例中，有两个对象 *a* 和 *b*，这将是一个 2 × 2 的矩阵。然后我们可以将注意力矩阵与每个值向量相乘，这将根据注意力权重增加或减少每个向量中的元素。这将给我们一组新的和更新的节点向量。每个节点都是基于其与其他节点的关联强度进行更新的。
- en: Rather than multiplying individual vectors together one at a time, we can instead
    multiply entire node matrices together. Indeed, we can efficiently combine the
    three steps of key-query multiplication (to form an attention matrix), then attention-matrix
    with value-matrix multiplication, and finally normalization, into an efficient
    matrix multiplication,
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不是逐个相乘向量，而是将整个节点矩阵相乘。实际上，我们可以有效地将键查询乘法（形成注意力矩阵）、注意力矩阵与值矩阵乘法以及最终归一化的三个步骤结合起来，进行高效的矩阵乘法。
- en: '![](pg297.jpg)'
  id: totrans-1353
  prefs: []
  type: TYPE_IMG
  zh: '![](pg297.jpg)'
- en: where ![](pg297a.jpg), where *n* is the number of nodes, *f* is the dimension
    of the node feature vector, *Q* is the query matrix, *K* is the key matrix, and
    *V* is the value matrix.
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](pg297a.jpg)，其中 *n* 是节点的数量，*f* 是节点特征向量的维度，*Q* 是查询矩阵，*K* 是键矩阵，*V* 是值矩阵。
- en: You can see that the result of *QK^T* will be a *n* × *n* dimensional matrix,
    which is an adjacency matrix as we described earlier, but in this context we call
    it the attention (weight) matrix. Each row and column represent a node. If the
    value in row 0 and column 1 is high, we know that node 0 attends strongly to node
    1\. The normalized attention (i.e., adjacency) weight matrix *A* = *softmax*(*QK^T*):![](icon-r.jpg)*^n*^×*^n*
    tells us all the pairwise interactions between nodes. We then multiply this by
    the value matrix, which will update each node’s feature vector according to its
    interactions with other nodes, such that the final result is an updated node matrix,
    ![](pg298.jpg). We can then pass this updated node matrix through a linear layer
    to do additional learning over the node features and apply a nonlinearity to model
    more complex features. We call this whole procedure a *relational module* or *relational
    block*. We can stack these relational modules sequentially to learn higher order
    and more complex relations.
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，*QK^T* 的结果将是一个 *n* × *n* 维的矩阵，这就像我们之前描述的邻接矩阵，但在这个上下文中我们称之为注意力（权重）矩阵。每一行和每一列代表一个节点。如果第
    0 行和第 1 列的值很高，我们知道节点 0 强烈关注节点 1。归一化的注意力（即邻接）权重矩阵 *A* = *softmax*(*QK^T*)：![](icon-r.jpg)*^n*^×*^n*
    告诉我们节点之间的所有成对交互。然后我们将其与值矩阵相乘，这将根据节点与其他节点的交互更新每个节点的特征向量，使得最终结果是更新后的节点矩阵，![](pg298.jpg)。然后我们可以将这个更新后的节点矩阵通过一个线性层，以对节点特征进行额外的学习，并应用非线性函数来模拟更复杂的功能。我们称这个过程为
    *关系模块* 或 *关系块*。我们可以按顺序堆叠这些关系模块来学习更高阶和更复杂的关联。
- en: In most cases, the final output of our neural network model needs to be a small
    vector, such as for Q values in DQN. After we’ve processed the input through 1
    or more relational modules, we can reduce the matrix down to a vector by either
    doing a MaxPool operation or an AvgPool operation. For a node matrix ![](pg298.jpg),
    either of these pooling operations applied over the *n* dimension would result
    in a *f*-dimensional vector. MaxPool just takes the maximum value along the *n*
    dimension. We can then run this pooled vector through one or more linear layers
    before returning the final result as our Q values.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们神经网络模型的最终输出需要是一个小向量，例如在 DQN 中的 Q 值。在通过一个或多个关系模块处理输入之后，我们可以通过执行 MaxPool
    操作或 AvgPool 操作将矩阵降低到向量。对于一个节点矩阵 ![](pg298.jpg)，在这 *n* 维上应用这两种池化操作中的任何一种都会得到一个
    *f*-维向量。MaxPool 只取 *n* 维上的最大值。然后我们可以运行这个池化向量通过一个或多个线性层，在返回最终结果作为我们的 Q 值之前。
- en: 10.3\. Implementing self-attention for MNIST
  id: totrans-1357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 实现用于 MNIST 的自注意力
- en: Before we delve into the difficulties of reinforcement learning, let’s try building
    a simple self-attention network to classify MNIST digits. The famous MNIST data
    set is 60,000 hand-drawn images of digits, where each image is 28 × 28 pixels
    in grayscale. The images are labeled according to the digit that is depicted.
    The goal is to train a machine learning model to accurately classify the digits.
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨强化学习的困难之前，让我们尝试构建一个简单的自注意力网络来分类 MNIST 数字。著名的 MNIST 数据集是 60,000 张手绘数字图像，每个图像是
    28 × 28 像素的灰度图。图像根据所绘制的数字进行标记。目标是训练一个机器学习模型，以准确分类数字。
- en: This data set is very easy to learn, even with a simple one-layer neural network
    (a linear model). A multilayer CNN can achieve in the 99% accuracy range. While
    easy, it is a great data set to use as a “sanity check,” just to make sure your
    algorithm can learn anything at all.
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集很容易学习，即使是一个简单的单层神经网络（线性模型）也可以。多层 CNN 可以达到 99% 的准确率。虽然容易，但它是一个很好的数据集，可以用作“理智检查”，只是为了确保你的算法可以学习任何东西。
- en: We will first test out our self-attention model on MNIST, but we ultimately
    plan to use it as our deep Q-network in game playing, so the only difference between
    a DQN and an image classifier is that the dimensionality of the inputs and outputs
    will be different—everything in between can remain the same.
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在 MNIST 上测试我们的自注意力模型，但我们的最终计划是将其用作游戏中的深度 Q 网络，因此 DQN 和图像分类器之间的唯一区别是输入和输出的维度将不同——中间的一切都可以保持不变。
- en: 10.3.1\. Transformed MNIST
  id: totrans-1361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1\. 转换后的 MNIST
- en: Before we build the model itself, we need to prepare the data and create some
    functions to preprocess the data so that it is in the right form for our model.
    For one, the raw MNIST images are grayscale pixel arrays with values from 0 to
    255, so we need to normalize those values to be between 0 and 1, or the gradients
    during training will be too variable and training will be unstable. Because MNIST
    is so easy, we can also strain our model a bit more by adding noise and perturbing
    the images randomly (e.g., random translations and rotations). This will also
    allow us to assess translational and rotational invariance. These preprocessing
    functions are defined in the following listing.
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建模型本身之前，我们需要准备数据和创建一些预处理数据的函数，以便它以适合我们模型的形式存在。首先，原始的 MNIST 图像是灰度像素数组，其值从
    0 到 255，因此我们需要将这些值归一化到 0 到 1 之间，否则训练过程中的梯度将过于变化，训练将不稳定。由于 MNIST 非常简单，我们还可以通过添加噪声和随机扰动图像（例如，随机平移和旋转）来稍微增加我们模型的压力。这将使我们能够评估平移和旋转不变性。这些预处理函数定义在下面的列表中。
- en: Listing 10.1\. Preprocessing functions
  id: totrans-1363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.1\. 预处理函数
- en: '[PRE90]'
  id: totrans-1364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '***1*** Downloads and loads the MNIST training data'
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 下载并加载 MNIST 训练数据'
- en: '***2*** Downloads and loads the MNIST testing data for validation'
  id: totrans-1366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 下载并加载 MNIST 测试数据以进行验证'
- en: '***3*** Adds random spots to the image'
  id: totrans-1367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 向图像中添加随机点'
- en: '***4*** Preprocesses the images and perform random transformations of rotation
    and translation'
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 预处理图像并执行随机旋转和平移变换'
- en: The `add_spots` function takes an image and adds random noise to it. This function
    is used by the `prepare_images` function, which normalizes the image pixels between
    0 and 1 and performs random minor transformations such as adding noise, translating
    (shifting) the image, and rotating the image.
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_spots` 函数接受一个图像并向其添加随机噪声。这个函数被 `prepare_images` 函数使用，该函数将图像像素归一化到 0 到
    1 之间，并执行随机的小型变换，例如添加噪声、平移（移动）图像和旋转图像。'
- en: '[Figure 10.14](#ch10fig14) shows an example of an original and perturbed MNIST
    digit. You can see that the image is translated up and to the right and has random
    dots sprinkled in. This makes the learning task more difficult because our model
    must learn translational, noise, and rotational invariant features in order to
    successfully classify. The `prepare_images` function has parameters that let you
    tune how much the image will be perturbed, so you can control the difficulty of
    the problem.'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.14](#ch10fig14)展示了原始和扰动后的MNIST数字的示例。你可以看到图像被向上和向右平移，并撒上随机点。这使得学习任务更加困难，因为我们的模型必须学习平移、噪声和旋转不变特征，才能成功分类。`prepare_images`函数有参数可以调整图像的扰动程度，因此你可以控制问题的难度。'
- en: 'Figure 10.14\. Left: Original MNIST digit for the number “5”. Right: Transformed
    version that is translated to the top right and with random noise sprinkled in.'
  id: totrans-1371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.14\. 左：数字“5”的原始MNIST数字。右：转换后的版本，被平移到右上角，并撒上随机噪声。
- en: '![](10fig14.jpg)'
  id: totrans-1372
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig14.jpg)'
- en: 10.3.2\. The relational module
  id: totrans-1373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2\. 关系模块
- en: Now we can dive into the relational neural network itself. Until now, all of
    the projects in this book were designed to be compelling enough to illustrate
    an important concept but simple enough to be able to run on a modern laptop without
    needing a GPU. The computational demands of the self-attention module, however,
    are significantly greater than any of the other models we have built so far in
    the book. You can still try running this model on your laptop, but it will be
    significantly faster if you have a CUDA-enabled GPU. If you don’t have a GPU,
    you can easily launch a cloud-based Jupyter Notebook using Amazon SageMaker, Google
    Cloud, or Google Colab (which is free as of this writing).
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以深入探讨关系神经网络本身。到目前为止，本书中的所有项目都是为了说明一个重要概念而设计的，既具有吸引力，又足够简单，可以在没有GPU的现代笔记本电脑上运行。然而，自注意力模块的计算需求比本书中我们构建的任何其他模型都要大得多。你仍然可以在你的笔记本电脑上尝试运行这个模型，但如果你有一个支持CUDA的GPU，它将运行得更快。如果你没有GPU，你可以轻松地使用Amazon
    SageMaker、Google Cloud或Google Colab（截至本文写作时是免费的）启动基于云的Jupyter Notebook。
- en: '|  |'
  id: totrans-1375
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The code we show in this book will not include the necessary (but very minor)
    modifications necessary to run on a GPU. Please refer to this book’s GitHub page
    at [http://mng.bz/JzKp](http://mng.bz/JzKp) to see how to enable the code to run
    on a GPU, or consult the PyTorch documentation at [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html).
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 本书展示的代码将不包括在GPU上运行所需的（但非常微小）修改。请参阅本书的GitHub页面[http://mng.bz/JzKp](http://mng.bz/JzKp)，了解如何使代码在GPU上运行，或者查阅PyTorch文档[https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)。
- en: '|  |'
  id: totrans-1378
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In [listing 10.2](#ch10ex02) we define a class that is the relational module.
    It is a single, but complex, neural network that includes an initial set of convolutional
    layers followed by the key, query, and value matrix multiplications.
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.2](#ch10ex02)中，我们定义了一个类，即关系模块。这是一个单一但复杂的神经网络，包括一组初始卷积层，随后是关键、查询和值矩阵乘法。
- en: Listing 10.2\. Relational module
  id: totrans-1380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2\. 关系模块
- en: '[PRE91]'
  id: totrans-1381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '***1*** Defines the number of channels for each convolutional layer'
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 定义了每个卷积层的通道数'
- en: '***2*** self.H and self.W are the height and width of the input image, respectively.'
  id: totrans-1383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** self.H和self.W分别是输入图像的高度和宽度。'
- en: '***3*** The dimension of the nodes after passing through the relational module'
  id: totrans-1384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 经过关系模块后的节点维度'
- en: '***4*** The number of objects or nodes, which is just the number of pixels
    after passing through the convolutions'
  id: totrans-1385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 物体或节点的数量，这仅仅是经过卷积后的像素数'
- en: '***5*** The dimensionality of each node vector is the number of channels in
    the last convolution plus 2 spatial dimensions.'
  id: totrans-1386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 每个节点向量的维度是最后一个卷积层的通道数加上2个空间维度。'
- en: '***6*** Layer normalization improves learning stability.'
  id: totrans-1387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 层归一化提高了学习稳定性。'
- en: The basic setup of our model is an initial block of four convolutional layers
    that we use to preprocess the raw pixel data into higher-level features. Our ideal
    relational model would be completely invariant to rotations and smooth deformations,
    and by including these convolutional layers that are only translation-invariant,
    our whole model is now less robust to rotations and deformations. However, the
    CNN layers are more computationally efficient than relational modules, so doing
    some preprocessing with CNNs usually works out well in practice.
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的基本设置是一个由四个卷积层组成的初始块，我们使用它来预处理原始像素数据，将其转换为更高级别的特征。我们理想的关联模型应该对旋转和平滑变形完全不变，通过包含这些仅对平移不变的卷积层，我们的整个模型现在对旋转和变形的鲁棒性降低。然而，CNN
    层比关联模块计算效率更高，所以在实践中使用 CNN 进行一些预处理通常效果很好。
- en: After the CNN layers, we have three linear projection layers that project a
    set of nodes into a higher-dimensional feature space. We also have some LayerNorm
    layers (discussed in more detail shortly), and a couple of linear layers at the
    end. Overall, it’s not a complicated architecture, but the details are in the
    forward pass of the model.
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 层之后，我们有三个线性投影层，将一组节点投影到更高维的特征空间。我们还有一些 `LayerNorm` 层（将在稍后详细讨论），以及几个线性层在最后。总的来说，这不是一个复杂的架构，但细节在模型的正向传播中。
- en: Listing 10.3\. The forward pass (continued from [listing 10.2](#ch10ex02))
  id: totrans-1390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.3\. 前向传播（从 [列表 10.2](#ch10ex02) 继续）
- en: '[PRE92]'
  id: totrans-1391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '***1*** Appends the (x,y) coordinates of each node to its feature vector and
    normalizes to within the interval [0, 1]'
  id: totrans-1392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将每个节点的 (x,y) 坐标添加到其特征向量中，并将其归一化到区间 [0, 1]'
- en: '***2*** Projects the input node matrix into key, query, and value matrices'
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将输入节点矩阵投影到键、查询和值矩阵'
- en: '***3*** Batch matrix multiplies the query and key matrices'
  id: totrans-1394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 批量矩阵乘以查询和键矩阵'
- en: '***4*** Batch matrix multiplies the attention weight matrix and the value matrix'
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 批量矩阵乘以注意力权重矩阵和值矩阵'
- en: Let’s see how this forward pass corresponds to the schematic back in [figure
    10.13](#ch10fig13). There are a few novelties used in this code that have not
    come up elsewhere in this book and that you may be unaware of. One is the use
    of the `LayerNorm` layer in PyTorch, which unsurprisingly stands for *layer normalization*.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个前向传播如何对应于 [图 10.13](#ch10fig13) 中的电路图。在这段代码中使用了几个新颖的技术，这些技术在本书的其他地方没有出现过，你可能还不知道。其中一个是
    PyTorch 中 `LayerNorm` 层的使用，它不出所料地代表 *层归一化*。
- en: '`LayerNorm` is one form of neural network normalization; another popular one
    is called *batch normalization* (or just `BatchNorm`). The problem with unnormalized
    neural networks is that the magnitude of the inputs to each layer in the neural
    network can vary dramatically, and the range of values that the inputs can take
    can change from batch to batch. This increases the variability of the gradients
    during training and leads to instability, which can significantly slow training.
    Normalization seeks to keep all inputs at each major step of computation to within
    a relatively fixed, narrow range (i.e., with some constant mean and variance).
    This keeps gradients more stable and can make training much faster.'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: '`LayerNorm` 是神经网络归一化的一种形式；另一种流行的称为 *批量归一化*（或简称 `BatchNorm`）。未归一化的神经网络的问题在于，神经网络中每一层的输入幅值可以变化很大，输入值的范围可以从一批到另一批发生变化。这增加了训练期间梯度的可变性，并导致不稳定性，这可能会显著减慢训练速度。归一化的目的是将所有输入保持在计算过程中的每个主要步骤的相对固定、狭窄的范围内（即，具有某些常数均值和方差）。这使梯度更加稳定，可以使训练速度大大提高。'
- en: As we have been discussing, self-attention (and the broader class of relational
    or graph) models are capable of feats that ordinary feedforward models struggle
    with due to their inductive bias of data being relational. Unfortunately, because
    the model involves a softmax in the middle, this can make training unstable and
    difficult as the softmax restricts outputs to within a very narrow range that
    can become saturated if the input is too big or small. Thus it is critical to
    include normalization layers to reduce these problems, and in our experiments
    `LayerNorm` improves training performance substantially, as expected.
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们一直在讨论的，自注意力（以及更广泛的关联或图）模型能够完成普通前馈模型由于数据关联的归纳偏差而难以完成的壮举。不幸的是，因为模型中涉及中间的 softmax，这可能会使训练不稳定且困难，因为
    softmax 限制了输出在一个非常狭窄的范围内，如果输入太大或太小，这个范围可能会饱和。因此，包括归一化层以减少这些问题是至关重要的，在我们的实验中，正如预期的那样，`LayerNorm`
    显著提高了训练性能。
- en: 10.3.3\. Tensor contractions and Einstein notation
  id: totrans-1399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.3\. 张量收缩与爱因斯坦符号
- en: The other novelty in this code is the use of the `torch.einsum` function`.`
    Einsum is short for *Einstein summation* (also called *Einstein notation*); it
    was introduced by Albert Einstein as a new notation for representing certain kinds
    of operations with tensors. While we could have written the same code without
    Einsum, it is much simpler with it, and we encourage its use when it offers improved
    code readability.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的另一个新特性是使用了 `torch.einsum` 函数。Einsum 是指 *爱因斯坦求和*（也称为 *爱因斯坦符号*）；它是由阿尔伯特·爱因斯坦提出的一种新的符号，用于表示某些类型的张量操作。虽然我们可以不使用
    Einsum 就写出相同的代码，但使用它会更简单，我们鼓励在提供更好的代码可读性时使用它。
- en: To understand it, you must recall that tensors (in the machine learning sense,
    where they are just multidimensional arrays) may have 0 or more dimensions that
    are accessed by corresponding indices. Recall that a scalar (single number) is
    a 0-tensor, a vector is a 1-tensor, a matrix is a 2-tensor, and so on. The number
    corresponds to how many indices each tensor has. A vector has one index because
    each element in a vector can be addressed and accessed by a single nonnegative
    integer index value. A matrix element is accessed by two indices, its row and
    column positions. This generalizes to arbitrary dimensions.
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解它，你必须回忆起张量（在机器学习意义上，它们只是多维数组）可能有 0 或更多维度，这些维度通过相应的索引来访问。回忆一下，标量（单个数字）是 0
    张量，向量是 1 张量，矩阵是 2 张量，依此类推。数字对应于每个张量有多少个索引。向量有一个索引，因为向量中的每个元素都可以通过一个非负整数索引值来访问和定位。矩阵元素通过两个索引来访问，其行和列位置。这推广到任意维度。
- en: If you’ve made it this far, you’re familiar with operations like the inner (dot)
    product between two vectors and matrix multiplication (either multiplying a matrix
    with a vector or another matrix). The generalization of these operations to arbitrary
    order tensors (e.g., the “multiplication” of two 3-tensors) is called a *tensor
    contraction*. Einstein notation makes it easy to represent and compute any arbitrary
    tensor contraction, and with self-attention, we’re attempting to contract two
    3-tensors (and later two 4-tensors) so it becomes necessary to use Einsum or we
    would have to reshape the 3-tensor into a matrix, do normal matrix multiplication,
    and then reshape it back into a 3-tensor (which is much less readable than just
    using Einsum).
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经读到这儿，你对两个向量之间的内积（点积）和矩阵乘法（无论是用一个矩阵乘以一个向量还是另一个矩阵）等操作已经很熟悉了。将这些操作推广到任意阶的张量（例如，两个
    3 张量的“乘法”）称为 *张量收缩*。爱因斯坦符号使得表示和计算任何任意张量收缩变得容易，并且随着自注意力机制的应用，我们试图收缩两个 3 张量（稍后是两个
    4 张量），因此有必要使用 Einsum，否则我们就必须将 3 张量重塑为矩阵，进行常规的矩阵乘法，然后再将其重塑回 3 张量（这比直接使用 Einsum
    要难读得多）。
- en: 'This is the general formula for a tensor contraction of two matrices:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个矩阵张量收缩的通用公式：
- en: '![](pg303.jpg)'
  id: totrans-1404
  prefs: []
  type: TYPE_IMG
  zh: '![](pg303.jpg)'
- en: The output on the left, *C[i,k]*, is the resulting matrix from multiplying matrices
    *A*:*i* × *j* and *B*:*j* × *k* (where *i*,*j*,*k* are the dimensions) such that
    dimension *j* for each matrix is the same size (which we know is required to do
    matrix multiplication). What this tells us is that element *C*[0,0], for example,
    is equal to Σ*A*[0,*j*]*B[j]*[,0] for all *j*. The first element in the output
    matrix *C* is computed by taking each element in the first row of *A*, multiplying
    it by each element in the first column of *B*, and then summing these all together.
    We can figure out each element of *C* by this process of summing over a particular
    *shared index* between two tensors. This summation over a shared index is the
    process of tensor contraction, since we start with, for example, two input tensors
    with two indices each (for a total of four indices) and the output has two indices
    because two of the four get contracted away. If we did a tensor contraction over
    two 3-tensors, the result would be a 4-tensor.
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的输出，*C[i,k]*，是矩阵 *A*:*i* × *j* 和 *B*:*j* × *k*（其中 *i*,*j*,*k* 是维度）相乘的结果矩阵，使得每个矩阵的维度
    *j* 大小相同（我们知道这是进行矩阵乘法所必需的）。这告诉我们，例如，元素 *C*[0,0] 等于 Σ*A*[0,*j*]*B[j]*[,0] 对于所有
    *j*。输出矩阵 *C* 的第一个元素是通过取 *A* 的第一行中的每个元素，将其与 *B* 的第一列中的每个元素相乘，然后将这些相加得到的。我们可以通过在两个张量之间的特定
    *共享索引* 上进行求和的过程来找出 *C* 的每个元素。这个共享索引上的求和过程是张量收缩的过程，因为我们从，例如，每个具有两个索引的两个输入张量（总共四个索引）开始，输出有两个索引，因为其中的两个被收缩掉了。如果我们对两个
    3 张量进行张量收缩，结果将是一个 4 张量。
- en: '|  |'
  id: totrans-1406
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Tensor contraction: example**'
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量收缩：示例**'
- en: Let’s tackle a concrete example of a tensor contraction; we’ll contract two
    matrices using Einstein notation.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决一个具体的张量合并例子；我们将使用 Einstein 符号合并两个矩阵。
- en: '![](pg304-1.jpg)'
  id: totrans-1409
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg304-1.jpg)'
- en: Matrix *A* is a 2 × 3 matrix and matrix *B* is 3 × 2\. We will label the dimensions
    of these matrices using arbitrary characters. For example, we’ll label matrix
    *A*:*i* × *j* with dimensions (indices) *i* and j, and matrix *B*:*j* × *k* with
    indices *j* and *k*. We could have labeled the indices using any characters, but
    we want to contract over the shared dimensions of *A[j]* = *B[j]* = 3, so we label
    them with the same characters.
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A* 是一个 2 × 3 的矩阵，矩阵 *B* 是 3 × 2。我们将使用任意字符标记这些矩阵的维度。例如，我们将标记矩阵 *A* 为 *i*
    × *j*，维度为 (索引) *i* 和 *j*，矩阵 *B* 为 *j* × *k*，索引为 *j* 和 *k*。我们本可以用任何字符来标记索引，但因为我们想要合并
    *A[j]* = *B[j]* = 3 的共享维度，所以我们用相同的字符来标记它们。
- en: '![](pg304-2.jpg)'
  id: totrans-1411
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg304-2.jpg)'
- en: This *C* matrix represents the output. Our goal is to figure out the values
    of the *x* values, which are labeled by their indexed positions. Using the previous
    tensor contraction formula, we can figure out *x*[0,0] by finding row 0 of matrix
    *A* and column 0 of matrix *B*:*A*[0,*j*] = [1,–2,4] and *B[j]*[,0] = [–3,5,0]*^T*.
    Now we loop over the *j* index, multiplying each element of *A*[0,*j*] with *B[j]*[,0]
    and then summing them together to get a single number, which will be *x*[0,0]
    In this case, *x*[0,0] = Σ*A*[0,*j*] × *B[j]*[,0] = (1 × –3) + (–2 × 5) + (4 ×
    0) = –3 – 10 = –13\. That was the calculation just for one element in the output
    matrix, element *C*[0,0]. We do this same process for all elements in *C* and
    we get all the values. Of course, we never do this by hand, but this is what is
    happening under the hood when we do a tensor contraction, and this process generalizes
    to tensors of higher order than just matrices.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 *C* 矩阵代表输出。我们的目标是找出 *x* 的值，这些值通过它们的索引位置进行标记。使用之前的张量合并公式，我们可以通过找到矩阵 *A* 的第
    0 行和矩阵 *B* 的第 0 列来找出 *x*[0,0]：*A*[0,*j*] = [1,–2,4] 和 *B[j]*[,0] = [–3,5,0]*^T*。现在我们遍历
    *j* 索引，将 *A*[0,*j*] 的每个元素与 *B[j]*[,0] 相乘，然后将它们相加得到一个单一的数字，这个数字将是 *x*[0,0]。在这种情况下，*x*[0,0]
    = Σ*A*[0,*j*] × *B[j]*[,0] = (1 × –3) + (–2 × 5) + (4 × 0) = –3 – 10 = –13。这只是输出矩阵中的一个元素的计算，即元素
    *C*[0,0]。我们对 *C* 中的所有元素都进行同样的过程，并得到所有值。当然，我们永远不会手动这样做，但这就是当我们进行张量合并时在底层发生的事情，这个过程可以推广到比矩阵更高阶的张量。
- en: Most of the time you will see Einstein notation written without the summation
    symbol, where it is assumed we sum over the shared index. That is, rather than
    explicitly writing *C[i,k]* = Σ*A[i]*[,*j*] × *B[j]*[,*k*], we often just write
    *C[i,k]* = *A[i]*[,*j*]*B[j]*[,*k*] and omit the summation.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，你会看到没有求和符号的 Einstein 符号，其中假设我们对共享的索引进行求和。也就是说，我们通常不会明确写出 *C[i,k]* = Σ*A[i]*[,*j*]
    × *B[j]*[,*k*]，我们通常只写 *C[i,k]* = *A[i]*[,*j*]*B[j]*[,*k*] 并省略求和。
- en: '|  |'
  id: totrans-1414
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Einstein notation can also easily represent a batch matrix multiplication in
    which we have two collections of matrices and we want to multiply the first two
    matrices together, the second two together, etc., until we get a new collection
    of multiplied matrices.
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: Einstein 符号也可以轻松表示一个批量矩阵乘法，其中我们有两个矩阵集合，我们想要将前两个矩阵相乘，然后是后两个矩阵，等等，直到我们得到一个新的乘积矩阵集合。
- en: This is the Einsum equation for batch matrix multiplication,
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: 这是批量矩阵乘法的 Einsum 方程，
- en: '![](pg305.jpg)'
  id: totrans-1417
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg305.jpg)'
- en: where the *b* dimension is the batch dimension and we just contract over the
    shared *j* dimension. We will use Einsum notation to do batch matrix multiplication,
    but we can also use it to contract over multiple indices at once when using higher-order
    tensors than matrices.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*b* 维度是批量维度，我们只是在共享的 *j* 维度上进行合并。我们将使用 Einsum 符号来进行批量矩阵乘法，但当我们使用比矩阵更高阶的张量时，我们也可以用它一次性合并多个索引。
- en: In [listing 10.3](#ch10ex03) we used `A = torch.einsum('bfe,bge->bfg',Q,K)`
    to compute batched matrix multiplication of the *Q* and *K* matrices. Einsum accepts
    a string that contains the instructions for which indices to contract over, and
    then the tensors that will be contracted. The string `'bfe,bge->bfg'` associated
    with tensors *Q* and *K* means that *Q* is a tensor with three dimensions labeled,
    `bfe`, and *K* is a tensor with three dimensions labeled, `bge,` and that we want
    to contract these tensors to get an output tensor with three dimensions labeled,
    `bfg`. We can only contract over dimensions that are the same size and are labeled
    the same, so in this case we contract over the `e` dimension, which is the node
    feature dimension, leaving us with two copies of the node dimension, which is
    why the output is of dimension *b* × *n* × *m*. When using Einsum, we can label
    the dimensions of each tensor with any alphabetic characters, but we must make
    sure that the dimension we wish to contract over is labeled with the same character
    for both tensors.
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.3](#ch10ex03)中，我们使用了 `A = torch.einsum('bfe,bge->bfg',Q,K)` 来计算 *Q* 和
    *K* 矩阵的批处理矩阵乘法。Einsum 接受一个包含要收缩的索引指令的字符串，然后是将被收缩的张量。与张量 *Q* 和 *K* 相关的字符串 `'bfe,bge->bfg'`
    意味着 *Q* 是一个具有三个维度标记的张量，`bfe`，而 *K* 是一个具有三个维度标记的张量，`bge`，并且我们想要收缩这些张量以获得一个具有三个维度标记的输出张量，`bfg`。我们只能收缩相同大小且标记相同的维度，因此在这种情况下，我们在
    `e` 维度上收缩，这是节点特征维度，留下两个节点维度的副本，这就是为什么输出维度为 *b* × *n* × *m*。当使用 Einsum 时，我们可以用任何字母字符标记每个张量的维度，但我们必须确保要收缩的维度在两个张量中都有相同的标记。
- en: After the batch matrix multiplication, and we have the unnormalized adjacency
    matrix, we did `A = A / np.sqrt(self.node_size)` to rescale the matrix to reduce
    excessively large values and improve training performance; this is why we earlier
    referred to this is as *scaled* dot product attention.
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: 在批矩阵乘法之后，我们得到了未归一化的邻接矩阵，我们执行了 `A = A / np.sqrt(self.node_size)` 来重新缩放矩阵，以减少过大的值并提高训练性能；这就是为什么我们之前将其称为*缩放*点积注意力。
- en: In order to get the *Q*, *K*, and *V* matrices, as we discussed earlier, we
    took the output of the last convolutional layer which is a tensor of dimensions
    batch × channels × height × width, and we collapse the height and width dimensions
    into a single dimension of (height × width = *n*) for the number of nodes, since
    each pixel position will become a potential node or object in the node matrix.
    Thus we get an initial node matrix of *N*:*b* × *c* × *n* that we reshape into
    *N*:*b* × *n* × *c*.
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取*Q*、*K*和*V*矩阵，正如我们之前讨论的，我们取了最后一个卷积层的输出，它是一个维度为批 × 通道 × 高 × 宽的张量，并将高度和宽度维度合并为一个维度为（高度
    × 宽度 = *n*）的单个维度，对于节点数量，因为每个像素位置都可能成为节点矩阵中的一个潜在节点或对象。因此，我们得到了一个初始节点矩阵 *N*:*b*
    × *c* × *n*，并将其重塑为 *N*:*b* × *n* × *c*。
- en: By collapsing the spatial dimensions into a single dimension, the spatial arrangement
    of the nodes is scrambled and the network would struggle to discover that certain
    nodes (which were originally nearby pixels) are related spatially. That is why
    we add two extra channel dimensions that encode the (*x*,*y*) position of each
    node before it was collapsed. We normalize the positions to be in the interval
    [0, 1], since normalization almost always helps with performance.
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将空间维度合并为一个维度，节点的空间排列被打乱，网络将难以发现某些节点（原本是附近的像素）在空间上相关。这就是为什么我们在节点合并之前添加了两个额外的通道维度来编码每个节点的
    (*x*, *y*) 位置。我们将位置归一化到区间 [0, 1]，因为归一化几乎总是有助于性能。
- en: Adding these absolute spatial coordinates to the end of each node’s feature
    vector helps maintain the spatial information, but it is not ideal since these
    coordinates are in reference to an external coordinate system, which means we’re
    dampening some of the invariance to spatial transformations that a relational
    module should have, in theory. A more robust approach is to encode *relative*
    positions with respect to other nodes, which would maintain spatial invariance.
    However, this approach is more complicated, and we can still achieve good performance
    and interpretability with the absolute encoding.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些绝对空间坐标添加到每个节点特征向量的末尾有助于保持空间信息，但这种方法并不理想，因为这些坐标是相对于外部坐标系而言的，这意味着我们在理论上削弱了关系模块应具有的一些对空间变换的不变性。一个更稳健的方法是使用相对于其他节点的*相对*位置进行编码，这将保持空间不变性。然而，这种方法更复杂，但我们仍然可以通过绝对编码实现良好的性能和可解释性。
- en: We then pass this initial node matrix through three different linear layers
    to project it into three different matrices with a potentially different channel
    dimension (which we will call *node-feature dimension* from this point), as shown
    in [figure 10.15](#ch10fig15).
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将这个初始节点矩阵通过三个不同的线性层传递，将其投影到三个具有不同通道维度的不同矩阵中（从现在起我们将称之为*节点特征维度*），如图10.15所示[图10.15](#ch10fig15)。
- en: Figure 10.15\. The projection step in self-attention. The input nodes are projected
    into a (usually) higher-dimensional feature space by simple matrix multiplication.
  id: totrans-1425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.15\. 自注意力中的投影步骤。输入节点通过简单的矩阵乘法被投影到一个（通常是）更高维的特征空间。
- en: '![](10fig15.jpg)'
  id: totrans-1426
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig15.jpg)'
- en: Once we multiply the query and key matrices, we get an unnormalized attention
    weight matrix, *A*:*b* × *n* × *n*, where *b* = batch and *n* = the number of
    nodes. We then normalize it by applying softmax across the rows (dimension 1,
    counting from 0) such that each row sums to 1\. This forces each node to only
    pay attention to a small number of other nodes, or to spread its attention very
    thinly across many nodes.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们乘以查询和键矩阵，我们得到一个未归一化的注意力权重矩阵，*A*:*b* × *n* × *n*，其中*b* = 批次和*n* = 节点数。然后我们通过在行（维度1，从0开始计数）上应用softmax来归一化它，使得每一行之和为1。这迫使每个节点只关注少数其他节点，或者将其注意力非常稀疏地分散在许多节点上。
- en: Then we multiply the attention matrix by the value matrix to get an updated
    node matrix, such that each node is now a weighted combination of all the other
    nodes. So if node 0 pays strong attention to nodes 5 and 9 but ignores the others,
    once we multiply the attention matrix with the value matrix, node 0 will be updated
    to be a weighted combination of nodes 5 and 9 (and itself, because nodes generally
    pay some attention to themselves). This general operation is termed *message passing*
    because each node sends a message (i.e., its own feature vector) to the nodes
    to which it is connected.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将注意力矩阵与值矩阵相乘，以得到更新的节点矩阵，使得每个节点现在都是所有其他节点的加权组合。所以如果节点0对节点5和9给予强烈的关注但忽略其他节点，一旦我们将注意力矩阵与值矩阵相乘，节点0将被更新为节点5和9（以及它自己，因为节点通常会对自身给予一些关注）的加权组合。这种一般操作被称为*消息传递*，因为每个节点向其连接的节点发送消息（即，它自己的特征向量）。
- en: Once we have our updated node matrix, we can reduce it down to a single vector
    by either averaging or max-pooling over the node dimension to get a single *d*-dimensional
    vector that should summarize the graph as a whole. We can pass that through a
    few ordinary linear layers before getting our final output, which is just a vector
    of Q values. Thus we are building a relational deep Q-network (Rel-DQN).
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了更新的节点矩阵，我们可以通过在节点维度上平均或最大池化来将其降低到单个向量，得到一个单*d*-维向量，该向量应该总结整个图。我们可以在得到最终输出之前通过几个普通的线性层传递它，最终输出只是一个Q值的向量。因此，我们正在构建一个关系深度Q网络（Rel-DQN）。
- en: 10.3.4\. Training the relational module
  id: totrans-1430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.4\. 训练关系模块
- en: You might have noticed the last function call in the code is actually `log_softmax`,
    which is not something we would use for Q-learning. But before we get to Q-learning,
    we will test our relational module on classifying MNIST digits and compare it
    to a conventional nonrelational convolutional neural network. Given that our relational
    module has the ability to model long-distance relationships in a way that a simple
    convolutional neural network cannot, we would expect our relational module to
    perform better in the face of strong transformations. Let’s see how it does.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到代码中的最后一个函数调用实际上是`log_softmax`，这不是我们会用于Q-learning的东西。但在我们到达Q-learning之前，我们将测试我们的关系模块在分类MNIST数字上的表现，并将其与传统的非关系卷积神经网络进行比较。鉴于我们的关系模块能够以简单卷积神经网络无法做到的方式模拟长距离关系，我们预计我们的关系模块在面对强烈变换时将表现得更好。让我们看看它的表现如何。
- en: Listing 10.4\. MNIST training loop
  id: totrans-1432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4\. MNIST训练循环
- en: '[PRE93]'
  id: totrans-1433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '***1*** Creates an instance of our relational module'
  id: totrans-1434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建我们的关系模块的一个实例'
- en: '***2*** Randomly selects a subset of the MNIST images'
  id: totrans-1435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 随机选择MNIST图像的一个子集'
- en: '***3*** Perturbs the images in the batch using the prepare_images function
    we created, using max rotation of 30 degrees'
  id: totrans-1436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用我们创建的`prepare_images`函数扰动批次中的图像，最大旋转为30度'
- en: '***4*** The predicted image class is the output vector’s argmax.'
  id: totrans-1437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 预测的图像类别是输出向量的argmax。'
- en: '***5*** Calculates prediction accuracy within the batch'
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 计算批次内的预测准确度'
- en: This is a pretty straightforward training loop to train our MNIST classifier.
    We omitted the code necessary to store the losses for later visualization, but
    the unabridged code can be found in this book’s GitHub repository. We told the
    `prepare_images` function to randomly rotate the images by up to 30 degrees in
    either direction, which is a significant amount.
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当直接的训练循环，用于训练我们的 MNIST 分类器。我们省略了存储损失以便稍后可视化的代码，但完整的代码可以在本书的 GitHub 仓库中找到。我们告诉
    `prepare_images` 函数随机旋转图像，最多30度，这是一个相当大的角度。
- en: '[Figure 10.16](#ch10fig16) shows how the relational module performed after
    1,000 epochs (which is not long enough to reach maximum accuracy). The plots look
    good, but this is just performance on the training data.'
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.16](#ch10fig16) 展示了关系模块在经过1,000个epoch（这还不足以达到最大准确率）后的表现。图表看起来不错，但这只是训练数据上的性能。'
- en: Figure 10.16\. The loss and accuracy over training epochs for the relational
    module on classifying MNIST digits
  id: totrans-1441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.16。关系模块在分类 MNIST 数字时，训练 epoch 中的损失和准确率。
- en: '![](10fig16_alt.jpg)'
  id: totrans-1442
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig16_alt.jpg)'
- en: To really know how well it performs, we need to run the model on the test data,
    which is a separate set of data that the model has never seen before. We’ll run
    it on 500 examples from the test data to calculate its accuracy.
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正了解其性能如何，我们需要在测试数据上运行模型，这是一个模型之前从未见过的独立数据集。我们将运行测试数据中的500个示例来计算其准确率。
- en: Listing 10.5\. MNIST test accuracy
  id: totrans-1444
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5。MNIST 测试准确率。
- en: '[PRE94]'
  id: totrans-1445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: We get nearly 95% accuracy at test time with the relational module after just
    1,000 epochs of testing. Again, 1,000 epochs with a batch size of 300 is not enough
    to reach maximal accuracy. Maximal accuracy with any decent neural network on
    (unperturbed) MNIST should be around the 98–99% mark. But we’re not going for
    maximum accuracy here; we’re just making sure it works and that it performs better
    than a convolutional neural network with a similar number of parameters.
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过1,000个epoch的测试后，使用关系模块我们得到了近95%的准确率。再次强调，1,000个epoch，批次大小为300，还不足以达到最大准确率。任何
    decent 神经网络在（未受干扰的）MNIST上的最大准确率应该在大约98-99%左右。但在这里我们并不是追求最大准确率；我们只是确保它能够工作，并且其性能优于具有相似参数数量的卷积神经网络。
- en: We used the following simple CNN as a baseline, which has 88,252 trainable parameters
    compared to the relational module’s 85,228\. The CNN actually has about 3,000
    more parameters than our relational module, so it has a bit of an advantage.
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下简单的 CNN 作为基线，它有88,252个可训练参数，而关系模块有85,228个。实际上，CNN 比我们的关系模块多出大约3,000个参数，因此它有一定的优势。
- en: Listing 10.6\. Convolutional neural network baseline for MNIST
  id: totrans-1448
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6。MNIST 的卷积神经网络基线。
- en: '[PRE95]'
  id: totrans-1449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '***1*** The architecture consists of 5 convolutional layers total.'
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 架构总共由5个卷积层组成。'
- en: '***2*** After the first 4 convolutional layers, we MaxPool to reduce the dimensionality.'
  id: totrans-1451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 在前4个卷积层之后，我们使用 MaxPool 来降低维度。'
- en: '***3*** The last layer is a linear layer after we flatten the output from the
    CNN.'
  id: totrans-1452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 最后一个层是在将 CNN 的输出展平后的线性层。'
- en: '***4*** Lastly, we apply the log_softmax function to classify the digits probabilistically.'
  id: totrans-1453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 最后，我们将 log_softmax 函数应用于对数字进行概率分类。'
- en: Instantiate this CNN and swap it in for the relational module in the previous
    training loop to see how it compares. We get a test accuracy of only 87.80% with
    this CNN, demonstrating that our relational module is outperforming a CNN architecture,
    controlling for the number of parameters. Moreover, if you crank up the transformation
    level (e.g., add more noise, rotate even more), the relational module will maintain
    a higher accuracy than the CNN. As we noted earlier, our particular implementation
    of the relational module is not practically invariant to rotations and deformations
    because, in part, we’ve added the absolute coordinate positions; it’s not all
    relational, but it has the ability to compute long-distance relations between
    features in the image, as opposed to a CNN that can just compute local features.
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化这个 CNN，并在之前的训练循环中将其替换为关系模块，以比较其性能。使用这个 CNN，我们得到了仅87.80%的测试准确率，这表明我们的关系模块在控制参数数量的情况下优于
    CNN 架构。此外，如果你提高变换级别（例如，添加更多噪声，旋转更多），关系模块将保持比 CNN 更高的准确率。正如我们之前提到的，我们特定的关系模块实现并不对旋转和变形具有实际上的不变性，部分原因是我们添加了绝对坐标位置；它并不完全是关系性的，但它具有计算图像中特征之间长距离关系的能力，而
    CNN 只能计算局部特征。
- en: We wanted to introduce relational modules not merely because they might get
    better accuracy on some data set, but because they are more interpretable than
    traditional neural network models. We can inspect the learned relationships in
    the attention weight matrix to see which parts of the input the relational module
    is using to classify images or predict Q values as shown in [figure 10.17](#ch10fig17).
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要引入关系模块，不仅仅是因为它们可能在某些数据集上获得更高的准确率，还因为它们比传统的神经网络模型更具可解释性。我们可以检查注意力权重矩阵中学习到的关系，以查看关系模块正在使用输入的哪些部分来分类图像或预测Q值，如图[10.17图](#ch10fig17)所示。
- en: 'Figure 10.17\. Left column: Original input MNIST images (after transformation).
    Right column: Corresponding self-attention weights showing where the model is
    paying the most attention.'
  id: totrans-1456
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.17。左侧列：原始输入MNIST图像（转换后）。右侧列：相应的自注意力权重，显示了模型最关注的输入部分。
- en: '![](10fig17.jpg)'
  id: totrans-1457
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig17.jpg)'
- en: 'We visualize this attention map by just reshaping the attention map into a
    square image:'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将注意力图重塑为方形图像来可视化这个注意力图：
- en: '[PRE96]'
  id: totrans-1459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The attention weight matrix is a *batch* × *n* × *n* matrix where *n* is the
    number of nodes, which is 16² = 256 in our example, since after the convolutional
    layers the spatial extent is reduced from the original 28 × 28\. Notice in the
    top two examples of [figure 10.17](#ch10fig17) that attention maps highlight the
    contour of the digit but with more intensity at certain parts. If you look through
    a number of these attention maps, you’ll notice that the model tends to pay most
    attention to the inflection and crossover points of the digit. For the digit 8,
    it can successfully classify this image as the number 8 just by paying attention
    to the center of the 8 and the bottom part. You can also notice that in none of
    the examples is attention given to the added spots of noise in the input; attention
    is only given to the real digit part of the image, demonstrating that the model
    is learning to separate the signal from the noise to a large degree.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力权重矩阵是一个 *批处理* × *n* × *n* 矩阵，其中 *n* 是节点的数量，在我们的例子中，由于卷积层后的空间范围从原始的28 × 28减少，所以
    *n* 是16² = 256。注意在[图10.17](#ch10fig17)的前两个例子中，注意力图突出了数字的轮廓，但在某些部分具有更强的强度。如果你查看多个这样的注意力图，你会注意到模型倾向于最关注数字的拐点和交叉点。对于数字8，它只需关注8的中心和底部部分，就能成功地将这个图像分类为数字8。你还可以注意到，在所有例子中，注意力都没有分配给输入中添加的噪声点；注意力只分配给图像的真实数字部分，这表明模型正在学会在很大程度上将信号与噪声分离。
- en: 10.4\. Multi-head attention and relational DQN
  id: totrans-1461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4. 多头注意力和关系DQN
- en: We’ve demonstrated that our relational model performs well on the simple task
    of classifying MNIST digits and furthermore that by visualizing the learned attention
    maps we can get a sense of what data the model is using to make its decisions.
    If the trained model keeps misclassifying a particular image, we can inspect its
    attention map and see if perhaps it is getting distracted by some noise.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明，我们的关系模型在分类MNIST数字的简单任务上表现良好，并且通过可视化学习到的注意力图，我们可以了解模型在做出决策时所使用的数据。如果训练好的模型持续错误地分类某个特定图像，我们可以检查其注意力图，看看它是否可能被一些噪声分散了注意力。
- en: One problem with the self-attention mechanism we’ve employed so far is that
    it severely constrains the amount of data that can be transmitted due to the softmax.
    If the input had hundreds or thousands of nodes, the model would only be able
    to put attention weight on a very small subset of those, and it may not be enough.
    We want to be able to bias the model toward learning relationships, which the
    softmax helps promote, but we don’t want to necessarily limit the amount of data
    that can pass through the self-attention layer.
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前使用的自注意力机制的一个问题是，由于softmax的存在，它严重限制了可以传输的数据量。如果输入有数百或数千个节点，模型只能对其中非常小的一部分节点分配注意力权重，这可能是不够的。我们希望模型能够偏向于学习关系，softmax有助于促进这一点，但我们不希望必然限制可以通过自注意力层传输的数据量。
- en: In effect, we need a way to increase the bandwidth of the self-attention layer
    without fundamentally altering its behavior. To address this issue, we’ll allow
    our model to have multiple attention *heads*, meaning that the model learns multiple
    attention maps that operate independently and are later combined ([figure 10.18](#ch10fig18)).
    One attention head might focus on a particular region or features of the input,
    whereas another head would focus elsewhere. This way we can increase the bandwidth
    through the attention layer but we can still keep the interpretability and relational
    learning intact. In fact, multi-head attention can improve interpretability because
    within each attention head, each node can more strongly focus on a smaller subset
    of other nodes rather than having to spread its attention more thinly. Thus, multi-head
    attention can give us a better idea of which nodes are strongly related.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们需要一种方法来增加自注意力层的带宽，而不改变其基本行为。为了解决这个问题，我们将允许我们的模型拥有多个注意力**头**，这意味着模型学习多个独立操作的注意力图，这些图随后被组合([图10.18](#ch10fig18))。一个注意力头可能专注于输入的特定区域或特征，而另一个头则可能专注于其他地方。这样我们就可以通过注意力层增加带宽，同时仍然保持可解释性和关系学习。事实上，多头注意力可以提高可解释性，因为在每个注意力头中，每个节点可以更强烈地关注其他节点的一个更小的子集，而不是需要更分散地分配其注意力。因此，多头注意力可以让我们更好地了解哪些节点之间关系紧密。
- en: Figure 10.18\. Multi-head dot product attention (MHDPA). Rather than use a single
    attention matrix, we can have multiple attention matrices called heads that can
    independently attend to different aspects of an input. The only difference is
    adding a new head dimension to the query, key and value tensors.
  id: totrans-1465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.18\. 多头点积注意力(MHDPA)。我们不是使用单个注意力矩阵，而是可以拥有多个称为头的注意力矩阵，这些矩阵可以独立地关注输入的不同方面。唯一的区别是向查询、键和值张量添加一个新的头维度。
- en: '![](10fig18_alt.jpg)'
  id: totrans-1466
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig18_alt.jpg)'
- en: With multi-head attention, the utility of Einsum becomes even more obvious as
    we will be operating on 4-tensors of dimension *batch* × *head* × *number of nodes*
    × *features*. Multi-head attention will not be particularly useful for MNIST because
    the input space is already small and sparse enough that a single attention head
    has enough bandwidth and interpretability. Hence, this is a good time to introduce
    our reinforcement learning task for this chapter. Because the relational module
    is the most computationally expensive model we’ve implemented in this book so
    far, we want to use a simple environment that still demonstrates the power of
    relational reasoning and interpretability in reinforcement learning.
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 在多头注意力中，Einsum的效用变得更加明显，因为我们将在4维张量上操作，其维度为*批处理* × *头* × *节点数* × *特征*。对于MNIST来说，多头注意力不会特别有用，因为输入空间已经足够小且稀疏，单个注意力头就有足够的带宽和可解释性。因此，现在是介绍本章强化学习任务的好时机。由于关系模块是我们迄今为止在本书中实现的最计算密集型的模型，我们希望使用一个简单但仍然能够展示关系推理和可解释性在强化学习中的力量的环境。
- en: 'We will be coming full circle and returning to a Gridworld environment that
    we first encountered in [chapter 3](kindle_split_012.html#ch03). But the Gridworld
    environment we will be using in this chapter is much more sophisticated. We’ll
    be using the MiniGrid library found on GitHub at [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid);
    it is implemented as an OpenAI Gym environment. It includes a wide variety of
    different kinds of Gridworld environments of varying complexity and difficulty.
    Some of these Gridworld environments are so difficult (largely due to sparse rewards)
    that only the most cutting-edge reinforcement learning algorithms are capable
    of making headway. Install the package using `pip`:'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到起点，回到我们在[第3章](kindle_split_012.html#ch03)中首次遇到的Gridworld环境。但本章我们将使用的Gridworld环境要复杂得多。我们将使用GitHub上找到的MiniGrid库[https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid)；它被实现为一个OpenAI
    Gym环境。它包括各种不同复杂性和难度的Gridworld环境。其中一些Gridworld环境非常困难（很大程度上是由于稀疏奖励），只有最前沿的强化学习算法才能取得进展。使用`pip`安装该包：
- en: '[PRE97]'
  id: totrans-1469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: We will be using a somewhat difficult environment in which the Gridworld agent
    must navigate to a key, pick it up, use it to open a door, and then navigate to
    a goalpost in order to receive a positive reward ([figure 10.19](#ch10fig19)).
    This is a lot of steps before it ever sees a reward, so we will encounter the
    sparse reward problem. This would actually be a great opportunity to employ curiosity-based
    learning, but we will restrict ourselves to the smallest version of the grid,
    the MiniGrid, so that even a random agent would eventually find the goal, so we
    can successfully train without curiosity. For the larger grid variants of this
    environment, curiosity or related approaches would be almost necessary.
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个相对困难的环境，在这个环境中，Gridworld智能体必须导航到钥匙，捡起它，使用它来打开门，然后导航到目标柱子以获得正奖励（[图10.19](#ch10fig19)）。在看到奖励之前，它需要完成很多步骤，因此我们将遇到稀疏奖励问题。这实际上是一个应用基于好奇心的学习的好机会，但我们将限制自己使用网格的最小版本，即MiniGrid，这样即使是随机智能体最终也能找到目标，因此我们可以成功训练而不需要好奇心。对于这个环境的更大网格变体，好奇心或相关方法几乎是必要的。
- en: Figure 10.19\. The MiniGrid-DoorKey environment. In this environment, the agent
    (the triangle) must first navigate to the key, pick it up, navigate to the door
    (the hollow square), open it, and then navigate to the solid square. Each game
    initializes the objects on the grid randomly, and the agent only has a partial
    view of the grid indicated by the highlighted region around it.
  id: totrans-1471
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.19。MiniGrid-DoorKey环境。在这个环境中，智能体（三角形）必须首先导航到钥匙，捡起它，导航到门（空心正方形），打开它，然后导航到实心正方形。每个游戏都会随机初始化网格上的对象，而智能体只能看到它周围的突出区域所指示的网格的部分视图。
- en: '![](10fig19.jpg)'
  id: totrans-1472
  prefs: []
  type: TYPE_IMG
  zh: '![图10.19](10fig19.jpg)'
- en: There are a few other complexities to the MiniGrid set of environments. One
    is that they are partially observable environments, meaning the agent cannot see
    the whole grid but only a small region immediately surrounding it. Another is
    that the agent does not simply move left, right, up, and down but has an orientation.
    The agent can only move forward, turn left, or turn right; it is always oriented
    in a particular direction and must turn around before moving backward, for example.
    The agent’s partial view of the environment is *egocentric*, meaning the agent
    sees the grid as if it were facing it. When the agent changes direction without
    moving position, its view changes. The state we receive from the environment is
    a 7 × 7 × 3 tensor, so the agent only sees a 7 × 7 subregion of the grid in front
    of it; the last (channel) dimension of the state encodes which object (if any)
    is present at that position.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: MiniGrid环境集还有一些其他复杂性。其中一个是它们是部分可观察环境，这意味着智能体无法看到整个网格，而只能看到围绕它的小区域。另一个是智能体不仅简单地左右上下移动，还具有方向性。智能体只能向前移动、左转或右转；它总是面向特定方向，并且在向后移动之前必须转身，例如。智能体对环境的部分视图是**以自我为中心**的，意味着智能体将网格视为它面对的样子。当智能体改变方向而不改变位置时，其视图会改变。我们从环境中接收到的状态是一个7
    × 7 × 3的张量，因此智能体只能看到它前面的7 × 7子区域的网格；状态的最后一个（通道）维度编码了在该位置是否存在某个对象（如果有的话）。
- en: This Gridworld environment is a good testbed for our relational module because
    in order to successfully learn how to play, the agent must learn how to relate
    the key to the lock and the lock to being able to access the goal, which is all
    a form of relational reasoning. In addition, the game is naturally represented
    by a set of objects (or nodes), since each “pixel” position in the grid really
    is an actual object, unlike in the MNIST example. This means we can see exactly
    which objects the agent is paying attention to. We might hope it learns to pay
    most attention to the key, door, and goal square, and that the key is related
    to the door. If this turns out to be the case, it suggests the agent is learning
    not too differently than how a human would learn how to relate the objects on
    the grid.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Gridworld环境是我们关系模块的良好测试平台，因为为了成功学习如何玩游戏，智能体必须学习如何将钥匙与锁相关联，将锁与能够访问目标相关联，这些都是关系推理的形式。此外，游戏自然地表示为一组对象（或节点），因为网格中的每个“像素”位置实际上都是一个真实对象，这与MNIST示例不同。这意味着我们可以确切地看到智能体正在关注哪些对象。我们可能希望它学会最关注钥匙、门和目标正方形，并且钥匙与门相关联。如果这种情况发生，这表明智能体学习的方式并不太不同于人类学习如何将网格上的对象相关联的方式。
- en: 'Overall we will repurpose the relational module we created earlier for the
    MNIST example as a relational DQN, so we really only need to change the output
    to a normal activation function rather than the `log_softmax` we used for classification.
    But first, let’s get back to implementing multi-head attention. As operating on
    higher-order tensors gets more complicated, we will get help from a package called
    Einops that extends the capabilities of PyTorch’s built-in Einsum function. You
    can install it using `pip`:'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们将重新使用我们之前为 MNIST 示例创建的关系模块作为关系 DQN，所以我们实际上只需要将输出更改为正常的激活函数，而不是我们用于分类的
    `log_softmax`。但首先，让我们回到实现多头注意力的实现。随着对高阶张量的操作变得更加复杂，我们将从扩展 PyTorch 内置 Einsum 函数功能的
    Einops 包中获得帮助。你可以使用 `pip` 安装它：
- en: '[PRE98]'
  id: totrans-1476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'There are only two important functions in this package (`rearrange` and `reduce`),
    and we will only use one, the `rearrange` function. `rearrange` basically lets
    us reshape the dimensions of a higher-order tensor more easily and readably than
    the built-in PyTorch functions, and it has a syntax similar to Einsum. For example,
    we can reorder the dimensions of a tensor like this:'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包中只有两个重要的函数（`rearrange` 和 `reduce`），我们将只使用一个，即 `rearrange` 函数。`rearrange`
    函数基本上让我们能够比内置的 PyTorch 函数更容易、更清晰地重塑高阶张量的维度，并且它的语法类似于 Einsum。例如，我们可以这样重新排列张量的维度：
- en: '[PRE99]'
  id: totrans-1478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Or if we had collapsed the spatial dimensions *h* and *w* into a single dimension
    *N* for nodes, we can undo this:'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们把节点维度 *h* 和 *w* 收缩成一个单一的维度 *N*，我们可以撤销这个操作：
- en: '[PRE100]'
  id: totrans-1480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: In this case, we tell it that the input has three dimensions but the second
    dimension is secretly two dimensions (*h*, *w*) that were collapsed, and we want
    to extract them out into separate dimensions again. We only need to tell it the
    size of *h* or *w*, and it can infer the size of the other dimension.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们告诉它输入有三个维度，但第二个维度实际上是两个维度（*h*，*w*）被合并了，并且我们希望将它们提取出来作为单独的维度。我们只需要告诉它
    *h* 或 *w* 的大小，它就可以推断出其他维度的大小。
- en: 'The main change for multi-head attention is that when we project the initial
    node matrix *N*:![](icon-r.jpg)*^b*^×*^n*^×*^f* into key, query, and value matrices,
    we add an extra head dimension: *Q*,*K*,*V*:![](icon-r.jpg)*^b*^×*^h*^×*^n*^×*^d*,
    where *b* is the batch dimension, and *h* is the head dimension. We will (arbitrarily)
    set the number of heads to be 3 for this example, so *h* = 3, *n* = 7 * 7 = 49,
    *d* = 64, where *n* is the number of nodes (which is just the total number of
    grid positions in view), and *d* is the dimensionality of the node feature vectors,
    which is just something we choose empirically to be 64, but smaller or larger
    values might work just as well.'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力的主要变化在于，当我们把初始节点矩阵 *N*:![](icon-r.jpg)*^b*^×*^n*^×*^f* 投影到键、查询和值矩阵时，我们增加了一个额外的头维度：*Q*、*K*、*V*:![](icon-r.jpg)*^b*^×*^h*^×*^n*^×*^d*，其中
    *b* 是批处理维度，*h* 是头维度。在这个例子中，我们将（任意地）将头数设置为 3，因此 *h* = 3，*n* = 7 * 7 = 49，*d* =
    64，其中 *n* 是节点的数量（这仅仅是视图中网格位置的总数），*d* 是节点特征向量的维度，这仅仅是我们根据经验选择的 64，但较小的或较大的值也可能同样有效。
- en: We will need to do a tensor contraction between the query and key tensors to
    get an attention tensor, *A*:![](icon-r.jpg)*^b*^×*^h*^×*^n*^×*^n*, pass it through
    a softmax, contract this with the value tensor, collapse the head dimension with
    the last *n* dimension, and contract the last (collapsed) dimension with a linear
    layer to get our updated node tensor, *N*:![](icon-r.jpg)*^b*^×*^n*^×*^d*, which
    we can then pass through another self-attention layer or collapse all the nodes
    into a single vector and pass it through some linear layers to the final output.
    We will stick with a single-attention layer for all examples.
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在查询和键张量之间进行张量收缩以获得一个注意力张量 *A*:![](icon-r.jpg)*^b*^×*^h*^×*^n*^×*^n*，通过 softmax
    传递它，与值张量进行收缩，将头维度与最后一个 *n* 维度合并，然后将最后一个（合并的）维度与一个线性层进行收缩以获得我们的更新节点张量 *N*:![](icon-r.jpg)*^b*^×*^n*^×*^d*，然后我们可以将其通过另一个自注意力层，或者将所有节点合并成一个向量并通过一些线性层传递到最终输出。我们将对所有示例坚持使用单个注意力层。
- en: First we’ll go over some specific lines in the code that are different from
    the single-head attention model; the full model is reproduced in [listing 10.7](#ch10ex07).
    In order to use PyTorch’s built-in linear layer module (which is just a matrix
    multiplication plus a bias vector), we will create a linear layer where the last
    dimension size is expanded by the number of attention heads.
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论一些与单头注意力模型不同的代码中的特定行；完整模型在[列表10.7](#ch10ex07)中重现。为了使用PyTorch的内置线性层模块（这只是一个矩阵乘法加上一个偏置向量），我们将创建一个线性层，其中最后一个维度的尺寸通过注意力头的数量扩展。
- en: '[PRE101]'
  id: totrans-1485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: We make three separate, ordinary linear layers just as we did for the single-head
    attention model, but this time we’ll expand the last dimension by multiplying
    it by the number of attention heads. The input to these projection layers is a
    batch of initial node matrices, *N*:![](icon-r.jpg)*^b*^×*^n*^×*^c*, and the *c*
    dimension is equal to the output channel dimension of the last convolutional layer
    plus the two spatial coordinates that we append. The linear layer thus contracts
    over the channel dimension to give us query, key, and value matrices, *Q*,*K*,*V*:![](icon-r.jpg)*^b*^×*^n*^(×()*^h*^×*^d*^),
    so we will use the Einops `rearrange` function to expand out the last dimension
    into head and *d* dimensions.
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了三个独立的、普通的线性层，就像我们在单头注意力模型中所做的那样，但这次我们将最后一个维度通过乘以注意力头的数量来扩展。这些投影层的输入是一个批次的初始节点矩阵*N*:![](icon-r.jpg)*^b*^×*^n*^×*^c*，其中*c*维度等于最后一个卷积层的输出通道维度加上我们附加的两个空间坐标。因此，线性层在通道维度上收缩，给我们查询、键和值矩阵*Q*、*K*、*V*:![](icon-r.jpg)*^b*^×*^n*^(×()*^h*^×*^d*^)，因此我们将使用Einops的`rearrange`函数将最后一个维度展开成头和*d*维度。
- en: '[PRE102]'
  id: totrans-1487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: We will extract out the separate head and *d* dimension and simultaneously reorder
    the dimensions so that the head dimension comes after the batch dimension. Without
    Einops, this would be more code and not nearly as readable.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提取出单独的头和*d*维度，并同时重新排列维度，使得头维度位于批处理维度之后。如果没有Einops，这将需要更多的代码，并且可读性远不如现在。
- en: For this example, we will also abandon the dot (inner) product as the compatibility
    function (recall, this is the function that determines the similarity between
    the query and keys) and instead use something called *additive attention* ([figure
    10.20](#ch10fig20)). The dot product attention would work fine, but we wanted
    to illustrate that it is not the only kind of compatibility function, and the
    additive function is actually a bit more stable and expressive.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们还将放弃点积（内积）作为兼容函数（回想一下，这是确定查询和键之间相似性的函数），而改用一种称为*加性注意力*的东西（[图10.20](#ch10fig20)）。点积注意力可以很好地工作，但我们想说明它不是唯一的一种兼容函数，而加性函数实际上更加稳定和表达能力强。
- en: Figure 10.20\. The compatibility function computes the similarity between each
    key and query vector, resulting in an adjacency matrix.
  id: totrans-1490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.20。兼容函数计算每个键和查询向量之间的相似性，从而得到一个邻接矩阵。
- en: '![](10fig20.jpg)'
  id: totrans-1491
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig20.jpg)'
- en: With dot product attention, we compute the compatibility (i.e., the similarity)
    between each query and key by simply taking the dot product between each vector.
    When the two vectors are similar element-wise, the dot product will yield a large
    positive value, and when they are dissimilar, it may yield a value near zero or
    a big negative value. This means the output of the (dot product) compatibility
    function is unbounded in both directions, and we can get arbitrarily large or
    small values. This can be problematic when we then pass it through the softmax
    function, which can easily saturate. By *saturate* we mean that when a particular
    value in an input vector is dramatically larger than other values in the vector,
    the softmax may assign all its probability mass to that single value, setting
    all the others to zero, or vice versa. This can make our gradients too large or
    too small for particular values and destabilize training.
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 在点积注意力中，我们通过简单地取每个向量之间的点积来计算每个查询和键之间的兼容性（即相似性）。当两个向量在元素级别上相似时，点积将产生一个大的正值，而当它们不相似时，可能产生接近零或很大的负值。这意味着（点积）兼容函数的输出在两个方向上都是无界的，我们可以得到任意大或小的值。当我们将其通过softmax函数时，这可能会变得有问题，因为softmax函数很容易饱和。当我们说*saturate*时，我们的意思是，当输入向量中的一个特定值显著大于向量中的其他值时，softmax可能会将所有的概率质量分配给那个单一值，将所有其他值设为零，反之亦然。这可能会使我们的梯度对于特定值过大或过小，从而不稳定训练。
- en: Additive attention can solve this problem at the expense of introducing additional
    parameters. Instead of simply multiplying the *Q* and *K* tensors together, we’ll
    instead pass them both through independent linear layers, add them together, and
    then apply an activation function, followed by another linear layer ([figure 10.21](#ch10fig21)).
    This allows for a more complex interaction between *Q* and *K* without as much
    risk of causing numerical instability, since addition will not exaggerate numerical
    differences like multiplication does. First we need to add three new linear layers
    for the additive attention.
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: 加性注意力可以通过引入额外的参数来解决这个问题。我们不会简单地相乘 *Q* 和 *K* 张量，而是将它们都通过独立的线性层，然后相加，并应用激活函数，随后通过另一个线性层
    ([图10.21](#ch10fig21))。这允许 *Q* 和 *K* 之间有更复杂的交互，同时风险较小，因为加法不会像乘法那样夸大数值差异。首先，我们需要为加性注意力添加三个新的线性层。
- en: Figure 10.21\. Additive attention is an alternative to dot product attention
    that can be more stable. Instead of multiplying the queries and keys together,
    we first pass them independently through linear layers and then add them together,
    apply a nonlinear function, and pass through another linear layer to change the
    dimensionality.
  id: totrans-1494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.21\. 加性注意力是点积注意力的一种替代方案，它可以更稳定。我们不是将查询和键相乘，而是首先将它们独立地通过线性层，然后相加，应用非线性函数，并通过另一个线性层改变维度。
- en: '![](10fig21_alt.jpg)'
  id: totrans-1495
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig21_alt.jpg)'
- en: '[PRE103]'
  id: totrans-1496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'In the forward method we define the actual computation steps for additive attention:'
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向方法中，我们定义了加性注意力的实际计算步骤：
- en: '[PRE104]'
  id: totrans-1498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: As you can see, we pass *Q* through a linear layer and *K* through its own linear
    layer, add them together, and then apply a nonlinear activation function. Then
    we pass this result through another linear layer and lastly apply the softmax
    across the node rows, which finally yields the attention weight tensor.
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们将 *Q* 通过一个线性层，将 *K* 通过其自己的线性层，然后相加，并应用非线性激活函数。然后我们通过另一个线性层，最后在节点行上应用softmax，最终得到注意力权重张量。
- en: Now we do the same as before and contract the attention tensor with the *V*
    tensor along the last *n* dimension to get a tensor with dimensions *b* × *h*
    × *n* × *d*, which is a multi-headed node matrix.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们和之前一样，将注意力张量与 *V* 张量在最后一个 *n* 维度上收缩，得到一个尺寸为 *b* × *h* × *n* × *d* 的张量，这是一个多头节点矩阵。
- en: '[PRE105]'
  id: totrans-1501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: What we want at the end of the self-attention module is an updated node matrix
    with dimensions *b* × *n* × *d*, so we will concatenate or collapse the head dimension
    and *d* dimension, and then pass this through a linear layer to reduce the dimensionality
    back down to size *d*.
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力模块的末尾，我们希望得到一个尺寸为 *b* × *n* × *d* 的更新节点矩阵，因此我们将头维度和 *d* 维度连接或折叠，然后通过一个线性层将维度降低到
    *d* 的大小。
- en: '[PRE106]'
  id: totrans-1503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: The final shape of this is now *b* × *n* × *d*, which is exactly what we want.
    Since we’re only going to use a single self-attention module, we want to reduce
    this 3-tensor into a 2-tensor of just a batch of vectors, so we will maxpool over
    the *n* dimension, and then pass the result through a final linear layer, which
    represents the Q values.
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个形状现在是 *b* × *n* × *d*，这正是我们想要的。由于我们只将使用一个自注意力模块，我们希望将这个3张量减少到一个2张量，仅包含一个批次的向量，因此我们将对
    *n* 维度进行maxpool操作，然后将结果通过一个最终的线性层，这代表了Q值。
- en: '[PRE107]'
  id: totrans-1505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: That’s it. We just went through all the core lines of code, but let’s see it
    all together and test it out.
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们刚刚走过了所有核心代码行，但让我们一起看看并测试一下。
- en: Listing 10.7\. Multi-head relational module
  id: totrans-1507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.7\. 多头关系模块
- en: '[PRE108]'
  id: totrans-1508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '***1*** We use 1 × 1 convolutions to preserve the spatial organization of the
    objects in the grid.'
  id: totrans-1509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们使用1 × 1卷积来保留网格中对象的空间组织。'
- en: '***2*** Sets up linear layers for additive attention'
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 为加性注意力设置线性层'
- en: '***3*** Saves a copy of the post-convolved input for later visualization'
  id: totrans-1511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 保存了卷积后的输入副本以供后续可视化'
- en: '***4*** Additive attention'
  id: totrans-1512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 加性注意力'
- en: '***5*** Saves a copy of the attention weights for later visualization'
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 保存了注意力权重的副本以供后续可视化'
- en: '***6*** Batch-matrix multiplies the attention weight matrix with the node matrix
    to get an updated node matrix.'
  id: totrans-1514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 批量矩阵乘法将注意力权重矩阵与节点矩阵相乘，以获得更新的节点矩阵。'
- en: '***7*** Collapses the head dimension with the feature d dimension'
  id: totrans-1515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将头维度与特征维度 *d* 折叠'
- en: 10.5\. Double Q-learning
  id: totrans-1516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5\. 双重Q学习
- en: Now let’s get to training it. Because this Gridworld environment has sparse
    rewards, we need to make our training process as smooth as possible, especially
    since we’re not using curiosity-based learning.
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来训练它。因为这个Gridworld环境有稀疏的奖励，我们需要尽可能使我们的训练过程平滑，尤其是因为我们没有使用基于好奇心的学习。
- en: 'Remember back in [chapter 3](kindle_split_012.html#ch03) when we introduced
    Q-learning and a target network to stabilize training? If not, the idea was that
    in ordinary Q-learning we compute the target Q value with this equation:'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第3章](kindle_split_012.html#ch03)中我们引入Q学习和目标网络来稳定训练吗？如果不记得，那么这个想法是在普通的Q学习中，我们用这个方程来计算目标Q值：
- en: '| *Q[new]* = *r[t]* + γ × max(*Q*(*s[t]*[+1])) |'
  id: totrans-1519
  prefs: []
  type: TYPE_TB
  zh: '| *Q[new]* = *r[t]* + γ × max(*Q*(*s[t]*[+1])) |'
- en: The problem with this is that every time we update our DQN according to this
    equation so that its predictions get closer to this target, the *Q*(*s[t]*[+1])
    is changed, which means the next time we go to update our Q function, the target
    *Q[new]* is going to be different even for the same state. This is problematic
    because as we train the DQN, its predictions are chasing a moving target, leading
    to very unstable training and poor performance. To stabilize training, we create
    a duplicate Q function called the *target function* that we can denote *Q*′, and
    we use the value *Q′*(*s[t]*[+1]) to plug into the equation and update the main
    Q function.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在于，每次我们根据这个方程更新我们的DQN，使其预测更接近这个目标时，*Q*(*s[t]*[+1])都会发生变化，这意味着下一次我们去更新我们的Q函数时，即使是相同的状态，目标*Q[new]*也会不同。这是问题所在，因为当我们训练DQN时，它的预测是在追逐一个移动的目标，导致训练非常不稳定和性能不佳。为了稳定训练，我们创建了一个名为*目标函数*的副本Q函数，我们可以用*Q*′表示，我们使用*Q′*(*s[t]*[+1])的值来插入方程并更新主Q函数。
- en: '| *Q[new]* = *r[t]* + γ × max(*Q*′(*s[t]*[+1])) |'
  id: totrans-1521
  prefs: []
  type: TYPE_TB
  zh: '| *Q[new]* = *r[t]* + γ × max(*Q*′(*s[t]*[+1])) |'
- en: We only train (and hence backpropagate into) the main Q function, but we copy
    the parameters from the main Q function to the target Q function, *Q*′, every
    100 (or some other arbitrary number of) epochs. This greatly stabilizes training
    because the main Q function is no longer chasing a constantly moving target, but
    a relatively fixed target.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只训练（因此反向传播到）主Q函数，但每隔100（或任何其他任意数量的）epoch，我们将主Q函数的参数复制到目标Q函数，*Q*′。这极大地稳定了训练，因为主Q函数不再追逐一个不断移动的目标，而是一个相对固定的目标。
- en: But that’s not all that’s wrong with that simple update equation. Because it
    involves the max function, i.e., we select the maximum predicted Q value for the
    next state, it leads our agent to overestimate Q values for actions, which can
    impact training especially early on. If the DQN takes action 1 and learns an erroneously
    high Q value for action 1, that means action 1 is going to get selected more often
    in subsequent epochs, further causing it to be overestimated, which again leads
    to training instability and poor performance.
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个简单的更新方程的问题还不止这些。因为它涉及到最大函数，即我们选择下一个状态的最大预测Q值，这会导致我们的智能体高估动作的Q值，这可能会在训练初期对训练产生重大影响。如果DQN采取动作1并学习到一个错误的高Q值，这意味着动作1在后续的epoch中将被更频繁地选择，这进一步导致它被高估，这又会导致训练不稳定和性能不佳。
- en: To mitigate this problem and get more accurate estimates for Q values, we will
    implement double Q-learning, which solves the problem by disentangling action-value
    estimation from action selection, as you will see. A *double deep Q-network* (DDQN)
    involves a simple modification to normal Q-learning with a target network. As
    usual, we use the main Q-network to select actions using an epsilon-greedy policy.
    But when it comes time to compute *Q[new]*, we will first find the argmax of Q
    (the main Q-network). Let’s say argmax(*Q*(*s[t]*[+1])) = 2, so action 2 is associated
    with the highest action value in the next state given the main Q function. We
    then use this to index into the target network, *Q*′, to get the action value
    we will use in the update equation.
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这个问题并获得Q值的更准确估计，我们将实现双Q学习，它通过将动作值估计与动作选择分离来解决这个问题，正如你将看到的。一个*双深度Q网络*（DDQN）涉及对正常Q学习的简单修改，并使用目标网络。像往常一样，我们使用主Q网络通过ε-贪婪策略选择动作。但是当计算*Q[new]*的时候，我们首先找到Q的argmax（主Q网络）。假设argmax(*Q*(*s[t]*[+1]))
    = 2，这意味着动作2与主Q函数给出的下一个状态的最高动作值相关联。然后我们使用这个值来索引目标网络，*Q*′，以获取我们在更新方程中使用的动作值。
- en: '| *a* = argmax(*Q*(*s[t]*[+1])) *x* = *Q*′(*s[t]*[+1])[*a*]'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: '| *a* = argmax(*Q*(*s[t]*[+1])) *x* = *Q*′(*s[t]*[+1])[*a*]'
- en: '*Q[new]* = *r[t]* + γ × *x* |'
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q[new]* = *r[t]* + γ × *x* |'
- en: 'We’re still using the Q value from the target network, *Q*′, but we don’t choose
    the highest Q value from *Q*′; we choose the Q value in *Q*′ based on the action
    associated with the highest Q value in the main Q function. In code:'
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然使用目标网络的 Q 值，*Q*′，但我们不选择 *Q*′ 中的最高 Q 值；我们根据与主 Q 函数中最高 Q 值相关联的动作在 *Q*′ 中选择
    Q 值。在代码中：
- en: '[PRE109]'
  id: totrans-1528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The `get_qtarget_ddqn` function just computes *Q[new]* = *r[t]* + γ × *x*:'
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_qtarget_ddqn` 函数仅计算 *Q[new]* = *r[t]* + γ × *x*:'
- en: '[PRE110]'
  id: totrans-1530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: We provide `done`, which is a Boolean, because if the episode of the game is
    done, there is no next state on which to compute *Q*(*s[t]*[+1]), so we just train
    on *r[t]* and set the rest of the equation to 0.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了 `done`，这是一个布尔值，因为如果游戏的回合结束，就没有下一个状态来计算 *Q*(*s[t]*[+1])，所以我们只训练 *r[t]*
    并将方程的其余部分设置为 0。
- en: That’s all there is to double Q-learning; just another simple way to improve
    training stability and performance.
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: 双重 Q-learning 的内容仅此而已；这只是另一种简单的方法来提高训练的稳定性和性能。
- en: 10.6\. Training and attention visualization
  id: totrans-1533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6\. 训练和注意力可视化
- en: We have most of the pieces now, but we need a few other helper functions before
    training.
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经有了大部分组件，但在训练之前，我们还需要一些其他辅助函数。
- en: Listing 10.8\. Preprocessing functions
  id: totrans-1535
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.8\. 预处理函数
- en: '[PRE111]'
  id: totrans-1536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '***1*** Normalizes the state tensor and converts to PyTorch tensor'
  id: totrans-1537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 标准化状态张量并将其转换为 PyTorch 张量'
- en: '***2*** Gets a random mini-batch from the experience replay memory'
  id: totrans-1538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从经验回放内存中获取随机的小批量数据'
- en: '***3*** Calculates the target Q value'
  id: totrans-1539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 计算目标 Q 值'
- en: These functions just prepare the state observation tensor, produce a mini-batch
    and calculate the target Q value as we discussed earlier.
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数只是准备状态观察张量，生成小批量数据，并计算我们之前讨论过的目标 Q 值。
- en: In [listing 10.9](#ch10ex09) we define the loss function we will use and also
    a function to update the experience replay.
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [代码列表 10.9](#ch10ex09) 中，我们定义了我们将使用的损失函数以及一个更新经验回放的功能。
- en: Listing 10.9\. Loss function and updating the replay
  id: totrans-1542
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.9\. 损失函数和更新回放
- en: '[PRE112]'
  id: totrans-1543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '***1*** Loss function'
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 损失函数'
- en: '***2*** Adds new experiences to the experience replay memory; if the reward
    is positive, we add 50 copies of the memory.'
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将新的经验添加到经验回放内存中；如果奖励是正的，我们添加 50 个该内存的副本。'
- en: '***3*** Maps the action outputs of the DQN to a subset of actions in the environment'
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将 DQN 的动作输出映射到环境中的动作子集'
- en: The `update_replay` function adds new memories to the experience replay if it
    is not yet full; if it is full, it will replace random memories with new ones.
    If the memory resulted in a positive reward, we add 50 copies of that memory,
    since positive reward memories are rare and we want to enrich the experience replay
    with these more important memories.
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_replay` 函数在经验回放未满时添加新的记忆；如果已满，它将用新的记忆替换随机的记忆。如果记忆导致了正奖励，我们添加 50 个该记忆的副本，因为正奖励的记忆很少见，我们希望用这些更重要的记忆丰富经验回放。'
- en: All the MiniGrid environments have seven actions, but in the environment we
    will use in this chapter, we only need to use five of the seven actions, so we
    use a dictionary to translate from the output of DQN, which will produce actions
    0–4, to the appropriate actions in the environment, which are {0,1,2,3,5}.
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 MiniGrid 环境都有七个动作，但在这章我们将使用的环境中，我们只需要使用七个动作中的五个，所以我们使用一个字典将 DQN 的输出转换为环境中的适当动作，这些动作是
    {0,1,2,3,5}。
- en: 'The MiniGrid’s action names and corresponding action numbers are listed here:'
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
  zh: MiniGrid 的动作名称和相应的动作编号如下所示：
- en: '[PRE113]'
  id: totrans-1550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: In [listing 10.10](#ch10ex10) we jump into the main training loop of the algorithm.
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [代码列表 10.10](#ch10ex10) 中，我们进入算法的主训练循环。
- en: Listing 10.10\. The main training loop
  id: totrans-1552
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.10\. 主训练循环
- en: '[PRE114]'
  id: totrans-1553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '***1*** Sets up environment'
  id: totrans-1554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置环境'
- en: '***2*** Creates main relational DQN'
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建主关系 DQN'
- en: '***3*** Creates target DQN'
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建目标 DQN'
- en: '***4*** Sets the maximum steps before game will end'
  id: totrans-1557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 设置游戏结束前的最大步数'
- en: '***5*** Creates the experience replay memory'
  id: totrans-1558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 创建经验回放内存'
- en: '***6*** Uses an epsilon-greedy policy for action selection'
  id: totrans-1559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用 ε-贪婪策略进行动作选择'
- en: '***7*** Rescales the reward to be slightly negative on nonterminal states'
  id: totrans-1560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 将奖励重新缩放，使非终端状态略微为负'
- en: '***8*** Clips the gradients to prevent overly large gradients'
  id: totrans-1561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 剪裁梯度以防止梯度过大'
- en: '***9*** Synchronizes the main DQN with the target DQN every 100 steps'
  id: totrans-1562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 每 100 步同步主 DQN 和目标 DQN'
- en: Our self-attention double DQN reinforcement learning algorithm will learn how
    to play fairly well after about 10,000 epochs, but it may take up to 50,000 epochs
    before it reaches maximum accuracy.
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自注意力双DQN强化学习算法在大约10,000个epoch后将会学会如何玩得相当好，但可能需要达到50,000个epoch才能达到最大准确度。
- en: '[Figure 10.22](#ch10fig22) shows the log-loss plot we get, and we also plotted
    the average episode length. As the agent learns to play, it should be able to
    solve the games in fewer and fewer steps.'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.22](#ch10fig22)显示了我们所得到的对数损失图，我们还绘制了平均剧集长度。随着代理学习玩游戏，它应该能够在更少的步骤中解决问题。'
- en: 'Figure 10.22\. Top: Log-loss plot during training. The loss drops quickly in
    the beginning, increases a bit, and then very slowly begins decreasing again.
    Bottom: Average episode length. This gives us a better idea of the performance
    of the agent since we can clearly see it is solving the episodes in a shorter
    number of steps during training.'
  id: totrans-1565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.22\. 顶部：训练过程中的对数损失图。损失在开始时迅速下降，然后略有上升，然后非常缓慢地再次开始下降。底部：平均剧集长度。这让我们更好地了解了代理的性能，因为我们可以在训练过程中清楚地看到它正在用更短的步骤解决剧集。
- en: '![](10fig22_alt.jpg)'
  id: totrans-1566
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig22_alt.jpg)'
- en: If you test the trained algorithm, it should be able to solve ≥ 94% of the episodes
    within the maximum step limit. We even recorded video frames during training,
    and the agent clearly knows what it is doing when you watch it in real time. We
    have omitted a lot of this accessory code to keep the text clear; please see the
    GitHub repository for the complete code.
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你测试训练好的算法，它应该能够在最大步数限制内解决≥94%的剧集。我们在训练过程中甚至记录了视频帧，当你在实时观看时，代理显然知道它在做什么。我们省略了大量的辅助代码以保持文本清晰；请参阅GitHub仓库以获取完整代码。
- en: 10.6.1\. Maximum entropy learning
  id: totrans-1568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.1\. 最大熵学习
- en: We are using an epsilon-greedy policy with epsilon set to 0.5, so the agent
    is taking random actions 50% of the time. We tested using a number of different
    epsilon levels but found 0.5 to be about the best. If you train the agent with
    epsilon values ranging from a low of 0.01, to 0.1, to 0.2, all the way to a high
    of say 0.95, you will notice the training performance follows an inverted-U curve,
    where too low a value for epsilon leads to poor learning due to lack of exploration,
    and too high a value for epsilon leads to poor learning due to lack of exploitation.
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是ε-贪婪策略，其中ε设置为0.5，因此代理有50%的时间采取随机行动。我们使用不同的ε水平进行了测试，但发现0.5大约是最好的。如果你用ε值从0.01到0.1，再到0.2，一直到最后0.95的高值来训练代理，你会注意到训练性能遵循一个倒U曲线，其中ε值太低会导致由于缺乏探索而学习效果差，而ε值太高会导致由于缺乏利用而学习效果差。
- en: How can the agent perform so well even though it is acting randomly half the
    time? By setting the epsilon to be as high as possible until it degrades performance,
    we are utilizing an approximation to the principle of maximum entropy, or *maximum
    entropy learning*.
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 即使代理有一半的时间在随机行动，它如何表现得如此出色？通过将ε设置得尽可能高，直到性能下降，我们正在利用最大熵原理的近似，即*最大熵学习*。
- en: We can think of the entropy of an agent’s policy as the amount of randomness
    it exhibits, and it turns out that maximizing entropy up until it starts to be
    counterproductive actually leads to better performance and generalization. If
    an agent can successfully achieve a goal even in the face of taking a high proportion
    of random actions, it must have a very robust policy that is insensitive to random
    transformations, so it will be able to handle more difficult environments.
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将代理策略的熵视为它所表现出的随机性量，结果发现，直到它开始变得适得其反，最大化熵实际上会导致更好的性能和泛化。如果一个代理即使在采取大量随机行动的情况下也能成功实现目标，它必须有一个非常稳健的策略，对随机变换不敏感，因此它将能够处理更困难的环境。
- en: 10.6.2\. Curriculum learning
  id: totrans-1572
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.2\. 课程学习
- en: We trained this agent only on the 5 × 5 version of this Gridworld environment
    so that it would have a small chance of randomly achieving the goal and receiving
    a reward. There are also bigger environments, including a 16 × 16 environment,
    which would make randomly winning extremely unlikely. An alternative to (or addition
    to) curiosity learning is to use a process called *curriculum learning*, which
    is when we train an agent on an easy variant of a problem, then retrain on a slightly
    harder variant, and keep retraining on harder and harder versions of the problem
    until the agent can successfully achieve a task that would have been too hard
    to start with. We could attempt to solve the 16 × 16 grid without curiosity by
    first training to maximum accuracy on the 5 × 5 grid, then retraining on the 6
    × 6 grid, then the 8 × 8 grid, and finally the 16 × 16 grid.
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在这个Gridworld环境的5 × 5版本上训练了这个代理，这样它就有很小几率随机达到目标并获得奖励。也存在更大的环境，包括一个16 × 16的环境，这使得随机获胜几乎不可能。好奇心学习的替代方案（或补充）是使用一种称为*课程学习*的过程，即我们首先在一个问题的简单变体上训练一个代理，然后在一个稍微困难一点的变体上重新训练，并继续在问题越来越困难的版本上重新训练，直到代理能够成功完成一个最初可能过于困难的任务。我们可以通过首先在5
    × 5网格上训练以达到最大精度，然后重新在6 × 6网格上训练，接着在8 × 8网格上训练，最后在16 × 16网格上训练，来尝试解决16 × 16网格而无需好奇心。
- en: 10.6.3\. Visualizing attention weights
  id: totrans-1574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.3. 可视化注意力权重
- en: We know we can successfully train a relational DQN on this somewhat difficult
    Gridworld task, but we could have used a much less fancy DQN to do the same thing.
    However, we also cared about visualizing the attention weights to see what exactly
    the agent has learned to focus on when playing the game. Some of the results are
    surprising, and some are what we would expect.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们可以在这个相对困难的Gridworld任务上成功训练一个关系型DQN，但我们本可以使用一个不那么花哨的DQN来完成同样的任务。然而，我们也关心可视化注意力权重，以了解代理在玩游戏时究竟学会了关注什么。一些结果是令人惊讶的，而一些是我们预期的。
- en: To visualize the attention weights, we had our model save a copy of the attention
    weights each time it was run forward, and we can access it by calling `GWagent.att_map`,
    which returns a *batch* × *head* × *height* × *width* tensor. All we need to do
    is run the model forward on some state, select an attention head, and select a
    node to visualize, and then reshape the tensor into a 7 × 7 grid and plot it using
    `plt.imshow`.
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化注意力权重，我们在模型每次向前运行时保存注意力权重的副本，我们可以通过调用`GWagent.att_map`来访问它，它返回一个*批次* ×
    *头* × *高度* × *宽度*张量。我们所需做的只是在一个状态上运行模型，选择一个注意力头，选择一个节点进行可视化，然后将张量重塑为7 × 7网格并使用`plt.imshow`进行绘图。
- en: '[PRE115]'
  id: totrans-1577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: We decided to look at the attention weights for the key node, the door node,
    and the agent node to see which objects are related to each other. We found the
    node in the attention weights that corresponds to the node in the grid by counting
    the grid cells, since both the attention weights and the original state are a
    7 × 7 grid. We intentionally designed the relational module such that the original
    state and attention weight matrices are the same dimensionality; otherwise it
    becomes difficult to map the attention weights onto the state. [Figure 10.23](#ch10fig23)
    shows the original full view of the grid in a random initial state and the corresponding
    prepared state.
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定查看钥匙节点、门节点和代理节点的注意力权重，以了解哪些对象彼此相关。我们通过计数网格单元来找到对应于网格中节点的注意力权重中的节点，因为注意力和原始状态都是一个7
    × 7网格。我们故意设计关系模块，使得原始状态和注意力权重矩阵具有相同的维度性；否则，将注意力权重映射到状态上会变得困难。[图10.23](#ch10fig23)显示了随机初始状态下的原始完整网格视图和相应的准备状态。
- en: 'Figure 10.23\. Left: The full observation of the environment. Right: The corresponding
    partial state view that the agent has access to.'
  id: totrans-1579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.23. 左：环境的完整观察。右：代理能够访问的相应部分状态视图。
- en: '![](10fig23_alt.jpg)'
  id: totrans-1580
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig23_alt.jpg)'
- en: The partial view is a little confusing at first because it is an egocentric
    view, so we annotated it with the positions of the agent (A), key (K), and the
    door (D). Because the agent’s partial view is always 7 × 7 and the size of the
    full grid is only 5 × 5, the partial view always includes some empty space. Now
    let’s visualize the corresponding attention weights for this state.
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: 部分视图一开始有点令人困惑，因为它是一个以自我为中心的视图，所以我们用代理（A）、钥匙（K）和门（D）的位置对其进行了标注。因为代理的部分视图总是7 ×
    7，而完整网格的大小仅为5 × 5，所以部分视图总是包括一些空白空间。现在让我们可视化这个状态对应的注意力权重。
- en: In [figure 10.24](#ch10fig24), each column is labeled as a particular node’s
    attention weights (i.e., the nodes to which it is paying attention), restricting
    ourselves to just the agent, key, and door nodes out of a total of 7 × 7 = 49
    nodes. Each row is an attention head, from head 1 to head 3, top to bottom. Curiously,
    attention head 1 does not appear to be focusing on anything obviously interesting;
    in fact, it is focusing on grid cells in empty space. Note that while we’re only
    looking at 3 of the 49 nodes, even after looking at all of the nodes, the attention
    weights are quite sparse; only a few grid cells at most are assigned any significant
    attention. But perhaps this isn’t surprising, as the attention heads appear to
    be specializing.
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10.24](#ch10fig24)中，每一列都标注为特定节点的注意力权重（即它所关注的节点），我们只关注代理、关键和门节点，总共 7 × 7
    = 49 个节点中的部分。每一行代表一个注意力头，从头 1 到头 3，从上到下。有趣的是，注意力头 1 似乎没有专注于任何明显有趣的事物；事实上，它专注于空空间中的网格单元。请注意，尽管我们只看了
    49 个节点中的 3 个，即使查看所有节点后，注意力权重仍然非常稀疏；最多只有少数几个网格单元被分配了任何显著的关注。但也许这并不令人惊讶，因为注意力头似乎在专业化。
- en: 'Figure 10.24\. Each row corresponds to an attention head (e.g., row 1 corresponds
    to attention head 1). Left column: The self-attention weights for the agent, which
    shows the objects to which the agent is paying most attention. Middle column:
    The self-attention weights for the key, which shows the objects to which the key
    is paying most attention. Right column: The self-attention weights for the door.'
  id: totrans-1583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.24\. 每一行对应一个注意力头（例如，第 1 行对应注意力头 1）。左侧列：代理的自我注意力权重，显示了代理最关注的对象。中间列：关键的自我注意力权重，显示了关键最关注的对象。右侧列：门的自我注意力权重。
- en: '![](10fig24_alt.jpg)'
  id: totrans-1584
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig24_alt.jpg)'
- en: Attention head 1 may be focusing on a small subset of landmarks in the environment
    to get an understanding of location and orientation. The fact that it can do this
    with just a few grid cells is impressive.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力头 1 可能正在关注环境中的一些地标的小子集，以了解位置和方向。它只需使用几个网格单元就能做到这一点，这很令人印象深刻。
- en: 'Attention heads 2 and 3 (rows 2 and 3 in [figure 10.24](#ch10fig24)) are more
    interesting and are doing close to what we expect. Look at attention head 2 for
    the agent node: it is strongly attending to the key (and essentially nothing else),
    which is exactly what we would hope, given that at this initial state its first
    job is to pick up the key. Reciprocally, the key is attending to the agent, suggesting
    there is a bidirectional relation from agent to key and key to agent. The door
    is also attending to the agent most strongly, but there’s also a small amount
    of attention given to the key and the space directly in front of the door.'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力头 2 和 3（[图 10.24](#ch10fig24)中的第 2 和第 3 行）更有趣，并且几乎做了我们预期的事情。看看代理节点的注意力头 2：它强烈关注着关键（几乎不关注其他任何事物），这正是我们希望看到的，因为在初始状态下，它的首要任务是拿起关键。反过来，关键也在关注代理，这表明从代理到关键和从关键到代理之间存在双向关系。门也最强烈地关注着代理，但同时也给予关键和门前面直接的空间一小部分关注。
- en: Attention head 3 for the agent is attending to a few landmark grid cells, again,
    probably to establish a sense of position and orientation. Attention head 3 for
    the key is attending to the door and the door is reciprocally attending to the
    key.
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的注意力头 3 正在关注几个地标网格单元，再次，这可能是为了建立位置和方向感。关键点的注意力头 3 正在关注门，而门也在相互关注着关键。
- en: Putting it all together, we get that the agent is related to the key which is
    related to the door. If the goal square was in view, we might see that the door
    is also related to the goal. While this is a simple environment, it has relational
    structure that we can learn with a relational neural network, and we can inspect
    the relations that it learns. It is interesting how sparsely the attention is
    assigned. Each node prefers to attend strongly to a single other node, with sometimes
    a couple other nodes that it weakly attends to.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们得到代理与关键相关，而关键与门相关。如果目标方块在视野中，我们可能会看到门也与目标相关。虽然这是一个简单的环境，但它具有我们可以用关系神经网络学习的结构，我们可以检查它所学习的关联。注意力分配得如此稀疏很令人感兴趣。每个节点更喜欢强烈关注另一个节点，有时还会弱关注几个其他节点。
- en: Since this is a Gridworld, it is easy to partition the state into discrete objects,
    but in many cases such as the Atari games, the state is a big RGB pixel array,
    and the objects we would want to focus on are collections of these pixels. In
    this case it becomes difficult to map the attention weights back to specific objects
    in the video frame, but we can still see which parts of the image the relational
    module as a whole is using to make its decisions. We tested a similar architecture
    on the Atari game Alien (we just used 4 × 4 kernel convolutions instead of 1 ×
    1 and added some maxpooling layers), and we can see (in [figure 10.25](#ch10fig25))
    that it indeed learns to focus on salient objects in the video frame (code not
    shown).
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个网格世界，将状态划分为离散对象很容易，但在许多情况下，例如在Atari游戏中，状态是一个大的RGB像素数组，而我们想要关注的是这些像素的集合。在这种情况下，将注意力权重映射回视频帧中的特定对象变得困难，但我们可以看到关系模块整体上使用哪些图像部分来做出决策。我们在Atari游戏《外星人》上测试了类似的架构（我们只是使用了4×4核卷积而不是1×1，并添加了一些最大池化层），我们可以看到（在[图10.25](#ch10fig25)中）它确实学会了关注视频帧中的显著对象（代码未展示）。
- en: 'Figure 10.25\. Left: Preprocessed state given to the DQN. Middle: Raw video
    frame. Right: Attention map over state. We can see that the attention map is focused
    on the alien in the bottom center of the screen, the player in the center, and
    the bonus at the top, which are the most salient objects in the game.'
  id: totrans-1590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.25. 左：DQN接收到的预处理状态。中：原始视频帧。右：状态上的注意力图。我们可以看到注意力图集中在屏幕底部中央的外星人、中央的玩家以及顶部的奖励，这些是游戏中最显著的对象。
- en: '![](10fig25_alt.jpg)'
  id: totrans-1591
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig25_alt.jpg)'
- en: Relational modules using the self-attention mechanism are powerful tools in
    the machine learning toolkit, and they can be very useful for training RL agents
    when we want some idea of how they’re making decisions. Self-attention is one
    mechanism for performing message passing on a graph, as we discussed, and it’s
    part of the broader field of graph neural networks, which we encourage you to
    explore further. There are many implementations of graph neural networks (GNNs)
    but particularly relevant for us after this chapter is the *graph attention network*,
    which uses the same self-attention mechanism we just implemented but with the
    added ability to operate on more general graph-structured data.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自注意力机制的关系模块是机器学习工具箱中的强大工具，当我们需要了解它们如何做出决策时，它们非常有用。正如我们讨论的那样，自注意力是执行图上消息传递的一种机制，它是更广泛的图神经网络领域的一部分，我们鼓励你进一步探索。有许多图神经网络（GNNs）的实现，但在此章之后对我们特别相关的是*图注意力网络*，它使用了与我们刚刚实现的相同的自注意力机制，但增加了在更通用的图结构数据上操作的能力。
- en: Summary
  id: totrans-1593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Graph neural networks are machine learning models that operate on graph-structured
    data. Graphs are data structures composed of a set of objects (called nodes) and
    relations between objects (called edges). A natural graph type is a social network
    in which the nodes are individuals and the edges between nodes represent friendships.
  id: totrans-1594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图神经网络是操作在图结构数据上的机器学习模型。图是由一组对象（称为节点）和对象之间的关系（称为边）组成的数据结构。一种自然的图类型是社交网络，其中节点是个人，节点之间的边代表友谊。
- en: An adjacency matrix is a matrix with dimensions *A*:*N* × *N* where *N* is the
    number of nodes in a graph that encodes the connectivity between each pair of
    nodes.
  id: totrans-1595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻接矩阵是一个维度为 *A*:*N* × *N* 的矩阵，其中 *N* 是图中节点数，它编码了每对节点之间的连通性。
- en: Message passing is an algorithm for computing updates to node features by iteratively
    aggregating information from a node’s neighbors.
  id: totrans-1596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息传递是一种通过迭代聚合节点邻居的信息来计算节点特征更新的算法。
- en: Inductive biases are the prior information we have about some set of data; we
    use them to constrain our model toward learning certain kinds of patterns. In
    this chapter we employed a relational inductive bias, for example.
  id: totrans-1597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归纳偏差是我们对某些数据集的先验信息；我们使用它们来约束我们的模型，使其学习某些类型的模式。在本章中，我们采用了关系归纳偏差，例如。
- en: 'We say a function, *f* is invariant to some transformation, *g*, when the function’s
    output remains unchanged when the input is first transformed by *g*: *f*(*g*(*x*))
    = *f*(*x*).'
  id: totrans-1598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当函数的输出在输入首先经过变换 *g* 后保持不变时，我们说函数 *f* 对某种变换 *g* 是不变的：*f*(*g*(*x*)) = *f*(*x*)。
- en: 'We say a function, *f*, is equivariant to some transformation, *g*, when applying
    the transformation to the input is the same as applying the transformation to
    the output: *f*(*g*(*x*)) = g(*f*(*x*)).'
  id: totrans-1599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将变换应用于输入与应用于输出相同，我们说一个函数，*f*，对某个变换，*g*，是等变的：*f*(*g*(*x*)) = *g*(*f*(*x*))。
- en: Attention models are designed to increase the interpretability and performance
    of machine learning models by forcing the model to only “look at” (attend to)
    a subset of the input data. By examining what the model learns to attend to, we
    can have a better idea of how it is making decisions.
  id: totrans-1600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力模型旨在通过迫使模型只“看”（关注）输入数据的一个子集来增加机器学习模型的可解释性和性能。通过检查模型学习关注的内容，我们可以更好地了解它是如何做出决策的。
- en: Self-attention models model attention between objects (or nodes) in an input
    rather than just the model attending to different parts of the input. This naturally
    leads to a form of graph neural network, since the attention weights can be interpreted
    as edges between nodes.
  id: totrans-1601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力模型在输入中建模对象（或节点）之间的注意力，而不仅仅是模型关注输入的不同部分。这自然导致了一种图神经网络的形式，因为注意力权重可以解释为节点之间的边。
- en: Multi-head self-attention allows the model to have independent attention mechanisms
    that can each attend to a different subset of the input data. This allows us to
    still get interpretable attention weights but increases the bandwidth of information
    that can flow through the model.
  id: totrans-1602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力允许模型具有独立的注意力机制，每个机制可以关注输入数据的不同子集。这使我们仍然可以得到可解释的注意力权重，但增加了模型中可以流动的信息带宽。
- en: Relational reasoning is a form of reasoning based on objects and relationships
    between objects, rather than using absolute frames of reference. For example,
    “a book is on the top” relates the book to the table, rather than saying the book
    is at position 10 and the table is at position 8 (an absolute reference frame).
  id: totrans-1603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系推理是一种基于对象及其之间关系的推理形式，而不是使用绝对参考框架。例如，“一本书在顶部”将书与桌子联系起来，而不是说书在位置10，桌子在位置8（一个绝对参考框架）。
- en: Inner (dot) product is a product between two vectors that results in a single
    scalar value.
  id: totrans-1604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内积（点积）是两个向量之间的乘积，结果是一个单一的标量值。
- en: Outer product is a product between two vectors that results in a matrix.
  id: totrans-1605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外积是两个向量之间的乘积，结果是一个矩阵。
- en: Einstein notation or Einsum lets us describe generalized tensor-tensor products
    called tensor contractions using a simple syntax based on labeling the indices
    of a tensor.
  id: totrans-1606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爱因斯坦符号或Einsum允许我们使用基于标记张量索引的简单语法来描述广义张量-张量乘积，称为张量收缩。
- en: Double Q-learning stabilizes training by separating action-selection and action-value
    updating.
  id: totrans-1607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双Q学习通过分离动作选择和动作值更新来稳定训练。
- en: 'Chapter 11\. In conclusion: A review and roadmap'
  id: totrans-1608
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章。总结：回顾和路线图
- en: In this final chapter we’ll first take a moment to briefly review what we’ve
    learned, highlighting and distilling what we think are the most important skills
    and concepts to take away. We have covered the fundamentals of *reinforcement
    learning* and if you have made it this far and have engaged with the projects,
    you’re well-positioned to implement many other algorithms and techniques.
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们将花一点时间简要回顾我们已经学到的内容，突出和提炼我们认为最重要的技能和概念。我们已经涵盖了**强化学习**的基础，如果你已经走到这一步并且参与了项目，你将能够实施许多其他算法和技术。
- en: This book is a *course* on the fundamentals of *deep reinforcement learning*,
    not a textbook or reference. That means we could not possibly have introduced
    all there is to know about DRL, and we had to make tough choices about what to
    leave out. There are a number of exciting topics in DRL we wished we could have
    included, and there are some topics that, despite being “industry standards,”
    were inappropriate to include in a project-focused introductory book like this
    one. However, we wanted to leave you with a roadmap for where to go from here
    with your new skills.
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是一本关于**深度强化学习**基础的**课程**，而不是教科书或参考书。这意味着我们不可能介绍所有关于DRL的知识，我们必须在保留什么和舍弃什么之间做出艰难的选择。在DRL中有许多激动人心的主题我们希望包括在内，也有一些主题，尽管是“行业标准”，但不适于包含在这本以项目为导向的入门书中。然而，我们希望给你留下一条路线图，以便你利用新技能继续前进。
- en: In the second part of this chapter, we’ll introduce at a high-level some topics,
    techniques, and algorithms in DRL that are worth knowing if you’re serious about
    continuing in the DRL field. We didn’t cover these areas because most of them
    involve advanced mathematics that we do not expect readers of this book to be
    familiar with, and we did not have the space to teach more mathematics.
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将从高层次介绍一些在 DRL 领域值得了解的主题、技术和算法。我们没有涵盖这些领域，因为其中大部分涉及高级数学，我们并不期望本书的读者熟悉这些数学，而且我们没有足够的空间来教授更多的数学。
- en: 11.1\. What did we learn?
  id: totrans-1612
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1. 我们学到了什么？
- en: Deep reinforcement learning is the combination of deep learning and reinforcement
    learning. Reinforcement learning is a framework for solving *control tasks*, which
    are problems in which an *agent* can take actions that lead to positive or negative
    rewards, given some *environment*. The environment is the universe in which the
    agent acts. The agent can either have full access to the state of the environment,
    or it may only have partial access to the state of the environment, which is called
    *par**tial observability*. The environment evolves in discrete time steps according
    to some dynamical rules, and at each time step the agent takes an action that
    may influence the next state. After taking each action, the agent receives feedback
    in the form of a reward signal. We described a mathematical formalization of this
    called the *Markov decision process* (MDP).
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是深度学习和强化学习的结合。强化学习是一个解决 *控制任务* 的框架，在这些任务中，一个 *代理* 可以采取导致正面或负面奖励的行动，给定某个
    *环境*。环境是代理行动的宇宙。代理可以完全访问环境的当前状态，或者它可能只能部分访问环境的当前状态，这被称为 *部分可观测性*。环境根据某些动力学规则在离散时间步中演变，在每个时间步，代理采取一个可能影响下一个状态的动作。在采取每个动作后，代理会收到以奖励信号形式提供的反馈。我们描述了这种数学形式，称为
    *马尔可夫决策过程*（MDP）。
- en: An MDP is a mathematical structure that includes a set of states, *S*, that
    the environment can be in, and a set of actions, *A*, that the agent can take,
    which may depend on the particular state of the environment. There is a *reward
    function*, *R*(*s[t]*,*a[t]*,*s[t]*[+1]), that produces the *reward signal*, given
    the transition from the current state to the next state and the agent’s action.
    The environment may evolve deterministically or stochastically, but in either
    case, the agent initially does not know the dynamical rules of the environment,
    so all state transitions must be described probabilistically from the perspective
    of the agent.
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是一个数学结构，包括环境可能处于的一组状态 *S* 和代理可以采取的一组动作 *A*，这些动作可能取决于环境的特定状态。存在一个 *奖励函数*，*R*(*s[t]*,*a[t]*,*s[t]*[+1])，它根据从当前状态到下一个状态的转换和代理的动作产生
    *奖励信号*。环境可能以确定性或随机性演变，但在任何情况下，代理最初都不知道环境的动力学规则，因此所有状态转换都必须从代理的角度以概率形式描述。
- en: 'We therefore have a conditional probability distribution over next states,
    *s[t]*[+1], given the current state and the action taken by the agent at the current
    time step, denoted Pr(*S[t]*[+1]|*s[t]*,*a[t]*). The agent follows some policy,
    *π*, which is a function that maps a probability distribution over actions given
    the current state *s[t]*: *π*:*S* → Pr(*A*). The objective of the agent is to
    take actions that maximize the time-discounted cumulative rewards over some time
    horizon. The time-discounted cumulative reward is termed the *return* (often denoted
    with the character *G* or *R*), and is equal to'
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一个条件概率分布，用于预测下一个状态 *s[t]*[+1]，这是基于当前状态和代理在当前时间步采取的动作，表示为 Pr(*S[t]*[+1]|*s[t]*,*a[t]*)。代理遵循某种策略
    *π*，这是一个函数，它将当前状态 *s[t]* 给定的动作的概率分布映射到 *A*：*π*:*S* → Pr(*A*)。代理的目标是采取最大化时间折扣累积奖励的行动，在某个时间范围内。时间折扣累积奖励被称为
    *回报*（通常用字符 *G* 或 *R* 表示），等于
- en: '![](pg330.jpg)'
  id: totrans-1616
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg330.jpg)'
- en: The return, *G[t]* at time *t*, is equal to the sum of discounted rewards for
    each time step until the end of the episode (for episodic environments) or until
    the sequence converges for non-episodic environments. The γ factor is a parameter
    in the interval (0,1) and is the discount rate that determines how quickly the
    sequence will converge, and thus how much the future is discounted. A discount
    rate close to 1 will mean that future rewards are given similar weight to immediate
    rewards (optimizing over the long term), whereas a low discount rate leads to
    preferring only short-term time horizons.
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间 *t* 的回报 *G[t]* 等于直到剧集结束（对于剧集环境）或直到非剧集环境的序列收敛为止的每个时间步的折现奖励之和。γ 因子是区间 (0,1)
    内的参数，是决定序列将如何收敛以及未来将如何折现的折现率。接近 1 的折现率意味着未来的奖励与即时奖励具有相似的权重（长期优化），而低折现率会导致只偏好短期时间范围。
- en: A derived concept from this basic MDP framework is that of a *value function*.
    A value function assigns a value to either states or state-action pairs (i.e.,
    a value for taking an action in a given state), with the former being called a
    *state-value function* (or often just the value function) and the latter being
    the *action value* or *Q function*. The value of a state is simply the expected
    return given that the agent starts in that state and follows some policy *π*,
    so value functions implicitly depend on the policy. Similarly, the action value
    or *Q value* of a state-action pair is the expected return given that the agent
    takes the action in that state and follows the policy *π* until the end. A state
    that puts the agent in close position to win a game, for example, would be assigned
    a high state value assuming the underlying policy was reasonable. We denote the
    value function as *V**[π]*(*s*), with the subscript *π* indicating the dependence
    of the value on the underlying policy, and the Q function as *Q**[π]*(*s*,*a*),
    although we often drop the *π* subscript for convenience.
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个基本的 MDP 框架中衍生出的一个概念是 *价值函数*。价值函数将值分配给状态或状态-动作对（即，在给定状态中采取动作的值），前者被称为 *状态价值函数*（或通常只是价值函数），后者是
    *动作价值* 或 *Q 函数*。状态的价值是给定代理从该状态开始并遵循某些策略 *π* 的期望回报，因此价值函数隐式地依赖于策略。同样，状态-动作对的动作价值或
    *Q 价值* 是给定代理在该状态下采取动作并遵循策略 *π* 直到结束的期望回报。例如，将代理置于接近赢得游戏的位置的状态将被分配一个较高的状态价值，假设基本策略是合理的。我们用
    *V**[π]*(*s*) 表示价值函数，下标 *π* 表示价值对基本策略的依赖性，Q 函数表示为 *Q**[π]*(*s*,*a*)，尽管我们经常为了方便省略
    *π* 下标。
- en: Right now we understand the function *Q**[π]*(*s*,*a*) as being some sort of
    black box that tells us the exact expected rewards for state action *a* in state
    *s*, but, of course, we do not have access to such an all-knowing function; we
    have to estimate it. In this book we used *neural networks* to estimate value
    functions and policy functions, although any suitable function could work. In
    the case of a neural-based *Q**[π]*(*s*,*a*), we trained neural networks to predict
    the expected rewards. The value functions are defined and approximated recursively,
    such that *Q**[π]*(*s*,*a*) is updated as
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将函数 *Q**[π]*(*s*,*a*) 理解为某种黑盒，它告诉我们状态 *s* 中动作 *a* 的确切期望奖励，但当然，我们没有访问这样一个无所不知的函数；我们必须估计它。在这本书中，我们使用了
    *神经网络* 来估计价值函数和政策函数，尽管任何合适的函数都可以工作。在基于神经网络的 *Q**[π]*(*s*,*a*) 的情况下，我们训练神经网络来预测期望奖励。价值函数被定义和递归近似，使得
    *Q**[π]*(*s*,*a*) 如下更新
- en: '| *V**[π]*(*s*) = *r[t]* + γ*V**[π]*(*s*′) |'
  id: totrans-1620
  prefs: []
  type: TYPE_TB
  zh: '| *V**[π]*(*s*) = *r[t]* + γ*V**[π]*(*s*′) |'
- en: where *s*′ refers to the next state, or *s[t]*[+1]. For example, in Gridworld,
    landing on the goal tile results in +10, landing on the pit results in –10, and
    losing the game, and all other non-terminal moves, are penalized at –1\. If the
    agent is two steps away from the winning goal tile, the final state reduces to
    *V**[π]*(*s*[3]) = 10\. Then, if γ = 0.9, the previous state is valued at *V**[π]*(*s*[2])
    = *r*[2] + 0.9*V**[π]*(*s*[3]) = –1 + 9 = 8\. The move before that must be *V**[π]*(*s*[1])
    = *r*[1] + 0.9*V**[π]*(*s*[2]) = –1 + 0.8 × 8 = 5.4\. As you can see, states farther
    from the winning state are valued less.
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *s*′ 指的是下一个状态，或 *s[t]*[+1]。例如，在 Gridworld 中，落在目标格子上得到 +10，落在陷阱上得到 –10，以及输掉游戏，所有其他非终端移动都受到
    –1 的惩罚。如果代理距离获胜的目标格子还有两步之遥，最终状态将简化为 *V**[π]*(*s*[3]) = 10。然后，如果 γ = 0.9，前一个状态的价值为
    *V**[π]*(*s*[2]) = *r*[2] + 0.9*V**[π]*(*s*[3]) = –1 + 9 = 8。之前的移动必须为 *V**[π]*(*s*[1])
    = *r*[1] + 0.9*V**[π]*(*s*[2]) = –1 + 0.8 × 8 = 5.4。正如你所看到的，离获胜状态越远的州价值越低。
- en: Training a reinforcement learning agent, then, just amounts to successfully
    training a neural network to either approximate the value function (so the agent
    will choose actions that lead to high-value states) or to directly approximate
    the policy function by observing rewards after actions and reinforcing actions
    based on the rewards received. Both approaches have their pros and cons, but often
    we combine learning both a policy and a value function together, which is called
    an *actor-critic algorithm*, where the actor refers to the policy and the critic
    refers to the value function.
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练一个强化学习智能体，本质上就是成功训练一个神经网络来近似价值函数（这样智能体将选择导致高价值状态的动作）或者通过观察动作后的奖励并基于收到的奖励来强化动作，直接近似策略函数。这两种方法都有其优缺点，但通常我们会结合学习策略和价值函数，这被称为*演员-评论家算法*，其中演员指的是策略，评论家指的是价值函数。
- en: 11.2\. The uncharted topics in deep reinforcement learning
  id: totrans-1623
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 深度强化学习中的未探索主题
- en: The Markov decision process framework and value and policy functions we just
    reviewed were detailed in [chapters 2](kindle_split_011.html#ch02)–[5](kindle_split_014.html#ch05).
    We then spent the rest of the book implementing more sophisticated techniques
    for successfully training value and policy functions in difficult environments
    (e.g., environments with sparse rewards) and environments with multiple interacting
    agents. Unfortunately, there were many exciting things we didn’t have the space
    to cover, so we’ll end the book with a brief tour of some other areas in deep
    reinforcement learning you might want to explore. We’ll only give a taste of a
    few topics we think are worth exploring more and hope you will look into these
    areas more deeply on your own.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚详细讨论的马尔可夫决策过程框架以及价值和策略函数在[第2章](kindle_split_011.html#ch02)–[第5章](kindle_split_014.html#ch05)中进行了介绍。然后，我们用本书的其余部分来实现更复杂的技巧，以在困难环境中（例如，具有稀疏奖励的环境）和具有多个交互智能体的环境中成功训练价值和策略函数。不幸的是，有许多激动人心的事情我们没有空间涵盖，所以我们将在本书的最后简要游览一些你可能想要探索的深度强化学习其他领域。我们只会对一些我们认为值得进一步探索的主题进行简要介绍，并希望你自己能更深入地研究这些领域。
- en: 11.2.1\. Prioritized experience replay
  id: totrans-1625
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 优先经验回放
- en: We briefly mentioned the idea of *prioritized replay* earlier in the book when
    we decided to add multiple copies of the same experience to the replay memory
    if the experience led to a winning state. Since winning states are rare, and we
    want our agent to learn from these informative events, we thought that adding
    multiple copies would ensure that each training epoch would include a few of these
    winning events. This was a very unsophisticated means of prioritizing experiences
    in the replay based on how informative they are in training the agent.
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，当我们决定将相同经验添加到回放记忆中，如果这种经验导致获胜状态时，我们简要提到了*优先回放*的概念。由于获胜状态很少见，而我们希望我们的智能体能够从这些信息性事件中学习，我们认为添加多个副本可以确保每个训练周期都包含一些这些获胜事件。这是一种非常简单的基于经验在回放中的信息性来优先处理的方法。
- en: The term *prioritized experience replay* generally refers to a specific implementation
    introduced in an academic paper titled “Prioritized Experience Replay” by Tom
    Schaul et al. (2015), and it uses a much more sophisticated mechanism to prioritize
    experiences. In their implementation, all experiences are recorded just once,
    unlike our approach, but rather than selecting a mini-batch from the replay completely
    randomly (i.e., uniformly), they preferentially select experiences that are more
    informative. They defined informative experiences as not merely those that led
    to a winning state like we did, but rather those where the DQN had a high error
    in predicting the reward. In essence, the model preferentially trains on the most
    surprising experiences. As the model trains, however, the once surprising experiences
    become less surprising, and the preferences get continually reweighted. This leads
    to substantially improved training performance. This kind of prioritized experience
    replay is standard practice for value-based reinforcement learning, whereas policy-based
    reinforcement learning still tends to rely on using multiple parallelized agents
    and environments.
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*优先经验回放*通常指的是由Tom Schaul等人（2015年）在名为“Prioritized Experience Replay”的学术论文中引入的一种特定实现，它使用了一种更复杂的机制来优先处理经验。在他们实现中，所有经验只记录一次，与我们的方法不同，但他们不是完全随机（即均匀）地从回放中选择一个迷你批次，而是优先选择更有信息量的经验。他们将信息量丰富的经验定义为不仅仅是那些导致胜利状态的情况，而是那些DQN在预测奖励时存在高误差的情况。本质上，模型优先在最令人惊讶的经验上进行训练。然而，随着模型训练的进行，曾经令人惊讶的经验变得不再令人惊讶，偏好会持续重新加权。这导致训练性能显著提高。这种优先经验回放是基于价值的强化学习中的标准做法，而基于策略的强化学习仍然倾向于依赖于使用多个并行化的代理和环境。
- en: 11.2.2\. Proximal policy optimization (PPO)
  id: totrans-1628
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2. 近端策略优化（PPO）
- en: We mostly implemented *deep Q-networks* (DQN) in this book rather than policy
    functions, and this is for good reason. The (deep) policy functions we implemented
    in [chapters 4](kindle_split_013.html#ch04) and [5](kindle_split_014.html#ch05)
    were rather unsophisticated and would not work very well for more complex environments.
    The problem is not with the policy networks themselves but with the training algorithm.
    The simple REINFORCE algorithm we used is fairly unstable. When the rewards vary
    significantly from action to action, the REINFORCE algorithm does not lead to
    stable results. We need a training algorithm that enforces smoother, more constrained
    updates to the policy network.
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中主要实现了*深度Q网络*（DQN），而不是策略函数，这有很好的理由。我们在[第4章](kindle_split_013.html#ch04)和[第5章](kindle_split_014.html#ch05)中实现的（深度）策略函数相当简单，对于更复杂的环境不太适用。问题不在于策略网络本身，而在于训练算法。我们使用的简单REINFORCE算法相当不稳定。当奖励因动作而显著变化时，REINFORCE算法不会导致稳定的结果。我们需要一个训练算法，该算法强制对策略网络进行更平滑、更受约束的更新。
- en: '*Proximal policy optimization* (PPO) is a more advanced training algorithm
    for policy methods that allows for far more stable training. It was introduced
    in the paper “Proximal Policy Optimization Algorithms” by John Schulman et al.
    (2017) at OpenAI. We did not cover PPO in this book because, while the algorithm
    itself is relatively simple, understanding it requires mathematical machinery
    that is outside the scope of this introductory book. Making deep Q-learning more
    stable required only a few intuitive upgrades like adding a target network and
    implementing double Q-learning, so that is why we preferred to use value learning
    over policy methods in this book. However, in many cases, directly learning a
    policy function is more advantageous than a value function, such as for environments
    with a continuous action space, since we cannot create a DQN that returns an infinite
    number of Q values for each action.'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: '*近端策略优化*（PPO）是一种更先进的策略方法训练算法，它允许进行更稳定的训练。它是由John Schulman等人（2017年）在OpenAI发表的论文“Proximal
    Policy Optimization Algorithms”中引入的。我们没有在这本书中涵盖PPO，因为虽然算法本身相对简单，但要理解它需要超出本入门书籍范围的数学工具。使深度Q学习更稳定只需要进行一些直观的升级，比如添加目标网络和实现双重Q学习，所以我们更倾向于在这本书中使用价值学习而不是策略方法。然而，在许多情况下，直接学习策略函数比学习价值函数更有优势，例如对于具有连续动作空间的环境，因为我们不能为每个动作创建返回无限多个Q值的DQN。'
- en: 11.2.3\. Hierarchical reinforcement learning and the options framework
  id: totrans-1631
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.3. 层次强化学习和选项框架
- en: When a child learns to walk, they aren’t thinking about which individual muscle
    fibers to activate and for how long, or when a businessperson is debating a business
    decision with colleagues, they aren’t thinking about the individual sequences
    of sounds they need to make for the other people to understand their business
    strategy. Our actions exist at various levels of abstraction, from moving individual
    muscles up to grand schemes. This is just like noticing that a story is made up
    of individual letters, but those letters are composed into words that are composed
    into sentences and paragraphs and so on. The writer may be thinking of a general
    next scene in the story, and only once that’s decided will they actually get to
    typing individual characters.
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个孩子学习走路时，他们不会考虑激活哪些单个肌肉纤维以及持续多长时间，或者当一位商业人士与同事讨论商业决策时，他们不会考虑需要发出哪些单个声音序列以便其他人理解他们的商业策略。我们的行为存在于各种抽象层次上，从移动单个肌肉到宏伟的计划。这就像注意到一个故事由单个字母组成，但这些字母被组合成单词，单词又组合成句子和段落等等。作家可能会考虑故事中的下一个场景，只有当这个场景确定后，他们才会真正开始打字单个字母。
- en: All the agents we’ve implemented in this book operate at the level of typing
    individual characters so to speak; they are incapable of thinking at a higher
    level. *Hierarchical reinforcement learning* is an approach to solving this problem,
    allowing agents to build up higher-level actions from lower ones. Rather than
    our Gridworld agent deciding one step at a time what to do, it might survey the
    board and decide on a higher-level sequence of actions. It might learn reusable
    sequences such as “move all the way up” or “move around obstacle” that can be
    implemented in a variety of game states.
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所实现的全部智能体在某种程度上都是在打字单个字符的层面上操作的；它们无法进行更高层次的思考。**分层强化学习**是解决此问题的一种方法，它允许智能体从较低层次构建起更高层次的动作。而不是我们的Gridworld智能体一次决定下一步要做什么，它可能会审视棋盘并决定一个更高层次的动作序列。它可能会学习可重用的序列，例如“一直向上移动”或“绕过障碍物”，这些序列可以在各种游戏状态下实现。
- en: The success of deep learning in reinforcement learning is due to its ability
    to represent complex high-dimensional states in a hierarchy of higher-level state
    representations. In hierarchical reinforcement learning, the goal is to extend
    this to representing states *and* actions hierarchically. One popular approach
    to this is called the *options framework*.
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在强化学习中的成功归功于其能够以高级状态表示的层次结构来表示复杂的高维状态。在分层强化学习中，目标是扩展这种表示方法以分层地表示状态和动作。对此问题的一种流行方法是称为**选项框架**。
- en: Consider Gridworld, which has four primitive actions of up, right, left, and
    down, and each action lasts one time step. In the options framework, there are
    options rather than just primitive actions. An option is the combination of an
    *option policy* (which like a regular policy takes a state and returns a probability
    distribution over actions), a *termination condition*, and an *input set* (which
    is a subset of states). The idea is that a particular option gets triggered when
    the agent encounters a state in the option’s input set, and that particular option’s
    policy is run until the termination condition is met, at which point a different
    option may be selected. These option policies might be simpler policies than a
    single, big, deep neural network policy that we have implemented in this book.
    But by intelligently selecting these higher-level options, efficiencies can be
    gained by not having to use a more computationally intensive policy for taking
    each primitive step.
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
  zh: 以Gridworld为例，它有向上、向右、向左和向下四种基本动作，每个动作持续一个时间步。在选项框架中，有选项而不是仅仅基本动作。选项是由一个**选项策略**（它像常规策略一样接受一个状态并返回一个关于动作的概率分布）、一个**终止条件**和一个**输入集**（它是状态的子集）的组合。其想法是，当智能体遇到选项输入集中的状态时，特定的选项会被触发，并且该选项的策略会一直运行，直到满足终止条件，此时可以选择不同的选项。这些选项策略可能比我们在本书中实现的单一、大型、深度神经网络策略要简单。但通过智能地选择这些高级选项，可以通过不必为每个基本步骤使用计算量更大的策略来提高效率。
- en: 11.2.4\. Model-based planning
  id: totrans-1636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.4\. 基于模型的规划
- en: We already discussed the idea of *models* in reinforcement learning in two contexts.
    In the first, a model is simply another term for an approximating function like
    a neural network. We sometimes just refer to our neural network as a model, since
    it approximates or models the value function or the policy function.
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在两个上下文中讨论了强化学习中**模型**的概念。在第一个上下文中，模型仅仅是近似函数（如神经网络）的另一个术语。我们有时直接将我们的神经网络称为模型，因为它近似或模拟了价值函数或策略函数。
- en: The other context is when we refer to *model-based* versus *model-free learning*.
    In both cases we are using a neural network as a model of the value function or
    a policy, but in this case model-based means the agent is making decisions based
    on an explicitly constructed model of the dynamics of the environment itself rather
    than just its value function. In model-free learning, all we care about is learning
    to accurately predict rewards, which may or may not require a deep understanding
    of how the environment actually works. In model-based learning, we actually want
    to learn how the environment works. Metaphorically, in model-free learning we
    are satisfied knowing that there is something called gravity that makes things
    fall, and we make use of this phenomenon, but in model-based learning we want
    to actually approximate the laws of gravity.
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个上下文是我们提到**基于模型**与**无模型学习**。在这两种情况下，我们都是使用神经网络作为价值函数或策略的模型，但在这个情况下，基于模型意味着代理是基于对环境本身动态的显式构建的模型来做出决策，而不是仅仅基于其价值函数。在无模型学习中，我们唯一关心的是学习准确预测奖励，这可能或可能不需要对环境实际工作方式的深入了解。在基于模型的学习中，我们实际上想要了解环境是如何工作的。比喻来说，在无模型学习中，我们满足于知道存在一种称为重力的事物使物体下落，并利用这一现象，但在基于模型的学习中，我们想要实际近似重力的定律。
- en: Our model-free DQN worked surprisingly well, especially when combined with other
    advances like curiosity, so what is the advantage of explicitly learning a model
    of the environment? With an explicit and accurate *environment model*, the agent
    can learn to make long-term plans rather than just deciding which next action
    to take. By using its environment model to predict the future several time steps
    ahead, it can evaluate the long-term consequences of its immediate actions, and
    this can lead to faster learning (due to increased sample efficiency). This is
    related to, but not necessarily the same as, the hierarchical reinforcement learning
    we discussed, since hierarchical reinforcement learning does not necessarily depend
    on an environment model. But with an environment model, the agent can plan out
    a sequence of primitive actions to accomplish some higher-level goal.
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型自由DQN工作得非常出色，尤其是在结合其他进步（如好奇心）时，那么明确学习环境模型的优势是什么？有了明确且准确的环境模型，代理可以学习制定长期计划，而不仅仅是决定下一步采取什么行动。通过使用其环境模型来预测几个时间步的未来，它可以评估其即时行动的长期后果，这可以导致学习速度更快（由于样本效率的提高）。这与我们讨论的分层强化学习相关，但不一定是同一件事，因为分层强化学习不一定依赖于环境模型。但有了环境模型，代理可以规划一系列基本动作以实现某些高级目标。
- en: The simplest way to train an environment model is to just have a separate deep
    learning module that predicts future states. In fact, we did just that in [chapter
    8](kindle_split_018.html#ch08) on *curiosity-based learning*, but we did not use
    the environment model to plan or look into the future; we only used it to explore
    surprising states. But with a model, *M*(*s[t]*), that takes a state and returns
    a predicted next state, *s[t]*[+1], we could then take that predicted next state
    and feed it back into the model to get the predicted state *s[t]*[+2], and so
    on. The distance into the future we could predict depends on the inherent randomness
    in the environment and the accuracy of the model, but even if we could only accurately
    predict out to a few time steps into the future, this would be immensely useful.
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: 训练环境模型最简单的方法是拥有一个独立的深度学习模块来预测未来的状态。实际上，我们在第8章[基于好奇心的学习](kindle_split_018.html#ch08)中就是这样做的，但我们没有使用环境模型来规划或展望未来；我们只是用它来探索令人惊讶的状态。但是，如果我们有一个模型，记为*M*(*s[t]*)，它接受一个状态并返回预测的下一个状态，*s[t]*[+1]，然后我们可以将那个预测的下一个状态反馈到模型中，以获取预测的状态
    *s[t]*[+2]，依此类推。我们可以预测的未来距离取决于环境的固有随机性和模型的准确性，但即使我们只能准确预测到几个时间步的未来，这也会非常有用。
- en: 11.2.5\. Monte Carlo tree search (MCTS)
  id: totrans-1641
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.5. 蒙特卡洛树搜索（MCTS）
- en: Many games have a finite set of actions and a finite length, such as chess,
    Go, and Tic-Tac-Toe. The Deep Blue algorithm that IBM developed to play chess
    didn’t use machine learning at all; it was a brute force algorithm that used a
    form of tree search. Consider the game of Tic-Tac-Toe. It is a two-player game
    typically played on a square 3 × 3 grid where player 1 places an X-shaped token
    and player 2 places an O-shaped token. The goal of the game is to be the first
    to get three of your tokens lined up in a row, column, or diagonal.
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: 许多游戏都有有限的动作集合和有限的游戏长度，例如国际象棋、围棋和井字棋。IBM开发的用于下国际象棋的Deep Blue算法根本不使用机器学习；它是一个使用树搜索形式的穷举算法。考虑一下井字棋。这是一款两人游戏，通常在一个3
    × 3的方形网格上进行，玩家1放置X形标记，玩家2放置O形标记。游戏的目标是第一个将你的三个标记排成一行、一列或对角线。
- en: The game is so simple that the human strategy also generally involves limited
    tree search. If you’re player 2 and there’s already one opposing token on the
    grid, you can consider all possible responses to all possible open spaces you
    have, and you can keep doing this until the end of the game. Of course, even for
    a 3 × 3 board, the first move has nine possible actions, and there are eight possible
    actions for player 2, and then seven possible actions for player 1 again, so the
    number of possible trajectories (the game tree) becomes quite large, but a brute
    force exhaustive search like this could be guaranteed win at Tic-Tac-Toe assuming
    the opponent isn’t using the same approach.
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: 这款游戏非常简单，以至于人类策略通常也只涉及有限的树搜索。如果你是玩家2，并且网格上已经有一个对手的标记，你可以考虑所有可能的回应，针对你所有可能的开阔空间，并且你可以一直这样做，直到游戏结束。当然，即使对于3
    × 3的棋盘，第一手有九种可能的行动，玩家2有八种可能的行动，然后玩家1又有七种可能的行动，所以可能的轨迹（游戏树）变得相当庞大，但这样的穷举搜索在对手不使用相同方法的情况下，可以保证在井字棋中获胜。
- en: For a game like chess, the game tree is far too large to use a completely brute
    force search of the game tree; one must necessarily limit the number of potential
    moves to consider. Deep Blue used a tree-search algorithm that is more efficient
    than exhaustive search but still involved no learning. It still amounted to searching
    possible trajectories and just computing which ones led to winning states.
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像国际象棋这样的游戏，游戏树太大，无法使用完全的穷举搜索；必须必然限制要考虑的可能移动的数量。Deep Blue使用了一种比穷举搜索更有效的树搜索算法，但仍然不涉及学习。它仍然相当于搜索可能的轨迹，并计算哪些轨迹导致了获胜状态。
- en: Another approach is the *Monte Carlo tree search*, in which you use some mechanism
    of randomly sampling a set of potential actions and expanding the tree from there,
    rather than considering *all* possible actions. The AlphaGo algorithm developed
    by DeepMind to play the game Go used a deep neural network to evaluate which actions
    were worth doing a tree search on and also to decide the value of selected moves.
    Therefore, AlphaGo combined brute force search with deep neural networks to get
    the best of both. These types of combination algorithms are currently state-of-the-art
    for games in the class of chess and Go.
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是*蒙特卡洛树搜索*，其中你使用某种机制随机采样一组潜在的动作，并从那里扩展树，而不是考虑*所有*可能的动作。DeepMind开发的用于下围棋的AlphaGo算法使用深度神经网络来评估哪些动作值得进行树搜索，并决定所选移动的价值。因此，AlphaGo结合了穷举搜索和深度神经网络，以获得两者的最佳效果。这类组合算法目前是国际象棋和围棋类游戏的顶尖技术。
- en: 11.3\. The end
  id: totrans-1646
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 结束
- en: Thank you for reading our book! We really hope you have learned a satisfying
    amount about deep reinforcement learning. Please reach out to us in the forums
    at [Manning.com](http://Manning.com) with any questions or comments. We look forward
    to hearing from you.
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您阅读我们的书籍！我们真心希望您已经对深度强化学习有了令人满意的了解。如果您有任何问题或评论，请通过[Manning.com](http://Manning.com)论坛联系我们。我们期待着您的回复。
