- en: 3 Uncertainty sampling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 不确定性采样
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the scores of a model prediction
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型预测的分数
- en: Combining predictions over multiple labels into a single uncertainty score
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个标签的预测组合成一个单一的不确定性分数
- en: Combining predictions from multiple models into a single uncertainty score
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个模型的预测组合成一个单一的不确定性分数
- en: Calculating uncertainty with different kinds of machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同种类的机器学习算法计算不确定性
- en: Deciding how many items to put in front of humans per iteration cycle
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定每次迭代周期向人类展示多少个项目
- en: Evaluating the success of uncertainty sampling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估不确定性采样的成功率
- en: The most common strategy that people use to make AI smarter is for the machine
    learning models to tell humans when they are uncertain about a task and then ask
    the humans for the correct feedback. In general, unlabeled data that confuses
    an algorithm is most valuable when it is labeled and added to the training data.
    If the algorithm can already label an item with high confidence, it is probably
    correct.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人们用来使人工智能变得更聪明的最常见策略是让机器学习模型告诉人类它们在任务上不确定的时候，然后请求人类提供正确的反馈。一般来说，混淆算法的无标签数据在标记并添加到训练数据中时最有价值。如果算法可以以高置信度标记一个项目，那么它很可能是正确的。
- en: This chapter is dedicated to the problem of interpreting when our model is trying
    to tell us when it is uncertain about its task. But it is not always easy to know
    when a model is uncertain and how to calculate that uncertainty. Beyond simple
    binary labeling tasks, the different ways of measuring uncertainty can produce
    vastly different results. You need to understand and consider all methods for
    determining uncertainty to select the right one for your data and objectives.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这章致力于解释我们的模型试图告诉我们它对其任务不确定时的问题。但知道模型不确定以及如何计算这种不确定性并不总是容易的。在简单的二进制标记任务之外，衡量不确定性的不同方式可以产生截然不同的结果。你需要理解和考虑所有确定不确定性的方法，以便为你的数据和目标选择正确的方法。
- en: For example, imagine that you are building a self-driving car. You want to help
    the car understand the new types of objects (pedestrians, cyclists, street signs,
    animals, and so on) that it is encountering as it drives along. To do that, however,
    you need to understand when your car is uncertain about what object it is seeing
    and how to best interpret and address that uncertainty.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下你正在构建一辆自动驾驶汽车。你希望帮助汽车理解它在行驶过程中遇到的新类型物体（行人、骑自行车的人、街牌、动物等等）。然而，要做到这一点，你需要了解汽车在看到什么物体时感到不确定，以及如何最好地解释和应对这种不确定性。
- en: 3.1 Interpreting uncertainty in a machine learning model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 解释机器学习模型中的不确定性
- en: '*Uncertainty sampling* is a set of techniques for identifying unlabeled items
    that are near a decision boundary in your current machine learning model. Although
    it is easy to identify when a model is confident—there is one result with very
    high confidence—you have many ways to calculate uncertainty, and your choice will
    depend on your use case and what is the most effective for your particular data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*不确定性采样* 是一系列技术，用于识别当前机器学习模型决策边界附近的无标签项目。虽然识别模型何时自信很容易——有一个非常高的置信度结果——但你有很多种计算不确定性的方法，你的选择将取决于你的用例以及对你特定数据最有效的方法。'
- en: 'We explore four approaches to uncertainty sampling in this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了四种不确定性采样的方法：
- en: '*Least confidence sampling*—Difference between the most confident prediction
    and 100% confidence. In our example, if the model was most confident that a pedestrian
    was in the image, least confidence captures how confident (or uncertain) that
    prediction was.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小置信度采样*—最自信预测与100%置信度之间的差异。在我们的例子中，如果模型最自信地认为图像中有一个行人，最小置信度捕捉了该预测的置信度（或不确定性）。'
- en: '*Margin of confidence sampling* Difference between the two most confident predictions.
    In our example, if the model is most confident that a pedestrian was in the image
    and second most confident that the image contained an animal, margin of confidence
    captures the difference between the two confidences.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*置信度采样范围* 两个最自信预测之间的差异。在我们的例子中，如果模型最自信地认为图像中有一个行人，第二自信地认为图像中包含一个动物，置信度范围捕捉了这两个置信度之间的差异。'
- en: Ratio of confidence—Ratio between the two most confident predictions. In our
    example, if the model is most confident that a pedestrian was in the image and
    the second most confident that the image contained an animal, ratio captures the
    *ratio* (not difference) between the two confidences.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 置信度比率——两个最自信预测之间的比率。在我们的例子中，如果模型最自信地认为图像中有一个行人，第二自信地认为图像中包含一个动物，比率捕捉了这两个置信度之间的*比率*（而不是差异）。
- en: '*Entropy-based sampling* Difference between all predictions, as defined by
    information theory. In our example, entropy-based sampling would capture how much
    *every* confidence differed from every other.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于熵的采样*，根据信息理论定义的所有预测之间的差异。在我们的例子中，基于熵的采样将捕捉每个置信度与每个其他置信度之间的差异程度。'
- en: We’ll also look at how to determine uncertainty from different types of machine
    learning algorithms and how to calculate uncertainty when you have multiple predictions
    for each data item, such as when you are using an ensemble of models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何从不同类型的机器学习算法中确定不确定性，以及当你对每个数据项有多个预测时如何计算不确定性，例如当你使用模型集成时。
- en: Understanding the strengths and weaknesses of each method requires going deeper
    into exactly what each strategy is doing, so this chapter provides detailed examples
    along with the equations and code. You also need to know how the confidences are
    generated before you can start interpreting them correctly, so this chapter starts
    with how to interpret your model’s probability distributions, especially if they
    are generated by softmax, the most popular algorithm for generating confidences
    from neural models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理解每种方法的优缺点需要深入了解每种策略的确切操作，因此本章提供了详细的示例，包括方程和代码。在你开始正确解释置信度之前，你还需要了解置信度是如何生成的，因此本章从如何解释你模型的概率分布开始，特别是如果它们是由softmax生成的，这是从神经网络模型生成置信度最流行的算法。
- en: 3.1.1 Why look for uncertainty in your model?
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 为什么要在你的模型中寻找不确定性？
- en: Let’s return to our self-driving-car example. Suppose that your car spends most
    of its time on highways, which it is already good at navigating and which have
    a limited number of objects. You don’t see many cyclists or pedestrians on major
    highways, for example. If you randomly selected video clips from the car’s video
    cameras, your selections will mostly be from highways, where the car is already
    confident and driving well. There will be little that a human can do to improve
    the driving skills of the car if humans are mostly giving the car feedback about
    highway driving, on which the car is already confident.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的自动驾驶汽车示例。假设你的汽车大部分时间都在高速公路上行驶，它已经擅长导航，而且物体数量有限。例如，在主要高速公路上，你很少看到骑自行车的人或行人。如果你从汽车的录像中随机选择视频片段，你的选择将主要来自高速公路，汽车在那里已经很有信心并且驾驶得很好。如果人类主要对汽车在已经很有信心的高速公路驾驶方面提供反馈，那么人类几乎无法做任何事情来提高汽车的驾驶技能。
- en: Therefore, you want to know when your self-driving car is most confused as it
    is driving. So you decide to take video clips from where the car is most uncertain
    about the objects it is detecting and then have the human provide the *ground
    truth* (training data) for the objects in those video clips. The human can identify
    whether a moving object is a pedestrian, another car, a cyclist, or some other
    important object that the car’s object detection system might have missed. Different
    objects can be expected to move at different speeds and to be more or less predictable,
    which will help the car anticipate the movements of those objects.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你想要知道当你的自动驾驶汽车在行驶时最困惑的时刻。所以你决定从汽车对其检测到的物体最不确定的视频片段中提取视频片段，然后让人类为这些视频片段中的物体提供*地面真实情况*（训练数据）。人类可以识别移动物体是行人、另一辆车、骑自行车的人，或者汽车的对象检测系统可能遗漏的其他重要物体。不同的物体可以预期以不同的速度移动，并且更或更不可预测，这将帮助汽车预测这些物体的移动。
- en: It might be the case, for example, that the car was most confused when driving
    through snowstorms. If you show video clips only from a snowstorm, that data doesn’t
    help the car in the 99% of situations when it is not in a snowstorm. In fact,
    that data could make the car worse. The snowstorm will limit the visible range,
    and you could unintentionally bias the data so that the car’s behavior makes sense
    only in a snowstorm and is dangerous elsewhere. You might teach the car to ignore
    all distant objects, as they simply cannot be seen when it is snowing; thus, you
    would limit the car’s ability to anticipate objects at a distance in nonsnowing
    conditions. So you need different kinds of conditions in which your car is experiencing
    uncertainty.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，汽车在暴风雪中驾驶时可能最困惑。如果你只展示暴风雪的视频片段，那么这些数据在99%的情况下对汽车没有帮助，因为那时汽车不在暴风雪中。事实上，这些数据可能会使汽车变得更糟。暴风雪会限制可见范围，你可能会无意中使数据产生偏差，使得汽车的行为只有在暴风雪中才有意义，而在其他地方则很危险。你可能会教会汽车忽略所有远处的物体，因为下雪时它们根本看不见；这样，你将限制汽车在非暴风雪条件下预测远处物体的能力。因此，你需要不同类型的条件，在这些条件下你的汽车正在经历不确定性。
- en: Furthermore, it’s not clear how to define uncertainty in the context of multiple
    objects. Is the uncertainty about the most likely object that was predicted? Was
    it between the two most likely predictions? Or should you take into account every
    possible object when coming up with an overall uncertainty score for some object
    that the car detected? When you drill down, deciding what objects from self-driving-car
    videos you should put in front of a human for review is difficult.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在多个对象的情况下，如何定义不确定性并不明确。是不确定性关于预测的最可能对象吗？是在两个最可能的预测之间吗？或者，在为汽车检测到的某个对象制定整体不确定性得分时，你应该考虑每个可能的对象吗？当你深入挖掘时，决定应该将自动驾驶汽车视频中的哪些对象展示给人类进行审查是困难的。
- en: 'Finally, your model is not telling you in plain language when it is uncertain:
    even for a single object, the machine learning model gives you a number that might
    *correspond* to the confidence of the prediction but might not be a reliable measure
    of accuracy. Our starting point in this chapter is knowing when your model is
    uncertain. From that base, you will be able to build your broader uncertainty
    sampling strategies.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当模型不确定时，它并没有用简单易懂的语言告诉你：即使对于单个对象，机器学习模型给出的数字可能*对应*预测的置信度，但不一定是准确性的可靠度量。我们本章的起点是了解模型何时不确定。从这个基础上，你将能够构建更广泛的样本不确定性策略。
- en: The underlying assumption of all active learning techniques is that some data
    points are more valuable to your model than others. (For a specific example, see
    the following sidebar.) In this chapter, we’ll start with interpreting your model’s
    outputs by taking a look at softmax.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主动学习技术的潜在假设是，某些数据点对你的模型比其他数据点更有价值。（具体例子见以下边栏。）在本章中，我们将从通过查看softmax来解释模型输出开始。
- en: Not all data is equal
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据都同等重要
- en: '*Expert anecdote by Jennifer Prendki*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jennifer Prendki的专家轶事*'
- en: If you care about your nutrition, you don’t go to the supermarket and randomly
    select items from the shelves. You might eventually get the nutrients you need
    by eating random items from the supermarket shelves, but you will eat a lot of
    junk food in the process. I think it is weird that in machine learning, people
    still think it’s better to sample the supermarket randomly than figure out what
    they need and focusing their efforts there.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关心你的营养，你不会去超市随意从货架上挑选商品。你可能会通过从超市货架上随机挑选商品最终获得所需的营养，但在这个过程中你会吃很多垃圾食品。我认为在机器学习中，人们仍然认为随机采样超市比找出他们需要的东西并集中精力在那里更好，这很奇怪。
- en: The first active learning system I built was by necessity. I was building machine
    learning systems to help a large retail store make sure that when someone searched
    on the website, the right combination of products came up. Almost overnight, a
    company reorg meant that my human labeling budget was cut in half, and we had
    a 10x increase in inventory that we had to label. So my labeling team had only
    5% the budget per item that we previously did. I created my first active learning
    framework to discover which was the most important 5%. The results were better
    than random sampling with a bigger budget. I have used active learning in most
    of my projects ever since, because not all data is equal!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我构建的第一个主动学习系统是迫不得已的。我正在构建机器学习系统，以帮助一家大型零售店确保当有人在网站上搜索时，正确的产品组合会显示出来。一夜之间，公司重组意味着我的人力标注预算减半，而我们需要标注的库存增加了10倍。因此，我的标注团队每个项目的预算只有我们之前的一半。我创建了第一个主动学习框架，以发现最重要的5%。结果比使用更大预算的随机采样要好。从那时起，我在大多数项目中都使用了主动学习，因为并非所有数据都是平等的！
- en: '*Jennifer Prendki is the CEO of Alectio, a company that specializes in finding
    data for machine learning. She previously led data science teams at Atlassian,
    Figure Eight, and Walmart*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jennifer Prendki是Alectio公司的首席执行官，该公司专门寻找机器学习所需的数据。她之前在Atlassian、Figure Eight和Walmart领导数据科学团队*。'
- en: 3.1.2 Softmax and probability distributions
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 Softmax和概率分布
- en: 'As you discovered in chapter 2, almost all machine learning models give you
    two things:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第2章中发现的那样，几乎所有的机器学习模型都给你两样东西：
- en: A predicted label (or set of predictions)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测标签（或一组预测）
- en: A number (or set of numbers) associated with each predicted label
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与每个预测标签相关联的数字（或一组数字）
- en: Let’s assume that we have a simple object detection model for our self-driving
    car, one that tries to distinguish among only four types of objects. The model
    might give us a prediction like the following.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个简单的自动驾驶汽车目标检测模型，它试图区分只有四种类型的对象。该模型可能会给出以下预测。
- en: Listing 3.1 JSON-encoded example of a prediction
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 JSON编码的预测示例
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ In this prediction, the object is predicted to be “Cyclist” with 91.9% accuracy.
    The scores will add to 100%, giving us the probability distribution for this item.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在这个预测中，对象被预测为“Cyclist”，准确率为91.9%。这些分数相加将等于100%，从而给出该项目的概率分布。
- en: This output is most likely from *softmax* which converts the logits to a 0–1
    range of scores using the exponents. Softmax is defined as follows
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出很可能是来自*softmax*，它通过指数将logits转换为0-1范围的分数。softmax的定义如下
- en: '![](../Images/CH03_F00_Munro_E01.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F00_Munro_E01.png)'
- en: and as shown in figure 3.1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.1所示。
- en: '![](../Images/CH03_F01_Munro.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F01_Munro.png)'
- en: Figure 3.1 How softmax creates probability distributions. A linear activation
    function is used on the output layer, creating model scores (logits) that are
    then converted to probability distributions via softmax.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 如何通过softmax创建概率分布。输出层使用线性激活函数，生成模型得分（logits），然后通过softmax转换为概率分布。
- en: Because *softmax* divides by exponentials, it loses the scale of the logits.
    The logits in figure 3.1, for example, are [1, 4, 2, 1]. If the logits were [101,
    104, 102, 101], softmax would produce the same probability distribution, so the
    level of activation in our model is lost in the output. We’ll look at how to take
    activation into account in chapter 4\. In this chapter, it’s important to understand
    how some information is lost when only the probability distribution is used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*softmax*除以指数，它失去了logits的规模。例如，图3.1中的logits是[1, 4, 2, 1]。如果logits是[101, 104,
    102, 101]，softmax将产生相同的概率分布，所以模型中的激活水平在输出中丢失了。我们将在第4章中探讨如何考虑激活。在本章中，理解仅使用概率分布时丢失了一些信息是很重要的。
- en: If you have only used the outputs of softmax in the past, I strongly recommend
    reading the appendix. As explained there, the softmax base (. is arbitrary, and
    by changing the base, you can change the ranked order of confidence for predictions
    on different items. This fact isn’t widely known and wasn’t reported at all before
    this book. Rank order is important for uncertainty sampling, as you will see in
    this chapter, so for your own experiments, you might want to try changing the
    softmax base (or, equivalently, the temperature) in addition to employing the
    techniques described later in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你过去只使用了softmax的输出，我强烈建议阅读附录。正如其中解释的那样，softmax的基数（.是任意的）可以通过改变基数来改变对不同项目预测的置信度排名顺序。这个事实并不广为人知，在此书之前从未有过报道。排名顺序对于不确定性采样很重要，正如你将在本章中看到的那样，因此在你自己的实验中，你可能想尝试改变softmax的基数（或等价地，温度），除了采用本章后面描述的技术之外。
- en: One common way to get more accurate confidences from your model is to adjust
    the base/temperature of softmax by using a validation dataset so that the probability
    distribution matches the actual accuracy as closely as possible. You might adjust
    the base/temperature of softmax so that a confidence score of 0.7 is correct 70%
    of the time, for example. A more powerful alternative to adjusting the base/temperature
    is using a local regression method such as LOESS to map your probability distributions
    to the actual accuracy on your validation data. Every stats package will have
    one or more local regression methods that you can experiment with.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的模型中获得更准确的置信度的一个常见方法是通过使用验证数据集调整softmax的基数/温度，以便概率分布尽可能接近实际准确性。例如，你可能调整softmax的基数/温度，使得置信度分数0.7有70%的时间是正确的。调整基数/温度的一个更强大的替代方法是使用局部回归方法，如LOESS，将你的概率分布映射到验证数据上的实际准确性。每个统计软件包都将包含一个或多个你可以实验的局部回归方法。
- en: If you are modeling uncertainty only so that you can sample the most uncertain
    items for active learning, however, it might not matter if the probability distributions
    are not accurate reflections of the accuracy. Your choice will depend on what
    you are trying to achieve, and it helps to know all the techniques that are available.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是建模不确定性以便采样最不确定的项目进行主动学习，那么概率分布是否准确反映准确性可能并不重要。你的选择将取决于你试图实现的目标，了解所有可用的技术会有所帮助。
- en: 3.1.3 Interpreting the success of active learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 解释主动学习的成功
- en: You can calculate the success of active learning with accuracy metrics such
    as F-score and AUC, as you did in chapter 2\. If you come from an algorithms background,
    this technique will be familiar to you.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用F分数和AUC等准确性指标来计算主动学习的成功，就像你在第2章中所做的那样。如果你来自算法背景，这项技术对你来说将是熟悉的。
- en: Sometimes, however, it makes more sense to look at the human cost. You could
    compare two active learning strategies in terms of the number of human labels
    that are required to get to a certain accuracy target, for example. This can be
    substantially bigger or smaller than comparing the accuracy with the same number
    of labels, so it can be useful to calculate both.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时考虑人力成本更有意义。例如，你可以通过所需的人类标签数量来比较两种主动学习策略以实现一定的准确性目标。这可能会比使用相同数量的标签比较准确性大得多或小得多，因此计算两者都很有用。
- en: If you are not putting the items back into the training data, and therefore
    not implementing the full active learning cycle, it makes more sense to evaluate
    purely in terms of how many *incorrect* predictions were surfaced by uncertainty
    sampling. That is, when you sample the *N* most uncertain items, what percentage
    was incorrectly predicted by the model?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有将项目放回训练数据中，因此没有实施完整的主动学习周期，那么仅从不确定性采样中暴露出的多少个*错误*预测来评估更有意义。也就是说，当你采样*N*个最不确定的项目时，模型错误预测的百分比是多少？
- en: For more on human-centric approaches to evaluating quality, such as the amount
    of time needed to annotate data, see the appendix, which goes into more detail
    about ways to measure model performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于以人为中心的评估质量方法，例如标注数据所需的时间，请参阅附录，其中更详细地介绍了衡量模型性能的方法。
- en: 3.2 Algorithms for uncertainty sampling
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 不确定性采样算法
- en: Now that you understand where the confidences in the model predictions come
    from, you can think about how to interpret the probability distributions to find
    out where your machine learning models are most uncertain.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了模型预测中的置信度来源，你可以思考如何解释概率分布，以找出你的机器学习模型最不确定的地方。
- en: Uncertainty sampling is a strategy for identifying unlabeled items that are
    near a decision boundary in your current machine learning model. If you have a
    binary classification task, like the one you saw in chapter 2, these items are
    predicted as being close to 50% probability of belonging to either label; therefore,
    the model is uncertain. These items are most likely to be classified wrongly;
    therefore, they are the most likely to result in a human label that is different
    from the predicted label. Figure 3.2 shows how uncertainty sampling should find
    items close to the decision boundary.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样是一种识别当前机器学习模型中接近决策边界的未标记项目的策略。如果你有一个二元分类任务，就像你在第2章中看到的那样，这些项目被预测为接近50%的概率属于任一标签；因此，模型是不确定的。这些项目最有可能被错误分类；因此，它们最有可能导致人类标签与预测标签不同。图3.2显示了不确定性采样应该如何找到接近决策边界的项目。
- en: '![](../Images/CH03_F02_Munro.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Munro.png)'
- en: Figure 3.2 Uncertainty sampling is an active learning strategy that oversamples
    unlabeled items that are closer to the decision boundary (and sometimes to one
    another), and are therefore more likely to get a human label that results in a
    change in that decision boundary.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 不确定性采样是一种主动学习策略，它对接近决策边界（有时彼此接近）的未标记项目进行过采样，因此更有可能得到导致决策边界变化的人类标签。
- en: 'There are many algorithms for calculating uncertainty, some of which we will
    visit here. They all follow the same principles:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多计算不确定性的算法，其中一些我们将在这里介绍。它们都遵循相同的原理：
- en: Apply the uncertainty sampling algorithm to a large pool of predictions to generate
    a single uncertainty score for each item.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不确定性采样算法应用于大量预测，为每个项目生成一个不确定性分数。
- en: Rank the predictions by the uncertainty score.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按不确定性分数对预测进行排序。
- en: Select the top *N* most uncertain items for human review.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择前*N*个最不确定的项目进行人工审查。
- en: Obtain human labels for the top *N* items, retrain the model with those items,
    and iterate on the processes.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为前*N*个项目获取人类标签，用这些项目重新训练模型，并迭代这个过程。
- en: 'The three methods covered in this chapter are invariant of the data being predicted:
    a given item will get the same uncertainty score independent of the scores given
    to other items being predicted. This invariance helps with the simplicity and
    predictability of the approaches in this chapter: the rank order of uncertainty
    scores is enough to find the most uncertain across a set of predictions. Other
    techniques, however, can take the distribution of predictions to change the individual
    scores. We will return to this topic in chapters 5 and 6.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的三种方法对预测的数据是无关的：给定项目将获得相同的置信度分数，无论其他被预测项目的分数如何。这种无关性有助于本章中方法的简单性和可预测性：不确定性分数的排名顺序足以在一系列预测中找到最不确定的。然而，其他技术可以采用预测的分布来改变个别分数。我们将在第5章和第6章回到这个话题。
- en: Note For binary classification tasks, the strategies in this chapter are identical,
    but for three or more labels, the strategies diverge quickly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于二元分类任务，本章中的策略是相同的，但对于三个或更多标签，策略会迅速分化。
- en: 3.2.1 Least confidence sampling
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 最小置信度采样
- en: 'The simplest and most common method for uncertainty sampling takes the difference
    between 100% confidence and the most confidently predicted label for each item.
    You saw this implementation of active learning in chapter 2\. Let’s refer to the
    softmax result as the probability of the label given the prediction. We know that
    softmax isn’t strictly giving us probabilities, but these equations are general
    equations that apply to probability distributions from any sources, not only from
    softmax. The basic equation is simply the probability of the highest confidence
    for the label, which you implemented in chapter 2:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且最常用的不确定性采样方法是将每个项目的100%置信度与最自信预测的标签之间的差异。你在第2章中看到了这种主动学习的实现。让我们将softmax的结果称为给定预测的标签概率。我们知道softmax并不严格给出概率，但这些方程是适用于任何来源的概率分布的通用方程，而不仅仅是softmax。基本方程只是标签最高置信度的概率，这是你在第2章中实现的：
- en: '![](../Images/CH03_F02_Munro_E01.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Munro_E01.png)'
- en: Although you can rank order by confidence alone, it can be useful to convert
    the uncertainty scores to a 0–1 range, where 1 is the most uncertain score. In
    that case, we have to normalize the score. We subtract the value from 1, multiply
    the result by the number of labels, and divide by the number of labels –1\. We
    do this because the minimum confidence can never be less than the one divided
    by the number of labels, which is when all labels have the same predicted confidence.
    So least confidence sampling with a 0-1 range is
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以仅根据置信度进行排序，但将不确定性得分转换为 0–1 范围可能很有用，其中 1 是最不确定的得分。在这种情况下，我们必须归一化得分。我们从 1
    减去值，将结果乘以标签数量，然后除以标签数量 - 1。我们这样做是因为最小置信度永远不会低于标签数量的倒数，这是当所有标签都有相同的预测置信度时。因此，0-1
    范围的最小置信度采样是
- en: '![](../Images/CH03_F02_Munro_E02.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Munro_E02.png)'
- en: The following listing has an implementation of least confidence sampling in
    PyTorch.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了 PyTorch 中最小置信度采样的实现。
- en: Listing 3.2 Least confidence sampling in PyTorch
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 PyTorch 中的最小置信度采样
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s apply least confidence to get an uncertainty score for our self-driving-car
    prediction. The confidence for “Pedestrian” is all that counts here. Using our
    example, this uncertainty score would be (1 – 0.6439) * (4 / 3) = 0.4748\. Least
    confidence sampling, therefore, gives you ranked order of predictions where you
    will sample items with the lowest confidence for their predicted label. This method
    is sensitive to the values of the second, third, and so on only in that the sum
    of the other predictions will be the score itself: the amount of confidence that
    will go to labels other than the most confident.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用最小置信度来获取自动驾驶汽车预测的不确定性得分。这里只计算“行人”的置信度。使用我们的示例，这个不确定性得分将是 (1 – 0.6439) *
    (4 / 3) = 0.4748。因此，最小置信度采样为你提供了预测的排序顺序，其中你会采样预测标签置信度最低的项目。这种方法对第二、第三等值敏感，仅在于其他预测的总和将是得分本身：将分配给最自信标签之外标签的置信度量。
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 骑行者 | 行人 | 标志 | 动物 |'
- en: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
- en: 'This method will not be sensitive to uncertainty between any of the other predictions:
    with the same confidence for the most confident, the second to *n*th confidences
    can take any values without changing the uncertainty score. If you care only about
    the most confident prediction for your particular use case, this method is a good
    starting point. Otherwise, you will want to use one of methods discussed in the
    following sections.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法不会对其他任何预测之间的不确定性敏感：对于最自信的预测具有相同的置信度，从第二到第 *n* 个置信度可以取任何值，而不会改变不确定性得分。如果你只关心特定用例中最自信的预测，这种方法是一个很好的起点。否则，你可能希望使用以下章节中讨论的其中一种方法。
- en: Least confidence is sensitive to the base used for the softmax algorithm. This
    example is a little counterintuitive, but recall the example in which softmax(base=10)
    gives ~0.9 confidence, which would result in an uncertainty score of 0.1—much
    less than 0.35 on the same data. For different bases, this score will change the
    overall ranking. Higher bases for softmax will stretch out the differences between
    the most confident label and the other labels; therefore, at higher bases, the
    difference between the label confidences will come to weigh more than the absolute
    difference between the most-confident label and 1.0.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最小置信度对 softmax 算法的基数敏感。这个例子有点反直觉，但回想一下 softmax(base=10) 给出 ~0.9 的置信度，这将导致不确定性得分为
    0.1——比相同数据上的 0.35 小得多。对于不同的基数，这个得分将改变整体排名。softmax 的更高基数会拉大最自信标签和其他标签之间的差异；因此，在更高基数下，标签置信度之间的差异将比最自信标签和
    1.0 之间的绝对差异更重要。
- en: 3.2.2 Margin of confidence sampling
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 置信度边际采样
- en: The most intuitive form of uncertainty sampling is the difference between the
    two most confident predictions. That is, for the label that the model predicted,
    how much more confident was it than for the next-most-confident label? This is
    defined as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观的不确定性采样形式是两个最自信预测之间的差异。也就是说，对于模型预测的标签，它比下一个最自信标签更自信多少？这被定义为
- en: '![](../Images/CH03_F02_Munro_E03.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Munro_E03.png)'
- en: 'Again, we can convert this to a 0–1 range. We have to subtract from 1.0 again,
    but the maximum possible score is already 1, so there is no need to multiply by
    any factor:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们可以将其转换为 0–1 范围。我们再次需要从 1.0 中减去，但最大可能的得分已经是 1，因此不需要乘以任何系数：
- en: '![](../Images/CH03_F02_Munro_E04.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Munro_E04.png)'
- en: Following is an implementation of margin of confidence sampling with PyTorch.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用 PyTorch 实现的置信度范围采样。
- en: Listing 3.3 Margin of confidence sampling in PyTorch
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 PyTorch 中的置信度范围采样
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s apply margin of confidence sampling to our example data. “Pedestrian”
    and “Animal” are the most-confident and second-most-confident prediction. Using
    our example, this uncertainty score would be 1.0 - (0.6439 - 0.2369) = 0.5930.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将置信度范围采样应用于我们的示例数据。“行人”和“动物”是最自信和次自信的预测。使用我们的示例，这个不确定性得分将是 1.0 - (0.6439
    - 0.2369) = 0.5930。
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 自行车 | 行人 | 标志 | 动物 |'
- en: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
- en: 'This method will not be sensitive to uncertainty for any but the two most confident
    predictions: with the same difference in confidence for the most and second-most
    confident, the third to *nth* confidences can take any values without changing
    the uncertainty score.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对除了两个最自信的预测之外的不确定性不会敏感：对于最自信和次自信的预测，相同的置信度差异，从第三到第 *n* 个置信度可以取任何值，而不会改变不确定性得分。
- en: If you care only about the uncertainty between the predicted label and the next-most-confident
    prediction for your particular use case, this method is a good starting point.
    This type of uncertainty sampling is the most common type that I’ve seen people
    use in industry.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只关心你特定用例中预测标签和次自信预测之间的不确定性，这个方法是一个好的起点。这种类型的不确定性采样是我在工业界看到人们使用最常见的一种。
- en: Margin of confidence is less sensitive than least confidence sampling to the
    base used for the softmax algorithm, but it is still sensitive. Although softmax(base=10)
    would give a margin of confidence score of 0.1899 for our dataset, compared with
    0.5930 with base *e*, all of the two most probable scores will move. Those scores
    will move at slightly different rates, depending on the total relative difference
    of all raw scores, but recall that we are sampling from when the model is most
    uncertain—that is, when the most-confident scores tend to be as low as possible
    and therefore most similar. For this reason, you might get a difference of only
    a few percentage points when you sample the most uncertain items by margin of
    confidence sampling under different bases of softmax.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度范围比最小置信度采样对 softmax 算法中使用的基数更不敏感，但它仍然敏感。尽管 softmax(base=10) 会给我们的数据集一个置信度范围得分为
    0.1899，与 base *e* 的 0.5930 相比，所有两个最可能的分数都会移动。这些分数将以不同的速率移动，这取决于所有原始分数的总相对差异，但请记住，我们是在模型最不确定的时候进行采样——也就是说，当最自信的分数尽可能低且因此最相似的时候。因此，当你在不同的
    softmax 基数下通过置信度范围采样采样最不确定的项目时，你可能只会得到几个百分点的差异。
- en: 3.2.3 Ratio sampling
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 比率采样
- en: 'Ratio of confidence is a slight variation on margin of confidence, looking
    at the ratio between the top two scores instead of the difference. It is the best
    uncertainty sampling method for improving your understanding of the relationship
    between confidence and softmax. To make the technique a little more intuitive,
    think of the ratio as capturing how many times more likely the first label was
    than the second-most-confident:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度比率是对置信度范围的轻微变化，它查看前两个分数之间的比率，而不是差异。这是改善你对置信度和 softmax 之间关系理解的最佳不确定性采样方法。为了使技术更直观，将比率视为捕捉第一个标签比次自信标签更有可能多少次：
- en: '![](../Images/CH03_F02_Munro_E05.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Munro_E05.png)'
- en: 'Now let’s plug in our numbers again:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次输入我们的数字：
- en: 0.6439 / 0.2369 = 2.71828
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 0.6439 / 0.2369 = 2.71828
- en: We get back the natural log, *e* = 2.71828! Similarly, if we use base 10, we
    get
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了自然对数，*e* = 2.71828！同样，如果我们使用以 10 为底，我们得到
- en: 90.01% / 9.001% = 10
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 90.01% / 9.001% = 10
- en: 'We get back 10—the base we used! This example is a good illustration of why
    *e* is an arbitrary base for generating confidences. (See the appendix for more
    on this topic.). Is “Pedestrian” really 2.71828 more likely as a prediction than
    “Animal” in this context? Probably not. It’s doubtful that it’s exactly 10 times
    more likely, either. The only thing that ratio of confidence is telling us is
    that the raw score from our models was “1” different between “Pedestrian” and
    “Animal”—nothing more. Ratio of confidence with a division can be defined in terms
    of the raw scores, in this case with softmax(base=), where is the base used for
    softmax (if not *e*):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了10——我们使用的基数！这个例子很好地说明了为什么 *e* 是生成置信度的任意基数。（有关此主题的更多信息，请参阅附录。）在这个上下文中，“行人”真的比“动物”更有可能作为预测吗？可能不是。怀疑它确实比“动物”多10倍的可能性。唯一告诉我们的是，置信度比率告诉我们“行人”和“动物”之间的原始得分差异为“1”——仅此而已。使用除法的置信度比率可以用原始得分来定义，在这种情况下，使用softmax(base=)，其中是softmax中使用的基数（如果不是
    *e*）：
- en: '![](../Images/CH03_F02_Munro_E06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Munro_E06.png)'
- en: 'Ratio of confidence is invariant across any base used in softmax. The score
    is determined wholly by the distance between the top two raw scores from your
    model; therefore, scaling by the base or temperature will not change the rank
    order. To give ratio of confidence a 0-1 normalized range, you can simply take
    the inverse of the preceding equation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度比率在softmax中使用的任何基数上都是不变的。得分完全由你的模型中最高两个原始得分之间的距离决定；因此，通过基数或温度进行缩放不会改变排名顺序。为了给置信度比率一个0-1的归一化范围，你只需取前面方程的倒数：
- en: '![](../Images/CH03_F02_Munro_E07.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Munro_E07.png)'
- en: We used the noninverted version above so that it directly outputs their softmax
    base for illustrative purposes. The following listing has an implementation of
    ratio of confidence sampling using PyTorch.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上面的非倒置版本是为了直接输出它们的softmax基数，以便说明。以下列表展示了使用PyTorch实现的置信度比率采样。
- en: Listing 3.4 Ratio of confidence sampling in PyTorch
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 PyTorch中的置信度比率采样
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'I hope that this example gives you another good way to intuit why margin of
    confidence sampling is relatively invariant: there’s no big difference between
    subtracting your two highest values and dividing your two highest values when
    your goal is to rank them.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个例子能给你另一个直观的理由，说明为什么置信度采样边际相对不变：当你的目标是进行排名时，从你的两个最高值中减去和除以你的两个最高值之间没有太大差异。
- en: Happily, where margin of confidence with subtraction *does* differ from ratio
    of confidence, it does what we want by favoring the most uncertain. Although margin
    of confidence and ratio of confidence don’t explicitly look at the confidences
    beyond the two most confident, they influence the possible values. If the third-most-confident
    value is 0.25, the first and second can differ by 0.5 at most. So if the third-most-confident
    prediction is relatively close to the first and second, the uncertainty score
    for margin of confidence increases. This variation is small and doesn’t occur
    directly as a result of margin of confidence; it is a byproduct of the denominator
    in the softmax equation being larger as a result of the larger score for the third-most-confident
    value, which becomes disproportionately larger as an exponential. Nonetheless,
    this behavior is right; all else being equal, margin of confidence looks for uncertainty
    beyond the two most confident predictions in what would otherwise be a tie.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，当置信度边际减法与置信度比率不同时，它通过优先考虑最不确定的来做到我们想要的事情。尽管置信度边际和置信度比率没有明确查看两个最自信的置信度之外的内容，但它们影响了可能的值。如果第三自信的值是0.25，第一和第二的值最多可以相差0.5。所以如果第三自信的预测相对接近第一和第二，置信度边际的不确定性得分就会增加。这种变化很小，并不是直接由置信度边际引起的；它是softmax方程中的分母更大的副产品，由于第三自信的值更大，它成为不成比例地更大的指数。尽管如此，这种行为是正确的；在其他条件相同的情况下，置信度边际在可能平局的情况下寻找两个最自信预测之外的不确定性。
- en: Unlike margin of confidence, in which the variation from the third to *nth*
    predictions is a lucky byproduct of softmax, our next-most-popular uncertainty
    sampling strategy explicitly models all the predictions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与置信度边际不同，其中从第三到 *nth* 预测的变化是softmax的幸运副产品，我们的下一个最受欢迎的不确定性采样策略明确地模拟了所有预测。
- en: 3.2.4 Entropy (classification entropy)
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 熵（分类熵）
- en: One way to look at uncertainty in a set of predictions is by whether you expect
    to be surprised by the outcome. This concept underlies the entropy technique.
    How surprised would you be by each of the possible outcomes, relative to their
    probability?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 观察一组预测中的不确定性的方法之一是看你是否会因结果而感到惊讶。这一概念是熵技术的基础。相对于它们的概率，你会对每个可能的结果感到多么惊讶？
- en: It is intuitive to think about entropy and surprise in terms of a sporting team
    you supported for a long time even though it was on a losing streak. For me, that
    team is the Detroit Lions American football team. In recent years, even when the
    Lions are ahead early in a game, they still have only a 50% chance of winning
    that game. So even if the Lions are up early in the game, I don’t know what the
    result will be, and there is an equal amount of surprise either way in every game.
    Entropy does not measure the emotional toll of losing—only the surprise. The entropy
    equation gives us a mathematically well-motivated way to calculate the surprise
    for outcomes, as shown in figure 3.3.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将熵和惊讶用你长期支持的体育队伍来考虑是很直观的，即使它处于输球状态。对我来说，这支队伍是底特律雄狮美式足球队。近年来，即使雄狮在比赛中早期领先，他们赢得比赛的概率也只有50%。所以即使雄狮在比赛中早期领先，我也不知道结果会怎样，而且每场比赛无论哪种结果都会有相等程度的惊讶。熵并不衡量输球的情感负担——只衡量惊讶。熵方程为我们提供了一个数学上合理的计算结果惊讶的方法，如图3.3所示。
- en: '![](../Images/CH03_F03_Munro.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F03_Munro.png)'
- en: Figure 3.3 Example of low entropy (left) and high entropy (right). High entropy
    occurs when the probabilities are most like one another and there is the most
    surprise in any one prediction from the distribution. Entropy is sometimes a little
    counterintuitive, because the left graph has the most variability and three highly
    unlikely events. Those three unlikely events, however, are more than canceled
    by the one highly likely event. Four events at around equal likelihood will have
    greater total entropy, even if the three rarer events would have greater information
    on the rare times that they occur.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 低熵（左）和高熵（右）的示例。高熵发生在概率最相似的时候，并且从分布中任何单个预测的惊讶程度最大。熵有时有点反直觉，因为左边的图有最大的变异性和三个极不可能的事件。然而，这三个不可能的事件却被一个高度可能的事件所抵消。四个在相等可能性附近的事件将具有更大的总熵，即使这三个罕见的事件在它们发生的罕见时刻会有更大的信息。
- en: 'Entropy applied to a probability distribution involves multiplying each probability
    by its own log and taking the negative sum:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将熵应用于概率分布，涉及将每个概率乘以其自身的对数并取负和：
- en: '![](../Images/CH03_F03_Munro_E01.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F03_Munro_E01.png)'
- en: 'We can convert the entropy to a 0–1 range by dividing by the log of the number
    of predictions (labels):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过除以预测（标签）数量的对数将熵转换为0-1范围：
- en: '![](../Images/CH03_F03_Munro_E03.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F03_Munro_E03.png)'
- en: The following listing shows an implementation of ratio of entropy score using
    Python and the PyTorch library.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用Python和PyTorch库实现熵分数比率的示例。
- en: Listing 3.5 Entropy-based sampling in PyTorch
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 PyTorch中的基于熵的采样
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Multiply each probability by its base 2 log.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将每个概率乘以其以2为底的对数。
- en: 'First, don’t be scared by another arbitrary base, log(base=2), which is used
    for historical reasons: the choice of base for entropy does not change the uncertainty
    sampling rank order. Unlike with softmax, calculating the entropy with different
    bases for uncertainty sampling does *not* change the rank order of scores across
    a dataset. You will get different entropy scores depending on the base, but the
    entropy scores will change monotonically for every probability distribution and
    therefore will not change the rank order for uncertainty sampling. Base 2 is used
    in entropy for historical reasons, as entropy comes from information theory, which
    deals with compressing data streams in binary bits. Let’s calculate the entropy
    on our example data:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，不要因为另一个任意的基数，即用于历史原因的log(base=2)，而感到害怕：熵的基数选择不会改变不确定性采样的排名顺序。与softmax不同，使用不同基数的熵计算对于不确定性采样不会改变数据集上分数的排名顺序。根据基数，你会得到不同的熵分数，但熵分数将对于每个概率分布单调变化，因此不会改变不确定性采样的排名顺序。以2为基数用于熵的历史原因，因为熵来自信息理论，它处理以二进制位压缩数据流。让我们计算我们示例数据的熵：
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 骑行者 | 行人 | 标志 | 动物 |'
- en: '| P(y&#124;x) aka softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| P(y|x) 即softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
- en: '| log2(P(y&#124;x)) | –4.963 | –0.635 | –3.520 | –2.078 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| log2(P(y|x)) | –4.963 | –0.635 | –3.520 | –2.078 |'
- en: '| P(y&#124;x) log2(P(y&#124;x)) | –0.159 | –0.409 | –0.307 | –0.492 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| P(y|x) log2(P(y|x)) | –0.159 | –0.409 | –0.307 | –0.492 |'
- en: Summing the numbers and negating them returns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些数字相加并取反
- en: 0 – SUM(–0.159, –0.409, –0.307, –0.492) = 1.367
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 0 – SUM(–0.159, –0.409, –0.307, –0.492) = 1.367
- en: Dividing by the log of the number of labels returns
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除以标签数量的对数返回
- en: 1.367 / log[2](4) = 0.684
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 1.367 / log[2](4) = 0.684
- en: Note that the P(y|x) log(P(y|x)) step is not monotonic with respect to the probability
    distribution given by softmax. “Pedestrian” returns –0.409, but “Animal” returns
    –0.492\. So “Animal” contributes most to the final entropy score even though it
    is neither the most-confident or least-confident prediction.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，P(y|x) log(P(y|x))这一步与softmax算法给出的概率分布不是单调的。“行人”返回-0.409，但“动物”返回-0.492。因此，“动物”尽管不是最自信或最不自信的预测，却对最终的熵得分贡献最大。
- en: 'Data ranked for uncertainty by entropy is sensitive to the base used by the
    softmax algorithm and about equally sensitive as least confidence. It is intuitive
    why this is the case: entropy *explicitly* uses every number in the probability
    distribution, so the further these numbers are spread out via a higher base, the
    more divergent the result will be.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据熵对数据进行排序的不确定性对softmax算法使用的基数很敏感，并且与最小置信度几乎同样敏感。为什么是这样很直观：熵*明确地*使用了概率分布中的每一个数字，因此这些数字通过更高的基数分散得越远，结果就会越发散。
- en: Recall our example in which softmax(base=10) gives ~0.9% confidence, which would
    result in an uncertainty score of 0.1—much less than 0.35 on the same data. For
    different bases, this score will change the overall ranking. Higher bases for
    softmax will stretch out the differences between the most-confident label and
    the other labels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们之前的例子，其中softmax(base=10)给出约0.9%的置信度，这将导致不确定性得分为0.1——远小于相同数据上的0.35。对于不同的基数，这个分数将改变整体排名。softmax的更高基数会拉大最自信标签和其他标签之间的差异。
- en: 3.2.5 A deep dive on entropy
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 深入探讨熵
- en: If you want to get deeper into entropy, you can try plugging different confidences
    into the inner part of the equation where each confidence is multiplied by its
    own log, such as 0.3 * log(0.3). For this measure of entropy, the per-prediction
    score of P(y|x) log(P(y|x)) will return the largest (negative) numbers for confidences
    of around 0.3679\. Unlike in softmax, Euler’s number is special, as e^(-1) = 0.3679\.
    The formula used to derive this result is known as *Euler’s Rule*, itself a derivation
    of the *Thâbit ibn Kurrah Rule* created sometime in the ninth century to generate
    amicable numbers. The largest (negative) numbers for each prediction will be around
    0.3679 no matter which base you use for entropy, which should help you understand
    why the base doesn’t matter in this case.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解熵，可以尝试将不同的置信度插入方程的内侧，每个置信度乘以其自身的对数，例如0.3 * log(0.3)。对于这种熵度量，P(y|x) log(P(y|x))的每次预测得分将返回大约0.3679的（负）最大值。与softmax不同，欧拉数是特殊的，因为e^(-1)
    = 0.3679。用来推导这个结果的公式被称为*欧拉公式*，它本身是9世纪左右创造的*Thâbit ibn Kurrah规则*的推导。无论你使用什么基数来计算熵，每个预测的最大（负）值都会接近0.3679，这应该有助于你理解为什么在这个情况下基数并不重要。
- en: 'You will encounter entropy in a few places in machine learning and signal processing,
    so this equation is a good one to get your head around. Fortunately, you don’t
    need to derive Euler’s Rule or the Thâbit ibn Kurrah Rule to use entropy for uncertainty
    sampling. The intuition that 0.3679 (or a number near it) contributes most to
    entropy is fairly simple:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和信号处理中，你会在几个地方遇到熵，所以这个方程是一个很好的理解点。幸运的是，你不需要推导欧拉公式或Thâbit ibn Kurrah规则来使用熵进行不确定性采样。0.3679（或接近这个数字）对熵贡献最大的直觉相当简单：
- en: If the probability is 1.0, the model is completely predictable and has no entropy.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果概率是1.0，模型是完全可预测的，没有熵。
- en: If the probability is 0.0, that data point provides no contribution to entropy,
    as it is never going to happen.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果概率是0.0，那么这个数据点对熵没有任何贡献，因为它永远不会发生。
- en: Therefore, some number between 0.0 and 1.0 is optimal for entropy on a per-prediction
    basis.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，在每次预测的基础上，0.0和1.0之间的某个数字对熵是最优的。
- en: But 0.3679 is optimal only for individual probabilities. By using 0.3679 of
    the probability for one label, you are leaving only 0.6431 for every other label.
    So the highest entropy for the entire probability distribution, not individual
    values alone, will always occur when each probability is identical and equal to
    one divided by the number of labels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 但0.3679仅对单个概率是最佳的。使用一个标签的概率为0.3679，你将只为每个其他标签留下0.6431。因此，整个概率分布的最高熵，而不仅仅是单个值，总是在每个概率都相同且等于标签数量的倒数时发生。
- en: 3.3 Identifying when different types of models are confused
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 识别不同类型模型混淆的时刻
- en: You are most likely using neural models in machine learning, but there are many
    different architectures for neural models and many other popular types of supervised
    machine learning algorithms. Almost every machine learning library or service
    will return some form of scores for the algorithms in them, and these scores can
    be used for uncertainty sampling. In some cases, you will be able to use the scores
    directly; in other cases, you will have to convert the scores to probability distributions
    using something like softmax.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你最有可能在机器学习中使用神经网络模型，但神经网络模型有许多不同的架构，还有许多其他流行的监督机器学习算法。几乎每个机器学习库或服务都会返回它们算法的某种形式的分数，这些分数可以用于不确定性采样。在某些情况下，你将能够直接使用这些分数；在其他情况下，你将不得不使用类似softmax的方法将分数转换为概率分布。
- en: Even if you are using only predictive models from neural networks or the default
    settings on common machine learning libraries and services, it is useful to understand
    the full range of algorithms and how uncertainty is defined in different kinds
    of machine learning models. Some are much different from the interpretations that
    we make from neural network models, but not necessarily any better or worse, so
    it will help you appreciate the strengths and weaknesses of different common approaches.
    The strategies for determining uncertainty for different types of machine learning
    algorithms are summarized in figure 3.4 and expanded on in more detail in this
    section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你只使用神经网络或常见机器学习库和服务的默认设置进行预测模型，了解算法的全范围以及不同类型的机器学习模型中不确定性的定义也是很有用的。有些与我们从神经网络模型中得出的解释大不相同，但并不一定更好或更差，因此这将帮助你欣赏不同常见方法的优缺点。不同类型机器学习算法的不确定性确定策略总结在图3.4中，并在本节中更详细地展开。
- en: '![](../Images/CH03_F04_Munro.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F04_Munro.png)'
- en: Figure 3.4 Uncertainty sampling from different supervised machine learning algorithms.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 不同监督机器学习算法的不确定性采样。
- en: '*Top left*: The decision boundary from a support vector machine (SVM). A discriminative
    learner, like a neural model, attempts to find a way to divide the data optimally.
    Unlike neural classifiers, SVMs are also trying to maximize the width of the boundary.
    This is how an SVM decides which of multiple possible central lines is the best
    division: it has the widest boundary. Note that the distance from the divider
    (the hyperplane for SVMs) is from the far side of the divider, not the middle
    line.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*左上角*：支持向量机（SVM）的决策边界。与神经网络模型一样，判别性学习器试图找到一种最优地划分数据的方法。与神经网络分类器不同，SVM还试图最大化边界的宽度。这就是SVM决定多个可能的中心线中哪一个是最佳划分的方法：它具有最宽的边界。请注意，从分隔器（SVM的超平面）的距离是从分隔器的远端，而不是中间线。'
- en: '*Top right*: A potential Bayesian model. This model is a generative supervised
    learning model, trying to model the distribution of each label rather than model
    the boundary between them. The confidence on a per-label basis can be read directly
    as a probability of being that label.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*右上角*：一个潜在的贝叶斯模型。这个模型是一个生成式监督学习模型，试图模拟每个标签的分布，而不是模拟它们之间的边界。基于每个标签的置信度可以直接读取为该标签的概率。'
- en: '*Bottom left*: The division that a decision tree might provide, dividing and
    recursively subdividing the data one feature at time. The confidence is defined
    by the percentage of a label in the final bucket (leaf), The bottom-left leaf,
    for example, has one Label A and three Label Bs, so a prediction in that leaf
    would be 25% confidence in Label A and 75% confidence in Label B. Decision trees
    are sensitive to how far you let them divide—they could keep dividing to leaves
    of one item—so probabilities tend not to be reliable.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bottom left*: 决策树可能提供的划分，一次只按一个特征划分并递归细分数据。置信度定义为最终桶（叶子节点）中标签的百分比，例如，左下角的叶子节点有一个标签A和三个标签B，因此在那个叶子节点中的预测会有25%的置信度在标签A上，75%的置信度在标签B上。决策树对它们可以划分多深很敏感——它们可以继续划分到只有一个项目的叶子节点——因此概率通常不可靠。'
- en: '*Bottom right*: An ensemble of decision trees, of which the most well-known
    variant is a random forest. Multiple decision trees are trained. The different
    trees are usually achieved by training on different subsets of the data and/or
    features. The confidence in a label can be the percentage of times an item was
    predicted across all models or the average confidence across all predictions.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bottom right*: 决策树的集成，其中最著名的变体是随机森林。训练了多个决策树。不同的树通常是通过在不同的数据子集和/或特征上训练来实现的。标签的置信度可以是所有模型预测中一个项目被预测的百分比，或者所有预测的平均置信度。'
- en: 3.3.1 Uncertainty sampling with logistic regression and MaxEnt models
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 使用逻辑回归和MaxEnt模型的不确定性采样
- en: 'For interpreting model confidence, you can treat logistic regression and MaxEnt
    (maximum entropy) models the same as neural models. There is little difference
    (sometimes none) between a logistic regression model, a MaxEnt model, and a single-layer
    neural model. Therefore, you can apply uncertainty sampling in the same way that
    you do for neural models: you might get softmax outputs, or you might get scores
    to which you can apply softmax. The same caveats apply: it is not the job of a
    logistic regression or MaxEnt model to calculate the confidence of a model accurately,
    as the model is trying to distinguish optimally between the labels, so you may
    want to experiment with different bases/temperatures for softmax if that is how
    you are generating your probability distribution.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解释模型置信度，你可以将逻辑回归和最大熵（MaxEnt）模型视为与神经网络模型相同。逻辑回归模型、MaxEnt模型和单层神经网络模型之间几乎没有区别（有时没有区别）。因此，你可以像对神经网络模型那样应用不确定性采样：你可能得到softmax输出，或者得到可以应用softmax的分数。同样需要注意：逻辑回归或MaxEnt模型的任务不是准确计算模型的置信度，因为模型试图在标签之间最优地区分，所以你可能需要尝试不同的基数/温度来生成你的概率分布。
- en: 3.3.2 Uncertainty sampling with SVMs
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 使用SVMs的不确定性采样
- en: Support vector machines (SVMs) represent another type of discriminative learning.
    Like neural models, they are attempting to find a way to divide the data optimally.
    Unlike neural classifiers, SVMs are also trying to maximize the width of the boundary
    and decide which of the multiple possible divisions is the right one. The optimal
    boundary is defined as the widest one—more specifically, the one that optimally
    models the greatest distance between a label and the far side of the dividing
    boundary. You can see an example of SVMs in figure 3.5\. The support vectors themselves
    are the data points that define the boundaries.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）代表另一种类型的判别学习。与神经网络模型一样，它们试图找到一种最优划分数据的方法。与神经网络分类器不同，SVMs还试图最大化边界的宽度并决定多个可能的划分中哪一个是正确的。最优边界定义为最宽的边界——更具体地说，是最佳地模拟标签与划分边界另一侧的最大距离的边界。你可以在图3.5中看到SVMs的例子。支持向量本身是定义边界的数据点。
- en: '![](../Images/CH03_F05_Munro.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F05_Munro.png)'
- en: 'Figure 3.5 SVM projecting our example 2D dataset (top) into 3D (bottom) so
    that a linear plane can separate the two sets of labels: Label A is above the
    plane, and Label B is below the plane. The sampled items are the least distance
    from the plane. If you want to learn from some of the important early active learning
    literature, you need to understand how SVMs work at this high level.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 SVM将我们的示例2D数据集（顶部）投影到3D（底部），以便线性平面可以分离两组标签：标签A位于平面之上，标签B位于平面之下。采样项是最接近平面的。如果你想学习一些重要的早期主动学习文献，你需要了解SVMs在这个高级别是如何工作的。
- en: SVMs also differ in how they model more complicated distributions. Neural networks
    use hidden layers to discover boundaries between labels that are more complicated
    than simple linear divisions. Two hidden layers are enough to define any function.
    SVMs more or less do the same thing, but with predefined functions that map the
    data into higher dimensions. In figure 3.5, our 2D example data is projected into
    a third dimension that raises items on one side of that function and lowers them
    on the other. With the projection into a higher dimension, the data is linearly
    separable, and a plane divides the two labels.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs also differ in how they model more complicated distributions. Neural networks
    use hidden layers to discover boundaries between labels that are more complicated
    than simple linear divisions. Two hidden layers are enough to define any function.
    SVMs more or less do the same thing, but with predefined functions that map the
    data into higher dimensions. In figure 3.5, our 2D example data is projected into
    a third dimension that raises items on one side of that function and lowers them
    on the other. With the projection into a higher dimension, the data is linearly
    separable, and a plane divides the two labels.
- en: It is many orders of magnitude more efficient to train a model when you’ve predefined
    the type of function (as in SVMs) rather than let your model find the function
    itself among all possible alternatives (as in neural models). The chance of predefining
    the correct function type is low, however, and the cost of hardware is coming
    down while speed is going up, so SVMs are rarely used today, compared with their
    earlier popularity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: It is many orders of magnitude more efficient to train a model when you’ve predefined
    the type of function (as in SVMs) rather than let your model find the function
    itself among all possible alternatives (as in neural models). The chance of predefining
    the correct function type is low, however, and the cost of hardware is coming
    down while speed is going up, so SVMs are rarely used today, compared with their
    earlier popularity.
- en: 3.3.3 Uncertainty sampling with Bayesian models
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 使用贝叶斯模型的不确定性采样
- en: '*Bayesian model* are generative supervised learning models, which means that
    they are trying to model the distribution of each label and the underlying samples
    rather than model the boundary between the labels. The advantage of Bayesian models
    is that you can read the probabilities straight off the model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*贝叶斯模型*是生成式监督学习模型，这意味着它们试图模拟每个标签及其潜在样本的分布，而不是模拟标签之间的边界。贝叶斯模型的优势在于你可以直接从模型中读取概率：'
- en: '![](../Images/CH03_F05_Munro_E01.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F05_Munro_E01.png)'
- en: You don’t need a separate step or specific activation function to convert arbitrary
    scores to a probability distribution; the model is explicitly calculating the
    probability of an item’s having a label. Therefore, confidence on a per-label
    basis can be read directly as a probability of that label.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要单独的步骤或特定的激活函数来将任意分数转换为概率分布；模型明确地计算一个项目具有标签的概率。因此，基于每个标签的置信度可以直接读取为该标签的概率。
- en: Because they are not trying to model the differences between labels, Bayesian
    models tend not to be able to capture more complicated decision boundaries without
    a lot more fine-tuning. The Naive Bayes algorithm gets the *Naive* part of its
    name from not being able to model linear relationships between features, let alone
    more complicated ones, although it can be retrained almost instantly with new
    training data, which is appealing for human-in-the-loop systems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们不试图模拟标签之间的差异，所以贝叶斯模型通常无法在不进行大量微调的情况下捕获更复杂的决策边界。朴素贝叶斯算法因其无法模拟特征之间的线性关系（更不用说更复杂的线性关系）而获得了“朴素”这个名字，尽管它可以几乎瞬间用新的训练数据进行重新训练，这对于需要人工干预的系统来说很有吸引力。
- en: Bayesian models also have to make assumptions about data distributions, such
    as real values falling within a normal distribution, which may not necessarily
    hold up in your actual data. These assumptions can skew the probabilities away
    from the true values if you are not careful. They will still tend to be better
    than probabilities from discriminative models, but you can trust them blindly
    without understanding their assumptions about the data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯模型也必须对数据分布做出假设，例如真实值落在正态分布内，这在你实际数据中可能并不一定成立。如果不小心，这些假设可能会使概率偏离真实值。尽管如此，它们通常会比判别模型的概率更可靠，但你不能盲目地信任它们，而不了解它们对数据的假设。
- en: 'Therefore, although Bayesian models aren’t always as likely to get the same
    accuracy as discriminative models, they typically produce more reliable confidence
    scores, so they can be used directly in active learning. If you trust your confidence
    score, for example, you can sample based on that score: sample 90% items with
    0.9 uncertainty, sample 10% of items with 0.1 uncertainty, and so on. Beyond simple
    labeling tasks, however, when people talk about Bayesian methods for active learning,
    they typically mean predictions over ensembles of discriminative models, which
    is covered in section 3.4 later in this chapter.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管贝叶斯模型并不总是像判别性模型那样有可能获得相同的准确度，但它们通常会产生更可靠的置信度分数，因此可以直接用于主动学习。例如，如果你信任你的置信度分数，你可以根据该分数进行采样：采样90%具有0.9不确定性的项目，采样10%具有0.1不确定性的项目，依此类推。然而，在简单的标记任务之外，当人们谈论主动学习的贝叶斯方法时，他们通常指的是在判别性模型集成上的预测，这将在本章后面的第3.4节中介绍。
- en: 3.3.4 Uncertainty sampling with decision trees and random forests
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 使用决策树和随机森林的不确定性采样
- en: '*Decision* *trees* are discriminative learners that divide the data one feature
    at a time, recursively subdividing the data into buckets until the final bucket—the
    leaves—has only one set of labels. The trees are often stopped early (*pruned*)
    so that the leaves ultimately have some diversity of labels and the models don’t
    overfit the data. Figure 3.4, earlier in this chapter, shows an example.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树* 是一种判别性学习器，它一次只根据一个特征划分数据，递归地将数据划分为桶，直到最终的桶——叶节点——只包含一组标签。树通常在早期停止（*剪枝*），这样叶节点最终会有一些标签的多样性，模型不会过度拟合数据。本章前面的图3.4展示了这样一个例子。'
- en: The confidence is defined by the percentage of a label in the leaf for that
    prediction. The bottom-left leaf in figure 3.4, for example, has one Label A and
    three Label Bs, so a prediction in that leaf would be 25% confidence in Label
    A and 75% confidence in Label B.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度由该预测的叶节点中标签的百分比定义。例如，图3.4中左下角的叶节点有一个标签A和三个标签B，因此在该叶节点中的预测会有25%的置信度在标签A上，以及75%的置信度在标签B上。
- en: Decision trees are sensitive to how far you let them divide; they could keep
    dividing to leaves of one item. By contrast, if they are not deep enough, each
    prediction will contain a lot of noise, and the bucket will be large, with relatively
    distant training items in the same bucket erroneously contributing the confidence.
    So probabilities tend not to be reliable.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对它们可以划分多深很敏感；它们可能会继续划分到只有一个项目的叶节点。相比之下，如果它们不够深，每个预测将包含很多噪声，桶会很大，同一桶中的相对较远的训练项目错误地贡献了置信度。因此，概率通常不可靠。
- en: The confidence of single decision trees are rarely trusted for this reason,
    and they are not recommended for uncertainty sampling. They can be useful for
    other active learning strategies, as we will cover later, but for any active learning
    involving decision trees, I recommended that you use multiple trees and combine
    the results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，单棵决策树的置信度很少被信任，并且不建议用于不确定性采样。它们在其他主动学习策略中可能很有用，我们将在后面介绍，但对于任何涉及决策树的主动学习，我建议你使用多棵树并合并结果。
- en: '*Random forests* are the best-known ensemble of decision trees. In machine
    learning, an *ensemble* means a collection of machine learning models that are
    combined to make a prediction, which we cover more in section 3.4.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林* 是最著名的决策树集成。在机器学习中，*集成* 指的是一组机器学习模型，这些模型被组合起来进行预测，我们将在第3.4节中更详细地介绍。'
- en: For a random forest, multiple different decision trees are trained, with the
    goal of getting slightly different predictions from each one. The different trees
    are usually achieved by training on different subsets of the data and/or features.
    The confidence in a label can be the percentage of times an item was predicted
    across all models or the average confidence across all predictions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林，训练了多个不同的决策树，目的是从每个决策树中得到略微不同的预测。不同的树通常是通过在不同的数据子集和/或特征上训练来实现的。标签的置信度可以是所有模型预测中一个项目被预测的百分比，或者是所有预测的平均置信度。
- en: As figure 3.4 shows with the combination of four decision trees in the bottom-right
    diagram, the decision boundary between the two labels is starting to become more
    gradual as you average across multiple predictions. Therefore, random forests
    make a good, useful approximation confidence along the boundary between two labels.
    Decision trees are fast to train, so there is little reason not to train many
    trees in a random forest if they are your algorithm of choice for active learning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.4所示，在右下角的图中结合了四个决策树，随着在多个预测中进行平均，两个标签之间的决策边界开始变得更加平缓。因此，随机森林在两个标签之间的边界上提供了一个很好的、有用的置信度近似。决策树训练速度快，所以如果您选择它们作为主动学习的算法，在随机森林中训练许多树几乎没有理由。
- en: 3.4 Measuring uncertainty across multiple predictions
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 在多个预测中测量不确定性
- en: Sometimes, you have multiple models built from your data. You may already be
    experimenting with different types of models or hyperparameters and want to combine
    the predictions into a single uncertainty score. If not, you may want to experiment
    with a few different models on your data to look at the variance. Even if you
    are not using multiple models for your data, looking at the variation in predictions
    from different models will give you an intuition for how stable your model is
    today.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能已经从您的数据中构建了多个模型。您可能已经在尝试不同的模型类型或超参数，并希望将预测组合成一个单一的不确定性得分。如果不是这样，您可能想对您的数据尝试几种不同的模型来观察方差。即使您没有使用多个模型来处理您的数据，观察不同模型预测的变异也能让您对当前模型的不稳定性有一个直观的认识。
- en: 3.4.1 Uncertainty sampling with ensemble models
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 使用集成模型进行不确定性采样
- en: Similar to how a random forest is an ensemble of one type of supervised learning
    algorithm, you can use multiple types of algorithms to determine uncertainty and
    aggregate across them. Figure 3.6 shows an example. Different classifiers have
    confidence scores that are unlikely to be directly compatible because of the different
    types of statistics used.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林是一个监督学习算法的集成类似，您可以使用多种类型的算法来确定不确定性，并在它们之间进行汇总。图3.6显示了一个示例。不同的分类器由于使用了不同类型的统计方法，其置信度得分可能不太可能直接兼容。
- en: The simplest way to combine multiple classifiers is to rank-order the items
    by their uncertainty score for each classifier, give each item a new score based
    on its rank order, and then combine those rank scores into one master rank of
    uncertainty.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结合多个分类器最简单的方法是按每个分类器的不确定性得分对项目进行排序，根据其排名顺序给每个项目分配一个新得分，然后将这些排名得分组合成一个主不确定性排名。
- en: '![](../Images/CH03_F06_Munro.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4](../Images/CH03_F06_Munro.png)'
- en: 'Figure 3.6 An ensemble model that combines predictions from different types
    of machine learning algorithms: neural models, SVMs, Bayesian models, and decision
    trees (decision forest). The predictions can be combined in various ways (max,
    average, and so on) to find the joint uncertainty of each unlabeled item.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：结合不同类型机器学习算法预测的集成模型：神经网络模型、SVMs、贝叶斯模型和决策树（决策森林）。预测可以通过各种方式（最大值、平均值等）组合，以找到每个未标记项目的联合不确定性。
- en: 'You can calculate uncertainty by how often different models agree on the label
    of an item. The items with the most disagreement are the ones to sample. You can
    also take the probability distributions of the predictions into account. You can
    combine the predictions from different models in multiple ways:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过不同模型对项目标签的一致性频率来计算不确定性。分歧最大的项目是需要采样的对象。您还可以考虑预测的概率分布。您可以通过多种方式组合不同模型的预测：
- en: Lowest maximum confidence across all models
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型中最低的最大置信度
- en: Difference between minimum and maximum confidence across models
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型间最小和最大置信度之间的差异
- en: Ratio between minimum and maximum confidence across models
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型间最小和最大置信度之间的比率
- en: Entropy across all confidences in all models
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型中所有置信度的熵
- en: Average confidence across all models
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型中的平均置信度
- en: You probably noticed that the first four methods are the same algorithms we
    used for uncertainty sampling within a single prediction, but in this case across
    multiple predictions. So you should already be able to implement these methods.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，前四种方法与我们在单个预测中进行不确定性采样时使用的相同算法，但在这个情况下是跨多个预测。因此，您应该已经能够实现这些方法。
- en: 3.4.2 Query by Committee and dropouts
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 委员会查询和dropout
- en: 'Within active learning, the ensemble-based approach is sometimes known as *Query
    by Committee*, especially when only one type of machine learning algorithm is
    used for the ensemble. You could try the ensemble approach with neural models:
    train a model multiple times and look at the agreement on the unlabeled data across
    the predictions from each neural model. If you’re already retraining your model
    multiple times to tune hyperparameters, you might as well take advantage of the
    different predictions to help with active learning.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习过程中，基于集成的方法有时被称为*委员会查询*，尤其是在仅使用一种机器学习算法进行集成时。你可以尝试使用基于神经模型的集成方法：多次训练一个模型，并查看每个神经模型预测中未标记数据的共识。如果你已经多次重新训练模型以调整超参数，那么利用不同的预测来帮助主动学习也是可以的。
- en: Following the random forest method, you could try retraining your models with
    different subsets of items or features to force diversity in the types of models
    that are built. This approach will prevent one feature (or a small number of features)
    from dominating the final uncertainty score.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 按照随机森林方法，你可以尝试使用不同子集的项目或特征重新训练你的模型，以强制在构建的模型类型中实现多样性。这种方法将防止一个特征（或少数几个特征）主导最终的不确定性得分。
- en: 'One recently popular method for neural models uses dropouts. You are probably
    familiar with using dropouts when training a model: you remove/ignore some random
    percentage of neurons/connections while training the model to avoid overfitting
    your model to any specific neuron.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最近流行的用于神经模型的方法是使用dropout。你可能熟悉在训练模型时使用dropout：在训练模型时，你移除/忽略一些随机百分比的神经元/连接，以避免将模型过度拟合到任何特定的神经元。
- en: 'You can apply the dropout strategy to predictions: get a prediction for an
    item multiple times, dropping out a different random selection of neurons/connections
    each time. This approach results in multiple confidences for an item, and you
    can use these confidences with the ensemble evaluation methods to sample the right
    items, as shown in figure 3.7.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将dropout策略应用于预测：多次对项目进行预测，每次丢弃不同的随机选择的神经元/连接。这种方法会导致项目有多个置信度，你可以使用这些置信度与集成评估方法一起采样正确的项目，如图3.7所示。
- en: '![](../Images/CH03_F07_Munro.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F07_Munro.png)'
- en: 'Figure 3.7 Applying dropout to a model to get multiple predictions for a single
    item. In each prediction, a random set of neurons is dropped (ignored), resulting
    in different confidences and (possibly) different predicted labels. Then the uncertainty
    can be calculated as the variation across all predictions: the higher the disagreement,
    the more uncertainty. This approach to getting multiple predictions from a single
    model is known as *Monte Carlo* dropouts.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 将dropout应用于模型以获得单个项目的多个预测。在每个预测中，随机丢弃（忽略）一组神经元，导致不同的置信度和（可能）不同的预测标签。然后，可以通过计算所有预测之间的变化来计算不确定性：不一致性越高，不确定性越大。从单个模型中获得多个预测的方法被称为*蒙特卡洛*dropout。
- en: You will see more examples throughout the book of using the neural architecture
    itself to help with active learning. Chapter 4, which covers diversity sampling,
    begins with a similar example that uses model activation to detect outliers, and
    many of the advanced techniques later in the book do the same thing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本书的其余部分看到更多使用神经网络架构本身来帮助主动学习的例子。第4章，涵盖了多样性采样，以一个类似的例子开始，该例子使用模型激活来检测异常值，本书后面的许多高级技术也做同样的事情。
- en: It is an exciting time to be working in human-in-the-loop machine learning.
    You get to work with the newest architectures for machine learning algorithms
    *and* think about how they relate to human–computer interaction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在人机交互的机器学习领域工作是一个令人兴奋的时代。你可以与机器学习算法的最新架构一起工作，并思考它们与人类-计算机交互的关系。
- en: 3.4.3 The difference between aleatoric and epistemic uncertainty
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 随机性和认知不确定性的区别
- en: The terms *aleatoric uncertainty* and *epistemic uncertainty* from the philosophy
    literature, are popular among machine learning scientists who have never read
    the philosophy literature. In the machine learning literature, the terms typically
    refer to the methods used. *Epistemic uncertainty* is uncertainty within a single
    model’s predictions, and *aleatoric uncertainty* is uncertainty across multiple
    predictions (especially Monte Carlo dropouts in the recent literature). *Aleatoric*
    historically meant inherent randomness, and *epistemic* meant lack of knowledge,
    but these definitions are meaningful only in machine learning contexts in which
    no new data can be annotated, which is rare outside academic research.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 来自哲学文献的术语*随机不确定性*和*认知不确定性*在从未阅读过哲学文献的机器学习科学家中很受欢迎。在机器学习文献中，这些术语通常指的是使用的方法。"认知不确定性"是指单个模型预测中的不确定性，而"随机不确定性"是指多个预测中的不确定性（特别是在最近的文献中，尤其是蒙特卡洛dropout）。"随机"在历史上意味着固有的随机性，而"认知"意味着缺乏知识，但这些定义只在机器学习中才有意义，在机器学习中没有新的数据可以标注，这在学术研究之外是罕见的。
- en: Therefore, when reading machine learning literature, assume that the researchers
    are talking only about the methods used to calculate uncertainty, not the deeper
    philosophical meanings. Figure 3.8 illustrates the differences.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在阅读机器学习文献时，假设研究人员只讨论用于计算不确定性的方法，而不是更深层次的哲学意义。图3.8展示了这些差异。
- en: '![](../Images/CH03_F08_Munro.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8](../Images/CH03_F08_Munro.png)'
- en: Figure 3.8 Differences between aleatoric and epistemic uncertainty, according
    to the definitions that are most widely used in machine learning literature. The
    first highlighted item is near the decision boundary of all five predictions,
    so it has high epistemic uncertainty, but the decision boundaries are clustered
    together, so it has low aleatoric uncertainty. The second highlighted item has
    low epistemic uncertainty because it is not near most decision boundaries, but
    its distance from the decision boundaries has a large amount of variation, so
    it has high aleatoric uncertainty. The final item is near the average decision
    boundary and has great variance in the distance between all boundaries, so it
    has high uncertainty for both types.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8展示了根据在机器学习文献中最广泛使用的定义，随机不确定性和认知不确定性之间的差异。第一个突出显示的项目靠近所有五个预测的决策边界，因此具有高认知不确定性，但决策边界聚集在一起，因此具有低随机不确定性。第二个突出显示的项目具有低认知不确定性，因为它不靠近大多数决策边界，但它的决策边界距离有很大的变化，因此具有高随机不确定性。最后一个项目靠近平均决策边界，所有边界之间的距离变化很大，因此两种类型都具有高不确定性。
- en: Figure 3.8 shows how multiple predictions allows you to predict uncertainty
    in terms of variance from multiple decision boundaries in addition to distance
    from a single decision boundary. For a neural model, the variation in distance
    from the decision boundary can be calculated as the variation in the labels predicted,
    the variation in any of the uncertainty sampling metrics covered in section 3.2,
    or variation across the entire probability distribution for each prediction.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8展示了如何通过多个预测来预测不确定性，除了从单个决策边界距离外，还可以从多个决策边界的方差来衡量。对于一个神经网络模型，从决策边界距离的变化可以计算为预测标签的变化，任何在3.2节中提到的不确定性采样指标的变化，或者每个预测整个概率分布的变化。
- en: See section 3.8 for further reading on starting places, as this area of research
    is an active one. The literature on aleatoric uncertainty tends to focus on the
    optimal types of ensembles or dropouts, and the literature on epistemic uncertainty
    tends to focus on getting more accurate probability distributions from within
    a single model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有关起始点的进一步阅读，请参阅3.8节，因为这个研究领域非常活跃。关于随机不确定性的文献往往关注最优类型的集成或dropout，而关于认知不确定性的文献往往关注从单个模型中获得更准确的概率分布。
- en: 3.4.4 Multilabeled and continuous value classification
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 多标签和连续值分类
- en: If your task is multilabeled, allowing multiple correct labels for each item,
    you can calculate uncertainty by using the same aggregation methods as for ensembles.
    You can treat each label as though it is a binary classifier. Then you can decide
    whether you want to average the uncertainty, take the maximum uncertainty, or
    use one of the other aggregation techniques covered earlier in this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的任务是多标签，允许每个项目有多个正确标签，你可以使用与集成相同的聚合方法来计算不确定性。你可以将每个标签视为一个二元分类器。然后你可以决定是否要平均不确定性，取最大不确定性，或使用本章前面介绍的其他聚合技术之一。
- en: When you treat each label as a binary classifier, there is no difference among
    the types of uncertainty sampling algorithms (least confidence, margin of confidence,
    and so on), but you might try the ensemble methods in this section *in addition*
    to aggregating across the different labels. You can train multiple models on your
    data and then aggregate the prediction for each label for each item, for example.
    This approach will give you different uncertainty values for each label for an
    item, and you can experiment with the right methods to aggregate the uncertainty
    per label in addition to across labels for each item.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将每个标签视为一个二元分类器时，不确定性采样算法的类型（最小置信度、置信度范围等）之间没有区别，但你可能尝试在本节中*除了*跨不同标签聚合之外，还可以尝试集成方法。例如，你可以在你的数据上训练多个模型，然后聚合每个项目的每个标签的预测。这种方法将为每个项目的每个标签提供不同的不确定性值，你可以尝试不同的方法来聚合每个标签的不确定性，包括跨标签的不确定性。
- en: For continuous values, such as a regression model that predicts a real value
    instead of a label, your model might not give you confidence scores in the prediction.
    You can apply ensemble methods and look at the variation to calculate the uncertainty
    in these cases. In fact, Monte Carlo dropouts were first used to estimate uncertainty
    in regression models in which no new data needed to be labeled. In that controlled
    environment, you could argue that *epistemic uncertainty* is the right term.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续值，例如预测真实值的回归模型而不是标签，你的模型可能不会在预测中给出置信度分数。在这些情况下，你可以应用集成方法并查看变化来计算不确定性。实际上，蒙特卡洛dropout最初用于估计不需要标注新数据的回归模型中的不确定性。在那个受控环境中，你可以认为*认知不确定性*是正确的术语。
- en: Chapter 6 covers the application of active learning to many use cases, and the
    section on object detection goes into more detail about uncertainty in regression.
    Chapter 10 has a section devoted to evaluating human accuracy for continuous tasks
    that may also be relevant to your task. I recommend that you read those two chapters
    for more details about working with models that predict continuous values.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章涵盖了主动学习在许多用例中的应用，关于目标检测的部分更详细地讨论了回归中的不确定性。第10章有一个专门的部分用于评估连续任务的标注准确性，这可能与你的任务也相关。我建议你阅读这两章，以了解更多关于与预测连续值的模型一起工作的细节。
- en: 3.5 Selecting the right number of items for human review
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 选择合适数量的人为审查项目
- en: Uncertainty sampling is an iterative process. You select some number of items
    for human review, retrain your model, and then repeat the process. Recall from
    chapter 1 the potential downside of sampling for uncertainty without also sampling
    for diversity, as shown in figure 3.9.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样是一个迭代过程。你选择一定数量的项目供人工审查，重新训练你的模型，然后重复这个过程。回想一下第1章中，如果不进行多样性采样，仅对不确定性进行采样的潜在缺点，如图3.9所示。
- en: '![](../Images/CH03_F09_Munro.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F09_Munro.png)'
- en: Figure 3.9 A selection of uncertain items that are all from the same region
    of the feature space, and therefore lack diversity
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 从特征空间相同区域选择的不确定性项目，因此缺乏多样性
- en: The most uncertain items here are all near one another. In a real example, thousands
    of examples might be clustered together, and it would not be necessary to sample
    them all. No matter where the item is sampled from, you can’t be entirely sure
    what the influence on the model will be until a human has provided a label and
    the model is retrained.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最不确定的项目都彼此靠近。在真实示例中，成千上万的示例可能会聚集在一起，并且不需要对它们全部进行采样。无论项目是从哪里采样的，你都无法完全确定直到人工提供了标签并且模型重新训练后，对模型的影响是什么。
- en: 'Retraining a model can take a long time, however, and it can be a waste of
    time to ask the human annotators to wait during that period. Two competing forces
    are at work:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练模型可能需要很长时间，然而，在此期间要求人工标注员等待可能会浪费他们的时间。有两个相互竞争的力量在起作用：
- en: Minimizing the sample size will ensure that the most benefit is gained from
    each data point at each iteration.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化样本大小将确保在每次迭代中从每个数据点中获得最大利益。
- en: Maximizing the sample size will ensure that more items get labeled sooner and
    the model needs to be retrained less often.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化样本大小将确保更多项目能够更快地被标注，并且模型需要重新训练的频率更低。
- en: As you saw in chapter 2, there was low diversity in the early iterations of
    your model, but this situation self-corrected in later iterations as the model
    was retrained. The decision ultimately comes down to a business process. In recent
    work in translation, we wanted our models to adapt in a few seconds so that they
    seemed to be responsive in real time to our translators while they worked. I’ve
    also seen companies being happy with about one iteration per year to adapt to
    new data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第2章中看到的，你的模型在早期迭代中多样性较低，但随着模型的重新训练，这种情况在后续迭代中自行纠正。最终的决定取决于业务流程。在最近的翻译工作中，我们希望我们的模型能在几秒钟内适应，以便在翻译者工作时看起来能够实时响应。我也看到过一些公司对每年大约一次的迭代来适应新数据感到满意。
- en: 3.5.1 Budget-constrained uncertainty sampling
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 预算约束的不确定性采样
- en: If you have a fixed budget for labels, you should try to get as many iterations
    as possible. The number of possible iterations will depend on whether you are
    compensating annotators per label (as with many crowdsourced worker models) or
    per hour (as with many expert human models).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个固定的标注预算，你应该尽量进行尽可能多的迭代。可能的迭代次数将取决于你是否按标签（如许多众包工作者模型）或按小时（如许多专家人类模型）支付标注者的报酬。
- en: 'If your budget is per label, meaning that you are paying a fixed price per
    label no matter how long the gap is between getting those labels, it is best to
    optimize for the maximum number of iterations possible. People do tend to get
    bored waiting for their models to train. When retraining a model takes more than
    a few days, I’ve seen people max out at about 10 iterations and plan accordingly.
    There’s no particular reason to choose 10: it’s an intuitive number of iterations
    to monitor for changes in accuracy.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的预算是按标签计算的，这意味着无论获取这些标签之间的时间间隔有多长，你都要支付固定的价格，那么最佳做法是优化以实现尽可能多的迭代次数。人们确实倾向于在等待模型训练时感到无聊。当重新训练模型需要超过几天时，我看到人们最多进行大约10次迭代并相应地计划。选择10并没有特别的原因：这是一个直观的迭代次数，用于监控准确性的变化。
- en: If your budget is per-hour, meaning that you have a set number of people labeling
    a set number of hours per day, it is best to optimize for always having data available
    to label. Have annotators gradually work through the rank order of unlabeled items
    by uncertainty, and retrain a model at regular intervals, subbing out an old uncertainty
    ranking for a new one whenever a new model is ready. If you are using uncertainty
    sampling and want to avoid oversampling from only one part of the problem space,
    you should replace models regularly. Realistically, if people are working full
    time to label data for you, you owe them the respect of implementing multiple
    active learning sampling strategies from this book and sampling from all those
    strategies, so that those people feel that they are contributing the greatest
    value possible. You are also less likely to introduce bias that could result from
    implementing only one of the algorithms, so both humans and machines score a win.
    We will return to strategies for different kinds of annotation workforces in chapter
    7.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的预算是按小时计算的，这意味着你每天有固定的人数来标注固定的小时数，那么最佳做法是始终确保有可用的数据用于标注。让标注者逐步通过不确定性的排名顺序处理未标注的项目，并在固定间隔内重新训练模型，每当新模型准备好时，就用新的不确定性排名替换旧的排名。如果你使用不确定性采样并且想要避免只从问题空间的一个部分过度采样，你应该定期更换模型。现实中，如果有人全职为你标注数据，你应该给予他们应有的尊重，实施本书中提到的多种主动学习采样策略，并从所有这些策略中进行采样，这样那些人就会觉得自己贡献了最大的价值。你也更不可能引入仅实施一个算法可能导致的偏差，因此人和机器都能得分。我们将在第7章中回到不同类型的标注工作队伍的策略。
- en: 3.5.2 Time-constrained uncertainty sampling
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 时间约束的不确定性采样
- en: If you are time-constrained and need to get an updated model out quickly, you
    should consider strategies to retrain the models as quickly as possible, as in
    chapter 2\. The quickest way is to use simple models. A model with only one or
    two layers (or, better yet, a Naive Bayes model) can be retrained incredibly quickly,
    allowing you to iterate quickly. Further, there is some evidence that uncertainty
    sampling from a simpler model can be as effective as sampling from a more complicated
    model. Remember that we’re looking for the most confusion, not the most accuracy.
    Provided that a simple model is the most confused about the same items as a more
    complicated model, both models will sample the same items.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你时间紧迫，需要快速推出更新后的模型，你应该考虑尽可能快速重新训练模型的策略，正如第2章所述。最快的方法是使用简单的模型。只有一个或两个层（或者更好的是，使用朴素贝叶斯模型）的模型可以非常快速地重新训练，让你能够快速迭代。此外，有一些证据表明，从更简单的模型中进行的不确定性采样可以与从更复杂的模型中进行采样一样有效。记住，我们寻找的是最大的困惑，而不是最大的准确性。只要一个简单模型对相同项目的困惑程度与一个更复杂的模型相同，这两个模型就会采样相同的项。
- en: A more advanced way is to retrain only the final layer(s) of a much larger model.
    You can retrain your model quickly by retraining only the last layer with new
    data, compared with retraining the whole model. This process can take a matter
    of seconds instead of weeks. The retrained model will not necessarily be as accurate,
    but it may be close. As with choosing a simpler model, this small loss in accuracy
    may not matter if the goal is to look for more uncertainty. The faster iteration
    may even result in a more accurate model than if you’d waited a long time to retrain
    the entire model with fewer iterations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更高级的方法是只重新训练一个更大模型的最後一层（或几层）。你可以通过只使用新数据重新训练最后一层来快速重新训练你的模型，而不是重新训练整个模型。这个过程可能只需要几秒钟，而不是几周。重新训练的模型不一定那么准确，但它可能很接近。与选择一个更简单的模型一样，如果目标是寻找更多的不确定性，这种准确性的微小损失可能并不重要。更快的迭代甚至可能产生比等待很长时间用更少的迭代重新训练整个模型更准确的模型。
- en: 'One advanced method is to have the best of both worlds: use methods to discover
    which parameters are the most important to retrain across your entire model, and
    retrain only them. This approach can give you the same accuracy as retraining
    the entire model, but in a fraction of the time.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更高级的方法是取两者之长：使用方法来发现哪些参数是重新训练整个模型时最重要的，然后只重新训练它们。这种方法可以提供与重新训练整个模型相同的准确性，但所需时间却少得多。
- en: 'Another advanced method that can be easier to implement is to have two models:
    an incremental model that is updated immediately with every new training item
    and a second model that is retrained from scratch at regular intervals. One of
    the example implementations in chapter 12 uses this architecture.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以更容易实施的高级方法是拥有两个模型：一个增量模型，它会立即更新每个新的训练项，另一个模型则定期从头开始重新训练。第12章中的一个示例实现就使用了这种架构。
- en: 3.5.3 When do I stop if I’m not time- or budget-constrained?
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 如果我没有时间或预算限制，何时停止？
- en: Lucky you! You should stop when your model stops getting more accurate. If you
    have tried many strategies for uncertainty sampling and are not getting any more
    gains after a certain accuracy is reached, this condition is a good signal to
    stop and think about other active learning and/or algorithmic strategies if your
    desired accuracy goal hasn’t been met.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 真幸运！你应该在你模型不再变得更准确的时候停止。如果你已经尝试了许多不确定性采样的策略，并且在达到一定准确性后没有获得任何更多收益，那么这个条件是一个很好的信号，表明你应该停止并思考其他主动学习和/或算法策略，如果你的期望准确性目标尚未实现。
- en: You will ultimately see diminishing returns as you label more data; no matter
    what strategy you use, the learning rate will decrease as you add more data. Even
    if the rate hasn’t plateaued, you should be able to run a cost-benefit analysis
    of the accuracy you are getting per label versus the cost of those labels.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你标注的数据越来越多，你最终会发现收益递减；无论你使用什么策略，随着你添加更多数据，学习率都会下降。即使这个比率还没有达到平台期，你也应该能够对每条标签的准确性以及这些标签的成本进行成本效益分析。
- en: 3.6 Evaluating the success of active learning
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 评估主动学习的成功
- en: 'Always evaluate uncertainty sampling on a randomly selected, held-out test
    set. If the test data is selected randomly from your training data after each
    iteration, you won’t know what your actual accuracy is. In fact, your accuracy
    is likely to appear to be lower than it is. By choosing items that are hard to
    classify, you are probably oversampling inherently ambiguous items. If you are
    testing more on inherently ambiguous items, you are more likely to see errors.
    (We covered this topic in chapter 2, but it is worth repeating here.) Therefore,
    don’t fall into the trap of forgetting to sample randomly in addition to using
    uncertainty sampling: you won’t know whether your model is improving!'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 总是评估不确定性采样在随机选择的、保留的测试集上。如果在每次迭代后从你的训练数据中随机选择测试数据，你就不知道你的实际准确率是多少。事实上，你的准确率可能看起来比实际要低。通过选择难以分类的项目，你很可能会过度采样固有的模糊项。如果你在固有的模糊项上测试得更多，你更有可能看到错误。（我们在第2章中讨论了这个问题，但在这里重复一遍。）因此，不要陷入忘记在不确定性采样之外随机采样的陷阱：你将不知道你的模型是否在改进！
- en: 3.6.1 Do I need new test data?
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 我需要新的测试数据吗？
- en: If you already have test data set aside, and you know that the unlabeled data
    is from more or less the same distribution as your training data, you do not need
    additional test data. You can keep testing on the same data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经保留了测试数据集，并且你知道未标记的数据与你的训练数据大致相同，你不需要额外的测试数据。你可以继续在相同的数据上进行测试。
- en: If you know that the test data has a different distribution from your original
    training data, or if you are unsure, you should collect additional labels through
    random selection of unlabeled items and add them to your test set or create a
    second, separate test set.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道测试数据与你的原始训练数据分布不同，或者如果你不确定，你应该通过随机选择未标记的项目来收集额外的标签，并将它们添加到你的测试集中或创建第二个独立的测试集。
- en: Tip Create your new test set before your first iteration of uncertainty sampling.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在第一次迭代的不确定性采样之前创建你的新测试集。
- en: As soon as you have removed some unlabeled items from the pool via uncertainty
    sampling, that pool is no longer a random selection. That pool is now biased toward
    *confidently* predicted items, so a random selection from this pool is likely
    to return erroneously high accuracy if it is used as a test set.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你通过不确定性采样从池中移除了一些未标记的项目，这个池就不再是随机选择了。现在，这个池偏向于*自信地*预测的项目，因此从这个池中随机选择很可能会在用作测试集时返回错误的高精度。
- en: Keep your test set separate throughout all iterations, and do not allow its
    items to be part of any sampling strategy. If you forget to do this until several
    iterations in, and your random sample includes items that were selected by uncertainty
    sampling, you will need to go back to the first iteration. You can’t simply remove
    those test items from the training data going forward, as they were trained on
    and contributed to selections in the interim uncertainty sampling strategies.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有迭代过程中保持你的测试集独立，并不要允许其项目成为任何采样策略的一部分。如果你在迭代了几次之后才忘记这样做，并且你的随机样本包括通过不确定性采样选择的项目，你需要回到第一次迭代。你不能简单地从训练数据中移除那些测试项目，因为它们在中间的不确定性采样策略中已经被训练并做出了贡献。
- en: 'It is also a good idea to see how well your uncertainty sampling technique
    is performing next to a baseline of random sampling. If you aren’t more accurate
    than random sampling, you should reconsider your strategy! Choose randomly selected
    items for which you know the comparison will be statistically significant: often,
    a few hundred items are sufficient. Unlike the evaluation data for your entire
    model, these items can be added to your training data in the next iteration, as
    you are comparing the sampling strategy at each step, given what is remaining
    to be labeled.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 也是一个好主意，看看你的不确定性采样技术在随机采样的基线旁边表现如何。如果你没有比随机采样更准确，你应该重新考虑你的策略！选择随机选择的项，对于这些项，你知道比较将具有统计学意义：通常，几百个项就足够了。与你的整个模型的评估数据不同，这些项可以在下一次迭代中添加到你的训练数据中，因为你正在比较每个步骤的采样策略，考虑到剩余需要标记的内容。
- en: Finally, you may want to include a random sample of items along with the ones
    chosen by uncertainty sampling. If you are not going to implement some of the
    diversity sampling methods in chapter 4, random sampling will give you the most
    basic form of diversity sampling and ensure that every data point has a chance
    of getting human review.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能希望包括一些随机选择的物品，以及由不确定性采样选择的物品。如果你不打算实现第4章中的一些多样性采样方法，随机采样将给你提供最基本的形式的多样性采样，并确保每个数据点都有机会得到人工审查。
- en: 3.6.2 Do I need new validation data?
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 我需要新的验证数据吗？
- en: You should also consider up to four validation sets at each iteration, with
    data drawn from
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该考虑在每个迭代中最多使用四个验证集，数据来自
- en: The same distribution as the test set
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与测试集相同的分布
- en: The remaining unlabeled items in each iteration
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个迭代中剩余的未标记物品
- en: The same distribution as the newly sampled items in each iteration
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个迭代中与新采样物品相同的分布
- en: The same distribution as the total training set in each iteration
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个迭代中与总训练集相同的分布
- en: If you are tuning the parameters of your model after each addition of data,
    you will use a validation set to evaluate the accuracy. If you tune a model on
    the test set, you won’t know whether your model has truly generalized or you have
    simply found a set of parameters that happen to work well with that specific evaluation
    data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是在每次添加数据后调整模型参数，你将使用验证集来评估准确性。如果你在测试集上调整模型，你将不知道你的模型是否真正泛化，或者你只是找到了一组恰好与特定评估数据配合得很好的参数。
- en: A validation set will let you tune the accuracy of the model without looking
    at the test set. Typically, you will have a validation set from the outset. As
    with your test set, you don’t need to update/replace it if you think that the
    unlabeled items come from the same distribution as your initial training data.
    Otherwise, you should update your validation data before the first iteration of
    your uncertainty sampling, as with your test data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集将让你在查看测试集的情况下调整模型的准确性。通常，你从一开始就会有一个验证集。与你的测试集一样，如果你认为未标记的物品来自与你的初始训练数据相同的分布，你不需要更新/替换它。否则，你应该在不确定性采样的第一次迭代之前更新你的验证数据，就像你的测试数据一样。
- en: You may want to use a second validation set to test how well your active learning
    strategy is doing within each iteration. After you start active learning iterations,
    the remaining unlabeled items will no longer be a random sample, so this distribution
    will not be the same as your existing test set and validation set. This dataset
    acts as a baseline for each iteration. Is uncertainty sampling still giving you
    better results than selecting from random among the remaining items? Because this
    dataset set is useful for only one iteration, it is fine to add these items to
    the training data at the end of each iteration; these labels aren’t human labels
    that get discarded.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想使用第二个验证集来测试你的主动学习策略在每个迭代中的表现如何。在你开始主动学习迭代后，剩余的未标记物品将不再是一个随机样本，因此这个分布将不同于你现有的测试集和验证集。这个数据集作为每个迭代的基线。不确定性采样是否仍然比从剩余物品中随机选择给出更好的结果？因为这个数据集只对一次迭代有用，所以在每个迭代结束时将这些物品添加到训练数据中是可以的；这些标签不是被丢弃的人工标签。
- en: If you want to evaluate the accuracy of the human-labels created in each iteration,
    you should do this on a third validation data set drawn from the same distribution
    as the newly sampled data. Your newly sampled data may be inherently easier or
    harder for humans to label, so you need to evaluate human accuracy on that same
    distribution.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想评估每一轮中创建的人为标签的准确性，你应该在从与新采样数据相同分布中抽取的第三个验证数据集上进行此操作。你的新采样数据可能对于人类来说天生更容易或更难进行标签化，因此你需要在该相同分布上评估人类准确性。
- en: Finally, you should consider a fourth validation set drawn randomly from the
    training data at each iteration. This validation data can be used to ensure that
    the model is not overfitting the training data, which a lot of machine learning
    libraries will do by default. If your validation data and training data are not
    from the same distribution, it will be hard to estimate how much you are overfitting,
    so having a separate validation set to check for overfitting is a good idea.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该考虑在每个迭代中随机从训练数据中抽取的第四个验证集。这些验证数据可以用来确保模型没有过度拟合训练数据，许多机器学习库默认会这样做。如果你的验证数据和训练数据不是来自相同的分布，将很难估计你过度拟合了多少，因此拥有一个单独的验证集来检查过度拟合是一个好主意。
- en: The downside is the human-labeling cost for up to four validation data sets.
    In industry, I see people using the wrong validation dataset more often than not,
    typically letting one validation set be used in all cases. The most common reason
    is that people want to put as many labeled items as possible into their training
    data to make that model more accurate sooner. That’s also the goal of active learning,
    of course, but without the right validation data, you won’t know what strategic
    direction to take next to get to greater accuracy.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是高达四个验证数据集的人工标注成本。在工业界，我经常看到人们使用错误的验证数据集，通常是在所有情况下都使用一个验证集。最常见的原因是人们希望尽可能多地将其标注项目放入训练数据中，以便更快地使模型更准确。当然，这也是主动学习的目标，但没有正确的验证数据，您将不知道下一步的战略方向以获得更高的准确性。
- en: 3.7 Uncertainty sampling cheat sheet
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 不确定性采样速查表
- en: Our example data in this text has only two labels. Uncertainty sampling algorithms
    will return the same samples with two labels. Figure 3.10 shows an example of
    target areas for the different algorithms when there are three labels. The figure
    shows that margin of confidence and ratio sample some items that have only pairwise
    confusion, which reflects the fact that the algorithms target only the two most
    likely labels. By contrast, entropy maximizes for confusion among all labels,
    which is why the highest concentration is between all three labels.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的示例数据只有两个标签。不确定性采样算法将返回具有两个标签的相同样本。图3.10展示了当有三个标签时，不同算法的目标区域示例。该图显示，置信度边际和比率采样了一些只有成对混淆的项目，这反映了算法只针对最有可能的两个标签。相比之下，熵最大化所有标签之间的混淆，这也是为什么最高浓度出现在所有三个标签之间。
- en: '![](../Images/CH03_F10_Munro.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F10_Munro.png)'
- en: Figure 3.10 A heat map of the four main uncertainty sampling algorithms and
    the areas that they sample for a three-label problem. In this example, each dot
    is an item with a different label, and the heat of each pixel is the uncertainty.
    The hottest (most uncertain) pixels are the lightest pixels (the red pixels, if
    you’re viewing in color). Top left is least confidence sampling, top right is
    margin of confidence sampling, bottom left is ratio sampling, and bottom right
    is entropy-based sampling. The main takeaway is that margin of confidence and
    ratio sample some items that have only pairwise confusion and entropy maximizes
    for confusion among all labels.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 四种主要不确定性采样算法的热图以及它们为三个标签问题采样的区域。在这个例子中，每个点是一个具有不同标签的项目，每个像素的热度是不确定性。最热（最不确定）的像素是最浅的像素（如果你在彩色模式下查看，是红色像素）。左上角是最小置信度采样，右上角是置信度边际采样，左下角是比率采样，右下角是基于熵的采样。主要收获是置信度边际和比率采样了一些只有成对混淆的项目，而熵最大化所有标签之间的混淆。
- en: Notice that the difference between the methods becomes even more extreme with
    more labels. Figure 3.11 compares configurations to highlight the differences
    among the methods.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着标签数量的增加，方法之间的差异变得更加极端。图3.11比较了配置以突出显示方法之间的差异。
- en: '![](../Images/CH03_F11_Munro.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F11_Munro.png)'
- en: Figure 3.11 A comparison of methods. The four left images show that a lot of
    the uncertainty space for margin of confidence and ratio is between two of the
    labels, which is ignored entirely by entropy because it is not ambiguous for the
    third label. The four right images show that especially in more complicated tasks,
    the items that will be sampled by different uncertainty sampling algorithms will
    be different.[¹](#pgfId-1005861)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 方法比较。左边的四个图像显示，置信度边际和比率的不确定性空间大部分在两个标签之间，由于对第三个标签来说并不模糊，因此熵完全忽略了这一点。右边的四个图像显示，特别是在更复杂的任务中，不同不确定性采样算法将要采样的项目将不同。[¹](#pgfId-1005861)
- en: TIP You can play around with interactive versions of figure 3.10 and figure
    3.11 at [http://robertmunro.com/uncertainty_sampling_example.html](http://robertmunro.com/uncertainty_sampling_example.html).
    The source code for the interactive example has implementations of the uncertainty
    sampling algorithms in JavaScript, but you’re more likely to want the Python examples
    in the code repository associated with this chapter in PyTorch and in NumPy.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 您可以在[http://robertmunro.com/uncertainty_sampling_example.html](http://robertmunro.com/uncertainty_sampling_example.html)上尝试3.10和3.11图标的交互式版本。交互式示例的源代码实现了JavaScript中的不确定性采样算法，但您可能更希望使用与本章相关的PyTorch和NumPy代码库中的Python示例。
- en: Figure 3.12 summarizes the four uncertainty sampling algorithms that you’ve
    implemented in this chapter.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12总结了本章中你已实现的四个不确定性采样算法。
- en: '![](../Images/CH03_F12_Munro.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F12_Munro.png)'
- en: Figure 3.12 Uncertainty sampling cheat sheet
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 不确定性采样速查表
- en: 3.8 Further reading
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 进一步阅读
- en: Uncertainty sampling has been around for a long time, and a lot of good literature
    has been written about it. For the most cutting-edge research on uncertainty sampling,
    look for recent papers that are frequently cited themselves.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样已经存在很长时间了，关于它的很多优秀文献已经被撰写。对于不确定性采样的最前沿研究，寻找那些自身被频繁引用的近期论文。
- en: Note that most of the papers do not normalize the scores to a [0, 1] range.
    If you’re going to deploy your models for real-world situations, I highly recommend
    that you normalize the outputs. Even if normalizing the outputs won’t change the
    accuracy, it will make spot checks easier and prevent problems with downstream
    processing, especially for advanced methods that you will learn in later chapters.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，大多数论文都没有将分数归一化到[0, 1]的范围。如果你打算将你的模型部署到现实世界的情况中，我强烈建议你归一化输出。即使归一化输出不会改变准确性，它也会使检查更容易，并防止下游处理中的问题，特别是对于你将在后续章节中学习的先进方法。
- en: 3.8.1 Further reading for least confidence sampling
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.1 关于最小置信度采样的进一步阅读
- en: A good early paper on least confidence is “Reducing labeling effort for structured
    prediction tasks,” by Aron Culotta and Andrew McCallum ([http://mng.bz/opYj](http://mng.bz/opYj)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最小置信度的一个早期论文是“Aron Culotta和Andrew McCallum的《Reducing labeling effort for structured
    prediction tasks》”，[链接](http://mng.bz/opYj)。
- en: 3.8.2 Further reading for margin of confidence sampling
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.2 关于置信度边界采样的进一步阅读
- en: A good early paper on margin of confidence is “Active Hidden Markov Models for
    Information Extraction,” by Tobias Scheffer, Christian Decomain, and Stefan Wrobel
    ([http://mng.bz/nMO8](http://mng.bz/nMO8)).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 关于置信度边界的早期论文是“Tobias Scheffer, Christian Decomain和Stefan Wrobel的《Active Hidden
    Markov Models for Information Extraction》”，[链接](http://mng.bz/nMO8)。
- en: 3.8.3 Further reading for ratio of confidence sampling
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.3 关于置信度采样比率的进一步阅读
- en: I’m not aware of papers on ratio of confidence, although I’ve taught the subject
    in classes on active learning. The relationship between ratio and softmax base/temperature
    was new when it was presented in this book. As ratio of confidence is similar
    to margin of confidence, in that both look at the relationship between the two
    most confident predictions, the literature for margin of confidence should be
    mostly relevant.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道关于置信度比率的文章，尽管我在主动学习课程中教授了这个主题。当这本书中提出时，比率和softmax基础/温度之间的关系是新的。由于置信度比率与置信度边界相似，因为两者都关注两个最自信预测之间的关系，所以置信度边界的文献应该大部分是相关的。
- en: 3.8.4 Further reading for entropy-based sampling
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.4 关于基于熵采样的进一步阅读
- en: A good early paper on entropy-based sampling is “Committee-Based Sampling For
    Training Probabilistic Classifiers,” by Ido Dagan and Sean P. Engelson ([http://mng.bz/vzWq](http://mng.bz/vzWq)).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于熵采样的一个早期论文是“Ido Dagan和Sean P. Engelson的《Committee-Based Sampling For Training
    Probabilistic Classifiers》”，[链接](http://mng.bz/vzWq)。
- en: 3.8.5 Further reading for other machine learning models
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.5 关于其他机器学习模型的进一步阅读
- en: A foundational paper for uncertainty sampling more generally is “A Sequential
    Algorithm for Training Text Classifiers,” by David D. Lewis and William A. Gale
    ([http://mng.bz/4ZQg](http://mng.bz/4ZQg)). This paper uses a Bayesian classifier.
    If you look at highly cited texts from the following decade, you will find that
    SVMs and linear models are common. For the reasons mentioned in this chapter,
    I do not recommend that you try to implement uncertainty sampling with decision
    trees.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不确定性采样的一般性基础论文是“David D. Lewis和William A. Gale的《A Sequential Algorithm for
    Training Text Classifiers》”，[链接](http://mng.bz/4ZQg)。这篇论文使用贝叶斯分类器。如果你查看下个十年高度引用的文本，你会发现SVMs和线性模型很常见。由于本章中提到的理由，我不建议你尝试使用决策树实现不确定性采样。
- en: 3.8.6 Further reading for ensemble-based uncertainty sampling
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.6 关于基于集成的不确定性采样的进一步阅读
- en: 'The Dagan and Engelson paper (section 3.8.4) covers the use case of multiple
    classifiers (Query by Committee), so it is a good starting point for ensemble
    models. For more recent work focused on neural models, including dropouts and
    Bayesian approaches to better uncertainty estimates, a good entry point is “Deep
    Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale
    Empirical Study,” by Zachary C. Lipton and Aditya Siddhant ([http://mng.bz/Qmae](http://mng.bz/Qmae)).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dagan和Engelson的论文（第3.8.4节）涵盖了多个分类器（委员会查询）的使用案例，因此它是集成模型的一个良好起点。对于更多关注神经模型的研究，包括dropout和贝叶斯方法以更好地估计不确定性，一个良好的入门点是Zachary
    C. Lipton和Aditya Siddhant的“Deep Bayesian Active Learning for Natural Language
    Processing: Results of a Large-Scale Empirical Study”（[http://mng.bz/Qmae](http://mng.bz/Qmae)）。'
- en: You will see random dropouts called Monte Carlo dropouts and Bayesian (deep)
    active learning in academic literature. Regardless of the name, the strategy is
    still randomly selecting neurons/connections to ignore during prediction. The
    term *Monte Carlo* was a joke made by the physicist who invented the term. The
    term *Bayesian* comes from the fact that if you squint at the variation, it looks
    like a Gaussian distribution; it is not an actual Bayesian classifier. On the
    positive side of understanding the terminology, by passing one extra parameter
    to your model during prediction, you can impress your friends by telling them
    that you just implemented *Monte Carlo dropouts for Bayesian deep active learning*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在学术论文中看到随机dropout称为蒙特卡洛dropout和贝叶斯（深度）主动学习。无论名称如何，策略仍然是随机选择在预测期间忽略的神经元/连接。术语“蒙特卡洛”是发明该术语的物理学家开的玩笑。术语“贝叶斯”来自如果你眯着眼睛看变化，它看起来像高斯分布；它实际上不是一个贝叶斯分类器。在理解术语的积极方面，通过在预测期间向你的模型传递一个额外的参数，你可以通过告诉你的朋友你刚刚实现了“蒙特卡洛dropout用于贝叶斯深度主动学习”来让他们印象深刻。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Four common algorithms are used for uncertainty sampling: least confidence,
    margin of confidence, ratio of confidence, and entropy. These algorithms can help
    you understand the different kinds of “known unknowns” in your models.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有四种常见的算法用于不确定性采样：最小置信度、置信度范围、置信度比和熵。这些算法可以帮助你理解你模型中的不同类型的“已知未知”。
- en: You can get different samples from each type of uncertainty sampling algorithm.
    Understanding why will help you decide which one is the best way to measure uncertainty
    in your models.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从每种不确定性采样算法中获取不同的样本。理解为什么将帮助你决定哪种方式是衡量你模型中不确定性的最佳方法。
- en: Different types of scores are output by different supervised machine learning
    algorithms, including neural models, Bayesian models, SVMs, and decision trees.
    Understanding each score will help you interpret them for uncertainty.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的监督机器学习算法，包括神经网络模型、贝叶斯模型、SVM和决策树，会输出不同类型的分数。理解每个分数将帮助您解释它们以确定不确定性。
- en: Ensemble methods and dropouts can be used to create multiple predictions for
    the same item. You can calculate uncertainty by looking at variation across the
    predictions from different models.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法和dropout可以用于为同一项目创建多个预测。你可以通过查看不同模型预测中的变化来计算不确定性。
- en: There is a trade-off between getting more annotations within each active learning
    cycle and getting fewer annotations with more cycles. Understanding the trade-offs
    will let you pick the right number of cycles and size of each cycle when using
    uncertainty sampling.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个主动学习周期内获取更多注释与在更多周期内获取较少注释之间存在权衡。理解这些权衡将让你在使用不确定性采样时能够选择正确的周期数量和每个周期的规模。
- en: You may want to create different kinds of validation data to evaluate different
    parts of your system. Understanding the different types of validation data will
    let you choose the right one to tune each component.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能想要创建不同类型的验证数据来评估你系统的不同部分。理解不同类型的验证数据将让你能够选择正确的数据来调整每个组件。
- en: The right testing framework will help you calculate the accuracy of your system,
    ensuring that you are measuring performance increases correctly and not inadvertently
    biasing your data.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的测试框架将帮助你计算你系统的准确性，确保你正确地测量性能提升，而不是无意中偏差了你的数据。
- en: '* * *'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)Thank you, Adrian Calma, for suggesting the left images as a great way
    to highlight the differences.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)感谢Adrian Calma建议使用左侧图像作为突出差异的绝佳方式。
