- en: 4 Using regression for call-center volume prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用回归进行呼叫中心量预测
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Applying linear regression to real-world data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线性回归应用于现实世界数据
- en: Cleaning data to fit curves and models you have not seen before
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据以适应你之前未见过的曲线和模型
- en: Using Gaussian distributions and predicting points along them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯分布并预测沿其的点
- en: Evaluating how well your linear regression predicts the expected values
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估你的线性回归如何预测预期值的效果
- en: Armed with the power of regression-based prediction and TensorFlow, you can
    get started working on real-world problems involving more of the steps in the
    machine-learning process, such as data cleaning, fitting models to unseen data,
    and identifying models that aren’t necessarily an easy-to-spot best-fit line or
    a polynomial curve. In chapter 3, I showed you how to use regression when you
    control all steps of the machine-learning process, from using NumPy to generate
    fake data points that nicely fit a linear function (a line) or a polynomial function
    (a curve). But what happens in real life, when the data points don’t fit one of
    the patterns you’ve seen before, such as the set of points shown in figure 4.1?
    Take a close look at figure 4.1\. Is a linear regression model a good predictor
    here?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 借助基于回归的预测和TensorFlow的力量，你可以开始处理涉及机器学习过程中更多步骤的现实世界问题，例如数据清洗、将模型拟合到未见过的数据，以及识别那些不一定是容易发现的最佳拟合线或多项式曲线的模型。在第3章中，我向你展示了如何在控制机器学习过程的每个步骤时使用回归，从使用NumPy生成适合线性函数（一条线）或多项式函数（一条曲线）的假数据点开始。但在现实生活中，当数据点不适应你之前见过的任何模式时，比如图4.1中的点集，会发生什么呢？仔细看看图4.1。线性回归模型在这里是一个好的预测器吗？
- en: '![CH04_F01_Mattmann2](../Images/CH04_F01_Mattmann2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Mattmann2](../Images/CH04_F01_Mattmann2.png)'
- en: Figure 4.1 A set of data points corresponding to weeks of the year on the x-axis
    (0-51) and normalized call volume (number of calls in a particular week/max calls
    for all weeks)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 一组数据点，对应于年份的周数（x轴上的0-51）和归一化呼叫量（特定一周的呼叫次数/所有周的最大呼叫次数）
- en: 'Figure 4.2 gives you two best-fit lines, using a linear regression model for
    the call data, and it seems to be wildly off. Can you imagine the errors between
    the predicted and actual values in the middle or the tail end of the graphs on
    the left and right sides of figure 4.2? Polynomial models would be equally abysmal
    because the data does not neatly fit a curve and both increases and decreases
    in y-value at seemingly random moments along the x-axis. Convince yourself by
    drawing a second- or third-order polynomial. Can you get it to fit the data? If
    you can’t, it’s likely that a computer program will have similar difficulty. In
    situations like this one, it’s perfectly normal to ask yourself these questions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2给出了两个最佳拟合线，使用线性回归模型对呼叫数据进行拟合，这似乎非常不合适。你能想象图4.2左右两侧图中预测值和实际值之间的误差吗？多项式模型也会同样糟糕，因为数据没有整齐地适合曲线，y值在x轴上看似随机的时刻同时增加和减少。通过绘制二阶或三阶多项式来证明自己。你能让它拟合数据吗？如果你做不到，那么计算机程序可能也会遇到类似的困难。在这种情况下，问自己这些问题是完全正常的：
- en: Can regression still help me predict the next set of points?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归还能帮助我预测下一组点吗？
- en: What regression models exist besides lines and polynomial curves?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了直线和多项式曲线之外，还存在哪些回归模型？
- en: Because real-life data traditionally doesn’t come in the form of nice (x, y)
    points that are easy to plot and model with things like regression, how do I prepare
    the data so that it cleanly fits a particular model or find models that better
    fit messy data?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为现实生活中的数据通常不是以容易绘制和用回归等模型表示的漂亮的(x, y)点形式出现，我该如何准备数据，以便它能够干净地适合特定的模型，或者找到更适合混乱数据的模型？
- en: '![CH04_F02_Mattmann2](../Images/CH04_F02_Mattmann2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Mattmann2](../Images/CH04_F02_Mattmann2.png)'
- en: Figure 4.2 Two linear best-fit models for the data points shown in figure 4.1
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 对图4.1中所示数据点的两个线性最佳拟合模型
- en: In chapter 3 and throughout the book, sometimes for illustration I generate
    fake data such as the output from the NumPy `np.random.normal` function, but it
    cannot be overstated that real data rarely looks this way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章和整本书中，有时为了说明，我生成了一些假数据，例如NumPy `np.random.normal`函数的输出，但必须强调的是，真实数据很少看起来是这样的。
- en: NOTE I know that several authors are writing books on the subject of data cleaning
    for machine learning. Although the gory details are beyond the scope of this book,
    I won’t hide data cleaning steps from you; you’ll get experience using techniques
    that make this process easier with TensorFlow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我知道有几位作者正在撰写关于机器学习数据清洗主题的书籍。尽管血腥的细节超出了本书的范围，但我不会向你隐藏数据清洗步骤；你将有机会使用使此过程更简单的TensorFlow技术来获得经验。
- en: Another question that arises in applying regression to real-world problems is
    how you evaluate the accuracy of your predictions by computing the bias, or the
    difference between the predicted values of the model and the actual values. The
    lines generated by the regression model in figure 4.2 seem to be off, but you
    can make stronger statements by quantitatively measuring and evaluating the error
    between your model predictions and the ground truth. TensorFlow provides easy-to-use
    constructs with its data flow graph to compute error in a few lines of code. Matplotlib
    similarly furnishes capabilities that let you visually inspect and evaluate the
    model error with a few lines of code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在将回归应用于现实世界问题时，另一个出现的问题是，如何通过计算偏差，即模型预测值与实际值之间的差异来评估预测的准确性。图4.2中回归模型生成的线似乎有些偏离，但你可以通过定量测量和评估模型预测与真实值之间的误差来做出更强烈的陈述。TensorFlow通过其数据流图提供易于使用的结构，可以在几行代码中计算误差。Matplotlib同样提供了能力，让你可以通过几行代码直观地检查和评估模型误差。
- en: Linear regression is a good predictor in many real-world problems, so it’s hard
    to choose one. In general, problems that are temporal in nature provide a natural
    ordering of data on the x-axis for historical data training and a future state
    (such as N hours or weeks in the future) that forms a good prediction target on
    which to test regression. So choosing a problem that involves time as a dependent
    variable lends itself quite nicely to regression models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归在许多现实世界问题中是一个好的预测器，因此很难选择一个。一般来说，具有时间性质的问题为历史数据训练提供了数据在x轴上的自然排序，以及一个未来状态（例如未来N小时或几周），这形成了一个良好的预测目标，可以用来测试回归。因此，选择一个涉及时间作为因变量的问题非常适合回归模型。
- en: Lots of time-based datasets exist for machine learning, and many open data challenges
    that use them are available for free on Kaggle ([https://www.kaggle.com](https://www.kaggle.com)),
    a widely-used open machine-learning platform. Kaggle provides datasets, documentation,
    shareable code, and a platform for executing machine learning, including first-class
    support for TensorFlow-based notebooks and code. Kaggle has a large number of
    temporal datasets to try regression machine-learning models on, such as the real-estate
    price challenge ([http://mng.bz/6Ady](http://mng.bz/6Ady)) and the New York City
    (NYC) 311 open dataset ([http://mng.bz/1gPX](http://mng.bz/1gPX)). The NYC 311
    dataset is interesting because it is time-based, requires some cleaning, and does
    not cleanly fit a line or polynomial-curve regression model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，存在大量的基于时间的数据集，许多使用这些数据集的开放数据挑战在Kaggle（[https://www.kaggle.com](https://www.kaggle.com)）上免费提供，Kaggle是一个广泛使用的开放机器学习平台。Kaggle提供了数据集、文档、可分享的代码以及执行机器学习的平台，包括对基于TensorFlow的笔记本和代码的一流支持。Kaggle拥有大量的时间数据集，可以尝试回归机器学习模型，例如房地产价格挑战（[http://mng.bz/6Ady](http://mng.bz/6Ady)）和纽约市（NYC）311开放数据集（[http://mng.bz/1gPX](http://mng.bz/1gPX)）。NYC
    311数据集很有趣，因为它基于时间，需要一些清洗，并且不适合线性或多项式曲线回归模型。
- en: New York City’s Open Data platform
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约市开放数据平台
- en: The city of New York has an Open Data initiative that provides easy application
    programming interfaces (APIs) to download data for machine learning and other
    uses. You can access the Open Data portal at [http://opendata.cityofnewyork.us](http://opendata.cityofnewyork.us/).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约市有一个开放数据计划，它提供了易于使用的应用程序编程接口（API），可以下载用于机器学习和其他用途的数据。您可以通过[http://opendata.cityofnewyork.us](http://opendata.cityofnewyork.us/)访问开放数据门户。
- en: New York City’s 311 data is a set of information collected for each call made
    by a resident to the city’s customer call center requesting information about
    city and other government non-emergency services, such as waste disposal, code
    enforcement, and building maintenance. It’s probably not hard to imagine that
    customer call centers like this one get a lot of calls each day, because they
    help people get to the information they need in a timely fashion. But how many
    calls does the call center get in a week? How about in a year? Are there particular
    months or weeks when the call center can expect to get more or fewer calls?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约市的311数据是收集居民向城市客服中心打电话请求关于城市和其他政府非紧急服务信息的一系列信息，例如废物处理、法规执行和建筑维护。想象一下，像这样的客服中心每天会接到很多电话，因为它们能帮助人们及时获取所需信息。但是，客服中心一周会接到多少电话？一年呢？有没有某些月份或周，客服中心可以预期接到更多或更少的电话？
- en: Imagine that you are responsible for staffing at this type of service, especially
    during the holidays. Should you have more or fewer customer service agents manning
    the phone lines? Do spikes in calls correspond to seasons? Should you allow for
    extra seasonal employees, or will the full-time employees be enough? Regression
    and TensorFlow can help you find answers to these questions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你负责此类服务的员工配置，尤其是在假日期间。你应该有更多还是更少的客服代表接听电话？电话激增是否与季节有关？你应该允许额外的季节性员工，还是全职员工就足够了？回归分析和TensorFlow可以帮助你找到这些问题的答案。
- en: 4.1 What is 311?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 什么是311？
- en: 311 is a national service in the United States and in Canada that provides information
    about non-emergency municipal services. The service allows you to report that
    the trash collectors did not arrive to take your garbage or to find out how to
    get leaves and shrubs removed from the common area in front of your residence.
    The volume of calls to 311 vary, but in large cities and municipalities, it can
    range from thousands to tens of thousands of calls per month.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 311是美国和加拿大的一项全国性服务，提供有关非紧急市政服务的信息。该服务允许您报告垃圾收集员没有来取垃圾，或者了解如何清除您住宅前公共区域内的树叶和灌木。311的电话量各不相同，但在大城市和市政区，每月的电话量可以从几千到几万不等。
- en: One key question that these call centers and their associated information services
    must deal with is how many calls will come in a particular month. This information
    may help the service plan staffing levels during holiday periods or decide how
    much storage and computing for their resource and services directories will cost.
    Also, it may help 311 advocate and provide information on the number of people
    it expects to serve in any given year, which would help justify continued support
    for critical services.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些客服中心及其相关信息服务必须处理的一个关键问题是，在特定月份会有多少电话进来。这些信息可能有助于服务在假日期间规划人员配备水平，或者决定为他们的资源和服务目录存储和计算成本。此外，它还可能帮助311倡导者提供关于它预期在任意一年为多少人提供服务的信息，这将有助于证明对关键服务的持续支持是合理的。
- en: The first 311
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个311
- en: The first 311 service opened its doors in Baltimore, Maryland, in 1996\. The
    two main goals of the service were to develop a closer connection between the
    government and its citizens, and to create a customer relationship management
    (CRM) capability to ensure better service to the community. The CRM function today
    lends itself to the types of data-driven predictions that we’ll explore in this
    chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个311服务于1996年在马里兰州巴尔的摩开业。该服务的两个主要目标是建立政府与其公民之间更紧密的联系，并创建客户关系管理（CRM）能力，以确保更好地为社区提供服务。CRM功能今天适合我们将在本章中探讨的数据驱动预测。
- en: 'Being able to predict call volume in any given month would be an extremely
    useful capability for any 311 service. You could perform this prediction by looking
    at a year’s worth of calls and their associated dates and times, and then rolling
    those calls up on a weekly basis to construct a set of points in which the x value
    is the week number (1-52, or 365 days divided by 7 days in a week) and the y value
    is the count of calls in that particular week. Then you would do the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 能够预测任意月份的电话量对于任何311服务来说都将是一项极其有用的能力。你可以通过查看一年的电话及其相关日期和时间来完成这项预测，然后将这些电话按周汇总，构建一系列点，其中x值是周数（1-52，或每周7天除以365天），y值是特定周的电话计数。然后你会做以下事情：
- en: Plot the number of calls on the y-axis and the week number (1-52) on the x-axis.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在y轴上绘制电话数量，在x轴上绘制周数（1-52）。
- en: Examine the trend to see whether it resembles a line, a curve, or something
    else.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查趋势，看它是否类似于一条线、一个曲线或其他东西。
- en: Select and train a regression model that best fits the data points (week number
    and number of calls).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择并训练一个最适合数据点（周数和调用次数）的回归模型。
- en: Evaluate how well your model performs by calculating and visualizing its errors.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算和可视化其错误来评估你的模型表现如何。
- en: Use your new model to predict how many calls 311 can expect in any given week,
    season, and year.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您的新模型来预测311在任何给定周、季节和年份可以期待多少次调用。
- en: This prediction seems like something that a linear regression model and TensorFlow
    could help you make, and that’s precisely what you are going to work on in this
    chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测看起来像是线性回归模型和TensorFlow可以帮助你完成的，这正是你将在本章中要做的。
- en: 4.2 Cleaning the data for regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 为回归清理数据
- en: 'First, download this data—a set of phone calls from the summer of 2014 from
    the New York City 311 service—from [http://mng.bz/P16w](http://mng.bz/P16w). Kaggle
    has other 311 datasets, but you’ll use this particular data due to its interesting
    properties. The calls are formatted as a comma-separated values (CSV) file that
    has several interesting features, including the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[http://mng.bz/P16w](http://mng.bz/P16w)下载这些数据——2014年夏纽约市311服务的一组电话调用。Kaggle有其他311数据集，但你会使用这个特定的数据，因为它具有有趣的特征。这些调用以逗号分隔值（CSV）文件格式化，具有几个有趣的特征，包括以下内容：
- en: A unique call identifier showing the date when the call was created
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个唯一的调用标识符，显示调用创建的日期
- en: The location and ZIP code of the reported incident or information request
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告事件或信息请求的位置和ZIP代码
- en: The specific action that the agent on the call took to resolve the issue
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理在通话中采取的具体行动以解决问题
- en: What borough (such as the Bronx or Queens) the call was made from
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用是从哪个区（如布朗克斯或皇后区）打来的
- en: The status of the call
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用的状态
- en: This dataset contains lot of useful information for machine learning, but for
    purposes of this exercise, you care only about the call-creation date. Create
    a new file named 311.py. Then write a function to read each line in the CSV file,
    detect the week number, and sum the call counts by week.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含大量对机器学习有用的信息，但在这个练习中，你只关心调用创建日期。创建一个名为311.py的新文件。然后编写一个函数来读取CSV文件中的每一行，检测周数，并按周汇总调用次数。
- en: Your code will need to deal with some messiness in this data file. First, you
    aggregate individual calls, sometimes hundreds in a single day, into a seven-day
    or weekly bin, as identified by the `bucket` variable in listing 4.1\. The `freq`
    (short for frequency) variable holds the value of calls per week and per year.
    If the 311 CSV contains more than a year’s worth of data (as other 311 CSVs that
    you can find on Kaggle do), gin up your code to allow for selection by year of
    calls to train on. The result of the code in listing 4.1 is a `freq` dictionary
    whose values are the number of calls indexed by year and by week number via the
    `period` variable. The `t.tm_year` variable holds the parsed year resulting from
    passing the call-creation-time value (indexed in the CSV as `date_idx`, an integer
    defining the column number where the date field is located) and the `date_parse`
    format string to Python’s `time` library’s `strptime` (or string parse time) function.
    The `date_parse` format string is a pattern defining the way the date appears
    as text in the CSV so that Python knows how to convert it to a datetime representation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码需要处理这个数据文件中的一些混乱。首先，将单个调用（有时一天内多达数百次）聚合到一个七天或每周的桶中，如列表4.1中的`bucket`变量所标识。`freq`（频率的简称）变量包含每周和每年的调用次数。如果311
    CSV包含超过一年的数据（如你可以在Kaggle上找到的其他311 CSV文件），编写代码以允许按年份选择用于训练的调用。列表4.1中代码的结果是一个`freq`字典，其值是按年份和周数通过`period`变量索引的调用次数。`t.tm_year`变量包含从调用创建时间值（在CSV中索引为`date_idx`，一个定义日期字段所在列号的整数）和`date_parse`格式字符串传递给Python的`time`库的`strptime`（或字符串解析时间）函数后解析出的年份。`date_parse`格式字符串是一个模式，定义了日期在CSV中作为文本出现的方式，以便Python知道如何将其转换为datetime表示。
- en: Listing 4.1 Reading and aggregating the call count by week from the 311 CSV
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 从311 CSV中读取和汇总按周计算的调用次数
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ 7 days in a week and 365 days in a year yields 52 weeks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一周有7天，一年有365天，因此有52周。
- en: ❷ If the year is specified, select only calls for that year.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果指定了年份，则仅选择该年的调用。
- en: ❸ If the year column is not present in the call data, skip the row.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果调用数据中不存在年份列，则跳过该行。
- en: ❹ Initialize the (year, week) cell value to 0 if the CSV file contains more
    than a year of data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果 CSV 文件包含超过一年的数据，则将（年，周）单元格的值初始化为 0。
- en: ❺ For each row, add 1 call to the (year, week) or (week) indexed cell previous
    count (starts at 0).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对于每一行，将 1 次调用添加到（年，周）或（周）索引单元格的前一个计数（从 0 开始）。
- en: Most of the code in listing 4.1 handles real-world data not generated by some
    NumPy call that generates random (x, y) data points along a normal distribution—a
    theme in the book. Machine learning expects clean data to perform all its black
    magic, and real-world data is not clean. Take a random set of 311 CSV files from
    NYC’s Open Data portal ([https://data.cityofnewyork.us/browse?q=311](https://data.cityofnewyork.us/browse?q=311)),
    and you will find a lot of variance. Some files have calls from multiple years,
    so your code needs to handle that situation; some files have missing rows, or
    missing year and date values for a particular cell, and your code still needs
    to hold up. Writing resilient data-cleaning code is one of the fundamental principles
    of machine learning, so writing this code will be the first step in many examples
    in this book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 中的大部分代码处理的是现实世界中的数据，而不是由 NumPy 调用生成的随机（x, y）数据点，这些数据点沿正态分布——这是本书中的一个主题。机器学习期望干净的数据来执行其所有黑魔法，而现实世界中的数据并不干净。从纽约市开放数据门户（[https://data.cityofnewyork.us/browse?q=311](https://data.cityofnewyork.us/browse?q=311)）随机选取
    311 个 CSV 文件，您会发现很多差异。有些文件包含来自多个年份的调用，因此您的代码需要处理这种情况；有些文件有缺失的行，或者特定单元格缺失年份和日期值，而您的代码仍然需要保持稳定。编写健壮的数据清理代码是机器学习的基本原则之一，因此编写此代码将是本书许多示例中的第一步。
- en: Call the `read` function defined in listing 4.1, and you will get back a Python
    dictionary indexed by (`year,` `week` `number`) or simply by (`week` `number`),
    depending on whether you passed in the year as the last functional argument. Call
    the function in your 311.py code. The function takes as input the column index
    (1 for the second column, indexed by 0) and then a string that looks like
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 调用列表 4.1 中定义的 `read` 函数，您将得到一个以（年，周数）或简单地以（周数）为索引的 Python 字典，具体取决于您是否将年份作为最后一个功能参数传递。在您的
    311.py 代码中调用此函数。该函数接受列索引（第二列，索引为 0）和看起来像的字符串作为输入。
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tell the function that the date field is in index 1 (or the second column based
    on a 0 index), that your dates are formatted as strings that look like `'month/day/year
    hour:minutes:seconds` `AM/PM'` or `'12/10/2014` `00:00:30` `AM'` (corresponding
    to December 10, 2014 at 30 seconds past midnight), and that you would like to
    obtain only dates from 2014 from the CSV.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉函数日期字段位于索引 1（或基于 0 索引的第二列），您的日期格式为类似 `'month/day/year hour:minutes:seconds
    AM/PM'` 或 `'12/10/2014 00:00:30 AM'` 的字符串（对应于 2014 年 12 月 10 日午夜过后的 30 秒），并且您希望从
    CSV 中获取 2014 年的日期。
- en: You can print the values from the frequency buckets if you are using Jupyter
    by inspecting the `freq` dictionary. The result is a 52-week (indexed from 0,
    so 0-51) histogram with call counts per week. As you can tell from the output,
    the data is clustered from week 22 to week 35, which is May 26 to August 25, 2014`:`
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Jupyter，可以通过检查 `freq` 字典来打印频率桶中的值。结果是每周调用次数的 52 周直方图（从 0 索引，因此为 0-51）。如您从输出中可以看出，数据从第
    22 周到第 35 周聚集，即 2014 年 5 月 26 日至 8 月 25 日：`
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The International Standards Organization (ISO) body, which defines standards
    for computer code, data, and software, publishes the frequently used ISO-8601
    standard for representing dates and times as strings. Python’s `time` and `iso8601`
    libraries implement this standard, which includes a specification of week numbers
    associated with dates and times for weeks starting on Mondays ([https://www.epochconverter
    .com/weeknumbers](https://www.epochconverter.com/weeknumbers)), which seems to
    be a useful representation for the 311 data. Though other week-number representations
    are available, most of them have a different starting day for the week, such as
    Sunday.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 国际标准化组织（ISO）机构，该机构定义了计算机代码、数据和软件的标准，发布了常用的 ISO-8601 标准，用于将日期和时间作为字符串表示。Python
    的 `time` 和 `iso8601` 库实现了此标准，该标准包括与以星期一为开始日期的周数相关的日期和时间的规范（[https://www.epochconverter.com/weeknumbers](https://www.epochconverter.com/weeknumbers)），这对于
    311 数据似乎是一个有用的表示。尽管其他周数表示方法也可用，但它们大多数都有不同的周起始日，例如星期日。
- en: By translating time-based dates to week numbers on the x-axis, you have an integer
    value representing time that can be easily ordered and visualized. This value
    goes along with your call-frequency y-axis values that the regression function
    will predict.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将基于时间的日期转换为x轴上的周数，你得到一个表示时间的整数值，它可以很容易地进行排序和可视化。这个值与回归函数将预测的调用频率y轴值相匹配。
- en: Converting week numbers to dates
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将周数转换为日期
- en: Much of the time-based data that you will deal with is sometimes better represented
    by a week number. Dealing with an integer from 1 to 52 is a lot better than the
    string value `'Wednesday, September 5, 2014'`, for example. The EpochConverter
    website can easily tell you the week number for a year and date. To see the list
    of days mapped to week numbers for 2014 output from listing 4.1, visit [https://www.epochconverter
    .com/weeks/2014](https://www.epochconverter.com/weeks/2014).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你将要处理的大部分基于时间的数据有时用周数表示会更好。处理一个从1到52的整数，比如字符串值`'周三，2014年9月5日'`，要好得多。EpochConverter网站可以轻松地告诉你某年某周的周数。要查看列表4.1中2014年输出映射到周数的日期列表，请访问[https://www.epochconverter.com/weeks/2014](https://www.epochconverter.com/weeks/2014)。
- en: 'You can get the same information in plain Python by using the `datetime` library:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`datetime`库在纯Python中获取相同的信息：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code outputs 24 because June 16, 2010, was the 24th week of that year.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出24，因为2010年6月16日是该年的第24周。
- en: You’re welcome!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不用谢！
- en: Open a Jupyter notebook, and paste listing 4.2 into it to visualize the binned
    histogram of call frequencies per week from your `freq` dictionary (returned from
    the `read` function) and to generate figure 4.3, which is the distribution of
    points discussed at the beginning of the chapter. By examining the point distribution,
    you can decide on a model for your regression predictor to code in TensorFlow.
    Listing 4.2 sets up your input training values as weeks 1-52 and your prediction
    output as the number of calls expected in that week. The `X_train` variable holds
    an array of the keys of the frequency dictionary (the integers 0-51, for 52 weeks),
    and the `Y_train` variable holds the number of 311 calls per week. `nY_train`
    holds the normalized Y values (divided by `maxY`) between 0 and 1\. I explain
    why later in the chapter, but the preview is that it eases learning during the
    training process. The final lines of code use Matplotlib to create a scatter plot
    of the (week number, call frequency) points.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个Jupyter笔记本，并将列表4.2粘贴进去，以可视化从你的`freq`字典（由`read`函数返回）中按周计算的调用频率的分组直方图，并生成图4.3，这是本章开头讨论的点分布。通过检查点分布，你可以决定在TensorFlow中编码的回归预测器的模型。列表4.2设置了你的输入训练值为第1-52周，你的预测输出为该周预期的调用次数。`X_train`变量包含频率字典的键（0-51的整数，代表52周），而`Y_train`变量包含每周的311次调用次数。`nY_train`包含标准化后的Y值（除以`maxY`），介于0到1之间。我将在本章后面解释原因，但预览是它在训练过程中简化了学习过程。代码的最后几行使用Matplotlib创建了一个（周数，调用频率）点的散点图。
- en: Listing 4.2 Visualizing and setting up the input data
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 可视化和设置输入数据
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Defines the input training data X as the week numbers, which are the keys
    of the python freq dictionary
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义输入训练数据X为周数，这是Python freq字典的键
- en: ❷ Defines the input training data Y as the number of calls corresponding to
    the specific week
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义输入训练数据Y为特定周的调用次数
- en: ❸ Normalizes the number of calls to values between 0 and 1 to ease learning
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将调用值之间的数量标准化到0到1之间，以简化学习过程
- en: ❹ Plots the data to learn
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制数据以学习
- en: The output of listing 4.2 is shown in figure 4.3\. it’s quite dissimilar from
    the lines and curves we tried to fit a model for in chapter 3\. Remember that
    I said that real-world data isn’t always pretty? Intuitively, the data tells us
    that for most of the first two quarters of the year, there were no calls; a large
    spike in spring extended through the summer; and in the fall and winter, there
    were no calls. Perhaps summer is a season when many people in New York use 311,
    at least in 2014\. More likely, the data is only a subset of the available actual
    information, but let’s still see whether we can make a good model of it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2的输出显示在图4.3中。它与我们在第3章中尝试拟合模型的线条和曲线相当不同。记得我说过现实世界的数据并不总是那么完美吗？直观上看，数据告诉我们，在年初的前两个季度的大部分时间里，没有调用；春季出现的大峰值一直延续到夏天；在秋冬季节，没有调用。也许夏天是纽约许多人在2014年使用311的季节。更有可能的是，这些数据只是可用实际信息的一个子集，但让我们看看我们是否可以从中构建一个好的模型。
- en: '![CH04_F03_Mattmann2](../Images/CH04_F03_Mattmann2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Mattmann2](../Images/CH04_F03_Mattmann2.png)'
- en: Figure 4.3 A plot of call-frequency counts on the y-axis compared with weeks
    of the year (0-51) on the x-axis. Activity swells in the May 2014 time-frame and
    tapers in late August 2014.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 在y轴上显示呼叫频率计数，与x轴上的年份周数（0-51）进行比较。活动在2014年5月期间增加，在2014年8月下旬减少。
- en: If you have ever looked at the distribution of scores on a test, one common
    model for describing the sorted score distribution from 0 to 100 is a curve or
    a bell curve. As it turns out, we’ll be teaching our TensorFlow predictor to emulate
    that type of model in the remainder of the chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经查看过测试分数的分布，描述从0到100排序的分数分布的常见模型是曲线或钟形曲线。结果证明，我们将在本章的剩余部分教会我们的TensorFlow预测器模仿这种类型的模型。
- en: 4.3 What’s in a bell curve? Predicting Gaussian distributions
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 钟形曲线中有什么？预测高斯分布
- en: A bell or normal curve is a common term to describe data that we say fits a
    normal distribution. The largest Y values of the data occur in the middle or statistically
    the mean X value of the distribution of points, and the smaller Y values occur
    on the early and tail X values of the distribution. We also call this a Gaussian
    distribution after the famous German mathematician Carl Friedrich Gauss, who was
    responsible for the Gaussian function that describes the normal distribution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 钟形曲线或正态曲线是描述我们所说的符合正态分布的数据的常用术语。数据中最大的Y值出现在中间或统计上的均值X值，而较小的Y值出现在分布的早期和尾部X值。我们也将这种分布称为高斯分布，以纪念著名的德国数学家卡尔·弗里德里希·高斯，他负责描述正态分布的高斯函数。
- en: 'We can use the NumPy method `np.random.normal` to generate random points sampled
    from the normal distribution in Python. The following equation shows the Gaussian
    function that underlies this distribution:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumPy方法`np.random.normal`在Python中生成从正态分布中抽取的随机点。以下方程显示了该分布背后的高斯函数：
- en: '![CH04_F03EQ01_Mattmann2](../Images/CH04_F03EQ01_Mattmann2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03EQ01_Mattmann2](../Images/CH04_F03EQ01_Mattmann2.png)'
- en: The equation includes the parameters μ (pronounced mu) and σ (pronounced sigma),
    where mu is the mean and sigma is the standard deviation of the distribution,
    respectively. Mu and sigma are the parameters of the model, and as you have seen,
    TensorFlow will learn the appropriate values for these parameters as part of training
    a model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 方程包括参数μ（发音为mu）和σ（发音为sigma），其中mu是分布的均值，sigma是分布的标准差。Mu和sigma是模型的参数，正如你所看到的，TensorFlow将在训练模型的过程中学习这些参数的适当值。
- en: To convince yourself that you can use these parameters to generate bell curves,
    you can type the code snippet in listing 4.3 into a file named gaussian.py and
    then run it to produce the plot that follows it. The code in listing 4.3 produces
    the bell curve visualizations shown in figure 4.4\. Note that I selected values
    of mu between -1 and 2\. You should see center points of the curve in figure 4.4,
    as well as standard deviations (sigma) between 1 and 3, so the width of the curves
    should correspond to those values inclusively. The code plots 120 linearly-spaced
    points with X values between -3 and 3 and Y values between 0 and 1 that fit the
    normal distribution according to mu and sigma, and the output should look like
    figure 4.4.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你相信你可以使用这些参数来生成钟形曲线，你可以将列表4.3中的代码片段输入到名为gaussian.py的文件中，然后运行它以生成随后的图表。列表4.3中的代码生成了图4.4中所示的高斯曲线可视化。请注意，我选择了mu在-1和2之间的值。你应该在图4.4中看到曲线的中心点，以及标准差（sigma）在1和3之间，因此曲线的宽度应该对应这些值。代码绘制了120个线性间隔的点，X值在-3和3之间，Y值在0和1之间，这些点根据mu和sigma符合正态分布，输出应该类似于图4.4。
- en: '![CH04_F04_Mattmann2](../Images/CH04_F04_Mattmann2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Mattmann2](../Images/CH04_F04_Mattmann2.png)'
- en: Figure 4.4 Three bell curves with means between -1 and -2 (their center points
    should be near those points) and with standard deviations between 1 and 3\. Curves
    are generated from 120 points generated with linear distribution between -3 and
    3.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 三个均值在-1和-2之间（它们的中心点应该接近这些点）且标准差在1和3之间的钟形曲线。曲线是由在-3和3之间线性分布的120个点生成的。
- en: Listing 4.3 Producing some Gaussian distributions and visualizing them
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 生成一些高斯分布并可视化它们
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Implement the Gaussian curve with mu and sigma.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用mu和sigma实现高斯曲线。
- en: ❷ Randomly pick 120 linearly spaced samples of X between -3 and 3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机选择X在-3和3之间的120个线性间隔样本。
- en: 4.4 Training your call prediction regressor
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 训练你的呼叫预测回归器
- en: Now you are ready to use TensorFlow to fit your NYC 311 data to this model.
    It’s probably clear by looking at the curves that they seem to comport naturally
    with the 311 data, especially if TensorFlow can figure out the values of mu that
    put the center point of the curve near spring and summer and that have a fairly
    large call volume, as well as the sigma value that approximates the best standard
    deviation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已准备好使用 TensorFlow 将你的纽约市 311 数据拟合到这个模型。通过观察曲线，它们似乎自然地与 311 数据相符，特别是如果 TensorFlow
    能够找出 mu 的值，使曲线的中心点接近春季和夏季，并且具有相当大的调用量，以及近似最佳标准差的 sigma 值。
- en: Listing 4.4 sets up the TensorFlow training session, associated hyperparameters,
    learning rate, and number of training epochs. I’m using a fairly large step for
    learning rate so that TensorFlow can appropriately scan the values of `mu` and
    `sig` by taking big-enough steps before settling down. The number of epochs—5,000—gives
    the algorithm enough training steps to settle on optimal values. In local testing
    on my laptop, these hyperparameters arrived at strong accuracy (99%) and took
    less than a minute. But I could have chosen other hyperparameters, such as a learning
    rate of 0.5, and given the training process more steps (epochs). Part of the fun
    of machine learning is hyperparameter training, which is more art than science,
    though techniques such as meta-learning and algorithms such as HyperOpt may ease
    this process in the future. A full discussion of hyperparameter tuning is beyond
    the scope of this chapter, but an online search should yields thousands of relevant
    introductions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 设置了 TensorFlow 训练会话、相关超参数、学习率和训练时代数。我使用了一个相当大的学习率步长，以便 TensorFlow 在稳定下来之前能够通过足够大的步长适当地扫描
    mu 和 sig 的值。时代数——5,000——为算法提供了足够的训练步骤以确定最佳值。在我的笔记本电脑上的本地测试中，这些超参数达到了强准确性（99%），并且用时不到一分钟。但我可以选择其他超参数，例如学习率为
    0.5，并给训练过程更多的步骤（时代）。机器学习的乐趣之一是超参数训练，这与其说是科学不如说是艺术，尽管元学习和 HyperOpt 等算法可能会在未来简化这一过程。超参数调整的全面讨论超出了本章的范围，但在线搜索应该会找到成千上万的相关介绍。
- en: When the hyperparameters are set up, define the placeholders X and Y, which
    will be used for the input week number and associated number of calls (normalized),
    respectively. Earlier, I mentioned normalizing the Y values and creating the `nY_train`
    variable in listing 4.2 to ease learning. The reason is that the model Gaussian
    function that we are attempting to learn has Y values only between 0 and 1 due
    to the exponent e. The `model` function defines the Gaussian model to learn, with
    the associated variables `mu` and `sig` initialized arbitrarily to `1`. The `cost`
    function is defined as the L2 norm, and the training uses Gradient descent. After
    training your regressor for 5,000 epochs, the final steps in listing 4.4 print
    the learned values for `mu` and `sig`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当超参数设置好之后，定义占位符 X 和 Y，它们将分别用于输入周数和相关的调用次数（归一化）。之前我提到过对 Y 值进行归一化并在列表 4.2 中创建
    `nY_train` 变量以简化学习。原因是由于指数 e，我们试图学习的模型 Gaussian 函数的 Y 值仅在 0 和 1 之间。`model` 函数定义了要学习的
    Gaussian 模型，相关的变量 `mu` 和 `sig` 随机初始化为 `1`。`cost` 函数定义为 L2 范数，训练使用梯度下降。在训练回归器 5,000
    个时代后，列表 4.4 中的最后几步打印出 mu 和 sig 的学习值。
- en: Listing 4.4 Setting up and training the TensorFlow model for your Gaussian curve
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 设置和训练 TensorFlow 模型以拟合你的 Gaussian 曲线
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Sets the learning rate for each epoch
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为每个时代设置学习率
- en: ❷ Trains for 5,000 epochs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练 5,000 个时代
- en: ❸ Sets up the input (X) and the values to predict (Y)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置输入（X）和预测值（Y）
- en: ❹ Defines the learned parameters mu and sig for the model
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义模型的学习参数 mu 和 sig
- en: ❺ Creates the model based on the TensorFlow graph
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 基于 TensorFlow 图创建模型
- en: ❻ Defines the cost function as the L2 norm and sets up the training operation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义成本函数为 L2 范数并设置训练操作
- en: ❼ Initializes the TensorFlow session
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 初始化 TensorFlow 会话
- en: ❽ Performs the training and learns the values for mu and sig
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 执行训练并学习 mu 和 sig 的值
- en: ❾ Prints the learned values for mu and
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 打印 mu 的学习值
- en: ❿ Closes the session
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 关闭会话
- en: 'The output of listing 4.4 should look similar to the following, which corresponds
    to the learned values for `mu` and `sig`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 的输出应类似于以下内容，它对应于 mu 和 sig 的学习值：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When you finish printing the values and saving them in the local variables `mu_val`
    and `sig_val`, don’t forget to close your TensorFlow session so that you can free
    the resources it was using for its 5,000 iterations of training. You can take
    care of this task by invoking `sess.close()`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成打印值并将它们保存在局部变量 `mu_val` 和 `sig_val` 中后，不要忘记关闭你的 TensorFlow 会话，以便你可以释放它用于其
    5,000 次训练迭代的资源。你可以通过调用 `sess.close()` 来处理这个任务。
- en: TIP Listing 4.4 shows you how to build the call-center volume prediction algorithm
    with TensorFlow 1.x. In case you were wondering how the algorithm would look in
    TensorFlow 2.x, jump to the appendix, where I’ve discussed and reworked the model
    for you in TensorFlow 2.x-speak. There are some minor differences, but they’re
    worth checking out. The other data cleaning and preparation code that you’ve seen
    thus far stays the same, as do the validation and error calculation that you’re
    about to see.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：列表 4.4 展示了如何使用 TensorFlow 1.x 构建呼叫中心体积预测算法。如果你想知道算法在 TensorFlow 2.x 中的样子，请跳转到附录，我在那里用
    TensorFlow 2.x 的语言讨论并重新工作了模型。有一些细微的差异，但它们值得一看。其他你迄今为止看到的数据清理和准备代码保持不变，即将看到的验证和错误计算也是如此。
- en: 4.5 Visualizing the results and plotting the error
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 可视化结果和绘制误差图
- en: How well did your linear regression do at predicting the 311 call frequencies
    per week? Take the mu and sigma values from `mu_val` and `sig_val`, and use them
    to plot the learned model in listing 4.5 and figure 4.5\. Matplotlib takes care
    of the initial scatter plot of week number (`x-axis`) by frequency of calls normalized
    (`y-axis`). Then you take the learned parameters for `mu_val` and `sig_val` and
    plug them into a one-line unfolding of the model equation from listing 4.4\. The
    reason to do this instead of reusing the model function and passing in `mu_val`
    and `sig_val` as arguments is that when TensorFlow learns the optimal parameter
    values, you don’t need to set up a TensorFlow graph again or pass a session to
    it to evaluate the model values. Instead, you can use NumPy and its analogous
    functions such as `np.exp` (exponent) and `np.power`. These functions are equivalent
    to TensorFlow’s same-named functions, except that NumPy doesn’t require a TensorFlow
    session and its associated resources to evaluate the value at each point. `trY2`
    has the resulting learned call-frequency predictions; because they were normalized
    to values between 0 and 1 to learn the Gaussian function, you have to multiply
    the learned values by `maxY` to discern the actual unnormalized call-frequency
    predictions per week. The result is plotted in red alongside the original training
    data, and the prediction for the number of calls in an arbitrary week is printed
    and compared with the original training data value to test the model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你的线性回归在预测每周 311 呼叫频率方面做得怎么样？从 `mu_val` 和 `sig_val` 中获取均值和标准差值，并使用它们在列表 4.5 和图
    4.5 中绘制学习到的模型。Matplotlib 会处理初始的周数（X 轴）与呼叫频率（Y 轴）归一化的散点图。然后，你将 `mu_val` 和 `sig_val`
    的学习参数插入到列表 4.4 中的模型方程的一行展开中。这样做而不是重用模型函数并将 `mu_val` 和 `sig_val` 作为参数传递的原因是，当 TensorFlow
    学习到最佳参数值时，你不需要再次设置 TensorFlow 图或传递会话来评估模型值。相反，你可以使用 NumPy 及其类似函数，如 `np.exp`（指数）和
    `np.power`。这些函数与 TensorFlow 的同名函数等效，但 NumPy 不需要 TensorFlow 会话及其相关资源来评估每个点的值。`trY2`
    包含了结果学习到的呼叫频率预测；因为它们被归一化到 0 和 1 之间以学习高斯函数，所以你必须将学习值乘以 `maxY` 来区分每周的实际未归一化呼叫频率预测。结果是红色绘制在原始训练数据旁边，并且打印出任意周的呼叫数量预测并与原始训练数据值进行比较，以测试模型。
- en: '![CH04_F05_Mattmann2](../Images/CH04_F05_Mattmann2.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Mattmann2](../Images/CH04_F05_Mattmann2.png)'
- en: Figure 4.5 The learned TensorFlow model (shown in red) that predicts call volume
    per week of the year (0-51) for weeks 1-52 (indexed at 0). The actual call data
    volume per week is shown in blue.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 学习到的 TensorFlow 模型（以红色显示），预测每年每周（0-51）的呼叫量，对于第 1-52 周（索引为 0）。每周的实际呼叫数据量以蓝色显示。
- en: Listing 4.5 Visualizing the learned model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 可视化学习到的模型
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Plots the training data points of week number (X) by number of calls (Y) as
    blue scatter points
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以蓝色散点点绘制周数（X）按呼叫数量（Y）的训练数据点
- en: ❷ Fits the Gaussian function model with the learned mu and sig parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用学习到的均值和标准差参数拟合高斯函数模型
- en: ❸ Plots the learned model in red
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以红色绘制学习到的模型
- en: 'The results of listing 4.5 should be similar to the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 的结果应类似于以下内容：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Although it may seem crazy that your model predicted a call volume that was
    ~15,000 calls off the actual value, remember the discussion of bias and variance
    in chapter 3\. You aren’t looking for a model that is insanely overfitted to the
    data and contorts every which way to ensure that it crosses through every point.
    Those models exhibit high bias and low variance, and as such, they are overfitted
    to the training data. Instead, you want models that show low bias and low variance
    and that perform well on unseen data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你的模型预测的呼叫量可能比实际值低15,000个呼叫，看起来可能有些疯狂，但请记住第三章中关于偏差和方差的讨论。你并不是在寻找一个过度拟合数据并且扭曲各种方式以确保它穿过每个点的模型。这些模型表现出高偏差和低方差，因此它们过度拟合了训练数据。相反，你想要的是表现出低偏差和低方差，并且在未见数据上表现良好的模型。
- en: Provided with new unseen calls, so long as NYC 311 populations seem to be seasonally
    based in spring and summer, your model is well fitted and will perform well. It
    can even handle distribution shift (shift in call volumes among seasons) as long
    as the input is within the standard deviation and/or a particular center mean
    corresponds to the largest number of calls. You can update your model and have
    it learn new `mu` and `sig` values; then you’re all set.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提供新的未见呼叫数据，只要纽约市311人口在春季和夏季似乎具有季节性，你的模型就拟合得很好，并且会表现良好。它甚至可以处理分布变化（季节间呼叫量的变化），只要输入在标准差内和/或特定的中心均值对应于最大呼叫数。你可以更新你的模型，让它学习新的`mu`和`sig`值；然后你就准备好了。
- en: You may have one question about this discussion, though. You are evaluating
    your model’s performance by talking about how far off the predictions are from
    the actual values. You can use some tools that you’ve already seen—Jupyter and
    Matplotlib—to measure the error quantitatively. Run listing 4.6 to compute the
    error, average error, and accuracy of your model and then visualize the model
    as shown in figure 4.6\. The `error` variable is defined as the square root of
    the squared difference between the predicted data and the training data (which,
    as you may recall from chapter 1, is the L2 norm). This error variable yields
    a distance for every predicted call frequency by week from training data and is
    best visualized as a bar graph. The `avg_error` is computed as the variance between
    each predicted point in `trY2` compared with `Y_train` divided by the total number
    of weeks to get the average error in predicted calls per week. The overall accuracy
    of the algorithm is 1 (the average error in call predictions per week divided
    by the maximum calls per week). The result is stored in the `acc` variable and
    printed at the end of the listing.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可能对这个讨论有一个疑问。你通过讨论预测值与实际值之间的差异来评估你的模型性能。你可以使用你已经看到的一些工具——Jupyter和Matplotlib——来定量地测量误差。运行列表4.6来计算模型的误差、平均误差和准确性，然后如图4.6所示可视化模型。`error`变量定义为预测数据与训练数据（如你可能在第一章中回忆的那样，是L2范数）之间平方差的平方根。这个误差变量为每周的每个预测呼叫频率提供了距离，最好以条形图的形式可视化。`avg_error`是通过将`trY2`中的每个预测点与`Y_train`之间的方差除以总周数来计算的，以获得每周预测呼叫的平均误差。算法的整体准确性为1（每周呼叫预测的平均误差除以每周的最大呼叫数）。结果存储在`acc`变量中，并在列表末尾打印出来。
- en: Listing 4.6 Computing the error and visualizing it
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 计算误差并可视化它
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Error is the square root of the squared difference between the model’s predicted
    value and the actual value.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 误差是模型预测值与实际值之间平方差的平方根。
- en: ❷ Shows the error as a histogram of errors at each week value
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以每周值误差的直方图形式显示误差
- en: ❸ Computes the overall average error (number of calls difference per week) by
    using Map Reduce to sum the differences and divide by total weeks
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过使用Map Reduce来求和差异并除以总周数来计算整体平均误差（每周呼叫差异数）
- en: ❹ Computes the accuracy by subtracting the average error divided by the max
    number of calls across the whole dataset from 1
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过从1中减去平均误差除以整个数据集的最大呼叫数量来计算准确性
- en: The resulting plot, showing the errors per week in phone-call prediction, also
    resembles the prediction curve.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表，显示了每周电话预测的误差，也类似于预测曲线。
- en: '![CH04_F06_Mattmann2](../Images/CH04_F06_Mattmann2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Mattmann2](../Images/CH04_F06_Mattmann2.png)'
- en: Figure 4.6 Visualizing the error in number of calls predicted per week. The
    model performs quite well in the weeks that have high call volume.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 展示了每周预测呼叫数量的误差可视化。模型在呼叫量高的周表现相当好。
- en: 'Though it may seem that the overall error is high—and it is at the tails of
    the distribution—overall, the average error is only 205 calls per week of variance
    due to the model’s spot-on predictions when it matters most: during high-call-volume
    weeks. You can see the model’s average error and accuracy (99%) in the output
    of listing 4.6:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然整体误差可能看起来很高——它确实在分布的尾部——但平均误差仅为每周205个呼叫的方差，这是由于模型在关键时刻（高呼叫量周）的预测非常准确。你可以在列表4.6的输出中看到模型的平均误差和准确率（99%）：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You learned about regularization and training test splits in chapter 3\. I cover
    them briefly in section 4.6.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第三章中学习了正则化和训练测试分割。我在第4.6节中简要介绍了它们。
- en: 4.6 Regularization and training test splits
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 正则化和训练测试分割
- en: The concept of regularization is probably best described in figure 4.7\. During
    training, a machine-learning model (`M`), given some input data (`X`) for the
    cost function, the training process explores the space for the parameters (`W`)
    that should minimize the difference between the model response (predictions) and
    the actual training input values (`X`). This cost is captured in the function
    M`(X, W)`. The challenge is that during training and parameter exploration, the
    algorithm sometimes picks locally optimal though globally poor parameter values
    for `W`. Regularization can influence this choice by penalizing larger values
    of `W` for weights and trying to keep the weight exploration to the optimal area
    denoted in figure 4.7 (the white circular region with the red outline).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的概念可能最好在图4.7中描述。在训练过程中，一个机器学习模型（`M`），给定一些用于成本函数的输入数据（`X`），训练过程会探索参数（`W`）的空间，以最小化模型响应（预测）与实际训练输入值（`X`）之间的差异。这种成本被捕获在函数`M(X,
    W)`中。挑战在于，在训练和参数探索过程中，算法有时会选择局部最优但全局较差的参数值`W`。正则化可以通过惩罚较大的`W`值来影响这种选择，并试图将权重探索保持在图4.7中标记的优化区域（白色带红色轮廓的圆形区域）。
- en: You should consider using regularization when models underfit or overfit your
    input data or when the weight exploration process during training needs some help
    with penalizing the higher or yellow spaces of the parameter space `W`. Your new
    Gaussian bell-curve model for 311 demonstrates suitably high accuracy, though
    it has errors on the tail ends of the call-volume distribution. Would regularization
    help get rid of these errors?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型对输入数据欠拟合或过拟合，或者在训练过程中权重探索过程需要帮助惩罚参数空间`W`中的较高或黄色区域时，你应该考虑使用正则化。你为311创建的新高斯钟形曲线模型具有相当高的准确度，尽管它在呼叫量分布的尾部存在一些误差。正则化能帮助消除这些误差吗？
- en: '![CH04_F07_Mattmann2](../Images/CH04_F07_Mattmann2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Mattmann2](../Images/CH04_F07_Mattmann2.png)'
- en: Figure 4.7 Reviewing the concept of regularization. Input data (`X`) provided
    during training is fitted to a model (`M`) through the training process of learning
    the parameters (`W`) by measuring the predictions of those parameters against
    a cost function `M(X,` `W)`. During the training process, you want the exploration
    of the parameter space `W` (top left) to zero in on the white area and stay out
    of the yellow. Regularization enables this result.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7回顾正则化的概念。在训练过程中提供的输入数据（`X`）通过学习参数（`W`）的训练过程与模型（`M`）相匹配，通过测量这些参数的预测与成本函数`M(X,
    W)`进行比较。在训练过程中，你希望参数空间`W`（左上角）的探索集中在白色区域，并避开黄色区域。正则化使得这种结果成为可能。
- en: 'It may be counterintuitive, but the answer is no, due to four concepts: the
    hyperparameters to our training process, the learning rate, the number of training
    steps (epochs), and the initial values of mu and sig from listing 4.4\. I’ve copied
    them here for your convenience:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能看起来有些反直觉，但答案是：不，这是由于四个概念：我们训练过程的超参数、学习率、训练步数（周期）以及列表4.4中mu和sig的初始值。为了方便起见，我将它们复制在这里：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are the hyperparameters and learned parameters of the model. The initial
    values matter significantly and were not picked out of thin air. Knowing the optimal
    values for `mu` and `sig` (`~27` and `~4.9`, respectively) would have been great
    ahead of time, but you didn’t have that knowledge. Instead, you set them to default
    values (`1` for each) and allowed the learning rate 1.5 to control the steps of
    exploration. Set the learning-rate value too low (such as `0.1` or `0.001`), and
    the algorithm could require hundreds of thousands of training epochs and still
    never achieve an optimal value for the learned parameters. Set the learning rate
    beyond 1.5 (say, 3.5.) and the algorithm will skip the optimal values in a particular
    training step.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是模型的超参数和学习的参数。初始值非常重要，并不是随意选择的。提前知道`mu`（均值）和`sig`（标准差）的最佳值（分别为`~27`和`~4.9`）会很好，但你没有那个知识。相反，你将它们设置为默认值（每个为`1`），并允许学习率1.5控制探索的步骤。将学习率设置得太低（如`0.1`或`0.001`），算法可能需要数十万个训练周期，却永远无法达到学习参数的最佳值。将学习率设置超过1.5（比如3.5），算法将跳过特定训练步骤中的最佳值。
- en: TIP Despite not knowing mu (mean) and sigma (standard deviation) ahead of time,
    you can eyeball the data and derive them partially. Near week 27 is the peak or
    mean of the distribution. As for the standard deviation, eyeballing it as ~5 is
    hard but not impossible. Standard deviation is a measure of the spread of numbers
    in each unit from the mean. Low standard deviation means that the tails of the
    distribution are close to the mean. Being able to use your eyeballs to explore
    the inputs and expected values and tune the initial model parameters will save
    you lots of time and epochs later.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：尽管事先不知道`mu`（均值）和`sigma`（标准差），你可以通过观察数据并部分地推导出它们。接近第27周是分布的峰值或均值。至于标准差，将其估计为~5很困难但并非不可能。标准差是衡量每个单位中数字从均值扩散开来的度量。低标准差意味着分布的尾部接近均值。能够使用你的眼睛探索输入和预期值并调整初始模型参数将为你节省大量的时间和训练周期。
- en: Regularization isn’t needed in this particular case because intuitively, you
    explored your training data, visualized it, and normalized the call frequencies
    to values between `0` and `1` to ease learning, and because your model fits the
    data and is not over- or underfitted. Penalizing the parameter exploration step
    would have a net negative effect in this case, possibly preventing you from fitting
    the model more precisely.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定情况下，不需要正则化，因为直观上，你已经探索了你的训练数据，可视化了它，并将呼叫频率归一化到`0`到`1`之间以简化学习，因为你的模型适合数据并且没有过拟合或欠拟合。惩罚参数探索步骤在这种情况下会产生净负面影响，可能阻止你更精确地拟合模型。
- en: Additionally, given the sparsity of the data—something you found out only by
    visualizing and performing the exploratory data analysis process—splitting the
    data into train/test isn’t tenable because you will lose too much information
    for your model. Suppose that your split at 70/30 removed anything other than the
    tails of the distribution. The information loss could make your regression see
    not a bell curve, but lines or small polynomial curves, and learn the wrong model—or,
    worse, learn the right model of the wrong data, such as the graphs in figure 4.2\.
    Train/test splits do not make sense here.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，鉴于数据的稀疏性——这是你在可视化和执行探索性数据分析过程中才发现的——将数据分为训练/测试集是不可行的，因为你将失去模型所需的大量信息。假设你的70/30分割去除了分布的尾部以外的所有内容。信息损失可能导致你的回归模型看到的不是钟形曲线，而是直线或小的多项式曲线，并学习到错误的模型——或者更糟糕的是，学习到错误数据的正确模型，例如图4.2中的图表。在这里，训练/测试分割没有意义。
- en: Rejoice! You’ve trained a real-world regressor that has 99% accuracy on the
    available data and reasonable error. You’ve helped 311 develop a seasonally accurate
    predictor for its call volume, helped it predict how many call agents it will
    need to answer the phones, and justified its value to the community by how many
    calls it takes each week. Was this task something that you thought machine learning
    and TensorFlow could help you with? They could, and they did.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 欢呼！你已经训练了一个真实世界的回归器，它在可用数据上达到了99%的准确率和合理的误差。你帮助311开发了一个季节性准确的呼叫量预测器，帮助它预测需要多少呼叫代理来接听电话，并通过每周处理的通话数量证明了其对社区的价值。这个任务是你认为机器学习和TensorFlow可以帮助你完成的吗？它们可以，并且做到了。
- en: In chapter 5, we delve into developing powerful predictive capabilities on discrete
    outputs by constructing classifiers with TensorFlow. Onward!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们深入探讨如何通过使用TensorFlow构建分类器来开发对离散输出的强大预测能力。继续前进！
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Applying linear regression in TensorFlow to lines and polynomials assumes that
    all data is clean and neatly fits lines and points. This chapter shows you real-world
    data that doesn’t look like what you saw in chapter 3 and explains how to use
    TensorFlow to fit the model.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中将线性回归应用于直线和多项式假设所有数据都是干净且整齐地适合直线和点。本章向您展示了看起来不像第3章中看到的那样真实世界的数据，并解释了如何使用TensorFlow来拟合模型。
- en: Visualizing your input data points helps you select a model to fit for regression—in
    this case, the Gaussian model.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化你的输入数据点有助于你选择适合回归的模型——在这种情况下，高斯模型。
- en: Learning how to evaluate your bias and error by using visualization is a key
    part of using TensorFlow and tuning your machine-learning model.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何通过可视化来评估你的偏差和误差是使用TensorFlow和调整你的机器学习模型的关键部分。
- en: Looking at regularization on this model doesn’t help fit the data better. Use
    regularization when the training step is producing parameters that are too far
    outside the bounds of your learning step.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个模型上应用正则化并不能帮助更好地拟合数据。当训练步骤产生超出学习步骤边界的参数时，使用正则化。
