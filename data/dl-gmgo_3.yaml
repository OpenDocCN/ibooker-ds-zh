- en: Appendix A. Mathematical foundations
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A. 数学基础
- en: You can’t do machine learning without math. In particular, linear algebra and
    calculus are essential. The goal of this appendix is to provide enough mathematical
    background to help you understand the code samples in the book. We don’t have
    nearly enough space to cover these massive topics thoroughly; if you want to understand
    these subjects better, we provide some suggestions for further reading.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数学就无法进行机器学习。特别是，线性代数和微积分是必不可少的。本附录的目的是提供足够的数学背景，帮助你理解书中的代码示例。我们没有足够的空间彻底覆盖这些庞大的主题；如果你想更好地理解这些主题，我们提供了一些进一步阅读的建议。
- en: If you’re already familiar with advanced machine-learning techniques, you can
    safely skip this appendix altogether.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉高级机器学习技术，你可以安全地跳过这个附录。
- en: '|  |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Further reading**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: 'In this book, we have room to cover only a few mathematical basics. If you’re
    interested in learning more about the mathematical foundations of machine learning,
    here are some suggestions:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们只能涵盖一些数学基础知识。如果你对机器学习的数学基础感兴趣，这里有一些建议：
- en: For a thorough treatment of linear algebra, we suggest Sheldon Axler’s *Linear
    Algebra Done Right* (Springer, 2015).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于线性代数的全面处理，我们建议阅读舍伦德·阿克勒的《线性代数：正确之道》（Springer, 2015）。
- en: 'For a complete and practical guide to calculus, including vector calculus,
    we like James Stewart’s *Calculus: Early Transcendentals* (Cengage Learning, 2015).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包括向量微积分在内的微积分的全面和实用指南，我们推荐詹姆斯·斯图尔特的《微积分：早期超越》（Cengage Learning, 2015）。
- en: If you’re serious about understanding the mathematical theory of how and why
    calculus works, it’s hard to beat Walter Rudin’s classic *Principles of Mathematical
    Analysis* (McGraw Hill, 1976).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你认真想要理解微积分如何以及为什么有效运用的数学理论，很难超越沃尔特·鲁宾的经典著作《数学分析原理》（McGraw Hill, 1976）。
- en: '|  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Vectors, matrices, and beyond: a linear algebra primer'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量、矩阵以及更多：线性代数入门
- en: Linear algebra provides tools for handling arrays of data known as *vectors*,
    *matrices*, and *tensors*. You can represent all of these objects in Python with
    NumPy’s `array` type.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数提供了处理称为**向量**、**矩阵**和**张量**的数据数组的工具。你可以使用NumPy的`array`类型在Python中表示所有这些对象。
- en: Linear algebra is fundamental to machine learning. This section covers only
    the most basic operations, with a focus on how to implement them in NumPy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是机器学习的基础。本节仅涵盖最基本操作，重点介绍如何在NumPy中实现它们。
- en: 'Vectors: one-dimensional data'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量：一维数据
- en: A *vector* is a one-dimensional array of numbers. The size of the array is the
    dimension of the vector. You use NumPy arrays to represent vectors in Python code.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**是一个一维数字数组。数组的大小是向量的维度。你使用NumPy数组在Python代码中表示向量。'
- en: '|  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: This isn’t the true mathematical definition of a vector, but for the purposes
    of our book, it’s close enough.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是向量的真正数学定义，但就我们本书的目的而言，已经足够接近了。
- en: '|  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'You can convert a list of numbers into a NumPy array with the `np.array` function.
    The `shape` attribute lets you check the dimension:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`np.array`函数将数字列表转换为NumPy数组。`shape`属性让你可以检查维度：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that `shape` is always a tuple; this is because arrays can be multidimensional,
    as you’ll see in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`shape`始终是一个元组；这是因为数组可以是多维的，你将在下一节中看到。
- en: 'You can access individual elements of a vector, just as if it were a Python
    list:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问向量的单个元素，就像它是一个Python列表：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Vectors support a few basic algebraic operations. You can add two vectors of
    the same dimension. The result is a third vector of the same dimension. Each element
    of the sum vector is the sum of the matching elements in the original vectors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 向量支持一些基本的代数运算。你可以添加相同维度的两个向量。结果是相同维度的第三个向量。和向量的每个元素是原始向量中匹配元素的和：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similarly, you can also multiply two vectors element-wise with the `*` operator.
    (Here, element-wise means you multiply each pair of corresponding elements separately.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你也可以使用`*`运算符逐元素乘以两个向量。（这里，逐元素意味着你分别乘以每一对对应的元素。）
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The element-wise product is also called the *Hadamard product*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级乘积也称为**Hadamard乘积**。
- en: 'You can also multiply a vector with a single float (or *scalar*). In this case,
    you multiply each value in the vector by the scalar:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将一个向量与单个浮点数（或**标量**）相乘。在这种情况下，你将向量中的每个值乘以标量：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Vectors support a third kind of multiplication, the *dot product*, or *inner
    product*. To compute the dot product, you multiply each pair of corresponding
    elements and sum the results. So the dot product of two vectors is a single float.
    The NumPy function `np.dot` calculates the dot product. In Python 3.5 and later,
    the `@` operator does the same thing. (In this book, we use `np.dot`.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 向量支持第三种乘法，即点积或内积。要计算点积，你需要将每一对对应元素相乘并求和。因此，两个向量的点积是一个单一的浮点数。NumPy函数`np.dot`计算点积。在Python
    3.5及以后的版本中，`@`运算符做同样的事情。（在这本书中，我们使用`np.dot`。）
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Matrices: two-dimensional data'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵：二维数据
- en: 'A two-dimensional array of numbers is called a *matrix*. You can also represent
    matrices with NumPy arrays. In this case, if you pass a list of lists into `np.array`,
    you get a two-dimensional matrix back:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数字构成的二维数组称为矩阵。你还可以使用NumPy数组来表示矩阵。在这种情况下，如果你将一个列表的列表传递给`np.array`，你会得到一个二维矩阵：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that the shape of a matrix is a two-element tuple: first is the number
    of rows, and second is the number of columns. You can access single elements with
    a double-subscript notation: first row, then column. Alternately, NumPy lets you
    pass in the indices in a `[row, column]` format. Both are equivalent:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，矩阵的形状是一个包含两个元素的元组：第一个是行数，第二个是列数。你可以使用双重下标符号访问单个元素：先行后列。或者，NumPy允许你以`[行, 列]`格式传递索引。两者是等效的：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can also pull out a whole row from a matrix and get a vector:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从一个矩阵中拉出一整行得到一个向量：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To pull out a column, you can use the funny-looking notation `[:, n]`. If it
    helps, think of `:` as Python’s list-slicing operator; so `[:, n]` means “get
    me all rows, but only column *n*.” Here’s an example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取一列，你可以使用看起来很奇怪的符号`[:, n]`。如果你有帮助，可以把`:`看作Python的列表切片运算符；所以`[:, n]`意味着“给我所有行，但只有列*n*。”以下是一个例子：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Just like vectors, matrices support element-wise addition, element-wise multiplication,
    and scalar multiplication:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就像向量一样，矩阵支持逐元素加法、逐元素乘法和标量乘法：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Rank 3 tensors
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 三阶张量
- en: Go is played on a grid; so are chess, checkers, and a variety of other classic
    games. Any point on the grid can contain one of a variety of different game pieces.
    How do you represent the contents of the board as a mathematical object? One solution
    is to represent the board as a stack of matrices, and each matrix is the size
    of the game board.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋是在一个网格上进行的；象棋、跳棋以及许多其他经典游戏也是如此。网格上的任何一点都可以包含多种不同的棋子。你如何将棋盘的内容表示为一个数学对象？一个解决方案是将棋盘表示为矩阵堆叠，每个矩阵的大小与棋盘相同。
- en: Each individual matrix in the stack is called a *plane,* or *channel*. Each
    channel can represent a single type of piece that can be on the game board. In
    Go, you might have one channel for black stones and a second channel for white
    stones; [figure A.1](#app01fig01) shows an example. In chess, maybe you have a
    channel for pawns, another channel for bishops, one for knights, and so forth.
    You can represent the whole stack of matrices as a single three-dimensional array;
    this is called a *rank 3 tensor*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠中的每个单独的矩阵称为*平面*或*通道*。每个通道可以表示棋盘上可以放置的单种类型的棋子。在围棋中，你可能有一个通道用于黑子，另一个通道用于白子；[图A.1](#app01fig01)显示了一个例子。在象棋中，可能有一个通道用于兵，另一个通道用于象，一个通道用于马，等等。你可以将整个矩阵堆叠表示为一个单一的三维数组；这被称为*三阶张量*。
- en: Figure A.1\. Representing a Go game board with a two-plane tensor. This is a
    5 × 5 board. You use one channel for black stones and a separate channel for white
    stones. So you use a 2 × 5 × 5 tensor to represent the board.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.1\. 使用二维张量表示围棋棋盘。这是一个5 × 5的棋盘。你用一个通道表示黑子，另一个通道表示白子。因此，你使用一个2 × 5 × 5的张量来表示棋盘。
- en: '![](Images/afig01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/afig01.jpg)'
- en: 'Another common case is representing an image. Let’s say you want to represent
    a 128 × 64 pixel image with a NumPy array. In that case, you start with a grid
    corresponding to the pixels in the image. In computer graphics, you typically
    break a color into red, green, and blue components. So you can represent that
    image with a 3 × 128 × 64 tensor: you have a red channel, a green channel, and
    a blue channel.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的情况是表示图像。假设你想用一个NumPy数组来表示一个128 × 64像素的图像。在这种情况下，你从一个与图像中的像素相对应的网格开始。在计算机图形学中，你通常将颜色分解为红色、绿色和蓝色成分。因此，你可以用一个3
    × 128 × 64的张量来表示那个图像：你有一个红色通道、一个绿色通道和一个蓝色通道。
- en: 'Once again, you can use `np.array` to construct a tensor. The shape will be
    a tuple with three components, and you can use subscripting to pull out individual
    channels:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以使用`np.array`来构建一个张量。形状将是一个包含三个组件的元组，你可以使用下标来提取单个通道：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As with vectors and matrices, tensors support element-wise addition, element-wise
    multiplication, and scalar multiplication.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与向量和矩阵一样，张量支持元素级加法、元素级乘法和标量乘法。
- en: 'If you have an 8 × 8 grid with three channels, you could represent it with
    a 3 × 8 × 8 tensor or an 8 × 8 × 3 tensor. The only difference is in the way you
    index it. When you process the tensors with library functions, you must make sure
    the functions are aware of which indexing scheme you chose. The Keras library,
    which you use for designing neural networks, calls these two options `channels_first`
    and `channels_last`. For the most part, the choice doesn’t matter: you just need
    to pick one and stick to it consistently. In this book, we use the `channels_first`
    format.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个8 × 8的网格，有三个通道，你可以用一个3 × 8 × 8的张量或一个8 × 8 × 3的张量来表示它。唯一的区别在于你索引的方式。当你用库函数处理张量时，你必须确保函数知道你选择了哪种索引方案。你用来设计神经网络的Keras库将这些两个选项称为`channels_first`和`channels_last`。在大多数情况下，选择并不重要：你只需要选择一个并始终如一地坚持。在这本书中，我们使用`channels_first`格式。
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If you need a motivation to pick a format, certain NVIDIA GPUs have special
    optimizations for the `channels_first` format.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个选择格式的动机，某些NVIDIA GPU对`channels_first`格式有特殊的优化。
- en: '|  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Rank 4 tensors
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 秩为4的张量
- en: 'In many places in the book, we use a rank 3 tensor to represent a game board.
    For efficiency, you may want to pass many game boards to a function at once. One
    solution is to pack the board tensors into a four-dimensional NumPy array: this
    is a tensor of rank 4\. You can think of this four-dimensional array as a list
    of rank 3 tensors, each of which represents a single board.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的许多地方，我们使用秩为3的张量来表示棋盘。为了效率，你可能希望一次传递多个棋盘到函数中。一个解决方案是将棋盘张量打包到一个四维NumPy数组中：这是一个秩为4的张量。你可以把这个四维数组想象成一个秩为3的张量列表，每个张量代表一个单独的棋盘。
- en: 'Matrices and vectors are just special cases of tensors: a matrix is a rank
    2 tensor, and a vector is a rank 1 tensor. And a rank 0 tensor is a plain old
    number.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵和向量只是张量的特殊情况：矩阵是一个秩为2的张量，向量是一个秩为1的张量。而秩为0的张量是一个普通的数字。
- en: Rank 4 tensors are the highest-order tensors you’ll see in this book, but NumPy
    can handle tensors of any rank. Visualizing high-dimensional tensors is hard,
    but the algebra is the same.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '| Rank 4 tensors | 是这本书中你将看到的最高阶张量，但NumPy可以处理任何阶的张量。可视化高维张量很困难，但代数是相同的。'
- en: 'Calculus in five minutes: derivatives and finding maxima'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 五分钟掌握微积分：导数和求极值
- en: In calculus, the rate of change of a function is called its *derivative*. [Table
    A.1](#app01table01) lists a few real-world examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在微积分中，函数的变化率被称为其*导数*。[表A.1](#app01table01)列出了几个现实世界的例子。
- en: Table A.1\. Examples of derivatives
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.1\. 导数的例子
- en: '| Quantity | Derivative |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 量 | 导数 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| How far you’ve traveled | How fast you moved |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 你已经走了多远 | 你移动的速度 |'
- en: '| How much water is in a tub | How fast the water drained out |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 水桶中有多少水 | 水排出的速度 |'
- en: '| How many customers you have | How many customers you gain (or lose) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 你有多少客户 | 你获得（或失去）多少客户 |'
- en: 'A derivative is not a fixed quantity: it’s another function that varies over
    time or space. On a trip in a car, you drive faster or slower at various times.
    But your speed is always connected to the distance you cover. If you had a precise
    record of where you were over time, you could go back and work out how fast you
    were traveling at any point in the trip. That is the derivative.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 导数不是一个固定量：它是一个随时间或空间变化的另一个函数。在开车旅行的过程中，你会在不同的时间开得快或慢。但你的速度始终与你覆盖的距离相关。如果你有你在一段时间内精确的位置记录，你就可以回过头来计算出你在旅途中任何一点的行驶速度。这就是导数的概念。
- en: When a function is increasing, its derivative is positive. When a function is
    decreasing, its derivative is negative. [Figure A.2](#app01fig02) illustrates
    this concept. With this knowledge, you can use the derivative to find a *local
    maximum* or a *local minimum*. Any place the derivative is positive, you can move
    to the right a little bit and find a larger value. If you go past the maximum,
    the function must now be decreasing, and its derivative is negative. In that case,
    you want to move a little bit to the left. At the local maximum, the derivative
    will be exactly zero. The logic for finding a local minimum is identical, except
    you move in the opposite direction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个函数在增加时，其导数是正的。当一个函数在减少时，其导数是负的。[图A.2](#app01fig02)说明了这个概念。有了这个知识，你可以使用导数来找到一个*局部最大值*或*局部最小值*。任何导数为正的地方，你都可以稍微向右移动一点，找到一个更大的值。如果你超过了最大值，函数现在必须是在减少，其导数是负的。在这种情况下，你想要稍微向左移动一点。在局部最大值处，导数将正好为零。寻找局部最小值的逻辑是相同的，只是你向相反的方向移动。
- en: Figure A.2\. A function and its derivative. Where the derivative is positive,
    the function is increasing. Where the derivative is negative, the function is
    decreasing. When the derivative is exactly zero, the function is at a local minimum
    or maximum. With this logic, you can use the derivative to find local minima or
    maxima.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.2\. 一个函数及其导数。导数为正的地方，函数在增加。导数为负的地方，函数在减少。当导数正好为零时，函数处于局部最小值或最大值。使用这个逻辑，你可以使用导数来找到局部最小值或最大值。
- en: '![](Images/afig02.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/afig02.jpg)'
- en: Many functions that show up in machine learning take a high-dimensional vector
    as input and compute a single number as output. You can extend the same idea to
    maximize or minimize such a function. The derivative of such a function is a vector
    of the same dimension as its input, called a *gradient*. For every element of
    the gradient, the sign tells you which direction to move that coordinate. Following
    the gradient to maximize a function is called *gradient ascent*; if you’re minimizing,
    the technique is called *gradient descent*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中出现的许多函数都接受一个高维向量作为输入，并计算出一个单一的数字作为输出。你可以将同样的想法扩展到最大化或最小化这样的函数。这种函数的导数是一个与输入相同维度的向量，称为*梯度*。对于梯度的每一个元素，其符号告诉你该向哪个方向移动该坐标。跟随梯度来最大化一个函数被称为*梯度上升*；如果你在最小化，该技术被称为*梯度下降*。
- en: In this case, it may help to imagine the function as a contoured surface. At
    any point, the gradient points to the steepest slope of the surface.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，想象函数为一个等高线图可能会有所帮助。在任何一点，梯度指向表面的最陡坡度。
- en: 'To use gradient ascent, you must have a formula for the derivative of the function
    you’re trying to maximize. Most simple algebraic functions have a known derivative;
    you can look them up in any calculus textbook. If you define a complicated function
    by chaining many simple functions together, a formula known as the *chain rule*
    describes how to calculate the derivative of the complicated function. Libraries
    like TensorFlow and Theano take advantage of the chain rule to automatically calculate
    the derivative of complicated functions. If you define a complicated function
    in Keras, you don’t need to figure out the formula for the gradient yourself:
    Keras will hand off the work to TensorFlow or Theano.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度上升，你必须有一个你试图最大化的函数的导数公式。大多数简单的代数函数都有一个已知的导数；你可以在任何微积分教科书中查找它们。如果你通过将许多简单函数链接在一起来定义一个复杂的函数，一个称为*链式法则*的公式描述了如何计算复杂函数的导数。像TensorFlow和Theano这样的库利用链式法则自动计算复杂函数的导数。如果你在Keras中定义了一个复杂的函数，你不需要自己找出梯度的公式：Keras会将这项工作转交给TensorFlow或Theano。
