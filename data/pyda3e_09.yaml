- en: 6  Data Loading, Storage, and File Formats
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 数据加载、存储和文件格式
- en: 原文：[https://wesmckinney.com/book/accessing-data](https://wesmckinney.com/book/accessing-data)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://wesmckinney.com/book/accessing-data](https://wesmckinney.com/book/accessing-data)
- en: '*This Open Access web version of *Python for Data Analysis 3rd Edition* is
    now available as a companion to the [print and digital editions](https://amzn.to/3DyLaJc).
    If you encounter any errata, [please report them here](https://oreilly.com/catalog/0636920519829/errata).
    Please note that some aspects of this site as produced by Quarto will differ from
    the formatting of the print and eBook versions from O’Reilly.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*本开放访问的网络版本*Python for Data Analysis第3版*现在作为[印刷版和数字版](https://amzn.to/3DyLaJc)的伴侣版本可用。如果您遇到任何勘误，请[在此处报告](https://oreilly.com/catalog/0636920519829/errata)。请注意，由Quarto制作的本站的某些方面将与O''Reilly的印刷版和电子书版本的格式不同。'
- en: If you find the online edition of the book useful, please consider [ordering
    a paper copy](https://amzn.to/3DyLaJc) or a [DRM-free eBook](https://www.ebooks.com/en-us/book/210644288/python-for-data-analysis/wes-mckinney/?affId=WES398681F)
    to support the author. The content from this website may not be copied or reproduced.
    The code examples are MIT licensed and can be found on GitHub or Gitee.*  *Reading
    data and making it accessible (often called *data loading*) is a necessary first
    step for using most of the tools in this book. The term *parsing* is also sometimes
    used to describe loading text data and interpreting it as tables and different
    data types. I’m going to focus on data input and output using pandas, though there
    are numerous tools in other libraries to help with reading and writing data in
    various formats.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现本书的在线版本有用，请考虑[订购纸质版](https://amzn.to/3DyLaJc)或[无DRM的电子书](https://www.ebooks.com/en-us/book/210644288/python-for-data-analysis/wes-mckinney/?affId=WES398681F)以支持作者。本网站的内容不得复制或复制。代码示例采用MIT许可证，可在GitHub或Gitee上找到。*
    * 读取数据并使其可访问（通常称为*数据加载*）是使用本书中大多数工具的必要第一步。术语*解析*有时也用于描述加载文本数据并将其解释为表格和不同数据类型。我将专注于使用pandas进行数据输入和输出，尽管其他库中有许多工具可帮助读取和写入各种格式的数据。
- en: 'Input and output typically fall into a few main categories: reading text files
    and other more efficient on-disk formats, loading data from databases, and interacting
    with network sources like web APIs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出通常分为几个主要类别：读取文本文件和其他更高效的磁盘格式、从数据库加载数据以及与网络源（如Web API）交互。
- en: 6.1 Reading and Writing Data in Text Format
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 以文本格式读取和写入数据
- en: pandas features a number of functions for reading tabular data as a DataFrame
    object. [Table 6.1](#tbl-table_parsing_functions) summarizes some of them; `pandas.read_csv`
    is one of the most frequently used in this book. We will look at binary data formats
    later in [Binary Data Formats](#io_binary).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: pandas提供了许多函数，用于将表格数据读取为DataFrame对象。[表6.1](#tbl-table_parsing_functions)总结了其中一些；`pandas.read_csv`是本书中最常用的之一。我们将在[二进制数据格式](#io_binary)中稍后查看二进制数据格式。
- en: 'Table 6.1: Text and binary data loading functions in pandas'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：pandas中的文本和二进制数据加载函数
- en: '| Function | Description |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `read_csv` | Load delimited data from a file, URL, or file-like object; use
    comma as default delimiter |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| `read_csv` | 从文件、URL或类似文件的对象中加载分隔数据；使用逗号作为默认分隔符 |'
- en: '| `read_fwf` | Read data in fixed-width column format (i.e., no delimiters)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| `read_fwf` | 以固定宽度列格式读取数据（即没有分隔符） |'
- en: '| `read_clipboard` | Variation of `read_csv` that reads data from the clipboard;
    useful for converting tables from web pages |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `read_clipboard` | 读取剪贴板中的数据的`read_csv`变体；用于将网页上的表格转换的有用工具 |'
- en: '| `read_excel` | Read tabular data from an Excel XLS or XLSX file |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `read_excel` | 从Excel XLS或XLSX文件中读取表格数据 |'
- en: '| `read_hdf` | Read HDF5 files written by pandas |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `read_hdf` | 读取pandas写入的HDF5文件 |'
- en: '| `read_html` | Read all tables found in the given HTML document |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `read_html` | 读取给定HTML文档中找到的所有表格 |'
- en: '| `read_json` | Read data from a JSON (JavaScript Object Notation) string representation,
    file, URL, or file-like object |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `read_json` | 从JSON（JavaScript对象表示）字符串表示、文件、URL或类似文件的对象中读取数据 |'
- en: '| `read_feather` | Read the Feather binary file format |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `read_feather` | 读取Feather二进制文件格式 |'
- en: '| `read_orc` | Read the Apache ORC binary file format |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `read_orc` | 读取Apache ORC二进制文件格式 |'
- en: '| `read_parquet` | Read the Apache Parquet binary file format |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `read_parquet` | 读取Apache Parquet二进制文件格式 |'
- en: '| `read_pickle` | Read an object stored by pandas using the Python pickle format
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `read_pickle` | 使用Python pickle格式读取由pandas存储的对象 |'
- en: '| `read_sas` | Read a SAS dataset stored in one of the SAS system''s custom
    storage formats |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `read_sas` | 读取存储在SAS系统的自定义存储格式之一中的SAS数据集 |'
- en: '| `read_spss` | Read a data file created by SPSS |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `read_spss` | 读取由SPSS创建的数据文件 |'
- en: '| `read_sql` | Read the results of a SQL query (using SQLAlchemy) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `read_sql` | 读取SQL查询的结果（使用SQLAlchemy） |'
- en: '| `read_sql_table` | Read a whole SQL table (using SQLAlchemy); equivalent
    to using a query that selects everything in that table using `read_sql` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `read_sql_table` | 读取整个SQL表（使用SQLAlchemy）；等同于使用选择该表中的所有内容的查询使用`read_sql`
    |'
- en: '| `read_stata` | Read a dataset from Stata file format |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `read_stata` | 从Stata文件格式中读取数据集 |'
- en: '| `read_xml` | Read a table of data from an XML file |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `read_xml` | 从XML文件中读取数据表 |'
- en: 'I’ll give an overview of the mechanics of these functions, which are meant
    to convert text data into a DataFrame. The optional arguments for these functions
    may fall into a few categories:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我将概述这些函数的机制，这些函数旨在将文本数据转换为DataFrame。这些函数的可选参数可能属于几个类别：
- en: Indexing
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 索引
- en: Can treat one or more columns as the returned DataFrame, and whether to get
    column names from the file, arguments you provide, or not at all.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将一个或多个列视为返回的DataFrame，并确定是否从文件、您提供的参数或根本不获取列名。
- en: Type inference and data conversion
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类型推断和数据转换
- en: Includes the user-defined value conversions and custom list of missing value
    markers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 包括用户定义的值转换和自定义缺失值标记列表。
- en: Date and time parsing
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 日期和时间解析
- en: Includes a combining capability, including combining date and time information
    spread over multiple columns into a single column in the result.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 包括一种组合能力，包括将分布在多个列中的日期和时间信息组合成结果中的单个列。
- en: Iterating
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代
- en: Support for iterating over chunks of very large files.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 支持迭代处理非常大文件的块。
- en: Unclean data issues
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不干净的数据问题
- en: Includes skipping rows or a footer, comments, or other minor things like numeric
    data with thousands separated by commas.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 包括跳过行或页脚、注释或其他像数字数据以逗号分隔的小事物。
- en: Because of how messy data in the real world can be, some of the data loading
    functions (especially `pandas.read_csv`) have accumulated a long list of optional
    arguments over time. It's normal to feel overwhelmed by the number of different
    parameters (`pandas.read_csv` has around 50). The online pandas documentation
    has many examples about how each of these works, so if you're struggling to read
    a particular file, there might be a similar enough example to help you find the
    right parameters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界中的数据可能会很混乱，一些数据加载函数（特别是`pandas.read_csv`）随着时间的推移积累了很长的可选参数列表。对于不同参数的数量感到不知所措是正常的（`pandas.read_csv`大约有50个）。在线pandas文档有许多关于每个参数如何工作的示例，因此如果您在阅读特定文件时感到困惑，可能会有足够相似的示例帮助您找到正确的参数。
- en: Some of these functions perform *type inference*, because the column data types
    are not part of the data format. That means you don’t necessarily have to specify
    which columns are numeric, integer, Boolean, or string. Other data formats, like
    HDF5, ORC, and Parquet, have the data type information embedded in the format.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些函数执行*类型推断*，因为列数据类型不是数据格式的一部分。这意味着您不一定需要指定哪些列是数字、整数、布尔值或字符串。其他数据格式，如HDF5、ORC和Parquet，将数据类型信息嵌入到格式中。
- en: Handling dates and other custom types can require extra effort.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 处理日期和其他自定义类型可能需要额外的努力。
- en: 'Let’s start with a small comma-separated values (CSV) text file:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个小的逗号分隔值（CSV）文本文件开始：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Note* *Here I used the Unix `cat` shell command to print the raw contents
    of the file to the screen. If you’re on Windows, you can use `type` instead of
    `cat` to achieve the same effect within a Windows terminal (or command line).*  *Since
    this is comma-delimited, we can then use `pandas.read_csv` to read it into a DataFrame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *这里我使用了Unix的`cat` shell命令将文件的原始内容打印到屏幕上。如果您使用Windows，可以在Windows终端（或命令行）中使用`type`代替`cat`来实现相同的效果。*
    *由于这是逗号分隔的，我们可以使用`pandas.read_csv`将其读入DataFrame：'
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A file will not always have a header row. Consider this file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 文件不总是有标题行。考虑这个文件：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To read this file, you have a couple of options. You can allow pandas to assign
    default column names, or you can specify names yourself:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取此文件，您有几个选项。您可以允许pandas分配默认列名，或者您可以自己指定名称：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Suppose you wanted the `message` column to be the index of the returned DataFrame.
    You can either indicate you want the column at index 4 or named `"message"` using
    the `index_col` argument:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望`message`列成为返回的DataFrame的索引。您可以使用`index_col`参数指示您希望在索引4处或使用名称`"message"`：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you want to form a hierarchical index (discussed in [Ch 8.1: Hierarchical
    Indexing](/book/data-wrangling#pandas_hierarchical)) from multiple columns, pass
    a list of column numbers or names:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要从多个列创建分层索引（在[Ch 8.1：分层索引](/book/data-wrangling#pandas_hierarchical)中讨论），请传递列编号或名称的列表：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In some cases, a table might not have a fixed delimiter, using whitespace or
    some other pattern to separate fields. Consider a text file that looks like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，表格可能没有固定的分隔符，而是使用空格或其他模式来分隔字段。考虑一个看起来像这样的文本文件：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'While you could do some munging by hand, the fields here are separated by a
    variable amount of whitespace. In these cases, you can pass a regular expression
    as a delimiter for `pandas.read_csv`. This can be expressed by the regular expression
    `\s+`, so we have then:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以手动进行一些数据处理，但这里的字段是由可变数量的空格分隔的。在这些情况下，您可以将正则表达式作为`pandas.read_csv`的分隔符传递。这可以通过正则表达式`\s+`表示，因此我们有：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because there was one fewer column name than the number of data rows, `pandas.read_csv`
    infers that the first column should be the DataFrame’s index in this special case.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列名比数据行数少一个，`pandas.read_csv`推断在这种特殊情况下第一列应该是DataFrame的索引。
- en: 'The file parsing functions have many additional arguments to help you handle
    the wide variety of exception file formats that occur (see a partial listing in
    [Table 6.2](#tbl-table_read_csv_function)). For example, you can skip the first,
    third, and fourth rows of a file with `skiprows`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文件解析函数有许多额外的参数，可帮助您处理发生的各种异常文件格式（请参见[表6.2](#tbl-table_read_csv_function)中的部分列表）。例如，您可以使用`skiprows`跳过文件的第一、第三和第四行：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Handling missing values is an important and frequently nuanced part of the
    file reading process. Missing data is usually either not present (empty string)
    or marked by some *sentinel* (placeholder) value. By default, pandas uses a set
    of commonly occurring sentinels, such as `NA` and `NULL`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值是文件读取过程中重要且经常微妙的部分。缺失数据通常要么不存在（空字符串），要么由某个*标记*（占位符）值标记。默认情况下，pandas使用一组常见的标记，例如`NA`和`NULL`：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Recall that pandas outputs missing values as `NaN`, so we have two null or
    missing values in `result`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，pandas将缺失值输出为`NaN`，因此在`result`中有两个空值或缺失值：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `na_values` option accepts a sequence of strings to add to the default
    list of strings recognized as missing:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`na_values`选项接受一个字符串序列，用于添加到默认识别为缺失的字符串列表中：'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`pandas.read_csv` has a list of many default NA value representations, but
    these defaults can be disabled with the `keep_default_na` option:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_csv`有许多默认的NA值表示列表，但这些默认值可以通过`keep_default_na`选项禁用：'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Different NA sentinels can be specified for each column in a dictionary:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在字典中为每列指定不同的NA标记：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Table 6.2](#tbl-table_read_csv_function) lists some frequently used options
    in `pandas.read_csv`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.2](#tbl-table_read_csv_function)列出了`pandas.read_csv`中一些经常使用的选项。'
- en: 'Table 6.2: Some `pandas.read_csv` function arguments'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：一些`pandas.read_csv`函数参数
- en: '| Argument | Description |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `path` | String indicating filesystem location, URL, or file-like object.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `path` | 指示文件系统位置、URL或类似文件的字符串。 |'
- en: '| `sep` or `delimiter` | Character sequence or regular expression to use to
    split fields in each row. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `sep`或`delimiter` | 用于在每行中拆分字段的字符序列或正则表达式。 |'
- en: '| `header` | Row number to use as column names; defaults to 0 (first row),
    but should be `None` if there is no header row. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `header` | 用作列名的行号；默认为0（第一行），但如果没有标题行，则应为`None`。 |'
- en: '| `index_col` | Column numbers or names to use as the row index in the result;
    can be a single name/number or a list of them for a hierarchical index. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `index_col` | 用作结果中行索引的列号或名称；可以是单个名称/编号或用于分层索引的列表。 |'
- en: '| `names` | List of column names for result. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `names` | 结果的列名列表。 |'
- en: '| `skiprows` | Number of rows at beginning of file to ignore or list of row
    numbers (starting from 0) to skip. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `skiprows` | 要忽略的文件开头的行数或要跳过的行号列表（从0开始）。 |'
- en: '| `na_values` | Sequence of values to replace with NA. They are added to the
    default list unless `keep_default_na=False` is passed. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `na_values` | 要替换为NA的值序列。除非传递`keep_default_na=False`，否则它们将添加到默认列表中。 |'
- en: '| `keep_default_na` | Whether to use the default NA value list or not (`True`
    by default). |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `keep_default_na` | 是否使用默认的NA值列表（默认为`True`）。 |'
- en: '| `comment` | Character(s) to split comments off the end of lines. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `comment` | 用于将注释从行末分隔出来的字符。 |'
- en: '| `parse_dates` | Attempt to parse data to `datetime`; `False` by default.
    If `True`, will attempt to parse all columns. Otherwise, can specify a list of
    column numbers or names to parse. If element of list is tuple or list, will combine
    multiple columns together and parse to date (e.g., if date/time split across two
    columns). |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `parse_dates` | 尝试解析数据为`datetime`；默认为`False`。如果为`True`，将尝试解析所有列。否则，可以指定要解析的列号或名称的列表。如果列表的元素是元组或列表，则将多个列组合在一起并解析为日期（例如，如果日期/时间跨越两列）。
    |'
- en: '| `keep_date_col` | If joining columns to parse date, keep the joined columns;
    `False` by default. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `keep_date_col` | 如果连接列以解析日期，则保留连接的列；默认为`False`。 |'
- en: '| `converters` | Dictionary containing column number or name mapping to functions
    (e.g., `{"foo": f}` would apply the function `f` to all values in the `"foo"`
    column). |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `converters` | 包含列号或名称映射到函数的字典（例如，`{"foo": f}`将对`"foo"`列中的所有值应用函数`f`）。 |'
- en: '| `dayfirst` | When parsing potentially ambiguous dates, treat as international
    format (e.g., 7/6/2012 -> June 7, 2012); `False` by default. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `dayfirst` | 在解析可能模糊的日期时，将其视为国际格式（例如，7/6/2012 -> 2012年6月7日）；默认为`False`。 |'
- en: '| `date_parser` | Function to use to parse dates. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `date_parser` | 用于解析日期的函数。 |'
- en: '| `nrows` | Number of rows to read from beginning of file (not counting the
    header). |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `nrows` | 从文件开头读取的行数（不包括标题）。 |'
- en: '| `iterator` | Return a `TextFileReader` object for reading the file piecemeal.
    This object can also be used with the `with` statement. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `iterator` | 返回一个用于逐步读取文件的`TextFileReader`对象。此对象也可以与`with`语句一起使用。 |'
- en: '| `chunksize` | For iteration, size of file chunks. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `chunksize` | 用于迭代的文件块的大小。 |'
- en: '| `skip_footer` | Number of lines to ignore at end of file. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `skip_footer` | 要忽略的文件末尾行数。 |'
- en: '| `verbose` | Print various parsing information, like the time spent in each
    stage of the file conversion and memory use information. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `verbose` | 打印各种解析信息，如文件转换各阶段所花费的时间和内存使用信息。 |'
- en: '| `encoding` | Text encoding (e.g., `"utf-8` for UTF-8 encoded text). Defaults
    to `"utf-8"` if `None`. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `encoding` | 文本编码（例如，UTF-8编码文本的`"utf-8"`）。如果为`None`，默认为`"utf-8"`。 |'
- en: '| `squeeze` | If the parsed data contains only one column, return a Series.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `squeeze` | 如果解析的数据只包含一列，则返回一个Series。 |'
- en: '| `thousands` | Separator for thousands (e.g., `","` or `"."`); default is
    `None`. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `thousands` | 千位分隔符（例如，`","`或`"."`）；默认为`None`。 |'
- en: '| `decimal` | Decimal separator in numbers (e.g., `"."` or `","`); default
    is `"."`. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `decimal` | 数字中的小数分隔符（例如，`"."`或`","`）；默认为`"."`。 |'
- en: '| `engine` | CSV parsing and conversion engine to use; can be one of `"c"`,
    `"python"`, or `"pyarrow"`. The default is `"c"`, though the newer `"pyarrow"`
    engine can parse some files much faster. The `"python"` engine is slower but supports
    some features that the other engines do not. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `engine` | 要使用的CSV解析和转换引擎；可以是`"c"`、`"python"`或`"pyarrow"`之一。默认为`"c"`，尽管较新的`"pyarrow"`引擎可以更快地解析一些文件。`"python"`引擎速度较慢，但支持其他引擎不支持的一些功能。
    |'
- en: Reading Text Files in Pieces
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块读取文本文件
- en: When processing very large files or figuring out the right set of arguments
    to correctly process a large file, you may want to read only a small piece of
    a file or iterate through smaller chunks of the file.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理非常大的文件或找出正确的参数集以正确处理大文件时，您可能只想读取文件的一小部分或迭代文件的较小块。
- en: 'Before we look at a large file, we make the pandas display settings more compact:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看大文件之前，我们将pandas显示设置更加紧凑：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we have:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The elipsis marks `...` indicate that rows in the middle of the DataFrame have
    been omitted.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 省略号`...`表示已省略数据框中间的行。
- en: 'If you want to read only a small number of rows (avoiding reading the entire
    file), specify that with `nrows`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想读取少量行（避免读取整个文件），请使用`nrows`指定：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To read a file in pieces, specify a `chunksize` as a number of rows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要分块读取文件，指定一个作为行数的`chunksize`：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `TextFileReader` object returned by `pandas.read_csv` allows you to iterate
    over the parts of the file according to the `chunksize`. For example, we can iterate
    over `ex6.csv`, aggregating the value counts in the `"key"` column, like so:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由`pandas.read_csv`返回的`TextFileReader`对象允许您根据`chunksize`迭代文件的部分。例如，我们可以迭代`ex6.csv`，聚合`"key"`列中的值计数，如下所示：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We have then:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`TextFileReader` is also equipped with a `get_chunk` method that enables you
    to read pieces of an arbitrary size.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextFileReader`还配备有一个`get_chunk`方法，使您能够以任意大小读取文件的片段。'
- en: Writing Data to Text Format
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据写入文本格式
- en: 'Data can also be exported to a delimited format. Let’s consider one of the
    CSV files read before:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据也可以导出为分隔格式。让我们考虑之前读取的一个CSV文件：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using DataFrame’s `to_csv` method, we can write the data out to a comma-separated
    file:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataFrame的 `to_csv` 方法，我们可以将数据写入逗号分隔的文件：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Other delimiters can be used, of course (writing to `sys.stdout` so it prints
    the text result to the console rather than a file):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当然也可以使用其他分隔符（写入到 `sys.stdout` 以便将文本结果打印到控制台而不是文件）：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Missing values appear as empty strings in the output. You might want to denote
    them by some other sentinel value:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值在输出中显示为空字符串。您可能希望用其他标记值来表示它们：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With no other options specified, both the row and column labels are written.
    Both of these can be disabled:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定其他选项，则将同时写入行标签和列标签。这两者都可以禁用：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can also write only a subset of the columns, and in an order of your choosing:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以仅写入列的子集，并按您选择的顺序进行写入：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Working with Other Delimited Formats
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理其他分隔格式
- en: 'It''s possible to load most forms of tabular data from disk using functions
    like `pandas.read_csv`. In some cases, however, some manual processing may be
    necessary. It’s not uncommon to receive a file with one or more malformed lines
    that trip up `pandas.read_csv`. To illustrate the basic tools, consider a small
    CSV file:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用函数如 `pandas.read_csv` 可以从磁盘加载大多数形式的表格数据。然而，在某些情况下，可能需要一些手动处理。接收到一个或多个格式错误的行可能会导致
    `pandas.read_csv` 出错。为了说明基本工具，考虑一个小的CSV文件：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For any file with a single-character delimiter, you can use Python’s built-in
    `csv` module. To use it, pass any open file or file-like object to `csv.reader`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何具有单字符分隔符的文件，您可以使用Python的内置 `csv` 模块。要使用它，将任何打开的文件或类似文件的对象传递给 `csv.reader`：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Iterating through the reader like a file yields lists of values with any quote
    characters removed:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 像处理文件一样迭代读取器会产生去除任何引号字符的值列表：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'From there, it’s up to you to do the wrangling necessary to put the data in
    the form that you need. Let''s take this step by step. First, we read the file
    into a list of lines:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要进行必要的整理以将数据放入所需的形式。让我们一步一步来。首先，我们将文件读取为行列表：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we split the lines into the header line and the data lines:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将行分割为标题行和数据行：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then we can create a dictionary of data columns using a dictionary comprehension
    and the expression `zip(*values)` (beware that this will use a lot of memory on
    large files), which transposes rows to columns:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用字典推导和表达式 `zip(*values)` 创建数据列的字典（请注意，这将在大文件上使用大量内存），将行转置为列：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'CSV files come in many different flavors. To define a new format with a different
    delimiter, string quoting convention, or line terminator, we could define a simple
    subclass of `csv.Dialect`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件有许多不同的风格。要定义一个具有不同分隔符、字符串引用约定或行终止符的新格式，我们可以定义一个简单的 `csv.Dialect` 的子类：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We could also give individual CSV dialect parameters as keywords to `csv.reader`
    without having to define a subclass:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将单独的CSV方言参数作为关键字传递给 `csv.reader`，而无需定义子类：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The possible options (attributes of `csv.Dialect`) and what they do can be found
    in [Table 6.3](#tbl-table_csv_dialect).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的选项（`csv.Dialect` 的属性）及其作用可以在 [表 6.3](#tbl-table_csv_dialect) 中找到。
- en: 'Table 6.3: CSV `dialect` options'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6.3: CSV `dialect` 选项'
- en: '| Argument | Description |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `delimiter` | One-character string to separate fields; defaults to `","`.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `delimiter` | 用于分隔字段的单字符字符串；默认为 `","`。 |'
- en: '| `lineterminator` | Line terminator for writing; defaults to `"\r\n"`. Reader
    ignores this and recognizes cross-platform line terminators. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `lineterminator` | 用于写入的行终止符；默认为 `"\r\n"`。读取器会忽略这个并识别跨平台的行终止符。 |'
- en: '| `quotechar` | Quote character for fields with special characters (like a
    delimiter); default is `''"''`. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `quotechar` | 用于具有特殊字符（如分隔符）的字段的引用字符；默认为 `''"''`。 |'
- en: '| `quoting` | Quoting convention. Options include `csv.QUOTE_ALL` (quote all
    fields), `csv.QUOTE_MINIMAL` (only fields with special characters like the delimiter),
    `csv.QUOTE_NONNUMERIC`, and `csv.QUOTE_NONE` (no quoting). See Python’s documentation
    for full details. Defaults to `QUOTE_MINIMAL`. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `quoting` | 引用约定。选项包括 `csv.QUOTE_ALL`（引用所有字段）、`csv.QUOTE_MINIMAL`（只有包含特殊字符如分隔符的字段）、`csv.QUOTE_NONNUMERIC`
    和 `csv.QUOTE_NONE`（不引用）。详细信息请参阅Python的文档。默认为 `QUOTE_MINIMAL`。 |'
- en: '| `skipinitialspace` | Ignore whitespace after each delimiter; default is `False`.
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `skipinitialspace` | 忽略每个分隔符后的空格；默认为 `False`。 |'
- en: '| `doublequote` | How to handle quoting character inside a field; if `True`,
    it is doubled (see online documentation for full detail and behavior). |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `doublequote` | 如何处理字段内的引用字符；如果为 `True`，则会加倍（请查看在线文档以获取完整的详细信息和行为）。 |'
- en: '| `escapechar` | String to escape the delimiter if `quoting` is set to `csv.QUOTE_NONE`;
    disabled by default. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `escapechar` | 如果 `quoting` 设置为 `csv.QUOTE_NONE`，用于转义分隔符的字符串；默认情况下禁用。 |'
- en: '*Note* *For files with more complicated or fixed multicharacter delimiters,
    you will not be able to use the `csv` module. In those cases, you’ll have to do
    the line splitting and other cleanup using the string’s `split` method or the
    regular expression method `re.split`. Thankfully, `pandas.read_csv` is capable
    of doing almost anything you need if you pass the necessary options, so you only
    rarely will have to parse files by hand.*  *To *write* delimited files manually,
    you can use `csv.writer`. It accepts an open, writable file object and the same
    dialect and format options as `csv.reader`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *对于具有更复杂或固定多字符分隔符的文件，您将无法使用 `csv` 模块。在这些情况下，您将需要使用字符串的 `split` 方法或正则表达式方法
    `re.split` 进行行分割和其他清理。幸运的是，如果传递必要的选项，`pandas.read_csv` 能够几乎做任何您需要的事情，因此您很少需要手动解析文件。*  *要
    *手动* 写入分隔文件，可以使用 `csv.writer`。它接受一个打开的可写文件对象以及与 `csv.reader` 相同的方言和格式选项：'
- en: '[PRE35]*  *### JSON Data'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE35]*  *### JSON 数据'
- en: 'JSON (short for JavaScript Object Notation) has become one of the standard
    formats for sending data by HTTP request between web browsers and other applications.
    It is a much more free-form data format than a tabular text form like CSV. Here
    is an example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: JSON（JavaScript对象表示法的缩写）已经成为在Web浏览器和其他应用程序之间通过HTTP请求发送数据的标准格式之一。它是比CSV等表格文本形式更自由的数据格式。这里是一个例子：
- en: '[PRE36]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'JSON is very nearly valid Python code with the exception of its null value
    `null` and some other nuances (such as disallowing trailing commas at the end
    of lists). The basic types are objects (dictionaries), arrays (lists), strings,
    numbers, Booleans, and nulls. All of the keys in an object must be strings. There
    are several Python libraries for reading and writing JSON data. I’ll use `json`
    here, as it is built into the Python standard library. To convert a JSON string
    to Python form, use `json.loads`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: JSON几乎是有效的Python代码，只是其空值`null`和一些其他细微差别（例如不允许在列表末尾使用逗号）。基本类型是对象（字典）、数组（列表）、字符串、数字、布尔值和空值。对象中的所有键都必须是字符串。有几个Python库可用于读取和写入JSON数据。我将在这里使用`json`，因为它内置在Python标准库中。要将JSON字符串转换为Python形式，请使用`json.loads`：
- en: '[PRE37]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`json.dumps`, on the other hand, converts a Python object back to JSON:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`json.dumps`，另一方面，将Python对象转换回JSON：'
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'How you convert a JSON object or list of objects to a DataFrame or some other
    data structure for analysis will be up to you. Conveniently, you can pass a list
    of dictionaries (which were previously JSON objects) to the DataFrame constructor
    and select a subset of the data fields:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将JSON对象或对象列表转换为DataFrame或其他数据结构以进行分析将取决于您。方便的是，您可以将字典列表（先前是JSON对象）传递给DataFrame构造函数并选择数据字段的子集：
- en: '[PRE39]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `pandas.read_json` can automatically convert JSON datasets in specific
    arrangements into a Series or DataFrame. For example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_json`可以自动将特定排列的JSON数据集转换为Series或DataFrame。例如：'
- en: '[PRE40]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The default options for `pandas.read_json` assume that each object in the JSON
    array is a row in the table:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_json`的默认选项假定JSON数组中的每个对象是表中的一行：'
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'For an extended example of reading and manipulating JSON data (including nested
    records), see the USDA food database example in [Ch 13: Data Analysis Examples](#data-analysis-examples).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有关阅读和操作JSON数据的扩展示例（包括嵌套记录），请参见[第13章：数据分析示例](#data-analysis-examples)中的美国农业部食品数据库示例。
- en: 'If you need to export data from pandas to JSON, one way is to use the `to_json`
    methods on Series and DataFrame:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将数据从pandas导出为JSON，一种方法是在Series和DataFrame上使用`to_json`方法：
- en: '[PRE42]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'XML and HTML: Web Scraping'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XML和HTML：网络抓取
- en: Python has many libraries for reading and writing data in the ubiquitous HTML
    and XML formats. Examples include lxml, Beautiful Soup, and html5lib. While lxml
    is comparatively much faster in general, the other libraries can better handle
    malformed HTML or XML files.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Python有许多用于读取和写入HTML和XML格式数据的库。示例包括lxml、Beautiful Soup和html5lib。虽然lxml通常在一般情况下更快，但其他库可以更好地处理格式不正确的HTML或XML文件。
- en: 'pandas has a built-in function, `pandas.read_html`, which uses all of these
    libraries to automatically parse tables out of HTML files as DataFrame objects.
    To show how this works, I downloaded an HTML file (used in the pandas documentation)
    from the US FDIC showing bank failures.[¹](#fn1) First, you must install some
    additional libraries used by `read_html`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: pandas有一个内置函数`pandas.read_html`，它使用所有这些库自动将HTML文件中的表格解析为DataFrame对象。为了展示这是如何工作的，我下载了一个HTML文件（在pandas文档中使用）从美国联邦存款保险公司显示银行倒闭。[¹](#fn1)首先，您必须安装一些`read_html`使用的附加库：
- en: '[PRE43]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If you are not using conda, `pip install lxml` should also work.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有使用conda，`pip install lxml`也应该可以工作。
- en: 'The `pandas.read_html` function has a number of options, but by default it
    searches for and attempts to parse all tabular data contained within `<table>`
    tags. The result is a list of DataFrame objects:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_html`函数有许多选项，但默认情况下它会搜索并尝试解析包含在`<table>`标签中的所有表格数据。结果是一个DataFrame对象的列表：'
- en: '[PRE44]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Because `failures` has many columns, pandas inserts a line break character `\`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`failures`有许多列，pandas会插入一个换行符`\`。
- en: 'As you will learn in later chapters, from here we could proceed to do some
    data cleaning and analysis, like computing the number of bank failures by year:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在后面的章节中了解到的那样，从这里我们可以继续进行一些数据清理和分析，比如计算每年的银行倒闭次数：
- en: '[PRE45]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parsing XML with lxml.objectify
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用`lxml.objectify`解析XML
- en: XML is another common structured data format supporting hierarchical, nested
    data with metadata. The book you are currently reading was actually created from
    a series of large XML documents.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: XML是另一种常见的结构化数据格式，支持具有元数据的分层嵌套数据。您当前正在阅读的书实际上是从一系列大型XML文档创建的。
- en: Earlier, I showed the `pandas.read_html` function, which uses either lxml or
    Beautiful Soup under the hood to parse data from HTML. XML and HTML are structurally
    similar, but XML is more general. Here, I will show an example of how to use lxml
    to parse data from a more general XML format.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我展示了`pandas.read_html`函数，它在底层使用lxml或Beautiful Soup来解析HTML中的数据。XML和HTML在结构上相似，但XML更通用。在这里，我将展示如何使用lxml来解析更一般的XML格式中的数据的示例。
- en: 'For many years, the New York Metropolitan Transportation Authority (MTA) published
    a number of data series about its bus and train services in XML format. Here we’ll
    look at the performance data, which is contained in a set of XML files. Each train
    or bus service has a different file (like *Performance_MNR.xml* for the Metro-North
    Railroad) containing monthly data as a series of XML records that look like this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，纽约大都会交通管理局（MTA）以XML格式发布了许多关于其公交车和火车服务的数据系列。在这里，我们将查看性能数据，这些数据包含在一组XML文件中。每个火车或公交车服务都有一个不同的文件（例如*Performance_MNR.xml*用于Metro-North
    Railroad），其中包含作为一系列XML记录的月度数据，看起来像这样：
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using `lxml.objectify`, we parse the file and get a reference to the root node
    of the XML file with `getroot`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`lxml.objectify`，我们解析文件并获取XML文件的根节点的引用：
- en: '[PRE47]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`root.INDICATOR` returns a generator yielding each `<INDICATOR>` XML element.
    For each record, we can populate a dictionary of tag names (like `YTD_ACTUAL`)
    to data values (excluding a few tags) by running the following code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`root.INDICATOR`返回一个生成器，产生每个`<INDICATOR>` XML元素。对于每条记录，我们可以通过运行以下代码填充一个标签名称（如`YTD_ACTUAL`）到数据值（排除一些标签）的字典：'
- en: '[PRE48]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Lastly, convert this list of dictionaries into a DataFrame:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将这个字典列表转换为DataFrame：
- en: '[PRE49]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'pandas''s `pandas.read_xml` function turns this process into a one-line expression:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的`pandas.read_xml`函数将此过程转换为一行表达式：
- en: '[PRE50]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: For more complex XML documents, refer to the docstring for `pandas.read_xml`
    which describes how to do selections and filters to extract a particular table
    of interest.**  **## 6.2 Binary Data Formats
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的XML文档，请参考`pandas.read_xml`的文档字符串，其中描述了如何进行选择和过滤以提取感兴趣的特定表格。**  **## 6.2
    二进制数据格式
- en: 'One simple way to store (or *serialize*) data in binary format is using Python’s
    built-in `pickle` module. pandas objects all have a `to_pickle` method that writes
    the data to disk in pickle format:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以二进制格式存储（或*序列化*）数据的一种简单方法是使用Python的内置`pickle`模块。所有pandas对象都有一个`to_pickle`方法，它以pickle格式将数据写入磁盘：
- en: '[PRE51]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Pickle files are in general readable only in Python. You can read any "pickled"
    object stored in a file by using the built-in `pickle` directly, or even more
    conveniently using `pandas.read_pickle`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle文件通常只能在Python中读取。您可以直接使用内置的`pickle`读取存储在文件中的任何“pickled”对象，或者更方便地使用`pandas.read_pickle`：
- en: '[PRE52]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '*Caution* *`pickle` is recommended only as a short-term storage format. The
    problem is that it is hard to guarantee that the format will be stable over time;
    an object pickled today may not unpickle with a later version of a library. pandas
    has tried to maintain backward compatibility when possible, but at some point
    in the future it may be necessary to “break” the pickle format.*  *pandas has
    built-in support for several other open source binary data formats, such as HDF5,
    ORC, and Apache Parquet. For example, if you install the `pyarrow` package (`conda
    install pyarrow`), then you can read Parquet files with `pandas.read_parquet`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *`pickle`仅建议作为短期存储格式。问题在于很难保证格式随时间稳定；今天使用pickle的对象可能无法在以后的库版本中解除pickle。pandas在可能的情况下尽力保持向后兼容性，但在将来的某个时候可能需要“破坏”pickle格式。*
    *pandas内置支持其他几种开源二进制数据格式，例如HDF5、ORC和Apache Parquet。例如，如果安装`pyarrow`包（`conda install
    pyarrow`），则可以使用`pandas.read_parquet`读取Parquet文件：'
- en: '[PRE53]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: I will give some HDF5 examples in [Using HDF5 Format](#io_hdf5). I encourage
    you to explore different file formats to see how fast they are and how well they
    work for your analysis.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在[HDF5格式使用](#io_hdf5)中给出一些HDF5示例。我鼓励您探索不同的文件格式，看看它们的速度和对您的分析工作的适用性。
- en: Reading Microsoft Excel Files
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取Microsoft Excel文件
- en: 'pandas also supports reading tabular data stored in Excel 2003 (and higher)
    files using either the `pandas.ExcelFile` class or `pandas.read_excel` function.
    Internally, these tools use the add-on packages `xlrd` and `openpyxl` to read
    old-style XLS and newer XLSX files, respectively. These must be installed separately
    from pandas using pip or conda:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: pandas还支持使用`pandas.ExcelFile`类或`pandas.read_excel`函数读取存储在Excel 2003（及更高版本）文件中的表格数据。在内部，这些工具使用附加包`xlrd`和`openpyxl`来分别读取旧式XLS和新式XLSX文件。这些必须使用pip或conda单独安装，而不是从pandas安装：
- en: '[PRE54]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'To use `pandas.ExcelFile`, create an instance by passing a path to an `xls`
    or `xlsx` file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`pandas.ExcelFile`，请通过传递路径到`xls`或`xlsx`文件来创建一个实例：
- en: '[PRE55]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This object can show you the list of available sheet names in the file:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象可以显示文件中可用工作表名称的列表：
- en: '[PRE56]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Data stored in a sheet can then be read into DataFrame with `parse`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`parse`将工作表中存储的数据读入DataFrame：
- en: '[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This Excel table has an index column, so we can indicate that with the `index_col`
    argument:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此Excel表具有索引列，因此我们可以使用`index_col`参数指示：
- en: '[PRE58]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'If you are reading multiple sheets in a file, then it is faster to create the
    `pandas.ExcelFile`, but you can also simply pass the filename to `pandas.read_excel`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要在一个文件中读取多个工作表，则创建`pandas.ExcelFile`会更快，但您也可以简单地将文件名传递给`pandas.read_excel`：
- en: '[PRE59]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To write pandas data to Excel format, you must first create an `ExcelWriter`,
    then write data to it using the pandas object''s `to_excel` method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要将pandas数据写入Excel格式，必须首先创建一个`ExcelWriter`，然后使用pandas对象的`to_excel`方法将数据写入其中：
- en: '[PRE60]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You can also pass a file path to `to_excel` and avoid the `ExcelWriter`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将文件路径传递给`to_excel`，避免使用`ExcelWriter`：
- en: '[PRE61]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Using HDF5 Format
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用HDF5格式
- en: HDF5 is a respected file format intended for storing large quantities of scientific
    array data. It is available as a C library, and it has interfaces available in
    many other languages, including Java, Julia, MATLAB, and Python. The “HDF” in
    HDF5 stands for *hierarchical data format*. Each HDF5 file can store multiple
    datasets and supporting metadata. Compared with simpler formats, HDF5 supports
    on-the-fly compression with a variety of compression modes, enabling data with
    repeated patterns to be stored more efficiently. HDF5 can be a good choice for
    working with datasets that don't fit into memory, as you can efficiently read
    and write small sections of much larger arrays.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: HDF5是一种受尊敬的文件格式，用于存储大量科学数组数据。它作为一个C库可用，并且在许多其他语言中都有接口，包括Java、Julia、MATLAB和Python。HDF5中的“HDF”代表*分层数据格式*。每个HDF5文件可以存储多个数据集和支持的元数据。与更简单的格式相比，HDF5支持各种压缩模式的即时压缩，使具有重复模式的数据能够更有效地存储。HDF5可以是处理不适合内存的数据集的良好选择，因为您可以有效地读取和写入更大数组的小部分。
- en: 'To get started with HDF5 and pandas, you must first install PyTables by installing
    the `tables` package with conda:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用HDF5和pandas，您必须首先通过使用conda安装`tables`包来安装PyTables：
- en: '[PRE62]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '*Note* *Note that the PyTables package is called "tables" in PyPI, so if you
    install with pip you will have to run `pip install tables`.*  *While it''s possible
    to directly access HDF5 files using either the PyTables or h5py libraries, pandas
    provides a high-level interface that simplifies storing Series and DataFrame objects.
    The `HDFStore` class works like a dictionary and handles the low-level details:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *请注意，PyTables包在PyPI中称为“tables”，因此如果您使用pip安装，您将需要运行`pip install tables`。*
    *虽然可以直接使用PyTables或h5py库访问HDF5文件，但pandas提供了一个简化存储Series和DataFrame对象的高级接口。`HDFStore`类的工作方式类似于字典，并处理底层细节：'
- en: '[PRE63]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Objects contained in the HDF5 file can then be retrieved with the same dictionary-like
    API:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用相同类似字典的API检索HDF5文件中包含的对象：
- en: '[PRE64]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '`HDFStore` supports two storage schemas, `"fixed"` and `"table"` (the default
    is `"fixed"`). The latter is generally slower, but it supports query operations
    using a special syntax:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`支持两种存储模式，`"fixed"`和`"table"`（默认为`"fixed"`）。后者通常较慢，但支持使用特殊语法进行查询操作：'
- en: '[PRE65]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The `put` is an explicit version of the `store["obj2"] = frame` method but allows
    us to set other options like the storage format.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`put`是`store["obj2"] = frame`方法的显式版本，但允许我们设置其他选项，如存储格式。'
- en: 'The `pandas.read_hdf` function gives you a shortcut to these tools:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_hdf`函数为您提供了这些工具的快捷方式：'
- en: '[PRE66]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If you''d like, you can delete the HDF5 file you created, like so:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，可以删除您创建的HDF5文件，方法如下：
- en: '[PRE67]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '*Note* *If you are processing data that is stored on remote servers, like Amazon
    S3 or HDFS, using a different binary format designed for distributed storage like
    [Apache Parquet](http://parquet.apache.org) may be more suitable.*  *If you work
    with large quantities of data locally, I would encourage you to explore PyTables
    and h5py to see how they can suit your needs. Since many data analysis problems
    are I/O-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate
    your applications.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *如果您正在处理存储在远程服务器上的数据，如Amazon S3或HDFS，使用设计用于分布式存储的不同二进制格式（如[Apache Parquet](http://parquet.apache.org)）可能更合适。*  *如果您在本地处理大量数据，我建议您探索PyTables和h5py，看看它们如何满足您的需求。由于许多数据分析问题受I/O限制（而不是CPU限制），使用HDF5等工具可以大大加速您的应用程序。'
- en: '*Caution* *HDF5 is *not* a database. It is best suited for write-once, read-many
    datasets. While data can be added to a file at any time, if multiple writers do
    so simultaneously, the file can become corrupted.****  ***## 6.3 Interacting with
    Web APIs'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *HDF5不是数据库。它最适合于一次写入，多次读取的数据集。虽然数据可以随时添加到文件中，但如果多个写入者同时这样做，文件可能会损坏。****  ***##
    6.3 与Web API交互'
- en: 'Many websites have public APIs providing data feeds via JSON or some other
    format. There are a number of ways to access these APIs from Python; one method
    that I recommend is the [`requests` package](http://docs.python-requests.org),
    which can be installed with pip or conda:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站都有提供数据源的公共API，可以通过JSON或其他格式提供数据。有许多方法可以从Python访问这些API；我推荐的一种方法是[`requests`包](http://docs.python-requests.org)，可以使用pip或conda进行安装：
- en: '[PRE68]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To find the last 30 GitHub issues for pandas on GitHub, we can make a `GET`
    HTTP request using the add-on `requests` library:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要在GitHub上找到pandas的最近30个问题，我们可以使用附加的`requests`库进行`GET` HTTP请求：
- en: '[PRE69]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: It's a good practice to always call `raise_for_status` after using `requests.get`
    to check for HTTP errors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`requests.get`后，始终调用`raise_for_status`以检查HTTP错误是一个好习惯。
- en: 'The response object’s `json` method will return a Python object containing
    the parsed JSON data as a dictionary or list (depending on what JSON is returned):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 响应对象的`json`方法将返回一个包含解析后的JSON数据的Python对象，作为字典或列表（取决于返回的JSON是什么）：
- en: '[PRE70]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Since the results retrieved are based on real-time data, what you see when you
    run this code will almost definitely be different.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检索到的结果基于实时数据，当您运行此代码时，您看到的结果几乎肯定会有所不同。
- en: 'Each element in `data` is a dictionary containing all of the data found on
    a GitHub issue page (except for the comments). We can pass `data` directly to
    `pandas.DataFrame` and extract fields of interest:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`中的每个元素都是一个包含GitHub问题页面上找到的所有数据的字典（评论除外）。我们可以直接将`data`传递给`pandas.DataFrame`并提取感兴趣的字段：'
- en: '[PRE71]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: With a bit of elbow grease, you can create some higher-level interfaces to common
    web APIs that return DataFrame objects for more convenient analysis.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些努力，您可以创建一些更高级的接口，用于常见的Web API，返回DataFrame对象以便进行更方便的分析。
- en: 6.4 Interacting with Databases
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 与数据库交互
- en: In a business setting, a lot of data may not be stored in text or Excel files.
    SQL-based relational databases (such as SQL Server, PostgreSQL, and MySQL) are
    in wide use, and many alternative databases have become quite popular. The choice
    of database is usually dependent on the performance, data integrity, and scalability
    needs of an application.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中，许多数据可能不存储在文本或Excel文件中。基于SQL的关系数据库（如SQL Server、PostgreSQL和MySQL）被广泛使用，许多替代数据库也变得非常流行。数据库的选择通常取决于应用程序的性能、数据完整性和可扩展性需求。
- en: 'pandas has some functions to simplify loading the results of a SQL query into
    a DataFrame. As an example, I’ll create a SQLite3 database using Python’s built-in
    `sqlite3` driver:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: pandas有一些函数可以简化将SQL查询结果加载到DataFrame中。例如，我将使用Python内置的`sqlite3`驱动程序创建一个SQLite3数据库：
- en: '[PRE72]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Then, insert a few rows of data:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，插入一些数据行：
- en: '[PRE73]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Most Python SQL drivers return a list of tuples when selecting data from a
    table:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Python SQL驱动程序在从表中选择数据时返回一个元组列表：
- en: '[PRE74]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You can pass the list of tuples to the DataFrame constructor, but you also
    need the column names, contained in the cursor’s `description` attribute. Note
    that for SQLite3, the cursor `description` only provides column names (the other
    fields, which are part of Python''s Database API specification, are `None`), but
    for some other database drivers, more column information is provided:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将元组列表传递给DataFrame构造函数，但还需要列名，这些列名包含在游标的`description`属性中。请注意，对于SQLite3，游标的`description`仅提供列名（其他字段，这些字段是Python的数据库API规范的一部分，为`None`），但对于其他一些数据库驱动程序，提供了更多的列信息：
- en: '[PRE75]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'This is quite a bit of munging that you’d rather not repeat each time you query
    the database. The [SQLAlchemy project](http://www.sqlalchemy.org/) is a popular
    Python SQL toolkit that abstracts away many of the common differences between
    SQL databases. pandas has a `read_sql` function that enables you to read data
    easily from a general SQLAlchemy connection. You can install SQLAlchemy with conda
    like so:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相当复杂的操作，您不希望每次查询数据库时都重复。[SQLAlchemy项目](http://www.sqlalchemy.org/)是一个流行的Python
    SQL工具包，它抽象了SQL数据库之间的许多常见差异。pandas有一个`read_sql`函数，可以让您轻松地从通用的SQLAlchemy连接中读取数据。您可以像这样使用conda安装SQLAlchemy：
- en: '[PRE76]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now, we''ll connect to the same SQLite database with SQLAlchemy and read data
    from the table created before:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用SQLAlchemy连接到相同的SQLite数据库，并从之前创建的表中读取数据：
- en: '[PRE77]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 6.5 Conclusion
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 结论
- en: Getting access to data is frequently the first step in the data analysis process.
    We have looked at a number of useful tools in this chapter that should help you
    get started. In the upcoming chapters we will dig deeper into data wrangling,
    data visualization, time series analysis, and other topics.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 获取数据通常是数据分析过程中的第一步。在本章中，我们已经介绍了一些有用的工具，这些工具应该可以帮助您入门。在接下来的章节中，我们将深入探讨数据整理、数据可视化、时间序列分析等主题。
- en: '* * *'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For the full list, see [https://www.fdic.gov/bank/individual/failed/banklist.html](https://www.fdic.gov/bank/individual/failed/banklist.html).[↩︎](#fnref1)******
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完整列表请参见[https://www.fdic.gov/bank/individual/failed/banklist.html](https://www.fdic.gov/bank/individual/failed/banklist.html)。
