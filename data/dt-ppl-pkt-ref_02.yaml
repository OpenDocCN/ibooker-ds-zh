- en: Chapter 2\. A Modern Data Infrastructure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 一个现代化的数据基础设施
- en: Before deciding on products and design for building pipelines, it’s worth understanding
    what makes up a modern data stack. As with most things in technology, there’s
    no single right way to design your analytics ecosystem or choose products and
    vendors. Regardless, there are some key needs and concepts that have become industry
    standard and set the stage for best practices in implementing pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定构建管道的产品和设计之前，了解构成现代数据堆栈的内容是很重要的。就像技术领域的大多数事物一样，设计您的分析生态系统或选择产品和供应商并没有单一正确的方法。但是，已经形成了一些行业标准和概念，这些标准为实施管道的最佳实践奠定了基础。
- en: Let’s take a look at the key components of such an infrastructure as displayed
    in [Figure 2-1](#fig_0201). Future chapters explore how each component factors
    into the design and implementation of data pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这种基础设施的关键组件，如[图 2-1](#fig_0201) 所示。未来的章节将探讨每个组件如何影响数据管道的设计和实施。
- en: Diversity of Data Sources
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源的多样性
- en: The majority of organizations have dozens, if not hundreds, of data sources
    that feed their analytics endeavors. Data sources vary across many dimensions
    covered in this section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数组织在进行分析工作时，有数十甚至数百个数据源。这些数据源在本节覆盖的多个维度上有所不同。
- en: '![dppr 0201](Images/dppr_0201.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0201](Images/dppr_0201.png)'
- en: Figure 2-1\. The key components of a modern data infrastructure.
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1 一个现代化数据基础设施的关键组件。
- en: Source System Ownership
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源系统的所有权
- en: It’s typical for an analytics team to ingest data from source systems that are
    built and owned by the organization as well as from third-party tools and vendors.
    For example, an ecommerce company might store data from their shopping cart in
    a PostgreSQL (also known as Postgres) database behind their web app. They may
    also use a third-party web analytics tool such as Google Analytics to track usage
    on their website. The combination of the two data sources (illustrated in [Figure 2-2](#fig_0202))
    is required to get a full understanding of customer behavior leading up to a purchase.
    Thus, a data pipeline that ends with an analysis of such behavior starts with
    the ingestion of data from both sources.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分析团队而言，从组织构建并拥有的源系统以及从第三方工具和供应商那里摄入数据是很典型的。例如，电子商务公司可能会在其网站后台使用 PostgreSQL（也称为
    Postgres）数据库存储购物车中的数据。他们还可能使用第三方网络分析工具，如 Google Analytics，来跟踪其网站的使用情况。这两种数据源的结合（如[图
    2-2](#fig_0202) 所示）是了解客户在购买前行为的完整理解所必需的。因此，从这两个源的数据摄入开始的数据管道，最终以分析此类行为告终。
- en: '![dppr 0202](Images/dppr_0202.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0202](Images/dppr_0202.png)'
- en: Figure 2-2\. A simple pipeline with data from multiple sources loaded into an
    S3 bucket and then a Redshift database.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2 一个简单的管道示例，将多个来源的数据加载到 S3 存储桶，然后加载到 Redshift 数据库。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The term *data ingestion* refers to extracting data from one source and loading
    it into another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *数据摄入* 指的是从一个源中提取数据并加载到另一个源的过程。
- en: Understanding the ownership of source systems is important for several reasons.
    First, for third-party data sources you’re likely limited as to what data you
    can access and how you can access it. Most vendors make a REST API available,
    but few will give you direct access to your data in the form of a SQL database.
    Even fewer will give you much in the way of customization of what data you can
    access and at what level of granularity.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理解源系统的所有权对于多个原因非常重要。首先，对于第三方数据源，您可能会受到限制，不能访问所有数据或以您希望的方式访问数据。大多数供应商提供 REST
    API，但很少有供应商会直接以 SQL 数据库的形式向您提供数据访问权。甚至更少的供应商会允许您对可以访问的数据进行多少程度上的自定义或以何种粒度访问数据。
- en: Internally built systems present the analytics team with more opportunities
    to customize the data available as well as the method of access. However, they
    present other challenges as well. Were the systems built with consideration of
    data ingestion? Often the answer is no, which has implications ranging from the
    ingestion putting unintended load on the system to the inability to load data
    incrementally. If you’re lucky, the engineering team that owns the source system
    will have the time and willingness to work with you, but in the reality of resource
    constraints, you may find it’s not dissimilar to working with an external vendor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 内部构建的系统为分析团队提供了更多定制可用数据和访问方法的机会。然而，它们也带来了其他挑战。这些系统是否考虑了数据摄入？通常答案是否定的，这会导致摄入将意外负载放在系统上，或者无法增量加载数据。如果你很幸运，拥有源系统的工程团队可能会有时间和愿意与你合作，但在资源限制的现实中，你可能会发现这与与外部供应商合作并没有太大不同。
- en: Ingestion Interface and Data Structure
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄入接口和数据结构
- en: 'Regardless of who owns the source data, how you get it and in what form is
    the first thing a data engineer will examine when building a new data ingestion.
    First, what is the interface to the data? Some of the most common include the
    following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据的所有者是谁，建立新的数据摄入时，数据工程师首先会检查如何获取数据以及以何种形式获取。首先，数据的接口是什么？一些最常见的包括以下几种：
- en: A database behind an application, such as a Postgres or MySQL database
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序背后的数据库，如 Postgres 或 MySQL 数据库
- en: A layer of abstraction on top of a system such as a REST API
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在系统顶部的一个抽象层，比如 REST API
- en: A stream processing platform such as Apache Kafka
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如 Apache Kafka 的流处理平台
- en: A shared network file system or cloud storage bucket containing logs, comma-separated
    value (CSV) files, and other flat files
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含日志、逗号分隔值（CSV）文件和其他平面文件的共享网络文件系统或云存储桶
- en: A data warehouse or data lake
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库或数据湖
- en: Data in HDFS or HBase database
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS 或 HBase 数据库中的数据
- en: 'In addition to the interface, the structure of the data will vary. Here are
    some common examples:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了接口外，数据的结构会有所不同。以下是一些常见的例子：
- en: JSON from a REST API
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 REST API 的 JSON
- en: Well-structured data from a MySQL database
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 MySQL 数据库的良好结构化数据
- en: JSON within columns of a MySQL database table
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL 数据库表的列内 JSON
- en: Semistructured log data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半结构化日志数据
- en: CSV, fixed-width format (FWF), and other flat file formats
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV、固定宽度格式（FWF）和其他平面文件格式
- en: JSON in flat files
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平面文件中的 JSON
- en: Stream output from Kafka
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Kafka 的流输出
- en: Each interface and data structure presents its own challenges and opportunities.
    Well-structured data is often easiest to work with, but it’s usually structured
    in the interest of an application or website. Beyond the ingestion of the data,
    further steps in the pipeline will likely be necessary to clean and transform
    into a structure better suited for an analytics project.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个接口和数据结构都有其自己的挑战和机遇。良好结构化的数据通常最容易处理，但通常是为了应用程序或网站的利益而结构化的。除了数据的摄入外，管道中进一步的步骤可能需要进行数据清理和转换，以便更适合分析项目的结构。
- en: Semistructured data such as JSON is increasingly common and has the advantage
    of the structure of attribute-value pairs and nesting of objects. However, unlike
    a relational database, there is no guarantee that each object in the same dataset
    will have the same structure. As you’ll see later in this book, how one deals
    with missing or incomplete data in a pipeline is context dependent and increasingly
    necessary as the rigidity of the structure in data is reduced.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 JSON 的半结构化数据越来越普遍，并且具有属性-值对的结构和对象的嵌套的优势。然而，与关系数据库不同的是，同一数据集中的每个对象并不保证具有相同的结构。正如本书后文所述，如何处理数据管道中的缺失或不完整数据是依赖于上下文的，并且随着数据结构的刚性减少而变得越来越必要。
- en: Unstructured data is common for some analytics endeavors. For example, Natural
    Language Processing (NLP) models require vast amounts of free text data to train
    and validate. Computer Vision (CV) projects require images and video content.
    Even less daunting projects such as scraping data from web pages have a need for
    free text data from the web in addition to the semistructured HTML markup of a
    web page.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些分析项目来说，非结构化数据很常见。例如，自然语言处理（NLP）模型需要大量的自由文本数据来训练和验证。计算机视觉（CV）项目需要图像和视频内容。即使是从网页上抓取数据这样较不艰难的项目，也需要来自网页的自由文本数据，以及网页的半结构化
    HTML 标记。
- en: Data Volume
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据量
- en: Though data engineers and hiring managers alike enjoy bragging about petabyte-scale
    datasets, the reality is that most organizations value small datasets as much
    as large ones. In addition, it’s common to ingest and model small and large datasets
    in tandem. Though the design decisions at each step in a pipeline must take data
    volume into consideration, high volume does not mean high value.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据工程师和招聘经理都喜欢吹嘘拥有PB级数据集，但事实是大多数组织既重视小数据集也重视大数据集。此外，常见的是同时接收和建模小和大数据集。尽管在管道的每个步骤中的设计决策必须考虑数据量，但高容量并不意味着高价值。
- en: All that said, most organizations have at least one dataset that is key to both
    analytical needs as well as high volume. What’s *high volume*? There’s no easy
    definition, but as it pertains to pipelines, it’s best to think in terms of a
    spectrum rather than a binary definition of *high-* and *low-* volume datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，大多数组织至少拥有一个对分析需求和高容量至关重要的数据集。什么是*高容量*？没有简单的定义，但就管道而言，最好是以一个光谱而非二元定义来思考*高-*和*低-*容量数据集。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As you’ll see throughout this book, there’s as much danger in oversimplifying
    data ingestion and processing—with the result being long and inefficient runs—as
    there is in overengineering pipeline tasks when the volume of data or complexity
    of the task is low.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本书中看到的那样，在简化数据摄取和处理时存在过度简化的危险，结果是运行时间长且效率低下，当数据量或任务复杂性较低时，过度工程化管道任务也同样存在风险。
- en: Data Cleanliness and Validity
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的清洁度和有效性
- en: Just as there is great diversity in data sources, the quality of source data
    varies greatly. As the old saying goes, “garbage in, garbage out.” It’s important
    to understand the limitations and deficiencies of source data and address them
    in the appropriate sections of your pipelines.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就像数据源具有很大的多样性一样，数据源的质量也存在很大的差异。正如老话所说：“垃圾进，垃圾出。”理解数据源的局限性和缺陷，并在管道的适当部分加以解决是非常重要的。
- en: 'There are many common characteristics of “messy data,” including, but not limited
    to, the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: “混乱数据”的常见特征有许多，包括但不限于以下内容：
- en: Duplicate or ambiguous records
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复或模棱两可的记录
- en: Orphaned records
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孤立记录
- en: Incomplete or missing records
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不完整或缺失的记录
- en: Text encoding errors
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码错误
- en: Inconsistent formats (for example, phone numbers with or without dashes)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不一致的格式（例如，电话号码有或没有连字符）
- en: Mislabeled or unlabeled data
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误标记或未标记的数据
- en: Of course, there are numerous others, as well as data validity issues specific
    to the context of the source system.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在源系统的上下文中，还有许多其他数据有效性问题。
- en: 'There’s no magic bullet for ensuring data cleanliness and validity, but in
    a modern data ecosystem, there are key characteristics and approaches that we’ll
    see throughout this book:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 保证数据清洁和有效性并非有灵丹妙药，但在现代数据生态系统中，我们会看到本书中的关键特征和方法：
- en: Assume the worst, expect the best
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最坏的情况，期待最好的结果
- en: Pristine datasets only exist in academic literature. Assume your input datasets
    will contain numerous validity and consistency issues, but build pipelines that
    identify and cleanse data in the interest of clean output.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的数据集只存在于学术文献中。假设你的输入数据集将包含大量的有效性和一致性问题，但构建能够识别和清洗数据的管道以获取干净输出是非常重要的。
- en: Clean and validate data in the system best suited to do so
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在最适合的系统中清洁和验证数据
- en: There are times when it’s better to wait to clean data until later in a pipeline.
    For example, modern pipelines tend to follow an extract-load-transform (ELT) rather
    than extract-transform-load (ETL) approach for data warehousing (more in [Chapter 3](ch03.xhtml#ch03)).
    It’s sometimes optimal to load data into a data lake in a fairly raw form and
    to worry about structuring and cleaning later in the pipeline. In other words,
    use the right tool for the right job rather than rushing the cleaning and validation
    processes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时最好等到管道的后期再清理数据。例如，现代管道倾向于采用提取-加载-转换（ELT）而不是提取-转换-加载（ETL）的方法进行数据仓库操作（详见[第3章](ch03.xhtml#ch03)）。将数据以相对原始的形式加载到数据湖中，并在管道的后期担心结构化和清洗有时是最优选择。换句话说，用合适的工具做合适的工作，而不是匆忙进行清洗和验证过程。
- en: Validate often
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 经常验证
- en: Even if you don’t clean up data early in a pipeline, don’t wait until the end
    of the pipeline to validate it. You’ll have a much harder time determining where
    things went wrong. Conversely, don’t validate once early in a pipeline and assume
    all will go well in subsequent steps. [Chapter 8](ch08.xhtml#ch08) digs deeper
    into validation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在流水线的早期阶段不清理数据，也不要等到流水线的最后阶段才验证数据。确定问题发生的位置会更加困难。反过来，也不要在流水线早期验证一次然后假设后续步骤都会顺利进行。[第8章](ch08.xhtml#ch08)深入探讨了验证的问题。
- en: Latency and Bandwidth of the Source System
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源系统的延迟和带宽
- en: The need to frequently extract high volumes of data from source systems is a
    common use case in a modern data stack. Doing so presents challenges, however.
    Data extraction steps in pipelines must contend with API rate limits, connection
    time-outs, slow downloads, and source system owners who are unhappy due to strain
    placed on their systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 需要频繁地从源系统中提取大量数据是现代数据堆栈中的常见用例。然而，这样做也带来了挑战。在流水线中进行数据提取步骤必须应对API速率限制、连接超时、下载缓慢以及源系统所有者因其系统负担而感到不满。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As I’ll discuss in Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05) in
    more detail, data ingestion is the first step in most data pipelines. Understanding
    the characteristics of source systems and their data is thus the first step in
    designing pipelines and making decisions regarding infrastructure further downstream.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第[4](ch04.xhtml#ch04)章和第[5](ch05.xhtml#ch05)章中将详细讨论的那样，数据摄入是大多数数据管道中的第一步。因此，了解源系统及其数据的特性是设计管道并在下游基础设施方面做出决策的第一步。
- en: Cloud Data Warehouses and Data Lakes
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云数据仓库和数据湖
- en: 'Three things transformed the landscape of analytics and data warehousing over
    the last 10 years, and they’re all related to the emergence of the major public
    cloud providers (Amazon, Google, and Microsoft):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 过去10年间，有三件事情改变了分析和数据仓库领域的格局，这三件事情都与主要公共云服务提供商（亚马逊、谷歌和微软）的出现有关：
- en: The ease of building and deploying data pipelines, data lakes, warehouses, and
    analytics processing in the cloud. No more waiting on IT departments and budget
    approval for large up-front costs. Managed services—databases in particular—have
    become mainstream.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中构建和部署数据管道、数据湖、数据仓库以及分析处理变得更加简便。不再需要等待IT部门和预算批准大笔前期成本。托管服务，特别是数据库，已经成为主流。
- en: Continued drop-in storage costs in the cloud.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云端存储成本持续下降。
- en: The emergence of highly scalable, columnar databases, such as Amazon Redshift,
    Snowflake, and Google Big Query.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出现了高度可扩展的列式数据库，比如Amazon Redshift、Snowflake和Google Big Query。
- en: These changes breathed new life into data warehouses and introduced the concept
    of a data lake. Though [Chapter 5](ch05.xhtml#ch05) covers data warehouses and
    data lakes in more detail, it’s worth briefly defining both now, in order to clarify
    their place in a modern data ecosystem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化为数据仓库注入了新的活力，并引入了数据湖的概念。尽管[第5章](ch05.xhtml#ch05)详细介绍了数据仓库和数据湖，但现在简要定义两者，以便澄清它们在现代数据生态系统中的位置。
- en: A *data warehouse* is a database where data from different systems is stored
    and modeled to support analysis and other activities related to answering questions
    with it. Data in a data warehouse is structured and optimized for reporting and
    analysis queries.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据仓库*是一个数据库，用于存储和建模来自不同系统的数据，以支持与之相关的分析和其他活动。数据仓库中的数据是结构化的，并且经过优化，用于报告和分析查询。'
- en: A *data lake* is where data is stored, but without the structure or query optimization
    of a data warehouse. It will likely contain a high volume of data as well as a
    variety of data types. For example, a single data lake might contain a collection
    of blog posts stored as text files, flat file extracts from a relational database,
    and JSON objects containing events generated by sensors in an industrial system.
    It can even store structured data like a standard database, though it’s not optimized
    for querying such data in the interest of reporting and analysis.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据湖*是数据存储的地方，但没有数据仓库的结构或查询优化。它可能包含大量数据以及各种数据类型。例如，单个数据湖可能包含存储为文本文件的博客文章集合，来自关系数据库的扁平文件提取，以及工业系统传感器生成的JSON对象事件。它甚至可以存储结构化数据，如标准数据库，尽管其未优化用于报告和分析查询。'
- en: There is a place for both data warehouses and data lakes in the same data ecosystem,
    and data pipelines often move data between both.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个数据生态系统中，数据仓库和数据湖都有各自的位置，数据管道经常在两者之间移动数据。
- en: Data Ingestion Tools
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄入工具
- en: The need to ingest data from one system to another is common to nearly all data
    pipelines. As previously discussed in this chapter, data teams must contend with
    a diversity of data sources from which to ingest data from. Thankfully, a number
    of commercial and open source tools are available in a modern data infrastructure.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据从一个系统摄取到另一个系统的需求几乎是所有数据管道的共同需求。正如本章之前讨论的，数据团队必须处理各种各样的数据源。幸运的是，在现代数据基础设施中，有许多商业和开源工具可供选择。
- en: 'In this Pocket Reference, I discuss some of the most common of these tools
    and frameworks, including:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本《便携参考指南》中，我讨论了一些最常见的工具和框架，包括：
- en: Singer
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singer
- en: Stitch
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stitch
- en: Fivetran
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fivetran
- en: Despite the prevalence of these tools, some teams decide to build custom code
    to ingest data. Some even develop their own frameworks. The reasons vary by organization
    but are often related to cost, a culture of building over buying, and concerns
    about the legal and security risks of trusting an external vendor. In [Chapter 5](ch05.xhtml#ch05),
    I discuss the build versus buy trade-offs that are unique to data ingestion tools.
    Of particular interest is whether the value of a commercial solution is to make
    it easier for data engineers to build data ingestions into their pipelines or
    to enable nondata engineers (such as data analysts) to build ingestions themselves.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些工具普遍存在，一些团队决定编写自定义代码来进行数据摄取。有些甚至开发了他们自己的框架。组织机构选择这样做的原因各不相同，但通常与成本、构建文化胜过购买、以及对于信任外部供应商的法律和安全风险的担忧有关。在第
    [5](ch05.xhtml#ch05) 章中，我讨论了数据摄取工具独特的构建与购买权衡。特别感兴趣的是商业解决方案的价值是否在于让数据工程师更容易将数据摄取集成到他们的流水线中，还是在于让非数据工程师（如数据分析师）自行构建摄取过程。
- en: 'As Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05) discuss, data ingestion
    is traditionally both the *extract* and *load* steps of an ETL or ELT process.
    Some tools focus on just these steps, while others provide the user with some
    *transform* capabilities as well. In practice, I find most data teams choose to
    limit the number of transformations they make during data ingestion and thus stick
    to ingestion tools that are good at two things: extracting data from a source
    and loading it into a destination.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第 [4](ch04.xhtml#ch04) 章和第 [5](ch05.xhtml#ch05) 章所述，数据摄取传统上包括 ETL 或 ELT 过程中的
    *提取* 和 *加载* 步骤。一些工具专注于这些步骤，而另一些还提供一些 *转换* 能力给用户。在实际应用中，我发现大多数数据团队选择在数据摄取过程中限制转换数量，并且使用擅长于从源中提取数据并加载到目的地的摄取工具。
- en: Data Transformation and Modeling Tools
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换和建模工具
- en: Though the bulk of this chapter has focused on moving data between sources and
    destinations (data ingestion), there is much more to data pipelines and the movement
    of data. Pipelines are also made up of tasks that transform and model data for
    new purposes, such as machine learning, analysis, and reporting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章的大部分内容集中在源和目的地之间移动数据（数据摄取）上，但数据管道和数据移动还包括更多内容。管道还包括转换和建模数据以实现新目的，如机器学习、分析和报告的任务。
- en: 'The terms *data modeling* and *data transformation* are often used interchangeably;
    however, for the purposes of this text, I will differentiate between them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据建模* 和 *数据转换* 这两个术语常常可以互换使用；然而，在本文中，我将对它们进行区分：'
- en: Data transformation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换
- en: Transforming data is a broad term that is signified by the *T* in an ETL or
    ELT process. A transformation can be something as simple as converting a timestamp
    stored in a table from one time zone to another. It can also be a more complex
    operation that creates a new metric from multiple source columns that are aggregated
    and filtered through some business logic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据是 ETL 或 ELT 过程中 *T* 的广义术语。转换可以是将表中存储的时间戳从一个时区转换到另一个时区这样简单的操作。它还可以是一个更复杂的操作，通过某些业务逻辑聚合和过滤多个源列，创建一个新的度量指标。
- en: Data modeling
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模
- en: Data modeling is a more specific type of data transformation. A data model structures
    and defines data in a format that is understood and optimized for data analysis.
    A data model is usually represented as one or more tables in a data warehouse.
    The process of creating data models is discussed in more detail in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模是数据转换的一种更具体的类型。数据模型以数据分析理解和优化的格式结构化和定义数据。数据模型通常以一个或多个表的形式存在于数据仓库中。有关创建数据模型的过程，将在第
    [6](ch06.xhtml#ch06) 章中详细讨论。
- en: Like data ingestion, there are a number of methodologies and tools that are
    present in a modern data infrastructure. As previously noted, some data ingestion
    tools provide some level of data transformation capabilities, but these are often
    quite simple. For example, for the sake of protecting *personally identifiable
    information* (*PII*) it may be desirable to turn an email address into a hashed
    value that is stored in the final destination. Such a transformation is usually
    performed during the ingestion process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据摄取类似，现代数据基础设施中存在许多方法和工具。如前所述，一些数据摄取工具提供一定程度的数据转换能力，但这些通常非常简单。例如，为了保护*个人身份信息*（PII），可能希望将电子邮件地址转换为哈希值，然后存储在最终目标中。这种转换通常在摄取过程中执行。
- en: For more complex data transformations and data modeling, I find it desirable
    to seek out tools and frameworks specifically designed for the task, such as dbt
    (see [Chapter 9](ch09.xhtml#ch09)). In addition, data transformation is often
    context-specific and can be written in a language familiar to data engineers and
    data analysts, such as SQL or Python.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的数据转换和数据建模，我发现寻找专门设计用于此任务的工具和框架很有必要，例如dbt（参见[第9章](ch09.xhtml#ch09)）。此外，数据转换通常是上下文特定的，可以使用熟悉的数据工程师和数据分析师使用的语言编写，如SQL或Python。
- en: Data models that will be used for analysis and reporting are typically defined
    and written in SQL or via point-and-click user interfaces. Just like build-versus-buy
    trade-offs, there are considerations in choosing to build models using SQL versus
    a *no-code* tool. SQL is a highly accessible language that is common to both data
    engineers and analysts. It empowers the analyst to work directly with the data
    and optimize the design of models for their needs. It’s also used in nearly every
    organization, thus providing a familiar entry point for new hires to a team. In
    most cases, choosing a transformation framework that supports building data models
    in SQL rather than via a point-and-click user interface is desirable. You’ll get
    far more customizability and own your development process from end to end.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分析和报告的数据模型通常是通过SQL或通过点对点用户界面定义和编写的。就像构建与购买的权衡一样，在选择使用SQL还是*无代码*工具构建模型时，需要考虑一些因素。SQL是一种非常易于访问的语言，对数据工程师和分析师都很常见。它使分析师能够直接处理数据，并优化模型设计以满足其需求。它几乎在每个组织中使用，因此为新员工加入团队提供了熟悉的入门点。在大多数情况下，选择支持使用SQL构建数据模型的转换框架比通过点对点用户界面更可取。您将获得更多的可定制性，并从头到尾拥有自己的开发过程。
- en: '[Chapter 6](ch06.xhtml#ch06) discusses transforming and modeling data at length.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch06.xhtml#ch06)详细讨论了数据转换和建模。'
- en: Workflow Orchestration Platforms
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作流编排平台
- en: As the complexity and number of data pipelines in an organization grows, it’s
    important to introduce a *workflow orchestration platform* to your data infrastructure.
    These platforms manage the scheduling and flow of tasks in a pipeline. Imagine
    a pipeline with a dozen tasks ranging from data ingestions written in Python to
    data transformations written in SQL that must run in a particular sequence throughout
    the day. It’s not a simple challenge to schedule and manage dependencies between
    each task. Every data team faces this challenge, but thankfully there are numerous
    workflow orchestration platforms available to alleviate the pain.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织中数据管道的复杂性和数量的增长，将*工作流编排平台*引入到您的数据基础设施中变得至关重要。这些平台管理管道中任务的调度和流动。想象一下一条管道，其中包括从Python编写的数据摄取到SQL编写的数据转换的十几个任务，必须在一天中特定的顺序中运行。安排和管理每个任务之间的依赖关系并不是一个简单的挑战。每个数据团队都面临这一挑战，但幸运的是，有许多工作流编排平台可用来减轻这种痛苦。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Workflow orchestration platforms are also referred to as *workflow management
    systems* (WMSs), *orchestration platforms*, or *orchestration frameworks*. I use
    these terms interchangeably in this text.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排平台也被称为*工作流管理系统*（WMSs）、*编排平台*或*编排框架*。在本文中，我将这些术语互换使用。
- en: Some platforms, such as Apache Airflow, Luigi, and AWS Glue, are designed for
    more general use cases and are thus used for a wide variety of data pipelines.
    Others, such as Kubeflow Pipelines, are designed for more specific use cases and
    platforms (machine learning workflows built on Docker containers in the case of
    Kubeflow Pipelines).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些平台，如Apache Airflow、Luigi和AWS Glue，设计用于更通用的用例，因此用于各种数据管道。另一些平台，如Kubeflow Pipelines，则设计用于更具体的用例和平台（在Kubeflow
    Pipelines的情况下，是构建在Docker容器上的机器学习工作流）。
- en: Directed Acyclic Graphs
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有向无环图
- en: Nearly all modern orchestration frameworks represent the flow and dependencies
    of tasks in a pipeline as a graph. However, pipeline graphs have some specific
    constraints.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代编排框架将任务的流程和依赖关系表示为图形管道。然而，管道图具有一些特定的约束条件。
- en: Pipeline steps are always *directed*, meaning they start with a general task
    or multiple tasks and end with a specific task or tasks. This is required to guarantee
    a path of execution. In other words, it ensures that tasks do not run before all
    their dependent tasks are completed successfully.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 管道步骤始终是*有向的*，这意味着它们从一个一般任务或多个任务开始，并以一个特定的任务或任务结束。这是为了确保执行路径。换句话说，它确保任务在其所有依赖任务成功完成之前不会运行。
- en: Pipeline graphs must also be *acyclic*, meaning that a task cannot point back
    to a previously completed task. In other words, it cannot cycle back. If it could,
    then a pipeline could run endlessly!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 管道图还必须是*无环的*，这意味着任务不能指向之前已完成的任务。换句话说，它不能循环。如果可以的话，管道可能会无限运行！
- en: With these two constraints in mind, orchestration pipelines produce graphs called
    directed acyclic graphs (DaGs). [Figure 2-3](#fig_0203) illustrates a simple DAG.
    In this example, Task A must complete before Tasks B and C can start. Once they
    are both completed, then Task D can start. Once Task D is complete, the pipeline
    is completed as well.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这两个约束条件的基础上，编排管道生成称为有向无环图（DaGs）的图形。[图 2-3](#fig_0203)展示了一个简单的DAG。在这个例子中，任务A必须在任务B和C开始之前完成。一旦它们都完成了，任务D就可以开始。一旦任务D完成，整个管道也完成了。
- en: '![dppr 0203](Images/dppr_0203.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0203](Images/dppr_0203.png)'
- en: Figure 2-3\. A DAG with four tasks. After Task A completes, Task B and Task
    C run. When they both complete, Task D runs.
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 一个有四个任务的DAG。任务A完成后，任务B和任务C运行。当它们都完成后，任务D运行。
- en: DAGs are a representation of a set of tasks and not where the logic of the tasks
    is defined. An orchestration platform is capable of running tasks of all sorts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: DAG是一组任务的表示，并不是任务逻辑定义的位置。编排平台能够运行各种任务。
- en: For example, consider a data pipeline with three tasks. It is represented as
    a DAG in [Figure 2-4](#fig_0204).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个有三个任务的数据管道。在[图 2-4](#fig_0204)中表示为一个DAG。
- en: The first executes a SQL script that queries data from a relational database
    and stores the result in a CSV file.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个执行一个SQL脚本，从关系型数据库中查询数据，并将结果存储在CSV文件中。
- en: The second runs a Python script that loads the CSV file, cleans, and then reshapes
    the data before saving a new version of the file.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个运行一个Python脚本，加载CSV文件，清理数据，然后重塑数据后保存新版本的文件。
- en: Finally, a third task, which runs the COPY command in SQL, loads the CSV created
    by the second task into a Snowflake data warehouse.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，第三个任务运行SQL中的COPY命令，将第二个任务创建的CSV加载到Snowflake数据仓库中。
- en: '![dppr 0204](Images/dppr_0204.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0204](Images/dppr_0204.png)'
- en: Figure 2-4\. A DAG with three tasks that run in sequence to extract data from
    a SQL database, clean and reshape the data using a Python script, and then load
    the resulting data into a data warehouse.
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 一个有三个任务的DAG，按顺序从SQL数据库提取数据，使用Python脚本清理和重塑数据，然后将结果数据加载到数据仓库中。
- en: The orchestration platform executes each task, but the logic of the tasks exists
    as SQL and Python code, which runs on different systems across the data infrastructure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 编排平台执行每个任务，但任务的逻辑存在于SQL和Python代码中，在数据基础设施的不同系统上运行。
- en: '[Chapter 7](ch07.xhtml#ch07) discusses workflow orchestration platforms in
    more detail and provides hands-on examples of orchestrating a pipeline in Apache
    Airflow.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.xhtml#ch07)更详细地讨论了工作流编排平台，并提供了Apache Airflow中管道编排的实际示例。'
- en: Customizing Your Data Infrastructure
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义数据基础设施
- en: It’s rare to find two organizations with exactly the same data infrastructure.
    Most pick and choose tools and vendors that meet their specific needs and build
    the rest on their own. Though I talk in detail about some of the most popular
    tools and products throughout this book, many more come to market each year.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 很少能找到两个完全相同数据基础设施的组织。大多数选择符合其特定需求的工具和供应商，并自行构建其余部分。尽管本书中详细介绍了一些最流行的工具和产品，但每年市场上还有更多新产品问世。
- en: As previously noted, depending on the culture and resources in your organization,
    you may be encouraged to build most of your data infrastructure on your own, or
    to rely on SaaS vendors instead. Regardless of which way you lean on the build-versus-buy
    scale, you can build the high-quality data infrastructure necessary to build high-quality
    data pipelines.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，根据您组织中的文化和资源情况，您可能会被鼓励大部分数据基础设施自行构建，或者依赖于SaaS供应商。无论您在构建与购买的权衡上倾向于哪一方，您都可以构建高质量的数据基础设施，这对于构建高质量的数据流水线至关重要。
- en: What’s important is understanding your constraints (dollars, engineering resources,
    security, and legal risk tolerance) and the resulting trade-offs. I speak to these
    throughout the text and call out key decision points in selecting a product or
    tool.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是理解你的约束条件（资金、工程资源、安全性和法律风险容忍度），以及由此产生的权衡。我在整个文本中都会谈及这些，并且指出在选择产品或工具时的关键决策点。
