- en: Chapter 46\. Toward Value-Based Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第46章。走向价值导向的机器学习
- en: Ron Bodkin
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ron Bodkin
- en: '![](Images/Ron_Bodkin.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Ron_Bodkin.png)'
- en: Office of the CTO, Google Cloud
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 首席技术官办公室
- en: Machine learning (ML) has become integral to many aspects of modern life, as
    digital experiences proliferate and we increasingly rely on automated algorithms
    for discovery, curation, and guiding our choices in areas as diverse as entertainment
    content (e.g., Medium and TikTok), communication (Slack and Gmail), navigation
    (Google Maps), and shopping (Amazon and Stitch Fix).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）已经成为现代生活中许多方面的重要组成部分，随着数字体验的增加，我们越来越依赖于自动化算法来进行发现、策划以及在娱乐内容（例如Medium和TikTok）、通信（Slack和Gmail）、导航（Google
    Maps）以及购物（Amazon和Stitch Fix）等领域做出选择。
- en: ML is often viewed as a value-neutral technology and as objective, unaligned
    with or dependent on values. But the reality is that ML is a tool—and like any
    tool, its use is based on values, and the consequences it creates impact our values.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习经常被视为一种价值中立的技术，客观的，不受价值观影响或依赖。但实际上，机器学习是一种工具 —— 像任何工具一样，它的使用基于价值观，并且它所产生的后果影响我们的价值观。
- en: I have been responsible for applying ML to real-world problems since 2007 and
    have repeatedly found that the use of ML leads to unintended consequences. Much
    like an evil genie, ML models will often grant exactly what you wished for (optimize
    what you specify) but not what you really intended. Ten years ago, when I was
    Vice President of Engineering at [Quantcast](http://www.quantcast.com), we would
    often be frustrated to see that ML models we created didn’t work properly. They
    would exploit subtle errors in our data or problem setup, and we had to work hard
    to understand what made them work so we could fix our data and fix our objectives
    (or *loss functions*) to achieve the results we *intended*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自2007年以来，我负责将机器学习应用于实际问题，并反复发现使用机器学习会导致意想不到的后果。就像一个邪恶的精灵，机器学习模型通常会准确地给予你你所期望的（优化你所指定的内容），但并不是你真正想要的结果。十年前，当我在Quantcast担任工程副总裁时，我们经常会感到沮丧，因为我们创建的机器学习模型无法正常工作。它们会利用我们数据或问题设置中的细微错误，我们不得不努力理解它们的工作原理，以便修复我们的数据并调整我们的目标（或*损失函数*）以实现我们*意图的*结果。
- en: More recently, there have been important instances of the unintended consequences
    of ML in areas like biased ML models. Examples include Amazon’s hiring algorithms
    that were [biased against hiring female engineers](https://oreil.ly/UIRbD) and
    Alphabet Jigsaw’s algorithms for toxic content that were [biased against targeted
    identity groups](https://oreil.ly/NVLck). More generally, recommendation systems
    show a bias toward trashy, inflammatory “clickbait” content (e.g., the impact
    of recent Facebook [algorithmic changes](https://oreil.ly/ZTC9I)). There are also
    value challenges from explicitly harming customer interests—for example, Amazon
    [changing ML-driven search results in favor of its own profitability over what
    customers want](https://oreil.ly/1ut8d).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在诸如偏见机器学习模型等领域，出现了重要的意外后果实例。例如，亚马逊的招聘算法对[女性工程师的偏见](https://oreil.ly/UIRbD)，以及Alphabet
    Jigsaw的有毒内容算法对[特定身份群体的偏见](https://oreil.ly/NVLck)。更普遍地说，推荐系统显示出对“垃圾”、具有煽动性的“点击诱饵”内容的偏见（例如，最近Facebook的[算法变更](https://oreil.ly/ZTC9I)的影响）。还有从明确损害客户利益的角度出发的价值挑战，例如，亚马逊在[利润优先于顾客需求改变机器学习驱动的搜索结果](https://oreil.ly/1ut8d)。
- en: To date, most schemes to address these problems have been based on the premise
    that you can optimize for a value-neutral objective function (like maximizing
    revenue or time spent on an app) while building in various guardrails. Common
    techniques include filtering out problematic cases, preparing data to avoid bias,
    devising model explanation tools, and tracking secondary metrics (e.g., tracking
    for long-term cohort engagement and not short-term response).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，解决这些问题的大多数方案都是基于这样一个前提：你可以优化一个价值中立的目标函数（例如最大化收入或应用程序上的时间），同时建立各种防护措施。常见的技术包括过滤出问题案例、准备数据以避免偏见、设计模型解释工具以及跟踪次要指标（例如长期队列参与度跟踪而非短期反应）。
- en: 'I believe this approach is fundamentally insufficient—value-neutral objectives
    are amoral by definition. Instead, I believe we must do more: we must encode values
    in the objectives we measure and work on ways to explicitly produce good outcomes
    with ML, in addition to other ethical AI practices.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这种方法基本上是不够的 —— 以价值中立的目标为导向在定义上是不道德的。相反，我认为我们必须做得更多：我们必须在我们测量的目标中编码价值观，并努力寻找明确产生良好结果的方法，除了其他伦理人工智能实践。
- en: An Example of the Importance of Values
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价值观重要性的例子
- en: Consider content recommendation systems. [Tristan Harris](https://twitter.com/tristanharris),
    cofounder of the [Center for Humane Technology](https://humanetech.com), has a
    nice way of thinking about this problem. Imagine a spectrum of content ranging
    from regenerative (i.e., reflective and thoughtful content, such as articles in
    widely respected publications) to extractive (i.e., inflammatory and extreme content,
    such as fringe or conspiracy sites), as illustrated in the following figure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑内容推荐系统。[Tristan Harris](https://twitter.com/tristanharris)，[人文技术中心](https://humanetech.com)的联合创始人，对这个问题有着独特的思考方式。想象一个内容光谱，从再生性的（例如反思性和深思熟虑的内容，比如广受尊重的出版物中的文章）到提取性的（例如煽动性和极端的内容，比如边缘或阴谋论网站），如下图所示。
- en: '![Image](Images/aeds_46in01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/aeds_46in01.png)'
- en: A few years ago, content recommendation systems were often trained to optimize
    clicks. This resulted in a lot of trashy clickbait content that wasn’t engaging
    for users (or regenerative!). The next evolution was to optimize for aggregate
    engagement time. This has resulted in more sustained user engagement but has led
    inevitably to more inflammatory and extreme content (like conspiracy theories,
    political extremism, and criminal activity). In that spectrum of content, the
    system is tilted to the right side of the graph—user engagement flows toward the
    more sensationalist and alarming material (see the following figure).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，内容推荐系统通常是为了优化点击而进行训练的。这导致了很多低俗的点击诱饵内容，对用户来说并不具有吸引力（或再生性！）。下一个发展阶段是为了优化总体参与时间。这导致了更持久的用户参与，但不可避免地导致了更多煽动性和极端的内容（如阴谋论、政治极端主义和犯罪活动）。在那个内容的光谱中，系统倾向于图表的右侧——用户参与流向更耸动和令人担忧的材料（见下图）。
- en: '![Image](Images/aeds_46in02.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/aeds_46in02.png)'
- en: This trend has also increased digital addiction, and in a meaningful way, it
    has *downgraded* the users of systems and society more broadly. The use of human
    review, policing of terms of service, and secondary systems to block and/or not
    recommend and/or not serve ads for inaccurate or other bad content has helped.
    However, bad actors continuously find ways to probe the boundary and create the
    worst, most addictive content that will be promoted. You can think of these practices
    as establishing barriers against the worst content while still keeping the same
    harmful flow, as illustrated in the following figure.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种趋势还增加了数字上瘾，而且以有意义的方式，*贬低*了系统和社会的用户。人工审核、服务条款的监管以及阻止和/或不推荐和/或不提供不准确或其他不良内容的广告的次级系统有所帮助。然而，不良行为者不断找到方法来探索边界并创造最糟糕、最具成瘾性的内容，这将被推广。您可以将这些做法看作是在防范最坏内容的同时保持同样有害流程的屏障，如下图所示。
- en: '![Image](Images/aeds_46in03.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/aeds_46in03.png)'
- en: Conversely, imagine a system that explicitly optimizes for high-quality engagement
    by amplifying regenerative content (see the following figure). This should include
    ideas such as matching resources spent by users to their intentions—for example,
    “I’d like to enjoy no more than five hours a week of entertaining content,” “I
    don’t want to be drawn into watching cat videos until 2 a.m.,” or “I’d like to
    spend at most $50 per month on video entertainment.” It should also include impact
    on society as a whole—for instance, by informing people, provoking meaningful
    engagement, encouraging deep personal interactions, and allowing political discourse
    to evolve.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相反地，想象一个明确优化高质量参与的系统，通过增强再生性内容（见下图）。这应该包括诸如匹配用户花费的资源与他们的意图——例如，“我想每周享受不超过五个小时的娱乐内容”，“我不想被拖入看猫视频直到凌晨两点”，或者“我希望每月的视频娱乐开支不超过50美元”。它还应该包括对整个社会的影响——例如，通过向人们提供信息、引发有意义的互动、鼓励深层次的个人互动以及促进政治话语的演变。
- en: '![Image](Images/aeds_46in04.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/aeds_46in04.png)'
- en: Of course, these are hard concepts to represent as a simple mathematical function—and
    there is a great diversity of perspective on what high-quality engagement means.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，将这些概念表示为简单的数学函数是很困难的——对于高质量参与的定义存在着多样化的观点。
- en: How to Proceed?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何继续？
- en: This shouldn’t be viewed as an obstacle to progress but as a grand challenge
    that the technology community should embrace. How can we better create objective
    functions that optimize our values? How can we better anticipate and mitigate
    unintended consequences? How can we incorporate better transparency and visibility
    into the objectives our systems are encoding?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应被视为进步的障碍，而是技术社区应该拥抱的重大挑战。我们如何更好地创建优化我们价值观的客观函数？我们如何更好地预见和减少意外后果？我们如何在系统的目标编码中增加更好的透明度和可见性？
- en: I believe it will become a critical engineering discipline—blending technical
    acumen with broader insight into policy goals and ethics. Likewise, François Chollet
    (the creator of [Keras](https://keras.io)) views having the right objectives for
    ML as important, [predicting](https://oreil.ly/WQ4HI) that “loss function engineer
    is probably going to be a job title in the future.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信这将成为一门至关重要的工程学科——将技术才能与对政策目标和伦理的广泛洞见相结合。同样，[Keras](https://keras.io)的创始人François
    Chollet认为，为机器学习设定正确的目标至关重要，并且[预测](https://oreil.ly/WQ4HI)“损失函数工程师可能会成为未来的职称”。
- en: There is a lot of opportunity to close the gap between long-term research in
    [AI value alignment](https://oreil.ly/bTSlx) (e.g., [inverse reinforcement learning](https://oreil.ly/mKy4_))
    and practical goals in systems today. But the most important step is to take ownership
    of the ethics of AI systems and incorporate values into their design, especially
    their objectives!^([1](ch46.xhtml#ch01fn40))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在长期研究与实际系统之间的差距中，[AI价值对齐](https://oreil.ly/bTSlx)（例如，[逆强化学习](https://oreil.ly/mKy4_)）有很多机会。但最重要的一步是对AI系统的伦理负责并将价值观融入设计中，尤其是它们的目标！^([1](ch46.xhtml#ch01fn40))
- en: ^([1](ch46.xhtml#ch01fn40-marker)) Adapted from Ron Bodkin, “Towards Value-Based
    Machine Learning,” Medium, October 12, 2019, [*https://oreil.ly/HDlOW*](https://oreil.ly/HDlOW).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch46.xhtml#ch01fn40-marker)) 改编自Ron Bodkin的文章，“走向基于价值的机器学习”，Medium，2019年10月12日，[*https://oreil.ly/HDlOW*](https://oreil.ly/HDlOW)。
