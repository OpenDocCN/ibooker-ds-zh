- en: 12 Network design alternatives to RNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 RNNs的12种网络设计替代方案
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Working around the limitations of RNNs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服RNNs的限制
- en: Adding time to a model using positional encodings
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用位置编码向模型添加时间
- en: Adapting CNNs to sequence-based problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CNN适应基于序列的问题
- en: Extending attention to multiheaded attention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将注意力扩展到多头注意力
- en: Understanding transformers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Transformer
- en: Recurrent neural networks—in particular, LSTMs—have been used for classifying
    and working with sequence problems for over two decades. While they have long
    been reliable tools for the task, they have several undesirable properties. First,
    RNNs are just plain slow. They take a long time to train, which means waiting
    around for results. Second, they do not scale well with more layers (hard to improve
    model accuracy) or with more GPUs (hard to make them train faster). With skip
    connections and residual layers, we have learned about many ways to get fully
    connected and convolutional networks to train with more layers to get better results.
    But RNNs just do not seem to like being deep. You can add more layers and skip
    connections, but they do not show the same degree of benefits as improved accuracy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络——特别是LSTMs——已经用于分类和解决序列问题超过二十年。虽然它们长期以来一直是可靠的工具，但它们有几个不希望的特性。首先，RNNs非常慢。它们需要很长时间来训练，这意味着需要等待结果。其次，它们在更多层（难以提高模型精度）或更多GPU（难以使它们训练更快）的情况下扩展得不好。通过跳过连接和残差层，我们已经了解了许多使全连接和卷积网络通过更多层来获得更好结果的方法。但RNNs似乎不喜欢深度。你可以添加更多层和跳过连接，但它们并不显示出与提高精度相同的程度的好处。
- en: In this chapter, we look at some methods that can help us with one or both of
    these problems. First, we tackle the slowness of RNNs by violating our prior beliefs.
    We use RNNs because we know the data is a sequence, but we pretend that it is
    not a normal sequence so we can do something faster—and potentially less accurate.
    Next, we look at a different way of representing the sequential component of our
    data to augment these faster alternatives and regain some of our accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨一些可以帮助我们解决一个或两个这些问题的方法。首先，我们通过违背先前的信念来解决RNNs的缓慢问题。我们使用RNNs是因为我们知道数据是序列，但我们假装它不是一个正常序列，这样我们就可以更快地做些事情——并且可能更不准确。接下来，我们探讨一种不同的方式来表示数据的序列组件，以增强这些更快的替代方案并恢复一些准确性。
- en: Finally, we learn about transformers (no, not the copyrighted kind), which are
    even slower to train but scale much better with depth and powerful compute devices.
    Transformer-based models are quickly becoming foundational to the highest accuracy
    solutions for many tasks in natural language processing and are likely to become
    a large part of your life as a practitioner.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们了解Transformer（不是受版权保护的类型），它们的训练速度甚至更慢，但与深度和强大的计算设备相比，扩展得更好。基于Transformer的模型正在迅速成为自然语言处理中许多任务最高精度解决方案的基础，并且很可能成为你作为从业者生活的一部分。
- en: '12.1 TorchText: Tools for text problems'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 TorchText：文本问题的工具
- en: To help us test the relative pros and cons of many alternatives to RNNs, the
    first thing we need is a dataset and some baseline results. For this chapter,
    I briefly introduce and use the `torchtext` package. Like `torchvision`, it is
    a subproject of PyTorch. Whereas `torchvision` provides extra tooling specifically
    for vision-based problems, `torchtext` provides extra tooling around text-based
    ones. We won’t go into all the special features it contains; we are simply going
    to use it for easy access to a harder dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们测试RNNs许多替代方案的相对优缺点，我们首先需要的是一个数据集和一些基线结果。对于本章，我简要介绍了并使用了`torchtext`包。像`torchvision`一样，它是PyTorch的一个子项目。而`torchvision`提供了针对基于视觉问题的额外工具，`torchtext`则提供了围绕基于文本问题的额外工具。我们不会深入探讨它包含的所有特殊功能；我们只是用它来方便地访问更难的数据集。
- en: 12.1.1  Installing TorchText
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 安装TorchText
- en: 'The first thing to do is quickly make sure `torchtext` and the optional dependency
    `sentencepiece` are installed; otherwise, we won’t be able to get all of our desired
    features to work. The following code installs both when run via Colab (or any
    Jupyter notebook):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的就是快速确认`torchtext`和可选依赖`sentencepiece`已安装；否则，我们无法使所有期望的功能正常工作。以下代码在通过Colab（或任何Jupyter笔记本）运行时安装这两个包：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 12.1.2  Loading datasets in TorchText
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 在TorchText中加载数据集
- en: 'Now we can import `torchtext` and the associated `datasets` package, where
    we will load the AG News dataset. This dataset has four classes, and `torchtext`
    will provide the utilities to get us ready for training quickly. But first we
    import the package, get the dataset iterators (this is how `torchtext` likes to
    provide data), and place the train and test data in their own respective lists:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以导入 `torchtext` 和相关的 `datasets` 包，我们将加载 AG News 数据集。这个数据集有四个类别，`torchtext`
    将提供快速准备训练的实用工具。但首先，我们导入包，获取数据集迭代器（这是 `torchtext` 喜欢提供数据的方式），并将训练数据和测试数据分别放入各自的列表中：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we have loaded the AG News dataset, which has four possible topic classes
    for each document: World, Sports, Business, and Sci/Tech. The following block
    of code prints out an example of this dataset. We have a `string` of words representing
    our input x and a label for the class y. `torchtext` unfortunately bucks the normal
    trend of returning tuples ordered by data/input first and label second (**x**,
    *y*) and instead swaps the order as (*y*,**x**):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了 AG News 数据集，该数据集为每个文档提供了四个可能的主题类别：世界、体育、商业和科技。以下代码块打印出该数据集的一个示例。我们有一个表示输入
    x 的单词 `string` 和表示类 y 的标签。不幸的是，`torchtext` 违背了通常按数据/输入顺序首先返回，然后是标签的常规趋势（**x**,
    *y*），而是交换了顺序为 (*y*,**x**）：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We need to break up each sentence into a set of discrete tokens (i.e., words)
    and build a vocabulary (Σ) to map each word to a unique integer value, because
    the `nn.Embedding` layer only takes integer inputs. `torchtext` provides a few
    tools for making this pretty painless and working with some of the existing Python
    tooling. Python brings the `Counter` object to count how many times different
    tokens are seen, and `torchtext` provides a `tokenizer` and `Vocab` object to
    split up strings and do the bookkeeping for us:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将每个句子分解成一组离散的标记（即单词），并构建一个词汇表（Σ）来将每个单词映射到唯一的整数值，因为 `nn.Embedding` 层只接受整数输入。`torchtext`
    提供了一些工具，使这个过程变得非常简单，并可以与一些现有的 Python 工具一起使用。Python 带来了 `Counter` 对象来计算不同标记出现的次数，`torchtext`
    提供了 `tokenizer` 和 `Vocab` 对象来分割字符串并为我们做账簿记录：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Tokenizers break strings like “this is a string" into lists of tokens like
    [‘this’, ‘is’, ‘a’, ‘string’].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分词器将“this is a string”这样的字符串分解成标记列表，如 [‘this’，‘is’，‘a’，‘string’]。
- en: ❷ We are fine with the default English style tokenizer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们对默认的英语风格分词器没有问题。
- en: ❸ How many lines in this dataset
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这个数据集有多少行
- en: ❹ We need to create a vocabulary of all the words in the training set.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们需要创建一个包含训练集中所有单词的词汇表。
- en: ❺ Loops through the training data
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历训练数据
- en: ❻ Counts the number of unique tokens and how often we see them (e.g., we see
    “the" a lot, but “sasquatch" maybe once or not at all)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算唯一标记的数量以及它们出现的频率（例如，我们经常看到“the”，但“sasquatch”可能只出现一次或根本不出现）
- en: ❼ Creates a vocab object, removing any word that didn’t occur at least 10 times,
    and adds special vocab items for unknown, start of sentence, end of sentence,
    and padding
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 创建一个词汇对象，删除任何至少没有出现 10 次的单词，并为未知、句子开头、句子结尾和填充添加特殊的词汇项
- en: 'Now we need two more helper functions to complete the `torchtext` specific
    setup. `text_transform` takes in a string and converts it into a list of integers
    based on the vocabulary Σ. The other, simpler item is `label_transform`, which
    makes sure the labels are in the right format (you may have to change this more
    for different datasets in `torchtext`):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要两个额外的辅助函数来完成 `torchtext` 特定的设置。`text_transform` 接收一个字符串并将其转换为基于词汇 Σ 的整数列表。另一个更简单的项目是
    `label_transform`，它确保标签处于正确的格式（你可能需要根据 `torchtext` 中的不同数据集进行更多更改）：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ String to a list of integers
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将字符串转换为整数列表
- en: ❷ vocab acts like a dictionary and handles unknown tokens. We can make it pre-
    and post-pend with the start and end markers, respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 词汇表就像一个字典，处理未知标记。我们可以分别用开始和结束标记进行前缀和后缀。
- en: ❸ Labels are originally [1, 2, 3, 4], but we need [0, 1, 2, 3].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 标签最初是 [1, 2, 3, 4]，但我们需要 [0, 1, 2, 3]。
- en: ❹ Transforms the first data point’s text into a list of tokens
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将第一个数据点的文本转换为标记列表
- en: 'The `torchtext` specifics are basically out of the way now; we can do some
    initial PyTorch ceremony to get set up. We want to note how many tokens are in
    our vocabulary and the number of classes, and decide on the embedding dimension,
    batch size, and number of training epochs. I’ve chosen some arbitrary values for
    each in the following starting code. We also save the integer representing our
    padding token `"<PAD>"` because we will need it for batching up our datasets:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext` 的具体细节现在基本已经处理完毕；我们可以进行一些初始的 PyTorch 仪式来设置环境。我们想要记录词汇表中有多少个标记以及类别的数量，并决定嵌入维度、批量大小和训练轮数。在下面的起始代码中，我为每个参数选择了任意值。我们还保存了代表我们的填充标记
    `"<PAD>"` 的整数，因为我们将在批处理数据集时需要它：'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We also define the function we use as our `collate_fn` for the `DataLoader`
    class in PyTorch. `collate_fn` is generally the best place to implement padding,
    as it allows you to pad by exactly the minimum amount needed for the current batch
    of data. It is also a convenient place to reverse the label and input order to
    match the standard `(input, label)` pattern we expect. That way, our returned
    batches will follow (**x**,*y*) like the rest of our code expects, and we can
    reuse everything we’ve done so far.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了用于 PyTorch 的 `DataLoader` 类的 `collate_fn` 函数。`collate_fn` 通常是最好的实现填充的地方，因为它允许你根据当前批次数据所需的最小量进行填充。它也是一个方便的地方，可以反转标签和输入顺序，以匹配我们期望的标准
    `(input, label)` 模式。这样，我们返回的批次将遵循（**x**，*y*）的模式，就像我们的其他代码所期望的那样，我们可以重用我们迄今为止所做的一切。
- en: 'All of this is done in the `pad_batch` function, which follows a normal approach
    to padding. We first determine what the longest sequence is of all the items in
    the batch and store its value in the `max_len` variable. Then we use the `F.pad`
    function to pad to the *right* of each item in the batch. This makes all the items
    the same length so we can `stack` them into a single tensor. Notice that we make
    sure to use the `padding_idx` for the value to pad by:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都在 `pad_batch` 函数中完成，它遵循正常的填充方法。我们首先确定批次中所有项的最长序列，并将其值存储在 `max_len` 变量中。然后我们使用
    `F.pad` 函数将每个批次中的项向右填充。这使得所有项具有相同的长度，我们可以将它们 `stack` 成一个单一的张量。注意，我们确保使用 `padding_idx`
    作为填充的值：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Gets and transforms every label in the batch
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取并转换批次中的每个标签
- en: ❷ Gets and tokenizes every text and puts them into a tensor
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取并标记化每个文本，并将它们放入一个张量中
- en: ❸ What is the longest sequence in this batch?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在这个批次中，最长的序列是哪一个？
- en: ❹ Pads each text tensor by whatever amount makes it max_len
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 whatever amount makes it max_len 对每个文本张量进行填充
- en: ❺ Makes x and y a single tensor
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将 x 和 y 合并为一个单一的张量
- en: 'Now we can build our `DataLoader`s as we have done many times before to feed
    data to our models. Thanks to the `Dataset` abstraction, we don’t have to know
    the detailsabout how `torchtext` implemented its loaders, and we can see that
    by setting `collate_fn``=pad_batch`, we can work around any quirks a dataset may
    have by how it returns data. Instead of rewriting the dataset, our chosen `collate_fn`
    simply cleans up the output of the `Dataset` the way we want:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像以前多次做的那样构建我们的 `DataLoader`，以向我们的模型提供数据。多亏了 `Dataset` 抽象，我们不需要知道 `torchtext`
    实现其加载器的细节，我们可以看到，通过设置 `collate_fn` = `pad_batch`，我们可以通过数据集返回数据的方式解决任何数据集可能存在的怪癖。我们不需要重写数据集，我们选择的
    `collate_fn` 简单地以我们想要的方式清理 `Dataset` 的输出：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 12.1.3  Defining a baseline model
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 定义基线模型
- en: 'Now that we have our dataset collate function and loaders ready, we can train
    some models. Let’s start by implementing a baseline RNN model to which we can
    compare each alternative, giving us a barometer to judge pros and cons. We’ll
    use a gated recurrent unit (GRU)-based RNN with three bidirectional layers to
    try to maximize the accuracy we get. We’re using a GRU here because it’s faster
    than an LSTM, and we want to give it every chance it can get to win the runtime
    comparisons we will explore. While we could explore different hidden dimension
    sizes, learning rates, and other knobs, this is a pretty straightforward and standard
    first attempt I would use for an RNN. So let’s pick our loss function and call
    our handy-dandy `train_network`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集合并函数和加载器，我们可以训练一些模型了。让我们先实现一个基线 RNN 模型，我们可以将其与每个替代方案进行比较，从而为我们提供一个衡量利弊的标尺。我们将使用基于门控循环单元（GRU）的
    RNN，并带有三个双向层，以尝试最大化我们获得的准确率。我们在这里使用 GRU 是因为它比 LSTM 快，并且我们希望给它尽可能多的机会在运行时比较中获胜。虽然我们可以探索不同的隐藏维度大小、学习率和其他旋钮，但这将是一个相当直接和标准的第一次尝试，我会用它来构建
    RNN。所以，让我们选择我们的损失函数并调用我们方便的 `train_network`：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ (B, T) -> (B, T, D)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T) -> (B, T, D)
- en: ❷ (B, T, D) -> ( (B,T,D) , (S, B, D) )
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, D) -> ( (B,T,D) , (S, B, D) )
- en: ❸ We need to take the RNN output and reduce it to one item, (B, 2*D).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们需要将 RNN 输出减少到一个项目，(B, 2*D)。
- en: ❹ (B, D) -> (B, classes)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, D) -> (B, classes)
- en: Our first attempt’s result is shown next and seems to be doing reasonably well,
    with some overfitting after the first few epochs but stabilizing around 91.5%
    accuracy. Through the rest of this chapter, as we develop new alternatives, we
    plot them onto the same figure to get an overall comparison. For example, we want
    to think about max accuracy (if we regularize better, what we might be able to
    achieve), stabilized accuracy (what can we get with little effort), and epochs
    until some level of accuracy (how long we need to train).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次尝试的结果显示在下面，看起来做得相当不错，在前几个时期后有一些过拟合，但稳定在约 91.5% 的准确率。在本章的其余部分，随着我们开发新的替代方案，我们将它们绘制在同一个图上以进行整体比较。例如，我们想要考虑最大准确率（如果我们更好地正则化，我们可能能够实现什么），稳定准确率（我们可以用很少的努力得到什么），以及达到某些准确率水平的时期（我们需要训练多长时间）。
- en: 'Here’s the code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/CH12_UN01_Raff.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN01_Raff.png)'
- en: 12.2 Averaging embeddings over time
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 时间上平均嵌入
- en: The GRU baseline is ready for us to compare against, so let’s learn our first
    new approach to handling a variable-length sequence classification problem. Instead
    of performing complex RNN operations over the time series, we just *average* all
    the items in the sequence. So if we have an input of shape (*B*,*T*,*D*), we compute
    the mean value along the second axis, creating a new tensor of shape (*B*,*D*).
    It has the benefit of being very simple to implement; it is fast; and it *removes
    the time dimension*, allowing us to then apply whatever approaches we desire (such
    as residual connections) after that point. Figure 12.1 shows what this looks like.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 基准已经准备好供我们比较，所以让我们学习处理可变长度序列分类问题的第一个新方法。我们不是在时间序列上执行复杂的 RNN 操作，而是对序列中的所有项目进行*平均*。所以如果我们有一个形状为
    (*B*,*T*,*D*) 的输入，我们计算第二个轴上的平均值，创建一个新的形状为 (*B*,*D*) 的张量。它有易于实现的优点；速度快；并且*消除了时间维度*，允许我们在那个点之后应用我们想要的任何方法（例如残差连接）。图
    12.1 展示了这看起来是什么样子。
- en: '![](../Images/CH12_F01_Raff.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F01_Raff.png)'
- en: Figure 12.1 Example of averaging embeddings over time. The T inputs over time
    are processed by the same neural network via weight sharing. Because there are
    *no* connections *across time*, the model is unaware that there is an order to
    them. The time dimension is resolved by simply averaging all of therepresentations
    so a regular fully connected network can run afterward to produce the output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 时间上平均嵌入的示例。时间上的 T 个输入通过权重共享由相同的神经网络处理。因为*没有*跨时间的连接，模型不知道它们有顺序。时间维度通过简单地平均所有表示来解析，这样就可以在之后运行一个常规的全连接网络来生成输出。
- en: Because we are averaging over the time dimension, we ignore that there is an
    order to the data. Another way to say this is that we are *ignoring the structure*
    of the data to simplify our model. We *might* want to do this to make our training
    and prediction faster, but it could also backfire and give us worse-quality results.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在时间维度上平均，我们忽略了数据有顺序的事实。另一种说法是我们*忽略了数据的结构*以简化我们的模型。我们*可能*想这样做以使我们的训练和预测更快，但这也可能适得其反，导致结果质量更差。
- en: One simple way we can implement averaging over time is to use the adaptive pooling
    we learned about in chapter 8\. Adaptive pooling works by taking the *desired
    output size* and adjusting the pooling size to force the output to have the desired
    size. The Faster R-CNN used adaptive pooling to shrink any input down to a 7 ×
    7 grid. Instead, we will be a little exploitive of how adaptive pooling works.
    If our input has a shape of (*B*,*T*,*D*), we want to perform adaptive pooling
    over the last two dimensions, so we use adaptive 2D pooling. We make the target
    shape for the pooling non-symmetric with a shape of (1,*D*). Our input already
    has a shape of D in the last dimension, so the last dimension will not be altered.
    The time dimension shrinks down to 1, forcing it to be averaged over time. Because
    of its adaptive pooling, this code will work even if the value T changes between
    batches. We can define and try this model very quickly in the following code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用的一种简单的时间平均实现方法是使用我们在第8章中学到的自适应池化。自适应池化通过取*期望输出大小*并调整池化大小来强制输出达到期望的大小来实现。Faster
    R-CNN使用自适应池化将任何输入缩小到7 × 7的网格。相反，我们将稍微利用一下自适应池化是如何工作的。如果我们的输入形状为(*B*,*T*,*D*)，我们想要在最后两个维度上执行自适应池化，因此我们使用自适应2D池化。我们使池化的目标形状非对称，形状为(1,*D*)。我们的输入在最后一个维度上已经具有D的形状，所以最后一个维度不会被改变。时间维度缩小到1，强制它进行时间平均。由于其自适应池化，即使T在批次之间发生变化，此代码也能正常工作。我们可以在下面的代码中非常快速地定义并尝试此模型。
- en: 'As mentioned before, this code relies on the fact that if you give a PyTorch
    `nn.Linear` layer a tensor of shape (*B*,*T*,*D*), it will apply the linear layer
    to all T different inputs independently, effectively performing weight sharing
    in a single call. We call `nn.AdaptiveAvgPool2d` with the explicit target shape
    of (1,*D*) that reduces the input tensor from (*B*,*T*,*D*) down to (*B*,1,*D*).
    Then we can end with one hidden layer and a `nn.Linear` to predict the classes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此代码依赖于这样一个事实：如果你给PyTorch `nn.Linear`层一个形状为(*B*,*T*,*D*)的张量，它将独立地对所有T个不同的输入应用线性层，从而在单个调用中有效地执行权重共享。我们使用具有显式目标形状(1,*D*)的`nn.AdaptiveAvgPool2d`，将输入张量从(*B*,*T*,*D*)减少到(*B*,1,*D*)。然后我们可以用一个隐藏层和一个`nn.Linear`来预测类别：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ (B, T) -> (B, T, D)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T) -> (B, T, D)
- en: ❷ (B, T, D) -> (B, 1, D)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, D) -> (B, 1, D)
- en: ❸ (B, 1, D) -> (B, D)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, 1, D) -> (B, D)
- en: 'The next two lines of code plot the results. Looking at the accuracy of these
    two approaches, we see that the GRU performs better. If you run this multiple
    times, you might find that the average embedding approach sometimes starts to
    overfit the data. So at *first glance*, this alternative was not worthwhile:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的两行代码绘制了结果。查看这两种方法的准确率，我们发现GRU表现更好。如果你多次运行，你可能会发现平均嵌入方法有时开始对数据进行过拟合。所以，**乍一看**，这种替代方案并不值得：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/CH12_UN02_Raff.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_UN02_Raff.png)'
- en: 'But we should ask why the averaged embeddings work at all. We implemented this
    approach by *ignoring* something we know to be true about the data: that it has
    a sequential order and that the order matters. You can’t just rearrange the words
    in a sentence and get an intelligible result.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们应该问为什么平均嵌入方法能起作用。我们通过*忽略*关于数据我们知道的一些真实情况来实现这种方法：它有一个顺序，并且顺序很重要。你不能只是重新排列句子中的单词，就能得到一个可理解的结果。
- en: The averaged embedding approach can get reasonable accuracy because there are
    often ways to cheat at a problem. For example, the four classes of AG news are
    World, Sports, Business, and Sci/Tech. So if your model sees a sentence with the
    words “NFL,” “touchdown,” and “win,” it does not really matter what order those
    words occurred in. You can probably guess from the existence of the words alone
    that this is a Sports article. Similarly, if you see the words “banking” and “acquisition,”
    you can tell there is a good chance it’s a Business article without knowing anything
    else about the sentence.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 平均嵌入方法可以得到合理的准确率，因为通常有办法在问题中作弊。例如，AG新闻的四个类别是世界、体育、商业和科技。所以如果你的模型看到包含“NFL”、“touchdown”和“win”这些单词的句子，这些单词出现的顺序实际上并不重要。你仅从这些单词的存在就能猜测这可能是一篇体育文章。同样，如果你看到“banking”和“acquisition”这些单词，你就可以判断有很高的可能性这是一篇商业文章，而不需要了解句子的其他内容。
- en: 'The average embeddings can get reasonable accuracy because we don’t always
    *need* our prior to do OK. The potential benefit of the average embeddings comes
    into play when we look at the time it takes to train the model. The following
    code re-creates the figure but with `total_time` on the x-axis. This makes it
    clear that training the average embeddings is around three times faster than the
    GRU model. If you had a huge dataset where training a GRU could take a week, a
    3× speedup becomes pretty attractive:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 平均嵌入可以得到合理的准确率，因为我们并不总是“需要”我们的先验知识来做得好。平均嵌入的潜在好处在于我们观察模型训练所需的时间。以下代码重新创建了图表，但将`total_time`放在x轴上。这清楚地表明，训练平均嵌入大约比GRU模型快三倍。如果你有一个巨大的数据集，训练GRU可能需要一周时间，那么3倍的速度提升就非常吸引人了：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/CH12_UN03_Raff.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN03_Raff.png)'
- en: 'Averaging the embeddings ultimately violates a prior belief that the data has
    an order and the order is important, in exchange for a simpler and faster model.
    I often build models like this simply because I’m working on a large amount of
    data and I want to get some initial results more quickly. But this is also an
    exercise in a larger task: learning to recognize when you have an acceptable model
    that violates things you believe.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对嵌入进行平均最终违反了一个先前的信念，即数据有顺序且顺序很重要，以换取一个更简单、更快的模型。我经常构建这样的模型，仅仅因为我正在处理大量数据，并且我希望更快地得到一些初步结果。但这也是一个更大任务的练习：学习识别当你有一个可接受的模型，但违反了你的一些信念时。
- en: In this specific scenario, we can explain *why* a model that violates what we
    believe (time is important) works (single words are too informative). In the future,
    you may find that a dumber model like this performs suspiciously well, and a good
    practitioner shouldn’t take good result at face value. If you can’t rationalize
    *why* a model that violates this assumption is working, you should do a deep dive
    into your data until you can. Look at the examples the model gets correct and
    wrong, and try to figure out how the model is learning to do the job. Are there
    any obvious patterns to the data points the model gets wrong or right? Is there
    something in the input data that should not be there? Does something innocuous
    like the length correlate surprisingly well with the goal?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定场景中，我们可以解释为什么一个违反我们信念（时间很重要）的模型（单个单词信息量太大）能够工作。在未来，你可能会发现这样一个更简单的模型表现异常出色，而一个优秀的从业者不应该仅仅根据结果的好坏来评价模型。如果你无法合理地解释为什么一个违反这个假设的模型能够工作，你应该深入挖掘你的数据，直到你能够解释为止。看看模型做对和做错的地方，并试图弄清楚模型是如何学习完成这项工作的。模型做错或做对的数据点中是否存在任何明显的模式？输入数据中是否有不应该存在的东西？有没有一些无害的因素，比如长度与目标之间意外地很好地相关？
- en: I can’t tell you in advance *how* to do this process other than to be *incredulous*
    about your model’s success. Put yourself in the mindset of a cheater, and try
    to figure out ways to cheat at the problem. Label some data by hand and see if
    you can come up with different ideas for getting answers, and look at how your
    approach matches your model’s biases. For example, if you can label a sentence
    by picking out just a few keywords, the sequential order of the words isn’t as
    critical as you thought.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法提前告诉你如何进行这个过程，除了对你的模型成功感到惊讶之外。把自己放在一个作弊者的心态中，试图找出作弊的方法。手动标记一些数据，看看你是否能想出不同的方法来得到答案，并看看你的方法与你的模型偏差如何匹配。例如，如果你可以通过挑选几个关键词来标记一个句子，那么单词的顺序就不像你想象的那么重要了。
- en: In doing this process, you may come across a case of *information leakage*,
    where information about the label is erroneously or unrealistically seeping into
    your training data. Discovering this is a good thing and means you need to fix
    the information leakage before doing any more model development, because it will
    taint any model you try to build. In my experience, overly simple modeling that
    works far better than I expected almost always denotes leakage that I eventually
    find.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这个过程时，你可能会遇到“信息泄露”的情况，即关于标签的信息错误或不切实际地渗入你的训练数据中。发现这一点是好事，这意味着在继续进行任何更多模型开发之前，你需要修复信息泄露，因为它会污染你试图构建的任何模型。根据我的经验，过于简单的模型比预期工作得更好，几乎总是表明我最终会发现的信息泄露。
- en: How does information leakage happen?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 信息泄露是如何发生的？
- en: Information leakage can happen infinite possible ways when you’re building your
    data if something strongly *correlated* with your label y exists in the data but
    the correlation does not naturally exist. Instead, it is an artifact of the process
    used to create and organize the data, often due to a bug in the data preparation.
    People often accidentally *place the label y into the input features x*, which
    the model quickly learns to exploit!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在构建数据时，如果数据中存在与你的标签y强*相关*的某些内容，但相关性并不自然存在，信息泄露就可能以无限可能的方式发生。相反，它是创建和组织数据过程中产生的错误，通常是由于数据准备中的错误。人们经常无意中将标签y*放置在输入特征x中*，模型很快就会学会利用这一点！
- en: I’ll give you an example from some of my published work on building malware
    detection models.[^a](#fn45) Many people were using data from a clean install
    of Microsoft Windows to represent benign or safe data. Then they found some malware
    online and built a classifier that appeared to have *near perfect* accuracy. When
    we dove into the data, we found that almost everything Microsoft releases comes
    with the string “Copyright Microsoft Corporation.” This string ended up *leaking*
    information that the label was “benign” because it occurred only in benign data
    and never in malware data. But in reality, the string “Copyright Microsoft Corporation”
    has nothing to do with a file being malware or not, so the models would not work
    well on new data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从我在构建恶意软件检测模型方面发表的一些工作中举一个例子。[^a](#fn45) 许多人使用微软Windows干净安装的数据来表示良性或安全数据。然后他们在网上找到了一些恶意软件，并构建了一个看起来*几乎完美*的准确率的分类器。当我们深入研究数据时，我们发现几乎微软发布的所有内容都带有“版权所有
    微软公司”的字符串。这个字符串最终*泄露*了标签是“良性”的信息，因为它只出现在良性数据中，从未出现在恶意软件数据中。但在现实中，“版权所有 微软公司”的字符串与文件是否为恶意软件无关，因此模型在新数据上表现不佳。
- en: Information leakage can also occur in silly or subtle ways. There is an apocryphal
    tale ([https://www.gwern.net/Tanks](https://www.gwern.net/Tanks)) about the military
    wanting to build an ML model to detect if a tank was present in an image sometime
    between 1960 and 2000\. The story goes (as I first heard it) that they collected
    a bunch of images of tanks and images of empty fields and trained a network that
    got perfect accuracy. But every picture of a tank had the sun present in the image,
    and the non-tank images did not include the sun. The model learned to detect the
    sun instead of tanks, because it was easier to recognize. This never really happened,
    but the story is a fun tale about how information can leak in unexpected ways.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 信息泄露也可能以愚蠢或微妙的方式发生。有一个关于军事想要在1960年至2000年之间某个时间检测图像中是否存在坦克的ML模型的轶事([https://www.gwern.net/Tanks](https://www.gwern.net/Tanks))。故事是这样的（我第一次听到时是这样的）：他们收集了大量坦克和空地的图片，并训练了一个网络，该网络获得了完美的准确率。但每张坦克的图片都有太阳，而非坦克的图片没有太阳。模型学会了检测太阳而不是坦克，因为识别太阳更容易。这从未真正发生过，但这个故事是一个关于信息如何以意想不到的方式泄露的有趣故事。
- en: '* * *'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^a E. Raff et al., “An investigation of byte n-gram features for malware classification,”
    *J Comput Virol Hack Tech*, vol. 14, pp. 1–20, 2018, [https://doi.org/10.1007/s11416-016-0283-1](https://doi.org/10.1007/s11416-016-0283-1).[↩](#fnref45)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ^a E. Raff等人，“对恶意软件分类中字节n-gram特征的调查”，*J Comput Virol Hack Tech*，第14卷，第1-20页，2018年，[https://doi.org/10.1007/s11416-016-0283-1](https://doi.org/10.1007/s11416-016-0283-1)。[↩](#fnref45)
- en: 12.2.1  Weighted average over time with attention
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1  时间加权平均与注意力
- en: 'While the code we implemented earlier works, it is *slightly* incorrect. Why?
    Because `nn.AdaptiveAvgPool2d` has no idea about the padded inputs, which result
    in embeddings of the value 0\. This has the impact of shrinking the magnitude
    of shorter sequences. Let’s take a look at how. Suppose, for simplicity, that
    we are embedding tokens into just one dimension. So maybe we have three data points
    that look like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们之前实现的代码是可行的，但它*稍微*有些不正确。为什么？因为`nn.AdaptiveAvgPool2d`对填充输入一无所知，这导致嵌入的值为0。这会影响到较短的序列的幅度。让我们看看这是如何发生的。为了简单起见，假设我们将标记嵌入到一维中。那么我们可能有三组看起来像这样的数据点：
- en: '![](../Images/CH12_F01_EQ01.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F01_EQ01.png)'
- en: 'When we compute the average value for each item, we want to get this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算每个项目的平均值时，我们希望得到这个：
- en: '![](../Images/CH12_F01_EQ02.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F01_EQ02.png)'
- en: But we have padded everything in this batch to have the same length as the longest
    item in the batch! The padding values are all zeros, so what is computed becomes
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们在这一批中填充了所有内容，使其长度与批次中最长的项目相同！填充值都是零，所以计算结果变为
- en: '![](../Images/CH12_F01_EQ03.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F01_EQ03.png)'
- en: This has significantly changed the values of *x̄*[1] and *x̄*[3]! To fix this,
    we need to implement the averaging ourselves in the same way we computed the context
    vector for our attention mechanism in the previous chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这显著改变了*x̄*[1]和*x̄*[3]的值！为了解决这个问题，我们需要像在上一章中计算注意力机制的上下文向量一样，我们自己实现平均。
- en: But if we compute that context vector, why not apply the attention mechanism?
    The attention will learn a weighted average of all the input words and could hypothetically
    learn to ignore certain words based on all the information available.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们计算那个上下文向量，为什么不应用注意力机制呢？注意力将学习所有输入词的加权平均，并且理论上可以学习根据所有可用信息忽略某些词。
- en: To try this, we will implement a new `Module` that does the embedding and computes
    a mask based on which values were padded or not. Since the mask has a shape of
    (*B*,*T*), we can take the sum over the second time dimension to know how many
    valid items are in each batch. Then we can sum over all the items over time, divide
    by the appropriate value, and get the context vector to pass along to our previously
    defined attention mechanism.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试这个，我们将实现一个新的`Module`，它执行嵌入并基于哪些值被填充或未填充来计算一个掩码。由于掩码的形状为(*B*,*T*)，我们可以通过对第二个时间维度求和来知道每个批次中有多少有效项。然后我们可以对时间上的所有项求和，除以适当的值，并得到传递给之前定义的注意力机制的上下文向量。
- en: 'The following code implements an `EmbeddingAttentionBad` class that does the
    work of putting each input token through an embedding layer, running some hidden
    layers (with shared weights across time), and then applying one of our attention
    mechanisms from chapter 10 to compute a weighted average result. It needs to know
    the `vocab_size` and the embedding dimension `D` as arguments, with an optional
    `embd_layers` to change the number of hidden layers and `padding_idx` to tell
    it what values are used to denote padding:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了一个`EmbeddingAttentionBad`类，该类负责将每个输入标记通过嵌入层，运行一些具有时间共享权重的隐藏层，然后应用第10章中我们的一种注意力机制来计算加权平均结果。它需要知道`vocab_size`和嵌入维度`D`作为参数，可选的`embd_layers`用于更改隐藏层的数量，以及`padding_idx`来告知它用于表示填充的值：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ (B, T, D) -> (B, T, D)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T, D) -> (B, T, D)
- en: ❷ Functions defined in chapter 10
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第10章中定义的函数
- en: ❸ All entries are True. mask is shape (B, T).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 所有条目都是True。掩码的形状为(B, T)。
- en: ❹ (B, T, D)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, T, D)
- en: ❺ (B, T, D)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, T, D)
- en: ❻ Averages over time. (B, T, D) -> (B, D).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对时间进行平均。(B, T, D) -> (B, D)。
- en: ❼ If we wanted to do normal averaging, we could return the context variable
    right now! ((B, T, D), (B, D)) -> (B, D).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果我们想要进行正常平均，我们现在就可以返回上下文变量了！((B, T, D), (B, D)) -> (B, D)。
- en: 'With this new module, we can build a simple new network in the following code
    block. It starts with the `EmbeddingAttentionBag` that computes the embedding,
    hidden layers shared over time, and attention. Then we follow up a hidden layer
    and `nn.Linear` to produce a prediction:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新模块，我们可以在以下代码块中构建一个简单的新的网络。它从计算嵌入、时间共享的隐藏层和注意力的`EmbeddingAttentionBag`开始。然后我们跟随一个隐藏层和`nn.Linear`来生成预测：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Now we can define a simple model!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 现在我们可以定义一个简单的模型了！
- en: ❷ (B, T) -> (B, D)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T) -> (B, D)
- en: 'Now we can plot the results. The attention-based embedding takes a little longer
    to train but is still over 2× faster than the RNN. This makes sense since the
    attention version is doing more operations. We can also see that attention has
    improved our model’s accuracy, getting very close to the GRU. It suffers from
    more overfitting as the attention embedding’s accuracy begins to plummet with
    more updates:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制结果。基于注意力的嵌入训练时间稍长，但仍然比RNN快2倍以上。这是有道理的，因为注意力版本执行了更多的操作。我们还可以看到，注意力提高了我们模型的准确性，接近GRU。随着注意力嵌入的准确性随着更新次数的增加而急剧下降，它更容易过拟合：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/CH12_UN04_Raff.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_UN04_Raff.png)'
- en: 'The biggest issue with the attention embedding is *still* that it lacks any
    sense about the order of words in the sentence. This is the time component of
    the model that is lacking. To further highlight it, let’s show another way to
    look at the problem. Consider the input sentence “the red fox chased the blue
    dog.” Our model embeds each word into its vector representation, and we are ultimately
    performing some kind of averaging over the embeddings (weighted or unweighted,
    depending on whether you used attention). Let’s look at how this sentence works
    if we have an embedding dimension of *D* = 1 and the dimension’s value is set
    to integers. That gives us the following setup:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力嵌入的最大问题*仍然*是它对句子中单词的顺序没有任何感知。这是模型中缺失的时间成分。为了进一步强调这一点，让我们展示另一种看待问题的方法。考虑输入句子“红色的狐狸追逐蓝色的狗。”我们的模型将每个单词嵌入到其向量表示中，我们最终在嵌入（加权或未加权，取决于你是否使用了注意力）上执行某种平均。让我们看看如果嵌入维度为*D*
    = 1且该维度的值设置为整数时，这个句子是如何工作的。这给我们以下设置：
- en: '![](../Images/CH12_F01_EQ04.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F01_EQ04.png)'
- en: 'This equation shows an unweighted average, but making it a weighted average
    would just change the value at the end—the fundamental flaw of not understanding
    the sequential nature of the data remains. So, for example, if we switch “fox”
    and “dog” in the sentence, the meaning changes—but the embedding approach will
    *return the same result*. This is shown in the following equation and demonstrates
    how the network cannot detect subtle changes in meaning:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个等式展示了一个未加权的平均值，但将其变为加权平均值只会改变最终的结果——不理解数据顺序性的基本缺陷仍然存在。因此，例如，如果我们交换句子中的“狐狸”和“狗”，意义就会改变——但嵌入方法将*返回相同的结果*。这在下述等式中得到体现，并展示了网络无法检测到意义的微妙变化：
- en: '![](../Images/CH12_F01_EQ05.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F01_EQ05.png)'
- en: 'This can be a particular problem if there is a risk of very noisy data. The
    next example randomly rearranges all the words in the sentence. The meaning of
    the sentence is lost, but the model still insists that nothing has changed. All
    of these examples are equivalent to the embedding-based models:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在非常嘈杂数据的风险，这可能会成为一个特别的问题。下一个示例随机重排句子中的所有单词。句子的意义已经丢失，但模型仍然坚持认为没有任何变化。所有这些例子都与基于嵌入的模型相当：
- en: '![](../Images/CH12_F01_EQ06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F01_EQ06.png)'
- en: The more important sequence order is for solving a problem, the more the current
    approach of averaging the vectors will struggle to make a correct classification.
    This applies to the attention-based version as well, because it is not aware of
    order. Each word **h**[i] is assigned a weight *α*[i] based on a context ![](../Images/bar_h.png),
    so it does not see the information that is missing! This makes the utility of
    the averaging approach very problem-specific for complex problems. Think about
    any situation where negation language like “not,” “isn’t,” or “didn’t” is used
    in a sentence. With negation, language order is critical. These are the kinds
    of things you should think about as the costs of getting faster training time
    with an embedding approach.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解决问题来说，序列顺序越重要，当前平均向量的方法就越难以做出正确的分类。这也适用于基于注意力的版本，因为它没有意识到顺序。每个单词**h**[i]根据上下文![图片](../Images/bar_h.png)分配一个权重*α*[i]，因此它看不到缺失的信息！这使得平均方法在复杂问题上的实用性非常特定于问题。想想看，在句子中使用否定语言，如“不”、“不是”或“没有”的任何情况。有了否定，语言顺序至关重要。这些都是你应该考虑的事情，作为使用嵌入方法获得更快的训练时间的代价。
- en: 'Note Since I’m talking about the cons, it’s still worth noting that the attention-based
    embeddings got *very* close to the GRU’s accuracy. What does that mean? If I saw
    these results on a dataset, I would have two initial hypotheses: (1) the order
    of my data is not as important as I thought for the problem at hand, or (2) I
    need a larger RNN with more layers or more hidden neurons to capture the complex
    information present in the order of the data. I would investigate hypothesis #2
    first by training a bigger model, because that just takes compute time. Investigating
    hypothesis #1 takes my personal time to dig into the data, and I usually value
    my time over a computer’s. If there is a way to answer the question by running
    a new model, I prefer to do that.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于我在谈论缺点，仍然值得指出的是，基于注意力的嵌入与GRU的准确性非常接近。这意味着什么？如果我在数据集上看到这些结果，我会有两个初步假设：(1)我的数据顺序对于当前问题并不像我想象的那么重要，或者(2)我需要一个更大的RNN，有更多的层或更多的隐藏神经元来捕捉数据顺序中存在的复杂信息。我会首先通过训练更大的模型来调查假设#2，因为这只需要计算时间。调查假设#1需要我个人的时间来深入挖掘数据，而我通常更重视我的时间而不是电脑的时间。如果有一种通过运行新模型来回答问题的方法，我更喜欢这样做。
- en: 12.3 Pooling over time and 1D CNNs
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 时间上的池化和一维CNN
- en: 'Since we have identified the lack of sequence order information as a serious
    bottleneck to better results, let’s try a different strategy that keeps *some*
    of the time information but not all of it. We have already learned about and used
    convolutions, which include a spatial prior: that things near each other are likely
    to be related. That captures a lot of the sequential ordering that our previous
    embedding approach lacked. RNNs and 1D convolutions operate on similarly shaped
    tensors, as shown in figure 12.2.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经确定序列顺序信息的缺乏是导致结果更好的一个严重瓶颈，让我们尝试一种不同的策略，它保留了一些时间信息，但不是全部。我们已经了解并使用了卷积，它包括一个空间先验：相邻的事物很可能相关。这捕捉了我们之前嵌入方法所缺乏的很多序列顺序。RNN和一维卷积在形状上相似，如图12.2所示。
- en: '![](../Images/CH12_F02_Raff.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F02_Raff.png)'
- en: Figure 12.2 Example showing the tensor shapes used by RNNs (left) and 1D convolutions
    (right), without the first batch dimension. In each case, we have one axis for
    the tensor representing something spatial(T versus W) and one representing features
    at a location (D versus C).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2示例显示了RNN（左）和一维卷积（右）使用的张量形状，没有第一个批量维度。在每个情况下，我们都有一个轴表示张量代表的空间（T与W），另一个表示位置的特性（D与C）。
- en: This shows that 1D convolutions and RNNs have the same shape but slightly different
    meanings *assigned* to each axis. The RNN’s *spatial* dimension is at index 1
    for the time axis T. The 1D convolution’s spatial dimension is at index 2 with
    the width of the input data (similar to the width of an image!). For the spatial
    prior, RNNs are assigning the belief that the exact order of all items is important.
    By contrast, a convolution is assigning the belief that only nearby items are
    related. Both an RNN and a CNN encode feature information about a specific item
    in the sequence; they simply assign different interpretations to that data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明一维卷积和RNN具有相同的形状，但每个轴分配的**意义**略有不同。RNN的**空间**维度在时间轴T的索引1处。一维卷积的空间维度在索引2处，宽度与输入数据（类似于图像的宽度！）相同。对于空间先验，RNN分配了这样的信念：所有项目的确切顺序都是重要的。相比之下，卷积分配的信念是只有相邻的项目是相关的。RNN和CNN都编码了序列中特定项目的特征信息；它们只是对那些数据分配了不同的解释。
- en: The 1D convolution’s sense of time and order is weaker than that of an RNN,
    but it is *better than nothing*. For example, negation words usually occur right
    before the word they are trying to negate, so 1D CNNs could capture that simple
    spatial relationship. If we can rearrange our tensor so that the spatial dimension
    matches what the convolution expects, we can use a 1D CNN to capture some information
    about time and use it as our classifier. Figure 12.3 shows that we can do this
    rearrangement by permuting the axes of the tensor.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一维卷积对时间和顺序的感觉比RNN弱，但总比没有好。例如，否定词通常出现在它们试图否定的词之前，因此一维CNN可以捕捉到这种简单的空间关系。如果我们能重新排列我们的张量，使其空间维度与卷积期望的相匹配，我们就可以使用一维CNN来捕捉一些关于时间的信息，并将其用作我们的分类器。图12.3显示了我们可以通过交换张量的轴来进行这种重新排列。
- en: '![](../Images/CH12_F03_Raff.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F03_Raff.png)'
- en: Figure 12.3 Converting an RNN’s tensor shape into one suitable for use with
    a 1D convolution. We use the `.permute` function to swap the second and third
    axes (leaving the batch axis alone). This moves the spatial dimension into the
    location a 1D convolution would expect.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 将RNN的张量形状转换为适合使用一维卷积的形状。我们使用 `.permute` 函数交换第二和第三轴（保留批次轴）。这会将空间维度移动到一维卷积期望的位置。
- en: Instead of 2D convolutions over images, we can use 1D convolutions over words.
    The `nn.Conv1d` layer looks at data as (*B*,*C*,*W*), where B is still the batch
    size, C is the number of channels, and W is the width (or words) of the input.
    The result of an embedding layer is (*B*,*T*,*D*), where T is the length of the
    sequence. If we reorder the output to be (*B*,*D*,*T*), it will fit the general
    expectations of a 1D convolution. We can use this trick to change basically *any*
    problem for which we would use an RNN into one where we can apply a convolutional
    network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与在图像上使用二维卷积不同，我们可以在单词上使用一维卷积。`nn.Conv1d` 层将数据视为 (*B*,*C*,*W*)，其中 B 仍然是批次大小，C
    是通道数，W 是输入的宽度（或单词数）。嵌入层的输出是 (*B*,*T*,*D*)，其中 T 是序列的长度。如果我们重新排列输出为 (*B*,*D*,*T*)，它将符合一维卷积的一般期望。我们可以使用这个技巧将基本上
    *任何* 我们原本会使用 RNN 的问题转换为可以应用卷积网络的问题。
- en: 'But there is still one problem. For all the CNNs we have been working with,
    the input has been a fixed size: for example, MNIST is always a 28 × 28 image.
    But the sequence length T is variable for our problem! Again, we can use adaptive
    pooling to help us resolve this situation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但仍然有一个问题。对于我们一直在使用的所有 CNN，输入都是一个固定大小：例如，MNIST 总是 28 × 28 的图像。但我们的问题中序列长度 T 是可变的！同样，我们可以使用自适应池化来帮助我们解决这个问题。
- en: 'The final shape of our tensor, after rounds of 1D convolutions, activation
    functions, and normal pooling, will be (*B*,*D*′,*T*′), where *D*′ and *T*′ indicate
    that the number of channels (*D*′) and length of the sequence (*T*′) may have
    been altered by our convolutions and normal pooling, respectively. If we use adaptive
    pooling over just the last dimension, we can reduce the shape to (*B*,*D*′,1),
    which will be the same size regardless of how long the original input was. If
    we do this with adaptive *max* pooling, we select the largest activation for each
    channel as the representative of that channel for the final classification problem.
    That means we can switch to using a linear layer afterward and perform the classification
    problem we want, because the shape will be consistent from this point forward!
    The following code shows all of this put together to define a 1D CNN for classifying
    our sequences:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一系列的一维卷积、激活函数和正常池化操作后，我们张量的最终形状将是 (*B*,*D*′,*T*′)，其中 *D*′ 和 *T*′ 表示通道数 (*D*′)
    和序列长度 (*T*′) 可能已经被我们的卷积和正常池化操作所改变。如果我们只对最后一个维度使用自适应池化，我们可以将形状减少到 (*B*,*D*′,1)，这将与原始输入的长度无关。如果我们使用自适应
    *max* 池化，我们将选择每个通道的最大激活作为该通道在最终分类问题中的代表。这意味着我们可以在之后切换到使用线性层并执行我们想要的分类问题，因为从这一点开始形状将保持一致！下面的代码展示了如何将这些操作组合起来定义一个用于分类序列的一维卷积神经网络：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ I’m being lazy; we should make k_size an argument, too.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我有点偷懒；我们也应该将 k_size 作为参数。
- en: ❷ (B, T) -> (B, T, D)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T) -> (B, T, D)
- en: ❸ (B, T, D) -> (B, D, T)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T, D) -> (B, D, T)
- en: ❹ We pretend that D is the number of channels in this new interpretation of
    the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们假设 D 是在这份数据的新解释中通道的数量。
- en: ❺ (B, D, T) -> (B, D, T/2)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, D, T) -> (B, D, T/2)
- en: ❻ (B, 2*D, T/2) -> (B, 2*D, T/4)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ (B, 2*D, T/2) -> (B, 2*D, T/4)
- en: ❼ Now that we have done some rounds of pooling and convolution, reduce the tensor
    shape to a fixed length. (B, 4*D, T/4) -> (B, 4*D, 1)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 现在我们已经进行了一些池化和卷积操作，将张量形状减少到固定长度。(B, 4*D, T/4) -> (B, 4*D, 1)
- en: ❽ (B, 4*D, 1) -> (B, 4*D)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ (B, 4*D, 1) -> (B, 4*D)
- en: 'Note Normally, we can choose max or average pooling based on individual preference.
    In this case, there is a good reason to prefer `nn.AdaptiveMaxPool1d` instead
    of `nn.AdaptiveAvgPool1d`: not all items in the batch will have the same length,
    and the batch items that received padding will return a vector of zero values.
    That means the activations where padding occurred will likely have smaller values
    and thus not be selected by the max-pooling operations. This helps our architecture
    work properly even though we are ignoring different inputs having different lengths.
    This is a tricky workaround so we don’t have to think about padding as much.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通常，我们可以根据个人偏好选择最大池化或平均池化。在这种情况下，有很好的理由选择`nn.AdaptiveMaxPool1d`而不是`nn.AdaptiveAvgPool1d`：批处理中的所有项目不一定具有相同的长度，并且接收填充的批处理项目将返回一个零值的向量。这意味着填充发生处的激活可能具有较小的值，因此可能不会被最大池化操作选中。这有助于我们的架构正常工作，即使我们忽略了不同输入具有不同长度的情况。这是一个棘手的解决方案，这样我们就不必过多考虑填充问题。
- en: 'The strategy we just coded up, converting a sequential problem with an RNN
    into a 1D convolution, is *very* popular for classification tasks. It gets us
    a lot of spatial information while allowing us to train faster. The following
    code plots the results:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚编写的策略，将具有RNN的序列问题转换为1D卷积，在分类任务中非常受欢迎。它为我们提供了大量的空间信息，同时允许我们更快地训练。以下代码显示了结果：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH12_UN08_Raff.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN08_Raff.png)'
- en: After running the CNN with adaptive pooling, some results show a variety of
    pros and cons. On the positive side, the CNN does not appear to be overfitting,
    meaning its performance may improve even more if we train for more epochs. This
    also means the final accuracy of the CNN is better than that of the averaged embeddings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行带有自适应池化的CNN后，一些结果显示了各种优点和缺点。在积极的一面，CNN似乎没有过度拟合，这意味着如果我们训练更多轮次，其性能可能会进一步提高。这也意味着CNN的最终准确率比平均嵌入更好。
- en: Our CNN is also pretty naive right now. Just as we improved 2D CNNs with more
    layers, residual connections, and other tricks, we could reimplement those methods
    (e.g., residual connections) and apply them here to get better results. This gives
    us an advantage over RNNs, which don’t tend to improve much as we add all these
    bells and whistles we’ve learned about.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的CNN也非常简单。正如我们通过添加更多层、残差连接和其他技巧来改进2D CNNs一样，我们可以重新实现这些方法（例如，残差连接）并将它们应用到此处以获得更好的结果。这使我们比RNN有优势，因为RNN不太可能随着我们添加所有这些我们学到的花哨功能而有很大改进。
- en: On the more disappointing side, the CNN’s peak accuracy is not as good as either
    of the averaged embeddings *on this dataset*. We already know that the spatial
    nature of the data is not critical because of how well the naive embedding approach
    has done. The more important the spatial information is to the problem, the more
    we would expect this CNN approach to perform better than the embedding approach.
    But training both models can give you some initial information about how much
    order matters in your data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在更令人失望的一侧，CNN的峰值准确率并不如这个数据集上的平均嵌入那么好。我们已经知道，由于原始嵌入方法做得很好，数据的空间特性并不关键。空间信息对问题越重要，我们越期望这种CNN方法比嵌入方法表现更好。但训练这两个模型可以给你一些关于数据中顺序重要性的初步信息。
- en: 12.4 Positional embeddings add sequence information to any model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 位置嵌入为任何模型添加序列信息
- en: RNNs capture all the sequential nature of the input data, and we’ve now seen
    that CNNs can capture some subset of that sequential-ness. Our embeddings are
    fast and potentially very accurate but often overfit the data since they lack
    this sequential information. This is hard to fix because the sequential information
    comes from our model design (i.e., using an RNN or CNN layer).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs能够捕捉输入数据的所有序列特性，而我们已看到CNNs可以捕捉其中的一些序列特性子集。我们的嵌入速度快且可能非常准确，但通常由于缺乏这种序列信息而过度拟合数据。这很难修复，因为序列信息来自于我们的模型设计（即使用RNN或CNN层）。
- en: But instead of relying on the architecture to capture sequential information,
    what if there was a way to embed the sequential information into the embeddings?
    If the embeddings themselves contain information about their relative order, can
    we improve the results of our algorithms? That’s the idea behind a recent technique
    called a *positional encoding*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们能够将序列信息嵌入到嵌入中，而不是依赖于架构来捕捉序列信息，那会怎么样？如果嵌入本身包含关于它们相对顺序的信息，我们能否改进算法的结果？这正是最近一种称为*位置编码*的技术背后的想法。
- en: Figure 12.4 shows how this process works. We represent the location t (e.g.,
    first, second, third, etc.) as a vector. Then we add those vectors to the inputs
    so the inputs contain themselves and information about where each item is in the
    input. This encodes sequential information into the network’s inputs, and now
    it is on the network to learn how to extract and use the information about time.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4展示了这个过程是如何工作的。我们将位置t（例如，第一、第二、第三等）表示为一个向量。然后我们将这些向量加到输入上，使得输入包含自身以及每个项目在输入中的位置信息。这将在网络的输入中编码顺序信息，现在网络需要学习如何提取和使用关于时间的信息。
- en: '![](../Images/CH12_F04_Raff.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F04_Raff.png)'
- en: Figure 12.4 Average embedding approach that we started with, augmented with
    positional embeddings. Separate from the inputs, vector values encode information
    about our location within a larger sequence. This is added to the vectors representing
    the inputs, creating a mix of content and sequence information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4我们最初使用的平均嵌入方法，增加了位置嵌入。与输入分开，向量值编码了我们在一个较大序列中的位置信息。这被添加到表示输入的向量中，创建了一个内容和序列信息的混合。
- en: 'The big question is, how do we create this mythical encoding? It is a slightly
    mathy approach, but not too complicated. We need to define some notation so we
    can discuss it. Let’s use **h**[i] ∈ ℝ^D to represent the embedding (from `nn.Embedding`)
    after feeding in token *x*[i]. Then we have a sequence of embeddings **h**[1],
    **h**[2], …, **h**[t], …, **h**[T]. We want a position vector *P*(*t*), which
    we can add to our embeddings to create an improved embedding ![](../Images/tilde_h.png)[t]
    that contains information about the original content **h**[t] and its location
    as the tth item in the input—something like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的问题是，我们如何创建这种神话般的编码？这是一个稍微有些数学的方法，但并不复杂。我们需要定义一些符号以便讨论。让我们用**h**[i] ∈ ℝ^D来表示在输入标记*x*[i]后得到的嵌入（来自`nn.Embedding`）。然后我们有一个嵌入序列**h**[1]，**h**[2]，…，**h**[t]，…，**h**[T]。我们想要一个位置向量*P*(*t*)，我们可以将其添加到我们的嵌入中，以创建一个改进的嵌入![~h](../Images/tilde_h.png)[t]，它包含关于原始内容**h**[t]及其作为输入中的第t个项目的位置信息——类似于这样：
- en: '![](../Images/CH12_UN09_Raff.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN09_Raff.png)'
- en: 'Then we can proceed with ![](../Images/tilde_h.png)[1], ![](../Images/tilde_h.png)[2],
    ..., ![](../Images/tilde_h.png)[t], ..., ![](../Images/tilde_h.png)[T] as the
    inputs to the rest of our network, knowing that the sequential nature has been
    placed inside the embeddings! It turns out we can do this with a surprisingly
    simple approach. We define a function for *P*(*t*) using the sin and cos functions,
    and we use the input to the sin and cos functions to represent the position of
    a vector t. To walk through how this works, let’s plot what sin (*t*) looks like:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以用![~h](../Images/tilde_h.png)[1]，![~h](../Images/tilde_h.png)[2]，…，![~h](../Images/tilde_h.png)[t]，…，![~h](../Images/tilde_h.png)[T]作为我们网络其余部分的输入，知道顺序性质已经被放入嵌入中！结果我们发现可以用一个令人惊讶简单的方法做到这一点。我们定义一个函数来表示*P*(*t*)，使用正弦和余弦函数，并使用正弦和余弦函数的输入来表示向量t的位置。为了说明这是如何工作的，让我们绘制正弦(*t*)看起来像什么：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/CH12_UN10_Raff.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN10_Raff.png)'
- en: The sin function oscillates up and down. If we have computed sin (*t*) = *y*,
    knowing y tells us something about what the input t may have been! Because sin
    (*π*⋅*c*) = 0 for all integers c, we can tell if we are some multiple of π in
    the sequence order (the *π*≈ third item, or the 2*π*≈ sixth item, or the 2*π*≈
    ninth item, etc). If *y* = 0, it is possible we were *t* = 3 ≈ *π* or *t* = 6
    ≈ 2 ⋅ *π*, but we can’t tell which of these specific positions we are in—only
    that we are in a position approximately a multiple of *π* ≈ 3.14! In this example,
    we have 100 possible positions, so we can tell that we are in one out of ≈ 32
    possible positions using this approach.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦函数上下波动。如果我们计算出sin(*t*) = *y*，知道y告诉我们输入t可能是什么！因为对于所有整数c，sin(*π*⋅*c*) = 0，我们可以判断我们是否在序列顺序中的某个π的倍数（π≈第三个项目，或2π≈第六个项目，或2π≈第九个项目等）。如果*y*
    = 0，那么我们可能在*t* = 3 ≈ π或*t* = 6 ≈ 2π，但我们无法确定我们处于这些特定位置中的哪一个——只知道我们处于大约π≈3.14的倍数位置。在这个例子中，我们有100个可能的位置，因此我们可以用这种方法判断我们处于约32个可能位置中的一个。
- en: 'We can improve the situation by adding a second sin call, but with a frequency
    component f. So we compute sin (*t*/*f*) with *f* = 1 (what we have already plotted)
    and with *f* = 10:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加第二个正弦调用来改善这种情况，但带有频率成分f。因此我们计算sin(*t*/f*)，其中f = 1（我们已绘制的）和f = 10：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/CH12_UN11_Raff.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN11_Raff.png)'
- en: With two values of f, it is possible to uniquely identify some positions. If
    sin (*t*) = 0 and sin (*t*/100) = 0, there are only four possible positions we
    could be in, *t* = 0, 31, 68, 94, which are the only four positions where both
    cases are (approximately) true. If sin (*t*) = 0 but sin (*t*/10) ≠ 0, then we
    know 0, 31, 68, and 94 are not options. In addition, the different values from
    the sin function are also informative about our position. If sin (*t*/10) = −
    1, we know we must be at position 48, because it is the only option where we get
    − 1 as the output, given a maximum time of *T* = 100.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个 f 的值，可以唯一地识别一些位置。如果 sin (*t*) = 0 且 sin (*t*/100) = 0，我们可能处于的四个可能位置只有 *t*
    = 0，31，68，94，这是唯一四个两种情况都（近似）为真的位置。如果 sin (*t*) = 0 但 sin (*t*/10) ≠ 0，那么我们知道 0，31，68
    和 94 不是选项。此外，正弦函数的不同值也关于我们的位置提供了信息。如果 sin (*t*/10) = − 1，我们知道我们必须在位置 48，因为它是唯一一个输出为
    − 1 的选项，给定最大时间 *T* = 100。
- en: This shows that if we keep adding frequencies f to the calculation, we can begin
    to infer from the combination of values our *exact* location within the input
    sequence. We define a position encoding function *P*(*t*) that returns a D dimensional
    vector by creating sin and cos values at different frequencies *f*[1], *f*[2],
    …, *f*[*D*/2]. We only need *D*/2 frequencies because we use both a sin and cos
    value for each frequency. That gives us
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，如果我们继续向计算中添加频率 f，我们就可以从值的组合中开始推断出输入序列中的**确切**位置。我们定义一个位置编码函数 *P*(*t*)，它通过在不同频率
    *f*[1]，*f*[2]，…，*f*[*D*/2] 处创建正弦和余弦值来返回一个 D 维向量。我们只需要 *D*/2 个频率，因为我们为每个频率使用一个正弦值和一个余弦值。这给我们
- en: '![](../Images/CH12_F04_EQ01.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F04_EQ01.png)'
- en: 'as the representation for our encoding vectors. But how do we define *f*[k]?
    The paper[¹](#fn46) that originally proposed this recommended using the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们编码向量的表示。但如何定义 *f*[k]？最初提出这一点的论文[¹](#fn46)建议使用以下：
- en: '![](../Images/CH12_F04_EQ02.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F04_EQ02.png)'
- en: 'Let’s see a quick example of what this looks like. For simplicity, we use *D*
    = 6 dimensions and just plot the sin components so the plot is not too crowded:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下这个例子是什么样的。为了简单起见，我们使用 *D* = 6 维度，并仅绘制正弦分量，以便图表不会太拥挤：
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Computes the frequency f in a numerically stable way
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以数值稳定的方式计算频率 f
- en: '![](../Images/CH12_UN12_Raff.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN12_Raff.png)'
- en: As we start adding more dimensions with more frequencies, it becomes easier
    to identify unique positions in time. Pick any location on the plot’s x-axis,
    and you can identify a unique combination of values for all six dimensions that
    will not be shared by any other position on the x-axis. This is what our network
    will use to extract information about the location from the data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始添加更多维度和更多频率时，识别时间中的唯一位置变得更容易。选择图表 x 轴上的任何位置，你都可以识别一个独特的六个维度的值组合，这些值将不会与其他
    x 轴上的位置共享。这就是我们的网络将用来从数据中提取关于位置信息的方法。
- en: This specific form of a positional encoding also has some nice mathematical
    properties that make it easier for neural networks to learn. For example, a single
    linear layer can learn to shift the position encoding by a fixed amount (i.e.,
    move to the left or right by t units), helping a network learn to perform logic
    over the time component.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种位置编码的具体形式也具有一些很好的数学特性，这使得神经网络更容易学习。例如，单个线性层可以学习通过固定量（即向左或向右移动 t 个单位）来移动位置编码，帮助网络学习在时间组件上执行逻辑。
- en: 12.4.1  Implementing a positional encoding module
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 实现位置编码模块
- en: 'We will add two more things to this approach that have been found helpful in
    practice. First, we do not want to give equal weight to both the content (original
    embedding **h**[t]) and the position *P*(*t*). So we use the following equation
    to up-weight the importance of the content over the position:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向这种方法添加两个在实践中发现有帮助的东西。首先，我们不希望对内容（原始嵌入 **h**[t]）和位置 *P*(*t*) 给予相同的权重。因此，我们使用以下方程来提高内容的相对重要性：
- en: '![](../Images/CH12_UN13_Raff.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN13_Raff.png)'
- en: 'Second, we include a dropout on the resulting vector so we do not learn to
    overfit to the fixed values of the positional encoding *P*(*t*). Now let’s define
    a new PyTorch `Module` that applies this positional encoding for us. The current
    implementation requires knowing the maximum sequence length T in advance, using
    the `max_len` argument in the constructor. I’ve borrowed the code for this positional
    encoding from the PyTorch examples at [http://mng.bz/B1Wl](http://mng.bz/B1Wl)[²](#fn47)
    and adapted it slightly:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们在结果向量上添加了dropout，这样我们就不会学习到对位置编码 *P*(*t*) 固定值的过拟合。现在让我们定义一个新的PyTorch `Module`，它为我们应用这个位置编码。当前实现需要提前知道最大序列长度
    T，使用构造函数中的`max_len`参数。我从PyTorch示例[http://mng.bz/B1Wl](http://mng.bz/B1Wl)[²](#fn47)
    中借用了这个位置编码的代码，并稍作修改：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Done so that when we call .to(device), this array will move to it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这样做是为了当我们调用 `.to(device)` 时，这个数组将移动到它那里。
- en: ❷ This code works on (T, B, D) data, so we need to reorder if the input is (B,
    T, D).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 此代码适用于 (T, B, D) 数据，因此如果输入是 (B, T, D)，则需要重新排序。
- en: ❸ Mixes the input and positional information
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 混合输入和位置信息
- en: ❹ Regularizes to avoid overfitting
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 正则化以避免过拟合
- en: ❺ Goes back to a (B, T, D) shape
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回到 (B, T, D) 形状
- en: 12.4.2  Defining positional encoding models
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 定义位置编码模型
- en: 'With our new positional encoding, we can redefine the simple averaging approach
    from before by inserting our `PositionalEncoding` class directly after the `nn.Embedding`
    layer. It does not impact the tensor’s shape in any way, so everything else can
    remain the same. It simply alters the values in the tensor with our new positional
    encoding *P*(*t*):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新的位置编码，我们可以重新定义之前简单的平均方法，通过在`nn.Embedding`层之后直接插入我们的`PositionalEncoding`类。它以任何方式都不会影响张量的形状，因此其他所有内容都可以保持不变。它只是用我们新的位置编码
    *P*(*t*) 改变了张量中的值：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ (B, T) -> (B, T, D)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T) -> (B, T, D)
- en: ❷ (B, T, D) -> (B, 1, D)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, D) -> (B, 1, D)
- en: ❸ (B, 1, D) -> (B, D)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, 1, D) -> (B, D)
- en: 'Adapting our attention-based embedding is also simple. When we defined`EmbeddingAttentionBag`,
    we added the optional argument `embd_layers`. If `embd_` `layers` is a PyTorch
    `Module`, it will use that network to run a hidden layer over the batch of (*B*,*T*,*D*)
    items. We define that `Module` ourselves, which will start with the`PositionalEncoding`
    module since the input to `embd_layers` is already embedded. That’s done in the
    following code in two parts. First we define `embd_layers` as the positional encoding
    followed by three rounds of hidden layers, and then we define `attnPosEmbd` to
    be an attention-based network with positional encoding. Now we can train both
    of these new positional averaging networks and compare them with the original
    averaging and attention-based versions. If the accuracy improves, then we know
    that the information order was indeed important:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 适应基于注意力的嵌入也很简单。当我们定义`EmbeddingAttentionBag`时，我们添加了可选参数`embd_layers`。如果`embd_layers`是一个PyTorch
    `Module`，它将使用该网络在 (*B*,*T*,*D*) 项的批次上运行一个隐藏层。我们定义这个`Module`，它将从`PositionalEncoding`模块开始，因为`embd_layers`的输入已经嵌入。这在以下代码中分为两部分完成。首先，我们将`embd_layers`定义为位置编码，然后是三轮隐藏层，然后我们定义`attnPosEmbd`为一个具有位置编码的基于注意力的网络。现在我们可以训练这两个新的平均网络，并将它们与原始的平均和基于注意力的版本进行比较。如果准确率有所提高，那么我们就知道信息顺序确实很重要：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ (B, T, D) -> (B, T, D)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T, D) -> (B, T, D)
- en: ❷ (B, T) -> (B, D)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T) -> (B, D)
- en: Positional Encoding Results
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码结果
- en: 'The following code plots the results for all the embedding models, with and
    without positional encodings. Our positional embeddings provide a significant
    benefit, accuracy is improved across the board, and there are less-severe dips
    in accuracy that indicate less overfitting. The impact on training time is also
    minuscule—the models take only a few more seconds to train than the originals:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制了所有嵌入模型的成果，包括带位置编码和不带位置编码的情况。我们的位置编码提供了显著的好处，整体准确率得到提升，且准确率下降幅度较小，这表明过拟合程度较低。对训练时间的影响也微乎其微——模型训练时间仅比原始模型多几秒钟：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/CH12_UN14_Raff.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN14_Raff.png)'
- en: These results provide evidence that our hypothesis about sequence order helping
    prevent overfitting was also correct, particular with the attention-based approach.
    Both versions using the positional encodings still fluctuate but do not drop in
    accuracy as quickly with more epochs of training. This shows how we can use a
    completely different kind of approach to encode sequence information into our
    models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果为我们关于序列顺序有助于防止过拟合的假设提供了证据，尤其是在基于注意力的方法中。使用位置编码的两种版本仍然有所波动，但与更多的训练轮数相比，准确率下降并不快。这展示了我们可以如何使用一种完全不同的方法将序列信息编码到我们的模型中。
- en: 'Our attention-based embedding is definitely a better idea: let’s compare just
    that to the earlier GRU results. With this combination, we begin to match or even
    outperform the GRU-based RNN in terms of accuracy, *and* it is over twice as fast
    to train! A pretty great combo. Here’s the code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于注意力的嵌入绝对是一个更好的想法：让我们仅比较这一点与早期的GRU结果。使用这种组合，我们在准确率方面开始与基于GRU的RNN相匹配，甚至超越，而且训练速度要快两倍以上！这是一个相当不错的组合。以下是代码：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH12_UN15_Raff.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN15_Raff.png)'
- en: 'The takeaway is that positional encodings are a cheap, fast, and effective
    way to encode sequence information into our networks. Surprisingly, positional
    encodings were not invented on their own but rather in conjunction with the topic
    of the next section of this chapter: transformers. So in most current deep learning,
    you won’t see a positional encoding used outside of transformers, but I have found
    them widely useful beyond transformers as a fast and easy way to endow a model
    with the concept of time/ordered data.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 可以得出的结论是，位置编码是将序列信息编码到我们的网络中的一种便宜、快速且有效的方法。令人惊讶的是，位置编码并非独立发明，而是在本章下一节的主题——转换器（transformers）——的背景下共同发明的。因此，在大多数当前的深度学习中，你不会看到在转换器之外使用位置编码，但我发现它们在转换器之外也非常有用，作为一种快速简单的方法，可以赋予模型时间/有序数据的概念。
- en: '12.5 Transformers: Big models for big data'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 转换器：大数据的大模型
- en: 'The last alternative to RNNs that we will learn about is the transformer architecture.
    It is built out of two primary subcomponents: positional encoding (which we just
    learned about) and *multiheaded attention*. Transformers are a very recent development
    and have become extremely popular due to some of their benefits. However, they
    tend to work best for big problems with *lots* of data (at least 50 GB, in my
    experience) and *lots* of compute (you’ll want 4+ GPUs), so we won’t see the full
    benefits in this chapter. Still, transformers are important to learn about and
    are the foundation for some of the current big advances in deep learning. The
    latest and greatest models for machine translation, question answering, few-shot
    learning, and dozens of NLP tasks are built with different transformer flavors.
    The goal of this section is to help you understand the standard, original, vanilla
    transformer with no special additions. After this chapter, I recommend reading
    Jay Alammar’s blog post “The Illustrated Transformer” ([http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))
    for a detailed step-by-step walk-through of a transformer.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要学习的最后一个RNN的替代方案是转换器架构。它由两个主要子组件组成：位置编码（我们刚刚学习过）和**多头注意力**。转换器是一个非常新的发展，由于一些优点而变得极其流行。然而，它们最适合处理大数据（至少50
    GB，根据我的经验）和大量计算（你需要4+个GPU）的大问题，所以我们在这个章节中不会看到全部的好处。尽管如此，了解转换器仍然很重要，它是当前深度学习一些重大进步的基础。最新的、最优秀的机器翻译、问答、少样本学习和数十个NLP任务的模型都是用不同的转换器变体构建的。本节的目标是帮助你理解标准的、原始的、无特殊添加的转换器。在本章之后，我建议阅读Jay
    Alammar的博客文章“Illustrated Transformer”（[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))，以详细了解转换器的逐步解析。
- en: 12.5.1  Multiheaded attention
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.1 多头注意力
- en: To understand how a transformer works, we need to understand *multiheaded attention*
    (MHA), which is an extension of the attention mechanisms we learned about in chapter
    10\. Like normal attention, MHA involves using the softmax function and `nn.Linear`
    layers to learn to selectively ignore or focus on different parts of an input.
    Our original attention mechanism has one context, which allows it to look for
    one kind of pattern. But what if you want to look for multiple different things
    at once (like a positive statement “good” preceded by a negation statement “not”)?
    This is where multiheaded attention comes into play. Each “head” of the multi*head*
    attention can learn to look for different kinds of patterns, similar to how each
    filter in a convolutional layer can learn to look for different patterns. The
    high-level strategy of how MHA works is shown in figure 12.5.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解transformer的工作原理，我们需要了解*多头注意力*（MHA），它是我们在第10章中学到的注意力机制的扩展。像正常注意力一样，MHA涉及使用softmax函数和`nn.Linear`层来学习有选择地忽略或关注输入的不同部分。我们原始的注意力机制有一个上下文，这使得它可以寻找一种类型的模式。但如果你想要同时寻找多种不同的事物（比如一个由否定陈述“not”
    precede的正陈述“good”）呢？这就是多头注意力发挥作用的地方。每个“头”的多头注意力可以学习寻找不同类型的模式，类似于卷积层中的每个滤波器可以学习寻找不同的模式。MHA如何工作的总体策略如图12.5所示。
- en: '![](../Images/CH12_F05_Raff.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F05_Raff.png)'
- en: 'Figure 12.5 Outline of the multiheaded attention block. There are three sequences
    as inputs: the queries Q and the keys and values K, V, which are paired together.
    The MHA returns a D-dimensional vector that answers each query **q**[i] ∈ *Q*.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 多头注意力模块的概述。有三个序列作为输入：查询Q和键值K, V，它们是配对的。MHA返回一个D维向量，回答每个查询**q**[i] ∈ *Q*。
- en: At an intuitive level, you can think of MHA as answering questions or queries
    about a dictionary of key-value pairs. Since this is a neural network, both the
    keys and values are vectors. Because each key has its own value, the key tensor
    K and value tensor V must have the same number of items T. So both have a shape
    of (*T*,*D*) for D features.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在直观层面上，你可以将MHA视为回答关于键值对的字典的问题或查询。由于这是一个神经网络，键和值都是向量。因为每个键都有自己的值，所以键张量K和值张量V必须具有相同的项目数T。因此，它们都具有(*T*,*D*)的形状，其中D表示特征数。
- en: Following this high-level analogy, you can ask as many or as few questions about
    the key-value dictionary as you like. The list of queries Q is its own tensor
    with a different length *T*′.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个高级类比，你可以对键值字典提出尽可能多或尽可能少的问题。查询列表Q是其自身的张量，长度为*T*′。
- en: The MHA output has one response for each query, and it has the same dimension,
    giving us a shape of (*T*′,*D*) for the output of the MHA. The z total heads of
    the MHA do not change the size of its output in the same way that changing the
    number of filters changes the number of channels in the output of a convolutional
    layer. That’s just an oddity of how the MHA was designed. Instead, the MHA tries
    to mix all the answers together into a single output.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: MHA的输出对每个查询都有一个响应，并且具有相同的维度，因此MHA的输出形状为(*T*′,*D*)。MHA的总头数不会以改变卷积层输出通道数的方式改变其输出的尺寸。这只是MHA设计中的一个奇特之处。相反，MHA试图将所有答案混合成一个单一的输出。
- en: As another way of explaining this analogy, we can think about MHA as a kind
    of deep learning alternative to the standard Python dictionary object. A dictionary
    `d = {key : value}` represents a set of keys, each with a specific value. You
    can then query that dictionary with something like `d[query`. If `query in d`,
    you get its associated value, but if not you get `None`. An MHA layer has the
    same overall goals but does not require perfect matches between queries and keys
    as a dictionary does. Instead, it’s a little soft, and multiple similar keys can
    respond to a single query, as shown in figure 12.6.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解释这个类比，我们可以将MHA视为一种深度学习替代品，类似于标准的Python字典对象。字典`d = {key : value}`代表一组键，每个键都有一个特定的值。你可以用类似`d[query]`的方式查询这个字典。如果`query
    in d`，你将得到其关联的值，如果没有，你将得到`None`。MHA层有相同的目标，但不需要查询和键之间像字典那样完美的匹配。相反，它稍微宽松一些，多个相似的键可以响应单个查询，如图12.6所示。'
- en: '![](../Images/CH12_F06_Raff.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F06_Raff.png)'
- en: Figure 12.6 Example of how both a Python dictionary and a multiheaded attention
    (MHA) have the concepts of queries, keys, and values. In a dictionary (top), a
    query and key must be a perfect match to get a value. In an MHA (bottom), each
    key contributes to answering each query based on how similar the key is to the
    query. This ensures that there is always an answer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6示例：Python字典和多头注意力（MHA）如何具有查询、键和值的理念。在字典（顶部）中，查询和键必须完全匹配才能得到值。在MHA（底部）中，每个键根据键与查询的相似程度，对每个查询的回答都有贡献。这确保了始终有一个答案。
- en: 'Your gut reaction may be to worry about giving answers when nothing looks similar
    to the query: isn’t returning a gibberish answer/value bad? This is actually a
    good thing because the MHA can learn that it needs to adjust some of the keys/values
    during training. Remember in chapter 3 that we saw how to manually specify useful
    convolutions, but we instead let the neural network learn what convolutions should
    be used. The MHA works the same way: it starts out with randomly initialized and
    meaningless keys and values but learns to adjust them to something useful during
    training.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你的直觉反应可能是担心当没有任何东西看起来与查询相似时给出答案：返回一个乱码答案/值不是坏事吗？这实际上是一件好事，因为MHA可以在训练过程中学习需要调整一些键/值。记得在第3章中我们看到了如何手动指定有用的卷积，但我们没有这样做，而是让神经网络学习应该使用哪些卷积。MHA也是这样工作的：它开始时使用随机初始化且无意义的键和值，但在训练过程中学会了调整它们以变得有用。
- en: Associative memories
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 联想记忆
- en: The idea of querying a nebulous cloud of neurons and retrieving specific representations
    from it is quite old. This is a reasonable description of what is called an associative
    memory, and you can read a brief summary of some of its history at [https://thegradient.pub/dont-forget-about-associative-memories](https://thegradient.pub/dont-forget-about-associative-memories).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 查询一个模糊的神经元云并从中检索特定表示的想法相当古老。这是所谓的联想记忆的合理描述，你可以在[https://thegradient.pub/dont-forget-about-associative-memories](https://thegradient.pub/dont-forget-about-associative-memories)上阅读其历史的简要总结。
- en: In short, ideas related to associative memories go all the way back to the 1960s,
    which is fun for those who don’t realize how old AI is as a field. While they
    are not currently in vogue among practitioners, there is still modern research
    on using them. I think they are also useful to learn about on your own to broaden
    how you think about problems and AI/ML. Many fields like cognitive science, psychology,
    neurology, and electrical engineering have helped shape early and foundational
    work, but sadly, associative memories do not often get the recognition they deserve
    today.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，与联想记忆相关的想法可以追溯到20世纪60年代，这对于那些没有意识到人工智能作为一个领域有多古老的人来说很有趣。虽然它们目前在从业者中并不流行，但仍然有现代研究在使用它们。我认为了解这些内容也有助于你自己拓宽对问题和人工智能/机器学习的思考。许多领域，如认知科学、心理学、神经学和电气工程，都帮助塑造了早期和基础性的工作，但遗憾的是，联想记忆并没有得到它们应得的认可。
- en: From attention to MHA
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意力到MHA
- en: The MHA can seem overly complex and opaque when we describe it on its own; instead,
    let’s describe it using only the attention mechanisms we learned about from chapter
    10\. This is different from the way most works discuss MHA and takes a little
    longer, but I think it is easier to understand.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们单独描述MHA时，它可能看起来过于复杂和晦涩；相反，让我们使用第10章中我们了解到的注意力机制来描述它。这与大多数讨论MHA的方式不同，需要更长的时间，但我认为这更容易理解。
- en: Let’s start by writing out the equation for what our normal attention mechanism
    does. We have a score function that takes in two items and returns a value α̃
    that indicates the level of importance/similarity of that pair. The score function
    could be any of the dot, general, or additive scores we learned about—but everyone
    uses the dot score for MHA because that is just what people do.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先写出我们正常注意力机制所做方程的表达式。我们有一个评分函数，它接受两个项目并返回一个α̃值，该值表示这对的重要性/相似性水平。评分函数可以是我们在前面学到的点积、一般或加法评分中的任何一种——但每个人在MHA中都使用点积评分，因为这就是人们所做的事情。
- en: 'We compute several scores for T different inputs, and they go into a softmax
    function to calculate a final vector of T scores α that says how important each
    of the T items was. Then we take the dot product between α and the original T
    items, which produces a single vector that is the result of our original attention
    mechanism. This is all shown in the following equation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 T 个不同的输入计算几个得分，并将它们输入到 softmax 函数中，以计算一个 T 得分的最终向量 α，它说明了 T 个项目中的每一个的重要性。然后我们计算
    α 和原始 T 个项目的点积，这产生了一个向量，它是我们原始注意力机制的输出结果。所有这些都在以下方程式中展示：
- en: '![](../Images/CH12_UN16_Raff.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN16_Raff.png)'
- en: 'Now let’s make a few changes to how this works. First, let’s rename our *context*
    vector ![](../Images/bar_h.png) to our *query* vector q and make it an input to
    the score function. The context or query tells us what we are looking for. Instead
    of using **h**[1], **h**[2], …, **h**[T] to determine the importances α *and*
    the output vectors, let’s separate them into two different sets of tensors that
    do not have to be the same. By random happenstance, we call the tensors used for
    α the *keys* *K* = [**k**[1],**k**[2],…,**k**[T]] and the *values* *V* = [**v**[1],**v**[2],…,**v**[T]].
    Then we get the following three-argument score function that uses the original
    two-argument score function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对如何实现这一点做一些修改。首先，让我们将我们的上下文向量 ![](../Images/bar_h.png) 重命名为查询向量 q 并将其作为得分函数的输入。上下文或查询告诉我们我们在寻找什么。我们不再使用
    **h**[1]，**h**[2]，…，**h**[T] 来确定重要性 α 和输出向量，而是将它们分成两组不同的张量，它们不必相同。由于偶然，我们将用于 α
    的张量称为**键** *K* = [**k**[1],**k**[2],…,**k**[T]] 和**值** *V* = [**v**[1],**v**[2],…,**v**[T]]。然后我们得到以下三个参数的得分函数，它使用原始的两个参数得分函数：
- en: '![](../Images/CH12_UN17_Raff.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN17_Raff.png)'
- en: All we have really done so far is *generalize* our score function to make it
    a bit more flexible. We can think of search (**q**,*K*,*V*) as addressing “Given
    this query q, give me the average value V from the dictionary based on its keys
    K.” This is a generalization because if we called score (1/*T* Σ*[i]^T*[=1] ***h**[i]*,
    [..., ***h**[i]*, ...] ,[..., ***h**[i]*, ...]) , we would get the same result
    as before!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止真正所做的一切就是将我们的得分函数进行**泛化**，使其更加灵活。我们可以将搜索（**q**，**K**，**V**）视为“给定查询 q，根据其键
    K 从字典中给出平均值 V。”这是一个泛化，因为如果我们调用得分（1/*T* Σ*[i]^T*[=1] ***h**[i]*, [..., ***h**[i]*,
    ...] ,[..., ***h**[i]*, ...])，我们会得到与之前相同的结果！
- en: 'This gives us the result for *one* query. To extend this to multiple queries,
    we call the three-argument score function multiple times. This gives us the result
    for *one head* of an MHA and is usually denoted by a function named `Attention`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个查询的结果。为了将此扩展到多个查询，我们多次调用三个参数的得分函数。这为我们提供了一个 MHA 的一个头的输出结果，通常用名为 `Attention`
    的函数表示：
- en: '![](../Images/CH12_UN18_Raff.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN18_Raff.png)'
- en: This equation is a straightforward adaption of what we just did. We now have
    multiple queries *Q* = [**q**[1],**q**[2],…,**q**[*T*′]], so we call `score` multiple
    times and stack the *T*′ results into one larger matrix.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式是我们刚才所做内容的直接改编。我们现在有多个查询 *Q* = [**q**[1],**q**[2],…,**q**[*T*′]]，因此我们多次调用
    `score` 并将 *T*′ 个结果堆叠成一个更大的矩阵。
- en: 'With that, we can finally define the MHA function. If we have z heads, you
    may have correctly guessed that we call the `Attention` function z times! But
    the input is still Q, K, and V. To make each head learn to look for something
    different, each head has three `nn.Linear` layers *W*^Q, *W*^K, and *W*^V. These
    linear layers have the job of preventing the multiple calls from all computing
    the exact same thing, because that would be silly. Then we concatenate all z results
    into one big vector and end with a final output `nn.Linear` layer *W*^O that has
    the sole job of making sure the output of MHA has D dimensions. This all looks
    like the following equation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们最终可以定义 MHA 函数。如果我们有 z 个头，你可能已经猜对了，我们调用 `Attention` 函数 z 次！但输入仍然是 Q，K
    和 V。为了使每个头学会寻找不同的事物，每个头都有三个 `nn.Linear` 层 *W*^Q，*W*^K 和 *W*^V。这些线性层的任务是防止所有调用都计算完全相同的事情，因为那将是愚蠢的。然后我们将所有
    z 个结果连接成一个大的向量，并以一个最终输出 `nn.Linear` 层 *W*^O 结尾，这个层的唯一任务是确保 MHA 的输出有 D 维。所有这些都像以下方程式所示：
- en: '![](../Images/CH12_UN19_Raff.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_UN19_Raff.png)'
- en: That gets us from our original attention mechanism to the more sophisticated
    MHA. Because the MHA is decently involved and has multiple layers, it does not
    need as many heads z as a convolutional layer needs filters C. Whereas we usually
    want somewhere between 32 and 512 filters (based on what gets the best results),
    for MFA, we usually want no more than *z* = 8 or *z* = 16 heads.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们从原始的注意力机制过渡到更复杂的MHA。因为MHA相当复杂并且有多个层，所以它不需要像卷积层需要的过滤器C那样多的头z。而通常我们希望有32到512个过滤器（基于什么能得到最佳结果），对于MFA，我们通常希望不超过*z*
    = 8或*z* = 16个头。
- en: 'We won’t implement the MHA function since PyTorch provides a good implementation
    for us. But let’s quickly look at some pseudo-Python to get an idea of how it
    goes. First we create the *W*[i]^Q, *W*[i]^K, *W*[i]^V layers and the output *W*^O.
    That happens in the constructor, and we can use the PyTorch `ModuleList` to store
    a list of modules that we want to use later:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会实现MHA函数，因为PyTorch为我们提供了一个很好的实现。但让我们快速看一下一些伪Python代码，以了解它是如何进行的。首先，我们创建*W*[i]^Q、*W*[i]^K、*W*[i]^V层和输出*W*^O。这发生在构造函数中，我们可以使用PyTorch的`ModuleList`来存储我们稍后想要使用的模块列表：
- en: '[PRE26]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then we can move into the `forward` function. Let’s assume an `Attention` function
    already exists, for simplicity. We basically just need to call this `Attention`
    function repeatedly, applying the linear layers defined in the constructor. The
    `zip` command can be used to make this pretty concise, giving us a triplet of
    *W*[i]^Q, *W*[i]^K, and *W*[i]^V with the result appended to a list of `heads`.
    They are combined at the end with concatenation, and the final *W*^O layer is
    applied:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以进入`forward`函数。为了简单起见，我们假设已经存在一个`Attention`函数。我们基本上只需要反复调用这个`Attention`函数，应用构造函数中定义的线性层。可以使用`zip`命令使这个过程更加简洁，给我们一个包含*W*[i]^Q、*W*[i]^K和*W*[i]^V的元组，并将结果附加到`heads`列表中。它们在最后通过连接组合，并应用最终的*W*^O层：
- en: '[PRE27]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Multiheaded attention standard equations
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力标准方程
- en: 'We used our score function from the last chapter to show how the MHA is really
    just an extension of what we have already learned. I think that journey helps
    to solidify what the MHA is doing. The MHA can also be expressed with fewer equations:
    I find them more opaque to understand, but it’s worth showing them since that’s
    how most people write them!'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了上一章中的得分函数来展示MHA实际上只是我们已学内容的扩展。我认为这次旅程有助于巩固我们对MHA的理解。MHA也可以用更少的方程来表示：我发现它们更难以理解，但展示它们是值得的，因为这是大多数人写作的方式！
- en: 'The main difference is how the `Attention` function is written. Normally it’s
    the result of three matrix multiplications:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于`Attention`函数的编写方式。通常它是三个矩阵乘法的结果：
- en: '![](../Images/CH12_F06_EQ01.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F06_EQ01.png)'
- en: This is equivalent to what I’ve already shown you, but it is not obvious how
    it does attention this way (in my opinion). This version is the preferred way
    to implement MHA because it runs faster.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我之前展示的等价，但在我看来，这种方式进行注意力的过程并不明显。这个版本是实现MHA的首选方式，因为它运行得更快。
- en: 12.5.2  Transformer blocks
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.2  Transformer块
- en: 'Now that we know what the MHA block looks like, we can describe the transformer!
    There are two kinds of transformer blocks: an encoder and a decoder. They are
    shown in figure 12.7 and use the familiar concept of a residual connection.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了MHA块的样子，我们可以描述transformer了！有两种类型的transformer块：编码器和解码器。它们在图12.7中展示，并使用了熟悉的残差连接概念。
- en: '![](../Images/CH12_F07_Raff.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F07_Raff.png)'
- en: 'Figure 12.7 The two types of transformer blocks: the encoder (left) and decoder
    (right). Both use layer normalization, residual connections, and MHA layers. The
    first MHA in each is called self-attention because the same input is used for
    Q, K, and V.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 两种类型的transformer块：编码器（左）和解码器（右）。它们都使用了层归一化、残差连接和MHA层。每个中的第一个MHA被称为自注意力，因为Q、K和V使用了相同的输入。
- en: 'The encoder block can be used in a network for almost any sequence-based architecture
    (e.g., sentiment classification); it does not need to be paired with a decoder.
    It begins with a residual connection with the MHA; the same input sequence is
    used for the queries, key, and values, so this is described as a self-attention
    layer because there is no external context or input. Then a second residual connection
    occurs that uses only a linear layer. You can repeat this encoder block multiple
    times to make a deeper and more powerful network: the original paper[³](#fn48)
    used six of them, so that has become a common default.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块可以用于几乎任何基于序列的网络架构（例如，情感分类）；它不需要与解码器配对。它从与MHA的残差连接开始；查询、键和值使用相同的输入序列，因此这被描述为自注意力层，因为没有外部上下文或输入。然后发生第二个残差连接，它只使用线性层。你可以多次重复这个编码器块来创建一个更深更强的网络：原始论文[³](#fn48)使用了六个这样的块，因此这已成为一个常见的默认值。
- en: The decoder block is almost always used with an encoder block and generally
    used for sequence tasks with multiple outputs (e.g., machine translation). The
    only difference is that a second MHA residual connection is inserted after the
    first one. This second MHA uses the output of the encoder block as the keys and
    values, and the previous MHA’s result is used as the queries. This is done so
    that you can build either sequence-to-sequence-like models like chapter 11’s or
    autoregressive style models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块几乎总是与编码器块一起使用，通常用于具有多个输出的序列任务（例如，机器翻译）。唯一的区别是在第一个MHA残差连接之后插入第二个MHA残差连接。这个第二个MHA使用编码器块的输出作为键和值，而前一个MHA的结果用作查询。这样做是为了你可以构建类似于第11章的序列到序列模型或自回归风格的模型。
- en: You may notice in this design that there are no explicit time connections like
    those that our RNNs have. Transformers get *all* of their sequence information
    from positional encodings! This is why we needed to learn about positional encodings
    first, so we can reuse them to create a transformer model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到在这个设计中没有像我们的RNN那样的显式时间连接。Transformers从位置编码中获取*所有*的序列信息！这就是为什么我们首先需要了解位置编码，这样我们就可以重复使用它们来创建一个transformer模型。
- en: Warning The encoder and decoder look at and process all time steps simultaneously.
    This can be bad for autoregressive models where you are trying to predict the
    next item. Naively using a transformer for an autoregressive model means the transformer
    can look at future inputs, which is cheating when the goal is to predict the future!
    For such applications, you need to use a special mask to prevent the transformer
    from looking into the future. This mask has a triangular shape so that interactions
    are limited across time and is provided by the `generate_square_subsequent_mask()`
    method of the `Transformer` class.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：编码器和解码器同时查看和处理所有时间步。这对于你试图预测下一个项目的自回归模型来说可能是个问题。天真地使用transformer进行自回归模型意味着transformer可以查看未来的输入，这在目标是预测未来时是一种作弊行为！对于此类应用，你需要使用特殊的掩码来防止transformer查看未来。这个掩码具有三角形形状，因此时间上的交互受到限制，由`Transformer`类的`generate_square_subsequent_mask()`方法提供。
- en: 'The following block of code implements a simple approach to classifying the
    sequence with a transformer. It starts with an `Embedding` layer followed by a
    `PositionalEncoding` and then three `TransformerEncoder` layers. Ideally, we would
    use six layers here; but transformers are even *more* expensive than RNNs, so
    we have to handicap the model to make this example run quickly. After the transformer
    runs, we still get an output of shape (*B*,*T*,*D*): we use our normal attention
    to get that down to a single vector with shape (*B*,*D*) so we can make a prediction.
    Note in this code the weird quirk that transformers require their tensors to be
    organized as (*T*,*B*,*D*), so we need to reorder the axis a few times to make
    everything work out:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块实现了一个使用transformer对序列进行分类的简单方法。它从一个`Embedding`层开始，接着是`PositionalEncoding`，然后是三个`TransformerEncoder`层。理想情况下，我们在这里会使用六个层；但是transformer比RNN更昂贵，所以我们必须对模型进行限制以使这个示例快速运行。transformer运行后，我们仍然得到一个形状为(*B*,*T*,*D*)的输出：我们使用正常的注意力机制将其降低到形状为(*B*,*D*)的单个向量，以便进行预测。注意在这段代码中，transformer需要它们的张量以(*T*,*B*,*D*)的顺序组织，因此我们需要重新排列几次轴才能使一切正常工作：
- en: '[PRE28]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ The main work for our transformer implementation
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们transformer实现的主要工作
- en: ❷ (B, 1, D) -> (B, D)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, 1, D) -> (B, D)
- en: ❸ All entries are True.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 所有条目都是True。
- en: ❹ (B, T, D)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, T, D)
- en: ❺ (B, T, D)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ (B, T, D)
- en: ❻ Because the rest of our code is (B, T, D), but transformers take input as
    (T, B, D), we have to change the order of the dimensions before and after.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 因为我们的其余代码是 (B, T, D)，但变压器以 (T, B, D) 作为输入，我们必须在前后改变维度的顺序。
- en: ❼ (B, T, D)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ (B, T, D)
- en: ❽ Average over time
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 平均时间
- en: ❾ Builds and trains this model
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 构建和训练此模型
- en: 'Now we can plot the results of all our methods. Transformers hit the highest
    accuracy of all of them and are still improving as we keep training. If we did
    more epochs and used more layers, they would probably improve even more! But that
    increases training time, and the transformer is already slower than the GRU model:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以绘制我们所有方法的成果。变压器在所有方法中达到了最高的准确率，并且在我们持续训练的过程中仍在提高。如果我们进行更多的迭代并使用更多的层，它们可能会进一步提高！但这会增加训练时间，而变压器已经比GRU模型慢：
- en: '[PRE29]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/CH12_UN20_Raff.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_UN20_Raff.png)'
- en: Where does this leave us with our results? By default, the attention-based embedding
    with positional encodings is a great tool. It doesn’t always outperform a modern
    RNN like the GRU or a LSTM, but it’s a good candidate that is faster to run. You
    can look at either approach for most problems. If you have excess compute, though,
    RNNs have the benefit of being more studied and understood, so it may be easier
    to trust your results with them.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果将如何？默认情况下，基于注意力的嵌入和位置编码是一个很好的工具。它并不总是优于现代的RNN，如GRU或LSTM，但它是一个运行速度更快的良好候选方案。对于大多数问题，您都可以考虑这两种方法。不过，如果您有额外的计算资源，RNNs的好处是它们被研究得更深入，理解得更透彻，因此您可能更容易信任它们的结果。
- en: Transformers are a tool that you bring out when you need the maximum possible
    accuracy and you’ve got oodles of GPUs available (and data) to pay their hefty
    price. RNNs have a bad habit of plateauing in accuracy after three to six layers,
    bucking the deep learning trend where more layers get you a more powerful model
    with better results. Transformers in total usually have 24+ layers and still gain
    accuracy from having more. That said, transformers can be more accurate *and*
    faster when you are working on massive datasets that *require* using multiple
    GPUs.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要尽可能高的准确率并且有大量的GPU（和数据）可用以支付其高昂的价格时，变压器是您会使用的工具。RNNs有一个坏习惯，就是在三到六层后准确率会达到平台期，这与深度学习趋势相反，即更多的层可以得到更强大的模型和更好的结果。总的来说，变压器通常有24+层，并且随着层数的增加而提高准确率。但话虽如此，当您在处理需要使用多个GPU的庞大数据集时，变压器可以更准确
    *并且* 更快。
- en: This is because transformers process all T items in a sequence *simultaneously*,
    whereas RNNs need to process them one at a time. Doing work this way makes transformers
    scale to multiple GPUs better because there is more work to split up and run simultaneously.
    With an RNN, you can’t break up the work because one step depends on the previous
    step, so you have to wait. That being said, researchers and companies are using
    *hundreds to thousands of GPUs* to train a single model on hundreds of gigabytes
    or more of data. This tells you the kind of scale it takes to really see the benefits
    of transformers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为变压器会同时处理序列中的所有T个项，而RNNs需要逐个处理它们。以这种方式进行工作使得变压器更适合扩展到多个GPU，因为可以分割和同时运行的工作更多。使用RNN时，您不能分割工作，因为每一步都依赖于前一步，所以您必须等待。尽管如此，研究人员和公司正在使用
    *数百到数千个GPU* 来在数百GB或更多的数据上训练单个模型。这告诉您，要真正看到变压器的优势，需要多大的规模。
- en: While transformers are not yet ready to replace RNNs, they are another approach
    that can be used. They currently hold the top slot to maximize accuracy when you
    have a large amount of data and compute available. In the next chapter, we learn
    how to extract the benefits of transformers for your own problems *without* needing
    a thousand GPUs!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然变压器尚未准备好取代循环神经网络（RNNs），但它们是另一种可以使用的方案。当您拥有大量数据和计算资源时，它们目前是最大化准确率的首选。在下一章中，我们将学习如何在不需要一千个GPU的情况下，为您的自身问题提取变压器的优势
    *而不* 需要一千个GPU！
- en: Exercises
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台Inside Deep Learning Exercises上分享和讨论您的解决方案（[https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)）。一旦您提交了自己的答案，您将能够看到其他读者提交的解决方案，并看到作者认为哪些是最佳的。
- en: Use Optuna to try to optimize the attention-based averaged embedding classifier.
    Try to tune the initial dimension used by the `nn.Embedding` layer, the number
    of hidden neurons in all subsequent layers, the total number of hidden layers,
    and which of the score functions (dot, general, additive) to use on the AG News
    corpus. How much higher can you get the accuracy?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Optuna 尝试优化基于注意力的平均嵌入分类器。尝试调整 `nn.Embedding` 层使用的初始维度、所有后续层中的隐藏神经元数量、总隐藏层数量，以及要在
    AG News 语料库上使用的评分函数（点积、通用、加法）。你能将精度提高多少？
- en: Going back to chapter 6, convert the `ResidualBlockE` and `ResidualBottleNeck`
    layers into their 1D counterparts `ResidualBlockE1D` and `ResidualBottleNeck1D`.
    Then use them to try to improve the 1D CNN from this chapter on the AG News corpus.
    How high can you get the accuracy?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到第 6 章，将 `ResidualBlockE` 和 `ResidualBottleNeck` 层转换为它们的 1D 对应层 `ResidualBlockE1D`
    和 `ResidualBottleNeck1D`。然后使用它们尝试改进本章在 AG News 语料库上的 1D CNN。你能将精度提高到多高？
- en: Using your best 1D CNN for AG News, try adding positional encoding to the network.
    How does it impact your results?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你最好的 1D CNN 对 AG News 进行处理，尝试向网络中添加位置编码。这对你的结果有何影响？
- en: PyTorch provides a `nn.MultiheadAttention` module that implements the MHA approach.
    Come up with your own modification to the attention-based averaged embedding classifier
    to use MHA, and try to get it to have better accuracy.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch 提供了一个 `nn.MultiheadAttention` 模块，实现了 MHA 方法。为基于注意力的平均嵌入分类器提出自己的修改，以使用
    MHA，并尝试使其具有更高的精度。
- en: Transformers often work best with a learning rate schedule. Try the schedules
    we learned about in chapter 5 to see if you can reduce the number of epochs it
    takes `SimpleTransformerClassifier` to learn or increase its final accuracy.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer 通常与学习率计划配合使用效果最佳。尝试第 5 章中我们学习到的计划，看看你是否可以减少 `SimpleTransformerClassifier`
    学习所需的周期数或提高其最终精度。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We can intentionally reduce how much our models understand the sequential nature
    of data to reduce runtime.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过故意减少模型对数据序列性质的理解来减少运行时间。
- en: We can encode positional information by modifying the inputs to the data instead
    of using an RNN or CNN that encodes positional information in their structure,
    which can endow other faster alternatives with sequential information or improve
    accuracy.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过修改数据的输入来编码位置信息，而不是使用在结构中编码位置信息的 RNN 或 CNN，这可以为其他更快的替代方案提供序列信息或提高精度。
- en: The multiheaded attention is a generalization of the attention from chapter
    10, which involves multiple contexts or queries and multiple results. It can obtain
    greater accuracy at a higher computational cost.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头注意力是第 10 章中提到的注意力的推广，涉及多个上下文或查询和多个结果。它可以在更高的计算成本下获得更高的精度。
- en: Transformers are a powerful alternative to RNNs that can obtain high accuracies
    at a steep computational cost. They work best when you have large amounts of data
    and compute, because they scale up and out better than RNNs do.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 是一种强大的 RNN 替代方案，能够在高昂的计算成本下获得高精度。它们在拥有大量数据和计算资源时表现最佳，因为它们的扩展性和扩展性优于
    RNN。
- en: '* * *'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: ¹ A. Vaswani et al., “Attention is all you need,” *Advances in Neural Information
    Processing Systems*, vol. 30,pp. 5998–6008, 2017.[↩](#fnref46)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ A. Vaswani 等人，“Attention is all you need”，*Advances in Neural Information
    Processing Systems*，第 30 卷，第 5998-6008 页，2017。[↩](#fnref46)
- en: ² This code is under a BDS-3 license; check out the repo for this book for the
    license included.[↩](#fnref47)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ² 此代码受 BDS-3 许可证保护；请查看本书的代码库以获取包含的许可证。[↩](#fnref47)
- en: ³ The “Attention Is All You Need" paper that introduced positional encodings!
    (See footnote 1.)[↩](#fnref48)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 引入位置编码的“Attention Is All You Need”论文！（见脚注 1。）[↩](#fnref48)
