- en: '7 Experimentation in action: Moving from prototype to MVP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 实验行动：从原型到MVP
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Techniques for hyperparameter tuning and the benefits of automated approaches
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整技术以及自动化方法的益处
- en: Execution options for improving the performance of hyperparameter optimization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高超参数优化性能的执行选项
- en: In the preceding chapter, we explored the scenario of testing and evaluating
    potential solutions to a business problem focused on forecasting passengers at
    airports. We ended up arriving at a decision on the model to use for the implementation
    (Holt-Winters exponential smoothing) but performed only a modicum of model tuning
    during the rapid prototyping phases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了针对机场乘客预测的业务问题的潜在解决方案的测试和评估场景。我们最终做出了关于要使用的模型（霍尔特-温特斯指数平滑）的决定，但在快速原型阶段，我们只进行了少量的模型调整。
- en: Moving from experimental prototyping to MVP development is challenging. It requires
    a complete cognitive shift that is at odds with the work done up to this point.
    We’re no longer thinking of how to *solve a problem* and get a good result. Instead,
    we’re thinking of how to *build a solution* that is good enough to solve the problem
    in a way that is robust enough so that it’s not breaking constantly. We need to
    shift focus to monitoring, automated tuning, scalability, and cost. We’re moving
    from scientific-focused work to the realm of engineering.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验原型转向MVP开发具有挑战性。这需要一种与迄今为止所做工作完全相反的全面认知转变。我们不再思考如何*解决问题*并获得良好的结果。相反，我们思考的是如何*构建一个解决方案*，这个解决方案足够好，能够以足够稳健的方式解决问题，使其不会不断崩溃。我们需要将重点转移到监控、自动化调整、可扩展性和成本上。我们正从以科学为重点的工作转向工程领域。
- en: The first priority when moving from prototype to MVP is ensuring that a solution
    is tuned correctly. See the following sidebar for additional details on why it’s
    so critical to tune models and how these seemingly optional settings in modeling
    APIs are actually important to test.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从原型转向最小可行产品（MVP）的首要任务是确保解决方案调整正确。请参阅以下侧边栏，以获取更多关于为何调整模型如此关键以及这些看似可选的建模API设置实际上为何重要的详细信息。
- en: Hyperparameters are important—very important
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数很重要——非常重要
- en: One of the most frustrating things to see in ML code bases is an *untuned model*
    (a model that will be generated by using the placeholder defaults provided by
    the API). With all of the advanced feature engineering, ETL, visualization work,
    and coding effort that is involved in building the rest of the solution, seeing
    a bare model using defaults is like buying a high-performance sports car and filling
    it with regular gas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习代码库中最令人沮丧的事情之一是看到一个*未调整的模型*（一个将由API提供的占位符默认值生成的模型）。在构建其余解决方案所涉及的先进特征工程、ETL、可视化和编码努力中，看到一个仅使用默认值的裸模型就像买了一辆高性能跑车，却给它加满了普通汽油。
- en: Will it run? Sure. Will it perform well? Nope. Not only will it underperform,
    but the chances of it breaking are high once you take it out into the “real world”
    (in reference to a model, using it on heretofore unseen data to make predictions).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它会运行吗？当然会。它会表现良好吗？不会。不仅它的表现会不佳，而且一旦将其带入“现实世界”（指模型，使用之前未见过的数据来做出预测），它崩溃的可能性很高。
- en: Some algorithms automatically handle their methodologies in arriving at an optimized
    solution, thus requiring no hyperparameters to be overridden. However, the vast
    majority have anywhere from a single to dozens of parameters that influence not
    only the core functionality of the algorithm’s optimizer (for example, the `family`
    parameter in generalized linear regression will directly influence the predictive
    performance of such a model more dramatically than any other hyperparameter),
    but the way the optimizer executes its search to find the minimum objective function.
    Some of these hyperparameters apply only to specific applications of an algorithm—the
    hyperparameters are applicable only if the variance within the feature vector
    is extreme or if a particular distribution is associated with the target variable.
    But for most of them, the influence of their set values over the manner in which
    the algorithm will “learn” an optimal fit to the data is exceptionally important.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法会自动处理其方法以到达优化的解决方案，因此不需要覆盖任何超参数。然而，绝大多数算法从单个到数十个参数不等，这些参数不仅影响算法优化器的核心功能（例如，广义线性回归中的`family`参数将比任何其他超参数更直接地影响该模型的预测性能），还影响优化器执行搜索以找到最小目标函数的方式。其中一些超参数仅适用于算法的特定应用——只有当特征向量中的方差极端或与目标变量相关联特定分布时，这些超参数才是适用的。但对于大多数超参数而言，它们的设置值对算法如何“学习”数据的最优拟合方式的影响至关重要。
- en: The following graphs are simplified examples of two such critical hyperparameters
    for linear regression models. It is impossible to guess where these values should
    be set, as each feature vector collection and problem will generally have dramatically
    different optimal hyperparameter settings from others.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是线性回归模型中两个关键超参数的简化示例。由于每个特征向量集合和问题通常会有与其他人截然不同的最佳超参数设置，因此无法猜测这些值应该设置在哪里。
- en: Note that these examples are for demonstration purposes only. The effects on
    models for different values set for hyperparameters is not only highly dependent
    on the algorithm type being used, but also on the nature of the data contained
    in the feature vector and the attributes of the target variable. This is why every
    model needs to be tuned.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些示例仅用于演示目的。对于不同超参数设置的模型效果不仅高度依赖于所使用的算法类型，还取决于特征向量中包含的数据的性质以及目标变量的属性。这就是为什么每个模型都需要进行调优。
- en: As you can see, the seemingly optional settings associated with each ML algorithm
    actually do matter a great deal in the way the training process executes. Without
    changing any of these values and optimizing them, there is little chance of having
    a successful ML-based solution to a problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与每个机器学习算法相关的看似可选设置实际上在训练过程的执行方式中起着非常重要的作用。如果不更改这些值并优化它们，几乎没有成功基于机器学习的解决方案解决问题的可能性。
- en: '![07-0-unnumb](../Images/07-0-unnumb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![07-0-unnumb](../Images/07-0-unnumb.png)'
- en: Hyperparameter impacts to overfitting and underfitting
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数对过拟合和欠拟合的影响
- en: '7.1 Tuning: Automating the annoying stuff'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 调优：自动化繁琐的工作
- en: Throughout the last two chapters, we’ve been focusing on a peanut forecasting
    problem. At the end of chapter 6, we had a somewhat passable prototype, validated
    on a single airport. The process used to adjust and tune the predictive performance
    of the model was manual and not particularly scientific, and left a large margin
    between what is possible for the model’s predictive ability and what we had manually
    tuned.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们一直专注于一个花生预测问题。在第6章结束时，我们有一个勉强可行的原型，在一个机场进行了验证。调整和调优模型预测性能的过程是手动进行的，并不特别科学，留下了模型预测能力可能实现的范围和手动调优之间的较大差距。
- en: In this scenario, the difference between OK and very good predictions could
    be a large margin of product that we want to stage at airports. Being off in our
    forecasts, after all, could translate to many millions of dollars. Spending time
    manually tuning by just trying a bunch of hyperparameters simply won’t scale for
    predictive accuracy or for timeliness of delivery.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，从良好预测到非常良好预测之间的差异可能是一个很大的产品差额，这是我们希望在机场进行展示的。毕竟，我们的预测失误可能导致数百万美元的损失。仅仅通过尝试大量超参数来手动调优，既无法提高预测准确性，也无法保证交付的及时性。
- en: If we want to come up with a better approach than tribal-knowledge guessing
    for tuning the model, we need to look at our options. Figure 7.1 shows various
    approaches that DS teams use to tune models, progressing in order from simple
    (less powerful and maintainable) to complex (custom framework).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要提出比部落知识猜测更好的方法来调整模型，我们需要考虑我们的选项。图7.1显示了DS团队用于调整模型的各种方法，从简单（较弱且不易维护）到复杂（自定义框架）的顺序。
- en: '![07-01](../Images/07-01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../Images/07-01.png)'
- en: Figure 7.1 Comparison of hyperparameter tuning approaches
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1超参数调整方法的比较
- en: The top section, manual tuning, is typically how prototypes are built. Manually
    testing values of hyperparameters, when doing rapid testing, is an understandable
    approach. The goal of the prototype, as mentioned in chapter 6, is getting an
    approximation of the tunability of a solution. At the stage of moving toward a
    production-capable solution, however, more maintainable and powerful solutions
    need to be considered.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部部分，手动调整，通常是构建原型的典型方式。在快速测试时，手动测试超参数的值是一种可以理解的方法。如第6章所述，原型的目标是获得解决方案可调整性的近似值。然而，在向生产级解决方案迈进的过程中，需要考虑更多可维护且强大的解决方案。
- en: 7.1.1 Tuning options
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 调整选项
- en: 'We know that we need to tune the model. In chapter 6, we saw clearly what happens
    if we don’t do that: generating a forecast so laughably poor that pulling numbers
    from a hat would be more accurate. However, multiple options could be pursued
    to arrive at the most optimal set of hyperparameters.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们需要调整模型。在第6章中，我们清楚地看到了如果我们不这样做会发生什么：生成的预测结果如此糟糕，以至于从帽子里抽数字会更准确。然而，可以追求多种选项以达到最优的超参数集。
- en: Manual tuning (educated guessing)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 手动调整（有根据的猜测）
- en: We will see later, when applying Hyperopt to our forecasting problem, just how
    difficult it will be to arrive at the optimal hyperparameters for each model that
    needs to be built for this project. Not only are the optimized values unintuitive
    to guess at, but each forecasting model’s optimal hyperparameter set is different
    from that of other models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后看到，当将Hyperopt应用于我们的预测问题时，到达每个需要为这个项目构建的模型的最佳超参数将有多么困难。不仅优化值难以猜测，而且每个预测模型的最佳超参数集与其他模型不同。
- en: Getting even remotely close to optimal parameters with a manual testing methodology
    is unlikely. The process is inefficient, frustrating, and an incredible waste
    of time to attempt, as shown in figure 7.2.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用手动测试方法接近最优参数几乎是不可能的。这个过程效率低下，令人沮丧，尝试这个过程是极大的时间浪费，如图7.2所示。
- en: '![07-02](../Images/07-02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../Images/07-02.png)'
- en: Figure 7.2 The acute pain of manual hyperparameter tuning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 手动超参数调整的剧痛
- en: Tip Don’t try manual tuning unless you’re working with an algorithm that has
    a very small number of hyperparameters (one or two, preferably Boolean or categorical).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：除非你正在使用具有非常少数量的超参数（一个或两个，最好是布尔型或分类型）的算法，否则不要尝试手动调整。
- en: The primary issue with this method is in tracking what has been tested. Even
    if a system was in place to record and ensure that the same values haven’t been
    tried before, the sheer amount of work required to maintain that catalog is overwhelming,
    prone to errors, and pointless in the extreme.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要问题是跟踪已经测试的内容。即使有系统来记录并确保之前没有尝试过相同的值，维护这个目录所需的工作量巨大，容易出错，且在极端情况下毫无意义。
- en: Project work, after the rapid prototyping phase, should always abandon this
    approach to tuning as soon as is practicable. You have so many better things to
    do with your time, believe me.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速原型阶段之后，项目工作应尽快放弃这种调整方法。相信我，你有很多更好的事情可以做。
- en: Grid search
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索
- en: A cornerstone of ML techniques, the brute-force-search approach of grid-based
    testing of hyperparameters has been around for quite some time. To perform a grid
    search, the DS will select a set collection of values to test for each hyperparameter.
    The grid search API will then assemble collections of hyperparameters to test
    by creating permutations of each value from each group that has been specified.
    Figure 7.3 illustrates how this works, as well as why it might not be something
    that you would entertain for models with a lot of hyperparameters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术的一个基石，基于网格的超参数暴力搜索方法已经存在很长时间了。为了执行网格搜索，数据科学家将选择一组要测试的值集合，对于每个超参数。然后，网格搜索API将通过创建每个指定组的每个值的排列来组装要测试的超参数集合。图7.3说明了这是如何工作的，以及为什么它可能不是你愿意用于具有许多超参数的模型的某种方法。
- en: As you can see, with high hyperparameter counts, the sheer number of permutations
    that need to be tested can quickly become overwhelming. The trade-off, clearly,
    is between the time required to run all of the permutations and the search capability
    of the optimization. If you want to explore more of the hyperparameter response
    surface, you’re going to have to run more iterations. There’s really no free lunch
    here.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，随着超参数数量的增加，需要测试的排列数量会迅速变得难以承受。显然，这种权衡是在运行所有排列所需的时间和搜索优化能力之间。如果你想探索更多超参数响应面，你将不得不运行更多的迭代。这里实际上没有免费的午餐。
- en: '![07-03](../Images/07-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../Images/07-03.png)'
- en: Figure 7.3 Brute-force grid search approach to tuning
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 基于暴力搜索的网格搜索方法进行调优
- en: Random search
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索
- en: With all of the grid search’s limitations that hamper its ability to arrive
    at an optimized set of hyperparameters, using it can be prohibitively expensive
    in terms of both time and money. Were we interested in thoroughly testing all
    continuously distributed hyperparameters in a forecasting model, the amount of
    time to get an answer, when running on a single CPU, would be measured in *weeks*
    rather than minutes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网格搜索存在许多限制，阻碍了其找到一组优化超参数的能力，但从时间和金钱的角度来看，使用它可能会变得过于昂贵。如果我们对彻底测试预测模型中所有连续分布的超参数感兴趣，那么在单核CPU上运行时得到答案所需的时间将是以周计，而不是以分钟计。
- en: An alternative to grid search, to attempt to simultaneously test the influencing
    effects of different hyperparameters at the same time (rather than relying on
    explicit permutations to determine the optimal values), is using random sampling
    of each of the hyperparameter groups. Figure 7.4 illustrates random search; compare
    it to figure 7.3 to see the differences in the approaches.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的一个替代方案，尝试同时测试不同超参数的影响（而不是依赖于显式的排列来确定最佳值），是使用每个超参数组的随机抽样。图7.4说明了随机搜索；将其与图7.3进行比较，以查看方法之间的差异。
- en: '![07-04](../Images/07-04.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../Images/07-04.png)'
- en: Figure 7.4 Random search process for hyperparameter optimization
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 超参数优化的随机搜索过程
- en: 'As you can see, the selection of candidates to test is random and is controlled
    not through the mechanism of permutations of all possible values, but rather through
    a maximum number of iterations to test. This is a bit of a double-edged sword:
    although the execution time is dramatically reduced, the search through the hyperparameter
    space is limited.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，要测试的候选者的选择是随机的，并且不是通过所有可能值的排列机制来控制的，而是通过测试的最大迭代次数来控制。这是一把双刃剑：虽然执行时间大大减少，但超参数空间的搜索是有限的。
- en: Nerdy arguments about parameter searching
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 关于参数搜索的学术性争论
- en: Numerous arguments can be made for why random search is superior to grid-based
    search, many of them quite valid. However, the vast majority of examples presented
    in online references, examples, and blog posts are still using grid search as
    a means to perform model tuning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关于为什么随机搜索优于基于网格的搜索，可以提出许多论点，其中许多论点相当有说服力。然而，在在线参考资料、示例和博客文章中展示的大多数例子仍然使用网格搜索作为模型调优的手段。
- en: 'There’s a clear reason for this: it’s fast. No package developer blogger wants
    to create an example that is incredibly complex or time-consuming for their readers
    to run. This doesn’t make it a good practice to follow, though.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个明显的理由：它速度快。没有任何包开发者或博客作者愿意创建一个对读者来说非常复杂或耗时的示例。尽管如此，这并不意味着这是一种好的做法。
- en: Seeing so many grid searches employed in examples has generated the mistaken
    impression in many practitioners that it is far more effective at finding good
    parameters, more so than other approaches. We may also have general entropic aversion,
    collectively as humans (we abhor randomness, so a random search must be bad, right?).
    I’m not entirely sure.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中看到如此多的网格搜索应用，给许多从业者造成了错误的印象，认为它在找到良好参数方面远比其他方法更有效。我们也许也有普遍的熵厌恶，作为人类（我们厌恶随机性，所以随机搜索一定是坏的，对吧？）。我并不完全确定。
- en: I can’t emphasize enough, however, how limiting grid search is (and expensive,
    if you want to be thorough). I’m not alone in this either; see “Random Search
    for Hyper-Parameter Optimization” by James Bergstra and Yoshua Bengio (2012) at
    [www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
    I generally agree with their conclusion that grid search is essentially flawed
    as an approach; since some hyperparameters are far more influential in the overall
    quality of a particular trained model, those with greater effect get the same
    amount of coverage as those with negligible influence, limiting the effective
    search because of computation time and the cost of more expansive testing. Random
    search is, in my opinion, a better approach than grid search, but it still isn’t
    the most effective or efficient approach.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我无法强调网格搜索的限制性（如果你想要彻底的话，它也很昂贵）。我并不孤单；参见詹姆斯·伯格斯特拉和约书亚·本吉奥的“随机搜索超参数优化”（2012）[www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)。我通常同意他们的结论，即网格搜索作为一种方法本质上是有缺陷的；因为某些超参数对特定训练模型的总体质量影响更大，那些影响更大的超参数与那些影响微不足道的超参数获得相同的覆盖量，这限制了有效的搜索，因为计算时间和更广泛测试的成本。在我看来，随机搜索比网格搜索是一个更好的方法，但它仍然不是最有效或最有效的方法。
- en: 'Bergstra and Bengio agree: “Our analysis of the hyperparameter response surface
    suggests that random experiments are more efficient because not all hyperparameters
    are equally important to tune. Grid search experiments allocate too many trials
    to the exploration of dimensions that do not matter and suffer from poor coverage
    in dimensions that are important.” In the next section, we talk about how they
    did something about it by creating a novel algorithm that is truly brilliant.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 伯格斯特拉和本吉奥达成共识：“我们对超参数响应表面的分析表明，随机实验更有效率，因为并非所有超参数对调整同等重要。网格搜索实验在探索不重要的维度上分配了过多的试验，并且在重要的维度上覆盖不足。”在下一节中，我们将讨论他们如何通过创建一个真正出色的新算法来解决这个问题。
- en: 'Model-based optimization: Tree-structured Parzen estimators (Hyperopt)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的优化：树结构帕累托估计器（Hyperopt）
- en: We face a complex search for hyperparameters in our time-series forecasting
    model—11 total hyperparameters, 3 continuously distributed and 1 ordinal—confounding
    the ability to effectively search the space. The preceding approaches are either
    too time-consuming (manual, grid search), expensive (grid search), or difficult
    to achieve adequate fit characteristics for validation against holdout data (all
    of them).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的时间序列预测模型中，我们面临着复杂的超参数搜索——总共有11个超参数，其中3个是连续分布的，1个是序数——这混淆了有效搜索空间的能力。前面提到的方法要么太耗时（手动、网格搜索），要么成本高昂（网格搜索），或者难以获得足够的拟合特征以验证保留数据（所有这些方法）。
- en: 'The same team that brought the paper arguing that random search is a superior
    methodology to grid search also arrived at a process for selecting an optimized
    hyperparameter response surface: using Bayesian techniques in a model-based optimization
    relying on either Gaussian processes or tree of Parzen estimators (TPEs). The
    results of their research are provided in the open source software package Hyperopt.
    Figure 7.5 shows at a high level how Hyperopt works.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个团队提出了随机搜索是比网格搜索更优越的方法论的论文，他们也提出了一种选择优化超参数响应表面的过程：使用基于模型的优化，依赖于高斯过程或帕累托树估计器（TPEs）的贝叶斯技术。他们的研究成果包含在开源软件包Hyperopt中。图7.5展示了Hyperopt工作的基本原理。
- en: '![07-05](../Images/07-05.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../Images/07-05.png)'
- en: Figure 7.5 A high-level diagram of how Hyperopt’s tree-structured Parzen estimator
    algorithm works
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 Hyperopt的树结构帕累托估计器算法的高层次图示
- en: This system is nearly guaranteed to outperform even the most experienced DS
    working through any of the earlier mentioned classical tuning approaches. Not
    only is it remarkably capable of exploring complex hyperparameter spaces, but
    it can do so in far fewer iterations than other methodologies. For further reading
    on this topic, I recommend perusing the original 2011 whitepaper, “Algorithms
    for Hyper-Parameter Optimization” by James Bergstra et al. ([http://mng.bz/W76w](https://shortener.manning.com/W76w))
    and reading the API documentation for the package for further evidence of its
    effectiveness ([http://hyperopt.github.io/hyperopt/](http://hyperopt.github.io/hyperopt/)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统几乎可以保证在通过任何前面提到的经典调优方法进行工作的经验丰富的DS（数据科学家）中表现优异。它不仅能够非常出色地探索复杂的超参数空间，而且比其他方法需要的迭代次数要少得多。关于这个主题的进一步阅读，我推荐阅读James
    Bergstra等人于2011年撰写的原始白皮书，“超参数优化算法”（[http://mng.bz/W76w](https://shortener.manning.com/W76w)），以及阅读该包的API文档以获取其有效性的更多证据（[http://hyperopt.github.io/hyperopt/](http://hyperopt.github.io/hyperopt/))。
- en: More advanced (and complex) techniques
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级（且复杂）的技术
- en: 'Anything more advanced than Hyperopt’s TPE and similar automated tuning packages
    typically means doing one of two things: paying a company that offers an automated-ML
    (autoML) solution or building your own. In the realm of building a custom tuner
    solution, you might look into a mixture of genetic algorithms with Bayesian prior
    search optimization to create search candidates within the *n*-dimensional hyperparameter
    space that have the highest likelihood of giving a good result, leveraging the
    selective optimization that genetic algorithms are known for.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 任何比Hyperopt的TPE和类似的自动化调优包更高级的技术通常意味着做两件事之一：支付提供自动化-ML（autoML）解决方案的公司，或者自己构建。在构建自定义调谐解决方案的领域，你可能会考虑将遗传算法与贝叶斯先验搜索优化相结合，以在*n*-维超参数空间中创建具有最高成功概率的搜索候选者，利用遗传算法所知的选择性优化。
- en: Speaking from the perspective of someone who has built one of these autoML solutions
    ([https://github.com/databrickslabs/automl-toolkit](https://github.com/databrickslabs/automl-toolkit)),
    I cannot recommend going down this path unless you’re building out a custom framework
    for hundreds (or more) different projects and have a distinct need for a high-performance
    and lower-cost optimization tool specifically customized to solve the sorts of
    problems that your company is facing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个构建了这些autoML解决方案（[https://github.com/databrickslabs/automl-toolkit](https://github.com/databrickslabs/automl-toolkit)）的人的角度来看，除非你正在为数百（或更多）个不同的项目构建自定义框架，并且有明确的需求来开发一个高性能且成本较低的优化工具，专门用于解决公司面临的问题，否则我不建议走这条路。
- en: AutoML is definitely not a palatable option for most experienced DS teams, however.
    The very nature of these solutions, being largely autonomous apart from a configuration-driven
    interface, forces you to relinquish control and visibility into the decision logic
    contained within the software. You lose the ability to discover the reasoning
    behind why some features are culled and others are created, why a particular model
    was selected, and what internal validations may have been performed on your feature
    vector to achieve the purported best results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AutoML绝对不是大多数经验丰富的DS团队的可接受选项。这些解决方案的本质，除了配置驱动的界面外，在很大程度上是自主的，这迫使你放弃对软件中包含的决策逻辑的控制和可见性。你失去了发现为什么某些特征被删除而其他特征被创建的原因，为什么选择了特定的模型，以及为了实现声称的最佳结果，可能对特征向量进行了哪些内部验证的能力。
- en: Setting aside that these solutions are black boxes, it’s important to recognize
    the target audience for these applications. These full-featured pipeline-generation
    toolkits are not designed or intended for use by seasoned ML developers in the
    first place. They’re built for the unfortunately named *citizen data scientist*—the
    SMEs who know their business needs intimately but don’t have the experience or
    knowledge to handcraft an ML solution by themselves.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些解决方案是黑盒之外，重要的是要认识到这些应用的目标受众。这些功能齐全的管道生成工具包最初并不是为经验丰富的ML（机器学习）开发者设计的或打算使用的。它们是为不幸被称为*公民数据科学家*的人构建的——这些人是业务领域的专家，他们深知自己的业务需求，但没有经验或知识自己手工制作ML解决方案。
- en: Building a framework to automate some of the more (arguably) boring and rudimentary
    modeling needs that your company faces may seem exciting. It certainly can be.
    These frameworks aren’t exactly simple to build, though. If you’re going down
    the path of building something custom, like an autoML framework, make sure that
    you have the bandwidth to do so, that the business understands and approves of
    this massive project, and that you can justify your return on a substantial investment
    of time and resources. During the middle of a project is not the time to tack
    on months of cool work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个框架来自动化公司面临的一些更（可以说是）无聊和基础建模需求可能看起来很令人兴奋。这确实可以。然而，这些框架并不简单构建。如果你正在走定制化构建的道路，比如一个
    autoML 框架，确保你有足够的带宽去做这件事，确保业务理解并批准这个庞大的项目，以及你能够证明在大量时间和资源投入上的回报。在项目进行到中途时，不是添加几个月酷炫工作的好时机。
- en: 7.1.2 Hyperopt primer
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 Hyperopt 入门
- en: Going back to our project work with forecasting, we can confidently assert that
    the best approach for tuning the models for each airport is going to be through
    using Hyperopt and its TPE approach.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的预测项目工作中，我们可以自信地断言，为每个机场调整模型的最佳方法将是通过使用 Hyperopt 和其 TPE 方法。
- en: NOTE Hyperopt is a package that is external to the build of Anaconda we’ve been
    using. To use it, you must perform a pip or conda install of the package in your
    environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Hyperopt 是一个Anaconda构建之外的外部包。要使用它，你必须在你的环境中执行 pip 或 conda 安装包。
- en: Before we get into the code that we’ll be using, let’s look at how this API
    works from a simplified implementation perspective. To begin, the first aspect
    of Hyperopt is in the definition of an objective function (listing 7.1 shows a
    simplified implementation of a function for finding a minimization). This objective
    function is, typically, a model that is fit on training data, validated on testing
    data, scored, and returns the error metric associated with the predicted data
    as compared to the validation data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究将要使用的代码之前，让我们从简化的实现角度来看看这个API是如何工作的。首先，Hyperopt 的第一个方面在于目标函数的定义（列表 7.1
    展示了寻找最小化的函数的简化实现）。这个目标函数通常是拟合在训练数据上的模型，在测试数据上验证，评分，并返回与验证数据相比预测数据的误差度量。
- en: 'Listing 7.1 Hyperopt fundamentals: The objective function'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 Hyperopt 基础：目标函数
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Defines the objective function to minimize
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义要最小化的目标函数
- en: ❷ A one-dimensional fourth-order polynomial equation that we want to solve for
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们想要求解的一维四次多项式方程
- en: ❸ Loss estimation for the minimization optimization
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 最小化优化的损失估计
- en: Why Hyperopt?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择 Hyperopt？
- en: I’m using Hyperopt for this discussion simply because it’s widely used. Other
    tools perform similar and arguably more advanced versions of what this package
    is designed to do (optimize hyperparameters). Optuna ([https://optuna.org](https://optuna.org/))
    is a rather notable continuation of the work of the original research that went
    into building Hyperopt. I highly encourage you to check it out.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 Hyperopt 进行这次讨论仅仅是因为它被广泛使用。其他工具执行类似甚至更高级的版本，这些版本是设计这个包要做的（优化超参数）。Optuna
    ([https://optuna.org](https://optuna.org/)) 是对构建 Hyperopt 的原始研究工作的一个相当显著的延续。我强烈建议您去了解一下。
- en: The point of this book isn’t about technology. It’s about the processes that
    surround the use of technology. At some point in the not so distant future, a
    better tech will come out. A more optimal way of finding optimized parameters
    will come along. Furtherance of the field is something that is constant, inevitable,
    and rapid. I’m not interested in discussing how one technology is better than
    another. Plenty of other books do that. I’m interested in discussing why it’s
    important to use something to solve this problem. Feel free to the choose the
    *something* that feels right for you.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的重点不在于技术。它关注的是围绕技术使用的过程。在不久的将来，会出现更好的技术。会有一种更优的方法来寻找优化参数。该领域的发展是持续、不可避免且快速的。我对讨论一种技术比另一种技术更好不感兴趣。很多其他书籍都在做这件事。我感兴趣的是讨论为什么使用某种东西来解决这个问题很重要。请随意选择对你来说感觉正确的“某种东西”。
- en: After we have declared an objective function, the next phase in using Hyperopt
    is to define a space to search over. For this example, we’re interested in only
    a single value to optimize for, in order to solve the minimization of the polynomial
    function in listing 7.1\. In the next listing, we define the search space for
    this one `x` variable for the function, instantiating the `Trials` object (for
    recording the history of the optimization), and running the optimization with
    the minimization function from the Hyperopt API.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们声明了目标函数之后，使用 Hyperopt 的下一个阶段是定义一个搜索空间。对于这个例子，我们只对优化单个值感兴趣，以便解决 7.1 列表中多项式函数的最小化问题。在下一个列表中，我们定义了这个函数一个
    `x` 变量的搜索空间，实例化 `Trials` 对象（用于记录优化历史），并使用 Hyperopt API 中的最小化函数运行优化。
- en: Listing 7.2 Hyperopt optimization for a simple polynomial
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 Hyperopt 对简单多项式的优化
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Defines the search space—in this case, a uniform sampling between -12 and
    12 for the seed and bounded Gaussian random selection for the TPE algorithm after
    the initial seed priors return
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义搜索空间——在这种情况下，种子在 -12 和 12 之间的均匀采样，以及在初始种子先验返回后的 TPE 算法的有界高斯随机选择
- en: ❷ Instantiates the Trials object to record the optimization history
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实例化 Trials 对象以记录优化历史
- en: ❸ The objective function as defined in listing 7.1, passed in to the fmin optimization
    function of Hyperopt
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如 7.1 列表中定义的目标函数，传递给 Hyperopt 的 fmin 优化函数
- en: ❹ The space to search, defined above (-12 to 12, uniformly)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 搜索空间，如上所述（-12 到 12，均匀分布）
- en: ❺ The optimization algorithm to use—in this case, tree-structured Parzen estimator
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 要使用的优化算法——在这种情况下，树结构 Parzen 估计器
- en: ❻ Passes the Trials object into the optimization function to record the history
    of the run
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将 Trials 对象传递给优化函数以记录运行历史
- en: ❼ The number of optimization runs to conduct. Since hpopt is iterations-bound,
    we can control the runtime of the optimization in this manner.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 进行优化运行的次数。由于 hpopt 是受迭代次数限制的，我们可以通过这种方式控制优化的运行时间。
- en: Once we execute this code, we will receive a progress bar (in Jupyter-based
    notebooks) that will return the best loss that has been discovered throughout
    the history of the run as it optimizes. At the conclusion of the run, we will
    get as a return value from `trial_estimator` the optimal setting for `x` to minimize
    the value returned from the polynomial defined in the function `objective_function`.
    The following listing shows how this process works for this simple example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行此代码，我们将收到一个进度条（在基于 Jupyter 的笔记本中），它将在优化过程中返回运行历史中发现的最佳损失。在运行结束时，我们将从 `trial_estimator`
    获取 `x` 的最佳设置，以最小化函数 `objective_function` 中定义的多项式返回的值。以下列表显示了此简单示例的工作过程。
- en: Listing 7.3 Hyperopt performance in minimizing a simple polynomial function
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 Hyperopt 在最小化简单多项式函数中的性能
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Generates a range of x values for plotting the function defined in listing
    7.1
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成一系列 x 值，用于绘制 7.1 列表中定义的函数
- en: ❷ Retrieves the corresponding y values for each of the x values from the rng
    collection
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 rng 集合中检索每个 x 值对应的 y 值
- en: ❸ Plots the function across the x space of rng
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在 rng 的 x 空间上绘制函数
- en: ❹ Plots the optimized minima that Hyperopt finds based on our search space
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制 Hyperopt 根据我们的搜索空间找到的优化最小值
- en: ❺ Adds an annotation to the graph to indicate the minimized value
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在图表上添加注释以指示最小化值
- en: Running this script results in the plot in figure 7.6.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本会导致图 7.6 中的图表。
- en: '![07-06](../Images/07-06.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![07-06](../Images/07-06.png)'
- en: Figure 7.6 Using Hyperopt to solve for the minimal value of a simple polynomial
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 使用 Hyperopt 求解简单多项式的最小值
- en: Linear models frequently have “dips” and “valleys” between parameters and their
    loss metrics. We use the terms *local minima* and *local maxima* to describe them.
    If the parameter search space isn’t explored sufficiently, a model’s tuning could
    reside in a local, instead of the global, minima or maxima.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型在参数和它们的损失度量之间经常有“凹陷”和“山谷”。我们使用术语 *局部最小值* 和 *局部最大值* 来描述它们。如果参数搜索空间没有得到充分探索，模型的调整可能位于局部而不是全局的最小值或最大值。
- en: 7.1.3 Using Hyperopt to tune a complex forecasting problem
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 使用 Hyperopt 调整复杂预测问题
- en: Now that you understand the concepts behind this automated model-tuning package,
    we can apply it to our complex forecasting modeling problem. As we discussed earlier
    in this chapter, tuning this model is going to be complex if we don’t have some
    assistance. Not only are there 11 hyperparameters to explore, but the success
    that we had in chapter 6 at manually tuning was not particularly impressive.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了这种自动化模型调整包背后的概念，我们可以将其应用于我们复杂的预测建模问题。正如我们在本章前面讨论的，如果没有一些帮助，调整这个模型将会很复杂。不仅我们有11个超参数要探索，而且我们在第6章中手动调整所取得的成果并不特别令人印象深刻。
- en: We need something to help us. Let’s let Thomas Bayes lend a hand (or, rather,
    Pierre-Simon Laplace). Listing 7.4 shows our optimization function for the Holt-Winters
    exponential smoothing (HWES) model for passengers at airports.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些帮助。让我们让托马斯·贝叶斯伸出援手（或者更确切地说，皮埃尔-西蒙·拉普拉斯）。列表7.4显示了我们的Holt-Winters指数平滑（HWES）模型针对机场乘客的优化函数。
- en: Listing 7.4 Minimization function for Holt-Winters exponential smoothing
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4：Holt-Winters指数平滑的最小化函数
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ selected_hp_values is a multilevel dictionary. Since we have two separate
    sections of hyperparameters to apply and some of the parameter names are similar,
    we separate them between “model” and “fit” to reduce confusion.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ selected_hp_values是一个多级字典。由于我们有两组独立的超参数部分要应用，并且一些参数名称相似，我们通过“model”和“fit”来区分它们，以减少混淆。
- en: ❷ Instantiates the ExponentialSmoothing class as an object, configured with
    the values that Hyperopt will be selecting for each model iteration to test
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将ExponentialSmoothing类实例化为一个对象，配置了Hyperopt将为每个模型迭代测试选择的价值
- en: ❸ The fit method has its own set of hyperparameters that Hyperopt will be selecting
    for the pool of models it will generate and test.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ fit方法有其自己的超参数集，Hyperopt将为它将生成和测试的模型池选择这些超参数。
- en: ❹ Generates the forecast for this run of the model to perform validation and
    scoring against. We are forecasting from the point of the end of the training
    set to the last value of the test set’s index.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为这次模型运行生成预测，以执行验证和评分。我们是从训练集的末尾到测试集索引的最后一个值进行预测。
- en: ❺ A utility function to get the number of parameters (viewable in the book’s
    GitHub repository)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 一个获取参数数量（可在书籍的GitHub仓库中查看）的实用函数
- en: ❻ Removes the first entry of the forecast since it overlaps with the training
    set’s last index entry
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 移除预测的第一个条目，因为它与训练集最后一个索引条目重叠
- en: ❼ Calculates all of the error metrics—Akaike information criterion (AIC) and
    Bayesian information criterion (BIC), newly added metrics, requires the hyperparameter
    count
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算所有错误指标——赤池信息准则（AIC）和贝叶斯信息准则（BIC），新添加的指标需要超参数计数
- en: ❽ The only return from the minimization function for Hyperopt is a dictionary
    containing the metric under test for optimization and a status report message
    from within the Hyperopt API. The Trials() object will persist all of the data
    about the runs and a tuned best model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ Hyperopt的最小化函数的唯一返回值是一个字典，包含用于优化的测试指标和来自Hyperopt API的状态报告消息。Trials()对象将持久化所有关于运行和最佳调整模型的数据。
- en: As you may recall from chapter 6, when creating the prototype for this algorithm,
    we hardcoded several of these values (`smoothing_level`, `smoothing_seasonal`,
    `use_brute`, `use_boxcox`, `use_basin_hopping`, and `remove_bias`) to make the
    prototyping tuning a bit easier. In listing 7.4, we’re setting all of these values
    as tunable hyperparameters for Hyperopt. Even with such a large search space,
    the algorithm will allow us to explore the influence of all of them over the predictive
    capabilities of the holdout space. If we were using something permutations-based
    (or, worse, human-short-term-memory-based) such as a grid search, we likely wouldn’t
    want to include all of these for the sole reason of factorially increasing runtime.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆的，在第6章中创建这个算法的原型时，我们硬编码了几个这些值（`smoothing_level`，`smoothing_seasonal`，`use_brute`，`use_boxcox`，`use_basin_hopping`和`remove_bias`），以使原型调整变得更容易。在列表7.4中，我们将所有这些值设置为Hyperopt的可调整超参数。即使搜索空间如此之大，该算法也会允许我们探索它们对保留空间预测能力的影响。如果我们使用基于排列的（或者更糟糕的是，基于人类短期记忆的）方法，如网格搜索，我们可能不会想包括所有这些，仅仅是因为它们会以阶乘的方式增加运行时间。
- en: 'Now that we have our model-scoring implementation done, we can move on to the
    next critical phase of efficiently tuning these models,: defining the search space
    for the hyperparameters.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了模型评分的实现，我们可以继续到下一个关键阶段，即高效调整这些模型：定义超参数的搜索空间。
- en: Listing 7.5 Hyperopt exploration space configuration
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 Hyperopt 探索空间配置
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ For readability’s sake, we’re splitting the configuration between the class-level
    hyperparameters (model) and the method-level hyperparameters (fit) since some
    of the names for the two are similar.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了可读性，我们将配置分为类级别超参数（模型）和方法级别超参数（fit），因为其中一些名称是相似的。
- en: ❷ hp.choice is used for Boolean and multivariate selection (choose one element
    from a list of possible values).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ hp.choice 用于布尔值和多变量选择（从可能的值列表中选择一个元素）。
- en: ❸ hp.quniform chooses a random value uniformly in a quantized space (in this
    example, we’re choosing a multiple of 12, between 12 and 120).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ hp.quniform 在量化空间中随机选择一个值（在本例中，我们选择 12 的倍数，介于 12 和 120 之间）。
- en: ❹ hp.uniform selects randomly through the continuous space (here, between 0.01
    and 0.99).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ hp.uniform 在连续空间中随机选择（此处，介于 0.01 和 0.99 之间）。
- en: The settings in this code are the total sum of hyperparameters available for
    the `ExponentialSmoothing()` class and the `fit()` method as of statsmodels version
    0.11.1\. Some of these hyperparameters may not influence the predictive power
    of our model. If we had been evaluating this through grid search, we would likely
    have omitted them from our evaluation. With Hyperopt, because of the manner in
    which its algorithm provides greater weight to influential parameters, leaving
    them in for evaluation doesn’t dramatically increase the total runtime.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码中的设置是截至 statsmodels 版本 0.11.1 可用的 `ExponentialSmoothing()` 类和 `fit()` 方法的所有超参数的总和。其中一些超参数可能不会影响我们模型的预测能力。如果我们通过网格搜索进行评估，我们可能会从评估中省略它们。由于
    Hyperopt 的算法以更大的权重提供有影响力的参数，因此将它们保留在评估中不会显著增加总运行时间。
- en: The next step for automating away the daunting task of tuning this temporal
    model is to build a function to execute the optimization, collect the data from
    the tuning run, and generate plots that we can use to further optimize the search
    space as defined in listing 7.5 on subsequent fine-tuning runs. Listing 7.6 shows
    our final execution function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化调整此时间模型的令人畏惧的任务的下一步是构建一个函数来执行优化，收集调优运行的数据，并生成我们可以用于在后续微调运行中进一步优化搜索空间的图表。列表
    7.6 显示了我们的最终执行函数。
- en: NOTE Please refer to the companion repository to this book at [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)
    to see the full code for all of the functions called in listing 7.6\. A more thorough
    discussion is included there in a downloadable and executable notebook.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请参阅本书的配套仓库 [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)，以查看列表
    7.6 中调用所有函数的完整代码。其中包含更详细的讨论，可在可下载和可执行的工作簿中找到。
- en: Listing 7.6 Hyperopt tuning execution
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 Hyperopt 调优执行
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Because of the volume of configurations used to execute the tuning run and
    collect all the visualizations and data from the optimization, we’ll use named
    dictionary-based argument passing (**kwargs).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于执行调优运行和收集优化过程中所有可视化和数据的配置量很大，我们将使用基于命名字典的参数传递（**kwargs**）。
- en: ❷ To calculate AIC and BIC, we need the total number of hyperparameters being
    optimized. Instead of forcing the user of this function to count them, we can
    extract them from the passed-in Hyperopt configuration element tuning_space.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要计算 AIC 和 BIC，我们需要优化中使用的超参数总数。我们不必强迫此函数的用户计数，我们可以从传递的 Hyperopt 配置元素 tuning_space
    中提取它们。
- en: ❸ The Trials() object records each of the, well, trials of different hyperparameter
    experiments and allows us to see how the optimization converged.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Trials() 对象记录了每个超参数实验的不同试验，并允许我们查看优化是如何收敛的。
- en: ❹ fmin() is the main method for initiating a Hyperopt run. We’re using a partial
    function as a wrapper around the per-model static attributes so that the sole
    differences between each Hyperopt iteration is in the variable hyperparameters,
    keeping the other attributes the same.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ fmin() 是启动 Hyperopt 运行的主方法。我们使用部分函数作为对每个模型静态属性的包装器，这样每个 Hyperopt 迭代的唯一区别就在于变量超参数，而其他属性保持不变。
- en: ❺ The tuning space defined in listing 7.5
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 列表 7.5 中定义的调优空间
- en: ❻ The optimization algorithm for Hyperopt (random, TPE, or adaptive TPE), which
    can be automated or manually controlled
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ Hyperopt 的优化算法（随机、TPE 或自适应 TPE），可以是自动化的或手动控制的
- en: ❼ The number of models to test and search through to find an optimal configuration
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 测试和搜索以找到最佳配置的模型数量
- en: ❽ Extracts the best model from the Trials() object
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 从Trials()对象中提取最佳模型
- en: ❾ Rebuilds the best model to record and store
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 重建最佳模型以记录和存储
- en: ❿ Pulls the tuning information out of the Trials() object for plotting
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 从Trials()对象中提取调整信息以进行绘图
- en: ⓫ Plots the trial history)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 绘制试验历史图
- en: ⓬ Builds the future forecast for as many points as specified in the future_forecast_periods
    configuration value
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 根据future_forecast_periods配置值指定的点数构建未来预测
- en: ⓭ Plots the forecast over the holdout validation period to show test vs. forecast
    (updated version from chapter 6 visualization)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 在保留验证期间绘制预测，以显示测试与预测（来自第6章的可视化更新版本）
- en: note To read more about how partial functions and Hyperopt work, see the Python
    documentation at [https://docs.python.org/3/library/functools.html#functools.partial](https://docs.python.org/3/library/functools.html#functools.partial)
    and the Hyperopt doc and source code at [https://github.com/hyperopt/hyperopt.github.io](https://github.com/hyperopt/hyperopt.github.io).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：想了解更多关于部分函数和Hyperopt如何工作的信息，请参阅Python文档中的[https://docs.python.org/3/library/functools.html#functools.partial](https://docs.python.org/3/library/functools.html#functools.partial)以及Hyperopt文档和源代码[https://github.com/hyperopt/hyperopt.github.io](https://github.com/hyperopt/hyperopt.github.io)。
- en: Note Listing 7.6’s custom plot code is available in the companion repository
    for this book; see the Chapter7 notebook at [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表7.6的自定义绘图代码可在本书的配套仓库中找到；请参阅[https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)的Chapter7笔记本。
- en: Executing the call to `plot_predictions``()` from listing 7.6 is shown in figure
    7.7\. Calling `generate_hyperopt_report``()` from listing 7.6 results in the plot
    shown in figure 7.8.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 执行列表7.6中的`plot_predictions()`调用如图7.7所示。从列表7.6中调用`generate_hyperopt_report()`产生如图7.8所示的图表。
- en: '![07-07](../Images/07-07.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![07-07](../Images/07-07.png)'
- en: Figure 7.7 Prediction backtesting on the most recent data from the total time
    series (x-axis zoomed for legibility)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 对总时间序列最近数据的预测回测（x轴放大以提高可读性）
- en: 'By using Hyperopt to arrive at the best predictions on our holdout data, we’ve
    optimized the hyperparameters to a degree that we can be confident of having a
    good projection of the future state (provided that no unexpected and unknowable
    latent factors affect it). Thus, we’ve addressed several key challenging elements
    in the optimization phase of ML work by using automated tuning:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Hyperopt在我们的保留数据上获得最佳预测，我们将超参数优化到了一个程度，可以自信地预测未来的状态（前提是没有意外和不可知的潜在因素影响它）。因此，我们通过使用自动调整来解决了ML工作中优化阶段的一些关键挑战性元素：
- en: '*Accuracy*—The forecast is as optimal as it can be (for each model, provided
    that we select a reasonable search space and run through enough iterations).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性*—预测是最优的（对于每个模型，前提是我们选择合理的搜索空间并运行足够的迭代）。'
- en: '*Timeliness in training*—With this level of automation, we get well-tuned models
    in minutes instead of days (or weeks).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练的及时性*—在这个自动化水平下，我们可以在几分钟内而不是几天（或几周）内获得调优良好的模型。'
- en: '*Maintainability*—Automating tuning keeps us from having to manually retrain
    models as the baseline shifts over time.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可维护性*—自动化调整使我们不必手动重新训练模型，因为基线随着时间的推移而变化。'
- en: '*Timeliness in development*—Since our code is pseudo-modular (using modularized
    functions within a notebook), the code is reusable, extensible, and capable of
    being utilized through a control loop to build all the models for each airport
    with ease.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开发的及时性*—由于我们的代码是准模块化的（在笔记本中使用模块化函数），代码是可重用的、可扩展的，并且可以通过控制循环轻松地用于构建每个机场的所有模型。'
- en: '![07-08](../Images/07-08.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![07-08](../Images/07-08.png)'
- en: Figure 7.8 Sampled results of hyperparameters for the Hyperopt trials run
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 Hyperopt试验中采样到的超参数结果
- en: NOTE The extracted code samples that we’ve just gone through with Hyperopt are
    part of a much larger end-to-end example hosted within the book’s repository in
    the Notebooks section for chapter 7\. In this example, you can see the automated
    tuning and optimization for all airports within this dataset and all utility functions
    that are built to support this effective tuning of models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们刚刚使用Hyperopt提取的代码示例是书中仓库Notebooks部分的一个更大端到端示例的一部分。在这个示例中，你可以看到对数据集中所有机场的自动化调整和优化，以及为支持这种有效的模型调整而构建的所有实用函数。
- en: 7.2 Choosing the right tech for the platform and the team
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 为平台和团队选择正确的技术
- en: The forecasting scenario we’ve been walking through, when executed in a virtual
    machine (VM) container and running automated tuning optimization and forecasting
    for a single airport, worked quite well. We got fairly good results for each airport.
    By using Hyperopt, we also managed to eliminate the unmaintainable burden of manually
    tuning each model. While impressive, it doesn’t change the fact that we’re not
    looking to forecast passengers at just a single airport. We need to create forecasts
    for thousands of airports.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在讨论的预测场景，当在虚拟机（VM）容器中执行，并对单个机场进行自动调整优化和预测时，效果相当不错。我们对每个机场都得到了相当好的结果。通过使用Hyperopt，我们还成功地消除了手动调整每个模型的不可维护的负担。虽然令人印象深刻，但这并没有改变我们不仅仅是在预测单个机场的乘客的事实。我们需要为数千个机场创建预测。
- en: Figure 7.9 shows what we’ve built, in terms of wall-clock time, in our efforts
    thus far. The synchronous nature of each airport’s models (in a `for`loop) and
    Hyperopt’s Bayesian optimizer (also a serial loop) means that we’re waiting for
    models to be built one by one, each next step waiting on the previous to be completed,
    as we discussed in section 7.1.2.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9展示了我们迄今为止在墙钟时间方面所做的工作。每个机场模型（在`for`循环中）和Hyperopt的贝叶斯优化器（也是一个串行循环）的同步性质意味着我们正在等待模型一个接一个地构建，每个后续步骤都在等待前一个步骤完成，正如我们在第7.1.2节中讨论的那样。
- en: '![07-09](../Images/07-09.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![07-09](../Images/07-09.png)'
- en: Figure 7.9 Serial tuning in single-threaded execution
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 单线程执行中的串行调整
- en: This problem of ML at scale, as shown in this diagram, is a stumbling block
    for many teams, mostly because of complexity, time, and cost (and is one the primary
    reasons why projects of this scale are frequently cancelled). Solutions exist
    for these scalability issues for ML project work; each involves stepping away
    from the realm of serial execution and moving into the world of distributed, asynchronous,
    or a mixture of both of these paradigms of computing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如此大规模的机器学习问题，如图所示，是许多团队的绊脚石，主要是因为复杂性、时间和成本（这也是为什么此类规模的项目经常被取消的主要原因之一）。对于这些可扩展性问题，存在机器学习项目工作的解决方案；每个解决方案都涉及离开串行执行领域，进入分布式、异步或这两种计算范例的混合世界。
- en: The standard structured code approach for most Python ML tasks is to execute
    in a serial fashion. Whether it be a list comprehension, a lambda, or a `for`
    (`while`) loop, ML is steeped in sequential execution. This approach can be a
    benefit, as it reduces memory pressure for many algorithms that have a high memory
    requirement, particularly those that use recursion, which are many. But this approach
    can also be a handicap, as it takes much longer to execute, since each subsequent
    task is waiting for the previous to complete.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数Python机器学习任务的标准结构化代码方法是以串行方式执行。无论是列表推导式、lambda函数还是`for`（`while`）循环，机器学习都深深植根于顺序执行。这种方法可能是一个优点，因为它可以减少许多具有高内存需求的算法的内存压力，尤其是那些使用递归的算法，而递归算法很多。但这种方法也可能是一个缺点，因为它执行时间更长，因为每个后续任务都在等待前一个任务完成。
- en: We will discuss concurrency in ML briefly in section 7.4 and in more depth in
    later chapters (both safe and unsafe ways of doing it). For now, with the issue
    of scalability with respect to wall-clock time for our project, we need to look
    into a *distributed approach* to this problem in order to explore our search spaces
    faster for each airport. It is at this point that we stray from the world of our
    single-threaded VM approach and move into the distributed computing world of Apache
    Spark.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第7.4节简要讨论机器学习中的并发性，并在后面的章节中更深入地讨论（包括安全和不可安全的方法）。现在，鉴于我们项目相对于墙钟时间的可扩展性问题，我们需要考虑一种*分布式方法*来解决这个问题，以便更快地为每个机场探索搜索空间。正是在这一点上，我们离开了单线程虚拟机方法的世界，进入了Apache
    Spark的分布式计算世界。
- en: 7.2.1 Why Spark?
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 为什么使用Spark？
- en: 'Why use Spark? In a word: speed.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用Spark？一言以蔽之：速度。
- en: For the problem that we’re dealing with here, forecasting each month the passenger
    expectations at each major airport in the United States, we’re not bound by SLAs
    that are measured in minutes or hours, but we still need to think about the amount
    of time it takes to run our forecasting. There are multiple reasons for this,
    chiefly
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在这里处理的问题，即预测美国每个主要机场每月的乘客期望值，我们不受以分钟或小时为单位的SLA（服务等级协议）的约束，但我们仍然需要考虑运行预测所需的时间量。这有多个原因，主要
- en: '*Time*—If we’re building this job as a monolithic modeling event, any failures
    in an extremely long-running job will require a restart (imagine the job failing
    after it was 99% complete, running for 11 days straight).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间*—如果我们将这个任务作为一个单体建模事件来构建，任何在运行时间极长的任务中的失败都将需要重启（想象一下任务在完成99%后失败，连续运行了11天）。'
- en: '*Stability*—We want to be very careful about object references within our job
    and ensure that we don’t create a memory leak that could cause the job to fail.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稳定性*—我们非常关注任务中的对象引用，并确保我们不会创建可能导致任务失败的内存泄漏。'
- en: '*Risk*—Keeping machines dedicated to extremely long-running jobs (even in cloud
    providers) risks platform issues that could bring down the job.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*风险*—将机器专门用于运行时间极长的任务（即使在云服务提供商那里）可能会带来平台问题，这些问题可能会导致任务失败。'
- en: '*Cost*—Regardless of where your virtual machines are running, someone is paying
    the bill for them.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本*—无论你的虚拟机在哪里运行，总有人为它们支付账单。'
- en: When we focus on tackling these high-risk factors, distributed computing offers
    a compelling alternative to serial looped execution, not only because of cost,
    but mostly because of the speed of execution. Were any issues to arise in the
    job, unforeseen issues with the data, or problems with the underlying hardware
    that the VMs are running on, these dramatically reduced execution times for our
    forecasting job will give us flexibility to get the job up and running again with
    predicted values returning much faster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们专注于解决这些高风险因素时，分布式计算提供了对串行循环执行的吸引人的替代方案，这不仅因为成本，而且主要是因为执行速度。如果在任务中出现问题，数据中不可预见的问题，或者虚拟机运行的底层硬件问题，这些显著减少的预测任务执行时间将给我们提供灵活性，以便使用预测值快速重新启动任务。
- en: A brief note on Spark
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Spark的简要说明
- en: Spark is a large topic, a monumentally large ecosystem, and an actively contributed-to
    open source distributed computing platform based on the Java Virtual Machine (JVM).
    Because this isn’t a book about Spark per se, I won’t go too deep into the inner
    workings of it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个大主题，一个巨大的生态系统，一个基于Java虚拟机（JVM）的活跃开源分布式计算平台。因为这不是一本关于Spark本身的书籍，所以我不打算深入探讨其内部工作原理。
- en: 'Several notable books have been written on the subject, and I recommend reading
    them if you are inclined to learn more about the technology: *Learning Spark*
    by Jules Damji et al. (O’Reilly, 2020), *Spark: The Definitive Guide* by Bill
    Chambers and Matei Zaharia (O’Reilly, 2018), and *Spark in Action* by Jean-Georges
    Perrin (Manning, 2020).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '关于这个主题已经写了几本值得注意的书，如果你倾向于了解更多关于这项技术的信息，我推荐阅读它们：Jules Damji等人所著的《Learning Spark》（O’Reilly，2020年），Bill
    Chambers和Matei Zaharia所著的《Spark: The Definitive Guide》（O’Reilly，2018年），以及Jean-Georges
    Perrin所著的《Spark in Action》（Manning，2020年）。'
- en: Suffice it to say, in this book, we will explore how to effectively utilize
    Spark to perform ML tasks. Many examples from this point forward are focused on
    leveraging the power of the platform to perform large-scale ML (both training
    and inference).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在这本书中，我们将探讨如何有效地利用Spark来执行机器学习任务。从现在开始，许多示例都集中在利用平台的力量来执行大规模机器学习（包括训练和推理）。
- en: For the current section, the information covered is relatively high level with
    respect to how Spark works for these examples; instead, we focus entirely on how
    we can use it to solve our problems.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前章节，所涵盖的信息与Spark如何工作这些示例相比相对较高层次；相反，我们完全专注于如何使用它来解决问题。
- en: But how is Spark going to help us with this problem? We can employ two relatively
    straightforward paradigms, shown in figure 7.10\. We could use more than just
    these two, but we’re going to start with the straightforward and less complex
    ones for now; the more advanced approaches are mentioned in section 7.4.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但Spark如何帮助我们解决这个问题呢？我们可以采用两种相对简单直观的范式，如图7.10所示。我们可以使用不止这两种，但现在我们将从简单且不太复杂的开始；更高级的方法在第7.4节中提到。
- en: '![07-10](../Images/07-10.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![07-10](../Images/07-10.png)'
- en: Figure 7.10 Scaling hyperparameter tuning using pandas_udf on Spark
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 使用pandas_udf在Spark上扩展超参数调整
- en: The first approach is to leverage the workers within the cluster to execute
    parallel evaluation of the hyperparameters. In this paradigm, our time-series
    dataset will need to be collected (materialized in full) from the workers to the
    driver. Limitations exist (serialization size of the data is currently limited
    to 2 GB at the time of this writing), and for many ML use cases on Spark, this
    approach shouldn’t be used. For time-series problems such as this one, this approach
    will work just fine.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是利用集群内的工作者来并行评估超参数。在这种范式下，我们的时间序列数据集需要从工作者收集（完全物化）到驱动器。存在限制（在撰写本文时，数据的序列化大小限制为2
    GB），并且对于许多Spark上的ML用例，这种方法不应使用。对于此类时间序列问题，这种方法将工作得很好。
- en: In the second approach, we leave the data on the workers. We utilize `pandas_udf`
    to distribute concurrent training of each airport on each worker by using our
    standalone Hyperopt `Trials()` object, just as we did in chapter 6 when running
    on a single-core VM.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，我们将数据留在工作者上。我们使用`pandas_udf`通过使用我们独立的Hyperopt `Trials()`对象来分发每个工作者上每个机场的并发训练，就像我们在第6章中在单核VM上运行时做的那样。
- en: Now that we’ve defined the two paradigms for speeding up hyperparameter tuning
    from a high-level architectural perspective, let’s look at the process execution
    (and trade-offs of each) in the next two subsections.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从高层架构的角度定义了两种加速超参数调整的范式，让我们看看下一两个小节中的过程执行（以及每个的权衡）。
- en: 7.2.2 Handling tuning from the driver with SparkTrials
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 使用SparkTrials从驱动器处理调整
- en: While figure 7.10 shows the physical layout of the operations occurring within
    a Spark cluster for handling distributed tuning with `SparkTrials()`, figure 7.11
    shows the execution in more detail. Each airport that needs to be modeled is iterated
    over on the driver, its optimization handled through a distributed implementation
    wherein each candidate hyperparameter collection is submitted to a different worker.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然7.10图显示了Spark集群中处理分布式调整的`SparkTrials()`操作的物理布局，但7.11图显示了更详细的执行。每个需要建模的机场在驱动器上迭代，其优化通过分布式实现来处理，其中每个候选超参数集合都提交给不同的工作者。
- en: '![07-11](../Images/07-11.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![07-11](../Images/07-11.png)'
- en: Figure 7.11 Logical architecture of utilizing Spark workers to distribute Hyperopt
    test iterations for hyperparameter optimization
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：利用Spark工作者分布Hyperopt测试迭代以进行超参数优化的逻辑架构
- en: This approach works remarkably well with a minimal amount of modification to
    achieve a similar level of hyperparameter space searching as compared to the single-core
    approach, needing only a small increase to the number of iterations as the level
    of parallelism is increased.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通过最小的修改就能非常有效地工作，与单核方法相比，它只需要在并行级别增加时稍微增加迭代次数。
- en: NOTE Increasing the number of iterations as a factor of the parallelism level
    is not advisable. In practice, I generally increase the iterations by a simple
    adjustment of the number of single-core iterations + (parallelism factor / 0.2).
    This is to give a larger pool of prior values to pull from. With parallel runs
    executing asynchronously, each boundary epoch that is initiated will not have
    the benefit of in-flight results that a synchronous execution would.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：将迭代次数作为并行级别的因子增加是不推荐的。在实践中，我通常通过简单地调整单核迭代次数 + (并行因子 / 0.2) 来增加迭代次数。这是为了提供一个更大的先前值池来抽取。由于并行运行异步执行，每个启动的边界纪元都不会有同步执行会有的在飞行结果的好处。
- en: This is so critical to do because of the nature of the optimizer in Hyperopt.
    Being a Bayesian estimator, the power of its ability to arrive at an optimized
    set of parameters to test lies directly in its access to prior data. If too many
    runs are executing concurrently, the lack of data on their results translates
    to a higher rate of searching through parameters that have a lower probability
    to work well. Without the prior results, the optimization becomes much more of
    a random search, defeating the purpose of using the Bayesian optimizer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于Hyperopt优化器的本质来说至关重要。作为一个贝叶斯估计器，其能力到达一组经过测试的优化参数的强大之处直接在于其对先前数据的访问。如果同时执行太多的运行，那么它们结果数据的缺乏转化为搜索那些不太可能有效工作的参数的更高频率。没有先前结果，优化就变成了更多随机的搜索，违背了使用贝叶斯优化器的目的。
- en: This trade-off is negligible, though, particularly when compared to the rather
    impressive performance achievable by utilizing *n* workers to distribute each
    iteration to. To port our functions over to Spark, only a few changes need to
    happen for this first paradigm.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种权衡是可以忽略不计的，尤其是与利用 *n* 个工作者将每个迭代分布开来的相当令人印象深刻的性能相比。要将我们的函数移植到 Spark，只需要对这个第一个范式进行一些修改。
- en: NOTE To follow along fully with a referenceable and executable example of distributed
    hyperparameter optimization with Apache Spark, please see the companion Spark
    notebook in the book’s repository entitled Chapter8_1, which we will be using
    throughout the next chapter as well.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要完全跟随 Apache Spark 分布式超参数优化的可参考和可执行示例，请参阅本书仓库中名为 Chapter8_1 的配套 Spark 笔记本，我们将在下一章中继续使用它。
- en: The first thing that we’ll need to do is to import the module `SparkTrials`
    from Hyperopt. `SparkTrials` is a tracking object that allows for the cluster’s
    driver to maintain a history of all the experiments that have been attempted with
    different hyperparameter configurations executed on the remote workers (as opposed
    to the standard `Trials` object that tracks the history of runs conducted on the
    same VM).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要做的是从 Hyperopt 中导入模块 `SparkTrials`。`SparkTrials` 是一个跟踪对象，允许集群的驱动器维护所有尝试过的不同超参数配置的历史记录，这些配置是在远程工作者上执行的（与跟踪在相同
    VM 上运行的运行历史的标准 `Trials` 对象相反）。
- en: Once we have the import completed, we can read in our data by using a native
    Spark reader (in this instance, our data has been stored in a Delta table and
    registered to the Apache Hive Metastore, making it available through the standard
    database and table name identifiers). Once we have the data loaded onto the workers,
    we can then collect the series data to the driver, as shown in the following listing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 完成导入后，我们可以通过使用本机 Spark 读取器（在这个例子中，我们的数据已经存储在 Delta 表中并注册到 Apache Hive 元存储中，使其可以通过标准数据库和表名标识符访问）。一旦我们将数据加载到工作者上，我们就可以将序列数据收集到驱动器，如下所示。
- en: Listing 7.7 Using Spark to collect data to the driver as a pandas DataFrame
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 使用 Spark 将数据收集到驱动器作为 pandas DataFrame
- en: '[PRE6]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Defines the name of the Delta table that we’ve written the airport data to
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义我们写入机场数据的 Delta 表的名称
- en: ❷ Defines the name of the Hive database that the Delta table is registered to
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义 Delta 表注册到的 Hive 数据库名
- en: ❸ Interpolates the database name and the table name into standard API signature
    for data retrieval
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据库名和表名插入到标准 API 签名中用于数据检索
- en: ❹ Reads in the data with the workers from Delta (there is no ability to directly
    read in data to the driver from Delta), and then collects the data to the driver
    node as a pandas DataFrame
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 Delta 中的工作者读取数据（从 Delta 中直接读取数据到驱动器没有能力），然后将数据收集到驱动节点作为一个 pandas DataFrame
- en: WARNING Be careful about collecting data in Spark. With the vast majority of
    large-scale ML (with a training dataset that could be in the tens or hundreds
    of gigabytes), a `.toPandas()` call, or any collect action at all, in Spark will
    fail. If you have a large collection of data that can be iterated through, simply
    filter the Spark `DataFrame` and use an iterator (loop) to collect chunks of the
    data with a `.toPandas()` method call to control the amount of data being processed
    on the driver at a time.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：在 Spark 中收集数据时要小心。对于大多数大规模机器学习（训练数据集可能达到数十或数百吉字节），Spark 中的 `.toPandas()`
    调用或任何收集操作都会失败。如果您有一大批可以迭代的重复数据，只需过滤 Spark `DataFrame` 并使用迭代器（循环）通过 `.toPandas()`
    方法调用收集数据块，以控制每次在驱动器上处理的数据量。
- en: After running the preceding code, we are left with our data residing on the
    driver, ready for utilizing the distributed nature of the Spark cluster to conduct
    a far more scalable tuning of the models than what we were dealing with in our
    Docker container VM from section 7.1\. The following listing shows the modifications
    to listing 7.6 that allow us to run in this manner.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们留下数据驻留在驱动器上，准备利用 Spark 集群的分布式特性，以比我们在 7.1 节的 Docker 容器 VM 中处理的可扩展性更高的方式调整模型。以下列表显示了修改列表
    7.6 的内容，使我们能够以这种方式运行。
- en: Listing 7.8 Modifying the tuning execution function for running Hyperopt on
    Spark
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 修改用于在 Spark 上运行 Hyperopt 的调整执行函数
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Configures Hyperopt to use SparkTrials() instead of Trials(), setting the
    number of concurrent experiments to run on the workers in the cluster and the
    global time-out level (since we’re using Futures to submit the tests)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置Hyperopt使用SparkTrials()而不是Trials()，设置在集群工作节点上运行的并发实验数量和全局超时级别（因为我们使用Futures提交测试）
- en: ❷ Configures MLflow to log the results of each hyperparameter test within a
    parent run for each airport
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 配置MLflow记录每个机场父运行中每个超参数测试的结果
- en: ❸ Logs the airport name to MLflow to make it easier to search through the results
    of the tracking service
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将机场名称记录到MLflow中，以便更容易地搜索跟踪服务的输出结果
- en: ❹ The minimization function remains largely unchanged with the exception of
    adding in MLflow logging of both the hyperparameters and the calculated loss metrics
    that are being tested for the iteration within the child run.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 最小化函数在添加MLflow记录超参数和正在测试的迭代中计算的损失指标方面基本保持不变。
- en: ❺ Logs the generated prediction plots for the best model to the parent MLflow
    run
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将最佳模型的预测图记录到父MLflow运行中
- en: ❻ Logs the Hyperopt report for the run, written to the parent MLflow run ID
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 记录运行的超参数报告，写入父MLflow运行ID
- en: 'Little modification needed to happen to the code to get it to work within the
    distributed framework of Spark. As a bonus (which we will discuss in more depth
    in section 7.3), we can also log information with ease to MLflow, solving one
    of our key needs for creating a maintainable project: provenance of tests for
    reference and comparison.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对代码进行的小修改就足以使其在Spark的分布式框架中工作。作为额外的好处（我们将在第7.3节中更深入地讨论），我们还可以轻松地将信息记录到MLflow中，解决我们创建可维护项目的关键需求之一：测试的来源，以便于参考和比较。
- en: Based on the side-by-side comparison of this methodology to that of the run
    conducted in our single-core VM, this approach meets the goals of timeliness that
    we were searching for. We’ve reduced the optimization phase of this forecasting
    effort from just over 3.5 hours to, on a relatively small four-node cluster, just
    under 30 minutes (using a higher Hyperopt iteration count of 600 and a parallelization
    parameter of 8 to attempt to achieve similar loss metric performance).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此方法与我们在单核虚拟机中进行的运行进行并列比较，这种方法满足了我们所寻找的时效性目标。我们已经将此预测努力的优化阶段从超过3.5小时减少到相对较小的四节点集群上的不到30分钟（使用更高的Hyperopt迭代计数600和并行化参数8，以尝试实现类似的损失指标性能）。
- en: In the next section, we will look at an approach that solves our scalability
    problem in a completely different way by parallelizing the per airport models
    instead of parallelizing the tuning.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一种完全不同的方法来解决我们的可扩展性问题，通过并行化每个机场的模型而不是并行化调整过程。
- en: 7.2.3 Handling tuning from the workers with a pandas_udf
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 使用pandas_udf处理工作节点上的调整
- en: With the previous section’s approach, we were able to dramatically reduce the
    execution time by leveraging Spark to distribute individual hyperparameter-tuning
    stages. However, we were still using a sequential loop for each airport. As the
    number of airports grows, the relationship between total job execution time and
    airport count is still going to increase linearly, no matter how many parallel
    operations we do within the Hyperopt tuning framework. Of course, this approach’s
    effectiveness has a limit, as raising Hyperopt’s concurrency level will essentially
    negate the benefits of running the TPE and turn our optimization into a random
    search.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节的方法，我们能够通过利用Spark来分布单个超参数调整阶段，从而显著减少执行时间。然而，我们仍然在每个机场使用顺序循环。随着机场数量的增加，总作业执行时间与机场数量之间的关系仍然会线性增加，无论我们在Hyperopt调整框架内进行多少并行操作。当然，这种方法的有效性有一个极限，因为提高Hyperopt的并发级别将基本上抵消运行TPE的好处，并将我们的优化变成随机搜索。
- en: Instead, we can parallelize the actual model phases themselves, effectively
    turning this runtime problem into a horizontally scaling problem (reducing the
    execution time of all airports’ modeling by adding more worker nodes to the cluster),
    rather than a vertically scaling problem (iterator-bound, which can improve runtime
    only by using faster hardware). Figure 7.12 illustrates this alternative architecture
    of tackling our many-model problem through the use of `pandas_udf` on Spark.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以并行化实际的模型阶段本身，有效地将此运行时问题转化为水平扩展问题（通过向集群添加更多工作节点来减少所有机场建模的执行时间），而不是垂直扩展问题（迭代受限，只能通过使用更快的硬件来提高运行时间）。图
    7.12 阐述了通过在 Spark 上使用 `pandas_udf` 解决我们的多模型问题的替代架构。
- en: '![07-12](../Images/07-12.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![07-12](../Images/07-12.png)'
- en: Figure 7.12 Using Spark to control a fleet of contained VMs to work on each
    forecast asynchronously
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 使用 Spark 控制一组 VM 以异步方式处理每个预测
- en: Here, we’re using Spark DataFrames—a distributed dataset based on resiliently
    distributed dataset (`rdd`) relations residing on different VMs—to control the
    grouping-by of our primary modeling key (in this case, our `Airport_Code` field).
    We then pass this aggregated state to a `pandas_udf` that will leverage Apache
    Arrow to serialize the aggregated data to workers as a pandas DataFrame. This
    creates a multitude of concurrent Python VMs that are all operating on their own
    airport’s data as if they were a single VM.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 Spark DataFrame——基于不同 VM 上驻留的弹性分布式数据集（`rdd`）关系的分布式数据集——来控制我们的主要建模键的分组（在本例中，我们的
    `Airport_Code` 字段）。然后我们将此聚合状态传递给一个 `pandas_udf`，它将利用 Apache Arrow 将聚合数据序列化为工作器作为
    pandas DataFrame。这创建了众多并发 Python VM，它们都在处理各自的机场数据，就像它们是一个单独的 VM 一样。
- en: 'A trade-off exists here, though. To make this approach work, we need to change
    some things with our code. Listing 7.9 shows the first of these changes: a movement
    of the MLflow logging logic to within our minimization function, the addition
    of logging arguments to our function arguments, and the generation of the forecast
    plots for each iteration from within the minimization function so that we can
    see them after the modeling phase is completed.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在权衡。为了使这种方法工作，我们需要对我们的代码进行一些更改。列表 7.9 显示了这些更改中的第一个：将 MLflow 记录逻辑移动到我们的最小化函数中，向我们的函数参数添加记录参数，并在最小化函数中生成每个迭代的预测图，以便在建模阶段完成后查看。
- en: Listing 7.9 Modifying the minimization function to support a distributed model
    approach
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 修改最小化函数以支持分布式模型方法
- en: '[PRE8]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Adds arguments to support MLflow logging
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加参数以支持 MLflow 记录
- en: ❷ Initializes each iteration to its own MLflow run with a unique name to prevent
    collisions
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化每个迭代到其自己的 MLflow 运行，具有唯一名称以防止冲突
- en: ❸ Adds searchable tags for the MLflow UI search functionality
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为 MLflow UI 搜索功能添加可搜索的标签
- en: ❹ Searchable tags for the collection of all models that have been built for
    a particular execution of the job
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为特定执行中构建的所有模型集合提供可搜索的标签
- en: ❺ Records the iteration number of Hyperopt
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 记录 Hyperopt 的迭代次数
- en: ❻ Records the hyperparameters for a particular iteration
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 记录特定迭代的超参数
- en: ❼ Logs the loss metrics for the iteration
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 记录迭代的损失指标
- en: ❽ Saves the image (in PNG format) generated from the plot_predictions function
    that builds the test vs. forecast data
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 保存从 plot_predictions 函数生成的图像（PNG 格式），该函数构建测试与预测数据
- en: Since we’re going to be executing a pseudo-local Hyperopt run from directly
    within the Spark workers, we need to create our training and evaluation logic
    directly within a new function that will consume the grouped data passed via Apache
    Arrow to the workers for processing as a pandas DataFrame. The next listing shows
    the creation of this user-defined function (`udf`).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在 Spark 工作器中直接执行伪本地 Hyperopt 运行，我们需要在新的函数中直接创建我们的训练和评估逻辑，该函数将消耗通过 Apache
    Arrow 传递给工作器作为 pandas DataFrame 处理的分组数据。下一个列表显示了此用户定义函数（`udf`）的创建。
- en: Listing 7.10 Creating the distributed model pandas_udf to build models concurrently
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 创建分布式模型 pandas_udf 以并发构建模型
- en: '[PRE9]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Since Spark is a strong-typed language, we need to provide expectations to
    the udf of what structure and data types pandas will be returning to the Spark
    DataFrame. This is accomplished by using a StructType object defining the field
    names and their types.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于 Spark 是强类型语言，我们需要向 udf 提供期望的结构和数据类型，即 pandas 将返回给 Spark DataFrame 的数据类型。这是通过使用定义字段名称及其类型的
    StructType 对象来实现的。
- en: ❷ Defines the type of the pandas_udf (here we are using a grouped map type that
    takes in a pandas DataFrame and returns a pandas DataFrame) through the decorator
    applied above the function
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过应用在函数上方的装饰器定义了pandas_udf的类型（在这里我们使用了一个分组映射类型，它接受一个pandas DataFrame并返回一个pandas
    DataFrame）
- en: ❸ We need to extract the airport name from the data itself since we can’t pass
    additional values into this function.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于我们不能将额外值传递到这个函数中，我们需要从数据本身提取机场名称。
- en: ❹ We need to define our search space from within the udf since we can’t pass
    it into the function.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们需要在udf内部定义我们的搜索空间，因为我们不能将其传递到函数中。
- en: ❺ Sets the run configuration for the search (within the udf, since we need to
    name the runs in MLflow by the airport name, which is defined only after the data
    is passed to a worker from within the udf)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置搜索的运行配置（在udf内部，因为我们需要在MLflow中按机场名称命名运行，而机场名称仅在数据传递给udf中的工作节点之后定义）
- en: ❻ The airport data manipulation of the pandas DataFrame is placed here since
    the index conditions and frequencies for the series data are not defined within
    the Spark DataFrame.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将pandas DataFrame的机场数据处理放在这里，因为系列数据的索引条件和频率没有在Spark DataFrame中定义。
- en: ❼ The only modification to the “run tuning” function is to remove the MLflow
    logging created for the driver-based distributed Hyperopt optimization and to
    return only the forecasted data instead of the dictionary containing the run metrics
    and data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对“运行调整”功能的唯一修改是移除为基于驱动器的分布式Hyperopt优化创建的MLflow日志，并仅返回预测数据，而不是包含运行指标和数据的字典。
- en: ❽ Returns the forecast pandas DataFrame (required so that this data can be “reassembled”
    into a collated Spark DataFrame when all the airports finish their asynchronous
    distributed tuning and forecast runs)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 返回预测的pandas DataFrame（这是必需的，以便在所有机场完成异步分布式调整和预测运行后，可以将这些数据“重新组装”成一个汇总的Spark
    DataFrame）
- en: With the creation of this `pandas_udf`, we can call the distributed modeling
    (using Hyperopt in its single-node `Trials()` mode).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建这个`pandas_udf`，我们可以调用分布式建模（使用Hyperopt的单节点`Trials()`模式）。
- en: Listing 7.11 Executing a fully distributed model-based asynchronous run of forecasting
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.11 执行基于模型的异步预测运行的全分布式运行
- en: '[PRE10]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ A modification of the airport filtering used in the single-node code, utilizing
    PySpark filtering to determine whether enough data is in a particular airport’s
    series to build and validate a forecasting model
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对单节点代码中使用的机场过滤进行了修改，利用PySpark过滤来确定特定机场的系列中是否有足够的数据来构建和验证预测模型
- en: ❷ Defines a unique name for the particular execution of a forecasting run (this
    sets the name of the MLflow experiment for the tracking API)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为特定的预测运行定义一个唯一名称（这设置了跟踪API的MLflow实验名称）
- en: ❸ Reads the data from Delta (raw historical passenger data for airports) into
    the workers on the cluster
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将Delta（机场的历史乘客原始数据）中的数据读取到集群上的工作节点
- en: ❹ Filters out insufficient data wherein a particular airport does not have enough
    data for modeling
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 过滤掉数据不足的情况，其中某个机场没有足够的数据进行建模
- en: ❺ Groups the Spark DataFrame and sends the aggregated data to the workers as
    pandas DataFrames for execution through the udf
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将Spark DataFrame分组并发送聚合数据到工作节点，作为pandas DataFrame通过udf执行
- en: ❻ Forces the execution (Spark is lazily evaluated)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 强制执行（Spark是惰性评估的）
- en: When we run this code, we can see a relatively flat relationship between the
    number of airport models being generated and the number of workers available for
    processing our optimization and forecasting runs. While the reality of modeling
    over 7,000 airports in the shortest amount of time (a Spark cluster with thousands
    of worker nodes) is more than a little ridiculous (the cost alone would be astronomical),
    we have a queue-able solution using this paradigm that can horizontally scale
    in a magnitude that any other solution cannot.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码时，我们可以看到正在生成的机场模型数量与可用于处理我们的优化和预测运行的工作节点数量之间存在相对平坦的关系。虽然在最短的时间内（具有数千个工作节点的Spark集群）对超过7,000个机场进行建模的现实可能有些荒谬（仅成本就天文数字），但我们使用这种范式有一个可排队解决方案，其横向扩展能力是任何其他解决方案都无法比拟的。
- en: Even though we wouldn’t be able to get an effective O(1) execution time because
    of cost and resources (that would require one worker for each model), we can start
    a cluster with 40 nodes that would, in effect, run 40 airport modeling, optimizing,
    and forecasting executions concurrently. This would dramatically reduce the total
    runtime to 23 hours for all 7,000 airports, as opposed to either running them
    in a VM through a sequential loop-within-a-loop (> 5,000 hours), or collecting
    the data to the driver of a Spark cluster and running distributed tuning (> 800
    hours).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于成本和资源限制（每个模型需要一个工作者），我们无法获得有效的O(1)执行时间，但我们可以从一个包含40个节点的集群开始，实际上可以并行运行40个机场建模、优化和预测执行。这将将所有7,000个机场的总运行时间显著减少到23小时，而不是通过在虚拟机中通过嵌套循环（>
    5,000小时）运行它们，或者收集数据到Spark集群的驱动器上并运行分布式调优（> 800小时）。
- en: When finding options for tackling large-scale projects of this nature, the scalability
    of the execution architecture is just as critical as any of the ML components.
    Regardless of how much effort, time, and diligence went into crafting the ML aspect
    of the solution, if solving the problem takes thousands (or hundreds) of hours,
    the chances that the project will succeed are slim. In the next chapter, section
    8.2, we will discuss alternative approaches that can reduce the already dramatically
    improved 23 hours of runtime down to something even more manageable.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当寻找处理此类大规模项目的选项时，执行架构的可扩展性与其他机器学习组件一样关键。无论在构建解决方案的机器学习方面投入了多少努力、时间和勤奋，如果解决问题需要数千（或数百）小时，项目成功的可能性都很小。在下一章，第8.2节中，我们将讨论可以进一步减少已经大幅改善的23小时运行时间的替代方法。
- en: '7.2.4 Using new paradigms for teams: Platforms and technologies'
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 使用团队的新范式：平台和技术
- en: Starting on a new platform, utilizing a new technology, and perhaps learning
    a new programming language (or paradigm within a language you already know) is
    a daunting task for many teams. In the preceding scenarios, it was a relatively
    large leap to move from a Jupyter notebook running on a single machine to a distributed
    execution engine like Spark.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个新的平台开始，利用新的技术，也许学习一种新的编程语言（或者在你已知的语言中的新范式）对于许多团队来说是一项艰巨的任务。在前面的场景中，从运行在单台机器上的Jupyter笔记本迁移到分布式执行引擎Spark是一个相对较大的飞跃。
- en: The world of ML provides a great many options—not only in algorithms, but also
    in programming languages (R, Python, Java, Scala, .NET, proprietary languages)
    and places to develop code (notebooks for prototyping, scripting tools for MVPs,
    and IDEs for production solution development). Most of all, a great many places
    are available to run the code that you’ve written. As we saw earlier, it wasn’t
    the language that caused the runtime of the project to drop so dramatically, but
    rather the platform that we chose to use.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的世界提供了许多选择——不仅包括算法，还包括编程语言（R、Python、Java、Scala、.NET、专有语言）以及开发代码的地方（用于原型设计的笔记本、用于MVP的脚本工具和用于生产解决方案开发的IDE）。最重要的是，有许多地方可以运行你编写的代码。正如我们之前看到的，导致项目运行时间大幅下降的不是语言，而是我们选择使用的平台。
- en: When exploring options for project work, it is absolutely critical to do your
    homework. It is critical to test different algorithm approaches to solve a particular
    problem, and it is arguably more critical to find a place to run the solutions
    that fits within the needs of that project.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索项目工作的选项时，做足准备工作是绝对关键的。测试不同的算法方法来解决特定问题至关重要，而找到适合该项目需求的地方来运行解决方案可能更为关键。
- en: 'To maximize the chances of a solution being adopted by the business, the right
    platform should be chosen to minimize execution cost, maximize the stability of
    the solution, and shorten the development cycle to meet delivery deadlines. The
    important point to keep in mind about where to run ML code is that it is like
    any other aspect of this profession: time spent learning the framework used to
    run your models and analyses will be well spent, enhancing your productivity and
    efficiency for future work. Without knowing how to actually use a particular platform
    or execution paradigm, as mentioned in section 7.2.3, this project could have
    been looking at hundreds of hours of runtime for each forecasting event initiated.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化解决方案被业务采纳的机会，应选择合适的平台以最小化执行成本，最大化解决方案的稳定性，并缩短开发周期以满足交付期限。关于在哪里运行机器学习代码的重要观点是，它就像这个职业的任何其他方面一样：花时间学习用于运行你的模型和分析的框架将是值得的，这将提高你未来工作的生产力和效率。正如7.2.3节中提到的，如果不了解如何实际使用特定的平台或执行范式，该项目可能需要为每个启动的预测事件花费数百小时的运行时间。
- en: A bit of advice on learning new things
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 关于学习新事物的建议
- en: Early in my DS career, I was a bit intimidated and reluctant to learn languages
    other than Python. I mistakenly thought that my language of choice could “do all
    the things” and that I had no need for any other language, because the algorithms
    I used were all there (as far as I was aware at the time) and I was familiar with
    the nuances of manipulating data in pandas and NumPy. I was sorely mistaken when
    I had to build my first extremely large-scale ML solution involving a prediction-delivery
    SLA that was simply too short to allow for looped inference processing of terabytes
    of data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的数据科学职业生涯早期，我对学习除Python之外的语言感到有些畏惧和犹豫。我错误地认为我的选择语言可以“做所有的事情”，并且我没有必要学习任何其他语言，因为当时我所使用的算法都在那里（据我所知），并且我对使用pandas和NumPy操作数据的细微差别很熟悉。当我不得不构建第一个涉及预测交付服务级别协议（SLA）的极其大规模的机器学习解决方案时，我深感错误，因为该SLA的时间太短，无法允许对兆字节级数据进行循环推理处理。
- en: Over the years following my exposure to Hadoop, I’ve become proficient in Java
    and Scala, used both to build custom algorithms and frameworks for ML use cases,
    and expanded my knowledge of concurrent asynchronous programming to allow me to
    leverage as much computational power in solutions as is available to me. My advice?
    Make learning new technologies part of a regular habit.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在接触Hadoop后的几年里，我精通了Java和Scala，两者都用于构建机器学习用例的定制算法和框架，并扩展了我的并发异步编程知识，使我能够利用尽可能多的计算能力。我的建议？将学习新技术变成一种常规习惯。
- en: DS and ML work is not about a single language, a single platform, or anything
    that is set in stone. It is a mutable profession of discovery, focused on solving
    problems in whatever is the best manner to solve them in. Learning new ways to
    solve problems will only benefit you and whatever company you work for, and may
    one day help you contribute back to the community with the knowledge that you’ve
    gained along your journey.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和机器学习工作不是关于单一语言、单一平台或任何固定不变的东西。它是一个可变的发现职业，专注于以最佳方式解决问题。学习新的解决问题方法将只会对你和你工作的公司有益，并且有一天可能会帮助你利用在旅程中获得的知识回馈社区。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Relying on manual and prescriptive approaches for model tuning is time-consuming,
    expensive, and unlikely to produce quality results. Utilizing model-driven parameter
    optimization is preferred.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于手动和规定性的方法进行模型调优既耗时又昂贵，而且不太可能产生高质量的结果。利用模型驱动的参数优化是首选。
- en: Selecting an appropriate platform and implementation methodology for time-consuming
    CPU-bound tasks can dramatically increase the efficiency and lower the cost of
    development for an ML project. For processes like hyperparameter tuning, maximizing
    parallel and distributed system approaches can reduce the development timeline
    significantly.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的平台和实现方法对于耗时的CPU密集型任务可以显著提高机器学习项目的效率并降低开发成本。对于超参数调优等过程，最大化并行和分布式系统方法可以显著缩短开发时间表。
