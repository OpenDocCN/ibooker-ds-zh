- en: Chapter 1\. Introduction to Spark and PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. Spark 和 PySpark 简介
- en: Spark is a powerful analytics engine for large-scale data processing that aims
    at speed, ease of use, and extensibility for big data applications. It’s a proven
    and widely adopted technology used by many companies that handle big data every
    day. Though Spark’s “native” language is Scala (most of Spark is developed in
    Scala), it also provides high-level APIs in Java, Python, and R.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个强大的大数据分析引擎，旨在实现速度、易用性和大数据应用的可扩展性。它是被许多每天处理大数据的公司广泛采用的验证技术。尽管 Spark
    的“原生”语言是 Scala（大部分 Spark 是用 Scala 开发的），它也提供了 Java、Python 和 R 的高级 API。
- en: 'In this book we’ll be using Python via PySpark, an API that exposes the Spark
    programming model to Python. With Python being the most accessible programming
    language and Spark’s powerful and expressive API, PySpark’s simplicity makes it
    the best choice for us. PySpark is an interface for Spark in the Python programming
    language that provides the following two important features:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用 PySpark，它是一个将 Spark 编程模型暴露给 Python 的 API。由于 Python 是最易于访问的编程语言，加上
    Spark 强大而表达力强的 API，PySpark 的简洁性使其成为我们的最佳选择。PySpark 是 Spark 在 Python 编程语言中的接口，提供以下两个重要功能：
- en: It allows us to write Spark applications using Python APIs.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许我们使用 Python API 编写 Spark 应用程序。
- en: It provides the *PySpark shell* for interactively analyzing data in a distributed
    environment.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了 *PySpark shell*，用于在分布式环境中交互式分析数据。
- en: The purpose of this chapter is to introduce PySpark as the main component of
    the Spark ecosystem and show you that it can be effectively used for big data
    tasks such as ETL operations, indexing billions of documents, ingesting millions
    of genomes, machine learning, graph data analysis, DNA data analysis, and much
    more. I’ll start by reviewing the Spark and PySpark architectures, and provide
    examples to show the expressive power of PySpark. I will present an overview of
    Spark’s core functions (transformations and actions) and concepts so that you
    are empowered to start using Spark and PySpark right away. Spark’s main data abstractions
    are resilient distributed datasets (RDDs), DataFrames, and Datasets. As you’ll
    see, you can represent your data (stored as Hadoop files, Amazon S3 objects, Linux
    files, collection data structures, relational database tables, and more) in any
    combinations of RDDs and DataFrames.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是介绍 PySpark 作为 Spark 生态系统的主要组件，并展示它可以有效用于大数据任务，如 ETL 操作、索引数十亿个文档、摄入数百万基因组、机器学习、图数据分析、DNA
    数据分析等。我将首先回顾 Spark 和 PySpark 的架构，并提供示例展示 PySpark 的表达能力。我将概述 Spark 的核心功能（转换和动作）和概念，以便您可以立即开始使用
    Spark 和 PySpark。Spark 的主要数据抽象包括弹性分布式数据集（RDDs）、DataFrame 和 Dataset。正如您将看到的，您可以在任何
    RDDs 和 DataFrame 的组合中表示您的数据（存储为 Hadoop 文件、Amazon S3 对象、Linux 文件、集合数据结构、关系数据库表等）。
- en: Once your data is represented as a Spark data abstraction, you can apply transformations
    on it and create new data abstractions until the data is in the final form that
    you’re looking for. Spark’s transformations (such as `map()` and `reduceByKey()`)
    can be used to convert your data from one form to another until you get your desired
    result. I will explain these data abstractions shortly, but first, let’s dig a
    little deeper into why Spark is the best choice for data analytics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的数据被表示为 Spark 数据抽象，您可以对其进行转换，并创建新的数据抽象，直到数据处于您所需的最终形式。Spark 的转换操作（如 `map()`
    和 `reduceByKey()`）可用于将数据从一种形式转换为另一种形式，直到您获得所需的结果。稍后我将简要解释这些数据抽象，但首先，让我们深入探讨一下为什么
    Spark 是进行数据分析的最佳选择。
- en: Why Spark for Data Analytics
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 Spark 进行数据分析
- en: 'Spark is a powerful analytics engine that can be used for large-scale data
    processing. The most important reasons for using Spark are:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个强大的分析引擎，可用于大规模数据处理。使用 Spark 的最重要原因包括：
- en: Spark is simple, powerful, and fast.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 简单、强大且快速。
- en: Spark is free and open source.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 是自由开源的。
- en: Spark runs everywhere (Hadoop, Mesos, Kubernetes, standalone, or in the cloud).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 可在各处运行（Hadoop、Mesos、Kubernetes、独立模式或云中）。
- en: Spark can read/write data from/to any data source (Amazon S3, Hadoop HDFS, relational
    databases, etc.).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 可读取/写入来自/到任何数据源的数据（Amazon S3、Hadoop HDFS、关系数据库等）。
- en: Spark can be integrated with almost any data application.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 可以与几乎任何数据应用集成。
- en: Spark can read/write data in row-based (such as Avro) and column-based (such
    as Parquet and ORC) formats.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 可以读取/写入行格式（例如 Avro）和列格式（例如 Parquet 和 ORC）的数据。
- en: Spark has a rich but simple set of APIs for all kinds of ETL processes.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark提供了丰富而简单的API，用于各种ETL过程。
- en: In the past five years Spark has progressed in such a way that I believe it
    can be used to solve any big data problem. This is supported by the fact that
    all big data companies, such as Facebook, Illumina, IBM, and Google, use Spark
    every day in production systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去五年中，Spark已经发展到我认为可以用来解决任何大数据问题的程度。这一观点得到了支持，因为所有的大数据公司，如Facebook、Illumina、IBM和Google，每天都在生产系统中使用Spark。
- en: Spark is one of the best choices for large-scale data processing and for solving
    MapReduce problems and beyond, as it unlocks the power of data by handling big
    data with powerful APIs and speed. Using MapReduce/Hadoop to solve big data problems
    is complex, and you have to write a ton of low-level code to solve even primitive
    problems—this is where the power and simplicity of Spark comes in. Apache [Spark](http://spark.apache.org)
    is considerably faster than Apache [Hadoop](http://hadoop.apache.org) because
    it uses in-memory caching and optimized execution for fast performance, and it
    supports general batch processing, streaming analytics, machine learning, graph
    algorithms, and SQL queries.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是处理大规模数据和解决MapReduce问题及其他问题的最佳选择之一，因为它通过强大的API和速度处理大数据，释放数据的力量。使用MapReduce/Hadoop解决大数据问题很复杂，即使是解决原始问题，你也必须编写大量的低级代码
    — 这就是Spark的强大和简单之处。Apache [Spark](http://spark.apache.org)比Apache [Hadoop](http://hadoop.apache.org)快得多，因为它使用内存缓存和优化执行来提供快速性能，并支持通用批处理、流式分析、机器学习、图算法和SQL查询。
- en: 'For PySpark, Spark has two fundamental data abstractions: the RDD and the DataFrame.
    I will teach you how to read your data and represent it as an RDD (a set of elements
    of the same type) or a DataFrame (a table of rows with named columns); this allows
    you to impose a structure onto a distributed collection of data, permitting higher-level
    abstraction. Once your data is represented as an RDD or a DataFrame, you may apply
    transformation functions (such as mappers, filters, and reducers) on it to transform
    your data into the desired form. I’ll present many Spark transformations that
    you can use for ETL processes, analysis, and data-intensive computations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PySpark，Spark有两种基本的数据抽象：RDD和DataFrame。我将教你如何读取数据并将其表示为RDD（相同类型元素的集合）或DataFrame（带有命名列的行表），这使你可以在分布式数据集合上施加结构，实现更高级别的抽象。一旦数据被表示为RDD或DataFrame，你可以对其应用转换函数（如映射器、过滤器和减少器），将数据转换为所需的形式。我将呈现许多用于ETL过程、分析和数据密集型计算的Spark转换。
- en: Some simple RDD transformations are represented in [Figure 1-1](#simple_rdd_transformation).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些简单的RDD转换在[图1-1](#simple_rdd_transformation)中表示。
- en: '![daws 0101](Images/daws_0101.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0101](Images/daws_0101.png)'
- en: Figure 1-1\. Simple RDD transformations
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 简单的RDD转换
- en: 'This figure shows the following transformations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本图显示了以下转换：
- en: 'First we read our input data (represented as a text file, *sample.txt*—here,
    I only show the first two rows/records of input data) with an instance of `SparkSession`,
    which is the entry point to programming Spark. The `SparkSession` instance is
    represented as a `spark` object. Reading input creates a new RDD as an `RDD[String]`:
    each input record is converted to an RDD element of the type `String` (if your
    input path has *`N`* records, then the number of RDD elements is *`N`*). This
    is accomplished by the following code:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用`SparkSession`的实例读取我们的输入数据（表示为文本文件，*sample.txt* — 这里只显示了输入数据的前两行/记录）。`SparkSession`是编程Spark的入口点。`SparkSession`实例表示为`spark`对象。读取输入会创建一个新的RDD，类型为`RDD[String]`：每个输入记录被转换为`String`类型的RDD元素（如果您的输入路径有*N*条记录，则RDD元素的数量为*N*）。这是通过以下代码实现的：
- en: '[PRE0]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we convert all characters to lowercase letters. This is accomplished
    by the `map()` transformation, which is a 1-to-1 transformation:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将所有字符转换为小写字母。这通过`map()`转换来实现，它是一种1对1的转换：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we use a `flatMap()` transformation, which is a 1-to-many transformation,
    to convert each element (representing a single record) into a sequence of target
    elements (each representing a word). The `flatMap()` transformation returns a
    new RDD by first applying a function (here, `split(",")`) to all elements of the
    source RDD and then flattening the results:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`flatMap()`转换，这是一种1对多的转换，将每个元素（表示单个记录）转换为目标元素序列（每个单词表示一个元素）。`flatMap()`转换通过首先将函数（此处为`split(",")`）应用于源RDD的所有元素，然后展平结果，返回一个新的RDD：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we drop word elements with a length less than or equal to 2\. The
    following `filter()` transformation drops unwanted words, keeping only those with
    a length greater than 2:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们删除长度小于或等于2的单词元素。以下`filter()`转换将删除不需要的单词，仅保留长度大于2的单词：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can observe, Spark transformations are high-level, powerful, and simple.
    Spark is by nature distributed and parallel: your input data is partitioned and
    can be processed by transformations (such as mappers, filters, and reducers) in
    parallel in a cluster environment. In a nutshell, to solve a data analytics problem
    in PySpark, you read data and represent it as an RDD or DataFrame (depending on
    the nature of the data format), then write a set of transformations to convert
    your data into the desired output. Spark automatically partitions your DataFrames
    and RDDs and distributes the partitions across different cluster nodes. Partitions
    are the basic units of parallelism in Spark. Parallelism is what allows developers
    to perform tasks on hundreds of computer servers in a cluster in parallel and
    independently. A partition in Spark is a chunk (a logical division) of data stored
    on a node in the cluster. DataFrames and RDDs are collections of partitions. Spark
    has a default data partitioner for RDDs and DataFrames, but you may override that
    partitioning with your own custom programming.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所观察到的，Spark的转换是高级别的、强大的和简单的。Spark天生具有分布式和并行的特性：你的输入数据被分区，并且可以在集群环境中并行地被转换（如mappers、filters和reducers）。简而言之，要在PySpark中解决数据分析问题，你需要读取数据并将其表示为RDD或DataFrame（取决于数据格式的特性），然后编写一系列转换来将数据转换为所需的输出。Spark会自动分区你的DataFrame和RDD，并将这些分区分发到不同的集群节点上。分区是Spark中并行性的基本单位。并行性使开发者能够在集群中的数百台计算机服务器上并行且独立地执行任务。在Spark中，分区是存储在集群节点上的数据的逻辑分割（块）。DataFrame和RDD是分区的集合。Spark为RDD和DataFrame提供了默认的数据分区器，但你也可以使用自定义编程来覆盖该分区。
- en: Next, let’s dive a little deeper into Spark’s ecosystem and architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解一下Spark的生态系统和架构。
- en: The Spark Ecosystem
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark生态系统
- en: 'Spark’s ecosystem is presented in [Figure 1-2](#spark_ecosystem_source_databricks).
    It has three main components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的生态系统显示在[图1-2](#spark_ecosystem_source_databricks)中。它有三个主要组成部分：
- en: Environments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 环境
- en: Spark can run anywhere and integrates well with other environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在任何地方运行，并且与其他环境集成良好。
- en: Applications
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: Spark integrates well with a variety of big data platforms and applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark与各种大数据平台和应用程序集成良好。
- en: Data sources
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源
- en: Spark can read and write data from and to many data sources.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以从许多数据源读取和写入数据。
- en: '![daws 0102](Images/daws_0102.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0102](Images/daws_0102.png)'
- en: 'Figure 1-2\. The Spark ecosystem (source: Databricks)'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2。Spark生态系统（来源：Databricks）
- en: Spark’s expansive ecosystem makes PySpark a great tool for ETL, data analysis,
    and many other tasks. With PySpark, you can read data from many different data
    sources (the Linux filesystem, Amazon S3, the Hadoop Distributed File System,
    relational tables, MongoDB, Elasticsearch, Parquet files, etc.) and represent
    it as a Spark data abstraction, such as RDDs or DataFrames. Once your data is
    in that form, you can use a series of simple and powerful Spark transformations
    to transform the data into the desired shape and format. For example, you may
    use the `filter()` transformation to drop unwanted records, use `groupByKey()`
    to group your data by your desired key, and finally use the `mapValues()` transformation
    to perform final aggregation (such as finding average, median, and standard deviation
    of numbers) on the grouped data. All of these transformations are very possible
    by using the simple but powerful PySpark API.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Spark广泛的生态系统使得PySpark成为ETL、数据分析等多种任务的强大工具。使用PySpark，你可以从许多不同的数据源（如Linux文件系统、Amazon
    S3、Hadoop分布式文件系统、关系表、MongoDB、Elasticsearch、Parquet文件等）读取数据，并将其表示为Spark数据抽象，如RDD或DataFrame。一旦你的数据处于这种形式，你可以使用一系列简单而强大的Spark转换将数据转换为所需的形状和格式。例如，你可以使用`filter()`转换来删除不需要的记录，使用`groupByKey()`按你需要的键对数据进行分组，最后使用`mapValues()`转换来对分组数据进行最终聚合（如找到数字的平均值、中位数和标准偏差）。所有这些转换都可以通过简单但强大的PySpark
    API实现。
- en: Spark Architecture
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark架构
- en: When you have small data, it is possible to analyze it with a single computer
    in a reasonable amount of time. When you have large volumes of data, using a single
    computer to analyze and process that data (and store it) might be prohibitively
    slow, or even impossible. This is why we want to use Spark.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有少量数据时，可以在合理的时间内用单台计算机进行分析。当你有大量数据时，使用单台计算机进行数据分析、处理和存储可能会非常慢，甚至不可能。这就是为什么我们要使用
    Spark。
- en: Spark has a core library and a set of built-in libraries (SQL, GraphX, Streaming,
    MLlib), as shown in [Figure 1-3](#spark_libraries). As you can see, through its
    DataSource API, Spark can interact with many data sources, such as Hadoop, HBase,
    Amazon S3, Elasticsearch, and MySQL, to mention a few.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 包括核心库和一组内置库（SQL、GraphX、Streaming、MLlib），如[图 1-3](#spark_libraries)所示。可以通过其
    DataSource API 与多个数据源进行交互，如 Hadoop、HBase、Amazon S3、Elasticsearch 和 MySQL 等。
- en: '![daws 0103](Images/daws_0103.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0103](Images/daws_0103.png)'
- en: Figure 1-3\. Spark libraries
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. Spark 库
- en: 'This figure shows the real power of Spark: you can use several different languages
    to write your Spark applications, then use rich libraries to solve assorted big
    data problems. Meanwhile, you can read/write data from a variety of data sources.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此图展示了 Spark 的真正威力：你可以使用多种不同的语言编写你的 Spark 应用程序，然后使用丰富的库来解决各种大数据问题。同时，你可以从各种数据源读取/写入数据。
- en: Key Terms
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键术语
- en: 'To understand Spark’s architecture, you’ll need to understand a few key terms:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 Spark 的架构，你需要了解一些关键术语：
- en: '`SparkSession`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`'
- en: The `SparkSession` class, defined in the `pyspark.sql` package, is the entry
    point to programming Spark with the Dataset and DataFrame APIs. In order to do
    anything useful with a Spark cluster, you first need to create an instance of
    this class, which gives you access to an instance of `SparkContext`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 定义在 `pyspark.sql` 包中的 `SparkSession` 类是使用 Dataset 和 DataFrame API 编程 Spark 的入口点。要在
    Spark 集群上执行任何有用的操作，首先需要创建该类的实例，该实例提供了对 `SparkContext` 的访问。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[PySpark](https://oreil.ly/fSTe9) has a comprehensive API (comprised of packages,
    modules, classes, and methods) to access the Spark API. It is important to note
    that all Spark APIs, packages, modules, classes, and methods discussed in this
    book are PySpark-specific. For example, when I refer to the `SparkContext` class
    I am referring to the `pyspark.SparkContext` Python class, defined in the `pyspark`
    package, and when I refer to the `SparkSession` class, I am referring to the `pyspark.sql.SparkSession`
    Python class, defined in the `pyspark.sql` module.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[PySpark](https://oreil.ly/fSTe9) 提供了一个全面的 API（由包、模块、类和方法组成），用于访问 Spark API。需要注意的是，本书讨论的所有
    Spark API、包、模块、类和方法都是特定于 PySpark 的。例如，当提到 `SparkContext` 类时，指的是 `pyspark.SparkContext`
    Python 类，定义在 `pyspark` 包中；提到 `SparkSession` 类时，指的是 `pyspark.sql.SparkSession`
    Python 类，定义在 `pyspark.sql` 模块中。'
- en: '`SparkContext`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`'
- en: The `SparkContext` class, defined in the `pyspark` package, is the main entry
    point for Spark functionality. A `SparkContext` holds a connection to the Spark
    cluster manager and can be used to create RDDs and broadcast variables in the
    cluster. When you create an instance of `SparkSession`, the `SparkContext` becomes
    available inside your session as an attribute, `SparkSession.sparkContext`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 定义在 `pyspark` 包中的 `SparkContext` 类是 Spark 功能的主要入口点。`SparkContext` 保存与 Spark
    集群管理器的连接，并可用于在集群中创建 RDD 和广播变量。创建 `SparkSession` 实例后，`SparkContext` 作为 `SparkSession.sparkContext`
    属性在会话中可用。
- en: Driver
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序
- en: All Spark applications (including the PySpark shell and standalone Python programs)
    run as independent sets of processes. These processes are coordinated by a `SparkContext`
    in a driver program. To submit a standalone Python program to Spark, you need
    to write a driver program with the PySpark API (or Java or Scala). This program
    is in charge of the process of running the `main()` function of the application
    and creating the `SparkContext`. It can also be used to create RDDs and DataFrames.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Spark 应用程序（包括 PySpark shell 和独立的 Python 程序）都作为独立的一组进程运行。这些进程由驱动程序中的 `SparkContext`
    协调。要将独立的 Python 程序提交给 Spark，你需要使用 PySpark API（或 Java 或 Scala）编写驱动程序。该程序负责运行应用程序的
    `main()` 函数并创建 `SparkContext`，还可以用于创建 RDD 和 DataFrame。
- en: Worker
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Worker
- en: 'In a Spark cluster environment, there are two types of nodes: one (or two,
    for high availability) master and a set of workers. A worker is any node that
    can run programs in the cluster. If a process is launched for an application,
    then this application acquires executors at worker nodes, which are responsible
    for executing Spark tasks.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark集群环境中，存在两种类型的节点：一个（或两个，用于高可用性）主节点和一组工作节点。工作节点是可以在集群中运行程序的任何节点。如果为应用程序启动了一个进程，则此应用程序在工作节点获取执行器，这些执行器负责执行Spark任务。
- en: Cluster manager
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器
- en: 'The “master” node is known as the cluster manager. The main function of this
    node is to manage the cluster environment and the servers that Spark will leverage
    to execute tasks. The cluster manager allocates resources to each application.
    Spark supports five types of cluster managers, depending on where it’s running:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: “主”节点称为集群管理器。此节点的主要功能是管理集群环境和Spark将利用其执行任务的服务器。集群管理器为每个应用程序分配资源。Spark支持五种类型的集群管理器，具体取决于其运行位置：
- en: Standalone (Spark’s own built-in clustered environment)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 独立模式（Spark内置的集群环境）
- en: '[Mesos](http://mesos.apache.org) (a distributed systems kernel)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Mesos](http://mesos.apache.org)（一个分布式系统内核）'
- en: '[Hadoop YARN](https://oreil.ly/SICSG)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Hadoop YARN](https://oreil.ly/SICSG)'
- en: '[Kubernetes](https://kubernetes.io)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Kubernetes](https://kubernetes.io)'
- en: '[Amazon EC2](https://aws.amazon.com/ec2)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Amazon EC2](https://aws.amazon.com/ec2)'
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While use of master/worker terminology is outmoded and being retired in many
    software contexts, it is still part of the functionality of Apache Spark, which
    is why I use this terminology in this book.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在许多软件环境中，主/工作术语的使用已过时并正在被淘汰，但它仍然是Apache Spark功能的一部分，这也是为什么我在本书中使用这些术语的原因。
- en: Spark architecture in a nutshell
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark架构概述
- en: A high-level view of the Spark architecture is presented in [Figure 1-4](#spark_architecture).
    Informally, a Spark cluster is comprised of a master node (the “cluster manager”),
    which is responsible for managing Spark applications, and a set of “worker” (executor)
    nodes, which are responsible for executing tasks submitted by the Spark applications
    (your applications, which you want to run on the Spark cluster).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark架构的高级视图显示在[图1-4](#spark_architecture)中。简而言之，一个Spark集群由一个主节点（“集群管理器”）组成，负责管理Spark应用程序，以及一组“工作”（执行器）节点，负责执行Spark应用程序提交的任务（您希望在Spark集群上运行的应用程序）。
- en: '![daws 0104](Images/daws_0104.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0104](Images/daws_0104.png)'
- en: Figure 1-4\. Spark architecture
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. Spark架构
- en: Depending on the environment Spark is running in, the cluster manager managing
    this cluster of servers will be either Spark’s standalone cluster manager, Kubernetes,
    Hadoop YARN, or Mesos. When the Spark cluster is running, you can submit Spark
    applications to the cluster manager, which will grant resources to your application
    so that you can complete your data analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark运行的环境不同，管理此服务器集群的集群管理器可能是Spark的独立集群管理器、Kubernetes、Hadoop YARN或Mesos。当Spark集群运行时，您可以向集群管理器提交Spark应用程序，后者将为您的应用程序分配资源，以便您完成数据分析。
- en: Your cluster may have one, tens, hundreds, or even thousands of worker nodes,
    depending on the needs of your business and your project requirements. You can
    run Spark on a standalone server such as a MacBook, Linux, or Windows PC, but
    typically for production environments Spark is run on cluster of Linux servers.
    To run a Spark program, you need to have access to a Spark cluster and have a
    driver program, which declares the transformations and actions on RDDs of data
    and submits such requests to the cluster manager. In this book, all driver programs
    will be in PySpark.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您的集群可能拥有一个、几十个、上百个，甚至上千个工作节点，具体取决于您业务和项目需求。您可以在独立服务器上（如MacBook、Linux或Windows
    PC）上运行Spark，但通常情况下，生产环境中Spark会在一组Linux服务器的集群上运行。要运行Spark程序，您需要访问一个Spark集群，并拥有一个驱动程序，该程序声明了对RDD数据的转换和操作，并将这些请求提交给集群管理器。在本书中，所有的驱动程序都将使用PySpark编写。
- en: 'When you start a PySpark shell (by executing `*<spark-installed-dir>*/bin/pyspark`),
    you automatically get two variables/objects defined:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动PySpark shell（通过执行`*<spark-installed-dir>*/bin/pyspark`）时，您将自动获得定义的两个变量/对象：
- en: '`spark`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark`'
- en: An instance of `SparkSession`, which is ideal for creating DataFrames
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`SparkSession`的实例，非常适合创建DataFrames
- en: '`sc`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc`'
- en: An instance of `SparkContext`, which is ideal for creating RDDs
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`SparkContext`的实例，非常适合创建RDDs
- en: 'If you write a self-contained PySpark application (a Python driver, which uses
    the PySpark API), then you have to explicitly create an instance of `SparkSession`
    yourself. A `SparkSession` can be used to:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您编写一个独立的PySpark应用程序（使用PySpark API的Python驱动程序），那么您必须显式地自行创建`SparkSession`的实例。`SparkSession`可用于：
- en: Create DataFrames
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DataFrames
- en: Register DataFrames as tables
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将DataFrames注册为表
- en: Execute SQL over tables and cache tables
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表上执行SQL并缓存表
- en: Read/write text, CSV, JSON, Parquet, and other file formats
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取/写入文本、CSV、JSON、Parquet和其他文件格式
- en: Read/write relational database tables
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取/写入关系型数据库表
- en: 'PySpark defines `SparkSession` as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark将`SparkSession`定义为：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To create a `SparkSession` in Python, use the builder pattern shown here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中创建`SparkSession`，请使用此处显示的构建器模式：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO1-1)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO1-1)'
- en: Imports the `SparkSession` class from the `pyspark.sql` module.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从`pyspark.sql`模块导入`SparkSession`类。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO1-2)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO1-2)'
- en: Provides access to the Builder API used to construct `SparkSession` instances.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 提供用于构建`SparkSession`实例的Builder API的访问。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO1-3)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO1-3)'
- en: Sets a `config` option. Options set using this method are automatically propagated
    to both `SparkConf` and the `SparkSession`’s own configuration. When creating
    a `SparkSession` object, you can define any number of `config(*<key>*, *<value>*)`
    options.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个`config`选项。使用此方法设置的选项会自动传播到`SparkConf`和`SparkSession`自身的配置中。创建`SparkSession`对象时，可以定义任意数量的`config(*<key>*,
    *<value>*)`选项。
- en: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO1-4)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO1-4)'
- en: Gets an existing `SparkSession` or, if there isn’t one, creates a new one based
    on the options set here.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 获取现有的`SparkSession`或者如果没有，则根据此处设置的选项创建一个新的。
- en: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO1-5)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO1-5)'
- en: For debugging purposes only.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用于调试目的。
- en: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO1-6)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO1-6)'
- en: A `SparkContext` can be referenced from an instance of `SparkSession`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`可以从`SparkSession`的实例中引用。'
- en: 'PySpark defines `SparkContext` as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark将`SparkContext`定义为：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`SparkContext` is the main entry point for Spark functionality. A shell (such
    as the PySpark shell) or PySpark driver program cannot create more than one instance
    of `SparkContext`. A `SparkContext` represents the connection to a Spark cluster,
    and can be used to create new RDDs and broadcast variables (shared data structures
    and collections—kind of read-only global variables) on that cluster. [Figure 1-5](#creation_of_rdds_by_sparkcontext)
    shows how a `SparkContext` can be used to create a new RDD from an input text
    file (labeled `records_rdd`) and then transform it into another RDD (labeled `words_rdd`)
    using the `flatMap()` transformation. As you can observe, `RDD.flatMap(*f*)` returns
    a new RDD by first applying a function (*`f`*) to all elements of the source RDD,
    and then flattening the results.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`是Spark功能的主要入口点。Shell（例如PySpark shell）或PySpark驱动程序不能创建多个`SparkContext`实例。`SparkContext`表示与Spark集群的连接，并可用于在该集群上创建新的RDD和广播变量（共享数据结构和集合—一种只读全局变量）。[图 1-5](#creation_of_rdds_by_sparkcontext)显示了如何使用`SparkContext`从输入文本文件（标记为`records_rdd`）创建新的RDD，然后使用`flatMap()`转换将其转换为另一个RDD（标记为`words_rdd`）。正如您所见，`RDD.flatMap(*f*)`通过首先将函数（*`f`*）应用于源RDD的所有元素，然后展平结果，返回一个新的RDD。'
- en: '![daws 0105](Images/daws_0105.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0105](Images/daws_0105.png)'
- en: Figure 1-5\. Creation of RDDs by `SparkContext`
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 由`SparkContext`创建RDDs
- en: 'To create `SparkSession` and `SparkContext` objects, use the following pattern:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`SparkSession`和`SparkContext`对象，请使用以下模式：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you will be working only with RDDs, you can create an instance of `SparkContext`
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只会处理RDDs，可以按以下方式创建`SparkContext`的实例：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that you know the basics of Spark, let’s dive a little deeper into PySpark.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Spark的基础知识，让我们深入了解一下PySpark。
- en: The Power of PySpark
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark的强大之处
- en: PySpark is a Python API for Apache Spark, designed to support collaboration
    between Spark and the Python programming language. Most data scientists already
    know Python, and PySpark makes it easy for them to write short, concise code for
    distributed computing using Spark. In a nutshell, it’s an all-in-one ecosystem
    that can handle complex data requirements with its support for RDDs, DataFrames,
    GraphFrames, MLlib, SQL, and more.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark是Apache Spark的Python API，旨在支持Spark与Python编程语言之间的协作。大多数数据科学家已经了解Python，PySpark使他们能够使用Spark编写简短、简洁的分布式计算代码变得更加容易。简而言之，它是一个全能生态系统，可以通过支持RDDs、DataFrames、GraphFrames、MLlib、SQL等复杂数据要求来处理数据。
- en: 'I’ll show you the amazing power of PySpark with a simple example. Suppose we
    have lots of records containing data on URL visits by users (collected by a search
    engine from many web servers) in the following format:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向您展示PySpark的惊人功能，以一个简单的例子。假设我们有大量记录，这些记录包含用户访问URL的数据（由搜索引擎从许多Web服务器收集），格式如下：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here are a few examples of what these records look like:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这些记录的几个示例：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s assume we want to find the average, median, and standard deviation of
    the visit numbers per key (i.e., `url_address`). Another requirement is that we
    want to drop any records with a length less than 5 (as these may be malformed
    URLs). It is easy to express an elegant solution for this in PySpark, as [Figure 1-6](#simple_workflow_to_compute_mean_median_standard_deviation)
    illustrates.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要查找每个键（即`url_address`）的访问次数的平均值、中位数和标准偏差。另一个要求是，我们希望丢弃长度小于5的记录（因为这些可能是格式错误的URL）。可以轻松地在PySpark中表达一个优雅的解决方案，正如[图1-6](#simple_workflow_to_compute_mean_median_standard_deviation)所示。
- en: '![daws 0106](Images/daws_0106.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0106](Images/daws_0106.png)'
- en: Figure 1-6\. Simple workflow to compute mean, median, and standard deviation
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 计算平均值、中位数和标准偏差的简单工作流程
- en: 'First, let’s create some basic Python functions that will help us in solving
    our simple problem. The first function, `create_pair()`, accepts a single record
    of the form `<url_address><,><frequency>` and returns a (key, value) pair (which
    will enable us to do a `GROUP` `BY` on the key field later), where the key is
    a `url_address` and the value is the associated `frequency`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一些基本的Python函数，这些函数将帮助我们解决简单的问题。第一个函数`create_pair()`接受形式为`<url_address><,><frequency>`的单个记录，并返回一个（键，值）对（这将使我们能够稍后在键字段上执行`GROUP
    BY`），其中键是`url_address`，值是关联的`frequency`：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO2-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO2-1)'
- en: Accept a record of the form `<url_address><,><frequency>`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接受形式为`<url_address><,><frequency>`的记录。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO2-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO2-2)'
- en: Tokenize the input record, using the `url_address` as a key (`tokens[0]`) and
    the `frequency` as a value (`tokens[1]`).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`url_address`作为键（`tokens[0]`）和`frequency`作为值（`tokens[1]`）对输入记录进行标记化。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO2-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO2-3)'
- en: Return a pair of `(url_address, frequency)`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一对`（url_address，frequency）`。
- en: 'The next function, `compute_stats()`, accepts a list of frequencies (as numbers)
    and computes three values, the average, median, and standard deviation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数`compute_stats()`接受频率列表（作为数字）并计算三个值，平均值、中位数和标准偏差：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO3-1)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO3-1)'
- en: This module provides functions for calculating mathematical statistics of numeric
    data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块提供了用于计算数值数据的数学统计函数。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO3-2)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO3-2)'
- en: Accept a list of frequencies.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接受频率列表。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO3-3)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO3-3)'
- en: Compute the average of the frequencies.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 计算频率的平均值。
- en: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO3-4)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO3-4)'
- en: Compute the median of the frequencies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 计算频率的中位数。
- en: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO3-5)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO3-5)'
- en: Compute the standard deviation of the frequencies.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 计算频率的标准偏差。
- en: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO3-6)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO3-6)'
- en: Return the result as a triplet.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回三元组作为结果。
- en: 'Next, I’ll show you the amazing power of PySpark in just few lines of code,
    using Spark transformations and our custom Python functions:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将在几行代码中展示PySpark的强大功能，使用Spark转换和我们的自定义Python函数：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO4-1)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO4-1)'
- en: '`spark` denotes an instance of `SparkSession`, the entry point to programming
    Spark.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark` 表示 `SparkSession` 的一个实例，这是编程 Spark 的入口点。'
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO4-2)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO4-2)'
- en: '`sparkContext` (an attribute of `SparkSession`) is the main entry point for
    Spark functionality.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparkContext`（`SparkSession` 的一个属性）是 Spark 功能的主要入口点。'
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO4-3)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO4-3)'
- en: Read data as a distributed set of `String` records (creates an `RDD[String]`).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据作为分布式 `String` 记录集合读取（创建 `RDD[String]`）。
- en: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO4-4)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO4-4)'
- en: Drop records with a length less than or equal to 5 (keep records with a length
    greater than 5).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 删除长度小于或等于 5 的记录（保留长度大于 5 的记录）。
- en: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO4-5)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_introduction_to_spark_and_pyspark_CO4-5)'
- en: Create `(url_address, frequency)` pairs from the input records.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入记录中创建 `(url_address, frequency)` 对。
- en: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO4-6)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_introduction_to_spark_and_pyspark_CO4-6)'
- en: Group the data by keys—each key (a `url_address`) will be associated with a
    list of frequencies.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 按键分组数据——每个键（一个 `url_address`）将与一个频率列表关联。
- en: '[![7](Images/7.png)](#co_introduction_to_spark_and_pyspark_CO4-7)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_introduction_to_spark_and_pyspark_CO4-7)'
- en: Apply the `compute_stats()` function to the list of frequencies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `compute_stats()` 函数应用于频率列表。
- en: 'The result will be a set of (key, value) pairs of the form:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一组形如 `(key, value)` 的键值对：
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: where `url-address` is a key and `(average, median, standard_deviation)` is
    a value.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `url-address` 是键，`(average, median, standard_deviation)` 是值。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The most important thing about Spark is that it maximizes concurrency of functions
    and operations by means of partitioning data. Consider an example:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 最重要的特点是通过数据分区最大化函数和操作的并发性。考虑一个例子：
- en: If your input data has 600 billion rows and you are using a cluster of 10 nodes,
    your input data will be partitioned into *`N`* ( > 1) chunks, which are processed
    independently and in parallel. If `*N*=20,000` (the number of chunks or partitions),
    then each chunk will have about 30 million records/elements (600,000,000,000 /
    20,000 = 30,000,000). If you have a big cluster, then all 20,000 chunks might
    be processed in one shot. If you have a smaller cluster, it may be that only every
    100 chunks can be processed independently and in parallel. This process will continue
    until all 20,000 chunks are processed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的输入数据有 6000 亿行，并且您使用了 10 个节点的集群，则您的输入数据将被分成 *`N`* (> 1) 个块，这些块将独立并行处理。如果
    `*N*=20,000`（块或分区的数量），那么每个块大约会有 3 千万条记录/元素（600,000,000,000 / 20,000 = 30,000,000）。如果您有一个大型集群，那么这
    20,000 个块可能会一次性处理完毕。如果您有一个较小的集群，可能只有每隔 100 个块可以独立并行处理。这个过程会一直持续，直到所有 20,000 个块都处理完毕。
- en: PySpark Architecture
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark 架构
- en: PySpark is built on top of Spark’s Java API. Data is processed in Python and
    cached/shuffled in the Java Virtual Machine, or JVM (I will cover the concept
    of shuffling in [Chapter 2](ch02.xhtml#Chapter-02)). A high-level view of PySpark’s
    architecture is presented in [Figure 1-7](#pyspark_high_level_architecture).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是建立在 Spark 的 Java API 之上的。数据在 Python 中处理，并在 Java 虚拟机（JVM）中进行缓存/洗牌（我将在
    [第 2 章](ch02.xhtml#Chapter-02) 中讨论洗牌的概念）。PySpark 架构的高层视图如 [图 1-7](#pyspark_high_level_architecture)
    所示。
- en: '![daws 0107](Images/daws_0107.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0107](Images/daws_0107.png)'
- en: Figure 1-7\. PySpark architecture
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. PySpark 架构
- en: And PySpark’s data flow is illustrated in [Figure 1-8](#pyspark_data_flow).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的数据流程如 [图 1-8](#pyspark_data_flow) 所示。
- en: '![daws 0108](Images/daws_0108.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0108](Images/daws_0108.png)'
- en: Figure 1-8\. PySpark data flow
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8\. PySpark 数据流
- en: In the Python driver program (your Spark application in Python), the `SparkContext`
    uses [Py4J](https://www.py4j.org) to launch a JVM, creating a `JavaSparkContext`.
    Py4J is only used in the driver for local communication between the Python and
    Java `SparkContext` objects; large data transfers are performed through a different
    mechanism. RDD transformations in Python are mapped to transformations on `PythonRDD`
    objects in Java. On remote worker machines, `PythonRDD` objects launch Python
    subprocesses and communicate with them using pipes, sending the user’s code and
    the data to be processed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 驱动程序（即 Python 中的 Spark 应用程序）中，`SparkContext` 使用 [Py4J](https://www.py4j.org)
    来启动 JVM，创建 `JavaSparkContext`。Py4J 仅用于驱动程序，用于 Python 和 Java `SparkContext` 对象之间的本地通信；大数据传输通过其他机制进行。Python
    中的 RDD 转换映射到 Java 中的 `PythonRDD` 对象上。在远程工作节点上，`PythonRDD` 对象启动 Python 子进程，并通过管道与其通信，发送用户的代码和要处理的数据。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Py4J enables Python programs running in a Python interpreter to dynamically
    access Java objects in a JVM. Methods are called as if the Java objects resided
    in the Python interpreter, and Java collections can be accessed through standard
    Python collection methods. Py4J also enables Java programs to call back Python
    objects.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Py4J 允许运行在 Python 解释器中的 Python 程序动态访问 JVM 中的 Java 对象。方法调用就像 Java 对象驻留在 Python
    解释器中一样，并且可以通过标准 Python 集合方法访问 Java 集合。Py4J 还使 Java 程序能够回调 Python 对象。
- en: Spark Data Abstractions
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 数据抽象
- en: 'To manipulate data in the Python programming language, you use integers, strings,
    lists, and dictionaries. To manipulate and analyze data in Spark, you have to
    represent it as a Spark dataset. Spark supports three types of dataset abstractions:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 编程语言中操作数据时，您使用整数、字符串、列表和字典。在 Spark 中操作和分析数据时，您必须将其表示为 Spark 数据集。Spark
    支持三种类型的数据集抽象：
- en: 'RDD (resilient distributed dataset):'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD（弹性分布式数据集）：
- en: Low-level API
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低级 API
- en: Denoted by `RDD[T]` (each element has type `T`)
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 `RDD[T]` 表示（每个元素类型为 `T`）
- en: 'DataFrame (similar to relational tables):'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框（类似于关系表）：
- en: High-level API
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级 API
- en: Denoted by `Table(column_name_1, column_name_2, ...)`
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 `表(column_name_1, column_name_2, ...)` 表示
- en: 'Dataset (similar to relational tables):'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集（类似于关系表）：
- en: High-level API (not available in PySpark)
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级 API（在 PySpark 中不可用）
- en: The Dataset data abstraction is used in strongly typed languages such as Java
    and is not supported in PySpark. RDDs and DataFrames will be discussed in detail
    in the following chapters, but I’ll give a brief introduction here.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集数据抽象用于强类型语言，如 Java，并不支持 PySpark。RDD 和 DataFrame 将在后续章节详细讨论，但我会在这里简要介绍。
- en: RDD Examples
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 示例
- en: Essentially, an RDD represents your data as a collection of elements. It’s an
    immutable set of distributed elements of type `T`, denoted as `RDD[T]`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，RDD 将数据表示为元素的集合。它是类型 `T` 的不可变分布式元素集合，表示为 `RDD[T]`。
- en: '[Table 1-1](#simple_rdds) shows examples of three simple types of RDDs:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1-1](#simple_rdds) 展示了三种简单类型的 RDD 示例：'
- en: '`RDD[Integer]`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD[整数]`'
- en: Each element is an `Integer`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素都是一个 `整数`。
- en: '`RDD[String]`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD[字符串]`'
- en: Each element is a `String`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素都是一个 `字符串`。
- en: '`RDD[(String, Integer)]`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD[(字符串, 整数)]`'
- en: Each element is a pair of `(String, Integer)`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素都是一个 `(字符串, 整数)` 对。
- en: Table 1-1\. Simple RDDs
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. 简单的 RDDs
- en: '| RDD[Integer] | RDD[String] | RDD[(String, Integer)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| RDD[整数] | RDD[字符串] | RDD[(字符串, 整数)] |'
- en: '| --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `2` | `"abc"` | `(''A'', 4)` |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `"abc"` | `(''A'', 4)` |'
- en: '| `-730` | `"fox is red"` | `(''B'', 7)` |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `-730` | `"fox is red"` | `(''B'', 7)` |'
- en: '| `320` | `"Python is cool"` | `(''ZZ'', 9)` |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `320` | `"Python is cool"` | `(''ZZ'', 9)` |'
- en: '| … | … | … |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: '[Table 1-2](#rdds_of_different_element_types_complex_rdd) is an example of
    a complex RDD. Each element is a (key, value) pair, where the key is a `String`
    and the value is a triplet of `(Integer, Integer, Double)`.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1-2](#rdds_of_different_element_types_complex_rdd) 是复杂 RDD 的示例。每个元素都是一个
    (键, 值) 对，其中键是一个 `字符串`，值是一个三元组 `(整数, 整数, 浮点数)`。'
- en: Table 1-2\. Complex RDD
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-2\. 复杂的 RDD
- en: '| `RDD[(String, (Integer, Integer, Double))]` |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| `RDD[(字符串, (整数, 整数, 浮点数))]` |'
- en: '| --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| `("cat", (20, 40, 1.8))` |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `("cat", (20, 40, 1.8))` |'
- en: '| `("cat", (30, 10, 3.9))` |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `("cat", (30, 10, 3.9))` |'
- en: '| `("lion king", (27, 32, 4.5))` |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `("lion king", (27, 32, 4.5))` |'
- en: '| `("python is fun", (2, 3, 0.6))` |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `("python is fun", (2, 3, 0.6))` |'
- en: '| … |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: Spark RDD Operations
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD 操作
- en: 'Spark RDDs are read-only, immutable, and distributed. Once created, they cannot
    be altered: you cannot add records, delete records, or update records in an RDD.
    However, they can be transformed. RDDs support two types of operations: transformations,
    which transform the source RDD(s) into one or more new RDDs, and actions, which
    transform the source RDD(s) into a non-RDD object such as a dictionary or array.
    The relationship between RDDs, transformations, and actions is illustrated in
    [Figure 1-9](#rdds_transformations_and_actions).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDD 是只读的、不可变的和分布式的。一旦创建，它们就无法更改：不能向 RDD 添加记录、删除记录或更新记录。但是可以对其进行转换。RDD
    支持两种类型的操作：转换操作（将源 RDD（们）转换为一个或多个新 RDD）和动作操作（将源 RDD（们）转换为非 RDD 对象，如字典或数组）。RDD、转换和动作之间的关系如图
    [1-9](#rdds_transformations_and_actions) 所示。
- en: '![daws 0109](Images/daws_0109.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0109](Images/daws_0109.png)'
- en: Figure 1-9\. RDDs, transformations, and actions
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-9\. RDD、转换和动作
- en: We’ll go into much more detail on Spark’s transformations in the following chapters,
    with working examples to help you understand them, but I’ll provide a brief introduction
    here.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细讨论 Spark 的转换，并通过工作示例帮助您理解它们，但我将在此简要介绍。
- en: Transformations
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: 'A transformation in Spark is a function that takes an existing RDD (the source
    RDD), applies a transformation to it, and creates a new RDD (the target RDD).
    Examples include: `map()`, `flatMap()`, `groupByKey()`, `reduceByKey()`, and `filter()`.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'A transformation in Spark is a function that takes an existing RDD (the source
    RDD), applies a transformation to it, and creates a new RDD (the target RDD).
    Examples include: `map()`, `flatMap()`, `groupByKey()`, `reduceByKey()`, and `filter()`.'
- en: 'Informally, we can express a transformation as:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'Informally, we can express a transformation as:'
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO5-1)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO5-1)'
- en: Transform `source_RDD` of type `V` into `target_RDD` of type `T`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 将类型为 `V` 的 `source_RDD` 转换为类型为 `T` 的 `target_RDD`。
- en: 'RDDs are not evaluated until an action is performed on them: this means that
    transformations are lazily evaluated. If an RDD fails during a transformation,
    the data lineage of transformations rebuilds the RDD.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 在执行动作操作之前不会被评估：这意味着转换操作是延迟评估的。如果在转换过程中出现 RDD 失败，转换的数据血统会重建 RDD。
- en: Most Spark transformations create a single RDD, but it is also possible for
    them to create multiple target RDDs. The target RDD(s) can be smaller, larger,
    or the same size as the source RDD.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Spark 转换都会创建一个单独的 RDD，但也可能会创建多个目标 RDD。目标 RDD 可能比源 RDD 更小、更大或者大小相同。
- en: 'The following example presents a sequence of transformations:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子展示了一系列的转换：
- en: '[PRE18]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Tip
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `groupByKey()` transformation groups the values for each key in the RDD
    into a single sequence, similar to a SQL `GROUP` `BY` statement. This transformation
    can cause out of memory (OOM) errors as data is sent over the network of Spark
    servers and collected on the reducer/workers when the number of values per key
    is in the thousands or millions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey()` 转换会将 RDD 中每个键的值分组为一个单一的序列，类似于 SQL 的 `GROUP BY` 语句。当每个键的值数量达到数千或数百万时，这种转换可能会导致内存不足（OOM）错误，因为数据会通过
    Spark 服务器的网络发送，并在 reducer/workers 上收集。'
- en: With the `reduceByKey()` transformation, however, data is combined in each partition,
    so there is only one output for each key in each partition to send over the network
    of Spark servers. This makes it more scalable than `groupByKey()`. `reduceByKey()`
    merges the values for each key using an associative and commutative reduce function.
    It combines all the values (per key) into another value with the exact same data
    type (this is a limitation, which can be overcome by using the `combineByKey()`
    transformation). Overall, the `reduceByKey()` is more scaleable than the `groupByKey()`.
    We’ll talk more about these issues in [Chapter 4](ch04.xhtml#unique_chapter_id_04).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用 `reduceByKey()` 转换时，数据在每个分区中进行合并，因此每个键在每个分区中只有一个输出需要发送到 Spark 服务器的网络上。这使得它比
    `groupByKey()` 更具可扩展性。`reduceByKey()` 使用关联和可交换的 reduce 函数将每个键的值合并。它将所有值（每个键）合并为具有相同数据类型的另一个值（这是一个限制，可以通过使用
    `combineByKey()` 转换来克服）。总体而言，`reduceByKey()` 比 `groupByKey()` 更具可伸缩性。我们将在第 4 章更详细地讨论这些问题。
- en: Actions
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: 'Spark actions are RDD operations or functions that produce non-RDD values.
    Informally, we can express an action as:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'Spark actions are RDD operations or functions that produce non-RDD values.
    Informally, we can express an action as:'
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Actions may trigger the evaluation of RDDs (which, you’ll recall, are evaluated
    lazily). However, the output of an action is a tangible value: a saved file, a
    value such as an integer, a count of elements, a list of values, a dictionary,
    and so on.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 操作可能触发RDD的评估（你会记得，RDD是惰性评估的）。然而，操作的输出是一个具体的值：一个保存的文件，如整数值，元素数量，值列表，字典等。
- en: 'The following are examples of actions:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些操作示例：
- en: '`reduce()`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`'
- en: Applies a function to deliver a single value, such as adding values for a given
    `RDD[Integer]`
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 应用函数以生成单个值，例如为给定的`RDD[Integer]`添加值
- en: '`collect()`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()`'
- en: Converts an `RDD[T]` into a list of type `T`
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 将`RDD[T]`转换为类型为`T`的列表
- en: '`count()`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`'
- en: Finds the number of elements in a given RDD
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 查找给定RDD中元素的数量
- en: '`saveAsTextFile()`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsTextFile()`'
- en: Saves RDD elements to a disk
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将RDD元素保存到磁盘
- en: '`saveAsMap()`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsMap()`'
- en: Saves `RDD[(K, V)]` elements to a disk as a `dict[K, V]`
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 将`RDD[(K, V)]`元素保存到磁盘作为`dict[K, V]`
- en: DataFrame Examples
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame示例
- en: Similar to an RDD, a DataFrame in Spark is an immutable distributed collection
    of data. But unlike in an RDD, the data is organized into named columns, like
    a table in a relational database. This is meant to make processing of large datasets
    easier. DataFrames allow programmers to impose a structure onto a distributed
    collection of data, allowing higher-level abstraction. They also make the processing
    of CSV and JSON files much easier than with RDDs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与RDD类似，Spark中的DataFrame是一个不可变的分布式数据集合。但不同于RDD的是，数据被组织成了命名列，类似于关系数据库中的表。这旨在使大数据集的处理更加简单。DataFrames允许程序员对分布式数据集合施加结构，提供了更高级别的抽象。它们还比RDD更容易处理CSV和JSON文件。
- en: 'The following DataFrame example has three columns:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的DataFrame示例有三列：
- en: '[PRE20]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: A DataFrame can be created from many different sources, such as Hive tables,
    Structured Data Files (SDF), external databases, or existing RDDs. The DataFrames
    API was designed for modern big data and data science applications, taking inspiration
    from DataFrames in R and pandas in Python. As we will see in later chapters, we
    can execute SQL queries against DataFrames.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame可以从许多不同的源创建，如Hive表、结构化数据文件（SDF）、外部数据库或现有的RDD。DataFrames API专为现代大数据和数据科学应用程序设计，灵感来自R中的DataFrames和Python中的pandas。正如我们将在后面的章节中看到的，我们可以对DataFrames执行SQL查询。
- en: 'Spark SQL comes with an extensive set of powerful DataFrame operations that
    includes:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一套强大的DataFrame操作，包括：
- en: Aggregate functions (min, max, sum, average, etc.)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合函数（最小值、最大值、总和、平均值等）
- en: Collection functions
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合函数
- en: Math functions
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学函数
- en: Sorting functions
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序函数
- en: String functions
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串函数
- en: User-defined functions (UDFs)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户定义的函数（UDFs）
- en: 'For example, you can easily read a CSV file and create a DataFrame from it:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以轻松地读取CSV文件，并从中创建一个DataFrame：
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To sort the results by number of cases in descending order, we can use the
    `sort()` function:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要按病例数量降序排序结果，我们可以使用`sort()`函数：
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also easily filter rows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以轻松地过滤行：
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To give you a better idea of the power of Spark’s DataFrames, let’s walk through
    an example. We will create a DataFrame and find the average and sum of hours worked
    by employees per department:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你更好地理解Spark的DataFrames的强大，让我们通过一个例子来详细说明。我们将创建一个DataFrame，并查找每个部门员工工作小时数的平均值和总和：
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, Spark’s DataFrames are powerful enough to manipulate billions
    of rows with simple but powerful functions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，Spark的DataFrames足以使用简单但强大的函数操作数十亿行数据。
- en: Using the PySpark Shell
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark Shell
- en: 'There are two main ways you can use PySpark:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用PySpark的两种主要方式：
- en: Use the PySpark shell (for testing and interactive programming).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark shell（用于测试和交互式编程）。
- en: 'Use PySpark in a self-contained application. In this case, you write a Python
    driver program (say, *my_pyspark_program.py*) using the PySpark API and then run
    it with the `spark-submit` command:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在独立应用程序中使用PySpark。在这种情况下，你会使用PySpark API编写一个Python驱动程序（比如*my_pyspark_program.py*），然后使用`spark-submit`命令运行它：
- en: '[PRE25]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: where *`<parameters>`* is a list of parameters consumed by your PySpark (*my_pyspark_program.py*)
    program.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里*`<parameters>`*是你的PySpark程序（*my_pyspark_program.py*）消耗的参数列表。
- en: Note
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For details on using the `spark-submit` command, refer to [“Submitting Applications”](https://spark.apache.org/docs/latest/submitting-applications.html)
    in the Spark documentation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用`spark-submit`命令的详细信息，请参阅Spark文档中的[“提交应用程序”](https://spark.apache.org/docs/latest/submitting-applications.html)。
- en: 'In this section we’ll focus on Spark’s interactive shell for Python users,
    a powerful tool that you can use to analyze data interactively and see the results
    immediately (Spark also provides a Scala shell). The PySpark shell can work on
    both single-machine installations and cluster installations of Spark. You use
    the following command to start the shell, where `SPARK_HOME` denotes the installation
    directory for Spark on your system:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于Python用户的Spark交互式shell，这是一个强大的工具，可以用于交互式分析数据并立即查看结果（Spark还提供Scala
    shell）。PySpark shell可以在单机安装和集群安装的Spark上运行。您可以使用以下命令启动shell，其中`SPARK_HOME`表示您系统上Spark的安装目录：
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO6-1)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO6-1)'
- en: Define the Spark installation directory.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 定义Spark安装目录。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO6-2)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO6-2)'
- en: Invoke the PySpark shell.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 调用PySpark shell。
- en: When you start the shell, PySpark displays some useful information including
    details on the Python and Spark versions it is using (note that the output here
    has been shortened). The `>>>` symbol is used as the PySpark shell prompt. This
    prompt indicates that you can now write Python or PySpark commands and view the
    results.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动shell时，PySpark会显示一些有用的信息，包括正在使用的Python和Spark版本（请注意，此处的输出已经被缩短）。`>>>`符号用作PySpark
    shell提示符。此提示符表示您现在可以编写Python或PySpark命令并查看结果。
- en: To get you comfortable with the PySpark shell, the following sections will walk
    you through some basic usage examples.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您熟悉PySpark shell，接下来的几节将引导您完成一些基本使用示例。
- en: Launching the PySpark Shell
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动PySpark Shell。
- en: 'To enter into a PySpark shell, we execute `pyspark` as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入PySpark shell，我们执行以下命令：`pyspark`。
- en: '[PRE28]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO7-1)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO7-1)'
- en: Executing `pyspark` will create a new shell. The output here has been shortened.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`pyspark`将创建一个新的shell。这里的输出已经被缩短。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO7-2)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO7-2)'
- en: Verify that `SparkContext` is created as `sc`.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 确认`SparkContext`已创建为`sc`。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO7-3)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO7-3)'
- en: Verify that `SparkSession` is created as `spark`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 确认`SparkSession`已创建为`spark`。
- en: Once you enter into the PySpark shell, an instance of `SparkSession` is created
    as the `spark` variable and an instance of `SparkContext` is created as the `sc`
    variable. As you learned earlier in this chapter, the `SparkSession` is the entry
    point to programming Spark with the Dataset and DataFrame APIs; a `SparkSession`
    can be used to create DataFrames, register DataFrames as tables, execute SQL over
    tables, cache tables, and read CSV, JSON, and Parquet files. If you want to use
    PySpark in a self-contained application, then you have to explicitly create a
    `SparkSession` using the builder pattern shown in [“Spark architecture in a nutshell”](#spark_in_a_nutshell).
    A `SparkContext` is the main entry point for Spark functionality; it can be used
    to create RDDs from text files and Python collections. We’ll look at that next.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入PySpark shell，将创建`SparkSession`实例作为`spark`变量，并创建`SparkContext`实例作为`sc`变量。如您在本章早些时候学到的，`SparkSession`是使用Dataset和DataFrame
    API编程Spark的入口点；`SparkSession`可用于创建DataFrame，将DataFrame注册为表，对表执行SQL，缓存表，并读取CSV、JSON和Parquet文件。如果要在独立应用程序中使用PySpark，则必须使用生成器模式显式创建`SparkSession`，如["Spark
    architecture in a nutshell"](#spark_in_a_nutshell)所示。`SparkContext`是Spark功能的主要入口点；它可用于从文本文件和Python集合创建RDD。接下来我们会详细讨论这一点。
- en: Creating an RDD from a Collection
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从集合创建RDD。
- en: 'Spark enables us to create new RDDs from files and collections (data structures
    such as arrays and lists). Here, we use `SparkContext.parallelize()` to create
    a new RDD from a collection (represented as `data`):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使我们能够从文件和集合（如数组和列表等数据结构）创建新的RDD。在这里，我们使用`SparkContext.parallelize()`从集合（表示为`data`）创建一个新的RDD：
- en: '[PRE31]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO8-1)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO8-1)'
- en: Define your Python collection.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 定义您的Python集合。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO8-2)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO8-2)'
- en: Create a new RDD from a Python collection.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 从Python集合创建新的RDD。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO8-3)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO8-3)'
- en: Display the contents of the new RDD.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新RDD的内容。
- en: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO8-4)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO8-4)'
- en: Count the number of elements in the RDD.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 计算RDD中元素的数量。
- en: Aggregating and Merging Values of Keys
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合和合并键的值
- en: 'The `reduceByKey()` transformation is used to merge and aggregate values. In
    this example, `x` and `y` refer to the values of the same key:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey()`转换用于使用可结合和可交换的减少函数合并和聚合值。在此示例中，`x`和`y`指的是同一键的值：'
- en: '[PRE32]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO9-1)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO9-1)'
- en: Merge and aggregate values of the same key.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 合并和聚合相同键的值。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO9-2)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO9-2)'
- en: Collect the elements of the RDD.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 收集RDD的元素。
- en: The source RDD for this transformation must consist of (key, value) pairs. `reduceByKey()`
    merges the values for each key using an associative and commutative reduce function.
    This will also perform the merging locally on each mapper before sending the results
    to a reducer, similarly to a “combiner” in MapReduce. The output will be partitioned
    with `numPartitions` partitions, or the default parallelism level if `numPartitions`
    is not specified. The default partitioner is `HashPartitioner`.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 此转换的源RDD必须由（键，值）对组成。`reduceByKey()`使用可结合和可交换的减少函数为每个键合并值。这也会在每个mapper上本地执行合并，然后将结果发送到reducer，类似于MapReduce中的“combiner”。输出将使用`numPartitions`分区或未指定`numPartitions`时的默认并行级别进行分区。默认分区器是`HashPartitioner`。
- en: 'If `T` is the type of the value for (key, value) pairs, then `reduceByKey()`’s
    `func()` can be defined as:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`T`是（键，值）对值的类型，则可以将`reduceByKey()`的`func()`定义为：
- en: '[PRE33]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This means that:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: There are two input arguments (of the same type, `T`) for the reducer `func()`.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数`func()`有两个相同类型的输入参数`T`。
- en: The return type of `func()` must be the same as the input type `T` (this limitation
    can be avoided if you use the `combineByKey()` transformation).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`func()`的返回类型必须与输入类型`T`相同（如果使用`combineByKey()`转换，则可以避免此限制）。'
- en: The reducer `func()` has to be associative. Informally, a binary operation `f()`
    on a set `T` is called associative if it satisfies the associative law, which
    states that the order in which numbers are grouped does not change the result
    of the operation.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reducer `func()`必须是可结合的。非正式地说，集合`T`上的二元操作`f()`称为可结合的，如果它满足结合律，即分组数字的顺序不改变操作的结果。
- en: Associative Law
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合律
- en: '[PRE34]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that the associative law holds for addition (+) and multiplication (*),
    but not for subtraction (-) or division (/).
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，加法（+）和乘法（*）遵循结合律，但减法（-）或除法（/）不遵循。
- en: 'The reducer `func()` has to be commutative: informally, a function `f()` for
    which `f(x, y) = f(y, x)` for all values of `x` and `y`. That is, a change in
    the order of the numbers should not affect the result of the operation.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reducer `func()`必须是可交换的：非正式地说，对于所有值`x`和`y`，函数`f()`满足`f(x, y) = f(y, x)`。也就是说，数字的顺序变化不应影响操作的结果。
- en: Commutative Law
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交换律
- en: '[PRE35]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The commutative law also holds for addition and multiplication, but not for
    subtraction or division. For example:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加法和乘法也遵循交换律，但减法或除法不遵循。例如：
- en: 5 + 3 = 3 + 5 but 5 – 3 ≠ 3 – 5
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5 + 3 = 3 + 5，但是5 - 3 ≠ 3 - 5
- en: Therefore, you may not use subtraction or division operations in a `reduceByKey()`
    transformation.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在`reduceByKey()`转换中不得使用减法或除法操作。
- en: Filtering an RDD’s Elements
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤RDD的元素
- en: 'Next, we’ll use the `filter()` transformation to return a new RDD containing
    only the elements that satisfy a predicate:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`filter()`转换返回一个新的RDD，其中仅包含满足谓词的元素：
- en: '[PRE36]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO10-1)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO10-1)'
- en: Keep the (key, value) pairs if the value is greater than 9.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果值大于9，则保留（键，值）对。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO10-2)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO10-2)'
- en: Collect the elements of the RDD.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 收集RDD的元素。
- en: Grouping Similar Keys
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组相似键
- en: 'We can use the `groupByKey()` transformation to group the values for each key
    in the RDD into a single sequence:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`groupByKey()`转换将RDD中每个键的值分组为单个序列：
- en: '[PRE37]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO11-1)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO11-1)'
- en: Group elements of the same key into a sequence of elements.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同键的元素分组成元素序列。
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO11-2)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO11-2)'
- en: View the result.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果。
- en: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO11-3)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introduction_to_spark_and_pyspark_CO11-3)'
- en: The full name of `ResultIterable` is `pyspark.resultiterable.ResultIterable`.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultIterable` 的全名是 `pyspark.resultiterable.ResultIterable`。'
- en: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO11-4)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_introduction_to_spark_and_pyspark_CO11-4)'
- en: First apply `map()` and then `collect()`, which returns a list that contains
    all of the elements in the resulting RDD. The `list()` function converts `ResultIterable`
    into a list of objects.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 先应用 `map()` 然后是 `collect()`，它返回一个包含结果RDD中所有元素的列表。`list()` 函数将 `ResultIterable`
    转换为对象列表。
- en: The source RDD for this transformation must be composed of (key, value) pairs.
    `groupByKey()` groups the values for each key in the RDD into a single sequence,
    and hash-partitions the resulting RDD with `numPartitions` partitions, or with
    the default level of parallelism if `numPartitions` is not specified. Note that
    if you are grouping (using the `groupByKey()` transformation) in order to perform
    an aggregation, such as a sum or average, over each key, using `reduceByKey()`
    or `aggregateByKey()` will provide much better performance.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 此转换的源RDD必须由（键，值）对组成。`groupByKey()` 将RDD中每个键的值分组为单个序列，并使用 `numPartitions` 分区进行哈希分区生成结果RDD，或者如果未指定
    `numPartitions`，则使用默认并行度。请注意，如果您正在分组（使用 `groupByKey()` 转换）以执行诸如求和或平均值之类的聚合操作，使用
    `reduceByKey()` 或 `aggregateByKey()` 将提供更好的性能。
- en: Aggregating Values for Similar Keys
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合相似键的值
- en: 'To aggregate and sum up the values for each key, we can use the `mapValues()`
    transformation and the `sum()` function:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要对每个键的值进行聚合和求和，我们可以使用 `mapValues()` 转换和 `sum()` 函数：
- en: '[PRE38]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO12-1)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introduction_to_spark_and_pyspark_CO12-1)'
- en: '`values` is a sequence of values per key. We pass each value in the (key, value)
    pair RDD through a mapper function (adding all `values` with `sum(values)`) without
    changing the keys.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`values` 是每个键的值序列。我们通过映射函数将每个（键，值）对RDD中的所有值相加（使用 `sum(values)`），而不改变键。'
- en: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO12-2)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introduction_to_spark_and_pyspark_CO12-2)'
- en: For debugging, we return a list that contains all of the elements in this RDD.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调试，我们返回一个包含此RDD中所有元素的列表。
- en: 'We have several choices for aggregating and summing up values: `reduceByKey()`
    and `groupByKey()`, to mention a few. In general, the `reduceByKey()` transformation
    is more efficient than the `groupByKey()` transformation. More details on this
    are provided in [Chapter 4](ch04.xhtml#unique_chapter_id_04).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种选择来聚合和汇总值：`reduceByKey()` 和 `groupByKey()`，仅举几例。一般而言，`reduceByKey()` 转换比
    `groupByKey()` 转换更有效率。有关更多详细信息，请参阅[第4章](ch04.xhtml#unique_chapter_id_04)。
- en: As you’ll see in the following chapters, Spark has many other powerful transformations
    that can convert an RDD into a new RDD. As mentioned earlier, RDDs are read-only,
    immutable, and distributed. RDD transformations return a pointer to a new RDD
    and allow you to create dependencies between RDDs. Each RDD in the dependency
    chain (or string of dependencies) has a function for calculating its data and
    a pointer (dependency) to its parent RDD.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在接下来的章节中看到的，Spark 还有许多其他强大的转换功能，可以将一个RDD转换为新的RDD。正如前面提到的，RDD 是只读的、不可变的，并且是分布式的。RDD
    转换返回一个指向新RDD的指针，并允许您在RDD之间创建依赖关系。依赖链中的每个RDD（或依赖链的字符串）都有一个用于计算其数据的函数和指向其父RDD的指针（依赖）。
- en: Data Analysis Tools for PySpark
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark的数据分析工具
- en: '[Jupyter](http://jupyter.org)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jupyter](http://jupyter.org)'
- en: Jupyter is a great tool to test and prototype programs. PySpark can also be
    used from Jupyter notebooks; it’s very practical for explorative data analysis.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 是一个测试和原型化程序的好工具。PySpark 也可以从Jupyter笔记本中使用；它非常适合探索性数据分析。
- en: '[Apache Zeppelin](https://zeppelin.apache.org)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Zeppelin](https://zeppelin.apache.org)'
- en: Zeppelin is a web-based notebook that enables data-driven, interactive data
    analytics and collaborative documents with SQL, Python, Scala, and more.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin 是一个基于Web的笔记本，支持使用SQL、Python、Scala等进行数据驱动的交互式数据分析和协作文档。
- en: ETL Example with DataFrames
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据框架的ETL示例
- en: In data analysis and computing, ETL is the general procedure of copying data
    from one or more sources into a destination system that represents the data differently
    from the source(s) or in a different context than the source(s). Here I will show
    how Spark makes ETL possible and easy.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析和计算中，ETL 是从一个或多个源复制数据到一个目标系统的一般过程，该系统以与源数据不同的方式或在不同的上下文中表示数据。在这里我将展示Spark如何实现ETL并使其变得简单。
- en: 'For this ETL example, I’ll use 2010 census data in JSON format (*census_2010.json*):'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个 ETL 示例，我将使用 JSON 格式的 2010 年人口普查数据（*census_2010.json*）：
- en: '[PRE39]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This data was pulled from U.S. Census Bureau data, which at the time of writing
    this book only provides the binary options of male and female. We strive to be
    as inclusive as possible, and hope that in the future national data sets such
    as these will provide more inclusive options.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据是从美国人口调查局数据中提取的，该数据在编写本书时仅提供男性和女性的二元选项。我们努力尽可能包容，并希望在未来，诸如此类的国家数据集将提供更多包容性选项。
- en: 'Let’s define our ETL process:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的 ETL 过程：
- en: Extraction
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 提取
- en: First, we create a DataFrame from a given JSON document.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从给定的 JSON 文档创建一个 DataFrame。
- en: Transformation
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 转换
- en: Then we filter the data and keep the records for seniors (`age > 54`). Next,
    we add a new column, `total`, which is the total of males and females.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们过滤数据并保留老年人（`age > 54`）的记录。接下来，我们添加一个新列，`total`，即男性和女性的总数。
- en: Loading
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 加载
- en: Finally, we write the revised DataFrame into a MySQL database and verify the
    load process.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将修订后的 DataFrame 写入 MySQL 数据库并验证加载过程。
- en: Let’s dig into this process a little more deeply.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这个过程。
- en: Extraction
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取
- en: 'To do a proper extraction, we first need to create an instance of the `SparkSession`
    class:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行正确的提取，我们首先需要创建一个`SparkSession`类的实例：
- en: '[PRE40]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we read the JSON and create a DataFrame:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取 JSON 并创建一个 DataFrame：
- en: '[PRE41]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Transformation
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: 'Transformation can involve many processes whose purpose is to clean, format,
    or perform computations on the data to suit your requirements. For example, you
    can remove missing or duplicate data, join columns to create new columns, or filter
    out certain rows or columns. Once we’ve created the DataFrame through the extraction
    process, we can perform many useful transformations, such as selecting just the
    seniors:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 转换可能涉及许多过程，其目的是根据您的要求清理、格式化或对数据执行计算。例如，您可以删除缺失或重复的数据，连接列以创建新列，或过滤某些行或列。一旦通过提取过程创建了
    DataFrame，我们就可以执行许多有用的转换，比如仅选择老年人：
- en: '[PRE42]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we create a new aggregated column called `total`, which adds up the numbers
    of males and females:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为`total`的新聚合列，它将男性和女性的数量相加：
- en: '[PRE43]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Loading
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载
- en: 'The loading process involves saving or writing the final output of the transformation
    step. Here, we will write the `seniors_final` DataFrame into a MySQL table:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 加载过程涉及保存或写入转换步骤的最终输出。在这里，我们将`seniors_final` DataFrame 写入一个 MySQL 表中：
- en: '[PRE44]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The final step of loading is to verify the load process:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的最后一步是验证加载过程：
- en: '[PRE45]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Summary
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Let’s recap some key points from the chapter:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下本章的一些关键点：
- en: Spark is a fast and powerful unified analytics engine (up to one hundred times
    faster than traditional Hadoop MapReduce) due to its in-memory operation, and
    it offers robust, distributed, fault-tolerant data abstractions (called RDDs and
    DataFrames). Spark integrates with the world of machine learning and graph analytics
    through the MLlib (machine learning library) and GraphX (graph library) packages.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 是一个快速而强大的统一分析引擎（比传统的 Hadoop MapReduce 快高达一百倍），由于其内存操作，并且它提供了健壮、分布式、容错的数据抽象（称为
    RDDs 和 DataFrames）。Spark 通过 MLlib（机器学习库）和 GraphX（图库）包集成到机器学习和图分析领域。
- en: 'You can use Spark’s transformations and actions in four programming languages:
    Java, Scala, R, and Python. PySpark (the Python API for Spark) can be used for
    solving big data problems, efficiently transforming your data into the desired
    result and format.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在四种编程语言（Java、Scala、R 和 Python）中使用 Spark 的转换和操作。PySpark（Spark 的 Python API）可用于解决大数据问题，有效地将您的数据转换为所需的结果和格式。
- en: Big data can be represented using Spark’s data abstractions (RDDs, DataFrames,
    and Datasets—all of these are distributed datasets).
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据可以使用 Spark 的数据抽象（RDDs、DataFrames 和 Datasets——所有这些都是分布式数据集）来表示。
- en: You can run PySpark from the PySpark shell (using the `pyspark` command from
    a command line) for interactive Spark programming. Using the PySpark shell, you
    can create and manipulate RDDs and DataFrames.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从 PySpark shell 运行 PySpark（使用命令行的`pyspark`命令进行交互式 Spark 编程）。使用 PySpark shell，您可以创建和操作
    RDDs 和 DataFrames。
- en: You can submit a standalone PySpark application to a Spark cluster by using
    the `spark-submit` command; self-contained applications using PySpark are deployed
    to production environments.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用`spark-submit`命令将独立的 PySpark 应用程序提交到 Spark 集群；使用 PySpark 开发的自包含应用程序可部署到生产环境。
- en: Spark offers many transformations and actions for solving big data problems,
    and their performance differs (for example, `reduceByKey()` versus `groupByKey()`
    and `combineByKey()` versus `groupByKey()`).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 提供了许多转换和操作，用于解决大数据问题，它们的性能有所不同（例如，`reduceByKey()` 和 `groupByKey()` 以及
    `combineByKey()` 和 `groupByKey()`）。
- en: The next chapter dives into some important Spark transformations.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将深入介绍一些重要的 Spark 转换。
