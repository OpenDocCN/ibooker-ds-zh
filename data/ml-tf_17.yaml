- en: 14 Convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Examining the components of a convolutional neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查卷积神经网络组件
- en: Classifying natural images using deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习对自然图像进行分类
- en: Improving neural network performance—tips and tricks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高神经网络性能——技巧和窍门
- en: Grocery shopping after an exhausting day is a taxing experience. My eyes get
    bombarded with too much information. Sales, coupons, colors, toddlers, flashing
    lights, and crowded aisles are a few examples of all the signals forwarded to
    my visual cortex, whether or not I try to pay attention. The visual system absorbs
    an abundance of information.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在疲惫的一天之后去购物是一种负担。我的眼睛被过多的信息轰炸。促销、优惠券、颜色、幼儿、闪烁的灯光和拥挤的通道是所有信号被发送到我的视觉皮层的一些例子，无论我是否试图注意。视觉系统吸收了大量的信息。
- en: Ever heard the phrase “A picture is worth a thousand words”? That might be true
    for you or me, but can a machine find meaning within images as well? The photoreceptor
    cells in our retinas pick up wavelengths of light, but that information doesn’t
    seem to propagate up to our consciousness. After all, I can’t put into words exactly
    what wavelengths of lights I’m picking up. Similarly, a camera picks up pixels,
    yet we want to squeeze out some form of higher-level knowledge instead, such as
    names or locations of objects. How do we get from pixels to human-level perception?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否听说过“一图胜千言”这句话？这可能对你或我来说是真的，但机器能否在图像中找到意义呢？我们视网膜中的光感受细胞接收光波长度，但这些信息似乎并没有传播到我们的意识中。毕竟，我无法用言语准确描述我接收到的光波长度。同样，相机捕捉像素，但我们希望提取某种形式的高级知识，例如物体的名称或位置。我们如何从像素到人类水平的感知过渡呢？
- en: 'To achieve intelligent meaning from raw sensory input with machine learning,
    you’ll design a neural network model. In previous chapters, you’ve seen a few
    types of neural network models, such as fully connected ones (chapter 13) and
    autoencoders (chapters 11 and 12). In this chapter, you’ll meet another type of
    model: a *convolutional neural network* (CNN), which performs exceptionally well
    on images and other sensory data such as audio. A CNN model can reliably classify
    what object is being displayed in an image, for example.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过机器学习从原始感官输入中获取智能意义，你将设计一个神经网络模型。在前面的章节中，你已经看到了几种类型的神经网络模型，例如全连接的（第13章）和自编码器（第11章和第12章）。在本章中，你将遇到另一种类型的模型：一种*卷积神经网络*（CNN），它在图像和其他感官数据（如音频）上表现非常出色。CNN模型可以可靠地分类图像中显示的物体，例如。
- en: The CNN model that you’ll implement in this chapter will learn how to classify
    images to one of ten candidate categories from the CIFAR-10 dataset (chapters
    11 and 12). In effect, “A picture is worth only *one* word” out of only 10 possibilities.
    It’s a tiny step toward human-level perception, but you have to start somewhere,
    right?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将实现一个CNN模型，该模型将从CIFAR-10数据集（第11章和第12章）中学习如何将图像分类为十个候选类别之一。实际上，“一图只值十个可能性中的一个词”。这是向人类水平感知迈出的一小步，但你必须从某个地方开始，对吧？
- en: 14.1 Drawback of neural networks
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 神经网络的缺点
- en: 'Machine learning constitutes an eternal struggle: designing a model that’s
    expressive enough to represent the data, yet not so flexible that it overfits
    and memorizes the patterns. Neural networks are proposed as a way to improve that
    expressive power, yet as you may guess, they often suffer from the pitfalls of
    overfitting.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习构成了一场永恒的斗争：设计一个足够表达数据，但又不过于灵活以至于过度拟合和记住模式的模型。神经网络被提出作为一种提高这种表达能力的途径，但正如你可能猜到的，它们经常受到过度拟合的陷阱。
- en: Note Overfitting occurs when your learned model performs exceptionally well
    on the training dataset, yet tends to perform poorly on the test dataset. The
    model is likely too flexible for what little data is available, and it ends up
    more or less memorizing the training data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当你的学习模型在训练数据集上表现异常出色，但在测试数据集上表现不佳时，会发生过度拟合。该模型可能对可用数据的灵活性过高，最终几乎只是记住训练数据。
- en: A quick-and-dirty heuristic you can use to compare the flexibility of two machine-learning
    models is to count the number of parameters to be learned. As shown in figure
    14.1, a fully connected neural network that takes in a 256 × 256 image and maps
    it to a layer of 10 neurons will have 256 × 256 × 10 = 655,360 parameters! Compare
    that with a model with perhaps only five parameters. It’s likely that the fully
    connected neural network can represent more complex data than the model with five
    parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一个快速而简单的启发式方法来比较两个机器学习模型的灵活性，就是计算需要学习的参数数量。如图14.1所示，一个完全连接的神经网络，它接受一个256
    × 256的图像并将其映射到一个包含10个神经元的层，将会有256 × 256 × 10 = 655,360个参数！与可能只有五个参数的模型相比，完全连接的神经网络可能能够表示比五个参数的模型更复杂的数据。
- en: '![CH14_F01_Mattmann2](../Images/CH14_F01_Mattmann2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F01_Mattmann2](../Images/CH14_F01_Mattmann2.png)'
- en: Figure 14.1 In a fully connected network, each pixel of an image is treated
    as an input. For a grayscale image of size 256 × 256, that’s 256 × 256 neurons.
    Connecting each neuron to 10 outputs yields 256 × 256 × 10 = 655,360 weights.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 在一个完全连接的网络中，图像的每个像素都被视为一个输入。对于一个256 × 256大小的灰度图像，那就有256 × 256个神经元。将每个神经元连接到10个输出会产生256
    × 256 × 10 = 655,360个权重。
- en: Section 14.2 introduces CNNs, which are a clever way to reduce the number of
    parameters. Instead of dealing with a fully connected network that would have
    to learn many more parameters individually, you can take the CNN approach, reusing
    the same parameter multiple times to reduce the number of learned weights.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第14.2节介绍了卷积神经网络（CNNs），这是一种减少参数数量的巧妙方法。而不是处理一个需要学习许多参数的完全连接网络，你可以采用CNN方法，多次重用相同的参数来减少学习到的权重数量。
- en: 14.2 Convolutional neural networks
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 卷积神经网络
- en: The big idea behind CNNs is that a local understanding of an image is good enough.
    The practical benefit is that having fewer parameters greatly improves the time
    it takes to learn, as well as reduces the amount of data required to train the
    model. The time improvement sometimes comes at the cost of accuracy, however.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs背后的主要思想是，对图像的局部理解已经足够好。实际的好处是，拥有更少的参数可以大大提高学习所需的时间，以及减少训练模型所需的数据量。然而，这种时间上的改进有时是以准确性为代价的。
- en: Instead of a fully connected network of weights from each pixel, a CNN has enough
    weights to look at a small patch of the image. Imagine that you’re reading a book
    by using a magnifying glass; eventually, you read the whole page, but you look
    at only a small patch of the page at any given time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个像素的权重完全连接的网络不同，CNN有足够的权重来观察图像的一小部分。想象一下，你正在用放大镜读书；最终，你读完了整个页面，但你每次只看页面的一个小块。
- en: Consider a 256 × 256 image. Instead of processing the whole image at the same
    time, your TensorFlow code can efficiently scan it chunk by chunk—say, a 5 × 5
    window that slides along the image (usually left to right and top to bottom),
    as shown in figure 14.2\. How “quickly” it slides is called its *stride length*.
    A stride length of 2, for example, means that the 5 × 5 sliding window moves 2
    pixels at a time until it spans the entire image. In TensorFlow, you can easily
    adjust the stride length and window size by using the built-in library functions,
    as you’ll soon see.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个256 × 256的图像。而不是同时处理整个图像，你的TensorFlow代码可以高效地分块扫描它——比如说，一个沿着图像滑动（通常是左到右和上到下）的5
    × 5窗口，如图14.2所示。它滑动的“快慢”被称为其步长。例如，步长为2意味着5 × 5滑动窗口每次移动2个像素，直到覆盖整个图像。在TensorFlow中，你可以通过使用内置的库函数轻松调整步长和窗口大小，正如你很快就会看到的。
- en: '![CH14_F02_Mattmann2](../Images/CH14_F02_Mattmann2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F02_Mattmann2](../Images/CH14_F02_Mattmann2.png)'
- en: Figure 14.2 Convolving a 5 × 5 patch over an image (left) produces another image
    (right). In this case, the produced image is the same size as the original. Converting
    an original image to a convolved image requires only 5 × 5 = 25 parameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 在图像上卷积一个5 × 5的块（左侧）会产生另一个图像（右侧）。在这种情况下，产生的图像与原始图像大小相同。将原始图像转换为卷积图像只需要5
    × 5 = 25个参数。
- en: This 5 × 5 window has an associated 5 × 5 matrix of weights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个5 × 5的窗口有一个相关的5 × 5权重矩阵。
- en: NOTE A *convolution* is a weighted sum of the pixel values of the image, as
    the window slides across the whole image. This convolution process throughout
    an image with a weight matrix produces another image of the same size, depending
    on the convention. *Convolving* is the process of applying a convolution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：*卷积*是图像像素值的加权求和，当窗口在整个图像上滑动时。这个带有权重矩阵的卷积过程在整个图像上产生另一个大小相同的图像，具体取决于惯例。*卷积*是将卷积应用于图像的过程。
- en: The sliding-window shenanigans happen on the *convolution layer* of the neural
    network. A typical CNN has multiple convolution layers. Each convolutional layer
    typically generates many alternate convolutions, so the weight matrix is a tensor
    of 5 × 5 × *n*, where *n* is the number of convolutions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口的把戏发生在神经网络的*卷积层*上。一个典型的CNN有多个卷积层。每个卷积层通常生成许多交替的卷积，所以权重矩阵是一个5 × 5 × *n*的张量，其中*n*是卷积的数量。
- en: Suppose that an image goes through a convolution layer on a weight matrix of
    5 × 5 × 64\. This layer generates 64 convolutions by sliding a 5 × 5 window. Therefore,
    this model has 5 × 5 × 64 (= 1,600) parameters, which is remarkably fewer parameters
    than a fully connected network, which has 256 × 256 (= 65,536).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个图像通过一个5 × 5 × 64的权重矩阵的卷积层。这个层通过滑动一个5 × 5的窗口生成64个卷积。因此，这个模型有5 × 5 × 64（=
    1,600）个参数，这比全连接网络少得多，全连接网络有256 × 256（= 65,536）个参数。
- en: The beauty of the CNN is that the number of parameters is independent of the
    size of the original image. You can run the same CNN on a 300 × 300 image, and
    the number of parameters won’t change on the convolution layer!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的美丽之处在于参数数量与原始图像的大小无关。你可以在一个300 × 300的图像上运行相同的CNN，卷积层的参数数量不会改变！
- en: 14.3 Preparing the image
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 准备图像
- en: To start implementing CNNs in TensorFlow, you need to obtain some images to
    work with. The code listings in this section help you set up a training dataset
    for the remainder of the chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始在TensorFlow中实现CNN，你需要获取一些图像来工作。本节中的代码列表帮助你设置本章剩余部分的训练数据集。
- en: First, download the CIFAR-10 dataset from [www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz);
    if you need to, review chapters 11 and 12 for more information on it. This dataset
    contains 60,000 images, split into 10 categories, which makes it a great resource
    for classification tasks, as I showed you in those chapters. Extract that file
    to your working directory. Figure 14.3 shows examples of images from the dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)下载CIFAR-10数据集；如果需要，回顾第11章和第12章以获取更多相关信息。这个数据集包含60,000张图像，分为10个类别，这使得它成为分类任务的极好资源，正如我在那些章节中向你展示的那样。将文件提取到你的工作目录中。图14.3显示了数据集的图像示例。
- en: '![CH14_F03_Mattmann2](../Images/CH14_F03_Mattmann2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F03_Mattmann2](../Images/CH14_F03_Mattmann2.png)'
- en: Figure 14.3 Images from the CIFAR-10 dataset. Because they’re only 32 × 32,
    they’re a bit difficult to see, but you can recognize some of the objects.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 CIFAR-10数据集的图像。因为它们只有32 × 32，所以有点难以看清，但你还是可以识别出一些物体。
- en: You used the CIFAR-10 dataset in chapter 12, so pull up that code again. Listing
    14.1 comes straight from the CIFAR-10 documentation at [www.cs.toronto.edu/~kriz/
    cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html). Place the code in a file
    called cifar_tools.py.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第12章使用了CIFAR-10数据集，所以再次调出那段代码。列表14.1直接来自CIFAR-10文档[www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)。将代码放入一个名为cifar_tools.py的文件中。
- en: Listing 14.1 Loading images from a CIFAR-10 file in Python
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 使用Python从CIFAR-10文件中加载图像
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Neural networks are already prone to overfitting, so it’s essential that you
    do as much as you can to minimize that error. For that reason, always remember
    to clean the data before processing it. You’ve seen by now that data cleaning
    and pipelines are sometimes the majority of the work.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经容易过拟合，所以你需要尽可能减少那个错误。因此，始终记得在处理之前清洗数据。到现在为止，你已经看到数据清洗和流程有时是大部分的工作。
- en: 'Cleaning data is a core process in the machine-learning pipeline. Listing 14.2
    implements the following three steps for cleaning a dataset of images:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗数据是机器学习流程中的核心过程。列表14.2实现了以下三个步骤来清洗图像数据集：
- en: If you have an image in color, try converting it to grayscale to lower the dimensionality
    of the input data and consequently lower the number of parameters.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有一个彩色图像，尝试将其转换为灰度图以降低输入数据的维度，从而降低参数数量。
- en: Consider center-cropping the image, because the edges of an image might provide
    no useful information.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑对图像进行中心裁剪，因为图像的边缘可能不提供任何有用的信息。
- en: Normalize your input by subtracting the mean and dividing by the standard deviation
    of each data sample so that the gradients during backpropagation don’t change
    too dramatically.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从每个数据样本中减去均值并除以标准差来归一化你的输入，这样在反向传播过程中梯度不会变化得太剧烈。
- en: Listing 14.2 shows how to clean a dataset of images by using these techniques.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2展示了如何使用这些技术清理图像数据集。
- en: Listing 14.2 Cleaning data
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2 清理数据
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Reorganizes the data so it’s a 32 × 32 matrix with three channels
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据重新组织成32 × 32的矩阵，具有三个通道
- en: ❷ Grayscales the image by averaging the color intensities
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过平均颜色强度将图像转换为灰度
- en: ❸ Crops the 32 × 32 image to a 24 × 24 image to reduce parameters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将32 × 32的图像裁剪成24 × 24的图像以减少参数
- en: ❹ Normalizes the pixels’ values by subtracting the mean and dividing by standard
    deviation
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过减去均值并除以标准差来归一化像素值
- en: Collect all the images from CIFAR-10 into memory, and run your cleaning function
    on them. Listing 14.3 sets up a convenient method to read, clean, and structure
    your data for use in TensorFlow. Include this code in cifar_tools.py as well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将CIFAR-10中的所有图像收集到内存中，并在它们上运行清理函数。列表14.3设置了一个方便的方法来读取、清理和结构化你的数据，以便在TensorFlow中使用。请将此代码包含在cifar_tools.py中。
- en: Listing 14.3 Preprocessing all CIFAR-10 files
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3 预处理所有CIFAR-10文件
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In another file called using_cifar.py, you can use the method by importing `cifar_tools`.
    Listings 14.4 and 14.5 show how to sample a few images from the dataset and visualize
    them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个名为using_cifar.py的文件中，你可以通过导入`cifar_tools`来使用该方法。列表14.4和14.5展示了如何从数据集中采样一些图像并可视化它们。
- en: Listing 14.4 Using the `cifar_tools` helper function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4 使用`cifar_tools`辅助函数
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can randomly select a few images and draw them along their corresponding
    label. Listing 14.5 does exactly that, giving you a better understanding of the
    type of data you’ll be dealing with.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以随机选择一些图像并绘制它们对应的标签。列表14.5正是这样做的，这有助于你更好地理解你将处理的数据类型。
- en: Listing 14.5 Visualizing images from the dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.5 可视化数据集中的图像
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Change to as many rows and columns as you desire.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 改变到你想要的行数和列数。
- en: ❷ Randomly pick images from the dataset to show
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从数据集中随机选择图像进行展示
- en: By running this code, you’ll generate a file called cifar_examples.png that
    will look similar to figure 14.3 earlier in this section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，将生成一个名为cifar_examples.png的文件，其外观将与本节中较早的图14.3相似。
- en: 14.3.1 Generating filters
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 生成过滤器
- en: In this section, you’ll convolve an image with a couple of random 5 × 5 patches,
    also called *filters*. This step is an important one in a CNN, so you’ll carefully
    examine how the data transforms. To understand a CNN model for image processing,
    it’s wise to observe the way that an image filter transforms an image. Filters
    extract useful image features such as edges and shapes. You can train a machine-learning
    model on these features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用几个随机的5 × 5补丁（也称为*过滤器*）对图像进行卷积。这一步在CNN中非常重要，因此你需要仔细检查数据是如何转换的。为了理解图像处理的CNN模型，观察图像过滤器如何转换图像是明智的。过滤器提取有用的图像特征，如边缘和形状。你可以在这些特征上训练机器学习模型。
- en: Remember that a feature vector indicates how you represent a data point. When
    you apply a filter to an image, the corresponding point in the transformed image
    is a feature—a feature that says, “When you apply this filter to this point, it
    has this new value.” The more filters you use on an image, the greater the dimensionality
    of the feature vector. The overall goal is to balance the number of filters that
    reduce the dimensionality while still capturing the important features in the
    original image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，特征向量表示了如何表示数据点。当你对一个图像应用过滤器时，转换后的图像中的对应点是一个特征——这个特征表明，“当你将这个过滤器应用到这个点时，它具有这个新的值。”你在一个图像上使用的过滤器越多，特征向量的维度就越大。整体目标是平衡减少维度的过滤器数量，同时仍然捕捉到原始图像中的重要特征。
- en: Open a new file called conv_visuals.py. Let’s randomly initialize 32 filters.
    You’ll do so by defining a variable called W of size 5 × 5 × 1 × 32\. The first
    two dimensions correspond to the filter size; the last dimension corresponds to
    the 32 convolutions. The 1 in the variable’s size corresponds to the input dimension,
    because the `conv2d` function is capable of convolving images of multiple channels.
    (In this example, you care about only grayscale images, so the number of input
    channels is 1.) Listing 14.6 provides the code to generate filters, which are
    shown in figure 14.4.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个名为 conv_visuals.py 的新文件。让我们随机初始化 32 个滤波器。你将通过定义一个名为 W 的大小为 5 × 5 × 1 × 32
    的变量来完成此操作。前两个维度对应于滤波器大小；最后一个维度对应于 32 个卷积。变量大小中的 1 对应于输入维度，因为 `conv2d` 函数能够卷积多通道的图像。
    (在这个例子中，你只关心灰度图像，所以输入通道的数量是 1。) 列表 14.6 提供了生成滤波器的代码，这些滤波器如图 14.4 所示。
- en: Listing 14.6 Generating and visualizing random filters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.6 生成和可视化随机滤波器
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Defines the tensor representing the random filters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义表示随机滤波器的张量
- en: ❷ Defines enough rows and columns to show the 32 figures in figure 14.4
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义足够的行和列以显示图 14.4 中的 32 个图像
- en: ❸ Visualizes each filter matrix
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 可视化每个滤波器矩阵
- en: '![CH14_F04_Mattmann2](../Images/CH14_F04_Mattmann2.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F04_Mattmann2](../Images/CH14_F04_Mattmann2.png)'
- en: Figure 14.4 Each of these 32 randomly initialized matrices is size 5 × 5\. The
    matrices represent the filters you’ll use to convolve an input image.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 这些 32 个随机初始化的矩阵大小为 5 × 5。这些矩阵代表你将用于卷积输入图像的滤波器。
- en: Exercise 14.1
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 14.1
- en: What would you change in listing 14.6 to generate 64 filters of size 3 × 3?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在列表 14.6 中做哪些更改以生成大小为 3 × 3 的 64 个滤波器？
- en: '**Answer**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: W = tf.Variable(tf.random_normal([3, 3, 1, 64]))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: W = tf.Variable(tf.random_normal([3, 3, 1, 64]))
- en: Use a session, as shown in listing 14.7, and initialize some weights by using
    the `global_variables_initializer` op. Call the `show_weights` function to visualize
    random filters, as shown in figure 14.4.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表 14.7 中所示的会话，并使用 `global_variables_initializer` 操作初始化一些权重。调用 `show_weights`
    函数来可视化如图 14.4 所示的随机滤波器。
- en: Listing 14.7 Using a session to initialize weights
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.7 使用会话初始化权重
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 14.3.2 Convolving using filters
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.2 使用滤波器进行卷积
- en: Section 14.3.1 showed you how to prepare filters to use. In this section, you’ll
    use TensorFlow’s convolve function on your randomly generated filters. Listing
    14.8 sets up code to visualize the convolution outputs. You’ll use it later, as
    you used `show_weights`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 14.3.1 节向你展示了如何准备用于卷积的滤波器。在本节中，你将使用 TensorFlow 的卷积函数对随机生成的滤波器进行卷积。列表 14.8 设置了可视化卷积输出的代码。你将在之后使用它，就像你使用了
    `show_weights`。
- en: Listing 14.8 Showing convolution results
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.8 展示卷积结果
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Unlike in listing 14.6, the tensor shape is different; it’s not the weights,
    but the resulting image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与列表 14.6 不同，张量形状不同；它不是权重，而是结果图像。
- en: Suppose that you have an example input image, such as the one shown in figure
    14.5\. You can convolve the 24 × 24 image by using 5 × 5 filters to produce many
    convolved images. All these convolutions are unique perspectives on the same image.
    These perspectives work together to comprehend the object that exists in the image.
    Listing 14.9 shows how to perform this task step by step.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个示例输入图像，例如图 14.5 中所示。你可以使用 5 × 5 滤波器对 24 × 24 图像进行卷积，从而产生许多卷积图像。所有这些卷积都是对同一图像的独特视角。这些视角共同工作，以理解图像中存在的对象。列表
    14.9 展示了如何逐步执行此任务。
- en: '![CH14_F05_Mattmann2](../Images/CH14_F05_Mattmann2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F05_Mattmann2](../Images/CH14_F05_Mattmann2.png)'
- en: Figure 14.5 An example 24 × 24 image from the CIFAR-10 dataset
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 来自 CIFAR-10 数据集的一个示例 24 × 24 图像
- en: Listing 14.9 Visualizing convolutions
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.9 可视化卷积
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Gets an image from the CIFAR dataset and visualizes it
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 CIFAR 数据集中获取一个图像并可视化
- en: ❷ Defines the input tensor for the 24 × 24 image
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义 24 × 24 图像的输入张量
- en: ❸ Defines the filters and corresponding parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义滤波器和相应的参数
- en: ❹ Runs the convolution on the selected image
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在选定的图像上运行卷积
- en: Finally, by running the `conv2d` function in TensorFlow, you get the 32 images
    in figure 14.6\. The idea of convolving images is that each of the 32 convolutions
    captures different features of the image.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过在 TensorFlow 中运行 `conv2d` 函数，你得到图 14.6 中的 32 个图像。卷积图像的想法是，每个 32 个卷积捕捉到图像的不同特征。
- en: '![CH14_F06_Mattmann2](../Images/CH14_F06_Mattmann2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F06_Mattmann2](../Images/CH14_F06_Mattmann2.png)'
- en: Figure 14.6 Resulting images from convolving the random filters on an image
    of a car
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 在汽车图像上应用随机滤波器后的结果图像
- en: With the addition of a bias term and an activation function such as `relu` (see
    listing 14.12 for an example), the convolution layer of the network behaves nonlinearly,
    which improves its expressiveness. Figure 14.7 shows what each of the 32 convolution
    outputs becomes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加偏置项和激活函数（如 `relu`，见列表 14.12 中的示例），网络的卷积层表现出非线性，这提高了其表达能力。图 14.7 显示了每个 32
    个卷积输出变成了什么。
- en: '![CH14_F07_Mattmann2](../Images/CH14_F07_Mattmann2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F07_Mattmann2](../Images/CH14_F07_Mattmann2.png)'
- en: Figure 14.7 After you add a bias term and an activation function, the resulting
    convolutions can capture more-powerful patterns within images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 在你添加偏置项和激活函数后，结果卷积可以捕捉到图像中更强大的模式。图 14.7 显示了每个 32 个卷积输出变成了什么。
- en: 14.3.3 Max pooling
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.3 最大池化
- en: After a convolution layer extracts useful features, it’s usually a good idea
    to reduce the size of the convolved outputs. Rescaling or subsampling a convolved
    output helps reduce the number of parameters, which in turn can help prevent overfitting
    the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层提取有用特征之后，通常减少卷积输出的尺寸是一个好主意。对卷积输出进行缩放或子采样有助于减少参数数量，这反过来又可以帮助防止数据过拟合。
- en: This concept is the main idea behind *max pooling*, which sweeps a window across
    an image and picks the pixel with the maximum value. Depending on the stride length,
    the resulting image is a fraction of the size of the original. This technique
    is useful because it lessens the dimensionality of the data, reducing the number
    of parameters in future steps.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念是 *最大池化* 的主要思想，它在图像上扫过一个窗口并选择具有最大值的像素。根据步长长度，结果图像是原始图像大小的分数。这项技术很有用，因为它减少了数据的维度，减少了后续步骤中的参数数量。
- en: Exercise 14.2
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 14.2
- en: Let’s say you want to max pool over a 32 × 32 image. If the window size is 2
    × 2 and the stride length is 2, how big will the resulting max-pooled image be?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要在一个 32 × 32 的图像上进行最大池化。如果窗口大小是 2 × 2，步长长度是 2，那么结果的最大池化图像有多大？
- en: '**Answer**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: 'The 2 × 2 window would need to move 16 times in each direction to span the
    32 × 32 image, so the image would shrink by half: 16 × 16\. Because it shrank
    by half in both dimensions, the image is one-fourth the size of the original image
    (½ × ½).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2 × 2 窗口需要在每个方向上移动 16 次，以覆盖 32 × 32 的图像，因此图像将缩小一半：16 × 16。因为它在两个维度上都缩小了一半，所以图像是原始图像的四分之一大小（½
    × ½）。
- en: Place listing 14.10 within the `Session` context.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表 14.10 放在 `Session` 上下文中。
- en: Listing 14.10 Running the `maxpool` function to subsample convolved images
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.10 运行 `maxpool` 函数进行卷积图像子采样
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As a result of running this code, the max-pooling function halves the image
    size and produces lower-resolution convolved outputs, as shown in figure 14.8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码的结果是，最大池化函数将图像尺寸减半，并产生如图 14.8 所示的低分辨率卷积输出。
- en: '![CH14_F08_Mattmann2](../Images/CH14_F08_Mattmann2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F08_Mattmann2](../Images/CH14_F08_Mattmann2.png)'
- en: Figure 14.8 After `maxpool` runs, the convolved outputs are halved in size,
    making the algorithm computationally faster without losing too much information.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8 在 `maxpool` 运行后，卷积输出的大小减半，这使得算法在计算上更快，同时没有丢失太多信息。
- en: You have the tools necessary to implement the full CNN. In section 14.4, you’ll
    finally train the classifier.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经有了实现完整 CNN 所需的工具。在第 14.4 节中，你将最终训练分类器。
- en: 14.4 Implementing a CNN in TensorFlow
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 在 TensorFlow 中实现 CNN
- en: A CNN has multiple layers of convolutions and max pooling. The convolution layer
    offers different perspectives on the image, and the max-pooling layer simplifies
    the computations by reducing the dimensionality without losing too much information.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 有多个卷积和最大池化层。卷积层提供了对图像的不同视角，最大池化层通过减少维度简化了计算，同时没有丢失太多信息。
- en: Consider a full-size 256 × 256 image convolved by a 5 × 5 filter into 64 convolutions.
    As shown in figure 14.9, each convolution is subsampled by using max pooling to
    produce 64 smaller convolved images of size 128 × 128.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个全尺寸 256 × 256 的图像通过一个 5 × 5 滤波器卷积成 64 个卷积。如图 14.9 所示，每个卷积通过最大池化进行子采样，产生
    64 个较小的 128 × 128 尺寸的卷积图像。
- en: '![CH14_F09_Mattmann2](../Images/CH14_F09_Mattmann2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH14_F09_Mattmann2](../Images/CH14_F09_Mattmann2.png)'
- en: Figure 14.9 An input image is convolved by multiple 5 × 5 filters. The convolution
    layer includes an added bias term with an activation function, resulting in 5
    × 5 + 5 = 30 parameters. Next, a max-pooling layer reduces the dimensionality
    of the data (which requires no extra parameters).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9 一个输入图像通过多个 5 × 5 滤波器进行卷积。卷积层包括一个附加的偏置项和激活函数，结果有 5 × 5 + 5 = 30 个参数。接下来，一个最大池化层减少了数据的维度（不需要额外的参数）。
- en: Now that you know how to make filters and use the convolution op, let’s create
    a new source file. You’ll start by defining all your variables. In listing 14.11,
    import all libraries, load the dataset, and define all variables.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何制作滤波器和使用卷积操作，让我们创建一个新的源文件。你将首先定义所有变量。在列表14.11中，导入所有库，加载数据集，并定义所有变量。
- en: Listing 14.11 Setting up CNN weights
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.11 设置CNN权重
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Loads the dataset
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集
- en: ❷ Defines the input and output placeholders
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义输入和输出占位符
- en: ❸ Applies 64 convolutions of window size 5 × 5
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用64个5 × 5窗口大小的卷积
- en: ❹ Applies 64 more convolutions of window size 5 × 5
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用64个更多5 × 5窗口大小的卷积
- en: ❺ Introduces a fully connected layer
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 引入一个全连接层
- en: ❻ Defines the variables for a fully connected linear layer
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义全连接线性层的变量
- en: In listing 14.12, you define a helper function to perform a convolution, add
    a bias term, and then add an activation function. Together, these three steps
    form a convolution layer of the network.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表14.12中，你定义了一个辅助函数来执行卷积、添加偏置项，然后添加激活函数。这三个步骤共同构成了网络的一个卷积层。
- en: Listing 14.12 Creating a convolution layer
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.12 创建卷积层
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Listing 14.13 shows how to define the max-pool layer by specifying the kernel
    and stride size.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.13展示了如何通过指定内核和步长大小来定义最大池化层。
- en: Listing 14.13 Creating a max-pool layer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.13 创建最大池化层
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can stack together the convolution and max-pool layers to define the CNN
    architecture. Listing 14.14 defines a possible CNN model. The last layer typically
    is a fully connected network connected to each of the 10 output neurons.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将卷积层和最大池化层堆叠起来，以定义CNN架构。列表14.14定义了一个可能的CNN模型。最后一层通常是连接到每个10个输出神经元的全连接网络。
- en: Listing 14.14 The full CNN model
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.14 完整的CNN模型
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Constructs the first layer of convolution and max pooling
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建卷积和最大池化的第一层
- en: ❷ Constructs the second layer
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建第二层
- en: ❸ Constructs the concluding fully connected layers
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建最后的全连接层
- en: 14.4.1 Measuring performance
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.1 测量性能
- en: 'With a neural network architecture designed, the next step is defining a cost
    function that you want to minimize. You’ll use TensorFlow’s `softmax_cross_entropy_
    with_logits` function, which is best described by the official documentation at
    [http://mng.bz/4Blw](http://mng.bz/4Blw):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 设计好神经网络架构后，下一步是定义一个你想要最小化的损失函数。你将使用TensorFlow的`softmax_cross_entropy_with_logits`函数，官方文档中对其有最佳描述，请参阅[http://mng.bz/4Blw](http://mng.bz/4Blw)：
- en: '[The function `softmax_cross_entropy_with_logits]` measures the probability
    error in discrete classification tasks in which the classes are mutually exclusive
    (each entry is in exactly one class). For example, each CIFAR-10 image is labeled
    with one and only one label: an image can be a dog or a truck, but not both.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[函数`softmax_cross_entropy_with_logits`]用于测量在类别互斥的离散分类任务中的概率误差（每个条目恰好属于一个类别）。例如，每个CIFAR-10图像都标记了一个标签：一个图像可以是狗或卡车，但不能同时是两者。'
- en: Because an image can belong to one of ten possible labels, you’ll represent
    that choice as a 10D vector. All elements of this vector have a value of `0` except
    the element that corresponds to the label, which will have a value of `1`. This
    representation, as you’ve seen in earlier chapters, is called *one-hot encoding*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个图像可能属于十个可能的标签之一，因此你将这个选择表示为一个10维向量。这个向量的所有元素值都是`0`，除了与标签对应的元素，其值将是`1`。正如你在前面的章节中看到的，这种表示方法称为*独热编码*。
- en: As shown in listing 14.15, you’ll calculate the cost via the cross-entropy loss
    function I mentioned in chapter 6\. This code returns the probability error for
    your classification. Note that the code works only for simple classifications—those
    in which your classes are mutually exclusive. (A truck can’t also be a dog, for
    example.) You can employ many types of optimizers, but in this example, you stick
    with the AdamOptimizer, which is a fast, simple optimizer described in detail
    at [http://mng.bz/QxJG](http://mng.bz/QxJG).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表14.15所示，你将通过我在第6章提到的交叉熵损失函数来计算成本。此代码返回你的分类的概率误差。请注意，此代码仅适用于简单的分类——那些你的类别是互斥的。例如，一个卡车不能同时是狗。你可以使用许多类型的优化器，但在这个例子中，你坚持使用AdamOptimizer，这是一个快速、简单的优化器，在[http://mng.bz/QxJG](http://mng.bz/QxJG)上有详细描述。
- en: It may be worth playing around with the arguments in real-world applications,
    but the AdamOptimizer works well off-the-shelf.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，可能需要尝试调整参数，但AdamOptimizer现成效果很好。
- en: Listing 14.15 Defining ops to measure cost and accuracy
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.15 定义测量成本和准确性的操作
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Defines the classification loss function
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义分类损失函数
- en: ❷ Defines the training op to minimize the loss function
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义训练操作以最小化损失函数
- en: Finally, in section 14.4.2, you’ll run the training op to minimize the cost
    of the neural network. Doing so multiple times throughout the dataset will teach
    the optimal weights (or parameters).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在14.4.2节中，你将运行训练操作以最小化神经网络的成本。在数据集上多次执行此操作将教会最优的权重（或参数）。
- en: 14.4.2 Training the classifier
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.2 训练分类器
- en: In listing 14.16, you’ll loop through the dataset of images in small batches
    to train the neural network. Over time, the weights will slowly converge to a
    local optimum to predict the training images accurately.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表14.16中，你将通过小批量遍历图像数据集来训练神经网络。随着时间的推移，权重将逐渐收敛到一个局部最优，以准确预测训练图像。
- en: Listing 14.16 Training the neural network by using the CIFAR-10 dataset
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.16 使用CIFAR-10数据集训练神经网络
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Loops through 1,000 epochs
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 循环遍历1,000个epoch
- en: ❷ Trains the network in batches
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 批量训练网络
- en: 'That’s it! You’ve successfully designed a CNN to classify images. Beware: training
    the CNN might take a lot more than 10 minutes. If you’re running this code on
    CPU, it might even take hours! Can you imagine discovering a bug in your code
    after a day of waiting? That’s why deep-learning researchers use powerful computers
    and GPUs to speed computations.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经成功设计了一个用于图像分类的CNN。请注意：训练CNN可能需要超过10分钟的时间。如果你在CPU上运行此代码，甚至可能需要数小时！你能想象在等待一天后发现代码中存在错误吗？这就是为什么深度学习研究人员使用强大的计算机和GPU来加速计算。
- en: 14.5 Tips and tricks to improve performance
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 提高性能的技巧和窍门
- en: 'The CNN you developed in this chapter is a simple approach to solving the problem
    of image classification, but many techniques can improve performance after you
    finish your first working prototype:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你开发的CNN是解决图像分类问题的一种简单方法，但在完成第一个工作原型后，许多技术可以提高性能：
- en: Augmenting data —From a single image, you can easily generate new training images.
    As a start, flip an image horizontally or vertically, and you can quadruple your
    dataset size. You can also adjust the brightness of the image or the hue to ensure
    that the neural network generalizes to other fluctuations. You may even want to
    add random noise to the image to make the classifier robust to small occlusions.
    Scaling the image up or down can also be helpful; having exactly the same-sized
    items in your training images will almost guarantee overfitting.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强——从一个单独的图像中，你可以轻松地生成新的训练图像。作为开始，你可以水平或垂直翻转图像，这样可以将数据集的大小增加到原来的四倍。你还可以调整图像的亮度或色调，以确保神经网络能够泛化到其他波动。你甚至可能想要向图像中添加随机噪声，使分类器对小的遮挡具有鲁棒性。调整图像的大小也可能很有帮助；在训练图像中保持完全相同大小的项目几乎可以保证过拟合。
- en: Early stopping —Keep track of the training and testing errors while you train
    the neural network. At first, both types of errors should dwindle slowly, because
    the network is learning. But sometimes, test errors go back up, which signals
    that the neural network has started overfitting on the training data and is unable
    to generalize to previously unseen input. You should stop the training the moment
    you witness this phenomenon.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止——在训练神经网络的同时，跟踪训练和测试错误。最初，这两种类型的错误应该缓慢减少，因为网络正在学习。但有时，测试错误会再次上升，这表明神经网络已经开始在训练数据上过拟合，并且无法泛化到之前未见过的输入。你应该在发现这种现象时立即停止训练。
- en: Regularizing weights —Another way to combat overfitting is to add a regularization
    term to the cost function. You’ve seen regularization in previous chapters, and
    the same concepts apply here.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化权重——另一种对抗过拟合的方法是在损失函数中添加一个正则化项。你已经在之前的章节中看到了正则化，这里同样适用相同的概念。
- en: Dropout —TensorFlow comes with a handy `tf.nn.dropout` function, which can be
    applied to any layer of the network to reduce overfitting. The function turns
    off a randomly selected number of neurons in that layer during training so that
    the network is redundant and robust to inferring output.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout——TensorFlow提供了一个方便的`tf.nn.dropout`函数，可以将它应用于网络的任何一层以减少过拟合。在训练过程中，该函数会关闭随机选择的一定数量的神经元，从而使网络在推断输出时具有冗余性和鲁棒性。
- en: Deeper architecture —A deeper architecture results from adding more hidden layers
    to the neural network. If you have enough training data, adding more hidden layers
    has been shown to improve performance. The network will take more time to train,
    however.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度架构——通过向神经网络添加更多的隐藏层，可以得到更深的架构。如果你有足够的训练数据，添加更多的隐藏层已被证明可以提高性能。然而，网络将需要更多的时间来训练。
- en: Exercise 14.3
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 14.3
- en: After the first iteration of this CNN architecture, try applying a couple of
    tips and tricks mentioned in this chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个CNN架构的第一轮迭代之后，尝试应用本章中提到的几个技巧和窍门。
- en: '**Answer**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Fine-tuning is part of the process, unfortunately. You should begin by adjusting
    the hyperparameters and retraining the algorithm until you find the setting that
    works best.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 精调是这个过程的一部分，遗憾的是。你应该从调整超参数和重新训练算法开始，直到找到最佳设置。
- en: 14.6 Application of CNNs
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.6 CNNs 的应用
- en: CNNs blossom when the input contains sensor data from audio or images. Images
    in particular are of major interest in industry. When you sign up for a social
    network, for example, you usually upload a profile photo, not an audio recording
    of yourself saying “Hello.” Humans seem to be naturally entertained by photos,
    so you can experiment to see how CNNs can be used to detect faces in images.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入包含来自音频或图像的传感器数据时，CNNs 会蓬勃发展。特别是图像在工业界中非常重要。例如，当你注册社交网络时，你通常会上传个人照片，而不是你自己的“你好”音频录音。人类似乎天生喜欢照片，因此你可以尝试看看CNNs如何用于检测图像中的面部。
- en: The overall CNN architecture can be as simple or as complicated as you desire.
    You should start simple and gradually tune your model until you’re satisfied.
    There’s no one correct path, because facial recognition isn’t completely solved.
    Researchers are still publishing papers that one-up previous state-of-the-art
    solutions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 整个CNN架构可以像你希望的那样简单或复杂。你应该从简单开始，逐渐调整你的模型，直到你满意为止。没有一条正确的路径，因为面部识别尚未完全解决。研究人员仍在发表超越先前最先进解决方案的论文。
- en: 'Your first step should be obtaining a dataset of images. One of the largest
    datasets of arbitrary images is ImageNet ([http://image-net.org](http://image-net.org/)).
    Here, you can find negative examples for your binary classifier. To obtain positive
    examples of faces, you can find numerous datasets at the following sites that
    specialize in human faces:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一步应该是获取一个图像数据集。最大的任意图像数据集之一是 ImageNet ([http://image-net.org](http://image-net.org/))。在这里，你可以找到你的二元分类器的负面示例。要获取面部正面示例，你可以在以下专注于人类面部的网站上找到许多数据集：
- en: VGG -Face Dataset ([http://www.robots.ox.ac.uk/~vgg/data/vgg_face](http://www.robots.ox.ac.uk/~vgg/data/vgg_face/))
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG -Face 数据集 ([http://www.robots.ox.ac.uk/~vgg/data/vgg_face](http://www.robots.ox.ac.uk/~vgg/data/vgg_face/))
- en: Face Detection Data Set and Benchmark (FDDB)([http://vis-www.cs.umass.edu/ fddb](http://vis-www.cs.umass.edu/fddb/))
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部检测数据集和基准（FDDB）([http://vis-www.cs.umass.edu/ fddb](http://vis-www.cs.umass.edu/fddb/))
- en: Databases for Face Detection and Pose Estimation ([http://mng.bz/X0Jv](http://mng.bz/X0Jv))
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部检测和姿态估计数据库 ([http://mng.bz/X0Jv](http://mng.bz/X0Jv))
- en: YouTube Faces Database ([www.cs.tau.ac.il/~wolf/ytfaces](http://www.cs.tau.ac.il/~wolf/ytfaces/))
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube 面部数据库 ([www.cs.tau.ac.il/~wolf/ytfaces](http://www.cs.tau.ac.il/~wolf/ytfaces/))
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CNNs make assumptions that capturing the local patterns of a signal is sufficient
    to characterize that signal and thus reduce the number of parameters of a neural
    network.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）假设捕捉信号的局部模式足以表征该信号，从而减少神经网络的参数数量。
- en: Cleaning data is vital to the performance of most machine-learning models. The
    hour you spend to write code that cleans data is nothing compared with the amount
    of time it can take for a neural network to learn that cleaning function by itself.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据对于大多数机器学习模型的表现至关重要。你用来编写清洗数据代码的小时与神经网络自己学习这个清洗函数所需的时间相比微不足道。
