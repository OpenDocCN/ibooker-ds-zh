- en: Chapter 1\. Analyzing Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章。分析大数据
- en: 'When people say that we live in an age of big data they mean that we have tools
    for collecting, storing, and processing information at a scale previously unheard
    of. The following tasks simply could not have been accomplished 10 or 15 years
    ago:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们说我们生活在大数据时代时，他们意味着我们有工具可以在以前听都没听说过的规模上收集、存储和处理信息。在10或15年前，以下任务根本无法完成：
- en: Build a model to detect credit card fraud using thousands of features and billions
    of transactions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数千个特征和数十亿笔交易构建检测信用卡欺诈的模型
- en: Intelligently recommend millions of products to millions of users
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能向数百万用户推荐数百万种产品
- en: Estimate financial risk through simulations of portfolios that include millions
    of instruments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过包含数百万个工具的投资组合模拟估算财务风险
- en: Easily manipulate genomic data from thousands of people to detect genetic associations
    with disease
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻松操作来自数千人的基因组数据，以检测与疾病相关的遗传关联
- en: Assess agricultural land use and crop yield for improved policymaking by periodically
    processing millions of satellite images
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过定期处理数百万张卫星图像评估农业用地利用和作物产量，以改进政策制定
- en: Sitting behind these capabilities is an ecosystem of open source software that
    can leverage clusters of servers to process massive amounts of data. The introduction/release
    of Apache Hadoop in 2006 has led to widespread adoption of distributed computing.
    The big data ecosystem and tooling have evolved at a rapid pace since then. The
    past five years have also seen the introduction and adoption of many open source
    machine learning (ML) and deep learning libraries. These tools aim to leverage
    vast amounts of data that we now collect and store.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些能力背后是一整套开源软件生态系统，可以利用服务器集群处理大量数据。2006年引入/发布的Apache Hadoop推动了分布式计算的广泛采用。从那时起，大数据生态系统和工具在快速发展。过去五年还见证了许多开源机器学习（ML）和深度学习库的引入和采用。这些工具旨在利用我们现在收集和存储的大量数据。
- en: But just as a chisel and a block of stone do not make a statue, there is a gap
    between having access to these tools and all this data and doing something useful
    with it. Often, “doing something useful” means placing a schema over tabular data
    and using SQL to answer questions like “Of the gazillion users who made it to
    the third page in our registration process, how many are over 25?” The field of
    how to architect data storage and organize information (data warehouses, data
    lakes, etc.) to make answering such questions easy is a rich one, but we will
    mostly avoid its intricacies in this book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但就像凿子和一块石头不能创造雕像一样，拥有这些工具和所有这些数据之间存在一定的差距。通常，“做些有用的事情”意味着在表格数据上放置架构，并使用 SQL
    来回答像“在我们的注册流程中成功到达第三页的无数用户中，超过 25 岁的有多少？”这样的问题。关于如何设计数据存储和组织信息（数据仓库、数据湖等）以便轻松回答此类问题的领域非常丰富，但在本书中我们大多数情况下将避免其复杂性。
- en: Sometimes, “doing something useful” takes a little extra work. SQL still may
    be core to the approach, but to work around idiosyncrasies in the data or perform
    complex analysis, we need a programming paradigm that’s more flexible and with
    richer functionality in areas like machine learning and statistics. This is where
    data science comes in and that’s what we are going to talk about in this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，“做些有用的事情”需要额外的工作。虽然 SQL 仍然可能是方法的核心，但为了解决数据的特殊性或进行复杂的分析，我们需要一种更灵活、功能更丰富的编程范式，特别是在机器学习和统计等领域。这就是数据科学的应用场景，也是我们在本书中将要讨论的内容。
- en: In this chapter, we’ll start by introducing big data as a concept and discuss
    some of the challenges that arise when working with large datasets. We will then
    introduce Apache Spark, an open source framework for distributed computing, and
    its key components. Our focus will be on PySpark, Spark’s Python API, and how
    it fits within a wider ecosystem. This will be followed by a discussion of the
    changes brought by Spark 3.0, the framework’s first major release in four years.
    We will finish with a brief note about how PySpark addresses challenges of data
    science and why it is a great addition to your skillset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍大数据的概念，并讨论处理大型数据集时出现的一些挑战。然后，我们将介绍 Apache Spark，一个用于分布式计算的开源框架，以及其关键组件。我们的重点将放在
    PySpark 上，这是 Spark 的 Python API，并探讨它在更广泛生态系统中的应用。接着，我们将讨论 Spark 3.0 带来的变化，这是该框架四年来的首个重要版本。最后，我们将简要说明
    PySpark 如何解决数据科学的挑战，并解释为什么它是你技能组合的重要补充。
- en: Previous editions of this book used Spark’s Scala API for code examples. We
    decided to use PySpark instead because of Python’s popularity in the data science
    community and an increased focus by the core Spark team to better support the
    language. By the end of this chapter, you will ideally appreciate this decision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的早期版本使用了 Spark 的 Scala API 来展示代码示例。我们决定改用 PySpark，因为 Python 在数据科学社区中很受欢迎，而且核心
    Spark 团队也在更好地支持这种语言。通过本章的学习，您理想地将会欣赏到这个决定。
- en: Working with Big Data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理大数据
- en: 'Many of our favorite small data tools hit a wall when working with big data.
    Libraries like pandas are not equipped to deal with data that can’t fit in our
    RAM. Then, what should an equivalent process look like that can leverage clusters
    of computers to achieve the same outcomes on large datasets? Challenges of distributed
    computing require us to rethink many of the basic assumptions that we rely on
    in single-node systems. For example, because data must be partitioned across many
    nodes on a cluster, algorithms that have wide data dependencies will suffer from
    the fact that network transfer rates are orders of magnitude slower than memory
    accesses. As the number of machines working on a problem increases, the probability
    of a failure increases. These facts require a programming paradigm that is sensitive
    to the characteristics of the underlying system: one that discourages poor choices
    and makes it easy to write code that will execute in a highly parallel manner.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大数据时，我们喜爱的许多小数据工具在某些时候会遇到瓶颈。像 pandas 这样的库无法处理不能放入内存的数据。那么，等效的过程应该是什么样的，可以利用计算机集群在大数据集上实现相同的结果？分布式计算的挑战要求我们重新思考在单节点系统中依赖的许多基本假设。例如，因为数据必须分布在集群的许多节点上，具有广泛数据依赖关系的算法将受到网络传输速率远远慢于内存访问的影响。随着解决问题的机器数量增加，失败的概率也会增加。这些事实要求一种编程范式，对底层系统的特性敏感：它阻止糟糕的选择，并且使编写能够高度并行执行的代码变得容易。
- en: Single-machine tools that have come to recent prominence in the software community
    are not the only tools used for data analysis. Scientific fields like genomics
    that deal with large datasets have been leveraging parallel-computing frameworks
    for decades. Most people processing data in these fields today are familiar with
    a cluster-computing environment called HPC (high-performance computing). Where
    the difficulties with Python and R lie in their inability to scale, the difficulties
    with HPC lie in its relatively low level of abstraction and difficulty of use.
    For example, to process a large file full of DNA-sequencing reads in parallel,
    we must manually split it up into smaller files and submit a job for each of those
    files to the cluster scheduler. If some of these fail, the user must detect the
    failure and manually resubmit them. If the analysis requires all-to-all operations
    like sorting the entire dataset, the large dataset must be streamed through a
    single node, or the scientist must resort to lower-level distributed frameworks
    like MPI, which are difficult to program without extensive knowledge of C and
    distributed/networked systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年在软件社区中备受关注的单机工具并不是数据分析的唯一工具。像基因组学这样处理大数据集的科学领域几十年来一直在利用并行计算框架。今天在这些领域处理数据的大多数人熟悉一个名为
    HPC（高性能计算）的集群计算环境。Python 和 R 的困难在于它们无法很好地扩展，而 HPC 的困难在于其相对低的抽象水平和使用难度。例如，要并行处理一个充满
    DNA 测序读数的大文件，我们必须手动将其分割成更小的文件，并为每个文件提交一个作业给集群调度器。如果其中一些失败，用户必须检测到故障并手动重新提交。如果分析需要全局排序等全对全操作，大数据集必须通过单个节点进行流式处理，或者科学家必须借助更低级别的分布式框架如
    MPI，而这些框架在没有对 C 语言和分布/网络系统的广泛知识的情况下很难编程。
- en: Tools written for HPC environments often fail to decouple the in-memory data
    models from the lower-level storage models. For example, many tools only know
    how to read data from a POSIX filesystem in a single stream, making it difficult
    to make tools naturally parallelize or to use other storage backends, like databases.
    Modern distributed computing frameworks provide abstractions that allow users
    to treat a cluster of computers more like a single computer—to automatically split
    up files and distribute storage over many machines, divide work into smaller tasks
    and execute them in a distributed manner, and recover from failures. They can
    automate a lot of the hassle of working with large datasets and are far cheaper
    than HPC.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 面向高性能计算环境的工具通常未能将内存中的数据模型与较低级别的存储模型解耦。例如，许多工具只知道如何从 POSIX 文件系统中以单一流的方式读取数据，这使得工具在自然并行化或使用其他存储后端（如数据库）方面变得困难。现代分布式计算框架提供了抽象层，允许用户将计算机集群视为单个计算机——自动分割文件并将存储分布到多台机器上，将工作分解为较小的任务并以分布式方式执行，以及从故障中恢复。它们可以自动化处理大型数据集的许多麻烦，并且比高性能计算成本更低。
- en: A simple way to think about *distributed systems* is that they are a group of
    independent computers that appear to the end user as a single computer. They allow
    for horizontal scaling. That means adding more computers rather than upgrading
    a single system (vertical scaling). The latter is relatively expensive and often
    insufficient for large workloads. Distributed systems are great for scaling and
    reliability but also introduce complexity when it comes to design, construction,
    and debugging. One should understand this trade-off before opting for such a tool.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*分布式系统*的一个简单理解是，它们是一组独立的计算机，对最终用户表现为单个计算机。它们支持横向扩展，即添加更多计算机，而不是升级单个系统（纵向扩展）。后者成本相对较高，通常无法满足大工作负载。分布式系统在扩展性和可靠性方面表现出色，但在设计、构建和调试时也引入了复杂性。在选择此类工具之前，应理解这种权衡。
- en: Introducing Apache Spark and PySpark
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Apache Spark 和 PySpark
- en: Enter Apache Spark, an open source framework that combines an engine for distributing
    programs across clusters of machines with an elegant model for writing programs
    atop it. Spark originated at the University of California, Berkeley, AMPLab and
    has since been contributed to the Apache Software Foundation. When released, it
    was arguably the first open source software that made distributed programming
    truly accessible to data scientists.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 Apache Spark，这是一个开源框架，结合了跨多台机器分发程序的引擎和一个优雅的编程模型。Spark 起源于加州大学伯克利分校的 AMPLab，并已贡献给
    Apache 软件基金会。在发布时，它可以说是第一个使分布式编程真正对数据科学家可访问的开源软件。
- en: Components
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件
- en: 'Apart from the core computation engine (Spark Core), Spark is comprised of
    four main components. Spark code written by a user, using either of its APIs,
    is executed in the workers’ JVMs (Java Virtual Machines) across the cluster (see
    [Chapter 2](ch02.xhtml#introduction_to_data_anlysis_with_pyspark)). These components
    are available as distinct libraries as shown in [Figure 1-1](#fig0101):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心计算引擎（Spark Core）外，Spark 还由四个主要组件组成。用户使用其任一 API 编写的 Spark 代码在集群中的工作节点的 JVM（Java
    虚拟机）中执行（参见 [第 2 章](ch02.xhtml#introduction_to_data_anlysis_with_pyspark)）。这些组件作为独立的库提供，如图
    [1-1](#fig0101) 所示：
- en: Spark SQL and DataFrames + Datasets
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 和 DataFrames + Datasets
- en: A module for working with structured data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 用于处理结构化数据的模块。
- en: MLlib
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib
- en: A scalable machine learning library.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可扩展的机器学习库。
- en: Structured Streaming
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: This makes it easy to build scalable fault-tolerant streaming applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得构建可扩展的容错流应用程序变得容易。
- en: GraphX (legacy)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX（已过时）
- en: GraphX is Apache Spark’s library for graphs and graph-parallel computation.
    However, for graph analytics, GraphFrames is recommended instead of GraphX, which
    isn’t being actively developed as much and lacks Python bindings. [GraphFrames](https://oreil.ly/p6TYQ)
    is an open source general graph processing library that is similar to Apache Spark’s
    GraphX but uses DataFrame-based APIs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX 是 Apache Spark 的图形和图形并行计算库。然而，对于图形分析，推荐使用 GraphFrames 而不是 GraphX，因为 GraphX
    的开发不如以前活跃，并且缺乏 Python 绑定。[GraphFrames](https://oreil.ly/p6TYQ) 是一个开源的通用图形处理库，类似于
    Apache Spark 的 GraphX，但使用基于 DataFrame 的 API。
- en: '![aaps 0101](assets/aaps_0101.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0101](assets/aaps_0101.png)'
- en: Figure 1-1\. Apache Spark components
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. Apache Spark 组件
- en: PySpark
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark
- en: PySpark is Spark’s Python API. In simpler words, PySpark is a Python-based wrapper
    over the core Spark framework, which is written primarily in Scala. PySpark provides
    an intuitive programming environment for data science practitioners and offers
    the flexibility of Python with the distributed processing capabilities of Spark.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API。简单来说，PySpark 是基于核心 Spark 框架的 Python 封装，而 Spark
    框架主要是用 Scala 编写的。PySpark 为数据科学从业者提供直观的编程环境，结合了 Python 的灵活性和 Spark 的分布式处理能力。
- en: 'PySpark allows us to work across programming models. For example, a common
    pattern is to perform large-scale extract, transform, and load (ETL) workloads
    with Spark and then collect the results to a local machine followed by manipulation
    using pandas. We’ll explore such programming models as we write PySpark code in
    the upcoming chapters. Here is a code example from the official documentation
    to give you a glimpse of what’s to come:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 允许我们跨编程模型工作。例如，一个常见模式是使用 Spark 执行大规模的提取、转换和加载（ETL）工作负载，然后将结果收集到本地机器上，并使用
    pandas 进行操作。在接下来的章节中，我们将探讨这些编程模型。这里是官方文档中的一个代码示例，让你一窥即将到来的内容：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Ecosystem
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生态系统
- en: Spark is the closest thing to a Swiss Army knife that we have in the big data
    ecosystem. To top it off, it integrates well with rest of the ecosystem and is
    extensible. Spark decouples storage and compute unlike Apache Hadoop and HPC systems
    described previously. That means we can use Spark to read data stored in many
    sources—Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs,
    and more—and process it all in memory. Spark’s DataFrameReader and DataFrameWriter
    APIs can also be extended to read data from other sources, such as Apache Kafka,
    Amazon Kinesis, Azure Storage, and Amazon S3, on which it can operate. It also
    supports multiple deployment modes, ranging from local environments to Apache
    YARN and Kubernetes clusters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是大数据生态系统中最接近瑞士军刀的工具。此外，它与生态系统的其余部分集成良好，并具有可扩展性。与之前描述的 Apache Hadoop 和
    HPC 系统不同，Spark 分离了存储和计算。这意味着我们可以使用 Spark 读取存储在多个来源中的数据 — 如 Apache Hadoop、Apache
    Cassandra、Apache HBase、MongoDB、Apache Hive、关系型数据库等 — 并在内存中处理它。Spark 的 DataFrameReader
    和 DataFrameWriter API 还可以扩展到从其他来源读取数据，例如 Apache Kafka、Amazon Kinesis、Azure Storage
    和 Amazon S3 等。它还支持多种部署模式，从本地环境到 Apache YARN 和 Kubernetes 集群。
- en: There also exists a wide community around it. This has led to creation of many
    third-party packages. A community-created list of such packages can be found [here](https://oreil.ly/N8ZDf).
    Major cloud providers ([AWS EMR](https://oreil.ly/29yh1), [Azure Databricks](https://oreil.ly/RAShf),
    [GCP Dataproc](https://oreil.ly/5i5MT)) also provide third-party vendor options
    for running managed Spark workloads. In addition, there are dedicated conferences
    and local meetup groups that can be of interest for learning about interesting
    applications and best practices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它还有一个庞大的社区。这导致了许多第三方包的创建。一个由社区创建的这类包的列表可以在[这里](https://oreil.ly/N8ZDf)找到。主要的云提供商（[AWS
    EMR](https://oreil.ly/29yh1)，[Azure Databricks](https://oreil.ly/RAShf)，[GCP Dataproc](https://oreil.ly/5i5MT)）还提供了用于运行托管
    Spark 工作负载的第三方供应商选项。此外，还有专门的会议和本地聚会组，可用于了解有趣的应用和最佳实践。
- en: Spark 3.0
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 3.0
- en: In 2020, Apache Spark made its first major release since 2016 when Spark 2.0
    was released—Spark 3.0\. This series’ last edition, released in 2017, covered
    changes brought about by Spark 2.0\. Spark 3.0 does not introduce as many major
    API changes as the last major release. This release focuses on performance and
    usability improvements without introducing significant backward incompatibility.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年，Apache Spark 发布了自 2016 年发布 Spark 2.0 以来的第一个重要版本 — Spark 3.0。这个系列的上一个版本，发布于
    2017 年，涵盖了由 Spark 2.0 带来的变化。与上一个主要版本发布相比，Spark 3.0 并没有引入太多重大的 API 变更。该版本主要侧重于性能和可用性的改进，而不引入显著的向后不兼容性。
- en: The Spark SQL module has seen major performance enhancements in the form of
    adaptive query execution and dynamic partition pruning. In simpler terms, they
    allow Spark to adapt a physical execution plan during runtime and skip over data
    that’s not required in a query’s results, respectively. These optimizations address
    significant effort that users had to previously put into manual tuning and optimization.
    Spark 3.0 is almost two times faster than Spark 2.4 on TPC-DS, an industry-standard
    analytical processing benchmark. Since most Spark applications are backed by the
    SQL engine, all the higher-level libraries, including MLlib and structured streaming,
    and higher-level APIs, including SQL and DataFrames, have benefited. Compliance
    with the ANSI SQL standard makes the SQL API more usable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL模块在自适应查询执行和动态分区修剪方面获得了主要的性能增强。简单来说，它们允许Spark在运行时调整物理执行计划，并在查询结果中跳过不需要的数据。这些优化解决了用户以前需要进行手动调优和优化的重大工作量。在TPC-DS上，Spark
    3.0比Spark 2.4快近两倍，这是一个行业标准的分析处理基准。由于大多数Spark应用程序都由SQL引擎支持，所有高级库，包括MLlib和结构化流处理，以及高级API，包括SQL和DataFrames，都受益于此。符合ANSI
    SQL标准使得SQL API更加可用。
- en: Python has emerged as the leader in terms of adoption in the data science ecosystem.
    Consequently, Python is now the most widely used language on Spark. PySpark has
    more than five million monthly downloads on PyPI, the Python Package Index. Spark
    3.0 improves its functionalities and usability. pandas user-defined functions
    (UDFs) have been redesigned to support Python type hints and iterators as arguments.
    New pandas UDF types have been included, and the error handling is now more pythonic.
    Python versions below 3.6 have been deprecated. From Spark 3.2 onward, Python
    3.6 support has been deprecated too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Python在数据科学生态系统中的采用率领先。因此，Python现在是Spark上最广泛使用的语言。PySpark在Python包索引(PyPI)上每月的下载量超过500万次。Spark
    3.0改进了其功能和可用性。重设计了pandas用户定义函数（UDFs），以支持Python类型提示和迭代器作为参数。新增了新的pandas UDF类型，并且错误处理现在更符合Python的风格。Python版本低于3.6已被弃用。从Spark
    3.2开始，也已弃用了Python 3.6支持。
- en: Over the last four years, the data science ecosystem has also changed at a rapid
    pace. There is an increased focus on putting machine learning models in production.
    Deep learning has provided remarkable results and the Spark team is currently
    experimenting to allow the project’s scheduler to leverage accelerators such as
    GPUs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的四年里，数据科学生态系统也发生了快速变化。现在更加关注将机器学习模型投入生产。深度学习已经取得了显著的成果，Spark团队目前正在实验，以便项目的调度程序可以利用像GPU这样的加速器。
- en: PySpark Addresses Challenges of Data Science
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark解决了数据科学的挑战
- en: For a system that seeks to enable complex analytics on huge data to be successful,
    it needs to be informed by—or at least not conflict with—some fundamental challenges
    faced by data scientists.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个旨在实现对海量数据进行复杂分析的系统来说，要取得成功，它需要考虑到或者至少不与数据科学家面临的一些基本挑战冲突。
- en: First, the vast majority of work that goes into conducting successful analyses
    lies in preprocessing data. Data is messy, and cleansing, munging, fusing, mushing,
    and many other verbs are prerequisites to doing anything useful with it.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，成功进行分析所需的绝大部分工作都涉及数据预处理。数据是杂乱的，清洗、整理、融合、拼接以及其他许多动作都是能够对其进行实用处理的先决条件。
- en: Second, *iteration* is a fundamental part of data science. Modeling and analysis
    typically require multiple passes over the same data. Popular optimization procedures
    like stochastic gradient descent involve repeated scans over their inputs to reach
    convergence. Iteration also matters within the data scientist’s own workflow.
    Choosing the right features, picking the right algorithms, running the right significance
    tests, and finding the right hyperparameters all require experimentation.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，*迭代* 是数据科学的一个基本部分。建模和分析通常需要多次对相同数据进行遍历。流行的优化过程如随机梯度下降涉及对其输入进行重复扫描以达到收敛。迭代也在数据科学家自身的工作流中起着重要作用。选择正确的特征，选择正确的算法，运行正确的显著性测试以及找到正确的超参数都需要进行实验。
- en: Third, the task isn’t over when a well-performing model has been built. The
    point of data science is to make data useful to non–data scientists. Uses of data
    recommendation engines and real-time fraud detection systems culminate in data
    applications. In such systems, models become part of a production service and
    may need to be rebuilt periodically or even in real time.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，当构建出一个表现良好的模型时，任务并未结束。数据科学的目的是使数据对非数据科学家有用。数据推荐引擎和实时欺诈检测系统的用途最终体现在数据应用程序中。在这样的系统中，模型成为生产服务的一部分，并可能需要定期甚至实时重建。
- en: PySpark deals well with the aforementioned challenges of data science, acknowledging
    that the biggest bottleneck in building data applications is not CPU, disk, or
    network, but analyst productivity. Collapsing the full pipeline, from preprocessing
    to model evaluation, into a single programming environment can speed up development.
    By packaging an expressive programming model with a set of analytic libraries
    under an REPL (read-eval-print loop) environment, PySpark avoids the round trips
    to IDEs. The more quickly analysts can experiment with their data, the higher
    likelihood they have of doing something useful with it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark很好地处理了数据科学中提到的挑战，承认构建数据应用程序中最大的瓶颈不是CPU、磁盘或网络，而是分析师的生产力。将从预处理到模型评估的整个流程折叠到单一编程环境中可以加快开发速度。通过在REPL（读取-评估-打印循环）环境中打包表达性编程模型和一组分析库，PySpark避免了与集成开发环境的来回往返。分析师能够快速地实验他们的数据，这样他们就更有可能从中获得实用的东西。
- en: A read-eval-print loop, or REPL, is a computer environment where user inputs
    are read and evaluated, and then the results are returned to the user.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 读取-评估-打印循环，或称REPL，是一种计算机环境，其中用户输入被读取和评估，然后结果被返回给用户。
- en: PySpark’s core APIs provide a strong foundation for data transformation independent
    of any functionality in statistics, machine learning, or matrix algebra. When
    exploring and getting a feel for a dataset, data scientists can keep data in memory
    while they run queries, and easily cache transformed versions of the data as well,
    without suffering a trip to disk. As a framework that makes modeling easy but
    is also a good fit for production systems, it is a huge win for the data science
    ecosystem.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的核心API为数据转换提供了坚实的基础，独立于统计、机器学习或矩阵代数的任何功能。在探索和感知数据集的同时，数据科学家可以将数据保留在内存中运行查询，并且可以轻松地缓存转换后的数据版本，而不需要经历磁盘的来回。作为一个既使建模变得简单又非常适合生产系统的框架，对于数据科学生态系统来说，这是一个巨大的胜利。
- en: Where to Go from Here
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来的去向
- en: Spark spans the gap between systems designed for exploratory analytics and systems
    designed for operational analytics. It is often said that a data scientist is
    someone who is better at engineering than most statisticians and better at statistics
    than most engineers. At the very least, Spark is better at being an operational
    system than most exploratory systems and better for data exploration than the
    technologies commonly used in operational systems. We hope that this chapter was
    helpful and you are now excited about getting hands-on with PySpark. That’s what
    we will do from the next chapter onward!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark跨越了为探索性分析设计的系统和为运营分析设计的系统之间的鸿沟。据说，数据科学家在工程方面比大多数统计学家更擅长，在统计方面比大多数工程师更擅长。至少，Spark在作为运营系统方面比大多数探索系统更出色，在数据探索方面也比运营系统中常用的技术更好。希望本章对您有所帮助，并且您现在对开始使用PySpark感到兴奋。从下一章开始我们将进入实际操作！
