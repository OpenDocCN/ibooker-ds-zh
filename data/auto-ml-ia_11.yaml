- en: 8 Scaling up AutoML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 扩展AutoML
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Loading large datasets into memory batch by batch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分批将大型数据集加载到内存中
- en: Using multiple GPUs to speed up search and training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个GPU加速搜索和训练
- en: Using Hyperband to efficiently schedule model training to make the best use
    of the available computing resources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hyperband高效调度模型训练，以充分利用可用的计算资源
- en: Using pretrained models and warm-start to accelerate the search process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型和warm-start加速搜索过程
- en: This chapter introduces various techniques for large-scale training—for example,
    using large datasets to train large models on multiple GPUs. For datasets that
    are too big to fit into memory all at once, we’ll show you how to load them batch
    by batch during the training. We’ll also introduce different parallelization strategies
    to distribute the training and search processes onto multiple GPUs. In addition,
    we’ll show you some strategies to accelerate the search process with limited computing
    resources, using advanced search algorithms and search spaces.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了各种大规模训练技术——例如，使用大型数据集在多个GPU上训练大型模型。对于一次性无法全部装入内存的大型数据集，我们将向您展示如何在训练过程中分批加载它们。我们还将介绍不同的并行化策略，以将训练和搜索过程分布到多个GPU上。此外，我们还将向您展示一些策略，利用高级搜索算法和搜索空间，在有限的计算资源下加速搜索过程。
- en: 8.1 Handling large-scale datasets
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 处理大规模数据集
- en: One of the most important factors behind the power of deep learning is the availability
    of large amounts of data to train the models. Usually, the larger and more diverse
    the dataset is, the better the performance of the trained model is. All the datasets
    we have used in the earlier examples in this book were small enough to fit into
    the main memory of the machine on which the training was done. However, you may
    not have—or need—enough memory to hold the entire dataset. If you use GPUs, the
    dataset will be chopped into small batches, which are loaded into the GPU batch
    by batch. This means you need only one slot in the GPU’s memory for a single batch
    of data, which will be overwritten when you load a new batch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习强大背后的一个重要因素是大量数据的可用性，用于训练模型。通常，数据集越大、越多样化，训练的模型性能越好。本书早期示例中使用的所有数据集都足够小，可以装入进行训练的机器的主内存中。然而，您可能没有——或者不需要——足够的内存来存储整个数据集。如果您使用GPU，数据集将被分割成小批次，这些批次将分批加载到GPU中。这意味着您只需要GPU内存中的一个槽位来存储单个数据批次，当您加载新批次时，它将被覆盖。
- en: So, if you want to use a larger dataset that doesn’t fit into the main memory
    of the machine you’re using, instead of attempting to load the entire dataset
    at once, you can load one or more batches at a time, overwriting the previous
    batches. The batches can then be loaded into GPU memory during training as usual.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想使用一个不适合您所使用机器主内存的大型数据集，而不是一次性尝试加载整个数据集，您可以一次加载一个或多个批次，覆盖之前的批次。然后，这些批次可以在训练过程中像往常一样加载到GPU内存中。
- en: In summary, we have two ways of loading data. In both cases, the GPU holds a
    buffer for batches of data to be consumed by the machine learning model. In the
    first case, we load the entire dataset into the main memory and then load the
    data into the GPU’s memory batch by batch. In the second case, the main memory
    is also used as a buffer to load the data in batches from the hard drive, where
    the entire dataset is held. The comparison between the two ways of loading data
    is shown in figure 8.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们有两种加载数据的方式。在两种情况下，GPU都保留一个缓冲区，用于存储将被机器学习模型消耗的数据批次。在第一种情况下，我们将整个数据集加载到主内存中，然后分批将数据加载到GPU的内存中。在第二种情况下，主内存也用作缓冲区，从硬盘上分批加载数据，整个数据集都存储在硬盘中。两种加载数据方式的比较如图8.1所示。
- en: '![08-01](../Images/08-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![08-01](../Images/08-01.png)'
- en: Figure 8.1 Different ways of loading data
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 不同加载数据的方式
- en: In this section, we’ll show you how to use the second option to load large datasets
    that do not fit into the main memory to search for a good model. We will first
    introduce loading image- and text-classification data, which is easy to do with
    existing data-loading APIs in AutoKeras. Then, we will show you how to load any
    dataset from the disk in batches.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何使用第二种选项来加载不适合主内存的大型数据集，以搜索一个好的模型。首先，我们将介绍加载图像和文本分类数据，这可以通过AutoKeras中现有的数据加载API轻松完成。然后，我们将向您展示如何分批从磁盘加载任何数据集。
- en: 8.1.1 Loading an image-classification dataset
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 加载图像分类数据集
- en: 'There is a convenient function in AutoKeras that can help you load image-classification
    data from the disk. Here, we’ll use the MNIST dataset as an example. To download
    and extract the dataset, you’ll need to run the commands in the next code sample.
    You can run them directly in a Python notebook, or run them in your local Linux
    or Unix command-line terminal without the ! symbol. First, we download the compressed
    file of the MNIST dataset with the wget command, which stores the file at the
    given URL into the current directory. Then, we extract the files from the compressed
    file with the tar command. xzf is the most commonly used configuration for extracting
    files, where x means extract, z means to filter the archive content through gzip,
    and f means to read the content from a file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AutoKeras 中有一个方便的函数可以帮助您从磁盘加载图像分类数据。在这里，我们将使用 MNIST 数据集作为示例。要下载和提取数据集，您需要运行下一个代码示例中的命令。您可以直接在
    Python 笔记本中运行它们，或者在您的本地 Linux 或 Unix 命令行终端中运行它们，无需使用 ! 符号。首先，我们使用 wget 命令下载 MNIST
    数据集的压缩文件，该文件将给定的 URL 中的文件存储到当前目录中。然后，我们使用 tar 命令从压缩文件中提取文件。xzf 是提取文件最常用的配置，其中
    x 表示提取，z 表示通过 gzip 过滤存档内容，f 表示从文件中读取内容：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After extraction, we can see the images are grouped by their classes into different
    folders. The train and test directories each contain 10 subdirectories, named
    0 to 9, which are the labels of the images, as shown here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提取后，我们可以看到图像根据它们的类别分组到不同的文件夹中。训练和测试目录各包含 10 个子目录，命名为 0 到 9，这些是图像的标签，如下所示：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can now use the built-in AutoKeras function image_dataset_from_directory()
    to load the image dataset. This function returns a tf.data.Dataset object containing
    the data. The first argument to the function is the path to the data directory,
    which is 'test' or 'train' in our case. Use image_size to specify the image size
    as a tuple of two integers, (height, width), and use batch_size to specify the
    batch size of the dataset. These are the required arguments for the function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用内置的 AutoKeras 函数 image_dataset_from_directory() 来加载图像数据集。此函数返回一个包含数据的
    tf.data.Dataset 对象。函数的第一个参数是数据目录的路径，在我们的例子中是 'test' 或 'train'。使用 image_size 指定图像大小为一个包含两个整数的元组，(高度，宽度)，并使用
    batch_size 指定数据集的批次大小。这些是函数的必需参数。
- en: Calling this function yields a batch of tuples of images and labels. The first
    element in each tuple is the shape (batch_size, image_height, image_width, number_of_channels).
    We set the number of channels using the color_mode argument, which can be one
    of 'grayscale', 'rgb', or 'rgba'. The corresponding numbers of channels are 1,
    3, and 4; the MNIST dataset consists of grayscale images, so the number of channels
    is 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此函数会生成一个包含图像和标签的元组批次的批次。每个元组的第一个元素是形状 (batch_size, image_height, image_width,
    number_of_channels)。我们使用 color_mode 参数设置通道数，它可以是 'grayscale'、'rgb' 或 'rgba' 之一。相应的通道数是
    1、3 和 4；MNIST 数据集由灰度图像组成，因此通道数为 1。
- en: The second element of the tuple is the label. The labels are strings, which
    are the same as the directory names in the test or train directory. In our case,
    they are numbers from '0' to '9'. The image_dataset_from_directory() function
    also takes a shuffle argument, which is True by default, meaning it will shuffle
    the images from different classes in different directories. To set the random
    seed for shuffling, you can use the seed argument.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 元组的第二个元素是标签。标签是字符串，与测试或训练目录中的目录名相同。在我们的例子中，它们是从 '0' 到 '9' 的数字。image_dataset_from_directory()
    函数还接受一个 shuffle 参数，默认值为 True，这意味着它将不同目录中的不同类别的图像进行洗牌。要设置洗牌的随机种子，您可以使用 seed 参数。
- en: We’ll start by loading the testing data, as shown in the following listing.
    This doesn’t require splitting the data, which means it’s easier than loading
    the training data. We’ll also print out some specs about the first batch of the
    loaded data—this is a useful debugging method when loading datasets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载测试数据，如下所示列表。这不需要分割数据，这意味着它比加载训练数据更容易。我们还将打印出有关加载的第一批数据的规格——这是加载数据集时的一种有用的调试方法。
- en: Listing 8.1 Loading testing data from disk
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 从磁盘加载测试数据
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Path to the testing dataset
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 测试数据集的路径
- en: ❷ Returns a new dataset with only the first batch
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回只包含第一批数据的新数据集
- en: 'As the output shows, this lists 10,000 images with 10 different class labels.
    The shape of one batch of loaded images is (32, 28, 28, 1), with a type of float32.
    The shape of the corresponding labels is (32,), with a type of string, as shown
    next:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，这列出了具有 10 个不同类别标签的 10,000 张图像。加载的一批图像的形状为 (32, 28, 28, 1)，类型为 float32。相应的标签形状为
    (32,), 类型为字符串，如下所示：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The dataset has been successfully loaded from the disk into a tf.data.Dataset
    object. However, it is not directly usable for AutoML because we will need at
    least two tf.data.Dataset objects for training and validation, respectively. Therefore,
    we need an efficient way to split the dataset into different subsets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已成功从磁盘加载到 tf.data.Dataset 对象中。然而，它不能直接用于 AutoML，因为我们至少需要两个 tf.data.Dataset
    对象分别用于训练和验证。因此，我们需要一种有效的方法将数据集分割成不同的子集。
- en: 8.1.2 Splitting the loaded dataset
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 加载数据集的分割
- en: 'We have multiple ways to split a dataset. Listing 8.2 shows a simple but inefficient
    solution: it loads the entire set of training data and splits it with the take()
    and skip() functions. If you call dataset.take(*n*), it will return a dataset
    consisting of the first *n* batches. If you call dataset.skip(*n*), it will return
    a dataset consisting of the rest of the batches after the first *n* batches.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法可以分割数据集。列表 8.2 展示了一种简单但效率低下的解决方案：它加载了整个训练数据集，并使用 take() 和 skip() 函数进行分割。如果你调用
    dataset.take(*n*)，它将返回一个包含前 *n* 批次的数据集。如果你调用 dataset.skip(*n*)，它将返回一个包含第一 *n*
    批次之后的所有批次的集合。
- en: Listing 8.2 Loading and splitting the training data
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 加载和分割训练数据
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is inefficient because the skip() function has to go through the first
    *n* batches before it can start to iterate. We propose a more efficient solution
    in listing 8.3\. Here, we call the image_dataset_from_directory() function twice,
    to get the training and validation sets separately. In each call, everything remains
    the same as when we loaded the testing set except for two new arguments, validation_split
    and subset, that control the splitting of the dataset. The validation_split argument
    specifies the percentage of the batches to include in the validation set, which
    should be between 0 and 1. The rest of the batches will be included in the training
    set. The subset argument should be either 'training' or 'validation', to indicate
    which set the function should return. Notably, the seed argument has to be set
    manually here to ensure that in the two calls to the function, there is no difference
    in how the data is split.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这效率低下是因为 skip() 函数必须遍历前 *n* 批次，才能开始迭代。我们提出了一种更有效的解决方案，如列表 8.3 所示。在这里，我们两次调用
    image_dataset_from_directory() 函数，分别获取训练集和验证集。在每次调用中，除了两个新的参数 validation_split
    和 subset，控制数据集的分割外，其他一切保持与加载测试集时相同。validation_split 参数指定要包含在验证集中的批次百分比，应在 0 和
    1 之间。其余的批次将包含在训练集中。subset 参数应该是 'training' 或 'validation'，以指示函数应返回哪个集合。值得注意的是，seed
    参数必须手动设置，以确保在两次函数调用中，数据的分割方式没有差异。
- en: Listing 8.3 Loading training and validation data from disk
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 从磁盘加载训练和验证数据
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Loads the training split
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载训练分割
- en: ❷ Path to the training data directory
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据目录的路径
- en: ❸ Loads the validation split
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 加载验证分割
- en: ❹ Path to the training data directory
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练数据目录的路径
- en: 'We’ve finished the data-loading specification part: all the data is wrapped
    into the format of a tf.data.Dataset. Next, we’ll look at an easy technique that
    we can use to further improve the efficiency of using the loaded dataset, called
    *prefetching*.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了数据加载指定部分：所有数据都被封装成 tf.data.Dataset 格式。接下来，我们将探讨一种可以进一步提高加载后数据集使用效率的简单技术，称为
    *预取*。
- en: Improving data loading efficiency with prefetching
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预取提高数据加载效率
- en: Due to read inefficiency, loading data from the disk can take some time. Without
    prefetching, the program would start loading the next batch of data from the disk
    into memory only when all the batches currently in memory have been used up by
    the machine learning model. The model training or inference process would then
    be suspended to wait for the next batch of data to load. Clearly, this approach
    is not an efficient one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于读取效率低下，从磁盘加载数据可能需要一些时间。如果没有预取，程序只有在内存中的所有批次都被机器学习模型使用完毕后，才会开始从磁盘加载下一批数据到内存中。然后，模型训练或推理过程将暂停，等待下一批数据加载。显然，这种方法效率不高。
- en: Prefetching loads additional batches of data from the disk into memory in advance,
    in parallel with training or inference, so this process is not suspended. The
    difference between the two approaches is shown with sequence diagrams in figure
    8.2\. There are two tasks involved, training and loading. The lines from the top
    to the bottom show which task is running at a given time during the execution.
    As you can see, without prefetching, the loading and training tasks never run
    together; the training process needs to wait for the loading process to finish.
    With prefetching, on the other hand, they can run in parallel to save some time,
    so the program runs more efficiently.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 预取会在训练或推理并行进行的同时，提前从磁盘加载额外的数据批次到内存中，因此这个过程不会被暂停。两种方法之间的区别在图 8.2 的序列图中展示。涉及两个任务，即训练和加载。从上到下的线条显示了执行过程中某一时刻正在运行的任务。正如你所见，没有预取时，加载和训练任务永远不会同时运行；训练过程需要等待加载过程完成。另一方面，使用预取，它们可以并行运行以节省时间，从而使程序运行得更高效。
- en: '![08-02](../Images/08-02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![08-02](../Images/08-02.png)'
- en: Figure 8.2 Prefetching sequence diagram
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 预取序列图
- en: 'To enable prefetching for a tf.data.Dataset, you can call its member function
    prefetch(). The only required argument is the number of batches you want it to
    prefetch into memory: for example, calling dataset.prefetch(5) loads five batches
    into memory in advance. If you are not sure how many batches would be ideal for
    prefetching, you can use dataset.prefetch(tf.data.AUTOTUNE), which will tune this
    number for you automatically.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 tf.data.Dataset 启用预取功能，你可以调用其成员函数 prefetch()。唯一必需的参数是你希望它预取到内存中的批次数：例如，调用
    dataset.prefetch(5) 会预先将五个批次加载到内存中。如果你不确定预取多少批次是理想的，可以使用 dataset.prefetch(tf.data.AUTOTUNE)，这将自动为你调整这个数字。
- en: The code for using prefetching for training, validation, and testing is shown
    in listing 8.4\. Here, we prefetch five batches for the training and validation
    sets and automatically tune the number of batches for the testing data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预取进行训练、验证和测试的代码在列表 8.4 中展示。在这里，我们为训练集和验证集预取五个批次，并自动调整测试数据的批次数。
- en: Listing 8.4 Improving loading efficiency with prefetching
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 使用预取提高加载效率
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s try to train a simple image-classification model with this data.
    The code is shown in the next listing. As you can see, there’s no difference from
    using a smaller dataset that we’re able to load into memory all at once: we pass
    the training and validation sets directly to the fit() function.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用这些数据训练一个简单的图像分类模型。代码将在下一列表中展示。正如你所见，与使用能够一次性加载到内存中的较小数据集没有区别：我们直接将训练集和验证集传递给
    fit() 函数。
- en: Listing 8.5 Fitting an image classifier with data loaded from disk
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 从磁盘加载数据拟合图像分类器
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You now know how to load a large image dataset from the disk. However, we could
    encounter other types of large dataset, such as text datasets, which are also
    too big to fit into the main memory in many cases. We’ll look at how to load a
    large text dataset from the disk next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道如何从磁盘加载大型图像数据集。然而，我们可能会遇到其他类型的大型数据集，例如文本数据集，在许多情况下这些数据集也太大，无法放入主内存。接下来，我们将探讨如何从磁盘加载大型文本数据集。
- en: 8.1.3 Loading a text-classification dataset
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 加载文本分类数据集
- en: Loading a large text-classification dataset is not much different from loading
    a large image-classification dataset. Again, you can use a built-in function from
    AutoKeras named text_dataset_from_directory(); the only difference in usage is
    the arguments.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 加载大型文本分类数据集与加载大型图像分类数据集没有太大区别。同样，你可以使用 AutoKeras 中的一个内置函数 text_dataset_from_directory()；在用法上唯一的区别是参数。
- en: The text_dataset_from_directory() function doesn’t have image_size and color_mode
    arguments, because those are related only to image data. Instead, it takes a new
    argument named max_length, which is the maximum number of characters in the string
    to keep for each text instance. If you leave max_length unspecified, the strings
    are preserved at their original length before being fed into the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: text_dataset_from_directory() 函数没有 image_size 和 color_mode 参数，因为它们仅与图像数据相关。相反，它接受一个名为
    max_length 的新参数，这是每个文本实例中要保留的字符串的最大字符数。如果你没有指定 max_length，字符串将在被输入到模型之前保留其原始长度。
- en: 'Let’s use the IMDb movie review dataset as an example to show how to load text
    data from the disk. First, we download the raw text files using the following
    commands. These two commands will download the dataset as a compressed file and
    extract the files into the current directory. As with downloading the image dataset,
    we can run the commands directly in a notebook, or we can run them locally in
    a Linux or Unix terminal without the ! symbol:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以IMDb电影评论数据集为例，展示如何从磁盘加载文本数据。首先，我们使用以下命令下载原始文本文件。这两个命令将数据集作为压缩文件下载，并将文件提取到当前目录中。与下载图像数据集一样，我们可以在notebook中直接运行这些命令，或者在没有!符号的情况下在Linux或Unix终端本地运行它们：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After extraction, you’ll have a directory named imdb with the following content.
    As you can see, the reviews are divided into training and testing datasets, each
    of which is organized by the two classes of reviews, positive and negative:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 提取后，您将得到一个名为imdb的目录，其中包含以下内容。如您所见，评论被分为训练集和测试集，每个集都按评论的两个类别组织，即正面和负面：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, we can load the text data in the same way we loaded the image data. We
    load the data from the train directory and split it into a training set and a
    validation set with text_dataset_from_directory(). We also use prefetching to
    speed up the iteration process, with max_length=1000 to limit the maximum length
    of the loaded strings. Any characters over the 1,000-character limit will be discarded.
    As you can see in the following listing, when loading the testing data, we can
    omit several of the arguments because they are important only for training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以以加载图像数据相同的方式加载文本数据。我们使用text_dataset_from_directory()从train目录加载数据，并将其分为训练集和验证集。我们还使用预取功能来加快迭代过程，max_length=1000限制加载字符串的最大长度。任何超过1,000个字符限制的字符将被丢弃。如下所示，在加载测试数据时，我们可以省略几个参数，因为它们仅对训练数据重要。
- en: Listing 8.6 Loading training, validation, and testing data from disk
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 从磁盘加载训练、验证和测试数据
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Loads the training set
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载训练集
- en: ❷ Loads the validation set
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载验证集
- en: ❸ Loads the testing set
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 加载测试集
- en: After loading the data, we can now use a simple text classifier to test if the
    loaded data works as intended, as shown next.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据后，我们现在可以使用一个简单的文本分类器来测试加载数据是否按预期工作，如下所示。
- en: Listing 8.7 Fittting a text classifier with data loaded from disk
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 从磁盘加载数据拟合文本分类器
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So far, we have shown you how to load image- and text-classification data from
    the disk with AutoKeras’s built-in functions and how to speed up the iteration
    of the dataset with prefetching. However, we need a more general way to load datasets
    of any data type, not just images and text.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经向您展示了如何使用AutoKeras的内置函数从磁盘加载图像和文本分类数据，以及如何使用预取功能加快数据集的迭代速度。然而，我们需要一种更通用的方式来加载任何类型的数据集，而不仅仅是图像和文本。
- en: 8.1.4 Handling large datasets in general
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 一般处理大型数据集
- en: In this section, we will introduce a way to load arbitrary datasets that are
    too large to fit into the main memory using an inner mechanism of tf.data.Dataset.
    We will continue to use this format to load the dataset to solve the memory issue.
    However, to enable more flexibility in the data types, we will use a *Python generator*
    to iterate through the data and convert the generator to a tf.data.Dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一种使用tf.data.Dataset的内部机制来加载太大而无法放入主内存的任意数据集的方法。我们将继续使用此格式来加载数据集以解决内存问题。然而，为了在数据类型上提供更多灵活性，我们将使用*Python生成器*来遍历数据，并将生成器转换为tf.data.Dataset。
- en: First, what is a Python generator? Conceptually, it’s an iterator for a sequence
    of data. For implementation purposes, it’s a Python function that uses yield to
    provide the data to be iterated through. You can use a for loop to iterate over
    the function to get all the data, as shown in the following listing. Here, we
    define a Python generator called generator(). It’s just a Python function that
    calls yield to provide all the elements in data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，什么是Python生成器？从概念上讲，它是一个数据序列的迭代器。为了实现目的，它是一个使用yield提供要迭代的数据的Python函数。您可以使用for循环遍历该函数以获取所有数据，如下所示。在这里，我们定义了一个名为generator()的Python生成器。它只是一个Python函数，它调用yield来提供数据列表中的所有元素。
- en: Listing 8.8 Python generator example
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8 Python生成器示例
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Defines the generator
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义生成器
- en: ❷ Iterates over the generator using a for loop and prints its elements
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用for循环遍历生成器并打印其元素
- en: 'The output of the generator, which follows, is the elements in the data list
    in listing 8.8:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的输出，如下所示，是列表8.8中的数据列表元素：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The tf.data.Dataset class has a function named from_generator that constructs
    a new instance using a Python generator. To use this, we just provide the generator
    function and specify the data type with the output_types argument. Let’s try to
    convert the Python generator we just built to a tf.data.Dataset instance. The
    code is shown in the next listing. The output will be the same as in the previous
    example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset 类有一个名为 from_generator 的函数，它使用 Python 生成器构建一个新的实例。要使用它，我们只需提供生成器函数并使用
    output_types 参数指定数据类型。让我们尝试将我们刚刚构建的 Python 生成器转换为 tf.data.Dataset 实例。代码将在下一个列表中展示。输出将与上一个示例相同。
- en: Listing 8.9 Converting the Python generator to a tf.data.Dataset
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 将 Python 生成器转换为 tf.data.Dataset
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Specifies the generator function
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定生成器函数
- en: ❷ Specifies the output type as integer
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定输出类型为整数
- en: Now that you know what a Python generator is and how to use one to build a tf.data.Dataset
    instance, let’s try to load a real dataset with this method.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 Python 生成器是什么以及如何使用它来构建 tf.data.Dataset 实例，让我们尝试使用这种方法加载一个真实的数据集。
- en: Loading a dataset with a Python generator
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 生成器加载数据集
- en: 'Now we’ll load the IMDb dataset using a Python generator using the following
    steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用以下步骤使用 Python 生成器加载 IMDb 数据集：
- en: Load all the filepaths and labels into a NumPy array. Shuffle the array to mix
    the data from different directories.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有文件路径和标签加载到 NumPy 数组中。对数组进行洗牌以混合来自不同目录的数据。
- en: Build a generator from the shuffled NumPy array to convert the filepaths to
    the actual text data by reading the contents of the files.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从洗牌后的 NumPy 数组构建一个生成器，通过读取文件的正文内容将文件路径转换为实际文本数据。
- en: Create the tf.data.Dataset instance from the generator.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从生成器创建 tf.data.Dataset 实例。
- en: 'In the first step, we iterate over all the files in the different classes (directories).
    We create tuples of two elements: the path to the file and the label of the file.
    The path to the file is needed to load its contents later, during training. The
    label information is indicated by the directory containing the file, so we need
    to record this information during the iteration of the directories.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们遍历不同类别的所有文件（目录）。我们创建两个元素的元组：文件的路径和文件的标签。文件路径是后来在训练期间加载其内容所需的，标签信息由包含文件的目录指示，因此我们需要在遍历目录的过程中记录此信息。
- en: However, because we iterated through all the files directory by directory, all
    the files in the same directory (i.e., with the same class label) are next to
    each other in the array. So, we need to shuffle them to mix the data from different
    classes before using the files for training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为我们按目录逐个迭代所有文件，所以同一目录（即具有相同类标签）的所有文件在数组中相邻。因此，在使用文件进行训练之前，我们需要对它们进行洗牌以混合来自不同类别的数据。
- en: The reason we create a NumPy array first instead of creating the generator directly
    to iterate the files is that it’s easier to do the shuffling and the split once
    the filepaths and labels have been loaded into memory. In addition, the filepaths
    won’t take up much space in memory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个 NumPy 数组而不是直接创建生成器来迭代文件的原因是，一旦文件路径和标签被加载到内存中，进行洗牌和分割就更容易了。此外，文件路径在内存中不会占用太多空间。
- en: The code to perform this process is shown in the following listing. The load_data()
    function loads, shuffles, and returns the data as a NumPy array.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此过程的代码如下所示。load_data() 函数加载、洗牌并返回作为 NumPy 数组的数据。
- en: Listing 8.10 Loading IMDb dataset filenames
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.10 加载 IMDb 数据集文件名
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Enumerates the class labels
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 枚举类标签
- en: ❷ Iterates all the filenames in each of the classes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历每个类别的所有文件名
- en: ❸ Creates tuples of (file_path, class_label)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建 (文件路径，类标签) 的元组
- en: ❹ Shuffles the data
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 洗牌数据
- en: ❺ Loads the training data
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 加载训练数据
- en: With this NumPy array, we can create an iterator to convert each element in
    the array to the actual text data. Instead of directly implementing the generator,
    we implemented a function named get_generator(data) to return a generator because
    we may need different generators for the training, validation, and testing sets.
    With get_ generator(data) we can pass the corresponding NumPy array to the function
    to dynamically create a generator for that NumPy array. Then, we can create the
    generator function as an inner function and return this function as the return
    value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个NumPy数组，我们可以创建一个迭代器，将数组中的每个元素转换为实际的文本数据。我们不是直接实现生成器，而是实现了一个名为get_generator(data)的函数来返回一个生成器，因为我们可能需要为训练、验证和测试集使用不同的生成器。通过get_generator(data)，我们可以将相应的NumPy数组传递给函数，以动态地为该NumPy数组创建生成器。然后，我们可以创建生成器函数作为内部函数，并将此函数作为返回值返回。
- en: In the generator function, we’ll use a for loop to iterate the filepaths, read
    the contents of each file, and yield the text and label together. In this way,
    the generator will generate the actual text data with the labels ready to be used
    for training. The code for get_generator() is shown in the following listing.
    The data_generator() function will be returned as the generator function.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器函数中，我们将使用for循环遍历文件路径，读取每个文件的内容，并一起产生文本和标签。这样，生成器将生成实际文本数据，并带有标签，以便用于训练。get_generator()的代码如下所示。data_generator()函数将作为生成器函数返回。
- en: Listing 8.11 Loading the IMDb dataset with a Python generator
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.11：使用Python生成器加载IMDb数据集
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The generator function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成器函数
- en: ❷ Iterates the NumPy array
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历NumPy数组
- en: ❸ Reads the file contents using the filepaths
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用文件路径读取文件内容
- en: ❹ Yields the text and class labels
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 产生文本和类别标签
- en: ❺ Returns the generator function
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回生成器函数
- en: The next step is to create a tf.data.Dataset instance. For the convenience of
    splitting the data, let’s write a function, np_to_dataset(), to convert a NumPy
    array to a tf.data.Dataset. This function will call the get_generator() function
    to get the generator function and use tf.data.Dataset.from_generator() to get
    the tf.data .Dataset instance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个tf.data.Dataset实例。为了方便分割数据，让我们编写一个函数，np_to_dataset()，将NumPy数组转换为tf.data.Dataset。这个函数将调用get_generator()函数来获取生成器函数，并使用tf.data.Dataset.from_generator()来获取tf.data.Dataset实例。
- en: We split the NumPy array and call the np_to_dataset() function twice, for the
    training and validation sets. These contain 20,000 and 5,000 instances, respectively.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将NumPy数组分割两次，并调用np_to_dataset()函数，分别用于训练集和验证集。这些包含20,000和5,000个实例。
- en: Several things need attention while creating the dataset. In the from_generator()
    function, we need to specify the output_types. Because both the text and the label
    are of type string, we can just specify the type as tf.string.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建数据集时需要注意几个问题。在from_generator()函数中，我们需要指定output_types。因为文本和标签都是字符串类型，所以我们可以直接指定类型为tf.string。
- en: Using the dataset with AutoKeras requires it to have a concrete shape. Therefore,
    we specify the shape of the dataset with the output_shapes argument. The value
    of this argument needs to be an instance of tf.TensorShape. We can create it easily
    by passing the shape as a list to its initializer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AutoKeras的数据集需要具有具体的形状。因此，我们使用output_shapes参数指定数据集的形状。此参数的值需要是tf.TensorShape的实例。我们可以通过将其初始化器传递形状列表来轻松创建它。
- en: The created dataset is not directly usable because each instance in it is one
    tensor with shape (2,). However, the dataset used by AutoKeras should be tuples
    of two tensors, (x, y). Therefore, we need to call the map() function of the dataset
    to change its format.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的数据集不能直接使用，因为其中的每个实例都是一个形状为(2,)的张量。然而，AutoKeras使用的数据集应该是两个张量的元组，(x, y)。因此，我们需要调用数据集的map()函数来更改其格式。
- en: The map() function accepts as its argument a lambda function, which takes the
    old instance as an argument and returns the new instance. We can then return the
    first and second dimensions of the original tensor as a tuple of two elements.
    The code for these steps follows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: map()函数接受一个lambda函数作为其参数，该函数接受旧实例作为参数并返回新实例。然后我们可以返回原始张量的第一和第二维度作为一个包含两个元素的元组。这些步骤的代码如下。
- en: Listing 8.12 Creating a dataset from a generator
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.12：从生成器创建数据集
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The function to convert the NumPy array to a tf.data.Dataset
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将NumPy数组转换为tf.data.Dataset的功能
- en: ❷ The generator function for the array
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数组的生成器函数
- en: ❸ Converts the data from tensor to tuple
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据从张量转换为元组
- en: ❹ Batches and sets prefetching for the dataset
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为数据集进行批量和集合预取
- en: ❺ Loads the training set
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 加载训练集
- en: ❻ Loads the validation set
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 加载验证集
- en: With all these procedures implemented as functions, we can load the testing
    set in the same way, as shown in the next code listing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些过程实现为函数后，我们可以以相同的方式加载测试集，如下面的代码示例所示。
- en: Listing 8.13 Loading the testing set of the IMDb dataset with a Python generator
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.13 使用 Python 生成器加载 IMDb 数据集的测试集
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With the training set, validation set, and testing set ready in tf.data.Dataset
    format, we can then test whether it runs as intended with the AutoKeras text classifier,
    as shown next.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好训练集、验证集和测试集，并以 tf.data.Dataset 格式存储后，我们可以使用 AutoKeras 文本分类器来测试它是否按预期运行，如下所示。
- en: Listing 8.14 Fitting the text classifier with data from the Python generator
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.14 使用 Python 生成器拟合文本分类器
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The primary purpose of this example was to show how to construct a dataset using
    a Python generator. Now you know how to use this method to load any dataset in
    any format into a tf.data.Dataset, which greatly improves your flexibility for
    loading a large dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的主要目的是展示如何使用 Python 生成器构建数据集。现在你知道如何使用这种方法将任何格式的数据集加载到 tf.data.Dataset 中，这大大提高了你加载大型数据集的灵活性。
- en: 'Furthermore, this approach isn’t limited to loading data from the disk: if
    your data comes from a remote machine over the network or is dynamically generated
    with some Python code, you can also use a Python generator to wrap it into a dataset.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种方法不仅限于从磁盘加载数据：如果你的数据来自通过网络从远程机器获取或通过一些 Python 代码动态生成，你也可以使用 Python 生成器将其包装成数据集。
- en: 8.2 Parallelization on multiple GPUs
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 多 GPU 并行化
- en: To scale up machine learning and AutoML for large models and large datasets,
    we can run our programs on multiple GPUs and multiple machines in parallel. Parallelization
    is typically used either to accelerate training and inference (*data parallelism*)
    or to load a very large model that does not fit into the memory of a single GPU
    (*model parallelism*). It’s also sometimes used to accelerate the hyperparameter
    tuning process (*parallel tuning*). Figure 8.3 illustrates these three different
    types of parallelization and how the memory allocation for the datasets and models
    differs in each.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展机器学习和 AutoML 以支持大型模型和大型数据集，我们可以在多个 GPU 和多台机器上并行运行我们的程序。并行化通常用于加速训练和推理（*数据并行*）或加载一个非常大的模型，该模型无法适应单个
    GPU 的内存（*模型并行*）。它有时也用于加速超参数调整过程（*并行调整*）。图 8.3 展示了这三种不同类型的并行化以及数据集和模型在每个情况下的内存分配差异。
- en: This figure shows what each of the strategies looks like with three GPUs. On
    the left is the data parallelism approach, which accelerates the training process
    for large datasets. Each of the three GPUs has a copy of the same model but works
    on different data batches. The weight updates on the different GPUs are synchronized
    periodically.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了三种策略在三个 GPU 上的样子。在左侧是数据并行方法，它加速了大型数据集的训练过程。每个 GPU 都有相同模型的副本，但处理不同的数据批次。不同
    GPU 上的权重更新定期同步。
- en: In the middle of the figure is an example of the model parallelism strategy,
    which is mainly used for large models that cannot be contained in a single GPU’s
    memory or for accelerating models whose inference processes can be parallelized.
    It breaks the model into pieces and allocates them to different GPUs. In figure
    8.3, the first GPU holds the first two layers of the model and the training data.
    The second and the third GPUs hold the rest of the layers and the intermediate
    outputs of the layers. During the inference process, some pieces of the model
    may run in parallel to save time.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图中中间部分是模型并行策略的示例，该策略主要用于无法包含在单个 GPU 内存中的大型模型或加速可以并行化的推理过程的模型。它将模型分解成多个部分，并将它们分配到不同的
    GPU 上。在图 8.3 中，第一个 GPU 持有模型的前两层和训练数据。第二和第三个 GPU 持有其余的层和层的中间输出。在推理过程中，模型的一些部分可能并行运行以节省时间。
- en: '![08-03](../Images/08-03.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![08-03](../Images/08-03.png)'
- en: Figure 8.3 Three types of parallelism
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 三种并行化类型
- en: The parallel tuning strategy, which is used to accelerate the AutoML process,
    is depicted on the right. With this approach, models with different hyperparameter
    settings are placed on different GPUs, and the same dataset is used to train them.
    Therefore, the hyperparameter tuning process runs in parallel.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 用于加速 AutoML 进程的并行调优策略如图所示。采用这种方法，具有不同超参数设置的模型被放置在不同的 GPU 上，并使用相同的训练数据集来训练它们。因此，超参数调优过程是并行运行的。
- en: Let’s look at each of these strategies a little more closely.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些策略。
- en: 8.2.1 Data parallelism
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 数据并行
- en: With TensorFlow, data parallelism is managed using tf.distribute.Strategy, whose
    subclasses, such as tf.distribute.MirroredStrategy, implement different parallelism
    strategies. You can use these subclasses directly with AutoKeras and KerasTuner.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow，数据并行是通过 tf.distribute.Strategy 管理的，其子类，如 tf.distribute.MirroredStrategy，实现了不同的并行策略。你可以直接使用这些子类与
    AutoKeras 和 KerasTuner 一起使用。
- en: In AutoKeras, the AutoModel class and all the task API classes (like ImageClassifier
    and TextClassifier) have an argument in the initializer called distribution_ strategy.
    You can pass an instance of tf.distribute.MirroredStrategy (or one of the other
    subclasses) to it to have all the model training processes during the search use
    data parallelism. An example with the MNIST dataset is shown in the next listing.
    The code will run in a distributed manner on all the GPUs that are visible to
    the program.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AutoKeras 中，AutoModel 类和所有任务 API 类（如 ImageClassifier 和 TextClassifier）的初始化器中都有一个名为
    distribution_strategy 的参数。你可以传递一个 tf.distribute.MirroredStrategy（或其其他子类）的实例，以便在搜索过程中使用数据并行进行所有模型训练过程。下一个列表展示了使用
    MNIST 数据集的示例。代码将在程序可见的所有 GPU 上以分布式方式运行。
- en: Listing 8.15 Data parallelism with AutoKeras
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.15 使用 AutoKeras 的数据并行
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Uses data parallelism for training
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于训练的数据并行
- en: For KerasTuner, all the tuners, which are subclasses of the Tuner class (BayesianOptimization,
    Hyperband, and RandomSearch), also have this distribution_strategy argument in
    their initializers, and it works the same way as in AutoKeras. You can pass an
    instance of a TensorFlow distribution strategy, like tf.distribute.MirroredStrategy,
    to the argument. The models will use the distribution strategy for training.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 KerasTuner，所有调优器，即 Tuner 类的子类（BayesianOptimization、Hyperband 和 RandomSearch），它们的初始化器中也有这个
    distribution_strategy 参数，并且其工作方式与 AutoKeras 相同。你可以传递一个 TensorFlow 分布式策略的实例，如 tf.distribute.MirroredStrategy，到该参数。模型将使用分布式策略进行训练。
- en: AutoKeras uses this KerasTuner feature under the hood. A simple example of using
    data parallelism with KerasTuner is shown in the following listing. Here, we’ve
    designed a very basic search space for the MNIST dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: AutoKeras 在底层使用这个 KerasTuner 功能。以下列表展示了使用 KerasTuner 进行数据并行的简单示例。在这里，我们为 MNIST
    数据集设计了一个非常基本的搜索空间。
- en: Listing 8.16 Data parallelism with KerasTuner
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.16 使用 KerasTuner 的数据并行
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Uses data parallelism for training
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于训练的数据并行
- en: The program can use all the GPUs available to split the data and aggregate the
    gradients to update the model. Besides data parallelism, let’s see how other types
    of distributed strategies can help accelerate the training process.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 程序可以使用所有可用的 GPU 来分割数据并聚合梯度以更新模型。除了数据并行之外，让我们看看其他类型的分布式策略如何帮助加速训练过程。
- en: 8.2.2 Model parallelism
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 模型并行
- en: As mentioned previously, model parallelism is used mainly when working with
    large models. For models that are too large to be contained in one GPU’s memory,
    it offers a way to ensure that all the computations are still done efficiently
    on GPUs by breaking the model into pieces and distributing them across the available
    processing units. It also enables you to offload some computations to different
    GPUs to run in parallel during inference.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型并行主要用于处理大型模型。对于太大而无法容纳在一个 GPU 内存中的模型，它提供了一种通过将模型拆分成多个部分并将它们分布到可用的处理单元上来确保所有计算仍然在
    GPU 上高效完成的方法。它还允许你在推理期间将一些计算卸载到不同的 GPU 上以并行运行。
- en: 'A typical example is a model with multiple branches. An intermediate output
    is the input of two independent layers, which do not rely in any way on each other’s
    output. An example of such a case is shown in figure 8.4\. Four GPUs in total
    are shown in this figure: the two convolutional layers on GPU 2, and GPU 3 can
    run in parallel during inference because their inputs do not rely on each other’s
    output.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是具有多个分支的模型。中间输出是两个独立层的输入，它们彼此之间没有任何依赖。图8.4展示了这样一个案例的例子。图中显示了总共四个GPU：GPU
    2上的两个卷积层，GPU 3可以在推理时并行运行，因为它们的输入不依赖于彼此的输出。
- en: '![08-04](../Images/08-04.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![08-04](../Images/08-04.png)'
- en: Figure 8.4 A model with multiple branches on multiple GPUs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 多个GPU上的多分支模型
- en: Another not-so-common example is one layer that can be split into multiple parts
    to run in parallel. For example, in a convolutional layer, each filter works independently.
    Therefore, they can be split to run on multiple GPUs. The outputs can then be
    aggregated to form the output tensor of the entire convolutional layer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不那么常见的例子是将一层分割成多个部分以并行运行。例如，在一个卷积层中，每个滤波器独立工作。因此，它们可以被分割到多个GPU上运行。然后可以将输出汇总以形成整个卷积层的输出张量。
- en: The model parallelism is not well wrapped into simple APIs by the popular open
    source deep learning frameworks nowadays. In most of the cases, the model would
    be small enough to fit into the memory of a single GPU. To implement a model with
    model parallelism, one need to learn to use Mesh TensorFlow, which we will not
    go into detail in this book. If you are interested, you can check out Mesh TensorFlow
    on GitHub at [https://github.com/tensorflow/mesh](https://github.com/tensorflow/mesh).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，流行的开源深度学习框架并没有很好地将模型并行性封装到简单的API中。在大多数情况下，模型足够小，可以适应单个GPU的内存。要实现具有模型并行的模型，需要学习如何使用Mesh
    TensorFlow，本书不会详细介绍这一点。如果您感兴趣，可以在GitHub上查看Mesh TensorFlow：[https://github.com/tensorflow/mesh](https://github.com/tensorflow/mesh)。
- en: 8.2.3 Parallel tuning
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 并行调优
- en: Parallel tuning means training models with different hyperparameters on different
    devices. For example, suppose you have four GPUs on a single machine, and you
    want to try eight different sets of hyperparameters in the search space. If you
    run four models in parallel, you only need the time it takes to train two models
    to finish the search.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 并行调优意味着在不同的设备上训练具有不同超参数的模型。例如，假设您在一台机器上拥有四个GPU，并且您想在搜索空间中尝试八组不同的超参数。如果您并行运行四个模型，您只需要训练两个模型的时间来完成搜索。
- en: Notably, parallel tuning also requires the tuning algorithm to be capable of
    receiving the evaluation results asynchronously. If not run in parallel, the tuning
    algorithm would start to train one model and wait for the evaluation result before
    starting to train another model. However, when run in parallel, the search algorithm
    needs to start the training of multiple models before receiving any evaluation
    results, and the evaluation results may not be received in the same order as the
    training processes started. The search algorithm will generate a new model to
    train whenever there is an available device to use.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，并行调优还要求调优算法能够异步接收评估结果。如果不并行运行，调优算法将开始训练一个模型，并在开始训练另一个模型之前等待评估结果。然而，当并行运行时，搜索算法需要在收到任何评估结果之前开始训练多个模型，并且评估结果可能不会按照训练过程启动的顺序接收。搜索算法将在有可用设备时随时生成一个新模型进行训练。
- en: 'Let’s take a look at how to run parallel tuning with KerasTuner. It implements
    a *chief/worker* model for this purpose: at any given time, only one chief process
    is running, but there are multiple worker processes running. The chief process
    runs the search algorithm. It manages the workers to start the training process
    and collects the evaluation results from them.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用KerasTuner运行并行调优。它为此实现了一个*首席/工人*模型：在任何给定时间，只有一个首席进程正在运行，但存在多个工人进程。首席进程运行搜索算法。它管理工人以启动训练过程，并从他们那里收集评估结果。
- en: One problem we encounter when tuning in parallel is where to store the models.
    When not running in parallel, the searched models and their trained weights are
    saved on the disk, so that we can load the best models after the search. When
    running in parallel, to save the models for loading them later, we need to have
    shared storage for all the workers and the chief. If we’re using multiple GPUs
    on a single machine, this isn’t a problem, because they use the same storage.
    If we’re using multiple GPUs on multiple machines, we can mount the shared storage
    to the machines or use network storage that all the machines have access to, for
    example, a Google Cloud Storage bucket.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行调优时遇到的一个问题是模型存储的位置。在不并行运行的情况下，搜索到的模型及其训练好的权重被保存在磁盘上，这样我们可以在搜索后加载最佳模型。在并行运行时，为了保存模型以便稍后加载，我们需要所有工作进程和主进程共享存储。如果我们在一台机器上使用多个
    GPU，这不是问题，因为它们使用相同的存储。如果我们使用多台机器上的多个 GPU，我们可以将共享存储挂载到机器上或使用所有机器都可以访问的网络存储，例如 Google
    Cloud Storage 存储桶。
- en: The communication pattern of the chief and the workers in parallel tuning is
    shown in figure 8.5\. The solid lines are the control flows. The chief node sends
    the different hyperparameter sets to different workers, which build and train
    the models using the given hyperparameters. When finished, the workers send the
    evaluation results back to the chief node. The dashed lines in the figure are
    the data flows. The trained models and results are written to centralized storage,
    which all the workers and the chief can access. When the user calls the chief
    to load the best models, the chief should load the models from the centralized
    storage. The training and validation data should also be stored in centralized
    storage that all the workers can access.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 并行调优中主进程和工作进程的通信模式如图 8.5 所示。实线是控制流。主节点将不同的超参数集发送到不同的工作进程，这些工作进程使用给定的超参数构建和训练模型。完成后，工作进程将评估结果发送回主节点。图中的虚线是数据流。训练好的模型和结果被写入集中式存储，所有工作进程和主进程都可以访问。当用户调用主进程加载最佳模型时，主进程应从集中式存储中加载模型。训练和验证数据也应存储在所有工作进程都可以访问的集中式存储中。
- en: '![08-05](../Images/08-05.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![08-05](../Images/08-05.png)'
- en: Figure 8.5 Communication pattern of parallel tuning
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 并行调优的通信模式
- en: Now, let’s see how to spin up this parallel training in KerasTuner. To start
    the chief and the worker processes, we run the same Python script. KerasTuner
    will use environment variables to define whether the current process should be
    the chief or a worker. The KERASTUNER_TUNER_ID environment variable is used to
    specify the IDs of different processes. You can set it to 'chief' for the chief
    process, and 'tuner0', 'tuner1', and so on for the workers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 KerasTuner 中启动这个并行训练。要启动主进程和工作进程，我们运行相同的 Python 脚本。KerasTuner 将使用环境变量来定义当前进程应该是主进程还是工作进程。KERASTUNER_TUNER_ID
    环境变量用于指定不同进程的 ID。你可以将其设置为 'chief' 以指定主进程，对于工作进程则设置为 'tuner0'、'tuner1' 等等。
- en: 'We need to set two more environment variables for the workers to find the address
    of the chief, so they can report the evaluation results: KERASTUNER_ORACLE_IP
    and KERASTUNER_ORACLE_PORT. These specify the IP address and the port that the
    chief service runs on; they need to be set for both the chief and the workers.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置两个额外的环境变量，以便工作进程找到主进程的地址，从而可以报告评估结果：KERASTUNER_ORACLE_IP 和 KERASTUNER_ORACLE_PORT。这些指定了主服务运行在上的
    IP 地址和端口号；它们需要为主进程和工作进程都设置。
- en: In summary, we need to set three environment variables before running the script
    (run_tuning.py, which we’ll look at shortly). Here, we provide two shell scripts
    to start the chief and worker processes. First we need to start the chief process.
    The following listing shows the commands to specify the environment variables
    and start the process. You can open a terminal to run these commands; the terminal
    will hang there and wait for the workers to start.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在运行脚本（run_tuning.py，我们很快就会看到）之前，我们需要设置三个环境变量。这里，我们提供了两个 shell 脚本以启动主进程和工作进程。首先，我们需要启动主进程。以下列表显示了指定环境变量并启动进程的命令。你可以打开一个终端来运行这些命令；终端将挂起并等待工作进程启动。
- en: Listing 8.17 Starting the chief process
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.17 启动主进程
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Marks the process as the chief
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标记进程为主进程
- en: ❷ The IP address of the chief
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 主进程的 IP 地址
- en: ❸ The port of the chief
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 主进程的端口号
- en: ❹ Starts the chief process
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 启动主进程
- en: Now, open another terminal to start a worker process. You can do this with the
    script in the following listing. It’s very similar to the previous one; the only
    difference is the KERASTUNER_ TUNER_ID to specify it as a worker.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打开另一个终端来启动工作者进程。你可以使用以下列表中的脚本完成此操作。它与前面的脚本非常相似；唯一的区别是KERASTUNER_TUNER_ID，用于指定它作为工作者。
- en: Listing 8.18 Starting a worker process
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.18 启动工作者进程
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Marks the process as a worker
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将进程标记为工作者
- en: ❷ The IP address of the chief
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 主控的IP地址
- en: ❸ The port of the chief
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 主控的端口
- en: ❹ Starts the worker process
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 启动工作者进程
- en: 'As soon as you start a worker process, the tuning starts. To start the rest
    of the worker processes, use the same commands but specify a different KERASTUNER_TUNER_ID
    for each: for example, ''tuner1'', ''tuner2'', and so on.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动工作者进程，调优就开始了。要启动其他工作者进程，使用相同的命令，但为每个指定不同的KERASTUNER_TUNER_ID：例如，'tuner1'、'tuner2'等等。
- en: The Python code in run_tuning.py starts the processes. Let’s take a look at
    what’s in this script. A simple KerasTuner example is shown in the following listing.
    No specific configuration is needed except for the directory argument, which must
    point to a directory accessible to all the workers and the chief.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: run_tuning.py中的Python代码启动进程。让我们看看这个脚本中有什么。以下列表展示了简单的KerasTuner示例。除了目录参数外，不需要任何特定配置，该参数必须指向所有工作者和主管都可以访问的目录。
- en: Listing 8.19 Parallel tuning with KerasTuner
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.19 使用KerasTuner进行并行调优
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ A directory that is accessible to all workers and the chief
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 可供所有工作者和主管访问的目录
- en: So far, we’ve been talking about how to speed up tuning by running on more GPUs.
    However, we have other strategies to speed up tuning on the algorithmic level.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在讨论如何通过在更多GPU上运行来加速调优。然而，我们还有其他策略可以在算法层面上加速调优。
- en: 8.3 Search speedup strategies
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 搜索加速策略
- en: In this section, we will introduce some speedup strategies for the search process.
    First, we will introduce a model-scheduling technique named *Hyperband*. Given
    a certain amount of computational resources, this technique can allocate them
    to train different models to different extents. Instead of fully training every
    model, it saves time by terminating some less promising models at an early stage.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些用于搜索过程的加速策略。首先，我们将介绍一种名为*Hyperband*的模型调度技术。给定一定量的计算资源，这项技术可以将它们分配给训练不同模型的不同程度。而不是完全训练每个模型，它通过在早期阶段终止一些不太有前途的模型来节省时间。
- en: Next, we’ll look at how to use models with pretrained weights to speed up the
    training process. Finally, we’ll introduce a widely used technique in AutoML,
    which is to *warm-start* the search space with some good models. This gives the
    tuner some guidance, requiring it to do less exploration to find a model with
    relatively good performance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用预训练权重的模型来加速训练过程。最后，我们将介绍AutoML中广泛使用的一种技术，即使用一些好的模型*预热启动*搜索空间。这为调优器提供了一些指导，使其需要做更少的探索来找到一个相对性能较好的模型。
- en: 8.3.1 Model scheduling with Hyperband
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 使用Hyperband进行模型调度
- en: Hyperband[¹](#pgfId-1020927) is a widely used model-scheduling algorithm in
    AutoML. Instead of trying to model the relationship between the performance and
    the hyperparameters of the models, the core idea of Hyperband is to not fully
    train all of the possible models in the search space but to focus on the more
    promising ones, saving computation resources and time for those.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband[¹](#pgfId-1020927)是AutoML中广泛使用的模型调度算法。Hyperband的核心思想不是试图模拟模型性能与超参数之间的关系，而是专注于搜索空间中更有前途的模型，而不是完全训练所有可能的模型，从而节省计算资源和时间。
- en: Here’s a concrete example. Suppose we have four different hyperparameter sets
    to try, each of which will be instantiated into a deep learning model and each
    of which needs to be trained for 40 epochs. We can first train all four models
    for 20 epochs and then discard the two models that are performing the poorest
    at this stage. Now we have two good models left. We then train these models for
    another 20 epochs, so they’re fully trained, and select the best one. We therefore
    save the time and resources required to train the two not-so-good models for 20
    more epochs each.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个具体的例子。假设我们有四个不同的超参数集要尝试，每个集都将实例化为一个深度学习模型，并且每个模型都需要训练40个epoch。我们首先训练这四个模型20个epoch，然后丢弃在这个阶段表现最差的两个模型。现在我们剩下两个好的模型。然后我们再训练这些模型20个epoch，使它们完全训练，并选择最好的一个。因此，我们节省了训练两个不太好的模型额外20个epoch所需的时间和资源。
- en: 'To understand the process of using Hyperband, we first need to understand its
    subprocess, called *successive halving*. Successive halving involves a loop of
    two alternating steps: the first step is to reduce the number of remaining models
    by deleting the less promising ones, and the second step is to further train the
    remaining models. You need to provide the following four arguments to the successive
    halving algorithm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解使用Hyperband的过程，我们首先需要了解其子过程，称为*连续减半*。连续减半涉及两个交替步骤的循环：第一步是通过删除不那么有希望的模型来减少剩余模型的数量，第二步是进一步训练剩余模型。您需要向连续减半算法提供以下四个参数：
- en: models—A list of all the models to be trained.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: models—所有待训练模型的列表。
- en: max_epochs—The total number of epochs needed to fully train a model.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_epochs—完全训练一个模型所需的总epoch数。
- en: start_epochs—The number of epochs to train all the models in the first round.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: start_epochs—第一轮中训练所有模型所需的epoch数。
- en: factor—A measure of how fast we want to reduce the number of models and increase
    the number of epochs to train the remaining models; it defaults to 3. For example,
    with factor=3, every time through the loop, we’ll reduce the number of remaining
    models to one-third of the current count and further train the remaining models
    to three times the number of already trained epochs.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: factor—衡量我们希望多快减少模型数量并增加剩余模型的训练epoch数的指标；默认值为3。例如，当factor=3时，每次循环通过，我们将剩余模型的数量减少到当前数量的三分之一，并将剩余模型进一步训练到已训练epoch数的三倍。
- en: This process repeats until all the remaining models are fully trained. It returns
    the best model and the validation loss of the best model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程会重复进行，直到所有剩余的模型都完全训练完毕。它将返回最佳模型及其验证损失。
- en: At the start of the algorithm, we’re given a list of models. We save them in
    remaining_ models, and we remove some models in every round. We use n_models to
    record the total number of models at the beginning. We use trained_epochs to record
    the number of epochs already trained for the remaining models, which will be updated
    in each round of further training. target_epochs represents the target number
    of epochs to reach for the current round, which is also updated at the end of
    each round. We record the evaluation results in a dictionary, eval_results, whose
    keys are the models and whose values are the validation loss for each model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法开始时，我们得到一个模型列表。我们将它们保存在remaining_models中，并在每一轮中删除一些模型。我们使用n_models记录初始的总模型数。我们使用trained_epochs记录剩余模型已训练的epoch数，这将在每一轮进一步训练中更新。target_epochs代表当前轮次需要达到的目标epoch数，这也在每一轮结束时更新。我们在字典eval_results中记录评估结果，其键是模型，值是每个模型的验证损失。
- en: With these variables initialized, we are ready to start the outer loop, which
    repeats until all the remaining models are fully trained—in other words, until
    trained_epochs is equal to or greater than max_epochs. The first step in the loop
    is an inner loop to train and evaluate all the models in remaining_models and
    record the results in eval_results.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化这些变量后，我们就可以开始外循环了，该循环会重复进行，直到所有剩余模型都完全训练完毕——换句话说，直到trained_epochs等于或大于max_epochs。循环中的第一步是内循环，用于训练和评估remaining_models中的所有模型，并将结果记录在eval_results中。
- en: Then we discard the worst-performing models, reducing the number of remaining
    models by the specified factor. At the end of the round, we update the variables
    keeping track of the epochs.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们丢弃表现最差的模型，通过指定的因子减少剩余模型的数量。在每一轮结束时，我们更新跟踪epoch的变量。
- en: Once the outer loop completes (when the models are fully trained), we can get
    the best model and return that model and its validation loss. The pseudocode of
    the successive halving process is shown in the following listing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦外循环完成（当模型完全训练完毕时），我们可以获取最佳模型并返回该模型及其验证损失。以下列表展示了连续减半过程的伪代码。
- en: Listing 8.20 Successive halving pseudocode
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.20 连续减半伪代码
- en: '[PRE25]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ The round counter
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 轮次计数器
- en: ❷ The outer loop (continues until the remaining models are fully trained)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 外循环（继续进行，直到剩余模型完全训练）
- en: ❸ The inner loop
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 内循环
- en: ❹ Reduces the number of models
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 减少模型数量
- en: ❺ Updates the variables for the next round
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 更新下一轮的变量
- en: For successive halving to be effective, we need to pick the right number of
    models to explore and the right number of epochs to start training them for. If
    we don’t start with enough models, the algorithm won’t be explorative enough.
    If we start with too many, the algorithm will discard a lot of models in the early
    stages without exploiting them. Therefore, Hyperband comes up with a method to
    avoid specifying a fixed number of models during successive halving.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使连续减半有效，我们需要选择合适的模型探索数量和合适的 epoch 数来开始训练。如果我们开始时模型不足，算法的探索性将不足。如果我们开始时模型过多，算法在早期阶段将丢弃大量模型而未加以利用。因此，Hyperband
    提出了一种方法来避免在连续减半过程中指定固定的模型数量。
- en: To avoid these problems, Hyperband runs the successive halving process multiple
    times to balance exploration and exploitation, trying out different factor values.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这些问题，Hyperband 多次运行连续减半过程以平衡探索和利用，尝试不同的因子值。
- en: To run Hyperband, we need to specify two arguments, max_epochs and factor, which
    are the same as the ones passed to the successive halving algorithm.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Hyperband，我们需要指定两个参数，max_epochs 和 factor，它们与传递给连续减半算法的参数相同。
- en: Hyperband runs the successive halving algorithm multiple times. Each iteration
    is called a *bracket*. We use s_max to specify the number of brackets to run,
    whose value is close to log(max_epochs, factor).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband 多次运行连续减半算法。每次迭代被称为一个 *括号*。我们使用 s_max 来指定要运行的括号数量，其值接近 log(max_epochs,
    factor)。
- en: For an intuitive understanding of how many models to generate in a bracket,
    in the first bracket, the number of models generated is equal to max_epochs. After
    that, in each bracket, the initial number of models is 1/factor of the number
    in the previous call (actually, in the implementation, the number of models is
    around pow(factor, s), but it’s adjusted to be larger in the later brackets).
    Hyperband also can prevent having too few models in a bracket; for example, a
    bracket won’t start with only one model. Different brackets do not share the models
    passed to them but randomly generate new models at the start of each bracket.
    The start_ epochs value for the bracket is strictly increased by factor times
    per bracket.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解在括号中生成多少模型，在第一个括号中，生成的模型数量等于 max_epochs。之后，在每个括号中，初始模型数量是前一次调用中数量的 1/factor（实际上，在实现中，模型数量大约是
    pow(factor, s)，但在后面的括号中调整为更大的数字）。Hyperband 还可以防止括号中模型过少；例如，括号不会从只有一个模型开始。不同的括号不共享传递给它们的模型，而是在每个括号的开始时随机生成新的模型。括号的
    start_epochs 值将严格增加 factor 倍每次括号。
- en: Now that you understand all the required arguments, let’s take a look at the
    pseudocode for Hyperband, shown here.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了所有必要的参数，让我们看看 Hyperband 的伪代码，如下所示。
- en: Listing 8.21 Hyperband pseudocode
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.21 Hyperband 伪代码
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Specifies the number of brackets
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定括号的数目
- en: ❷ Loops through the brackets
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历括号
- en: ❸ Generates new models
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成新的模型
- en: ❹ Calls successive_halving
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 调用 successive_halving
- en: ❺ Updates the best model
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 更新最佳模型
- en: To better understand the code, let’s walk through an example with concrete numbers.
    If we let max_epochs be 81 and let factor be 3, s will iterate from 4 to 0\. The
    number of models for the brackets will be [81, 34, 15, 8, 5]. This is roughly
    pow(factor, s) but is adjusted to a larger number as pow(factor, s) becomes smaller.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解代码，让我们通过一个具体的例子来讲解。如果我们让 max_epochs 为 81，让 factor 为 3，s 将从 4 迭代到 0。括号中的模型数量将是
    [81, 34, 15, 8, 5]。这大约是 pow(factor, s)，但随着 pow(factor, s) 的减小，它被调整为更大的数字。
- en: Hyperband is already implemented in KerasTuner as one of the tuners, named Hyperband,
    which you can use directly. You can use it either on a single GPU or on multiple
    GPUs for parallel tuning. You can specify factor and max_epochs in the initializer
    of the Hyperband class. In addition, you can specify the number of brackets with
    hyperband_iterations, which corresponds to the s_max argument in listing 8.21,
    to control the time of the search. The following listing shows an example.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband 已经在 KerasTuner 中实现，作为其中一个调优器，名为 Hyperband，你可以直接使用。你可以在单个 GPU 或多个 GPU
    上使用它进行并行调优。你可以在 Hyperband 类的初始化器中指定 factor 和 max_epochs。此外，你可以通过 hyperband_iterations
    指定括号的数目，这对应于列表 8.21 中的 s_max 参数，以控制搜索的时间。以下列表显示了一个示例。
- en: Listing 8.22 Running Hyperband with KerasTuner
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.22 使用 KerasTuner 运行 Hyperband
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Specifies the maximum number of epochs as 10
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定最大 epoch 数为 10
- en: ❷ Specifies the factor by which to decrease the number of remaining models as
    3
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定减半剩余模型数量的因子为 3
- en: ❸ Specifies the number of brackets as 2
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定括号的数量为2
- en: The code is just a normal usage of KerasTuner but with a different tuner class.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 代码只是KerasTuner的正常用法，但使用了不同的调优类。
- en: 8.3.2 Faster convergence with pretrained weights in the search space
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 在搜索空间中使用预训练权重以实现更快的收敛
- en: Training a deep learning model usually takes a long time. We can use *pretrained
    weights* to accelerate the training, which makes the model converge in fewer epochs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个深度学习模型通常需要很长时间。我们可以使用*预训练权重*来加速训练过程，这使得模型能在更少的迭代次数中收敛。
- en: Note Pretrained weights are the weights of a trained model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：预训练权重是指已经训练好的模型的权重。
- en: 'In some cases, you may find your dataset is not large enough to train a good
    model that generalizes well. Pretrained weights can also help with this case:
    you can download and use a model with the weights trained using other (larger
    and more comprehensive) datasets. The features learned by the model should be
    generalizable to the new dataset. When using pretrained weights, you will need
    to further train the pretrained model with your dataset. This process is often
    referred to as a kind of transfer learning.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你可能发现你的数据集不够大，无法训练出一个泛化良好的好模型。预训练权重也可以帮助这种情况：你可以下载并使用使用其他（更大、更全面）数据集训练的权重模型。模型学习到的特征应该可以泛化到新的数据集。当使用预训练权重时，你需要进一步使用你的数据集训练预训练模型。这个过程通常被称为一种迁移学习。
- en: 'We can use pretrained weights in two ways. The first is fairly simple: just
    train the model with the new dataset directly. Usually, a smaller learning rate
    is preferred, because a large learning rate may change the original weights very
    quickly. The second approach you can take is to freeze most parts of the model
    and train only the output layers.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两种方式使用预训练权重。第一种方法相对简单：直接使用新数据集训练模型。通常，更小的学习率是首选的，因为较大的学习率可能会非常快地改变原始权重。第二种方法是冻结模型的大部分部分，只训练输出层。
- en: For example, suppose we’re using a convolutional neural network with pretrained
    weights for a classification task. The output layer is a fully connected layer
    with the same number of neurons as the number of classes. We can keep just the
    convolutional part of the model and discard any fully connected layers. We then
    append our fully connected layer with newly initialized weights to the convolutional
    layers and start training the model with the convolutional layers frozen, so we
    update only the fully connected layer we added to the model (see figure 8.6).
    If the model is a different type of neural network, it usually can still be divided
    into the feature learning part (the convolutional layers, recurrent layers, and
    transformers) and the head (usually fully connected layers). Therefore, we can
    still apply the same methods for using pretrained weights.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在使用一个具有预训练权重的卷积神经网络进行分类任务。输出层是一个具有与类别数量相同神经元数量的全连接层。我们可以只保留模型的卷积部分，并丢弃任何全连接层。然后，我们将使用新初始化的权重添加的全连接层附加到卷积层上，并开始以冻结卷积层的方式训练模型，这样我们只更新模型中添加的全连接层（见图8.6）。如果模型是不同类型的神经网络，它通常仍然可以分成特征学习部分（卷积层、循环层和变换器）和头部（通常是全连接层）。因此，我们仍然可以应用相同的方法来使用预训练权重。
- en: '![08-06](../Images/08-06.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![08-06](../Images/08-06.png)'
- en: Figure 8.6 Two methods of using pretrained weights
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 使用预训练权重的两种方法
- en: Besides the two methods just mentioned, you can be more flexible in choosing
    which part of the model to freeze, which part of the model to keep, and how many
    layers to add to the model. For example, you can freeze some of the convolutional
    layers and leave others unfrozen. You can also keep one or more fully connected
    layers and append more than one layer to the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上面提到的两种方法，你还可以更灵活地选择冻结模型的哪一部分，保留哪一部分，以及向模型中添加多少层。例如，你可以冻结一些卷积层，而将其他层保持未冻结状态。你也可以保留一个或多个全连接层，并向模型中添加多个层。
- en: One requirement when using pretrained weights is that the training data and
    the dataset used to create the pretrained model must be of the same type. For
    example, if your dataset consists of English sentences, the pretrained model would
    also need to be a natural language model trained on sentences in English. If your
    dataset instead consisted of Chinese characters, the performance boost gained
    by using the pretrained weights from such a model would not be that significant,
    and in fact it might even have a negative effect.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练权重的一个要求是，训练数据和用于创建预训练模型的数据库必须是同一类型。例如，如果您的数据集由英语句子组成，预训练模型也需要是一个在英语句子上训练的自然语言模型。如果您的数据集由中文字符组成，使用此类模型的预训练权重获得的表现提升可能并不显著，甚至可能产生负面影响。
- en: AutoKeras already uses some pretrained weights in the search space. For some
    image- or text-related blocks, there’s a Boolean hyperparameter named pretrained
    that you can specify in the initializer to indicate whether to use pretrained
    weights for the model. For image data, these include ResNetBlock, XceptionBlock,
    and EfficientNetBlock, which use the ImageNet dataset for pretraining. For text
    data, BertBlock uses text extracted from Wikipedia for pretraining.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: AutoKeras 已经在搜索空间中使用了一些预训练权重。对于一些与图像或文本相关的块，初始化器中有一个名为 pretrained 的布尔超参数，您可以使用它来指定是否为模型使用预训练权重。对于图像数据，这些包括
    ResNetBlock、XceptionBlock 和 EfficientNetBlock，它们使用 ImageNet 数据集进行预训练。对于文本数据，BertBlock
    使用来自维基百科的文本进行预训练。
- en: To use pretrained weights in AutoKeras, we can directly connect these blocks
    to form a search space using AutoModel, as shown in the following listing. This
    is a simple example of using a pretrained ResNet for image classification with
    the CIFAR-10 dataset. In the code, we specify the pretrained argument of ResNetBlock
    as True so that it will search for only ResNet models with pretrained weights.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 AutoKeras 中使用预训练权重，我们可以直接使用 AutoModel 将这些块连接起来形成一个搜索空间，如下所示列表。这是一个使用 CIFAR-10
    数据集用预训练的 ResNet 进行图像分类的简单示例。在代码中，我们指定 ResNetBlock 的预训练参数为 True，以便它只搜索带有预训练权重的
    ResNet 模型。
- en: Listing 8.23 Using pretrained ResNets in AutoKeras for image classification
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.23 在 AutoKeras 中使用预训练的 ResNets 进行图像分类
- en: '[PRE28]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Uses pretrained weights for the block
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为块使用预训练权重
- en: To build your own search space with models with pretrained weights, you can
    use Keras Applications, which has a collection of pretrained models to use. They
    can be imported under tf.keras.applications; for example, you can import a ResNet
    model from tf.keras.applications.ResNet50. Refer to [https://keras.io/api/applications/](https://keras.io/api/applications/)
    for a complete list of models.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用具有预训练权重的模型构建自己的搜索空间，您可以使用 Keras Applications，它包含了一组可用的预训练模型。它们可以在 tf.keras.applications
    下导入；例如，您可以从 tf.keras.applications.ResNet50 导入一个 ResNet 模型。有关模型完整列表，请参阅 [https://keras.io/api/applications/](https://keras.io/api/applications/)。
- en: To initialize the pretrained model object, you usually need to specify two arguments,
    include_top and weights. include_top is a Boolean argument specifying whether
    to include the classification head of the pretrained model. weights can be 'imagenet'
    or None, specifying whether to use the ImageNet pretrained weights or randomly
    initialized weights. An example using ResNet is shown in the following listing.
    Here, we create a ResNet without the fully connected layers and use the ImageNet
    pretrained weights.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化预训练模型对象，您通常需要指定两个参数，include_top 和 weights。include_top 是一个布尔参数，指定是否包含预训练模型的分类头。weights
    可以是 'imagenet' 或 None，指定是否使用 ImageNet 预训练权重或随机初始化的权重。以下是一个使用 ResNet 的示例列表。在这里，我们创建了一个不带全连接层的
    ResNet，并使用 ImageNet 预训练权重。
- en: Listing 8.24 Keras Applications example
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.24 Keras 应用示例
- en: '[PRE29]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can use Keras Applications with KerasTuner to build a search space of pretrained
    models. For example, here we build a search space with two hyperparameters: the
    first one indicates whether to use pretrained weights, and the second indicates
    whether to freeze the model.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Keras Applications 与 KerasTuner 一起构建预训练模型的搜索空间。例如，在这里我们构建了一个包含两个超参数的搜索空间：第一个超参数表示是否使用预训练权重，第二个表示是否冻结模型。
- en: Listing 8.25 Using a pretrained ResNet with KerasTuner
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.25 使用 KerasTuner 预训练 ResNet
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ The hyperparameter indicating whether to use pretrained weights
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指示是否使用预训练权重的超参数
- en: ❷ Does not include the classification head
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不包括分类头
- en: ❸ Specifies whether to use ImageNet pretrained weights or random weights
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定是否使用 ImageNet 预训练权重或随机权重
- en: ❹ The hyperparameter indicating whether to freeze the model
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 冻结模型的超参数
- en: ❺ Builds and returns the Keras model
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 构建并返回Keras模型
- en: Building such a search space to determine whether to use the pretrained weights
    and whether to freeze the model can help you select the best solution. Using pretrained
    weights and pretrained models is usually a good way to accelerate training and
    make your model generalize well if you have only limited training data. However,
    only through experimentation can you tell whether the pretrained weights are appropriate
    for your problem and dataset.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样的搜索空间以确定是否使用预训练权重以及是否冻结模型，可以帮助你选择最佳解决方案。使用预训练权重和预训练模型通常是一种加速训练并使你的模型在有限的训练数据下具有良好的泛化能力的好方法。然而，只有通过实验，你才能知道预训练权重是否适合你的问题和数据集。
- en: 8.3.3 Warm-starting the search space
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 预热搜索空间
- en: In AutoML and hyperparameter tuning, without warm-start, the search algorithm
    doesn’t have any prior knowledge about the search space. It doesn’t know the meaning
    of the different hyperparameters and has no idea about which models might perform
    well or poorly. Therefore, it has to explore a large, unknown search space gradually,
    sample by sample, which is not very efficient.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在AutoML和超参数调整中，如果没有预热启动，搜索算法对搜索空间没有任何先验知识。它不知道不同超参数的含义，也不知道哪些模型可能表现良好或不好。因此，它必须逐渐、逐个样本地探索一个庞大且未知的搜索空间，这并不非常高效。
- en: '*Warm-starting* the search space means we hand-pick some good models and hyperparameters
    for the search algorithm to evaluate before the search starts. This is a good
    way to inject human knowledge of the performance of different models into the
    search process. Otherwise, the search algorithm may spend a lot of time on bad
    models and bad hyperparameter combinations before finding the good ones, which
    are far fewer in number than the bad ones!'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*预热搜索空间*意味着我们在搜索开始之前手动挑选一些好的模型和超参数供搜索算法评估。这是一种将人类对不同模型性能的知识注入搜索过程的好方法。否则，搜索算法可能会在找到好的模型之前，花费大量时间在不好的模型和不好的超参数组合上，而这些好的模型数量远少于不好的模型！'
- en: With warm-start, the search algorithm can quickly find a good model with limited
    computational resources by exploiting the startup models. Based on this idea,
    we can use a greedy strategy to search the space. We start by evaluating the startup
    models. We then pick the best model and randomly modify its hyperparameter values
    a little bit to produce the model to be evaluated next.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预热启动，搜索算法可以通过利用启动模型，在有限的计算资源下快速找到一个好的模型。基于这个想法，我们可以使用贪婪策略来搜索空间。我们首先评估启动模型。然后选择最佳模型，并随机稍微修改其超参数值，以产生下一个要评估的模型。
- en: This greedy strategy is already implemented in AutoKeras as the Greedy tuner.
    Some task-specific tuners, like ImageClassifierTuner and TextClassifierTuner,
    are subclasses of the Greedy tuner. These are the default tuners when using the
    task APIs, like ImageClassifier and TextClassifier. They offer a list of predefined
    hyperparameter values to be tried first before exploring the search space. Therefore,
    when you run these tasks in AutoKeras, these tuners are more efficient than tuners
    without warm-start.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这种贪婪策略已经在AutoKeras中实现，作为贪婪调优器。一些特定任务的调优器，如ImageClassifierTuner和TextClassifierTuner，是贪婪调优器的子类。当使用任务API，如ImageClassifier和TextClassifier时，这些是默认的调优器。它们提供了一系列预定义的超参数值，在探索搜索空间之前先尝试。因此，当你在AutoKeras中运行这些任务时，这些调优器比没有预热启动的调优器更有效率。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: When the training dataset is too large to fit into the main memory, we can load
    the dataset into memory batch by batch for training and prediction.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练数据集太大而无法放入主内存时，我们可以分批将数据集加载到内存中进行训练和预测。
- en: Data parallelism speeds up model training by keeping synchronized copies of
    the model on different devices and splitting the dataset to train them in parallel.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行通过在不同设备上保持模型同步副本并分割数据集以并行训练来加速模型训练。
- en: Model parallelism divides a large model, putting different layers on different
    devices and allowing them to run in parallel (using the same data). This can also
    speed up training.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型并行将大模型分割，将不同的层放在不同的设备上，并允许它们并行运行（使用相同的数据）。这也可以加速训练。
- en: Parallel tuning involves running models with different hyperparameter settings
    on different devices in parallel to speed up the tuning process.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行调优涉及在不同设备上并行运行具有不同超参数设置的模型，以加快调优过程。
- en: Hyperband can speed up the search process by allocating limited resources to
    unpromising models and using the saved resources for more promising models.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyperband可以通过将有限的资源分配给没有前途的模型，并使用节省下来的资源为更有前途的模型提供支持，从而加速搜索过程。
- en: Using pretrained weights learned on large datasets can allow for faster convergence
    on a new dataset and make the model generalize better when the new dataset is
    small.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在大数据集上学习到的预训练权重可以允许在新的数据集上更快地收敛，并在新数据集较小时使模型更好地泛化。
- en: Warm-starting the search space gives the tuner a better overview of the space,
    thus speeding up the search process.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对搜索空间进行预热启动，给调优器提供了一个更好的空间概览，从而加快搜索过程。
- en: '* * *'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.) Li, Lisha, et al., “Hyperband: A Novel Bandit-Based Approach to Hyperparameter
    Optimization,” *The Journal of Machine Learning Research* 18, no. 1 (2017): 6765-6816.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '(1.) 李丽沙，等，“Hyperband: 一种基于Bandit的新的超参数优化方法”，*《机器学习研究杂志》* 18，第1期（2017年）：6765-6816。'
