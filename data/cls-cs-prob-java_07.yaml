- en: 6 K-means clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 K-means 聚类
- en: Humanity has never had more data about more facets of society than it does today.
    Computers are great for storing data sets, but those data sets have little value
    to society until they are analyzed by human beings. Computational techniques can
    guide humans on the road to deriving meaning from a data set.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与今天相比，人类从未拥有过更多关于社会各个方面的数据。计算机非常适合存储数据集，但这些数据集在人类分析之前对社会几乎没有价值。计算技术可以帮助人类在从数据集中提取意义的过程中找到方向。
- en: Clustering is a computational technique that divides the points in a data set
    into groups. A successful clustering results in groups that contain points that
    are related to one another. Whether those relationships are meaningful generally
    requires human verification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种计算技术，它将数据集中的点划分为组。成功的聚类结果会产生包含相互关联点的组。这些关系是否有意义通常需要人类验证。
- en: In clustering, the group (a.k.a. cluster) that a data point belongs to is not
    predetermined, but instead is decided during the run of the clustering algorithm.
    In fact, the algorithm is not guided to place any particular data point in any
    particular cluster by presupposed information. For this reason, clustering is
    considered an unsupervised method within the realm of machine learning. You can
    think of unsupervised as meaning not guided by foreknowledge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，数据点所属的组（即聚类）不是预先确定的，而是在聚类算法运行过程中决定的。事实上，算法没有通过预设信息引导将任何特定的数据点放置在任何特定的聚类中。因此，聚类被认为是机器学习领域中的无监督方法。你可以将无监督理解为没有先验知识的指导。
- en: Clustering is a useful technique when you want to learn about the structure
    of a data set but you do not know ahead of time its constituent parts. For example,
    imagine you own a grocery store, and you collect data about customers and their
    transactions. You want to run mobile advertisements of specials at relevant times
    of the week to bring customers into your store. You could try clustering your
    data by day of the week and demographic information. Perhaps you will find a cluster
    that indicates younger shoppers prefer to shop on Tuesdays, and you could use
    that information to run an ad specifically targeting them on that day.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想了解数据集的结构但事先不知道其组成部分时，聚类是一种有用的技术。例如，想象你拥有一家杂货店，你收集了有关客户及其交易的数据。你希望在相关的时间段运行移动广告，以吸引顾客到你的店里。你可以尝试通过一周中的某一天和人口统计信息来聚类你的数据。也许你会发现一个表明年轻购物者更喜欢在周二购物的组，你可以利用这个信息在那天专门针对他们投放广告。
- en: 6.1 Preliminaries
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 前言
- en: Our clustering algorithm will require some statistical primitives (mean, standard
    deviation, and so on). Since Java version 8, the Java standard library provides
    several useful statistical primitives via the class DoubleSummaryStatistics in
    the util package. We will use these primitives to develop some more sophisticated
    statistics. It should be noted that while we keep to the standard library in this
    book, there are many useful third-party statistics libraries for Java that should
    be utilized in performance-critical applications--notably, those dealing with
    big data. A veteran, battle-tested library will almost always be a better choice
    in terms of performance and capability than rolling your own. However, in this
    book we’re in the business of learning by rolling our own.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚类算法将需要一些统计原语（均值、标准差等）。自 Java 8 版本以来，Java 标准库通过 util 包中的 DoubleSummaryStatistics
    类提供了几个有用的统计原语。我们将使用这些原语来开发更复杂的统计方法。需要注意的是，尽管我们在本书中坚持使用标准库，但还有许多有用的第三方 Java 统计库，应在性能关键的应用中使用——特别是那些处理大数据的应用。一个经验丰富、经过实战考验的库在性能和能力方面几乎总是比自行开发要好。然而，在本书中，我们致力于通过自行开发来学习。
- en: For simplicity’s sake, the data sets we will work with in this chapter are all
    expressible with the double type, or its object equivalent, Double. In the Statistics
    class that follows, the statistical primitives sum(), mean(), max(), and min()
    are implemented via DoubleSummaryStatistics. variance(), std() (standard deviation),
    and zscored() are built on top of these primitives. Their definitions follow directly
    from the formulas you would find in a statistics textbook.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，本章中我们将使用双精度类型或其对象等价物 Double 来表示我们将要处理的所有数据集。在接下来的 Statistics 类中，统计原语
    sum()、mean()、max() 和 min() 是通过 DoubleSummaryStatistics 实现的。variance()、std()（标准差）和
    zscored() 是在这些原语之上构建的。它们的定义直接来自你可以在统计学教科书中找到的公式。
- en: Listing 6.1 Statistics.java
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 Statistics.java
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip variance() finds the variance of a population. A slightly different formula,
    which we are not using, finds the variance of a sample. We will always be evaluating
    the entire population of data points at one time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`variance()` 函数用于计算总体的方差。一个略有不同的公式，我们目前没有使用，用于计算样本的方差。我们总是会在一次评估整个数据点的总体。'
- en: zscored() transforms each item in the list into its z-score, which is the number
    of standard deviations the original value is from the data set’s mean. There will
    be more about z-scores later in the chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`zscored()` 函数将列表中的每个项目转换为它的 z 分数，即原始值与数据集平均值的标准差数。关于 z 分数的更多内容将在本章后面介绍。'
- en: Note It is beyond the purview of this book to teach elementary statistics, but
    you do not need more than a rudimentary understanding of mean and standard deviation
    to follow the rest of the chapter. If it has been a while and you need a refresher,
    or you never previously learned these terms, it may be worthwhile to quickly peruse
    a statistics resource that explains these two fundamental concepts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本书的范围不包括教授基础统计学，但你不需要对均值和标准差有超过初步的了解就能理解本章的其余内容。如果你已经有一段时间没有接触过这些内容，或者你以前从未学习过这些术语，快速浏览一下解释这两个基本概念的统计学资源可能是有益的。
- en: All clustering algorithms work with points of data, and our implementation of
    k-means will be no exception. We will define a common base class called DataPoint.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聚类算法都使用数据点，我们的 k-means 实现也不例外。我们将定义一个名为 DataPoint 的通用基类。
- en: Listing 6.2 DataPoint.java
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 DataPoint.java
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Every data point must be human-readable for debug printing (toString()). Every
    data point type has a certain number of dimensions (numDimensions). The list dimensions
    stores the actual values for each of those dimensions as doubles. The constructor
    takes a list of initial values. These dimensions may later be replaced with z-scores
    by k-means, so we also keep a copy of the initial data in originals for later
    printing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点都必须是可读的，以便进行调试打印（`toString()`）。每个数据点类型都有一定数量的维度（`numDimensions`）。`dimensions`
    列表存储了这些维度实际值的列表，作为双精度浮点数。构造函数接受一个初始值的列表。这些维度可能后来会被 k-means 替换为 z 分数，因此我们也保留了一份初始数据的副本，以便稍后打印。
- en: One final preliminary we need before we can dig into k-means is a way of calculating
    the distance between any two data points of the same type. There are many ways
    to calculate distance, but the form most commonly used with k-means is Euclidean
    distance. This is the distance formula familiar to most from a grade-school course
    in geometry, derivable from the Pythagorean theorem. In fact, we already discussed
    the formula and derived a version of it for two-dimensional spaces in chapter
    2, where we used it to find the distance between any two locations within a maze.
    Our version for DataPoint needs to be slightly more sophisticated, because a DataPoint
    can involve any number of dimensions. The squares of each of the differences are
    summed, and the final value returned by distance() is the square root of this
    sum.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究 k-means 之前，我们还需要一个计算相同类型任意两个数据点之间距离的方法。有许多计算距离的方法，但与 k-means 最常使用的形式是欧几里得距离。这是大多数人在几何课程中学到的距离公式，可以从勾股定理推导出来。实际上，我们在第
    2 章中已经讨论了该公式，并推导出适用于二维空间的一个版本，我们用它来找到迷宫中任意两个位置之间的距离。我们为 DataPoint 的版本需要稍微复杂一些，因为
    DataPoint 可以涉及任意数量的维度。每个差值的平方被求和，`distance()` 函数返回的最终值是这个和的平方根。
- en: 6.2 The k-means clustering algorithm
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 K-means 聚类算法
- en: K-means is a clustering algorithm that attempts to group data points into a
    certain predefined number of clusters. In every round of k-means, the distance
    between every data point and every center of a cluster (a point known as a centroid)
    is calculated. Points are assigned to the cluster whose centroid they are closest
    to. Then the algorithm recalculates all of the centroids, finding the mean of
    each cluster’s assigned points and replacing the old centroid with the new mean.
    The process of assigning points and recalculating centroids continues until the
    centroids stop moving or a certain number of iterations occurs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是一种聚类算法，试图将数据点分组到预定义的特定数量的簇中。在 k-means 的每一轮中，计算每个数据点与每个簇中心（称为质心）之间的距离。点被分配到它们最近的质心所在的簇。然后算法重新计算所有质心，找到每个簇分配点的平均值，并用新的平均值替换旧的质心。分配点和重新计算质心的过程会继续进行，直到质心停止移动或发生一定数量的迭代。
- en: Each dimension of the initial points provided to k-means needs to be comparable
    in magnitude. If not, k-means will skew toward clustering based on dimensions
    with the largest differences. The process of making different types of data (in
    our case, different dimensions) comparable is known as normalization. One common
    way of normalizing data is to evaluate each value based on its z-score (also known
    as standard score) relative to the other values of the same type. A z-score is
    calculated by taking a value, subtracting the mean of all of the values from it,
    and dividing that result by the standard deviation of all of the values. The zscored()
    function devised near the beginning of the previous section does exactly this
    for every value in a list of doubles.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给 k-means 的初始点的每个维度需要具有可比的幅度。如果不具有可比性，k-means 将偏向于基于具有最大差异的维度进行聚类。将不同类型的数据（在我们的案例中，是不同的维度）进行比较的过程称为归一化。归一化数据的一种常见方法是基于其
    z 分数（也称为标准分数）评估每个值相对于同一类型其他值的相对值。z 分数是通过取一个值，从中减去所有值的平均值，然后将该结果除以所有值的方差来计算的。上一节开头设计的
    zscored() 函数正是对列表中的每个双精度值执行此操作。
- en: The main difficulty with k-means is choosing how to assign the initial centroids.
    In the most basic form of the algorithm, which is what we will be implementing,
    the initial centroids are placed randomly within the range of the data. Another
    difficulty is deciding how many clusters to divide the data into (the “k” in k-means).
    In the classical algorithm, that number is determined by the user, but the user
    may not know the right number, and this will require some experimentation. We
    will let the user define “k.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 的主要困难在于选择如何分配初始质心。在算法的最基本形式中，这是我们将要实现的，初始质心被放置在数据范围内的随机位置。另一个困难是决定将数据分成多少个簇（k-means
    中的“k”）。在经典算法中，这个数字由用户确定，但用户可能不知道正确的数字，这需要一些实验。我们将允许用户定义“k”。
- en: 'Putting all of these steps and considerations together, here is our k-means
    clustering algorithm:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些步骤和考虑因素综合起来，以下是我们的 k-means 聚类算法：
- en: Initialize all of the data points and “k” empty clusters.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有数据点和“k”个空簇。
- en: Normalize all of the data points.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化所有数据点。
- en: Create random centroids associated with each cluster.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个簇创建随机质心。
- en: Assign each data point to the cluster of the centroid it is closest to.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配到与其最近的质心的簇中。
- en: Recalculate each centroid so it is the center (mean) of the cluster it is associated
    with.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算每个质心，使其成为其关联簇的中心（均值）。
- en: Repeat steps 4 and 5 until a maximum number of iterations is reached or the
    centroids stop moving (convergence).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 4 和 5，直到达到最大迭代次数或质心停止移动（收敛）。
- en: 'Conceptually, k-means is actually quite simple: In each iteration, every data
    point is associated with the cluster that it is closest to in terms of the cluster’s
    center. That center moves as new points are associated with the cluster. This
    is illustrated in figure 6.1.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，k-means 实际上非常简单：在每次迭代中，每个数据点都与它最接近的簇的中心相关联。随着新点与簇相关联，这个中心会移动。这如图 6.1 所示。
- en: We will implement a class for maintaining state and running the algorithm, similar
    to GeneticAlgorithm in chapter 5\. We will start with an internal class to represent
    a cluster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个用于维护状态和运行算法的类，类似于第 5 章中的 GeneticAlgorithm。我们将从一个内部类开始，用于表示簇。
- en: Listing 6.3 KMeans.java
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 KMeans.java
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![6-1](../Images/6-1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![6-1](../Images/6-1.png)'
- en: Figure 6.1 An example of k-means running through three generations on an arbitrary
    data set. Stars indicate centroids. Colors and shapes represent current cluster
    membership (which changes).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 k-means 在任意数据集上运行三个代次的示例。星号表示质心。颜色和形状表示当前簇成员资格（这会变化）。
- en: KMeans is a generic class. It works with DataPoint or any subclass of DataPoint,
    as defined by the Point type’s bound (Point extends DataPoint). It has an internal
    class, Cluster, that keeps track of the individual clusters in the operation.
    Each Cluster has data points and a centroid associated with it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans 是一个泛型类。它与 DataPoint 或任何 DataPoint 的子类一起工作，如 Point 类型边界所定义的（Point extends
    DataPoint）。它有一个内部类 Cluster，用于跟踪操作中的各个簇。每个 Cluster 都与其关联的数据点和质心相关。
- en: NOTE In this chapter, to condense the code size and make it more readable, we
    are letting some instance variables be public that would normally be accessed
    through getters/setters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，为了压缩代码大小并使其更易于阅读，我们允许一些实例变量是公共的，而通常这些变量是通过 getters/setters 访问的。
- en: Now we will continue with the outer class’s constructor.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续外部类的构造函数。
- en: Listing 6.4 KMeans.java continued
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 KMeans.java继续
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: KMeans has an array, points, associated with it. This is all of the points in
    the data set. The points are further divided between the clusters, which are stored
    in the appropriately titled clusters variable. When KMeans is instantiated, it
    needs to know how many clusters to create (k). Every cluster initially has a random
    centroid. All of the data points that will be used in the algorithm are normalized
    by z-score. The centroids method returns all of the centroids associated with
    the clusters that are associated with the algorithm.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans有一个与之关联的数组，points。这是数据集中的所有点。这些点进一步分为簇，存储在名为clusters的相应变量中。当KMeans实例化时，它需要知道要创建多少个簇（k）。每个簇最初都有一个随机中心点。算法中将使用的所有数据点都通过z-score进行归一化。centroids方法返回与算法关联的簇的所有中心点。
- en: Listing 6.5 KMeans.java continued
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 KMeans.java继续
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: dimensionSlice() is a convenience method that can be thought of as returning
    a column of data. It will return a list composed of every value at a particular
    index in every data point. For instance, if the data points were of type DataPoint,
    then dimensionSlice(0) would return a list of the value of the first dimension
    of every data point. This is helpful in the following normalization method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: dimensionSlice()是一个便利方法，可以将其视为返回数据的一列。它将返回一个列表，包含每个数据点中特定索引处的每个值。例如，如果数据点是DataPoint类型，那么dimensionSlice(0)将返回每个数据点第一维值的列表。这在以下归一化方法中很有帮助。
- en: Listing 6.6 KMeans.java continued
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 KMeans.java继续
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: zScoreNormalize() replaces the values in the dimensions list of every data point
    with its z-scored equivalent. This uses the zscored() function that we defined
    for lists of double earlier. Although the values in the dimensions list are replaced,
    the originals list in the DataPoint are not. This is useful; the user of the algorithm
    can still retrieve the original values of the dimensions before normalization
    after the algorithm runs if they are stored in both places.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: zScoreNormalize()将每个数据点的维度列表中的值替换为其z-score等价值。这使用了我们在之前为double列表定义的zscored()函数。尽管维度列表中的值被替换，但DataPoint中的原始列表originals不会被替换。这很有用；算法的用户在算法运行后，如果它们存储在两个地方，仍然可以检索归一化之前的原始维度值。
- en: Listing 6.7 KMeans.java continued
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 KMeans.java继续
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding randomPoint() method is used in the constructor to create the
    initial random centroids for each cluster. It constrains the random values of
    each point to be within the range of the existing data points’ values. It uses
    the constructor we specified earlier on DataPoint to create a new point from a
    list of values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的randomPoint()方法在构造函数中使用，为每个簇创建初始的随机中心点。它将每个点的随机值限制在现有数据点值范围内。它使用我们之前在DataPoint中指定的构造函数，从一个值列表中创建一个新的点。
- en: Now we will look at our method that finds the appropriate cluster for a data
    point to belong to.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看我们的方法，该方法用于找到数据点所属的适当簇。
- en: Listing 6.8 KMeans.java continued
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 KMeans.java继续
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Throughout the book, we have created several functions that find the minimum
    or find the maximum in a list. This one is not dissimilar. In this case we are
    looking for the cluster centroid that has the minimum distance to each individual
    point. The point is then assigned to that cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们创建了几个函数，用于在列表中查找最小值或最大值。这个函数与此类似。在这种情况下，我们正在寻找与每个单独的点距离最小的簇中心点。然后，该点被分配到那个簇。
- en: Listing 6.9 KMeans.java continued
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 KMeans.java继续
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After every point is assigned to a cluster, the new centroids are calculated.
    This involves calculating the mean of each dimension of every point in the cluster.
    The means of these dimensions are then combined to find the “mean point” in the
    cluster, which becomes the new centroid. Note that we cannot use dimensionSlice()
    here, because the points in question are a subset of all of the points (just those
    belonging to a particular cluster). How could dimensionSlice() be rewritten to
    be more generic? We will leave thinking about this as an exercise for the reader.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个点被分配到簇之后，将计算新的中心点。这涉及到计算簇中每个点的每个维度的平均值。这些维度的平均值然后组合起来，找到簇中的“平均点”，这成为新的中心点。请注意，我们在这里不能使用dimensionSlice()，因为所讨论的点只是所有点的一个子集（仅属于特定簇）。dimensionSlice()应该如何重写以使其更通用？我们将把这个思考留给读者作为练习。
- en: Now let’s look at the method (and a helper) that will actually execute the algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看实际执行算法的方法（以及一个辅助函数）。
- en: Listing 6.10 KMeans.java continued
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 KMeans.java继续
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: run() is the purest expression of the original algorithm. The only change to
    the algorithm you may find unexpected is the removal of all points at the beginning
    of each iteration. If this were not to occur, the assignClusters() method, as
    written, would end up putting duplicate points in each cluster. listsEqual() is
    a helper that checks if two lists of DataPoints are holding the same points. This
    is useful for checking if there were no changes to the centroids between generations
    (indicating movement has ceased, and the algorithm should stop).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: run()是原始算法最纯粹的表达。你可能发现的唯一意外的算法更改是在每次迭代的开始移除所有点。如果不这样做，按照目前的写法，assignClusters()方法最终会在每个聚类中放入重复的点。listsEqual()是一个辅助函数，用于检查两个DataPoints列表是否持有相同的点。这对于检查在代之间没有变化到质心（表明移动已停止，算法应该停止）非常有用。
- en: You can perform a quick test using test DataPoints and k set to 2.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用测试数据点和k设置为2进行快速测试。
- en: Listing 6.11 KMeans.java continued
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 KMeans.java继续
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Because there is randomness involved, your results may vary. The expected result
    is something along these lines:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因为涉及随机性，你的结果可能会有所不同。预期结果大致如下：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 6.3 Clustering governors by age and longitude
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 按年龄和经度聚类州长
- en: Every American state has a governor. In June 2017, those governors ranged in
    age from 42 to 79\. If we take the United States from east to west, looking at
    each state by its longitude, perhaps we can find clusters of states with similar
    longitudes and similar-age governors. Figure 6.2 is a scatter plot of all 50 governors.
    The x-axis is state longitude, and the y-axis is governor age.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个美国州都有一个州长。截至2017年6月，这些州长的年龄从42岁到79岁不等。如果我们从东到西看美国，按每个州的经度来看，也许我们可以找到具有相似经度和相似年龄州长的州集群。图6.2是所有50个州长的散点图。x轴是州经度，y轴是州长年龄。
- en: Are there any obvious clusters in figure 6.2? In this figure, the axes are not
    normalized. Instead, we are looking at raw data. If clusters were always obvious,
    there would be no need for clustering algorithms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2中是否有任何明显的聚类？在这个图中，轴没有归一化。相反，我们正在查看原始数据。如果聚类总是明显的，那么聚类算法就没有必要了。
- en: '![6-2](../Images/6-2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![6-2](../Images/6-2.png)'
- en: Figure 6.2 State governors, as of June 2017, plotted by state longitude and
    governor age
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 2017年6月的州长，按州经度和州长年龄绘制
- en: Let’s try running this data set through k-means. First, we will need a way of
    representing an individual data point.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试运行这个数据集通过k-means。首先，我们需要一种表示单个数据点的方法。
- en: Listing 6.12 Governor.java
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 Governor.java
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A Governor has two named and stored dimensions: longitude and age. Other than
    that, Governor makes no modifications to the machinery of its superclass, DataPoint,
    other than an overridden toString() for pretty-printing. It would be pretty unreasonable
    to enter the following data manually, so check out the source code repository
    that accompanies this book.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 州长有两个命名并存储的维度：经度和年龄。除此之外，州长对其超类DataPoint的机制没有进行任何修改，除了重写toString()方法以实现美观打印。手动输入以下数据显然是不合理的，因此请查看本书附带的源代码仓库。
- en: Listing 6.13 Governor.java continued
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.13 Governor.java继续
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We will run k-means with k set to 2.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用k设置为2来运行k-means。
- en: Listing 6.14 Governor.java continued
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.14 Governor.java继续
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Because it starts with randomized centroids, every run of KMeans may potentially
    return different clusters. It takes some human analysis to see if the clusters
    are actually relevant. The following result is from a run that did have an interesting
    cluster:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它从随机的质心开始，所以每次运行KMeans都可能产生不同的聚类。需要一些人为分析才能看到聚类是否真正相关。以下结果是从一个确实有有趣聚类的运行中得到的：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Cluster 1 represents the extreme Western states, all geographically next to
    each other (if you consider Alaska and Hawaii next to the Pacific coast states).
    They all have relatively old governors and hence formed an interesting cluster.
    Do folks on the Pacific Rim like older governors? We cannot determine anything
    conclusive from these clusters beyond a correlation. Figure 6.3 illustrates the
    result. Squares are cluster 1, and circles are cluster 0.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第1个聚类代表极端的西部州，它们在地理上相邻（如果你认为阿拉斯加和夏威夷与太平洋沿岸州相邻）。它们都有相对较老的州长，因此形成了一个有趣的聚类。太平洋沿岸的人们是否喜欢较老的州长？我们不能从这些聚类中得出任何有结论性的东西，而只是相关性。图6.3展示了结果。正方形代表第1个聚类，圆圈代表第0个聚类。
- en: '![6-3](../Images/6-3.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![6-3](../Images/6-3.png)'
- en: Figure 6.3 Data points in cluster 0 are designated by circles, and data points
    in cluster 1 are designated by squares.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 集群 0 的数据点用圆圈表示，集群 1 的数据点用方块表示。
- en: Tip It cannot be emphasized enough that your results with k-means using random
    initialization of centroids will vary. Be sure to run k-means multiple times with
    any data set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：使用随机初始化质心进行 k-means 聚类时，结果可能会有很大的变化。请确保对任何数据集运行 k-means 多次。
- en: 6.4 Clustering Michael Jackson albums by length
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 按长度聚类迈克尔·杰克逊的专辑
- en: 'Michael Jackson released 10 solo studio albums. In the following example, we
    will cluster those albums by looking at two dimensions: album length (in minutes)
    and number of tracks. This example is a nice contrast with the preceding governors
    example because it is easy to see the clusters in the original data set without
    even running kmeans. An example like this can be a good way of debugging an implementation
    of a clustering algorithm.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 迈克尔·杰克逊发行了 10 张个人录音室专辑。在以下示例中，我们将通过查看两个维度：专辑长度（以分钟计）和曲目数量来对这些专辑进行聚类。这个示例与前面的州长示例形成了一个很好的对比，因为它甚至在没有运行
    kmeans 的情况下也能很容易地看到原始数据集中的聚类。这样的示例可以是一个很好的调试聚类算法实现的方法。
- en: NOTE Both of the examples in this chapter make use of two-dimensional data points,
    but k-means can work with data points of any number of dimensions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章中的两个示例都使用了二维数据点，但 k-means 可以与任何维度的数据点一起工作。
- en: 'The example is presented here in its entirety as one code listing. If you look
    at the album data in the following code listing before even running the example,
    it is clear that Michael Jackson made longer albums toward the end of his career.
    So the two clusters of albums should probably be divided between earlier albums
    and later albums. HIStory: Past, Present, and Future, Book I is an outlier and
    can also logically end up in its own solo cluster. An outlier is a data point
    that lies outside the normal limits of a data set.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '以下示例以一个代码列表的形式完整呈现。在运行示例之前，如果你查看以下代码列表中的专辑数据，很明显迈克尔·杰克逊在其职业生涯的后期制作了较长的专辑。因此，这些专辑的两个聚类可能应该分别分为早期专辑和后期专辑。HIStory:
    Past, Present, and Future, Book I 是一个异常值，也可以逻辑上结束在自己的个人专辑聚类中。异常值是位于数据集正常范围之外的数据点。'
- en: Listing 6.15 Album.java
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.15 Album.java
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that the attributes name and year are only recorded for labeling purposes
    and are not included in the actual clustering. Here is an example output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，属性名称和年份仅用于标记目的，不包括在实际的聚类中。以下是一个示例输出：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The reported cluster averages are interesting. Note that the averages are z-scores.
    Cluster 1’s three albums, Michael Jackson’s final three albums, were about one
    standard deviation longer than the average of all ten of his solo albums.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的聚类平均值很有趣。请注意，平均值是 z 分数。聚类 1 的三个专辑，迈克尔·杰克逊的最后三个专辑，比他所有十张个人专辑的平均值长了一个标准差。
- en: 6.5 K-means clustering problems and extensions
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 K-means 聚类问题及其扩展
- en: When k-means clustering is implemented using random starting points, it may
    completely miss useful points of division within the data. This often results
    in a lot of trial and error for the operator. Figuring out the right value for
    “k” (the number of clusters) is also difficult and error prone if the operator
    does not have good insight into how many groups of data should exist.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用随机起始点实现 k-means 聚类时，它可能会完全错过数据中的有用分割点。这通常会导致操作员进行大量的试错。如果操作员没有对应该存在多少数据组有良好的洞察力，那么确定“k”的正确值（集群数量）也是困难且容易出错的。
- en: There are more sophisticated versions of k-means that can try to make educated
    guesses or do automatic trial and error regarding these problematic variables.
    One popular variant is k-means++, which attempts to solve the initialization problem
    by choosing centroids based on a probability distribution of distance to every
    point instead of pure randomness. An even better option for many applications
    is to choose good starting regions for each of the centroids based on information
    about the data that is known ahead of time--in other words, a version of k-means
    where the user of the algorithm chooses the initial centroids.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着更复杂的 k-means 版本，它们可以尝试对这些有问题的变量做出有根据的猜测或进行自动的试错。一个流行的变体是 k-means++，它通过根据每个点到所有点的距离的概率分布来选择质心，而不是完全随机，从而尝试解决初始化问题。对于许多应用来说，根据事先已知的数据信息为每个质心选择良好的起始区域是一个更好的选择——换句话说，这是一种
    k-means 版本，其中算法的用户选择初始质心。
- en: The runtime for k-means clustering is proportional to the number of data points,
    the number of clusters, and the number of dimensions of the data points. It can
    become unusable in its basic form when there are a high number of points that
    have a large number of dimensions. There are extensions that try to not do as
    much calculation between every point and every center by evaluating whether a
    point really has the potential to move to another cluster before doing the calculation.
    Another option for numerous-point or high-dimension data sets is to run just a
    sampling of the data points through k-means. This will approximate the clusters
    that the full k-means algorithm may find.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类的运行时间与数据点的数量、聚类的数量以及数据点的维度数成正比。当有大量具有大量维度的点时，其基本形式可能变得无法使用。有一些扩展尝试通过评估在计算之前一个点是否真的有可能移动到另一个聚类，来减少每一点和每个中心之间的计算量。对于大量点或高维数据集的另一个选项是仅对数据点进行抽样，通过k-means。这将近似于完整k-means算法可能找到的聚类。
- en: Outliers in a data set may result in strange results for k-means. If an initial
    centroid happens to fall near an outlier, it could form a cluster of one (as could
    potentially happen with the HIStory album in the Michael Jackson example). K-means
    may run better with outliers removed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的异常值可能会导致k-means出现奇怪的结果。如果初始质心恰好位于异常值附近，它可能会形成一个只有一个点的聚类（正如在迈克尔·杰克逊的HIStory专辑例子中可能发生的那样）。移除异常值后，k-means可能会运行得更好。
- en: Finally, the mean is not always considered a good measure of the center. K-medians
    looks at the median of each dimension, and k-medoids uses an actual point in the
    data set as the middle of each cluster. There are statistical reasons beyond the
    scope of this book for choosing each of these centering methods, but common sense
    dictates that for a tricky problem it may be worth trying each of them and sampling
    the results. The implementations of each are not that different.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，均值并不总是被认为是衡量中心的好方法。K-medians考虑每个维度的中位数，而k-medoids则使用数据集中的实际点作为每个聚类的中心。选择这些中心方法背后的统计原因超出了本书的范围，但常识告诉我们，对于复杂的问题，尝试每种方法并采样结果可能是有价值的。每种方法的实现并没有太大的不同。
- en: 6.6 Real-world applications
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 现实世界应用
- en: Clustering is often the purview of data scientists and statistical analysts.
    It is used widely as a way to interpret data in a variety of fields. K-means clustering,
    in particular, is a useful technique when little is known about the structure
    of the data set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类通常是数据科学家和统计分析师的领域。它在各个领域广泛用作解释数据的方式。特别是，当对数据集的结构知之甚少时，k-means聚类是一种有用的技术。
- en: In data analysis, clustering is an essential technique. Imagine a police department
    that wants to know where to put cops on patrol. Imagine a fast-food franchise
    that wants to figure out where its best customers are, to send promotions. Imagine
    a boat-rental operator that wants to minimize accidents by analyzing when they
    occur and who causes them. Now imagine how they could solve their problems using
    clustering.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，聚类是一种基本技术。想象一个警察局想知道在哪里部署警察巡逻。想象一个快餐连锁店想要找出其最佳顾客在哪里，以便发送促销活动。想象一个船只租赁运营商通过分析事故发生的时间和原因来最小化事故。现在想象他们如何使用聚类来解决他们的问题。
- en: Clustering helps with pattern recognition. A clustering algorithm may detect
    a pattern that the human eye misses. For instance, clustering is sometimes used
    in biology to identify groups of incongruous cells.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类有助于模式识别。聚类算法可能会检测到人眼忽略的模式。例如，聚类有时在生物学中用于识别不一致的细胞群。
- en: In image recognition, clustering helps to identify non-obvious features. Individual
    pixels can be treated as data points with their relationship to one another being
    defined by distance and color difference.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别中，聚类有助于识别非明显特征。单个像素可以被当作数据点，它们之间的关系由距离和颜色差异定义。
- en: In political science, clustering is sometimes used to find voters to target.
    Can a political party find disgruntled voters concentrated in a single district
    that they should focus their campaign dollars on? What issues are similar voters
    likely to be concerned about?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在政治学中，聚类有时用于寻找目标选民。政党能否找到一个选民不满的单一区域，他们应该在这个区域集中他们的竞选资金？相似选民可能关注哪些问题？
- en: 6.7 Exercises
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 练习
- en: Create a function that can import data from a CSV file into DataPoints.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，可以将数据从CSV文件导入到DataPoints中。
- en: Create a function using a GUI framework (like AWT, Swing, or JavaFX) or a graphing
    library that creates a color-coded scatter plot of the results of any run of KMeans
    on a two-dimensional data set.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GUI框架（如AWT、Swing或JavaFX）或绘图库创建一个函数，该函数可以创建一个对KMeans在二维数据集上任何运行结果的彩色散点图。
- en: Create a new initializer for KMeans that takes initial centroid positions instead
    of assigning them randomly.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为KMeans创建一个新的初始化器，它接受初始质心位置而不是随机分配。
- en: Research and implement the k-means++ algorithm
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究并实现k-means++算法
