- en: Chapter 2\. Data Quality Monitoring Strategies and the Role of Automation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 数据质量监控策略及自动化的角色
- en: There are many different ways you can approach data quality monitoring. Before
    evaluating the options, it helps to think about what success looks like. In this
    chapter, we’ll define the requirements for success. Then we’ll walk through the
    traditional strategies—manual checks, rule-based testing, and metrics monitoring—and
    see how they measure up.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以用来处理数据质量监控。在评估选项之前，思考成功的标志很有帮助。在本章中，我们将定义成功的要求。然后，我们将详细讨论传统策略——手动检查、基于规则的测试和指标监控——以及它们的测量结果。
- en: Next, we’ll explore the idea of *automating* data quality monitoring. We’ll
    explain how unsupervised machine learning can help us satisfy some missing aspects
    of our success criteria, scaling monitoring to large amounts of data while reducing
    alert fatigue.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨*自动化*数据质量监控的概念。我们将解释无监督机器学习如何帮助我们满足成功标准的一些缺失方面，将监控扩展到大量数据，并减少警报疲劳。
- en: 'We’ll wrap up by introducing the data quality monitoring strategy we advocate
    for in this book: a four-pillar approach combining data observability, rule-based
    testing, metrics monitoring, and unsupervised machine learning. As we’ll show,
    this approach has many advantages. It allows subject matter experts (SMEs) to
    enforce essential constraints and track key performance indicators (KPIs) for
    important tables—all while providing a base level of monitoring for a large volume
    of diverse data. No server farms or legions of analysts required.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍本书中提倡的数据质量监控策略：一个四柱结构方法，结合数据可观性、基于规则的测试、指标监控和无监督机器学习。正如我们将展示的那样，这种方法有许多优势。它允许主题专家（SMEs）强制执行基本约束，并跟踪重要表格的关键绩效指标（KPIs）——所有这些同时为大量多样化数据提供基础级别的监控，而无需服务器农场或大量分析师。
- en: Monitoring Requirements
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控要求
- en: 'To address the vast array of problems outlined in [Chapter 1](ch01.html#the_data_quality_imperative),
    a successful data quality monitoring strategy must deliver across four dimensions
    (as shown in [Figure 2-1](#a_data_quality_monitoring_solution_shou)):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决[第一章](ch01.html#the_data_quality_imperative)中概述的各种问题，一个成功的数据质量监控策略必须在四个维度上交付成果（如[图 2-1](#a_data_quality_monitoring_solution_shou)所示）：
- en: First, it must *detect* quality issues in all your important data so that you
    can be confident no issues are slipping through the cracks—whether they appear
    at the level of tables, columns, or individual rows.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一，它必须*检测*所有重要数据的质量问题，以便您可以确信没有任何问题被忽视——无论这些问题出现在表格、列还是单个行的级别。
- en: Second, it must *alert* the right people in a timely manner when there is a
    real issue, without causing *alert fatigue* by notifying people about nonissues.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，当出现真正问题时，它必须*及时*向相关人员*发出警报*，同时避免由于非问题而导致*警报疲劳*。
- en: Third, it must help you *resolve* issues quickly and efficiently. (For a complete
    list of what these issues could be, see the [Appendix A](app01.html#types_of_data_quality_issues).)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，它必须帮助您*迅速和高效地解决*问题。（要查看这些问题的完整列表，请参阅[附录 A](app01.html#types_of_data_quality_issues)。）
- en: And finally, it must *scale* to monitor the health of your data enterprise-wide.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它必须*扩展*以监控企业范围内数据的健康状况。
- en: '![A data quality monitoring solution should succeed across the four dimensions
    of detect, alert, resolve, and scale.](assets/adqm_0201.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![数据质量监控解决方案应跨越检测、警报、解决和扩展四个维度取得成功。](assets/adqm_0201.png)'
- en: Figure 2-1\. A data quality monitoring solution should succeed across the four
    dimensions of detect, alert, resolve, and scale.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 数据质量监控解决方案应跨越检测、警报、解决和扩展四个维度取得成功。
- en: 'Data Observability: Necessary, but Not Sufficient'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可观性：必要但不充分
- en: Before talking about data quality monitoring approaches, let’s touch on data
    observability. It’s a key part of any comprehensive data quality strategy, with
    the important distinction that data observability *monitors metadata about your
    tables, not the contents of the data itself*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论数据质量监控方法之前，让我们先了解一下数据可观性。这是任何全面的数据质量战略的关键部分，其重要区别在于数据可观性*监控关于你的表格的元数据，而不是数据内容本身*。
- en: 'Data observability is similar to infrastructure or networking observability
    but applied to data in a data warehouse. Namely, it’s about answering the following
    questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可观性类似于基础设施或网络可观性，但应用于数据仓库中的数据。换句话说，它解答以下问题：
- en: Does this table still exist?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表格还存在吗？
- en: Have there been any adverse changes to the schema of the table?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格的模式有任何不利的变化吗？
- en: Has the table been updated recently?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近是否更新过表格？
- en: Is the volume of data in the table consistent with my expectations?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格中的数据量是否与我的预期一致？
- en: These are important questions when determining whether you can trust your data,
    and you’ll notice that some of the data issues in the [Appendix A](app01.html#types_of_data_quality_issues)
    can be detected with observability alone.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当确定您是否可以信任您的数据时，这些是重要问题。您会注意到在 [附录 A](app01.html#types_of_data_quality_issues)
    中的一些数据问题仅通过可观测性就可以检测到。
- en: 'So, how does data observability work? Fortunately, the metadata it uses can
    be gathered from most modern data warehouses *without having to query the table
    at all*. There’s typically either an API that makes this data available or a system
    view that can be queried that keeps this data up to date. The data needed includes
    table-level statistics like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，数据可观测性是如何工作的呢？幸运的是，它使用的元数据可以从大多数现代数据仓库中收集，*甚至无需查询表格*。通常有一个 API 使得这些数据可用，或者可以查询的系统视图保持这些数据的更新。所需的数据包括表级统计数据，例如：
- en: Table
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格
- en: Last updated time
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后更新时间
- en: Number of rows (or size in bytes)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行数（或字节大小）
- en: 'And column-level information like:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以及列级信息如：
- en: Table
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格
- en: Column name
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列名
- en: Column type
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列类型
- en: The platform simply needs to capture observability metadata at regular intervals
    (say, every hour), and then it can answer questions about how the metadata is
    changing over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平台只需要定期（例如每小时）捕获可观测性元数据，然后可以回答关于元数据如何随时间变化的问题。
- en: 'For example, in Snowflake, you can retrieve the table-level metadata using
    this query:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Snowflake中，您可以使用以下查询检索表级元数据：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When it comes to “Has the table been updated recently?,” the system needs to
    make a tough decision: What does “recently” mean, and when has a delay become
    significant? Some tables are updated continuously using streaming data. Others
    are updated every hour. Others may be updated multiple times per day, or only
    multiple times per week (or month, or year). Time series models (as discussed
    in the sidebar [“Time Series Metric Monitoring”](#time_series_metric_monitoring))
    are a powerful tool here—they can use the history of updates to predict the expected
    upper bound for when the next update will arrive.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及“最近是否更新过表格？”时，系统需要做出艰难的决定：什么是“最近”，什么时候延迟变得显著？有些表格使用流数据进行持续更新，有些每小时更新一次，还有些可能一天多次更新，或者仅多次更新每周（或每月、每年）。时间序列模型（如侧边栏中讨论的
    [“时间序列度量监控”](#time_series_metric_monitoring) ）在这里是一个强大的工具——它们可以利用更新历史来预测下一次更新到达的预期上限。
- en: Something similar has to be done to decide if the volume of data “is consistent
    with expectations.” A time series model can observe the volume history and identify
    if the most recent update is anomalously low. Once these problems have been addressed,
    data observability can be applied to thousands, or even tens of thousands, of
    tables in a data warehouse.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的事情也必须做出来决定数据量“是否与预期一致”。时间序列模型可以观察数据量的历史，并识别最近更新是否异常低。一旦解决了这些问题，数据可观测性可以应用于数据仓库中的成千上万个表格。
- en: Does this mean you’ve solved data quality? Of course not! Data observability
    checks are just about the flow of data through your warehouse. They don’t address
    whether the contents of that data are high quality. Using only data observability
    would be akin to operating a water treatment plant where your only quality control
    was the water pressure you delivered—with no concern about whether that water
    was even potable! [Table 2-1](#the_differences_between_data_observabil) reiterates
    the key differences between data observability and data quality monitoring.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着您解决了数据质量问题？当然不是！数据可观测性检查只涉及数据通过您的仓库的流动。它们不解决数据内容是否高质量的问题。仅使用数据可观测性就像在运营一个水处理厂，您的唯一质量控制是提供的水压——完全不关心这些水是否适用于饮用！
    [表格 2-1](#the_differences_between_data_observabil) 重申了数据可观测性和数据质量监控之间的主要区别。
- en: Table 2-1\. The differences between data observability and data quality monitoring
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-1\. 数据可观测性和数据质量监控之间的差异
- en: '|   | Data observability | Data quality monitoring |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|   | 数据可观测性 | 数据质量监控 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **What it answers** | Is data moving through my warehouse in a timely manner?
    | Is the data my warehouse produces of high quality? |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **它回答了什么问题** | 数据是否及时通过我的仓库？ | 我的仓库生成的数据是否高质量？ |'
- en: '| **How it works** | Catalog metadata Job monitoring'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '| **它是如何工作的** | 目录元数据 作业监控'
- en: Data lineage | Queries the data Requires experts or ML
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据血统 | 查询数据 需要专家或 ML
- en: Explainability is key |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性至关重要 |
- en: '| **Why it’s needed** | Catches data *movement* failures | Does deep monitoring
    of data values |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **为什么需要** | 捕捉数据的*移动*失败 | 对数据值进行深度监控 |'
- en: '| **Major drawback** | Ignores data *contents* | Difficult to scale |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **主要缺点** | 忽视数据的*内容* | 难以扩展 |'
- en: Traditional Approaches to Data Quality
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量的传统方法
- en: Many teams find it relatively easy to implement data observability for their
    entire warehouse but struggle when it comes to scaling their data quality monitoring.
    Historically, the most common ways that teams have approached data quality monitoring
    are through manual detection, rule-based testing, and metrics monitoring. While
    we present these strategies separately here, organizations often employ a mix
    of all three at once. There is value in each of these strategies, but also significant
    drawbacks at scale.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队发现，对整个数据仓库实施数据可观察性相对容易，但在扩展数据质量监控方面却遇到了困难。在历史上，团队通常通过手动检测、基于规则的测试和指标监控来监控数据质量。虽然我们在这里单独介绍了这些策略，但组织通常同时采用这三种策略的混合方法。每种策略都有其价值，但在规模化时也存在显著的缺陷。
- en: Manual Data Quality Detection
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动数据质量检测
- en: Since the invention of digital data, it’s been possible—but increasingly difficult—for
    humans to comb through data by hand and find potential issues.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自数字数据问世以来，人们已经有可能通过手工查找潜在问题，但这越来越困难。
- en: At some businesses, there’s an intentional process of manual data quality review,
    whether in the form of spot-checking, reviewing summaries, or looking at visualizations.
    This generally isn’t sufficient for monitoring data quality. Manual inspection
    might work when the data is small and simple enough that a human can look at a
    spreadsheet and quickly spot potential issues (by the way, [various studies report](https://oreil.ly/oEMGS)
    that nearly 90% of spreadsheets contain errors!). But it’s not effective at scale.
    Furthermore, a manual process is inherently subjective. Give the same complex
    dataset to 10 different analysts, and you’ll get a large number of divergent conclusions
    about the quality of the data they are evaluating.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些企业中，存在一个有意的手动数据质量审查流程，无论是抽样检查、审查总结还是查看可视化数据。这通常对于监控数据质量是不够的。当数据足够小且简单，以至于人类可以查看电子表格并迅速发现潜在问题时，手动检查可能是有效的（顺便说一句，[各种研究报告](https://oreil.ly/oEMGS)称，近90%的电子表格存在错误！）。但是，在规模化时，这种方法并不有效。此外，手动过程本质上是主观的。将同一复杂数据集分别交给10个不同的分析师，他们对所评估数据质量的结论可能会大相径庭。
- en: 'Manual data quality detection also happens in a very different way: by accident.
    Someone is in the midst of doing something with the data, and they “stumble upon”
    a data quality issue. Here are a few examples:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 手动数据质量检测也以一种截然不同的方式发生：偶然发现。有人在处理数据时，突然“发现”了一个数据质量问题。以下是一些例子：
- en: Computing summary statistics and comparing these to known figures or other reference
    data points
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 计算汇总统计数据并将其与已知数据或其他参考数据点进行比较
- en: For instance, a data scientist might find that the number of customers is 50%
    higher in an aggregated dataset than in another known source, indicating there
    must be a data quality issue.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据科学家可能会发现，在聚合数据集中，客户数量比另一个已知来源高出50%，这表明可能存在数据质量问题。
- en: Creating visualizations that summarize the data in ways that make it clear that
    there are data quality issues
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 创建可视化图表以总结数据，清楚地显示数据质量问题
- en: For example, a visualization of missing values over time may show a very sharp
    increase in recent weeks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，关于缺失值的时间序列可视化可能显示最近几周出现了非常明显的增加。
- en: Reaching conclusions from an analysis, or from interrogating models, that suggest
    things that are provably untrue
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从分析结论或模型询问中得出的结论，这些结论实际上是不可证明的。
- en: An analyst might find that growth in new accounts exceeded 1,000% per week in
    Europe for the last three weeks, and yet there is no possible way that could happen.
    Or an ML model might suggest that the most important feature in predicting user
    churn is a user’s date of birth—but on closer inspection, this is due to a large
    fraction of users being born on January 1, 1970 (the beginning of the Unix epoch,
    indicating bad data).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分析人员可能会发现，过去三周欧洲新账户的增长超过了每周1000%，但实际上这是不可能的。或者，一个机器学习模型可能会建议，预测用户流失的最重要特征是用户的出生日期，但仔细检查后发现，这是由于有很大一部分用户出生于1970年1月1日（Unix纪元的开始，表明存在错误数据）。
- en: Relying on analysts and data scientists to discover data quality issues as they
    work is not a winning strategy either. Practitioners will examine the data only
    periodically, and with a very specific purpose in mind. They will most likely
    catch data quality issues long after they’ve occurred and will almost certainly
    miss data quality issues that are outside the scope of their project.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 依靠分析师和数据科学家在工作过程中发现数据质量问题并不是一个成功的策略。从业者只会定期检查数据，并且有特定的目的。他们很可能会在数据质量问题发生很久之后才发现，并且几乎肯定会错过超出项目范围的数据质量问题。
- en: This kind of resolve-as-you-go approach is actually quite detrimental. In some
    organizations we’ve worked with, *more than 50%* of analysts’ time is dedicated
    to investigating and working around data quality issues. Not only does this manual
    work cut the team’s effectiveness in half, but over time it takes a huge toll
    on morale.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种边做边解决的方法实际上相当有害。在我们合作的一些组织中，分析师花费的时间*超过50%*用于调查和解决数据质量问题。这种手工作业不仅削弱了团队的效率，而且随着时间的推移，对士气造成了巨大的打击。
- en: All this being said, analysts and data scientists invariably want visibility
    into the data they work with, and depending on the situation, a manual review
    can add value. Humans are capable of bringing together disparate data sources
    and contextual knowledge and drawing conclusions in ways that algorithms are unable
    to automate. Plus, when data quality issues are found in the context of an analysis
    or ML model, they are by definition “important” data quality issues that need
    to be addressed—there’s no risk of false positives.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，分析师和数据科学家总是希望能够看到他们处理的数据，并且根据情况，手动审查可以增加价值。人类能够将不同的数据源和背景知识结合起来，并以算法无法自动化的方式得出结论。此外，当在分析或ML模型的背景中发现数据质量问题时，它们从定义上就是“重要”的数据质量问题，需要解决——没有误报的风险。
- en: Ultimately, whatever monitoring approach you choose should reduce manual effort
    and make it possible to monitor data at scale. However, it should still make it
    easy for humans to profile their data and spot issues manually, and you can aid
    this process by producing summary statistics and visualizations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，无论选择哪种监控方法，都应减少手工工作量，并使其能够扩展监控数据。然而，它仍应使人类能够轻松地对其数据进行概述和手动检测问题，并且您可以通过生成摘要统计信息和可视化来帮助此过程。
- en: Rule-Based Testing
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的测试
- en: When testing software, engineers write unit tests that invoke components, measure
    the actions taken, and apply deterministic rules to those measurements to validate
    that the software is working as expected. For example, an ecommerce application
    might have a method for computing the tax rate. A unit test could supply this
    method with various baskets of goods and store locations and ensure that it produces
    the right answer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试软件时，工程师编写单元测试来调用组件，测量所采取的操作，并对这些测量应用确定性规则，以验证软件是否按预期工作。例如，电子商务应用程序可能有一种计算税率的方法。单元测试可以向此方法提供各种商品篮子和商店位置，并确保它产生正确的答案。
- en: It’s natural to try to take what works for testing software—writing lots of
    unit tests—and apply that to data. We call this approach *rule-based testing*.
    Common tools for rule-based testing include Great Expectations and dbt. A rule-based
    test is a deterministic test that can be applied to data from a specific source.
    The data either passes the test or fails the test; there is no gray area in between.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将适用于测试软件的方法（编写大量单元测试）应用于数据是很自然的。我们称之为*基于规则的测试*。常见的基于规则的测试工具包括Great Expectations和dbt。基于规则的测试是一种确定性测试，可以应用于特定来源的数据。数据要么通过测试，要么未通过测试；中间没有灰色地带。
- en: 'Examples  of rule-based tests include:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于规则的测试包括：
- en: The column `number_of_tickets` in the table `ticket_sales` is never NULL.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表`ticket_sales`中的列`number_of_tickets`从不为NULL。
- en: There are no values in the column `listing_time` that are from the future.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列`listing_time`中没有来自未来的值。
- en: The average of the column `price_per_ticket` is always between 50 and 100 every
    day.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列`price_per_ticket`的平均值每天始终在50到100之间。
- en: The result of the equation `number_of_tickets * price_per_ticket * (1 + tax_rate)`
    is always equal to `total_price` for every row in the table `ticket_sales`.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等式`number_of_tickets * price_per_ticket * (1 + tax_rate)`对于表`ticket_sales`中的每一行始终等于`total_price`的结果。
- en: 'Every rule can be thought of as having a scope, a type, and (usually) a number
    of constraints:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则都可以看作具有范围、类型和（通常）若干约束条件：
- en: Scope
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 范围
- en: What data does the rule apply to? What data store? What table or view? What
    column(s)? For what time range? For which specific rows?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 规则适用于哪些数据？什么数据存储？什么表或视图？哪些列？在什么时间范围内？哪些具体行？
- en: Note that in most cases, rule-based tests can be evaluated for each row of a
    given dataset, which allows you to cleanly separate “good” rows from “bad” rows.
    But rules can also be applied to statistics computed from the data (e.g., `0 <=
    sum(column_x) <= 50`). In general, we’ll talk about rules as applying to each
    row of the data independently. The special case of applying a rule to a statistic
    can be viewed as first computing an aggregated dataset (which could be aggregated
    by some entity, like a customer; or a unit of time, like a date; or without any
    grouping at all) and then applying a row-based rule to the result of that aggregate.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在大多数情况下，可以对给定数据集的每一行评估基于规则的测试，这允许您清晰地将“好”行与“坏”行分开。但规则也可以应用于从数据中计算的统计信息（例如，`0
    <= sum(column_x) <= 50`）。通常，我们将讨论规则作为独立应用于每一行数据的情况。将规则应用于统计数据的特殊情况可以视为首先计算聚合数据集（可以由某个实体（如客户）、时间单位（如日期）或根本不进行任何分组）然后对该聚合结果应用基于行的规则。
- en: Type
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 类型
- en: 'What is the type of rule that will be applied? For example, for rules that
    apply to a given column, you could choose from a variety of types: column is unique,
    column never contains NULL values, column string values are within a specified
    set, etc. Rule types can extend beyond individual columns, covering metadata about
    the table (last updated time, total number of rows), the schema of the table (specific
    column names, types, and order), and much more. Rules can involve multiple columns
    and their relations to one another, or how the table relates to other tables via
    join semantics (e.g., joins 1:1 on a given primary key).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用的规则类型是什么？例如，对于适用于给定列的规则，可以从各种类型中选择：列是唯一的，列从不包含NULL值，列字符串值在指定集合内等。规则类型可以扩展到个别列之外，还涵盖表的元数据（上次更新时间，总行数），表的模式（特定列名、类型和顺序）等。规则可以涉及多列及其相互关系，或表如何通过联接语义与其他表关联（例如，在给定主键上的1：1联接）。
- en: 'The most complex types of rules are often expressed as SQL queries that may
    include joins, subqueries, and complex statements that look for conditions that
    should “never appear” in the table. For example: “Every customer that has an order
    that completed checkout and was not subsequently canceled, voided, or completed
    should have a record in the `customers_with_active_orders` table.”'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最复杂的规则类型通常表达为SQL查询，可能包括联接、子查询和寻找表中“永远不会出现”的条件的复杂语句。例如：“每个完成结账但后来未被取消、作废或完成的订单的客户应在`customers_with_active_orders`表中有记录。”
- en: 'This can be represented in SQL as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中可以表示为：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Constraint
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 约束
- en: Once you’ve chosen the scope and type, you often have to provide some constant
    constraints to the rule. For example, in the rule “`price_per_ticket` is always
    between 50 and 100,” the values of 50 and 100 are the constant constraints. In
    some cases, these aren’t necessary (“column X is unique” doesn’t require a constraint)
    or they are redundant with the rule type (“column is never NULL” is equivalent
    to “NULL count is equal to 0,” where 0 is the constant).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了范围和类型后，通常还需要对规则提供一些常量约束。例如，在规则“`price_per_ticket`始终介于50和100之间”中，50和100的值是常量约束。在某些情况下，这些不是必需的（“列X是唯一的”不需要约束），或者它们与规则类型重复（“列不为空”等同于“NULL计数等于0”，其中0是常量）。
- en: Rules are an essential part of any data quality monitoring strategy. Compared
    to human analysis, rules are cheap to run and don’t make mistakes. Rules are also
    clear and deterministic. Each row either passes or doesn’t. Once you fully grok
    the rule, you also understand why the rule-based test would fail, and what would
    need to be true in order for the test to pass for that row. When a rule-based
    test fails, you can trust that it truly failed—it’s impossible for there to be
    a false positive where the data was good but the rule said it was bad (unless,
    of course, the rule itself is incorrect; we’ll address that shortly).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 规则是任何数据质量监控策略的重要组成部分。与人工分析相比，规则运行成本低且不会出错。规则也清晰而确定。每一行要么通过要么不通过。一旦完全理解了规则，你也会理解为什么基于规则的测试会失败，以及为使该行的测试通过，必须满足什么条件。当基于规则的测试失败时，你可以信任它确实失败了——数据良好但规则说不好（当然，除非规则本身是错误的；我们稍后会讨论这点）。
- en: Additionally, rules are one of the most reliable ways to identify historical
    data quality issues that have existed from the beginning of a dataset or were
    never caught and addressed historically. Why? Rules allow an SME to express a
    requirement that they have for a given dataset, based on their knowledge of the
    system that generated the data or the business context in which the data was collected.
    An SME can write a rule saying that a column should never be NULL in the data’s
    past, present, or future. Contrast this with approaches that learn from the history
    of the data (such as using metrics or unsupervised ML to detect unexpected changes,
    both of which we’ll address in a moment). Those approaches are inherently looking
    for sudden changes in the data, which are by definition *new* rather than historical
    data quality issues. Such approaches cannot tell if the data was always bad.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，规则是识别从数据集开始就存在的或从历史上未被捕捉和解决的数据质量问题的最可靠方式之一。为什么呢？规则允许专家在其对生成数据的系统或收集数据的业务背景的知识基础上表达他们对给定数据集的要求。专家可以编写一条规则，说明数据的过去、现在或未来不应该为空。相比之下，那些从数据历史中学习的方法（例如使用指标或无监督机器学习来检测意外变化）则是在寻找数据中的突然变化，这些变化从定义上来说是*新的*，而不是历史数据质量问题。这样的方法无法判断数据是否一直存在问题。
- en: Rules are also good at identifying the needle in the haystack. If you’re working
    with a table that has billions of rows, then a rule is often the most reliable
    way to spot if there are a handful of records that violate a given condition.
    (Note that this can be a liability if you don’t care about each and every record,
    as you’ll have to find ways to exclude data quality “scars” from the past that
    you no longer care about but that violate your rule.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 规则还擅长于识别海量数据中的个别问题。如果您正在处理一个有数十亿行的表格，那么规则通常是发现少数违反给定条件的记录最可靠的方式。（请注意，如果您不关心每一条记录，这可能是一个负担，因为您将不得不找到方法来排除您不再关心但仍然违反规则的数据质量“疤痕”。）
- en: 'Relying solely on rule-based testing, however, is a mistake. For one thing,
    there’s a lot of room for error in specifying high-quality rules:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅依靠基于规则的测试是错误的。首先，在指定高质量规则时存在很大的出错空间：
- en: The *scope* can be too narrowly specified (too tight a `WHERE` SQL clause),
    causing the rule to miss data quality issues (e.g., the rule was only applied
    to segment X of a table, but it should also apply to segment Y).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围*可能被过于狭隘地指定（`WHERE` SQL 子句太紧凑），导致规则忽略数据质量问题（例如，规则仅适用于表的 X 段，但也应适用于表的 Y 段）。'
- en: The *scope* can be too widely specified (too expansive a `WHERE` SQL clause),
    causing the rule to incorrectly flag valid data as invalid (e.g., this column
    actually *should* be NULL for certain records).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围*可以过于广泛地指定（`WHERE` SQL 子句太过宽泛），导致规则错误地将有效数据标记为无效（例如，对于某些记录，这列实际上应该是*应该*为空）。'
- en: The *constraint* of the rule can be incorrectly specified. This is very common
    when setting ranges for column values or for statistics. The range may be too
    wide (and thus miss real data quality issues), or too narrow (and thus be very
    noisy as data changes).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则的*约束*可能被错误地指定。在设置列值或统计数据的范围时，这种情况非常普遍。范围可能过于广泛（从而忽略真实的数据质量问题），或者过于狭窄（从而在数据变化时产生非常嘈杂的结果）。
- en: The wrong *type* of rule may be selected. The test may either fail to capture
    the real intent of the user or produce notifications that are not meaningful because
    the test was inappropriate (the column was always intended to have NULL values,
    but a “never NULL” rule was applied anyway).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能选择了错误的*类型*规则。测试可能无法捕获用户的真实意图，或者因为测试不恰当而产生无意义的通知（该列始终打算具有 NULL 值，但仍然应用了“永不为
    NULL”的规则）。
- en: 'Furthermore, covering all of a modern enterprise’s data with high-quality rules
    is a Sisyphean task. Consider the following realistic hypothetical example, where
    an organization has 10,000 tables to monitor for data quality issues:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅用高质量规则覆盖现代企业的所有数据是一个劳神费力的任务。考虑以下现实的假设性例子，组织需要监控 10,000 张表格以检测数据质量问题：
- en: 10 tables are mission-critical fact tables that the entire company depends on
    (key statistics aggregated for the board come from these tables).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司整体依赖的 10 张关键事实表至关重要（董事会的关键统计数据来自这些表）。
- en: 90 tables contain critical data used to make business and operational decisions
    on a daily basis across the company.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司每天在业务和运营决策中使用的关键数据包含在 90 张表中。
- en: 900 tables are of critical importance to individual teams or initiatives, used
    by product managers, ML engineers, analysts, data scientists, or other data-savvy
    professionals on a weekly basis.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 900个表对于个别团队或倡议至关重要，每周由产品经理、ML工程师、分析师、数据科学家或其他数据专家使用。
- en: The remaining 9,000 tables could have data quality issues that manifest in subtle,
    difficult-to-detect ways in the other 1,000 tables.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的9,000个表可能会在其他1,000个表中以微妙且难以检测的方式显示数据质量问题。
- en: Each of these tables might have tens, hundreds, or even thousands of columns.
    For example, fact tables often aggregate a wide variety of information about a
    given type of entity into a single very broad table that launches many analyses
    and use cases. Some tables can also have hundreds or thousands of segments (groups
    of rows) that have different behavior. Web event tables, for instance, often capture
    a large volume of structured data (device, IP address, URL, user, time, etc.)
    and semistructured data (JSON payloads) for hundreds or thousands of different
    events or actions that users can take.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每个表可能有数十、数百甚至数千列。例如，事实表通常将关于特定类型实体的各种信息聚合到一个非常广泛的表中，以启动许多分析和用例。有些表还可能有数百或数千个段（行的组合），具有不同的行为。例如，Web事件表通常捕获大量关于各种不同事件或用户行为的结构化数据（设备、IP地址、URL、用户、时间等）和半结构化数据（JSON负载）。
- en: Each column or segment of data for each table might require 5 to 10 rules to
    cover the most important constraints on that data. Thus, to monitor their most
    important tables with rule-based testing, an organization could end up writing
    *1,000 tables * 50 columns per table * 5 rules per column = 250,000 rules*. And
    this doesn’t cover semistructured data, segment variation, or the other 9,000
    potentially important tables!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 每个表的每一列或数据段可能需要5到10条规则来覆盖数据的最重要约束条件。因此，要使用基于规则的测试监控它们最重要的表，一个组织可能最终会写入*1,000个表
    * 每个表50列 * 每列5条规则 = 250,000条规则*。这还不包括半结构化数据、段变化或其他9,000个潜在重要的表！
- en: '![The Sisyphean task of creating and maintaining data quality rules in large
    warehouses (illustration by Josie Stanley)](assets/adqm_02in01.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![在大型数据仓库中创建和维护数据质量规则的苦差事（Josie Stanley插图）](assets/adqm_02in01.png)'
- en: The Sisyphean task of creating and maintaining data quality rules in large warehouses
    (illustration by Josie Stanley)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据仓库中创建和维护数据质量规则是一项苦不堪言的任务（Josie Stanley插图）。
- en: In addition to creating rule-based tests, you have to maintain them, which is
    much more difficult than maintaining unit tests. A unit test for code should produce
    the same result every time it’s run, until there’s a software update that breaks
    the expected behavior, intentionally or unintentionally. But unlike code, data
    changes constantly and in unpredictable ways—as new products are launched, the
    macroenvironment shifts, or user behavior changes, to name a few reasons. Therefore,
    rule-based tests can be very brittle. To ensure that their definitions—and particularly
    their constraints—remain accurate, rules need to be constantly updated as the
    product and data evolve. It can be tempting to save some time by loosening the
    constraints up front, but this risks missing real issues.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建基于规则的测试之外，您还必须维护这些规则，这比维护单元测试要困难得多。代码的单元测试应该在每次运行时产生相同的结果，直到有软件更新打破预期行为，无论是有意还是无意。但是与代码不同，数据不断以不可预测的方式变化——随着新产品的推出、宏观环境的变化或用户行为的改变等等。因此，基于规则的测试可能非常脆弱。为了确保它们的定义——特别是它们的约束——保持准确，规则需要在产品和数据演变过程中不断更新。在一开始放宽约束以节省时间可能很诱人，但这会冒着忽略真实问题的风险。
- en: In summary, rule-based testing is not a scalable solution to monitoring data
    in a modern enterprise. However, rules are a powerful tool for SMEs to express
    and enforce their expectations of the data from first principles. For example,
    [Figure 2-3](#an_excerpt_from_the_medium_post_quotati), from Airbnb’s report of
    its data quality strategy, gives practical examples of SME-defined rules that
    enforce consistency in listings data. The ideal data quality solution makes it
    easy for rules to be created, edited, and analyzed by a diverse audience of SMEs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，基于规则的测试并非是在现代企业中监控数据的可扩展解决方案。然而，规则是中小企业表达和强制执行数据期望的有力工具。例如，[图2-3](#an_excerpt_from_the_medium_post_quotati)，来自Airbnb关于其数据质量战略的报告，提供了由中小型企业定义的规则的实际示例，以确保列表数据的一致性。理想的数据质量解决方案使得各种中小企业能够轻松创建、编辑和分析规则。
- en: '![An excerpt from the Medium post “Data Quality at Airbnb” that shows examples
    of validation rules and metrics checks that Airbnb runs to ensure data quality
    internally.](assets/adqm_0203.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![从Medium文章“Airbnb的数据质量”中摘录的验证规则和指标检查示例，展示了Airbnb内部用来确保数据质量的方法。](assets/adqm_0203.png)'
- en: Figure 2-3\. An excerpt from the Medium post [“Data Quality at Airbnb”](https://oreil.ly/eTed_)
    that shows examples of validation rules and metrics checks that Airbnb runs to
    ensure data quality internally.
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 从Medium文章[“Airbnb的数据质量”](https://oreil.ly/eTed_)摘录，展示了Airbnb内部用来确保数据质量的验证规则和指标检查的示例。
- en: Metrics Monitoring
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标监控
- en: The next data quality monitoring approach is also inspired by software engineering.
    Most software systems are monitored by tracking metrics about the infrastructure
    and notifying when there are sudden adverse changes. These stats could be about
    the hardware itself (CPU utilization, memory, etc.), the networking activities
    (packets lost, etc.), or individual services that are running (queue length, average
    latency, etc.).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个数据质量监控方法也受到软件工程的启发。大多数软件系统通过跟踪有关基础设施的指标来进行监控，并在有突然不利变化时通知。这些统计数据可以涉及硬件本身（CPU利用率、内存等）、网络活动（丢包等）或正在运行的个别服务（队列长度、平均延迟等）。
- en: Analogously, you can monitor statistics about data and set thresholds to tell
    the system to alert if the data spikes above or below expectations. The challenge
    is that, for data quality, the surface area of metrics to monitor explodes. To
    ensure proper coverage, you need metrics for every column, segment, and statistic
    you might care about—such as percentage of NULL values, percentage of duplicates,
    mean, min/max, etc.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，您可以监控有关数据的统计信息，并设置阈值，以告知系统在数据高于或低于预期时发出警报。挑战在于，对于数据质量来说，需要监控的指标范围广泛。为了确保适当的覆盖范围，您需要为每一列、段和可能关心的统计量（如NULL值百分比、重复百分比、均值、最小/最大值等）准备指标。
- en: Beyond the scalability issues, metrics monitoring for data quality presents
    other problems, too. For one thing, because it tests the data at the aggregate
    statistical level, it may miss data quality issues that affect only a small percentage
    of the records. Furthermore, most implementations of metrics monitoring don’t
    identify the specific records that were responsible for the metric changing—making
    it hard to understand why metrics changed and whether the reason was valid (such
    as an external trend) or due to a data quality issue.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可伸缩性问题外，数据质量的指标监控还存在其他问题。首先，因为它在聚合统计水平上测试数据，可能会错过只影响少数记录的数据质量问题。此外，大多数指标监控的实现并不识别导致指标变化的具体记录，这使得理解指标变化的原因以及其是否合理（如外部趋势）或由于数据质量问题而起变得困难。
- en: Metrics monitoring can also miss issues that creep into the data over time.
    For example, imagine the source of a data quality issue is a code change that’s
    behind a feature flag. If the feature is slowly rolled out to customer segments,
    the data will change gradually. Any change to the metrics will also be slow and
    might never reach the threshold for an alert.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 指标监控还可能忽略随时间积累的数据问题。例如，假设数据质量问题的根源是一个位于功能标志后面的代码更改。如果功能逐步推出到客户段，数据将逐渐变化。任何指标的变化也将很缓慢，并且可能永远不会达到警报的阈值。
- en: That said, metrics monitoring is essential when you want to pay close attention
    to a very specific slice of the data. For example, [Figure 3-1](ch03.html#a_feature_shock_in_the_data_for_an_ml_m)
    shows some of the metrics that Airbnb has chosen to prioritize, such as number
    of active listings, new active listings, reactivations, and deactivations. Important
    metrics like these are often heavily influenced by a small subset of the overall
    dataset, and any trends therein might not be caught by other monitoring methods
    that consider the data as a whole.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，当您希望密切关注数据的特定片段时，指标监控至关重要。例如，[图 3-1](ch03.html#a_feature_shock_in_the_data_for_an_ml_m)展示了Airbnb选择优先考虑的一些指标，如活跃列表数、新活跃列表数、重新激活和停用。这些重要指标通常受整体数据集的小部分子集的影响，这些子集中的任何趋势可能不会被考虑整体数据的其他监控方法捕获。
- en: Similarly, metrics monitoring can help when the data is degraded for some expected
    percentage of records, but the user wants to avoid that percentage going up in
    a significant way. For example, it could be that 20% of the time, user records
    don’t have a valid address because of how those user records are created. If that
    percentage were to increase significantly, then there might be a data quality
    issue that has corrupted or removed address information for a larger number of
    users than expected.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，指标监控可以帮助当数据在预期的一定比例记录中降级时，但用户希望避免该百分比显著增加时。例如，可能有20%的时间，用户记录因为创建用户记录的方式而没有有效地址。如果这个百分比显著增加，那么可能存在一个数据质量问题，该问题影响了更多用户的地址信息，超出预期范围。
- en: Given these pros and cons, a successful data quality monitoring strategy should
    let users set up monitoring on key metrics and, ideally, use time series models
    to set appropriate thresholds. But it’s not enough to rely on metrics monitoring
    alone.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些优缺点，一个成功的数据质量监控策略应该让用户在关键指标上设置监控，并理想情况下使用时间序列模型来设置适当的阈值。但仅依赖指标监控是不够的。
- en: Automating Data Quality Monitoring with Unsupervised Machine Learning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化无监督机器学习进行数据质量监控
- en: 'Having covered the traditional (nonautomated) approaches to monitoring, it’s
    time to introduce a new strategy: unsupervised machine learning. Data quality
    is no different from many other fields, where processes that were once entirely
    manual, like fraud detection, underwriting, and product recommendations, can now
    be handled by ML algorithms. These algorithms allow for far greater efficiency
    and can operate at a cadence and with a consistency that drives better business
    outcomes.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在覆盖了传统（非自动化）监控方法后，现在是时候介绍一种新策略：无监督机器学习。数据质量与许多其他领域无异，这些领域的流程曾经完全手动，如欺诈检测、核保和产品推荐，现在可以由机器学习算法处理。这些算法允许更高效率地操作，并以驱动更好业务结果的节奏和一致性运行。
- en: While we explore unsupervised ML in greater detail in Chapters [4](ch04.html#automating_data_quality_monitoring_wi)
    and [5](ch05.html#building_a_model_that_works_on_real_wor), we’ll give an overview
    here and explain the key role it plays in automating data quality monitoring at
    scale.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将在第[4](ch04.html#automating_data_quality_monitoring_wi)章和第[5](ch05.html#building_a_model_that_works_on_real_wor)章更详细地探讨无监督机器学习，并在此概述其作用及其在规模化自动化数据质量监控中的关键角色。
- en: What Is Unsupervised Machine Learning?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是无监督机器学习？
- en: Broadly, an ML algorithm can be categorized as either supervised or unsupervised.
    In supervised learning, the data that the model uses to learn is labeled by humans.
    Image classifiers often use supervised learning—a human shows a model thousands
    of images labeled as a tree, a cat, and so on, and the model then learns to recognize
    similar objects in new images. In unsupervised learning, the model does not have
    human labels—it just has the data, with all of that data’s inherent patterns and
    relationships. The model learns from the data itself and interprets new inputs
    based on everything it has seen so far.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，机器学习算法可以分为有监督和无监督两类。在有监督学习中，模型用于学习的数据是由人类标记的。图像分类器通常使用有监督学习——人类向模型展示成千上万张被标记为树、猫等的图像，模型随后学习识别新图像中类似的对象。在无监督学习中，模型没有人类标签，只有数据本身，包含所有数据固有的模式和关系。模型从数据本身学习，并根据迄今为止看到的一切来解释新输入。
- en: 'Supervised learning does not make practical sense as a strategy for data monitoring,
    as it requires humans to collect and label a large, diverse set of training data
    representing real-world data quality issues the model needs to reliably detect.
    Given how data differs wildly from table to table, not to mention from company
    to company, it would be onerous to collect enough labeled data to make supervised
    ML work well. This makes unsupervised learning the better fit for monitoring data
    quality. Assuming you’ve developed a model that works well, it can begin monitoring
    a dataset without any initial setup and continue to learn and adapt as the data
    changes. These algorithms can be tuned to detect deep, complex issues in the data,
    such as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据监控策略，有监督学习在实际中并不实际，因为它需要人类收集和标记大量多样的训练数据，这些数据代表模型需要可靠检测的真实世界数据质量问题。考虑到数据从表到表、从公司到公司的巨大差异，收集足够的标记数据使有监督机器学习能够有效工作将是困难的。这使得无监督学习更适合监控数据质量。假设您已开发出效果良好的模型，它可以开始监控数据集而无需任何初始设置，并在数据变化时继续学习和适应。这些算法可以被调整以检测数据中的深层复杂问题，例如：
- en: The percentage of NULL values in a set of columns has increased.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组柱子中的NULL值的百分比已经增加。
- en: A specific segment of data (e.g., one country) has disappeared or is arriving
    with fewer records than expected.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定数据段（例如一个国家）已经消失或到达的记录比预期少。
- en: The distribution in a column changed significantly (e.g., the credit score distribution
    is skewing much higher than expected).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在柱子中的分布发生了显著变化（例如，信用评分的分布偏高于预期）。
- en: The relationship between multiple columns has changed (e.g., these columns used
    to sum to equal one another, but now no longer do for a subset of records).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多列之间的关系发生了变化（例如，这些列过去相加等于彼此，但现在对于一部分记录不再是这样）。
- en: We’ve noticed that some data quality monitoring solutions claim to be doing
    “machine learning” but are really just relying on time series models to monitor
    many metrics. Depending on the technique used, a time series model might be using
    ML under the hood, but it’s narrowly scoped to trying to predict the next metric
    value in a sequence. It cannot analyze the underlying data and will not discover
    data quality issues that do not directly affect the metric being monitored. In
    this book, we will use the term unsupervised learning to refer to the more expansive
    challenge of detecting unexpected changes in the entirety of a complex dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到一些数据质量监控解决方案声称正在进行“机器学习”，但实际上只是依赖于时间序列模型来监控许多指标。根据使用的技术，时间序列模型可能在幕后使用ML，但其范围狭窄，仅仅是试图预测序列中的下一个指标值。它无法分析基础数据，也不会发现不直接影响正在监控的指标的数据质量问题。在本书中，我们将使用无监督学习这个术语来指称在整个复杂数据集中检测到意外变化的更广泛挑战。
- en: One of the most powerful aspects of unsupervised ML is that it aims to understand
    changes in the relationships of data in the table as a whole. This is important
    because data in a table is usually highly interrelated.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习最强大的一个方面之一是，它旨在理解表中数据的关系变化作为一个整体。这一点非常重要，因为表中的数据通常是高度相互关联的。
- en: '![The relationships between columns in a credit card dataset of default payments
    from Taiwan (UCI Machine Learning Repository)](assets/adqm_0204.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![台湾信用卡违约支付数据集中列之间的关系（UCI机器学习库）](assets/adqm_0204.png)'
- en: Figure 2-4\. The relationships between columns in a credit card dataset of default
    payments from Taiwan ([UCI Machine Learning Repository](https://oreil.ly/j6kw8)).
    See a full-sized version of this image at [*https://oreil.ly/adqm_2_5*](https://oreil.ly/adqm_2_5).
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 台湾信用卡违约支付数据集中列之间的关系（[UCI机器学习库](https://oreil.ly/j6kw8)）。查看此图片的完整尺寸版本，请访问[*https://oreil.ly/adqm_2_5*](https://oreil.ly/adqm_2_5)。
- en: The credit card dataset shown in [Figure 2-4](#the_relationships_between_columns_in_a)
    is a good real-world example. Each column appears on the vertical and horizontal
    axes, and the circles are colored based on the columns’ [phi-K correlations](https://oreil.ly/VcXnJ)
    with one another.^([1](ch02.html#ch01fn1)) This reveals a great deal about the
    relationships between the columns in the dataset. Looking at the clusters of dark
    circles near the diagonal, you can see that all of the `pay_N, bill_amt_N`, and
    `pay_amt_N` columns are correlated with one another. There are more surprising
    relationships too. For example, `age` is strongly correlated with `limit_balance`
    and with `marital_status`. But `marital_status` is only weakly correlated with
    `limit_​bal⁠ance`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-4](#the_relationships_between_columns_in_a)中展示的信用卡数据集是一个很好的现实世界的例子。每个柱子出现在垂直和水平轴上，圆圈的颜色基于柱子之间的[phi-K相关性](https://oreil.ly/VcXnJ)进行着色。^([1](ch02.html#ch01fn1))
    这揭示了数据集中柱子之间关系的许多信息。看看靠近对角线的黑色圆圈的集群，你会发现`pay_N, bill_amt_N`和`pay_amt_N`列彼此之间存在相关性。还有更多令人惊讶的关系。例如，`age`与`limit_balance`以及`marital_status`强相关。但`marital_status`与`limit_​bal⁠ance`之间的相关性较弱。'
- en: 'When monitoring data for data quality, we tend to oversimplify things and consider
    just the values, distribution, or summary statistics in columns individually and
    create rules or metrics to monitor the column in isolation. But in practice, real-world
    data has all of the rich and complex correlation structure that we see in this
    credit card dataset. This has two significant implications for monitoring:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控数据质量时，我们往往会过于简化事物，仅考虑单独列中的值、分布或摘要统计，并创建规则或指标来监视单独列。但实际上，现实世界的数据具有我们在这个信用卡数据集中看到的丰富而复杂的相关结构。这对监控有两个重要的影响：
- en: If we monitor *every* column of this dataset in isolation with metrics or validation
    rules, then our monitors will also be very highly correlated. If the column correlations
    are causally linked to some data or process, then a change in that underlying
    mechanism may cause many dependent metrics to change, and validations to fail
    simultaneously. So, instead of getting one alert, we may well get dozens or more.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用度量或验证规则分别监控这个数据集的每一列，那么我们的监控器也会高度相关。如果列之间的相关性与某些数据或过程有因果关系，那么底层机制的变化可能会导致许多依赖的度量值同时改变，验证失败。因此，我们可能会收到数十个或更多的警报，而不是一个警报。
- en: If we evaluate each column for data quality in isolation, we disregard a tremendous
    amount of contextual information that could be very important for data quality.
    In the credit card example, if we suddenly found that `age` and `limit_balance`
    were less correlated, then this could signal a risk that one of the two columns
    has experienced a sudden data quality shock.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们仅仅评估每列数据质量的话，将忽略大量可能对数据质量非常重要的上下文信息。在信用卡的例子中，如果突然发现`age`和`limit_balance`的相关性较小，这可能表明其中一列经历了突然的数据质量问题。
- en: In order to fully leverage the rich structure of real-world datasets, avoid
    sending many alerts for correlated issues, and successfully automate data quality
    monitoring at scale, we need an approach to monitoring that can operate on the
    data arriving in a table as a whole rather than upon each column in isolation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用现实世界数据集的丰富结构，避免为相关问题发送过多警报，并成功实现规模化的自动化数据质量监控，我们需要一种能够对整个表中的数据进行操作而不是单独对每列进行操作的监控方法。
- en: This is exactly what unsupervised ML algorithms are designed to do. Compared
    to more narrowly scoped metrics or validation rules, a good model will find a
    wider range of data quality issues, including unknown unknowns that humans didn’t
    think to check for. It will automatically suppress repeated anomalies; on the
    second day that an issue recurs it will be able to use the first day as a “new
    normal” baseline. And issues that affect multiple columns can be clustered together
    and presented as a single issue; they’re detected in one pass with the unsupervised
    algorithm and allocated to the appropriate columns and rows accordingly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是无监督机器学习算法的设计目的。与更窄范围的指标或验证规则相比，一个好的模型将发现更广泛的数据质量问题，包括人类未曾考虑到的未知未知。它将自动抑制重复的异常；在问题再次发生的第二天，它将能够将第一天作为“新正常”的基线。影响多列的问题可以被聚类在一起，并作为单一问题呈现；它们通过无监督算法的一次遍历被检测到，并相应地分配到适当的列和行中。
- en: 'An Analogy: Lane Departure Warnings'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类比：车道偏离警告
- en: '![Generated with DALL-E](assets/adqm_02in02.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![Generated with DALL-E](assets/adqm_02in02.png)'
- en: Generated with DALL-E
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DALL-E 生成
- en: 'When we thought about how data quality monitoring is being automated with ML,
    an analogy came to mind: how driving has evolved from an entirely manual practice
    to one assisted by ML algorithms. These algorithms are so valuable because they
    can account for a vast amount of context that may be too hidden, or too complex,
    for humans to capture via hard-coded, rules- and threshold-based logic.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考数据质量监控如何通过机器学习自动化时，脑海中浮现了一个类比：驾驶从完全手动变为由机器学习算法辅助的过程。这些算法之所以如此宝贵，是因为它们可以考虑到大量的上下文信息，这些信息可能对于人类来说太隐蔽或太复杂，无法通过硬编码、规则和阈值逻辑来捕捉。
- en: 'Consider just one aspect of driving: ensuring that your vehicle stays in the
    lane and makes safe lane changes. If we applied the data quality methods we’ve
    discussed to this problem, we’d have something like the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑驾驶的一个方面：确保您的车辆保持在车道内并进行安全的车道变换。如果我们将讨论过的数据质量方法应用于这个问题，我们可能会得到以下结果：
- en: The fully manual approach
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 完全手动方法
- en: The driver simply pays attention to lane markings and never gets distracted
    or accidentally falls asleep at the wheel. This is how we have always ensured
    that vehicles stay within the lanes. It requires a great deal of attention and
    focus from the driver, and we still have accidents related to lane departures.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶员只需注意车道标线，从不分神或在驾驶时意外入睡。这就是我们一直以来确保车辆保持在车道内的方式。这需要驾驶员高度的注意力和集中力，但仍然会发生与车道偏离相关的事故。
- en: The system based on rules and metrics
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则和度量的系统
- en: To picture an approach that relies on rules and metrics, think about trying
    to detect the lane lines on the road using a car-mounted camera and some basic
    statistics. If a certain percentage of the pixels arriving into the camera are
    yellow (essentially, a metric threshold), then the car would trigger a warning.
    This would be catastrophic, as many lane markings aren’t yellow or solid or large
    enough to trigger this threshold. Plus, there will be many markings on roads (or
    other surfaces) that might appear yellow but have nothing to do with lane lines,
    causing false positives.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种依赖规则和指标的方法，考虑使用车载摄像头和一些基本统计数据来检测道路上的车道线。如果进入摄像头的像素中有一定百分比是黄色（基本上是一个指标阈值），那么车辆就会触发警告。这将是灾难性的，因为许多车道标线不是黄色的，也不是实线或足够大来触发这个阈值。此外，道路（或其他表面）上会有许多标记可能看起来是黄色的，但与车道线无关，导致误报。
- en: Additionally, there are many situations when drivers *should* cross over the
    lane line, such as when making a left turn or changing lanes to get around traffic
    or avoid an obstacle. In practice, this overly simplified system may lead to so
    many disruptive, unhelpful notifications that the driver disables the alerting
    mechanism in the vehicle—returning to the default case of having no notifications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有许多情况下驾驶员**应该**跨越车道线，例如左转或变道以避开交通或障碍物。在实际操作中，这种过于简化的系统可能会导致过多的干扰性、无用的通知，使驾驶员禁用车辆的警报机制——回到默认情况下没有通知的状态。
- en: The ML approach
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法
- en: An ML approach to lane departure warnings would be more intelligent about whether
    or not to send the notification. It would use the context of the vehicle to avoid
    sending the notification if the turn signal is on—or if an object detection system
    notices that there is an obstruction in the way of the vehicle and that the vehicle
    should be departing from the lane. It could also prioritize alerting the user
    only if there is a high risk of encountering another vehicle or obstruction while
    crossing the lane.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法应用于车道偏离警告，能够更智能地判断是否发送通知。它会利用车辆的上下文，避免在转向灯开启时发送通知——或者当物体检测系统发现车辆前方有障碍物，并且车辆应该偏离车道时也不发送通知。它还可以优先提醒用户，只有在跨越车道时有高风险遇到其他车辆或障碍物时才会发出警报。
- en: This approach has the benefit of being far less noisy than the naive lane notification
    system and thus is much more likely to be listened to by the user. This dramatically
    increases the safety of the vehicle and the user’s satisfaction while driving.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比简单的车道通知系统噪声要少得多，因此用户更有可能听从它。这显著提高了车辆和用户在驾驶时的安全性和满意度。
- en: The Limits of Automation
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化的局限性
- en: '![Charting the value created by automation as the degree of automation increases](assets/adqm_0205.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![随着自动化程度增加而创建的价值图示](assets/adqm_0205.png)'
- en: Figure 2-5\. Charting the value created by automation as the degree of automation
    increases.
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 随着自动化程度增加而创建的价值图示。
- en: It’s important to think carefully about the use cases for automation with ML.
    Poorly designed automation tends to be worse than no automation at all. That’s
    why we’ve devoted a large portion of this book to developing a model that performs
    well on real-world data and avoids overwhelming your users with alerts.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要仔细考虑机器学习自动化的使用案例。设计不良的自动化往往比没有自动化更糟糕。这就是为什么我们在本书的大部分内容中致力于开发一个在真实数据上表现良好，并避免用警报压倒用户的模型。
- en: As [Figure 2-5](#charting_the_value_created_by_automatio) shows, there are also
    diminishing returns beyond a certain level of automation. Even if you have a powerful
    model in place, there will be problems that unsupervised ML can’t solve. First,
    because it’s sampling the data, it will never find a needle in the haystack like
    a validation rule can. For example, if you say a relationship must always be true
    between three columns, then if even a single record violates that constraint,
    a rule will find it—whereas it might simply not be sampled or significant in unsupervised
    learning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2-5](#charting_the_value_created_by_automatio) 所示，超过一定程度的自动化也会有收益递减。即使您拥有强大的模型，也会有无监督机器学习无法解决的问题。首先，因为它在对数据进行采样时，永远无法像验证规则那样找到针对三个列之间关系必须始终为真的情况。例如，如果说关系必须始终为真，而即使是单个记录违反了该约束，规则也能找到它——而在无监督学习中，可能根本不会对其进行采样或认为它不重要。
- en: Second, unsupervised models are by definition looking for new changes in the
    data and will never catch things that have historically always been wrong. For
    example, imagine that a column representing the number of vehicles owned by a
    customer was incorrectly coded at inception, such that missing values were coded
    as `0` instead of as `NULL`. This would cause downstream systems to think that
    many more customers owned zero vehicles than was true, when in fact, the data
    simply hadn’t been collected for these customers. An unsupervised model or time
    series metric monitor would not catch this issue, because the relationships in
    the data have not changed over time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，无监督模型在定义上是在寻找数据中的新变化，永远不会捕捉到历史上一直存在问题的事物。例如，想象一下，一个代表客户拥有的车辆数量的列在创建时被错误编码，使得缺失值被编码为`0`而不是`NULL`。这会导致下游系统认为拥有零辆车的客户比实际情况更多，实际上，只是这些客户的数据尚未收集到。无监督模型或时间序列度量监视器将无法捕捉到这个问题，因为数据中的关系随时间并未发生变化。
- en: And third, unsupervised models treat every column and every row as being equally
    important—and so may not pay as close attention to a given slice of the data as
    a metric that is defined directly on that data. For instance, if you’re monitoring
    the percentage of times a column is equal to a value, then those records are much
    more influential in the metric outcome than they will be for an unsupervised monitoring
    approach that looks at the entirety of the table.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，无监督模型将每列和每行视为同等重要——因此可能不会像直接定义在数据上的度量那样密切关注数据的某个切片。例如，如果您正在监视列等于某个值的百分比的次数，则这些记录对度量结果的影响要大得多，而对于无监督监视方法来说，它们将不会那么重要，因为后者看待的是整个表格。
- en: Automating rule and metric creation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化规则和度量的创建
- en: A natural question regarding automation is whether it’s a good idea to use algorithms
    to automate the creation of rules and metrics. While automatic rule and metrics
    monitoring is possible, it’s very costly and leads to both false positives and
    false negatives (undetected issues).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化是否能够使用算法来自动创建规则和度量是一个自然的问题。尽管自动规则和度量监控是可能的，但这非常昂贵，并且会导致假阳性和假阴性（未检测到的问题）。
- en: Rules
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则
- en: Suppose you want to automatically create rules for your data quality monitoring
    system, rather than asking SMEs to specify rules by hand. You could start by predefining
    the common types of rules (uniqueness, NULL values, regular expression patterns,
    etc.) Then, you could sample some historical data to analyze. For each rule you’d
    defined up front, you could check each column in the table to determine if that
    rule is satisfied for sample data. Then you’d also need to check all the historical
    data to make sure that the rule continues to pass. Finally, you could productionize
    all the rules that passed on all the historical data. Great, right? Well, not
    exactly.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要自动为您的数据质量监控系统创建规则，而不是要求专家手动指定规则。您可以从预定义的常见规则类型（唯一性、NULL 值、正则表达式模式等）开始。然后，您可以对一些历史数据进行抽样分析。对于每个事先定义的规则，您可以检查表中的每列，以确定样本数据是否满足该规则。然后，您还需要检查所有历史数据，以确保规则继续通过。最后，您可以对所有历史数据上通过的规则进行生产。听起来不错，对吧？嗯，不完全是这样。
- en: For tables with large volumes of data, the approach can be very expensive, as
    it ultimately requires evaluating every rule on every historical record to avoid
    situations where the rule will fail frequently in the future. For tables with
    small amounts of data, the rules will be very brittle—many may have passed only
    due to chance. And any time the data changes, you’ll need to edit the rules or
    rerun the automated setup process. Finally, most of the really important rules
    that users want require customization with SQL so that they can be applied to
    only a subset of records in the table, or to express a complex relationship between
    columns or tables. And these customizations are very unlikely to be part of the
    predefined classes of rules in the system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含大量数据的表格，这种方法可能会非常昂贵，因为最终需要对每个历史记录评估每条规则，以避免未来频繁失败的情况。对于数据量较小的表格，规则会非常脆弱——许多规则可能仅仅是由于偶然才通过的。而且每次数据发生变化时，您都需要编辑规则或重新运行自动设置过程。最后，大多数用户希望的真正重要的规则需要使用
    SQL 进行定制，以便仅适用于表格中的部分记录，或者表达列或表格之间复杂的关系。这些定制很少会成为系统预定义规则类别的一部分。
- en: In practice, we have not seen this approach work in real-world situations. We
    recommend that you make it easy for end users to voluntarily add rules, using
    their own first-principles judgment, for situations where you need the data to
    be perfect. This has two advantages. First, the only rules that are created are
    the rules that really matter, reducing false positives and the number of rules
    you have to maintain over time. Second, if a new rule fails on the historical
    data, it becomes a learning opportunity. The end user might find and fix a historical
    data quality issue or learn something new about the structure of the data in uncovering
    *why* the rule fails.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们没有看到这种方法在现实世界中起作用。我们建议您让最终用户可以轻松地自愿添加规则，使用他们自己的首要原则判断，用于需要数据完美的情况。这有两个优点。首先，只创建真正重要的规则，减少误报和随时间需要维护的规则数量。其次，如果新规则在历史数据上失败，这将成为一个学习机会。最终用户可能会找到并修复历史数据质量问题，或者在揭示为何规则失败时了解数据结构的新内容。
- en: Metrics
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: Automating metrics creation would mean that rather than asking that a user manually
    opt into the specific metrics they want to monitor, the system automatically computes
    and monitors a set of metrics for every table and column.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化指标创建意味着，与要求用户手动选择要监视的特定指标相比，系统会自动计算和监视每个表和列的一组指标。
- en: This would be easy enough to achieve. You could decide on the out-of-box metrics
    that you believe most users will care about. For tables, this might be the number
    of records per day and the time since the table was last updated. For columns,
    this might be the percentage of records that are NULL, zero, blank, or unique.
    Getting more sophisticated, you could configure each metric to only look for adverse
    events—a reduction in the number of rows, an increase in the number of NULL values,
    etc.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这将非常容易实现。您可以决定您认为大多数用户会关心的开箱即用指标。对于表格来说，这可能是每天的记录数和表格上次更新时间。对于列来说，这可能是记录的空值、零值、空白值或唯一值的百分比。更进一步，您可以配置每个指标仅寻找不利事件——行数减少、空值数量增加等。
- en: The approach of defining some metrics to automatically monitor ahead of time
    helps users get up and running with some insight into their tables’ data quality
    right away, and it’s a strategy we use at Anomalo. However, note that for even
    a small number of metrics, this solution can be quite expensive, as you’ll need
    to do a significant amount of computation for every table and every column that
    needs to be monitored. Unless you have strategies for false positive suppression,
    it can also lead to alert fatigue—a single underlying data quality change might
    cause 10 columns to have NULL value increases, which will present as 10 separate
    alerts that a user has to triage.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 提前定义一些指标自动监控的方法，帮助用户立即了解其表格数据质量的一些见解，这是我们在Anomalo使用的策略。但请注意，即使是少量指标，这种解决方案也可能非常昂贵，因为您需要对每个需要监控的表格和每个列进行大量计算。除非您有假阳性抑制策略，否则也可能导致警报疲劳——单一的基础数据质量变化可能导致10个列出现空值增加，这将呈现为用户需要处理的10个单独警报。
- en: While some metrics can be automated, we believe it’s essential to give users
    the ability to define custom metrics as well. This ensures that important segments
    aren’t missed, where there could be minor but critical changes to track. And it
    lets you capture the metrics that are most important to users—which often involve
    computing use case–specific statistics, like the percentage of records satisfying
    a multicolumn constraint, that would be excluded from an entirely automated approach.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以自动化一些指标，我们认为让用户能够定义自定义指标至关重要。这确保了不会错过重要的段落，可能出现轻微但关键的变化需要跟踪。它还允许您捕获对用户最重要的指标——通常涉及计算用例特定统计数据，例如满足多列约束条件的记录百分比，这些数据在完全自动化的方法中会被排除在外。
- en: A Four-Pillar Approach to Data Quality Monitoring
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量监控的四支柱方法
- en: 'We’ve covered a wide range of monitoring strategies in this chapter. To recap,
    here are the different approaches an organization might take:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中涵盖了广泛的监控策略。总结一下，组织可能采取的不同方法如下：
- en: Do nothing (yikes!).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么都不做（呀！）。
- en: Implement data observability for your data warehouse (table stakes, but you
    aren’t really monitoring data quality at all).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的数据仓库实施数据可观察性（基本要求，但您实际上并没有完全监控数据质量）。
- en: Monitor a small subset of the data with handcrafted rules or metrics. This misses
    unknown unknowns (the issues you didn’t know to check for) and covers only a fraction
    of the data.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用手工制定的规则或度量监视数据的一个小子集。这会忽略未知的问题（您没有意识到需要检查的问题），并且仅覆盖数据的一小部分。
- en: Monitor all the data using handcrafted rules and metrics. This is very expensive
    in setup and maintenance time, results in noisy alerts, and will also miss unknown
    unknowns.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用手工制定的规则和度量监视所有数据。这在设置和维护时间上非常昂贵，导致嘈杂的警报，并且还会错过未知的问题。
- en: Automate monitoring with rules. This is extremely expensive, very brittle, and
    still misses tons of distribution and correlation issues.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用规则自动化监控。这非常昂贵，非常脆弱，并且仍然会错过大量分布和相关性问题。
- en: Automate monitoring with metrics. This is also extremely expensive, extremely
    noisy, and still misses tons of record-level issues.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用度量自动化监控。这也非常昂贵，非常嘈杂，并且仍然会错过大量记录级问题。
- en: Automate with unsupervised monitoring only. While this provides good coverage,
    it can’t catch everything and won’t pay as close attention to critical data as
    handcrafted rules and metrics will.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用无监督监控自动化。虽然这提供了良好的覆盖率，但不能捕捉所有问题，并且不会像手工制定的规则和度量那样密切关注关键数据。
- en: (Our recommendation) *Use a four-pillar approach*. You can implement data observability
    at low cost across your entire warehouse. Meanwhile, for data quality, automated
    unsupervised ML can provide a base level of coverage for obvious issues and unknown
    unknowns. Your platform should make it very easy for SMEs to augment automated
    monitoring with low-code validation rules and time series metric monitoring for
    the most important data and relationships.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （我们的建议）*采用四柱方法*。您可以以较低成本在整个数据仓库中实施数据可观察性。同时，对于数据质量，自动化的无监督机器学习可以提供明显问题和未知未知的基础覆盖。您的平台应该使中小企业能够通过低代码验证规则和时间序列度量监控最重要的数据和关系，从而非常容易地增强自动化监控。
- en: Data observability, rule-based testing, metrics monitoring, and unsupervised
    ML can be used in combination to achieve the previously stated goals of *detect*,
    *alert*, *resolve*, and *scale*. This strategy gives you high coverage of real
    data quality risk while minimizing false positives and alert fatigue—all without
    having an army of analysts dedicated to the problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可观察性、基于规则的测试、度量监控和无监督机器学习可以结合使用，以实现之前提到的*检测*、*警报*、*解决*和*扩展*的目标。这种策略可以高效覆盖真实数据质量风险，同时最小化误报和警报疲劳，而无需专门解决这个问题的分析师团队。
- en: '[Figure 2-6](#the_four_pillars_of_a_comprehensive_aut) explains how the four
    components of this strategy balance each other’s strengths and weaknesses. [Figure 2-7](#examples_of_how_different_strategies_ca)
    illustrates the capabilities of rules, metrics, and ML with some basic sample
    data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-6](#the_four_pillars_of_a_comprehensive_aut)解释了这种策略的四个组成部分如何平衡彼此的优势和劣势。[图 2-7](#examples_of_how_different_strategies_ca)展示了规则、度量和机器学习在一些基本示例数据中的能力。'
- en: '![The four pillars of a comprehensive automated data quality solution](assets/adqm_0206.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![全面自动化数据质量解决方案的四个支柱](assets/adqm_0206.png)'
- en: Figure 2-6\. The four pillars of a comprehensive automated data quality solution.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6。全面自动化数据质量解决方案的四个支柱。
- en: '![Examples of how different strategies can detect different types of changes
    in the data. Note that this is a simplified example, both in the data shown and
    the scope of issues detected.](assets/adqm_0207.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![不同策略如何检测数据中不同类型变化的示例。请注意，这是一个简化的示例，无论是显示的数据还是检测到的问题范围。](assets/adqm_0207.png)'
- en: Figure 2-7\. Examples of how different strategies can detect different types
    of changes in the data. Note that this is a simplified example, both in the data
    shown and the scope of issues detected.
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7。不同策略如何检测数据中不同类型变化的示例。请注意，这是一个简化的示例，无论是显示的数据还是检测到的问题范围。
- en: The rest of this book is a guide to automating data quality monitoring at your
    organization following the four-pillar approach we’ve described here. We’ll start
    by making sure that this approach is right for your organization and that the
    return on investment (ROI) would make sense for you and your data team. Then,
    we’ll cover modeling strategies and trade-offs before moving on to key features
    such as notifications and integrations. We’ll close by sharing how you can continue
    to maintain and grow your data health as your organization evolves.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分是关于在您的组织中自动化数据质量监控的指南，按照我们在这里描述的四柱方法。我们将首先确保这种方法适合您的组织，并且投资回报率（ROI）对您和您的数据团队是合理的。然后，我们将涵盖建模策略和权衡，然后转向关键功能，如通知和集成。最后，我们将分享如何在您的组织发展过程中继续维护和增强您的数据健康。
- en: ^([1](ch02.html#ch01fn1-marker)) The phi-K correlation is similar to traditional
    correlation methods (the Spearman or Pearson correlation) but can be used across
    data of varying types (numeric and categorical) and captures nonlinear relationships
    while behaving identically to the Pearson correlation for bivariate normal distributions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#ch01fn1-marker)) Phi-K 相关性类似于传统的相关方法（如斯皮尔曼或皮尔逊相关性），但可以用于不同类型的数据（数值和分类），并捕捉非线性关系，同时对于双变量正态分布的行为与皮尔逊相关性相同。
