- en: 4 Analyzing tabular data with pyspark.sql
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用 pyspark.sql 分析表格数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Reading delimited data into a PySpark data frame
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分隔符数据读取到 PySpark 数据框中
- en: Understanding how PySpark represents tabular data in a data frame
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 PySpark 如何在数据框中表示表格数据
- en: Ingesting and exploring tabular or relational data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入和探索表格或关系数据
- en: Selecting, manipulating, renaming, and deleting columns in a data frame
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据框中选择、操作、重命名和删除列
- en: Summarizing data frames for quick exploration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述数据框以快速探索
- en: 'Our first example in chapters 2 and 3 worked with unstructured textual data.
    Each line of text was mapped to a record in a data frame, and, through a series
    of transformations, we counted word frequencies from one (and multiple) text files.
    This chapter goes deeper into data transformation, this time using structured
    data. Data comes in many shapes and forms: we start with *relational* (or *tabular*,[¹](#pgfId-1011836)
    or row and columns) data, one of the most common formats popularized by SQL and
    Excel. This chapter and the next follow the same blueprint as we did with our
    first data analysis. We use the public Canadian television schedule data to identify
    and measure the proportion of commercials over its total programming.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章 2 和 3 章中的第一个例子是处理非结构化文本数据。每一行文本都被映射到数据框中的一个记录，通过一系列转换，我们从（一个或多个）文本文件中计算单词频率。本章深入数据转换，这次使用结构化数据。数据有多种形状和形式：我们开始于
    *关系型*（或 *表格型*，[¹](#pgfId-1011836) 或行和列）数据，这是 SQL 和 Excel 流行的一种最常见格式。本章和下一章遵循与我们的第一个数据分析相同的蓝图。我们使用公共加拿大电视节目时间表数据来识别和测量总节目中的广告比例。
- en: More specifically, I start by giving a primer on tabular data and how the data
    frame provides the necessary abstractions to represent a data table. I then specialize
    the `SparkReader` object once more, this time for delimited data rather than unstructured
    text. I then cover the most common operations on `Column` objects, processing
    data in a two-dimensional setting. Finally, I cover PySpark’s lightweight EDA
    (exploratory data analysis) capabilities through the `summary()` and `describe()`
    methods. By the end of this chapter, you will be able to explore and change the
    columnar structure of a data frame. More excitingly, this knowledge will apply
    regardless of the data format (e.g., hierarchical data in chapter 6).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我首先对表格数据及其如何通过数据框提供必要的抽象来表示数据表进行入门介绍。然后，我将 `SparkReader` 对象再次专门化，这次是为了分隔符数据而不是非结构化文本。然后，我涵盖了
    `Column` 对象上最常用的操作，在二维设置中处理数据。最后，我通过 `summary()` 和 `describe()` 方法介绍了 PySpark
    的轻量级 EDA（探索性数据分析）功能。到本章结束时，你将能够探索和更改数据框的列结构。更有趣的是，这些知识将适用于任何数据格式（例如，第 6 章中的层次数据）。
- en: Just like with every PySpark program, we start by initializing our `SparkSession`
    object, as in the next listing. I also proactively import the `pyspark.sql.functions`
    as a qualified `F`, since we saw in chapter 3 that it helps with readability and
    avoiding potential name clashes for functions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像每个 PySpark 程序一样，我们首先初始化我们的 `SparkSession` 对象，如下所示。我还主动导入 `pyspark.sql.functions`
    作为合格的 `F`，因为我们看到在第 3 章中，这有助于提高可读性并避免函数名称冲突的潜在问题。
- en: Listing 4.1 Creating our `SparkSession` object to start using PySpark
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 创建我们的 `SparkSession` 对象以开始使用 PySpark
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4.1 What is tabular data?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 什么是表格数据？
- en: 'We call data tabular when we represent it in a two-dimensional table. You have
    cells, each containing a single (or *simple*) value, organized into rows and columns.
    A good example is your grocery list: you may have one column for the item you
    wish to purchase, one for the quantity, and one for the expected price. Figure
    4.1 provides an example of a small grocery list. We have the three columns mentioned,
    as well as four rows, each representing an entry in our grocery list.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据表示为二维表格时，我们称之为表格数据。你有单元格，每个单元格包含一个单一（或 *简单*）值，组织成行和列。一个很好的例子是你的购物清单：你可能有一列是你想要购买的物品，一列是数量，一列是预期的价格。图
    4.1 提供了一个小型购物清单的例子。我们有提到的三个列，以及四行，每行代表购物清单中的一个条目。
- en: '![](../Images/04-01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-01.png)'
- en: Figure 4.1 My grocery list represented as tabular data. Each row represents
    an item, and each column represents an attribute.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 我将购物清单表示为表格数据。每一行代表一个项目，每一列代表一个属性。
- en: 'The easiest analogy we can make for tabular data is the spreadsheet format:
    the interface provides you with a large number of rows and columns where you can
    input and perform computations on data. SQL databases can be thought of as tables
    made up of rows and columns. Tabular data is an extremely common data format,
    and because it’s so popular and easy to reason about, it makes for a perfect first
    dive into PySpark’s data manipulation API.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来类比表格数据的最佳类比是电子表格格式：界面为你提供了一个大量行和列，你可以在其中输入和执行数据计算。可以将 SQL 数据库视为由行和列组成的表格。表格数据是一种极其常见的数据格式，因为它如此流行且易于推理，因此它成为
    PySpark 数据操作 API 的完美入门。
- en: PySpark’s data frame structure maps very naturally to tabular data. In chapter
    2, I explain that PySpark operates either on the whole data frame structure (via
    methods such as `select()` and `groupby()`) or on `Column` objects (e.g., when
    using a function like `split()`). The data frame is *column-major*, so its API
    focuses on manipulating the columns to transform the data. Because of this, we
    can simplify how we reason about data transformations by thinking about what operations
    to perform and which columns will be impacted by them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的数据框结构非常自然地映射到表格数据。在第 2 章中，我解释了 PySpark 可以通过 `select()` 和 `groupby()`
    等方法在整个数据框结构上操作，或者操作 `Column` 对象（例如，使用 `split()` 这样的函数）。数据框是列主序的，因此它的 API 专注于操纵列以转换数据。正因为如此，我们可以通过考虑要执行的操作以及哪些列会受到它们的影响来简化我们对数据转换的推理。
- en: Note The resilient distributed dataset, briefly introduced in chapter 1, is
    a good example of a structure that is *row-major*. Instead of thinking about columns,
    you are thinking about items (rows) with attributes in which you apply functions.
    It’s an alternative way of thinking about your data, and chapter 8 contains more
    information about where/when it can be useful.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在第 1 章中简要介绍的弹性分布式数据集是一个列主序结构的良好例子。你不需要考虑列，而是考虑具有属性的项目（行），在这些属性上应用函数。这是思考数据的一种替代方式，第
    8 章包含了更多关于在哪里/何时它可能有用的信息。
- en: 4.1.1 How does PySpark represent tabular data?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 PySpark 如何表示表格数据？
- en: In chapters 2 and 3, our data frame always contained a single column, up to
    the very end when we counted the occurrence of each word. In other words, we took
    unstructured data (a body of text), performed some transformations, and created
    a two-column table containing the information we wanted. Tabular data is, in a
    way, an extension of this, where we have more than one column to work with.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章和第 3 章中，我们的数据框始终只包含一个列，直到我们最后计算每个单词的出现次数。换句话说，我们处理了非结构化数据（一段文本），进行了一些转换，并创建了一个包含我们所需信息的两列表格。表格数据在某种程度上是这种方法的扩展，其中我们有多于一个列可以操作。
- en: Let’s take my very healthy grocery list as an example, and load it into PySpark.
    To make things simple, we’ll encode our grocery list into a list of lists. PySpark
    has multiple ways to import tabular data, but the two most popular are the list
    of lists and the pandas data frame. In chapter 9, I briefly cover how to work
    with pandas. It would be a bit overkill to import a library just for loading four
    records (four items on our grocery list), so I kept it in a list of lists.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我的非常健康的购物清单为例，并将其加载到 PySpark 中。为了简化问题，我们将我们的购物清单编码为一个列表的列表。PySpark 有多种导入表格数据的方式，但最流行的是列表的列表和
    pandas 数据框。在第 9 章中，我简要介绍了如何使用 pandas。仅为了导入四个记录（我们购物清单上的四个项目）而导入一个库可能有点过度，所以我将其保留在列表的列表中。
- en: Listing 4.2 Creating a data frame out of our grocery list
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 从我们的购物清单创建数据框
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ My grocery list is encoded in a list of lists.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我的购物清单编码在一个列表的列表中。
- en: ❷ PySpark automatically inferred the type of each field from the information
    Python had about each value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ PySpark 自动从 Python 对每个值的了解中推断出每个字段的类型。
- en: We can easily create a data frame from the data in our program with the `spark`
    `.createDataFrame` function, as listing 4.2 shows. Our first parameter is the
    data itself. You can provide a list of items (here, a list of lists), a pandas
    data frame, or a resilient distributed dataset, which I cover in chapter 8\. The
    second parameter is the *schema* of the data frame. Chapter 6 covers the automatic
    and manual schema definitions in greater depth. In the meantime, passing a list
    of column names will make PySpark happy while it infers the types (`string`, `long`,
    and `double`, respectively) of our columns. Visually, the data frame will look
    like figure 4.2, although much more simplified. The master node knows about the
    structure of the data frame, but the actual data is represented on the worker
    nodes. Each column maps to data stored somewhere on our cluster that is managed
    by PySpark. We operate on the abstract structure and let the master delegate the
    work efficiently.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`spark`的`.createDataFrame`函数轻松地从程序中的数据创建一个数据框，如列表4.2所示。我们的第一个参数是数据本身。你可以提供一个项目列表（这里是一个列表的列表）、一个pandas数据框或一个弹性分布式数据集，这些内容我在第8章中进行了介绍。第二个参数是数据框的*模式*。第6章更深入地介绍了自动和手动模式定义。在此期间，传递一个列名列表将使PySpark感到高兴，同时它会推断出我们列的类型（分别为`string`、`long`和`double`）。从视觉上看，数据框将类似于图4.2，尽管更加简化。主节点了解数据框的结构，但实际数据是在工作节点上表示的。每一列映射到我们集群上某个位置存储的数据，该数据由PySpark管理。我们在抽象结构上操作，让主节点有效地分配工作。
- en: '![](../Images/04-02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-02.png)'
- en: Figure 4.2 Each column of our data frame maps to some place on our worker nodes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2我们的数据框的每一列都映射到我们的工作节点上的某个位置。
- en: PySpark gladly represented our tabular data using our column definitions. This
    means that all the functions we’ve learned so far apply to our tabular data. By
    having one flexible structure for many data representations—we’ve covered text
    and tabular so far—PySpark makes it easy to move from one domain to another. It
    removes the need to learn yet another set of functions and a whole new abstraction
    for our data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark很乐意使用我们的列定义来表示我们的表格数据。这意味着我们迄今为止学到的所有函数都适用于我们的表格数据。通过为多种数据表示提供一个灵活的结构——到目前为止我们已经涵盖了文本和表格——PySpark使得从一个领域转移到另一个领域变得容易。它消除了学习另一组函数和全新的数据抽象的需求。
- en: This section covered the look and feel of a simple two-dimensional/tabular data
    frame. In the next section, we ingest and process a more significant data frame.
    It’s time for some coding!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了简单二维/表格型数据框的外观和感觉。在下一节中，我们将处理一个更重要的数据框。是时候开始编码了！
- en: 4.2 PySpark for analyzing and processing tabular data
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 使用PySpark分析和处理表格数据
- en: My grocery list was fun, but the potential for analysis work is pretty limited.
    We’ll get our hands on a larger data set, explore it, and ask a few introductory
    questions that we might find interesting. This process is called *exploratory
    data analysis* (or EDA) and is usually the first step data analysts and scientists
    undertake when placed in front of new data. Our goal is to get familiar with the
    data discovery functions and methods, as well as with performing some basic data
    assembly. Being familiar with those steps will remove the awkwardness of working
    with data you won’t see transforming before your eyes. This section shows you
    a blueprint you can reuse when facing new data frames until you can visually process
    millions of records per second.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我的购物清单很有趣，但分析工作的潜力相当有限。我们将接触到更大的数据集，对其进行探索，并提出一些我们可能感兴趣的基本问题。这个过程被称为*探索性数据分析*（或EDA），通常是数据分析师和科学家在面临新数据时采取的第一步。我们的目标是熟悉数据发现函数和方法，以及进行一些基本的数据组装。熟悉这些步骤将消除在数据面前无法看到其转换的尴尬。本节向您展示了一个蓝图，您可以在面对新的数据框时重复使用，直到您能够以每秒处理数百万条记录的速度进行视觉处理。
- en: Graphical exploratory data analysis
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图形探索性数据分析
- en: A lot of the EDA work you’ll see in the wild incorporates charts and/or tables.
    Does this mean that PySpark has the option to do the same?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你在野外看到的许多EDA工作都包含图表和/或表格。这意味着PySpark有选项做同样的事情吗？
- en: We saw in chapter 2 how to print a data frame so that we can view the content
    at a glance. This still applies to summarizing information and displaying it on
    the screen. If you want to export the table in an easy-to-process format (e.g.,
    to incorporate it in a report), you can use `spark.write.csv`, making sure you
    coalesce the data frame in a single file. (See chapter 3 for a refresher on `coalesce()`.)
    By its very nature, table summaries won’t be very large, so you won’t risk running
    out of memory.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 2 章中看到了如何打印数据框，以便我们可以一目了然地查看内容。这同样适用于总结信息和在屏幕上显示。如果你想以易于处理的形式（例如，将其纳入报告）导出表格，可以使用
    `spark.write.csv`，确保将数据框合并为单个文件。（参见第 3 章中对 `coalesce()` 的复习。）由于表格摘要的性质，它不会很大，所以你不会面临内存不足的风险。
- en: 'PySpark doesn’t provide any charting capabilities and doesn’t play with other
    charting libraries (like Matplotlib, seaborn, Altair, or plot.ly), and this makes
    a lot of sense: PySpark distributes your data over many computers. It doesn’t
    make much sense to distribute a chart creation. The usual solution will be to
    transform your data using PySpark, use the `toPandas()` method to transform your
    PySpark data frame into a pandas data frame, and then use your favorite charting
    library. When using charts, I provide the code I used to generate them.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 不提供任何图表功能，也不与其他图表库（如 Matplotlib、seaborn、Altair 或 plot.ly）交互，这很有道理：PySpark
    将你的数据分布到多台计算机上。分布图表创建没有太多意义。通常的解决方案将是使用 PySpark 转换你的数据，使用 `toPandas()` 方法将你的 PySpark
    数据框转换为 pandas 数据框，然后使用你喜欢的图表库。在使用图表时，我会提供我用来生成它们的代码。
- en: When using `toPandas()`, remember that you lose the advantages of working with
    multiple machines, as the data will accumulate on the driver. Reserve this operation
    for an aggregated or manageable data set. While this is a crude formula, I usually
    take the number of rows times the number of columns; if this number is over 100,000
    (for a 16 GB driver), I try to reduce it further. This simple trick helps me get
    a sense of the size of the data I am dealing with, as well as what’s possible
    given my driver size.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `toPandas()` 时，请记住，你将失去在多台机器上工作的优势，因为数据将累积在驱动器上。将此操作保留用于聚合或可管理的数据集。虽然这是一个粗略的公式，我通常取行数乘以列数；如果这个数字超过
    100,000（对于 16 GB 的驱动器），我会进一步尝试减少它。这个简单的技巧帮助我了解我正在处理的数据的大小，以及根据我的驱动器大小可能实现的内容。
- en: You do not want to move your data between a pandas and a PySpark data frame
    all the time. Reserve `toPandas()` for either discrete operations or for moving
    your data into a pandas data frame once and for all. Moving back and forth will
    yield a ton of unnecessary work in distributing and collecting the data for nothing.
    If you need pandas functionality on a Spark data frame, check out pandas UDFs
    in chapter 9.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你不希望总是将数据在 pandas 和 PySpark 数据框之间移动。将 `toPandas()` 保留用于离散操作，或者一次性将数据移动到 pandas
    数据框中。来回移动会产生大量不必要的劳动，用于分配和收集数据，却毫无意义。如果你需要在 Spark 数据框上使用 pandas 功能，请查看第 9 章中的
    pandas UDFs。
- en: 'For this exercise, we’ll use some open data from the government of Canada,
    more specifically the CRTC (Canadian Radio-Television and Telecommunications Commission).
    Every broadcaster is mandated to provide a complete log of the programs and commercials
    showcased to the Canadian public. This gives us a lot of potential questions to
    answer, but we’ll select just one: *What are the channels with the greatest and
    least proportion of commercials?*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将使用来自加拿大政府的公开数据，更具体地说，是加拿大广播和电视电信委员会（CRTC）。每个广播机构都要求提供向加拿大公众展示的节目和商业广告的完整日志。这为我们提供了许多潜在的问题要回答，但我们将选择一个问题：*哪些频道拥有最大和最小的商业广告比例？*
- en: You can download the file on the Canada Open Data portal ([http://mng.bz/y4YJ](http://mng.bz/y4YJ));
    select the `BroadcastLogs_2018_Q3_M8` file. The file is 994 MB to download, which
    might be too large, depending on your computer. The book’s repository contains
    a sample of the data under the data/broadcast_logs directory, which you can use
    in place of the original file. You also need to download the Data Dictionary in
    .doc form, as well as the Reference Tables zip file, unzipping them into a `ReferenceTables`
    directory in `data/ broadcast_logs`. Once again, the examples assume that the
    data is downloaded under data/broadcast_logs and that PySpark is launched from
    the root of the repository.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从加拿大开放数据门户下载该文件（[http://mng.bz/y4YJ](http://mng.bz/y4YJ)）；选择`BroadcastLogs_2018_Q3_M8`文件。文件大小为994
    MB，下载可能太大，取决于您的计算机。本书的存储库在`data/broadcast_logs`目录下包含数据的样本，您可以使用它替换原始文件。您还需要下载以.doc格式存在的数据字典，以及参考表的压缩文件，将它们解压缩到`data/
    broadcast_logs`目录下的`ReferenceTables`目录中。再次强调，示例假设数据下载在`data/broadcast_logs`下，并且PySpark是从存储库的根目录启动的。
- en: 'Before moving to the next section, make sure you have the following. With the
    exception of the large `BroadcastLogs` file, the rest is in the repository:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请确保您有以下内容。除了大的`BroadcastLogs`文件外，其余都在存储库中：
- en: '`data/BroadcastLogs_2018_Q3_M8.CSV` (either download from the website or use
    the sample from the repo)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/BroadcastLogs_2018_Q3_M8.CSV`（可以从网站下载或使用存储库中的样本）'
- en: '`data/broadcast_logs/ReferenceTables`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/broadcast_logs/ReferenceTables`'
- en: '`data/broadcast_logs/data_dictionary.doc`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/broadcast_logs/data_dictionary.doc`'
- en: 4.3 Reading and assessing delimited data in PySpark
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 在PySpark中读取和评估分隔数据
- en: Now that we have tested the waters with a small synthetic tabular data set,
    we are ready to dive into real data. Just like in chapter 3, our first step is
    to read the data before we can perform exploration and transformation. This time,
    we read data that is a little more complex than just some unorganized text. Because
    of this, I cover the `SparkReader` usage in more detail. As the two-dimensional
    table is one of the most common organization formats, knowing how to ingest tabular
    or relational data will become second nature very quickly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用一个小型的合成表格数据集进行了测试，我们准备深入实际数据。就像在第3章中一样，我们的第一步是在进行探索和转换之前读取数据。这次，我们读取的数据比一些无组织的文本要复杂一些。因此，我更详细地介绍了`SparkReader`的使用。由于二维表是最常见的组织格式之一，了解如何摄取表格或关系数据将很快变得自然而然。
- en: 'Tip Relational data is often in a SQL database. Spark can read from SQL (or
    SQL-like) data stores very easily: check chapter 9 for an example where I read
    from Google BigQuery.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：关系数据通常存储在SQL数据库中。Spark可以轻松地从SQL（或类似SQL）数据存储中读取：请参阅第9章的示例，其中我读取了Google BigQuery。
- en: In this section, I start by covering the usage of the `SparkReader` for delimited
    data, or data that is separated by a *delimited character* (to create this second
    dimension), by applying it to one of the CRTC data tables. I then review the most
    common reader’s options, so you can read other types of delimited files with ease.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我首先介绍`SparkReader`在分隔数据或以分隔字符分隔的数据（以创建第二个维度）中的应用，通过将其应用于CRTC数据表之一。然后，我回顾了最常见的读取器选项，以便您可以轻松地读取其他类型的分隔文件。
- en: 4.3.1 A first pass at the SparkReader specialized for CSV files
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 针对CSV文件的SparkReader的第一次尝试
- en: Delimited data is a very common, popular, and tricky way of sharing data. In
    this section, I cover how to read the CRTC tables, which use a pretty common set
    of conventions for CSV files.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔数据是一种非常常见、流行且棘手的数据共享方式。在本节中，我介绍了如何读取CRTC表，这些表使用一组相当常见的约定来处理CSV文件。
- en: 'The CSV file format stems from a simple idea: we use *text*, separated in two-dimensional
    *records* (rows and columns), that are separated by two types of delimiters. Those
    delimiters are characters, but they serve a special purpose when applied in the
    context of a CSV file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件格式源于一个简单的想法：我们使用*文本*，以二维*记录*（行和列）的形式分隔，这些记录由两种类型的分隔符分隔。这些分隔符是字符，但在CSV文件的应用中它们具有特殊的作用：
- en: The first one is a *row delimiter*. The row delimiter splits the file into logical
    records. There is one and only one record between delimiters.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是*行分隔符*。行分隔符将文件分割成逻辑记录。分隔符之间只有一个记录。
- en: The second one is a *field delimiter*. Each record is made up of an identical
    number of fields, and the field delimiter tells where one field starts and ends.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种是*字段分隔符*。每个记录由相同数量的字段组成，字段分隔符指示一个字段开始和结束的位置。
- en: '![](../Images/04-03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-03.png)'
- en: Figure 4.3 A sample of our data, highlighting the field delimiter (`|`) and
    row delimiter (`\n`)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 我们数据的一个样本，突出显示了字段分隔符（`|`）和行分隔符（`\n`）
- en: The newline character (`\n`, when depicted explicitly) is the de facto record
    delimiter. It naturally breaks down the file into visual lines, where one record
    starts at the beginning of the line and ends, well, at the end. The comma character
    (`,`) is the most frequent field delimiter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 换行符（`\n`，当明确表示时）是事实上的记录分隔符。它自然地将文件分解为视觉行，其中一条记录从行的开头开始，并在行尾结束。逗号字符（`,`）是最常见的字段分隔符。
- en: 'CSV files are easy to produce and have a loose set of rules to follow to be
    considered usable. Because of this, PySpark provides a whopping 25 optional parameters
    when ingesting a CSV file. Compare this to the two for reading text data. In listing
    4.3, I use three configuration parameters: the record delimiter through `sep`
    and the presence of a header (column names) row through `header`, and I finally
    ask Spark to infer the data types for me with `inferSchema` (more on this in section
    4.3.2). This is enough to parse our data into a data frame.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件易于生成，遵循一套松散的规则即可被认为是可用的。因此，PySpark 在读取 CSV 文件时提供了 25 个可选参数。将其与读取文本数据的两个参数进行比较。在列表
    4.3 中，我使用了三个配置参数：通过 `sep` 的记录分隔符、通过 `header` 的存在（列名）行，以及我最终要求 Spark 通过 `inferSchema`
    推断数据类型（更多内容请见第 4.3.2 节）。这足以将我们的数据解析为数据框。
- en: Listing 4.3 Reading our broadcasting information
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 读取我们的广播信息
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ We specify the file path where our data resides first.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们首先指定数据所在文件的路径。
- en: ❷ Our file uses a vertical bar as delimiter/separator, so we pass | as a parameter
    to sep.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的文件使用竖线作为分隔符/分隔符，因此我们将 | 作为参数传递给 sep。
- en: ❸ header takes a Boolean. When true, the first row of your file is parsed as
    the column names.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ header 接受一个布尔值。当为 true 时，文件的第一行被解析为列名。
- en: ❹ inferSchema takes a Boolean as well. When true, it’ll pre-parse the data to
    infer the type of the column.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ inferSchema 也接受一个布尔值。当为 true 时，它将预解析数据以推断列的类型。
- en: ❺ timestampFormat is used to inform the parser of the format (year, month, day,
    hour, minutes, seconds, microseconds) of the timestamp fields (see section 4.4.3).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ timestampFormat 用于通知解析器时间戳字段的格式（年、月、日、小时、分钟、秒、微秒）（见第 4.4.3 节）。
- en: While we were able to read the CSV data for our analysis, this is just one narrow
    example of the usage of the `SparkReader`. The next section expands on the most
    important parameters when reading CSV data and provides more detailed explanations
    behind the code used in listing 4.3\.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们能够读取用于分析的数据的 CSV 数据，但这只是 `SparkReader` 使用的一个狭义示例。下一节将扩展到读取 CSV 数据时最重要的参数，并提供对列表
    4.3 中使用的代码的更详细解释。
- en: 4.3.2 Customizing the SparkReader object to read CSV data files
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 自定义 SparkReader 对象以读取 CSV 数据文件
- en: This section focuses on how we can specialize the `SparkReader` object to read
    delimited data and the most popular configuration parameters to accommodate the
    various declinations of CSV data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍如何专门化 `SparkReader` 对象以读取分隔数据以及最流行的配置参数，以适应 CSV 数据的各种变体。
- en: Listing 4.4 The `spark.read.csv` function, with every parameter explicitly laid
    out
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 `spark.read.csv` 函数，其中每个参数都明确列出
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Reading delimited data can be a dicey business. Because of how flexible and
    human-editable the format is, a CSV reader needs to provide many options to cover
    the many use cases possible. There is also a risk that the file is malformed,
    in which case you will need to treat it as text and gingerly infer the fields
    manually. I will stay on the happy path and cover the most popular scenario: a
    single file, properly delimited.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 读取分隔数据可能是一个风险业务。由于格式非常灵活且可由人类编辑，CSV 读取器需要提供许多选项来覆盖可能出现的许多用例。还存在文件格式不正确的风险，在这种情况下，您需要将其视为文本，并谨慎地手动推断字段。我将遵循快乐路径，并介绍最流行的场景：单个文件，正确分隔。
- en: The path to the file you want to read as the only mandatory parameter
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要读取的文件的路径作为唯一必填参数
- en: Just like when reading text, the only truly mandatory parameter is the `path`,
    which contains the file or files’ path. As we saw in chapter 2, you can use a
    glob pattern to read multiple files inside a given directory, as long as they
    have the same structure. You can also explicitly pass a list of file paths if
    you want specific files to be read.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 就像读取文本一样，唯一真正必填的参数是 `path`，它包含文件或文件的路径。正如我们在第 2 章中看到的，您可以使用 glob 模式读取给定目录内的多个文件，只要它们具有相同的结构。您也可以显式传递文件路径列表，如果您只想读取特定文件。
- en: Passing an explicit field delimiter with the sep parameter
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sep 参数传递显式字段分隔符
- en: 'The most common variation you’ll encounter when ingesting and producing CSV
    files is selecting the right delimiter. The comma is the most popular, but it
    suffers from being a popular character in text, which means you need a way to
    differentiate which commas are part of the text and which are delimiters. Our
    file uses the vertical bar character, an apt choice: it’s easily reachable on
    the keyboard yet infrequent in text.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当摄取和生成 CSV 文件时，你将遇到的最常见的变体是选择正确的分隔符。逗号是最受欢迎的，但它的问题在于它是一个在文本中很常见的字符，这意味着你需要一种方法来区分哪些逗号是文本的一部分，哪些是分隔符。我们的文件使用的是竖线字符，这是一个合适的选择：它在键盘上容易到达，但在文本中不常见。
- en: Note In French, we use the comma for separating numbers between their integral
    part and their decimal part (e.g., `1.02` → `1,02`). This is pretty awful when
    in a CSV file, so most French CSVs will use the semicolon (`;`) as a field delimiter.
    This is one more example of why you need to be vigilant when using CSV data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在法语中，我们使用逗号来分隔整数部分和小数部分之间的数字（例如，`1.02` → `1,02`）。这在 CSV 文件中相当糟糕，因此大多数法语 CSV
    文件将使用分号（`;`）作为字段分隔符。这是你需要在使用 CSV 数据时保持警惕的另一个例子。
- en: When reading CSV data, PySpark will default to using the comma character as
    a field delimiter. You can set the optional parameter `sep` (for separator) to
    the single character you want to use as a field delimiter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当读取 CSV 数据时，PySpark 将默认使用逗号字符作为字段分隔符。你可以设置可选参数 `sep`（分隔符）为你想要用作字段分隔符的单个字符。
- en: Quoting text to avoid mistaking a character for a delimiter
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 引用文本以避免将字符误认为是分隔符
- en: When working with CSVs that use the comma as a delimiter, it’s common practice
    to *quote* the text fields to make sure any comma in the text is not mistaken
    for a field separator. The CSV reader object provides an optional `quote` parameter
    that defaults to the double-quote character (`"`). Since I am not passing an explicit
    value to `quote`, we are keeping the default value. This way, we can have a field
    with the value `"Three` `|` `Trois"`, whereas without the quotation characters,
    we would consider this two fields. If we don’t want to use any character as a
    quote, we need to explicitly pass the empty string to `quote`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理使用逗号作为分隔符的 CSV 文件时，通常的做法是 *引用* 文本字段，以确保文本中的任何逗号都不会被误认为是字段分隔符。CSV 读取器对象提供了一个可选的
    `quote` 参数，默认值为双引号字符（`"`）。由于我没有向 `quote` 传递显式值，我们保留了默认值。这样，我们可以有一个值为 `"Three`
    `|` `Trois"` 的字段，而如果没有引号字符，我们会将其视为两个字段。如果我们不想使用任何字符作为引号，我们需要显式地将空字符串传递给 `quote`。
- en: Using the first row as the column names
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第一行作为列名
- en: The `header` optional parameter takes a Boolean flag. If set to true, it’ll
    use the first row of your file (or files, if you’re ingesting many) and use it
    to set your column names.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`header` 可选参数接受一个布尔标志。如果设置为 true，它将使用你的文件的第一行（或如果你正在摄取多个文件，则为这些文件）并将其用于设置列名。'
- en: You can also pass an explicit schema (see chapter 6) or a DDL string (see chapter
    7) as the `schema` optional parameter if you wish to explicitly name your columns.
    If you don’t fill any of those, your data frame will have `_c*` for column names,
    where the `*` is replaced with increasing integers (`_c0`, `_c1`, . . .).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望显式地命名列，也可以传递一个显式模式（见第 6 章）或 DDL 字符串（见第 7 章）作为 `schema` 可选参数。如果你不填写任何这些，你的数据框将使用
    `_c*` 作为列名，其中 `*` 被替换为递增的整数（`_c0`，`_c1`，……）。
- en: Inferring column type while reading the data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取数据时推断列类型
- en: 'PySpark has a schema-discovering capacity. You turn it on by setting `inferSchema`
    to `True` (by default, this is turned off). This optional parameter forces PySpark
    to go over the ingested data twice: one time to set the type of each column, and
    one time to ingest the data. This makes the ingestion quite a bit longer but helps
    us avoid writing the schema by hand (I go down to this level of detail in chapter
    6). Let the machine do the work!'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 具有模式发现能力。你可以通过将 `inferSchema` 设置为 `True`（默认情况下是关闭的）来开启它。这个可选参数会强制 PySpark
    对摄取的数据进行两次遍历：一次用于设置每列的类型，一次用于摄取数据。这使得摄取过程变得相当长，但有助于我们避免手动编写模式（我在第 6 章中深入到这个细节）。让机器来做这项工作吧！
- en: Tip Inferring the schema can be very expensive if you have a lot of data. In
    chapter 6, I cover how to work with (and extract) schema information; if you read
    a data source multiple times, it’s a good idea to keep the schema information
    once inferred! You can also take a small representative data set to infer the
    schema, followed by reading the large data set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如果你有大量数据，推断模式可能会非常昂贵。在第 6 章中，我介绍了如何处理（和提取）模式信息；如果你多次读取数据源，保留推断出的模式信息是个好主意！你也可以取一个小型的代表性数据集来推断模式，然后读取大型数据集。
- en: We are lucky enough that the government of Canada is a good steward of data
    and provides us with clean, properly formatted files. In the wild, malformed CSV
    files are legion, and you will run into errors when trying to ingest some of them.
    Furthermore, if your data is large, you often won’t get the chance to inspect
    each row to fix mistakes. Chapter 6 covers some strategies to ease the pain and
    shows you some ways to share your data with the schema included.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运，加拿大政府是数据的好管家，为我们提供了干净、格式正确的文件。在野外，格式不正确的 CSV 文件很多，当你尝试摄入其中的一些文件时，你会遇到错误。此外，如果你的数据很大，你通常不会有时间检查每一行来修复错误。第
    6 章介绍了一些减轻痛苦的策略，并展示了包含模式共享数据的方法。
- en: Our data frame schema, displayed in the next listing, is coherent with the documentation
    we’ve downloaded. The column names are properly displayed, and the types make
    sense. That’s enough to get started with some exploration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据帧模式，在下一条列表中显示，与我们所下载的文档是一致的。列名被正确显示，类型也合理。这足以开始一些探索。
- en: Listing 4.5 The schema of our `logs` data frame
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 我们的 `logs` 数据帧的模式
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Exercise 4.1
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4.1
- en: 'Let’s take the following file, called sample.csv, which contains three columns:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下文件，称为 sample.csv，它包含三个列：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Complete the following code to ingest the file successfully.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下代码以成功摄入文件。
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 4.3.3 Exploring the shape of our data universe
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 探索我们的数据宇宙的形状
- en: When working with tabular data, especially if it comes from a SQL data warehouse,
    you’ll often find that the data set is split between tables. In our case, our
    logs table contains a majority of fields suffixed by `ID`; those IDs are listed
    in other tables, and we have to link them to get the legend of those IDs. This
    section briefly introduces star schemas, why they are so frequently encountered,
    and how we can visually represent them to work with them.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理表格数据时，尤其是如果数据来自 SQL 数据仓库，你经常会发现数据集被分割在多个表中。在我们的例子中，我们的日志表包含大量以 `ID` 结尾的字段；这些
    ID 列在其它表中，我们必须将它们链接起来以获取这些 ID 的说明。本节简要介绍了星型模式，为什么它们如此频繁地出现，以及我们如何可视地表示它们以便于工作。
- en: 'Our data universe (the set of tables we are working with) follows a very common
    pattern in relational databases: a center table containing a bunch of IDs (or
    *keys*) and some ancillary tables containing a legend between each key and its
    value. This is called a *star schema* since it looks like a star. Star schemas
    are common in the relational database world because of *normalization*, a process
    used to avoid duplicating pieces of data and improve data integrity. Data normalization
    is illustrated in figure 4.4, where our center table `logs` contain IDs that map
    to the auxiliary tables called *link tables*. In the case of the `CD_Category`
    link table, it contains many fields (e.g., `Category_CD` and `English_description`)
    that are made available to `logs` when you link the tables with the `Category_ID`
    key.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据宇宙（我们正在处理的一组表）遵循关系数据库中一个非常常见的模式：一个中心表包含许多 ID（或 *键*）和一些辅助表，这些辅助表在每个键和其值之间包含说明。这被称为
    *星型模式*，因为它看起来像一颗星星。由于 *规范化*，星型模式在关系数据库世界中很常见，这是一种避免数据重复并提高数据完整性的过程。数据规范化在图 4.4
    中展示，我们的中心表 `logs` 包含映射到辅助表（称为 *链接表*）的 ID。在 `CD_Category` 链接表中，它包含许多字段（例如，`Category_CD`
    和 `English_description`），当使用 `Category_ID` 键链接表时，这些字段对 `logs` 表是可用的。
- en: '![](../Images/04-04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-04.png)'
- en: Figure 4.4 The `logs` table `ID` columns map to other tables, like the `CD_category`
    table, which links the `Category_ID` field.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 `logs` 表的 `ID` 列映射到其他表，如 `CD_category` 表，它链接了 `Category_ID` 字段。
- en: In Spark’s universe, we often prefer working with a single table instead of
    linking a multitude of tables to get the data. We call these *denormalized* tables,
    or, colloquially, *fat* tables. We start by assessing the data directly available
    in the `logs` table before plumping our table, a topic I cover in chapter 5\.
    By looking at the `logs` table, its content, and the data documentation, we avoid
    linking tables that contain data with no real value for our analysis.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的宇宙中，我们通常更喜欢使用单个表而不是链接多个表来获取数据。我们称这些为*非规范化*表，或者通俗地说，*胖表*。我们首先评估`logs`表中直接可用的数据，然后再扩充我们的表，这个话题我在第5章中进行了介绍。通过查看`logs`表、其内容和数据文档，我们避免了链接包含对我们分析没有实际价值的数据的表。
- en: The right structure for the right work
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 适合工作的正确结构
- en: Normalization, denormalization—what gives? Isn’t this a book about data analysis?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化、非规范化——这到底是什么？这不是一本关于数据分析的书吗？
- en: While this book isn’t about data modelling, it’s important to understand, at
    least a little, how data might be structured so that we can work with it. Normalized
    data has many advantages when you’re working with relational information (e.g.,
    our broadcast tables). In addition to being easier to maintain, data normalization
    reduces the probability of getting anomalies or illogical records in the data.
    On the flip side, large-scale data systems sometimes embrace denormalized tables
    to avoid costly join operations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书不是关于数据建模的，但至少需要了解一些数据可能的结构方式，这样我们才能与之工作。当与关系信息（例如，我们的广播表）一起工作时，规范化数据有许多优点。除了更容易维护外，数据规范化还能降低数据中出现异常或不合逻辑记录的概率。另一方面，大规模数据系统有时会采用非规范化表来避免昂贵的连接操作。
- en: When dealing with analytics, a single table containing all the data is best.
    However, having to link/join the data by hand can be tedious, especially when
    working with dozens or even hundreds of link tables (check out chapter 5 for more
    information about joins). Fortunately, data warehouses don’t change their structure
    very often. If you’re faced with a complex star schema one day, befriend one of
    the database managers. There is a very good chance they’ll provide you with the
    information to denormalize the tables, most often in SQL, and chapter 7 will show
    how you can adapt the code into PySpark with minimum effort.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分析时，包含所有数据的单个表是最好的。然而，手动连接/合并数据可能会很繁琐，尤其是在处理数十个甚至数百个链接表时（有关连接的更多信息，请参阅第5章）。幸运的是，数据仓库的结构变化并不频繁。如果你有一天面临一个复杂的星型模式，那就与数据库管理员交朋友吧。他们非常有可能提供给你将表非规范化的信息，通常是通过SQL，第7章将展示如何以最小的努力将代码适配到PySpark中。
- en: '4.4 The basics of data manipulation: Selecting, dropping, renaming, ordering,
    diagnosing'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 数据操作的基础：选择、删除、重命名、排序、诊断
- en: 'It is common practice to explore and summarize the data when you first get
    acquainted with it. It’s just like a first date with your data: you want a good
    overview, not to have agonize over the details. (In Québécois French, we say *s’enfarger
    dans les fleurs du tapis* to refer to someone who’s too bogged down on the details.
    Translated, it means “to trip over the rug’s flowers.”) This section shows the
    most common manipulations done on a data frame in greater detail. I show how you
    can select, delete, rename, reorder, and create columns so you can customize how
    a data frame is shown. I also cover summarizing a data frame so you can have a
    quick diagnostic overview of the data inside your structure. No flowers required.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次接触数据时，探索和总结数据是一种常见的做法。这就像与你的数据第一次约会：你想要一个良好的概述，而不是纠结于细节。（在魁北克法语中，我们说 *s’enfarger
    dans les fleurs du tapis* 来指代那些过于纠结于细节的人。翻译过来就是“绊倒在地毯的花上。”）本节将更详细地展示在数据框上最常见的操作。我展示了如何选择、删除、重命名、重新排序和创建列，以便你可以自定义数据框的显示方式。我还涵盖了数据框的汇总，这样你可以快速诊断结构内的数据。无需任何花朵。
- en: '4.4.1 Knowing what we want: Selecting columns'
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 知道我们想要什么：选择列
- en: So far, we’ve learned that typing our data frame variable into the shell prints
    the structure of the data frame, not the data, unless you’re using eagerly evaluated
    Spark (referenced in chapter 2). We can also use the `show()` command to display
    a few records for exploration. I won’t show the results, but if you try it, you’ll
    see that the table-esque output is garbled because we are showing too many columns
    at once. This section reintroduces the `select()` method, which, this time, instructs
    PySpark on the columns you want to keep in your data frame. I also introduce how
    you can refer to columns when using PySpark methods and functions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到，将我们的数据框变量输入到shell中会打印出数据框的结构，而不是数据，除非你使用的是急切评估的Spark（在第2章中提到）。我们还可以使用`show()`命令来显示一些记录以供探索。我不会显示结果，但如果你尝试，你会看到表格式的输出是混乱的，因为我们一次显示了太多的列。本节重新介绍了`select()`方法，这次它指导PySpark选择你想要保留在数据框中的列。我还介绍了在使用PySpark方法和函数时如何引用列。
- en: At its simplest, `select()` can take one or more column objects—or strings representing
    column names—and return a data frame containing only the listed columns. This
    way, we can keep our exploration tidy and check a few columns at a time. An example
    is displayed in the next listing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，`select()`可以接受一个或多个列对象——或者表示列名的字符串——并返回一个只包含所列列的数据框。这样，我们可以保持我们的探索整洁，一次检查几个列。一个例子将在下一个列表中显示。
- en: Listing 4.6 Selecting five rows of the first three columns of our data frame
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6：选择数据框前三个列的五行
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In chapter 2, you learned that `.show(5,` `False)` shows five rows without truncating
    their representation so that we can show the whole content. The `.select()` statement
    is where the magic happens. In the documentation, `select()` takes a single parameter,
    `*cols`; the `*` means that the method will accept an arbitrary number of parameters.
    If we pass multiple column names, `select()` will simply clump all the parameters
    in a tuple called `cols`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，你了解到`.show(5, False)`显示五行而不截断它们的表示，这样我们就可以显示全部内容。`.select()`语句是魔法发生的地方。在文档中，`select()`接受一个参数，`*cols`；星号表示该方法将接受任意数量的参数。如果我们传递多个列名，`select()`将简单地将这些参数聚集成一个名为`cols`的元组。
- en: Because of this, we can use the same de-structuring trick for selecting columns.
    From a PySpark perspective, the four statements in listing 4.7 are interpreted
    the same. Note how prefixing the list with a star removed the container so that
    each element becomes a parameter of the function. If this looks a little confusing
    to you, fear not! Appendix C provides you with a good overview of collection unpacking.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，我们可以使用相同的解构技巧来选择列。从PySpark的角度来看，列表4.7中的四个语句被解释为相同。注意，在列表前加上星号移除了容器，使得每个元素都成为函数的参数。如果你觉得这有点令人困惑，不要担心！附录C为你提供了一个关于集合解包的良好概述。
- en: Listing 4.7 Four ways to select columns in PySpark, all equivalent in terms
    of results
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.7：在PySpark中选择列的四种方式，在结果方面都等效
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When explicitly selecting a few columns, you don’t have to wrap them into a
    list. If you’re already working on a list of columns, you can unpack them with
    a `*` prefix. This argument unpacking pattern is worth remembering as many other
    data frame methods taking columns as input use the same approach.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当明确选择几个列时，你不需要将它们包装成一个列表。如果你已经在处理一个列的列表，你可以使用`*`前缀来解包它们。这种参数解包模式值得记住，因为许多其他以列作为输入的数据框方法都使用相同的方法。
- en: In the spirit of being clever (or lazy), let’s expand our selection code to
    see every column in groups of three. This will give us a sense of the content.
    A data frame keeps track of its columns in the `columns` attributes; `logs.columns`
    is a Python list containing all the column names of the `logs` data frame. In
    the next listing, I slice the columns into groups of three to display them by
    small groups rather than in one fell swoop.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在追求聪明（或懒惰）的精神下，让我们扩展我们的选择代码，以便以每组三列的方式查看每一列。这将给我们一种内容的感觉。数据框通过`columns`属性跟踪其列；`logs.columns`是一个包含`logs`数据框所有列名的Python列表。在下一个列表中，我将列切片成三列一组，以便以小批量而不是一次性显示它们。
- en: Listing 4.8 Peeking at the data frame in chunks of three columns
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.8：以三列一组查看数据框
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The array_split() function comes from the numpy package, imported as np at
    the beginning of this listing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶`array_split()`函数来自numpy包，在本列表的开头导入为np。
- en: Let’s take each line one at a time. We start by splitting the `logs.columns`
    list into approximate groups of three. To do so, we rely on a function from the
    `numpy` package called `array_split()`. The function takes an array and a number
    of desired sub-arrays, `N`, and returns a list of `N` sub-arrays. We wrap our
    list of columns, `logs.columns`, into an array via the `np.array` function and
    pass this as a first parameter. For the number of sub-arrays, we divide the number
    of columns by three, using an integer division, `//`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析。我们首先将 `logs.columns` 列表分割成大约三组的近似组。为此，我们依赖于来自 `numpy` 包的一个函数 `array_split()`。该函数接受一个数组和所需子数组的数量
    `N`，并返回一个包含 `N` 个子数组的列表。我们通过 `np.array` 函数将列的列表 `logs.columns` 包装成一个数组，并将其作为第一个参数传递。对于子数组的数量，我们用整数除法
    `//` 将列数除以三。
- en: Tip To be perfectly honest, the call to `np.array` can be eschewed since `np.array_split()`
    can work on lists, albeit more slowly. I am still using it because if you are
    using a static type checker, such as mypy, you’ll get a type error. Chapter 8
    has a basic introduction to type checking in your PySpark program.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 完全诚实地说，调用 `np.array` 可以避免，因为 `np.array_split()` 可以在列表上工作，尽管速度较慢。我仍然在使用它，因为如果你使用静态类型检查器，如
    mypy，你会得到一个类型错误。第 8 章介绍了 PySpark 程序中的类型检查基础知识。
- en: The last part of listing 4.8 iterates over the list of sub-arrays, using `select()`;
    select the columns present inside each sub-array and use `show()` to display them
    on the screen.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.8 的最后一部分遍历子数组的列表，使用 `select()`；选择每个子数组中存在的列，并使用 `show()` 在屏幕上显示它们。
- en: Tip If you use Databricks notebooks, you can use `display(logs)` to show the
    `logs` data frame in an attractive table format.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 如果你使用 Databricks 笔记本，你可以使用 `display(logs)` 以吸引人的表格格式显示 `logs` 数据框。
- en: This example shows how easy it is to blend Python code with PySpark. In addition
    to providing a trove of functions, the data frame API also exposes information,
    such as column names, into convenient Python structures. I won’t avoid using functionality
    from libraries when it makes sense, but, like in listing 4.8, I’ll do my best
    to explain what it does and why we’re using it. Chapter 8 and 9 go deeper into
    how you can combine pure Python code in PySpark.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了将 Python 代码与 PySpark 混合是多么容易。除了提供大量函数外，DataFrame API 还将诸如列名等信息暴露在方便的
    Python 结构中。当这样做有意义时，我不会避免使用库中的功能，但就像在列表 4.8 中一样，我会尽力解释它是做什么的以及为什么我们要使用它。第 8 章和第
    9 章将更深入地探讨如何在 PySpark 中结合纯 Python 代码。
- en: '4.4.2 Keeping what we need: Deleting columns'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 保留所需内容：删除列
- en: 'The other side of selecting columns is choosing what not to select. We could
    do the full trip with `select()`, carefully crafting our list of columns to keep
    just the one we want. Fortunately, PySpark also provides a shorter trip: simply
    drop what you don’t want.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 选择列的另一面是选择不选择什么。我们可以通过 `select()` 完成整个旅程，仔细构建我们想要保留的列的列表。幸运的是，PySpark 也提供了一个更短的旅程：简单地删除你不需要的。
- en: 'Let’s get rid of two columns in our current data frame in the spirit of tidying
    up. Hopefully, it will bring us joy:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们本着整理的精神，从当前数据框中删除两列。希望这能给我们带来快乐：
- en: '`BroadCastLogID` is the primary key of the table and will not serve us in answering
    our questions.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BroadCastLogID` 是表的主键，在回答我们的问题时不会对我们有帮助。'
- en: '`SequenceNo` is a sequence number and won’t be useful either.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SequenceNo` 是一个序列号，也不会有用。'
- en: More will come off later when we start looking at the link tables. The code
    in the next listing does this simply.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始查看链接表时，还会有更多内容被删除。下一列表中的代码就是这样做的。
- en: Listing 4.9 Getting rid of columns using the `drop()` method
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.9 使用 `drop()` 方法删除列
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Just like `select()`, `drop()` takes a `*cols` and returns a data frame, this
    time excluding the columns passed as parameters. Just like every other method
    in PySpark, `drop()` returns a new data frame, so we overwrite our `logs` variable
    by assigning the result of our code.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `select()` 一样，`drop()` 也是一个 `*cols` 并返回一个数据框，这次排除了作为参数传递的列。就像 PySpark 中的每个其他方法一样，`drop()`
    返回一个新的数据框，因此我们通过分配代码的结果来覆盖我们的 `logs` 变量。
- en: Warning Unlike `select()`, where selecting a column that doesn’t exist will
    return a runtime error, dropping a nonexistent column is a no-op. PySpark will
    simply ignore the columns it doesn’t find. Be careful with the spelling of your
    column names!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 与 `select()` 不同，选择一个不存在的列将返回一个运行时错误，删除一个不存在的列是一个无操作。PySpark 将简单地忽略它找不到的列。注意你的列名拼写！
- en: 'Depending on how many columns you want to preserve, `select()` might be a neater
    way to keep only what you want. We can view `drop()` and `select()` as being two
    sides of the same coin: one drops what you specify; the other keeps what you specify.
    We could reproduce listing 4.9 with a `select()` method, and the next listing
    does just that.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你想要保留多少列，`select()` 可能是一个更整洁的方式来只保留你想要的内容。我们可以将 `drop()` 和 `select()` 视为同一枚硬币的两面：一个删除你指定的内容；另一个保留你指定的内容。我们可以用
    `select()` 方法重现列表 4.9，下一个列表就是这样做的。
- en: Listing 4.10 Getting rid of columns, select style
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.10 删除列，选择样式
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Advanced topic: An unfortunate inconsistency'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 高级主题：不幸的不一致性
- en: 'In theory, you can also `select()` columns with a list without unpacking them.
    This code will work as expected:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，你也可以不解包列表来 `select()` 列。这段代码将按预期工作：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is not the case for `drop()`, where you need to explicitly unpack:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `drop()` 而言并非如此，你需要显式解包：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I’d rather unpack explicitly and avoid the cognitive load of remembering when
    it’s mandatory and when it’s optional.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我宁愿显式解包，以避免记住何时是强制性的，何时是可选性的认知负担。
- en: You now know the most fundamental operations to perform on a data frame. You
    can select and drop columns, and with the flexibility of `select()` presented
    in chapters 2 and 3, you can apply functions on existing columns to transform
    them. The next section will cover how you can create new columns without having
    to rely on `select()`, simplifying your code and improving its resiliency.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经知道了在数据框上执行的最基本操作。你可以选择和删除列，并且利用第 2 章和第 3 章中介绍的 `select()` 的灵活性，你可以对现有列应用函数以转换它们。下一节将介绍如何在不依赖
    `select()` 的情况下创建新列，从而简化你的代码并提高其弹性。
- en: Note In chapter 6, we learn about restricting the data read directly from the
    schema definition. This is an attractive way to avoid dropping columns in the
    first place.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在第 6 章中，我们将学习如何限制直接从模式定义中读取的数据。这是一种避免最初就删除列的有吸引力的方法。
- en: Exercise 4.2
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4.2
- en: What is the printed result of this code?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的打印结果是什么？
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: a) `['item'` `'UPC']`
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: a) `['item'` `'UPC']`
- en: b) `['item',` `'upc']`
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: b) `['item',` `'upc']`
- en: c) `['price',` `'quantity']`
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: c) `['price',` `'quantity']`
- en: d) `['price',` `'quantity',` `'UPC']`
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: d) `['price',` `'quantity',` `'UPC']`
- en: e) Raises an error
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: e) 抛出错误
- en: '4.4.3 Creating what’s not there: New columns with withColumn()'
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 创建不存在的内容：使用 withColumn() 创建新列
- en: Creating new columns is such a basic operation that it seems a little far-fetched
    to rely on `select()`. It also puts a lot of pressure on code readability; for
    instance, using `drop()` makes it obvious we’re removing columns. It would be
    nice to have something that signals we’re creating a new column. PySpark named
    this function `withColumn()`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新列是一项如此基本的操作，以至于依赖 `select()` 似乎有些牵强。这也给代码的可读性带来了很大压力；例如，使用 `drop()` 可以清楚地表明我们正在删除列。如果能有一种信号表明我们正在创建新列那就太好了。PySpark
    将这个函数命名为 `withColumn()`。
- en: Before going crazy with column creation, let’s take a simple example, build
    what we need iteratively, and then move the data to `withColumn()`. Let’s take
    the `Duration` column, which contains the length of each program shown.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在疯狂创建列之前，让我们用一个简单的例子开始，逐步构建我们需要的内容，然后将数据移动到 `withColumn()`。让我们以 `Duration` 列为例，该列包含每个展示节目的长度。
- en: Listing 4.11 Selecting and displaying the `Duration` column
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.11 选择并显示 `Duration` 列
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ The dtypes attribute of a data frame contains the name of the column and its
    type, wrapped in a tuple.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据框的 `dtypes` 属性包含列名及其类型，封装在一个元组中。
- en: PySpark doesn’t have a default type for time without dates or duration, so it
    kept the column as a string. We verified the exact type via the `dtypes` attribute,
    which returns both the name and type of a data frame’s columns. A string is a
    safe and reasonable option, but this isn’t remarkably useful for our purpose.
    Thanks to our peeking, we can see that the string is formatted like `HH:MM:SS.mmmmmm`,
    where
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 没有日期或持续时间的时间类型的默认类型，因此它将列保留为字符串类型。我们通过 `dtypes` 属性验证了确切的类型，该属性返回数据框列的名称和类型。字符串是一个安全且合理的选项，但这对我们的目的来说并不特别有用。多亏了我们的窥视，我们可以看到字符串的格式类似于
    `HH:MM:SS.mmmmmm`，其中
- en: '`HH` is the duration in hours.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HH` 是以小时为单位的时间长度。'
- en: '`MM` is the duration in minutes.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MM` 是以分钟为单位的时间长度。'
- en: '`SS` is the duration in seconds.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SS` 是以秒为单位的时间长度。'
- en: '`mmmmmmm` is the duration in microseconds.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mmmmmmm` 是以微秒为单位的时间长度。'
- en: Note To match an arbitrary date/timestamp pattern, refer to the Spark documentation
    for date-time patterns at [http://mng.bz/M2X2](http://mng.bz/M2X2).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了匹配任意的日期/时间戳模式，请参考 Spark 文档中关于日期时间模式的说明，[http://mng.bz/M2X2](http://mng.bz/M2X2)。
- en: I ignore the duration in microseconds since I don’t think it’ll make much of
    a difference. The `pyspark.sql.functions` module (which we aliased as `F`) contains
    the `substr()` function that extracts a substring from a string column. In listing
    4.12, I use it to extract the hours, minutes, and seconds from the `Duration`
    columns. The `substr()` method takes two parameters. The first gives the position
    of where the sub-string starts, the first character being `1`, not `0` like in
    Python. The second gives the length of the sub-string we want to extract in a
    number of characters. The function application returns a string `Column` that
    I convert to an `Integer` via the `cast()` method. Finally, I provide an alias
    for each column so that we can easily tell which is which.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我忽略了微秒的持续时间，因为我认为这不会有多大影响。`pyspark.sql.functions` 模块（我们将其别名为 `F`）包含 `substr()`
    函数，该函数可以从字符串列中提取子字符串。在列表 4.12 中，我使用它从 `Duration` 列中提取小时、分钟和秒。`substr()` 方法接受两个参数。第一个参数给出子字符串开始的位位置，第一个字符是
    `1`，而不是 Python 中的 `0`。第二个参数给出我们想要提取的子字符串的长度，以字符数表示。函数应用返回一个字符串 `Column`，我通过 `cast()`
    方法将其转换为 `Integer`。最后，我为每个列提供一个别名，这样我们就可以轻松地分辨出它们。
- en: Listing 4.12 Extracting the hours, minutes, and seconds from the `Duration`
    column
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.12 从 `Duration` 列提取小时、分钟和秒
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The original column, for sanity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了保持清晰，原始列。
- en: ❷ The first two characters are the hours.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 前两个字符是小时。
- en: ❸ The fourth and fifth characters are the minutes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第四个和第五个字符是分钟。
- en: ❹ The seventh and eighth characters are the seconds.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第七个和第八个字符是秒。
- en: ❺ To avoid seeing identical rows, I’ve added a distinct() to the results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为了避免看到重复的行，我在结果中添加了 distinct()。
- en: I use the `distinct()` method before `show()`, which de-dupes the data frame.
    This is explained further in chapter 5\. I added `distinct()` to avoid seeing
    identical occurrences that would provide no additional information when displayed.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `show()` 之前，我使用了 `distinct()` 方法，这会去重数据框。这将在第 5 章中进一步解释。我添加 `distinct()` 以避免在显示时看到重复的行，这些行不会提供任何额外的信息。
- en: Note We could rely on the `datetime` and `timedelta` Python constructs through
    a UDF (see chapters 8 and 9). Depending on the type of UDF (simple versus vectorized),
    the performance can be slower or comparable to using this approach. While UDFs
    have their dedicated chapters, I try to use as much functionality from the PySpark
    API as possible, leveraging UDF when I need functionality beyond what’s available.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们可以通过 UDF（见第 8 章和第 9 章）依赖 `datetime` 和 `timedelta` Python 构造。根据 UDF 的类型（简单与矢量化），性能可能会较慢或与使用此方法相当。虽然
    UDF 有其专门的章节，但我尽量使用 PySpark API 的尽可能多的功能，在需要超出可用功能的功能时使用 UDF。
- en: 'I think that we’re in good shape! Let’s merge all these values into a single
    field: the duration of the program in seconds. PySpark can perform arithmetic
    with column objects using the same operators as Python, so this will be a breeze!
    In the next listing, we apply addition and multiplication on integer columns,
    just like if they were simple number values.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们做得很好！让我们将这些值合并到一个字段中：程序的秒数长度。PySpark 可以使用与 Python 相同的运算符对列对象进行算术运算，所以这将非常简单！在下一个列表中，我们像处理简单的数值一样对整数列进行加法和乘法运算。
- en: Listing 4.13 Creating a duration in second field from the `Duration` column
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.13 从 `Duration` 列创建秒字段的时间长度
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We kept the same definitions, removed the alias, and performed arithmetic directly
    on the columns. There are 60 seconds in a minute, and 60 * 60 seconds in an hour.
    PySpark respects operator precedence, so we don’t have to clobber our equation
    with parentheses. Overall, our code is quite easy to follow, and we are ready
    to add our column to our data frame.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保留了相同的定义，移除了别名，并直接在列上执行算术运算。一分钟有 60 秒，一小时有 60 * 60 秒。PySpark 遵守运算符优先级，所以我们不需要用括号来混淆我们的方程。总的来说，我们的代码相当容易理解，我们准备将我们的列添加到数据框中。
- en: What if we want to add a column at the end of our data frame? Instead of using
    `select()` on all the columns *plus* our new one, let’s use `withColumn()`. Applied
    to a data frame, it’ll return a data frame with the new column appended. The next
    listing takes our field and adds it to our `logs` data frame. I also include a
    sample of the `printSchema()` method so that you can see the column added at the
    end.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在数据框的末尾添加一个列怎么办？而不是在所有列 *加上* 我们的新列上使用 `select()`，让我们使用 `withColumn()`。应用于数据框，它将返回一个带有新列附加的数据框。下一个列表将我们的字段添加到我们的
    `logs` 数据框中。我还包括 `printSchema()` 方法的示例，以便您可以看到末尾添加的列。
- en: Listing 4.14 Creating a new column with `withColumn()`
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.14 使用 `withColumn()` 创建新列
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Our Duration_seconds columns have been added at the end of our data frame.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的数据框末尾添加了 `Duration_seconds` 列。
- en: Warning If you create a column `withColumn()` and give it a name that already
    exists in your data frame, PySpark will happily overwrite the column. This is
    often very useful for keeping the number of columns manageable, but make sure
    you are seeking this effect!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 如果你使用 `withColumn()` 创建一个列，并给它一个在数据框中已经存在的名字，PySpark 会愉快地覆盖这个列。这通常非常有用，可以保持列的数量可控，但请确保你确实想要达到这种效果！
- en: We can create columns using the same expression with `select()` and with `withColumn()`.
    Both approaches have their use. `select()` will be useful when you’re explicitly
    working with a few columns. When you need to create a few new ones without changing
    the rest of the data frame, I prefer `withColumn()`. You’ll quickly gain intuition
    about which is easiest when faced with the choice.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的表达式使用 `select()` 和 `withColumn()` 来创建列。两种方法都有其用途。当你明确地处理几个列时，`select()`
    将非常有用。当你需要创建几个新列而不改变数据框的其余部分时，我更喜欢 `withColumn()`。面对选择时，你会很快对哪个更容易有直观的认识。
- en: Warning Creating many (100+) new columns using `withColumns()` will slow Spark
    down to a grind. If you need to create a lot of columns at once, use the `select()`
    approach. While it will generate the same work, it is less tasking on the query
    planner.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 使用 `withColumns()` 创建许多（100+）新列将使 Spark 变得非常缓慢。如果你需要一次性创建很多列，请使用 `select()`
    方法。虽然它会产生相同的工作量，但它对查询计划器的负担较小。
- en: '![](../Images/04-05.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-05.png)'
- en: Figure 4.5 `select()` versus `withColumn()`, visually. `withColumn()` keeps
    all the preexisting columns without the need the specify them explicitly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 `select()` 与 `withColumn()` 的视觉对比。`withColumn()` 在不需要显式指定的情况下保留了所有现有列。
- en: '4.4.4 Tidying our data frame: Renaming and reordering columns'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 整理我们的数据框：重命名和重新排序列
- en: This section covers how to make the order and names of the columns friendlier.
    It might seem a little vapid, but after a few hours of hammering code on a particularly
    tough piece of data, you’ll be happy to have this in your toolbox.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何使列的顺序和名称更加友好。这可能会显得有些空洞，但当你花费几个小时在特别棘手的数据上敲代码时，你会很高兴拥有这个工具箱。
- en: Renaming columns can be done with `select()` and `alias()`, of course. We saw
    briefly in chapter 3 that PySpark provides you an easier way to do so. Enter `withColumnRenamed()`!
    In the following listing, I use `withColumnRenamed()` to remove the capital letters
    of my newly created `duration_seconds` column.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `select()` 和 `alias()` 来重命名列，当然。我们在第 3 章中简要介绍了 PySpark 提供的更简单的方法。进入 `withColumnRenamed()`！在以下列表中，我使用
    `withColumnRenamed()` 来移除我新创建的 `duration_seconds` 列中的大写字母。
- en: Listing 4.15 Renaming one column at a type, the `withColumnRenamed()` way
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.15 通过 `withColumnRenamed()` 方式重命名一个列
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: I’m a huge fan of having column names without capital letters. I’m a lazy typist,
    and pressing Shift all the time adds up! I could potentially use `withColumnRenamed()`
    with a `for` loop over all the columns to rename them in my data frame. The PySpark
    developers thought about this and offered a better way to rename all the columns
    of your data frame in one fell swoop. This relies on a method, `toDF()`, that
    returns a new data frame with the new columns. Just like `drop()`, `toDF()` takes
    a `*cols`, and just like `select()` and `drop()`, we need to unpack our column
    names if they’re in a list. The code in the next listing shows how you can rename
    all your columns to lowercase in a single line using that method.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常喜欢使用没有大写字母的列名。我是一个懒惰的打字员，一直按着 Shift 键会增加工作量！我可以用 `withColumnRenamed()` 结合一个遍历所有列的
    `for` 循环来重命名我的数据框中的列。PySpark 开发者考虑到了这一点，并提供了一种更好的方法，可以在一次操作中重命名数据框中的所有列。这依赖于一个名为
    `toDF()` 的方法，它返回一个新的数据框，其中包含新的列。就像 `drop()` 一样，`toDF()` 也接受一个 `*cols` 参数，就像 `select()`
    和 `drop()` 一样，如果列名在列表中，我们需要解包它们。下一列表中的代码展示了如何使用该方法在一行中重命名所有列到小写。
- en: Listing 4.16 Batch lowercasing using the `toDF()` method
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.16 使用 `toDF()` 方法批量转换为小写
- en: '[PRE20]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you carefully look at the code, you can see that I’m not assigning the resulting
    data frame. I wanted to showcase the functionality, but since we have ancillary
    tables with column names that match, I wanted to avoid the trouble of lowercasing
    every column in every table.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看代码，你会看到我没有分配结果数据框。我想展示这个功能，但由于我们有一些辅助表，它们的列名与我们的匹配，我想避免在每个表中将每个列都转换为小写的麻烦。
- en: Our final step is *reordering* columns. Since reordering columns is equivalent
    to selecting columns in a different order, `select()` is the perfect method for
    the job. For instance, if we wanted to sort the columns alphabetically, we could
    use the `sorted` function on the list of our data frame columns, just like in
    the next listing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是**重新排序**列。由于重新排序列等同于以不同的顺序选择列，因此 `select()` 是完成这项工作的完美方法。例如，如果我们想按字母顺序排序列，我们可以使用列表中的数据框列上的
    `sorted` 函数，就像在下一个列表中一样。
- en: Listing 4.17 Selecting our columns in alphabetical order using `select()`
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.17 使用 `select()` 按字母顺序选择我们的列
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Remember that, in most programming languages, capital letters come before
    lowercase ones.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记住，在大多数编程语言中，大写字母在小写字母之前。
- en: 'In this section, we covered a lot of ground: through selecting, dropping, creating,
    renaming, and reordering columns, we gained intuition about how PySpark manages
    and provides visibility over the structure of the data frame. In the next section,
    I cover a way to quickly explore the data in the data frame.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们覆盖了很多内容：通过选择、删除、创建、重命名和重新排序列，我们获得了关于 PySpark 如何管理和提供数据框结构可见性的直觉。在下一节中，我将介绍一种快速探索数据框数据的方法。
- en: 4.4.5 Diagnosing a data frame with describe() and summary()
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.5 使用 `describe()` 和 `summary()` 诊断数据框
- en: When working with numerical data, looking at a long column of values isn’t very
    useful. We’re often more concerned about some key information, which may include
    count, mean, standard deviation, minimum, and maximum. In this section, I cover
    how we can quickly explore numerical columns using PySpark’s `describe()` and
    `summary()` methods.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数值数据时，查看一长串值并不很有用。我们通常更关心一些关键信息，这可能包括计数、平均值、标准差、最小值和最大值。在本节中，我将介绍如何使用 PySpark
    的 `describe()` 和 `summary()` 方法快速探索数值列。
- en: When applied to a data frame with no parameters, `describe()` will show summary
    statistics (count, mean, standard deviation, min, and max) on all numerical and
    string columns. To avoid screen overflow, I display the column descriptions one
    by one by iterating over the list of columns and showing the output of `describe()`
    in the next listing. Note that `describe()` will (lazily) compute the data frame
    but won’t display it, just like any transformation, so we have to `show()` the
    result.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于没有参数的数据框时，`describe()` 将显示所有数值和字符串列的摘要统计信息（计数、平均值、标准差、最小值和最大值）。为了避免屏幕溢出，我通过遍历列列表逐个显示列描述，并在下一个列表中显示
    `describe()` 的输出。请注意，`describe()` 将（懒加载地）计算数据框但不会显示它，就像任何转换一样，因此我们必须 `show()`
    结果。
- en: Listing 4.18 Describing everything in one fell swoop
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.18 一次性描述所有内容
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Numerical columns will display the information in a description table, like
    so.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数值列将以描述表的形式显示信息，如下所示。
- en: ❷ If the type of the column isn’t compatible, PySpark displays only the title
    column.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果列的类型不兼容，PySpark 只会显示标题列。
- en: It will take more time than doing everything in one fell swoop, but the output
    will be a lot friendlier. Since we can’t compute the mean or standard deviation
    of a string, you’ll see `null` values for those columns. Furthermore, some columns
    won’t be displayed (you’ll see time tables with only the title column), as `describe()`
    will only work for numerical and string columns. For a short line to type, you
    still get a lot!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这会比一次性完成所有操作花费更多时间，但输出将更加友好。由于我们无法计算字符串的平均值或标准差，因此您将看到这些列的 `null` 值。此外，某些列可能不会显示（您将看到只有标题列的时间表），因为
    `describe()` 只适用于数值和字符串列。对于简短的行来说，您仍然可以得到很多信息！
- en: '`describe()` is a fantastic method, but what if you want more? `summary()`
    to the rescue!'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()` 是一个很棒的方法，但如果您想要更多呢？`summary()` 来拯救！'
- en: Where `describe()` will take `*cols` as a parameter (one or more columns, the
    same way as `select()` or `drop()`), `summary()` will take `*statistics` as a
    parameter. This means that you’ll need to select the columns you want to see before
    passing the `summary()` method. On the other hand, we can customize the statistics
    we want to see. By default, `summary()` shows everything `describe()` shows, adding
    the approximate 25-50% and 75% percentiles. The next listing shows how you can
    replace `describe()` for `summary()` and the result of doing so.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()` 函数将 `*cols` 作为参数（一个或多个列，与 `select()` 或 `drop()` 相同），而 `summary()`
    函数将 `*statistics` 作为参数。这意味着在传递 `summary()` 方法之前，您需要选择您想要查看的列。另一方面，我们可以自定义我们想要查看的统计信息。默认情况下，`summary()`
    会显示 `describe()` 显示的所有内容，并添加约 25-50% 和 75% 的分位数。下面的列表显示了如何用 `summary()` 替换 `describe()`
    以及这样做的结果。'
- en: Listing 4.19 Summarizing everything in one fell swoop
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.19 一次性总结所有内容
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ By default, we have count, mean, stddev, min, 25%, 50%, 75%, max as statistics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认情况下，我们有计数、平均值、标准差、最小值、25%、50%、75%、最大值作为统计量。
- en: ❷ We can also pass our own, following the same nomenclature convention.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们也可以按照相同的命名约定传递自己的参数。
- en: If you want to limit yourself to a subset of those metrics, `summary()` will
    accept a number of string parameters representing the statistic. You can input
    `count`, `mean`, `stddev`, `min`, or `max` directly. For approximate percentiles,
    you need to provide them in `XX%` format, such as `25%`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想限制自己查看这些指标的一部分，`summary()` 将接受代表统计量的多个字符串参数。您可以直接输入 `count`、`mean`、`stddev`、`min`
    或 `max`。对于近似百分位数，您需要以 `XX%` 格式提供它们，例如 `25%`。
- en: Both methods will work only on non-`null` values. For the summary statistics,
    it’s the expected behavior, but the “count” entry will also count only the non-`null`
    values for each column. This is a good way to see which columns are mostly empty!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都只能在非 `null` 值上工作。对于汇总统计，这是预期的行为，但“计数”条目也将只计算每个列的非 `null` 值。这是一种查看哪些列大部分为空的好方法！
- en: Warning `describe()` and `summary()` are two very useful methods, but they are
    not meant to be used for anything other than quickly peeking at data during development.
    The PySpark developers don’t guarantee that the output will look the same from
    version to version, so if you need one of the outputs for your program, use the
    corresponding function in `pyspark.sql .functions`. They’re all there.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 `describe()` 和 `summary()` 是两个非常有用的方法，但它们不是用于开发期间快速查看数据的。PySpark 开发者不保证输出在各个版本之间看起来相同，因此如果您需要程序中的一个输出，请使用
    `pyspark.sql.functions` 中的相应函数。它们都在那里。
- en: This chapter covered the ingestion and discovery of a tabular data set, one
    of the most popular data representation formats. We built on the basics of PySpark
    data manipulation, covered in chapters 2 and 3, and added a new layer by working
    with columns. The next chapter will be the direct continuation of this one, where
    we will explore more advanced aspects of the data frame structure.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了表格数据集的摄取和发现，这是最受欢迎的数据表示格式之一。我们基于第 2 章和第 3 章中介绍的 PySpark 数据操作基础知识，通过处理列添加了一个新层次。下一章将是本章的直接延续，我们将探讨数据框结构的更多高级方面。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PySpark uses the `SparkReader` object to directly read any kind of data in a
    data frame. The specialized `CSVSparkReader` is used to ingest CSV files. Just
    like when reading text, the only mandatory parameter is the source location.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 使用 `SparkReader` 对象直接读取数据框中的任何类型的数据。专门的 `CSVSparkReader` 用于摄取 CSV 文件。就像读取文本一样，唯一的强制参数是源位置。
- en: The CSV format is very versatile, so PySpark provides many optional parameters
    to account for this flexibility. The most important ones are the field delimiter,
    the record delimiter, and the quotation character, all of which have sensible
    defaults.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV 格式非常灵活，因此 PySpark 提供了许多可选参数来处理这种灵活性。其中最重要的参数是字段分隔符、记录分隔符和引号字符，它们都有合理的默认值。
- en: 'PySpark can infer the schema of a CSV file by setting the `inferSchema` optional
    parameter to `True`. PySpark accomplishes this by reading the data twice: once
    for setting the appropriate types for each column and once to ingest the data
    in the inferred format.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将 `inferSchema` 可选参数设置为 `True`，PySpark 可以推断 CSV 文件的模式。PySpark 通过读取数据两次来完成此操作：一次用于为每个列设置适当的类型，另一次用于以推断的格式摄取数据。
- en: Tabular data is represented in a data frame in a series of columns, each having
    a name and a type. Since the data frame is a column-major data structure, the
    concept of rows is less relevant.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据在数据框中以一系列列的形式表示，每个列都有一个名称和类型。由于数据框是一个列主数据结构，行的概念不太相关。
- en: You can use Python code to explore the data efficiently, using the column list
    as any Python list to expose the elements of the data frame of interest.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 Python 代码高效地探索数据，将列列表作为任何 Python 列来暴露感兴趣的数据框的元素。
- en: The most common operations on a data frame are the selection, deletion, and
    creation of columns. In PySpark, the methods used are `select()`, `drop()`, and
    `withColumn()`, respectively.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框上最常用的操作是选择、删除和创建列。在 PySpark 中，分别使用 `select()`、`drop()` 和 `withColumn()` 方法。
- en: '`select` can be used for column reordering by passing a reordered list of columns.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select` 可以通过传递一个重新排序的列列表来用于列重排。'
- en: You can rename columns one by one with the `withColumnRenamed()` method, or
    all at once by using the `toDF()` method.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `withColumnRenamed()` 方法逐个重命名列，或者使用 `toDF()` 方法一次性重命名所有列。
- en: You can display a summary of the columns with the `describe()` or `summary()`
    methods. `describe()` has a fixed set of metrics, while `summary()` will take
    functions as parameters and apply them to all columns.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `describe()` 或 `summary()` 方法来显示列的摘要。`describe()` 方法具有一组固定的度量指标，而 `summary()`
    方法将接受函数作为参数并将它们应用于所有列。
- en: Additional exercises
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 4.3
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.3
- en: Reread the data in a `logs_raw` data frame (the data file is `./data/broadcast_logsBroadcastLogs_2018_Q3_M8.CSV`),
    this time without passing any optional parameters. Print the first five rows of
    data, as well as the schema. What are the differences in terms of data and schema
    between `logs` and `logs_raw`?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 重新读取 `logs_raw` 数据框中的数据（数据文件位于 `./data/broadcast_logsBroadcastLogs_2018_Q3_M8.CSV`），这次不传递任何可选参数。打印数据的前五行以及模式。`logs`
    和 `logs_raw` 在数据和模式方面有哪些不同？
- en: Exercise 4.4
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.4
- en: Create a new data frame, `logs_clean`, that contains only the columns that do
    not end with `ID`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的数据框 `logs_clean`，其中只包含不以 `ID` 结尾的列。
- en: '* * *'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ If we are being very picky, tabular and relational data are not exactly the
    same. In chapter 5, when working with joining multiple data frames together, the
    differences will matter. When working with a single table, we can lump those two
    concepts together.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果我们非常挑剔，表格数据和关系数据并不完全相同。在第 5 章中，当将多个数据框一起使用时，这些差异将很重要。当处理单个表时，我们可以将这两个概念合并在一起。
