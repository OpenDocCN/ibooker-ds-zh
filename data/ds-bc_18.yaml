- en: 14 Dimension reduction of matrix data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 矩阵数据的降维
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Simplifying matrices with geometric rotations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过几何旋转简化矩阵
- en: What is principal component analysis?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是主成分分析？
- en: Advanced matrix operations for reducing matrix size
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于减少矩阵大小的先进矩阵运算
- en: What is singular value decomposition?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是奇异值分解？
- en: Dimension reduction using scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行降维
- en: 'Dimension reduction is a series of techniques for shrinking data while retaining
    its information content. These techniques permeate many of our everyday digital
    activities. Suppose, for instance, that you’ve just returned from a vacation in
    Belize. There are 10 vacation photos on your phone that you wish to message to
    a friend. Unfortunately, these photos are quite large, and your current wireless
    connection is slow. Each photo is 1,200 pixels tall and 1,200 pixels wide. It
    takes up 5.5 MB of memory and requires 15 seconds to transfer. Transferring all
    10 photos will take 2.5 minutes. Fortunately, your messaging app offers you a
    better option: You can shrink each photo from 1,200 × 1,200 pixels to 600 × 480
    pixels. This reduces the dimensions of each photo sixfold. By lowering the resolution,
    you’ll sacrifice a little detail. However, the vacation photos will maintain most
    of their information—the lush jungles, blue seas, and shimmering sands will remain
    clearly visible in the images. Therefore, the trade-off is worth it. Reducing
    the dimensionality by six will increase the transfer speed by six: it will take
    just 25 seconds to share the 10 photos with your friend.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是一系列在保留数据信息内容的同时缩小数据的技术。这些技术渗透到我们许多日常的数字活动中。例如，假设你刚刚从伯利兹度假回来。你的手机上有10张度假照片，你希望发送给朋友。不幸的是，这些照片相当大，而你当前的无线连接速度很慢。每张照片高1,200像素，宽1,200像素。每张照片占用5.5
    MB的内存，需要15秒来传输。传输所有10张照片将需要2.5分钟。幸运的是，你的消息应用为你提供了一个更好的选择：你可以将每张照片从1,200 × 1,200像素缩小到600
    × 480像素。这使每张照片的维度减少了六倍。通过降低分辨率，你将牺牲一些细节。然而，度假照片将保留大部分信息——郁郁葱葱的热带雨林、蔚蓝的大海和闪烁的沙滩将清晰地可见于图像中。因此，这种权衡是值得的。通过减少六倍的维度将使传输速度提高六倍：只需25秒就可以与朋友分享这10张照片。
- en: 'There are, of course, more benefits to dimension reduction than just transfer
    speed. Consider, for instance, mapmaking, which can be treated as a dimension-reduction
    problem. Our Earth is a 3D sphere that can be accurately modeled as a globe. We
    can turn that globe into a map by projecting its 3D shape onto a 2D piece of paper.
    The paper map is easier to carry from place to place—unlike a globe, we can fold
    the map and put it in our pocket. Furthermore, the 2D map offers us additional
    advantages. Suppose we’re asked to locate all the countries that border at least
    10 other neighboring territories. Finding these dense border regions on a map
    is easy: we can glance at the 2D map to hone in on crowded country clusters. If
    we we use a globe, instead, the task becomes more challenging. We need to spin
    the globe across multiple perspectives because we can’t see all the countries
    at once. In some ways, the globe’s curvature acts as a layer of noise that interferes
    with our given task. Removing that curvature simplifies our effort, but at a price:
    the continent of Antarctica is essentially deleted from the map. Of course, there
    are no countries in Antarctica, so from the perspective of our task, that trade-off
    is worth it.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，降维的好处不仅仅是传输速度。例如，地图制作可以被视为一个降维问题。我们的地球是一个三维球体，可以精确地模拟成一个地球仪。我们可以通过将地球的3D形状投影到二维纸张上来将地球仪变成地图。纸张地图更容易从一个地方带到另一个地方——与地球仪不同，我们可以折叠地图并放入口袋。此外，二维地图还提供了额外的优势。假设我们被要求定位至少与10个邻近领土接壤的所有国家。在地图上找到这些密集的边界区域很容易：我们可以通过查看二维地图来聚焦于拥挤的国家集群。如果我们使用地球仪，这项任务将变得更加具有挑战性。我们需要在多个视角上旋转地球仪，因为我们不能一次看到所有国家。在某种程度上，地球仪的曲率充当了一层噪声，干扰了我们的任务。移除这种曲率可以简化我们的努力，但代价是：南极洲大陆几乎从地图上被删除。当然，南极洲没有国家，所以从我们任务的视角来看，这种权衡是值得的。
- en: 'Our map analogy demonstrates the following advantages of dimensionally reduced
    data:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的地图类比展示了降维数据的以下优势：
- en: More compact data is easier to transfer and store.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更紧凑的数据更容易传输和存储。
- en: Algorithmic tasks require less time when our data is smaller.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们的数据更小的时候，算法任务所需的时间更少。
- en: Certain complex tasks, like cluster detection, can be simplified by removing
    unnecessary information.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过移除不必要的信息，某些复杂任务，如聚类检测，可以简化。
- en: The last two bullet points are very relevant to this case study. We want to
    cluster thousands of text documents by topic. The clustering will entail computing
    a matrix of all-by-all document similarities. As discussed in the previous section,
    this computation can be slow. Dimension reduction can speed up the process by
    reducing the number of data matrix columns. As a further bonus, dimensionally
    reduced text data has been shown to yield higher-quality topic clusters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个要点与这个案例研究非常相关。我们希望根据主题将成千上万的文本文档进行聚类。聚类将涉及计算所有文档相似性的矩阵。如前所述，这个计算可能很慢。降维可以通过减少数据矩阵的列数来加快这个过程。作为额外的奖励，降维后的文本数据已被证明可以产生更高质量的主题聚类。
- en: Let’s dive deeper into the relationship between dimension reduction and data
    clustering. We start with a simple task in which we cluster 2D data in one dimension.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨降维与数据聚类之间的关系。我们从一项简单的任务开始，即在一维中聚类2D数据。
- en: 14.1 Clustering 2D data in one dimension
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 在一维中聚类2D数据
- en: 'Dimension reduction has many uses, including more interpretable clustering.
    Consider a scenario in which we manage an online clothing store. When customers
    visit our website, they are asked to provide their height and weight. These measurements
    are added to a customer database. Two database columns are required to store each
    customer’s measurements, and therefore the data is two-dimensional. The 2D measurements
    are used to offer customers appropriately sized clothing based on whatever inventory
    is available. Our inventory comes in three sizes: small, medium, and large. Given
    the measurement data for 180 customers, we would like to do the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 降维有许多用途，包括更可解释的聚类。考虑一个场景，我们管理一个在线服装店。当客户访问我们的网站时，他们被要求提供他们的身高和体重。这些测量值被添加到客户数据库中。需要两个数据库列来存储每位客户的测量值，因此数据是二维的。2D测量值用于根据可用的库存向客户提供适当尺寸的服装。我们的库存有三种尺寸：小号、中号和大号。给定180位客户的测量数据，我们希望做以下事情：
- en: Group our customers into three distinct clusters based on size.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据尺寸将我们的客户分成三个不同的集群。
- en: Build an interpretable model to determine the clothing size category of each
    new customer using our computed clusters.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个可解释的模型，使用我们计算出的聚类来确定每位新客户的服装尺码类别。
- en: Make our clustering simple enough for our nontechnical investors to comprehend.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们的聚类足够简单，以便我们的非技术型投资者能够理解。
- en: The third point in particular limits our decisions. Our clustering can’t rely
    on technical concepts, such as centroids or distances to the mean. Ideally, we’d
    be able to explain our model in a single figure. We can achieve this level of
    simplicity using dimension reduction. However, we first need to simulate the 2D
    measurement data for 180 customers. Let’s start by simulating customer heights.
    Our heights range from 60 inches (5 ft) to 78 inches (6.5 ft). We fabricate these
    heights by calling `np.arange(60, 78, 0.1)`. This returns an array of heights
    between 60 and 78 inches, where each consecutive height increases by 0.1 inches.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是第三点限制了我们的决策。我们的聚类不能依赖于技术概念，例如质心或到平均值的距离。理想情况下，我们能够用一张图来解释我们的模型。我们可以通过降维来实现这种程度的简单性。然而，我们首先需要为180位客户模拟2D测量数据。让我们先从模拟客户身高开始。我们的身高范围从60英寸（5英尺）到78英寸（6.5英尺）。我们通过调用`np.arange(60,
    78, 0.1)`来制造这些身高。这返回一个介于60英寸和78英寸之间的身高数组，其中每个连续的身高增加0.1英寸。
- en: Listing 14.1 Simulating a range of heights
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 模拟一系列身高
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Meanwhile, our simulated weights depend strongly on the height. A taller person
    is likely to weigh more than a shorter person. It has been shown that, on average,
    a person’s weight in pounds equals approximately `4 * height - 130`. Of course,
    each individual person’s weight fluctuates around this average value. We’ll model
    these random fluctuations using a normal distribution with a standard deviation
    of 10 lb. Thus, given a `height` variable, we can model the weight as `4 * height
    - 130 + np.random.normal(scale=10)`. Next, we use this relationship to fabricate
    weights for each of our 180 heights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们的模拟体重强烈依赖于身高。一个更高的人可能比一个更矮的人重。研究表明，平均而言，一个人的体重（磅）大约等于`4 * height - 130`。当然，每个个体的体重都会围绕这个平均值波动。我们将使用标准差为10磅的正态分布来模拟这些随机波动。因此，给定一个`height`变量，我们可以将体重建模为`4
    * height - 130 + np.random.normal(scale=10)`。接下来，我们使用这种关系为我们的180个身高制造体重。
- en: Listing 14.2 Simulating weights using heights
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2 使用身高模拟重量
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The scale parameter sets the standard deviation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 尺度参数设置标准差。
- en: We can treat the heights and weights as two-dimensional coordinates in a `measurements`
    matrix. Let’s store and plot these measured coordinates (figure 14.1).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将身高和体重视为`measurements`矩阵中的二维坐标。让我们存储并绘制这些测量坐标（图14.1）。
- en: Listing 14.3 Plotting 2D measurements
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3 绘制2D测量结果
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/14-01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-01.png)'
- en: Figure 14.1 A plot of heights vs. weights. Their linear relationship is clearly
    visible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 身高与体重的图表。它们之间的线性关系清晰可见。
- en: The linear relationship between height and weight is clearly visible in the
    plot. Also, as expected, the height and weight axes are scaled differently. As
    a reminder, Matplotlib manipulates its 2D axes to make the final plot aesthetically
    pleasing. Normally, this is a good thing. However, we’ll soon be rotating the
    plot to simplify our data. The rotation will shift the axes scaling, making the
    rotated data difficult to compare with the original data plot. Consequently, we
    should equalize our axes to obtain consistent visual output. Let’s equalize the
    axes by calling `plt.axis('equal')` and then regenerate the plot (figure 14.2).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中清晰地显示了身高与体重之间的线性关系。同样，正如预期的那样，身高和体重的坐标轴缩放不同。作为一个提醒，Matplotlib操纵其2D坐标轴以使最终图表看起来美观。通常，这是一件好事。然而，我们很快将旋转图表以简化数据。旋转将改变坐标轴的缩放，使得旋转后的数据难以与原始数据图表进行比较。因此，我们应该使坐标轴等比例，以获得一致的视觉输出。让我们通过调用`plt.axis('equal')`来等比例化坐标轴，然后重新生成图表（图14.2）。
- en: '![](../Images/14-02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-02.png)'
- en: Figure 14.2 A plot of heights vs. weights with both axes scaled equally
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 高度与体重的关系图，两个坐标轴均等比例缩放
- en: Listing 14.4 Plotting 2D measurements using equally scaled axes
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4 使用等比例刻度的坐标轴绘制2D测量结果
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our plot now forms a thin, cigar-like shape. We can cluster the cigar by size
    if we slice it into three equal parts. One way of obtaining the clusters is to
    utilize K-means. Of course, interpreting that output will require an understanding
    of the K-means algorithm. A less technical solution is to just tip the cigar on
    its side. If the cigar-shaped plot was positioned horizontally, we could separate
    it into three parts using two vertical slices, as shown in figure 14.3\. The first
    slice would isolate 60 of the leftmost data points, and the second slice would
    isolate 60 of the rightmost customer points. These operations would segment our
    customers in a manner that’s easy to explain, even to a nontechnical person.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的图表形成了一个细长的雪茄形状。如果我们将其切成三等份，就可以按大小对雪茄进行聚类。获得聚类的一种方法是通过使用K-means。当然，解释这个输出需要理解K-means算法。一个不那么技术性的解决方案是将雪茄倾斜。如果雪茄形状的图表是水平放置的，我们可以通过两个垂直切片将其分成三部分，如图14.3所示。第一刀将隔离最左边的60个数据点，第二刀将隔离最右边的60个客户点。这些操作将以一种易于解释的方式将我们的客户进行分割，即使是非技术人员也能理解。
- en: '![](../Images/14-03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-03.png)'
- en: 'Figure 14.3 Our linear measurements, rotated horizontally so that they lie
    primarily on the x-axis. Two vertical cutoffs are sufficient to divide the data
    into three equal clusters: small, medium, and large. In the plot, the x-axis is
    sufficient for distinguishing between measurements. Thus, we can eliminate the
    y-axis with minimal information loss. Note that this figure was generated using
    listing 14.15.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 我们线性测量的图表，水平旋转以便主要位于x轴上。两个垂直截断足以将数据分成三个等量的集群：小、中、大。在图表中，x轴足以区分测量值。因此，我们可以通过最小信息损失消除y轴。请注意，此图是使用列表14.15生成的。
- en: Note For the purposes of the exercise, we assume that small, medium, and large
    sizes are distributed equally. In the real-world clothing industry, this might
    not be the case.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了练习的目的，我们假设小、中、大尺寸是均匀分布的。在现实世界的服装行业中，情况可能并非如此。
- en: If we rotate our data toward the x-axis, the horizontal x-values should be sufficient
    to distinguish between points. We can thus cluster the data without relying on
    the vertical y-values. Effectively, we’ll be able to delete the y-values with
    minimal information loss. That deletion will reduce our data from two dimensions
    to one (figure 14.4).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据旋转到x轴方向，水平x值应该足以区分点。因此，我们可以不依赖于垂直y值来聚类数据。实际上，我们将能够通过最小信息损失删除y值。这种删除将把我们的数据从二维减少到一维（图14.4）。
- en: '![](../Images/14-04.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-04.png)'
- en: Figure 14.4 Our linear measurements, reduced to one dimension using horizontal
    rotation. The data points have been rotated toward the x-axis, and their y-value
    coordinates have been deleted. Nevertheless, the remaining x-values are sufficient
    to distinguish between points. Thus, our 1D output still allows us to split the
    data into three equal clusters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 使用水平旋转将我们的线性测量值降低到一维。数据点已旋转朝向x轴，并且它们的y值坐标已被删除。尽管如此，剩余的x值足以区分点。因此，我们的1D输出仍然允许我们将数据分成三个相等的簇。
- en: We’ll now attempt to cluster our 2D data by flipping the data on its side. This
    horizontal rotation will allow us to both cluster the data and reduce it to one
    dimension.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将尝试通过翻转数据来对2D数据进行聚类。这种水平旋转将使我们能够聚类数据并将其降低到一维。
- en: 14.1.1 Reducing dimensions using rotation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 使用旋转降低维度
- en: 'To flip our data on its side, we must execute two separate steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的数据翻转到侧面，我们必须执行两个独立的步骤：
- en: Shift all of our data points so that they are centered on the origin of the
    plot, which is located at coordinates (0, 0). This will make it easier to rotate
    the plot toward the x-axis.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有数据点移动到绘图的原点中心，该原点位于坐标(0, 0)。这将使旋转绘图朝向x轴更容易。
- en: Rotate the plotted data until the total distance of the data points to the x-axis
    is minimized.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将绘制的数据旋转，直到数据点到x轴的总距离最小化。
- en: Centering our data at the origin is trivial. The central point of every dataset
    is equal to its mean. Thus, we need to adjust our coordinates so that their x-value
    mean and y-value mean both equal zero. This can be done by subtracting the current
    mean from every coordinate; in other words, subtracting the mean height for `heights`
    and the mean weight from `weights` will produce a dataset that’s centered at (0,
    0).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在原点中心化我们的数据是微不足道的。每个数据集的中心点等于其平均值。因此，我们需要调整我们的坐标，使它们的x值平均值和y值平均值都等于零。这可以通过从每个坐标中减去当前的平均值来完成；换句话说，从`heights`中减去平均身高，从`weights`中减去平均体重，将产生一个以(0,
    0)为中心的数据集。
- en: Let’s shift our height and weight coordinates and store these changes in a `centered_
    data` array. Then we plot the shifted coordinates to verify that they are centered
    on the origin (figure 14.5).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调整身高和体重的坐标，并将这些变化存储在`centered_data`数组中。然后我们绘制调整后的坐标，以验证它们是否以原点为中心（图14.5）。
- en: '![](../Images/14-05.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-05.png)'
- en: Figure 14.5 A plot of heights vs. weights, centered at the origin. The centered
    data can be rotated like a propeller.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 以原点为中心的身高与体重的绘图。中心化的数据可以像螺旋桨一样旋转。
- en: Listing 14.5 Centering the measurements at the origin
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.5 在原点中心化测量值
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Visualizes the x-axis and y-axis to mark the location of the origin
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 可视化x轴和y轴以标记原点的位置
- en: 'Our data is now perfectly centered at the origin. However, the orientation
    of the data is closer to the y-axis than the x-axis. Our goal is to adjust this
    orientation through rotation. We want to spin the plotted points around the origin
    until they overlap with the x-axis. Rotating a 2D plot around its center requires
    the use of a *rotation matrix*: a two-by-two array of the form `np.array([[cos(x),
    -sin(x)], [sin(x), cos(x)]])`, where `x` is the angle of rotation. The matrix
    product of this array and `centered_data` rotates the data by `x` radians. The
    rotation occurs in the counterclockwise direction. We can also rotate the data
    in the clockwise direction by inputting `-x` instead of `x`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在完美地以原点为中心。然而，数据的方向更接近y轴而不是x轴。我们的目标是通过对数据进行旋转来调整这个方向。我们希望将绘制的点绕原点旋转，直到它们与x轴重叠。围绕中心旋转2D绘图需要使用一个*旋转矩阵*：一个形式为`np.array([[cos(x),
    -sin(x)], [sin(x), cos(x)]])`的二维数组，其中`x`是旋转角度。这个数组和`centered_data`的矩阵乘积将数据旋转`x`弧度。旋转是逆时针方向的。我们也可以通过输入`-x`而不是`x`来将数据顺时针旋转。
- en: Let’s utilize the rotation matrix to rotate `centered_data` clockwise by 90
    degrees. Then we plot both the rotated data and the original `centered_data` array
    (figure 14.6).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用旋转矩阵将`centered_data`顺时针旋转90度。然后我们绘制旋转后的数据和原始的`centered_data`数组（图14.6）。
- en: Listing 14.6 Rotating `centered_data` by 90 degrees
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.6 将`centered_data`旋转90度
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Converts the angle from degrees to radians
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将角度从度转换为弧度
- en: '![](../Images/14-06.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-06.png)'
- en: Figure 14.6 A plot of `centered_data` before and after a rotation. The data
    has been rotated 90 degrees about the origin. It is now positioned closer to the
    x-axis.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 旋转前后`centered_data`的绘图。数据已绕原点旋转90度。现在它更靠近x轴。
- en: As expected, our `rotated_data` result is perpendicular to the `centered_data`
    plot. We have successfully rotated the plot by 90 degrees. Furthermore, our rotation
    has shifted the plot closer to the x-axis. We need a way to quantify this shift.
    Let’s generate a penalty score that decreases as the data is rotated toward the
    x-axis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们的`rotated_data`结果与`centered_data`图垂直。我们已经成功将图表旋转了90度。此外，我们的旋转使图表更靠近x轴。我们需要一种方法来量化这种移动。让我们生成一个随着数据向x轴旋转而减少的惩罚分数。
- en: We’ll penalize all vertical y-axis values. Our penalty is based on the concept
    of squared distance, introduced in section 5\. The penalty square equals the average
    squared y-value of `rotated_data`. Since y-values represent the distance to the
    x-axis, our penalty equals the average squared distance to the x-axis. When a
    rotated dataset moves closer to the x-axis, its average squared y-value decreases.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对所有垂直y轴值进行惩罚。我们的惩罚基于5节中引入的平方距离概念。惩罚平方等于`rotated_data`的平均平方y值。由于y值代表到x轴的距离，我们的惩罚等于到x轴的平均平方距离。当旋转后的数据集靠近x轴时，其平均平方y值会降低。
- en: Given any `y_values` array, we can compute the penalty by running `sum([y **
    2 for y in y_values]) / y_values.size`. However, we can also compute the penalty
    by executing `y_values @ y_values / y.size`. The two results are identical, and
    the dot product computation is more efficient. Let’s compare the penalty scores
    for `rotated_data` and `centered_data`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何`y_values`数组，我们可以通过运行`sum([y ** 2 for y in y_values]) / y_values.size`来计算惩罚。然而，我们也可以通过执行`y_values
    @ y_values / y.size`来计算惩罚。这两个结果相同，点积计算更高效。让我们比较`rotated_data`和`centered_data`的惩罚分数。
- en: Listing 14.7 Penalizing vertical y-values
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.7惩罚垂直y值
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Rotating the data has reduced the penalty score by more than 20-fold. This
    reduction carries a statistical interpretation. We can link the penalty to the
    variance if we consider the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转数据将惩罚分数降低了20多倍。这种减少具有统计意义。如果我们考虑以下内容，我们可以将惩罚与方差联系起来：
- en: Our penalty score equals the average squared y-value distance from 0 across
    a `y_values` array.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的惩罚分数等于`y_values`数组中从0到平均平方y值距离的平均值。
- en: '`y_values.mean()` equals 0.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_values.mean()`等于0。'
- en: Thus our penalty square equals the average squared y-value distance from the
    mean.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们的惩罚平方等于从平均值到平均平方y值距离。
- en: The average square distance from the mean equals the variance.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从平均值到平均平方距离等于方差。
- en: Our penalty score equals `y_values.var()`.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的惩罚分数等于`y_values.var()`。
- en: We’ve inferred that the penalty score equals the y-axis variance. Consequently,
    our data rotation has reduced the y-axis variance by more than 20-fold. Let’s
    confirm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推断出惩罚分数等于y轴方差。因此，我们的数据旋转将y轴方差降低了20多倍。让我们确认。
- en: Listing 14.8 Equating penalties with y-axis variance
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.8将惩罚与y轴方差相等
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Rounds to take floating-point errors into account
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶四舍五入以考虑浮点误差
- en: We can score rotations based on variance. Rotating the data toward the x-axis
    reduces the variance along the y-axis. How does this rotation influence the variance
    along the x-axis? Let’s find out.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据方差来评分旋转。将数据旋转到x轴方向会减少y轴上的方差。这种旋转如何影响x轴上的方差呢？让我们来看看。
- en: Listing 14.9 Measuring rotational x-axis variance
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.9测量旋转x轴方差
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The rotation has completely flipped the x-axis variance and the y-axis variance.
    However, the total sum of variance values has remained unchanged. Total variance
    is conserved even after the rotation. Let’s verify this fact.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转完全翻转了x轴方差和y轴方差。然而，方差值的总和保持不变。总方差在旋转后仍然保持不变。让我们验证这个事实。
- en: Listing 14.10 Confirming the conservation of total variance
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.10确认总方差的守恒
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Conservation of variance allows us to infer the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 方差的守恒使我们能够推断出以下内容：
- en: x-axis variance and y-axis variance can be combined into a single percentage
    score, where `x_values.var() / total_variance` is equal to `1 - y_values.var()
    / total_variance`.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x轴方差和y轴方差可以合并成一个单一的比例分数，其中`x_values.var() / total_variance`等于`1 - y_values.var()
    / total_variance`。
- en: Rotating the data toward the x-axis leads to an increase in the x-axis variance
    and an equivalent decrease in the y-axis variance. Decreasing the vertical dispersion
    by `p` percent increases the horizontal dispersion by `p` percent.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据旋转到x轴会导致x轴方差增加，y轴方差相应减少。通过减少`p`百分比的垂直分散度，会增加`p`百分比的水平分散度。
- en: The following code confirms these conclusions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码确认了这些结论。
- en: Listing 14.11 Exploring the percent coverage of axis variance
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.11 探索轴方差覆盖率
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Rotating the data toward the x-axis has increased the x-axis variance by 90
    percentage points. Simultaneously, the rotation has reduced the y-axis variance
    by these same 90 percentage points.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据旋转到x轴方向增加了x轴方差90个百分点。同时，旋转减少了y轴方差相同的90个百分点。
- en: Let’s rotate `centered_data` even further until its distance to the x-axis is
    minimized. Minimizing the distance to the x-axis is equivalent to
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步旋转`centered_data`，直到其与x轴的距离最小化。最小化与x轴的距离相当于
- en: Minimizing the percent of total variance covered by the y-axis. This minimizes
    vertical dispersion.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化x轴覆盖的总方差百分比。这最小化了垂直分散。
- en: Maximizing the percent of total variance covered by the x-axis. This maximizes
    horizontal dispersion.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化x轴覆盖的总方差百分比。这最大化了水平分散。
- en: We rotate `centered_data` toward the x-axis by maximizing horizontal dispersion.
    That dispersion is measured over all angles ranging from 1 degree to 180 degrees.
    We visualize these measurements in a plot (figure 14.7). Additionally, we extract
    the rotation angle that maximizes the percent of x-axis coverage. Our code prints
    out that angle and percentage while also marking the angle in our plot.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最大化水平分散将`centered_data`旋转到x轴方向。这种分散是在从1度到180度的所有角度范围内测量的。我们在图中可视化这些测量结果（图14.7）。此外，我们提取了最大化x轴覆盖百分比的旋转角度。我们的代码打印出该角度和百分比，同时在图中标记该角度。
- en: '![](../Images/14-07.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-07.png)'
- en: Figure 14.7 A plot of the rotation-angle vs. the percent of total variance covered
    by the x-axis. A vertical line marks the angle at which the x-axis variance is
    maximized. A rotation angle of 78.3 degrees shifts over 99% of the total variance
    to the x-axis. Rotating by that angle will allow us to dimensionally reduce our
    data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：旋转角度与x轴覆盖的总方差百分比的对比图。一条垂直线标记了x轴方差最大化的角度。78.3度的旋转角度将超过99%的总方差转移到x轴上。以该角度旋转将使我们能够降低数据的维度。
- en: Listing 14.12 Maximizing horizontal dispersion
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.12 最大化水平分散
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Rotates the data by input degrees. The data variable is preset to centered_data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过输入度数旋转数据。数据变量预设为centered_data。
- en: ❷ Returns an array of angles ranging from 0 to 180, where each consecutive angle
    increases by 0.1 degrees
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回一个从0到180度的角度数组，其中每个连续角度增加0.1度
- en: ❸ Computes the x-axis variance for each rotation across every angle
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算每个旋转角度的x轴方差
- en: ❹ Computes the angle of rotation resulting in the maximum variance
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算导致最大方差的角度
- en: ❺ Plots a vertical line through optimal_angle.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在optimal_angle处绘制一条垂直线。
- en: Rotating `centered_data` by 78.3 degrees will maximize the horizontal dispersion.
    At that rotation angle, 99.08% of the total variance will be distributed across
    the x-axis. Thus, we can expect the rotated data to mostly lie along the 1D axis
    line. Let’s confirm by running the rotation and then plotting the results (figure
    14.8).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将`centered_data`旋转78.3度将最大化水平分散。在该旋转角度下，99.08%的总方差将分布在x轴上。因此，我们可以预期旋转后的数据主要沿着1D轴线。让我们通过运行旋转并绘制结果（图14.8）来确认。
- en: '![](../Images/14-08.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-08.png)'
- en: Figure 14.8 A plot of `centered_data` rotated by 78.3 degrees. This rotation
    maximizes variance along the x-axis and minimizes variance along the y-axis. Less
    than 1% of total variance lies along the y-axis. Thus, we can delete the y-coordinate
    with minimal information loss.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：78.3度旋转的`centered_data`的对比图。这种旋转最大化了x轴上的方差并最小化了y轴上的方差。总方差中不到1%沿着y轴。因此，我们可以删除y坐标，而信息损失最小。
- en: Listing 14.13 Plotting rotated data with high x-axis coverage
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.13 绘制具有高x轴覆盖率的旋转数据
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Most of the data lies close to the x-axis. The data’s dispersion is maximized
    in that horizontal direction. Highly dispersed points are, by definition, highly
    separated. Separated points are easier to distinguish from each other. By contrast,
    the dispersion along our vertical y-axis has been minimized. Vertically, the data
    points are difficult to distinguish. Consequently, we can delete all y-axis coordinates
    with minimal information loss. That deletion should account for less than 1% of
    the total variance, so the remaining x-axis values will be sufficient to cluster
    our measurements.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分数据靠近x轴。数据在该水平方向上的分散度最大。高度分散的点，按定义，是高度分离的。分离的点更容易相互区分。相比之下，垂直y轴上的分散度已经被最小化。垂直方向上，数据点难以区分。因此，我们可以删除所有y轴坐标，信息损失最小。这种删除应占不到总方差的1%，因此剩余的x轴值将足以聚类我们的测量值。
- en: Let’s reduce `best_rotated_data` to 1D by disposing of the y-axis. Then we’ll
    use the remaining 1D array to extract two clustering thresholds. The first threshold
    separates the small-sized customers from the medium-sized customers, and the second
    threshold separates the medium-sized customers from the large-sized customers.
    Together, the two thresholds separate our 180 customers into three equally sized
    clusters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`best_rotated_data`简化为一维，通过丢弃y轴。然后我们将使用剩余的一维数组来提取两个聚类阈值。第一个阈值将小型客户与中型客户分开，第二个阈值将中型客户与大型客户分开。这两个阈值共同将我们的180名客户分成三个大小相等的集群。
- en: Listing 14.14 Reducing the rotated data to 1D for the purposes of clustering
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.14 将旋转数据简化为一维以进行聚类
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can visualize our thresholds by utilizing them to vertically slice our `best_reduced_
    data` plot. The two slices split the plot into three segments, where each segment
    corresponds to a customer size. Next, we visualize the thresholds and the segments
    while coloring each segment (figure 14.9).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用它们垂直切割`best_reduced_data`图来可视化我们的阈值。这两个切割将图分为三个部分，每个部分对应一个客户大小。接下来，我们将可视化阈值和部分，并为每个部分着色（图14.9）。
- en: '![](../Images/14-09.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-09.png)'
- en: 'Figure 14.9 A horizontal plot of `centered_data`, segmented using two vertical
    thresholds. The segmentation splits the plot into three customer clusters: small,
    medium, and large. The one-dimensional x-axis is sufficient to extract these clusters.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9 使用两个垂直阈值对`centered_data`进行水平绘制。细分将图分为三个客户集群：小型、中型和大型。一维x轴足以提取这些集群。
- en: Listing 14.15 Plotting horizontal customer data separated into three segments
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.15 绘制分为三个段落的水平客户数据
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Takes as input a horizontally positioned customer dataset, segments the data
    using vertical thresholds, and plots each customer segment separately. This function
    is reused elsewhere in this section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入一个水平定位的客户数据集，使用垂直阈值细分数据，并分别绘制每个客户细分。此函数在本节的其他地方被重复使用。
- en: ❷ 1D x-value thresholds are utilized to segment the data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用一维x值阈值来细分数据。
- en: ❸ Each customer segment is plotted separately.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个客户细分单独绘制。
- en: Our 1D `x_values` array can sufficiently segment the customer data because it
    captures 99.08% of the data’s variance. Consequently, we can use the array to
    reproduce 99.08% of our `centered_data` dataset (figure 14.10). We simply need
    to reintroduce the y-axis dimension by adding an array of zeros. Then we need
    to rotate the resulting array back to its original position. These steps are carried
    out next.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的1D `x_values`数组足以对客户数据进行细分，因为它捕捉到了数据方差的99.08%。因此，我们可以使用该数组来复制99.08%的`centered_data`数据集（图14.10）。我们只需通过添加一个全零数组来重新引入y轴维度。然后我们需要将得到的数组旋转回原始位置。这些步骤将在下一步进行。
- en: '![](../Images/14-10.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-10.png)'
- en: Figure 14.10 A plot of the reproduced data together with the original data points.
    Our `reproduced_data` array forms a single line that cuts through our `centered_data`
    scatter plot. That line represents the linear direction in which data variance
    is maximized. 99.08% of the total variance is covered by the `reproduced_data`
    line.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10 复制数据与原始数据点的对比图。我们的`reproduced_data`数组形成了一条穿过`centered_data`散点图的直线。这条线代表了数据方差最大化的线性方向。`reproduced_data`线覆盖了总方差的99.08%。
- en: Listing 14.16 Reproducing 2D data from a 1D array
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.16 从一维数组中复制二维数据
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Returns a vector of zeros
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个全零向量
- en: Let’s plot `reproduced_data` together with our `centered_data` matrix to gauge
    the quality of the reproduction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起绘制`reproduced_data`和我们的`centered_data`矩阵，以评估复制的质量。
- en: Listing 14.17 Plotting reproduced and original data
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.17 绘制重现数据和原始数据
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The reproduced data forms a line that cuts directly through the middle of the
    `centered_data` scatter plot. The line represents the *first principal direction*,
    which is the linear direction in which the data’s variance is maximized. Most
    2D datasets contain two principal directions. The *second principal direction*
    is perpendicular to the first; it represents the remaining variance not covered
    by the first direction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 重现的数据形成了一条直接穿过`centered_data`散点图中间的线。这条线代表*第一主方向*，这是数据方差最大化的线性方向。大多数二维数据集包含两个主方向。*第二主方向*与第一个方向垂直；它代表第一方向未覆盖的剩余方差。
- en: We can use the first principal direction to process the heights and weights
    of future customers. We’ll assume that these customers originate from the same
    distribution that underlies our existing `measurements` data. If so, then their
    centralized heights and weights also lie along the first principal direction seen
    in figure 14.10\. That alignment will eventually allow us to segment the new customer
    data using our existing thresholds.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用第一主方向来处理未来客户的身高和体重。我们假设这些客户来自与我们现有的`measurements`数据相同的分布。如果是这样，那么他们的中心化身高和体重也将沿着图14.10中看到的第一个主方向。这种对齐最终将使我们能够使用现有的阈值来分割新客户数据。
- en: Let’s explore this scenario more concretely by simulating new customer measurements.
    Then we’ll centralize and plot our measurement data (figure 14.11). We also plot
    a line representing the first principal direction—we expect the plotted measurements
    to align with that directional line.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过模拟新的客户测量数据来更具体地探讨这个场景。然后我们将集中并绘制我们的测量数据（图14.11）。我们还绘制了一条代表第一主方向的线——我们预计绘制的测量数据将与这条方向线对齐。
- en: '![](../Images/14-11.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-11.png)'
- en: Figure 14.11 A centralized plot containing new customer data together with the
    principal direction from our original customer dataset. The principal direction
    cuts directly through a previously unseen data plot. That direction’s angle with
    the x-axis is known. Thus, we can confidently flip that data on its side for the
    purposes of clustering.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11 包含新客户数据和原始客户数据集的主方向的集中绘图。主方向直接穿过一个以前未见过的数据图。该方向与x轴的夹角是已知的。因此，我们可以自信地将这些数据侧过来，以便进行聚类。
- en: Note The first principal direction intersects with the origin. Thus, we need
    to ensure that our new customer data also intersects with the origin, for alignment
    purposes. Consequently, we must centralize that data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：第一主方向与原点相交。因此，我们必须确保我们的新客户数据也与原点相交，以便对齐。因此，我们必须集中这些数据。
- en: Listing 14.18 Simulating and plotting new customer data
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.18 模拟和绘制新客户数据
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Separates all new heights by 0.11 inches to minimize overlap with previous
    heights
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有新的身高分开0.11英寸，以最大限度地减少与先前身高的重叠
- en: ❷ We assume that the new customer distribution is the same as the previously
    seen distribution. This allows us to utilize existing means for data centralization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们假设新的客户分布与之前看到的分布相同。这使我们能够利用现有的数据集中化方法。
- en: Our new customer data continues to lie along the first principal direction.
    That direction covers more than 99% of the data variance while also forming a
    78.3-degree angle with the x-axis. Consequently, we can flip our new data on its
    side by rotating it 78.3 degrees. The resulting x-values cover more than 99% of
    the total variance. That high horizontal dispersion permits us to segment our
    customers without relying on y-value information. Our existing 1D segmentation
    thresholds should prove sufficient for that purpose.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新客户数据继续沿着第一主方向。这个方向覆盖了超过99%的数据方差，同时与x轴形成78.3度的角度。因此，我们可以通过旋转78.3度将我们的新数据侧过来。结果x值覆盖了超过99%的总方差。这种高水平的水平分散使我们能够在不依赖y值信息的情况下分割客户。我们现有的1D分割阈值应该足以完成这个目的。
- en: Next, we position our new customer data horizontally and segment that data using
    our `plot_customer_segments` function (figure 14.12).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将新客户数据水平放置，并使用我们的`plot_customer_segments`函数来分割这些数据（图14.12）。
- en: Listing 14.19 Rotating and segmenting our new customer data
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.19 旋转和分割我们的新客户数据
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/14-12.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-12.png)'
- en: 'Figure 14.12 A horizontal plot of our new customer data. The data is segmented
    using two previously computed vertical thresholds. The segmentation splits the
    plot into three customer clusters: small, medium, and large. The one-dimensional
    x-axis is sufficient to extract these clusters.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12 新客户数据的水平图。数据使用先前计算的两个垂直阈值进行分段。分段将图分为三个客户集群：小型、中型和大型。一维x轴足以提取这些集群。
- en: We’ll now briefly recap our observations. We can reduce any 2D array of customer
    measurements to one dimension by flipping the data on its side. The 1D horizontal
    x-values should be sufficient to cluster customers by size. Also, it’s easier
    to flip the data when we know the principal direction along which the variance
    is maximized. Given the first principal direction, we dimensionally reduce customer
    data for easier clustering.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将简要回顾我们的观察结果。我们可以通过将数据侧翻来将任何二维客户测量数组降低到一维。一维的水平x值应该足以按大小对客户进行聚类。此外，当我们知道方差最大化的主方向时，翻转数据更容易。给定第一个主方向，我们通过降维来简化客户数据，以便更容易聚类。
- en: Note As an added bonus, dimension reduction allows us to simplify our customer
    database. Rather than storing both height and weight, we can just store the horizontal
    x-value. Reducing database storage from 2D to one dimension will speed up customer
    lookups and lower our storage costs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：作为额外的好处，降维使我们能够简化我们的客户数据库。我们不必存储身高和体重，只需存储水平x值即可。将数据库存储从二维降低到一维将加快客户查找速度并降低我们的存储成本。
- en: Thus far, we have extracted the first principal direction by rotating our data
    to maximize the variance. Unfortunately, this technique does not scale to higher
    dimensions. Imagine if we analyze a 1,000-dimensional dataset; checking every
    angle across 1,000 different axes is not computationally feasible. Fortunately,
    there’s an easier way to extract all principal directions. We just need to apply
    a scalable algorithm known as *principal component analysis* (PCA).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过旋转数据以最大化方差来提取了第一个主方向。不幸的是，这种技术无法扩展到更高维度。想象一下，如果我们分析一个1000维度的数据集，检查1000个不同轴的每个角度在计算上是不切实际的。幸运的是，有一种更简单的方法来提取所有主方向。我们只需要应用一种称为*主成分分析*（PCA）的可扩展算法。
- en: We explore PCA in the next few subsections. It is simple to implement, but it
    can be tricky to understand. We thus explore the algorithm in parts. We begin
    by running scikit-learn’s PCA implementation. We apply PCA to several datasets
    to achieve better clustering and visualization. Then we probe the weaknesses of
    the algorithm by deriving PCA from scratch. Finally, we eliminate these weaknesses.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个小节中，我们将探讨PCA。它易于实现，但理解起来可能有些棘手。因此，我们将分部分探讨该算法。我们首先运行scikit-learn的PCA实现。我们将PCA应用于几个数据集以实现更好的聚类和可视化。然后，我们通过从头开始推导PCA来探究算法的弱点。最后，我们消除这些弱点。
- en: 14.2 Dimension reduction using PCA and scikit-learn
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 使用PCA和scikit-learn进行降维
- en: The PCA algorithm adjusts a dataset’s axes so that most of the variance is spread
    across a small number of dimensions. Consequently, not every dimension is required
    to distinguish between the data points. Simplified data distinction leads to simplified
    clustering. Hence, it is fortunate that scikit-learn provides a principal component
    analysis class called `PCA`. Let’s import `PCA` from `sklearn.decomposition`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法调整数据集的轴，使得大部分方差分布在少数几个维度上。因此，并非每个维度都需要用来区分数据点。简化的数据区分导致聚类简化。因此，scikit-learn提供了一个名为`PCA`的主成分分析类，这真是幸运。让我们从`sklearn.decomposition`中导入`PCA`。
- en: Listing 14.20 Importing `PCA` from scikit-learn
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.20 从scikit-learn导入`PCA`
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Running `PCA()` initializes a `pca_model` object, which is structured similarly
    to the scikit-learn `cluster_model` objects utilized in section 10\. In that section,
    we created models capable of clustering inputted arrays. Now, we create a PCA
    model capable of flipping our `measurements` array onto its side.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`PCA()`初始化一个`pca_model`对象，其结构与第10节中使用的scikit-learn `cluster_model`对象相似。在那个章节中，我们创建了能够聚类输入数组的模型。现在，我们创建一个能够将我们的`measurements`数组侧翻的PCA模型。
- en: Listing 14.21 Initializing a `pca_model` object
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.21 初始化`pca_model`对象
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using `pca_model`, we can horizontally flip a 2D `data` matrix by running `pca_
    model.fit_transform(data)`. That method call assigns axes to the matrix columns
    and subsequently reorients these axes to maximize the variance. However, in our
    `measurements` array, the axes are stored in the matrix rows. Thus, we need to
    swap the rows and columns by taking the transpose of the matrix. Running `pca_model.fit_
    transform(measurements.T)` returns a `pca_transformed_data` matrix. The first
    matrix column represents the x-axis across which the variance is maximized, and
    the second column represents the y-axis across which the variance is minimized.
    The plot of these two columns should resemble a cigar lying on its side. Let’s
    verify: we run the `fit_transform` method on `measurements.T` and then plot the
    columns of the result (figure 14.13). As a reminder, the *i*th column of a NumPy
    matrix `M` can be accessed by running `M[:,i]`.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pca_model`，我们可以通过运行 `pca_model.fit_transform(data)` 来水平翻转一个 2D `data` 矩阵。该方法调用将轴分配给矩阵列，并随后重新定位这些轴以最大化方差。然而，在我们的
    `measurements` 数组中，轴存储在矩阵行中。因此，我们需要通过取矩阵的转置来交换行和列。运行 `pca_model.fit_transform(measurements.T)`
    返回一个 `pca_transformed_data` 矩阵。第一个矩阵列代表方差最大化的 x 轴，第二个列代表方差最小化的 y 轴。这两个列的绘图应类似于侧躺的雪茄。让我们验证一下：我们在
    `measurements.T` 上运行 `fit_transform` 方法，然后绘制结果列（图 14.13）。作为提醒，NumPy 矩阵 `M` 的第
    *i* 个列可以通过运行 `M[:,i]` 来访问。
- en: Listing 14.22 Running PCA using scikit-learn
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.22 使用 scikit-learn 运行 PCA
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/14-13.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-13.png)'
- en: Figure 14.13 Plotted output from scikit-learn’s PCA implementation. The plot
    is a mirror image of the horizontally positioned customer data from figure 14.8\.
    PCA has reoriented that data so its variance lies primarily along the x-axis.
    Thus, the y-axis can be deleted with minimal information loss.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13 scikit-learn 的 PCA 实现的绘图输出。该图是图 14.8 中水平放置的客户数据的镜像。PCA 已重新定位这些数据，使其方差主要沿
    x 轴分布。因此，可以删除 y 轴而损失的信息最少。
- en: Our plot is a mirror of figure 14.8, with the y-values reflected across the
    y-axis. Running PCA on a 2D dataset is guaranteed to tip over that data so it
    lies horizontally on the x-axis. However, the actual reflection of data is not
    restricted to one particular orientation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图是图 14.8 的镜像，y 值在 y 轴上反射。对二维数据集运行 PCA 保证将数据倾斜，使其水平位于 x 轴上。然而，数据的实际反射并不局限于特定的方向。
- en: Note We can re-create our original horizontal plot by multiplying all y-values
    by –1\. Consequently, running `plot_customer_segments( (pca_transformed_ data
    * np.array([1, -1])).T)` generates a plot of segmented customer sizes that’s equivalent
    to figure 14.9.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们可以通过将所有 y 值乘以 –1 来重新创建原始的水平图。因此，运行 `plot_customer_segments((pca_transformed_data
    * np.array([1, -1])).T)` 会生成一个分段客户规模的图，与图 14.9 相当。
- en: Even though our plotted data is oriented differently, its x-axis variance coverage
    should remain consistent with our previous observations. We can confirm using
    the `explained_variance_ratio_` attribute of `pca_object`. This attribute holds
    an array of fractional variances covered by each axis. Thus, `100 * pca_object.explained_
    variance_ratio_[0]` should equal the previously observed x-axis coverage of approximately
    99.08%. Let’s verify.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的绘图数据方向不同，但其 x 轴的方差覆盖率应与之前的观察结果一致。我们可以使用 `pca_object` 的 `explained_variance_ratio_`
    属性来确认。该属性包含每个轴覆盖的分数方差数组。因此，`100 * pca_object.explained_variance_ratio_[0]` 应等于之前观察到的约
    99.08% 的 x 轴覆盖率。让我们验证一下。
- en: Listing 14.23 Extracting variance from scikit-learn’s PCA output
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.23 从 scikit-learn 的 PCA 输出中提取方差
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The attribute is a NumPy array containing fractional coverage for each axis.
    Multiplying by 100 converts these fractions into percentages.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该属性是一个 NumPy 数组，包含每个轴的分数覆盖率。乘以 100 将这些分数转换为百分比。
- en: ❶ Each *i* th element of the array corresponds to the variance coverage of the
    *i* th axis.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数组的第 *i* 个元素对应于第 *i* 个轴的方差覆盖率。
- en: Our `pca_object` has maximized the x-axis variance by uncovering the dataset’s
    two principal directions. These directions are stored as vectors in the `pca.components`
    attribute (which is itself a matrix). As a reminder, vectors are linear segments
    that point in a certain direction from the origin. Also, the first principal direction
    is a line that rises from the origin. Consequently, we can represent the first
    principal direction as a vector called the *first principal component*. We can
    access the first principal component of our data by printing `pca_object.components[0]`.
    Next, we output that vector, as well as its magnitude.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`pca_object`通过揭示数据集的两个主方向，最大化了x轴的方差。这些方向存储在`pca.components`属性中（它本身是一个矩阵）。提醒一下，向量是从原点指向某个方向的线性段。此外，第一主方向是从原点升起的一条线。因此，我们可以将第一主方向表示为一个称为*第一主成分*的向量。我们可以通过打印`pca_object.components[0]`来访问我们数据的第一个主成分。接下来，我们输出该向量及其模量。
- en: Listing 14.24 Outputting the first principal component
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.24 输出第一个主成分
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first principal component is a unit vector with a magnitude of 1.0\. It
    stretches one whole unit length from the origin. Multiplying the vector by a number
    stretches the magnitude out further. If we stretch it far enough, we can capture
    the entire principal direction of our data. In other words, we can stretch the
    vector until it completely skewers the interior of our cigar-shaped plot, like
    a corn dog on a stick. The visualized result should be identical to figure 14.10.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一主成分是一个模量为1.0的单位向量。它从原点延伸出一个完整的单位长度。将向量乘以一个数可以进一步拉伸其模量。如果我们拉伸得足够远，我们可以捕捉到数据的整个主方向。换句话说，我们可以将向量拉伸到完全贯穿我们香肠形状图表的内部，就像一根棒上的热狗。可视化的结果应该与图14.10相同。
- en: Note If vector `pc` is a principal component, then `-pc` is also a principal
    component. The `-pc` vector represents the mirror image of `pc`. Both vectors
    lie along the first principal direction, even though they point away from each
    other. Hence, `pc` and `-pc` can be used interchangeably during the dimension
    reduction process.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果向量`pc`是主成分，那么`-pc`也是主成分。`-pc`向量代表`pc`的镜像。尽管这两个向量指向对方，但它们都沿着第一主方向。因此，在降维过程中，`pc`和`-pc`可以互换使用。
- en: Next, we generate that plot (figure 14.14). First, we stretch our `first_pc`
    vector until it extends 50 units in both the positive and negative directions
    from the origin. We then plot the stretched segment along with our previously
    computed `centered_data` matrix. Later, we’ll use the plotted segment to gain
    deeper insights into the workings of the PCA algorithm.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成那个图表（图14.14）。首先，我们将`first_pc`向量拉伸，使其从原点向正负两个方向延伸50个单位。然后，我们将拉伸的部分与之前计算出的`centered_data`矩阵一起绘制。稍后，我们将使用绘制的部分来深入了解PCA算法的工作原理。
- en: '![](../Images/14-14.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-14.png)'
- en: Figure 14.14 A centralized plot containing customer data together with the first
    principal direction. The direction has been plotted by stretching out the first
    principal component.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14 包含客户数据和第一主方向的集中化图表。方向是通过拉伸第一主成分绘制的。
- en: Note We plot `centered_data` and not `measurements` because `centered_ data`
    is centered at the origin. Our stretched-out vector is also centered at the origin.
    This makes the centered matrix and the vector visually comparable.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们绘制的是`centered_data`而不是`measurements`，因为`centered_data`是以原点为中心的。我们拉伸出的向量也是以原点为中心的。这使得中心化的矩阵和向量在视觉上可以比较。
- en: Listing 14.25 Stretching a unit vector to cover the first principal direction
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.25 拉伸单位向量以覆盖第一主方向
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The function stretches out an inputted unit vector v. The stretched segment
    extends 50 units in both the positive and negative directions from the origin.
    Then the stretched segment is plotted. We reuse this function shortly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 函数将输入的单位向量v拉伸。拉伸的部分从原点向正负两个方向延伸50个单位。然后绘制拉伸的部分。我们很快会再次使用这个函数。
- en: We’ve used the first principal component to skewer our dataset along its first
    principal direction. In that same manner, we can stretch the other directional
    unit vector returned by the PCA algorithm. As stated earlier, most 2D datasets
    contain two principal directions. The second principal direction is perpendicular
    to the first, and its vectorized representation is called the *second principal
    component*. That component is stored in the second row of our computed `components`
    matrix.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用第一个主成分来沿着其第一个主方向刺穿我们的数据集。同样地，我们可以拉伸PCA算法返回的其他方向单位向量。如前所述，大多数二维数据集包含两个主方向。第二个主方向与第一个方向垂直，其向量表示称为**第二个主成分**。该成分存储在我们计算的`components`矩阵的第二行。
- en: Why should we care about the second principal component? After all, it points
    in a direction that covers less than 1% of data variance. Nonetheless, that component
    has its uses. Both the first and second principal components share a special relationship
    with our data’s x- and y-axes. Visually uncovering that relationship will make
    PCA easier to understand. Hence, we now stretch and plot both of the components
    in the `components` matrix. Additionally, we plot `centered_data`, as well as
    both our axes. The final visualization will provide us with valuable insights
    (figure 14.15).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们应该关注第二个主成分呢？毕竟，它指向的方向覆盖的数据方差不到1%。尽管如此，这个成分有其用途。第一个和第二个主成分都与我们的数据x轴和y轴有特殊的关系。直观地揭示这种关系会使主成分分析（PCA）更容易理解。因此，我们现在在`components`矩阵中拉伸并绘制这两个成分。此外，我们还绘制了`centered_data`以及两个轴。最终的可视化将为我们提供宝贵的见解（图
    14.15）。
- en: '![](../Images/14-15.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-15.png)'
- en: Figure 14.15 The first and second principal directions, plotted together with
    the customer data. These directions are perpendicular to each other. If we were
    to rotate the x- and y-axes by 78.3 degrees, they would align perfectly with the
    principal directions. Thus, swapping the axes with the principal directions reproduces
    the horizontal plot shown in figure 14.8.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.15 将第一和第二个主方向与客户数据一起绘制。这些方向相互垂直。如果我们把x轴和y轴旋转78.3度，它们将完美地与主方向对齐。因此，用主方向交换轴可以重现图
    14.8 中显示的水平图。
- en: Listing 14.26 Plotting principal directions, axes, and data information
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.26 绘制主方向、轴和数据信息
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Plots the x-axis and y-axis by stretching out two unit vectors (one vertical
    and one horizontal) so that their magnitudes align with the stretched principal
    components. Consequently, the stretched axes and stretched principal components
    are rendered visually comparable.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过拉伸两个单位向量（一个垂直和一个水平）来绘制x轴和y轴，使它们的幅度与拉伸的主成分对齐。因此，拉伸的轴和拉伸的主成分在视觉上是可比较的。
- en: According to the plot, the two principal directions are essentially rotated
    versions of the x- and y-axes. Imagine if we rotated our two axes counterclockwise
    by 78.3 degrees. After the rotation, the x-axis and y-axis would align with the
    two principal components. The variances covered by these axes would equal 99.02
    and 0.08%, respectively. Hence, this axis swap would reproduce the horizontal
    plot in figure 14.8.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 根据绘图，两个主方向实际上是x轴和y轴的旋转版本。想象一下，如果我们把两个轴逆时针旋转78.3度。旋转后，x轴和y轴将与两个主成分对齐。这些轴覆盖的方差将分别等于99.02%和0.08%。因此，这种轴交换将重现图
    14.8 中的水平图。
- en: Note Tilting your head to the left while staring at figure 14.15 will help you
    picture this outcome.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在注视图 14.15 的同时向左倾斜头部将帮助你想象这个结果。
- en: The aforementioned axis swap is known as a *projection*. Swapping our two axes
    for the principal directions is referred to as a *projection onto the principal
    directions*. Using trigonometry, we can show that the projection of `centered_data`
    onto the principal directions is equal to the matrix product of `centered_data`
    and the two principal components. In other words, `principal_components @ centered_data`
    repositions the dataset’s x and y coordinates relative to the principal directions.
    The final output should equal `pca_transformed_data.T`. Let’s confirm with the
    code in listing 14.27.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上文提到的轴交换被称为**投影**。将我们的两个轴交换为主方向被称为**投影到主方向**。通过三角学，我们可以证明`centered_data`投影到主方向等于`centered_data`和两个主成分的矩阵乘积。换句话说，`principal_components
    @ centered_data`重新定位了数据集的x和y坐标相对于主方向。最终的输出应该等于`pca_transformed_data.T`。让我们通过列表
    14.27 中的代码来确认。
- en: Note More generally, the dot product between the *i* th principal component
    and a centered data point projects that data point onto the *i* th principal direction.
    Thus, running `first_pc @ centered_data[i]` projects the *i* th data point onto
    the first principal direction. The result equals the x-value obtained when the
    x-axis is swapped with the first principal direction (`pca_transformed_ data[i][0]`).
    In this manner, we can project multiple data points onto multiple principal directions
    using matrix multiplication.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更普遍地，第 *i* 个主成分与一个中心化数据点的点积将数据点投影到第 *i* 个主方向。因此，运行 `first_pc @ centered_data[i]`
    将第 *i* 个数据点投影到第一个主方向。结果等于当x轴与第一个主方向交换时获得的x值（`pca_transformed_data[i][0]`）。这样，我们可以通过矩阵乘法将多个数据点投影到多个主方向。
- en: Listing 14.27 Swapping standard axes for principal directions using projection
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.27 使用投影交换标准轴与主方向
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The reoriented output of PCA is dependent on projection. In general terms,
    the PCA algorithm works as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的重新定位输出取决于投影。一般而言，PCA算法的工作原理如下：
- en: Centralize the input data by subtracting the mean from each data point.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从每个数据点中减去平均值来集中输入数据。
- en: Compute the dataset’s principal components. The computation details are discussed
    later in this section.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集的主成分。计算细节将在本节稍后讨论。
- en: Take the matrix product between the centralized data and the principal components.
    This swaps the data’s standard axes for its principal directions.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集中化数据与主成分之间的矩阵乘积。这交换了数据的标准轴为其主方向。
- en: 'Generally, an *N*-dimensional dataset has *N* principal directions (one for
    each axis). The *k*th principal direction maximizes the variance not covered by
    the first *k* – 1 directions. Thus, a 4D dataset has four principal directions:
    the first principal direction maximizes unidirectional dispersion, the second
    maximizes all unidirectional dispersion not covered by the first, and the final
    two cover all the remaining variance.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个 *N*-维数据集有 *N* 个主方向（每个轴一个）。第 *k* 个主方向最大化未被前 *k* – 1 个方向覆盖的方差。因此，一个4维数据集有四个主方向：第一个主方向最大化单向分散，第二个最大化未被第一个方向覆盖的所有单向分散，最后两个覆盖所有剩余的方差。
- en: Here’s where it gets interesting. Suppose we project a 4D dataset onto its four
    principal directions. The dataset’s standard axes are thus swapped with its principal
    directions. Under the right circumstances, two of the new axes will cover a good
    chunk of the variance. Consequently, the remaining axes could be discarded, with
    minimal information loss. Disposing of the two axes would reduce the 4D dataset
    to two dimensions. We would then be able to visualize that data in a 2D scatter
    plot. Ideally, the 2D plot would maintain enough dispersion for us to correctly
    identify data clusters. Let’s explore an actual scenario where we visualize 4D
    data in two dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这里变得有趣了。假设我们将一个4维数据集投影到其四个主方向。因此，数据集的标准轴与其主方向交换。在适当的条件下，两个新轴将覆盖相当一部分方差。因此，剩余的轴可以被丢弃，信息损失最小。丢弃这两个轴将把4维数据集减少到两个维度。然后我们就能在2维散点图中可视化这些数据。理想情况下，2维图将保持足够的分散，使我们能够正确识别数据簇。让我们探索一个实际场景，其中我们在二维中可视化4维数据。
- en: Key terminology
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 关键术语
- en: '*First principal direction*—The linear direction in which data dispersion is
    maximized. Swapping the x-axis with the first principal direction reorients the
    dataset to maximize its spread horizontally. The reorientation can allow for more
    straightforward 1D clustering.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第一个主方向*—数据分散最大化的线性方向。将x轴与第一个主方向交换重新定位数据集以最大化其水平方向的分布。这种重新定位可以允许更简单的1D聚类。'
- en: K*th principal direction*—The linear direction that maximizes the variance not
    covered by the first *K* – 1 principal directions.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 *k* 个主方向*—最大化未被前 *K* – 1 个主方向覆盖的方差的线性方向。
- en: K*th principal component*—A unit vector representation of the *K*th principal
    direction. This vector can be utilized for directional projection.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 *k* 个主成分*—第 *k* 个主方向的单位向量表示。这个向量可以用于方向投影。
- en: '*Projection*—Projecting data onto a principal direction is analogous to swapping
    a standard axis with that direction. We can project a dataset onto its top *K*
    principal directions by taking the matrix product of the centralized dataset with
    its top *K* principal components.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*投影*—将数据投影到主方向上相当于将标准轴与该方向交换。我们可以通过将集中化数据集与其前 *K* 个主成分的矩阵乘积来将数据集投影到其前 *K* 个主方向。'
- en: 14.3 Clustering 4D data in two dimensions
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 在二维中对 4D 数据进行聚类
- en: 'Imagine we are botanists studying flowers in a blooming meadow. We randomly
    select 150 flowers. For every flower, we record the following measurements:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是一群在盛开的花草丛中研究花朵的植物学家。我们随机选择了 150 朵花。对于每一朵花，我们记录以下测量值：
- en: The length of a colorful petal
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彩色花瓣的长度
- en: The width of the colorful petal
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彩色花瓣的宽度
- en: The length of a green leaf supporting the petal
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的绿色叶片的长度
- en: The width of the green leaf supporting the petal
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的绿色叶片的宽度
- en: 'These 4D flower measurements already exist and can be accessed using scikit-learn.
    We can obtain the measurements by importing `load_iris` from `sklearn.datasets`.
    Calling `load_iris()[''data'']` returns a matrix containing 150 rows and 4 columns:
    each row corresponds to a flower, and each column corresponds to the leaf and
    petal measurements. Next, we load the data and print the measurements for a single
    flower. All recorded measurements are in centimeters.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 4D 花朵测量值已经存在，并且可以使用 scikit-learn 访问。我们可以通过从 `sklearn.datasets` 导入 `load_iris`
    来获取测量值。调用 `load_iris()['data']` 返回一个包含 150 行和 4 列的矩阵：每一行对应一朵花，每一列对应叶子和花瓣的测量值。接下来，我们加载数据并打印单朵花的测量值。所有记录的测量值都是以厘米为单位的。
- en: Listing 14.28 Loading flower measurements from scikit-learn
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.28 从 scikit-learn 加载花朵测量值
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Given this matrix of flower measurements, our goals are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个花朵测量值的矩阵，我们的目标如下：
- en: We want to visualize our flower data in 2D space.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想在二维空间中可视化我们的花朵数据。
- en: We want to determine whether any clusters are present in the 2D visualization.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想确定在 2D 可视化中是否存在任何聚类。
- en: We want to build a very simple model to distinguish between flower cluster types
    (assuming any clusters are found).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想构建一个非常简单的模型来区分花朵簇类型（假设找到了任何簇）。
- en: We start by visualizing the data. It is four-dimensional, but we want to plot
    it in 2D. Reducing the data to two dimensions requires that we project it onto
    its first and second principal directions. The remaining two directions can be
    discarded. Thus, our analysis only requires the first two principal components.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先可视化数据。它是四维的，但我们想将其绘制在二维中。将数据降低到二维需要将其投影到其第一和第二主方向上。其余的两个方向可以被丢弃。因此，我们的分析只需要前两个主成分。
- en: Using scikit-learn, we can limit PCA analysis to the top two principal components.
    We just need to run `PCA(n_components=2)` during the initialization of the `PCA`
    object. The initialized object will be capable of reducing input data to a two-dimensional
    projection. Next, we initialize a two-component `PCA` object and use `fit_transform`
    to reduce our flower measurements to 2D.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn，我们可以将 PCA 分析限制在前两个主成分。我们只需在 `PCA` 对象初始化时运行 `PCA(n_components=2)`。初始化后的对象将能够将输入数据降低到二维投影。接下来，我们初始化一个两成分的
    `PCA` 对象，并使用 `fit_transform` 将我们的花朵测量值降低到二维。
- en: Listing 14.29 Reducing flower measurements to two dimensions
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.29 将花朵测量值降至二维
- en: '[PRE28]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The computed `transformed_data_2D` matrix should be two-dimensional, containing
    just two columns. Let’s confirm.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的 `transformed_data_2D` 矩阵应该是二维的，只包含两列。让我们确认一下。
- en: Listing 14.30 Checking the shape of a dimensionally reduced matrix
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.30 检查降维矩阵的形状
- en: '[PRE29]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How much of the total data variance is covered by our outputted data matrix?
    We can find out using the `explained_variance_ratio_` attribute of `pca_object_2D`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输出的数据矩阵覆盖了总数据方差的多少？我们可以使用 `pca_object_2D` 的 `explained_variance_ratio_` 属性来找出。
- en: Listing 14.31 Measuring the variance coverage of a dimensionally reduced matrix
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.31 测量降维矩阵的方差覆盖率
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Computes the variance coverage of the dimensionally reduced 2D dataset associated
    with pca_object. This function is reused elsewhere in the section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算与 pca_object 相关的降维 2D 数据集的方差覆盖率。此函数在本节的其他地方被重用。
- en: Our dimensionally reduced matrix covers more than 97% of the total data variance.
    Thus, a scatter plot of `transformed_data_2D` should display most of the clustering
    patterns present in the dataset (figure 14.16).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的降维矩阵覆盖了超过 97% 的总数据方差。因此，`transformed_data_2D` 的散点图应该显示数据集中存在的大多数聚类模式（图 14.16）。
- en: Listing 14.32 Plotting flower data in 2D
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.32 在 2D 中绘制花朵数据
- en: '[PRE31]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](../Images/14-16.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-16.png)'
- en: 'Figure 14.16 4D flower measurements plotted in 2D. The measurements were reduced
    to two dimensions using PCA. The reduced data covers more than 97% of the total
    variance. Our 2D plot is informative: two or three flower clusters are clearly
    visible.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16 4D花测量值在二维空间中的绘制。这些测量值使用PCA降维到两个维度。降维数据覆盖了总方差的97%以上。我们的二维图很有信息量：两个或三个花簇清晰可见。
- en: Our flower data forms clusters when plotted in 2D. Based on the clustering,
    we can assume that two or three flower types are present. In fact, our measured
    data represents three unique species of flowers. This species information is stored
    in the `flower_data` dictionary. Next, we color our flower plot by species and
    verify that the colors fall in three distinct clusters (figure 14.17).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的花数据在二维空间中绘制时，会形成簇。基于聚类，我们可以假设存在两种或三种花类型。实际上，我们的测量数据代表三种独特的花种。这种物种信息存储在 `flower_data`
    字典中。接下来，我们按物种给花图着色，并验证颜色是否落在三个不同的簇中（图14.17）。
- en: Listing 14.33 Coloring plotted data by flower species
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.33 按花种着色绘制的数据
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Plots dimensionally reduced flower data while coloring it by species. This
    function is reused elsewhere in the section.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在按物种着色的同时绘制降维花数据。此函数在本节的其他地方被重用。
- en: ❷ Returns the names of the three flower species in our dataset
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回数据集中三种花种的名字
- en: ❸ Extracts just those coordinates associated with a particular species. For
    filtering purposes, we use flower_data[target], which maps to a list of species
    IDs. The IDs correspond to the three species names. If the *j*th flower corresponds
    to species_name[i], then its species ID equals *j*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅提取与特定物种相关的坐标。为了过滤目的，我们使用 `flower_data[target]`，它映射到一个物种ID列表。这些ID对应于三个物种名称。如果第
    *j* 朵花对应于 species_name[i]，那么它的物种ID等于 *j*。
- en: ❹ Plots each species using a unique color
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用独特的颜色绘制每个物种
- en: '![](../Images/14-17.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-17.png)'
- en: Figure 14.17 4D flower measurements plotted in 2D. Each plotted flower point
    is colored based on its species. The three species fall into three clusters. Thus,
    our 2D reduction correctly captures the signal required to distinguish between
    species.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.17 4D花测量值在二维空间中的绘制。每个绘制的花点根据其物种着色。三种物种分为三个簇。因此，我们的二维降维正确地捕捉了区分物种所需的信号。
- en: 'For the most part, the three species are spatially distinct. *Versicolor* and
    *Virgincia* share a bit of overlap, implying that they have similar petal properties.
    On the other hand, *Setosa* forms an entirely separate cluster. A vertical x-value
    threshold of –2 is sufficient to isolate *Setosa* from all other species. Hence,
    we can define a very simple *Setosa* detection function. The function takes as
    input a four-element array called `flower_sample`, which holds four petal measurements.
    The function will do the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分情况下，三种物种在空间上是分开的。*Versicolor* 和 *Virgincia* 有一点重叠，这意味着它们有相似的 petals 属性。另一方面，*Setosa*
    形成一个完全独立的簇。一个垂直的x值阈值为-2足以将 *Setosa* 从所有其他物种中隔离出来。因此，我们可以定义一个非常简单的 *Setosa* 检测函数。该函数接受一个名为
    `flower_sample` 的四个元素数组作为输入，该数组包含四个花瓣测量值。该函数将执行以下操作：
- en: Centralize the sample by subtracting the mean of `flower_measurements` from
    `flower_sample`. This mean is stored as an attribute in `pca_object_2D`. It is
    equal to `pca_object_2D.mean_`.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从 `flower_sample` 中减去 `flower_measurements` 的平均值来集中样本。这个平均值存储在 `pca_object_2D`
    的一个属性中。它等于 `pca_object_2D.mean_`。
- en: Project the centralized sample onto the first principal direction by taking
    its dot product with the first principal component. As a reminder, the first principal
    component is stored in `pca_object_2D.components_[0]`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集中样本投影到第一主方向，通过将其与第一主成分的点积来实现。提醒一下，第一主成分存储在 `pca_object_2D.components_[0]`
    中。
- en: Check if the projected value is less than –2\. If so, then the flower sample
    will be treated as a possible *Setosa* species.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查投影值是否小于-2。如果是这样，那么这个花样本将被视为可能的 *Setosa* 物种。
- en: Note Our *Setosa* detection function doesn’t take into account any flower species
    beyond the three that we’ve recorded. However, the function should still sufficiently
    analyze new flowers in our meadow, where no additional species have been observed.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们的 *Setosa* 检测函数不考虑我们已记录的三种花以外的任何花种。然而，该函数仍然应该足够分析我们草地上的新花，因为我们没有观察到其他物种。
- en: Next, we define the `detect_setosa` function and then analyze a flower sample
    with measurements (in cm) of `[4.8, 3.7, 1.2, 0.24]`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `detect_setosa` 函数，然后分析一个具有测量值（单位为厘米）的 `[4.8, 3.7, 1.2, 0.24]` 的花样本。
- en: Listing 14.34 Defining a Setosa detector based on dimensionally reduced data
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.34 基于降维数据定义 Setosa 检测器
- en: '[PRE33]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The flower sample could be a *Setosa* according to our simple threshold analysis,
    which was made possible by PCA. Various benefits of PCA include the following:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的简单阈值分析，花朵样本可能是 *Setosa*，这是由 PCA 实现的。PCA 的各种好处包括以下内容：
- en: Visualization of complex data.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂数据的可视化。
- en: Simplified data classification and clustering.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化的数据分类和聚类。
- en: Simplified classification.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化的分类。
- en: Decreased memory usage. Reducing the data from four to two dimensions halves
    the number of bytes required to store the data.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了内存使用。将数据从四维减少到二维，将存储数据所需的字节数减半。
- en: Faster computations. Reducing the data from four to two dimensions speeds up
    the computation time required to compute a similarity matrix fourfold.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的计算。将数据从四维减少到二维，将计算相似性矩阵所需的时间缩短了四倍。
- en: So are we ready to use PCA to cluster our text data? Unfortunately, the answer
    is no. We must first discuss and address certain flaws that are inherent to the
    algorithm.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是否准备好使用 PCA 对我们的文本数据进行聚类？不幸的是，答案是否定的。我们必须首先讨论并解决算法固有的某些缺陷。
- en: Common scikit-learn PCA methods
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 scikit-learn PCA 方法
- en: '`pca_object = PCA()`—Creates a `PCA` object capable of reorienting input data
    so that its axes align with its principal directions.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_object = PCA()`—创建一个 `PCA` 对象，能够重新定位输入数据，使其轴与主方向对齐。'
- en: '`pca_object = PCA(n_components=K)`—Creates a `PCA` object capable of reorienting
    input data so that *K* of its axes align with the top *K* principal directions.
    All other axes are ignored. This reduces data to *K* dimensions.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_object = PCA(n_components=K)`—创建一个 `PCA` 对象，能够重新定位输入数据，使其 *K* 个轴与最高的 *K*
    个主方向对齐。所有其他轴被忽略。这减少了数据到 *K* 维。'
- en: '`pca_transformed_data = pca_object.fit_transform(data)`—Executes PCA on inputted
    data using an initialized `PCA` object. The `fit_transform` method assumes that
    the columns of the `data` matrix correspond to spatial axes. The axes are subsequently
    aligned with the data’s principal direction. This result is stored in the `pca_transformed_data`
    matrix.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_transformed_data = pca_object.fit_transform(data)`—使用初始化的 `PCA` 对象对输入数据执行
    PCA。`fit_transform` 方法假设 `data` 矩阵的列对应于空间轴。轴随后与数据的主方向对齐。此结果存储在 `pca_transformed_data`
    矩阵中。'
- en: '`pca_object.explained_variance_ratio_`—Returns the fractional variance coverage
    associated with each principal direction of a fitted `PCA` object. Each *i* th
    element corresponds to fractional variance coverage along the *i* th principal
    direction.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_object.explained_variance_ratio_`—返回与拟合 `PCA` 对象的每个主方向相关的方差覆盖率分数。第 *i*
    个元素对应于第 *i* 个主方向的方差覆盖率分数。'
- en: '`pca_object.mean_`—Returns the mean of the input data, which has been fitted
    to the `PCA` object.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_object.mean_`—返回拟合到 `PCA` 对象的输入数据的平均值。'
- en: '`pca_object.components_`—Returns the principal components of the input data,
    which have been fitted to the `PCA` object. Each *i*th row of the `components_`
    matrix corresponds to the *i*th principal component. Running `pca_ object.components_[i]
    @ (data[j] - pca_object.mean_)` projects the *j* th data point onto the *i* th
    principal component. The projected output equals `pca_transformed_data[j][i]`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pca_object.components_`—返回输入数据的特征值，这些特征值已经拟合到 `PCA` 对象中。`components_` 矩阵的每一行对应于第
    *i* 个主成分。运行 `pca_object.components_[i] @ (data[j] - pca_object.mean_)` 将第 *j*
    个数据点投影到第 *i* 个主成分上。投影输出等于 `pca_transformed_data[j][i]`。'
- en: 14.3.1 Limitations of PCA
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 PCA 的局限性
- en: PCA does have some serious limitations. It is overly sensitive to units of measurement.
    For example, our flower measurements are all in centimeters, but we can imagine
    converting the first axis into millimeters by running `10 * flower_measurements[0]`.
    The information content of that axis should not change; however, its variance
    will shift. Let’s convert the axis units to evaluate how the variance is affected.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 确实有一些严重的局限性。它对测量单位过于敏感。例如，我们的花朵测量值都是以厘米为单位，但我们可以想象通过运行 `10 * flower_measurements[0]`
    将第一个轴转换为毫米。该轴的信息含量不应该改变；然而，其方差将会发生变化。让我们转换轴单位来评估方差是如何受到影响的。
- en: Listing 14.35 Measuring the effect of unit change on axis variance
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.35 测量单位变化对轴方差的影响
- en: '[PRE34]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Our variance has increased 100-fold. Now the first axis variance dominates
    our dataset. Consider the consequences of running PCA on these modified flower
    measurements: PCA will attempt to find the axis where variance is maximized. This,
    of course, will yield the first axis, where variance has increased from 0.68 to
    68\. Consequently, PCA will project all the data onto the first axis. Our reduced
    data will collapse to one dimension! We can prove this by refitting `pca_object_2D`
    to `flower_measurements` and then printing the variance coverage.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据方差增加了100倍。现在，第一个轴方差主导了我们的数据集。考虑对这些修改后的花朵测量值运行PCA的后果：PCA将尝试找到方差最大化的轴。当然，这将产生方差从0.68增加到68的第一个轴。因此，PCA将所有数据投影到第一个轴上。我们的数据将缩减到一维！我们可以通过重新拟合`pca_object_2D`到`flower_measurements`并打印方差覆盖来证明这一点。
- en: Listing 14.36 Measuring the effect of unit change on PCA
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.36 测量单位变化对PCA的影响
- en: '[PRE35]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Refits our PCA object to the updated flower_measurements data
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将我们的PCA对象重新拟合到更新的flower_measurements数据
- en: More than 98% of the variance now lies along a single axis. Previously, two
    dimensions were required to capture 97% of the data variance. Clearly, we’ve introduced
    an error into our data. How do we resolve it? One obvious solution is to ensure
    that all the axes share the same units of measurements. However, such a practical
    approach is not always possible. Sometimes the units of measurement are just not
    available. Other times, the axes correspond to different measurement types (such
    as length and weight), so the units are incompatible. What should we do?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，超过98%的方差沿着一个单一的轴。之前，需要两个维度来捕获97%的数据方差。显然，我们已经在数据中引入了一个错误。我们如何解决这个问题？一个明显的解决方案是确保所有轴具有相同的测量单位。然而，这种实际的方法并不总是可行的。有时测量单位根本不可用。其他时候，轴对应于不同的测量类型（如长度和重量），因此单位不兼容。我们该怎么办？
- en: Let’s consider the root cause of our variance shift. We’ve made the values in
    `flower_ measurements[:,0]` larger, so their variance also grew larger. Differences
    in axis variance are caused by differences in value size. In the previous section,
    we were able to eliminate such differences in size using normalization. As a reminder,
    during normalization, a vector is divided by its magnitude. This produces a unit
    vector whose magnitude equals 1.0\. Thus, if we normalize our axes, all the axes
    values will lie between 0 and 1\. The dominance of the first axis will thus be
    eliminated. Let’s normalize `flower_ measurements` and then reduce the normalized
    data to two dimensions. The resulting 2D variance coverage should once again approximate
    97%.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑方差变化的根本原因。我们在`flower_measurements[:,0]`中的值变大了，因此它们的方差也变大了。轴方差的不同是由值大小的不同引起的。在上一个部分，我们能够通过标准化消除这种大小差异。提醒一下，在标准化过程中，一个向量被其大小除以。这产生了一个大小等于1.0的单位向量。因此，如果我们标准化我们的轴，所有轴的值都将介于0和1之间。因此，第一个轴的主导地位将被消除。让我们标准化`flower_measurements`并将标准化后的数据缩减到二维。结果二维方差覆盖应该再次接近97%。
- en: Listing 14.37 Normalizing data to eliminate measurement-unit differences
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.37 标准化数据以消除测量单位差异
- en: '[PRE36]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Normalization has slightly modified our data. Now the first principal direction
    covers 94% of the total variance rather than 92.46%. Meanwhile, the second principal
    component covers 3.67% of total variance rather than 5.31%. Despite these changes,
    the total 2D variance coverage still sums to approximately 97%. We replot the
    PCA output to confirm that the 2D clustering patterns remain unchanged (figure
    14.18).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化略微修改了我们的数据。现在，第一个主成分方向覆盖了总方差的94%，而不是92.46%。同时，第二个主成分覆盖了总方差的3.67%，而不是5.31%。尽管有这些变化，总的二维方差覆盖仍然大约是97%。我们重新绘制PCA输出以确认二维聚类模式保持不变（图14.18）。
- en: Listing 14.38 Plotting 2D PCA output after normalization
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.38 标准化后绘制二维PCA输出
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../Images/14-18.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-18.png)'
- en: Figure 14.18 4D normalized flower measurements plotted in 2D. Each plotted flower
    point is colored based on its species. The three species fall into three clusters.
    Thus, our 2D reduction correctly captures the signal required to distinguish between
    species.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.18 4D标准化花朵测量值在二维中绘制。每个绘制的花朵点根据其物种着色。三种物种分为三个簇。因此，我们的二维降维正确地捕捉了区分物种所需的信号。
- en: Our plot is slightly different from our previous observations. However, the
    three species of flowers continue to separate into three clusters, and *Setosa*
    remains spatially distinct from other species. Normalization has retained existing
    cluster separations while eliminating the error caused by unit differences.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图表与我们之前的观察略有不同。然而，三种花卉种类继续分离成三个簇，*Setosa*仍然与其他种类在空间上明显不同。归一化保留了现有的簇分离，同时消除了由单位差异引起的错误。
- en: 'Unfortunately, normalization does lead to unintended consequences. Our normalized
    axis values now lie between 0 and 1, so each axis mean is likewise between 0 and
    1\. All values lie less than 1 unit from their mean. This is a problem: PCA requires
    us to subtract the mean from each axis value to centralize our data. Then the
    centralized matrix is multiplied by the principal components to realign the axes.
    Regrettably, data centralization is not always achievable due to floating-point
    errors. It’s computationally difficult to subtract similar values with 100% precision,
    so it is difficult to subtract the mean from a value that is very close to that
    mean. For example, suppose we analyze an array containing two data points, `1
    + 1e-3` and `1 - 1e-3`. The mean of the array is equal to 1\. Subtracting 1 from
    the array should lead to a centralized mean of 0, but the actual mean will not
    equal 0 due to an error, illustrated next.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，归一化确实导致了意外的后果。我们的归一化轴值现在介于0和1之间，因此每个轴的平均值也介于0和1之间。所有值都小于1个单位远离其平均值。这是一个问题：PCA要求我们从每个轴值中减去平均值以归一化我们的数据。然后，归一化的矩阵乘以主成分以重新对齐轴。遗憾的是，由于浮点数错误，数据归一化并不总是可行的。从相似值中减去100%的精度在计算上是困难的，因此很难从非常接近平均值的值中减去平均值。例如，假设我们分析一个包含两个数据点`1
    + 1e-3`和`1 - 1e-3`的数组。该数组的平均值等于1。从数组中减去1应该导致归一化平均值为0，但由于错误，实际平均值不会等于0，如下所示。
- en: Listing 14.39 Illustrating errors caused by values proximate to their mean
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.39 展示由接近其平均值的值引起的错误
- en: '[PRE38]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ The mean of the centralized data does not equal 0 as intended.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 归一化数据的平均值并不等于预期的0。
- en: We cannot reliably centralize data that lies close to the mean. Hence, we can’t
    reliably execute PCA on normalized data. What should we do?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法可靠地归一化接近平均值的数值。因此，我们无法在归一化数据上可靠地执行PCA。我们应该怎么办？
- en: A solution to our problem does exist. However, to derive it, we must dive deep
    into the guts of the PCA algorithm. We must learn how to compute the principal
    components from scratch without rotation. That computation process is a bit abstract,
    but it can be understood without studying advanced mathematics. Once we derive
    the PCA algorithm, we’ll be able to modify it slightly. That minor modification
    will completely bypass data centralization. The modified algorithm, known as *singular
    value decomposition* (SVD), will allow us to efficiently cluster text data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问题确实存在解决方案。然而，要推导出它，我们必须深入到PCA算法的核心。我们必须学习如何从头开始计算主成分而不进行旋转。这个过程有点抽象，但无需学习高级数学就能理解。一旦我们推导出PCA算法，我们就能对其进行轻微的修改。这个微小的修改将完全绕过数据归一化。这个修改后的算法被称为*奇异值分解*（SVD），它将使我们能够有效地聚类文本数据。
- en: Note If you’re not interested in the SVD derivation, you can skip ahead to the
    final subsection. It describes SVD usage in scikit-learn.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您对SVD的推导不感兴趣，可以跳到最后一小节。它描述了scikit-learn中SVD的使用。
- en: 14.4 Computing principal components without rotation
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 不旋转计算主成分
- en: 'In this subsection, we learn how to extract the principal components from scratch.
    To better illustrate the extraction process, we’ll visualize our component vectors.
    Of course, vectors are easier to plot when they are two-dimensional. Thus, we
    start by revisiting our customer `measurements` dataset whose principal components
    are 2D. As a reminder, we’ve computed the following outputs for this data:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将学习如何从头开始提取主成分。为了更好地说明提取过程，我们将可视化我们的成分向量。当然，当向量是二维的时候更容易绘制。因此，我们首先回顾我们的客户`measurements`数据集，其主成分是二维的。作为提醒，我们已经为这些数据计算了以下输出：
- en: '`centralized_data`—A centralized version of the `measurements` dataset. The
    mean of `centralized_data` is `[0 0]`.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`centralized_data`—`measurements`数据集的归一化版本。`centralized_data`的平均值是`[0 0]`。'
- en: '`first_pc`—The first principal component of the `measurements` dataset. It
    is a two-element array.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first_pc`—`measurements`数据集的第一个主成分。它是一个二维数组。'
- en: 'As we’ve discussed, `first_pc` is a unit vector that points in the first principal
    direction. That direction maximizes the dispersion of the data. Earlier, we discovered
    the first principal direction by rotating our 2D dataset. The goal of that rotation
    was to either maximize the x-axis variance or minimize the y-axis variance. Previously,
    we computed axis variances using vector dot-product operations. However, we can
    measure axis variance more efficiently using matrix multiplication. More importantly,
    by storing all our variances in a matrix, we can extract our components without
    rotation. Let’s consider the following:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，`first_pc` 是一个指向第一个主方向的单位向量。这个方向最大化了数据的分散度。之前，我们通过旋转我们的二维数据集来发现第一个主方向。旋转的目标是最大化
    x 轴的方差或最小化 y 轴的方差。之前，我们使用向量点积运算来计算轴方差。然而，我们可以通过矩阵乘法更有效地测量轴方差。更重要的是，通过将所有方差存储在矩阵中，我们可以在不旋转的情况下提取我们的成分。让我们考虑以下内容：
- en: We’ve already shown that the variance of an `axis` array equals `axis @ axis
    / axis.size` (see listing 14.8).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经展示了 `axis` 数组的方差等于 `axis @ axis / axis.size`（参见列表 14.8）。
- en: Thus, the variance of axis `i` in `centered_data` equals `centered_data[i] @
    centered_data[i] / centered_data.shape[1]`.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，`centered_data` 中轴 `i` 的方差等于 `centered_data[i] @ centered_data[i] / centered_data.shape[1]`。
- en: Consequently, running `centered_data @ centered_data.T / centered_data .shape[1]`
    will produce a matrix `m`, where `m[i][i]` equals the variance of axis `i`.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，运行 `centered_data @ centered_data.T / centered_data .shape[1]` 将产生一个矩阵 `m`，其中
    `m[i][i]` 等于轴 `i` 的方差。
- en: Essentially, we can compute all axis variances in a single matrix operation.
    We just need to multiply our matrix with its transpose while also dividing by
    data size. This produces a new matrix, called the *covariance matrix*. The diagonal
    of a covariance matrix stores the variance along each axis.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们可以在一次矩阵运算中计算出所有轴的方差。我们只需将我们的矩阵与其转置相乘，同时除以数据大小。这会产生一个新的矩阵，称为*协方差矩阵*。协方差矩阵的对角线存储了每个轴上的方差。
- en: 'Note The non-diagonal elements of the covariance matrix have informative properties
    as well: they determine the direction of the linear slope between two axes. In
    `centered_data`, the slope between x and y is positive. Therefore, the non-diagonal
    elements in `centered_data @ centered_data.T` are also positive.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：协方差矩阵的非对角元素也具有信息性：它们决定了两个轴之间线性斜率的方向。在 `centered_data` 中，x 和 y 之间的斜率是正的。因此，`centered_data
    @ centered_data.T` 中的非对角元素也是正的。
- en: Next we compute the covariance matrix of `centered_data` and assign it to variable
    `cov_matrix`. Then we confirm that `cov_matrix[i][i]` equals the variance of the
    *i* th axis for every `i`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算 `centered_data` 的协方差矩阵并将其分配给变量 `cov_matrix`。然后我们确认对于每个 `i`，`cov_matrix[i][i]`
    等于第 *i* 个轴的方差。
- en: Listing 14.40 Computing a covariance matrix
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.40 计算协方差矩阵
- en: '[PRE39]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Rounds because of floating-point errors
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于浮点数错误而四舍五入
- en: 'The covariance matrix and the principal components share a very special (and
    useful) relationship: the normalized product of a covariance matrix and a principal
    component equals that principal component! Thus, normalizing `cov_matrix @ first_pc`
    produces a vector that’s identical to `first_pc`. Let’s illustrate this relationship
    by plotting `first_pc` and the normalized product of `cov_matrix` and `first_pc`
    (figure 14.19).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵和主成分之间存在一个非常特殊（且有用）的关系：协方差矩阵与主成分的归一化乘积等于该主成分！因此，对 `cov_matrix @ first_pc`
    进行归一化会产生一个与 `first_pc` 完全相同的向量。让我们通过绘制 `first_pc` 和 `cov_matrix` 与 `first_pc`
    的归一化乘积（图 14.19）来展示这种关系。
- en: '![](../Images/14-19.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-19.png)'
- en: Figure 14.19 A plot of `first_pc` together with the normalized product of `cov_matrix`
    and `first_pc`. The two plotted vectors are identical. The product of the covariance
    matrix and the principal component points in the same direction as that principal
    component.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.19 `first_pc` 与 `cov_matrix` 和 `first_pc` 的归一化乘积的绘图。两个绘制的向量是相同的。协方差矩阵与主成分的乘积指向与该主成分相同的方向。
- en: Note When taking the product of a matrix and a vector, we treat the vector as
    a single-column table. Thus, a vector with `x` elements is treated as a matrix
    with `x` rows and one column. Once the vector is reconfigured as a matrix, standard
    matrix multiplication is carried out. That multiplication produces another single-column
    matrix, which is equivalent to a vector. Consequently, the product of matrix `M`
    and vector `v` is equal to `np.array([row @ v for row in M])`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当计算矩阵和向量的乘积时，我们将向量视为一个单列表。因此，具有`x`个元素的向量被视为一个有`x`行和一列的矩阵。一旦向量被重新配置为矩阵，就执行标准的矩阵乘法。这种乘法产生另一个单列矩阵，这相当于一个向量。因此，矩阵`M`和向量`v`的乘积等于`np.array([row
    @ v for row in M])`。
- en: Listing 14.41 Exposing the relationship between `cov_matrix` and `first_pc`
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.41 揭示`cov_matrix`和`first_pc`之间的关系
- en: '[PRE40]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ This helper function plots a 2D vector as a line segment stretching from the
    origin.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个辅助函数将二维向量绘制为从原点延伸的线段。
- en: 'The two plotted vectors are identical! The matrix-vector product of `cov_matrix`
    and `first_pc` points in the same direction as `first_pc`. Thus, by definition,
    `first_pc` is an *eigenvector* of `cov_matrix`. An eigenvector of a matrix satisfies
    the following special property: the product of the matrix and the eigenvector
    points in the same direction as the eigenvector. The direction will not shift,
    no matter how many times we take the product. So, `cov_matrix @ product_vector`
    points in the same direction as `product_ vector`, and the angle between the vectors
    equals zero. Let’s confirm.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的两个向量是相同的！`cov_matrix`和`first_pc`的矩阵-向量乘积指向与`first_pc`相同的方向。因此，根据定义，`first_pc`是`cov_matrix`的*特征向量*。矩阵的特征向量满足以下特殊性质：矩阵和特征向量的乘积指向与特征向量相同的方向。方向不会改变，无论我们进行多少次乘法。所以，`cov_matrix
    @ product_vector`指向与`product_vector`相同的方向，向量之间的角度等于零。让我们来确认。
- en: Listing 14.42 Computing the angle between eigenvector products
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.42 计算特征向量乘积之间的角度
- en: '[PRE41]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Both vectors are unit vectors. As discussed in section 13, the dot product
    of two unit vectors equals the cosine of their angle.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 两个向量都是单位向量。在第13节中讨论过，两个单位向量的点积等于它们之间角度的余弦值。
- en: ❷ Taking the arccosine of the cosine similarity returns the angle between vectors.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对余弦相似度的反余弦返回向量之间的角度。
- en: The product of a matrix and its eigenvector maintains the eigenvector’s direction.
    However, in most cases, it alters the eigenvector’s magnitude. For example, `first_pc`
    is an eigenvector with a magnitude of 1\. Multiplying `first_pc` by the covariance
    matrix will increase that magnitude x-fold. Let’s print the actual shift in magnitude
    by running `norm(cov_matrix @ first_pc)`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵与其特征向量的乘积保持特征向量的方向。然而，在大多数情况下，它会改变特征向量的幅度。例如，`first_pc`是一个幅度为1的特征向量。将`first_pc`与协方差矩阵相乘将使该幅度增加x倍。让我们通过运行`norm(cov_matrix
    @ first_pc)`来打印实际幅度变化。
- en: Listing 14.43 Measuring the shift in magnitude
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.43 测量幅度变化
- en: '[PRE42]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Multiplication has stretched `first_pc` by 541.8 units along the first principal
    direction. Thus, `cov_matrix @ first_pc` equals `541.8 * first_pc`. Given any
    matrix `m` and its eigenvector `eigen_vec`, the product of `m` and `eigen_vec`
    always equals `v * eigen_ vec`, where `v` is a numeric value formally called the
    *eigenvalue*. Our `first_pc` eigenvector has an eigenvalue of approximately 541\.
    This value may seem familiar because we have seen it before: early in this section,
    we printed the maximized x-axis variance, which was approximately 541\. Thus,
    our eigenvalue equals the variance along the first principal direction. We can
    confirm by calling `(centered_data @ first_pc).var()`.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法将`first_pc`在第一主方向上拉伸了541.8个单位。因此，`cov_matrix @ first_pc`等于`541.8 * first_pc`。对于任何矩阵`m`及其特征向量`eigen_vec`，`m`和`eigen_vec`的乘积始终等于`v
    * eigen_vec`，其中`v`是一个数值，正式称为*特征值*。我们的`first_pc`特征向量的特征值约为541。这个值可能看起来很熟悉，因为我们之前见过它：在本节的开头，我们打印了最大化的x轴方差，其值约为541。因此，我们的特征值等于第一主方向上的方差。我们可以通过调用`(centered_data
    @ first_pc).var()`来确认。
- en: Listing 14.44 Comparing an eigenvalue to the variance
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.44 比较特征值与方差
- en: '[PRE43]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s recap our observations:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾我们的观察：
- en: The first principal component is an eigenvector of the covariance matrix.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一主成分是协方差矩阵的特征向量。
- en: The associated eigenvalue equals the variance along the first principal direction.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关联的特征值等于第一主方向上的方差。
- en: 'These observations are not coincidental. Mathematicians have proven the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察并非巧合。数学家已经证明了以下内容：
- en: The principal components of a dataset are equal to the normalized eigenvectors
    of the dataset’s covariance matrix.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的主成分等于数据集协方差矩阵的规范化特征向量。
- en: The variance along a principal direction is equal to the eigenvalue of the associated
    principal component.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主方向上的方差等于相关主成分的特征值。
- en: 'Consequently, to uncover the first principal component, it is sufficient to
    do the following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了揭示第一个主成分，只需执行以下操作：
- en: Compute the covariance matrix.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵。
- en: Find the eigenvector of the matrix with the largest eigenvalue. That eigenvector
    corresponds to the direction with the highest variance coverage.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到具有最大特征值的矩阵的特征向量。该特征向量对应于具有最高方差覆盖率的方向。
- en: We can extract the eigenvector with the largest eigenvalue using a straightforward
    algorithm called *power iteration*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用称为 *幂迭代法* 的简单算法提取具有最大特征值的特征向量。
- en: Key terminology
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 关键术语
- en: '*Covariance matrix*—`m @ m.T / m.shape[1]`, where `m` is a matrix with a mean
    of zero. The diagonal of the covariance matrix equals the variances along each
    axis of `m`.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*协方差矩阵*——`m @ m.T / m.shape[1]`，其中 `m` 是具有零均值的矩阵。协方差矩阵的对角线等于 `m` 每个轴上的方差。'
- en: '*Eigenvector*—A special type of vector associated with a matrix. If `m` is
    a matrix with eigenvector `eigen_vec`, then `m @ eigen_vec` points in the same
    direction as `eigen_vec`. Also, if `m` is a covariance matrix, `eigen_vec` is
    a principal component.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征向量*——与矩阵相关联的特殊类型的向量。如果 `m` 是具有特征向量 `eigen_vec` 的矩阵，那么 `m @ eigen_vec` 将指向与
    `eigen_vec` 相同的方向。此外，如果 `m` 是协方差矩阵，则 `eigen_vec` 是主成分。'
- en: '*Eigenvalue*—A numeric value associated with an eigenvector. If `m` is a matrix
    with eigenvector `eigen_vec`, then `m @ eigen_vec` stretches the eigenvector by
    `eigenvalue` units. Therefore, the eigenvalue equals `norm(m @ eigen_vec) / norm(eigen_vec)`.
    The eigenvalue of a principal component equals the variance covered by that component.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征值*——与特征向量相关联的数值。如果 `m` 是具有特征向量 `eigen_vec` 的矩阵，那么 `m @ eigen_vec` 将特征向量拉伸
    `eigenvalue` 个单位。因此，特征值等于 `norm(m @ eigen_vec) / norm(eigen_vec)`。主成分的特征值等于该成分覆盖的方差。'
- en: 14.4.1 Extracting eigenvectors using power iteration
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.1 使用幂迭代法提取特征向量
- en: Our goal is to obtain the eigenvectors of `cov_matrix`. The procedure for doing
    this is simple. We start by generating a random unit vector, `random_vector`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是获得 `cov_matrix` 的特征向量。执行此操作的步骤很简单。我们首先生成一个随机单位向量，`random_vector`。
- en: Listing 14.45 Generating a random unit vector
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.45 生成随机单位向量
- en: '[PRE44]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Next, we compute `cov_matrix @ random_vector`. This matrix-vector product both
    rotates and stretches our random vector. We normalize the new vector so that its
    magnitude is comparable to `random_vector` and then plot both the new vector and
    the random vector (figure 14.20). Our expectation is that the two vectors will
    point in different directions.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算 `cov_matrix @ random_vector`。这个矩阵-向量乘积既旋转又拉伸我们的随机向量。我们规范化新向量，使其大小与
    `random_vector` 相当，然后绘制新向量和随机向量（图14.20）。我们期望这两个向量将指向不同的方向。
- en: '![](../Images/14-20.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-20.png)'
- en: Figure 14.20 A plot of `random_vector` together with the normalized product
    of `cov_matrix` and `random_vector`. The two plotted vectors point in different
    directions.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.20 `random_vector` 与规范化后的 `cov_matrix` 和 `random_vector` 的乘积的绘图。两个绘制的向量指向不同的方向。
- en: Listing 14.46 Taking the product of `cov_matrix` and `random_vector`
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.46 计算 `cov_matrix` 和 `random_vector` 的乘积
- en: '[PRE45]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Our two vectors have nothing in common. Let’s see what happens when we repeat
    the previous step by running `cov_matrix @ product_vector`. Next, we normalize
    and plot this additional vector along with the previously plotted `product_vector`
    (figure 14.21).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两个向量没有任何共同点。让我们看看当我们通过运行 `cov_matrix @ product_vector` 重复前面的步骤会发生什么。接下来，我们规范化并绘制这个额外的向量，以及之前绘制的
    `product_vector`（图14.21）。
- en: Listing 14.47 Taking the product of `cov_matrix` and `product_vector`
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.47 计算 `cov_matrix` 和 `product_vector` 的乘积
- en: '[PRE46]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](../Images/14-21.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-21.png)'
- en: Figure 14.21 A plot of `product_vector` together with the normalized product
    of `cov_matrix` and `product_vector`. The two plotted vectors are identical. We’ve
    thus discovered an eigenvector of the covariance matrix.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.21 `product_vector` 与规范化后的 `cov_matrix` 和 `product_vector` 的乘积的绘图。两个绘制的向量完全相同。因此，我们已经发现了协方差矩阵的一个特征向量。
- en: 'Our product vectors point in an identical direction! Therefore, `product_vector`
    is an eigenvector of `cov_matrix`. Basically, we’ve carried out a *power iteration*,
    which is a simple algorithm for eigenvector detection. We were lucky in our usage
    of the algorithm: a single matrix multiplication was enough to uncover the eigenvector.
    More commonly, a few additional iterations are required.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的产品向量指向相同的方向！因此，`product_vector` 是 `cov_matrix` 的特征向量。基本上，我们进行了一次 *幂迭代*，这是一种简单的特征向量检测算法。我们在算法的使用上很幸运：一次矩阵乘法就足以揭示特征向量。更常见的情况是，需要额外的几次迭代。
- en: 'Power iteration works as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 幂迭代的工作原理如下：
- en: Generate a random unit vector.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机单位向量。
- en: Multiply the vector by our matrix, and normalize the result. Our unit vector
    is rotated.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量乘以我们的矩阵，并对结果进行归一化。我们的单位向量被旋转了。
- en: Iteratively repeat the previous step until the unit vector gets “stuck”—it won’t
    rotate anymore. By definition, it is now an eigenvector.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行前面的步骤，直到单位向量“卡住”——它不再旋转。根据定义，它现在是一个特征向量。
- en: The power iteration is guaranteed to converge onto an eigenvector (if one exists).
    Generally, 10 iterations are more than sufficient to achieve convergence. The
    resulting eigenvector has the largest possible eigenvalue relative to the other
    eigenvectors of the matrix.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 幂迭代保证收敛到一个特征向量（如果存在的话）。一般来说，10 次迭代就足够实现收敛。得到的特征向量相对于矩阵的其他特征向量具有可能的最大特征值。
- en: Note Some matrices have eigenvectors with negative eigenvalues. In such circumstances,
    the power iteration returns the eigenvector with the largest absolute eigenvalue.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：一些矩阵具有具有负特征值的特征向量。在这种情况下，幂迭代返回具有最大绝对特征值的特征向量。
- en: Let’s define a `power_iteration` function that takes a matrix as input. It returns
    an eigenvector and an eigenvalue as output. We test the function by running `power_
    iteration(cov_matrix)`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个 `power_iteration` 函数，它接受一个矩阵作为输入。它返回一个特征向量和特征值作为输出。我们通过运行 `power_ iteration(cov_matrix)`
    来测试这个函数。
- en: Listing 14.48 Implementing the power iteration algorithm
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.48 实现幂迭代算法
- en: '[PRE47]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `power_iteration` function has extracted an eigenvector with an eigenvalue
    of approximately 541\. This corresponds to the variance along the first principal
    axis. Hence, our eigenvector equals the first principal component.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '`power_iteration` 函数提取了一个特征值为约 541 的特征向量。这对应于第一个主轴的方差。因此，我们的特征向量等于第一个主成分。'
- en: Note You may have noticed that the extracted eigenvector in figure 14.21 stretches
    toward positive values in the plot. Meanwhile, the first principal component in
    figure 14.19 stretches toward negative values. As stated earlier, the principal
    component `pc` can be used interchangeably with `-pc` during PCA execution—projection
    onto principal directions will not be erroneously affected. The only noticeable
    effect will be a difference in reflection over the projected axes, as seen in
    figure 14.13.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能已经注意到，图 14.21 中提取的特征向量在图中向正值方向拉伸。同时，图 14.19 中的第一个主成分向负值方向拉伸。如前所述，在 PCA
    执行期间，主成分 `pc` 可以与 `-pc` 互换使用——投影到主方向不会产生错误。唯一明显的影响将是反射在投影轴上的差异，如图 14.13 所示。
- en: Our function returns a single eigenvector with the largest eigenvalue. Consequently,
    `power_iteration(cov_matrix)` returns the principal component with the largest
    variance coverage. As stated earlier, the second principal component is also an
    eigenvector. Its eigenvalue corresponds to the variance along the second principal
    direction. Thus, that component is an eigenvector with the second largest eigenvalue.
    How do we find it? The solution requires just a few lines of code. That solution
    is not easy to understand without knowing higher mathematics, but we’ll review
    its basic steps.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的函数返回具有最大特征值的单个特征向量。因此，`power_iteration(cov_matrix)` 返回具有最大方差覆盖的主成分。如前所述，第二个主成分也是一个特征向量。它的特征值对应于第二个主方向的方差。因此，该分量是具有第二大的特征值的特征向量。我们如何找到它？这个解决方案只需要几行代码。没有了解高等数学，这个解决方案不容易理解，但我们将回顾其基本步骤。
- en: To extract the second eigenvector, we must eliminate all traces of the first
    eigenvector from `cov_matrix`. This process is known as *matrix deflation*. Once
    a matrix is deflated, its second-largest eigenvalue becomes its largest eigenvalue.
    To deflate `cov_matrix`, we must take the *outer product* of `eigenvector` with
    itself. That outer product is computed by taking the pairwise product of `eigenvector[i]
    * eigenvector[j]` for every possible value of `i` and `j`. The pairwise products
    are stored in a matrix `M`, where `M[i][j] = eigenvector[i] * eigenvector[j]`.
    We can compute the outer product using two nested loops or using NumPy and running
    `np.outer(eigenvector, eigenvector)`.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取第二个特征向量，我们必须从 `cov_matrix` 中消除第一个特征向量的所有痕迹。这个过程被称为 *矩阵降维*。一旦矩阵被降维，其第二大特征值就变成了其最大的特征值。为了降维
    `cov_matrix`，我们必须计算 `eigenvector` 与其自身的 *外积*。这个外积是通过计算 `eigenvector[i] * eigenvector[j]`
    的成对乘积来计算的，对于 `i` 和 `j` 的所有可能值。成对乘积存储在一个矩阵 `M` 中，其中 `M[i][j] = eigenvector[i] *
    eigenvector[j]`。我们可以使用两个嵌套循环或使用 NumPy 并运行 `np.outer(eigenvector, eigenvector)`
    来计算外积。
- en: Note Generally, the outer product is computed between two vectors `v1` and `v2`.
    That outer product returns a matrix `m` where `m[i][j]` equals `v1[i] * v2[j]`.
    During matrix deflation, both `v1` and `v2` are equal to `eigenvector`.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 通常，外积是在两个向量 `v1` 和 `v2` 之间计算的。那个外积返回一个矩阵 `m`，其中 `m[i][j]` 等于 `v1[i] * v2[j]`。在矩阵降维过程中，`v1`
    和 `v2` 都等于 `eigenvector`。
- en: Listing 14.49 Computing the outer product of an eigenvector with itself
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.49 计算特征向量与其自身的外积
- en: '[PRE48]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Given the outer product, we can deflate `cov_matrix` by running `cov_matrix
    - eigenvalue * outer_product`. That basic operation produces a matrix whose primary
    eigenvector is equal to the second principal component.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 给定外积，我们可以通过运行 `cov_matrix - eigenvalue * outer_product` 来降维 `cov_matrix`。这个基本操作产生一个矩阵，其主特征向量等于第二个主成分。
- en: Listing 14.50 Deflating the covariance matrix
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.50 降维协方差矩阵
- en: '[PRE49]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Running `product_iteration(deflated_matrix)` returns an eigenvector that we’ll
    call `next_eigenvector`. Based on our discussion, we know that the following should
    be true:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `product_iteration(deflated_matrix)` 返回一个我们将称之为 `next_eigenvector` 的特征向量。根据我们的讨论，我们知道以下应该是正确的：
- en: '`next_eigenvector` equals the second principal component.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_eigenvector` 等于第二个主成分。'
- en: Thus, `np.array([eigenvector, next_eigenvector])` equals a matrix of principal
    components that we call `components`.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，`np.array([eigenvector, next_eigenvector])` 等于一个主成分矩阵，我们称之为 `components`。
- en: Executing `components @ centered_data` projects our dataset onto its principal
    directions.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行 `components @ centered_data` 将我们的数据集投影到其主方向上。
- en: Plotting the projections should produce a horizontally positioned, cigar-shaped
    plot similar to the one in figure 14.8 or figure 14.13.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制投影应产生一个水平放置的、类似图 14.8 或图 14.13 中的雪茄形状的图。
- en: Next, we extract `next_eigenvector` and execute the aforementioned projections.
    We then plot the projections to confirm that our assumptions are true (figure
    14.22).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取 `next_eigenvector` 并执行上述投影。然后我们绘制投影以确认我们的假设是正确的（图 14.22）。
- en: Listing 14.51 Extracting the second principal component from the deflated matrix
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.51 从降维矩阵中提取第二个主成分
- en: '[PRE50]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](../Images/14-22.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.22](../Images/14-22.png)'
- en: Figure 14.22 A plot of `centered_data` projected onto its principal components,
    in which the components were computed using power iteration. The plot is identical
    to figure 14.13, which was generated using scikit-learn’s PCA implementation.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.22 将 `centered_data` 投影到其主成分上的图，其中主成分是通过幂迭代法计算的。该图与图 14.13 相同，后者是使用 scikit-learn
    的 PCA 实现生成的。
- en: NumPy matrix deflation computations
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 矩阵降维计算
- en: '`np.outer(eigenvector, eigenvector)`—Computes the outer product of an eigenvector
    with itself. Returns a matrix `m` where `m[i][j]` equals `eigenvector[i] * eigenvector[j]`.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.outer(eigenvector, eigenvector)`—计算一个特征向量的外积。返回一个矩阵 `m`，其中 `m[i][j]` 等于
    `eigenvector[i] * eigenvector[j]`。'
- en: '`matrix -= eigenvalue * np.outer(eigenvector, eigenvector)`—Deflates a matrix
    by removing all traces of the eigenvector with the largest eigenvalue from the
    matrix. Running `power_iteration(matrix)` returns an eigenvector with the next
    largest eigenvalue.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix -= eigenvalue * np.outer(eigenvector, eigenvector)`—通过从矩阵中移除具有最大特征值特征向量的所有痕迹来降维矩阵。运行
    `power_iteration(matrix)` 返回具有下一个最大特征值的特征向量。'
- en: 'We’ve basically developed an algorithm for extracting the top *K* principal
    components of a matrix whose rows all average to zero. Given any such `centered_matrix`,
    the algorithm is executed as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上开发了一个算法，用于提取所有行平均值为零的矩阵的前 *K* 个主成分。给定任何这样的 `centered_matrix`，算法的执行如下：
- en: Compute the covariance matrix of `centered_matrix` by running `centered_ matrix
    @ centered_matrix.T`.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行 `centered_matrix @ centered_matrix.T` 计算中心化矩阵的协方差矩阵。
- en: Run `power_iteration` on the covariance matrix. The function returns an eigenvector
    of the covariance matrix (`eigenvector`), corresponding to the largest possible
    eigenvalue (`eigenvalue`). This eigenvector equals the first principal component.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在协方差矩阵上运行 `power_iteration`。该函数返回协方差矩阵的一个特征向量（`eigenvector`），对应于最大的可能特征值（`eigenvalue`）。这个特征向量等于第一个主成分。
- en: Deflate the matrix by subtracting `eigenvalue * np.outer(eigenvector, eigenvector)`.
    Running `power_iteration` on the deflated matrix extracts the next principal component.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去 `eigenvalue * np.outer(eigenvector, eigenvector)` 来降维矩阵。在降维矩阵上运行 `power_iteration`
    提取下一个主成分。
- en: Repeat the previous step *K* – 2 more times to extract the top *K* principal
    components.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复之前的步骤 *K* – 2 次以提取前 *K* 个主成分。
- en: Let’s implement the algorithm by defining `find_top_principal_components`. The
    function extracts the top *K* principal components from a `centered_matrix` input.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过定义 `find_top_principal_components` 函数来实现该算法。该函数从 `centered_matrix` 输入中提取前
    *K* 个主成分。
- en: Listing 14.52 Extracting the top *K* principal components
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.52 提取前 *K* 个主成分
- en: '[PRE51]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ❶ Extracts the top K principal components from a matrix whose rows all average
    to zero. The value of K is preset to 2.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从一个所有行平均值为零的矩阵中提取前 *K* 个主成分。K 的值预设为 2。
- en: ❷ Principal components are simply the top eigenvectors of the covariance matrix
    (where eigenvector rank is determined by the eigenvalues). To emphasize this point,
    we define a separate function for extracting the top K eigenvectors of any matrix.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 主成分仅仅是协方差矩阵的前几个特征向量（特征向量的秩由特征值确定）。为了强调这一点，我们定义了一个单独的函数来提取任何矩阵的前 *K* 个特征向量。
- en: ❸ Makes a copy of the matrix so we can deflate that copy without modifying the
    original
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 制作矩阵的副本，这样我们就可以在不修改原始矩阵的情况下对副本进行降维
- en: We defined a function for extracting the top *K* principal components of a dataset.
    The components allow us to project the dataset onto its top *K* principal directions.
    These directions maximize data dispersion along *K* axes. The remaining data axes
    can thus be disregarded, shrinking the coordinate column size to *K*. Consequently,
    we can reduce any dataset to *K* dimensions.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个函数来提取数据集的前 *K* 个主成分。这些成分使我们能够将数据集投影到其前 *K* 个主方向上。这些方向在 *K* 个轴上最大化数据分散。因此，可以忽略剩余的数据轴，将坐标列的大小缩减到
    *K*。因此，我们可以将任何数据集降低到 *K* 维。
- en: 'Basically, we’re now able to run PCA from scratch without relying on scikit-learn.
    We can use PCA to reduce an *N*-dimensional dataset to *K* dimensions (where *N*
    is the column count of an input data matrix). To run the algorithm, we must execute
    the following steps:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们现在能够从头开始运行 PCA，而不依赖于 scikit-learn。我们可以使用 PCA 将 *N*-维数据集降低到 *K* 维（其中 *N*
    是输入数据矩阵的列数）。要运行该算法，我们必须执行以下步骤：
- en: Compute the mean along each of the axes in the dataset.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集中每个轴上的均值。
- en: Subtract the mean from every axis, thus centering the dataset at the origin.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个轴上减去均值，从而将数据集中心化在原点。
- en: Extract the top *K* principal components of the centered dataset using the `find_
    top_principal_components` function.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `find_top_principal_components` 函数从中心化数据集中提取前 *K* 个主成分。
- en: Take the matrix product between the principal components and the centered dataset.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将主成分与中心化数据集进行矩阵乘法。
- en: Let’s implement these steps in a single function named `reduce_dimensions`.
    Why not name the function `pca`? Well, the first two steps of PCA require us to
    centralize our data. However, we’ll soon learn that dimension reduction can be
    achieved without centralization. Thus, we pass an optional `centralize_data` parameter
    into our function. We preset the parameter to `True`, guaranteeing that the function
    executes PCA under default conditions.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个名为 `reduce_dimensions` 的函数中实现这些步骤。为什么不叫这个函数 `pca` 呢？嗯，PCA 的前两个步骤要求我们对数据进行中心化。然而，我们很快就会了解到，降维可以在不进行中心化的情况下实现。因此，我们将一个可选的
    `centralize_data` 参数传递到我们的函数中。我们预设该参数为 `True`，以确保函数在默认条件下执行 PCA。
- en: Listing 14.53 Defining a `reduce_dimensions` function
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.53 定义 `reduce_dimensions` 函数
- en: '[PRE52]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ❶ The function takes as input a data matrix whose columns correspond to axes.
    This is consistent with the input orientation of scikit-learn’s fit_transform
    method. The function then reduces the matrix from N columns to k columns.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 函数接受一个数据矩阵作为输入，其列对应于轴。这与 scikit-learn 的 fit_transform 方法的输入方向一致。然后函数将矩阵从 N
    列减少到 k 列。
- en: ❷ Data is transposed so that it remains consistent with the expected input into
    find_principal_components.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据被转置，以保持与 `find_principal_components` 预期输入的一致性。
- en: ❸ Optionally centralizes the data by subtracting its mean so the new mean equals
    0
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 可选地通过减去其均值来集中数据，使新的均值等于 0
- en: Let’s test `reduce_dimensions` by applying it to our previously analyzed `flower_
    measurements` data. We reduce that data to 2D using our custom PCA implementation
    and then visualize the results (figure 14.23). Our plot should be consistent with
    the plot in figure 14.18.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将 `reduce_dimensions` 应用于之前分析的 `flower_measurements` 数据来测试它。我们使用自定义的 PCA
    实现将数据降至 2D，然后可视化结果（图 14.23）。我们的图表应该与图 14.18 中的图表一致。
- en: Listing 14.54 Reducing flower data to 2D using a custom PCA implementation
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.54 使用自定义 PCA 实现将花数据降至 2D
- en: '[PRE53]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](../Images/14-23.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![图片 14-23](../Images/14-23.png)'
- en: Figure 14.23 4D normalized flower measurements reduced to two dimensions using
    a custom PCA implementation. The plot is identical to the PCA output generated
    by scikit-learn.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.23 使用自定义 PCA 实现将 4D 归一化花测量数据降至二维。图表与 scikit-learn 生成的 PCA 输出相同。
- en: 'Our plot perfectly resembles scikit-learn’s PCA output. We have reengineered
    scikit-learn’s implementation, but with one big difference: in our function, centralization
    is optional. This will prove useful! As we’ve discussed, we can’t reliably perform
    centralization on normalized data. Also, our flower dataset has been normalized
    to eliminate unit differences. Consequently, we cannot reliably run PCA on `flower_measurements`.
    One alternative is to bypass centralization by passing `centralize_data=False`
    into `reduce_dimensions`. This, of course, violates many assumptions of the PCA
    algorithm. However, the output could still be useful. What will happen if we reduce
    the dimensions of `flower_measurements` without centralization? Let’s find out
    by setting `centralize_data` to `False` and plotting the results (figure 14.24).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图表完美地类似于 scikit-learn 的 PCA 输出。我们重新设计了 scikit-learn 的实现，但有一个很大的不同：在我们的函数中，集中化是可选的。这将会很有用！正如我们之前讨论的，我们无法可靠地对归一化数据进行集中化。此外，我们的花数据集已经被归一化以消除单位差异。因此，我们无法可靠地运行
    `flower_measurements` 的 PCA。一个替代方案是通过将 `centralize_data=False` 传递给 `reduce_dimensions`
    来绕过集中化。当然，这违反了 PCA 算法的许多假设。然而，输出仍然可能是有用的。如果我们不进行集中化就降低 `flower_measurements` 的维度会发生什么？让我们通过将
    `centralize_data` 设置为 `False` 并绘制结果（图 14.24）来找出答案。
- en: Listing 14.55 Running `reduce_dimensions` without centralization
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.55 在未集中化的情况下运行 `reduce_dimensions`
- en: '[PRE54]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: ❶ As previously stated, the randomness of eigenvector extraction can influence
    2D plot orientation. Here, we seed the algorithm to ensure that the orientation
    aligns with another plot, presented later (figure 14.25).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如前所述，特征向量提取的随机性可以影响 2D 图表的方向。在这里，我们播种算法以确保方向与稍后呈现的另一个图表一致（图 14.25）。
- en: '![](../Images/14-24.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图片 14-24.png]'
- en: Figure 14.24 4D normalized flower measurements reduced to two dimensions without
    centralization. Each plotted flower point is colored based on its species. The
    three species continue to fall into three clusters. However, the plot no longer
    resembles our PCA output.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.24 4D 归一化花测量数据在未集中化的情况下降至二维。每个绘制的花点根据其物种着色。三种物种继续落入三个簇中。然而，图表不再类似于我们的 PCA
    输出。
- en: In the output, the three species of flowers continue to separate into three
    clusters. Furthermore, *Setosa* remains spatially distinct from other species.
    However, there are changes in the plot. *Setosa* forms a tighter cluster than
    previously observed in the PCA results. This begs the question, is our latest
    plot as comprehensive as our PCA output? In other words, does it continue to represent
    97% of the total data variance? We can check by measuring the variance of `dim_reduced_data`
    and dividing it by the total variance of `flower_measurements`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，三种花物种继续分离成三个簇。此外，*Setosa* 仍然在空间上与其他物种明显不同。然而，图表中有所变化。*Setosa* 形成的簇比之前在
    PCA 结果中观察到的更紧密。这引发了一个问题，我们的最新图表是否像我们的 PCA 输出那样全面？换句话说，它是否继续代表 97% 的总数据方差？我们可以通过测量
    `dim_reduced_data` 的方差并将其除以 `flower_measurements` 的总方差来检查。
- en: Listing 14.56 Checking the variance of data reduced without centralization
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.56 检查未中心化降低数据时的方差
- en: '[PRE55]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Our 2D variance coverage is maintained. Even though its value fluctuates slightly,
    the coverage remains at approximately 97%. We thus can reduce dimensionality without
    relying on centralization. However, centralization remains a defining feature
    of PCA, so our modified technique needs a different name. Officially, as mentioned
    earlier, the technique is called *singular value decomposition* (SVD).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持了二维方差覆盖。尽管其值略有波动，但覆盖率保持在约97%。因此，我们可以降低维度而不依赖于中心化。然而，中心化仍然是 PCA 的一个定义特征，因此我们的改进技术需要一个不同的名称。官方名称，如前所述，该技术称为
    *奇异值分解*（SVD）。
- en: Warning Unlike PCA, SVD is not guaranteed to maximize the variance for each
    axis in the reduced output. However, in most real-world circumstances, SVD is
    able to dimensionally reduce data to a very practical degree.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 与 PCA 不同，SVD 并不保证在降低输出中的每个轴上最大化方差。然而，在大多数实际情况下，SVD 能够将数据降维到非常实用的程度。
- en: The mathematical properties of SVD are complicated and lie beyond the scope
    of this book. Nonetheless, computer scientists can use these properties to execute
    SVD very efficiently, and these optimizations have been incorporated into scikit-learn.
    In the next subsection, we utilize scikit-learn’s optimized SVD implementation.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 的数学性质很复杂，超出了本书的范围。尽管如此，计算机科学家可以使用这些性质来非常高效地执行 SVD，并且这些优化已经集成到 scikit-learn
    中。在下一个小节中，我们将利用 scikit-learn 优化的 SVD 实现。
- en: 14.5 Efficient dimension reduction using SVD and scikit-learn
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 使用 SVD 和 scikit-learn 进行高效降维
- en: Scikit-learn contains a dimension-reduction class called `TruncatedSVD` that
    is designed for optimal execution of SVD. Let’s import `TruncatedSVD` from `sklearn.decomposition`.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包含一个名为 `TruncatedSVD` 的降维类，该类旨在优化 SVD 的执行。让我们从 `sklearn.decomposition`
    中导入 `TruncatedSVD`。
- en: Listing 14.57 Importing `TruncatedSVD` from scikit-learn
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.57 从 scikit-learn 导入 `TruncatedSVD`
- en: '[PRE56]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Applying `TruncatedSVD` to our `flower_measurements` data is easy. First, we
    need to run `TruncatedSVD(n_components=2)` to create an `svd_object` object capable
    of reducing data to two dimensions. Then, we can execute SVD by running `svd_object.fit_
    predict(flower_measurements)`. That method call returns a two-dimensional `svd_
    transformed_data` matrix. Next, we apply `TruncatedSVD` and plot our results (figure
    14.25). The plot should resemble our custom SVD output, shown in figure 14.24.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `TruncatedSVD` 应用于我们的 `flower_measurements` 数据非常简单。首先，我们需要运行 `TruncatedSVD(n_components=2)`
    来创建一个 `svd_object` 对象，该对象能够将数据降至二维。然后，我们可以通过运行 `svd_object.fit_(flower_measurements)`
    来执行 SVD。该方法调用返回一个二维的 `svd_transformed_data` 矩阵。接下来，我们应用 `TruncatedSVD` 并绘制我们的结果（图14.25）。该图表应该类似于我们的自定义
    SVD 输出，如图14.24所示。
- en: '![](../Images/14-25.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-25.png)'
- en: Figure 14.25 4D normalized flower measurements reduced to two dimensions using
    scikit-learn’s SVD implementation. The output is identical to figure 14.24, which
    was generated using our custom SVD implementation.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.25 使用 scikit-learn 的 SVD 实现将 4D 标准化花朵测量数据降至二维。输出与图14.24相同，后者是使用我们的自定义 SVD
    实现生成的。
- en: Note Unlike the `PCA` class, scikit-learn’s `TruncatedSVD` implementation requires
    an `n_components` input. The default value for that parameter is preset to 2.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 与 `PCA` 类不同，scikit-learn 的 `TruncatedSVD` 实现需要一个 `n_components` 输入。该参数的默认值预设为
    2。
- en: Listing 14.58 Running SVD using scikit-learn
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.58 使用 scikit-learn 运行 SVD
- en: '[PRE57]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Not surprisingly, scikit-learn’s results are identical to our custom SVD implementation.
    Scikit-learn’s algorithm is faster and more memory efficient, but its output does
    not diverge from ours.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，scikit-learn 的结果与我们的自定义 SVD 实现完全相同。scikit-learn 的算法更快，更节省内存，但其输出并未偏离我们的结果。
- en: Note The outputs will not diverge when we reduce the number of dimensions. However,
    as the dimension count goes up, our implementation will become less precise because
    minor errors will creep into our eigenvector calculations. These minor errors
    will be magnified with each computed eigenvector. Scikit-learn, on the other hand,
    uses mathematical tricks to limit these errors.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 当我们降低维度数量时，输出不会偏离。然而，随着维度数量的增加，我们的实现将变得不够精确，因为微小的错误会渗入我们的特征向量计算中。这些微小错误会随着每个计算的特征向量而放大。另一方面，scikit-learn
    使用数学技巧来限制这些错误。
- en: We can further verify the overlap between outputs by comparing variance coverage.
    Our `svd_object` has an `explained_variance_ratio_` attribute that holds an array
    of the fractional variances covered by each reduced dimension. Summing over `100
    * explained_variance_ratio_` should return the percent of total variance covered
    in the 2D plot. Based on our analysis, we expect that output to approximate 97.29%.
    Let’s confirm.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过比较方差覆盖率来进一步验证输出之间的重叠。我们的 `svd_object` 有一个 `explained_variance_ratio_`
    属性，它包含一个数组，表示每个降维维度覆盖的分数方差。对 `100 * explained_variance_ratio_` 进行求和应该返回在 2D 图中覆盖的总方差的百分比。根据我们的分析，我们预计这个输出将接近
    97.29%。让我们来确认一下。
- en: Listing 14.59 Extracting variance from scikit-learn’s SVD output
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.59 从 scikit-learn 的 SVD 输出中提取方差
- en: '[PRE58]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: ❶ The attribute is a NumPy array containing fractional coverage for each axis.
    Multiplying by 100 converts these fractions into percentages.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该属性是一个 NumPy 数组，包含每个轴的分数覆盖率。乘以 100 将这些分数转换为百分比。
- en: ❷ Each *i*th element of the array corresponds to the variance coverage of the
    *i*th axis.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数组的第 *i* 个元素对应于第 *i* 个轴的方差覆盖率。
- en: Common scikit-learn SVD methods
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 scikit-learn SVD 方法
- en: '`svd_object = TruncatedSVD(n_components=K)`—Creates an SVD object capable of
    reducing input data to *K* dimensions.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_object = TruncatedSVD(n_components=K)`—创建一个可以将输入数据降至 *K* 维度的 SVD 对象。'
- en: '`svd_tranformed_data = svd_object.fit_transform(data)`—Executes SVD on inputted
    data using an initialized `TruncatedSVD` object. The `fit_transform` method assumes
    that the columns of the `data` matrix correspond to spatial axes. The dimensionally
    reduced results are stored in the `svd_transformed_data` matrix.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_tranformed_data = svd_object.fit_transform(data)`—使用初始化的 `TruncatedSVD`
    对象对输入数据进行 SVD 计算。`fit_transform` 方法假定 `data` 矩阵的列对应于空间轴。降维后的结果存储在 `svd_transformed_data`
    矩阵中。'
- en: '`svd_object.explained_variance_ratio_`—Returns the fractional variance coverage
    associated with each dimensionally reduced axis of a fitted `TruncatedSVD` object.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_object.explained_variance_ratio_`—返回与拟合的 `TruncatedSVD` 对象的每个降维轴相关联的分数方差覆盖率。'
- en: Scikit-learn’s optimized SVD implementation can decrease data from tens of thousands
    of dimensions to only a few hundred or a few dozen. The shrunken data can more
    efficiently be stored, transferred, and processed by predictive algorithms. Many
    real-world data tasks require SVD for data shrinkage before analysis. Applications
    range from image compression, to audio-noise removal, to natural language processing.
    NLP, in particular, is dependent on the algorithm due to the bloated nature of
    text data. As we discussed in the previous section, real-world documents form
    very large matrices whose column counts are way too high. We cannot multiply such
    matrices efficiently and therefore cannot compute text similarities. Fortunately,
    SVD makes these document matrices much more manageable. SVD allows us to shrink
    text-matrix column counts while retaining most of the variance, so we can compute
    large-scale text similarities in a timely manner. These text similarities can
    then be used to cluster the input documents.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的优化 SVD 实现可以将数据从数万个维度减少到只有几百或几十个维度。压缩后的数据可以更有效地被存储、传输和处理，以便由预测算法使用。许多实际数据任务在分析之前需要使用
    SVD 进行数据压缩。应用范围从图像压缩、音频噪声消除到自然语言处理。特别是，NLP 由于文本数据的膨胀性质，依赖于该算法。正如我们在上一节中讨论的，现实世界的文档形成非常大的矩阵，其列数非常高。我们无法有效地乘以这样的矩阵，因此无法计算文本相似度。幸运的是，SVD
    使这些文档矩阵变得更容易管理。SVD 允许我们在保留大部分方差的同时减少文本矩阵的列数，这样我们就可以及时地计算大规模文本相似度。然后，这些文本相似度可以用来对输入文档进行聚类。
- en: In the subsequent section, we finally analyze large document datasets. We learn
    how to clean and cluster these datasets while also visualizing the clustering
    output. Our use of SVD will prove absolutely fundamental to this analysis.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们最终分析大型文档数据集。我们学习如何清理和聚类这些数据集，同时可视化聚类输出。我们使用 SVD 的方法将证明对这个分析至关重要。
- en: Summary
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Reducing dataset dimensionality can simplify certain tasks, like clustering.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低数据集的维度可以简化某些任务，如聚类。
- en: We can reduce a 2D dataset to one dimension by rotating the data about the origin
    until the data points lie close to the x-axis. Doing so maximizes data spread
    along the x-axis, thus allowing us to delete the y-axis. However, the rotation
    requires us to first centralize the dataset so its mean coordinates lie at the
    origin.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过围绕原点旋转数据，直到数据点靠近x轴，将二维数据集减少到一维。这样做最大化了沿x轴的数据分布，从而允许我们删除y轴。然而，这种旋转需要我们首先将数据集中心化，使其平均坐标位于原点。
- en: Rotating the data toward the x-axis is analogous to rotating the x-axis toward
    the *first principal direction*. The first principal direction is the linear direction
    in which data variance is maximized. The *second principal direction* is perpendicular
    to the first. In a 2D dataset, that direction represents the remaining variance
    not covered by the first principal direction.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据旋转到x轴方向相当于将x轴旋转到**第一主方向**。第一主方向是数据方差最大化的线性方向。**第二主方向**与第一主方向垂直。在二维数据集中，这个方向代表第一主方向未覆盖的剩余方差。
- en: Dimensional reduction can be carried out using *principal component analysis*
    (PCA). PCA uncovers a dataset’s principal directions and represents them as using
    unit vectors called *principal components*. The product of the centralized data
    matrix and the principal components swaps the data’s standard axes with the principal
    directions. This axis swap is called a *projection*. When we project our data
    onto the principal directions, we maximize data dispersion along some axes and
    minimize it along others. Axes with minimized dispersion can be deleted.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用**主成分分析**（PCA）进行降维。PCA揭示了数据集的主方向，并使用称为**主成分**的单位向量来表示它们。中心化数据矩阵与主成分的乘积交换了数据的标准轴与主方向。这种轴交换称为**投影**。当我们将数据投影到主方向上时，我们最大化了某些轴上的数据分散，并最小化了其他轴上的数据分散。分散度降低的轴可以被删除。
- en: 'We can extract principal components by computing a *covariance matrix* : the
    matrix product of a centered dataset with itself, divided by the dataset size.
    The diagonal of that matrix represents the axes’ variance values.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过计算一个**协方差矩阵**来提取主成分：将中心化数据集与其自身进行矩阵乘积，然后除以数据集的大小。该矩阵的对角线表示轴的方差值。
- en: Principal components are the *eigenvectors* of the covariance variance. Thus,
    by definition, the normalized product of the covariance matrix and each principal
    component is equal to that principal component.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分是协方差方差的**特征向量**。因此，根据定义，协方差矩阵与每个主成分的归一化乘积等于该主成分。
- en: We can extract the top eigenvector of a matrix using the *power iteration* algorithm.
    Power iteration consists of repeated multiplication and normalization of a vector
    by a matrix. Applying power iteration to a covariance matrix returns the first
    principal component.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用**幂迭代**算法提取矩阵的最高特征向量。幂迭代包括通过矩阵重复乘法和归一化一个向量。将幂迭代应用于协方差矩阵返回第一个主成分。
- en: By applying *matrix deflation*, we can eliminate all traces of an eigenvector.
    Deflating the covariance matrix and reapplying the power iteration returns the
    second principal component. Repeating that process iteratively returns all principal
    components.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过应用**矩阵降维**，我们可以消除特征向量的所有痕迹。对协方差矩阵进行降维并重新应用幂迭代返回第二个主成分。重复此过程会迭代地返回所有主成分。
- en: PCA is sensitive to units of measurement. Normalizing our input data reduces
    that sensitivity. However, as a consequence, the normalized data will be proximate
    to its mean. This is a problem because PCA requires us to subtract the mean from
    each axis value to centralize the data. Subtracting proximate values leads to
    floating-points errors.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA对测量单位敏感。对输入数据进行归一化可以减少这种敏感性。然而，作为后果，归一化后的数据将接近其平均值。这是一个问题，因为PCA要求我们从每个轴值中减去平均值以中心化数据。减去接近的值会导致浮点误差。
- en: We can avoid these floating-point errors by refusing to centralize our data
    before running dimension reduction. The resulting output captures data variance
    to a sufficiently meaningful degree. This modified technique is called *singular
    value decomposition* (SVD).
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过在运行降维之前拒绝中心化我们的数据来避免这些浮点误差。结果输出以足够有意义的程度捕捉数据方差。这种修改后的技术称为**奇异值分解**（SVD）。
