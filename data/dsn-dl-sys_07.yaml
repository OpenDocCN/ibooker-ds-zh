- en: 7 Model serving in practice
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 实践中的模型服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building a sample predictor with the model service approach
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型服务方法构建示例预测器
- en: Building a sample service with TorchServe and the model server approach
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TorchServe和模型服务器方法构建示例服务
- en: Touring popular open source model serving libraries and systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索流行的开源模型服务库和系统
- en: Explaining the production model release process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释生产模型发布流程
- en: Discussing postproduction model monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论生产后模型监控
- en: In the previous chapter, we discussed the concept of model serving, as well
    as user scenarios and design patterns. In this chapter, we will focus on the actual
    implementation of these concepts in production.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了模型服务的概念，以及用户场景和设计模式。在本章中，我们将重点关注这些概念在生产环境中的实际实现。
- en: As we’ve said, one of the challenges to implementing model serving nowadays
    is that we have too many possible ways of doing it. In addition to multiple black-box
    solutions, there are many options for customizing and building all or part of
    it from scratch. We think the best way to teach you how to choose the right approach
    is with concrete examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所言，实现模型服务当前的一个挑战是我们有太多的可能方法。除了多种黑盒解决方案外，还有许多选项可以自定义和从头开始构建全部或部分内容。我们认为，用具体的例子教你如何选择正确的方法是最好的方式。
- en: 'In this chapter, we implement two sample services to demo two of the most commonly
    used model serving approaches: one uses a self-build model serving container,
    which demonstrates the model service approach (section 7.1), and the other uses
    TorchServe (a model server for the PyTorch model), which demonstrates the model
    server approach (section 7.2). Both of these serve the intent classification model
    trained in chapter 3\. Once you work through the examples, we provide (in section
    7.3) a tour of the most popular open source model serving tools to help you understand
    their features, best uses, and other factors important to your decision on which
    to use. In the rest of the chapter, we will focus on the model serving operation
    and monitoring, including shipping models to production and monitoring the model
    performance.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了两个示例服务来演示两种最常用的模型服务方法：一个使用自建的模型服务容器，这演示了模型服务方法（第7.1节），另一个使用TorchServe（PyTorch模型的模型服务器），这演示了模型服务器方法（第7.2节）。这两个都服务于在第3章中训练的意图分类模型。一旦你完成了这些示例，我们将在第7.3节中提供对最流行的开源模型服务工具的浏览，以帮助你了解它们的功能、最佳用途和其他对你选择使用哪些工具重要的因素。在本章的其余部分，我们将重点关注模型服务的运营和监控，包括将模型部署到生产环境和监控模型性能。
- en: By reading this chapter, you will not only have a concrete understanding of
    different model serving designs but also have the acumen to choose the right approach
    for your own situation. More importantly, this chapter will present a holistic
    view of the model serving field, not just *building* model serving but also *operating*
    and *monitoring* it after the model serving system is built.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读本章，你不仅会对不同的模型服务设计有具体的理解，而且还会具备选择适合自己情况正确方法的洞察力。更重要的是，本章将提供一个关于模型服务领域的整体视角，不仅包括*构建*模型服务，还包括模型服务系统构建后的*运营*和*监控*。
- en: Note In this chapter, the terms *model serving*, *model inference*, and *model
    prediction* are used interchangeably. They all refer to executing a model with
    given data points.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，术语*模型服务*、*模型推理*和*模型预测*是互换使用的。它们都指的是使用给定的数据点执行模型。
- en: 7.1 A model service sample
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 模型服务示例
- en: In this section, we will show you the first sample prediction service. This
    service takes the model service approach (section 6.2.2), and it can be used for
    both single-model (section 6.3.1) and multitenant applications (section 6.3.2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示第一个示例预测服务。此服务采用模型服务方法（第6.2.2节），可用于单模型（第6.3.1节）和多租户应用程序（第6.3.2节）。
- en: 'This sample service follows the single model application design (section 6.3.1),
    which has a frontend API component and a backend predictor. We also made some
    enhancements in the predictor so it can support multiple intent classification
    models. We will tour this sample service by following these steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例服务遵循单一模型应用程序设计（第6.3.1节），它包含一个前端API组件和一个后端预测器。我们还对预测器进行了一些增强，使其能够支持多个意图分类模型。我们将通过以下步骤来浏览此示例服务：
- en: Running the sample prediction service locally
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地运行示例预测服务
- en: Discussing the system design
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 讨论系统设计
- en: 'Looking at the implementation details of its subcomponents: frontend service
    and backend predictor'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看其子组件的实现细节：前端服务和后端预测器
- en: 7.1.1 Play with the service
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 与服务互动
- en: Listing 7.1 shows how to run the sample prediction service on your local machine.
    The following scripts first run the backend predictor and then the frontend service.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1展示了如何在您的本地机器上运行示例预测服务。以下脚本首先运行后端预测器，然后运行前端服务。
- en: Note Setting up a prediction service is a bit tedious; we need to run the metadata
    and artifactory store service and prepare the models. To demonstrate the idea
    clearly, listing 7.1 highlights the main setup steps. To make model serving work
    on your local machine, please complete the lab (section A.2) in appendix A and
    then use the code `./scripts/lab-004-model-serving.sh` `{run_id}` `{document}`
    to send the model prediction requests.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：设置预测服务稍微有些繁琐；我们需要运行元数据和工件存储服务并准备模型。为了清楚地展示这个想法，列表7.1突出了主要的设置步骤。为了在您的本地机器上使模型服务工作，请完成附录A中的实验室（部分A.2），然后使用代码`./scripts/lab-004-model-serving.sh`
    `{run_id}` `{document}` 发送模型预测请求。
- en: Listing 7.1 Starting a prediction service
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 启动预测服务
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Builds the predictor Docker image
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建预测器Docker镜像
- en: ❷ Runs the predictor service container
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行预测器服务容器
- en: ❸ Builds the prediction service image
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建预测服务镜像
- en: ❹ Runs the prediction service container
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 运行预测服务容器
- en: 'Once the service starts, you can send prediction requests to it; the service
    will load the intent classification model trained in chapter 3, run the model
    prediction with the given text, and return the prediction results. In the following
    example, a text string “merry christmas” is sent to the service and is predicted
    to the “joy” category:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务启动，您可以向它发送预测请求；服务将加载第3章中训练的意图分类模型，使用给定的文本运行模型预测，并返回预测结果。在以下示例中，文本字符串“merry
    christmas”被发送到服务，并被预测为“joy”类别：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Specifies the model ID to the response
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将模型ID指定给响应
- en: ❷ Prediction payload
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测负载
- en: ❸ Prediction response, the predicted category
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预测响应，预测的类别
- en: 7.1.2 Service design
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 服务设计
- en: 'This sample service consists of a frontend interface component and a backend
    predictor. The frontend component does three things: hosts the public prediction
    API, downloads model files from the metadata store to a shared disk volume, and
    forwards the prediction request to the backend predictor. The backend predictor
    is a self-built predictor container that responds to load intent classification
    models and executes these models to serve prediction requests.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例服务由前端接口组件和后端预测器组成。前端组件执行三项任务：托管公共预测API、从元数据存储下载模型文件到共享磁盘卷，并将预测请求转发到后端预测器。后端预测器是一个自建的预测容器，它响应负载意图分类模型并执行这些模型以服务预测请求。
- en: 'This prediction service has two external dependencies: the metadata store service
    and a shared disk volume. The metadata store keeps all the information about a
    model, such as the model algorithm name, the model version, and the model URL,
    which points to the cloud storage of real model files. The shared volume enables
    model file sharing between the frontend service and the backend predictor. You
    can see an end-to-end overview of the model serving process in figure 7.1.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此预测服务有两个外部依赖：元数据存储服务和共享磁盘卷。元数据存储保存有关模型的所有信息，例如模型算法名称、模型版本和模型URL，该URL指向真实模型文件的云存储。共享卷允许前端服务和后端预测器之间共享模型文件。您可以在图7.1中看到模型服务过程的端到端概述。
- en: '![](../Images/07-01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-01.png)'
- en: Figure 7.1 A system overview and model serving end-to-end workflow
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 系统概述和模型服务端到端工作流程
- en: 'Going through the system design of the sample model serving service shown in
    figure 7.1, you can see it takes six steps to complete a prediction request. Let’s
    go through each step numbered in the figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看图7.1所示的示例模型服务系统的设计，您可以看到完成一个预测请求需要六个步骤。让我们逐一查看图中编号的每个步骤：
- en: The user sends a prediction request to the prediction service (frontend component)
    with a specified model ID and a text string—namely, `document`. The model ID is
    a unique identifier produced by the training service to identify each model it
    produces.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户使用指定的模型ID和文本字符串（即“document”）向预测服务（前端组件）发送预测请求。模型ID是训练服务生成的唯一标识符，用于标识它产生的每个模型。
- en: The frontend service fetches the model metadata from the metadata store by searching
    the model ID. For each successful model training, the training service will save
    the model files to cloud storage and also save the model metadata (model ID, model
    version, name, and URL) to the metadata store; this is why we can find the model
    information in the metadata store.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前端服务通过搜索模型ID从元数据存储中检索模型元数据。对于每次成功的模型训练，训练服务将模型文件保存到云存储，并将模型元数据（模型ID、模型版本、名称和URL）保存到元数据存储中；这就是为什么我们可以在元数据存储中找到模型信息。
- en: If the model file is not already downloaded, the frontend component will download
    it to the shared disk volume.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型文件尚未下载，前端组件将将其下载到共享磁盘卷中。
- en: The frontend component forwards the inference request to the backend predictor.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前端组件将推理请求转发给后端预测器。
- en: The backend predictor loads the intent classification model to memory by reading
    model files from the shared disk volume.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后端预测器通过从共享磁盘卷中读取模型文件来将意图分类模型加载到内存中。
- en: The backend predictor executes the model to make a prediction on the given text
    string and returns the prediction result to the frontend component.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后端预测器执行模型对给定的文本字符串进行预测，并将预测结果返回给前端组件。
- en: 7.1.3 The frontend service
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 前端服务
- en: 'Now, let’s focus on the frontend service. The frontend service has three main
    components: a web interface, a predictor management, and a predictor backend client
    (`CustomGrpcPredictorBackend`). These components respond to the host public gRPC
    model serving the API and manage the backend predictors’ connection and communication.
    Figure 7.2 shows the internal structure of the frontend service and its inner
    workflow when receiving a prediction request.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于前端服务。前端服务有三个主要组件：一个Web界面、一个预测器管理和一个预测器后端客户端（`CustomGrpcPredictorBackend`）。这些组件响应主机公共gRPC模型服务API，并管理后端预测器的连接和通信。图7.2显示了前端服务的内部结构和接收预测请求时的内部工作流程。
- en: '![](../Images/07-02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-02.png)'
- en: Figure 7.2 The frontend service design and the model serving workflow
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 前端服务设计及模型服务流程
- en: 'Let’s consider the intent prediction scenario in the model serving workflow
    described in figure 7.2, applying the six steps we just reviewed:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑图7.2中描述的模型服务流程中的意图预测场景，应用我们刚刚回顾的六个步骤：
- en: T he user sends an intent prediction request with model ID A to the web interface.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户向Web界面发送带有模型ID A的意图预测请求。
- en: The web interface calls the predictor connection manager to serve this request.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Web界面调用预测器连接管理器来处理此请求。
- en: The predictor connection manager queries the metadata store to get model metadata
    by searching model IDs that equal A; the returned model metadata contains the
    model algorithm type and model file URL.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测器连接管理器通过搜索等于A的模型ID来查询元数据存储，以获取模型元数据；返回的模型元数据包含模型算法类型和模型文件URL。
- en: Based on the model algorithm type, the predictor manager picks the right predictor
    backend client to handle the request. In this case, it chooses `CustomGrpcPredictorBackend`
    because we are demoing a self-built model serving container for intent classification.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据模型算法类型，预测器管理器选择合适的预测器后端客户端来处理请求。在这种情况下，它选择了`CustomGrpcPredictorBackend`，因为我们正在演示一个用于意图分类的自建模型服务容器。
- en: The `CustomGrpcPredictorBackend` client first checks the existence of the model
    file in the shared model file disk for model A. If the model hasn’t been downloaded
    before, it uses the model URL (from model metadata) to download model files from
    cloud storage to the shared file disk.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CustomGrpcPredictorBackend`客户端首先检查共享模型文件磁盘中的模型A文件是否存在。如果模型之前尚未下载，它将使用模型URL（来自模型元数据）从云存储下载模型文件到共享文件磁盘。'
- en: The `CustomGrpcPredictorBackend` client then calls the model predictor that
    is preregistered with this backend client in the service configuration file. In
    this example, the `CustomGrpcPredictorBackend` will call our self-built predictor,
    the intent predictor, which will be discussed in section 7.1.4.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`CustomGrpcPredictorBackend`客户端调用在服务配置文件中预先注册的模型预测器。在这个例子中，`CustomGrpcPredictorBackend`将调用我们自建的预测器，即意图预测器，这将在7.1.4节中讨论。
- en: Now that we have reviewed the system design and workflow, let’s consider the
    actual code implementation of the main components, including the web interface
    (prediction API), predictor connection manager, predictor backend clients, and
    intent predictor.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经审查了系统设计和工作流程，让我们考虑主要组件的实际代码实现，包括网络界面（预测 API）、预测器连接管理器、预测器后端客户端和意图预测器。
- en: Frontend service model serving code walkthrough
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前端服务模型服务代码遍历
- en: The following code listing highlights the core implementation of the prediction
    workflow mentioned in figure 7.2\. You can also find the full implementation at
    `src/main/` `java/org/orca3/miniAutoML/prediction/PredictionService.java.`
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码列表突出了图 7.2 中提到的预测工作流程的核心实现。您也可以在 `src/main/java/org/orca3/miniAutoML/prediction/PredictionService.java`
    中找到完整的实现。
- en: Listing 7.2 Frontend service prediction workflow
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 前端服务预测工作流程
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Obtains the required model ID
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取所需的模型 ID
- en: ❷ Fetches the model metadata from the metadata store
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从元数据存储中获取模型元数据
- en: ❸ Chooses the backend predictor based on the model algorithm type
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据模型算法类型选择后端预测器
- en: ❹ Downloads the model file
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 下载模型文件
- en: ❺ Calls the backend predictor to run model inference
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 调用后端预测器以运行模型推理
- en: Prediction API
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 API
- en: The frontend service offers only one API—`Predict`—for issuing a prediction
    request. The request has two parameters, `runId` and `document`. The `runId` not
    only is used for referencing a model training run in the training service (chapter
    3), but it also can be used as the model ID to reference a model. The `document`
    is the text on which the customer wants to run predictions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前端服务仅提供了一个 API——`Predict`——用于发出预测请求。请求有两个参数，`runId` 和 `document`。`runId` 不仅用于在训练服务（第
    3 章）中引用模型训练运行，还可以用作模型 ID 来引用模型。`document` 是客户想要运行预测的文本。
- en: By using the `Predict` API, users can specify an intent model (with `runId`)
    to predict the intent of a given text string (`document`). The following listing
    shows the gRPC contract of the `Predict` API (`grpc-contract/src/main/proto/prediction_service
    .proto`).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `Predict` API，用户可以指定一个意图模型（带有 `runId`）来预测给定文本字符串（`document`）的意图。以下列表显示了
    `Predict` API 的 gRPC 合同（`grpc-contract/src/main/proto/prediction_service.proto`）。
- en: Listing 7.3 Prediction service gRPC interface
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 预测服务 gRPC 接口
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Predictor connection manager
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器连接管理器
- en: One important role of the frontend service is routing prediction requests. Given
    a prediction request, the frontend service needs to find the right backend predictor
    based on the model algorithm type required in the request. This routing is done
    in the `PredictorConnectionManager`. In our design, the mapping of model algorithms
    and predictors is predefined in environment properties. When the service starts,
    `PredictorConnectionManager` will read the mapping, so the service knows which
    backend predictor to use for which model algorithm type.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前端服务的一个重要角色是路由预测请求。给定一个预测请求，前端服务需要根据请求中所需的模型算法类型找到正确的后端预测器。这种路由是在 `PredictorConnectionManager`
    中完成的。在我们的设计中，模型算法和预测器的映射是在环境属性中预定义的。当服务启动时，`PredictorConnectionManager` 将读取映射，以便服务知道为哪种模型算法类型使用哪个后端预测器。
- en: Although we are only demoing our self-built intent classification predictor
    in this example, `PredictorConnectionManager` can support any other type of backend
    predictors. Let’s look at the following listing (`config/config-docker-docker.properties`)
    to see how the model algorithm and predictor mapping are configured.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个示例中我们只演示了我们自建的意图分类预测器，但 `PredictorConnectionManager` 可以支持任何其他类型的后端预测器。让我们查看以下列表（`config/config-docker-docker.properties`），以了解模型算法和预测器映射是如何配置的。
- en: Listing 7.4 Model algorithm and predictor mapping configuration
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 模型算法和预测器映射配置
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Maps the intent-classification predictor to the intent-classification algorithm
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将意图分类预测器映射到意图分类算法
- en: Now, let’s review code listing 7.5 to see how the predictor manager reads the
    algorithm and predictor mapping and uses that information to initialize the predictor
    backend client to send prediction requests. The full implementation is located
    at `prediction-service/src/main/java/org/orca3/miniAutoML/prediction/PredictorConnectionManager.java.`
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾代码列表 7.5，以了解预测器管理器如何读取算法和预测器映射，并使用该信息初始化预测器后端客户端以发送预测请求。完整的实现位于 `prediction-service/src/main/java/org/orca3/miniAutoML/prediction/PredictorConnectionManager.java`。
- en: Listing 7.5 Predictor manager load algorithm and predictor mapping
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 预测器管理器加载算法和预测器映射
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The algorithm for the predictor backend mapping
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预测器后端映射的算法
- en: ❷ The model metadata cache; the key string is the model ID.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型元数据缓存；键字符串是模型ID。
- en: ❸ Reads the algorithm and predictor mapping from the configuration
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从配置中读取算法和预测器映射
- en: ❹ Creates the predictor backend client and saves it in the memory
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建预测器后端客户端并将其保存在内存中
- en: In listing 7.5, we see the `PredictorConnectionManager` class offers the `registerPredictor`
    function to register predictors. It first reads the algorithm and predictor mapping
    information from the properties, and then it creates the actual predictor backend
    client—`CustomGrpcPredictorBackend`—to communicate with the backend intent predictor
    container.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.5中，我们看到`PredictorConnectionManager`类提供了`registerPredictor`函数来注册预测器。它首先从属性中读取算法和预测器映射信息，然后创建实际的预测器后端客户端——`CustomGrpcPredictorBackend`——以与后端意图预测器容器通信。
- en: You may also notice `PredictorConnectionManager` class has several caches, such
    as the model metadata cache (`artifactCache`) and the model backend predictor
    clients (`clients)`. These caches can greatly improve the model serving efficiency.
    For example, the model metadata cache (`artifactCache`) can reduce the serving
    request response time by avoiding calling the metadata store service for the model
    that has already been downloaded.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到`PredictorConnectionManager`类有几个缓存，例如模型元数据缓存（`artifactCache`）和模型后端预测客户端（`clients`）。这些缓存可以大大提高模型服务的效率。例如，模型元数据缓存（`artifactCache`）可以通过避免为已下载的模型调用元数据存储服务来减少服务请求响应时间。
- en: Predictor backend clients
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器后端客户端
- en: Predictor clients are the objects that the frontend service uses to talk to
    different predictor backends. By design, each type of predictor backend supports
    its own kind of model, and it has its own client for communication, which is created
    and stored in `PredictorConnectionManager`. Every predictor backend client inherits
    an interface `PredictorBackend`, as in the following listing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器客户端是前端服务用来与不同预测器后端通信的对象。按照设计，每种类型的预测器后端支持其自己的模型类型，并且它有自己的客户端用于通信，这些客户端在`PredictorConnectionManager`中被创建和存储。每个预测器后端客户端继承了一个接口`PredictorBackend`，如下所示。
- en: Listing 7.6 Predictor backend interface
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 预测器后端接口
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The three methods, `downloadMode`, `predict``,` and `registerModel``,` are self-explanatory.
    Each client implements these methods to download models and send prediction requests
    to its registered backend service. The parameter `GetArtifactResponse` is a model’s
    metadata object that is fetched from the metadata store.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个方法，`downloadModel`、`predict`和`registerModel`是显而易见的。每个客户端实现这些方法以下载模型并向其注册的后端服务发送预测请求。参数`GetArtifactResponse`是从元数据存储中检索到的模型元数据对象。
- en: 'In this (intent predictor) example, the predictor backend client is `CustomGrpcPredictorBackend`.
    You can find the detailed implementation in `prediction-service/ src/main/java/org/orca3/miniAutoML/prediction/CustomGrpcPredictorBackend.java`.
    The following code snippet shows how this client sends prediction requests to
    the self-built intent predictor container by using gRPC protocol:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个（意图预测器）示例中，预测器后端客户端是`CustomGrpcPredictorBackend`。你可以在`prediction-service/src/main/java/org/orca3/miniAutoML/prediction/CustomGrpcPredictorBackend.java`中找到详细的实现。以下代码片段展示了这个客户端如何通过使用gRPC协议向自建的意图预测器容器发送预测请求：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Text input for the model
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型的文本输入
- en: ❷ Model ID
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型ID
- en: 7.1.4 Intent classification predictor
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 意图分类预测器
- en: 'We have seen the frontend service and its internal routing logic, so now let’s
    look at the last piece of this sample prediction service: the backend predictor.
    To show you a complete deep learning use case, we implement a predictor container
    to execute the intent classification models trained in chapter 3.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了前端服务和其内部路由逻辑，现在让我们看看这个样本预测服务的最后一部分：后端预测器。为了展示一个完整的深度学习用例，我们实现了一个预测器容器来执行第3章训练的意图分类模型。
- en: We can see this self-built intent classification predictor as an independent
    microservice, which can serve multiple intent models simultaneously. It has a
    gRPC web interface and a model manager. The model manager is the heart of the
    predictor; it does multiple things, including loading model files, initializing
    the model, caching the model in memory, and executing the model with user input.
    Figure 7.3 shows the predictor’s design graph and the prediction workflow within
    the predictor.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个自建的意图分类预测器视为一个独立的微服务，它可以同时服务于多个意图模型。它有一个gRPC网络接口和一个模型管理器。模型管理器是预测器的核心；它执行多项任务，包括加载模型文件、初始化模型、在内存中缓存模型以及使用用户输入执行模型。图7.3显示了预测器的设计图和预测器内部的预测工作流程。
- en: 'Let’s use an intent prediction request for model A to consider the workflow
    in figure 7.3\. It runs in the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用模型A的意图预测请求来考虑图7.3中的工作流程。它按照以下步骤运行：
- en: The predictor client in the frontend service calls the predictor’s web gRPC
    interface to run an intent prediction with model A.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前端服务中的预测器客户端调用预测器的web gRPC接口，以模型A运行意图预测。
- en: The model manager is invoked for the request.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型管理器被调用以处理请求。
- en: The model manager loads the model files of model A from the shared disk volume,
    initializes the model, and puts it into the model cache. The model file should
    be placed at the shared disk volume by the frontend service already.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型管理器从共享磁盘卷加载模型A的模型文件，初始化模型，并将其放入模型缓存。模型文件应由前端服务提前放置在共享磁盘卷上。
- en: The model manager executes model A with the transformer’s help to preprocess
    and postprocess the input and output data.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型管理器在transformer的帮助下执行模型A，以预处理和后处理输入和输出数据。
- en: The predicted result is returned.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测结果被返回。
- en: '![](../Images/07-03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-03.png)'
- en: Figure 7.3 The backend intent predictor design and prediction workflow
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 后端意图预测器设计和预测工作流程
- en: Next, let’s look at the actual implementation of the components mentioned in
    the workflow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看工作流程中提到的组件的实际实现。
- en: Prediction API
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 预测API
- en: Intent predictor has one API—`PredictorPredict` (see code listing 7.7). It accepts
    two parameters, `runId` and `document`. The `runId` is the model ID, and the `document`
    is a text string. You can find the full gRPC contract at `grpc-contract/src/main/proto/`
    `prediction_service.proto.`
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 意图预测器有一个API——`PredictorPredict`（见代码列表7.7）。它接受两个参数，`runId`和`document`。`runId`是模型ID，而`document`是一个文本字符串。你可以在`grpc-contract/src/main/proto/`
    `prediction_service.proto`找到完整的gRPC合约。
- en: Listing 7.7 Intent predictor gRPC interface
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 意图预测器gRPC接口
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You may notice the predictor API is the same as the frontend API (code listing
    7.2); this is for simplicity. But in real-world applications, they are normally
    different, mostly because they are designed for different purposes. The predictor’s
    predict API is designed in favor of model execution, whereas the frontend predict
    API is designed in favor of the customer’s and business’s requirements.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到预测器API与前端API（代码列表7.2）相同；这是为了简化。但在现实世界的应用中，它们通常是不同的，主要是因为它们是为不同的目的设计的。预测器的预测API是为了模型执行而设计的，而前端预测API是为了满足客户和业务的需求而设计的。
- en: Model files
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型文件
- en: 'Each intent classification model produced in our model training service (chapter
    3) has three files. The `manifest.json` file contains both model metadata and
    dataset labels; the predictor needs this information to translate the model prediction
    result from an integer to a meaningful text string. The `model.pth` is the model’s
    learned parameters; the predictor will read these network parameters to set up
    the model’s neural network for model serving. The `vocab.pth` is the vocabulary
    file used in model training, which is also necessary for serving because we need
    it to transform user input (string) to model input (decimal number). Let’s review
    the sample intent model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型训练服务（第3章）中产生的每个意图分类模型都有三个文件。`manifest.json`文件包含模型元数据和数据集标签；预测器需要这些信息来将模型预测结果从整数转换为有意义的文本字符串。`model.pth`是模型的学到的参数；预测器将读取这些网络参数来设置模型的服务器中的神经网络。`vocab.pth`是模型训练中使用的词汇文件，对于服务也是必要的，因为我们需要它将用户输入（字符串）转换为模型输入（十进制数）。让我们回顾一下样本意图模型：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Model metadata and dataset labels
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型元数据和数据集标签
- en: ❷ Model weights file
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型权重文件
- en: ❸ Vocabulary file
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 词汇文件
- en: ❹ Model metadata
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 模型元数据
- en: ❺ Dataset labels
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 数据集标签
- en: 'When saving a PyTorch model, there are two choices: serialize the entire model
    or serialize only the learned parameters. The first option serializes the entire
    model object, including its classes and directory structure, whereas the second
    option only saves the learnable parameters of the model network.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当保存 PyTorch 模型时，有两种选择：序列化整个模型或仅序列化已学习参数。第一种选项序列化整个模型对象，包括其类和目录结构，而第二种选项仅保存模型网络的可学习参数。
- en: 'From Matthew Inkawhich’s article “PyTorch: Saving and Loading Models” ([http://mng.bz/zm9B](http://mng.bz/zm9B)),
    the PyTorch team recommends only saving the model’s learned parameters (a model’s
    `state_dict`). If we save the entire model, the serialized data is bound to the
    specific classes and the exact directory structure used when the model is saved.
    The model class itself is not saved; rather, the file containing the class is
    saved. Consequently, during loading time, the serialized model code can break
    in various ways when used in other projects or after refactors.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '来自 Matthew Inkawhich 的文章“PyTorch: 保存和加载模型”([http://mng.bz/zm9B](http://mng.bz/zm9B))，PyTorch
    团队建议只保存模型的已学习参数（模型的 `state_dict`）。如果我们保存整个模型，序列化的数据将绑定到保存模型时使用的特定类和精确的目录结构。模型类本身并未保存；而是保存了包含类的文件。因此，在加载时，序列化的模型代码在用于其他项目或重构后可能会以各种方式出错。'
- en: 'For this reason, we only save the model `state_dict` (learned parameters) as
    the model file after training; in this example, it is the `model.pth` file. We
    use the following code to save it: `torch.save(model.state_dict(),` `model_local_path)`.
    As a result, the predictor needs to know the model’s neural network architecture
    (see code listing 7.8) to load the model file because the model file is just `state_dict`—the
    model network’s parameters.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在训练后只保存模型 `state_dict`（已学习参数）作为模型文件；在这个例子中，是 `model.pth` 文件。我们使用以下代码来保存它：`torch.save(model.state_dict(),
    ` `model_local_path`)`。结果，预测器需要知道模型的神经网络架构（参见代码列表 7.8），以便加载模型文件，因为模型文件只是 `state_dict`—模型网络的参数。
- en: Listing 7.8 (`predictor/predict.py`) shows the model architecture that we use
    to load the model file—`model.pth` (parameters only)—in the predictor. The model
    execution code in serving is derived from the model training code. If you compare
    the model definition in the following listing with the `TextClassificationModel`
    class in our training code (`training-code/text-classification/train.py`), you
    will find they are identical. This is because model serving is essentially a model
    training run.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 (`predictor/predict.py`) 展示了我们用于在预测器中加载模型文件—`model.pth`（仅参数）—的模型架构。服务中的模型执行代码源自模型训练代码。如果你将以下列表中的模型定义与我们的训练代码中的
    `TextClassificationModel` 类（`training-code/text-classification/train.py`）进行比较，你会发现它们是相同的。这是因为模型服务本质上是一个模型训练运行。
- en: Listing 7.8 The model’s neural network (architecture)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 模型的神经网络（架构）
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Defines model architecture
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义模型架构
- en: You might wonder whether the training code and the model serving code are now
    combined. When the training code changes, it seems the model serving code in the
    predictor also needs to be adjusted. This is only partially true; the context
    tends to dictate how model serving is affected by changes to the model training
    algorithm. The following are some nuances of that relationship.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道训练代码和模型服务代码现在是否合并了。当训练代码发生变化时，似乎预测器中的模型服务代码也需要进行调整。这只有部分正确；上下文往往决定了模型服务如何受到模型训练算法变化的影响。以下是一些关于这种关系的细微差别。
- en: First, the training code and the serving code will only need to sync on the
    neural network architecture and input/output data schema. Other model training
    changes, such as training strategy, hyperparameter tuning, dataset splitting,
    and enhancements, will not affect serving because they result in model weights
    and bias files. Second, model versioning should be introduced when neural network
    architecture changes in training. In practice, every model training or retraining
    assigns a new model version to the output model. So the problem to address is
    how to serve different versions of a model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，训练代码和服务代码只需要同步神经网络架构和输入/输出数据模式。其他模型训练更改，如训练策略、超参数调整、数据集拆分和增强，不会影响服务，因为它们导致模型权重和偏差文件。其次，当训练中的神经网络架构发生变化时，应引入模型版本控制。在实践中，每次模型训练或重新训练都会为输出模型分配一个新的模型版本。因此，需要解决的问题是如何服务不同版本的模型。
- en: This sample service does not handle model version management. However, in section
    7.5 and chapter 8, we will discuss metadata management for the model version in
    depth. We just describe the rough idea here.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例服务不处理模型版本管理。然而，在第7.5节和第8章中，我们将深入讨论模型版本的元数据管理。这里我们只是描述一个大致的想法。
- en: If you are using a similar model service approach with a customized predictor
    backend, you need to prepare multiple versions of the predictor backend to match
    the models that are trained with different neural network architectures. When
    releasing a model, the versions of the training code, serving code, and model
    file need to be related as part of the model metadata and saved in the metadata
    store. So, at the serving time, the prediction service (frontend service) can
    search the metadata store to determine which predictor version it should route
    a request to for the given model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是具有定制预测后端的类似模型服务方法，你需要准备多个预测后端版本，以匹配使用不同神经网络架构训练的模型。在发布模型时，训练代码、部署代码和模型文件的版本需要作为模型元数据的一部分，并保存在元数据存储中。因此，在部署时，预测服务（前端服务）可以搜索元数据存储以确定它应该将请求路由到哪个预测器版本，以针对给定的模型。
- en: If you are using a model server approach, serving models with different versions
    becomes a lot easier because this approach breaks the dependency between the serving
    code (model execution code) and training code. You can see a concrete example
    in section 7.2.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是模型服务器方法，使用不同版本的模型进行部署变得容易得多，因为这种方法打破了部署代码（模型执行代码）和训练代码之间的依赖关系。你可以在第7.2节中看到一个具体的例子。
- en: 'Note As we mentioned in chapter 6 (section 6.1.3), model training and serving
    both utilize the same machine learning algorithm but in different execution modes:
    learning and evaluation. However, we would like to clarify this concept once more.
    Understanding the relationship between training code, serving code, and model
    files is the foundation of a serving system design.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：正如我们在第6章（第6.1.3节）中提到的，模型训练和部署都使用相同的机器学习算法，但在不同的执行模式下：学习和评估。然而，我们还想再次澄清这个概念。理解训练代码、部署代码和模型文件之间的关系是部署系统设计的基础。
- en: Model manager
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理器
- en: The model manager is the key component of this intent predictor. It hosts a
    memory model cache, loads the model file, and executes the model. The following
    listing (`predictor/predict.py`) shows the core code of the model manager.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理器是这个意图预测器的关键组件。它托管一个内存模型缓存，加载模型文件并执行模型。以下列表（`predictor/predict.py`）显示了模型管理器的核心代码。
- en: Listing 7.9 Intent predictor model manager
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9意图预测器模型管理器
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Hosts the model in memory
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在内存中托管模型
- en: ❷ Caches model graph; dependencies and classes in memory
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 缓存模型图；内存中的依赖和类
- en: ❸ Runs the model to obtain prediction results
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行模型以获取预测结果
- en: Intent predictor prediction request workflow
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 意图预测器预测请求工作流程
- en: You’ve met the main components of the intent predictor, so let’s see an end-to-end
    workflow inside this predictor. First, we expose the prediction API by registering
    `PredictorServicer` to the gRPC server, so the frontend service can talk to the
    predictor remotely. Second, when the frontend service calls the `PredictorPredict`
    API, the model manager will load the model into memory, run the model, and return
    the prediction result. Code listing 7.10 highlights the aforementioned workflow’s
    code implementation. You can find the full implementation at `predictor/predict.py`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经遇到了意图预测器的主要组件，那么让我们看看这个预测器内部的一个端到端工作流程。首先，我们通过将`PredictorServicer`注册到gRPC服务器来公开预测API，这样前端服务就可以远程与预测器通信。其次，当前端服务调用`PredictorPredict`
    API时，模型管理器将加载模型到内存中，运行模型并返回预测结果。代码列表7.10突出了上述工作流程的代码实现。你可以在`predictor/predict.py`中找到完整的实现。
- en: Listing 7.10 Intent predictor prediction workflow
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10意图预测器预测工作流程
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Starts the gRPC server
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启动gRPC服务器
- en: ❷ Registers the model serving logic to the public API
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型部署逻辑注册到公共API
- en: ❸ Makes the prediction
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 进行预测
- en: 7.1.5 Model eviction
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5 模型回收
- en: The sample code did not cover model eviction—that is, evicting infrequently
    used model files from the prediction service’s memory space. In the design, for
    every prediction request, the prediction service will query and download the request
    model from the metadata store and then read and initialize the model from the
    local disk to memory. For some models, these operations are time-consuming.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码没有涵盖模型驱逐——即从预测服务的内存空间中驱逐不常使用的模型文件。在设计上，对于每个预测请求，预测服务将查询并从元数据存储中下载请求模型，然后从本地磁盘读取并初始化模型到内存中。对于某些模型，这些操作是耗时的。
- en: To reduce the latency for each model prediction request, our design caches model
    graphs in the model manager component (in memory) to avoid model loading a used
    model. But imagine that we could continue training new intent classification models
    and running predictions on them. These newly produced models will keep loading
    into the model manager’s model cache in memory. Eventually, the predictor will
    run out of memory.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少每个模型预测请求的延迟，我们的设计在模型管理器组件（内存中）缓存模型图，以避免加载已使用的模型。但想象一下，如果我们能够继续训练新的意图分类模型并在它们上运行预测。这些新产生的模型将不断加载到模型管理器的模型缓存中。最终，预测器将耗尽内存。
- en: To address such problems, the model manager needs to be upgraded to include
    a model eviction feature. For example, we could introduce the LRU (least recently
    used) algorithm to rebuild the model manager’s model cache. With the help of the
    LRU, we can keep only the recently visited model in the model cache and evict
    the least visited models when the currently loaded model exceeds the memory threshold.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，模型管理器需要升级以包括模型驱逐功能。例如，我们可以引入 LRU（最近最少使用）算法来重建模型管理器的模型缓存。借助 LRU，我们可以在模型缓存中保留最近访问的模型，并在当前加载的模型超过内存阈值时驱逐最少访问的模型。
- en: 7.2 TorchServe model server sample
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 TorchServe 模型服务器示例
- en: In this section, we will show you an example of building a prediction service
    with the model server approach. More specifically, we use the TorchServe backend
    (a model serving tool built for the PyTorch model) to replace the self-built predictor
    discussed in the previous section (7.1.4).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示使用模型服务器方法构建预测服务的一个示例。更具体地说，我们使用 TorchServe 后端（为 PyTorch 模型构建的模型服务工具）来替换上一节（7.1.4）中讨论的自建预测器。
- en: To make a fair comparison to the model service approach in section 7.1, we develop
    this model server approach example by reusing the frontend service shown in the
    previous section. More precisely, we add only another predictor backend and still
    use the frontend service, gRPC API, and intent classification models to demo the
    same end-to-end prediction workflow.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与 7.1 节中模型服务方法的比较公平，我们通过重用上一节中显示的前端服务来开发这个模型服务器方法示例。更确切地说，我们只添加了另一个预测器后端，并仍然使用前端服务、gRPC
    API 和意图分类模型来演示相同的端到端预测工作流程。
- en: There is one big difference between the intent predictor in section 7.1.4 and
    the TorchServe predictor (model server approach). The same predictor can serve
    any PyTorch model, regardless of its prediction algorithm.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 7.1.4 节中的意图预测器和 TorchServe 预测器（模型服务器方法）之间有一个很大的区别。相同的预测器可以为任何 PyTorch 模型提供服务，无论其预测算法如何。
- en: 7.2.1 Playing with the service
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 与服务互动
- en: Because this model server sample is developed on top of the previous sample
    service, we interact with the prediction service in the same way. The only difference
    is we launch a TorchServe backend (container) instead of launching a self-built
    intent predictor container. Code listing 7.11 shows only the key steps to starting
    the service and sending intent prediction requests. To run the lab locally, please
    complete the lab in appendix A (section A.2), and refer to the `scripts/lab-006-model-serving-torchserve.sh`
    file ([http://mng.bz/0yEN](http://mng.bz/0yEN)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个模型服务器示例是在之前的示例服务之上开发的，因此我们以相同的方式与预测服务互动。唯一的区别是我们启动一个 TorchServe 后端（容器）而不是启动一个自建的意图预测器容器。代码列表
    7.11 仅显示了启动服务和发送意图预测请求的关键步骤。要在本地运行实验室，请完成附录 A 中的实验室（A.2 节），并参考 `scripts/lab-006-model-serving-torchserve.sh`
    文件 ([http://mng.bz/0yEN](http://mng.bz/0yEN))。
- en: Listing 7.11 Starting the prediction service and making a prediction call
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 启动预测服务并进行预测调用
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Mounts local dir to the TorchServe container
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将本地目录挂载到 TorchServe 容器
- en: ❷ Starts TorchServe
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 启动 TorchServe
- en: ❸ Sets TorchServe to load the model from /models dir
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将 TorchServe 设置为从 /models 目录加载模型
- en: ❹ Sets local model dir for the prediction service to download the model
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为预测服务设置本地模型目录以下载模型
- en: 7.2.2 Service design
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 服务设计
- en: This sample service follows the same system design in figure 7.1; the only difference
    is the predictor backend becomes the TorchServe server. See figure 7.4 for the
    updated system design.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例服务遵循图7.1中的相同系统设计；唯一的区别是预测后端变成了TorchServe服务器。参见图7.4中的更新后的系统设计。
- en: '![](../Images/07-04.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-04.png)'
- en: Figure 7.4 The system overview and model serving end-to-end workflow
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 系统概述和端到端模型服务工作流程
- en: From figure 7.4, we see the model serving workflow remains the same as the model
    service sample in figure 7.1\. The user calls the prediction service’s frontend
    API to send model serving requests; the frontend service then downloads the model
    files and forwards the prediction request to the TorchServe backend.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从图7.4中，我们看到模型服务工作流程与图7.1中的模型服务示例保持相同。用户调用预测服务的 frontend API 发送模型服务请求；然后前端服务下载模型文件并将预测请求转发到TorchServe后端。
- en: 7.2.3 The frontend service
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 前端服务
- en: In section 7.1.3, we established that the frontend service can support different
    predictor backends by registering predictors in the predictor connection manager.
    When a prediction request comes in, the predictor connection manager will route
    the request to the proper predictor backend by checking the model algorithm type
    of the request.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在7.1.3节中，我们确定前端服务可以通过在预测连接管理器中注册预测器来支持不同的预测后端。当预测请求到来时，预测连接管理器将通过检查请求的模型算法类型将请求路由到适当的预测后端。
- en: Following the previous design, to support our new TorchServe backend, we add
    a new predictor client (`TorchGrpcPredictorBackend`) to the frontend service to
    represent the TorchServe backend; see figure 7.5 for the updated system design.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面的设计，为了支持我们新的TorchServe后端，我们在前端服务中添加了一个新的预测客户端（`TorchGrpcPredictorBackend`），以表示TorchServe后端；参见图7.5中的更新后的系统设计。
- en: '![](../Images/07-05.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-05.png)'
- en: Figure 7.5 The frontend service design and the model serving workflow
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 前端服务设计和模型服务工作流程
- en: In figure 7.5, two gray boxes are added; they are the TorchServe gRPC predictor
    backend client (`TorchGrpcPredictorBackend`) and the backend TorchServe server.
    `TorchGrpcPredictorBackend` responds by downloading the model files and then sending
    prediction requests to the TorchServe container. The TorchServe backend will be
    chosen by the predictor connection manager in this example because the requested
    model’s metadata (in the metadata store) defines TorchServe as its predictor.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.5中，增加了两个灰色框；它们是TorchServe gRPC预测后端客户端（`TorchGrpcPredictorBackend`）和后端TorchServe服务器。`TorchGrpcPredictorBackend`通过下载模型文件并发送预测请求到TorchServe容器来响应。在本例中，预测连接管理器将选择TorchServe后端，因为请求的模型元数据（在元数据存储中）定义了TorchServe作为其预测器。
- en: 7.2.4 TorchServe backend
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 TorchServe后端
- en: TorchServe is a tool built by the PyTorch team to serve PyTorch models. TorchServe
    runs as a black box, and it provides HTTP and gRPC interfaces for model prediction
    and internal resource management. Figure 7.6 visualizes the workflow for how we
    use TorchServe in this sample.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe是由PyTorch团队构建的一个工具，用于服务PyTorch模型。TorchServe作为一个黑盒运行，并为模型预测和内部资源管理提供HTTP和gRPC接口。图7.6展示了我们在本示例中使用TorchServe的工作流程。
- en: '![](../Images/07-06.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-06.png)'
- en: 'Figure 7.6 The model serving workflow in the TorchServe backend: the TorchServe
    application runs as a black box.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 在TorchServe后端中的模型服务工作流程：TorchServe应用程序作为一个黑盒运行。
- en: In our sample code, we run TorchServe as a Docker container, which is provided
    by the PyTorch team, and then mount a local file directory to the container. This
    file directory runs as the model store for the TorchServe process. In figure 7.6,
    we take three steps to run a model prediction. First, we copy PyTorch model files
    to the model store directory. Second, we call the TorchServe management API to
    register the model to the TorchServe process. Finally, we call the TorchServe
    API to run the model prediction for the model—in our case, the intent classification
    model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例代码中，我们以Docker容器形式运行TorchServe，这是由PyTorch团队提供的，然后将本地文件目录挂载到容器中。这个文件目录作为TorchServe进程的模型存储。在图7.6中，我们采取三个步骤来运行模型预测。首先，我们将PyTorch模型文件复制到模型存储目录。其次，我们调用TorchServe管理API将模型注册到TorchServe进程中。最后，我们调用TorchServe
    API对模型进行预测——在我们的案例中，是对意图分类模型的预测。
- en: Compared to the self-built intent predictor in section 7.1.4, TorchServe is
    much simpler. We can make the model serving work without writing any code; we
    just need to set up a Docker container with disk sharing. Also, unlike the intent
    predictor that only works for intent classification algorithms, TorchServe is
    not tied to any specific training algorithm; it can serve any model as long as
    it’s trained with the PyTorch framework.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与7.1.4节中自建的意图预测器相比，TorchServe要简单得多。我们可以不编写任何代码就使模型服务工作；我们只需设置一个具有磁盘共享的Docker容器。而且，与仅适用于意图分类算法的意图预测器不同，TorchServe不依赖于任何特定的训练算法；只要使用PyTorch框架进行训练，它就可以服务任何模型。
- en: The great flexibility and convenience offered by TorchServe come with requirements.
    TorchServe requires operators to use their own set of APIs to send model serving
    requests, and it also requires that the model files are packaged in the TorchServe
    format. Let’s look at these mandates in the next two subsections.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe提供的巨大灵活性和便利性也伴随着要求。TorchServe要求操作员使用自己的API集合发送模型服务请求，并且它还要求模型文件以TorchServe格式打包。让我们在接下来的两个小节中看看这些要求。
- en: 7.2.5 TorchServe API
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 TorchServe API
- en: 'TorchServe offers many types of APIs, such as health checks, model explanation,
    model serving, worker management, and model registration. Each API has two types
    of implementations: HTTP and gRPC. Because TorchServe has very detailed explanations
    of its API contract and usage on its official website ([https://pytorch.org/serve/](https://pytorch.org/serve/))
    and GitHub repo ([https://github.com/pytorch/serve](https://github.com/pytorch/serve)),
    you can find the details there. In this section, we will focus on the model registration
    and model inference APIs that we use in our sample service.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe提供了许多类型的API，例如健康检查、模型解释、模型服务、工作管理以及模型注册。每个API有两种实现方式：HTTP和gRPC。由于TorchServe在其官方网站([https://pytorch.org/serve/](https://pytorch.org/serve/))和GitHub仓库([https://github.com/pytorch/serve](https://github.com/pytorch/serve))上对API合约和用法有非常详细的说明，您可以在那里找到详细信息。在本节中，我们将重点关注我们在示例服务中使用的模型注册和模型推理API。
- en: Model registration API
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型注册API
- en: Because TorchServe takes a black-box approach to model serving, it requires
    a model to be registered first before using it. More specifically, after we place
    model files in TorchServe’s model store (a local file directory), TorchServe won’t
    load the model automatically. We need to register the model file and the model’s
    execution method to TorchServe, so TorchServe knows how to work with this model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TorchServe采用黑盒方法进行模型服务，在使用模型之前需要先对其进行注册。更具体地说，在我们将模型文件放置在TorchServe的模型存储（一个本地文件目录）之后，TorchServe不会自动加载模型。我们需要将模型文件及其执行方法注册到TorchServe中，以便TorchServe知道如何处理此模型。
- en: 'In our code example, we use the TorchServe’s gRPC model registration API to
    register our intent model from the prediction service, as in the following snippet:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码示例中，我们使用TorchServe的gRPC模型注册API将我们的意图模型从预测服务中注册，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Registers the model to TorchServe by providing the model file and model name
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过提供模型文件和模型名称将模型注册到TorchServe
- en: The TorchServe model file already contains the model’s metadata—including the
    model version, model runtime, and model serving entry point. So when registering
    models, we normally just set the model file name in the `registerModel` API. In
    addition to model registration, we can also use the `scaleWorker` API to control
    how much compute resources we allocate to this model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe模型文件已经包含了模型的元数据——包括模型版本、模型运行时和模型服务入口点。因此，在注册模型时，我们通常只需在`registerModel`
    API中设置模型文件名。除了模型注册之外，我们还可以使用`scaleWorker` API来控制我们分配给此模型的计算资源量。
- en: Model inference API
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模型推理API
- en: TorchServe provides a unified model serving API for diverse models; this makes
    TorchServe simple to use. To run predictions for the default version of a model,
    make a REST call to `POST` `/predictions/{model_name}`. To run predictions for
    a specific version of a loaded model, make a REST call to `POST` `/predictions/{model_name}/
    {version}`. The content to be predicted in the prediction request is entered in
    binary format. For example,
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe为各种模型提供了一个统一的模型服务API；这使得TorchServe使用起来非常简单。要对模型的默认版本进行预测，请向`POST`
    `/predictions/{model_name}`发起REST调用。要对已加载模型的特定版本进行预测，请向`POST` `/predictions/{model_name}/{version}`发起REST调用。预测请求中要预测的内容以二进制格式输入。例如，
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our sample service, we use the gRPC interface to send prediction requests
    to TorchServe. Code listing 7.12 shows the `TorchGrpcPredictorBackend` client
    translating a prediction request from a frontend API call to a TorchServe backend
    gRPC call. You can find the full source ode of `TorchGrpcPredictorBackend` at
    `prediction-service/` `src/main/java/org/orca3/miniAutoML/prediction/TorchGrpcPredictorBackend.java.`
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的样本服务中，我们使用 gRPC 接口将预测请求发送到 TorchServe。代码列表 7.12 显示了 `TorchGrpcPredictorBackend`
    客户端将前端 API 调用的预测请求转换为 TorchServe 后端 gRPC 调用。您可以在 `prediction-service/src/main/java/org/orca3/miniAutoML/prediction/TorchGrpcPredictorBackend.java`
    找到 `TorchGrpcPredictorBackend` 的完整源代码。
- en: Listing 7.12 Calling the TorchServe prediction API from the frontend service
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 从前端服务调用 TorchServe 预测 API
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Converts text input to binary format for calling TorchServe
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将文本输入转换为二进制格式以调用 TorchServe
- en: 7.2.6 TorchServe model files
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 TorchServe 模型文件
- en: So far, you have seen the TorchServe model serving workflow and API. You may
    wonder how model serving works in TorchServe when it knows nothing about the model
    it serves. In chapter 6, we learned that to serve a model, the prediction service
    needs to know the model algorithm and model input/output schema. Counterintuitively,
    TorchServe runs model serving without knowing the model algorithm and model input/output
    data format. The trick lies in the TorchServe model file.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了 TorchServe 模型服务流程和 API。您可能会想知道当 TorchServe 对它所服务的模型一无所知时，模型服务在
    TorchServe 中是如何工作的。在第 6 章中，我们了解到为了服务一个模型，预测服务需要知道模型算法和模型输入/输出模式。出人意料的是，TorchServe
    在不知道模型算法和模型输入/输出数据格式的情况下运行模型服务。这个技巧在于 TorchServe 模型文件。
- en: TorchServe requires models to be packed into a special .mar file. We can use
    the `torch-model-archiver` CLI or `model_archiver` Python library to package PyTorch
    model files into a .mar file.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 需要将模型打包到特殊的 .mar 文件中。我们可以使用 `torch-model-archiver` CLI 或 `model_archiver`
    Python 库将 PyTorch 模型文件打包到 .mar 文件中。
- en: To archive a TorchServe .mar file, we need to provide the model name, model
    files (.pt or .pth), and a handler file. The handler file is the key piece; it
    is a Python code file that defines the logic for handling custom TorchServe inference
    logic. Because TorchServe’s model package (.mar file) contains the model algorithm,
    model data, and model execution code and the model execution code follows TorchServe’s
    prediction interface (protocol), TorchServe can execute any model (.mar file)
    by using its generic prediction API without knowing the model algorithm.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要存档 TorchServe .mar 文件，我们需要提供模型名称、模型文件 (.pt 或 .pth) 和处理程序文件。处理程序文件是关键部分；它是一个
    Python 代码文件，定义了处理自定义 TorchServe 推理逻辑的逻辑。因为 TorchServe 的模型包 (.mar 文件) 包含模型算法、模型数据和模型执行代码，并且模型执行代码遵循
    TorchServe 的预测接口（协议），所以 TorchServe 可以通过使用其通用的预测 API 来执行任何模型 (.mar 文件)，而无需了解模型算法。
- en: 'When TorchServe receives a prediction request, it will first find the internal
    worker process that hosts the model and then trigger the model’s handler file
    to process the request. The handler file contains four pieces of logic: model
    network initialization, input data preprocess, model inference, and prediction
    result postprocess. To make the previous explanation more concrete, let’s look
    at our intent model file as an example.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当 TorchServe 收到预测请求时，它将首先找到托管模型的内部工作进程，然后触发模型的处理程序文件来处理请求。处理程序文件包含四个逻辑部分：模型网络初始化、输入数据预处理、模型推理和预测结果后处理。为了使前面的解释更加具体，让我们以我们的意图模型文件为例。
- en: Intent classification .mar file
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 意图分类 .mar 文件
- en: 'If we open the .mar file of an intent model in our sample service, we will
    see that two additional files—`MANIFEST.json` and `torchserve_handler.py`—are
    added, compared with the model files we see in section 7.1.4\. The following is
    the folder structure of an intent .mar file:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开样本服务中意图模型的 .mar 文件，我们会看到与 7.1.4 节中看到的模型文件相比，增加了两个额外的文件——`MANIFEST.json`
    和 `torchserve_handler.py`。以下是一个意图 .mar 文件的文件夹结构：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ TorchServe .mar file metadata
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ TorchServe .mar 文件元数据
- en: ❷ Contains label information
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含标签信息
- en: ❸ Model weights file
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 模型权重文件
- en: ❹ Model architecture and model serving logic
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 模型架构和模型服务逻辑
- en: ❺ Vocabulary file, required by the intent algorithm
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 必要的词汇文件，由意图算法使用
- en: The `MANIFEST.json` file defines the metadata of a model, including the model
    version, model weights, model name, and handler file. By having a `MANIFEST.json`
    file, TorchServe knows how to load and run prediction on arbitrary models without
    knowing their implementation details.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`MANIFEST.json` 文件定义了模型的元数据，包括模型版本、模型权重、模型名称和处理文件。通过拥有 `MANIFEST.json` 文件，TorchServe
    就知道如何加载和运行任意模型的预测，而无需了解其实现细节。'
- en: TorchServe handler file
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 处理文件
- en: Once a model is registered in TorchServe, TorchServe will use the `handle(self`,
    `data`, `context)` function in the model’s handler file as the entry point for
    model prediction. The handler file manages the entire process of model serving,
    including model initialization, preprocess on input request, model execution,
    and postprocess on the predicted outcome.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在 TorchServe 中注册了模型，TorchServe 将使用模型处理文件中的 `handle(self, data, context)` 函数作为模型预测的入口点。处理文件管理模型服务的整个流程，包括模型初始化、输入请求的预处理、模型执行和预测结果的后处理。
- en: Code listing 7.13 highlights the key pieces of the handler file defined for
    the intent classification .mar file used in this sample service. You can find
    this file in our Git repository at `training-code/text-classification/torchserve_handler.py`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表 7.13 强调了为在此示例服务中使用的意图分类 `.mar` 文件定义的处理文件的关键部分。您可以在我们的 Git 仓库 `training-code/text-classification/torchserve_handler.py`
    中找到此文件。
- en: Listing 7.13 Intent model TorchServe handler file
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.13 意图模型 TorchServe 处理文件
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: By starting from the `handle` function in listing 7.13, you will have a clear
    view of how model serving is executed by the handler file. The `initialize` function
    loads all the model files (weights, labels, and vocabulary) and initializes the
    model. The `handle` function is the entry point of model serving; it preprocesses
    the binary model input, runs the model inference, postprocesses the model output,
    and returns the result.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表 7.13 中的 `handle` 函数开始，你可以清楚地了解处理文件是如何执行模型服务的。`initialize` 函数加载所有模型文件（权重、标签和词汇表）并初始化模型。`handle`
    函数是模型服务的入口点；它预处理二进制模型输入，运行模型推理，后处理模型输出，并返回结果。
- en: Packaging .mar file in training
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中打包 `.mar` 文件
- en: When we decide to use TorchServe for model serving, it’s better to produce the
    .mar file at training time. Also, because the TorchServe handler file contains
    the model architecture and model execution logic, it is usually a part of the
    model training code.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们决定使用 TorchServe 进行模型服务时，最好在训练时生成 `.mar` 文件。此外，由于 TorchServe 处理文件包含模型架构和模型执行逻辑，它通常是模型训练代码的一部分。
- en: 'There are two methods of packaging a .mar file. First, when model training
    completes, we can run the `torch-model-archiver` CLI tool to package model weights
    as serialized files and dependent files as extra files. Second, we can use the
    `model_ archiver` Python library to produce the .mar file as the last step of
    the model training code. The following code snippets are the examples we used
    for packaging intent classification models:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 打包 `.mar` 文件有两种方法。首先，当模型训练完成后，我们可以运行 `torch-model-archiver` CLI 工具将模型权重打包为序列化文件，并将依赖文件作为额外文件打包。其次，我们可以使用
    `model_archiver` Python 库在模型训练代码的最后一步生成 `.mar` 文件。以下是我们用于打包意图分类模型的代码片段示例：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 7.2.7 Scaling up in Kubernetes
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.7 在 Kubernetes 中扩展
- en: 'In our sample service, for demo purposes, we run a single TorchServe container
    as the prediction backend, but this is not the case for the production environment.
    The challenges for scaling up TorchServe are as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例服务中，出于演示目的，我们运行单个 TorchServe 容器作为预测后端，但在生产环境中并非如此。扩大 TorchServe 的挑战如下：
- en: The load balancer makes TorchServe model registration difficult. In TorchServe,
    model files need to be registered to the TorchServe server first before they can
    be used. But in production, the TorchServe instances are put behind a network
    load balancer, so we can only send prediction requests to the load balancer and
    let it route the request to a random TorchServe instance. In this case, it’s difficult
    to register models because we can’t specify which TorchServe instance serves which
    model. The load balancer hides the TorchServe instances from us.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器使得在 TorchServe 中注册模型变得困难。在 TorchServe 中，模型文件需要首先注册到 TorchServe 服务器，然后才能使用。但在生产中，TorchServe
    实例位于网络负载均衡器后面，因此我们只能向负载均衡器发送预测请求，让它将请求路由到任意的 TorchServe 实例。在这种情况下，注册模型变得困难，因为我们无法指定哪个
    TorchServe 实例服务于哪个模型。负载均衡器隐藏了 TorchServe 实例。
- en: Each TorchServe instance needs to have a model store directory for loading models,
    and model files need to be put in the model store directory before they can be
    registered. Having multiple TorchServe instances makes model file copying difficult
    to manage because we need to know every TorchServe instance’s IP address or DNS
    to copy the model files.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个TorchServe实例都需要一个模型存储目录来加载模型，并且模型文件在注册之前需要放在模型存储目录中。拥有多个TorchServe实例会使模型文件复制难以管理，因为我们需要知道每个TorchServe实例的IP地址或DNS才能复制模型文件。
- en: We need to balance the models among the TorchServe instances. Letting every
    TorchServe instance load every model file is a bad idea; it would be a great waste
    of compute resources. We should spread the load evenly across different TorchServe
    instances.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要在TorchServe实例之间平衡模型。让每个TorchServe实例加载每个模型文件不是一个好主意；这将是一种巨大的计算资源浪费。我们应该将负载均匀地分散到不同的TorchServe实例上。
- en: To address these challenges and scale up the TorchServe backend, we can introduce
    the “sidecar” pattern in Kubernetes. Figure 7.7 illustrates the overall concept.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战并扩展TorchServe后端，我们可以在Kubernetes中引入“侧车”模式。图7.7说明了整体概念。
- en: '![](../Images/07-07.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7](../Images/07-07.png)'
- en: Figure 7.7 Add a proxy container in the TorchServe pod to scale up TorchServe
    in Kubernetes.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 在TorchServe pod中添加代理容器以在Kubernetes中扩展TorchServe](../Images/07-07.png)'
- en: The proposal in figure 7.7 is to add a proxy container (as a sidecar) along
    with the TorchServe container in each TorchServe pod. Instead of calling the TorchServe
    API directly, we send the prediction requests to the proxy container. The proxy
    API in the proxy container will hide the TorchServe model management details,
    including model downloading and model registration. It will prepare the TorchServe
    container to serve arbitrary models.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7中的建议是在每个TorchServe pod中添加一个代理容器（作为侧车），与TorchServe容器一起。我们不是直接调用TorchServe
    API，而是将预测请求发送到代理容器。代理容器中的代理API将隐藏TorchServe模型管理的细节，包括模型下载和模型注册。它将为TorchServe容器准备服务任意模型。
- en: After adding a proxy container, the model serving workflow (figure 7.7) occurs
    as follows. First, the prediction request lands on the proxy container. Second,
    the proxy downloads the model file and inputs the shared disk (model store). Third,
    the proxy registers the model to the TorchServe container and converts the inference
    request to the TorchServe format. Fourth, the TorchServe container runs model
    serving and returns the result to the proxy. Finally, the proxy container returns
    the prediction response to the user.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加代理容器后，模型服务工作流程（图7.7）如下。首先，预测请求落在代理容器上。其次，代理下载模型文件并将其输入到共享磁盘（模型存储）。第三，代理将模型注册到TorchServe容器并将推理请求转换为TorchServe格式。第四，TorchServe容器运行模型服务并将结果返回给代理。最后，代理容器将预测响应返回给用户。
- en: By having a proxy container, we don’t need to worry about sending a prediction
    request to a TorchServe instance that doesn’t have that model registered. The
    proxy container (sidecar) will get the TorchServe container ready for any prediction
    request by copying model files to the model store and registering the model. It
    also simplifies the resource management effort because now we can simply rely
    on the load balancer to spread the prediction workload (models) across the TorchServe
    pods. Also, by sharing a disk across all TorchServe pods, we can share the model
    store for all the TorchServe instances, which reduces model downloading time and
    saves network bandwidth.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通过拥有代理容器，我们不需要担心将预测请求发送到没有注册该模型的TorchServe实例。代理容器（侧车）将通过复制模型文件到模型存储并注册模型来准备TorchServe容器以处理任何预测请求。它还简化了资源管理的工作量，因为现在我们可以简单地依赖负载均衡器来分散预测工作负载（模型）到TorchServe
    pod。此外，通过在所有TorchServe pod之间共享磁盘，我们可以共享所有TorchServe实例的模型存储，这减少了模型下载时间并节省了网络带宽。
- en: 'The sidecar pattern: A common approach to running the model server'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 侧车模式：运行模型服务器的一种常见方法
- en: In section 7.4, we will introduce several other model server approaches, such
    as TensorFlow serving and Triton. Although the implementation of these model servers
    is different, their design ideas are similar. They all take a black-box approach
    and require certain model formats and some model management to enable model serving.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在7.4节中，我们将介绍几种其他模型服务器方法，例如TensorFlow serving和Triton。尽管这些模型服务器的实现不同，但它们的设计思想是相似的。它们都采用黑盒方法，并需要某些模型格式和一些模型管理来启用模型服务。
- en: The sidecar pattern in figure 7.7 is a common solution to running these different
    model server containers in a Kubernetes pod. The proxy container encapsulates
    all the special requirements of the model server and only exposes a general model
    serving API.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7中的边车模式是运行这些不同模型服务器容器在Kubernetes Pod中的常见解决方案。代理容器封装了模型服务器的所有特殊要求，并且只暴露通用的模型服务API。
- en: 7.3 Model server vs. model service
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 模型服务器与模型服务
- en: Choosing between the model server approach and the model service approach is
    the first decision we need to make when designing a model serving application.
    When we choose improperly, our serving application either is hard to use and maintain
    or takes an unnecessarily long time to build.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计模型服务应用程序时，选择模型服务器方法还是模型服务方法是我们需要做的第一个决定。如果我们选择不当，我们的服务应用程序可能难以使用和维护，或者构建时间过长。
- en: We’ve already reviewed the differences between these two approaches in chapter
    6 (sections 6.2 and 6.3), but this is such a crucial choice that it’s worth examining
    again. Now that you’ve seen concrete examples of each approach in action, these
    ideas may make more intuitive sense.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第6章（第6.2节和第6.3节）中回顾了这两种方法之间的差异，但这是一个如此关键的选择，值得再次审视。现在你已经看到了每种方法的具体实例，这些想法可能更有直观性。
- en: From working through the two sample services in sections 7.1 and 7.2, it’s clear
    that the model server approach avoids the effort of building dedicated backend
    predictors for specific model types. Instead, it works out of the box and can
    serve arbitrary models regardless of which algorithm the model is implementing.
    So, it might seem like the model server approach should always be the best choice.
    But this is not true; the choice between the model server or model service should
    depend on the use case and business requirements.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析第7.1节和第7.2节中的两个示例服务，可以清楚地看出，模型服务器方法避免了为特定模型类型构建专用后端预测器的努力。相反，它即插即用，可以服务任意模型，无论模型实现的是哪种算法。因此，模型服务器方法似乎应该是最佳选择。但这并非事实；模型服务器或模型服务之间的选择应取决于用例和业务需求。
- en: For single-application scenarios, the model service approach is simpler to build
    and maintain in practice. Model service backend predictors are quite straightforward
    to build because model serving code is a simplified version of the training code.
    This means we can easily convert a model training container to a model serving
    container. Once it is built, the model service approach is easier to maintain
    because we own the code end to end and the workflow is simple. For the model server
    approach, whether we choose open source, prebuilt model servers, or build our
    own server, the process of setting up the system is complicated. It takes a lot
    of effort to learn the system well enough to operate and maintain it.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单应用程序场景，在实际中，模型服务方法更简单，更容易构建和维护。由于模型服务代码是训练代码的简化版本，因此模型服务后端预测器相当容易构建。这意味着我们可以轻松地将模型训练容器转换为模型服务容器。一旦构建完成，模型服务方法更容易维护，因为我们拥有端到端的代码，并且工作流程简单。对于模型服务器方法，无论我们选择开源、预构建的模型服务器还是构建自己的服务器，设置系统的过程都很复杂。需要投入大量精力才能充分了解系统，以便操作和维护。
- en: For model serving platform scenarios, where the system needs to support many
    different types of models, the model server approach is unquestionably the best.
    When you are building a model serving system for 500 different types of models,
    if you choose the model server approach, you only need to have one single type
    of predictor backend for all the models. In contrast, using the model service
    approach, you would need to have 500 different model predictors! It is incredibly
    hard to manage the compute resources and perform the maintenance work for all
    those predictors.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要支持许多不同类型模型的模型服务平台场景，模型服务器方法无疑是最佳选择。当你为500种不同类型的模型构建模型服务系统时，如果你选择模型服务器方法，你只需要为所有模型有一个单一的预测器后端。相比之下，使用模型服务方法，你需要有500个不同的模型预测器！管理所有这些预测器的计算资源并执行维护工作极其困难。
- en: Our recommendation is to use the model service approach when you are first learning
    because it is simpler and easier. You can move to the model server approach when
    you need to support more than 5 to 10 types of models or applications in your
    serving system.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的建议是，当你刚开始学习时使用模型服务方法，因为它更简单、更容易。当你需要支持你的服务系统中超过5到10种模型或应用程序时，你可以转向模型服务器方法。
- en: 7.4 Touring open source model serving tools
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 探索开源模型服务工具
- en: There are plenty of open source model serving tools available. It’s great to
    have options, but having so many of them can be overwhelming. To help make that
    choice easier for you, we will introduce you to some popular model serving tools,
    including TensorFlow Serving, TorchServe, Triton, and KServe. All of these can
    work out of the box and are applicable to production use cases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多开源的模型服务工具可供选择。拥有多种选择固然很好，但过多的选择可能会让人感到不知所措。为了帮助您更容易地做出选择，我们将向您介绍一些流行的模型服务工具，包括
    TensorFlow Serving、TorchServe、Triton 和 KServe。所有这些工具都可以直接使用，并且适用于生产环境。
- en: Because each of the tools we describe here has thorough documentation, we will
    keep the discussion at a general level, looking just at their overall design,
    main features, and suitable use cases. This information should be enough to act
    as a starting point from which to explore further on your own.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里描述的每个工具都有详尽的文档，我们将保持讨论在一般水平，仅关注它们的整体设计、主要功能和适用用例。这些信息应该足以作为您进一步探索的起点。
- en: 7.4.1 TensorFlow Serving
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 TensorFlow Serving
- en: TensorFlow Serving ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving))
    is a customizable, standalone web system for serving TensorFlow models in production
    environments. TensorFlow Serving takes a model server approach; it can serve all
    types of TensorFlow models with the same server architecture and APIs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving))
    是一个可定制的、独立的网络系统，用于在生产环境中提供 TensorFlow 模型。TensorFlow Serving 采用模型服务器方法；它可以使用相同的服务器架构和
    API 为所有类型的 TensorFlow 模型提供服务。
- en: Features
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 功能
- en: 'TensorFlow Serving offers the following features:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 提供以下功能：
- en: Can serve multiple models or multiple versions of the same model
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以服务多个模型或同一模型的多个版本
- en: Has out-of-the-box integration with TensorFlow models
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 TensorFlow 模型具有开箱即用的集成
- en: Automatically discovers new model versions and supports different model file
    sources
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动发现新的模型版本并支持不同的模型文件来源
- en: Has unified gRPC and HTTP endpoints for model inference
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有统一的 gRPC 和 HTTP 端点进行模型推理
- en: Supports batching prediction requests and performance tuning
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持批量预测请求和性能调整
- en: Has an extensible design, which is customizable on version policy and model
    loading
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有可扩展的设计，可以在版本策略和模型加载上进行自定义
- en: high-level architecture
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 高级架构
- en: In TensorFlow Serving, a model is composed of one or more servables. A servable
    is the underlying object to perform computation (for example, a lookup or inference);
    it is the central abstraction in TensorFlow Serving. Sources are plugin modules
    that find and provide servables. Loader standards are the API for loading and
    unloading a servable. The manager handles the full lifecycle of servables, including
    loading, unloading, and serving servables.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow Serving 中，一个模型由一个或多个可服务对象组成。可服务对象是执行计算的基础对象（例如，查找或推理）；它是 TensorFlow
    Serving 中的核心抽象。源是插件模块，用于查找和提供可服务对象。加载器标准是加载和卸载可服务对象的 API。管理器处理可服务对象的整个生命周期，包括加载、卸载和服务可服务对象。
- en: '![](../Images/07-08.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-08.png)'
- en: 'Figure 7.8 TensorFlow Serving architecture and model serving life cycle. Blue
    = darkest gray; green = lighter gray; yellow = lightest gray. (Source: TensorFlow;
    [http://mng.bz/KlNj](http://mng.bz/KlNj))'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 TensorFlow Serving 架构和模型服务生命周期。蓝色 = 最深灰色；绿色 = 浅灰色；黄色 = 最浅灰色。（来源：TensorFlow；[http://mng.bz/KlNj](http://mng.bz/KlNj)）
- en: Figure 7.8 illustrates the workflow of presenting a servable to the customer.
    First, the source plugin creates a loader for a specific servable; the loader
    contains the metadata to load the servable. Second, the source finds a servable
    in the filesystem (a model repository); it notifies the servable’s version and
    loader to DynamicManager. Third, based on the predefined version policy, DynamicManager
    determines whether to load the model. Finally, the client sends a prediction request
    for a servable, and DynamicManager returns a handle, so the client can execute
    the model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 展示了向客户展示可服务对象的流程。首先，源插件为特定的可服务对象创建一个加载器；加载器包含加载可服务对象的元数据。其次，源在文件系统（模型存储库）中找到一个可服务对象；它通知
    DynamicManager 可服务对象的版本和加载器。第三，根据预定义的版本策略，DynamicManager 确定是否加载模型。最后，客户端为可服务对象发送预测请求，DynamicManager
    返回一个句柄，以便客户端可以执行模型。
- en: TensorFlow Serving model file
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 模型文件
- en: TensorFlow Serving requires models to be saved in SavedModel ([http://mng.bz/9197](http://mng.bz/9197))
    format. We could use the `tf.saved_model.save(model,` `save_path)` API for this
    purpose. A saved model is a directory containing serialized signatures and the
    state needed to run them, including variable values and vocabularies. For example,
    a saved model directory has two subdirectories, `assets` and `variables`, and
    one file, `saved_model.pb`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 要求模型以 SavedModel ([http://mng.bz/9197](http://mng.bz/9197))
    格式保存。我们可以使用 `tf.saved_model.save(model, save_path)` API 来实现这一点。已保存的模型是一个包含序列化签名和运行它们所需状态的目录，包括变量值和词汇表。例如，已保存的模型目录有两个子目录，`assets`
    和 `variables`，以及一个文件，`saved_model.pb`。
- en: The assets folder contains files used by TensorFlow graphs, such as text files
    for initializing vocabulary tables. The variables folder contains training checkpoints.
    The `saved_model.pb` file stores the actual TensorFlow program, or model, and
    a set of named signatures, each identifying a function that accepts tensor inputs
    and produces tensor outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 资产文件夹包含 TensorFlow 图使用的文件，例如用于初始化词汇表的文本文件。变量文件夹包含训练检查点。`saved_model.pb` 文件存储实际的
    TensorFlow 程序或模型以及一系列命名的签名，每个签名标识一个接受张量输入并产生张量输出的函数。
- en: Model serving
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务
- en: 'Because TensorFlow’s SavedModel files can be directly loaded into the TensorFlow
    Serving process, running model serving is straightforward. Once the serving process
    starts, we can copy model files to TensorFlow Serving’s model directory and then
    send gRPC or REST prediction requests right away. Let’s review the following prediction
    example:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 的 SavedModel 文件可以直接加载到 TensorFlow Serving 进程中，运行模型服务非常简单。一旦服务进程启动，我们就可以将模型文件复制到
    TensorFlow Serving 的模型目录，然后立即发送 gRPC 或 REST 预测请求。让我们回顾以下预测示例：
- en: '[PRE20]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For loading multiple models and multiple versions of the same model into the
    serving server, we can configure the model’s versions in the model config as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将多个模型及其多个版本加载到服务服务器中，我们可以在模型配置中按如下方式配置模型的版本：
- en: '[PRE21]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Finds model v2 at /models/model_a/versions/2
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 /models/model_a/versions/2 找到模型 v2
- en: ❷ Finds model v3 at /models/model_a/versions/3
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 /models/model_a/versions/3 找到模型 v3
- en: In this config, we defined two models, `model_a` and `model_b`. Because `model_a`
    has a `model_version_policy`, both the two versions (v2 and v3) are loaded and
    can serve requests. By default, the latest version of the model will be served,
    so when a new version of `model_b` is detected, the previous one will be replaced
    by the new one.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们定义了两个模型，`model_a` 和 `model_b`。因为 `model_a` 有一个 `model_version_policy`，所以两个版本（v2
    和 v3）都被加载并可以处理请求。默认情况下，将提供模型的最新版本，所以当检测到 `model_b` 的新版本时，旧版本将被新版本替换。
- en: Review
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 复习
- en: TensorFlow Serving is a production-level model serving solution for TensorFlow
    models; it supports REST, gRPC, GPU acceleration, minibatching, and model serving
    on edge devices. Although TensorFlow Serving falls short on advanced metrics,
    flexible model management, and deployment strategies, it’s still a good choice
    if you only have TensorFlow models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 是 TensorFlow 模型的生产级模型服务解决方案；它支持 REST、gRPC、GPU 加速、小批量处理以及边缘设备上的模型服务。尽管
    TensorFlow Serving 在高级指标、灵活的模型管理和部署策略方面有所不足，但如果您只有 TensorFlow 模型，它仍然是一个不错的选择。
- en: The main disadvantage of TensorFlow Serving is that it’s a vendor lock-in solution;
    it only supports TensorFlow models. If you are looking for a training framework
    agnostic approach, TensorFlow Serving wouldn’t be your choice.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 的主要缺点是它是一个供应商锁定解决方案；它只支持 TensorFlow 模型。如果您正在寻找一个与训练框架无关的方法，TensorFlow
    Serving 不会是您的选择。
- en: 7.4.2 TorchServe
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 TorchServe
- en: TorchServe ([https://pytorch.org/serve/](https://pytorch.org/serve/)) is a performant,
    flexible, and easy-to-use tool for serving PyTorch eager mode and torchscripted
    models (an intermediate representation of a PyTorch model that can be run in a
    high-performance environment such as C++). Similar to TensorFlow Serving, TorchServe
    takes a model server approach to serving all kinds of PyTorch models with a unified
    API. The difference is TorchServe provides a set of management APIs that makes
    model management very convenient and flexible. For example, we can programmatically
    register and unregister models or different versions of a model. And we can also
    scale up and scale down serving workers for models and different versions of a
    model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe([https://pytorch.org/serve/](https://pytorch.org/serve/))是一个性能优良、灵活且易于使用的工具，用于服务PyTorch
    eager模式和torchscripted模型（PyTorch模型的一个中间表示，可以在高性能环境如C++中运行）。类似于TensorFlow Serving，TorchServe采用模型服务器方法，通过统一的API服务所有类型的PyTorch模型。不同之处在于TorchServe提供了一套管理API，使得模型管理变得非常方便和灵活。例如，我们可以通过编程方式注册和注销模型或模型的多个版本。我们还可以调整模型及其不同版本的服务的worker的数量。
- en: High-level architecture
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 高级架构
- en: 'A TorchServe server is composed of three components: the frontend, backend,
    and model store. The frontend handles TorchServe’s request/response. It also manages
    the life cycles of the models. The backend is a list of model workers that are
    responsible for running the actual inference on the models. The model store is
    a directory in which all the loadable models exist; it can be a cloud storage
    folder or a local host folder. Figure 7.9 shows the high-level architecture of
    a TorchServing instance.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TorchServe服务器由三个组件组成：前端、后端和模型存储。前端处理TorchServe的请求/响应。它还管理模型的生命周期。后端是一系列负责在模型上执行实际推理的模型worker。模型存储是一个目录，其中包含所有可加载的模型；它可以是云存储文件夹或本地主机文件夹。图7.9显示了TorchServing实例的高级架构。
- en: '![](../Images/07-09.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9](../Images/07-09.png)'
- en: 'Figure 7.9 A TorchServe architecture diagram (Source: Kuldeep Singh, “Deploying
    Named Entity Recognition model to production using TorchServe,” Analytics Vidhya,
    2020)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 TorchServe架构图（来源：Kuldeep Singh，“使用TorchServe将命名实体识别模型部署到生产中，”Analytics
    Vidhya，2020）
- en: 'Figure 7.9 draws two workflows: model inference and model management. For model
    inference, first, the user sends a prediction request to the inference endpoint
    of a model, such as `/predictions/{model_name}/{version}`. Next, the inference
    request is routed to one of the worker processes that already loaded the model.
    Then, the worker process will read model files from the model store and let the
    model handler load the model, preprocess the input data, and run the model to
    obtain a prediction result.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9展示了两个工作流程：模型推理和模型管理。对于模型推理，首先，用户将预测请求发送到模型的推理端点，例如`/predictions/{model_name}/{version}`。接下来，推理请求被路由到已经加载了模型的某个worker进程。然后，worker进程将从模型存储中读取模型文件，并让模型处理器加载模型，预处理输入数据，并运行模型以获得预测结果。
- en: For model management, a model needs to be registered before users can access
    it. This is done by using the management API. We can also scale up and down the
    worker process count for a model. We will see an example in the upcoming sample
    usage section.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型管理，用户在访问模型之前需要先对其进行注册。这通过使用管理API来完成。我们还可以根据需要调整模型的worker进程数量。我们将在接下来的示例使用部分中看到一个示例。
- en: Features
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 功能
- en: 'TorchServe offers the following features:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe提供以下功能：
- en: Can serve multiple models or multiple versions of the same model
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以为多个模型或同一模型的多个版本提供服务
- en: Has unified gRPC and HTTP endpoints for model inference
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有统一的gRPC和HTTP端点进行模型推理
- en: Supports batching prediction requests and performance tuning
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持批量预测请求和性能调整
- en: Supports workflow to compose PyTorch models and Python functions in sequential
    and parallel pipelines
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持将PyTorch模型和Python函数在顺序和并行管道中组合的工作流程
- en: Provides management API to register/unregister models and scale up/down workers
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供管理API以注册/注销模型和调整worker的数量
- en: Handles model versioning for A/B testing and experimentation
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理模型的版本控制，以进行A/B测试和实验
- en: Torch serving model file
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Torch服务模型文件
- en: Pure PyTorch models cannot be loaded to the Torch serving server directly. TorchServe
    requires all its models to be packaged into a .mar file. Please refer to section
    7.2.6 for a detailed example of how a .mar file is created.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 纯PyTorch模型不能直接加载到Torch服务服务器。TorchServe要求所有模型都打包成一个`.mar`文件。请参阅7.2.6节以获取创建`.mar`文件的详细示例。
- en: Model serving
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务
- en: 'The following code snippet lists five general steps to running model inference
    with TorchServe. For a concrete example, you can check out the README doc of our
    sample intent classification predictor ([http://mng.bz/WA8a](http://mng.bz/WA8a)):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段列出了使用TorchServe运行模型推理的五个一般步骤。具体示例，您可以查看我们样本意图分类预测器的README文档（[http://mng.bz/WA8a](http://mng.bz/WA8a)）：
- en: '[PRE22]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Creates local model dir and copies the intent classification model
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建本地模型目录并复制意图分类模型
- en: ❷ Binds local model dir as the model store dir for TorchServe
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将本地模型目录绑定为TorchServe的模型存储目录
- en: ❸ Intent_1.mar contains the model file and model metadata, such as the model
    version.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Intent_1.mar包含模型文件和模型元数据，例如模型版本。
- en: 'Besides using management API to register models, we can also use the scale
    worker API to dynamically adjust the number of workers for any version of a model
    to better serve different inference request loads, as in the following example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用管理API注册模型外，我们还可以使用scale worker API动态调整任何版本模型的worker数量，以更好地服务于不同的推理请求负载，如下例所示：
- en: '[PRE23]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Review
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 审查
- en: TorchServe is a production-level model serving solution for PyTorch models;
    it’s designed for high-performance inference and production use cases. TorchServe’s
    management API adds a lot of flexibility for customizing model deployment strategy,
    and it allows us to manage compute resources at the per-model level.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe是针对PyTorch模型的量产级模型服务解决方案；它旨在进行高性能推理和生产用例。TorchServe的管理API为定制模型部署策略提供了很多灵活性，并允许我们在每个模型级别管理计算资源。
- en: Similar to TensorFlow Serving, the main disadvantage of TorchServe is that it’s
    a vendor lock-in solution; it only supports PyTorch models. So, if you are looking
    for a training framework agnostic approach, TorchServe wouldn’t be your choice.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorFlow Serving类似，TorchServe的主要缺点是它是供应商锁定解决方案；它仅支持PyTorch模型。因此，如果您正在寻找一种与训练框架无关的方法，TorchServe可能不是您的选择。
- en: 7.4.3 Triton Inference Server
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 Triton推理服务器
- en: Triton Inference Server ([https://developer.nvidia.com/nvidia-triton-inference-server](https://developer.nvidia.com/nvidia-triton-inference-server))
    is an open source inference server developed by NVIDIA. It provides a cloud and
    edge inferencing solution optimized for both CPUs and GPUs. Triton supports an
    HTTP/ REST and gRPC protocol that allows remote clients to request inferencing
    for any model being managed by the server. For edge deployments, Triton is available
    as a shared library with a C API that allows the full functionality of Triton
    to be included directly in an application.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器（[https://developer.nvidia.com/nvidia-triton-inference-server](https://developer.nvidia.com/nvidia-triton-inference-server)）是由NVIDIA开发的开源推理服务器。它提供了一种针对CPU和GPU优化的云和边缘推理解决方案。Triton支持HTTP/REST和gRPC协议，允许远程客户端请求由服务器管理的任何模型的推理。对于边缘部署，Triton作为一个带有C
    API的共享库提供，允许将Triton的全部功能直接包含在应用程序中。
- en: Training framework compatibility is one of Triton’s main advantages when compared
    with other serving tools. Unlike TensorFlow Serving, which only works with the
    TensorFlow model, and Torch serving, which only works with the PyTorch model,
    the Triton server can serve models trained from almost any framework, including
    TensorFlow, TensorRT, PyTorch, ONNX, and XGBoost. Triton server can load model
    files from local storage, Google Cloud Platform, or Amazon Simple Storage Service
    (Amazon S3) on any GPU- or CPU-based infrastructure (cloud, data center, or edge).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他服务工具相比，训练框架兼容性是Triton的主要优势之一。与仅适用于TensorFlow模型的TensorFlow Serving和仅适用于PyTorch模型的Torch
    serving不同，Triton服务器可以服务于从几乎任何框架训练的模型，包括TensorFlow、TensorRT、PyTorch、ONNX和XGBoost。Triton服务器可以在任何基于GPU或CPU的基础设施（云、数据中心或边缘）上从本地存储、Google
    Cloud Platform或Amazon Simple Storage Service（Amazon S3）加载模型文件。
- en: Inference performance is also an advantage for Triton. Triton runs models concurrently
    on GPUs to maximize throughput and utilization; supports x86 and ARM CPU-based
    inferencing; and offers features like dynamic batching, model analyzer, model
    ensemble, and audio streaming. These features make model serving memory efficient
    and robust.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 推理性能也是Triton的优势之一。Triton在GPU上并发运行模型以最大化吞吐量和利用率；支持基于x86和ARM CPU的推理；并提供动态批处理、模型分析器、模型集成和音频流等功能。这些功能使模型服务内存高效且稳健。
- en: High-level architecture
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 高级架构
- en: Figure 7.10 shows the Triton Inference Server’s high-level architecture. All
    the inference requests are sent as REST or gRPC requests, and then they are converted
    to C API calls internally. Models are loaded from the model repository, which
    is a filesystem-based repository that we can see as folders/directories.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10展示了Triton推理服务器的高级架构。所有的推理请求都以REST或gRPC请求的形式发送，然后内部转换为C API调用。模型从模型仓库加载，该仓库是一个基于文件系统的仓库，我们可以将其视为文件夹/目录。
- en: '![](../Images/07-10.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-10.png)'
- en: 'Figure 7.10 Triton Inference Server high-level architecture (Source: NVIDIA
    Developer, [https://developer.nvidia.com/nvidia-triton-inference-server](https://developer.nvidia.com/nvidia-triton-inference-server))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 Triton推理服务器高级架构（来源：NVIDIA开发者，[https://developer.nvidia.com/nvidia-triton-inference-server](https://developer.nvidia.com/nvidia-triton-inference-server))
- en: For each model, Triton prepares a scheduler. The scheduling and batching algorithms
    are configurable on a model-by-model basis. Each model’s scheduler optionally
    performs batching of inference requests and then passes the requests to the backend
    corresponding to the model type, such as PyTorch backend for the PyTorch model.
    A Triton backend is the implementation that executes a model. It can be a wrapper
    around a deep learning framework, like PyTorch, TensorFlow, TensorRT, or ONNX
    Runtime. Once the backend performs inferencing using the inputs provided in the
    batched requests to produce the requested outputs, the outputs are returned.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，Triton都会准备一个调度器。调度和批处理算法可以按模型进行配置。每个模型的调度器可以选择性地对推理请求进行批处理，然后将请求传递给对应于模型类型的后端，例如PyTorch模型的后端是PyTorch。Triton后端是执行模型的实现。它可以是一个围绕深度学习框架（如PyTorch、TensorFlow、TensorRT或ONNX
    Runtime）的包装器。一旦后端使用批量请求中提供的输入进行推理以生成所需的输出，这些输出就会被返回。
- en: One thing worth noting is that Triton supports a backend C API that allows Triton
    to be extended with new functionality, such as custom pre- and postprocessing
    operations or even a new deep learning framework. This is how we can extend the
    Triton server. You can check out the triton-inference-server/backend GitHub repo
    ([https://github.com/triton-inference-server/backend](https://github.com/triton-inference-server/backend))
    to find all Triton backend implementations. As a bonus, the models being served
    by Triton can be queried and controlled by a dedicated model management API that
    is available by HTTP/REST, gRPC protocol, or the C API.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Triton支持一个后端C API，允许Triton通过新的功能进行扩展，例如自定义的前后处理操作，甚至是一个新的深度学习框架。这就是我们如何扩展Triton服务器的方式。您可以查看triton-inference-server/backend
    GitHub仓库（[https://github.com/triton-inference-server/backend](https://github.com/triton-inference-server/backend)）以找到所有Triton后端实现。作为额外的好处，由Triton提供的模型可以通过专用的模型管理API进行查询和控制，该API通过HTTP/REST、gRPC协议或C
    API提供。
- en: Features
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 功能
- en: 'Triton offers the following features:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Triton提供以下功能：
- en: Supports all major deep learning and machine learning framework backends.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持所有主要的深度学习和机器学习框架后端。
- en: Runs multiple models from the same or different frameworks concurrently on a
    single GPU or CPU. In a multi-GPU server, Triton automatically creates an instance
    of each model on each GPU to increase utilization.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个GPU或CPU上同时运行来自相同或不同框架的多个模型。在多GPU服务器中，Triton会自动在每个GPU上为每个模型创建一个实例，以提高利用率。
- en: Optimizes inference serving for real-time inferencing, batch inferencing to
    maximize GPU/CPU utilization, and streaming inference with built-in support for
    audio streaming input. Triton also supports model ensembles for use cases that
    require multiple models to perform end-to-end inference, such as conversational
    AI.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化推理服务以实现实时推理、批量推理以最大化GPU/CPU利用率，以及具有内置音频流输入支持的流式推理。Triton还支持模型集成，用于需要多个模型执行端到端推理的场景，例如对话式人工智能。
- en: Handles dynamic batching of input requests for high throughput and utilization
    under strict latency constraints.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理输入请求的动态批处理，以满足严格的延迟约束下的高吞吐量和利用率。
- en: Updates models live in production without restarting the inference server or
    disrupting the application.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新模型可以在生产环境中进行，无需重启推理服务器或中断应用程序。
- en: Uses model analyzer to automatically find the optimal model configuration and
    maximize performance.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型分析器自动找到最佳模型配置并最大化性能。
- en: Supports multi-GPU, multinode inference of large models.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持大型模型的多GPU、多节点推理。
- en: Triton model file
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Triton模型文件
- en: 'Each model in Triton must include a model configuration that provides required
    and optional information about the model. Typically, it’s a config.pbtxt file
    specified as ModelConfig protobuf ([http://mng.bz/81Kz](http://mng.bz/81Kz)).
    See a simple model config (config.pbtxt) for a PyTorch model as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Triton 中的每个模型都必须包含一个模型配置，该配置提供有关模型所需和可选信息的说明。通常，它是一个 ModelConfig protobuf 格式的
    config.pbtxt 文件。以下是一个 PyTorch 模型的简单模型配置（config.pbtxt）示例：
- en: '[PRE24]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Specifies the PyTorch serving backend for this model
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定此模型的 PyTorch 服务后端
- en: ❷ Indicates this is a PyTorch backend config
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指示这是一个 PyTorch 后端配置
- en: ❸ Defines the maximum batch size that the model supports
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义模型支持的最大的批量大小
- en: ❹ Models input data schema
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 模型输入数据模式
- en: ❺ Models output data schema
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模型输出数据模式
- en: Normally, the training application creates this config.pbtxt file when training
    completes at the training service and then uploads this config as part of the
    model files to the model repository. For more detail on Triton model configs,
    please check out the Triton model configuration documentation at [http://mng.bz/Y6mA](http://mng.bz/Y6mA).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练应用程序在训练服务完成训练后创建此 config.pbtxt 文件，然后将此配置作为模型文件的一部分上传到模型仓库。有关 Triton 模型配置的更多详细信息，请查看
    Triton 模型配置文档，链接为 [http://mng.bz/Y6mA](http://mng.bz/Y6mA)。
- en: Besides a unified config file, the Triton model file format is different per
    training framework. For example, TensorFlow models in SavedModel format ([http://mng.bz/El4d](http://mng.bz/El4d))
    can be loaded with Triton directly. But PyTorch models need to be saved by the
    TorchScript program.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统一的配置文件外，Triton 模型文件格式因训练框架而异。例如，SavedModel 格式的 TensorFlow 模型 ([http://mng.bz/El4d](http://mng.bz/El4d))
    可以直接由 Triton 加载。但 PyTorch 模型需要通过 TorchScript 程序保存。
- en: TorchScript
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript
- en: 'TorchScript is a way to create serializable and optimizable models from PyTorch
    code. The reason Triton requires PyTorch models to be serialized as TorchScript
    is that TorchScript can be used as an intermediate representation of a PyTorch
    model. It can run independently from Python, such as in a standalone C++ program.
    See the following code snippet for creating a TorchScript model from a PyTorch
    model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript 是从 PyTorch 代码创建可序列化和可优化的模型的一种方式。Triton 要求 PyTorch 模型以 TorchScript
    格式序列化的原因是 TorchScript 可以用作 PyTorch 模型的中间表示。它可以独立于 Python 运行，例如在独立的 C++ 程序中。以下是一个从
    PyTorch 模型创建 TorchScript 模型的代码片段：
- en: '[PRE25]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For the model format requirement of other training frameworks, please check
    out the triton-inference-server/backend GitHub repo ([http://mng.bz/NmOn](http://mng.bz/NmOn)).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他训练框架的模型格式要求，请查看 triton-inference-server/backend GitHub 仓库 ([http://mng.bz/NmOn](http://mng.bz/NmOn))。
- en: Model serving
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务
- en: 'Model serving in Triton involves the following three steps: first, copy the
    model file to the model repository; second, call the management API `(POST` `v2/repository/
    models/${MODEL_NAME}/load`) to register the model; and third, send an inference
    request `(POST` `v2/models/${MODEL_NAME}/versions/${MODEL_VERSION})`. For more
    information on the Triton management API, you can check the Triton HTTP/REST and
    gRPC protocol documentation ([http://mng.bz/DZvR](http://mng.bz/DZvR)). For inference
    API, you can check the KServe community standard inference protocols documentation
    ([https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/](https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/)).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Triton 中，模型服务涉及以下三个步骤：首先，将模型文件复制到模型仓库；其次，调用管理 API (`POST` `v2/repository/models/${MODEL_NAME}/load`)
    注册模型；最后，发送推理请求 (`POST` `v2/models/${MODEL_NAME}/versions/${MODEL_VERSION})`)。有关
    Triton 管理API的更多信息，您可以查看 Triton HTTP/REST 和 gRPC 协议文档 ([http://mng.bz/DZvR](http://mng.bz/DZvR))。有关推理
    API 的信息，您可以查看 KServe 社区标准推理协议文档 ([https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/](https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/))。
- en: Review
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 审查
- en: As we write this book, we consider Triton the best model serving approach for
    three reasons. First, Triton is training-framework agnostic; it provides a well-designed
    and extensible backend framework, which allows it to execute the models built
    by almost any training framework. Second, Triton offers better model serving performance,
    such as serving throughput. Triton has multiple mechanisms to improve its serving
    performance, such as dynamic batching, GPU optimization, and model analyzing tools.
    Third, Triton supports advanced model serving use cases such as model ensembles
    and audio streaming.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，我们认为 Triton 是最佳模型服务方法，原因有三。首先，Triton 对训练框架没有限制；它提供了一个设计精良且可扩展的后端框架，这使得它能够执行几乎任何训练框架构建的模型。其次，Triton
    提供了更好的模型服务性能，例如服务吞吐量。Triton 有多种机制来提高其服务性能，例如动态批处理、GPU 优化和模型分析工具。第三，Triton 支持高级模型服务用例，如模型集成和音频流。
- en: WARNING Be cautious! Triton may not be free. Triton is under BSD 3-Clause “new”
    or “revised” licensing, meaning it can be modified and distributed for commercial
    purposes for free. But what about troubleshooting and bug fixing? The project
    is complex, with a large code base, so you’ll have a hard time debugging and fixing
    performance concerns, such as memory leaks. If you look for the NVIDIA AI-enterprise
    license to get the support, as this book is being written, it would cost you several
    thousand dollars per GPU per year. So be sure that you understand the Triton codebase
    before signing up.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：请注意！Triton 可能不是免费的。Triton 采用 BSD 3-Clause “新”或“修订”许可，这意味着它可以免费修改和用于商业目的。但是，关于故障排除和错误修复呢？该项目复杂，代码库庞大，因此调试和修复性能问题，如内存泄漏，可能会很困难。如果你寻找
    NVIDIA AI 企业许可证以获得支持，正如本书撰写时的情况，每块 GPU 每年可能需要花费数千美元。所以，在注册之前，请确保你理解 Triton 代码库。
- en: 7.4.4 KServe and other tools
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 KServe 及其他工具
- en: 'The list of open source serving tools is extensive and includes KServe ([https://www.kubeflow.org/docs/external-add-ons/kserve/](https://www.kubeflow.org/docs/external-add-ons/kserve/)),
    Seldon Core ([https://www.seldon.io/solutions/open-source-projects/core](https://www.seldon.io/solutions/open-source-projects/core)),
    and BentoML ([https://github.com/bentoml/BentoML](https://github.com/bentoml/BentoML)).
    Each of these tools has some unique strengths. They either run lightweight and
    are easy to use, like BentoML, or they make model deployment easy and fast in
    Kubernetes, as do Seldon Core and KServe. Despite the diversity of the serving
    tools, they have a lot in common: they all need to pack models in a certain format,
    define a model wrapper and configuration file to execute the model, upload models
    to a repository, and send prediction requests via a gRPC or HTTP/REST endpoint.
    By reading the TorchServe, TensorFlow, and Triton examples in this chapter, you
    should be able to explore other tools on your own.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 开源服务工具的列表非常广泛，包括 KServe ([https://www.kubeflow.org/docs/external-add-ons/kserve/](https://www.kubeflow.org/docs/external-add-ons/kserve/))、Seldon
    Core ([https://www.seldon.io/solutions/open-source-projects/core](https://www.seldon.io/solutions/open-source-projects/core))
    和 BentoML ([https://github.com/bentoml/BentoML](https://github.com/bentoml/BentoML))。这些工具中的每一个都有其独特的优势。它们要么运行轻量级且易于使用，如
    BentoML，要么像 Seldon Core 和 KServe 一样，使模型在 Kubernetes 中的部署变得简单快捷。尽管服务工具种类繁多，但它们有很多共同点：它们都需要以某种格式打包模型，定义一个模型包装器和配置文件以执行模型，将模型上传到存储库，并通过
    gRPC 或 HTTP/REST 端点发送预测请求。通过阅读本章中的 TorchServe、TensorFlow 和 Triton 示例，你应该能够自行探索其他工具。
- en: Before we end the serving tools discussion, we want to call out KServe specifically.
    KServe is a collaboration on model serving between several established high-tech
    companies, including Seldon, Google, Bloomberg, NVIDIA, Microsoft, and IBM. This
    open source project is worth your attention because it is designed to create a
    standardized solution for common machine learning serving problems.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束服务工具的讨论之前，我们特别想提及 KServe。KServe 是由包括 Seldon、Google、Bloomberg、NVIDIA、Microsoft
    和 IBM 等几家知名高科技公司共同合作开发的一个模型服务项目。这个开源项目值得你的关注，因为它旨在为常见的机器学习服务问题提供一个标准化的解决方案。
- en: KServe aims to provide a serverless inference solution on Kubernetes. It provides
    an abstract model serving interface that works for common machine learning frameworks
    like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: KServe 旨在在 Kubernetes 上提供无服务器推理解决方案。它提供了一个适用于常见机器学习框架（如 TensorFlow、XGBoost、scikit-learn、PyTorch
    和 ONNX）的抽象模型服务接口。
- en: From our point of view, KServe’s main contribution is that it creates a standard
    serving interface that works for all major serving tools. For example, all the
    serving tools we mentioned previously now support the KServe model inference protocol.
    This means we can use only one set of inference APIs (the KServe API) to query
    any model hosted by different serving tools, such as Triton, TorchServe, and TensorFlow.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的角度来看，KServe 的主要贡献是它创建了一个适用于所有主要服务工具的标准服务接口。例如，我们之前提到的所有服务工具现在都支持 KServe
    模型推理协议。这意味着我们可以仅使用一套推理 API（KServe API）来查询由不同服务工具托管的所有模型，例如 Triton、TorchServe 和
    TensorFlow。
- en: 'Another strength of KServe is that it is designed to provide a serverless solution
    natively on Kubernetes. KServe uses Knative to take care of the network routing,
    model worker autoscaling (even to zero), and model revision tracking. With a simple
    config (see the following example), you can deploy a model to your Kubernetes
    cluster and then use the standardized API to query it:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: KServe 的另一个优点是它被设计为在 Kubernetes 上提供原生的无服务器解决方案。KServe 使用 Knative 来处理网络路由、模型工作负载自动缩放（甚至到零）和模型修订跟踪。通过一个简单的配置（见以下示例），您可以将模型部署到您的
    Kubernetes 集群，然后使用标准化的 API 来查询它：
- en: '[PRE26]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ A sample model deployment config for KServe
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ KServe 的一个示例模型部署配置
- en: ❷ A backend server type
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 后端服务器类型
- en: ❸ A model file location
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 模型文件位置
- en: Behind the scenes, KServe uses different serving tools to run inference, such
    as TensorFlow Serving and Triton. KServe provides the benefit of hiding all the
    details behind a simple Kubernetes CRD config. In the previous example, the `InferenceService`
    CRD config hides the work, including prediction server setup, model copy, model
    version tracking, and prediction request routing.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，KServe 使用不同的服务工具来运行推理，例如 TensorFlow Serving 和 Triton。KServe 通过一个简单的 Kubernetes
    CRD 配置隐藏了所有细节，从而提供了这一好处。在先前的示例中，`InferenceService` CRD 配置隐藏了包括预测服务器设置、模型复制、模型版本跟踪和预测请求路由在内的所有工作。
- en: As the book is being written, KServe’s newer version (v2) is still in beta.
    Although it’s not quite mature, its unique advantage of a standardized inference
    protocol across platform support and serverless model deployment makes it stand
    out among other approaches. If you want to set up a large serving platform that
    works for all major training frameworks on Kubernetes, KServe is worth your attention.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本书的编写，KServe 的新版本（v2）仍在测试中。尽管它还不完全成熟，但其跨平台支持和无服务器模型部署的独特优势使其在众多方法中脱颖而出。如果您想搭建一个适用于
    Kubernetes 上所有主要训练框架的大型服务平台，KServe 值得您关注。
- en: 7.4.5 Integrating a serving tool into an existing serving system
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.5 将服务工具集成到现有服务系统中
- en: In many cases, replacing an existing prediction service with a new serving backend
    is not an option. Each serving tool has its own requirements for model storage,
    model registration, and inference request format. These requirements sometimes
    conflict with the existing system’s prediction interface and the internal model
    metadata and file systems. To introduce new technology without disrupting the
    business, we usually take an integration approach instead of completely replacing
    it.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，用新的服务后端替换现有的预测服务不是一个选择。每个服务工具都有自己的模型存储、模型注册和推理请求格式的要求。这些要求有时与现有系统的预测接口以及内部模型元数据和文件系统相冲突。为了在不影响业务的情况下引入新技术，我们通常采取集成方法而不是完全替换它。
- en: 'Here, we use the Triton server as an example to show how to integrate a serving
    tool into an existing prediction service. In this example, we assume three things:
    first, the existing prediction service runs in Kubernetes; second, the existing
    prediction service’s web inference interface is not allowed to change; and third,
    there is a model storage system that stores model files in cloud storage, such
    as Amazon S3\. Figure 7.11 shows the process.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以 Triton 服务器为例，展示如何将服务工具集成到现有的预测服务中。在这个例子中，我们假设三个条件：首先，现有的预测服务运行在 Kubernetes
    上；其次，现有的预测服务的网络推理接口不允许更改；第三，存在一个模型存储系统，该系统将模型文件存储在云存储中，例如 Amazon S3。图 7.11 展示了该过程。
- en: '![](../Images/07-11.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-11.png)'
- en: Figure 7.11 A proposal to integrate a list of Triton server instances into an
    existing serving system
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 将一系列 Triton 服务器实例集成到现有服务系统中的建议
- en: Figure 7.11 (A) illustrates the system overview. A list of Triton server Kubernetes
    pods is added behind the existing prediction API. With the Kubernetes load balancer,
    a prediction request can land on any Triton pod. We also add a shared volume that
    all Triton pods can access; this shared volume acts as a shared Triton model repository
    for all Triton instances.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11（A）说明了系统概述。在现有的预测API后面添加了一个Triton服务器Kubernetes pod列表。借助Kubernetes负载均衡器，预测请求可以落在任何Triton
    pod上。我们还添加了一个所有Triton pod都可以访问的共享卷；这个共享卷作为所有Triton实例的共享Triton模型仓库。
- en: 'Figure 7.11 (B) shows what’s inside a Triton server Kubernetes pod. Each Triton
    pod has two Docker containers: a Triton Server Container and a sidecar container.
    The Triton server container is the Triton inference server we discussed in section
    7.4.3\. The model prediction happens in this container, and we can simply treat
    this container as a black box. The sidecar container acts as an adapter/proxy
    to prepare what Triton needs before forwarding the prediction request to the Triton
    container. This sidecar container downloads the model from cloud storage to the
    Triton local model repository (the shared volume), calls Triton to register the
    model, and converts the prediction request to the Triton API call.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11（B）展示了Triton服务器Kubernetes pod内部结构。每个Triton pod包含两个Docker容器：一个Triton服务器容器和一个边车容器。Triton服务器容器是我们第7.4.3节中讨论的Triton推理服务器。模型预测发生在这个容器中，我们可以简单地将这个容器视为一个黑盒。边车容器充当适配器/代理，在将预测请求转发到Triton容器之前准备Triton所需的内容。这个边车容器从云存储下载模型到Triton本地模型仓库（共享卷），调用Triton注册模型，并将预测请求转换为Triton
    API调用。
- en: By using this integration approach, all the changes happen inside the prediction
    service. The public prediction API and the external model storage system remain
    untouched, and our users won’t be affected when we switch to a Triton backend.
    Although we use a specific tool (Triton) and a specific infrastructure (Kubernetes)
    to demo the idea, you can apply this pattern to any other system as long as they
    use Docker.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种集成方法，所有更改都在预测服务内部发生。公共预测API和外部模型存储系统保持不变，当我们切换到Triton后端时，我们的用户不会受到影响。尽管我们使用了一个特定的工具（Triton）和特定的基础设施（Kubernetes）来演示这个想法，但你可以在任何使用Docker的其他系统中应用这种模式。
- en: Note Because the Triton server supports major training frameworks and KServe
    provides a standardized serving protocol, we can combine them to produce a serving
    system that works for all kinds of models trained by different frameworks.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于Triton服务器支持主要的训练框架，而KServe提供了一个标准化的服务协议，我们可以将它们结合起来，产生适用于由不同框架训练的所有类型模型的系统。
- en: 7.5 Releasing models
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 发布模型
- en: Releasing a model is the act of deploying the newly trained model to the prediction
    service and exposing it to users. Automating the model deployment and supporting
    model evaluation are the two main problems we need to address when building model
    serving systems in production.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 发布模型是将新训练的模型部署到预测服务并对其用户公开的行为。在生产环境中构建模型服务系统时，自动化模型部署和支持模型评估是我们需要解决的两个主要问题。
- en: First, when the training service finishes the model building, the model should
    be published to the prediction service in the production environment automatically.
    Second, the newly published model and its previous versions should all be accessible
    in the prediction service, so we can evaluate them in the same environment and
    make a fair comparison. In this section, we propose a three-step model release
    process to address these challenges.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当训练服务完成模型构建后，模型应自动发布到生产环境中的预测服务。其次，新发布的模型及其所有先前版本都应在预测服务中可访问，这样我们就可以在相同的环境中评估它们并进行公平的比较。在本节中，我们提出一个三步模型发布流程来解决这些挑战。
- en: First, the data scientist (Alex) or training service registers the recently
    produced model (consisting of the model’s files and its metadata) to a metadata
    store—a cloud metadata and artifact storage system that will be discussed in the
    next chapter. Second, Alex runs the model evaluation on the newly registered models.
    He can test the performance of these models by sending prediction requests with
    their specific model versions to the prediction service. The prediction service
    has a built-in mechanism to load any specific version of a model from the metadata
    store.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据科学家（Alex）或训练服务将最近生产的模型（包括模型的文件及其元数据）注册到元数据存储中——这是一个将在下一章讨论的云元数据和工件存储系统。其次，Alex在新注册的模型上运行模型评估。他可以通过向预测服务发送带有特定模型版本的预测请求来测试这些模型的性能。预测服务内置了一种机制，可以从元数据存储中加载任何特定版本的模型。
- en: Third, Alex sets the best-performing model version as the release model version
    in the metadata store. Once this is set, the selected version of the model will
    go public! Customer applications will unknowingly start using the new release
    version of the model from the prediction service. Figure 7.12 illustrates this
    three-step process.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，Alex将性能最佳的模型版本设置为元数据存储中的发布模型版本。一旦设置完成，所选版本的模型将公开！客户应用程序将不知不觉地开始从预测服务使用新发布的模型版本。图7.12说明了这一三步过程。
- en: '![](../Images/07-12.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-12.png)'
- en: 'Figure 7.12 The model release process workflow: (1) registers models in the
    model metadata store; (2) loads arbitrary versions of a model to serve prediction
    requests; and (3) releases the model in the metadata store'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 模型发布流程：1）在模型元数据存储中注册模型；2）加载任意版本的模型以处理预测请求；3）在元数据存储中发布模型
- en: In the next three sections, we will delve into the three model release steps
    (pictured in figure 7.12) one by one. As we do this, we will also explore the
    details of the metadata store and its interactions with storage and with the prediction
    service. Let’s get started!
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个部分中，我们将逐一深入探讨三个模型发布步骤（如图7.12所示）。在这个过程中，我们还将探索元数据存储的细节及其与存储和预测服务的交互。让我们开始吧！
- en: 7.5.1 Registering a model
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 注册模型
- en: In most deep learning systems, there is a storage service to store models. In
    our example, this service is called the *metadata* *store*; it is used to manage
    the metadata of the artifacts produced by the deep learning system, such as models.
    The metadata and artifact store service will be discussed in detail in the next
    chapter.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数深度学习系统中，都有一个用于存储模型的存储服务。在我们的例子中，这个服务被称为*元数据* *存储*；它用于管理深度学习系统产生的工件（如模型）的元数据。元数据和工件存储服务将在下一章中详细讨论。
- en: To register a model to the metadata store, we usually need to provide model
    files and model metadata. Model files can be model weights, embeddings, and other
    dependent files to execute the model. Model metadata can be any data that describes
    the fact of the model, such as the model name, model ID, model version, training
    algorithm, dataset info, and training execution metrics. Figure 7.13 illustrates
    how metadata stores model metadata and model files internally.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型注册到元数据存储中，我们通常需要提供模型文件和模型元数据。模型文件可以是模型权重、嵌入和其他执行模型的依赖文件。模型元数据可以是描述模型事实的任何数据，例如模型名称、模型ID、模型版本、训练算法、数据集信息以及训练执行指标。图7.13说明了元数据存储如何内部存储模型元数据和模型文件。
- en: '![](../Images/07-13.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-13.png)'
- en: Figure 7.13 The internal storage design of the metadata store; model metadata
    are stored as object files with lookup tables in front of them.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 元数据存储的内部存储设计；模型元数据以对象文件的形式存储，其前有查找表。
- en: 'In figure 7.13, we can see the metadata store has two sections: the model lookup
    table and the model metadata list. The model metadata list is just pure metadata
    storage; all the model metadata objects are stored in this list. The model lookup
    table is used as an index table for quick searches. Each record in the lookup
    table points to an actual metadata object in the metadata list.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.13中，我们可以看到元数据存储有两个部分：模型查找表和模型元数据列表。模型元数据列表仅仅是纯元数据存储；所有的模型元数据对象都存储在这个列表中。模型查找表用作快速搜索的索引表。查找表中的每条记录都指向元数据列表中的一个实际元数据对象。
- en: Training service can register models automatically to the metadata store after
    training completes. Data scientists can also register models manually, which often
    happens when data scientists want to deploy the model they build locally (without
    using the deep learning system).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 训练服务可以在训练完成后自动将模型注册到元数据存储中。数据科学家也可以手动注册模型，这通常发生在数据科学家想要部署他们本地构建的模型（不使用深度学习系统）时。
- en: When the metadata store receives a model register request, first, it creates
    a metadata object for this model. Second, it updates the model lookup table by
    adding a new search record; the record enables us to find that model metadata
    object by using the model name and version. Besides searching the lookup table
    by using the model name and version, the metadata store also allows a model metadata
    search by using the model ID.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 当元数据存储接收到模型注册请求时，首先，它为该模型创建一个元数据对象。其次，它通过添加新的搜索记录来更新模型查找表；该记录使我们能够通过使用模型名称和版本找到该模型元数据对象。除了通过使用模型名称和版本搜索查找表之外，元数据存储还允许通过使用模型ID进行模型元数据搜索。
- en: The actual model files are stored in the artifact store—a cloud object storage,
    such as Amazon S3\. A model’s storage location in the artifact store is saved
    in the model’s metadata object as a pointer.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 实际模型文件存储在工件存储中——一个云对象存储，例如Amazon S3。模型在工件存储中的存储位置作为指针保存在模型的元数据对象中。
- en: 'Figure 7.13 shows two search records in the model lookup table for model A:
    versions 1.0.0 and 1.1.0\. Each search record maps to a different model metadata
    object (respectively, ID = 12345 and ID = 12346). With this storage structure,
    we can find any model metadata by using the model name and model version; for
    example, we can find model metadata object ID = 12346 by searching “model A” and
    version “1.1.0.”'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13显示了模型查找表中模型A的两个搜索记录：版本1.0.0和1.1.0。每个搜索记录映射到不同的模型元数据对象（分别，ID = 12345和ID
    = 12346）。使用这种存储结构，我们可以通过使用模型名称和模型版本找到任何模型元数据；例如，我们可以通过搜索“模型A”和版本“1.1.0”来找到ID =
    12346的模型元数据对象。
- en: Using the model’s canonical names and versions to find the actual metadata and
    model files is foundational to the prediction service’s ability to serve different
    model versions at the same time. Let’s see how the metadata store is used in the
    prediction service in the next section.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型的规范名称和版本来查找实际的元数据和模型文件是预测服务能够同时服务不同模型版本的基础。让我们在下一节中看看元数据存储如何在预测服务中使用。
- en: 7.5.2 Loading an arbitrary version of a model in real time with a prediction
    service
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 使用预测服务实时加载任意版本的模型
- en: To make decisions on which model version to use in production, we want to evaluate
    the model performance of each model version fairly (in the same environment) and
    easily (using the same API). To do so, we can call the prediction service to run
    prediction requests with different model versions.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在生产中对使用哪个模型版本做出决策，我们希望公平（在相同的环境中）且容易（使用相同的API）地评估每个模型版本的性能。为此，我们可以调用预测服务来运行不同模型版本的预测请求。
- en: In our proposal, the prediction service loads a model in real time from the
    metadata store when it receives a prediction request. Data scientists can allow
    the prediction services to use any model version to run the prediction by defining
    the model name and version in the prediction request. Figure 7.14 illustrates
    the process.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的提案中，预测服务在接收到预测请求时从元数据存储中实时加载模型。数据科学家可以通过在预测请求中定义模型名称和版本，允许预测服务使用任何模型版本来运行预测。图7.14说明了这个过程。
- en: '![](../Images/07-14.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-14.png)'
- en: Figure 7.14 Model serving in prediction service with the metadata store
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 模型服务中的元数据存储
- en: 'Figure 7.14 shows the prediction service loads models specified in the serving
    request in real time. When receiving a prediction request, the routing layer first
    finds the requested model in the metadata store, downloads the model files, and
    then passes the request to the backend predictor. Here is a detailed explanation
    of the seven steps of the runtime model loading and serving process:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14显示了预测服务实时加载在服务请求中指定的模型。当接收到预测请求时，路由层首先在元数据存储中查找请求的模型，下载模型文件，然后将请求传递给后端预测器。以下是运行时模型加载和提供服务过程的七个步骤的详细说明：
- en: The user sends prediction requests to the prediction service. In the request,
    they can specify which model to use by providing the model name and version (`/predict/{model_name}/{version}`)
    or model ID (`/predict/{model_id}`).
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户向预测服务发送预测请求。在请求中，他们可以通过提供模型名称和版本（`/predict/{model_name}/{version}`）或模型ID（`/predict/{model_id}`）来指定要使用哪个模型。
- en: The routing layer inside the prediction service searches the metadata store
    and finds the model metadata object.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测服务内部的路由层搜索元数据存储并找到模型元数据对象。
- en: The routing layer then downloads the model files to a shared disk that all the
    predictors can access.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，路由层将模型文件下载到所有预测器都可以访问的共享磁盘。
- en: By checking the model metadata, such as the algorithm type, the routing layer
    routes the prediction request to the correct backend predictor.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过检查模型元数据，例如算法类型，路由层将预测请求路由到正确的后端预测器。
- en: The predictor loads the model from the shared disk.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测器从共享磁盘加载模型。
- en: The predictor handles data preprocessing, executes the model, performs postprocessing,
    and returns the result to the routing layer.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测器处理数据预处理，执行模型，执行后处理，并将结果返回给路由层。
- en: The routing layer returns prediction results to the caller.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 路由层将预测结果返回给调用者。
- en: 7.5.3 Releasing the model by updating the default model version
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 通过更新默认模型版本来发布模型
- en: After model evaluation, the last step of the model release is letting the customers
    consume the newly verified model version in the prediction service. We want the
    model release process to happen unknowingly, so customers aren’t aware of the
    underlying model version changes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型评估之后，模型发布的最后一步是让客户在预测服务中消费经过验证的新模型版本。我们希望模型发布过程是未知的，这样客户就不会意识到底层模型版本的变化。
- en: In step 1 of the previous section (7.5.2), users can request a model serving
    on any specified model version by using the `/predict/{model_name}/{version}`
    API. This capability is crucial to evaluating multiple versions of the same model,
    so we can prevent model performance regression.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节（7.5.2）的第1步中，用户可以通过使用`/predict/{model_name}/{version}` API请求任何指定的模型版本进行模型服务。这种能力对于评估同一模型的多个版本至关重要，因此我们可以防止模型性能退化。
- en: But in the production scenario, we don’t expect our customers to track the model
    versions and model IDs. Alternatively, we can define a few static version strings
    as variables to represent the newly released models and let customers use them
    in the prediction request instead of using the real model version.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 但在生产场景中，我们不希望我们的客户跟踪模型版本和模型ID。作为替代，我们可以定义几个静态版本字符串作为变量来表示新发布的模型，并让客户在预测请求中使用它们，而不是使用实际的模型版本。
- en: For example, we can define two special static model versions or tags, such as
    `STG` and `PROD`, which represent the preproduction and production environments,
    respectively. If the model version associated with the `PROD` tag for model A
    is `1.0.0`, a user can call `/predict/model_A/PROD` and the prediction service
    will load model A and version `1.0.0` to run model serving. When we upgrade the
    newly released model version to `1.2.0`—by associating the `PROD` tag to version
    1.2.0—the `/predict/model_A/PROD` request will land on model version `1.2.0`.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以定义两个特殊的静态模型版本或标签，如`STG`和`PROD`，分别代表预生产和生产环境。如果与`PROD`标签关联的模型A的模型版本是`1.0.0`，用户可以调用`/predict/model_A/PROD`，预测服务将加载模型A和版本`1.0.0`以运行模型服务。当我们通过将`PROD`标签关联到版本1.2.0来升级新发布的模型版本时，`/predict/model_A/PROD`请求将落在版本`1.2.0`上。
- en: With the special static version/tag strings, prediction users don’t need to
    remember model ID or versions; they can just use `/predict/{model_name}/PROD`
    to send prediction requests to consume the newly released model. Behind the scenes,
    we (data scientists or engineers) maintain the mapping between these special strings
    and the actual version in the metadata store’s lookup table, so the prediction
    service knows which model version to download for a `/STG` or `/PROD` request.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特殊的静态版本/标签字符串，预测用户不需要记住模型ID或版本；他们只需使用`/predict/{model_name}/PROD`来发送预测请求以消费新发布的模型。在幕后，我们（数据科学家或工程师）维护这些特殊字符串与元数据存储查找表中的实际版本之间的映射，以便预测服务知道对于`/STG`或`/PROD`请求应该下载哪个模型版本。
- en: In our proposal, we named the operation of mapping a specific model version
    to the static model version the *model release operation*. Figure 7.15 illustrates
    the model release process.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的提案中，我们将将特定模型版本映射到静态模型版本的操作命名为*模型发布操作*。图7.15说明了模型发布过程。
- en: '![](../Images/07-15.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-15.png)'
- en: Figure 7.15 Model serving in prediction service with the metadata store
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 预测服务中的模型服务与元数据存储
- en: In figure 7.15, data scientists first register model A, version 1.0.0 to model
    A, version `PROD` in the metadata store. Then in the model lookup table, the (`Model`
    `A,` `PROD)` record changes to point to the actual model object record (`ModelA,`
    `version:` `1.0.0)`. So when users call `/predict/ModelA/PROD` in the prediction
    service, they are actually calling `/predict/ModelA/1.0.0`.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.15中，数据科学家首先将模型A，版本1.0.0注册到元数据存储中的模型A，版本`PROD`。然后在模型查找表中，(`Model` `A,` `PROD`)记录更改为指向实际的模型对象记录(`ModelA,`
    `version:` `1.0.0`)。因此，当用户在预测服务中调用`/predict/ModelA/PROD`时，他们实际上是在调用`/predict/ModelA/1.0.0`。
- en: Next, when the prediction service receives a prediction request with a model
    version equal to `STG` or `PROD`, the service will search the lookup table in
    the metadata store and use the actual model version, which is registered to `PROD`,
    to download model files. In figure 7.15, the prediction service will load model
    `ModelA,version:` 1.0.0 for the `/ModelA/PROD` `request`, and it will load model
    `ModelA,version:` 1.1.0 for the `/ModelA/STG` `request`.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，当预测服务接收到一个模型版本等于`STG`或`PROD`的预测请求时，服务将在元数据存储库的查找表中搜索，并使用实际注册到`PROD`的实际模型版本来下载模型文件。在图7.15中，预测服务将为`/ModelA/PROD`请求加载`ModelA,version:`
    1.0.0的模型，并为`/ModelA/STG`请求加载`ModelA,version:` 1.1.0的模型。
- en: For future model releases, data scientists only need to update the model records
    to map the latest model version to `STG` and `PROD` in the metadata store’s lookup
    table. The prediction service will load the new model version automatically for
    new prediction requests. All of these operations happen automatically and are
    imperceptible to users.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来模型版本的发布，数据科学家只需要更新模型记录，将最新的模型版本映射到元数据存储库的查找表中`STG`和`PROD`。预测服务将自动为新预测请求加载新的模型版本。所有这些操作都是自动发生的，并且对用户来说是不可察觉的。
- en: Note The proposed release workflow is not the only way to release models. Model
    release approaches are highly dependent on a company’s internal DevOps process
    and the prediction service design, so there is no single best design on this topic.
    We hope by reading the problem analysis and the proposed solution in section 7.5,
    you can derive a model release process that suits your situation.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所提出的发布工作流程并非发布模型的唯一方式。模型发布方法高度依赖于公司的内部DevOps流程和预测服务设计，因此在这个问题上没有单一的最佳设计。我们希望您通过阅读第7.5节中的问题分析和提出的解决方案，可以推导出适合您情况的模型发布流程。
- en: 7.6 Postproduction model monitoring
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 产后模型监控
- en: Compared with monitoring other services, such as data management, in machine
    learning systems, the job is still not complete after the model goes into production.
    We need not only to monitor and maintain the prediction service itself but also
    look at *the performance of models* that the service serves. Model drifting is
    a shift in the knowledge domain distribution that no longer matches the training
    dataset and leads to the deterioration of the model performance. This can happen
    while the prediction service is completely healthy because model inference runs
    independently from the prediction service.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 与监控其他服务，如数据管理，在机器学习系统中，模型投入生产后工作并未完成。我们需要监控和维护预测服务本身，还要关注服务所服务的*模型性能*。模型漂移是指知识域分布的变化，不再与训练数据集匹配，导致模型性能下降。这可能会在预测服务完全健康的情况下发生，因为模型推理是独立于预测服务运行的。
- en: To battle model drifting, data scientists need to retrain the model with new
    data or rebuild the model with an improved training algorithm. This sounds like
    a data science project on the surface, but it requires a lot of underlying engineering
    work, such as collecting and analyzing the model metrics from the prediction service
    to detect model drifting. In this section, we discuss model monitoring from an
    engineering perspective and look at the role that engineers can play in the monitoring
    process.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗模型漂移，数据科学家需要用新数据重新训练模型，或者用改进的训练算法重建模型。表面上这听起来像是一个数据科学项目，但它需要大量的底层工程工作，例如收集和分析预测服务中的模型指标以检测模型漂移。在本节中，我们从工程的角度讨论模型监控，并探讨工程师在监控过程中的作用。
- en: 7.6.1 Metric collection and quality gate
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.1 指标收集和质量门
- en: The two most important areas where engineers can contribute are *model metric
    collection* and *model quality gate setup*. Let us explain.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师可以贡献的两个最重要的领域是*模型指标收集*和*模型质量门设置*。让我们来解释一下。
- en: To run an analysis to detect model drift, data scientists need data to analyze,
    and engineers can find ways to deliver the necessary data (metrics). Although
    engineers would have to create a separate data pipeline to collect model performance
    metrics, it would be overkill in most cases. Normally, model performance metrics
    can be collected and visualized with the existing telemetry system (like Datadog)
    and logging system (like Sumo and Splunk). So do yourself a favor and try to fully
    utilize the existing logging and metric systems you already have, instead of doing
    the heavy lifting of building a new metric system.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行一个检测模型漂移的分析，数据科学家需要用于分析的数据，工程师可以找到方法来提供必要的数据（指标）。尽管工程师可能需要创建一个独立的数据管道来收集模型性能指标，但在大多数情况下这将是过度行为。通常，模型性能指标可以通过现有的遥测系统（如Datadog）和日志系统（如Sumo和Splunk）收集和可视化。所以，请自己行个方便，尽量充分利用现有的日志和指标系统，而不是承担构建新指标系统的繁重工作。
- en: Engineers can also help with building model-quality gates. Engineers can work
    with data scientists to automate their troubleshooting steps, such as checking
    data quality and generating model inference analysis reports. With a given threshold,
    these checks will eventually form a model quality gate.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师还可以帮助构建模型质量门。工程师可以与数据科学家合作，自动化他们的故障排除步骤，例如检查数据质量和生成模型推理分析报告。给定一个阈值，这些检查最终将形成一个模型质量门。
- en: 7.6.2 Metrics to collect
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.2 需要收集的指标
- en: 'Theoretically, we need to collect at least five kinds of metrics to support
    model performance measurements. They are prediction tracing, the date of the prediction,
    model versions, observation, and observation rate and date. Let’s look at them
    one by one:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们需要收集至少五种类型的指标来支持模型性能测量。它们是预测追踪、预测日期、模型版本、观察和观察率及日期。让我们逐一看看：
- en: '*Prediction tracing*—We normally track each prediction request by assigning
    it a unique request ID, but this is not enough. For some complicated scenarios,
    such as PDF scanning, we composite different types of model predictions together
    to produce a final result. For example, we first send a PDF doc to an OCR (optical
    character recognition) model to extract text information and then send the text
    to an NLP (natural language processing) model to recognize the targeted entities.
    In this case, besides assigning a unique request ID for a parent prediction request,
    we can also assign a `groupRequestID` to each sub/child prediction request, so
    we can group all the associated prediction requests when troubleshooting.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测追踪*——我们通常通过为每个预测请求分配一个唯一的请求ID来跟踪每个预测请求，但这还不够。对于一些复杂的场景，例如PDF扫描，我们将不同类型的模型预测组合起来以产生最终结果。例如，我们首先将PDF文档发送到OCR（光学字符识别）模型以提取文本信息，然后将文本发送到NLP（自然语言处理）模型以识别目标实体。在这种情况下，除了为父预测请求分配一个唯一的请求ID外，我们还可以为每个子/子预测请求分配一个`groupRequestID`，这样我们就可以在故障排除时将所有相关的预测请求分组。'
- en: '*Date of the prediction*—Normally, a prediction request completes within a
    second. To track the date of a prediction, we can either use prediction start
    time or complete time because there is not much difference. But for cases like
    fraud detection, the prediction’s completion timestamp might be a lot different
    from the prediction start timestamp because it can take multiple days of user
    activities as input.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测日期*——通常，一个预测请求在一秒内完成。为了跟踪预测日期，我们可以使用预测开始时间或完成时间，因为两者之间没有太大差异。但是，对于像欺诈检测这样的案例，预测的完成时间戳可能与预测开始时间戳有很大不同，因为可能需要多天的用户活动作为输入。'
- en: '*Model version*—To map model performance data to the exact model file, we need
    to know the model version. Furthermore, when we combine multiple models to serve
    one prediction request, the version of every model needs to be tracked in logs.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型版本*——为了将模型性能数据映射到确切的模型文件，我们需要知道模型版本。此外，当我们结合多个模型来处理一个预测请求时，每个模型的版本都需要在日志中进行跟踪。'
- en: '*Observation*—The prediction result needs to be logged along with prediction
    input for future comparison. Additionally, we can provide a feedback or investigation
    API for customers to report model performance concerns. By using the feedback
    API, customers can report the model ID, expected prediction result, and current
    prediction result.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*观测*—预测结果需要与预测输入一起记录，以便将来进行比较。此外，我们可以提供一个反馈或调查API，供客户报告模型性能问题。通过使用反馈API，客户可以报告模型ID、预期预测结果和当前预测结果。'
- en: '*Observation date and rate*—Many times, observations are collected manually,
    and the frequency of observation needs to be logged as well. Data scientists need
    the date and rate to decide whether the data can statistically represent the model’s
    overall performance.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*观测日期和频率*—很多时候，观测数据是手动收集的，同时观测的频率也需要被记录。数据科学家需要日期和频率来决定数据是否能够从统计上代表模型的总体性能。'
- en: It is great that you have read this far! Model serving is an essential component
    of a machine learning system because external business applications depend on
    it. As types of models, numbers of prediction requests, and types of inference
    (online/offline) increase, many model serving frameworks/systems are invented,
    and they become increasingly complex. If you follow the serving mental model introduced
    in chapters 6 and 7, starting with how a model is loaded and executed, you can
    easily navigate these serving systems, regardless of how large the codebase or
    the number of components is.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 你能读到这一步真是太好了！模型服务是机器学习系统的一个基本组成部分，因为外部业务应用依赖于它。随着模型类型、预测请求数量和推理类型（在线/离线）的增加，许多模型服务框架/系统被发明出来，并且变得越来越复杂。如果你遵循第6章和第7章中介绍的模型服务思维模型，从模型加载和执行开始，你可以轻松地导航这些服务系统，无论代码库有多大或组件数量有多少。
- en: Summary
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The model service sample in this chapter is made of a frontend API component
    and a backend model predictor container. Because the predictor is built on top
    of the intent model training code in chapter 3, it can only serve intent classification
    models.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中的模型服务示例由前端 API 组件和后端模型预测器容器组成。因为预测器建立在第3章中意图模型训练代码的基础上，所以它只能服务意图分类模型。
- en: The model server sample is composed of the same frontend API as in chapter 3
    and a different backend—TorchServe predictor. The TorchServe backend is not limited
    to intent classification models; it can serve arbitrary PyTorch models. This is
    a great advantage for the model server approach over the model service approach.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务器示例由第3章中相同的 frontend API 和不同的后端——TorchServe 预测器组成。TorchServe 后端不仅限于意图分类模型；它可以服务任意
    PyTorch 模型。这是模型服务器方法相对于模型服务方法的一个巨大优势。
- en: For implementing model server approaches, we recommend using existing tools—for
    example, the Triton server—instead of building your own.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实现模型服务器方法，我们建议使用现有的工具——例如，Triton 服务器——而不是自己构建。
- en: The model service approach works for single application scenarios; it can be
    implemented quickly, and you have full control of the code implementation of the
    end-to-end workflow.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务方法适用于单一应用场景；它可以快速实现，并且你对端到端工作流程的代码实现有完全的控制权。
- en: The model server approach fits platform scenarios; it can greatly reduce development
    and maintenance efforts when the serving system needs to support five or more
    different types of models.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务器方法适合平台场景；当服务系统需要支持五种或更多不同类型的模型时，它可以大大减少开发和维护的工作量。
- en: TorchServe, TensorFlow Serving, and Triton are all solid open source model serving
    tools, and they all take a model server approach. If applicable, we recommend
    Triton because it is compatible with most model training frameworks and has a
    performance advantage in terms of GPU acceleration.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TorchServe、TensorFlow Serving和Triton都是优秀的开源模型服务工具，它们都采用模型服务器的方法。如果适用，我们推荐使用Triton，因为它与大多数模型训练框架兼容，并且在GPU加速方面具有性能优势。
- en: KServe provides a standard serving interface that works for all major serving
    tools, including TensorFlow Serving, TorchServe, and Triton. KServe can greatly
    improve the compatibility of our serving system because we can use a single set
    API to run model serving with different backends.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KServe 提供了一个适用于所有主要服务工具的标准服务接口，包括 TensorFlow Serving、TorchServe 和 Triton。KServe
    可以极大地提高我们服务系统的兼容性，因为我们可以使用单个 API 来运行具有不同后端的模型服务。
- en: Releasing a new model or new version of the model serving system in production
    shouldn’t be an afterthought; we need to consider it properly in the design phase.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产中发布新的模型或模型服务系统的版本不应是事后考虑的事情；我们需要在设计阶段就妥善考虑这一点。
- en: Model metric collection and model quality gates are the two areas on which engineers
    need to focus for model performance monitoring.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型指标收集和模型质量门是工程师在模型性能监控上需要关注的两个领域。
