- en: 7 Evolutionary convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 进化卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Convolutional neural networks with a Keras primer
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有Keras入门的卷积神经网络
- en: Defining a neural network architecture with a gene sequence
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基因序列定义神经网络架构
- en: Building a custom crossover operator
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自定义交叉算子
- en: Applying a custom mutation operator
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用自定义变异算子
- en: Evolving the best convolutional network architecture for a given dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为给定数据集进化最佳卷积网络架构
- en: The last chapter showed us the limits of evolutionary algorithms when applied
    to a complex problem like parameter search. As we have seen, genetic algorithms
    can provide excellent results on a certain class of problems. However, they fail
    to deliver when employed for larger image classification networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一章向我们展示了当将进化算法应用于像参数搜索这样的复杂问题时，其局限性。正如我们所见，遗传算法在处理某一类问题时可以提供出色的结果。然而，当用于更大的图像分类网络时，它们却无法达到预期效果。
- en: In this chapter, we continue looking at larger networks for image classification.
    However, this time instead of optimizing parameter weights or model hyperparameters,
    we look at improving network architecture. More specifically, we cover the network
    architecture of convolutional neural networks (CNNs).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续探讨用于图像分类的更大网络。然而，这一次，我们不是优化参数权重或模型超参数，而是关注改进网络架构。更具体地说，我们涵盖了卷积神经网络（CNN）的网络架构。
- en: CNNs were instrumental to the adoption of DL for image classification and other
    tasks. They are a fantastic tool in the DL practitioner’s toolbelt but are often
    misunderstood and under-utilized. In the next section, we review CNN models and
    how they are built in TensorFlow and Keras.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CNN对于图像分类和其他任务的深度学习采用起到了关键作用。它们是深度学习实践者工具箱中的绝佳工具，但通常被误解和低效使用。在下一节中，我们将回顾CNN模型以及它们在TensorFlow和Keras中的构建方式。
- en: 7.1 Reviewing convolutional neural networks in Keras
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 在Keras中回顾卷积神经网络
- en: This section’s project is a review of constructing CNN models for image classification
    with Keras. While we cover some of the basics of CNN, our focus is more on the
    details of what makes building these types of networks difficult.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的项目是对使用Keras构建图像分类CNN模型的回顾。虽然我们涵盖了CNN的一些基础知识，但我们的重点更多地在于构建这些类型网络所面临的细节问题。
- en: The future of CNN
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的未来
- en: CNN layers are quickly being replaced with more advanced technologies, like
    residual networks and attention mechanisms (aka transformers). The same principles
    we learn in this chapter could be applied to optimizing these other architectures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN层正迅速被更先进的技术所取代，如残差网络和注意力机制（即转换器）。我们在本章中学到的相同原则可以应用于优化这些其他架构。
- en: In this project, we perform image classification over the Fashion-MNIST dataset,
    shown in figure 7.1\. This is a good basic test dataset that can be trimmed down,
    without compromising results too drastically. Trimming the amount of data we use
    for training or inference reduces the running time of our later evolutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们在Fashion-MNIST数据集上执行图像分类，如图7.1所示。这是一个很好的基本测试数据集，可以缩减数据量，而不会对结果产生太大的影响。减少用于训练或推理的数据量可以缩短我们后续进化的运行时间。
- en: GPU training
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GPU训练
- en: The notebook projects used in this chapter are ready to use GPU, due to the
    heavy processing. However, Colab may put limitations on or restrict your access
    to a GPU instance. If you find this problematic and you have access to a machine
    with a GPU, you can always run Colab connected to a local instance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的笔记本项目已经准备好使用GPU，因为处理量很大。然而，Colab可能会对你的GPU实例访问设置限制或限制。如果你发现这有问题，并且你有访问带有GPU的机器，你总是可以通过连接到本地实例来运行Colab。
- en: Open the EDL_7_1_Keras_CNN.ipynb notebook in Colab. Check the appendix if you
    need help opening the notebook. As always, the first few cells are installs, imports,
    and set up. We can ignore those and go ahead and run the entire notebook via Runtime
    > Run All from the menu.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_7_1_Keras_CNN.ipynb笔记本。如果你需要帮助打开笔记本，请查看附录。像往常一样，前几个单元是安装、导入和设置。我们可以忽略这些，并通过菜单中的“运行”>“运行所有”来运行整个笔记本。
- en: The first cell we want to look at is the data loading, shown in listing 7.1\.
    Where we load the Fashion dataset, normalize and reshape the data into `28,` `28,`
    `1` tensors, where the `,1` at the end represents the channel. We do this because
    the dataset comes in as a 2D array without channels defined. At the end of the
    code block, we extract the first 1,000 samples of the original for training and
    100 for testing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想要查看的是数据加载，如列表 7.1 所示。在这里我们加载 Fashion 数据集，将数据归一化并重塑为 `28,` `28,` `1` 张量，其中末尾的
    `,1` 表示通道。我们这样做是因为数据集以没有定义通道的二维数组形式提供。在代码块末尾，我们提取原始数据的前 1,000 个样本用于训练，100 个用于测试。
- en: '![](../Images/CH07_F01_Lanham.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F01_Lanham.png)'
- en: Figure 7.1 Fashion-MNIST dataset
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 Fashion-MNIST 数据集
- en: Reducing the dataset this much is not ideal but will save us minutes or hours
    later when we try to optimize tens or hundreds of `individuals` or numerous `generations`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样大幅减少数据集并不是最佳选择，但当我们尝试优化成百上千的 `个体` 或众多 `代` 时，这将节省我们几分钟或几小时的时间。
- en: 'Listing 7.1 EDL_7_1_Keras_CNN.ipynb: Loading data'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 EDL_7_1_Keras_CNN.ipynb：加载数据
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Load the dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集。
- en: ❷ Normalize and reshape the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 归一化和重塑数据。
- en: ❸ Extract a smaller subset of data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取数据的一个较小子集。
- en: The next couple of cells construct the output shown in figure 7.1\. We don’t
    review them further here.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个单元格构建了图 7.1 中所示的输出。我们在此不再进一步讨论它们。
- en: Figure 7.2 demonstrates how a single layer is defined, from code to visual implementation.
    Each CNN layer defines a set of filters or neurons that describe a patch or kernel.
    A single kernel is passed over an image using a stride, typically of 1 pixel by
    one pixel. For simplicity, we keep the stride fixed at `1, 1`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 展示了如何定义单个层，从代码到可视化实现。每个 CNN 层定义了一组滤波器或神经元，它们描述了一个块或核。单个核通过步长在图像上移动，通常为
    1 像素乘以 1 像素。为了简单起见，我们将步长固定在 `1, 1`。
- en: '![](../Images/CH07_F02_Lanham.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F02_Lanham.png)'
- en: Figure 7.2 How a CNN layer is defined in Keras
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 在 Keras 中定义 CNN 层
- en: Code for building the convolutional layers of the model is shown in listing
    7.2\. Each `Conv2D` layer defines a convolutional operation applied to the input.
    At each successive layer, the number of filters or channels expands from the last
    layer. For example, the first `Conv2D` layer expands the input channels from `1`
    to `64`. Then, successive layers reduce this to `32` and then `16`, where each
    convolution layer is following by a `MaxPooling` layer that collects or summarizes
    the features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型卷积层的代码如列表 7.2 所示。每个 `Conv2D` 层定义了对输入应用的卷积操作。在每一连续层中，滤波器或通道的数量从上一层扩展。例如，第一个
    `Conv2D` 层将输入通道从 `1` 扩展到 `64`。然后，连续层将其减少到 `32`，然后是 `16`，其中每个卷积层后面都跟着一个 `MaxPooling`
    层，该层收集或总结特征。
- en: 'Listing 7.2 EDL_7_1_Keras_CNN.ipynb: Building CNN layers'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 EDL_7_1_Keras_CNN.ipynb：构建 CNN 层
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The first CNN layer takes the tensor input shape.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一层卷积神经网络（CNN）接收张量输入形状。
- en: ❷ The max pooling layer
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 最大池化层
- en: ❸ The middle CNN layer
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 中间 CNN 层
- en: ❹ The max pooling layer
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 最大池化层
- en: ❺ The middle CNN layer
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 中间 CNN 层
- en: ❻ The max pooling layer
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 最大池化层
- en: Figure 7.3 shows how a single filter or kernel operation is applied to a single
    image patch and the way it extracts a value corresponding to the output. The corresponding
    output is produced by sliding the filter patch across the image, where each kernel
    operation represents a single output value. Note that the kernel values or weights/parameters
    in the filter are learned.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 展示了单个滤波器或核操作如何应用于单个图像块，以及它如何提取与输出相对应的值。相应的输出是通过在图像上滑动滤波器块产生的，其中每个核操作代表一个单独的输出值。请注意，滤波器中的核值或权重/参数是学习得到的。
- en: '![](../Images/CH07_F03_Lanham.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F03_Lanham.png)'
- en: Figure 7.3 Demonstration of the convolutional filter operation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 展示了卷积滤波器操作
- en: Typically, the output of such convolutional operations is quite large and noisy.
    Keep in mind that each kernel/filter produces an output patch resembling the image.
    A useful operation to reduce the amount of data is to use another layer type called
    *pooling*. Figure 7.4 demonstrates how a max pooling layer reduces the output
    from the previous operation. Maximum pooling is one option; you can also use other
    variations to take the minimum or average of the collated features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，此类卷积操作的输出相当大且噪声较多。请记住，每个核/滤波器产生一个类似于图像的输出块。为了减少数据量，可以使用另一种称为 *池化* 的层类型。图
    7.4 展示了最大池化层如何减少前一个操作的输出。最大池化是一种选项；您还可以使用其他变体来取合并特征的最小值或平均值。
- en: '![](../Images/CH07_F04_Lanham.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F04_Lanham.png)'
- en: Figure 7.4 Max pooling operation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4最大池化操作
- en: After setting up the convolution and max pooling layers of the model, a summary
    is printed using `model.summary`(), as shown in the following listing. Keep in
    mind this is just the top, or feature extractor, portion of the full model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好模型的卷积和最大池化层之后，使用`model.summary()`打印摘要，如下所示。请注意，这仅仅是完整模型的上部，或特征提取器部分。
- en: 'Listing 7.3 EDL_7_1_Keras_CNN.ipynb: CNN model summary'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3 EDL_7_1_Keras_CNN.ipynb：CNN模型摘要
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ A 3×3 kernel plus bias gives 10 parameters per filter—10×64 = 640.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个3×3的核加上偏置为每个过滤器提供10个参数——10×64 = 640。
- en: ❷ Pooling layers are not trainable and have no parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 池化层不可训练且没有参数。
- en: ❸ The total number of parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 参数总数
- en: In the next cell, the output from the CNN layers is flattened and input into
    a single dense layer, which outputs to 10 classes, as shown in the following listing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元中，CNN层的输出被展平并输入到一个单一的密集层，该层输出到10个类别，如下所示。
- en: 'Listing 7.4 EDL_7_1_Keras_CNN.ipynb: Finishing the model'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 EDL_7_1_Keras_CNN.ipynb：完成模型
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Flattens the output from 2D convolution to 1D
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将2D卷积的输出展平到1D
- en: ❷ Add a dense layer for classification inference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个密集层进行分类推理。
- en: ❸ Add a final dense layer for outputting 10 classes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个最终的密集层以输出10个类别。
- en: Figure 7.5 shows the output of the model being trained over the much-reduced
    dataset. Typically, this dataset is optimized to perform at an accuracy around
    98%. However, for reasons mentioned earlier, training on the full dataset is time-consuming
    and not practical when we apply evolution. Instead, focus on the accuracy we see
    with this reduced dataset; we don’t review the model compile and training code,
    since we discussed it in chapter 6.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5显示了在大量减少的数据集上训练的模型输出。通常，这个数据集被优化以在约98%的准确率下运行。然而，由于前面提到的原因，在完整数据集上训练既耗时又不切实际，尤其是在我们应用进化算法时。相反，关注这个减少数据集的准确率；我们不会回顾模型的编译和训练代码，因为我们在第6章中已经讨论过。
- en: '![](../Images/CH07_F05_Lanham.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F05_Lanham.png)'
- en: Figure 7.5 Model training on a reduced dataset
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5在减少数据集上模型训练
- en: Your results may vary somewhat, but you should see consistent values max out
    around 81% for the training or validation data. If you do decide to use other
    datasets for this project, be aware that your results may vary dramatically. The
    Fashion-MNIST works well for this application because there is little class variability.
    This certainly wouldn’t be the case for a dataset like CIFAR-10 or CIFAR-100,
    for instance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果可能会有所不同，但您应该看到训练或验证数据的最大值大约在81%。如果您决定为这个项目使用其他数据集，请注意，您的结果可能会有很大差异。Fashion-MNIST适用于这个应用，因为类之间的差异很小。这当然不会是像CIFAR-10或CIFAR-100这样的数据集的情况。
- en: Refer to figure 7.5; look at the problematic differences in training, and test
    both loss and accuracy. We can see the model falls apart around epoch 3 for performing
    any good inference on blind test data. This is likely related to the reduced size
    of our data but also, in part, the construction of the model. In the next section,
    we cover a couple obvious CNN layer architectures and see what problems they introduce.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图7.5；查看训练和测试中的问题差异，以及损失和准确率。我们可以看到，模型在3个epoch左右就崩溃了，无法对盲测试数据进行任何好的推理。这可能与我们的数据量减少有关，部分也与模型的结构有关。在下一节中，我们将介绍一些明显的CNN层架构，并看看它们引入了什么问题。
- en: 7.1.1 Understanding CNN layer problems
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 理解CNN层问题
- en: In this section, we explore a couple further CNN layer architecture examples
    and understand the problems they introduce. CNN is a great tool when used properly
    but can quickly become a disaster if used ineffectively. Understanding when problems
    arise can be beneficial to our later attempts at evolutionary optimization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了一些CNN层架构的进一步示例，并了解它们引入的问题。CNN是一个很好的工具，如果使用得当，但如果不有效地使用，它很快就会变成一场灾难。了解何时出现问题是我们在进化优化后续尝试中受益的。
- en: Reopen the EDL_7_1_Keras_CNN.ipynb notebook, and then navigate to the section
    labeled SECTION 7.1.1\. Be sure to run all the cells using Runtime > Run All from
    the menu.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重新打开EDL_7_1_Keras_CNN.ipynb笔记本，然后导航到标记为SECTION 7.1.1的部分。请确保使用菜单中的“运行”>“运行所有”运行所有单元格。
- en: The first cell contains the code of a new model, this time with only one CNN
    layer. As you can see, we have a single layer defined with 64 filters/neurons
    and a 3×3 kernel. Figure 7.6 shows the output of running this cell; note the extreme
    difference between the total parameters in this model (over 6 million), shown
    in the following listing, and those in the previous model (23 thousand), shown
    in listing 7.3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单元格包含了一个新模型的代码，这次只有一个CNN层。正如你所见，我们定义了一个包含64个滤波器/神经元和3×3核的单层。图7.6显示了运行此单元格的输出；注意以下列表中此模型的总参数（超过600万个）与之前模型（2.3万个）在列表7.3中的参数之间的极端差异。
- en: 'Listing 7.5 EDL_7_1_Keras_CNN.ipynb: Single CNN layer'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 EDL_7_1_Keras_CNN.ipynb：单个CNN层
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ A single 2D convolutional layer
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 单个2D卷积层
- en: ❷ A single dense layer
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 单个密集层
- en: ❸ Outputs to 10 classes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出到10个类别
- en: '![](../Images/CH07_F06_Lanham.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F06_Lanham.png)'
- en: Figure 7.6 The summary of a single CNN layer model
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 单个CNN层模型的总结
- en: Figure 7.7 shows the training output of the model from running the next cell.
    Notice how well the model performs on the training data but how poorly it performs
    on the validation/test data. This is because the model with over 6 million parameters
    memorizes the reduced dataset. As a result, you can see that the accuracy of the
    training set moves to almost 100%, which is fantastic. However, the test/validation
    set begins to decrease.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7显示了运行下一个单元格的模型的训练输出。注意模型在训练数据上的表现如何良好，但在验证/测试数据上的表现如何糟糕。这是因为具有超过600万个参数的模型记忆了减少的数据集。因此，你可以看到训练集的准确率几乎达到了100%，这是非常棒的。然而，测试/验证集开始下降。
- en: '![](../Images/CH07_F07_Lanham.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F07_Lanham.png)'
- en: Figure 7.7 A single-layer CNN model training output
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 单层CNN模型训练输出
- en: Model memorization/specialization vs. generalization
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型记忆/专业化与泛化
- en: We often want to build models that generalize, and therefore, we break our data
    into training and test sets to validate this generalization. There are some other
    techniques we can apply to help generalize, like batch normalization and dropout,
    that we look at later. However, in some cases, generalization may not be your
    end goal, and instead, you may want to identify very specific sets of data. If
    that is the case, then a model that memorizes data is ideal.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常希望构建泛化的模型，因此我们将数据分为训练集和测试集以验证这种泛化。我们还可以应用一些其他技术来帮助泛化，比如批量归一化和dropout，我们稍后会讨论。然而，在某些情况下，泛化可能不是你的最终目标，你可能希望识别非常具体的数据集。如果是这样，那么一个记忆数据的模型是理想的。
- en: Now, we move on to a discussion of the effect of pooling on convolutional output.
    Listing 7.6 shows the change in the model and a summary of the total trained parameters.
    It is worth noting that this model is about a quarter of the size of the previous
    model from the addition of pooling. We also added a batch normalization layer
    between the pooling layer to better generalize the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续讨论池化对卷积输出的影响。列表7.6显示了模型的变化和总训练参数的总结。值得注意的是，这个模型的大小大约是之前模型的四分之一，这是由于添加了池化。我们还在池化层之间添加了一个批量归一化层，以更好地泛化模型。
- en: 'Listing 7.6 EDL_7_1_Keras_CNN.ipynb: Adding pooling'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 EDL_7_1_Keras_CNN.ipynb：添加池化
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ 2D convolutional layer
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 2D卷积层
- en: ❷ Batch normalization layer
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 批量归一化层
- en: ❸ Pooling layer with a 2×2 kernel
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用2×2核的池化层
- en: ❹ The total number of trainable parameters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 可训练参数总数
- en: Figure 7.8 shows the output of training the model over 10 epochs. While this
    model is still showing signs of memorization, the model is also better at generalizing.
    We can see indications of this by looking at the increasing validation accuracy
    and corresponding decreasing loss.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8显示了在10个周期上训练模型的输出。虽然这个模型仍然显示出记忆的迹象，但模型在泛化方面也做得更好。我们可以通过查看不断提高的验证准确率和相应的损失下降来看到这一点。
- en: '![](../Images/CH07_F08_Lanham.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F08_Lanham.png)'
- en: Figure 7.8 The output of training a more advanced CNN model
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 训练更高级CNN模型的输出
- en: We could, of course, continue going through numerous variations of the model,
    adding in more CNN layers or layers like batch normalization, dropout, or pooling.
    Then, we would go through and tweak the various hyperparameters, like kernel sizes
    and the number of neurons and filters, but that would obviously be time-consuming.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以继续通过模型的多种变体进行尝试，添加更多的CNN层或类似批量归一化、dropout或池化的层。然后，我们会调整各种超参数，如核大小、神经元和滤波器的数量，但这显然会消耗大量时间。
- en: 7.1.2 Learning exercises
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 学习练习
- en: 'Use the following learning exercise to help improve your understanding of convolution,
    if required:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，使用以下学习练习来帮助提高你对卷积的理解：
- en: Increase or decrease the kernel size in listing 7.6, and then see what effect
    this has on the results.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表7.6中增加或减少内核大小，然后看看这会对结果产生什么影响。
- en: Increase or decrease the size of the pooling from (2,2) in listing 7.6, and
    then rerun.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表7.6中将池化大小从(2,2)增加或减少，然后重新运行。
- en: Add an additional convolutional layer to the model in listing 7.6, and then
    rerun.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表7.6中将额外的卷积层添加到模型中，然后重新运行。
- en: Ultimately, understanding how and where to use CNN layers requires some trial
    and error—not unlike hyperparameter optimization. Even if you deeply understand
    the convolutional process, defining the right CNN architecture can be difficult.
    This, of course, makes this an ideal candidate for employing some evolutionary
    process for optimizing CNN network architecture.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，理解如何以及在哪里使用CNN层需要一些尝试和错误——这与超参数优化类似。即使你深刻理解了卷积过程，定义正确的CNN架构也可能很困难。这当然使得使用某种进化过程来优化CNN网络架构成为理想的选择。
- en: 7.2 Encoding a network architecture in genes
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 使用基因编码网络架构
- en: In this section’s project, we look at the details of encoding the network architecture
    of a CNN model into `genes`. This is a precursor to evolving these `individual`
    `gene` sequences to produce the optimum model for a given dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的项目中，我们查看将CNN模型的网络架构编码为`基因`的细节。这是进化这些`个体``基因`序列以产生针对给定数据集的优化模型的先决条件。
- en: There have been several papers and a few tools published for evolving network
    architectures. The code in this project was partly derived from a paper titled
    “Evolving Deep Convolutional Neural Networks for Image Classification” by Yanan
    Sun et al. In this paper, the authors develop a process called EvoCNN for building
    CNN model architectures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 已有数篇论文和少数工具被发布，用于进化网络架构。本项目中的代码部分源自一篇题为“用于图像分类的进化深度卷积神经网络”的论文，作者为Yanan Sun等人。在这篇论文中，作者开发了一个名为EvoCNN的过程，用于构建CNN模型架构。
- en: EvoCNN defined a process for encoding a convolutional network into a variable
    length `gene` sequence, as shown in figure 7.9\. When building our `gene` sequence,
    we want to define a base rule that all will start with a convolutional layer and
    finish with a dense layer that will feed into another dense output layer. To simplify
    things, we don’t worry about encoding the last output layer here.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: EvoCNN定义了一个将卷积网络编码为可变长度`基因`序列的过程，如图7.9所示。在构建我们的`基因`序列时，我们希望定义一个基本规则，即所有序列都将从卷积层开始，并以一个将输入到另一个密集输出层的密集层结束。为了简化问题，我们在这里不考虑编码最后一个输出层。
- en: '![](../Images/CH07_F09_Lanham.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F09_Lanham.png)'
- en: Figure 7.9 Variable length `gene` encodings of network architecture
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 网络架构的可变长度`基因`编码
- en: Inside each main component layer, we also want to define corresponding hyperparameter
    options, such as the number of filters/neurons and kernel sizes. To encode such
    varied data, we use a negation trick to separate the main layer components and
    related hyperparameters. The code in this next notebook project only looks at
    building the encoding sequence; we work through the remaining bits later.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个主要组件层内部，我们还想定义相应的超参数选项，例如滤波器/神经元的数量和内核大小。为了编码这种多样的数据，我们使用否定技巧来分离主要层组件和相关超参数。这个下一个笔记本项目中的代码只关注构建编码序列；我们稍后处理剩余的部分。
- en: Open the EDL_7_2_Encoding_CNN.ipynb notebook in Colab. Don’t worry if you can’t
    use a GPU for this project; we are only looking at the architecture encoding and
    not performing evolutionary training just yet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_7_2_Encoding_CNN.ipynb笔记本。如果你不能为这个项目使用GPU，请不要担心；我们目前只关注架构编码，还没有进行进化训练。
- en: The first block of code we look at (listing 7.7) is the constants we set up
    to help us define the layer types and lengths to encapsulate the various relevant
    hyperparameters. We start with constants that define the total maximum number
    of layers and other ranges for various layer hyperparameters. After that, we can
    see the block identifiers for each type and their corresponding size. This size
    value denotes the length of each layer definition, inclusive of hyperparameters.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看的代码块（列表7.7）是我们设置的常数，帮助我们定义层类型和长度，以封装各种相关超参数。我们首先定义了总的最大层数和其他各种层超参数的范围。之后，我们可以看到每种类型的块标识符及其对应的大小。这个大小值表示每个层定义的长度，包括超参数。
- en: 'Listing 7.7 EDL_7_2_Encoding_CNN.ipynb: Encoding constants'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 EDL_7_2_Encoding_CNN.ipynb：编码常量
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Set the max and min build parameters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置最大和最小构建参数。
- en: ❷ Identify layer block starts.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 识别层块开始。
- en: ❸ Identify layer block sizes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 识别层块大小。
- en: Figure 7.10 demonstrates how a `gene` sequence looks with encoding layer blocks
    and their corresponding hyperparameters. Notice how the negated values -1, -2,
    -3, and -4 represent the start of a layer component. Then, depending on the layer
    type, additional hyperparameters are further defined for the number of filters/neurons
    and kernel size.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 展示了带有编码层块及其相应超参数的 `gene` 序列的外观。注意，负值 -1、-2、-3 和 -4 代表层组件的开始。然后，根据层类型，进一步定义了过滤器/神经元数量和内核大小的额外超参数。
- en: '![](../Images/CH07_F10_Lanham.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F10_Lanham.png)'
- en: Figure 7.10 The `gene` encoding of CNN model architecture
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 CNN 模型架构的 `gene` 编码
- en: We can now go over the code that constructs a `gene` sequence (`chromosome`)
    of an `individual`, shown in listing 7.8\. First, we look at the function `create_offspring`,
    which is the base for how the sequence is built. This code loops over the maximum
    layer count and checks, with a 50% chance of adding a convolution layer. If so,
    it further checks, with a 50% chance of adding a batch normalization and/or pooling
    layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看构建一个 `individual` 的 `gene` 序列（染色体）的代码，如列表 7.8 所示。首先，我们来看 `create_offspring`
    函数，这是构建序列的基础。此代码遍历最大层计数，并带有 50% 的概率添加一个卷积层。如果是这样，它将进一步检查，带有 50% 的概率添加批量归一化和/或池化层。
- en: 'Listing 7.8 EDL_7_2_Encoding_CNN.ipynb: Creating offspring (`gene` sequences)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 EDL_7_2_Encoding_CNN.ipynb：创建后代（`gene` 序列）
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Add a convolutional layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加一个卷积层。
- en: ❷ Add a batch normalization layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个批量归一化层。
- en: ❸ Add a pooling layer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个池化层。
- en: For completeness, we can also review the various layer-building functions. Not
    all the code is shown in the following listing, but what is shown should give
    you an idea of how the helper functions work.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还可以回顾各种层构建函数。以下列表中并未显示所有代码，但所示内容应能让你了解辅助函数的工作方式。
- en: 'Listing 7.9 EDL_7_2_Encoding_CNN.ipynb: Layer component helper functions'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 EDL_7_2_Encoding_CNN.ipynb：层组件辅助函数
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Add a layer marker to start the sequence block.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加一个层标记以开始序列块。
- en: ❷ Add hyperparameters for kernel size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为内核大小添加超参数。
- en: ❸ Add hyperparameters for filters/neurons.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为过滤器/神经元添加超参数。
- en: Calling `create_offspring` generates a `gene` sequence, as shown in the output
    of running the last cell. Go ahead and run the cell a few times to see the variation
    of the `gene` sequences created, as shown in the following listing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `create_offspring` 生成一个 `gene` 序列，如运行最后一个单元格的输出所示。请运行单元格几次，以查看创建的 `gene`
    序列的变体，如下列所示。
- en: 'Listing 7.10 EDL_7_2_Encoding_CNN.ipynb: Examining the generated `gene` sequence'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 EDL_7_2_Encoding_CNN.ipynb：检查生成的 `gene` 序列
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Create an offspring individual.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个后代个体。
- en: ❷ Example output of a random gene sequence
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机基因序列的示例输出
- en: With a `gene` sequence, we can now go on to build the model, essentially parsing
    the `gene` sequence and creating a Keras model. As you can see from the code,
    the input to `build_model` is a single `gene` sequence that produces a Keras model.
    Otherwise, the code is a standard token parser that looks for layer component
    tokens -1, -2, -3, or -4\. After defining the layer, it adds the additional hyperparameters
    based on the layer type, as shown in the following listing.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `gene` 序列，我们现在可以继续构建模型，本质上解析 `gene` 序列并创建一个 Keras 模型。如你所见，`build_model` 的输入是一个单一的
    `gene` 序列，它产生一个 Keras 模型。否则，代码是一个标准的标记解析器，寻找层组件标记 -1、-2、-3 或 -4。在定义层之后，它根据层类型添加额外的超参数，如下列所示。
- en: 'Listing 7.11 EDL_7_2_Encoding_CNN.ipynb: Building the model'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 EDL_7_2_Encoding_CNN.ipynb：构建模型
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Add a convolution layer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加一个卷积层。
- en: ❷ Add an input shape to the first convolution layer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入形状添加到第一个卷积层。
- en: ❸ Add a pooling layer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个池化层。
- en: ❹ Add a batch normalization layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个批量归一化层。
- en: ❺ Add a dense layer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加一个密集层。
- en: The next block of code creates a new `individual` `gene` sequence, builds a
    model from the sequence, and then trains the model, outputting the training/validation
    plots, as we have already looked at.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码创建一个新的 `individual` `gene` 序列，从序列构建模型，然后训练模型，输出训练/验证图，正如我们之前所看到的。
- en: Your results may be quite poor or relatively good, depending on the random initial
    sequence. Go ahead and run this last cell a few times to see the differences between
    different initial randomized `individuals`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能非常差或相对较好，这取决于随机的初始序列。继续运行这个最后的单元格几次，以查看不同初始随机 `individuals` 之间的差异。
- en: 7.2.1 Learning exercises
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 学习练习
- en: 'Use the following exercises to improve your understanding:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你的理解：
- en: Create a list of new `gene` encoded sequences from listing 7.8 by calling `create_
    offspring` in a loop. Print, and then compare the `individuals`.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在循环中调用 `create_offspring` 从列表 7.8 创建新的 `gene` 编码序列。打印，然后比较 `individuals`。
- en: Alter the max/min range hyperparameters from listing 7.6, and then produce a
    list of new offspring (see exercise 1).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改列表 7.6 中的最大/最小范围超参数，然后生成新的后代列表（见练习 1）。
- en: Add a new input to `create_offspring` that changes the static probability from
    0.5 to the new value. Then, produce a list of offspring (see exercise 1) to compare.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `create_offspring` 添加一个新的输入，将静态概率从 0.5 更改为新值。然后，生成后代列表（见练习 1）进行比较。
- en: Now that we have a way to define a `gene` sequence that represents a model architecture,
    we can move on to building the genetic operators that support such a sequence.
    Unfortunately, we can’t just use a built-in operator from DEAP but must create
    our own for mating (`crossover`) and `mutation`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种定义表示模型架构的 `gene` 序列的方法，我们可以继续构建支持此类序列的遗传算子。不幸的是，我们无法使用 DEAP 的内置算子，而必须为交配（`crossover`）和
    `mutation` 创建自己的算子。
- en: 7.3 Creating the mating crossover operation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 创建交配交叉操作
- en: The standard genetic operators available in the DEAP `toolbox` are insufficient
    for our custom network architecture `gene` sequences. This is because any standard
    mating operator would likely corrupt the format of our `gene` sequence. Instead,
    we need to build our own custom operators for both mating (`crossover`) and `mutation`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP `toolbox` 中可用的标准遗传算子不足以满足我们自定义网络架构 `gene` 序列的需求。这是因为任何标准的交配算子都可能破坏我们 `gene`
    序列的格式。因此，我们需要为交配（`crossover`）和 `mutation` 建立自己的自定义算子。
- en: Figure 7.11 shows what this custom `crossover` operation look like when applied
    to two mating parents. The operation works by taking the two parents and extracting
    the various layer collections into lists—one for convolution, one for pooling,
    and so on. From each list, a random selection of pairs of layers are swapped between
    `gene` sequences. The resulting sequences of `genes` become the produced offspring.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 显示了当应用于两个交配父母时，这个自定义 `crossover` 操作看起来是什么样子。该操作通过获取两个父母并提取各种层集合到列表中——一个用于卷积，一个用于池化等。从每个列表中，随机选择一对层在
    `gene` 序列之间交换。生成的 `genes` 序列成为产生的后代。
- en: '![](../Images/CH07_F11_Lanham.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F11_Lanham.png)'
- en: Figure 7.11 The `crossover` operation visualized
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 `crossover` 操作的可视化
- en: The code to perform this custom `crossover` operation is in our next notebook,
    but really, it is an extension of the last notebook we looked at. Keep in mind
    while reviewing this code that this is just one option for performing `crossover`,
    and you may likely consider others. What is important is maintaining a correctly
    formatted `gene` sequence after the `crossover` operation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个自定义 `crossover` 操作的代码在我们的下一个笔记本中，但实质上，它是我们之前查看的最后一个笔记本的扩展。在审查此代码时，请记住，这只是执行
    `crossover` 的一种选项，你可能还会考虑其他选项。重要的是在 `crossover` 操作后保持 `gene` 序列的正确格式。
- en: Open the EDL_7_3_Crossover_CNN.ipynb notebook in Colab. Run all the cells (Runtime
    > Run All), and then scroll to near the bottom of the notebook. Again, this notebook
    just builds on our last exercises, and we don’t need to review that previous code
    here.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 EDL_7_3_Crossover_CNN.ipynb 笔记本。运行所有单元格（运行 > 运行所有），然后滚动到笔记本的底部附近。再次强调，这个笔记本只是基于我们之前的练习，我们在这里不需要回顾之前的代码。
- en: Scroll down the cell titled Custom Crossover Operator. There is a bit of code
    here, so we break it down into sections to review starting with the main `crossover`
    function, shown in the following listing. This main function calls the `swap_layers`
    function for each set of layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到标题为“自定义交叉算子”的单元格。这里有一些代码，所以我们将其分解成几个部分来审查，从下面的列表中的主要 `crossover` 函数开始。这个主要函数为每一组层调用
    `swap_layers` 函数。
- en: 'Listing 7.12 EDL_7_3_Crossover_CNN.ipynb: A custom `crossover` function'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 EDL_7_3_Crossover_CNN.ipynb：自定义 `crossover` 函数
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The function takes two individuals as input.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该函数接受两个个体作为输入。
- en: ❷ Swap various groups of layers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 交换各种层组。
- en: ❸ return results in two new offspring.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回两个新的后代。
- en: The `swap_layers` function is where each layer type is extracted from the sequence
    and then randomly swapped. We start by getting the list of layers by type from
    each sequence. `c1` and `c2` are both index lists we loop through to determine
    the swap points. From these lists, we randomly grab a value to swap for each sequence
    and then perform the swap with the `swap` function, as shown in the following
    listing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`swap_layers`函数是每个层类型从序列中提取并随机交换的地方。我们首先通过类型从每个序列中获取层列表。`c1`和`c2`都是我们循环以确定交换点的索引列表。从这些列表中，我们随机抓取一个值来交换每个序列，然后使用`swap`函数执行交换，如下面的列表所示。'
- en: 'Listing 7.13 EDL_7_3_Crossover_CNN.ipynb: Swapping layers'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.13 EDL_7_3_Crossover_CNN.ipynb：交换层
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Get a list of layers of type for each sequence.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取每个序列中层的列表。
- en: ❷ Find the minimum length of layer lists.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 找到层列表的最小长度。
- en: ❸ Randomly pick indexes from each layer group.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从每个层组中随机选择索引。
- en: ❹ Swap the layers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 交换层。
- en: The `get_layers` function is where we extract the layer indexes from each `gene`
    sequence. This can be done rather succinctly with a list comprehension by checking
    each value in the sequence and extracting the matching positions in a list, as
    shown in the following listing.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_layers`函数是我们从每个`gene`序列中提取层索引的地方。这可以通过列表推导式简洁地完成，通过检查序列中的每个值并提取列表中匹配的位置，如下面的列表所示。'
- en: 'Listing 7.14 EDL_7_3_Crossover_CNN.ipynb: Finding the layer indexes'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.14 EDL_7_3_Crossover_CNN.ipynb：查找层索引
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Inputs a sequence and the type of layer to extract
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入序列和要提取的层类型
- en: ❷ Returns a list of indexes of layer type in sequence
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回序列中层类型的索引列表
- en: The last function we look at here is the `swap` function, shown in the following
    listing, which is responsible for swapping the layer block of each `individual`.
    `swap` works by extracting each layer block from the sequence from the given index.
    Since the layer types are always the same length, a simple index replace is appropriate.
    Keep in mind that if our layer blocks were variable in length, we would have to
    develop a more advanced solution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里查看的最后一个函数是`swap`函数，如下面的列表所示，它负责交换每个`individual`的层块。`swap`通过从给定索引的序列中提取每个层块来工作。由于层类型总是相同长度，简单的索引替换是合适的。记住，如果我们的层块长度可变，我们就必须开发一个更高级的解决方案。
- en: 'Listing 7.15 EDL_7_3_Crossover_CNN.ipynb: The `swap` function'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.15 EDL_7_3_Crossover_CNN.ipynb：`swap`函数
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Extracts the chunk from the sequence
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从序列中提取块
- en: ❷ Prints the output of the layer swap
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印层交换的输出
- en: ❸ Swaps the sequence of chunks
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 交换块序列
- en: Figure 7.12 shows the results of performing the `crossover` function on two
    initial offspring. Notice from the figure how we are swapping three convolutional,
    one pooling, one batch normalization, and one dense layer group. The resulting
    output sequences are shown in figure 7.12.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12显示了在两个初始后代上执行`crossover`函数的结果。注意从图中我们是如何交换三个卷积层、一个池化层、一个批量归一化层和一个密集层组的。结果输出序列显示在图7.12中。
- en: '![](../Images/CH07_F12_Lanham.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F12_Lanham.png)'
- en: Figure 7.12 Examining `crossover` output
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 检查`crossover`输出
- en: 'The rest of the notebook builds, compiles, and trains the resulting `individuals`
    and outputs the results. Be sure to review those last cells to confirm the `crossover`
    operation is not corrupting the `gene` sequence format. Now that we have a `crossover`
    operation for mating and producing offspring, we can move on to developing the
    last operation: `mutation`.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的其余部分构建、编译和训练生成的`individuals`，并输出结果。务必查看最后几个单元格，以确认`crossover`操作没有破坏`gene`序列格式。现在我们有了用于交配和产生后代的`crossover`操作，我们可以继续开发最后一个操作：`mutation`。
- en: 7.4 Developing a custom mutation operator
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 开发定制的突变算子
- en: Again, the standard `mutation` operators available in DEAP are of no use for
    our custom `gene` sequences. As such, we need to develop a custom `mutation` operator
    to simulate the type of `mutations` we would like to apply to our `gene` sequences.
    For the purposes of this project, we keep the `mutation` rather simple and only
    alter the current layer blocks. In more advanced applications, a `mutation` could
    add or remove new layer blocks, but we leave that up to you to implement.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，DEAP中可用的标准`mutation`算子对我们定制的`gene`序列没有用。因此，我们需要开发一个定制的`mutation`算子来模拟我们希望应用于`gene`序列的`mutations`类型。在本项目的目的上，我们保持`mutation`相对简单，仅改变当前层块。在更高级的应用中，`mutation`可以添加或删除新的层块，但这留给你来实现。
- en: Open notebook EDL_7_4_Mutation_CNN.ipynb in Colab. Run all the cells (Runtime
    > Run All). Scroll down to near the bottom of the notebook to the section titled
    Custom Mutation Operator.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开笔记本EDL_7_4_Mutation_CNN.ipynb。运行所有单元格（运行 > 运行所有）。滚动到笔记本底部附近的标题为“自定义变异算子”的部分。
- en: We start by examining the main `mutation` function, as shown in the following
    listing. The function starts by checking if the `individual` is not empty. If
    it isn’t, we move on to mutating each of the layer groups using the `mutate_layers`
    function. Finally, we return the result in a tuple, per DEAP convention.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查主要的`变异`函数，如下所示列表。函数开始时检查`个体`是否为空。如果不为空，我们继续使用`mutate_layers`函数对每个层组进行变异。最后，我们按照DEAP的约定返回结果。
- en: 'Listing 7.16 EDL_7_4_Mutation_CNN.ipynb: A custom `mutation` operator'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.16 EDL_7_4_Mutation_CNN.ipynb：自定义`变异`算子
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Only mutate convolution networks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅变异卷积网络。
- en: ❷ Mutate layers by type.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 按类型变异层。
- en: ❸ Return tuple, per DEAP convention.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 按DEAP约定返回元组。
- en: The `mutate_layers` function loops through the layer groups of a particular
    type and `mutates` just the corresponding hyperparameters. Start by extracting
    the layer group indexes for the given type using `get_layers`, as seen in the
    last section. Then, wrapped in a `try`/`except` block, we apply `mutation` by
    calling the `mutate` function to replace the given indexed layer block, as shown
    in the following listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`mutate_layers`函数遍历特定类型的层组，并仅变异相应的超参数。首先，使用`get_layers`提取给定类型的层组索引，如上一节所示。然后，在`try`/`except`块中，我们通过调用`mutate`函数来替换给定的索引层块，如下所示列表。'
- en: 'Listing 7.17 EDL_7_4_Mutation_CNN.ipynb: The `mutate_layers` function'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.17 EDL_7_4_Mutation_CNN.ipynb：`mutate_layers`函数
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Use get_layers to extract layer indexes by type.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用get_layers提取按类型的层索引。
- en: ❷ Loop through indexes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历索引。
- en: ❸ Call the mutate function to replace the layer chunk.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调用变异函数以替换层块。
- en: ❹ Print out layers that cause errors.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出导致错误的层。
- en: The `mutate` function is where all the work happens. We start by checking that
    the extracted part has the correct length, as shown in listing 7.18\. This is
    done to prevent any potential formatting corruption issues that may happen to
    an `individual`. Next, depending on the layer type, we may alter the number of
    filters/neurons and kernel sizes. Notice how we limit the kernel sizes to a value
    within the original min/max ranges but leave the number of filters/neurons to
    grow or shrink. At this point, we also check whether the `individual gene` sequence
    has any corrupt blocks—blocks that don’t match the required length. If we do find
    that a `gene` sequence is corrupt during `mutate`, then we throw an exception.
    This exception will be caught in the `mutation` function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`mutate`函数是所有工作发生的地方。我们首先检查提取的部分具有正确的长度，如列表7.18所示。这样做是为了防止任何可能发生的格式化损坏问题，这些问题可能发生在`个体`上。接下来，根据层类型，我们可能会改变滤波器/神经元和核的大小。注意我们如何将核的大小限制在原始的最小/最大范围内，但将滤波器/神经元的数量留出增长或缩小的空间。在此阶段，我们还检查`个体基因`序列是否有任何损坏的块——这些块不匹配所需的长度。如果我们发现在`变异`过程中`基因`序列是损坏的，那么我们将抛出一个异常。这个异常将在`变异`函数中被捕获。'
- en: 'Listing 7.18 EDL_7_4_Mutation_CNN.ipynb: The `mutate` function'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18 EDL_7_4_Mutation_CNN.ipynb：`变异`函数
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Check that the layer type and part have the appropriate lengths.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查层类型和部分是否具有适当的长度。
- en: ❷ Apply a random increase/decrease to filters/neurons.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对滤波器/神经元应用随机增加/减少。
- en: ❸ Randomly change kernel sizes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 随机更改核大小。
- en: ❹ Throw an error if the format is corrupt.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果格式损坏，则抛出错误。
- en: Figure 7.13 shows the result of running the `mutation` function/operator on
    an `individual` `gene` sequence. Notice how the hyperparameters defining the layer
    groups number of neurons/filters or kernel sizes are the only things modified.
    You will likely see different results when you run the notebook, but you should
    still observe the changes highlighted in figure 7.13.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13显示了在`个体`基因序列上运行`变异`函数/算子的结果。注意定义层组数量、神经元/滤波器或核大小的超参数是唯一被修改的东西。当你运行笔记本时，你可能会看到不同的结果，但你应该仍然观察到图7.13中突出显示的变化。
- en: '![](../Images/CH07_F13_Lanham.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F13_Lanham.png)'
- en: Figure 7.13 Example of a `mutation` operator applied
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 应用`变异`算子的示例
- en: Again, the rest of the notebook builds, compiles, and trains the `mutated gene`
    sequence to confirm we can still produce a valid Keras model. Go ahead and run
    the `mutation` code block a few times to confirm the output `gene` sequences are
    valid. With the custom operators built to handle the `crossover` and `mutation`
    operations, we can now move on to applying evolution in the next section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，笔记本的其余部分构建、编译和训练 `突变基因` 序列，以确认我们仍然可以生成有效的 Keras 模型。请运行几次 `突变` 代码块以确认输出的 `基因`
    序列是有效的。通过构建用于处理 `交叉` 和 `突变` 操作的自定义算子，我们现在可以继续在下一节应用进化。
- en: Advantages of using Keras
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras 的优势
- en: Keras model compilation is robust and forgiving, which is useful when it is
    likely some of the models we randomly build will be problematic and not produce
    good results. In comparison, a framework like PyTorch is much less forgiving and
    would likely complain about several build issues, producing blocking errors. With
    Keras, we can get away with minimal error handling, as most of the models will
    run; however, they likely won’t run well. If we were to apply this same evolution
    on PyTorch, we would likely encounter more build issues over minor concerns producing
    fewer surviving offspring. Conversely, Keras would produce more viable offspring
    that could develop into a more fit solution. This doesn’t necessarily mean PyTorch
    lacks as a DL framework; instead, it points more to the rigidity of both frameworks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 模型编译稳健且宽容，这在可能随机构建的一些模型存在问题且无法产生良好结果时很有用。相比之下，像 PyTorch 这样的框架宽容度要小得多，可能会对几个构建问题进行抱怨，产生阻塞错误。使用
    Keras，我们可以通过最小的错误处理来避免问题，因为大多数模型都会运行；然而，它们可能运行不佳。如果我们将这种相同的进化应用于 PyTorch，我们可能会遇到更多由于次要问题而产生的构建问题，从而产生较少的幸存后代。相反，Keras
    会产生更多可行的后代，这些后代可以发展成为更合适的解决方案。这并不一定意味着 PyTorch 作为深度学习框架缺乏功能；相反，它更多地指向了这两个框架的僵化性。
- en: 7.5 Evolving convolutional network architecture
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 进化卷积网络架构
- en: Evolving the convolutional network architecture is now just a matter of adding
    DEAP to employ genetic algorithms. A lot of what we cover in this section is review
    from previous chapters, but it should be useful for understanding how the custom
    operators work. In this section, we continue working off the previous notebooks
    and extending them to perform evolving architecture search.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 进化卷积网络架构现在只是添加 DEAP 以使用遗传算法的问题。本节中我们涵盖的大部分内容是前几章的复习，但它应该有助于理解自定义算子是如何工作的。在本节中，我们继续使用之前的笔记本，并扩展它们以执行进化架构搜索。
- en: Open the EDL_7_5_Evo_CNN.ipynb notebook in Colab. Go ahead and run all the cells
    (Runtime > Run All). Notice that at the top of this notebook, we install DEAP
    with pip and import the standard modules we have used in previous chapters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 EDL_7_5_Evo_CNN.ipynb 笔记本。请运行所有单元格（运行 > 运行所有）。注意，在这个笔记本的顶部，我们使用
    pip 安装 DEAP 并导入我们在前几章中使用的标准模块。
- en: Scroll down to the section titled Evolutionary CNN, and examine the DEAP `toolbox`
    set up code, as shown in the following listing. Notice how we reuse the `create_
    offspring` function from listing 7.8 and register with the `toolbox` using the
    name `network`. This function is responsible for creating new first-generation
    offspring. Then, a list is used to hold the `individual` `gene` sequence. The
    benefit of using a list here is that a set of `individuals` can vary in length.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动到名为进化 CNN 的部分，检查 DEAP `toolbox` 设置代码，如下所示。注意我们如何重用列表 7.8 中的 `create_offspring`
    函数并将其注册到 `toolbox` 中，使用名称 `network`。这个函数负责创建新的第一代后代。然后，使用列表来保存 `individual` `基因`
    序列。在这里使用列表的好处是，一组 `individuals` 的长度可以不同。
- en: 'Listing 7.19 EDL_7_5_Evo_CNN.ipynb: DEAP `toolbox` setup'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.19 EDL_7_5_Evo_CNN.ipynb：DEAP `toolbox` 设置
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Add the custom create_offspring function called network.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加名为 network 的自定义 create_offspring 函数。
- en: ❷ Register the new network initialization function.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注册新的网络初始化函数。
- en: ❸ Use a list to contain individuals in population.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用列表来包含种群中的个体。
- en: ❹ Use a standard tournament selection operator.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用标准的锦标赛选择算子。
- en: Scroll down a little to see how to register the custom `crossover` (listing
    7.12) and `mutation` (listing 7.16) functions we created earlier, as shown in
    the following listing.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动一点，查看如何注册我们之前创建的自定义 `交叉`（列表 7.12）和 `突变`（列表 7.16）函数，如下所示。
- en: 'Listing 7.20 EDL_7_5_Evo_CNN.ipynb: Register custom functions'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20 EDL_7_5_Evo_CNN.ipynb：注册自定义函数
- en: '[PRE19]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Register a custom mate function.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注册自定义配对函数。
- en: ❷ Register a custom mutate function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注册自定义突变函数。
- en: The next cell, shown in listing 7.21, contains the code for building, compiling,
    training, and evaluating the model. We start by looking at the `evaluate` function.
    This function first builds the model using the `build_model` function (listing
    7.11), and then it compiles and trains the model with a new function, `compile_train`.
    After that, it returns the `1/accuracy` clamped to a range between almost 0 and
    1\. We do this because we want to minimize the `fitness` by `1/accuracy`. Notice
    that we wrap the code in `try`/`except` to be sure that if anything fails, we
    gracefully recover. Our code still has the potential to build nonsensical models,
    and this is a way of protecting against failures. If the code does fail, we return
    `1/.5` or, 50% accuracy—not 0 or close to 0\. By doing this, we allow these failures
    to remain within the `population` and hopefully `mutate` into something better
    later.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格，如列表 7.21 所示，包含构建、编译、训练和评估模型的代码。我们首先查看 `evaluate` 函数。该函数首先使用 `build_model`
    函数（列表 7.11）构建模型，然后使用新的函数 `compile_train` 编译和训练模型。之后，它将 `1/准确度` 钳位到几乎 0 到 1 之间。我们这样做是因为我们想要通过
    `1/准确度` 最小化 `适应性`。请注意，我们将代码包裹在 `try`/`except` 中，以确保如果发生任何错误，我们可以优雅地恢复。我们的代码仍然有可能构建无意义的模型，这是防止失败的一种方法。如果代码确实失败，我们返回
    `1/.5` 或 50% 的准确度——而不是 0 或接近 0。通过这样做，我们允许这些失败保留在 `种群` 中，并希望它们以后能够 `变异` 成更好的东西。
- en: 'Listing 7.21 EDL_7_5_Evo_CNN.ipynb: The `evaluate` function'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.21 EDL_7_5_Evo_CNN.ipynb：`evaluate` 函数
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Build the model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建模型。
- en: ❷ Compile and train the model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编译和训练模型。
- en: ❸ Return 1/accuracy clamped.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回 1/准确度钳位。
- en: ❹ If there is a failure, return the base accuracy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果出现失败，返回基本准确度。
- en: ❺ Register the function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 注册该函数。
- en: Survival of the fittest
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 适者生存
- en: By allowing the failed `individuals` some base `fitness`, we are encouraging
    those `gene` sequences to potentially remain in the `population` pool. In nature,
    `individuals` with severe `mutations` almost certainly quickly fail. Cooperative
    species, like humans, are better at caring for weaker `individuals` with potential.
    This, most certainly, is the reason human babies can be born so weak and frail
    yet grow and survive to become contributing `individuals`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许失败的 `个体` 一些基本的 `适应性`，我们鼓励这些 `基因` 序列可能仍然保留在 `种群` 池中。在自然界中，具有严重 `变异` 的 `个体`
    几乎肯定会迅速失败。像人类这样的合作物种，在照顾有潜力的较弱 `个体` 方面做得更好。这，无疑是人类婴儿可以如此虚弱和脆弱地出生，但仍然能够成长和生存，成为贡献者的原因。
- en: The `compile_train` function is very similar to our earlier training code, but
    it’s worth a quick look in the following listing. Not much is different here,
    but notice we have fixed training at 3 epochs, for brevity. Again, you may want
    to alter this and see what effect it has on the result.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`compile_train` 函数与我们早期的训练代码非常相似，但以下列表中快速看一下是值得的。这里没有太多不同，但请注意，我们为了简洁起见将训练固定在
    3 个周期。再次提醒，你可能想要改变这一点，看看它对结果有什么影响。'
- en: 'Listing 7.22 EDL_7_5_Evo_CNN.ipynb: The `compile` and `train` functions'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.22 EDL_7_5_Evo_CNN.ipynb：`compile` 和 `train` 函数
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Train for accuracy.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为准确度进行训练。
- en: ❷ Fit the model over 3 epochs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 3 个周期内拟合模型。
- en: Scroll down to the evolution set up code we have reviewed in previous chapters,
    and look at the output of evolving the `population` over 5 `generations`, shown
    in figure 7.14\. Since our `gene` sequences are relatively small, we should generally
    expect a quick convergence. Your results may vary, but in most cases, your accuracy
    should maximize at around 0.81, or 81%. Go ahead and try to increase the size
    of the `population` or number of `generations` to see what effects this has.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到我们在前几章中审查的进化设置代码，并查看在 5 个 `代` 中进化 `种群` 的输出，如图 7.14 所示。由于我们的 `基因` 序列相对较小，我们通常可以期望快速收敛。你的结果可能会有所不同，但在大多数情况下，你的准确度应该在大约
    0.81，或 81% 左右达到最大值。尝试增加 `种群` 的大小或 `代` 的数量，看看这会产生什么影响。
- en: '![](../Images/CH07_F14_Lanham.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F14_Lanham.png)'
- en: Figure 7.14 The results of evolving a `population` over 5 `generations`
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 在 5 个 `代` 中进化 `种群` 的结果
- en: After evolution is done, we build, compile, and train the best `individual`
    to see the results in figure 7.15\. We can still see a divergence after 3 epochs,
    suggesting that if we want a more durable model, we likely need to increase the
    training epochs in evolution. This can be easily achieved, but it increases evolution
    time substantially.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 进化完成后，我们构建、编译和训练最佳 `个体`，以查看图 7.15 中的结果。我们仍然在 3 个周期后看到发散，这表明如果我们想要一个更持久的模型，我们可能需要增加进化中的训练周期。这很容易实现，但它会显著增加进化时间。
- en: '![](../Images/CH07_F15_Lanham.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F15_Lanham.png)'
- en: Figure 7.15 The results of evaluating the best `individual` after evolution
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 评估进化后最佳`个体`的结果
- en: Finally, we can look at the summary of the evolved model architecture in figure
    7.16\. It is quite likely your results will vary a little, but you should see
    a similar layer structure to what is shown in the figure. In fact, if you have
    worked with Fashion-MNIST dataset previously, this is likely an architecture you
    have seen applied.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看图7.16中进化模型架构的摘要。您的结果可能会有所不同，但您应该会看到与图中显示的类似层结构。实际上，如果您之前已经使用过Fashion-MNIST数据集，这可能是您已经见过的架构。
- en: '![](../Images/CH07_F16_Lanham.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F16_Lanham.png)'
- en: Figure 7.16 Model summary results produced from evolution
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 从进化产生的模型摘要结果
- en: 'You are welcome, of course, to modify this notebook as you see fit and add
    several customizations we discussed throughout the chapter. The following are
    summaries of the modifications you may want to make to this notebook:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以根据需要修改这个笔记本，并添加我们在本章中讨论的几个自定义功能。以下是对您可能想要对此笔记本进行的修改的总结：
- en: '*Dataset size*—We reduced the size of the original dataset drastically to reduce
    runtime. Expect that if you increase the dataset size, you will also see an increase
    in simulation runtime.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据集大小*——我们将原始数据集的大小大幅减少以减少运行时间。预期如果您增加数据集大小，您也会看到模拟运行时间的增加。'
- en: '*Training epochs*—During our earlier evaluation, we decided to use 3 epochs
    as our training limit. Depending on your data, you may want to increase or decrease
    this value.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练轮数*——在我们早期的评估中，我们决定使用3轮作为我们的训练限制。根据您的数据，您可能想要增加或减少这个值。'
- en: '*Layer types*—For this simple demonstration, we kept to standard layer types,
    like convolutional, pooling, batch normalization, and dense. You may want to add
    different layer types, like dropout, and/or increase the number of dense layers
    or other variations.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层类型*——在这个简单的演示中，我们坚持使用标准层类型，如卷积、池化、批量归一化和密集层。您可能想要添加不同的层类型，如dropout，以及/或者增加密集层或其他变体的数量。'
- en: '*Crossover/mutation*—The custom operators we built for mating and `mutation`
    are just one implementation. As mentioned, when building the `mutation` function,
    there is a lot of room for further customization, perhaps by letting `mutation`
    add or remove layer blocks.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉/变异*——我们为配对和`变异`构建的自定义算子只是其中一种实现方式。正如之前提到的，在构建`变异`函数时，有很多空间进行进一步定制，也许可以通过让`变异`添加或删除层块来实现。'
- en: '*Fitness/evaluation function*—We based our `individual` `fitness` on a straight
    accuracy score. If we wanted to minimize the number of trainable parameters or
    layers, we could have added that as logic into our `evaluate` function.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*适应度/评估函数*——我们基于直接的准确率分数来评估`个体`的`适应度`。如果我们想最小化可训练参数或层的数量，我们可以在`evaluate`函数中添加相应的逻辑。'
- en: 7.5.1 Learning exercises
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 学习练习
- en: 'Use the following exercises to improve your understanding of EvoCNN:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高您对EvoCNN的理解：
- en: Modify the dataset size or type. Explore different datasets, taking note of
    the differences in evolved CNN models.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改数据集的大小或类型。探索不同的数据集，注意进化后的CNN模型之间的差异。
- en: Add a new layer type of `Dropout` to the `gene` sequence. This will require
    some work but could provide a basis for enhanced CNN model building.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`gene`序列中添加一个新的`Dropout`层类型。这可能需要一些工作，但可能为增强CNN模型构建提供一个基础。
- en: Think about how other forms of evolution could be applied from hyperparameter
    optimization to neuroevolving the weights/parameters.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑如何将其他形式的进化应用于从超参数优化到神经进化权重/参数。
- en: Hopefully, as the concept of evolutionary optimization for automated ML models
    evolves, we can expect frameworks to package all this up for us. However, the
    amount of code to perform such powerful optimizations isn’t too difficult to produce,
    as you’ve seen in this chapter. In the end, even if an encompassing framework
    comes about, you would likely need to customize functions like `mate` and `crossover`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自动机器学习模型进化优化概念的发展，我们有望期待框架为我们打包所有这些功能。然而，执行这种强大优化的代码量并不难产生，正如您在本章中看到的。最终，即使出现一个全面的框架，您可能也需要自定义像`mate`和`crossover`这样的函数。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'Convolutional neural networks are layer extensions to DL models that provide
    localized feature extraction:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络是深度学习模型的层扩展，提供了局部特征提取：
- en: Typically used for 2D image processing, CNN can be very successful in enhancing
    classification or other tasks.
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常用于2D图像处理，CNN在增强分类或其他任务方面可以非常成功。
- en: CNN layers are complex to set up and define for various image recognition tasks,
    given the amount of hyperparameters, configuration, and placement.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于超参数、配置和放置的数量，CNN层对于各种图像识别任务来说设置和定义都很复杂。
- en: '*Neuroevolution* is another term used to describe evolutionary methods for
    DL optimization, specifically those related to architecture and parameter optimization:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经进化*是描述深度学习优化进化方法的另一个术语，特别是那些与架构和参数优化相关的：'
- en: The CNN architecture of a DL network can be optimized with genetic algorithms
    and DEAP.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用遗传算法和DEAP来优化深度学习网络的CNN架构。
- en: The complex architecture of the CNN layers includes the type, size, and placement
    of layers that can be encoded in a custom genetic sequence.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN层的复杂架构包括可以编码在定制遗传序列中的层的类型、大小和位置。
- en: This genetic encoding takes the number, kernel size, stride, normalization,
    and activation function of the various CNN layers.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种遗传编码考虑了各种CNN层的数量、核大小、步长、归一化和激活函数。
- en: Custom `crossover` (mating) and `mutation` genetic operators need to be developed
    to support custom genetic encoding structures.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要开发定制的`交叉`（配对）和`变异`遗传算子来支持定制的遗传编码结构。
- en: Evolve a `population` of `individuals` with genetic algorithms to optimize CNN
    model architecture on a particular dataset.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用遗传算法进化一个`种群`的`个体`，以优化特定数据集上的CNN模型架构。
- en: The EvoCNN custom encoding architecture has limitations to the number of layers
    used in a model. However, the use of neuroevolution can quickly assist with the
    complex task of defining complex CNN architectures.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EvoCNN定制的编码架构对模型中使用的层数有限制。然而，使用神经进化可以快速协助完成定义复杂CNN架构的复杂任务。
