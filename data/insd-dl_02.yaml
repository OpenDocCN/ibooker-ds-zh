- en: 1 The mechanics of learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 学习的机制
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Using Google Colab for coding
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Google Colab 进行编码
- en: Introducing PyTorch, a tensor-based API for deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 PyTorch，一种基于张量的深度学习 API
- en: Running faster code with PyTorch’s GPU acceleration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 PyTorch 的 GPU 加速运行更快的代码
- en: Understanding automatic differentiation as the basis of learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自动微分作为学习的基础
- en: Using the `Dataset` interface to prepare data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `Dataset` 接口准备数据
- en: '*Deep learning*, also called *neural networks* or *artificial neural networks*,
    has led to dramatic advances in machine learning quality, accuracy, and usability.
    Technology that was considered impossible 10 years ago is now widely deployed
    or considered technically possible. Digital assistants like Cortana, Google, Alexa,
    and Siri are ubiquitous and can react to natural spoken language. Self-driving
    cars have been racking up millions of miles on the road as they are refined for
    eventual deployment. We can finally catalog and calculate just *how much* of the
    internet is made of cat photos. Deep learning has been instrumental to the success
    of all these use cases and many more.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*，也称为 *神经网络* 或 *人工神经网络*，在机器学习的质量、准确性和可用性方面取得了显著的进步。10 年前被认为不可能的技术现在已被广泛部署或被认为在技术上可行。像
    Cortana、Google、Alexa 和 Siri 这样的数字助手无处不在，并能对自然语言进行反应。自动驾驶汽车在经过改进以最终部署的过程中已经在路上行驶了数百万英里。我们终于可以统计和计算互联网中有多少是猫的照片。深度学习对于所有这些用例以及更多用例的成功都起到了关键作用。'
- en: This book exposes you to some of the most common and useful techniques in deep
    learning today. A significant focus is how to use and code these networks and
    how and why they work at a deep level. With a deeper understanding, you’ll be
    better equipped to select the best approach for your problems and keep up with
    advances in this rapidly progressing field. To make the best use of this book,
    you should be familiar with programming in Python and have some passing memory
    of calculus, statistics, and linear algebra courses. You should also have prior
    experience with machine learning (ML), although it is OK if you aren’t an expert;
    ML topics are quickly introduced, but our goal is to move into details about deep
    learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '本书向您介绍了当今深度学习中最常见和最有用的技术。一个重要的焦点是如何使用和编码这些网络，以及它们如何在深层工作以及为什么这样做。有了更深入的理解，您将更好地装备自己，以选择最适合您问题的最佳方法，并跟上这个快速发展的领域的进步。为了最大限度地利用本书，您应该熟悉
    Python 编程，并对微积分、统计学和线性代数课程有一些基本的记忆。您还应该有机器学习（ML）的先前经验，尽管您不是专家也行；ML 主题会快速介绍，但我们的目标是进入深度学习的细节。 '
- en: 'Let’s get a clearer idea of what deep learning is and how this book teaches
    about it. Deep learning is a subdomain of ML, which is a subdomain of artificial
    intelligence (AI). (Some may take offense at how I’m categorizing these groups.
    It’s an oversimplification.) Broadly, we could describe AI as getting computers
    to make decisions that *look* smart. I say *look* because it is hard to define
    what *smart* or *intelligence* truly is; AI should be making decisions that we
    think are reasonable and what a smart person would do. Your GPS telling you how
    to get home uses some old AI techniques to work (these classic tried-and-true
    methods are sometimes called “good old-fashioned AI," or GOFAI), and taking the
    fastest route home is a smart decision. Getting computers to play video games
    has been accomplished with purely AI-based approaches: only the rules of the game
    are encoded; the AI does not need to be shown how to play a game of chess. Figure
    1.1 shows AI as the outermost layer of these fields.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更清晰地了解深度学习是什么，以及本书是如何介绍它的。深度学习是机器学习的一个子领域，而机器学习又是人工智能（AI）的一个子领域。（有些人可能会对我的这种分类方式感到不满。这是一种过度简化的方式。）广义上，我们可以将
    AI 描述为让计算机做出看起来聪明的决策。我说“看起来”，因为很难定义什么是“聪明”或“智能”；AI 应该做出我们认为合理的决策，以及一个聪明人可能会做出的决策。你的
    GPS 告诉你如何回家，使用了某些老式的 AI 技术（这些经典的方法有时被称为“老式的传统 AI”，或 GOFAI），选择最快的路线回家是一个明智的决策。让计算机玩电子游戏已经通过纯
    AI 方法实现：只需要编码游戏的规则；AI 不需要被展示如何下棋。图 1.1 显示 AI 是这些领域的最外层。
- en: '![](../Images/CH01_F01_Raff.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F01_Raff.png)'
- en: Figure 1.1 A (simplified) hierarchy of AI, ML, and deep learning
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 AI、机器学习和深度学习的（简化）层次结构
- en: With ML, we start to give AI *examples* of previous smart and not-so-smart decisions.
    For example, we can improve our chess-playing AI by giving it example games played
    by chess grandmasters to learn from (each game has a winner and a loser—a smart
    and a not-as-smart set of decisions). This is a supervised-centric definition,
    but the critical component is that we have data that reflects the real world.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，我们开始向AI提供之前智能和不太智能的决策的**例子**。例如，我们可以通过给它提供国际象棋大师们玩过的例子游戏来改进我们的国际象棋AI（每场比赛都有一个赢家和一个输家——一组明智和不太明智的决策）。这是一个以监督为中心的定义，但关键组成部分是我们有反映现实世界的数据。
- en: Note A common saying is that data is truth, but that’s also an oversimplification.
    Many biases can impact the data you receive, giving you a biased view of the world.
    That is an advanced topic for another book!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有一句俗语说数据即真理，但这也是一种过于简化的说法。许多偏见可能会影响你接收到的数据，给你一个有偏见的对世界的看法。这是一个高级话题，适合另一本书来探讨！
- en: Deep learning, in turn, is not one algorithm, but *hundreds* of small algorithms
    that act like building blocks. Part of being a good practitioner is knowing what
    building blocks are available and which ones to stick together to create a larger
    model for *your* problem. Each building block is designed to work well for certain
    problems, giving the model valuable information. Figure 1.2 shows how we might
    combine blocks together to tackle three situations. One of the goals in this book
    is to cover a wide variety of building blocks so that you know and understand
    how they can be used for different kinds of problems. Some of the blocks are generic
    (“Data is a sequence” could be used for literally *any* kind of sequence), while
    others are more specific (“Data is an image” applies to only images), which impacts
    when and how you use them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习本身并非一个算法，而是由**数百**个像积木一样的小算法组成。成为一名优秀实践者的部分是了解有哪些积木可用，以及如何将它们组合在一起来为**你的**问题创建一个更大的模型。每个积木都是为了解决特定问题而设计的，为模型提供有价值的信息。图1.2展示了我们如何将积木组合起来以应对三种情况。本书的一个目标就是涵盖广泛的积木，以便你了解并理解它们如何用于不同类型的问题。有些积木是通用的（“数据是一个序列”可以用于任何类型的序列），而有些则更具体（“数据是一个图像”仅适用于图像），这影响了你何时以及如何使用它们。
- en: '![](../Images/CH01_F02_Raff.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F02_Raff.png)'
- en: Figure 1.2 A defining characteristic of deep learning is building models from
    reusable blocks. Different blocks are useful for different kinds of data and can
    be mixed and matched to deal with different problems. The first row shows how
    blocks of the same type can be repeated to make a deeper model, which can improve
    accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 深度学习的一个定义特征是从可重用积木中构建模型。不同的积木适用于不同类型的数据，并且可以混合搭配来处理不同的问题。第一行展示了如何重复使用相同类型的积木来构建一个更深层次的模型，这可以提高准确性。
- en: 'The first row uses two “Data is an image” blocks to create a *deep* model.
    Applying blocks repeatedly is where the *deep* in *deep learning* comes from.
    Adding depth makes a model capable of solving more complex problems. This depth
    is often obtained by stacking the same kind of block multiple times. The second
    row in the figure shows a case for a sequence problem: for example, text can be
    represented as a sequence of words. But not all words are meaningful, and so we
    may want to give the model a block that helps it learn to *ignore* certain words.
    The third row shows how to describe new problems using the blocks we know about.
    If we want our AI to watch a video and predict what is happening (e.g., “running,”
    “tennis,” or “adorable puppy attack”) we can use the “Data is an image” and “Data
    is a sequence” blocks to create a sequence of images—a video.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行使用两个“数据是一个图像”的积木来创建一个**深度**模型。重复应用积木是深度学习中的“深度”来源。增加深度使模型能够解决更复杂的问题。这种深度通常是通过多次堆叠相同类型的积木来获得的。图中的第二行展示了序列问题的案例：例如，文本可以被表示为一系列单词。但并非所有单词都有意义，因此我们可能希望给模型一个帮助它学习**忽略**某些单词的积木。第三行展示了如何使用我们已知的积木来描述新的问题。如果我们想让我们的AI观看视频并预测正在发生的事情（例如，“跑步”、“网球”或“可爱的狗攻击”），我们可以使用“数据是一个图像”和“数据是一个序列”的积木来创建一系列图像——一个视频。
- en: These building blocks define our model, but as in all ML, we also need data
    and a mechanism for learning. When we say *learning*, we are not talking about
    the way humans learn. In machine (and deep) learning, *learning* is the mechanical
    process of getting the model to make smart-looking predictions about data. This
    happens via a process called *optimization* or *function minimization*. Before
    we see any data, our model returns random outputs because all of the parameters
    (the numbers that control what is computed) are initialized to random values.
    In a common tool like linear regression, the regression coefficients are the parameters.
    By *optimizing* the blocks over the data, we make our models learn. This gives
    us the larger picture in figure 1.3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些构建模块定义了我们的模型，但在所有机器学习（ML）中，我们还需要数据和一种学习机制。当我们说**学习**时，我们不是在谈论人类学习的方式。在机器（和深度）学习中，**学习**是使模型对数据进行智能预测的机械过程。这是通过称为**优化**或**函数最小化**的过程来实现的。在我们看到任何数据之前，我们的模型返回随机输出，因为所有的参数（控制计算内容的数字）都被初始化为随机值。在常见的工具如线性回归中，回归系数是参数。通过**优化**模块来处理数据，我们使我们的模型学习。这为我们提供了图1.3中的更大图景。
- en: '![](../Images/CH01_F03_Raff.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F03_Raff.png)'
- en: Figure 1.3 The “car” of deep learning. The car is built from many different
    building blocks, and we can use assortments of building blocks to build cars for
    different tasks. But we need fuel and wheels to make the car go. The wheels are
    the task of learning, which is done via a process called optimization; and the
    fuel is data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 深度学习的“汽车”。这辆汽车由许多不同的构建模块组成，我们可以使用各种构建模块来构建用于不同任务的汽车。但是，我们需要燃料和轮子来让汽车行驶。轮子是学习任务，这是通过称为优化的过程来完成的；而燃料是数据。
- en: In most chapters of this book, you learn about new building blocks that you
    can use to build deep learning models for different applications. You can think
    of each block as a kind of (potentially very simple) algorithm. We discuss the
    uses of each block and explain how or why they work and how to combine them in
    code to create a new model. Thanks to the nature of building blocks, we can ramp
    up from simple tasks (e.g., simple prediction problems you could tackle with a
    non-deep ML algorithm) to complex examples like machine translation (e.g., having
    a computer translate from English to French). We start with basic approaches and
    methods that have been used to train and build neural networks since the 1960s,
    but using a modern framework. As we progress through the book, we build on what
    we’ve learned, introducing new blocks, extending old blocks, or building new blocks
    from existing ones.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分章节中，你将学习到可以用来构建针对不同应用场景的深度学习模型的新构建模块。你可以将每个模块视为一种（可能非常简单）的算法。我们讨论了每个模块的用途，并解释了它们是如何或为什么工作，以及如何在代码中将它们组合起来以创建一个新的模型。由于构建模块的性质，我们可以从简单的任务（例如，可以使用非深度机器学习算法解决的简单预测问题）逐步过渡到更复杂的例子，如机器翻译（例如，让计算机从英语翻译成法语）。我们从20世纪60年代以来用于训练和构建神经网络的基本方法和方法开始，但使用现代框架。随着我们在本书中的进展，我们将在所学的基础上继续前进，引入新的模块，扩展旧的模块，或者从现有的模块中构建新的模块。
- en: That said, this book is *not* a cookbook of code snippets to throw at any new
    problem. The goal is to get you comfortable with the language that deep learning
    researchers use to describe new and improved blocks so you can recognize when
    a new block may be useful. Math can often express complex changes succinctly,
    so I will be sharing the math behind the building blocks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '话虽如此，这本书**不是**一本代码片段的食谱，可以随意应对任何新问题。目标是让你熟悉深度学习研究人员用来描述新和改进模块的语言，以便你能识别出何时一个新模块可能是有用的。数学通常可以简洁地表达复杂的变化，因此我将分享构建模块背后的数学。 '
- en: 'We won’t *do* a lot of math—that is, *derive* or *prove* the math. Instead,
    I *show* the math: present the final equations, explain what they do, and attach
    helpful intuition to them. I’m calling it intuition because we go through the
    bare minimum math needed. Explaining the high-level idea of what is happening
    and showing why a result is the way it is would require more math than I’m asking
    you to have. As I show the equations, I interweave corresponding PyTorch code
    whenever possible so you can start to build a mental map between the equations
    and the deep learning code that implements them.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进行很多数学运算——也就是说，**推导**或**证明**数学。相反，我会**展示**数学：呈现最终的方程式，解释它们的作用，并附上有用的直觉。我称之为直觉，因为我们只通过最基本所需的数学。解释正在发生的高层次想法以及为什么结果是这样需要比我要你拥有的更多的数学。当我展示方程式时，尽可能地将相应的PyTorch代码交织在一起，这样你就可以开始建立方程式和实现它们的深度学习代码之间的心理地图。
- en: 'This chapter first introduces our compute environment: Google Colab. Next,
    we talk about PyTorch and tensors, which is how we represent information in PyTorch.
    After that, we dive into the use of graphics processing units (GPUs), which make
    PyTorch fast, and *automatic differentiation*, which is the “mechanics” that PyTorch
    uses to make neural network models learn. Finally, we quickly implement a dataset
    object that PyTorch needs to feed our data into the model for the learning process.
    This gives us the fuel and wheels to get our deep learning car moving, starting
    in chapter 2\. From there on, we can focus on *just* the deep learning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍我们的计算环境：Google Colab。接下来，我们将讨论PyTorch和张量，这是我们在PyTorch中表示信息的方式。然后，我们将深入了解图形处理单元（GPU）的使用，这使得PyTorch运行速度快，以及*自动微分*，这是PyTorch用来使神经网络模型学习的“机制”。最后，我们快速实现一个PyTorch需要的数据集对象，以便将数据输入模型进行学习过程。这为我们提供了推动深度学习汽车前进的燃料和轮子，从第2章开始。从那时起，我们可以专注于*仅仅*深度学习。
- en: This book is designed to be read linearly. Each chapter uses skills or concepts
    developed in the preceding chapters. If you are already familiar with the concepts
    in a chapter, feel free to jump ahead to the next one. But if deep learning is
    new to you, I encourage you to proceed one chapter at a time instead of jumping
    to one that sounds more interesting, as these can be challenging concepts, and
    growing one step at a time will make it easier overall.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书设计为线性阅读。每一章都使用了前一章中开发的技能或概念。如果你已经熟悉了章节中的概念，可以自由地跳到下一章。但如果你对深度学习是新手，我鼓励你一章接一章地学习，而不是跳到一个听起来更有趣的章节，因为这些概念可能具有挑战性，一步一步地学习将使整体过程更容易。
- en: 1.1 Getting started with Colab
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 Colab入门
- en: We will use GPUs for *everything* we do with deep learning. It is, unfortunately,
    a computationally demanding practice, and GPUs are essentially a *requirement*
    for getting started, especially when you start to work on larger applications.
    I use deep learning all the time as part of my work and regularly kick off jobs
    that take a few *days* to train on multiple GPUs. Some of my research experiments
    can take *a month* of compute for each run.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在深度学习的所有工作中使用GPU。遗憾的是，这是一个计算密集型的实践，GPU基本上是入门的*必需品*，尤其是在你开始处理更大规模的应用时。我经常在我的工作中使用深度学习，并定期启动需要几天时间在多个GPU上训练的任务。我的某些研究实验每次运行可能需要*一个月*的计算时间。
- en: Unfortunately, GPUs are expensive. Currently, the best option for most people
    who want to get started with deep learning is to spend $600–$1,200 on a higher-end
    NVIDIA GTX or Titan GPUs. That is, *if* you have a computer that can be expanded/upgraded
    with a high-end GPU. If not, you are probably looking at at least $1,500–$2,500
    to build a nice workstation to put those GPUs in. That’s a steep cost just to
    *learn* about deep learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，GPU价格昂贵。目前，对于大多数想要开始深度学习的人来说，最好的选择是花费600-1200美元购买高端的NVIDIA GTX或Titan GPU。也就是说，*如果*你的电脑可以升级到高端GPU。如果不能，你可能需要至少1500-2500美元来构建一个配备这些GPU的优质工作站。这只是为了*学习*深度学习而付出的高昂成本。
- en: 'Google’s Colab ([https://colab.research.google.com](https://colab.research.google.com))
    provides a GPU for free for a limited amount of time. I’ve designed every example
    in this book to run within Colab’s time limits. The appendix contains the instructions
    for setting up Colab. Once you have it set up, the common data science and ML
    tools like `seaborn`, `matplotlib`, `tqdm`, and `pandas` are all built-in and
    ready to go. Colab operates like a familiar Jupyter notebook, where you run code
    in cells that produce output directly below. This book is a Jupyter notebook,
    so you can run the code blocks (like the next one) to get the same results (I’ll
    let you know if a code cell isn’t meant to be run):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Colab([https://colab.research.google.com](https://colab.research.google.com))在有限的时间内免费提供GPU。我为本书中的每个示例都设计了在Colab的时间限制内运行。附录中包含了设置Colab的说明。一旦设置好，常见的数据科学和机器学习工具如`seaborn`、`matplotlib`、`tqdm`和`pandas`都是内置的，随时可以使用。Colab的操作方式类似于熟悉的Jupyter笔记本，你在单元格中运行代码，输出会直接显示在下方。本书是一本Jupyter笔记本，因此你可以运行代码块（如下一个）以获得相同的结果（如果代码单元格不打算运行，我会告诉你）：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we progress through this book, I do not repeatedly show all the imports,
    as that would be mostly a waste of paper. Instead, they are available online as
    part of the downloadable copy of the code, which can be found at [https://github.com/EdwardRaff/Inside-Deep-Learning](https://github.com/EdwardRaff/Inside-Deep-Learning).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们这本书的进展，我不会反复展示所有的导入，因为这大部分是浪费纸张。相反，它们作为代码下载副本的一部分在线提供，可以在[https://github.com/EdwardRaff/Inside-Deep-Learning](https://github.com/EdwardRaff/Inside-Deep-Learning)找到。
- en: 1.2 The world as tensors
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 世界作为张量
- en: 'Deep learning has been used on spreadsheets, audio, images, and text, but deep
    learning frameworks don’t use classes or objects to distinguish between kinds
    of data. Instead, they work with one data type, and we must convert our data into
    this format. For PyTorch, this singular view of the world is through a *tensor*
    object. Tensors are used to represent both data, the inputs/outputs to any deep
    learning block, and the parameters that control the behavior of our networks.
    Two essential features are built into tensor objects: the ability to do fast parallel
    computation with GPUs and the ability to do some calculus (derivatives) automatically.
    With prior ML experience in Python, you should have prior experience with NumPy,
    which also uses the tensor concept. In this section, we quickly review the tensor
    concept and note how tensors in PyTorch differ from NumPy and form the foundation
    for our deep learning building blocks.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经在电子表格、音频、图像和文本上得到了应用，但深度学习框架并不使用类或对象来区分数据类型。相反，它们使用一种数据类型，我们必须将我们的数据转换为这种格式。对于PyTorch来说，这种对世界的单一视角是通过一个*tensor*对象实现的。张量用于表示数据，任何深度学习块的输入/输出，以及控制我们网络行为的参数。张量对象中内置了两个基本功能：使用GPU进行快速并行计算的能力，以及自动进行一些微积分（导数）的能力。如果你在Python中有先前的机器学习经验，你应该对NumPy也有先前的经验，NumPy也使用了张量概念。在本节中，我们快速回顾了张量概念，并说明了PyTorch中的张量与NumPy的不同之处，这为我们深度学习构建块奠定了基础。
- en: We begin by importing the `torch` library and discussing tensors, which are
    also called n-dimensional arrays. Both NumPy and PyTorch allow us to create n-dimensional
    arrays. A zero-dimensional array is called a *scalar* and is any single number
    (e.g., 3.4123). A one-dimensional array is a *vector* (e.g., [1.9, 2.6, 3.1, 4.0,
    5.5]), and a two-dimensional array is a *matrix*. Scalars, vectors, and matrices
    are all tensors. In fact, any value of n for an n-dimensional array is still a
    tensor. The word *tensor* refers to the overall concept of an n-dimensional array.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入`torch`库，并讨论张量，它们也被称为n维数组。NumPy和PyTorch都允许我们创建n维数组。零维数组称为*标量*，是任何单个数字（例如，3.4123）。一维数组是*向量*（例如，[1.9,
    2.6, 3.1, 4.0, 5.5]），二维数组是*矩阵*。标量、向量和矩阵都是张量。实际上，n维数组的任何n值仍然是张量。*张量*这个词指的是n维数组的整体概念。
- en: We care about tensors because they are a convenient way to organize much of
    our data and algorithms. This is the first foundation that PyTorch provides, and
    we often convert NumPy tensors to PyTorch tensors. Figure 1.4 shows four tensors,
    their shapes, and the mathematical way to express the shape. Extending the pattern,
    a four-dimensional tensor could be written as (*B*,*C*,*W*,*H*) or as ℝ^(*B*,
    *C*, *W*, *H*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注张量，因为它们是组织我们大部分数据和算法的一种方便方式。这是PyTorch提供的第一个基础，我们通常将NumPy张量转换为PyTorch张量。图1.4显示了四个张量、它们的形状以及表示形状的数学方法。按照这个模式扩展，四维张量可以写成(*B*,*C*,*W*,*H*)或作为ℝ^(*B*,
    *C*, *W*, *H*)。
- en: '![](../Images/CH01_F04_Raff.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F04_Raff.png)'
- en: Figure 1.4 Examples of tensors, with more dimensions or *axes* as we move from
    left to right. A scalar represents a single value. A vector is a list of values
    and is how we often think about one datapoint. A matrix is a grid of values and
    is often used for a dataset. A three-dimensional tensor can be used to represent
    a dataset of sequences.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4展示了张量的示例，随着我们从左到右移动，维度或*轴*越来越多。标量代表单个值。向量是一系列值，这是我们通常思考一个数据点的方式。矩阵是一系列值的网格，通常用于数据集。三维张量可以用来表示序列数据集。
- en: Tensor dimensions
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 张量维度
- en: 'When writing out the dimensions of a tensor like (*B*,*C*,*W*,*H*), we often
    use single-letter names with common notations or meanings behind the letters.
    This works as a useful shorthand, and you will see it frequently in code. Most
    are explained in more detail in later chapters, but some common dimension letters
    you will see include the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当写出张量的维度，如(*B*,*C*,*W*,*H*)时，我们经常使用具有常见符号或含义的单字母名称。这是一个有用的缩写，你将在代码中经常看到它。大多数将在后面的章节中详细解释，但以下是一些你将看到的常见维度字母：
- en: B—The number of batches being used.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B—正在使用的批次数。
- en: D or H—The number of neurons/outputs in a hidden layer (sometimes N is also
    used for this).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D或H—隐藏层中的神经元/输出数量（有时N也用于此）。
- en: C—The number of channels in an input (e.g., think of “Red, Green, Blue” as three
    channels) or the number of classes/categories that a model could output.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C—输入中的通道数（例如，将“红色、绿色、蓝色”视为三个通道）或模型可能输出的类别/类别的数量。
- en: W and H—The width and height of an image (almost always in conjunction with
    a “C” dimension for the channels of an image).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W和H—图像的宽度和高度（几乎总是与图像通道的“C”维度一起使用）。
- en: T—The number of items in a sequence (more on that in chapter 4).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T—序列中元素的数量（更多内容请参考第4章）。
- en: We use common notation to associate math symbols with tensors of a specific
    shape. A capital letter like X or Q represents a tensor with two or more dimensions.
    If we are talking about a vector, we use a lowercase bold letter like x or h.
    Last, we use a lowercase non-bold letter like x or h for a scalar.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用常见的符号来将数学符号与特定形状的张量关联起来。大写字母如X或Q代表具有两个或更多维度的张量。如果我们谈论一个向量，我们使用小写粗体字母如x或h。最后，我们使用小写非粗体字母如x或h来表示标量。
- en: In talking about and implementing neural networks, we often refer to a row in
    a larger matrix or a scalar in a larger vector. This is shown in figure 1.5 and
    is often called *slicing*. So if we have a matrix X, we can use **x**[i] to reference
    the ith row of X. In code, that is `x_i = X[i,:]`. If we want the ith row and
    jth column, it becomes *x*[*i*, *j*], which is not bold because it is a reference
    to a single value—making it a scalar. The code version is `x_ij = X[i,j]`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论和实现神经网络时，我们经常提到较大矩阵中的一行或较大向量中的一个标量。这如图1.5所示，通常被称为*切片*。所以如果我们有一个矩阵X，我们可以使用**x**[i]来引用X的第i行。在代码中，这表示为`x_i
    = X[i,:]`。如果我们想要第i行和第j列，它变为*x*[*i*, *j*]，因为它是一个对单个值的引用——使其成为一个标量。代码版本是`x_ij =
    X[i,j]`。
- en: '![](../Images/CH01_F05_Raff.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F05_Raff.png)'
- en: Figure 1.5 A tensor can be sliced to grab sub-tensors from a larger one. For
    example, in red, we grab a row-vector from the larger matrix; and in blue, we
    grab a column-vector from the matrix. Depending on what the tensor represents,
    this can let us manipulate different parts of the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 张量可以被切割以从较大的张量中获取子张量。例如，在红色部分，我们从较大的矩阵中获取一个行向量；在蓝色部分，我们从矩阵中获取一个列向量。根据张量所代表的内容，这可以让我们操作数据的不同部分。
- en: 'To use PyTorch, we need to import it as the `torch` package. With it, we can
    immediately start creating tensors. Every time you nest a list within another
    list, you create a new dimension of the tensor that PyTorch will produce:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用PyTorch，我们需要将其导入为`torch`包。有了它，我们就可以立即开始创建张量。每次你在另一个列表内部嵌套一个列表时，你都会创建PyTorch将产生的张量的新维度：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ You don’t have to format it like I did; that’s just for clarity.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你不必像我这样格式化它；这只是为了清晰起见。
- en: 'If we print the shapes of these tensors, you should see the same shapes shown
    earlier. Again, while scalars, vectors, and matrices are different things, they
    are unified under the larger umbrella of tensors. We care about this because we
    use tensors of different shapes to represent different types of data. We get to
    those details later; for now, we focus on the mechanics PyTorch provides to work
    with tensors:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印这些张量的形状，你应该看到之前显示的相同形状。同样，虽然标量、向量和矩阵是不同的事物，但它们都统一在更大的张量范畴之下。我们关注这一点是因为我们使用不同形状的张量来表示不同类型的数据。我们将在稍后详细讨论这些细节；现在，我们专注于PyTorch提供的用于处理张量的机制：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you have done any ML or scientific computing in Python, you have probably
    used the NumPy library. As you would expect, PyTorch supports converting NumPy
    objects into their PyTorch counterparts. Since both of them represent data as
    tensors, this is a painless process. The following two code blocks show how we
    can create a random matrix in NumPy and then convert it into a PyTorch `Tensor`
    object:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Python中做过任何机器学习或科学计算，您可能已经使用过NumPy库。正如您所期望的，PyTorch支持将NumPy对象转换为它们的PyTorch对应物。由于它们都表示数据为张量，这是一个无缝的过程。以下两个代码块展示了我们如何在NumPy中创建一个随机矩阵，然后将其转换为PyTorch的`Tensor`对象：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Both NumPy and torch support multiple different data types. By default, NumPy
    uses 64-bit floats, and PyTorch defaults to 32-bit floats. However, if you create
    a PyTorch tensor from a NumPy tensor, it uses the same type as the given NumPy
    tensor. You can see that in the previous output, where PyTorch let us know that
    `dtype=torch.float64` since it is not the default choice.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy和torch都支持多种不同的数据类型。默认情况下，NumPy使用64位浮点数，而PyTorch默认使用32位浮点数。然而，如果您从NumPy张量创建PyTorch张量，它将使用与给定NumPy张量相同的类型。您可以在前面的输出中看到这一点，其中PyTorch告诉我们`dtype=torch.float64`，因为它不是默认选择。
- en: 'The most common types we care about for deep learning are 32-bit floats, 64-bit
    integers (`Long`s), and booleans (i.e., binary `True`/`False`). Most operations
    leave the tensor type unchanged unless we explicitly create or cast it to a new
    type. To avoid issues with types, you can specify explicitly what type of tensor
    you want to create when calling a function. The following code checks what type
    of data is contained in our tensor using the `dtype` attribute:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习，我们最关心的类型是32位浮点数、64位整数（`Long`s）和布尔值（即二进制`True`/`False`）。大多数操作不会改变张量类型，除非我们明确创建或将其转换为新的类型。为了避免类型问题，您可以在调用函数时明确指定您想要创建的张量类型。以下代码使用`dtype`属性检查我们的张量中包含的数据类型：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Let’s force them to be 32-bit floats.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 让我们强制它们成为32位浮点数。
- en: The main exception to using 32-bit floats or 64-bit integers as the `dtype`
    is when we need to perform logic operations (like Boolean AND, OR, NOT), which
    we can use to quickly create *binary masks*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用32位浮点数或64位整数作为`dtype`的主要例外是当我们需要执行逻辑运算（如布尔AND、OR、NOT）时，我们可以使用这些运算来快速创建*二进制掩码*。
- en: 'A mask is a tensor that tells us which portions of another tensor are valid
    to use. We use masks in some of our more complex neural networks. For example,
    let’s say we want to find every value greater than 0.5 in a tensor. Both PyTorch
    and NumPy let us use the standard logic operators to check for things like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码是一个张量，它告诉我们另一个张量的哪些部分是有效的。我们在一些更复杂的神经网络中使用掩码。例如，假设我们想在张量中找到所有大于0.5的值。PyTorch和NumPy都允许我们使用标准的逻辑运算符来检查这类情况：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While the NumPy and PyTorch APIs are not identical, they share many functions
    with the same names, behaviors, and characteristics:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NumPy和PyTorch的API并不完全相同，但它们共享许多具有相同名称、行为和特性的函数：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'While many functions are the same, some are not *quite* identical. There may
    be slight differences in behavior or in the arguments required. These discrepancies
    are usually because the PyTorch version has made changes that are particular to
    how these methods are used for neural network design and execution. Following
    is an example of the `transpose` function, where PyTorch requires us to specify
    which two dimensions to transpose. NumPy takes the two dimensions and transposes
    them without complaint:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多函数是相同的，但有些并不完全相同。它们的行为或所需的参数可能存在细微差异。这些差异通常是因为PyTorch版本对这些方法用于神经网络设计和执行的方式进行了特定的更改。以下是一个`transpose`函数的示例，其中PyTorch要求我们指定要转置的两个维度。NumPy则不提出异议地转置这两个维度：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'PyTorch does this because we often want to transpose dimensions of a tensor
    for deep learning applications, whereas NumPy tries to stay with more general
    expectations. As shown next, we can transpose two of the dimensions in our `torch_tensor3d`
    from the start of the chapter. Originally it had a shape of (4,2,3). If we transpose
    the first and third dimensions, we get a shape of (3,2,4):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch这样做是因为我们通常希望为深度学习应用转置张量的维度，而NumPy则试图保持更通用的期望。如下所示，我们可以从本章的开头开始转置`torch_tensor3d`中的两个维度。最初它的形状是(4,2,3)。如果我们转置第一和第三维度，我们得到形状为(3,2,4)：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Because such differences exist, you should always double-check the PyTorch documentation
    at [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
    if you attempt to use a function you are familiar with but suddenly find it does
    not behave as expected. It is also a good tool to have open when using PyTorch.
    There are many different functions that can help you in PyTorch, and we cannot
    review them all.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在这样的差异，如果你尝试使用一个你熟悉的函数，但突然发现它没有按预期工作，你应该始终在 [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
    上双检查 PyTorch 文档。这也是在使用 PyTorch 时打开的好工具。PyTorch 中有许多不同的函数可以帮助你，我们无法全部进行审查。
- en: 1.2.1  PyTorch GPU acceleration
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 PyTorch GPU 加速
- en: The first important functionality that PyTorch gives us beyond what NumPy can
    do is using a GPU to accelerate mathematical calculations. GPUs are hardware in
    your computer specifically designed for 2D and 3D graphics, mainly to accelerate
    videos (watching an HD movie) or play video games. What does that have to do with
    neural networks? Well, a lot of the math involved in making 2D and 3D graphics
    fast istensor-based or at least tensor-related. For this reason, GPUs have been
    getting good at doing many things we want very quickly. As graphics, and thus
    GPUs, became better and more powerful, people realized they could also be used
    for scientific computing and ML.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 给我们的第一个重要功能，超出了 NumPy 的能力，是使用 GPU 加速数学计算。GPU 是你电脑中专门为 2D 和 3D 图形设计的硬件，主要用于加速视频（观看高清电影）或玩视频游戏。这与神经网络有什么关系呢？嗯，制作
    2D 和 3D 图形所需的大部分数学都是基于张量或至少与张量相关。因此，GPU 在快速执行我们想要做的事情方面变得越来越擅长。随着图形和 GPU 的变得更好、更强大，人们意识到它们也可以用于科学计算和机器学习。
- en: At a high level, you can think of GPUs as giant tensor calculators. You should
    almost always use a GPU when doing anything with neural networks. It is a good
    pair-up since neural networks are compute-intensive, and GPUs are fast at the
    exact type of computations we need to perform. If you want to do deep learning
    in a professional context, you should invest in a computer with a powerful NVIDIA
    GPU. But for now, we can get by for *free* using Colab.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，你可以将 GPU 视为巨大的张量计算器。在处理任何与神经网络相关的事情时，你应该几乎总是使用 GPU。这是一个很好的搭配，因为神经网络是计算密集型的，而
    GPU 在执行我们需要的精确类型计算方面非常快。如果你想在专业环境中进行深度学习，你应该投资一台配备强大 NVIDIA GPU 的电脑。但就目前而言，我们可以免费使用
    Colab 来解决问题。
- en: The trick to using GPUs effectively is to avoid computing on a *small* amount
    of data. This is because your computer’s CPU must first move data to the GPU,
    then ask the GPU to perform its math, wait for the GPU to finish, and then copy
    the results back from the GPU. The steps in this process are fairly slow; and
    if we are only calculating a few things, using a GPU takes longer than the CPU
    would take to do the math.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPU 的技巧是避免在 *少量* 数据上进行计算。这是因为你的电脑的 CPU 必须首先将数据移动到 GPU，然后请求 GPU 执行数学运算，等待
    GPU 完成运算，然后将结果从 GPU 复制回来。这个过程中的步骤相当慢；如果我们只计算少量的事情，使用 GPU 比使用 CPU 做数学运算要慢。
- en: What counts as “too small?" Well, that depends on your CPU, the GPU, and the
    math you are doing. If you are worried about this problem, you can do some benchmarking
    to see if using the CPU is faster. If so, you are probably working on too little
    data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是“太小”？这取决于你的 CPU、GPU 和你正在做的数学。如果你担心这个问题，你可以进行一些基准测试，看看使用 CPU 是否更快。如果是这样，你可能在处理的数据太少。
- en: Let’s test this with matrix multiplication—a basic linear algebra operation
    that is common in neural networks. If we have matrices *X*^(*n*, *m*) and *Y*^(*m*,
    *p*), we can compute a resulting matrix *C*^(*n*, *p*) = *X*^(*n*, *m*)*Y*^(*m*,
    *p*). Note that C has as many rows as X and as many columns as Y. When implementing
    neural networks, we do lots of operations that change the *shape* of a tensor,
    like what happens when we multiply two matrices together. This is a common source
    of bugs, so you should think about tensor shapes when writing code.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用矩阵乘法来测试这个方法——这是一种基本的线性代数运算，在神经网络中很常见。如果我们有矩阵 *X*^(*n*, *m*) 和 *Y*^(*m*,
    *p*)，我们可以计算出一个结果矩阵 *C*^(*n*, *p*) = *X*^(*n*, *m*)*Y*^(*m*, *p*)。请注意，C 的行数与 X
    相同，列数与 Y 相同。在实现神经网络时，我们会进行很多改变张量 *shape* 的操作，就像当我们相乘两个矩阵时发生的情况一样。这是错误的一个常见来源，因此在编写代码时你应该考虑张量的形状。
- en: 'We can use the `timeit` library: it lets us run code multiple times and tells
    us how long it took to run. We make a larger matrix X, compute XX several times,
    and see how long that takes to run:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `timeit` 库：它允许我们多次运行代码，并告诉我们运行所需的时间。我们创建一个较大的矩阵 X，多次计算 XX，看看这需要多长时间运行：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It takes a bit of time to run that code, but not too long. On my computer,
    it took 6.172 seconds to run, which is stored in the `time_cpu` variable. Now,
    how do we get PyTorch to use our GPU? First we need to create a `device` reference.
    We can ask PyTorch to give us one using the `torch.device` function. If you have
    an NVIDIA GPU, and the CUDA drivers are installed properly, you should be able
    to pass in `cuda` as a string and get back an object representing that device:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码需要一点时间，但不算太长。在我的电脑上，它运行了 6.172 秒，这个时间存储在 `time_cpu` 变量中。现在，我们如何让 PyTorch
    使用我们的 GPU？首先，我们需要创建一个 `device` 引用。我们可以使用 `torch.device` 函数请求 PyTorch 给我们一个。如果你有一个
    NVIDIA GPU，并且 CUDA 驱动程序安装正确，你应该能够传入 `cuda` 作为字符串，并得到代表该设备的对象：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have a reference to the GPU (device) we want to use, we need to
    ask PyTorch to move that object to the given device. Luckily, that can be done
    with a simple `to` function; then we can use the same code as before:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了想要使用的 GPU（设备）的引用，我们需要请求 PyTorch 将该对象移动到指定的设备。幸运的是，这可以通过一个简单的 `to` 函数来完成；然后我们可以使用之前的相同代码：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When I run this code, the time to perform 100 multiplications is 0.6191 seconds,
    which is an instant 9.97× speedup. This was a pretty ideal case, as matrix multiplications
    are super-efficient on GPUs, and we created a big matrix. You should try making
    the matrix smaller and see how that impacts the speedup you get.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这段代码时，执行 100 次乘法的时间是 0.6191 秒，这几乎是瞬间 9.97 倍的速度提升。这是一个相当理想的情况，因为矩阵乘法在 GPU
    上非常高效，而且我们创建了一个大矩阵。你应该尝试减小矩阵的大小，看看这对你的速度提升有何影响。
- en: 'Be aware that this only works if every object involved is on the same device.
    Say you run the following code, where the variable `x` has been moved onto the
    GPU and `y` has not (so it is on the CPU by default):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这只有在涉及的所有对象都在同一设备上时才有效。比如说，你运行以下代码，其中变量 `x` 已经被移动到 GPU 上，而 `y` 没有被移动（因此默认在
    CPU 上）：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You will end up getting an error message that says:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你最终会得到一个错误信息，它说：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The error tells you which device the first variable is on (`cuda:0`) but that
    the second variable was on a different device (`cpu`). If we instead wrote `y*x`
    you would see the error change to `expected device cpu but got device cuda:0`.
    Whenever you see an error like this, you have a bug that kept you from moving
    everything to the same compute device.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 错误信息告诉你第一个变量在哪个设备上（`cuda:0`），但第二个变量在另一个设备上（`cpu`）。如果我们改为写 `y*x`，你会看到错误变为 `expected
    device cpu but got device cuda:0`。每次你看到这样的错误时，你都有一个阻止你将所有内容移动到同一计算设备上的错误。
- en: 'The other thing to be aware of is how to convert PyTorch data back to the CPU.
    For example, we may want to convert a tensor back to a NumPy array so that we
    can pass it to Matplotlib or save it to disk. The PyTorch `tensor` object has
    a `.numpy()` method that will do this, but if you call `x.numpy()`, you will get
    this error:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件需要注意的事情是如何将 PyTorch 数据转换回 CPU。例如，我们可能希望将一个张量转换回 NumPy 数组，以便我们可以将其传递给 Matplotlib
    或保存到磁盘。PyTorch 的 `tensor` 对象有一个 `.numpy()` 方法可以完成这个操作，但如果你调用 `x.numpy()`，你会得到这个错误：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Instead, you can use the handy shortcut function `.cpu()` to move an object
    back to the CPU, where you can interact with it normally. So you will often see
    code that looks like `x.cpu().numpy()` when you want to access the results of
    your work.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以使用方便的快捷函数 `.cpu()` 将对象移回 CPU，在那里你可以正常与之交互。所以，当你想要访问你工作的结果时，你经常会看到 `x.cpu().numpy()`
    这样的代码。
- en: 'The `.to()` and `.cpu()` methods make it easy to write code that is suddenly
    GPU accelerated. Once on a GPU or similar compute device, almost *every* method
    that comes with PyTorch can be used and will net you a nice speedup. But sometimes
    we want to store tensors and other PyTorch objects in a list, dictionary, or other
    standard Python collection. To help with that, we can define this `moveTo` function,
    which goes recursively through the common Python and PyTorch containers and moves
    every object found onto the specified device:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`.to()` 和 `.cpu()` 方法使得编写突然加速的 GPU 代码变得容易。一旦在 GPU 或类似的计算设备上，几乎 PyTorch 伴随的
    *每个* 方法都可以使用，并且会带来不错的速度提升。但有时我们希望将张量和其他 PyTorch 对象存储在列表、字典或其他标准 Python 集合中。为了帮助解决这个问题，我们可以定义这个
    `moveTo` 函数，它会递归地遍历常见的 Python 和 PyTorch 容器，并将找到的每个对象移动到指定的设备上：'
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first time we printed the arrays, we saw `tensor(1)` and `tensor(2)`; but
    after using the `moveTo` function, `device=cuda:0` appeared. We won’t have to
    use this function often, but when we do, it will make our code easier to read
    and write. With that, we now have the fundamentals to write *fast* code accelerated
    by GPUs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次打印数组时，看到了`tensor(1)`和`tensor(2)`；但使用`moveTo`函数后，出现了`device=cuda:0`。我们不会经常使用这个函数，但当我们使用它时，它会使我们的代码更容易阅读和编写。有了这个，我们现在有了使用GPU加速编写**快速**代码的基础。
- en: Why do we care about GPUs?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么关心GPU？
- en: Using a GPU is fundamentally about *speed*. It can literally be the difference
    between waiting *hours* or *minutes* for a neural network to train—and this is
    before we consider very large networks or huge datasets. I’ve tried to make every
    individual neural network that is trained in this book take 10 minutes or less,
    and in most cases under 5 minutes, when using a GPU. That means using toy problems
    and finagling them to show behavior that is representative of real life.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU本质上是为了**速度**。它实际上可能是等待神经网络训练**数小时**或**数分钟**的区别——这还是在考虑非常大的网络或巨大的数据集之前。我试图让这本书中训练的每一个神经网络在10分钟或更短的时间内完成，在大多数情况下不到5分钟，当使用GPU时。这意味着使用玩具问题和调整它们以展示代表现实生活的行为。
- en: Why not learn on real-world data and problems? Because real-world neural networks
    can take *days* or *weeks* to train. Some of the research I have done for my day
    job can take *a month* to train using *multiple* GPUs. The code we write is perfectly
    good and valid for real-world tasks, but we need to wait a little longer for the
    results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用真实世界的数据和问题进行学习？因为真实世界的神经网络可能需要**数天**或**数周**来训练。我为我的日常工作所做的某些研究可能需要使用**多个**GPU**一个月**的时间来训练。我们编写的代码对于真实世界的任务来说是完全良好和有效的，但我们需要等待更长的时间才能得到结果。
- en: 'This long compute time also means you need to learn how to be productive while
    your model is training. One way is to develop new code for your next model on
    a spare machine or use the CPUs while your GPU is busy. You won’t be able to train
    it, but you can push a tiny amount of data through to make sure no errors happen.
    This is also why I want you to learn how to map the math used in deep learning
    to code: so while your model is busy training, you can be reading about the latest
    and greatest deep learning tools that might help you.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这漫长的计算时间也意味着你需要学会如何在模型训练时保持高效。一种方法是在备用机器上为你的下一个模型开发新代码，或者在你使用GPU时使用CPU。你将无法训练它，但你可以推送一小部分数据以确保没有错误发生。这也是为什么我想让你学习如何将深度学习中使用的数学映射到代码中：这样，当你的模型忙于训练时，你可以阅读关于最新和最伟大的深度学习工具的信息，这些工具可能对你有所帮助。
- en: 1.3 Automatic differentiation
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 自动微分
- en: 'So far, we’ve seen that PyTorch provides an API similar to NumPy for performing
    mathematical operations on tensors, with the advantage of using a GPU (when available)
    to perform faster math operations. The second major foundation that PyTorch gives
    us is *automatic differentiation*: as long as we use PyTorch-provided functions,
    PyTorch can compute *derivatives* (also called *gradients*) automatically for
    us. In this section, we learn what that means and how automatic differentiation
    ties into the task of minimizing a function. In the next section, we see how to
    wrap it all up in a simple API provided by PyTorch.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到PyTorch提供了一个类似于NumPy的API，用于在张量上执行数学运算，并且当可用时使用GPU来执行更快的数学运算。PyTorch给我们提供的第二个主要基础是**自动微分**：只要我们使用PyTorch提供的函数，PyTorch就可以为我们自动计算**导数**（也称为**梯度**）。在本节中，我们将了解这意味着什么以及自动微分如何与最小化函数的任务相结合。在下一节中，我们将看到如何使用PyTorch提供的简单API将这些内容封装起来。
- en: 'Your first thought may be, “What is a derivative, and why do I care about that?”
    Remember from calculus that the derivative of a function *f*(*x*) tells us how
    quickly the value of *f*(*x*) is changing. We care about this because we can use
    the derivative of a function *f*(*x*) to help us find an input *x*^* that is a
    *minimizer* of *f*(*x*). The value *x*^* being a minimizer means the value of
    *f*(*x*^*) is smaller than *f*(*x*^*+*z*) for whatever value we set z to. The
    mathy way to say this is *f*(*x*^*) ≤ *f*(*z*), ∀*x*^* ≠ *z*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一个想法可能是，“什么是导数，我为什么要关心它？”记住，从微积分中我们知道函数**f**(*x*)的导数告诉我们**f**(*x*)的值变化有多快。我们关心这一点，因为我们可以使用函数**f**(*x*)的导数来帮助我们找到输入**x**^*，它是**f**(*x*)的**最小化者**。**x**^*是**最小化者**意味着**f**(*x*^*)的值小于**f**(*x*^*+*z*)，无论我们设置z的值是多少。用数学的方式来说，就是**f**(*x*^*)
    ≤ **f**(*z*)，∀**x**^* ≠ **z**：
- en: '![](../Images/ch1-eqs-to-illustrator0x.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch1-eqs-to-illustrator0x.png)'
- en: 'Another way to say this is that if I wrote the following code, I would be stuck
    waiting for an infinite loop:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种说法是，如果我编写以下代码，我就会陷入无限循环的等待：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Why do we want to minimize a function? For all the kinds of ML and deep learning
    we discuss in this book, we train neural networks by defining a *loss function*.
    The loss function tells the network, in a numeric and quantifiable way, how *badly*
    it is doing at the problem. So if the loss is high, things are going poorly. A
    high loss means the network is losing the game, and badly. If the loss is zero,
    the network has solved the problem perfectly. We don’t usually allow the loss
    to go negative because that gets confusing to think about.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要最小化一个函数呢？对于本书中讨论的所有机器学习和深度学习类型，我们通过定义 *损失函数* 来训练神经网络。损失函数以数值和可量化的方式告诉网络，它在问题上的表现有多“糟糕”。所以如果损失值高，事情就进行得不好。高损失意味着网络正在输掉比赛，而且输得很惨。如果损失为零，则网络完美地解决了问题。我们通常不允许损失为负，因为这会让思考变得混乱。
- en: When you read math about neural networks, you will often see the loss function
    defined as ℓ(*x*), where x are the inputs to the network and ℓ(*x*) gives us the
    loss the network received. Because of this, *loss functions return scalars*. This
    is important because we can compare scalars and say that one is definitively bigger
    or smaller than another, so it becomes unambiguous how bad a network is at the
    game. The derivative is generally defined with respect to a single variable, but
    our networks will have many variables (parameters). When getting the derivative
    with respect to multiple variables, we call it a *gradient*; you can apply the
    same intuition about derivatives and one variable to gradients over many variables.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读关于神经网络的数学时，你经常会看到损失函数被定义为 ℓ(*x*)，其中 x 是网络的输入，ℓ(*x*) 给出网络收到的损失。正因为如此，*损失函数返回标量*。这很重要，因为我们可以比较标量，并说一个绝对比另一个大或小，这样就可以明确网络在游戏中的表现有多糟糕。导数通常定义与一个单一变量相关，但我们的网络将有许多变量（参数）。当我们对多个变量求导时，我们称之为
    *梯度*；你可以将关于导数和单一变量的相同直觉应用到多个变量的梯度上。
- en: We have stated that gradients are helpful, and perhaps you remember from a calculus
    class about minimizing functions using derivatives and gradients. Let’s do a bit
    of a math reminder about how to find the minimum of a function using calculus.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说过梯度是有帮助的，也许你还记得从微积分课程中学到的关于使用导数和梯度最小化函数的内容。让我们通过微积分来回顾一下如何找到函数的极小值。
- en: 'Say we have the function *f*(*x*) = (*x*−2)². Let’s define that with some PyTorch
    code and plot what the function looks like:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数 *f*(*x*) = (*x*−2)²。让我们用一些 PyTorch 代码来定义它，并绘制函数的形状：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH01_UN01_Raff.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_UN01_Raff.png)'
- en: 1.3.1  Using derivatives to minimize losses
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 使用导数最小化损失
- en: We can clearly see that the minimum of this function is at *x* = 2, where we
    get the value *f*(2) = 0. But this is an intentionally easy problem. Let’s say
    we can’t plot it; we can use calculus to help us find the answer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到这个函数的极小值在 *x* = 2，在那里我们得到 *f*(2) = 0。但这是一个故意设计得比较简单的问题。假设我们无法绘制它；我们可以使用微积分来帮助我们找到答案。
- en: We denote the derivative of *f*(*x*) as *f*′(*x*), and we can get the answer
    (using calculus) that *f*′(*x*) = 2 ⋅ *x* − 4. The minimum of a function (*x*^*)
    exists at *critical points*, which are points where *f*′(*x*) = 0. So let’s find
    them by solving for x. In our case, we get
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *f*(*x*) 的导数表示为 *f*′(*x*)，并且我们可以通过微积分得到答案：*f*′(*x*) = 2 ⋅ *x* − 4。函数的极小值存在于
    *临界点*，即 *f*′(*x*) = 0 的点。因此，让我们通过解 x 来找到它们。在我们的例子中，我们得到
- en: 2 ⋅ *x* − 4 = 0
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2 ⋅ *x* − 4 = 0
- en: 2 ⋅ *x* = 4
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 2 ⋅ *x* = 4
- en: (add 4 to both sides)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: （两边同时加4）
- en: '*x* = 4/2 = 2'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* = 4/2 = 2'
- en: (divide each side by 2).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: （两边同时除以2）。
- en: This required us to solve the equation for when *f*′(*x*) = 0. PyTorch can’t
    quite do that for us because we are going to be developing more complicated functions
    where finding the *exact* answer is not possible. But say we have a current guess,
    *x*^?, that we are pretty sure is not the minimizer. We can use *f*′(*x*^?) to
    help us determine how to adjust *x*^? so that we move closer to a minimizer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求我们解方程，当 *f*′(*x*) = 0 时。PyTorch 无法为我们做到这一点，因为我们将要开发更复杂的函数，在这些函数中找到 *确切* 的答案是不可能的。但是，假设我们有一个当前猜测，*x*^?，我们相当确信它不是最小值。我们可以使用
    *f*′(*x*^?) 来帮助我们确定如何调整 *x*^?，以便我们更接近最小值。
- en: 'How is that possible? Let’s plot *f*(*x*) and *f*′(*x*) at the same time:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？让我们同时绘制 *f*(*x*) 和 *f*′(*x*)：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Defines the derivative of f(x) manually
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 手动定义 f(x) 的导数
- en: ❷ Draws a black line at 0 so we can easily tell if something is positive or
    negative
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 0 处画一条黑色线，这样我们就可以很容易地判断某个值是正数还是负数
- en: '![](../Images/CH01_UN02_Raff.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_UN02_Raff.png)'
- en: Look at the orange line. When we are too far to the left of the minimum (*x*
    = 2), we see that *f*′(*x*^?) < 0. When we are to the right of the minimum, we
    instead get *f*′(*x*) > 0. Only when we are at a minimum do we see that *f*′(*x*^?)
    = 0. So if *f*′(*x*^?) < 0, we need to increase *x*^?; and if *f*′(^?*x*) > 0,
    we need to decrease the value of *x*^?. The *sign* of the gradient *f*′ tells
    us which *direction* we should move to find a minimizer. This process of *gradient
    descent* is summarized in figure 1.6.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 看看橙色线。当我们离最小值 (*x* = 2) 太远时，我们看到 *f*′(*x*^?) < 0。当我们位于最小值右侧时，我们反而得到 *f*′(*x*)
    > 0。只有当我们处于最小值时，我们才看到 *f*′(*x*^?) = 0。所以如果 *f*′(*x*^?) < 0，我们需要增加 *x*^?；如果 *f*′(^?*x*)
    > 0，我们需要减小 *x*^? 的值。梯度的 *符号*告诉我们应该朝哪个 *方向*移动以找到最小值。这个过程称为 *梯度下降*，总结在图 1.6 中。
- en: '![](../Images/CH01_F06_Raff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F06_Raff.png)'
- en: 'Figure 1.6 The process to minimize a function *f*(*x*) using its derivative
    *f*′(*x*) is called *gradient descent*, and this figure shows how it is done.
    We iteratively compute *f*′(*x*) to decide whether x should be larger or smaller
    to make the value of *f*(*x*) as small as possible. The process stops when we
    are close enough to the gradient being zero. You can also stop early if you have
    done a lot of updates: “close enough is good enough” holds true for deep learning,
    and we rarely need to perfectly minimize a function.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 使用函数的导数 *f*′(*x*) 来最小化函数 *f*(*x*) 的过程称为 *梯度下降*，此图展示了它是如何进行的。我们迭代地计算 *f*′(*x*)
    来决定 x 应该更大还是更小，以使 *f*(*x*) 的值尽可能小。当我们的位置足够接近梯度为零时，这个过程停止。如果你已经进行了很多更新，你也可以提前停止：“足够接近就是足够好”在深度学习中是成立的，我们很少需要完美地最小化一个函数。
- en: 'We also care about the *magnitude* of *f*′(*x*^?). Because we are looking at
    a one-dimensional function, the magnitude just means the absolute value of *f*′(*x*^?):
    i.e., |*f*′(*x*^?)|. The magnitude gives us an idea how far we are from the minimizer.
    So the sign of *f*′(*x*^?) (<0 or >0) tells us which *direction* we should move,
    and the size (|*f*′(*x*)|) tells us how *far* we should move.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也关心 *f*′(*x*^?) 的 *大小*。因为我们正在看一维函数，大小只是 *f*′(*x*^?) 的绝对值：即，|*f*′(*x*^?)|。大小给我们一个我们离最小值有多远的想法。所以
    *f*′(*x*^?) 的符号 (<0 或 >0) 告诉我们应该朝哪个 *方向*移动，而大小 (|*f*′(*x*)|) 告诉我们应该移动多 *远*。
- en: 'This is not a coincidence. It is *always* true for any function. If we can
    compute a derivative, we can find a minimizer. You may be thinking, “I don’t remember
    my calculus all that well,” or complaining that I skipped the steps about how
    to compute *f*′(*x*). This is why we use PyTorch: automatic differentiation computes
    the value of *f*′(*x*) for us. Let’s use the toy example of *f*(*x*) = (*x*−2)²
    to see how it works.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是巧合。对于任何函数，这总是**始终**成立的。如果我们能计算导数，我们就能找到一个最小值。你可能正在想，“我不太记得我的微积分了，”或者抱怨我跳过了如何计算
    *f*′(*x*) 的步骤。这就是我们使用 PyTorch 的原因：自动微分会为我们计算 *f*′(*x*) 的值。让我们用 *f*(*x*) = (*x*−2)²
    的玩具例子来看看它是如何工作的。
- en: 1.3.2  Calculating a derivative with automatic differentiation
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 使用自动微分计算导数
- en: 'Now that we understand the concept of minimizing a function using its derivative,
    let’s walk through the mechanics of doing it in PyTorch. First, let’s create a
    new variable to minimize. We do this similar to before, but we add a new flag
    telling PyTorch to keep track of the gradient. This is stored in a variable called
    `grad`, which does not exist yet since we haven’t computed anything:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了使用函数的导数来最小化函数的概念，让我们来看看在 PyTorch 中实现它的机制。首先，让我们创建一个新的变量来最小化。我们这样做与之前类似，但我们会添加一个新的标志告诉
    PyTorch 保持跟踪梯度。这存储在一个名为 `grad` 的变量中，因为我们还没有计算任何东西，所以它还不存在：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We see there is no current gradient. Let’s try computing *f*(*x*), though,
    and see if anything changes now that we set `requires_grad=True`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到当前没有梯度。不过，让我们尝试计算 *f*(*x*)，看看现在我们设置了 `requires_grad=True` 后是否有什么变化：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now when we print the value of the returned variable, we get slightly different
    output. In the first part, the value 30.25 is printed, which is the correct value
    of *f*(−3.5). But we also see this new `grad_fn=<PowBackward0>`. Once we tell
    PyTorch to start calculating gradients, it begins to keep track of *every* computation
    we do. It uses this information to go backward and calculate the gradients for
    everything that was used and had a `requires_grad` flag set to `True`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们打印返回变量的值时，得到的结果略有不同。在第一部分，打印了值30.25，这是* f*(−3.5)的正确值。但我们还看到了这个新的`grad_fn=<PowBackward0>`。一旦我们告诉PyTorch开始计算梯度，它就开始跟踪我们做的*每一个*计算。它使用这些信息来反向计算所有使用过并且设置了`requires_grad`标志为`True`的梯度的梯度。
- en: 'Once we have a single *scalar* value, we can tell PyTorch to go back and use
    this information to compute the gradients. This is done using the `.backward()`
    function, after which we see a gradient in our original object:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到一个单一的*标量*值，我们可以告诉PyTorch回过头来使用这些信息来计算梯度。这是通过`.backward()`函数完成的，之后我们在原始对象中看到梯度：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With that, we have now computed a gradient for the variable `x`. Part of the
    power of PyTorch and automatic differentiation is that you can make the function
    `f(x)` do almost anything, as long as it is implemented using PyTorch functions.
    The code we wrote for computing the gradient of `x` will not change. PyTorch handles
    all the details of how to compute it for us.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们现在已经计算了变量`x`的梯度。PyTorch和自动微分的力量之一是，只要使用PyTorch函数实现，你就可以让函数`f(x)`做几乎所有的事情。我们为计算`x`的梯度编写的代码不会改变。PyTorch会为我们处理所有计算的细节。
- en: '1.3.3  Putting it together: Minimizing a function with derivatives'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3  将它们组合起来：使用导数最小化函数
- en: Now that PyTorch can compute gradients for us, we can use the automatic differentiation
    of our PyTorch function *f*(*x*) to numerically find the answer *f*(2) = 0. We
    are going to describe it first using mathematical notation and then in code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在PyTorch可以为我们计算梯度，我们可以使用PyTorch函数*f*(*x*)的自动微分来数值地找到答案*f*(2) = 0。我们首先用数学符号描述它，然后用代码描述。
- en: We start with our current guess, *x*[cur] = − 3.5. I chose 3.5 arbitrarily;
    in real life, you would usually pick a random value. We also keep track of our
    previous guess using *x*[prev]. Since we have not done anything yet, it is fine
    to set the previous step to any large value (e.g., *x*[prev] = *x*[cur] * 100).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从当前的猜测开始，*x*[cur] = − 3.5。我任意选择了3.5；在现实生活中，你通常会随机选择一个值。我们还使用*x*[prev]跟踪我们的前一个猜测。由于我们还没有做任何事情，将前一步设置为任何大值（例如，*x*[prev]
    = *x*[cur] * 100）是可以的。
- en: Next, we compare whether our current and previous guesses are similar. We do
    this by checking whether ∥*x*[cur] − *x*[prev]∥[2] > *ϵ*. The function ∥*z*∥[2]
    is called the *norm* or *2-norm*. Norms are the most common and standard ways
    of measuring *magnitude* for vectors and matrices. For one-dimensional cases (like
    this one), the 2-norm is the same as the absolute value. If we do not explicitly
    state what kind of norm we are talking about, you should always assume the 2-norm.
    The value ϵ is a common mathematical notation to refer to an arbitrary small value.
    So the way to read this is
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较当前的猜测和之前的猜测是否相似。我们通过检查∥*x*[cur] − *x*[prev]∥[2] > *ϵ*来做这件事。函数∥*z*∥[2]被称为*范数*或*2-范数*。范数是测量向量矩阵*幅度*最常见和标准的方式。对于一维情况（就像这个一样），2-范数与绝对值相同。如果我们没有明确说明我们谈论的是哪种范数，你应该始终假设是2-范数。值ϵ是表示任意小值的常见数学符号。所以，读法是这样的
- en: '![](../Images/ch1-eqs-to-illustrator2x.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch1-eqs-to-illustrator2x.png)'
- en: Now we know that ∥*x*[cur] − *x*[prev]∥[2] > *ϵ* is how we check whether there
    are large ( > *ϵ*) magnitude (∥ ⋅ ∥[2]) changes (*x*[cur] − *x*[prev]) between
    our guesses. If this is false, ∥*x*[cur] − *x*[prev]∥[2] ≤ *ϵ*, which means the
    change was small and we can stop. Once we stop, we accept *x*[cur] as our answer
    to the value of x that minimized *f*(*x*) If not, we need a new, *better* guess.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，∥*x*[cur] − *x*[prev]∥[2] > *ϵ* 是我们检查我们的猜测之间是否存在大的（> *ϵ*）幅度（∥ ⋅ ∥[2]）变化（*x*[cur]
    − *x*[prev]）的方法。如果这是假的，∥*x*[cur] − *x*[prev]∥[2] ≤ *ϵ*，这意味着变化很小，我们可以停止。一旦我们停止，我们就接受*x*[cur]作为x的值，它是使*f*(*x*)最小化的值。如果不是，我们需要一个新的、*更好的*猜测。
- en: 'To get this new guess, we move in the *opposite* direction of the derivative.
    This looks like this: *x*[cur] = *x*[cur] − *η* ⋅ *f*′(*x*[cur]). The value η
    is called the *learning rate* and is usually a small value like *η* = 0.1 or *η*
    = 0.01. We do this because the gradient *f*′(*x*) tells us which way to move but
    only gives us a *relative* answer about how far away we are. It doesn’t tell us
    exactly how far we should travel in that direction. Since we don’t know how far
    to travel, we want to be conservative and go a little slower. Figure 1.7 shows
    why.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到这个新猜测，我们向导数的相反方向移动。这看起来是这样的：*x*[cur] = *x*[cur] − *η* ⋅ *f*′(*x*[cur])。值
    η 被称为 *学习率*，通常是一个很小的值，如 *η* = 0.1 或 *η* = 0.01。我们这样做是因为梯度 *f*′(*x*) 告诉我们移动的方向，但只提供了一个
    *相对* 答案，关于我们有多远。它没有告诉我们在这个方向上应该走多远。由于我们不知道要走多远，我们希望保守一些，走得慢一些。图 1.7 展示了原因。
- en: '![](../Images/CH01_F07_Raff.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F07_Raff.png)'
- en: Figure 1.7 Three examples of how the learning rate η (also called step size)
    impacts learning. On the left, η is smaller than necessary. This still reaches
    the minimum but takes more steps than needed. If we knew the perfect value of
    η, we could set it just right, to take the smallest number of steps to the minimum
    value (middle). On the right, η is too big, which causes divergence. We never
    reach the solution!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 展示了学习率 η（也称为步长）对学习的影响的三个例子。在左侧，η 小于必要的值。这仍然达到了最小值，但比所需的步数多。如果我们知道 η 的完美值，我们可以将其设置得恰到好处，以采取最小的步数达到最小值（中间）。在右侧，η
    太大，导致发散。我们永远无法达到解！
- en: By taking smaller steps in the current direction, we don’t “drive past” the
    answer and have to turn around. Look at the previous example of our function to
    understand how that happens. If we have the *exactly* correct best value of η
    (middle image), we can take one step to the minimum. But we do not know what that
    value is. If we are instead conservative and choose a value that is likely smaller
    than we need, we may take more steps to get to the answer, but we eventually get
    there (left image). If we set our learning rate too high, we can end up shooting
    past the solution and bouncing around it (right image).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在当前方向上采取更小的步长，我们不会“驶过”答案并需要掉头。看看我们函数的先前例子，了解这是如何发生的。如果我们有 *完全正确* 的最佳 η 值（中间图像），我们可以一步到达最小值。但我们不知道这个值是多少。如果我们保守地选择一个可能小于我们需要的值，我们可能需要更多步数才能到达答案，但最终我们会到达那里（左侧图像）。如果我们设置学习率过高，我们可能会错过解并围绕它弹跳（右侧图像）。
- en: 'That might sound like a lot of scary math, but hopefully you will feel better
    about it when you look at the code that does the work. It is only a few lines
    long. At the end of the loop, we print the value of *x*[cur] and see that it is
    equal to 2.0; PyTorch found the answer. Notice that when we define a PyTorch `Tensor`
    object, it has a child member `.grad` that stores the computed gradients for that
    variable, as well as a `.data` member that holds the underlying value. You usually
    shouldn’t access either of these fields unless you have a specific reason to;
    for now, we are using them to demonstrate the mechanics of autograd:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来像很多令人害怕的数学，但当你看到执行这项工作的代码时，你可能会感觉好一些。它只有几行长。在循环结束时，我们打印 *x*[cur] 的值，并看到它等于
    2.0；PyTorch 找到了答案。注意，当我们定义 PyTorch `Tensor` 对象时，它有一个子成员 `.grad`，用于存储该变量的计算梯度，以及一个
    `.data` 成员，用于存储底层值。你通常不应该访问这些字段，除非你有特定的原因；现在，我们正在使用它们来演示 autograd 的机制：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Makes the initial “previous" solution larger so it’s different and the while
    loop will start
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将初始的“前一个”解设置得更大，以便它与当前解不同，while 循环将开始
- en: ❷ Threshold for the current and previous value to be close enough that we stop
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当前值和前一个值足够接近，以至于我们停止
- en: ❸ Learning rate
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 学习率
- en: ❹ Makes a clone so x_prev and x_cur don’t point to the same object
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建一个克隆，以便 x_prev 和 x_cur 不指向同一个对象
- en: ❺ These next few lines compute the function, gradient, and update. We want autograd
    to work, so we to access the .data member field.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 接下来的几行计算函数、梯度和更新。我们希望 autograd 能够工作，因此我们需要访问 .data 成员字段。
- en: ❻ Zeros out the old gradient, as PyTorch will not do that for us
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将旧梯度置零，因为 PyTorch 不会为我们做这件事
- en: ❼ Accesses .data to avoid autograd mechanics. We want to change the value without
    any side effects.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 访问 .data 以避免 autograd 机制。我们希望在不产生任何副作用的情况下更改值。
- en: What is this backpropagation I keep hearing about?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直听说的反向传播是什么？
- en: Many books begin their discussion of deep learning with an algorithm called
    *backpropagation*. This is the name of the original algorithm used to compute
    all the gradients in a neural network. Personally, I think backpropagation is
    a very intimidating place to start, as it involves a lot more math and drawing
    graphs, but it is fully encapsulated by automatic differentiation. With modern
    frameworks like PyTorch, you don’t need to know about the mechanics of backpropagation
    to get started. If you want to understand backpropagation and how it works with
    automatic differentiation, I like the approach in chapter 6 of Andrew W. Trask’s
    book, *Grokking Deep Learning* (Manning, 2019).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 许多书籍都是从一个名为 *反向传播* 的算法开始讨论深度学习的。这是用于计算神经网络中所有梯度的原始算法的名称。我个人认为反向传播是一个非常令人畏惧的起点，因为它涉及更多的数学和绘图，但它完全被自动微分所封装。使用现代框架如
    PyTorch，你不需要了解反向传播的机制就可以开始。如果你想了解反向传播以及它是如何与自动微分一起工作的，我喜欢 Andrew W. Trask 的书 *Grokking
    Deep Learning*（Manning，2019）第 6 章中的方法。
- en: 1.4 Optimizing parameters
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 优化参数
- en: What we just did, finding the minimum of a function *f*(⋅), is called *optimization*.
    Because we specify the goal of our network using a loss function ℓ(⋅), we can
    optimize *f*(⋅) to minimize our loss. If we reach a loss ℓ(⋅) = 0, our network
    appears to have solved the problem. This is why we care about optimization and
    is foundational to how most modern neural networks are trained today. Figure 1.8
    shows a simplification of how it works.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才所做的是找到函数 *f*(⋅) 的最小值，这被称为 *优化*。因为我们使用损失函数 ℓ(⋅) 来指定我们网络的目标，所以我们可以优化 *f*(⋅)
    以最小化我们的损失。如果我们达到损失 ℓ(⋅) = 0，我们的网络似乎已经解决了问题。这就是我们为什么关心优化，并且它是大多数现代神经网络训练的基础。图 1.8
    展示了其工作原理的简化。
- en: '![](../Images/CH01_F08_Raff.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F08_Raff.png)'
- en: Figure 1.8 How neural networks use the loss ℓ(⋅) and optimization process. The
    neural network is controlled by its parameters θ. To make useful predictions about
    the data, we need to alter the parameters. We do so by first computing the loss
    ℓ(⋅), which tells us how badly the network is doing. Since we want to minimize
    the loss, we can use the gradient to alter the parameters! This gets the network
    to make useful predictions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 神经网络如何使用损失 ℓ(⋅) 和优化过程。神经网络由其参数 θ 控制。为了对数据进行有用的预测，我们需要改变参数。我们通过首先计算损失 ℓ(⋅)，它告诉我们网络做得有多糟糕。由于我们想要最小化损失，我们可以使用梯度来改变参数！这使网络能够做出有用的预测。
- en: 'Because of how important optimization is, PyTorch includes two additional concepts
    to help us: parameters and optimizers. A `Parameter` of a model is a value that
    we alter using an `Optimizer` to try to reduce our loss ℓ(⋅). We can easily convert
    any tensor into a `Parameter` using the `nn.Parameter` class. To do that, let’s
    re-solve the previous problem of minimizing *f*(*x*) = (*x*−2)² with an initial
    guess of *x*[cur] = 3.5. The first thing we do is create a `Parameter` object
    for the value of x, since that is what we are going to alter:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于优化的重要性，PyTorch 包含了两个额外的概念来帮助我们：参数和优化器。模型的 `Parameter` 是一个值，我们使用 `Optimizer`
    来改变它，以尝试减少我们的损失 ℓ(⋅)。我们可以使用 `nn.Parameter` 类轻松地将任何张量转换为 `Parameter`。为此，让我们重新解决之前的最小化
    *f*(*x*) = (*x*−2)² 的问题，初始猜测为 *x*[cur] = 3.5。我们首先要做的是为 x 的值创建一个 `Parameter` 对象，因为这是我们打算改变的：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The object `x_param` is now a `nn.Parameter`, which behaves the same way tensors
    do. We can use a `Parameter` anywhere we would use a tensor in PyTorch, and the
    code will work fine. But now we can create an `Optimizer` object. The simplest
    optimizer we use is called `SGD`, which stands for *stochastic gradient descent*.
    The word *gradient* is there because we are using the gradients/derivatives of
    functions. *Descent* means we are minimizing or *descending* to a lower value
    of the function that we are minimizing. We get to the *stochastic* part in the
    next chapter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_param` 对象现在是 `nn.Parameter`，其行为与张量相同。我们可以在 PyTorch 中使用 `Parameter` 的任何地方使用张量，代码将正常工作。但现在我们可以创建一个
    `Optimizer` 对象。我们使用的最简单的优化器称为 `SGD`，代表 *随机梯度下降*。单词 *gradient* 在那里是因为我们正在使用函数的梯度/导数。*Descent*
    意味着我们正在最小化或 *下降* 到我们正在最小化的函数的更低值。我们将在下一章中了解到 *stochastic* 部分。'
- en: 'To use SGD, we need to create the associated object with a `list` of `Parameter`s
    that we want to adjust. We can also specify the learning rate η or accept the
    default. The following code specifies η to match the original code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 SGD，我们需要创建一个与 `Parameter`s 的 `list` 相关的关联对象，这些 `Parameter`s 是我们想要调整的。我们还可以指定学习率
    η 或接受默认值。以下代码指定 η 以匹配原始代码：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can rewrite the previous ugly loop into something cleaner that looks
    much closer to how we train neural networks in practice. We will loop over the
    optimization problem a fixed number of times, which we often call *epochs*. The
    `zero_grad` method does the cleanup we did manually before for every parameter
    passed in as an input. We compute our loss, call `.backward()` on that loss, and
    then ask the optimizer to perform one `.step()` of the optimization:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将之前的丑陋循环重写成一个更干净、看起来更接近我们实际训练神经网络的代码。我们将固定次数地遍历优化问题，这通常被称为*epochs*。`zero_grad`方法为我们之前手动为每个输入参数所做的清理工作。我们计算损失，对那个损失调用`.backward()`，然后要求优化器执行一次`.step()`的优化：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ x.grad.zero_()
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x.grad.zero_()
- en: ❷ x.data -= eta * x.grad
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x.data -= eta * x.grad
- en: The code prints out `tensor(2.0000)`, just like before. This will make our lives
    easier when we have literally *millions* of parameters in our network.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 代码打印出`tensor(2.0000)`，就像之前一样。这将在我们的网络中实际上有*数百万*个参数时使我们的生活变得更简单。
- en: 'You’ll notice a significant change in this code: we are not optimizing until
    we hit a gradient of zero or the difference between the previous and current solutions
    is very small. Instead, we are doing something dumber: a fixed number of steps.
    In deep learning, we rarely get to a loss of zero, and we would have to wait way
    too long for that to happen. So most people pick a fixed number of epochs that
    they are willing to wait for and then see what the results look like at the end.
    This way, we get an answer faster, and it’s usually good enough to use.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到代码中的一个显著变化：我们不是在达到零梯度或前一次和当前解之间的差异非常小之前进行优化。相反，我们正在做一件更简单的事情：固定次数的步骤。在深度学习中，我们很少能达到损失为零，而且我们不得不等待很长时间才能发生这种情况。所以大多数人会选择一个他们愿意等待的固定epochs数，然后看看最终的结果是什么。这样，我们就能更快地得到答案，而且通常足够好用来使用。
- en: Why PyTorch?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择PyTorch？
- en: There are many deep learning frameworks, including TensorFlow and Keras, MXNet,
    others like fast.ai built on top of PyTorch, and newer ones like JAX. My opinion
    is that PyTorch sits at a better balance between “make things easy” and “make
    things accessible” than most of the other tools. The NumPy-like function calls
    make development fairly easy and, more importantly, easier to debug. While PyTorch
    has nice abstractions like the `Optimizer`, we have just seen how painless it
    is to switch between different levels of abstraction. This is another nice feature
    that makes debugging easier when you hit a strange bugs or want to try an exotic
    idea. PyTorch is also flexible for use outside classical deep learning tasks.
    The other platforms have their own strengths and weaknesses, but these are the
    reasons I’ve chosen PyTorch for this book.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架有很多，包括TensorFlow和Keras，MXNet，以及其他基于PyTorch构建的如fast.ai，以及一些新的如JAX。我的观点是，PyTorch在“让事情变得简单”和“让事情变得可访问”之间取得了比大多数其他工具更好的平衡。NumPy-like函数调用使得开发变得相对容易，更重要的是，更容易调试。虽然PyTorch有像`Optimizer`这样的良好抽象，但我们刚刚看到在不同的抽象级别之间切换是多么的无痛。这是另一个使调试更容易的不错特性，当你遇到奇怪的bug或想尝试一个异国情调的想法时。PyTorch在经典深度学习任务之外的使用也很灵活。其他平台也有自己的优点和缺点，但这些都是我选择PyTorch编写这本书的原因。
- en: 1.5 Loading dataset objects
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 加载数据集对象
- en: 'We have learned a little about the basic PyTorch tools. Now we want to start
    training a neural network. But first we need some data. Using the common notation
    of ML, we need a set of input data X and associated output labels y. In PyTorch,
    we represent that with a `Dataset` object. By using this interface, PyTorch provides
    efficient loaders that automatically handle using multiple CPU cores to pre-fetch
    the data and keep a limited amount of data in memory at a time. Let’s start by
    loading a familiar dataset from scikit-learn: MNIST. We convert it from a NumPy
    array to the form PyTorch likes.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对PyTorch的基本工具了解了一些。现在我们想要开始训练一个神经网络。但首先我们需要一些数据。使用ML的常见符号，我们需要一组输入数据X和相关的输出标签y。在PyTorch中，我们用`Dataset`对象来表示这一点。通过使用这个接口，PyTorch提供了高效的加载器，它可以自动处理使用多个CPU核心来预取数据，并在任何时候保持有限的数据量在内存中。让我们先从加载一个熟悉的scikit-learn数据集开始：MNIST。我们将它从NumPy数组转换为PyTorch喜欢的形式。
- en: 'PyTorch uses a `Dataset` class to represent a dataset, and it encodes the information
    about how many items are in the dataset and how to get the *n*th item in the dataset.
    Let’s see what that looks like:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch使用`Dataset`类来表示数据集，并编码了数据集中有多少项以及如何获取数据集中的第*n*项的信息。让我们看看它是什么样子：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Loads data from https://www.openml.org/d/554
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从https://www.openml.org/d/554加载数据
- en: 'We have loaded the classic MNIST dataset with a total of 70,000 rows and 784
    features. Now we will create a simple `Dataset` class that takes in `X, y` as
    input. We need to define a `__getitem__` method, which will return the data and
    label as a `tuple(inputs, outputs)`. The `inputs` are the objects we want to give
    to our model as inputs, and the `outputs` are used for the output. We also need
    to implement the `__len__` function that returns how large the dataset is:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经加载了包含 70,000 行和 784 个特征的经典 MNIST 数据集。现在我们将创建一个简单的 `Dataset` 类，它接受 `X, y`
    作为输入。我们需要定义一个 `__getitem__` 方法，它将返回数据标签作为一个 `tuple(inputs, outputs)`。`inputs`
    是我们想要给模型作为输入的对象，而 `outputs` 用于输出。我们还需要实现 `__len__` 函数，它返回数据集的大小：
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ This “work" could have gone in the constructor, but you should get into the
    habit of placing it in getitem.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这“工作”原本可以放在构造函数中，但你应该养成将其放在 getitem 的习惯。
- en: ❷ Makes a PyTorch dataset
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建 PyTorch 数据集
- en: Notice that we do the minimal amount of work in the constructor and instead
    move it into the `__getitem__` function. This is intentional design and a habit
    you should emulate when doing deep learning work. In many cases, we need to do
    non-trivial preprocessing, preparation, and conversions to get our data into a
    form that a neural network can learn from. If you put those tasks into the `__getitem__`
    function, you get the benefit of PyTorch doing the work on an as-needed basis
    while you wait for your GPU to finish processing some other batch of data, making
    your overall process more compute efficient. This becomes really important when
    you work on larger datasets where preprocessing would cause a long delay up front
    or require extra memory, and doing the prep only when needed can save you a lot
    of storage.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在构造函数中只做了最小量的工作，而不是将其移动到 `__getitem__` 函数中。这是一个有意的设计，并且是一个你应该在深度学习工作中效仿的习惯。在许多情况下，我们需要进行非平凡的预处理、准备和转换，以便将数据转换成神经网络可以学习的形式。如果你将这些任务放入
    `__getitem__` 函数中，你将获得 PyTorch 在需要时执行工作的好处，同时你等待 GPU 完成处理其他批次数据，使你的整体过程更加计算高效。当你处理大型数据集时，这变得尤为重要，因为预处理可能会造成初始阶段的长时间延迟或需要额外的内存，而仅在需要时进行准备可以节省你大量的存储空间。
- en: Note You may wonder why we use `int64` as the tensor type for the targets. Why
    not `int32` or even `int8`, if we know our labels are in a smaller range, or `uint32`
    if no negative values will occur? The unsatisfying answer is that for any situation
    where `int` types are required, PyTorch is hardcoded to work only with `int64`,
    so you just have to use it. Similarly, when floating-point values are needed,
    most of PyTorch will only work with `float32`, so you have to use `float32` instead
    of `float64` or other types. There are exceptions, but they are not worth getting
    into while learning the fundamentals.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你可能想知道为什么我们使用 `int64` 作为目标张量的类型。为什么不使用 `int32` 或甚至 `int8`，如果我们知道我们的标签在一个更小的范围内，或者
    `uint32` 如果不会出现负值？令人不满意的答案是，对于任何需要 `int` 类型的场景，PyTorch 都是硬编码为仅与 `int64` 一起工作，所以你只能使用它。同样，当需要浮点值时，PyTorch
    的大部分功能将仅与 `float32` 一起工作，所以你必须使用 `float32` 而不是 `float64` 或其他类型。虽然有一些例外，但在学习基础知识时，它们不值得深入研究。
- en: 'Now we have a simple dataset object. It keeps the entire dataset in memory,
    which is OK for small datasets, but we want to fix it in the future. We can confirm
    that the dataset still has 70,000 examples, and each example has 784 features,
    as before, and quickly confirm that the length and index functions we implemented
    work as expected:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个简单的数据集对象。它将整个数据集保存在内存中，这对于小型数据集来说是可行的，但我们希望在将来修复它。我们可以确认数据集仍然有 70,000
    个示例，每个示例有 784 个特征，就像之前一样，并且可以快速确认我们实现的长度和索引函数按预期工作：
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Returns 784
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回 784
- en: 'MNIST is a dataset of hand-drawn numbers. We can visualize this by reshaping
    the data back into an image to confirm that our data loader is working:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 是一个手绘数字的数据集。我们可以通过将数据重塑回图像来可视化它，以确认我们的数据加载器正在正常工作：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/CH01_UN03_Raff.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_UN03_Raff.png)'
- en: 1.5.1  Creating a training and testing split
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 创建训练和测试分割
- en: Now we have all of our data in one dataset. However, like good ML practitioners,
    we should create a training split and a testing split. In some cases, we have
    a dedicated training and testing dataset. If that is the case, you should create
    two separate `Dataset` objects—one for training and one for testing—from the respective
    data sources.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有数据放在了一个数据集中。然而，像优秀的机器学习从业者一样，我们应该创建一个训练分割和一个测试分割。在某些情况下，我们有一个专门用于训练和测试的数据集。如果是这种情况，你应该从相应的数据源创建两个单独的
    `Dataset` 对象——一个用于训练，一个用于测试。
- en: 'In this case, we have one dataset. PyTorch has a simple utility to break the
    corpus into train and test splits. Let’s say we want 20% of the data to be used
    for testing. We can do that as follows using the `random_split` method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个数据集。PyTorch 有一个简单的实用工具可以将语料库拆分为训练集和测试集。假设我们想用 20% 的数据用于测试。我们可以使用
    `random_split` 方法这样做：
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we have train and test sets. In reality, the first 60,000 points are the
    standard training set for MNIST, and the last 10,000 are the standard test set.
    But the point was to show you the function for creating randomized splits yourself.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练集和测试集。实际上，前 60,000 个点是 MNIST 的标准训练集，最后 10,000 个点是标准测试集。但重点是展示如何自己创建随机分割的功能。
- en: 'With that, we have learned about all of the foundational tools that PyTorch
    provides:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经了解了 PyTorch 提供的所有基础工具：
- en: A NumPy-like tensor API, which supports GPU acceleration
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个类似于 NumPy 的张量 API，支持 GPU 加速
- en: Automatic differentiation, which lets us solve optimization problems
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动微分，它使我们能够解决优化问题
- en: An abstraction for datasets
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的抽象
- en: We will build on this foundation, and you may notice that it starts to impact
    how you think about neural networks in the future. They do not magically do what
    is asked, but they try to numerically solve a goal specified by a loss function
    ℓ(⋅). We need to be careful in how we define or choose ℓ(⋅) because that will
    determine what the algorithm learns.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此基础上构建，你可能会注意到这开始影响你未来对神经网络的思考。它们不会神奇地完成所要求的事情，而是尝试通过损失函数 ℓ(⋅) 指定的目标进行数值求解。我们需要小心地定义或选择
    ℓ(⋅)，因为这将决定算法学习的内容。
- en: Exercises
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Manning 在线平台 Inside Deep Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))
    上分享和讨论你的解决方案。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: Write a series of `for` loops that compute the average value in `torch_tensor3d`.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一系列 `for` 循环，计算 `torch_tensor3d` 中的平均值。
- en: Write code that indexes into `torch_tensor3d` and prints out the value 13.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，索引到 `torch_tensor3d` 并打印出值 13。
- en: For every power of 2 (i.e., 2^i or `2**i` ) up to 2^(11), create a random matrix
    *X* ∈ ℝ^(2^i, 2^i) (i.e., `X.shape` should give `(2**i, 2**i)`). Time how long
    it takes to compute XX (i.e., `X @ X`) on a CPU and on a GPU, and plot the speedup.
    For what matrix sizes is the CPU faster than the GPU?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 2 的幂（即，2^i 或 `2**i`）直到 2^(11)，创建一个随机矩阵 *X* ∈ ℝ^(2^i, 2^i)（即，`X.shape` 应该给出
    `(2**i, 2**i)`）。计算 XX（即，`X @ X`）在 CPU 和 GPU 上的时间，并绘制加速图。对于什么矩阵大小 CPU 比GPU快？
- en: We used PyTorch to find the numeric solution to *f*(*x*) = (*x*−2)². Write code
    that finds the solution to *f*(*x*) = sin(*x* − 2) · (*x* + 2)² + √|cos(*x*)|.
    What answer doyou get?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 PyTorch 找到 *f*(*x*) = (*x*−2)² 的数值解。编写代码找到 *f*(*x*) = sin(*x* − 2) · (*x*
    + 2)² + √|cos(*x*)| 的解。你得到什么答案？
- en: Write a new function that takes two inputs, x and y, where
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个新的函数，该函数接收两个输入，x 和 y，其中
- en: '*f*(*x*,*y*) = exp (sin(*x*)²)/(*x*−*y*)² + (*x*−*y*)²'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*f*(*x*,*y*) = exp (sin(*x*)²)/(*x*−*y*)² + (*x*−*y*)²'
- en: Use an `Optimizer` with initial parameter values of *x* = 0.2 and *y* = 10.
    What do they converge to?
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用初始参数值 *x* = 0.2 和 *y* = 10 的 `Optimizer`。它们会收敛到什么？
- en: Create a function `libsvm2Dataset` that takes a path to a libsvm dataset file
    (see [https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)
    for many that you can download) and create a new dataset object. Check that it
    is the correct length and that each row has the expected number of features.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `libsvm2Dataset` 的函数，该函数接收一个指向 libsvm 数据集文件的路径（参见 [https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)
    以下载更多数据集），并创建一个新的数据集对象。检查其长度是否正确，以及每一行是否具有预期的特征数量。
- en: '**Challenging**: Use NumPy’s `memmap` functionality to write the MNIST dataset
    to disk. Then create a `MemmapedSimpleDataset` that takes the mem-mapped file
    as input, reading the matrix from disk to create PyTorch tensors in the `__getitem__`
    method. Why do you think this would be useful?'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挑战性**：使用 NumPy 的 `memmap` 功能将 MNIST 数据集写入磁盘。然后创建一个 `MemmapedSimpleDataset`，它以
    mem-mapped 文件作为输入，在 `__getitem__` 方法中从磁盘读取矩阵以创建 PyTorch 张量。你认为这会有什么用？'
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PyTorch represents almost everything using tensors, which are multidimensional
    arrays.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch几乎使用张量来表示一切，张量是多维数组。
- en: Using a GPU, PyTorch can accelerate any operation done using tensors.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPU，PyTorch可以加速使用张量进行的任何操作。
- en: PyTorch tracks what we do with tensors to perform automatic differentiation,
    which means it can calculate gradients.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch跟踪我们对张量的操作以执行自动微分，这意味着它可以计算梯度。
- en: We can use gradients to minimize a function; the values altered by a gradient
    are parameters.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用梯度来最小化一个函数；由梯度改变值的参数。
- en: We use a loss function to quantify how well a network is doing at a task and
    gradients to minimize that loss, which results in learning the parameters of the
    network.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用损失函数来量化网络在任务上的表现，并使用梯度来最小化该损失，从而学习网络的参数。
- en: PyTorch provides a `Dataset` abstraction so that we can let PyTorch deal with
    some nuisance tasks and minimize memory usage.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch提供了一个`Dataset`抽象，这样我们就可以让PyTorch处理一些繁琐的任务并最小化内存使用。
