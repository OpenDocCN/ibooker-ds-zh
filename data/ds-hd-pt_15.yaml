- en: Chapter 13\. Storytelling in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章\. 机器学习中的讲故事
- en: In [Chapter 7](ch07.html#ch07_narratives), I argued that data scientists ought
    to become better storytellers. This holds true in general, but it takes on special
    importance with regard to machine learning (ML).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](ch07.html#ch07_narratives) 中，我主张数据科学家们应该成为更好的讲故事者。这在一般情况下是正确的，但在机器学习领域尤为重要。
- en: This chapter walks you through the main aspects of storytelling in ML, starting
    with feature engineering and finishing with the problem of interpretability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带你深入了解机器学习中讲故事的主要方面，从特征工程开始，到可解释性问题结束。
- en: A Holistic View of Storytelling in ML
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中讲故事的整体视角
- en: Storytelling plays two related but distinct roles in ML ([Figure 13-1](#ch13_stt_ml_flow)).
    The better-known role is a salesperson, where you need to engage with an audience,
    possibly to gain or maintain stakeholder buy-in, a process that usually takes
    place after you’ve developed a model. The lesser-known role is a scientist, where
    you need to find hypotheses that will guide you throughout the process of developing
    the model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 讲故事在机器学习中扮演着两个相关但不同的角色（[图 13-1](#ch13_stt_ml_flow)）。较为熟知的是销售人员的角色，在这个角色中，你需要与观众互动，可能是为了获得或维护利益相关者的支持，这个过程通常是在你开发模型之后进行的。较少人知的是科学家的角色，在这个角色中，你需要找到假设，这些假设将指导你在开发模型过程中的全程。
- en: '![storytelling flow](assets/dshp_1301.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![讲故事流程](assets/dshp_1301.png)'
- en: Figure 13-1\. Storytelling in ML
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 机器学习中的讲故事
- en: Since the former takes place *after* you have developed your model, I call it
    *ex post* storytelling; your scientist persona is mostly invoked before (*ex ante*)
    and during (*interim*) the process of training the model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为前者是在你开发模型之后进行的 *ex post* 讲故事，我称之为销售人员角色；而科学家角色主要是在训练模型过程中的 *ex ante* 和 *interim*
    阶段。
- en: Ex Ante and Interim Storytelling
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*Ex Ante* 和 *Interim* 讲故事'
- en: 'Ex ante storytelling has four main steps: defining the problem, creating hypotheses,
    feature engineering, and training the model ([Figure 13-2](#ch13_exante_st)).
    While they usually flow in that direction, there’s a feedback loop between all
    of them, so it’s not uncommon that after you train a first model, you iterate
    on the features, hypotheses, or even on the problem itself.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ex ante* 讲故事有四个主要步骤：定义问题、创建假设、特征工程和训练模型（[图 13-2](#ch13_exante_st)）。虽然它们通常按照这个顺序进行，但在它们之间存在反馈循环，因此在训练第一个模型后，经常需要对特征、假设甚至问题本身进行迭代。'
- en: '![storytelling flow](assets/dshp_1302.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![讲故事流程](assets/dshp_1302.png)'
- en: Figure 13-2\. Ex ante storytelling
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. *Ex ante* 讲故事
- en: 'The first step is always the problem definition: *what* do you want to predict
    and *why*? This is better done early and collaboratively with your stakeholders
    to ensure you have their buy-in, as many promising ML projects fail because of
    this.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步始终是问题定义：*你想预测什么*，*为什么*？最好早期与利益相关者合作完成这个过程，确保他们支持，因为很多有前景的机器学习项目因此而失败。
- en: 'Recall from [Chapter 12](ch12.html#ch12_productionML) that a model is only
    good if it has been deployed in production. Deploying to production is a costly
    endeavor, not only in terms of time and effort, but also in terms of any alternative
    project you could’ve been working on (opportunity cost). Because of this, it’s
    always good to ask yourself: *do I really need an ML implementation for this project*?
    Don’t fall into the trap of doing ML just because it’s sexy or fun: your objective
    should always be to create the maximum value, and ML is just one more tool in
    the bag.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想[第 12 章](ch12.html#ch12_productionML) 中提到的，只有在模型部署到生产环境中时，它才能算是一个好模型。将模型部署到生产环境是一项昂贵的工作，不仅在时间和精力上，还有机会成本上。因此，时常自问：*我真的需要为这个项目实施机器学习吗*？不要陷入只因为它很吸引人或者有趣而做机器学习的陷阱中：你的目标始终是创造最大的价值，而机器学习只是你工具箱中的一种工具。
- en: 'Finally, in the problem definition, don’t forget to have good answers to the
    questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在问题定义阶段，不要忘记对以下问题有清晰的答案：
- en: How is this model going to be used?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型将如何被使用？
- en: What are the levers that can be pulled using predictions from the model?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从模型预测中可以拉动的杠杆是什么？
- en: How does it improve your company’s decision-making capabilities?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它如何提升公司的决策能力？
- en: Having sound answers to these questions will help the business case for developing
    an ML model, thereby increasing the likelihood of success.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些问题有明确的答案将有助于开发机器学习模型的商业案例，从而增加成功的可能性。
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'As a general recommendation, the sooner you involve your stakeholders in the
    definition of the problem, the better. This helps with having stakeholder buy-in
    from the outset. Also ensure that ML is the appropriate tool for the problem at
    hand: deploying, monitoring, and maintaining a model are costly, so you should
    have a good business case for it.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般建议，尽早让利益相关者参与问题的定义是有益的。这有助于从一开始就获得利益相关者的支持。还要确保ML是解决手头问题的合适工具：部署、监控和维护模型成本高昂，因此您应该有一个良好的业务案例支持它。
- en: Creating Hypotheses
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建假设
- en: With a well-defined problem, you can now switch into your scientist persona
    and start creating hypotheses for the problem at hand. Each of these hypotheses
    is a story about the drivers for your prediction; it’s in this specific sense
    that scientists are also storytellers. Successful stories improve the predictive
    performance of your model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个明确定义的问题，您现在可以切换到科学家的角色，并开始为手头的问题创建假设。每个假设都是关于您预测驱动因素的故事；正是在这种特定意义上，科学家也是讲故事者。成功的故事可以提高您模型的预测性能。
- en: 'At this point, the key questions are: *what am I predicting, and what drives
    this prediction*? [Figure 13-3](#ch13_lever_beh_loop) shows a high-level overview
    of the types of prediction problems and their relationship to the levers at your
    disposal. Understanding the levers is critical to ensure that an ML model creates
    value ([Chapter 1](ch01.html#ch1_sowhat)).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此时的关键问题是：*我在预测什么，以及什么驱动了这一预测*？[图13-3](#ch13_lever_beh_loop)展示了预测问题类型的高级概述，以及它们与您可以操作的杠杆的关系。了解这些杠杆对于确保ML模型创造价值至关重要（[第1章](ch01.html#ch1_sowhat)）。
- en: '![lever behavior loop](assets/dshp_1303.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![杠杆行为循环](assets/dshp_1303.png)'
- en: Figure 13-3\. Lever-behavior-metrics flow
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3\. 杠杆-行为-指标流程
- en: 'From here it follows that most prediction problems fall into one of these categories:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从这里可以得出结论，大多数预测问题可以归为以下几类：
- en: Metrics that arise from human behavior
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人类行为产生的指标
- en: Many times, the metric that you care about depends on your customers acting
    in some specific way. For instance, will my user click on a banner? Will they
    purchase the product at the reference price? Will they churn next month? How much
    time will they spend on the marketplace?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，您关心的指标取决于客户以某种特定方式行动。例如，我的用户会点击横幅吗？他们会以参考价格购买产品吗？他们会在下个月流失吗？他们会在市场上花费多少时间？
- en: Metrics that arise from systems behavior
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 系统行为产生的指标
- en: Metrics depend also on how your systems perform. One of the best well-known
    examples is data center optimization, and most specifically, cracking the [air
    cooling problem](https://oreil.ly/5guWh). Another is predicting the loading time
    for your web page, which [has been found](https://oreil.ly/xXtbS) to directly
    impact churn metrics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 指标还取决于系统的表现。其中最为人熟知的一个例子是数据中心优化，尤其是解决[空气冷却问题](https://oreil.ly/5guWh)。另一个例子是预测您的网页加载时间，已经发现直接影响流失指标。
- en: Downstream metrics
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下游指标
- en: Many times you just care about aggregate downstream metrics, such as revenue.
    This is most common with data scientists working directly in financial planning
    and analysis (FP&A).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，您只关心汇总的下游指标，比如收入。这在直接从事财务规划与分析（FP&A）的数据科学家中最为常见。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Many data scientists struggle with the process of creating and engineering features
    that are predictive. A general recommendation is to always start by writing down
    and discussing with others a list of hypotheses for the prediction problem. Only
    then should you move forward with the process of feature engineering. Don’t forget
    to write down the reasons you believe a hypothesis might be right. Only with this
    rationale will you be able to challenge your logic and improve upon a given story.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家在创建和设计有预测性的特征的过程中感到困难。一个通用的建议是始终首先写下并与他人讨论预测问题的假设列表。只有在此之后，您才应该继续进行特征工程的过程。不要忘记写下您认为假设可能正确的原因。只有有了这样的理由，您才能挑战自己的逻辑，并改进给定的故事。
- en: 'Some high-level recommendations to come up with hypotheses for your problem
    are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为您的问题提出假设，有一些高层次的建议：
- en: Know your problem really well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 很好地了解您的问题。
- en: The not-so-secret sauce to building great ML models is to have substantial domain
    expertise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 构建出色的ML模型的不太秘密的关键是具有深厚的领域专业知识。
- en: Be curious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要保持好奇心。
- en: This is one defining trait that makes a data scientist a scientist.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使数据科学家成为科学家的一个定义性特征。
- en: Challenge the status quo.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战现状。
- en: Don’t be afraid to challenge the status quo. This includes challenging your
    own hypotheses and iterating when needed (be aware of any signs of confirmation
    bias on your side).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不要害怕挑战现状。这包括挑战你自己的假设，并在需要时进行迭代（请注意你是否有任何确认偏见的迹象）。
- en: This said, let’s go into some more specific recommendations on how to proceed
    on your hypothesis discovery and formulation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们进一步具体推荐如何进行你的假设发现和制定。
- en: Predicting human behavior
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测人类行为
- en: For predicting human behavior, it’s useful to always remember that people do
    what they *want* and *can* do. You may want to go to Italy, but if you can’t afford
    it (money or timewise), you won’t do it. Tastes and resource availability are
    of first-order importance whenever you want to predict human behavior, and this
    can take you a long way toward coming up with hypotheses for your problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测人类行为，始终记住人们做他们*想做*和*能做*的事情。你可能想去意大利，但如果你负担不起（无论是金钱还是时间），你就不会去。口味和资源的可用性在你想预测人类行为时是至关重要的一级重要因素，这可以帮助你解决问题的假设。
- en: Thinking about motivations will also force you to think really hard about your
    product. For instance, why would anyone want to buy it? What is the value proposition?
    Which customers would be willing to pay for it?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 思考动机也会迫使你非常认真地思考你的产品。例如，为什么有人会想购买它？价值主张是什么？哪些顾客愿意为此付费？
- en: Another trick is to use your capacity to empathize with your customers; ask
    yourself what would *you* do if you were them? Of course, the easier it is to
    put yourself in their shoes, the better (for me it would be really hard to put
    myself in an influencer’s or professional boxer’s shoes). This trick can take
    you far, but bear in mind that you may not be your typical customer, which brings
    me to the next trick.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个技巧是利用你与顾客的共情能力；问问自己如果你是他们会做什么？当然，能够轻松置身于他们的角色中的情况越容易越好（对我来说，很难置身于意见领袖或职业拳击手的角色）。这个技巧可以带你走得更远，但请记住，你可能不是你典型的顾客，这就引出了下一个技巧。
- en: At least at the beginning, aim for understanding and modeling your *average*
    customer. You should first and foremost get first-order effects right, meaning
    that modeling the average unit of analysis will buy you quite a bit of predictive
    performance. I’ve seen many data scientists start hypothesizing about corner or
    edge cases which, by definition, will have a negligible impact on overall predictive
    performance. Corner cases are interesting and important, but for prediction, it’s
    almost always better to start with the average cases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，你至少要力求理解并模拟*普通*顾客。首先，你应该正确获取一阶效应，也就是说，对分析单元的平均建模将为你带来相当多的预测性能。我见过许多数据科学家开始假设边角案例，这些案例定义上对整体预测性能几乎没有影响。边角案例很有趣也很重要，但对于预测来说，从平均案例开始几乎总是更好的选择。
- en: Predicting system behavior
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测系统行为
- en: Some of the previous remarks also apply for predicting a system. The main difference
    is that since systems lack purpose or sentience, you can restrict yourself to
    understanding technical bottlenecks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些先前的评论也适用于预测一个系统。主要区别在于，由于系统缺乏目的或知觉，你可以限制自己理解技术瓶颈。
- en: Clearly, you have to master the technical details of your system, and the more
    knowledgeable you become about the physical constraints, the easier it will be
    to come up with hypotheses.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你必须掌握系统的技术细节，你对物理约束了解得越多，制定假设就会越容易。
- en: Predicting downstream metrics
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测下游指标
- en: Downstream metrics prediction is both harder and easier than predicting individual
    metrics that result from human or system behavior. It’s harder because the more
    distanced from the underlying drivers the metric is, the weaker and more diffused
    your hypotheses become. Moreover, it inherits the difficulty of coming up with
    stories about these drivers, and some of these may compound and create higher-level
    complexity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下游指标的预测比预测由人类或系统行为产生的单个指标更难也更容易。它更难是因为指标与基础驱动因素的距离越远，你的假设就越弱和更加扩散。此外，它还继承了关于这些驱动因素故事的困难，其中一些可能会复合并创建更高层次的复杂性。
- en: This said, many times you can do some hand-waving and exploit the time and space
    correlations to create some features. In a sense, you’re accepting that any stories
    you come up with will be beaten by a simple autoregressive structure that is common
    in time series and spatial autoregressive models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，许多时候你可以做些手势，利用时间和空间的相关性创建一些特征。在某种意义上，你接受任何你提出的故事都会被时间序列和空间自回归模型中常见的简单自回归结构击败。
- en: Feature Engineering
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: Generally speaking, the process of feature engineering entails converting hypotheses
    into measurable variables that have enough signal to help your algorithm learn
    the data generating process. It’s a good practice to split this into several stages,
    as depicted in [Figure 13-4](#ch13_feat_eng_flow).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，特征工程的过程涉及将假设转化为可测量变量，这些变量具有足够的信号来帮助你的算法学习数据生成过程。将这个过程拆分为几个阶段是一个好的做法，如[图 13-4](#ch13_feat_eng_flow)所示。
- en: '![feat eng flow](assets/dshp_1304.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![特征工程流程](assets/dshp_1304.png)'
- en: Figure 13-4\. Feature engineering flow
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-4\. 特征工程流程
- en: 'The stages for feature engineering are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的阶段包括：
- en: Create a set of ideal features.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一组理想特征。
- en: The first step is about translating your hypotheses into *ideal* features, if
    you were able to measure everything precisely. This step is important, as it allows
    you to set a baseline for the second stage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将你的假设转化为*理想*特征，如果你能够精确地测量所有内容的话。这一步非常重要，因为它可以为第二阶段奠定基础。
- en: An example is the role that *intentionality* has on *early churn*, defined as
    those customers that try a product once and leave. One hypothesis is that these
    customers didn’t really intend to use the product (because they were just trying
    it, or the sale was [pushed](https://oreil.ly/HDGj-), or there was sales fraud,
    or the like). Wouldn’t it be great if you could ask them and they answered truthfully?
    Unfortunately, this isn’t practical or achievable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是*意图性*对*早期流失*的影响，定义为那些只试用产品一次就离开的顾客。一个假设是这些顾客实际上并不打算使用该产品（因为他们只是试用，或者销售是[强制性的](https://oreil.ly/HDGj-)，或者存在销售欺诈等）。如果你能问他们并得到真实回答那该多好啊？不幸的是，这不现实也不可实现。
- en: Approximate the ideal features with realistic features.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 用现实特征来近似理想特征。
- en: If you realize that the ideal set of features is unavailable, you need to find
    good proxy features, that is, features that are correlated with the ideal ones.
    Many times, the degree of correlation can be very low, and you need to settle
    for including controls with a very weak correspondence to the original hypothesis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你意识到理想的特征集不可用，你需要找到良好的代理特征，即与理想特征相关的特征。许多时候，相关程度可能非常低，你需要接受包含与原始假设之间联系非常弱的控制变量。
- en: An example of the latter is how culture affects your tastes and thus your likelihood
    to purchase a product. For instance, there may be cultural differences to explain
    why users in different countries decide to accept or reject the cookies in their
    browser (people from some countries may be more sensitive to sharing this information).
    Needless to say, measuring culture is hard. But if you suspect that country-level
    variation will capture a big part of the variation of the cultural hypothesis,
    all you need is to include country dummy variables. It’s a relatively weak set
    of features because these will proxy any feature at this level, and not only culture
    (for instance, differences in regulatory environments).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的一个例子是文化如何影响你的口味，从而影响你购买产品的可能性。例如，可能有文化差异解释为什么来自不同国家的用户决定接受或拒绝其浏览器中的 cookie（一些国家的人可能更加敏感于分享这些信息）。不用说，测量文化是困难的。但如果你怀疑国家层面的变化会捕捉到文化假设变化的大部分，你只需要包含国家虚拟变量即可。这是一个相对弱的特征集，因为这些将代理这一级别的任何特征，而不仅仅是文化（例如，法规环境的差异）。
- en: Transform features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 转换特征。
- en: This is the process of extracting the maximal amount of signal from your features
    by applying a set of transformations on them. Note that I’m departing a bit from
    the literature, since most textbook treatments on feature engineering refer exclusively
    to this stage.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过对特征应用一系列变换来从中提取最大信号量的过程。请注意，我与文献稍有不同，因为大多数有关特征工程的教材只专注于这一阶段。
- en: This stage involves transformations such as [scaling](https://oreil.ly/Hak0v),
    [binarizing and one-hot encoding](https://oreil.ly/ralbT), [imputation of missing
    values](https://oreil.ly/MhGuK), [feature interactions](https://oreil.ly/bT-1q),
    and the like. I provide several references at the end of this chapter where you
    can consult the vast array of available transformations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段涉及转换，比如 [缩放](https://oreil.ly/Hak0v)、[二值化和独热编码](https://oreil.ly/ralbT)、[缺失值的填补](https://oreil.ly/MhGuK)、[特征交互](https://oreil.ly/bT-1q)
    等等。我在本章的末尾提供了几个参考文献，您可以查阅其中丰富的变换方法。
- en: Importantly, transformations depend on your data *and* the algorithm of your
    choice. For instance, with classification and regression trees you may not need
    to take care of outliers by yourself, since the algorithm will do it for you.
    Similarly, with generally nonlinear algorithms, like trees and tree-based ensembles,
    you need not include multiplicative interactions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，变换取决于你的数据 *和* 所选算法。例如，使用分类和回归树时，你可能不需要自己处理异常值，因为算法会替你处理。同样地，对于通常的非线性算法，比如树和基于树的集成，你不需要包括乘法交互。
- en: 'Ex Post Storytelling: Opening the Black Box'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事后叙事：打开黑匣子
- en: 'The problem of ex post storytelling is mainly one of understanding why your
    model makes predictions as it does, what are the most predictive features, and
    how these are correlated to predictions. The two main points you want to convey
    to your audience are:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 事后叙事的问题主要在于理解为什么你的模型做出这样的预测，什么是最具预测性的特征，以及这些特征如何与预测相关。你想向观众传达的两个主要观点是：
- en: The model is incrementally predictive, that is, the prediction error is lower
    than that of the baseline alternative.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是逐步预测的，也就是说，预测误差低于基准替代方案。
- en: The model *makes sense*. A good practice is to start discussing the hypotheses,
    how they were modeled, and how they are consistent with the results.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型 *有意义*。一个好的做法是开始讨论假设，它们是如何被建模的，以及它们如何与结果一致。
- en: Generally speaking, a model is *interpretable* if you can understand what drives
    its predictions. *Local* interpretability aims at understanding specific predictions,
    such as why a customer is deemed highly likely to default on a credit. *Global*
    interpretability aims at providing a general understanding of how features affect
    the outcome. This topic deserves a book-length presentation, but in this chapter
    I can only delve into the more practical matters, and specifically, I will only
    go through methods to achieve global interpretability, as I’ve found these to
    be most useful for storytelling purposes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，一个模型是 *可解释的*，如果你能理解驱动其预测的因素。*局部* 可解释性旨在理解特定预测，比如为什么一个客户被认为极有可能在信用上违约。*全局*
    可解释性旨在提供对特征如何影响结果的总体理解。这个主题值得一本书的详细介绍，但在本章中，我只能深入探讨更实际的问题，具体来说，我只会介绍实现全局可解释性的方法，因为我发现这些方法对叙述目的最有用。
- en: Warning
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Before opening the black box, be sure that your model has enough predictive
    performance, and that there’s no data leakage. You’ll need to devote enough time
    and effort into ex post storytelling, so you’d better start with a good prediction
    model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开黑匣子之前，请确保你的模型具有足够的预测性能，并且没有数据泄漏。你需要投入足够的时间和精力进行事后叙述，所以最好从一个良好的预测模型开始。
- en: Also, when presenting performance metrics, try to make them as relatable to
    your audience as possible. Common metrics, such as the root mean square error
    (RMSE) or the area under the curve (AUC), can be cryptic for your business stakeholder.
    It’s generally worth the effort to translate them to precise business outcomes.
    For instance, if you have a 5% lower RMSE, how is the business better?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当展示性能指标时，尽量使其与你的观众尽可能相关。常见的指标，如均方根误差（RMSE）或曲线下面积（AUC），对于你的业务利益相关者来说可能是神秘的。通常值得努力将它们转化为精确的业务结果。例如，如果你的RMSE降低了5%，业务会变得更好吗？
- en: Interpretability-Performance Trade-Off
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释性与性能的权衡
- en: It can be argued that an ideal ML algorithm is both performant and interpretable.
    Unfortunately, there is usually a trade-off between interpretability and predictive
    performance, so you have to give up part of your understanding of what’s happening
    inside the algorithm if you want to achieve lower prediction error ([Figure 13-5](#ch13_tradeoff_inter)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，理想的机器学习算法既高效又可解释。不幸的是，通常在解释性和预测性能之间存在一种权衡，因此如果你希望达到更低的预测误差，你必须放弃对算法内部发生的事情的部分理解（[图 13-5](#ch13_tradeoff_inter)）。
- en: On one side of the spectrum, you have linear models that are generally considered
    to be highly interpretable but have subpar predictive performance. This set includes
    linear and logistic regression, as well as nonlinear learning algorithms, such
    as classification and regression trees. On the other side of the spectrum are
    the more flexible, and usually highly nonlinear, models, like deep neural networks,
    tree-based ensembles, and support vector machines. These algorithms are generally
    known as *black box* learners. The objective is to open the black box and gain
    a better understanding of what’s going on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的一端，通常被认为具有高可解释性但预测性能不佳的线性模型。这一集合包括线性回归和逻辑回归，以及非线性学习算法，如分类和回归树。在光谱的另一端是更灵活、通常高度非线性的模型，如深度神经网络、基于树的集成和支持向量机。这些算法通常被称为*黑匣子*学习器。目标是打开黑匣子，更好地理解发生的情况。
- en: '![tradeoff](assets/dshp_1305.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![tradeoff](assets/dshp_1305.png)'
- en: Figure 13-5\. Interpretability-performance trade-off
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5\. 解释性与性能的权衡
- en: 'Before moving on, it’s not obvious that you need to interpret the results,
    so let’s briefly discuss why you may want to do so:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，并不明显您需要解释结果，所以让我们简要讨论为什么您可能希望这样做：
- en: Adoption and buy-in
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 采用和认同
- en: Many people need to understand why a prediction is made in order to accept it
    as valid, thereby adopting it. This is most common in organizations that are not
    used to the ML approach, and decisions are usually made using a quasi-data-driven
    approach that involves a lot of gut instinct. You may find it easier for your
    stakeholders to accept your results and sponsor your project if you are able to
    open the black box for them.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人需要了解为什么做出预测，以便将其接受为有效，从而采用它。这在不习惯机器学习方法的组织中最为常见，决策通常是基于准数据驱动方法，其中包括大量直觉。如果您能够为他们打开黑匣子，可能会更容易让您的利益相关者接受您的结果并赞助您的项目。
- en: Low real-world predictive performance
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 低现实世界预测性能
- en: Opening the black box is one of the most effective ways to detect and correct
    problems like data leakage ([Chapter 11](ch11.html#ch11_dataleakage)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 打开黑匣子是检测和纠正数据泄漏等问题的最有效方法之一（见[第11章](ch11.html#ch11_dataleakage)）。
- en: Ethics and regulatory requirements
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 道德和法规要求
- en: In certain industries it’s actually required that companies explain why a certain
    prediction was made. For instance, [in the US](https://oreil.ly/5zj9j), the Equal
    Opportunity Act entitles anyone to ask for the reasons why a credit was denied.
    A similar criterion applies with the European General Data Protection Regulation
    (GDPR). Even if you are not required to, you may want to validate whether the
    predictions and subsequent decisions follow a minimal ethical standard by opening
    the black box.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些行业中，公司确实需要解释为什么会做出某些预测。例如，在美国，平等机会法授权任何人询问为何拒绝授信。欧洲的一般数据保护条例（GDPR）也有类似的标准。即使您没有这样的要求，您可能希望通过打开黑匣子来验证预测和后续决策是否符合最低道德标准。
- en: 'Linear Regression: Setting a Benchmark'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归：设定一个基准
- en: 'Linear regression provides a useful benchmark to understand interpretability
    (see also [Chapter 10](ch10.html#ch10_linreg)). Consider the following simple
    model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归为理解可解释性提供了一个有用的基准（另见[第10章](ch10.html#ch10_linreg)）。考虑以下简单模型：
- en: <math alttext="y equals alpha 0 plus alpha 1 x 1 plus alpha 2 x 2 plus epsilon"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals alpha 0 plus alpha 1 x 1 plus alpha 2 x 2 plus epsilon"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: 'By making strong linearity assumptions about the underlying data generating
    process, you immediately get:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对底层数据生成过程做出强线性假设，您将立即得到：
- en: Effect directionality
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 效果方向性
- en: The sign of each coefficient tells you if the feature is positively or negatively
    correlated with the outcome, after controlling for all other features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个系数的符号告诉您特征在控制所有其他特征后与结果正相关还是负相关。
- en: Effect magnitude
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 效果大小
- en: Each coefficient is interpreted as the change in the outcome associated with
    a one-unit change in each feature, holding other features fixed. Importantly,
    no causal interpretation can be given without further assumptions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个系数被解释为每个特征变化一个单位时结果变化的大小，其他特征保持不变。重要的是，在没有进一步假设的情况下不能给出因果解释。
- en: Local interpretability
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本地可解释性
- en: From the first two items, you can assert why any individual prediction was made.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前两个项目，您可以断定为什么会做出任何个别预测。
- en: 'Some data scientists make the mistake of giving the absolute magnitude of the
    coefficients a relative *importance* interpretation. To see why this doesn’t work,
    take the following model, where revenue is expressed as a function of the size
    of the sales force and paid marketing spend (search engine marketing or SEM):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据科学家犯了一个错误，他们对系数的绝对大小给予了相对*重要性*的解释。要看出这种方法不起作用的原因，请考虑以下模型，其中收入被表达为销售团队规模和付费营销支出（搜索引擎营销或
    SEM）的函数：
- en: <math alttext="revenue equals 100 plus 1000 times Num period sales execs plus
    0.5 times SEM spend" display="block"><mrow><mtext>revenue</mtext> <mo>=</mo> <mn>100</mn>
    <mo>+</mo> <mn>1000</mn> <mo>×</mo> <mtext>Num.</mtext> <mtext>sales</mtext> <mtext>execs</mtext>
    <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>×</mo> <mtext>SEM</mtext> <mtext>spend</mtext></mrow></math>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="revenue equals 100 plus 1000 times Num period sales execs plus
    0.5 times SEM spend" display="block"><mrow><mtext>revenue</mtext> <mo>=</mo> <mn>100</mn>
    <mo>+</mo> <mn>1000</mn> <mo>×</mo> <mtext>Num.</mtext> <mtext>sales</mtext> <mtext>execs</mtext>
    <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>×</mo> <mtext>SEM</mtext> <mtext>spend</mtext></mrow></math>
- en: 'This says that, on average and holding other factors fixed, each additional:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，平均而言，保持其他因素不变，每额外：
- en: Sales executive is associated with an increase of $1,000 in revenue.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售执行人员与收入增加 $1,000 相关。
- en: Each dollar spent on SEM (for example, bids on Google, Bing, or Facebook ads)
    is associated with an increase of 50 cents in revenue.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每花费一美元在搜索引擎营销（例如在谷歌、必应或 Facebook 广告上的出价）上，与收入增加 50 美分相关。
- en: 'You would be tempted to conclude that increasing the size of the sales force
    is *more important* for your revenues, compared to paid marketing spend. Unfortunately,
    this is an apples-to-oranges comparison since each feature is measured in different
    units. A trick to measure everything in the same units is to run a regression
    on standardized features:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会倾向于认为增加销售人员比增加付费营销支出*更重要*，对你的收入更有利。不幸的是，这是一个不同单位的比较，就像是苹果和橙子。一个将所有特征标准化的技巧是在标准化特征上进行回归分析：
- en: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    beta 0 plus beta 1 x overTilde Subscript 1 plus beta 2 x overTilde Subscript 2
    plus eta 2nd Row 1st Column where 2nd Column Blank 3rd Column z overTilde equals
    StartFraction z minus m e a n left-parenthesis z right-parenthesis Over s t d
    left-parenthesis z right-parenthesis EndFraction for any variable z EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi>y</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <msub><mover accent="true"><mi>x</mi>
    <mo>˜</mo></mover> <mn>1</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <msub><mover accent="true"><mi>x</mi> <mo>˜</mo></mover> <mn>2</mn></msub> <mo>+</mo>
    <mi>η</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mtext>where</mtext></mtd>
    <mtd columnalign="left"><mrow><mover accent="true"><mi>z</mi> <mo>˜</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>z</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow>
    <mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mfrac>
    <mtext>for</mtext> <mtext>any</mtext> <mtext>variable</mtext> <mi>z</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    beta 0 plus beta 1 x overTilde Subscript 1 plus beta 2 x overTilde Subscript 2
    plus eta 2nd Row 1st Column where 2nd Column Blank 3rd Column z overTilde equals
    StartFraction z minus m e a n left-parenthesis z right-parenthesis Over s t d
    left-parenthesis z right-parenthesis EndFraction for any variable z EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi>y</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <msub><mover accent="true"><mi>x</mi>
    <mo>˜</mo></mover> <mn>1</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <msub><mover accent="true"><mi>x</mi> <mo>˜</mo></mover> <mn>2</mn></msub> <mo>+</mo>
    <mi>η</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mtext>where</mtext></mtd>
    <mtd columnalign="left"><mrow><mover accent="true"><mi>z</mi> <mo>˜</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>z</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow>
    <mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mfrac>
    <mtext>for</mtext> <mtext>any</mtext> <mtext>variable</mtext> <mi>z</mi></mrow></mtd></mtr></mtable></math>
- en: 'Note that regression coefficients on standardized variables are generally different
    from those in the original model (hence the different greek letters), and thus
    have a different interpretation: by standardizing all features, you measure everything
    in units of standard deviations (*unitless* is a better term), ensuring that you
    compare apples to apples. You can then say things like: *<math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> is more important than <math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math> , since an additional standard deviation in <math alttext="x
    1"><msub><mi>x</mi> <mn>1</mn></msub></math> increases revenue by more than a
    corresponding increase in <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>*
    .'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，标准化变量的回归系数通常与原始模型中的系数不同（因此希腊字母不同），因此具有不同的解释：通过标准化所有特征，你用标准偏差的单位（*无单位* 是更好的术语）来衡量所有事物，确保你在进行苹果对苹果的比较。然后，你可以说类似于：<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> 比 <math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math> 更重要，因为在 <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    的一个额外标准偏差增加比在 <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> 的相应增加带来更多的收入*。
- en: 'The trick here is to find a way to convert the original units into a common
    unit (in this case, standard deviations). But any other common unit could also
    work. For instance, imagine that each additional sales executive costs $5,000
    per month, on average. Since marketing spend is already in dollars, you end up
    saying that on average, each additional dollar spent in:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍是找到一种方法将原始单位转换为一个通用单位（在本例中是标准偏差）。但任何其他通用单位也可以起作用。例如，假设每增加一个销售执行人员平均每月成本为
    5,000 美元。由于营销支出已经以美元计量，你最终得出结论，平均每额外花费一美元在：
- en: Sales executives is associated with a 20 cent increase in revenue
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售执行人员与收入增加 20 美分相关。
- en: Paid marketing is associated with a 50 cent increase in revenue
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 付费营销与收入增加 50 美分相关。
- en: While this last method also works, standardization is a much more common method
    to find a common unit for all features. The important thing to remember is that
    you’re now able to *rank* features in some meaningful way.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后这种方法也有效，但标准化是一种更常见的方法，用于找到所有特征的通用单位。需要记住的重要事情是，现在你能够*排名*各个特征，有意义地进行比较。
- en: '[Figure 13-6](#ch13_std_reg) plots the estimated coefficients, along with 95%
    confidence intervals, for a simulated linear model with two zero-mean, normally
    distributed features ( <math alttext="x 1 comma x 2"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math> ), as in the previous
    equations. Features <math alttext="z 1 comma z 2 comma z 3"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>z</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>z</mi>
    <mn>3</mn></msub></mrow></math> are additional variables correlated to <math alttext="x
    2"><msub><mi>x</mi> <mn>2</mn></msub></math> , but are otherwise unrelated to
    the outcome. Importantly, I set the true parameters to <math alttext="alpha 1
    equals alpha 2 equals 1"><mrow><msub><mi>α</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>α</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math> and <math alttext="upper
    V a r left-parenthesis x 1 right-parenthesis equals 1 comma upper V a r left-parenthesis
    x 2 right-parenthesis equals 5"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mi>V</mi> <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mn>5</mn></mrow></math> . This has two effects:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-6](#ch13_std_reg) 绘制了一个带有两个零均值、正态分布特征（ <math alttext="x 1 comma x 2"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
    ）的模拟线性模型的估计系数，以及 95% 置信区间，与前述方程式相同。特征 <math alttext="z 1 comma z 2 comma z 3"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>z</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>z</mi>
    <mn>3</mn></msub></mrow></math> 是与 <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    相关但与结果无关的附加变量。重要的是，我将真实参数设置为 <math alttext="alpha 1 equals alpha 2 equals 1"><mrow><msub><mi>α</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>α</mi> <mn>2</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    和 <math alttext="upper V a r left-parenthesis x 1 right-parenthesis equals 1 comma
    upper V a r left-parenthesis x 2 right-parenthesis equals 5"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <mi>V</mi> <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>5</mn></mrow></math>
    。这样做有两个效果：'
- en: It increases the signal-to-noise ratio for the second feature, thereby making
    it more informative.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它增加了第二特征的信噪比，从而使其更具信息性。
- en: It increases the true coefficient:^([1](ch13.html#id720)) <math alttext="beta
    2 equals StartRoot 5 EndRoot alpha 2"><mrow><msub><mi>β</mi> <mn>2</mn></msub>
    <mo>=</mo> <msqrt><mn>5</mn></msqrt> <msub><mi>α</mi> <mn>2</mn></msub></mrow></math>
    .
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它增加了真实系数：^([1](ch13.html#id720)) <math alttext="beta 2 equals StartRoot 5 EndRoot
    alpha 2"><mrow><msub><mi>β</mi> <mn>2</mn></msub> <mo>=</mo> <msqrt><mn>5</mn></msqrt>
    <msub><mi>α</mi> <mn>2</mn></msub></mrow></math> 。
- en: '![standardized features](assets/dshp_1306.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![标准化特征](assets/dshp_1306.png)'
- en: Figure 13-6\. Regression in linear versus standardized features
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-6\. 线性与标准化特征的回归
- en: By standardizing both features, it becomes noteworthy that the second feature
    ranks higher in terms of importance, as defined earlier. Thanks to the confidence
    intervals, you can also conclude that the last three features are uninformative.
    An alternative to the statistical approach would be to use *regularization*, such
    as in a Lasso regression.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化这两个特征，显然第二个特征在重要性上排名较高，正如之前定义的那样。由于置信区间的存在，你还可以得出结论，最后三个特征并不具有信息性。在统计方法之外的另一种选择是使用*正则化*，例如
    Lasso 回归。
- en: Feature Importance
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'Many times you want to rank the features according to some objective measure
    of importance. This is useful for ex ante and ex post storytelling purposes. From
    an ex post point of view, you can say things like: *we found that the time of
    the transaction is the most important predictor of fraud*, which might help you
    sell the result of your model, and will also deliver potentially great Aha! moments
    for you and your audience (see also [Chapter 7](ch07.html#ch07_narratives)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，你希望根据某种客观的重要性度量对特征进行排名。这对前瞻性和事后叙事目的都很有用。从事后的角度来看，你可以说：“我们发现交易时间是欺诈的最重要预测因子”，这可能有助于你销售模型的结果，并为你和你的听众带来潜在的顿悟时刻（也见
    [第 7 章](ch07.html#ch07_narratives)）。
- en: From an ex ante point of view, having a way to rank features by importance can
    help you iterate on your hypotheses or feature engineering, or improve your understanding
    of a problem. If you have well-thought-out hypotheses and your results look suspicious,
    it’s more likely that you made a programming error on the feature engineering
    side, or that you have data leakage.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从前瞻角度来看，通过一种按重要性对特征进行排名的方法可以帮助你对假设或特征工程进行迭代，或者提高你对问题的理解。如果你有深思熟虑的假设，但结果看起来可疑，那么更有可能是在特征工程方面出现了编程错误，或者有数据泄漏。
- en: 'Earlier, I used standardized features in a linear regression to get one possible
    such importance ranking:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我在线性回归中使用标准化特征得出了一个可能的重要性排名：
- en: Standardized feature importance in linear regression
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归中的标准化特征重要性
- en: A feature *x* is more important than feature *z* if a one standard deviation
    increase in *x* is associated with a larger change in the outcome, in absolute
    value.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*x*的一个标准差增加与结果的绝对值变化更大相关，那么特征*x*就比特征*z*更重要。
- en: 'Alternatively, *importance* can be defined in terms of each feature’s amount
    of information content for the prediction problem at hand. Intuitively, the higher
    the information content of a feature (for a given outcome), the lower the prediction
    error if the feature is included. There are two commonly used metrics that follow
    this route:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，*重要性*可以根据每个特征对手头预测问题的信息量来定义。直观上，特征的信息量越高（针对给定结果），如果包含该特征，则预测误差就越低。存在两种常用的按此途径的度量标准：
- en: Impurity-based feature importance
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不纯度的特征重要性
- en: A feature *x* is more important than feature *z*, from a node impurity point
    of view, if the relative improvement in prediction error from nodes where *x*
    was chosen as a splitting variable is larger than the corresponding increase for
    *z*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从节点不纯度的角度来看，如果选择*x*作为分割变量的节点导致的预测误差相对改进大于*z*的相应增加，则特征*x*比特征*z*更重要。
- en: Permutation importance
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 排列重要性
- en: A feature *x* is more important than feature *z*, from a permutation point of
    view, if the relative loss in performance when the values of *x* are permuted
    is larger than that for *z*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对*x*值进行排列时性能损失的相对大小大于*z*的话，那么特征*x*比特征*z*更重要，从排列的角度看。
- en: Note that [impurity-based feature importance](https://oreil.ly/acJDH) only works
    for tree-based ML algorithms. Every time a node is split using a feature, the
    improvement in performance is saved, so at the end you can compute the share of
    improvements for all features relative to the total improvement. With ensembles,
    this is the average across all trees grown.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[基于不纯度的特征重要性](https://oreil.ly/acJDH) 仅适用于基于树的机器学习算法。每次使用一个特征分割节点时，性能改进都会被保存，因此最终你可以计算各个特征的改进占总体改进的比例。在集成模型中，这是所有生成树的平均值。
- en: On the other hand, [permutation importance](https://oreil.ly/84XXY) works with
    any ML algorithm since you just shuffle the values of each feature (several times,
    as in a bootstrapping procedure) and compute the loss in performance. The intuition
    is that the actual order matters more for *important* features, so there should
    be a larger loss in performance from the permutation of values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[排列重要性](https://oreil.ly/84XXY) 适用于任何机器学习算法，因为你只需对每个特征的值进行洗牌（多次，就像启发法样本集程序中一样），然后计算性能损失。直觉是真实顺序对*重要*特征更重要，因此值的排列应该会导致性能损失更大。
- en: '[Figure 13-7](#ch13_feat_imps) shows permutation and impurity-based feature
    importances using the same simulated dataset as before, trained with a gradient
    boosting regression (no metaparameter optimization), along with 95% confidence
    intervals. Confidence intervals for permutation importances are computed parametrically
    (assuming normality) using the means and standard deviations provided by scikit-learn.
    I obtain analogous intervals for impurity-based features using bootstrapping (see
    [Chapter 9](ch09.html#ch09_simulation)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-7](#ch13_feat_imps) 显示了使用与之前相同的模拟数据集训练的梯度提升回归（不进行元参数优化）得出的排列和基于不纯度的特征重要性，以及
    95% 置信区间。排列重要性的置信区间是使用 scikit-learn 提供的均值和标准差进行参数计算（假设正态分布）。我使用自助法获得了基于不纯度特征的类似区间（参见[第
    9 章](ch09.html#ch09_simulation)）。'
- en: '![feature importance](assets/dshp_1307.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![特征重要性](assets/dshp_1307.png)'
- en: Figure 13-7\. Feature importances for simulated model using gradient boosting
    regression
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-7\. 使用梯度提升回归的模拟模型的特征重要性
- en: Heatmaps
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 热图
- en: 'Heatmaps are very easy to compute and are generally quite good at visually
    displaying the correlation between each feature and the predicted outcome. This
    is quite handy to say things like *when x increases, y falls*. Many hypotheses
    are stated directionally, so a quick first test of whether this holds in practice
    is quite useful. The process to calculate them is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 热图非常易于计算，通常在直观显示每个特征与预测结果之间的相关性方面表现良好。这对于说出诸如 *当 x 增加时，y 会下降* 这样的假设非常方便。许多假设在方向上陈述，因此快速测试它们在实践中是否成立非常有用。计算它们的过程如下：
- en: Split the predicted outcome (regression) or probability (classification) in
    deciles, or any other quantile.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果（回归）或概率（分类）分成十分位数或其他任何分位数。
- en: 'For each feature *<math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>*
    and decile *d*, calculate the average across all units in that bucket: <math alttext="x
    overbar Subscript j comma d"><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mrow><mi>j</mi><mo>,</mo><mi>d</mi></mrow></msub></math>
    .'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个特征 *<math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>*
    和分位数 *d*，计算该桶中所有单位的平均值： <math alttext="x overbar Subscript j comma d"><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mrow><mi>j</mi><mo>,</mo><mi>d</mi></mrow></msub></math> 。
- en: These can be arranged in a table with deciles in the columns, and features in
    the rows. It’s usually good to order the features using some measure of importance
    so that you focus on the most relevant features first.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以按照列中的十分位数和行中的特征进行表格排列。通常最好使用某种重要性度量来排序特征，这样可以首先关注最相关的特征。
- en: '[Figure 13-8](#ch13_heatmap_reg) shows a heatmap for the linear regression
    trained on the previous simulated example, where features have already been sorted
    by feature importance. Just by inspecting the relative shades for each feature
    (row), you can easily identify any patterns, or lack thereof.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-8](#ch13_heatmap_reg) 展示了在之前的模拟示例上训练的线性回归的热图，特征已按重要性排序。仅通过检查每个特征（行）的相对色调，你可以轻松识别任何模式或其缺失。'
- en: '![A figure with a heatmap](assets/dshp_1308.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![带有热图的图例](assets/dshp_1308.png)'
- en: Figure 13-8\. Feature heatmap for previous simulated example
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-8\. 上一个模拟示例的特征热图
- en: For instance, <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    is positively correlated with the outcome, as expected since the true coefficient
    in the simulation is equal to one. Units in the lower decile have -3.58 units
    on average, and this increases monotonically up to 4.23 units on average for the
    top decile.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如预期的那样，<math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> 与结果呈正相关，因为模拟中的真实系数等于一。低分位数的单位平均减少了
    -3.58 个单位，而这在最高分位数中逐渐增加到平均值为 4.23 个单位。
- en: 'Inspecting the row for <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    shows the main problem that heatmaps have: they present bivariate correlations
    only. The true correlation is positive ( <math alttext="alpha 1 equals 1"><mrow><msub><mi>α</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math> ), but the heatmap fails
    to capture this monotonicity. To understand why, notice that <math alttext="x
    1"><msub><mi>x</mi> <mn>1</mn></msub></math> and <math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math> are *negatively* correlated ([Figure 13-9](#ch13_heat_scatter)).
    However, the larger variance of the second feature gives it more predictive power,
    and thus more weight in the final ordering of the predicted outcome (and deciles).
    These two facts break the monotonicity that was expected for the second feature.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> 的行显示了热图存在的主要问题：它们仅呈现双变量相关性。真实的相关性是正的（
    <math alttext="alpha 1 equals 1"><mrow><msub><mi>α</mi> <mn>1</mn></msub> <mo>=</mo>
    <mn>1</mn></mrow></math> ），但热图未能捕捉到这种单调性。要理解原因，请注意 <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> 和 <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    是 *负相关* 的（[图 13-9](#ch13_heat_scatter)）。然而，第二个特征具有更大的方差，因此在最终预测结果（和十分位数）的排序中具有更多的预测能力。这两个事实破坏了对第二个特征预期的单调性。
- en: '![scatterplot](assets/dshp_1309.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![散点图](assets/dshp_1309.png)'
- en: Figure 13-9\. x[2] and x[1] are negatively correlated
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-9\. x[2] 和 x[1] 是负相关的
- en: Partial Dependence Plots
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分依赖图
- en: With partial dependence plots (PDPs), you predict the outcome or probability
    by only changing one feature at a time, while fixing everything else. It’s quite
    appealing because of the similarity to what you get from taking the partial derivatives
    in linear regression.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用部分依赖图（PDPs），你可以逐个更改一个特征而固定其他所有内容来预测结果或概率。由于与线性回归中的偏导数相似，这一方法非常吸引人。
- en: 'In [Chapter 9](ch09.html#ch09_simulation), I used the following method to calculate
    PDPs that captures this intuition very closely. You first calculate the means
    for all features, then create a linear grid of size *G* for the feature you want
    to simulate, and assemble everything into a matrix of the form:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#ch09_simulation)中，我使用了以下方法计算PDPs，非常接近捕捉这种直觉。首先计算所有特征的均值，然后为要模拟的特征创建大小为*G*的线性网格，并将所有内容组合成以下形式的矩阵：
- en: <math alttext="bold upper X overbar Subscript bold j Baseline equals Start 4
    By 6 Matrix 1st Row 1st Column x overbar Subscript 1 2nd Column x overbar Subscript
    2 3rd Column midline-horizontal-ellipsis 4th Column x Subscript 0 j 5th Column
    midline-horizontal-ellipsis 6th Column x overbar Subscript upper K 2nd Row 1st
    Column x overbar Subscript 1 2nd Column x overbar Subscript 2 3rd Column midline-horizontal-ellipsis
    4th Column x Subscript 1 j 5th Column midline-horizontal-ellipsis 6th Column x
    overbar Subscript upper K 3rd Row 1st Column vertical-ellipsis 2nd Column vertical-ellipsis
    3rd Column down-right-diagonal-ellipsis 4th Column vertical-ellipsis 5th Column
    vertical-ellipsis 4th Row 1st Column x overbar Subscript 1 2nd Column x overbar
    Subscript 2 3rd Column midline-horizontal-ellipsis 4th Column x Subscript upper
    G j 5th Column midline-horizontal-ellipsis 6th Column x overbar Subscript upper
    K EndMatrix Subscript upper G times upper K" display="block"><mrow><msub><mover><mi>𝐗</mi>
    <mo>¯</mo></mover> <mi>𝐣</mi></msub> <mo>=</mo> <msub><mfenced close=")" open="("><mtable><mtr><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover>
    <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi> <mrow><mn>0</mn><mi>j</mi></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi>
    <mrow><mn>1</mn><mi>j</mi></mrow></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover>
    <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi> <mrow><mi>G</mi><mi>j</mi></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr></mtable></mfenced>
    <mrow><mi>G</mi><mo>×</mo><mi>K</mi></mrow></msub></mrow></math>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper X overbar Subscript bold j Baseline equals Start 4
    By 6 Matrix 1st Row 1st Column x overbar Subscript 1 2nd Column x overbar Subscript
    2 3rd Column midline-horizontal-ellipsis 4th Column x Subscript 0 j 5th Column
    midline-horizontal-ellipsis 6th Column x overbar Subscript upper K 2nd Row 1st
    Column x overbar Subscript 1 2nd Column x overbar Subscript 2 3rd Column midline-horizontal-ellipsis
    4th Column x Subscript 1 j 5th Column midline-horizontal-ellipsis 6th Column x
    overbar Subscript upper K 3rd Row 1st Column vertical-ellipsis 2nd Column vertical-ellipsis
    3rd Column down-right-diagonal-ellipsis 4th Column vertical-ellipsis 5th Column
    vertical-ellipsis 4th Row 1st Column x overbar Subscript 1 2nd Column x overbar
    Subscript 2 3rd Column midline-horizontal-ellipsis 4th Column x Subscript upper
    G j 5th Column midline-horizontal-ellipsis 6th Column x overbar Subscript upper
    K EndMatrix Subscript upper G times upper K" display="block"><mrow><msub><mover><mi>𝐗</mi>
    <mo>¯</mo></mover> <mi>𝐣</mi></msub> <mo>=</mo> <msub><mfenced close=")" open="("><mtable><mtr><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover>
    <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi> <mrow><mn>0</mn><mi>j</mi></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi>
    <mrow><mn>1</mn><mi>j</mi></mrow></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mover><mi>x</mi>
    <mo>¯</mo></mover> <mn>1</mn></msub></mtd> <mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover>
    <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd><mtd><msub><mi>x</mi> <mrow><mi>G</mi><mi>j</mi></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd><mtd><msub><mover><mi>x</mi> <mo>¯</mo></mover> <mi>K</mi></msub></mtd></mtr></mtable></mfenced>
    <mrow><mi>G</mi><mo>×</mo><mi>K</mi></mrow></msub></mrow></math>
- en: 'You then use this matrix to create a prediction with your trained model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用这个矩阵来使用你的训练模型进行预测：
- en: <math alttext="PDP Superscript left-parenthesis 1 right-parenthesis Baseline
    left-parenthesis x Subscript j Baseline right-parenthesis equals ModifyingAbove
    f With caret left-parenthesis bold upper X overbar Subscript j Baseline right-parenthesis"
    display="block"><mrow><msup><mtext>PDP</mtext> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msup>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mover><mi>𝐗</mi>
    <mo>¯</mo></mover> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="PDP Superscript left-parenthesis 1 right-parenthesis Baseline
    left-parenthesis x Subscript j Baseline right-parenthesis equals ModifyingAbove
    f With caret left-parenthesis bold upper X overbar Subscript j Baseline right-parenthesis"
    display="block"><mrow><msup><mtext>PDP</mtext> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msup>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mover><mi>𝐗</mi>
    <mo>¯</mo></mover> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: This method is fast and intuitively appealing, and it also allows you to quickly
    simulate the impact of interactions between features. However, from a statistical
    point of view, it’s not really correct since the average of a function is generally
    different from the function evaluated on the averages of the inputs (unless your
    model is linear). The main advantage is that it requires only one evaluation of
    the trained model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法快速且直观吸引人，同时还允许你快速模拟特征之间的相互作用的影响。但是从统计学角度来看，它并不完全正确，因为函数的平均值通常与在输入的平均值上评估的函数不同（除非你的模型是线性的）。其主要优势在于只需要评估一次训练模型。
- en: 'The correct way to do it—and the method used by [scikit-learn](https://oreil.ly/waddK)
    to compute PDPs—requires *N* (sample size) evaluations of the trained model for
    each value *g* in the grid. These are then averaged out to get:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的方法——以及[scikit-learn](https://oreil.ly/waddK)用于计算PDPs的方法——对于网格中每个值*g*需要*N*（样本大小）次训练模型的评估。然后对这些评估结果进行平均，得到：
- en: <math alttext="PDP Superscript left-parenthesis 2 right-parenthesis Baseline
    left-parenthesis x Subscript j Baseline equals g right-parenthesis equals StartFraction
    1 Over upper N EndFraction sigma-summation Underscript i equals 1 Overscript upper
    N Endscripts ModifyingAbove f With caret left-parenthesis x Subscript 1 comma
    i Baseline comma midline-horizontal-ellipsis comma x Subscript j minus 1 comma
    i Baseline comma g comma x Subscript j plus 1 comma i Baseline comma midline-horizontal-ellipsis
    comma x Subscript upper K comma i Baseline right-parenthesis" display="block"><mrow><msup><mtext>PDP</mtext>
    <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>=</mo> <mi>g</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>j</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>,</mo> <mi>g</mi> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>j</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>K</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="PDP Superscript left-parenthesis 2 right-parenthesis Baseline
    left-parenthesis x Subscript j Baseline equals g right-parenthesis equals StartFraction
    1 Over upper N EndFraction sigma-summation Underscript i equals 1 Overscript upper
    N Endscripts ModifyingAbove f With caret left-parenthesis x Subscript 1 comma
    i Baseline comma midline-horizontal-ellipsis comma x Subscript j minus 1 comma
    i Baseline comma g comma x Subscript j plus 1 comma i Baseline comma midline-horizontal-ellipsis
    comma x Subscript upper K comma i Baseline right-parenthesis" display="block"><mrow><msup><mtext>PDP</mtext>
    <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>=</mo> <mi>g</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>j</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>,</mo> <mi>g</mi> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>j</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>K</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: Interactions can be easily simulated by changing several features at a time.
    In practice, often the two methods provide similar results, but this really depends
    on the distribution of the features and the real unobserved data generating process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同时更改多个特征可以轻松模拟交互作用。实际上，通常两种方法提供类似的结果，但这实际上取决于特征的分布和真实的未观察到的数据生成过程。
- en: Before moving on, notice that in this last computation you have to compute a
    prediction for each row in your dataset. With *individual conditional expectation
    (ICE) plots*, you visually display these effects across units, making it a method
    of local interpretability, as opposed to PDPs.^([2](ch13.html#id733))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请注意，在这最后的计算中，你必须为数据集中的每一行计算预测。使用*个体条件期望（ICE）图*，你可以在单位之间视觉显示这些效果，使其成为局部可解释性方法，与PDPs不同。^([2](ch13.html#id733))
- en: 'Let’s simulate a nonlinear model to see the two methods in action, using the
    following data generating process:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们模拟一个非线性模型，以查看这两种方法的效果，使用以下数据生成过程：
- en: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    x 1 plus 2 x 1 squared minus 2 x 1 x 2 minus x 2 squared plus epsilon 2nd Row
    1st Column x 1 2nd Column tilde 3rd Column upper G a m m a left-parenthesis shape
    equals 1 comma scale equals 1 right-parenthesis 3rd Row 1st Column x 2 2nd Column
    tilde 3rd Column upper N left-parenthesis 0 comma 1 right-parenthesis 4th Row
    1st Column epsilon 2nd Column tilde 3rd Column upper N left-parenthesis 0 comma
    5 right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>2</mn> <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>-</mo> <mn>2</mn> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>ϵ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>x</mi> <mn>1</mn></msub></mtd> <mtd><mo>∼</mo></mtd>
    <mtd columnalign="left"><mrow><mi>G</mi> <mi>a</mi> <mi>m</mi> <mi>m</mi> <mi>a</mi>
    <mo>(</mo> <mtext>shape</mtext> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mtext>scale</mtext>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msub><mi>x</mi>
    <mn>2</mn></msub></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>ϵ</mi></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>5</mn> <mo>)</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    x 1 plus 2 x 1 squared minus 2 x 1 x 2 minus x 2 squared plus epsilon 2nd Row
    1st Column x 1 2nd Column tilde 3rd Column upper G a m m a left-parenthesis shape
    equals 1 comma scale equals 1 right-parenthesis 3rd Row 1st Column x 2 2nd Column
    tilde 3rd Column upper N left-parenthesis 0 comma 1 right-parenthesis 4th Row
    1st Column epsilon 2nd Column tilde 3rd Column upper N left-parenthesis 0 comma
    5 right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>2</mn> <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>-</mo> <mn>2</mn> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>ϵ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>x</mi> <mn>1</mn></msub></mtd> <mtd><mo>∼</mo></mtd>
    <mtd columnalign="left"><mrow><mi>G</mi> <mi>a</mi> <mi>m</mi> <mi>m</mi> <mi>a</mi>
    <mo>(</mo> <mtext>shape</mtext> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mtext>scale</mtext>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msub><mi>x</mi>
    <mn>2</mn></msub></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>ϵ</mi></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>5</mn> <mo>)</mo></mrow></mtd></mtr></mtable></math>
- en: I use a gamma distribution for the first feature to highlight the effect that
    outliers may have when you use either method.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用伽马分布为第一个特征，以突出使用任一方法时可能出现的异常值的影响。
- en: '[Figure 13-10](#ch13_pdps_nonlinear) shows the estimated and true PDPs using
    both methods. PDPs for the first feature capture well the shape of the true relationship,
    but the two methods start diverging from each other for larger values of <math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> . This is expected because
    the sample mean is sensitive to outliers, so with the first method you end up
    using an average unit with a relatively large first feature. With the second method,
    this isn’t as pronounced since individual predictions are averaged out, and in
    this particular example the functional form smooths out the effect of the outliers.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-10](#ch13_pdps_nonlinear)展示了使用两种方法估计的和真实的PDPs。对于第一个特征，PDPs很好地捕捉了真实关系的形状，但是随着<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>的值变大，这两种方法开始出现分歧。这是预期的，因为样本均值对异常值很敏感，所以使用第一种方法时，你最终会使用具有相对较大第一个特征的平均单元。而第二种方法中，这种情况没有那么明显，因为个别预测被平均掉了，而且在这个特定示例中，函数形式平滑了异常值的影响。'
- en: '![pdps for nonlinear](assets/dshp_1310.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![非线性的PDPs](assets/dshp_1310.png)'
- en: Figure 13-10\. PDPs using both methods in the simulated data
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-10\. 使用两种方法在模拟数据中的PDPs
- en: While PDPs are great, they are biased with correlated features. For instance,
    if <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> and <math alttext="x
    2"><msub><mi>x</mi> <mn>2</mn></msub></math> are positively correlated, both will
    then have small or large values at the same time. But with a PDP you may end up
    unrealistically imposing a small value (from the grid) for <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> when the corresponding value for the second feature is
    large.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PDPs很棒，但它们受到相关特征偏差的影响。例如，如果<math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>和<math
    alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>呈正相关，它们会同时具有较小或较大的值。但是使用PDP时，可能会在第二个特征对应的值较大时，对<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>施加不现实的小值（从网格中取）。
- en: 'To see this in practice, I simulated this modified version of the previous
    nonlinear model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要在实践中看到这一点，我模拟了上一个非线性模型的修改版本：
- en: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    x 1 plus 2 x 1 squared minus 2 x 1 x 2 minus x 2 squared plus epsilon 2nd Row
    1st Column x 1 comma x 2 2nd Column tilde 3rd Column upper N left-parenthesis
    bold 0 comma bold upper Sigma left-parenthesis rho right-parenthesis right-parenthesis
    3rd Row 1st Column epsilon 2nd Column tilde 3rd Column upper N left-parenthesis
    0 comma 5 right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>2</mn> <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>-</mo> <mn>2</mn> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>ϵ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn mathvariant="bold">0</mn> <mo>,</mo> <mi>Σ</mi> <mo>(</mo> <mi>ρ</mi>
    <mo>)</mo> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>ϵ</mi></mtd>
    <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mn>5</mn> <mo>)</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column
    x 1 plus 2 x 1 squared minus 2 x 1 x 2 minus x 2 squared plus epsilon 2nd Row
    1st Column x 1 comma x 2 2nd Column tilde 3rd Column upper N left-parenthesis
    bold 0 comma bold upper Sigma left-parenthesis rho right-parenthesis right-parenthesis
    3rd Row 1st Column epsilon 2nd Column tilde 3rd Column upper N left-parenthesis
    0 comma 5 right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>2</mn> <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>-</mo> <mn>2</mn> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>ϵ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd> <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>(</mo> <mn mathvariant="bold">0</mn> <mo>,</mo> <mi>Σ</mi> <mo>(</mo> <mi>ρ</mi>
    <mo>)</mo> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>ϵ</mi></mtd>
    <mtd><mo>∼</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mn>5</mn> <mo>)</mo></mrow></mtd></mtr></mtable></math>
- en: where the features are now drawn from a multivariate normal distribution with
    a covariance matrix indexed by a correlation parameter. [Figure 13-11](#ch13_pdps_correlated)
    plots the estimated and true PDPs for uncorrelated ( <math alttext="rho equals
    0"><mrow><mi>ρ</mi> <mo>=</mo> <mn>0</mn></mrow></math> ) and ( <math alttext="rho
    equals 0.9"><mrow><mi>ρ</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>9</mn></mrow></math>
    ) correlated features, where you can readily verify that PDPs are biased when
    features are correlated.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，特征是从具有由相关参数索引的协方差矩阵的多元正态分布中绘制的。[图13-11](#ch13_pdps_correlated)绘制了不相关（ <math
    alttext="rho equals 0"><mrow><mi>ρ</mi> <mo>=</mo> <mn>0</mn></mrow></math> ）和（
    <math alttext="rho equals 0.9"><mrow><mi>ρ</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>9</mn></mrow></math> ）相关特征的估计和真实PDPs，您可以轻松验证在特征相关时PDPs存在偏差。
- en: '![pdps correlated](assets/dshp_1311.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![pdps correlated](assets/dshp_1311.png)'
- en: Figure 13-11\. PDPs with correlated and uncorrelated features
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-11. 具有相关和不相关特征的PDPs
- en: Accumulated Local Effects
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 累积本地效应
- en: Accumulated local effects (ALE) is a relatively new method that takes care of
    the shortcomings of PDPs when handling correlated features. It’s also less computationally
    expensive since the number of evaluations of the trained function is smaller.^([3](ch13.html#id736))
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 累积本地效应（ALE）是一种相对较新的方法，解决了处理相关特征时PDPs的缺陷。由于训练函数的评估次数较少，它也不那么计算密集。^([3](ch13.html#id736))
- en: 'As discussed, the problem with PDPs arises from imposing unrealistic values
    of a feature given its correlation with the remaining ones, which end up biasing
    the estimates. As before, you start by creating a grid for any feature *k* under
    inspection. ALE handles this by doing three things:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PDP的问题在于，在给定其与其他特征的相关性时，施加了特征的不现实值，从而偏倚了估计值。与以前一样，您首先为正在检查的任何特征*k*创建一个网格。ALE通过以下三个步骤处理此问题：
- en: Focusing on local effects
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 关注本地效应
- en: For a given value in the grid *g*, select only those units (*i*) in your data
    for which the value of the feature falls in a neighborhood of that point ( <math
    alttext="StartSet i colon g minus delta less-than-or-equal-to x Subscript i k
    Baseline less-than-or-equal-to g plus delta EndSet"><mrow><mo>{</mo> <mi>i</mi>
    <mo>:</mo> <mi>g</mi> <mo>-</mo> <mi>δ</mi> <mo>≤</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <mo>≤</mo> <mi>g</mi> <mo>+</mo> <mi>δ</mi> <mo>}</mo></mrow></math> ). With correlated
    features, all of these units should have relatively consistent values for all
    other variables.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网格*g*中的给定值，仅选择数据中特征值落在该点邻域内的那些单位*i*（ <math alttext="StartSet i colon g minus
    delta less-than-or-equal-to x Subscript i k Baseline less-than-or-equal-to g plus
    delta EndSet"><mrow><mo>{</mo> <mi>i</mi> <mo>:</mo> <mi>g</mi> <mo>-</mo> <mi>δ</mi>
    <mo>≤</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <mo>≤</mo>
    <mi>g</mi> <mo>+</mo> <mi>δ</mi> <mo>}</mo></mrow></math> ）。对于相关特征，所有这些单位的所有其他变量的值应相对一致。
- en: Computing the slope of the function
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 计算函数的斜率
- en: Within that neighborhood, you compute the slope for each unit, and these are
    then averaged out.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在该邻域内，您计算每个单位的斜率，然后将这些斜率平均。
- en: Accumulating these effects
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 积累这些效应
- en: 'For visualization purposes, all of these effects are accumulated: this allows
    you to move from the local level of a neighborhood in the grid to the global range
    of the feature.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 出于可视化目的，所有这些效应都是累积的：这使您可以从网格中的邻域级别移动到特征的全局范围。
- en: 'The second step is quite important: instead of just evaluating the function
    on one point of the grid, you actually compute the slope of the function in the
    interval. Otherwise, you might end up confusing the effect of the feature of interest
    with that of other highly correlated features.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步非常重要：不仅要在网格的一个点上评估函数，而是在区间内计算函数的斜率。否则，你可能会把感兴趣特征的效果与其他高度相关的特征混淆。
- en: '[Figure 13-12](#ch13_ale) shows the ALE for the same simulated dataset used
    before, along with bootstrapped 90% confidence intervals. With uncorrelated features
    (first row), ALE does a great job of recovering the true effects. With correlated
    features (second row), the true effect of the second feature is recovered correctly,
    but some parts for the first feature still display some bias; nonetheless, ALE
    still does a better job than PDPs.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-12](#ch13_ale) 显示了同一模拟数据集的ALE，以及自举法得出的90%置信区间。对于不相关的特征（第一行），ALE在恢复真实效果方面表现出色。对于相关的特征（第二行），第二个特征的真实效果被正确地恢复，但第一个特征的某些部分仍显示出一些偏差；尽管如此，ALE仍然比PDP做得更好。'
- en: '![ale for data](assets/dshp_1312.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![数据的ALE](assets/dshp_1312.png)'
- en: Figure 13-12\. ALE for the same simulated data (90% CI)
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-12\. 同一模拟数据的ALE（90% CI）
- en: Key Takeaways
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: 'These are the key takeaways from this chapter:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是本章的关键要点：
- en: Holistic storytelling in ML
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的整体叙事
- en: In its most common usage, the act of storytelling in ML comes after you’ve developed
    your model and faced your stakeholders. The holistic approach presented in this
    chapter supports a vision where your scientist persona creates and iterates through
    stories that help you create a good predictive model, and then switches to the
    more traditional salesperson persona.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最常见的用法中，ML中的叙事行为发生在你开发模型并面对利益相关者之后。本章提出的整体方法支持一个视觉，即你的科学家角色创造并迭代通过故事帮助你创建一个良好预测模型，然后切换到更传统的销售人员角色。
- en: Ex ante storytelling
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Ex ante 叙事
- en: Ex ante storytelling starts by creating stories or hypotheses about what drives
    the outcome you aim to predict. These are then translated to features through
    a multistep feature engineering stage.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Ex ante 叙事从创建关于你想预测的结果驱动因素的故事或假设开始。然后通过多步特征工程阶段将它们转化为特征。
- en: Ex post storytelling
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Ex post 叙事
- en: Ex post storytelling helps you understand and interpret the predictions coming
    from your model. Techniques like heatmaps, partial dependence plots, and accumulated
    local effects should help you tell a story about the role that different features
    have on your outcome. Feature importance provides a way to rank them.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Ex post 叙事帮助你理解和解释模型预测的结果。像热图、偏依赖图和累积局部效应这样的技术应该帮助你讲述关于不同特征在结果中的作用的故事。特征重要性提供了一种排名它们的方式。
- en: Structure the storytelling into steps
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 将叙事结构化为步骤
- en: At least at the beginning, it’s good to put some structure on your storytelling
    toolkit, both from an ex ante and ex post point of view.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 至少在开始阶段，从前瞻和事后叙事的角度来看，为你的叙事工具箱设置一些结构是很有益的。
- en: Further Reading
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: I discuss first- and second-order effects in *Analytical Skills for AI and Data
    Science*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我在*《AI和数据科学的分析技能》*中讨论了一阶和二阶效应。
- en: Rolf Dobelli’s *The Art of Thinking Clearly* (Harper) is good if you want to
    gain some knowledge of the many biases and heuristics that are present in human
    behavior. These can greatly enrich the set of hypotheses for your specific problem.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Rolf Dobelli的*《清晰思考的艺术》*（Harper）非常适合希望了解人类行为中存在的许多偏见和启发式的人。这些可以极大丰富你特定问题的假设集。
- en: On the problem of feature engineering, from a data transformation point of view,
    there are several comprehensive references out there. You can check out Alice
    Zheng and Amanda Casari’s *Feature Engineering for Machine Learning* (O’Reilly),
    Sinan Ozdemir’s *Feature Engineering Bookcamp* (Manning), Soledad Galli’s *Python
    Feature Engineering Cookbook*, 2nd ed. (Packt Publishing), or Wing Poon’s [“Feature
    Engineering for Machine Learning”](https://oreil.ly/Zg3EI) series of blog posts.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征工程的问题，从数据转换的角度来看，有几本全面的参考书。你可以查看Alice Zheng和Amanda Casari的*《机器学习特征工程》*（O’Reilly），Sinan
    Ozdemir的*《特征工程训练营》*（Manning），Soledad Galli的*《Python特征工程食谱》*，第二版（Packt Publishing），或者Wing
    Poon的[“机器学习特征工程”](https://oreil.ly/Zg3EI) 系列博文。
- en: I adapted [Figure 13-5](#ch13_tradeoff_inter) from Figure 2.7 in *An Introduction
    to Statistical Learning with Applications in R*, 2nd ed. by Gareth James et al.
    (Springer) and available [online from the authors](https://oreil.ly/LZPDX). This
    book is highly recommended if you’re more interested in gaining some intuition
    than in understanding the more technical details.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我从Gareth James等人的《统计学习导论及其在R中的应用》第二版（Springer）中的图2.7改编了[图13-5](#ch13_tradeoff_inter)，并且作者[在线提供](https://oreil.ly/LZPDX)。如果您更感兴趣于获得直觉而非理解更多技术细节，强烈推荐这本书。
- en: 'On ML interpretability, I highly recommend Christoph Molnar’s *Interpretable
    Machine Learning: A Guide for Making Black Box Models Explainable* ([available
    online](https://oreil.ly/FujJr), independently published, 2023). Trevor Hastie
    et al., *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*,
    2nd ed. (Springer), has an excellent discussion on feature importance and interpretability
    for different algorithms (in particular, sections 10.13 and 15.13.2). Finally,
    Michael Munn and David Pitman give a very comprehensive and up-to-date overview
    of the different techniques in *Explainable AI for Practitioners: Designing and
    Implementing Explainable ML Solutions* (O’Reilly).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习可解释性方面，我强烈推荐Christoph Molnar的《可解释机器学习：使黑盒模型可解释的指南》（[在线获取](https://oreil.ly/FujJr)，独立出版，2023年）。Trevor
    Hastie等人的《统计学习基础：数据挖掘、推断与预测》第二版（Springer）在不同算法的特征重要性和可解释性方面有出色的讨论（特别是第10.13节和第15.13.2节）。最后，Michael
    Munn和David Pitman在《面向实践者的可解释AI：设计和实施可解释的ML解决方案》（O’Reilly）中对不同技术提供了非常全面和最新的概述。
- en: On ALEs, you can check the original article by Daniel W. Apley and Jingyu Zhu,
    “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning
    Models” (August 2019, retrieved from [arXiv](https://oreil.ly/gbZlu)). Molnar’s
    account on ALE is very good, but this article can provide some further details
    into a somewhat less intuitive algorithm.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ALEs，您可以查阅Daniel W. Apley和Jingyu Zhu的原始文章“Visualizing the Effects of Predictor
    Variables in Black Box Supervised Learning Models”（2019年8月，从[arXiv](https://oreil.ly/gbZlu)检索得到）。Molnar在ALE方面的描述非常好，但这篇文章可以提供一些更少直觉算法的详细信息。
- en: ^([1](ch13.html#id720-marker)) It’s easy to show that in linear regression,
    rescaling a feature <math alttext="x"><mi>x</mi></math> to <math alttext="k x"><mrow><mi>k</mi>
    <mi>x</mi></mrow></math> changes the true coefficient from <math alttext="alpha"><mi>α</mi></math>
    to <math alttext="alpha slash k"><mrow><mi>α</mi> <mo>/</mo> <mi>k</mi></mrow></math>
    .
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.html#id720-marker)) 很容易证明在线性回归中，将特征<math alttext="x"><mi>x</mi></math>重新缩放到<math
    alttext="k x"><mrow><mi>k</mi> <mi>x</mi></mrow></math>会将真实系数从<math alttext="alpha"><mi>α</mi></math>改变为<math
    alttext="alpha slash k"><mrow><mi>α</mi> <mo>/</mo> <mi>k</mi></mrow></math>。
- en: ^([2](ch13.html#id733-marker)) The implementation on the code [repo](https://oreil.ly/dshp-repo)
    provides the ICE and the PDP.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.html#id733-marker)) 代码[仓库](https://oreil.ly/dshp-repo)上的实现提供了ICE和PDP。
- en: '^([3](ch13.html#id736-marker)) At the time of writing, two Python packages
    are available that calculate ALEs: [ALEPython](https://oreil.ly/znDHe) and [alibi](https://oreil.ly/QIZkS).
    You can find my own implementation for the case of continuous features and no
    interactions in the code [repo](https://oreil.ly/dshp-repo).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.html#id736-marker)) 在写作时，有两个Python包可用于计算ALEs：[ALEPython](https://oreil.ly/znDHe)和[alibi](https://oreil.ly/QIZkS)。您可以在代码[仓库](https://oreil.ly/dshp-repo)中找到我对连续特征和无交互作用情况的实现。
