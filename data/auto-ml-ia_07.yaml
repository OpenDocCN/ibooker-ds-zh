- en: 5 Customizing the search space by creating AutoML pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 通过创建AutoML管道来定制搜索空间
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Understanding an AutoML pipeline
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解AutoML管道
- en: Customizing sequential and graph-structured AutoML pipelines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制序列和图结构化的AutoML管道
- en: Automated hyperparameter tuning and model selection with customized AutoML pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用定制AutoML管道进行自动超参数调整和模型选择
- en: Customizing the AutoML blocks in the AutoML pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制AutoML管道中的AutoML块
- en: In chapter 4, we solved a variety of problems with AutoKeras without customizing
    the search space. To recap, in AutoML, a search space is a pool of models with
    specific hyperparameter values that potentially can be built and selected by the
    tuning algorithm. In practice, you may want to use a specific ML algorithm or
    data preprocessing method to solve a problem, such as an MLP for a regression
    task. Designing and tuning a particular ML component requires tailoring the search
    space, tuning only the relevant hyperparameters while fixing some others.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们使用AutoKeras解决了各种问题，而没有自定义搜索空间。为了回顾，在AutoML中，搜索空间是一组具有特定超参数值的模型池，这些模型可能由调优算法构建和选择。在实践中，你可能想使用特定的ML算法或数据预处理方法来解决问题，例如使用MLP进行回归任务。设计和调整特定的ML组件需要定制搜索空间，仅调整相关的超参数，同时固定其他一些参数。
- en: This chapter introduces how to customize the search space based on your requirements
    and automatically discover certain kinds of deep learning solutions for different
    types of tasks. Constraining the search space can also reduce your search time,
    allowing you to achieve better results with fewer trials. You will learn how to
    customize the search space by creating both sequential and graph-structured AutoML
    pipelines. I’ll show you how to implement an AutoML pipeline with the AutoKeras
    functional API and how to use the built-in blocks in AutoKeras to conduct automated
    hyperparameter tuning and model selection for different tasks. You’ll also learn
    how to customize your own building blocks when the pre-existing blocks do not
    satisfy your need.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节介绍了如何根据您的需求定制搜索空间，并自动发现针对不同类型任务的某些深度学习解决方案。限制搜索空间还可以减少您的搜索时间，让您在更少的尝试中实现更好的结果。您将学习如何通过创建序列和图结构化的AutoML管道来定制搜索空间。我将向您展示如何使用AutoKeras功能API实现AutoML管道，以及如何使用AutoKeras内置的块进行自动超参数调整和模型选择。您还将学习当现有的块不能满足您的需求时，如何自定义自己的构建块。
- en: 5.1 Working with sequential AutoML pipelines
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 使用序列AutoML管道
- en: An ML pipeline comprises a sequence of ML components, such as the data preprocessing
    methods, the ML algorithm used for conducting the ML task, and so on. A sequential
    AutoML pipeline characterizes a search space of sequential ML pipelines. It’s
    composed of a sequence of *blocks*, each of which represents one or more ML components,
    as well as the search space of their hyperparameters. By selecting a component
    in each block and fixing its hyperparameters, the AutoML pipeline will instantiate
    an ML pipeline to be trained and evaluated on the dataset. An illustrative example
    of selecting a deep learning pipeline from the search space created by a sequential
    AutoML pipeline is shown in figure 5.1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ML管道由一系列ML组件组成，例如数据预处理方法、用于执行ML任务的ML算法等。序列AutoML管道表征了序列ML管道的搜索空间。它由一系列*块*组成，每个块代表一个或多个ML组件，以及它们的超参数搜索空间。通过在每个块中选择一个组件并固定其超参数，AutoML管道将实例化一个ML管道，并在数据集上进行训练和评估。一个从序列AutoML管道创建的搜索空间中选择深度学习管道的示例如图5.1所示。
- en: '![05-01](../Images/05-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](../Images/05-01.png)'
- en: Figure 5.1 Instantiating a deep learning pipeline with a sequential AutoML pipeline
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 使用序列AutoML管道实例化深度学习管道
- en: 'This can be thought of as a two-level search space, because in each iteration
    the search algorithm first selects the type of model and preprocessing methods
    to use and then selects their appropriate hyperparameters. If we have only one
    concerned model and processing method in the pipeline, we need to perform only
    a single-step selection of the appropriate hyperparameters. So, we can separate
    the AutoML problems that we mainly target in practice into the following two categories:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被视为一个双层搜索空间，因为在每次迭代中，搜索算法首先选择要使用的模型类型和预处理方法，然后选择它们适当的超参数。如果我们只在管道中有一个关注的模型和一种处理方法，我们只需要执行一步选择适当的超参数。因此，我们可以将我们在实践中主要针对的AutoML问题分为以下两类：
- en: '*Automated hyperparameter tuning (a general definition)*—The model type and
    preprocessing methods are fixed. We want to tune only the hyperparameters of each
    specified ML component in the pipeline. In this case, the AutoML blocks in the
    AutoML pipeline will each contain only one ML model or preprocessing method. The
    search space will include only the relevant hyperparameters for each fixed component.
    For example, suppose we want to apply an MLP to solve a regression problem and
    want to tune the model’s hyperparameters, such as the number of layers and units.
    In that case, we can constrain the search space by creating the AutoML algorithm
    block with only MLPs in it. The search space will be the feasible number of layers
    and units in the MLP model.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动化超参数调整（一般定义）*—模型类型和预处理方法是固定的。我们只想调整管道中每个指定ML组件的超参数。在这种情况下，AutoML管道中的AutoML模块将每个只包含一个ML模型或预处理方法。搜索空间将只包括每个固定组件的相关超参数。例如，假设我们想应用MLP来解决回归问题，并想调整模型超参数，如层数和单元数。在这种情况下，我们可以通过创建只包含MLP的AutoML算法模块来限制搜索空间。搜索空间将是MLP模型中可行的层数和单元数。'
- en: '*Automated pipeline search*—In some situations, we may not know which model
    or data preparation method to adopt ahead of time. We want to search not only
    the suitable models and preprocessing methods but also their hyperparameters.
    In this case, one or more of the AutoML blocks will contain multiple components.
    For example, we might want to explore both the CNN and MLP models to see which
    is more suitable for our task and find the best hyperparameters for each. To do
    this, we can include blocks for each model architecture. The preprocessing method
    can be either fixed or selected and tuned along with the model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动化管道搜索*—在某些情况下，我们可能事先不知道应该采用哪种模型或数据准备方法。我们希望搜索不仅包括合适的模型和预处理方法，还包括它们的超参数。在这种情况下，一个或多个AutoML模块将包含多个组件。例如，我们可能想要探索CNN和MLP模型，看看哪一个更适合我们的任务，并为每个模型找到最佳的超参数。为此，我们可以为每个模型架构包含模块。预处理方法可以是固定的，也可以与模型一起选择和调整。'
- en: Another category of AutoML problems, which is especially useful for serving
    shallow models, is *automated feature engineering*. It aims to automatically discover
    informative and discriminative features for learning the ML models based on certain
    feature selection criteria, such as Pearson correlations introduced in chapter
    2\. Automated feature engineering often involves an iterative feature-generation
    and feature selection process that mimics the way of manual feature engineering.
    Because a deep learning algorithm enjoys the natural talent of extracting and
    learning condensed data representations without much feature engineering operation,
    let’s first focus on the tuning of the deep learning algorithm and its data preparation
    methods in this chapter and then touch on the automated feature engineering and
    the tuning of shallow models in chapter 6.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类AutoML问题，特别是对于服务浅层模型非常有用，是*自动化特征工程*。它的目的是自动发现基于某些特征选择标准（如第2章中介绍的皮尔逊相关系数）的有信息和判别性特征，以学习ML模型。自动化特征工程通常涉及一个迭代特征生成和特征选择过程，类似于手动特征工程的方式。由于深度学习算法具有在无需大量特征工程操作的情况下提取和学习的天然才能，因此让我们首先关注本章中深度学习算法及其数据准备方法的调整，然后在第6章中简要介绍自动化特征工程和浅层模型的调整。
- en: In the next two sections, we’ll explore how to create a sequential AutoML pipeline
    with the AutoKeras functional API to solve the automated hyperparameter tuning
    and automated pipeline search in the context of deep learning. An extension of
    the sequential AutoML pipeline to a more general graph-structured pipeline will
    be introduced afterward. Beyond using the built-in AutoML blocks of AutoKeras,
    you’ll also learn how to customize your own block in the last section of this
    chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将探讨如何使用AutoKeras功能API创建一个顺序AutoML管道，以解决深度学习环境中的自动化超参数调整和自动化管道搜索。之后，我们将介绍将顺序AutoML管道扩展到更通用的图结构管道。除了使用AutoKeras的内置AutoML模块之外，你还将学习如何在本章的最后部分自定义自己的模块。
- en: Note The taxonomy of AutoML tasks can be more complex than we describe here
    and could be categorized differently. The most widely used categorization methods
    are automated data preprocessing, automated feature engineering, automated model
    selection, and automated hyperparameter tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：AutoML任务的分类可能比我们这里描述的更复杂，并且可能被分类为不同的类别。最广泛使用的分类方法包括自动数据预处理、自动特征工程、自动模型选择和自动超参数调整。
- en: 'In this book, we are considering a more general definition of automated hyperparameter
    tuning, where we take the ML model type and data preprocessing methods as special
    hyperparameters. This will unify the automated data preprocessing, automated model
    selection, and automated hyperparameter tuning into one category: automated hyperparameter
    tuning, as we’ve described in the previous sections. Of course, we can also have
    automated pipeline tuning to tune the entire ML workflow as we’ve introduced earlier.
    In particular, some work also explicitly separates the subfield of selecting and
    tuning of deep learning algorithms as *automated deep learning*, considering the
    complexity of designing the neural network architectures (or we call it *neural
    architecture search*).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们考虑了自动超参数调整的更广义定义，其中我们将机器学习模型类型和数据预处理方法视为特殊超参数。这将统一自动数据预处理、自动模型选择和自动超参数调整为一个类别：自动超参数调整，正如我们在前面的章节中所描述的。当然，我们也可以有自动管道调整，以调整整个机器学习工作流程，正如我们之前所介绍的。特别是，一些工作还明确地将选择和调整深度学习算法的子领域作为*自动深度学习*，考虑到设计神经网络架构的复杂性（或者我们称之为*神经架构搜索*）。
- en: 5.2 Creating a sequential AutoML pipeline for automated hyperparameter tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 创建用于自动超参数调整的顺序AutoML管道
- en: In this section, I’ll show you how to create an AutoML pipeline to do automated
    hyperparameter tuning. Creating an AutoML pipeline with the AutoKeras functional
    API is quite similar to building up a neural network with the Keras functional
    API, as introduced in chapter 3\. The only difference is that the Keras layers
    are replaced with AutoKeras’s built-in AutoML blocks. Each block contains one
    or more deep learning models (or preprocessing methods) and a default search space
    for their hyperparameters. You can also modify the search space for each hyperparameter.
    To build up a network, we stack multiple Keras layers by wiring together their
    inputs and outputs sequentially. Comparably, to form a sequential AutoML pipeline,
    we select AutoKeras blocks and wire them together one by one, as shown in figure
    5.2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何创建一个AutoML管道以进行自动超参数调整。使用AutoKeras功能API创建AutoML管道与第3章中介绍的用Keras功能API构建神经网络非常相似。唯一的区别是Keras层被AutoKeras的内置AutoML块所取代。每个块包含一个或多个深度学习模型（或预处理方法）以及它们超参数的默认搜索空间。您还可以修改每个超参数的搜索空间。为了构建网络，我们通过按顺序连接它们的输入和输出堆叠多个Keras层。相应地，为了形成一个顺序AutoML管道，我们选择AutoKeras块并逐个连接它们，如图5.2所示。
- en: '![05-02](../Images/05-02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](../Images/05-02.png)'
- en: Figure 5.2 A sequential AutoML pipeline created with the AutoKeras functional
    API
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 使用AutoKeras功能API创建的顺序AutoML管道
- en: 'The pipeline should start with an input placeholder indicating the data type,
    such as images or text, and end with an output head corresponding to the task
    we want to solve, such as classification or regression. The two intermediate blocks
    are AutoML blocks characterizing the search space of the preprocessing methods
    and deep learning models. Let’s take a closer look at the components (blocks)
    in the pipeline, described next:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 管道应该从一个表示数据类型（如图像或文本）的输入占位符开始，并以一个对应于我们想要解决的问题的任务（如分类或回归）的输出头结束。两个中间块是表征预处理方法和深度学习模型搜索空间的AutoML块。让我们更详细地看看管道中的组件（块），如下所述：
- en: The *input node* is a placeholder for the tensor input of the pipeline, such
    as ImageInput,TextInput, or StructuredDataInput (as introduced in chapter 4).
    You can also define a general tensor input with the Input class in AutoKeras.
    The input node accepts data in multiple formats, such as NumPy arrays, pandas
    DataFrames, and TensorFlow Datasets. It will also conduct certain preprocessing
    operations automatically, such as extending the dimensions of images if they do
    not have a channel dimension. The input node does not have any hyperparameters
    that can be set or tuned.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入节点*是管道张量输入的占位符，例如图像输入（ImageInput）、文本输入（TextInput）或结构化数据输入（StructuredDataInput）（如第4章所述）。您还可以使用AutoKeras中的Input类定义一个通用的张量输入。输入节点接受多种格式的数据，如NumPy数组、pandas
    DataFrame和TensorFlow数据集。它还将自动执行某些预处理操作，例如，如果图像没有通道维度，则扩展图像的维度。输入节点没有可以设置或调整的超参数。'
- en: The *preprocessor block* defines additional preprocessing operations (i.e.,
    if certain operations have already been conducted by the input node, as mentioned
    previously) to perform on the inputs, such as image normalization, text embedding,
    and so on. Depending on the operation, we may have hyperparameters to tune, such
    as the maximum size of the vocabulary table to use to convert text documents to
    their vector representations if text embedding is performed. In this block, there
    are no weights to be trained through backpropagation.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理块*定义了在输入上执行额外的预处理操作（即如果如前所述，某些操作已经由输入节点执行），例如图像归一化、文本嵌入等。根据操作，我们可能有一些超参数需要调整，例如，如果执行文本嵌入，则用于将文本文档转换为它们的向量表示的词汇表的最大大小。在这个块中，没有通过反向传播训练的权重。'
- en: The *network block* is the most important type of AutoML block in AutoKeras.
    Each block represents a set of neural network models of the same structure. For
    example, a ConvBlock, which you’ll see in this section, encompasses a set of convolutional
    neural networks (CNNs). Each CNN is composed of convolutional layers and pooling
    layers. The number and types of layers are treated as hyperparameters. You can
    select one or more network blocks to create the pipelines based on the task at
    hand and specify the search space of their hyperparameters based on your requirements.
    Unlike the preprocessor block, there are weights to be trained through backpropagation
    after specifying the hyperparameters in the network block.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络块*是AutoKeras中最重要的一种AutoML块。每个块代表一组具有相同结构的神经网络模型。例如，在本节中您将看到的ConvBlock，包含一组卷积神经网络（CNNs）。每个CNN由卷积层和池化层组成。层数和类型被视为超参数。您可以选择一个或多个网络块来创建基于当前任务的管道，并根据您的需求指定其超参数的搜索空间。与预处理块不同，在指定网络块中的超参数后，将通过网络反向传播训练权重。'
- en: The *output head* is a task-specific component used to generate the final outputs,
    such as the ClassificationHead and RegressionHead introduced in the discussion
    of the IO API in chapter 4\. It reshapes each instance’s representation to a vector
    and applies a dense layer to transform it to the size of the target output. For
    example, if the head is a ClassificationHead and the problem is a multi-class
    classification problem, the output of each instance from the dense layer will
    be a vector of length 10 corresponding to the ten labels. Each head also specifies
    the loss function and metrics to help compile each deep learning pipeline selected
    from the search space for training.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出头*是一个用于生成最终输出的特定任务组件，例如在第4章讨论IO API时引入的分类头（ClassificationHead）和回归头（RegressionHead）。它将每个实例的表示重塑为向量，并应用密集层将其转换为目标输出的大小。例如，如果头是分类头且问题是多类分类问题，那么从密集层输出的每个实例将是一个长度为10的向量，对应于十个标签。每个头还指定了损失函数和度量标准，以帮助编译从搜索空间中选择的每个深度学习管道进行训练。'
- en: In the rest of this section, we will walk through two hyperparameter tuning
    examples using a sequential AutoML pipeline. The examples will also introduce
    several built-in AutoML blocks in AutoKeras that can be used to create an AutoML
    pipeline.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将通过使用顺序AutoML管道，逐步介绍两个超参数调整示例。这些示例还将介绍AutoKeras中可以用于创建AutoML管道的几个内置AutoML块。
- en: 5.2.1 Tuning MLPs for structured data regression
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 调整结构化数据回归的MLP
- en: 'Our first task will be to tune the network structure of an MLP to solve a structured
    data regression problem. In chapter 3, we solved the California housing price-prediction
    problem by creating an MLP with Keras. We tuned the number of training epochs
    by observing the MSE curves for the training and validation sets during the training
    process. Here, we’ll use AutoML to tune the structural hyperparameters of the
    MLP: the number of layers and the number of units in each layer. An intuitive
    way of doing this is to create multiple MLPs with different numbers of layers
    and units, train them, and select the best one based on the validation MSE. This
    process can be done by creating a sequential AutoML pipeline without manually
    creating and exploring multiple MLPs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是调整一个多层感知器（MLP）的网络结构，以解决结构化数据的回归问题。在第3章中，我们通过使用Keras创建了一个MLP来解决加利福尼亚房价预测问题。我们在训练过程中通过观察训练集和验证集的均方误差（MSE）曲线来调整训练的轮数。在这里，我们将使用自动机器学习（AutoML）来调整MLP的结构超参数：层数和每层的单元数。一个直观的方法是创建多个具有不同层数和单元数的MLP，对它们进行训练，并根据验证集的MSE选择最佳的一个。这个过程可以通过创建一个顺序的AutoML管道来完成，而不需要手动创建和探索多个MLP。
- en: 'To create the AutoML pipeline, we can leverage two of AutoKeras’s built-in
    AutoML blocks as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建AutoML管道，我们可以利用AutoKeras的两个内置AutoML模块，如下所示：
- en: Normalization is a preprocessor block that performs featurewise normalization
    by subtracting the means of the features and dividing by their standard deviations.
    We used this operation in chapter 3 to normalize the features for the California
    housing price data. This block helps preprocess the data for the MLPs. It does
    not contain any hyperparameters to be tuned.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化是一个预处理块，它通过减去特征的平均值并除以它们的标准差来执行特征归一化。我们在第3章中使用这个操作来归一化加利福尼亚房价数据的特征。这个块有助于预处理MLP的数据。它不包含任何需要调整的超参数。
- en: 'DenseBlock is a network block forming a search space of models with the MLP
    structure. Unlike the simplest MLP, which only stacks dense layers with certain
    activation functions, each “layer” (or *cell* ) in the DenseBlock is a combination
    of three Keras layers: a dense layer, a dropout layer to help mitigate the overfitting
    issue, and a *batch normalization layer*, which normalizes the input tensor of
    a batch of instances to a mean of 0 and a standard deviation of 1\. The batch
    normalization layer is added between a dense layer without an activation and a
    ReLU activation layer. Whether to use the batch normalization layer is a hyperparameter
    to be tuned. The dropout layer is added at the end (as shown in figure 5.3). The
    number of dense layers, the number of units in each dense layer, and the dropout
    rate (ranging from 0 to 1) are also hyperparameters to be tuned in this block,
    if not fixed. The default choices for the number of layers are in the range 1
    to 3, and the default choices for the number of units are in the list [16, 32,
    64, 128, 256, 512, 1024].'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseBlock是一个网络块，它形成了一个具有MLP结构的模型搜索空间。与最简单的MLP不同，它只堆叠了具有特定激活函数的密集层，DenseBlock中的每个“层”（或*单元*）是三个Keras层的组合：一个密集层、一个dropout层以帮助减轻过拟合问题，以及一个*批归一化层*，该层将一批实例的输入张量归一化到均值为0和标准差为1。批归一化层被添加在一个没有激活函数的密集层和一个ReLU激活层之间。是否使用批归一化层是一个需要调整的超参数。dropout层被添加在最后（如图5.3所示）。密集层的数量、每个密集层的单元数以及dropout率（范围从0到1）也是在这个块中需要调整的超参数，除非它们被固定。层数的默认选择范围在1到3之间，单元数的默认选择列表为[16,
    32, 64, 128, 256, 512, 1024]。
- en: '![05-04](../Images/05-03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../Images/05-03.png)'
- en: Figure 5.3 A cell in the DenseBlock
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 DenseBlock中的一个单元
- en: As you can see in listing 5.1, we stack the two blocks to form a structured
    data regression pipeline with the structure shown in figure 5.2\. We’ll use this
    pipeline to find a good MLP structure for the California housing price-prediction
    problem. The output head is defined as a RegressionHead, which generates the final
    prediction by applying a linear transformation to its input. By default, a dropout
    layer exists before the final linear transformation in the output head. We remove
    it by fixing the dropout rate at 0 for simplicity. We also remove the batch normalization
    layer by setting the use_batchnorm argument to False. Besides the two hyperparameters
    (number of layers and number of units) in the DenseBlock, the search space also
    contains two hyperparameters for the optimization algorithm, which are the algorithm’s
    type and the learning rate. By tuning them jointly with the MLP structure, we
    can achieve more accurate performance for different pipelines, making it easier
    for us to compare and select among them. The number of trials is set to 10 in
    the last line, which means we select 10 different pipelines from the search space
    in total and choose the best one among them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表 5.1 所示，我们将两个块堆叠起来形成一个结构化数据回归管道，其结构如图 5.2 所示。我们将使用此管道来寻找加利福尼亚房价预测问题的良好 MLP
    结构。输出头被定义为 RegressionHead，它通过对其输入应用线性变换来生成最终预测。默认情况下，输出头中在最终线性变换之前存在一个 dropout
    层。我们通过将 dropout 率固定为 0 来简化它。我们还通过将 use_batchnorm 参数设置为 False 来移除批归一化层。除了 DenseBlock
    中的两个超参数（层数和单元数）之外，搜索空间还包含两个优化算法的超参数，即算法的类型和学习率。通过将它们与 MLP 结构联合调整，我们可以为不同的管道实现更精确的性能，这使得我们更容易比较和选择它们。最后一行将试验次数设置为
    10，这意味着我们从搜索空间中选择总共 10 个不同的管道，并从中选择最佳的一个。
- en: Listing 5.1 Creating an AutoML pipeline with MLPs for structured data regression
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 使用 MLP 为结构化数据回归创建 AutoML 管道
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Creates an input placeholder for structured data
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为结构化数据创建输入占位符
- en: ❷ Stacks a normalization preprocessor block on top of the input node
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在输入节点上方堆叠一个归一化预处理块
- en: ❸ Adds an AutoML block for tuning the MLP structure
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加用于调整 MLP 结构的 AutoML 块
- en: ❹ Adds a regression output head at the end of the pipeline
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在管道末尾添加回归输出头
- en: ❺ Forms an AutoML pipeline and defines the number of search trial
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 形成AutoML管道并定义搜索试验次数
- en: Now we load the data with scikit-learn and conduct the search process by feeding
    the data to it (as shown in listing 5.2). The batch size and maximum number of
    epochs to train for are fixed at 1,024 and 150, respectively, to help reduce the
    search time. Generally, a larger batch size and fewer epochs will reduce the time
    it takes to train a network. This is a naive way of accelerating the search speed
    in AutoML work—it is not guaranteed that each pipeline explored will be able to
    converge within 150 epochs—but we’ll assume this is long enough to give an indication
    of how they perform and allow us to discriminate among them. More ways of accelerating
    the speed, even without harnessing the evaluation performance of each pipeline,
    will be introduced in chapter 8.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 scikit-learn 加载数据，并通过将其输入到其中（如列表 5.2 所示）来进行搜索过程。批大小和训练的最大轮数分别固定为 1,024
    和 150，以帮助减少搜索时间。一般来说，较大的批大小和较少的轮数将减少训练网络所需的时间。这是一种在 AutoML 工作中加速搜索速度的朴素方法——不能保证每个探索的管道都能在
    150 个轮次内收敛——但我们假设这已经足够长，可以给出它们性能的指示，并允许我们区分它们。在第 8 章中还将介绍更多加速速度的方法，即使不利用每个管道的评估性能。
- en: Listing 5.2 Tuning the MLP for structured data regression
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 调整 MLP 以进行结构化数据回归
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loads the dataset, packs it into a pandas DataFrame, and splits off 20% for
    testing
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集，将其打包到 pandas DataFrame 中，并分割出 20% 用于测试
- en: ❷ Fits the AutoML pipeline with the data
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用数据拟合 AutoML 管道
- en: As shown in listing 5.3, the final testing MSE of the best MLP is 0.28—better
    than that of the one we designed in chapter 3 (MSE = 0.31). We can display the
    hyperparameters of the MLP with the results_summary() method; it has two layers
    with 32 and 512 units, respectively. Its validation MSE during the search process
    is 0.29.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表 5.3 所示，最佳 MLP 的最终测试 MSE 为 0.28——优于我们在第 3 章中设计的模型（MSE = 0.31）。我们可以使用 results_summary()
    方法显示 MLP 的超参数；它有两个层，分别有 32 和 512 个单元。其在搜索过程中的验证 MSE 为 0.29。
- en: Listing 5.3 Evaluating the best deep learning pipeline
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 评估最佳深度学习管道
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Evaluates the best MLP
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 评估最佳 MLP
- en: ❷ Summarizes the best trial during search
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 总结搜索过程中的最佳试验
- en: ❸ Exports the best MLP
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 导出最佳 MLP
- en: ❹ Visualizes the best MLP
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 可视化最佳MLP
- en: 'Because the exported best model is a Keras model, you can easily save and load
    your best model as shown here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于导出的最佳模型是一个Keras模型，你可以像这里所示一样轻松地保存和加载你的最佳模型：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We export the best MLP and visualize its structure, shown in figure 5.4\. Each
    of its layers can be associated with the corresponding component in the sequential
    AutoML pipeline. For example, the two dense layers with ReLU activation are selected
    from the search space characterized in the DenseBlock.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导出最佳MLP并可视化其结构，如图5.4所示。它的每一层都可以与顺序AutoML管道中的相应组件相关联。例如，具有ReLU激活的两个密集层是从DenseBlock中定义的搜索空间中选择的。
- en: '![05-04](../Images/05-04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../Images/05-04.png)'
- en: Figure 5.4 The best discovered MLP structure and the corresponding component
    in the AutoML pipeline
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 最佳发现的MLP结构和AutoML管道中的相应组件
- en: 'Because we use the default search space for the number of layers ([1, 2, 3])
    and units ([16, 32, 64, 128, 256, 512, 1024]) in the DenseBlock, the total number
    of different MLP structures is 7 + 7² + 7³ = 399\. This is a fairly large search
    space compared to the 10 trials we performed, indicating that we won’t have tested
    many of the possibilities. The search space is even larger if we take the optimization
    algorithms (three choices by default) and learning rates (six choices by default)
    into account. And by extension, by testing only 10 options, we’re relatively unlikely
    to have hit upon the top ones out of all the possibilities. To help constrain
    the search space, we can fix some of the hyperparameters or constrain their scope
    manually. For example, we can draw on past experience in tuning MLPs to constrain
    the number of layers and units. Here are a few assumptions we can base our changes
    on:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用默认的层数搜索空间([1, 2, 3])和DenseBlock中的单元数([16, 32, 64, 128, 256, 512, 1024])，因此不同的MLP结构总数为7
    + 7² + 7³ = 399。与我们所进行的10次试验相比，这是一个相当大的搜索空间，这意味着我们可能没有测试到许多可能性。如果我们考虑优化算法（默认有三种选择）和学习率（默认有六种选择），搜索空间将更大。并且通过扩展，通过仅测试10个选项，我们相对不太可能从所有可能性中找到最好的一个。为了帮助约束搜索空间，我们可以手动固定一些超参数或限制它们的范围。例如，我们可以借鉴过去调整MLP的经验来约束层数和单元数。以下是一些我们可以基于我们的更改的假设：
- en: Because the dataset in this example is small, an MLP with fewer layers should
    be likely to have enough capacity to learn the data and avoid overfitting.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于本例中的数据集较小，具有较少层的MLP应该有足够的容量来学习数据并避免过拟合。
- en: MLPs with triangular or diamond structures often perform better than ones with
    rectangular structures. Using a three-layer MLP as an example, the units in an
    MLP with a triangular structure could be [32, 64, 128] or [128, 64, 32]. Two MLPs
    with diamond and rectangular structures could have units [32, 64, 32] and [32,
    32, 32], respectively. An illustration of these three types of structures is shown
    in figure 5.5.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有三角形或菱形结构的MLP通常比具有矩形结构的MLP表现更好。以三层MLP为例，具有三角形结构的MLP中的单元可以是[32, 64, 128]或[128,
    64, 32]。具有菱形和矩形结构的两个MLP分别可以有单元[32, 64, 32]和[32, 32, 32]。这些三种类型的结构的示意图如图5.5所示。
- en: '![05-05](../Images/05-05.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![05-05](../Images/05-05.png)'
- en: Figure 5.5 Three types of MLP structures
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 三种MLP结构
- en: We can fix the number of layers to be two in this example and constrain the
    numbers of units in the two layers to be selected from [128, 256, 512, 1024] and
    [16, 32, 64], respectively. This will help form a search space of inverted triangular
    MLP structures. The constraints can be implemented by connecting two DenseBlocks
    and defining the choices for the units in each layer, as shown in listing 5.4\.
    KerasTuner provides a hyperparameters module (hp for short) to help create the
    search space for both continuous and discrete hyperparameters. For example, because
    the number of units is a discrete hyperparameter, we can use the hyperparameters.Choice
    class in the module to specify a list of possible values for that hyperparameter.
    You’ll see more uses of this class in chapter 6 when designing your own AutoML
    block.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以将层数固定为两层，并将两个层的单元数限制为分别从[128, 256, 512, 1024]和[16, 32, 64]中选择。这将有助于形成一个倒三角形MLP结构的搜索空间。可以通过连接两个DenseBlock并定义每层的单元选择来实现这些约束，如列表5.4所示。KerasTuner提供了一个超参数模块（简称hp）来帮助创建连续和离散超参数的搜索空间。例如，由于单元数是一个离散超参数，我们可以使用模块中的hyperparameters.Choice类来指定该超参数的可能值的列表。你将在第6章中看到这个类在设计和自己的AutoML块时的更多用途。
- en: Listing 5.4 Customizing the search space for tuning the MLP
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 调整MLP的搜索空间
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Customizes the search space of the units hyperparameter in the dense layers
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在密集层中自定义单元超参数的搜索空间
- en: The new search space only has 12 different MLP structures. We use a similar
    method to search for, retrieve, and evaluate the best MLP; this time, the best
    MLP discovered in 10 trials achieves a testing MSE of 0.27, which beats the previous
    MLP discovered in the larger search space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '新的搜索空间只有12种不同的MLP结构。我们使用类似的方法来搜索、检索和评估最佳的MLP；这次，在10次试验中发现的最佳MLP在测试中达到了0.27的均方误差，这优于在更大搜索空间中发现的先前MLP。 '
- en: Note The construction of the search space often plays a vital role in the success
    of AutoML. A good search space can help you discover a promising pipeline with
    less search time. Designing a good search space can be even more important than
    designing a good search algorithm, because it provides cheap constraints to accelerate
    the search process. However, it often requires prior knowledge and understanding
    of the models as well as the coupled search algorithm, which runs counter to the
    ultimate goal of AutoML (saving human effort). If you don’t have prior knowledge
    to build on, you can start with a large search space and gradually decrease its
    size with a trial-and-error approach. This idea also motivates some advanced AutoML
    algorithms that aim to gradually tailor the search space or narrow it down to
    a finer region. We will introduce some representative examples in chapter 7.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：搜索空间的构建在AutoML的成功中起着至关重要的作用。一个好的搜索空间可以帮助你在更短的时间内发现一个有希望的管道。设计一个好的搜索空间甚至可能比设计一个好的搜索算法更重要，因为它提供了廉价的约束以加速搜索过程。然而，这通常需要先验知识以及对模型以及耦合的搜索算法的理解，这与AutoML的最终目标（节省人力）相悖。如果你没有先验知识可以依赖，你可以从一个大的搜索空间开始，并通过试错法逐渐减小其大小。这个想法也激励了一些高级AutoML算法，这些算法旨在逐步调整搜索空间或将其缩小到一个更精细的区域。我们将在第7章介绍一些代表性的例子。
- en: 'Now that you’ve seen how to tune MLPs for a structured data regression task,
    let’s take a look at another example: tuning CNNs for an image classification
    task.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了如何调整MLPs以适应结构化数据回归任务，让我们看看另一个例子：调整CNN以适应图像分类任务。
- en: 5.2.2 Tuning CNNs for image classification
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 调整CNN以进行图像分类
- en: In this example, we’ll tune a CNN with a sequential AutoML pipeline to solve
    an image classification problem using the MNIST dataset. In chapter 3, we created
    a CNN and showed that it achieved better performance on this task than an MLP
    network. But we didn’t explore how we set up and tuned the hyperparameters in
    the CNN, such as the number of filters in the convolutional layers. Let’s now
    build up an AutoML pipeline to improve the CNN structure and achieve better classification
    accuracy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用序列AutoML管道调整CNN，以使用MNIST数据集解决图像分类问题。在第3章中，我们创建了一个CNN，并展示了它在这一任务上的性能优于MLP网络。但我们没有探讨如何设置和调整CNN中的超参数，例如卷积层中的过滤器数量。现在，让我们构建一个AutoML管道来改进CNN结构，以实现更好的分类精度。
- en: 'We use a ConvBlock in AutoKeras to tune the three main hyperparameters of the
    CNN: the number of filters, the number of convolutional layers, and the kernel
    size of the convolutional layers. A ConvBlock sequentially stacks multiple *convolutional
    blocks* (or *convolutional cells*). Each convolutional block sequentially stacks
    multiple convolutional layers, a max pooling layer, and a dropout layer (see figure
    5.6).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在AutoKeras中使用ConvBlock调整CNN的三个主要超参数：过滤器的数量、卷积层的数量以及卷积层的核大小。ConvBlock按顺序堆叠多个*卷积块*（或*卷积单元*）。每个卷积块按顺序堆叠多个卷积层、一个最大池化层和一个dropout层（见图5.6）。
- en: '![05-06](../Images/05-06.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![05-06](../Images/05-06.png)'
- en: Figure 5.6 The structure of each convolutional block in a ConvBlock
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 ConvBlock中每个卷积块的结构
- en: 'All the convolutional blocks have the same number of convolutional layers,
    but each layer can contain a different number of filters. The search space of
    a ConvBlock has the following seven hyperparameters:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的卷积块都有相同数量的卷积层，但每一层可以包含不同数量的过滤器。ConvBlock的搜索空间具有以下七个超参数：
- en: '*Number of convolutional blocks*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积块的数量*'
- en: '*Number of convolutional layers in each block*—This is the same in all the
    convolutional blocks.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个块中的卷积层数量*——这在所有卷积块中都是相同的。'
- en: '*Type of the convolutional layer*—Each convolutional layer can be one of two
    types: it can be a regular 2-D convolutional layer, as introduced in chapter 3,
    or a *separable convolutional layer*, which contains fewer weights than a normal
    convolutional layer but may achieve comparable performance. A more detailed explanation
    of this layer type will be provided when we discuss the XceptionBlock in the next
    section.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积层的类型*——每个卷积层可以是两种类型之一：它可以是一个常规的2-D卷积层，如第3章中介绍的那样，或者是一个*可分离卷积层*，它比常规卷积层包含更少的权重，但可能实现相当的性能。在下一节讨论XceptionBlock时，将提供关于此层类型的更详细解释。'
- en: '*Number of filters in the convolutional layer*—This can be different for each
    layer of each block.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积层中的滤波器数量*——每个块的每个层的滤波器数量可能不同。'
- en: '*Kernel size of the convolutional layer*—The kernel size of the max pooling
    layers is set to be the kernel size minus one. Once the kernel size is selected
    by the tuning algorithm for a ConvBlock in a trial, it will be applied for every
    pooling layer and convolutional layer in all the cells of that ConvBlock.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积层的核大小*——最大池化层的核大小设置为核大小减一。一旦在试验中为ConvBlock选择核大小，它将应用于该ConvBlock中所有单元的所有池化层和卷积层。'
- en: '*Whether to apply the max pooling layer in each cell*—Once this is selected
    for a trial, it’s applied for every cell in the ConvBlock.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*是否在每个单元中应用最大池化层*——一旦为试验选择，它将应用于ConvBlock中的每个单元。'
- en: '*Whether to apply the dropout layer in each cell*—Once this is selected for
    a trial, it’s applied for every cell in the ConvBlock.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*是否在每个单元中应用dropout层*——一旦为试验选择，它将应用于ConvBlock中的每个单元。'
- en: To keep this example simple, we’ll constrain the search space by fixing the
    number of blocks at two, as shown in listing 5.5\. We do not apply the dropout
    layer or use separable convolutional layers. The hyperparameters to be tuned are
    the number of layers, the kernel size, and the number of filters in each layer
    in the blocks. By default, they are selected from the lists [1, 2], [3, 5, 7],
    and [16, 32, 64, 128, 256, 512], respectively. Without considering the optimizer
    and learning rate, we have 3 * (6 + 6 * 6) * (6 + 6 * 6) = 5,292 different CNN
    structures in this search space.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个例子简单，我们将通过将块的数量固定为两个来限制搜索空间，如列表5.5所示。我们不应用dropout层或使用可分离卷积层。要调整的超参数包括块中的层数、核大小以及每层的滤波器数量。默认情况下，它们分别从列表[1,
    2]、[3, 5, 7]和[16, 32, 64, 128, 256, 512]中选择。不考虑优化器和学习率，在这个搜索空间中有5,292种不同的CNN结构。
- en: Note This code example may take a long time to run.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此代码示例可能需要很长时间才能运行。
- en: Listing 5.5 MNIST classification with the AutoKeras functional API
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5 使用AutoKeras功能API进行MNIST分类
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Loads the MNIST data
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载MNIST数据
- en: ❷ Creates the input node
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建输入节点
- en: ❸ Adds a Normalization preprocessor block
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个归一化预处理块
- en: ❹ Stacks a ConvBlock to create a search space for CNNs
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将ConvBlock堆叠以创建CNN的搜索空间
- en: ❺ Finalizes the pipeline with a classification head
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用分类头最终确定管道
- en: ❻ Wraps up the pipeline into an AutoModel
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将管道封装为AutoModel
- en: ❼ Executes the search process by fitting the training data to the pipeline
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 通过将训练数据拟合到管道来执行搜索过程
- en: ❽ Evaluates the best CNN on the test set
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在测试集上评估最佳卷积神经网络
- en: ❾ Exports the best CNN and prints out its structure
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 导出最佳卷积神经网络并打印其结构
- en: The best CNN achieves 99.38% accuracy on the test set, which decreases the error
    rate of the CNN we manually designed in chapter 3 by over 30%. However, the size
    of the network is larger, mainly due to the large number of filters. To discover
    smaller architectures, we can limit the number of layers and filters in the search
    space. It is possible to find a smaller architecture with comparable performance
    to the CNN we constructed here; I’ll leave that as an exercise for you to try
    out.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳卷积神经网络在测试集上实现了99.38%的准确率，这降低了我们在第3章中手动设计的CNN的错误率超过30%。然而，网络的尺寸更大，主要是由于滤波器数量众多。为了发现更小的架构，我们可以限制搜索空间中的层数和滤波器数量。有可能找到具有与这里构建的CNN相当性能的更小架构；我将把这个作为练习留给你尝试。
- en: 5.3 Automated pipeline search with hyperblocks
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 使用超块进行自动管道搜索
- en: 'In this section, we’ll talk about another commonly faced scenario in AutoML
    applications: selecting the best types of components (models or preprocessors)
    to use in the deep learning pipeline. This is a more complex scenario than tuning
    only the hyperparameters of a specific type of model, as introduced in the previous
    section, because different models and preprocessors may comprise different operations
    and have unique hyperparameters. It requires us to jointly select the combination
    of preprocessors and models and their coupled hyperparameters. For example, in
    image classification, a lot of advanced models are proposed beyond the naive CNN
    we used previously, such as ResNet, Xception, and so on. Even if you have heard
    of these models, you may not know how they work, which tasks they are best used
    for, or how to tune them. You’ll also need to decide on suitable preprocessing
    methods, such as choosing whether to use normalization. We’ll work through some
    image classification examples here to show you how to automatically select models
    and preprocessing methods.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论AutoML应用中经常遇到的另一个场景：选择在深度学习管道中使用最佳类型的组件（模型或预处理程序）。这比上一节中仅调整特定类型模型的超参数更为复杂，因为不同的模型和预处理程序可能包含不同的操作和独特的超参数。它要求我们共同选择预处理程序和模型的组合及其耦合的超参数。例如，在图像分类中，除了我们之前使用的原始CNN之外，还提出了许多高级模型，如ResNet、Xception等。即使你已经听说过这些模型，你可能也不知道它们是如何工作的，它们最适合哪些任务，或者如何调整它们。你还需要决定合适的预处理方法，例如选择是否使用归一化。在这里，我们将通过一些图像分类示例来展示如何自动选择模型和预处理方法。
- en: 5.3.1 Automated model selection for image classification
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 图像分类的自动模型选择
- en: The CNN model introduced in chapter 3, which stacks convolutional and pooling
    layers recursively, is the simplest CNN architecture, often called a *vanilla
    CNN*. Existing work has proposed multiple advanced variations that try to improve
    the runtime performance and accuracy of the CNN. Two of the most powerful are
    the ResNet (residual network)[¹](#pgfId-1014897) and Xception[²](#pgfId-1014911)
    architectures. Because no model performs the best in all situations, selecting
    the model and its hyperparameters based on the task and the dataset at hand is
    very important. We’ll begin by looking at these two models and how to tune their
    hyperparameters separately, and then I’ll show you how to do joint model selection
    and hyperparameter tuning with the AutoML pipeline.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章中介绍的CNN模型，它递归地堆叠卷积和池化层，是最简单的CNN架构，通常称为*普通CNN*。现有工作提出了多种高级变体，试图提高CNN的运行性能和准确性。其中最强大的两种是ResNet（残差网络）[¹](#pgfId-1014897)和Xception[²](#pgfId-1014911)架构。由于没有模型在所有情况下都表现最佳，因此根据任务和手头的数据集选择模型及其超参数非常重要。我们将首先查看这两个模型及其超参数的调整方法，然后我会向你展示如何使用AutoML管道进行联合模型选择和超参数调整。
- en: ResNet
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet
- en: In a ResNet, multiple small neural network blocks (or cells) are stacked together
    to build a complete neural network. The block structure is similar to the convolutional
    block in AutoKeras’s ConvBlock, but with a special connection called a *skip connection*
    that adds the input tensor of a block to its output tensor in an elementwise fashion
    (see figure 5.7). The sizes of the input and output of a cell should be the same
    to ensure a valid skip connection. The result of the addition will serve as the
    input tensor for the next cell. This is helpful for building up a deeper network
    because it avoids the problem of *gradient vanishing*, where the gradients for
    updating the weights of the first layers become smaller and smaller during the
    backpropagation process due to the use of the chain rule for calculating the derivative
    of composite layer transformations. The “vanished” gradients are unable to update
    the weights in the early layers, which prevents the creation of a deeper network.
    You can learn more about this in Chollet’s book *Deep Learning with Python*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在ResNet中，多个小的神经网络块（或细胞）被堆叠在一起以构建一个完整的神经网络。其块结构类似于AutoKeras的ConvBlock中的卷积块，但增加了一种特殊连接，称为*跳跃连接*，它以逐元素的方式将块的输入张量添加到其输出张量中（见图5.7）。确保有效的跳跃连接，细胞的输入和输出大小应该相同。加法的结果将作为下一个细胞的输入张量。这对于构建更深的网络是有帮助的，因为它避免了*梯度消失*的问题，在反向传播过程中，由于使用链式法则计算复合层变换的导数，第一层的权重更新梯度变得越来越小。这些“消失”的梯度无法更新早期层的权重，这阻碍了更深网络的形成。你可以在Chollet的《Python深度学习》一书中了解更多相关信息。
- en: '![05-07](../Images/05-07.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![05-07](../Images/05-07.png)'
- en: Figure 5.7 Substructure of ResNet
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 ResNet 的子结构
- en: We can create various ResNets with different cell structures or different numbers
    of cells. Some traditional architectures include ResNet-18 and ResNet-50, where
    the number in the name indicates the total number of layers accumulated from all
    its stacked cells. To implement a ResNet and tune its structure, we can build
    up a sequential AutoML pipeline with AutoKeras using the built-in ResNetBlock,
    as shown in the following listing. The ResNetBlock contains a search space of
    classical ResNet structures, which are predefined and included in the Keras API
    ([https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建具有不同细胞结构或不同数量细胞的多种 ResNet。一些传统架构包括 ResNet-18 和 ResNet-50，其中名称中的数字表示从所有堆叠细胞中累积的总层数。为了实现
    ResNet 并调整其结构，我们可以使用 AutoKeras 内置的 ResNetBlock 构建一个顺序 AutoML 流程，如下所示。ResNetBlock
    包含经典 ResNet 结构的搜索空间，这些结构是预定义的，并包含在 Keras API 中 ([https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/))。
- en: Listing 5.6 Creating a ResNet AutoML pipeline for image classification
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 创建用于图像分类的 ResNet AutoML 流程
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Xception
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Xception
- en: Xception is a CNN architecture that uses separable convolutional layers to improve
    the network’s performance. As briefly mentioned in the earlier discussion of the
    ConvBlock, a separable convolutional layer contains fewer weights than a normal
    convolutional layer but is able to achieve comparable performance on many tasks.
    It produces the filters (weights) of a normal convolutional layer using the weights
    from two separable layers, and then uses its generated filters in the same way
    as the standard convolutional layer. The following listing shows how this works.
    We use a 2-D squared weight matrix of size 3×3 and a vector of length 16 to generate
    a regular 3-D convolutional filter of size 3×3×16 via tensor product.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Xception 是一种使用可分离卷积层来提高网络性能的 CNN 架构。正如在前面关于 ConvBlock 的简短讨论中提到的，可分离卷积层比常规卷积层包含更少的权重，但在许多任务上能够实现可比的性能。它使用来自两个可分离层的权重生成常规卷积层的滤波器（权重），然后像标准卷积层一样使用其生成的滤波器。以下列表显示了这是如何工作的。我们使用一个
    3×3 大小的 2-D 平方权重矩阵和一个长度为 16 的向量，通过张量积生成一个 3×3×16 大小的常规 3-D 卷积滤波器。
- en: Listing 5.7 Producing the weights with a separable convolutional layer
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 使用可分离卷积层生成权重
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Initialize a variable for what we are bringing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化一个变量来表示我们要带来的内容。
- en: ❷ The weight vector of a separable convolutional layer
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 可分离卷积层的权重向量
- en: ❸ Initializes an array with the convolutional layer weights
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用卷积层的权重初始化一个数组
- en: ❹ Computes the convolutional weights with the tensor product
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用张量积计算卷积层的权重
- en: As shown in figure 5.8, Xception uses two types of neural network cells. They
    are similar to ResNet cells but with separable convolutional layers. The first
    type of cell uses a convolutional layer with the kernel size equal to 1 to process
    the input before adding it to the output of the separable convolutional layers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.8 所示，Xception 使用两种类型的神经网络细胞。它们与 ResNet 细胞相似，但具有可分离卷积层。第一种类型的细胞使用一个核大小等于
    1 的卷积层来处理输入，然后再将其添加到可分离卷积层的输出中。
- en: '![05-08](../Images/05-08.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![05-08](../Images/05-08.png)'
- en: Figure 5.8 Two types of Xception cells
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 两种类型的 Xception 细胞
- en: The original Xception architecture is shown in figure 5.9\. It contains normal
    convolutional layers at the beginning, different types of cells in the middle,
    and some separable convolutional layers at the end. Different variations of the
    Xception architecture can be generated by stacking a different number of cells
    or selecting different hyperparameters, such as the number of filters or kernel
    size for the layers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Xception 架构如图 5.9 所示。它包含开头的常规卷积层，中间的不同类型细胞，以及末尾的一些可分离卷积层。通过堆叠不同数量的细胞或选择不同的超参数，如层的滤波器数量或核大小，可以生成
    Xception 架构的不同变体。
- en: '![05-09](../Images/05-09.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![05-09](../Images/05-09.png)'
- en: Figure 5.9 Xception architecture
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 Xception 架构
- en: We can build up an AutoML pipeline to search for a good Xception structure with
    the help of the XceptionBlock in AutoKeras, as shown in the next listing. It covers
    the original Xception architecture as described by Chollet ([https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    as well as a bunch of variations included in the TensorFlow Keras API ([https://keras.io/api/applications/xception/](https://keras.io/api/applications/xception/)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用AutoKeras中的XceptionBlock来帮助构建一个AutoML管道，以搜索一个好的Xception结构，如下一列表所示。它涵盖了Chollet描述的原版Xception架构([https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))以及TensorFlow
    Keras API中包含的许多变体([https://keras.io/api/applications/xception/](https://keras.io/api/applications/xception/))。
- en: Listing 5.8 Creating an Xception AutoML pipeline for image classification
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 创建用于图像分类的Xception AutoML管道
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Besides ResNet and Xception, many other popular variations on vanilla CNNs exist.
    I recommend that you explore the available models based on your interests. For
    practical use, you can directly apply the AutoML pipeline to save you effort in
    learning how they work and tuning them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ResNet和Xception之外，还有许多其他基于普通CNN的流行变体。我建议您根据自己的兴趣探索可用的模型。对于实际应用，您可以直接应用AutoML管道，以节省学习它们的工作原理和调整它们所花费的努力。
- en: Joint model selection and hyperparameter tuning for image classification
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的联合模型选择和超参数调整
- en: 'The “no free lunch” theorem[³](#pgfId-1016954) tells us that no single model
    works best in any situation. “Which ML model should I use for my task?” is a commonly
    asked question. Because we know how to design an AutoML pipeline to tune a specific
    type of model, such as a vanilla CNN, a straightforward option is to tune different
    types of models one by one, find the optimal set of hyperparameters for each,
    and select the best-performing one among them. This is a feasible solution, but
    it’s not an elegant one because it requires us to create multiple AutoML pipelines.
    We expect to be able to create a single AutoML pipeline to address the problem
    in one step. This requires the AutoML pipeline to cover all the relevant types
    of models as well as their unique hyperparameters. In each search trial, the search
    algorithm can first select a model and then select its hyperparameters to generate
    a pipeline. We can implement such an AutoML pipeline with an ImageBlock in AutoKeras
    (see figure 5.10), which is a sort of “hyperblock” that groups together several
    lower-level AutoML blocks: the ConvBlock, ResNetBlock, and XceptionBlock. You
    can use the block_type hyperparameter to choose which type of block to use, or
    if left unspecified, it will be tuned automatically. It also includes normalization
    and image augmentation preprocessor blocks (we’ll talk more about preprocessing
    methods in the next section).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: “没有免费午餐”定理[³](#pgfId-1016954)告诉我们，没有任何一个模型在任何情况下都是最佳选择。“我应该为我的任务使用哪种机器学习模型？”是一个常见的问题。因为我们知道如何设计一个AutoML管道来调整特定类型的模型，例如一个普通的卷积神经网络（CNN），一个直接的选择是逐一调整不同类型的模型，为每个模型找到最佳的超参数集，并从中选择表现最好的一个。这是一个可行的解决方案，但它并不优雅，因为它需要我们创建多个AutoML管道。我们期望能够创建一个单一的AutoML管道，一步解决问题。这要求AutoML管道涵盖所有相关的模型类型以及它们独特的超参数。在每次搜索尝试中，搜索算法可以先选择一个模型，然后选择其超参数来生成一个管道。我们可以使用AutoKeras中的ImageBlock来实现这样的AutoML管道（见图5.10），它是一种“超块”，将几个低级AutoML块组合在一起：ConvBlock、ResNetBlock和XceptionBlock。您可以使用block_type超参数来选择要使用的块类型，或者如果未指定，它将自动调整。它还包括归一化和图像增强预处理块（我们将在下一节中更多地讨论预处理方法）。
- en: '![05-10](../Images/05-10.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![05-10](../Images/05-10.png)'
- en: Figure 5.10 An ImageBlock in AutoKeras
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 AutoKeras中的ImageBlock
- en: The number of models in the search space is quite large, because it encompasses
    all of the included block types. It often requires many search trials to find
    a good model. In addition, some of the models, such as ResNet-152, are much larger
    than the vanilla CNN we designed previously. These models may require a larger
    number of training epochs to achieve good accuracy and ensure a fair comparison
    and selection. Both of these factors result in a longer search process. Moreover,
    large model sizes lead to increased memory consumption during the search process,
    which may prevent us from using large datasets or batch sizes (in an effort to
    reduce the search cost). These are the crucial barriers in the study and application
    of AutoML. We will not explore these issues in more depth here, but instead, we
    will use a small number of search trials and training epochs as a workaround to
    help you get a sense of the model selection process. We’ll talk more about how
    to accelerate the search process and reduce memory consumption in chapter 8.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间中的模型数量相当大，因为它包括了所有包含的块类型。通常需要许多搜索试验才能找到一个好的模型。此外，一些模型，如 ResNet-152，比我们之前设计的
    vanilla CNN 大得多。这些模型可能需要更多的训练 epoch 来达到良好的准确率，并确保公平的比较和选择。这两个因素都导致了更长的搜索过程。此外，大模型的大小在搜索过程中会导致内存消耗增加，这可能会阻止我们使用大型数据集或批次大小（为了减少搜索成本）。这些都是
    AutoML 研究和应用中的关键障碍。我们在这里不会深入探讨这些问题，而是将使用少量搜索试验和训练 epoch 作为一种变通方法来帮助您了解模型选择过程。我们将在第
    8 章中更多地讨论如何加速搜索过程并减少内存消耗。
- en: In listing 5.9, we implement an AutoML pipeline for selecting a suitable model
    (vanilla CNN, ResNet, or Xception) to use for image classification on the MNIST
    dataset. We search for 10 trials. Each model is trained for three epochs with
    a batch size of 32 (the default batch size of AutoKeras). If the batch size is
    too large, AutoKeras will automatically reduce it to avoid memory overflow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 5.9 中，我们实现了一个 AutoML 管道，用于选择一个合适的模型（vanilla CNN、ResNet 或 Xception）用于在 MNIST
    数据集上进行图像分类。我们进行了 10 次试验。每个模型使用 32 个批次的默认大小（AutoKeras 的默认批次大小）进行训练，共三个 epoch。如果批次大小过大，AutoKeras
    会自动将其减小以避免内存溢出。
- en: Listing 5.9 Selecting an image classification model using ImageBlock
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.9 使用 ImageBlock 选择图像分类模型
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Fixes the two preprocessor blocks contained in the ImageBlock
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 修复包含在 ImageBlock 中的两个预处理块
- en: ❷ Performs the search and marks the total time
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行搜索并标记总时间
- en: ❸ Summarizes the best found pipeline
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 总结最佳找到的管道
- en: ❹ Evaluates the best model
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评估最佳模型
- en: From the search results, we can see that it takes more than an hour on a single
    GPU (NVIDIA 2080 Titan) to finish the 10 trials, which is super long. The best
    model is an Xception model, but its performance is not as good as a vanilla CNN
    we found earlier with a ConvBlock using the same number of search trials and the
    same settings for model training. This shows that although enlarging the search
    space enables selection from different models, it may cost you more resources,
    such as more search time using more trials or more computing resources, to find
    a good architecture. Although it can save you effort on tuning, the tradeoff between
    the convenience of AutoML and its cost should never be ignored and remains an
    active area of study in the AutoML field.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从搜索结果中我们可以看到，在一个单独的 GPU（NVIDIA 2080 Titan）上完成 10 次试验需要超过一个小时，这非常长。最佳模型是一个 Xception
    模型，但其性能并不如我们之前使用相同数量的搜索试验和相同的模型训练设置找到的带有 ConvBlock 的 vanilla CNN。这表明，虽然扩大搜索空间可以让我们从不同的模型中进行选择，但它可能需要更多的资源，例如使用更多试验的更多搜索时间或更多计算资源，来找到一个好的架构。尽管它可以节省调优的努力，但
    AutoML 的便利性和其成本之间的权衡绝不能被忽视，并且仍然是 AutoML 领域的一个活跃的研究领域。
- en: 5.3.2 Automated selection of image preprocessing methods
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 自动选择图像预处理方法
- en: 'Besides model selection, we may also want to select suitable data-preprocessing
    methods to better prepare the data for our models and improve their performance.
    For example, working with a small dataset is a common situation in deep learning
    applications. Learning from insufficient training data can introduce a high risk
    of overfitting, especially when we have a larger model, causing the model to not
    generalize well to new data. This problem can be mitigated through the following
    two main approaches:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型选择之外，我们还可能想要选择合适的数据预处理方法，以更好地为我们的模型准备数据并提高它们的性能。例如，在深度学习应用中，处理小型数据集是一种常见情况。从不足的训练数据中学习可能会引入过拟合的高风险，尤其是在我们有一个较大的模型时，导致模型对新数据泛化不良。这个问题可以通过以下两种主要方法来缓解：
- en: On the model or learning algorithm side, we can use a technique called *regularization*.
    We’ve already seen some examples of this, such as using dropout layers, limiting
    the model size by reducing the number of layers or neurons, and using fewer training
    epochs.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型或学习算法方面，我们可以使用一种称为**正则化**的技术。我们已经看到了一些例子，例如使用dropout层，通过减少层数或神经元数量来限制模型大小，以及使用更少的训练轮次。
- en: On the data side, we may be able to collect more data or use *data augmentation*
    methods to tweak the instances in the existing dataset to generate new instances.
    Data augmentation gives the ML models a larger pool of instances to learn from,
    which can improve their performance. For example, for an image dataset, each image
    might be flipped horizontally or rotated by a certain degree before passing it
    into the neural network. We can use many such operations to tweak the images,
    and we can apply different operations to different images randomly to achieve
    more diversified training data. In different epochs, we may also apply different
    operations on the same image. In figure 5.11, you can see some images generated
    this way. The first image is the original, and the other nine images were all
    generated with data augmentation techniques. As you can see, the content is always
    the same, but the size, position, and so on have been changed.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据方面，我们可能能够收集更多数据或使用**数据增强**方法来调整现有数据集中的实例，以生成新的实例。数据增强为ML模型提供了更大的实例池来学习，这可以提高它们的性能。例如，对于图像数据集，每个图像在传递到神经网络之前可能会水平翻转或旋转一定角度。我们可以使用许多此类操作来调整图像，并且我们可以随机对不同的图像应用不同的操作以实现更多样化的训练数据。在不同的轮次中，我们也可以对同一图像应用不同的操作。在图5.11中，你可以看到以这种方式生成的某些图像。第一幅是原始图像，其他九幅图像都是使用数据增强技术生成的。正如你所见，内容始终相同，但大小、位置等已经发生了变化。
- en: '![05-11](../Images/05-11.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![05-11](../Images/05-11.png)'
- en: Figure 5.11 Image augmentation
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 图像增强
- en: Because many regularization techniques are related to the selection of model
    structures, the AutoML method introduced in previous sections is already able
    to discover some of them to mitigate the issue of overfitting. In fact, it is
    also straightforward to extend the AutoML pipeline to tune and select a suitable
    data augmentation method—that is, to use an AutoML block to select and evaluate
    various data augmentation methods. The ImageBlock also allows us to select among
    multiple data preprocessing methods, such as deciding whether to use normalization
    and/or data augmentation methods to prepare the data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多正则化技术都与模型结构的选择相关，因此在前几节中介绍的AutoML方法已经能够发现其中的一些，以减轻过拟合的问题。实际上，将AutoML管道扩展以调整和选择合适的数据增强方法也很简单——即使用AutoML模块来选择和评估各种数据增强方法。ImageBlock还允许我们在多种数据预处理方法中进行选择，例如决定是否使用归一化和/或数据增强方法来准备数据。
- en: Let’s use an image classification example to illustrate how to automatically
    select preprocessing methods for a ResNet model. We decide whether to use data
    augmentation and normalization methods. The dataset we use in listing 5.10 is
    a subset of the CIFAR-10 dataset, which contains 60,000 RGB images of size 32×32×3\.
    The 50,000 images in the training set belong to 10 classes, such as “bird,” “cat,”
    “dog,” and so on (5,000 images per class). To make things easier, we’ll use images
    from only two classes, “airplane” and “automobile.” The first nine images from
    the subsampled dataset are visualized in figure 5.12.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个图像分类示例来说明如何自动选择ResNet模型的预处理方法。我们决定是否使用数据增强和归一化方法。我们在5.10列表中使用的数据集是CIFAR-10数据集的一个子集，包含60,000张32×32×3大小的RGB图像。训练集中的50,000张图像属于10个类别，例如“鸟”、“猫”、“狗”等（每个类别5,000张图像）。为了简化问题，我们将只使用两个类别的图像，“飞机”和“汽车”。子采样数据集的前九张图像在图5.12中进行了可视化。
- en: '![05-12](../Images/05-12.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![05-12](../Images/05-12.png)'
- en: Figure 5.12 The first nine images from the “airplane” and “automobile” classes
    in the CIFAR-10 dataset
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 CIFAR-10数据集中“飞机”和“汽车”类的前九张图像
- en: Listing 5.10 Loading and visualizing a subset of the CIFAR-10 dataset
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.10 加载和可视化CIFAR-10数据集的子集
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Loads the CIFAR-10 dataset
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载CIFAR-10数据集
- en: ❷ Picks the images belonging to the “airplane” and “automobile” classes
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择属于“飞机”和“汽车”类别的图像
- en: ❸ Plots the first nine images
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制前九张图像
- en: Let’s first create an AutoML pipeline to select the data augmentation method
    for the ResNet models. The pipeline has the same structure as the sequential AutoML
    pipelines we built in the previous section for tuning a single ResNet model. The
    only difference is that we add an ImageAugmentation AutoML block between the normalization
    preprocessor and the network block, as shown in listing 5.11\. The ImageAugmentation
    block in AutoKeras does not have parameters to be updated via backpropagation
    but contains multiple image transformation operations that can be jointly selected
    with other hyperparameters in the pipeline. We fix the type of ResNet to narrow
    down the search space. 'v2' here means the version 2 search space, covering three
    ResNet structures.[⁴](#pgfId-1018127) The augmentation methods are selected along
    with the structure and other hyperparameters, such as the optimization method
    and learning rate.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个AutoML管道来选择ResNet模型的数据增强方法。该管道的结构与我们在上一节中为调整单个ResNet模型所构建的顺序AutoML管道相同。唯一的区别是我们将一个ImageAugmentation
    AutoML模块添加到归一化预处理模块和网络模块之间，如列表5.11所示。AutoKeras中的ImageAugmentation模块没有通过反向传播更新的参数，但它包含多个可以与管道中的其他超参数一起联合选择的图像变换操作。我们固定ResNet的类型以缩小搜索空间。'v2'在这里表示版本2的搜索空间，包括三种ResNet结构。[⁴](#pgfId-1018127)
    增强方法将与结构和其他超参数（如优化方法和学习率）一起选择。
- en: Note This code example may take a long time to run.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此代码示例可能需要很长时间才能运行。
- en: Listing 5.11 Selecting image preprocessing methods for a ResNet model
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.11 选择ResNet模型的图像预处理方法
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As mentioned in the previous section, the image hyperblock (ImageBlock) in AutoKeras
    also contains preprocessing methods, so we can use it to select the data augmentation
    method as well. It can also decide whether to use normalization if we set the
    normalize argument to None, as in the following listing. Setting normalize to
    True (or False) will fix that we want to use (or not use) the normalization method.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AutoKeras中的图像超模块（ImageBlock）也包含预处理方法，因此我们可以用它来选择数据增强方法。如果我们设置normalize参数为None，它还可以决定是否使用归一化，如下面的列表所示。将normalize设置为True（或False）将确定我们想要使用（或不想使用）归一化方法。
- en: Note This code example may take a long time to run.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此代码示例可能需要很长时间才能运行。
- en: Listing 5.12 Selecting augmentation and normalization methods for ResNet models
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.12 选择ResNet模型的增强和归一化方法
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Does not specify whether we want to use normalization and data augmentation
    methods; lets them be searched automatically
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不指定是否要使用归一化和数据增强方法；让它们自动搜索
- en: ❷ Searches only the ResNet architectures
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只搜索ResNet架构
- en: Because the search and model evaluation process is the same as in all the examples
    introduced earlier, we do not repeat it again here.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因为搜索和模型评估过程与前面介绍的所有示例相同，所以我们在此不再重复。
- en: 'You now know how to use an AutoML pipeline to do hyperparameter tuning for
    a single type of model and how to use an ImageBlock for model selection. Although
    we used an image classification task as an example, this process can be generalized
    to text and structured data use cases. For example, to jointly select models and
    preprocessors for text or structured data classification or regression tasks,
    you can use TextBlock or StructuredDataBlock in AutoKeras to create the AutoML
    pipeline and change the input node from ImageInput to TextInput or StructuredDataInput.
    TextBlock and StructuredDataBlock both cover some representative models and preprocessors
    for the corresponding data types. For more details on these and the other AutoML
    blocks, check out [https://autokeras.com/tutorial/overview/](https://autokeras.com/tutorial/overview/).
    For practical use, you can pick the one that is related to the type of neural
    network you want to tune or select and use it to create your AutoML pipeline using
    the default search space for the hyperparameters, or you can customize the hyperparameters
    based on your requirements. In the next section, we’ll turn to a slightly more
    complicated use case: designing graph-structured AutoML pipeline beyond just sequentially
    stacking blocks.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道了如何使用AutoML管道对一个单一类型的模型进行超参数调整，以及如何使用ImageBlock进行模型选择。尽管我们以图像分类任务为例，但这个过程可以推广到文本和结构化数据用例。例如，为了联合选择文本或结构化数据分类或回归任务中的模型和预处理程序，你可以在AutoKeras中使用TextBlock或StructuredDataBlock来创建AutoML管道，并将输入节点从ImageInput更改为TextInput或StructuredDataInput。TextBlock和StructuredDataBlock都涵盖了相应数据类型的某些代表性模型和预处理程序。有关这些和其他AutoML块更详细的信息，请参阅[https://autokeras.com/tutorial/overview/](https://autokeras.com/tutorial/overview/)。对于实际应用，你可以选择与你想调整或选择的神经网络类型相关的选项，并使用超参数的默认搜索空间来创建你的AutoML管道，或者根据你的需求自定义超参数。在下一节中，我们将转向一个稍微复杂一些的用例：设计超出仅仅顺序堆叠块的图结构化AutoML管道。
- en: 5.4 Designing a graph-structured AutoML pipeline
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 设计图结构化的AutoML管道
- en: In many applications, our requirements go beyond a sequential deep learning
    model (stacking layers sequentially). For example, for multi-input and multi-output
    classification problems, we may need to use different layers and preprocessing
    components for different inputs and create different heads to generate different
    outputs. We may need to preprocess images with the normalization method before
    feeding them into ResNet models and encode structured data with categorical features
    into numerical values before feeding them into MLPs. We can then merge the outputs
    from the models to generate the outputs for different targets. In other situations,
    we may want to leverage the combined power of multiple deep learning models, such
    as using ResNet and Xception networks together to do image classification. Different
    models can learn different feature representations from the data, and combining
    these representations can potentially enhance their predictive power. Tuning the
    models in these scenarios requires going beyond a sequential pipeline to a *graph-structured*
    *pipeline*, where each block can take input from multiple blocks (see figure 5.13).
    The pipeline is a *directed acyclic graph* (DAG) in which the nodes are the AutoML
    blocks introduced in the previous sections. Their order indicates the input/output
    connections among the blocks, which also denotes the flow of data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，我们的需求超出了顺序深度学习模型（顺序堆叠层）。例如，对于多输入和多输出分类问题，我们可能需要为不同的输入使用不同的层和预处理组件，并创建不同的头部来生成不同的输出。我们可能需要在将图像输入到ResNet模型之前使用归一化方法对图像进行预处理，并在将具有分类特征的结构化数据输入到MLPs之前将其编码为数值。然后我们可以合并来自模型的输出以生成针对不同目标的输出。在其他情况下，我们可能希望利用多个深度学习模型的组合力量，例如使用ResNet和Xception网络一起进行图像分类。不同的模型可以从数据中学习不同的特征表示，并且结合这些表示可能会增强它们的预测能力。在这些场景中调整模型需要超出顺序管道到*图结构化*
    *管道*，其中每个块可以从多个块接收输入（见图5.13）。该管道是一个*有向无环图*（DAG），其中的节点是上一节中引入的AutoML块。它们的顺序表示块之间的输入/输出连接，这也表示了数据流。
- en: '![05-13](../Images/05-13.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![05-13](../Images/05-13.png)'
- en: Figure 5.13 Graph-structured AutoML pipelines
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 图结构化的AutoML管道
- en: 'In this section, we will take a quick look at how to tune models with multiple
    inputs and outputs by creating a graph-structured pipeline. The models we want
    to tune have a cross structure, as shown on the left side of figure 5.13\. We’ll
    begin by creating a synthetic dataset similar to the one we used in chapter 4,
    with both synthetic images and structured data. We have two targets: a classification
    target and a regression response. Code for creating the dataset is shown in listing
    5.13\. We generate 1,000 synthetic instances, of which 800 are used for training
    and validation and 200 are reserved for testing. Each instance comprises an image
    with size 32×32×3 and three categorical features. The output consists of a classification
    label (one of five classes) and a regression response.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍如何通过创建图结构管道来调整具有多个输入和输出的模型。我们想要调整的模型具有交叉结构，如图5.13左侧所示。我们将首先创建一个与第4章中使用的类似的合成数据集，其中包含合成图像和结构化数据。我们有两个目标：一个分类目标和回归响应。创建数据集的代码在列表5.13中给出。我们生成了1,000个合成实例，其中800个用于训练和验证，200个保留用于测试。每个实例包含一个大小为32×32×3的图像和三个分类特征。输出包括一个分类标签（五个类别之一）和一个回归响应。
- en: Listing 5.13 Creating a synthetic dataset with multiple inputs and outputs
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.13 创建具有多个输入和输出的合成数据集
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Generates image data
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成图像数据
- en: ❷ Generates structured data with three categorical features
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成具有三个分类特征的结构化数据
- en: ❸ Generates classification labels for five classes
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为五个类别生成分类标签
- en: ❹ Generates regression targets
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成回归目标
- en: ❺ Displays the categorical features of the first five instances
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示前五个实例的分类特征
- en: The creation of the pipeline should follow the topological order of the nodes
    in the graph, which means we should follow the flow of the data—we begin by creating
    the AutoML blocks that appear at the front of the pipeline, so that their outputs
    can be fed into the blocks that appear later. You can imagine how a data instance
    will go from input to output in order to set up the blocks of the AutoML pipeline
    one by one. The code for creating the AutoML pipeline shown on the left in figure
    5.13 is presented in listing 5.14\. After stacking the AutoML blocks to process
    each type of data, the outputs from the two branches are combined to generate
    the two responses via a Merge block. This block will add the two outputs elementwise
    if their dimensions are the same. Otherwise, it will reshape the input tensors
    into vectors and then concatenate them. The specific merge operations to be used
    are tuned during the search process if not specified during initialization.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的创建应遵循图中节点的拓扑顺序，这意味着我们应该遵循数据流——我们首先创建管道前端的AutoML块，以便它们的输出可以输入到后续出现的块中。你可以想象一个数据实例如何从输入到输出，以便逐个设置AutoML管道的块。图5.13左侧所示创建AutoML管道的代码在列表5.14中给出。在堆叠AutoML块以处理每种类型的数据后，两个分支的输出通过一个合并块组合起来以生成两个响应。如果它们的维度相同，该块将逐元素添加两个输出。否则，它将输入张量重塑为向量，然后进行连接。如果在初始化期间未指定，则将在搜索过程中调整要使用的特定合并操作。
- en: Listing 5.14 Tuning a model with a graph-structured AutoML pipeline
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.14 使用图结构AutoML管道调整模型
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Stacks two blocks in the image branch
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在图像分支中堆叠两个块
- en: ❷ Stacks two blocks in the structured data branch
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在结构化数据分支中堆叠两个块
- en: ❸ Merges the two blocks
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 合并两个块
- en: ❹ Generates the graph-structured AutoML pipeline with multiple inputs and outputs
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成具有多个输入和输出的图结构AutoML管道
- en: ❺ Feeds the data into the AutoML pipeline
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据输入到AutoML管道中
- en: ❻ Plots the best model
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 绘制最佳模型
- en: The best architecture found in three trials is visualized in figure 5.14\. I’ve
    annotated each component in the discovered deep network with the corresponding
    element in the AutoML pipeline. Their hyperparameters are selected to form the
    search space of the AutoML block. For example, two dense layers are used to process
    structured data, and there are two convolutional cells, each of which contains
    two convolutional layers with a max pooling layer to encode the images.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在三次试验中找到的最佳架构在图5.14中进行了可视化。我已经将发现的深度网络中的每个组件与AutoML管道中的相应元素进行了注释。它们的超参数被选用来形成AutoML块的搜索空间。例如，使用两个密集层来处理结构化数据，并且有两个卷积单元，每个单元包含两个卷积层和一个最大池化层来编码图像。
- en: '![05-14](../Images/05-14.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![05-14](../Images/05-14.png)'
- en: Figure 5.14 The best model identified for the task with multiple inputs and
    outputs
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 为具有多个输入和输出的任务识别的最佳模型
- en: We can create more complicated graph-structured pipelines to tune more complex
    architectures, and we can use a hyperblock to help us select among different models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建更复杂的图结构流水线来调整更复杂的架构，并且我们可以使用超块来帮助我们选择不同的模型。
- en: 'Though it is convenient to use the built-in blocks of AutoKeras to create these
    pipelines, you may find that they don’t cover all the models you know about or
    are not able to support all the hyperparameters you want to tune. This introduces
    a question: is it possible to create our own AutoML blocks to select and tune
    our concerned neural networks or preprocessing methods? The aim of the last section
    is to introduce how to customize your own AutoML blocks to build up the AutoML
    pipeline.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用AutoKeras内置的块来创建这些流水线很方便，但你可能会发现它们并不能涵盖你了解的所有模型，或者无法支持你想要调整的所有超参数。这引发了一个问题：我们是否可以创建自己的AutoML块来选择和调整我们关心的神经网络或预处理方法？最后一节的目标是介绍如何自定义自己的AutoML块来构建AutoML流水线。
- en: 5.5 Designing custom AutoML blocks
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 设计自定义AutoML块
- en: Using AutoKeras’s built-in blocks may not always fulfill your hyperparameter
    tuning and model selection needs. Hundreds, if not thousands, of neural networks
    might exist beyond the scope of the search space the blocks define. For example,
    suppose you want to tune a CNN that you’ve created yourself or found in the literature
    for image classification. It is not contained in the search space of any of the
    built-in AutoKeras blocks, including the ConvBlock, ResNetBlock, and so on. You
    need a new AutoML block, whose search space consists of models with the same structure
    as this new CNN model but with different hyperparameters (such as different numbers
    of units or layers). With this new AutoML block, you can tune the hyperparameters
    of the new neural network and compare it with other models covered by the pre-existing
    AutoKeras blocks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AutoKeras的内置块可能并不总是能满足你的超参数调整和模型选择需求。可能存在数百甚至数千个神经网络超出了这些块定义的搜索空间范围。例如，假设你想调整你自己创建的或从文献中找到的用于图像分类的CNN（卷积神经网络）。它不包含任何内置AutoKeras块的搜索空间中，包括ConvBlock、ResNetBlock等。你需要一个新的AutoML块，其搜索空间由与这个新的CNN模型具有相同结构但具有不同超参数（如不同数量的单元或层）的模型组成。有了这个新的AutoML块，你可以调整新神经网络的超参数，并将其与其他由现有AutoKeras块覆盖的模型进行比较。
- en: This section teaches you how to create your own AutoML block to do hyperparameter
    tuning and model selection. We’ll start by creating a custom block to tune MLPs
    without using the built-in MLP block (DenseBlock). Then we’ll explore how to customize
    a hyperblock to perform model selection. The aim of this section is not to teach
    you how to design advanced neural networks but to show you a general way of creating
    the search space (the AutoML block) that can be used to tune and select from different
    types of neural networks if you already know how to create them with TensorFlow
    and Keras.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将指导你如何创建自己的AutoML块来进行超参数调整和模型选择。我们将首先创建一个自定义块来调整MLP（多层感知器）而无需使用内置的MLP块（DenseBlock）。然后我们将探讨如何自定义超块来进行模型选择。本节的目标不是教你如何设计高级神经网络，而是展示一种创建搜索空间（AutoML块）的一般方法，这样你就可以在已知如何使用TensorFlow和Keras创建它们的情况下，从不同类型的神经网络中进行调整和选择。
- en: 5.5.1 Tuning MLPs with a custom MLP block
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 使用自定义MLP块调整MLP
- en: In section 5.2, we used a built-in AutoKeras block (DenseBlock) to select a
    good MLP structure for regression tasks. This section will show you how to implement
    an MLP block yourself to achieve the same goal. It will familiarize you with the
    basic operations to create an AutoML block that defines the search space for deep
    learning models and can be generalized to create AutoML blocks for tuning more
    complex architectures.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在5.2节中，我们使用内置的AutoKeras块（DenseBlock）为回归任务选择了一个好的MLP结构。本节将向你展示如何自己实现一个MLP块以实现相同的目标。这将使你熟悉创建定义深度学习模型搜索空间的AutoML块的基本操作，并且可以推广到创建用于调整更复杂架构的AutoML块。
- en: Customizing a block for tuning the number of units
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义用于调整单元数量的块
- en: Chapter 3 showed you how to create an MLP with TensorFlow and Keras. We start
    with an input node to specify the shape of the inputs. Then we create multiple
    dense layers and stack them sequentially, layer by layer. Finally, the layers
    are grouped into an MLP model for training and testing via the tensorflow.keras.Model
    class (or the tensorflow.keras.Sequential class we’ve used before). Listing 5.15
    creates a three-layer MLP, whose two hidden layers have 32 units each. We ignore
    the model-compiling part and implement only the creation of the model structure.
    Calling the build function will return an MLP model with the structure defined
    here.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章向您展示了如何使用TensorFlow和Keras创建一个MLP。我们从指定输入形状的输入节点开始。然后我们创建多个密集层并将它们按顺序逐层堆叠。最后，这些层被组合成一个MLP模型，通过tensorflow.keras.Model类（或我们之前使用的tensorflow.keras.Sequential类）进行训练和测试。列表5.15创建了一个三层MLP，其两个隐藏层各有32个单元。我们忽略了模型编译的部分，只实现了模型结构的创建。调用build函数将返回一个具有此处定义结构的MLP模型。
- en: Listing 5.15 MLP implementation in Keras
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.15 Keras中的MLP实现
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Defines the input dimensions of the network
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义网络的输入维度
- en: ❷ Stacks two hidden dense layers with 32 units each and ReLU activation
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 堆叠两个具有32个单元和ReLU激活的隐藏密集层
- en: ❸ Adds a dense classification layer with sigmoid activation
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个具有sigmoid激活的密集分类层
- en: ❹ Groups the layers into a Keras model
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将层组合成一个Keras模型
- en: 'Suppose we want to tune the number of units in the two hidden layers. The units
    hyperparameter should be an integer, so to create a finite search space, we’ll
    assume its value to be a multiple of 32 and smaller than 512\. This gives us the
    following search space of values to choose from: [32, 64, ..., 512]. To create
    an AutoML block that defines this search space, we need to do three things. First,
    in listing 5.16, we create a class that extends the basic AutoKeras Block class
    (ak.Block). We’ll use this for tuning our MLP, so we’ll name it MlpBlock. Extending
    the basic Block class guarantees we can use the customized block to create an
    AutoML pipeline by connecting it with other AutoKeras components. It contains
    a build() function, to be overridden, that can help us define the search space
    and instantiate MLP models during the search process.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要调整两个隐藏层中的单元数。单元超参数应该是一个整数，因此为了创建一个有限的搜索空间，我们将假设其值为32的倍数且小于512。这给我们提供了一个以下值的搜索空间：[32,
    64, ..., 512]。为了创建一个定义此搜索空间的AutoML块，我们需要做三件事。首先，在列表5.16中，我们创建了一个扩展基本AutoKeras
    Block类的类（ak.Block）。我们将使用它来调整我们的MLP，因此我们将它命名为MlpBlock。扩展基本Block类确保我们可以使用定制的块通过将其与其他AutoKeras组件连接起来来创建一个AutoML管道。它包含一个要重写的build()函数，这可以帮助我们在搜索过程中定义搜索空间并实例化MLP模型。
- en: Listing 5.16 Tuning the units of an MLP with two hidden layers
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.16 调整具有两个隐藏层的MLP的单元数
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Imports the autokeras package for customizing an AutoML block
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入autokeras包以自定义AutoML块
- en: ❷ Starts to implement a class that extends the basic AutoKeras Block class
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开始实现一个扩展基本AutoKeras Block类的类
- en: ❸ A function to be implemented
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 需要实现的函数
- en: 'Next, we implement the build function. This function is the core part of the
    AutoML block. It has two roles: defining the search space for the hyperparameters
    we want to tune and building an MLP in each trial during the search process whenever
    it is called by the search algorithm. Implementing it is very similar to writing
    a function to build an MLP model (see listing 5.15), but with the following main
    changes:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现build函数。这是AutoML块的核心部分。它有两个作用：定义我们想要调整的超参数的搜索空间，并在搜索过程中每次被搜索算法调用时构建一个MLP。实现它非常类似于编写一个构建MLP模型的函数（参见列表5.15），但有以下主要变化：
- en: Because this AutoML block will connect with other blocks to form an AutoML pipeline,
    it should take the outputs from one or more previous blocks as its input and output
    a tensor that can be fed as input into other blocks, rather than a complete Keras
    model to be trained and evaluated.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为这个AutoML块将与其他块连接起来形成一个AutoML管道，所以它应该接受一个或多个先前块的输出作为其输入，并输出一个张量，可以作为输入馈送到其他块，而不是一个完整的Keras模型进行训练和评估。
- en: Because an AutoML block is used to define the search space of the relevant hyperparameters,
    we should define a search space for each hyperparameter we want to tune. These
    hyperparameters will not be assigned fixed values in advance, like the units=32
    in listing 5.15, but will be dynamically assigned a value sampled from the search
    space (by the search algorithm) in each search trial.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用AutoML块来定义相关超参数的搜索空间，因此我们应该为每个想要调整的超参数定义一个搜索空间。这些超参数不会预先分配固定值，例如列表5.15中的units=32，而是在每个搜索试验中动态地从搜索空间（由搜索算法）中分配一个值。
- en: To reflect the first change, in listing 5.17, we remove the Keras input initializer
    (keras.Input()) and directly feed a list of the output nodes (or tensors) of the
    previous AutoML blocks to the build() function. Because the inputs can be a list
    of tensors, we use a flatten operation (tf.nest.flatten()) to combine them into
    a single tensor that can be directly fed into the dense layers. We leave only
    the two hidden layers to be tuned in the block and return the outputs without
    adding a classification layer or wrapping them into a Keras model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反映第一个变化，在列表5.17中，我们移除了Keras输入初始化器（keras.Input()）并直接将前一个AutoML块的输出节点（或张量）列表馈送到build()函数。由于输入可以是张量列表，我们使用展平操作（tf.nest.flatten()）将它们组合成一个可以直接馈送到密集层的单个张量。我们只留下块中的两个隐藏层进行调整，并返回输出而不添加分类层或将其包装到Keras模型中。
- en: Listing 5.17 Tuning the two hidden layers with the same number of units
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.17 使用相同数量的单元调整两个隐藏层
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Implements a class extending the Block class
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实现一个扩展Block类的类
- en: ❷ Overrides the build function
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 覆盖构建函数
- en: ❸ Gets the input node from the inputs, which may be either a list of nodes or
    a single node
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从输入中获取输入节点，这些输入可能是一个节点列表或单个节点
- en: ❹ Declares the number of units as an integer hyperparameter
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 声明单元数量为一个整型超参数
- en: ❺ Uses the same number of units for both of the layers
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 两个层使用相同数量的单元
- en: ❻ The return value of the build function should be the output node.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ build函数的返回值应该是输出节点。
- en: 'To define the search space of the units hyperparameter and assign it a value
    dynamically, we use a module in KerasTuner called keras_tuner.engine.hyperparameters.
    It contains different classes to create the search spaces for different types
    of hyperparameters (integer, float, Boolean) and a container named HyperParameters
    (hp for short) that contains information about the search spaces for all the relevant
    hyperparameters. The hp container has different methods corresponding to the different
    search-space-creation classes. For example, the hp.Choice method corresponds to
    the keras_tuner.engine.hyperparameters.Choice class we used in chapter 5\. Here,
    we use hp.Int to define a search space of integer values for the units hyperparameter
    (see the fourth annotation in listing 5.17). It creates a list of values ([32,
    64, ..., 512]), similar to the hp.Choice method, but more conveniently: you don’t
    have to list every value in the search space one by one but can define a *step
    value* to generate them automatically (step=32 in this example, because the values
    are multiples of 32). The max_value and min_value arguments limit the scope of
    the search space. The name argument provides a reference to the hyperparameter
    in the search space.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义单元超参数的搜索空间并动态地为其分配值，我们使用KerasTuner中的一个模块，即keras_tuner.engine.hyperparameters。它包含用于创建不同类型超参数（整数、浮点、布尔）的搜索空间的不同类，以及一个名为HyperParameters（简称hp）的容器，它包含有关所有相关超参数搜索空间的信息。hp容器具有对应于不同搜索空间创建类的不同方法。例如，hp.Choice方法对应于我们在第5章中使用的keras_tuner.engine.hyperparameters.Choice类。在这里，我们使用hp.Int为单元超参数定义一个整数值的搜索空间（参见列表5.17中的第四个注释）。它创建了一个值列表（[32,
    64, ..., 512]），类似于hp.Choice方法，但更方便：您不需要逐个列出搜索空间中的每个值，但可以定义一个*步长值*来自动生成它们（本例中为32，因为值是32的倍数）。max_value和min_value参数限制了搜索空间的范围。name参数为搜索空间中的超参数提供了一个参考。
- en: In listing 5.18, we assume the two hidden layers have the same number of units.
    We can also separately tune the number of units in each of the hidden layers,
    as shown in the following listing. In this case, we assign a different name to
    each hyperparameter and assign them to the corresponding layers.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.18中，我们假设两个隐藏层具有相同数量的单元。我们也可以分别调整每个隐藏层中的单元数量，如下面的列表所示。在这种情况下，我们为每个超参数分配不同的名称并将它们分配给相应的层。
- en: Listing 5.18 Separately tuning the number of units in the two hidden layers
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.18 分别调整两个隐藏层中的单元数量
- en: '[PRE18]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Creates separate search spaces for the number of units in each layer
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为每一层的单元数创建单独的搜索空间
- en: ❷ Assigns the units to the corresponding layers
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将单元分配给相应的层
- en: 'We do not create the hp container in the build() function but rather feed it
    to the build() function as input. The container is a global container for all
    the AutoML blocks. It plays the following two roles:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不在build()函数中创建hp容器，而是将其作为输入传递给build()函数。容器是所有AutoML块的全球容器。它扮演以下两个角色：
- en: '*Search space container*—The hp container holds information on all the hyperparameter
    search spaces created in each AutoML block. Thus, once the complete AutoML pipeline
    has been created, it will hold details on the complete search space for the entire
    pipeline.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索空间容器*—hp容器包含每个AutoML块中创建的所有超参数搜索空间的信息。因此，一旦完整的AutoML管道被创建，它将包含整个管道的完整搜索空间细节。'
- en: '*Current hyperparameter value container*—During the search process, in each
    trial, the container will keep track of the hyperparameter values given by the
    search algorithm and assign them to the corresponding hyperparameters in each
    block based on their names. In this example, the hp container will provide fixed
    units values selected by the search algorithm to help build the two hidden layers
    in our MlpBlock.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当前超参数值容器*—在搜索过程中，在每次试验中，容器将跟踪搜索算法提供的超参数值，并根据它们的名称将它们分配给每个块中相应的超参数。在这个例子中，hp容器将提供由搜索算法选择的固定单元值，以帮助我们构建MlpBlock中的两个隐藏层。'
- en: 'Although it looks like we are assigning a search space for each hyperparameter
    (such as units=hp.Int(...)) in the build() function, during the search process,
    the function will always create the layers with fixed hyperparameter values. These
    are set by default or selected by the search algorithm. This means the hp.Int()
    method will always return a fixed value when it is called, and the search space’s
    definition is saved in the hp container. This also illustrates why the names of
    hyperparameters should be different: so that the container can distinguish between
    them when saving their search spaces and assign the correct values to them when
    building the models in each trial.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来我们在build()函数中为每个超参数（如units=hp.Int(...))分配了一个搜索空间，但在搜索过程中，该函数总是会创建具有固定超参数值的层。这些值默认设置或由搜索算法选择。这意味着当调用hp.Int()方法时，它总是会返回一个固定值，搜索空间的定义保存在hp容器中。这也说明了为什么超参数的名称应该不同：这样容器在保存它们的搜索空间时可以区分它们，并在每次试验构建模型时将正确的值分配给它们。
- en: Customizing a block for tuning different types of hyperparameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为调整不同类型的超参数定制一个块
- en: We tuned an integer hyperparameter in the previous example, but many different
    types of hyperparameters can exist in an MLP. For example, whether we should use
    a dropout layer (to help avoid overfitting, as discussed in chapter 4) can be
    treated as a hyperparameter with a Boolean (true/false) value. The dropout rate
    in the dropout layer is a floating-point value. The key point in tuning different
    types of hyperparameters is selecting the correct search-space-creation method.
    We list several examples in table 5.1\. Intuitively, the methods for creating
    the search space accord with the value types of the hyperparameters. Keep in mind
    that although the hp container methods, such as hp.Choice() and hp.Int(), define
    a search space, they will always return a value.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们调整了一个整型超参数，但在MLP中可能存在许多不同类型的超参数。例如，是否应该使用dropout层（如第4章所述，有助于避免过拟合）可以被视为具有布尔（真/假）值的超参数。dropout层中的dropout率是一个浮点值。调整不同类型超参数的关键点是选择正确的搜索空间创建方法。我们在表5.1中列出了几个示例。直观上，创建搜索空间的方法与超参数的值类型相符合。请记住，尽管hp容器方法，如hp.Choice()和hp.Int()定义了一个搜索空间，但它们总是会返回一个值。
- en: Table 5.1 Different hyperparameter types in an MLP mode
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 MLP模式中的不同超参数类型
- en: '| Hyperparameter | Type | Search space example | Search-space-creation method
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 类型 | 搜索空间示例 | 搜索空间创建方法 |'
- en: '| Number of units in a layer | Integer | [10, 30, 100, 200] / [10, 20, 30,
    40] | hp.Choice, hp.Int |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 层中的单元数量 | 整数 | [10, 30, 100, 200] / [10, 20, 30, 40] | hp.Choice, hp.Int
    |'
- en: '| Number of layers | Integer | [1, 2, 3, 4] | hp.Choice, hp.Int |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 层数数量 | 整数 | [1, 2, 3, 4] | hp.Choice, hp.Int |'
- en: '| Whether to use a dropout layer | Boolean | [True, False] | hp.Boolean |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 是否使用dropout层 | 布尔 | [True, False] | hp.Boolean |'
- en: '| Dropout rate | Float | Any real value between 0.1 and 0.2 | hp.Float |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Dropout率 | 浮点数 | 介于0.1和0.2之间的任何实数值 | hp.Float |'
- en: Tuning a single hyperparameter is straightforward. But what if we want to jointly
    search the number of layers as well as the number of units in each layer? These
    two hyperparameters are dependent on each other, because the number of layers
    determines how many “units” hyperparameters we want to tune if we assume the hidden
    layers can have different numbers of units. We can create a search space for the
    number of layers using the hp.Choice() method as usual, as shown in listing 5.19\.
    The returned value will determine how many layers we will have in the MlpBlock
    and can be used in a for loop to help create the layers and set up the search
    space for the number of units in each layer. Note that we use different names
    for the units hyperparameter in each layer to distinguish between them for the
    search algorithm.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 调整单个超参数很简单。但如果我们想同时搜索层数以及每层的单元数呢？这两个超参数相互依赖，因为层数决定了如果我们假设隐藏层可以有不同数量的单元，我们想要调整多少个“单元”超参数。我们可以使用
    hp.Choice() 方法创建层数的搜索空间，如列表 5.19 所示。返回值将决定 MlpBlock 中将有多少层，并可用于 for 循环以帮助创建层并设置每层单元数的搜索空间。请注意，我们为每层的单元超参数使用不同的名称，以便在搜索算法中区分它们。
- en: Listing 5.19 Selecting the number of layers and the number of units for each
    layer
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.19 选择层数和每层的单元数
- en: '[PRE19]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Defines the number of layers as a hyperparameter
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将层数定义为超参数
- en: ❷ Dynamically generates one new hyperparameter for each layer, while ensuring
    the hyperparameter names are not the same
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 动态地为每一层生成一个新的超参数，同时确保超参数名称不重复
- en: We can flesh out our MlpBlock by adding more hyperparameters, as shown listing
    5.20\. For example, whether to use a dropout layer is worth exploring in practice.
    Because it’s a choice between true and false, we can use hp.Boolean() to tune
    this hyperparameter. As mentioned earlier, we can also tune the dropout rate,
    which determines the percentage of neurons to ignore during the training process.
    This is a floating-point value, so we’ll use hp.Float() to tune this hyperparameter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加更多超参数来完善我们的 MlpBlock，如列表 5.20 所示。例如，是否使用 dropout 层在实践中值得探索。因为它是一个真或假的选项，我们可以使用
    hp.Boolean() 来调整这个超参数。如前所述，我们还可以调整 dropout 率，这决定了在训练过程中要忽略的神经元百分比。这是一个浮点值，因此我们将使用
    hp.Float() 来调整这个超参数。
- en: Listing 5.20 Tuning more types of hyperparameters
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.20 调整更多类型的超参数
- en: '[PRE20]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Uses hp.Boolean to decide whether to use a dropout layer
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 hp.Boolean 来决定是否使用 dropout 层
- en: ❷ Uses hp.Float to decide on the dropout rate
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 hp.Float 来决定 dropout 率
- en: When using hp.Float(), note that we did not specify the step argument like in
    hp.Int(). The search algorithm will select a floating-point value in the continuous
    range.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 hp.Float() 时，请注意，我们没有像在 hp.Int() 中那样指定 step 参数。搜索算法将在连续范围内选择一个浮点值。
- en: Using the custom block to create an AutoML pipeline
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义块创建 AutoML 流程
- en: Now you know how to write a neural network block to define a customized search
    space. The next step is to connect it with other components (input node, output
    head, and other blocks) to create a full AutoML pipeline for tuning the relevant
    hyperparameters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何编写一个神经网络块来定义一个自定义的搜索空间。下一步是将它与其他组件（输入节点、输出头部和其他块）连接起来，以创建一个完整的 AutoML
    流程来调整相关的超参数。
- en: To make sure the block does not contain implementation bugs before the connection,
    you can write a simple test to see if it builds correctly, as shown in the next
    listing. The inputs can be a single Keras input node or a list of nodes. An hp
    container is created for testing purposes. You may also insert some print statements
    (or assertions) in the build() function to print out (or assert) some intermediate
    outputs to better test the block.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接之前确保块不包含实现错误，你可以编写一个简单的测试来查看它是否正确构建，如下一列表所示。输入可以是一个单一的 Keras 输入节点或节点列表。为了测试目的，创建了一个
    hp 容器。你还可以在 build() 函数中插入一些打印语句（或断言）以打印出（或断言）一些中间输出，以便更好地测试块。
- en: Listing 5.21 Testing the neural network block
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.21 测试神经网络块
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If the build() function of the block runs smoothly without errors, we can use
    this block just as we would use any of the built-in blocks in AutoKeras introduced
    in chapter 4\. Let’s use it to tune an MLP for a structured data regression task
    with a synthetic dataset. In the following listing, we first randomly generate
    a tabular dataset with 20 features, 100 training instances, and 100 test instances.
    We then connect the MlpBlock with the input node and the output regression head
    to create a full AutoML pipeline and initialize the AutoModel to conduct the search
    process on the training dataset.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果块的 build() 函数运行顺利且没有错误，我们可以像使用 AutoKeras 中第 4 章介绍的任何内置块一样使用这个块。让我们用它来调整一个用于结构化数据回归任务的
    MLP，并使用合成数据集。在下面的列表中，我们首先随机生成一个包含 20 个特征、100 个训练实例和 100 个测试实例的表格数据集。然后，我们将 MlpBlock
    与输入节点和输出回归头连接起来，创建一个完整的 AutoML 管道，并初始化 AutoModel 以在训练数据集上执行搜索过程。
- en: Listing 5.22 Building and fitting a model with the custom block
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.22 使用自定义块构建和拟合模型
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Generates the synthetic structured data for regression
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成用于回归的合成结构化数据
- en: ❷ Passes the input node to the custom block
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入节点传递给自定义块
- en: ❸ Passes the output node of the custom block to the regression head
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将自定义块的输出节点传递给回归头
- en: 'We can also print out the search space of the created AutoML pipeline, as shown
    in listing 5.23\. As you can see, it contains seven hyperparameters. Four of them
    are the ones we designed in the MlpBlock: number of layers, number of units, whether
    to use a dropout layer, and the dropout rate (mlp_block_1/dropout_rate). The other
    three are the dropout rate in the regression head, the optimization algorithm,
    and the learning rate of the optimization algorithm.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印出创建的 AutoML 管道的搜索空间，如列表 5.23 所示。正如你所见，它包含七个超参数。其中四个是我们设计在 MlpBlock 中的：层数、单元数、是否使用
    dropout 层以及 dropout 率（mlp_block_1/dropout_rate）。其余三个是回归头中的 dropout 率、优化算法以及优化算法的学习率。
- en: Listing 5.23 Printing out a summary of the search space
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.23 打印搜索空间的摘要
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The process for designing an MLP block is generalizable to designing an AutoML
    block for tuning any kind of neural network architecture that can be directly
    built up with Keras layers. You may want to create a custom block for tuning your
    architectures if the following two conditions are satisfied:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 设计 MLP 块的过程可以推广到设计用于调整任何可以直接用 Keras 层构建的神经网络架构的 AutoML 块。如果你满足以下两个条件，你可能想要创建一个自定义块来调整你的架构：
- en: There is no built-in block in AutoKeras that you can use to build up your AutoML
    pipeline.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoKeras 中没有内置的块可以直接用来构建你的 AutoML 管道。
- en: You know how to create the network architecture by stacking the Keras layers.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道如何通过堆叠 Keras 层来创建网络架构。
- en: 5.5.2 Designing a hyperblock for model selection
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 设计用于模型选择的超块
- en: Beyond tuning the hyperparameters of a single model, you may want to implement
    an AutoML block for model selection. In this section, you’ll learn how to implement
    your own hyperblock like the ones we used in chapter 4 for selecting among different
    models. Because different models can also have different hyperparameters, this
    is in fact a joint hyperparameter tuning and model selection task, which requires
    a hierarchical search space to show the relations between each model and its unique
    hyperparameters. We will first look at a simple case of model selection, where
    each model does not have hyperparameters to tune, and then learn to how to deal
    with the joint hyperparameter tuning and model selection.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整单个模型的超参数之外，你可能还想要实现一个用于模型选择的 AutoML 块。在本节中，你将学习如何实现自己的超块，就像我们在第 4 章中用于在不同模型之间进行选择的那样。因为不同的模型也可能有不同的超参数，这实际上是一个联合超参数调整和模型选择任务，需要分层搜索空间来展示每个模型及其独特超参数之间的关系。我们将首先查看一个简单的模型选择案例，其中每个模型都没有需要调整的超参数，然后学习如何处理联合超参数调整和模型选择。
- en: Selecting among different DenseNet models
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的 DenseNet 模型之间进行选择
- en: '*DenseNet* is a widely used type of CNN. It stacks multiple DenseNet cells
    together to build a complete neural network. The basic cell of a DenseNet is shown
    in figure 5.15\. In the cell, the input for each convolutional layer is the concatenation
    of all the output tensors of the previous convolutional layers within the same
    cell as well as the input tensor to the cell. The tensors are concatenated on
    their last dimension. For example, a tensor with shape (32, 32, 3) and a tensor
    with shape (32, 32, 16) can be concatenated to become (32, 32, 19). Extra pooling
    layers can be applied to reduce the size of the first two dimensions of the resulting
    tensor. Stacking different numbers of cells or different cell structures (e.g.,
    with different numbers of convolutional layers in each cell or a different number
    of filters in the convolutional layers) can result in different versions of the
    DenseNet. For more details on DenseNet, check out Ferlitsch’s book, *Deep Learning
    Design Patterns*.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*DenseNet* 是一种广泛使用的卷积神经网络（CNN）。它通过堆叠多个 DenseNet 单元来构建一个完整的神经网络。DenseNet 的基本单元如图
    5.15 所示。在该单元中，每个卷积层的输入是同一单元内所有先前卷积层的输出张量以及单元的输入张量的拼接。张量在其最后一个维度上进行拼接。例如，一个形状为
    (32, 32, 3) 的张量和一个形状为 (32, 32, 16) 的张量可以拼接成 (32, 32, 19)。可以通过应用额外的池化层来减少结果张量前两个维度的大小。堆叠不同数量的单元或不同的单元结构（例如，每个单元中卷积层的数量不同或卷积层中的过滤器数量不同）可以产生不同的
    DenseNet 版本。有关 DenseNet 的更多详细信息，请参阅 Ferlitsch 的著作 *深度学习设计模式*。'
- en: '![05-15](../Images/05-15.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![05-15](../Images/05-15.png)'
- en: Figure 5.15 A DenseNet cell
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 DenseNet 单元
- en: 'Because DenseNets are quite popular in image-related applications, several
    representative ones are implemented in Keras Applications, a TensorFlow Keras
    module that collects various widely used deep learning models. It contains functions
    for instantiating three versions of DenseNet: DenseNet121, DenseNet169, and DenseNet201,
    which are created by stacking different numbers of DenseNet cells (each of which
    has two convolutional layers). The numbers in the names indicate how many layers
    the final model contains. We can directly call these functions to use these models
    without implementing them by ourselves layer by layer. For example, to create
    a DenseNet121, we can call the function tf.keras.applications.DenseNet121, as
    shown in listing 5.24\. The function returns a Keras model. We can pass a NumPy
    array to it to see its output shape or directly call model.summary() to see the
    details of the model. For example, the model can take 100 synthetic images of
    shape 32×32×3\. The output is a tensor with shape (100, 1, 1, 1024) (see listing
    5.24). If we want the network to not include the classification head but only
    the convolutional layers of the neural network, we can use include_top=False in
    the arguments. By default, the weights in the network layers are initialized with
    weights that have been pretrained on larger datasets to improve the training speed
    and accuracy on the dataset at hand.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DenseNets 在图像相关应用中非常流行，因此有几个代表性的在 Keras Applications 中实现，这是一个 TensorFlow
    Keras 模块，它收集了各种广泛使用的深度学习模型。它包含实例化三种 DenseNet 版本（DenseNet121、DenseNet169 和 DenseNet201）的函数，这些版本是通过堆叠不同数量的
    DenseNet 单元（每个单元包含两个卷积层）创建的。名称中的数字表示最终模型包含的层数。我们可以直接调用这些函数来使用这些模型，而无需自己逐层实现。例如，要创建一个
    DenseNet121，我们可以调用函数 tf.keras.applications.DenseNet121，如图 5.24 列所示。该函数返回一个 Keras
    模型。我们可以传递一个 NumPy 数组给它以查看其输出形状，或者直接调用 model.summary() 来查看模型的详细信息。例如，该模型可以接受 100
    个形状为 32×32×3 的合成图像。输出是一个形状为 (100, 1, 1, 1024) 的张量（见 5.24 列）。如果我们想使网络不包含分类头，而只包含神经网络的卷积层，我们可以在参数中使用
    include_top=False。默认情况下，网络层的权重使用在大数据集上预训练的权重初始化，以提高在当前数据集上的训练速度和准确性。
- en: Listing 5.24 Building up a DenseNet model using a function in Keras Applications
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.24 使用 Keras Applications 中的函数构建 DenseNet 模型
- en: '[PRE24]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ A function to create a DenseNet model
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建 DenseNet 模型的函数
- en: ❷ This argument says to use only the convolution part of the network without
    the classification head.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 此参数表示只使用网络的卷积部分，而不使用分类头。
- en: ❸ This argument says to not use any pretrained weights.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 此参数表示不使用任何预训练权重。
- en: ❹ Feeds a synthetic input to the created DenseNet121 model and prints out the
    output shape
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将合成输入馈送到创建的 DenseNet121 模型，并打印输出形状
- en: Considering the different versions of DenseNet available in Keras Applications,
    you may be wondering how you determine which version is best for the task at hand.
    You can implement a custom AutoML block to select the best model with the help
    of the hp.Choice() function. In listing 5.25, we implement a DenseNetBlock to
    select among the three versions of DenseNet models. Note that we should not directly
    pass the models in Keras Applications to the hp.Choice() function because they
    are not serializable; we should always use the basic data types in Python, such
    as strings, Booleans, and numeric types, in the list we pass to Choice(). So,
    we use strings here to represent the three versions of DenseNet models, and we
    use an if statement to judge the selection and create the model. Feeding the input
    tensor to the created model results in an output tensor that can be fed into other
    AutoML blocks or classification/regression heads.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 Keras Applications 中可用的 DenseNet 的不同版本，你可能想知道如何确定哪个版本最适合当前任务。你可以通过使用 hp.Choice()
    函数实现一个自定义的 AutoML 块来选择最佳模型。在列表 5.25 中，我们实现了一个 DenseNetBlock 来在 DenseNet 模型的三个版本之间进行选择。请注意，我们不应直接将
    Keras Applications 中的模型传递给 hp.Choice() 函数，因为它们是不可序列化的；我们应该始终在传递给 Choice() 的列表中使用
    Python 的基本数据类型，如字符串、布尔值和数值类型。因此，我们在这里使用字符串来表示 DenseNet 模型的三个版本，并使用 if 语句来判断选择并创建模型。将输入张量输入到创建的模型中，将产生一个输出张量，该张量可以被输入到其他
    AutoML 块或分类/回归头部。
- en: Listing 5.25 Implementing a DenseNet block
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.25 实现一个 DenseNet 块
- en: '[PRE25]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Uses strings as the hyperparameter values for model selection because the
    functions are not serializable
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用字符串作为模型选择的超参数值，因为函数不可序列化
- en: ❷ Gets the model and calls it with the input tensor
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取模型并使用输入张量调用它
- en: Because the architectures included in Keras Applications are fixed, we cannot
    directly tune their hyperparameters (e.g., tuning the cell structure of the DenseNet121).
    But suppose that in addition to selecting the best model architecture, we want
    to tune the cell structure. In that case, we will need to define each DenseNet
    model ourselves, layer by layer, and specify the search space of the relevant
    hyperparameters, as we did for the MlpBlock. It’s feasible to create multiple
    AutoML blocks, each of which defines the search space for one type of DenseNet
    model. In this case, we can create a hyperblock to select among the different
    AutoML blocks and tune the hyperparameters in the selected block. The next section
    will show how to create a hyperblock to do joint model selection and hyperparameter
    tuning, leveraging the existing AutoML blocks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keras Applications 中包含的架构是固定的，我们无法直接调整它们的超参数（例如，调整 DenseNet121 的细胞结构）。但是，假设除了选择最佳模型架构之外，我们还想调整细胞结构。在这种情况下，我们将需要自己定义每个
    DenseNet 模型，一层一层地定义，并指定相关超参数的搜索空间，就像我们对 MlpBlock 所做的那样。创建多个 AutoML 块是可行的，每个块定义一种
    DenseNet 模型的搜索空间。在这种情况下，我们可以创建一个超块来在不同的 AutoML 块之间进行选择，并在所选块中调整超参数。下一节将展示如何创建一个超块来进行联合模型选择和超参数调整，利用现有的
    AutoML 块。
- en: Selecting between DenseNet and ResNet
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DenseNet 和 ResNet 之间进行选择
- en: We’ve created a DenseNetBlock to search among different DenseNet architectures.
    We also know that we have some built-in blocks in AutoKeras that can be used for
    image classification, such as the ResNetBlock we used in the section 5.3 for tuning
    a ResNet. Suppose we would like to select the best model, choosing between DenseNet
    and ResNet architectures. As listing 5.26 shows, we can take advantage of these
    existing blocks and create a hyperblock to select between the two model types,
    similar to how we created a normal AutoML block. The names of each block can be
    fed into the hp.Choice function as strings to define the search space for model
    selection. However, because the hyperblock is also an AutoML block, it should
    not directly return a selected AutoML block but instead return output tensors
    processed by Keras layers that can be used for other AutoML blocks. This requires
    us to call the build() function of each selected AutoML block to return its outputs.
    In other words, our hyperblock (SelectionBlock) should call the methods to create
    the other blocks as subroutines when building a search space. This also helps
    the hp container to collect all the search spaces defined in each block during
    the search process.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个DenseNetBlock来在不同的DenseNet架构中进行搜索。我们还知道在AutoKeras中内置了一些块，可以用于图像分类，例如我们在5.3节中用于调整ResNet的ResNetBlock。假设我们想要选择最佳模型，在DenseNet和ResNet架构之间进行选择。如列表5.26所示，我们可以利用这些现有块创建一个超块来在两种模型类型之间进行选择，类似于我们创建一个正常的AutoML块。每个块的名字可以作为字符串输入到hp.Choice函数中，以定义模型选择的搜索空间。然而，因为超块也是一个AutoML块，它不应该直接返回一个选定的AutoML块，而应该返回由Keras层处理过的输出张量，这些张量可以用于其他AutoML块。这要求我们调用每个选定的AutoML块的build()函数来返回其输出。换句话说，我们的超块（SelectionBlock）应该在构建搜索空间时将创建其他块的方法作为子例程调用。这也帮助hp容器在搜索过程中收集每个块中定义的所有搜索空间。
- en: Listing 5.26 Creating a model-selection block
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.26 创建模型选择块
- en: '[PRE26]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Defines the model_type hyperparameter for model selection
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义用于模型选择的model_type超参数
- en: The model_type hyperparameter used here is called a *conditional hyperparameter*,
    which means selecting the hyperparameters in the subroutine is conditioned on
    which model we select. For example, the hyperparameters in the DenseNetBlock will
    be selected only when the value of the model_type hyperparameter in the SelectionBlock
    is 'densenet'.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的model_type超参数被称为*条件超参数*，这意味着在子例程中选择的超参数取决于我们选择哪个模型。例如，当SelectionBlock中的model_type超参数的值为'densenet'时，才会选择DenseNetBlock中的超参数。
- en: A problem with conditional hyperparameters is that they can cause problems with
    the tuning algorithm. If we do not tell the tuning algorithm explicitly about
    conditional hyperparameters, it will lead to redundancy in searching wrong things
    and may affect the search performance. For example, the tuning algorithm may want
    to find an optimal value for the DenseNet version while 'resnet' is selected as
    the value for the model_type, even though changing the DenseNet version will not
    affect the model. To declare such an affiliation between the hyperparameters,
    we use the hp.conditional_scope() method to inform the tuning algorithm of the
    dependency. Any hyperparameter defined under the conditional scope will then be
    considered active only when the condition is met. For example, in listing 5.27,
    hp.conditional_ scope('model_type', ['densenet']) sets the condition that the
    model_type hyperparameter’s value should be 'densenet' to activate the scope.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 条件超参数的一个问题是它们可能会对调整算法造成问题。如果我们没有明确地告诉调整算法有关条件超参数，它会导致搜索错误事物的冗余，并可能影响搜索性能。例如，当model_type的值为'reset'时，调整算法可能想要找到一个DenseNet版本的优化值，即使改变DenseNet版本也不会影响模型。为了声明这种超参数之间的关联，我们使用hp.conditional_scope()方法通知调整算法依赖关系。任何在条件作用域下定义的超参数将在条件满足时才被视为活动状态。例如，在列表5.27中，hp.conditional_scope('model_type',
    ['densenet'])设置了条件，即model_type超参数的值应该是'densenet'以激活作用域。
- en: Listing 5.27 Creating a model-selection block with conditional scope
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.27 创建具有条件作用域的模型选择块
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Activates the scope only if the value of the model hyperparameter is 'densenet'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅当模型超参数的值为'densenet'时激活作用域
- en: ❷ All hyperparameters in the build function of DenseNet are under this scope.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ DenseNet的build函数中的所有超参数都处于此作用域之下。
- en: Now that we’ve created our hyperblock, we can build up a complete AutoML pipeline
    for joint model selection and hyperparameter tuning. In listing 5.28, we pass
    an ImageInput node to our SelectionBlock and connect it with a ClassificationHead
    to select the best model from the search space for the image classification task
    on the CIFAR-10 dataset. We can also check out the entire search space by printing
    a summary by calling the function search_space_summary().
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的超块，我们可以构建一个完整的AutoML流水线，用于联合模型选择和超参数调整。在列表5.28中，我们将一个ImageInput节点传递给我们的SelectionBlock，并通过与ClassificationHead连接来选择CIFAR-10数据集上图像分类任务搜索空间中的最佳模型。我们还可以通过调用search_space_summary()函数打印一个摘要来查看整个搜索空间。
- en: Listing 5.28 Building the model and conducting the search
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.28 构建模型和进行搜索
- en: '[PRE28]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Sets the number of epochs to 1 to run faster for illustrative purposes
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将epoch数设置为1以加快演示速度
- en: ❷ Prints out the search space of the created AutoML pipeline
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印出创建的AutoML流水线的搜索空间
- en: Now that you know how to create your own custom AutoML blocks and hyperblocks
    for tuning deep learning models on classification and regression tasks, in the
    next chapter, we’ll shift gears and look at defining the search space without
    connecting a series of these blocks. This will give you a more flexible way of
    designing the search space for a broader range of AutoML tasks, such as tuning
    unsupervised learning models, tuning the optimization algorithm or the loss function,
    and jointly selecting deep learning and shallow models.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何创建自己的自定义AutoML块和超块，用于调整分类和回归任务上的深度学习模型，在下一章中，我们将转换方向，探讨在不连接一系列这些块的情况下定义搜索空间。这将为你提供一种更灵活的方式来设计适用于更广泛AutoML任务的搜索空间，例如调整无监督学习模型，调整优化算法或损失函数，以及联合选择深度学习和浅层模型。
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'An AutoML pipeline can be regarded as a search space of ML pipelines. You can
    create pipelines with the AutoKeras functional API by stacking four components:
    input nodes, preprocessor blocks, network blocks, and output heads.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML流水线可以被视为ML流水线的搜索空间。你可以通过堆叠四个组件来使用AutoKeras功能API创建流水线：输入节点、预处理块、网络块和输出头。
- en: AutoKeras has multiple built-in preprocessor blocks and network blocks. Each
    preprocessor block represents a specific preprocessing method and the search space
    of its hyperparameters. Each network block represents a particular type of model,
    such as MLPs or CNNs, and the default search space of the model’s hyperparameters,
    such as the number of layers and units in an MLP. You can use these blocks to
    build the AutoML pipeline and tune the relevant hyperparameters by customizing
    their search spaces while fixing others.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoKeras包含多个内置的预处理块和网络块。每个预处理块代表一种特定的预处理方法及其超参数的搜索空间。每个网络块代表一种特定的模型类型，例如MLP或CNN，以及模型超参数的默认搜索空间，例如MLP中的层数和单元数。你可以使用这些块来构建AutoML流水线，并通过自定义它们的搜索空间来调整相关的超参数，同时固定其他参数。
- en: A hyperblock is a type of AutoML block that enables selection among different
    kinds of models and preprocessing methods. AutoKeras contains three hyperblocks,
    for image, text, and structured data, to help you create AutoML pipelines for
    joint model selection and hyperparameter tuning.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超块是一种AutoML块，它允许在多种模型和预处理方法之间进行选择。AutoKeras包含三个超块，用于图像、文本和结构化数据，以帮助你创建用于联合模型选择和超参数调整的AutoML流水线。
- en: The AutoML pipeline can be generalized from a sequential structure to a graph
    structure to tune models with multiple inputs and outputs or pipelines containing
    an ensemble of preprocessing methods or models. You can follow the data flow to
    create the pipeline with the AutoKeras functional API, setting up each AutoML
    block in turn, based on the order in which they appear in the graph.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML流水线可以从顺序结构泛化到图结构，以调整具有多个输入和输出的模型或包含预处理方法或模型集成（ensemble）的流水线。你可以遵循数据流，使用AutoKeras功能API创建流水线，依次设置每个AutoML块，基于它们在图中的出现顺序。
- en: You can create a custom AutoML block containing the search space of your own
    model and connect it with the built-in blocks in AutoKeras. You can also set up
    a conditional search space in an AutoML block for model selection.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以创建一个包含你自己的模型搜索空间的自定义AutoML块，并将其与AutoKeras中的内置块连接起来。你还可以在AutoML块中设置一个条件搜索空间以进行模型选择。
- en: '* * *'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) See Kaiming He et al., “Deep Residual Learning for Image Recognition,”
    available at [https://arxiv.org/ pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) 请参阅 Kaiming He 等人撰写的“用于图像识别的深度残差学习”，可在[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)找到。
- en: '^(2.) See “Xception: Deep Learning with Depthwise Separable Convolutions” by
    François Chollet, available at [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '(2.) 请参阅 François Chollet 撰写的“Xception: 基于深度学习的深度可分离卷积”，可在[https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)找到。'
- en: ^(3.) Described by David Wolpert in “The Lack of A Priori Distinctions between
    Learning Algorithms,” available at [http://mng.bz/xvde](http://mng.bz/xvde).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: (3.) 由 David Wolpert 在“学习算法之间缺乏先验区分”一文中描述，可在[http://mng.bz/xvde](http://mng.bz/xvde)找到。
- en: ^(4.) See “Identity Mappings in Deep Residual Networks” by Kaiming He et al.,
    available at [https://arxiv.org/ abs/1603.05027](https://arxiv.org/abs/1603.05027).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (4.) 请参阅 Kaiming He 等人撰写的“深度残差网络中的恒等映射”，可在[https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)找到。
