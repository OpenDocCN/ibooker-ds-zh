- en: 'Chapter 8\. Unsupervised Methods: Topic Modeling and Clustering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 无监督方法：主题建模和聚类
- en: When working with a large number of documents, one of the first questions you
    want to ask without reading all of them is “What are they talking about?” You
    are interested in the general topics of the documents, i.e., which (ideally semantic)
    words are often used together.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量文档时，您想要在不阅读所有文档的情况下首先问的问题之一是“它们在谈论什么？”您对文档的主题感兴趣，即文档中经常一起使用的（理想情况下是语义的）单词。
- en: Topic modeling tries to solve that challenge by using statistical techniques
    for finding out topics from a corpus of documents. Depending on your vectorization
    (see [Chapter 5](ch05.xhtml#ch-vectorization)), you might find different kinds
    of topics. Topics consist of a probability distribution of features (words, n-grams,
    etc.).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模试图通过使用统计技术从文档语料库中找出主题来解决这个挑战。根据您的向量化（见第5章），您可能会发现不同类型的主题。主题由特征（单词、n-gram等）的概率分布组成。
- en: 'Topics normally overlap with each other; they are not clearly separated. The
    same is true for documents: it is not possible to assign a document uniquely to
    a single topic; a document always contains a mixture of different topics. The
    aim of topic modeling is not primarily to assign a topic to an arbitrary document
    but to find the global structure of the corpus.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主题通常彼此重叠；它们并不明确分开。文档也是如此：不可能将文档唯一地分配给一个主题；文档始终包含不同主题的混合体。主题建模的目的不是将主题分配给任意文档，而是找出语料库的全局结构。
- en: Often, a set of documents has an explicit structure that is given by categories,
    keywords, and so on. If we want to take a look at the organic composition of the
    corpus, then topic modeling will help a lot to uncover the latent structure.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一组文档具有由类别、关键词等确定的显式结构。如果我们想要查看语料库的有机构成，那么主题建模将对揭示潜在结构有很大帮助。
- en: Topic modeling has been known for a long time and has gained immense popularity
    during the last 15 years, mainly due to the invention of LDA,^([1](ch08.xhtml#idm45634189170488))
    a stochastic method for discovering topics. LDA is flexible and allows many modifications.
    However, it is not the only method for topic modeling (although you might believe
    this by looking at the literature, much of which is biased toward LDA). Conceptually
    simpler methods are non-negative matrix factorization, singular-value decomposition
    (sometimes called *LSI*), and a few others.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模已经被人们熟知很长一段时间，并在过去的15年中获得了巨大的流行，这主要归因于LDA（一种用于发现主题的随机方法）的发明。LDA灵活多变，允许进行许多修改。然而，它并不是主题建模的唯一方法（尽管通过文献，您可能会认为它是唯一的，因为很多文献都倾向于LDA）。概念上更简单的方法包括非负矩阵分解、奇异值分解（有时称为LSI）等。
- en: What You’ll Learn and What We’ll Build
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么以及我们将构建什么
- en: In this chapter, we will take an in-depth look at the various methods of topic
    modeling, try to find differences and similarities between the methods, and run
    them on the same use case. Depending on your requirements, it might also be a
    good idea to not only try a single method but compare the results of a few.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将深入研究各种主题建模方法，试图找到这些方法之间的差异和相似之处，并在同一个用例上运行它们。根据您的需求，尝试单一方法可能是个不错的主意，但比较几种方法的结果也是一个好选择。
- en: After studying this chapter, you will know the different methods of topic modeling
    and their specific advantages and drawbacks. You will understand how topic modeling
    can be applied not only to find topics but also to create quick summaries of document
    corpora. You will learn about the importance of choosing the correct granularity
    of entities for calculating topic models. You have experimented with many parameters
    to find the optimal topic model. You are able to judge the quality of the resulting
    topic models by quantitative methods and numbers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了本章后，您将了解到不同的主题建模方法及其特定的优缺点。您将了解到主题建模不仅可以用于发现主题，还可以用于快速创建文档语料库的摘要。您将学会选择正确的实体粒度来计算主题模型的重要性。您已经通过许多参数实验找到了最佳的主题模型。您可以通过数量方法和数据来评判生成的主题模型的质量。
- en: 'Our Dataset: UN General Debates'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的数据集：联合国大会辩论
- en: Our use case is to semantically analyze the corpus of the UN general debates.
    You might know this dataset from the earlier chapter about text statistics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用例是语义分析联合国大会辩论语料库。您可能从早期关于文本统计的章节了解过这个数据集。
- en: 'This time we are more interested in the meaning and in the semantic content
    of the speeches and how we can arrange them topically. We want to know what the
    speakers are talking about and answer questions like these: Is there a structure
    in the document corpus? What are the topics? Which of them is most prominent?
    Does this change over time?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们更感兴趣的是演讲的含义和语义内容，以及我们如何将它们按主题排列。我们想知道演讲者在谈论什么，并回答这样的问题：文档语料库中是否有结构？有哪些主题？哪一个最突出？这种情况随时间而变化吗？
- en: Checking Statistics of the Corpus
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查语料库的统计数据
- en: Before starting with topic modeling, it is always a good idea to check the statistics
    of the underlying text corpus. Depending on the results of this analysis, you
    will often choose to analyze different entities, e.g., documents, sections, or
    paragraphs of text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始主题建模之前，检查底层文本语料库的统计数据总是一个好主意。根据此分析的结果，您通常会选择分析不同的实体，例如文档、部分文本或段落。
- en: 'We are not so much interested in authors and additional information, so it’s
    enough to work with one of the supplied *CSV* files:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对作者和其他信息不是很感兴趣，因此只需处理提供的一个 *CSV* 文件即可：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result looks fine. There are no null values in the text column; we might
    use years and countries later, and they also have only non-null values.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来不错。文本列中没有空值；我们可能稍后会使用年份和国家，它们也只有非空值。
- en: 'The speeches are quite long and cover a lot of topics as each country is allowed
    only to deliver a single speech per year. Different parts of the speeches are
    almost always separated by paragraphs. Unfortunately, the dataset has some formatting
    issues. Compare the text of two selected speeches:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 演讲非常长，涵盖了许多主题，因为每个国家每年只能发表一次演讲。演讲的不同部分几乎总是由段落分隔。不幸的是，数据集存在一些格式问题。比较两篇选定演讲的文本：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see, in some speeches the newline character is used to separate
    paragraphs. In the transcription of other speeches, a newline is used to separate
    lines. To recover the paragraphs, we therefore cannot just split at newlines.
    It turns out that splitting at stops, exclamation points, or question marks occurring
    at line ends works well enough. We ignore spaces after the stops:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，在一些演讲中，换行符用于分隔段落。在其他演讲的转录中，换行符用于分隔行。因此，为了恢复段落，我们不能只是在换行符处拆分。事实证明，在行尾出现的句号、感叹号或问号处拆分效果也很好。我们忽略停止后的空格：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From the analysis in [Chapter 2](ch02.xhtml#ch-api), we already know that the
    number of speeches per year does not change much. Is this also true for the number
    of paragraphs?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [第2章](ch02.xhtml#ch-api) 中的分析，我们已经知道每年的演讲数量变化不大。段落数量也是这样吗？
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_08in01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in01.jpg)'
- en: The average number of paragraphs has dropped considerably over time. We would
    have expected that, as the number of speakers per year increased and the total
    time for speeches is limited.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 段落平均数量随时间显著减少。我们本应该预期，随着每年演讲者人数的增加和演讲总时间的限制。
- en: Apart from that, the statistical analysis shows no systematic problems with
    the dataset. The corpus is still quite up-to-date; there is no missing data for
    any year. We can now safely start with uncovering the latent structure and detect
    topics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，统计分析显示数据集没有系统性问题。语料库仍然很新；任何一年都没有缺失数据。我们现在可以安全地开始揭示潜在结构并检测主题。
- en: Preparations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Topic modeling is a machine learning method and needs vectorized data. All topic
    modeling methods start with the document-term matrix. Recalling the meaning of
    this matrix (which was introduced in [Chapter 4](ch04.xhtml#ch-preparation)),
    its elements are word frequencies (or often scaled as TF-IDF weights) of the words
    (columns) in the corresponding documents (rows). The matrix is sparse, as most
    documents contain only a small fraction of the vocabulary.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种机器学习方法，需要矢量化数据。所有主题建模方法都从文档-术语矩阵开始。回顾这个矩阵的含义（它在 [第4章](ch04.xhtml#ch-preparation)
    中介绍过），其元素是对应文档（行）中单词（列）的词频（或经常作为 TF-IDF 权重进行缩放）。该矩阵是稀疏的，因为大多数文档只包含词汇的一小部分。
- en: 'Let’s calculate the TF-IDF matrix both for the speeches and for the paragraphs
    of the speeches. First, we have to import the necessary packages from scikit-learn.
    We start with a naive approach and use the standard spaCy stop words:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算演讲和演讲段落的 TF-IDF 矩阵。首先，我们需要从 scikit-learn 中导入必要的包。我们从一个简单的方法开始，使用标准的 spaCy
    停用词：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Calculating the document-term matrix for the speeches is easy; we also include
    bigrams:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 计算演讲的文档-术语矩阵很容易；我们还包括二元组：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Out:`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For the paragraphs, it’s a bit more complicated as we have to flatten the list
    first. In the same step, we omit empty paragraphs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于段落来说，稍微复杂一些，因为我们首先必须展平列表。在同一步骤中，我们省略空段落：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Of course, the paragraph matrix has many more rows. The number of columns (words)
    is also different because `min_df` and `max_df` have an effect in selecting features,
    as the number of documents has changed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，段落矩阵的行数要多得多。列数（单词数）也不同，因为 `min_df` 和 `max_df` 在选择特征时有影响，文档的数量也已经改变。
- en: Nonnegative Matrix Factorization (NMF)
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）
- en: 'The conceptually easiest way to find a latent structure in the document corpus
    is the factorization of the document-term matrix. Fortunately, the document-term
    matrix has only positive-value elements; therefore, we can use methods from linear
    algebra that allow us to represent the [matrix as the product of two other nonnegative
    matrices](https://oreil.ly/JVpFA). Conventionally, the original matrix is called
    *V*, and the factors are *W* and *H*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档语料库中找到潜在结构的概念上最简单的方法是对文档-术语矩阵进行因子分解。幸运的是，文档-术语矩阵只有正值元素；因此，我们可以使用线性代数中允许我们表示[矩阵为两个其他非负矩阵的乘积的方法](https://oreil.ly/JVpFA)。按照惯例，原始矩阵称为
    *V*，而因子是 *W* 和 *H*：
- en: <math alttext="normal upper V almost-equals normal upper W dot normal upper
    H"><mrow><mi mathvariant="normal">V</mi> <mo>≈</mo> <mi mathvariant="normal">W</mi>
    <mo>·</mo> <mi mathvariant="normal">H</mi></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper V almost-equals normal upper W dot normal upper
    H"><mrow><mi mathvariant="normal">V</mi> <mo>≈</mo> <mi mathvariant="normal">W</mi>
    <mo>·</mo> <mi mathvariant="normal">H</mi></mrow></math>
- en: Or we can represent it graphically (visualizing the dimensions necessary for
    matrix multiplication), as in [Figure 8-1](#nmf-decomposition).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以以图形方式表示它（可视化进行矩阵乘法所需的维度），如[图 8-1](#nmf-decomposition)所示。
- en: Depending on the dimensions, the factorization can be performed exactly. But
    as this is so much more computationally expensive, an approximate factorization
    is sufficient.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维度的不同，可以执行精确的因子分解。但由于这样做计算成本更高，近似因子分解已经足够。
- en: '![](Images/btap_0801.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0801.jpg)'
- en: Figure 8-1\. Schematic nonnegative matrix factorization; the original matrix
    V is decomposed into W and H.
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 概要的非负矩阵分解；原始矩阵 V 被分解为 W 和 H。
- en: In the context of text analytics, both *W* and *H* have an interpretation. The
    matrix *W* has the same number of rows as the document-term matrix and therefore
    maps documents to topics (document-topic matrix). *H* has the same number of columns
    as features, so it shows how the topics are constituted of features (topic-feature
    matrix). The number of topics (the columns of *W* and the rows of *H*) can be
    chosen arbitrarily. The smaller this number, the less exact the factorization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分析的背景下，*W* 和 *H* 都有一个解释。矩阵 *W* 的行数与文档-术语矩阵相同，因此将文档映射到主题（文档-主题矩阵）。*H* 的列数与特征数相同，因此显示了主题由特征构成的方式（主题-特征矩阵）。主题的数量（*W*
    的列数和 *H* 的行数）可以任意选择。这个数字越小，因子分解的精确度就越低。
- en: 'Blueprint: Creating a Topic Model Using NMF for Documents'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用 NMF 创建文档的主题模型
- en: 'It’s really easy to perform this decomposition for speeches in scikit-learn.
    As (almost) all topic models need the number of topics as a parameter, we arbitrarily
    choose 10 topics (which will later turn out to be a good choice):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中为演讲执行此分解真的很容易。由于（几乎）所有主题模型都需要主题数量作为参数，我们任意选择了 10 个主题（后来证明这是一个很好的选择）：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Similar to the `TfidfVectorizer`, NMF also has a `fit_transform` method that
    returns one of the positive factor matrices. The other factor can be accessed
    by the `components_` member variable of the NMF class.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `TfidfVectorizer` 类似，NMF 也有一个 `fit_transform` 方法，返回其中一个正因子矩阵。可以通过 NMF 类的 `components_`
    成员变量访问另一个因子。
- en: 'Topics are word distributions. We are now going to analyze this distribution
    and see whether we can find an interpretation of the topics. Taking a look at [Figure 8-1](#nmf-decomposition),
    we need to consider the *H* matrix and find the index of the largest values in
    each row (topic) that we then use as a lookup index in the vocabulary. As this
    is helpful for all topic models, we define a function for outputting a summary:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是单词分布。我们现在将分析这个分布，看看我们是否可以找到主题的解释。看看[图 8-1](#nmf-decomposition)，我们需要考虑 *H*
    矩阵，并找到每行（主题）中最大值的索引，然后将其用作词汇表中的查找索引。因为这对所有主题模型都有帮助，我们定义一个输出摘要的函数：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Calling this function, we get a nice summary of the topics that NMF detected
    in the speeches (the numbers are the percentage contributions of the words to
    the respective topic):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此函数，我们可以得到 NMF 在演讲中检测到的主题的良好总结（数字是单词对各自主题的百分比贡献）：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Out:`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| **Topic 00** co (0.79)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** co (0.79)'
- en: operation (0.65)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 操作 (0.65)
- en: disarmament (0.36)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军 (0.36)
- en: nuclear (0.34)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 核 (0.34)
- en: relations (0.25) | **Topic 01** terrorism (0.38)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关系 (0.25) | **主题 01** 恐怖主义 (0.38)
- en: challenges (0.32)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战 (0.32)
- en: sustainable (0.30)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续 (0.30)
- en: millennium (0.29)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 千年 (0.29)
- en: reform (0.28) | **Topic 02** africa (1.15)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 改革 (0.28) | **主题 02** 非洲 (1.15)
- en: african (0.82)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲 (0.82)
- en: south (0.63)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 南 (0.63)
- en: namibia (0.36)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 纳米比亚 (0.36)
- en: delegation (0.30) | **Topic 03** arab (1.02)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 代表团 (0.30) | **主题 03** 阿拉伯 (1.02)
- en: israel (0.89)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以色列 (0.89)
- en: palestinian (0.60)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 巴勒斯坦的 (0.60)
- en: lebanon (0.54)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 黎巴嫩 (0.54)
- en: israeli (0.54) | **Topic 04** american (0.33)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以色列的 (0.54) | **主题 04** 美国的 (0.33)
- en: america (0.31)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 美国 (0.31)
- en: latin (0.31)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 拉丁 (0.31)
- en: panama (0.21)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 巴拿马 (0.21)
- en: bolivia (0.21) |
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 玻利维亚 (0.21) |
- en: '| **Topic 05** pacific (1.55)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 太平洋 (1.55)'
- en: islands (1.23)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 岛屿 (1.23)
- en: solomon (0.86)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 索罗门 (0.86)
- en: island (0.82)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 岛屿 (0.82)
- en: fiji (0.71) | **Topic 06** soviet (0.81)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 斐济 (0.71) | **主题 06** 苏联 (0.81)
- en: republic (0.78)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 共和国 (0.78)
- en: nuclear (0.68)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 核 (0.68)
- en: viet (0.64)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 越南 (0.64)
- en: socialist (0.63) | **Topic 07** guinea (4.26)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 社会主义 (0.63) | **主题 07** 几内亚 (4.26)
- en: equatorial (1.75)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 赤道 (1.75)
- en: bissau (1.53)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 比绍 (1.53)
- en: papua (1.47)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 巴布亚 (1.47)
- en: republic (0.57) | **Topic 08** european (0.61)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 共和国 (0.57) | **主题 08** 欧洲 (0.61)
- en: europe (0.44)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲 (0.44)
- en: cooperation (0.39)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 合作 (0.39)
- en: bosnia (0.34)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 波斯尼亚 (0.34)
- en: herzegovina (0.30) | **Topic 09** caribbean (0.98)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔采哥维纳 (0.30) | **主题 09** 加勒比 (0.98)
- en: small (0.66)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 小 (0.66)
- en: bahamas (0.63)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 巴哈马 (0.63)
- en: saint (0.63)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 圣 (0.63)
- en: barbados (0.61) |
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 巴巴多斯 (0.61) |
- en: Topic 00 and Topic 01 look really promising as people are talking about nuclear
    disarmament and terrorism. These are definitely real topics in the UN general
    debates.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 主题 00 和 主题 01 看起来非常有前景，因为人们正在讨论核裁军和恐怖主义。这些确实是联合国大会辩论中的真实主题。
- en: The subsequent topics, however, are more or less focused on different regions
    of the world. This is due to speakers mentioning primarily their own country and
    neighboring countries. This is especially evident in Topic 03, which reflects
    the conflict in the Middle East.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，后续主题或多或少集中在世界不同地区。这是因为演讲者主要提到自己的国家和邻国。这在主题 03 中特别明显，反映了中东的冲突。
- en: It’s also interesting to take a look at the percentages with which the words
    contribute to the topics. Due to the large number of words, the individual contributions
    are quite small, except for *guinea* in Topic 07\. As we will see later, the percentages
    of the words within a topic are a good indication for the quality of the topic
    model. If the percentage within a topic is rapidly decreasing, the topic is well-defined,
    whereas slowly decreasing word probabilities indicate a less-pronounced topic.
    It’s much more difficult to intuitively find out how well the topics are separated;
    we will take a look at that later.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 查看单词在主题中的贡献百分比也很有趣。由于单词数量众多，单个贡献相当小，除了主题 07 中的*几内亚*。正如我们后面将看到的，单词在主题内的百分比是主题模型质量的一个很好的指标。如果主题内的百分比迅速下降，则表明该主题定义良好，而缓慢下降的单词概率表明主题不太明显。直觉上找出主题分离得有多好要困难得多；我们稍后将进行审视。
- en: 'It would be interesting to find out how “big” the topics are, i.e., how many
    documents could be assigned mainly to each topic. This can be calculated using
    the document-topic matrix and summing the individual topic contributions over
    all documents. Normalizing them with the total sum and multiplying by 100 gives
    a percentage value:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 发现“大”主题有多大将会很有趣，即每个主题主要可以分配给多少篇文档。可以通过文档-主题矩阵计算，并对所有文档中的各个主题贡献求和来计算这一点。将它们与总和归一化，并乘以100给出一个百分比值：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can easily see that there are smaller and larger topics but basically no
    outliers. Having an even distribution is a quality indicator. If your topic models
    have, for example, one or two large topics compared to all the others, you should
    probably adjust the number of topics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，有较小和较大的主题，但基本上没有离群值。具有均匀分布是质量指标。例如，如果你的主题模型中有一两个大主题与其他所有主题相比，你可能需要调整主题数量。
- en: In the next section, we will use the paragraphs of the speeches as entities
    for topic modeling and try to find out if that improves the topics.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用演讲段落作为主题建模的实体，并尝试找出是否改进了主题。
- en: 'Blueprint: Creating a Topic Model for Paragraphs Using NMF'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用NMF创建段落的主题模型
- en: In UN general debates, as in many other texts, different topics are often mixed,
    and it is hard for the topic modeling algorithm to find a common topic of an individual
    speech. Especially in longer texts, it happens quite often that documents do not
    cover just one but several topics. How can we deal with that? One idea is to find
    smaller entities in the documents that are more coherent from a topic perspective.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合国的一般辩论中，以及许多其他文本中，不同的主题通常会混合，这使得主题建模算法难以找到个别演讲的共同主题。特别是在较长的文本中，文档往往涵盖多个而不仅仅是一个主题。我们如何处理这种情况？一种想法是在文档中找到更具主题一致性的较小实体。
- en: 'In our corpus, paragraphs are a natural subdivision of speeches, and we can
    assume that the speakers try to stick to one topic within one paragraph. In many
    documents, paragraphs are a good candidate (if they can be identified as such),
    and we have already prepared the corresponding TF-IDF vectors. Let’s try to calculate
    their topic models:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的语料库中，段落是演讲的自然分割，我们可以假设演讲者在一个段落内试图坚持一个主题。在许多文档中，段落是一个很好的候选对象（如果可以识别），我们已经准备好了相应的TF-IDF向量。让我们尝试计算它们的主题模型：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our `display_topics` function developed earlier can be used to find the content
    of the topics:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前开发的`display_topics`函数可以用来找到主题的内容：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`Out:`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| **Topic 00** nations (5.63)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 国家 (5.63)'
- en: united (5.52)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: united (5.52)
- en: organization (1.27)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 组织 (1.27)
- en: states (1.03)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 州 (1.03)
- en: charter (0.93) | **Topic 01** general (2.87)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 宪章 (0.93) | **主题 01** 总的 (2.87)
- en: session (2.83)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 会议 (2.83)
- en: assembly (2.81)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 大会 (2.81)
- en: mr (1.98)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 先生 (1.98)
- en: president (1.81) | **Topic 02** countries (4.44)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 主席 (1.81) | **主题 02** 国家 (4.44)
- en: developing (2.49)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 发展 (2.49)
- en: economic (1.49)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 经济 (1.49)
- en: developed (1.35)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 发展 (1.35)
- en: trade (0.92) | **Topic 03** people (1.36)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 贸易 (0.92) | **主题 03** 人民 (1.36)
- en: peace (1.34)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 和平 (1.34)
- en: east (1.28)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 东 (1.28)
- en: middle (1.17)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 中 (1.17)
- en: palestinian (1.14) | **Topic 04** nuclear (4.93)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 巴勒斯坦 (1.14) | **主题 04** 核 (4.93)
- en: weapons (3.27)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 武器 (3.27)
- en: disarmament (2.01)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军 (2.01)
- en: treaty (1.70)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 条约 (1.70)
- en: proliferation (1.46) |
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散 (1.46) |
- en: '| **Topic 05** rights (6.49)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 权利 (6.49)'
- en: human (6.18)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 人类 (6.18)
- en: respect (1.15)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尊重 (1.15)
- en: fundamental (0.86)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基础 (0.86)
- en: universal (0.82) | **Topic 06** africa (3.83)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 全球 (0.82) | **主题 06** 非洲 (3.83)
- en: south (3.32)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 南 (3.32)
- en: african (1.70)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲 (1.70)
- en: namibia (1.38)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 纳米比亚 (1.38)
- en: apartheid (1.19) | **Topic 07** security (6.13)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 种族隔离 (1.19) | **主题 07** 安全 (6.13)
- en: council (5.88)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 理事会 (5.88)
- en: permanent (1.50)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 永久 (1.50)
- en: reform (1.48)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 改革 (1.48)
- en: peace (1.30) | **Topic 08** international (2.05)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 和平 (1.30) | **主题 08** 国际 (2.05)
- en: world (1.50)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 世界 (1.50)
- en: community (0.92)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 共同体 (0.92)
- en: new (0.77)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 新 (0.77)
- en: peace (0.67) | **Topic 09** development (4.47)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 和平 (0.67) | **主题 09** 发展 (4.47)
- en: sustainable (1.18)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续 (1.18)
- en: economic (1.07)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 经济 (1.07)
- en: social (1.00)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 社会 (1.00)
- en: goals (0.93) |
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 目标 (0.93) |
- en: Compared to the previous results for topic modeling speeches, we have almost
    lost all countries or regions except for South Africa and the Middle East. These
    are due to the regional conflicts that sparked interest in other parts of the
    world. Topics in the paragraphs like “Human rights,” “international relations,”
    “developing countries,” “nuclear weapons,” “security council,” “world peace,”
    and “sustainable development” (the last one probably occurring only lately) look
    much more reasonable compared to the topics of the speeches. Taking a look at
    the percentage values of the words, we can observe that they are dropping much
    faster, and the topics are more pronounced.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前用于演讲主题建模的结果相比，我们几乎失去了所有国家或地区，除了南非和中东地区。这些都是由于引发了世界其他地区兴趣的地区冲突。段落中的主题如“人权”，“国际关系”，“发展中国家”，“核武器”，“安理会”，“世界和平”和“可持续发展”（最后一个可能只是最近才出现）与演讲的主题相比显得更加合理。观察单词的百分比值，我们可以看到它们下降得更快，主题更加显著。
- en: Latent Semantic Analysis/Indexing
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在语义分析/索引
- en: Another algorithm for performing topic modeling is based on the so-called singular
    value decomposition (SVD), another method from linear algebra.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种执行主题建模的算法是基于所谓的奇异值分解（SVD），这是线性代数中的另一种方法。
- en: Graphically, it is possible to conceive SVD as rearranging documents and words
    in a way to uncover a block structure in the document-term matrix. There is a
    nice visualization of that process at [topicmodels.info](https://oreil.ly/yJnWL).
    [Figure 8-2](#svd-animation) shows the start of the document-term matrix and the
    resulting block diagonal form.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，我们可以将奇异值分解（SVD）视为以一种方式重新排列文档和单词，以揭示文档-词矩阵中的块结构。在[topicmodels.info](https://oreil.ly/yJnWL)有一个这个过程的良好可视化。[图8-2](#svd-animation)显示了文档-词矩阵的开始和最终的块对角形式。
- en: 'Making use of the principal axis theorem, orthogonal *n* × *n* matrices have
    an eigenvalue decomposition. Unfortunately, we do not have orthogonal square document-term
    matrices (except for rare cases). Therefore, we need a generalization called *singular
    value decomposition*. In its most general form, the theorem states that any *m*
    × *n* matrix **V **can be decomposed as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 利用主轴定理，正交 *n* × *n* 矩阵有一个特征值分解。不幸的是，我们没有正交的方形文档-词矩阵（除了少数情况）。因此，我们需要一种称为*奇异值分解*的泛化。在其最一般的形式中，该定理表明任何
    *m* × *n* 矩阵**V **都可以分解如下：
- en: <math alttext="normal upper V equals normal upper U dot normal upper Sigma dot
    normal upper V Superscript asterisk"><mrow><mi mathvariant="normal">V</mi> <mo>=</mo>
    <mi mathvariant="normal">U</mi> <mo>·</mo> <mi>Σ</mi> <mo>·</mo> <msup><mi mathvariant="normal">V</mi>
    <mo>*</mo></msup></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper V equals normal upper U dot normal upper Sigma dot
    normal upper V Superscript asterisk"><mrow><mi mathvariant="normal">V</mi> <mo>=</mo>
    <mi mathvariant="normal">U</mi> <mo>·</mo> <mi>Σ</mi> <mo>·</mo> <msup><mi mathvariant="normal">V</mi>
    <mo>*</mo></msup></mrow></math>
- en: '![](Images/btap_0802.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0802.jpg)'
- en: Figure 8-2\. Visualization of topic modeling with SVD.
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 使用 SVD 进行主题建模的可视化。
- en: '*U* is a unitary *m* × *m* matrix, *V** is an *n* × *n* matrix, and *Σ* is
    an *m* × *n* diagonal matrix containing the singular values. There are exact solutions
    for this equation, but as they take a lot of time and computational effort to
    find, we are looking for approximate solutions that can be found quickly. The
    approximation works by only considering the largest singular values. This leads
    to *Σ* becoming a *t* × *t* matrix; in turn, *U* has *m* × *t* and *V** *t* ×
    *n* dimensions. Graphically, this is similar to the nonnegative matrix factorization,
    as shown in [Figure 8-3](#svd-decomposition).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* 是一个单位 *m* × *m* 矩阵，*V** 是一个 *n* × *n* 矩阵，*Σ* 是一个 *m* × *n* 对角矩阵，其中包含奇异值。对于这个方程，有确切的解，但是由于它们需要大量的时间和计算工作来找到，所以我们正在寻找可以快速找到的近似解。这个近似方法仅考虑最大的奇异值。这导致*Σ*成为一个
    *t* × *t* 矩阵；相应地，*U*有 *m* × *t* 和 *V** 有 *t* × *n* 的维度。从图形上看，这类似于非负矩阵分解，如[图 8-3](#svd-decomposition)所示。'
- en: '![](Images/btap_0803.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0803.jpg)'
- en: Figure 8-3\. Schematic singular value decomposition.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 示意奇异值分解。
- en: The singular values are the diagonal elements of *Σ*. The document-topic relations
    are included in *U*, whereas the word-to-topic mapping is represented by *V**.
    Note that neither the elements of *U* nor the elements of *V** are guaranteed
    to be positive. The relative sizes of the contributions will still be interpretable,
    but the probability explanation is no longer valid.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值是*Σ*的对角元素。文档-主题关系包含在*U*中，而词-主题映射由*V**表示。注意，*U*的元素和*V**的元素都不能保证是正的。贡献的相对大小仍然是可解释的，但概率解释不再有效。
- en: 'Blueprint: Creating a Topic Model for Paragraphs with SVD'
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用 SVD 为段落创建主题模型
- en: 'In scikit-learn the interface to SVD is identical to that of NMF. This time
    we start directly with the paragraphs:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，SVD 的接口与 NMF 的接口相同。这次我们直接从段落开始：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our previously defined function for evaluating the topic model can also be
    used:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义的用于评估主题模型的函数也可以使用：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Out:`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| **Topic 00** nations (0.67)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 国家（0.67）'
- en: united (0.65)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（0.65）
- en: international (0.58)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（0.58）
- en: peace (0.46)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（0.46）
- en: world (0.46) | **Topic 01** general (14.04)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 世界（0.46） | **主题 01** 一般（14.04）
- en: assembly (13.09)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 装配（13.09）
- en: session (12.94)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 会话（12.94）
- en: mr (10.02)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 先生（10.02）
- en: president (8.59) | **Topic 02** countries (19.15)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 总统（8.59） | **主题 02** 国家（19.15）
- en: development (14.61)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 发展（14.61）
- en: economic (13.91)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 经济（13.91）
- en: developing (13.00)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 发展中（13.00）
- en: session (10.29) | **Topic 03** nations (4.41)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 会议（10.29） | **主题 03** 国家（4.41）
- en: united (4.06)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（4.06）
- en: development (0.95)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 发展（0.95）
- en: organization (0.84)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 组织（0.84）
- en: charter (0.80) | **Topic 04** nuclear (21.13)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 宪章（0.80） | **主题 04** 核（21.13）
- en: weapons (14.01)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 武器（14.01）
- en: disarmament (9.02)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军（9.02）
- en: treaty (7.23)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 条约（7.23）
- en: proliferation (6.31) |
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散（6.31） |
- en: '| **Topic 05** rights (29.50)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 权利（29.50）'
- en: human (28.81)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 人类（28.81）
- en: nuclear (9.20)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 核（9.20）
- en: weapons (6.42)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 武器（6.42）
- en: respect (4.98) | **Topic 06** africa (8.73)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尊重（4.98） | **主题 06** 非洲（8.73）
- en: south (8.24)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 南方（8.24）
- en: united (3.91)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（3.91）
- en: african (3.71)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲（3.71）
- en: nations (3.41) | **Topic 07** council (14.96)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（3.41） | **主题 07** 理事会（14.96）
- en: security (13.38)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（13.38）
- en: africa (8.50)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲（8.50）
- en: south (6.11)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 南方（6.11）
- en: african (3.94) | **Topic 08** world (48.49)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲（3.94） | **主题 08** 世界（48.49）
- en: international (41.03)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（41.03）
- en: peace (32.98)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（32.98）
- en: community (23.27)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 社区（23.27）
- en: africa (22.00) | **Topic 09** development (63.98)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲（22.00） | **主题 09** 发展（63.98）
- en: sustainable (20.78)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续（20.78）
- en: peace (20.74)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（20.74）
- en: goals (15.92)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 目标（15.92）
- en: africa (15.61) |
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲（15.61） |
- en: Most of the resulting topics are surprisingly similar to those of the nonnegative
    matrix factorization. However, the Middle East conflict does not appear as a separate
    topic this time. As the topic-word mappings can also have negative values, the
    normalization varies from topic to topic. Only the relative sizes of the words
    constituting the topics are relevant.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数生成的主题与非负矩阵分解的主题非常相似。然而，中东冲突这一主题这次没有单独出现。由于主题-词映射也可能具有负值，因此归一化从主题到主题有所不同。只有构成主题的单词的相对大小才是相关的。
- en: Don’t worry about the negative percentages. These arise as SVD does not guarantee
    positive values in W, so contributions of individual words might be negative.
    This means that words appearing in documents “reject” the corresponding topic.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心负百分比。这是因为SVD不保证W中的值为正，因此个别单词的贡献可能为负。这意味着出现在文档中的单词“排斥”相应的主题。
- en: 'If we want to determine the sizes of the topics, we now have to take a look
    at the singular values of the decomposition:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想确定主题的大小，现在就要查看分解的奇异值：
- en: '[PRE20]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`Out:`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The sizes of the topics also correspond quite nicely with the ones from the
    NMF method for the paragraphs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 主题的大小与NMF方法的段落相当相符。
- en: Both NMF and SVF have used the document-term matrix (with TF-IDF transformations
    applied) as a basis for the topic decomposition. Also, the dimensions of the *U*
    matrix are identical to those of *W*; the same is true for *V** and *H*. It is
    therefore not surprising that both of these methods produce similar and comparable
    results. As these methods are really fast to calculate, for real-life projects
    we recommend starting with the linear algebra methods.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: NMF和SVF都使用了文档-词矩阵（应用了TF-IDF转换）作为主题分解的基础。此外，*U*矩阵的维度与*W*的维度相同；*V*和*H*也是如此。因此，这两种方法产生类似且可比较的结果并不奇怪。由于这些方法计算速度很快，因此我们建议在实际项目中首先使用线性代数方法。
- en: We will now turn away from these linear-algebra-based methods and focus on probabilistic
    topic models, which have become immensely popular in the past 20 years.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将摆脱这些基于线性代数的方法，专注于概率主题模型，在过去20年中已经变得极为流行。
- en: Latent Dirichlet Allocation
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: LDA is arguably the most prominent method of topic modeling in use today. It
    has been popularized during the last 15 years and can be adapted in flexible ways
    to different usage scenarios.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: LDA可以说是当今使用最广泛的主题建模方法。它在过去15年间变得流行，并且可以灵活地适应不同的使用场景。
- en: How does it work?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: LDA views each document as consisting of different topics. In other words, each
    document is a mix of different topics. In the same way, topics are mixed from
    words. To keep the number of topics per document low and to have only a few, important
    words constituting the topics, LDA initially uses a [Dirichlet distribution](https://oreil.ly/Kkd9k),
    a so-called *Dirichlet prior*. This is applied both for assigning topics to documents
    and for finding words for the topics. The Dirichlet distribution ensures that
    documents have only a small number of topics and topics are mainly defined by
    a small number of words. Assuming that LDA generated topic distributions like
    the previous ones, a topic could be made up of words like *nuclear*, *treaty*,
    and *disarmament*, while another topic would be sampled by *sustainable*, *development*,
    etc.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: LDA将每个文档视为包含不同主题。换句话说，每个文档是不同主题的混合。同样，主题是从词中混合而来。为了保持每个文档中主题数量的少而且只包含一些重要词语，LDA最初使用[狄利克雷分布](https://oreil.ly/Kkd9k)，即所谓的*狄利克雷先验*。这一分布用于为文档分配主题和为主题找到单词。狄利克雷分布确保文档只有少量主题，并且主题主要由少量单词定义。假设LDA生成了像之前那样的主题分布，一个主题可能由诸如*核*、*条约*和*裁军*等词汇构成，而另一个主题则由*可持续*、*发展*等词汇组成。
- en: After the initial assignments, the generative process starts. It uses the Dirichlet
    distributions for topics and words and tries to re-create the words from the original
    documents with stochastic sampling. This process has to be iterated many times
    and is therefore computationally intensive.^([2](ch08.xhtml#idm45634187759704))
    On the other hand, the results can be used to generate documents for any identified
    topic.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始分配之后，生成过程开始。它使用主题和单词的狄利克雷分布，并尝试用随机抽样重新创建原始文档中的单词。这个过程必须多次迭代，因此计算量很大。^([2](ch08.xhtml#idm45634187759704))
    另一方面，结果可以用来为任何确定的主题生成文档。
- en: 'Blueprint: Creating a Topic Model for Paragraphs with LDA'
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用LDA为段落创建主题模型
- en: 'Scikit-learn hides all these differences and uses the same API as the other
    topic modeling methods:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn隐藏了所有这些差异，并使用与其他主题建模方法相同的API：
- en: '[PRE22]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Waiting Time
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等待时间
- en: Due to the probabilistic sampling, the process takes a lot longer than NMF and
    SVD. Expect at least minutes, if not hours, of runtime.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概率抽样的原因，该过程比NMF和SVD需要更长时间。期望至少分钟，甚至小时的运行时间。
- en: 'Our utility function can again be used to visualize the latent topics of the
    paragraph corpus:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的效用函数可以再次用于可视化段落语料库的潜在主题：
- en: '[PRE24]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| **Topic 00** africa (2.38)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 非洲（2.38）'
- en: people (1.86)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 人们（1.86）
- en: south (1.57)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 南方（1.57）
- en: namibia (0.88)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 纳米比亚（0.88）
- en: regime (0.75) | **Topic 01** republic (1.52)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 政权（0.75）| **主题 01** 共和国（1.52）
- en: government (1.39)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 政府（1.39）
- en: united (1.21)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（1.21）
- en: peace (1.16)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（1.16）
- en: people (1.02) | **Topic 02** general (4.22)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 人民（1.02）| **主题 02** 普通（4.22）
- en: assembly (3.63)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 大会（3.63）
- en: session (3.38)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 会议（3.38）
- en: president (2.33)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总统（2.33）
- en: mr (2.32) | **Topic 03** human (3.62)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 先生（2.32）| **主题 03** 人类（3.62）
- en: rights (3.48)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 权利（3.48）
- en: international (1.83)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（1.83）
- en: law (1.01)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 法律（1.01）
- en: terrorism (0.99) | **Topic 04** world (2.22)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 恐怖主义（0.99）| **主题 04** 世界（2.22）
- en: people (1.14)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 人们（1.14）
- en: countries (0.94)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（0.94）
- en: years (0.88)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 年（0.88）
- en: today (0.66) |
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 今天（0.66）|
- en: '| **Topic 05** peace (1.76)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 和平（1.76）'
- en: security (1.63)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（1.63）
- en: east (1.34)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 东方（1.34）
- en: middle (1.34)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 中间（1.34）
- en: israel (1.24) | **Topic 06** countries (3.19)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以色列（1.24）| **主题 06** 国家（3.19）
- en: development (2.70)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 发展（2.70）
- en: economic (2.22)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 经济（2.22）
- en: developing (1.61)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 发展（1.61）
- en: international (1.45) | **Topic 07** nuclear (3.14)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（1.45）| **主题 07** 核（3.14）
- en: weapons (2.32)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 武器（2.32）
- en: disarmament (1.82)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军（1.82）
- en: states (1.47)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（1.47）
- en: arms (1.46) | **Topic 08** nations (5.50)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 军备（1.46）| **主题 08** 国家（5.50）
- en: united (5.11)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（5.11）
- en: international (1.46)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（1.46）
- en: security (1.45)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（1.45）
- en: organization (1.44) | **Topic 09** international (1.96)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 组织（1.44）| **主题 09** 国际（1.96）
- en: world (1.91)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 世界（1.91）
- en: peace (1.60)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（1.60）
- en: economic (1.00)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 经济（1.00）
- en: relations (0.99) |
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 关系（0.99）|
- en: It’s interesting to observe that LDA has generated a completely different topic
    structure compared to the linear algebra methods described earlier. *People* is
    the most prominent word in three quite different topics. In Topic 04, South Africa
    is related to Israel and Palestine, while in Topic 00, Cyprus, Afghanistan, and
    Iraq are related. This is not easy to explain. This is also reflected in the slowly
    decreasing word weights of the topics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是观察到，与前述的线性代数方法相比，LDA生成了完全不同的主题结构。*人们*是三个完全不同主题中最突出的词。在主题 04 中，南非与以色列和巴勒斯坦有关联，而在主题
    00 中，塞浦路斯、阿富汗和伊拉克有关联。这不容易解释。这也反映在主题的逐渐减少的单词权重中。
- en: Other topics are easier to comprehend, such as climate change, nuclear weapons,
    elections, developing countries, and organizational questions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其他主题更容易理解，比如气候变化、核武器、选举、发展中国家和组织问题。
- en: 'In this example, LDA does not yield much better results than either NMF or
    SVD. However, due to the sampling process, LDA is not limited to sample topics
    just consisting of words. There are several variations, such as author-topic models,
    that can also sample categorical features. Moreover, as there is so much research
    going on in LDA, other ideas are published quite frequently, which extend the
    focus of the method well beyond text analytics (see, for example, Minghui Qiu
    et al., [“It Is Not Just What We Say, But How We Say Them: LDA-based Behavior-Topic
    Model”](https://oreil.ly/dnqq5) or Rahji Abdurehman, [“Keyword-Assisted LDA: Exploring
    New Methods for Supervised Topic Modeling”](https://oreil.ly/DDClf)).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，LDA 的结果并不比 NMF 或 SVD 好多少。然而，由于抽样过程，LDA 并不仅限于样本主题仅仅由单词组成。还有几种变体，比如作者-主题模型，也可以抽样分类特征。此外，由于在LDA领域有很多研究，其他想法也经常被发表，这些想法大大超出了文本分析的焦点（例如，见Minghui
    Qiu等人的 [“不仅仅是我们说了什么，而是我们如何说它们：基于LDA的行为-主题模型”](https://oreil.ly/dnqq5) 或Rahji Abdurehman的
    [“关键词辅助LDA：探索监督主题建模的新方法”](https://oreil.ly/DDClf)）。
- en: 'Blueprint: Visualizing LDA Results'
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：可视化LDA结果
- en: As LDA is so popular, there is a nice package in Python to visualize the LDA
    results called pyLDAvis.^([3](ch08.xhtml#idm45634187547192)) Fortunately, it can
    directly use the results from sciki-learn for its visualization.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LDA非常流行，Python中有一个很好的包来可视化LDA结果，称为pyLDAvis。^([3](ch08.xhtml#idm45634187547192))
    幸运的是，它可以直接使用sciki-learn的结果进行可视化。
- en: 'Be careful, this takes some time:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这需要一些时间：
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_08in02.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in02.jpg)'
- en: 'There is a multitude of information available in the visualization. Let’s start
    with the topic “bubbles” and click the topic. Now take a look at the red bars,
    which symbolize the word distribution in the currently selected topic. As the
    length of the bars is not decreasing quickly, Topic 2 is not very pronounced.
    This is the same effect you can see in the table from [“Blueprint: Creating a
    Topic Model for Paragraphs with LDA”](#ch08-topic-model-para) (look at Topic 1,
    where we have used the array indices, whereas pyLDAvis starts enumerating the
    topics with 1).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '可视化中提供了大量信息。让我们从“气泡”话题开始，并点击它。现在看一下红色条，它们象征着当前选定话题中的单词分布。由于条的长度没有迅速减少，说明话题
    2 并不十分显著。这与我们在 [“Blueprint: Creating a Topic Model for Paragraphs with LDA”](#ch08-topic-model-para)
    表格中看到的效果相同（看看话题 1，在那里我们使用了数组索引，而 pyLDAvis 从 1 开始枚举话题）。'
- en: To visualize the results, the topics are mapped from their original dimension
    (the number of words) into two dimensions using principal component analysis (PCA),
    a standard method for dimension reduction. This results in a point; the circle
    is added to see the relative sizes of the topics. It is possible to use T-SNE
    instead of PCA by passing `mds="tsne"` as a parameter in the preparation stage.
    This changes the intertopic distance map and shows fewer overlapping topic bubbles.
    This is, however, just an artifact of projecting the many word dimensions in just
    two for visualization. Therefore, it’s always a good idea to look at the word
    distribution of the topics and not exclusively trust a low-dimensional visualization.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化结果，话题从原始维度（单词数）通过主成分分析（PCA）映射到二维空间，这是一种标准的降维方法。这导致了一个点；圆圈被添加以查看话题的相对大小。可以通过在准备阶段传递
    `mds="tsne"` 参数来使用 T-SNE 替代 PCA。这改变了话题之间的距离映射，并显示了较少重叠的话题气泡。然而，这只是在可视化时将许多单词维度投影到仅两个维度的一个副作用。因此，查看话题的单词分布并不完全依赖于低维度的可视化是一个好主意。
- en: It’s interesting to see the strong overlap of Topics 4, 6, and 10 (“international”),
    whereas Topic 3 (“general assembly”) seems to be far away from all other topics.
    By hovering over the other topic bubbles or clicking them, you can take a look
    at their respective word distributions on the right side. Although not all the
    topics are perfectly separated, there are some (like Topic 1 and Topic 7) that
    are far away from the others. Try to hover over them and you will find that their
    word content is also different from each other. For such topics, it might be useful
    to extract the most representative documents and use them as a training set for
    supervised learning.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 看到话题 4、6 和 10（“国际”）之间的强重叠是很有趣的，而话题 3（“大会”）似乎远离其他所有话题。通过悬停在其他话题气泡上或点击它们，您可以查看右侧的单词分布。尽管不是所有话题都完全分离，但有些话题（如话题
    1 和话题 7）远离其他话题。尝试悬停在它们上面，您会发现它们的单词内容也不同。对于这样的话题，提取最具代表性的文档并将它们用作监督学习的训练集可能是有用的。
- en: pyLDAvis is a nice tool to play with and is well-suited for screenshots in presentations.
    Even though it looks explorative, the real exploration in the topic models takes
    place by modifying the features and the hyperparameters of the algorithms.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: pyLDAvis 是一个很好的工具，适合在演示文稿中使用截图。尽管看起来探索性十足，但真正的探索在于修改算法的特征和超参数。
- en: Using pyLDAvis gives us a good idea how the topics are arranged with respect
    to one another and which individual words are important. However, if we need a
    more qualitative understanding of the topics, we can use additional visualizations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pyLDAvis 能让我们很好地了解话题是如何相互排列的，以及哪些单词是重要的。然而，如果我们需要更质量的话题理解，可以使用额外的可视化工具。
- en: 'Blueprint: Using Word Clouds to Display and Compare Topic Models'
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Blueprint: 使用词云来显示和比较话题模型'
- en: So far, we have used lists to display the topic models. This way, we could nicely
    identify how pronounced the different topics were. However, in many cases topic
    models are used to give you a first impression about the validity of the corpus
    and better visualizations. As we have seen in [Chapter 1](ch01.xhtml#ch-exploration),
    word clouds are a qualitative and intuitive instrument to show this.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用列表显示了话题模型。这样，我们可以很好地识别不同话题的显著程度。然而，在许多情况下，话题模型用于给出关于语料库有效性和更好可视化的第一印象。正如我们在
    [第 1 章](ch01.xhtml#ch-exploration) 中看到的，词云是展示这一点的定性和直观工具。
- en: 'We can directly use word clouds to show our topic models. The code is easily
    derived from the previously defined `display_topics` function:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用词云来展示我们的主题模型。代码可以很容易地从之前定义的`display_topics`函数中推导出来：
- en: '[PRE26]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'By using this code, we can qualitatively compare the results of the NMF model
    ([Figure 8-4](#fig-wordcloud-nmf)) with those of the LDA model([Figure 8-5](#fig-wordcloud-lda)).
    Larger words are more important in their respective topics. If many words have
    roughly the same size, the topic is not well-pronounced:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用此代码，我们可以定性地比较NMF模型（[图 8-4](#fig-wordcloud-nmf)）的结果与LDA模型（[图 8-5](#fig-wordcloud-lda)）。较大的单词在各自的主题中更为重要。如果许多单词的大小大致相同，则该主题没有明显表现：
- en: '[PRE27]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Word Clouds Use Individual Scaling
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单独的缩放制作词云
- en: The font sizes in the word clouds use scaling within each topic separately,
    and therefore it’s important to verify with the actual numbers before drawing
    any final conclusions.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 词云中的字体大小在每个主题内部使用缩放，因此在绘制任何最终结论之前，验证实际数字非常重要。
- en: The presentation is now way more compelling. It is much easier to match topics
    between the two methods, like 0-NMF with 8-LDA. For most topics, this is quite
    obvious, but there are also differences. 1-LDA (“people republic”) has no equivalent
    in NMF, whereas 9-NMF (“sustainable development”) cannot be found in LDA.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的展示更加引人入胜。很容易在两种方法之间匹配主题，比如0-NMF与8-LDA。对于大多数主题来说，这是显而易见的，但也存在差异。1-LDA（“人民共和国”）在NMF中没有相对应项，而9-NMF（“可持续发展”）在LDA中找不到。
- en: As we have found a nice qualitative visualization of the topics, we are now
    interested in how that topic distribution has changed over time.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们找到了主题的良好定性可视化，我们现在对主题分布随时间的变化感兴趣。
- en: '![](Images/btap_0804.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0804.jpg)'
- en: Figure 8-4\. Word clouds representing the NMF topic model.
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4。展示NMF主题模型的词云。
- en: '![](Images/btap_0805.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0805.jpg)'
- en: Figure 8-5\. Word clouds representing the LDA topic model.
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5。展示LDA主题模型的词云。
- en: 'Blueprint: Calculating Topic Distribution of Documents and Time Evolution'
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：计算文档主题分布和时间演变
- en: As you can see in the analysis at the beginning of the chapter, the speech metadata
    changes over time. This leads to the interesting question of how the distribution
    of the topics changes over time. It turns out that this is easy to calculate and
    insightful.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本章开头的分析中所看到的，演讲的元数据随时间变化。这引发了一个有趣的问题，即主题的分布随时间如何变化。结果表明，这很容易计算并且具有洞察力。
- en: 'Like the scikit-learn vectorizers, the topic models also have a `transform`
    method, which calculates the topic distribution of existing documents keeping
    the already fitted topic model. Let’s use this to first separate speeches before
    1990 from those after 1990\. For this, we create NumPy arrays for the documents
    before and after 1990:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 像scikit-learn的向量化器一样，主题模型也有一个`transform`方法，用于计算现有文档的主题分布，保持已拟合的主题模型不变。让我们首先使用这个方法将1990年之前和之后的演讲分开。为此，我们为1990年之前和之后的文档创建NumPy数组：
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then we can calculate the respective *W* matrices:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算相应的*W*矩阵：
- en: '[PRE29]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE30]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The result is interesting, as some percentages have changed considerably; specifically,
    the size of the second-to-last topic is much smaller in the later years. We will
    now try to take a deeper look at the topics and their changes over time.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常有趣，某些百分比发生了显著变化；特别是后期年份中倒数第二个主题的大小要小得多。现在，我们将尝试更深入地研究主题及其随时间的变化。
- en: 'Let’s try to calculate the distribution for individual years and see whether
    we can find a visualization to uncover possible patterns:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试计算各个年份的分布，看看是否能找到可视化方法来揭示可能的模式：
- en: '[PRE31]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To make the plots more intuitive, we first create a list of topics with their
    two most important words:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使图表更直观，我们首先创建一个包含两个最重要单词的主题列表：
- en: '[PRE32]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then combine the results in a `DataFrame` with the previous topics as column
    names, so we can easily visualize that as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将结果与以前的主题作为列名合并到一个`DataFrame`中，这样我们可以轻松地进行可视化，如下所示：
- en: '[PRE33]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_08in03.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in03.jpg)'
- en: In the resulting graph you can see how the topic distribution changes over the
    year.  We can recognize that the “sustainable development” topic is continuously
    increasing, while “south africa” has lost popularity after the apartheid regime
    ended.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的图表中，您可以看到主题分布随着年份的变化而变化。我们可以看到，“可持续发展”主题在持续增加，而“南非”在种族隔离制度结束后失去了流行度。
- en: Compared to showing the time development of single (guessed) words, topics seem
    to be a more natural entity as they arise from the text corpus itself. Note that
    this chart was generated with an unsupervised method exclusively, so there is
    no bias in it. Everything was already in the debates data; we have just uncovered
    it.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于展示单个（猜测的）单词的时间发展，主题似乎更自然，因为它们源于文本语料库本身。请注意，此图表是通过一种纯无监督的方法生成的，因此其中没有偏见。一切都已经在辩论数据中；我们只是揭示了它。
- en: So far, we have used scikit-learn exclusively for topic modeling. In the Python
    ecosystem, there is a specialized library for topic models called Gensim, which
    we will now investigate.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在主题建模中仅使用了 scikit-learn。在 Python 生态系统中，有一个专门用于主题模型的库称为 Gensim，我们现在将对其进行调查。
- en: Using Gensim for Topic Modeling
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gensim 进行主题建模
- en: Apart from scikit-learn, [*Gensim*](https://oreil.ly/Ybn63) is another popular
    tool for performing topic modeling in Python. Compared to scikit-learn, it offers
    more algorithms for calculating topic models and can also give estimates about
    the quality of the model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 scikit-learn，[*Gensim*](https://oreil.ly/Ybn63) 是另一个在 Python 中执行主题建模的流行工具。与
    scikit-learn 相比，它提供了更多用于计算主题模型的算法，并且还可以给出关于模型质量的估计。
- en: 'Blueprint: Preparing Data for Gensim'
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：为 Gensim 准备数据
- en: 'Before we can start calculating the Gensim models, we have to prepare the data.
    Unfortunately, the API and the terminology are different from scikit-learn. In
    the first step, we have to prepare the vocabulary. Gensim has no integrated tokenizer
    and expects each line of a document corpus to be already tokenized:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始计算 Gensim 模型之前，我们必须准备数据。不幸的是，API 和术语与 scikit-learn 不同。在第一步中，我们必须准备词汇表。Gensim
    没有集成的分词器，而是期望每篇文档语料库的每一行已经被分词了：
- en: '[PRE34]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After tokenization, we can initialize the Gensim dictionary with these tokenized
    documents. Think of the dictionary as a mapping from words to columns (like the
    features we used in [Chapter 2](ch02.xhtml#ch-api)):'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 分词后，我们可以用这些分词后的文档初始化 Gensim 字典。将字典视为从单词到列的映射（就像我们在 [第二章](ch02.xhtml#ch-api)
    中使用的特征）：
- en: '[PRE35]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Similar to the scikit-learn `TfidfVectorizer`, we can reduce the vocabulary
    by filtering out words that appear not often enough or too frequently. To keep
    the dimensions low, we choose a minimum of five documents in which words must
    appear, but not in more than 70% of the documents. As we saw in [Chapter 2](ch02.xhtml#ch-api),
    these parameters can be optimized and require some experimentation.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与 scikit-learn 的 `TfidfVectorizer` 类似，我们可以通过过滤出现频率不够高或者太高的单词来减少词汇量。为了保持低维度，我们选择单词至少出现在五篇文档中，但不能超过文档的
    70%。正如我们在 [第二章](ch02.xhtml#ch-api) 中看到的，这些参数可以进行优化，并需要一些实验。
- en: 'In Gensim, this is implemented via a filter with the parameters `no_below`
    and `no_above` (in scikit-learn, the analog would be `min_df` and `max_df`):'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gensim 中，这通过参数 `no_below` 和 `no_above` 过滤器实现（在 scikit-learn 中，类似的是 `min_df`
    和 `max_df`）：
- en: '[PRE36]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'With the dictionary read, we can now use Gensim to calculate the bag-of-words
    matrix (which is called a *corpus* in Gensim, but we will stick with our current
    terminology):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 读取了字典后，我们现在可以使用 Gensim 计算词袋矩阵（在 Gensim 中称为 *语料库*，但我们将坚持我们当前的术语）：
- en: '[PRE37]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we can perform the TF-IDF transformation. The first line fits the
    bag-of-words model, while the second line transforms the weights:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以执行 TF-IDF 转换。第一行适配词袋模型，而第二行转换权重：
- en: '[PRE38]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `vectors_gensim_para` matrix is the one that we will use for all upcoming
    topic modeling tasks with Gensim.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`vectors_gensim_para` 矩阵是我们将在 Gensim 中进行所有即将进行的主题建模任务的矩阵。'
- en: 'Blueprint: Performing Nonnegative Matrix Factorization with Gensim'
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用 Gensim 进行非负矩阵分解
- en: 'Let’s check first the results of NMF and see whether we can reproduce those
    of scikit-learn:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先检查 NMF 的结果，看看我们是否可以复现 scikit-learn 的结果：
- en: '[PRE39]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The evaluation can take a while. Although Gensim offers a `show_topics` method
    for directly displaying the topics, we have a different implementation to make
    it look like the scikit-learn results so it’s easier to compare them:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 评估可能需要一些时间。虽然 Gensim 提供了一个 `show_topics` 方法来直接显示主题，但我们有一个不同的实现，使其看起来像 scikit-learn
    的结果，这样更容易进行比较：
- en: '[PRE40]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`Out:`'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| **Topic 00** nations (0.03)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 国家 (0.03)'
- en: united (0.02)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 联合 (0.02)
- en: human (0.02)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 人类 (0.02)
- en: rights (0.02)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 权利 (0.02)
- en: role (0.01) | **Topic 01** africa (0.02)
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 角色 (0.01) | **主题 01** 非洲 (0.02)
- en: south (0.02)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 南部 (0.02)
- en: people (0.02)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 人们 (0.02)
- en: government (0.01)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 政府 (0.01)
- en: republic (0.01) | **Topic 02** economic (0.01)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 共和国 (0.01) | **主题 02** 经济 (0.01)
- en: development (0.01)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 发展 (0.01)
- en: countries (0.01)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.01)
- en: social (0.01)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 社会 (0.01)
- en: international (0.01) | **Topic 03** countries (0.02)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 国际（0.01）| **主题 03** 国家（0.02）
- en: developing (0.02)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 发展中（0.02）
- en: resources (0.01)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 资源（0.01）
- en: sea (0.01)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 海（0.01）
- en: developed (0.01) | **Topic 04** israel (0.02)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 发达（0.01）| **主题 04** 以色列（0.02）
- en: arab (0.02)
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 阿拉伯（0.02）
- en: palestinian (0.02)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 巴勒斯坦（0.02）
- en: council (0.01)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 理事会（0.01）
- en: security (0.01) |
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（0.01）|
- en: '| **Topic 05** organization (0.02)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 组织（0.02）'
- en: charter (0.02)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 宪章（0.02）
- en: principles (0.02)
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 原则（0.02）
- en: member (0.01)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 成员（0.01）
- en: respect (0.01) | **Topic 06** problem (0.01)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 尊重（0.01）| **主题 06** 问题（0.01）
- en: solution (0.01)
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案（0.01）
- en: east (0.01)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 东部（0.01）
- en: situation (0.01)
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 情况（0.01）
- en: problems (0.01) | **Topic 07** nuclear (0.02)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 问题（0.01）| **主题 07** 核（0.02）
- en: co (0.02)
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 公司（0.02）
- en: operation (0.02)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 操作（0.02）
- en: disarmament (0.02)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军（0.02）
- en: weapons (0.02) | **Topic 08** session (0.02)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 武器（0.02）| **主题 08** 会议（0.02）
- en: general (0.02)
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 将军（0.02）
- en: assembly (0.02)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 大会（0.02）
- en: mr (0.02)
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 先生（0.02）
- en: president (0.02) | **Topic 09** world (0.02)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 总统（0.02）| **主题 09** 世界（0.02）
- en: peace (0.02)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（0.02）
- en: peoples (0.02)
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 人民（0.02）
- en: security (0.01)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（0.01）
- en: states (0.01) |
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（0.01）|
- en: 'NMF is also a statistical method, so the results are not supposed to be identical
    to the ones that we calculated with scikit-learn, but they are similar enough.
    Gensim has code for calculating the coherence score for topic models, a quality
    indicator. Let’s try this:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: NMF也是一种统计方法，因此结果不应与我们用scikit-learn计算的结果完全相同，但它们非常相似。Gensim有用于计算主题模型的一致性评分的代码，作为质量指标。让我们试试这个：
- en: '[PRE41]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE42]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The score varies with the number of topics. If you want to find the optimal
    number of topics, a frequent approach is to run NMF for several different values,
    calculate the coherence score, and take the number of topics that maximizes the
    score.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 分数随主题数量变化。如果想找到最佳主题数，常见的方法是运行多个不同值的NMF，计算一致性评分，然后选择最大化评分的主题数。
- en: Let’s try the same with LDA and compare the quality indicators.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用LDA做同样的事情并比较质量指标。
- en: 'Blueprint: Using LDA with Gensim'
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用Gensim的LDA
- en: 'Running LDA with Gensim is as easy as using NMF if we have the data prepared.
    The `LdaModel` class has a lot of parameters for tuning the model; we use the
    recommended values here:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gensim运行LDA与准备好的数据一样简单如使用NMF。 `LdaModel` 类有许多用于调整模型的参数；我们在这里使用推荐的数值：
- en: '[PRE43]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We are interested in the word distribution of the topics:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对主题的词分布很感兴趣：
- en: '[PRE44]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`Out:`'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| **Topic 00** climate (0.12)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 气候（0.12）'
- en: convention (0.03)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 公约（0.03）
- en: pacific (0.02)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 太平洋（0.02）
- en: environmental (0.02)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 环境（0.02）
- en: sea (0.02) | **Topic 01** country (0.05)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 海（0.02）| **主题 01** 国家（0.05）
- en: people (0.05)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 人民（0.05）
- en: government (0.03)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 政府（0.03）
- en: national (0.02)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（0.02）
- en: support (0.02) | **Topic 02** nations (0.10)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 支持（0.02）| **主题 02** 国家（0.10）
- en: united (0.10)
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 联合（0.10）
- en: human (0.04)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 人类（0.04）
- en: security (0.03)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 安全（0.03）
- en: rights (0.03) | **Topic 03** international (0.03)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 权利（0.03）| **主题 03** 国际（0.03）
- en: community (0.01)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 社区（0.01）
- en: efforts (0.01)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 努力（0.01）
- en: new (0.01)
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 新（0.01）
- en: global (0.01) | **Topic 04** africa (0.06)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 全球（0.01）| **主题 04** 非洲（0.06）
- en: african (0.06)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲人（0.06）
- en: continent (0.02)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 大陆（0.02）
- en: terrorist (0.02)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 恐怖主义者（0.02）
- en: crimes (0.02) |
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 罪行（0.02）|
- en: '| **Topic 05** world (0.05)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 05** 世界（0.05）'
- en: years (0.02)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 年份（0.02）
- en: today (0.02)
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 今天（0.02）
- en: peace (0.01)
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 和平（0.01）
- en: time (0.01) | **Topic 06** peace (0.03)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 时间（0.01）| **主题 06** 和平（0.03）
- en: conflict (0.02)
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 冲突（0.02）
- en: region (0.02)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 区域（0.02）
- en: people (0.02)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 人民（0.02）
- en: state (0.02) | **Topic 07** south (0.10)
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（0.02）| **主题 07** 南（0.10）
- en: sudan (0.05)
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 苏丹（0.05）
- en: china (0.04)
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 中国（0.04）
- en: asia (0.04)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 亚洲（0.04）
- en: somalia (0.04) | **Topic 08** general (0.10)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 索马里（0.04）| **主题 08** 将军（0.10）
- en: assembly (0.09)
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 大会（0.09）
- en: session (0.05)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 会议（0.05）
- en: president (0.04)
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 总统（0.04）
- en: secretary (0.04) | **Topic 09** development (0.07)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 秘书（0.04）| **主题 09** 发展（0.07）
- en: countries (0.05)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（0.05）
- en: economic (0.03)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 经济（0.03）
- en: sustainable (0.02)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续（0.02）
- en: 2015 (0.02) |
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年（0.02）|
- en: The topics are not as easy to interpret as the ones generated by NMF. Checking
    the coherence score as shown earlier, we find a lower score of 0.45270703180962374\.
    Gensim also allows us to calculate the perplexity score of an LDA model. Perplexity
    measures how well a probability model predicts a sample. When we execute `lda_gensim_para.log_perplexity(vectors_gensim_para)`,
    we get a perplexity score of -9.70558947109483.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 主题的解释并不像NMF生成的解释那样容易。如前所示检查一致性评分，我们发现较低的评分为0.45270703180962374。Gensim还允许我们计算LDA模型的困惑度评分。困惑度衡量概率模型预测样本的能力。当我们执行
    `lda_gensim_para.log_perplexity(vectors_gensim_para)` 时，我们得到一个困惑度评分为 -9.70558947109483。
- en: 'Blueprint: Calculating Coherence Scores'
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：计算一致性评分
- en: 'Gensim can also calculate topic coherence. The method itself is a four-stage
    process consisting of segmentation, probability estimation, a confirmation measure
    calculation, and aggregation. Fortunately, Gensim has a `CoherenceModel` class
    that encapsulates all these single tasks, and we can directly use it:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还可以计算主题一致性。方法本身是一个包含分割、概率估计、确认度量计算和聚合的四阶段过程。幸运的是，Gensim有一个`CoherenceModel`类，封装了所有这些单一任务，我们可以直接使用它：
- en: '[PRE45]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`Out:`'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE46]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Substituting `nmf` for `lda`, we can calculate the same score for our NMF model:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 用`nmf`替换`lda`，我们可以为我们的NMF模型计算相同的得分：
- en: '[PRE47]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Out:`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE48]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The score is quite a bit higher, which means that the NMF model is a better
    approximation to the real topics compared to LDA.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 分数要高得多，这意味着与LDA相比，NMF模型更接近真实主题。
- en: 'Calculating the coherence score of the individual topics for LDA is even easier,
    as it is directly supported by the LDA model. Let’s take a look at the average
    first:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 计算LDA模型各个主题的一致性得分更加简单，因为它直接由LDA模型支持。让我们首先看一下平均值：
- en: '[PRE49]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`Out:`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE50]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We are also interested in the coherence scores of the individual topics, which
    is contained in `top_topics`. However, the output is verbose (check it!), so we
    try to condense it a bit by just printing the coherence scores together with the
    most important words of the topics:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对各个主题的一致性得分感兴趣，这些得分包含在`top_topics`中。但是，输出内容太冗长（检查一下！），因此我们试图通过仅将一致性得分与主题中最重要的单词一起打印来压缩它：
- en: '[PRE51]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`Out:`'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE52]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Coherence scores for topic models can easily be calculated using Gensim. The
    absolute values are difficult to interpret, but varying the methods (NMF versus
    LDA) or the number of topics can give you ideas about which way you want to proceed
    in your topic models. Coherence scores and coherence models are a big advantage
    of Gensim, as they are not (yet) included in scikit-learn.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gensim可以轻松计算主题模型的一致性得分。绝对值很难解释，但是通过变化方法（NMF与LDA）或主题数可以让您了解您希望在主题模型中前进的方向。一致性得分和一致性模型是Gensim的一大优势，因为它们（尚）未包含在scikit-learn中。
- en: As it’s difficult to estimate the “correct” number of topics, we are now taking
    a look at an approach that creates hierarchical models and does not need a fixed
    number of topics as a parameter.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 由于很难估计“正确”的主题数量，我们现在看一种创建层次模型的方法，不需要固定的主题数作为参数。
- en: 'Blueprint: Finding the Optimal Number of Topics'
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：找到最佳主题数
- en: In the previous sections, we have always worked with 10 topics. So far we have
    not compared the quality of this topic model to different ones with a lower or
    higher number of topics. We want to find the optimal number of topics in a structured
    way without having to go into the interpretation of each constellation.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们始终使用了10个主题。到目前为止，我们还没有将此主题模型的质量与具有较少或更多主题数的不同模型进行比较。我们希望找到一种结构化的方式来找到最佳主题数量，而无需深入解释每个主题模型。
- en: 'It turns out there is a way to achieve this. The “quality” of a topic model
    can be measured by the previously introduced coherence score. To find the best
    coherence score, we will now calculate it for a different number of topics with
    an LDA model. We will try to find the highest score, which should give us the
    optimal number of topics:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 原来有一种方法可以实现这一点。主题模型的“质量”可以通过先前引入的一致性得分来衡量。为了找到最佳一致性得分，我们现在将为不同数量的主题使用LDA模型来计算它。我们将尝试找到最高得分，这应该给我们提供最佳的主题数量：
- en: '[PRE53]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Coherence Calculations Take Time
  id: totrans-476
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性计算需要时间
- en: Calculating the LDA model (and the coherence) is computationally expensive,
    so in real life it would be better to optimize the algorithm to calculate only
    a minimal number of models and perplexities. Sometimes it might make sense if
    you calculate the coherence scores for only a few numbers of topics.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 计算LDA模型（及其一致性）在计算上是昂贵的，因此在现实生活中，最好优化算法，仅计算少量模型和困惑度。有时，如果只计算少量主题的一致性得分，这可能是有意义的。
- en: 'Now we can choose which number of topics produces a good coherence score. Note
    that typically the score grows with the number of topics. Taking too many topics
    makes interpretation difficult:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以选择哪个主题数产生良好的一致性得分。注意，通常随着主题数量的增加，得分会增加。选择太多的主题会使解释变得困难：
- en: '[PRE54]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_08in04.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in04.jpg)'
- en: 'Overall, the graph grows with the number of topics, which is almost always
    the case. However, we can see “spikes” at 13 and 17 topics, so these numbers look
    like good choices. We will visualize the results for 17 topics:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，图表随主题数量增加而增长，这几乎总是情况。但是，我们可以看到在13和17个主题时出现了“峰值”，因此这些数字看起来是不错的选择。我们将为17个主题的结果进行可视化：
- en: '[PRE55]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`Out:`'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| **Topic 00** peace (0.02)'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 00** 和平 (0.02)'
- en: international (0.02)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 国际 (0.02)
- en: cooperation (0.01)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 合作 (0.01)
- en: countries (0.01)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.01)
- en: region (0.01) | **Topic 01** general (0.05)
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 地区 (0.01) | **主题 01** 将军 (0.05)
- en: assembly (0.04)
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 装配 (0.04)
- en: session (0.03)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 会议 (0.02)
- en: president (0.03)
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 总统 (0.03)
- en: mr (0.03) | **Topic 02** united (0.04)
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 先生 (0.03) | **主题 02** 联合 (0.04)
- en: nations (0.04)
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.04)
- en: states (0.03)
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.03)
- en: european (0.02)
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲 (0.02)
- en: union (0.02) | **Topic 03** nations (0.07)
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 联盟 (0.02) | **主题 03** 国家 (0.07)
- en: united (0.07)
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 联合 (0.07)
- en: security (0.03)
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 安全 (0.03)
- en: council (0.02)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 理事会 (0.02)
- en: international (0.02) | **Topic 04** development (0.03)
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 国际 (0.02) | **主题 04** 发展 (0.03)
- en: general (0.02)
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 将军 (0.02)
- en: conference (0.02)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 会议 (0.02)
- en: assembly (0.02)
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 装配 (0.02)
- en: sustainable (0.01) | **Topic 05** international (0.03)
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续 (0.01) | **主题 05** 国际 (0.03)
- en: terrorism (0.03)
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 恐怖主义 (0.03)
- en: states (0.01)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.01)
- en: iraq (0.01)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 伊拉克 (0.01)
- en: acts (0.01) |
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 行为 (0.01) |
- en: '| **Topic 06** peace (0.03)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 06** 和平 (0.03)'
- en: east (0.02)
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 东部 (0.02)
- en: middle (0.02)
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 中东 (0.02)
- en: israel (0.02)
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 以色列 (0.02)
- en: solution (0.01) | **Topic 07** africa (0.08)
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案 (0.01) | **主题 07** 非洲 (0.08)
- en: south (0.05)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 南方 (0.05)
- en: african (0.05)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 非洲 (0.05)
- en: namibia (0.02)
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 纳米比亚 (0.02)
- en: republic (0.01) | **Topic 08** states (0.04)
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 共和国 (0.01) | **主题 08** 国家 (0.04)
- en: small (0.04)
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 小 (0.04)
- en: island (0.03)
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 岛屿 (0.03)
- en: sea (0.02)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 海洋 (0.02)
- en: pacific (0.02) | **Topic 09** world (0.03)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 太平洋 (0.02) | **主题 09** 世界 (0.03)
- en: international (0.02)
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 国际 (0.02)
- en: problems (0.01)
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 (0.01)
- en: war (0.01)
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 战争 (0.01)
- en: peace (0.01) | **Topic 10** human (0.07)
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 和平 (0.01) | **主题 10** 人类 (0.07)
- en: rights (0.06)
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 权利 (0.06)
- en: law (0.02)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 法律 (0.02)
- en: respect (0.02)
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 尊重 (0.02)
- en: international (0.01) | **Topic 11** climate (0.03)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 国际 (0.01) | **主题 11** 气候 (0.03)
- en: change (0.03)
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 变革 (0.03)
- en: global (0.02)
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 全球 (0.02)
- en: environment (0.01)
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 环境 (0.01)
- en: energy (0.01) |
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 能源 (0.01) |
- en: '| **Topic 12** world (0.03)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题 12** 世界 (0.03)'
- en: people (0.02)
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 人们 (0.02)
- en: future (0.01)
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 未来 (0.01)
- en: years (0.01)
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 年度 (0.01)
- en: today (0.01) | **Topic 13** people (0.03)
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 今天 (0.01) | **主题 13** 人民 (0.03)
- en: independence (0.02)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 独立 (0.02)
- en: peoples (0.02)
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 人民 (0.02)
- en: struggle (0.01)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 斗争 (0.01)
- en: countries (0.01) | **Topic 14** people (0.02)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.01) | **主题 14** 人民 (0.02)
- en: country (0.02)
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 国家 (0.02)
- en: government (0.02)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 政府 (0.02)
- en: humanitarian (0.01)
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 人道主义 (0.01)
- en: refugees (0.01) | **Topic 15** countries (0.05)
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 难民 (0.01) | **主题 15** 国家 (0.05)
- en: development (0.03)
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 发展 (0.03)
- en: economic (0.03)
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 经济 (0.03)
- en: developing (0.02)
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 发展中 (0.02)
- en: trade (0.01) | **Topic 16** nuclear (0.06)
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 贸易 (0.01) | **主题 16** 核 (0.06)
- en: weapons (0.04)
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 武器 (0.04)
- en: disarmament (0.03)
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 裁军 (0.03)
- en: arms (0.03)
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 武器 (0.03)
- en: treaty (0.02) |
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 条约 (0.02) |
- en: Most of the topics are easy to interpret, but quite a few are difficult (like
    0, 3, 8) as they contain many words with small, but not too different, sizes.
    Is the topic model with 17 topics therefore easier to explain? Not really. The
    coherence measure is higher, but that does not necessarily mean a more obvious
    interpretation. In other words, relying solely on coherence scores can be dangerous
    if the number of topics gets too large. Although in theory higher coherence should
    contribute to better interpretability, it is often a trade-off, and choosing smaller
    numbers of topics can make life easier. Taking a look back at the coherence graph,
    10 seems to be a good value as it is a *local maximum* of the coherence score.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数主题都很容易解释，但有些主题很难（如0、3、8），因为它们包含许多单词，大小相近，但不完全相同。17个主题的主题模型是否更容易解释？实际上并非如此。连贯性得分更高，但这并不一定意味着更明显的解释。换句话说，如果主题数量过多，仅依赖连贯性得分可能是危险的。尽管理论上，较高的连贯性应有助于更好的可解释性，但通常存在权衡，选择较少的主题可以使生活更轻松。回顾连贯性图表，10似乎是一个不错的选择，因为它是连贯性得分的*局部最大值*。
- en: As it’s obviously difficult to find the “correct” number of topics, we will
    now take a look at an approach that creates hierarchical models and does not need
    a fixed number of topics as a parameter.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 由于明显很难找到“正确”的主题数量，我们现在将看看一种创建层次模型并且不需要固定主题数量作为参数的方法。
- en: 'Blueprint: Creating a Hierarchical Dirichlet Process with Gensim'
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用Gensim创建层次狄利克雷过程
- en: 'Take a step back and recall the visualization of the topics in [“Blueprint:
    Using LDA with Gensim”](#ch08usingldawithgensim). The sizes of the topics vary
    quite a bit, and some topics have a large overlap. It would be nice if the results
    gave us broader topics first and some subtopics below them. This is the exact
    idea of the hierarchical Dirichlet process (HDP). The hierarchical topic model
    should give us just a few broad topics that are well separated, then go into more
    detail by adding more words and getting more differentiated topic definitions.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '退一步，回想一下在 [“Blueprint: Using LDA with Gensim”](#ch08usingldawithgensim) 中关于主题的可视化。主题的大小差异很大，有些主题有较大的重叠。如果结果能先给我们更广泛的主题，然后在其下方列出一些子主题，那将是非常好的。这正是层次狄利克雷过程（HDP）的确切想法。层次主题模型应该先给我们几个广泛的主题，这些主题有良好的分离性，然后通过添加更多词汇和更详细的主题定义来进一步详细说明。'
- en: 'HDP is still quite new and has not yet been extensively analyzed. Gensim is
    also often used in research and has an experimental implementation of HDP integrated.
    As we can directly use our already existing vectorization, it’s not complicated
    to try it. Note that we are again using the bag-of-words vectorization as the
    Dirichlet processes themselves handle frequent words correctly:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: HDP 目前仍然比较新，尚未进行广泛的分析。Gensim 在研究中也经常被使用，并且已经集成了 HDP 的实验性实现。由于我们可以直接使用已有的向量化，尝试起来并不复杂。请注意，我们再次使用词袋向量化，因为狄利克雷过程本身可以正确处理频繁出现的词：
- en: '[PRE56]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'HDP can estimate the number of topics and can show all that it identified:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: HDP 能够估计主题的数量，并能展示其识别出的所有内容：
- en: '[PRE57]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`Out:`'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_08in05.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in05.jpg)'
- en: The results are sometimes difficult to understand. It can be an option to first
    perform a “rough” topic modeling with only a few topics. If you find out that
    a topic is really big or suspect that it might have subtopics, you can create
    a subset of the original corpus where the only documents included are those that
    have a significant mixture of this topic. This needs some manual interaction but
    often yields much better results compared to HDP. At this stage of development,
    we would not recommend using HDP exclusively.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 结果有时很难理解。可以先执行一个只包含少数主题的“粗略”主题建模。如果发现某个主题确实很大或者怀疑可能有子主题，可以创建原始语料库的子集，其中仅包含那些与该主题具有显著混合的文档。这需要一些手动交互，但通常比仅使用
    HDP 得到更好的结果。在这个开发阶段，我们不建议仅使用 HDP。
- en: Topic models focus on uncovering the topic structure of a large corpus of documents.
    As all documents are modeled as a mixture of different topics, they are not well-suited
    for assigning documents to exactly one topic. This can be achieved using clustering.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型专注于揭示大量文档语料库的主题结构。由于所有文档被建模为不同主题的混合物，它们不适合于将文档分配到确切的一个主题中。这可以通过聚类来实现。
- en: 'Blueprint: Using Clustering to Uncover the Structure of Text Data'
  id: totrans-568
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Blueprint: 使用聚类揭示文本数据的结构'
- en: Apart from topic modeling, there is a multitude of other unsupervised methods.
    Not all are suitable for text data, but many clustering algorithms can be used.
    Compared to topic modeling, it is important for us to know that each document
    (or paragraph) gets assigned to exactly one cluster.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主题建模，还有许多其他无监督方法。并非所有方法都适用于文本数据，但许多聚类算法可以使用。与主题建模相比，对我们来说重要的是每个文档（或段落）都被分配到一个簇中。
- en: Clustering Works Well for Mono-Typical Texts
  id: totrans-570
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于单一类型的文本，聚类效果良好
- en: In our case, it is a reasonable assumption that each document belongs to exactly
    one cluster, as there are probably not too many different things contained in
    one paragraph. For larger text fragments, we would rather use topic modeling to
    take possible mixtures into account.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，合理假设每个文档属于一个簇，因为一个段落中可能包含的不同内容并不多。对于更大的文本片段，我们更倾向于使用主题建模来考虑可能的混合情况。
- en: Most clustering methods need the number of clusters as a parameter, while there
    are a few (like mean-shift) that can guess the correct number of clusters. Most
    of the latter do not work well with sparse data and therefore are not suitable
    for text analytics. In our case, we decided to use k-means clustering, but birch
    or spectral clustering should work in a similar manner. There are a few nice explanations
    of how the k-means algorithm works.^([4](ch08.xhtml#idm45634185352648))
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数聚类方法需要簇的数量作为参数，虽然有少数方法（如均值漂移）可以猜测正确的簇数量。后者大多数不适用于稀疏数据，因此不适合文本分析。在我们的情况下，我们决定使用
    k-means 聚类，但 birch 或谱聚类应该以类似的方式工作。有几种解释说明了 k-means 算法的工作原理。^([4](ch08.xhtml#idm45634185352648))
- en: Clustering Is Much Slower Than Topic Modeling
  id: totrans-573
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类比主题建模慢得多
- en: For most algorithms, clustering takes considerable time, much more than even
    LDA. So, be prepared to wait for roughly one hour when executing the clustering
    in the next code fragment.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数算法，聚类需要相当长的时间，甚至比 LDA 还要长。因此，在执行下一个代码片段中的聚类时，请做好大约等待一小时的准备。
- en: 'The scikit-learn API for clustering is similar to what we have seen with topic
    models:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的聚类 API 与我们在主题模型中看到的类似：
- en: '[PRE58]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'But now it’s much easier to find out how many paragraphs belong to which cluster.
    Everything necessary is in the `labels_` field of the `k_means_para` object. For
    each document, it contains the label that was assigned by the clustering algorithm:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在要找出有多少段落属于哪个聚类变得更容易了。所有必要的东西都在 `k_means_para` 对象的 `labels_` 字段中。对于每个文档，它包含了聚类算法分配的标签：
- en: '[PRE60]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '`Out:`'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE61]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In many cases, you might already have found some conceptual problems here.
    If the data is too heterogeneous, most clusters tend to be small (containing a
    comparatively small vocabulary) and are accompanied by a large cluster that absorbs
    all the rest. Fortunately (and due to the short paragraphs), this is not the case
    here; cluster 0 is much bigger than the others, but it’s not orders of magnitude.
    Let’s visualize the distribution with the y-axis showing the size of the clusters
    ([Figure 8-6](#fig-cluster-size)):'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，你可能已经发现了一些概念上的问题。如果数据太异构，大多数聚类往往很小（包含相对较小的词汇），并伴随着一个吸收所有剩余的大聚类。幸运的是（由于段落很短），这在这里并不是问题；聚类
    0 比其他聚类要大得多，但它并不是数量级。让我们用 y 轴显示聚类的大小来可视化分布（参见 [图 8-6](#fig-cluster-size)）：
- en: '[PRE62]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Visualizing the clusters works in a similar way to the topic models. However,
    we have to calculate the individual feature contributions manually. For this,
    we add up the TF-IDF vectors of all documents in the cluster and keep only the
    largest values.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化聚类的工作方式与主题模型类似。但是，我们必须手动计算各个特征的贡献。为此，我们将集群中所有文档的 TF-IDF 向量相加，并仅保留最大值。
- en: '![](Images/btap_08in06.jpg)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in06.jpg)'
- en: Figure 8-6\. Visualization of the size of the clusters.
  id: totrans-586
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. 聚类大小的可视化。
- en: 'These are the weights for their corresponding words. In fact, that’s the only
    change compared to the previous code:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是它们相应单词的权重。实际上，这与前面的代码唯一的区别就是：
- en: '[PRE63]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Out:`'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_08in07.jpg)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_08in07.jpg)'
- en: As you can see, the results are (fortunately) not too different from the various
    topic modeling approaches; you might recognize the topics of nuclear weapons,
    South Africa, general assembly, etc. Note, however, that the clusters are more
    pronounced. In other words, they have more specific words. Unfortunately, this
    is not true for the biggest cluster, 1, which has no clear direction but many
    words with similar, smaller sizes. This is a typical phenomenon of clustering
    algorithms compared to topic modeling.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，结果与各种主题建模方法（幸运地）并没有太大不同；你可能会认出核武器、南非、大会等主题。然而，请注意，聚类更加明显。换句话说，它们有更具体的单词。不幸的是，这并不适用于最大的聚类
    1，它没有明确的方向，但有许多具有相似较小尺寸的单词。这是与主题建模相比聚类算法的典型现象。
- en: Clustering calculations can take quite long, especially compared to NMF topic
    models. On the positive side, we are now free to choose documents in a certain
    cluster (opposed to a topic model, this is well-defined) and perform additional,
    more sophisticated operations, such as hierarchical clustering, etc.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类计算可能需要相当长的时间，尤其是与 NMF 主题模型相比。积极的一面是，我们现在可以自由选择某个聚类中的文档（与主题模型相反，这是明确定义的）并执行其他更复杂的操作，如层次聚类等。
- en: The quality of the clustering can be calculated by using coherence or the Calinski-Harabasz
    score. These metrics are not optimized for sparse data and take a long time to
    calculate, and therefore we skip this here.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的质量可以通过使用一致性或 Calinski-Harabasz 分数来计算。这些指标并不针对稀疏数据进行优化，计算时间较长，因此我们在这里跳过它们。
- en: Further Ideas
  id: totrans-594
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步的想法
- en: 'In this chapter, we have shown different methods for performing topic modeling.
    However, we have only scratched the surface of the possibilities:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了执行主题建模的不同方法。但是，我们只是触及了可能性的表面：
- en: It’s possible to add n-grams in the vectorization process. In scikit-learn this
    is straightforward by using the `ngram_range` parameter. Gensim has a special
    `Phrases` class for that. Due to the higher TF-IDF weights of n-grams, they can
    contribute considerably to the features of a topic and add a lot of context information.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在向量化过程中添加n-gram。在scikit-learn中，通过使用`ngram_range`参数可以轻松实现这一点。Gensim有一个特殊的`Phrases`类。由于n-gram具有更高的TF-IDF权重，它们可以在话题的特征中起到重要作用，并添加大量的上下文信息。
- en: As we have used years to have time-dependent topic models, you could also use
    countries or continents and find the topics that are most relevant in the speeches
    of their ambassadors.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们已经使用多年来依赖时间相关的话题模型，您也可以使用国家或大洲，并找出其大使在演讲中最相关的话题。
- en: Calculate the coherence score for an LDA topic model using the whole speeches
    instead of the paragraphs and compare the scores.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用整个演讲而不是段落来计算LDA话题模型的一致性分数，并进行比较。
- en: Summary and Recommendation
  id: totrans-599
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和建议
- en: In your daily work, it might turn out that unsupervised methods such as topic
    modeling or clustering are often used as first methods to understand the content
    of unknown text corpora. It is further useful to check whether the right features
    have been chosen or this can still be optimized.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常工作中，无监督方法（如话题建模或聚类）通常被用作了解未知文本语料库内容的首选方法。进一步检查是否选择了正确的特征或是否仍可优化，这也是非常有用的。
- en: One of the most important decisions is the entity on which you will be calculating
    the topics. As shown in our blueprint example, documents don’t always have to
    be the best choice, especially when they are quite long and consist of algorithmically
    determinable subentities.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 计算话题时，最重要的决定之一是你将用来计算话题的实体。正如我们蓝图示例所示，文件并不总是最佳选择，特别是当它们非常长，并且由算法确定的子实体组成时。
- en: Finding the correct number of topics is always a challenge. Normally, this must
    be solved iteratively by calculating the quality indicators. A frequently used,
    more pragmatic approach is to try with a reasonable number of topics and find
    out whether the results can be interpreted.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 找到正确的话题数量始终是一个挑战。通常，这必须通过计算质量指标来迭代解决。一个经常使用的更为实用的方法是尝试合理数量的话题，并找出结果是否可解释。
- en: Using a (much) higher number of topics (like a few hundred), topic models are
    often used as techniques for the dimensionality reduction of text documents. With
    the resulting vectorizations, similarity scores can then be calculated in the
    latent space and frequently yield better results compared to the naive distance
    in TF-IDF space.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 使用（大量）更多的话题（如几百个），话题模型经常被用作文本文档的降维技术。通过生成的向量化，可以在潜在空间中计算相似度分数，并且通常与TF-IDF空间中的朴素距离相比，产生更好的结果。
- en: Conclusion
  id: totrans-604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Topic models are a powerful technique and are not computationally expensive.
    Therefore, they can be used widely in text analytics. The first and foremost reason
    to use them is uncovering the latent structure of a document corpus.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 话题模型是一种强大的技术，并且计算成本不高。因此，它们可以广泛用于文本分析。使用它们的首要原因是揭示文档语料库的潜在结构。
- en: Topic models are also useful for getting a summarization and an idea about the
    structure of large unknown texts. For this reason, they are often used routinely
    in the beginning of an analysis.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 话题模型对于获取大型未知文本的总结和结构的概念也是有用的。因此，它们通常在分析的开始阶段被常规使用。
- en: As there is a large number of different algorithms and implementations, it makes
    sense to experiment with the different methods and see which one yields the best
    results for a given text corpus. The linear-algebra-based methods are quite fast
    and make analyses possible by changing the number of topics combined with calculating
    the respective quality indicators.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在大量不同的算法和实现方法，因此尝试不同的方法并查看哪种方法在给定的文本语料库中产生最佳结果是有意义的。基于线性代数的方法速度很快，并且通过计算相应的质量指标，可以进行分析。
- en: Aggregating data in different ways before performing topic modeling can lead
    to interesting variations. As we have seen in the UN general debates dataset,
    paragraphs were more suited as the speakers talked about one topic after the other.
    If you have a corpus with texts from many authors, concatenating all texts per
    author will give you persona models for different types of authors.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行主题建模之前以不同方式聚合数据可以导致有趣的变化。正如我们在联合国大会辩论数据集中看到的那样，段落更适合，因为发言者一个接一个地讨论了一个话题。如果您有来自许多作者的语料库，将每位作者的所有文本串联起来将为您提供不同类型作者的人物模型。
- en: '^([1](ch08.xhtml#idm45634189170488-marker)) Blei, David M., et al. “Latent
    Dirichlet Allocation.” *Journal of Machine Learning Research* 3 (4–5): 993–1022\.
    doi:10.1162/jmlr.2003.3.4-5.993.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch08.xhtml#idm45634189170488-marker)) Blei, David M., et al. “潜在狄利克雷分配。”
    *机器学习研究杂志* 3 (4–5): 993–1022\. doi:10.1162/jmlr.2003.3.4-5.993.'
- en: ^([2](ch08.xhtml#idm45634187759704-marker)) For a more detailed description,
    see the [Wikipedia page](https://oreil.ly/yr5yA).
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.xhtml#idm45634187759704-marker)) 要了解更详细的描述，请参阅[Wikipedia页面](https://oreil.ly/yr5yA)。
- en: ^([3](ch08.xhtml#idm45634187547192-marker)) pyLDAvis must be installed separately
    using **`pip install pyldavis`** or **`conda install pyldavis`**.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.xhtml#idm45634187547192-marker)) pyLDAvis必须单独安装，使用**`pip install
    pyldavis`**或**`conda install pyldavis`**。
- en: ^([4](ch08.xhtml#idm45634185352648-marker)) See, for example, Andrey A. Shabalin’s
    [k-means clustering page](https://oreil.ly/OTGWX) or Naftali Harris’s [“Visualizing
    K-Means Clustering”](https://oreil.ly/Po3bL).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.xhtml#idm45634185352648-marker)) 参见，例如，安德烈·A·沙巴林的[k-means聚类页面](https://oreil.ly/OTGWX)或纳夫塔利·哈里斯的[“可视化K-Means聚类”](https://oreil.ly/Po3bL)。
