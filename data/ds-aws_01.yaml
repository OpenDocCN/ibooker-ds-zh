- en: Chapter 1\. Introduction to Data Science on AWS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。在AWS上介绍数据科学
- en: In this chapter, we discuss the benefits of building data science projects in
    the cloud. We start by discussing the benefits of cloud computing. Next, we describe
    a typical machine learning workflow and the common challenges to move our models
    and applications from the prototyping phase to production. We touch on the overall
    benefits of developing data science projects on Amazon Web Services (AWS) and
    introduce the relevant AWS services for each step of the model development workflow.
    We also share architectural best practices, specifically around operational excellence,
    security, reliability, performance, and cost optimization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论在云中构建数据科学项目的好处。我们首先讨论云计算的好处。接下来，我们描述一个典型的机器学习工作流程及从原型阶段将我们的模型和应用程序移动到生产环境的常见挑战。我们介绍在亚马逊网络服务(AWS)上开发数据科学项目的整体好处，并介绍每个模型开发工作流程步骤相关的AWS服务。我们还分享了关于运营卓越、安全性、可靠性、性能和成本优化的架构最佳实践。
- en: Benefits of Cloud Computing
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云计算的好处
- en: Cloud computing enables the on-demand delivery of IT resources via the internet
    with pay-as-you-go pricing. So instead of buying, owning, and maintaining our
    own data centers and servers, we can acquire technology such as compute power,
    storage, databases, and other services on an as-needed basis. Similar to a power
    company sending electricity instantly when we flip a light switch in our home,
    the cloud provisions IT resources on-demand with the click of a button or invocation
    of an API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算通过互联网按需提供IT资源，并采用按使用量付费的定价模式。因此，我们可以获得计算能力、存储、数据库和其他服务等技术，而不是购买、拥有和维护自己的数据中心和服务器。类似于电力公司在我们家里打开电灯开关时立即发送电力，云通过点击按钮或调用API按需提供IT资源。
- en: “There is no compression algorithm for experience” is a famous quote by Andy
    Jassy, CEO, Amazon Web Services. The quote expresses the company’s long-standing
    experience in building reliable, secure, and performant services since 2006.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “没有经验的压缩算法”是亚马逊网络服务首席执行官安迪·贾西的一句名言。这句话表达了公司自2006年以来在建立可靠、安全和高性能服务方面的长期经验。
- en: 'AWS has been continually expanding its service portfolio to support virtually
    any cloud workload, including many services and features in the area of artificial
    intelligence and machine learning. Many of these AI and machine learning services
    stem from Amazon’s pioneering work in recommender systems, computer vision, speech/text,
    and neural networks over the past 20 years. A paper from 2003 titled [“Amazon.com
    Recommendations: Item-to-Item Collaborative Filtering”](https://oreil.ly/UlCDV)
    recently won the Institute of Electrical and Electronics Engineers award as a
    paper that withstood the “test of time.” Let’s review the benefits of cloud computing
    in the context of data science projects on AWS.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 'AWS一直在不断扩展其服务组合，以支持几乎所有的云工作负载，包括人工智能和机器学习领域的许多服务和功能。这些AI和机器学习服务中的许多源于亚马逊在过去20年中在推荐系统、计算机视觉、语音/文本和神经网络方面的开创性工作。2003年的一篇题为[“Amazon.com
    Recommendations: Item-to-Item Collaborative Filtering”](https://oreil.ly/UlCDV)的论文最近荣获电气和电子工程师学会颁发的“历久弥新”奖。让我们在AWS上的数据科学项目背景下审视云计算的好处。'
- en: Agility
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏捷性
- en: Cloud computing lets us spin up resources as we need them. This enables us to
    experiment quickly and frequently. Maybe we want to test a new library to run
    data-quality checks on our dataset, or speed up model training by leveraging the
    newest generation of GPU compute resources. We can spin up tens, hundreds, or
    even thousands of servers in minutes to perform those tasks. If an experiment
    fails, we can always deprovision those resources without any risk.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算使我们能够根据需要快速和频繁地启动资源。这使我们能够快速进行实验。也许我们想测试一个新的库来运行数据质量检查，或者通过利用最新一代GPU计算资源加速模型训练。我们可以在几分钟内启动数十、数百甚至数千台服务器来执行这些任务。如果一个实验失败，我们可以随时取消这些资源而没有任何风险。
- en: Cost Savings
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本节约
- en: Cloud computing allows us to trade capital expenses for variable expenses. We
    only pay for what we use with no need for upfront investments in hardware that
    may become obsolete in a few months. If we spin up compute resources to perform
    our data-quality checks, data transformations, or model training, we only pay
    for the time those compute resources are in use. We can achieve further cost savings
    by leveraging Amazon EC2 Spot Instances for our model training. Spot Instances
    let us take advantage of unused EC2 capacity in the AWS cloud and come with up
    to a 90% discount compared to on-demand instances. Reserved Instances and Savings
    Plans allow us to save money by prepaying for a given amount of time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算允许我们将资本支出转换为可变支出。我们只支付我们使用的部分，无需预先投资于可能在几个月后过时的硬件。如果我们启动计算资源来执行数据质量检查、数据转换或模型训练，我们只需为使用这些计算资源的时间付费。通过利用Amazon
    EC2 Spot实例进行模型训练，我们可以进一步节省成本。Spot实例允许我们利用AWS云中未使用的EC2容量，并且相较于按需实例可享受高达90%的折扣。预留实例和储蓄计划允许我们通过预付一定时间来节省资金。
- en: Elasticity
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性
- en: Cloud computing enables us to automatically scale our resources up or down to
    match our application needs. Let’s say we have deployed our data science application
    to production and our model is serving real-time predictions. We can now automatically
    scale up the model hosting resources in case we observe a peak in model requests.
    Similarly, we can automatically scale down the resources when the number of model
    requests drops. There is no need to overprovision resources to handle peak loads.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算使我们能够根据应用程序的需求自动扩展或缩减资源。假设我们已将数据科学应用部署到生产环境，并且我们的模型正在提供实时预测。如果我们观察到模型请求量的高峰，我们现在可以自动扩展模型托管资源。同样地，当模型请求量下降时，我们也可以自动缩减资源。无需过度配置资源来处理高峰负载。
- en: Innovate Faster
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更快创新
- en: Cloud computing allows us to innovate faster as we can focus on developing applications
    that differentiate our business, rather than spending time on the undifferentiated
    heavy lifting of managing infrastructure. The cloud helps us experiment with new
    algorithms, frameworks, and hardware in seconds versus months.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算允许我们更快地创新，因为我们可以专注于开发区别于我们业务的应用程序，而不是花时间在管理基础设施的无差别的重活上。云帮助我们在几秒钟而不是几个月内尝试新算法、框架和硬件。
- en: Deploy Globally in Minutes
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 几分钟内全球部署
- en: Cloud computing lets us deploy our data science applications globally within
    minutes. In our global economy, it is important to be close to our customers.
    AWS has the concept of a Region, which is a physical location around the world
    where AWS clusters data centers. Each group of logical data centers is called
    an *Availability Zone* (AZ). Each AWS Region consists of multiple, isolated, and
    physically separate AZs within a geographic area. [The number of AWS Regions and
    AZs is continuously growing](https://oreil.ly/qegDk).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算使我们能够在几分钟内全球部署我们的数据科学应用。在全球经济中，与客户接近非常重要。AWS引入了区域的概念，这是AWS数据中心集群的物理位置，每个逻辑数据中心组称为一个*可用区*（AZ）。每个AWS区域包括多个隔离且物理分离的可用区，这些区域分布在全球各地。[AWS区域和可用区的数量正在不断增长](https://oreil.ly/qegDk)。
- en: We can leverage the global footprint of AWS Regions and AZs to deploy our data
    science applications close to our customers, improve application performance with
    ultra-fast response times, and comply with the data-privacy restrictions of each
    Region.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用AWS区域和可用区的全球部署优势，使我们的数据科学应用靠近客户，通过超快的响应时间提高应用性能，并遵守每个区域的数据隐私限制。
- en: Smooth Transition from Prototype to Production
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从原型到生产的平稳过渡
- en: One of the benefits of developing data science projects in the cloud is the
    smooth transition from prototype to production. We can switch from running model
    prototyping code in our notebook to running data-quality checks or distributed
    model training across petabytes of data within minutes. And once we are done,
    we can deploy our trained models to serve real-time or batch predictions for millions
    of users across the globe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中开发数据科学项目的好处之一是从原型到生产的平稳过渡。我们可以在几分钟内从在笔记本中运行模型原型代码切换到运行数据质量检查或跨PB级数据进行分布式模型训练。一旦完成，我们可以部署我们训练好的模型，为全球数百万用户提供实时或批处理预测。
- en: Prototyping often happens in single-machine development environments using Jupyter
    Notebook, NumPy, and pandas. This approach works fine for small data sets. When
    scaling out to work with large datasets, we will quickly exceed the single machine’s
    CPU and RAM resources. Also, we may want to use GPUs—or multiple machines—to accelerate
    our model training. This is usually not possible with a single machine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 原型开发通常在单机开发环境中使用 Jupyter Notebook、NumPy 和 pandas 进行。这种方法对小数据集效果良好。当扩展到大数据集时，我们很快会超出单机的
    CPU 和 RAM 资源。此外，我们可能需要使用 GPU 或多台机器来加速模型训练。这通常是单机无法实现的。
- en: The next challenge arises when we want to deploy our model (or application)
    to production. We also need to ensure our application can handle thousands or
    millions of concurrent users at global scale.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将模型（或应用程序）部署到生产环境时，下一个挑战就出现了。我们还需要确保我们的应用程序能够处理全球规模的成千上万个并发用户。
- en: Production deployment often requires a strong collaboration between various
    teams including data science, data engineering, application development, and DevOps.
    And once our application is successfully deployed, we need to continuously monitor
    and react to model performance and data-quality issues that may arise after the
    model is pushed to production.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生产部署通常需要数据科学、数据工程、应用程序开发和 DevOps 等各个团队之间的强大协作。一旦我们的应用程序成功部署，我们需要持续监控并对模型性能和数据质量问题做出反应，这些问题可能在模型推送到生产后出现。
- en: Developing data science projects in the cloud enables us to transition our models
    smoothly from prototyping to production while removing the need to build out our
    own physical infrastructure. Managed cloud services provide us with the tools
    to automate our workflows and deploy models into a scalable and highly performant
    production environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中开发数据科学项目使我们能够顺利将我们的模型从原型转换到生产，同时消除构建自己的物理基础设施的需求。托管云服务为我们提供了工具，可以自动化我们的工作流程，并将模型部署到可扩展和高性能的生产环境中。
- en: Data Science Pipelines and Workflows
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学流水线和工作流程
- en: Data science pipelines and workflows involve many complex, multidisciplinary,
    and iterative steps. Let’s take a typical machine learning model development workflow
    as an example. We start with data preparation, then move to model training and
    tuning. Eventually, we deploy our model (or application) into a production environment.
    Each of those steps consists of several subtasks as shown in [Figure 1-1](#a_typical_machine_learning_workflow_inv).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的流水线和工作流程涉及许多复杂的、跨学科的和迭代的步骤。让我们以典型的机器学习模型开发工作流程为例。我们从数据准备开始，然后进行模型训练和调整。最终，我们将我们的模型（或应用程序）部署到生产环境中。每一个步骤都包含几个子任务，如图
    [1-1](#a_typical_machine_learning_workflow_inv) 所示。
- en: '![](assets/dsaw_0101.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0101.png)'
- en: Figure 1-1\. A typical machine learning workflow involves many complex, multidisciplinary,
    and iterative steps.
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 [1-1](#a_typical_machine_learning_workflow_inv)。典型的机器学习工作流程涉及许多复杂的、跨学科的和迭代的步骤。
- en: If we are using AWS, our raw data is likely already in Amazon Simple Storage
    Service (Amazon S3) and stored as CSV, Apache Parquet, or the equivalent. We can
    start training models quickly using the Amazon AI or automated machine learning
    (AutoML) services to establish baseline model performance by pointing directly
    to our dataset and clicking a single “train” button. We dive deep into the AI
    Services and AutoML in Chapters [2](ch02.html#data_science_use_cases) and [3](ch03.html#automated_machine_learnin).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在使用 AWS，我们的原始数据很可能已经存储在亚马逊简单存储服务（Amazon S3）中，以 CSV、Apache Parquet 或等效格式存储。我们可以通过直接指向数据集并点击一个“训练”按钮，快速开始使用亚马逊人工智能或自动化机器学习（AutoML）服务来建立基线模型性能。我们在第
    [2](ch02.html#data_science_use_cases) 章和第 [3](ch03.html#automated_machine_learnin)
    章深入探讨 AI 服务和 AutoML。
- en: For more customized machine learning models—the primary focus of this book—we
    can start the manual data ingestion and exploration phases, including data analysis,
    data-quality checks, summary statistics, missing values, quantile calculations,
    data skew analysis, correlation analysis, etc. We dive deep into data ingestion
    and exploration in Chapters [4](ch04.html#ingest_data_into_the_cloud) and [5](ch05.html#explore_the_dataset).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更加定制化的机器学习模型——本书的主要关注点——我们可以开始手动数据摄入和探索阶段，包括数据分析、数据质量检查、汇总统计、缺失值、分位数计算、数据偏斜分析、相关性分析等等。我们在第
    [4](ch04.html#ingest_data_into_the_cloud) 章和第 [5](ch05.html#explore_the_dataset)
    章深入探讨数据摄入和探索。
- en: We should then define the machine learning problem type—regression, classification,
    clustering, etc. Once we have identified the problem type, we can select a machine
    learning algorithm best suited to solve the given problem. Depending on the algorithm
    we choose, we need to select a subset of our data to train, validate, and test
    our model. Our raw data usually needs to be transformed into mathematical vectors
    to enable numerical optimization and model training. For example, we might decide
    to transform categorical columns into one-hot encoded vectors or convert text-based
    columns into word-embedding vectors. After we have transformed a subset of the
    raw data into features, we should split the features into train, validation, and
    test feature sets to prepare for model training, tuning, and testing. We dive
    deep into feature selection and transformation in Chapters [5](ch05.html#explore_the_dataset)
    and [6](ch06.html#prepare_the_dataset_for_model_training).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应该定义机器学习问题类型——回归、分类、聚类等。一旦确定了问题类型，我们可以选择最适合解决给定问题的机器学习算法。根据我们选择的算法，我们需要选择我们数据的子集来训练、验证和测试我们的模型。我们的原始数据通常需要转换为数学向量，以实现数值优化和模型训练。例如，我们可能决定将分类列转换为独热编码向量，或将基于文本的列转换为词嵌入向量。在将一部分原始数据转换为特征后，我们应该将这些特征拆分为训练、验证和测试特征集，为模型训练、调优和测试做准备。我们在第[5章](ch05.html#explore_the_dataset)和第[6章](ch06.html#prepare_the_dataset_for_model_training)深入探讨了特征选择和转换。
- en: In the model training phase, we pick an algorithm and train our model with our
    training feature set to verify that our model code and algorithm is suited to
    solve the given problem. We dive deep into model training in [Chapter 7](ch07.html#train_your_first_model).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练阶段，我们选择一个算法，并使用训练特征集训练我们的模型，以验证我们的模型代码和算法是否适合解决给定的问题。我们在[第7章](ch07.html#train_your_first_model)深入探讨了模型训练。
- en: In the model tuning phase, we tune the algorithm hyper-parameters and evaluate
    model performance against the validation feature set. We repeat these steps—adding
    more data or changing hyper-parameters as needed—until the model achieves the
    expected results on the test feature set. These results should be in line with
    our business objective before pushing the model to production. We dive deep into
    hyper-parameter tuning in [Chapter 8](ch08.html#train_and_optimize_models_at_scale).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型调优阶段，我们调整算法的超参数，并根据验证特征集评估模型性能。我们重复这些步骤——根据需要添加更多数据或更改超参数——直到模型在测试特征集上达到预期结果。在[第8章](ch08.html#train_and_optimize_models_at_scale)中，我们深入探讨了超参数调优。
- en: The final stage—moving from prototyping into production—often presents the biggest
    challenge to data scientists and machine learning practitioners. We dive deep
    into model deployment in [Chapter 9](ch09.html#deploy_models_to_production).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终阶段——从原型开发进入生产——通常对数据科学家和机器学习从业者来说是最大的挑战。我们在[第9章](ch09.html#deploy_models_to_production)深入探讨了模型部署。
- en: In [Chapter 10](ch10.html#pipelines_and_mlops), we tie everything together into
    an automated pipeline. In [Chapter 11](ch11.html#streaming_analytics_and_machine_lear),
    we perform data analytics and machine learning on streaming data. [Chapter 12](ch12.html#secure_data_science_on_aws)
    summarizes best practices for securing data science in the cloud.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#pipelines_and_mlops)，我们将所有内容综合到一个自动化流水线中。在[第11章](ch11.html#streaming_analytics_and_machine_lear)，我们对流数据进行数据分析和机器学习。[第12章](ch12.html#secure_data_science_on_aws)总结了在云中保障数据科学的最佳实践。
- en: Once we have built every individual step of our machine learning workflow, we
    can start automating the steps into a single, repeatable machine learning pipeline.
    When new data lands in S3, our pipeline reruns with the latest data and pushes
    the latest model into production to serve our applications. There are several
    workflow orchestration tools and AWS services available to help us build automated
    machine learning pipelines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们建立了机器学习工作流的每个步骤，我们可以开始将这些步骤自动化为单一、可重复的机器学习流水线。当新数据进入S3时，我们的流水线会使用最新数据重新运行，并将最新模型推送到生产环境以服务我们的应用程序。有几种工作流编排工具和AWS服务可供我们构建自动化的机器学习流水线。
- en: Amazon SageMaker Pipelines
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon SageMaker Pipelines
- en: Amazon SageMaker Pipelines are the standard, full-featured, and most complete
    way to implement AI and machine learning pipelines on Amazon SageMaker. SageMaker
    Pipelines have integration with SageMaker Feature Store, SageMaker Data Wrangler,
    SageMaker Processing Jobs, SageMaker Training Jobs, SageMaker Hyper-Parameter
    Tuning Jobs, SageMaker Model Registry, SageMaker Batch Transform, and SageMaker
    Model Endpoints, which we discuss throughout the book. We will dive deep into
    managed SageMaker Pipelines in [Chapter 10](ch10.html#pipelines_and_mlops) along
    with discussions on how to build pipelines with AWS Step Functions, Kubeflow Pipelines,
    Apache Airflow, MLflow, TFX, and human-in-the-loop workflows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Pipelines 是在亚马逊 SageMaker 上实现 AI 和机器学习流水线的标准、功能完整且最全面的方式。SageMaker
    Pipelines 与 SageMaker 特征存储、SageMaker 数据整形器、SageMaker 处理作业、SageMaker 训练作业、SageMaker
    超参数调整作业、SageMaker 模型注册表、SageMaker 批量转换和 SageMaker 模型端点集成，我们将在整本书中讨论这些。我们将在 [第10章](ch10.html#pipelines_and_mlops)
    深入研究托管的 SageMaker Pipelines，同时讨论如何使用 AWS Step Functions、Kubeflow Pipelines、Apache
    Airflow、MLflow、TFX 和人工协作工作流建立流水线。
- en: AWS Step Functions Data Science SDK
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Step Functions Data Science SDK
- en: Step Functions, a managed AWS service, is a great option for building complex
    workflows without having to build and maintain our own infrastructure. We can
    use the Step Functions Data Science SDK to build machine learning pipelines from
    Python environments, such as Jupyter Notebook. We will dive deeper into the managed
    Step Functions for machine learning in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Step Functions 是一个托管的 AWS 服务，非常适合构建复杂的工作流，无需建立和维护自己的基础设施。我们可以使用 Step Functions
    Data Science SDK 从诸如 Jupyter Notebook 的 Python 环境构建机器学习流水线。我们将在 [第10章](ch10.html#pipelines_and_mlops)
    深入探讨托管的用于机器学习的 Step Functions。
- en: Kubeflow Pipelines
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines
- en: Kubeflow is a relatively new ecosystem built on Kubernetes that includes an
    orchestration subsystem called *Kubeflow Pipeline*s. With Kubeflow, we can restart
    failed pipelines, schedule pipeline runs, analyze training metrics, and track
    pipeline lineage. We will dive deeper into managing a Kubeflow cluster on Amazon
    Elastic Kubernetes Service (Amazon EKS) in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是建立在 Kubernetes 上的一个相对较新的生态系统，包括一个名为*Kubeflow Pipelines*的编排子系统。使用 Kubeflow，我们可以重新启动失败的流水线，调度流水线运行，分析训练指标，并跟踪流水线的来源。我们将在
    [第10章](ch10.html#pipelines_and_mlops) 深入探讨如何在亚马逊弹性 Kubernetes 服务（Amazon EKS）上管理
    Kubeflow 集群。
- en: Managed Workflows for Apache Airflow on AWS
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 AWS 上的托管 Apache Airflow 工作流
- en: Apache Airflow is a very mature and popular option primarily built to orchestrate
    data engineering and extract-transform-load (ETL) pipelines. We can use Airflow
    to author workflows as directed acyclic graphs of tasks. The Airflow scheduler
    executes our tasks on an array of workers while following the specified dependencies.
    We can visualize pipelines running in production, monitor progress, and troubleshoot
    issues when needed via the Airflow user interface. We will dive deeper into Amazon
    Managed Workflows for Apache Airflow (Amazon MWAA) in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow 是一个非常成熟和流行的选项，主要用于编排数据工程和ETL（抽取-转换-加载）流水线。我们可以使用 Airflow 来编写任务的有向无环图（DAG）工作流。Airflow
    调度程序在一组工作节点上执行任务，并遵循指定的依赖关系。我们可以通过 Airflow 用户界面可视化运行中的流水线，监视进度，并在需要时解决问题。我们将在
    [第10章](ch10.html#pipelines_and_mlops) 深入探讨亚马逊托管的 Apache Airflow 工作流（Amazon MWAA）。
- en: MLflow
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLflow
- en: MLflow is an open source project that initially focused on experiment tracking
    but now supports pipelines called *MLflow Workflows*. We can use MLflow to track
    experiments with Kubeflow and Apache Airflow workflows as well. MLflow requires
    us to build and maintain our own Amazon EC2 or Amazon EKS clusters, however. We
    will discuss MLflow in more detail in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 是一个开源项目，最初专注于实验追踪，现在支持名为*MLflow Workflows*的流水线。我们可以在 Kubeflow 和 Apache
    Airflow 工作流中使用 MLflow 来追踪实验。然而，MLflow 要求我们建立和维护自己的亚马逊 EC2 或亚马逊 EKS 集群。我们将在 [第10章](ch10.html#pipelines_and_mlops)
    更详细地讨论 MLflow。
- en: TensorFlow Extended
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Extended
- en: TensorFlow Extended (TFX) is an open source collection of Python libraries used
    within a pipeline orchestrator such as AWS Step Functions, Kubeflow Pipelines,
    Apache Airflow, or MLflow. TFX is specific to TensorFlow and depends on another
    open source project, Apache Beam, to scale beyond a single processing node. We
    will discuss TFX in more detail in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Extended（TFX）是一组开源的 Python 库，用于在管道编排器（如 AWS Step Functions、Kubeflow
    Pipelines、Apache Airflow 或 MLflow）中使用。TFX 特定于 TensorFlow，并依赖于另一个开源项目 Apache Beam，以实现超越单个处理节点的扩展。我们将在[第
    10 章](ch10.html#pipelines_and_mlops)更详细地讨论 TFX。
- en: Human-in-the-Loop Workflows
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人在回路工作流程
- en: While AI and machine learning services make our lives easier, humans are far
    from being obsolete. In fact, the concept of “human-in-the-loop” has emerged as
    an important cornerstone in many AI/ML workflows. Humans provide important quality
    assurance for sensitive and regulated models in production.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AI 和机器学习服务使我们的生活更加轻松，但人类远未被淘汰。事实上，“人在回路”概念已成为许多 AI/ML 工作流中的重要基石。在生产中，人类为敏感和受监管的模型提供重要的质量保证。
- en: Amazon Augmented AI (Amazon A2I) is a fully managed service to develop human-in-the-loop
    workflows that include a clean user interface, role-based access control with
    AWS Identity and Access Management (IAM), and scalable data storage with S3\.
    Amazon A2I is integrated with many Amazon services including Amazon Rekognition
    for content moderation and Amazon Textract for form-data extraction. We can also
    use Amazon A2I with Amazon SageMaker and any of our custom ML models. We will
    dive deeper into human-in-the-loop workflows in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊增强型 AI（Amazon A2I）是一个完全托管的服务，用于开发包括清晰用户界面、基于角色的访问控制（使用 AWS Identity and Access
    Management（IAM））、可扩展数据存储（使用 S3）在内的人在回路工作流程。亚马逊 A2I 与许多亚马逊服务集成，包括用于内容审核的亚马逊 Rekognition
    和用于表单数据提取的亚马逊 Textract。我们还可以将亚马逊 A2I 与亚马逊 SageMaker 和我们的任何自定义 ML 模型一起使用。我们将在[第
    10 章](ch10.html#pipelines_and_mlops)深入探讨人在回路工作流程。
- en: MLOps Best Practices
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps 最佳实践
- en: The field of machine learning operations (MLOps) has emerged over the past decade
    to describe the unique challenges of operating “software plus data” systems like
    AI and machine learning. With MLOps, we are developing the end-to-end architecture
    for automated model training, model hosting, and pipeline monitoring. Using a
    complete MLOps strategy from the beginning, we are building up expertise, reducing
    human error, de-risking our project, and freeing up time to focus on the hard
    data science challenges.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习运营（MLOps）领域在过去十年中出现，用于描述操作“软件加数据”系统（如 AI 和机器学习）的独特挑战。通过 MLOps，我们正在开发端到端架构，用于自动化模型训练、模型托管和管道监控。从一开始就使用完整的
    MLOps 策略，我们正在建立专业知识，减少人为错误，降低项目风险，并释放时间集中解决难点数据科学挑战。
- en: 'We’ve seen MLOps evolve through three different stages of maturity:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 MLOps 在过去十年中经历了三个不同成熟阶段：
- en: MLOps v1.0
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v1.0
- en: Manually build, train, tune, and deploy models
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建、训练、调优和部署模型
- en: MLOps v2.0
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v2.0
- en: Manually build and orchestrate model pipelines
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建和编排模型管道
- en: MLOps v3.0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v3.0
- en: Automatically run pipelines when new data arrives or code changes from deterministic
    triggers such as GitOps or when models start to degrade in performance based on
    statistical triggers such as drift, bias, and explainability divergence
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当新数据到达或代码更改时，自动运行管道，基于确定性触发器如 GitOps 或基于统计触发器如漂移、偏差和可解释性差异，当模型开始性能下降时
- en: AWS and Amazon SageMaker Pipelines support the complete MLOps strategy, including
    automated pipeline retraining with both deterministic GitOps triggers as well
    as statistical triggers such as data drift, model bias, and explainability divergence.
    We will dive deep into statistical drift, bias, and explainability in Chapters
    [5](ch05.html#explore_the_dataset), [6](ch06.html#prepare_the_dataset_for_model_training),
    [7](ch07.html#train_your_first_model), and [9](ch09.html#deploy_models_to_production).
    And we implement continuous and automated pipelines in [Chapter 10](ch10.html#pipelines_and_mlops)
    with various pipeline orchestration and automation options, including SageMaker
    Pipelines, AWS Step Functions, Apache Airflow, Kubeflow, and other options including
    human-in-the-loop workflows. For now, let’s review some best practices for operational
    excellence, security, reliability, performance efficiency, and cost optimization
    of MLOps.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS和Amazon SageMaker Pipelines支持完整的MLOps策略，包括使用确定性的GitOps触发器以及数据漂移、模型偏差和可解释性差异等统计触发器进行自动化管道重训练。我们将在第[5](ch05.html#explore_the_dataset)、[6](ch06.html#prepare_the_dataset_for_model_training)、[7](ch07.html#train_your_first_model)和[9](ch09.html#deploy_models_to_production)章节深入探讨统计漂移、偏差和可解释性。在[第10章](ch10.html#pipelines_and_mlops)中，我们将使用各种管道编排和自动化选项，包括SageMaker
    Pipelines、AWS Step Functions、Apache Airflow、Kubeflow等选项，以及包括人在回路工作流在内的选项，来实施持续和自动化流水线。现在，让我们回顾一些操作卓越、安全性、可靠性、性能效率和MLOps成本优化的最佳实践。
- en: Operational Excellence
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运营卓越。
- en: 'Here are a few machine-learning-specific best practices that help us build
    successful data science projects in the cloud:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些云中机器学习特定的最佳实践，帮助我们在云中构建成功的数据科学项目：
- en: Data-quality checks
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量检查。
- en: Since all our ML projects start with data, make sure to have access to high-quality
    datasets and implement repeatable data-quality checks. Poor data quality leads
    to many failed projects. Stay ahead of these issues early in the pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有我们的机器学习项目都始于数据，请确保能够访问高质量的数据集，并实施可重复的数据质量检查。数据质量不佳导致许多项目失败。请在管道的早期阶段解决这些问题。
- en: Start simple and reuse existing solutions
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从简单开始，并重用现有解决方案。
- en: Start with the simplest solution as there is no need to reinvent the wheel if
    we don’t need to. There is likely an AI service available to solve our task. Leverage
    managed services such as Amazon SageMaker that come with a lot of built-in algorithms
    and pre-trained models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有必要，就从最简单的解决方案开始，没有必要重新发明轮子。很可能有人工智能服务可用来解决我们的任务。利用像Amazon SageMaker这样的托管服务，其中包含大量内置算法和预训练模型。
- en: Define model performance metrics
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型性能指标。
- en: Map the model performance metrics to business objectives, and continuously monitor
    these metrics. We should develop a strategy to trigger model invalidations and
    retrain models when performance degrades.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型性能指标映射到业务目标，并持续监控这些指标。我们应该制定策略，在性能下降时触发模型失效和重新训练模型。
- en: Track and version everything
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪和版本控制所有内容。
- en: Track model development through experiments and lineage tracking. We should
    also version our datasets, feature-transformation code, hyper-parameters, and
    trained models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验和谱系跟踪跟踪模型开发。我们还应该对数据集、特征转换代码、超参数和训练模型进行版本控制。
- en: Select appropriate hardware for both model training and model serving
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型训练和模型服务选择适当的硬件。
- en: In many cases, model training has different infrastructure requirements than
    does model-prediction serving. Select the appropriate resources for each phase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，模型训练与模型预测服务具有不同的基础设施要求。选择每个阶段的适当资源。
- en: Continuously monitor deployed models
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 持续监控已部署的模型。
- en: Detect data drift and model drift—and take appropriate action such as model
    retraining.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 检测数据漂移和模型漂移，并采取适当的措施，如模型重训练。
- en: Automate machine learning workflows
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习工作流程。
- en: Build consistent, automated pipelines to reduce human error and free up time
    to focus on the hard problems. Pipelines can include human-approval steps for
    approving models before pushing them to production.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一致的自动化流水线，以减少人为错误，并释放时间专注于难题。流水线可以包括人工批准步骤，用于在推送到生产之前批准模型。
- en: Security
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性。
- en: Security and compliance is a shared responsibility between AWS and the customer.
    AWS ensures the security “of” the cloud, while the customer is responsible for
    security “in” the cloud.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和合规性是AWS和客户之间共同的责任。AWS确保“云中”的安全性，而客户负责“云中”的安全性。
- en: The most common security considerations for building secure data science projects
    in the cloud touch the areas of access management, compute and network isolation,
    encryption, governance, and auditability.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中构建安全数据科学项目时，最常见的安全考虑涉及访问管理、计算和网络隔离、加密、治理和可审计性。
- en: We need deep security and access control capabilities around our data. We should
    restrict access to data-labeling jobs, data-processing scripts, models, inference
    endpoints, and batch prediction jobs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要围绕我们的数据实施深度安全和访问控制功能。我们应限制对数据标记作业、数据处理脚本、模型、推断端点和批处理预测作业的访问。
- en: We should also implement a data governance strategy that ensures the integrity,
    security, and availability of our datasets. Implement and enforce data lineage,
    which monitors and tracks the data transformations applied to our training data.
    Ensure data is encrypted at rest and in motion. Also, we should enforce regulatory
    compliance where needed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应实施数据治理策略，确保数据集的完整性、安全性和可用性。实施并强制执行数据血统，监控和跟踪应用于我们的训练数据的数据转换。确保数据在静态和运动状态下都进行加密。此外，我们应根据需要强制执行法规遵从性。
- en: We will discuss best practices to build secure data science and machine learning
    applications on AWS in more detail in [Chapter 12](ch12.html#secure_data_science_on_aws).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [第12章](ch12.html#secure_data_science_on_aws) 更详细地讨论在AWS上构建安全数据科学和机器学习应用的最佳实践。
- en: Reliability
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: Reliability refers to the ability of a system to recover from infrastructure
    or service disruptions, acquire computing resources dynamically to meet demand,
    and mitigate disruptions such as misconfigurations or transient network issues.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性指系统从基础设施或服务中断中恢复的能力，动态获取计算资源以满足需求，并减轻配置错误或瞬时网络问题等中断。
- en: We should automate change tracking and versioning for our training data. This
    way, we can re-create the exact version of a model in the event of a failure.
    We will build once and use the model artifacts to deploy the model across multiple
    AWS accounts and environments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该自动化变更跟踪和版本控制我们的训练数据。这样，我们可以在发生故障时重新创建模型的确切版本。我们将构建一次，并使用模型工件在多个AWS账户和环境中部署模型。
- en: Performance Efficiency
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能效率
- en: '*Performance efficiency* refers to the efficient use of computing resources
    to meet requirements and how to maintain that efficiency as demand changes and
    technologies evolve.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能效率* 指的是有效利用计算资源以满足要求，并在需求变化和技术演进时如何保持效率。'
- en: We should choose the right compute for our machine learning workload. For example,
    we can leverage GPU-based instances to more efficiently train deep learning models
    using a larger queue depth, higher arithmetic logic units, and increased register
    counts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该为我们的机器学习工作负载选择合适的计算资源。例如，我们可以利用基于GPU的实例更有效地训练深度学习模型，使用更大的队列深度、更高的算术逻辑单元和增加的寄存器计数。
- en: Know the latency and network bandwidth performance requirements of models, and
    deploy each model closer to customers, if needed. There are situations where we
    might want to deploy our models “at the edge” to improve performance or comply
    with data-privacy regulations. “Deploying at the edge” refers to running the model
    on the device itself to run the predictions locally. We also want to continuously
    monitor key performance metrics of our model to spot performance deviations early.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉模型的延迟和网络带宽性能要求，并根据需要将每个模型部署更接近客户。在某些情况下，我们可能希望在“边缘”部署我们的模型，以提高性能或遵守数据隐私法规。“边缘部署”指的是在设备本身上运行模型以在本地进行预测。我们还希望持续监控模型的关键性能指标，及早发现性能偏差。
- en: Cost Optimization
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本优化
- en: We can optimize cost by leveraging different Amazon EC2 instance pricing options.
    For example, Savings Plans offer significant savings over on-demand instance prices,
    in exchange for a commitment to use a specific amount of compute power for a given
    amount of time. Savings Plans are a great choice for known/steady state workloads
    such as stable inference workloads.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用不同的Amazon EC2实例定价选项来优化成本。例如，储蓄计划相比按需实例价格提供了显著的节省，换取对特定时间段内使用特定数量计算能力的承诺。储蓄计划非常适合稳定的推断工作负载等已知/稳定状态工作负载。
- en: With on-demand instances, we pay for compute capacity by the hour or the second
    depending on which instances we run. On-demand instances are best for new or stateful
    spiky workloads such as short-term model training jobs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用按需实例，我们根据所运行的实例的小时或秒数付费计算能力。按需实例适合新的或状态性强的工作负载，如短期模型训练作业。
- en: Finally, Amazon EC2 Spot Instances allow us to request spare Amazon EC2 compute
    capacity for up to 90% off the on-demand price. Spot Instances can cover flexible,
    fault-tolerant workloads such as model training jobs that are not time-sensitive.
    [Figure 1-2](#optimize_cost_by_choosing_a_mix_of_savi) shows the resulting mix
    of Savings Plans, on-demand instances, and Spot Instances.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，亚马逊EC2 Spot实例允许我们请求多余的亚马逊EC2计算能力，价格可以低至按需价格的90%。Spot实例可以覆盖灵活、容错的工作负载，例如不受时间限制的模型训练作业。[图1-2](#optimize_cost_by_choosing_a_mix_of_savi)显示了节省计划、按需实例和Spot实例的混合结果。
- en: '![](assets/dsaw_0102.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0102.png)'
- en: Figure 1-2\. Optimize cost by choosing a mix of Savings Plans, on-demand instances,
    and Spot Instances.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 通过选择节省计划、按需实例和Spot实例的混合优化成本。
- en: With many of the managed services, we can benefit from the “only pay for what
    you use” model. For example, with Amazon SageMaker, we only pay for the time our
    model trains, or we run our automatic model tuning. Start developing models with
    smaller datasets to iterate more quickly and frugally. Once we have a well-performing
    model, we can scale up to train with the full dataset. Another important aspect
    is to right-size the model training and model hosting instances.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过许多托管服务，我们可以从“按使用量付费”的模式中受益。例如，使用Amazon SageMaker，我们只支付我们模型训练或运行自动模型调优的时间。开始使用较小的数据集开发模型，以便更快、更节省地迭代。一旦我们有一个表现良好的模型，我们可以扩展到使用完整数据集进行训练。另一个重要方面是适当调整模型训练和模型托管实例的大小。
- en: Many times, model training benefits from GPU acceleration, but model inference
    might not need the same acceleration. In fact, most machine learning workloads
    are actually predictions. While the model may take several hours or days to train,
    the deployed model likely runs 24 hours a day, 7 days a week across thousands
    of prediction servers supporting millions of customers. We should decide whether
    our use case requires a 24 × 7 real-time endpoint or a batch transformation on
    Spot Instances in the evenings.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，模型训练受益于GPU加速，但模型推断可能不需要同样的加速。事实上，大多数机器学习工作负载实际上是预测。虽然模型可能需要几小时或几天来训练，但部署的模型可能每天24小时、每周7天运行，支持数百万客户的数千个预测服务器。我们应该决定我们的用例是否需要24
    × 7实时端点或在晚间使用Spot实例进行批量转换。
- en: Amazon AI Services and AutoML with Amazon SageMaker
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊AI服务和使用亚马逊SageMaker的自动机器学习
- en: We know that data science projects involve many complex, multidisciplinary,
    and iterative steps. We need access to a machine learning development environment
    that supports the model prototyping stage and equally provides a smooth transition
    to prepare our model for production. We will likely want to experiment with various
    machine learning frameworks and algorithms and develop custom model training and
    inference code.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，数据科学项目涉及许多复杂的、跨学科的和迭代的步骤。我们需要访问一个支持模型原型阶段的机器学习开发环境，并提供顺畅过渡以准备我们的模型投入生产的环境。我们可能希望尝试各种机器学习框架和算法，并开发定制的模型训练和推断代码。
- en: Other times, we might want to just use a readily available, pre-trained model
    to solve a simple task. Or we might want to leverage AutoML techniques to create
    a first baseline for our project. AWS provides a broad set of services and features
    for each scenario. [Figure 1-3](#amazon_ai_and_machine_learning_stack_in) shows
    the entire Amazon AI and machine learning stack, including AI services and Amazon
    SageMaker Autopilot for AutoML.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其他时候，我们可能只想使用现成的、预训练的模型来解决简单的任务。或者我们可能希望利用AutoML技术为我们的项目创建第一个基线。AWS为每种情景提供了广泛的服务和功能。[图1-3](#amazon_ai_and_machine_learning_stack_in)显示了整个亚马逊AI和机器学习堆栈，包括AI服务和Amazon
    SageMaker Autopilot用于AutoML。
- en: '![](assets/dsaw_0103.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0103.png)'
- en: Figure 1-3\. The Amazon AI and machine learning stack.
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 亚马逊AI和机器学习堆栈。
- en: Amazon AI Services
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊AI服务
- en: For many common use cases, such as personalized product recommendations, content
    moderation, or demand forecasting, we can also use Amazon’s managed AI services
    with the option to fine-tune on our custom datasets. We can integrate these “1-click”
    AI services into our applications via simple API calls without much (sometimes
    no) machine learning experience needed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多常见的使用场景，例如个性化产品推荐、内容审核或需求预测，我们还可以使用亚马逊的托管AI服务，并选择在我们的定制数据集上进行微调。我们可以通过简单的API调用将这些“一键式”AI服务集成到我们的应用程序中，而无需太多（有时甚至不需要）机器学习经验。
- en: The fully managed AWS AI services are the fastest and easiest way to add intelligence
    to our applications using simple API calls. The AI services offer pre-trained
    or automatically trained machine learning models for image and video analysis,
    advanced text and document analysis, personalized recommendations, or demand forecasting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完全托管的 AWS AI 服务是通过简单的 API 调用为我们的应用程序增加智能的最快最简单的方式。这些 AI 服务提供预训练或自动训练的机器学习模型，用于图像和视频分析、高级文本和文档分析、个性化推荐或需求预测。
- en: AI services include Amazon Comprehend for natural language processing, Amazon
    Rekognition for computer vision, Amazon Personalize for generating product recommendations,
    Amazon Forecast for demand forecasting, and Amazon CodeGuru for automated source
    code reviews.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AI 服务包括 Amazon Comprehend 用于自然语言处理，Amazon Rekognition 用于计算机视觉，Amazon Personalize
    用于生成产品推荐，Amazon Forecast 用于需求预测，以及 Amazon CodeGuru 用于自动化源代码审查。
- en: AutoML with SageMaker Autopilot
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Autopilot 的 AutoML
- en: In another scenario, we might want to automate the repetitive steps of data
    analysis, data preparation, and model training for simple and well-known machine
    learning problems. This helps us focus our time on more complex use cases. AWS
    offers AutoML as part of the Amazon SageMaker service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种情况下，我们可能希望自动化数据分析、数据准备和简单且知名的机器学习问题的模型训练的重复步骤。这帮助我们将时间集中在更复杂的用例上。AWS 在 Amazon
    SageMaker 服务中提供 AutoML。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: AutoML is not limited to SageMaker. Many of the Amazon AI services perform AutoML
    to find the best model and hyper-parameters for the given dataset.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 并不局限于 SageMaker。许多 Amazon AI 服务执行 AutoML，以找到给定数据集的最佳模型和超参数。
- en: “AutoML” commonly refers to the effort of automating the typical steps of a
    model development workflow that we described earlier. Amazon SageMaker Autopilot
    is a fully managed service that applies AutoML techniques to our datasets.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: “AutoML” 通常指自动化模型开发工作流程的典型步骤，正如我们之前描述的那样。Amazon SageMaker Autopilot 是一种完全托管的服务，将
    AutoML 技术应用于我们的数据集。
- en: SageMaker Autopilot first analyzes our tabular data, identifies the machine
    learning problem type (i.e., regression, classification) and chooses algorithms
    (i.e., XGBoost) to solve the problem. It also creates the required data transformation
    code to preprocess the data for model training. Autopilot then creates a number
    of diverse machine learning model candidate pipelines representing variations
    of data transformations and chosen algorithms. It applies the data transformations
    in a feature engineering step, then trains and tunes each of those model candidates.
    The result is a ranked list (leaderboard) of the model candidates based on a defined
    objective metric such as the validation accuracy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Autopilot 首先分析我们的表格数据，识别机器学习问题类型（如回归、分类），选择算法（如 XGBoost）来解决问题。它还创建必要的数据转换代码，以预处理模型训练数据。Autopilot
    随后创建多种不同的机器学习模型候选管道，表示数据转换和选择算法的变化。它在特征工程步骤中应用数据转换，然后训练和调整每个模型候选。其结果是基于定义的客观度量标准（如验证准确率）的模型候选排名列表（排行榜）。
- en: SageMaker Autopilot is an example of transparent AutoML. Autopilot not only
    shares the data transformation code with us, but it also generates additional
    Jupyter notebooks that document the results of the data analysis step and the
    model candidate pipelines to reproduce the model training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Autopilot 是透明的 AutoML 的一个例子。Autopilot 不仅与我们分享数据转换的代码，还生成额外的 Jupyter
    笔记本，记录数据分析步骤的结果以及模型候选管道，以重现模型训练。
- en: We can leverage SageMaker Autopilot in many scenarios. We can empower more people
    in our organization to build models, i.e., software developers who might have
    limited machine learning experience. We can automate model creation for simple-to-solve
    machine learning problems and focus our time on the new, complex use cases. We
    can automate the first steps of data analysis and data preparation and then use
    the result as a baseline to apply our domain knowledge and experience to tweak
    and further improve the models as needed. The Autopilot-generated model metrics
    also give us a good baseline for the model quality achievable with the provided
    dataset. We will dive deep into SageMaker Autopilot in [Chapter 3](ch03.html#automated_machine_learnin).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多场景下，我们可以利用SageMaker Autopilot。我们可以让更多在我们组织中的人构建模型，例如，具有有限机器学习经验的软件开发人员。我们可以自动化解决简单机器学习问题的模型创建，并将我们的时间集中在新的复杂用例上。我们可以自动化数据分析和数据准备的第一步，然后使用结果作为基线，应用我们的领域知识和经验来调整和进一步改进模型。Autopilot生成的模型指标也为我们提供了可通过提供的数据集实现的模型质量的良好基线。我们将在[第3章](ch03.html#automated_machine_learnin)深入探讨SageMaker
    Autopilot。
- en: Data Ingestion, Exploration, and Preparation in AWS
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS中的数据摄入、探索和准备
- en: We will cover data ingestion, exploration, and preparation in Chapters [4](ch04.html#ingest_data_into_the_cloud),
    [5](ch05.html#explore_the_dataset), and [6](ch06.html#prepare_the_dataset_for_model_training),
    respectively. But, for now, let’s discuss this portion of the model-development
    workflow to learn which AWS services and open source tools we can leverage at
    each step.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第4章](ch04.html#ingest_data_into_the_cloud)，[第5章](ch05.html#explore_the_dataset)和[第6章](ch06.html#prepare_the_dataset_for_model_training)分别讨论数据摄入、探索和准备。但现在，让我们讨论模型开发工作流程的这一部分，以了解我们可以在每个步骤中利用哪些AWS服务和开源工具。
- en: Data Ingestion and Data Lakes with Amazon S3 and AWS Lake Formation
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon S3和AWS Lake Formation的数据摄入和数据湖
- en: Everything starts with data. And if we have seen one consistent trend in recent
    decades, it’s the continued explosion of data. Data is growing exponentially and
    is increasingly diverse. Today business success is often closely related to a
    company’s ability to quickly extract value from their data. There are now more
    and more people, teams, and applications that need to access and analyze the data.
    This is why many companies are moving to a highly scalable, available, secure,
    and flexible data store, often called a *data lake*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都始于数据。如果我们在过去几十年中看到了一个一致的趋势，那就是数据的持续爆炸增长。数据呈指数增长，且日益多样化。今天，业务成功往往与公司快速从其数据中提取价值的能力密切相关。现在有越来越多的人员、团队和应用程序需要访问和分析数据。这就是为什么许多公司正在转向一个高度可扩展、可用、安全和灵活的数据存储，通常被称为*数据湖*。
- en: A data lake is a centralized and secure repository that enables us to store,
    govern, discover, and share data at any scale. With a data lake, we can run any
    kind of analytics efficiently and use multiple AWS services without having to
    transform or move our data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个集中和安全的存储库，使我们能够以任何规模存储、管理、发现和共享数据。有了数据湖，我们可以高效地运行任何类型的分析，并且可以使用多个AWS服务，而无需转换或移动数据。
- en: Data lakes may contain structured relational data as well as semi-structured
    and unstructured data. We can even ingest real-time data. Data lakes give data
    science and machine learning teams access to large and diverse datasets to train
    and deploy more accurate models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖可以包含结构化关系数据以及半结构化和非结构化数据。我们甚至可以摄入实时数据。数据湖为数据科学和机器学习团队提供了访问大型和多样化数据集的能力，以训练和部署更准确的模型。
- en: Amazon Simple Storage Service (Amazon S3) is object storage built to store and
    retrieve any amount of data from anywhere, in any format. We can organize our
    data with fine-tuned access controls to meet our business and compliance requirements.
    We will discuss security in depth in [Chapter 12](ch12.html#secure_data_science_on_aws).
    Amazon S3 is designed for 99.999999999% (11 nines) of durability as well as for
    strong read-after-write consistency. S3 is a popular choice for data lakes in
    AWS.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon简单存储服务（Amazon S3）是一种对象存储，专为从任何地方、以任何格式存储和检索任意量的数据而构建。我们可以通过精细调整的访问控制组织我们的数据，以满足业务和合规要求。我们将在[第12章](ch12.html#secure_data_science_on_aws)深入讨论安全性。Amazon
    S3设计的耐久性为99.999999999%（11个九），并且具有强大的写后一致性。在AWS中，S3是数据湖的热门选择。
- en: We can leverage the AWS Lake Formation service to create our data lake. The
    service helps collect and catalog data from both databases and object storage.
    Lake Formation not only moves our data but also cleans, classifies, and secures
    access to our sensitive data using machine learning algorithms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用AWS Lake Formation服务来创建我们的数据湖。该服务帮助收集和目录化来自数据库和对象存储的数据。Lake Formation不仅移动我们的数据，还使用机器学习算法清洗、分类和安全地访问我们的敏感数据。
- en: We can leverage AWS Glue to automatically discover and profile new data. AWS
    Glue is a scalable and serverless data catalog and data preparation service. The
    service consists of an ETL engine, an Apache Hive–compatible data catalog service,
    and a data transformation and analysis service. We can build data crawlers to
    periodically detect and catalog new data. AWS Glue DataBrew is a service with
    an easy-to-use UI that simplifies data ingestion, analysis, visualization, and
    transformation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用AWS Glue自动发现和分析新数据。AWS Glue是一个可伸缩的无服务器数据目录和数据准备服务。该服务包括ETL引擎、与Apache Hive兼容的数据目录服务，以及数据转换和分析服务。我们可以构建数据爬虫定期检测和目录化新数据。AWS
    Glue DataBrew是一个具有易于使用UI的服务，简化了数据摄入、分析、可视化和转换。
- en: Data Analysis with Amazon Athena, Amazon Redshift, and Amazon QuickSight
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon Athena、Amazon Redshift和Amazon QuickSight进行数据分析
- en: Before we start developing any machine learning model, we need to understand
    the data. In the data analysis step, we explore our data, collect statistics,
    check for missing values, calculate quantiles, and identify data correlations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始开发任何机器学习模型之前，我们需要理解数据。在数据分析步骤中，我们探索我们的数据，收集统计信息，检查缺失值，计算分位数，并识别数据相关性。
- en: Sometimes we just want to quickly analyze the available data from our development
    environment and prototype some first model code. Maybe we just quickly want to
    try out a new algorithm. We call this “ad hoc” exploration and prototyping, where
    we query parts of our data to get a first understanding of the data schema and
    data quality for our specific machine learning problem at hand. We then develop
    model code and ensure it is functionally correct. This ad hoc exploration and
    prototyping can be done from development environments such as SageMaker Studio,
    AWS Glue DataBrew, and SageMaker Data Wrangler.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们只是想快速分析开发环境中可用的数据，并原型化一些首个模型代码。也许我们只是想快速尝试一个新的算法。我们称这种情况为“特别探索”和原型化，我们查询部分数据以首先理解特定机器学习问题中的数据架构和数据质量。然后我们开发模型代码并确保其功能正确。这种特别探索和原型化可以从开发环境如SageMaker
    Studio、AWS Glue DataBrew和SageMaker Data Wrangler进行。
- en: Amazon SageMaker offers us a hosted managed Jupyter environment and an integrated
    development environment with SageMaker Studio. We can start analyzing data sets
    directly in our notebook environment with tools such as [pandas](https://pandas.pydata.org),
    a popular Python open source data analysis and manipulation tool. Note that pandas
    uses in-memory data structures (DataFrames) to hold and manipulate data. As many
    development environments have constrained memory resources, we need to be careful
    how much data we pull into the pandas DataFrames.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker为我们提供了托管的Jupyter环境以及与SageMaker Studio集成的集成开发环境。我们可以使用诸如[pandas](https://pandas.pydata.org)之类的工具直接在笔记本环境中开始分析数据集。pandas是一个流行的Python开源数据分析和操作工具。请注意，pandas使用内存数据结构（DataFrames）来保存和操作数据。由于许多开发环境具有受限的内存资源，我们需要小心地拉取多少数据到pandas
    DataFrames中。
- en: For data visualizations in our notebook, we can leverage popular open source
    libraries such as [Matplotlib](https://matplotlib.org) and [Seaborn](https://seaborn.pydata.org).
    Matplotlib lets us create static, animated, and interactive visualizations in
    Python. Seaborn builds on top of Matplotlib and adds support for additional statistical
    graphics—as well as an easier-to-use programming model. Both data visualization
    libraries integrate closely with pandas data structures.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的笔记本中进行数据可视化，我们可以利用诸如[Matplotlib](https://matplotlib.org)和[Seaborn](https://seaborn.pydata.org)等流行的开源库。Matplotlib允许我们在Python中创建静态、动画和交互式可视化。Seaborn建立在Matplotlib之上，增加了对额外统计图形的支持，以及更易于使用的编程模型。这两个数据可视化库与pandas数据结构密切集成。
- en: The open source [AWS Data Wrangler library](https://oreil.ly/Q7gNs) extends
    the power of pandas to AWS. AWS Data Wrangler connects pandas DataFrames with
    AWS services such as Amazon S3, AWS Glue, Amazon Athena, and Amazon Redshift.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 开源的[AWS Data Wrangler库](https://oreil.ly/Q7gNs)扩展了pandas在AWS上的功能。AWS Data Wrangler连接pandas
    DataFrames与AWS服务，如Amazon S3、AWS Glue、Amazon Athena和Amazon Redshift。
- en: 'AWS Data Wrangler provides optimized Python functions to perform common ETL
    tasks to load and unload data between data lakes, data warehouses, and databases.
    After installing AWS Data Wrangler with `pip install awswrangler` and importing
    AWS Data Wrangler, we can read our dataset directly from S3 into a pandas DataFrame
    as shown here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Wrangler 提供了优化的 Python 函数，执行在数据湖、数据仓库和数据库之间加载和卸载数据的常见 ETL 任务。安装 AWS
    Data Wrangler 后，使用 `pip install awswrangler` 导入 AWS Data Wrangler，我们可以直接从 S3 中将数据集读取到
    pandas DataFrame 中，如下所示：
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: AWS Data Wrangler also comes with additional memory optimizations, such as reading
    data in chunks. This is particularly helpful if we need to query large datasets.
    With chunking enabled, AWS Data Wrangler reads and returns every dataset file
    in the path as a separate pandas DataFrame. We can also set the chunk size to
    return the number of rows in a DataFrame equivalent to the numerical value we
    defined as chunk size. For a full list of capabilities, check [the documentation](https://oreil.ly/4sGjc).
    We will dive deeper into AWS Data Wrangler in [Chapter 5](ch05.html#explore_the_dataset).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Wrangler 还带有额外的内存优化，例如按块读取数据。如果我们需要查询大型数据集，则这尤为有帮助。启用块读取后，AWS Data
    Wrangler 会将路径中的每个数据集文件作为单独的 pandas DataFrame 读取和返回。我们还可以设置块大小，以返回与我们定义的块大小相当的
    DataFrame 行数。要查看完整的功能列表，请参阅[文档](https://oreil.ly/4sGjc)。我们将在[第五章](ch05.html#explore_the_dataset)深入探讨
    AWS Data Wrangler。
- en: We can leverage managed services such as Amazon Athena to run interactive SQL
    queries on the data in S3 from within our notebook. Amazon Athena is a managed,
    serverless, dynamically scalable distributed SQL query engine designed for fast
    parallel queries on extremely large datasets. Athena is based on Presto, the popular
    open source query engine, and requires no maintenance. With Athena, we only pay
    for the queries we run. And we can query data in its raw form directly in our
    S3 data lake without additional transformations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用托管服务（例如 Amazon Athena）在我们的笔记本内部对 S3 中的数据运行交互式 SQL 查询。Amazon Athena 是一个托管的、无服务器的、动态可扩展的分布式
    SQL 查询引擎，专为对极其大型数据集进行快速并行查询而设计。Athena 基于流行的开源查询引擎 Presto，并且无需维护。使用 Athena，我们只需为运行的查询付费。并且我们可以直接在我们的
    S3 数据湖中以原始形式查询数据，无需进行额外的转换。
- en: Amazon Athena also leverages the AWS Glue Data Catalog service to store and
    retrieve the schema metadata needed for our SQL queries. When we define our Athena
    database and tables, we point to the data location in S3\. Athena then stores
    this table-to-S3 mapping in the AWS Glue Data Catalog. We can use PyAthena, a
    popular open source library, to query Athena from our Python-based notebooks and
    scripts. We will dive deeper into Athena, AWS Glue Data Catalog, and PyAthena
    in Chapters [4](ch04.html#ingest_data_into_the_cloud) and [5](ch05.html#explore_the_dataset).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Athena 还利用 AWS Glue 数据目录服务存储和检索我们 SQL 查询所需的模式元数据。当我们定义 Athena 数据库和表时，我们指向
    S3 中的数据位置。Athena 然后将这种表到 S3 的映射存储在 AWS Glue 数据目录中。我们可以使用 PyAthena，一个流行的开源库，从基于
    Python 的笔记本和脚本查询 Athena。我们将在[第四章](ch04.html#ingest_data_into_the_cloud)和[第五章](ch05.html#explore_the_dataset)深入探讨
    Athena、AWS Glue 数据目录和 PyAthena。
- en: Amazon Redshift is a fully managed cloud data warehouse service that allows
    us to run complex analytic queries against petabytes of structured data. Our queries
    are distributed and parallelized across multiple nodes. In contrast to relational
    databases that are optimized to store data in rows and mostly serve transactional
    applications, Amazon Redshift implements columnar data storage, which is optimized
    for analytical applications where we are mostly interested in the summary statistics
    on those columns.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 是一个完全托管的云数据仓库服务，允许我们针对 PB 级结构化数据运行复杂的分析查询。我们的查询在多个节点上分布和并行化执行。与优化用于行存储数据并主要服务事务应用的关系数据库相反，Amazon
    Redshift 实施的是列存储，这对于我们主要关注这些列的汇总统计信息的分析应用来说是优化的。
- en: Amazon Redshift also includes Amazon Redshift Spectrum, which allows us to directly
    execute SQL queries from Amazon Redshift against exabytes of unstructured data
    in our Amazon S3 data lake without the need to physically move the data. Amazon
    Redshift Spectrum automatically scales the compute resources needed based on how
    much data is being received, so queries against Amazon S3 run fast, regardless
    of the size of our data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 还包括 Amazon Redshift Spectrum，允许我们直接从 Amazon Redshift 对接 PB 级非结构化数据的
    Amazon S3 数据湖执行 SQL 查询，无需实际移动数据。Amazon Redshift Spectrum 根据接收到的数据量自动扩展所需的计算资源，因此对
    Amazon S3 的查询运行速度快，无论我们的数据大小如何。
- en: If we need to create dashboard-style visualizations of our data, we can leverage
    Amazon QuickSight. QuickSight is an easy-to-use, serverless business analytics
    service to quickly build powerful visualizations. We can create interactive dashboards
    and reports and securely share them with our coworkers via browsers or mobile
    devices. QuickSight already comes with an extensive library of visualizations,
    charts, and tables.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要创建仪表板样式的数据可视化，可以利用 Amazon QuickSight。QuickSight 是一个易于使用的无服务器业务分析服务，可以快速构建强大的可视化。我们可以创建交互式仪表板和报告，并通过浏览器或移动设备安全共享给同事。QuickSight
    已经配备了广泛的可视化、图表和表格库。
- en: QuickSight implements machine learning and natural language capabilities to
    help us gain deeper insights from our data. Using ML Insights, we can discover
    hidden trends and outliers in our data. The feature also enables anyone to run
    what-if analysis and forecasting, without any machine learning experience needed.
    We can also build predictive dashboards by connecting QuickSight to our machine
    learning models built in Amazon SageMaker.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: QuickSight 实现了机器学习和自然语言处理能力，帮助我们从数据中获得更深入的洞见。使用 ML Insights，我们可以发现数据中的隐藏趋势和异常值。该功能还使任何人都能够进行假设分析和预测，无需任何机器学习经验。我们还可以通过将
    QuickSight 连接到在 Amazon SageMaker 中构建的机器学习模型来构建预测性仪表板。
- en: Evaluate Data Quality with AWS Deequ and SageMaker Processing Jobs
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[使用 AWS Deequ 和 SageMaker 处理作业评估数据质量](https://wiki.example.org/aws_deequ_and_sagemaker_processing_jobs)'
- en: We need high-quality data to build high-quality models. Before we create our
    training dataset, we want to ensure our data meets certain quality constraints.
    In software development, we run unit tests to ensure our code meets design and
    quality standards and behaves as expected. Similarly, we can run unit tests on
    our dataset to ensure the data meets our quality expectations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高质量模型需要高质量数据。在创建训练数据集之前，我们希望确保数据符合某些质量约束。在软件开发中，我们运行单元测试以确保代码符合设计和质量标准，并且按预期运行。类似地，我们可以对数据集运行单元测试，以确保数据符合我们的质量预期。
- en: '[AWS Deequ](https://oreil.ly/a6cVE) is an open source library built on top
    of Apache Spark that lets us define unit tests for data and measure data quality
    in large datasets. Using Deequ unit tests, we can find anomalies and errors early,
    before the data gets used in model training. Deequ is designed to work with very
    large datasets (billions of rows). The open source library supports tabular data,
    i.e., CSV files, database tables, logs, or flattened JSON files. Anything we can
    fit in a Spark data frame, we can validate with Deequ.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS Deequ](https://oreil.ly/a6cVE) 是建立在 Apache Spark 之上的开源库，允许我们为数据定义单元测试并在大数据集中测量数据质量。使用
    Deequ 单元测试，我们可以在数据用于模型训练之前早期发现异常和错误。Deequ 设计用于处理非常大的数据集（数十亿行）。这个开源库支持表格数据，如 CSV
    文件、数据库表、日志或扁平化的 JSON 文件。任何可以放入 Spark 数据框架的东西，我们都可以用 Deequ 进行验证。'
- en: In a later example, we will leverage Deequ to implement data-quality checks
    on our sample dataset. We will leverage the SageMaker Processing Jobs support
    for Apache Spark to run our Deequ unit tests at scale. In this setup, we don’t
    need to provision any Apache Spark cluster ourselves, as SageMaker Processing
    handles the heavy lifting for us. We can think of this as “serverless” Apache
    Spark. Once we are in possession of high-quality data, we can now create our training
    dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续示例中，我们将利用 Deequ 对样本数据集实施数据质量检查。我们将利用 SageMaker Processing Jobs 支持 Apache
    Spark 以规模化运行我们的 Deequ 单元测试。在此设置中，我们无需自行配置任何 Apache Spark 集群，因为 SageMaker Processing
    为我们处理了繁重的工作。我们可以将其视为“无服务器”Apache Spark。一旦拥有高质量数据，我们现在可以创建我们的训练数据集。
- en: Label Training Data with SageMaker Ground Truth
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[使用 SageMaker Ground Truth 标记训练数据](https://wiki.example.org/sagemaker_ground_truth)'
- en: Many data science projects implement supervised learning. In supervised learning,
    our models learn by example. We first need to collect and evaluate, then provide
    accurate labels. If there are incorrect labels, our machine learning model will
    learn from bad examples. This will ultimately lead to inaccurate predictions.
    SageMaker Ground Truth helps us to efficiently and accurately label data stored
    in Amazon S3\. SageMaker Ground Truth uses a combination of automated and human
    data labeling.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学项目都实现了监督学习。在监督学习中，我们的模型通过示例进行学习。我们首先需要收集和评估数据，然后提供准确的标签。如果存在错误的标签，我们的机器学习模型将从错误的示例中学习，最终导致不准确的预测结果。SageMaker
    Ground Truth 帮助我们高效准确地标记存储在 Amazon S3 中的数据。SageMaker Ground Truth 使用自动化和人工数据标记的组合。
- en: SageMaker Ground Truth provides pre-built workflows and interfaces for common
    data labeling tasks. We define the labeling task and assign the labeling job to
    either a public workforce via Amazon Mechanical Turk or a private workforce, such
    as our coworkers. We can also leverage third-party data labeling service providers
    listed on the AWS Marketplace, which are prescreened by Amazon.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Ground Truth为常见的数据标记任务提供预构建的工作流和界面。我们定义标记任务，并将标记工作分配给公共工作力量（通过Amazon
    Mechanical Turk）或私人工作力量，例如我们的同事。我们还可以利用AWS Marketplace列出的经过亚马逊预筛选的第三方数据标记服务提供商。
- en: SageMaker Ground Truth implements active learning techniques for pre-built workflows.
    It creates a model to automatically label a subset of the data, based on the labels
    assigned by the human workforce. As the model continuously learns from the human
    workforce, the accuracy improves, and less data needs to be sent to the human
    workforce. Over time and with enough data, the SageMaker Ground Truth active-learning
    model is able to provide high-quality and automatic annotations that result in
    lower labeling costs overall. We will dive deeper into SageMaker Ground Truth
    in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Ground Truth实施预构建工作流的主动学习技术。它创建一个模型来自动标记数据子集，基于人工工作人员分配的标签。随着模型不断从人工工作人员中学习，准确性会提高，并且需要发送给人工工作人员的数据量会减少。随着时间的推移和足够的数据量，SageMaker
    Ground Truth的主动学习模型能够提供高质量和自动注释，从而降低总体标记成本。我们将在[第10章](ch10.html#pipelines_and_mlops)深入探讨SageMaker
    Ground Truth。
- en: Data Transformation with AWS Glue DataBrew, SageMaker Data Wrangler, and SageMaker
    Processing Jobs
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AWS Glue DataBrew、SageMaker Data Wrangler和SageMaker Processing Jobs进行数据转换
- en: Now let’s move on to data transformation. We assume we have our data in an S3
    data lake, or S3 bucket. We also gained a solid understanding of our dataset through
    the data analysis. The next step is now to prepare our data for model training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行数据转换。我们假设我们的数据存储在一个S3数据湖或S3存储桶中。通过数据分析，我们对数据集有了深入的理解。现在的下一步是准备我们的数据用于模型训练。
- en: Data transformations might include dropping or combining data in our dataset.
    We might need to convert text data into word embeddings for use with natural language
    models. Or perhaps we might need to convert data into another format, from numerical
    to text representation, or vice versa. There are numerous AWS services that could
    help us achieve this.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换可能包括在数据集中删除或合并数据。我们可能需要将文本数据转换为词嵌入以供自然语言模型使用。或者我们可能需要将数据转换为另一种格式，从数值到文本表示，反之亦然。有许多AWS服务可以帮助我们实现这一点。
- en: AWS Glue DataBrew is a visual data analysis and preparation tool. With 250 built-in
    transformations, DataBrew can detect anomalies, converting data between standard
    formats and fixing invalid or missing values. DataBrew can profile our data, calculate
    summary statistics, and visualize column correlations.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue DataBrew是一种可视化数据分析和准备工具。使用250种内置转换，DataBrew可以检测异常值，将数据转换为标准格式并修复无效或缺失值。DataBrew可以对我们的数据进行分析，计算摘要统计信息，并可视化列之间的相关性。
- en: We can also develop custom data transformations at scale with Amazon SageMaker
    Data Wrangler. SageMaker Data Wrangler offers low-code, UI-driven data transformations.
    We can read data from various sources, including Amazon S3, Athena, Amazon Redshift,
    and AWS Lake Formation. SageMaker Data Wrangler comes with pre-configured data
    transformations similar to AWS DataBrew to convert column types, perform one-hot
    encoding, and process text fields. SageMaker Data Wrangler supports custom user-defined
    functions using Apache Spark and even generates code including Python scripts
    and SageMaker Processing Jobs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Amazon SageMaker Data Wrangler开发大规模的自定义数据转换。SageMaker Data Wrangler提供低代码、UI驱动的数据转换。我们可以从各种来源读取数据，包括Amazon
    S3、Athena、Amazon Redshift和AWS Lake Formation。SageMaker Data Wrangler具有预配置的数据转换功能，类似于AWS
    DataBrew，用于转换列类型、执行独热编码和处理文本字段。SageMaker Data Wrangler支持使用Apache Spark进行自定义用户定义函数，并生成包括Python脚本和SageMaker处理作业的代码。
- en: SageMaker Processing Jobs let us run custom data processing code for data transformation,
    data validation, or model evaluation across data in S3\. When we configure the
    SageMaker Processing Job, we define the resources needed, including instance types
    and number of instances. SageMaker takes our custom code, copies our data from
    Amazon S3, and then pulls a Docker container to execute the processing step.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 处理作业允许我们对 S3 中的数据运行自定义数据处理代码，进行数据转换、数据验证或模型评估。配置 SageMaker 处理作业时，我们定义所需的资源，包括实例类型和实例数量。SageMaker
    接收我们的自定义代码，从 Amazon S3 复制数据，然后拉取 Docker 容器执行处理步骤。
- en: SageMaker offers pre-built container images to run data processing with Apache
    Spark and scikit-learn. We can also provide a custom container image if needed.
    SageMaker then spins up the cluster resources we specified for the duration of
    the job and terminates them when the job has finished. The processing results
    are written back to an Amazon S3 bucket when the job finishes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 提供预构建的容器镜像，用于使用 Apache Spark 和 scikit-learn 运行数据处理。如果需要，我们还可以提供自定义容器镜像。SageMaker
    然后根据作业的持续时间启动我们指定的集群资源，并在作业完成时终止它们。处理结果在作业完成时写回 Amazon S3 存储桶。
- en: Model Training and Tuning with Amazon SageMaker
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 进行模型训练和调优
- en: Let’s discuss the model training and tuning steps of our model development workflow
    in more detail and learn which AWS services and open source tools we can leverage.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论我们模型开发工作流程中的模型训练和调优步骤，并学习如何利用哪些 AWS 服务和开源工具。
- en: Train Models with SageMaker Training and Experiments
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker 训练和实验训练模型
- en: Amazon SageMaker Training Jobs provide a lot of functionality to support our
    model training. We can organize, track, and evaluate our individual model training
    runs with SageMaker Experiments. With SageMaker Debugger, we get transparency
    into our model training process. Debugger automatically captures real-time metrics
    during training and provides a visual interface to analyze the debug data. Debugger
    also profiles and monitors system resource utilization and identifies resource
    bottlenecks such as overutilized CPUs or GPUs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 训练作业提供了大量功能来支持我们的模型训练。我们可以使用 SageMaker 实验组织、跟踪和评估我们的单个模型训练运行。使用
    SageMaker 调试器，我们可以透明地查看我们的模型训练过程。调试器在训练期间自动捕获实时指标，并提供可视化界面分析调试数据。调试器还分析和监视系统资源利用率，并识别资源瓶颈，如过度利用的
    CPU 或 GPU。
- en: With SageMaker training, we simply specify the Amazon S3 location of our data,
    the algorithm container to execute our model training code, and define the type
    and number of SageMaker ML instances we need. SageMaker will take care of initializing
    the resources and run our model training. If instructed, SageMaker spins up a
    distributed compute cluster. Once the model training completes, SageMaker writes
    the results to S3 and terminates the ML instances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker 训练，我们只需指定我们数据的 Amazon S3 位置，执行我们模型训练代码的算法容器，以及定义我们需要的 SageMaker
    机器学习实例的类型和数量。SageMaker 将负责初始化资源并运行我们的模型训练。如果指定，SageMaker 将启动一个分布式计算集群。一旦模型训练完成，SageMaker
    将结果写入 S3 并终止机器学习实例。
- en: SageMaker also supports Managed Spot Training. Managed Spot Training leverages
    Amazon EC2 Spot Instances to perform model training. Using Spot Instances, we
    can reduce model training cost up to 90% compared to on-demand instances.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 还支持托管 Spot 实例训练。托管 Spot 实例训练利用 Amazon EC2 Spot 实例执行模型训练。使用 Spot 实例，我们可以将模型训练成本降低高达
    90%，相比按需实例。
- en: Besides SageMaker Autopilot, we can choose from any of the built-in algorithms
    that come with Amazon SageMaker or customize the model training by bringing our
    own model code (script mode) or our own algorithm/framework container.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 SageMaker 自动驾驶之外，我们可以从 Amazon SageMaker 提供的任何内置算法中选择，或通过带来自己的模型代码（脚本模式）或自己的算法/框架容器来定制模型训练。
- en: Built-in Algorithms
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置算法
- en: SageMaker comes with many built-in algorithms to help machine learning practitioners
    get started on training and deploying machine learning models quickly. Built-in
    algorithms require no extra code. We only need to provide the data and any model
    settings (hyper-parameters) and specify the compute resources. Most of the built-in
    algorithms also support distributed training out of the box to support large datasets
    that cannot fit on a single machine.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 配备了许多内置算法，帮助机器学习从业者快速开始训练和部署机器学习模型。内置算法无需额外的代码。我们只需提供数据和任何模型设置（超参数），并指定计算资源。大多数内置算法还支持开箱即用的分布式训练，以支持无法在单台机器上容纳的大型数据集。
- en: For supervised learning tasks, we can choose from regression and classification
    algorithms such as Linear Learner and XGBoost. Factorization Machines are well
    suited to recommender systems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习任务，我们可以选择回归和分类算法，例如线性学习器和XGBoost。分解机（Factorization Machines）非常适合推荐系统。
- en: For unsupervised learning tasks, such as clustering, dimension reduction, pattern
    recognition, and anomaly detection, there are additional built-in algorithms available.
    Such algorithms include Principal Component Analysis (PCA) and K-Means Clustering.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无监督学习任务，如聚类、降维、模式识别和异常检测，还提供了额外的内置算法。这些算法包括主成分分析（PCA）和K均值聚类。
- en: We can also leverage built-in algorithms for text analysis tasks such as text
    classification and topic modeling. Such algorithms include BlazingText and Neural
    Topic Model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以利用内置算法来处理文本分析任务，如文本分类和主题建模。这些算法包括BlazingText和神经主题模型。
- en: For image processing, we will find built-in algorithms for image classification
    and object detection, including Semantic Segmentation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像处理，我们将找到用于图像分类和物体检测的内置算法，包括语义分割。
- en: Bring Your Own Script (Script Mode)
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自带脚本（脚本模式）
- en: If we need more flexibility, or there is no built-in solution that works for
    our use case, we can provide our own model training code. This is often referred
    to as “script mode.” Script mode lets us focus on our training script, while SageMaker
    provides highly optimized Docker containers for each of the familiar open source
    frameworks, such as TensorFlow, PyTorch, Apache MXNet, XGBoost, and scikit-learn.
    We can add all our needed code dependencies via a requirements file, and SageMaker
    will take care of running our custom model training code with one of the built-in
    framework containers, depending on our framework of choice.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要更灵活性，或者没有适合我们用例的内置解决方案，我们可以提供自己的模型训练代码。这通常被称为“脚本模式”。脚本模式让我们专注于我们的训练脚本，而SageMaker为每个熟悉的开源框架（如TensorFlow、PyTorch、Apache
    MXNet、XGBoost和scikit-learn）提供高度优化的Docker容器。我们可以通过一个requirements文件添加所有需要的代码依赖项，SageMaker将根据我们选择的框架之一运行我们的自定义模型训练代码。
- en: Bring Your Own Container
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自带容器
- en: In case neither the built-in algorithms or script mode covers our use case,
    we can bring our own custom Docker image to host the model training. Docker is
    a software tool that provides build-time and runtime support for isolated environments
    called *Docker containers*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果既没有内置算法也不适用于脚本模式的情况下，我们可以使用自己的定制Docker镜像来托管模型训练。Docker是一个软件工具，提供了构建时和运行时支持称为“Docker容器”的隔离环境。
- en: SageMaker uses Docker images and containers to provide data processing, model
    training, and prediction serving capabilities.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker使用Docker镜像和容器提供数据处理、模型训练和预测服务的功能。
- en: We can use *bring your own container* (BYOC) if the package or software we need
    is not included in a supported framework. This approach gives us unlimited options
    and the most flexibility, as we can build our own Docker container and install
    anything we require. Typically, we see people use this option when they have custom
    security requirements, or want to preinstall libraries into the Docker container
    to avoid a third-party dependency (i.e., with PyPI, Maven, or Docker Registry).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要的软件包或软件不包含在支持的框架中，我们可以使用“自带容器”（BYOC）。这种方法为我们提供了无限的选择和最大的灵活性，因为我们可以构建自己的Docker容器并安装我们需要的任何内容。通常情况下，当用户有定制的安全要求，或者希望预先安装库到Docker容器中以避免第三方依赖时（如PyPI、Maven或Docker
    Registry），他们会选择这个选项。
- en: When using the BYOC option to use our own Docker image, we first need to upload
    the Docker image to a Docker registry like DockerHub or Amazon Elastic Container
    Registry (Amazon ECR). We should only choose the BYOC option if we are familiar
    with developing, maintaining, and supporting custom Docker images with an efficient
    Docker-image pipeline. Otherwise, we should use the built-in SageMaker containers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用BYOC选项来使用我们自己的Docker镜像时，我们首先需要将Docker镜像上传到像DockerHub或Amazon Elastic Container
    Registry（Amazon ECR）这样的Docker注册表中。如果我们熟悉开发、维护和支持具有高效Docker镜像管道的定制Docker镜像，我们应该选择BYOC选项。否则，我们应该使用内置的SageMaker容器。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We don’t need to “burn” our code into a Docker image at build time. We can simply
    point to our code in Amazon S3 from within the Docker image and load the code
    dynamically when a Docker container is started. This helps avoid unnecessary Docker
    image builds every time our code changes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要在构建时将我们的代码“烧入”到 Docker 镜像中。我们可以简单地从 Docker 镜像内部指向 Amazon S3 中的代码，并在启动 Docker
    容器时动态加载代码。这有助于避免每次代码更改时都进行不必要的 Docker 镜像构建。
- en: Pre-Built Solutions and Pre-Trained Models with SageMaker JumpStart
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker JumpStart 的预构建解决方案和预训练模型
- en: SageMaker JumpStart gives us access to pre-built machine learning solutions
    and pre-trained models from AWS, TensorFlow Hub, and PyTorch Hub. The pre-built
    solutions cover many common use cases such as fraud detection, predictive maintenance,
    and demand forecasting. The pre-trained models span natural language processing,
    object detection, and image classification domains. We can fine-tune the models
    with our own datasets and deploy them to production in our AWS account with just
    a few clicks. We will dive deeper into SageMaker JumpStart in [Chapter 7](ch07.html#train_your_first_model).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker JumpStart 提供了从 AWS、TensorFlow Hub 和 PyTorch Hub 获取预构建的机器学习解决方案和预训练模型的访问。预构建的解决方案涵盖了许多常见用例，如欺诈检测、预测性维护和需求预测。预训练模型涵盖了自然语言处理、对象检测和图像分类等领域。我们可以使用自己的数据集对模型进行微调，并通过几次点击将其部署到我们
    AWS 账户的生产环境中。我们将在[第 7 章](ch07.html#train_your_first_model)中深入探讨 SageMaker JumpStart。
- en: Tune and Validate Models with SageMaker Hyper-Parameter Tuning
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker 超参数调整调优和验证模型
- en: Another important step in developing high-quality models is finding the right
    model configuration or model hyper-parameters. In contrast to the model parameters
    that are learned by the algorithm, hyper-parameters control how the algorithm
    learns the parameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 开发高质量模型的另一个重要步骤是找到正确的模型配置或模型超参数。与算法学习的模型参数相比，超参数控制算法如何学习参数。
- en: Amazon SageMaker comes with automatic model tuning and validating capabilities
    to find the best performing model hyper-parameters for our model and dataset.
    We need to define an objective metric to optimize, such as validation accuracy,
    and the hyper-parameter ranges to explore. SageMaker will then run many model
    training jobs to explore the hyper-parameter ranges that we specify and evaluate
    the results against the objective metric to measure success.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 自带自动模型调优和验证功能，以找到最佳的模型超参数，适用于我们的模型和数据集。我们需要定义一个优化的客观指标，例如验证准确率，并探索要优化的超参数范围。然后，SageMaker
    将运行多个模型训练作业，以探索我们指定的超参数范围，并根据客观指标评估结果以衡量成功。
- en: 'There are different strategies to explore hyper-parameter ranges: grid search,
    random search, and Bayesian optimization are the most common ones. We will dive
    deeper into SageMaker Hyper-Parameter Tuning in [Chapter 8](ch08.html#train_and_optimize_models_at_scale).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的策略可以探索超参数范围：网格搜索、随机搜索和贝叶斯优化是最常见的方法。我们将深入探讨 SageMaker 超参数调整在[第 8 章](ch08.html#train_and_optimize_models_at_scale)中。
- en: Model Deployment with Amazon SageMaker and AWS Lambda Functions
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 和 AWS Lambda 函数部署模型
- en: 'Once we have trained, validated, and optimized our model, we are ready to deploy
    and monitor our model. There are generally three ways to deploy our models with
    Amazon SageMaker, depending on our application requirements: SageMaker Endpoints
    for REST-based predictions, AWS Lambda functions for serverless predictions, and
    SageMaker Batch Transform for batch predictions.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们已经训练、验证和优化了我们的模型，我们就可以准备部署和监控我们的模型。通常有三种方式可以使用 Amazon SageMaker 部署我们的模型，具体取决于我们的应用需求：SageMaker
    端点用于基于 REST 的预测，AWS Lambda 函数用于无服务器预测，以及 SageMaker 批量转换用于批量预测。
- en: SageMaker Endpoints
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 端点
- en: If we need to optimize the model deployment for low-latency, real-time predictions,
    we can deploy our model using SageMaker hosting services. These services will
    spin up a SageMaker endpoint to host our model and provide a REST API to serve
    predictions. We can call the REST API from our applications to receive model predictions.
    SageMaker model endpoints support auto-scaling to match the current traffic pattern
    and are deployed across multiple AZs for high availability.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要优化低延迟、实时预测的模型部署，我们可以使用 SageMaker 托管服务部署我们的模型。这些服务将启动一个 SageMaker 端点来托管我们的模型，并提供一个
    REST API 来提供预测。我们可以从我们的应用程序调用 REST API 来接收模型预测结果。SageMaker 模型端点支持根据当前流量模式自动扩展，并在多个可用区部署以实现高可用性。
- en: SageMaker Batch Transform
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 批量转换
- en: If we need to get predictions for an entire dataset, we can use SageMaker Batch
    Transform. Batch Transform is optimized for high throughput, without the need
    for real-time, low-latency predictions. SageMaker will spin up the specified number
    of resources to perform large-scale, batch predictions on our S3 data. Once the
    job completes, SageMaker will write the data to S3 and tear down the compute resources.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要为整个数据集获取预测结果，我们可以使用 SageMaker 批转换。批转换针对高吞吐量进行了优化，无需实时、低延迟预测。SageMaker
    将启动指定数量的资源来对我们的 S3 数据执行大规模批量预测。作业完成后，SageMaker 将数据写入 S3 并拆除计算资源。
- en: Serverless Model Deployment with AWS Lambda
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 进行无服务器模型部署
- en: Another option to serve our model predictions are AWS Lambda functions for serverless
    model servers. After training the model with SageMaker, we use an AWS Lambda function
    that retrieves the model from S3 and serves the predictions. AWS Lambda does have
    memory and latency limitations, so be sure to test this option at scale before
    finalizing on this deployment approach.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于提供模型预测的选项是 AWS Lambda 函数用于无服务器模型服务器。在使用 SageMaker 训练模型之后，我们使用 AWS Lambda
    函数从 S3 检索模型并提供预测。AWS Lambda 具有内存和延迟限制，因此在最终确定部署方法之前，请务必在规模上进行测试。
- en: Streaming Analytics and Machine Learning on AWS
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 上的流分析和机器学习
- en: Until now, we assumed that we have all of our data available in a centralized
    static location, such as our S3-based data lake. In reality, data is continuously
    streaming from many different sources across the world simultaneously. In many
    cases, we want to perform real-time analytics and machine learning on this streaming
    data before it lands in a data lake. A short time to (business) insight is required
    to gain competitive advances and to react quickly to changing customer and market
    trends.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设我们所有的数据都集中存储在一个静态位置，例如基于 S3 的数据湖。实际上，数据不断地从世界各地的许多不同来源同时流动。在许多情况下，我们希望在数据落入数据湖之前对这些流数据进行实时分析和机器学习。需要在短时间内（业务）洞察以获取竞争优势，并迅速响应变化的客户和市场趋势。
- en: Streaming technologies provide us with the tools to collect, process, and analyze
    data streams in real time. AWS offers a wide range of streaming technology options,
    including Amazon Kinesis and Amazon Managed Streaming for Apache Kafka (Amazon
    MSK). We will dive deep into streaming analytics and machine learning in [Chapter 11](ch11.html#streaming_analytics_and_machine_lear).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 流技术为我们提供了在实时中收集、处理和分析数据流的工具。AWS 提供了多种流技术选项，包括 Amazon Kinesis 和 Amazon 管理的 Apache
    Kafka（Amazon MSK）。我们将深入探讨流分析和机器学习在 [第 11 章](ch11.html#streaming_analytics_and_machine_lear)
    中。
- en: Amazon Kinesis Streaming
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Kinesis 流式处理
- en: Amazon Kinesis is a streaming data service, which helps us collect, process,
    and analyze data in real time. With Kinesis Data Firehose, we can prepare and
    load real-time data continuously to various destinations including Amazon S3 and
    Amazon Redshift. With Kinesis Data Analytics, we can process and analyze the data
    as it arrives. And with Amazon Kinesis Data Streams, we can manage the ingest
    of data streams for custom applications.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis 是一个流数据服务，帮助我们实时收集、处理和分析数据。通过 Kinesis Data Firehose，我们可以连续准备和加载实时数据到各种目的地，包括
    Amazon S3 和 Amazon Redshift。通过 Kinesis Data Analytics，我们可以处理和分析数据在到达时。通过 Amazon
    Kinesis Data Streams，我们可以管理数据流的摄取，用于定制应用程序。
- en: Amazon Managed Streaming for Apache Kafka
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon 管理的 Apache Kafka 流式处理
- en: Amazon MSK is a streaming data service that manages Apache Kafka infrastructure
    and operations. Apache Kafka is a popular open source, high-performance, fault-tolerant,
    and scalable platform for building real-time streaming data pipelines and applications.
    Using Amazon MSK, we can run our Apache Kafka applications on AWS without the
    need to manage Apache Kafka clusters ourselves.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon MSK 是一个流式数据服务，管理 Apache Kafka 基础架构和运营。Apache Kafka 是一个流行的开源、高性能、容错和可扩展平台，用于构建实时流式数据管道和应用程序。使用
    Amazon MSK，我们可以在 AWS 上运行我们的 Apache Kafka 应用程序，无需自行管理 Apache Kafka 集群。
- en: Streaming Predictions and Anomaly Detection
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式预测和异常检测
- en: In the streaming data chapter, we will focus on analyzing a continuous stream
    of product review messages that we collect from available online channels. We
    will run streaming predictions to detect the sentiment of our customers, so we
    can identify which customers might need high-priority attention.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在流数据章节中，我们将专注于分析我们从可用的在线渠道收集的连续产品评价消息流。我们将运行流式预测以检测客户的情绪，从而可以识别哪些客户可能需要高优先级的关注。
- en: Next, we run continuous streaming analytics over the incoming review messages
    to capture the average sentiment per product category. We visualize the continuous
    average sentiment in a metrics dashboard for the line of business (LOB) owners.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对传入的评论消息进行连续流分析，以捕获每个产品类别的平均情感。我们为业务线（LOB）所有者在度量仪表板中可视化连续的平均情感。
- en: The LOB owners can now detect sentiment trends quickly and take action. We also
    calculate an anomaly score of the incoming messages to detect anomalies in the
    data schema or data values. In case of a rising anomaly score, we can alert the
    application developers in charge to investigate the root cause.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: LOB所有者现在可以快速检测情感趋势并采取行动。我们还计算传入消息的异常分数，以检测数据模式或数据值中的异常。在异常分数上升的情况下，我们可以提醒负责的应用程序开发人员调查根本原因。
- en: As a last metric, we also calculate a continuous approximate count of the received
    messages. This number of online messages could be used by the digital marketing
    team to measure effectiveness of social media campaigns.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的度量标准，我们还计算收到消息的持续近似计数。数字营销团队可以使用在线消息数量来衡量社交媒体活动的有效性。
- en: AWS Infrastructure and Custom-Built Hardware
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS基础设施和自定义构建硬件
- en: A key benefit of cloud computing is the ability to try infrastructure options
    that specifically match our workload. AWS provides many options for high-performance
    compute, networking, and storage infrastructure for our data science projects,
    as [Figure 1-4](#aws_infrastructure_options_for_data_sci) shows. Let’s see each
    of these options, which we will reference throughout.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的一个关键优势是能够尝试特别匹配我们工作负载的基础设施选项。AWS为我们的数据科学项目提供了许多高性能计算、网络和存储基础设施选项，如[图1-4](#aws_infrastructure_options_for_data_sci)所示。让我们看看每个选项，我们将在整个过程中引用它们。
- en: '![](assets/dsaw_0104.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0104.png)'
- en: Figure 1-4\. AWS infrastructure options for data science and machine learning
    projects.
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4. AWS用于数据科学和机器学习项目的基础设施选项。
- en: SageMaker Compute Instance Types
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker计算实例类型
- en: 'AWS allows us to choose from a diverse set of instance types depending on our
    workload. Following is a list of instance types commonly used for data science
    use cases:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: AWS允许我们根据工作负载选择多种实例类型。以下是常用于数据科学用例的实例类型列表：
- en: T instance type
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: T实例类型
- en: General-purpose, burstable-performance instances when we don’t need consistently
    high levels of CPU but benefit from having fast CPUs when we need them
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通用型、突发性能实例，当我们不需要持续高水平CPU时，但在需要时受益于快速CPU
- en: M instance type
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: M实例类型
- en: General-purpose instances with a good balance of compute, memory, and network
    bandwidth
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 具有良好计算、内存和网络带宽平衡的通用实例
- en: C instance type
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: C实例类型
- en: Compute-optimized instances ideal for compute-bound workloads with high-CPU
    requirements
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于计算密集型工作负载和高CPU需求的计算优化实例
- en: R instance type
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: R实例类型
- en: Memory-optimized instances optimized for workloads that benefit from storing
    large datasets in memory such as Apache Spark
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 针对需要将大型数据集存储在内存中的工作负载进行优化的内存优化实例，如Apache Spark
- en: P, G, Inferentia, and Trainium instance types
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: P、G、Inferentia和Trainium实例类型
- en: High-performance compute instances with hardware accelerators or coprocessors
    such as GPUs or Amazon custom-built hardware such as AWS Inferentia for inference
    and AWS Trainium for training workloads
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 配备硬件加速器或协处理器（如GPU或亚马逊自定义硬件，例如AWS Inferentia用于推断和AWS Trainium用于训练工作负载）的高性能计算实例
- en: Amazon Elastic Inference Accelerator
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Elastic推断加速器
- en: Network-attached coprocessors used by other instance types when additional compute
    power is needed for specific workloads, such as batch transformations and inference
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要特定工作负载的额外计算能力时，其他实例类型使用的网络附加协处理器，如批处理转换和推断
- en: GPUs and Amazon Custom-Built Compute Hardware
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU和亚马逊自定义计算硬件
- en: Similar to how Amazon S3 provides storage in the cloud, Amazon Elastic Compute
    Cloud (Amazon EC2) provides compute resources. We can choose from over 350 instances
    for our business needs and workload. AWS also offers a choice of Intel, AMD, and
    ARM-based processors. The hardware-accelerated P4, P3, and G4 instance types are
    a popular choice for high-performance, GPU-based model training. Amazon also provides
    custom-build hardware optimized for both model training and inference.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Amazon S3在云中提供存储，Amazon Elastic Compute Cloud（Amazon EC2）提供计算资源。我们可以根据业务需求和工作负载选择超过350个实例。AWS还提供选择英特尔、AMD和基于ARM的处理器。硬件加速的P4、P3和G4实例类型是基于GPU的高性能模型训练的热门选择。亚马逊还提供了优化用于模型训练和推断的自定义构建硬件。
- en: P4d instances consist of eight NVIDIA A100 Tensor Core GPUs with 400 Gbps instance
    networking and support for Elastic Fabric Adapter (EFA) with NVIDIA GPUDirect
    RDMA (remote direct memory access). P4d instances are deployed in hyperscale clusters
    called *Amazon EC2 UltraClusters* that provide supercomputer-class performance
    for everyday ML developers, researchers, and data scientists. Each EC2 UltraCluster
    of P4d instances gives us access to more than 4,000 NVIDIA A100 GPUs, petabit-scale
    nonblocking networking, and high-throughput/low-latency storage via Amazon FSx
    for Lustre.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: P4d 实例由八个 NVIDIA A100 Tensor Core GPU 组成，具备 400 Gbps 的实例网络和支持带 NVIDIA GPUDirect
    RDMA（远程直接内存访问）的 Elastic Fabric Adapter（EFA）。P4d 实例部署在称为 *Amazon EC2 超级集群* 的超大规模集群中，为日常的机器学习开发人员、研究人员和数据科学家提供超级计算机级别的性能。每个
    P4d 实例的 EC2 超级集群可以访问超过 4,000 个 NVIDIA A100 GPU、PB 级非阻塞网络以及通过 Amazon FSx for Lustre
    实现的高吞吐量/低延迟存储。
- en: P3 instances consist of up to eight NVIDIA V100 Tensor Core GPUs and deliver
    up to 100 Gbps of networking throughput. P3 instances deliver up to one petaflop
    of mixed-precision performance per instance. P3dn.24xlarge instances also support
    EFA.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: P3 实例包含最多八个 NVIDIA V100 Tensor Core GPU，并提供高达 100 Gbps 的网络吞吐量。每个 P3 实例可以提供高达一
    petaflop 的混合精度性能。P3dn.24xlarge 实例也支持 EFA。
- en: The G4 instances are a great option for cost-sensitive, small-scale training
    or inference workloads. G4 instances consist of NVIDIA T4 GPUs with up to 100
    Gbps of networking throughput and up to 1.8 TB of local NVMe storage.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: G4 实例是成本敏感、小规模训练或推断工作负载的理想选择。G4 实例包含 NVIDIA T4 GPU，具备高达 100 Gbps 的网络吞吐量和高达 1.8
    TB 的本地 NVMe 存储。
- en: AWS also offers custom-built silicon for machine learning training with the
    AWS Trainium chip and for inference workloads with the AWS Inferentia chip. Both
    AWS Trainium and AWS Inferentia aim at increasing machine learning performance
    and reducing infrastructure cost.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 还提供了专为机器学习训练而定制的 AWS Trainium 芯片，以及专为推断工作负载而设计的 AWS Inferentia 芯片。AWS Trainium
    和 AWS Inferentia 都旨在提高机器学习性能并降低基础设施成本。
- en: AWS Trainium has been optimized for deep learning training workloads, including
    image classification, semantic search, translation, voice recognition, natural
    language processing, and recommendation engines.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Trainium 已经针对深度学习训练工作负载进行了优化，包括图像分类、语义搜索、翻译、语音识别、自然语言处理和推荐引擎。
- en: AWS Inferentia processors support many popular machine learning models, including
    single-shot detectors and ResNet for computer vision—as well as Transformers and
    BERT for natural language processing.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia 处理器支持许多流行的机器学习模型，包括用于计算机视觉的单次检测器和 ResNet，以及用于自然语言处理的 Transformers
    和 BERT。
- en: AWS Inferentia is available via the Amazon EC2 Inf1 instances. We can choose
    between 1 and 16 AWS Inferentia processors per Inf1 instance, which deliver up
    to 2,000 tera operations per second. We can use the AWS Neuron SDK to compile
    our TensorFlow, PyTorch, or Apache MXNet models to run on Inf1 instances.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia 可通过 Amazon EC2 Inf1 实例使用。我们可以在每个 Inf1 实例中选择 1 到 16 个 AWS Inferentia
    处理器，其提供高达 2,000 万亿次操作每秒的性能。我们可以使用 AWS Neuron SDK 来编译我们的 TensorFlow、PyTorch 或 Apache
    MXNet 模型以在 Inf1 实例上运行。
- en: Inf1 instances can help to reduce our inference cost, with up to 45% lower cost
    per inference and 30% higher throughput compared to Amazon EC2 G4 instances. [Table 1-1](#ectwo_instance_options_for_model_infere)
    shows some instance-type options to use for model inference, including Amazon
    custom-built Inferentia chip, CPUs, and GPUs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Inf1 实例有助于降低推断成本，每次推断的成本比 Amazon EC2 G4 实例低高达 45%，吞吐量则高出 30%。[表 1-1](#ectwo_instance_options_for_model_infere)
    展示了用于模型推断的一些实例类型选项，包括 Amazon 自定义的 Inferentia 芯片、CPU 和 GPU。
- en: Table 1-1\. EC2 instance options for model inference
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. 用于模型推断的 EC2 实例选项
- en: '| Model characteristics | EC2 Inf1 | EC2 C5 | EC2 G4 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 模型特征 | EC2 Inf1 | EC2 C5 | EC2 G4 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Requires low latency and high throughput at low cost | X |   |   |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 需要低延迟和高吞吐量以低成本 | X |   |   |'
- en: '| Low sensitivity to latency and throughput |   | X |   |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 对延迟和吞吐量的敏感性低 |   | X |   |'
- en: '| Requires NVIDIA’s CUDA, CuDNN or TensorRT libraries |   |   | X |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 需要 NVIDIA 的 CUDA、CuDNN 或 TensorRT 库 |   |   | X |'
- en: Amazon Elastic Inference is another option to leverage accelerated compute for
    model inference. Elastic Inference allows us to attach a fraction of GPU acceleration
    to any Amazon EC2 (CPU-based) instance type. Using Elastic Inference, we can decouple
    the instance choice for model inference from the amount of inference acceleration.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Elastic Inference 是利用加速计算进行模型推断的另一种选择。弹性推断允许我们将部分 GPU 加速附加到任何 Amazon
    EC2（基于 CPU）实例类型。使用弹性推断，我们可以将模型推断的实例选择与推断加速的数量分离开来。
- en: Choosing Elastic Inference over Inf1 might make sense if we need different instance
    characteristics than those offered with Inf1 instances or if our performance requirements
    are lower than what the smallest Inf1 instance provides.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要不同于 Inf1 实例提供的实例特性，或者性能需求低于最小的 Inf1 实例提供的性能要求，选择 Elastic Inference 而不是
    Inf1 可能是有意义的选择。
- en: Elastic Inference scales from single-precision TFLOPS (trillion floating point
    operations per second) up to 32 mixed-precision TFLOPS of inference acceleration.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性推断从单精度 TFLOPS（每秒万亿浮点运算）扩展到 32 个混合精度 TFLOPS 的推断加速。
- en: Graviton processors are AWS custom-built ARM processors. The CPUs leverage 64-bit
    Arm Neoverse cores and custom silicon designed by AWS using advanced 7 nm manufacturing
    technology. ARM-based instances can offer an attractive price-performance ratio
    for many workloads running in Amazon EC2.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Graviton 处理器是 AWS 自定义的 ARM 处理器。这些 CPU 利用 64 位 Arm Neoverse 核心和 AWS 设计的使用先进的
    7 纳米制造技术的定制硅。基于 ARM 的实例可以为在 Amazon EC2 中运行的许多工作负载提供具有吸引力的性价比。
- en: The first generation of Graviton processors are offered with Amazon EC2 A1 instances.
    Graviton2 processors deliver 7x more performance, 4x more compute cores, 5x faster
    memory, and 2x larger caches compared to the first generation. We can find the
    Graviton2 processor in Amazon EC2 T4g, M6g, C6g, and R6g instances.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Graviton 处理器的第一代提供了 Amazon EC2 A1 实例。与第一代相比，Graviton2 处理器的性能提升了 7 倍，计算核心增加了
    4 倍，内存速度提升了 5 倍，缓存大小增加了 2 倍。我们可以在 Amazon EC2 T4g、M6g、C6g 和 R6g 实例中找到 Graviton2
    处理器。
- en: The AWS Graviton2 processors provide enhanced performance for video encoding
    workloads, hardware acceleration for compression workloads, and support for machine
    learning predictions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Graviton2 处理器为视频编码工作负载提供了增强的性能，为压缩工作负载提供了硬件加速，并支持机器学习预测。
- en: GPU-Optimized Networking and Custom-Built Hardware
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 优化的网络和定制硬件
- en: AWS offers advanced networking solutions that can help us to efficiently run
    distributed model training and scale-out inference.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供先进的网络解决方案，可以帮助我们有效地进行分布式模型训练和扩展推断。
- en: The Amazon EFA is a network interface for Amazon EC2 instances that optimizes
    internode communications at scale. EFA uses a custom-built OS bypass hardware
    interface that enhances the performance of internode communications. If we are
    using the NVIDIA Collective Communications Library for model training, we can
    scale to thousands of GPUs using EFA.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EFA 是 Amazon EC2 实例的网络接口，可优化规模化的节点间通信。EFA 使用定制的 OS 旁路硬件接口，增强了节点间通信的性能。如果我们正在使用
    NVIDIA Collective Communications Library 进行模型训练，我们可以使用 EFA 扩展到数千个 GPU。
- en: We can combine the setup with up to 400 Gbps network bandwidth per instance
    and the NVIDIA GPUDirect RDMA for low-latency, GPU-to-GPU communication between
    instances. This gives us the performance of on-premises GPU clusters with the
    on-demand elasticity and flexibility of the cloud.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将每个实例的网络带宽组合到 400 Gbps，并使用 NVIDIA GPUDirect RDMA 实现实例之间低延迟的 GPU 到 GPU 通信。这使我们可以在云中获得与本地
    GPU 集群相当的性能，同时具有按需的弹性和灵活性。
- en: Storage Options Optimized for Large-Scale Model Training
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对大规模模型训练优化的存储选项
- en: We already learned about the benefits of building our data lake on Amazon S3\.
    If we need faster storage access for distributed model training, we can use Amazon
    FSx for Lustre.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，在 Amazon S3 上构建数据湖的好处。如果我们需要更快的存储访问以进行分布式模型训练，我们可以使用 Amazon FSx for
    Lustre。
- en: Amazon FSx for Lustre offers the open source Lustre filesystem as a fully managed
    service. Lustre is a high-performance filesystem, offering submillisecond latencies,
    up to hundreds of gigabytes per second of throughput, and millions of IOPS.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon FSx for Lustre 提供开源 Lustre 文件系统作为完全托管的服务。Lustre 是一种高性能文件系统，提供亚毫秒级的延迟，高达数百
    GB/s 的吞吐量和数百万 IOPS。
- en: We can link FSx for Lustre filesystems with Amazon S3 buckets. This allows us
    to access and process data through the FSx filesystem and from Amazon S3\. Using
    FSx for Lustre, we can set up our model training compute instances to access the
    same set of data through high-performance shared storage.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将FSx for Lustre文件系统与Amazon S3存储桶链接起来。这使我们能够通过FSx文件系统访问和处理数据，并从Amazon S3获取数据。使用FSx
    for Lustre，我们可以设置我们的模型训练计算实例通过高性能共享存储访问相同的数据集。
- en: Amazon Elastic File System (Amazon EFS) is another file storage service that
    provides a filesystem interface for up to thousands of Amazon EC2 instances. The
    filesystem interface offers standard operating system file I/O APIs and enables
    filesystem access semantics, such as strong consistency and file locking.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon弹性文件系统（Amazon EFS）是另一种文件存储服务，为多达数千个Amazon EC2实例提供文件系统接口。文件系统接口提供标准操作系统文件I/O
    API，并支持文件系统访问语义，例如强一致性和文件锁定。
- en: Reduce Cost with Tags, Budgets, and Alerts
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标签、预算和警报降低成本
- en: Throughout the book, we provide tips on how to reduce cost for data science
    projects with the Amazon AI and machine learning stack. Overall, we should always
    tag our resources with the name of the business unit, application, environment,
    and user. We should use tags that provide visibility into where our money is spent.
    In addition to the AWS built-in cost-allocation tags, we can provide our own user-defined
    allocation tags specific to our domain. AWS Budgets help us create alerts when
    cost is approaching—or exceeding—a given threshold.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们提供了如何利用亚马逊人工智能和机器学习堆栈来减少数据科学项目成本的建议。总体而言，我们应始终为我们的资源打上业务单位、应用程序、环境和用户的名称标签。我们应使用能够显示我们的花费情况的标签。除了AWS内置的成本分配标签外，我们还可以提供自己领域特定的用户定义的分配标签。AWS预算帮助我们在成本接近或超过给定阈值时创建警报。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the benefits of developing data science projects
    in the cloud, with a specific focus on AWS. We showed how to quickly add intelligence
    to our applications leveraging the Amazon AI and machine learning stack. We introduced
    the concept of AutoML and explained how SageMaker Autopilot offers a transparent
    approach to AutoML. We then discussed a typical machine learning workflow in the
    cloud and introduced the relevant AWS services that assist in each step of the
    workflow. We provided an overview of available workflow orchestration tools to
    build and automate machine learning pipelines. We described how to run streaming
    analytics and machine learning over real-time data. We finished this chapter with
    an overview of AWS infrastructure options to leverage in our data science projects.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在云中开发数据科学项目的好处，特别关注了AWS。我们展示了如何利用亚马逊人工智能和机器学习堆栈快速为我们的应用程序增加智能。我们介绍了AutoML的概念，并解释了SageMaker
    Autopilot如何提供透明的AutoML方法。然后我们讨论了云中典型的机器学习工作流程，并介绍了在工作流程的每个步骤中帮助的相关AWS服务。我们概述了可用的工作流编排工具，用于构建和自动化机器学习流水线。我们描述了如何在实时数据上运行流式分析和机器学习。我们在本章末尾概述了AWS基础设施选项，以利用在我们的数据科学项目中使用。
- en: In [Chapter 2](ch02.html#data_science_use_cases), we will discuss prominent
    data science use cases across industries such as media, advertising, IoT, and
    manufacturing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#data_science_use_cases)中，我们将讨论媒体、广告、物联网和制造业等行业中显著的数据科学用例。
