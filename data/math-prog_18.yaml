- en: 15 Classifying data with logistic regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 使用逻辑回归对数据进行分类
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding classification problems and measuring classifiers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类问题和衡量分类器
- en: Finding decision boundaries to classify two kinds of data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找决策边界以对两种数据进行分类
- en: Approximating classified data sets with logistic functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑函数近似分类数据集
- en: Writing a cost function for logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为逻辑回归编写成本函数
- en: Carrying out gradient descent to find a logistic function of best fit
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行梯度下降以找到最佳拟合的逻辑函数
- en: One of the most important classes of problems in machine learning is *classification*,
    which we’ll focus on in the last two chapters of this book. A classification problem
    is one where we’ve got one or more pieces of raw data, and we want to say what
    *kind* of object each one represents. For instance, we might want an algorithm
    to look at the data of all email messages entering our inbox and classify each
    one as an interesting message or as unwanted spam. As an even more impactful example,
    we could write a classification algorithm to analyze a data set of medical scans
    and decide whether they contain benign or malevolent tumors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中最重要的问题类别之一是*分类*，我们将在这本书的最后两章中重点关注。分类问题是指我们有一份或多份原始数据，我们想要说明每一份数据代表的是哪种类型的对象。例如，我们可能希望一个算法查看进入我们收件箱的所有电子邮件的数据，并将每一封电子邮件分类为有趣的邮件或不受欢迎的垃圾邮件。作为一个更有影响力的例子，我们可以编写一个分类算法来分析医学扫描数据集，并决定它们是否包含良性或恶性的肿瘤。
- en: We can build machine learning algorithms for classification where the more real
    data our algorithm sees, the more it learns, and the better it performs at the
    classification task. For instance, every time an email user flags an email as
    spam or a radiologist identifies a malignant tumor, this data can be passed back
    to the algorithm to improve its calibration.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建机器学习算法进行分类，我们的算法看到的真实数据越多，它学到的就越多，在分类任务上的表现就越好。例如，每次电子邮件用户将电子邮件标记为垃圾邮件或放射科医生识别出恶性肿瘤时，这些数据都可以反馈给算法以改进其校准。
- en: 'In this chapter, we look at the same simple data set as in the last chapter:
    mileages and prices of used cars. Instead of using data for a single model of
    car like in the last chapter, we’ll look at two car models: Toyota Priuses and
    BMW 5 series sedans. Based only on the numeric data of the car’s mileage and price,
    and a reference data set of known examples, we want our algorithm to give us a
    yes or no answer as to whether the car is a BMW. As opposed to a regression model
    that takes in a number and produces another number, the classification model will
    take in a vector and produce a number between zero and one, representing the confidence
    that the vector represents a BMW instead of a Prius (figure 15.1).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将查看与上一章相同的简单数据集：二手车的里程和价格。与上一章使用单一车型数据不同，我们将查看两种车型：丰田普锐斯和宝马5系列轿车。仅基于车辆的里程和价格等数值数据以及已知示例的参考数据集，我们希望我们的算法能够给出一个肯定或否定的答案，即该车辆是否为宝马。与接受一个数字并产生另一个数字的回归模型不同，分类模型将接受一个向量并产生一个介于零和一之间的数字，表示该向量代表宝马而不是普锐斯的置信度（图15.1）。
- en: '![](../Images/CH15_F01_Orland.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F01_Orland.png)'
- en: Figure 15.1 Our classifier takes a vector of two numbers, the mileage and price
    of a used car, and returns a number representing its confidence that the car is
    a BMW.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 我们的分类器接受一个包含两个数字的向量，即二手车的里程和价格，并返回一个表示其对车辆是宝马的置信度的数字。
- en: Even though classification has different inputs and outputs than regression,
    it turns out we can build our classifier using a type of regression. The algorithm
    we’ll implement in this chapter is called logistic regression. To train this algorithm,
    we start with a known data set of used car mileages and prices, labeled with a
    1 if they are BMWs and a 0 if they are Priuses. Table 15.1 shows sample points
    in this data set that we use to train our algorithm.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类与回归的输入和输出不同，但结果表明我们可以使用回归的一种类型来构建我们的分类器。本章我们将实现的算法称为逻辑回归。为了训练这个算法，我们从一个已知的数据集开始，该数据集包含二手车的里程和价格，如果它们是宝马则标记为1，如果是普锐斯则标记为0。表15.1显示了用于训练我们的算法的该数据集中的样本点。
- en: Table 15.1 Sample data points used to train the algorithm
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.1 用于训练算法的样本数据点
- en: '| Mileage (mi) | Price ($) | Is BMW? |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 里程（英里） | 价格（美元） | 是否为宝马？ |'
- en: '| 110,890.0 | 13,995.00 | 1 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 110,890.0 | 13,995.00 | 1 |'
- en: '| 94,133.0 | 13,982.00 | 1 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 94,133.0 | 13,982.00 | 1 |'
- en: '| 70,000.0 | 9,900.00 | 0 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 70,000.0 | 9,900.00 | 0 |'
- en: '| 46,778.0 | 14,599.00 | 1 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 46,778.0 | 14,599.00 | 1 |'
- en: '| 84,507.0 | 14,998.00 | 0 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 84,507.0 | 14,998.00 | 0 |'
- en: '| . . . | . . . | . . . |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| . . . | . . . | . . . |'
- en: We want a function that takes the values in the first two columns and produces
    a result that is between zero and one, and hopefully, close to the correct choice
    of car. I’ll introduce you to a special kind of function called a *logistic function*,
    which takes a pair of input numbers and produces a single output number that is
    always between zero and one. Our classification function is the logistic function
    that “best fits” the sample data we provide.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望一个函数能够接受前两列的值，并产生一个介于零和一之间的结果，并且希望这个结果尽可能接近正确的汽车选择。我将向你介绍一种特殊类型的函数，称为*逻辑函数*，它接受一对输入数字并产生一个始终介于零和一之间的单个输出数字。我们的分类函数是“最佳拟合”我们提供的样本数据的逻辑函数。
- en: Our classification function won’t always get the answer right, but then again
    neither would a human. BMW 5 series sedans are luxury cars, so we would expect
    to get a lower price for a Prius than a BMW with the same mileage. Defying our
    expectations, the last two rows of the data in table 5.1 show a Prius and BMW
    at roughly the same price, where the Prius has nearly twice the mileage of the
    BMW. Due to fluke examples like this, we won’t expect the logistic function to
    produce exactly one or zero for each BMW or Prius it sees. Rather it can return
    0.51, which is the function’s way of telling us it’s not sure, but the data is
    slightly more likely to represent a BMW.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类函数并不总是能得到正确的答案，但同样，人类也不总是能得到正确的答案。宝马5系列轿车是豪华车，所以我们预计，与宝马相比，普锐斯的售价会低一些。出乎我们的意料，表5.1中的最后两行显示，普锐斯和宝马的价格大致相同，而普锐斯的里程几乎是宝马的两倍。由于这样的意外例子，我们不会期望逻辑函数对每个宝马或普锐斯都能产生精确的1或0。相反，它可以返回0.51，这是函数告诉我们它不确定，但数据稍微更有可能代表宝马。
- en: In the last chapter, we saw that the linear function we chose was determined
    by the two parameters *a* and *b* in the formula *f*(*x*) = *ax* + *b*. The logistic
    functions we’ll use in this chapter are parametrized by three parameters, so the
    task of logistic regression boils down to finding three numbers that get the logistic
    function as close as possible to the sample data provided. We’ll create a special
    cost function for the logistic function and find the three parameters that minimize
    the cost function using gradient descent. There’s a lot of steps here, but fortunately,
    they all parallel what we did in the last chapter, so it will be a useful review
    if you’re learning about regression for the first time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到我们选择的线性函数是由公式 *f*(*x*) = *ax* + *b* 中的两个参数 *a* 和 *b* 决定的。在本章中，我们将使用的逻辑函数由三个参数参数化，因此逻辑回归的任务可以归结为找到三个数字，使逻辑函数尽可能接近提供的样本数据。我们将为逻辑函数创建一个特殊的成本函数，并使用梯度下降法找到最小化成本函数的三个参数。这里有很多步骤，但幸运的是，它们都与我们在上一章中做的事情平行，所以如果你是第一次学习回归，这将是一个有用的复习。
- en: Coding the logistic regression algorithm to classify the cars is the meat of
    the chapter, but before doing that, we spend a bit more time getting you familiar
    with the process of classification. And before we train a computer to do the classification,
    let’s measure how well we can do the task. Then, once we build our logistic regression
    model, we can evaluate how well it does by comparison.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将逻辑回归算法编码为分类汽车是本章的重点，但在做这件事之前，我们花更多的时间让你熟悉分类的过程。在我们训练计算机进行分类之前，让我们衡量我们能够完成这个任务的程度。然后，一旦我们构建了逻辑回归模型，我们可以通过比较来评估它的表现。
- en: 15.1 Testing a classification function on real data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 在实际数据上测试分类函数
- en: 'Let’s see how well we can identify BMWs in our data set using a simple criterion.
    Namely, if a used car has a price above $25,000, it’s probably too expensive to
    be a Prius (after all, you can get a brand new Prius for near that amount). If
    the price is above $25,000, we’ll say that it is a BMW; otherwise, we’ll say that
    it’s a Prius. This classification is easy to build as a Python function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何能够使用一个简单的标准来识别数据集中的宝马。也就是说，如果一辆二手车的价格高于25,000美元，那么它可能太贵了，不能是普锐斯（毕竟，你可以以接近这个价格买到一辆全新的普锐斯）。如果价格高于25,000美元，我们将说它是宝马；否则，我们将说它是普锐斯。这个分类很容易构建为一个Python函数：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The performance of this classifier might not be that great because it’s conceivable
    that BMWs with a lot of miles might sell for less than $25,000\. But we don’t
    have to speculate: we can measure how well this classifier does on actual data.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器的性能可能不会很好，因为可以想象，行驶里程很多的宝马可能售价低于25,000美元。但我们不必猜测：我们可以衡量这个分类器在实际数据上的表现如何。
- en: In this section, we measure the performance of our algorithm by writing a function
    called `test_classifier`, which takes a classification function like `bmw_finder`
    as well as the data set to test. The data set is an array of tuples of mileages,
    prices, and a `1` or `0`, indicating whether the car is a BMW or a Prius. Once
    we run the `test _classifier` function with real data, it returns a percent value,
    telling us how many of the cars it identifies correctly. At the end of the chapter
    when we’ve implemented logistic regression, we can instead pass in our logistic
    classification function to `test_classifier` and see its relative performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过编写一个名为 `test_classifier` 的函数来衡量我们算法的性能，该函数接受一个分类函数（如 `bmw_finder`）以及要测试的数据集。数据集是一个包含里程数、价格和
    `1` 或 `0` 的元组数组，表示汽车是宝马还是普锐斯。一旦我们用真实数据运行 `test_classifier` 函数，它将返回一个百分比值，告诉我们它正确识别了多少辆车。在章节末尾，当我们实现了逻辑回归时，我们可以将我们的逻辑分类函数传递给
    `test_classifier` 并查看其相对性能。
- en: 15.1.1 Loading the car data
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 加载汽车数据
- en: 'It is easier to write the `test_classifier` function if we first load the car
    data. Rather than fuss with loading the data from CarGraph.com or from a flat
    file, I’ve made it easy for you by providing a Python file called cardata.py in
    the source code for the book. It contains two arrays of data: one for Priuses
    and one for BMWs. You can import the two arrays as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们首先加载汽车数据，编写 `test_classifier` 函数会更容易。而不是在从 CarGraph.com 或从平面文件加载数据上浪费时间，我已经通过在本书的源代码中提供一个名为
    cardata.py 的 Python 文件来简化了这一过程。它包含两个数据数组：一个用于普锐斯，一个用于宝马。你可以如下导入这两个数组：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you inspect either the BMW or Prius raw data in the car_data.py file, you’ll
    see that this file contains more data than we need. For now, we’re focusing on
    the mileage and price of each car, and we know what car it is, based on the list
    it belongs to. For instance, the BMW list begins like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查 car_data.py 文件中的宝马或普锐斯原始数据，你会看到这个文件包含比我们所需更多的数据。目前，我们专注于每辆汽车的里程数和价格，并且我们知道它属于哪个列表。例如，宝马列表开始如下：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Each tuple represents one car for sale, and the mileage and price are given
    by the fourth and fifth entries of the tuple, respectively. Within car_data.py,
    these are converted to `Car` objects, so we can write `car.price` instead of `car[4]`,
    for example. We can make a list, called `all_car_data`, of the shape we want by
    pulling the desired entries from the BMW tuples and Prius tuples:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元组代表一辆出售的汽车，其里程数和价格分别由元组的第四和第五个条目给出。在 car_data.py 中，这些被转换为 `Car` 对象，因此我们可以写
    `car.price` 而不是 `car[4]`，例如。我们可以通过从宝马元组和普锐斯元组中提取所需的条目来创建一个形状符合我们要求的 `all_car_data`
    列表：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once this is run, `all_car_data` is a Python list starting with the BMWs and
    ending with the Priuses, labeled with 1’s and 0’s, respectively:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行，`all_car_data` 就是一个以宝马车开始并以普锐斯车结束的 Python 列表，分别用 1 和 0 标记：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 15.1.2 Testing the classification function
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 测试分类函数
- en: With the data in a suitable format, we can now write the `test_classifier` function.
    The job of the `bmw_finder` is to look at the mileage and price of a car and tell
    us whether these represent a BMW. If the answer is yes, it returns a 1; otherwise,
    it returns a 0\. It’s likely that `bmw_finder` will get some answers wrong. If
    it predicts that a car is a BMW (returning 1), but the car is actually a Prius,
    we’ll call that a *false positive*. If it predicts the car is a Prius (returning
    0), but the car is actually a BMW, we’ll call that a *false negative*. If it correctly
    identifies a BMW or a Prius, we’ll call that a *true positive* or *true negative*,
    respectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据以合适的格式存在时，我们现在可以编写 `test_classifier` 函数。`bmw_finder` 的任务是查看一辆汽车的里程数和价格，并告诉我们这些是否代表一辆宝马。如果答案是肯定的，它返回
    1；否则，它返回 0。很可能会出现 `bmw_finder` 预测错误的情况。如果它预测一辆车是宝马（返回 1），但实际上是普锐斯，我们将称之为 *假阳性*。如果它预测汽车是普锐斯（返回
    0），但实际上是宝马，我们将称之为 *假阴性*。如果它正确地识别出宝马或普锐斯，我们将称之为 *真阳性* 或 *真阴性*，分别。
- en: 'To test a classification function against the all_car_data data set, we need
    to run the classification function on each mileage and price in that list, and
    see whether the result of 1 or 0 matches the given value. Here’s what that looks
    like in code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试分类函数对所有汽车数据集，我们需要在该列表中的每个里程数和价格上运行分类函数，并查看结果 1 或 0 是否与给定的值匹配。以下是代码中的样子：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Adds 1 to the trues counter if the classification is correct
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果分类正确，则将 trues 计数器加 1
- en: ❷ Otherwise, adds 1 to the falses counter
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 否则，将 falses 计数器加 1
- en: 'If we run this function with the `bmw_finder` classification function and the
    all_car_data data set, we see that it has 59% accuracy:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用 `bmw_finder` 分类函数和所有汽车数据集运行这个函数，我们看到它的准确率是 59%：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That’s not too bad; we got most of the answers right. But we’ll see we can do
    much better than this! In the next section, we plot the data set to understand
    what’s qualitatively wrong with the `bmw_finder` function. This helps us to see
    how we can improve the classification with our logistic classification function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不错；我们大部分的答案都对了。但我们会看到我们可以做得比这更好！在下一节中，我们将数据集绘制出来，以了解 `bmw_finder` 函数在定性上有什么问题。这有助于我们了解如何通过我们的逻辑分类函数改进分类。
- en: 15.1.3 Exercises
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.3 练习
- en: '| **Exercise 15.1**: Update the `test_classifier` function to print the number
    of true positives, true negatives, false positives, and false negatives. Printing
    these for the `bmw_finder` classifier, what can you tell about the performance
    of the classifier?**Solution**: Rather than just keeping track of correct and
    incorrect predictions, we can track true and false positives and negatives separately:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习 15.1**：更新 `test_classifier` 函数以打印出真正的正例、真正的负例、错误的正例和错误的负例的数量。对于 `bmw_finder`
    分类器打印这些信息，你能对分类器的性能有什么了解？**解答**：我们不仅跟踪正确和错误的预测，还可以分别跟踪真正的正例、真正的负例、错误的正例和错误的负例：'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '❶ We now have 4 counters to keep track of.❷ Specifies whether to print the
    data (we might not want to print it every time).❸ Depending on whether the car
    is a Prius or BMW and whether it’s classified correctly, increments 1 of 4 counters❹
    Prints the results of each counter❺ Returns the number of correct classifications
    (true positives or negatives) divided by the length of the data setFor the `bmw_finder`
    function, this prints the following text:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们现在有 4 个计数器来跟踪。❷ 指定是否打印数据（我们可能不想每次都打印）。❸ 根据汽车是普锐斯还是宝马以及它是否被正确分类，增加 4 个计数器中的一个❹
    打印每个计数器的结果❺ 返回正确分类的数量（真正的正例或负例）除以数据集的长度。对于 `bmw_finder` 函数，这会打印以下文本：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Because the classifier returns no false positives, this tells us it always correctly
    identifies when the car is *not* a BMW. But we can’t be too proud of our function
    yet, because it says most of the cars are not BMWs, including many that are! In
    the next exercise, you can relax the constraint to get a higher overall success
    rate. |
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因为分类器没有返回任何误报，这告诉我们它总是正确地识别出汽车不是宝马。但我们还不能过于自豪我们的函数，因为它说大部分的汽车都不是宝马，包括很多确实是宝马的汽车！在下一个练习中，你可以放宽限制以获得更高的整体成功率。|
- en: '| **Exercise 15.2**: Find a way to update the `bmw_finder` function to improve
    its performance and use the `test_classifier` function to confirm that your improved
    function has better than 59% accuracy.**Solution**: If you solved the last exercise,
    you saw that `bmw_finder` was too aggressive in saying that cars were not BMWs.
    We can lower the price threshold to $20,000 and see if it makes a difference:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习 15.2**：找到一种方法来更新 `bmw_finder` 函数以提高其性能，并使用 `test_classifier` 函数来确认你的改进函数的准确率超过
    59%。**解答**：如果你解决了上一个练习，你会看到 `bmw_finder` 在说汽车不是宝马时过于激进。我们可以降低价格阈值到 $20,000 看看是否有所改变：'
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Indeed, by lowering this threshold, `bmw_finder` improved the success rate
    to 73.5%:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '| 确实，通过降低这个阈值，`bmw_finder` 提高了成功率到 73.5%：'
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 15.2 Picturing a decision boundary
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 描绘决策边界
- en: Before we implement the logistic regression function, let’s look at one more
    way to measure our success at classification. Because two numbers, mileage and
    price, define our used car data points, we can think of these as 2D vectors and
    plot them as points on a 2D plane. This plot gives us a better sense of where
    our classification function “draws the line” between BMWs and Priuses, and we
    can see how to improve it. It turns out that using our `bmw_finder` function is
    equivalent to drawing a literal line in the 2D plane, calling any point above
    the line a BMW and any point below it a Prius.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现逻辑回归函数之前，让我们看看另一种衡量我们在分类中成功的方法。因为里程和价格这两个数字定义了我们的二手车数据点，我们可以把它们看作是二维向量，并将它们绘制在二维平面上作为点。这个图让我们更好地了解我们的分类函数在宝马和普锐斯之间“划线”的位置，我们可以看到如何改进它。结果发现，使用我们的
    `bmw_finder` 函数相当于在二维平面上画一条实际的线，任何在线上方的点被称作宝马，任何在下方的不被称作宝马。
- en: In this section, we use Matplotlib to draw our plot and see where `bmw_finder`
    places the dividing line between BMWs and Priuses. This line is called the *decision
    boundary*, because what side of the line a point lies on, helps us decide what
    class it belongs to. After looking at the car data on a plot, we can figure out
    where to draw a better dividing line. This lets us define an improved version
    of the `bmw_finder` function, and we can measure exactly how much better it performs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用Matplotlib绘制我们的图表，并查看`bmw_finder`在宝马和普锐斯之间放置的分割线。这条线被称为**决策边界**，因为一个点位于线的哪一侧，有助于我们决定它属于哪个类别。在图表上查看汽车数据后，我们可以找出绘制更好分割线的地方。这使得我们可以定义`bmw_finder`函数的改进版本，并可以精确地测量它的性能提升。
- en: 15.2.1 Picturing the space of cars
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 想象汽车的空间
- en: All of the cars in our data set have mileage and price values, but some of them
    represent BMWs and some represent Priuses, depending on whether they are labeled
    with a 1 or with a 0\. To make our plot readable, we want to make a BMW and a
    Prius visually distinct on the scatter plot.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的所有汽车都有里程和价格值，但其中一些代表宝马，一些代表普锐斯，这取决于它们是否被标记为1或0。为了使我们的图表易于阅读，我们希望在散点图上使宝马和普锐斯在视觉上明显不同。
- en: '![](../Images/CH15_F02_Orland.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F02_Orland.png)'
- en: Figure 15.2 A plot of price vs. mileage for all cars in the data set with each
    BMW represented by an X and each Prius represented with a circle
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2数据集中所有汽车的售价与里程对比图，其中每个宝马用X表示，每个普锐斯用圆圈表示
- en: The `plot_data` helper function in the source code takes the whole list of car
    data and automatically plots the BMWs with X’s and the Priuses with circles. Figure
    15.2 shows the plot.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码中的`plot_data`辅助函数接受整个汽车数据列表，并自动用X标记宝马，用圆圈标记普锐斯。图15.2显示了该图表。
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In general, we can see that the BMWs are more expensive than the Priuses; most
    BMWs are higher on the price axis. This justifies our strategy of classifying
    the more expensive cars as BMWs. Specifically, we drew the line at a price of
    $25,000 (figure 15.3). On the plot, this line separates the top of the plot with
    more expensive cars from the bottom with less expensive cars.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可以看到宝马比普锐斯更贵；大多数宝马在价格轴上更高。这证明了我们将更贵的汽车分类为宝马的策略是合理的。具体来说，我们在25,000美元的价格上画了这条线（图15.3）。在图表上，这条线将图表上更贵的汽车顶部与更便宜的汽车底部分开。
- en: '![](../Images/CH15_F03_Orland.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F03_Orland.png)'
- en: Figure 15.3 Shows the decision line with car data plotted
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3展示了绘制了汽车数据的决策线
- en: This is our decision boundary. Every X above the line was correctly identified
    as a BMW, while every circle below the line was correctly identified as a Prius.
    All other points were classified incorrectly. It’s clear that if we move this
    decision boundary, we can improve our accuracy. Let’s give it a try.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们决策边界。线上的每一个X都被正确地识别为宝马，而线下的每一个圆都被正确地识别为普锐斯。所有其他点都被错误地分类。很明显，如果我们移动这个决策边界，我们可以提高我们的准确度。让我们试一试。
- en: 15.2.2 Drawing a better decision boundary
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 绘制更好的决策边界
- en: Based on the plot in figure 15.3, we could lower the line and correctly identify
    a few more BMWs, while not incorrectly identifying any Priuses. Figure 15.4 shows
    what the decision boundary looks like if we lower the cut-off price to $21,000.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图15.3中的图表，我们可以降低线并正确识别更多的宝马，同时不会错误地识别任何普锐斯。图15.4显示了如果我们把截止价格降低到21,000美元，决策边界看起来会是什么样子。
- en: '![](../Images/CH15_F04_Orland.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F04_Orland.png)'
- en: Figure 15.4 Lowering the decision boundary line appears to increase our accuracy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4降低决策边界线似乎提高了我们的准确度。
- en: The $21,000 cut-off might be a good boundary for low-mileage cars, but the higher
    the mileage, the lower the threshold. For instance, it looks like most BMWs with
    75,000 miles or more are below $21,000\. To model this, we can make our cut-off
    price *mileage dependent*. Geometrically that means drawing a line that slopes
    downward (figure 15.5).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 21,000美元的截止点可能适合低里程汽车，但里程越高，阈值越低。例如，看起来大多数75,000英里或以上的宝马价格都低于21,000美元。为了建模这一点，我们可以使我们的截止价格与里程相关。从几何上讲，这意味着绘制一条向下倾斜的线（图15.5）。
- en: '![](../Images/CH15_F05_Orland.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F05_Orland.png)'
- en: Figure 15.5 Using a downward-sloping decision boundary
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 使用向下倾斜的决策边界
- en: This line is given by the function *p*(*x*) = 21,000 − 0.07 · *x*, where *p*
    is price and *x* is mileage. There is nothing special about this equation; I just
    played around with the numbers until I plotted a line that looked reasonable.
    But it looks like it correctly identifies even more BMWs than before, with only
    a handful of false positives (Priuses incorrectly classified as BMWs). Rather
    than just eyeballing these decision boundaries, we can turn them into classifier
    functions and measure their performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线由函数*p*(x) = 21,000 − 0.07 · *x*给出，其中*p*是价格，*x*是里程。这个方程式没有什么特别之处；我只是随意调整数字，直到我绘制出一条看起来合理的线。但它看起来甚至可以正确识别比以前更多的宝马车，只有少数误判（将普锐斯错误地分类为宝马）。与其只是凭直觉判断这些决策边界，我们不如将它们转换成分类函数并衡量它们的性能。
- en: 15.2.3 Implementing the classification function
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 实现分类函数
- en: 'To turn this decision boundary into a classification function, we need to write
    a Python function that takes a car mileage and price, and returns one or zero
    depending on whether the point falls above or below the line. That means taking
    the given mileage, plugging it into the decision boundary function, *p*(*x*),
    to see what the threshold price is and comparing the result to the given price.
    This is what it looks like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个决策边界转换成一个分类函数，我们需要编写一个Python函数，它接受汽车里程和价格作为参数，并根据该点是否位于直线之上或之下返回一个或零。这意味着需要将给定的里程值插入到决策边界函数*p*(x)中，以查看阈值价格是多少，并将结果与给定的价格进行比较。这看起来是这样的：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Testing this out, we can see it is much better than our first classifier; 80.5%
    of the cars are correctly classified by this line. Not bad!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过测试，我们可以看到它比我们的第一个分类器要好得多；80.5%的汽车被这条线正确分类。不错！
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You might ask why we can’t just do a gradient descent on the parameters defining
    the decision boundary line. If 20,000 and 0.07 don’t give the most accurate decision
    boundary, maybe some pair of numbers near them do. This isn’t a crazy idea. When
    we implement logistic regression, you’ll see that under the hood, it moves the
    decision boundary around using gradient descent until it finds the best one.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问为什么我们不能直接对定义决策边界线的参数进行梯度下降。如果20,000和0.07不能给出最准确的决策边界，也许它们附近的某个数字对可以。这不是一个疯狂的想法。当我们实现逻辑回归时，你将看到在底层，它使用梯度下降移动决策边界，直到找到最佳位置。
- en: There are two important reasons we’ll implement the more sophisticated logistic
    regression algorithm rather than doing a gradient descent on the parameters *a*
    and *b* of the decision boundary function, *ax* + *b*. The first is that if the
    decision boundary is close to vertical at any step in the gradient descent, the
    numbers *a* and *b* could get very large and cause numerical issues. The other
    is that there isn’t an obvious cost function. In the next section, we see how
    logistic regression takes care of both of these issues so we can search for the
    best decision boundary using gradient descent.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现更复杂的逻辑回归算法，而不是对决策边界函数*ax* + *b*的参数*a*和*b*进行梯度下降，有两个重要的原因。第一个原因是，如果在梯度下降的任何步骤中决策边界接近垂直，*a*和*b*的值可能会变得非常大，导致数值问题。另一个原因是没有明显的成本函数。在下一节中，我们将看到逻辑回归如何处理这两个问题，以便我们可以使用梯度下降来寻找最佳决策边界。
- en: 15.2.4 Exercises
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.4 练习
- en: '| **Exercise 15.3-Mini Project**: What is the decision boundary of the form
    *p* = *constant* that gives the best classification accuracy on the test data
    set?**Solution**: The following function builds a classifier function for any
    specified, constant cut-off price. In other words, the resulting classifier returns
    true if the test car has price above the cutoff and false otherwise:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习15.3-迷你项目**：哪种形式的决策边界*p* = *constant*在测试数据集上给出最佳的分类准确率？**解答**：以下函数为任何指定的、常数的截止价格构建一个分类器函数。换句话说，生成的分类器如果测试汽车的价格高于截止值则返回true，否则返回false：'
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The accuracy of this function can be measured by passing the resulting classifier
    to the `test_classify` function. Here’s a helper function to automate this check
    for any price we want to test as a cut-off value:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的准确性可以通过将生成的分类器传递给`test_classify`函数来衡量。这里有一个辅助函数，可以自动检查我们想要测试的任何价格作为截止值的情况：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The best cut-off price is between two of the prices in our list. It’s sufficient
    to check each price and see if it is the best cut-off price. We can do that quickly
    in Python using the `max` function. The keyword argument `key` lets us choose
    what function we want to maximize by. In this case, we want to find the price
    in the list that is the best cut-off, so we can maximize by the `cutoff_accuracy`
    function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的截断价格位于我们列表中的两个价格之间。检查每个价格并查看它是否是最佳截断价格就足够了。我们可以使用 Python 中的 `max` 函数快速做到这一点。关键字参数
    `key` 允许我们选择通过哪个函数来最大化。在这种情况下，我们想要找到列表中最佳的截断价格，因此我们可以通过 `cutoff_accuracy` 函数来最大化：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This tells us that according to our data set, $17,998 is the best price to
    use as a cut-off when deciding whether a car is a BMW 5 series or a Prius. It
    turns out to be quite accurate for our data set, with 79.5% accuracy:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，根据我们的数据集，$17,998 是决定汽车是宝马 5 系列还是普锐斯时作为截断的最佳价格。对于我们的数据集来说，它相当准确，准确率为 79.5%：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 15.3 Framing classification as a regression problem
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 将分类问题作为回归问题来处理
- en: The way that we can reframe our classification task as a regression problem
    is to create a function that takes in the mileage and price of a car, and returns
    a number measuring how likely it is to be a BMW instead of a Prius. In this section,
    we implement a function called `logistic_classifier` that, from the outside, looks
    a lot like the classifiers we’ve built so far; it takes a mileage and a price,
    and outputs a number telling us whether the car is a BMW or a Prius. The only
    difference is that rather than outputting one or zero, it outputs a value between
    zero and one, telling us how likely it is that the car is a BMW.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的分类任务重新构造成回归问题的方法是通过创建一个函数，该函数接受汽车的里程和价格作为输入，并返回一个数字，衡量它成为宝马而不是普锐斯的可能性。在本节中，我们实现了一个名为
    `logistic_classifier` 的函数，从外部看，它与我们迄今为止构建的分类器非常相似；它接受里程和价格，并输出一个数字，告诉我们汽车是宝马还是普锐斯。唯一的区别是，它不是输出一个或零，而是输出一个介于零和一之间的值，告诉我们汽车是宝马的可能性有多大。
- en: You can think of this number as the probability that the mileage and price describe
    a BMW, or more abstractly, you can think of it as giving the “BMWness” of the
    data point (figure 15.6). (Yes, that’s a made-up word, which I pronounce “bee-em-doubleyou-ness.”
    It means how much it looks like a BMW. Maybe we could call the antonym “Priusity.”)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个数字看作是里程和价格描述的是宝马的概率，或者更抽象地说，你可以将其视为给出数据点的“宝马特性”（图 15.6）。（是的，这是一个虚构的词，我读作“bee-em-doubleyou-ness。”它的意思是有多像宝马。也许我们可以将反义词称为“普锐斯性”。）
- en: '![](../Images/CH15_F06_Orland.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F06_Orland.png)'
- en: Figure 15.6 The concept of “BMWness” describes how much like a BMW a point in
    the plane is.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 “宝马特性”的概念描述了平面上一个点有多像宝马。
- en: To build the logistic classifier, we start with a guess of a good decision boundary
    line. Points above the line have high “BMWness,” meaning these are likely to be
    BMWs and the logistic function should return values close to one. Data points
    below the line have a low “BMWness,” meaning these are more likely to be Priuses
    and our function should return values close to zero. On the decision boundary,
    the “BMWness” value will be 0.5, meaning a data point there is equally as likely
    to be a BMW as it is to be a Prius.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建逻辑分类器，我们从一个好的决策边界线的猜测开始。位于线上的点具有高的“宝马特性”，意味着这些点很可能是宝马，逻辑函数应该返回接近一的值。位于线下的数据点具有低的“宝马特性”，意味着这些点更有可能是普锐斯，我们的函数应该返回接近零的值。在决策边界上，“宝马特性”值将是
    0.5，这意味着该点的宝马和普锐斯的概率是相等的。
- en: 15.3.1 Scaling the raw car data
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 缩放原始汽车数据
- en: There’s *a* chore we need to take care of at some point in the regression process,
    so we might as well take care of it now. As we discussed in the last chapter,
    the large values of mileage and price can cause numerical errors, so it’s better
    to rescale them to a small, consistent size. We should be safe if we scale all
    of the mileages and the prices linearly to values between zero and one.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归过程中，我们迟早需要处理一项任务，所以现在处理它也无妨。正如我们在上一章中讨论的，里程和价格的大数值可能会引起数值错误，所以最好将它们缩放到一个小的、一致的大小。如果我们将所有里程和价格线性缩放到零到一之间的值，我们应该是安全的。
- en: 'We need to be able to scale and unscale each of mileage and price, so we need
    four functions in total. To make this a little bit less painful, I’ve written
    a helper function that takes a list of numbers and returns functions to scale
    and unscale these linearly, between zero and one, using the maximum and minimum
    values in the list. Applying this helper function to the whole list of mileages
    and of prices gives us the four functions we need:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够缩放和未缩放里程和价格中的每一个，因此我们需要总共四个函数。为了使这个过程稍微不那么痛苦，我编写了一个辅助函数，它接受一个数字列表并返回用于将这些数字线性缩放和未缩放到零和一之间的函数，使用列表中的最大和最小值。将此辅助函数应用于里程和价格的整个列表，我们得到了所需的四个函数：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The maximum and minimum provide the current range of the data set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最大值和最小值提供了当前数据集的范围。
- en: ❷ Puts the data point at the same fraction of the way between 0 and 1 as it
    was from min_val to max_val
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据点放置在 0 到 1 之间的相同分数位置，就像它在 min_val 到 max_val 之间一样
- en: ❸ Puts the scaled data point at the same fraction of the way from min_val to
    max_val as it was from 0 to 1
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将缩放数据点放置在 min_val 到 max_val 之间的相同分数位置，就像它在 0 到 1 之间一样
- en: ❹ Returns the scale and unscale functions (closures, if you’re familiar with
    that term) to use when we want to scale or unscale members of this data set.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回用于缩放或未缩放此数据集成员时的缩放和未缩放函数（如果你熟悉这个术语，则是闭包）。
- en: ❺ Returns two sets of functions, one for price and one for mileage
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回两组函数，一组用于价格，另一组用于里程
- en: 'We can now apply these scaling functions to every car data point in our list
    to get a scaled version of the data set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些缩放函数应用于我们列表中的每个汽车数据点，以获得数据集的缩放版本：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The good news is that the plot looks the same (figure 15.7), except that the
    values on the axes are different.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，图表看起来相同（图15.7），只是坐标轴上的数值不同。
- en: '![](../Images/CH15_F07_Orland.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F07_Orland.png)'
- en: Figure 15.7 The mileage and price data scaled so that all values are between
    zero and one. The plot looks the same as before, but our risk of numerical error
    has decreased.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 将里程和价格数据缩放，使所有值都在零和一之间。图表看起来与之前相同，但我们的数值误差风险降低了。
- en: Because the geometry of the scaled data set is the same, it should give us confidence
    that a good decision boundary for this scaled data set translates to a good decision
    boundary for the original data set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因为缩放数据集的几何形状相同，这应该让我们有信心，这个缩放数据集的良好决策边界将转化为原始数据集的良好决策边界。
- en: 15.3.2 Measuring the “BMWness” of a car
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 测量汽车的“宝马”程度
- en: Let’s start with a decision boundary that looks similar to the one from the
    last section. The function *p*(*x*) = 0.56 − 0.35 · *x* gives price at the decision
    boundary as a function of mileage. This is pretty close to the one I found by
    eyeballing in the last section, but it applies to the scaled data set instead
    (figure 15.8).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从与上一节中相似的决策边界开始。函数 *p*(*x*) = 0.56 − 0.35 · *x* 给出了决策边界上价格作为里程的函数。这非常接近我在上一节中通过目测找到的，但它适用于缩放后的数据集（图15.8）。
- en: '![](../Images/CH15_F08_Orland.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F08_Orland.png)'
- en: Figure 15.8 The decision boundary *p*(*x*) = 0.56 − 0.35 · *x* on the scaled
    data set
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 缩放数据集上的决策边界 *p*(*x*) = 0.56 − 0.35 · *x*
- en: We can still test classifiers on the scaled data set with our `test_classifier`
    function; we just need to take care to pass in the scaled data instead of the
    original. It turns out this decision boundary gives us a 78.5% accurate classification
    of the data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以使用我们的 `test_classifier` 函数在缩放后的数据集上测试分类器；我们只需要确保传入缩放后的数据而不是原始数据。结果发现这个决策边界给我们提供了78.5%准确度的数据分类。
- en: It also turns out that this decision boundary function can be rearranged to
    give a measure of the “BMWness” of a data point. To make our algebra easier, let’s
    write the decision boundary as
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个决策边界函数可以被重新排列，以给出数据点的“宝马”程度的度量。为了使我们的代数更简单，让我们将决策边界写成
- en: '*p* = *ax* + *b*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = *ax* + *b*'
- en: 'where *p* is price, *x* is still mileage, and *a* and *b* are the slope and
    intercept of the line (in this case, *a* = -0.35 and *b* = 0.56), respectively.
    Instead of thinking of this as a function, we can think of it as an equation satisfied
    by points (*x*, *p*) on the decision boundary. If we subtract *ax* + *b* from
    both sides of the equation, we get another correct equation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是价格，*x* 仍然是里程，而 *a* 和 *b* 是直线的斜率和截距（在这种情况下，*a* = -0.35 和 *b* = 0.56），分别。我们不必将其视为函数，我们可以将其视为满足决策边界上点
    (*x*, *p*) 的方程。如果我们从方程的两边减去 *ax* + *b*，我们得到另一个正确的方程：
- en: '*p* − *ax* − *b* = 0'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* − *ax* − *b* = 0'
- en: Every point (*x*, *p*) on the decision boundary satisfies this equation as well.
    In other words, the quantity *p* − *ax* − *b* is zero for every point on the decision
    boundary.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界上的每一个点 (*x*, *p*) 都满足这个方程。换句话说，对于决策边界上的每一个点，*p* − *ax* − *b* 的值都是零。
- en: 'Here’s the point of this algebra: the quantity *p* − *ax* − *b* is a measure
    of the “BMWness” of the point (*x*, *p*). If (*x*, *p*) is above the decision
    boundary, it means *p* is too big, relative to *x*, so *p* − *ax* − *b* > 0\.
    If, instead, (*x*, *p*) is below the decision boundary, it means *p* is too small
    relative to *x*, then *p* − *ax* − *b* < 0\. Otherwise, the expression *p* − *ax*
    − *b* is exactly zero, and the point is right at the threshold of being interpreted
    as a Prius or a BMW. This might be a little bit abstract on the first read, so
    table 15.2 lists the three cases.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代数的关键在于，*p* − *ax* − *b* 是点 (*x*, *p*) 的“宝马度”的度量。如果 (*x*, *p*) 在决策边界之上，这意味着相对于
    *x*，*p* 太大，所以 *p* − *ax* − *b* > 0。相反，如果 (*x*, *p*) 在决策边界之下，这意味着相对于 *x*，*p* 太小，那么
    *p* − *ax* − *b* < 0。否则，表达式 *p* − *ax* − *b* 精确为零，点正好位于将解释为普锐斯或宝马的阈值。这可能在第一次阅读时有点抽象，所以表15.2列出了三种情况。
- en: Table 15.2 Summary of the possible cases
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.2 可能情况的总结
- en: '| (*x*, *p*) above decision boundary | *p* − *ax* − *b* > 0 | Likely to be
    a BMW |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| (*x*, *p*) 在决策边界之上 | *p* − *ax* − *b* > 0 | 很可能是一辆宝马 |'
- en: '| (*x*, *p*) on decision boundary | *p* − *ax* − *b* = 0 | Could be either
    car model |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| (*x*, *p*) 在决策边界上 | *p* − *ax* − *b* = 0 | 可能是任何车型 |'
- en: '| (*x*, *p*) below decision boundary | *p* − *ax* − *b* < 0 | Likely to be
    a Prius |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| (*x*, *p*) 在决策边界之下 | *p* − *ax* − *b* < 0 | 很可能是一辆普锐斯 |'
- en: If you’re not convinced that *p* − *ax* − *b* is a measure of “BMWness” compatible
    with the decision boundary, an easier way to see this is to look at the heat map
    of *f*(*x*, *p*) = *p* − *ax* − *b*, together with the data (figure 15.9). When
    *a* = -0.35 and *b =* 0.56, the function is *f*(*x*, *p*) = *p* − 0.35 · *x* −
    0.56.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有确信 *p* − *ax* − *b* 是与决策边界兼容的“宝马度”的度量，一个更简单的方法是查看 *f*(*x*, *p*) = *p*
    − *ax* − *b* 的热图，以及数据（图15.9）。当 *a* = -0.35 和 *b =* 0.56 时，函数是 *f*(*x*, *p*) =
    *p* − 0.35 · *x* − 0.56。
- en: '![](../Images/CH15_F09_Orland.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F09_Orland.png)'
- en: Figure 15.9 A plot of the heatmap and decision boundary showing that the bright
    values (positive “BMWness”) are above the decision boundary and dark values (negative
    “BMWness”) occur below the decision boundary
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 展示了热图和决策边界的图，显示明亮的值（正“宝马度”）位于决策边界之上，而暗的值（负“宝马度”）出现在决策边界之下
- en: The function, *f*(*x*, *p*), *almost* meets our requirements. It takes a mileage
    and a price, and it outputs a number that is higher if the numbers are likely
    to represent a BMW, and lower if the values are likely to represent a Prius. The
    only thing missing is that the output numbers aren’t constrained to be between
    zero and one, and the cutoff is at a value of zero rather than at a value of 0.5
    as desired. Fortunately, there’s a handy kind of mathematical helper function
    we can use to adjust the output.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f*(*x*, *p*) 几乎满足我们的要求。它接受里程和价格作为输入，并输出一个数字，如果这个数字可能代表宝马车，则数值较高；如果数值可能代表普锐斯，则数值较低。唯一缺少的是输出数字没有限制在零到一之间，截止值在零而不是期望的0.5。幸运的是，有一个方便的数学辅助函数我们可以用来调整输出。
- en: 15.3.3 Introducing the sigmoid function
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.3 介绍 Sigmoid 函数
- en: 'The function *f*(*x*, *p*) = *p* − *ax* − *b* is linear, but this is not a
    chapter on linear regression! The topic at hand is *logistic regression*, and
    to do logistic regression, you need to use a logistic function. The most basic
    logistic function is the one that follows, which is often called a *sigmoid* function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f*(*x*, *p*) = *p* − *ax* − *b* 是线性的，但这不是关于线性回归的章节！当前的主题是 *逻辑回归*，要进行逻辑回归，你需要使用逻辑函数。最基本的逻辑函数如下，通常称为
    *Sigmoid* 函数：
- en: '![](../Images/CH15_F09_Orland_EQ01.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F09_Orland_EQ01.png)'
- en: 'We can implement this function in Python with the `exp` function, which stands
    in for *ex*, where *e* = 2.71828... and is the constant we’ve used for exponential
    bases before:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Python 中的 `exp` 函数来实现这个函数，它代表 *ex*，其中 *e* = 2.71828... 是我们之前用于指数底数的常数：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Figure 15.10 shows its graph.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10展示了其图像。
- en: '![](../Images/CH15_F10_Orland.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F10_Orland.png)'
- en: Figure 15.10 The graph of the sigmoid function σ(*x*)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10 Sigmoid 函数 σ(*x*) 的图像
- en: 'In the function, we use the Greek letter σ (sigma) because σ is the Greek version
    of the letter *S*, and the graph of σ(*x*) looks a bit like the letter *S*. Sometimes
    the words *logistic function* and *sigmoid function* are used interchangeably
    to mean a function like the one in figure 15.10, which smoothly ramps up from
    one value to another. In this chapter (and the next), when I refer to the sigmoid
    function, I’ll be talking about this specific function: σ(*x*).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们使用希腊字母σ（西格玛），因为σ是字母*S*的希腊版本，σ(*x*)的图形看起来有点像字母*S*。有时“逻辑函数”和“S形函数”这两个词可以互换使用，指的是像图15.10中的那种函数，它从一个值平滑地过渡到另一个值。在本章（以及下一章）中，当我提到S形函数时，我会谈论这个特定的函数：σ(*x*)。
- en: You don’t need to worry too much about how this function is defined, but you
    do need to understand the shape of the graph and what it means. This function
    sends any input number to a value between zero and one, with big negative numbers
    yielding results closer to zero, and big positive numbers yielding results closer
    to one. The result of σ(0) is 0.5\. We can think of σ as translating the range
    from -∞ to ∞ to the more manageable range from zero to one.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要过于担心这个函数是如何定义的，但你确实需要理解图形的形状及其含义。这个函数将任何输入数字映射到0到1之间的一个值，大负数产生接近0的结果，而大正数产生接近1的结果。σ(0)的结果是0.5。我们可以把σ看作是将从-∞到∞的范围转换到更易管理的从0到1的范围。
- en: 15.3.4 Composing the sigmoid function with other functions
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.4 将S形函数与其他函数组合
- en: Returning to our function *f*(*x*, *p*) = *p* − *ax* − *b*, we saw that it takes
    a mileage value and a price value and returns a number measuring how much the
    values look like a BMW rather than a Prius. This number could be large or positive
    or negative, and a value of zero indicates that it is on the boundary between
    being a BMW and being a Prius.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的函数*f*(*x*, *p*) = *p* − *ax* − *b*，我们看到了它接受一个里程值和一个价格值，并返回一个衡量这些值看起来像宝马而不是普锐斯的数字。这个数字可以是大的、正的或负的，而零值表示它位于宝马和普锐斯之间的边界上。
- en: What we want our function to return is a value between zero and one (with values
    close to zero and one), representing cars likely to be Priuses or BMWs, respectively,
    and a value of 0.5, representing a car that is equally likely to be either a Prius
    or a BMW. All we have to do to adjust the outputs of *f*(*x*, *p*) to be in the
    expected range is to pass through the sigmoid function σ(*x*) as shown in figure
    15.11\. That is, the function we want is σ(*f*(*x*, *p*)), where *x* and *p* are
    the mileage and price.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的函数返回一个介于0到1之间的值（接近0和1的值），分别代表可能是普锐斯或宝马的汽车，而0.5的值表示一辆汽车有同等可能性是普锐斯或宝马。我们只需要调整*f*(*x*,
    *p*)的输出，使其处于预期的范围内，就像图15.11中所示的那样通过S形函数σ(*x*)。也就是说，我们想要的函数是σ(*f*(*x*, *p*))，其中*x*和*p*是里程和价格。
- en: '![](../Images/CH15_F11_Orland.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F11_Orland.png)'
- en: Figure 15.11 Schematic diagram of composing the “BMWness” function *f*(*x*,
    *p*) with the sigmoid function σ(*x*)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11 “BMWness”函数*f*(*x*, *p*)与S形函数σ(*x*)组合的示意图
- en: Let’s call the resulting function *L*(*x*, *p*), so in other words, *L*(*x*,
    *p*) = σ(*f*(*x*, *p*)). Implementing the function *L*(*x*, *p*) in Python and
    plotting its heatmap (figure 15.12), we can see that it increases in the same
    direction as *f*(*x*, *p*), but its values are different.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将得到的函数称为*L*(*x*, *p*)，换句话说，*L*(*x*, *p*) = σ(*f*(*x*, *p*))。在Python中实现函数*L*(*x*,
    *p*)并绘制其热图（图15.12），我们可以看到它沿着与*f*(*x*, *p*)相同的方向增加，但其值不同。
- en: '![](../Images/CH15_F12_Orland.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F12_Orland.png)'
- en: Figure 15.12 The heatmaps look basically the same, but the values of the function
    are slightly different.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12 热图看起来基本上是一样的，但函数的值略有不同。
- en: Based on this picture, you might wonder why we went through the trouble of passing
    the “BMWness” function through the sigmoid. From this perspective, the functions
    look mostly the same. However, if we plot their graphs as 2D surfaces in 3D (figure
    15.13), you can see that the curvy shape of the sigmoid has an effect.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这张图，你可能会想知道为什么我们费尽心机将“BMWness”函数通过S形函数。从这个角度看，函数看起来几乎一样。然而，如果我们将其图形作为3D空间中的2D表面来绘制（图15.13），你会发现S形函数的曲线形状有影响。
- en: '![](../Images/CH15_F13_Orland.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F13_Orland.png)'
- en: Figure 15.13 While *f*(*x*, *p*) slopes upward linearly, *L*(*x*, *p*) curves
    up from a minimum value of 0 to a maximum value of 1.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13 当*f*(*x*, *p*)线性向上倾斜时，*L*(*x*, *p*)从0的最小值曲线上升到1的最大值。
- en: In fairness, I had to zoom out a bit in (*x*, *p*) space to make the curvature
    clear. The point is that if the type of car is indicated by a 0 or 1, the values
    of the function *L*(*x*, *p*) actually come close to these numbers, whereas the
    values of *f*(*x*, *p*) go off to positive and negative infinity!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 公平起见，我不得不在 (*x*, *p*) 空间中稍微放大一些，以便使曲率清晰。重点是，如果汽车类型由 0 或 1 表示，函数 *L*(*x*, *p*)
    的值实际上接近这些数字，而 *f*(*x*, *p*) 的值则趋向于正负无穷大！
- en: Figure 15.14 illustrates two exaggerated diagrams to show you what I mean. Remember
    that in our data set, scaled_car_data, we represented Priuses as triples of the
    form (mileage, price, 0) and BMWs as triples of the form (mileage, price, 1).
    We can interpret these as points in 3D where the BMWs live in the plane *z* =
    1 and Priuses live in the plane *z* = 0\. Plotting scaled_car_data as a 3D scatter
    plot, you can see that a linear function can’t come close to many of the data
    points in the same way as a logistic function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14 展示了两个夸张的图来展示我的意思。记住，在我们的数据集 scaled_car_data 中，我们用 (mileage, price, 0)
    形式的三元组来表示普锐斯，用 (mileage, price, 1) 形式的三元组来表示宝马。我们可以将这些解释为 3D 中的点，其中宝马位于 *z* =
    1 的平面上，而普锐斯位于 *z* = 0 的平面上。将 scaled_car_data 作为 3D 散点图绘制，你可以看到线性函数无法像逻辑函数那样接近许多数据点。
- en: With functions shaped like *L*(*x*, *p*), we can actually hope to *fit* the
    data, and we’ll see how to do that in the next section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于形状像 *L*(*x*, *p*) 的函数，我们实际上可以希望拟合数据，我们将在下一节中看到如何做到这一点。
- en: '![](../Images/CH15_F14_Orland.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F14_Orland.png)'
- en: Figure 15.14 The graph of a linear function in 3D can’t come as close to the
    data points as the graph of a logistic function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14 3D 中线性函数的图形无法像逻辑函数的图形那样接近数据点。
- en: 15.3.5 Exercises
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.5 练习
- en: '| **Exercise 15.4**: Find a function *h*(*x*) such that large positive values
    of *x* cause *h*(*x*) to be close to 0, large negative values of *x* cause *h*(*x*)
    to be close to 1, and *h*(3) = 0.5.**Solution**: The function *y*(*x*) = 3 − *x*
    has *y*(3) = 0 and it goes off to positive infinity when *x* is large and negative
    and off to negative infinity when *x* is large and posi-tive. That means passing
    the result of *y*(*x*) into our sigmoid function gives us a function with the
    desired properties. Specifically, *h*(*x*) = σ(*y*(*x*)) = σ(3 − *x*) works, and
    its graph is shown here to convince you:![](../Images/CH15_F14_Orland_UN01.png)
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.4**: 找到一个函数 *h*(*x*)，使得当 *x* 的正值很大时，*h*(*x*) 接近 0，当 *x* 的负值很大时，*h*(*x*)
    接近 1，并且 *h*(3) = 0.5。**解答**: 函数 *y*(*x*) = 3 − *x* 在 *y*(3) = 0 时成立，并且当 *x* 很大且为负时，它趋向于正无穷，当
    *x* 很大且为正时，它趋向于负无穷。这意味着将 *y*(*x*) 的结果传递到我们的 sigmoid 函数中，可以得到具有所需特性的函数。具体来说，*h*(*x*)
    = σ(*y*(*x*)) = σ(3 − *x*) 是有效的，其图形如下以说服你：![](../Images/CH15_F14_Orland_UN01.png)
    |'
- en: '| **Exercise 15.5−Mini Project**: There is actually a lower bound on the result
    of *f*(*x*, *p*) because *x* and *p* are not allowed to be negative (negative
    mileages and prices don’t make sense, after all). Can you figure out the lowest
    value of *f* that a car could produce?**Solution**: According to the heatmap,
    the function *f*(*x*, *p*) gets smaller as we go down and to the left. The equation
    confirms this as well; if we decrease *x* or *p*, the value of *f* = *p* − *ax*
    − *b* = *p* + 0.35 · *x* − 0.56 gets smaller. Therefore, the minimum value of
    *f*(*x*, *p*) occurs at (*x*, *p*) = (0, 0), and it’s *f*(0, 0) = -0.056. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.5-迷你项目**: 实际上，*f*(*x*, *p*) 的结果有一个下限，因为 *x* 和 *p* 不允许是负数（毕竟，负里程和价格没有意义）。你能找出汽车可能产生的
    *f* 的最低值吗？**解答**: 根据热图，函数 *f*(*x*, *p*) 随着我们向下和向左移动而减小。方程也证实了这一点；如果我们减小 *x* 或
    *p*，*f* = *p* − *ax* − *b* = *p* + 0.35 · *x* − 0.56 的值会减小。因此，*f*(*x*, *p*) 的最小值发生在
    (*x*, *p*) = (0, 0)，并且它为 *f*(0, 0) = -0.056。 |'
- en: 15.4 Exploring possible logistic functions
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 探索可能的逻辑函数
- en: Let’s quickly retrace our steps. Plotting the mileages and prices of our set
    of Priuses and BMWs on a scatter plot, we could try to draw a line between these
    values, called a decision boundary, that defines a rule by which to distinguish
    a Prius from a BMW. We wrote our decision boundary as a line in the form *p*(*x*)
    = *ax* + *b*, and it looked like -0.35 and 0.56 were reasonable choices for *a*
    and *b*, giving us a classification that was about 80% correct.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下步骤。在散点图上绘制我们的一组普锐斯和宝马的里程和价格，我们可以尝试在这些值之间画一条线，称为决策边界，它定义了一个区分普锐斯和宝马的规则。我们将决策边界写成形式
    *p*(*x*) = *ax* + *b* 的线，看起来 -0.35 和 0.56 是 *a* 和 *b* 的合理选择，这给我们带来了大约 80% 正确的分类。
- en: Rearranging this function, we found that *f*(*x*, *p*) = *p* − *ax* − *b* was
    a function taking a mileage and price (*x*, *p*) and returning a number that was
    greater than zero on the BMW side of the decision boundary and smaller than zero
    on the Prius side. On the decision boundary, *f*(*x*, *p*) returned zero, meaning
    a car would be equally likely to be a BMW or a Prius. Because we represent BMWs
    with a 1 and Priuses with a 0, we wanted a version of *f*(*x*, *p*) that returned
    values between zero and one, where 0.5 would represent a car equally likely to
    be a BMW or a Prius. Passing the result of *f* into a sigmoid function σ, we got
    a new function *L*(*x*, *p*) = σ(*f*(*x*, *p*)), satisfying that requirement.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列这个函数，我们发现 f(x, p) = p − ax − b 是一个接受里程和价格 (x, p) 作为输入并返回一个数字的函数，这个数字在决策边界的宝马一侧大于零，在普锐斯一侧小于零。在决策边界上，f(x,
    p) 返回零，这意味着一辆车成为宝马或普锐斯的概率是相等的。因为我们用 1 表示宝马，用 0 表示普锐斯，所以我们希望 f(x, p) 返回的值在零和一之间，其中
    0.5 表示一辆车成为宝马或普锐斯的概率相等。将 f 的结果传递给 sigmoid 函数 σ，我们得到了一个新的函数 L(x, p) = σ(f(x, p))，满足这一要求。
- en: But we don’t want the *L*(*x*, *p*) I made by eyeballing the best decision boundary−we
    want the *L*(*x*, *p*) that *best fits the data*. On our way to doing that, we’ll
    see that there are three parameters we can control to write a general logistic
    function that takes 2D vectors and returns numbers between zero and one, and also
    has a decision boundary *L*(*x*, *p*) = 0.5, which is a straight line. We’ll write
    a Python function, `make_logistic (a,b,c)`, that takes in three parameters *a*
    [,](https://www.codecogs.com/eqnedit.php?latex=%5Csigma(x)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D%250)
    *b*, and *c*, and returns the logistic function they define. As we explored a
    2D space of (*a*, *b*) pairs to choose a linear function in chapter 14, we’ll
    explore a 3D space of values (*a*, *b*, *c*) to define our logistic function (figure
    15.15).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不想用肉眼确定的最佳决策边界 L(x, p) 我制作的 L(x, p) — 我们想要的是 *best fits the data* 的 L(x,
    p)。在我们实现这一目标的过程中，我们将看到有三个参数我们可以控制，以编写一个通用的逻辑函数，它接受二维向量并返回零到一之间的数字，并且具有决策边界 L(x,
    p) = 0.5，这是一条直线。我们将编写一个 Python 函数 `make_logistic(a,b,c)`，它接受三个参数 *a*、*b* 和 *c*，并返回它们定义的逻辑函数。正如我们在第14章中探索了
    (*a*, *b*) 对的二维空间来选择线性函数一样，我们将探索 (*a*, *b*, *c*) 的三维空间来定义我们的逻辑函数（图15.15）。
- en: '![](../Images/CH15_F15_Orland.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F15_Orland.png)'
- en: Figure 15.15 Exploring a 3D space of parameter values (*a*, *b*, *c*) to define
    a function *L*(*x*, *p*)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.15 探索参数值 (*a*, *b*, *c*) 的三维空间以定义函数 L(x, p)
- en: Then we’ll create a cost function, much like the one we created for linear regression.
    The cost function, which we’ll call `logistic_cost(a,b,c)`, takes the parameters
    *a*, *b*, and *c*, which define a logistic function and produce one number, measuring
    how far the logistic function is from our car data set. The `logistic_cost` function
    needs to be implemented in such a way that the lower its value, the better the
    predictions from the associated logistic function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个成本函数，这与我们为线性回归创建的成本函数非常相似。我们将称之为 `logistic_cost(a,b,c)` 的成本函数，它接受参数
    *a*、*b* 和 *c*，这些参数定义了一个逻辑函数并产生一个数字，衡量逻辑函数与我们的汽车数据集的距离。`logistic_cost` 函数需要以这种方式实现，即其值越低，相关的逻辑函数的预测就越好。
- en: 15.4.1 Parameterizing logistic functions
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 逻辑函数的参数化
- en: The first task is to find the general form of a logistic function *L*(*x*, *p*),
    whose values range from zero to one and whose decision boundary *L*(*x*, *p*)
    = 0.5 is a straight line. We got close to this in the last section, starting with
    the decision boundary *p*(*x*) = *ax* + *b* and reverse engineering a logistic
    function from that. The only problem is that a linear function of the form *ax*
    + *b* can’t represent any line in the plane. For instance, figure 15.16 shows
    a data set where a vertical decision boundary, *x* = 0.6, makes sense. Such a
    line can’t be represented in the form *p* = *ax* + *b*, however.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是找到逻辑函数 L(x, p) 的一般形式，其值在零到一之间，其决策边界 L(x, p) = 0.5 是一条直线。我们在上一节中接近了这个目标，从决策边界
    p(x) = ax + b 开始，并从那里反向工程出一个逻辑函数。唯一的问题是，形式为 ax + b 的线性函数不能表示平面上的任何直线。例如，图15.16显示了一个数据集，其中垂直的决策边界
    x = 0.6 是有意义的。然而，这样的线不能用 p = ax + b 的形式表示。
- en: '![](../Images/CH15_F16_Orland.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F16_Orland.png)'
- en: Figure 15.16 A vertical decision boundary might make sense, but it can’t be
    represented in the form p = ax + b.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.16 垂直的决策边界可能是有意义的，但它不能以 p = ax + b 的形式表示。
- en: 'The general form of a line that does work is the one we met in chapter 7: *ax*
    + *by* = *c*. Because we’re calling our variables *x* and *p*, we’ll write *ax*
    + *bp* = c. Given an equation like this, the function *z*(*x*, *p*) = *ax* + *bp*
    − *c* is zero on the line with positive values on one side and negative values
    on the other. For us, the side of the line where *z*(*x*, *p*) is positive is
    the BMW side, and the side where *z*(*x*, *p*) is negative is the Prius side.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有效的工作线的通用形式是我们第7章遇到的形式：*ax* + *by* = *c*。因为我们把我们的变量命名为 *x* 和 *p*，我们将写作 *ax*
    + *bp* = c。给定这样的方程，函数 *z*(*x*, *p*) = *ax* + *bp* − *c* 在具有正值的这一边和负值的另一边的线上为零。对我们来说，*z*(*x*,
    *p*) 为正的线的一边是宝马的一边，而 *z*(*x*, *p*) 为负的线的一边是普锐斯的一边。
- en: 'Passing *z*(*x*, *p*) through the sigmoid function, we get a general logistic
    function *L*(*x*, *p*) = σ(*z*(*x*, *p*)), where *L*(*x*, *p*) = 0.5 on the line
    where *z*(*x*, *p*) = 0\. In other words, the function *L*(*x*, *p*) = σ(*ax*
    + *bp* − *c*) is the general form we’re looking for. This is easy to translate
    to Python, giving us a function of *a*, *b*, and *c* that returns a corresponding
    logistic function *L*(*x*, *p*) = σ(*ax* + *bp* − *c*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *z*(*x*, *p*) 通过sigmoid函数传递，我们得到一个通用的逻辑函数 *L*(*x*, *p*) = σ(*z*(*x*, *p*))，其中当
    *z*(*x*, *p*) = 0 时，*L*(*x*, *p*) = 0.5。换句话说，函数 *L*(*x*, *p*) = σ(*ax* + *bp*
    − *c*) 是我们寻找的通用形式。这很容易翻译成Python，给我们一个返回对应逻辑函数 *L*(*x*, *p*) = σ(*ax* + *bp* −
    *c*) 的 *a*，*b* 和 *c* 的函数：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The next step is to come up with a measure of how close this function comes
    to our scaled_car_data dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是提出一个衡量这个函数接近我们的缩放汽车数据集的指标。
- en: 15.4.2 Measuring the quality of fit for a logistic function
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 测量逻辑函数的拟合质量
- en: For any BMW, the `scaled_car_data` list contains an entry of the form (*x*,
    *p*, 1), and for every Prius, it contains an entry of the form (*x*, *p*, 0),
    where *x* and *p* denote (scaled) mileage and price values, respectively. If we
    apply a logistic function, *L*(*x*, *p*), to the *x* and *p* values, we’ll get
    a result between zero and one.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何宝马车，`scaled_car_data` 列表中包含一个形式为 (*x*, *p*, 1) 的条目，而对于每辆普锐斯，它包含一个形式为 (*x*,
    *p*, 0) 的条目，其中 *x* 和 *p* 分别表示（缩放后的）里程和价格值。如果我们对 *x* 和 *p* 值应用一个逻辑函数，*L*(*x*, *p*)，我们将得到一个介于零和一之间的结果。
- en: 'A simple way to measure the error or cost of the function *L* is to find how
    far off it is from the correct value, which is either zero or one. If you add
    up all of these errors, you’ll get a total value telling you how far the function
    *L*(*x*, *p*) comes from the data set. Here’s what that looks like in Python:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 测量函数 *L* 的错误或成本的一个简单方法就是找出它与正确值（要么是零，要么是一）有多远。如果你把这些错误加起来，你会得到一个总值，告诉你函数 *L*(*x*,
    *p*) 离数据集有多远。以下是Python中的样子：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This cost reports the error reasonably well, but it isn’t good enough to get
    our gradient descent to converge to a best value of *a*, *b*, and *c*. I won’t
    go into a full explanation of why this is, but I’ll try to quickly give you the
    general idea.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本报告了合理的错误，但它还不够好，不能让我们的梯度下降收敛到 *a*，*b* 和 *c* 的最佳值。我不会深入解释为什么是这样，但我将尝试快速给你一个大致的想法。
- en: Suppose we have two logistic functions, *L*[1](*x*, *p*) and *L*[2](*x*, *p*),
    and we want to compare the performance of both. Let’s say they both look at the
    same data point (*x*, *p*, 0), meaning a data point representing a Prius. Then
    let’s say *L*[1](*x*, *p*) returns 0.99, which is greater than 0.5, so it predicts
    incorrectly that the car is a BMW. The error for this point is |0-0.99| = 0.99\.
    If another logistic function, *L*[2](*x*, *p*), predicts a value of 0.999, the
    model predicts with more certainty that the car is a BMW, and is even more wrong.
    That said, the error would be only |0-0.999| = 0.999, which is not much different.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个逻辑函数，*L*[1](*x*, *p*) 和 *L*[2](*x*, *p*)，我们想要比较两者的性能。让我们假设它们都查看相同的数据点
    (*x*, *p*, 0)，这意味着一个代表普锐斯的数据点。那么，假设 *L*[1](*x*, *p*) 返回 0.99，这大于 0.5，因此它错误地预测这辆车是宝马。这个点的错误是
    |0-0.99| = 0.99。如果另一个逻辑函数 *L*[2](*x*, *p*) 预测值为 0.999，模型更有信心地预测这辆车是宝马，并且错误更大。也就是说，错误将是
    |0-0.999| = 0.999，这并没有太大的不同。
- en: '![](../Images/CH15_F17_Orland.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F17_Orland.png)'
- en: Figure 15.17 The function -lo*g*(*x*) returns big values for small inputs, and
    −log(1) = 0.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.17 函数 -lo*g*(*x*)对于小的输入返回大值，且 -log(1) = 0。
- en: It’s more appropriate to think of *L*[1] as reporting a 99% chance the data
    point represents a BMW and a 1% chance that it represents a Prius, with *L*[2]
    reporting a 99.9% chance it is a BMW and a 0.1% chance it is a Prius. Instead
    of thinking of this as a 0.09% worse Prius prediction, we should really think
    of it as being *ten times* worse! We can, therefore, think of *L*[2] as being
    ten times more wrong than *L*[1].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将*L*[1]视为报告有99%的可能性数据点代表宝马，有1%的可能性代表普锐斯，而*L*[2]报告有99.9%的可能性是宝马，有0.1%的可能性是普锐斯。与其将其视为比普锐斯预测差0.09%，我们实际上应该认为它差了十倍！因此，我们可以认为*L*[2]比*L*[1]错误十倍。
- en: We want a cost function such that if *L*(*x*, *p*) is *really sure* of the wrong
    answer, then the cost of *L* is high. To get that, we can look at the difference
    between *L*(*x*, *p*) and the wrong answer, and pass it through a function that
    makes tiny values big. For instance, *L*[1](*x*, *p*) returned 0.99 for a Prius,
    meaning it was 0.01 units from the wrong answer, while *L*[2](*x*, *p*) returned
    0.999 for a Prius, meaning it was 0.001 units from the wrong answer. A good function
    to return big values from tiny ones is −log(*x*), where log is the special natural
    logarithm function. It’s not critical that you know what the −log function does,
    only that it returns big numbers for small inputs. Figure 15.17 shows the plot
    of −log(*x*).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一个成本函数，如果*L*(*x*, *p*)对错误答案非常确定，那么*L*的成本就很高。为了达到这个目的，我们可以查看*L*(*x*, *p*)与错误答案之间的差异，并通过一个将小值放大成大值的函数。例如，*L*[1](*x*,
    *p*)对普锐斯返回了0.99，这意味着它距离错误答案有0.01个单位，而*L*[2](*x*, *p*)对普锐斯返回了0.999，这意味着它距离错误答案有0.001个单位。从小的输入返回大值的好函数是−log(*x*)，其中log是特殊的自然对数函数。你不必了解−log函数的具体作用，只需知道它对小的输入返回大数字。图15.17显示了−log(*x*)的图像。
- en: 'To familiarize yourself with −log(*x*), you can test it with some small inputs.
    For *L*[1](*x*, *p*), which was 0.01 units from the wrong answer, we get a smaller
    cost than *L*[2](*x*, *p*), which was 0.001 units from the wrong answer:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉−log(*x*)，你可以用一些小的输入对其进行测试。对于*L*[1](*x*, *p*)，它距离错误答案有0.01个单位，我们得到的成本比*L*[2](*x*,
    *p*)小，后者距离错误答案有0.001个单位：
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By comparison, if *L*(*x*, *p*) returns zero for a Prius, it would be giving
    the correct answer. That’s one unit away from the wrong answer, and −log(1) =
    0, so there is zero cost for the right answer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果*L*(*x*, *p*)对普锐斯返回零，它就会给出正确答案。这离错误答案有1个单位，所以−log(1) = 0，因此正确答案的成本为零。
- en: 'Now we’re ready to implement the `logistic_cost` function that we set out to
    create. To find the cost for a given point, we calculate how close the given logistic
    function comes to the wrong answer and then take the negative logarithm of the
    result. The total cost is the sum of the cost at every data point in the `scaled_car_data`
    data set:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备实现我们设定的`logistic_cost`函数。为了找到一个给定点的成本，我们计算给定的逻辑函数接近错误答案的程度，然后取结果的负对数。总成本是`scaled_car_data`数据集中每个数据点的成本之和：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Determines the cost of a single data point
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确定单个数据点的成本
- en: ❷ The overall cost of the logistic function is the same as before, except that
    we use the new point_cost function for each data point instead of just the absolute
    value of the error.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 逻辑函数的整体成本与之前相同，只是我们为每个数据点使用新的point_cost函数，而不是仅仅使用误差的绝对值。
- en: It turns out, we get good results if we try to minimize the `logistic_cost`
    function using gradient descent. But before we do that, let’s do a sanity check
    and confirm that `logistic_cost` returns lower values for a logistic function
    with an (obviously) better decision boundary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，如果我们尝试使用梯度下降法最小化`logistic_cost`函数，我们会得到好的结果。但在我们这样做之前，让我们进行一个合理性检查，并确认`logistic_cost`对于具有（显然）更好的决策边界的逻辑函数返回更低的值。
- en: 15.4.3 Testing different logistic functions
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.3 测试不同的逻辑函数
- en: Let’s try out two logistic functions with different decision boundaries, and
    confirm if one has an obviously better decision boundary than if it has a lower
    cost. As our two examples, let’s use *p* = 0.56 − 0.35 · *x*, my best-guess decision
    boundary, which is the same as 0.35 · *x* + 1 · *p* = 0.56, and also an arbitrarily
    selected one, say *x* + *p* = 1\. Clearly, the former is a better dividing line
    between the Priuses and the BMWs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试两个具有不同决策边界的逻辑函数，并确认一个是否比另一个具有更明显的更好决策边界，或者它是否具有更低的成本。作为我们的两个例子，让我们使用*p*
    = 0.56 − 0.35 · *x*，这是我最好的猜测决策边界，它与0.35 · *x* + 1 · *p* = 0.56相同，还有一个任意选择的，比如*x*
    + *p* = 1。显然，前者是普锐斯和宝马之间更好的分割线。
- en: 'In the source code, you’ll find a `plot_line` function to draw a line based
    on the values *a*, *b*, and *c* in the equation *ax* + *by* = *c*(and as an exercise
    at the end of the section, you can try implementing this function yourself). The
    respective values of (*a*, *b*, *c*) are (0.35, 1, 0.56) and (1, 1, 1). We can
    plot them alongside the scatter plot of car data (shown in figure 15.18) with
    these three lines:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在源代码中，你可以找到一个 `plot_line` 函数，用于根据方程 *ax* + *by* = *c* 中的值 *a*、*b* 和 *c* 绘制一条线（并且作为本节末尾的练习，你可以尝试自己实现这个函数）。相应的
    (*a*, *b*, *c*) 值是 (0.35, 1, 0.56) 和 (1, 1, 1)。我们可以用这三条线与汽车数据的散点图（如图 15.18 所示）一起绘制：
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH15_F18_Orland.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F18_Orland.png)'
- en: Figure 15.18 The graphs of two decision boundary lines. One is clearly better
    than the other at separating Priuses from BMWs.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.18 两个决策边界线的图形。其中一条在将普锐斯与宝马分开方面明显优于另一条。
- en: 'The corresponding logistic functions are σ(0.35 · *x* + *p* − 0.56) and σ(*x*
    + *p* − 1), and we expect the first one has a lower cost with respect to the data.
    We can confirm this with the `logistic_cost` function:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的逻辑函数是 σ(0.35 · *x* + *p* − 0.56) 和 σ(*x* + *p* − 1)，我们预计第一个函数在数据方面具有更低的成本。我们可以使用
    `logistic_cost` 函数来确认这一点：
- en: '[PRE26]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As expected, the line *x* + *p* = 1 is a worse decision boundary, so the logistic
    function σ(*x* + *p* − 1) has a higher cost. The first function σ(0.35 · *x* +
    *p* − 0.56) has a lower cost and a better fit. But is it the best fit? When we
    run gradient descent on the `logistic_cost` function in the next section, we’ll
    find out.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，直线 *x* + *p* = 1 是一个较差的决策边界，因此逻辑函数 σ(*x* + *p* − 1) 具有更高的成本。第一个函数 σ(0.35
    · *x* + *p* − 0.56) 具有更低的成本和更好的拟合度。但它是最佳拟合吗？当我们下一节中对 `logistic_cost` 函数进行梯度下降时，我们将找到答案。
- en: 15.4.4 Exercises
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.4 练习
- en: '| **Exercise 15.6**: Implement the function `plot_line(a,b,c)` referenced in
    section 15.4.3 that plots the line *ax* + *by* = *c*, where 0 ≤ *x* ≤ 1 and 0
    ≤ *y* ≤ 1.**Solution**: Note that I used different names other than *a*, *b*,
    and *c* for the function arguments because `c` is a keyword argument that sets
    the color of the plotted line for Matplotlib’s `plot` function, which I commonly
    make use of:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习 15.6**：实现 15.4.3 节中提到的 `plot_line(a,b,c)` 函数，该函数绘制直线 *ax* + *by* = *c*，其中
    0 ≤ *x* ≤ 1 和 0 ≤ *y* ≤ 1。**解答**：请注意，我使用了除了 *a*、*b* 和 *c* 之外的其他名称作为函数参数，因为 `c`
    是一个关键字参数，用于设置 Matplotlib 的 `plot` 函数绘制的线条颜色，我经常使用这个函数：'
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Exercise 15.7**: Use the formula for the sigmoid function σ to write an
    expanded formula for σ(*ax* + *by* − *c*).**Solution**: Given that![](../Images/CH15_F18_Orland_EQ02.png)we
    can write![](../Images/CH15_F18_Orland_EQ03.png) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.7**：使用 S 型函数 σ 的公式，写出 σ(*ax* + *by* − *c*) 的展开公式。**解答**：鉴于![](../Images/CH15_F18_Orland_EQ02.png)，我们可以写出![](../Images/CH15_F18_Orland_EQ03.png)
    |'
- en: '| **Exercise 15.8−Mini Project**: What does the graph of *k*(*x*, *y*) = σ(*x*²
    + y² − 1) look like? What does the decision boundary look like, meaning the set
    of points where *k*(*x*, *y*) = 0.5?**Solution**: We know that σ(*x*² + y² − 1)
    = 0.5, wherever *x*² + y² − 1 = 0 or where *x*² + y² = 1\. You can recognize the
    solutions to this equation as the points of distance one from the origin or a
    circle of radius 1\. Inside the circle, the distance from the origin is smaller,
    so *x*² + y² < 1 and σ(*x*² + y²) < 0.5, while outside the circle *x*² + y² >
    1, so σ(*x*² + y² − 1) > 0.5\. The graph of this function approaches 1as we move
    further away from the origin in any direction, while it decreases inside the circle
    to a minimum value of about 0.27 at the origin. Here’s the graph:![](../Images/CH15_F18_Orland_UN02.png)A
    graph of σ(*x*² + *y*² − 1). Its value is less than 0.5 inside the circle of a
    radius of 1, and it increases to a value of 1 in every direction outside that
    circle. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.8-迷你项目**：函数 *k*(*x*, *y*) = σ(*x*² + y² − 1) 的图形是什么样的？决策边界是什么样的，即
    *k*(*x*, *y*) = 0.5 的点的集合。**解答**：我们知道 σ(*x*² + y² − 1) = 0.5，无论 *x*² + y² − 1
    = 0 还是 *x*² + y² = 1。你可以识别出这个方程的解为距离原点一个单位的点或半径为 1 的圆。在圆内，从原点的距离较小，因此 *x*² + y²
    < 1 且 σ(*x*² + y²) < 0.5，而在圆外 *x*² + y² > 1，因此 σ(*x*² + y² − 1) > 0.5。随着我们沿任何方向远离原点，这个函数的图形趋近于
    1，而在圆内下降到原点处的最小值约为 0.27。以下是图形！[](../Images/CH15_F18_Orland_UN02.png)。σ(*x*² +
    *y*² − 1) 的图形。在半径为 1 的圆内，其值小于 0.5，而在该圆外的每个方向上增加到 1。|'
- en: '| **Exercise 15.9−Mini Project**: Two equations, 2*x* + *y* = 1 and 4*x* +
    2*y* = 2, define the same line and, therefore, the same decision boundary. Are
    the logistic functions σ(2*x* + *y* − 1) and σ(4*x* + 2*y* − 2) the same?**Solution**:
    No, they aren’t the same function. The quantity 4*x* + 2*y* − 2 increases more
    rapidly with respect to increases in *x* and *y*, so the graph of the latter function
    is steeper:![](../Images/CH15_F18_Orland_UN03.png)The graph of the second logistic
    function is steeper than the graph of the first. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.9-迷你项目**：两个方程 2*x* + *y* = 1 和 4*x* + 2*y* = 2 定义了同一条线，因此也定义了相同的决策边界。逻辑函数
    σ(2*x* + *y* − 1) 和 σ(4*x* + 2*y* − 2) 是否相同？**解答**：它们不是同一个函数。与 *x* 和 *y* 的增加相比，量
    4*x* + 2*y* − 2 的增加速度更快，因此后一个函数的图像更陡峭：![图像](../Images/CH15_F18_Orland_UN03.png)第二个逻辑函数的图像比第一个更陡峭。|'
- en: '| **Exercise 15.10-Mini Project**: Given a line *ax* + *by* = *c*, it’s not
    as easy to define what is above that line and what is below. Can you describe
    which side of the line the function *z*(*x*, *y*) = *ax* + *by* − *c* returns
    positive values?**Solution**: The line *ax* + *by* = *c* is the set of points
    where *z*(*x*, *y*) = *ax* + *by* − *c* = 0\. As we saw for equations of this
    form in chapter 7, the graph of *z*(*x*, *y*) = *ax* + *by* − *c* is a plane,
    so it increases in one direction from the line and decreases in the other direction.
    The gradient of *z*(*x*, *y*) is ∇*z*(*x*, *y*) = (*a*, *b*), so *z*(*x*, *y*)
    increases most rapidly in the direction of the vector (*a*, *b*) and decreases
    most rapidly in the opposite direction (− *a*, − *b*). Both of these directions
    are perpendicular to the direction of the line. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.10-迷你项目**：给定一条直线 *ax* + *by* = *c*，定义这条线以上和以下的部分并不容易。你能描述函数 *z*(*x*,
    *y*) = *ax* + *by* − *c* 返回正值的那一侧吗？**解答**：直线 *ax* + *by* = *c* 是点集，其中 *z*(*x*,
    *y*) = *ax* + *by* − *c* = 0。正如我们在第 7 章中看到的那样，这种形式的方程的 *z*(*x*, *y*) = *ax* +
    *by* − *c* 的图像是一个平面，因此它从直线开始在一个方向上增加，在另一个方向上减少。*z*(*x*, *y*) 的梯度是 ∇*z*(*x*, *y*)
    = (*a*, *b*)，因此 *z*(*x*, *y*) 在向量 (*a*, *b*) 的方向上增加最快，在相反方向（− *a*, − *b*）上减少最快。这两个方向都与直线的方向垂直。|'
- en: 15.5 Finding the best logistic function
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 寻找最佳逻辑函数
- en: We now have a straightforward minimization problem to solve; we’d like to find
    the values *a*, *b*, and *c* that make the `logistic_cost` function as small as
    possible. Then the corresponding function, *L*(*x*, *p*) = σ(*ax* + *bp* − *c*)
    will be the best fit to the data. We can use that resulting function to build
    a classifier by plugging in the mileage *x* and price *p* for an unknown car and
    labeling it as a BMW if *L*(*x*, *p*) > 0.5 and as a Prius, otherwise. We’ll call
    this classifier `best_logistic_classifier(x,p)`, and we can pass it to `test_classifier`
    to see how well it does.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个直接的最小化问题要解决；我们希望找到使 `logistic_cost` 函数尽可能小的 *a*，*b* 和 *c* 的值。然后相应的函数，*L*(*x*,
    *p*) = σ(*ax* + *bp* − *c*) 将是数据的最佳拟合。我们可以使用这个结果函数通过插入未知汽车的里程 *x* 和价格 *p* 来构建一个分类器，如果
    *L*(*x*, *p*) > 0.5，则将其标记为宝马，否则标记为普锐斯。我们将这个分类器命名为 `best_logistic_classifier(x,p)`，并将其传递给
    `test_classifier` 以查看其表现如何。
- en: The only major work we have to do here is upgrading our `gradient_descent` function.
    So far, we’ve only done gradient descent with functions that take 2D vectors and
    return numbers. The `logistic_cost` function takes a 3D vector (*a*, *b*, *c*)
    and outputs a number, so we need a new version of gradient descent. Fortunately,
    we covered 3D analogies for every 2D vector operation we’ve used, so it won’t
    be too hard.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要做的唯一一项主要工作是升级我们的 `gradient_descent` 函数。到目前为止，我们只对那些接受二维向量并返回数字的函数进行了梯度下降。`logistic_cost`
    函数接受一个三维向量 (*a*, *b*, *c*) 并输出一个数字，因此我们需要一个新的梯度下降版本。幸运的是，我们已经为每个二维向量操作覆盖了三维类比，所以这不会太难。
- en: 15.5.1 Gradient descent in three dimensions
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 三维梯度下降
- en: 'Let’s look at our existing gradient calculation that we used to work with functions
    of two variables in chapters 12 and 14\. The partial derivatives of a function
    *f*(*x*, *y*) at a point (*x*[0], *y*[0]) are the derivatives with respect to
    *x* and *y* individually, while assuming the other variable is a constant. For
    instance, plugging in *y*[0] into the second slot of *f*(*x*, *y*), we get *f*(*x*,
    *y*[0]), which we can treat as a function of *x* alone and take its ordinary derivative.
    Putting the two partial derivatives together as components of a 2D vector gives
    us the gradient:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们在第12章和第14章中用来处理两个变量函数的现有梯度计算方法。函数 *f*(*x*, *y*) 在点 (*x*[0], *y*[0]) 的偏导数是相对于
    *x* 和 *y* 的单独导数，同时假设另一个变量是常数。例如，将 *y*[0] 插入到 *f*(*x*, *y*) 的第二个槽中，我们得到 *f*(*x*,
    *y*[0])，我们可以将其视为仅关于 *x* 的函数并对其求普通导数。将这两个偏导数作为二维向量的分量放在一起，我们就得到了梯度：
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The difference for a function of three variables is that there’s one other
    partial derivative we can take. If we look at *f*(*x*, *y*, *z*) at some point
    (*x*[0], *y*[0], *z*[0]), we can look at *f*(*x*, *y*[0], *z*[0]), *f*(*x*[0],
    *y*, *z*[0]), and *f*(*x*[0], *y*[0], *z*) as functions of *x*, *y*, and *z*,
    respectively, and take their ordinary derivatives to get three partial derivatives.
    Putting these three partial derivatives together in a vector, we get the 3D version
    of the gradient:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三个变量的函数，区别在于我们还可以取另一个偏导数。如果我们看 *f*(*x*, *y*, *z*) 在点 (*x*[0], *y*[0], *z*[0])，我们可以将
    *f*(*x*, *y*[0], *z*[0])、*f*(*x*[0], *y*, *z*[0]) 和 *f*(*x*[0], *y*[0], *z*) 分别视为
    *x*、*y* 和 *z* 的函数，并对其求普通导数以得到三个偏导数。将这些三个偏导数作为一个向量放在一起，我们就得到了梯度的三维版本：
- en: '[PRE29]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To do the gradient descent in 3D, the procedure is just as you’d expect; we
    start at some point in 3D, calculate the gradient, and step a small amount in
    that direction to arrive at a new point, where hopefully, the value of *f*(*x*,
    *y*, *z*) is smaller. As one additional enhancement, I’ve added a `max_steps`
    parameter so we can set a maximum number of steps to take during the gradient
    descent. With that parameter set to a reasonable limit, we won’t have to worry
    about our program stalling even if the algorithm doesn’t converge to a point within
    the tolerance. Here’s what the result looks like in Python:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中进行梯度下降，过程正如你所期望的那样；我们从三维空间中的某个点开始，计算梯度，然后朝那个方向迈出小一步到达一个新的点，在那里，希望 *f*(*x*,
    *y*, *z*) 的值会更小。作为额外的增强，我添加了一个 `max_steps` 参数，这样我们就可以设置梯度下降过程中可以采取的最大步数。有了这个参数设置为合理的限制，即使算法没有收敛到容差内的点，我们也不必担心我们的程序会停滞。以下是Python中的结果：
- en: '[PRE30]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: All that remains is to plug in the `logistic_cost` function, and the `gradient_descent3`
    function finds inputs that minimize it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是插入 `logistic_cost` 函数，然后 `gradient_descent3` 函数会找到最小化它的输入。
- en: 15.5.2 Using gradient descent to find the best fit
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.2 使用梯度下降法寻找最佳拟合
- en: 'To be cautious, we can start by using a small number of `max_steps`, like 100:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了谨慎起见，我们可以先使用少量的 `max_steps`，比如100：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we allow it to take 200 steps instead of 100, we see that it has further
    to go after all:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们允许它走200步而不是100步，我们会看到它实际上需要走得更远：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Remember, these results are the parameters required to define the logistic function,
    but they are also the parameters (*a*, *b*, *c*) defining the decision boundary
    in the form *ax* + *bp* = *c*. If we run gradient descent for 100 steps, 200 steps,
    300 steps, and so on, and plot the corresponding lines with `plot_line`, we can
    see the decision boundary converging as in figure 15.19.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这些结果是定义逻辑函数所需的参数，但它们也是定义形式为 *ax* + *bp* = *c* 的决策边界的参数 (*a*, *b*, *c*)。如果我们对梯度下降进行100步、200步、300步等，并用
    `plot_line` 绘制相应的线，我们可以看到决策边界如图15.19所示的那样收敛。
- en: '![](../Images/CH15_F19_Orland.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F19_Orland.png)'
- en: Figure 15.19 With more and more steps, the values of (*a*, *b*, *c*) returned
    by gradient descent seem to be settling on a clear decision boundary.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.19 随着步数的增加，梯度下降法返回的 (*a*, *b*, *c*) 值似乎正在稳定在一个清晰的决策边界上。
- en: 'Somewhere between 7,000 and 8,000 steps, the algorithm actually converges,
    meaning it finds a point where the length of the gradient is less than 10^(−6).
    Approximately speaking, that’s the minimum point we’re looking for:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在7,000到8,000步之间，算法实际上收敛了，这意味着它找到了一个梯度长度小于10^(-6)的点。大致来说，这就是我们寻找的最小点：
- en: '[PRE33]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can see what this decision boundary looks like relative to the one we’ve
    been using (figure 15.20 shows the result):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个决策边界相对于我们一直在使用的决策边界（图15.20显示了结果）的样子：
- en: '[PRE34]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/CH15_F20_Orland.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F20_Orland.png)'
- en: Figure 15.20 Comparing our previous best-guess decision boundary to the one
    implied by the result of gradient descent
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.20 比较我们之前最佳猜测的决策边界与梯度下降结果所暗示的决策边界
- en: This decision boundary isn’t too far off from our guess. The result of the logistic
    regression appears to have moved the decision boundary slightly downward from
    our guess, trading off a few false positives (Priuses that are now incorrectly
    above the line in figure 15.20) for a few more true positives (BMWs that are now
    correctly above the line).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策边界与我们猜测的并不太远。逻辑回归的结果似乎将决策边界稍微向下移动，以换取一些假阳性（现在错误地位于图15.20中的线上方的普锐斯）和一些更多的真阳性（现在正确地位于线上方的宝马）。
- en: 15.5.3 Testing and understanding the best logistic classifier
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.3 测试和理解最佳逻辑分类器
- en: 'We can easily plug these values for (*a*, *b*, *c*) into a logistic function
    and then use it to make a car classification function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地将(*a*, *b*, *c*)的这些值插入到逻辑函数中，然后使用它来创建一个汽车分类函数：
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Plugging this function into the `test_classifier` function, we can see its
    accuracy rate on the test data set is about what we got from our best attempts,
    80% on the dot:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个函数插入到`test_classifier`函数中，我们可以看到它在测试数据集上的准确率大约与我们的最佳尝试结果一致，精确到80%：
- en: '[PRE36]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The decision boundaries are fairly close, so it makes sense that the performance
    is not too far off of our guess from section 15.2\. That said, if what we had
    previously was close, why did the decision boundary converge so decisively where
    it did?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界相当接近，所以性能没有偏离我们在第15.2节中的猜测也就不足为奇了。然而，如果我们之前的结果已经很接近，为什么决策边界会如此果断地收敛到那个位置呢？
- en: It turns out logistic regression does more than simply find the optimal decision
    boundary. In fact, we saw a decision boundary early in the section that outperformed
    this best fit logistic classifier by 0.5%, so the logistic classifier doesn’t
    even maximize accuracy on the test data set. Rather, logistic regression looks
    holistically at the data set and finds the model that is most likely to be accurate
    given all of the examples. Rather than moving the decision boundary slightly to
    grab one or two more percentage points of accuracy on the test set, the algorithm
    orients the decision boundary based on a holistic view of the data set. If our
    data set is representative, we can trust our logistic classifier to do well on
    data it hasn’t seen yet, not just the data in our training set.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，逻辑回归不仅仅是找到最优的决策边界。实际上，我们在本节早期就看到了一个决策边界，它的性能比这个最佳拟合逻辑分类器高出0.5%，所以逻辑分类器并没有在测试数据集上最大化准确率。相反，逻辑回归从整体上审视数据集，并找到在所有示例中最有可能准确性的模型。而不是稍微移动决策边界以获取测试集上的一两个百分点的准确率，算法基于对数据集的整体视角来定位决策边界。如果我们的数据集具有代表性，我们可以相信我们的逻辑分类器在未见过的数据上也能表现良好，而不仅仅是训练集中的数据。
- en: The other information that our logistic classifier has is an amount of certainty
    about every point it classifies. A classifier based only on a decision boundary
    is 100% certain that a point above that boundary is a BMW and that a point below
    that is a Prius. Our logistic classifier has a more nuanced view; we can interpret
    the values it returns between zero and one as a probability a car is a BMW rather
    than a Prius. For real-world applications, it can be valuable to know not only
    the best guess from your machine learning model, but also how trustworthy it considers
    itself to be. If we were classifying benign tumors from malignant ones based on
    medical scans, we might act much differently if the algorithm told us it was 99%
    sure, as opposed to 51% sure, if a tumor was malignant.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逻辑分类器还有其他信息，那就是对每个分类点的确定性程度。仅基于决策边界的分类器对位于该边界之上的点是一辆宝马，位于该边界之下的点是一辆普锐斯的100%确定。我们的逻辑分类器有更细微的看法；我们可以将它在0到1之间返回的值解释为汽车是宝马而不是普锐斯的概率。在现实世界的应用中，了解机器学习模型的最佳猜测以及它认为自己的可信度如何可能非常有价值。如果我们根据医学扫描将良性肿瘤与恶性肿瘤分类，如果算法告诉我们肿瘤有99%的确定性是恶性的，而不是51%，我们可能会采取截然不同的行动。
- en: The way certainty comes through in the shape of the classifier is the magnitude
    of the coefficients (*a*, *b*, *c*). For instance, you can see that the ratio
    between (*a*, *b*, *c*) in our guess of (0.35, 1, 0.56) is similar to the ratio
    in the optimal values of (3.717, 11.42, 5.597). The optimal values are approximately
    ten times bigger than our best guess. The biggest difference that causes this
    change is the steepness of the logistic function. The optimal logistic function
    is much more certain of the decision boundary than the first. It tells us that
    as soon as you cross the decision boundary, certainty of the result increases
    significantly as figure 15.21 shows.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性在分类器形状中体现为系数的幅度（*a*，*b*，*c*）。例如，你可以看到在我们猜测的 (0.35, 1, 0.56) 中，(*a*，*b*，*c*)
    的比例与最优值 (3.717, 11.42, 5.597) 中的比例相似。最优值大约是我们最佳猜测的十倍。造成这种变化的最大差异是逻辑函数的陡峭程度。最优逻辑函数比第一个更确定决策边界。它告诉我们，一旦你越过决策边界，结果确定性就会显著增加，如图
    15.21 所示。
- en: '![](../Images/CH15_F21_Orland.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F21_Orland.png)'
- en: Figure 15.21 The optimized logistic function is much steeper, meaning its certainty
    that a car is a BMW rather than a Prius increases rapidly as you cross the decision
    boundary.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.21 优化后的逻辑函数更陡峭，这意味着当你越过决策边界时，它确定一辆车是宝马而不是普锐斯的确定性会迅速增加。
- en: In the final chapter, we’ll continue to use sigmoid functions to produce certainties
    of results between zero and one as we implement classification using neural networks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章，我们将继续使用 Sigmoid 函数来生成介于零和一之间的结果确定性，当我们使用神经网络实现分类时。
- en: 15.5.4 Exercises
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.4 练习
- en: '| **Exercise 15.11**: Modify the `gradient_descent3` function to print the
    total number of steps taken before it returns its result. How many steps does
    the gradient descent take to converge for `logistic_cost` ?**Solution**: All you
    need to do is add the line `print(steps)` right before `gradient_descent3` to
    return its result:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习 15.11**: 修改 `gradient_descent3` 函数，使其在返回结果前打印出所采取的总步数。梯度下降法对 `logistic_cost`
    的收敛需要多少步？**解答**：你只需要在 `gradient_descent3` 返回结果前添加一行 `print(steps)` 即可：'
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Running the following gradient descent
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下梯度下降
- en: '[PRE38]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: the number printed is `7244`, meaning the algorithm converges in 7,244 steps.
    |
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出的数字是 `7244`，这意味着算法在 7,244 步中收敛。|
- en: '| **Exercise 15.12-Mini Project**: Write an `approx_gradient` function that
    calculates the gradient of a function in any number of dimensions. Then write
    a `gradient_descent` function that works in any number of dimensions. To test
    your `gradient_descent` on an *n* -dimensional function, you can try a function
    like *f*(*x*[1], *x*[2], ... , *x^n* ) = (*x*[1] − 1)² + (*x*[2] − 1)² + ... +
    (*x^n* − 1)², where *x*[1], *x*[2], ... , *x^n* are the *n* input variables to
    the function *f* . The minimum of this function should be (1, 1, ..., 1), an *n*
    -dimensional vector with the number 1 in every entry.**Solution**: Let’s model
    our vectors of arbitrary dimension as lists of numbers. To take partial derivatives
    in the *i*^(th) coordinate at a vector ***v*** = (*v*[1], *v*[2], ... , *v[n]*),
    we want to take the ordinary derivative of the *i*^(th) coordinate *x[i]*. That
    is, we want to look at the function:*f*(*v*[1], *v*[2], ..., *v*[*i*−1], *x[i]*,
    *v*[*i*+1], ..., *v[n]*)that is, in other words, every coordinate of ***v*** plugged
    in to *f* , except the *i*^(th) entry, which is left as a variable *x[i]*. This
    gives us a function of a single variable *x[i]*, and its ordinary derivative is
    the *i*^(th) partial derivative. The code for partial derivatives looks like this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习 15.12-迷你项目**：编写一个 `approx_gradient` 函数，该函数可以计算任何数量维度的函数的梯度。然后编写一个 `gradient_descent`
    函数，该函数可以在任何数量维度上工作。为了测试你的 `gradient_descent` 在 *n* 维函数上的效果，你可以尝试一个函数，例如 *f*(*x*[1]，*x*[2]，...
    ，*x^n* ) = (*x*[1] − 1)² + (*x*[2] − 1)² + ... + (*x^n* − 1)²，其中 *x*[1]，*x*[2]，...
    ，*x^n* 是函数 *f* 的 *n* 个输入变量。这个函数的最小值应该是 (1, 1, ..., 1)，一个每个条目都是数字 1 的 *n* 维向量。**解答**：让我们将任意维度的向量建模为数字列表。为了在向量
    ***v*** = (*v*[1]，*v*[2]，... ，*v[n]*）的 *i*^(th) 坐标上求偏导数，我们想要对 *i*^(th) 坐标 *x[i]*
    求普通导数。也就是说，我们想要查看函数:*f*(*v*[1]，*v*[2]，... ，*v*[*i*−1]，*x[i]*，*v*[*i*+1]，... ，*v[n]*)，换句话说，就是将
    ***v*** 的每个坐标都插入到 *f* 中，除了 *i*^(th) 条目，它被留作变量 *x[i]*。这给我们一个单变量 *x[i]* 的函数，它的普通导数就是
    *i*^(th) 偏导数。偏导数的代码看起来像这样：'
- en: '[PRE39]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that our coordinates are zero-indexed, and the dimension of input to *f*
    is inferred from the length of ***v***.The rest of the work is easy by comparison.
    To build the gradient, we just take the *n* partial derivatives and put them in
    order in a list:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的坐标是零索引的，输入到 *f* 的维度从 ***v*** 的长度推断出来。其余的工作相对容易。要构建梯度，我们只需取 *n* 个偏导数并将它们按顺序放入列表：
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: To do the gradient descent, we replace all of the manipulations of named coordinate
    variables, like *x*, *y*, and *z*, with list operations on the list vector of
    coordinates called ***v*** *:*
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行梯度下降，我们将所有对命名坐标变量的操作，如 *x*、*y* 和 *z*，替换为对坐标列表向量 ***v*** 的列表操作：*
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| To implement the suggested test function, we can write a generalized version
    of it that takes any number of inputs and returns the sum of their squared difference
    from one:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '| 要实现建议的测试函数，我们可以编写一个通用版本，它接受任意数量的输入，并返回它们与一个值的平方差的和：'
- en: '[PRE42]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This function can’t be lower than zero because it’s a sum of squares, and a
    square cannot be less than zero. The value zero is obtained if every entry of
    the input vector ***v*** is one, so that’s the minimum. Our gradient descent confirms
    this (with only a small numerical error), so everything looks good! Note that
    because the starting vector ***v*** is 5D, all vectors in the computation are
    automatically 5D.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数不能低于零，因为它是由平方和组成的，而平方不能小于零。当输入向量 ***v*** 的每个元素都是一的时候，得到零值，这就是最小值。我们的梯度下降法确认了这一点（只有很小的数值误差），所以一切看起来都很正常！请注意，因为起始向量
    ***v*** 是5维的，所以计算中的所有向量都是自动5维的。
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Exercise 15.13-Mini Project**: Attempt to run the gradient descent with
    the `simple_logistic_cost` cost function. What happens?**Solution**: It does not
    appear to converge. The values of *a*, *b*, and *c* continue increasing without
    bound even though the decision boundary stabilizes. This means as the gradient
    descent explores more and more logistic functions, these are staying oriented
    in the same direction but becoming infinitely steep. It is incentivized to become
    closer and closer to most of the points, while neglecting the ones it has already
    mislabeled. As I mentioned, this can be solved by penalizing the incorrect classifications
    for which the logistic function is the most confident, and our `logistic_cost`
    function does that well. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **练习 15.13-迷你项目**：尝试使用 `simple_logistic_cost` 成本函数运行梯度下降。会发生什么？**解决方案**：它似乎没有收敛。尽管决策边界稳定，但
    *a*、*b* 和 *c* 的值仍然无限增加。这意味着随着梯度下降探索越来越多的逻辑函数，这些函数保持同一方向，但变得越来越陡峭。它被激励着越来越接近大多数点，而忽略了它已经错误分类的点。正如我提到的，可以通过惩罚逻辑函数最自信的错误分类来解决此问题，我们的
    `logistic_cost` 函数就很好地做到了这一点。|'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Classification is a type of machine learning task where an algorithm is asked
    to look at unlabeled data points and identify each one as a member of a class.
    In our examples for this chapter, we looked at mileage and price data for used
    cars and wrote an algorithm to classify them either as 5 series BMWs or Toyota
    Priuses.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类是一种机器学习任务，其中算法被要求查看未标记的数据点，并将每个点识别为某一类的成员。在本章的示例中，我们查看二手车里程和价格数据，并编写了一个算法来将它们分类为5系列宝马或丰田普锐斯。
- en: A simple way to classify vector data in 2D is to establish a decision boundary;
    that means drawing a literal boundary in the 2D space where your data lives, where
    points on one side of the boundary are classified in one class and points on the
    other side are classified in another. A simple decision boundary is a straight
    line.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2D空间中对向量数据进行分类的一种简单方法就是建立决策边界；这意味着在数据存在的2D空间中绘制一个实际的边界，边界的一侧的点被分类为一类，另一侧的点被分类为另一类。简单的决策边界是一条直线。
- en: If our decision boundary line takes the form *ax* + *by* = *c*, then the quantity
    *ax* + *by* − *c* is positive on one side of the line and negative on the other.
    We can interpret this value as a measure of how much the data point looks like
    a BMW. A positive value means that the data point looks like a BMW, while a negative
    value means that it looks more like a Prius.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的决策边界线具有形式 *ax* + *by* = *c*，那么 *ax* + *by* − *c* 的值在直线的一侧为正，在另一侧为负。我们可以将这个值解释为衡量数据点看起来像宝马的程度的一个指标。正值意味着数据点看起来像宝马，而负值则意味着它更像普锐斯。
- en: 'The sigmoid function, defined as follows, takes numbers between -∞ and ∞ and
    crunches them into the finite interval from zero to one:'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义如下，sigmoid函数将介于 -∞ 和 ∞ 之间的数字压缩到从零到一的有限区间：
- en: '![](../Images/CH15_F21_Orland_EQ04.png)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F21_Orland_EQ04.png)'
- en: Composing the sigmoid with the function *ax* + *by* − *c*, we get a new function
    σ(*ax* + *by* − *c*) that also measures how much the data point looks like a BMW,
    but it only returns values between zero and one. This type of function is a logistic
    function in 2D.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Sigmoid函数与函数*ax* + *by* − *c*组合，我们得到一个新的函数σ(*ax* + *by* − *c*)，它也衡量数据点看起来有多像宝马，但它只返回介于零和一之间的值。这种类型的函数是二维中的对数函数。
- en: The value between zero and one that a logistic classifier outputs can be interpreted
    as how confident it is that a data point belongs to one class versus another.
    For instance, return values of 0.51 or 0.99 would both indicate that the model
    thinks we’re looking at a BMW, but the latter would be a much more confident prediction.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数分类器输出的介于零和一之间的值可以解释为它有多自信地认为一个数据点属于某一类而不是另一类。例如，返回值0.51或0.99都表明模型认为我们正在看一辆宝马，但后者将是一个更加自信的预测。
- en: With an appropriate cost function that penalizes confident, incorrect classifications,
    we can use gradient descent to find the logistic function of best fit. This is
    the best logistic classifier according to the data set.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个对自信但错误的分类进行惩罚的适当代价函数，我们可以使用梯度下降法找到最佳拟合的对数函数。这是根据数据集得出的最佳对数分类器。
