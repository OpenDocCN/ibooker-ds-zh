- en: 9 Recommended next steps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 建议的下一步行动
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A review of what we have covered so far in this book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾本书到目前为止所涵盖的内容
- en: Additional enhancements you could make to the streetcar delay prediction project
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以对电车延误预测项目进行的额外改进
- en: How you can apply what you have learned to other real-world projects
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何将所学应用到其他现实世界项目中
- en: Criteria to use to select a deep learning project that uses structured data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择使用结构化数据的深度学习项目时应使用的标准
- en: Resources for additional learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外学习的资源
- en: We have almost reached the end of the book. In this chapter, we’ll look back
    and look forward. First, we’ll review what we have learned in the previous chapters,
    from cleaning up a real-world dataset to deploying a trained deep learning model.
    Next, we’ll go over the steps you can take to enhance the streetcar delay prediction
    project with new data sources. Then we’ll talk about how you can apply what you
    have learned to other real-world projects, including how to determine whether
    a given problem involving structured data is a good candidate for a deep learning
    project. Finally, we’ll review some resources for additional learning about deep
    learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎到达了这本书的结尾。在本章中，我们将回顾和展望。首先，我们将回顾在前几章中学到的内容，从清理现实世界的数据集到部署训练好的深度学习模型。接下来，我们将讨论你可以采取的步骤来通过新的数据源增强电车延误预测项目。然后，我们将讨论如何将你所学应用到其他现实世界项目中，包括如何确定一个涉及结构化数据的问题是否适合作为深度学习项目。最后，我们将回顾一些关于深度学习的额外学习资源。
- en: 9.1 Reviewing what we have covered so far
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 回顾本书到目前为止所涵盖的内容
- en: To review what we have learned so far in this book, let’s return to the end-to-end
    diagram introduced in chapter 2, shown in figure 9.1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾本书到目前为止所学的知识，让我们回到第二章中引入的端到端图，如图9.1所示。
- en: '![CH09_F01_Ryan](../Images/CH09_F01_Ryan.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_Ryan](../Images/CH09_F01_Ryan.png)'
- en: Figure 9.1 The end-to-end view of the streetcar delay prediction project
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 电车延误预测项目的端到端视图
- en: In chapter 2, we learned how to ingest a tabular structured dataset into Python
    by using Pandas. In chapters 3 and 4, we went through the process of fixing problems
    with the dataset, including ill-formed entries, errors, and missing values. In
    chapter 5, we refactored the dataset to account for the fact that it included
    information about streetcar delays but no explicit information about situations
    when there was no delay. We used this refactored dataset to create a simple Keras
    model whose layers were automatically generated based on the column structure
    of the dataset. In chapter 6, we iteratively trained this model using the prepared
    dataset and took advantage of Keras facilities to control the training process
    and save the model that had the best performance characteristics. In chapter 7,
    we conducted a set of experiments on the trained model to validate the impact
    of removing bad records and using embeddings. We also ran an experiment to compare
    the deep learning model with a key competitor, XGBoost. In chapter 8, we deployed
    the trained model with a simple web deployment and with a more sophisticated Facebook
    Messenger deployment. With these deployments, we completed the journey from raw
    dataset to a working system that a user could use to get predictions on streetcar
    delays.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们学习了如何使用Pandas将表格结构化数据集导入Python。在第三章和第四章中，我们经历了处理数据集问题的过程，包括格式不正确的条目、错误和缺失值。在第五章中，我们对数据集进行了重构，以考虑到它包含了电车延误的信息，但没有关于没有延误情况下的明确信息。我们使用这个重构后的数据集创建了一个简单的Keras模型，其层是根据数据集的列结构自动生成的。在第六章中，我们使用准备好的数据集迭代训练这个模型，并利用Keras的功能来控制训练过程并保存具有最佳性能特性的模型。在第七章中，我们对训练好的模型进行了一系列实验，以验证删除不良记录和使用嵌入的影响。我们还进行了一个实验，比较深度学习模型与关键竞争对手XGBoost。在第八章中，我们通过简单的Web部署和更复杂的Facebook
    Messenger部署部署了训练好的模型。通过这些部署，我们完成了从原始数据集到一个用户可以使用它来获取电车延误预测的工作系统的旅程。
- en: 9.2 What we could do next with the streetcar delay prediction project
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 我们可以用电车延误预测项目做什么下一步
- en: We have covered a lot of ground in this book so far, but we could take several
    other paths with the streetcar prediction project. We could augment the training
    dataset to include additional data sources, for example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书到目前为止已经覆盖了很多内容，但我们还可以在电车预测项目中采取其他几种路径。例如，我们可以增加训练数据集以包含额外的数据源。
- en: Why would we want to train the model with additional sources of data? The first
    reason would be to try to improve the accuracy of the model. It is possible that
    a model trained with additional data sources would make more accurate predictions
    than the model we trained in chapter 6\. Adding data from other sources (such
    as historical weather data or traffic data) or making use of more data from the
    original dataset (such as delay locations) could provide a stronger signal for
    the model to detect as it tries to predict delays.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要用额外的数据源来训练模型呢？第一个原因可能是为了尝试提高模型的准确性。可能一个使用额外数据源训练的模型会比我们在第6章中训练的模型做出更准确的预测。从其他来源（如历史天气数据或交通数据）添加数据或利用原始数据集中更多的数据（如延迟位置）可以为模型提供更强的信号，以便在尝试预测延误时检测到。
- en: 'Do we know ahead of time whether training the model with additional data sources
    will make the model’s delay predictions more accurate? In short, no, but making
    the model more accurate isn’t the only goal of augmenting the training dataset.
    The second reason to train the model with additional data sources is that doing
    so is a great learning exercise. As you go through the process of training the
    model with additional data sources, you will learn more about the code and prepare
    yourself for the next step: adapting the deep learning with structured data approach
    to entirely new datasets, as introduced in section 9.8.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否事先知道使用额外的数据源训练模型是否会提高模型的延迟预测准确性？简而言之，不知道，但提高模型的准确性并不是增强训练数据集的唯一目标。使用额外的数据源训练模型的第二个原因是，这样做是一个很好的学习练习。在您使用额外数据源训练模型的过程中，您将了解更多关于代码的知识，并为下一步做好准备：将结构化数据方法应用于全新的数据集，如第9.8节中介绍的那样。
- en: 'In the following sections, we will briefly review some additional data that
    you could add to the dataset that is used to train the model. Section 9.4 reviews
    how you could take advantage of the delay location data that is in the original
    dataset but not used in the model training we did in chapter 6\. It shows you
    how you could train the model with a dataset that includes a net new data source:
    historical weather information. Section 9.5 gives you some ideas on how you could
    augment the training data by deriving new columns from the dataset used to train
    the model in chapter 6\. When you have reviewed these sections, you will be ready
    for the sections 9.8-9.11 where you will learn how to adapt the approach taken
    with the streetcar delay prediction problem to new problems involving structured
    data. In section 9.12 you will see the approach applied to a specific new problem,
    predicting the price of Airbnb listings in New York City.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下几节中，我们将简要回顾一些您可以添加到用于训练模型的数据库中的额外数据。第9.4节回顾了您如何利用原始数据集中存在但未用于第6章中我们进行的模型训练的延迟位置数据。它展示了您如何使用包含全新数据源的数据集来训练模型：历史天气信息。第9.5节为您提供了一些想法，说明您如何通过从第6章中用于训练模型的数据库中派生新列来增强训练数据。在您审阅了这些章节后，您将准备好阅读第9.8-9.11节，在那里您将学习如何将用于电车延迟预测问题的方法适应涉及结构化数据的新问题。在第9.12节中，您将看到该方法应用于一个具体的新问题，即预测纽约市Airbnb列表的价格。
- en: 9.3 Adding location details to the streetcar delay prediction project
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 将位置细节添加到电车延迟预测项目中
- en: In chapter 4, we explained how you can use Google’s geocoding API ([http://
    mng.bz/X06Y](http://mng.bz/X06Y)) to replace the location data in the streetcar
    delay dataset with latitude and longitude values. We did not end up using this
    approach in the rest of the extended example, but you could revisit it to see
    whether adding geospatial data to the refactored dataset could yield a model with
    improved performance. You would expect to get higher-fidelity predictions because,
    as the delay heat map in figure 9.2 shows, delays are clustered in the center
    of the city. A trip that starts and ends outside the city center should be less
    likely to be delayed, even if it is on a route/direction/ time combination that
    is likely to encounter delays.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们解释了您如何使用谷歌的地理编码API（[http:// mng.bz/X06Y](http://mng.bz/X06Y)）将电车延迟数据集中的位置数据替换为经纬度值。我们没有在扩展示例的其余部分使用这种方法，但您可以回顾一下，看看是否将地理空间数据添加到重构后的数据集中可以提高模型的性能。您预计会得到更高保真度的预测，因为，如图9.2所示的延迟热图所示，延迟集中在城市中心。一个起点和终点都在城市中心以外的行程不太可能延误，即使它是在可能遇到延误的路线/方向/时间组合上。
- en: '![CH09_F02_Ryan](../Images/CH09_F02_Ryan.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Ryan](../Images/CH09_F02_Ryan.png)'
- en: Figure 9.2 Delays are clustered in the central part of the city.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2延迟集中在城市的中心部分。
- en: 'One way that you could take advantage of latitude and longitude values derived
    from locations in the original dataset is to divide each route into subroutes.
    Here is an approach you could take to automatically divide each route into subsections
    based on the latitude and longitude values for the route:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以利用从原始数据集中的位置推导出的经纬度值的一种方法是将每条路线划分为子路线。以下是一种基于路线的经纬度值自动将每条路线划分为子节的方法：
- en: Define a bounding box around the entire route, defined by the maximum and minimum
    longitude and latitude values for the route. You can use the latitude and longitude
    values for delays on the route as a proxy for the entire route to get the maximum
    and minimum values. The streetcar_data_geocode_get_boundaries notebook includes
    code you can use, including the `def_min_max()` function, which creates a dataframe
    with minimum and maximum latitude and longitude values for each route, as the
    following listing shows.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个路线上定义一个边界框，该边界框由路线的最大和最小经纬度值确定。您可以使用路线上的延迟经纬度值作为整个路线的代理来获取最大和最小值。streetcar_data_geocode_get_boundaries笔记本中包含了您可以使用的代码，包括`def_min_max()`函数，该函数创建一个包含每个路线的最小和最大经纬度值的dataframe，如下所示。
- en: Listing 9.1 Code to define a dataframe with route boundaries
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表9.1定义包含路线边界的dataframe的代码
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Figure 9.3 shows the resulting minimum and maximum latitude and longitude values
    for a subset of routes.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图9.3显示了部分路线的最小和最大纬度和经度值。
- en: '![CH09_F03_Ryan](../Images/CH09_F03_Ryan.png)'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F03_Ryan](../Images/CH09_F03_Ryan.png)'
- en: Figure 9.3 Minimum and maximum latitude and longitude values for a subset of
    routes
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.3部分路线的最小和最大纬度和经度值
- en: Now that you have bounding boxes for each route defined as the maximum and minimum
    latitude and longitude values for each route, you can divide the bounding box
    into a number (say, 10) of equal-size rectangles along its main axis. For most
    routes, this axis would be the east-west axis. For the Spadina and Bathurst routes,
    it would be the north-south axis. The result would be subroutes for each route
    defined by minimum and maximum longitude and latitude values. Figure 9.4 shows
    what the bounding boxes for the subroutes might look like for the St. Clair route.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您已经为每条路线定义了边界框，作为每条路线的最大和最小经纬度值，您可以将边界框沿其主轴划分为若干（例如，10个）等大小的矩形。对于大多数路线，这个轴将是东西轴。对于斯帕丁纳和巴瑟斯特路线，它将是南北轴。结果将为每条路线定义子路线，由最小和最大经纬度值确定。图9.4显示了斯克莱尔路线的子路线边界框可能的样子。
- en: '![CH09_F04_Ryan](../Images/CH09_F04_Ryan.png)'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F04_Ryan](../Images/CH09_F04_Ryan.png)'
- en: Figure 9.4 Subroute bounding boxes for the St Clair route
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.4斯克莱尔路线的子路线边界框
- en: With these subroutes defined for each route, you could add a column in the refactored
    dataset so that each row of the revised refactored dataset represented a route/subroute/direction/date
    and time combination. Using the latitude and longitude values for delay locations,
    for each delay you could identify the subroute where the delay occurred. Figure
    9.5 shows a snippet of the original refactored dataset, and figure 9.6 shows what
    the refactored dataset would look like after a subroute column was added to it.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为每条路线定义了这些子路线之后，您可以在重构的数据集中添加一个列，以便每个修订后的重构数据集的行代表一个路线/子路线/方向/日期和时间组合。使用延迟位置的经纬度值，对于每个延迟，您可以确定发生延迟的子路线。图9.5显示了原始重构数据集的一个片段，图9.6显示了在添加子路线列后重构数据集的外观。
- en: '![CH09_F05_Ryan](../Images/CH09_F05_Ryan.png)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F05_Ryan](../Images/CH09_F05_Ryan.png)'
- en: Figure 9.5 Original refactored dataset
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.5原始重构数据集
- en: '![CH09_F06_Ryan](../Images/CH09_F06_Ryan.png)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F06_Ryan](../Images/CH09_F06_Ryan.png)'
- en: Figure 9.6 Refactored dataset with subroute column added
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.6添加了子路线列的重构数据集
- en: 'When you have added subroutes to the refactored dataset and retrained the model
    with this augmented dataset, you face the challenge of how to enable the user
    to define what subroutes of a given route they are traveling through. To score
    with the revised model, you would need to get start and end locations from the
    user for their trip. For the web deployment, you could add a new control in home.html
    to allow the user to select the subroute for their trip. The user experience would
    not be ideal with web deployment, so what about enhancing the Facebook Messenger
    deployment to allow the user to specify a subroute? You could take two approaches:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为重构后的数据集添加了子路由并使用这个增强的数据集重新训练了模型后，你面临的一个挑战是如何让用户定义他们正在通过给定路由的哪些子路由。为了使用修订后的模型得分，你需要从用户那里获取他们的旅行起点和终点位置。对于网络部署，你可以在home.html中添加一个新的控件，让用户选择他们的旅行子路由。使用网络部署的用户体验可能不是理想的，那么增强Facebook
    Messenger部署以允许用户指定子路由怎么样？你可以采取两种方法：
- en: Enhance the Rasa model to allow the user to enter major cross-street names and
    then use the geocode API to translate the intersections of these streets with
    the streetcar route into latitude and longitude values.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强Rasa模型，允许用户输入主要交叉街道名称，然后使用geocode API将这些街道的交叉口转换为纬度和经度值。
- en: Use Facebook Messenger’s webview feature ([http://mng.bz/xmB6](http://mng.bz/xmB6))
    to display an interactive map widget in a web page that allows the user to select
    route points.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Facebook Messenger的webview功能([http://mng.bz/xmB6](http://mng.bz/xmB6))在网页中显示一个交互式地图小部件，允许用户选择路线点。
- en: Overall, adding subroutes to the streetcar delay prediction model would likely
    improve performance, but it would be nontrivial work to adapt the web deployment
    or the Facebook Messenger deployment to allow the user to specify the start and
    end points of their trip.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，将子路由添加到电车延误预测模型可能会提高性能，但将网络部署或Facebook Messenger部署调整为允许用户指定他们的旅行起点和终点将是一项非同小可的工作。
- en: 9.4 Training our deep learning model with weather data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 使用天气数据训练我们的深度学习模型
- en: Toronto has four distinct seasons, with extremes of weather in the winter and
    summer. These extremes can have an impact on streetcar delays. Even a modest snowfall,
    for example, can cause gridlock that delays streetcars across the network. Suppose
    that we want to harness weather data to see whether it yields better predictions
    of streetcar delays. Where do we start? This section summarizes what you need
    to do to add weather data to the streetcar delay prediction model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多伦多拥有四个不同的季节，冬季和夏季的天气极端。这些极端可能会影响电车延误。例如，即使是轻微的降雪也可能导致交通拥堵，从而延误整个网络中的电车。假设我们想利用天气数据来查看它是否能够提供更好的电车延误预测。我们从哪里开始？本节总结了将天气数据添加到电车延误预测模型中所需进行的操作。
- en: 'The first challenge is finding a source for weather data to incorporate into
    the training dataset. There are several open source data sources ([http://mng.bz/5pp4](http://mng.bz/5pp4))
    for weather information. Figure 9.7 shows the interface for exercising endpoints
    for one such source: Dark Sky ([http://mng.bz/A0DQ](http://mng.bz/A0DQ)). You
    will need to provide credentials (such as your GitHub ID and password) to access
    this interface, and although you get a free allocation of API calls, you will
    need to provide payment information to run a test exercise of the API.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个挑战是找到一个天气数据源，将其纳入训练数据集。有几个开源数据源([http://mng.bz/5pp4](http://mng.bz/5pp4))提供天气信息。图9.7显示了练习此类数据源端点的界面：Dark
    Sky([http://mng.bz/A0DQ](http://mng.bz/A0DQ))。你需要提供凭证（如GitHub ID和密码）来访问此接口，尽管你得到了免费的API调用配额，但你仍需要提供支付信息来运行API的测试练习。
- en: '![CH09_F07_Ryan](../Images/CH09_F07_Ryan.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07_Ryan](../Images/CH09_F07_Ryan.png)'
- en: Figure 9.7 Exercising a weather information API
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 练习天气信息API
- en: 'Suppose that we want to see what the weather was like early in the morning
    of March 3, 2007, at Toronto City Hall. Here are the parameters that the API requires:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想查看2007年3月3日早上在多伦多市政厅的天气情况。以下是API所需的参数：
- en: 'Date time: 2007-03-01T01:32:33'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期时间：2007-03-01T01:32:33
- en: 'Longitude: -79.383186'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经度：-79.383186
- en: 'Latitude: 43.653225'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纬度：43.653225
- en: The API interface shows you what the Python API call looks like for this request,
    as shown in the next listing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: API接口显示了此请求的Python API调用看起来是什么样子，如下一列表所示。
- en: Listing 9.2 Sample code produced by the Dark Sky API interface
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 Dark Sky API接口生成的示例代码
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ URL built with the date/time, latitude, and longitude input
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用日期/时间和经纬度输入构建的URL
- en: ❷ To run this call, you need to get an API key for Dark Sky and paste it here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要运行此调用，您需要获取Dark Sky的API密钥并将其粘贴在此处。
- en: 'Figure 9.8 shows the results returned by this API call. How could we use this
    weather data? To begin with, we want to control the number of API calls we need
    to make to minimize the overall cost. The cost per Dark Sky call is a fraction
    of a cent, but the overall cost could add up if we aren’t careful. Consider the
    following approach to getting weather data that is specific in location and time:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8显示了此API调用返回的结果。我们如何使用这些天气数据？首先，我们希望控制需要进行的API调用次数，以最小化整体成本。每次Dark Sky调用的成本是几分之一美分，但如果不够小心，整体成本可能会很高。考虑以下获取特定位置和时间的天气数据的方法：
- en: Use the subroutes introduced in section 9.3, and get distinct weather data points
    for each subroute by using the mean latitude and longitude value for each subroute
    bounding box.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第9.3节中介绍的子路由，并通过使用每个子路由边界框的平均纬度和经度值来获取每个子路由的独立天气数据点。
- en: For each subroute, get four weather data points per hour.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个子路由，每小时获取四个天气数据点。
- en: '![CH09_F08_Ryan](../Images/CH09_F08_Ryan.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Ryan](../Images/CH09_F08_Ryan.png)'
- en: Figure 9.8 Response to weather API call with weather details for Toronto City
    Hall early on March 3, 2007
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 2007年3月3日早上天气API调用对多伦多市政厅的天气详情响应
- en: With this approach, we would need more than 31 million weather data points to
    cover the training data starting in January 2014\. The cost of all these API calls
    would be more than $40,000 US —a huge cost for an experiment. How can we still
    get useful weather data without needing so many data points?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们需要超过3100万个天气数据点来覆盖从2014年1月开始的数据集。所有这些API调用的成本将超过40,000美元——对于一个实验来说，这是一个巨大的成本。我们如何在不需要这么多数据点的情况下仍然获取有用的天气数据？
- en: 'We are lucky that the streetcar delay problem is limited to a relatively small
    geographic area with predictable weather patterns, so we can make some simplifying
    assumptions to control the number of weather data points we need. The following
    simplifying assumptions will let us add weather data to the refactored dataset
    with a minimal number of API calls:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运，电车延误问题局限于一个相对较小的地理区域，并且有可预测的天气模式，因此我们可以做出一些简化的假设来控制所需的天气数据点数量。以下简化假设将使我们能够以最少的API调用次数将天气数据添加到重构后的数据集中：
- en: '*Weather conditions are consistent for a particular hour.* We get only 24 weather
    data points for a day, rather than multiple data points per hour. Weather can
    certainly change within an hour, but the kind of weather that contributes to streetcar
    delays (such as downpours of rain or major snowfalls) rarely starts and stops
    in Toronto in the course of a single hour. We can safely take a single weather
    reading per hour.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特定小时的天气条件是一致的。我们每天只获取24个天气数据点，而不是每小时多个数据点。天气确实可能在小时内发生变化，但导致电车延误的天气（如大雨或大雪）很少在多伦多一个小时内开始和结束。我们可以安全地每小时读取一次天气数据。*'
- en: '*Weather conditions are consistent throughout the streetcar network.* The entire
    streetcar network is contained in an area 26 km from east to west and 11 km from
    north to south, as shown by the bounding box in figure 9.9, so it’s a reasonable
    assumption to say that the kind of weather that would cause streetcar delays is
    consistent at any time across the entire network. That is, if it’s snowing heavily
    at Long Branch, at the western end of the network, there is probably snow in The
    Beach, at the eastern end of the network. With this assumption, we can use (43.653225,
    -79.383186), the latitude and longitude of Toronto’s City Hall, for all calls
    to the weather API.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电车网络中的天气条件是一致的。整个电车网络位于一个东西宽26公里、南北宽11公里的区域内，如图9.9中的边界框所示，因此可以合理地假设，导致电车延误的天气类型在整个网络中的任何时间都是一致的。也就是说，如果网络西端的Long
    Branch下大雪，那么网络东端的The Beach很可能也在下雪。基于这个假设，我们可以使用(43.653225, -79.383186)，即多伦多市政厅的纬度和经度，对所有天气API的调用。*'
- en: '![CH09_F09_Ryan](../Images/CH09_F09_Ryan.png)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![CH09_F09_Ryan](../Images/CH09_F09_Ryan.png)'
- en: Figure 9.9 Bounding box for the streetcar network
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.9电车网络的边界框
- en: 'Here are the extremes of the streetcar network as latitudes and longitudes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是电车网络纬度和经度的极端值：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With these simplifying assumptions, we need weather data for every hour since
    January 1, 2014, or about 52,500 data points. Given what Dark Sky charges per
    API call, it would cost about $60 US to generate the required weather datapoints.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些简化假设下，我们需要从2014年1月1日以来的每小时天气数据，大约有52,500个数据点。考虑到Dark Sky按API调用收费，生成所需天气数据点将花费大约60美元。
- en: 'Now that we’ve established the volume of historical weather data that we need,
    which weather features do we want to incorporate into the dataset? Here are some
    obvious weather dataset fields that could be relevant for predicting streetcar
    delays:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了所需的历天气候数据量，我们希望将哪些天气特征纳入数据集中？以下是一些可能对预测电车延误相关的明显天气数据集字段：
- en: '*Temperature* —In the absence of active precipitation, temperature extremes
    could have a correlation with delays. Temperature would be a continuous column
    in the dataset.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*温度* —在没有活跃降水的情况下，温度极端值可能与延误有关。温度将是数据集中的连续列。'
- en: '*Icon* —Values in this field, such as “snow” or “rain,” neatly encapsulate
    the weather conditions. Icon would be a categorical column in the dataset.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图标* —该字段中的值，如“雪”或“雨”，整洁地封装了天气状况。图标将是数据集中的分类列。'
- en: '*Summary* —Values in this field, such as “Rain throughout the day” and “Light
    snow starting in the morning,” provide extra context about the overall weather
    condition captured in the icon column. The summary column could be a text column.
    Recall that the refactored dataset used to train the deep learning model in chapter
    6 did not contain any text columns. It would be interesting to add summary as
    a text column because it would exercise an aspect of the deep learning model code
    that the core streetcar delay dataset didn’t exploit.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摘要* —该字段中的值，如“全天降雨”和“上午开始轻雪”，提供了关于图标列中捕获的整体天气状况的额外背景信息。摘要列可能是一个文本列。回想一下，用于在第6章训练深度学习模型的重构数据集不包含任何文本列。将摘要作为文本列添加将很有趣，因为它将锻炼深度学习模型代码的一个方面，而核心电车延误数据集没有利用这一点。'
- en: 'Assuming that you get the weather datapoints described in the preceding list,
    you will need to update the dataset refactoring code in the streetcar_model_training
    notebook to incorporate the weather fields. In particular, you need to add the
    column names for the weather fields to the appropriate lists in `def_col_lists`
    `()` :'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你得到了前面列表中描述的天气数据点，你需要更新电车模型训练笔记本中的数据集重构代码，以包含天气字段。特别是，你需要将天气字段的列名添加到`def_col_lists`
    `()`中的适当列表中：
- en: Add the temperature column name to `continuouscols`.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将温度列名添加到`continuouscols`。
- en: Add the summary column name to `textcols`.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将摘要列名添加到`textcols`。
- en: If you don’t put the icon column name in any other list, it will be added automatically
    to the list for categorical columns, which is exactly what we want.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有将图标列名放入任何其他列表中，它将自动添加到分类列的列表中，这正是我们想要的。
- en: The training code is written to work for any set of columns as long as the column
    names are correctly identified in `def_col_lists` `()`. The rest of the training
    code should work with the new columns and give you a new trained model that incorporates
    the weather columns.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码编写为适用于任何一列，只要在`def_col_lists` `()`中正确识别了列名。其余的训练代码应与新列一起工作，并为你提供一个包含天气列的新训练模型。
- en: 'When you have a trained model that incorporates the weather columns, how do
    you account for the weather conditions at scoring time when a user wants to know
    whether their streetcar trip will be delayed? First, you add the new weather columns
    to the `score_cols` list in the scoring code. This list is the list of columns
    that get scored and is used to define `score_df` , the dataframe that contains
    the values that are run through the pipelines in the scoring code. You can call
    the Dark Sky API in the scoring code to get the current weather conditions, using
    the Toronto City Hall latitude and longitude noted earlier, and build a string
    for the current time in the format required by Dark Sky: [YYYY]-[MM]-[DD]T[HH]:[MM]:[SS].
    So if the current date is May 24, 2021, and the time is noon, the date time string
    for the Dark Sky API is 2020-05-24T12:00:00\. When you get the required weather
    fields returned from the API call, you can use these values to set the weather
    columns in `score_df`. The scoring code runs `score_df` through the pipelines
    and applies the output of the pipelines to the trained model, and you get a delay
    prediction. Because of the simplifying assumptions stated earlier, you don’t need
    any information from the user at scoring time to get the weather data needed for
    scoring.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有一个包含天气列的已训练模型时，当用户想知道他们的电车行程是否会延误时，您如何在评分时间考虑天气条件？首先，您将新的天气列添加到评分代码中的`score_cols`列表中。这个列表是得分列的列表，用于定义`score_df`，即包含通过评分代码中的管道运行值的dataframe。您可以在评分代码中调用Dark
    Sky API以获取当前天气条件，使用之前提到的多伦多市政厅的纬度和经度，并构建符合Dark Sky要求的当前时间字符串：[YYYY]-[MM]-[DD]T[HH]:[MM]:[SS]。所以如果当前日期是2021年5月24日，时间是中午，Dark
    Sky API的日期时间字符串是2020-05-24T12:00:00。当您从API调用中获取所需的天气字段时，您可以使用这些值来设置`score_df`中的天气列。评分代码将`score_df`通过管道运行，并将管道的输出应用于训练模型，从而得到延误预测。由于之前提到的简化假设，您在评分时间不需要从用户那里获取任何信息来获取评分所需的天气数据。
- en: Figure 9.10 summarizes the changes required to incorporate weather data into
    the streetcar delay prediction deep learning project via Facebook Messenger deployment.
    To adapt the web deployment, you would make similar changes to the scoring code
    in the main Python program for web deployment, flask_server.py , as specified
    in figure 9.10 for actions.py , the main Python program for Facebook Messenger
    deployment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10总结了通过Facebook Messenger部署将天气数据纳入电车延误预测深度学习项目所需的更改。为了适应Web部署，您需要对Web部署的主要Python程序`flask_server.py`中的评分代码进行类似的更改，如图9.10中为`actions.py`（Facebook
    Messenger部署的主要Python程序）所指定的。
- en: '![CH09_F10_Ryan](../Images/CH09_F10_Ryan.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F10_Ryan](../Images/CH09_F10_Ryan.png)'
- en: Figure 9.10 Summary of changes required to add weather data to the streetcar
    delay model
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 添加天气数据到电车延误模型所需更改的总结
- en: This exercise of adding weather data to your deep learning model not only gives
    you the opportunity to improve the performance of the model, but also shows the
    steps you need to take to add other data sources to the streetcar delay deep learning
    model. You can extrapolate from the steps described in this section to create
    a deep learning model on a new dataset. Section 9.8 introduces the additional
    steps you need to take to apply the approach described in this book to a new structured
    dataset. But before we look at a new project, in section 9.5 we are going to look
    at two simple options to augment the training dataset for the streetcar delay
    prediction project.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将天气数据添加到您的深度学习模型中的这项练习不仅为您提供了提高模型性能的机会，还展示了您需要采取的步骤以将其他数据源添加到电车延误深度学习模型中。您可以从本节中描述的步骤中推断出创建基于新数据集的深度学习模型的方法。第9.8节介绍了将本书中描述的方法应用于新结构化数据集所需采取的额外步骤。但在我们查看新项目之前，在第9.5节中，我们将探讨两个简单选项，以增强电车延误预测项目的训练数据集。
- en: 9.5 Adding season or time of day to the streetcar delay prediction project
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 将季节或一天中的时间添加到电车延误预测项目中
- en: 'In sections 9.3 and 9.4, we reviewed two additional data sources that we could
    add to the training data for the model: delay location data and weather data.
    Both of these data sources are relatively challenging to add to the training process.
    If you want to take a simpler approach to adding data to the training dataset,
    you can try to derive new columns from columns in the dataset we used to train
    the model in chapter 6\. A season column derived from the month column, with values
    0-3 for the four seasons, is a relatively simple addition, for example.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在9.3节和9.4节中，我们回顾了我们可以添加到模型训练数据中的两个额外数据源：延迟位置数据和天气数据。这两个数据源相对难以添加到训练过程中。如果你想采取更简单的方法向训练数据集添加数据，你可以尝试从我们在第6章中用于训练模型的数据库中的列推导出新列。例如，从月份列中推导出的季节列，值为0-3代表四个季节，这是一个相对简单的添加。
- en: 'As you can derive a season column from the month column, you can derive a time-of-day
    column from the hour column. The interesting aspect of this column is that you
    could control the boundaries of each time of day. Suppose that you define a time-of-day
    column with five values:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你可以从月份列中推导出季节列，你也可以从小时列中推导出一天中的时间列。这个列的有趣之处在于你可以控制每天每个时间段的边界。假设你定义了一个包含五个值的一天中的时间列：
- en: Overnight
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深夜
- en: Morning rush hour
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早晨高峰时段
- en: Midday
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中午
- en: Afternoon rush hour
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下午高峰时段
- en: Evening
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晚上
- en: You could experiment with different start and end times for each category to
    see the impact on the model’s performance. Would it make a difference in the model’s
    performance if you defined a morning rush hour from 5:30 to 10 a.m. versus 6:30
    to 9:00 a.m.?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试为每个类别不同的开始和结束时间进行实验，看看对模型性能的影响。如果你将早晨高峰时段定义为从5:30到上午10点而不是从6:30到上午9:00，这会对模型性能产生影响吗？
- en: '9.6 Imputation: An alternative to removing records with bad values'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 插补：移除包含不良值的记录的替代方案
- en: 'In chapter 7, we did an experiment to compare the model’s performance with
    two forms of the training dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们进行了一个实验，比较了模型性能与两种训练数据集形式：
- en: Excluding records with bad values (such as records with invalid routes)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除包含不良值的记录（如包含无效路线的记录）
- en: Including records with bad values
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含包含不良值的记录
- en: The conclusion of this experiment was that the model has better performance
    when the bad values are removed. Despite this conclusion, we pay a price for removing
    the records with bad values. For a given run on the data preparation notebook
    using delay data up until the end of 2019, there are about 78,500 delay records
    in the input dataset, but only about 61,500 records after the records with bad
    values have been removed. In this case, we lose around 20% of the records when
    we remove the bad records. It’s important to remember that we remove the whole
    record when it is has a bad value in one field, so we could be losing a useful
    portion of the signal when we throw out all the records with bad values. Are there
    any alternatives that would allow us to preserve some of this lost signal?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验的结论是，当移除不良值时，模型的性能更好。尽管有这个结论，我们为移除包含不良值的记录付出了代价。对于在2019年底之前使用延迟数据在数据准备笔记本上进行的给定运行，输入数据集中大约有78,500条延迟记录，但在移除不良值记录后，只有大约61,500条记录。在这种情况下，当我们移除不良记录时，我们失去了大约20%的记录。重要的是要记住，当记录在一个字段中有不良值时，我们会移除整个记录，因此当我们丢弃所有包含不良值的记录时，我们可能会丢失信号的有用部分。有没有任何替代方案可以让我们保留一些丢失的信号？
- en: 'As it turns out, an approach called *imputation* , which replaces missing values
    with another value, could help. In the case of structured, tabular data, the kind
    of imputation that is available depends on the type of the column:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一种称为*插补*的方法，即用另一个值替换缺失值，可能会有所帮助。在结构化、表格数据的情况下，可用的插补类型取决于列的类型：
- en: '*Continuous* —You can replace missing values with a fixed value (such as zero)
    or a computed value (such as the average of all the values in the column).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续型* —你可以用固定值（如零）或计算值（如该列所有值的平均值）来替换缺失值。'
- en: '*Categorical* —You can replace missing values with the most common value in
    the column or take a more sophisticated approach to apply a model (such as the
    1,000 nearest neighbors) to find a replacement for missing values.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类型* —你可以用列中最常见的值来替换缺失值，或者采取更复杂的方法应用模型（如使用1,000个最近邻）来找到缺失值的替代值。'
- en: If you want to experiment with imputation for the streetcar delay prediction
    model, you can find a more complete discussion of imputation approaches in an
    article on dealing with missing values ([http://mng.bz/6AAG](http://mng.bz/6AAG)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要对电车延迟预测模型进行插补实验，你可以在处理缺失值的文章中找到关于插补方法的更完整讨论（[http://mng.bz/6AAG](http://mng.bz/6AAG)）。
- en: 9.7 Making the web deployment of the streetcar delay prediction model generally
    available
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 使电车延迟预测模型的网页部署普遍可用
- en: In chapter 8, we described how to create a simple web deployment of the trained
    model. The web deployment described in chapter 8 is entirely local; you can access
    it only on the system where the deployment is done. What if you want to share
    this deployment with your friends on other systems?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们描述了如何创建训练模型的简单网页部署。第 8 章中描述的网页部署完全是本地的；你只能在部署的系统上访问它。如果你想与其他系统上的朋友分享这个部署怎么办？
- en: The simplest way to open the web deployment is to use ngrok, the utility we
    used in chapter 8 for the Facebook Messenger deployment. In that chapter, we used
    ngrok to externalize localhost so that your Facebook app could communicate with
    the Rasa chatbot server running on your local system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 打开网页部署最简单的方法是使用 ngrok，这是我们第 8 章中用于 Facebook Messenger 部署的实用程序。在第 8 章中，我们使用 ngrok
    将 localhost 外部化，以便你的 Facebook 应用程序可以与运行在你本地系统上的 Rasa 聊天机器人服务器通信。
- en: 'To use ngrok to make your web deployment accessible outside your local system,
    follow these steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ngrok 使你的网页部署在本地系统之外可访问，请按照以下步骤操作：
- en: If you haven’t already done so, follow the installation instructions at [https://
    ngrok.com/download](https://ngrok.com/download) to install ngrok.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请按照 [https:// ngrok.com/download](https://ngrok.com/download) 上的安装说明安装
    ngrok。
- en: 'In the directory where you installed ngrok, invoke ngrok to make localhost:5000
    on your system externally available. Here is the command for Windows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你安装 ngrok 的目录中，调用 ngrok 使你的系统上的 localhost:5000 可以外部访问。以下是 Windows 的命令：
- en: '[PRE3]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Copy the https forwarding URL in the ngrok output, highlighted in figure 9.11.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制 ngrok 输出中的 https 转发 URL，如图 9.11 所示。
- en: '![CH09_F11_Ryan](../Images/CH09_F11_Ryan.png)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F11_Ryan](../Images/CH09_F11_Ryan.png)'
- en: Figure 9.11 ngrok output with forwarding URL highlighted
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.11 ngrok 输出，突出显示转发 URL
- en: 'Start the web deployment by running this command in the deploy_web directory:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 deploy_web 目录下运行此命令以启动网页部署：
- en: '[PRE4]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that you have ngrok running to externalize localhost:5000, the web deployment
    will be available to other users at the forwarding URL provided by ngrok. If other
    users open the ngrok forwarding URL in a browser, they will see home.html, as
    shown in figure 9.12.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经运行了 ngrok 将 localhost:5000 外部化，网页部署将通过 ngrok 提供的转发 URL 对其他用户可用。如果其他用户在浏览器中打开
    ngrok 转发 URL，他们将看到如图 9.12 所示的 home.html。
- en: '![CH09_F12_Ryan](../Images/CH09_F12_Ryan.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F12_Ryan](../Images/CH09_F12_Ryan.png)'
- en: Figure 9.12 home.html served via ngrok with an externally accessible URL
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 通过 ngrok 提供的具有外部访问 URL 的 home.html
- en: When a user on another system has opened home.html at the ngrok forwarding URL,
    they can select scoring parameters and click Get Prediction to show a delay prediction
    for their streetcar trip. Note that this deployment will be available to other
    users only when your local system is connected to the internet and flask_server.py
    is running. Also note that with the free ngrok plan, you get a different forwarding
    URL each time you invoke ngrok. If you need a fixed URL for your web deployment
    with ngrok, you need to get one of the ngrok paid subscription options.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当另一个系统上的用户在 ngrok 转发 URL 中打开 home.html 时，他们可以选择评分参数并点击获取预测以显示他们的电车行程的延迟预测。请注意，只有当你的本地系统连接到互联网且
    flask_server.py 正在运行时，此部署才对其他用户可用。另外请注意，使用免费 ngrok 计划，每次调用 ngrok 时都会获得不同的转发 URL。如果你需要为
    ngrok 的网页部署获取一个固定 URL，你需要选择 ngrok 的付费订阅选项之一。
- en: In sections 9.3-9.5, we introduced some ideas for additional data sources that
    could improve the performance of the streetcar delay prediction model. In section
    9.8, we’ll outline how you could apply the approach used for the streetcar delay
    problem to a new problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 9.3-9.5 节中，我们介绍了一些可能改进电车延迟预测模型性能的额外数据源的想法。在第 9.8 节中，我们将概述如何将用于电车延迟问题的方法应用于新的问题。
- en: 9.8 Adapting the streetcar delay prediction model to a new dataset
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 将电车延迟预测模型适配到新的数据集
- en: The previous sections outlined how you could augment the streetcar delay prediction
    model by incorporating additional data in the training set for the model. What
    if you wanted to adapt the model for a different dataset? This section summarizes
    the steps to adapt the approach described in this book to a new structured dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节概述了如何通过在模型的训练集中加入额外的数据来增强电车延误预测模型。如果您想将模型适应不同的数据集，本节总结了将本书中描述的方法适应新的结构化数据集的步骤。
- en: The code examples in this book can be applied to other tabular structured datasets,
    but you need to take some steps, summarized in figure 9.13, to adapt the streetcar
    delay prediction code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以应用于其他表格结构化数据集，但您需要采取一些步骤，如图9.13所示，以适应电车延误预测代码。
- en: '![CH09_F13_Ryan](../Images/CH09_F13_Ryan.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F13_Ryan](../Images/CH09_F13_Ryan.png)'
- en: Figure 9.13 Summary of changes required to create a model for new dataset
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 创建新数据集模型所需更改的摘要
- en: 'When you consider applying the approach described in this book to a new dataset,
    the first challenge is whether the dataset meets the minimum requirements to be
    applicable to deep learning. Here are some of the characteristics of a structured
    dataset that could be considered for deep learning:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当您考虑将本书中描述的方法应用于新的数据集时，第一个挑战是数据集是否满足应用深度学习的最低要求。以下是一些可以考虑用于深度学习的结构化数据集的特征：
- en: '*Big enough* —Recall the discussion in chapter 3 about how big a structured
    dataset needed to be for deep learning to have a chance of success. A dataset
    that doesn’t have at least tens of thousands of records is too small. On the other
    side, unless you have the experience and resources to add big data approaches
    to your toolkit, a dataset with tens of millions of records will be a challenge.
    A dataset with more than 70,000 records and fewer than 10,000,000 records is a
    good place to start.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*足够大* —回想一下第3章中关于为了使深度学习有成功机会，结构化数据集需要有多大规模的讨论。没有至少数万条记录的数据集太小。另一方面，除非您有经验和资源将大数据方法添加到您的工具箱中，否则拥有数千万条记录的数据集将是一个挑战。一个拥有超过70,000条记录但少于1,000万条记录的数据集是一个良好的起点。'
- en: '*Heterogenous* —If your dataset is made up entirely of continuous columns,
    you will get a better return on your investment from using a non-deep-learning
    approach, such as XGBoost, to make predictions on it. But if you have a dataset
    that includes a variety of column types, including categorical and especially
    text columns, it could be a good candidate for deep learning. If your dataset
    contains columns with nontext BLOB data, such as images, you could get many benefits
    from applying deep learning to the whole dataset instead of taking the more conventional
    approach of applying deep learning only to the BLOB data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异构性* —如果您的数据集完全由连续列组成，您将使用XGBoost等非深度学习方法进行预测，从而获得更好的投资回报。但如果您的数据集包括各种列类型，包括分类列和特别是文本列，那么它可能是一个深度学习的良好候选者。如果您的数据集包含包含非文本BLOB数据（如图像）的列，您可以通过将深度学习应用于整个数据集而不是仅应用于BLOB数据，从而获得许多好处。'
- en: '*Not too imbalanced* —In the refactored streetcar delay dataset, about 2% of
    the records indicated a delay for the route/direction/time slot of the record.
    As described in chapter 6, Keras has parameters for the `fit` command that take
    imbalanced datasets into account. But if the dataset is extremely imbalanced,
    with only a small fraction of a percent belonging to one of the outcomes, it may
    not be possible for the deep learning model to pick up the signal of what characterizes
    the minority outcome.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不太失衡* —在重构的电车延误数据集中，大约2%的记录表明记录的路线/方向/时间段有延误。如第6章所述，Keras的`fit`命令有参数可以考虑到不平衡的数据集。但如果数据集极度失衡，只有极小的一部分属于某个结果，深度学习模型可能无法捕捉到表征少数结果的信号。'
- en: 'Let’s consider a couple of open datasets and apply these guidelines to them
    to get a quick assessment of whether problems associated with these datasets could
    be amenable to a deep learning approach:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些公开的数据集，并将这些指南应用于它们，以快速评估与这些数据集相关的问题是否可以通过深度学习方法解决：
- en: '*Traffic signal vehicle and pedestrian volumes* ([http://mng.bz/ZPw9](http://mng.bz/ZPw9))—This
    dataset comes from the same curated collection as the streetcar delay dataset.
    It contains traffic volume information for a set of intersections in Toronto.
    It would be interesting to use this dataset to predict traffic volume in the future.
    Does this problem lend itself to a deep learning project? Figure 9.14 shows that
    the dataset has a variety of columns, including continuous, categorical, and geospatial
    columns.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交通信号车辆和行人流量* ([http://mng.bz/ZPw9](http://mng.bz/ZPw9))——这个数据集来自与电车延误数据集相同的精选集合。它包含多伦多一组交叉路口的交通流量信息。使用这个数据集来预测未来的交通流量将很有趣。这个问题适合作为深度学习项目吗？图9.14显示，该数据集包含各种列，包括连续列、分类列和地理空间列。'
- en: '![CH09_F14_Ryan](../Images/CH09_F14_Ryan.png)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F14_Ryan](../Images/CH09_F14_Ryan.png)'
- en: Figure 9.14 Columns in the traffic signal vehicle and pedestrian volumes dataset
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.14 交通信号车辆和行人流量数据集的列
- en: Figure 9.15 shows that the distribution of traffic volumes is not too imbalanced.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.15显示，流量分布并不太不平衡。
- en: '![CH09_F15_Ryan](../Images/CH09_F15_Ryan.png)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F15_Ryan](../Images/CH09_F15_Ryan.png)'
- en: Figure 9.15 Distribution of traffic volumes in the traffic signal vehicle and
    pedestrian volumes dataset
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.15 交通信号车辆和行人流量数据集的流量分布
- en: The problem with this dataset is that it is too small—only 2,300 records—so
    despite having an interesting set of columns and decent balance, this dataset
    is not a good candidate for a deep learning project. What about a dataset that
    covers a similar but distinct problem to streetcar delays?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个数据集的问题在于它太小——只有2,300条记录——所以尽管它有一组有趣的列和良好的平衡性，但这个数据集并不适合作为深度学习项目。那么，一个覆盖与电车延误类似但不同的问题的数据集会怎样呢？
- en: '*Toronto subway delay dataset* ([https://open.toronto.ca/dataset/ttc-subway-delay-data](https://open.toronto.ca/dataset/ttc-subway-delay-data))—As
    you can see in figure 9.16, the subway delay dataset includes a range of column
    types, including categorical, continuous, and geospatial. The whole dataset has
    about half a million records, so it is big enough to be interesting without being
    unwieldly.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多伦多地铁延误数据集* ([https://open.toronto.ca/dataset/ttc-subway-delay-data](https://open.toronto.ca/dataset/ttc-subway-delay-data))——如图9.16所示，地铁延误数据集包括多种类型的列，包括分类列、连续列和地理空间列。整个数据集大约有50万条记录，因此它足够大，有趣，但又不至于难以处理。'
- en: Compared with the streetcar delay dataset, this dataset is a bit more balanced,
    because there are about seven times as many delays reported on the subway compared
    with the streetcar system. The interesting aspect of the location data for this
    dataset is that it is exact. Every delay is identified with one of 75 stations
    in the Toronto subway, and the spatial relationship between any two stations is
    easy to encode without needing to use latitude and longitude. Also, at scoring
    time the user could specify the start and end of a trip exactly by selecting subway
    stations from a picklist. Accordingly, it would be easier to incorporate location
    in the training data for a subway delay prediction model than to add location
    data to the streetcar delay model. Overall, the subway delay prediction project
    would be a decent candidate for deep learning.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与电车延误数据集相比，这个数据集稍微平衡一些，因为地铁系统报告的延误是电车系统的约七倍。这个数据集的位置数据的有趣之处在于它是精确的。每个延误都与多伦多地铁的75个车站中的一个相对应，任何两个车站之间的空间关系很容易编码，无需使用经纬度。此外，在评分时，用户可以通过从下拉列表中选择地铁站来精确指定行程的开始和结束。因此，将位置信息纳入地铁延误预测模型的训练数据比向电车延误模型添加位置数据要容易得多。总的来说，地铁延误预测项目是深度学习的一个不错的候选项目。
- en: '![CH09_F16_Ryan](../Images/CH09_F16_Ryan.png)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F16_Ryan](../Images/CH09_F16_Ryan.png)'
- en: Figure 9.16 Subway delay dataset
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.16 地铁延误数据集
- en: Now that we have looked at a couple of structured datasets that could be candidates
    for deep learning, section 9.9 provides an overview of the steps required to prepare
    a new dataset for training a deep learning model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了一些可能适合深度学习的结构化数据集，第9.9节概述了为训练深度学习模型准备新数据集所需的步骤。
- en: 9.9 Preparing the dataset and training the model
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 准备数据集和训练模型
- en: When you have picked a dataset that you want to build a deep learning project
    around, the next challenge is cleaning up the dataset. The examples that you saw
    in chapters 3 and 4 should guide you in dealing with errors and missing values
    in your dataset, although the required cleanup steps will vary depending on the
    dataset and how messy it is.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当你选择了一个想要围绕其构建深度学习项目的dataset后，下一个挑战就是清理这个dataset。你在第3章和第4章中看到的例子应该能指导你处理dataset中的错误和缺失值，尽管所需的清理步骤将根据dataset及其杂乱程度而有所不同。
- en: 'On the subject of cleaning up data, you may ask why cleaning the data was not
    a consideration for the weather data described in section 9.4\. To answer this
    question, I’d like to return to an example from chapter 2: creating a Blu-ray
    disc from an assortment of media (figure 9.17).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据清理的话题，你可能会问为什么在第9.4节中描述的天气数据没有考虑数据清理。为了回答这个问题，我想回到第2章的一个例子：从各种媒体创建蓝光光盘（图9.17）。
- en: '![CH09_F17_Ryan](../Images/CH09_F17_Ryan.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Ryan](../Images/CH09_F17_Ryan.png)'
- en: Figure 9.17 Creating a Blu-ray disc from an assortment of media
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 从各种媒体创建蓝光光盘
- en: 'The point of this example was to demonstrate that as the various bits of media
    were not initially recorded with Blu-ray in mind, datasets that are interesting
    to explore with deep learning were not collected with the intention of applying
    them to machine learning or deep learning. These messy, real-world datasets must
    be cleaned up before they can be used to train a deep learning model. A data source
    like Dark Sky’s weather data or the geocoding data available from the Google API,
    on the other hand, is designed to provide a clean, coherent stream of data that
    does not require cleaning. These data sources are to deep learning what high-definition
    digital video clips are to the Blu-ray disc problem: ready to be incorporated
    without cleanup.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的目的是为了说明，由于各种媒体内容最初并没有考虑到使用蓝光光盘进行记录，因此，那些对深度学习感兴趣的dataset并不是有意用于机器学习或深度学习的应用而收集的。这些杂乱的真实世界dataset在使用之前必须进行清理，以便用于训练深度学习模型。另一方面，像Dark
    Sky的天气数据或从Google API可用的地理编码数据这样的数据源，是为了提供干净、连贯的数据流而设计的，不需要进行清理。这些数据源对于深度学习来说，就像高清数字视频片段对于蓝光光盘问题一样：无需清理即可直接整合。
- en: When you have your new dataset cleaned up, the next step is replacing any non-numeric
    values, such as by replacing categorical values with integer identifiers. You
    can adapt the training code to create a dataframe for your new dataset. As in
    the example of adding weather data, you need to associate the columns you will
    use to train the model to the correct category in the `def_col_lists()` function.
    In addition, you need to identify which column contains the target in your dataset.
    With these changes, you should be able to train a model to make predictions on
    your new dataset and end up with a trained model along with trained pipelines
    for preparing data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你清理完新的dataset后，下一步是替换任何非数值值，例如通过将分类值替换为整数标识符。你可以调整训练代码来创建一个新的dataset的dataframe。就像添加天气数据的例子一样，你需要将用于训练模型的列与`def_col_lists()`函数中的正确类别关联起来。此外，你还需要确定你的dataset中哪个列包含目标值。通过这些更改，你应该能够训练一个模型来对新dataset进行预测，并最终得到一个训练好的模型以及用于准备数据的训练好的管道。
- en: 'Before we go into more detail about how to adapt the code for the streetcar
    delay problem to another subject area, it’s worth revisiting an idea introduced
    in chapter 4: the critical importance of domain knowledge in a machine learning
    project. In chapter 4, I explained that one of the reasons I chose the streetcar
    delay problem as the extended example in this book is that it’s a topic about
    which I happen to have knowledge. As you consider applying deep learning to new
    subject areas, keep this in mind: if you want to have a successful project, whether
    it’s a modest side project done to hone your own skills or a major project that
    your organization is betting its future on, you need to have access to domain
    expertise about the subject area.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更详细地讨论如何将电车延误问题的代码适应到其他领域之前，值得回顾一下在第4章中引入的一个想法：在机器学习项目中领域知识的重要性。在第4章中，我解释了选择电车延误问题作为本书扩展示例的原因之一，即我对这个话题恰好有所了解。当你考虑将深度学习应用于新的领域时，请记住这一点：无论是一个旨在磨练自己技能的适度副项目，还是一个组织押注未来的重大项目，你都需要获取该领域专业知识。
- en: 9.10 Deploying the model with web deployment
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.10 使用Web部署部署模型
- en: 'Now that you have a trained deep learning model and pipelines for your new
    dataset, it’s time to consider how you will deploy the model. If you choose to
    use the web deployment option described in chapter 8 to deploy your new model,
    you will need to make the following updates to the code in the deploy_web directory:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有一个训练好的深度学习模型和针对新数据集的管道，是时候考虑如何部署模型了。如果你选择使用第 8 章中描述的 Web 部署选项来部署你的新模型，你将需要在
    deploy_web 目录中的代码进行以下更新：
- en: 'Update the drop-down lists in home.html (as shown in the following code snippet)
    to reflect the scoring parameters that you want the user to be able to select
    to send to the trained model for scoring. In the streetcar delay prediction web
    deployment, all the scoring parameters are categorical (that is, they can be selected
    from elements in a list). If your model includes continuous scoring parameters
    you will need to add controls in `home` `.html` where the user can enter continuous
    values:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 home.html 中的下拉列表（如下代码片段所示），以反映用户能够选择并发送到训练模型进行评分的评分参数。在街车延误预测的 Web 部署中，所有评分参数都是分类的（也就是说，可以从列表中的元素中选择）。如果你的模型包括连续的评分参数，你需要在
    `home.html` 中添加控件，以便用户可以输入连续值：
- en: '[PRE5]'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Update the `getOption` `()` JavaScript functions in home.html to load the scoring
    parameters for your model into JavaScript variables. The following snippet shows
    the code block in `getOption()` that loads the scoring parameters for the streetcar
    delay prediction model into JavaScript variables:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 home.html 中的 `getOption` `()` JavaScript 函数，以便将模型的评分参数加载到 JavaScript 变量中。以下代码块展示了
    `getOption()` 函数中加载街车延误预测模型评分参数到 JavaScript 变量的代码：
- en: '[PRE6]'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Update the `getOption()` JavaScript functions in home.html to build a target
    URL with the scoring parameters for your model. The following code snippet shows
    the statement in `getOption` `()` that defines the target URL for the streetcar
    delay prediction deployment:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 home.html 中的 `getOption()` JavaScript 函数，以构建包含模型评分参数的目标 URL。以下代码片段展示了 `getOption`
    `()` 中定义街车延误预测部署目标 URL 的语句：
- en: '[PRE7]'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Update the view function for show_prediction.html in flask_server.py to build
    the strings that you want displayed in show_prediction.html for each prediction
    outcome:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 flask_server.py 中 show_prediction.html 的视图函数，以构建你希望在 show_prediction.html
    中显示的字符串，用于每个预测结果：
- en: '[PRE8]'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With these changes, you should be able to use the web deployment from chapter
    8 to do a simple deployment of your new model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些更改，你应该能够使用第 8 章中描述的 Web 部署来简单地部署你的新模型。
- en: 9.11 Deploying the model with Facebook Messenger
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.11 使用 Facebook Messenger 部署模型
- en: If you choose to use the Facebook Messenger deployment approach described in
    chapter 8 for your new problem, you will need to make updates to the scoring code
    in actions.py, including setting `score_cols` to the names of the columns used
    to train your model. You will also need to update the code that sets defaults
    for any values that may not be provided by the user at scoring time. With these
    changes, you have Python code ready for scoring new data points with your trained
    model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择使用第 8 章中描述的 Facebook Messenger 部署方法来解决你的新问题，你将需要更新 actions.py 中的评分代码，包括将
    `score_cols` 设置为训练模型所使用的列名。你还需要更新设置默认值的代码，以防用户在评分时没有提供这些值。通过这些更改，你将拥有准备就绪的 Python
    代码，用于使用训练好的模型评分新的数据点。
- en: The Python code isn’t the whole story for deployment using Rasa with Facebook
    Messenger. You also want to have a natural interface in Facebook Messenger, and
    to accomplish this task, you will need to create a simple Rasa model. You can
    use the examples from the streetcar delay deployment to see how to specify sentence-level
    Rasa training examples in the nlu.md file and multisentence Rasa examples in the
    stories.md file. Defining the correct set of slots is more challenging. I recommend
    that you create a slot to correspond with each column name in `score_cols`. You
    define these slots in the domain.yml file. For the sake of simplicity, you can
    set the type of each slot to text. It’s best to avoid extraneous slots, so if
    you are copying the domain.yml file from the streetcar delay example as a starting
    point, clear out the existing slot values before defining new slot values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Rasa与Facebook Messenger进行部署时，Python代码并不是全部。你还需要在Facebook Messenger中有一个自然的界面，为此，你需要创建一个简单的Rasa模型。你可以从街车延误部署的示例中了解如何指定nlu.md文件中的句子级Rasa训练示例和在stories.md文件中的多句子Rasa示例。定义正确的slot集合更具挑战性。我建议你为`score_cols`中的每个列名创建一个slot。你可以在domain.yml文件中定义这些slot。为了简化，你可以将每个slot的类型设置为text。最好避免不必要的slot，所以如果你是从街车延误示例中复制domain.yml文件作为起点，请在定义新的slot值之前清除现有的slot值。
- en: 'To complete the rest of the deployment of your new model, you will be going
    through the following subset of the steps you followed in chapter 8 to deploy
    the streetcar delay prediction model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成新模型部署的其余部分，你将遵循第8章中部署街车延误预测模型时遵循的以下步骤子集：
- en: Create a directory called new_deploy.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为new_deploy的目录。
- en: 'Run the following command in the deploy directory to set up a basic Rasa environment:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在deploy目录中运行以下命令以设置基本的Rasa环境：
- en: '[PRE9]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Copy the h5 file for your trained model and the pkl files for your pipelines,
    respectively, into the models and pipelines directories.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别将你的训练模型的h5文件和pipelines的pkl文件复制到models和pipelines目录。
- en: Replace the actions.py file in the new_deploy directory with the actions.py
    file that you updated for your new deployment.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将new_deploy目录中的actions.py文件替换为你为新的部署更新的actions.py文件。
- en: Replace the nlu.md and stories.md files in the data subdirectory with the nlu.md
    and stories.md files that you created for your new deployment.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将data子目录中的nlu.md和stories.md文件替换为你为新的部署创建的nlu.md和stories.md文件。
- en: Replace the domain.yml file in the new_deploy directory with the domain.yml
    file for your new deployment.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将new_deploy目录中的domain.yml文件替换为你的新部署的domain.yml文件。
- en: Copy the custom_classes.py and endpoints.yml files from the repo to the new_deploy
    directory.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将repo中的custom_classes.py和endpoints.yml文件复制到新的deploy目录。
- en: Copy the deploy_config.yml config file to the new_deploy directory, and update
    the pipeline and model filename parameters to match the files you copied to the
    pipelines and models directories in step 3, as shown in the next listing.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将deploy_config.yml配置文件复制到新的deploy目录，并更新pipeline和模型文件名参数以匹配步骤3中复制的文件，如下所示。
- en: Listing 9.3 Parameters that need to be updated in the deploy config file
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表9.3 需要在部署配置文件中更新的参数
- en: '[PRE10]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Replace with the filenames of the pipeline files you copied to the pipelines
    directory in step 3.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 替换为步骤3中复制到pipelines目录的pipeline文件名。
- en: ❷ Replace with the name of the trained model h5 file you copied to the models
    directory in step 3.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 替换为步骤3中复制到models目录的训练模型h5文件名。
- en: 'Run the following command in your new_deploy directory to invoke actions.py
    in the Python environment for Rasa:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的new_deploy目录中运行以下命令以在Rasa的Python环境中调用actions.py：
- en: '[PRE11]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the directory where you installed ngrok, invoke ngrok to make your localhost
    available to Facebook Messenger on port 5005\. Here is the command for Windows;
    make a note of the HTTPS forwarding URL in the ngrok output:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你安装ngrok的目录中，调用ngrok以使你的localhost在端口5005上可供Facebook Messenger使用。以下是Windows的命令；注意ngrok输出中的HTTPS转发URL：
- en: '[PRE12]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the following command in the new_deploy directory to train the Rasa model:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在new_deploy目录中运行以下命令以训练Rasa模型：
- en: '[PRE13]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Follow the instructions at http://mng.bz/oRRN to add a new Facebook page. You
    can keep using the same Facebook app you created in chapter 8 for this new deployment.
    Make note of the page access token and app secret; you will need to update the
    credentials.yml file with these values in step 13.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下链接中的说明 http://mng.bz/oRRN 添加一个新的Facebook页面。您可以使用在第8章中创建的相同Facebook应用进行这次新部署。注意记录页面访问令牌和应用程序密钥；您需要在第13步更新credentials.yml文件，并使用这些值。
- en: 'Update the credentials.yml file in the new_deploy directory to set the verify
    token (a string value you choose) and secret and page-access-token (provided during
    the Facebook setup you did in step 12):'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新new_deploy目录中的credentials.yml文件，以设置验证令牌（您选择的字符串值）、密钥和页面访问令牌（在第12步的Facebook设置期间提供）：
- en: '[PRE14]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Run the following command in the new_deploy directory to start the Rasa server,
    using the credentials you set in credentials.yml in step 13:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在new_deploy目录中运行以下命令以启动Rasa服务器，使用第13步在credentials.yml中设置的凭据：
- en: '[PRE15]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the Facebook app you created in chapter 8, choose Messenger -> Settings,
    scroll down to the Webhooks section, and click Edit Callback URL. Replace the
    initial part of the Callback URL value with the HTTPS forwarding URL that you
    noted when you invoked ngrok in step 10\. Enter the verify token that you set
    in credentials.yml in step 13 in the Verify Token field, and click Verify and
    Save.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第8章中创建的Facebook应用中，选择“消息传递”->“设置”，滚动到Webhooks部分，并点击“编辑回调URL”。将回调URL值的前一部分替换为在第10步调用ngrok时记录的HTTPS转发URL。在第13步中设置的verify
    token（您选择的字符串值）输入到“验证令牌”字段中，然后点击“验证并保存”。
- en: In Facebook Messenger (mobile or web application), search for the ID of the
    Facebook page you created in step 12, and enter a query to confirm that your deployed
    model is accessible.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Facebook消息传递（移动或Web应用）中，搜索第12步创建的Facebook页面的ID，并输入查询以确认您的部署模型是可访问的。
- en: We have reviewed the steps required to adapt the approach used for the streetcar
    delay problem to a new structured dataset. Now you have what you need to apply
    the code samples from this book to new problems when you want to apply deep learning
    to structured data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经审查了将用于电车延误问题的方法适应新结构数据集所需的步骤。现在，当您想要将深度学习应用于结构化数据时，您已经有了应用本书中代码示例所需的新问题。
- en: 9.12 Adapting the approach in this book to a different dataset
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.12 将本书中的方法适应不同的数据集
- en: To make it easier to apply the approach in this book to a new problem area,
    we are going to go through the process of adapting the code that we used for the
    streetcar delay prediction problem to a new dataset. We aren’t going to work through
    the entire end-to-end problem—only from the initial dataset to a minimal trained
    deep learning model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使将本书中的方法应用于新的问题领域更容易，我们将通过将用于电车延误预测问题的代码适应新数据集的过程。我们不会处理整个端到端问题——只从初始数据集到最小训练的深度学习模型。
- en: We want to find a dataset that is big enough to give deep learning a fighting
    chance (a dataset with a record count at least in the tens of thousands), but
    not so big that the data volume will become the main focus of the exercise. I
    examined the popular machine learning competition site Kaggle to find a problem
    with tabular structured data that would be a good fit. I found that the problem
    of predicting prices for Airbnb properties in New York City ([https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data))
    had a dataset that was interesting and might be a decent candidate for adapting
    the methodology described in this book. Figure 9.18 shows a snippet of data from
    this dataset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一个足够大的数据集，以便给深度学习一个公平的机会（记录数至少在数千条），但又不能太大，以至于数据量成为练习的主要焦点。我检查了流行的机器学习竞赛网站Kaggle，以找到一个适合的表格结构化数据问题。我发现预测纽约市Airbnb物业价格的[https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)问题有一个有趣的数据集，可能适合适应本书中描述的方法。图9.18显示了该数据集的数据片段。
- en: '![CH09_F18_Ryan](../Images/CH09_F18_Ryan.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F18_Ryan](../Images/CH09_F18_Ryan.png)'
- en: Figure 9.18 Example records in the Airbnb NYC dataset
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 Airbnb纽约市数据集的示例记录
- en: 'The dataset has slightly fewer than 89,000 records across 16 columns, so the
    size and complexity of the dataset are in the right ballpark. Let’s examine what’s
    in each of the columns:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含略少于89,000条记录，分布在16列中，因此数据集的大小和复杂性是合适的。让我们检查每一列的内容：
- en: '`id` —Numeric identifier for the listing'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` —列表的数字标识符'
- en: '`name` —Description of the listing'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` —列表的描述'
- en: '`host_id` —Numeric identifier for the host associated with the listing'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_id` —与列表关联的主人的数字标识符'
- en: '`host_name` —Name of the host associated with the listing'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_name` —与列表关联的主人的名字'
- en: '`neighbourhood_group` —New York borough of the listing: Manhattan, Brooklyn,
    Queens, Bronx, or Staten Island'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood_group` —列表所在的纽约市行政区：曼哈顿、布鲁克林、皇后区、布朗克斯或斯塔滕岛'
- en: '`neighbourhood` —Neighborhood of the listing'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood` —列表所在的社区'
- en: '`latitude` —Latitude of the listing'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latitude` —列表的纬度'
- en: '`longitude` —Longitude of the listing'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`longitude` —列表的经度'
- en: '`room_type` —Room type of the listing: entire dwelling, private room, or shared
    room'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`room_type` —列表的房间类型：整个住宅、私人房间或共享房间'
- en: '`price` —Price of the listing (the target of the deep learning model)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`price` —列表的价格（深度学习模型的目标）'
- en: '`minimum_nights` —Minimum number of nights for which the listing can be booked'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minimum_nights` —列表可以预订的最少夜数'
- en: '`number_of_reviews` —Number of reviews available for the listing on the Airbnb
    site'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_reviews` —Airbnb网站上可用的列表评论数量'
- en: '`last_review` —Date of the most recent review for the listing'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_review` —列表最近一次评论的日期'
- en: '`reviews_per_month` —Average number or reviews per month for the listing'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reviews_per_month` —列表每月的平均评论数量'
- en: '`calculated_host_listings_count` —Number of listings associated with the host
    for this listing'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculated_host_listings_count` —与该列表关联的主人的列表数量'
- en: '`availability_365` —Proportion of the year that the listing is available for
    rental'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`availability_365` —该列表一年中可供出租的比例'
- en: 'We get an interesting mix of types with these columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这些列类型的一个有趣的混合：
- en: '*Continuous* —`price` , `minimum_nights,` `number_of_reviews` , `reviews_per_month`
    , `calculated_host_listings_count` , `availability_365`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续* —`price`、`minimum_nights`、`number_of_reviews`、`reviews_per_month`、`calculated_host_listings_count`、`availability_365`'
- en: '*Categorical* —`neighbourhood_group` , `neighbourhood` , `room_type` , `host_id`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类* —`neighbourhood_group`、`neighbourhood`、`room_type`、`host_id`'
- en: '*Text* —`name` and maybe `host_name`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本* —`name` 和可能 `host_name`'
- en: In addition to these easily categorized columns, there are `id` (not interesting
    from a model training perspective), `longitude` and `latitude` (for the purposes
    of this exercise, we will depend on `neighborhood` for location), and `last_review`
    (for the purposes of this exercise, we won’t use this column).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些容易分类的列之外，还有 `id`（从模型训练的角度来看并不有趣）、`longitude` 和 `latitude`（在本练习中，我们将依赖 `neighborhood`
    来确定位置），以及 `last_review`（在本练习中，我们不会使用此列）。
- en: Compared with the streetcar delay dataset, the Airbnb NYC dataset is much less
    messy. First, let’s look at the count of missing values and unique values in each
    column (figure 9.19).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与电车延误数据集相比，Airbnb纽约市数据集的混乱程度要低得多。首先，让我们看看每个列中缺失值和唯一值的数量（图9.19）。
- en: Compared with the streetcar delay dataset, the Airbnb NYC dataset has fewer
    columns with missing values. Further, all the categorical columns (`neighbourhood_
    group` , `neighbourhood` , and `host_name`) seem to have legitimate numbers of
    values. By comparison, the direction, location, and route columns in the streetcar
    delay dataset all had extraneous values. The direction column in the raw streetcar
    delay dataset has 15 distinct values but only 5 valid values, for example.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与电车延误数据集相比，Airbnb纽约市数据集具有较少的缺失值列。此外，所有分类列（`neighbourhood_group`、`neighbourhood`
    和 `host_name`）似乎都有合理的值数量。相比之下，电车延误数据集中的方向、位置和路线列都包含了一些无关的值。例如，原始电车延误数据集中的方向列有15个不同的值，但只有5个有效值。
- en: The relative lack of messiness in the Airbnb NYC dataset highlights one of the
    problems with the datasets available from Kaggle. Although these datasets are
    useful for exercising aspects of machine learning, and although participating
    in competitions on Kaggle is a great way to learn, the curated and sanitized datasets
    used for the competitions are no substitute for real-world datasets. As you saw
    in chapters 3 and 4, a real-world dataset is messy in unexpected ways and can
    take nontrivial work to prepare for training a deep learning model. The limited
    amount of data preparation that is necessary for the Airbnb dataset (ingesting
    the CSV file into a Pandas dataframe and replacing missing values with defaults)
    is contained in the airbnb_data_preparation notebook.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Airbnb纽约数据集中相对较少的杂乱突显了从Kaggle可用的数据集的问题之一。尽管这些数据集对于练习机器学习的各个方面很有用，尽管参加Kaggle的比赛是学习的好方法，但用于比赛的精心策划和清洗的数据集并不能替代真实世界的数据集。正如您在第3章和第4章中看到的，真实世界的数据集在意外的方式下很杂乱，并且需要非平凡的工作来准备训练深度学习模型。Airbnb数据集所需的数据准备量有限（将CSV文件导入Pandas
    dataframe并用默认值替换缺失值）包含在airbnb_data_preparation笔记本中。
- en: '![CH09_F19_Ryan](../Images/CH09_F19_Ryan.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F19_Ryan](../Images/CH09_F19_Ryan.png)'
- en: Figure 9.19 Column characteristics for the Airbnb NYC dataset
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 Airbnb纽约数据集的列特征
- en: The next listing shows the files from the repo that are related to this example.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 下一列表显示了与此示例相关的存储库中的文件。
- en: Listing 9.4 Code in the repo related to the Airbnb pricing prediction example
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4与Airbnb定价预测示例相关的代码存储库中的代码
- en: '[PRE16]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Saved, cleaned-up dataset output from airbnb_data_preparation notebook
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ airbnb_data_preparation笔记本保存的清洗后的数据集输出
- en: ❷ Data preparation notebook
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据准备笔记本
- en: ❸ Config file for data preparation
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据准备配置文件
- en: ❹ Model training notebook
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 模型训练笔记本
- en: ❺ Config file for model training
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模型训练配置文件
- en: 'The airbnb_data_preparation notebook saves a pickled version of the cleaned-up
    dataframe that can be used as input to the deep learning model training notebook,
    airbnb_model_training. This notebook is a simplified version of the model training
    notebook for the streetcar delay prediction problem. The target for this simple
    model is whether the price for an Airbnb property is going to be below (`0`) or
    above (`1`) the average price. The key changes in this notebook from the version
    used in the main example in this book include the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: airbnb_data_preparation笔记本保存了清洗后的dataframe的pickle版本，该版本可以用作输入到深度学习模型训练笔记本airbnb_model_training。这个笔记本是街车延误预测问题模型训练笔记本的简化版本。这个简单模型的目标是Airbnb物业的价格是否会低于（`0`）或高于（`1`）平均价格。与本书主要示例中使用的版本相比，这个笔记本的关键变化包括以下内容：
- en: Membership in the column lists for categorical, continuous, and text columns
    is set in the config file airbnb_model_training_config.yml rather than in the
    notebook itself (see the next listing).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表列的成员资格（分类、连续和文本列）在配置文件airbnb_model_training_config.yml中设置，而不是在笔记本本身中（见下一列表）。
- en: Listing 9.5 Parameters for column categories for the Airbnb price prediction
    model
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表9.5 Airbnb价格预测模型列类别的参数
- en: '[PRE17]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ List of categorical columns
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 分类列列表
- en: ❷ List of continuous columns
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 连续列列表
- en: ❸ List of text columns
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 文本列列表
- en: ❹ List of columns excluded from training the model
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 从模型训练中排除的列列表
- en: The dataset is read in directly from the pickle file produced by the airbnb_data
    _preparation notebook and fed into the pipeline. By contrast, the model training
    notebook for the streetcar delay prediction model contains extensive code to refactor
    the dataset.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集直接从由airbnb_data_preparation笔记本生成的pickle文件中读取，并输入到管道中。相比之下，用于街车延误预测模型的模型训练笔记本包含大量代码来重构数据集。
- en: 'To examine how the model training works with the Airbnb NYC dataset, we’ll
    run the same series of experiments we ran for the streetcar delay prediction model
    in chapter 6, using the following columns to train the model:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查模型训练如何与Airbnb纽约数据集一起工作，我们将运行第6章中用于街车延误预测模型的相同一系列实验，使用以下列来训练模型：
- en: '*Continuous* —`minimum_nights` , `number_of_reviews` , `reviews_per_month`
    , `calculated_host_listings_count`'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续* —`minimum_nights`、`number_of_reviews`、`reviews_per_month`、`calculated_host_listings_count`'
- en: '*Categorical* —`neighbourhood_group` , `neighbourhood` , `room_type`'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类* —`neighbourhood_group`、`neighbourhood`、`room_type`'
- en: You can see the results of these experiments on the Airbnb NYC model in figure
    9.20.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图9.20中看到这些实验在Airbnb纽约模型上的结果。
- en: As you can see, with a minimal amount of code changes, we were able to get reasonable
    results with the Airbnb NYC dataset. The adaptation described in this section
    is by no means complete, but it demonstrates how you can adapt the approach described
    in this book to train a deep learning model for a new dataset.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，通过最小量的代码更改，我们能够使用Airbnb纽约市数据集获得合理的结果。本节中描述的适应方法远非完整，但它展示了如何将本书中描述的方法适应于训练新的数据集的深度学习模型。
- en: '![CH09_F20_Ryan](../Images/CH09_F20_Ryan.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F20_Ryan](../Images/CH09_F20_Ryan.png)'
- en: Figure 9.20 Airbnb NYC model experiment summary
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 Airbnb纽约市模型实验摘要
- en: 9.13 Resources for additional learning
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.13 额外学习资源
- en: 'We have covered a broad range of technical issues in this book to build an
    end-to-end deep learning solution to the streetcar delay prediction problem. But
    we have only scratched the surface when it comes to the incredibly rich and fast-moving
    world of deep learning. Here are some additional resources that I recommend if
    you want to get a deeper background in deep learning:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们涵盖了构建端到端深度学习解决方案以解决电车延误预测问题的广泛技术问题。但当我们谈到深度学习这个极其丰富且快速发展的世界时，我们只是触及了表面。以下是一些额外的资源，如果你想要更深入地了解深度学习，我推荐你查看：
- en: '*Online* *deep learning overview courses* —The fast.ai Practical Deep Learning
    for Coders course ([https://course.fast.ai/](https://course.fast.ai/)) is an accessible
    introduction to deep learning that focuses on learning by doing. It covers classic
    deep learning applications like image classification as well as recommender systems
    and other applications of deep learning. I credit this course with sparking my
    interest in applying deep learning to structured data. You can follow along with
    the course online for free while using the course forum to connect with other
    learners. The instructor, Jeremy Howard, is incredibly clear and passionate.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在线* *深度学习概述课程* — fast.ai的《面向程序员的实用深度学习》课程([https://course.fast.ai/](https://course.fast.ai/))是一个易于理解的深度学习入门课程，侧重于通过实践学习。它涵盖了经典深度学习应用，如图像分类，以及推荐系统和其他深度学习应用。我感谢这门课程激发了我将深度学习应用于结构化数据的兴趣。你可以免费在线跟随课程学习，同时使用课程论坛与其他学习者建立联系。讲师Jeremy
    Howard讲解清晰且充满激情。'
- en: A contrasting approach to learning about deep learning is the deeplearning.ai
    Deep Learning Specialization ([http://mng.bz/PPm5](http://mng.bz/PPm5)). This
    series of online courses, taught by deep learning legend Andrew Ng, starts with
    the theoretical basics of deep learning and grows from there to cover coding topics.
    You can audit the deeplearning.ai course for free, but there is a charge to get
    your coursework marked and to get a certificate on completion of the courses.
    The specialization is split into five topics that cover technical and practical
    problems related to deep learning. The coding work in fast.ai is more interesting,
    but deeplearning.ai does a better job on the math behind deep learning. If you
    have the time and energy, doing both of these programs will give you a well-rounded
    grounding in deep learning.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种了解深度学习的方法是deeplearning.ai的《深度学习专项课程》([http://mng.bz/PPm5](http://mng.bz/PPm5))。由深度学习传奇人物Andrew
    Ng教授的这一系列在线课程从深度学习的理论基础知识开始，逐步扩展到涵盖编码主题。你可以免费审计deeplearning.ai课程，但若要获得课程评分和完成课程后的证书，则需要付费。该专项课程分为五个主题，涵盖了与深度学习相关的技术和实际问题。fast.ai中的编码工作更有趣，但deeplearning.ai在深度学习背后的数学方面做得更好。如果你有时间和精力，完成这两个项目将为你提供一个全面的深度学习基础。
- en: '*Books* —I introduced Francois Chollet’s [Deep Learning with Python](https://livebook.manning.com/book/deep-learning-with-python/about-this-book/)
    in chapter 1 as a great overview of applying deep learning with Python. If you
    want to get more experience with the PyTorch library that the fast.ai course uses,
    [Deep Learning with PyTorch by Eli Stevens et al.](https://www.manning.com/books/deep-learning-with-pytorch?query=deep%20learning)
    is a great resource. [Deep Learning for Natural Language Processing by Stephan
    Raaijmakers](https://www.manning.com/books/deep-learning-for-natural-language-processing?query=deep%20learning)
    is a good example of a book that focuses on a particular application of deep learning,
    as does [Deep Learning for Vision Systems by Mohamed Elgendy](https://www.manning.com/books/deep-learning-for-vision-systems?query=deep%20learning).
    If you want to examine deep learning in languages other than Python, [Deep Learning
    with R](https://www.manning.com/books/deep-learning-with-r?query=deep%20learning)
    , by François Chollet with J. J. Allaire, provides an exploration of deep learning
    with the other classic machine learning language, and [Deep Learning with JavaScript
    by Shanqing Cai et al.](https://www.manning.com/books/deep-learning-with-javascript?query=deep%20learning)
    shows you how to take advantage of TensorFlow.js to create deep learning models
    entirely in the lingua franca of web development, JavaScript. Finally, the main
    instructor for the fast.ai course, Jeremy Howard, is co-author of [Deep Learning
    for Coders with Fastai & PyTorch (O’Reilly Media, 2020)](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)
    , which not only expands on the fast.ai course material, but also contains a section
    on deep learning with structured data.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*书籍* —我在第一章介绍了Francois Chollet的[《Python深度学习》](https://livebook.manning.com/book/deep-learning-with-python/about-this-book/)，这是一本关于如何使用Python应用深度学习的优秀概述。如果你想获得更多使用fast.ai课程中使用的PyTorch库的经验，[Eli
    Stevens等人所著的《PyTorch深度学习》](https://www.manning.com/books/deep-learning-with-pytorch?query=deep%20learning)是一个很好的资源。[Stephan
    Raaijmakers所著的《自然语言处理深度学习》](https://www.manning.com/books/deep-learning-for-natural-language-processing?query=deep%20learning)是一本专注于深度学习特定应用的书籍，[Mohamed
    Elgendy所著的《视觉系统深度学习》](https://www.manning.com/books/deep-learning-for-vision-systems?query=deep%20learning)也是如此。如果你想在其他语言中考察深度学习，[François
    Chollet和J. J. Allaire合著的《R语言深度学习》](https://www.manning.com/books/deep-learning-with-r?query=deep%20learning)提供了使用其他经典机器学习语言探索深度学习的方法，[Shanqing
    Cai等人合著的《JavaScript深度学习》](https://www.manning.com/books/deep-learning-with-javascript?query=deep%20learning)则展示了如何利用TensorFlow.js在Web开发的通用语言JavaScript中创建深度学习模型。最后，fast.ai课程的主要讲师Jeremy
    Howard是[《Fastai & PyTorch深度学习编码者指南》（O’Reilly Media，2020）](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)的合著者，这本书不仅扩展了fast.ai课程的内容，还包含了一个关于结构化数据深度学习的章节。'
- en: '*Other resources* —In addition to online courses and books, there are many
    sources for learning more about deep learning. In fact, there is such a huge amount
    of material being produced about deep learning that it’s hard to identify the
    best sources. To see what is happening on the cutting edge, the arXiv moderated
    list for recent machine learning submissions ([https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent))
    is a good starting point, although the volume and challenging nature of the material
    can be daunting. I depend on Medium, especially the Towards Data Science publication
    ([https://towardsdatascience.com](https://towardsdatascience.com)), for a regular
    stream of accessible, bite-size articles on deep learning topics. Medium is also
    a friendly place to write articles to share your technical accomplishments with
    other people who are interested in machine learning.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*其他资源* —除了在线课程和书籍之外，还有许多关于深入学习更多知识的来源。实际上，关于深度学习的材料如此之多，以至于很难确定最佳来源。为了了解前沿动态，arXiv对最近机器学习提交的审阅列表([https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent))是一个很好的起点，尽管材料的数量和挑战性可能会让人望而却步。我依赖Medium，尤其是Towards
    Data Science出版物([https://towardsdatascience.com](https://towardsdatascience.com))，以获取关于深度学习主题的定期、易于消化的文章。Medium也是一个友好的地方，可以撰写文章与对机器学习感兴趣的其他人分享你的技术成就。'
- en: In addition to these resources for deep learning, there are some interesting
    developments in the area of applying deep learning to structured data. Google’s
    TabNet, for example ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442)),
    is aimed squarely at the problem of applying deep learning to structured data.
    The article at [http://mng.bz/ v99x](https://shortener.manning.com/v99x) provides
    a great summary of the TabNet approach as well as a useful guide to applying TabNet
    to new problems. The article explains that TabNet implements an attention mechanism
    ([http://mng.bz/JDmQ](http://mng.bz/JDmQ)), which allows the network to learn
    which subset of the input to apply focus to and facilitates explainability (identifying
    which inputs are significant for the output).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些用于深度学习的资源之外，在将深度学习应用于结构化数据领域的进展方面也有一些有趣的发展。例如，Google的TabNet ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442))
    直接针对将深度学习应用于结构化数据的问题。在 [http://mng.bz/ v99x](https://shortener.manning.com/v99x)
    的文章中，提供了对TabNet方法的精彩总结以及将TabNet应用于新问题的实用指南。文章解释说，TabNet实现了一个注意力机制 ([http://mng.bz/JDmQ](http://mng.bz/JDmQ))，这使得网络能够学习对输入的哪个子集进行关注，并促进了可解释性（识别哪些输入对输出是重要的）。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can add additional data sources to the dataset used to train the streetcar
    delay prediction model. For example, you can add information about subsets of
    the streetcar routes to allow your users to get predictions that are focused on
    a specific part of a streetcar route. You can also incorporate weather data to
    account for the impact that extreme weather can have on streetcar delays.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以向用于训练电车延误预测模型的数据集中添加额外的数据源。例如，你可以添加关于电车路线子集的信息，以便你的用户能够获得针对电车路线特定部分的预测。你还可以结合天气数据来考虑极端天气对电车延误的影响。
- en: The approach described in this book can be applied to other datasets. With some
    minor modifications, you can adapt the streetcar delay data preparation and model
    training notebooks to train a basic model to predict the prices of Airbnb properties
    in New York City.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书描述的方法可以应用于其他数据集。通过一些小的修改，你可以将电车延误数据准备和模型训练笔记本适应，以训练一个基本模型来预测纽约市Airbnb物业的价格。
- en: When you are assessing whether a particular structured data problem is a good
    candidate for deep learning, you should confirm that the dataset is large enough
    (at least tens of thousands of records), varied enough (a variety of types of
    columns), and balanced enough (enough examples for the deep learning model to
    pick up a signal) to be applicable to deep learning.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你在评估某个结构化数据问题是否适合深度学习时，你应该确认数据集足够大（至少有数万条记录），足够多样化（各种类型的列），并且足够平衡（有足够的示例，以便深度学习模型能够捕捉到信号），以便适用于深度学习。
- en: You can use ngrok to make the web deployment from chapter 8 available to users
    outside your local system.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用ngrok将第8章中的Web部署提供给本地系统之外的用户。
- en: The body of information about deep learning keeps growing all the time. You
    can take advantage of books that explore deep learning from different perspectives,
    such as other programming languages (such as JavaScript) or other application
    areas (such as natural language processing). In addition to books, there are excellent
    online resources, including courses, blogs, and scholarly articles.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的知识体系一直在不断增长。你可以利用从不同角度探讨深度学习的书籍，例如其他编程语言（如JavaScript）或其他应用领域（如自然语言处理）。除了书籍之外，还有优秀的在线资源，包括课程、博客和学术论文。
