- en: 'Chapter 13\. Introducing automatic optimization: let’s build a deep learning
    framework'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13章：介绍自动优化：让我们构建一个深度学习框架
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章**'
- en: What is a deep learning framework?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习框架？
- en: Introduction to tensors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量简介
- en: Introduction to autograd
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动求导简介
- en: How does addition backpropagation work?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加法反向传播是如何工作的？
- en: How to learn a framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何学习一个框架
- en: Nonlinearity layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性层
- en: The embedding layer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层
- en: The cross-entropy layer
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵层
- en: The recurrent layer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环层
- en: “Whether we are based on carbon or on silicon makes no fundamental difference;
    we should each be treated with appropriate respect.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “无论我们基于碳还是硅，在本质上都没有区别；我们都应该得到适当的尊重。”
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Arthur C. Clarke, *2010: Odyssey Two* (1982)*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*亚瑟·C·克拉克，2010年：《2001太空漫游》（1982年）*'
- en: What is a deep learning framework?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是深度学习框架？
- en: Good tools reduce errors, speed development, and increase runtime performance
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 好的工具可以减少错误，加快开发速度，并提高运行时性能
- en: If you’ve been reading about deep learning for long, you’ve probably come across
    one of the major frameworks such as PyTorch, TensorFlow, Theano (recently deprecated),
    Keras, Lasagne, or DyNet. Framework development has been extremely rapid over
    the past few years, and, despite all frameworks being free, open source software,
    there’s a light spirit of competition and comradery around each framework.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你长期关注深度学习，你可能已经遇到了一些主要的框架，例如PyTorch、TensorFlow、Theano（最近已弃用）、Keras、Lasagne或DyNet。在过去几年中，框架的发展非常迅速，尽管所有框架都是免费的开源软件，但每个框架周围都有一股竞争和团结的气氛。
- en: Thus far, I’ve avoided the topic of frameworks because, first and foremost,
    it’s extremely important for you to know what’s going on under the hood of these
    frameworks by implementing algorithms yourself (from scratch in NumPy). But now
    we’re going to transition into using a framework, because the networks you’ll
    be training next—long short-term memory networks (LSTMs)—are very complex, and
    NumPy code describing their implementation is difficult to read, use, or debug
    (gradients are flying everywhere).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我一直在避免讨论框架的话题，因为首先，了解这些框架底层的工作原理对于你来说极其重要，这可以通过自己实现算法（从NumPy的零开始）来实现。但现在我们将过渡到使用框架，因为接下来你将要训练的网络——长短期记忆网络（LSTMs）——非常复杂，描述它们实现的NumPy代码难以阅读、使用或调试（梯度无处不在）。
- en: It’s exactly this code complexity that deep learning frameworks were created
    to mitigate. Especially if you wish to train a neural network on a GPU (giving
    10–100× faster training), a deep learning framework can significantly reduce code
    complexity (reducing errors and increasing development speed) while also increasing
    runtime performance. For these reasons, their use is nearly universal within the
    research community, and a thorough understanding of a deep learning framework
    will be essential on your journey toward becoming a user or researcher of deep
    learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架的创建正是为了减轻这种代码复杂性。特别是如果你希望在GPU上训练神经网络（提供10-100倍的训练速度），深度学习框架可以显著减少代码复杂性（减少错误并提高开发速度），同时提高运行时性能。出于这些原因，它们在研究社区中几乎被普遍使用，对深度学习框架的深入了解将成为你成为深度学习用户或研究人员的旅程中必不可少的。
- en: But we won’t jump into any deep learning frameworks you’ve heard of, because
    that would stifle your ability to learn about what complex models (such as LSTMs)
    are doing under the hood. Instead, you’ll build a light deep learning framework
    according to the latest trends in framework development. This way, you’ll have
    no doubt about what frameworks do when using them for complex architectures. Furthermore,
    building a small framework yourself should provide a smooth transition to using
    actual deep learning frameworks, because you’ll already be familiar with the API
    and the functionality underneath it. I found this exercise beneficial myself,
    and the lessons learned in building my own framework are especially useful when
    attempting to debug a troublesome model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不会跳入你听说过的任何深度学习框架，因为这会阻碍你了解复杂模型（如LSTMs）底层的工作原理。相反，你将根据框架发展的最新趋势构建一个轻量级的深度学习框架。这样，你将毫无疑问地了解框架在用于复杂架构时的作用。此外，自己构建一个小框架应该会为使用实际的深度学习框架提供一个平稳的过渡，因为你已经熟悉了API及其底层的功能。我发现这项练习很有益，我在构建自己的框架中学到的教训在尝试调试麻烦的模型时特别有用。
- en: How does a framework simplify your code? Abstractly, it eliminates the need
    to write code that you’d repeat multiple times. Concretely, the most beneficial
    pieces of a deep learning framework are its support for automatic backpropagation
    and automatic optimization. These features let you specify only the forward propagation
    code of a model, with the framework taking care of backpropagation and weight
    updates automatically. Most frameworks even make the forward propagation code
    easier by providing high-level interfaces to common layers and loss functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 框架是如何简化你的代码的？抽象地说，它消除了多次重复编写代码的需要。具体来说，深度学习框架最有益的部分是其对自动反向传播和自动优化的支持。这些功能让你只需指定模型的正向传播代码，框架会自动处理反向传播和权重更新。大多数框架甚至通过提供高级接口来简化常见的层和损失函数，使正向传播代码更容易编写。
- en: Introduction to tensors
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量简介
- en: Tensors are an abstract form of vectors and matrices
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量是向量和矩阵的抽象形式
- en: 'Up to this point, we’ve been working exclusively with vectors and matrices
    as the basic data structures for deep learning. Recall that a matrix is a list
    of vectors, and a vector is a list of scalars (single numbers). A *tensor* is
    the abstract version of this form of nested lists of numbers. A vector is a one-dimensional
    tensor. A matrix is a two-dimensional tensor, and higher dimensions are referred
    to as *n*-dimensional tensors. Thus, the beginning of a new deep learning framework
    is the construction of this basic type, which we’ll call `Tensor`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用向量和矩阵作为深度学习的基本数据结构。回想一下，矩阵是一系列向量的列表，而向量是一系列标量（单个数字）的列表。张量是这种嵌套数字列表形式的抽象版本。向量是一维张量。矩阵是二维张量，更高维度的称为*n*维张量。因此，一个新的深度学习框架的开始是构建这种基本类型，我们将称之为`Tensor`：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is the first version of this basic data structure. Note that it stores
    all the numerical information in a NumPy array (`self.data`), and it supports
    one tensor operation (addition). Adding more operations is relatively simple:
    create more functions on the tensor class with the appropriate functionality.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这种基本数据结构的第一个版本。请注意，它将所有数值信息存储在NumPy数组（`self.data`）中，并且支持一个张量操作（加法）。添加更多操作相对简单：在张量类上创建更多具有适当功能的功能。
- en: Introduction to automatic gradient computation (autograd)
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动梯度计算（autograd）简介
- en: Previously, you performed backpropagation by hand. Let’s make it automatic!
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 以前，你手动执行了反向传播。让我们让它自动化吧！
- en: 'In [chapter 4](kindle_split_012.xhtml#ch04), you learned about derivatives.
    Since then, you’ve been computing derivatives by hand for each neural network
    you train. Recall that this is done by moving backward through the neural network:
    first compute the gradient at the output of the network, then use that result
    to compute the derivative at the next-to-last component, and so on until all weights
    in the architecture have correct gradients. This logic for computing gradients
    can also be added to the tensor object. Let me show you what I mean. New code
    is in **bold**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](kindle_split_012.xhtml#ch04)中，你学习了关于导数的内容。从那时起，你一直在为每个训练的神经网络手动计算导数。回想一下，这是通过在神经网络中向后移动来完成的：首先计算网络的输出处的梯度，然后使用该结果来计算下一个组件的导数，依此类推，直到架构中的所有权重都有正确的梯度。这种计算梯度的逻辑也可以添加到张量对象中。让我向你展示我的意思。新的代码以**粗体**显示：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This method introduces two new concepts. First, each tensor gets two new attributes.
    `creators` is a list containing any tensors used in the creation of the current
    tensor (which defaults to `None`). Thus, when the two tensors `x` and `y` are
    added together, `z` has two `creators`, `x` and `y`. `creation_op` is a related
    feature that stores the instructions `creators` used in the creation process.
    Thus, performing `z` = `x` + `y` creates a *computation graph* with three nodes
    (`x`, `y`, and `z`) and two edges (`z` -> `x` and `z` -> `y`). Each edge is labeled
    by the `creation_op add`. This graph allows you to recursively backpropagate gradients.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法引入了两个新概念。首先，每个张量都获得两个新属性。`creators`是一个列表，包含用于创建当前张量的任何张量（默认为`None`）。因此，当两个张量`x`和`y`相加时，`z`有两个`creators`，即`x`和`y`。`creation_op`是一个相关功能，它存储创建过程中使用的`creators`指令。因此，执行`z`
    = `x` + `y`创建了一个包含三个节点（`x`、`y`和`z`）和两个边（`z` -> `x`和`z` -> `y`）的计算图。每个边都标记为`creation_op
    add`。此图允许你递归地反向传播梯度。
- en: '![](Images/f0235-01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0235-01.jpg)'
- en: The first new concept in this implementation is the automatic creation of this
    graph whenever you perform math operations. If you took `z` and performed further
    operations, the graph would continue with whatever resulting new variables pointed
    back to `z`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中引入的第一个新概念是，每当进行数学运算时，自动创建这个图。如果你对`z`进行了进一步的操作，图将继续与指向`z`的任何结果新变量相关联。
- en: 'The second new concept introduced in this version of `Tensor` is the ability
    to use this graph to compute gradients. When you call `z .backward()`, it sends
    the correct gradient for `x` and `y` given the function that was applied to create
    `z` (`add`). Looking at the graph, you place a vector of gradients (`np.array([1,1,1,1,1])`)
    on `z`, and then they’re applied to their parents. As you learned in [chapter
    4](kindle_split_012.xhtml#ch04), backpropagating through addition means also applying
    addition when backpropagating. In this case, because there’s only one gradient
    to `add` into `x` or `y`, you copy the gradient from `z` onto `x` and `y`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Tensor`的这个版本中引入的第二个新概念是使用这个图来计算梯度。当你调用`z.backward()`时，它会根据应用于创建`z`（`add`）的函数，发送`x`和`y`的正确梯度。查看图，你将一个梯度向量（`np.array([1,1,1,1,1])`）放在`z`上，然后它们被应用到它们的父节点上。正如你在[第4章](kindle_split_012.xhtml#ch04)中学到的，通过加法进行反向传播意味着在反向传播时也要应用加法。在这种情况下，因为只有一个梯度要加到`x`或`y`上，所以你将`z`上的梯度复制到`x`和`y`上：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Perhaps the most elegant part of this form of autograd is that it works recursively
    as well, because each vector calls `.backward()` on all of its `self.creators`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式的autograd最优雅的部分可能是它还可以递归地工作，因为每个向量都会在其所有`self.creators`上调用`.backward()`：
- en: '|'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '| **Output**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '| **输出**'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: A quick checkpoint
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个快速的检查点
- en: Everything in Tensor is another form of lessons already learned
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tensor中的所有内容都是已经学到的知识的另一种形式
- en: Before moving on, I want to first acknowledge that even if it feels like a bit
    of a stretch or a heavy lift to think about gradients flowing over a graphical
    structure, this is nothing new compared to what you’ve already been working with.
    In the previous chapter on RNNs, you forward propagated in one direction and then
    back propagated across a (virtual graph) of activations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想首先承认，即使考虑梯度在图形结构上流动可能感觉有点牵强或费劲，但与你已经使用过的内容相比，这并不是什么新鲜事。在关于RNN的上一章中，你在一个方向上进行了正向传播，然后在（虚拟图）激活上进行了反向传播。
- en: You just didn’t explicitly encode the nodes and edges in a graphical data structure.
    Instead, you had a list of layers (dictionaries) and hand-coded the correct order
    of forward and backpropagation operations. Now you’re building a nice interface
    so you don’t have to write as much code. This interface lets you backpropagate
    recursively instead of having to handwrite complicated backprop code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你并没有在图形数据结构中明确编码节点和边。相反，你有一个层的列表（字典），并手动编码了正向和反向传播操作的正确顺序。现在你正在构建一个很好的接口，这样你就不需要写那么多代码了。这个接口让你可以递归地进行反向传播，而无需手动编写复杂的反向传播代码。
- en: This chapter is only somewhat theoretical. It’s mostly about commonly used engineering
    practices for learning deep neural networks. In particular, this notion of a graph
    that gets built during forward propagation is called a *dynamic computation graph*
    because it’s built on the fly during forward prop. This is the type of autograd
    present in newer deep learning frameworks such as DyNet and PyTorch. Older frameworks
    such as Theano and TensorFlow have what’s called a *static computation graph*,
    which is specified before forward propagation even begins.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要涉及一些理论性的内容。它主要讲述的是学习深度神经网络时常用的工程实践。特别是，在正向传播过程中构建的这种图结构被称为**动态计算图**，因为它是在正向传播过程中即时构建的。这种类型的autograd存在于较新的深度学习框架中，如DyNet和PyTorch。而像Theano和TensorFlow这样的旧框架则有一个所谓的**静态计算图**，它是在正向传播开始之前就定义好的。
- en: In general, dynamic computation graphs are easier to write/experiment with,
    and static computation graphs have faster runtimes because of some fancy logic
    under the hood. But note that dynamic and static frameworks have lately been moving
    toward the middle, allowing dynamic graphs to compile to static ones (for faster
    runtimes) or allowing static graphs to be built dynamically (for easier experimentation).
    In the long run, you’re likely to end up with both. The primary difference is
    whether forward propagation is happening during graph construction or after the
    graph is already defined. In this book, we’ll stick with dynamic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，动态计算图更容易编写/实验，而静态计算图由于底层的一些复杂逻辑，运行速度更快。但请注意，动态和静态框架最近正朝着中间发展，允许动态图编译为静态图（以获得更快的运行时间）或允许静态图动态构建（以获得更易实验）。从长远来看，你很可能会两者都拥有。主要区别在于正向传播是在图构建期间发生还是在图已经定义之后发生。在这本书中，我们将坚持使用动态的。
- en: The main point of this chapter is to help prepare you for deep learning in the
    real world, where 10% (or less) of your time will be spent thinking up a new idea
    and 90% of your time will be spent figuring out how to get a deep learning framework
    to play nicely. Debugging these frameworks can be extremely difficult at times,
    because most bugs don’t raise an error and print out a stack trace. Most bugs
    lie hidden within the code, keeping the network from training as it should (even
    if it appears to be training somewhat).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目的是帮助您为现实世界中的深度学习做好准备，在那里您将花费10%（或更少）的时间思考新的想法，90%的时间用于弄清楚如何让深度学习框架协同工作。有时调试这些框架可能极其困难，因为大多数错误不会引发错误并打印出堆栈跟踪。大多数错误隐藏在代码中，导致网络无法按预期训练（即使它看起来似乎在训练）。
- en: All that is to say, really dive into this chapter. You’ll be glad you did when
    it’s 2:00 a.m. and you’re chasing down an optimization bug that’s keeping you
    from getting that juicy state-of-the-art score.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些只是为了真正深入本章。当你在凌晨2点追逐一个优化错误，这个错误阻止你获得那个美味的最新分数时，你会很高兴你做了这件事。
- en: Tensors that are used multiple times
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 被多次使用的张量
- en: The basic autograd has a rather pesky bug. Let’s squish it!
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本autograd有一个相当讨厌的错误。让我们把它压扁！
- en: 'The current version of `Tensor` supports backpropagating into a variable only
    once. But sometimes, during forward propagation, you’ll use the same tensor multiple
    times (the weights of a neural network), and thus multiple parts of the graph
    will backpropagate gradients into the same tensor. But the code will currently
    compute the incorrect gradient when backpropagating into a variable that was used
    multiple times (is the parent of multiple children). Here’s what I mean:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的`Tensor`只支持将反向传播到变量一次。但有时，在正向传播过程中，你会多次使用同一个张量（神经网络的权重），因此图的不同部分将反向传播梯度到同一个张量。但当前代码在反向传播到被多次使用的变量时（是多个子张量的父张量）会计算错误的梯度。我的意思如下：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this example, the `b` variable is used twice in the process of creating
    `f`. Thus, its gradient should be the sum of two derivatives: `[2,2,2,2,2]`. Shown
    here is the resulting graph created by this chain of operations. Notice there
    are now two pointers pointing into `b`: so, it should be the sum of the gradient
    coming from both `e` and `d`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`b`变量在创建`f`的过程中被使用了两次。因此，其梯度应该是两个导数的总和：`[2,2,2,2,2]`。这里展示了由这一系列操作创建的结果图。注意现在有两个指针指向`b`：因此，它应该是来自`e`和`d`的梯度的总和。
- en: '![](Images/f0237-01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0237-01.jpg)'
- en: But the current implementation of `Tensor` merely overwrites each derivative
    with the previous. First, `d` applies its gradient, and then it gets overwritten
    with the gradient from `e`. We need to change the way gradients are written.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但当前`Tensor`的实现只是用前一个导数覆盖每个导数。首先，`d`应用其梯度，然后它被`e`的梯度覆盖。我们需要改变梯度写入的方式。
- en: Upgrading autograd to support multiuse tensors
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将autograd升级以支持多用途张量
- en: Add one new function, and update three old ones
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加一个新函数，并更新三个旧函数
- en: 'This update to the `Tensor` object adds two new features. First, gradients
    can be accumulated so that when a variable is used more than once, it receives
    gradients from all children:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对`Tensor`对象的这次更新增加了两个新功能。首先，梯度可以累积，以便当变量被多次使用时，它从所有子张量接收梯度：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* Keeps track of how many children a tensor has**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 跟踪张量有多少个子张量**'
- en: '***2* Checks whether a tensor has received the correct number of gradients
    from each child**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 检查张量是否从每个子张量接收到了正确数量的梯度**'
- en: '***3* Checks to make sure you can backpropagate or whether you’re waiting for
    a gradient, in which case decrement the counter**'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 检查是否可以反向传播，或者你是否正在等待梯度，如果是，则递减计数器**'
- en: '***4* Accumulates gradients from several children**'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从多个子节点累积梯度**'
- en: '***5* Begins actual backpropagation**'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 开始实际的反向传播**'
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Additionally, you create a `self.children` counter that counts the number of
    gradients received from each child during backpropagation. This way, you also
    prevent a variable from accidentally backpropagating from the same child twice
    (which throws an exception).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你创建了一个`self.children`计数器，在反向传播期间计算每个子节点接收到的梯度数量。这样，你也可以防止变量意外地从同一个子节点两次进行反向传播（这会抛出异常）。
- en: The second added feature is a new function with the rather verbose name `all_children_grads_accounted_for()`.
    The purpose of this function is to compute whether a tensor has received gradients
    from all of its children in the graph. Normally, whenever `.backward()` is called
    on an intermediate variable in a graph, it immediately calls `.backward()` on
    its parents. But because some variables receive their gradient value from multiple
    parents, each variable needs to wait to call `.backward()` on its parents until
    it has the final gradient locally.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个新增功能是一个具有相当冗长名称的新函数`all_children_grads_accounted_for()`。这个函数的目的是计算一个张量是否从其图中的所有子节点接收到了梯度。通常，每当在图中的中间变量上调用`.backward()`时，它会立即对其父节点调用`.backward()`。但由于一些变量从多个父节点接收梯度值，每个变量需要等待直到它具有局部的最终梯度后，再调用`.backward()`对其父节点进行反向传播。
- en: As mentioned previously, none of these concepts are new from a deep learning
    theory perspective; these are the kinds of engineering challenges that deep learning
    frameworks seek to face. More important, they’re the kinds of challenges you’ll
    face when debugging neural networks in a standard framework. Before moving on,
    take a moment to play around and get familiar with this code. Try deleting different
    parts and seeing how it breaks in various ways. Try calling `.backprop()` twice.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，从深度学习理论的角度来看，这些概念并不新颖；这些都是深度学习框架试图面对的工程挑战。更重要的是，它们是你在标准框架中调试神经网络时将面临的那种挑战。在继续之前，花点时间玩一下这段代码，熟悉它。尝试删除不同的部分，看看它以各种方式崩溃。尝试两次调用`.backprop()`。
- en: How does addition backpropagation work?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加法反向传播是如何工作的？
- en: Let’s study the abstraction to learn how to add support for more functions
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们研究这个抽象，以了解如何添加对更多函数的支持
- en: 'At this point, the framework has reached an exciting place! You can now add
    support for arbitrary operations by adding the function to the `Tensor` class
    and adding its derivative to the `.backward()` method. For addition, there’s the
    following method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这个框架已经达到了一个令人兴奋的地方！你现在可以通过将函数添加到`Tensor`类并添加其导数到`.backward()`方法来支持任意操作。对于加法，有以下方法：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And for backpropagation through the addition function, there’s the following
    gradient propagation within the `.backward()` method:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通过加法函数的反向传播，以下是`.backward()`方法中的以下梯度传播：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice that addition isn’t handled anywhere else in the class. The generic
    backpropagation logic is abstracted away so everything necessary for addition
    is defined in these two places. Note further that backpropagation logic calls
    `.backward()` two times, once for each variable that participated in the addition.
    Thus, the default setting in the backpropagation logic is to always backpropagate
    into every variable in the graph. But occasionally, backpropagation is skipped
    if the variable has autograd turned off (`self.autograd` == `False`). This check
    is performed in the `.backward()` method:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个类中其他地方并没有处理加法。通用的反向传播逻辑被抽象化，所以所有必要的加法定义都包含在这两个地方。进一步注意，反向传播逻辑在每次加法中都会调用`.backward()`两次，一次针对每个参与加法的变量。因此，反向传播逻辑的默认设置是始终将反向传播应用于图中的每个变量。但有时，如果变量关闭了自动微分（`self.autograd`
    == `False`），则会跳过反向传播。这个检查是在`.backward()`方法中进行的：
- en: '![](Images/f0240-01_alt.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0240-01_alt.jpg)'
- en: Even though the backpropagation logic for addition backpropagates the gradient
    into all the variables that contributed to it, the backpropagation won’t run unless
    `.autograd` is set to `True` for that variable (for `self.creators[0]` or `self.creators[1]`,
    respectively). Also notice in the first line of `__add__()` that the tensor created
    (which is later the tensor `running.backward()`) has `self.autograd` == `True`
    only if `self.autograd` == `other.autograd` == `True`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管加法的反向传播逻辑将梯度反向传播到所有对其有贡献的变量，但除非将该变量的 `.autograd` 设置为 `True`，否则反向传播不会运行（对于
    `self.creators[0]` 或 `self.creators[1]` 分别）。注意，在 `__add__()` 的第一行中，创建的张量（稍后是 `running.backward()`）的
    `self.autograd` 仅当 `self.autograd` == `other.autograd` == `True` 时才为 `True`。
- en: Adding support for negation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加对否定的支持
- en: Let’s modify the support for addition to support negation
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们修改支持加法以支持否定
- en: 'Now that addition is working, you should be able to copy and paste the addition
    code, create a few modifications, and add autograd support for negation. Let’s
    try it. Modifications from the `__add__` function are in bold:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加法功能已经正常工作，你应该能够复制并粘贴加法代码，进行一些修改，并为否定添加自动微分支持。让我们试试。`__add__` 函数的修改内容以粗体显示：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Nearly everything is identical. You don’t accept any parameters so the parameter
    “other” has been removed in several places. Let’s take a look at the backprop
    logic you should add to `.backward()`. Modifications from the `__add__` function
    backpropagation logic are in bold:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有内容都是相同的。你不接受任何参数，所以“other”参数已在多个地方被移除。让我们看看你应该添加到 `.backward()` 中的反向传播逻辑。`__add__`
    函数反向传播逻辑的修改内容以粗体显示：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Because the `__neg__` function has only one creator, you end up calling `.backward()`
    only once. (If you’re wondering how you know the correct gradients to backpropagate,
    revisit [chapters 4](kindle_split_012.xhtml#ch04), [5](kindle_split_013.xhtml#ch05),
    and [6](kindle_split_014.xhtml#ch06).) You can now test out the new code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `__neg__` 函数只有一个创建者，所以你只需要调用一次 `.backward()`。如果你想知道如何知道正确的梯度进行反向传播，请回顾第 [4](kindle_split_012.xhtml#ch04)、[5](kindle_split_013.xhtml#ch05)
    和 [6](kindle_split_014.xhtml#ch06) 章。你现在可以测试新的代码了：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When you forward propagate using `-b` instead of `b`, the gradients that are
    backpropagated have a flipped sign as well. Furthermore, you don’t have to change
    anything about the general backpropagation system to make this work. You can create
    new functions as you need them. Let’s add some more!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `-b` 而不是 `b` 进行前向传播时，反向传播的梯度也会翻转符号。此外，你不需要对一般的反向传播系统做任何修改来使其工作。你可以根据需要创建新的函数。让我们添加一些吧！
- en: Adding support for additional functions
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加对其他函数的支持
- en: Subtraction, multiplication, sum, expand, transpose, and matrix multiplication
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减法、乘法、求和、扩展、转置和矩阵乘法
- en: 'Using the same ideas you learned for addition and negation, let’s add the forward
    and backpropagation logic for several additional functions:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你为加法和否定学到的相同思想，让我们添加几个其他函数的前向和反向传播逻辑：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We’ve previously discussed the derivatives for all these functions, although
    `sum` and `expand` might seem foreign because they have new names. `sum` performs
    addition across a dimension of the tensor; in other words, say you have a 2 ×
    3 matrix called `x`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了所有这些函数的导数，尽管 `sum` 和 `expand` 可能看起来有些陌生，因为它们有新的名称。`sum` 在张量的一个维度上执行加法；换句话说，假设你有一个名为
    `x` 的 2 × 3 矩阵：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `.sum(dim)` function sums across a dimension. `x.sum(0)` will result in
    a 1 × 3 matrix (a length 3 vector), whereas `x.sum(1)` will result in a 2 × 1
    matrix (a length 2 vector):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`.sum(dim)` 函数在一个维度上求和。`x.sum(0)` 将得到一个 1 × 3 矩阵（长度为 3 的向量），而 `x.sum(1)` 将得到一个
    2 × 1 矩阵（长度为 2 的向量）：'
- en: '![](Images/f0243-00_alt.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0243-00_alt.jpg)'
- en: 'You use `expand` to backpropagate through a `.sum()`. It’s a function that
    copies data along a dimension. Given the same matrix `x`, copying along the first
    dimension gives two copies of the tensor:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用 `expand` 来通过 `.sum()` 进行反向传播。这是一个复制数据沿一个维度的函数。给定相同的矩阵 `x`，沿第一个维度复制会得到两个张量副本：
- en: '![](Images/f0243-01_alt.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0243-01_alt.jpg)'
- en: 'To be clear, whereas `.sum()` removes a dimension (2 × 3 -> just 2 or 3), `expand`
    adds a dimension. The 2 × 3 matrix becomes 4 × 2 × 3\. You can think of this as
    a list of four tensors, each of which is 2 × 3\. But if you expand to the last
    dimension, it copies along the last dimension, so each entry in the original tensor
    becomes a list of entries instead:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见， whereas `.sum()` 移除一个维度（2 × 3 -> 只剩 2 或 3），`expand` 添加一个维度。2 × 3 的矩阵变成了
    4 × 2 × 3。您可以将其视为一个包含四个张量的列表，每个张量都是 2 × 3。但如果您扩展到最后一个维度，它将沿着最后一个维度复制，因此原始张量中的每个条目都变成了一个条目的列表：
- en: '![](Images/f0243-02_alt.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0243-02_alt.jpg)'
- en: Thus, when you perform `.sum(dim=1)` on a tensor with four entries in that dimension,
    you need to perform `.expand(dim=1, copies=4)` to the gradient when you backpropagate
    it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当您对一个在该维度有四个条目的张量执行 `.sum(dim=1)` 时，您需要在反向传播时对梯度执行 `.expand(dim=1, copies=4)`。
- en: 'You can now add the corresponding backpropagation logic to the `.backward()`
    method:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以将相应的反向传播逻辑添加到 `.backward()` 方法中：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Usually an activation**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通常是一个激活**'
- en: '***2* Usually a weight matrix**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通常是一个权重矩阵**'
- en: If you’re unsure about this functionality, the best thing to do is to look back
    at how you were doing backpropagation in [chapter 6](kindle_split_014.xhtml#ch06).
    That chapter has figures showing each step of backpropagation, part of which I’ve
    shown again here.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定这个功能，最好的做法是回顾一下您在[第6章](kindle_split_014.xhtml#ch06)中是如何进行反向传播的。那一章有展示反向传播每个步骤的图表，其中一部分我在这里又展示了一遍。
- en: The gradients start at the end of the network. You then move the error signal
    *backward through the network* by calling functions that correspond to the functions
    used to move activations *forward through the network*. If the last operation
    was a matrix multiplication (and it was), you backpropagate by performing matrix
    multiplication (dot) on the transposed matrix.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度从网络的末端开始。然后您通过调用与用于将激活向前传递到网络中的函数相对应的函数，将错误信号 *反向传递到网络中*。如果最后一个操作是矩阵乘法（并且确实是），您通过在转置矩阵上执行矩阵乘法（点积）来进行反向传播。
- en: In the following image, this happens at the line `layer_1_delta=layer_2_delta.dot
    (weights_1_2.T)`. In the previous code, it happens in `if(self.creation_op ==
    "mm")` (highlighted in bold). You’re doing the exact same operations as before
    (in reverse order of forward propagation), but the code is better organized.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图像中，这发生在 `layer_1_delta=layer_2_delta.dot (weights_1_2.T)` 这一行。在之前的代码中，它发生在
    `if(self.creation_op == "mm")`（加粗显示）。您正在执行与之前（按前向传播的相反顺序）完全相同的操作，但代码组织得更好。
- en: '![](Images/f0245-01_alt.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0245-01_alt.jpg)'
- en: '![](Images/f0245-02_alt.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0245-02_alt.jpg)'
- en: Using autograd to train a neural network
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 autograd 训练神经网络
- en: You no longer have to write backpropagation logic!
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您不再需要编写反向传播逻辑！
- en: 'This may have seemed like quite a bit of engineering effort, but it’s about
    to pay off. Now, when you train a neural network, you don’t have to write any
    backpropagation logic! As a toy example, here’s a neural network to backprop by
    hand:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来需要相当多的工程努力，但很快就会得到回报。现在，当您训练神经网络时，您不必编写任何反向传播逻辑！作为一个玩具示例，这里有一个用于手动反向传播的神经网络：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Predict**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Mean squared error loss**'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 均方误差损失**'
- en: '***4* Learn; this is the backpropagation piece.**'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 学习；这是反向传播的部分。**'
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You have to forward propagate in such a way that `layer_1`, `layer_2`, and `diff`
    exist as variables, because you need them later. You then have to backpropagate
    each gradient to its appropriate weight matrix and perform the weight update appropriately.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须以这种方式进行前向传播，使得 `layer_1`、`layer_2` 和 `diff` 作为变量存在，因为您稍后需要它们。然后您必须将每个梯度反向传播到相应的权重矩阵，并适当地更新权重。
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Predict**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: But with the fancy new autograd system, the code is much simpler. You don’t
    have to keep around any temporary variables (because the dynamic graph keeps track
    of them), and you don’t have to implement any backpropagation logic (because the
    `.backward()` method handles that). Not only is this more convenient, but you’re
    less likely to make silly mistakes in the backpropagation code, reducing the likelihood
    of bugs!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但有了新潮的 autograd 系统，代码要简单得多。您不需要保留任何临时变量（因为动态图会跟踪它们），也不需要实现任何反向传播逻辑（因为 `.backward()`
    方法处理这个）。这不仅更方便，而且您在反向传播代码中犯愚蠢错误的可能性更小，从而降低了出错的可能性！
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Before moving on, I’d like to point out one stylistic thing in this new implementation.
    Notice that I put all the parameters in a list, which I could iterate through
    when performing the weight update. This is a bit of foreshadowing for the next
    piece of functionality. When you have an autograd system, stochastic gradient
    descent becomes trivial to implement (it’s just that `for` loop at the end). Let’s
    try making this its own class as well.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想指出这个新实现中的一个风格问题。注意，我把所有参数都放在了一个列表中，这样我就可以在执行权重更新时遍历它们。这是对下一个功能功能的一点暗示。当你有一个自动微分系统时，随机梯度下降的实现变得非常简单（它只是最后的那个`for`循环）。让我们试着把它也做成一个类。
- en: Adding automatic optimization
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加自动优化
- en: Let’s make a stochastic gradient descent optimizer
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们创建一个随机梯度下降优化器
- en: 'At face value, creating something called a stochastic gradient descent optimizer
    may sound difficult, but it’s just copying and pasting from the previous example
    with a bit of good, old-fashioned object-oriented programming:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从字面上看，创建一个名为随机梯度下降优化器的东西可能听起来很困难，但实际上只是从上一个例子中复制粘贴，加上一点老式的面向对象编程：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The previous neural network is further simplified as follows, with exactly
    the same results as before:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的神经网络进一步简化如下，与之前完全相同的结果：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* Predict**'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: Adding support for layer types
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加对层类型的支持
- en: You may be familiar with layer types in Keras or PyTorch
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可能熟悉Keras或PyTorch中的层类型
- en: 'At this point, you’ve done the most complicated pieces of the new deep learning
    framework. Further work is mostly about adding new functions to the tensor and
    creating convenient higher-order classes and functions. Probably the most common
    abstraction among nearly all frameworks is the layer abstraction. It’s a collection
    of commonly used forward propagation techniques packaged into an simple API with
    some kind of `.forward()` method to call them. Here’s an example of a simple linear
    layer:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经完成了新深度学习框架中最复杂的部分。接下来的工作主要是向张量添加新函数，创建方便的高阶类和函数。在几乎所有框架中，最常见的一种抽象是层抽象。它是一组常用的正向传播技术，封装在一个简单的API中，并带有某种`.forward()`方法来调用它们。以下是一个简单线性层的示例：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Nothing here is particularly new. The weights are organized into a class (and
    I added bias weights because this is a true linear layer). You can initialize
    the layer all together, such that both the weights and bias are initialized with
    the correct sizes, and the correct forward propagation logic is always employed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么特别新的。权重被组织到一个类中（并且我添加了偏置权重，因为这是一个真正的线性层）。你可以一次性初始化这个层，使得权重和偏置都使用正确的尺寸，并且始终使用正确的正向传播逻辑。
- en: Also notice that I created an abstract class `Layer`, which has a single getter.
    This allows for more-complicated layer types (such as layers containing other
    layers). All you need to do is override `get_parameters()` to control what tensors
    are later passed to the optimizer (such as the `SGD` class created in the previous
    section).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我创建了一个抽象类`Layer`，它有一个单一的getter。这允许更复杂的层类型（例如包含其他层的层）。你只需要重写`get_parameters()`来控制稍后传递给优化器（如上一节中创建的`SGD`类）的张量。
- en: Layers that contain layers
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包含层的层
- en: Layers can also contain other layers
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层也可以包含其他层
- en: 'The most popular layer is a sequential layer that forward propagates a list
    of layers, where each layer feeds its outputs into the inputs of the next layer:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的层是顺序层，它正向传播一个层的列表，其中每个层将其输出馈送到下一个层的输入：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1* Predict**'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: Loss-function layers
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数层
- en: Some layers have no weights
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一些层没有权重
- en: 'You can also create layers that are functions on the input. The most popular
    version of this kind of layer is probably the loss-function layer, such as mean
    squared error:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以创建函数层，这些函数作用于输入。这类层中最受欢迎的版本可能是损失函数层，例如均方误差：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Predict**'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If you’ll forgive the repetition, again, nothing here is particularly new. Under
    the hood, the last several code examples all do the exact same computation. It’s
    just that autograd is doing all the backpropagation, and the forward propagation
    steps are packaged in nice classes to ensure that the functionality executes in
    the correct order.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能原谅重复，再次强调，这里没有什么特别新的。在底层，最后几个代码示例都执行了完全相同的计算。只是自动微分正在执行所有的反向传播，正向传播步骤被封装在漂亮的类中，以确保功能按正确的顺序执行。
- en: How to learn a framework
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何学习一个框架
- en: Oversimplified, frameworks are autograd + a list of prebuilt laye- ers and optimizers
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过于简化地说，框架是自动微分加上一系列预构建的层和优化器
- en: You’ve been able to write (rather quickly) a variety of new layer types using
    the underlying autograd system, which makes it quite easy to piece together arbitrary
    layers of functionality. Truth be told, this is the main feature of modern frameworks,
    eliminating the need to handwrite each and every math operation for forward and
    backward propagation. Using frameworks greatly increases the speed with which
    you can go from idea to experiment and will reduce the number of bugs in your
    code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经能够（相当快速地）使用底层自动微分系统编写各种新的层类型，这使得组合任意功能层变得相当容易。说实话，这是现代框架的主要功能，消除了为正向和反向传播手动编写每个数学运算的需要。使用框架大大增加了您从想法到实验的速度，并将减少您代码中的错误数量。
- en: 'Viewing a framework as merely an autograd system coupled with a big list of
    layers and optimizers will help you learn them. I expect you’ll be able to pivot
    from this chapter into almost any framework fairly quickly, although the framework
    that’s most similar to the API built here is PyTorch. Either way, for your reference,
    take a moment to peruse the lists of layers and optimizers in several of the big
    frameworks:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将框架视为仅是一个与大量层和优化器耦合的自动微分系统，这有助于您学习它们。我预计您将能够快速地从本章转向几乎任何框架，尽管与在此处构建的API最相似的框架是
    PyTorch。无论如何，为了您的参考，花点时间浏览一下几个大型框架中的层和优化器列表：
- en: '[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)'
- en: '[https://keras.io/layers/about-keras-layers](https://keras.io/layers/about-keras-layers)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/about-keras-layers](https://keras.io/layers/about-keras-layers)'
- en: '[https://www.tensorflow.org/api_docs/python/tf/layers](https://www.tensorflow.org/api_docs/python/tf/layers)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/api_docs/python/tf/layers](https://www.tensorflow.org/api_docs/python/tf/layers)'
- en: The general workflow for learning a new framework is to find the simplest possible
    code example, tweak it and get to know the autograd system’s API, and then modify
    the code example piece by piece until you get to whatever experiment you care
    about.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 学习新框架的一般工作流程是找到最简单的代码示例，对其进行调整，了解自动微分系统的API，然后逐步修改代码示例，直到达到您关心的任何实验。
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: One more thing before we move on. I’m adding a nice convenience function to
    `Tensor.backward()` that makes it so you don’t have to pass in a gradient of 1s
    the first time you call `.backward()`. It’s not, strictly speaking, necessary—but
    it’s handy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我正在向 `Tensor.backward()` 添加一个方便的函数，这样您在第一次调用 `.backward()` 时就不必传入1的梯度。严格来说，这不是必需的——但它很方便。
- en: Nonlinearity layers
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性层
- en: Let’s add nonlinear functions to Tensor and then create some layer types
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们在 Tensor 中添加非线性函数，然后创建一些层类型
- en: 'For the next chapter, you’ll need `.sigmoid()` and `.tanh()`. Let’s add them
    to the `Tensor` class. You learned about the derivative for both quite some time
    ago, so this should be easy:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一章，您将需要 `.sigmoid()` 和 `.tanh()`。让我们将它们添加到 `Tensor` 类中。您很久以前就学过了这两个函数的导数，所以这应该很容易：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following code shows the backprop logic added to the `Tensor.backward()`
    method:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了添加到 `Tensor.backward()` 方法的反向传播逻辑：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Hopefully, this feels fairly routine. See if you can make a few more nonlinearities
    as well: try `HardTanh` or `relu`.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这感觉相当常规。看看您能否创建更多的非线性函数：尝试 `HardTanh` 或 `relu`。
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s try out the new nonlinearities. New additions are in bold:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试新的非线性函数。新添加的内容以粗体显示：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '***1* Predict**'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, you can drop the new `Tanh()` and `Sigmoid()` layers into the
    input parameters to `Sequential()`, and the neural network knows exactly how to
    use them. Easy!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您可以将新的 `Tanh()` 和 `Sigmoid()` 层直接放入 `Sequential()` 的输入参数中，神经网络会确切地知道如何使用它们。简单！
- en: 'In the previous chapter, you learned about recurrent neural networks. In particular,
    you trained a model to predict the next word, given the previous several words.
    Before we finish this chapter, I’d like for you to translate that code into the
    new framework. To do this, you’ll need three new layer types: an embedding layer
    that learns word embeddings, an RNN layer that can learn to model sequences of
    inputs, and a softmax layer that can predict a probability distribution over labels.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了循环神经网络。特别是，你训练了一个模型来预测下一个单词，给定前几个单词。在我们完成本章之前，我希望你能将那段代码翻译成新的框架。为此，你需要三种新的层类型：一个学习词嵌入的嵌入层，一个可以学习建模输入序列的RNN层，以及一个可以预测标签概率分布的softmax层。
- en: The embedding layer
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层
- en: An embedding layer translates indices into activations
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入层将索引转换为激活
- en: 'In [chapter 11](kindle_split_019.xhtml#ch11), you learned about word embeddings,
    which are vectors mapped to words that you can forward propagate into a neural
    network. Thus, if you have a vocabulary of 200 words, you’ll also have 200 embeddings.
    This gives the initial spec for creating an embedding layer. First, initialize
    a list (of the right length) of word embeddings (of the right size):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](kindle_split_019.xhtml#ch11)中，你学习了关于词嵌入的内容，这些嵌入是将向量映射到单词，你可以将其前向传播到神经网络中。因此，如果你有一个200个单词的词汇表，你也将有200个嵌入。这为创建嵌入层提供了初始规范。首先，初始化一个（正确长度的）单词嵌入列表（正确的大小）：
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1* This initialization style is a convention from word2vec.**'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这种初始化风格是来自word2vec的惯例。**'
- en: 'So far, so good. The matrix has a row (vector) for each word in the vocabulary.
    Now, how will you forward propagate? Well, forward propagation always starts with
    the question, “How will the inputs be encoded?” In the case of word embeddings,
    you obviously can’t pass in the words themselves, because the words don’t tell
    you which rows in `self.weight` to forward propagate with. Instead, as you hopefully
    remember from [chapter 11](kindle_split_019.xhtml#ch11), you forward propagate
    indices. Fortunately, NumPy supports this operation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。矩阵为词汇表中的每个单词都有一个行（向量）。现在，你将如何进行前向传播呢？好吧，前向传播始终从问题“输入将如何编码？”开始。在词嵌入的情况下，显然你不能传递单词本身，因为单词不会告诉你应该用`self.weight`中的哪一行进行前向传播。相反，如你从[第11章](kindle_split_019.xhtml#ch11)中可能记得的，你前向传播索引。幸运的是，NumPy支持这个操作：
- en: '![](Images/f0255-01_alt.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0255-01_alt.jpg)'
- en: Notice how, when you pass a matrix of integers into a NumPy matrix, it returns
    the same matrix, but with each integer replaced with the row the integer specified.
    Thus a 2D matrix of indices turns into a 3D matrix of embeddings (rows). This
    is perfect!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你将整数矩阵传递给NumPy矩阵时，它返回相同的矩阵，但每个整数都被替换为指定的行。因此，一个索引的二维矩阵变成了一个三维的嵌入矩阵（行）。这太完美了！
- en: Adding indexing to autograd
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将索引添加到autograd
- en: Before you can build the embedding layer, autograd needs to support indexing
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在构建嵌入层之前，autograd需要支持索引
- en: 'In order to support the new embedding strategy (which assumes words are forward
    propagated as matrices of indices), the indexing you played around with in the
    previous section must be supported by autograd. This is a pretty simple idea.
    You need to make sure that during backpropagation, the gradients are placed in
    the same rows as were indexed into for forward propagation. This requires that
    you keep around whatever indices were passed in, so you can place each gradient
    in the appropriate location during backpropagation with a simple `for` loop:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持新的嵌入策略（该策略假设单词作为索引矩阵进行前向传播），你在上一节中玩弄的索引必须由autograd支持。这是一个相当简单的想法。你需要确保在反向传播过程中，梯度被放置在与前向传播中索引到的相同行中。这要求你保留传递的任何索引，以便在反向传播期间使用简单的`for`循环将每个梯度放置在适当的位置：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'First, use the NumPy trick you learned in the previous section to select the
    correct rows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用你在上一节中学到的NumPy技巧来选择正确的行：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, during `backprop()`, initialize a new gradient of the correct size (the
    size of the original matrix that was being indexed into). Second, flatten the
    indices so you can iterate through them. Third, collapse `grad_` to a simple list
    of rows. (The subtle part is that the list of indices in `indices_` and the list
    of vectors in `grad_` will be in the corresponding order.) Then, iterate through
    each index, add it into the correct row of the new gradient you’re creating, and
    backpropagate it into `self.creators[0]`. As you can see, `grad_[i]` correctly
    updates each row (adds a vector of 1s, in this case) in accordance with the number
    of times the index is used. Indices 2 and 3 update twice (in bold):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 `backprop()` 期间，初始化一个正确大小的新梯度（原始矩阵的大小，该矩阵正在被索引）。其次，展平索引，以便你可以遍历它们。第三，将
    `grad_` 压缩成一个简单的行列表。（微妙之处在于 `indices_` 中的索引列表和 `grad_` 中的向量列表将按对应顺序排列。）然后，遍历每个索引，将其添加到你正在创建的新梯度的正确行中，并将其反向传播到
    `self.creators[0]`。正如你所看到的，`grad_[i]` 正确地更新了每一行（在这种情况下，添加一个由 1 组成的向量），并按照索引被使用的次数进行更新。索引
    2 和 3 更新了两次（加粗）：
- en: '![](Images/f0256-01_alt.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0256-01_alt.jpg)'
- en: The embedding layer (revisited)
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层（重访）
- en: Now you can finish forward propagation using the new .index_select() method
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 现在你可以使用新的 .index_select() 方法完成正向传播
- en: 'For forward prop, call `.index_select()`, and autograd will handle the rest:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正向传播，调用 `.index_select()`，autograd 将处理其余部分：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '***1* This initialization style is a convention from word2vec.**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这种初始化风格是来自 word2vec 的约定。**'
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '***1* Predict**'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this neural network, you learn to correlate input indices 1 and 2 with the
    prediction 0 and 1\. In theory, indices 1 and 2 could correspond to words (or
    some other input object), and in the final example, they will. This example was
    to show the embedding working.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个神经网络中，你学习将输入索引 1 和 2 与预测 0 和 1 相关联。在理论上，索引 1 和 2 可以对应于单词（或某种其他输入对象），在最终的例子中，它们将这样做。这个例子是为了展示嵌入的工作。
- en: The cross-entropy layer
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵层
- en: Let’s add cross entropy to the autograd and create a layer
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们向 autograd 添加交叉熵并创建一个层
- en: Hopefully, at this point you’re starting to feel comfortable with how to create
    new layer types. Cross entropy is a pretty standard one that you’ve seen many
    times throughout this book. Because we’ve already walked through how to create
    several new layer types, I’ll leave the code here for your reference. Attempt
    to do it yourself before copying this code.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到这一点，你已经开始对如何创建新的层类型感到舒适。交叉熵是一个相当标准的层，你在本书中已经多次见过。因为我们已经介绍了如何创建几个新的层类型，所以我会在这里留下代码供你参考。在复制此代码之前，尝试自己完成它。
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* Predict**'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测**'
- en: '***2* Compare**'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 比较**'
- en: '***3* Learn**'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 学习**'
- en: '[PRE39]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Using the same cross-entropy logic employed in several previous neural networks,
    you now have a new loss function. One noticeable thing about this loss is different
    from others: both the final `softmax` and the computation of the loss are within
    the loss class. This is an extremely common convention in deep neural networks.
    Nearly every framework will work this way. When you want to finish a network and
    train with cross entropy, you can leave off the `softmax` from the forward propagation
    step and call a cross-entropy class that will automatically perform the `softmax`
    as a part of the loss function.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在几个先前神经网络中使用的相同交叉熵逻辑，你现在有一个新的损失函数。这个损失函数的一个显著特点是与其他不同：最终的 `softmax` 和损失的计算都在损失类内部完成。这在深度神经网络中是一个非常常见的约定。几乎每个框架都会这样做。当你想要完成一个网络并使用交叉熵进行训练时，你可以在正向传播步骤中省略
    `softmax`，并调用一个交叉熵类，该类将自动将 `softmax` 作为损失函数的一部分执行。
- en: The reason these are combined so consistently is performance. It’s much faster
    to calculate the gradient of `softmax` and negative log likelihood together in
    a cross-entropy function than to forward propagate and backpropagate them separately
    in two different modules. This has to do with a shortcut in the gradient math.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这些之所以如此一致地组合在一起，是因为性能。在交叉熵函数中一起计算 `softmax` 和负对数似然比梯度的速度比在两个不同的模块中分别进行正向传播和反向传播要快得多。这与梯度数学中的捷径有关。
- en: The recurrent neural network layer
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络层
- en: By combining several layers, you can learn over time series
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过组合多个层，你可以学习时间序列
- en: 'As the last exercise of this chapter, let’s create one more layer that’s the
    composition of multiple smaller layer types. The point of this layer will be to
    learn the task you finished at the end of the previous chapter. This layer is
    the *recurrent layer*. You’ll construct it using three linear layers, and the
    `.forward()` method will take both the output from the previous hidden state and
    the input from the current training data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的最后一个练习，让我们再创建一个由多个较小的层类型组成的层。这个层的目的是学习你在上一章结束时完成的任务。这个层是**循环层**。你将使用三个线性层来构建它，`.forward()`方法将接受前一个隐藏状态和当前训练数据的输入：
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It’s out of scope for this chapter to reintroduce RNNs, but it’s worth pointing
    out the pieces that should be familiar already. RNNs have a state vector that
    passes from timestep to timestep. In this case, it’s the variable `hidden`, which
    is both an input parameter and output variable to the `forward` function. RNNs
    also have several different weight matrices: one that maps input vectors to hidden
    vectors (processing input data), one that maps from hidden to hidden (which updates
    each hidden vector based on the previous), and optionally a hidden-to-output layer
    that learns to make predictions based on the hidden vector. This RNNCell implementation
    includes all three. The `self.w_ih` layer is the input-to-hidden layer, `self.w_hh`
    is the hidden-to-hidden layer, and `self.w_ho` is the hidden-to-output layer.
    Note the dimensionality of each. The input size of `self.w_ih` and the output
    size of `self.w_ho` are both the size of the vocabulary. All other dimensions
    are configurable based on the `n_hidden` parameter.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不涉及重新介绍RNNs，但指出一些应该已经熟悉的组成部分是值得的。RNNs有一个状态向量，它在时间步长之间传递。在这种情况下，它是变量`hidden`，它既是`forward`函数的输入参数也是输出变量。RNNs还有几个不同的权重矩阵：一个将输入向量映射到隐藏向量（处理输入数据），一个将隐藏向量映射到隐藏向量（根据前一个更新每个隐藏向量），以及可选的隐藏到输出层，该层学习根据隐藏向量进行预测。这个RNNCell实现包括了这三个。`self.w_ih`层是输入到隐藏层，`self.w_hh`是隐藏到隐藏层，`self.w_ho`是隐藏到输出层。注意每个的维度。`self.w_ih`的输入大小和`self.w_ho`的输出大小都是词汇表的大小。所有其他维度都是基于`n_hidden`参数可配置的。
- en: 'Finally, an `activation` input parameter defines which nonlinearity is applied
    to hidden vectors at each timestep. I’ve added two possibilities (`Sigmoid` and
    `Tanh`), but there are many options to choose from. Let’s train a network:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个`activation`输入参数定义了在每个时间步长应用于隐藏向量的非线性函数。我添加了两种可能性（`Sigmoid`和`Tanh`），但有很多选项可供选择。让我们训练一个网络：
- en: '[PRE41]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can learn to fit the task you previously accomplished in the - preceding
    chapter
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以学习适应你在上一章中完成的任务
- en: Now you can initialize the recurrent layer with an embedding input and train
    a network to solve the same task as in the previous chapter. Note that this network
    is slightly more complex (it has one extra layer) despite the code being much
    simpler, thanks to the little framework.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用嵌入输入初始化循环层，并训练一个网络来解决上一章相同的任务。请注意，尽管代码要简单得多，但由于这个小框架，这个网络稍微复杂一些（它有一个额外的层）。
- en: '[PRE42]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: First, define the input embeddings and then the recurrent cell. (Note that *cell*
    is a conventional name given to recurrent layers when they’re implementing only
    a single recurrence. If you created another layer that provided the ability to
    configure arbitrary numbers of cells together, it would be called an RNN, and
    `n_layers` would be an input parameter.)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义输入嵌入，然后定义循环单元。（注意，当循环层仅实现单个递归时，通常将其命名为*cell*。如果你创建了一个可以配置任意数量单元格的层，它将被称为RNN，`n_layers`将是一个输入参数。）
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As you can see, the neural network learns to predict the first 100 examples
    of the training dataset with an accuracy of around 37% (near perfect, for this
    toy task). It predicts a plausible location for Mary to be moving toward, much
    like at the end of [chapter 12](kindle_split_020.xhtml#ch12).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，神经网络学会了以大约37%的准确率预测训练数据集的前100个示例（对于这个玩具任务来说几乎是完美的）。它预测了玛丽可能移动的方向，就像在第[12章](kindle_split_020.xhtml#ch12)的结尾一样。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Frameworks are efficient, convenient abstractions of forward and backward logic
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 框架是前向和反向逻辑的高效、方便的抽象
- en: I hope this chapter’s exercise has given you an appreciation for how convenient
    frameworks can be. They can make your code more readable, faster to write, faster
    to execute (through built-in optimizations), and much less buggy. More important,
    this chapter will prepare you for using and extending industry standard frameworks
    like PyTorch and TensorFlow. Whether debugging existing layer types or prototyping
    your own, the skills you’ve learned here will be some of the most important you
    acquire in this book, because they bridge the abstract knowledge of deep learning
    from previous chapters with the design of real-world tools you’ll use to implement
    models in the future.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一章的练习让你体会到了框架的便利性。它们可以使你的代码更易读，编写更快，执行更快（通过内置优化），并且错误更少。更重要的是，这一章将为你使用和扩展行业标准框架如PyTorch和TensorFlow做好准备。无论是调试现有的层类型还是原型设计你自己的，你在本章学到的技能将是你在本书中获得的最重要的技能之一，因为它们将之前章节中关于深度学习的抽象知识与你未来将用于实现模型的真实工具的设计联系起来。
- en: The framework that’s most similar to the one built here is PyTorch, and I highly
    recommend diving into it when you complete this book. It will likely be the framework
    that feels most familiar.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与这里构建的框架最相似的是PyTorch，我强烈建议你在完成这本书后深入探索它。它很可能是你感觉最熟悉的框架。
