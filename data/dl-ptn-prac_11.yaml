- en: 9 Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 自编码器
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Understanding the design principles and patterns for DNN and CNN autoencoders
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度神经网络（DNN）和卷积神经网络（CNN）自编码器的设计原则和模式
- en: Coding these models using the procedural design pattern
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用过程设计模式编码这些模型
- en: Regularization when training an autoencoder
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练自编码器时的正则化
- en: Using an autoencoder for compression, denoising, and super resolution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器进行压缩、去噪和超分辨率
- en: Using an autoencoder for pretraining to improve the model’s ability to generalize
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器进行预训练以提高模型泛化能力
- en: Up to now, we’ve discussed only models for supervised learning. An *autoencoder
    model* falls into the category of unsupervised learning. As a reminder, in supervised
    learning our data consists of the features (for example, image data) and labels
    (for example, classes), and we train the model to learn to predict the labels
    from the features. In unsupervised learning, we either have no labels or don’t
    use them, and we train the model to find correlating patterns in the data. You
    might ask, what can we do without labels? We can do a lot of things, and autoencoders
    are one type of model architecture that can learn from unlabeled data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了监督学习模型。*自编码器模型*属于无监督学习的范畴。提醒一下，在监督学习中，我们的数据由特征（例如，图像数据）和标签（例如，类别）组成，我们训练模型学习从特征预测标签。在无监督学习中，我们可能没有标签或者不使用它们，我们训练模型在数据中找到相关模式。你可能会问，没有标签我们能做什么？我们可以做很多事情，自编码器就是可以从未标记数据中学习的一种模型架构。
- en: Autoencoders are the fundamental deep learning models for unsupervised learning.
    Even without human labeling, autoencoders can learn image compression, representational
    learning, image denoising, super-resolution and pretext tasks—and we’ll cover
    each of these in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是无监督学习的根本深度学习模型。即使没有人工标记，自编码器也可以学习图像压缩、表示学习、图像去噪、超分辨率和预训练任务——我们将在本章中介绍每个这些内容。
- en: So how does unsupervised learning work with autoencoders? Even though we don’t
    have labels for the image data, we can manipulate images to be both the input
    data and output label, and train the model to predict the output label. For example,
    the output label could simply be the input image—here, the model would be learning
    the identity function. Or we could make a copy of the image and add noise to it,
    and then use the noisy version as the input and the original image as the output
    label—this is how our model could learn to denoise the image. In this chapter,
    we will cover these and several other techniques for manipulating the input image
    into output labels.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，无监督学习是如何与自编码器一起工作的呢？尽管我们没有图像数据的标签，我们可以操作图像使其同时成为输入数据和输出标签，并训练模型来预测输出标签。例如，输出标签可以是简单的输入图像——在这里，模型将学习恒等函数。或者，我们可以复制图像并向其添加噪声，然后使用噪声版本作为输入，原始图像作为输出标签——这就是我们的模型学习去噪图像的方式。在本章中，我们将介绍这些以及其他几种将输入图像转换为输出标签的技术。
- en: 9.1 Deep neural network autoencoders
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 深度神经网络自编码器
- en: We will start this chapter on autoencoders with the classic deep neural network
    version. While you can learn interesting things using just a DNN, it does not
    scale well when it comes to image data, so in subsequent sections we will move
    onto using CNN autoencoders.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始介绍自编码器的经典深度神经网络版本。虽然你可以仅使用DNN学习到有趣的东西，但它不适用于图像数据，所以接下来的几节我们将转向使用CNN自编码器。
- en: 9.1.1 Autoencoder architecture
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 自编码器架构
- en: An example of how DNN autoencoders can be useful is when reconstructing images.
    One of my favorite reconstructions, typically used as a pretext task, is the jigsaw
    puzzle. In this case, the input image is divided into nine tiles and then randomly
    shuffled. The reconstruction task is to predict the order that the tiles were
    shuffled. Since this task is essentially a multivalue regressor output, it works
    well with a traditional CNN, where the multiclass classifier is replaced with
    a multivalue regressor.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DNN自编码器如何有用的一个例子是在图像重建方面。我最喜欢的重建之一，通常用作预训练任务，是拼图。在这种情况下，输入图像被分成九个拼块，然后随机打乱。重建任务就是预测拼块被打乱的顺序。由于这个任务本质上是一个多值回归器输出，它非常适合传统的CNN，其中多类分类器被多值回归器所取代。
- en: 'An autoencoder is composed of two basic components: an encoder and a decoder.
    For image reconstruction, the *encoder* learns an optimal (or nearly optimal)
    method to progressively pool the image data into the latent space, and the *decoder*
    learns an optimal (or nearly optimal) method to progressively unpool the latent
    space for image reconstruction. The reconstruction task determines the type of
    representational and transformational learning. For example, in the identity function,
    the reconstruction task is reconstructing the input image. But you could also
    reconstruct an image without noise (by denoising) or an image of higher resolution
    (super-resolution). These types of reconstructions work well with an autoencoder.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器由两个基本组件组成：编码器和解码器。对于图像重建，*编码器*学习一个最优（或几乎最优）的方法来逐步将图像数据池化到潜在空间，而*解码器*学习一个最优（或几乎最优）的方法来逐步反池化潜在空间以进行图像重建。重建任务决定了表示学习和转换学习的类型。例如，在恒等函数中，重建任务是重建输入图像。但你也可以重建一个无噪声的图像（通过降噪）或更高分辨率的图像（超分辨率）。这些类型的重建与自动编码器工作得很好。
- en: Let’s see how the encoders and decoders work together in an autoencoder to do
    these kinds of reconstructions. The basic autoencoder architecture, shown in figure
    9.1, actually has three key components, with latent space between the encoder
    and decoder. The encoder performs representational learning on the input to learn
    a function *f*(*x*) = *x*'. That *x*' is referred to as the *latent space*, which
    is the learned representation from *x* at a lower dimensionality. Then the decoder
    performs transformational learning from the latent space to perform some form
    of reconstruction of the original image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看编码器和解码器在自动编码器中如何协同工作来完成这些类型的重建。基本的自动编码器架构，如图9.1所示，实际上有三个关键组件，编码器和解码器之间有潜在空间。编码器对输入进行表示学习，学习一个函数
    *f*(*x*) = *x*'。这个 *x*' 被称为 *潜在空间*，它是从 *x* 学习到的低维表示。然后解码器从潜在空间进行转换学习，以执行原始图像的某种形式的重建。
- en: '![](Images/CH09_F01_Ferlitsch.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH09_F01_Ferlitsch.png)'
- en: Figure 9.1 Learning the identity function for image input/output in the autoencoder
    macro-architecture
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 自动编码器宏架构中学习图像输入/输出的恒等函数
- en: 'Let’s say the autoencoder in figure 9.1 learns the identity function *f*(*x*)
    = *x*. Since the latent space *x*'' is of lower dimensionality, we typically describe
    this form of an autoencoder as learning the optimal way to compress images in
    a dataset (encoder) and then decompress the images (decoder). We could also describe
    this as the sequence of functions: encoder(*x*) = *x*'', decoder(*x*'') = *x*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设图9.1中的自动编码器学习恒等函数 *f*(*x*) = *x*。由于潜在空间 *x*' 的维度更低，我们通常将这种形式的自动编码器描述为学习在数据集中压缩图像的最优方式（编码器）然后解压缩图像（解码器）。我们也可以将这描述为函数序列：编码器(*x*)
    = *x*', 解码器(*x*') = *x*。
- en: In other words, the dataset represents a distribution, and for that distribution
    the autoencoder learns the optimal method to compress the images to a lower dimensionality
    and learns the optimal decompression to reconstruct the image. Let’s take a closer
    look at the encoder and decoder, and then see how we’d train this kind of model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，数据集代表了一种分布，对于这种分布，自动编码器学习最优的方法来压缩图像到更低的维度，并学习最优的解压缩方法来重建图像。让我们更详细地看看编码器和解码器，然后看看我们如何训练这种模型。
- en: 9.1.2 Encoder
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 编码器
- en: The basic form of an autoencoder for learning the identity function uses dense
    layers (hidden units). Pooling is performed by having each layer in the encoder
    progressively reduce the number of nodes (hidden units), and unpooling is learned
    by having each layer progressively increase the number of nodes. The number of
    nodes in the final unpooling dense layer is the same as the number of pixels in
    the input.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 学习恒等函数的基本自动编码器形式使用密集层（隐藏单元）。池化是通过编码器中的每一层逐渐减少节点（隐藏单元）的数量来实现的，而反池化是通过每一层逐渐增加节点数量来学习的。最终反池化密集层中的节点数与输入像素数相同。
- en: For the identity function, the image itself is the label. You do not need to
    know what the image depicts, whether it is a cat, dog, horse, airplane, or whatever.
    When the model is trained, the images are both the independent variables (features)
    and the dependent variables (labels).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于恒等函数，图像本身是标签。你不需要知道图像描绘的是什么，无论是猫、狗、马、飞机还是其他什么。当模型训练时，图像既是自变量（特征）也是因变量（标签）。
- en: The following code is an example implementation of the encoder for an auto-encoder
    to learn the identity function. It follows the process depicted in figure 9.1,
    progressively pooling the number of nodes (hidden units) through the parameter
    `layers`. The output from the encoder is the latent space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是自动编码器学习恒等函数的编码器的一个示例实现。它遵循图9.1中描述的过程，通过`layers`参数逐步池化节点（隐藏单元）的数量。编码器的输出是潜在空间。
- en: 'We start by flattening the image input to a 1D vector. The parameter `layers`
    is a list; the number of elements is the number of hidden layers, and the element
    value is the number of units at that layer. Since we are progressively pooling,
    each subsequent element value is progressively smaller. While an encoder tends
    to be shallow in layers when compared to a CNN used for classification, we add
    batch normalization for its regularizing effect:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将图像输入展平成一个一维向量。参数`layers`是一个列表；元素的数量是隐藏层的数量，元素值是该层的单元数。由于我们是逐步池化，每个后续元素的值逐渐减小。与用于分类的CNN相比，编码器在层上通常较浅，我们添加批量归一化以增强其正则化效果：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Flattening of the input image
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入图像的展平
- en: ❷ Progressive unit pooling (dimensionality reduction)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 逐步单元池化（降维）
- en: ❸ The encoding (latent space)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编码（潜在空间）
- en: 9.1.3 Decoder
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 解码器
- en: Now let’s look at an example implementation of a decoder for an autoencoder.
    Again, following the process depicted in figure 9.1, we progressively unpool the
    number of nodes (hidden units) through the parameter layers. The output from the
    decoder is the reconstructed image. For symmetry with the encoder, we iterate
    through the `layers` parameter in the reverse direction. The activation function
    for the final `Dense` layer is a `sigmoid`. Why? Each node represents a reconstructed
    pixel. Since we’ve normalized the image data between 0 and 1, we want to squash
    the output into the same range of 0 to 1.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看自动编码器解码器的一个示例实现。同样，遵循图9.1中描述的过程，我们通过`layers`参数逐步反池化节点（隐藏单元）的数量。解码器的输出是重构的图像。为了与编码器对称，我们以相反的方向遍历`layers`参数。最终`Dense`层的激活函数是`sigmoid`。为什么？每个节点代表一个重构的像素。由于我们已经将图像数据归一化到0到1之间，我们希望将输出挤压到相同的0到1范围内。
- en: 'Finally, to reconstruct the image, we do a `Reshape` to reshape the 1D vector
    from the final `Dense` layer into an image format (*H* × *W* × *C*):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了重构图像，我们对来自最终`Dense`层的1D向量进行`Reshape`操作，将其重塑为图像格式（*H* × *W* × *C*）：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Progressive unit unpooling (dimensionality expansion)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 逐步单元反池化（维度扩展）
- en: ❷ Last unpooling
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 最后一次反池化
- en: ❸ Reshapes back into the image input shape
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重塑回图像输入形状
- en: ❹ The decoded image
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 解码后的图像
- en: 9.1.4 Training
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 训练
- en: The autoencoder wants to learn a representation (which we call the *latent space*)
    of a lower dimensionality and then learn a transformation to reconstruct the image
    according to a predefined task; in this case, the identity function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器想要学习一个低维度的表示（我们称之为*潜在空间*），然后学习一个根据预定义任务重构图像的变换；在这种情况下，恒等函数。
- en: The following code example will train the preceding autoencoder to learn an
    identity function for the MNIST dataset. The example creates an autoencoder with
    hidden units 256, 128, 64 (latent space), 128, 256, and 784 (for pixel reconstruction).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例将训练前面的自动编码器，以学习MNIST数据集的恒等函数。该示例创建了一个具有隐藏单元256、128、64（潜在空间）、128、256和784（用于像素重构）的自动编码器。
- en: Typically, a DNN autoencoder will consist of three or sometimes four layers
    in both the encoder and decoder component. Since DNNs have limited effectiveness,
    adding more capacity as in layers generally will not improve on learning the identity
    function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个深度神经网络自动编码器在编码器和解码器组件中都会包含三个或有时四个层。由于DNNs的有效性有限，增加更多的容量通常不会提高学习恒等函数的效果。
- en: Another convention you see here for DNN autoencoders is that each layer in the
    encoder reduces the number of nodes by one-half and, conversely, the decoder doubles
    the number of nodes, with the exception of the last layer. The last layer reconstructs
    the image, so the number of nodes is the same as the number of pixels in the input
    vector; in this case, 784\. The choice of starting with 256 nodes in the example
    is somewhat arbitrary; other than starting with a large size that would increase
    capacity, it would aid little, or not at all, in improving learning the identity
    function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DNN自编码器，你在这里看到的另一个约定是，编码器中的每一层将节点数量减半，相反，解码器将节点数量加倍，除了最后一层。最后一层重建图像，因此节点数量与输入向量的像素数量相同；在这种情况下，784。在示例中选择从256个节点开始是有些任意的；除了从一个大尺寸开始会增加容量外，它对提高学习恒等函数的能力帮助很小，或者根本不起作用。
- en: For the dataset, we expand the image shape from (28, 28) to (28, 28, 1) since
    the TF.Keras models expect the number of channels to be explicitly specified—even
    when there is just one channel. Finally, we train the autoencoder with the `fit()`
    method and pass `x_train` as both the training data and the corresponding labels
    (identity function). Likewise, when evaluating, we pass `x_test` as both the test
    data and corresponding labels. Figure 9.2 shows an autoencoder learning the identity
    function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集，我们将图像形状从（28，28）扩展到（28，28，1），因为TF.Keras模型期望显式指定通道数——即使只有一个通道。最后，我们使用`fit()`方法训练自编码器，并将`x_train`作为训练数据和相应的标签（恒等函数）。同样，在评估时，我们将`x_test`作为测试数据和相应的标签。图9.2显示了自编码器学习恒等函数。
- en: '![](Images/CH09_F02_Ferlitsch.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH09_F02_Ferlitsch.png)'
- en: 'Figure 9.2 An The autoencoder learns two functions: the encoder learns to convert
    a high-dimensional representation to a low-dimensional representation, and then
    a decoder learns to reconstruct back to a high-dimensional representation that
    is a translation of the input.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 自编码器学习两个函数：编码器学习将高维表示转换为低维表示，然后解码器学习将输入转换回高维表示，即输入的翻译。
- en: 'The following code demonstrates the construction and training of an autoencoder,
    as depicted in figure 9.2, where the training data is the MNIST dataset:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如图9.2所示的自动编码器的构建和训练，其中训练数据是MNIST数据集：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Metaparameter for number of filters per layer
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每层的过滤器数量元参数
- en: ❷ Constructs the autoencoder
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建自动编码器
- en: ❸ Unsupervised training, where the input and labels are the same
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 无监督训练，其中输入和标签相同
- en: Let’s summarize. The autoencoder wants to learn a representation (the latent
    space) of a lower dimensionality and then learn a transformation to reconstruct
    the image according to a predefined task, such as the identity function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下。自编码器想要学习一个低维度的表示（潜在空间），然后学习一个变换来根据预定义的任务（如恒等函数）重建图像。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for a DNN autoencoder is available on GitHub ([http://mng.bz/JvaK](https://shortener.manning.com/JvaK)).
    Next, we will describe designing and coding an autoencoder by using convolutional
    layers in place of dense layers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic procedure reuse设计模式为DNN自编码器提供的完整代码版本可在GitHub上找到（[http://mng.bz/JvaK](https://shortener.manning.com/JvaK)）。接下来，我们将描述如何使用卷积层代替密集层来设计和编写一个自编码器。
- en: 9.2 Convolutional autoencoders
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 卷积自编码器
- en: With the small images in the MNIST or CIFAR-10 datasets, the DNN autoencoders
    work fine. But when we work with larger images, autoencoders using nodes (which
    are the hidden units) for (un)pooling are computationally expensive. For larger
    images, deep convolutional (DC) autoencoders are more efficient. Instead of learning
    to (un)pool nodes, they learn to (un)pool feature maps. To do this, they use convolutions
    in the encoder, and *deconvolutions*, also known as *transpose convolutions*,
    in the decoder.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST或CIFAR-10数据集中的小图像中，DNN自编码器运行良好。但是，当我们处理较大图像时，使用节点（即隐藏单元）进行（反）池化的自编码器在计算上很昂贵。对于较大图像，深度卷积（DC）自编码器更有效。它们不是学习（反）池化节点，而是学习（反）池化特征图。为此，它们在编码器中使用卷积，在解码器中使用*反卷积*，也称为*转置卷积*。
- en: While a strided convolution, which does feature pooling, learns the optimal
    method to downsample a distribution, a strided deconvolution (feature unpooling)
    does the opposite and learns the optional method to upsample a distribution. Both
    feature pooling and unpooling are depicted in figure 9.3.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当步长卷积（进行特征池化）学习下采样分布的最佳方法时，步长反卷积（特征反池化）则做相反的操作，并学习上采样分布的可行方法。特征池化和反池化都在图9.3中展示。
- en: Let’s describe this process by using the same context as for the DNN autoencoder
    for MNIST. In that example, the encoder and decoder each had three layers, and
    the encoder started with 256 feature maps. The corresponding equivalent for a
    CNN autoencoder would be an encoder with three convolution layers of 256, 128,
    and 64 filters, respectively, and the decoder with three deconvolution layers
    of 128, 256, and C , where C is the number of channels of the input, respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与MNIST的DNN自动编码器相同的上下文来描述这个过程。在那个例子中，编码器和解码器各有三层，编码器从256个特征图开始。对于CNN自动编码器，相应的等效结构是一个编码器，具有256、128和64个过滤器的三个卷积层，以及一个具有128、256和C个过滤器的解码器，其中C是输入的通道数。
- en: '![](Images/CH09_F03_Ferlitsch.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F03_Ferlitsch.png)'
- en: Figure 9.3 Contrasting feature pooling with feature unpooling
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 对比特征池化与特征反池化
- en: 9.2.1 Architecture
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 架构
- en: 'The macro-architecture for a deep convolutional autoencoder (DC-autoencoder)
    can be decomposed as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积自动编码器（DC自动编码器）的宏观架构可以分解如下：
- en: '*Stem*—Does coarse-level feature extraction'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Stem*—进行粗粒度特征提取'
- en: '*Learner*—Does representational and transformational learning'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Learner*—代表性和转换性学习'
- en: '*Task* (*reconstruction*)—Does projection and reconstruction'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Task* (*重建*)—进行投影和重建'
- en: Figure 9.4 shows the macro-architecture for a DC-autoencoder.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4显示了DC自动编码器的宏观架构。
- en: '![](Images/CH09_F04_Ferlitsch.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F04_Ferlitsch.png)'
- en: Figure 9.4 The DC-autoencoder macro-architecture distinguishes between representational
    learning and transformational learning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 DC自动编码器的宏观架构区分了表示学习和转换学习。
- en: 9.2.2 Encoder
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 编码器
- en: The *encoder* in a deep convolutional autoencoder (shown in figure 9.5) progressively
    reduces the number of feature maps (via feature reduction) and the size of the
    feature maps (via feature pooling) using strided convolutions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积自动编码器（如图9.5所示）中的*编码器*通过使用步长卷积逐步减少特征图的数量（通过特征减少）和特征图的大小（通过特征池化）。
- en: '![](Images/CH09_F05_Ferlitsch.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F05_Ferlitsch.png)'
- en: Figure 9.5 Progressive reduction in number and size of output feature maps in
    a CNN encoder
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 CNN编码器中输出特征图的数量和尺寸的逐步减少
- en: As you can see, the encoder progressively reduces the number of filters, also
    known as *channels*, and corresponding size. The output from the encoder is the
    latent space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，编码器逐步减少过滤器的数量，也称为*通道*，以及相应的尺寸。编码器的输出是潜在空间。
- en: Now take a look at an example code implementation of an encoder. The parameter
    `layers` is a list, in which the number of elements is the number of convolutional
    layers and the element value is the number of filters per convolution. Since we
    are progressively pooling, each subsequent element value is progressively smaller.
    Additionally, each convolutional layer further pools the feature maps by reducing
    their size using a stride of 2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个编码器的示例代码实现。参数`layers`是一个列表，其中元素的数量是卷积层的数量，元素值是每个卷积的过滤器数量。由于我们是逐步池化，每个后续元素的值都是逐步变小的。此外，每个卷积层通过使用步长为2来减少特征图的大小，进一步对特征图进行池化。
- en: For the convolutions, we use the Conv-BN-RE convention in this implementation.
    You may want to try to see if you get better results using BN-RE-Conv.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，对于卷积，我们使用Conv-BN-RE约定。你可能想尝试使用BN-RE-Conv来查看是否能得到更好的结果。
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Progressive feature pooling (dimensionality reduction)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 逐步特征池化（降维）
- en: ❷ The encoding (latent space)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码（潜在空间）
- en: 9.2.3 Decoder
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 解码器
- en: Now for the decoder, shown in figure 9.6\. The decoder progressively increases
    the number of feature maps (via feature expansion) and the size of the feature
    maps (via feature unpooling) by using strided deconvolutions (transpose convolutions).
    The last unpooling layer projects the feature maps according to the reconstruction
    task. For the identity function example, the layer would project the feature maps
    into the shape of the input images to the encoder.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器，如图9.6所示。解码器通过使用步长反卷积（转置卷积）逐步增加特征图的数量（通过特征扩展）和特征图的大小（通过特征反池化）。最后一个反池化层根据重建任务将特征图投影。对于恒等函数示例，该层将特征图投影到编码器输入图像的形状。
- en: '![](Images/CH09_F06_Ferlitsch.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F06_Ferlitsch.png)'
- en: Figure 9.6 Progressive expansion in number and size of output feature maps in
    a CNN decoder
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 CNN解码器中输出特征图数量和尺寸的渐进扩展
- en: 'Here’s an implementation of a decoder for the identity function. In this example,
    the output is an RGB image; therefore, there are three filters on the last transpose
    convolution, each one corresponding to an RGB channel:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个实现恒等函数解码器的示例。在这个例子中，输出是一个RGB图像；因此，在最后一个转置卷积层上有三个过滤器，每个过滤器对应一个RGB通道：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Progressive feature unpooling (dimensionality expansion)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 渐进特征反池化（维度扩展）
- en: ❷ Last unpooling and restore to image input shape
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 最后的反池化和恢复到图像输入形状
- en: ❸ The decoded image
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解码后的图像
- en: Now let’s assemble the encoder with the decoder.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将编码器与解码器组装起来。
- en: In this example, the convolutional layers will progressively feature pool from
    64, to 32, to 16 filters, and the deconvolutional layers will progressively feature
    unpool from 32, to 64, and then 3 for reconstruction of the image. The image size
    for CIFAR is very small (32 × 32 × 3), so if we add more layers, the latent space
    would be too small for reconstruction, and if we widen the layers with more filters,
    we risk memorization (overfitting) by the additional parameter capacity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，卷积层将逐步从64个、32个到16个过滤器进行特征池化，而反卷积层将逐步从32个、64个到3个过滤器进行特征反池化，以重建图像。对于CIFAR，图像大小非常小（32
    × 32 × 3），因此如果我们添加更多层，潜在空间将太小，无法进行重建；如果我们通过更多过滤器加宽层，我们可能会因为额外的参数容量而面临过拟合（欠拟合）的风险。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Metaparameter for the number of filters per layer in the encoder
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码器每层的过滤器数量元参数
- en: ❷ Constructs the autoencoder
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建自编码器
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for a CNN autoencoder is on GitHub ([http://mng.bz/JvaK](https://shortener.manning.com/JvaK)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic程序重用设计模式为CNN自编码器编写的一个完整代码示例在GitHub上（[http://mng.bz/JvaK](https://shortener.manning.com/JvaK)）。
- en: 9.3 Sparse autoencoders
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 稀疏自编码器
- en: The size of the latent space is a tradeoff. If we go too big, the model may
    overfit to the representational space of the training data and not generalize.
    If we go too small, it may underfit so that we are unable to perform the transformation
    and reconstruction for the designated tasks (for example, identity function).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间的大小是一个权衡。如果我们做得太大，模型可能会过度拟合训练数据的表示空间，而无法泛化。如果我们做得太小，它可能会欠拟合，以至于我们无法执行指定的任务（例如，恒等函数）的转换和重建。
- en: We want to find that “sweet spot” between the two. One method to increase the
    likelihood of the autoencoder to not under- or overfit is to add a *sparsity constraint*.
    The concept of the sparsity constraint is to limit the activation of the neurons
    on the bottleneck layer that outputs the latent space. This acts as both a squashing
    function and a regularizer, which helps the autoencoder generalize the latent
    space representation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在这两者之间找到一个“甜蜜点”。为了增加自编码器不过度拟合或欠拟合的可能性，一种方法是添加一个*稀疏性约束*。稀疏性约束的概念是限制瓶颈层输出潜在空间的神经元激活。这既是一个压缩函数，也是一个正则化器，有助于自编码器泛化潜在空间表示。
- en: The sparsity constraint is typically described as activating only the units
    with large activation values and making the rest output zero. In other words,
    activations that are close to zero are set to zero (sparseness).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性约束通常描述为仅激活具有大激活值的单元，并使其余单元输出为零。换句话说，接近零的激活被设置为零（稀疏性）。
- en: 'Mathematically, we could state this as follows: we want the activation of any
    unit (σ[i]) to be constrained within the vicinity of the average activation value
    (σ[µ]):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以这样表述：我们希望任何单元（σ[i]）的激活被限制在平均激活值（σ[µ]）的附近：
- en: σ[i] `≈` σ[µ]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: σ[i] `≈` σ[µ]
- en: To achieve this, we add a penalty term that penalizes an activation σ[i] when
    it deviates significantly from σ[µ].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们添加了一个惩罚项，该惩罚项惩罚当激活 σ[i] 显著偏离 σ[µ] 时。
- en: In TF.Keras, we add the sparsity constraint with the `activity_regularizer`
    parameter to the last layer in the encoder. The value specifies the threshold
    for which an activation within +/– of zero is changed to zero. A typical value
    is 1e-4.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TF.Keras 中，我们通过在编码器的最后一层添加 `activity_regularizer` 参数来添加稀疏性约束。该值指定了激活值在 +/–
    零附近的阈值，将其更改为零。一个典型的值是 1e-4。
- en: 'Here’s the implementation of a DC-autoencoder using a sparsity constraint.
    The parameter `layers` is a list of progressively pooling the number of feature
    maps. We start by popping off the end of the list, which is the last layer in
    the encoder. We then proceed to build the remaining layers. We then use the number
    of feature maps in the popped (last) layer to construct the last layer, where
    we add the sparsity constraint. This last convolutional layer is the latent space:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用稀疏性约束实现的 DC-自编码器的实现。参数 `layers` 是一个列表，表示逐步池化特征图的数量。我们首先从列表的末尾弹出，这是编码器的最后一层。然后我们继续构建剩余的层。然后我们使用弹出（最后一层）的特征图数量来构建最后一层，其中我们添加稀疏性约束。这个最后的卷积层是潜在空间：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Sets aside the last layer
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保留最后一层
- en: ❷ Feature pooling
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 特征池化
- en: ❸ Adds sparsity constraint to last layer in encoder
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在编码器的最后一层添加稀疏性约束
- en: 9.4 Denoising autoencoders
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 去噪自编码器
- en: 'Another way to use an autoencoder is to train it as an image denoiser. We input
    a noisy image, and then output a denoised version of the image. Think of this
    process as learning the identity function with some noise. If we represent this
    process as an equation, assume *x* is the image and *e* is the noise. The function
    learns to return *x*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自编码器的另一种方式是将其训练为图像去噪器。我们输入一个噪声图像，然后输出图像的去噪版本。将这个过程视为学习带有一些噪声的恒等函数。如果我们用方程表示这个过程，假设
    *x* 是图像，*e* 是噪声。该函数学习返回 *x*：
- en: '*f*(*x* + *e*) = *x*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x* + *e*) = *x*'
- en: 'We don’t need to change the autoencoder architecture for this purpose; instead,
    we change our training data. Changing the training data requires three basic steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要为此目的更改自编码器架构；相反，我们更改我们的训练数据。更改训练数据需要三个基本步骤：
- en: Construct a random generator that will output a random distribution with the
    value range of the noise you want to add to the training (and test) images.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个随机生成器，它将输出一个具有你想要添加到训练（和测试）图像中的噪声值范围的随机分布。
- en: When training, add the noise to the training data.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练时，向训练数据中添加噪声。
- en: For labels, use the original images.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于标签，使用原始图像。
- en: 'Here’s code for training an autoencoder to denoise. We set the noise to be
    within a normal distribution centered at 0.5 with a standard deviation of 0.5\.
    We then add the random noise distribution to a copy of the training data (`x_train_noisy`).
    We use the `fit()` method to train the denoiser, where the noisy training data
    is the training data and the original (denoised) training data is the corresponding
    labels:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是训练用于去噪的自编码器的代码。我们将噪声设置为在以 0.5 为中心的正态分布内，标准差为 0.5。然后我们将随机噪声分布添加到训练数据的副本（`x_train_noisy`）中。我们使用
    `fit()` 方法来训练去噪器，其中噪声训练数据是训练数据，原始（去噪）训练数据是对应的标签：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Generates the noise as a normal distribution centered at 0.5 and standard
    deviation of 0.5
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成噪声为以 0.5 为中心，标准差为 0.5 的正态分布
- en: ❷ Adds the noise to a copy of the image training data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将噪声添加到图像训练数据的副本中
- en: ❸ Trains the encoder by feeding the noisy images as the training data and the
    original images as the labels
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过将噪声图像作为训练数据，原始图像作为标签来训练编码器
- en: 9.5 Super-resolution
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 超分辨率
- en: 'Autoencoders were also used to develop models for *super-resolution* (*SR*).
    This process takes a low-resolution (LR) image and upscales it to improve the
    details to a high-resolution (HR) image. Instead of learning an identity function,
    such as in compression, or a noisy identity function as in denoising, we want
    to learn a representational mapping between a low-resolution image and a high-resolution
    image. Let’s use a function to express this mapping that we want to learn:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器也被用来开发用于 *超分辨率* (*SR*) 的模型。这个过程将低分辨率（LR）图像上采样以提高细节，以获得高分辨率（HR）图像。与压缩中学习恒等函数或去噪中学习噪声恒等函数不同，我们想要学习低分辨率图像和高分辨率图像之间的表示映射。让我们用一个函数来表示我们想要学习的这个映射：
- en: '*f*(*x*[lr]) = *x*[hr]'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*[lr]) = *x*[hr]'
- en: In this equation, *f()* represents the transformation function that is being
    learned by the model. The term *x*[lr] represents the low-resolution image input
    to the function, and the term *x*[hr] is the transformed high-resolution predicted
    output from the function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*f()*代表模型正在学习的变换函数。术语*x*[lr]代表函数输入的低分辨率图像，而术语*x*[hr]是函数从高分辨率预测输出的变换。
- en: Although very advanced models now do super-resolution, early versions (~2015)
    used variations of autoencoders to learn a mapping from a low-resolution representation
    and a high-resolution representation. One example is the super-resolution convolutional
    neural network (SRCNN) model, which was presented in “Image Super-Resolution Using
    Deep Convolutional Networks” by Chao Dong et al. ([https://arxiv.org/pdf/1501.00092.pdf](https://arxiv.org/pdf/1501.00092.pdf)).
    In this approach, the model learns a representation (latent space) of the low-resolution
    image in a high-dimensional space. Then it learns a mapping from the high-dimensional
    space of the low-resolution image to a high-resolution image, to reconstruct the
    high-resolution image. Note, this is opposite of a typical autoencoder, which
    learns a representation in a low-dimensional space.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现在非常先进的模型可以进行超分辨率处理，但早期版本（约2015年）使用自动编码器的变体来学习从低分辨率表示到高分辨率表示的映射。一个例子是Chao
    Dong等人提出的超分辨率卷积神经网络（SRCNN）模型，该模型在“使用深度卷积网络进行图像超分辨率”一文中被介绍（[https://arxiv.org/pdf/1501.00092.pdf](https://arxiv.org/pdf/1501.00092.pdf)）。在这种方法中，模型学习在多维空间中对低分辨率图像的表示（潜在空间）。然后它学习从低分辨率图像的高维空间到高分辨率图像的映射，以重建高分辨率图像。注意，这与典型的自动编码器相反，自动编码器在低维空间中学习表示。
- en: 9.5.1 Pre-upsampling SR
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 预上采样SR
- en: 'The creators of the SRCNN model introduced the use of a fully convolutional
    neural network for image super-resolution. This approach is called a *pre-upsampling
    SR approach*, depicted in figure 9.7\. We can decompose the model into four components:
    a low-resolution feature extraction, a high-dimensional representation, an encoder
    to a low-dimensional representation, and a convolutional layer for reconstruction.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: SRCNN模型的创造者引入了全卷积神经网络在图像超分辨率中的应用。这种方法被称为*预上采样SR方法*，如图9.7所示。我们可以将模型分解为四个组件：低分辨率特征提取、高维表示、编码到低维表示，以及用于重建的卷积层。
- en: '![](Images/CH09_F07_Ferlitsch.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F07_Ferlitsch.png)'
- en: Figure 9.7 A pre-upsampling super-resolution model learns to reconstruct a high-resolution
    image from a low-resolution image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 预上采样超分辨率模型学习从低分辨率图像重建高分辨率图像。
- en: Let’s dive into more detail. Unlike in an autoencoder, there is no feature pooling
    (or downsampling) in the low-resolution feature extraction component. Instead,
    the size of the feature maps stays the same as the size of the channels in the
    low-input image. For example, if the input shape is (16, 16, 3), the *H* × *W*
    of the feature maps will stay 16 × 16.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解。与自动编码器不同，在低分辨率特征提取组件中没有特征池化（或下采样）。相反，特征图的大小与低输入图像中的通道大小相同。例如，如果输入形状是（16，16，3），则特征图的*H*
    × *W*将保持16 × 16。
- en: In the stem convolution, the number of feature maps is substantially increased
    from the number of channels (3) in the input, which gives us the high-dimensionality
    representation of the low-resolution image. An encoder then reduces the high-dimensionality
    representation into a low-dimensionality representation. A final convolution reconstructs
    the image as a high-resolution image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在主干卷积中，特征图的数量从输入的通道数（3）显著增加到，这为我们提供了低分辨率图像的高维表示。然后编码器将高维表示降低到低维表示。最后的卷积将图像重建为高分辨率图像。
- en: Typically, you would train this approach to a super-resolution model by using
    an existing image dataset that becomes the HR images. You then make a copy of
    the training data, where each image has been resized smaller and then resized
    back to its original size. To do both resizings, you use a static algorithm, like
    bicubic interpolation. The LR images will be the same size as the HR images, but
    because of the approximations done during the resizing operations, the LR images
    will be of lower quality than the original images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您会通过使用现有的图像数据集来训练这种方法，该数据集成为HR图像。然后您复制训练数据，其中每个图像都已被调整大小为更小，然后调整回原始大小。为了进行这两次调整大小，您使用静态算法，如双三次插值。LR图像将与HR图像具有相同的大小，但由于调整大小操作期间所做的近似，LR图像的质量将低于原始图像。
- en: 'What exactly is interpolation, and, more specifically, *bicubic interpolation*?
    Think of it this way: if we have 4 pixels and replace them with 2 pixels, or vice
    versa, you need a mathematical method that makes a good estimate for the replacement
    representation—this is what *interpolation* is. *Cubic interpolation* is a specific
    method for doing this with a vector (1D), and *bicubic* is a variation used for
    a matrix (2D). For image reduction, bicubic interpolation tends to give a better
    estimation than other interpolation algorithms.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 究竟什么是插值，更具体地说，*双三次插值*？可以这样想：如果我们有 4 个像素，用 2 个像素替换它们，或者反过来，你需要一种数学方法来对替换表示进行良好的估计——这就是插值。*三次插值*是用于向量的特定方法（1D），而
    *双三次* 是用于矩阵（2D）的变体。对于图像缩小，双三次插值通常比其他插值算法给出更好的估计。
- en: 'Here’s a code example to demonstrate this training data preparation using the
    CIFAR-10 dataset. In this example, the NumPy array `x_train` contains the training
    data images. We then make a mirror list `x_train_lr` for the low-resolution pairs
    by sequentially first resizing each image in `x_train` to one-half the *H* × *W*
    (16, 16), and then resize the image back to the original *H* × *W* (32, 32), and
    place it at the same index location in `x_train_lr`. Finally, we normalize the
    pixel data in both sets of images:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个代码示例，用于展示使用 CIFAR-10 数据集进行此训练数据准备的过程。在这个例子中，NumPy 数组 `x_train` 包含了训练数据图像。然后我们通过依次将
    `x_train` 中的每个图像调整大小到一半的 *H* × *W*（16, 16），然后将图像调整回原始的 *H* × *W*（32, 32），并在 `x_train_lr`
    中放置相同的索引位置，来创建一个低分辨率配对列表 `x_train_lr`。最后，我们对两组图像中的像素数据进行归一化：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Downloads into memory the CIFAR-10 dataset as the high-resolution images
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 CIFAR-10 数据集下载到内存中作为高分辨率图像
- en: ❷ Makes a low-resolution pairing of the training images
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建训练图像的低分辨率配对
- en: ❸ Normalizes the pixel data for training
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对训练中的像素数据进行归一化
- en: Now let’s look at code for a pre-upsampling SR model for HR reconstruction quality
    on small images like CIFAR-10\. To train it, we treat the original CIFAR-10 32
    × 32 images (`x_train`) as the HR images, and the mirrored pairing images (`x_train_lr`)
    as the LR images. For training, the LR images are the input, and the paired HR
    images are the corresponding label.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看用于在小型图像（如 CIFAR-10）上实现高分辨率重建质量的预上采样 SR 模型的代码。为了训练它，我们将原始 CIFAR-10 32
    × 32 图像（`x_train`）视为高分辨率图像，将镜像配对图像（`x_train_lr`）视为低分辨率图像。对于训练，低分辨率图像是输入，配对的 HR
    图像是相应的标签。
- en: 'This example gets fairly good reconstruction results on CIFAR-10 in just 20
    epochs with an 88% reconstruction accuracy. As you can see in the code, the `stem()`
    component does the low-resolution feature extraction using a coarse 9 × 9 filter
    and outputs 64 feature maps for the high-dimensional representation. The `encoder()`
    consists of a convolution to reduce the low-resolution representation from a high
    to a low dimensionality using a 1 × 1 bottleneck convolution and reducing the
    number of feature maps to 32\. A final convolution using a coarse 5 × 5 filter
    learns the mapping from the low-resolution representation to a high resolution
    for reconstruction:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子在 CIFAR-10 上仅用 20 个周期就得到了相当好的重建结果，重建准确率为 88%。如代码所示，`stem()` 组件使用粗略的 9 ×
    9 滤波器进行低分辨率特征提取，并为高维表示输出 64 个特征图。`encoder()` 由一个卷积组成，使用 1 × 1 瓶颈卷积将低分辨率表示从高维度降低到低维度，并将特征图的数量减少到
    32。最后，使用粗略的 5 × 5 滤波器学习从低分辨率表示到高分辨率的映射以进行重建：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The low-resolution feature extraction
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 低分辨率特征提取
- en: ❷ The high-dimensional representation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 高维表示
- en: ❸ 1 × 1 bottleneck convolution as the encoder
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 作为编码器的 1 × 1 瓶颈卷积
- en: ❹ 5 × 5 convolution for the reconstruction into HR image
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用于将重建为高分辨率图像的 5 × 5 卷积
- en: Let’s see some actual images now. Figure 9.8 shows a set of the same image of
    a peacock from the CIFAR-10 training data. The first two images are the low- and
    high-resolution pair used in training, and the third is the super-resolution reconstruction
    of the same peacock image after the model was trained. Notice that the low-resolution
    image has more artifacts—regions that are boxy, without smoothness in color transitions
    around contours—than the high-resolution image. The reconstructed SR image shows
    a smoother color transition around contours, similar to that of the high-resolution
    image.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一些实际的图像。图 9.8 展示了 CIFAR-10 训练数据集中同一只孔雀的一组图像。前两个图像是用于训练的低分辨率和高分辨率图像对，第三个是模型训练后对同一孔雀图像的超分辨率重建。请注意，低分辨率图像比高分辨率图像有更多的伪影——即边缘周围的区域是方形的，颜色过渡不平滑。重建的超分辨率图像在边缘周围的色彩过渡更平滑，类似于高分辨率图像。
- en: '![](Images/CH09_F08_Ferlitsch.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F08_Ferlitsch.png)'
- en: Figure 9.8 Comparison of LR, HR pairing and reconstructed SR image for pre-upsampling
    SR
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 预上采样超分辨率中 LR、HR 配对和重建 SR 图像的比较
- en: 9.5.2 Post-upsampling SR
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 后上采样超分辨率
- en: 'Another example of an SRCNN style of model is a post-upsampling SR model, depicted
    in figure 9.9\. We can decompose this model into three components: a low-resolution
    feature extraction, a high-dimensional representation, and a decoder for reconstruction.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 SRCNN 风格模型的例子是后上采样超分辨率模型，如图 9.9 所示。我们可以将这个模型分解为三个部分：低分辨率特征提取、高维表示和重建的解码器。
- en: '![](Images/CH09_F09_Ferlitsch.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F09_Ferlitsch.png)'
- en: Figure 9.9 A post-upsampling super-resolution model
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 后上采样超分辨率模型
- en: Let’s dive into more detail. Again, unlike in an autoencoder, there is no feature
    pooling (or downsampling) in the low-resolution feature extraction component.
    Instead, the size of the feature maps stays the same as the size as the channels
    in the low-input image. For example, if the input shape is (16, 16, 3), the *H*
    × *W* of the feature maps will stay 16 × 16.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨。与自动编码器不同，在低分辨率特征提取组件中没有特征池化（或下采样）。相反，特征图的大小与低输入图像中的通道大小相同。例如，如果输入形状是
    (16, 16, 3)，特征图的 *H* × *W* 将保持 16 × 16。
- en: During the convolutions, we progressively increase the number of feature maps—which
    is where we get the high-dimensional space. For example, we might go from the
    three-channel input to 16, then 32, and then 64 feature maps. So you may be asking
    why the higher dimensionality? We want the abundance of different low-resolution
    feature extraction representations to aid us in learning a mapping from them to
    a high resolution so we can do a reconstruction using a deconvolution. But if
    we have too many feature maps, we might be exposing the model to memorizing mappings
    in the training data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积过程中，我们逐步增加特征图的数量——这就是我们得到高维空间的原因。例如，我们可能从三通道输入到 16，然后到 32，再到 64 个特征图。所以你可能想知道为什么维度更高？我们希望丰富的不同低分辨率特征提取表示有助于我们学习从它们到高分辨率的映射，这样我们就可以使用反卷积进行重建。但是，如果我们有太多的特征图，我们可能会使模型暴露在训练数据中的映射记忆中。
- en: Typically, we train a super-resolution model using an existing image dataset
    that will be the HR images, and then make a copy of the training data in which
    each image has been resized smaller for the LR image pairs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们使用现有的图像数据集来训练超分辨率模型，这些数据集将成为高分辨率图像，然后复制训练数据，其中每个图像都被调整大小以生成低分辨率图像对。
- en: The following code example demonstrates this training data preparation using
    the CIFAR-10 dataset. In this example, the NumPy array `x_train` contains the
    training data images. We then make a mirror list `x_train_lr` for the low-resolution
    pairs by sequentially resizing each image in `x_train` and placing it at the same
    index location in `x_train_lr`. Finally, we normalize the pixel data in both sets
    of images.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了使用 CIFAR-10 数据集进行此训练数据准备的过程。在这个例子中，NumPy 数组 `x_train` 包含训练数据图像。然后我们通过逐个调整
    `x_train` 中每个图像的大小，并将其放置在 `x_train_lr` 中的相同索引位置，创建了一个低分辨率图像对列表 `x_train_lr`。最后，我们对两组图像中的像素数据进行归一化。
- en: In the post-upsampling case, the LR images are left as 16 × 16 and not resized
    back to 32 × 32, as was in the case of pre-upsampling; whereby the lower resolution
    was injected by the loss of pixel information through static interpolation when
    resizing back to 32 × 32.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在后上采样的情况下，低分辨率图像保持为 16 × 16，而不是像预上采样那样调整回 32 × 32，这是因为在调整回 32 × 32 时，通过静态插值丢失了像素信息。
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Downloads into memory the CIFAR-10 dataset as the high-resolution images
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 CIFAR-10 数据集作为高分辨率图像下载到内存中
- en: ❷ Makes a low-resolution pairing of the training images
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对训练图像进行低分辨率配对
- en: ❸ Normalizes the pixel data for training
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对训练的像素数据进行归一化
- en: The following is a code implementation of a post-upsampling SR model that gets
    good HR reconstruction quality on small images like CIFAR-10\. We’ve coded this
    implementation specifically for CIFAR-10\. To train it, we treat the original
    CIFAR-10 32 × 32 images (`x_train`) as the HR images, and the mirrored pairing
    images (`x_train_lr`) as the LR images. For training, the LR images are the input,
    and the paired HR images are the corresponding label.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码实现了一个后上采样 SR 模型，它在 CIFAR-10 等小图像上获得了良好的 HR 重建质量。我们专门为 CIFAR-10 编写了这个实现。为了训练它，我们将原始
    CIFAR-10 32 × 32 图像 (`x_train`) 作为 HR 图像，将镜像配对图像 (`x_train_lr`) 作为 LR 图像。对于训练，LR
    图像是输入，配对的 HR 图像是相应的标签。
- en: 'This example gets fairly good reconstruction results on CIFAR-10 in just 20
    epochs with a 90% reconstruction accuracy. In this example, the `stem()` and `learner()`
    components do the low-resolution feature extraction, and progressively expand
    the feature map dimensionality from 16, 32, and then 64 feature maps. The output
    from the last convolution of 64 feature maps is the high-dimensional representation.
    The `decoder()` consists of a deconvolution to learn the mapping from the low-resolution
    representation to a high-resolution for reconstruction:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例在 CIFAR-10 上仅用 20 个 epoch 就获得了相当好的重建结果，重建准确率达到 90%。在这个示例中，`stem()` 和 `learner()`
    组件执行低分辨率特征提取，并逐步扩展特征图维度从 16、32 到 64 个特征图。64 个特征图的最后一个卷积的输出是高维表示。`decoder()` 由一个反卷积组成，用于学习从低分辨率表示到高分辨率的映射以进行重建：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The low-resolution feature extraction
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 低分辨率特征提取
- en: ❷ The high-dimensional representation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 高维表示
- en: ❸ The low- to high-resolution reconstruction
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 低到高分辨率重建
- en: Let’s go back to those same peacock images we looked at earlier. In figure 9.10,
    the first two images are the low- and high-resolution pair used in training, and
    the third is the super-resolution reconstruction of the same peacock image after
    the model was training. As in the previous pre-upsampling SR model, the post-upsampling
    SR model produced a reconstructed SR image with fewer artifacts than the low-resolution
    image.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到之前看过的那些孔雀图像。在图 9.10 中，前两个图像是用于训练的低分辨率和高分辨率配对，第三个是模型训练后对同一孔雀图像的超分辨率重建。与之前的预上采样
    SR 模型一样，后上采样 SR 模型产生的重建 SR 图像比低分辨率图像的伪影更少。
- en: '![](Images/CH09_F10_Ferlitsch.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH09_F10_Ferlitsch.png)'
- en: Figure 9.10 Comparison of LR, HR pairing, and reconstructed SR image for post-upsampling
    SR
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 LR、HR 配对和后上采样 SR 重建图像的比较
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for SRCNN is available on GitHub ([http://mng.bz/w0a2](https://shortener.manning.com/w0a2)).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 上提供了使用 Idiomatic 程序重用设计模式对 SRCNN 进行完整代码实现的示例 ([http://mng.bz/w0a2](https://shortener.manning.com/w0a2)).
- en: 9.6 Pretext tasks
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 预训练任务
- en: 'As we have discussed, autoencoders can be trained without labels to learn feature
    extraction of essential features, which we can repurpose beyond the examples given
    so far: compression and denoising.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，自动编码器可以在没有标签的情况下进行训练，以学习关键特征的特征提取，这些特征我们可以重新用于迄今为止给出的示例之外：压缩和去噪。
- en: What do we mean by *essential features*? For imaging, we want our models to
    learn the essential features of the data, and not the data itself. This enables
    the models to not only generalize to unseen data in the same distribution, but
    also be better able to correctly predict when a shift occurs in the input distribution
    after it is deployed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的“关键特征”是什么意思？对于成像，我们希望我们的模型学习数据的本质特征，而不是数据本身。这使得模型不仅能够泛化到同一分布中的未见数据，而且还能在模型部署后，当输入分布发生偏移时，更好地预测其正确性。
- en: For example, let’s say we have a model trained to recognize airplanes, and the
    images used in training consisted of a wide variety of scenes, including on the
    tarmac, taxied to the terminal, and in the air, but none of them in a hanger.
    If, after deploying the model, it now sees planes in a hanger, we have a change
    in the input distribution; this is referred to as *data drift*. And when an image
    of a plane appears in a hanger, we get a lower accuracy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个训练好的模型用于识别飞机，训练时使用的图像包括各种场景，如停机坪、滑向航站楼和在空中，但没有一个是停机库中的。如果在部署模型后，它现在看到了停机库中的飞机，那么输入分布发生了变化；这被称为*数据漂移*。而当飞机图像出现在停机库中时，我们得到的准确度会降低。
- en: In this example case, we might try to improve the model by retraining it with
    additional images that contain planes in the background. Great, now it works when
    deployed. But let’s say the new model sees planes with other backgrounds it was
    not trained on, like planes on water (seaplanes), planes on sand in a plane boneyard,
    planes partially assembled in a factory. Well, in the real world there is always
    something you don’t anticipate!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例案例中，我们可能会尝试通过重新训练模型并添加包含背景中飞机的额外图像来改进模型。很好，现在部署时它工作了。但假设新模型看到了它没有训练过的其他背景中的飞机，比如在水面上的飞机（水上飞机）、在飞机坟场上的沙地上的飞机、在工厂中部分组装的飞机。好吧，在现实世界中，总有你预料不到的事情！
- en: And that is why it is important to learn the essential features in a dataset
    and not the data. For autoencoders to work, they have to learn how the pixels
    are correlated—that is, representational learning. The more correlated, the more
    likely the relationship will show up in the latent space representation, and the
    less correlated, the more likely it will not.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，学习数据集中的基本特征而不是数据本身非常重要。对于自动编码器来说，它们必须学习像素之间的相关性——即表示学习。相关性越强，关系越有可能在潜在空间表示中显现出来，相关性越弱，则不太可能显现。
- en: 'We won’t go into detail here on pretraining with pretext tasks, but we will
    briefly touch on it in the context of an autoencoder. For our purposes, we want
    to use an autoencoder approach to train the stem convolutional group to learn
    to extract the essential coarse-level features, before training the model on the
    dataset. Here are the steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细讨论使用前缀任务进行预训练，但我们将简要地在此处提及它，特别是在自动编码器的上下文中。就我们的目的而言，我们希望使用自动编码器方法来训练主干卷积组，以便在数据集上训练模型之前学习提取基本粗略级特征。以下是步骤：
- en: Do warmup (supervised) training on the target model for numerical stabilization
    (subsequently discussed in chapter 14).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标模型上进行预热（监督学习）训练，以实现数值稳定（将在第14章中进一步讨论）。
- en: Construct an autoencoder consisting of the stem group from the model as the
    encoder and inverted stem group as the decoder.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个自动编码器，其中模型的主干组作为编码器，反转的主干组作为解码器。
- en: Transfer the numerically stabilized weights from the target model to the encoder
    in the autoencoder.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标模型中的数值稳定权重转移到自动编码器的编码器中。
- en: Train (unsupervised) the autoencoder on the pretext task (for example, compression,
    denoising).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前缀任务（例如，压缩、去噪）上训练（无监督学习）自动编码器。
- en: Transfer the pretext task’s trained weights from the encoder of the autoencoder
    to the target model.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前缀任务训练的权重从自动编码器的编码器转移到目标模型。
- en: Train (supervised) the target model.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练（监督学习）目标模型。
- en: Figure 9.11 depicts these steps.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11描述了这些步骤。
- en: '![](Images/CH09_F11_Ferlitsch.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH09_F11_Ferlitsch.png)'
- en: Figure 9.11 Pretraining a stem group using an autoencoder to improve generalization
    to unseen data when the model is fully trained with labeled data
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 使用自动编码器预训练主干组，以改善在模型完全使用标记数据训练后对未见数据的泛化。
- en: Let’s cover one more part of this form of pretext task. It may occur to you
    that the output from a stem convolutional group will be larger than the input.
    While we do either a static or feature pooling on the channels, we increase the
    number of overall channels. For example, we might use pooling to reduce the channel
    size to 25% or even just 6%, but we increase the number of channels from three
    (RGB) to something like 64.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再讨论一下这种前缀任务的一部分。你可能已经想到，来自主干卷积组的输出将大于输入。当我们对通道进行静态或特征池化时，我们增加了总通道数。例如，我们可能使用池化将通道大小减少到25%甚至仅为6%，但我们将通道数从三个（RGB）增加到64个左右。
- en: Thus, the latent space is now larger than the input and much more prone to overfitting.
    For this particular purpose, we build a sparse autoencoder to offset the potential
    to overfit.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，潜在空间现在比输入更大，更容易过拟合。为此特定目的，我们构建了一个稀疏自动编码器来抵消过拟合的潜在可能性。
- en: 'The following is an example implementation. While we have not discussed the
    layer `UpSampling2D`, it is the inverse of a strided `MaxPooling2D`. Instead of
    using a static algorithm to decrease the height and width by one-half, it uses
    a static algorithm to increase the height and width by 2:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例实现。虽然我们尚未讨论`UpSampling2D`层，但它是对步长`MaxPooling2D`的逆操作。它不是使用静态算法将高度和宽度减半，而是使用静态算法将高度和宽度增加2：
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ A 5 × 5 filter for coarse feature extraction with feature pooling
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用5 × 5滤波器进行粗略特征提取并使用特征池化
- en: ❷ Uses max pooling to reduce feature maps to 6% of image size
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用最大池化将特征图减少到图像大小的6%
- en: ❸ Inverts the max pooling
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 反转最大池化
- en: ❹ Inverts the feature pooling and reconstructs the image
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 反转特征池化并重建图像
- en: 'The following is the output from the `summary()` method for this autoencoder.
    Note that the input size equals the output size:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从该自动编码器的`summary()`方法输出的内容。请注意，输入大小等于输出大小：
- en: '[PRE13]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '9.7 Beyond computer vision: sequence to sequence'
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 超越计算机视觉：序列到序列
- en: Let’s take a short look at a basic natural-language processing model architecture
    called *sequence-to-sequence* (*Seq2Seq*). This type of model incorporates both
    natural-language understanding (NLU)—understanding the text, and natural-language
    generation (NLG)—generating new text. For NLG, a Seq2Seq model could do things
    like language translation, summarization, and question and answering. For example,
    chatbots are Seq2Seq models that do question and answering.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看看一种基本的自然语言处理模型架构，称为*序列到序列*（*Seq2Seq*）。此类模型结合了自然语言理解（NLU）——理解文本，和自然语言生成（NLG）——生成新文本。对于NLG，Seq2Seq模型可以执行诸如语言翻译、摘要和问答等操作。例如，聊天机器人是执行问答的Seq2Seq模型。
- en: At the end of chapter 5, we introduced NLU model architecture and saw how the
    component design was comparable to computer vision. We also looked at the attention
    mechanism, which is comparable to the identity link in a residual network. What
    we didn’t cover is the transformer model architecture, which introduced the attention
    mechanism in 2017\. This innovation converted NLU from a time-series-based solution,
    using an RNN, to a spatial problem. In an RNN, the model could look only at chunks
    of the text input at a time and preserve the ordering. Additionally, with each
    chunk, the model had to retain memory of the important features. This added complexity
    to the model design in that you needed cycles in the graph to implement retaining
    previously seen features. With the transformer and attention mechanism, the model
    looks at the text in a single shot.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章的结尾，我们介绍了NLU模型架构，并看到了组件设计如何与计算机视觉相媲美。我们还研究了注意力机制，它与残差网络中的身份链接相当。我们没有涵盖的是在2017年引入的Transformer模型架构，它引入了注意力机制。这一创新将NLU从基于时间序列的解决方案，使用RNN，转变为空间问题。在RNN中，模型一次只能查看文本输入的片段并保持顺序。此外，对于每个片段，模型必须保留重要特征的记忆。这增加了模型设计的复杂性，因为您需要在图中实现循环以保留先前看到的特征。有了Transformer和注意力机制，模型可以一次性查看文本。
- en: Figure 9.12 shows the transformer model architecture, which implements a Seq2Seq
    model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12展示了Transformer模型架构，该架构实现了一个Seq2Seq模型。
- en: '![](Images/CH09_F12_Ferlitsch.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH09_F12_Ferlitsch.png)'
- en: Figure 9.12 Transformer architecture consists of both an encoder for NLU and
    decoder for NLG
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 Transformer架构包括用于NLU的编码器和用于NLG的解码器
- en: As you can see, the learner component consists of both an encoder for NLU and
    a decoder for NLG. You train the model by using text pairs, sentences, paragraphs,
    and the like. For example, if you are training a Q&A chatbot, the input would
    be the questions, and the labels are the answers. For summarization, the input
    would be the text and the label the summarization.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，学习组件包括用于NLU的编码器和用于NLG的解码器。您通过使用文本对、句子、段落等来训练模型。例如，如果您正在训练一个问答聊天机器人，输入将是问题，标签是答案。对于摘要，输入将是文本，标签是摘要。
- en: In the transformer model, the encoder sequentially learns a dimensionality reduction
    of the context of the input, comparable to representational learning by the encoder
    in a computer vision autoencoder. The output from the encoder is referred to as
    the *intermediate representation*, comparable to the latent space in a computer
    vision autoencoder.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换器模型中，编码器按顺序学习输入上下文的降维，这与计算机视觉自动编码器中编码器的表征学习相当。编码器的输出被称为*中间表示*，与计算机视觉自动编码器中的潜在空间相当。
- en: The decoder sequentially learns a dimensionality expansion of the intermediate
    representation into a transformed context, comparable to the transformational
    learning by a decoder in a computer vision autoencoder.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器按顺序学习将中间表示扩展到变换上下文的维度扩展，这与计算机视觉自动编码器中解码器的变换学习相当。
- en: The output from the decoder is passed to the task component, which learns the
    text generation. The text generation task is comparable to the reconstruction
    task in a computer vision autoencoder.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输出传递给任务组件，该组件学习文本生成。文本生成任务与计算机视觉自动编码器中的重建任务相当。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: An autoencoder learns an optimal mapping of an input into a low-dimensional
    representation, and then learns a mapping back to high-dimensional representation,
    such that a transformation reconstruction of the image can be done.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器学习输入到低维表示的最佳映射，然后学习映射回高维表示，以便可以进行图像的变换重建。
- en: Examples of transformation functions that an autoencoder can learn include the
    identity function (compression), denoising an image, and constructing a higher
    resolution of the image.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器可以学习的变换函数示例包括恒等函数（压缩）、去噪图像和构建图像的高分辨率版本。
- en: In a CNN autoencoder, pooling is done with a strided convolution, and unpooling
    is done with a strided deconvolution.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积神经网络自动编码器中，池化操作通过步长卷积完成，而反池化操作通过步长反卷积完成。
- en: Using an autoencoder in unsupervised learning can train a model to learn essential
    features of the dataset distribution without labels.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无监督学习中使用自动编码器可以训练模型学习数据集分布的基本特征，而无需标签。
- en: Using an encoder as a pre-stem with an unsupervised learning pretext task can
    assist in subsequent supervised learning to learn essential features to better
    generalize.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码器作为无监督学习预训练任务的前缀可以辅助后续的监督学习，以学习更好的泛化所需的基本特征。
- en: The Seq2Seq model pattern for NLU uses an encoder and decoder comparable to
    an autoencoder.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLU的Seq2Seq模型模式使用一个编码器和解码器，与自动编码器相当。
