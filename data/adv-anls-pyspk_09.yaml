- en: Chapter 9\. Analyzing Genomics Data and the BDG Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。分析基因组数据和BDG项目
- en: The advent of next-generation DNA sequencing (NGS) technology has rapidly transformed
    the life sciences into a data-driven field. However, making the best use of this
    data is butting up against a traditional computational ecosystem that builds on
    difficult-to-use, low-level primitives for distributed computing and a jungle
    of semistructured text-based file formats.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代DNA测序（NGS）技术的出现迅速将生命科学转变为数据驱动的领域。然而，最好利用这些数据的问题正在于传统的计算生态系统，它基于难以使用的低级别分布式计算基元和半结构化文本文件格式的丛林。
- en: This chapter will serve two primary purposes. First, we introduce a set of popular
    serialization and file formats (Avro and Parquet) that simplify many problems
    in data management. These serialization technologies enable us to convert data
    into compact, machine-friendly binary representations. This helps with movement
    of data across networks and helps with cross-compatibility across programming
    languages. Although we will use data serialization techniques with genomics data,
    the concepts will be useful whenever processing large amounts of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将有两个主要目的。首先，我们介绍了一组流行的序列化和文件格式（Avro和Parquet），这些格式简化了数据管理中的许多问题。这些序列化技术使我们能够将数据转换为紧凑的、机器友好的二进制表示。这有助于在网络中传输数据，并帮助在不同编程语言之间实现交叉兼容。虽然我们将这些序列化技术应用于基因组数据，但这些概念在处理大量数据时也是有用的。
- en: Second, we show how to perform typical genomics tasks in the PySpark ecosystem.
    Specifically, we’ll use PySpark and the open source ADAM library to manipulate
    large quantities of genomics data and process data from multiple sources to create
    a dataset for predicting transcription factor (TF) binding sites. For this, we
    will join genome annotations from the [ENCODE dataset](https://oreil.ly/h0yOq).
    This chapter will serve as a tutorial to the ADAM project, which comprises a set
    of genomics-specific Avro schemas, PySpark-based APIs, and command-line tools
    for large-scale genomics analysis. Among other applications, ADAM provides a natively
    distributed implementation of the [Genome Analysis Toolkit (GATK)](https://oreil.ly/k2YZH)
    using PySpark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们展示了如何在PySpark生态系统中执行典型的基因组任务。具体来说，我们将使用PySpark和开源ADAM库来操作大量的基因组数据，并处理来自多个来源的数据，创建一个用于预测转录因子（TF）结合位点的数据集。为此，我们将从[ENCODE数据集](https://oreil.ly/h0yOq)中加入基因组注释。本章将作为ADAM项目的教程，该项目包括一组专用于基因组的Avro模式、基于PySpark的API以及用于大规模基因组分析的命令行工具。除了其他应用程序外，ADAM还提供了使用PySpark的[基因组分析工具包（GATK）](https://oreil.ly/k2YZH)的本地分布式实现。
- en: We’ll start by talking about the various data formats used in the bioinformatics
    domain, associated challenges, and how serialization formats can help. After that,
    we’ll install the ADAM project and explore its API using a sample dataset. We
    will then work with multiple genomics datasets to prepare a dataset that can be
    used for predicting binding sites in DNA sequences for a particular type of protein—CTCF
    transcription factor. The datasets will be obtained from the publicly available
    ENCODE dataset. Because the genome implies a 1D coordinate system, many genomics
    operations are spatial in nature. The ADAM project provides a genomics-targeted
    API for performing distributed spatial joins that we will use.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论生物信息学领域使用的各种数据格式，相关挑战以及序列化格式如何帮助。然后，我们将安装ADAM项目，并使用示例数据集探索其API。然后，我们将使用多个基因组数据集准备一个用于预测DNA序列中特定类型蛋白质（CTCF转录因子）结合位点的数据集。这些数据集将从公开可用的ENCODE数据集获取。由于基因组暗示了一个一维坐标系，许多基因组操作具有空间性质。ADAM项目提供了一个面向基因组的API，用于执行我们将使用的分布式空间连接。
- en: For those interested, a great introduction to biology is [Eric Lander’s EdX
    course](https://oreil.ly/WIky1). For an introduction to bioinformatics, see Arthur
    Lesk’s *Introduction to Bioinformatics* (Oxford University Press).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些感兴趣的人，生物学的一个很好的入门课程是[Eric Lander的EdX课程](https://oreil.ly/WIky1)。想要了解生物信息学的人，可以参考Arthur
    Lesk的《生物信息学导论》（牛津大学出版社）。
- en: Decoupling Storage from Modeling
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解耦存储和建模
- en: Bioinformaticians spend a disproportionate amount of time worrying about file
    formats—*.fasta*, *.fastq*, *.sam*, *.bam*, *.vcf*, *.gvcf*, *.bcf*, *.bed*, *.gff*,
    *.gtf*, *.narrowPeak*, *.wig*, *.bigWig*, *.bigBed*, *.ped*, and *.tped*, to name
    a few. Some scientists also feel it is necessary to specify their own custom format
    for their custom tool. On top of that, many of the format specifications are incomplete
    or ambiguous (which makes it hard to ensure implementations are consistent or
    compliant) and specify ASCII-encoded data. ASCII data is very common in bioinformatics,
    but it is inefficient and compresses relatively poorly. In addition, the data
    must always be parsed, necessitating additional compute cycles.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生物信息学家在处理文件格式上花费了大量时间，如*.fasta*, *.fastq*, *.sam*, *.bam*, *.vcf*, *.gvcf*,
    *.bcf*, *.bed*, *.gff*, *.gtf*, *.narrowPeak*, *.wig*, *.bigWig*, *.bigBed*, *.ped*,
    和 *.tped*等等。一些科学家还觉得有必要为他们的自定义工具指定自己的自定义格式。此外，许多格式规范都是不完整或不明确的（这使得确保实现的一致性或合规性变得困难），并且指定ASCII编码数据。ASCII数据在生物信息学中非常常见，但效率低且相对难以压缩。此外，数据必须始终被解析，需要额外的计算周期。
- en: 'This is particularly troubling because all of these file formats essentially
    store just a few common object types: an aligned sequence read, a called genotype,
    a sequence feature, and a phenotype. (The term *sequence feature* is slightly
    overloaded in genomics, but in this chapter we mean it in the sense of an element
    from a track of the UCSC Genome Browser.) Libraries like [`biopython`](http://biopython.org)
    are popular because they are chock-full of parsers (e.g., `Bio.SeqIO`) that attempt
    to read all the file formats into a small number of common in-memory models (e.g.,
    `Bio.Seq`, `Bio.SeqRecord`, `Bio.SeqFeature`).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这特别令人困扰，因为所有这些文件格式本质上都只存储几种常见的对象类型：对齐的序列读取，调用的基因型，序列特征和表型。（在基因组学中，“序列特征”这个术语略有重载，但在本章中，我们指的是UCSC基因组浏览器轨迹的元素意义上的内容。）像[`biopython`](http://biopython.org)这样的库非常受欢迎，因为它们充斥着解析器（例如`Bio.SeqIO`），试图将所有文件格式读入少数常见的内存模型中（例如`Bio.Seq`，`Bio.SeqRecord`，`Bio.SeqFeature`）。
- en: 'We can solve all of these problems in one shot using a serialization framework
    like Apache Avro. The key lies in Avro’s separation of the data model (i.e., an
    explicit schema) from the underlying storage file format and also the language’s
    in-memory representation. Avro specifies how data of a certain type should be
    communicated between processes, whether that’s between running processes over
    the internet, or a process trying to write the data into a particular file format.
    For example, a Java program that uses Avro can write the data into multiple underlying
    file formats that are all compatible with Avro’s data model. This allows each
    process to stop worrying about compatibility with multiple file formats: the process
    only needs to know how to read Avro, and the filesystem needs to know how to supply
    Avro.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过像Apache Avro这样的序列化框架一次性解决所有这些问题。关键在于Avro将数据模型（即显式模式）与底层存储文件格式以及语言的内存表示分离开来。Avro指定了如何在不同进程之间传递特定类型的数据，无论是在互联网上运行的进程之间，还是尝试将数据写入特定文件格式的进程中。例如，使用Avro的Java程序可以将数据写入与Avro数据模型兼容的多种底层文件格式中。这使得每个进程都不必再担心与多种文件格式的兼容性：进程只需要知道如何读取Avro，而文件系统则需要知道如何提供Avro。
- en: 'Let’s take the sequence feature as an example. We begin by specifying the desired
    schema for the object using the Avro interface definition language (IDL):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以序列特征为例。我们首先使用Avro接口定义语言（IDL）为对象指定所需的模式：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO1-1)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO1-1)'
- en: For example, “conservation,” “centipede,” “gene”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“保护”，“蜈蚣”，“基因”
- en: This data type could be used to encode, for example, conservation level, the
    presence of a promoter or ribosome binding site, a TF binding site, and so on
    at a particular location in the genome. One way to think about it is as a binary
    version of JSON, but more restricted and with higher performance. Given a particular
    data schema, the Avro spec then determines the precise binary encoding for the
    object so that it can be easily communicated between processes (even if written
    in different programming languages), over the network, or onto disk for storage.
    The Avro project includes modules for processing Avro-encoded data from many languages,
    including Java, C/C++, Python, and Perl; after that, the language is free to store
    the object in memory in whichever way is deemed most advantageous. The separation
    of data modeling from the storage format provides another level of flexibility/abstraction;
    Avro data can be stored as Avro-serialized binary objects (Avro container file),
    in a columnar file format for fast queries (Parquet file), or as text JSON data
    for maximum flexibility (minimum efficiency). Finally, Avro supports schema evolution,
    allowing the user to add new fields as they become necessary, while the software
    gracefully deals with new/old versions of the schema.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据类型可以用来编码基因组中特定位置的保守水平、启动子或核糖体结合位点的存在、转录因子结合位点等。可以将其视为 JSON 的二进制版本，但更为受限且性能更高。根据特定的数据模式，Avro
    规范确定对象的精确二进制编码，以便可以在不同编程语言中的进程之间（甚至是不同编程语言中的进程）、通过网络或存储到磁盘进行轻松通信。Avro 项目包括用于处理来自多种语言的
    Avro 编码数据的模块，包括 Java、C/C++、Python 和 Perl；然后，语言可以自由选择以最有利的方式将对象存储在内存中。数据建模与存储格式的分离提供了另一层灵活性/抽象；Avro
    数据可以存储为 Avro 序列化的二进制对象（Avro 容器文件）、为快速查询存储为列式文件格式（Parquet 文件）或以文本 JSON 数据的方式存储以获得最大灵活性（最小效率）。最后，Avro
    支持模式演化，允许用户在需要时添加新字段，而软件可以优雅地处理新旧版本的模式。
- en: Overall, Avro is an efficient binary encoding that allows you to specify evolvable
    data schemas, process the same data from many programming languages, and store
    the data using many formats. Deciding to store your data using Avro schemas frees
    you from perpetually working with more and more custom data formats, while simultaneously
    increasing the performance of your computations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Avro 是一种高效的二进制编码，允许您指定可演变的数据模式，从许多编程语言处理相同的数据，并使用多种格式存储数据。决定使用 Avro 模式存储数据使您摆脱了不断使用越来越多自定义数据格式的困扰，同时提高了计算性能。
- en: 'The particular `SequenceFeature` model used in the preceding example is a bit
    simplistic for real data, but the Big Data Genomics (BDG) project has already
    defined Avro schemas to represent the following objects, as well as many others:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面示例中使用的特定的 `SequenceFeature` 模型对于真实数据来说有些简单，但是大数据基因组学（BDG）项目已经定义了 Avro 模式来表示以下对象，以及许多其他对象：
- en: '`AlignmentRecord` for reads'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AlignmentRecord` 表示读取时的对齐记录。'
- en: '`Variant` for known genome variants and metadata'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Variant` 表示已知的基因组变异和元数据。'
- en: '`Genotype` for a called genotype at a particular locus'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Genotype` 表示特定位点上的基因型。'
- en: '`Feature` for a sequence feature (annotation on a genome segment)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Feature` 表示基因组片段上的序列特征（注释）。'
- en: The actual schemas can be found in [the `bdg-formats` GitHub repo](https://oreil.ly/gCf1f).
    The BDG formats can function as a replacement of the ubiquitous “legacy” formats
    (like BAM and VCF), but more commonly function as high-performance “intermediate”
    formats. (The original goal of these BDG formats was to replace the use of BAM
    and VCF, but their stubborn ubiquity has proved this goal to be difficult to attain.)
    Avro provides many performance and data modeling benefits over the custom ASCII
    status quo.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的模式可以在 [bdg-formats GitHub 仓库](https://oreil.ly/gCf1f) 中找到。BDG 格式可以替代广泛使用的“传统”格式（如
    BAM 和 VCF），但更常见的是作为高性能的“中间”格式。（这些 BDG 格式最初的目标是取代 BAM 和 VCF 的使用，但它们顽固的普遍性使得这个目标变得难以实现。）相对于自定义的
    ASCII 标准，Avro 提供了许多性能和数据建模的优势。
- en: In the remainder of the chapter, we’ll use some of the BDG schemas to accomplish
    some typical genomics tasks. Before we can do that, we will need to install the
    ADAM project. That’s what we’ll do in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用一些 BDG 模式来完成一些典型的基因组学任务。在此之前，我们需要安装 ADAM 项目。这将在下一节中进行。
- en: Setting Up ADAM
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 ADAM
- en: BDG’s core set of genomics tools is called ADAM. Starting from a set of mapped
    reads, this core includes tools that can perform mark-duplicates, base quality
    score recalibration, indel realignment, and variant calling, among other tasks.
    ADAM also contains a command-line interface that wraps the core for ease of use.
    In contrast to traditional HPC tools, ADAM can automatically parallelize across
    a cluster without having to split files or schedule jobs manually.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BDG 的核心基因组工具集称为 ADAM。从一组映射读取开始，此核心包括可以执行标记重复、基质质量分数校正、插入缺失实线和变异调用等任务的工具。ADAM
    还包含一个命令行界面，用于简化使用。与传统的 HPC 工具不同，ADAM 可以在集群中自动并行化，无需手动拆分文件或调度作业。
- en: 'We can start by installing ADAM using pip:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 pip 安装 ADAM 开始：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Alternative installation methods can be found on the [GitHub page](https://oreil.ly/4eFnX).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[GitHub 页面](https://oreil.ly/4eFnX)找到替代的安装方法。
- en: 'ADAM also comes with a submission script that facilitates interfacing with
    Spark’s `spark-submit` script:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM 还附带一个提交脚本，可以方便地与 Spark 的 `spark-submit` 脚本进行交互：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, you should be able to run ADAM from the command line and get
    the usage message. As noted in the usage message, Spark arguments are given before
    ADAM-specific arguments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该能够从命令行运行 ADAM 并获得使用消息。如使用消息中所述，Spark 参数在 ADAM 特定参数之前给出。
- en: With ADAM set up, we can start working with genomic data. We will explore ADAM’s
    API by working with a sample dataset next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好 ADAM 后，我们可以开始处理基因组数据。接下来，我们将使用一个样本数据集探索 ADAM 的 API。
- en: Introduction to Working with Genomics Data Using ADAM
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍使用 ADAM 处理基因组数据的工作方式
- en: 'We’ll start by taking a *.bam* file containing some mapped NGS reads, converting
    them to the corresponding BDG format (`AlignedRecord` in this case), and saving
    them to HDFS. First, we get our hands on a suitable *.bam* file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个包含一些映射的 *.bam* 文件开始，将其转换为相应的 BDG 格式（在这种情况下为 `AlignedRecord`），并将其保存到 HDFS
    中。首先，我们获取一个合适的 *.bam* 文件：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Move the downloaded file into a directory where we’ll store all data for this
    chapter:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将下载的文件移动到我们将存储本章所有数据的目录中：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next up, we’ll use the ADAM CLI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 ADAM CLI。
- en: File Format Conversion with the ADAM CLI
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ADAM CLI 进行文件格式转换
- en: 'We can then use the ADAM `transform` command to convert the *.bam* file to
    Parquet format (described in [“Parquet Format and Columnar Storage”](#parquet-format)).
    This would work both on a cluster and in `local` mode:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 ADAM 的 `transform` 命令将 *.bam* 文件转换为 Parquet 格式（在[“Parquet 格式和列式存储”](#parquet-format)中描述）。这在集群和
    `local` 模式下都可以工作：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-1)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-1)'
- en: Example Spark args for running on YARN
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在 YARN 上的示例 Spark 参数
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-2)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-2)'
- en: The ADAM subcommand itself
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM 子命令本身
- en: This should kick off a pretty large amount of output to the console, including
    the URL to track the progress of the job.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会启动大量输出到控制台，包括跟踪作业进度的 URL。
- en: The resulting dataset is the concatenation of all the files in the *data/genomics/reads/HG00103/*
    directory, where each *part-*.parquet* file is the output from one of the PySpark
    tasks. You’ll also notice that the data has been compressed more efficiently than
    the initial *.bam* file (which is gzipped underneath) thanks to the columnar storage
    (see [“Parquet Format and Columnar Storage”](#parquet-format)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据集是 *data/genomics/reads/HG00103/* 目录中所有文件的串联，每个 *part-*.parquet* 文件都是一个
    PySpark 任务的输出。您还会注意到，由于列式存储，数据比初始的 *.bam* 文件（其底层是经过 gzip 压缩的）更有效地压缩（请参见[“Parquet
    格式和列式存储”](#parquet-format)）。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s see what one of these objects looks like in an interactive session.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些对象在交互会话中是什么样子的。
- en: Ingesting Genomics Data Using PySpark and ADAM
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PySpark 和 ADAM 吸入基因组数据
- en: First, we start up the PySpark shell using the ADAM helper command. It loads
    all of the JARs that are necessary.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 ADAM 助手命令启动 PySpark shell。它会加载所有必要的 JAR 包。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In some cases, you can encounter a TypeError error with a mention of JavaPackage
    object not being when trying to use ADAM with PySpark. It is a known issue and
    is documented [here](https://oreil.ly/67uBd).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，当尝试在 PySpark 中使用 ADAM 时，可能会遇到 TypeError 错误，其中提到 JavaPackage 对象不存在。这是一个已知问题，详细记录在[这里](https://oreil.ly/67uBd)。
- en: 'In such a scenario, please try the solutions suggested in the thread. One could
    be running the following command to start PySpark shell with ADAM:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，请尝试在线程中建议的解决方案。例如，可以运行以下命令启动带有ADAM的PySpark shell：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we’ll load the aligned read data as an `AlignmentDataset`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将加载对齐的读取数据作为`AlignmentDataset`：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may get a different read because the partitioning of the data may be different
    on your system, so there is no guarantee which read will come back first.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据的分区可能在您的系统上不同，因此无法保证哪个读取会先返回，您可能会得到不同的读取结果。
- en: Now we can interactively ask questions about our dataset, all while executing
    the computations across a cluster in the background. How many reads do we have
    in this dataset?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以与数据集互动地提出问题，同时在后台跨集群执行计算。在这个数据集中有多少读取？
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Do the reads in this dataset derive from all human chromosomes?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集的读取是否源自所有人类染色体？
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Yep, we observe reads from chromosomes 1 through 22, X and Y, along with some
    other chromosomal chunks that are not part of the “main” chromosomes or whose
    locations are unknown. Let’s analyze the code a little more closely:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们观察到来自染色体1到22、X和Y的读取，还有一些不属于“主”染色体的其他染色体片段或位置不明的片段。让我们更仔细地分析一下代码：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-1)'
- en: '`AlignmentDataset`: an ADAM type that contains all our data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`AlignmentDataset`：一种包含所有数据的ADAM类型。'
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-2)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-2)'
- en: '`DataFrame`: the underlying Spark DataFrame.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`：底层Spark DataFrame。'
- en: '[![3](assets/3.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-3)'
- en: This will aggregate all the distinct contig names; it will be small.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将聚合所有不同的contig名称；这将很小。
- en: '[![4](assets/4.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-4)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-4)'
- en: This triggers the computation and brings the data in the DataFrame back to the
    client app (the shell).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发计算并将DataFrame中的数据带回客户端应用程序（即shell）。
- en: 'For a more clinical example, say we are testing an individual’s genome to check
    whether they carry any gene variants that put them at risk for having a child
    with cystic fibrosis (CF). Our genetic test uses next-generation DNA sequencing
    to generate reads from multiple relevant genes, such as the CFTR gene (whose mutations
    can cause CF). After running our data through our genotyping pipeline, we determine
    that the CFTR gene appears to have a premature stop codon that destroys its function.
    However, this mutation has never been reported before in the [Human Gene Mutation
    Database](https://oreil.ly/wULRR), nor is it in the [Sickkids CFTR database](https://oreil.ly/u1L0j),
    which aggregates CF gene variants. We want to go back to the raw sequencing data
    to see if the potentially deleterious genotype call is a false positive. To do
    so, we need to manually analyze all the reads that map to that variant locus,
    say, chromosome 7 at 117149189 (see [Figure 9-1](#IGV_HG00103_CFTR)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 举个临床例子，假设我们正在测试一个个体的基因组，以检查其是否携带任何会导致其子代患囊性纤维化（CF）风险基因变异。我们的基因检测使用下一代DNA测序从多个相关基因生成读取结果，例如CFTR基因（其突变可能导致CF）。在运行数据通过我们的基因分型管道后，我们确定CFTR基因似乎有一个早期终止密码子破坏其功能。然而，这种突变在[Human
    Gene Mutation Database](https://oreil.ly/wULRR)中从未报告过，也不在[Sickkids CFTR database](https://oreil.ly/u1L0j)中，该数据库汇总了CF基因变异。我们想要回到原始测序数据，查看潜在有害基因型调用是否是假阳性。为此，我们需要手动分析所有映射到该变异位点的读取，例如染色体7的117149189位点（见[Figure 9-1](#IGV_HG00103_CFTR)）：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is now possible to manually inspect these nine reads, or process them through
    a custom aligner, for example, and check whether the reported pathogenic variant
    is a false positive.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以手动检查这九个读取结果，或者通过自定义的对齐器处理它们，例如检查报告的致病变异是否是假阳性。
- en: '![aaps 0901](assets/aaps_0901.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0901](assets/aaps_0901.png)'
- en: Figure 9-1\. Integrative Genomic Viewer visualization of the HG00103 at chr7:117149189
    in the CFTR gene
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 在CFTR基因的chr7:117149189处的HG00103的整合基因组查看器可视化
- en: Say we’re running a clinical lab that is performing such carrier screening as
    a service to clinicians. Archiving the raw data using a cloud storage system such
    as AWS S3 ensures that the data stays relatively warm (compared with, say, tape
    archive). In addition to having a reliable system for actually performing the
    data processing, we can easily access all of the past data for quality control
    or for cases where there needs to be manual interventions, like the CFTR example
    presented earlier. In addition to the rapid access to the totality of the data,
    the centrality also makes it easy to perform large analytical studies, like population
    genetics, large-scale quality-control analyses, and so on.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在作为临床实验室运行，为临床医生提供这样的携带者筛查服务。使用AWS S3等云存储系统存档原始数据可确保数据保持相对温暖（与磁带存档相比）。除了确保实际进行数据处理的可靠系统外，我们还可以轻松访问所有过去的数据进行质量控制，或在需要手动干预的情况下（例如前面介绍的CFTR示例）进行操作。除了快速访问全部数据的能力外，中心化还使得进行大规模分析研究（如人口遗传学、大规模质量控制分析等）变得更加容易。
- en: Now that we are familiar with the ADAM API, let’s start work on creation of
    our transcription factor prediction dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了ADAM API，让我们开始创建我们的转录因子预测数据集。
- en: Predicting Transcription Factor Binding Sites from ENCODE Data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从ENCODE数据预测转录因子结合位点
- en: In this example, we will use publicly available sequence feature data to build
    a simple model for transcription factor binding. TFs are proteins that bind to
    specific DNA sequences in the genome and help control the expression of different
    genes. As a result, they are critical in determining the phenotype of a particular
    cell and are involved in many physiological and disease processes. ChIP-seq is
    an NGS-based assay that allows the genome-wide characterization of binding sites
    for a particular TF in a particular cell/tissue type. However, in addition to
    ChIP-seq’s cost and technical difficulty, it requires a separate experiment for
    each tissue/TF pair. In contrast, DNase-seq is an assay that finds regions of
    open chromatin genome-wide and needs to be performed only once per tissue type.
    Instead of assaying TF binding sites by performing a ChIP-seq experiment for each
    tissue/TF combination, we’d like to predict TF binding sites in a new tissue type
    assuming only the availability of DNase-seq data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用公开可用的序列特征数据来构建一个简单的转录因子结合模型。TFs是在基因组中结合到特定DNA序列的蛋白质，并帮助控制不同基因的表达。因此，它们对于确定特定细胞的表型至关重要，并参与许多生理和疾病过程。ChIP-seq是一种基于NGS的分析方法，允许在特定细胞/组织类型中全基因组特征化特定TF的结合位点。然而，除了ChIP-seq的成本和技术难度外，它还需要对每种组织/TF对进行单独的实验。相比之下，DNase-seq是一种寻找全基因组开放染色质区域的分析方法，每种组织类型只需执行一次。我们不想通过为每种组织/TF组合执行ChIP-seq实验来分析TF结合位点，而是希望仅凭借DNase-seq数据的可用性来预测新组织类型中的TF结合位点。
- en: In particular, we will predict the binding sites for the CTCF TF using DNase-seq
    data along with known sequence motif data (from [HT-SELEX](https://oreil.ly/t5OEkL))
    and other data from [the publicly available ENCODE dataset](https://oreil.ly/eFJ9n).
    We have chosen six different cell types that have available DNase-seq and CTCF
    ChIP-seq data for training. A training example will be a DNase hypersensitivity
    (HS) peak (a segment of the genome), and the binary label for whether the TF is
    bound/unbound will be derived from the ChIP-seq data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用DNase-seq数据以及来自[HT-SELEX](https://oreil.ly/t5OEkL)的已知序列模式数据和[公开可用的ENCODE数据集](https://oreil.ly/eFJ9n)中的其他数据，预测CTCF
    TF的结合位点。我们选择了六种不同的细胞类型，这些细胞类型具有用于训练的DNase-seq和CTCF ChIP-seq数据。训练示例将是一个DNase敏感性（HS）峰（基因组的一个片段），TF是否结合/未结合的二进制标签将根据ChIP-seq数据导出。
- en: 'To summarize the overall data flow: the main training/test examples will be
    derived from the DNase-seq data. Each region of open chromatin (an interval on
    the genome) will be used to generate a prediction of whether a particular TF in
    a particular tissue type will be bound there. To do so, we spatially join the
    ChIP-seq data to the DNase-seq data; every overlap is a positive label for the
    DNase seq objects. Finally, to improve the prediction accuracy, we generate an
    additional feature at each interval in the DNase-seq data—distance to a transcription
    start site (using the GENCODE dataset). The feature is added into the training
    examples by performing a spatial join (with a possible aggregation).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总结整体数据流程：主要的训练/测试示例将从DNase-seq数据中派生。每个开放染色质区域（基因组上的一个区间）将用于生成是否会在特定组织类型中结合特定TF的预测。为此，我们将ChIP-seq数据与DNase-seq数据进行空间连接；每个重叠部分都是DNase
    seq对象的正标签。最后，为了提高预测准确性，我们在DNase-seq数据的每个区间中生成一个额外的特征——到转录起始位点的距离（使用GENCODE数据集）。通过空间连接（可能进行聚合），将该特征添加到训练示例中。
- en: 'We will use data from the following cell lines:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下细胞系的数据：
- en: GM12878
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GM12878
- en: Commonly studied lymphoblastoid cell line
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 常见研究的淋巴母细胞系
- en: K562
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: K562
- en: Female chronic myelogenous leukemia
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 女性慢性髓系白血病
- en: BJ
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: BJ
- en: Skin fibroblast
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤成纤维细胞
- en: HEK293
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HEK293
- en: Embryonic kidney
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 胚胎肾
- en: H54
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: H54
- en: Glioblastoma
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 胶质母细胞瘤
- en: HepG2
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: HepG2
- en: Hepatocellular carcinoma
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 肝细胞癌
- en: 'First, we download the DNase data for each cell line in *.narrowPeak* format:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们下载每个细胞系的DNase数据，格式为*.narrowPeak*：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO4-1)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO4-1)'
- en: Streaming decompression
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 流式解压缩
- en: 'Next, we download the ChIP-seq data for the CTCF TF, also in *.narrowPeak*
    format, and the GENCODE data, in GTF format:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们下载CTCF TF的ChIP-seq数据，同样是*.narrowPeak*格式，并且GENCODE数据，格式为GTF：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note how we unzip the stream of data with `gunzip` on the way to depositing
    it in our filesystem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们如何在将数据流解压缩的过程中使用`gunzip`来将其存储到文件系统中。
- en: 'From all of this raw data, we want to generate a training set with a schema
    like the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有这些原始数据中，我们希望生成一个类似以下的训练集模式：
- en: Chromosome
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 染色体
- en: Start
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始
- en: End
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束
- en: Distance to closest transcription start site (TSS)
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到最近转录起始位点（TSS）的距离
- en: TF identity (always “CTCF” in this case)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF标识（在这种情况下始终为“CTCF”）
- en: Cell line
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞系
- en: TF binding status (boolean; the target variable)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF结合状态（布尔值；目标变量）
- en: 'This dataset can easily be converted into a DataFrame to carry into a machine
    learning library. Since we need to generate the data for multiple cell lines,
    we will define a DataFrame for each cell line individually and concatenate them
    at the end:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集可以轻松转换为DataFrame，以便导入机器学习库中。由于我们需要为多个细胞系生成数据，我们将为每个细胞系单独定义一个DataFrame，并在最后将它们连接起来：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We define a utility function and a broadcast variable that will be used to
    generate the features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个实用函数和一个广播变量，用于生成特征：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have loaded the data necessary for defining our training examples,
    we define the body of the “loop” for computing the data on each cell line. Note
    how we read the text representations of the ChIP-seq and DNase data, because the
    datasets are not so large that they will hurt performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经加载了定义训练示例所需的数据，我们为计算每个细胞系的数据定义“循环”的主体部分。请注意我们如何读取ChIP-seq和DNase数据的文本表示，因为数据集并不是很大，不会影响性能。
- en: 'To do so, we load the DNase and ChIP-seq data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们加载DNase和ChIP-seq数据：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-1)'
- en: '`FeatureDataset`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeatureDataset`'
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-2)'
- en: Columns in Dnase DataFrame
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Dnase DataFrame中的列
- en: Sites that overlap a ChIP-seq peak, as defined by a `ReferenceRegion` in `chipseq_data`,
    have TF binding sites and are therefore labeled `true`, while the rest of the
    sites are labeled `false`. This is accomplished using the 1D spatial join primitives
    provided in the ADAM API. The join functionality requires an RDD that is keyed
    by a `ReferenceRegion` and will produce tuples that have overlapping regions,
    according to usual join semantics (e.g., inner versus outer).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `chipseq_data` 中的 `ReferenceRegion` 定义的 ChIP-seq 峰重叠的位点具有 TF 结合位点，因此标记为 `true`，而其余的位点标记为
    `false`。这是通过 ADAM API 中提供的一维空间连接原语来实现的。连接功能需要一个按 `ReferenceRegion` 键入的 RDD，并将生成根据通常的连接语义（例如，内连接与外连接）具有重叠区域的元组。
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we compute the final set of features on each DNase peak:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在每个 DNase 峰上计算最终的特征集：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-1)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-1)'
- en: Left join with `tss_df` created earlier.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前创建的 `tss_df` 进行左连接。
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-2)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-2)'
- en: Get the closest TSS distance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最近的 TSS 距离。
- en: 'This final DF is computed in each pass of the loop over the cell lines. Finally,
    we union each DF from each cell line and cache this data in memory in preparation
    for training models off of it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次对细胞系循环的过程中计算出这个最终的 DF。最后，我们将来自每个细胞系的每个 DF 进行联合，并将这些数据缓存在内存中，以备用于训练模型：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: At this point, the data in `preTrainingData` can be normalized and converted
    into a DataFrame for training a classifier, as described in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
    Note that you should perform cross-validation, where in each fold, you hold out
    the data from one of the cell lines.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`preTrainingData` 中的数据可以被归一化并转换为一个 DataFrame，以用于训练分类器，如 [“随机森林”](ch04.xhtml#RandomDecisionForests)
    中所述。请注意，应执行交叉验证，在每一折中，将一个细胞系的数据保留下来。
- en: Where to Go from Here
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来的步骤
- en: Many computations in genomics fit nicely into the PySpark computational paradigm.
    When you’re performing ad hoc analysis, the most valuable contribution that projects
    like ADAM provide is the set of Avro schemas that represents the underlying analytical
    objects (along with the conversion tools). We saw how once data is converted into
    the corresponding Avro schemas, many large-scale computations become relatively
    easy to express and distribute.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基因组学中的计算非常适合于 PySpark 的计算范式。当您进行即席分析时，像 ADAM 这样的项目最有价值的贡献是代表底层分析对象的 Avro 模式集（以及转换工具）。我们看到，一旦数据转换为相应的
    Avro 模式，许多大规模计算变得相对容易表达和分布。
- en: While there may still be a relative dearth of tools for performing scientific
    research on PySpark, there do exist a few projects that could help avoid reinventing
    the wheel. We explored the core functionality implemented in ADAM, but the project
    already has implementations for the entire GATK best-practices pipeline, including
    indel realignment, and deduplication. In addition to ADAM, the Broad Institute
    is now developing major software projects using Spark, including the newest version
    of the [GATK4](https://oreil.ly/hGR87) and a project called [Hail](https://oreil.ly/V6Wpl)
    for large-scale population genetics computations. All of these tools are open
    source, so if you start using them in your own work, please consider contributing
    improvements!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 PySpark 上执行科学研究的工具可能仍然相对不足，但确实存在一些项目可以帮助避免重复发明轮子。我们探索了 ADAM 中实现的核心功能，但该项目已经为整个
    GATK 最佳实践流水线，包括插入缺失重排和去重，提供了实现。除了 ADAM 外，Broad Institute 现在也正在使用 Spark 开发重大软件项目，包括最新版本的
    [GATK4](https://oreil.ly/hGR87) 和一个名为 [Hail](https://oreil.ly/V6Wpl) 的用于大规模种群遗传学计算的项目。所有这些工具都是开源的，因此，如果您开始在自己的工作中使用它们，请考虑贡献改进！
