- en: 2 Introduction to the example problem and Pandas dataframes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 介绍示例问题和 Pandas 数据框
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Development environment options for deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习开发环境选项
- en: Introduction to Pandas dataframes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 数据框的介绍
- en: 'Introduction to the major example used in this book to illustrate deep learning
    on structured data: predicting streetcar delays'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍本书中用于说明结构化数据深度学习的主要示例：预测电车延误
- en: Format and scope of the example dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据集的格式和范围
- en: More details on common objections to using deep learning with structured data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于使用结构化数据进行深度学习的常见反对意见的更多细节
- en: A peek ahead at the process of training a deep learning model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型训练过程的初步了解
- en: In this chapter, you’ll learn about the options that you can pick for a deep
    learning environment and how to bring tabular structured data into your Python
    program. You will get an overview of Pandas, the Pythonic facility for manipulating
    tabular structured data. You will also learn about the major example that is used
    throughout this book to demonstrate deep learning for structured data, including
    details about the dataset used in the major example and the overall structure
    of the code for this example. Then you will get more details on the objections
    to deep learning with structured data that were introduced in chapter 1\. Finally,
    we will take a peek ahead and go through a round of training the deep learning
    model to whet your appetite for the rest of the extended example that is examined
    in chapters 3-8.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解你可以选择的深度学习环境选项以及如何将表格结构化数据引入你的 Python 程序。你将获得 Pandas 的概述，这是用于操作表格结构化数据的
    Python 工具。你还将了解本书中用于演示结构化数据深度学习的主要示例，包括主要示例中使用的详细数据集和该示例代码的整体结构。然后，你将获得关于第 1 章中引入的结构化数据深度学习反对意见的更多细节。最后，我们将提前一瞥，并通过一轮深度学习模型的训练来激发你对第
    3-8 章中检查的扩展示例的兴趣。
- en: 2.1 Development environment options for deep learning
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 深度学习开发环境选项
- en: Before you can start a deep learning project, you need to have access to an
    environment that provides the hardware and software stack that you need. This
    section describes the choices that you have for deep learning environments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始一个深度学习项目之前，你需要访问一个提供你所需要的硬件和软件堆栈的环境。本节描述了你在深度学习环境方面可以选择的选项。
- en: Before you look at environments specifically intended for deep learning, it
    is important to know that you can work through the extended code example in this
    book in a standard Windows or Linux environment. Using an environment with deep-learning-specific
    hardware will expedite the model training process, but doing so is not necessary.
    I ran the code examples in this book on my Windows 10 laptop with 8 GB of RAM
    and a single-core processor, as well as in the Paperspace Gradient environment
    (described in this section). Model training was about 30% faster in Gradient,
    but that meant a difference of only a minute or two in training time for each
    of the experiments described in chapter 7\. For bigger deep learning projects,
    I strongly recommend one of the deep-learning-enabled environments described in
    this section, but a reasonably provisioned modern laptop is sufficient for the
    extended example in this book. If you decide to try to run the code examples on
    your local system, ensure that your Python version is at least 3.7.4\. If you
    are doing a fresh install of Python or using a virtual Python environment, you
    will need to install Pandas, Jupyter, sci-kit learn, and TensorFlow 2.0\. As you
    work your way through the examples, you may need to install additional libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在你查看专门为深度学习设计的特定环境之前，重要的是要知道，你可以在标准的 Windows 或 Linux 环境中完成本书中的扩展代码示例。使用具有深度学习特定硬件的环境可以加快模型训练过程，但这并不是必需的。我在我的
    Windows 10 笔记本电脑上运行了本书中的代码示例，该电脑有 8 GB 的 RAM 和单核处理器，以及 Paperspace Gradient 环境（在本节中描述）。在
    Gradient 中模型训练快了大约 30%，但这意味着第 7 章中描述的每个实验的训练时间只相差一分钟左右。对于更大的深度学习项目，我强烈推荐本节中描述的深度学习启用环境之一，但配备合理的现代笔记本电脑就足以完成本书中的扩展示例。如果你决定在你的本地系统上尝试运行代码示例，请确保你的
    Python 版本至少为 3.7.4。如果你正在安装 Python 或使用虚拟 Python 环境，你需要安装 Pandas、Jupyter、sci-kit
    learn 和 TensorFlow 2.0。随着你通过示例的进展，你可能需要安装额外的库。
- en: 'IMPORTANT The majority of the code samples in the book can be run in the same
    Python environment. The one exception is the Facebook Messenger deployment described
    in chapter 8\. This deployment needs to be done in a Python environment with TensorFlow
    1.x, whereas the model training requires a Python environment with TensorFlow
    2.0 or later. To get around this contention on TensorFlow levels, you should take
    advantage of Python virtual environments to run the code examples. I suggest that
    you bring your base Python environment up to the latest TensorFlow 1.x level and
    use it for everything except for running the model training notebook. Create a
    virtual environment for the model training notebook, and in that environment,
    install TensorFlow 2.0\. Doing this will give you the benefits of TensorFlow 2.0
    for the model training step while maintaining backward compatibility and stability
    for the rest of your Python environment. You can find details on setting up a
    Python virtual environment in this article: [http://mng.bz/zrjr](http://mng.bz/zrjr).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：本书中的大多数代码示例都可以在相同的Python环境中运行。唯一的例外是第8章中描述的Facebook Messenger部署。这个部署需要在具有TensorFlow
    1.x的Python环境中完成，而模型训练则需要具有TensorFlow 2.0或更高版本的Python环境。为了解决TensorFlow版本之间的冲突，您应该利用Python虚拟环境来运行代码示例。我建议您将您的基Python环境升级到最新的TensorFlow
    1.x版本，并用于除运行模型训练笔记本之外的所有操作。为模型训练笔记本创建一个虚拟环境，并在该环境中安装TensorFlow 2.0。这样做将使您在模型训练步骤中享受到TensorFlow
    2.0的好处，同时保持其余Python环境的向后兼容性和稳定性。您可以在本文中找到设置Python虚拟环境的详细信息：[http://mng.bz/zrjr](http://mng.bz/zrjr)。
- en: 'Several cloud vendors provide complete deep learning environments for around
    the cost of a cup of coffee per hour. Each cloud environment has its strengths
    and weaknesses, with some (Azure and IBM Cloud) emphasizing ease of creating your
    first project and others (Amazon Web Services [AWS]) providing the benefits of
    scale and incumbency. Here are some cloud vendors that provide deep learning environments:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 几个云服务提供商提供的深度学习环境大约每小时只需一杯咖啡的费用。每个云环境都有其优势和劣势，其中一些（Azure和IBM Cloud）强调创建第一个项目的简便性，而其他一些（Amazon
    Web Services [AWS]）则提供规模和先发优势。以下是一些提供深度学习环境的云服务提供商：
- en: '*AWS* can be accessed here: [http://mng.bz/0Z4m](http://mng.bz/0Z4m). The SageMaker
    environment in AWS abstracts some of the complexity of managing machine learning
    models. AWS includes good tutorials on SageMaker, including one ([http://mng.bz/9A0a](http://mng.bz/9A0a))
    that takes you through the end-to-end process of training and deploying a model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS* 可以在这里访问：[http://mng.bz/0Z4m](http://mng.bz/0Z4m)。AWS中的SageMaker环境抽象了一些管理机器学习模型的复杂性。AWS包括关于SageMaker的良好教程，包括一个([http://mng.bz/9A0a](http://mng.bz/9A0a))，它引导您完成训练和部署模型的端到端过程。'
- en: '*Google Cloud* *(* [http://mng.bz/K524](http://mng.bz/K524) *)* also has easy-to-use
    tutorials, including one ([http://mng.bz/jVgy](http://mng.bz/jVgy)) that shows
    you how to deploy a deep learning model on the Google Cloud platform.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google Cloud* *(* [http://mng.bz/K524](http://mng.bz/K524) *)* 也提供了易于使用的教程，包括一个([http://mng.bz/jVgy](http://mng.bz/jVgy))，展示了如何在Google
    Cloud平台上部署深度学习模型。'
- en: '*Azure* *(* [http://mng.bz/oREN](http://mng.bz/oREN)) is the Microsoft cloud
    environment and includes several options for deep learning projects. The tutorial
    at [http://mng.bz/8Gp2](http://mng.bz/8Gp2) provides a simple introduction.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Azure* *(* [http://mng.bz/oREN](http://mng.bz/oREN)) 是微软的云环境，包括几个深度学习项目的选项。在[http://mng.bz/8Gp2](http://mng.bz/8Gp2)上的教程提供了一个简单的介绍。'
- en: '*Watson Studio Cloud* *(* [http://mng.bz/nz8v](http://mng.bz/nz8v)) provides
    an environment focused on machine learning that you can exploit without having
    to delve into all the details of IBM Cloud. The article at [http://mng.bz/Dz29](http://mng.bz/Dz29)
    provides a quick overview, along with links to companion overview articles for
    AWS SageMaker, Google Cloud, and Azure.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Watson Studio Cloud* *(* [http://mng.bz/nz8v](http://mng.bz/nz8v)) 提供了一个专注于机器学习的环境，您可以在不深入了解IBM
    Cloud的所有细节的情况下利用它。[http://mng.bz/Dz29](http://mng.bz/Dz29) 上的文章提供了一个快速概述，以及到AWS
    SageMaker、Google Cloud和Azure的配套概述文章的链接。'
- en: All these cloud environments provide what you need for a deep learning project,
    including Python, most of the required libraries, and access to deep learning
    acceleration hardware, including graphic processing units (GPUs) and tensor processing
    units (TPUs).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些云环境都提供了深度学习项目所需的一切，包括Python、大多数所需的库，以及访问深度学习加速硬件，包括图形处理单元（GPUs）和张量处理单元（TPUs）。
- en: The training of deep learning models depends on massive matrix manipulations,
    and these accelerators allow these operations to run faster. As noted previously,
    it’s possible to train simple deep learning models in an environment that doesn’t
    have GPUs or TPUs, but the training process will run more slowly. See section
    2.11 for details on which parts of the extended example in this book will benefit
    from being run in an environment with deep-learning-specific hardware.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的训练依赖于大规模矩阵操作，这些加速器使得这些操作可以更快地运行。正如之前所述，您可以在没有GPU或TPU的环境中训练简单的深度学习模型，但训练过程将会更慢。有关本书扩展示例中哪些部分将受益于在具有深度学习特定硬件的环境中运行的详细信息，请参阅第2.11节。
- en: 'In addition to providing the hardware and software needed for deep learning,
    the cloud environments listed in this section cater to a wide spectrum of users
    beyond people who are interested in developing deep learning models. Here are
    two cloud environments that are specialized for machine learning:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供深度学习所需的硬件和软件外，本节列出的云环境还满足了许多对开发深度学习模型感兴趣之外的用户需求。以下是两个专门针对机器学习的云环境：
- en: '*Google Colaboratory* *(Colab* ; [http://mng.bz/QxNm](http://mng.bz/QxNm) )
    is a free Jupyter Notebooks environment provided by Google.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google Colaboratory* *(Colab* ; [http://mng.bz/QxNm](http://mng.bz/QxNm) )
    是由Google提供的一个免费Jupyter Notebooks环境。'
- en: '*Paperspace* ([https://towardsdatascience.com/paperspace-bc56efaf6c1f](https://towardsdatascience.com/paperspace-bc56efaf6c1f))
    is a cloud environment laser focused on machine learning. You can use the Paperspace
    Gradient environment ([https://gradient.paperspace.com/](https://gradient.paperspace.com/))
    to create with one click a Jupyter Notebooks environment where you can start your
    deep learning project. Figure 2.1 shows a Gradient notebook in the Paperspace
    console.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Paperspace* ([https://towardsdatascience.com/paperspace-bc56efaf6c1f](https://towardsdatascience.com/paperspace-bc56efaf6c1f))
    是一个专注于机器学习的云环境。您可以使用Paperspace Gradient环境 ([https://gradient.paperspace.com/](https://gradient.paperspace.com/))
    通过一键创建一个Jupyter Notebooks环境，在那里您可以开始您的深度学习项目。图2.1显示了Paperspace控制台中的Gradient笔记本。'
- en: '![CH02_F01_Ryan](../Images/CH02_F01_Ryan.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F01_Ryan](../Images/CH02_F01_Ryan.png)'
- en: 'Figure 2.1 Paperspace Gradient: one-click deep learning cloud environment'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 Paperspace Gradient：一键式深度学习云环境
- en: You can use any of these cloud environments to exercise the code that accompanies
    this book. For the sake of simplicity and to maximize the time you spend learning
    about deep learning, if you are going to use a cloud environment instead of your
    local system, I recommend that you take advantage of Paperspace Gradient. You
    will get a reliable environment that provides exactly what you need without having
    to worry about any of the additional cloud artifacts that other cloud environments
    offer. Gradient requires a credit card to set up. You can expect to pay about
    $1 per hour for a basic Gradient environment. Depending on how quickly you work
    through the code and how diligent you are about shutting down your Gradient notebook
    when you aren’t using it, you can expect to pay a total $30-$50 to work through
    the code examples in this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这些云环境中的任何一个来练习本书附带的代码。为了简化流程并最大限度地利用您学习深度学习的时间，如果您打算使用云环境而不是本地系统，我建议您利用Paperspace
    Gradient。您将获得一个可靠的环境，它正好提供您所需的一切，无需担心其他云环境提供的任何附加云服务。Gradient需要信用卡来设置。您预计每小时大约需要支付1美元的基本Gradient环境费用。根据您处理代码的速度以及您在不用时关闭Gradient笔记本的勤奋程度，您可能需要支付总计30-50美元来完成本书中的代码示例。
- en: If cost is a key consideration and you don’t want to use your local system,
    Colab is a good alternative cloud environment. Your experience with Colab will
    not be as smooth as that with Paperspace Gradient, but you won’t have to worry
    about cost. See appendix A for more details on what you need to know to get set
    up with Colab, along with a description of the pros and cons of Colab compared
    with Paperspace Gradient.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成本是关键考虑因素，并且您不想使用本地系统，Colab是一个很好的替代云环境。您在使用Colab时的体验可能不会像在Paperspace Gradient中那么顺畅，但您不必担心成本。参见附录A，了解有关设置Colab所需了解的更多信息，以及与Paperspace
    Gradient相比Colab的优缺点描述。
- en: In addition to Colab and Paperspace Gradient, the mainline cloud providers (including
    AWS, Google Cloud Platform, and IBM Cloud) provide ML environments that you can
    use for deep learning development. All the providers offer some kind of limited
    free access to their ML environments. If you are already using one of these platforms,
    and you can tolerate paying after you have exhausted your free access, one of
    these mainline providers could be a good option for you.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Colab 和 Paperspace Gradient 之外，主流的云服务提供商（包括 AWS、Google Cloud Platform 和 IBM
    Cloud）都提供了可用于深度学习开发的 ML 环境。所有提供商都提供某种形式的有限免费访问其 ML 环境。如果你已经在使用这些平台之一，并且在你用完免费访问后可以接受付费，那么这些主流提供商中的一个可能是一个不错的选择。
- en: 2.2 Code for exploring Pandas
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 探索 Pandas 的代码
- en: When you have cloned the GitHub repo ([http://mng.bz/v95x](http://mng.bz/v95x))
    associated with this book, you’ll find the code related to exploring Pandas in
    the notebooks subdirectory. The next listing shows the files that contain the
    code described in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你克隆了与这本书相关的 GitHub 仓库 ([http://mng.bz/v95x](http://mng.bz/v95x))，你将在 notebooks
    子目录中找到与探索 Pandas 相关的代码。下一个列表显示了包含本章描述的代码的文件。
- en: Listing 2.1 Code in the repo related to Pandas basics
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 与 Pandas 基础相关的代码
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Directory for data files
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据文件目录
- en: ❷ SQL code to define a table with the same columns as the input dataset
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义与输入数据集相同列的表的 SQL 代码
- en: ❸ Notebook containing basic Pandas examples
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含基本 Pandas 示例的笔记本
- en: ❹ Notebook containing examples of how to do in Pandas operations that you commonly
    do in SQL
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 包含如何在 Pandas 中执行你通常在 SQL 中做的操作的示例的笔记本
- en: 2.3 Pandas dataframes in Python
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 Python 中的 Pandas 数据框
- en: If you are reading this book, you will be familiar with relational databases
    and the organization of data in tables with rows and columns. You can think of
    the Pandas library as Python’s native (Pythonic) approach to representing and
    manipulating tabular structured data. The key structure in Pandas is the *dataframe*
    . You can think of a dataframe as being a Python approach to a relational table.
    Like a table, a dataframe
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本书，你将熟悉关系数据库以及数据在行和列组成的表中的组织。你可以将 Pandas 库视为 Python 的本地（Pythonic）方法来表示和操作表格结构化数据。Pandas
    中的关键结构是 *dataframe* 。你可以将 dataframe 视为 Python 对关系表的实现。像表一样，dataframe
- en: Has rows and columns (and columns can have different data types).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有行和列（并且列可以有不同的数据类型）。
- en: Can have indexes.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以有索引。
- en: Can be joined with other dataframes based on the values in certain columns.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以根据某些列的值与其他数据框连接。
- en: Before I go into more detail about Pandas dataframes, suppose that you want
    to manipulate a simple tabular dataset, the Iris dataset ([https://gist.github.com/curran/
    a08a1080b88344b0c8a7](https://gist.github.com/curran/a08a1080b88344b0c8a7)), in
    Python by
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我详细介绍 Pandas 数据框之前，假设你想通过 Python 操作一个简单的表格数据集，即 Iris 数据集 ([https://gist.github.com/curran/a08a1080b88344b0c8a7](https://gist.github.com/curran/a08a1080b88344b0c8a7))。
- en: Loading the CSV file into a Python structure
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 CSV 文件加载到 Python 结构中
- en: Counting the number of rows in the dataset
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算数据集中的行数
- en: Counting how many rows in the dataset have setosa as the species
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算数据集中有多少行物种为 setosa
- en: Figure 2.2 shows what a subset of the Iris dataset looks like as a CSV file.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 显示了 Iris 数据集的子集作为 CSV 文件的样子。
- en: '![CH02_F02_Ryan](../Images/CH02_F02_Ryan.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F02_Ryan](../Images/CH02_F02_Ryan.png)'
- en: Figure 2.2 Iris dataset as a CSV file
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 Iris 数据集作为 CSV 文件
- en: To do this, you need to create a Pandas dataframe containing the contents of
    the dataset. You can find the code in the next listing in the [chapter2.ipynb](https://github.com/ryanmark1867/deep_learning_for_structured_data/blob/master/notebooks/chapter2.ipynb)
    notebook.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，你需要创建一个包含数据集内容的 Pandas 数据框。你可以在下一个列表中找到 [chapter2.ipynb](https://github.com/ryanmark1867/deep_learning_for_structured_data/blob/master/notebooks/chapter2.ipynb)
    笔记本中的代码。
- en: Listing 2.2 Creating a Pandas dataframe from CSV referenced by URL
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 从 URL 引用的 CSV 创建 Pandas 数据框
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Import the pandas library.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 pandas 库。
- en: ❷ Raw GitHub URL for Iris dataset
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Iris 数据集的原始 GitHub URL
- en: ❸ Read the contents of the URL into a Pandas dataframe.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将 URL 的内容读取到 Pandas 数据框中。
- en: ❹ Display the first few rows from the new dataframe.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示新数据框的前几行。
- en: Run this cell in the chapter2 notebook, and note the output. The `head` `()`
    call lists the first few (by default, five) rows from the dataframe in an easy-to-read
    format, as shown in figure 2.3.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 chapter2 笔记本中运行此单元格，并注意输出。`head` `()` 调用以易于阅读的格式列出数据框的前几行（默认为五行），如图 2.3 所示。
- en: '![CH02_F03_Ryan](../Images/CH02_F03_Ryan.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_Ryan](../Images/CH02_F03_Ryan.png)'
- en: Figure 2.3 Output of head() for the dataframe loaded with the Iris dataset
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 使用 Iris 数据集加载数据框的 head() 输出
- en: 'Compare with the first few rows of the raw dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始数据集的前几行进行比较：
- en: '![CH02_F03_RyanB](../Images/CH02_F03_RyanB.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_RyanB](../Images/CH02_F03_RyanB.png)'
- en: The raw CSV file and Pandas dataframe have the same column names and the same
    values, but what’s up with the first column in the dataframe? By default, every
    row in a Pandas dataframe is named, and this name (contained in the first column
    of the dataframe by default) is a sequential number, starting at zero for the
    first row. You can also think of this first column as being the default index
    for the dataframe.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 CSV 文件和 Pandas 数据框具有相同的列名和相同的值，但数据框中的第一列是怎么回事呢？默认情况下，Pandas 数据框中的每一行都有一个名称，这个名称（默认情况下包含在数据框的第一列中）是一个从零开始的顺序号。你也可以将这个第一列视为数据框的默认索引。
- en: 'Now you want to get the number of rows in the dataframe. The following statement
    returns the total number of rows in the dataframe:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你想要获取数据框中的行数。以下语句返回数据框中的总行数：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, you want to count the number of rows in the dataframe with setosa
    as the species. In the following statement, `iris_dataframe[iris_dataframe["species"]`
    `== ''setosa'']` defines a dataframe containing only the rows from the original
    dataframe in which `species` `=` `"setosa"`. Using the `shape` attribute the same
    way that you did to get the row count for the original dataframe, you can use
    this Python statement to get the number of rows in the dataframe with `species`
    `=` `"setosa"` :'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你想要计算数据框中物种为 setosa 的行数。在以下语句中，`iris_dataframe[iris_dataframe["species"]
    == 'setosa']` 定义了一个只包含原始数据框中 `species` 等于 `"setosa"` 的行的数据框。使用与获取原始数据框行数相同的方式使用
    `shape` 属性，你可以使用这个 Python 语句来获取 `species` 等于 `"setosa"` 的数据框中的行数：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will explore many more features of Pandas dataframes as we go through the
    main example in this book. Also, section 2.5 contains examples of using Pandas
    to perform common SQL operations. For now, this exploration has taught you how
    to bring tabular structured data into a Python program where you can prepare it
    to train a deep learning model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们通过本书中的主要示例进行学习，我们将探索 Pandas 数据框的更多功能。此外，2.5 节包含使用 Pandas 执行常见 SQL 操作的示例。到目前为止，这次探索已经教会了你如何将表格结构化数据导入
    Python 程序，在那里你可以准备它以训练深度学习模型。
- en: 2.4 Ingesting CSV files into Pandas dataframes
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 将 CSV 文件导入 Pandas 数据框
- en: In section 2.3, you saw how to ingest a CSV file identified as a URL into a
    Pandas dataframe, but suppose that you have your own private copy of a dataset
    that you have modified. You want to load data from that modified file in your
    filesystem into a dataframe. In this section, you’ll look at how to read a CSV
    file from the filesystem into a dataframe. In chapter 3, you’ll learn how to ingest
    an XLS file with multiple tabs into a single dataframe.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2.3 节中，你看到了如何将标识为 URL 的 CSV 文件导入 Pandas 数据框，但假设你有一个自己修改过的数据集的私有副本。你想要将这个修改后的文件中的数据加载到数据框中。在本节中，你将了解如何从文件系统中读取
    CSV 文件到数据框中。在第 3 章中，你将学习如何将具有多个标签的 XLS 文件导入单个数据框。
- en: Suppose that you want to load the Iris dataset into a dataframe, but you have
    made modifications to your own copy of the dataset (called iriscaps.csv) on your
    local filesystem, capitalizing the species names to meet the style guidelines
    for the application that uses this dataset. You need to load this modified dataset
    from a file on your filesystem instead of from the original Iris dataset. The
    code to bring a CSV file from the filesystem into a Pandas dataframe (shown in
    the following listing) resembles the code you have already seen for loading a
    dataframe from a URL.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想将 Iris 数据集加载到一个数据框中，但你已经在本地文件系统中修改了你自己的数据集副本（称为 iriscaps.csv），将物种名称大写以符合使用此数据集的应用程序的样式指南。你需要从文件系统中加载这个修改后的数据集，而不是从原始的
    Iris 数据集中加载。将文件系统中的 CSV 文件导入 Pandas 数据框的代码（如下所示）与您已经看到的从 URL 加载数据框的代码相似。
- en: Listing 2.3 Creating a Pandas dataframe from CSV referenced by filename
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 从 CSV 文件名创建 Pandas 数据框
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Import the pandas library.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 pandas 库。
- en: ❷ Define the filename.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义文件名。
- en: ❸ Read the contents of the file into a Pandas dataframe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将文件内容读入 Pandas 数据框。
- en: ❹ Display the first few rows from the new dataframe.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示新数据框的前几行。
- en: How do you get the correct value for path, the directory where the data files
    reside? All the code samples in this book assume that all the data exists in a
    directory called data that is a sibling of the directory containing the notebook.
    In this repo, the top-level directories’ notebooks and data contain the code files
    and data files, respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获取正确的路径值，即数据文件所在的目录？本书中的所有代码示例都假设所有数据都存在于一个名为 data 的目录中，该目录是笔记本所在目录的兄弟目录。在这个存储库中，顶级目录
    notebooks 和 data 分别包含代码文件和数据文件。
- en: Listing 2.4 is the code that gets the directory containing the notebook (rawpath).
    Then the code uses that directory to get the directory containing the data files
    by going first to the parent directory of the directory containing the notebook,
    and then to the data directory in the parent directory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 是获取笔记本（rawpath）所在目录的代码。然后，代码使用该目录通过先访问笔记本所在目录的父目录，然后访问父目录中的数据目录来获取数据文件所在的目录。
- en: Listing 2.4 Getting the path for the data directory
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 获取数据目录的路径
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Get the directory that this notebook is in.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取这个笔记本所在的目录。
- en: ❷ Get the directory that the data files are in.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取数据文件所在的目录。
- en: Note that you use the same `read_csv` function, but the argument is the filesystem
    path of the file as opposed to a URL. Figure 2.4 shows the modified dataset with
    the species name capitalized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你使用的是相同的 `read_csv` 函数，但参数是文件的文件系统路径，而不是 URL。图 2.4 显示了修改后的数据集，其中物种名称已大写。
- en: '![CH02_F04_Ryan](../Images/CH02_F04_Ryan.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F04_Ryan](../Images/CH02_F04_Ryan.png)'
- en: Figure 2.4 Dataframe loaded from a file in which the species values have been
    capitalized
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 从文件加载的 DataFrame，其中物种值已大写
- en: In this section, you’ve reviewed how to load a Pandas dataframe with the contents
    of a CSV file in the filesystem. In chapter 3, you’ll see how to load a dataframe
    with the contents of an XLS file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已经回顾了如何使用文件系统中的 CSV 文件内容加载 Pandas DataFrame。在第 3 章中，你将看到如何加载包含 XLS 文件内容的
    DataFrame。
- en: 2.5 Using Pandas to do what you would do with SQL
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 使用 Pandas 实现与 SQL 相同的操作
- en: Section 2.3 introduced the Pandas library as the Python solution for manipulating
    structured, tabular data. In this section, we will dig deeper into this part of
    the stack to show some examples of how you can use Pandas to accomplish tabular
    operations that you are used to doing with SQL. This section is not an exhaustive
    SQL-to-Pandas dictionary; see the articles at [http://mng.bz/lXGM](http://mng.bz/lXGM)
    and http://sergilehkyi.com/ translating-sql-to-pandas for more SQL-to-Pandas examples.
    Following are some examples that illustrate how to use Pandas to produce the same
    results as SQL.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3 节介绍了 Pandas 库作为 Python 解决方案，用于操作结构化、表格数据。在本节中，我们将深入探讨这个堆栈的部分，展示一些如何使用 Pandas
    完成你习惯用 SQL 完成的表格操作的示例。本节不是 SQL 到 Pandas 的完整字典；请参阅 [http://mng.bz/lXGM](http://mng.bz/lXGM)
    和 http://sergilehkyi.com/ translating-sql-to-pandas 以获取更多 SQL 到 Pandas 的示例。以下是一些说明如何使用
    Pandas 产生与 SQL 相同结果的示例。
- en: To exercise the following examples
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习以下示例
- en: Create a table called streetcarjan2014 in a relational database (the example
    assumes Postgres) by loading a CSV file with the contents of the first tab of
    the 2014 XLS file in the streetcar delay dataset. Ensure that the type of `"Min
    Delay"` is numeric.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过加载 2014 年街车延误数据集中第一个标签页的 CSV 文件内容，在关系型数据库中创建一个名为 streetcarjan2014 的表（示例假设使用
    Postgres）。确保 `"Min Delay"` 的类型是数值型。
- en: Use the chapter5.ipynb notebook to create a Pandas dataframe from the same CSV
    file. This notebook assumes that the CSV file is in a directory called data that
    is a sibling of the directory containing the notebook.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 chapter5.ipynb 笔记本从相同的 CSV 文件创建一个 Pandas DataFrame。此笔记本假设 CSV 文件位于一个名为 data
    的目录中，该目录是笔记本所在目录的兄弟目录。
- en: Now let’s look at some pairs of equivalent SQL and Pandas statements. First,
    to get the first three rows of the table
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一些等价的 SQL 和 Pandas 语句。首先，获取表的头三行
- en: '*SQL* —`select * from streetcarjan2014 limit 3`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL* —`select * from streetcarjan2014 limit 3`'
- en: '*Pandas* —`streetcarjan2014.head(3)`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* —`streetcarjan2014.head(3)`'
- en: Figure 2.5 shows the SQL query and results, and figure 2.6 shows the same for
    Pandas.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 显示了 SQL 查询和结果，图 2.6 显示了 Pandas 的相同内容。
- en: To have a single condition on a `select` statement
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `select` 语句上有一个单一的条件
- en: '*SQL* —`select` `"Route"` `from` `streetcarjan2014` `where` `"Location` `"`
    `=` `''King and` `Shaw''`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL* —`select` `"Route"` `from` `streetcarjan2014` `where` `"Location` `"`
    `=` `''King and` `Shaw''`'
- en: '*Pandas* —`streetcarjan2014[streetcarjan2014.Location` `==` `"King` `and` `Shaw"]`
    `.Route`'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* —`streetcarjan2014[streetcarjan2014.Location == "King and Shaw"]`.Route'
- en: '![CH02_F05_Ryan](../Images/CH02_F05_Ryan.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F05_Ryan](../Images/CH02_F05_Ryan.png)'
- en: Figure 2.5 SQL first three records
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 SQL前三条记录
- en: '![CH02_F06_Ryan](../Images/CH02_F06_Ryan.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F06_Ryan](../Images/CH02_F06_Ryan.png)'
- en: Figure 2.6 Pandas first three records
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 Pandas前三条记录
- en: To list the unique entries in a column
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列出列中的唯一条目
- en: '*SQL* —select `distinct "Incident" from streetcarjan2014`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL* —`select distinct "Incident" from streetcarjan2014`'
- en: '*Pandas* —`streetcarjan2014.Incident.unique()`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* —`streetcarjan2014.Incident.unique()`'
- en: To have multiple conditions on a `select` statement
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在`select`语句中设置多个条件
- en: '*SQL* —`select` `*` `from` `streetcarjan2014` `where` `"Min Delay"` `>` `20`
    `and` `"Day"` `= ''Sunday''`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL* —`select * from streetcarjan2014 where "Min Delay" > 20 and "Day" = ''Sunday''`'
- en: '*Pandas* —`streetcarjan2014[(streetcarjan2014[''Min` `Delay'']` `>` `20)` `&`
    `(streetcarjan2014[''Day'']` `==` `"Sunday")]`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* —`streetcarjan2014[(streetcarjan2014[''Min Delay''] > 20) & (streetcarjan2014[''Day'']
    == "Sunday")]`'
- en: Figure 2.7 shows the SQL query and results, and figure 2.8 shows the same for
    Pandas.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7显示了SQL查询和结果，图2.8显示了Pandas的相同内容。
- en: '![CH02_F07_Ryan](../Images/CH02_F07_Ryan.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F07_Ryan](../Images/CH02_F07_Ryan.png)'
- en: Figure 2.7 SQL select with multiple conditions
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 多条件SQL查询
- en: '![CH02_F08_Ryan](../Images/CH02_F08_Ryan.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F08_Ryan](../Images/CH02_F08_Ryan.png)'
- en: Figure 2.8 Pandas select with multiple conditions
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 多条件Pandas查询
- en: To have `order by` on a `select` statement
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在`select`语句中设置`order by`
- en: '*SQL* —`select` `"Route",` `"Min` `Delay"` `from` `streetcarjan2014` `where`
    `"Min` `Delay"` `>` `20` `order` `by` `"Min` `Delay"`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL* —`select "Route", "Min Delay" from streetcarjan2014 where "Min Delay"
    > 20 order by "Min Delay"`'
- en: '*Pandas* —`streetcarjan2014[[''Route'',''Min` `Delay'']][(streetcarjan2014[''Min
    Delay'']` `>` `20)].sort_values(''Min Delay'')`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* —`streetcarjan2014[[''Route'',''Min Delay'']][(streetcarjan2014[''Min
    Delay''] > 20)].sort_values(''Min Delay'')`'
- en: In this section, we have gone through a few examples of how to use Pandas to
    do common SQL operations. As you continue to use Pandas, you will find many other
    ways that Pandas can make it easy for you to use your SQL experience in the world
    of Python.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些如何使用Pandas进行常见SQL操作的例子。随着你继续使用Pandas，你会发现许多其他方式，Pandas可以使你在Python的世界中使用SQL经验变得更加容易。
- en: '2.6 The major example: Predicting streetcar delays'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 主要示例：预测电车延误
- en: 'Now that you have had a taste of how tabular structured data is brought into
    a Python program, let’s examine the major example used throughout this book: predicting
    streetcar delays.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经体验了如何将表格结构化数据引入Python程序，让我们来检查本书中使用的重大示例：预测电车延误。
- en: To have a successful deep learning project, you need data and a well-defined
    problem to solve. In this book, I am using a publicly available dataset ([http://mng.bz/4B2B](http://mng.bz/4B2B))
    published by the city of Toronto that describes every delay encountered in the
    city’s streetcar system since January 2014\. The problem to solve is how to predict
    delays in the Toronto streetcar system so that they can be prevented. In this
    chapter, you will learn about the format of this dataset. In subsequent chapters,
    you will learn how to correct the issues in the dataset that need to be fixed
    before it can be used to train a deep learning model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功进行深度学习项目，你需要数据和一个明确要解决的问题。在本书中，我使用了一个由多伦多市政府发布的公开数据集([http://mng.bz/4B2B](http://mng.bz/4B2B))，该数据集描述了自2014年1月以来城市电车系统中遇到的每一个延误。要解决的问题是如何预测多伦多电车系统的延误，以便可以预防。在本章中，你将了解这个数据集的格式。在随后的章节中，你将学习如何纠正数据集中需要修复的问题，以便可以使用它来训练深度学习模型。
- en: Why does the problem of streetcar delays in Toronto matter? Before World War
    II, many cities in North America had streetcar systems. These systems, called
    *trams* in some parts of the world, consist of light rail vehicles, usually running
    individually, powered by electricity drawn from overhead cables or sometimes from
    rails in the street, and running on rails in common space with other road traffic.
    Although some of Toronto’s streetcar network is on a dedicated right-of-way, the
    majority of the system runs on public streets mixed with other traffic.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么多伦多电车延误问题很重要？在第二次世界大战之前，北美许多城市都有电车系统。这些系统在一些地区被称为*有轨电车*，由轻轨车辆组成，通常单独运行，由从架空电缆或有时从街道上的轨道获取的电力驱动，并在与其他道路交通共享的空间上行驶。尽管多伦多的一些电车网络在专用车道上，但大多数系统都在公共街道上与其他交通混合运行。
- en: In the postwar period, most North American cities replaced their streetcars
    with buses. Some cities kept a token streetcar service as a tourist attraction.
    Toronto, however, was unique among North American cities because it maintained
    its extensive streetcar network as a critical part of its overall public transit
    system. Today, streetcars service four of the five busiest surface routes in Toronto
    and carry up to 300,000 passengers every weekday.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在战后时期，大多数北美城市用公交车取代了电车。一些城市保留了一些象征性的电车服务作为旅游景点。然而，多伦多在北美城市中是独一无二的，因为它将其广泛的电车网络作为其整体公共交通系统的一个关键部分保留下来。如今，电车服务多伦多五个最繁忙的地面路线中的四个，并且每个工作日可搭载多达30万名乘客。
- en: The streetcar network has many advantages over buses and subways, the other
    modes of transit that make up Toronto’s public transit system. Compared with buses,
    streetcars last longer, produce no emissions, carry at least twice as many passengers
    per driver, are cheaper to build and maintain, and provide more flexible service.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 电车网络相对于公交车和地铁等其他构成多伦多公共交通系统的交通方式有许多优势。与公交车相比，电车使用寿命更长，不排放污染物，每名驾驶员可搭载的乘客至少是公交车的一倍，建设和维护成本更低，并提供更灵活的服务。
- en: 'Streetcars have two big disadvantages: they are vulnerable to obstructions
    in general traffic, and they cannot easily get around these obstructions. When
    a streetcar gets blocked, it can cause compounded delays in the streetcar network
    and contribute to overall gridlock on the city’s busiest streets.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 电车有两个主要缺点：它们容易受到一般交通中的障碍物的影响，并且难以绕过这些障碍物。当电车被阻挡时，它会导致电车网络中的累积延误，并加剧城市最繁忙街道的整体交通拥堵。
- en: Using the streetcar delay dataset provided by the city of Toronto, we will apply
    deep learning to predict and prevent streetcar delays. Figure 2.9 shows a heat
    map of streetcar delays superimposed on a map of Toronto. You can find the code
    to generate this map in the streetcar_data-geocode-get-boundaries notebook. The
    areas with the most streetcar delays (the darker blobs on the map) are the busiest
    streets in the core of the city.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多伦多市提供的电车延误数据集，我们将应用深度学习来预测和预防电车延误。图2.9显示了叠加在多伦多地图上的电车延误热图。您可以在streetcar_data-geocode-get-boundaries笔记本中找到生成此地图的代码。地图上电车延误最严重的区域（较暗的块状区域）是城市核心最繁忙的街道。
- en: '![CH02_F09_Ryan](../Images/CH02_F09_Ryan.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F09_Ryan](../Images/CH02_F09_Ryan.png)'
- en: 'Figure 2.9 The Toronto streetcar network: heat map of delays'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 多伦多电车网络：延误热图
- en: 'Before I go into detail about the dataset for this problem, it makes sense
    to explain why I chose this particular problem. Why not chose a standard business
    problem like customer churn (predicting whether a customer is going to cancel
    a service) or inventory control (predicting when a retail outlet is going to run
    out of stock of a particular item)? Why choose a problem that is specific to a
    particular activity (public transit) and a particular place (Toronto)? Here are
    some of the reasons for selecting this problem:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我详细说明这个问题的数据集之前，解释一下为什么选择这个特定问题是有意义的。为什么不选择一个标准的商业问题，比如客户流失（预测客户是否会取消服务）或库存控制（预测零售店何时会耗尽某种商品的库存）？为什么选择一个特定活动（公共交通）和特定地点（多伦多）的问题？以下是选择这个问题的几个原因：
- en: It has a “Goldilocks” dataset (not too big and not too small). A really huge
    dataset presents additional data management problems that are not central to learning
    about deep learning. A big dataset can also mask inadequacies in the code and
    algorithm. The adage “He who has the most data wins” may apply to deep learning,
    but when you’re learning, there is something to be said for not having masses
    of data as a crutch. On the other hand, if you have too little data, deep learning
    simply doesn’t have enough signal to detect. The streetcar dataset (currently,
    more than 70,000 rows) is big enough to be applicable to deep learning, but not
    so big that it makes exploration difficult.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个“金发姑娘”数据集（既不太大也不太小）。一个非常大的数据集会带来额外的数据管理问题，这些问题与学习深度学习并不相关。大数据集也可能掩盖代码和算法中的不足。俗语“数据越多，胜算越大”可能适用于深度学习，但在学习过程中，没有大量数据作为支撑也有其道理。另一方面，如果数据太少，深度学习就没有足够的信号来检测。电车数据集（目前，超过70,000行）足够大，可以应用于深度学习，但又不至于太大，使得探索变得困难。
- en: The dataset is live. It gets updated every couple of months, so there is ample
    opportunity to test a model with data that it has never seen before.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据集是实时更新的。每隔几个月就会更新一次，因此有足够的机会使用它从未见过的数据进行模型测试。
- en: The dataset is real and raw. This dataset was collected over several years for
    several purposes, none of which was to train a deep learning model. As you will
    see in the next two chapters, this dataset has many errors and anomalies that
    need to be dealt with before the dataset can train a deep learning model. In practice,
    you will see similarly messy datasets across many business applications. By going
    through the process of cleaning up the real-world streetcar dataset, warts and
    all, you will be prepared to tackle other real-world datasets.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据集是真实且原始的。这个数据集是为了几个目的在几年内收集的，其中没有一个目的是训练深度学习模型。正如你将在下一章中看到的，这个数据集有许多错误和异常，需要在数据集用于训练深度学习模型之前进行处理。在实践中，你将在许多商业应用中看到类似的混乱数据集。通过清理现实世界的电车数据集，包括所有瑕疵，你将准备好应对其他现实世界的数据集。
- en: Businesses have to deal with competitive and regulatory pressures that make
    it impossible for them to share their datasets openly, which makes it difficult
    to find real, nontrivial datasets from serious businesses. Public agencies, by
    contrast, often have a legal obligation to publish their datasets. I’ve taken
    advantage of Toronto’s openness about the streetcar delay dataset to build the
    primary example for this book.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业必须应对竞争和监管压力，这使得它们无法公开分享他们的数据集，这使得从严肃的商业中找到真实、非平凡的数据集变得困难。相比之下，公共机构通常有法律义务发布他们的数据集。我利用多伦多对电车延误数据集的开放性，为本书构建了主要示例。
- en: The problem is accessible to a broad audience and is not tied to any particular
    industry or discipline.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该问题对广泛的受众都是可访问的，并且与任何特定行业或学科无关。
- en: Although the streetcar problem has the advantage of not being specific to any
    business, it can be directly related to common business problems to which deep
    learning is commonly applied, including
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管电车问题具有不特定于任何商业的优势，但它可以直接关联到深度学习通常应用的常见商业问题，包括
- en: '*Customer support* —Each row in the dataset is comparable to a ticket in a
    customer support system. The Incident column is similar to the freeform text abstracts
    that appear in ticketing systems, whereas the Min Delay and Min Gap columns play
    a role similar to the problem-severity information that is commonly recorded in
    ticketing systems. The Report Date, Time, and Day columns map to the timestamp
    information in customer support ticketing systems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客户支持* —数据集中的每一行都相当于客户支持系统中的一个工单。事件列类似于工单系统中出现的自由文本摘要，而最小延迟和最小间隔列则扮演着类似于在工单系统中通常记录的问题严重性信息的作用。报告日期、时间和日期列映射到客户支持工单系统中的时间戳信息。'
- en: '*Logistics* —Like logistics systems, the streetcar network has a spatial nature
    (implicit in the Route, Location, and Direction columns in the dataset) and a
    temporal nature (implicit in the Report Date, Time, and Day columns).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*物流* —与物流系统类似，电车网络具有空间性质（在数据集中的路线、位置和方向列中隐含）和时间性质（在报告日期、时间和日期列中隐含）。'
- en: 2.7 Why is a real-world dataset critical for learning about deep learning?
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 为什么现实世界的数据集对于学习深度学习至关重要？
- en: 'When you are learning about deep learning, why is it so important to work with
    a dataset that is real and messy? When you think about applying deep learning
    to a real-world dataset, you can compare it to being asked to create a Blu-ray
    disc from a box full of media collected over the past four decades. This box contains
    an assortment of formats, including analog video in 3:4 aspect ratio, analog photos,
    mono audio recording, and digital video. One thing is clear: none of the media
    in the box was created with Blu-ray in mind because most of it was recorded before
    Blu-ray existed. You will have to preprocess each of the media sources to prepare
    it for the Blu-ray package, such as correcting color, cleaning up VHS judder,
    and fixing the aspect ratio in the analog video. For the mono audio, you will
    need to remove tape hiss and extrapolate the mono audio track to stereo. Figure
    2.10 summarizes this process of assembling various elements to generate a Blu-ray
    disc.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当你学习深度学习时，为什么与真实且混乱的数据集一起工作如此重要？当你考虑将深度学习应用于真实世界的数据集时，你可以将其比作被要求从过去四十年收集的媒体盒中创建蓝光光盘。这个盒子包含各种格式，包括3:4宽高比的模拟视频、模拟照片、单声道音频录制和数字视频。有一点很清楚：盒中的所有媒体都不是为了蓝光而制作的，因为其中大部分是在蓝光存在之前录制的。你将不得不对每个媒体源进行预处理，以便为蓝光包做准备，例如校正颜色、清理VHS抖动和调整模拟视频的宽高比。对于单声道音频，你需要去除磁带噪音并将单声道音频轨道扩展到立体声。图2.10总结了将各种元素组装起来生成蓝光光盘的过程。
- en: Similarly, you can bet that the problems in your organization that are candidates
    for deep learning have datasets that were *not* collected with deep learning in
    mind. You will need to clean up, transform, and extrapolate to get a dataset that
    is ready to train a deep learning model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以打赌你组织中的深度学习候选问题都有数据集，这些数据集在收集时并没有考虑到深度学习。你需要进行清理、转换和扩展，以获得一个可以用于训练深度学习模型的准备好的数据集。
- en: '![CH02_F10_Ryan](../Images/CH02_F10_Ryan.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F10_Ryan](../Images/CH02_F10_Ryan.png)'
- en: Figure 2.10 Preparing a real-world dataset for deep learning is like creating
    a Blu-ray disc from a box of media.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 准备真实世界数据集以用于深度学习就像从媒体盒中创建蓝光光盘。
- en: 2.8 Format and scope of the input dataset
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 输入数据集的格式和范围
- en: 'Now that we have reviewed the streetcar delay problem and the importance of
    working with a real-world dataset, let’s dig into the structure of the streetcar
    delay dataset ([http://mng.bz/4B2B](http://mng.bz/4B2B)). This dataset has the
    following file structure:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了街车延误问题和与真实世界数据集一起工作的重要性，让我们深入了解街车延误数据集的结构（[http://mng.bz/4B2B](http://mng.bz/4B2B)）。这个数据集具有以下文件结构：
- en: An XLS file for each year
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每年一个XLS文件
- en: In each XLS file, a tab for each month
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个XLS文件中，为每个月份设置一个标签页
- en: Figure 2.11 shows the file structure of the streetcar delay dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11展示了街车延误数据集的文件结构。
- en: '![CH02_F11_Ryan](../Images/CH02_F11_Ryan.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F11_Ryan](../Images/CH02_F11_Ryan.png)'
- en: Figure 2.11 File structure of the streetcar delay dataset
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 街车延误数据集的文件结构
- en: '![CH02_F12_Ryan](../Images/CH02_F12_Ryan.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F12_Ryan](../Images/CH02_F12_Ryan.png)'
- en: Figure 2.12 Columns in the streetcar delay dataset
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 街车延误数据集的列
- en: 'Figure 2.12 shows the columns in the streetcar delay dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12展示了街车延误数据集中的列：
- en: '*Report Date* —The date (YYYY/MM/DD) when the delay-causing incident occurred'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*报告日期* — 导致延误事件发生的日期（YYYY/MM/DD）'
- en: '*Route* —The number of the streetcar route'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路线* — 街车路线的编号'
- en: '*Time* —The time (hh:mm:ss AM/PM) when the delay-causing incident occurred'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间* — 导致延误事件发生的时间（hh:mm:ss AM/PM）'
- en: '*Day* —The name of the day'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期* — 日期名称'
- en: '*Location* —The location of the delay-causing incident'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置* — 导致延误事件的位置'
- en: '*Incident* —The description of the delay-causing incident'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件* — 导致延误的事件描述'
- en: '*Min Delay* —The delay, in minutes, in the schedule of the following streetcar'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小延误* — 下一个街车在计划中的延误时间（分钟）'
- en: '*Min Gap* —The total scheduled time, in minutes, from the streetcar ahead of
    the following streetcar'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小间隔* — 前一个街车与下一个街车之间的总计划时间（分钟）'
- en: '*Direction* —The direction of the route (E/B, W/B, N/B, S/B, B/W, and variants),
    where B/W indicates both ways'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方向* — 路线的方向（E/B, W/B, N/B, S/B, B/W等），其中B/W表示双向'
- en: Vehicle—The ID number of the vehicle involved in the incident
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆—涉及事件的车辆ID
- en: 'It’s worth taking a bit more time to review the characteristics of some of
    these columns:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花更多时间来审查这些列的一些特征：
- en: '*Report Date* —This column embeds a lot of information that could be valuable
    for the deep learning model. In chapter 5, we will revisit this column to add
    derived columns to the dataset for subcomponents of this column: year, month,
    and day of month.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*报告日期* — 这列包含了许多对深度学习模型可能很有价值的信息。在第5章中，我们将重新审视这一列，为该列的子组件（年、月和日）添加派生列到数据集中。'
- en: '*Day* —Does this column duplicate information that is already encoded in the
    Report Date column? For the purposes of this problem, is it relevant that an incident
    happened on a Monday that also happened to be the last day of the month? We will
    explore these questions in chapter 9.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期* — 这列是否重复了已经在“报告日期”列中编码的信息？对于这个问题，一个事件发生在周一并且恰好是月底的最后一天是否相关？我们将在第9章中探讨这些问题。'
- en: '*Location* —This column is the most interesting one in the dataset. It encodes
    the geographical aspect of the dataset in a challenging, open-ended format. In
    chapter 4, we will revisit this column to answer some important questions, including
    why this data isn’t encoded as longitude and latitude and how the unique topography
    of the streetcar network manifests itself in the values in the Location column.
    In chapter 9, we will look at the most effective way to encode this information
    for the deep learning model.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置* — 这列是数据集中最有趣的一列。它以具有挑战性和开放式的方式编码了数据集的地理方面。在第4章中，我们将重新审视这一列，以回答一些重要问题，包括为什么这些数据没有以经纬度的形式编码，以及电车网络的独特地形是如何在“位置”列的值中体现出来的。在第9章中，我们将探讨为深度学习模型编码这些信息的最有效方法。'
- en: '![CH02_F13_Ryan](../Images/CH02_F13_Ryan.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F13_Ryan](../Images/CH02_F13_Ryan.png)'
- en: Figure 2.13 Streetcar dataset record count by year
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 电车数据集按年记录数
- en: The dataset currently has more than 70,000 records, and 1,000 to 2,000 new records
    are added every month. Figure 2.13 shows the number of records per year since
    the beginning of the dataset in January 2014\. The raw record count is the number
    of records in the input dataset. The cleaned record count is the number of records
    after records with invalid values (such as specifying delays on routes that aren’t
    valid streetcar routes) are dropped, as described in chapter 3.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当前数据集有超过70,000条记录，每月新增1,000至2,000条新记录。图2.13显示了自2014年1月数据集开始以来的每年记录数。原始记录数是输入数据集中的记录数。清理后的记录数是删除了无效值（如指定在无效的电车路线上延误）的记录后的记录数，如第3章所述。
- en: '2.9 The destination: An end-to-end solution'
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 目标：端到端解决方案
- en: Throughout the rest of this book, we will work through the problem of predicting
    streetcar delays. We will clean up the input dataset, build a deep learning model,
    train it, and then deploy it to help users get predictions on whether their streetcar
    trips are going to be delayed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将解决预测电车延误的问题。我们将清理输入数据集，构建深度学习模型，对其进行训练，然后部署它以帮助用户获得关于他们的电车行程是否会延误的预测。
- en: Figure 2.14 shows one of the results you will get after following through the
    extended example in this book. You will be able to get predictions on whether
    a particular streetcar trip will be delayed from the deep learning model that
    you trained with data derived from the raw dataset introduced in this chapter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14显示了遵循本书中扩展示例后你将得到的一个结果。你将能够从使用本章引入的原始数据集派生出的数据训练的深度学习模型中获得关于特定电车行程是否会延误的预测。
- en: '![CH02_F14_Ryan](../Images/CH02_F14_Ryan.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F14_Ryan](../Images/CH02_F14_Ryan.png)'
- en: 'Figure 2.14 One result of the extended example: Facebook Messenger deployment'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 扩展示例的一个结果：Facebook Messenger部署
- en: 'Figure 2.15 summarizes the end-to-end journey that you will take through this
    book with the extended example of streetcar delays, from the raw dataset introduced
    in this chapter to the model deployments that allow users to get predictions on
    whether their streetcar trips will be delayed. Note that the two deployment methods
    shown in figure 2.15 (Facebook Messenger and web deployment, respectively) are
    two means to the same end: making the trained deep learning model accessible to
    users who want to get predictions on whether their streetcar trips are going to
    be delayed.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15总结了通过扩展示例电车延误的端到端旅程，从本章引入的原始数据集到允许用户获得关于他们的电车行程是否会延误的模型部署。请注意，图2.15中显示的两个部署方法（Facebook
    Messenger和Web部署）是达到同一目的的两种手段：让想要获得关于他们的电车行程是否会延误的预测的用户能够访问训练好的深度学习模型。
- en: '![CH02_F15_Ryan](../Images/CH02_F15_Ryan.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F15_Ryan](../Images/CH02_F15_Ryan.png)'
- en: Figure 2.15 The journey from raw data to streetcar trip predictions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 从原始数据到街车行程预测的旅程
- en: 'The numbers in figure 2.15 highlight what’s covered in each chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 中的数字突出了每个章节所涵盖的内容：
- en: Chapter 2 introduces the raw dataset that we will be using in the extended example.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 2 章介绍了我们将用于扩展示例的原始数据集。
- en: Chapters 3 and 4 describe the steps you take to clean up the raw dataset and
    prepare it to train a deep learning model.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 3 章和第 4 章描述了您清理原始数据集并准备训练深度学习模型的步骤。
- en: Chapter 5 describes how to create a simple deep learning model using the Keras
    library.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 5 章描述了如何使用 Keras 库创建一个简单的深度学习模型。
- en: Chapter 6 describes how to train the Keras deep learning model using the dataset
    you prepared in chapters 3 and 4.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 6 章描述了如何使用第 3 章和第 4 章中准备的数据集来训练 Keras 深度学习模型。
- en: Chapter 7 describes how to conduct a series of experiments to determine the
    impact of changing aspects of the deep learning model and of replacing the deep
    learning model with a non-deep-learning approach.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 7 章描述了如何进行一系列实验，以确定深度学习模型变化方面的影响，以及用非深度学习方法替换深度学习模型的影响。
- en: Chapter 8 shows how to deploy the deep learning model you trained in chapter
    6\. You will use the pipeline facility from the scikit-learn library to process
    the trip data that users provide so that the trained deep learning model can make
    predictions. Chapter 8 leads you through two deployment options. First, we go
    through deploying the trained model via a web page served by Flask ([https://
    flask.palletsprojects.com/en/1.1.x](https://flask.palletsprojects.com/en/1.1.x)),
    a basic web application framework for Python. This deployment method is straightforward,
    but the user experience is limited. The second deployment approach provides a
    richer user experience by using the Rasa chatbot framework to interpret the user’s
    trip prediction requests and display the trained model’s predictions in Facebook
    Messenger.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 8 章展示了如何部署在第 6 章中训练的深度学习模型。您将使用 scikit-learn 库的管道工具来处理用户提供的行程数据，以便训练的深度学习模型可以进行预测。第
    8 章将引导您了解两种部署选项。首先，我们将通过 Flask（[https://flask.palletsprojects.com/en/1.1.x](https://flask.palletsprojects.com/en/1.1.x)）这个
    Python 基本网络应用框架提供的网页来部署训练好的模型。这种方法简单直接，但用户体验有限。第二种部署方法通过使用 Rasa 聊天机器人框架来解释用户的行程预测请求，并在
    Facebook Messenger 中显示训练模型的预测结果，从而提供更丰富的用户体验。
- en: 2.10 More details on the code that makes up the solutions
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 关于构成解决方案的代码的更多细节
- en: 'The end-to-end journey described in section 2.9 is implemented through a series
    of Python programs and the files that are inputs and outputs to each program.
    As described in chapter 1, these files are available in the repo for this book:
    [http://mng .bz/v95x](http://mng.bz/v95x). The next listing shows the key directories
    in the repo and summarizes the files in each directory.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2.9 节中描述的端到端旅程是通过一系列 Python 程序以及每个程序的输入和输出文件来实现的。正如第 1 章所述，这些文件可在本书的代码库中找到：[http://mng.bz/v95x](http://mng.bz/v95x)。下面的列表显示了代码库中的关键目录，并总结了每个目录中的文件。
- en: Listing 2.5 Directory structure of the repo
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 代码库的目录结构
- en: '[PRE6]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Saved intermediate Pandas dataframes and other data inputs. Note that the
    XLS files for the raw dataset are not in the repo; you need to get them from [http://mng.bz/4B2B](http://mng.bz/4B2B).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保存的中间 Pandas 数据框和其他数据输入。请注意，原始数据集的 XLS 文件不在代码库中；您需要从 [http://mng.bz/4B2B](http://mng.bz/4B2B)
    获取它们。
- en: ❷ Files for Facebook Messenger deployment
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Facebook Messenger 部署文件
- en: ❸ Training data for Rasa chatbot
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Rasa 聊天机器人训练数据
- en: ❹ Rasa chatbot models. Note that the Rasa chatbot models used in the Facebook
    Messenger deployment are distinct from the deep learning models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Rasa 聊天机器人模型。请注意，在 Facebook Messenger 部署中使用的 Rasa 聊天机器人模型与深度学习模型不同。
- en: ❺ Files for the web deployment
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 网络部署文件
- en: ❻ CSS for the web deployment
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 网络部署的 CSS
- en: ❼ HTML files for the web deployment
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 网络部署的 HTML 文件
- en: ❽ Saved trained deep learning and XGBoost models
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 保存的训练深度学习和 XGBoost 模型
- en: ❾ Jupyter notebooks for data cleanup, data exploration, and model training,
    along with associated config files. See chapter 3 for a description of how config
    files are used in the streetcar delay prediction example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 用于数据清理、数据探索和模型训练的 Jupyter 笔记本，以及相关的配置文件。有关配置文件在街车延误预测示例中使用的描述，请参阅第 3 章。
- en: ❿ Saved pipeline files
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 保存的管道文件
- en: ⓫ SQL examples
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ SQL 示例
- en: 'Figures 2.16 and 2.17 describe the same end-to-end journey as figure 2.15 from
    the perspective of the Python programs and the files that flow between them. Figure
    2.16 shows the progression from the raw dataset to a trained deep learning model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16和图2.17从Python程序及其之间的文件流的角度描述了与图2.15相同的端到端旅程。图2.16展示了从原始数据集到训练好的深度学习模型的演变过程：
- en: '![CH02_F16_Ryan](../Images/CH02_F16_Ryan.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F16_Ryan](../Images/CH02_F16_Ryan.png)'
- en: Figure 2.16 File progression from raw dataset to trained model
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 从原始数据集到训练好的模型的文件演变
- en: We begin with the raw dataset made up of XLS files as described in this chapter.
    When you have run the data preparation notebook, you can save the data from the
    XLS files as a pickled dataframe that you can use to conveniently rerun the data
    preparation notebook. Chapter 3 explains the pickle facility that lets you serialize
    Python objects in files so that objects can be saved between Python sessions.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从本章描述的由XLS文件组成的原始数据集开始。当你运行数据准备笔记本后，你可以将XLS文件中的数据保存为pickle格式的dataframe，这样你就可以方便地重新运行数据准备笔记本。第3章解释了pickle功能，它允许你在文件中序列化Python对象，以便在Python会话之间保存对象。
- en: The dataset is cleaned up (duplicate values are mapped to a common value, for
    example, and records with invalid values are removed) by the data preparation
    notebook streetcar_data_preparation.ipynb to generate a cleansed pickled dataframe.
    This part of the process is described in chapters 3 and 4.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备笔记本streetcar_data_preparation.ipynb通过清理（例如，将重复值映射到公共值，并删除无效值的记录）数据集来生成清洁后的pickle格式的dataframe。这个过程在第3章和第4章中有所描述。
- en: The cleansed pickled dataframe is input to the model training notebook streetcar
    _model_training.ipynb that refactors the dataset and uses it to generate pickled
    pipeline files and a trained deep learning model file. This part of the process
    is described in chapters 5 and 6.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洁后的pickle格式的dataframe被输入到模型训练笔记本streetcar_model_training.ipynb中，该笔记本重构数据集并使用它生成pickle格式的管道文件和训练好的深度学习模型文件。这个过程在第5章和第6章中有所描述。
- en: 'Figure 2.17 picks up the story from the trained model file and pickled pipeline
    files generated by the model training notebook. The pipelines are used in the
    deployments to take the trip information entered by users (such the streetcar
    route and direction) and transform this information into a format that the trained
    deep learning model can take in to make a prediction. As you will see in chapter
    8, these pipelines appear twice in the process. First, they transform the data
    that is used to train the deep learning model; then they transform user input
    so that the trained model can generate a prediction for it. Chapter 8 describes
    two kinds of deployment: web deployment, where the user enters the trip information
    in a web page, and Facebook Messenger deployment, where the user enters the trip
    information in a Messenger session.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17从模型训练笔记本生成的训练好的模型文件和pickle格式的管道文件中继续讲述故事。这些管道在部署中使用，用于将用户输入的行程信息（例如，电车路线和方向）转换为训练好的深度学习模型可以接受的格式。正如你将在第8章中看到的，这些管道在过程中出现两次。首先，它们转换用于训练深度学习模型的数据；然后它们转换用户输入，以便训练好的模型可以为其生成预测。第8章描述了两种部署方式：网页部署，用户在网页中输入行程信息，以及Facebook
    Messenger部署，用户在Messenger会话中输入行程信息。
- en: '![CH02_F17_Ryan](../Images/CH02_F17_Ryan.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F17_Ryan](../Images/CH02_F17_Ryan.png)'
- en: Figure 2.17 File progression from trained model to deployments
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 从训练好的模型到部署的文件演变
- en: In this section, you have seen two perspectives on the journey from the input
    dataset to a deployed deep learning model that you can use to predict whether
    a streetcar trip is going to be delayed. This journey covers a broad range of
    technical components, but don’t worry if some of these components aren’t familiar
    to you. We’ll examine them one by one as we go through the chapters. By the time
    you complete chapter 8, you will have an end-to-end deep learning with structured
    data solution for predicting streetcar delays.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你看到了从输入数据集到可部署的深度学习模型旅程的两个视角，该模型可以用来预测电车行程是否会延误。这个旅程涵盖了广泛的技术组件，但如果你对其中的一些组件不熟悉，请不要担心。我们将逐章检查它们。当你完成第8章时，你将拥有一个端到端的深度学习解决方案，用于预测电车延误，该解决方案使用结构化数据。
- en: '2.11 Development environments: Vanilla vs. deep-learning-enabled'
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.11 开发环境：普通与深度学习支持
- en: In section 2.1, we reviewed your options for the environment to use in this
    book. In this section, we’ll review which subsets of the code would benefit from
    a deep-learning-enabled environment and which are fine with a *vanilla* system—one
    that does not have access to any deep-learning-specific hardware such as GPUs
    and TPUs. This vanilla system could be your local system (after you have installed
    Jupyter Notebooks, Python, and the required Python libraries) or a non-GPU/non-TPU
    cloud environment.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.1节中，我们回顾了在这本书中使用的环境选项。在本节中，我们将回顾哪些代码子集会从深度学习支持的环境中受益，哪些则可以在没有深度学习特定硬件（如GPU和TPU）的*vanilla*系统中运行良好。这个vanilla系统可以是您的本地系统（在您安装了Jupyter
    Notebooks、Python和所需的Python库之后）或非GPU/非TPU的云环境。
- en: 'Figure 2.18 shows the end-to-end solution highlighting which areas would benefit
    from a deep-learning-enabled environment and which can be worked in a vanilla
    environment:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18展示了端到端解决方案，突出了哪些区域将受益于深度学习支持的环境，哪些可以在vanilla环境中工作：
- en: The data preparation code (described in chapters 2-4) can be used in either
    a vanilla environment or a deep learning environment. The operations that you
    will be performing in these sections do not require any deep-learning-specific
    hardware. I have run the data preparation code in both Paperspace Gradient and
    on my local Windows machine, which has no deep-learning-specific hardware.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备代码（在第2-4章中描述）可以在vanilla环境或深度学习环境中使用。在这些部分中您将执行的操作不需要任何深度学习特定硬件。我已经在Paperspace
    Gradient和我的没有深度学习特定硬件的本地Windows机器上运行了数据准备代码。
- en: The model training code described in chapters 5-7 will run more slowly if you
    don’t have access to deep-learning-specific hardware, such as the GPUs available
    in Paperspace, Azure, or Colab, or the TPUs available in Google Cloud Services
    and Colab.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您没有访问深度学习特定硬件（如Paperspace、Azure或Colab中可用的GPU，或Google Cloud Services和Colab中可用的TPU），那么第5-7章中描述的模型训练代码将运行得更慢。
- en: The deployment code described in chapter 8 can be used in either a vanilla environment
    or a deep learning environment. I have done the deployment on both Azure (using
    a standard, non-GPU-enabled VM) and on my local Windows machine. The trained model
    does not require deep-learning-specific hardware when it is deployed.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第8章中描述的部署代码可以在vanilla环境或深度学习环境中使用。我已经在Azure（使用标准、非GPU启用虚拟机）和我的本地Windows机器上完成了部署。当模型部署时，它不需要深度学习特定硬件。
- en: '![CH02_F18_Ryan](../Images/CH02_F18_Ryan.png)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH02_F18_Ryan](../Images/CH02_F18_Ryan.png)'
- en: Figure 2.18 Portions of the process that benefit from a deep-learning-enabled
    environment
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.18 深度学习支持环境受益的过程部分
- en: 2.12 A deeper look at the objections to deep learning
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.12 深入探讨深度学习的反对意见
- en: In chapter 1, we briefly reviewed some of the pros and cons of deep learning.
    It’s worthwhile to do a more detailed comparison of deep learning with non-deep-learning
    machine learning. For the sake of simplicity, in this chapter we will simply refer
    to the latter as *classic machine learning* .
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们简要回顾了深度学习的优缺点。对深度学习与非深度学习机器学习进行更详细的比较是值得的。为了简化，在本章中，我们将简单地称后者为*经典机器学习*。
- en: We need to contrast classic machine learning with deep learning when we’re dealing
    with problems with structured tabular data. The conventional wisdom is to use
    classic machine learning, rather than deep learning, on structured data. The whole
    point of this book is to examine how deep learning can be applied to structured
    data, so we need to provide some motivation for this approach and examine the
    reasoning behind the dictum “If it’s structured data, don’t use deep learning.”
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理具有结构化表格数据的问题时，我们需要将经典机器学习与深度学习进行对比。传统观点是，在结构化数据上使用经典机器学习，而不是深度学习。本书的整个目的就是探讨深度学习如何应用于结构化数据，因此我们需要为这种方法提供一些动机，并检查“如果数据是结构化的，不要使用深度学习”这一格言背后的推理。
- en: '![CH02_F19_Ryan](../Images/CH02_F19_Ryan.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F19_Ryan](../Images/CH02_F19_Ryan.png)'
- en: Figure 2.19 Objections to using deep learning with structured data
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 使用结构化数据进行深度学习的反对意见
- en: 'Let’s dig a bit deeper into the objections to deep learning introduced in Chapter
    1 and illustrated in figure 2.19:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨第1章中引入的、图2.19中展示的深度学习的反对意见：
- en: '*Structured datasets are too small to feed deep learning.* There is a very
    valid point behind this objection, even if the whole objection does not hold up
    to scrutiny. Certainly, many structured datasets are too small for deep learning,
    and perhaps that’s the source of the perception that structured datasets are too
    small for deep learning. It is not uncommon, however, for relational tables to
    have tens of millions and even billions of rows. The top commercial relational
    database vendors support tables with more than a billion rows. Easy-to-find, open
    source structured datasets do tend to be small, so when you’re looking for a problem
    to investigate, it’s easier to find a structured dataset that is small and appropriate
    for classic machine learning than it is to find a large, open source structured
    dataset. So the problem with dataset size is more a question of convenience than
    an intrinsic size problem with structured datasets. Luckily, the streetcar dataset
    used in this book is both openly available *and* big enough to make it an interesting
    subject for deep learning.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化数据集太小，无法用于深度学习。* 这个反对意见背后有一个非常合理的观点，即使整个反对意见经不起推敲。当然，许多结构化数据集对于深度学习来说太小了，这可能就是人们认为结构化数据集太小无法用于深度学习的来源。然而，关系表通常有数千万甚至数十亿行。顶级商业关系数据库供应商支持超过十亿行的表。易于找到的开源结构化数据集往往很小，因此当您在寻找要研究的问题时，找到适合经典机器学习的小型结构化数据集比找到大型开源结构化数据集要容易。因此，数据集大小的问题更多的是一个便利性问题，而不是结构化数据集固有的规模问题。幸运的是，本书中使用的电车数据集既公开可用，又足够大，使其成为深度学习的一个有趣主题。'
- en: '*Keep it simple.* The most common argument for choosing classic machine learning
    over deep learning for structured datasets is simplicity. The rationale is that
    classic machine learning algorithms are simpler, easier to use, and more transparent
    than deep learning. In chapter 7, you will see a direct comparison of the deep
    learning solution to the streetcar delay prediction problem and a solution based
    on XGBoost, a non-deep-learning approach. Both implementations use the same data
    preparation steps and the same pipelines, which constitute most of the code that
    makes up the solution. The heart of the solution is the definition, training,
    and evaluation of the model where the two solutions diverge. In that section of
    the solution, the deep learning approach has about 60 lines of code compared with
    fewer than 20 for XGBoost. Although the XGBoost solution has fewer lines of code
    at the heart of its solution, this section of the code represents less than 10%
    of the total lines of code in either approach. Also, the additional complexity
    in the heart of the deep learning approach allows it to be much more flexible
    because it can handle datasets with columns containing freeform text.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*保持简单。* 选择经典机器学习而不是深度学习来处理结构化数据集的最常见论点是简单性。其理由是，经典机器学习算法比深度学习算法更简单、更容易使用，并且更透明。在第7章中，您将看到针对电车延误预测问题的深度学习解决方案与基于XGBoost（一种非深度学习方法）的解决方案的直接比较。这两种实现都使用了相同的数据准备步骤和相同的管道，这构成了构成解决方案的大部分代码。解决方案的核心是模型的定义、训练和评估，这是两种解决方案的分岔点。在该解决方案的部分中，深度学习方法的代码行数大约有60行，而XGBoost的代码行数少于20行。尽管XGBoost解决方案的核心代码行数较少，但这一部分代码在两种方法的总代码行数中不到10%。此外，深度学习方法核心部分的额外复杂性使其具有更大的灵活性，因为它可以处理包含自由文本列的数据集。'
- en: '*Deep learning is vulnerable to adversarial attacks* *.* There are well-publicized
    examples of deep learning systems being fooled into incorrect scoring of data
    examples that were purposely altered to exploit vulnerabilities in the system.
    An image of a panda, for example, can be doctored in a way that is imperceptible
    to humans but tricks a deep learning model into misidentifying the image. You
    can see this image at [http://mng.bz/BE2g](http://mng.bz/BE2g). To us, the doctored
    image still looks like a panda, but the deep learning system scores the image
    as that of a gibbon.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习容易受到对抗性攻击。* *.* 有一些广为人知的例子表明，深度学习系统被愚弄，错误地评分了故意修改以利用系统漏洞的数据示例。例如，一只熊猫的照片可以通过一种对人类来说难以察觉的方式被篡改，但会欺骗深度学习模型将其误识别为长臂猿。您可以在[http://mng.bz/BE2g](http://mng.bz/BE2g)看到这张照片。对我们来说，篡改后的图像仍然看起来像一只熊猫，但深度学习系统将该图像评分为一长臂猿。'
- en: I argued in chapter 1 that the crown jewels of commerce and government reside
    in structured data. If it’s possible for a malicious actor to mislead a deep learning
    system, why would we trust a deep learning system with the analysis of such valuable
    data? Is this vulnerability unique to deep learning? The article at [http://mng.bz/dwyX](http://mng.bz/dwyX)
    argues that classic machine learning suffers from the same vulnerability to spoofing
    as deep learning. If that’s the case, why has so much attention been given to
    deep learning being fooled? The hype around deep learning invites counter narratives,
    and the examples of deep learning being fooled make better sound bites. It’s more
    interesting to hear about a deep learning model mistaking a giraffe for a polar
    bear than it is to hear about a linear regression model predicting an outcome
    incorrectly due to a doctored value in one column of an input table. Finally,
    the vulnerability of both deep learning and classic machine learning to adversarial
    attacks depends on the attackers getting inside information about the nature of
    the models. If there is proper governance of model security, the crown jewels
    of data should not be threatened by adversarial attacks.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在第一章中提出，商业和政府的瑰宝在于结构化数据。如果恶意行为者能够误导深度学习系统，我们为何还要信任深度学习系统去分析如此有价值的数据呢？这种漏洞是否仅限于深度学习？[http://mng.bz/dwyX](http://mng.bz/dwyX)上的文章认为，经典机器学习也遭受与深度学习相同的欺骗性漏洞。如果情况如此，为何对深度学习被欺骗的关注如此之多？围绕深度学习的炒作引发了反叙事，深度学习被欺骗的例子成为更好的口头禅。听到一个深度学习模型将长颈鹿误认为是北极熊，比听到一个线性回归模型因为输入表中某一列被篡改的值而预测错误更有趣。最后，深度学习和经典机器学习对对抗性攻击的脆弱性取决于攻击者获取关于模型性质的内部信息。如果对模型安全有适当的治理，数据瑰宝不应受到对抗性攻击的威胁。
- en: '*The* *days of handcrafted deep learning solutions are coming to an end.* The
    world of machine learning is moving fast enough, and its potential impact is large
    enough, that it would be unwise to predict how nonspecialists will harness machine
    learning in the early 2030s. Think about the interest in multimedia in the mid-1990s.
    Back then, even people who were not directly involved in creating multimedia platforms
    had to concern themselves with the arcana of sound card device drivers and IRQ
    conflicts if they wanted to take advantage of multimedia. Today, we take the audio
    and video capabilities of computers for granted, and only a select few need to
    worry about the details of multimedia. The term *multimedia* itself is now quaint
    due to lack of use; the technology has become so ubiquitous that we don’t need
    a term for it anymore. Will machine learning and deep learning share the same
    fate? It’s possible that automated solutions like Google’s Auto ML ([https://cloud.google.com/automl](https://cloud.google.com/automl))
    will mature to the point that only a relatively small set of deep ML specialists
    will need to do hand-coding of machine learning solutions. Even if that does happen,
    the fundamental concepts behind deep learning, as well as the practical steps
    that need to be taken to harness deep learning on a real-world dataset, will still
    be important to understand. These concepts are to machine learning what the periodic
    table is to chemistry. Unless we specialize in chemistry or a related discipline,
    most of us won’t have to directly apply the foundational concepts expressed in
    the periodic table. It’s still critical to learn about the periodic table, however,
    and understand what it tells us about the nature of matter. In a similar way,
    the fundamental concepts of deep learning will still be useful to understand,
    even if the implementation of machine learning systems is largely automated in
    the near future. You will be able to make better judgments about the applicability
    of deep learning because you understand at some level how it works.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*手工制作深度学习解决方案的时代即将结束*。机器学习的世界发展迅速，其潜在影响巨大，因此预测非专业人士在2030年代初如何利用机器学习是不明智的。回想一下1990年代中期对多媒体的兴趣。当时，即使没有直接参与创建多媒体平台的人，如果想要利用多媒体，也必须关注声卡设备驱动程序和中断请求冲突的奥秘。如今，我们理所当然地认为计算机具有音频和视频功能，只有少数人需要担心多媒体的细节。由于缺乏使用，*多媒体*这个术语现在已经过时；这项技术已经变得如此普遍，以至于我们不再需要为它命名。机器学习和深度学习会步上相同的命运吗？自动化解决方案，如谷歌的Auto
    ML ([https://cloud.google.com/automl](https://cloud.google.com/automl))，可能会发展到只有相对少数的深度机器学习专家需要手动编码机器学习解决方案的程度。即使真的发生这种情况，深度学习的根本概念以及在实际数据集上利用深度学习所需采取的实际步骤仍然非常重要，需要理解。这些概念对机器学习来说就像元素周期表对化学一样。除非我们专攻化学或相关学科，否则我们中的大多数人不必直接应用元素周期表中表达的基础概念。然而，了解元素周期表以及它告诉我们关于物质性质的信息仍然至关重要。以类似的方式，即使在未来不久机器学习系统的实现大部分是自动化的，深度学习的根本概念仍然值得理解。由于你了解它在某种程度上是如何工作的，你将能够更好地判断深度学习的适用性。'
- en: 'We’ve examined in more detail some of the objections to deep learning. One
    objection in particular—deep learning is too difficult for nonspecialists to use—was
    more true five years ago than it is today. What changed in recent years to make
    deep learning a contender in more problem areas and to make it accessible to nonspecialists?
    We’ll answer this question in the next section. First, we can ask which approach
    is better for dealing with structured data: deep learning or classic machine learning.
    In chapters 5 and 6, you will see the code for the deep learning model to predict
    streetcar delays. You will also see the results of a series of experiments to
    measure the performance of the trained deep learning model. In addition, chapter
    7 describes an alternative solution to the streetcar delay prediction problem
    that uses a classic machine learning algorithm, XGBoost. In that chapter, you
    will be able to directly compare the code for the deep learning solution with
    the code for the XGBoost solution. You will also see a head-to-head comparison
    of the performance of the two solutions so that you can draw your own conclusions
    about which approach is better suited to solving this problem.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更详细地考察了一些对深度学习的反对意见。特别是，深度学习对非专业人士来说过于困难这一反对意见，五年前比现在更为真实。近年来发生了什么变化，使得深度学习能够在更多的问题领域中成为竞争者，并使其对非专业人士易于访问？我们将在下一节中回答这个问题。首先，我们可以问哪种方法更适合处理结构化数据：深度学习还是经典机器学习。在第5章和第6章中，你将看到用于预测电车延误的深度学习模型的代码。你还将看到一系列实验的结果，以衡量训练好的深度学习模型的表现。此外，第7章描述了使用经典机器学习算法XGBoost的替代解决方案来解决电车延误预测问题。在该章中，你将能够直接比较深度学习解决方案的代码与XGBoost解决方案的代码。你还将看到两种解决方案的性能对比，以便你可以自己得出结论，关于哪种方法更适合解决此问题。
- en: 2.13 How deep learning has become more accessible
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.13 深度学习如何变得更加易于访问
- en: In section 2.12, we reviewed some of the objections to deep learning. What has
    changed to make this comparison even worth making, and why is deep learning now
    a viable approach alongside classic machine learning? Figure 2.20 summarizes some
    of the changes in the past decade that have opened up the power of deep learning
    to a wider audience.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.12节中，我们回顾了一些对深度学习的反对意见。是什么变化使得这种比较更有意义，为什么深度学习现在成为与经典机器学习并驾齐驱的可行方法？图2.20总结了过去十年中的一些变化，这些变化使得深度学习的力量得以向更广泛的受众开放。
- en: '![CH02_F20_Ryan](../Images/CH02_F20_Ryan.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F20_Ryan](../Images/CH02_F20_Ryan.png)'
- en: Figure 2.20 Recent changes that make deep learning accessible to nonspecialists
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 使深度学习对非专业人士易于访问的近期变化
- en: 'Following are some more details on the changes listed in figure 2.20:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是图2.20中列出的变化的更多细节：
- en: '*Cloud environments* —As outlined in section 2.1, all the major cloud vendors
    provide deep learning environments that include the software stack and the specialized
    hardware needed for efficient training of deep learning models. The choice available
    in these environments, from the deep-learning-focused Gradient environment of
    Paperspace to the all-encompassing breadth of AWS, the hardware options and value
    for money have been improving year by year. Deep learning is no longer the preserve
    of the lucky few who work for tech giants or well-endowed academic institutions
    that can afford dedicated, on-premises hardware for deep learning.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云环境* ——如2.1节所述，所有主要的云服务提供商都提供深度学习环境，包括用于深度学习模型高效训练的软件堆栈和专用硬件。在这些环境中可用的选择，从专注于深度学习的Paperspace的Gradient环境到AWS的全覆盖范围，硬件选项和性价比都在逐年提高。深度学习不再是只有那些在科技巨头或资金充足的学术机构工作的幸运儿才能享有的专属技术，这些机构能够负担得起用于深度学习的专用本地硬件。'
- en: '*Stable, usable deep learning libraries* —Since the initial release of Keras
    in early 2015, it’s been possible to take advantage of the power of deep learning
    through an intuitive and easy-to-learn interface. The release of PyTorch ([https://pytorch.org](https://pytorch.org))
    in late 2016 and TensorFlow 2.0 in mid-2019 gave developers even more usable choices
    for deep learning.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稳定的、可用的深度学习库* ——自从2015年初Keras的首次发布以来，通过直观且易于学习的界面，人们已经可以利用深度学习的力量。2016年末PyTorch([https://pytorch.org](https://pytorch.org))的发布和2019年中TensorFlow
    2.0的推出，为开发者提供了更多可用的深度学习选择。'
- en: '*Large, open datasets* —In the past decade, there has been an explosion of
    large datasets that lend themselves to deep learning. The advent of social media
    and smartphones in the 2000s, government initiatives to provide open datasets,
    and Google’s Dataset Search ([https://toolbox.google.com/datasetsearch](https://toolbox.google.com/datasetsearch))
    combined to make large, interesting datasets accessible. To complement this tsunami
    of data, sites such as Kaggle ([https://www.kaggle.com](https://www.kaggle.com))
    provide a community of machine learning investigators who want to exploit these
    datasets.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大型、开放的数据集* —— 在过去十年中，大量适合深度学习的大数据集如雨后春笋般涌现。2000年代社交媒体和智能手机的出现、政府提供开放数据集的倡议以及谷歌的数据集搜索([https://toolbox.google.com/datasetsearch](https://toolbox.google.com/datasetsearch))共同使得大型、有趣的数据集变得可访问。为了补充这股数据洪流，Kaggle([https://www.kaggle.com](https://www.kaggle.com))等网站提供了一个想要利用这些数据集的机器学习研究者社区。'
- en: '*High-quality, accessible educational material on deep learning* —This material
    includes excellent online courses for self-study. Since 2017, courses such as
    deeplearning.ai and fast.ai have made it possible for nonspecialists to learn
    about deep learning and, particularly in the case of fast.ai, exercise what they
    have learned in real coding examples. These courses mean that you don’t need to
    get a graduate degree in artificial intelligence to learn about deep learning.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高质量的深度学习教育资源* —— 这包括一些优秀的自学在线课程。自2017年以来，deeplearning.ai和fast.ai等课程使得非专业人士能够学习深度学习，尤其是在fast.ai的情况下，可以在真实的编码示例中练习所学知识。这些课程意味着你不需要获得人工智能的硕士学位来学习深度学习。'
- en: 2.14 A first taste of training a deep learning model
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.14 深度学习模型训练初体验
- en: Over the course of the next six chapters, we will work through the code in the
    extended example, starting with cleaning up data in chapter 3 and ending with
    deploying the trained model in chapter 8\. If you think of the extended example
    as being an action movie, the big car chase takes place in chapter 6 when we train
    the deep learning model. To give you a taste of the climax, in this section we
    will go through a simplified scenario of training a deep learning model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的六章中，我们将逐步分析扩展示例中的代码，从第3章的数据清理开始，到第8章部署训练好的模型结束。如果你把扩展示例看作是一部动作片，那么第6章中训练深度学习模型的部分就是高潮部分。为了让你提前感受一下高潮，在本节中，我们将通过一个简化的场景来训练深度学习模型。
- en: 'If you cloned the repo described in section 2.10, you see the following files,
    which we will use in this section:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你克隆了第2.10节中描述的仓库，你将看到以下文件，我们将在本节中使用这些文件：
- en: In the notebooks directory, the notebook streetcar_model_training.ipynb contains
    the code for defining and training the deep learning model. The config file streetcar_model_training_config.yml
    lets you set the parameters for the model training process.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在notebooks目录中，名为streetcar_model_training.ipynb的notebook包含了定义和训练深度学习模型的代码。配置文件streetcar_model_training_config.yml允许你设置模型训练过程的参数。
- en: In the data directory, you will find a ready-made, cleaned-up dataset file called
    2014_2019_df_cleaned_remove_bad_values_may16_2020.pkl. You can use this file as
    input to the model training notebook, so you don’t have to run the dataset cleanup
    code for this scenario. If the trained deep learning model is a gourmet meal,
    you can think of this cleaned-up dataset file as being a dish of ingredients that
    have been cleaned and chopped up, ready to cook. When you go through chapters
    3 and 4, you will be rinsing and slicing the ingredients yourself, but for the
    purposes of this section, the prep work has been done for you, and you will simply
    be popping everything into the oven.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在data目录中，你可以找到一个现成的、清理过的数据集文件，名为2014_2019_df_cleaned_remove_bad_values_may16_2020.pkl。你可以使用这个文件作为模型训练notebook的输入，因此你不需要为这个场景运行数据集清理代码。如果训练好的深度学习模型是一顿美食，那么这个清理过的数据集文件就可以看作是一份已经清洗和切好的食材，准备烹饪。当你阅读第3章和第4章时，你需要自己清洗和切片食材，但在这个部分，准备工作已经为你完成，你只需将所有食材放入烤箱即可。
- en: 'Here are the steps to follow to do a simple training run of the deep learning
    model:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是进行深度学习模型简单训练运行的步骤：
- en: Update the streetcar_model_training_config.yml file to set the parameters as
    shown in the next listing.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将streetcar_model_training_config.yml文件更新为设置以下列表中的参数。
- en: Listing 2.6 Items that need to be updated in the config file for a simple training
    run
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表2.6 简单训练运行中需要更新的配置文件项
- en: '[PRE7]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Modifier to generate unique names for output pipeline and trained model files
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 生成输出管道和训练模型文件唯一名称的修饰符
- en: ❷ Filename for input file to the model training process
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 模型训练过程输入文件名
- en: ❸ Preset experiment that specifies the characteristic of the training run, including
    how many runs are done through the training set, whether the training accounts
    for imbalances in the training set, and whether the training stops early if the
    model is no longer improving.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 预设实验，指定训练运行的特性，包括通过训练集进行的运行次数、训练是否考虑训练集的不平衡性，以及模型不再改进时是否提前停止训练。
- en: ❹ Switch to control whether the extended calculation of test and training accuracy
    is run. This section of code can take a lot of time to run, so for this preliminary
    training run, we set this switch to False.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 切换是否运行测试和训练准确率的扩展计算。这段代码运行可能需要很长时间，因此在这个初步训练运行中，我们将此开关设置为False。
- en: Open the model training notebook in the environment you chose for the example,
    and run all cells in the notebook.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您为示例选择的环境中打开模型训练笔记本，并运行笔记本中的所有单元格。
- en: When the notebook has finished running, check the models subdirectory for a
    file called scmodelinitial_walkthrough_2020_9.h5\. This file is the trained model
    that has all the weights that were learned in the training process.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当笔记本运行完成后，请检查模型子目录中名为 scmodelinitial_walkthrough_2020_9.h5 的文件。此文件是训练过程中学习到的所有权重的训练模型。
- en: You have successfully trained a deep learning model. If you wanted to, you could
    follow the instructions in chapter 8 to create a simple website that would invoke
    this model to make predictions on whether a given streetcar trip would be delayed.
    But although this model is a functionally complete trained deep learning model,
    its performance isn’t the best. If you examine the end of the notebook, you will
    see a colored box called a confusion matrix that summarizes how good of a model
    you got in this training run. The confusion matrix looks something like figure
    2.21.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功训练了一个深度学习模型。如果您愿意，可以按照第8章的说明创建一个简单的网站，该网站将调用此模型来预测给定的电车行程是否会延误。但尽管这个模型是一个功能完整的训练好的深度学习模型，其性能并不是最好的。如果您查看笔记本的末尾，您将看到一个称为混淆矩阵的彩色框，它总结了您在这个训练运行中获得的好模型的程度。混淆矩阵看起来类似于图2.21。
- en: '![CH02_F21_Ryan](../Images/CH02_F21_Ryan.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F21_Ryan](../Images/CH02_F21_Ryan.png)'
- en: Figure 2.21 Example confusion matrix
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 示例混淆矩阵
- en: The top-left and bottom-right quadrants show the number of correct predictions
    that the model made on the test set (that is, a subset of the dataset that was
    not used in the training process), and the top-right and bottom-left quadrants
    show the number of predictions that the model got wrong. Chapter 6 provides a
    detailed description of the confusion matrix and how to interpret it, along with
    details on the parameters that make up a training experiment. For now, simply
    note that the bottom row of the confusion matrix shows that about 40% of the time,
    the model predicted no delay when a delay occurred. As you will see in chapter
    6, this outcome is the worst one for our users, so having it occur so frequently
    is not good. What can you do to get a better deep learning model? What is the
    difference between the raw input dataset described in section 2.8 and the cleaned-up
    dataset that was fed into this training run? Now that you have had a taste of
    training a deep learning model, see chapters 3-7 to answer these questions, and
    then see chapter 8 to make your trained deep learning model available to the outside
    world.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 左上角和右下角象限显示了模型在测试集（即未用于训练过程的dataset的子集）上做出的正确预测数量，而右上角和左下角象限显示了模型预测错误的数量。第6章提供了对混淆矩阵的详细描述以及如何解释它，以及构成训练实验的参数的详细信息。现在，只需注意混淆矩阵的最后一行显示，大约40%的时间，模型在发生延误时预测没有延误。正如您将在第6章中看到的，这种结果对我们用户来说是最糟糕的，因此它如此频繁地发生并不好。您能做些什么来获得更好的深度学习模型？第2.8节中描述的原始输入数据集与用于此训练运行的清理后的数据集有什么区别？现在您已经尝试过训练深度学习模型，请参阅第3-7章来回答这些问题，然后参阅第8章来使您的训练好的深度学习模型可供外界使用。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Two of the fundamental decisions in a deep learning project are what environment
    to use and what problem to tackle.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习项目中，有两个基本决策：使用什么环境和解决什么问题。
- en: You can choose your local system to run a deep learning project, or you can
    choose a full-featured cloud environment like Azure or AWS. In between are environments
    specifically designed for deep learning, including Paperspace and Google Colab.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以选择你的本地系统来运行深度学习项目，或者你可以选择一个功能齐全的云环境，如 Azure 或 AWS。介于两者之间的是专门为深度学习设计的环境，包括
    Paperspace 和 Google Colab。
- en: Pandas is the standard Python library for working with tabular datasets. If
    you are familiar with SQL, you will find that Pandas can conveniently do what
    you are used to doing with SQL.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 是用于处理表格数据集的标准 Python 库。如果你熟悉 SQL，你会发现 Pandas 可以方便地完成你习惯用 SQL 做的事情。
- en: One of the biggest objections to applying deep learning to structured data is
    that deep learning is too complex. Thanks to the accessibility of environments
    that target deep learning development, better deep learning frameworks, and deep
    learning education aimed at nonspecialists, this objection is not as relevant
    as it was five years ago.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习应用于结构化数据最大的反对意见之一是深度学习过于复杂。得益于针对深度学习开发的易用环境、更好的深度学习框架以及面向非专业人士的深度学习教育，这种反对意见不像五年前那样相关了。
- en: The code for the major example in this book is designed so that you can run
    subsets of it without having to run all the preceding steps. You can do deep learning
    model training runs directly to get a taste of training, for example.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书中的主要示例代码设计得如此，你可以运行它的子集而不必运行所有前面的步骤。例如，你可以直接运行深度学习模型训练来体验训练过程。
