- en: Chapter 3\. Recommending Music and the Audioscrobbler Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 推荐音乐与 Audioscrobbler 数据集
- en: The recommender engine is one of the most popular example of large-scale machine
    learning; for example, most people are familiar with Amazon’s. It is a common
    denominator because recommender engines are everywhere, from social networks to
    video sites to online retailers. We can also directly observe them in action.
    We’re aware that a computer is picking tracks to play on Spotify, in much the
    same way we don’t necessarily notice that Gmail is deciding whether inbound email
    is spam.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎是大规模机器学习的最受欢迎的示例之一；例如，大多数人都熟悉亚马逊的推荐系统。推荐引擎是一个共同点，因为它们无处不在，从社交网络到视频站点再到在线零售商。我们也可以直接观察它们的运行。我们知道
    Spotify 在选择播放曲目时是由计算机决定的，就像我们并不一定注意到 Gmail 在判断传入邮件是否是垃圾邮件时的操作一样。
- en: The output of a recommender is more intuitively understandable than other machine
    learning algorithms. It’s exciting, even. For as much as we think that musical
    taste is personal and inexplicable, recommenders do a surprisingly good job of
    identifying tracks we didn’t know we would like. For domains like music or movies,
    where recommenders are often deployed, it’s comparatively easy to reason why a
    recommended piece of music fits with someone’s listening history. Not all clustering
    or classification algorithms match that description. For example, a support vector
    machine classifier is a set of coefficients, and it’s hard even for practitioners
    to articulate what the numbers mean when they make predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的输出比其他机器学习算法更直观易懂，甚至有些令人兴奋。尽管我们认为音乐品味是个人的、难以解释的，推荐系统却出奇地能够准确识别我们可能会喜欢的音乐曲目。在像音乐或电影这样的领域，推荐系统通常能够相对容易地解释为什么推荐某首音乐与某人的收听历史相符。并非所有的聚类或分类算法都能满足这一描述。例如，支持向量机分类器是一组系数，即使对于从业者来说，在进行预测时这些数字代表的含义也很难表达清楚。
- en: It seems fitting to kick off the next three chapters, which will explore key
    machine learning algorithms on PySpark, with a chapter built around recommender
    engines, and recommending music in particular. It’s an accessible way to introduce
    real-world use of PySpark and MLlib and some basic machine learning ideas that
    will be developed in subsequent chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将探讨 PySpark 上的关键机器学习算法，其中第一章围绕推荐引擎展开，特别是音乐推荐。这是一个介绍 PySpark 和 MLlib
    实际应用的方式，同时也会涉及到随后章节中发展的一些基础机器学习概念。
- en: In this chapter, we’ll implement a recommender system in PySpark. Specifically,
    we will use the Alternating Least Squares (ALS) algorithm on an open dataset provided
    by a music streaming service. We’ll start off by understanding the dataset and
    importing it in PySpark. Then we’ll discuss our motivation for choosing the ALS
    algorithm and its implementation in PySpark. This will be followed by data preparation
    and building our model using PySpark. We’ll finish up by making some user recommendations
    and discussing ways to improve our model through hyperparameter selection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在 PySpark 中实现一个推荐系统。具体来说，我们将使用交替最小二乘（ALS）算法处理由音乐流媒体服务提供的开放数据集。我们将首先了解数据集并在
    PySpark 中导入它。然后，我们将讨论选择 ALS 算法的动机以及在 PySpark 中的实现。接着是数据准备和使用 PySpark 构建模型。最后，我们将提供一些用户推荐，并讨论通过超参数选择改进我们的模型的方法。
- en: Setting Up the Data
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据设置
- en: We will use a dataset published by Audioscrobbler. Audioscrobbler was the first
    music recommendation system for [Last.fm](http://www.last.fm), one of the first
    internet streaming radio sites, founded in 2002\. Audioscrobbler provided an open
    API for “scrobbling,” or recording listeners’ song plays. Last.fm used this information
    to build a powerful music recommender engine. The system reached millions of users
    because third-party apps and sites could provide listening data back to the recommender
    engine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由 Audioscrobbler 发布的数据集。Audioscrobbler 是 [Last.fm](http://www.last.fm)
    的第一个音乐推荐系统，也是最早的互联网流媒体广播站之一，成立于 2002 年。Audioscrobbler 提供了一个开放的 API 用于“scrobbling”，即记录听众的歌曲播放情况。Last.fm
    利用这些信息构建了一个强大的音乐推荐引擎。由于第三方应用和网站可以将收听数据返回给推荐引擎，该系统达到了数百万用户。
- en: 'At that time, research on recommender engines was mostly confined to learning
    from rating-like data. That is, recommenders were usually viewed as tools that
    operated on input like “Bob rates Prince 3.5 stars.” The Audioscrobbler dataset
    is interesting because it merely records plays: “Bob played a Prince track.” A
    play carries less information than a rating. Just because Bob played the track
    doesn’t mean he actually liked it. You or I may occasionally play a song by an
    artist we don’t care for, or even play an album and walk out of the room.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，关于推荐引擎的研究大多限于从类似评分的数据中学习。也就是说，推荐系统通常被视为在输入数据上运行的工具，如“Bob对Prince的歌曲评级为3.5星”。Audioscrobbler数据集很有趣，因为它仅记录了播放事件：“Bob播放了一首Prince的歌曲”。播放事件比评级事件包含的信息更少。仅因为Bob播放了这首歌，并不意味着他实际上喜欢它。你我可能偶尔会播放一首不喜欢的歌曲，甚至播放整张专辑然后离开房间。
- en: However, listeners rate music far less frequently than they play music. A dataset
    like this is therefore much larger, covers more users and artists, and contains
    more total information than a rating dataset, even if each individual data point
    carries less information. This type of data is often called *implicit feedback*
    data because the user-artist connections are implied as a side effect of other
    actions, and not given as explicit ratings or thumbs-up.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，听众评价音乐的频率远低于播放音乐的频率。因此，这样的数据集要大得多，涵盖的用户和艺术家更多，包含的总信息也更多，即使每个个体数据点的信息量较少。这种类型的数据通常被称为*隐式反馈*数据，因为用户与艺术家的连接是作为其他行为的副作用而暗示的，而不是作为显式评分或赞赏给出的。
- en: A snapshot of a dataset distributed by Last.fm in 2005 can be found [online
    as a compressed archive](https://oreil.ly/Z7sfL). Download the archive, and find
    within it several files. First, the dataset’s files need to be made available.
    If you are using a remote cluster, copy all three data files into storage. This
    chapter will assume that the files are available at *data/*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[在线压缩存档](https://oreil.ly/Z7sfL)中找到Last.fm在2005年发布的数据集快照。下载该存档，并在其中找到几个文件。首先，需要将数据集的文件可用化。如果您使用远程集群，请将所有三个数据文件复制到存储中。本章将假定这些文件在*data/*目录下可用。
- en: 'Start `pyspark-shell`. Note that the computations in this chapter will take
    up more memory than simple applications. If you are running locally rather than
    on a cluster, for example, you will likely need to specify something like `--driver-memory
    4g` to have enough memory to complete these computations. The main dataset is
    in the *user_artist_data.txt* file. It contains about 141,000 unique users, and
    1.6 million unique artists. About 24.2 million users’ plays of artists are recorded,
    along with their counts. Let’s read this dataset into a DataFrame and have a look
    at it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 启动`pyspark-shell`。请注意，本章中的计算将比简单的应用程序占用更多的内存。例如，如果您在本地而不是在集群上运行，可能需要指定类似`--driver-memory
    4g`的参数来确保有足够的内存完成这些计算。主要数据集位于*user_artist_data.txt*文件中。该文件包含约141,000个唯一用户和1.6百万个唯一艺术家。记录了约2420万个用户对艺术家的播放次数及其计数。让我们将这个数据集读入DataFrame并查看一下：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Machine learning tasks like ALS are likely to be more compute-intensive than
    simple text processing. It may be better to break the data into smaller pieces—more
    partitions—for processing. You can chain a call to `.repartition(n)` after reading
    the text file to specify a different and larger number of partitions. You might
    set this higher to match the number of cores in your cluster, for example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像ALS这样的机器学习任务可能比简单的文本处理更需要计算资源。最好将数据分成更小的片段，即更多的分区进行处理。您可以在读取文本文件后链式调用`.repartition(n)`来指定不同且更大的分区数。例如，可以将此值设置为与集群中的核心数相匹配。
- en: 'The dataset also gives the names of each artist by ID in the *artist_data.txt*
    file. Note that when plays are scrobbled, the client application submits the name
    of the artist being played. This name could be misspelled or nonstandard, and
    this may only be detected later. For example, “The Smiths,” “Smiths, The,” and
    “the smiths” may appear as distinct artist IDs in the dataset even though they
    are plainly the same artist. So, the dataset also includes *artist_alias.txt*,
    which maps artist IDs that are known misspellings or variants to the canonical
    ID of that artist. Let’s read these two datasets into PySpark too:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还在*artist_data.txt*文件中按ID提供了每位艺术家的名称。请注意，在提交播放时，客户端应用程序会提交正在播放的艺术家的名称。这个名称可能拼写错误或非标准，这可能在稍后才会被发现。例如，“The
    Smiths”，“Smiths, The”和“the smiths”可能会作为数据集中不同的艺术家ID出现，尽管它们实际上是同一位艺术家。因此，数据集还包括*artist_alias.txt*，它将已知的拼写错误或变体的艺术家ID映射到该艺术家的规范ID上。让我们也将这两个数据集读入PySpark：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have a basic understanding of the datasets, we can discuss our requirements
    for a recommender algorithm and, subsequently, understand why the Alternating
    Least Squares algorithm is a good choice.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据集有了基本的理解，我们可以讨论我们对推荐算法的需求，并且随后理解为什么交替最小二乘算法是一个好选择。
- en: Our Requirements for a Recommender System
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们对推荐系统的需求
- en: 'We need to choose a recommender algorithm that is suitable for our data. Here
    are our considerations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择一个适合我们数据的推荐算法。以下是我们的考虑：
- en: Implicit feedback
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式反馈
- en: The data is comprised entirely of interactions between users and artists’ songs.
    It contains no information about the users or about the artists other than their
    names. We need an algorithm that learns without access to user or artist attributes.
    These are typically called collaborative filtering algorithms. For example, deciding
    that two users might share similar tastes because they are the same age *is not*
    an example of collaborative filtering. Deciding that two users might both like
    the same song because they play many other songs that are the same *is* an example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据完全由用户和艺术家歌曲之间的互动组成。除了它们的名称，它没有任何关于用户或艺术家的信息。我们需要一种能够在没有用户或艺术家属性访问权限的情况下学习的算法。这些通常被称为协同过滤算法。例如，决定两个用户可能有相似品味是因为他们同岁*不是*协同过滤的例子。决定两个用户可能喜欢同一首歌是因为他们播放了许多其他相同的歌*是*协同过滤的例子。
- en: Sparsity
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性
- en: Our dataset looks large because it contains tens of millions of play counts.
    But in a different sense, it is small and skimpy, because it is sparse. On average,
    each user has played songs from about 171 artists—out of 1.6 million. Some users
    have listened to only one artist. We need an algorithm that can provide decent
    recommendations to even these users. After all, every single listener must have
    started with just one play at some point!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集看起来很大，因为它包含数千万次播放。但从另一个角度来看，它是稀疏的和不足的，因为它是稀疏的。平均而言，每个用户只听了大约171位艺术家的歌曲
    —— 而这些艺术家共有160万。有些用户只听过一个艺术家的歌曲。我们需要一种算法，即使对这些用户也能提供良好的推荐。毕竟，每个单独的听众最初肯定是从一个播放开始的！
- en: Scalability and real-time predictions
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和实时预测
- en: Finally, we need an algorithm that scales, both in its ability to build large
    models and to create recommendations quickly. Recommendations are typically required
    in near real time—within a second, not tomorrow.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个能够扩展的算法，无论是建立大型模型还是快速生成推荐。推荐通常需要在几乎实时内提供 —— 在一秒钟内，而不是明天。
- en: A broad class of algorithms that may be suitable is latent factor models. They
    try to explain *observed interactions* between large numbers of users and items
    through a relatively small number of *unobserved, underlying reasons*. For example,
    consider a customer who has bought albums by metal bands Megadeth and Pantera
    but also classical composer Mozart. It may be difficult to explain why exactly
    these albums were bought and nothing else. However, it’s probably a small window
    on a much larger set of tastes. Maybe the customer likes a coherent spectrum of
    music from metal to progressive rock to classical. That explanation is simpler
    and, as a bonus, suggests many other albums that would be of interest. In this
    example, “liking metal, progressive rock, and classical” are three latent factors
    that could explain tens of thousands of individual album preferences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一类可能适合的算法是潜在因子模型。它们试图通过相对较少数量的*未观察到的潜在原因*来解释大量用户和项目之间的*观察到的互动*。例如，考虑一个客户购买了金属乐队Megadeth和Pantera的专辑，但也购买了古典作曲家莫扎特的专辑。可能很难解释为什么只有这些专辑被购买而没有其他的。然而，这可能只是更大音乐品味集合中的一个小窗口。也许这个客户喜欢从金属到前卫摇滚再到古典的连贯音乐光谱。这种解释更简单，并且额外地建议了许多其他可能感兴趣的专辑。在这个例子中，“喜欢金属、前卫摇滚和古典音乐”是三个潜在因子，可以解释成千上万个个别专辑的偏好。
- en: 'In our case, we will specifically use a type of matrix factorization model.
    Mathematically, these algorithms treat the user and product data as if it were
    a large matrix *A*, where the entry at row *i* and column *j* exists if user *i*
    has played artist *j*. *A* is sparse: most entries of *A* are 0, because only
    a few of all possible user-artist combinations actually appear in the data. They
    factor *A* as the matrix product of two smaller matrices, *X* and *Y*. They are
    very skinny—both have many rows because *A* has many rows and columns, but both
    have just a few columns (*k*). The *k* columns correspond to the latent factors
    that are being used to explain the interaction data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将专门使用一种类型的矩阵因子分解模型。从数学上讲，这些算法将用户和产品数据视为一个大矩阵 *A*，如果用户 *i* 玩了艺术家 *j*，则在第
    *i* 行和第 *j* 列的条目存在。*A* 是稀疏的：大多数 *A* 的条目为0，因为实际数据中只有少数可能的用户-艺术家组合出现。它们将 *A* 分解为两个较小矩阵
    *X* 和 *Y* 的矩阵乘积。它们非常瘦长—都有很多行，因为 *A* 有很多行和列，但是两者都只有少数列（*k*）。*k* 列对应用于解释交互数据的潜在因子。
- en: The factorization can only be approximate because *k* is small, as shown in
    [Figure 3-1](#ALSFactorization).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*k*较小，因此因子分解只能是近似的，如[图 3-1](#ALSFactorization)所示。
- en: '![aaps 0301](assets/aaps_0301.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0301](assets/aaps_0301.png)'
- en: Figure 3-1\. Matrix factorization
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 矩阵因子分解
- en: These algorithms are sometimes called matrix completion algorithms, because
    the original matrix *A* may be quite sparse, but the product *XY*^(*T*) is dense.
    Very few, if any, entries are 0, and therefore the model is only an approximation
    of *A*. It is a model in the sense that it produces (“completes”) a value for
    even the many entries that are missing (that is, 0) in the original *A*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法有时被称为矩阵补全算法，因为原始矩阵 *A* 可能非常稀疏，但乘积 *XY*^(*T*) 是密集的。很少或没有条目为0，因此模型只是 *A* 的近似。从模型的角度来看，它产生（“补全”）了即使在原始
    *A* 中缺少的（即0的）条目的值。
- en: This is a case where, happily, the linear algebra maps directly and elegantly
    to intuition. These two matrices contain a row for each user and each artist.
    The rows have few values—*k*. Each value corresponds to a latent feature in the
    model. So the rows express how much users and artists associate with these *k*
    latent features, which might correspond to tastes or genres. And it is simply
    the product of a user-feature and feature-artist matrix that yields a complete
    estimation of the entire, dense user-artist interaction matrix. This product might
    be thought of as mapping items to their attributes and then weighting those by
    user attributes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子，令人高兴的是，线性代数直接而优雅地映射到直觉中。这两个矩阵包含每个用户和每个艺术家的一行。这些行具有很少的值—*k*。每个值对应模型中的一个潜在特征。因此，这些行表达了用户和艺术家与这些*k*个潜在特征的关联程度，这些特征可能对应于品味或流派。简单地说，用户特征和特征-艺术家矩阵的乘积产生了整个稠密的用户-艺术家互动矩阵的完整估计。可以将这个乘积视为将项目映射到它们的属性，然后按用户属性加权的过程。
- en: 'The bad news is that *A* = *XY*^(*T*) generally has no exact solution at all,
    because *X* and *Y* aren’t large enough (technically speaking, too low [rank](https://oreil.ly/OfVj4))
    to perfectly represent *A*. This is actually a good thing. *A* is just a tiny
    sample of all interactions that *could* happen. In a way, we believe *A* is a
    terribly spotty and therefore hard-to-explain view of a simpler underlying reality
    that is well explained by just some small number of factors, *k*, of them. Think
    of a jigsaw puzzle depicting a cat. The final puzzle is simple to describe: a
    cat. When you’re holding just a few pieces, however, the picture you see is quite
    difficult to describe.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，通常情况下 *A* = *XY*^(*T*) 没有确切的解，因为 *X* 和 *Y* 不够大（严格来说是太低的[rank](https://oreil.ly/OfVj4)）以完美地表示
    *A*。这实际上是件好事。*A* 只是所有可能互动的一个微小样本。在某种程度上，我们认为 *A* 是一个极其零星且因此难以解释的简单潜在现实的视角，只需要几个因子
    *k* 就能很好地解释。想象一下描绘猫的拼图。最终的拼图描述很简单：一只猫。然而，当你手里只有几片时，你看到的图片却很难描述。
- en: '*XY*^(*T*) should still be as close to *A* as possible. After all, it’s all
    we’ve got to go on. It will not and should not reproduce it exactly. The bad news
    again is that this can’t be solved directly for both the best *X* and best *Y*
    at the same time. The good news is that it’s trivial to solve for the best *X*
    if *Y* is known, and vice versa. But neither is known beforehand!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*XY*^(*T*) 应尽可能接近 *A*。毕竟，这是我们唯一的线索。它不会也不应该完全复制它。再次的坏消息是，不能直接求解得到最佳的 *X* 和 *Y*。好消息是，如果
    *Y* 已知，那么求解最佳的 *X* 是微不足道的，反之亦然。但是事先都不知道！'
- en: Fortunately, there are algorithms that can escape this catch-22 and find a decent
    solution. One such algorithm that’s available in PySpark is the ALS algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有些算法可以摆脱这种困境并找到一个合理的解决方案。PySpark中提供的一种算法就是ALS算法。
- en: Alternating Least Squares Algorithm
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交替最小二乘算法
- en: We will use the Alternating Least Squares algorithm to compute latent factors
    from our dataset. This type of approach was popularized around the time of the
    Netflix Prize competition by papers like [“Collaborative Filtering for Implicit
    Feedback Datasets”](https://oreil.ly/3pSzk) and [“Large-Scale Parallel Collaborative
    Filtering for the Netflix Prize”](https://oreil.ly/LULpp). PySpark MLlib’s ALS
    implementation draws on ideas from both of these papers and is the only recommender
    algorithm currently implemented in Spark MLlib.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交替最小二乘算法从我们的数据集中计算潜在因子。这种方法在Netflix Prize竞赛时期被如《“面向隐性反馈数据集的协同过滤”》和《“Netflix
    Prize的大规模并行协同过滤”》等论文广泛应用。PySpark MLlib的ALS实现借鉴了这些论文的思想，并且是目前Spark MLlib中唯一实现的推荐算法。
- en: 'Here’s a code snippet (non-functional) to give you a peek at what lies ahead
    in the chapter:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个代码片段（非功能性），让你一窥后面章节的内容：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With ALS, we will treat our input data as a large, sparse matrix *A*, and find
    out *X* and *Y*, as discussed in previous section. At the start, *Y* isn’t known,
    but it can be initialized to a matrix full of randomly chosen row vectors. Then
    simple linear algebra gives the best solution for *X*, given *A* and *Y*. In fact,
    it’s trivial to compute each row *i* of *X* separately as a function of *Y* and
    of one row of *A*. Because it can be done separately, it can be done in parallel,
    and that is an excellent property for large-scale computation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ALS，我们将把输入数据视为一个大型稀疏矩阵*A*，并找出*X*和*Y*，如前面讨论的那样。起初，*Y*是未知的，但可以初始化为一个填满随机选定行向量的矩阵。然后，简单的线性代数给出了在给定*A*和*Y*的情况下最佳解决方案的方法。事实上，可以分别计算*X*的每一行*i*作为*Y*和*A*的一行的函数。因为可以分别完成，所以可以并行进行，这对大规模计算是一个极好的特性：
- en: <math display="block"><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>Y</mi><mo>(</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mo>)</mo><mi>–1</mi></msup>
    <mo>=</mo> <msub><mi>X</mi><mi>i</mi></msub></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>Y</mi><mo>(</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mo>)</mo><mi>–1</mi></msup>
    <mo>=</mo> <msub><mi>X</mi><mi>i</mi></msub></mrow></math>
- en: Equality can’t be achieved exactly, so in fact the goal is to minimize |*A*[*i*]*Y*(*Y*^(*T*)*Y*)^(*–1*)
    – *X*[*i*]|, or the sum of squared differences between the two matrices’ entries.
    This is where the “least squares” in the name comes from. In practice, this is
    never solved by actually computing inverses, but faster and more directly via
    methods like the QR decomposition. This equation simply elaborates on the theory
    of how the row vector is computed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无法完全实现等式，因此实际上目标是最小化|*A*[*i*]*Y*(*Y*^(*T*)*Y*)^(*–1*) – *X*[*i*]|，或两个矩阵条目之间平方差的总和。这就是名称中“最小二乘”的含义。实际上，这永远不会通过计算逆解决，而是更快、更直接地通过QR分解等方法。该方程简单地阐述了计算行向量的理论。
- en: 'The same thing can be done to compute each *Y*[*j*] from *X*. And again, to
    compute *X* from *Y*, and so on. This is where the “alternating” part comes from.
    There’s just one small problem: *Y* was made up—and random! *X* was computed optimally,
    yes, but gives a bogus solution for *Y*. Fortunately, if this process is repeated,
    *X* and *Y* do eventually converge to decent solutions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用相同的方法计算每个*Y*[*j*]从*X*中得到。再次，从*Y*计算*X*，依此类推。这就是“交替”的来源。这里只有一个小问题：*Y*是虚构的——随机的！*X*计算得最优，没错，但对*Y*给出了一个虚假的解。幸运的是，如果这个过程重复进行，*X*和*Y*最终会收敛到合理的解决方案。
- en: When used to factor a matrix representing implicit data, there is a little more
    complexity to the ALS factorization. It is not factoring the input matrix *A*
    directly, but a matrix *P* of 0s and 1s, containing 1 where *A* contains a positive
    value and 0 elsewhere. The values in *A* are incorporated later as weights. This
    detail is beyond the scope of this book but is not necessary to understand how
    to use the algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当用于分解表示隐式数据的矩阵时，ALS分解会更加复杂。它不是直接分解输入矩阵*A*，而是一个包含0和1的矩阵*P*，其中*A*包含正值的地方为1，其他地方为0。后续会将*A*中的值作为权重加入。这些细节超出了本书的范围，但并不影响理解如何使用该算法。
- en: Finally, the ALS algorithm can take advantage of the sparsity of the input data
    as well. This, and its reliance on simple, optimized linear algebra and its data-parallel
    nature, make it very fast at large scale.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，ALS 算法还可以利用输入数据的稀疏性。这一点，以及它依赖简单优化的线性代数和数据并行性质，使得它在大规模上非常快速。
- en: Next, we will preprocess our dataset and make it suitable for use with the ALS
    algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预处理我们的数据集，使其适合与ALS算法一起使用。
- en: Preparing the Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The first step in building a model is to understand the data that is available
    and parse or transform it into forms that are useful for analysis in Spark.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的第一步是了解可用的数据，并将其解析或转换为在Spark中进行分析时有用的形式。
- en: Spark MLlib’s ALS implementation does not strictly require numeric IDs for users
    and items, but is more efficient when the IDs are in fact representable as 32-bit
    integers. That is the case because under the hood the data is being represented
    using the JVM’s data type. Does this dataset conform to this requirement already?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib的ALS实现在用户和项目的ID不需要严格为数字时也可以使用，但当ID确实可表示为32位整数时更有效。这是因为在底层数据使用JVM的数据类型表示。这个数据集已经符合这个要求了吗？
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each line of the file contains a user ID, an artist ID, and a play count, separated
    by spaces. To compute statistics on the user ID, we split the line by space characters
    and parse the values as integers. The result is conceptually three “columns”:
    a user ID, artist ID, and count as `int`s. It makes sense to transform this to
    a dataframe with columns named “user”, “artist”, and “count” because it then becomes
    simple to compute simple statistics like the maximum and minimum:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的每一行包含一个用户ID、一个艺术家ID和一个播放计数，由空格分隔。为了对用户ID进行统计，我们通过空格字符拆分行并将值解析为整数。结果在概念上有三个“列”：一个用户ID，一个艺术家ID和一个整数计数。将其转换为具有“user”、“artist”和“count”列的数据框是有意义的，因为这样可以简单地计算最大值和最小值：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The maximum user and artist IDs are 2443548 and 10794401, respectively (and
    their minimums are 90 and 1; no negative values). These are comfortably smaller
    than 2147483647\. No additional transformation will be necessary to use these
    IDs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的用户和艺术家ID分别为2443548和10794401（它们的最小值分别为90和1；没有负值）。这些值远远小于2147483647。在使用这些ID时不需要进行额外的转换。
- en: 'It will be useful later in this example to know the artist names corresponding
    to the opaque numeric IDs. `raw_artist_data` contains the artist ID and name separated
    by a tab. PySpark’s split function accepts regular expression values for the `pattern`
    parameter. We can split using the whitespace character, `\s`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例的后续过程中了解与不透明数字ID对应的艺术家名称将会很有用。`raw_artist_data` 包含以制表符分隔的艺术家ID和名称。PySpark的
    `split` 函数接受 `pattern` 参数的正则表达式值。我们可以使用空白字符 `\s` 进行拆分：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This results in a dataframe with the artist ID and name as columns `id` and
    `name`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个数据框，其艺术家ID和名称作为列 `id` 和 `name`。
- en: '`raw_artist_alias` maps artist IDs that may be misspelled or nonstandard to
    the ID of the artist’s canonical name. This dataset is relatively small, containing
    about 200,000 entries. It contains two IDs per line, separated by a tab. We will
    parse this in a similar manner as we did `raw_artist_data`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`raw_artist_alias`将可能拼写错误或非标准的艺术家ID映射到艺术家规范名称的ID。这个数据集相对较小，大约有20万条目。每行包含两个ID，由制表符分隔。我们将以类似的方式解析这个数据集，就像我们对`raw_artist_data`所做的那样：'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first entry maps ID 1092764 to 1000311\. We can look these up from the
    `artist_by_id` DataFrame:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个条目将ID 1092764映射到1000311。我们可以从 `artist_by_id` DataFrame 中查找这些ID：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This entry evidently maps “Winwood, Steve” to “Steve Winwood,” which is in fact
    the correct name for the artist.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此条目显然将“Winwood, Steve”映射到“Steve Winwood”，这实际上是艺术家的正确名称。
- en: Building a First Model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建第一个模型
- en: 'Although the dataset is in nearly the right form for use with Spark MLlib’s
    ALS implementation, it requires a small, extra transformation. The aliases dataset
    should be applied to convert all artist IDs to a canonical ID, if a different
    canonical ID exists:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据集几乎符合与Spark MLlib的ALS实现一起使用的形式，但需要进行小的额外转换。如果存在不同的规范化ID，则应用别名数据集以将所有艺术家ID转换为规范化ID：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO1-1)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO1-1)'
- en: Get artist’s alias if it exists; otherwise, get original artist.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 获取艺术家的别名（如果存在）；否则，获取原始艺术家。
- en: We `broadcast` the `artist_alias` DataFrame created earlier. This makes Spark
    send and hold in memory just one copy for *each executor* in the cluster. When
    there are thousands of tasks and many execute in parallel on each executor, this
    can save significant network traffic and memory. As a rule of thumb, it’s helpful
    to broadcast a significantly smaller dataset when performing a join with a very
    big dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 `broadcast` 之前创建的 `artist_alias` DataFrame。这使得Spark在集群中每个执行器上发送并保存一个副本。当有成千上万个任务并且许多任务在每个执行器上并行执行时，这可以节省大量的网络流量和内存。作为经验法则，在与一个非常大的数据集进行连接时，广播一个明显较小的数据集是有帮助的。
- en: The call to `cache` suggests to Spark that this DataFrame should be temporarily
    stored after being computed and, furthermore, kept in memory in the cluster. This
    is helpful because the ALS algorithm is iterative and will typically need to access
    this data 10 times or more. Without this, the DataFrame could be repeatedly recomputed
    from the original data each time it is accessed! The Storage tab in the Spark
    UI will show how much of the DataFrame is cached and how much memory it uses,
    as shown in [Figure 3-2](#Recommender_StorageTag). This one consumes about 120
    MB across the cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`cache`表明向Spark建议，在计算后，应该临时存储这个DataFrame，并且在集群中保留在内存中。这很有帮助，因为ALS算法是迭代的，通常需要多次访问这些数据。如果没有这个，每次访问时DataFrame都可能会从原始数据中重新计算！Spark
    UI中的Storage标签将显示DataFrame的缓存量和内存使用情况，如[Figure 3-2](#Recommender_StorageTag)所示。这个DataFrame在整个集群中消耗约120
    MB。
- en: '![aaps 0302](assets/aaps_0302.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0302](assets/aaps_0302.png)'
- en: Figure 3-2\. Storage tab in the Spark UI, showing cached DataFrame memory usage
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. Spark UI中的Storage标签，显示了缓存的DataFrame内存使用情况
- en: When you use `cache` or `persist`, the DataFrame is not fully cached until you
    trigger an action that goes through every record (e.g., `count`). If you use an
    action like `show(1)`, only one partition will be cached. That is because PySpark’s
    optimizer will figure out that you do not need to compute all the partitions just
    to retrieve one record.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用`cache`或`persist`时，DataFrame在触发遍历每条记录的操作（例如`count`）之前并不完全缓存。如果您使用`show(1)`这样的操作，只有一个分区会被缓存。这是因为PySpark的优化器会发现您并不需要计算所有分区才能检索一条记录。
- en: Note that the label “Deserialized” in the UI in [Figure 3-2](#Recommender_StorageTag)
    is actually only relevant for RDDs, where “Serialized” means data is stored in
    memory, not as objects, but as serialized bytes. However, DataFrame instances
    like this one perform their own “encoding” of common data types in memory separately.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，UI中的“Deserialized”标签在[Figure 3-2](#Recommender_StorageTag)中仅与RDD相关，其中“Serialized”意味着数据存储在内存中，而不是作为对象，而是作为序列化字节。然而，像这样的DataFrame实例会单独在内存中“编码”常见数据类型。
- en: Actually, 120 MB is surprisingly small. Given that there are about 24 million
    plays stored here, a quick back-of-the-envelope calculation suggests that this
    would mean that each user-artist-count entry consumes only 5 bytes on average.
    However, the three 32-bit integers alone ought to consume 12 bytes. This is one
    of the advantages of a DataFrame. Because the types of data stored are primitive
    32-bit integers, their representation can be optimized in memory internally.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，120 MB大小令人惊讶地小。考虑到这里存储了约2400万次播放，一个快速的估算计算表明，每个用户-艺术家-计数条目平均消耗仅5个字节。然而，仅三个32位整数就应该消耗12个字节。这是DataFrame的一个优点之一。因为存储的数据类型是基本的32位整数，它们的内存表示可以在内部进行优化。
- en: 'Finally, we can build a model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以构建一个模型：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This constructs `model` as an `ALSModel` with some default configuration. The
    operation will likely take several minutes or more depending on your cluster.
    Compared to some machine learning models, whose final form may consist of just
    a few parameters or coefficients, this type of model is huge. It contains a feature
    vector of 10 values for each user and product in the model, and in this case there
    are more than 1.7 million of them. The model contains these large user-feature
    and product-feature matrices as DataFrames of their own.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认配置将`model`构建为一个`ALSModel`。操作可能需要几分钟或更长时间，具体取决于您的集群。与某些机器学习模型相比，这种类型的模型非常庞大。模型中为每个用户和产品包含一个包含10个值的特征向量，在这种情况下，超过170万个特征向量。模型包含这些大型用户特征和产品特征矩阵，它们作为自己的DataFrame存在。
- en: The values in your results may be somewhat different. The final model depends
    on a randomly chosen initial set of feature vectors. The default behavior of this
    and other components in MLlib, however, is to use the same set of random choices
    every time by defaulting to a fixed seed. This is unlike other libraries, where
    behavior of random elements is typically not fixed by default. So, here and elsewhere,
    a random seed is set with `(… seed=0,…)`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果中的值可能略有不同。最终模型取决于随机选择的初始特征向量集。然而，MLlib中这些组件的默认行为是通过默认的固定种子使用相同的随机选择集。这与其他库不同，其他库中随机元素的行为通常默认不是固定的。因此，在这里和其他地方，随机种子设置为`(…
    seed=0, …)`。
- en: 'To see some feature vectors, try the following, which displays just one row
    and does not truncate the wide display of the feature vector:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看一些特征向量，请尝试以下操作，显示仅一行，不截断特征向量的宽显示：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The other methods invoked on `ALS`, like `setAlpha`, set *hyperparameters*
    whose values can affect the quality of the recommendations that the model makes.
    These will be explained later. The more important first question is this: is the
    model any good? Does it produce good recommendations? That is what we will try
    to answer in the next section.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ALS`上调用的其他方法，如`setAlpha`，设置了*超参数*，其值可能会影响模型做出的推荐的质量。稍后将对此进行解释。更重要的第一个问题是：模型好不好？它是否能产生好的推荐？这是我们将在下一节中尝试回答的问题。
- en: Spot Checking Recommendations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点建议
- en: 'We should first see if the artist recommendations make any intuitive sense,
    by examining a user, plays, and recommendations for that user. Take, for example,
    user 2093760\. First, let’s look at his or her plays to get a sense of the person’s
    tastes. Extract the IDs of artists that this user has listened to and print their
    names. This means searching the input for artist IDs played by this user and then
    filtering the set of artists by these IDs to print the names in order:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该看看艺术家的推荐是否有直观的意义，方法是检查一个用户、播放和该用户的推荐。以用户2093760为例，首先让我们看看他或她的播放记录，以了解其品味。提取该用户听过的艺术家ID，并打印他们的名称。这意味着搜索由该用户播放的艺术家ID的输入，然后通过这些ID筛选艺术家集以按顺序打印名称：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-1)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-1)'
- en: Find lines whose user is 2093760.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查找其用户为2093760的行。
- en: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-2)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-2)'
- en: Collect dataset of artist ID.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 收集艺术家ID的数据集。
- en: '[![3](assets/3.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-3)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-3)'
- en: Filter in those artists.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤这些艺术家。
- en: The artists look like a mix of mainstream pop and hip-hop. A Jurassic 5 fan?
    Remember, it’s 2005. In case you’re wondering, the Saw Doctors is a very Irish
    rock band popular in Ireland.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些艺术家看起来是主流流行音乐和嘻哈的混合体。一位 Jurassic 5 的粉丝？请记住，现在是2005年。如果你在想，Saw Doctors是一支非常受爱尔兰人欢迎的爱尔兰摇滚乐队。
- en: 'Now, it’s simple to make recommendations for a user, though computing them
    this way will take a few moments. It’s suitable for batch scoring but not real-time
    use cases:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于用户进行推荐很简单，尽管通过这种方式计算需要一些时间。适用于批量评分，但不适用于实时用例：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The resulting recommendations contain lists comprised of artist ID and, of course,
    “predictions.” For this type of ALS algorithm, the prediction is an opaque value
    normally between 0 and 1, where higher values mean a better recommendation. It
    is not a probability but can be thought of as an estimate of a 0/1 value indicating
    whether the user won’t or will interact with the artist, respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果推荐包含由艺术家ID组成的列表和当然的“预测”。对于这种ALS算法，预测是一个通常在0到1之间的不透明值，较高的值意味着更好的推荐。它不是概率，但可以被视为估计一个0/1值，指示用户是否会与艺术家互动。
- en: 'After extracting the artist IDs for the recommendations, we can look up artist
    names in a similar way:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 提取推荐的艺术家ID后，我们可以类似地查找艺术家的名称：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result is all hip-hop. This doesn’t look like a great set of recommendations,
    at first glance. While these are generally popular artists, they don’t appear
    to be personalized to this user’s listening habits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果全是嘻哈。乍一看，这并不像是一个很好的推荐集。虽然这些通常是受欢迎的艺术家，但它们似乎并不符合这位用户的听歌习惯。
- en: Evaluating Recommendation Quality
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估推荐质量
- en: Of course, that’s just one subjective judgment about one user’s results. It’s
    hard for anyone but that user to quantify how good the recommendations are. Moreover,
    it’s infeasible to have any human manually score even a small sample of the output
    to evaluate the results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是对一个用户结果的主观判断。对于除该用户外的任何人来说，很难量化推荐的好坏。此外，即使对少量输出进行人工评分也是不可行的。
- en: It’s reasonable to assume that users tend to play songs from artists who are
    appealing, and not play songs from artists who aren’t appealing. So, the plays
    for a user give a partial picture of “good” and “bad” artist recommendations.
    This is a problematic assumption but about the best that can be done without any
    other data. For example, presumably user 2093760 likes many more artists than
    the 5 listed previously, and of the 1.7 million other artists not played, a few
    are of interest, and not all are “bad” recommendations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 合理地假设用户倾向于播放吸引人的艺术家的歌曲，而不播放不吸引人的艺术家的歌曲是合理的。因此，用户的播放行为为用户推荐的“好”和“坏”艺术家提供了部分信息。这是一个问题性的假设，但是在没有其他数据的情况下，这是最好的选择。例如，据推测用户2093760喜欢的艺术家远不止5位，而170万其他未播放的艺术家中，有一些是有趣的，而不是所有的推荐都是“坏”的。
- en: What if a recommender were evaluated on its ability to rank good artists high
    in a list of recommendations? This is one of several generic metrics that can
    be applied to a system that ranks things, like a recommender. The problem is that
    “good” is defined as “artists the user has listened to,” and the recommender system
    has already received all of this information as input. It could trivially return
    the user’s previously listened-to artists as top recommendations and score perfectly.
    But this is not useful, especially because the recommender’s role is to recommend
    artists that the user has never listened to.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个推荐系统被评估其在推荐列表中将好的艺术家排名高的能力，会怎样？这是可应用于像推荐系统这样的排名系统的几个通用度量标准之一。问题在于，“好”被定义为“用户曾经听过的艺术家”，而推荐系统已经将所有这些信息作为输入接收。它可以简单地返回用户之前听过的艺术家作为顶级推荐并获得完美分数。但这并不实用，特别是因为推荐系统的角色是推荐用户以前未曾听过的艺术家。
- en: To make this meaningful, some of the artist play data can be set aside and hidden
    from the ALS model-building process. Then, this held-out data can be interpreted
    as a collection of good recommendations for each user but one that the recommender
    has not already been given. The recommender is asked to rank all items in the
    model, and the ranks of the held-out artists are examined. Ideally, the recommender
    places all of them at or near the top of the list.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其具有意义，可以将部分艺术家播放数据保留并隐藏在ALS模型构建过程之外。然后，可以将这些保留的数据解释为每个用户的一组好的推荐，但是推荐系统尚未收到这些推荐。要求推荐系统对模型中的所有项目进行排名，并检查保留艺术家的排名。理想情况下，推荐系统应将它们全部或几乎全部排在列表的顶部。
- en: We can then compute the recommender’s score by comparing all held-out artists’
    ranks to the rest. (In practice, we compute this by examining only a sample of
    all such pairs, because a potentially huge number of such pairs may exist.) The
    fraction of pairs where the held-out artist is ranked higher is its score. A score
    of 1.0 is perfect, 0.0 is the worst possible score, and 0.5 is the expected value
    achieved from randomly ranking artists.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过比较所有保留艺术家的排名与其余排名来计算推荐系统的得分。（在实践中，我们只检查所有这些对中的一个样本，因为可能存在大量这样的对。）在保留艺术家排名较高的对中的比例就是其得分。得分为1.0表示完美，0.0是最差的分数，0.5是从随机排名艺术家中获得的期望值。
- en: This metric is directly related to an information retrieval concept called the
    [receiver operating characteristic (ROC) curve](https://oreil.ly/Pt2bn). The metric
    in the preceding paragraph equals the area under this ROC curve and is indeed
    known as AUC, or area under the curve. AUC may be viewed as the probability that
    a randomly chosen good recommendation ranks above a randomly chosen bad recommendation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量指标直接与信息检索概念“接收者操作特征曲线（ROC曲线）”（[oreil.ly/Pt2bn](https://oreil.ly/Pt2bn)）相关联。上一段中的度量值等于该ROC曲线下的面积，通常称为AUC，即曲线下面积。AUC可以看作是随机选择一个好推荐高于随机选择一个坏推荐的概率。
- en: The AUC metric is also used in the evaluation of classifiers. It is implemented,
    along with related methods, in the MLlib class `BinaryClassificationMetrics`.
    For recommenders, we will compute AUC *per user* and average the result. The resulting
    metric is slightly different and might be called “mean AUC.” We will implement
    this, because it is not (quite) implemented in PySpark.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AUC度量标准还用于分类器的评估。它与相关方法一起实现在MLlib类`BinaryClassificationMetrics`中。对于推荐系统，我们将计算每个用户的AUC并对结果进行平均。得到的度量标准略有不同，可能称为“平均AUC”。我们将实现这一点，因为它在PySpark中尚未（完全）实现。
- en: Other evaluation metrics that are relevant to systems that rank things are implemented
    in `RankingMetrics`. These include metrics like precision, recall, and [mean average
    precision (MAP)](https://oreil.ly/obbTT). MAP is also frequently used and focuses
    more narrowly on the quality of the top recommendations. However, AUC will be
    used here as a common and broad measure of the quality of the entire model output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与排名系统相关的其他评估指标在`RankingMetrics`中实现。这些包括精确度、召回率和[平均精度（MAP）](https://oreil.ly/obbTT)。MAP也经常被使用，更专注于顶部推荐的质量。然而，在这里，AUC将作为衡量整个模型输出质量的常见和广泛指标。
- en: 'In fact, the process of holding out some data to select a model and evaluate
    its accuracy is common practice in all of machine learning. Typically, data is
    divided into three subsets: training, cross-validation (CV), and test sets. For
    simplicity in this initial example, only two sets will be used: training and CV.
    This will be sufficient to choose a model. In [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    this idea will be extended to include the test set.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，事实上，将一些数据保留用于选择模型并评估其准确性的过程是常见做法。通常，数据被分成三个子集：训练集、交叉验证（CV）集和测试集。在这个初始示例中，为简单起见，只使用两个集合：训练集和CV集。这足以选择一个模型。在[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)中，这个想法将被扩展到包括测试集。
- en: Computing AUC
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算AUC
- en: An implementation of mean AUC is provided in the source code accompanying this
    book. It is not reproduced here, but is explained in some detail in comments in
    the source code. It accepts the CV set as the “positive” or “good” artists for
    each user and a prediction function. This function translates a dataframe containing
    each user-artist pair into a dataframe that also contains its estimated strength
    of interaction as a “prediction,” a number wherein higher values mean higher rank
    in the recommendations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随本书的源代码提供了平均AUC的实现。这里不会重复，但在源代码的注释中有详细说明。它接受CV集作为每个用户的“正面”或“好”的艺术家，并且一个预测函数。这个函数将包含每个用户-艺术家对的数据帧转换为一个数据帧，也包含其预估的互动强度作为“预测”，其中更高的值意味着在推荐中更高的排名。
- en: 'To use the input data, we must split it into a training set and a CV set. The
    ALS model will be trained on the training dataset only, and the CV set will be
    used to evaluate the model. Here, 90% of the data is used for training and the
    remaining 10% for cross-validation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用输入数据，我们必须将其分成训练集和CV集。ALS模型将仅在训练数据集上进行训练，CV集将用于评估模型。这里，90%的数据用于训练，剩余的10%用于交叉验证：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that `areaUnderCurve` accepts a function as its third argument. Here, the
    `transform` method from `ALSModel` is passed in, but it will shortly be swapped
    out for an alternative.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`areaUnderCurve`接受一个函数作为其第三个参数。在这里，从`ALSModel`中传入的`transform`方法被传递进去，但很快会被另一种方法替代。
- en: The result is about 0.879\. Is this good? It is certainly higher than the 0.5
    that is expected from making recommendations randomly, and it’s close to 1.0,
    which is the maximum possible score. Generally, an AUC over 0.9 would be considered
    high.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果约为0.879。这好吗？它显然高于从随机推荐中预期的0.5，接近1.0，这是可能得分的最大值。通常，AUC超过0.9被认为是高的。
- en: But is it an accurate evaluation? This evaluation could be repeated with a different
    90% as the training set. The resulting AUC values’ average might be a better estimate
    of the algorithm’s performance on the dataset. In fact, one common practice is
    to divide the data into *k* subsets of similar size, use *k* – 1 subsets together
    for training, and evaluate on the remaining subset. We can repeat this *k* times,
    using a different set of subsets each time. This is called [*k-fold cross-validation*](https://oreil.ly/DolrQ).
    This won’t be implemented in examples here, for simplicity, but some support for
    this technique exists in MLlib in its `CrossValidator` API. The validation API
    will be revisited in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个准确的评估吗？可以将这个评估用不同的90%作为训练集重复。得到的AUC值的平均可能会更好地估计算法在数据集上的性能。事实上，一个常见的做法是将数据分成*k*个大小相似的子集，使用*k*-1个子集进行训练，并在剩余的子集上进行评估。我们可以重复这个过程*k*次，每次使用不同的子集。这被称为[*k*-折交叉验证](https://oreil.ly/DolrQ)。为了简单起见，在这些示例中不会实施这种技术，但MLlib中的`CrossValidator`
    API支持这种技术。验证API将在[“随机森林”](ch04.xhtml#RandomDecisionForests)中再次讨论。
- en: 'It’s helpful to benchmark this against a simpler approach. For example, consider
    recommending the globally most-played artists to every user. This is not personalized,
    but it is simple and may be effective. Define this simple prediction function
    and evaluate its AUC score:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与更简单的方法进行基准测试是有帮助的。例如，考虑向每个用户推荐全球播放次数最多的艺术家。这不是个性化的，但简单且可能有效。定义这个简单的预测函数，并评估其AUC分数：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result is also about 0.880\. This suggests that nonpersonalized recommendations
    are already fairly effective according to this metric. However, we’d expect the
    “personalized” recommendations to score better in comparison. Clearly, the model
    needs some tuning. Can it be made better?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果也约为0.880\. 这表明根据这个度量标准，非个性化推荐已经相当有效。然而，我们预期“个性化”的推荐在比较中得分更高。显然，模型需要一些调整。它能做得更好吗？
- en: Hyperparameter Selection
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数选择
- en: 'So far, the hyperparameter values used to build the `ALSModel` were simply
    given without comment. They are not learned by the algorithm and must be chosen
    by the caller. The configured hyperparameters were:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，用于构建`ALSModel`的超参数值仅仅是给出的，没有评论。这些参数不会被算法学习，必须由调用者选择。配置的超参数包括：
- en: '`setRank(10)`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`setRank(10)`'
- en: The number of latent factors in the model, or equivalently, the number of columns
    *k* in the user-feature and product-feature matrices. In nontrivial cases, this
    is also their rank.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中潜在因子的数量，或者等价地说，用户特征矩阵和产品特征矩阵中的列数*k*。在非平凡情况下，这也是它们的秩。
- en: '`setMaxIter(5)`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`setMaxIter(5)`'
- en: The number of iterations that the factorization runs. More iterations take more
    time but may produce a better factorization.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分解运行的迭代次数。更多的迭代需要更多的时间，但可能会产生更好的因子分解。
- en: '`setRegParam(0.01)`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`setRegParam(0.01)`'
- en: A standard overfitting parameter, also usually called *lambda*. Higher values
    resist overfitting, but values that are too high hurt the factorization’s accuracy.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的过拟合参数，通常也称为*lambda*。更高的值抵抗过拟合，但值太高会损害因子分解的准确性。
- en: '`setAlpha(1.0)`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`setAlpha(1.0)`'
- en: Controls the relative weight of observed versus unobserved user-product interactions
    in the factorization.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 控制因子分解中观察到的与未观察到的用户-产品交互的相对权重。
- en: '`rank`, `regParam`, and `alpha` can be considered *hyperparameters* to the
    model. (`maxIter` is more of a constraint on resources used in the factorization.)
    These are not values that end up in the matrices inside the `ALSModel`—those are
    simply its *parameters* and are chosen by the algorithm. These hyperparameters
    are instead parameters to the process of building itself.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`rank`、`regParam`和`alpha`可以被视为模型的*超参数*。（`maxIter`更多地是对因子分解中使用的资源的限制。）这些不是最终出现在`ALSModel`内部矩阵中的值，那些只是其*参数*，由算法选择。这些超参数实际上是构建过程的参数。'
- en: The values used in the preceding list are not necessarily optimal. Choosing
    good hyperparameter values is a common problem in machine learning. The most basic
    way to choose values is to simply try combinations of values and evaluate a metric
    for each of them, and choose the combination that produces the best value of the
    metric.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前述列表中使用的值不一定是最优的。选择好的超参数值是机器学习中常见的问题。选择值的最基本方法是简单地尝试不同的组合，并评估每个组合的度量标准，并选择产生最佳度量值的组合。
- en: 'In the following example, eight possible combinations are tried: `rank` = 5
    or 30, `regParam` = 4.0 or 0.0001, and `alpha` = 1.0 or 40.0\. These values are
    still something of a guess, but are chosen to cover a broad range of parameter
    values. The results are printed in order by top AUC score:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，尝试了八种可能的组合：`rank` = 5或30，`regParam` = 4.0或0.0001，`alpha` = 1.0或40.0\.
    这些值仍然有些随意选择，但被选来涵盖广泛的参数值范围。结果按照顶部AUC分数的顺序打印：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-1)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-1)'
- en: Free up model resources immediately.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 立即释放模型资源。
- en: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-2)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-2)'
- en: Sort by first value (AUC), descending, and print.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按第一个值（AUC）降序排列，并打印。
- en: The differences are small in absolute terms, but are still somewhat significant
    for AUC values. Interestingly, the parameter `alpha` seems consistently better
    at 40 than 1\. (For the curious, 40 was a value proposed as a default in one of
    the original ALS papers mentioned earlier.) This can be interpreted as indicating
    that the model is better off focusing far more on what the user did listen to
    than what he or she did not listen to.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对值上的差异很小，但对于AUC值来说仍然有些显著。有趣的是，参数`alpha`在40时似乎一直比1更好。(对于好奇的人来说，40是早期提到的原始ALS论文中建议的默认值之一。)
    这可以解释为表明模型更专注于用户听过的内容，而不是未听过的内容。
- en: A higher `regParam` looks better too. This suggests the model is somewhat susceptible
    to overfitting, and so needs a higher `regParam` to resist trying to fit the sparse
    input given from each user too exactly. Overfitting will be revisited in more
    detail in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的`regParam`看起来也更好。这表明模型在某种程度上容易过拟合，因此需要更高的`regParam`来抵抗试图过度拟合每个用户给出的稀疏输入。过拟合将在[“随机森林”](ch04.xhtml#RandomDecisionForests)中进一步详细讨论。
- en: As expected, 5 features are pretty low for a model of this size, and it underperforms
    the model that uses 30 features to explain tastes. It’s possible that the best
    number of features is actually higher than 30 and that these values are alike
    in being too small.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，5个特征对于这样大小的模型来说相当低，它的表现不如使用30个特征来解释口味的模型。也许最佳特征数实际上比30更高，而这些值之间因为都太小而相似。
- en: Of course, this process can be repeated for different ranges of values or more
    values. It is a brute-force means of choosing hyperparameters. However, in a world
    where clusters with terabytes of memory and hundreds of cores are not uncommon,
    and with frameworks like Spark that can exploit parallelism and memory for speed,
    it becomes quite feasible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个过程可以针对不同的值范围或更多的值重复进行。这是一种选择超参数的蛮力方法。然而，在一个拥有数百个核心和大量内存的集群不少见的世界里，以及可以利用并行性和内存加速的Spark等框架中，这变得非常可行。
- en: It is not strictly required to understand what the hyperparameters mean, although
    it is helpful to know what normal ranges of values are in order to start the search
    over a parameter space that is neither too large nor too tiny.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 不严格要求理解超参数的含义，但知道值的正常范围有助于开始搜索一个既不太大也不太小的参数空间。
- en: This was a fairly manual way to loop over hyperparameters, build models, and
    evaluate them. In [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    after learning more about the Spark ML API, we’ll find that there is a more automated
    way to compute this using `Pipeline`s and `TrainValidationSplit`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当手动的方式来循环超参数、构建模型并评估它们。在[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)中，通过学习更多关于Spark
    ML API的知识，我们将发现有一种更自动化的方式，可以使用`Pipeline`和`TrainValidationSplit`来计算这个过程。
- en: Making Recommendations
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行推荐
- en: Proceeding for the moment with the best set of hyperparameters, what does the
    new model recommend for user 2093760?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时使用最佳超参数集，新模型为用户2093760推荐什么？
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Anecdotally, this makes a bit more sense for this user, being dominated by pop
    rock instead of all hip-hop. `[unknown]` is plainly not an artist. Querying the
    original dataset reveals that it occurs 429,447 times, putting it nearly in the
    top 100! This is some default value for plays without an artist, maybe supplied
    by a certain scrobbling client. It is not useful information, and we should discard
    it from the input before starting again. It is an example of how the practice
    of data science is often iterative, with discoveries about the data occurring
    at every stage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 传闻，这对于此用户来说更有意义，因为他主要听的是流行摇滚而不是所有的嘻哈。`[unknown]`明显不是一个艺术家。查询原始数据集发现它出现了429,447次，几乎进入了前100名！这是一些没有艺术家信息的播放的默认值，可能由某个特定的scrobbling客户端提供。这不是有用的信息，我们在重新开始之前应该从输入中丢弃它。这是数据科学实践通常是迭代的一个例子，在每个阶段都会有关于数据的发现。
- en: This model can be used to make recommendations for all users. This could be
    useful in a batch process that recomputes a model and recommendations for users
    every hour or even less, depending on the size of the data and speed of the cluster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以用来为所有用户做推荐。这在批处理过程中可能很有用，该过程每小时或甚至更短时间重新计算一次模型和用户的推荐，具体取决于数据的大小和集群的速度。
- en: 'At the moment, however, Spark MLlib’s ALS implementation does not support a
    method to recommend to all users. It is possible to recommend to one user at a
    time, as shown above, although each will launch a short-lived distributed job
    that takes a few seconds. This may be suitable for rapidly recomputing recommendations
    for small groups of users. Here, recommendations are made to 100 users taken from
    the data and printed:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前Spark MLlib的ALS实现并不支持一次向所有用户推荐的方法。可以像上面展示的那样一次向一个用户推荐，尽管每次会启动一个持续几秒钟的短期分布式作业。这对于快速重新计算小组用户的推荐可能是合适的。这里，推荐给了从数据中选取的100名用户并打印出来：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO4-1)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO4-1)'
- en: Subset of 100 distinct users
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 100名不同用户的子集
- en: Here, the recommendations are just printed. They could just as easily be written
    to an external store like [HBase](https://oreil.ly/SQImy), which provides fast
    lookup at runtime.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，推荐只是被打印出来了。它们也可以被写入像[HBase](https://oreil.ly/SQImy)这样的外部存储，它提供了运行时快速查找。
- en: Where to Go from Here
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从这里开始
- en: Naturally, it’s possible to spend more time tuning the model parameters and
    finding and fixing anomalies in the input, like the `[unknown]` artist. For example,
    a quick analysis of play counts reveals that user 2064012 played artist 4468 an
    astonishing 439,771 times! Artist 4468 is the implausibly successful alternate-metal
    band System of a Down, which turned up earlier in recommendations. Assuming an
    average song length of 4 minutes, this is over 33 years of playing hits like “Chop
    Suey!” and “B.Y.O.B.” Because the band started making records in 1998, this would
    require playing four or five tracks at once for seven years. It must be spam or
    a data error, and another example of the types of real-world data problems that
    a production system would have to address.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 自然而然，可以花更多时间调整模型参数，查找并修复输入中的异常，如`[unknown]`艺术家。例如，快速分析播放次数显示用户2064012播放了艺术家4468惊人的439,771次！艺术家4468是一个不可思议成功的另类金属乐队——System
    of a Down，早在推荐中就出现过。假设平均歌曲长度为4分钟，这相当于连续33年播放像“Chop Suey!”和“B.Y.O.B.”这样的热门曲目。因为该乐队从1998年开始制作唱片，这需要每次同时播放四到五首歌曲七年。这必然是垃圾信息或数据错误的例子，也是生产系统可能需要解决的真实数据问题之一。
- en: ALS is not the only possible recommender algorithm, but at this time, it is
    the only one supported by Spark MLlib. However, MLlib also supports a variant
    of ALS for non-implicit data. Its use is identical, except that `ALS` is configured
    with `setImplicitPrefs(false)`. This is appropriate when data is rating-like,
    rather than count-like. For example, it is appropriate when the dataset is user
    ratings of artists on a 1–5 scale. The resulting `prediction` column returned
    from `ALSModel.transform` recommendation methods then really is an estimated rating.
    In this case, the simple RMSE (root mean squared error) metric is appropriate
    for evaluating the recommender.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ALS并不是唯一可能的推荐算法，但目前它是Spark MLlib唯一支持的算法。不过，MLlib也支持ALS的一种变体用于非隐式数据。它的使用方式相同，只是`ALS`配置为`setImplicitPrefs(false)`。当数据更像是评分而不是计数时，这是合适的。例如，当数据集是用户对艺术家评分在1-5分之间时。从`ALSModel.transform`推荐方法返回的`prediction`列确实是一个估计的评分。在这种情况下，简单的RMSE（均方根误差）指标适合评估推荐系统。
- en: Later, other recommender algorithms may be available in Spark MLlib or other
    libraries.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以后，Spark MLlib或其他库可能会提供其他推荐算法。
- en: In production, recommender engines often need to make recommendations in real
    time, because they are used in contexts like ecommerce sites where recommendations
    are requested frequently as customers browse product pages. Precomputing and storing
    recommendations is a reasonable way to make recommendations available at scale.
    One disadvantage of this approach is that it requires precomputing recommendations
    for all users who might need recommendations soon, which is potentially any of
    them. For example, if only 10,000 of 1 million users visit a site in a day, precomputing
    all million users’ recommendations each day is 99% wasted effort.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，推荐引擎经常需要实时推荐，因为它们用于像电子商务网站这样的环境中，顾客在浏览产品页面时频繁请求推荐。预先计算并存储推荐是一种在规模上可行的方法。这种方法的一个缺点是需要为所有可能需要推荐的用户预先计算推荐结果，而实际上只有其中的一小部分用户每天访问网站。例如，如果一百万用户中只有10,000名用户在一天内访问网站，每天为所有一百万用户预先计算推荐是99%的浪费努力。
- en: It would be nicer to compute recommendations on the fly, as needed. While we
    can compute recommendations for one user using the `ALSModel`, this is necessarily
    a distributed operation that takes several seconds, because `ALSModel` is uniquely
    large and therefore actually a distributed dataset. This is not true of other
    models, which afford much faster scoring.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好根据需要动态计算推荐。虽然我们可以使用`ALSModel`为一个用户计算推荐，但这是一个必须分布式执行的操作，需要几秒钟的时间，因为`ALSModel`非常庞大，实际上是一个分布式数据集。而对于其他模型来说，并不是这样，它们可以提供更快的评分。
