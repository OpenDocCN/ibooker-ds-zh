- en: Chapter 5\. From Data Ponds/Big Data Warehouses to Data Lakes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。从数据池/大数据仓库到数据湖
- en: Although when they were introduced over three decades ago, data warehouses were
    envisioned as a means of providing historical storage for enterprise data that
    would make it available for all types of new analytics, most data warehouses ended
    up being repositories of production-quality data used for only the most critical
    analytics. The majority could not process the vast amount and wide variety of
    data they contained. Some particularly high-end systems like Teradata could provide
    admirable scalability, but at very high costs. A lot of time and effort was spent
    tuning the performance of the data warehousing systems. As a result, any change—whether
    a new query or a schema change—had to go through elaborate architectural review
    and a lengthy approval and testing process. The ETL jobs that loaded the data
    warehouse were just as carefully constructed and tuned, and any new data required
    changes to those jobs and a similarly elaborate review and testing procedure.
    This prevented ad hoc querying and discouraged schema changes, and meant that
    data warehouses lacked agility.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管三十多年前引入时，数据仓库被设想为提供企业数据的历史存储的手段，使其可用于各种新的分析，但大多数数据仓库最终成为仅用于最关键分析的生产级数据存储库。这些数据仓库大多数无法处理它们所包含的大量和多样化的数据。像Teradata这样的一些高端系统能够提供令人钦佩的可伸缩性，但成本非常高昂。大量时间和精力花费在调整数据仓库系统的性能上。因此，任何变更——无论是新查询还是模式变更——都必须经过详尽的架构审查和漫长的批准和测试过程。加载数据仓库的ETL作业同样精心设计和调优，任何新数据都需要更改这些作业，并进行类似复杂的审查和测试流程。这种做法阻碍了即席查询，并且抑制了模式变更，导致数据仓库缺乏灵活性。
- en: Data lakes attempt to fulfill the original promise of an enterprise data repository
    by introducing extreme scalability, agility, future-proofing, and end user self-service.
    In this chapter we will take a closer look at data ponds—data warehouses implemented
    using big data technology—and explain how these ponds (or the data lakes that
    encompass them) can provide the functions for which organizations currently use
    a traditional data warehouse.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖试图通过引入极限可扩展性、灵活性、未来保障和最终用户自服务，来实现企业数据存储库的原始承诺。在本章中，我们将更详细地探讨数据池——使用大数据技术实施的数据仓库，并解释这些池（或涵盖它们的数据湖）如何提供组织目前使用传统数据仓库的功能。
- en: Data lakes are ideal repositories for enterprise data because they house different
    types of data in appropriate ways, to be used for different purposes by different
    processing systems within one massively parallel interoperable system. We will
    discuss adding data that it was difficult or impossible to add to a data warehouse,
    integrating a data lake with data sources, and consuming the data in a data lake
    with other systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是企业数据的理想存储库，因为它们以适当的方式托管不同类型的数据，可以被同一大规模并行可互操作系统内的不同处理系统用于不同目的。我们将讨论向数据仓库难以或不可能添加的数据的添加、将数据湖与数据源集成以及将数据湖中的数据与其他系统消耗的问题。
- en: Essential Functions of a Data Warehouse
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据仓库的基本功能
- en: Since many data lakes aim to complement or even replace data warehouses, and
    since data warehouses are frequently the easiest, biggest, and best sources of
    data in the enterprise, it is important to understand why they do things a certain
    way, how data lakes are constrained from doing these things, and what techniques
    can be used in big data technology to address these challenges.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多数据湖旨在补充甚至取代数据仓库，并且数据仓库通常是企业中最容易、最大、最好的数据源，因此了解它们以特定方式执行任务的原因以及数据湖如何受到这些限制，以及在大数据技术中可以使用什么技术来解决这些挑战非常重要。
- en: 'The original vision for data warehouses was to host (or warehouse) all historical
    data for future use. As the concept became formalized, data warehouses became
    highly managed systems with carefully controlled schemas and time-consuming change
    processes. Modern data warehouses generally focus on supporting analytics on high
    volumes of historical data read in from multiple sources. In order to achieve
    this, data architects need to make sure that:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库的最初愿景是为未来使用托管（或存储）所有历史数据。随着概念的正式化，数据仓库变成了高度管理的系统，具有精心控制的模式和耗时的变更过程。现代数据仓库通常专注于支持来自多个来源的大量历史数据的分析。为了实现这一目标，数据架构师需要确保：
- en: Data is organized to facilitate high-performance analytics. This is usually
    accomplished by means of dimensional modeling that creates star schemas. In addition,
    because of cost and performance implications, data warehouses usually cannot keep
    a complete history and have to aggregate or archive older data.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被组织以促进高性能分析。通常通过创建星型模式的维度建模来实现。此外，由于成本和性能的影响，数据仓库通常无法保留完整的历史数据，必须聚合或存档旧数据。
- en: Data from multiple systems can be analyzed in a consistent way. This is accomplished
    by integrating data from different systems into a consistent representation using
    techniques that include conforming dimensions, harmonization, and normalization.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个系统的数据可以以一致的方式进行分析。这是通过使用包括符合尺寸、协调和规范化等技术，将来自不同系统的数据集成为一致表示来实现的。
- en: Managing changes in a way that preserves the accuracy of historical analysis.
    This is usually accomplished using a technique called slowly changing dimensions,
    described in [Chapter 2](ch02.xhtml#historical_perspective).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以确保历史分析准确性的方式管理变更。通常使用称为慢变维的技术实现，详细描述在[第二章](ch02.xhtml#historical_perspective)中。
- en: Making sure that data is clean and consistent. This is accomplished using data
    quality tools and techniques, also discussed in [Chapter 2](ch02.xhtml#historical_perspective).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据干净和一致。这是通过使用数据质量工具和技术来实现的，也在[第二章](ch02.xhtml#historical_perspective)中讨论过。
- en: As described in previous chapters, an ETL (extract, transform, load) process
    is used to convert data from source systems into a form that can be loaded into
    a data warehouse. This conversion can be done either externally to the data warehouse
    or internally. External solutions make use of a range of ETL tools, many of which
    have been on the market for decades. Internal solutions load raw source data into
    the warehouse and apply transformations using SQL scripts executed by the data
    warehouse, a technique known as ELT (extract, load [into the target data warehouse],
    transform). In both cases, data quality tools are frequently integrated with the
    ETL tools and executed as part of the process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前几章所述，ETL（抽取、转换、加载）过程用于将数据从源系统转换为可加载到数据仓库中的形式。这种转换可以在数据仓库外部或内部完成。外部解决方案利用多种已有几十年历史的ETL工具。内部解决方案将原始源数据加载到数据仓库中，并使用由数据仓库执行的SQL脚本应用转换，这种技术称为ELT（抽取、加载[到目标数据仓库]、转换）。在两种情况下，数据质量工具经常与ETL工具集成，并作为过程的一部分执行。
- en: 'Since a data pond or lake based on big data technology is massively scalable
    and cost-effective, it can easily overcome the performance and data volume limitations
    of a data warehouse. Therefore, good performance does not require either dimensional
    modeling or aggregation (summarizing) of older data, as it does in most data warehouses.
    However, with regard to historical analysis, many of the challenges of data warehousing
    still apply. These include:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于大数据技术的数据池或湖具有高度可扩展性和成本效益，因此它可以轻松克服数据仓库的性能和数据量限制。因此，良好的性能不需要像大多数数据仓库那样进行维度建模或汇总旧数据。然而，关于历史分析，许多数据仓库的挑战仍然适用。这些挑战包括：
- en: Modeling data for analytics
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分析建模数据
- en: Integrating data from disparate systems to a common representation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将来自不同系统的数据集成到一个公共表示中
- en: Managing changes without losing data’s history
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理变更而不丢失数据的历史记录
- en: A data pond or data lake is the ideal place to store data for future use and
    perform large-scale analytics—but its use of big data technologies like Hadoop
    also makes it a great place to transform vast amounts of data. Data ponds often
    begin life as a result of performing the transformations required to create a
    data warehouse schema (known as ETL offloading). They evolve into data ponds by
    making both the raw and the transformed data warehouse data available for analytics,
    then eventually expand to include data from external or internal sources that
    was not in the original data warehouse and grow into full-fledged data lakes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池或数据湖是存储未来使用并进行大规模分析的理想场所，但其使用像Hadoop这样的大数据技术也使其成为转换大量数据的好地方。数据池通常通过执行创建数据仓库模式所需的转换（称为ETL卸载）而开始生命周期。它们通过为分析提供原始和转换后的数据仓库数据，最终扩展以包括来自原始数据仓库中不含的外部或内部源数据，并发展成为完整的数据湖。
- en: Before we examine how data ponds deal with the aforementioned challenges, let’s
    take another look at the issues in the context of traditional data warehouses.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究数据池如何处理上述挑战之前，让我们再次从传统数据仓库的背景下看看这些问题。
- en: Dimensional Modeling for Analytics
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于分析的维度建模
- en: As we saw in [Chapter 2](ch02.xhtml#historical_perspective), when relational
    databases are used to support operational systems and applications such as enterprise
    resource management (ERM) and customer relationship management (CRM), data is
    usually stored in highly normalized data models. Operational systems tend to do
    many small reads and writes. This activity is part of the reason for normalized
    data models, which attempt to create tables with minimum redundancy and the smallest
    possible number of fields. In addition to making updates and reads very fast,
    normalization eliminates the risk of inconsistent data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2章](ch02.xhtml#historical_perspective)中看到的，当关系数据库用于支持操作系统和企业资源管理（ERM）以及客户关系管理（CRM）等应用时，数据通常存储在高度规范化的数据模型中。操作系统倾向于进行许多小的读写操作。这部分活动是规范化数据模型的原因之一，它试图创建具有最小冗余和最小字段数的表。除了使更新和读取非常快外，规范化还消除了数据不一致的风险。
- en: In contrast, most data warehouses favor denormalized data models, with each
    table containing as many related attributes as possible. This makes it possible
    to process all the information needed by an analytical application with a single
    pass through the data. Furthermore, data warehouses typically receive data from
    many sources and applications, each with its own schema, and data from these different
    sources has to be converted to a single common schema.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，大多数数据仓库倾向于使用去规范化的数据模型，每个表包含尽可能多的相关属性。这使得可以通过一次数据遍历处理分析应用程序所需的所有信息。此外，数据仓库通常从许多来源和应用程序接收数据，每个来源都有其自己的模式，这些不同来源的数据必须转换为单一的通用模式。
- en: This topic was covered in detail in [Chapter 2](ch02.xhtml#historical_perspective),
    but as a brief refresher, a popular data model used by data warehouses is the
    star schema, consisting of dimension tables representing the entities being analyzed
    (e.g., customer, time, product) and one or more fact tables representing the activities
    that involve the dimensions (e.g., orders placed).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题在[第2章](ch02.xhtml#historical_perspective)中有详细论述，但作为简短的复习，数据仓库常用的一种流行数据模型是星型模式，包含代表被分析实体的维度表（例如客户、时间、产品）和代表涉及这些维度的活动的一个或多个事实表（例如已下订单）。
- en: 'The difficulty is that the sources of data often represent the same information
    in different ways: for example, one may break each address into multiple fields
    such as street, city, and state, whereas another stores the address in a single
    field. Similarly, some may keep a date of birth, while others store an age for
    each customer. In such cases, data needs to be converted to the format used by
    the data warehouse. For example, all the address fields might need to be concatenated,
    or a person’s current age might need to be calculated based on their date of birth.
    If data from all the source systems is kept in the same dimension tables in the
    same destination format, these tables are said to be *conforming*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 难点在于数据的来源通常以不同的方式表示相同的信息：例如，一个可能将每个地址拆分为多个字段（如街道、城市和州），而另一个则将地址存储在单个字段中。类似地，一些系统可能保留出生日期，而其他系统则为每位客户存储年龄。在这种情况下，需要将数据转换为数据仓库使用的格式。例如，所有地址字段可能需要连接起来，或者可以根据出生日期计算一个人的当前年龄。如果所有来源系统的数据都保留在相同的维度表中，并采用相同的目标格式，这些表就称为*符合*的表。
- en: Integrating Data from Disparate Sources
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合来自不同来源的数据
- en: Most modern ETL tools were developed around two decades ago, as part of the
    data warehousing movement, with the aim of converting data from different operational
    systems with different schemas and representations to a single common schema. Thus,
    the first challenge solved by ETL tools is converting records from the normalized
    schemas favored by operational systems to the denormalized schemas favored by
    data warehouses, as described in the previous section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代ETL工具是大约二十年前作为数据仓库运动的一部分开发的，旨在将来自不同操作系统的数据，其具有不同模式和表示，转换为单一的通用模式。因此，ETL工具解决的第一个挑战是将来自操作系统偏好的规范化模式的记录转换为数据仓库偏好的去规范化模式，正如前一节所述。
- en: The second challenge is converting data from many different operational applications
    to a single common schema and representation—the “conforming” data mentioned in
    that section. We saw an example in [Chapter 2](ch02.xhtml#historical_perspective)
    of how an ETL job might be used to convert an operational system’s representation
    of customer data into the representation expected by the data warehouse’s customer
    dimension table (see [Figure 2-7](ch02.xhtml#an_etl_tool_extracts_data_from_multiple)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个挑战是将来自许多不同操作应用程序的数据转换为单一的公共模式和表示法——这是在该部分提到的“符合”数据。我们在[第2章](ch02.xhtml#historical_perspective)中看到了一个例子，展示了如何使用ETL作业将操作系统对客户数据的表示转换为数据仓库期望的客户维度表的表示（参见[图2-7](ch02.xhtml#an_etl_tool_extracts_data_from_multiple)）。
- en: Data warehouses often contain data from many different sources and applications,
    each with its own schema, and data from all of these sources has to be normalized
    and converted to the data warehouse’s preferred schema differently, using a different
    ETL process for each source system. This leads to a rapid growth in the number
    of ETL scripts that have to be maintained and versioned.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库通常包含来自许多不同来源和应用程序的数据，每个都有自己的模式，并且来自所有这些来源的数据必须以不同的方式标准化和转换为数据仓库的首选模式，使用每个源系统的不同ETL过程。这导致必须维护和版本化的ETL脚本数量迅速增加。
- en: Preserving History Using Slowly Changing Dimensions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留历史数据的慢变维度
- en: Most dimensional data in a data warehouse is fairly static (customer data, data
    about retail or geographic locations, etc.). However, changes can occur in this
    data over time, and for accurate data analysis it’s necessary to keep track of
    them. A special construct has been developed to represent dimensional changes
    when considering historical data, called *slowly changing dimensions*. This ensures
    that, in the event of changes in certain aspects of the data (marital or job status,
    address, etc.), the correct state is taken into account for the analysis. The
    use of slowly changing dimensions in data warehouses is described in detail in
    [Chapter 2](ch02.xhtml#historical_perspective).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库中的大部分维度数据都是相当静态的（客户数据、零售或地理位置数据等）。然而，随着时间的推移，这些数据可能会发生变化，为了准确的数据分析，有必要跟踪这些变化。为了在考虑历史数据时表示维度变化，已经开发出了一种特殊的结构，称为*慢变维度*。这确保在数据某些方面发生变化（婚姻或工作状态、地址等）时，正确的状态被考虑进分析中。关于数据仓库中慢变维度的详细使用已在[第2章](ch02.xhtml#historical_perspective)中描述。
- en: Limitations of the Data Warehouse as a Historical Repository
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库作为历史存储库的局限性
- en: 'With legacy systems and data warehouses, because of the high cost of storage
    and processing, enterprises are forced to keep historical data at a coarser granularity
    than their more recent data. For example, a data warehouse may hold individual
    transactions for the last three years, daily totals for the last seven years,
    and monthly totals for data that’s more than seven years old. This causes a number
    of problems:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 遗留系统和数据仓库，因为存储和处理成本高昂，企业被迫将历史数据保留在比最近数据更粗粒度的级别上。例如，数据仓库可能保留过去三年的单个交易，过去七年的每日总数，以及超过七年的数据的月度总数。这引发了一系列问题：
- en: Data aggregation loses a lot of useful detail, limiting the types of analysis
    that can be performed.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据聚合会丢失很多有用的细节，从而限制了可以进行的分析类型。
- en: Most historical analysis has to be done at coarse granularity (at a level all
    the data can support, which will be either daily or monthly in our example, depending
    on whether the analysis extends back beyond seven years).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数历史分析必须以粗粒度进行（在我们的例子中，可能是每日或每月的级别，这取决于分析是否超过七年的时间）。
- en: Writing reports and queries that account for different levels of granularity
    is complex and error-prone.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写考虑不同粒度级别的报告和查询是复杂且容易出错的。
- en: Managing this system and moving data into various levels of granularity increases
    processing and administration overhead.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理这个系统并将数据移动到各种粒度级别会增加处理和管理开销。
- en: Most advanced analytic applications can benefit from having more historical
    data. Even simple analytics and historical trends give a more complete picture
    when given more history, both in terms of duration and number of attributes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数高级分析应用程序可以从拥有更多历史数据中受益。即使是简单的分析和历史趋势，在给定更多历史数据时，都能给出更完整的图像，无论是在持续时间还是属性数量上。
- en: A scalable and cost-effective storage and execution system like Hadoop allows
    enterprises to store and analyze their historical data at the finest granularity,
    thus increasing the richness and accuracy of analytical results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 像Hadoop这样的可扩展和成本效益的存储和执行系统允许企业以最精细的粒度存储和分析其历史数据，从而提高分析结果的丰富性和准确性。
- en: For example, fraud detection algorithms rely on analyzing large numbers of transactions
    to identify patterns of fraud. One [well-publicized case study](https://on.wsj.com/2RMdspZ)
    describes how Visa started using Hadoop for fraud detection and went from analyzing
    2% of customer transactions across 40 attributes using a single model to analyzing
    100% of the transactions across 500 attributes using 18 models, thereby allowing
    the company to identify billions of dollars’ worth of fraudulent transactions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，欺诈检测算法依赖于分析大量的交易来识别欺诈模式。一项[广为人知的案例研究](https://on.wsj.com/2RMdspZ)描述了Visa如何开始使用Hadoop进行欺诈检测，并从使用单一模型分析40个属性的客户交易的2%转变为使用18个模型分析500个属性的所有交易，从而使公司能够识别数十亿美元的欺诈交易。
- en: Moving to a Data Pond
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转向数据池
- en: Now that we have discussed the challenges of working in a traditional data warehouse,
    we can explore how to solve these problems with a data pond, or some combination
    of data warehouse and data pond. In this section we’ll cover alternative ways
    to organize data for efficient intake and processing, and how to preserve history
    (traditionally implemented using dimension tables).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了在传统数据仓库中工作的挑战，我们可以探讨如何通过数据池或数据仓库和数据池的组合来解决这些问题。在本节中，我们将讨论为有效摄入和处理数据组织数据的替代方法，以及如何保留历史记录（传统上使用维度表实现）。
- en: Keeping History in a Data Pond
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据池中保留历史记录
- en: Let’s first examine how history is kept in the data pond using partitions, and
    the limitations of this approach for keeping track of slowly changing dimensions.
    A new approach, using snapshots, is then discussed as a solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先检查如何在数据池中使用分区保留历史记录，以及这种方法在跟踪缓慢变化的维度方面的局限性。接下来讨论一种新方法，即使用快照来解决这个问题。
- en: In a data pond, as data is ingested, it is typically stored in multiple files
    or partitions. Each ingestion batch is typically loaded into a separate folder.
    All the files from all the folders are treated as a single “logical” file or table.
    Hive, the most popular SQL interface to Hadoop data, has a special construct called
    *partitioned tables* for working on these files. Partitioned tables allow Hive
    to intelligently optimize queries based on the partitioning structure.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据池中，随着数据的摄取，通常会存储在多个文件或分区中。每个摄取批次通常加载到一个单独的文件夹中。所有文件夹中的所有文件都被视为单个“逻辑”文件或表。Hive，作为Hadoop数据最流行的SQL接口，有一个称为
    *分区表* 的特殊结构来处理这些文件。分区表允许Hive根据分区结构智能优化查询。
- en: '[Figure 5-1](#directory_structure_for_partitioned_tabl) illustrates a typical
    partitioning schema used for daily loads of transaction data. A *transactions*
    directory contains all the transactions. The files are organized by year (for
    instance, */transactions/Year=2016*), inside a year by month (e.g., with */transactions/Year=2016/Month=3*
    containing all transactions for March 2016), and inside a month by day (with */transactions/Year=2016/Month=3/Day=2*
    containing all the transactions for March 2, 2016). Because Hadoop does a lot
    of parallel processing, to avoid contention for a single file, it generates multiple
    files in a */transactions/Year=2016/Month=3/Day=2* directory. These files are
    all concatenated to form a single file of transactions for that day.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](#directory_structure_for_partitioned_tabl)展示了用于每日加载交易数据的典型分区模式。一个 *transactions*
    目录包含所有的交易。文件按年份组织（例如，*/transactions/Year=2016*），在每年内按月份组织（例如，*/transactions/Year=2016/Month=3*
    包含了所有2016年3月的交易），在每月内按日份组织（例如，*/transactions/Year=2016/Month=3/Day=2* 包含了2016年3月2日的所有交易）。由于
    Hadoop 进行大量并行处理，为了避免对单个文件的竞争，它会在 */transactions/Year=2016/Month=3/Day=2* 目录生成多个文件。这些文件会合并成一个当天的交易文件。'
- en: '![Directory structure for partitioned tables in Hive](Images/ebdl_0501.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Hive 中分区表的目录结构](Images/ebdl_0501.png)'
- en: Figure 5-1\. Directory structure for partitioned tables in Hive
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. Hive 中分区表的目录结构
- en: In Hive, the user would create a single table (say, `all_transactions`), associate
    it with the *transactions* folder, and specify partition keys (`Year`, `Month`,
    `Day`). This `all_transactions` table would include all the data from all the
    files in the *transactions* folder. For example, the SQL statement `select * from
    all_transactions` would return all rows in the table by returning every single
    record in every single file in every subfolder under *transactions*, from the
    oldest file in the directory tree—say, */transactions/Year=2014/Month=1/Day=1/99312312311333.json*
    to the latest—say, */transactions/Year=2016/Month=3/Day=2/722121344412221.json*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hive 中，用户会创建一个单独的表（比如，`all_transactions`），关联到*transactions*文件夹，并指定分区键（`Year`、`Month`、`Day`）。这个`all_transactions`表会包含*transactions*文件夹下所有文件中的数据。例如，SQL语句`select
    * from all_transactions`会返回表中的所有行，实际上是返回*transactions*文件夹下每个子文件夹中每个文件的每一条记录，从目录树中最旧的文件开始，比如*/transactions/Year=2014/Month=1/Day=1/99312312311333.json*，到最新的，比如*/transactions/Year=2016/Month=3/Day=2/722121344412221.json*。
- en: In addition, the naming convention of `Field``=``Value` (e.g., `Year=2016`)
    allows Hive to intelligently direct each query to the files that may contain the
    data needed by the query. For example, a SQL query of `select * from all_transactions
    where Year = 2016 and Month = 3 and Day=2` would read data only in the files in
    the */transactions/Year=2016/Month=3/Day=2* folder, instead of reading all the
    files in all the folders and then filtering out the transactions for March 2,
    2016.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`Field``=``Value`的命名约定（例如，`Year=2016`）允许 Hive 智能地将每个查询定向到可能包含所需数据的文件。例如，一个SQL查询`select
    * from all_transactions where Year = 2016 and Month = 3 and Day=2`会仅读取*/transactions/Year=2016/Month=3/Day=2*文件夹中的数据文件，而不是读取所有文件夹中的所有文件，然后再过滤出2016年3月2日的交易记录。
- en: Implementing Slowly Changing Dimensions in a Data Pond
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据池中实现逐渐变化的维度
- en: Now we have to handle the dimensional or reference data, such as a customer’s
    marital status or other life changes. If we are loading dimension tables from
    a data warehouse that has used slowly changing dimensions, we can load the changed
    records—new customers and customers who have undergone a state change—into a separate
    file or append them to a single file containing all the customer data, because
    all the work of figuring out and handling changes to customer state has already
    been done.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须处理维度或参考数据，比如客户的婚姻状况或其他生活变化。如果我们从使用逐渐变化维度的数据仓库加载维度表，我们可以将更改记录—新客户和经历状态变化的客户—加载到单独的文件或附加到包含所有客户数据的单个文件中，因为所有处理客户状态变化的工作都已经完成。
- en: However, if we are loading data directly from operational systems, we need some
    way to identify changes. The technique used for data warehouses, creating slowly
    changing dimensions, complicates ingestion and analytics in the data pond. Each
    read can potentially add a record not only to the main data table, but to the
    dimensional data. Even worse, during later analytics, reads must join the temporal
    data in one table to the correct records in the other table.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们直接从操作系统加载数据，我们需要一种识别变化的方法。数据仓库使用的技术，创建逐渐变化的维度，会使数据池中的摄入和分析变得复杂。每次读取可能不仅向主数据表中添加记录，还向维度数据中添加记录。更糟糕的是，在后续的分析中，读取必须将一个表中的时间数据与另一个表中的正确记录进行关联。
- en: Denormalizing attributes to preserve state
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 去规范化属性以保留状态
- en: Another option is to denormalize the data and add all the important attributes
    to the file containing the transaction data. For example, when we load transactions
    from operational systems, we add information about customer demographics, marital
    status, and the like at the time of the transaction. This avoids the need for
    expensive and complicated joins. To save space and processing, we can optimize
    by adding attributes only where state information is important—in other words,
    we can add only the fields for which we would provide slowly changing dimensions
    in a data warehouse.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将数据去规范化，并将所有重要属性添加到包含交易数据的文件中。例如，当我们从操作系统加载交易时，我们会在交易时添加有关客户人口统计、婚姻状况等信息。这样可以避免昂贵和复杂的连接操作。为了节省空间和处理能力，我们可以优化，仅在状态信息重要时添加属性—换句话说，我们可以仅添加数据仓库中逐渐变化的维度所需的字段。
- en: The big drawback of this approach is that including these attributes in a data
    set with transaction data makes them available for use with the data in that particular
    data set, but not with other data. For example, we may have a separate data set
    for returns, a separate one for warranties, and so forth. To apply this technique,
    we would have to add all the customer attributes to each of these data sets, increasing
    storage and processing costs and adding complexity to our ingestion process. We
    would also have to remember to update all these data sets whenever we introduced
    a new customer attribute or made changes to the use of an existing attribute.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要缺点是，将这些属性包含在与交易数据一起的数据集中，使它们可以与该特定数据集中的数据一起使用，但不能与其他数据一起使用。例如，我们可能有一个用于退货的单独数据集，一个用于保修的单独数据集等。要应用此技术，我们必须将所有客户属性添加到每个数据集中，从而增加存储和处理成本，并增加数据接收过程的复杂性。当引入新的客户属性或更改现有属性的使用时，我们还必须记住更新所有这些数据集。
- en: Preserving state using snapshots
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用快照保留状态
- en: Yet another alternative is to ingest the latest version of the data every day.
    To support this, we would have a directory tree of dimensional data, but instead
    of a folder for each day being part of the data set (a partition), each day’s
    folder would be a complete version or snapshot of the data set. In other words,
    we could create a structure to keep changed data for customers using the same
    process as described earlier for transactions. Thus, the */customers/Year=2016/Month=3/Day=2*
    folder in [Figure 5-2](#partitioned_folders_for_dimension_table) would contain
    files that, when concatenated, would have the version of the customers data set
    for March 2, 2016.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是每天摄取最新版本的数据。为支持这一点，我们将有一个维度数据的目录树，但每天的文件夹不是数据集的一部分（一个分区），而是数据集的完整版本或快照。换句话说，在图5-2中的*/customers/Year=2016/Month=3/Day=2*文件夹将包含文件，这些文件连接后将得到2016年3月2日客户数据集的版本。
- en: '![Partitioned folders for dimension table](Images/ebdl_0502.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![维度表的分区文件夹](Images/ebdl_0502.png)'
- en: Figure 5-2\. Partitioned folders for dimension table
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 维度表的分区文件夹
- en: To get the appropriate representation of a customer record, we have to join
    each *transactions* record with a *customers* record from the same date. For example,
    if we’d created Hive tables for our *transactions* and *customers* data sets,
    we would join them using a SQL query on both customer ID and transaction date
    (e.g., `all_transactions.customer_id = customers.customer_id and transactions.Year
    = customers.Year and transactions.Month = customers.Month and transactions.Day
    = customers.Day`) to get the customer state at the time of the transaction.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得客户记录的适当表示，我们必须将每个*transactions*记录与同一日期的*customers*记录连接起来。例如，如果我们为我们的*transactions*和*customers*数据集创建了Hive表，我们将使用SQL查询在客户ID和交易日期上进行连接（例如，`all_transactions.customer_id
    = customers.customer_id and transactions.Year = customers.Year and transactions.Month
    = customers.Month and transactions.Day = customers.Day`）以获取交易时客户状态。
- en: The easiest way to see all the data is to create a Hive table that includes
    all the files in the folder. However, if for whatever reason using Hive or a similar
    tool is not an option, we will have to write custom code to correlate each partition
    of the *transactions* data set with the corresponding snapshot of the *customers*
    data set to make sure that we are considering the correct state of customer data
    at the time of a transaction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所有数据的最简单方法是创建一个包含文件夹中所有文件的Hive表。但是，如果由于某种原因无法使用Hive或类似工具，则必须编写自定义代码，将*transactions*数据集的每个分区与*customers*数据集的相应快照相关联，以确保我们在交易时考虑的是客户数据的正确状态。
- en: Although this is an expensive way to track changes because we have to store
    complete customer data for every day, it has several advantages over creating
    slowly changing dimensions. First, ingestion is straightforward and can use simple
    tools like Sqoop. Second, the snapshots preserve history for all attributes of
    the customer (whereas slowly changing dimensions track only some attributes).
    Additionally, this approach does not require us to assign a new customer key every
    time an important attribute changes, making it easier to do certain customer-related
    analytics, such as figuring out how many real customers we have over time. The
    final advantage of the snapshot approach is that if at some point storage becomes
    too expensive, this snapshot tree can be converted to a data set that captures
    only the slowly changing dimensions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一种昂贵的跟踪变化的方式，因为我们必须每天存储完整的客户数据，但它比创建缓慢变化的维度具有几个优点。首先，摄取是直接的，可以使用像Sqoop这样的简单工具。其次，快照保留了客户的所有属性的历史记录（而缓慢变化的维度仅跟踪一些属性）。此外，这种方法不要求我们在重要属性更改时每次分配一个新的客户关键字，这使得进行某些与客户相关的分析更加容易，比如随时间推移了解我们实际有多少真实客户。快照方法的最后一个优势是，如果某个时刻存储变得过于昂贵，那么可以将这个快照树转换为仅捕获缓慢变化维度的数据集。
- en: Growing Data Ponds into a Data Lake—Loading Data That’s Not in the Data Warehouse
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据池扩展为数据湖——加载不在数据仓库中的数据
- en: Most data in the enterprise today is thrown away because there is no known business
    use case for it yet. Without clear business value, there is no budget to cover
    the cost of keeping data, and without a use case in mind, it is not clear what
    schema to create to store it or how to transform or cleanse it. The data lake
    paradigm makes it possible to keep this data inexpensively and process it efficiently
    using the scalable computing model of Hadoop MapReduce or Spark.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如今企业中的大多数数据都被丢弃，因为尚未知道其业务用例。没有明确的业务价值，就没有预算来支付数据保留的成本；而且在没有明确的用例的情况下，也不清楚创建什么模式来存储数据，或者如何转换或清洗数据。数据湖范式使得能够廉价地保留这些数据，并利用Hadoop
    MapReduce或Spark的可伸缩计算模型进行高效处理。
- en: Raw Data
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始数据
- en: 'As we discussed previously, a data warehouse keeps only clean, normalized data.
    Unfortunately, a lot of important information is lost as part of the normalization
    process. Issues include:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，数据仓库仅保留干净、标准化的数据。不幸的是，很多重要信息在标准化过程中丢失了。问题包括：
- en: Data breadth
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据广度
- en: Typically, operational systems have many more attributes than a data warehouse.
    Only the most critical and common attributes end up in the data warehouse. The
    main reason for this is to reduce the cost of storing and processing all the attributes,
    as well as the management, ETL development, and other costs associated with loading
    anything into a data warehouse.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 典型情况下，操作系统比数据仓库具有更多的属性。只有最关键和最常见的属性才会最终出现在数据仓库中。这样做的主要原因是为了减少存储和处理所有属性的成本，以及与加载任何内容到数据仓库相关的管理、ETL开发和其他成本。
- en: With the scalability and cost efficiency of a data lake, it becomes possible
    to store and process much more information. And with frictionless ingestion (where
    the new data is loaded without any processing), the cost of ETL development is
    eliminated until there is a need to use this data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 利用数据湖的可伸缩性和成本效益，可以存储和处理更多的信息。而且通过无摩擦的摄取（新数据加载时不经过任何处理），可以消除直到需要使用这些数据时才进行的ETL开发成本。
- en: Original or raw data
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 原始或原始数据
- en: In a data warehouse, all data is treated the same and converted to a single
    format. For example, some systems may indicate that salary is not known with a
    `NULL` value or an illegal value such as `-1`, but since many databases cannot
    perform aggregation on `NULL` fields and since `-1` is not a legal salary, these
    values may be replaced with a default value of, say, `0`, either during the ETL
    process or as a separate data cleansing step. Data scientists would prefer to
    be able to tell the difference between someone who is really not earning any money
    and someone whose income is unknown—say, so they can replace the unknown income
    with an average income for that demographic to create a more accurate model. (This
    type of change is known as *data interpolation*, a routine analytic activity.)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库中，所有数据都被视为相同，并转换为单一格式。例如，一些系统可能指示薪水未知的字段使用`NULL`值或非法值如`-1`，但由于许多数据库无法对`NULL`字段执行聚合操作，并且`-1`不是合法的薪水，这些值可能会在ETL过程中或作为单独的数据清洗步骤中被替换为默认值，例如`0`。数据科学家们更倾向于能区分那些真的没有收入和那些收入未知的人——比如，他们可以将未知收入替换为该人群的平均收入，以创建更精确的模型。（这种变化被称为*数据插值*，是常规的分析活动。）
- en: A data lake typically keeps both original or raw data and processed data, giving
    the analysts a choice.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖通常同时保留原始数据和处理后的数据，为分析人员提供选择。
- en: Non-tabular formats
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 非表格格式
- en: A lot of big data (for example, social media data like Twitter feeds) is not
    in a tabular format but instead is represented as documents (e.g., JSON or XML),
    in columnar formats (e.g., Parquet), as log files (e.g., Apache log format), or
    as one of many other specialized representations. Thus, it cannot be easily translated
    to a relational data warehouse schema.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的大数据（例如，社交媒体数据如Twitter feeds）不是以表格格式存在，而是以文档形式（例如JSON或XML）、列格式（例如Parquet）、日志文件（例如Apache日志格式）或许多其他专用表示形式存在。因此，它不能轻易转换为关系型数据仓库模式。
- en: Because a data lake is typically built using big data technology like Hadoop,
    it can easily accommodate non-tabular formats. In fact, these formats are popular
    and are handled well by Hive, Spark, and many other big data projects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据湖通常是使用Hadoop等大数据技术构建的，它可以轻松容纳非表格格式。事实上，这些格式很受欢迎，并且在Hive、Spark和许多其他大数据项目中处理得很好。
- en: External Data
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部数据
- en: External data has been a multi-billion-dollar industry for decades. From Nielsen
    ratings to Equifax, TransUnion, and Experian credit reports, and from Morningstar
    ratings to Dun and Bradstreet business information to Bloomberg and Dow Jones
    financial transactions, enterprises have been buying and utilizing external data
    for years. Recently, the range of sources and data providers has expanded to include
    social media companies such as Twitter and Facebook, plus free government data
    available through [Data.gov](http://data.gov) and other portals.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 外部数据在数十年来一直是一个价值数十亿美元的行业。从尼尔森评级到Equifax、TransUnion和Experian信用报告，从晨星评级到邓白氏商业信息到彭博和道琼斯的金融交易，企业多年来一直在购买和利用外部数据。最近，数据来源和数据提供者的范围扩展到包括Twitter和Facebook等社交媒体公司，以及通过[Data.gov](http://data.gov)和其他门户网站提供的免费政府数据。
- en: 'Enterprises face big challenges with external data, including:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 企业面临外部数据的重大挑战，包括：
- en: Data quality
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量
- en: Quality issues range from incorrect and missing data to conflicting data from
    different suppliers. Data quality remains a major hurdle to incorporating external
    data into decision-making processes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 质量问题涵盖了从不正确和缺失的数据到不同供应商提供的冲突数据。数据质量仍然是将外部数据整合到决策过程中的主要障碍。
- en: Licensing costs
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 许可成本
- en: Data is expensive. To make matters worse, in many enterprises the same data
    is purchased multiple times by different groups because there is no easy way to
    share data sets or know whether the company has already purchased a specific data
    set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据成本高昂。更糟糕的是，在许多企业中，同一数据被不同的部门多次购买，因为没有简单的方法共享数据集或知道公司是否已经购买了特定的数据集。
- en: Intellectual property
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 知识产权
- en: Data providers sell subscriptions to data feeds, and many require their customers
    to remove all their data from their systems when they stop subscribing. Usually
    this includes not only the original purchased data sets, but any sets generated
    using that data. To accomplish that, the enterprises need to know where the data
    is and how it has been used.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据提供商向数据提供订阅，许多要求客户在停止订阅时从其系统中删除所有数据。通常这不仅包括原始购买的数据集，还包括使用该数据生成的任何数据集。为了实现这一点，企业需要知道数据的位置以及数据的使用方式。
- en: As [Figure 5-3](#two_different_teams_purchasing_and_using) illustrates, two
    different teams can purchase the same external data set, paying the vendor twice.
    Each team addresses quality problems differently and the resulting data is used
    in a variety of data sets and reports, which introduces conflicting information
    into the ecosystem. Even worse, lineage information is usually lost, so the enterprise
    cannot find all the instances where the data is used or trace the data back to
    the original data sets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 5-3](#two_different_teams_purchasing_and_using) 所示，两个不同团队可以购买相同的外部数据集，向供应商支付两次费用。每个团队都以不同的方式解决质量问题，并且生成的数据用于各种数据集和报告中，这导致生态系统中存在冲突的信息。更糟糕的是，谱系信息通常丢失，因此企业无法找到数据使用的所有实例，或者追溯数据回到原始数据集。
- en: '![Two different teams purchasing and using the same external data set](Images/ebdl_0503.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![两个不同团队购买和使用相同外部数据集](Images/ebdl_0503.png)'
- en: Figure 5-3\. Two different teams purchasing and using the same external data
    set
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 两个不同团队购买和使用相同外部数据集
- en: A Hadoop data lake can become a central place to load external data, address
    quality issues, manage access to original and clean versions, and track how the
    data is used. A simple approach is to create a folder hierarchy for keeping external
    data, such as */Data/External/<vendor_name>/<data_set_name>*. So, for instance,
    if the company purchases credit rating data from CreditSafe, it can place this
    data in */Data/External/CreditSafe/CreditRatings*. Additional folders can be used
    to capture more detail. For example, 2016 data for the United Kingdom could go
    into */Data/External/CreditSafe/CreditRatings/UK/2016*. If anyone in the organization
    needs 2016 UK credit ratings, they’ll know where to look before buying the data
    set again, as illustrated in [Figure 5-4](#single_place_to_keep_external_data_in_or).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 数据湖可以成为一个集中加载外部数据、解决质量问题、管理原始和清洁版本访问权限以及跟踪数据使用情况的中心位置。一个简单的方法是创建一个文件夹层次结构来存放外部数据，例如
    */Data/External/<vendor_name>/<data_set_name>*。因此，例如，如果公司从 CreditSafe 购买信用评级数据，可以将这些数据放置在
    */Data/External/CreditSafe/CreditRatings* 中。可以使用额外的文件夹来捕获更多细节。例如，2016 年的英国数据可以放在
    */Data/External/CreditSafe/CreditRatings/UK/2016* 中。如果组织中的任何人需要2016年英国的信用评级，他们在购买数据集之前就会知道去哪里找，如
    [图 5-4](#single_place_to_keep_external_data_in_or) 所示。
- en: '![Single place to keep external data in order to avoid duplicate purchases](Images/ebdl_0504.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![统一地存放外部数据以避免重复购买](Images/ebdl_0504.png)'
- en: Figure 5-4\. Single place to keep external data in order to avoid duplicate
    purchases
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 统一地存放外部数据以避免重复购买
- en: One drawback of this approach is that similar information might be provided
    by different vendors—so, analysts looking for UK credit ratings for a particular
    year would have to check the folder for each vendor to see if the data they need
    is already available. However, if we instead organize data by subject (e.g., */Data/External/CreditRatings/UK/2016/CreditSafe*),
    we run into other challenges. Vendor data sets do not always align well with predefined
    subjects, and may contain additional attributes. For example, the *CreditRatings*
    data set might also contain demographic data. If another analyst wanted to buy
    demographic data from CreditSafe, the company could well end up paying for this
    data twice. Even if someone noticed that the company had the data already, it
    would then have to be stored in two partitions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是，不同供应商可能提供相似的信息——因此，寻找特定年份的英国信用评级的分析师需要检查每个供应商的文件夹，看看他们需要的数据是否已经可用。然而，如果我们改为按主题组织数据（例如
    */Data/External/CreditRatings/UK/2016/CreditSafe*），我们会遇到其他挑战。供应商的数据集并不总是与预定义的主题很好地对齐，并且可能包含额外的属性。例如，*CreditRatings*
    数据集可能还包含人口统计数据。如果另一位分析师想要从 CreditSafe 购买人口统计数据，公司可能最终会支付两次这笔数据费用。即使有人注意到公司已经有了这些数据，它们也必须存储在两个分区中。
- en: Additionally, the data owner (the department that purchased the data for the
    organization) may require other information, like vendor ID or name, to uniquely
    identify a data set, and this information is difficult to capture in a single
    fixed folder hierarchy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，数据所有者（为组织购买数据的部门）可能需要其他信息，如供应商 ID 或名称，以唯一标识数据集，但这些信息很难在一个固定的文件夹层次结构中捕获。
- en: A more elegant and productive approach is to create a *catalog* of the external
    data sets that can capture multiple aspects of the data through properties, tags,
    and descriptions. For example, all data sets in the catalog can have common properties
    such as *vendor* and *owning department*, as well as properties specific to each
    data set such as *country* and *year*. This way, the data sets can physically
    live wherever the organization wants to keep them, but still be findable through
    their properties. In addition, because the catalog usually captures all the attributes
    or fields of a data set, analysts will be able to find the relevant data sets
    easily regardless of the purpose for which they want to use them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更加优雅和高效的方法是创建一个*目录*，用于捕捉外部数据集的多个方面，通过属性、标签和描述。例如，目录中的所有数据集都可以具有共同的属性，如*供应商*和*所属部门*，以及每个数据集特有的属性，如*国家*和*年份*。这样，数据集可以物理上存放在组织希望存放它们的任何地方，但仍然可以通过它们的属性找到它们。此外，因为目录通常捕捉数据集的所有属性或字段，分析师们可以很容易地找到相关的数据集，无论他们想要用它们做什么目的。
- en: Internet of Things (IoT) and Other Streaming Data
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物联网（IoT）和其他流数据
- en: Data lakes are especially appealing for the human interaction data within social
    media and web logs. This data usually far exceeds typical business transaction
    data in volume and complexity. Data lakes are even more appealing for the data
    that comes automatically from digital IoT devices. Because machines can produce
    data so much faster than humans, machine-generated data is bound to dwarf human-generated
    data and is expected to be the main source of data going forward. Most complex
    machines, from computer equipment to airplanes, medical devices, and elevators,
    generate log files that are sent back to the factory when problems arise. Increasingly,
    this  data is being streamed back for real-time automated monitoring. This data
    is used to monitor and address problems as well as to make machines smarter. From
    self-driving cars to automated temperature controls, smart machines are increasingly
    using data and analytics to self-manage their operations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖特别适用于社交媒体和网络日志中的人类互动数据。这些数据通常远远超过典型的商业交易数据，无论是在数量还是复杂性上。数据湖对于来自数字物联网设备的自动数据尤为吸引人。因为机器可以比人类更快地产生数据，机器生成的数据预计将超过人类生成的数据，并且预计将成为未来的主要数据来源。从计算设备到飞机、医疗设备和电梯等大多数复杂机器都会生成在出现问题时发送回工厂的日志文件。越来越多地，这些数据被实时流回用于自动监控。这些数据用于监控和解决问题，以及使机器更智能化。从自动驾驶汽车到自动温控，智能机器越来越多地使用数据和分析来自我管理其操作。
- en: While monitoring is performed in real time, it is difficult to interpret real-time
    behavior without comparing it to historical data. For example, to identify unexpected
    or anomalous behavior, we first have to establish a baseline of normal behavior,
    which requires analyzing historical data and comparing it to what we are seeing
    in real time. If a malfunction occurs, it will need to be handled right away.
    In addition, the behavior leading up to it—sometimes over many days, months, or
    even years—should be analyzed for clues, with the aim of understanding, detecting,
    and preventing such malfunctions in the future. Since a data lake is an ideal
    place to keep this history, a number of approaches and architectures have been
    developed to combine real-time data processing and historical analytics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实时监控是实时执行的，但在没有与历史数据进行比较之前，很难解释实时行为。例如，要识别意外或异常行为，我们首先必须建立正常行为的基线，这需要分析历史数据并将其与我们实时观察到的情况进行比较。如果发生故障，将需要立即处理。此外，导致故障的行为——有时是在许多天、月甚至年内——应该被分析以寻找线索，目的是理解、检测和预防将来的这类故障。由于数据湖是保存这些历史记录的理想场所，已经开发了许多方法和架构来结合实时数据处理和历史分析。
- en: In the following essay, big data visionary Michael Hausenblas discusses some
    best practices for such real-time data lakes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下文章中，大数据先锋迈克尔·豪森布拉斯讨论了实时数据湖的一些最佳实践。
- en: Real-Time Data Lakes
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时数据湖
- en: '![](Images/ebdl_05in01.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ebdl_05in01.png)'
- en: '***Michael Hausenblas** is a long-term big data visionary and practitioner
    who first got involved with Hadoop and other big data technologies in 2008\. Currently,
    Michael handles DevOps relations at Mesosphere. He is an Apache Contributor for
    Mesos and Drill, and former Chief Data Engineer for MapR in EMEA.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**迈克尔·豪森布拉斯**是一位长期致力于大数据的前瞻者和实践者，他于2008年首次接触Hadoop和其他大数据技术。目前，迈克尔在Mesosphere负责DevOps关系。他是Mesos和Drill的Apache贡献者，曾是MapR在EMEA的首席数据工程师。'
- en: Traditionally, data lakes have been associated with data at rest. Whether the
    data itself was machine-generated (for example, log files) or is a collection
    of manually generated data such as spreadsheets, the basic idea was to introduce
    a self-service approach to data exploration, making business-relevant data sets
    available across the organization.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据湖与静态数据相关联。无论数据本身是机器生成的（例如日志文件）还是手动生成的数据集（例如电子表格），基本思想是引入自助数据探索方法，使业务相关的数据集在整个组织中可用。
- en: 'Increasingly, there is a need to take streaming data sources into account,
    be it in the context of mobile devices, constrained devices such as sensors, or
    simply human online interactions (think, for example, embedded customer support
    chats): in all these cases, the data should usually be processed as it arrives.
    This is in contrast to the rather static idea of data sets manifesting themselves
    in dumps and being processed in a batch fashion. The question is, how can one
    build such real-time data lakes (for lack of a better term)? What guiding architectural
    considerations exist for building them so as to derive insights that you can act
    on instantly?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多地需要考虑流数据源，无论是在移动设备、受限设备（如传感器）还是简单的人类在线交互（例如嵌入式客户支持聊天）的情况下：在所有这些情况下，数据通常应该在到达时进行处理。这与数据集静态地表现为转储并以批处理方式处理的想法形成鲜明对比。问题在于，如何构建这样的实时数据湖（暂时用此术语）？构建它们的指导性架构考虑因素是什么，以便即时获取可操作的洞察力？
- en: In the past couple of years, several principal architectures have been proposed
    that allow the processing of data at rest and in motion together, at scale. Notably,
    Nathan Marz came up with the term *Lambda Architecture* for a generic, scalable,
    and fault-tolerant data processing architecture based on his experience working
    on distributed data processing systems at BackType and Twitter. The Lambda Architecture
    aims to satisfy the need for a robust system that is tolerant of hardware failures
    and human mistakes and able to serve a wide range of workloads and use cases for
    which low-latency reads and updates are required. It combines a *batch layer*
    that spans all the (historical) facts and a *speed layer* for the real-time data.
    You can learn more about it at *[*http://lambda-architecture.net*](http://lambda-architecture.net)*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，提出了几种主要的架构，允许同时处理静态和运动数据，实现大规模处理。值得注意的是，Nathan Marz提出了Lambda架构的术语，用于描述一种通用的、可扩展的、容错的数据处理架构，基于他在BackType和Twitter上分布式数据处理系统的经验。Lambda架构旨在满足对硬件故障和人为错误具有容忍性，并能为需要低延迟读取和更新的广泛工作负载和用例提供服务的系统需求。它结合了跨越所有（历史）事实的*批处理层*和用于实时数据的*速度层*。您可以在[*http://lambda-architecture.net*](http://lambda-architecture.net)了解更多相关信息。
- en: Another related and relevant architecture is the *Kappa Architecture*, introduced
    by [Jay Kreps](https://oreil.ly/2LSEdqz) in 2014\. In essence, it has a distributed
    log at its core and is simpler than the Lambda Architecture. Further variations
    of architectures relevant for realizing real-time data lakes can be found in Martin
    Kleppmann’s excellent book [*Designing Data-Intensive Applications*](http://dataintensive.net/)
    (O’Reilly).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关且相关的架构是*Kappa架构*，由[杰伊·克雷普斯](https://oreil.ly/2LSEdqz)于2014年引入。其核心是分布式日志，比Lambda架构更为简单。用于实现实时数据湖的其他架构变体可以在马丁·克莱普曼的优秀著作[*《设计数据密集型应用》*](http://dataintensive.net/)（O’Reilly）中找到。
- en: 'No matter what architecture you choose, at the end of the day you’ll need to
    select concrete technologies for the implementation part. Here I’ve grouped them
    into three buckets, and you’ll likely end up with at least one technology from
    each:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择何种架构，最终您都需要为实施部分选择具体的技术。在这里，我将它们分为三类，您可能最终会至少选择每类中的一种技术：
- en: 'Data stores: HDFS, HBase, Cassandra, Kafka'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储：HDFS、HBase、Cassandra、Kafka
- en: 'Processing engines: Spark, Flink, Beam'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理引擎：Spark、Flink、Beam
- en: 'Interaction: Zeppelin/Spark notebook, Tableau/Datameer'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互：Zeppelin/Spark notebook、Tableau/Datameer
- en: Last but not least, in a data lake scenario, provenance is of paramount importance.
    Being able to tell where a data set (or data stream) comes from and what it contains,
    and having access to other related metadata, is crucial to enable data scientists
    to select and interpret the data correctly and provide confidence measurements
    along with the results.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要，在数据湖场景中，来源追溯至关重要。能够了解数据集（或数据流）的来源及其内容，并访问其他相关元数据，对于数据科学家正确选择和解释数据，以及提供结果的可信度测量至关重要。
- en: Real-time data lakes have been successfully implemented in a variety of domains,
    including the financial industry (from fraud detection to bonus programs), telecommunications,
    and retail. Most organizations start out with a small and focused application,
    learn from the outcomes, and proceed to grow these application-specific data sets
    into data lakes that span different departments and applications, providing organizations
    with a data infrastructure that’s scalable both in terms of technology and human
    users. The technological aspect of scalability is satisfied by the properties
    of the data infrastructure, including the ability to scale out on commodity hardware
    and the inherently distributed nature of the processing and storage systems. The
    human aspect, however, can turn out to be more challenging. Firstly, without metadata,
    one risks turning a data lake into a data swamp. Further, the smooth interplay
    of data scientists, data engineers, and developers deserves special attention.
    Akin to the [DevOps philosophy](http://itrevolution.com/book/the-phoenix-project/),
    a culture of sharing and joint responsibility should be in place.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据湖已成功应用于多个领域，包括金融行业（从诈骗检测到奖励计划）、电信和零售。大多数组织从一个小而专注的应用程序开始，从结果中学习，并继续将这些应用程序特定的数据集扩展为跨不同部门和应用程序的数据湖，为组织提供既能在技术和人员用户方面扩展的数据基础设施。技术方面的可扩展性由数据基础设施的属性满足，包括在商品硬件上扩展的能力以及处理和存储系统的固有分布特性。然而，人员方面可能会更具挑战性。首先，缺乏元数据可能会将数据湖变成数据沼泽。此外，数据科学家、数据工程师和开发人员之间的顺畅互动值得特别关注。类似于[DevOps哲学](http://itrevolution.com/book/the-phoenix-project/)，需要建立分享和共同责任的文化。
- en: The Lambda Architecture
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda架构
- en: Let’s take a closer look at the Lambda Architecture that Michael Hausenblas
    describes. It combines real-time and batch processing of the same data, as illustrated
    in [Figure 5-5](#lambda_architecture).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看Michael Hausenblas描述的Lambda架构。它结合了相同数据的实时和批处理处理，如[图5-5](#lambda_architecture)所示。
- en: '![Lambda architecture](Images/ebdl_0505.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda架构](Images/ebdl_0505.png)'
- en: Figure 5-5\. Lambda architecture
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. Lambda架构
- en: An incoming real-time data stream is stored in master data batch layers as well
    as being kept in a memory cache in a speed layer. Data from the master data set
    is then indexed and made available through batch views, while real-time data in
    the speed layer is exposed through real-time views.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 传入的实时数据流存储在主数据批处理层中，并在速度层的内存缓存中保留。来自主数据集的数据随后被索引并通过批处理视图提供，而速度层中的实时数据则通过实时视图公开。
- en: Both batch and real-time views can be queried either independently or together
    to answer any historical or real-time questions. This architecture is well suited
    to Hadoop data lakes, where HDFS can be used to store the master data set, Spark
    or Storm can form the speed layer, HBase can be the service layer, and Hive creates
    views that can be queried.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理和实时视图可以独立或合并查询，以回答任何历史或实时问题。这种架构非常适合于Hadoop数据湖，其中HDFS用于存储主数据集，Spark或Storm形成速度层，HBase可以作为服务层，而Hive创建可以查询的视图。
- en: 'To learn more about the Lambda Architecture, see *Big Data: Principles and
    Best Practices of Scalable Realtime Data Systems* by Nathan Marz and James Warren
    (Manning).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Lambda架构的信息，请参阅《大数据：可扩展实时数据系统的原理与最佳实践》（Nathan Marz和James Warren著，Manning出版社）。
- en: Data Transformations
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'When using operational data for analytics, it can be useful to transform it
    for several reasons:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用运营数据进行分析时，由于多种原因进行转换可能会很有用：
- en: Harmonization
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 协调
- en: Data from different sources is converted to a common format or schema. This
    requires data architects to understand and carefully map every single attribute
    from every source system to that common schema. Because of the amount of work
    required to harmonize data, as a practical matter, most analytic schemas contain
    only a small subset of attributes; most of the attributes are thrown away.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不同来源的数据被转换成共同的格式或架构。这要求数据架构师理解并仔细映射每个来源系统的每个属性到这个共同的架构中。由于需要协调数据的工作量很大，实际上，大多数分析架构只包含很小一部分属性；大多数属性都被丢弃了。
- en: Entity resolution and reconciliation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析和对账
- en: Different instances of the same entity (e.g., customer) coming from different
    sources need to be recognized as referring to the same instance. For example,
    the names and addresses of the same customer may be slightly different in different
    systems and have to be recognized and matched. Once an entity is resolved and
    all instances are grouped together, any conflicts have to be resolved (e.g., different
    sources may have different addresses for the same customer, and conflict resolution
    involves deciding which address to keep).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同来源的同一实体的不同实例（例如客户）需要被识别为指向同一个实例。例如，同一客户的姓名和地址在不同系统中可能略有不同，必须被识别和匹配。一旦一个实体被解析并且所有实例被分组在一起，任何冲突必须被解决（例如，不同来源可能为同一客户有不同的地址，冲突解决涉及决定保留哪个地址）。
- en: Performance optimization
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 性能优化
- en: In certain systems, such as relational databases, some schemas facilitate faster
    analytic queries. A star schema, as mentioned earlier in this chapter, is one
    such common optimization.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些系统中，例如关系数据库，一些架构可以促进更快的分析查询。星形架构，正如本章早些时候提到的，是一种常见的优化方案。
- en: Fortunately, in a data lake, because schemas are imposed only as data is read
    (and not enforced when data is written, as described in [Chapter 3](ch03.xhtml#introduction_to_big_data_and_data_scienc)),
    operational data can be ingested from various sources as is and harmonized as
    necessary for analytics. Instead of throwing away attributes that we cannot afford
    to harmonize now, we keep them in the data lake until there is a need for them
    and we can justify doing the work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在数据湖中，由于只有在读取数据时强加模式（并且不像写入数据时那样强制执行，如第3章中描述的那样），操作数据可以按原样从各种来源摄入，并根据需要进行调和以供分析使用。我们不再丢弃我们现在无法调和的属性，而是将它们保留在数据湖中，直到我们需要它们并且可以证明我们可以进行这项工作为止。
- en: The same approach can be taken for entity resolution. Instead of going through
    the effort and expense of reconciling all the entities from different systems
    and resolving conflicts for all the attributes, we reconcile only the entities
    that we need for our project and consider only the conflicts for the attributes
    we care about. We can then resolve them in the way that’s most appropriate for
    the project. For example, general entity resolution might focus on finding the
    current address for a customer. However, if we are identifying target customers
    for a promotion around the San Francisco 49ers football team, having all the past
    addresses for customers is a huge benefit. Our conflict resolution will focus
    on determining whether a person ever lived in San Francisco, rather than trying
    to figure out their current address.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析可以采取相同的方法。与其费力费钱地协调来自不同系统的所有实体并解决所有属性的冲突，我们只解决我们项目所需的实体，并且仅考虑我们关心的属性的冲突。然后我们可以根据项目最合适的方式来解决它们。例如，通常实体解析可能集中于查找客户的当前地址。但是，如果我们正在为旧金山49人橄榄球队的促销活动确定目标客户，那么拥有客户的所有过去地址将是一个巨大的优势。我们的冲突解决将集中于确定一个人是否曾经住在旧金山，而不是试图弄清他们当前的地址。
- en: Finally, because Hadoop is such a powerful transformation engine and can efficiently
    execute massive queries that require significant transformations during analysis,
    we will less often need to transform data to an analytics-friendly schema for
    performance reasons.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于Hadoop是如此强大的转换引擎，并且可以在分析期间高效执行需要重大转换的大规模查询，因此出于性能原因，我们将不再频繁地将数据转换为适合分析的架构。
- en: However, interestingly enough, Hadoop is frequently used to perform transformations
    to feed other systems, such as data warehouses. As described earlier, this process
    of transforming operational data into the analytic schema required by a data warehouse
    is a form of ETL offloading. The operational data is ingested into Hadoop as is
    and then transformed and loaded into the data warehouse. A practical approach
    is to ingest all or most of the operational data into the data lake, not just
    the data needed by the data warehouse. Then some of the data can be used to load
    the data warehouse, while all of the data is available for analysis and data science
    in the lake. In addition, if this data needs to be added to the data warehouse
    later, it is already present in the data lake.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有趣的是，Hadoop 经常被用于执行转换以供应其他系统，比如数据仓库。正如前面所述，这个将运营数据转换为数据仓库所需的分析模式的过程是一种 ETL
    卸载的形式。运营数据被按原样摄取到 Hadoop 中，然后转换和加载到数据仓库中。一个实用的方法是将所有或大部分运营数据摄取到数据湖中，而不仅仅是数据仓库需要的数据。然后可以使用其中的一部分数据加载数据仓库，而所有数据都可以用于数据湖的分析和数据科学。此外，如果这些数据稍后需要添加到数据仓库中，它已经存在于数据湖中。
- en: Figures [5-6](#traditional_etl_process) through [5-9](#a_hadoop_etl_offloading_project_containi)
    illustrate the expansion of pure ETL offloading to a more generalized data lake.
    We start with the traditional data warehouse (DW) design illustrated in [Figure 5-6](#traditional_etl_process),
    where an ETL tool is used to extract data from operational systems, transform
    it into the data warehouse schema, and load it into the data warehouse.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5-6](#traditional_etl_process) 到 [5-9](#a_hadoop_etl_offloading_project_containi)
    说明了纯 ETL 卸载向更通用的数据湖的扩展。我们从 [图5-6](#traditional_etl_process) 中所示的传统数据仓库（DW）设计开始，其中使用
    ETL 工具从运营系统中提取数据，将其转换为数据仓库模式，并将其加载到数据仓库中。
- en: '![Traditional ETL process](Images/ebdl_0506.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![传统 ETL 过程](Images/ebdl_0506.png)'
- en: Figure 5-6\. Traditional ETL process
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 传统 ETL 过程
- en: For many years, high-end database vendors encouraged their customers to use
    their database engines to do the transformations (the ELT model discussed in [Chapter 2](ch02.xhtml#historical_perspective)
    and shown in [Figure 5-7](#elt_process)) instead of leaving it to external ETL
    tools. These vendors argued that only highly scalable systems like theirs could
    handle the volume and complexity of loading their data warehouses.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，高端数据库供应商鼓励他们的客户使用他们的数据库引擎进行转换（在 [第2章](ch02.xhtml#historical_perspective)
    中讨论的 ELT 模型以及 [图5-7](#elt_process) 中显示的），而不是交给外部的 ETL 工具来做。这些供应商认为，只有像他们这样高度可伸缩的系统才能处理数据仓库的容量和复杂性。
- en: '![ELT process](Images/ebdl_0507.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![ELT 过程](Images/ebdl_0507.png)'
- en: Figure 5-7\. ELT process
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. ELT 过程
- en: With ETL offloading, Hadoop-based ETL jobs built using MapReduce or Spark or
    one of the existing projects like Hive, Pig, or Sqoop replace the ETL tool or
    the work done by the data warehouse in ELT, as illustrated in [Figure 5-8](#using_hadoop_for_etl_offloading).
    Operational data is ingested into Hadoop as is, then transformed into the required
    schema and loaded into the data warehouse.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ETL 卸载中，使用 MapReduce 或 Spark 构建的基于 Hadoop 的 ETL 作业或现有项目之一（如 Hive、Pig 或 Sqoop）取代了
    ETL 工具或数据仓库在 ELT 中的工作，如 [图5-8](#using_hadoop_for_etl_offloading) 所示。运营数据被摄入 Hadoop
    中，然后按所需的模式转换并加载到数据仓库中。
- en: '![Using Hadoop for ETL offloading](Images/ebdl_0508.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Hadoop 进行 ETL 卸载](Images/ebdl_0508.png)'
- en: Figure 5-8\. Using Hadoop for ETL offloading
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. 使用 Hadoop 进行 ETL 卸载
- en: At this point, Hadoop contains the original source data used to load the data
    warehouse in the original format. If we add the remaining operational data as
    well, we have the beginnings of a data lake that will contain the original (raw)
    data in the landing zone, plus the cleansed and transformed data in the curated
    or gold zone, as illustrated in [Figure 5-9](#a_hadoop_etl_offloading_project_containi).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Hadoop 包含用于加载数据仓库的原始源数据，格式与原来相同。如果我们也添加其余的运营数据，那么我们就开始了一个包含着原始（原始）数据的着陆区的数据湖，以及包含着清洁和转换数据的经过策划或金牌区，如
    [图5-9](#a_hadoop_etl_offloading_project_containi) 所示。
- en: '![A Hadoop ETL offloading project containing raw, cleansed, and transformed
    data](Images/ebdl_0509.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![一个 Hadoop ETL 卸载项目，包含原始、清洁和转换数据](Images/ebdl_0509.png)'
- en: Figure 5-9\. A Hadoop ETL offloading project containing raw, cleansed, and transformed
    data
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 一个 Hadoop ETL 卸载项目，包含原始、清洁和转换数据
- en: Target Systems
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标系统
- en: Data from the data lake can be consumed by a variety of target systems. These
    systems are commonly data warehouses and specialized analytical databases and
    data marts, but consumers of the information can also be operational applications
    such as ERP or CRM and real-time applications, or even data scientists who want
    raw data for their models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖中的数据可以被多种目标系统消费。这些系统通常是数据仓库、专业分析数据库和数据集市，但信息的使用者也可以是诸如ERP或CRM以及实时应用程序的运营应用，甚至是希望为其模型获取原始数据的数据科学家。
- en: 'We will examine the consumption paradigms for the following target systems:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查以下目标系统的消耗范式：
- en: Data warehouses
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Operational data stores (ODSs)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运营数据存储（ODSs）
- en: Real-time applications and data products
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时应用程序和数据产品
- en: Data Warehouses
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库
- en: We covered ETL offloading in the previous section. The data generated by the
    offloaded ETL jobs running in the lake typically gets loaded into the data warehouse
    by creating files that can be bulk-loaded using native database utilities or by
    creating simplistic ETL jobs that just load the data without any transformations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中已经讨论了ETL卸载。由湖中运行的卸载ETL作业生成的数据通常通过创建文件进行加载，这些文件可以使用本地数据库实用程序进行批量加载，或者创建简单的ETL作业，只加载数据而不进行任何转换。
- en: Operational Data Stores
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运营数据存储
- en: An operational data store is used to consolidate, cleanse, and normalize data.
    It addresses a disadvantage of the ELT approach, wherein the ELT jobs interfere
    with and affect the performance of analytics jobs. By moving all the processing
    to a separate ODS, enterprises can protect analytics queries from being slowed
    down by ELT jobs. While a data lake can be used to feed processed data to an ODS,
    it is actually a very attractive replacement for the ODS, and offers the side
    benefit that the resulting data can be kept in the data lake as well, to be used
    by analytics there. In many ways, using Hadoop or another big data platform as
    an ODS is a natural extension of ETL offloading. In this configuration, more functionality,
    such as data quality and master data management, is offloaded to Hadoop and the
    results are distributed to the other data systems that used to get their data
    from an ODS.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 运营数据存储用于 consoli、清洗和规范数据。它解决了ELT方法的一个缺点，即ELT作业干扰和影响分析作业的性能。通过将所有处理移动到独立的ODS中，企业可以保护分析查询免受由ELT作业减速的影响。虽然数据湖可以用来向ODS提供处理过的数据，但实际上它是ODS的一个非常有吸引力的替代品，并且提供的数据还可以保留在数据湖中，供分析使用。在许多方面，将Hadoop或其他大数据平台用作ODS是ETL卸载的自然延伸。在这种配置中，更多的功能，如数据质量和主数据管理，被卸载到Hadoop，结果被分发到其他以前从ODS获取数据的数据系统。
- en: Real-Time Applications and Data Products
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时应用程序和数据产品
- en: Real-time applications process incoming data streams. Various industry-specific
    use cases exist for processing real-time information, from automated inventory
    replenishment to health monitoring. Data products are production deployments of
    statistical models created by data scientists. Data products can process data
    in real time or in batches.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 实时应用程序处理传入的数据流。存在各种行业特定的用例，用于处理实时信息，从自动库存补充到健康监控。数据产品是数据科学家创建的统计模型的生产部署。数据产品可以实时处理数据或批处理处理数据。
- en: 'We can roughly generalize the output of real-time applications and data products
    into several categories, as illustrated in [Figure 5-10](#results_of_processing_in_the_data_lake):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将实时应用程序和数据产品的输出大致归类为几类，如[图5-10](#results_of_processing_in_the_data_lake)所示：
- en: Dashboards
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘
- en: These display the current state of a system. Stock tickers, real-time election
    results, and airport arrival and departure displays are examples of real-time
    dashboards. As real-time events are processed, the state is continuously updated.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些显示系统的当前状态。股票行情、实时选举结果以及机场到达和离开显示是实时仪表板的示例。随着实时事件的处理，状态会持续更新。
- en: Automated actions
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化操作
- en: As events are being processed, based on specific conditions, these systems automatically
    respond. This is also called *complex event processing* (CEP) and can control
    anything from factory operations to inventory management, automated replenishment,
    transportation logistics, and climate control. This kind of data product is often
    used to perform automated stock trading or advertising auction bidding, where
    millions of bids are placed in seconds.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理事件时，根据特定条件，这些系统会自动响应。这也被称为*复杂事件处理*（CEP），可以控制从工厂操作到库存管理、自动补货、运输物流和气候控制的各种情况。这种类型的数据产品通常用于执行自动股票交易或广告拍卖竞标，其中可以在几秒钟内进行数百万次竞标。
- en: Alerts and notifications
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 警报和通知
- en: These are an alternative to both human-intensive processes—making staff constantly
    monitor dashboards—and writing complex automated programs to handle any possible
    condition. Many real-time systems augment human intelligence with automation,
    specifying notification conditions and sending notifications to human users when
    those conditions are triggered. Conditions can vary from simple ones (e.g., when
    a temperature gets to a certain point, pop up a warning on the control panel)
    to very complex ones that incorporate historical and real-time data (e.g., when
    the website traffic exceeds 20% of normal traffic for this time of the day and
    day of the year, send an email message to the administrator).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统是人力密集型流程（需要员工不断监控仪表板）和编写复杂自动程序的替代方案，用于处理任何可能的条件。许多实时系统通过自动化增强人类智能，指定通知条件，并在触发这些条件时向人类用户发送通知。这些条件可以从简单的条件（例如，当温度达到某个点时，在控制面板上弹出警告）到非常复杂的条件（例如，当网站流量超过这一天和这一年这个时间段正常流量的20%时，向管理员发送电子邮件）。
- en: Data sets
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: Data products frequently perform bulk operations that produce data sets, such
    as generating a list of customers for an email campaign by doing customer segmentation,
    or producing reports to estimate house prices.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据产品经常执行生成数据集的批量操作，例如通过进行客户分群生成电子邮件营销活动的客户列表，或生成估算房价的报告。
- en: '![Results of processing in the data lake](Images/ebdl_0510.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖处理结果](Images/ebdl_0510.png)'
- en: Figure 5-10\. Results of processing in the data lake
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10\. 数据湖处理结果
- en: Conclusion
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As laid out in this chapter, data lakes can be attractive replacements for data
    warehouses and can subsume some existing legacy systems and processes, like ETL
    and ODSs. However, their true power and incredible value are realized when data
    lakes are used to address different needs that arise in the enterprise, like advanced
    analytics, ad hoc analysis, and business user self-service, that will be covered
    in subsequent chapters. The journey is not simple, but the processing power of
    the data lake, the benefits of centralizing and sharing data and processing, and
    the economics of big data are compelling and make it eminently worthwhile.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，数据湖可以作为数据仓库的吸引替代方案，并可以包含一些现有的遗留系统和流程，如ETL和ODS。然而，数据湖真正的力量和令人惊叹的价值在于，当其用于解决企业中出现的不同需求时，如高级分析、临时分析和业务用户自助服务，这些内容将在后续章节中涵盖。这段旅程并不简单，但数据湖的处理能力、集中和共享数据与处理的好处，以及大数据的经济效益都是非常引人注目的，使其非常值得。
