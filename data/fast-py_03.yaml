- en: 2 Extracting maximum performance from built-in features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 从内置功能中提取最大性能
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Profiling code to find speed and memory bottlenecks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析代码以找到速度和内存瓶颈
- en: Making more efficient use of existing Python data structures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高效地使用现有的 Python 数据结构
- en: Understanding Python’s memory cost of allocating typical data structures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Python 分配典型数据结构的内存成本
- en: Using lazy programming techniques to process large amounts of data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用惰性编程技术处理大量数据
- en: There are many tools and libraries to help us write more efficient Python. But
    before we dive into all the external options to improve performance, let’s first
    take a closer look at how we can write pure Python code that is more efficient,
    in both computing and IO performance. Indeed many, although certainly not all,
    Python performance problems can be solved by being more mindful of Python’s limits
    and capabilities.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具和库可以帮助我们编写更高效的 Python。但在我们深入研究所有外部选项以提高性能之前，让我们首先更仔细地看看我们如何编写更高效的纯 Python
    代码，无论是在计算性能还是 IO 性能上。事实上，尽管并非所有，许多 Python 性能问题都可以通过更加关注 Python 的限制和能力来解决。
- en: To demonstrate Python’s own tools for improving performance, let’s use them
    on a hypothetical, although realistic problem. Let’s say you are a data engineer
    tasked with preparing the analysis of climate data around the world. The data
    will be based on the Integrated Surface Database from the US National Oceanic
    and Atmospheric Administration (NOAA; [http://mng.bz/ydge](http://mng.bz/ydge)).
    You are on a tight deadline, and you will only be able to use mostly standard
    Python. Furthermore, buying more processing power is out of the question due to
    budgetary constraints. The data will start to arrive in one month, and you plan
    on using the time before it arrives to increase code performance. Your task, then,
    is to find the places in need of optimization and to increase their performance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 Python 自身用于提高性能的工具，让我们在一个假设的、尽管是现实的问题上使用它们。假设你是一名数据工程师，负责准备全球气候数据分析。数据将基于美国国家海洋和大气管理局（NOAA）的集成表面数据库（[http://mng.bz/ydge](http://mng.bz/ydge)）。你面临紧迫的截止日期，并且只能使用大部分标准的
    Python。此外，由于预算限制，购买更多处理能力是不可能的。数据将在一个月后开始到达，你计划在数据到达之前利用这段时间来提高代码性能。那么，你的任务就是找到需要优化的地方，并提高它们的性能。
- en: The first thing that you want to do is to profile the existing code that will
    ingest the data. You know that the code that you already have is slow, but before
    you try to optimize it, you need to find empirical evidence of the bottlenecks.
    Profiling is important because it allows you to search, in a rigorous and systematic
    way, for bottlenecks in your code. The most common alternative, guesstimating,
    is particularly ineffective here because many slowdown points can be quite unintuitive.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先想做的事情是对将要处理数据的现有代码进行性能分析。你知道你现有的代码很慢，但在尝试优化它之前，你需要找到瓶颈的实证证据。性能分析很重要，因为它允许你以严格和系统的方式搜索代码中的瓶颈。最常见的替代方案，即猜测，在这里特别无效，因为许多减速点可能非常不直观。
- en: 'Optimizing pure Python code is the low-hanging fruit, but it is also where
    most problems tend to reside, so optimizing can often be quite advantageous. In
    this chapter, we will see what pure Python offers out of the box to help us develop
    more performant code. We will start by profiling the code, using several profiling
    tools, to detect problem areas. Then we will focus on Python’s basic data structures:
    lists, sets, and dictionaries. Our goal here will be to improve the efficiency
    of these data structures and to allocate memory to them in the best way for optimal
    performance. Finally, we will see how modern Python lazy programming techniques
    can help us to improve the performance of our data pipeline.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 优化纯 Python 代码是低垂的果实，但也是大多数问题倾向于驻留的地方，因此优化通常非常有优势。在本章中，我们将看到纯 Python 提供了哪些现成功能来帮助我们开发更高效的代码。我们将从使用几个性能分析工具对代码进行性能分析开始，以检测问题区域。然后我们将关注
    Python 的基本数据结构：列表、集合和字典。我们的目标将是提高这些数据结构的效率，并以最佳方式分配内存以实现最佳性能。最后，我们将看到现代 Python
    惰性编程技术如何帮助我们提高数据管道的性能。
- en: This chapter will only discuss optimizing Python without external libraries,
    but we will still use some external tools to help us optimize performance and
    access data. We will be using Snakeviz to visualize the output of Python profiling,
    as well as line_profiler to profile code line by line. Finally, we will use the
    requests library to download data from the internet.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将仅讨论在不使用外部库的情况下优化Python，但我们仍将使用一些外部工具来帮助我们优化性能和访问数据。我们将使用Snakeviz来可视化Python性能分析的结果，以及line_profiler逐行分析代码。最后，我们将使用requests库从互联网下载数据。
- en: If you use Docker, the default image has all you need. If you follow the instructions
    for Anaconda Python from appendix A, you are all set. Let’s now start our profiling
    process by downloading data from weather stations and studying the temperature
    at each station.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Docker，默认镜像已经包含了你需要的一切。如果你遵循附录A中Anaconda Python的说明，你就可以开始了。现在，让我们通过从气象站下载数据并研究每个站点的温度来开始我们的性能分析过程。
- en: 2.1 Profiling applications with both IO and computing workloads
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 使用IO和计算工作负载进行性能分析
- en: Our first objective will be to download data from a weather station and get
    the minimum temperature for a certain year on that station. Data on NOAA’s site
    has CSV files, one per year and then per station. For example, the file [https://www.ncei.noaa.gov/data/global-hourly/access/2021/01494099999.csv](https://www.ncei.noaa.gov/data/global-hourly/access/2021/01494099999.csv)
    has all the entries for station 01494099999 for the year 2021\. This includes,
    among other entries, temperature and pressure, recorded potentially several times
    a day.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要目标将是下载气象站的数据，并获取该站某一年份的最低温度。NOAA网站上的数据有CSV文件，每年一个，每个站点一个。例如，文件[https://www.ncei.noaa.gov/data/global-hourly/access/2021/01494099999.csv](https://www.ncei.noaa.gov/data/global-hourly/access/2021/01494099999.csv)包含了2021年站点01494099999的所有条目。这包括其他条目，如温度和压力，可能每天记录几次。
- en: Let’s develop a script to download the data for a set of stations on an interval
    of years. After downloading the data of interest, we will get the minimum temperature
    for each station.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个脚本来下载一组站点在多年间的数据。在下载所需数据后，我们将得到每个站点的最低温度。
- en: 2.1.1 Downloading data and computing minimum temperatures
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 下载数据和计算最低温度
- en: 'Our script will have a simple command-line interface, where we pass a list
    of stations and an interval of years of interest. Here is the code to parse the
    input (the code can be found in `02-python/sec1-io-cpu/load.py`):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的脚本将有一个简单的命令行界面，其中我们传递一个站点列表和一个感兴趣的年份间隔。以下是解析输入的代码（代码可在`02-python/sec1-io-cpu/load.py`中找到）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To ease the coding part, we will be using the requests library to get the file.
    Here is the code to download the data from the server:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化编码部分，我们将使用requests库来获取文件。以下是下载服务器数据的代码：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Requests makes it easy to access web content.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ① Requests库使得访问网络内容变得简单。
- en: 'This code will write each downloaded file to disk for all the requested stations
    across all years. Now, let’s get all the temperatures in a single file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将把所有请求的站点在所有年份下载的每个文件写入磁盘。现在，让我们将所有温度放入一个文件中：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① The format for the temperature field includes a subfield with the status quality
    of the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ① 温度字段的格式包括一个表示数据质量状态的字段。
- en: ② We ignore entries for which the data is not available.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们忽略数据不可用的条目。
- en: 'Let’s now get all temperatures and the minimum temperature per station:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们获取所有温度和每个站点的最低温度：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we can tie everything together: download the data, get all temperatures,
    compute the minimum per station, and print the results:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将所有东西结合起来：下载数据，获取所有温度，计算每个站点的最低温度，并打印结果：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For example, to load the data for stations 01044099999 and 02293099999 for the
    year 2021, we do
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了加载2021年站点01044099999和02293099999的数据，我们执行以下操作
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: with the output being
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, the real fun starts. Our goal is to continue to download lots of data from
    lots of stations over many years. To handle this quantity of data, we want to
    make the code as efficient as possible. The first step in making the code more
    efficient is to profile it in an organized and thorough way to find the bottlenecks
    slowing it down. For this, we will use Python’s built-in profiling machinery.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，真正的乐趣开始了。我们的目标是继续从许多站点下载多年的大量数据。为了处理这些数据量，我们希望使代码尽可能高效。使代码更高效的第一步是以有组织和彻底的方式进行性能分析，以找到减缓其速度的瓶颈。为此，我们将使用Python内置的性能分析工具。
- en: 2.1.2 Python’s built-in profiling module
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 Python内置的性能分析模块
- en: As we want to make sure our code is as efficient as possible, the first thing
    we need to do is to find existing bottlenecks in that code. Our first port of
    call will be profiling the code to check each function’s time consumption. For
    this, we run the code via Python’s `cProfile` module. This module is built into
    Python and allows us to obtain profiling information from our code. Make sure
    you do not use the `profile` module, as it is orders of magnitude slower; it’s
    only useful if you are developing profiling tools yourself.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望确保我们的代码尽可能高效，我们首先需要做的是找到该代码中现有的瓶颈。我们的第一步将是分析代码以检查每个函数的时间消耗。为此，我们通过 Python
    的 `cProfile` 模块运行代码。此模块是 Python 内置的，允许我们从代码中获取分析信息。确保您不要使用 `profile` 模块，因为它慢得多；它仅在你自己开发分析工具时有用。
- en: 'We can run the profiler with:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式运行分析器：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Remember that running Python with the `-m` flag will execute the module, so
    we are running the `cProfile` module. This is Python’s recommended module to gather
    profiling information. We are asking for profile statistics ordered by cumulative
    time. The easiest way to use the module is by passing our script to the profiler
    in a module call like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，使用 `-m` 标志运行 Python 将执行模块，因此我们正在运行 `cProfile` 模块。这是 Python 推荐的模块，用于收集分析信息。我们要求按累积时间排序的统计信息。使用该模块的最简单方法是将我们的脚本传递给分析器，如下所示：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '① Basic summary information can be found on the first line: the number of function
    calls and total run time.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ① 基本摘要信息可以在第一行找到：函数调用次数和总运行时间。
- en: ② The computing costs of our code (computing is done in get_min_temperatures)
    are neglegible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们代码的计算成本（计算在 `get_min_temperatures` 中完成）可以忽略不计。
- en: The output is ordered by cumulative time, which is all the time spent inside
    a certain function. Another output is the number of calls per function. For example,
    there is only a single call to `download_all_data` (which takes care of downloading
    all data), but its cumulative time is almost equal to the total time of the script.
    You will notice two columns called `percall`. The first one states the time spent
    on the function *excluding* the time spent on all the subcalls. The second one
    includes the time spent on subcalls. In the case of `download_all_data`, it is
    clear that most time is consumed by some of the subfunctions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输出按累积时间排序，即在一个特定函数内部花费的所有时间。另一个输出是每个函数的调用次数。例如，只有一个对 `download_all_data` 的调用（负责下载所有数据），但其累积时间几乎等于脚本的总体时间。你会注意到有两个名为
    `percall` 的列。第一个列表示不包括所有子调用花费的时间在函数上的时间。第二个列包括子调用花费的时间。在 `download_all_data` 的例子中，很明显，大部分时间被一些子函数消耗了。
- en: In many cases, when you have some intensive form of I/O like here, there is
    a strong possibility that I/O dominates in terms of time needed. In our case,
    we have both network I/O (getting the data from NOAA) and disk I/O (writing it
    to disk). Network costs can vary widely, even between runs, as they are dependent
    on many connection points along the way. As network costs are normally the biggest
    time sink, let’s try to mitigate those.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，当你有一些像这里这样的密集型 I/O 时，有很强的可能性 I/O 在所需时间方面占主导地位。在我们的例子中，我们既有网络 I/O（从 NOAA
    获取数据）也有磁盘 I/O（将其写入磁盘）。网络成本可能差异很大，甚至在运行之间，因为它们依赖于沿途的许多连接点。由于网络成本通常是最大的时间消耗，让我们尝试减轻这些成本。
- en: 2.1.3 Using local caches to reduce network usage
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 使用本地缓存以减少网络使用
- en: 'To reduce network communication, let’s save a copy for future use when we download
    a file for the first time. We will build a local cache of data. We will use the
    same code as the previous, save for the function `download_all_data` (the code
    can be found in `02-python/sec1-io-cpu/load_cache.py`):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少网络通信，当我们在第一次下载文件时，让我们保存一个副本以备将来使用。我们将建立一个本地数据缓存。我们将使用与之前相同的代码，除了函数 `download_all_data`（代码可以在
    `02-python/sec1-io-cpu/load_cache.py` 中找到）：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① We check whether the file already exists and only download it if not.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们检查文件是否已存在，如果不存在则下载它。
- en: 'The first run of the code will take the same time as the previous solution,
    but a second run will not require any network access. For example, given the same
    run as the previous, it goes from 2.8 s to 0.26 s—more than an order of magnitude
    increase. Remember that due to high variability in network access, the time to
    download files can vary substantially in your case. This is yet another reason
    to consider caching network data: having a more predictable execution time:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一次运行将与之前的解决方案花费相同的时间，但第二次运行将不需要任何网络访问。例如，给定与之前相同的运行，它从2.8秒减少到0.26秒——超过一个数量级。记住，由于网络访问的高变异性，下载文件的时间在你的情况下可能会有很大的变化。这是考虑缓存网络数据的另一个原因：拥有更可预测的执行时间：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, the result is different in where time is consumed:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，消耗时间的地方不同了：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While the time to run decreased one order of magnitude, IO is still top. Now,
    it’s not the network but disk access. This is mostly caused by the computation
    being acually low.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然运行时间减少了一个数量级，但IO仍然是首要的。现在，不是网络而是磁盘访问。这主要是由于计算实际上很低。
- en: Warning Caches, as this example shows, can speed up code by orders of magnitude.
    However, cache management can be problematic and is a common source of bugs. In
    our example, the files never change over time, but there are many use cases for
    caches where the source might be changing. In that case, the cache management
    code needs to recognize that problem. We will revisit caches in other parts of
    the book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：缓存，正如这个例子所示，可以以数量级的方式加快代码速度。然而，缓存管理可能会出现问题，并且是bug的常见来源。在我们的例子中，文件随时间不会改变，但有许多缓存用例，源可能会改变。在这种情况下，缓存管理代码需要识别这个问题。我们将在本书的其他部分重新审视缓存。
- en: We are now going to consider a case where CPU is the limiting factor.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考虑一个CPU是限制因素的情况。
- en: 2.2 Profiling code to detect performance bottlenecks
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 对代码进行性能分析以检测性能瓶颈
- en: Here we look at code where CPU is the resource costing the most time in a process.
    We’ll take all stations in the NOAA database and compute the distance between
    them, a problem of complexity `n2`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们查看CPU是过程中耗时最多的资源的代码。我们将取NOAA数据库中的所有站点并计算它们之间的距离，这是一个复杂度为`n2`的问题。
- en: 'In the repository, you will find a file (`02-python/sec2-cpu/locations.csv`)
    with all the geographical coordinates of the stations (the code can be found in
    `02-python/sec2-cpu/distance_cache.py`):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在仓库中，你会找到一个文件(`02-python/sec2-cpu/locations.csv`)，其中包含所有站点的地理坐标（代码可以在`02-python/sec2-cpu/distance_cache.py`中找到）：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① This is the code to compute the distance between two stations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是计算两个站点之间距离的代码。
- en: ② As we are comparing all the stations between each other, the complexity is
    of the order n2.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ② 由于我们正在比较所有站点之间的相互关系，其复杂度为n2。
- en: The previous code will take a long time to run. It also takes a lot of memory.
    If you have memory problems, limit the number of stations that you are processing.
    Let’s now use Python’s profiling infrastructure to see where most time is spent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码将运行很长时间。它也消耗了很多内存。如果你有内存问题，限制你正在处理的站点的数量。现在，让我们使用Python的性能分析基础设施来查看大部分时间花在了哪里。
- en: 2.2.1 Visualizing profiling information
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 可视化性能分析信息
- en: Again, we use Python’s profiling infrastructure to find pieces of code that
    are delaying execution. But to better inspect the trace, we’ll use an external
    visualization tool, SnakeViz ([https://jiffyclub.github.io/snakeviz/](https://jiffyclub.github.io/snakeviz/)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们使用Python的性能分析基础设施来找到延迟执行的部分代码。但为了更好地检查跟踪，我们将使用一个外部可视化工具，SnakeViz ([https://jiffyclub.github.io/snakeviz/](https://jiffyclub.github.io/snakeviz/))。
- en: 'We start by saving a profile trace:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先保存一个性能分析跟踪：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `-o` parameter specifies the file where the profiling information will be
    stored. After that, we have the call to our code as usual.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`-o`参数指定了性能信息将被存储的文件。之后，我们像往常一样调用我们的代码。'
- en: Note Python provides the `pstats` module to analyze traces written to disk.
    You can do `python -m pstats distance_cache.prof`, which will start a command-line
    interface to analyze the cost of our script. You can find more information about
    this module in the Python documentation or in the profiling section of chapter
    5.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Python提供了`pstats`模块来分析写入磁盘的跟踪。你可以执行`python -m pstats distance_cache.prof`，这将启动一个命令行界面来分析我们脚本的成本。你可以在Python文档或第5章的性能分析部分找到更多关于此模块的信息。
- en: To analyze this information, we will use the web-based visualization tool, SnakeViz.
    You just need to do `snakeviz distance_cache.prof`. This will start an interactive
    browser window (figure 2.1 shows a screenshot).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析这些信息，我们将使用基于网络的可视化工具 SnakeViz。你只需要执行 `snakeviz distance_cache.prof`。这将启动一个交互式浏览器窗口（图
    2.1 展示了截图）。
- en: Familiarizing yourself with the SnakeViz interface
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉 SnakeViz 界面
- en: This would be a good time to play with the SnakeViz interface a bit. For example,
    you can change the style from Icicle to Sunburst (arguably cuter but with less
    information as the file name disappears). Reorder the table at the bottom. Check
    the Depth and Cutoff entries. Do not forget to click some of the colored blocks,
    and, finally, return to the main view by clicking Call Stack and choosing the
    0 entry.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是玩 SnakeViz 界面一段时间的好时机。例如，你可以将样式从 Icicle 改为 Sunburst（虽然可能更可爱，但信息较少，因为文件名消失了）。重新排列底部的表格。检查深度和截止值。别忘了点击一些彩色块，最后通过点击调用栈并选择
    0 条记录返回主视图。
- en: '![](../Images/CH02_F01_Antao.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F01_Antao.png)'
- en: Figure 2.1 Using SnakeViz to inspect profiling information of our script
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 使用 SnakeViz 检查脚本分析信息
- en: Most of the time is spent inside the function `get_distance`, but exactly where?
    We can see the cost of some of the math functions, but Python’s profiling doesn’t
    allow us to have a fine-grained view of what happens inside each function. We
    only get aggregate views for each trigonometric function. Yes, there is some time
    spent in `math.sin`, but given that we use it in several lines, where exactly
    are we paying a steep price? For that, we need to recruit the help of the line
    profiling module.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分时间都是在 `get_distance` 函数内部度过的，但具体在哪里呢？我们可以看到一些数学函数的成本，但 Python 的分析并不允许我们以细粒度查看每个函数内部发生的事情。我们只能得到每个三角函数的汇总视图。是的，我们在
    `math.sin` 上花费了一些时间，但鉴于我们在几行代码中使用它，我们到底在哪些地方付出了高昂的代价？为此，我们需要请线分析模块帮忙。
- en: 2.2.2 Line profiling
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 行分析
- en: Built-in profiling, like we used previously, allowed us to find the piece of
    code that was causing a massive delay. But there are limits to what we can do
    with it. We’ll discuss those limits here and introduce line profiling as a way
    to find further performance bottlenecks in our code.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 内置分析，就像我们之前使用的那样，允许我们找到导致巨大延迟的代码片段。但它的功能是有限的。我们将在下面讨论这些限制，并介绍行分析作为找到代码中进一步性能瓶颈的方法。
- en: 'To understand the cost of each line of `get_distance`, we will use the `line_profiler`
    package, which is available at [https://github.com/pyutils/line_profiler](https://github.com/pyutils/line_profiler).
    Using the line profiler is quite easy: you just need to add an annotation to `get_distance`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 `get_distance` 每一行的成本，我们将使用 `line_profiler` 包，该包可在 [https://github.com/pyutils/line_profiler](https://github.com/pyutils/line_profiler)
    找到。使用行分析器相当简单：你只需要在 `get_distance` 上添加一个注释：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You might have noticed that we have not imported the `profile` annotation from
    anywhere. This is because we will be using the convenience script `kernprof` from
    the `line_profiler` package that will take care of this. Let’s then run the line
    profiler in our code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们没有从任何地方导入 `profile` 注释。这是因为我们将使用来自 `line_profiler` 包的便利脚本 `kernprof`，它会处理这个问题。那么，让我们在我们的代码中运行行分析器：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Be prepared for the instrumentation required by the line profiler to slow the
    code substantially, by several orders of magnitude. Let it run for a minute or
    so and, after that, interrupt it (`kernprof` would probably run for many hours
    if you let it complete). If you interrupt it, you will still have a trace. After
    the profiler ends, you can have a look at the results with the command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好线分析器所需的仪器可能会显著减慢代码，降低几个数量级。让它运行一分钟或更长时间，然后中断它（如果你让它完成，`kernprof` 可能会运行数小时）。如果你中断它，你仍然会有一个跟踪记录。分析器结束后，你可以使用以下命令查看结果：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you look at the output shown in listing 2.1, you can see that it has many
    calls that take a long time. So we will probably want to optimize that code. At
    this stage, as we are discussing only profiling, we will stop here, but afterward,
    we would need to optimize those lines (we will do so later in this chapter). If
    you are interested in optimizing this piece of code, have a look at chapter 6
    about Cython or appendix B on Numba as they provide the most straightforward avenues
    to increase the speed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看2.1列表中显示的输出，您会看到其中有很多耗时较长的调用。因此，我们可能希望优化这段代码。在这个阶段，因为我们只讨论性能分析，所以我们在这里停止，但之后，我们需要优化这些行（我们将在本章后面进行）。如果您对优化这段代码感兴趣，可以查看第6章关于Cython的内容或附录B关于Numba的内容，因为它们提供了最直接提高速度的方法。
- en: Listing 2.1 The output of the `line_profiler` package for our code
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 `line_profiler`包为我们代码的输出
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① The total running time for our code
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们代码的总运行时间
- en: ② The information that we are getting for each line that is being profiled.
    For each line, we get the number of times the line is called, the sum of the time
    spent on the line, the time per call, and the percentage of time on the line.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们为每个正在性能分析的行获得的信息。对于每一行，我们得到该行被调用的次数、该行上花费的总时间、每次调用的时间和该行上花费的时间百分比。
- en: Hopefully, you will find line_profiler’s output substantially more intuitive
    than the output from the built-in profiler.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您会发现`line_profiler`的输出比内置性能分析器的输出更直观。
- en: '2.2.3 The takeaway: Profiling code'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 要点：代码性能分析
- en: As we’ve seen, overall built-in profiling is a big help as a first approach;
    it is also substantially faster than line profiling. But line profiling is significantly
    more informative, mostly because built-in Python profiling doesn’t provide a breakdown
    inside the function. Instead, Python’s profiling only provides cumulative values
    per function, as well as showing how much time is spent on subcalls. In specific
    cases, it is possible to know if a subcall belongs to another function, but, in
    general, that is not possible. An overall strategy for profiling needs to take
    all this into account.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，整体内置性能分析是一个很好的起点；它也比行性能分析快得多。但是，行性能分析提供了更多的信息，主要是因为内置的Python性能分析没有在函数内部提供细分。相反，Python的性能分析只提供每个函数的累积值，以及显示在子调用上花费的时间。在特定情况下，可以知道子调用是否属于另一个函数，但通常情况下，这是不可能的。性能分析的整体策略需要考虑所有这些因素。
- en: 'The strategy we used here is a generally sensible approach: first, try the
    built-in Python profiling module `cProfile` because it is fast and does provide
    some high-level information. If that is not enough, use line profiling, which
    is more informative but also slower. Remember, here we are mostly concerned with
    locating bottlenecks; later chapters will provide ways to optimize the code. Sometimes
    just changing parts of an existing solution is not enough and a general re-architecturing
    will be necessary; we will also discuss that in due time.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的策略是一种通常合理的做法：首先，尝试使用内置的Python性能分析模块`cProfile`，因为它速度快，并提供了一些高级信息。如果这还不够，使用行性能分析，它提供了更多的信息，但速度较慢。记住，我们在这里主要关注定位瓶颈；后面的章节将提供优化代码的方法。有时仅仅改变现有解决方案的一部分是不够的，需要进行整体重构；我们也将适时讨论这一点。
- en: Other profiling tools
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其他性能分析工具
- en: 'Many other utilities can be useful if you are profiling code, but a profiling
    section would not be complete without a reference to one of these, the `timeit`
    module. This is probably the most common approach that newcomers take to profile
    code and you can find endless examples using the `timeit` module on the Internet.
    The easiest way to use the `timeit` module is by using IPython or Jupyter Notebook,
    as these systems make `timeit` very streamlined. Just add the `%timeit` magic
    to what you want to profile, for example, inside iPython:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在性能分析代码，许多其他实用工具都可能很有用，但如果没有提到这些工具之一，性能分析部分将不会完整，那就是`timeit`模块。这可能是新来者最常用的性能分析方法，您可以在互联网上找到无数使用`timeit`模块的示例。使用`timeit`模块的最简单方法是使用IPython或Jupyter
    Notebook，因为这些系统使`timeit`非常流畅。只需将`%timeit`魔法命令添加到您想要性能分析的代码中即可，例如，在iPython中：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This gives you the run time of several runs of the function that you are profiling.
    The magic will decide how many times to run and report basic statistical information.
    In the previous snippet, you have the difference between a `range(1000000)` and
    a `list(range(1000000))`. In this specific case, `timeit` shows that the lazy
    version of `range` is two orders of magnitude faster than the eager one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出你正在分析的功能的多次运行时间。魔法将决定运行多少次以及报告哪些基本统计信息。在上一个代码片段中，你看到了 `range(1000000)` 和
    `list(range(1000000))` 之间的差异。在这个特定案例中，`timeit` 显示 `range` 的惰性版本比急切版本快两个数量级。
- en: 'You will be able to find more details in the documentation of the `timeit`
    module, but for most use cases, the `%timeit` magic of IPython will be enough
    to access its functionality. You are encouraged to use IPython and its magic,
    but in most of the rest of the book, we will use the standard interpreter. You
    can read more about the `%timeit` magic here: [https://ipython.readthedocs.io/en/stable/interactive/magics.html](https://ipython.readthedocs.io/en/stable/interactive/magics.html).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `timeit` 模块的文档中找到更多详细信息，但对于大多数用例，IPython 的 `%timeit` 魔法就足够访问其功能了。我们鼓励你使用
    IPython 和它的魔法，但在本书的大部分内容中，我们将使用标准解释器。你可以在这里了解更多关于 `%timeit` 魔法的详情：[https://ipython.readthedocs.io/en/stable/interactive/magics.html](https://ipython.readthedocs.io/en/stable/interactive/magics.html)。
- en: 'Now that you are familiar with both a toolset and an approach to profiling,
    let’s direct our attention to a different subject: optimizing the usage of Python
    data structures.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了工具集和性能分析的方法，让我们将注意力转向不同的主题：优化 Python 数据结构的用法。
- en: '2.3 Optimizing basic data structures for speed: Lists, sets, and dictionaries'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 优化基本数据结构以提高速度：列表、集合和字典
- en: Next, we will try to find inefficient uses of Python basic data structures and
    rewrite pieces of code more efficiently. To demonstrate this process, we will
    continue to use the temperature data from NOAA. But here our challenge is to determine
    whether certain temperatures occurred in a station during a specified time interval.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试找出 Python 基本数据结构的低效使用，并更高效地重写代码片段。为了演示这个过程，我们将继续使用来自 NOAA 的温度数据。但在这里，我们的挑战是确定在指定时间段内某个站点是否发生了特定的温度。
- en: 'We will reuse the code from the first section of the chapter to read the data
    (the code can be found in `02-python/sec3-basic-ds/exists_temperature.py`). What
    we are interested in, for the sake of this example, is the data from station 01044099999
    for the years 2005 to 2021:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用本章第一部分中的代码来读取数据（代码可以在 `02-python/sec3-basic-ds/exists_temperature.py` 中找到）。为了这个示例，我们感兴趣的是从
    2005 年到 2021 年该站点的 01044099999 号站点的数据：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`first_all_temperatures` has a list of temperatures for the station. We can
    get some basic stats with `print(len(first_all_temperatures)`, `max(first_all_temperatures)`,
    `min(first_all_temperatures)))`. We have 141,082 entries with a maximum of 27.0
    C and a minimum of -16.0 C.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`first_all_temperatures` 包含了该站点的温度列表。我们可以使用 `print(len(first_all_temperatures),
    max(first_all_temperatures), min(first_all_temperatures))` 来获取一些基本统计数据。我们共有 141,082
    条记录，最高温度为 27.0 摄氏度，最低温度为 -16.0 摄氏度。'
- en: 2.3.1 Performance of list searches
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 列表搜索的性能
- en: 'Checking whether a temperature is in the list is a matter of `temperature in
    first_all_temperatures`. Let’s get a rough estimate of how much time it takes
    to check whether -10.7 is in the list:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 检查温度是否在列表中是 `temperature in first_all_temperatures` 的问题。让我们大致估算一下检查 -10.7 是否在列表中需要多少时间：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output on my computer is:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我的计算机上的输出如下：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s now try this query with a value that we know is not on the list:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试使用一个我们知道不在列表上的值来执行这个查询：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The result is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is roughly one order of magnitude slower than our search for -10.7.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约比我们搜索 -10.7 的速度慢一个数量级。
- en: Why such low performance in the second search? Because to complete this search,
    the `in` operator does a sequential scan starting from the beginning of the list.
    This approach means, in a worst-case scenario, that the entire list will be searched,
    which is exactly the case when the element that we are looking for (-100) is *not*
    on the list. For small lists, it adds a trivial amount of time to start the search
    at the top and go straight through. But as the list grows, as well as the number
    of searches that you might have to do on those ever-growing lists, the time adds
    up significantly.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么第二次搜索的性能如此低？因为为了完成这个搜索，`in`操作符从列表的开始进行顺序扫描。这种方法意味着，在最坏的情况下，整个列表都将被搜索，这正是我们要找的元素（-100）不在列表上的情况。对于小列表，从顶部开始搜索并直接通过，只会增加微不足道的时间。但随着列表的增长，以及你可能需要在那些不断增长的列表上进行的搜索数量，时间会显著增加。
- en: At this stage, we have no numbers to compare against, but it’s safe to assume
    that ranges between a millisecond and even a microsecond are not very encouraging.
    This should be doable in orders-of-magnitude less time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们还没有可以比较的数字，但可以安全地假设在毫秒甚至微秒之间的范围并不令人鼓舞。这应该在数量级上少得多的时间内完成。
- en: 2.3.2 Searching using sets
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 使用集合进行搜索
- en: Let’s see whether we can do better by switching our data structure from lists
    to sets. Let’s convert our ordered list into a set and try to do a search
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以通过将数据结构从列表切换到集合来做得更好。让我们将有序列表转换为集合，并尝试进行搜索
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: with the time costs being
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 时间成本如下
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is several orders of magnitude faster than the solutions in the previous
    section! But why such an improvement? There are two main reasons: one is related
    to set size and another is related to complexity. The complexity part will be
    discussed in the next subsection. Here we’ll look at the role of set size.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这比上一节中的解决方案快几个数量级！但为什么会有这样的改进？主要有两个原因：一个是与集合大小相关，另一个是与复杂度相关。复杂度部分将在下一小节中讨论。这里我们将看看集合大小的作用。
- en: With regards to the size, remember that the original list had 141,082 elements.
    But with a set, all repeated values are collapsed into a single value—and there
    are plenty of repeated elements on the original list. The set size is reduced
    to `print(len(set_ first_all_temperatures))`, which is 400 elements (350 times
    fewer). No wonder searching is so much faster as the size of the structure is
    much smaller.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大小，记住原始列表有141,082个元素。但使用集合后，所有重复的值都会合并成一个单一值——原始列表中有很多重复的元素。集合的大小减少到`print(len(set_
    first_all_temperatures))`，即400个元素（减少了350倍）。难怪搜索速度如此之快，因为结构的大小要小得多。
- en: The takeaway is that we should be aware of possible repeated elements in a list
    and know that there are potential advantages of using sets so the search can happen
    on much smaller data structures. But there is also a more profound difference
    between the implementation of lists and sets in Python.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该意识到列表中可能存在的重复元素，并知道使用集合有潜在的优势，这样搜索就可以在更小的数据结构上发生。但Python中列表和集合的实现之间也存在更深刻的差异。
- en: 2.3.3 List, set, and dictionary complexity in Python
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 Python中列表、集合和字典的复杂度
- en: 'The improved performance from the previous example was mostly due to the de
    facto reduction in the size of the data structure when we switched from a list
    to a set. This begs the question: what would happen if there was no repetition,
    so both the list and the set were the same size? Let’s find out. We can simulate
    this with a range, which will specify that all elements will be different:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 之前示例中性能的提升主要归因于当我们从列表切换到集合时，数据结构的大小实际上减少了。这引发了一个问题：如果没有重复，列表和集合的大小相同会怎样？让我们找出答案。我们可以用范围来模拟这种情况，这将指定所有元素都将不同：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So now we have a range of 0 to 99,999 that is implemented as both a list and
    a set. We search both data structures for 50,000 and 500,000\. Here are the timings:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个从0到99,999的范围，它既实现了列表也实现了集合。我们在这两种数据结构中搜索了50,000和500,000。以下是时间：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The set implementation still has much better performance. That is because in
    Python (to be more precise, CPython) a set is implemented with a hash. Finding
    an element thus has the cost of searching a hash. Hash functions come in many
    flavors and have to deal with many design problems. But when comparing lists and
    sets, we can generally assume that set lookup is mostly constant and will perform
    as well with a collection of size 10 or 10 million. This is not actually correct,
    but it is reasonable for understanding, in an intuitive way, why set lookups compare
    favorably against list lookups.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 集合实现仍然有更好的性能。这是因为Python（更准确地说，CPython）中集合是通过哈希实现的。因此，查找一个元素的成本就是搜索哈希的成本。哈希函数有很多种类，需要处理许多设计问题。但是，当我们比较列表和集合时，我们可以一般假设集合查找主要是常数时间，并且对于大小为10或1000万的集合，性能都会很好。这实际上并不完全正确，但为了直观理解为什么集合查找比列表查找有优势，这是合理的。
- en: Remember also that a set is usually implemented like a dictionary, without values,
    which means that when you search on a dictionary key, you get the same performance
    as searching on a set. However, sets and dictionaries are not the silver bullets
    that they might seem here. For example, if you want to search an interval, an
    ordered list is substantially more efficient. In an ordered list, you can find
    the lowest element and then traverse from that point up until you find the first
    element above the interval and then stop. In a set or dictionary, you would have
    to do a lookup for each element in the interval. So if you know the value you
    are searching for, then a dictionary can be extremely fast. But if you are looking
    in an interval, then it suddenly stops being a reasonable option; an ordered list
    with a bisection algorithm would perform much better.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，集合通常像字典一样实现，没有值，这意味着当你在一个字典键上搜索时，你得到与在集合上搜索相同的性能。然而，集合和字典并不是他们看起来那么万能的银弹。例如，如果你想搜索一个区间，一个有序列表会大大更有效率。在一个有序列表中，你可以找到最小元素，然后从这个点开始遍历，直到你找到区间之上的第一个元素然后停止。在集合或字典中，你将不得不对区间中的每个元素进行查找。所以如果你知道你要搜索的值，那么字典可以非常快。但如果你在一个区间中查找，那么它突然就不再是合理的选择；一个带有二分查找算法的有序列表会表现得更好。
- en: Given that lists are so pervasive and easy to use in Python, there are many
    cases where more appropriate data structures exist. But it is worth stressing
    that lists are a fundamental data structure that has many good use cases. The
    point is to be mindful of your options, not to banish lists.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列表在Python中如此普遍且易于使用，存在许多更适合的数据结构。但强调列表是一个具有许多良好用例的基本数据结构是值得的。关键是注意你的选择，而不是摒弃列表。
- en: Tip Be careful when using `in` to search inside large lists. If you browse through
    Python code, the pattern of using `in` to find elements in a list (the method
    `index` of list objects is, in practice, the same thing) is quite common. This
    is not a problem for small lists as the time penalty is quite small and perfectly
    reasonable, but it can be serious with large lists.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在使用`in`搜索大型列表时要小心。如果你浏览Python代码，使用`in`在列表中查找元素的模式（列表对象的`index`方法在实践中是同一件事）相当常见。对于小列表来说，时间惩罚相当小，完全合理，但对于大列表来说，可能会很严重。
- en: From a very down-to-earth software engineering perspective, the use of `in`
    with lists can go from an unnoticed problem in development to a massive problem
    in production. The common pattern is a developer testing with small data examples,
    because feeding big data is normally not practical with most unit testing. The
    real data might be very large, however, and once it’s introduced, it could bring
    a production system to a halt.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个非常实际的软件工程角度来看，使用`in`与列表一起使用可能会从开发中的未注意问题变成生产中的重大问题。常见的模式是开发者使用小的数据示例进行测试，因为在大多数单元测试中，提供大数据通常不切实际。然而，实际数据可能非常大，一旦引入，它可能会使生产系统停止运行。
- en: A more systematic solution would be to test the code—maybe not always but at
    least from time to time—with very large data sets. This can occur in different
    stages of testing, from unit to end-to-end testing. This should not be construed
    as an argument against using `in` with lists. Just be mindful of the discrepancies
    between performance during development and production due to data size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更系统的解决方案是在不同的测试阶段——从单元测试到端到端测试——用非常大的数据集测试代码——也许不是总是，但至少偶尔。这不应该被理解为反对使用列表中的`in`的论点。只是要注意由于数据大小导致的开发期间和生产期间性能差异。
- en: 'By the way, for most searching operations, there is a substantially better
    family of data structures than lists, sets, or dictionaries: trees. But in this
    chapter, we are evaluating Python’s built-in data structures, which do not include
    trees.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，对于大多数搜索操作，比列表、集合或字典有实质上更好的数据结构家族：树。但在本章中，我们正在评估Python的内置数据结构，这些数据结构不包括树。
- en: The whole topic of choosing appropriate algorithms and data structures is the
    subject of many books and often makes up some of the most difficult courses for
    a computer science degree. The point is not to have an exhaustive discussion of
    the topic but to make you aware of the most common alternatives in Python. If
    you believe existing Python data structures are not enough for your needs, you
    may want to consider other types of data structures. This book’s focus is on Python,
    but other resources will cover data structures outside of Python; for example,
    *Data Structures and Algorithms in Python*, by Michael T. Goodrich, Roberto Tamassia,
    and Michael H. Goldwasser (Wiley 2013), provides a good introduction.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的算法和数据结构是许多书籍的主题，也是计算机科学学位课程中最难的部分之一。这里的目的不是对这一主题进行详尽的讨论，而是让你了解Python中最常见的替代方案。如果你认为现有的Python数据结构不能满足你的需求，你可能需要考虑其他类型的数据结构。本书的重点是Python，但其他资源将涵盖Python之外的数据结构；例如，Michael
    T. Goodrich、Roberto Tamassia和Michael H. Goldwasser所著的《Python中的数据结构和算法》（Wiley 2013年出版），提供了良好的介绍。
- en: Another helpful resource is Python’s own data on TimeComplexity ([https://wiki.python.org/moin/TimeComplexity](https://wiki.python.org/moin/TimeComplexity)).
    Here you can look up the time complexity of a wide range of operations over many
    Python data structures.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的资源是Python自己的关于时间复杂性的数据（[https://wiki.python.org/moin/TimeComplexity](https://wiki.python.org/moin/TimeComplexity)）。在这里，你可以查找许多Python数据结构上广泛操作的复杂度。
- en: 'So far in this chapter we have focused on time performance. But that is not
    the only factor when dealing with performance problems with large data sets. Let’s
    turn to another important factor: conserving memory.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直关注时间性能。但处理大数据集的性能问题时，这并不是唯一因素。让我们转向另一个重要因素：节省内存。
- en: 2.4 Finding excessive memory allocation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 寻找过度的内存分配
- en: Memory consumption can be crucial for performance, and it’s not just that you
    might run out of memory. Effective memory allocation can allow for more processes
    to be run in parallel on the same machine. Even more significantly, judicious
    memory use might allow for in-memory algorithms.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 内存消耗对于性能至关重要，而不仅仅是可能耗尽内存。有效的内存分配可以允许在同一台机器上并行运行更多的进程。更重要的是，合理的内存使用可能允许内存中的算法。
- en: Let’s return to our familiar scenario, the NOAA database, to see how we can
    reduce the disk consumption of our data. To do this, we will start with a study
    of the content of the data files. Our objective here is to load a few of those
    files and do some statistics on character distributions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们熟悉的场景，NOAA数据库，看看我们如何减少数据的磁盘消耗。为此，我们将从研究数据文件的内容开始。我们的目标是加载一些这些文件并对字符分布进行一些统计分析。
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`all_files` now has a dictionary where each item contains the contents for
    all the files related to a station. Let’s study the memory usage of this.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`all_files`现在有一个字典，其中每个条目包含与一个站点相关的所有文件的内容。让我们研究一下这个字典的内存使用情况。'
- en: 2.4.1 Navigating the minefield of Python memory estimation
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 在Python内存估计的雷区中导航
- en: 'Python provides a function in the `sys` module, `getsizeof`, that supposedly
    returns the memory occupied by an object. We can get an understanding of the memory
    occupied by our dictionary using the following code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Python在`sys`模块中提供了一个名为`getsizeof`的函数，该函数据说返回对象占用的内存。我们可以使用以下代码了解我们的字典占用的内存：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`getsizeof` might not return what you expect. The files on the disk are in
    the megabyte range, so estimates below 1 KB sound quite suspicious. `getsizeof`
    is actually returning the size of the containers (the first is a dictionary, the
    second is an iterator, and the third is a list) *without* accounting for the content.
    So, we have to account for two things occupying memory: the content of the container
    and the container itself.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`getsizeof`可能不会返回你所期望的结果。磁盘上的文件大小在兆字节范围内，所以低于1 KB的估计听起来相当可疑。实际上，`getsizeof`返回的是容器的大小（第一个是字典，第二个是迭代器，第三个是列表）*而不包括*内容。因此，我们必须考虑占用内存的两个方面：容器的内容和容器本身。'
- en: Note Note that there is no problem with the `getsizeof` implementation in the
    language; it is just that the expectation of an unsuspecting user is typically
    of something different—namely, that it would return the memory footprint of everything
    referred in the object. If you read the official documentation, you will even
    find instructions for a recursive implementation that solves most problems. For
    us, the intricacies of `getsizeof` are mostly a starting point to discuss CPython
    memory allocation in depth.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，语言中`getsizeof`的实现没有问题；只是出乎意料的使用者通常期望的是不同的东西——即它会返回对象中引用的所有内容的内存占用。如果你阅读官方文档，你甚至可以找到递归实现的说明，这种实现可以解决大多数问题。对我们来说，`getsizeof`的复杂性主要是一个深入讨论CPython内存分配的起点。
- en: 'Let’s get some basic information about our station data:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取一些关于我们站点数据的基本信息：
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Our dictionary has only one entry, corresponding to a single station. It contains
    a list with 17 entries. The list itself takes 248 bytes, but remember, that doesn’t
    include the content. Now let’s inspect the size of the first entry:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的字典只有一个条目，对应一个单独的站点。它包含一个包含17个条目的列表。列表本身占用248字节，但请记住，这并不包括内容。现在让我们检查第一个条目的大小：
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The length is 1,303,981, corresponding to the size of the file. We have a `getsizeof`
    of 10,431,904\. This is around eight times the size of the underlying file. Why
    eight times? Because each entry is a pointer to a character, and a pointer is
    8 bytes in size. At this stage, this looks quite bad, as we have a large data
    structure, and we haven’t yet accounted for the data proper. Let’s have a look
    at a single character:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为1,303,981，对应文件的大小。我们有`getsizeof`为10,431,904。这大约是底层文件大小的八倍。为什么是八倍？因为每个条目都是一个指向字符的指针，而指针的大小是8字节。在这个阶段，这看起来相当糟糕，因为我们有一个大的数据结构，但我们还没有计算实际的数据。让我们看看单个字符：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This is colossal in size. The output is 28 with a type of `int`. So every character,
    which should take only one 1 byte, is represented by 28 bytes. Hence, we have
    10,431,904 for the size of the list plus 28 * 1,303,981 (36,511,468) for a grand
    total of 46,943,372\. This is 36 times bigger than the original file! Fortunately,
    the situation is not as bad as it seems, but we can do much better. We will start
    by seeing that Python (or rather, CPython) is quite smart with memory allocation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这在大小上非常巨大。输出是28，类型为`int`。所以每个字符，本应只占用1个字节，现在却由28个字节表示。因此，列表的大小为10,431,904，加上28
    * 1,303,981（36,511,468），总计为46,943,372。这是原始文件大小的36倍！幸运的是，情况并没有看起来那么糟糕，但我们能做得更好。我们将从看到Python（或者说CPython）在内存分配方面相当智能开始。
- en: 'CPython can allocate objects in a more sophisticated way, and it turns out
    that our approach to computing memory allocation is quite naive. Let’s compute
    the size of only the inner content, but instead of going through all the integers
    in our matrix, we will make sure that we are not double-counting. In Python, if
    an object is used many times, it gets the same `id`. So if we see the same `id`
    many times, we should only count a single memory allocation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CPython可以以更复杂的方式分配对象，结果我们的内存分配计算方法相当天真。让我们只计算内部内容的大小，但不是通过遍历矩阵中的所有整数，而是确保我们不会重复计数。在Python中，如果一个对象被多次使用，它将获得相同的`id`。所以如果我们看到相同的`id`多次，我们应该只计算一个内存分配：
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① The id function allows us to get the unique ID of an object.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ①`id`函数允许我们获取对象的唯一ID。
- en: The previous code gets the unique identifier for all of our numbers. In CPython,
    that happens to be memory location. CPython is smart enough to see that the same
    string content is being used over and over again—remember that each ASCII character
    is represented by an integer between 0 and 127—and, as such, the output of the
    previous code is 46.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码获取了我们所有数字的唯一标识符。在CPython中，这实际上是内存位置。CPython足够智能，能够看到相同的字符串内容被反复使用——记住，每个ASCII字符都由一个介于0到127之间的整数表示——因此，之前代码的输出是46。
- en: So, dumb allocation of memory would be dreadful, but Python (or, to be more
    precise, CPython) is much smarter. The memory cost of this solution is just the
    list infrastructure (10,431,904). Note that in our case, we only have 46 distinct
    characters; with such a small subset, Python is quite good at smart memory allocation.
    Do not expect this best-case scenario to occur at all times because it will depend
    on your data pattern.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简单的内存分配将会很糟糕，但Python（更准确地说，CPython）要聪明得多。这种解决方案的内存成本仅仅是列表基础设施（10,431,904）。请注意，在我们的情况下，我们只有46个不同的字符；对于如此小的子集，Python在智能内存分配方面相当出色。不要期望这种情况总是发生，因为这将取决于你的数据模式。
- en: Object caching and reuse in Python
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的对象缓存和复用
- en: Python tries to be as smart as possible with object reuse, but we need to be
    careful with expectations. The first reason is that this is *implementation-dependent*.
    CPython is different from other Python implementations in terms of this behavior.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Python试图在对象复用方面尽可能聪明，但我们需要对期望保持谨慎。第一个原因是这*依赖于实现*。CPython在这一点上与其他Python实现不同。
- en: Another reason is that even CPython makes no promises about most of its allocation
    policies from version to version. What works for your specific version might change
    in a different version.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是，即使是CPython，在版本之间对其大多数分配策略也没有做出任何承诺。适用于您特定版本的方法可能在不同的版本中会发生变化。
- en: 'Finally, even if you have a fixed version, how things work might not be completely
    obvious. Consider this code in Python 3.7.3 (this might vary on other versions):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，即使你有固定的版本，事情是如何工作的可能并不完全明显。考虑以下Python 3.7.3中的代码（在其他版本中可能会有所不同）：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Here we are getting the string aa by multiplying a times 2.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这里我们通过将a乘以2来得到字符串aa。
- en: ② Here we are getting the string aa by multiplying a times s which is 2.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ② 这里我们通过将a乘以s（即2）来得到字符串aa。
- en: ③ All these strings are equal in terms of content.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 所有这些字符串在内容上是相等的。
- en: 'The result will be:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With the size of the string as a variable, the allocator is not able to determine
    that the content is the same, even if the size is the same. If this simple example
    works like this, what about more complicated cases? Of course, you can still use
    knowledge of how the allocator works, and for code, you have control over the
    Python version, this makes special sense. But adjust your expectations accordingly.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当字符串的大小作为变量时，分配器无法确定内容是否相同，即使大小相同。如果这个简单的例子是这样的，那么更复杂的情况会怎样？当然，你仍然可以使用对分配器工作方式的知识，对于代码，你控制着Python版本，这使得这一点特别有意义。但相应地调整你的期望。
- en: We are using a file representation based on a list of numbers. What if we considered
    alternative representations?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用基于数字列表的文件表示法。如果我们考虑替代表示法会怎样呢？
- en: 2.4.2 The memory footprint of some alternative representations
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 一些替代表示法的内存占用
- en: 'We are now going to consider some simple alternatives to representing a file.
    Some will be better; some will be worse. The main point here is to understand
    the underlying cost of each alternative. Instead of using integers to represent
    each character, we could use strings of length 1—something like this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将考虑一些表示文件的简单替代方案。有些会更好；有些会更差。这里的主要目的是理解每种替代方案的成本。我们不是用整数来表示每个字符，而是可以使用长度为1的字符串——类似于这样：
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This approach would even be worse than the one that we used before. Just look
    at the size of each string with a single character:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法甚至比我们之前使用的方法更差。只需看看每个字符串的大小：
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This returns 50, whereas the representation for our previous integer representation
    was only 28\. This is a step backward, so we won’t be doing it.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回50，而之前整数表示法的表示只有28。这是一个退步，所以我们不会这样做。
- en: Python object overheads are quite bad with lots of small objects. Why do small
    numbers require 28 bytes and single character strings, 50 bytes? It turns out
    that every Python object requires at least 24 bytes of overhead, and to that overhead,
    you have to add the overhead of the object type, which will vary from type to
    type. As we have seen, it’s bigger for strings than byte arrays (figure 2.2).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Python中大量小对象的内存开销相当大。为什么小数字需要28字节，单个字符字符串需要50字节？实际上，每个Python对象至少需要24字节的内存开销，并且你必须将对象类型的开销加到这个开销上，这会因类型而异。正如我们所看到的，字符串的开销比字节数组大（图2.2）。
- en: The internal representation of strings and numbers
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串和数字的内部表示
- en: 'Python has an efficient internal representation for strings, which can vary
    and thus confuse expectations about memory allocation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Python 对字符串有一个高效的内部表示，它可以变化，因此可能会让人对内存分配的预期感到困惑：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output would be:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The empty string takes 49 bytes; the `c` string takes 50; 10,000 `c`’s take
    10049 bytes. So far so good. But a c with a cedilla takes 74 and 10,000 `ç`’s
    take 10,073\. If you are a bit confused now, know that a single confused smiley
    takes 80 bytes and 10,000 of those take 40,076 bytes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 空字符串占用 49 字节；`c` 字符串占用 50 字节；10,000 个 `c` 字符串占用 10049 字节。到目前为止，一切看起来都很好。但是，带有
    cedilla 的 c 字符串占用 74 字节，10,000 个 `ç` 字符串占用 10,073 字节。如果你现在有点困惑，知道一个单独的困惑表情符号占用
    80 字节，10,000 个这样的表情符号占用 40,076 字节。
- en: 'Python 3 strings represent Unicode characters, but there is a nuance: the internal
    representation is optimized as a function of the string being represented. For
    details, you can see PEP 393--*Flexible String Representation*. For Latin-1 characters
    (a superset of ASCII), Python uses 1 byte (the c with a cedilla is part of that
    set), but for other types of characters it can take up to 4 bytes (in the case
    of our confused emoji). But from our perspective, string sizes are difficult to
    calculate.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3 字符串表示 Unicode 字符，但有一个细微差别：内部表示是作为表示的字符串的函数进行优化的。有关详细信息，请参阅 PEP 393--*灵活的字符串表示*。对于
    Latin-1 字符（ASCII 的超集），Python 使用 1 字节（带有 cedilla 的 c 字符串是那个集合的一部分），但对于其他类型的字符，它可能需要多达
    4 字节（在我们的困惑表情符号的情况下）。但从我们的角度来看，字符串的大小很难计算。
- en: Integers have also an optimized implementation. The precision is arbitrary,
    but for signed integers fitting 30 bits, we get the smaller representation possibility
    of 28 bytes (the number 0 is an exception; it is represented by 24 bytes only,
    which you might recall is the smallest object size possible due to CPython’s object
    overhead).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 整数也有一个优化的实现。精度是任意的，但对于适合 30 位的有符号整数，我们得到较小的表示可能性，即 28 字节（数字 0 是一个例外；它只由 24 字节表示，你可能还记得这是由于
    CPython 的对象开销而可能的最小对象大小）。
- en: '![](../Images/CH02_F02_Antao.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F02_Antao.png)'
- en: Figure 2.2 Object overhead for strings and bytes
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 字符串和字节的对象开销
- en: 'There is a more obvious representation for a file: instead of using a list
    of one-character strings, we can use *a string with the whole file*:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文件，有一个更明显的表示方式：我们不需要使用一个字符字符串的列表，而可以使用*整个文件的字符串*：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This will be a size of 1,304,030—the size of our file plus the string object
    overhead. While this is an obvious and simple solution, we will continue with
    the approach of containers of sequences of bytes because, as it turns out, those
    approaches can still be improved.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是 1,304,030 字节的大小——我们文件的大小加上字符串对象的开销。虽然这是一个明显且简单的解决方案，但我们将继续使用字节序列容器的方案，因为事实证明，这些方案仍然可以改进。
- en: 2.4.3 Using arrays as a compact representation alternative to lists
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 使用数组作为列表紧凑表示的替代方案
- en: 'Here we will look at how an alternative container to elements might be substantially
    more efficient in terms of memory: arrays. Let’s revisit the implementation of
    our `get_all_files` function:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨一个替代元素容器的内存效率可能显著更高的方法：数组。让我们重新审视我们的 `get_all_files` 函数的实现：
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ① The original implementation had content = list(f.read()).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ① 原始实现是 content = list(f.read())。
- en: 'The line `content = list(f.read())` was converting the output of the `read`
    function into a list. Now, we implemented it without the list call, returning
    a byte array. Let’s check the object size:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码 `content = list(f.read())` 将 `read` 函数的输出转换成了一个列表。现在，我们没有使用列表调用来实现它，而是返回了一个字节数组。让我们检查对象的大小：
- en: '[PRE44]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The type is `bytes`, and the size including data is 1,304,014.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 类型是 `bytes`，包括数据在内的总大小是 1,304,014 字节。
- en: 'Arrays are of fixed size and can only contain objects of the same type. Hence,
    their representation can be made much more compact: it can be stored with the
    object overhead. Recall that for our integers, there was a 28-byte size for a
    storage of, really, a single byte of data.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 数组的大小是固定的，并且只能包含相同类型的对象。因此，它们的表示可以做得更加紧凑：它可以与对象开销一起存储。回想一下，对于我们的整数，存储一个实际只有
    1 字节数据的存储空间需要 28 字节。
- en: Memory occupation in lists
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的内存占用
- en: When you allocate a list, Python creates an extra space for potential future
    additions, so the list will normally have more space than you expect. This makes
    insertions substantially cheaper because there is no need to allocate memory every
    time a new element is added—just when the extra space allocated is exhausted.
    The cost, of course, is the memory overhead. As a rule, such overhead is not serious
    unless you have lots of tiny lists; that is, the “lots of tiny objects” argument
    is especially true for lists. While knowing this is interesting, the overhead
    is normally OK with all other cases.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当你分配一个列表时，Python会为潜在的将来添加创建额外的空间，因此列表通常比预期的空间要多。这使得插入操作的成本大大降低，因为不需要在每次添加新元素时分配内存——只需当分配的额外空间耗尽时。当然，成本是内存开销。一般来说，这种开销并不严重，除非你有大量的微小列表；也就是说，“大量微小对象”的论点对于列表尤其正确。虽然了解这一点很有趣，但通常情况下，这种开销是可以接受的。
- en: Much of the code related to array management is available in the `array` module.
    Except for this chapter, however, we won’t be using the `array` module anymore;
    instead, we will use NumPy, which supersedes it in many ways. But the point here
    has less to do with the module and more with understanding and getting rid of
    the object overhead.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与数组管理相关的代码大多可在`array`模块中找到。然而，除了本章之外，我们不会再使用`array`模块；相反，我们将使用NumPy，它在许多方面都取代了它。但这里的重点与模块本身关系不大，更多的是理解和消除对象开销。
- en: At this stage, you should have an insight into the costs and pitfalls of object
    memory allocation in Python. Finally, we’ll now try to understand how to compute
    the memory usage of Python objects.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你应该已经对Python中对象内存分配的成本和陷阱有了深刻的认识。最后，我们现在将尝试理解如何计算Python对象的内存使用情况。
- en: '2.4.4 Systematizing what we have learned: Estimating memory usage of Python
    objects'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 系统化我们所学的知识：估计Python对象的内存使用
- en: At this stage, you have the basis to understand how memory allocation works.
    Now that you have a grasp of the underlying principles, we will try to devise
    some code to allow us to gather all the knowledge of the previous section into
    a utility function that gives a good approximation of the memory footprint.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你有了理解内存分配工作原理的基础。现在，你已经掌握了基本原理，我们将尝试编写一些代码，使我们能够将上一节的所有知识汇总到一个实用函数中，该函数可以给出良好的内存占用近似值。
- en: We’ll now distill all the tidbits that we learned in the rest of the section.
    In the following discussion, we’ll write a function that will return the estimated
    memory size of an object. It will return both the size of all the objects along
    with the expenditure on containers. If you look at the following code, you should
    be able to find ID tracking, container counting (including mapper objects like
    dictionaries where we need to track both the key and the value), and string and
    array management.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将提炼本节其余部分所学的所有零散知识。在接下来的讨论中，我们将编写一个函数，该函数将返回对象的估计内存大小。它将返回所有对象的大小以及容器上的开销。如果你查看以下代码，你应该能够找到ID跟踪、容器计数（包括需要跟踪键和值的映射对象，如字典），以及字符串和数组管理。
- en: Computing the size of general objects is a veritable minefield (it is actually
    not possible in general for external objects using Python only approaches). Our
    code in listing 2.2 tries to be smart by not double-counting repeated objects
    and containers/iterators that report the full size of the container and the content
    (like strings or arrays; the code can be found in `02-python/sec4-memory/compute_allocation.py`).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 计算通用对象的大小实际上是一个真正的雷区（在一般情况下，仅使用Python方法实际上是不可能的）。我们列表2.2中的代码试图通过不重复计算重复的对象和报告容器和内容全尺寸的容器/迭代器（如字符串或数组；代码可在`02-python/sec4-memory/compute_allocation.py`中找到）来变得聪明。
- en: Listing 2.2 Computing the size of general Python objects
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 计算通用Python对象的大小
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ① We need to store the IDs of previously seen objects to avoid double-counting
    them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们需要存储先前看到的对象的ID，以避免重复计算它们。
- en: ② We will also return the memory spent in containers like lists or dictionaries.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们还将返回在列表或字典等容器中花费的内存。
- en: ③ Strings and arrays are iterables that return the size of their content; we
    do not want to double-count the content.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 字符串和数组是可迭代的，它们返回它们内容的大小；我们不希望重复计算内容。
- en: ④ We will ignore the content of generators.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们将忽略生成器的内容。
- en: ⑤ For maps, we will need to count the keys and the values.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 对于映射，我们需要计算键和值。
- en: ⑥ Finally, for other iterators, we will need to check the size.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 最后，对于其他迭代器，我们需要检查其大小。
- en: Here we are using an iterative approach to compute memory allocation. This is
    a type of algorithm that would have lent itself to a recursive implementation,
    but due to Python’s lack of proper support for good tail call optimization and
    recursive implementations in general, we will use an iterative approach.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用迭代方法来计算内存分配。这是一种适合递归实现的算法，但由于Python缺乏对良好尾调用优化和递归实现的适当支持，我们将使用迭代方法。
- en: Computing the size of objects from external libraries that are implemented in
    a system programming language like C or Rust will mostly depend on the implementation
    making that information available in some form. For those libraries, consult the
    documentation for details.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从使用系统编程语言如C或Rust实现的系统库中计算对象大小将主要取决于该实现以某种形式提供这些信息。对于这些库，请查阅文档以获取详细信息。
- en: Warning There are memory profiler libraries for Python that you could try to
    use instead. I have a mixed experience with the reliability of estimates from
    some of the tools available, which is not shocking due to the minefield that is
    memory estimation in Python. If you use them, be careful.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：有Python内存分析库可供尝试使用。我对一些可用工具的估计可靠性有混合的经验，这在Python内存估计的雷区中并不令人惊讶。如果您使用它们，请小心。
- en: There are more lower-level ways to check the memory allocation of Python, but
    we will discuss those when we use NumPy. In this chapter, we restrict ourselves
    to Python without external libraries.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Python内存分配还有更多底层方法，但我们将在我们使用NumPy时讨论这些方法。在本章中，我们限制自己使用Python而不使用外部库。
- en: '2.4.5 The takeaway: Estimating memory usage of Python objects'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 要点：估计Python对象内存使用量
- en: 'To summarize, estimating the size of memory objects is not as easy as one might
    expect. `sys.getsizeof` doesn’t report all the object sizes, and as such, extra
    effort is needed to accurately compute object sizes. In the general case, the
    problem is not even solvable: libraries written in low-level languages might not
    report the size of the allocations that they do.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，估计内存对象的大小并不像人们预期的那样简单。`sys.getsizeof`不会报告所有对象的大小，因此需要额外努力才能准确计算对象大小。在一般情况下，这个问题甚至无法解决：用低级语言编写的库可能不会报告它们所做的分配的大小。
- en: Lean memory allocation has several side advantages. One is allowing the run
    of more parallel processes in cases where memory is the limiting factor, as it
    sometimes is. Another advantage is that it may create room for using in-memory
    algorithms, which are faster than algorithms, which need disk space and are much
    slower due to disk access.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 精简内存分配有几个副作用。一个是允许在内存是限制因素的情况下运行更多并行进程，因为有时就是这样。另一个优点是，它可能为使用内存算法腾出空间，这些算法比需要磁盘空间且由于磁盘访问而速度慢得多的算法要快。
- en: 2.5 Using laziness and generators for big-data pipelining
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 使用延迟和生成器进行大数据管道
- en: 'We now shift our attention to a feature that was extensively introduced with
    Python 3: lazy semantics. Lazy semantics delays any computation until the data
    is required and not before. This is extremely helpful to process large amounts
    of data, as sometimes computation (and related memory allocation) doesn’t need
    to be done or can be spread over time. If you use generators, you are using lazy
    semantics already. Python 3 is way lazier than Python 2 as functions like `range`,
    `map`, and `zip` became lazy. A lazy approach will allow you to process more data,
    typically with substantially less memory, and permit the creation of data pipelines
    inside the code in a much easier way.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转向Python 3中广泛引入的一个特性：延迟语义。延迟语义将任何计算推迟到数据需要时再进行，而不是在此之前。这对于处理大量数据非常有帮助，因为有时计算（和相关内存分配）不需要进行，或者可以分散到一段时间内进行。如果您使用生成器，您已经在使用延迟语义。Python
    3比Python 2懒惰得多，因为`range`、`map`和`zip`等函数变成了延迟的。懒惰方法将允许您处理更多数据，通常内存使用量会显著减少，并且可以更轻松地在代码中创建数据管道。
- en: 2.5.1 Using generators instead of standard functions
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 使用生成器而不是标准函数
- en: 'Let’s revisit the original code of the first section of this chapter:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾本章第一节的原始代码：
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① A yield in a definition indicates a generator.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义中的`yield`表示生成器。
- en: '`get_file_temperatures` is a generator (notice the `yield`). Let’s run the
    generator:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_file_temperatures`是一个生成器（注意`yield`）。让我们运行这个生成器：'
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The type reported will be `generator`, and the size of the structure will be
    112\. In reality, not much was done as generators are lazy. Only when you start
    iterating through it will the code execute as needed:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的类型将是 `generator`，结构的大小将是 112。实际上，由于生成器是惰性的，并没有做太多事情。只有当你开始迭代它时，代码才会根据需要执行：
- en: '[PRE48]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ① Every time the for loop is repeated, the generator code will be called to
    provide a new value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ① 每次for循环重复时，生成器代码将被调用以提供新的值。
- en: There are several advantages of this approach. The first, and biggest, one is
    that you will not need to have memory allocated for all temperatures as each one
    will be processed in turn. Contrast this with a list where you need memory to
    maintain all the temperatures at the same time. This can be quite important when
    a function returns very large data structures with many elements—the difference
    between having enough memory to execute the code or not.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有几个优点。首先，也是最大的优点是，你不需要为所有温度分配内存，因为每个温度都会依次处理。与此相对比的是，列表需要内存来同时维护所有温度。这在函数返回包含许多元素的大型数据结构时可能非常重要——这可能是代码能否执行的关键。
- en: 'Second, sometimes we do not need to get all the results, and as such, being
    eager just spends time in useless computation. Imagine, for example, that you
    wanted to write a function to see whether there is at least one temperature below
    zero. You don’t need to get all results: computation can stop as soon as a single
    value is below zero.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，有时我们不需要得到所有结果，因此急切地执行只会浪费时间。例如，假设你想编写一个函数来查看是否至少有一个温度低于零。你不需要得到所有结果：一旦有一个值低于零，计算就可以停止。
- en: 'It’s quite trivial to make an eager version of a generator, as simple as:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 制作生成器的急切版本非常简单，就像这样：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In this case, you lose the advantage of generators, but there are situations
    where that might be useful. For example, when the compute time is not long and
    the memory used by the list representation is tolerable, then in circumstances
    where you need to visit the results many times, an eager version makes more sense.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你失去了生成器的优势，但有时这可能是有用的。例如，当计算时间不长且列表表示法使用的内存可容忍时，在需要多次访问结果的情况下，一个急切版本更有意义。
- en: Note One of the biggest differences between Python 2 and Python 3 is that many
    built-ins that were eager became lazy. For example, in our case, `zip`, `map`,
    and `filter` would behave in very different ways in Python 2
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Python 2 和 Python 3 之间最大的区别之一是许多原本急切的内置函数变成了惰性的。例如，在我们的案例中，`zip`、`map` 和
    `filter` 在 Python 2 中的行为会有很大的不同。
- en: Generators can be used to reduce the memory footprint and, in some cases, compute
    time. So when you are writing code that returns sequences, ask yourself whether
    it makes sense to convert it to a generator.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器可以用来减少内存占用，在某些情况下，还可以减少计算时间。所以当你编写返回序列的代码时，问问自己是否将其转换为生成器是有意义的。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Detection of performance bottlenecks is not easy to do in an intuitive, nonempirical
    way. Profiling is the necessary first step to be able to find exactly where performance
    lacks. “Gut feelings” tend to be wrong when finding performance problems, and
    empirical approaches almost always win.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以直观、非经验的方式检测性能瓶颈并不容易。分析是找到性能缺乏的确切位置的必要第一步。“直觉”在寻找性能问题时往往是不正确的，而经验方法几乎总是获胜。
- en: Python’s internal profiling system is very useful, but it is sometimes difficult
    to interpret. Visualization tools like SnakeViz can help us make sense of profiling
    information.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的内部分析系统非常有用，但有时很难解释。像 SnakeViz 这样的可视化工具可以帮助我们理解分析信息。
- en: Python’s internal profiling system has substantial limitations in helping us
    find the exact spot where a bottleneck occurs. Tools like line_profiler can be
    substantially more precise at the expense of running very slowly to collect information.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的内部分析系统在帮助我们找到瓶颈发生的确切位置方面存在重大局限性。像 line_profiler 这样的工具在收集信息时运行速度非常慢，但可以提供更高的精确度。
- en: While CPU performance is typically our first port of call for performance optimization,
    memory usage is equally important and can have major indirect benefits. For example,
    a solution that has poor memory optimization and requires an out-of-memory algorithm
    may sometimes be replaced with a fully in-memory approach, producing enormous
    time gains.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 CPU 性能通常是我们的性能优化的首要考虑，但内存使用同样重要，并且可以产生重大的间接效益。例如，一个内存优化不良且需要内存不足算法的解决方案有时可以被完全内存方法所取代，从而产生巨大的时间收益。
- en: Python provides basic data structures that can be used and misused to affect
    performance. For example, searching for elements in unordered lists can become
    quite expensive. We have to be mindful of the complexity cost of many operations
    over Python’s basic data structures. These data structures appear in all Python
    programs and are typically low-hanging fruit that can have a massive effect on
    performance.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python提供了基本的数据结构，这些结构可以被正确使用或误用，从而影响性能。例如，在无序列表中搜索元素可能会变得相当昂贵。我们必须注意Python基本数据结构上许多操作的复杂度成本。这些数据结构出现在所有Python程序中，通常是低垂的果实，可以对性能产生巨大影响。
- en: Having a basic understanding of the computational complexity—Big-O notation—of
    Python’s data structures is crucial for writing efficient code. Be sure to check
    these from time to time as Python versions change, and sometimes the underlying
    implementation is replaced, thus changing the performance of the algorithm.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Python数据结构的计算复杂度——大O符号——有基本的了解对于编写高效代码至关重要。确保定期检查这些内容，因为Python版本的变化可能会替换底层实现，从而改变算法的性能。
- en: Lazy programming techniques allow us to develop programs that tend to have smaller
    memory footprints. They sometimes also make it possible to outright avoid large
    parts of a computation.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒惰的编程技巧使我们能够开发出内存占用较小的程序。它们有时还可能使我们完全避免计算中的大部分内容。
- en: All the content of this chapter is of wide applicability—both profiling and
    pure Python optimizations—and it can be used before any techniques discussed in
    the rest of the book.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的所有内容都具有广泛的应用性——既包括性能分析，也包括纯Python优化——并且可以在本书其余部分讨论的任何技术之前使用。
