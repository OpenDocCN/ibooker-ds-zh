- en: 9 Stateful applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 状态化应用
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The Kubernetes constructs used to represent disks and state
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 用于表示磁盘和状态的结构
- en: Adding persistent storage to Pods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将持久存储添加到 Pods
- en: Deploying a multiple-Pod stateful application with a leader role using StatefulSet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 StatefulSet 以具有领导者角色的方式部署多 Pod 状态化应用
- en: Migrating and recovering data by relinking Kubernetes objects to disk resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将 Kubernetes 对象重新链接到磁盘资源来迁移和恢复数据
- en: Giving Pods large ephemeral storage volumes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Pods 提供大型的临时存储卷
- en: Stateful applications (i.e., workloads that have attached storage) finally have
    a home with Kubernetes. While state*less* applications are often lauded for their
    ease of deployment and high scalability, helped greatly by avoiding the need to
    attach and manage storage, that doesn’t mean that state*ful* applications don’t
    have their place. Whether you’re deploying a sophisticated database or are migrating
    an old stateful application from a virtual machine (VM), Kubernetes has you covered.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 状态化应用（即具有附加存储的工作负载）终于在 Kubernetes 中找到了归宿。虽然无状态应用因其部署简便性和高可扩展性而常受到赞誉，这得益于无需附加和管理存储的需求，但这并不意味着状态化应用没有其位置。无论你是部署复杂的数据库还是将旧状态化应用从虚拟机（VM）迁移过来，Kubernetes
    都能为你提供支持。
- en: Using persistent volumes, you can attach stateful storage to any Kubernetes
    Pod. When it comes to multi-replica workloads with state, just as Kubernetes offers
    Deployment as a high-level construct for managing a stateless application, StatefulSet
    exists to provide high-level management of stateful applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用持久卷，您可以将状态化存储附加到任何 Kubernetes Pod。当涉及到具有状态的多副本工作负载时，正如 Kubernetes 提供了 Deployment
    作为管理无状态应用的高级结构一样，StatefulSet 存在就是为了提供状态化应用的高级管理。
- en: 9.1 Volumes, persistent volumes, claims, and storage classes
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 卷、持久卷、持久卷声明和存储类
- en: To get started with storing state in Kubernetes, there are a few concepts around
    volume (disk) management to cover before moving on to the higher-level StatefulSet
    construct. Just like nodes are the Kubernetes representation of a VM, Kubernetes
    has its own representation of disks as well.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 中开始存储状态，在继续到更高层次的状态化集结构之前，需要了解一些关于卷（磁盘）管理的基本概念。就像节点是 Kubernetes
    对 VM 的表示一样，Kubernetes 也有自己的磁盘表示。
- en: 9.1.1 Volumes
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 卷
- en: 'Kubernetes offers functionality to Pods that allows them to mount a volume.
    What’s a *volume*? The docs describe it like so:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 为 Pods 提供了功能，允许它们挂载卷。什么是 *卷*？文档这样描述它：
- en: At its core, a volume is just a directory, possibly with some data in it, which
    is accessible to the containers in a Pod. How that directory comes to be, the
    medium that backs it, and the contents of it are determined by the particular
    volume type used[¹](#pgfId-1097998).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，卷只是一个目录，可能包含一些数据，这些数据对 Pod 中的容器是可访问的。这个目录是如何形成的，支持它的介质以及它的内容是由特定卷类型使用的[¹](#pgfId-1097998)决定的。
- en: Kubernetes ships with some built-in volume types, and others can be added by
    your platform administrator via storage drivers. One type you may encounter frequently
    is `emptyDir`, an ephemeral volume tied to the lifecycle of the node, ConfigMap,
    which allows you to specify files in Kubernetes manifests and present them to
    your application as a file on disk, and cloud provider disks for persistent storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 随带一些内置的卷类型，其他类型可以通过您的平台管理员通过存储驱动程序添加。您可能会经常遇到的一种类型是 `emptyDir`，这是一个与节点生命周期相关的临时卷，ConfigMap，它允许您在
    Kubernetes 清单中指定文件，并将它们作为磁盘上的文件呈现给您的应用程序，以及云提供商的持久存储磁盘。
- en: EmptyDir volumes
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: EmptyDir 卷
- en: The built-in volume type `emptyDir` is an ephemeral volume that is allocated
    on space from the node’s boot disk. If the Pod is deleted or moved to another
    node or the node itself becomes unhealthy, all data is lost. So what’s the benefit?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的卷类型 `emptyDir` 是一种临时卷，它从节点的启动磁盘上分配空间。如果 Pod 被删除或移动到另一个节点，或者节点本身变得不健康，所有数据都会丢失。那么它的好处是什么呢？
- en: Pods can have multiple containers, and `emptyDir` mounts can be shared between
    them. So when you need to share data between containers, you would define an `emptyDir`
    volume and mount it in each container in the Pod (listing 9.1). The data is also
    persisted between container *restarts*, just not all the other events I mentioned
    earlier. This is useful for ephemeral data such as that of an on-disk cache, where
    it is beneficial if the data is preserved between Pod restarts but where long-term
    storage isn’t necessary.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Pods可以有多个容器，并且`emptyDir`挂载可以在它们之间共享。所以当你需要在容器之间共享数据时，你会在Pod中的每个容器中定义一个`emptyDir`卷并将其挂载（见9.1列表）。数据也会在容器*重启*之间持久化，只是不是之前提到的所有事件。这对于像磁盘缓存这样的临时数据很有用，如果数据在Pod重启之间被保留，但长期存储不是必需的。
- en: Listing 9.1 Chapter09/9.1.1_Volume/emptydir_pod.yaml
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 第09章/9.1.1_卷/emptydir_pod.yaml
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The mount path
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 挂载路径
- en: ❷ The volume definition
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 卷定义
- en: Why on earth is this called `emptyDir`? Because the data is stored in an initially
    empty directory on the node. It’s a misnomer, in my opinion, but what can you
    do?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这被称为`emptyDir`？因为数据存储在节点上最初为空的目录中。在我看来，这是一个误称，但你又能怎么办呢？
- en: TIP If you’re looking for scratch space for a workload, see section 9.4 on generic
    ephemeral volumes, a more modern way to get ephemeral storage without relying
    on the host volume.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你在寻找工作负载的临时空间，请参阅9.4节关于通用临时卷，这是一种更现代的方法，可以在不依赖于主机卷的情况下获得临时存储。
- en: For a practical example, see section 9.2.2, in which `emptyDir` is used to share
    data between two containers in the same Pod, where one of them is an init container
    that runs first and can perform setup steps for the main container.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个实际示例，请参阅9.2.2节，其中使用`emptyDir`在同一个Pod中的两个容器之间共享数据，其中一个容器是一个首先运行的init容器，可以为主容器执行设置步骤。
- en: ConfigMap volume
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMap卷
- en: ConfigMap is a useful Kubernetes object. You can define key-value pairs in one
    place and reference them from multiple other objects. You can also use them to
    store entire files! Typically, these files would be configuration files like `my.cnf`
    for MariaDB, `httpd.conf` for Apache, `redis.conf` for Redis, and so on. You can
    mount the ConfigMap as a volume, which allows the files it defines to be read
    from the container. ConfigMap volumes are read-only.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMap是一个有用的Kubernetes对象。你可以在一个地方定义键值对，并从多个其他对象中引用它们。你还可以使用它们来存储整个文件！通常，这些文件将是配置文件，如MariaDB的`my.cnf`，Apache的`httpd.conf`，Redis的`redis.conf`等。你可以将ConfigMap作为卷挂载，这允许从容器中读取它定义的文件。ConfigMap卷是只读的。
- en: This technique is particularly useful for defining a configuration file for
    use by a public container image, as it allows you to provide configuration without
    needing to extend the image itself. For example, to run Redis, you can reference
    the official Redis image and just mount your config file using ConfigMap wherever
    Redis expects it—no need to build your own image just to provide this one file.
    See sections 9.2.1 and 9.2.2 for examples of configuring Redis with a custom configuration
    file specified via a ConfigMap volume.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术特别适用于定义用于公共容器镜像的配置文件，因为它允许你提供配置，而无需扩展镜像本身。例如，要运行Redis，你可以引用官方的Redis镜像，只需将你的配置文件挂载到ConfigMap中，Redis期望的位置即可——无需构建自己的镜像仅为了提供这个文件。有关使用通过ConfigMap卷指定的自定义配置文件配置Redis的示例，请参阅9.2.1和9.2.2节。
- en: Cloud Provider Volumes
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商卷
- en: More applicable for building *stateful* applications, where you don’t typically
    want to use ephemeral or read-only volumes, is mounting disks from your cloud
    provider as volumes. Wherever you are running Kubernetes, your provider should
    have supplied drivers into the cluster that allow you to mount persistent storage,
    whether that’s NFS or block-based (often both).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 更适用于构建*有状态*的应用程序，在这些应用程序中，你通常不希望使用临时或只读卷，是将云提供商的磁盘作为卷挂载。无论你在哪里运行Kubernetes，你的提供商都应该已经将驱动程序提供给集群，允许你挂载持久存储，无论是NFS还是基于块的（通常是两者都有）。
- en: '![09-01](../../OEBPS/Images/09-01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![09-01](../../OEBPS/Images/09-01.png)'
- en: Figure 9.1 A Pod with a mounted cloud provider volume
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 挂载云提供商卷的Pod
- en: By way of example, the following listing provides the specification for a MariaDB
    Pod running in Google Kubernetes Engine (GKE) mounting a GCE persistent disk at
    `/var/lib/mysql` for persistent storage as illustrated in figure 9.1.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下列表提供了在Google Kubernetes Engine (GKE)中运行的MariaDB Pod的规范，该Pod在`/var/lib/mysql`挂载GCE持久磁盘以进行持久存储，如图9.1所示。
- en: Listing 9.2 Chapter09/9.1.1_Volume/mariadb_pod.yaml
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 第09章/9.1.1_卷/mariadb_pod.yaml
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Node selector targeting the zone where the disk exists, so that the Pod will
    be created in the same zone
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 节点选择器针对磁盘存在的区域，以确保 Pod 将在该区域创建
- en: ❷ Directory where the disk will be mounted
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 磁盘将要挂载的目录
- en: ❸ Name of the persistent disk Google Cloud resource
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Google Cloud 资源持久磁盘的名称
- en: Unlike the more automated and cloud-agnostic approaches we’ll cover next, this
    method is tied to your cloud provider and requires manual creation of the disk.
    You need to ensure that a disk with the name specified exists, which you need
    to create out of band (i.e., using your cloud provider’s tools), and that both
    the disk and the Pod are in the same zone. In this example, I use `nodeSelector`
    to target the zone of the disk, which is important for any Kubernetes cluster
    that exists in multiple zones; otherwise, your Pod could be scheduled on a different
    zone from that of the disk.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们接下来将要介绍的更自动化和云无关的方法不同，这种方法与您的云提供商相关联，并需要手动创建磁盘。您需要确保存在指定名称的磁盘，您需要通过外部方式（即使用您的云提供商的工具）创建它，并且磁盘和
    Pod 都在同一个区域。在这个例子中，我使用 `nodeSelector` 来定位磁盘的区域，这对于存在于多个区域的任何 Kubernetes 集群来说都很重要；否则，您的
    Pod 可能会被调度到与磁盘不同的区域。
- en: 'Creating the disk used by this example out of band can be achieved using the
    following command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令可以创建本例中使用的磁盘：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note This example and the accompanying cloud-provider-specific instructions
    are provided for completeness, and to illustrate how volumes developed, but it’s
    not the recommended way to use volumes. Read on for a better, platform-agnostic
    way to create disks with PersistentVolumes and StatefulSet!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本例及其伴随的特定于云提供者的说明提供是为了完整性，并说明如何开发卷，但这不是推荐使用卷的方式。继续阅读以了解使用 PersistentVolumes
    和 StatefulSet 创建磁盘的更好、平台无关的方法！
- en: 'Since we’re creating this disk manually, pay close attention to the location
    where the resource is being created. The zone in the previous command and the
    zone set via the `nodeSelector` configuration need to match. If you see your Pod
    stuck in `Container` `Creating`, inspect your event log for the answer. Here’s
    a case where I hadn’t created the disk in the right project:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在手动创建此磁盘，请密切关注资源创建的位置。之前命令中的区域和通过 `nodeSelector` 配置设置的区域需要匹配。如果您看到您的 Pod
    卡在 `Container` `Creating` 状态，请检查事件日志以获取答案。以下是一个我没有在正确项目中创建磁盘的情况：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The downside to mounting volumes directly is that the disks need to be created
    outside of Kubernetes, which means the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 直接挂载卷的缺点是磁盘需要在 Kubernetes 之外创建，这意味着以下：
- en: The user creating the Pod must have permissions to create the disk, which is
    not always the case.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 Pod 的用户必须具有创建磁盘的权限，这并不总是如此。
- en: Steps exist outside of Kubernetes configuration that need to be remembered and
    run manually.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在于 Kubernetes 配置之外且需要记住并手动运行的步骤。
- en: The volume descriptors are platform-dependent, so this Kubernetes YAML is not
    portable and won’t work on another provider.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷描述符是平台相关的，因此这个 Kubernetes YAML 文件不可移植，且在其他提供者上无法使用。
- en: Naturally, Kubernetes has a solution for this lack of portability. By using
    the persistent volume abstraction provided by Kubernetes, you can simply request
    the disk resources you need and have them provisioned for you with no need to
    perform any out-of-band steps. Read on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，Kubernetes 为这种不可移植性提供了解决方案。通过使用 Kubernetes 提供的持久卷抽象，您可以简单地请求所需的磁盘资源，并且它们将为您配置，无需执行任何外部步骤。请继续阅读。
- en: 9.1.2 Persistent volumes and claims
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 持久卷和声明
- en: 'To provide a way to manage volumes in a more platform-agnostic way, Kubernetes
    offers higher-level primitives: *PersistentVolume* (PV) and *PersistentVolumeClaim*
    (PVC). Instead of linking to the volume directly, the Pod references a *PersistentVolumeClaim*
    object, which defines the disk resources that the Pod requires in platform-agnostic
    terms (e.g., “1 gigabyte of storage”). The disk resources themselves are represented
    in Kubernetes using a *PersistentVolume* object, much like how nodes in Kubernetes
    represent the VM resource. When the *PersistentVolumeClaim* is created, Kubernetes
    will seek to provide the resources requested in the claim by creating or matching
    it with a *PersistentVolume* and binding the two objects together (figure 9.2).
    Once bound, the PV and PVC, which now reference each other, typically remain linked
    until the underlying disk is deleted.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以更平台无关的方式管理卷，Kubernetes 提供了更高层次的原始操作：*PersistentVolume*（PV）和 *PersistentVolumeClaim*（PVC）。Pod
    不是直接链接到卷，而是引用一个 *PersistentVolumeClaim* 对象，该对象以平台无关的术语定义了 Pod 所需的磁盘资源（例如，“1 GB
    的存储空间”）。磁盘资源本身在 Kubernetes 中使用 *PersistentVolume* 对象表示，就像 Kubernetes 中的节点表示 VM
    资源一样。当创建 *PersistentVolumeClaim* 时，Kubernetes 将寻求通过创建或匹配它来提供声明中请求的资源，并将这两个对象绑定在一起（图
    9.2）。一旦绑定，PV 和 PVC，现在相互引用，通常在底层磁盘被删除之前保持链接。
- en: '![09-02](../../OEBPS/Images/09-02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![09-02](../../OEBPS/Images/09-02.png)'
- en: Figure 9.2 A Pod that references a `PersistentVolumeClaim` that gets bound to
    a `PersistentVolume`, which references a disk
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 一个引用 `PersistentVolumeClaim` 的 Pod，该 `PersistentVolumeClaim` 被绑定到 `PersistentVolume`，后者引用一个磁盘
- en: This behavior of having the claim, which requests resources, and an object that
    represents the availability of resources is similar to how a Pod requests compute
    resources like CPU and memory and the cluster finds a node that has these resources
    to schedule the Pod on. It also means that the storage requests are defined in
    a platform-independent manner. Unlike using the cloud provider disk directly,
    when using *PersistentVolumeClaim*, your Pods can be deployed anywhere, provided
    the platform supports persistent storage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种拥有请求资源的需求和表示资源可用性的对象的行为，类似于 Pod 请求计算资源（如 CPU 和内存），而集群找到具有这些资源的节点来调度 Pod 的方式。这也意味着存储请求是以平台无关的方式定义的。与直接使用云提供商的磁盘不同，当使用
    *PersistentVolumeClaim* 时，只要平台支持持久存储，您的 Pods 可以部署在任何地方。
- en: Let’s rewrite our Pod from the previous section to use PersistentVolumeClaim
    to request a new PersistentVolume for our Pod. This Pod will attach an external
    disk mounted to `/var/lib/mysql`, which is where MariaDB stores its data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写上一节中的 Pod，使用 PersistentVolumeClaim 请求为我们的 Pod 请求一个新的 PersistentVolume。这个
    Pod 将挂载一个连接到 `/var/lib/mysql` 的外部磁盘，这是 MariaDB 存储其数据的地方。
- en: Listing 9.3 Chapter09/9.1.2_PersistentVolume/pvc-mariadb.yaml
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3 Chapter09/9.1.2_PersistentVolume/pvc-mariadb.yaml
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The MariaDB data directory where the PVC-backed volume will be mounted
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PVC 支持的卷将被挂载的 MariaDB 数据目录
- en: ❷ The compute resources the Pod is requesting
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Pod 请求的计算资源
- en: ❸ Reference to the persistentVolumeClaim object instead of a disk resource
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用持久化卷声明对象而不是磁盘资源
- en: ❹ The PersistentVolumeClaim object
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 持久化卷声明对象
- en: ❺ The storage resources the Pod is requesting
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ Pod 请求的存储资源
- en: In the *PersistentVolumeClaim* definition, we’re making a request for 2 GiB
    of storage and specifying the desired `accessMode`. The `ReadWriteOnce` access
    mode is for a volume that behaves like a traditional hard drive, where your storage
    is mounted to a single Pod for read/write access and is the most common. The other
    choices for `accessMode` are `ReadOnlyMany`, which can be used to mount a volume
    of existing data that’s shared across many Pods, and `ReadWriteMany` for mounting
    file storage (like NFS), where multiple Pods can read/write at the same time (a
    fairly special mode, only supported by a few storage drivers). In this chapter,
    the goal is stateful applications backed by traditional block-based volumes, so
    `ReadWriteOnce` is used throughout.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *PersistentVolumeClaim* 定义中，我们请求了 2 GiB 的存储空间，并指定了所需的 `accessMode`。`ReadWriteOnce`
    访问模式适用于像传统硬盘一样行为的卷，其中您的存储被挂载到单个 Pod 以进行读写访问，这是最常见的方式。`accessMode` 的其他选择包括 `ReadOnlyMany`，它可以用来挂载跨多个
    Pod 共享的现有数据卷，以及 `ReadWriteMany` 用于挂载文件存储（如 NFS），在这种情况下，多个 Pod 可以同时进行读写（这是一种相当特殊的模式，仅由少数存储驱动程序支持）。在本章中，目标是使用基于传统块存储的持久化应用，因此
    `ReadWriteOnce` 被用于整个过程中。
- en: If your provider supports dynamic provisioning, a PersistentVolume backed by
    a disk resource will be created to fulfill the storage requested by the PersistentVolumeClaim,
    after which the PersistentVolumeClaim and PersistentVolume will be bound together.
    The dynamic provisioning behavior of the PersistentVolume is defined through the
    StorageClass, which we will cover in the next section. GKE and almost every provider
    support dynamic provisioning and will have a default storage class, so the previous
    Pod definition in listing 9.3 can be deployed pretty much anywhere.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提供商支持动态配置，则会创建一个由磁盘资源支持的`PersistentVolume`来满足`PersistentVolumeClaim`请求的存储，之后`PersistentVolumeClaim`和`PersistentVolume`将被绑定在一起。`PersistentVolume`的动态配置行为通过`StorageClass`定义，我们将在下一节中介绍。GKE和几乎每个提供商都支持动态配置，并将有一个默认的存储类，因此列表9.3中的先前Pod定义几乎可以在任何地方部署。
- en: In the rare event that your provider *doesn’t* have dynamic provisioning, you
    (or the cluster operator/admin) will need to manually create PersistentVolume
    yourself with enough resources to satisfy the PersistentVolumeClaim request (figure
    9.3). Kubernetes still does the matchmaking of linking the claim to the volume
    of the manually created PersistentVolumes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在罕见的情况下，如果你的提供商*不支持*动态配置，你（或集群操作员/管理员）将需要手动创建足够的资源来满足`PersistentVolumeClaim`请求的`PersistentVolume`（图9.3）。Kubernetes仍然会执行将声明与手动创建的PersistentVolumes的卷匹配的配对。
- en: '![09-03](../../OEBPS/Images/09-03.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![09-03](../../OEBPS/Images/09-03.png)'
- en: Figure 9.3 The lifecycle of a PersistentVolumeClaim and PersistentVolume in
    a dynamically provisioned system
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 动态配置系统中`PersistentVolumeClaim`和`PersistentVolume`的生命周期
- en: The PersistentVolumeClaim, as defined in the previous example, can be thought
    of as a request for resources. The claim on a resource happens later when it is
    matched with, and is bound to, a *PersistentVolume* resource, and both resources
    are linked to each other. Essentially, the *PersistentVolumeClaim* has a lifecycle
    that starts as a request and becomes a claim when bound.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例中定义的`PersistentVolumeClaim`可以被视为对资源的请求。当它与`PersistentVolume`资源匹配并绑定时，对资源的声明发生，这两个资源相互链接。本质上，`PersistentVolumeClaim`的生命周期从请求开始，在绑定时成为声明。
- en: 'We could leave it there, but since your precious data will be stored on these
    disks, let’s dig in to see just how this binding works. If we create the resource
    in listing 9.3 and then query the YAML of the PersistentVolumeClaim *after* it’s
    bound, you’ll see that it’s been updated with a `volumeName`. This `volumeName`
    is the name of the PersistentVolume that it was linked to and now claims. Here’s
    what it looks like (with some superfluous information omitted for readability):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其留在这里，但由于你的宝贵数据将存储在这些磁盘上，让我们深入了解这种绑定是如何工作的。如果我们创建列表9.3中的资源，然后查询绑定后的`PersistentVolumeClaim`的YAML，你会看到它已经更新了`volumeName`。这个`volumeName`是它链接到的`PersistentVolume`的名称，现在它声称。以下是它的样子（省略了一些冗余信息以提高可读性）：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The PersistentVolume that this object is now bound to
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当前对象绑定的`PersistentVolume`
- en: 'We can query the PersistentVolume named in this configuration with `kubectl`
    `get` `-o` `yaml` `pv` `$NAME`, and we’ll see that it links right back to the
    PVC. Here is what mine looked like:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl` `get` `-o` `yaml` `pv` `$NAME`查询此配置中命名的`PersistentVolume`，我们会看到它直接链接回PVC。以下是我的查询结果：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The PersistentVolumeClaim that this PersistentVolume is bound to
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当前`PersistentVolume`绑定的`PersistentVolumeClaim`
- en: ❷ The pointer to the underlying disk resources
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指向底层磁盘资源的指针
- en: ❸ The status is now bound.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 状态现在是已绑定。
- en: It helps to visualize this side by side, so take a look at figure 9.4.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些并排放置有助于可视化，所以请查看图9.4。
- en: '![09-04](../../OEBPS/Images/09-04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![09-04](../../OEBPS/Images/09-04.png)'
- en: Figure 9.4 The `PersistentVolumeClaim` and `PersistentVolume` after the latter
    was provisioned, and they were bound together
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 在`PersistentVolume`配置后，`PersistentVolumeClaim`和`PersistentVolume`的绑定情况
- en: The PersistentVolumeClaim has really undergone a metamorphosis here, going from
    a request for resources to being a claim for a specific disk resource that will
    contain your data. This is not really like any other Kubernetes object I can think
    of. While it’s common to have Kubernetes add fields and perform actions on the
    object, few changes like these do, starting as a generic request for and representation
    of storage and ending up as a bound stateful object.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim在这里经历了真正的蜕变，从对资源的请求变成了对特定磁盘资源的声明，该资源将包含你的数据。这与其他我能想到的Kubernetes对象真的不太一样。虽然通常Kubernetes会在对象上添加字段并执行操作，但像这样的变化很少，它从一个通用的存储请求和表示开始，最终变成了一个绑定状态对象。
- en: There is one exception to this typical lifecycle of a PersistentVolumeClaim,
    which is when you have existing data that you wish to mount into a Pod. In that
    case, you create the PersistentVolumeClaim and the PersistentVolume objects already
    pointing at each other, so they are bound immediately at creation. This scenario
    is discussed in section 9.3 on migrating and recovering disks, including a fully
    worked data recovery scenario.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PersistentVolumeClaim的典型生命周期有一个例外，那就是当你有现有数据希望挂载到Pod中时。在这种情况下，你创建PersistentVolumeClaim和PersistentVolume对象，它们已经相互指向，因此它们在创建时立即绑定。这种情况在9.3节中讨论了迁移和恢复磁盘，包括一个完整的数据恢复场景。
- en: Testing the MariaDB Pod locally
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本地测试MariaDB Pod
- en: 'Want to connect to MariaDB and check everything was setup correctly? It’s easy.
    Just forward to the mariadb container’s port to your machine:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 想要连接到MariaDB并检查一切是否设置正确？很简单。只需将mariadb容器的端口转发到你的机器：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then, connect to it from a local mysql client. Don’t have a client handy? You
    can run one via Docker!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从本地mysql客户端连接到它。没有客户端？你可以通过Docker运行一个！
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The database password can be found in the environment variable of the Pod (listing
    9.3). Once you connect you can run a SQL query to test it out, for example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库密码可以在Pod的环境变量中找到（见9.3列表）。一旦连接，你可以运行一个SQL查询来测试它，例如：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 9.1.3 Storage classes
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 存储类
- en: So far, we’ve relied on the default dynamic provisioning behavior of the platform
    provider. But what about if we want to change what type of disks we get during
    the binding process? Or, what happens to the data if the PersistentVolumeClaim
    is deleted? That’s where storage classes come in.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直依赖于平台提供者的默认动态预配行为。但如果我们想在绑定过程中更改我们想要的磁盘类型怎么办？或者，如果删除PersistentVolumeClaim，数据会怎样？这就是存储类发挥作用的地方。
- en: 'Storage classes are a way to describe the different types of *dynamic* storage
    that are available to be requested from PersistentVolumeClaims and how the volumes
    that are requested in this way should be configured. Your Kubernetes cluster probably
    has a few defined already. Let’s view them with `kubectl` `get` `storageclass`
    (some columns in the output have been removed for readability):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类是一种描述可以从PersistentVolumeClaims请求的不同类型的*动态*存储的方式，以及以这种方式请求的卷应该如何配置。你的Kubernetes集群可能已经定义了一些。让我们用`kubectl
    get storageclass`来查看它们（为了可读性，输出中已删除一些列）：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When we created the Pod in the previous section with a PersistentVolumeClaim,
    the default storage class (`standard-rwo`, in this case) was used. If you go back
    and look at the bound PersistentVolumeClaim object, you’ll see this storage class
    in the configuration under `storageClassName`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中创建Pod并使用PersistentVolumeClaim时，使用了默认的存储类（在本例中为`standard-rwo`）。如果你回到之前查看绑定的PersistentVolumeClaim对象，你会在`storageClassName`配置下看到这个存储类。
- en: This is a pretty good start, and you may not need to change much, but there
    is one aspect that might be worth reviewing. If you read the `RECLAIMPOLICY` column
    in the output from `kubectl` `get` `storageclass` earlier, you may notice it states
    `Delete`. This means that if the PVC is deleted, the bound PV and the disk resource
    that backs it will also be deleted. If your stateful workloads are mostly just
    caching services storing noncritical data, this might be fine. However, if your
    workloads store unique and precious data, this default behavior is not ideal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的开始，你可能不需要做太多改变，但有一个方面可能值得审查。如果你阅读了之前`kubectl get storageclass`命令输出的`RECLAIMPOLICY`列，你可能注意到它表示`Delete`。这意味着如果PVC被删除，绑定的PV及其背后的磁盘资源也将被删除。如果你的有状态工作负载主要是缓存服务，存储非关键数据，这可能没问题。然而，如果你的工作负载存储的是独特且宝贵的数据，这种默认行为并不理想。
- en: Kubernetes also offers a `Retain` reclaim policy, which means that the underlying
    disk resource will not be deleted on the deletion of the PVC. This allows you
    to keep the disk, and bind it to a new PV and PVC, potentially even one that you
    create in a completely separate cluster (which you might do when migrating workloads).
    The downside to `Retain`, and why it’s typically not the default, is that you’ll
    need to manually delete disks you don’t wish to keep, which isn’t ideal for testing
    and development, or workloads with ephermal data (like caches).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还提供了一个`Retain`回收策略，这意味着在删除PVC时，底层磁盘资源不会被删除。这允许你保留磁盘，并将其绑定到一个新的PV和PVC上，甚至可能是你在一个完全独立的集群中创建的（你可能会在迁移工作负载时这样做）。`Retain`的缺点，以及为什么它通常不是默认设置，是你需要手动删除你不想保留的磁盘，这对测试和开发，或者具有临时数据（如缓存）的工作负载来说并不理想。
- en: 'To build our own StorageClass, it’s simplest to start with an existing one
    to use as a template, such as the current default. We can export the default StorageClass
    listed earlier as follows. If your StorageClass name is different than mine, replace
    `standard-rwo` with the storage class you want to modify:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们自己的StorageClass，最简单的方法是从现有的一个开始，将其用作模板，例如当前的默认设置。我们可以按照以下方式导出前面列出的默认StorageClass。如果你的StorageClass名称与我的不同，将`standard-rwo`替换为你想要修改的存储类：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we can customize and set the all-important `Retain` reclaim policy. Since
    we want to create a new policy, it’s also important to give it a new name and
    strip the `uid` and other unneeded metadata fields. After performing those steps,
    I get the following listing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以自定义并设置至关重要的`Retain`回收策略。由于我们想要创建一个新的策略，因此给它一个新的名称并删除`uid`和其他不需要的元数据字段也很重要。完成这些步骤后，我得到了以下列表。
- en: Listing 9.4 Chapter09/9.1.3_StorageClass/storageclass.yaml
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 第09章/9.1.3_StorageClass/storageclass.yaml
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Optional annotation to set as default
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置为默认的可选注释
- en: ❷ Platform-specific value to set the storage type
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置存储类型的平台特定值
- en: ❸ Configured to preserve the disk if the PV is deleted
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 配置为在删除PV时保留磁盘
- en: You can reference your new StorageClass directly in any PersistantVolumeClaim
    object or template with the `storageClassName` field. This is a great option,
    say, if you only want to use the retain reclaim policy for a select few workloads.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接在包含`storageClassName`字段的任何PersistentVolumeClaim对象或模板中引用你的新StorageClass。这是一个很好的选择，比如说，如果你只想为少数几个工作负载使用保留回收策略。
- en: 'Optionally, you can set a new default storage class by adding the `is-default-class`
    annoation shown in listing 9.4\. If you want to change the default, you’ll need
    to mark the current default as nondefault. You can edit it with `kubectl` `edit`
    `storageclass` `standard-rwo`, or patch it with the following one-liner. Again,
    replace `standard-rwo` with whatever the name of your default class is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，你可以通过添加列表9.4中显示的`is-default-class`注释来设置一个新的默认存储类。如果你想更改默认设置，你需要将当前默认设置标记为非默认。你可以使用`kubectl
    edit storageclass standard-rwo`编辑它，或者使用以下单行命令修补它。再次提醒，将`standard-rwo`替换为你默认类的名称：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When ready, create the new storage class with `kubectl` `create` `-f` `storageclass.yaml`.
    If you changed the default, any new PersistentVolume created will use your new
    StorageClass.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 准备就绪后，使用`kubectl create -f storageclass.yaml`创建新的存储类。如果你更改了默认设置，任何新创建的PersistentVolume都将使用你的新StorageClass。
- en: It’s typical to have multiple storage classes with different performance and
    retention characteristics defined for different types of data. For example, you
    might have critical production data for a database that needs fast storage and
    to be retained, cache data that benefits from high performance but can be deleted,
    and ephermal storage for batch processing that can use average performance disks
    and not be retained. Pick a good default based on your preferences, and reference
    the others manually by specifying `storageClassName` in the PersistentVolumeClaim.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会有多个存储类，具有不同的性能和保留特性，为不同类型的数据定义。例如，你可能有一个数据库的临界生产数据，它需要快速存储并保留，缓存数据可以从高性能中受益但可以被删除，以及用于批处理的临时存储，可以使用平均性能的磁盘且不需要保留。根据你的偏好选择一个好的默认设置，并通过在PersistentVolumeClaim中指定`storageClassName`手动引用其他存储类。
- en: 9.1.4 Single-Pod stateful workload deployments
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 单Pod有状态工作负载部署
- en: Utilizing PersistentVolumeClaims, we can provision a one-replica stateful workload
    by simply enclosing our Pod into a Deployment. The benefit of using a Deployment
    even for a single-replica Pod is that if the Pod is terminated, it will be re-created.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 PersistentVolumeClaims，我们可以通过将我们的 Pod 包裹在一个 Deployment 中来简单地部署一个单副本有状态的工作负载。即使对于单个副本的
    Pod，使用 Deployment 的好处是，如果 Pod 被终止，它将被重新创建。
- en: Listing 9.5 Chapter09/9.1.4_Deployment_MariaDB/mariadb-deploy.yaml
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 第 09/9.1.4_Deployment_MariaDB/mariadb-deploy.yaml
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Re-create strategy used to prevent multiple replicas attempting to mount the
    same volume during rollouts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重新创建用于防止在滚动部署期间多个副本尝试挂载同一卷的策略。
- en: ❷ The Pod template spec is identical to the one shown in section 9.1.2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Pod 模板规范与第 9.1.2 节中显示的规范相同。
- en: So, there we have it. This is a single Pod deployment of a MariaDB database
    with an attached disk that won’t be deleted even if the entire Kubernetes cluster
    is deleted, thanks to the `Retain` policy in the default storage class we created
    in the prior section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们得到的结果。这是一个 MariaDB 数据库的单 Pod 部署，它附加了一个磁盘，即使整个 Kubernetes 集群被删除，这个磁盘也不会被删除，这要归功于我们在上一节中创建的默认存储类中的
    `Retain` 策略。
- en: If you want to give this database a spin, create a Service for it (Chapter09/9.1.4_
    Deployment_MariaDB/service.yaml). Once the Service is created, you can connect
    to the database from a local client (see the sidebar “Testing the MariaDB Pod
    locally”), or you can try out a containerized phpMyAdmin (see the Bonus/phpMyAdmin
    folder in the code repository that accompanies the book).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试这个数据库，为它创建一个 Service（第 09/9.1.4_ Deployment_MariaDB/service.yaml 章节）。一旦
    Service 创建成功，您就可以从本地客户端连接到数据库（参见侧边栏“在本地测试 MariaDB Pod”），或者您可以尝试容器化的 phpMyAdmin（参见伴随本书的代码仓库中的
    Bonus/phpMyAdmin 文件夹）。
- en: Running databases in Kubernetes
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中运行数据库
- en: Before you take the plunge to manage your own MariaDB database in Kubernetes,
    you probably want to look for a managed solution with your cloud provider. I know
    it’s tempting just to deploy in Kubernetes because it’s fairly easy to create
    such a database, as I demonstrated. However, the operational cost comes later
    when you have to secure, update, and manage it. Generally, I recommend reserving
    the stateful workload functionality of Kubernetes for customized or bespoke services
    or services that your cloud provider doesn’t provide as a managed offering.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在您决定在 Kubernetes 中管理自己的 MariaDB 数据库之前，您可能希望寻找云提供商提供的托管解决方案。我知道直接在 Kubernetes
    中部署很有吸引力，因为创建这样的数据库相对容易，正如我演示的那样。然而，运营成本会在您必须对其进行安全、更新和管理时出现。通常，我建议将 Kubernetes
    的有状态工作负载功能保留用于定制或定制的服务，或者您的云提供商不作为托管服务提供的那些服务。
- en: As demonstrated in this section, we can make our workloads stateful by attaching
    volumes using PersistentVolumeClaims. Using Pod and Deployment objects for this,
    however, limits us to single-replica stateful workloads. This might be enough
    for some, but what if you have a sophisticated stateful workload like Elasticsearch
    or Redis with multiple replicas? You could try to stitch together a bunch of Deployments,
    but fortunately, Kubernetes has a high-level construct that is designed to represent
    exactly this type of workload called the StatefulSet.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节所示，我们可以通过使用 PersistentVolumeClaims 挂载卷来使我们的工作负载具有状态。然而，使用 Pod 和 Deployment
    对象进行此操作限制了我们对单副本有状态工作负载的限制。这可能对一些人来说已经足够了，但如果你有一个具有多个副本的复杂有状态工作负载，如 Elasticsearch
    或 Redis，你会怎么办？你可以尝试将多个 Deployment 连接起来，但幸运的是，Kubernetes 有一个高级构建块，专门用于表示这种类型的工作负载，称为
    StatefulSet。
- en: 9.2 StatefulSet
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 StatefulSet
- en: We’ve seen how persistent storage can be added to a Pod in Kubernetes—a useful
    feature because Pods are the basic building block in Kubernetes, and they are
    used in many different workload constructs, like Deployments (chapter 3) and Jobs
    (chapter 10). Now, you can add persistent storage to any of them and build stateful
    Pods wherever you need, provided that the volume specification is the same for
    all instances.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何在 Kubernetes 中的 Pod 中添加持久存储——这是一个有用的功能，因为 Pod 是 Kubernetes 中的基本构建块，并且它们被用于许多不同的工作负载结构中，如
    Deployments（第 3 章）和 Jobs（第 10 章）。现在，您可以为它们中的任何一个添加持久存储，并在需要的地方构建有状态 Pod，只要所有实例的卷规范相同即可。
- en: The limitation of the workload constructs like Deployment is that all Pods share
    the same specification, which creates a problem for traditional volumes with a
    `ReadWriteOnce` access method, as they can only be mounted by a single instance.
    This is OK when there is only one replica in your Deployment, but it means that
    if you create a second replica, that Pod will fail to be created as the volume
    is already mounted.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载构造（如Deployment）的限制是所有Pod共享相同的规范，这为具有`ReadWriteOnce`访问方法的传统卷创建问题，因为它们只能由单个实例挂载。当你的Deployment中只有一个副本时，这是可以接受的，但如果你创建第二个副本，那么该Pod将无法创建，因为卷已经被挂载。
- en: Fortunately, Kubernetes has a high-level workload construct that makes our lives
    easier when we need multiple Pods where they each get their own disk (a highly
    common pattern). Just like Deployment is a higher-level construct for managing
    continuously running services (typically stateless), StatefulSet is the construct
    provided for managing stateful services.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes有一个高级工作负载构造，当我们需要多个Pod且每个Pod都有自己的磁盘时（这是一个高度常见的模式），它使我们的生活变得更简单。就像Deployment是一个高级构造，用于管理持续运行的服务（通常是无状态的）一样，StatefulSet是为管理有状态服务提供的构造。
- en: StatefulSet has a few helpful properties for building such services. You can
    define a volume template instead of referencing a single volume in the PodSpec,
    and Kubernetes will create a new PersistentVolumeClaim (PVC) for each Pod (thus
    solving the problem of using Deployment with volumes, where each instance got
    the exact same PVC). StatefulSet assigns each Pod a stable identifier, which is
    linked to a particular PVC and provides ordering guarantees during creation, scaling,
    and updates. With StatefulSet, you can get multiple Pods and coordinate them by
    using this stable identifier to potentially assign each a different role.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet为构建此类服务提供了一些有用的属性。您可以在PodSpec中定义一个卷模板，而不是引用单个卷，Kubernetes将为每个Pod创建一个新的PersistentVolumeClaim
    (PVC)（从而解决了使用Deployment与卷的问题，其中每个实例都获得了完全相同的PVC）。StatefulSet为每个Pod分配一个稳定的标识符，它与特定的PVC相关联，并在创建、扩展和更新期间提供排序保证。使用StatefulSet，您可以通过使用此稳定的标识符来协调多个Pod，并可能为每个Pod分配不同的角色。
- en: 9.2.1 Deploying StatefulSet
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 部署StatefulSet
- en: Putting this into practice, let’s look at two popular stateful workloads—MariaDB
    and Redis—and how to deploy them as a StatefulSet. At first, we’ll stay with a
    single-Pod StatefulSet, which is the simplest to demonstrate without multiple
    roles to worry about. The next section will add additional replicas with different
    roles to use the power of StatefulSet fully.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将其付诸实践，让我们看看两个流行的有状态工作负载——MariaDB和Redis——以及如何将它们作为StatefulSet部署。一开始，我们将保持单个Pod的StatefulSet，这是在不涉及多个角色的情况下最容易演示的。下一节将添加具有不同角色的额外副本，以充分利用StatefulSet的功能。
- en: MariaDB
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: MariaDB
- en: First, let’s convert the single-Pod MariaDB Deployment we created in the previous
    section to one using StatefulSet and take advantage of the PVC template to avoid
    the need to create a separate *PVC* object ourselves.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将上一节中创建的单Pod MariaDB Deployment转换为使用StatefulSet，并利用PVC模板来避免我们需要自己创建单独的*PVC*对象。
- en: Listing 9.6 Chapter09/9.2.1_StatefulSet_MariaDB/mariadb-statefulset.yaml
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 Chapter09/9.2.1_StatefulSet_MariaDB/mariadb-statefulset.yaml
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ StatefulSet uses the same match labels pattern as Deployments, discussed in
    chapter 3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ StatefulSet使用与Deployments相同的匹配标签模式，这在第3章中已有讨论。
- en: ❷ This is a reference to the headless Service, which is defined at the bottom
    of the file.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是对无头服务的引用，该服务在文件底部定义。
- en: ❸ StatefulSet requires that a graceful termination period be set. This is the
    number of seconds the Pod has to exit on its own before being terminated.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ StatefulSet要求设置一个优雅的终止期。这是Pod在终止前必须自行退出的秒数。
- en: ❹ The MariaDB data volume mount, now defined in the volumeClaimTemplates section
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在在volumeClaimTemplates部分定义的MariaDB数据卷挂载
- en: ❺ With StatefulSet, we can define a template of a PersisantVolumeClaim just
    like we define the template of the Pod replicas. This template is used to create
    the PersistentVolumeClaims, associating one to each of the Pod replicas.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用StatefulSet，我们可以定义一个PersistentVolumeClaim的模板，就像我们定义Pod副本的模板一样。此模板用于创建PersistentVolumeClaims，将每个Pod副本与一个关联。
- en: ❻ The headless Service for this StatefulSet
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为此StatefulSet的无头服务
- en: How is this StatefulSet specification different from the Deployment specification
    of the same MariaDB Pod in the previous section? Other than the different object
    metadata, there are two key changes. The first difference is how the PersistentVolumeClaim
    is configured. When used in the previous section, it was defined as a standalone
    object. With StatefulSet, this is rolled into the definition itself under `volumeClaimTemplates`,
    much like how a Deployment has a Pod template. In each Pod, the StatefulSet will
    have a PersistentVolumeClaim created based on this template. For a single-Pod
    StatefulSet, you end up with a similar result (but without the need to define
    a separate PersistentVolumeClaim object), and it becomes critical later when creating
    multiple replica StatefulSets. Figure 9.5 shows the PersistentVolumeClai*m* (used
    in the Deployment) and the `volumeClaimTemplates` (used in the StatefulSet) side
    by side.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个StatefulSet规范与上一节中相同MariaDB Pod的Deployment规范有何不同？除了不同的对象元数据外，有两个关键变化。第一个区别是PersistentVolumeClaim的配置方式。在上一节中使用时，它被定义为独立的对象。在StatefulSet中，这被整合到定义本身中，在`volumeClaimTemplates`下，就像Deployment有一个Pod模板一样。在每个Pod中，StatefulSet将根据此模板创建一个PersistentVolumeClaim。对于单Pod
    StatefulSet，你最终得到类似的结果（但不需要定义单独的PersistentVolumeClaim对象），这在创建多个副本的StatefulSet时变得至关重要。图9.5显示了在Deployment中使用的PersistentVolumeClai*m*（`volumeClaimTemplates`在StatefulSet中使用）并并排展示。
- en: '![09-05](../../OEBPS/Images/09-05.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![09-05](../../OEBPS/Images/09-05.png)'
- en: Figure 9.5 PersistentVolumeClaim vs. `volumeClaimTemplates`
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 PersistentVolumeClaim与`volumeClaimTemplates`
- en: 'If you query the PVCs after creating the StatefulSet, you’ll see that one was
    created with this template (with some columns removed for readability):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建StatefulSet之后查询PVC，你会看到其中一个是用此模板创建的（为了可读性移除了一些列）：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The main difference is that the PVC created with the template has the Pod name
    (`mariadb-0`, in the case of the first Pod) appended to it. So, instead of being
    `mariadb-pvc` (the name of the claim template), it’s `mariadb-pvc-mariadb-0` (the
    claim template name and Pod name combined).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于，使用模板创建的PVC附加了Pod名称（在第一个Pod的情况下为`mariadb-0`）。因此，它不再是`mariadb-pvc`（索赔模板的名称），而是`mariadb-pvc-mariadb-0`（索赔模板名称和Pod名称的组合）。
- en: 'Another difference compared to Deployment is that a Service is referenced in
    the StatefulSet with the `serviceName:` `mariadb-service` line and defined as
    follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与Deployment相比的另一个区别是，在StatefulSet中通过`serviceName:` `mariadb-service`行引用了服务，并定义如下：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This Service is a bit different from the ones presented in the book so far,
    as it’s what’s known as a headless Service (indicated by the `clusterIP:` `None`
    in the specification). Unlike the other Service types, there’s no virtual cluster
    IP created to balance traffic over the Pods. If you query the DNS record of this
    Service (e.g., by execing into a Pod and running `host` `mariadb-service`), you
    will notice that it still returns an `A` record. This record is actually the IP
    address of the Pod itself, rather than the virtual cluster IP. For a headless
    Service with multiple Pods (like the Redis StatefulSet; see the next section),
    querying the Service will return multiple `A` records (i.e., one for each Pod).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此服务与书中迄今为止所介绍的服务略有不同，因为它被称为无头服务（在规范中的`clusterIP:` `None`表示）。与其他服务类型不同，不会创建虚拟集群IP来在Pods之间平衡流量。如果你查询此服务的DNS记录（例如，通过进入Pod并运行`host`
    `mariadb-service`），你会注意到它仍然返回一个`A`记录。这个记录实际上是Pod本身的IP地址，而不是虚拟集群IP。对于具有多个Pod的无头服务（如Redis
    StatefulSet；参见下一节），查询服务将返回多个`A`记录（即每个Pod一个）。
- en: The other useful property of the headless Service is that the Pods in a StatefulSet
    get their own stable network identities. Since each Pod in the StatefulSet is
    unique, and each has its own attached volume, it’s useful to be able to address
    them individually. This is unlike Pods in a Deployment, which are designed to
    be identical, such that it shouldn’t matter which one you connect to for any given
    request. To facilitate direct connections to Pods in the StatefulSet, each is
    assigned an incrementing integer value known as an ordinal (0, 1, 2, and so on).
    If the Pod in the StatefulSet is re-created following a disruption, it retains
    the same ordinal, whereas those replaced in a Deployment are assigned a new random
    name.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 无头 Service 的另一个有用属性是 StatefulSet 中的 Pods 获得它们自己的稳定网络标识。由于 StatefulSet 中的每个 Pod
    都是唯一的，并且每个都附加了自己的卷，因此能够单独地访问它们是有用的。这与 Deployment 中的 Pods 不同，Deployment 中的 Pods
    被设计成是相同的，因此对于任何给定的请求，连接到哪一个并不重要。为了便于直接连接到 StatefulSet 中的 Pods，每个都分配了一个递增的整数值，称为序号（0、1、2
    等等）。如果 StatefulSet 中的 Pod 在中断后重新创建，它将保留相同的序号，而那些在 Deployment 中被替换的则被分配一个新的随机名称。
- en: Pods in the StatefulSet can be addressed using their ordinal with the construction
    `$STATEFULSET_NAME-$POD_ORDINAL.$SERVICE_NAME.` In this example, our single Pod
    can be referenced using the DNS address `mariadb-0.mariadb-service`. From outside
    the namespace, you can append the namespace (like with any Service). For example,
    for the namespace named `production,` the Pod could be addressed with `mariadb-0-mariadb-service.production.svc`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用它们的序号通过构造 `$STATEFULSET_NAME-$POD_ORDINAL.$SERVICE_NAME.` 来访问 StatefulSet
    中的 Pods。在这个例子中，我们的单个 Pod 可以使用 DNS 地址 `mariadb-0.mariadb-service` 来引用。从命名空间外部，你可以附加命名空间（就像任何
    Service 一样）。例如，对于名为 `production` 的命名空间，Pod 可以通过 `mariadb-0-mariadb-service.production.svc`
    来访问。
- en: To try out this MariaDB instance running in a StatefulSet we can forward the
    port and connect locally with `kubectl` `port-forward` `sts/mariadb` `3306:3306`,
    but for something more interesting let’s create a MariaDB client using an ephemeral
    Pod running in the cluster, and connect using the service hostname.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试运行在 StatefulSet 中的这个 MariaDB 实例，我们可以转发端口并使用 `kubectl port-forward sts/mariadb
    3306:3306` 在本地连接，但为了更有趣，让我们在集群中运行一个临时的 Pod 来创建 MariaDB 客户端，并使用服务主机名进行连接。
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This creates a Pod in the cluster running the MariaDB client configured to connect
    to the primary Pod in our StatefulSet. It’s ephemeral and will be deleted once
    you exit the interactive session, making it a convenient way to perform one-off
    debugging from within the cluster. When the Pod is ready, type the database password
    found in the MARIADB_ROOT_PASSWORD environment variable in listing 9.6, and you
    can now execute database commands. When you’re done type `exit` to end the session.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这在集群中创建了一个运行 MariaDB 客户端的 Pod，该客户端配置为连接到我们的 StatefulSet 中的主 Pod。它是临时的，一旦你退出交互会话，它就会被删除，这使得它成为在集群内部执行一次性调试的便捷方式。当
    Pod 准备就绪时，在列表 9.6 中输入 MARIADB_ROOT_PASSWORD 环境变量中找到的数据库密码，你现在可以执行数据库命令。当你完成时，输入
    `exit` 结束会话。
- en: Redis
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Redis
- en: Another example we can use is Redis. Redis is a very popular workload deployment
    in Kubernetes and has many different possible uses, which often include caching
    and other real-time data storage and retrieval needs. For this example, let’s
    imagine the caching use case where the data isn’t super precious. You still want
    to persist the data to disk to avoid rebuilding the cache in the event of a restart,
    but there’s no need to back it up. What follows is a perfectly usable single-Pod
    Redis setup for Kubernetes for such applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以使用的例子是 Redis。Redis 是 Kubernetes 中非常流行的负载部署，有许多不同的可能用途，通常包括缓存和其他实时数据存储和检索需求。对于这个例子，让我们想象一个数据不是特别珍贵的缓存用例。你仍然希望将数据持久化到磁盘，以避免在重启时重建缓存，但不需要进行备份。以下是为此类应用程序提供的完全可用的单个
    Pod Redis 设置，适用于 Kubernetes。
- en: To configure Redis, let’s first define our config file, which we can mount as
    a volume in the container.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置 Redis，我们首先定义我们的配置文件，我们可以将其挂载为容器中的卷。
- en: Listing 9.7 Chapter09/9.2.1_StatefulSet_Redis/redis-configmap.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.7 第 09 章/9.2.1_StatefulSet_Redis/redis-configmap.yaml
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Binds to all interfaces so that other Pods can connect
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绑定到所有接口，以便其他 Pods 可以连接
- en: ❷ The port to use
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要使用的端口
- en: ❸ Disables protected mode so that other Pods in the cluster can connect without
    a password
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 禁用保护模式，以便集群中的其他 Pods 可以无密码连接
- en: ❹ Enables the append log to persist data to disk
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 启用将日志附加到磁盘以持久化数据
- en: ❺ Specifies the data directory
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 指定数据目录
- en: The key to note with this configuration is that we’re persisting the Redis state
    to the `/redis/data` directory, so it can be reloaded if the Pod is re-created,
    and we’ll next need to configure the volume to be mounted to that directory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置的关键是我们将Redis状态持久化到`/redis/data`目录，这样如果Pod被重新创建，它就可以被重新加载，接下来我们需要配置卷以挂载到该目录。
- en: This example does not configure authentication for Redis, which means that every
    Pod in the cluster will have read/write access. If you take this example and use
    it in a production cluster, please consider how you wish to configure the cluster.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子没有为Redis配置身份验证，这意味着集群中的每个Pod都将具有读写访问权限。如果你将此示例用于生产集群，请考虑你希望如何配置集群。
- en: Now let’s go ahead and create a StatefulSet that will reference this config
    and mount the `/redis/data` directory as a PersistentVolume.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续创建一个将引用此配置并将`/redis/data`目录作为持久卷挂载的有状态副本集。
- en: Listing 9.8 Chapter09/9.2.1_StatefulSet_Redis/redis-statefulset.yaml
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 Chapter09/9.2.1_StatefulSet_Redis/redis-statefulset.yaml
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ 1 replica, as this is a single-role StatefulSet
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 1个副本，因为这是一个单角色有状态副本集
- en: ❷ Mount the configuration file from listing 9.7 into a directory in the container.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将列表9.7中的配置文件挂载到容器中的目录。
- en: ❸ The Redis data volume mount, defined in the volumeClaimTemplates section
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在volumeClaimTemplates部分定义的Redis数据卷挂载
- en: ❹ References the ConfigMap object defined in listing 9.7
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 引用了列表9.7中定义的ConfigMap对象
- en: Compared to the MariaDB StatefulSet, it’s a similar setup, other than the application-specific
    differences, like the different ports used, the container image, and the mounting
    of the config map into `/redis/conf.`
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与MariaDB有状态副本集相比，除了应用特定的差异（如使用的不同端口、容器镜像以及将配置映射挂载到`/redis/conf`）之外，设置是相似的。
- en: 'After creating the resources in Chapter09/9.2.1_StatefulSet_Redis, to connect
    to Redis and verify it’s working, you can forward the port to your local machine
    and connect using the redis-cli tool, like so:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Chapter09/9.2.1_StatefulSet_Redis中的资源后，为了连接到Redis并验证其是否正常工作，你可以将端口转发到你的本地机器，并使用redis-cli工具连接，如下所示：
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So that’s two examples of a one-replica StatefulSet. Even with just one replica,
    it’s more convenient than using a Deployment for such a workload, as Kubernetes
    can take care of creating the PersistentVolumeClaim automatically.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是单个副本有状态副本集的两个例子。即使只有一个副本，使用Deployment来处理这种工作负载也更加方便，因为Kubernetes可以自动处理创建持久卷声明。
- en: If you delete the StatefulSet object, the PersistentVolumeClaim object will
    remain. If you then re-create the StatefulSet, it will reattach to the same PersistentVolumeClaim,
    so no data is lost. Deleting the PersistentVolumeClaim object itself *can* delete
    the underlying data, though, depending on how the storage class is configured.
    If you care about the data being stored (e.g., not just a cache that can be re-created),
    be sure to follow the steps in section 9.1.3 to set up a StorageClass that will
    retain the underlying cloud resources if the PersistentVolumeClaim object is deleted
    for whatever reason.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你删除了有状态副本集对象，持久卷声明对象将保留。如果你随后重新创建有状态副本集，它将重新附加到相同的持久卷声明，因此不会丢失任何数据。不过，根据存储类配置，删除持久卷声明对象本身*可能*会删除底层数据。如果你关心存储的数据（例如，不仅仅是可以重新创建的缓存），请确保遵循第9.1.3节中的步骤来设置一个存储类，以便在持久卷声明对象因任何原因被删除时保留底层云资源。
- en: If we increased the replicas for this StatefulSet, it would give us new Pods
    with their own volumes, but it doesn’t automatically mean they will actually talk
    to each other. For the Redis StatefulSet defined here, creating more replicas
    would just give us more individual Redis instances. The next section goes into
    detail about how to set up a multiple-Pod architecture within a single StatefulSet,
    where each unique Pod is configured differently, based on the ordinal of the Pod,
    and connected together.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们增加这个有状态副本集的副本数量，它将为我们提供带有自己存储卷的新Pod，但这并不意味着它们会自动相互通信。对于这里定义的Redis有状态副本集，增加副本数量只会给我们更多的单个Redis实例。下一节将详细介绍如何在单个有状态副本集中设置多Pod架构，其中每个独特的Pod根据Pod的序号配置不同，并相互连接。
- en: 9.2.2 Deploying a multirole StatefulSet
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 部署多角色有状态副本集
- en: The real power of a StatefulSet comes into play when you need to have multiple
    Pods. When designing an application that will use StatefulSet, Pod replicas within
    the StatefulSet need to know about each other and communicate with each other
    as part of the stateful application design. This is the benefit, though, of using
    the StatefulSet type because each of the Pods gets a unique identifier in a set
    known as the ordinal. You can use this uniqueness and guaranteed ordering to assign
    different roles to the different unique Pods in the set and associate the same
    persistent disk through updates and even deletion and re-creation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要多个 Pod 时，有状态集的真正威力才显现出来。在设计将使用有状态集的应用程序时，有状态集内的 Pod 副本需要相互了解并作为有状态应用程序设计的一部分进行通信。这是使用有状态集类型的优势，因为每个
    Pod 都在称为序号的集合中获得一个唯一的标识符。你可以使用这种唯一性和保证的顺序来为集合中的不同唯一 Pod 分配不同的角色，并通过更新甚至删除和重新创建来关联相同的持久磁盘。
- en: For this example, we’ll take the single Pod Redis StatefulSet from the previous
    section and convert it to a three-Pod setup by introducing the replica role. Redis
    uses a leader/follower replication strategy, consisting of a primary Pod (in section
    9.2.1, this was the only Pod) and additional Pods with the replica role (not to
    be confused with Kubernetes “replicas,” which refer to all of the Pods in the
    StatefulSet or Deployment).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将从上一节中的单个 Pod Redis 有状态集转换为三个 Pod 的设置，通过引入副本角色。Redis 使用主/从复制策略，由一个主
    Pod（在第 9.2.1 节中，这是唯一的 Pod）和具有副本角色的额外 Pods 组成（不要与 Kubernetes 中的“副本”混淆，它指的是有状态集或部署中的所有
    Pods）。
- en: Building on the example in the previous section, we’ll keep the same Redis configuration
    for the primary Pod and add an additional configuration file for the replicas,
    which contains a reference to the address of the primary Pod. Listing 9.9 is the
    ConfigMap where these two configuration files are defined.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节示例的基础上，我们将保持主 Pod 的相同 Redis 配置，并为副本添加一个额外的配置文件，该文件包含对主 Pod 地址的引用。列表 9.9
    是定义这两个配置文件的 ConfigMap。
- en: Listing 9.9 Chapter09/9.2.2_StatefulSet_Redis_Multi/redis-configmap.yaml
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9 第 09 章/9.2.2_有状态集_Redis_Multi/redis-configmap.yaml
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The first file in the config map, to configure the primary role
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置映射中的第一个文件，用于配置主角色
- en: ❷ The second file in the config map, to configure the replica role
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 配置映射中的第二个文件，用于配置副本角色
- en: ❸ Configure the Redis replica to reference the primary Pod by its name
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 配置 Redis 副本来通过其名称引用主 Pod
- en: ConfigMaps are simply a convenient way for us to define two configuration files,
    one for each of the two roles. We could equally build our own container using
    the Redis base image and put these two files in there. But since this is the only
    customization we need, it’s simpler to just define them here and mount them into
    our container.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMaps 只是我们定义两个配置文件的便捷方式，每个角色一个。我们同样可以使用 Redis 基础镜像构建自己的容器，并将这两个文件放入其中。但由于我们只需要这种唯一定制，所以在这里定义它们并将其挂载到我们的容器中会更简单。
- en: Next, we’ll update the StatefulSet workload to use an `init` container (i.e.,
    a container that runs during the initialization of the Pod) to set the role of
    each Pod replica. The script that runs in this `init` container looks up the ordinal
    of the Pod being initialized to determine its role and copies the relevant configuration
    for that role—recall that a special feature of StatefulSets is that each Pod is
    assigned a unique ordinal. We can use the ordinal value of `0` to designate the
    primary role, while assigning the remaining Pods to the replica role.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更新有状态集工作负载以使用 `init` 容器（即在 Pod 初始化期间运行的容器）来设置每个 Pod 副本的角色。在这个 `init`
    容器中运行的脚本查找正在初始化的 Pod 的序号以确定其角色，并复制该角色的相关配置——回想一下，有状态集的一个特殊功能是每个 Pod 都分配了一个唯一的序号。我们可以使用
    `0` 的序数值来指定主角色，而将剩余的 Pod 分配给副本角色。
- en: This technique can be applied to a variety of different stateful workloads where
    you have multiple roles. If you’re looking for MariaDB, there’s a great guide[²](#pgfId-1098636)
    provided with the Kubernetes docs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可以应用于具有多个角色的各种不同有状态工作负载。如果你在寻找 MariaDB，Kubernetes 文档中提供了一个非常好的指南[²](#pgfId-1098636)。
- en: Listing 9.10 Chapter09/9.2.2_StatefulSet_Redis_Multi/redis-statefulset.yaml
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 第 09 章/9.2.2_有状态集_Redis_Multi/redis-statefulset.yaml
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ 3 replicas for the multi-role StatefulSet
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 多角色有状态集的 3 个副本
- en: ❷ The init container that runs once on boot, to copy the config file from the
    ConfigMap mount to the emptyDir mount
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在启动时运行一次的初始化容器，用于将配置文件从 ConfigMap 挂载复制到 emptyDir 挂载
- en: ❸ Run the following script.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行以下脚本。
- en: ❹ emptyDir mount, shared with the main container
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 与主容器共享的emptyDir挂载
- en: ❺ ConfigMap mount with the 2 files from listing 9.9
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用列表9.9中的2个文件配置ConfigMap挂载
- en: ❻ The main Redis container
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 主Redis容器
- en: ❼ emptyDir mount, shared with the init container
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 与init容器共享的emptyDir挂载
- en: ❽ The Redis data volume mount, defined in the volumeClaimTemplates section
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在volumeClaimTemplates部分定义的Redis数据卷挂载
- en: ❾ References the ConfigMap object defined in listing 9.9
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 引用列表9.9中定义的ConfigMap对象
- en: ❿ Defines the emptyDir volume
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 定义emptyDir卷
- en: 'There’s a bit to unpack here, so let’s take a closer look. The main difference
    to our single-instance Redis StatefulSet is the presence of an `init` container.
    This `init` container, as its name suggests, runs during the initialization phase
    of the Pod. It mounts two volumes, the ConfigMap and a new volume `redis-config-volume`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要解释的内容，让我们仔细看看。与我们的单实例Redis StatefulSet的主要区别是存在一个`init`容器。正如其名称所暗示的，这个`init`容器在Pod的初始化阶段运行。它挂载两个卷，ConfigMap和一个新的卷`redis-config-volume`：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `redis-config-volume` is of type `emptyDir`, which allows data to be shared
    between containers but does not persist data if the Pod is rescheduled (unlike
    PersistentVolume). We are only using this `emptyDir` volume to store a copy of
    the config, and it is ideal for that. The `init` container runs a bash script
    contained in the YAML:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`redis-config-volume`类型为`emptyDir`，允许容器之间共享数据，但如果Pod被重新调度（与PersistentVolume不同），则不会持久化数据。我们只使用这个`emptyDir`卷来存储配置的副本，这对于它是理想的。`init`容器运行一个包含在YAML中的bash脚本：'
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This script will copy one of the two different configurations from the *ConfigMap*
    volume (mounted at `/mnt/redis-configmap`) to this shared `emptyDir` volume (mounted
    at `/redis/conf`), depending on the ordinal number of the Pod. That is, if the
    Pod is `redis-0`, the `primary.conf` file is copied; for the rest, `replica.conf`
    is copied.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将根据Pod的序号从*ConfigMap*卷（挂载在`/mnt/redis-configmap`）复制两个不同的配置之一到这个共享的`emptyDir`卷（挂载在`/redis/conf`）。也就是说，如果Pod是`redis-0`，则复制`primary.conf`文件；对于其余的，复制`replica.conf`。
- en: The main container then mounts the same `redis-config-volume` `emptyDir` volume
    at `/redis/conf`. When the Redis process is started, it will use whatever configuration
    resides at `/redis/conf/redis.conf`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 主容器随后在`/redis/conf`上挂载相同的`redis-config-volume` `emptyDir`卷。当Redis进程启动时，它将使用位于`/redis/conf/redis.conf`的任何配置。
- en: 'To try it out you can connect to the primary Pod using the port-forward/local
    client combination, or by creating an ephemeral Pod as documented in the previous
    sections. We can also connect directly with exec to quickly write some data, like
    so:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试它，您可以使用端口转发/本地客户端组合连接到主Pod，或者按照前几节中所述创建一个短暂的Pod。我们还可以通过exec直接连接，快速写入一些数据，如下所示：
- en: '[PRE26]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then connect to a replica and read it back:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后连接到一个副本并读取它：
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The replicas are read-only so you won’t be able to write data directly:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 副本为只读，因此您无法直接写入数据：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 9.3 Migrating/recovering disks
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 迁移/恢复磁盘
- en: 'Now I know what you’re thinking: can I really trust Kubernetes with my precious
    data? There’s a bit too much magic going on here; how can I be confident that
    my data is safe and recoverable if the Kubernetes cluster goes away?'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我知道你在想什么：我真的可以信任Kubernetes来处理我宝贵的数据吗？这里有点太神奇了；如果Kubernetes集群消失了，我怎么能确信我的数据是安全且可恢复的？
- en: Time to build some confidence. Let’s create a stateful workload in Kubernetes.
    Then, we’ll completely delete every Kubernetes object associated with it and try
    to re-create that workload from scratch, relinking it to the underlying cloud
    disk resources.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一些信心的时候到了。让我们在Kubernetes中创建一个有状态的工作负载。然后，我们将完全删除与之相关的所有Kubernetes对象，并尝试从头开始重新创建该工作负载，将其重新链接到底层云磁盘资源。
- en: One thing to be very aware of is that commonly, by default, disk resources that
    Kubernetes creates are deleted if you delete the associated bound PersistentVolumeClaim
    because they are configured with a `reclaimPolicy` set to `Delete`. Deleting the
    StatefulSet itself doesn’t delete the associated PersistentVolumeClaim objects,
    which is useful as it forces you to manually delete those objects if you no longer
    need the data, but deleting the PersistentVolumeClaim objects *will* delete the
    underlying disk resources, and it’s not that hard to do (e.g., by passing `--all`
    to the relevant `kubectl` `delete` command).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个事情需要非常注意，那就是通常情况下，默认情况下，Kubernetes创建的磁盘资源如果删除了相关的绑定PersistentVolumeClaim，就会被删除，因为它们配置了`reclaimPolicy`设置为`Delete`。删除StatefulSet本身不会删除相关的PersistentVolumeClaim对象，这很有用，因为它迫使你在不再需要数据时手动删除这些对象，但删除PersistentVolumeClaim对象*将会*删除底层的磁盘资源，而且这并不难做到（例如，通过将`--all`传递给相关的`kubectl
    delete`命令）。
- en: So, if you value your data, the first thing is to make sure the StorageClass
    that’s used when creating the disks for your precious data has its `reclaimPolicy`
    set to `Retain`, not `Delete`. This will preserve the underlying cloud disk when
    the Kubernetes objects are deleted, allowing you to manually re-create the PersistentVolumeClaim-PersistentVolume
    pairing in the same or a different cluster (which I will demonstrate).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你重视你的数据，首要任务是确保创建用于你宝贵数据的磁盘时使用的StorageClass的`reclaimPolicy`设置为`Retain`，而不是`Delete`。这样，当Kubernetes对象被删除时，将保留底层云盘，允许你手动在相同或不同的集群中重新创建PersistentVolumeClaim-PersistentVolume配对（我将演示）。
- en: To run this experiment, deploy the Redis example from section 9.2.2, with either
    the default or explicit storage class configured to retain data. To verify the
    status of the PersistentVolumes after creation, use `kubectl` `get` `pv` to inspect,
    and `kubectl` `edit` `pv` `$PV_NAME` to modify, the `persistentVolumeReclaimPolicy`
    field, if needed.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个实验，请从第9.2.2节部署Redis示例，使用默认存储类或显式配置为保留数据的存储类。为了验证创建PersistentVolumes后的状态，使用`kubectl
    get pv`来检查，如果需要，使用`kubectl edit pv $PV_NAME`来修改`persistentVolumeReclaimPolicy`字段。
- en: 'With our reclaim policy set correctly, we can now add some data that we can
    use to validate our ability to recover our Kubernetes StatefulSet after we delete
    it. To add data, first exec into the primary Pod and run the `redis-cli` tool.
    You can do both with the following command:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的reclaim策略设置正确后，我们现在可以添加一些数据，以验证我们在删除Kubernetes StatefulSet后恢复其能力。要添加数据，首先进入主Pod并运行`redis-cli`工具。你可以使用以下命令完成这两步：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once connected, we can add some data. If you’ve not used Redis before, don’t
    worry about this—we’re just adding some trivial data to prove we can recover it.
    This example data is some key-value pairs for world capitals:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 连接后，我们可以添加一些数据。如果你之前没有使用过Redis，不用担心这个问题——我们只是添加一些琐碎的数据来证明我们可以恢复它。这个示例数据是一些世界首都的键值对：
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If you like, at this point, you can delete the StatefulSet and re-create it.
    Then, exec back into the CLI and test the data. Here’s how:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，此时你可以删除StatefulSet并重新创建它。然后，回到CLI并测试数据。以下是方法：
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This works (i.e., the data is persisted) because when the StatefulSet is re-created.
    It references the same PersistentVolumeClaim that has our data for Redis to load
    when it boots, and so Redis picks off right where it left off.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效（即数据被持久化），是因为当StatefulSet重新创建时，它引用了包含Redis启动时加载数据的相同PersistentVolumeClaim，因此Redis可以从上次停止的地方继续运行。
- en: Good so far. Now let’s take a more drastic step and delete the PVC and VC and
    attempt to re-create. The re-creation can optionally be done in a completely new
    cluster if you like to simulate the entire cluster being deleted. Just be sure
    to use the same cloud region so the disk can be accessed.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利。现在让我们采取一个更激进的步骤，删除PVC和VC，并尝试重新创建。如果你喜欢，可以在一个全新的集群中重新创建，以模拟整个集群被删除的情况。只需确保使用相同的云区域，以便可以访问磁盘。
- en: 'Before we delete those objects, though, let’s save their configuration. This
    isn’t strictly necessary; you certainly can re-create them from scratch if needed,
    but it will help save some time. Use the following commands to list and save the
    objects (output truncated for readability):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们删除这些对象之前，让我们保存它们的配置。这并不是绝对必要的；你当然可以在需要时从头开始重新创建它们，但这将有助于节省时间。使用以下命令列出并保存对象（输出已截断以提高可读性）：
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, the nuclear option: delete all StatefulSets, PVCs, and PVs in the namespace:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们采取一个更激进的步骤：删除命名空间中的所有StatefulSets、PVCs和PVs。
- en: '[PRE33]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: WARNING Only run that comment in a test cluster! It will delete all objects
    of that type in the namespace, not just the example Redis deployment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：仅在测试集群中运行该注释！它将删除命名空间中该类型的所有对象，而不仅仅是示例Redis部署。
- en: Due to the `Retain` policy on the *StorageClass* (hopefully, you used a storage
    class with `Retain` as instructed!), the cloud disk resource will still exist.
    Now it’s just a matter of manually creating a PV to link to that disk and a PVC
    to link to that.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`StorageClass`上的`Retain`策略（希望您按照说明使用了具有`Retain`的存储类！），云磁盘资源仍然存在。现在的问题只是手动创建一个PV来链接到该磁盘，以及一个PVC来链接到该磁盘。
- en: 'Here’s what we know (figure 9.6):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们所知道的信息（图9.6）：
- en: We know (or can find out) the name of the underlying disk resource from our
    cloud provider
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道（或可以找出）底层磁盘资源的名称来自我们的云提供商
- en: We know the name of the PVC that the StatefulSet will consume (`redis-pvc-redis-0`)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道StatefulSet将要消耗的PVC的名称（`redis-pvc-redis-0`）
- en: '![09-06](../../OEBPS/Images/09-06.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![09-06](../../OEBPS/Images/09-06.png)'
- en: Figure 9.6 The known values and the objects we need to re-create
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 已知值和我们需要重新创建的对象
- en: So, we need to create a PVC with the name `redis-pvc-redis-0` that is bound
    with a PV that references the disk. Importantly, the PVC needs to name the PV,
    and the PV needs to define the bound PVC; otherwise, the PVC could bind a different
    PV, and the PV could be bound by a different PVC.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要创建一个名为`redis-pvc-redis-0`的PVC，并将其绑定到一个引用磁盘的PV上。重要的是，PVC需要命名PV，PV需要定义绑定的PVC；否则，PVC可能会绑定到不同的PV，PV也可能被不同的PVC绑定。
- en: Creating the objects directly from our saved config with `kubectl` `create`
    `-f` `pv.yaml` and `kubectl` `create` `-f` `pvc.yaml` unfortunately won’t work.
    That configuration also exported the *state* of the binding, which uses unique
    identifiers that don’t carry over when you delete and create the object from config.
    If you create those objects without modification, you’ll see that the PVC `Status`
    is `Lost`, and the PV status is `Released`—not what we want.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f pv.yaml`和`kubectl create -f pvc.yaml`直接从我们的保存配置创建对象不起作用。该配置还导出了绑定的*状态*，它使用在从配置删除和创建对象时不会传递的唯一标识符。如果您不修改地创建这些对象，您会看到PVC的`Status`是`Lost`，PV的状态是`Released`——这不是我们想要的。
- en: 'To fix this, we just need to remove the binding status and the `uid`s from
    the saved config:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们只需要从保存的配置中移除绑定状态和`uid`：
- en: 'Edit the PV (the configuration we exported to `pv.yaml`) and make two changes:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑PV（我们导出到`pv.yaml`的配置）并做出两个更改：
- en: Remove the `uid` field from the `claimRef` section (the `claimRef` is the pointer
    to the PVC; the problem is that the PVC’s `uid` has changed).
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`claimRef`部分移除`uid`字段（`claimRef`是指向PVC的指针；问题是PVC的`uid`已更改）。
- en: Set the `storageClassName` to the empty string "" (we’re manually provisioning
    and don’t want to use a `storageClass`).
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`storageClassName`设置为空字符串""（我们手动配置，不想使用`storageClass`）。
- en: 'Edit to the PVC (the configuration we exported to `pvc.yaml`) and make two
    changes:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑PVC（我们导出到`pvc.yaml`的配置）并做出两个更改：
- en: Delete the annotation `pv.kubernetes.io/bind-completed:` `"yes"` (this PVC needs
    to be re-bound, and this annotation will prevent that).
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除注释`pv.kubernetes.io/bind-completed:` `"yes"`（这个PVC需要重新绑定，而这个注释将阻止这种情况）。
- en: Set the `storageClassName` to the empty string "" (same reason as the previous
    step).
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`storageClassName`设置为空字符串""（与上一步相同的原因）。
- en: Alternatively, if you’re re-creating this config from scratch, the key is that
    the `volumeName` of the PVC needs to be set to that of the PV, the `claimRef`
    of the PV needs to reference the PVC’s name and namespace, and both have the `storageClassName`
    of `""`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您是从头开始重新创建此配置，关键是PVC的`volumeName`需要设置为PV的`volumeName`，PV的`claimRef`需要引用PVC的名称和命名空间，并且两者都具有`storageClassName`为`""`。
- en: It’s easier to visualize side by side. Figure 9.7 is based on the configuration
    I exported when I ran this test and removed the fields as documented as previously
    outlined.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 逐行可视化会更直观。图9.7是基于我在运行此测试并按照之前概述的文档移除字段时导出的配置。
- en: '![09-07](../../OEBPS/Images/09-07.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![09-07](../../OEBPS/Images/09-07.png)'
- en: Figure 9.7 Prelinked PVC and PV objects
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 预链接的PVC和PV对象
- en: Once prepared, you can create both configuration files in the cluster and then
    inspect their status with `kubectl` `get` `pvc,pv`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 准备就绪后，您可以在集群中创建这两个配置文件，然后使用`kubectl get pvc,pv`检查它们的状态。
- en: 'If it goes correctly, the status for the PV should read `Bound`, and the PVC
    `Pending` (while it waits for the StatefulSet to be created). If, instead, one
    or both are listed as `Pending` or `Released`, go back and check that they are
    linked correctly with all the information needed and without any extra information.
    Yes, unfortunately, this is a bit of a pain, but it is possible to rebind these
    objects, provided that the underlying cloud resource is still there (which it
    will be, since you used the `Retain` policy on your StorageClass, didn’t you?).
    This is what success looks like (with some columns removed for readability):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，PV的状态应该显示为`Bound`，而PVC则显示为`Pending`（在等待StatefulSet创建时）。如果其中一个或两个都列为`Pending`或`Released`，请返回并检查它们是否与所有必要的信息正确链接，且没有额外的信息。是的，遗憾的是，这确实有些麻烦，但如果底层的云资源仍然存在（由于你在StorageClass上使用了`Retain`策略，对吧？），这些对象是可以重新绑定的。这就是成功的样子（为了可读性移除了一些列）：
- en: '[PRE34]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Once you have your manually created PVC and PV objects, it’s time to re-create
    the StatefulSet. As we tested earlier when we deleted and re-created the StatefulSet,
    as long as the PVC exists with the expected name, it will be reattached to the
    Pods of the StatefulSet. The name of the PVCs that the StatefulSet creates is
    deterministic, so when we re-create the StatefulSet, it will see the existing
    PVC objects and reference them rather than creating new ones. Basically, everything
    should work the same as when we re-created these objects using the same names
    as before.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你手动创建了PVC和PV对象，就是时候重新创建StatefulSet了。正如我们之前在删除和重新创建StatefulSet时测试的那样，只要PVC存在且名称符合预期，它就会重新附加到StatefulSet的Pod上。StatefulSet创建的PVC名称是确定的，因此当我们重新创建StatefulSet时，它会看到现有的PVC对象并引用它们，而不是创建新的。基本上，一切都应该和之前使用相同名称重新创建这些对象时一样工作。
- en: Notice, in this example, that although the StatefulSet has three PVCs and, therefore,
    three associated disks, we only manually recovered one disk—the one attached to
    the primary Pod. The Redis replicas will automatically re-create their data from
    that source. You can, of course, manually re-link all three disks.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个例子中，尽管StatefulSet有三个PVC，因此有三个相关的磁盘，但我们只手动恢复了其中一个磁盘——即连接到主Pod的磁盘。Redis副本将自动从该源重新创建其数据。当然，你也可以手动重新链接所有三个磁盘。
- en: '[PRE35]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With the StatefulSet created, all the PV and PVC objects show `Bound`. Once
    the StatefulSet is deployed, let’s exec into one of those replicas and see whether
    the data we created earlier is still there:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet创建完成后，所有的PV和PVC对象都显示为`Bound`。一旦StatefulSet部署完成，让我们进入其中一个副本，看看我们之前创建的数据是否还在那里：
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you can read back the data written to Redis earlier, congratulations! You’ve
    recovered the StatefulSet from scratch. This same technique can be used to migrate
    a disk to a StatefulSet in a new cluster. Just follow these steps, but create
    the objects in a new cluster. Pay attention to the location of the disk as typically
    the cluster needs to be in the same region.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够读取之前写入Redis的数据，恭喜你！你已经从头恢复了StatefulSet。同样的技术可以用来将磁盘迁移到新集群中的StatefulSet。只需遵循这些步骤，但在新集群中创建对象。请注意磁盘的位置，因为通常集群需要位于同一区域。
- en: I hope this section has given you some confidence about the persistence of the
    data when the `Retain` policy is used. As demonstrated, you can completely delete
    all the objects (heck, even the entire cluster) and re-create all the links from
    scratch. It’s a bit laborsome, but it’s possible. To reduce the toil, it’s advisable
    (but not essential) to export the config for your PVC and PV objects and store
    them in your configuration repository to make it faster to re-create these objects
    in the future.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一节已经让你对使用`Retain`策略时数据的持久性有了信心。正如所示，你可以完全删除所有对象（甚至整个集群），并从头开始重新创建所有链接。这确实有些费劲，但却是可行的。为了减少工作量，建议（但不是必需的）导出你的PVC和PV对象的配置，并将它们存储在你的配置仓库中，以便将来更快地重新创建这些对象。
- en: 9.4 Generic ephemeral volume for scratch space
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 通用临时卷用于临时空间
- en: So far, we’ve used *PersistentVolumes and PersistentVolumesClaims* for stateful
    services. What about when you just need a really big disk to do some temporary
    calculations—scratch space for tasks like data processing? At the beginning of
    the chapter, `emptyDir` was mentioned as an option for scratch space, but it has
    some drawbacks. Namely, you need to preallocate storage on the node to be able
    to use `emptyDir`, which requires a upfront planning of node boot disk sizes (and
    which may not be possible at all on platforms that don’t expose nodes). Generic
    ephemeral volumes are a way to get scratch space by mounting an attached volume
    in the same way we do persistent volumes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了*PersistentVolumes和PersistentVolumesClaims*来为有状态服务提供支持。那么，当您只需要一个很大的磁盘来进行一些临时计算——例如数据处理任务的临时空间时怎么办？在本章的开头，`emptyDir`被提及为一种临时空间的选择，但它有一些缺点。具体来说，您需要在节点上预分配存储空间才能使用`emptyDir`，这需要预先规划节点启动磁盘的大小（而且在没有暴露节点的平台上可能根本不可能做到）。通用临时卷是通过以与持久卷相同的方式挂载附加卷来获取临时空间的一种方法。
- en: There are numerous benefits of using ephemeral volumes over `emptyDir` when
    you have large amounts of temporary data to handle. By being independent of the
    boot disk, you can provision very large volumes of space on the fly without prior
    planning (e.g., Google Cloud supports up to 64 TB at the time of this writing).
    You can attach multiple volumes, too, so that limit is a per-volume limit. You
    can also access different storage classes and configure different attributes on
    the storage class, like, for example, provisioning a higher-performant SSD disk
    than the node’s own boot disk. The following listing provides an example.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要处理大量临时数据时，使用临时卷而不是`emptyDir`有许多好处。由于与启动磁盘独立，您可以在不进行预先规划的情况下动态分配非常大的空间（例如，在撰写本文时，Google
    Cloud支持高达64 TB）。您还可以挂载多个卷，因此这个限制是每个卷的限制。您还可以访问不同的存储类，并在存储类上配置不同的属性，例如，配置一个比节点自己的启动磁盘性能更高的SSD磁盘。以下列表提供了一个示例。
- en: Listing 9.11 Chapter09/9.4_EphemeralVolume/ephemeralvolume_pod.yaml
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 第09章/9.4_临时卷/ephemeralvolume_pod.yaml
- en: '[PRE37]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Mount point for the scratch volume
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 临时卷的挂载点
- en: ❷ Defines a 1TB ephemeral volume
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个1TB的临时卷
- en: When using generic ephemeral volumes, you want to ensure that your storage class
    has a reclaim policy set to `Delete`. Otherwise, the ephemeral storage will be
    retained, which is not really the point. The following listing provides such a
    StorageClass.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用通用临时卷时，您想要确保您的存储类已设置reclaim策略为`Delete`。否则，临时存储将被保留，这并不是真正的目的。以下列表提供了一个这样的StorageClass。
- en: Listing 9.12 Chapter09/9.4_EphemeralVolume/ephemeral_storageclass.yaml
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.12 第09章/9.4_临时卷/ephemeral_storageclass.yaml
- en: '[PRE38]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Set the disk type based on the performance requirements of the ephemeral storage.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据临时存储的性能要求设置磁盘类型。
- en: ❷ The Delete reclaimPolicy is used as this is intended for ephemeral use.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Delete reclaimPolicy是因为这是为临时使用而设计的。
- en: 'Putting it together, let’s run, inspect, and clean up the sample. The following
    commands (run from the sample root directory) show the Pod being created, with
    a 1 TB disk attached, and then deleted, which cleans up all the resources (output
    is truncated for readability):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些放在一起，让我们运行、检查并清理示例。以下命令（从示例根目录运行）显示了创建的Pod，其中附加了一个1TB磁盘，然后删除了它，这清理了所有资源（输出被截断以提高可读性）：
- en: '[PRE39]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ The 1TiB volume is available for use.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 1TiB卷可供使用。
- en: As you can see, with ephmeral volumes, deleting the Pod will delete the associated
    PVC object. This is unlike the StatefulSet where the PVC object needs to be deleted
    manually when deleting the StatefulSet. You can also wrap this Pod into a Deployment,
    where each replica will get its own ephemeral volume.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用临时卷时，删除Pod将删除相关的PVC对象。这与StatefulSet不同，当删除StatefulSet时，需要手动删除PVC对象。您还可以将此Pod包装在一个Deployment中，其中每个副本都将获得自己的临时卷。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Kubernetes isn’t just confined to running stateless workloads; it can also deftly
    handle stateful workloads.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes不仅限于运行无状态工作负载；它还可以巧妙地处理有状态工作负载。
- en: Kubernetes supports several types of volumes, including the ability to directly
    mount a persistent disk from a cloud provider resource.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes支持多种类型的卷，包括直接从云提供商资源挂载持久磁盘的能力。
- en: PersistentVolume and PersistentVolumeClaim, together with StorageClass, are
    Kubernetes’s abstraction layer for dynamically provisioned disk resources and
    make the workload portable.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PersistentVolume和PersistentVolumeClaim，连同StorageClass一起，是Kubernetes动态分配磁盘资源的抽象层，并使工作负载可移植。
- en: StatefulSet is the high-level workload type designed for running stateful workloads
    with advantages such as being able to define different roles for each replica.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSet是为运行有状态工作负载而设计的高级工作负载类型，具有诸如能够为每个副本定义不同角色等优势。
- en: PersistentVolume and PersistentVolumeClaim objects have a complex lifecycle,
    starting as a request and then being bound into a single logical object.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PersistentVolume和PersistentVolumeClaim对象具有复杂的生命周期，从请求开始，然后绑定成一个单独的逻辑对象。
- en: StorageClasses can be configured to enable dynamic storage with your preferred
    options—most importantly, the option to retain the cloud provider disk resources
    should the Kubernetes objects be deleted.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以配置StorageClasses以启用具有您首选选项的动态存储——最重要的是，如果Kubernetes对象被删除，应该保留云提供商磁盘资源的选项。
- en: Data can be recovered even if all objects in the cluster are deleted, provided
    you use the Retain reclaim policy.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使集群中的所有对象都被删除，只要使用保留回收策略，数据仍然可以恢复。
- en: Generic ephemeral volumes provide a way to use mounted disks for ephemeral scratch
    space.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用临时卷提供了一种使用挂载磁盘作为临时擦除空间的方法。
- en: '* * *'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
- en: ^(2.) [https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: (2.) [https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/)
