- en: 13 Toward artificial general intelligence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 向着通用人工智能迈进
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节
- en: You will look back at the algorithms you learned in this book, as well as learn
    about deep reinforcement learning methods that weren’t covered in depth.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将回顾本书中学到的算法，以及了解那些没有深入探讨的深度强化学习方法。
- en: You will learn about advanced deep reinforcement learning techniques that, when
    combined, allow agents to display more general intelligence.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解高级深度强化学习技术，当这些技术结合在一起时，能够让智能体展现出更广泛的智能。
- en: You will get my parting advice on how to follow your dreams and contribute to
    these fabulous fields of artificial intelligence and deep reinforcement learning.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将得到我关于如何追逐梦想并为这些令人惊叹的人工智能和深度强化学习领域做出贡献的离别建议。
- en: Our ultimate objective is to make programs that learn from their experience
    as effectively as humans do.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终目标是创建出能够像人类一样有效地从经验中学习的程序。
- en: — John McCarthy Founder of the field of Artificial Intelligence Inventor of
    the Lisp programming language
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 约翰·麦卡锡 人工智能领域的创始人 Lisp编程语言的发明者
- en: In this book, we have surveyed a wide range of decision-making algorithms and
    reinforcement learning agents; from the planning methods that you learned about
    in chapter 3 to the state-of-the-art deep reinforcement learning agents that we
    covered in the previous chapter. The focus of this book is to teach the ins and
    outs of the algorithms; however, there’s more to DRL than what we covered in this
    book, and I want you to have some direction going forward.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们调查了广泛的决策算法和强化学习智能体；从你在第三章中学到的规划方法到我们在上一章中涵盖的最先进的深度强化学习智能体。本书的重点是教授算法的细节；然而，深度强化学习（DRL）的内容远不止本书所涵盖的，我希望你能有一个前进的方向。
- en: I designed this chapter to hit a couple of points. In the first section, we
    recap the entire book. I’d like you to zoom out and look at the big picture again.
    I want you to see what you’ve learned so that you can choose for yourself where
    to go next. I also mention several of the notable types of agents that I couldn’t
    cover before I ran out of pages. But, know that while there are more types of
    algorithms, what you learned in this book covers the foundational methods and
    concepts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我设计这一章节是为了强调几个要点。在第一部分，我们回顾了整本书。我希望你能退后一步，再次审视全局。我希望你能看到你所学到的，以便你可以自己决定下一步该去哪里。我还提到了几种我在书页用尽之前未能涵盖的显著类型的智能体。但要知道，尽管有更多类型的算法，本书中所学的内容涵盖了基础的方法和概念。
- en: After going over what was covered and what wasn’t, I introduce several of the
    more advanced research areas in DRL that may lead to the eventual creation of
    artificial general intelligence (AGI). I know AGI is a hot topic, and lots of
    people use it in a deceiving way. Being such an exciting and controversial topic,
    people use it to get attention. Don’t give your energy to those folks; don’t be
    misled; don’t get distracted. Instead, focus on what matters, what’s right in
    front of you. And make progress toward your goals, whatever they may be.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述了所涵盖的内容和未涵盖的内容之后，我介绍了几个在深度强化学习（DRL）中更为高级的研究领域，这些领域可能最终会导致通用人工智能（AGI）的创造。我知道AGI是一个热门话题，很多人都在误导性地使用它。作为一个既令人兴奋又具有争议的话题，人们用它来吸引注意力。不要把你的精力浪费在那些人身上；不要被误导；不要分心。相反，专注于眼前的事情。朝着你的目标前进，无论你的目标是什么。
- en: I do believe humans can create AGI because we’ll keep trying forever. Understanding
    intelligence and automating tasks is a quest we’ve been longing for and working
    on for centuries, and that’s never going to change. We try to understand intelligence
    through philosophy and the understanding of the self. We look for answers about
    intelligence through introspection. I would argue that most AI researchers are
    part-time philosophers themselves. They use what they’ve learned in reinforcement
    learning to better themselves and the other way around, too.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实相信人类能够创造出通用人工智能（AGI），因为我们将会永远地尝试。理解智能和自动化任务是我们长久以来渴望并努力追求的目标，而且这种情况永远不会改变。我们试图通过哲学和自我理解来理解智能。我们通过内省寻找关于智能的答案。我认为大多数人工智能研究者本质上也是哲学家。他们利用他们在强化学习中学到的知识来提升自己，反之亦然。
- en: Also, humans love automation; that’s what intelligence has allowed us to do.
    We’re going to continue trying to automate life, and we’ll get there. Now, while
    we can argue whether AGI is the beginning of human-like robots that overtake the
    world, today, we still cannot train a single agent to play all Atari games at
    a super-human level. That is, a single trained agent cannot play all games, though
    a single general-purpose algorithm can be trained independently. But, we should
    be cautious when considering AGI.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，人类喜欢自动化；这就是智能让我们能够做到的事情。我们将继续尝试自动化生活，我们最终会实现这一点。现在，虽然我们可以争论通用人工智能（AGI）是否是超越世界的类人机器人的开始，但今天，我们仍然无法训练一个代理以超人类水平玩所有Atari游戏。也就是说，单个训练过的代理不能玩所有游戏，尽管可以独立训练单个通用算法。但是，在考虑AGI时，我们应该谨慎。
- en: 'To close the chapter and the book, I provide ideas for you going forward. I
    receive many questions regarding applying DRL to custom problems, environments
    of your own. I do this for a living on my full-time job, so I can share my two
    cents as to how to go about it. I also give career advice for those interested,
    and a parting message. It’s one more chapter: let’s do this.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束这一章和整本书，我为你提供了前进的想法。我收到了很多关于如何将DRL应用于自定义问题、你自己的环境的提问。我在全职工作中以此为生，所以我可以分享我的看法，关于如何着手去做。我还为有兴趣的人提供职业建议，以及一个告别信息。这是另一个章节：让我们这么做吧。
- en: What was covered and what notably wasn’t?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 覆盖了什么，又明显没有涵盖什么？
- en: This book covers most of the foundations of deep reinforcement learning, from
    MDPs and their inner workings to state-of-the-art actor-critic algorithms and
    how to train them in complex environments. Deep reinforcement learning is an active
    research field in which new algorithms are published every month. The field is
    advancing at a rapid pace, and it’s unfortunately not possible to provide high-quality
    explanations for everything there is in a single book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖了深度强化学习的大部分基础，从马尔可夫决策过程（MDPs）及其内部工作原理到最先进的演员-评论家算法以及如何在复杂环境中训练它们。深度强化学习是一个活跃的研究领域，每个月都会发布新的算法。该领域正在快速发展，不幸的是，不可能在单本书中提供对一切的高质量解释。
- en: Thankfully, most of the things left out are advanced concepts that aren’t required
    in most applications. That doesn’t mean that they aren’t relevant; I highly recommend
    you continue the journey of learning DRL. You can count on me to help along the
    way; I’m easy to find. For now, though, of the things left out of this book, I
    only consider two essential; they’re model-based deep reinforcement learning methods
    and derivative-free optimization methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数被省略的内容都是高级概念，在大多数应用中并不需要。但这并不意味着它们不重要；我强烈建议你继续学习深度强化学习（DRL）的旅程。你可以依赖我在路上的帮助；我很容易找到。然而，就这本书省略的内容而言，我只认为两个是基本要素；它们是基于模型的深度强化学习方法和无导数优化方法。
- en: In this section, we quickly review the algorithms and methods that you learned
    about in this book and touch on these two essential methods that were notably
    missing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们快速回顾了你在本书中学到的算法和方法，并简要介绍了这两个明显缺失的基本方法。
- en: '![](../Images/13_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_01.png)'
- en: Comparison of different algorithmic approaches to deep reinforcement learning
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习不同算法方法的比较
- en: Markov decision processes
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: The first two chapters were an introduction to the field of reinforcement learning
    and to the way we describe the problems we’re trying to solve. MDPs are an essential
    concept to have in mind, and even if they look simple and limited, they’re powerful.
    There’s much more we could have explored in this area. The thing I want you to
    take from these concepts is the ability to think of problems as MDPs. Practice
    this yourself. Think about a problem, and break it down into states, observations,
    actions, and all the components that would make that problem an MDP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前两章是强化学习领域的介绍以及我们描述试图解决的问题的方式。MDPs是一个需要记住的基本概念，即使它们看起来简单且有限，它们也是强大的。我们在这个领域可以探索的还有很多。我希望你从这些概念中获得的能力是能够将问题视为MDPs。自己练习一下。思考一个问题，并将其分解为状态、观察、动作以及使该问题成为MDPs的所有组成部分。
- en: '![](../Images/13_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_02.png)'
- en: The transition function of the frozen lake environment
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖环境的转换函数
- en: You’ll notice that even though it seems the world is non-stationary and non-Markovian,
    we can transform some of the things that make it seem that way, and then see the
    world as an MDP. Do the probability distributions of the real world change, or
    is it that we don’t have enough data to determine the actual distributions? Does
    the future depend on past states, or is the state space so high-dimensional that
    we can’t conceive all history of the world being part of a single state? Again,
    as an exercise, think of problems and try to fit them into this MDP framework.
    It may turn out to be useful if you want to apply DRL to your problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，尽管世界看起来是非平稳和非马尔可夫的，但我们仍然可以转换一些使其看起来如此的东西，然后将其视为一个MDP。现实世界的概率分布是否改变，或者是我们没有足够的数据来确定实际的分布？未来是否取决于过去的状态，或者状态空间如此高维，以至于我们无法想象世界的历史是单个状态的一部分？再次，作为一个练习，思考问题并尝试将它们拟合到这个MDP框架中。如果你想要将DRL应用于你的问题，这可能是有用的。
- en: Planning methods
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规划方法
- en: In the third chapter, we discussed methods that help you find optimal policies
    of problems that have MDPs available. These methods, such as value iteration and
    policy iteration, iteratively compute the optimal value functions, which in turn
    allow extracting optimal policies quickly. Policies are nothing but universal
    plans—a plan for every situation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们讨论了帮助你找到具有MDP的问题的最优策略的方法。这些方法，如值迭代和政策迭代，迭代地计算最优值函数，从而允许快速提取最优策略。策略不过是普遍的计划——针对每种情况的计划。
- en: '![](../Images/13_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_03.png)'
- en: Policy evaluation on the always-left policy on the SWF environment
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在SWF环境中对始终向左策略进行策略评估
- en: The two most important takeaways from this section are first. These algorithms
    isolate sequential decision-making problems. There’s no uncertainty because they
    require the MDP, and there’s no complexity because they only work for discrete
    state and action spaces. Second, there’s a general pattern to see here; we’re
    interested in evaluating behaviors, perhaps as much as we’re interested in improving
    them. This realization was something I didn’t get for some time. To me, improving,
    optimizing sounded more interesting, so policy evaluation methods didn’t get my
    attention. But you later understand that if you get evaluation right, improving
    is a piece of cake. The main challenge is usually evaluating policies accurately
    and precisely. But, if you have the MDP, you calculate these values correctly
    and straightforwardly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节最重要的两点收获是首先。这些算法隔离了序列决策问题。由于它们需要MDP，因此没有不确定性，由于它们只适用于离散状态和动作空间，因此没有复杂性。其次，这里有一个普遍的模式可以观察；我们对评估行为感兴趣，可能与我们改进它们的兴趣一样大。这种认识是我一段时间后才意识到的。对我来说，改进、优化听起来更有趣，所以策略评估方法没有引起我的注意。但后来你理解，如果你正确评估，改进就变得轻而易举。主要的挑战通常是准确和精确地评估策略。但是，如果你有MDP，你就可以直接正确地计算这些值。
- en: Bandit methods
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bandit methods
- en: The fourth chapter was about learning from evaluative feedback. In this case,
    we learned about the uncertainty aspect of reinforcement learning by taking the
    MDP away. We hide the MDP, but make the MDP super simple; a single-state single-step
    horizon MDP, in which the challenge is to find the optimal action or action distribution
    in the fewest number of episodes; that is, minimizing total regret.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章讲述了从评估反馈中进行学习。在这种情况下，我们通过移除MDP来了解强化学习的不确定性方面。我们隐藏了MDP，但使其超级简单；一个单状态单步长MDP，其中的挑战是在最少的回合数中找到最优动作或动作分布；也就是说，最小化总后悔。
- en: '![](../Images/13_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_04.png)'
- en: In chapter 4, you learned more effective ways for dealing with the exploration-exploitation
    trade-off
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四章，你学习了处理探索-利用权衡的更有效的方法
- en: We studied several different exploration strategies and tested them in a couple
    of bandit environments. But at the end of the day, my goal for that chapter was
    to show you that uncertainty on its own creates a challenge worth studying separately.
    There are a few great books about this topic, and if you’re interested in it,
    you should pursue that path; it’s a reasonable path that needs much attention.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了多种不同的探索策略，并在几个bandit环境中进行了测试。但最终，我对那一章的目标是向你展示，不确定性本身就是一个值得单独研究的挑战。关于这个主题有一些非常好的书籍，如果你对此感兴趣，你应该追求这条道路；这是一个需要大量关注的合理路径。
- en: '![](../Images/13_05.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_05.png)'
- en: 10-armed Gaussian bandits
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 10臂高斯bandit
- en: The right nugget to get out of this chapter and into reinforcement learning
    is that reinforcement learning is challenging because we don’t have access to
    the MDP, as in the planning methods in chapter 3\. Not having the MDP creates
    uncertainty, and we can only solve uncertainty with exploration. Exploration strategies
    are the reason why our agents can learn on their own, by trial-and-error learning,
    and it’s what makes this field exciting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章中提炼出正确的要点进入强化学习是，强化学习之所以具有挑战性，是因为我们没有访问到第3章中规划方法中的MDP。没有MDP会产生不确定性，而我们只能通过探索来解决不确定性。探索策略是我们代理能够通过试错学习自行学习的理由，这也是这个领域令人兴奋的原因。
- en: Tabular reinforcement learning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表格强化学习
- en: Chapters 5, 6, and 7 are all about mixing the sequential and uncertain aspects
    of reinforcement learning. Sequential decision-making problems under uncertainty
    are at the core of reinforcement learning when presented in a way that can be
    more easily studied; that is, without the complexity of large and high-dimensional
    state or action spaces.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章、第6章和第7章都是关于强化学习的顺序性和不确定性方面的细微差别。在可以更容易研究的方式下，即没有大型和高维状态或动作空间复杂性的情况下，不确定性下的顺序决策问题是强化学习的核心。
- en: Chapter 5 was about evaluating policies, chapter 6 was about optimizing policies,
    and chapter 7 was about advanced techniques for evaluating and optimizing policies.
    To me, this is the core of reinforcement learning, and learning these concepts
    well helps you understand deep reinforcement learning more quickly. Don’t think
    of DRL as something separate from tabular reinforcement learning; that’s the wrong
    thinking. Complexity is only one dimension of the problem, but it’s the same exact
    problem. You often see top, deep reinforcement learning–research labs releasing
    papers solving problems in discrete state and action spaces. There’s no shame
    in that. That’s often the smart approach, and you should have that in mind when
    you experiment. Don’t start with the highest-complexity problem; instead, isolate,
    then tackle, and finally, increase complexity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章讲述了评估策略，第6章讲述了优化策略，第7章讲述了评估和优化策略的高级技术。对我来说，这是强化学习的核心，而且对这些概念的理解有助于你更快地理解深度强化学习。不要将深度强化学习视为与表格强化学习分离的东西；那是错误的想法。复杂性只是问题的一个维度，但它是完全相同的问题。你经常看到顶级深度强化学习研究实验室发布解决离散状态和动作空间问题的论文。这没有什么可耻的。这通常是明智的方法，当你进行实验时，你应该记住这一点。不要从最高复杂性的问题开始；相反，先隔离，然后解决，最后再增加复杂性。
- en: In these three chapters, we covered a wide variety of algorithms. We covered
    evaluation methods such as first-visit and every-visit Monte Carlo prediction,
    temporal-difference prediction, *n*-step *TD*, and *TD*(*λ*)). We also covered
    control methods such as first-visit and every-visit Monte Carlo control, SARSA,
    Q-learning, double Q-learning, and more advanced methods, such as SARSA(*λ*) and
    Q(*λ*) both with replacing and also with accumulating traces. We also covered
    model-based approaches, such as Dyna-Q and trajectory sampling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三章中，我们涵盖了各种算法。我们涵盖了评估方法，如首次访问和每次访问蒙特卡洛预测、时序差分预测、*n*-步时序差分（*TD*）和*TD*(*λ*)）。我们还涵盖了控制方法，如首次访问和每次访问蒙特卡洛控制、SARSA、Q学习、双Q学习以及更高级的方法，如SARSA(*λ*)和Q(*λ*)，这两种方法都涉及替换和累积轨迹。我们还涵盖了基于模型的方法，如Dyna-Q和轨迹采样。
- en: '![](../Images/13_06.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_06.png)'
- en: Deep reinforcement learning is part of the larger field of reinforcement learning
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是强化学习更大领域的一部分
- en: Value-based deep reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于价值的深度强化学习
- en: Chapters 8, 9, and 10 are all about the nuances of value-based deep reinforcement
    learning methods. We touched on neural fitted Q-iteration (NFQ), deep Q-networks
    (DQN), double deep Q-networks (DDQN), dueling architecture in DDQN (dueling DDQN),
    and prioritized experience replay (PER). We started with DQN and added improvements
    to this baseline method one at a time. We tested all algorithms in the cart-pole
    environment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章、第9章和第10章都是关于基于价值的深度强化学习方法的细微差别。我们提到了神经拟合Q迭代（NFQ）、深度Q网络（DQN）、双深度Q网络（DDQN）、DDQN中的对抗架构（对抗DDQN）和优先级经验回放（PER）。我们从DQN开始，并逐个添加改进到这个基线方法。我们在小车-杆环境中测试了所有算法。
- en: There are many more improvements that one can implement to this baseline algorithm,
    and I recommend you try that. Check out an algorithm called Rainbow, and implement
    some of the improvements to DQN not in this book. Create a blog post about it
    and share it with the world. Techniques you learn when implementing value-based
    deep reinforcement learning methods are essential to other deep reinforcement
    learning approaches, including learning critics in actor-critic methods. There
    are a many improvements and techniques to discover. Keep playing around with these
    methods.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对这个基线算法实施许多改进，我建议你尝试一下。查看一个名为Rainbow的算法，并实现一些书中未提到的DQN的改进。写一篇关于它的博客文章，并与世界分享。在实现基于价值的深度强化学习方法时学到的技术对于其他深度强化学习方法至关重要，包括演员-评论家方法中的学习评论家。有许多改进和技术需要发现。继续对这些方法进行实验。
- en: Policy-based and actor-critic deep reinforcement learning
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于策略的演员-评论家深度强化学习
- en: Chapter 11 was an introduction to policy-based and actor-critic methods. Policy-based
    was a new approach to reinforcement learning at that point in the book, so we
    introduced the concepts in a straightforward algorithm known as REINFORCE, which
    only parameterizes the policy. For this, we approximated the policy directly and
    didn’t use any value function at all. The signal that we use to optimize the policy
    in REINFORCE is the Monte Carlo return, the actual returns experienced by the
    agent during an episode.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章是关于基于策略的演员-评论家方法的介绍。在书的那个阶段，基于策略是强化学习的一个新方法，所以我们通过一个简单的算法介绍了这些概念，即REINFORCE，它只参数化策略。为此，我们直接近似策略，完全没有使用任何价值函数。在REINFORCE中，我们用来优化策略的信号是蒙特卡洛回报，即代理在一段时间内实际体验到的回报。
- en: We then explored an algorithm that learns a value function to reduce the variance
    of the MC return. We called this algorithm vanilla policy gradient (VPG). The
    name is somewhat arbitrary, and perhaps a better name would have been *rEINFORCE
    with Baseline*. Nevertheless, it’s important to note that this algorithm, even
    though it learns a value function, is not an actor-critic method because it uses
    the value function as a baseline and not as a critic. The crucial insight here
    is that we don’t use the value function for bootstrapping purposes, and because
    we also train the value-function model using MC returns, there’s minimal bias
    in it. The only bias in the algorithm is the bias introduced by the neural network,
    nothing else.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们探索了一种学习价值函数以减少MC回报方差的算法。我们称这个算法为vanilla策略梯度（VPG）。这个名字有些随意，也许更好的名字应该是*rEINFORCE
    with Baseline*。尽管如此，重要的是要注意，尽管这个算法学习了一个价值函数，但它不是一个演员-评论家方法，因为它使用价值函数作为基线而不是作为评论家。这里的关键洞察是我们不使用价值函数进行自举，而且因为我们还使用MC回报来训练价值函数模型，所以它的偏差最小。算法中的唯一偏差是由神经网络引入的，没有其他。
- en: Then, we covered more advanced actor-critic methods that do use bootstrapping.
    A3C, which uses *n*-step returns; GAE, which is a form of lambda return for policy
    updates; and A2C, which uses synchronous updates to the policy. Overall, these
    are state-of-the-art methods, and you should know that they’re reliable methods
    that are still widely used. One of the main advantages and unique characteristics
    of A3C, for instance, is that it only needs CPUs, and can train faster than other
    methods, if you lack a GPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们介绍了更多使用自举的先进演员-评论家方法。A3C使用*n*-步回报；GAE，它是一种用于策略更新的lambda回报形式；以及A2C，它使用同步更新策略。总的来说，这些是当前最先进的方法，你应该知道它们是可靠的方法，并且仍然被广泛使用。例如，A3C的一个主要优点和独特特性是它只需要CPU，并且可以比其他方法更快地训练，如果你没有GPU的话。
- en: Advanced actor-critic techniques
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先进的演员-评论家技术
- en: Even though A3C, GAE, and A2C, are actor-critic methods, they don’t use the
    critic in unique ways. In chapter 12, we explored methods that do. For instance,
    many people consider DDPG and *TD*3 actor-critic methods, but they fit better
    as value-based methods for continuous action spaces. If you look at the way A3C
    uses the actor and the critic, for instance, you find substantial differences
    in DDPG. Regardless, DDPG and *TD*3 are state-of-the-art methods, and whether
    actor-critic or not, it doesn’t make much of a difference when solving a problem.
    The main caveat is that these two methods can only solve continuous action-space
    environments. They could be high-dimensional action spaces, but the actions must
    be continuous. Other methods, such as A3C, can solve both continuous and discrete
    action spaces.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 即使A3C、GAE和A2C都是演员-评论家方法，它们在独特地使用评论家方面并不相同。在第12章中，我们探讨了使用独特方法的情况。例如，许多人认为DDPG和*TD*3是演员-评论家方法，但它们更适合作为连续动作空间的基于价值的方法。如果你看看A3C使用演员和评论家的方式，例如，你会在DDPG中发现实质性的差异。无论如何，DDPG和*TD*3都是最先进的方法，无论是否是演员-评论家，在解决问题时影响不大。主要的注意事项是这两种方法只能解决连续动作空间的环境。它们可能是高维动作空间，但动作必须是连续的。其他方法，如A3C，可以解决连续和离散动作空间。
- en: SAC is an animal of its own. The only reason why it follows after DDPG and *TD*3
    is because SAC uses many of the same techniques as DDPG and *TD*3\. But the unique
    characteristic of SAC is that it’s an entropy-maximization method. The value function
    maximizes not only the return but also the entropy of the policy. These kinds
    of methods are promising, and I wouldn’t be surprised to see new state-of-the-art
    methods that derive from SAC.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SAC是一种独特的动物。它之所以跟在DDPG和*TD*3之后，仅仅是因为SAC使用了与DDPG和*TD*3许多相同的技巧。但SAC的独特特性是它是一种熵最大化方法。值函数不仅最大化回报，还最大化策略的熵。这类方法很有前景，我不惊讶会看到源自SAC的新最先进的方法。
- en: Finally, we looked at another exciting kind of actor-critic method with PPO.
    PPO is an actor-critic method, and you probably notice that because we reused
    much of the code from A3C. The critical insight with PPO is the policy update
    step. In short, PPO improves the policy a bit at a time; we make sure the policy
    doesn’t change too much with an update. You can think of it as a conservative
    policy-optimization method. PPO can be easily applied to both continuous and discrete
    action spaces, and PPO is behind some of the most exciting results in DRL, such
    as OpenAI Five, for instance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了另一种令人兴奋的演员-评论家方法，即PPO。PPO是一种演员-评论家方法，你可能已经注意到了，因为我们重用了A3C的大部分代码。PPO的关键洞察力在于策略更新步骤。简而言之，PPO一次只改进策略一点；我们确保策略在更新时不会改变太多。你可以将其视为一种保守的策略优化方法。PPO可以轻松应用于连续和离散动作空间，并且PPO是DRL中一些最激动人心的结果背后的原因，例如OpenAI
    Five。
- en: We covered many great methods throughout these chapters, but more importantly,
    we covered the foundational methods that allow you to understand the field going
    forward. Many of the algorithms out there derive from the algorithms we covered
    in this book, with a few exceptions, namely, model-based deep reinforcement learning
    methods, and derivative-free optimization methods. In the next two sections, I
    give insights into what these methods are so that you can continue your journey
    exploring deep reinforcement learning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些章节中，我们介绍了许多优秀的方法，但更重要的是，我们介绍了使你能够继续了解该领域的基础方法。许多现有的算法都源自本书中介绍过的算法，有一些例外，即基于模型的深度强化学习方法和无导数优化方法。在接下来的两个部分中，我将提供关于这些方法的信息，以便你能够继续探索深度强化学习之旅。
- en: '![](../Images/13_07.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_07.png)'
- en: DRL algorithms in this book
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的DRL算法
- en: Model-based deep reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于模型的深度强化学习
- en: In chapter 7, you learned about model-based reinforcement learning methods such
    as Dyna-Q and trajectory sampling. Model-based deep reinforcement learning is,
    at its core, what you’d expect; the use of deep learning techniques for learning
    the transition, the reward function, or both, and then using that for decision
    making. As with the methods you learned about in chapter 7, one of the significant
    advantages of model-based deep reinforcement learning is sample efficiency; model-based
    methods are the most sample efficient in reinforcement learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，你学习了基于模型的重强化学习方法，例如Dyna-Q和轨迹采样。基于模型的重深度强化学习在本质上是你所预期的；使用深度学习技术来学习状态转移、奖励函数或两者，然后使用这些信息进行决策。与你在第7章中学到的那些方法一样，基于模型的重深度强化学习的一个显著优点是样本效率；基于模型的方法在强化学习中是最有效的样本使用方法。
- en: '![](../Images/13_08.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13_08.png)'
- en: Model-based reinforcement learning algorithms to have in mind
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的基于模型的重强化学习算法
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyHow derivative-free methods
    work |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Miguel.png) | 米格尔的类比如何无导数方法工作 |'
- en: '|  | To get an intuitive sense of how gradient-based and gradient-free methods
    compare, imagine for a second the game of Hot and Cold. Yeah, that game kids play
    in which one kid, the hunter, is supposed to find a hidden object of interest,
    while the rest of the kids, who know where the object is, yell “cold” if the hunter
    is far from the object, and “hot” if the hunter is close to it. In this analogy,
    the location of the hunter is the parameters of the neural network. The hidden
    object of interest is the global optimum, which is either the lowest value of
    the loss function to minimize or the highest value of the objective function to
    maximize. The intent is to optimize the distance between the hunter and the object.
    For this, in the game, you use the kids yelling “cold” or “hot” to optimize the
    hunter’s position.Here’s when this analogy gets interesting. Imagine you have
    kids who, in addition to yelling “cold” or “hot,” get louder as the hunter gets
    closer to the object. You know, kids get excited too quickly and can’t keep a
    secret. As you hear them go from saying “cold” softly to getting louder every
    second, you know, as the hunter, you’re walking in the right direction. The distance
    can be minimized using that “gradient” information. The use of this information
    for getting to the object of interest is what gradient-based methods do. If the
    information comes in a continuous form, meaning the kids yell a couple of times
    per second, and get louder or softer and go from yelling “cold” to yelling “hot”
    with distance, then you can use the increase or decrease in the magnitude of the
    information, which is the gradient, to get to the object. Great!On the other hand,
    imagine the kids yelling the information are mean, or maybe just not perfect.
    Imagine they give discontinuous information. For instance, they may not be allowed
    to say anything while the hunter is in certain areas. They go from “softly cold”
    to nothing for a while to “softly cold” again. Or perhaps, imagine the object
    is right behind the middle of a long wall. Even if the hunter gets close to the
    object, the hunter won’t be able to reach the object with gradient information.
    The hunter would be close to the object, so the right thing to yell is “hot,”
    but in reality, the object is out of reach behind a wall. In all these cases,
    perhaps a gradient-based optimization approach isn’t the best strategy, and gradient-free
    methods, even if moving randomly, may be better for finding the object.A gradient-free
    method approach could be as simple as that. The hunter would pick a random place
    to go and ignore “gradient” information while getting there, then check with the
    yelling kids, and then try another random position. After getting an idea of a
    few random positions, say, 10, the hunter would take the top 3 and try random
    variations from those 3 that are apparently better locations. In this case, gradient
    information isn’t useful.Trust me, this analogy can keep going, but I’m going
    to stop it there. The bottom line is gradient-based and gradient-free methods
    are only strategies for reaching a point of interest. The effectiveness of these
    strategies depends on the problem at hand. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 要直观地比较基于梯度和无梯度方法的差异，想象一下“热与冷”这个游戏。是的，就是孩子们玩的那种游戏，其中一个孩子，猎人，需要找到隐藏的感兴趣物品，而其他知道物品位置的孩子，如果猎人离物品远，就喊“冷”，如果猎人靠近物品，就喊“热”。在这个类比中，猎人的位置是神经网络的参数。感兴趣的隐藏物品是全局最优解，要么是损失函数的最小值，要么是目标函数的最大值。目的是优化猎人和物品之间的距离。为此，在游戏中，你使用孩子们喊“冷”或“热”来优化猎人的位置。这里这个类比变得有趣起来。想象一下，有孩子们，除了喊“冷”或“热”，当猎人靠近物品时，他们会变得更响亮。你知道，孩子们很容易兴奋，而且不能保守秘密。当你听到他们从轻声说“冷”到每秒声音越来越大时，你知道，作为猎人，你正在朝着正确的方向前进。可以使用那个“梯度”信息来最小化距离。使用这些信息到达感兴趣物品的方法就是基于梯度的方法。如果信息以连续的形式出现，意味着孩子们每秒喊几次，声音变大或变小，从喊“冷”到喊“热”随着距离变化，那么你可以使用信息幅度的增加或减少，即梯度，来到达物品。太棒了！另一方面，想象一下，喊信息的孩子们很刻薄，或者可能只是不完美。想象一下，他们给出的是不连续的信息。例如，当猎人处于某些区域时，他们可能不允许说任何话。他们从“轻声冷”到一段时间什么也不说，然后再回到“轻声冷”。或者，也许想象一下，物品就在一堵长墙的中间。即使猎人靠近物品，猎人也无法用梯度信息触及物品。猎人会靠近物品，所以正确喊的应该是“热”，但现实中，物品在墙的另一边，无法触及。在所有这些情况下，可能基于梯度的优化方法并不是最好的策略，无梯度方法，即使移动是随机的，也可能更适合找到物品。无梯度方法的方法可能就像这样简单。猎人会随机选择一个地方去，在到达那里时忽略“梯度”信息，然后检查喊叫的孩子们，然后尝试另一个随机位置。在得到几个随机位置的概念后，比如10个，猎人会选取前3个，并尝试从这3个明显更好的位置中随机变化。在这种情况下，梯度信息是没有用的。相信我，这个类比可以继续下去，但我将在这里停止。底线是，基于梯度和无梯度方法只是到达感兴趣点的策略。这些策略的有效性取决于手头的问题。
    |'
- en: In addition to sample efficiency, another inherent advantage of using model-based
    methods is transferability. Learning a model of the dynamics of the world can
    help you achieve different related tasks. For instance, if you train an agent
    to control a robotic arm to reach an object, a model-based agent that learns how
    the environment reacts to the agent’s attempts to move toward the object might
    more easily learn to pick up that object in a later task. Notice that, in this
    case, learning a model of the reward function isn’t useful for transfer. However,
    learning how the environment reacts to its motion commands is transferable knowledge
    that can allow the accomplishment of other tasks. Last time I checked, the laws
    of physics hadn’t been updated for hundreds of years—talk about a slow-moving
    field!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本效率之外，使用基于模型的方法的另一个固有优势是可迁移性。学习世界动态的模型可以帮助你完成不同的相关任务。例如，如果你训练一个智能体来控制机械臂去抓取一个物体，一个学习环境如何对智能体的移动尝试做出反应的基于模型的智能体可能会更容易在后续任务中学会抓取那个物体。请注意，在这种情况下，学习奖励函数的模型对迁移并没有帮助。然而，学习环境对其运动命令的反应是可迁移的知识，这可以允许完成其他任务。上次我检查时，物理定律已经几百年来没有更新了——这真是一个缓慢发展的领域！
- en: A couple of other pluses worth mentioning follow. First, learning a model is
    often a supervised-learning task, which is much more stable and well-behaved than
    reinforcement learning. Second, if we have an accurate model of the environment,
    we can use theoretically grounded algorithms for planning, such as trajectory
    optimization, model-predictive control, or even heuristic search algorithms, such
    as Monte Carlo tree search. Last, by learning a model, we make better use of experiences
    overall because we extract the most information from the environment, which means
    more possibilities for better decisions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的还有几个优点。首先，学习模型通常是一个监督学习任务，这比强化学习更加稳定和表现良好。其次，如果我们有一个准确的环境模型，我们可以使用理论上有根据的规划算法，如轨迹优化、模型预测控制，甚至启发式搜索算法，如蒙特卡洛树搜索。最后，通过学习模型，我们总体上更好地利用经验，因为我们从环境中提取了最多的信息，这意味着有更多更好的决策可能性。
- en: But it isn’t all roses; model-based learning is also challenging. There are
    a few disadvantages to have in mind when using model-based methods. First, learning
    a model of the dynamics of an environment, in addition to a policy, a value function,
    or both, is more computationally expensive. And if you were to learn only a model
    of the dynamics, then the compounding of model error from the model would make
    your algorithm impractical.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非一切尽善尽美；基于模型的学习也有挑战。在使用基于模型的方法时，有几个缺点需要考虑。首先，除了策略、价值函数或两者之外，学习环境动态的模型在计算上更加昂贵。而且，如果你只学习动态模型，那么模型误差的累积会使你的算法变得不切实际。
- en: Not all aspects of the dynamics are directly beneficial to the policy. We covered
    this issue when arguing for learning a policy directly instead of learning a value
    function. Imagine a pouring task; if you need first to learn fluid dynamics, the
    viscosity of fluids, and fluid flow when you only want to pick up a cup and pour,
    then we’re overcomplicating the task. Trying to learn a model of the environment
    is more complicated than learning the policy directly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 动态的各个方面并不都是对策略有直接益处的。我们在主张直接学习策略而不是学习价值函数时已经讨论了这个问题。想象一下倒水任务；如果你需要首先学习流体动力学、流体的粘度和流体流动，而你只想拿起一个杯子倒水，那么我们就过于复杂化了任务。试图学习环境的模型比直接学习策略更复杂。
- en: It’s essential to recall that deep learning models are data hungry. As you know,
    to get the best out of a deep neural network, you need lots of data, and this
    is a challenge for model-based deep reinforcement learning methods. The problem
    compounds with the fact that it’s also hard to estimate model uncertainty in neural
    networks. And so, given that a neural network tries to generalize, regardless
    of model uncertainty, you can end up with long-term predictions that are total
    garbage.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 必须记住，深度学习模型是数据饥渴的。正如你所知，为了从深度神经网络中获得最佳效果，你需要大量的数据，这对基于模型的深度强化学习方法来说是一个挑战。这个问题还与在神经网络中很难估计模型不确定性的事实相叠加。因此，鉴于神经网络试图泛化，无论模型不确定性如何，你可能会得到长期预测结果完全是垃圾。
- en: This issue makes the argument that model-based methods are the most sample efficient
    questionable because you may end up needing more data to learn a useful model
    than the data you need for learning a good policy under model-free methods. However,
    if you have that model, or acquire the model independently of the task, then you
    can reuse that model for other tasks. Additionally, if you were to use “shallow”
    models, such as Gaussian processes, or Gaussian mixture models, then we’re back
    at square one, having model-based methods as the most sample efficient.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题提出了一个论点，即基于模型的方法是最具样本效率的，这是值得怀疑的，因为你可能最终需要比在无模型方法下学习良好策略所需的数据更多的数据来学习一个有用的模型。然而，如果你拥有那个模型，或者独立于任务获取该模型，那么你可以将该模型用于其他任务。此外，如果你要使用“浅层”模型，例如高斯过程或高斯混合模型，那么我们又回到了起点，基于模型的方法再次成为最具样本效率的方法。
- en: I’d like you to move from this section, knowing that it isn’t about model-based
    versus model-free. And even though you can combine model-based and model-free
    methods and get attractive solutions, at the end of the day, engineering isn’t
    about that either, the same way that it isn’t a matter of value-based versus policy-based,
    and it also isn't actor-critic. You don’t want to use a hammer when you need a
    screwdriver. My job is to describe what each type of algorithm is suitable for,
    but it’s up to you to use that knowledge the right way. Of course, explore, have
    fun, that matters, but, when it’s time to solve a problem, pick wisely.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你从这个部分开始，知道这并不是关于基于模型与无模型之争。尽管你可以结合基于模型和无模型的方法，得到有吸引力的解决方案，但最终，工程学并不是关于这个的，同样，这也不是基于价值与基于策略之争，也不是演员-评论家之争。当你需要螺丝刀时，不要用锤子。我的工作是描述每种算法适合什么，但如何正确使用这些知识取决于你。当然，探索、享受乐趣很重要，但在解决问题的时候，要明智地选择。
- en: Derivative-free optimization methods
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无导数优化方法
- en: Deep learning is the use of multi-layered function approximators to learn a
    function. A traditional deep learning use case goes as follows. First, we create
    a parametric model that mirrors a function of interest. Then, we define an objective
    function to know how wrong the model is at any given time. Next, we iteratively
    optimize the model by calculating where to move the parameters, using backpropagation.
    And finally, we update the parameters, using gradient descent.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是使用多层函数逼近器来学习函数。一个传统的深度学习用例如下。首先，我们创建一个参数模型，它反映了感兴趣的函数。然后，我们定义一个目标函数，以了解模型在任何给定时间有多错误。接下来，我们通过计算参数移动的方向来迭代优化模型，使用反向传播。最后，我们使用梯度下降更新参数。
- en: Backpropagation and gradient descent are practical algorithms for optimizing
    neural networks. These methods are valuable for finding the lowest or highest
    point of a function within a given range; for instance, a local optimum of the
    loss or objective function. But, interestingly, they aren’t the only way of optimizing
    parametric models, such as deep neural networks, and, more importantly, they aren’t
    always the most effective.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播和梯度下降是优化神经网络的实用算法。这些方法对于在给定范围内找到函数的最低点或最高点非常有价值；例如，损失函数或目标函数的局部最优。但有趣的是，它们并不是优化参数模型（如深度神经网络）的唯一方法，更重要的是，它们并不总是最有效的方法。
- en: Derivative-free optimization, such as genetic algorithms or evolution strategies,
    is a different model-optimization technique that has gotten attention from the
    deep reinforcement learning community in recent years. Derivative-free methods,
    which are also known as gradient-free, black-box, and zeroth-order methods, don’t
    require derivatives and can be useful in situations in which gradient-based optimization
    methods suffer. Gradient-based optimization methods suffer when optimizing discrete,
    discontinuous, or multi-model functions, for instance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 无导数优化，如遗传算法或进化策略，是一种近年来受到深度强化学习社区关注的不同的模型优化技术。无导数方法，也称为无梯度、黑盒和零阶方法，不需要导数，在基于梯度的优化方法受限制的情况下可能很有用。例如，在优化离散、不连续或多模型函数时，基于梯度的优化方法会遇到困难。
- en: Derivative-free methods can be useful and straightforward in many cases. Even
    randomly perturbing the weights of a neural network, if given enough compute,
    can get the job done. The main advantage of derivative-free methods is that they
    can optimize an arbitrary function. They don’t need gradients to work. Another
    advantage is that these methods are straightforward to parallelize. It’s not uncommon
    to hear hundreds or thousands of CPUs used with derivative-free methods. On the
    flip side, it’s good that they’re easy to parallelize because they’re sample inefficient.
    Being black-box optimization methods, they don’t exploit the structure of the
    reinforcement learning problem. They ignore the sequential nature of reinforcement
    learning problems, which can otherwise give valuable information to optimization
    methods.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无导数方法在许多情况下可能是有用且直接的。即使随机扰动神经网络的权重，如果计算资源足够，也能完成任务。无导数方法的主要优势是它们可以优化任意函数。它们不需要梯度来工作。另一个优势是这些方法易于并行化。使用无导数方法时，听到数百或数千个CPU并不罕见。另一方面，它们易于并行化是好事，因为它们是样本低效的。作为黑盒优化方法，它们没有利用强化学习问题的结构。它们忽略了强化学习问题的顺序性质，否则这些信息可以为优化方法提供有价值的信息。
- en: '![](../Images/13_09.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_09.png)'
- en: Derivative-free methods are an extreme case
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无导数方法是极端情况
- en: More advanced concepts toward AGI
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向AGI迈进的高级概念
- en: In the previous section, we reviewed the foundational concepts of deep reinforcement
    learning that are covered in this book and touched on the two essential types
    of methods that we didn’t cover in depth. But, as I mentioned before, there are
    still many advanced concepts that, even though not required for an introduction
    to deep reinforcement learning, are crucial for devising artificial general intelligence
    (AGI), which is the ultimate goal for most AI researchers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们回顾了本书涵盖的深度强化学习的基础概念，并简要提到了两种我们没有深入探讨的方法类型。但是，正如我之前提到的，仍然有许多高级概念，尽管它们对于深度强化学习的介绍不是必需的，但对于设计通用人工智能（AGI）至关重要，这是大多数AI研究者的最终目标。
- en: In this section, we start by going one step deeper into AGI and argue for some
    of the traits AI agents need to tackle tasks requiring more-general intelligence.
    I explain at a high level what these traits are and their intent so that you can
    continue your journey studying AI and perhaps one day contribute to one of these
    cutting-edge research fields.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先深入一步探讨AGI，并论证AI代理需要具备的一些特质以应对需要更广泛智能的任务。我以高层次解释了这些特质及其意图，以便你能够继续你的AI学习之旅，也许有一天能贡献于这些尖端研究领域。
- en: What is AGI, again?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用人工智能（AGI）是什么，再次？
- en: 'In this book, you’ve seen many examples of AI agents that seem impressive at
    first sight. The fact that the same computer program can learn to solve a wide
    variety of tasks is remarkable. Moreover, after you move on to more complex environments,
    it’s easy to get carried away by these results: AlphaZero learns to play chess,
    Go, and Shogi. OpenAI Five defeats human teams at the game of Dota2\. AlphaStar
    beats a top professional player at the game of StarCraft II. These are compelling
    general-purpose algorithms. But do these general-purpose algorithms show any sign
    of general intelligence? First of all, what is general intelligence?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你已经看到了许多AI代理的例子，它们一开始看起来令人印象深刻。同一个计算机程序能够学习解决各种各样的问题，这是非常了不起的。此外，当你转向更复杂的环境时，很容易被这些结果冲昏头脑：AlphaZero学会了下国际象棋、围棋和将棋。OpenAI
    Five在Dota2游戏中击败了人类团队。AlphaStar在星际争霸II游戏中击败了顶尖职业选手。这些都是令人信服的通用算法。但这些通用算法是否显示出任何通用智能的迹象？首先，什么是通用智能？
- en: 'General intelligence is the ability to combine various cognitive abilities
    to solve new problems. For artificial general intelligence (AGI), we then expect
    a computer program to show general intelligence. Okay. Now, let’s ask the following
    question: are any of the algorithms presented in this book, or even state-of-the-art
    methods such as AlphaZero, OpenAI Five, and AlphaStar, examples of artificial
    general intelligence? Well, it’s not clear, but I’d say no.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通用智能是指结合各种认知能力来解决新问题的能力。对于通用人工智能（AGI），我们期望计算机程序展现出通用智能。好的。现在，让我们提出以下问题：本书中提出的任何算法，或者甚至像AlphaZero、OpenAI
    Five和AlphaStar这样的最先进方法，是否是通用人工智能的例子？嗯，并不清楚，但我认为不是。
- en: You see, on the one hand, many of these algorithms can use “multiple cognitive
    abilities,” including perception and learning for solving a new task, say, playing
    Pong. If we stick to our definition, the fact that the algorithm uses multiple
    cognitive abilities to solve new problems is a plus. However, one of the most
    dissatisfying parts of these algorithms is that none of these trained agents are
    good at solving new problems unless you train them, which most of the time requires
    millions of samples before you get any impressive results. In other words, if
    you train a DQN agent to play Pong from pixels, that trained agent, which can
    be at superhuman level at Pong, has no clue about how to play a decent game of
    Breakout and has to train for millions of frames before it shows any skill.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，一方面，许多这些算法可以使用“多种认知能力”，包括感知和学习来解决新的任务，比如说，玩乒乓球。如果我们坚持我们的定义，算法使用多种认知能力来解决新问题是加分项。然而，这些算法中最令人不满意的部分之一是，除非你训练它们，否则这些训练过的代理在解决新问题上都不擅长，而这通常需要数百万个样本才能得到任何令人印象深刻的结果。换句话说，如果你训练一个DQN代理从像素中玩乒乓球，那么这个训练过的代理，在乒乓球上可以达到超人的水平，但对如何玩好《Breakout》一无所知，并且必须训练数百万帧才能显示出任何技能。
- en: Humans don’t have this problem. If you learn to play Pong, I’m pretty sure you
    can pick up Breakout in like two seconds. Both games have the same task of hitting
    a ball with a paddle. On the other hand, even the AlphaZero agent, a computer
    program with the most impressive skills of all time at multiple fundamentally
    different board games, and that can beat professional players who dedicate their
    lives at these games, but will never do your laundry.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 人类没有这个问题。如果你学会了玩乒乓球，我相当确信你可以在两秒钟内学会玩《Breakout》。两款游戏都有相同的任务，那就是用球拍击球。另一方面，即使是AlphaZero代理，这个在多个根本不同的棋类游戏中拥有史上最令人印象深刻技能的计算机程序，并且能够打败那些致力于这些游戏的职业玩家，但它永远不会帮你洗衣服。
- en: Certain AI researchers say their goal is to create AI systems that perceive,
    learn, think, and even feel emotions like humans do. Machines that learn, think,
    feel, and perhaps even look like people, are most definitely an exciting thought.
    Other researchers have a more practical approach; they don’t necessarily want
    an AI that thinks like humans unless thinking like a human is a requirement for
    making a good lunch. And perhaps emotions are what make a great cook, who knows.
    The point is that while some folks want AGI to delegate, to stop doing mundane
    tasks, other folks have a more philosophical goal. Creating AGI could be a path
    to understanding intelligence itself, to understanding the self, and that on its
    own would be a remarkable accomplishment for humanity.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 某些AI研究人员表示，他们的目标是创建能够感知、学习、思考和甚至像人类一样感受情绪的AI系统。能够学习、思考、感受，甚至可能看起来像人的机器，无疑是一个令人兴奋的想法。其他研究人员则采取更实际的方法；他们并不一定想要一个像人类一样思考的AI，除非像人类一样思考是做出一顿好午餐的必要条件。也许情感是造就一位伟大厨师的因素，谁知道呢。关键是，尽管有些人希望AGI（通用人工智能）可以委托，停止做日常任务，但其他人则有一个更哲学的目标。创建AGI可能是理解智能本身、理解自我的一条途径，而这本身就是人类的一项非凡成就。
- en: Either way, every AI researcher would agree that, regardless of the end goal,
    we still need AI algorithms that display more general and transferable skills.
    There are many traits that AI systems likely require before they can do more human-like
    tasks, such as doing the laundry, cooking lunch, or washing the dishes. Interestingly,
    it’s those mundane tasks that are the most difficult to solve for AI. Let’s review
    several of the research areas that are currently pushing the frontier on making
    deep reinforcement learning and artificial intelligence show signs of general
    intelligence.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，每个AI研究人员都会同意，无论最终目标是什么，我们仍然需要展示更多通用和可迁移技能的AI算法。在AI系统能够执行更类似人类任务的许多特质中，有许多特质是AI系统可能需要的，例如洗衣服、做午餐或洗碗。有趣的是，这些日常任务对于AI来说是最难解决的。让我们回顾一下目前正在推动深度强化学习和人工智能展示通用智能迹象的几个研究领域。
- en: The following sections introduce several of the concepts that you may want to
    explore further as you keep learning advanced deep reinforcement learning techniques
    that get the field of AI closer to human-level intelligence. I spend only a few
    sentences so that you’re aware of as many as possible. I intend to show you the
    door, not what’s inside it. It’s for you to decide which door to open.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节介绍了几个你可能希望在深入学习将人工智能领域推向人类水平智能的先进深度强化学习技术时进一步探索的概念。我只用了几句话，以便让你尽可能多地了解它们。我的目的是指给你门路，而不是门里面的东西。决定打开哪扇门，由你自己来定。
- en: '![](../Images/13_10.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_10.png)'
- en: Workforce revolutions
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 劳动力革命
- en: Advanced exploration strategies
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级探索策略
- en: One area of research that’s showing exciting results has to do with the reward
    function. Throughout this book, you’ve seen agents that learn from the reward
    signal, but interestingly, there’s recent research that shows agents can learn
    without any reward at all. Learning from things other than rewards is an exciting
    thought, and it may be essential for developing human-like intelligence. If you
    observe a baby learn, there’s much unsupervised and self-supervised learning going
    on. Sure, at one point in their lives, we reward our children. You know you get
    an A, you get B; your salary is x, yours is y. But agents aren’t always after
    the rewards we put along their way. What is the reward function of life? Is it
    career success? Is it to have children? It’s not clear.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 研究领域中的一个令人兴奋的成果与奖励函数有关。在这本书中，你已经看到了从奖励信号中学习的智能体，但有趣的是，最近的研究表明智能体可以在没有任何奖励的情况下学习。从除了奖励之外的事物中学习是一个令人兴奋的想法，这可能是开发类似人类智能的必要条件。如果你观察一个婴儿学习，会发现有很多无监督和自监督学习在进行。当然，在他们生命中的某个时刻，我们会奖励我们的孩子。你知道你得到A，你得到B；你的薪水是x，你的薪水是y。但智能体并不总是追求我们为他们设定的奖励。生活的奖励函数是什么？是职业成功吗？是有孩子吗？这并不清楚。
- en: Now, removing the reward function from the reinforcement learning problem can
    be a bit scary. If we’re not defining the reward function for the agent to maximize,
    how do we make sure their goals align with ours? How do we make artificial general
    intelligence that’s suitable for the goals of humankind? Maybe it’s the case that,
    to create human-like intelligence, we need to give agents the freedom to choose
    their destiny. Either way, to me, this is one of the critical research areas to
    pursue.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从强化学习问题中移除奖励函数可能会有些可怕。如果我们没有为智能体定义要最大化的奖励函数，我们如何确保他们的目标与我们的目标一致？我们如何创造适合人类目标的人工通用智能？也许，为了创造类似人类的智能，我们需要给智能体选择命运的自由。无论如何，对我来说，这是需要追求的关键研究领域之一。
- en: Inverse reinforcement learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向强化学习
- en: There are other ways to learn behavior without a reward function, and even though
    we often prefer a reward function, learning to imitate a human first can help
    learn policies with fewer samples. There are a few related fields to look for
    here. *behavioral cloning* is the application of supervised learning techniques
    to learn a policy from demonstrations, often from a human. As the name suggests,
    there’s no reasoning going on here, merely generalization. A related field, called
    *inverse reinforcement learning*, consists of inferring the reward function from
    demonstrations. In this case, we’re not merely copying the behavior, but we’re
    learning the intentions of another agent. *inferring intentions* can be a powerful
    tool for multiple goals. For instance, in multi-agent reinforcement learning,
    for both adversarial and cooperative settings, knowing what other agents are after
    can be useful information. If we know what an agent wants to do, and what it wants
    to do goes against our goals, we can devise strategies for stopping it before
    it’s too late.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 没有奖励函数也可以学习行为，尽管我们通常更喜欢奖励函数，但首先学习模仿人类可以帮助用更少的样本学习策略。这里有几个相关的领域可以寻找。*行为克隆*是将监督学习技术应用于从演示中学习策略，通常是从人类那里学习。正如其名所示，这里没有推理发生，只是泛化。一个相关的领域，称为*反向强化学习*，包括从演示中推断奖励函数。在这种情况下，我们不仅仅是复制行为，而是在学习另一个智能体的意图。*推断意图*可以是实现多个目标的有力工具。例如，在多智能体强化学习中，对于对抗和协作设置，了解其他智能体的目标可以是有用的信息。如果我们知道一个智能体想要做什么，以及它想要做的事情是否与我们的目标相悖，我们可以在为时已晚之前制定阻止它的策略。
- en: But, inverse reinforcement learning allows agents to learn new policies. Learning
    the reward function from another agent, such as a human, and learning a policy
    from this learned reward function is a technique often referred to as *apprenticeship
    learning*. One interesting point to consider when learning about inverse reinforcement
    learning is that the reward function is often more succinct than the optimal policy.
    Attempting to learn the reward function can make sense in multiple cases. Techniques
    that learn policies from demonstrations are also called *imitation learning*,
    often whether a reward function is inferred before the policy or straight behavioral
    cloning. A frequent use case for imitation learning is the initialization of agents
    to a good enough policy. For instance, if an agent has to learn from random behavior,
    it could take a long time before it learns a good policy. The idea is that imitating
    a human, even if suboptimal, may lead to optimal policies with fewer interactions
    with the environment. However, this isn’t always the case, and policies pretrained
    with demonstrations by humans may introduce unwanted bias and prevent agents from
    finding optimal policies.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，逆强化学习允许智能体学习新的策略。从另一个智能体（如人类）那里学习奖励函数，并从这个学到的奖励函数中学习策略，这种技术通常被称为*学徒学习*。在学习逆强化学习时，一个值得考虑的有趣点是，奖励函数通常比最优策略更简洁。尝试学习奖励函数在多种情况下是有意义的。从演示中学习策略的技术也被称为*模仿学习*，通常是在学习策略之前推断奖励函数还是直接进行行为克隆。模仿学习的常见用例是将智能体初始化到足够好的策略。例如，如果一个智能体必须从随机行为中学习，那么它可能需要很长时间才能学习到一个好的策略。想法是模仿人类，即使不是最优的，也可能通过与环境更少的交互导致最优策略。然而，这并不总是情况，由人类演示预训练的策略可能会引入不希望的偏差，并阻止智能体找到最优策略。
- en: Transfer learning
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: You probably notice that the agent trained on an environment, in general, cannot
    be transferred to new environments. Reinforcement learning algorithms are general
    purpose in the sense that the same agent can be trained in different environments,
    but they don’t have general intelligence, and what they learn cannot be straightforwardly
    transferred to new environments.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在某个环境中训练的智能体通常不能转移到新的环境中。强化学习算法是通用目的的，意味着同一个智能体可以在不同的环境中进行训练，但它们没有通用智能，它们学到的知识不能直接转移到新的环境中。
- en: Transfer learning is an area of research that looks at ways of transferring
    knowledge from a set of environments to a new environment. One approach, for instance,
    that may be intuitive to you if you have a deep learning background, is what’s
    called *fine-tuning*. Similar to reusing the weights of a pretrained network in
    supervised learning, agents trained in related environments can reuse the features
    learned by the convolution layers on a different task. If the environments are
    related, such as Atari games, for instance, several of the features may be transferable.
    In certain environments, even policies can be transferred.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一个研究领域，它探讨将知识从一组环境转移到新环境的方法。例如，如果你有深度学习背景，可能会觉得以下方法直观：所谓的*微调*。类似于在监督学习中重用预训练网络的权重，在相关环境中训练的智能体可以重用在不同任务上卷积层学到的特征。如果环境相关，例如在Atari游戏中，那么一些特征可能是可迁移的。在某些环境中，甚至策略也可以迁移。
- en: '![](../Images/13_11.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_11.png)'
- en: Sim-to-real transfer learning task is a common need in the real world
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实际世界中的仿真到现实迁移学习任务是一个常见需求
- en: The general area of research on making agents learn more general skills is called
    *transfer learning*. Another frequent use of transfer learning is to transfer
    policies learned in simulation to the real world. Sim-to-real transfer learning
    is a common need in robotics, in which training agents controlling robots can
    be tricky, costly, and dangerous. It’s also not as scalable as training in simulation.
    A common need is to train an agent in simulation and then transfer the policy
    to the real world. A common misconception is that simulations need to be high-fidelity
    and realistic for transferring agents from simulation to the real world. There’s
    research suggesting that it’s the opposite. It’s the variety, the diversity of
    observations, that makes agents more transferable. Techniques such as domain randomization
    are at the forefront of this research area and show much promise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 研究使智能体学习更通用技能的一般领域被称为*迁移学习*。迁移学习的另一常见用途是将在模拟中学习到的策略转移到现实世界。从模拟到现实的迁移学习在机器人领域是一个常见需求，其中训练控制机器人的智能体可能很棘手、成本高昂且危险。它也不像在模拟中的训练那样可扩展。一个常见需求是在模拟中训练智能体，然后将策略转移到现实世界。一个常见的误解是，为了将智能体从模拟转移到现实世界，模拟需要具有高保真度和现实性。有研究表明情况正好相反。是观察的多样性、多样性使得智能体更具迁移性。如领域随机化等技术处于这一研究领域的最前沿，并显示出巨大的潜力。
- en: Multi-task learning
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多任务学习
- en: A related area of research, called *multi-task learning*, looks at transfer
    learning from a different perspective. In multi-task learning, the goal is to
    train on multiple tasks, instead of one, and then transfer to a new task. In this
    case, model-based reinforcement learning approaches come to mind. In robotics,
    for instance, learning a variety of tasks with the same robot can help the agent
    learn a robust model of the dynamics of the environment. The agent learns about
    gravity, how to move toward the right, or left, and so on. Regardless of the tasks,
    the model of the dynamics learned can be transferred to a new task.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个被称为*多任务学习*的研究领域，从不同的角度研究迁移学习。在多任务学习中，目标是训练多个任务，而不是一个，然后将它们转移到新任务。在这种情况下，基于模型的强化学习方法会浮现在脑海中。例如，在机器人领域，使用同一台机器人学习各种任务可以帮助智能体学习环境的动态的鲁棒模型。智能体了解重力、如何向右或向左移动等等。无论任务如何，学习的动态模型都可以转移到新任务。
- en: '![](../Images/13_12.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13_12.png)'
- en: Multi-task learning consists of training on multiple related tasks and testing
    on a new one
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习包括在多个相关任务上训练并在新任务上测试
- en: Curriculum learning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 课程学习
- en: A common use-case scenario for multi-task learning is decomposing a task into
    multiple tasks sorted by difficulty level. In this case, the agent is put through
    a curriculum, learning more complicated tasks progressively. *curriculum learning*
    makes sense and can be useful when developing scenarios. If you need to create
    an environment for an agent to solve, it often makes sense for you to create the
    most straightforward scenario with a dense reward function. By doing this, your
    agent can quickly show progress toward learning the goal, and this validates that
    your environment is working well. Then, you can increase the complexity and make
    the reward function more sparse. After you do this for a handful of scenarios,
    you naturally create a curriculum that can be used by your agent. Then, you can
    train your agent in progressively more complex environments, and hopefully, have
    an agent reach the desired behaviors more quickly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习的常见用例场景是将任务分解为按难度级别排序的多个任务。在这种情况下，智能体会经历一个课程，逐步学习更复杂的任务。*课程学习*在开发场景时是有意义的，并且可能很有用。如果你需要为智能体创建一个解决问题的环境，那么创建一个最直接的场景并具有密集的奖励函数通常是有意义的。通过这样做，你的智能体可以快速展示学习目标方面的进步，这验证了你的环境运行良好。然后，你可以增加复杂性并使奖励函数更稀疏。在你为几个场景这样做之后，你自然会创建一个智能体可以使用的课程。然后，你可以在越来越复杂的环境中训练你的智能体，并希望智能体能够更快地达到期望的行为。
- en: Meta learning
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元学习
- en: Another super exciting research area is called *meta learning*. If you think
    about it, we’re hand-coding agents to learn many different tasks. At one point,
    we become the bottleneck. If we could develop an agent that, instead of learning
    to solve a challenging task, learns to learn itself, we could remove humans from
    the equation; well, not quite, but take a step in that direction. Learning to
    learn is an exciting approach to using experiences from learning multiple tasks
    to get good at learning itself. It makes intuitive sense. Other exciting research
    paths coming out of meta learning are automatically discovering neural network
    architectures and optimization methods. Keep an eye out for these.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个超级激动人心的研究领域被称为*元学习*。如果您这么想，我们正在手动编码智能体来学习许多不同的任务。在某个时刻，我们成为了瓶颈。如果我们能够开发出一个智能体，它不是学习解决具有挑战性的任务，而是学习如何学习自己，我们就可以从方程式中移除人类；好吧，不是完全移除，但朝着那个方向迈出一步。学习如何学习是使用从学习多个任务中获得的经验来提高自身学习能力的激动人心的方法。这从直觉上是有意义的。从元学习中出现的一些其他激动人心的研究路径是自动发现神经网络架构和优化方法。请密切关注这些。
- en: Hierarchical reinforcement learning
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次化强化学习
- en: Often, we find ourselves developing environments that have problems with multiple
    horizons. For instance, if we want an agent to find the best high-level strategy,
    but give it only low-level control commands for actions, then the agent needs
    to learn to go from low-level to high-level action space. Intuitively, there’s
    a hierarchy in the policies for most agents. When I plan, I do so on a higher-level
    action space. I think about going to the store, not moving my arms to get to the
    store. *Hierarchical reinforcement learning* enables agents to create a hierarchy
    of actions internally to tackle long-horizon problems. Agents no longer reason
    about left-right commands, but more about go here or there.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们发现自己在开发具有多个视野问题的环境。例如，如果我们想让智能体找到最佳的高级策略，但只给它低级控制命令来执行动作，那么智能体需要学会从低级动作空间到高级动作空间的转换。直观地说，大多数智能体的策略中存在层次结构。当我规划时，我是在一个更高级的动作空间中进行的。我想的是去商店，而不是移动我的手臂去商店。*层次化强化学习*使智能体能够内部创建一个动作层次结构来处理长期问题。智能体不再推理左右命令，而是更多地考虑去这里或那里。
- en: Multi-agent reinforcement learning
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多智能体强化学习
- en: The world wouldn’t be as exciting without other agents. In multi-agent reinforcement
    learning, we look at techniques for having agents learn when there are multiple
    agents around. One of the main issues that arise, when learning in multi-agent
    settings, is that as your agent learns, other agents learn too, and therefore
    change their behavior. The problem is that this change makes the observations
    non-stationary, because what your agent learns is outdated right after the other
    agents learn, and so learning becomes challenging.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他智能体，世界就不会那么有趣。在多智能体强化学习中，我们研究智能体在周围有多个智能体时如何学习的技术。在多智能体环境中学习时出现的一个主要问题是，随着您的智能体学习，其他智能体也在学习，因此改变它们的行为。问题是这种变化使得观察变得非平稳，因为您的智能体学习的内容在其他智能体学习后立即过时，因此学习变得具有挑战性。
- en: One exciting approach to cooperative *multi-agent reinforcement learning* is
    to use actor-critic methods in which the critic uses the full state information
    of all agents during training. The advantage here is that your agents learn to
    cooperate through the critic, and we can then use the policy during testing using
    a more realistic observation space. Sharing the full state may seem unrealistic,
    but you can think about it as similar to how teams practice. During practice,
    everything is allowed. Say that you’re a soccer player, and you can tell other
    agents you intend to run on the wing when you make this move, and so on. You get
    to practice moves with full information during training; then, you can only use
    your policy with limited information during testing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一种令人兴奋的协作多智能体强化学习方法是在训练过程中让评论家使用所有智能体的完整状态信息。这里的优势是您的智能体通过评论家学会合作，然后我们可以在测试期间使用更真实的观察空间来应用策略。共享完整状态可能看起来不切实际，但您可以将其视为类似于团队练习的方式。在练习期间，一切都被允许。比如说，您是一名足球运动员，当您做出这个动作时，您可以告诉其他智能体您打算在边路跑动，等等。在训练期间，您可以得到关于动作的完整信息进行练习；然后，在测试期间，您只能使用有限信息的策略。
- en: Another appealing thought when looking into multi-agent reinforcement learning
    is that hierarchical reinforcement learning can be thought of as another case
    of multi-agent reinforcement learning. How so? Think about multiple agents deciding
    on different horizons. The multiple-horizon structure is similar to the way most
    companies do business. Folks at the top plan higher-level goals for the next few
    years and other folks decide on how to get there on a month-to-month and a day-to-day
    basis. The ones at the top set the goals for those on the bottom. The whole system
    gets rewarded for the performance of all agents.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究多智能体强化学习时，另一个吸引人的想法是分层强化学习可以被视为多智能体强化学习的另一种案例。如何做到这一点呢？想想多个智能体在不同时间范围内做出决定。多时间结构类似于大多数公司做生意的方式。顶层的人为未来几年设定更高层次的目标，其他人则决定如何按月和按日实现目标。顶层的人为底层的人设定目标。整个系统因所有智能体的表现而获得奖励。
- en: 'Of course, multi-agent reinforcement learning isn’t only for the cooperative
    case but also for the adversarial case, which is perhaps the most exciting. Humans
    often see competition and adversaries as something inherently unfortunate, but
    multi-agent reinforcement learning suggests that our adversaries often are the
    best way to make ourselves better. Underlying many recent reinforcement learning
    success stories are training techniques that include adversaries: either a previous
    version of the same agent, such as in self-play, to a whole tournament-like distribution
    of other agents that form after all the matches—only the best agents survive.
    Adversaries often make us better, and for better or worse, they might be needed
    for optimal behavior.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，多智能体强化学习不仅适用于合作案例，也适用于对抗案例，这可能是最令人兴奋的。人类通常将竞争和对手视为本质上不幸的事情，但多智能体强化学习表明，我们的对手往往是让我们变得更好的最佳方式。许多最近的强化学习成功故事背后的训练技术包括对手：要么是同一智能体的先前版本，例如在自我对战中，要么是在所有比赛结束后形成的整个锦标赛式的其他智能体分布——只有最好的智能体才能生存。对手往往使我们变得更好，不管好坏，它们可能对于最佳行为是必需的。
- en: Explainable AI, safety, fairness, and ethical standards
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释人工智能、安全性、公平性和伦理标准
- en: There are a few other critical areas of research that, even though not directly
    a push for human-level intelligence, are fundamental for the successful development,
    deployment, and adoption of artificial intelligence solutions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 研究中还有一些其他的关键领域，尽管它们并非直接推动人类水平智能的发展，但对于人工智能解决方案的成功开发、部署和采用是基础性的。
- en: '*explainable artificial intelligence* is an area of research that tries to
    create agents that are more easily understandable by humans. The motives are apparent.
    A court of law can interrogate any person that breaks the law; however, machine
    learning models aren’t designed to be explainable. To ensure the fast adoption
    of AI solutions by society, researchers must investigate ways to ease the problem
    of explainability. To be clear, I don’t think this is a requirement. I prefer
    having an AI give me an accurate prediction in the stock market, whether it can
    explain to me why or not. However, neither decision is straightforward. In life-or-death
    decisions involving humans, things become hairy quickly.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*可解释人工智能* 是一个研究领域，试图创造更易于人类理解的智能体。动机是明显的。法律法庭可以审问任何违法的人；然而，机器学习模型并不是为了可解释性而设计的。为了确保人工智能解决方案能够快速被社会接受，研究人员必须研究减轻可解释性问题的方法。为了明确起见，我认为这不是一个必要条件。我更喜欢让人工智能给我提供准确的股市预测，无论它能否向我解释原因。然而，这两个决定都不是直截了当的。在涉及人类生死存亡的决定中，事情会迅速变得复杂。'
- en: Safety is another area of research that should get more attention. It’s often
    the case that AIs fail catastrophically in ways that are too obvious to humans.
    Also, AIs are vulnerable to attacks that humans aren’t. We need to make sure that
    when AIs are deployed, we know how the systems react to a variety of situations.
    AIs currently don’t have a way to go through classical validation and verification
    (V&V) of software approaches, and this poses a significant challenge for the adoption
    of AI.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性是另一个应得到更多关注的研究领域。通常情况下，人工智能以对人类来说过于明显的方式失败。此外，人工智能容易受到人类无法抵御的攻击。我们需要确保当人工智能被部署时，我们知道系统如何对各种情况进行反应。目前人工智能还没有通过经典验证和验证（V&V）软件方法的方式，这对人工智能的采用构成了重大挑战。
- en: Fairness is another crucial issue. We need to start thinking about who controls
    AIs. If a company creates an AI to maximize profits at the expense of society,
    then what’s the point of AI technologies? We already have something similar going
    on with advertising. Top companies use AI to maximize gains through a form of
    manipulation. Should these companies be allowed to do this for profit? How about
    when AIs get better and better? What’s the purpose of this, destroy a human through
    manipulation? These are things that need to be seriously considered.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性是另一个关键问题。我们需要开始思考谁控制着人工智能。如果一家公司为了社会利益最大化利润而创建人工智能，那么人工智能技术的意义何在？我们已经在广告中看到了类似的情况。顶级公司使用人工智能通过一种操纵方式来最大化收益。这些公司是否应该被允许为了利润而这样做？当人工智能越来越好时呢？这样做的目的是什么，通过操纵来摧毁人类？这些都是需要认真考虑的事情。
- en: Finally, AI ethical standards are another issue that has gotten recent attention
    with the Montreal Declaration for Responsible Development of Artificial Intelligence.
    These are 10 ethical principles for AI that serve the interests of society, and
    not merely for-profit companies. These are several of the top fields to have in
    mind when you’re ready to contribute.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，人工智能伦理标准也是近期受到关注的另一个问题，这得益于蒙特利尔人工智能负责任发展宣言。这些是针对人工智能的10项伦理原则，旨在服务于社会的利益，而不仅仅是盈利公司。当你准备贡献时，这些是几个需要考虑的顶级领域。
- en: What happens next?
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来会发生什么？
- en: While this section marks the end of this book, it should only mark the beginning
    or continuation of your contributions to the field of AI and DRL. My intention
    with this book was, not only to get you understanding the basics of DRL, but also
    to onboard you into this fantastic community. You don’t need much other than a
    commitment to continue the journey. There are many things you could do next, and
    in this section, I’d like to give you ideas to get you started. Have in mind that
    the world is a choir needing a wide variety of voice types and talents; your job
    is to accept the talents given to you, develop them to the best of your abilities,
    and play your part with all you've got. While I can give you ideas, it’s up to
    you what happens next; the world needs and awaits your voice.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这一部分标志着这本书的结束，但它应该只是你为人工智能和DRL领域做出贡献的开始或延续。我写这本书的意图不仅是让你理解DRL的基础知识，而且是要让你加入这个精彩的社区。你需要的不仅仅是继续旅程的承诺。你可以做很多事情，在这一部分，我想给你一些想法，帮助你开始。记住，世界是一个需要各种声音类型和才能的合唱团；你的工作是接受赋予你的才能，尽你所能发展它们，并全力以赴地扮演你的角色。虽然我可以给你提供想法，但接下来发生什么取决于你；世界需要并等待着你的声音。
- en: How to use DRL to solve custom problems
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用深度强化学习（DRL）解决定制问题
- en: There’s something super cool about RL algorithms that I want you to have in
    mind as you learn about other types of agents. The fact is that most RL agents
    can solve any problem that you choose, as long as you can represent the problem
    as a correct MDP, the way we discussed in chapter 2\. When you ask yourself, “What
    can X or Y algorithm solve?” the answer is the same problems other algorithms
    can solve. While in this book, we concentrate on a handful of algorithms, all
    the agents presented can solve many other environments with some hyperparameter
    tuning. The need for solving custom environments is something many people want
    but could take a whole other book to get right. My recommendation is to look at
    some of the examples available online. For instance, Atari environments use an
    emulator called Stella in the backend. The environments pass images for observations
    and actions back and forth between the environment and the emulator. Likewise,
    MuJoCo and the Bullet Physics simulation engine are the backends that drive continuous-control
    environments. Take a look at the way these environments work.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）算法有一些非常酷的特点，我想在你了解其他类型的智能体时让你记住。事实是，大多数RL智能体可以解决你选择的任何问题，只要你能将问题表示为一个正确的MDP（马尔可夫决策过程），就像我们在第2章讨论的那样。当你问自己，“X或Y算法能解决什么问题？”时，答案是其他算法能解决的问题。虽然在这本书中，我们专注于一些算法，但所有展示的智能体都可以通过一些超参数调整来解决许多其他环境。解决定制环境的需求是许多人想要解决的问题，但可能需要另一本书来正确处理。我的建议是查看一些在线可用的示例。例如，Atari环境在后台使用名为Stella的模拟器。环境通过模拟器与环境之间传递图像进行观察和行动。同样，MuJoCo和Bullet物理仿真引擎是驱动连续控制环境的后端。看看这些环境是如何工作的。
- en: Pay attention to how observations are passed from the simulation to the environment
    and then to the agent. Then, the actions selected by the agent are passed to the
    environment and then to the simulation engine. This pattern is widespread, so
    if you want to create a custom environment, investigate how others have done it,
    and then do it yourself. Do you want to create an environment for an agent to
    learn to invest in the stock market? Think about which platforms have an API that
    allows you to do that. Then, you can create different environments using the same
    API. One environment, for instance, can buy stocks, another buys options, and
    so on. There are so many potential applications for state-of-the-art deep reinforcement
    learning methods that it’s a shame we have a limited number of quality environments
    at our disposal. Contributions in this area are undoubtedly welcome. If you want
    to create an environment and don’t find it out there, consider investing the time
    to create your own and share it with the world.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察是如何从模拟传递到环境，然后再传递到智能体的。然后，智能体选择的动作传递到环境中，然后再传递到仿真引擎。这种模式很普遍，所以如果你想创建一个自定义环境，调查其他人是如何做到的，然后自己动手做。你想要为智能体创建一个学习在股市中投资的环境吗？考虑一下哪些平台有API允许你这样做。然后，你可以使用相同的API创建不同的环境。例如，一个环境可以购买股票，另一个可以购买期权，等等。最先进的深度强化学习方法有如此多的潜在应用，而我们可用的优质环境数量有限，这真是遗憾。这个领域的贡献无疑是受欢迎的。如果你想要创建一个环境，但没有找到，考虑投入时间创建自己的，并与世界分享。
- en: Going forward
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 展望未来
- en: You have learned a lot; there’s no doubt about that. But, if you look at the
    big picture, there’s so much more to learn. Now, if you zoom out even further,
    you realize that there’s even more to discover; things nobody has learned before.
    You see that what the AI community is after is no easy feat; we’re trying to understand
    how the mind works.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学到了很多，这一点毫无疑问。但是，如果你从大局出发，你会发现还有更多东西需要学习。现在，如果你进一步放大视角，你会意识到还有更多未知的领域；是之前没有人学习过的东西。你会发现，人工智能社区追求的目标并不容易；我们正在试图理解大脑是如何工作的。
- en: The fact is, even other fields, such as Psychology, Philosophy, Economics, Linguistic,
    Operations Research, Control Theory, and many more, are after the same goal, each
    from their perspective, and using their language. But the bottom line is that
    all of these fields would benefit from understanding how the mind works, how humans
    make decisions, and how to help them make optimal decisions. Here are some ideas
    for us moving forward.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是其他领域，比如心理学、哲学、经济学、语言学、运筹学、控制理论等等，它们的目标都是相同的，每个领域都从自己的角度出发，使用自己的语言。但归根结底，所有这些领域都会从理解大脑如何工作、人类如何做决策以及如何帮助他们做出最佳决策中受益。以下是我们前进的一些想法。
- en: First, find your motivation, your ambition, and focus. Certain people find the
    sole desire to explore; to discover facts about the mind is exciting. Others want
    to leave a better world behind. Whatever your motivation, find it. Find your drive.
    If you aren’t used to reading research papers, you won’t enjoy them unless you
    know your motives. As you find your motivation and drive, you must remain calm,
    humble, and transparent; you need your drive to focus and work hard toward your
    goals. Don’t let your excitement get in your way. You must learn to keep your
    motivation in your heart, yet move forward. Our ability to focus is in constant
    jeopardy from a myriad of readily available distractions. I’m guaranteed to find
    new notifications on my cell phone every 15 minutes. And we’re trained to think
    that this is a good thing. It isn’t. We must get back in control of our lives
    and be able to concentrate long and hard on something that interests us, that
    we love. Practice focus.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，找到你的动力、你的抱负和专注力。有些人唯一的愿望就是探索；去发现关于大脑的事实是令人兴奋的。其他人则希望留下一个更美好的世界。无论你的动力是什么，找到它。找到你的驱动力。如果你不习惯阅读研究论文，除非你知道你的动机，否则你不会喜欢它们。当你找到你的动力和驱动力时，你必须保持冷静、谦逊和透明；你需要你的驱动力来集中精力，努力实现你的目标。不要让你的兴奋阻碍你。你必须学会将你的动力放在心中，同时继续前进。我们能够集中精力的能力经常受到无数现成的干扰的威胁。我保证每15分钟我的手机上都会有新的通知。而我们被训练去认为这是好事。这并不是。我们必须重新控制我们的生活，能够长时间、专注地投入到我们感兴趣、我们热爱的事物中。练习专注力。
- en: Second, balance learning and contributions and give yourself time to rest. What
    do you think would happen if, for the next 30 days, I eat 5,000 calories a day
    and burn 1,000? What if I, instead, eat 1,000 and burn 5,000? How about I’m an
    athlete and eat and burn 5,000, but train every day of the week? Right, all of
    those spell trouble to the body. The same happens with the mind; certain people
    think they need to learn for years before they can do anything, so they read,
    watch videos, but don’t do anything with it. Others think they no longer need
    to read any papers; after all, they’ve already implemented a DQN agent and written
    a blog post about it. They quickly become obsolete and lack the fuel to think.
    Some people get those two right, but never include the time to relax, and enjoy
    their family and reflect. That’s the wrong approach. Find a way to balance what
    you take and what you give, and set time to rest. We’re athletes of the mind,
    too much “fat” in your mind, too much information that you put in without purpose,
    and you become sluggish and too slow. Too many blog posts without doing any research,
    and you become outdated, repetitive, and dry. Not enough rest, and you won’t plan
    well enough for the long term.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，平衡学习和贡献，并给自己留出休息的时间。你认为如果接下来30天，我每天摄入5000卡路里，消耗1000卡路里，会发生什么？如果相反，我每天摄入1000卡路里，消耗5000卡路里呢？如果我是一个运动员，每天消耗5000卡路里，但每周都进行训练呢？没错，所有这些对身体的危害都是显而易见的。同样，对心智来说也是如此；有些人认为他们需要学习多年才能有所作为，所以他们阅读、观看视频，但并不付诸实践。另一些人认为他们不再需要阅读任何论文；毕竟，他们已经实现了一个DQN智能体，并撰写了一篇关于它的博客文章。他们很快就会变得过时，缺乏思考的动力。有些人做对了这两件事，但从未包括放松、享受家庭和反思的时间。这是错误的方法。找到一种平衡你所得和所给予的方法，并留出休息的时间。我们也是心智的运动员，心智中过多的“脂肪”——没有目的性地输入过多信息，会让你变得迟钝和缓慢。写太多没有进行研究的博客文章，会让你变得过时、重复和枯燥。休息不足，你将无法为长期规划做出充分的准备。
- en: Also, know that you can’t learn everything. Again, we’re learning about the
    mind, and there’s much information about it out there. Be wise, and be picky about
    what you read. Who’s the author? What’s her background? Still, you can read it
    but have a higher sense of what you’re doing. Try to give often. You should be
    able to explain things that you learn another way. The quote, “Don’t reinvent
    the wheel,” is misleading at best. It would be best if you tried things on your
    own; that’s vital. It’s inevitable that if you explore, early on, you may find
    yourself having a great idea, that you later realize has been worked on before.
    There’s no shame in that; it’s more critical for you to keep moving forward than
    to keep yourself waiting for a eureka moment that solves world hunger. I heard
    Rich Sutton say something along the lines of “the obvious to you is your biggest
    contribution.” But if you don’t allow yourself to “reinvent the wheel,” you run
    the risk of not sharing the “obvious to you,” thinking it’s probably not worthy.
    I’m not saying you need to publish a paper about this new algorithm you thought
    about, Q-learning. I’m asking, please, don’t let the fear of doing “worthless”
    work stop you from experimenting. The bottom line is to keep reading, keep exploring,
    keep contributing, and let it all sink in. It’s a cycle; it’s a flow, so keep
    it going.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要知道你不可能学会一切。再次强调，我们是在学习心智，而关于心智的信息有很多。要明智，对阅读的内容要挑剔。作者是谁？她的背景是什么？尽管如此，你可以阅读它，但要有更高的自我意识。尽量多给予。你应该能够用另一种方式解释你所学的知识。这句话“不要重复造轮子”至多有些误导性。最好是你自己尝试一些事情；这是至关重要的。如果你早期就开始探索，你可能会发现自己有一个很好的想法，后来你意识到这个想法之前已经被研究过了。这没有什么可耻的；对你来说，继续前进比等待一个解决世界饥饿问题的顿悟时刻更为关键。我听说Rich
    Sutton说过类似的话：“对你来说显而易见的是你最大的贡献。”但如果你不允许自己“重复造轮子”，你就有可能不分享“对你来说显而易见”的东西，认为它可能不值得。我并不是说你需要发表一篇关于你思考过的这个新算法——Q-learning——的论文。我是在请求，请不要让害怕做“无价值”的工作阻止你进行实验。底线是继续阅读，继续探索，继续贡献，让这一切都沉淀下来。这是一个循环；这是一个流程，所以让它继续下去。
- en: Third and last, embrace the process, lose yourself on the path. Your dreams
    are only a way to get you going, but it’s in the going that you live your dreams.
    Get deep into it; don’t just follow what others are doing, and follow your interests.
    Think critically of your ideas, experiment, gather data, try to understand the
    results, and detach yourself from the result. Don’t bias your experiments, discover
    facts. If you lose yourself for long enough, you start to specialize, which is
    good. This field is so vast that being great at everything isn’t possible. However,
    if you follow your interests and intuition for long enough, you automatically
    spend more time on certain things versus other things. Keep going. Some of us
    feel a need to stay in the know, and once we start asking questions that have
    no answers, we feel the need to get back to shore. Don’t be afraid to ask hard
    questions, and work on getting answers. There are no dumb questions; each question
    is a clue for solving the mystery. Keep asking questions. Keep playing the game,
    and enjoy it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点也是最后一点，拥抱这个过程，让自己沉浸在这条道路上。你的梦想只是让你开始行动的一种方式，但你的梦想是在行动中实现的。深入其中；不要只是跟随别人的做法，要跟随你的兴趣。批判性地思考你的想法，进行实验，收集数据，尝试理解结果，并从结果中抽离出来。不要让你的实验带有偏见，去发现事实。如果你能长时间地沉浸其中，你开始专业化，这是好事。这个领域如此广泛，不可能在所有事情上都做得很好。然而，如果你长时间地跟随你的兴趣和直觉，你自然会花更多的时间在某些事情上，而不是其他事情上。继续前进。我们中的一些人感到需要保持知情，一旦我们开始提出没有答案的问题，我们就感到需要回到岸边。不要害怕提出难题，并努力寻找答案。没有愚蠢的问题；每个问题都是解开谜团的一个线索。继续提问。继续玩游戏，并享受它。
- en: Get yourself out there! Now!
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现在就让自己出去吧！
- en: Assuming you just finished this book, make sure to get out there right away,
    think about how to put things together, and contribute something to this amazing
    community. How about writing a blog post about several of the algorithms not covered
    in this book that you find interesting? How about investigating some of the advanced
    concepts discussed in this chapter and sharing what you find? Write a blog post,
    create a video, and let the world know about it. Become part of the movement;
    let’s find out what intelligence is, let’s build intelligent systems, together.
    It’s never the right time unless it’s now.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你刚刚完成了这本书，确保立即出去，思考如何将这些事情组合起来，并为这个了不起的社区做出贡献。比如写一篇关于你感兴趣的、这本书没有涵盖的算法的博客文章？或者调查本章讨论的一些高级概念，并分享你发现的内容？写一篇博客文章，制作一个视频，让世界知道这件事。成为运动的一部分；让我们找出什么是智能，让我们一起构建智能系统。除非现在是正确的时机，否则永远不会是正确的时机。
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: That’s it! You did it! That’s a wrap for this book. The ball is in your court.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！你做到了！这本书到此结束。球现在在你这边。
- en: 'In the first chapter, I defined deep reinforcement learning as follows: “Deep
    reinforcement learning is a machine learning approach to artificial intelligence
    concerned with creating computer programs that can solve problems requiring intelligence.
    The distinct property of DRL programs is learning through trial and error from
    feedback that’s simultaneously sequential, evaluative, and sampled by leveraging
    powerful non-linear function approximation.”'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我将深度强化学习定义为：“深度强化学习是一种机器学习方法，旨在创建能够解决需要智能问题的计算机程序。DRL程序的独特属性是通过利用强大的非线性函数逼近，从同时具有序列性、评估性和采样的反馈中通过试错进行学习。”
- en: I mentioned that success to me was that after you complete this book, you should
    be able to come back to this definition and understand it precisely. I said that
    you should be able to tell why I used the words that I used, and what each of
    these words means in the context of deep reinforcement learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到对我来说成功就是，在你完成这本书之后，你应该能够回到这个定义并精确地理解它。我说你应该能够说出我为什么使用这些词，以及这些词在深度强化学习背景下的含义。
- en: Did I succeed? Do you intuitively now understand this definition? Now it’s your
    turn to send your reward to the agent behind this book. Was this project a –1,
    a 0, or a +1? Whatever your comments, I learn, just like DRL agents, from feedback,
    and I look forward to reading your review and what you have to say. For now, my
    part is complete.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我成功了吗？你现在直觉上理解这个定义了吗？现在轮到你将你的奖励发送给这本书背后的代理人了。这个项目是-1，0，还是+1？无论你的评论如何，我都会像DRL代理一样从反馈中学习，我期待阅读你的评论和你的想法。现在，我的部分已经完成了。
- en: In this final chapter, we reviewed everything the book teaches, and we also
    discussed the core methods that we skipped, and some of the advanced concepts
    that could play a part in the eventual creation of artificial general intelligence
    agents.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们回顾了本书所教授的所有内容，我们还讨论了我们跳过的核心方法，以及一些可能最终在人工通用智能智能体的创建中发挥作用的先进概念。
- en: As a parting message, I’d like to first thank you for the opportunity you gave
    me to share with you my take on the field of deep reinforcement learning. I also
    want to encourage you to keep going, to concentrate on the day-to-day, to think
    more about what you can do next, with your current abilities, with your unique
    talents.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为告别信息，我想首先感谢你给我分享我对深度强化学习领域的看法的机会。我还想鼓励你继续前进，专注于日常，更多地思考你接下来能做什么，用你当前的能力，用你独特的才能。
- en: By now, you
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经
- en: Intuitively understand what deep reinforcement learning is; you know the details
    of the most critical deep reinforcement learning methods, from the most basic
    and foundational to the state-of-the-art.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观地理解深度强化学习是什么；你知道最关键的深度强化学习方法的细节，从最基本和基础到最前沿。
- en: Have a sense of where to go next because you understand how what we’ve learned
    fits into the big picture of the fields of deep reinforcement learning and artificial
    intelligence.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个明确的方向感，因为你理解我们所学的知识如何融入深度强化学习和人工智能领域的整体图景。
- en: Are ready to show us what you’ve got, your unique talents, and interests. Go,
    make the RL community proud. Now, it’s your turn!
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备好向我们展示你所拥有的，你独特的才能和兴趣。加油，让RL社区为你感到自豪。现在，轮到你了！
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 在自己的工作上努力，分享你的发现 |'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you’ll take advantage of it.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你所学的知识提升到下一个层次。如果你愿意，与世界分享你的成果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch13_tf01:** Implement model-based deep reinforcement learning methods.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf01:** 实现基于模型的深度强化学习方法。'
- en: '**#gdrl_ch13_tf02:** Implement gradient-free deep reinforcement learning methods.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf02:** 实现无梯度深度强化学习方法。'
- en: '**#gdrl_ch13_tf03:** Implement a multi-agent environment or agent, and share
    it.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf03:** 实现一个多智能体环境或智能体，并分享它。'
- en: '**#gdrl_ch13_tf04:** Use advance deep learning techniques not discussed in
    this book to get better results from deep reinforcement learning agents. To give
    you an idea, variational autoencoders (VAE) could be a promising way to compress
    the observation space. This way the agent can learn much quicker. Any other DL
    technique?'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf04:** 使用本书未讨论的先进深度学习技术，以获得深度强化学习智能体的更好结果。为了给你一个想法，变分自编码器（VAE）可能是压缩观察空间的一个有希望的方法。这样，智能体可以更快地学习。还有其他深度学习技术吗？'
- en: '**#gdrl_ch13_tf05:** Create a list of resources for learning several of the
    more advance techniques to develop artificial general intelligence, whether mentioned
    or not.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf05:** 创建一个资源列表，用于学习几个更高级的技术，以开发人工通用智能，无论是否提及。'
- en: '**#gdrl_ch13_tf06:** Grab your favorite algorithm from one of the approaches
    to AGI in this chapter, create a Notebook, and make a blog post explaining the
    details.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf06:** 从本章中关于AGI的某个方法中挑选你喜欢的算法，创建一个笔记本，并写一篇博客解释细节。'
- en: '**#gdrl_ch13_tf07:** Create a list of interesting environments already out
    there.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf07:** 创建一个现有有趣环境的列表。'
- en: '**#gdrl****_ch13_tf08:** Create a custom environment that you are passionate
    about, something unique, maybe a wrapper for AI to play a game, or the stock market,
    and so on.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl****_ch13_tf08:** 创建一个你热衷于的定制环境，可能是独特的，也许是为AI玩游戏或股市等创建的包装器。'
- en: '**#gdrl_ch13_tf09:** Update your resume and send it my way, and I’ll retweet
    it. Make sure to include several of the projects that you worked on, DRL related,
    of course.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf09:** 更新你的简历并发给我，我会转发它。确保包括你参与的一些项目，当然，与DRL相关。'
- en: '**#gdrl_ch13_tf10:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch13_tf10:** 在每一章中，我都在使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的工作。没有什么比为自己创造作业更令人兴奋的了。确保分享你打算调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它吧，<link>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。
    |
