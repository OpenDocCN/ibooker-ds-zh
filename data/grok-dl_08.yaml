- en: 'Chapter 9\. Modeling probabilities and nonlinearities: activation functions'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章：建模概率和非线性：激活函数
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本章中**'
- en: What is an activation function?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是激活函数？
- en: Standard hidden activation functions
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 标准隐藏激活函数
- en: Sigmoid
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Tanh
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: Standard output activation functions
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 标准输出激活函数
- en: Softmax
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax
- en: Activation function installation instructions
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数安装说明
- en: “I know that 2 and 2 make 4—& should be glad to prove it too if I could—though
    I must say if by any sort of process I could convert 2 & 2 into five it would
    give me much greater pleasure.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我知道2和2等于4——如果我能证明这一点，我也会很高兴——但我必须说，如果通过某种方式我能把2和2变成5，那会给我带来更大的快乐。”
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*George Gordon Byron, letter to Annabella Milbanke, November 10, 1813*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*乔治·戈登·拜伦，致安abella Milbanke的信，1813年11月10日*'
- en: What is an activation function?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是激活函数？
- en: It’s a function applied to the neurons in a layer during prediction
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是在预测期间应用于层的神经元的函数
- en: An *activation function* is a function applied to the neurons in a layer during
    prediction. This should seem very familiar, because you’ve been using an activation
    function called `relu` (shown here in the three-layer neural network). The `relu`
    function had the effect of turning all negative numbers to 0.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*激活函数*是在预测期间应用于层的神经元的函数。这应该看起来非常熟悉，因为你一直在使用一个名为`relu`的激活函数（如这里所示的三层神经网络）。`relu`函数的作用是将所有负数转换为0。'
- en: Oversimplified, an activation function is any function that can take one number
    and return another number. But there are an infinite number of functions in the
    universe, and not all them are useful as activation functions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，激活函数是任何可以取一个数字并返回另一个数字的函数。但是，宇宙中有无限多的函数，并不是所有的函数都适合作为激活函数。
- en: There are several constraints on what makes a function an activation function.
    Using functions outside of these constraints is usually a bad idea, as you’ll
    see.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有几个约束条件。使用这些约束之外的函数通常是个坏主意，你很快就会看到。
- en: '![](Images/f0162-01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0162-01.jpg)'
- en: 'Constraint 1: The function must be continuous and infinite in domain'
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束1：函数必须是连续的，并且在定义域内是无限的
- en: The first constraint on what makes a proper activation function is that it must
    have an output number for *any* input. In other words, you shouldn’t be able to
    put in a number that doesn’t have an output for some reason.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正确激活函数的第一个约束是它必须对任何输入都有一个输出数字。换句话说，你不应该能够输入一个没有输出的数字。
- en: A bit overkill, but see how the function on the left (four distinct lines) doesn’t
    have `y` values for every `x` value? It’s defined in only four spots. This would
    make for a horrible activation function. The function on the right, however, is
    continuous and infinite in domain. There is no input (`x`) for which you can’t
    compute an output (`y`).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 稍显过度，但看看左侧的函数（四条不同的线）是否没有为每个`x`值定义`y`值？它只在四个点上定义。这将是一个糟糕的激活函数。然而，右侧的函数是连续的，并且在定义域内是无限的。对于任何输入（`x`），你都可以计算出输出（`y`）。
- en: '![](Images/f0162-02_alt.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0162-02_alt.jpg)'
- en: 'Constraint 2: Good activation functions are monotonic, never changing direction'
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束2：好的激活函数是单调的，永远不会改变方向
- en: The second constraint is that the function is 1:1\. It must never change direction.
    In other words, it must either be always increasing or always decreasing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个约束是函数必须是1:1的。它永远不能改变方向。换句话说，它必须始终增加或始终减少。
- en: As an example, look at the following two functions. These shapes answer the
    question, “Given `x` as input, what value of `y` does the function describe?”
    The function on the left (`y = x * x`) isn’t an ideal activation function because
    it isn’t either always increasing or always decreasing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看以下两个函数。这些形状回答了“给定`x`作为输入，函数描述的`y`值是什么？”的问题。左侧的函数（`y = x * x`）不是一个理想的激活函数，因为它既不是始终增加也不是始终减少。
- en: 'How can you tell? Well, notice that there are many cases in which two values
    of `x` have a single value of `y` (this is true for every value except 0). The
    function on the right, however, is always increasing! There is no point at which
    two values of `x` have the same value of `y`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么判断呢？注意，有许多情况下两个`x`的值对应一个`y`的值（除了0以外的所有值都如此）。然而，右侧的函数始终在增加！没有任何两个`x`的值会有相同的`y`值：
- en: '![](Images/f0163-01_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0163-01_alt.jpg)'
- en: This particular constraint isn’t technically a requirement. Unlike functions
    that have missing values (noncontinuous), you can optimize functions that aren’t
    monotonic. But consider the implication of having multiple input values map to
    the same output value.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的约束在技术上并不是一个要求。与有缺失值的函数（非连续的）不同，你可以优化非单调的函数。但考虑多个输入值映射到相同输出值的含义。
- en: When you’re learning in neural networks, you’re searching for the right weight
    configurations to give a specific output. This problem can get a lot harder if
    there are multiple right answers. If there are multiple ways to get the same output,
    then the network has multiple possible perfect configurations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在神经网络中学习时，你正在寻找正确的权重配置以产生特定的输出。如果有多个正确答案，这个问题可能会变得更加困难。如果有多种方式可以得到相同的输出，那么网络就有多个可能的完美配置。
- en: An optimist might say, “Hey, this is great! You’re more likely to find the right
    answer if it can be found in multiple places!” A pessimist would say, “This is
    terrible! Now you don’t have a correct direction to go to reduce the error, because
    you can go in either direction and theoretically make progress.”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个乐观主义者可能会说：“嘿，这太好了！如果答案可以在多个地方找到，那么你更有可能找到正确的答案！”而一个悲观主义者可能会说：“这太糟糕了！现在你没有正确的方向去减少错误，因为你可以在任何方向上前进，理论上都可以取得进步。”
- en: Unfortunately, the phenomenon the pessimist identified is more important. For
    an advanced study of this subject, look more into convex versus non-convex optimization;
    many universities (and online classes) have entire courses dedicated to these
    kinds of questions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，悲观主义者所识别的现象更为重要。对于这个主题的高级研究，更多地了解凸优化与非凸优化；许多大学（和在线课程）都有专门针对这些问题的课程。
- en: 'Constraint 3: Good activation functions are nonlinear (they squiggle or turn)'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束3：好的激活函数是非线性的（它们会弯曲或转向）
- en: The third constraint requires a bit of recollection back to [chapter 6](kindle_split_014.xhtml#ch06).
    Remember *sometimes correlation*? In order to create it, you had to allow the
    neurons to selectively correlate to input neurons such that a very negative signal
    from one input into a neuron could reduce how much it correlated to any input
    (by forcing the neuron to drop to 0, in the case of `relu`).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个约束需要回顾一下[第6章](kindle_split_014.xhtml#ch06)。还记得“有时相关性”吗？为了创建它，你必须允许神经元选择性地与输入神经元相关联，使得一个来自一个输入到神经元的非常负的信号可以减少它与任何输入的相关性（在`relu`的情况下，通过迫使神经元降至0）。
- en: As it turns out, this phenomenon is facilitated by *any function that curves*.
    Functions that look like straight lines, on the other hand, scale the weighted
    average coming in. Scaling something (multiplying it by a constant like 2) doesn’t
    affect how correlated a neuron is to its various inputs. It makes the collective
    correlation that’s represented louder or softer. But the activation doesn’t allow
    one weight to affect how correlated the neuron is to the other weights. What you
    *really* want is *selective* correlation. Given a neuron with an activation function,
    you want one incoming signal to be able to increase or decrease how correlated
    the neuron is to all the other incoming signals. All curved lines do this (to
    varying degrees, as you’ll see).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种现象是由任何曲线函数促进的。另一方面，看起来像直线的函数会放大进入的加权平均值。放大某物（乘以一个常数，如2）不会影响神经元与其各种输入的相关性。它会使表示的集体相关性更响亮或更微弱。但激活函数不允许一个权重影响神经元与其他权重的相关性。你真正想要的是“选择性”的相关性。给定一个具有激活函数的神经元，你希望一个进入的信号能够增加或减少神经元与所有其他进入信号的相关性。所有曲线都做到了这一点（程度不同，你将会看到）。
- en: Thus, the function shown here on the left is considered a linear function, whereas
    the one on the right is considered nonlinear and will usually make for a better
    activation function (there are exceptions, which we’ll discuss later).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里左边显示的函数被认为是线性函数，而右边的是非线性的，通常会成为更好的激活函数（尽管有例外，我们稍后会讨论）。
- en: '![](Images/f0164-01_alt.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0164-01_alt.jpg)'
- en: 'Constraint 4: Good activation functions (and their derivatives) s- should be
    efficiently computable'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束4：好的激活函数（及其导数）应该能够高效地计算
- en: This one is pretty simple. You’ll be calling this function a lot (sometimes
    billions of times), so you don’t want it to be too slow to compute. Many recent
    activation functions have become popular because they’re so easy to compute at
    the expense of their expressiveness (`relu` is a great example of this).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个很简单。你将频繁地调用这个函数（有时是数十亿次），所以你不想让它计算得太慢。许多最近的激活函数之所以受欢迎，是因为它们易于计算，尽管牺牲了它们的表达能力（`relu`就是这样一个很好的例子）。
- en: Standard hidden-layer activation functions
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准隐藏层激活函数
- en: Of the infinite possible functions, which ones are most commonly used?
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在无限可能的功能中，哪些是最常用的？
- en: Even with these constraints, it should be clear that an infinite (possibly transfinite?)
    number of functions could be used as activation functions. The last few years
    have seen a lot of progress in state-of-the-art activations. But there’s still
    a relatively small list of activations that account for the vast majority of activation
    needs, and improvements on them have been minute in most cases.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有这些限制，也应该清楚，可以使用无限（可能还是超越无限的？）数量的函数作为激活函数。在过去的几年里，最先进的激活函数取得了很大的进展。但仍然只有相对较少的激活函数能够满足大部分激活需求，而且大多数情况下对它们的改进都是微小的。
- en: sigmoid is the bread-and-butter activation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: sigmoid 是最基本的激活函数
- en: '`sigmoid` is great because it smoothly squishes the infinite amount of input
    to an output between 0 and 1\. In many circumstances, this lets you interpret
    the output of any individual neuron as a probability. Thus, people use this nonlinearity
    both in hidden layers and output layers.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`很棒，因为它可以平滑地将无限多的输入压缩到0和1之间的输出。在许多情况下，这让你可以解释任何单个神经元的输出作为一个概率。因此，人们既在隐藏层也在输出层使用这种非线性。'
- en: '![](Images/f0165-01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0165-01.jpg)'
- en: tanh is better than sigmoid for hidden layers
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: tanh 对于隐藏层比 sigmoid 更好
- en: Here’s the cool thing about `tanh`. Remember modeling selective correlation?
    Well, `sigmoid` gives varying degrees of positive correlation. That’s nice. `tanh`
    is the same as `sigmoid` except it’s between –1 and 1!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`tanh`的酷之处在于。还记得建模选择性相关性吗？好吧，`sigmoid`提供不同程度的正相关性。这很好。`tanh`与`sigmoid`相同，只是它的范围在-1和1之间！
- en: This means it can also throw in some *negative correlation*. Although it isn’t
    that useful for output layers (unless the data you’re predicting goes between
    –1 and 1), this aspect of negative correlation is powerful for hidden layers;
    on many problems, `tanh` will outperform `sigmoid` in hidden layers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它也可以引入一些*负相关性*。尽管它对输出层（除非你预测的数据在-1和1之间）并不是特别有用，但负相关性的这一方面在隐藏层中非常强大；在许多问题上，`tanh`在隐藏层中会比`sigmoid`表现得更好。
- en: '![](Images/f0165-02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0165-02.jpg)'
- en: Standard output layer activation functions
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准输出层激活函数
- en: Choosing the best one depends on what you’re trying to predict
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择最好的一个取决于你试图预测什么
- en: It turns out that what’s best for hidden-layer activation functions can be quite
    different from what’s best for output-layer activation functions, especially when
    it comes to classification. Broadly speaking, there are three major types of output
    layer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，对隐藏层激活函数的最佳选择可能与输出层激活函数的最佳选择大不相同，尤其是在分类方面。总的来说，输出层有三种主要类型。
- en: 'Configuration 1: Predicting raw data values (no activation function)'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置1：预测原始数据值（无激活函数）
- en: This is perhaps the most straightforward but least common type of output layer.
    In some cases, people want to train a neural network to transform one matrix of
    numbers into another matrix of numbers, where the range of the output (difference
    between lowest and highest values) is something other than a probability. One
    example might be predicting the average temperature in Colorado given the temperature
    in the surrounding states.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最直接但最不常见的一种输出层。在某些情况下，人们希望训练一个神经网络将一个数字矩阵转换成另一个数字矩阵，其中输出的范围（最低值和最高值之间的差异）不是概率。一个例子可能是根据周围州的温度预测科罗拉多州的平均温度。
- en: The main thing to focus on here is ensuring that the output nonlinearity can
    predict the right answers. In this case, a `sigmoid` or `tanh` would be inappropriate
    because it forces every prediction to be between 0 and 1 (you want to predict
    any temperature, not just between 0 and 1). If I were training a network to do
    this prediction, I’d very likely train the network without an activation function
    on the output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要关注的主要是确保输出非线性可以预测正确的答案。在这种情况下，`sigmoid` 或 `tanh` 会被认为是不合适的，因为它强制每个预测都在 0
    和 1 之间（你想要预测任何温度，而不仅仅是 0 到 1 之间的温度）。如果我要训练一个网络来做这个预测，我非常可能会在没有输出激活函数的情况下训练网络。
- en: 'Configuration 2: Predicting unrelated yes/no probabilities (sigmoid)'
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置 2：预测无关的 yes/no 概率（sigmoid）
- en: You’ll often want to make multiple binary probabilities in one neural network.
    We did this in the “[Gradient descent with multiple inputs and outputs](kindle_split_013.xhtml#ch05lev1sec6)”
    section of [chapter 5](kindle_split_013.xhtml#ch05), predicting whether the team
    would win, whether there would be injuries, and the morale of the team (happy
    or sad) based on the input data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常希望在一个神经网络中做出多个二元概率。我们在第 5 章的“[具有多个输入和输出的梯度下降](kindle_split_013.xhtml#ch05lev1sec6)”部分中这样做过，根据输入数据预测球队是否会赢，是否有伤害，以及球队的士气（快乐或悲伤）。
- en: As an aside, when a neural network has hidden layers, predicting multiple things
    at once can be beneficial. Often the network will learn something when predicting
    one label that will be useful to one of the other labels. For example, if the
    network got really good at predicting whether the team would win ballgames, the
    same hidden layer would likely be very useful for predicting whether the team
    would be happy or sad. But the network might have a harder time predicting happiness
    or sadness without this extra signal. This tends to vary greatly from problem
    to problem, but it’s good to keep in mind.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为旁白，当一个神经网络有隐藏层时，同时预测多个事物可能是有益的。通常，当网络预测一个标签时，它会学到一些对其他标签有用的东西。例如，如果网络在预测球队是否会赢得球赛方面非常出色，那么相同的隐藏层很可能对预测球队是否会高兴或悲伤也非常有用。但是，如果没有这个额外信号，网络可能很难预测快乐或悲伤。这通常因问题而异很大，但值得记住。
- en: In these instances, it’s best to use the `sigmoid` activation function, because
    it models individual probabilities separately for each output node.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，最好使用 `sigmoid` 激活函数，因为它为每个输出节点分别建模单独的概率。
- en: 'Configuration 3: Predicting which-one probabilities (softmax)'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置 3：预测哪个概率（softmax）
- en: By far the most common use case in neural networks is predicting a single label
    out of many. For example, in the MNIST digit classifier, you want to predict *which*
    number is in the image. You know ahead of time that the image can’t be more than
    one number. You can train this network with a `sigmoid` activation function and
    declare that the highest output probability is the most likely. This will work
    reasonably well. But it’s far better to have an activation function that models
    the idea that “The more likely it’s one label, the less likely it’s any of the
    other labels.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，最常见的情况是预测许多标签中的一个。例如，在 MNIST 数字分类器中，你想要预测图像中是哪个数字。你知道提前图像不可能包含超过一个数字。你可以用
    `sigmoid` 激活函数训练这个网络，并声明最高的输出概率是最可能的。这会工作得相当好。但有一个激活函数来模拟“越有可能是一个标签，其他标签的可能性就越小”的想法会更好。
- en: 'Why do we like this phenomenon? Consider how weight updates are performed.
    Let’s say the MNIST digit classifier should predict that the image is a 9\. Also
    say that the raw weighted sums going into the final layer (before applying an
    activation function) are the following values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么喜欢这种现象？考虑权重更新的方式。假设 MNIST 数字分类器应该预测图像是 9。也假设原始加权总和进入最终层（在应用激活函数之前）的值如下：
- en: '![](Images/f0167-01_alt.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0167-01_alt.jpg)'
- en: 'The network’s raw input to the last layer predicts a 0 for every node but 9,
    where it predicts 100\. You might call this perfect. Let’s see what happens when
    these numbers are run through a `sigmoid` activation function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的原始输入到最后一层预测除了节点 9 以外的每个节点为 0，而节点 9 预测为 100。你可能称之为完美。让我们看看当这些数字通过 `sigmoid`
    激活函数运行时会发生什么：
- en: '![](Images/f0167-02_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0167-02_alt.jpg)'
- en: 'Strangely, the network seems less sure now: 9 is still the highest, but the
    network seems to think there’s a 50% chance that it could be any of the other
    numbers. Weird! `softmax`, on the other hand, interprets the input very differently:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，网络现在似乎不太确定：9仍然是最高，但网络似乎认为有50%的可能性它可能是其他任何数字。真奇怪！另一方面，`softmax`对输入的解释非常不同：
- en: '![](Images/f0167-03_alt.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0167-03_alt.jpg)'
- en: 'This looks great. Not only is 9 the highest, but the network doesn’t even suspect
    it’s any of the other possible MNIST digits. This might seem like a theoretical
    flaw of `sigmoid`, but it can have serious consequences when you backpropagate.
    Consider how the mean squared error is calculated on the `sigmoid` output. In
    theory, the network is predicting nearly perfectly, right? Surely it won’t backprop
    much error. Not so for `sigmoid`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很棒。不仅9是最高的，而且网络甚至没有怀疑它可能是其他任何可能的MNIST数字。这看起来可能像是`sigmoid`的理论缺陷，但当你反向传播时，它可能会有严重的后果。考虑在`sigmoid`输出上如何计算均方误差。从理论上讲，网络预测得几乎完美，对吧？当然，它不会反向传播很多错误。但`sigmoid`不是这样：
- en: '![](Images/f0167-04_alt.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0167-04_alt.jpg)'
- en: Look at all the error! These weights are in for a massive weight update even
    though the network predicted perfectly. Why? For `sigmoid` to reach 0 error, it
    doesn’t just have to predict the highest positive number for the true output;
    it also has to predict a 0 everywhere else. Where `softmax` asks, “Which digit
    seems like the best fit for this input?” `sigmoid` says, “You better believe that
    it’s only digit 9 and doesn’t have anything in common with the other MNIST digits.”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 看看所有的错误！尽管网络预测得完美，但这些权重将进行大规模的权重更新。为什么？为了`sigmoid`达到0错误，它不仅必须预测真实输出的最高正数；它还必须预测其他所有地方都是0。而`softmax`问，“哪个数字似乎最适合这个输入？”`sigmoid`说，“你最好相信它只有数字9，并且与其他MNIST数字没有共同之处。”
- en: 'The core issue: Inputs have similarity'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心问题：输入具有相似性
- en: Different numbers share characteristics. It’s good to let the net- twork believe
    that
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同的数字具有特征。让网络相信这是好事
- en: 'MNIST digits aren’t all completely different: they have overlapping pixel values.
    The average 2 shares quite a bit in common with the average 3.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数字并不都是完全不同的：它们有重叠的像素值。平均的2与平均的3有很多共同之处。
- en: Why is this important? Well, as a general rule, similar inputs create similar
    outputs. When you take some numbers and multiply them by a matrix, if the starting
    numbers are pretty similar, the ending numbers will be pretty similar.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？嗯，作为一条一般规则，相似的输入会产生相似的输出。当你拿一些数字并将它们乘以一个矩阵时，如果起始数字很相似，那么结束数字也会很相似。
- en: '![](Images/f0168-01.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0168-01.jpg)'
- en: Consider the 2 and 3 shown here. If we forward propagate the 2 and a small amount
    of probability accidentally goes to the label 3, what does it mean for the network
    to consider this a big mistake and respond with a big weight update? It will penalize
    the network for recognizing a 2 by anything other than features that are exclusively
    related to 2s. It penalizes the network for recognizing a 2 based on, say, the
    top curve. Why? Because 2 and 3 share the same curve at the top of the image.
    Training with `sigmoid` would penalize the network for trying to predict a 2 based
    on this input, because by doing so it would be looking for the same input it does
    for 3s. Thus, when a 3 came along, the 2 label would get some probability (because
    part of the image looks 2ish).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这里显示的2和3。如果我们正向传播2，并且不小心有一部分概率流向了标签3，这对网络来说意味着什么，以至于它认为这是一个大错误并做出大的权重更新？它会惩罚网络，使其不能通过除了与2s独家相关的特征之外的其他任何特征来识别2。它会惩罚网络，使其基于，比如说，顶部曲线来识别2。为什么？因为2和3在图像顶部的曲线是相同的。使用`sigmoid`训练会惩罚网络尝试根据这个输入预测2，因为这样做它就会寻找与3s相同的输入。因此，当出现3时，2标签会得到一些概率（因为图像的一部分看起来像2）。
- en: What’s the side effect? Most images share lots of pixels in the middle of images,
    so the network will start trying to focus on the edges. Consider the 2-detector
    node weights shown at right.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 会有什么副作用？大多数图像在图像中间共享很多像素，所以网络会开始尝试专注于边缘。考虑右侧显示的2检测节点权重。
- en: See how muddy the middle of the image is? The heaviest weights are the end points
    of the 2 toward the edge of the image. On one hand, these are probably the best
    individual indicators of a 2, but the best overall is a network that sees the
    entire shape for what it is. These individual indicators can be accidentally triggered
    by a 3 that’s slightly off-center or tilted the wrong way. The network isn’t learning
    the true essence of a 2 because it needs to learn 2 and *not* 1, *not* 3, *not*
    4, and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 看看图像中间有多混乱？最重的权重是图像边缘2的端点。一方面，这些可能是2的最佳单个指标，但最好的整体是网络能够看到整个形状。这些单个指标可能会被稍微偏离中心或倾斜错误方向的3意外触发。网络没有学习到2的真正本质，因为它需要学习2而不是1，不是3，不是4，等等。
- en: '![](Images/f0168-02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0168-02.jpg)'
- en: We want an output activation that won’t penalize labels that are similar. Instead,
    we want it to pay attention to all the information that can be indicative of any
    potential input. It’s also nice that a `softmax`’s probabilities always sum to
    1\. You can interpret any individual prediction as a global probability that the
    prediction is a particular label. `softmax` works better in both theory and practice.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望输出激活函数不会惩罚相似的标签。相反，我们希望它关注所有可能指示任何潜在输入的信息。而且，`softmax`的概率总和总是等于1。你可以将任何单个预测解释为预测是特定标签的全局概率。`softmax`在理论和实践中都表现更好。
- en: softmax computation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`softmax`计算'
- en: softmax raises each input value exponentially and then divides by- y the layer’s
    sum
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`softmax`将每个输入值进行指数化，然后除以层的总和'
- en: 'Let’s see a `softmax` computation on the neural network’s hypothetical output
    values from earlier. I’ll show them here again so you can see the input to `softmax`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看之前神经网络假设输出值的`softmax`计算。我将再次在这里展示，这样你可以看到`softmax`的输入：
- en: '![](Images/f0169-01_alt.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0169-01_alt.jpg)'
- en: To compute a `softmax` on the whole layer, first raise each value exponentially.
    For each value `x`, compute *e* to the power of `x` (*e* is a special number ~2.71828...).
    The value of *`e`*`^x` is shown on the right.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要在整个层上计算`softmax`，首先将每个值进行指数化。对于每个值`x`，计算`e`的`x`次幂（`e`是一个特殊的数，大约是2.71828...）。`e``^x`的值显示在右侧。
- en: '![](Images/f0169-02.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0169-02.jpg)'
- en: Notice that it turns every prediction into a positive number, where negative
    numbers turn into very small positive numbers, and big numbers turn into very
    big numbers. (If you’ve heard of exponential growth, it was likely talking about
    this function or one very similar to it.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它将每个预测转换成正数，其中负数变成了非常小的正数，而大数变成了非常大的数。（如果你听说过指数增长，那可能就是在谈论这个函数或一个非常相似的函数。）
- en: '![](Images/f0169-03_alt.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0169-03_alt.jpg)'
- en: In short, all the 0s turn to 1s (because 1 is the y intercept of *`e`*`^x`),
    and the 100 turns into a massive number (2 followed by 43 zeros). If there were
    any negative numbers, they turned into something between 0 and 1\. The next step
    is to sum all the nodes in the layer and divide each value in the layer by that
    sum. This effectively makes every number 0 except the value for label 9.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，所有的0都变成了1（因为1是*`e`*`^x`的y截距），而100变成了一个巨大的数字（2后面跟着43个0）。如果有任何负数，它们变成了介于0和1之间的数。下一步是将这一层的所有节点相加，然后将这一层的每个值除以这个总和。这实际上使得除了标签9的值之外的所有数字都变成了0。
- en: '![](Images/f0169-04_alt.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0169-04_alt.jpg)'
- en: The nice thing about `softmax` is that the higher the network predicts one value,
    the lower it predicts all the others. It increases what is called the *sharpness
    of attenuation*. It encourages the network to predict one output with very high
    probability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmax`的好处是，网络预测的某个值越高，它预测的其他值就越低。它增加了所谓的衰减的“尖锐度”。它鼓励网络以非常高的概率预测一个输出。'
- en: To adjust how aggressively it does this, use numbers slightly higher or lower
    than *e* when exponentiating. Lower numbers will result in lower attenuation,
    and higher numbers will result in higher attenuation. But most people just stick
    with *e*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要调整它执行这一操作的积极性，在指数化时使用比`e`稍高或稍低的数字。较小的数字会导致较低的衰减，而较大的数字会导致较高的衰减。但大多数人只是坚持使用`e`。
- en: Activation installation instructions
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数安装说明
- en: How do you add your favorite activation function to any layer?
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你如何将你最喜欢的激活函数添加到任何层中？
- en: 'Now that we’ve covered a wide variety of activation functions and explained
    their usefulness in hidden and output layers of neural networks, let’s talk about
    the proper way to install one into a neural network. Fortunately, you’ve already
    seen an example of how to use a nonlinearity in your first deep neural network:
    you added a `relu` activation function to the hidden layer. Adding this to forward
    propagation was relatively straightforward. You took what `layer_1` would have
    been (without an activation) and applied the `relu` function to each value:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了各种激活函数，并解释了它们在神经网络隐藏层和输出层中的有用性，让我们谈谈将激活函数正确安装到神经网络中的方法。幸运的是，你已经在你的第一个深度神经网络中看到了如何使用非线性的例子：你向隐藏层添加了`relu`激活函数。将其添加到正向传播相对简单。你取了`layer_1`将有的值（没有激活），并对每个值应用了`relu`函数：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There’s a bit of lingo here to remember. The *input to a layer* refers to the
    value before the nonlinearity. In this case, the input to `layer_1` is `np.dot(layer_0,weights_0_1)`.
    This isn’t to be confused with the previous layer, `layer_0`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些术语需要记住。*层的输入*指的是非线性之前的值。在这种情况下，`layer_1`的输入是`np.dot(layer_0,weights_0_1)`。这不要与前面的层`layer_0`混淆。
- en: Adding an activation function to a layer in forward propagation is relatively
    straightforward. But properly compensating for the activation function in backpropagation
    is a bit more nuanced.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传播中向层添加激活函数相对简单。但在反向传播中正确补偿激活函数则要微妙得多。
- en: In [chapter 6](kindle_split_014.xhtml#ch06), we performed an interesting operation
    to create the `layer_1_delta` variable. Wherever `relu` had forced a `layer_1`
    value to be 0, we also multiplied the `delta` by 0\. The reasoning at the time
    was, “Because a `layer_1` value of 0 had no effect on the output prediction, it
    shouldn’t have any impact on the weight update either. It wasn’t responsible for
    the error.” This is the extreme form of a more nuanced property. Consider the
    shape of the `relu` function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_014.xhtml#ch06)中，我们进行了一个有趣的操作来创建`layer_1_delta`变量。无论`relu`如何将`layer_1`的值强制设为0，我们也会将`delta`乘以0。当时的推理是，“因为`layer_1`的值为0对输出预测没有影响，它也不应该对权重更新有任何影响。它不负责错误。”这是更微妙属性的一种极端形式。考虑`relu`函数的形状。
- en: The slope of `relu` for positive numbers is exactly 1\. The slope of `relu`
    for negative numbers is exactly 0\. Modifying the input to this function (by a
    tiny amount) will have a 1:1 effect if it was predicting positively, and will
    have a 0:1 effect (none) if it was predicting negatively. This slope is a measure
    of how much the output of `relu` will change given a change in its input.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正数，`relu`的斜率正好是1。对于负数，`relu`的斜率正好是0。修改这个函数的输入（通过一个非常小的量）如果它预测的是正值，将产生1:1的效果，如果它预测的是负值，将产生0:1的效果（没有效果）。这个斜率是衡量`relu`的输出在输入变化时将如何变化的一个指标。
- en: Because the purpose of `delta` at this point is to tell earlier layers “make
    my input higher or lower next time,” this `delta` is very useful. It modifies
    the `delta` backpropagated from the following layer to take into account whether
    this node contributed to the error.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此时`delta`的目的在于告诉前面的层“下次让我的输入更高或更低”，这个`delta`非常有用。它修改了从后续层反向传播回来的`delta`，以考虑这个节点是否对错误有贡献。
- en: '![](Images/f0170-01.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0170-01.jpg)'
- en: 'Thus, when you backpropagate, in order to generate `layer_1_delta`, multiply
    the backpropagated `delta` from `layer_2` (`layer_2_delta.dot(weights_1_2.T)`)
    by the slope of `relu` *at the point predicted in forward propagation*. For some
    `delta`s the slope is 1 (positive numbers), and for others it’s 0 (negative numbers):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在反向传播时，为了生成`layer_1_delta`，将来自`layer_2`的反向传播`delta`（`layer_2_delta.dot(weights_1_2.T)`）乘以在正向传播中预测的点处的`relu`斜率。对于某些`delta`，斜率是1（正值），而对于其他`delta`，斜率是0（负值）：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Returns x if x > 0; returns 0 otherwise**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果x > 0，则返回x；否则返回0**'
- en: '***2* Returns 1 for input > 0; returns 0 otherwise**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果输入 > 0，则返回1；否则返回0**'
- en: '`relu2deriv` is a special function that can take the output of `relu` and calculate
    the slope of `relu` at that point (it does this for all the values in the output
    vector). This begs the question, how do you make similar adjustments for all the
    other nonlinearities that aren’t `relu`? Consider `relu` and `sigmoid`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`relu2deriv`是一个特殊函数，它可以接受`relu`的输出并计算在该点`relu`的斜率（它对输出向量中的所有值都这样做）。这引发了一个问题，如何对其他不是`relu`的非线性进行调整？考虑`relu`和`sigmoid`：'
- en: '![](Images/f0171-01_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0171-01_alt.jpg)'
- en: The important thing in these figures is that the slope is an indicator of how
    much a *tiny* change to the input affects the output. You want to modify the incoming
    `delta` (from the following layer) to take into account whether a weight update
    before this node would have any effect. Remember, the end goal is to adjust weights
    to reduce error. This step encourages the network to leave weights alone if adjusting
    them will have little to no effect. It does so by multiplying it by the slope.
    It’s no different for `sigmoid`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图中的重要之处在于斜率是输入的微小变化对输出影响程度的一个指标。你想要修改输入的`delta`（来自下一层），以考虑在此节点之前更新权重是否会有任何影响。记住，最终目标是调整权重以减少误差。这一步鼓励网络在调整权重将几乎没有效果的情况下保持权重不变。它是通过乘以斜率来实现的。对于`sigmoid`来说，也是如此。
- en: Multiplying delta by the slope
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将差分乘以斜率
- en: To compute layer_delta, multiply the backpropagated delta by the layer’s slope
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要计算层差分（layer_delta），需要将反向传播的差分乘以层的斜率
- en: '`layer_1_delta[0]` represents how much higher or lower the first hidden node
    of layer 1 should be in order to reduce the error of the network (for a particular
    training example). When there’s no nonlinearity, this is the weighted average
    `delta` of `layer_2`.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_1_delta[0]`表示为了减少网络（对于特定的训练示例）的误差，层1的第一个隐藏节点应该比现在高多少或低多少。当没有非线性时，这是`layer_2`的加权平均`delta`。'
- en: '![](Images/f0172-01.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0172-01.jpg)'
- en: But the end goal of `delta` on a neuron is to inform the weights whether they
    should move. If moving them would have no effect, they (as a group) should be
    left alone. This is obvious for `relu`, which is either on or off. `sigmoid` is,
    perhaps, more nuanced.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但神经元上`delta`的最终目标是通知权重它们是否应该移动。如果移动它们没有任何效果，它们（作为一个组）应该保持不变。对于`relu`来说，这是显而易见的，它要么开启要么关闭。`sigmoid`可能更为微妙。
- en: '![](Images/f0172-02_alt.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0172-02_alt.jpg)'
- en: Consider a single `sigmoid` neuron. `sigmoid`’s sensitivity to change in the
    input slowly increases as the input approaches 0 from either direction. But very
    positive and very negative inputs approach a slope of very near 0\. Thus, as the
    input becomes very positive or very negative, small changes to the incoming weights
    become less relevant to the neuron’s error at this training example. In broader
    terms, many hidden nodes are irrelevant to the accurate prediction of a 2 (perhaps
    they’re used only for 8s). You shouldn’t mess with their weights too much, because
    you could corrupt their usefulness elsewhere.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个单一的`sigmoid`神经元。当输入从任一方向接近0时，`sigmoid`对输入变化的敏感性会逐渐增加。但非常正的和非常负的输入接近斜率接近0。因此，当输入变得非常正或非常负时，对输入权重的小幅变化对神经元在此训练示例中的误差变得不那么相关。更广泛地说，许多隐藏节点对于准确预测2（可能它们只用于8）是不相关的。你不应该过多地调整它们的权重，因为这样可能会损害它们在其他地方的有用性。
- en: Inversely, this also creates a notion of *stickiness*. Weights that have previously
    been updated a lot in one direction (for similar training examples) confidently
    predict a high value or low value. These nonlinearities help make it harder for
    occasional erroneous training examples to corrupt intelligence that has been reinforced
    many times.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这也产生了一种**粘性**的概念。那些在一个方向上（对于相似的训练示例）之前被大量更新的权重会自信地预测一个高值或低值。这些非线性特性有助于使偶尔的错误训练示例难以破坏多次被强化的智能。
- en: Converting output to slope (derivative)
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将输出转换为斜率（导数）
- en: Most great activations can convert their output to their slope. (- (Efficiency
    win!)
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大多数优秀的激活函数都能将它们的输出转换为斜率。(- (效率提升！)
- en: Now that you know that adding an activation to a layer changes how to compute
    `delta` for that layer, let’s discuss how the industry does this efficiently.
    The new operation necessary is the computation of the derivative of whatever nonlinearity
    was used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道向层添加激活函数会改变该层的`delta`计算方式，让我们讨论一下行业是如何高效地做到这一点的。必要的新的操作是计算所使用的非线性的导数。
- en: Most nonlinearities (all the popular ones) use a method of computing a derivative
    that will seem surprising to those of you who are familiar with calculus. Instead
    of computing the derivative at a certain point on its curve the normal way, most
    great activation functions have a means by which the *output* of the layer (at
    forward propagation) can be used to compute the derivative. This has become the
    standard practice for computing derivatives in neural networks, and it’s quite
    handy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非线性（所有流行的非线性）使用一种计算导数的方法，这可能会让熟悉微积分的你们感到惊讶。大多数优秀的激活函数不是以通常的方式在曲线上的某个点上计算导数，而是有一种方法，可以通过层的前向传播的
    *输出* 来计算导数。这已经成为计算神经网络中导数的标准做法，并且非常方便。
- en: Following is a small table for the functions you’ve seen so far, paired with
    their derivatives. `input` is a NumPy vector (corresponding to the input to a
    layer). `output` is the prediction of the layer. `deriv` is the derivative of
    the vector of activation derivatives corresponding to the derivative of the activation
    at each node. `true` is the vector of true values (typically 1 for the correct
    label position, 0 everywhere else).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个小型表格，列出了你迄今为止看到的函数及其导数。`输入` 是 NumPy 向量（对应于层的输入）。`输出` 是层的预测。`导数` 是对应于每个节点激活导数导数的向量导数。`真实值`
    是真实值的向量（通常对于正确标签位置为 1，其他地方为 0）。
- en: '| Function | Forward prop | Backprop delta |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 前向传播 | 反向传播 delta |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| relu | ones_and_zeros = (input > 0) output = input*ones_and_zeros | mask
    = output > 0 deriv = output * mask |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| relu | ones_and_zeros = (输入 > 0) 输出 = 输入 * ones_and_zeros | mask = 输出 > 0
    导数 = 输出 * mask |'
- en: '| sigmoid | output = 1/(1 + np.exp(-input)) | deriv = output*(1-output) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| sigmoid | 输出 = 1 / (1 + np.exp(-输入)) | 导数 = 输出 * (1 - 输出) |'
- en: '| tanh | output = np.tanh(input) | deriv = 1 - (output**2) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| tanh | 输出 = np.tanh(输入) | 导数 = 1 - (输出^2) |'
- en: '| softmax | temp = np.exp(input) output /= np.sum(temp) | temp = (output -
    true) output = temp/len(true) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| softmax | temp = np.exp(输入) 输出 /= np.sum(temp) | temp = (输出 - 真实值) 输出 = temp
    / len(真实值) |'
- en: Note that the `delta` computation for `softmax` is special because it’s used
    only for the last layer. There’s a bit more going on (theoretically) than we have
    time to discuss here. For now, let’s install some better activation functions
    in the MNIST classification network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`softmax` 的 `delta` 计算是特殊的，因为它只用于最后一层。理论上还有更多的事情发生，但我们没有时间在这里讨论。现在，让我们在 MNIST
    分类网络中安装一些更好的激活函数。
- en: Upgrading the MNIST network
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级 MNIST 网络
- en: Let’s upgrade the MNIST network to reflect what you’ve learned
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们将 MNIST 网络升级，以反映你所学的知识。
- en: Theoretically, the `tanh` function should make for a better hidden-layer activation,
    and `softmax` should make for a better output-layer activation function. When
    we test them, they do in fact reach a higher score. But things aren’t always as
    simple as they seem.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，`tanh` 函数应该是一个更好的隐藏层激活函数，而 `softmax` 应该是一个更好的输出层激活函数。当我们测试它们时，它们确实达到了更高的分数。但事情并不总是像看起来那么简单。
- en: I had to make a couple of adjustments in order to tune the network properly
    with these new activations. For `tanh`, I had to reduce the standard deviation
    of the incoming weights. Remember that you initialize the weights randomly. `np.random.random`
    creates a random matrix with numbers randomly spread between 0 and 1\. By multiplying
    by 0.2 and subtracting by 0.1, you rescale this random range to be between –0.1
    and 0.1\. This worked great for `relu` but is less optimal for `tanh`. `tanh`
    likes to have a narrower random initialization, so I adjusted it to be between
    –0.01 and 0.01.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确调整这些新激活函数的网络，我不得不进行一些调整。对于 `tanh`，我必须减小输入权重的标准差。记住，你随机初始化权重。`np.random.random`
    创建一个在 0 和 1 之间随机分布的随机矩阵。通过乘以 0.2 并减去 0.1，你将这个随机范围重新缩放为 -0.1 和 0.1 之间。这对于 `relu`
    工作得很好，但对于 `tanh` 来说则不太理想。`tanh` 喜欢有一个更窄的随机初始化范围，所以我将其调整为 -0.01 和 0.01 之间。
- en: I also removed the error calculation, because we’re not ready for that yet.
    Technically, `softmax` is best used with an error function called *cross entropy*.
    This network properly computes `layer_2_delta` for this error measure, but because
    we haven’t analyzed why this error function is advantageous, I removed the lines
    to compute it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我还移除了错误计算，因为我们还没有准备好进行这项工作。技术上讲，`softmax` 最好与一个称为 *交叉熵* 的错误函数一起使用。这个网络正确地计算了用于此误差测量的
    `layer_2_delta`，但由于我们还没有分析为什么这个误差函数是有利的，所以我移除了计算它的代码。
- en: Finally, as with almost all changes made to a neural network, I had to revisit
    the `alpha` tuning. I found that a much higher `alpha` was required to reach a
    good score within 300 iterations. And voilà! As expected, the network reached
    a higher testing accuracy of 87%.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像对神经网络所做的几乎所有改动一样，我不得不重新审视`alpha`调整。我发现为了在300次迭代内达到良好的分数，需要更高的`alpha`值。哇哦！正如预期的那样，网络达到了更高的测试准确率，达到了87%。
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
